[
  {
    "title": "GLaMM: Pixel Grounding Large Multimodal Model",
    "link": "https://arxiv.org/pdf/2311.03356.pdf",
    "upvote": "31",
    "text": "GLaMM\n: Pixel Grounding Large Multimodal Model\nHanoona Rasheed1*, Muhammad Maaz1*, Sahal Shaji1, Abdelrahman Shaker1, Salman Khan1,2\nHisham Cholakkal1, Rao M. Anwer1,3, Eric Xing1,4, Ming-Hsuan Yang5,7, Fahad S. Khan1,6\n1Mohamed bin Zayed University of AI, 2Australian National University, 3Aalto University\n4Carnegie Mellon University, 5University of California - Merced, 6Link\u00f6ping University, 7Google Research\nhanoona.bangalath@mbzuai.ac.ae, muhammad.maaz@mbzuai.ac.ae\nhttps://mbzuai-oryx.github.io/groundingLMM\nAbstract\nLarge Multimodal Models (LMMs) extend Large Lan-\nguage Models to the vision domain.\nInitial LMMs used\nholistic images and text prompts to generate ungrounded\ntextual responses. Recently, region-level LMMs have been\nused to generate visually grounded responses. However,\nthey are limited to only referring to a single object cate-\ngory at a time, require users to specify the regions, or can-\nnot offer dense pixel-wise object grounding. In this work,\nwe present Grounding LMM (GLaMM), the first model that\ncan generate natural language responses seamlessly in-\ntertwined with corresponding object segmentation masks.\nGLaMM not only grounds objects appearing in the con-\nversations but is flexible enough to accept both textual and\noptional visual prompts (region of interest) as input. This\nempowers users to interact with the model at various lev-\nels of granularity, both in textual and visual domains. Due\nto the lack of standard benchmarks for the novel setting of\nvisually Grounded Conversation Generation (GCG), we in-\ntroduce a comprehensive evaluation protocol with our cu-\nrated grounded conversations.\nOur proposed GCG task\nrequires densely grounded concepts in natural scenes at\na large-scale.\nTo this end, we propose a densely an-\nnotated Grounding-anything Dataset (GranD) using our\nproposed automated annotation pipeline that encompasses\n7.5M unique concepts grounded in a total of 810M regions\navailable with segmentation masks. Besides GCG, GLaMM\nalso performs effectively on several downstream tasks, e.g.,\nreferring expression segmentation, image and region-level\ncaptioning and vision-language conversations.\n1. Introduction\nFueled by the generative AI wave, Large Multimodal Mod-\nels (LMMs) have emerged as a pivotal advancement, bridg-\ning the gap between vision and language tasks [2]. Initial ef-\n*Equally contributing first authors.\nDescribe the image in detail. Please output \ninterleaved segmentation mask\nThe\nimage\nshowcases\na\nlarge,\nwhite\nbuilding\nwith\na\nred\nroof, surrounded by a well-manicured lawn and palm trees.\nThe sky is visible over the building, the pavement, and the\ngrass. The grass is also seen extending to the pavement.\nFigure 1. Grounded Conversation Generation with GLaMM.\nOur multimodal conversational model can provide natural lan-\nguage responses grounded at the pixel-level in the input image.\nDifferent levels of granularity are depicted in the output ground-\nings, e.g., things (building, tree), stuff (grass, sky, pavement), and\nobject parts (roof as a subpart of the building) alongside the ob-\nject attributes (white house, red roof, well-manicured lawn) and\nobject relationships (grass extending to the pavement, sky over the\nbuilding). Existing LMMs, open-source (e.g., LLaVa, miniGPT4,\nShikra, Kosmos-2) and closed-source (e.g., GPT4-V, Bard), do not\noffer pixel-level grounded conversational capability.\nforts like [6, 8, 22, 29, 52, 61] demonstrate effective textual\nresponses based on input images. Although these models\nare sophisticated, they cannot still ground their responses in\nthe visual context. Such grounding is crucial for advanced\napplications like detailed visual understanding, interactive\nembodied agents, and localized content manipulation. Re-\ncent efforts have started to address this limitation by en-\nabling models to process user-defined regions specified via\nbounding boxes [5, 31, 35, 36, 57].\n1\narXiv:2311.03356v2  [cs.CV]  29 Dec 2023\nA few recent works have explored grounded text re-\nsponse generation [5, 21, 35, 59] but do not provide de-\ntailed pixel-level groundings. Parallel to these, efforts have\nbeen made in the referring segmentation literature to ground\ntextual descriptions in natural images [21]. However, they\nare limited to grounding a single object and cannot en-\ngage in natural, coherent conversations, thereby restricting\ntheir practical applicability in interactive tasks that demand\na deep understanding of both visual and textual content. To\naddress these limitations of existing works, we introduce\nGrounding LMM (GLaMM), that simultaneously provides\nin-depth region understanding, pixel-level groundings, and\nconversational abilities through an end-to-end training ap-\nproach (see Fig. 1 and Tab. 1).\nTo address the lack of benchmarks for visually grounded\nconversations, we introduce the novel task of Grounded\nConversation Generation (GCG). The GCG task aims to\nproduce natural language responses interleaved with ob-\nject segmentation masks.\nThis challenging task unifies\nseveral existing tasks in computer vision that are typi-\ncally treated in isolation, i.e., referring expression segmen-\ntation, image and region-level captioning, phrase ground-\ning, and vision-language conversations. Thereby, our uni-\nfied model and proposed pretraining dataset can effectively\ntransfer to several downstream tasks (referring expression\nsegmentation, region-level captioning, image captioning,\nand conversational-style QA). We present GLaMM as the\nfirst model specifically designed for this challenging task.\nUnlike prior works, GLaMM can work with both textual\nand visual prompts and can generate visually grounded out-\nputs, thus offering a versatile user experience.\nDetailed region-level understanding requires the labo-\nrious process of collecting large-scale annotations for im-\nage regions. We propose an automated pipeline to anno-\ntate the large-scale Grounding-anything Dataset (GranD)\nto alleviate the manual labeling effort. Leveraging the au-\ntomated pipeline with dedicated verification steps, GranD\ncomprises 7.5M unique concepts anchored in 810M re-\ngions, each with a segmentation mask.\nUsing state-of-\nthe-art vision and language models, the dataset annotates\nSAM [18] images through a multi-level hierarchical scheme\nthat enhances annotation quality. With 11M images, 84M\nreferring expressions, and 33M grounded captions, GranD\nsets a new benchmark in comprehensiveness. In addition to\nthe automatically generated dataset for the GCG, we pro-\nvide the first high-quality dataset for grounded conversa-\ntions obtained by revamping the existing manually anno-\ntated datasets [16, 37, 49] for GCG using GPT-4 [34] in-\ncontext learning. We refer to the high-quality dataset as\nGranDf, denoting its suitability for fine-tuning.\nOur work has three main contributions:\n\u2022 We present GLaMM, the first model capable of gener-\nating natural language responses seamlessly integrated\nwith object segmentation masks. Unlike existing mod-\nels, GLaMM accommodates textual and visual prompts,\nfacilitating enhanced multimodal user interaction.\n\u2022 Recognizing the lack of standardized benchmarks for\nvisually grounded conversations, we propose the new\nGrounded Conversation Generation (GCG) task. We also\nintroduce a comprehensive evaluation protocol to mea-\nsure the efficacy of models for GCG that unifies multiple\nisolated tasks, filling a significant gap in the literature.\n\u2022 To facilitate model training and evaluation, we cre-\nate Grounding-anything Dataset (GranD), a large-scale\ndensely annotated dataset. Developed using an automatic\nannotation pipeline and verification criteria, it encom-\npasses 7.5M unique concepts grounded in 810M regions.\nAdditionally, we propose GranDf, a high-quality dataset\nexplicitly designed for the GCG task finetuning, by re-\npurposing existing open-source datasets.\n2. Related Work\nLMMs provide a versatile interface for a diverse array of\ntasks, encompassing language and vision. Prominent mod-\nels such as BLIP-2 [24], LLaVA [29], InstructBLIP [6]\nand MiniGPT-4 [61] first conduct image-text feature align-\nment followed by instruction tuning.\nOther representa-\ntive works include Otter [22], mPLUG-Owl [52], LLaMa-\nAdapter [56], Video-ChatGPT [32], InternGPT [31]. How-\never, these approaches lack region-specific understanding.\nRecent works like Kosmos-2 [35], Shikra [5], GPT4RoI\n[57], VisionLLM [44], Ferret [53] and All-Seeing [45] aim\nto allow region-specific conversation. Some methods [5, 35,\n45, 53] input location bins and bounding boxes with image\ndata for region-level understanding, relying on the LLM ex-\nclusively for interpreting these regions. GPT4RoI advances\nthis by using spatial boxes and RoI-aligned features for in-\nput and training on region-text pairs. BuboGPT [59] uti-\nlizes an off-the-shelf grounding model [30] and matches the\ngroundings with the language response. In contrast, LISA\n[21] utilizes embeddings from the vision language model\nand the SAM [18] decoder to generate output segmentation\nmasks. However, LISA cannot comprehend specific image\nregions or handle multiple instances.\nTo classify the LMM landscape, methods can be parti-\ntioned into four distinct categories (see Tab. 1 - separated\nvia dotted lines). The first encompasses models effective\nin textual responses but lacking in region-specific capabil-\nities [6, 8, 22, 29, 51, 52, 61]. In contrast, among models\nthat handle region inputs or offer visual grounding, three\nmore categories emerge. The first of these incorporates ex-\nternal vision modules [31, 59], and the next relies exclu-\nsively on LMMs for region understanding [5, 35, 36, 44].\nThe last category combines specialized vision modules with\nLMMs, trained end-to-end for a comprehensive understand-\ning of regions [21, 45, 57]. Our approach belongs to the\n2\nMethod\nImage\nInput / Output\nRegion\nPixel-Wise\nMulti-turn\nEnd-End\nRegion\nMulti-Region\nEnc. / Dec.\nGrounding\nConversation\nModel\nMM-REACT (arXiv-23) [51]\n/\n/\n/\nLLaVA (NeurIPS-23) [29]\n/\n/\n/\nminiGPT4 (arXiv-23) [61]\n/\n/\n/\nmPLUG-OWL (arXiv-23) [52]\n/\n/\n/\nLLaMA-Adapter v2 (arXiv-23) [8]\n/\n/\n/\nOtter (arXiv-23) [22]\n/\n/\n/\nInstruct-BLIP (arXiv-23) [6]\n/\n/\n/\nInternGPT (arXiv-23) [31]\n/\n/\n/\nBubo-GPT (arXiv-23) [59]\n/\n/\n/\nVision-LLM (arXiv-23) [44]\n/\n/\n/\nDet-GPT (arXiv-23) [36]\n/\n/\n/\nShikra (arXiv-23) [5]\n/\n/\n/\nKosmos-2 (arXiv-23) [35]\n/\n/\n/\nGPT4RoI (arXiv-23) [57]\n/\n/\n/\nASM (arXiv-23) [45]\n/\n/\n/\nLISA (arXiv-23) [21]\n/\n/\n/\nGLaMM (ours)\n/\n/\n/\nTable 1. Comparison of recent Large Multimodal Models (LMMs) emphasizing their capabilities for region-level understanding. The\nInput denotes models that can process regions defined by users via bounding boxes, with Multi-Region indicating models that can handle\nmultiple such regions. The Output represents models capable of delivering grounded responses. While some methods employ external\nvision modules for region understanding, others rely solely on the LMM, which may result in imprecise localization. However, a few\nintegrate specialized vision modules and LMMs, as indicated by the Region Enc./Dec.. The End-End Model distinction separates models\nthat leverage LMMs for region understanding from those employing external modules. Pixel-wise Grounding highlights models that can\nrespond with segmentation masks, and Multi-turn Conversation represents models that can hold an interactive dialogue with the user.\nAmong these, our proposed GLaMM stands out by offering comprehensive region understanding, pixel-wise grounding in its responses,\nconversational capabilities, and an end-to-end training approach.\nlast category and distinctly offers pixel-level grounding to-\ngether with multi-turn conversations and the flexibility to\noperate on both input images and specific regions. Further,\nwe provide large-scale instance-level grounded visual un-\nderstanding dataset that allows generalizability of GLaMM\nto multiple vision-language tasks.\n3. Method\nExisting Large Multimodal Models (LMMs) either gener-\nate ungrounded text or are restricted by limitations such as\nsingle-object grounding, user-specified region inputs, or the\nlack of dense pixel-level object grounding (see Tab. 1). Our\nGrounding LMM (GLaMM) aims to overcome these limita-\ntions by generating natural language responses seamlessly\nintegrated with object segmentation masks. This enables a\nvisually grounded human-machine conversation.\n3.1. GLaMM Architecture\nGLaMM consists of five core components: i) Global Im-\nage Encoder, ii) Region Encoder, iii) LLM, iv) Grounding\nImage Encoder, and v) Pixel Decoder. These components\nare cohesively designed to handle both textual and optional\nvisual prompts (image level and region), allowing for in-\nteraction at multiple levels of granularity and generating\ngrounded text responses (see Fig. 2). These blocks together\nenable scene-level, region-level, and pixel-level grounding,\nas explained next. Training specifics are detailed in Ap-\npendix A.2.\nScene-Level Understanding: To achieve a holistic under-\nstanding of the scene, we employ ViT-H/14 CLIP [38] as\nour global image encoder (I), in conjunction with a vicuna-\nbased LLM (L) and a vision-to-language (V-L) projection\nlayer (f). Specifically, given an image ximg and a text in-\nstruction xt, the image is first encoded into a feature vec-\ntor Ix = I(ximg) \u2208 RDv and projected to language space\nf(Ix) \u2208 RDt. The LLM then integrates both the projected\nimage features and the text instruction to generate output yt:\nyt = L\n\u0010\nf(Ix), xt\n\u0011\n.\nThis maps image features to language space, enabling\nGLaMM to offer holistic scene understanding, achieved\nthrough specific prompts like, \u201cThe <image> provides an\noverview of the image. Could you please give me\na detailed description of the image?\u201d The <image>\ntoken is replaced with 256 tokens from the CLIP global im-\nage encoder.\nRegion-Level Understanding: Building on the shortcom-\nings of existing models that can handle only image-level\n3\nGlobal Image \nEncoder\nLLM\nGrounding \nImage Encoder\nPixel Decoder\nRegion Encoder\nL P\n-\nV L\n-\nA large, colorful hot air balloon is flying over the river. \nThe sky is overarching the river, trees, and buildings.\nCan you give a detailed\ndescription of this image?\nCan you segment the balloon in \nthis given picture? \nSure, the \nsegmentation is\nReferring Expression Segmentation\nCould you tell me more \nabout this region?\nA yellow, red and \nblue hot air balloon\nRegion Level Captioning\nPlease describe the given image.\nA\nhot\nair\nballoon\nis\nseen\nin\nthe\nair, flying over a river. The balloon,\nhigh up in the sky, is yellow and red\nin\ncolor,\nwith\na\nbridge\nand\ntrees\nvisible in the background.\nImage Level Captioning\nGrounded Conversation Generation\nImage-region prompts\nOutput prompts\nA large hot air \nballoon\nA large body of \nwater\nCan you segment phrases in, \u201cA <hot air \nballoon> over the <river>\u201d\nA <hot air balloon> \nover the <river>\nPhrase Grounding\nFigure 2. GLaMM\u2019s architecture. The figure illustrates our model architecture, showcasing its ability to offer scene-level understanding,\nregion-level interpretation, and pixel-level grounding. Top: The core components of GLaMM, including the global image encoder, region\nencoder, LLM, grounding image encoder, and pixel decoder, are cohesively tailored for vision-language tasks across different granularities.\nThe vision-to-language (V-L) projection layer efficiently maps image features into the language domain, and the pixel decoder utilizes the\nlanguage-to-prompt (L-P) projection layer, transforming text embeddings related to segmentation into the decoder space. A major feature\nof GLaMM is its ability to perform our newly introduced Grounded Conversation Generation (GCG) task. This highlights the model\u2019s\ncapability to anchor specific phrases to corresponding segmentation masks in the image. Bottom: The diverse downstream applications of\nGLaMM, including referring expression segmentation, region-level captioning, image-level captioning, and phrase grounding.\nvisual inputs, and in alignment with recent work [57], the\nregion encoder (R) extends the model\u2019s capability to in-\nterpret and interact with user-specified regions in an image.\nThis component constructs a hierarchical feature pyramid\nfrom four selected CLIP global image encoder layers, fol-\nlowed by RoIAlign [10] to generate a 14x14 feature map.\nCombining these features yields a unified region-of-interest\n(RoI) representation. To facilitate region-targeted responses\nfrom GLaMM, we augment the existing vocabulary with a\nspecialized token <bbox>. This is integrated into a prompt\nlike, \u201cThe <image> provides an overview of the\nimage.\nCan you provide a detailed description\nof the region <bbox>?\u201d. Here the <bbox> token is re-\nplaced with the RoI extracted features.\nFor the region-level understanding, alongside the global\nimage features Ix, we also take user-specified regions r as\ninputs, encoded as Rx = R(Ix, r), followed by projection\nto language space through the same V-L projection layer f\nemployed in scene-level understanding. We augment the\ntext instruction xt by replacing <bbox> tokens with the cor-\nresponding region features to obtain x\u2032\nt = [xt \u2190 f(Rx)].\nThe LLM then generates the output yt as,\nyt = L\n\u0010\nf(Ix), x\u2032\nt\n\u0011\n.\nPixel-Level Grounding: Utilizing the grounding image en-\ncoder denoted as V and the pixel decoder represented as P,\nGLaMM facilitates fine-grained pixel-level object ground-\ning, allowing it to ground its responses visually. We instan-\ntiate V with a pretrained SAM encoder [18] and design P\nbased on a SAM decoder-like architecture. To activate the\npixel-level grounding, our model\u2019s vocabulary is augmented\nwith a specialized token, <SEG>. Prompts, such as \u201cPlease\nsegment the \u2018man in red\u2019 in the given image,\"\ntrigger the model to generate responses with corresponding\n<SEG> tokens. A language-to-prompt (L-P) projection layer\n(g) transforms the last-layer embeddings corresponding to\n<SEG> tokens (lseg) into the decoder\u2019s feature space. Sub-\nsequently, P produces binary segmentation masks M,\nM = P\n\u0010\ng(lseg), V(ximg)\n\u0011\n, s.t., Mi \u2208 {0, 1}.\nUsing an end-to-end training approach, GLaMM excels in\nregion understanding, pixel-level grounding, and conver-\nsational capabilities.\nHowever, due to the lack of stan-\ndard benchmarks for the novel setting of generating visually\ngrounded detailed conversations, we introduce a novel task,\nGrounded Conversation Generation (GCG), and a compre-\nhensive evaluation protocol as explained next.\n3.2. Grounded Conversation Generation (GCG)\nThe objective of the GCG task is to construct image-level\ncaptions with specific phrases directly tied to corresponding\nsegmentation masks in the image. For example, \u201c<A man>\nand <a boy> sit on <a bench> next to <an old white\ncar>.\u201d, shown in Fig. 3 (left), features how each bracketed\nphrase (highlighted in the image) is anchored to a unique\nimage segmentation mask. This creates a densely annotated\ncaption that aligns textual descriptions with visual regions,\nenriching the image\u2019s contextual interpretation.\nGCG Output Representation:\nA sample prompt for\nquerying the model in this task is: \u201cCould you please\n4\nA man and a boy sit on a bench next\nto an old white car.\nA\nwoman\nin\na\nnavy\nblue\njacket\nand\nhat with a hair ribbon in her hair.\nA soccer player in a red uniform is about\nto kick the ball while a player in a white\nuniform is trying to block the shot.\nFigure 3. Qualitative results of GLaMM on grounded conversation generation (GCG). Given user queries, the LMM generates textual\nresponses and grounds objects, object parts, attributes, and phrases using pixel-level masks, showing its detailed understanding.\ngive me a detailed description of the image?\nPlease\nrespond\nwith\ninterleaved\nsegmentation\nmasks\nfor\nthe\ncorresponding\nparts\nof\nthe\nanswer.\u201d The model generates a detailed caption along\nwith interleaved segmentation masks, employing the for-\nmat \u201c<p>A man</p><SEG> and <p>a boy</p><SEG> sit\non <p>a bench</p><SEG> next to <p>an old white\ncar</p><SEG>.\u201d\nWe use special tokens, namely <p>,\n</p> and <SEG>, to delineate the start and end of each\nphrase and its corresponding region mask, respectively.\nOur GranD dataset is meticulously constructed using a\nstage-wise annotation pipeline, capturing annotations that\nrange from fine-grained specifics to high-level context. This\nenables the automatic generation of densely annotated cap-\ntions well-suited for the GCG task, thereby significantly fa-\ncilitating GLaMM\u2019s training for this task. Some qualitative\nresults of our model on the GCG task are shown in Fig. 3.\nEvaluation Criteria: We introduce a benchmarking suite\nfor GCG, with a validation set of 2.5K images and a test set\nof 5K images. Four key aspects are evaluated: i) generated\ndense caption quality, ii) mask-to-phrase correspondence\naccuracy, iii) generated mask quality, and iv) region-specific\ngrounding ability. Metrics include METEOR and CIDEr\nfor captions, class-agnostic mask AP for grounding, mask\nIoU for segmentation, and mask recall for region-specific\ngrounding (refer to Appendix A.1 for details).\nHaving delineated the architecture of GLaMM and the\nintricacies of the GCG task, it becomes imperative to ad-\ndress the scarcity of large-scale annotated data for region-\nlevel understanding.\nWe next focus on devising a new,\ndensely annotated dataset to optimize the model\u2019s perfor-\nmance and overcome this data limitation.\n4. Data Annotation Pipeline\nWe introduce our automated annotation pipeline used to\ncreate the Grounding-anything Dataset (GranD). GranD is\na comprehensive, multi-purpose image-text dataset offer-\ning a range of contextual information, from fine-grained to\nhigh-level details. It aims to overcome challenges in im-\nage understanding and dense pixel-level grounding, thereby\nexpanding the capabilities of visual instruction tuning in\nLMMs.\nThe pipeline contains four distinct levels (see Fig. 4).\ni) Level-1 focuses on object localization and provides se-\nmantic labels, segmentation masks, attributes, and depth\ninformation. ii) Level-2 defines relationships between de-\ntected objects. iii) Level-3 organizes information from the\nfirst two levels into a hierarchical scene graph, used to\ngenerate dense captions using LLM with in-context exam-\nples. iv) Level-4 offers enriched contextual information for\na deeper understanding of the scene, going beyond what\u2019s\nobserved (e.g., historical information of a landmark). Please\nrefer to Appendix A.4 for pipeline implementation details.\n4.1. Object Localization and Attributes (Level-1)\nIn level-1, the focus is on detailed object identification\nwithin images.\nFirst, object-bounding boxes are identi-\nfied using multiple SoTA object detection models. Class-\nagnostic NMS is applied to each model to filter out false\npositives. After this step, bounding boxes from different\nmodels are compared using IoU, with a bounding box re-\ntained as an object only if detected by at least two other de-\ntection models. We also generate attributes for each filtered\nobject using region-based vision-language models and in-\ncorporate depth information to contextualize each object\u2019s\nrelative position within the scene.\n4.2. Relationships and Landmarks (Level-2)\nIn level-2, multiple short textual descriptions of the overall\nscene are generated. Phrases extracted from these descrip-\ntions are grounded to specific objects in level-1 to form re-\nlationships. These relationships articulate connections be-\ntween multiple objects or define an object\u2019s role within the\nscene. Further, each scene is assigned a landmark category\nthat includes a primary and a more specific sub-category\n(see Tab. 7 in Appendix 7).\n5\nA large brown dog wearing a chain collar\nA dog sitting on the steps\nObjects and Attributes\ndog, pub dog, a brown and white dog\ndog collar, black color, chain collar\nbell, cowbell\nsteps, stairs, the steps of a building\nsack, a large white bag with black writing\nLandmarks: Outdoor \u2013 Urban Landscape\nA large brown dog is sitting on the steps of a building. It\nis wearing a black chain dog collar. The collar has a\ncowbell attached to it. There is a bag in the background\nwith black writings on it.\nExtra Context\nDogs, especially pugs and bulldogs, have been a part of human families for thousands of years, serving as loyal companions. They have been bred for specific traits,\nmaking them popular pets. Dogs have been trained for various tasks, including assisting people with disabilities and serving as search and rescue animals. Dog collars,\noften bearing identification tags, are essential for keeping pets safe and ensuring they can be returned home if lost. Cowbells, once used to signal the arrival of a cow,\nhave been repurposed as dog collars, providing a distinct sound to help locate a dog if it wanders off. In outdoor urban landscape, dogs are often found sitting on\nsteps, as they may choose to rest in spots that offer a good view of their surroundings.\nObject locatlization and attributes\n\u00a7 Image Tagging and Object Detection\n\u00a7 Open Vocabulary Detection\n\u00a7 Region Attribute Detection\nLevel-1\nRelationships\n\u00a7 Short Captions and Phrase extraction \n\u00a7 Grounding expression\n\u00a7 Landmarks\nLevel-2\nScene Graph & Dense Captioning\n\u00a7 Hierarchical Scene Graph\n\u00a7 In-contex Learning with LLM\n\u00a7 Verification Pipeline\nLevel-3\nExtra Contextual Insights\n\u00a7 Lanmark Details\n\u00a7 History and Background\n\u00a7 Precautionary Measures\nLevel-4\nCowbell attached to dog collar\nRelationships and Landmarks\nScene Graph\nDense Grounded Caption\nFigure 4. Automatic Annotation Pipeline of the Grounding-anything Dataset (GranD). Comprising four levels, this pipeline plays a\npivotal role in generating GranD\u2019s 7.5M unique concepts grounded in 810M regions. level-1 details objects and attributes, level-2 includes\nshort captions and relational markers, level-3 builds a scene graph, hierarchically organizing information from earlier levels to facilitate\nLLM for grounded dense captions, level-4 provides additional historical and societal context for a richer visual understanding.\n4.3. Scene Graph and Dense Captioning (Level-3)\nIn level-3, object attributes and labels from level-1 are\ncombined with the relationships and phrases obtained from\nlevel-2 to form a hierarchical scene graph. This structured\ndata serves as a query for LLM to generate dense image\ncaptions. To provide additional context, depth values and\nbounding box coordinates are used to assign each object to\nspecific spatial layers within the scene, such as immediate\nforeground, foreground, midground, or background. Addi-\ntionally, short scene-level captions are incorporated into the\nscene graph to enhance LLMs\u2019 contextual understanding.\nDense Captioning Verification: To enhance the fidelity of\nthe LLM-generated dense captions, we implement an auto-\nmatic verification pipeline using chain-of-thoughts prompt-\ning. This pipeline produces a checklist of objects derived\nfrom the generated dense caption expected to be present in\nthe image. The associated caption is flagged as inaccurate\nif any object specified in the checklist is absent from the\nscene graph. Such captions are then regenerated, incorpo-\nrating feedback from the initial assessment.\n4.4. Extra Contextual Insights (Level-4)\nLevel-4 builds on the scene graph from level-3 to obtain a\nmore detailed visual understanding. we query LLM to ex-\ntract extended contextual insights beyond basic object iden-\ntification and relationships, including details about the land-\nmarks, historical context, guidelines for interacting with the\nscene, and even predictive elements about future events. To\nfacilitate this, we prompt LLM with in-context examples.\nDataset\nImages\nRegions\nConcepts\nTokens\nCaptions\u2020\nCOCO [25]\n0.1M\n0.9M\n80\n-\n-\nLVIS [9]\n0.1M\n1.5M\n1,203\n-\n-\nObjects365 [42]\n0.6M\n10.1M\n365\n-\n-\nOpen Images [20]\n1.5M\n14.8M\n600\n-\n-\nBigDetection [4]\n3.5M\n36.0M\n600\n-\n-\nV3Det [43]\n0.2M\n1.5M\n13,029\n-\n-\nVG [19]\n0.1M\n0.3M\n18,136\n51.2M\n-\nSA-1B [18]\n11M\n1.1B\n-\n-\n-\nAS-1B [45]\n11M\n1.2B\n3.5M\n132.2B\n-\nGranD (Ours)\n11M\n810M\n7.5M\n5.0B\n33M\nTable 2.\nGranD versus existing datasets.\nGranD uniquely\nprovides three \u2020grounded captions per image with segmentation\nmasks for every region. AS-1B is shaded to denote its concurrent,\nnon-public status at the time of this publication.\nUtilizing our automated annotation pipeline, we annotate\na corpus of 11M SAM images [18], which are inherently di-\nverse, high-resolution, and privacy-compliant. The result-\ning dataset comprises 810M regions, each associated with\na segmentation mask, and includes 7.5M unique concepts.\nFurther, the dataset features 84M referring expressions,\n22M grounded short captions, and 11M densely grounded\ncaptions. To our knowledge, this is the first dataset of this\nscale generated entirely through an automated annotation\npipeline (see Tab. 2 for details and Fig. 15 for dataset sam-\nple visualizations).\n4.5. Building GranDf for GCG\nMotivated by the need for higher-quality data in fine-tuning\nstage, we introduce GranDf.\nIt contains 214K image-\ngrounded text pairs with 2.5K validation and 5K test sam-\n6\nModel\nValidation Set\nTest Set\nM\nC\nAP50\nmIoU\nRecall\nM\nC\nAP50\nmIoU\nRecall\nBuboGPT [59]\n17.2\n3.6\n19.1\n54.0\n29.4\n17.1\n3.5\n17.3\n54.1\n27.0\nKosmos-2 [35]\n16.1\n27.6\n17.1\n55.6\n28.3\n15.8\n27.2\n17.2\n56.8\n29.0\nLISA* [21]\n13.0\n33.9\n25.2\n62.0\n36.3\n12.9\n32.2\n24.8\n61.7\n35.5\nGLaMM\u2020\n15.2\n43.1\n28.9\n65.8\n39.6\n14.6\n37.9\n27.2\n64.6\n38.0\nGLaMM\n16.2\n47.2\n30.8\n66.3\n41.8\n15.8\n43.5\n29.2\n65.6\n40.8\nTable 3. GLaMM Performance\non GCG Task:\nMetrics in-\nclude METEOR (M), CIDEr (C),\nAP50, mIoU, and Mask Recall.\nLISA* indicates LISA adapted\nfor\nGCG.\nGLaMM\u2020\ndenotes\ntraining excluding 1000 human\nannotated images.\nGLaMM\nshows better performance.\nMethod\nrefCOCO\nrefCOCO+\nrefCOCOg\nval\ntestA\ntestB\nval\ntestA\ntestB\nval(U)\ntest(U)\nCRIS [47]\n70.5\n73.2\n66.1\n65.3\n68.1\n53.7\n59.9\n60.4\nLAVT [50]\n72.7\n75.8\n68.8\n62.1\n68.4\n55.1\n61.2\n62.1\nGRES [26]\n73.8\n76.5\n70.2\n66.0\n71.0\n57.7\n65.0\n66.0\nX-Decoder [63]\n-\n-\n-\n-\n-\n-\n64.6\n-\nSEEM [64]\n-\n-\n-\n-\n-\n-\n65.7\n-\nLISA-7B [21]\n74.9\n79.1\n72.3\n65.1\n70.8\n58.1\n67.9\n70.6\nGLaMM\n79.5\n83.2\n76.9\n72.6\n78.7\n64.6\n74.2\n74.9\nTable 4.\nQualitative Assess-\nment of GLaMM in Referring-\nExpression Segmentation: Per-\nformance across refCOCO, re-\nfCOCO+,\nand\nrefCOCOg\nin\ngenerating accurate segmenta-\ntion masks based on text-based\nreferring expressions surpasses\nthat of closely related work, in-\ncluding LISA which is specifi-\ncally designed for this task.\nples. GranDf comprises two primary components: one sub-\nset is manually annotated, and the other subset is derived by\nre-purposing existing open-source datasets.\nWe\nextend\nopen-source\ndatasets\u2014namely\nFlickr-\n30K [37], RefCOCOg [16], and PSG [49] by generating\ncompatible GCG annotations. For RefCOCOg, we use the\ndataset\u2019s referring expressions and their connected masks.\nThese expressions offer concise descriptions of distinct\nobjects in the image. With the aid of GPT-4, we seam-\nlessly blend these referring expressions with contextual\ninformation from COCO captions, crafting detailed yet\naccurate grounded captions while preserving the original\nreferring expressions. This ensures zero error in matching\nphrases with their corresponding segmentation masks.\nThis technique yields approximately 24K GCG samples.\nFor PSG, we leverage the dataset\u2019s triplet structures,\nwhich describe relations between two objects in a scene.\nThese triplets are integrated with COCO captions using\nGPT-4, resulting in densely annotated captions that can\nbe mapped to segmentation masks. This gives us around\n31K additional GCG samples.\nFor Flickr-30K, we use\nthe 158K Flickr captions and their referring expressions\nalongside associated bounding boxes. These boxes are then\naccurately segmented using HQ-SAM [17].\nIn addition, we contribute a minor, high-quality manual\nannotation set to benchmark the GCG task. Using GranD\u2019s\nautomatic annotations as a base, annotators refine referring\nexpressions to match SAM GT masks, yielding around 1000\nfocused samples for training and 1000 for evaluation (refer\nto Appendix D and Fig. 14 for designed prompts and dataset\nvisualizations).\n5. Experiments\nWe perform quantitative evaluations of GLaMM on six\nbenchmarks: i) Grounded Conversation Generation (GCG),\nii) referring-expression segmentation, iii) region-level cap-\ntioning, iv) image-level captioning, v) conversational-style\nquestion answering and vi) phrase grounding. We present\nthe first four benchmarks next, and the remaining are dis-\ncussed in Appendix B.\nGrounded Conversation Generation (GCG). We pretrain\nGLaMM on GranD dataset followed by fine-tuning on the\nGranDf dataset.\nThe results are presented in Tab. 3 on\nboth validation and test splits of the GranDf dataset (re-\nfer to Sec. 3.2 and Sec. 4.5 for details). GLaMM shows\nimproved performance compared to baseline methods. Pre-\ntrained models for BuboGPT and Kosmos-2 are sourced\nfrom official releases, and LISA is adapted and trained on\nthe GranDf dataset for the GCG task. GLaMM\u2020 denotes\nthe variant trained on GranDf dataset excluding the 1000\nhuman-annotated images. Qualitative results are shown in\nFig. 3 and supplementary Fig. 7.\nReferring Expression Segmentation.\nIn this task, the\nmodel processes an image and a text-based referring expres-\nsion to output a segmentation mask. The prompt used is,\n\u201cPlease segment the <referring expression> in the\nimage.\" The model responds with \u201cSure, it is <SEG>.\",\nwhere the <SEG> token is decoded to obtain the mask. We\nachieve better results over recent works like LISA on the\nrefCOCO, refCOCO+, and refCOCOg validation and test\nsets in Tab. 4. This demonstrates the efficacy of our GranD\ndataset, offering the model extensive concept vocabulary\nduring pre-training (refer to Fig. 5 (middle) and supplemen-\ntary Fig. 8 for qualitative results).\n7\nCan you please describe this region?\nA Yellow Volkswagen Beetle\nNo, it is an old model\nIs this car a new model ?\nCan you segment the McDonald's \ncup in this image?\nCan you please segment the smaller cup now?\nCould you give a comprehensive explanation \nof what can be found within this picture?\nTwo girls in costume stand in front of\na\nblack\ncurtain\nwith\na\nsmiling\nface\nand skeleton head on it. They are next\nto a poster of a man in a tuxedo.\nFigure 5. Qualitative results of GLaMM\u2019s performance across downstream tasks. The figure showcases examples from three tasks:\nregion-level understanding (left), referring-expression segmentation (center), and image-level captioning (right), demonstrating its capabili-\nties in offering in-depth region understanding, pixel-level groundings, and conversational abilities through an end-to-end training approach.\nModel\nrefCOCOg\nVisual Genome\nMETEOR\nCIDEr\nMETEOR\nCIDEr\nGRIT [48]\n15.2\n71.6\n17.1\n142\nKosmos-2 [35]\n14.1\n62.3\n-\n-\nGPT4RoI [57]\n-\n-\n17.4\n145.2\nGLaMM (ZS)\n15.7\n104.0\n17.0\n127.0\nGLaMM (FT)\n16.2\n106.0\n19.7\n180.5\nTable 5. Performance of GLaMM in Region-Level Captioning:\nMetrics include METEOR and CIDEr scores, assessed on Visual\nGenome and refCOCOg Datasets, exhibiting competitive results.\nModel\nNoCap\nFlickr30k\nCIDEr\nSPICE\nCIDEr\nSPICE\nVinVLM [55]\n95.5\n13.5\n-\n-\nLEMON [12]\n106.8\n14.1\n-\n-\nSimVLM [46]\n110.3\n14.5\n-\n-\nCoCa [54]\n120.6\n15.5\n-\n-\nBLIP [23]\n113.2\n14.7\n-\n-\nBLIP-2 [24]\n121.6\n15.8\n-\n-\nInstructBLIP [6]\n123.1\n-\n82.8\n-\nShikra-13B [5]\n-\n-\n73.9\n-\nKosmos-1 [13]\n-\n-\n67.1\n14.5\nKosmos-2 [35]\n-\n-\n66.7\n-\nGLaMM\n106.8\n15.8\n95.3\n18.8\nTable 6. Performance of GLaMM in Zero-Shot Image Cap-\ntioning: Assessed on Flickr30k and NoCap datasets, showing fa-\nvorable results compared to recent models in the field.\nRegion Level Captioning. In this task, models generate\nregion-specific captions given an image, a user-specified\nregion via a bounding box and related text. We utilize a\nprompt like, \u201cCan you provide a detailed description\nof the region <bbox>?\u201d, to instruct the model for this\ntask, where the special token <bbox> is replaced with the\nactual region representations. We evaluate GLaMM on Vi-\nsual Genome and refCOCOg, using METEOR and CIDEr\nmetrics with results presented in Tab. 5. GLaMM shows\nimproved results over GRiT and GPT4RoI after fine-tuning\nand demonstrates robust zero-shot performance, highlight-\ning the significance of GranD\u2019s region-text pairs (refer to\nFig.5 (left) and supplementary Fig.9 for qualitative results).\nImage Level Captioning. For this task, GLaMM responds\nto queries like, \u201cCould you please give me a detailed\ndescription\nof\nthe\nimage?\" with a textual descrip-\ntion.\nWe evaluate GLaMM\u2019s zero-shot performance on\nFlickr30k [37] and NoCap [1] datasets, with Tab. 6 showing\nits favorable performance against recent image captioning\nmodels and other LMMs (refer to Fig. 5 (right) and supple-\nmentary Fig. 10 for qualitative results).\nRefer to Appendix C for qualitative results on six down-\nstream tasks, as well as conditional image generation.\n6. Conclusion\nWe introduce GLaMM, the first model capable of gener-\nating natural language responses intertwined with object\nsegmentation masks, allowing for enhanced multimodal\nuser interactions.\nRecognizing the lack of standard-\nized benchmarks for visually grounded conversations,\nwe introduce the novel task of Grounded Conversation\nGeneration and establish a comprehensive evaluation\nprotocol.\nTo facilitate research and model development,\nwe create the Grounding-anything Dataset (GranD), a\nlarge-scale, densely annotated dataset with 7.5 million\nunique concepts grounded in 810 million regions.\nOur\nautomated annotation pipeline ensures the reliability and\nscalability of this dataset used for our model. In addition\nto these contributions, we curated a dataset specifically\n8\ntailored for the GCG task (GranDf) by leveraging ex-\nisting open-source datasets, establishing a high-quality\nfine-tuning dataset to develop visually grounded conver-\nsations.\nOur model performs well on downstream tasks\nbesides GCG, including region and image captioning,\nreferring segmentation, and vision-language conversations.\nReferences\n[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Ste-\nfan Lee, and Peter Anderson. nocaps: novel object caption-\ning at scale. In ICCV, 2019. 8\n[2] Muhammad Awais, Muzammal Naseer, Salman Khan,\nRao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah,\nMing-Hsuan Yang, and Fahad Shahbaz Khan. Foundational\nmodels defining a new era in vision: A survey and outlook.\narXiv:2307.13721, 2023. 1\n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv:2308.12966, 2023. 3\n[4] Likun Cai, Zhi Zhang, Yi Zhu, Li Zhang, Mu Li, and Xi-\nangyang Xue. Bigdetection: A large-scale benchmark for\nimproved object detector pre-training. In CVPR, 2022. 6\n[5] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng\nZhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s\nreferential dialogue magic. arXiv:2306.15195, 2023. 1, 2, 3,\n8\n[6] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning.\narXiv:2305.06500, 2023. 1, 2, 3, 8\n[7] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xin-\nlong Wang, and Yue Cao. Eva-02: A visual representation\nfor neon genesis. arXiv:2303.11331, 2023. 1\n[8] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shi-\njie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, Hongsheng Li, and Yu Qiao.\nLlama-\nadapter v2:\nParameter-efficient visual instruction model.\narXiv:2304.15010, 2023. 1, 2, 3\n[9] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In CVPR,\n2019. 6\n[10] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017. 4\n[11] Matthew Honnibal and Ines Montani.\nspaCy: Industrial-\nstrength Natural Language Processing in Python. 2020. 1\n[12] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,\nZicheng Liu, Yumao Lu, and Lijuan Wang.\nScaling up\nvision-language pre-training for image captioning. In CVPR,\n2022. 8\n[13] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al.\nLanguage is\nnot all you need: Aligning perception with language mod-\nels. arXiv:2302.14045, 2023. 8\n[14] Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui\nFeng, Yuejie Zhang, Yaqian Li, Yandong Guo, and Lei\nZhang. Tag2text: Guiding vision-language model via image\ntagging. arXiv:2303.05657, 2023. 1\n[15] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion.\nMdetr-\nmodulated detection for end-to-end multi-modal understand-\ning. In ICCV, 2021. 1\n[16] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg.\nReferitgame: Referring to objects in pho-\ntographs of natural scenes. In EMNLP, 2014. 2, 7\n[17] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing\nTai, Chi-Keung Tang, and Fisher Yu. Segment anything in\nhigh quality. arXiv:2306.01567, 2023. 7\n[18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and\nRoss Girshick. Segment anything. arXiv:2304.02643, 2023.\n2, 4, 6\n[19] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. IJCV, 2017. 6\n[20] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Alexander Kolesnikov, et al. The\nopen images dataset v4: Unified image classification, object\ndetection, and visual relationship detection at scale. IJCV,\n2020. 6\n[21] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmenta-\ntion via large language model. arXiv:2308.00692, 2023. 2,\n3, 7, 1\n[22] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal model\nwith in-context instruction tuning. arXiv:2305.03726, 2023.\n1, 2, 3\n[23] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn ICML,\n2022. 8\n[24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2:\nbootstrapping language-image pre-training with\nfrozen image encoders and large language models. In ICML,\n2023. 2, 8, 1, 3\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 6\n[26] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Gen-\neralized referring expression segmentation. In CVPR, 2023.\n7\n[27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee.\nImproved baselines with visual instruction tuning.\narXiv:2310.03744, 2023. 3\n9\n[28] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee.\nImproved baselines with visual instruction tuning.\narXiv:2310.03744, 2023. 1, 2, 3\n[29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In NeurIPS, 2023. 1, 2, 3\n[30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang\nSu, Jun Zhu, et al.\nGrounding dino:\nMarrying dino\nwith grounded pre-training for open-set object detection.\narXiv:2303.05499, 2023. 2\n[31] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi\nWang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang\nYang, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen,\nXue Yang, Xizhou Zhu, Yali Wang, Limin Wang, Ping\nLuo, Jifeng Dai, and Yu Qiao. Interngpt: Solving vision-\ncentric tasks by interacting with chatgpt beyond language.\narXiv:2305.05662, 2023. 1, 2, 3\n[32] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and\nFahad Shahbaz Khan.\nVideo-chatgpt: Towards detailed\nvideo understanding via large vision and language models.\narXiv:2306.05424, 2023. 2\n[33] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim\nNeumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh\nMahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran\nShen, et al. Simple open-vocabulary object detection. In\nECCV, 2022. 1\n[34] OpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023. 2,\n1, 7\n[35] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shao-\nhan Huang, Shuming Ma, and Furu Wei.\nKosmos-2:\nGrounding multimodal large language models to the world.\narXiv:2306.14824, 2023. 1, 2, 3, 7, 8\n[36] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong,\nJipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, Lingpeng\nKong, and Tong Zhang. Detgpt: Detect what you need via\nreasoning. arXiv:2305.14167, 2023. 1, 2, 3\n[37] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. In ICCV,\n2015. 2, 7, 8\n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 3\n[39] Ren\u00e9 Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE TPAMI, 2020. 1\n[40] Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai\nZheng, Mu Li, Alex Smola, and Xu Sun.\nPrompt pre-\ntraining with twenty-thousand classes for open-vocabulary\nvisual recognition. arXiv:2304.04704, 2023. 1\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image syn-\nthesis with latent diffusion models. arXiv:2112.10752, 2021.\n3\n[42] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun.\nObjects365:\nA large-scale, high-quality dataset for object detection. In\nICCV, 2019. 6\n[43] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou,\nTong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det:\nVast vocabulary visual detection dataset. arXiv:2304.03752,\n2023. 6\n[44] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu\nQiao, et al. Visionllm: Large language model is also an open-\nended decoder for vision-centric tasks. arXiv:2305.11175,\n2023. 2, 3\n[45] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhen-\nhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu,\nZhiguo Cao, et al. The all-seeing project: Towards panop-\ntic visual recognition and understanding of the open world.\narXiv:2308.01907, 2023. 2, 3, 6\n[46] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai,\nYulia Tsvetkov, and Yuan Cao.\nSimvlm:\nSimple vi-\nsual language model pretraining with weak supervision.\narXiv:2108.10904, 2021. 8\n[47] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong\nGuo, Mingming Gong, and Tongliang Liu. Cris: Clip-driven\nreferring image segmentation. In CVPR, 2022. 7\n[48] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan,\nZicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A gen-\nerative region-to-text transformer for object understanding.\narXiv:2212.00280, 2022. 8, 1\n[49] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou,\nWayne Zhang, and Ziwei Liu. Panoptic scene graph gen-\neration. In ECCV, 2022. 2, 7\n[50] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-\nshuang Zhao, and Philip HS Torr. Lavt: Language-aware vi-\nsion transformer for referring image segmentation. In CVPR,\n2022. 7\n[51] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,\nEhsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,\nMichael Zeng, and Lijuan Wang. Mm-react: Prompting chat-\ngpt for multimodal reasoning and action. arXiv:2303.11381,\n2023. 2, 3\n[52] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Jun-\nfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl:\nModularization empowers large language models with mul-\ntimodality. arXiv:2305.03726, 2023. 1, 2, 3\n[53] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen\nZhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and\nYinfei Yang. Ferret: Refer and ground anything anywhere\nat any granularity. arXiv:2310.07704, 2023. 2\n[54] Jiahui Yu,\nZirui Wang,\nVijay Vasudevan,\nLegg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu.\nCoca:\nContrastive captioners are image-text foundation models.\narXiv:2205.01917, 2022. 8\n[55] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,\nLei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.\n10\nVinvl:\nMaking visual representations matter in vision-\nlanguage models. arXiv:2101.00529, 2021. 8\n[56] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.\nLlama-adapter:\nEfficient fine-tuning of language models\nwith zero-init attention. arXiv:2303.16199, 2023. 2\n[57] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-\nstruction tuning large language model on region-of-interest.\narXiv:2307.03601, 2023. 1, 2, 3, 4, 8\n[58] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,\nZhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo,\nYaqian Li, Shilong Liu, et al. Recognize anything: A strong\nimage tagging model. arXiv:2306.03514, 2023. 1\n[59] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi\nFeng, and Bingyi Kang. Bubogpt: Enabling visual ground-\ning in multi-modal llms. arXiv:2307.08581, 2023. 2, 3, 7\n[60] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-\nlez, and Ion Stoica. Judging llm-as-a-judge with mt-bench\nand chatbot arena. arXiv:2306.05685, 2023. 1, 2\n[61] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny.\nMinigpt-4:\nEnhancing vision-\nlanguage understanding with advanced large language mod-\nels. arXiv:2304.10592, 2023. 1, 2, 3\n[62] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with col-\nlaborative hybrid assignments training. In ICCV, 2023. 1\n[63] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,\nChunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang,\nLu Yuan, et al. Generalized decoding for pixel, image, and\nlanguage. In CVPR, 2023. 7\n[64] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,\nJianfeng Gao, and Yong Jae Lee. Segment everything every-\nwhere all at once. In NeurIPS, 2023. 7\n11\nGLaMM\n: Pixel Grounding Large Multimodal Model\nSupplementary Material\nWe provide supplementary material for a deeper under-\nstanding and more analysis related to the main paper, ar-\nranged as follows:\n1. Additional implementation details (Appendix A)\n2. Additional downstream tasks (Appendix B\n3. Additional qualitative results (Appendix C)\n4. Dataset visualizations (Appendix D)\n5. Limitations and future work (Appendix E)\n6. Ethics and societal impact (Appendix F)\nA. Additional Implementation Details\nA.1. Evaluation Metrics\nMask Recall: To quantify region-specific grounding, we\npropose a \u2018mask recall\u2019 metric, utilizing a two-tiered vali-\ndation approach. Initially, predicted masks are mapped to\nground-truth masks via a one-to-one set assignment, fol-\nlowed by IoU computation for these pairs. Pairs surpass-\ning a 0.5 IoU threshold proceed to a textual similarity as-\nsessment using BERT. A pair is considered a true positive\n(TP) only if both IoU and BERT similarity exceed their 0.5\nthresholds; otherwise, it is classified as a false positive (FP).\nThe mask recall is subsequently calculated using the stan-\ndard formula, normalizing the number of TPs by the total\nground-truth mask count.\nA.2. Model Architecture and Training\nIn all of our experiments, we use Vicuna LLM [60] with\n7B parameters. The design of region encoder is motivated\nfrom GPT4RoI [57] and grounding image encoder and pixel\ndecoder are inspired from LISA [21]. The V-L and L-P lay-\ners are implemented using 2 layer MLP with GELU activa-\ntion as in LLaVA-v1.5 [28]. We use PyTorch to implement\nour GLaMM and use Deepspeed zero-2 optimization during\ntraining.\nSpecifically, our model is trained using two types of\nlosses: auto-regressive cross-entropy loss for text gener-\nation and a linear combination of per-pixel binary cross-\nentropy loss and DICE loss for segmentation. During train-\ning, the global image encoder and grounding image encoder\nare kept frozen and the region encoder, projection layers (V-\nL and L-P) and the pixel decoder are fully finetuned, while\nthe LLM is LORA finetuned with \u03b1 = 8. Our codes and\npretrained models will be publicly released.\nA.2.1\nPretraining on GranD\nDuring pretraining GLaMM is trained on GranD dataset for\nreferring expression segmentation, region-level captioning,\nimage-level captioning and grounded conversation genera-\ntion (GCG) tasks simultaneously. We use a batch size of\n160 and train for a total of 35K iterations during pretrain-\ning. We use LORA-8 for efficiently adapting the LLM and\ninitialize the pretraining from GPT4RoI [57] for faster con-\nvergence. In the experiment tables in Section. 5, we refer\nto this model as GLaMM (ZS) which is obtained after pre-\ntraining on GranD.\nA.3. Finetuning on Downstream Tasks\nWe finetune GLaMM on multiple downstream tasks includ-\ning GCG, referring expression segmentation, region-level\ncaptioning and image-level captioning. For GCG, we fine-\ntune our model on GranDf dataset. A batch size of 160\nis used and the model is trained for 5K iterations in total.\nIt is worth noting that GranDf dataset is a combination of\nmultiple open-source datasets that we repurposed for GCG\ntask using GPT4 [34]. Please refer to Appendix. D for the\nprompts designed to query GPT4 for constructing GranDf\ndataset, along with the dataset visualizations.\nFor referring expressions segmentation, we finetune\nGLaMM on refCOCO, refCOCO+ and refCOCOg datasets.\nWe represent this model as GLaMM (FT) in Tab. 4. Simi-\nlarly, for region-level captioning, GLaMM (FT) is finetuned\non refCOCOg and Visual Genome datasets.\nFor image-\nlevel captioning, we fine tune GLaMM on LLaVA-Instruct-\n150K [29] dataset. For LLaVA-bench, the model is fine-\ntuned on LLaVA-Instruct-80K [29] instruction set. We use\neight NVIDIA A100-40GB GPUs in all of our pretraining\nand finetuning experiments.\nA.4. Automated Dataset Annotation Pipeline\nOur automated annotation pipeline incorporates diverse\nstate-of-the-art models at various levels. For Level-1, we\nuse Tag2Text [14] and RAM [58] for image tagging, Co-\nDETR [62], EVAv02 [7], OWL-ViT [33], and POMP [40]\nfor object localization, GRiT [48] and GPT4RoI [57] for\nattribute generation, and MiDAS [39] for depth estimation.\nLevel-2 leverages BLIP-2 [24] and LLaVA-v1.5 [28, 29] for\nscene descriptions and landmark categorization, SpaCy [11]\nfor phrase extraction, and MDETR [15] for phrase ground-\ning. For both Level-3 and Level-4, we use Vicuna-v1.5 [60]\nwith 13B parameters, supplemented with in-context exam-\nples. Please refer to Appendix A.4 for further details on\n1\nPrompt: The provided prompt is a scene graph, which is a structured representation of a scene detailing its various elements and their relationships. \nThe scene graph consists of:\n1. Layers of Depth: The scene is divided into different layers based on proximity - 'Immediate Foreground', 'Foreground', 'Midground', and \n'Background'. Each layer depicts objects or entities at that depth in the scene.\n2. Groups: Within each depth layer, objects and entities are clustered into groups, sometimes with specific attributes.\n3. Relationships: This section illustrates the interactions or spatial relationships between various objects or groups.\n4. Landmarks: It gives a broader view or categorization of the scene, defining its overarching theme or environment.\n---\n##Example - 1:\nPrompt: {scene_graph_1}\nDesired caption: {dense_caption_1}\n------\n##Example - 2:\nPrompt: {scene_graph_2}\nDesired caption: {dense_caption_2}\n------\nPlease provide a simple and straightforward 2-4 sentence image caption based on the following scene graph details: {scene_graph}.\nCreate the caption as if you are directly observing the image. Do not mention the use of any source data like 'The relationship indicates ...' or 'No \nrelations specified'. \nPrompt: \n##Example - 1:\nPrompt: {scene_graph_1}\nAdditional context: {caption_1}\n------\n##Example - 2:\nPrompt: {scene_graph_2}\nAdditional context: {caption_2}\n------\nProvide context based on the typical usage, history, potential dangers, and other interesting aspects surrounding the general theme presented by the \nobjects and elements in the following scene graph: {scene_graph}\nLimit the response to one paragraph with 5-7 sentences.\nDO NOT mention, refer to, or hint about \"objects\", \"scene\", or \"scene graph\".\nONLY focus on explaining use cases, history, potential dangers, etc.\n(a) Illustration of LLM in-context learning for dense captioning used in the construction of our GranD dataset.\n(b) Illustration of LLM in-context learning for extra contextual insights used in the construction of our GranD dataset.\nFigure 6. Prompts used to construct GranD dataset. The figure shows the prompts used to query Vicuna [60] to generate dense captions\nand the extra context in our automated training pipeline. We provide in-context examples to guide the LLM.\nimplementation and LLM prompts used across different\npipeline levels.\nWe design a fully automated dataset annotation pipeline\nusing multiple hierarchical levels in the visual domain to\nconstruct GranD dataset.\nThe segmentation masks for\nmost of the regions are obtained from SAM [18] annota-\ntions by comparing our detected labeled regions with SAM-\nprovided class-agnostic regions. For the remaining regions\nthat do not match with any of the SAM regions, we run\nSAM model with a bounding box query to obtain masks.\nOur automated annotation pipeline utilizes only open-\nsource models and incorporates a feedback loop using the\nchain of thoughts prompting via LLM. As it does not require\nfeedback from the human in the loop, it can be scaled to\ngenerate dense noisy labels for a larger number of images,\nwhich can then be used to pretrain a larger LMM. Given the\navailability of enough compute power, this could be a step\ntowards building a larger generic large multi-modal model.\nWe will release our GranD dataset along with the imple-\nmentation of our automated dataset annotation pipeline for\nfurther research. Below we present the LLM prompts we\nuse at different levels of our automated dataset annotation\npipeline.\nA.4.1\nLLM Prompts and In-context Learning\nLandmark categorization: We use LLaVA-v1.5-13B [28]\nmodel to assign landmark categories to each image. Please\nrefer to Tab. 7 for primary and fine categories used.\nDense Captioning: We arrange objects, attributes and rela-\ntionships hierarchically to construct a visual scene graph,\nthat is used to query Vicuna-v1.5-13B [60] model along\nwith in-context examples to generate dense captions. The\ndesigned prompt is shown in Fig. 6 (a).\nExtra Context: We query Vicuna-v1.5-13B model to gen-\nerate additional context about the visual scene. The prompt\ndesigned for this purpose is shown in Fig. 6 (b).\n2\nMain category\nFine Category\nIndoor scene\nLiving space, Work space, Public space, In-\ndustrial space\nOutdoor scene\nUrban landscape, Rural landscape, Natural\nlandscape\nTransportation\nscene\nRoad, Airport, Train station, Port and harbor\nSports and recre-\nation scene\nSporting venue, Recreational area, Gym and\nfitness center\nTable 7. Summary of landmark categories and their corresponding\nfine-grained categories. We use LLaVA-v1.5 [28] for assigning\nlandmark categories to images.\nB. Additional Downstream Tasks\nB.1. Phrase Grounding\nIn order to adapt the GLaMM model for phrase ground-\ning, we repurpose the GCG dataset to suit this particular\ntask. Specifically, the answers in the GCG dataset are now\nused as questions, and the parts of the captions containing\ngroundings are regarded as phrases. The model is subse-\nquently trained to locate pixel-level groundings for these\nphrases, which are enclosed within <p> and </p> tokens.\nThe results of this adaptation are shown in the following\nfigure.\nB.2. Conversational Style Question Answering\nWe evaluate our model on the LLaVA-Bench [28, 29] that\nuses GPT-4 for evaluation of models.\nThis benchmark\ntests the model on three different types of tasks: conver-\nsation question-answering, detailed descriptions, and com-\nplex reasoning tasks.\nThe evaluation provides insights\ninto the model\u2019s conversational and reasoning capabilities.\nThe results in Tab. 8 present a comparison of GLaMM\nwith previous open-source models. We note that GLaMM\nperformance is on par with the recently released LLaVA-\n1.5 which leverages additional data for vision-to-language\nalignment.\nQualitative results are shown in Fig. 11 and\nFig. 13.\nMethod\nLLM\nLLaVAW\nBLIP-2 [24]\nVicuna-13B\n38.1\nInstructBLIP [6]\nVicuna-7B\n60.9\nQwen-VL [3]\nQwen-7B\n63.4\nQwen-VL-Chat [3]\nQwen-7B\n58.6\nLLaVA-1.5 [27]\nVicuna-7B\n63.4\nGLaMM\nVicuna-7B\n63.3\nTable 8. Evaluation of GLaMM on conversational style QA us-\ning LLaVA-Bench. The table compares GLaMM\u2019s performance\nwith previous open-source models in conversation question-\nanswering, detailed descriptions, and complex reasoning tasks.\nC. Additional Qualitative Results\nIn this section, we provide more qualitative examples to bet-\nter understand the capacity of GLaMM.\nC.1. Grounded Conversation Generation (GCG)\nFig. 7 shows qualitative results of GLaMM finetuned on\nGranDf dataset. The model could produce dense captions\nand provide dense pixel-level groundings of the caption.\nC.2. Referring Segmentation\nFig. 8 shows the effectiveness of GLaMM in understanding\nthe natural language query and segmenting the correspond-\ning objects. Note that GLaMM can also segment multiple\nobjects via multi-round conversations.\nC.3. Region-level Captioning\nFig. 9 shows the qualitative results of GLaMM for region-\nlevel understanding. Our model can generate detailed de-\nscriptions about the user-specified regions in an image.\nC.4. Image-level Captioning\nFig. 10 shows GLaMM\u2019s qualitative results on captioning\ntasks. Our model can generate dense captions for images.\nC.5. Conditional Image Generation\nFig. 12 shows GLaMM\u2019s seamless integration for genera-\ntive tasks. We use the Stable Diffusion inpainting model\nstable-diffusion-xl-1.0-inpainting [41] for this task. We first\ngenerate a segmentation mask using our GlaMM model\nbased on the user query. This segmentation mask along with\nthe user prompt is given as the input to the Stable Diffusion\ninpainting model, which generates the final output.\n3\nThe\nimage\nfeatures\na\nlarge,\nold\nbuilding\nwith a roof, situated on a grassy field. A\ntree is also present on the grass. The sky\nis seen overarching the building.\nA\nperson\nin\na\nblack\njacket\nand\na\nstraw hat is cooking some food.\nThe image showcases a boat\nsailing\non a\nriver.\nThe\nsky\nis\noverarching\nthe\nboat,\nbridge, and the buildings.\nFigure 7. Qualitative results of GLaMM\u2019s performance in grounded conversation generation. The figure shows how GLaMM\nseamlessly generates detailed responses, grounding phrases using pixel-level masks showing its detailed understanding.\nCan you segment the woman\u2019s hair \nin this image\nNow please segment the hat.\nCan you segment Ramos in this \nimage ?\nNow can you segment the football ?\nCan you segment what the person is \nusing to ride ?\nNow can you please segment the sea.\nFigure 8. Qualitative results of GLaMM\u2019s capability in referring expression segmentation. The figure illustrates how GLaMM\neffectively translates text-based referring expressions into corresponding segmentation masks. Leveraging its training on the GranD dataset,\nthe model can provide pixel-grounded reasoning and operate across various levels of granularity.\nThey are wrestling in the sand.\nWhat are they doing ?\nIt could\nbe potentially dangerous for the\nwoman\nto\nbe\nusing\nher\ncellphone\nwhile\ncrossing\nthe\nstreet.\nShe\nmight\nnot\nbe\npaying\nfull\nattention\nto\nthe\ntraffic\nand\nmight be more susceptible to accidents.\nCan you please describe the region ?\nWhy does he need this ?\nWhat is she doing in this image ?\nDo you think this is dangerous ?\nA man sitting on a motorcycle\nIt\nis\na\nhelmet\nand\nhe\nneeds\nit\nfor\nprotection while riding his motorcycle.\nShe is using her cellphone while crossing\nthe street.\nThis is an image from India.\nCan you tell me which country this \nimage is from, given the context ?\nFigure 9. Qualitative illustration of GLaMM\u2019s performance in region-level captioning. The figure demonstrates GLaMM\u2019s ability\nto generate region-specific captions adeptly, translating the intricate details from designated regions into coherent textual descriptions,\nenriched by its training on the comprehensive GranD dataset. This capability, combined with the inherent reasoning abilities of LLMs,\nenables it to tackle reasoning-based visual questions about these regions.\n4\nThe image features a large building with a sign that\nreads \u201cTESCO\u201d. A light is also visible, hanging from the\nbuilding. The sky is seen over the building.\nThe image shows a large elephant statue with tusks,\nstanding on a wooden floor. There is also a skull of an\nelephant in front of the elephant statue.\nThe image showcases a large, ornate ceiling with various\nreligious paintings. The ceiling is part of a church\u2019s\ninterior, featuring a vibrant artwork.\nFigure 10. Qualitative results of GLaMM on image-level captioning tasks. The figure shows the capabilities of GLaMM in generating\ndetailed and context-aware captions for a diverse range of images. On the left, GLaMM demonstrates its proficiency in text recognition\nwithin images; it accurately identifies and incorporates specific textual information, such as the brand name \"TESCO,\" into its caption.\nIn the middle image, GLaMM\u2019s capability to discern subtleties in visual content is showcased. It can effectively distinguish between live\nentities and inanimate objects, such as differentiating a living creature from a statue. On the right, the figure demonstrates GLaMM\u2019s\ncompetence in reasoning about complex visual scenes. It can analyze and describe intricate details and interactions within an image,\nreflecting a deep understanding of both the individual elements and the overall context of the scene.\nCould you please give me a detailed description of the image? Please respond \nwith interleaved segmentation masks for the corresponding parts of the answer\nA man in a fishing boat uses an oar\nto navigate the water.\nWhat can you tell me about the \nregion indicated in the image?\n\u201cA straw Hat\u201d\nWhy does he need the hat ?\nThe man is wearing the hat to protect\nhis head from the sun.\nCan you segment the water body ?\nSure, here\u2019s the segmentation.\nCould you provide me with a detailed analysis of this photo? Please output with \ninterleaved segmentation masks for the corresponding parts of the answer.\nA\nwoman\nin\na\nwhite\nhat\nand\napron is presenting a bowl to a\nman in a red shirt.\nCan you please describe this region ?\nA\nTable\nwith\na\nlarge\nnumber of bowls on it\nWhy do we need so many bowls ?\nThis is a restaurant setting where the bowls are used\nto serve different dishes. It is a common practice in\nsome\ncultures\nto\nserve\na\nvariety\nof\nsmall\ndishes\nor\nappetizers in bowls to customers.\nCan you segment what he is using \nfor cooking ?\nSure, the segmentation result is.\nFigure 11. Multimodal conversational interactions facilitated by GLaMM. The figure showcases GLaMM engaging in multi-turn\ndialogues, providing detailed descriptions, addressing region-specific inquiries, and presenting grounded conversations. This effectively\nhighlights its adaptability in intricate visual-language interactions and robustly retaining reasoning capabilities inherent to LLMs.\n5\nGenerate \u201cAn airplane lands in the \nwater surrounded by green hills.\u201d\nCan you please segment the yacht ?   \nSure, Here's the new image\nSure, It is.\nGenerate \u201cSpider man riding a bike \non the street.\u201d\nSure, Here's the new image\nSure, It is.\nCan you please segment the person wearing the orange jacket ?\nFigure 12. Qualitative results of GLaMM on conditional image generation. The figure shows the integration of GLaMM with an image\ngeneration model (stable diffusion). GlaMM first generates the segmentation mask (e.g. \"yacht\" in the left image and \"person wearing\norange jacket\" in the right image) which is used along with a text prompt as input to the diffusion model to generate the desired images.\nThe image suggests that the two individuals are dressed up in animal\nmasks and playing accordions as a form of entertainment or performance.\nThis\ncould\nbe\npart\nof\na\nstreet\nperformance,\na\nparade,\nor\na\nthemed\nevent. The use of animal masks adds an element of humor and novelty to\ntheir appearance, drawing attention and curiosity from passersby. The\naccordions\nserve\nas\na\nmusical\naccompaniment,\nadding\nto\nthe\noverall\nspectacle and providing a lively, upbeat atmosphere.\nWhy do you think they wear masks \nand play the accordion ?\nA white horse mask.\nCan you please segment the snow \naround him?\nWhy does he need the jacket and \ncap ?\nA blue knit cap\nCould you provide me with a detailed analysis of this photo? Please output with \ninterleaved segmentation masks for the corresponding parts of the answer\nA child in a blue jacket is playing\nwith a fox in the snow.\nWhat can you tell me about the \nregion indicated in the image?\nIt is cold outside the jacket and the\ncap provides warmth.\nSure, here\u2019s the segmentation.\nCould you provide me with a detailed analysis of this photo? Please output with \ninterleaved segmentation masks for the corresponding parts of the answer.\nTwo people wearing animal masks\nplaying accordions.\nCan you please describe this region ?\nCan you segment the man wearing \nthe red shoes ?\nSure, the segmentation result is.\nFigure 13. Multimodal conversational with GLaMM. The figure shows multimodal conversations generated through GLaMM. The\nmodel is flexible enough to process multimodal inputs and respond with multimodal outputs in a single conversation.\n6\n\u2018person about to hit sports ball\u2019, \u2018person swinging tennis \nracket\u2019, \u2018person running on playingfield\u2019, \u2018banner attached \nto fence-merged\u2019, \u2018playingfield beside fence-merged\u2019\n\u2018person in truck\u2019, \u2018car parked on pavement-merged\u2019, \u2018truck \nparked on dirt-merged\u2019, \u2018house beside tree-merged\u2019, \u2018dirt-\nmerged beside pavement-merged\u2019\n\u2018person attached to dog\u2019, \u2018person on bed\u2019 , \u2018dog on bed\u2019,\n\u2018dog attached to blanket\u2019, \u2018bed in front of wall\u2019, \u2018curtain \nattached to wall\u2019, \u2018cabinet attached to curtain\u2019\nThe image depicts a person on a bed, lovingly hugging\na dog. Both the person and the dog are on the bed, with\nthe dog also attached to a blanket. Further aspects of\nthe room include a curtain attached to a wall, and a\ncabinet attached to this curtain.\nA person is running on a playing field situated beside a\nfence. This individual is about to hit a sports ball,\nswinging a tennis racket in preparation. Attached to the\nfence is a banner.\nA large commercial truck is parked on a dirt area, with its\nlights on. A person is sitting inside this truck. Close by the\ntruck, a car is parked on the pavement. There is a house\nnestled beside a tree that's near the dirt and pavement.\nPrompt: You are given five base captions for an image briefly describing the image from different perspective. You are also given a number of relationships \nbetween objects in the image. Each relationship consists of [subject, relation/verb, object]. Note that each subject and object follows the format <entitiy name>-\n<object number in image>. For example, if there are five persons and two tables, they will be formatted as person-1, person-2,.., person-5 and table-1, table-2. \nProvide a concise image caption that straightforwardly describes objects/things visible in the image, using provided relationships. Use the base captions for \ncontext understanding, but it is not mandatory to include them verbatim in the final description. Break down the description into shorter sentences. Importantly \nkeep the names of the subject and object unchanged. Break down the description into shorter, clear sentences. Do not add any extra information. Keep it brief.\n\u2018a woman with glasses and a purple shirt is having a \nmeal\u2019, \u2018a man with a mustache wearing a blue shirt with \nfood on his plate\u2019, \u2018the man with the glasses on\u2019\n\u2018 a tupperware box filled with fruit\u2019, \u2018a plastic container \nwith sliced radishes, green peppers, cucumbers and a \nsauce\u2019, \u2018slices of pizza are also visibly kept within a \nplastic container\u2019\n\u2018a laptop computer sitting to the left of a red book\u2019, \u2018 laptop on \nthe right side of the table with a scrabble game open\u2019\nA laptop computer sitting to the left of a red book. There is also\na laptop on the right side of the table with a scrabble game \nopen.\nA woman with glasses and a purple shirt is having a\nmeal at a table with a man with a mustache wearing a \nblue shirt with food on his plate and another man, the \nman with the glasses on.\nA\ntupperware \nbox \nfilled \nwith \nfruit\nfeatures\nprominently\non\nthe\ntable,\nalong\nwith\na plastic \ncontainer \nwith \nsliced \nradishes, \ngreen \npeppers, \ncucumbers and a sauce. Several slices of pizza are also \nvisibly kept within a plastic container.\nPrompt: Create a concise image caption that straightforwardly describes things visible in the image, using provided phrases verbatim without altering them in any \nway. Do not infer actions or relationships not explicitly mentioned in the phrases. Use the base captions for contextual understanding, but it is not mandatory to \ninclude them verbatim in the final description. Preserve the exact wording of the provided phrases in the final caption. Break down the description into shorter, \nclear sentences, but without sacrificing the exact wording of the provided phrases. Construct a simple, coherent, and succinct caption that accurately uses the \nexact phrases, organizing the information into shorter sentences without assuming or inferring additional actions or relationships.\nToddler sits on carpet in living room touching guitar .\nA man in a striped shirt poses with a blond girl in a\nblack apron.\nA small group of three enjoy the view of the water as a \nsmall boy wanders off and two companions enjoy a\nwalk along the shoreline.\n(a) Samples from our GranD\ud835\udc53 dataset: Illustrating the repurposing of the OpenPSG dataset for the GCG task.\n(b) Samples from our GranD\ud835\udc53 dataset: Illustrating the repurposing of the RefCOCO-g dataset for the GCG task.\n(c) Samples from our GranD\ud835\udc53 dataset: Illustrating the repurposing of the Flickr-30k dataset for the GCG task.\nFigure 14. Dataset samples from GranDf. The figure shows the GPT4 [34] prompts used and the created dataset samples from Grandf\ndataset. This repurposed human-annotated dataset provides rich semantics to GLaMM for GCG task.\n7\nPerson, Man:\nA\nman\nin\njacket\ncarrying a backpack.\nPerson, Woman: A girl wearing \nholding pink bow and bag.\nCoat, Jacket:\nA woman in gray\ncoat with a backpack.\nJeans: Blue bag on the womans\narm.\nShopping bag, Plastic bag: The\nwoman holding a cell phone.\nJacket: A girl wearing a jacket.\nScarf: A burgundy scarf around\na womans neck.\nBag, backpack: A white and gray\nbackpack.\nDense Caption: Two women with backpacks are taking a selfie with cellphone in front of a flower-covered wall, enjoying their time together in the city. \nOne of them is wearing a gray jacket and a white and gray backpack, while the other is holding a umbrella and a black backpack with a white tag. They \nare surrounded by potted plants and a tall planter of flowers.\nBag, Backpack: A gray and black\nbackpack with a white tag.\nTwo women\nAdditional Context: In the urban landscape, individuals often carry various bags and backpacks to store their belongings, such as handbags, shopping\nbags, and backpacks. These bags are usually made of durable materials like canvas or nylon and come in different colors, sizes, and styles. Some people\nprefer to carry a scarf or a jacket to protect themselves from the elements, while others wear jeans or trousers for comfort and convenience. Outdoor\nspaces in the city may feature potted plants, flower arrangements, and other decorative elements to enhance the aesthetic appeal of the area.. Cell\nphones and other electronic devices have become essential for communication and accessing information on-the-go. In outdoor settings, people often\nuse these devices to capture memories, stay connected with others, and navigate their surroundings.\nCellphone: A cellular phone.\nPerson, Girl: A girl carrying bags \nand a cellphone.\nCellphone: A cellular phone.\nFlower arrangement: Pink \nflowers on the sidewalk.\nWood floor, Classroom: A first aid\ncourse in classroom.\nPerson, Woman, Girl: A girl \nwearing orange t-shirt, the girl is \nbending.\nLife buoy, Life belt, Life jacket: A red and white lifesaver.\nMannequin, Person: A \nmannequin with blue clothes \nlying on the floor.\nBoy, Person: A boy wearing blue\nshirt. The boy is kneeling.\nBoy, Person: A boy in blue shirt.\nThe boy is bending.\nLaptop, Laptop computer: Laptop\non a brown table.\nChair: A black chair with a girl in\nan orange shirt leaning on it.\nDense Caption: A group of children are practicing a first aid course in a classroom. A mannequin wearing a blue shirt, surrounded by young boys,\nincluding two wearing blue shirts. There are various items on the table, such as a laptop, projector, and markers. A chair and a blackboard with a red \nand white lifesaver jacket is also seen in the room. The boys are bending over the mannequin, possibly practicing CPR, while a women watches.\nJersey: A boy wearing blue shirt.\nyoung boys\nAdditional Context: The image depicts a first aid training session, likely aimed at teaching children basic life-saving techniques like CPR. Given the\nclassroom setting and the presence of a projector and laptop, it may be part of a structured educational program. First aid courses like this are crucial\nfor empowering people to handle emergencies effectively, reducing the severity of injuries or even saving lives. However, improper technique can be\npotentially dangerous, emphasizing the importance of qualified supervision, in this case, provided by the watching woman. The lifesaver jacket on the\nblackboard hints at a broader scope of training, possibly including water safety. The brown table with a laptop on it serves as a functional workspace,\nallowing for remote work or study in a cozy environment.\nFigure 15. Dataset samples from GranD. The figure shows a few samples from the GranD dataset, generated using the automated\nannotation pipeline. It provides multiple semantic labels and attributes for detected objects, along with the grounded dense caption and\nadditional context.\nC.6. Conversations\nFig. 13 illustrates the unique functionality of GLaMM to\nengage in multi-purpose task conversations. GLaMM is a\ngeneric conversational model that can accept prompts in the\nform of text and/or region and can answer in the form of\ntext and/or segmentation masks. Note that our model is not\nexplicitly trained to handle such scenarios, and this behavior\n8\nemerges mainly due to our pretraining on GranD dataset,\nwhere an image is presented to LMM in different contexts.\nD. Dataset Visualization\nIn this section, we provide additional dataset samples of our\nGranD and GranDf datasets to better understand the func-\ntionalities they offer. Please see Fig. 15 and Fig. 14.\nE. Limitations and Future Work\nThe large-scale automated pipeline provides dense labelings\nthat are important for our pretraining but still contains some\nnoise. A high-quality, clean dataset could help further im-\nprove the pretrained representations, although this comes at\na significantly higher annotation cost. A potential research\ndirection is to develop a cost-effective annotation pipeline\naimed at reducing noise in dense labeling. Additionally, ex-\npanding the GLaMM framework to include modalities such\nas video and 3D is also a future research direction.\nF. Ethics and Societal Impact\nOur Grounding-anything Dataset (GranD) utilizes SAM im-\nages that have de-identified personal information, with all\nfaces and license plates obscured. To the best of our knowl-\nedge, the dataset does not portray any strong biases or dis-\ncrimination. We urge for the responsible use of GranD and\nGLaMM, promoting research progress while safeguarding\nprivacy.\n9\n"
  },
  {
    "title": "Levels of AGI: Operationalizing Progress on the Path to AGI",
    "link": "https://arxiv.org/pdf/2311.02462.pdf",
    "upvote": "29",
    "text": "originally published Nov. 2023; updated Jan. 2024\nLevels of AGI: Operationalizing Progress on\nthe Path to AGI\nMeredith Ringel Morris1, Jascha Sohl-dickstein1, Noah Fiedel1, Tris Warkentin1, Allan Dafoe1,\nAleksandra Faust1, Clement Farabet1 and Shane Legg1\n1Google DeepMind\nWe propose a framework for classifying the capabilities and behavior of Artificial General Intelligence\n(AGI) models and their precursors. This framework introduces levels of AGI performance, generality,\nand autonomy. It is our hope that this framework will be useful in an analogous way to the levels of\nautonomous driving, by providing a common language to compare models, assess risks, and measure\nprogress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and\ndistill six principles that a useful ontology for AGI should satisfy. These principles include focusing on\ncapabilities rather than mechanisms; separately evaluating generality and performance; and defining\nstages along the path toward AGI, rather than focusing on the endpoint. With these principles in mind,\nwe propose \u201cLevels of AGI\u201d based on depth (performance) and breadth (generality) of capabilities, and\nreflect on how current systems fit into this ontology. We discuss the challenging requirements for future\nbenchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, we\ndiscuss how these levels of AGI interact with deployment considerations such as autonomy and risk, and\nemphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and\nsafe deployment of highly capable AI systems.\nKeywords: AI, AGI, Artificial General Intelligence, General AI, Human-Level AI, HLAI, ASI, frontier models,\nbenchmarking, metrics, AI safety, AI risk, autonomous systems, Human-AI Interaction\nIntroduction\nArtificial General Intelligence (AGI)1 is an important and sometimes controversial concept in computing\nresearch, used to describe an AI system that is at least as capable as a human at most tasks. Given the\nrapid advancement of Machine Learning (ML) models, the concept of AGI has passed from being the\nsubject of philosophical debate to one with near-term practical relevance. Some experts believe that\n\u201csparks\u201d of AGI (Bubeck et al., 2023) are already present in the latest generation of large language\nmodels (LLMs); some predict AI will broadly outperform humans within about a decade (Bengio et al.,\n2023); some even assert that current LLMs are AGIs (Ag\u00fcera y Arcas and Norvig, 2023). However, if\nyou were to ask 100 AI experts to define what they mean by \u201cAGI,\u201d you would likely get 100 related\nbut different definitions.\nThe concept of AGI is important as it maps onto goals for, predictions about, and risks of AI:\nGoals: Achieving human-level \u201cintelligence\u201d is an implicit or explicit north-star goal for many\nin our field, from the 1956 Dartmouth AI Conference (McCarthy et al., 1955) that kick-started the\n1 There is controversy over use of the term \u201cAGI.\" Some communities favor \u201cGeneral AI\u201d or \u201cHuman-Level AI\u201d (Gruet-\nzemacher and Paradice, 2019) as alternatives, or even simply \u201cAI\u201d as a term that now effectively encompasses AGI (or soon\nwill, under optimistic predictions). However, AGI is a term of art used by both technologists and the general public, and is\nthus useful for clear communication. Similarly, for clarity we use commonly understood terms such as \u201cArtificial Intelligence\u201d\nand \u201cMachine Learning,\u201d although we are sympathetic to critiques (Bigham, 2019) that these terms anthropomorphize\ncomputing systems.\nCorresponding author(s): merrie@google.com\n\u00a9 2024 Google DeepMind. All rights reserved\narXiv:2311.02462v2  [cs.AI]  5 Jan 2024\nLevels of AGI: Operationalizing Progress on the Path to AGI\nmodern field of AI to some of today\u2019s leading AI research firms whose mission statements allude to\nconcepts such as \u201censure transformative AI helps people and society\u201d (Anthropic, 2023a) or \u201censure\nthat artificial general intelligence benefits all of humanity\u201d (OpenAI, 2023).\nPredictions: The concept of AGI is related to a prediction about progress in AI, namely that\nit is toward greater generality, approaching and exceeding human generality. Additionally, AGI is\ntypically intertwined with a notion of \u201cemergent\u201d properties (Wei et al., 2022), i.e. capabilities not\nexplicitly anticipated by the developer. Such capabilities offer promise, perhaps including abilities\nthat are complementary to typical human skills, enabling new types of interaction or novel industries.\nSuch predictions about AGI\u2019s capabilities in turn predict likely societal impacts; AGI may have\nsignificant economic implications, i.e., reaching the necessary criteria for widespread labor substitution\n(Dell\u2019Acqua et al., 2023; Ellingrud et al., 2023; Eloundou et al., 2023), as well as geo-political\nimplications relating not only to the economic advantages AGI may confer, but also to military\nconsiderations (Kissinger et al., 2022).\nRisks: Lastly, AGI is viewed by some as a concept for identifying the point when there are extreme\nrisks (Bengio et al., 2023; Shevlane et al., 2023), as some speculate that AGI systems might be able\nto deceive and manipulate, accumulate resources, advance goals, behave agentically, outwit humans\nin broad domains, displace humans from key roles, and/or recursively self-improve.\nIn this paper, we argue that it is critical for the AI research community to explicitly reflect on what\nwe mean by \"AGI,\" and aspire to quantify attributes like the performance, generality, and autonomy\nof AI systems. Shared operationalizable definitions for these concepts will support: comparisons\nbetween models; risk assessments and mitigation strategies; clear criteria from policymakers and\nregulators; identifying goals, predictions, and risks for research and development; and the ability to\nunderstand and communicate where we are along the path to AGI.\nDefining AGI: Case Studies\nMany AI researchers and organizations have proposed definitions of AGI. In this section, we consider\nnine prominent examples, and reflect on their strengths and limitations. This analysis informs our\nsubsequent introduction of a two-dimensional, leveled ontology of AGI.\nCase Study 1: The Turing Test. The Turing Test (Turing, 1950) is perhaps the most well-known\nattempt to operationalize an AGI-like concept. Turing\u2019s \u201cimitation game\u201d was posited as a way to\noperationalize the question of whether machines could think, and asks a human to interactively\ndistinguish whether text is produced by another human or by a machine. The test as originally framed\nis a thought experiment, and is the subject of many critiques (Wikipedia, 2023b); in practice, the\ntest often highlights the ease of fooling people (Weizenbaum, 1966; Wikipedia, 2023a) rather than\nthe \u201cintelligence\u201d of the machine. Given that modern LLMs pass some framings of the Turing Test,\nit seems clear that this criteria is insufficient for operationalizing or benchmarking AGI. We agree\nwith Turing that whether a machine can \u201cthink,\u201d while an interesting philosophical and scientific\nquestion, seems orthogonal to the question of what the machine can do; the latter is much more\nstraightforward to measure and more important for evaluating impacts. Therefore we propose that\nAGI should be defined in terms of capabilities rather than processes2.\nCase Study 2: Strong AI \u2013 Systems Possessing Consciousness. Philosopher John Searle\nmused, \"according to strong AI, the computer is not merely a tool in the study of the mind; rather,\nthe appropriately programmed computer really is a mind, in the sense that computers given the\n2 As research into mechanistic interpretability (R\u00e4uker et al., 2023) advances, it may enable process-oriented metrics.\nThese may be relevant to future definitions of AGI.\n2\nLevels of AGI: Operationalizing Progress on the Path to AGI\nright programs can be literally said to understand and have other cognitive states\" (Searle, 1980).\nWhile strong AI might be one path to achieving AGI, there is no scientific consensus on methods\nfor determining whether machines possess strong AI attributes such as consciousness (Butlin et al.,\n2023), making the process-oriented focus of this framing impractical.\nCase Study 3: Analogies to the Human Brain. The original use of the term \"artificial general\nintelligence\" was in a 1997 article about military technologies by Mark Gubrud (Gubrud, 1997),\nwhich defined AGI as \u201cAI systems that rival or surpass the human brain in complexity and speed, that\ncan acquire, manipulate and reason with general knowledge, and that are usable in essentially any\nphase of industrial or military operations where a human intelligence would otherwise be needed.\u201d\nThis early definition emphasizes processes (rivaling the human brain in complexity) in addition to\ncapabilities; while neural network architectures underlying modern ML systems are loosely inspired\nby the human brain, the success of transformer-based architectures (Vaswani et al., 2023) whose\nperformance is not reliant on human-like learning suggests that strict brain-based processes and\nbenchmarks are not inherently necessary for AGI.\nCase Study 4: Human-Level Performance on Cognitive Tasks. Legg (Legg, 2008) and Goertzel\n(Goertzel, 2014) popularized the term AGI among computer scientists in 2001 (Legg, 2022), describing\nAGI as a machine that is able to do the cognitive tasks that people can typically do. This definition\nnotably focuses on non-physical tasks (i.e., not requiring robotic embodiment as a precursor to AGI).\nLike many other definitions of AGI, this framing presents ambiguity around choices such as \u201cwhat\ntasks?\u201d and \u201cwhich people?\u201d.\nCase Study 5: Ability to Learn Tasks. In The Technological Singularity (Shanahan, 2015),\nShanahan suggests that AGI is \u201cArtificial intelligence that is not specialized to carry out specific tasks,\nbut can learn to perform as broad a range of tasks as a human.\u201d An important property of this framing\nis its emphasis on the value of including metacognitive tasks (learning) among the requirements for\nachieving AGI.\nCase Study 6: Economically Valuable Work. OpenAI\u2019s charter defines AGI as \u201chighly autonomous\nsystems that outperform humans at most economically valuable work\u201d (OpenAI, 2018). This definition\nhas strengths per the \u201ccapabilities, not processes\u201d criteria, as it focuses on performance agnostic to\nunderlying mechanisms; further, this definition offers a potential yardstick for measurement, i.e.,\neconomic value. A shortcoming of this definition is that it does not capture all of the criteria that\nmay be part of \u201cgeneral intelligence.\u201d There are many tasks that are associated with intelligence\nthat may not have a well-defined economic value (e.g., artistic creativity or emotional intelligence).\nSuch properties may be indirectly accounted for in economic measures (e.g., artistic creativity might\nproduce books or movies, emotional intelligence might relate to the ability to be a successful CEO),\nthough whether economic value captures the full spectrum of \u201cintelligence\u201d remains unclear. Another\nchallenge with a framing of AGI in terms of economic value is that this implies a need for deployment\nof AGI in order to realize that value, whereas a focus on capabilities might only require the potential\nfor an AGI to execute a task. We may well have systems that are technically capable of performing\neconomically important tasks but don\u2019t realize that economic value for varied reasons (legal, ethical,\nsocial, etc.).\nCase Study 7: Flexible and General \u2013 The \"Coffee Test\" and Related Challenges. Marcus\nsuggests that AGI is \u201cshorthand for any intelligence (there might be many) that is flexible and general,\nwith resourcefulness and reliability comparable to (or beyond) human intelligence\u201d (Marcus, 2022b).\nThis definition captures both generality and performance (via the inclusion of reliability); the mention\nof \u201cflexibility\u201d is noteworthy, since, like the Shanahan formulation, this suggests that metacognitive\ntasks such as the ability to learn new skills must be included in an AGI\u2019s set of capabilities in order to\nachieve sufficient generality. Further, Marcus operationalizes his definition by proposing five concrete\n3\nLevels of AGI: Operationalizing Progress on the Path to AGI\ntasks (understanding a movie, understanding a novel, cooking in an arbitrary kitchen, writing a\nbug-free 10,000 line program, and converting natural language mathematical proofs into symbolic\nform) (Marcus, 2022a). Accompanying a definition with a benchmark is valuable; however, more\nwork would be required to construct a sufficiently comprehensive benchmark. While failing some of\nthese tasks may indicate a system is not an AGI, it is unclear that passing them is sufficient for AGI\nstatus. In the Testing for AGI section, we further discuss the challenge in developing a set of tasks that\nis both necessary and sufficient for capturing the generality of AGI. We also note that one of Marcus\u2019\nproposed tasks, \u201cwork as a competent cook in an arbitrary kitchen\u201d (a variant of Steve Wozniak\u2019s\n\u201cCoffee Test\u201d (Wozniak, 2010)), requires robotic embodiment; this differs from other definitions that\nfocus on non-physical tasks3.\nCase Study 8: Artificial Capable Intelligence. In The Coming Wave, Suleyman proposed the\nconcept of \"Artificial Capable Intelligence (ACI)\" (Mustafa Suleyman and Michael Bhaskar, 2023) to\nrefer to AI systems with sufficient performance and generality to accomplish complex, multi-step tasks\nin the open world. More specifically, Suleyman proposed an economically-based definition of ACI skill\nthat he dubbed the \u201cModern Turing Test,\u201d in which an AI would be given $100,000 of capital and\ntasked with turning that into $1,000,000 over a period of several months. This framing is more narrow\nthan OpenAI\u2019s definition of economically valuable work and has the additional downside of potentially\nintroducing alignment risks (Kenton et al., 2021) by only targeting fiscal profit. However, a strength\nof Suleyman\u2019s concept is the focus on performing a complex, multi-step task that humans value.\nConstrued more broadly than making a million dollars, ACI\u2019s emphasis on complex, real-world tasks\nis noteworthy, since such tasks may have more ecological validity than many current AI benchmarks;\nMarcus\u2019 aforementioned five tests of flexibility and generality (Marcus, 2022a) seem within the spirit\nof ACI, as well.\nCase Study 9: SOTA LLMs as Generalists. Ag\u00fcera y Arcas and Norvig (Ag\u00fcera y Arcas and\nNorvig, 2023) suggested that state-of-the-art LLMs (e.g. mid-2023 deployments of GPT-4, Bard, Llama\n2, and Claude) already are AGIs, arguing that generality is the key property of AGI, and that because\nlanguage models can discuss a wide range of topics, execute a wide range of tasks, handle multimodal\ninputs and outputs, operate in multiple languages, and \u201clearn\u201d from zero-shot or few-shot examples,\nthey have achieved sufficient generality. While we agree that generality is a crucial characteristic of\nAGI, we posit that it must also be paired with a measure of performance (i.e., if an LLM can write code\nor perform math, but is not reliably correct, then its generality is not yet sufficiently performant).\nDefining AGI: Six Principles\nReflecting on these nine example formulations of AGI (or AGI-adjacent concepts), we identify properties\nand commonalities that we feel contribute to a clear, operationalizable definition of AGI. We argue\nthat any definition of AGI should meet the following six criteria:\n1. Focus on Capabilities, not Processes. The majority of definitions focus on what an AGI can\naccomplish, not on the mechanism by which it accomplishes tasks. This is important for identifying\ncharacteristics that are not necessarily a prerequisite for achieving AGI (but may nonetheless be\ninteresting research topics). This focus on capabilities allows us to exclude the following from our\nrequirements for AGI:\n\u2022 Achieving AGI does not imply that systems think or understand in a human-like way (since this\nfocuses on processes, not capabilities)\n3 Though robotics might also be implied by the OpenAI charter\u2019s focus on \u201ceconomically valuable work,\u201d the fact that\nOpenAI shut down its robotics research division in 2021 (Wiggers, 2021) suggests this is not their intended interpretation.\n4\nLevels of AGI: Operationalizing Progress on the Path to AGI\n\u2022 Achieving AGI does not imply that systems possess qualities such as consciousness (subjective\nawareness) (Butlin et al., 2023) or sentience (the ability to have feelings) (since these qualities\nnot only have a process focus, but are not currently measurable by agreed-upon scientific\nmethods)\n2. Focus on Generality and Performance. All of the above definitions emphasize generality\nto varying degrees, but some exclude performance criteria. We argue that both generality and\nperformance are key components of AGI. In the next section we introduce a leveled taxonomy that\nconsiders the interplay between these dimensions.\n3. Focus on Cognitive and Metacognitive Tasks. Whether to require robotic embodiment (Roy\net al., 2021) as a criterion for AGI is a matter of some debate. Most definitions focus on cognitive\ntasks, by which we mean non-physical tasks. Despite recent advances in robotics (Brohan et al.,\n2023), physical capabilities for AI systems seem to be lagging behind non-physical capabilities. It is\npossible that embodiment in the physical world is necessary for building the world knowledge to be\nsuccessful on some cognitive tasks (Shanahan, 2010), or at least may be one path to success on some\nclasses of cognitive tasks; if that turns out to be true then embodiment may be critical to some paths\ntoward AGI. We suggest that the ability to perform physical tasks increases a system\u2019s generality, but\nshould not be considered a necessary prerequisite to achieving AGI. On the other hand, metacognitive\ncapabilities (such as the ability to learn new tasks or the ability to know when to ask for clarification\nor assistance from a human) are key prerequisites for systems to achieve generality.\n4. Focus on Potential, not Deployment. Demonstrating that a system can perform a requisite\nset of tasks at a given level of performance should be sufficient for declaring the system to be an\nAGI; deployment of such a system in the open world should not be inherent in the definition of AGI.\nFor instance, defining AGI in terms of reaching a certain level of labor substitution would require\nreal-world deployment, whereas defining AGI in terms of being capable of substituting for labor would\nfocus on potential. Requiring deployment as a condition of measuring AGI introduces non-technical\nhurdles such as legal and social considerations, as well as potential ethical and safety concerns.\n5. Focus on Ecological Validity. Tasks that can be used to benchmark progress toward AGI are\ncritical to operationalizing any proposed definition. While we discuss this further in the \u201cTesting for\nAGI\u201d section, we emphasize here the importance of choosing tasks that align with real-world (i.e.,\necologically valid) tasks that people value (construing \u201cvalue\u201d broadly, not only as economic value but\nalso social value, artistic value, etc.). This may mean eschewing traditional AI metrics that are easy to\nautomate or quantify (Raji et al., 2021) but may not capture the skills that people would value in an\nAGI.\n6. Focus on the Path to AGI, not a Single Endpoint. Much as the adoption of a standard set of\nLevels of Driving Automation (SAE International, 2021) allowed for clear discussions of policy and\nprogress relating to autonomous vehicles, we posit there is value in defining \u201cLevels of AGI.\u201d As we\ndiscuss in subsequent sections, we intend for each level of AGI to be associated with a clear set of\nmetrics/benchmarks, as well as identified risks introduced at each level, and resultant changes to\nthe Human-AI Interaction paradigm (Morris et al., 2023). This level-based approach to defining AGI\nsupports the coexistence of many prominent formulations \u2013 for example, Aguera y Arcas & Norvig\u2019s\ndefinition (Ag\u00fcera y Arcas and Norvig, 2023) would fall into the \u201cEmerging AGI\u201d category of our\nontology, while OpenAI\u2019s threshold of labor replacement (OpenAI, 2018) better matches \u201cVirtuoso\nAGI.\u201d Our \u201cCompetent AGI\u201d level is probably the best catch-all for many existing definitions of AGI\n(e.g., the Legg (Legg, 2008), Shanahan (Shanahan, 2015), and Suleyman (Mustafa Suleyman and\nMichael Bhaskar, 2023) formulations). In the next section, we introduce a level-based ontology of\nAGI.\n5\nLevels of AGI: Operationalizing Progress on the Path to AGI\nLevels of AGI\nPerformance (rows) x\nGenerality (columns)\nNarrow\nclearly scoped task or set of tasks\nGeneral\nwide range of non-physical tasks,\nincluding metacognitive abilities\nlike learning new skills\nLevel 0: No AI\nNarrow Non-AI\ncalculator software; compiler\nGeneral Non-AI\nhuman-in-the-loop computing,\ne.g., Amazon Mechanical Turk\nLevel 1: Emerging\nequal to or somewhat better than\nan unskilled human\nEmerging Narrow AI\nGOFAI (Boden, 2014);\nsim-\nple rule-based systems, e.g.,\nSHRDLU (Winograd, 1971)\nEmerging AGI\nChatGPT (OpenAI, 2023), Bard\n(Anil et al., 2023), Llama 2\n(Touvron et al., 2023), Gemini\n(Pichai and Hassabis, 2023)\nLevel 2: Competent\nat least 50th percentile of skilled\nadults\nCompetent Narrow AI\ntoxicity detectors such as Jig-\nsaw (Das et al., 2022); Smart\nSpeakers such as Siri (Apple),\nAlexa (Amazon), or Google As-\nsistant (Google); VQA systems\nsuch as PaLI (Chen et al., 2023);\nWatson (IBM); SOTA LLMs for a\nsubset of tasks (e.g., short essay\nwriting, simple coding)\nCompetent AGI\nnot yet achieved\nLevel 3: Expert\nat least 90th percentile of skilled\nadults\nExpert Narrow AI\nspelling & grammar checkers\nsuch as Grammarly (Gram-\nmarly, 2023); generative im-\nage models such as Imagen (Sa-\nharia et al., 2022) or Dall-E 2\n(Ramesh et al., 2022)\nExpert AGI\nnot yet achieved\nLevel 4: Virtuoso\nat least 99th percentile of skilled\nadults\nVirtuoso Narrow AI\nDeep Blue (Campbell et al.,\n2002), AlphaGo (Silver et al.,\n2016, 2017)\nVirtuoso AGI\nnot yet achieved\nLevel 5: Superhuman\noutperforms 100% of humans\nSuperhuman Narrow AI\nAlphaFold (Jumper et al., 2021;\nVaradi et al., 2021), AlphaZero\n(Silver et al., 2018), StockFish\n(Stockfish, 2023)\nArtificial\nSuperintelligence\n(ASI)\nnot yet achieved\nTable 1 | A leveled, matrixed approach toward classifying systems on the path to AGI based on\ndepth (performance) and breadth (generality) of capabilities. Example systems in each cell are\napproximations based on current descriptions in the literature or experiences interacting with deployed\nsystems. Unambiguous classification of AI systems will require a standardized benchmark of tasks, as\nwe discuss in the Testing for AGI section. Note that general systems that broadly perform at a level N\nmay be able to perform a narrow subset of tasks at higher levels. The \"Competent AGI\" level, which\nhas not been achieved by any public systems at the time of writing, best corresponds to many prior\nconceptions of AGI, and may precipitate rapid social change once achieved.\n6\nLevels of AGI: Operationalizing Progress on the Path to AGI\nIn accordance with Principle 2 (\"Focus on Generality and Performance\") and Principle 6 (\"Focus\non the Path to AGI, not a Single Endpoint\"), in Table 1 we introduce a matrixed leveling system that\nfocuses on performance and generality as the two dimensions that are core to AGI:\n\u2022 Performance refers to the depth of an AI system\u2019s capabilities, i.e., how it compares to human-\nlevel performance for a given task. Note that for all performance levels above \u201cEmerging,\u201d\npercentiles are in reference to a sample of adults who possess the relevant skill (e.g., \u201cCompetent\u201d\nor higher performance on a task such as English writing ability would only be measured against\nthe set of adults who are literate and fluent in English).\n\u2022 Generality refers to the breadth of an AI system\u2019s capabilities, i.e., the range of tasks for which\nan AI system reaches a target performance threshold.\nThis taxonomy specifies the minimum performance over most tasks needed to achieve a given\nrating \u2013 e.g., a Competent AGI must have performance at least at the 50th percentile for skilled adult\nhumans on most cognitive tasks, but may have Expert, Virtuoso, or even Superhuman performance\non a subset of tasks. As an example of how individual systems may straddle different points in our\ntaxonomy, we posit that as of this writing in September 2023, frontier language models (e.g., ChatGPT\n(OpenAI, 2023), Bard (Anil et al., 2023), Llama2 (Touvron et al., 2023), etc.) exhibit \u201cCompetent\u201d\nperformance levels for some tasks (e.g., short essay writing, simple coding), but are still at \u201cEmerging\u201d\nperformance levels for most tasks (e.g., mathematical abilities, tasks involving factuality). Overall,\ncurrent frontier language models would therefore be considered a Level 1 General AI (\u201cEmerging\nAGI\u201d) until the performance level increases for a broader set of tasks (at which point the Level 2\nGeneral AI, \u201cCompetent AGI,\u201d criteria would be met). We suggest that documentation for frontier\nAI models, such as model cards (Mitchell et al., 2019), should detail this mixture of performance\nlevels. This will help end-users, policymakers, and other stakeholders come to a shared, nuanced\nunderstanding of the likely uneven performance of systems progressing along the path to AGI.\nThe order in which stronger skills in specific cognitive areas are acquired may have serious\nimplications for AI safety (e.g., acquiring strong knowledge of chemical engineering before acquiring\nstrong ethical reasoning skills may be a dangerous combination). Note also that the rate of progression\nbetween levels of performance and/or generality may be nonlinear. Acquiring the capability to learn\nnew skills may particularly accelerate progress toward the next level.\nWhile this taxonomy rates systems according to their performance, systems that are capable of\nachieving a certain level of performance (e.g., against a given benchmark) may not match this level\nin practice when deployed. For instance, user interface limitations may reduce deployed performance.\nConsider the example of DALLE-2 (Ramesh et al., 2022), which we estimate as a Level 3 Narrow AI\n(\u201cExpert Narrow AI\u201d) in our taxonomy. We estimate the \u201cExpert\u201d level of performance since DALLE-2\nproduces images of higher quality than most people are able to draw; however, the system has\nfailure modes (e.g., drawing hands with incorrect numbers of digits, rendering nonsensical or illegible\ntext) that prevent it from achieving a \u201cVirtuoso\u201d performance designation. While theoretically an\n\u201cExpert\u201d level system, in practice the system may only be \u201cCompetent,\u201d because prompting interfaces\nare too complex for most end-users to elicit optimal performance (as evidenced by user studies\n(Zamfirescu-Pereira et al., 2023) and by the existence of marketplaces (e.g., PromptBase) in which\nskilled prompt engineers sell prompts). This observation emphasizes the importance of designing\necologically valid benchmarks (that would measure deployed rather than idealized performance) as\nwell as the importance of considering how human-AI interaction paradigms interact with the notion\nof AGI (a topic we return to in the \u201cCapabilities vs. Autonomy\u201d Section).\nThe highest level in our matrix in terms of combined performance and generality is ASI (Artificial\nSuperintelligence). We define \"Superhuman\" performance as outperforming 100% of humans. For\n7\nLevels of AGI: Operationalizing Progress on the Path to AGI\ninstance, we posit that AlphaFold (Jumper et al., 2021; Varadi et al., 2021) is a Level 5 Narrow AI\n(\"Superhuman Narrow AI\") since it performs a single task (predicting a protein\u2019s 3D structure from an\namino acid sequence) above the level of the world\u2019s top scientists. This definition means that Level 5\nGeneral AI (\"ASI\") systems will be able to do a wide range of tasks at a level that no human can match.\nAdditionally, this framing also implies that Superhuman systems may be able to perform an even\nbroader generality of tasks than lower levels of AGI, since the ability to execute tasks that qualitatively\ndiffer from existing human skills would by definition outperform all humans (who fundamentally\ncannot do such tasks). For example, non-human skills that an ASI might have could include capabilities\nsuch as neural interfaces (perhaps through mechanisms such as analyzing brain signals to decode\nthoughts (Bellier et al., 2023; Tang et al., 2023)), oracular abilities (perhaps through mechanisms\nsuch as analyzing large volumes of data to make high-quality predictions (Schoenegger and Park,\n2023)), or the ability to communicate with animals (perhaps by mechanisms such as analyzing\npatterns in their vocalizations, brain waves, or body language (Andreas et al., 2022; Goldwasser et al.,\n2023)).\nTesting for AGI\nTwo of our six proposed principles for defining AGI (Principle 2: Generality and Performance; Principle\n6: Focus on the Path to AGI) influenced our choice of a matrixed, leveled ontology for facilitating\nnuanced discussions of the breadth and depth of AI capabilities. Our remaining four principles\n(Principle 1: Capabilities, not Processes; Principle 3: Cognitive and Metacognitive Tasks; Principle 4:\nPotential, not Deployment; and Principle 5: Ecological Validity) relate to the issue of measurement.\nWhile our performance dimension specifies one aspect of measurement (e.g., percentile ranges\nfor task performance relative to particular subsets of people), our generality dimension leaves open\nimportant questions: What is the set of tasks that constitute the generality criteria? What proportion\nof such tasks must an AI system master to achieve a given level of generality in our schema? Are there\nsome tasks that must always be performed to meet the criteria for certain generality levels, such as\nmetacognitive tasks?\nOperationalizing an AGI definition requires answering these questions, as well as developing\nspecific diverse and challenging tasks. Because of the immense complexity of this process, as well\nas the importance of including a wide range of perspectives (including cross-organizational and\nmulti-disciplinary viewpoints), we do not propose a benchmark in this paper. Instead, we work to\nclarify the ontology a benchmark should attempt to measure. We also discuss properties an AGI\nbenchmark should possess.\nOur intent is that an AGI benchmark would include a broad suite of cognitive and metacognitive\ntasks (per Principle 3), measuring diverse properties including (but not limited to) linguistic intelli-\ngence, mathematical and logical reasoning (Webb et al., 2023), spatial reasoning, interpersonal and\nintra-personal social intelligences, the ability to learn new skills (Chollet, 2019), and creativity. A\nbenchmark might include tests covering psychometric categories proposed by theories of intelligence\nfrom psychology, neuroscience, cognitive science, and education; however, such \u201ctraditional\u201d tests\nmust first be evaluated for suitability for benchmarking computing systems, since many may lack\necological and construct validity in this context (Serapio-Garc\u00eda et al., 2023).\nOne open question for benchmarking performance is whether to allow the use of tools, including\npotentially AI-powered tools, as an aid to human performance. This choice may ultimately be task\ndependent and should account for ecological validity in benchmark choice (per Principle 5). For\nexample, in determining whether a self-driving car is sufficiently safe, benchmarking against a person\ndriving without the benefit of any modern AI-assisted safety tools would not be the most informative\n8\nLevels of AGI: Operationalizing Progress on the Path to AGI\ncomparison; since the relevant counterfactual involves some driver-assistance technology, we may\nprefer a comparison to that baseline.\nWhile an AGI benchmark might draw from some existing AI benchmarks (Lynch, 2023) (e.g.,\nHELM (Liang et al., 2023), BIG-bench (Srivastava et al., 2023)), we also envision the inclusion of\nopen-ended and/or interactive tasks that might require qualitative evaluation (Bubeck et al., 2023;\nPapakyriakopoulos et al., 2021; Yang et al., 2023). We suspect that these latter classes of complex,\nopen-ended tasks, though difficult to benchmark, will have better ecological validity than traditional\nAI metrics, or than adapted traditional measures of human intelligence.\nIt is impossible to enumerate the full set of tasks achievable by a sufficiently general intelligence.\nAs such, an AGI benchmark should be a living benchmark. Such a benchmark should therefore include\na framework for generating and agreeing upon new tasks.\nDetermining that something is not an AGI at a given level simply requires identifying several4 tasks\nthat people can typically do but the system cannot adequately perform. Systems that pass the majority\nof the envisioned AGI benchmark at a particular performance level (\"Emerging,\" \"Competent,\" etc.),\nincluding new tasks added by the testers, can be assumed to have the associated level of generality\nfor practical purposes (i.e., though in theory there could still be a test the AGI would fail, at some\npoint unprobed failures are so specialized or atypical as to be practically irrelevant).\nDeveloping an AGI benchmark will be a challenging and iterative process. It is nonetheless a\nvaluable north-star goal for the AI research community. Measurement of complex concepts may be\nimperfect, but the act of measurement helps us crisply define our goals and provides an indicator of\nprogress.\nRisk in Context: Autonomy and Human-AI Interaction\nDiscussions of AGI often include discussion of risk, including \"x-risk\" \u2013 existential (for AI Safety, 2023)\nor other very extreme risks (Shevlane et al., 2023). A leveled approach to defining AGI enables a\nmore nuanced discussion of how different combinations of performance and generality relate to\ndifferent types of AI risk. While there is value in considering extreme risk scenarios, understanding\nAGI via our proposed ontology rather than as a single endpoint (per Principle 6) can help ensure that\npolicymakers also identify and prioritize risks in the near-term and on the path to AGI.\nLevels of AGI as a Framework for Risk Assessment\nAs we advance along our capability levels toward ASI, new risks are introduced, including misuse\nrisks, alignment risks, and structural risks (Zwetsloot and Dafoe, 2019). For example, the \u201cExpert AGI\u201d\nlevel is likely to involve structural risks related to economic disruption and job displacement, as more\nand more industries reach the substitution threshold for machine intelligence in lieu of human labor.\nOn the other hand, reaching \u201cExpert AGI\u201d likely alleviates some risks introduced by \u201cEmerging AGI\u201d\nand \u201cCompetent AGI,\u201d such as the risk of incorrect task execution. The \u201cVirtuoso AGI\u201d and \u201cASI\u201d levels\nare where many concerns relating to x-risk are most likely to emerge (e.g., an AI that can outperform\nits human operators on a broad range of tasks might deceive them to achieve a mis-specified goal, as\n4 We hesitate to specify the precise number or percentage of tasks that a system must pass at a given level of performance\nin order to be declared a General AI at that Level (e.g., a rule such as \"a system must pass at least 90% of an AGI benchmark\nat a given performance level to get that rating\"). While we think this will be a very high percentage, it will probably not\nbe 100%, since it seems clear that broad but imperfect generality is impactful (individual humans also lack consistent\nperformance across all possible tasks, but remain generally intelligent). Determining what portion of benchmarking tasks at\na given level demonstrate generality remains an open research question.\n9\nLevels of AGI: Operationalizing Progress on the Path to AGI\nin misalignment thought experiments (Christian, 2020)).\nSystemic risks such as destabilization of international relations may be a concern if the rate of\nprogression between levels outpaces regulation or diplomacy (e.g., the first nation to achieve ASI may\nhave a substantial geopolitical/military advantage, creating complex structural risks). At levels below\n\u201cExpert AGI\u201d (e.g., \u201cEmerging AGI,\u201d \u201cCompetent AGI,\u201d and all \u201cNarrow\u201d AI categories), risks likely\nstem more from human actions (e.g., risks of AI misuse, whether accidental, incidental, or malicious).\nA more complete analysis of risk profiles associated with each level is a critical step toward developing\na taxonomy of AGI that can guide safety/ethics research and policymaking.\nWe acknowledge that whether an AGI benchmark should include tests for potentially dangerous\ncapabilities (e.g., the ability to deceive, to persuade (Veerabadran et al., 2023), or to perform advanced\nbiochemistry (Morris, 2023)) is controversial. We lean on the side of including such capabilities\nin benchmarking, since most such skills tend to be dual use (having valid applications to socially\npositive scenarios as well as nefarious ones). Dangerous capability benchmarking can be de-risked\nvia Principle 4 (Potential, not Deployment) by ensuring benchmarks for any dangerous or dual-use\ntasks are appropriately sandboxed and not defined in terms of deployment. However, including such\ntests in a public benchmark may allow malicious actors to optimize for these abilities; understanding\nhow to mitigate risks associated with benchmarking dual-use abilities remains an important area for\nresearch by AI safety, AI ethics, and AI governance experts.\nConcurrent with this work, Anthropic released Version 1.0 of its Responsible Scaling Policy (RSP)\n(Anthropic, 2023b). This policy uses a levels-based approach (inspired by biosafety level standards)\nto define the level of risk associated with an AI system, identifying what dangerous capabilities may\nbe associated with each AI Safety Level (ASL), and what containment or deployment measures should\nbe taken at each level. Current SOTA generative AIs are classified as an ASL-2 risk. Including items\nmatched to ASL capabilities in any AGI benchmark would connect points in our AGI taxonomy to\nspecific risks and mitigations.\nCapabilities vs. Autonomy\nWhile capabilities provide prerequisites for AI risks, AI systems (including AGI systems) do not and\nwill not operate in a vacuum. Rather, AI systems are deployed with particular interfaces and used to\nachieve particular tasks in specific scenarios. These contextual attributes (interface, task, scenario,\nend-user) have substantial bearing on risk profiles. AGI capabilities alone do not determine destiny\nwith regards to risk, but must be considered in combination with contextual details.\nConsider, for instance, the affordances of user interfaces for AGI systems. Increasing capabilities\nunlock new interaction paradigms, but do not determine them. Rather, system designers and end-\nusers will settle on a mode of human-AI interaction (Morris et al., 2023) that balances a variety of\nconsiderations, including safety. We propose characterizing human-AI interaction paradigms with six\nLevels of Autonomy, described in Table 2.\nThese Levels of Autonomy are correlated with the Levels of AGI. Higher levels of autonomy are\n\u201cunlocked\u201d by AGI capability progression, though lower levels of autonomy may be desirable for\nparticular tasks and contexts (including for safety reasons) even as we reach higher levels of AGI.\nCarefully considered choices around human-AI interaction are vital to safe and responsible deployment\nof frontier AI models.\nUnlike prior taxonomies of computer automation proposed by Sheridan and colleagues (Parasura-\nman et al., 2000; Sheridan and Parasuraman, 2005; Sheridan et al., 1978) that take a computer-centric\nperspective (framing automation in terms of how much control the designer relinquishes to comput-\n10\nLevels of AGI: Operationalizing Progress on the Path to AGI\nAutonomy Level\nExample Systems\nUnlocking\nAGI Level(s)\nExample Risks\nIntroduced\nAutonomy Level 0:\nNo AI\nhuman does every-\nthing\nAnalogue\napproaches\n(e.g.,\nsketching with pencil on paper)\nNon-AI\ndigital\nworkflows\n(e.g., typing in a text editor;\ndrawing in a paint program)\nNo AI\nn/a (status quo risks)\nAutonomy Level 1:\nAI as a Tool\nhuman fully controls\ntask and uses AI to\nautomate mundane\nsub-tasks\nInformation-seeking with the\naid of a search engine\nRevising\nwriting\nwith\nthe\naid\nof\na\ngrammar-checking\nprogram\nReading a sign with a\nmachine translation app\nPossible:\nEmerging Narrow AI\nLikely:\nCompetent\nNarrow\nAI\nde-skilling\n(e.g., over-reliance)\ndisruption of\nestablished\nindustries\nAutonomy Level 2:\nAI as a Consultant\nAI takes on a\nsubstantive role, but\nonly when invoked by\na human\nRelying on a language model\nto summarize a set of documents\nAccelerating computer program-\nming with a code-generating\nmodel\nConsuming\nmost\nentertain-\nment\nvia\na\nsophisticated\nrecommender system\nPossible:\nCompetent\nNarrow\nAI\nLikely:\nExpert Narrow AI;\nEmerging AGI\nover-trust\nradicalization\ntargeted\nmanipulation\nAutonomy Level 3:\nAI as a\nCollaborator\nco-equal human-AI\ncollaboration; inter-\nactive coordination\nof goals & tasks\nTraining\nas\na\nchess\nplayer\nthrough interactions with and\nanalysis of a chess-playing AI\nEntertainment\nvia\nsocial\ninteractions with AI-generated\npersonalities\nPossible:\nEmerging AGI\nLikely:\nExpert Narrow AI;\nCompetent AGI\nanthropomorphization\n(e.g., parasocial\nrelationships)\nrapid societal change\nAutonomy Level 4:\nAI as an Expert\nAI drives interaction;\nhuman\nprovides\nguidance & feedback\nor performs subtasks\nUsing an AI system to advance\nscientific discovery (e.g., protein-\nfolding)\nPossible:\nVirtuoso Narrow AI\nLikely:\nExpert AGI\nsocietal-scale ennui\nmass labor\ndisplacement\ndecline\nof\nhuman\nexceptionalism\nAutonomy Level 5:\nAI as an Agent\nfully autonomous AI\nAutonomous AI-powered\npersonal assistants\n(not yet unlocked)\nLikely:\nVirtuoso AGI;\nASI\nmisalignment\nconcentration\nof power\nTable 2 | More capable AI systems unlock new human-AI interaction paradigms (including fully\nautonomous AI). The choice of appropriate autonomy level need not be the maximum achievable\ngiven the capabilities of the underlying model. One consideration in the choice of autonomy level\nare resulting risks. This table\u2019s examples illustrate the importance of carefully considering human-AI\ninteraction design decisions.\n11\nLevels of AGI: Operationalizing Progress on the Path to AGI\ners), we characterize the concept of autonomy through the lens of the nature of human-AI interaction\nstyle; further, our ontology considers how AI capabilities may enable particular interaction paradigms\nand how the combination of level of autonomy and level of AGI may impact risk. Shneiderman\n(Shneiderman, 2020) observes that automation is not a zero-sum game, and that high levels of au-\ntomation can co-exist with high levels of human control; this view is compatible with our perspective\nof considering automation through the perspective of varying styles of human-AI partnerships.\nWe emphasize the importance of the \u201cNo AI\u201d paradigm. There may be many situations where this\nis desirable, including for education, enjoyment, assessment, or safety reasons. For example, in the\ndomain of self-driving vehicles, when Level 5 Self-Driving technology is widely available, there may\nbe reasons for using a Level 0 (No Automation) vehicle. These include for instructing a new driver\n(education), for pleasure by driving enthusiasts (enjoyment), for driver\u2019s licensing exams (assessment),\nor in conditions where sensors cannot be relied upon such as technology failures or extreme weather\nevents (safety). While Level 5 Self-Driving (SAE International, 2021) vehicles would likely be a Level\n5 Narrow AI (\u201cSuperhuman Narrow AI\u201d) under our taxonomy5, the same considerations regarding\nhuman vs. computer autonomy apply to AGIs. We may develop an AGI, but choose not to deploy it\nautonomously (or choose to deploy it with differentiated autonomy levels in distinct circumstances as\ndictated by contextual considerations).\nCertain aspects of generality may be required to make particular interaction paradigms desirable.\nFor example, the Autonomy Levels 3, 4, and 5 (\"Collaborator,\" \"Expert,\" and \"Agent\") may only work\nwell if an AI system also demonstrates strong performance on certain metacognitive abilities (learning\nwhen to ask a human for help, theory of mind modeling, social-emotional skills). Implicit in our\ndefinition of Autonomy Level 5 (\"AI as an Agent\") is that such a fully autonomous AI can act in an\naligned fashion without continuous human oversight, but knows when to consult humans (Shah et al.,\n2021). Interfaces that support human-AI alignment through better task specification, the bridging of\nprocess gulfs, and evaluation of outputs (Terry et al., 2023) are a vital area of research for ensuring\nthat the field of human-computer interaction keeps pace with the challenges and opportunities of\ninteracting with AGI systems.\nHuman-AI Interaction Paradigm as a Framework for Risk Assessment\nTable 2 illustrates the interplay between AGI Level, Autonomy Level, and risk. Advances in model\nperformance and generality unlock additional interaction paradigm choices (including potentially fully\nautonomous AI). These interaction paradigms in turn introduce new classes of risk. The interplay of\nmodel capabilities and interaction design will enable more nuanced risk assessments and responsible\ndeployment decisions than considering model capabilities alone.\nTable 2 also provides concrete examples of each of our six proposed Levels of Autonomy. For each\nlevel of autonomy, we indicate the corresponding levels of performance and generality that \"unlock\"\nthat interaction paradigm (i.e., levels of AGI at which it is possible or likely for that paradigm to be\nsuccessfully deployed and adopted).\nOur predictions regarding \"unlocking\" levels tend to require higher levels of performance for\nNarrow than for General AI systems; for instance, we posit that the use of AI as a Consultant is\nlikely with either an Expert Narrow AI or an Emerging AGI. This discrepancy reflects the fact that for\nGeneral systems, capability development is likely to be uneven; for example, a Level 1 General AI\n(\"Emerging AGI\") is likely to have Level 2 or perhaps even Level 3 performance across some subset of\n5 Fully autonomous vehicles might arguably be classified as Level 4 Narrow AI (\"Virtuoso Narrow AI\") per our tax-\nonomy; however, we suspect that in practice autonomous vehicles may need to reach the Superhuman performance\nstandard to achieve widespread social acceptance regarding perceptions of safety, illustrating the importance of contextual\nconsiderations.\n12\nLevels of AGI: Operationalizing Progress on the Path to AGI\ntasks. Such unevenness of capability for General AIs may unlock higher autonomy levels for particular\ntasks that are aligned with their specific strengths.\nConsidering AGI systems in the context of use by people allows us to reflect on the interplay\nbetween advances in models and advances in human-AI interaction paradigms. The role of model\nbuilding research can be seen as helping systems\u2019 capabilities progress along the path to AGI in their\nperformance and generality, such that an AI system\u2019s abilities will overlap an increasingly large portion\nof human abilities. Conversely, the role of human-AI interaction research can be viewed as ensuring\nnew AI systems are usable by and useful to people such that AI systems successfully extend people\u2019s\ncapabilities (i.e., \"intelligence augmentation\" (Brynjolfsson, 2022; Englebart, 1962)).\nConclusion\nArtificial General Intelligence (AGI) is a concept of both aspirational and practical consequences. In this\npaper, we analyzed nine prominent definitions of AGI, identifying strengths and weaknesses. Based\non this analysis, we introduce six principles we believe are necessary for a clear, operationalizable\ndefinition of AGI: focusing on capabilities, not processes; focusing on generality and performance;\nfocusing on cognitive and metacognitive (rather than physical) tasks; focusing on potential rather\nthan deployment; focusing on ecological validity for benchmarking tasks; and focusing on the path\ntoward AGI rather than a single endpoint.\nWith these principles in mind, we introduced our Levels of AGI ontology, which offers a more\nnuanced way to define our progress toward AGI by considering generality (either Narrow or General)\nin tandem with five levels of performance (Emerging, Competent, Expert, Virtuoso, and Superhuman).\nWe reflected on how current AI systems and AGI definitions fit into this framing. Further, we discussed\nthe implications of our principles for developing a living, ecologically valid AGI benchmark, and argue\nthat such an endeavor (while sure to be challenging) is a vital one for our community to engage with.\nFinally, we considered how our principles and ontology can reshape discussions around the risks\nassociated with AGI. Notably, we observed that AGI is not necessarily synonymous with autonomy.\nWe introduced Levels of Autonomy that are unlocked, but not determined by, progression through\nthe Levels of AGI. We illustrated how considering AGI Level jointly with Autonomy Level can provide\nmore nuanced insights into likely risks associated with AI systems, underscoring the importance of\ninvesting in human-AI interaction research in tandem with model improvements.\nAcknowledgements\nThank you to the members of the Google DeepMind PAGI team for their support of this effort, and to\nMartin Wattenberg, Michael Terry, Geoffrey Irving, Murray Shanahan, Dileep George, Blaise Ag\u00fcera y\nArcas, and Ben Shneiderman for helpful discussions about this topic.\nReferences\nBlaise\nAg\u00fcera\ny\nArcas\nand\nPeter\nNorvig.\nArtificial\nGeneral\nIntelligence\nis\nAl-\nready\nHere.\nNoema,\nOctober\n2023.\nURL\nhttps://www.noemamag.com/\nartificial-general-intelligence-is-already-here/.\nAmazon. Amazon Alexa. URL https://alexa.amazon.com/. accessed on October 20, 2023.\nJacob Andreas, Ga\u0161per Begu\u0161, Michael M. Bronstein, Roee Diamant, Denley Delaney, Shane Gero,\nShafi Goldwasser, David F. Gruber, Sarah de Haas, Peter Malkin, Nikolay Pavlov, Roger Payne,\n13\nLevels of AGI: Operationalizing Progress on the Path to AGI\nGiovanni Petri, Daniela Rus, Pratyusha Sharma, Dan Tchernov, Pernille T\u00f8nnesen, Antonio Torralba,\nDaniel Vogt, and Robert J. Wood. Toward understanding the communication in sperm whales.\niScience, 25(6):104393, 2022. ISSN 2589-0042. doi: https://doi.org/10.1016/j.isci.2022.104393.\nURL https://www.sciencedirect.com/science/article/pii/S2589004222006642.\nRohan Anil, Andrew M. Dai, Orhan Firat, and et al. PaLM 2 Technical Report. CoRR, abs/2305.10403,\n2023. doi: 10.48550/arXiv.2305.10403. URL https://arxiv.org/abs/2305.10403.\nAnthropic. Company: Anthropic, 2023a. URL https://www.anthropic.com/company. Accessed\nOctober 12, 2023.\nAnthropic. Anthropic\u2019s Responsible Scaling Policy, September 2023b. URL https://www-files.\nanthropic.com/production/files/responsible-scaling-policy-1.0.pdf. accessed\non October 20, 2023.\nApple. Siri. URL https://www.apple.com/siri/. accessed on October 20, 2023.\nLudovic Bellier, Ana\u00efs Llorens, D\u00e9borah Marciano, Aysegul Gunduz, Gerwin Schalk, Peter Brunner, and\nRobert T. Knight. Music can be reconstructed from human auditory cortex activity using nonlinear\ndecoding models. PLOS Biology, 21(8):1\u201327, 08 2023. doi: 10.1371/journal.pbio.3002176. URL\nhttps://doi.org/10.1371/journal.pbio.3002176.\nYoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin\nZhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter,\nAt\u0131l\u0131m G\u00fcne\u015f Baydin, Sheila McIlraith, Qiqi Gao, Ashwin Acharya, David Krueger, Anca Dragan,\nPhilip Torr, Stuart Russell, Daniel Kahneman, Jan Brauner, and S\u00f6ren Mindermann. Managing AI\nRisks in an Era of Rapid Progress. CoRR, abs/2310.17688, 2023. doi: 10.48550/arXiv.2310.17688.\nURL https://arxiv.org/abs/2310.17688.\nJeffrey Bigham. The Coming AI Autumn. Blog Post, 2019. URL https://jeffreybigham.com/\nblog/2019/the-coming-ai-autumnn.html.\nMargaret A. Boden. GOFAI, page 89\u2013107. Cambridge University Press, 2014.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski,\nTianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gon-\nzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine\nHsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel\nLeal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch,\nKarl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre\nSermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan\nVuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun\nXu, Tianhe Yu, and Brianna Zitkovich. RT-2: Vision-Language-Action Models Transfer Web Knowl-\nedge to Robotic Control. CoRR, abs/2307.15818, 2023. doi: 10.48550/arXiv.2307.15818. URL\nhttps://arxiv.org/abs/2307.15818.\nErik Brynjolfsson. The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence. CoRR,\nabs/2201.04200, 2022. doi: 10.48550/arXiv.2201.04200. URL https://arxiv.org/abs/2201.\n04200.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio\nRibeiro, and Yi Zhang. Sparks of Artificial General Intelligence: Early experiments with GPT-4.\n14\nLevels of AGI: Operationalizing Progress on the Path to AGI\nCoRR, abs/2303.12712, 2023. doi: 10.48550/arXiv.2303.12712. URL https://arxiv.org/\nabs/2303.12712.\nPatrick Butlin, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George\nDeane, Stephen M. Fleming, Chris Frith, Xu Ji, Ryota Kanai, Colin Klein, Grace Lindsay, Matthias\nMichel, Liad Mudrik, Megan A. K. Peters, Eric Schwitzgebel, Jonathan Simon, and Rufin Van-\nRullen. Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. CoRR,\nabs/2308.08708, 2023. doi: 10.48550/arXiv.2308.08708. URL https://arxiv.org/abs/2308.\n08708.\nMurray Campbell, A. Joseph Hoane, and Feng-hsiung Hsu. Deep Blue. Artif. Intell., 134(1\u20132):57\u201383,\njan 2002. ISSN 0004-3702. doi: 10.1016/S0004-3702(01)00129-1. URL https://doi.org/10.\n1016/S0004-3702(01)00129-1.\nXi Chen, Xiao Wang, Soravit Changpinyo, and et al. PaLI: A Jointly-Scaled Multilingual Language-\nImage Model. CoRR, abs/2209.06794, 2023. doi: 10.48550/arXiv.2209.06794. URL https:\n//arxiv.org/abs/2209.06794.\nFran\u00e7ois Chollet. On the measure of intelligence, 2019.\nBrian Christian. The Alignment Problem. W. W. Norton & Company, 2020.\nMillon Madhur Das, Punyajoy Saha, and Mithun Das. Which One is More Toxic? Findings from Jigsaw\nRate Severity of Toxic Comments. CoRR, abs/2206.13284, 2022. doi: 10.48550/arXiv.2206.13284.\nURL https://arxiv.org/abs/2206.13284.\nFabrizio Dell\u2019Acqua, Edward McFowland, Ethan R. Mollick, Hila Lifshitz-Assaf, Katherine Kellogg,\nSaran Rajendran, Lisa Krayer, Fran\u00e7ois Candelon, and Karim R. Lakhani. Navigating the Jagged\nTechnological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker\nProductivity and Quality. Harvard Business School Technology & Operations Management Unit\nWorking Paper Number 24-013, September 2023.\nKweilin Ellingrud, Saurabh Sanghvi, Gurneet Singh Dandona, Anu Madgavkar, Michael Chui, Olivia\nWhite, and Paige Hasebe.\nGenerative AI and the future of work in America.\nMcKinsey In-\nstitute Global Report, July 2023. URL https://www.mckinsey.com/mgi/our-research/\ngenerative-ai-and-the-future-of-work-in-america.\nTyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. Gpts are gpts: An early look at the\nlabor market impact potential of large language models, 2023.\nD.C.\nEnglebart.\nAugmenting\nhuman\nintellect:\nA\nconceptual\nframework.\nOctober\n1962.\nURL\nhttps://www.dougengelbart.org/pubs/papers/scanned/Doug_\nEngelbart-AugmentingHumanIntellect.pdf.\nCenter for AI Safety.\nStatement on AI Risk,\n2023.\nURL https://www.safe.ai/\nstatement-on-ai-risk.\nBen Goertzel. Artificial General Intelligence: Concept, State of the Art, and Future Prospects. Journal\nof Artificial General Intelligence, 01 2014. doi: 10.2478/jagi-2014-0001.\nShafi Goldwasser, David F. Gruber, Adam Tauman Kalai, and Orr Paradise. A theory of unsupervised\ntranslation motivated by understanding animal communication, 2023.\nGoogle. Google Assistant, your own personal Google. URL https://assistant.google.com/.\naccessed on October 20, 2023.\n15\nLevels of AGI: Operationalizing Progress on the Path to AGI\nGrammarly, 2023. URL https://www.grammarly.com/.\nRoss Gruetzemacher and David B. Paradice. Alternative Techniques for Mapping Paths to HLAI. CoRR,\nabs/1905.00614, 2019. doi: 10.48550/arXiv.1905.00614. URL http://arxiv.org/abs/1905.\n00614.\nMark Gubrud. Nanotechnology and International Security. Fifth Foresight Conference on Molecular\nNanotechnology, November 1997.\nIBM. IBM Watson. URL https://www.ibm.com/watson. accessed on October 20, 2023.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,\nKathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, Alex Bridgland, Clemens\nMeyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav\nNikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal\nZielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David\nSilver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis.\nHighly Accurate Protein Structure Prediction with AlphaFold. Nature, 596:583\u2013589, 2021. doi:\n10.1038/s41586-021-03819-2.\nZachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving.\nAlignment of Language Agents. CoRR, abs/2103.14659, 2021. doi: 10.48550/arXiv.2103.14659.\nURL https://arxiv.org/abs/2103.14659.\nHenry Kissinger, Eric Schmidt, and Daniel Huttenlocher. The Age of AI. Back Bay Books, November\n2022.\nShane Legg. Machine Super Intelligence. Doctoral Dissertation submitted to the Faculty of Informatics\nof the University of Lugano, June 2008.\nShane Legg. Twitter (now \"X\"), May 2022. URL https://twitter.com/ShaneLegg/status/\n1529483168134451201. Accessed on October 12, 2023.\nPercy Liang, Rishi Bommasani, Tony Lee, and et al. Holistic Evaluation of Language Models. CoRR,\nabs/2211.09110, 2023. doi: 10.48550/arXiv.2211.09110. URL https://arxiv.org/abs/2211.\n09110.\nShana Lynch. AI Benchmarks Hit Saturation. Stanford Human-Centered Artificial Intelligence Blog,\nApril 2023. URL https://hai.stanford.edu/news/ai-benchmarks-hit-saturation.\nGary Marcus.\nDear Elon Musk, here are five things you might want to consider about\nAGI. \"Marcus on AI\" Substack, May 2022a. URL https://garymarcus.substack.com/p/\ndear-elon-musk-here-are-five-things?s=r.\nGary Marcus. Twitter (now \"X\"), May 2022b. URL https://twitter.com/GaryMarcus/status/\n1529457162811936768. Accessed on October 12, 2023.\nJ. McCarthy, M.L. Minsky, N. Rochester, and C.E. Shannon. A Proposal for The Dartmouth Summer\nResearch Project on Artificial Intelligence. Dartmouth Workshop, 1955.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model Cards for Model Reporting. In\nProceedings of the Conference on Fairness, Accountability, and Transparency. ACM, jan 2019. doi:\n10.1145/3287560.3287596. URL https://doi.org/10.1145%2F3287560.3287596.\n16\nLevels of AGI: Operationalizing Progress on the Path to AGI\nMeredith Ringel Morris. Scientists\u2019 Perspectives on the Potential for Generative AI in their Fields.\nCoRR, abs/2304.01420, 2023. doi: 10.48550/arXiv.2304.01420. URL https://arxiv.org/\nabs/2304.01420.\nMeredith Ringel Morris, Carrie J. Cai, Jess Holbrook, Chinmay Kulkarni, and Michael Terry. The Design\nSpace of Generative Models. CoRR, abs/2304.10547, 2023. doi: 10.48550/arXiv.2304.10547. URL\nhttps://arxiv.org/abs/2304.10547.\nMustafa Suleyman and Michael Bhaskar. The Coming Wave: Technology, Power, and the 21st Century\u2019s\nGreatest Dilemma. Crown, September 2023.\nOpenAI. OpenAI Charter, 2018. URL https://openai.com/charter. Accessed October 12, 2023.\nOpenAI. OpenAI: About, 2023. URL https://openai.com/about. Accessed October 12, 2023.\nOpenAI. GPT-4 Technical Report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774.\nURL https://arxiv.org/abs/2303.08774.\nOrestis Papakyriakopoulos, Elizabeth Anne Watkins, Amy Winecoff, Klaudia Ja\u017awi\u0144ska, and Tithi\nChattopadhyay. Qualitative Analysis for Human Centered AI. CoRR, abs/2112.03784, 2021. doi:\n10.48550/arXiv.2112.03784. URL https://arxiv.org/abs/2112.03784.\nR. Parasuraman, T.B. Sheridan, and C.D. Wickens. A model for types and levels of human interaction\nwith automation. IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans,\n30(3):286\u2013297, 2000. doi: 10.1109/3468.844354.\nSundar Pichai and Demis Hassabis. Introducing gemini: our largest and most capable ai model,\nDecember 2023. URL https://blog.google/technology/ai/google-gemini-ai/.\nPromptBase. PromptBase: Prompt Marketplace. URL https://promptbase.com/. accessed on\nOctober 20, 2023.\nInioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, Emily Denton, and Alex Hanna.\nAI and the Everything in the Whole Wide World Benchmark. CoRR, abs/2111.15366, 2021. doi:\n10.48550/arXiv.2111.15366. URL https://arxiv.org/abs/2111.15366.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-\nConditional Image Generation with CLIP Latents. April 2022. URL https://cdn.openai.com/\npapers/dall-e-2.pdf.\nTilman R\u00e4uker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward Transparent AI: A\nSurvey on Interpreting the Inner Structures of Deep Neural Networks. CoRR, abs/2207.13243,\n2023. doi: 10.48550/arXiv.2207.13243. URL https://arxiv.org/abs/2207.13243.\nNicholas Roy, Ingmar Posner, Tim Barfoot, Philippe Beaudoin, Yoshua Bengio, Jeannette Bohg, Oliver\nBrock, Isabelle Depatie, Dieter Fox, Dan Koditschek, Tomas Lozano-Perez, Vikash Mansinghka,\nChristopher Pal, Blake Richards, Dorsa Sadigh, Stefan Schaal, Gaurav Sukhatme, Denis Therien,\nMarc Toussaint, and Michiel Van de Panne. From Machine Learning to Robotics: Challenges and\nOpportunities for Embodied Intelligence. CoRR, abs/2110.15245, 2021. doi: 10.48550/arXiv.2110.\n15245. URL https://arxiv.org/abs/2110.15245.\nSAE International. Taxonomy and Definitions for Terms Related to Driving Automation Systems\nfor On-Road Motor Vehicles, April 2021. URL https://www.sae.org/standards/content/\nj3016_202104. Accessed October 12, 2023.\n17\nLevels of AGI: Operationalizing Progress on the Path to AGI\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam-\nyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Sal-\nimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi.\nPhotorealistic Text-to-Image\nDiffusion Models with Deep Language Understanding.\nCoRR, abs/2205.11487, 2022.\ndoi:\n10.48550/arXiv.2205.11487. URL https://arxiv.org/abs/2205.11487.\nPhilipp Schoenegger and Peter S. Park. Large language model prediction capabilities: Evidence from\na real-world forecasting tournament, 2023.\nJohn R. Searle. Minds, Brains, and Programs. Behavioral and Brain Sciences, 3:417\u2013424, 1980. doi:\n10.1017/S0140525X00005756.\nGreg Serapio-Garc\u00eda, Mustafa Safdari, Cl\u00e9ment Crepy, Luning Sun, Stephen Fitz, Peter Romero,\nMarwa Abdulhai, Aleksandra Faust, and Maja Matari\u0107. Personality Traits in Large Language Models.\nCoRR, abs/2307.00184, 2023. doi: 10.48550/arXiv.2307.00184. URL https://arxiv.org/\nabs/2307.00184.\nRohin Shah, Pedro Freire, Neel Alex, Rachel Freedman, Dmitrii Krasheninnikov, Lawrence Chan,\nMichael D Dennis, Pieter Abbeel, Anca Dragan, and Stuart Russell. Benefits of Assistance over\nReward Learning, 2021. URL https://openreview.net/forum?id=DFIoGDZejIB.\nMurray Shanahan. Embodiment and the Inner Life. Oxford University Press, 2010.\nMurray Shanahan. The Technological Singularity. MIT Press, August 2015.\nThomas B. Sheridan and Raja Parasuraman. Human-automation interaction. Reviews of Human\nFactors and Ergonomics, 1(1):89\u2013129, 2005. doi: 10.1518/155723405783703082. URL https:\n//doi.org/10.1518/155723405783703082.\nThomas B Sheridan, William L Verplank, and TL Brooks. Human/computer control of undersea\nteleoperators. In NASA. Ames Res. Center The 14th Ann. Conf. on Manual Control, 1978.\nToby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung,\nDaniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth,\nShahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul\nChristiano, and Allan Dafoe. Model evaluation for extreme risks. CoRR, abs/2305.15324, 2023.\ndoi: 10.48550/arXiv.2305.15324. URL https://arxiv.org/abs/2305.15324.\nBen Shneiderman. Human-centered artificial intelligence: Reliable, safe & trustworthy, 2020. URL\nhttps://arxiv.org/abs/2002.04087v1.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,\nDominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach,\nKoray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the Game of Go with Deep\nNeural Networks and Tree Search. Nature, 529:484\u2013489, 2016. doi: 10.1038/nature16961.\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui,\nLaurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the Game\nof Go Without Human Knowledge. Nature, 550:354\u2013359, 2017. doi: 10.1038/nature24270.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,\nMarc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan,\n18\nLevels of AGI: Operationalizing Progress on the Path to AGI\nand Demis Hassabis. A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and\nGo through Self-play. Science, 362(6419):1140\u20131144, 2018. doi: 10.1126/science.aar6404. URL\nhttps://www.science.org/doi/abs/10.1126/science.aar6404.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, and et al. Beyond the Imitation Game: Quantifying\nand Extrapolating the Capabilities of Language Models. CoRR, abs/2206.04615, 2023. doi:\n10.48550/arXiv.2206.04615. URL https://arxiv.org/abs/2206.04615.\nStockfish. Stockfish - Open Source Chess Engine, 2023. URL https://stockfishchess.org/.\nJerry Tang, Amanda LeBel, Shailee Jain, and Alexander G. Huth.\nSemantic Reconstruction of\nContinuous Language from Non-invasive Brain Recordings. Nature Neuroscience, 26:858\u2013866, 2023.\ndoi: 10.1038/s41593-023-01304-9.\nMichael Terry, Chinmay Kulkarni, Martin Wattenberg, Lucas Dixon, and Meredith Ringel Morris.\nAI Alignment in the Design of Interactive AI: Specification Alignment, Process Alignment, and\nEvaluation Support.\nCoRR, abs/2311.00710, 2023.\ndoi: 10.48550/arXiv.2311.00710.\nURL\nhttps://arxiv.org/abs/2311.00710.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Can-\nton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu,\nBrian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,\nRui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-\nrenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang\nKuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open\nFoundation and Fine-Tuned Chat Models, 2023.\nA.M. Turing. Computing Machinery and Intelligence. Mind, LIX:433\u2013460, October 1950. URL\nhttps://doi.org/10.1093/mind/LIX.236.433.\nMihaly Varadi, Stephen Anyango, Mandar Deshpande, Sreenath Nair, Cindy Natassia, Galabina\nYordanova, David Yuan, Oana Stroe, Gemma Wood, Agata Laydon, Augustin \u017d\u00eddek, Tim Green,\nKathryn Tunyasuvunakool, Stig Petersen, John Jumper, Ellen Clancy, Richard Green, Ankur Vora,\nMira Lutfi, Michael Figurnov, Andrew Cowie, Nicole Hobbs, Pushmeet Kohli, Gerard Kleywegt, Ewan\nBirney, Demis Hassabis, and Sameer Velankar. AlphaFold Protein Structure Database: Massively\nExpanding the Structural Coverage of Protein-Sequence Space with High-Accuracy Models. Nucleic\nAcids Research, 50:D439\u2013D444, 11 2021. ISSN 0305-1048. doi: 10.1093/nar/gkab1061. URL\nhttps://doi.org/10.1093/nar/gkab1061.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention Is All You Need. CoRR, abs/1706.03762, 2023. doi:\n10.48550/arXiv.1706.03762. URL https://arxiv.org/abs/1706.03762.\nV. Veerabadran, J. Goldman, S. Shankar, and et al.\nSubtle Adversarial Image Manipulations\nInfluence Both Human and Machine Perception.\nNature Communications, 14, 2023.\ndoi:\n10.1038/s41467-023-40499-0.\n19\nLevels of AGI: Operationalizing Progress on the Path to AGI\nTaylor Webb, Keith J. Holyoak, and Hongjing Lu. Emergent Analogical Reasoning in Large Lan-\nguage Models. Nature Human Behavior, 7:1526\u20131541, 2023. URL https://doi.org/10.1038/\ns41562-023-01659-w.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,\nPercy Liang, Jeff Dean, and William Fedus. Emergent Abilities of Large Language Models. CoRR,\nabs/2206.07682, 2022. doi: 10.48550/arXiv.2206.07682. URL https://arxiv.org/abs/2206.\n07682.\nJoseph Weizenbaum. ELIZA\u2014a Computer Program for the Study of Natural Language Communication\nbetween Man and Machine.\nCommun. ACM, 9(1):36\u201345, jan 1966.\nISSN 0001-0782.\ndoi:\n10.1145/365153.365168. URL https://doi.org/10.1145/365153.365168.\nKyle Wiggers. OpenAI Disbands its Robotics Research Team. VentureBeat, July 2021. URL https:\n//venturebeat.com/business/openai-disbands-its-robotics-research-team/.\nWikipedia.\nEugene\nGoostman\n-\nWikipedia,\nThe\nFree\nEncyclopedia.\nhttps://en.wikipedia.org/wiki/Eugene_Goostman, 2023a. Accessed October 12, 2023.\nWikipedia.\nTuring\nTest:\nWeaknesses\n\u2014\nWikipedia,\nThe\nFree\nEncyclopedia.\nhttps://en.wikipedia.org/wiki/Turing_test, 2023b. Accessed October 12, 2023.\nTerry Winograd. Procedures as a Representation for Data in a Computer Program for Understanding\nNatural Language. MIT AI Technical Reports, 1971.\nSteve Wozniak.\nCould a Computer Make a Cup of Coffee?\nFast Company interview:\nhttps://www.youtube.com/watch?v=MowergwQR5Y, 2010.\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan\nWang. The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision). CoRR, abs/2309.17421,\n2023. doi: 10.48550/arXiv.2309.17421. URL https://arxiv.org/abs/2309.17421.\nJ.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang. Why johnny can\u2019t\nprompt: How non-ai experts try (and fail) to design llm prompts. In Proceedings of the 2023\nCHI Conference on Human Factors in Computing Systems, CHI \u201923, New York, NY, USA, 2023.\nAssociation for Computing Machinery. ISBN 9781450394215. doi: 10.1145/3544548.3581388.\nURL https://doi.org/10.1145/3544548.3581388.\nRemco Zwetsloot and Allan Dafoe.\nThinking about Risks from AI: Accidents, Misuse and\nStructure.\nLawfare, 11:2019, 2019.\nURL https://www.lawfaremedia.org/article/\nthinking-about-risks-ai-accidents-misuse-and-structure.\n20\n"
  },
  {
    "title": "S-LoRA: Serving Thousands of Concurrent LoRA Adapters",
    "link": "https://arxiv.org/pdf/2311.03285.pdf",
    "upvote": "27",
    "text": "S-LORA: SERVING THOUSANDS OF CONCURRENT LORA ADAPTERS\nYing Sheng * 1 2 Shiyi Cao * 1 Dacheng Li 1 Coleman Hooper 1 Nicholas Lee 1 Shuo Yang 3 Christopher Chou 1\nBanghua Zhu 1 Lianmin Zheng 1 Kurt Keutzer 1 Joseph E. Gonzalez 1 Ion Stoica 1\nABSTRACT\nThe \u201cpretrain-then-finetune\u201d paradigm is commonly adopted in the deployment of large language models. Low-\nRank Adaptation (LoRA), a parameter-efficient fine-tuning method, is often employed to adapt a base model to a\nmultitude of tasks, resulting in a substantial collection of LoRA adapters derived from one base model. We observe\nthat this paradigm presents significant opportunities for batched inference during serving. To capitalize on these\nopportunities, we present S-LoRA, a system designed for the scalable serving of many LoRA adapters. S-LoRA\nstores all adapters in the main memory and fetches the adapters used by the currently running queries to the\nGPU memory. To efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes Unified Paging.\nUnified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and KV cache\ntensors with varying sequence lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and\nhighly optimized custom CUDA kernels for heterogeneous batching of LoRA computation. Collectively, these\nfeatures enable S-LoRA to serve thousands of LoRA adapters on a single GPU or across multiple GPUs with a\nsmall overhead. Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with naive support\nof LoRA serving), S-LoRA can improve the throughput by up to 4 times and increase the number of served\nadapters by several orders of magnitude. As a result, S-LoRA enables scalable serving of many task-specific\nfine-tuned models and offers the potential for large-scale customized fine-tuning services. The code is available at\nhttps://github.com/S-LoRA/S-LoRA.\n1\nINTRODUCTION\nLarge language models (LLMs) have become ubiquitous in\nmodern applications, ranging from natural language pro-\ncessing to more general tasks (OpenAI, 2023; Touvron\net al., 2023b; Alayrac et al., 2022). Within these domains,\nLLMs have consistently demonstrated superior performance,\nespecially when fine-tuned for specific tasks (Kenton &\nToutanova, 2019; Houlsby et al., 2019; Ouyang et al., 2022).\nThis \u201cpretrain-then-finetune\u201d paradigm has led to the pro-\nliferation of numerous fine-tuned variants of a single base\nLLM, each tailored to a specific task or domain.\nWhen scaling the fine-tuning of a base model for numerous\ntasks, such as personalized assistants, the associated train-\ning and serving costs can become substantial. To address\nthis, several parameter-efficient fine-tuning methods have\nbeen developed. A prime exemplar is Low-Rank Adaptation\n(LoRA) (Hu et al., 2021), which enables efficient fine-tuning\nby updating only low-rank additive matrices. These matri-\nces consist of a small number of parameters, referred to as\n*Equal contribution. Part of the work was done when Ying\nwas visiting UC Berkeley.\n1UC Berkeley 2Stanford University\n3Shanghai Jiao Tong University. Correspondence to: Ying Sheng\n<ying1123@stanford.edu>, Shiyi Cao <shicao@berkeley.edu>.\nadapter weights. LoRA has shown that by fine-tuning just\nthese adapter weights, it is possible to achieve performance\non par with full-weight fine-tuning. However, despite con-\nsiderable research into fine-tuning, the question of how to\nserve these fine-tuned variants at scale remains unexplored.\nOne of the key innovations in the LoRA paper was the\nelimination of adapter inference latency by directly merging\nthe adapter with the model parameters. Additionally, to\nsupport multiple models on a single machine, the same\npaper proposes swapping adapters by adding and subtracting\nLoRA weights from the base model. While this approach\nenables low-latency inference for a single adapter and serial\nexecution across adapters, it significantly reduces overall\nserving throughput and increases total latency when serving\nmultiple adapters concurrently. Moreover, the paper does\nnot consider the opportunity to leverage host memory to\nincrease the number of adapters hosted by a single machine.\nIn this paper, we study how to scalably serve thousands\nof LoRA adapters on a single machine. We observe that\nthe shared base model, which underpins numerous LoRA\nadapters, presents a substantial opportunity for batched in-\nference. To achieve high-throughput multi-adapter serving,\nit is advantageous to separate the batchable base model\ncomputation from individual LoRA computations.\narXiv:2311.03285v2  [cs.LG]  7 Nov 2023\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\nWhile leveraging batching in the base model is straight-\nforward (as all queries share the base model), extending\nbatching to the adapters is challenging. First, serving many\nLoRA adapters simultaneously requires efficient memory\nmanagement. Since GPU memory is limited, we must store\nadapter weights outside the GPU and dynamically fetch\nthem when needed. However, dynamically loading and un-\nloading adapters of varying sizes, coupled with the dynamic\nallocation and deallocation of KV cache tensors for requests\nwith different sequence lengths, can lead to significant mem-\nory fragmentation and I/O overhead. Second, apart from\nthe easily batchable base model computation, the separated\ncomputation of many adapters with distinct ranks in non-\ncontiguous memory is challenging to batch and demands the\ndevelopment of new computation kernels. Third, leveraging\nmultiple GPUs on a single machine requires novel paral-\nlelism strategies to accommodate the added LoRA weights\nand computations. It is essential to carefully design this strat-\negy to minimize communication and memory overheads.\nTo this end, we introduce S-LoRA, a scalable LoRA serving\nsystem. S-LoRA exploits batching opportunities, efficiently\nmanages both host and GPU memory, and orchestrates par-\nallelism across multiple GPUs. The primary contributions\nof S-LoRA are summarized as follows:\n\u2022 Unified Paging: To reduce memory fragmentation and\nincrease batch size, S-LoRA introduces a unified mem-\nory pool. This pool manages dynamic adapter weights\nand KV cache tensors by a unified paging mechanism.\n\u2022 Heterogeneous Batching: To minimize the latency over-\nhead when batching different adapters of varying ranks,\nS-LoRA employs highly optimized custom CUDA ker-\nnels. These kernels operate directly on non-contiguous\nmemory and align with the memory pool design, facili-\ntating efficient batched inference for LoRA.\n\u2022 S-LoRA TP: To ensure effective parallelization across\nmultiple GPUs, S-LoRA introduces a novel tensor par-\nallelism strategy. This approach incurs minimal com-\nmunication cost for the added LoRA computation com-\npared to that of the base model. This is realized by\nscheduling communications on small intermediate ten-\nsors and fusing the large ones with the communications\nof the base model.\nWe evaluate S-LoRA by serving Llama-7B/13B/30B/70B.\nResults show that S-LoRA can serve thousands of LoRA\nadapters on a single GPU or across multiple GPUs with\na small overhead. When compared to the state-of-the-art\nparameter-efficient fine-tuning library, Huggingface PEFT,\nS-LoRA can enhance throughput by up to 30\u00d7. In com-\nparison to the high-throughput serving system vLLM using\na naive support of LoRA serving, S-LoRA can improve\nthroughput by up to 4\u00d7 and increase the number of served\nadapters by several orders of magnitude.\n2\nBACKGROUND\nLow-Rank Adaptation (LoRA) (Hu et al., 2021) is a\nparameter-efficient fine-tuning method designed to adapt\npre-trained large language models to new tasks. The mo-\ntivation behind LoRA stems from the low intrinsic dimen-\nsionality of model updates during adaptation. In the training\nphase, LoRA freezes the weights of a pre-trained base model\nand adds trainable low-rank matrices to each layer. This\napproach significantly reduces the number of trainable pa-\nrameters and memory consumption. When compared to full\nparameter fine-tuning, LoRA can often reduce the number of\ntrainable parameters by orders of magnitude (e.g., 10000\u00d7)\nwhile retaining comparable accuracy. For the inference\nphase, the original paper suggests merging the low-rank\nmatrices with the weights of the base model. As a result,\nthere is no added overhead during inference, setting it apart\nfrom previous adapters like (Houlsby et al., 2019) or prompt\ntuning methods such as (Lester et al., 2021).\nFormally, for a pre-trained weight matrix W \u2208 Rh\u00d7d, LoRA\nintroduces the update as W \u2032 = W +AB, where A \u2208 Rh\u00d7r,\nB \u2208 Rr\u00d7d, and the rank r \u226a min(h, d). If the forward pass\nof a base model is defined by h = xW, then after applying\nLoRA, the forward pass becomes\nh = xW \u2032 = x(W + AB)\n(1)\n= xW + xAB.\n(2)\nTypically, this adjustment is only applied to the query, key,\nvalue, and output projection matrices in the self-attention\nmodule, excluding the feed-forward module.\nBecause LoRA greatly reduces the training and weight stor-\nage costs, it has been widely adopted by the community,\nand people have created hundreds of thousands of LoRA\nadapters for pre-trained large language models and diffusion\nmodels (Mangrulkar et al., 2022).\n2.1\nServing Large Language Models\nMost large language models (LLMs) are based on the trans-\nformer architecture (Vaswani et al., 2017). The number of\nparameters in an LLM ranges from several billion to several\ntrillion (Brown et al., 2020; Chowdhery et al., 2022; Fedus\net al., 2022), corresponding to disk sizes spanning several gi-\ngabytes to even terabytes. This scale results in LLM serving\nhaving significant computational and memory demands.\nAdditionally, the inference process for LLMs requires iter-\native autoregressive decoding. Initially, the model carries\nout a forward pass to encode the prompt. Following this, it\ndecodes the output one token at a time. The sequential pro-\ncess makes decoding slow. Since each token attends to the\nhidden states of all its preceding tokens, it becomes essential\nto store the hidden states of all previous tokens. This storage\nis referred to as the \u201cKV cache\u201d. Such a mechanism adds to\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\nthe memory overhead and causes the decoding process to\nbe more memory-intensive than computation-intensive.\nThe challenges become even more pronounced in online\nsettings, where requests of varying sequence lengths ar-\nrive dynamically. To accommodate such dynamic incoming\nrequests, Orca (Yu et al., 2022) introduces a method of fine-\ngrained, iteration-level scheduling. Instead of scheduling at\nthe request level, Orca batches at the token level. This ap-\nproach allows for the continuous addition of new requests to\nthe currently running batch, resulting in substantially higher\nthroughput. vLLM (Kwon et al., 2023) further optimizes\nOrca\u2019s memory efficiency using PagedAttention. PagedAt-\ntention adopts concepts from virtual memory and paging in\noperating systems and manages the storage and access of\ndynamic KV cache tensors in a paged fashion. This method\nefficiently reduces fragmentation, facilitating larger batch\nsizes and higher throughput.\nWhen serving very large models that exceed the memory\ncapacity of a single GPU, or when there are stringent la-\ntency requirements, it is necessary to parallelize the model\nacross multiple GPUs. Several model parallelism methods\nhave been proposed, such as tensor parallelism (Shoeybi\net al., 2019), sequence parallelism (Korthikanti et al., 2023),\npipeline parallelism (Huang et al., 2019), and their combi-\nnations (Narayanan et al., 2021; Zheng et al., 2022).\n3\nOVERVIEW OF S-LORA\nS-LoRA encompasses three principal components of in-\nnovation. In Section 4, we introduce our batching strat-\negy, which decomposes the computation between the base\nmodel and the LoRA adapters. Additionally, we address\nthe challenge of request scheduling, covering aspects like\nadapter clustering and admission control. The ability to\nbatch across concurrent adapters, introduces new challenges\naround memory management. In Section 5, we generalize\nPagedAttention (Kwon et al., 2023) to Unfied Paging which\nsupport dynamically loading LoRA adapters. This approach\nuses a unified memory pool to store the KV caches and\nadapter weights in a paged fashion, which can reduce frag-\nmentation and balance the dynamic changing size of the\nKV caches and adapter weights. Lastly, in Section 6, we\nintroduce our new tensor parallelism strategy that enables us\nto efficiently decoupled the base model and LoRA adapters.\n4\nBATCHING AND SCHEDULING\n4.1\nBatching\nOur batching strategy aims to support online and high-\nthroughput serving of many LoRA adapters simultaneously.\nFor a single adapter, the method recommended by (Hu et al.,\n2021) is to merge the adapter weights into the base model\nW\nx1\nx2\nx3\nx\nA\n1\nB1\nx1\nx\nx\nx2\nA\n2\nB2\nx\nx\nx3\nA\n3\nB3\nx\nx\nx\nBatched base computation\nBatched LoRA computation\nadd\nFigure 1. Separated batched computation for the base model and\nLoRA computation. The batched computation of the base model\nis implemented by GEMM. The batched computation for LoRA\nadapters is implemented by custom CUDA kernels which support\nbatching various sequence lengths and adapter ranks.\nMain Memory\nAdapter 1\nAdapter 2\nAdapter 3\nAdapter 4\nAdapter 5\nAdapter 6\n\u2026\nGPU Memory\nKV cache\nUnified memory pool\nfor dynamic tensors\nBase\nModel\nWeights\nOther\nTemporary\nTensors\nFetch active \nadapters for\nthe current batch\nAdapter 2\nAdapter 5\nFigure 2. Overview of memory allocation in S-LoRA. S-LoRA\nstores all adapters in the main memory and fetches the active\nadapters for the current batch to the GPU memory. The GPU\nmemory is used to store the KV cache, adapter weights, base\nmodel weights, and other temporary tensors.\nweights, resulting in a new model (see Eq. 1). This has the\nadvantage that there is no additional adapter overhead during\ninference, since the new model has the same number of\nparameters as the base model. In fact, this was a prominent\nfeature of the original LoRA work.\nHowever, when there are multiple adapters, merging the\nweights into the base model leads to multiple weight copies\nand missed batching opportunities. Directly merging the\nmodels requires maintaining many copies of the full lan-\nguage model. In the original LoRA paper, the authors pro-\nposed adding and subtracting LoRA weights on the fly to\nenable serving multiple models without increasing the mem-\nory overhead. However, this approach doesn\u2019t support con-\ncurrent inference on separate LoRA adapters and therefore\nlimits batching opportunities.\nIn this paper, we show that merging LoRA adapters into\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\nthe base model is inefficient for the multi-LoRA high-\nthroughput serving setting. Instead, we propose computing\nthe LoRA computation xAB on-the-fly as shown in Eq. 2.\nThis avoids weight duplication and enables batching of the\nmore costly xW operation. But this approach also increases\nthe computation overhead. However, because the cost of\nxAB is substantially lower than xW and there is a consid-\nerable savings from batching xW across different adapters,\nwe show that the savings far exceed the additional overhead.\nUnfortunately, directly implementing the factored computa-\ntion of the base model and individual LoRA adapters using\nthe batch GEMM kernel from the existing BLAS libraries\nwould require significant padding and result in poor hard-\nware utilization. This is because of the heterogeneity of\nsequence lengths and adapter ranks.\nIn S-LoRA, we batch the computation of the base model and\nthen employ custom CUDA kernels to execute the additional\nxAB for all adapters separately. This process is illustrated\nby Figure 1. Instead of naively using padding and using the\nbatch GEMM kernel from the BLAS library for the LoRA\ncomputation, we implement custom CUDA kernels for more\nefficient computation without padding. In Subsection 5.3,\nwe discuss the implementation details.\nWhile the number of LoRA adapters can be large if we\nstore them in main memory, the number of LoRA adapters\nneeded for the currently running batch is manageable, be-\ncause the batch size is bounded by the GPU memory. To\ntake advantage of this, we store all LoRA adapters in the\nmain memory and fetch only the LoRA adapters needed\nfor the currently running batch to the GPU RAM when\nrunning the inference for that batch. In this case, the max-\nimum number of adapters that can be served is bounded\nby the main memory size. This process is illustrated by\nFigure 2. To achieve high-throughput serving, we adopt the\niteration-level scheduling batching strategy from Orca (Yu\net al., 2022). In this approach, requests are scheduled at the\ntoken level. We immediately incorporate a new request into\nthe running batch if space is available. The request will exit\nthe batch once it reaches the maximum number of gener-\nated tokens or fulfills other stopping criteria. This process\nreduces GPU memory usage but introduces new memory\nmanagement challenges. In Section 5, we will discuss our\ntechniques to manage memory efficiently.\n4.2\nAdapter Clustering\nTo enhance batching efficiency, one potential strategy is\nreducing the number of active adapters in a running batch.\nBy using fewer adapters, there is an opportunity to allocate\nmore memory to the KV cache, which in turn can facili-\ntate larger batch sizes. Given the common memory capaci-\nties of GPUs, they are often underutilized while decoding.\nConsequently, increasing the batch size can lead to higher\nthroughput. A direct approach to reducing the number of\nadapters in a running batch is to prioritize batching requests\nthat use the same adapter, a strategy we term \u201cadapter clus-\ntering\u201d. However, adapter clustering comes with its own set\nof trade-offs. For example, it can hurt the average latency\nor fairness among adapters. We provide an ablation study in\nAppendix A to illustrate how throughput and latency change\naccording to the cluster size.\n4.3\nAdmission Control\nIn S-LoRA, we also applied an admission control strategy\nto sustain good attainment when the traffic is higher than the\nserving system capacity. A serving system is typically char-\nacterized by a service level objective (SLO) which specifies\nthe desired latency of processing requests. If the serving\nsystem has fixed capacity, it must implement an admission\ncontrol mechanism, that drops a request, if the system can-\nnot meet its SLO. Otherwise, if no request is dropped, and\nthe number of incoming requests is larger than the system\ncapacity for long enough, the serving system is bound to\nviolate the SLO. We implemented an abort strategy to mimic\nadmission control in S-LoRA, called early abort strategy.\nIntuitively, we estimate the set of latest requests that we\ncan serve in SLO, and then serve them in the order of ar-\nrival time. More implementation details and mathematical\njustifications are deferred to Appendix B.\n5\nMEMORY MANAGEMENT\nCompared to serving a single base model, serving multiple\nLoRA adapters simultaneously presents new memory man-\nagement challenges. To support many adapters, S-LoRA\nstores them in the main memory and dynamically loads the\nadapter weights needed for the currently running batch into\nGPU RAM. During this process, there are two noticeable\nchallenges. The first is memory fragmentation, resulting\nfrom the dynamic loading and offloading adapter weights of\nvarious sizes. The second is the latency overhead introduced\nby adapter loading and offloading. To tackle these chal-\nlenges efficiently, we propose Unfied Paging and overlap\nthe I/O with computation by prefetching adapter weights.\n5.1\nUnified Paging\nUnderstanding the nature of adapter weights is essential for\noptimizing memory usage. Our primary observation is that\nthese dynamic adapter weights are analogous to dynamic\nKV caches in several ways:\n\u2022 Variable sizes and operations: Just as the size of\nKV cache size fluctuates with the sequence length, the\nranks of the active adapters can also depend on the\nchoice of adapter associated with each request. KV\ncaches are allocated when requests arrive and deal-\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\nKV caches\nAdapter weights\nEmpty\nH\nFigure 3. Unified memory pool. We use a unified memory pool to\nstore both KV caches and adapter weights in a non-contiguous way\nto reduce memory fragmentation. The page size is H elements.\nlocated once the requests are completed. Similarly,\nadapter weights are loaded and cleared with each re-\nquest. If not managed properly, this variability can\nresult in fragmentation.\n\u2022 Dimensionality: A KV cache tensor for a request in\na layer has a shape of (S, H), where S denotes the se-\nquence length and H represents the hidden dimension.\nMeanwhile, the shape of a LoRA weight is (R, H),\nwith R standing for the rank and H the hidden dimen-\nsion. Both share a dimension size of H that can be\nleveraged to reduce fragmentation.\nMotivated by these parallels, we extend the idea of Page-\ndAttention (Kwon et al., 2023) to Unified Paging which\nmanages adapter weights in addition to the KV cache. Uni-\nfied Paging uses a unified memory pool to jointly manage\nboth KV cache and adapter weights. To implement this, we\nfirst allocate a large buffer statically for the memory pool.\nThis buffer uses all available space except for the space oc-\ncupied by the base model weights and temporary activation\ntensors. Both KV caches and adapter weights are stored in\nthis memory pool in a paged manner, with each page corre-\nsponding to a vector of H. Thus, a KV cache tensor with a\nsequence length of S uses up S pages, while a LoRA weight\ntensor of rank R takes up R pages. Figure 3 illustrates the\nlayout of our memory pool, where KV caches and adapter\nweights are stored interleaved and non-contiguously. This\napproach significantly reduces fragmentation, ensuring that\nadapters weights of various ranks can coexist with dynamic\nKV caches in a structured and systematic manner.\n5.2\nPrefetching and Overlapping\nAlthough the unified memory pool mitigates fragmentation,\nthe I/O overhead from loading and offloading remains a\nconcern\u2014especially when dealing with numerous or large\nadapters. The latency introduced by waiting to load these\nadapters can compromise the efficiency of the system.\nTo proactively address this issue, we introduce a dynamic\nprediction mechanism. While running the current decoding\nbatch, we predict the adapters required for the next batch\nbased on the current waiting queue. This prediction allows\nus to prefetch and store them in available memory. Such a\nforward-looking strategy keeps most of the adapters needed\nfor the next batch already in place before running it, which\nreduces I/O time for adapter swapping.\n5.3\nCustom Kernels for heterogeneous LoRA batching\non Non-Contiguous Memory\nDue to the design of the unified memory pool, the adapter\nweights are stored in non-contiguous memory. To run com-\nputations efficiently under this design, we implement cus-\ntom CUDA kernels that support batching LoRA compu-\ntations with varying ranks and sequence lengths in a non-\ncontiguous memory layout. In the prefill stage, the kernel\nhandles a sequence of tokens and gathers adapter weights\nwith different ranks from the memory pool. We call this\nkernel Multi-size Batched Gather Matrix-Matrix Multipli-\ncation (MBGMM). It is implemented in Triton (Tillet et al.,\n2019) with tiling. In the decode stage, the kernel handles\na single token and gathers adapter weights with different\nranks from the memory pool. We call this kernel Multi-size\nBatched Gather Matrix-Vector Multiplication (MBGMV).\nIt is modified from Punica (Chen, 2023) to support multiple\nranks in a batch and more fine-grained memory gathering.\n6\nTENSOR PARALLELISM\nWe design novel tensor parallelism strategies for batched\nLoRA inference to support multi-GPU inference of large\ntransformer models.\nTensor parallelism is the most\nwidely used parallelism method because its single-program\nmultiple-data pattern simplifies its implementation and inte-\ngration with existing systems. Tensor parallelism can reduce\nthe per-GPU memory usage and latency when serving large\nmodels. In our setting, the additional LoRA adapters intro-\nduce new weight matrices and matrix multiplications, which\ncalls for new partition strategies for these added items.\n6.1\nPartition Strategy\nSince the base model uses the Megatron-LM tensor paral-\nlelism strategy (Shoeybi et al., 2019), our approach aims\nto align the partition strategies of inputs and outputs of the\nadded LoRA computation with those of the base model.\nIn this way, we can minimize the communication costs\nby avoiding unnecessary communications and fusing some\ncommunications.\nWe use the feed-forward module (2-layer MLP) to illus-\ntrate our partition strategy. We will explain later how this\nstrategy can easily be adapted to the self-attention layer.\nAs depicted in Figure 4, the upper box illustrates the base\nmodel\u2019s Megatron-LM partition strategy: the first weight\nmatrix (W1) is column-partitioned, and the second (W2) is\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\ninput X\nmatmul_1\nmatmul_2\nW1\nW2\nB1\nadd_1\nmatmul_3\nB2\nmatmul_4\nall-reduce\nadd_2\nmatmul all-gather\nmatmul all-reduce\nA1\nA2\nColumn Partition\nRow Partition\nPartial Sum\nReplication\n(B, h)\n(h, r/N)\n(B, r/N)\n(B, r)\n(r, d/N)\n(B, d/N)\n(h, d/N)\n(B, d/N)\n(B, d/N)\n(d/N, r)\n(B, r)\n(B, r)\n(r, h/N)\n(d/N, h)\n(B, h)\n(B, h/N)\n(B, h)\n(B, h)\nFuse all-gather\nand all-reduce\nFigure 4. Tensor parallelism partition strategy for batched LoRA computation. This is a computational graph where nodes represent\ntensors/operators and the edges represent dependency. We use different colors to represent different partition strategies, which include\ncolumn partition, row partition, partial sum, and replication. The per-GPU shape of each tensor is also annotated in gray. Note that B is\nthe number of tokens, h is the input dimension, N is the number of devices, d is the hidden size, and r is the adapter rank.\nrow-partitioned. An all-reduce communication is required\nto accumulate the partial sum from distributed devices.\nThe lower box illustrates the partitioning strategy for the\nadded LoRA computation. The matrices A1 and B1 for\nthe adapter of the first weight matrix (W1) are column-\npartitioned. An all-gather operation is used to collect the in-\ntermediate results. The matrices A2 and B2 for the adapter\nof the second weight (W2) are row-partitioned and column-\npartitioned, respectively. An all-reduce operation is used\nto sum up the intermediate results. Finally, the result from\nthe LoRA computation is added to that from the base model\n(add 2). A single all-reduce operation is sufficient to ac-\ncumulate the final results. It is worth noting that we are\nessentially fusing an all-gather operation for matmul 4\nwith the final all-reduce. To our knowledge, this paralleliza-\ntion strategy has not been studied before.\nNext, we discuss adapting the strategy from the 2-layer MLP\nto the self-attention layer. Similar to the Megatron-LM strat-\negy, we partition the head dimension of the self-attention\nlayer. The query-key-value projection weight matrix can be\nseen as W1 in our example and the output projection weight\nmatrix can be seen as W2 in our example.\n6.2\nCommunication and Memory Cost Analysis\nLet N be the number of devices, B be the number of to-\nkens, h be the hidden size, and r be the adapter rank. The\ncommunication cost of the base model is one all-reduce, or\n2(N\u22121)Bh\nN\n. The communication cost of the added LoRA\ncomputation is three all-gather for query, key, and value\nprojections, and one all-reduce for the output projection.\nFormally, it is 3 (N\u22121)Br\nN\n+ 2(N\u22121)Br\nN\n= 5(N\u22121)Br\nN\n.\nUnder our strategy, the additional communication cost intro-\nduced by LoRA is negligible when compared to the commu-\nnication cost of the base model, because r \u226a h. Intuitively,\nthis is achieved by carefully scheduling communications on\nthe small intermediate tensors of LoRA computation and\nfusing communications with base models.\nIn terms of memory usage, our strategy is optimal because\nwe partition all weight matrices among all devices and there\nis no replicated weight matrix.\n7\nEVALUATION\nWe evaluate the performance of S-LoRA on both synthetic\nand real production workloads. S-LoRA is built on top of\nLightLLM (ModelTC, 2023), a single-model LLM serv-\ning system based on PyTorch (Tillet et al., 2019) and Tri-\nton (Tillet et al., 2019). We evaluate the scalability of S-\nLoRA by serving up to two thousand LoRA adapters simul-\ntaneously and compare it with other strong baselines. We\nthen perform ablation studies to verify the effectiveness of\nindividual components.\n7.1\nSetup\nModel. We test the Llama model series (Touvron et al.,\n2023a;b), one of the most popular open large language mod-\nels. We consider 5 different model and adapter configura-\ntions, which are listed in Table 1 1. Our optimizations can\n1For Llama-70B, we used different architecture parameters\nthan the official model and did not employ group-query attention.\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\nbe easily adapted to other transformer-based architectures as\nwell, such as GPT-3 (Brown et al., 2020) and PaLM (Chowd-\nhery et al., 2022; Anil et al., 2023).\nSetting\nBase model\nHidden size\nAdapter ranks\nS1\nLlama-7B\n4096\n{8}\nS2\nLlama-7B\n4096\n{64, 32, 16, 8}\nS4\nLlama-13B\n5120\n{64, 32, 16}\nS5\nLlama-30B\n7168\n{32}\nS6\nLlama-70B\n8192\n{64}\nTable 1. Model and adapter configurations.\nHardware. We conduct tests on various hardware settings,\nincluding a single NVIDIA A10G GPU (24GB), a single\nA100 GPU (40GB), a single A100 GPU (80GB), and mul-\ntiple A100 GPUs (40GB/80GB). The host\u2019s main memory\nvaries based on the GPU setup, ranging from 64 GB to\n670 GB. We will show that S-LoRA can efficiently scale\nthe number of adapters, limited only by the available main\nmemory.\nBaselines. We benchmark several variants of S-LoRA, Hug-\ngingFace PEFT (Mangrulkar et al., 2022), and vLLM (Kwon\net al., 2023).\n\u2022 \u201cHuggingFace PEFT\u201d is a library for training and run-\nning parameter-efficient fine-tuning models. It lacks ad-\nvanced batching and memory management. We build a\nserver using it that batches single adapter requests and\nswitches adapter weights between batches.\n\u2022 \u201cvLLM m-packed\u201d is a simple multi-model serving\nsolution based on vLLM, a high-throughput serving\nsystem. Because vLLM does not support LoRA, we\nmerge the LoRA weights into the base model and serve\nthe multiple versions of the merged weights separately.\nTo serve m LoRA adapters, we run m vLLM workers\non a single GPU, where multiple workers are separate\nprocesses managed by NVIDIA MPS. We statistically\nallocate the GPU memory proportionally to the average\nrequest rate for each process.\n\u2022 \u201cS-LoRA\u201d is S-LoRA with all the optimizations and it\nis using the first-come-first-serve scheduling strategy.\n\u2022 \u201cS-LoRA-no-unify-mem\u201d is S-LoRA without Unified\nPaging.\n\u2022 \u201cS-LoRA-bmm\u201d is S-LoRA without Unified Paging\nand customized kernels. It copies the adapter weights\nto continuous memory space and performs batched\nmatrix multiplication with padding.\nMetrics. There are several metrics to measure the perfor-\nmance of serving systems, including latency and throughput.\nFollowing common practice, we report the throughput, av-\nerage request latency, average first token latency, and SLO\nattainment. SLO attainment is defined as the percentage\nof requests that return the first token in 6 seconds. Addi-\ntionally, we introduce a new metric termed user satisfaction\n(see Appendix B), which offers a more fine-grained analysis\nof the first token latency. Intuitively, a shorter first token\nlatency gives a higher satisfaction. The satisfaction becomes\n0 if the first token latency exceeds the SLO.\n7.2\nEnd-to-End Results on Synthetic Workloads\nWorkload trace. We generate synthetic workload traces\nusing the Gamma process. Given n adapters, the requests for\nadapter i are modeled using a Gamma arrival process with\na mean rate of \u03bbi and a coefficient of variance (CV) of cv.\nThe mean rate, \u03bbi, adheres to a power-law distribution with\nan exponent \u03b1. The total request rate for all adapters is R\nrequests per second. For the n adapters, we set their ranks\nbased on the list provided in Table 1 with a round-robin\nmethod. Our tests cover various combinations of n, \u03b1, R,\nand cv. For every request, the input and output lengths are\nsampled from uniform distributions U[Il, Iu] and U[Ol, Ou]\nrespectively. The default duration of a trace is 5 minutes. To\nconduct comprehensive experiments, we first pick a set of\ndefault parameters for generating workloads, as shown in\nTable 2. We then vary one of the n, \u03b1, R, and cv to see how\neach factor affects the performance.\nTable 2. Default parameters for generating the synthetic workloads.\n\u201c7B @ A10G\u201d means running a Llama-7B on a single A10G.\nSetting\nn\n\u03b1\nR\ncv\n[Il, Iu]\n[Ol, Ou]\n7B @ A10G (24G)\n200\n1\n2\n1\n[8, 512]\n[8, 512]\n7B @ A100 (80G)\n200\n1\n10\n1\n[8, 512]\n[8, 512]\n13B @ A100 (40G)\n200\n1\n2\n1\n[8, 512]\n[8, 512]\n13B @ A100 (80G)\n400\n1\n6\n1\n[8, 512]\n[8, 512]\nTable 3. Throughput (req/s) comparison between S-LoRA, vLLM-\npacked, and PEFT. The hardware is a single A100 (80GB). We run\nPEFT for a shorter duration when n = 100. We do not evaluate\nPEFT for n \u2265 1000, as its throughput is already very low for a\nsmall n. \u201cOOM\u201d denotes out-of-memory.\nModel Setup\nn\nS-LoRA\nvLLM-packed\nPEFT\nS1\n5\n8.05\n2.04\n0.88\n100\n7.99\nOOM\n0.25\n1000\n7.64\nOOM\n-\n2000\n7.61\nOOM\n-\nS2\n5\n7.48\n2.04\n0.74\n100\n7.29\nOOM\n0.24\n1000\n6.69\nOOM\n-\n2000\n6.71\nOOM\n-\nS4\n2\n4.49\n3.83\n0.54\n100\n4.28\nOOM\n0.13\n1000\n3.96\nOOM\n-\nComparison with other systems. We compare S-LoRA\nwith both vLLM-packed and HuggingFace PEFT for serv-\ning many LoRA adapters. The results are shown in Table 3.\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\n0\n100\n200\n0.0\n0.5\n1.0\n1.5\n0\n100\n200\n0.0\n0.5\n1.0\n1.5\n0\n100\n200\n0.0\n0.5\n1.0\n1.5\n0\n100\n200\n0.0\n2.0\n4.0\n6.0\n8.0\n0\n200\n400\n0.0\n1.0\n2.0\n3.0\n4.0\n0\n100\n200\n0.0\n20.0\n40.0\n60.0\n0\n100\n200\n0.0\n25.0\n50.0\n75.0\n100.0\n0\n100\n200\n0.0\n50.0\n100.0\n150.0\n0\n100\n200\n0.0\n50.0\n100.0\n150.0\n200.0\n0\n200\n400\n0.0\n100.0\n200.0\nS1 (Llama-7b)\nA10G (24GB)\nS2 (Llama-7b)\nA10G (24GB)\nS4 (Llama-13b)\nA100 (40GB)\nS2 (Llama-7b)\nA100 (80GB)\nS4 (Llama-13b)\nA100 (80GB)\nThroughput (req/s)\nNumber of Adapters\nNumber of Adapters\nNumber of Adapters\nNumber of Adapters\nNumber of Adapters\nAverage Latency (s)\nS-LoRA\nS-LoRA-bmm\nS-LoRA-no-unify-mem\nFigure 5. The throughput and average request latency of S-LoRA and its variants under different numbers of adapters. S-LoRA achieves\nsignificantly better performance and can scale to a large number of adapters. We run S-LoRA-bmm for a shorter duration since it has a\nsignificantly lower throughput. Some S-LoRA-bmm curves are omitted because it is out of the figures\u2019s scope.\nRemarkably, S-LoRA can serve 2,000 adapters simultane-\nously, maintaining minimal overhead for the added LoRA\ncomputation. In contrast, vLLM-packed needs to maintain\nmultiple weight copies and can only serve fewer than 5\nadapters due to the GPU memory constraint. The through-\nput of vLLM-packed is also much lower due to the missed\nbatching opportunity. Although PEFT can swap adapters\nbetween batches, enabling it to handle a large number of\nadapters, its lack of advanced batching methods and mem-\nory management results in significantly worse performance.\nOverall, S-LoRA achieves a throughput up to 4x higher than\nvLLM-packed when serving a small number of adapters,\nand up to 30x higher than PEFT, while supporting a signifi-\ncantly larger number of adapters.\nComparing with own variants. Since no baseline system\ncan efficiently scale to a large number of adapters, we now\nfocus on comparing S-LoRA with its own variants. Fig-\nure 5 illustrates how they scale with the number of adapters.\nS-LoRA achieves noticeably higher throughput and lower\nlatency compared to S-LoRA-bmm and S-LoRA-no-unify-\nmem. This implies that our memory pool and custom ker-\nnels are effective. When the number of adapters increases,\nthe throughput of S-LoRA initially experiences a slight de-\ncline due to the overhead introduced by LoRA. However,\nonce the number of adapters reaches a certain threshold\n(e.g., 100 in most experiments), the throughput of S-LoRA\nno longer decreases. This stability can be attributed to the\nfact that as the number of adapters grows, the number of\nactivated adapters for the currently running batch remains\nunchanged, maintaining a constant overhead. Consequently,\nS-LoRA can scale to a much larger number of adapters with-\nout incurring additional overhead, constrained only by the\navailable main memory.\nFigure 6 demonstrates the variation in throughput, first token\nlatency, and SLO attainment relative to the total request\nrate, revealing a pattern consistent with the aforementioned\nobservations and underscoring the efficacy of our design.\n1.0\n1.5\n2.0\n2.5\n0.0\n0.5\n1.0\n1.5\n2\n4\n6\n8\n0.0\n2.0\n4.0\n1.0\n1.5\n2.0\n2.5\n0.0\n100.0\n200.0\n2\n4\n6\n8\n0.0\n200.0\n400.0\n1.0\n1.5\n2.0\n2.5\n0.0\n0.5\n1.0\n2\n4\n6\n8\n0.0\n0.5\n1.0\nS2 (Llama-7b) A10G (24GB)\nS4 (Llama-13b) A100 (80GB)\nThroughput (req/s)\nFirst Token Latency (s)\nRequest Rate\nRequest Rate\nSLO Attainment\nS-LoRA\nS-LoRA-bmm\nS-LoRA-no-unify-mem\nFigure 6. The throughput, first token latency, and SLO attainment\nof S-LoRA and its variants under different request rates. Note that\nin both settings the first token latency of S-LoRA-bmm is out of\nthe figure\u2019s scope.\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\n1\n2\n3\n4\n0.0\n1.0\n2.0\n3.0\n1\n2\n3\n4\n0.0\n0.2\n0.5\n0.8\n1.0\nRequest Rate\nThroughput (req/s)\nRequest Rate\nSLO Attainment\nS-LoRA\nS-LoRA-bmm\nS-LoRA-no-unify-mem\nFigure 7. The throughput of S-LoRA and its variants on real work-\nload traces with different request rates. The model and hardware\nconfiguration is S2 on an A10G (24GB).\n7.3\nEnd-to-End Results on Real Workloads\nReal workload trace. We construct real-world serving\ntraces by downsampling from the traces of LMSYS Chatbot\nArena (Zheng et al., 2023b;a), a website that serves multiple\nLLMs. The raw log from Arena does not concern LoRA\nadapters; it focuses on different base models. Nonetheless,\nwe treat the distribution of different base models as if they\nwere the distribution of different adapters of a single base\nmodel. The raw log can be sampled into traces that exhibit\nvarying request rates, denoted as R, and durations, repre-\nsented by D. To achieve this, we sample R \u00b7 D requests\nfrom the raw log and rescale the time stamps to fit within\nthe range of [0, D]. The number of models n corresponds\nto the number of adapters. Furthermore, we set the adapter\nranks based on Table 1 with a round-robin method.\nSince we are using a real workload trace, there are no pa-\nrameters such as \u03b1, \u03bbi, or cv. For consistency, we set the\nduration to 5 minutes. We adjust the request rate R to study\nits impact on performance metrics. In the sampled trace, the\naverage input length is 85 tokens, the average output length\nis 165 tokens, and the number of adapters is around 26.\nResults. Figure 7 shows the throughput and attainment\nresults, which show a similar pattern to the synthetic work-\nloads. This means the strong performance S-LoRA holds\nfor real world workloads.\n7.4\nMulti-GPU Tensor Parallelism\nWe test the scalability of our tensor parallelism strategy by\nrunning 1) Llama-30B on two A100 (40GB) and four A100\n(40GB) GPUs with 10 to 100 adapters; and 2) Llama-70B\non two A100 (80GB) and four A100 (80GB) GPUs with 10\nadapters. We then report the serving throughputs.\nAs depicted in Figure 8, the disparity between S-LoRA\nwith and without LoRA communication is small. This sug-\ngests that the added LoRA communication in our strategy\nhas a very small overhead. The figure further reveals that\nthe communication overhead due to LoRA is less than the\ncomputational overhead it introduces. Furthermore, when\ntransitioning from 2 GPUs to 4 GPUs, the serving through-\nput increases by more than 2 times. This significant increase\ncan be attributed to the fact that the system is predominantly\nmemory-bound in this context. Adding more GPUs alle-\nviates memory constraints, leading to superlinear scaling.\nIn conclusion, the results verify both the minimal overhead\nand the scalability of our tensor parallelism strategy.\nLlama-30B\n2xA100(40GB)\nn=10\nLlama-30B\n2xA100(40GB)\nn=10\n0\n1\n2\n3\n4\n5\nThroughput (req/s)\nS-LoRA\nS-LoRA (w/o LoRA communication)\nS-LoRA (base only)\nLlama-30B\n2xA100(40GB)\nn=100\nLlama-30B\n4xA100(40GB)\nn=100\n0\n1\n2\n3\n4\n5\nLlama-70B\n2xA100(80GB)\nn=10\nLlama-70B\n4xA100(80GB)\nn=10\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nFigure 8. Throughput with tensor parallelism.\n7.5\nAblation Study\nMerging adapter weights versus computing on-the-fly.\nWhile S-LoRA does not merge adapter weights and com-\nputes LoRA matrices on-the-fly each time, we compare it\nwith an alternative design that merges an adapter with the\nbase model, denoted as x(W + AB), as proposed in the\nLoRA paper. This approach involves: 1) Updating the base\nmodel with the current adapter weights before each new\nbatch; and 2) Switching to a new adapter if there are too\nmany waiting requests.2 This method is efficient for a small\nnumber of adapters due to the reduced LoRA computation\noverhead.\nResults in Figure 9 demonstrate that with just one adapter,\nthe merging approach outperforms the on-the-fly computa-\ntion owing to a one-time merging cost. However, its per-\nformance declines with more than 2 adapters, primarily\nbecause of the time-consuming switch between adapters.\nSuch switching results in periods of GPU under-utilization.\nFurthermore, a smaller value of \u03b1 causes requests to be\ndistributed unevenly across adapters, which in turn reduces\nbatch sizes and overall performance.\nEarly abort strategy experiments.\nWe compared S-\nLoRA\u2019s early abort strategy to First Come First Serve\n(FCFS) and Last Come First Serve (LCFS) for optimiz-\ning user satisfaction and SLO attainment. As shown in\nFigure 10, S-LoRA-Abort outperforms both, especially as\ncv scales. FCFS is least effective, often processing requests\nthat have already missed the SLO. LCFS, similar to a greedy\nalgorithm that only prioritizes the newest requests, works\nwell for small cv, but its performance drops with larger\n2This is different from PEFT. For example, it has continuous\nbatching and PagedAttention, which are not enabled in PEFT.\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\n1\n2\n3\n4\n5\nnumber of adapters\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\nthroughput (req/s)\nS-LoRA-merge alpha=0.1\nS-LoRA alpha=0.1\nS-LoRA-merge alpha=1\nS-LoRA alpha=1\nFigure 9. Ablation study comparing adapter merging and on-the-\nfly compute for S2 on A10G (24GB) with different \u03b1 and number\nof adapters. The settings for the synthetic workloads are R =\n2, cv = 1, [It, Iu] = [8, 512], [Ol, Ou] = [8, 512].\n2\n4\n6\n8\n0.0\n0.2\n0.5\n0.8\n2\n4\n6\n8\n0.0\n0.2\n0.4\n0.6\n2\n4\n6\n8\n0.0\n0.2\n0.4\n0.6\n2\n4\n6\n8\n0.0\n0.2\n0.4\n0.6\nS2 (Llama-7b)\nS4 (Llama-13b)\nSLO Attainment\nCV Scale\nCV Scale\nUser Satisfaction\nS-LoRA-FCFS\nS-LoRA-LCFS\nS-LoRA-Abort\nFigure 10. Ablation study for early abort scheduling strategy on\nA10G-24G (S1) and A100-80G (S4). Other settings follow the\ndescription in Table 2.\ncv. S-LoRA-Abort excels as it avoids prioritizing only the\nnewest requests, as detailed in Appendix B.\n8\nRELATED WORK\nOptimize LLM serving with system techniques. The sig-\nnificance of the transformer architecture has led to the devel-\nopment of many specialized serving systems for it. These\nsystems use advanced batching mechanisms (Fang et al.,\n2021; Yu et al., 2022), memory optimizations (Sheng et al.,\n2023; Kwon et al., 2023), GPU kernel optimizations (Wang\net al., 2021; Aminabadi et al., 2022; NVIDIA, 2023; Dao,\n2023), model parallelism (Pope et al., 2022; Aminabadi\net al., 2022), parameter sharing (Zhou et al., 2022), and\nspeculative execution (Stern et al., 2018; Miao et al., 2023)\nfor efficient serving. Among them, PetS (Zhou et al., 2022)\nis most relevant to ours. However, PetS only considers\nthe serving for small encoder-only BERT models. It does\nnot consider generative inference, a very large number of\nadapters or large models go beyond a single GPU, so it does\nnot address the problems in our settings.\nIn a concurrent work (Chen et al., 2023), the concept of\ndecomposed computation for the base model and adapters\nwas explored. Our CUDA kernels were developed based\non the implementation presented in a prior blog post of this\nstudy, with additional support for batching different ranks\nand non-contiguous memory. Moreover, our novel memory\nmanagement and tensor parallelism techniques have not\nbeen covered in any previous work.\nOptimize LLM serving with algorithm techniques. In\naddition to system-level improvements, inference efficiency\ncan be enhanced using algorithm techniques like quanti-\nzation (Yao et al., 2022; Dettmers et al., 2022; Frantar\net al., 2022; Xiao et al., 2023; Lin et al., 2023), sparsifi-\ncation (Frantar & Alistarh, 2023; Zhang et al., 2023b) and\nmodel architecture improvements (Shazeer, 2019). These\napproaches can reduce memory consumption and accelerate\nthe computation, with a minor compromise in model quality.\nThey are complementary to the techniques in this paper.\nParameter-efficient fine-tuning. Recent work has devel-\noped methods for parameter-efficient fine-tuning of large\npre-trained language models. These methods show fine-\ntuning is possible with only a small fraction of tuned param-\neters. The state-of-the-art methods include LoRA (Hu et al.,\n2021), Prefix-tuning (Li & Liang, 2021), P-Tuning (Liu\net al., 2021), Prompt tuning (Liu et al., 2023; Lester et al.,\n2021), AdaLoRA (Zhang et al., 2022), and (IA)3 (Liu et al.,\n2022). While our paper focuses on LoRA due to its wide\nadoption, most techniques can be easily applied to other\nparameter-efficient fine-tuning methods as well.\nGeneral purpose model serving systems. Over the years,\nthe domain of general model serving has seen significant\nadvancements, Notable systems from earlier research in-\nclude Clipper (Crankshaw et al., 2017), TensorFlow Serv-\ning (Olston et al., 2017), Nexus (Shen et al., 2019), Infer-\nLine (Crankshaw et al., 2020), and Clockwork (Gujarati\net al., 2020). These systems delve into topics such as batch-\ning, caching, and model placement, catering to both individ-\nual and multiple model deployments. In more recent devel-\nopments, DVABatch (Cui et al., 2022), REEF (Han et al.,\n2022), Shepherd (Zhang et al., 2023a) and AlpaServe (Li\net al., 2023) have explored the ideas of multi-entry multi-\nexit batching, preemption, and statistical multiplexing with\nmodel parallelism. Although these systems have made sig-\nnificant contributions, they overlook the auto-regressive\ncharacteristics and parameter-efficient adapters in LLM serv-\ning, leading to potential optimization gaps.\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\n9\nCONCLUSION\nWe present S-LoRA, a system capable of serving thou-\nsands of LoRA adapters from a single machine with much\nhigher throughput compared to existing systems. S-LoRA\nis made possible by our innovative design of the unified\nmemory pool, tensor parallelism strategy, adapter batch-\ning, and CUDA kernels. S-LoRA enables large-scale, cus-\ntomized fine-tuning services essential for deploying models\ntailored to diverse requirements. Future extensions of S-\nLoRA will encompass support for additional adapter meth-\nods, enhanced fused kernels, and the use of multiple CUDA\nstreams to parallelize base model and LoRA computations.\n10\nACKNOWLEDGMENT\nThis research was supported by gifts from Anyscale, As-\ntronomer, Google, IBM, Intel, Lacework, Microsoft, Mo-\nhamed Bin Zayed University of Artificial Intelligence, Sam-\nsung SDS, Uber, and VMware. Ying is partly supported by\nthe Stanford Center for Automated Reasoning. We thank\nClark Barrett for academic advising and funding support.\nWe also thank Yonghao Zhuang and Lisa Dunlap for their\nhelpful discussions and feedback.\nREFERENCES\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., et al. Flamingo: a visual language model for few-shot\nlearning. Advances in Neural Information Processing\nSystems, 35:23716\u201323736, 2022.\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li,\nD., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley,\nJ., and He, Y. Deepspeed- inference: Enabling efficient\ninference of transformer models at unprecedented scale.\nIn Wolf, F., Shende, S., Culhane, C., Alam, S. R., and\nJagode, H. (eds.), SC22: International Conference for\nHigh Performance Computing, Networking, Storage and\nAnalysis. IEEE, 2022.\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\nD., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\nZ., et al.\nPalm 2 technical report.\narXiv preprint\narXiv:2305.10403, 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nChen, L. Potentials of multitenancy fine-tuned llm serv-\ning. https://le.qun.ch/en/blog/2023/09/\n11/multi-lora-potentials/, 2023.\nChen, L., Ye, Z., Wu, Y., Zhuo, D., Ceze, L., and Krishna-\nmurthy, A. Punica: Multi-tenant lora serving, 2023.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nCrankshaw, D., Wang, X., Zhou, G., Franklin, M. J., Gon-\nzalez, J. E., and Stoica, I. Clipper: A low-latency online\nprediction serving system. In 14th USENIX Symposium\non Networked Systems Design and Implementation (NSDI\n17), pp. 613\u2013627, 2017.\nCrankshaw, D., Sela, G.-E., Mo, X., Zumar, C., Stoica, I.,\nGonzalez, J., and Tumanov, A. Inferline: latency-aware\nprovisioning and scaling for prediction serving pipelines.\nIn Proceedings of the 11th ACM Symposium on Cloud\nComputing, pp. 477\u2013491, 2020.\nCui, W., Zhao, H., Chen, Q., Wei, H., Li, Z., Zeng, D.,\nLi, C., and Guo, M. Dvabatch: Diversity-aware multi-\nentry multi-exit batching for efficient processing of dnn\nservices on gpus. In 2022 USENIX Annual Technical\nConference (USENIX ATC 22), pp. 183\u2013198, 2022.\nDao, T.\nFlashattention-2:\nFaster attention with bet-\nter parallelism and work partitioning. arXiv preprint\narXiv:2307.08691, 2023.\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.\nLlm.int8(): 8-bit matrix multiplication for transformers at\nscale. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho,\nK. (eds.), Advances in Neural Information Processing\nSystems, 2022.\nFang, J., Yu, Y., Zhao, C., and Zhou, J. Turbotransformers:\nan efficient gpu serving system for transformer models.\nIn Proceedings of the 26th ACM SIGPLAN Symposium\non Principles and Practice of Parallel Programming, pp.\n389\u2013402, 2021.\nFedus, W., Zoph, B., and Shazeer, N. Switch transform-\ners: Scaling to trillion parameter models with simple\nand efficient sparsity. The Journal of Machine Learning\nResearch, 23(1):5232\u20135270, 2022.\nFrantar, E. and Alistarh, D.\nMassive language models\ncan be accurately pruned in one-shot. arXiv preprint\narXiv:2301.00774, 2023.\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:\nAccurate post-training quantization for generative pre-\ntrained transformers. arXiv preprint arXiv:2210.17323,\n2022.\nGujarati, A., Karimi, R., Alzayat, S., Hao, W., Kaufmann,\nA., Vigfusson, Y., and Mace, J. Serving {DNNs} like\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\nclockwork: Performance predictability from the bottom\nup. In 14th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 20), pp. 443\u2013462,\n2020.\nHan,\nM.,\nZhang,\nH.,\nChen,\nR.,\nand\nChen,\nH.\nMicrosecond-scale preemption for concurrent {GPU-\naccelerated}{DNN} inferences. In 16th USENIX Sympo-\nsium on Operating Systems Design and Implementation\n(OSDI 22), pp. 539\u2013558, 2022.\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,\nDe Laroussilhe, Q., Gesmundo, A., Attariyan, M., and\nGelly, S. Parameter-efficient transfer learning for nlp.\nIn International Conference on Machine Learning, pp.\n2790\u20132799. PMLR, 2019.\nHu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,\nL., Chen, W., et al. Lora: Low-rank adaptation of large\nlanguage models. In International Conference on Learn-\ning Representations, 2021.\nHuang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen,\nM., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe:\nEfficient training of giant neural networks using pipeline\nparallelism. Advances in neural information processing\nsystems, 32, 2019.\nJamin, S., Shenker, S., Zhang, L., and Clark, D. D. An ad-\nmission control algorithm for predictive real-time service.\nIn Network and Operating System Support for Digital\nAudio and Video: Third International Workshop La Jolla,\nCalifornia, USA, November 12\u201313, 1992 Proceedings 3,\npp. 347\u2013356. Springer, 1993.\nKenton, J. D. M.-W. C. and Toutanova, L. K. Bert: Pre-\ntraining of deep bidirectional transformers for language\nunderstanding. In Proceedings of NAACL-HLT, pp. 4171\u2013\n4186, 2019.\nKorthikanti, V. A., Casper, J., Lym, S., McAfee, L., Ander-\nsch, M., Shoeybi, M., and Catanzaro, B. Reducing activa-\ntion recomputation in large transformer models. Proceed-\nings of Machine Learning and Systems, 5, 2023.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. In Flinn, J., Seltzer, M. I., Druschel,\nP., Kaufmann, A., and Mace, J. (eds.), Proceedings of the\n29th Symposium on Operating Systems Principles, SOSP\n2023, pp. 611\u2013626. ACM, 2023.\nLester, B., Al-Rfou, R., and Constant, N. The power of scale\nfor parameter-efficient prompt tuning. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 3045\u20133059, 2021.\nLi, X. L. and Liang, P. Prefix-tuning: Optimizing continu-\nous prompts for generation. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers),\npp. 4582\u20134597, 2021.\nLi, Z., Zheng, L., Zhong, Y., Liu, V., Sheng, Y., Jin, X.,\nHuang, Y., Chen, Z., Zhang, H., Gonzalez, J. E., et al.\n{AlpaServe}: Statistical multiplexing with model paral-\nlelism for deep learning serving. In 17th USENIX Sympo-\nsium on Operating Systems Design and Implementation\n(OSDI 23), pp. 663\u2013679, 2023.\nLin, J., Tang, J., Tang, H., Yang, S., Dang, X., and\nHan, S.\nAwq: Activation-aware weight quantization\nfor llm compression and acceleration. arXiv preprint\narXiv:2306.00978, 2023.\nLiu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal,\nM., and Raffel, C. A. Few-shot parameter-efficient fine-\ntuning is better and cheaper than in-context learning. Ad-\nvances in Neural Information Processing Systems, 35:\n1950\u20131965, 2022.\nLiu, X., Ji, K., Fu, Y., Tam, W. L., Du, Z., Yang, Z., and\nTang, J. P-tuning v2: Prompt tuning can be comparable\nto fine-tuning universally across scales and tasks. arXiv\npreprint arXiv:2110.07602, 2021.\nLiu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z.,\nand Tang, J. Gpt understands, too. AI Open, 2023. ISSN\n2666-6510. doi: https://doi.org/10.1016/j.aiopen.2023.08.\n012. URL https://www.sciencedirect.com/\nscience/article/pii/S2666651023000141.\nMangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul,\nS., and Bossan, B.\nPeft: State-of-the-art parameter-\nefficient fine-tuning methods.\nhttps://github.\ncom/huggingface/peft, 2022.\nMiao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Wong,\nR. Y. Y., Chen, Z., Arfeen, D., Abhyankar, R., and Jia,\nZ. Specinfer: Accelerating generative llm serving with\nspeculative inference and token tree verification. arXiv\npreprint arXiv:2305.09781, 2023.\nModelTC. Lightllm: Python-based llm inference and serv-\ning framework. https://github.com/ModelTC/\nlightllm, 2023. GitHub repository.\nNaghshineh, M. and Schwartz, M. Distributed call admis-\nsion control in mobile/wireless networks. IEEE Journal\non Selected Areas in Communications, 14(4):711\u2013717,\n1996.\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\nNarayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Pat-\nwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P.,\nBernauer, J., Catanzaro, B., et al. Efficient large-scale\nlanguage model training on gpu clusters using megatron-\nlm. In Proceedings of the International Conference for\nHigh Performance Computing, Networking, Storage and\nAnalysis, pp. 1\u201315, 2021.\nNVIDIA. Fastertransformer. https://github.com/\nNVIDIA/FasterTransformer, 2023.\nOlston, C., Fiedel, N., Gorovoy, K., Harmsen, J., Lao, L., Li,\nF., Rajashekhar, V., Ramesh, S., and Soyke, J. Tensorflow-\nserving: Flexible, high-performance ml serving. arXiv\npreprint arXiv:1712.06139, 2017.\nOpenAI. Gpt-4 technical report, 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems, 35:27730\u201327744, 2022.\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury,\nJ., Levskaya, A., Heek, J., Xiao, K., Agrawal, S., and\nDean, J. Efficiently scaling transformer inference. arXiv\npreprint arXiv:2211.05102, 2022.\nShazeer, N. Fast transformer decoding: One write-head is\nall you need. arXiv preprint arXiv:1911.02150, 2019.\nShen, H., Chen, L., Jin, Y., Zhao, L., Kong, B., Philipose,\nM., Krishnamurthy, A., and Sundaram, R. Nexus: A gpu\ncluster engine for accelerating dnn-based video analysis.\nIn Proceedings of the 27th ACM Symposium on Operating\nSystems Principles, pp. 322\u2013337, 2019.\nSheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen,\nB., Liang, P., R\u00b4e, C., Stoica, I., and Zhang, C. Flex-\ngen: High-throughput generative inference of large lan-\nguage models with a single GPU. In International Confer-\nence on Machine Learning, ICML 2023, volume 202 of\nProceedings of Machine Learning Research, pp. 31094\u2013\n31116. PMLR, 2023.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\nJ., and Catanzaro, B.\nMegatron-lm: Training multi-\nbillion parameter language models using model paral-\nlelism. arXiv preprint arXiv:1909.08053, 2019.\nStern, M., Shazeer, N., and Uszkoreit, J. Blockwise parallel\ndecoding for deep autoregressive models. Advances in\nNeural Information Processing Systems, 31, 2018.\nTillet, P., Kung, H.-T., and Cox, D. Triton: an intermediate\nlanguage and compiler for tiled neural network computa-\ntions. In Proceedings of the 3rd ACM SIGPLAN Interna-\ntional Workshop on Machine Learning and Programming\nLanguages, pp. 10\u201319, 2019.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nVin, H., Goyal, P., and Goyal, A. A statistical admission con-\ntrol algorithm for multimedia servers. In Proceedings of\nthe second ACM international conference on Multimedia,\npp. 33\u201340, 1994.\nWang, X., Xiong, Y., Wei, Y., Wang, M., and Li, L. Light-\nseq: A high performance inference library for transform-\ners. In Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies: Industry\nPapers, pp. 113\u2013120, 2021.\nXiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han,\nS. Smoothquant: Accurate and efficient post-training\nquantization for large language models. In International\nConference on Machine Learning, ICML 2023, volume\n202 of Proceedings of Machine Learning Research, pp.\n38087\u201338099. PMLR, 2023.\nYao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and\nHe, Y. Zeroquant: Efficient and affordable post-training\nquantization for large-scale transformers. In Oh, A. H.,\nAgarwal, A., Belgrave, D., and Cho, K. (eds.), Advances\nin Neural Information Processing Systems, 2022.\nYu, G.-I., Jeong, J. S., Kim, G.-W., Kim, S., and Chun, B.-\nG. Orca: A distributed serving system for {Transformer-\nBased} generative models. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI\n22), pp. 521\u2013538, 2022.\nZhang, H., Tang, Y., Khandelwal, A., and Stoica, I. SHEP-\nHERD: Serving DNNs in the wild. In 20th USENIX\nSymposium on Networked Systems Design and Imple-\nmentation (NSDI 23), pp. 787\u2013808, Boston, MA, April\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\n2023a. USENIX Association. ISBN 978-1-939133-33-5.\nURL https://www.usenix.org/conference/\nnsdi23/presentation/zhang-hong.\nZhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y.,\nChen, W., and Zhao, T. Adaptive budget allocation for\nparameter-efficient fine-tuning. In The Eleventh Interna-\ntional Conference on Learning Representations, 2022.\nZhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,\nR., Song, Z., Tian, Y., R\u00b4e, C., Barrett, C., et al. H 2 o:\nHeavy-hitter oracle for efficient generative inference of\nlarge language models. arXiv preprint arXiv:2306.14048,\n2023b.\nZheng, L., Li, Z., Zhang, H., Zhuang, Y., Chen, Z., Huang,\nY., Wang, Y., Xu, Y., Zhuo, D., Xing, E. P., et al. Alpa:\nAutomating inter-and intra-operator parallelism for dis-\ntributed deep learning. In 16th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI\n22), pp. 559\u2013578, 2022.\nZheng, L., Chiang, W.-L., Sheng, Y., Li, T., Zhuang, S., Wu,\nZ., Zhuang, Y., Li, Z., Lin, Z., Xing, E., et al. Lmsys-chat-\n1m: A large-scale real-world llm conversation dataset.\narXiv preprint arXiv:2309.11998, 2023a.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. arXiv\npreprint arXiv:2306.05685, 2023b.\nZhou, Z., Wei, X., Zhang, J., and Sun, G.\n{PetS}: A\nunified framework for {Parameter-Efficient} transformers\nserving. In 2022 USENIX Annual Technical Conference\n(USENIX ATC 22), pp. 489\u2013504, 2022.\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\nA\nADDITIONAL EXPERIMENT RESULTS\nA.1\nAnalysis of PEFT\nIn our evaluation of PEFT, several key observations were\ndiscerned. First, the lack of KV cache support makes the\nmaximal batch size of PEFT much smaller compared to\nS-LoRA. For instance, in A10G S1, S-LoRA can accom-\nmodate a maximal batch size of 30, while PEFT can only\naccommodate a maximal batch size of 6. Secondly, the\nlack of continuous batching support makes shorter requests\nwait for longer requests in a batch. These two factors to-\ngether result in the low throughput of PEFT even when\nthere is only one adapter. When there are more adapters, the\nlack of batching support across different adapters makes the\nthroughput even lower, resulting in only 0.17 request/sec-\nond performance in the largest number of adapters we test.\nAs another result, the average latency explodes because the\nrequest rate is far beyond the maximal capacity of the PEFT\nsystem. In Table 5, we show that even in the lowest request\nrate we test, PEFT fails to process with a low latency.\nnum adapters\nthroughput\navg. latency\navg. attainment\n1\n0.26\n1021.86\n0.0\n20\n0.23\n1178.52\n0.0\n50\n0.22\n1293.97\n0.0\n100\n0.20\n1421.16\n0.0\n200\n0.17\n1609.50\n0.0\nTable 4. PEFT results on the synthetic workload S1 against number\nof adapters.\nreq rate\nthroughput\navg. latency\navg. attainment\n1\n0.11\n1165.46\n0.0\n1.5\n0.13\n1398.56\n0.0\n2\n0.17\n1614.37\n0.0\n2.5\n0.18\n1904.73\n0.0\nTable 5. PEFT results on the synthetic workload S1 against request\nrate.\nA.2\nExperiments for adapter clustering.\nWe implement a straightforward adapter clustering algo-\nrithm. Let parameter d be the number of adapters in a batch.\nIn addition to the FCFS order (or early abort order if turned\non), if the number of adapters reaches d, we will prioritize\nthe requests that have their adapter already in the batch. But\nif the requests from the d adapters cannot fill all the space\nfor a running batch, we allow other requests to be added. We\nrun some additional experiments to study how the number\nof clusters impacts throughput and SLO attainment. We\ncall d as the number of clusters in the figure. As shown in\nFigure 11 and Figure 12, the impact is not significant but ob-\nservable, especially for larger \u03b1 and cv. Generally, a small\nd can result in better performance. The small fluctuation\nfor small d\u2019s may be because of the scheduler overhead and\nrandom noise.\n0\n10\n20\n30\n1.0\n1.2\n1.4\n1.6\n0\n10\n20\n30\n1.0\n1.1\n1.1\n1.2\n1.2\n1.2\n0\n10\n20\n30\n0.0\n0.2\n0.4\n0.6\n0.8\n0\n10\n20\n30\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nS2 (Llama-7b)\nS4 (Llama-13b)\nThroughput (req/s)\nNumber of Clusters\nNumber of Clusters\nSLO Attainment\nalpha = 0.1\nalpha = 0.3\nalpha = 0.6\nalpha = 1\nFigure 11. Ablation study for different number of clusters on A100\n(40GB) with different \u03b1. The settings for the synthetic work-\nload trace are n = 32, \u03b1 = [0.1, 0.3, 0.6, 1], R = 2, cv =\n1, [It, Iu] = [8, 512], [Ol, Ou] = [8, 512]\n0\n10\n20\n30\n1.0\n1.2\n1.5\n1.8\n2.0\n0\n10\n20\n30\n0.0\n0.2\n0.5\n0.8\n1.0\nNumber of Clusters\nThroughput (req/s)\nNumber of Clusters\nSLO Attainment\ncv = 1\ncv = 2\ncv = 4\ncv = 6\ncv = 8\nFigure 12. Ablation study for different number of clusters on S2\n(Llama-7b) A100 (80GB) with different cv. The settings for the\nsynthetic workload trace are n = 32, \u03b1 = 1, R = 2, cv =\n[1, 2, 4, 6, 8], [It, Iu] = [8, 512], [Ol, Ou] = [8, 512]\nB\nADMISSION CONTROL IN S-LORA\nTraditional admission control usually assumes a hard thresh-\nold for the delay in the service (Jamin et al., 1993; Vin\net al., 1994; Naghshineh & Schwartz, 1996), and controls\nthe total number of violations of delay. Here for LoRA\nserving, we assume a soft threshold characterized by the\nuser\u2019s reward function. For illustration purposes, let the\narrival time of the requests be integers, and assume that\nwe process one query in each time period of length 1. Let\nQ = {q1, q2, \u00b7 \u00b7 \u00b7 qn} be the request queue in the ascending\norder of the arrival time, and l be the desired number of\nserved requests. We quantify the user\u2019s satisfaction with a\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\nreward function r : R+ 7\u2192 [0, 1] that maps the first token\nlatency of a request to a scalar in between [0, 1], where 0\nrepresents the user losing patience and giving up the query,\nand 1 represents the user is completely satisfied with the\nlatency. Let ti be the latency of serving the request qi in the\nqueue Q. Then we aim to solve the following constrained\noptimization:\nmax\nn\nX\ni=1\nr(ti)\n(3)\ns.t. 1(r(ti) > 0) = l.\nWe show that when the derivative of reward is non-\nincreasing, the optimal solution to the above constrained\noptimization problem is to serve the most recent l elements\nqn\u2212l+1, qn\u2212l+2, \u00b7 \u00b7 \u00b7 , qn in order.\nTheorem B.1. Assume that r\u2032(t) \u2264 0 for any t \u2208 R+. The\noptimal solution to Equation (3) is to serve the most recent\nl elements qn\u2212l+1, qn\u2212l+2, \u00b7 \u00b7 \u00b7 , qn in order.\nThe proof is deferred to Appendix B.1. In practice, for a\ngiven request queue, we can estimate the largest possible\nnumber of requests to be served in SLO as l. Then we\ntake the most recent l elements for serving. Such an l can\nbe approximated by simulating a First-Come-First-Serve\n(FCFS) strategy, which is optimized to serve requests as\nmany as possible.\nIn S-LoRA, the scenario is more complicated because of the\nheterogeneity and unpredictability of the sequence length.\nAs an approximation, we implement a heuristic as follows.\nThe high-level scheduling is that we will fetch a minibatch\nof new requests to be added into the running batch every\nseveral decode step. From the history, we use the moving\naverage to estimate a current request rate R1 measured in\nhow many requests will be added to the waiting queue per\nperiod of fetching new requests. We also use the moving\naverage to estimate the number of new requests R2 that can\nbe added to the running batch for a period. Let rti be the\ncoming time of request ri, ct be the current time, tlmax be\nthe maximum allowed first token latency to meet the SLO\nand lprefill be the maximum prefill latency for a minibatch\nin history. Each time we generate the new minibatch, we\nwill first abort the requests R = {rk | ct \u2212 rtk + lprefill >\ntlmax}. Requests in R are highly likely to miss the SLO\neven if they get scheduled immediately due to the high\nprefill latency. Then if R1 > R2, which means the system\nis temporarily overloaded, we will fetch the newest requests\ninto the minibatch. If R1 \u2264 R2, the waiting queue will be\nshortened if the trend continues. In this case, we will choose\nfrom the earliest.\nB.1\nProof of Theorem B.1\nWe first prove that for any admission control strategy that\nserves l elements, one can always find another admission\ncontrol strategy that serves the most recent l elements with\na larger cumulative reward.\nAssume that we serve l elements qs1, qs2, \u00b7 \u00b7 \u00b7 , qsl in the l\ntimesteps. Assume without loss of generality that qs1 is not\namong the most recent l elements, and assume that the k-th\nelement is not served with k \u2208 [n \u2212 l, n]. By definition\nwe know that s1 < k. Now at the time of serving qs1, we\nserve qk rather than qs1, and keep the rest of the choices in\nother time steps same. In this case, the number of served\nqueries remains the same. On the other hand, we know that\nthe latency satisfies ts1 > tk since the k-th element is more\nrecent. This gives that\nr(ts1) < r(tk).\nSince the reward for other elements does not change, the\ntotal reward is increased while the constraint is still satisfied.\nBy repeating the operations until all the elements served are\nthe most recent l elements, we prove that claim.\nNext, we prove that serving the most recent l elements in\norder of qn\u2212l+1, qn\u2212l+2, \u00b7 \u00b7 \u00b7 , qn is optimal. For any i, j \u2208\n[n \u2212 l + 1, n], we assume that i < j and j is first served at\ntime t1 while i is served at time t2 with t1 < t2. Let ta\ni , ta\nj\nbe the arrival time of i, j. The reward for serving i, j in this\ncase becomes\nr(t2 \u2212 ta\ni ) + r(t1 \u2212 ta\nj ).\nNow we show that by swapping the time of serving i, j, the\nreward does not decrease. This is equivalent to showing that\nr(t1 \u2212 ta\ni ) + r(t2 \u2212 ta\nj ) \u2265 r(t2 \u2212 ta\ni ) + r(t1 \u2212 ta\nj ).\nRearranging the above equation, we know that it is equiva-\nlent to\nr(t1 \u2212 ta\ni ) \u2212 r(t2 \u2212 ta\ni )\nt1 \u2212 t2\n\u2264 r(t1 \u2212 ta\nj ) \u2212 r(t2 \u2212 ta\nj )\nt1 \u2212 t2\n.\nThis is true due to the concavity of the reward function, thus\nfinishing the proof.\n"
  },
  {
    "title": "Ziya2: Data-centric Learning is All LLMs Need",
    "link": "https://arxiv.org/pdf/2311.03301.pdf",
    "upvote": "16",
    "text": "Ziya2: Data-centric Learning is All LLMs Need\nRuyi Gan\u2665\u2660\nZiwei Wu\u2665\nRenliang Sun\u2665\nJunyu Lu\u2665\nXiaojun Wu\u2665\nDixiang Zhang\u2665\nKunhao Pan\u2665\nPing Yang\u2665\u2217\nQi Yang\u2665\u2217\nJiaxing Zhang\u2665\nYan Song\u2660\n\u2665International Digital Economy Academy\n\u2660University of Science and Technology of China\n{ganruyi, wuziwei, sunrenliang, lujunyu, zhangjiaxing}@idea.edu.cn\nclksong@gmail.com\nAbstract\nVarious large language models (LLMs) have\nbeen proposed in recent years, including closed-\nand open-source ones, continually setting new\nrecords on multiple benchmarks.\nHowever,\nthe development of LLMs still faces several\nissues, such as high cost of training models\nfrom scratch, and continual pre-training lead-\ning to catastrophic forgetting, etc. Although\nmany such issues are addressed along the line\nof research on LLMs, an important yet practi-\ncal limitation is that many studies overly pur-\nsue enlarging model sizes without comprehen-\nsively analyzing and optimizing the use of pre-\ntraining data in their learning process, as well as\nappropriate organization and leveraging of such\ndata in training LLMs under cost-effective set-\ntings. In this work, we propose Ziya2, a model\nwith 13 billion parameters adopting LLaMA2\nas the foundation model, and further pre-trained\non 700 billion tokens, where we focus on pre-\ntraining techniques and use data-centric opti-\nmization to enhance the learning process of\nZiya2 on different stages. Experiments show\nthat Ziya2 significantly outperforms other mod-\nels in multiple benchmarks especially with\npromising results compared to representative\nopen-source ones.1\n1\nIntroduction\nLLMs have achieved great success in the field of\nartificial intelligence (AI), especially natural lan-\nguage processing (NLP), over the past few years.\nGenerally, LLMs are pre-trained on large amounts\nof text and show promising performance in a vari-\nety of NLP tasks without requiring intensive task-\nspecific tuning on huge amounts of labeled data\n(Devlin et al., 2019; Raffel et al., 2020; Joshi et al.,\n2020; Qin et al., 2021; Wang et al., 2022; Lu et al.,\n2022; Tian et al., 2023; Ping et al., 2023; Huang\n*They contributed to this work when they worked in IDEA.\n1Ziya2 (Base) is released at https://huggingface.co/\nIDEA-CCNL/Ziya2-13B-Base and https://modelscope.\ncn/models/Fengshenbang/Ziya2-13B-Base/summary\net al., 2023a). Among all LLMs, representative\nones include GPT-3 (Brown et al., 2020) and its\nsuccessors ChatGPT (OpenAI, 2022), GPT-4 (Ope-\nnAI, 2023), and PaLM-2 (Anil et al., 2023), demon-\nstrate strong adaptability and applicability. How-\never, owing to the fact that the aforementioned\nLLMs are developed in rather restricted environ-\nments, the lack of access to developers\u2019 source\ncode and parameters becomes a barrier for many\nfollowing researchers and developers in continuing\nLLM research based on existing well-performed\nmodels. This paradox leads to the phenomenon that\nmany researchers turn to train open-source alter-\nnatives, such as LLaMA2 (Touvron et al., 2023b)\nand Falcon (Penedo et al., 2023), etc., since the\nopen-source LLMs provide a foundation for fur-\nther learning and improvement on the shoulders\nof successful predecessors, which promotes trans-\nparency and accountability in following AI studies.\nYet, besides a variety of benefits that have been\nbrought along with LLMs, especially the open-\nsource counterparts, their development is currently\nfacing the following three significant challenges.\nThe first one is the high cost of pre-training models\nfrom scratch, making LLM learning a cost process,\nso that continual training (or pre-training) is po-\ntentially cost-effective when starting from a foun-\ndation LLM. The second one is that open-source\nLLMs often do not come with open-source data,\nleading to discrepancies in data distribution, implic-\nitly resulting in catastrophic forgetting when mixed\nwith different data in later usage. The third one is\nthat many studies on LLMs prioritize increasing\nmodel size so as to maximize their benefits from\nvarious aspects, and they thus often overlook the\ninfluence of training data quality on model perfor-\nmance, which has been demonstrated to be signifi-\ncant in many NLP tasks (Liu et al., 2019a; Brown\net al., 2020; Black et al., 2022; Anil et al., 2023;\nTouvron et al., 2023b). The pursuit of enlarging\nmodel sizes necessitates more extensive GPU con-\narXiv:2311.03301v1  [cs.CL]  6 Nov 2023\nFigure 1: The overall data-centric process for learning Ziya2, where the pipeline to obtain high-quality data, the\ntraining strategies, and the three-stage training process are presented in the bottom, middle, and top part of the figure,\nrespectively. Notably, the training strategies for different stages are illustrated by comparisons of data distributions.\nsumption, accordingly increasing training costs as\nwell. Therefore, such challenges ferment expecta-\ntions of investigating data quality and its impact on\nthe LLMs and research conducting on better lever-\naging data for LLM learning at different stages.\nIn this work, we focus on the technique of contin-\nual pre-training and understanding the intricate rela-\ntionship between data and model performance. We\ndelve into an in-depth exploration of how the high-\nquality pre-training data enhance the performance\nof an LLM, while keeping its size and structure\nessentially unchanged. In doing so, we build Ziya2\nby using LLaMA2-13B (Touvron et al., 2023b) as\nthe foundational model and continue pre-training\nit on approximately 700 billion tokens in Chinese\nand English. Specifically, a three-stage training pro-\ncess has been adopted to leverage both general and\ndomain-specific corpora to enhance Ziya2\u2019s bilin-\ngual generation capabilities, where the first stage\ntrains Ziya2 with huge high-quality data, including\nChinese and English languages; the second stage\nuses supervised data to optimize the LLM; and the\nthird stage mainly focuses on training Ziya2 with\nmathematical data. The entire learning process cul-\nminates in the creation of the Ziya2 model with 13\nbillion parameters, as that shown in Figure 1.\nThe evaluation of Ziya2 is performed on sev-\neral representative benchmarks including multidis-\nciplinary datasets in Chinese and English, as well\nas that specifically designed to test mathematical\nand programming abilities. Compared to LLaMA2\n(Touvron et al., 2023b), Ziya2 shows its superior-\nity significantly across different benchmarks. Par-\nticularly, using LLaMA2 as a standard LLM as-\nsessment, Ziya2 improves it by 10% on MMLU\n(Hendrycks et al., 2020), 61% on CMMLU (Li\net al., 2023), and 68% on C-Eval (Huang et al.,\n2023b), respectively. Especially in terms of math-\nematical benchmarks, Ziya2 improves LLaMA2\nby 138% on GSM8K (Cobbe et al., 2021) and\n120% on MATH (Hendrycks et al., 2021), and\nfor programming benchmarks, Ziya2 improves it\nby 89% on HumanEval (Chen et al., 2021). For\nfurther illustration of the effectiveness of Ziya2,\nwe also select several representative open-source\nmodels of similar size for comparison. It is shown\nthat Ziya2 achieves outstanding performance\non multidisciplinary datasets2, where, especially,\nit surpasses all open-source models used for com-\n2Detailed results are reported in Table 3.\nparison in mathematics and programming skills.\nNotably, Ziya2\u2019s performance on Chinese tasks\nalso surpasses the GPT-3.5-turbo3. These results\ndemonstrate that high-quality data and appropri-\nate continual pre-training are able to significantly\nimprove LLMs\u2019 performance without significantly\nchanging model size and affecting their stability.\n2\nThe Approach\nThe overview of our learning process for Ziya2 is\nillustrated in Figure 1, where it commences with\nthe establishment of a data pipeline capable of con-\ntinuous cleansing and evaluation of extensive web-\nbased datasets, ensuring the acquisition of large-\nscale, high-quality data for the training of Ziya2.\nSubsequently, resource conservation and the ex-\nploration of effective continual pre-training strate-\ngies are facilitated by leveraging LLaMA2-13B as\nthe foundation for extensive further pre-training,\nculminating in the completing of Ziya2. This pro-\ncess still necessitates large-scale domain-specific\ndata to augment particular capabilities. Therefore,\na data-centric, three-stage continual pre-training\nstrategy is employed to endow Ziya2 with supe-\nrior capabilities in bilingual understanding, code\ngeneration, and mathematical reasoning compared\nto LLaMA2-13B, with details on data processing,\nmodel architecture and training optimization illus-\ntrated in following texts.\n2.1\nThe Data Factory\nData serves as a pivotal cornerstone for LLMs, with\nits scale and quality directly influencing model\nperformance. Based on this fact, we develop a\nproprietary data factory based on CCNet (Wenzek\net al., 2019), as depicted in Figure 2, to facilitate a\nmultitude of functions including data preprocess-\ning, automatic scoring, rule-based filtering, content\nde-duplication, and data evaluation. This factory\nis specifically tailored in this work to meet the re-\nquirements of continual pre-training, data sampling\nand augmentation. In doing so, it is able to con-\ntinuously process data from the Internet at a rate\nof 15GB per hour, ensuring a consistent influx of\nrefined data to further enhance our LLM training\nprocess.4 Details are illustrated in following texts.\nData Preprocessing (DP)\nThis is the first step\nin data processing, in which we conduct several\n3We use the model under its \u201cgpt-3.5-turbo-0613\u201d version.\n4With our data factory, we accumulate a volume of more\nthan 4.5TB of high-quality data in our internal repository.\nRaw Corpus\nTrain Corpus\n100%\nDP\n14.66%\n85.34%\n30.94%\n54.40%\n19.73%\n34.67%\nAS\nRF\nCD\nDE\nFigure 2: An overview of the proposed data-centric\nlearning approach of Ziya2, which includes five compo-\nnents: data preprocessing (DP), automatic scoring (AS),\nrule-based filtering (RF), content de-duplication (CD),\nand data evaluation (DE). The gray and colored blocks\nindicate the proportion of data that is filtered out and\nretained relative to the original dataset, respectively.\nbasic operations. The first is language detection on\nthe collected corpus and selection of only Chinese\nand English language data. Once the selection is\ndone, we proceed to standardize the encoding of\nthe corpus and transform all Chinese text into the\nsimplified version. Then, we remove non-usable to-\nkens such as non-visible control characters, special\nsymbols, emojis, and improper punctuation marks.\nAutomatic Scoring (AS)\nUpon the resulted data\nfrom preprocessing, we perform an automatic qual-\nity control (filtering) by language model scoring.\nIn doing so, we employ KenLM (Heafield, 2011)\nto train two language models from Chinese and\nEnglish Wikipedia, respectively, and conduct per-\nplexity (PPL) assessments on the input data. Subse-\nquently, we select data based on their ranking from\nlower to higher PPL scores, with the top 30% data\nmarking as high-quality and those in between 30%\nand 60% in their PPL ranking as medium quality.\nRule-based Filtering (RF)\nGiven the abundance\nof sensitive and unsafe content on the Internet,\nwhich are easy to be memorized by LLMs, so that it\nis required to eliminate text with significant tenden-\ncies toward pornography, violence, politics, and\nadvertising. Therefore, we design more than 30\nfiltering rules at three granular levels: document,\nparagraph, and sentence, and filter out texts by ap-\nplying rules from large to small granular levels. At\nthe document level, rules are principally designed\naround content length and format, while at the para-\ngraph and sentence levels, the focus of rules shifts\nto the toxicity of the content. It is worth noting\nthat, in the preliminary stages of rule design, we\nMetric\nMethod\nLevel\nComments\nPrivacy Word Rate\nRe Matching\n1\nIncludes email addresses, phone numbers and ect.\nToxic Word Rate\nRe Matching\n1\nContains political, explicit, violent, or similar content.\nAdv Word Rate\nRe Matching\n1\nChecking for the presence of advertising keywords.\nWebpage Funcword Rate\nRe Matching\n1\nDetecting the presence of HTML tags.\nPerplexity\nLM\n2\nWhether the sentences in the article are coherent.\nInformativeness\nLM\n2\nAssessing the information content within the text.\nReadability\nCounts\n2\nAssessing text readability\nLanguage\nLM\n3\nAnalyzing the distribution of languages.\nDoc Length\nCounts\n3\nEvaluating the distribution of text lengths.\nTopic\nStatistics\n3\nDetecting the topic distribution of the text.\n(a) Machine Metrics\nMetric\nMethod\nLevel\nComments\nIncoherent\nHuman Check\n1\nAssessing text readability\nSensitive Topics\nHuman Check\n1\nContains political, explicit, violent, or similar content.\nRedundancy\nHuman Check\n1\nTo detect if the text contains redundant information.\n(b) Human Metrics\nTable 1: These are the metrics we utilize for evaluating data quality. The Method denotes our detection method,\nRe Matching signifies the method involving regular expressions for counting. LM represents the utilization of a\nlanguage model for predicting relevant metrics. Counts indicates the use of statistical methods for directly countable\nmetrics. Human Check indicates manual spot-checking conducted by humans. Level indicates the degree of\nstringency we apply to the respective metrics. We consider it compliant if it does not exceed one in a thousand.\nalso conduct random sampling of the original text\nfor manual evaluation. Rule iterations are then in-\nformed by feedback from human reviews, ensuring\nthe precision of later filtration process.\nContent De-duplication (CD)\nAs mentioned in\nthe following studies (Lee et al., 2021; Tirumala\net al., 2023; Penedo et al., 2023), repetitive con-\ntent does not significantly improve training and\nhurts training efficiency.\nWe used Bloomfilter\n(Bloom, 1970) and Simhash (Charikar, 2002) to\nde-duplicate the text in the data through the follow-\ning steps. First, we find that Common Crawl and\nother open-source datasets contain substantial du-\nplication of web pages and thus use Bloomfilter5 to\nde-duplicate URLs, which significantly reduces the\ncomputational load required for subsequent con-\ntent de-duplication. Second, our analysis shows\nthat many remaining web pages share similar con-\ntent, where the main differences among them are\nspecial characters such as punctuation marks and\nemoticons. Thus, we perform a round of precise de-\nduplication on these web pages. Third, we employ\nSimHash for the fuzzy de-duplication of textual\ncontent for the remaining data. Although this pro-\ncess may incorrectly exclude some high-quality\n5We use our own implementation of Bloomfilter.\ndata, our manual sampling evaluation of the de-\nduplicated data indicates that this loss is acceptable\nin order to pursue higher efficiency. Moreover, to\nfacilitate efficient content de-duplication for new\ndata collected by our data factory, we use caching\nand bucketing techniques, where new data do not\nnecessitate redundancy checks against all existing\ndata, which optimizes the de-duplication process.\nData Evaluation (DE)\nAfter data cleaning, we\nevaluate the data quality with the machine and hu-\nman metrics illustrated in Table 1. For machine\nmetrics, we randomly select 1% of the processed\ndata and run scripts to compute the machine met-\nrics. For human metrics, we randomly sample\n1,000 instances and manually evaluate the extracted\ncorpus accordingly. Based on the machine and hu-\nman metrics, we determine whether an example\nqualifies our standard for data quality. Afterwards,\nwe compute the rate of the unqualified examples\nover all evaluated instances. If the rate is lower than\na threshold, it proves the data meets our require-\nment, so we use them as a part of the training cor-\npus. If the rate is higher than the threshold, which\nmeans the data does not meet our standard, we\nimprove the processes of automatic scoring, rule-\nbased filtering, and content de-duplication, which\nPre-training Stage\nDataset\nLanguage\nSize\nDoc #\nToken #\nSampling\nStage 1/2/3\nPile-Pajama\nen\n800GB\n94M\n220B\n50%\nCC\nen\n600GB\n81M\n170B\n64%\nWudao-Ziya\nzh\n156GB\n51M\n48B\n100%\nYuan1.0\nzh\n54GB\n260M\n193B\n100%\nTranslate\nmulti\n3GB\n12M\n1.5B\n100%\nCode\nmulti\n480GB\n124M\n191B\n100%\nInstruct\nmulti\n1.6GB\n0.9M\n0.8B\n100%\nStage 2/3\nWanjuan-Ziya\nzh\n76GB\n16M\n29B\n100%\nStage 3\nMetaMath-Ziya\nen\n0.3GB\n0.4M\n0.1B\n100%\nTable 2: The information of high-quality pre-training datasets used to train Ziya2. The \u201cpre-training stage\u201d illustrates\nthe stages that the datasets are used; \u201cen\u201d, \u201czh\u201d, \u201cmulti\u201d mean the language of the datasets are English, Chinese, and\nmulti-lingual, respectively; \u201cSampling\u201d refers to the ratio of data sampled from the original dataset.\nare then utilized to process data.\nThe Resulted High-quality Data\nBy cleaning\ndatasets from Common Crawl6, Wudao (Yuan et al.,\n2021), Yuan1.0 (Wu et al., 2021), Pile (Gao et al.,\n2020), Redpajama (Computer, 2023), Wanjuan (He\net al., 2023), MetaMath (Yu et al., 2023) and our\nown collected data, which included code and book-\nrelated content, we sample a total of 4.5TB of high-\nquality data from the initial 13TB of open-source\ndata. Table 2 lists primary information (i.e., the\nlanguage, size, number of tokens and documents,\nand the ratio of data sampled from datasets) about\nthe processed data, whose details are illustrated as\nfollows. Pile-Pajama is a de-duplicated fusion of\nthe Pile and Redpajama datasets after removing\nCommon Crawl data. CC is a de-duplicated fusion\nof data from Pile and Redpajama that originated\nfrom Common Crawl. Wudao-Ziya is a dataset\nthat combines our collected data with the Wudao\ndataset. Yuan1.0 is an open-source dataset pro-\nvided by Inspur Technology, and we filter the raw\ndata using our cleaning rules. Translate is the mul-\ntilingual translation dataset we collect. Code is the\ncode data we collect from GitHub, which includes\nmultiple programming languages such as C, C++,\nand Python. We add the program language type\nbefore the code and change it to a format that the\nMarkdown syntax is able to recognize. In this way,\nthe model we train is able to generate formatted\ncode. Instruct is a dataset constructed from instruc-\ntions that we collect. Wanjuan-Ziya is a dataset\nthat combines high-quality data from the Wanjuan\ndataset, as well as math-related data we collect\nourselves. MetaMath-Ziya is a dataset derived\n6https://commoncrawl.org/\nfrom the Huawei\u2019s open-source MetaMath dataset\nafter data augmentation. We construct some Chi-\nnese and English prompts for Instruct, Wanjuan-\nZiya, and MetaMath-Ziya datasets, such as \u201cQ-\nA\u201d, \u201cquestion-answer\u201d, \u201cproblem-solution\u201d, etc.7\n2.2\nThe Architecture of Ziya2\nThe architecture of Ziya2 is based on LLaMA2,\nwhere we propose to improve the quality of input\ndata processing, token and hidden representations\nthrough enhancing tokenizer, positional embed-\nding, as well as layer normalization and attention,\nrespectively, so as to facilitate Chinese text process-\ning, adapt to the changes in text length and data\ndistribution, and improve its efficiency and stability\nin pre-training. The details of the aforementioned\nimprovements are illustrated as follows.\nTokenizer\nIn order to maximize the preservation\nof semantic meaning in the text and provide better\nsupport for Chinese, we adopt a BPE (Byte-Pair\nEncoding) tokenizer (Sennrich et al., 2015). For\nthe vocabulary of the tokenizer, we reuse over 600\nChinese characters originally used in LLaMA and\nadd extra 7,400 commonly used Chinese charac-\nters, which include both simplified and traditional\nChinese characters as well as Chinese punctuation\nmarks. The reason for adding extra Chinese charac-\nters is the following. The original LLaMA2 vocab-\nulary with BPE tokenizer is not highly efficient for\nChinese, primarily due to how computers handle\ncharacter encoding. For instance, in most cases,\none UTF-8 Chinese character is encoded into 2-4\ntokens using BPE encoding. After adding Chinese\ncharacters to the vocabulary, testing on a 10GB\n7Some examples training data are shown in Appendix A.\nChinese corpus shows an efficiency improvement\nof approximately 34% in Chinese encoding and\ndecoding compared to the original tokenizer.\nPositional Embedding\nDuring the continual pre-\ntraining, we observe that there is a divergence in the\ndistribution of text lengths between our continued\ntraining dataset and the original LLaMA2 dataset,\nnecessitating an adaptation of the position embed-\nding to different data distributions. Meanwhile,\nconsidering downstream tasks involving lengthy\ntexts, the scalability of position embedding is of\nsignificant importance. LLaMA2 employs rotary\nposition encoding (Su et al., 2021), which, through\nthe mechanism of absolute position encoding, ac-\ncomplishes relative position encoding. To avoid the\noverflow issues associated with mixed precision,\nwe implement rotary position encoding using FP32\nprecision, thereby accommodating the variation in\ndata length distribution in continual training.\nLayer Normalization and Attention\nDuring the\ncontinual pre-training process, we find that due\nto divergences in the data distribution between\nour continued training dataset and the original\nLLaMA2 dataset, the direct implementation of\nmixed precision training in layer normalization and\nattention leads to precision overflow, which results\nin training instability. To maintain the efficiency\nand stability of the model training, we improve\nlayer normalization and attention in LLaMA2.\nSpecifically, for layer normalization, we utilize an\nAPEX8 RMSNorm (Zhang and Sennrich, 2019)\nimplementation, which also operates under FP32\nprecision training. For attention, we employ a\nfused operator to replace the original scaling, mask,\nand softmax operators within the attention module,\nthereby expediting the computation of attention.\nFurthermore, to prevent overflow during mixed pre-\ncision training in the softmax operator, it is trained\nusing FP32 precision. Through these structural im-\nprovements, the training process is better adapted\nto the instability brought about by changes in data\ndistribution, thus enhancing training efficiency.\n2.3\nContinual Pre-training\nWe continually pre-train LLaMA2 on datasets with\nthree-stage pre-training to benefit from high-quality\ndata in training Ziya2 model. The following text\nillustrates the pre-training process, which covers\n8https://github.com/NVIDIA/apex\nRandom Sampling\nCorpus\nTask Type\nCorpus\nPre-training on unsupervised data\nPre-training on supervised data\nMutual Invisibility\nMutual Visibility\nPadding\n~ Doc 1\nDoc 2\nDoc 3 ~\nContext window = 4096\nDoc 1\nDoc 2\nPAD\nPAD\nDoc 1\nDoc 2\nPAD\nDoc 1\nDoc 2\nContext window = 4096\nDoc\n~ Doc\nDoc ~\nComplete | left truncated | right truncated document\nP\nP\nP\nP\nP\nP\nP\nPrompt\nFigure 3: The process of constructing unsupervised and\nsupervised pre-training data in the three stages.\nmodel initialization, training strategies, optimizer,\ntraining efficiency, and training stability.\nInitialization\nConsidering the vocabulary with\n8,000 Chinese characters, during continual pre-\ntraining, each Chinese character represents a new\ntoken. For this token, in the continual pre-training\nof Ziya-13B, we employ random initialization.\nGiven that LLaMA2 already learns some informa-\ntion related to Chinese and in LLaMA2, one Chi-\nnese character corresponds to 2-4 tokens; we take\nthe weighted average of the embeddings of these to-\nkens as the embedding for the Chinese character in\nZiya2. We then proceed with further pre-training.\nTraining Strategy\nDuring continual pre-training,\na critical challenge is catastrophic forgetting. The\ngoal is to enhance LLaMA2\u2019s Chinese and coding\ncapabilities without compromising its proficiency\nin English, which is also Ziya2\u2019s objective. To\nachieve this, we conduct continual pre-training in\nthree stages, where the details of data concatenation\nfor these stages are illustrated in Figure 3. The fol-\nlowing texts present the details of the three stages.\nIn the first stage, as shown in Table 2, we sample\nEnglish data close to LLaMA2\u2019s original distribu-\ntion. We also sample Chinese data from Wudao-\nZiya and Yuan 1.0, as well as data from Code,\nwhich together form 650B unsupervised data for\ncontinual pre-training. We thoroughly shuffle these\ndatasets, concatenate different data segments into\na 4,096 context as a training example, and use an\nattention mask to prevent different data segments\nfrom influencing each other. This design maxi-\nmizes training efficiency and enables Ziya2 to pos-\nsess powerful Chinese and coding knowledge.\nIn the second stage, we enhance Ziya2\u2019s perfor-\n7 Bits\n1 Bit\n5 Bits\n10 Bits\nFP16 \nBF16\n1 Bit\n8 Bits\nSign\nRange of Number\nPrecision of Number\nFigure 4: An illustration of the representations of float-\ning numbers by BF16 and FP16.\nmance on downstream tasks by adding supervised\ndata with Chinese and English instructions. Dif-\nferent from fine-tuning with instructions, we still\nemploy the LLM\u2019s next token prediction training\nmethod. Instead of randomly combining data as\nin the first stage, we concatenate the same type of\ninstruct data into a 4,096 context as a training ex-\nample, where the rest of the positions are filled by\nspecial pad tokens. To retain the knowledge already\nacquired by Ziya2, we also sample unsupervised\nChinese and English data in the same proportion\nas the instruct data for continual training.\nIn the third stage, we incorporate supervised data\nrelated to inference, such as Wanjuan-Ziya and\nMetaMath-Ziya, using the same composition ap-\nproach as in the second stage. After the third stage,\nZiya2 shows significant improvement in mathe-\nmatical reasoning abilities and exhibits noticeable\nenhancement in coding capabilities. This indicates\nthat mathematical reasoning skills are vital for tasks\n(e.g., coding) that require reasoning.\nOptimizer\nWe train the model using the AdamW\n(Loshchilov and Hutter, 2017) optimizer with the\nfollowing hyperparameters: \u03b21 = 0.9, \u03b22 = 0.95.\nIn our findings, due to the incorporation of ad-\nditional Chinese and code data in our dataset\ncompared to LLaMA2, there exists a disparity\nin the overall data distribution.\nAs a result, a\nmore extended warmup is beneficial for continual\npre-training. Consequently, instead of the 0.4%\nwarmup ratio utilized in LLaMA2, we adopt a\nwarmup ratio of 1%. Followed by a cosine learn-\ning rate decay schedule, reaching a final learning\nrate of 1e\u22125. Additionally, we implement a weight\ndecay of 0.1 and gradient clipping set at 1.0.\nTraining\nEfficiency\nWe\nutilize\nMegatron\n(Shoeybi et al., 2019) and DeepSpeed as founda-\ntional frameworks. Megatron ensures distributed\ntraining of large-scale models through data\nparallelism,\ntensor parallelism,\nand pipeline\nparallelism. Concurrently, the ZeRO (Rajbhandari\net al., 2020) technology from DeepSpeed is\nFigure 5: The pre-training loss of Ziya2-13B and Ziya-\n13B with respect to the number of training steps.\nemployed to optimize memory savings. Building\non this, we have implemented adaptations for\nthe LLaMA2 model structure and incorporated\nadvanced techniques such as flash-attention (Dao\net al., 2022) and fused-softmax to enhance training\nefficiency.\nThis results in an industry-leading\nefficiency of 163.0 TFLOPS per GPU per second\nand thus guarantees efficient training.\nTraining Stability\nZiya-13B adopts FP16 mixed-\nprecision training (Micikevicius et al., 2017) and\nfrequently encounters loss spike issues in the later\nstages of training. Upon analysis, we identify that\nthe limited numerical range of FP16 leads to over-\nflow problems, especially in operations such as\nsoftmax. As illustrated in Figure 4, in comparison\nto FP16, BF16 offers superior precision. Hence, for\nthe continual pre-training of Ziya2-13B, we opt for\nBF16 mixed-precision training. Simultaneously,\nwe rectify some underlying bugs in DeepSpeed to\nensure the stability of training under BF16 preci-\nsion, addressing the loss spike challenges. This\nensures convergence even when training continues\nfor 700B tokens, as shown in Figure 5.\n3\nEvaluation\nWe evaluate Ziya2 on benchmark datasets and com-\npare its performance with state-of-the-art LLMs,\nwhere the results and further analysis of data effi-\nciency demonstrate the effectiveness of Ziya2 with\nthe proposed data-centric learning process. The\ndetails are illustrated in the following text.\n3.1\nDatasets\nFollowing existing studies (Zeng et al., 2022; Tou-\nvron et al., 2023b; Yang et al., 2023), we evaluate\nZiya2 on six benchmark datasets using OpenCom-\npass (Contributors, 2023), where we perform 5-\nshot in experiments and use the official evaluation\nscripts to compute the accuracy. The details of the\ndatasets are illustrated as follows.\n\u2022 MMLU (Massive Multitask Language Under-\nstanding) (Hendrycks et al., 2020) provides a\nSource\nModel\nMMLU\nCMMLU\nC-Eval\nGSM8K\nMATH\nHumanEval\nClosed-source\nGPT-4\n83.93\n70.33\n68.40\n89.99\n40.20\n69.51\nGPT-3.5-turbo\n68.54\n54.06\n51.10\n57.77\n13.96\n52.44\nOpen-source\nChatGLM2-6B\n47.86\n49.30\n51.70\n28.89\n6.40\n9.15\nFalcon-7B\n26.03\n25.66\n24.23\n5.46\n1.68\n-\nVicuna-13B\n52.00\n36.28\n32.80\n28.13\n4.36\n16.46\nXVERSE-13B\n55.21\n58.44\n53.70\n18.20\n2.18\n15.85\nWeMix-13B\n59.70\n42.60\n42.70\n45.20\n6.90\n24.40\nBaichuan2-13B\n59.17\n61.97\n58.10\n52.77\n10.08\n17.07\nLLaMA2-13B\n55.74\n37.90\n34.99\n28.81\n4.98\n15.24\nOurs\nZiya-13B\n43.88\n31.09\n28.97\n17.97\n3.08\n14.02\nZiya2-13B\n61.36\n60.95\n58.84\n68.46\n10.98\n28.87\nTable 3: Comparison of Ziya2 with other closed-source and open-source LLMs on six benchmark datasets for LLM\nevaluation, where the boldface indicates the best-performing result over all open-source LLMs.\ncomprehensive evaluation of models in both zero-\nshot and few-shot settings, spanning across a\nwide range of 57 subjects. The unique aspect of\nMMLU is that it tests the models\u2019 world knowl-\nedge and problem-solving capabilities.\n\u2022 CMMLU (Chinese Massive Multitask Language\nUnderstanding) (Li et al., 2023) is an extensive\nevaluation suite tailored to measure the advanced\nknowledge and reasoning skills of LLMs in the\ncontext of the Chinese language and culture. It\ncontains a broad spectrum of 67 topics, ranging\nfrom basic to advanced professional levels.\n\u2022 C-Eval (Chinese Evaluation) (Huang et al.,\n2023b) is a thorough evaluation suite specifically\ndesigned for foundational models in Chinese. It\nis composed of 13,948 multiple-choice questions\nthat cover a wide array of 52 different disciplines\nand span across four levels of difficulty.\n\u2022 GSM8K (Grade School Math 8K) (Cobbe et al.,\n2021) is a collection of 8,500 high-quality math\nword problems created by human writers. The\nobjective of this dataset is to facilitate the task\nof question answering on fundamental problems\nthat necessitate reasoning through multiple steps.\n\u2022 MATH (Hendrycks et al., 2021) aggregates\n12,500 intricate competition mathematics prob-\nlems. A unique feature of this dataset is that each\nproblem comes with a comprehensive step-by-\nstep solution. These detailed solutions serve as a\nvaluable resource for teaching models to generate\nderivation processes and explanations.\n\u2022 HumanEval (Chen et al., 2021) is a meticulously\nconstructed set of 164 programming challenges.\nThis dataset serves as a benchmark for evalu-\nating the ability of a system to generate func-\ntionally correct programs based on provided doc\nstrings. The challenges encompass a range of\ntopics, including language comprehension, algo-\nrithmic problems, and basic mathematics.\n3.2\nBaselines\nWe choose several open-source models with similar\nsizes to Ziya2 as baselines.\n\u2022 ChatGLM2-6B (Zeng et al., 2022) is developed\nby Tsinghua University and Zhipu AI. The model\nis the second generation of the bilingual dialogue\nmodel ChatGLM-6B.\n\u2022 Falcon-7B (Penedo et al., 2023) is a causal\ndecoder-only model developed by Technology\nInnovation Institute (TII). It\u2019s trained on 1,500\nbillion tokens of RefinedWeb.\n\u2022 Vicuna-13B (Chiang et al., 2023) is an open-\nsource LLM developed by the Language Model\nSystems (LMSYS) organization and is fine-tuned\nfrom LLaMA on user-shared conversations be-\ntween humans and ChatGPT.\n\u2022 Baichuan2-13B (Yang et al., 2023) is devel-\noped by Baichuan Intelligent Technology and\nis trained with 2.6 trillion tokens.\n\u2022 XVERSE-13B9 is developed by XVERSE Tech-\nnology and is trained with 1.4 trillion tokens.\n\u2022 WeMix-13B10 is developed by Shanghai AI lab\nand is built on LLaMA2-Accessory.\n\u2022 LLaMA2-13B (Touvron et al., 2023b) is Meta\u2019s\nopen-source large language model. It is designed\n9https://github.com/xverse-ai/XVERSE-13B.\n10https://github.com/Alpha-VLLM/WeMix-LLM.\nFigure 6: The performance (i.e., accuracy) of Ziya2 on the six benchmark datasets with respect to the three training\nstages. The training process in different stages is represented by the number of tokens used to train Ziya2 at a\nparticular training step. The performance of LLaMA2 on the datasets is illustrated by dashed lines for reference.\nfor dialogue scenarios and is freely available for\nresearch and commercial use.\n\u2022 Ziya-13B (Zhang et al., 2022) is continue pre-\ntrained on the LLaMA-13B model and performs\nwell on many downstream tasks.\nWe also choose closed-source models GPT-3.5-\nturbo and GPT-4 (OpenAI, 2023) as references.\n\u2022 GPT-3.5-turbo is a high-performance variant of\nGPT-3 and it is proficient in text completion.\n\u2022 GPT-4 (OpenAI, 2023) is the latest state-of-\nthe-art LLM developed by OpenAI. It exhibits\nhuman-level performance on various professional\nand academic benchmark datasets.\n3.3\nBenchmark Results\nThe results of different LLMs on benchmark eval-\nuations are shown in Table 3 with several obser-\nvations.11 First, Ziya2 significantly outperforms\nLLaMA2 and other representative open-source\n11In Appendix B, we present examples produced by Ziya2\nacross three major domains: Chinese, English, and code.\nLLMs on all general Chinese and English datasets\n(i.e., C-Eval and MMLU). Remarkably, Ziya2\u2019s\nChinese proficiency is even superior than that of\nGPT-3.5-turbo. These results underscore the effi-\ncacy of our continual pre-training method, which\nnot only enhances LLaMA2\u2019s English capabilities\nand mitigates catastrophic forgetting but also sub-\nstantially boosts its performance on Chinese tasks,\nwhich LLaMA2 does not focus on. Second, for\nspecific downstream tasks, Ziya2 demonstrates re-\nmarkable mathematical and coding capabilities,\nwhere it outperforms all open-source LLMs, es-\npecially LLaMA2, and obtains better performance\nthan GPT-3.5-turbo on GSM8K. In technical mea-\nsuring, Ziya2 outperforms LLaMA2 by 40%, 6%,\nand 13% on GSM8K, MATH, and HumanEval\ndatasets, respectively. These results indicate that\ncontinual pre-training not only enhances the perfor-\nmance of LLMs on general tasks, but also consis-\ntently improves their performance in specific fields.\n3.4\nData Efficiency\nTo investigate data efficiency in training Ziya2, we\npresent the performance (i.e., accuracy) of Ziya2 on\nsix datasets with respect to the three training stages\nin Figure 6, where the results from LLaMA2 is\nrepresented by horizontal dashed lines for compari-\nsion. In doing so, we use the number of tokens that\nhave been used to train Ziya2 at a particular train-\ning step to represent particular training periods, so\nas to better illustrate the effect of data on LLM per-\nformance. The following description presents the\nobservations from the three training stages, which\nemphasize the contribution of high-quality data to\nthe promising performance of Ziya2.\nDuring the first training stage, initially, the per-\nformance of Ziya2 on MMLU degrades owing to\nthe inclusion of a large amount of Chinese corpus\nin the training data that different from the setting\nof LLaMA2. With training step increases, Ziya2\nlearns from a broader view from more data, which\nenhances its capabilities in both Chinese and En-\nglish text processing. Particularly, in Chinese tasks\nthat LLaMA2 is not optimized for, new data signif-\nicantly improves Ziya2\u2019s performance on CMMLU\nand C-Eval benchmarks. In the meantime, a modest\nenhancement is synchronously observed in Ziya2\u2019s\nmathematical and programming abilities.\nIn the second stage of continual pre-training,\nZiya2 exhibits a more substantial enhancement\non six benchmarks relative to the first stage, with\nnotable advancements on C-Eval, GSM8K, and\nMATH. These results underscore the greater con-\ntribution of supervised data over unsupervised data\nto LLM pre-training. Therefore, employing super-\nvised data for pre-training is able to reduce the num-\nber of training steps, thereby economizing costs.\nIn the third stage, we observe that using the Meta-\nMath dataset for training significantly enhances\nthe performance of Ziya2 on GSM8K and MATH,\nwhile maintaining its performance on general tasks.\nOn one hand, the experimental results prove that\ndata augmentation specific to a particular dataset is\nable to significantly boost the model\u2019s performance\non that dataset. On the other hand, such an \u201cef-\nfortless\u201d enhancement in model performance may\nnot necessarily be beneficial, as the model might\nmerely learn the format of the problems, rather than\ngenuinely improving its mathematical capabilities.\n4\nRelated Work\nIn the past decade, text representation serves as the\nfundamental of NLP and its quality determines how\nwell a model is able to perform on downstream\ntasks. Along the road of text representation re-\nsearch, pre-trained word embeddings and LLMs\nhave demonstrated their effectiveness in providing\nhigh-quality text representations and thus achieved\noutstanding performance on many NLP tasks for\nyears (Mikolov et al., 2013; Pennington et al., 2014;\nSong et al., 2017, 2018; Radford et al., 2019; De-\nvlin et al., 2019; Liu et al., 2019b; Diao et al., 2020;\nSong et al., 2021; Tian et al., 2022; Yang et al.,\n2022). Among LLMs, decoder-only ones from\nGPT-3 (Brown et al., 2020) represents a significant\njourney in AI, since it demonstrates impressive\nzero- and few-shot learning performance by using\nremarkably large model size (i.e., 175B parame-\nters). InstructGPT (Ouyang et al., 2022) further\nimproves GPT-3 by supervised fine-tuning and rein-\nforcement learning from human feedback (RLHF),\nwhere human-annotated data is used to train the\nLLM. Then, ChatGPT (OpenAI, 2022) is released\nas a general conversational model attracting much\nattention from the public since its noticeable capa-\nbilities in processing text. Later, GPT-4 (OpenAI,\n2023) presents huge improvements over its prede-\ncessors and provides multimodel ability that able\nto process images and texts whenever needed.\nIn order to better utilize and study LLMs, people\nare committed to developing open-source models\nsince GPT-3, instructGPT, ChatGPT, and GPT-4\nare all not publicly available. LLaMA (Touvron\net al., 2023a) and LLaMA2 (Touvron et al., 2023b)\nare two representative publicly available LLMs\nthat have been used in many downstream tasks,\nLLaMA2 is trained mainly on English data with\nmore tokens applied than LLaMA, and extends to\ngenerate longer text. In response to the subpar per-\nformance of open-source models such as LLaMA\nseries, many researchers propose LLMs for Chi-\nnese text processing. For instance, Chinese-Vicuna\n(Fan et al., 2023) is a low-cost Chinese dialogue\nmodel based on LLaMA. ChatGLM2 (Zeng et al.,\n2022) is another LLM that supports dialogue in\nboth Chinese and English, which is specifically\noptimized for Chinese dialogue tasks, achieving ex-\ncellent results in Chinese question-answering and\ndialogue tasks. Baichuan2 (Yang et al., 2023) is an\nLLM that supports Chinese and has achieved com-\nmendable results on multi-disciplinary benchmarks\nin both Chinese and English. Compared with ex-\nisting LLMs for Chinese, Ziya2 aligns with the\nmodel structure of LLaMA2, facilitating seamless\nintegration with the established LLaMA ecosystem.\nThis alignment ensures the adaptivity of Ziya2 on\nboth academic and industrial purposes. In contrast\nto the Chinese LLMs that are trained extensively\nfrom scratch, Ziya2 achieves the same objectives of\nenhanced Chinese language comprehension, code\ngeneration, and mathematical reasoning through\nsignificant effective continual pre-training with\ndata-centric optimizations. Our approach not only\nsubstantiates the efficacy of continual pre-training\nbut also showcases its efficiency in augmenting the\ncapabilities of existing models.\n5\nConclusion\nIn this paper, we propose Ziya2, an open-source\nLLM with 13 billion parameters for Chinese and\nEnglish text processing. Ziya2 is based on the\nopen-source LLaMA2 model and is continually\npre-trained through the proposed data-centric learn-\ning approach. Specifically, we collect and clean\nopen-source data from the internet, developing a\ncomprehensive data processing system and accu-\nmulating terabytes of high-quality data, which is\nused to train Ziya2 through three stages. The per-\nformance of Ziya2 in Chinese and English down-\nstream tasks not only surpasses LLaMA2 but also\noutperforms contemporaneous open-source LLMs\nof similar size, which demonstrates the effective-\nness of the data-centric learning approach for LLM\npre-training. In the future, we plan to continue\ntraining Ziya2, explore larger models with 70B\nparameters, and align Ziya2 to achieve better in-\nstructional compliance with the Ziya2-Chat model.\nWe aim to release specialized LLMs for various do-\nmains such as writing, coding, and multimodality.\nReferences\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. PaLM 2 Technical Report. arXiv\npreprint arXiv:2305.10403.\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit,\nLaria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An Open-\nSource Autoregressive Language Model.\narXiv\npreprint arXiv:2204.06745.\nBurton H Bloom. 1970. Space/time Trade-offs in Hash\nCoding with Allowable Errors. Communications of\nthe ACM, 13(7):422\u2013426.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language Models are Few-shot\nLearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nMoses S Charikar. 2002. Similarity Estimation Tech-\nniques from Rounding Algorithms. In Proceedings\nof the thiry-fourth annual ACM symposium on Theory\nof computing, pages 380\u2013388.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021.\nEvaluating Large Lan-\nguage Models Trained on Code.\narXiv preprint\narXiv:2107.03374.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez,\net al. 2023.\nVicuna: An Open-source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nhttps://vicuna.lmsys.org (accessed 14 April 2023).\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training Verifiers to Solve Math\nWord Problems. arXiv preprint arXiv:2110.14168.\nTogether Computer. 2023.\nRedPajama: An Open\nDataset for Training Large Language Models.\nhttps://github.com/togethercomputer/RedPajama-\nData.\nOpenCompass Contributors. 2023. Opencompass: A\nUniversal Evaluation Platform for Foundation Mod-\nels.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and\nChristopher R\u00e9. 2022.\nFlashAttention:\nFast\nand Memory-efficient Exact Attention with IO-\nAwareness. Advances in Neural Information Pro-\ncessing Systems, 35:16344\u201316359.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL-HLT (1), pages 4171\u20134186.\nAssociation for Computational Linguistics.\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and\nYonggang Wang. 2020. ZEN: Pre-training Chinese\nText Encoder Enhanced by N-gram Representations.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 4729\u20134740.\nZhenyi Fan, Chenghao Lu, and Jie Tian. 2023. Chinese-\nVicuna: A Chinese Instruction-following LLaMA-\nbased Model.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe Pile: An 800GB Dataset of Diverse Text for Lan-\nguage Modeling. arXiv preprint arXiv:2101.00027.\nConghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin\nWang, Wei Li, Hang Yan, JiaQi Wang, and Dahua\nLin. 2023. Wanjuan: A Comprehensive Multimodal\nDataset for Advancing English and Chinese Large\nModels. arXiv preprint arXiv:2308.10755.\nKenneth Heafield. 2011. KenLM: Faster and Smaller\nLanguage Model Queries. In Proceedings of the sixth\nworkshop on statistical machine translation, pages\n187\u2013197.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring Massive Multitask Language Un-\nderstanding. arXiv preprint arXiv:2009.03300.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring Mathematical\nProblem Solving with the MATH Dataset. arXiv\npreprint arXiv:2103.03874.\nYongfeng Huang, Yanyang Li, Yichong Xu, Lin Zhang,\nRuyi Gan, Jiaxing Zhang, and Liwei Wang. 2023a.\nMVP-Tuning: Multi-View Knowledge Retrieval with\nPrompt Tuning for Commonsense Reasoning. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 13417\u201313432, Toronto, Canada.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei\nZhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, et al. 2023b.\nC-Eval: A Multi-level Multi-discipline Chinese Eval-\nuation Suite for Foundation Models. arXiv preprint\narXiv:2305.08322.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020.\nSpan-\nBERT: Improving Pre-training by Representing and\nPredicting Spans. Transactions of the Association\nfor Computational Linguistics, 8:64\u201377.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2021. Deduplicating Training\nData Makes Language Models Better. arXiv preprint\narXiv:2107.06499.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai\nZhao, Yeyun Gong, Nan Duan, and Timothy Bald-\nwin. 2023. CMMLU: Measuring Massive Multitask\nLanguage Understanding in Chinese. arXiv preprint\narXiv:2306.09212.\nMiaofeng Liu, Yan Song, Hongbin Zou, and Tong\nZhang. 2019a. Reinforced Training Data Selection\nfor Domain Adaptation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1957\u20131968, Florence, Italy.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly Optimized Bert Pretraining Ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2017.\nDecou-\npled Weight Decay Regularization. arXiv preprint\narXiv:1711.05101.\nJunyu Lu, Ping Yang, Ruyi Gan, Jing Yang, and Ji-\naxing Zhang. 2022.\nUnified BERT for Few-shot\nNatural Language Understanding. arXiv preprint\narXiv:2206.12094.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gre-\ngory Diamos, Erich Elsen, David Garcia, Boris Gins-\nburg, Michael Houston, Oleksii Kuchaiev, Ganesh\nVenkatesh, et al. 2017. Mixed Precision Training.\narXiv preprint arXiv:1710.03740.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013.\nEfficient Estimation of Word\nRepresentations in Vector Space.\narXiv preprint\narXiv:1301.3781.\nOpenAI. 2022. Introducing ChatGPT.\nOpenAI. 2023. GPT-4 Technical Report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training Language Models to Follow Instruc-\ntions with Human Feedback. Advances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The Refinedweb Dataset\nfor Falcon LLM: Outperforming Curated Corpora\nwith Web Data, and Web Data Only. arXiv preprint\narXiv:2306.01116.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global Vectors for Word\nRepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532\u20131543, Doha, Qatar.\nYang Ping, JunYu Lu, Ruyi Gan, Junjie Wang, Yuxi-\nang Zhang, Pingjian Zhang, and Jiaxing Zhang. 2023.\nUniEX: An Effective and Efficient Framework for\nUnified Information Extraction via a Span-extractive\nPerspective. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 16424\u201316440,\nToronto, Canada.\nHan Qin, Yuanhe Tian, and Yan Song. 2021. Rela-\ntion Extraction with Word Graphs from N-grams.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2860\u20132868, Online and Punta Cana, Dominican Re-\npublic.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nModels are Unsupervised Multitask Learners. Ope-\nnAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Unified Text-to-text\nTransformer. The Journal of Machine Learning Re-\nsearch, 21(1):5485\u20135551.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and\nYuxiong He. 2020. Zero: Memory Optimizations to-\nward Training Trillion Parameter Models. In SC20:\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1\u2013\n16. IEEE.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015.\nNeural Machine Translation of Rare\nWords with Subword Units.\narXiv preprint\narXiv:1508.07909.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training Multi-billion Pa-\nrameter Language Models using Model Parallelism.\narXiv preprint arXiv:1909.08053.\nYan Song, Chia-Jung Lee, and Fei Xia. 2017. Learn-\ning Word Representations with Regularization from\nPrior Knowledge. In Proceedings of the 21st Confer-\nence on Computational Natural Language Learning\n(CoNLL 2017), pages 143\u2013152.\nYan Song, Shuming Shi, and Jing Li. 2018. Joint Learn-\ning Embeddings for Chinese Words and Their Com-\nponents via Ladder Structured Networks. In Proceed-\nings of the 27th International Joint Conference on\nArtificial Intelligence, pages 4375\u20134381.\nYan Song, Tong Zhang, Yonggang Wang, and Kai-Fu\nLee. 2021. ZEN 2.0: Continue Training and Adap-\ntion for N-gram Enhanced Text Encoders. arXiv\npreprint arXiv:2105.01279.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu. 2021. Roformer: En-\nhanced Transformer with Rotary Position Embedding.\narXiv preprint arXiv:2104.09864.\nYuanhe Tian, Weidong Chen, Bo Hu, Yan Song, and\nFei Xia. 2023. End-to-end Aspect-based Sentiment\nAnalysis with Combinatory Categorial Grammar. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, pages 13597\u201313609, Toronto,\nCanada.\nYuanhe Tian, Yan Song, and Fei Xia. 2022. Improv-\ning Relation Extraction through Syntax-induced Pre-\ntraining with Dependency Masking. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics.\nKushal Tirumala, Daniel Simig, Armen Aghajanyan,\nand Ari S Morcos. 2023. D4: Improving Llm Pre-\ntraining Via Document De-duplication and Diversifi-\ncation. arXiv preprint arXiv:2308.12284.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLLaMA: Open and Effi-\ncient Foundation Language Models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b. LLaMA 2: Open Founda-\ntion and Fine-tuned Chat Models. arXiv preprint\narXiv:2307.09288.\nJunjie Wang, Yuxiang Zhang, Ping Yang, and Ruyi Gan.\n2022. Towards No. 1 in CLUE Semantic Match-\ning Challenge: Pre-trained Language Model Erlang-\nshen with Propensity-Corrected Loss. arXiv preprint\narXiv:2208.02959.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Ar-\nmand Joulin, and Edouard Grave. 2019. CCNet: Ex-\ntracting High Quality Monolingual Datasets from\nWeb Crawl Data. arXiv preprint arXiv:1911.00359.\nShaohua Wu, Xudong Zhao, Tong Yu, Rongguo\nZhang, Chong Shen, Hongli Liu, Feng Li, Hong\nZhu, Jiangang Luo, Liang Xu, et al. 2021. Yuan\n1.0: Large-scale Pre-trained Language Model in\nZero-shot and Few-shot Learning. arXiv preprint\narXiv:2110.04725.\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong\nZhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,\nDong Yan, Fan Yang, et al. 2023.\nBaichuan 2:\nOpen Large-scale Language Models. arXiv preprint\narXiv:2309.10305.\nPing Yang, Junjie Wang, Ruyi Gan, Xinyu Zhu, Lin\nZhang, Ziwei Wu, Xinyu Gao, Jiaxing Zhang,\nand Tetsuya Sakai. 2022.\nZero-Shot Learners\nfor Natural Language Understanding via a Uni-\nfied Multiple Choice Perspective.\narXiv preprint\narXiv:2210.08590.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu,\nZhengying Liu, Yu Zhang, James T Kwok, Zhenguo\nLi, Adrian Weller, and Weiyang Liu. 2023. Meta-\nmath: Bootstrap Your Own Mathematical Ques-\ntions for Large Language Models. arXiv preprint\narXiv:2309.12284.\nSha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding,\nXiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie\nTang. 2021. Wudaocorpora: A Super Large-scale\nChinese Corpora for Pre-training Language Models.\nAI Open, 2:65\u201368.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. GLM-130B: An\nOpen Bilingual Pre-trained Model. arXiv preprint\narXiv:2210.02414.\nBiao Zhang and Rico Sennrich. 2019.\nRoot Mean\nSquare Layer Normalization. Advances in Neural\nInformation Processing Systems, 32.\nJiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang,\nLin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiao-\nqun Dong, Junqing He, et al. 2022. Fengshenbang\n1.0: Being the Foundation of Chinese Cognitive In-\ntelligence. arXiv preprint arXiv:2209.02970.\nA\nPretraining Data\nThe following charts presents examples of unsupervised and supervised training data.\nEnglish Unsupervised Data\nText\nABC News\u2019 Good Morning America outstripped NBC News\u2019 Today by 761,000 viewers and 279,000 news demo viewers\nthe week of April 7. It\u2019s GMA\u2018s seventh consecutive week on top of the morning infotainment show race in both metrics,\nand its largest demo margin in three months. GMA has ranked No. 1 in overall audience for 89 of the past 93 weeks,\nand No. 1 in the news demo for 25 of this season\u2019s 29 weeks to date. Today meanwhile, boasted it finished first with the\nyounger, 18-49 year old age bracket, for the 42nd consecutive week. Today is on top of the ratings in the day part with\nmen 25-54 this season, NBC noted \u2014 as well as adults, men and women 18-49. Today has posted seven consecutive\nmonths of ratings growth in total viewers, and both the 25-54 and 18-49 demos which NBC says is the show\u2019s biggest\nratings uptick since \u201997. For the week, GMA clocked 5.617 million viewers \u2014 2.212 million in the demo. Today logged\n4.856 million viewers \u2014 1.933 million in the demo. GMA bested CBS This Morning\u2018s 3.041 million viewers \u2014 956,000\nin the news demo.\nChinese Unsupervised Data\nText\n\u4f60\u8d70\u4e86\uff0c\u5e26\u7740\u6211\u4eec\u8fd8\u6765\u4e0d\u53ca\u5f00\u59cb\u7684\u7231\u3002\u4f60\u8bf4\u6211\u4e0d\u591f\u52c7\u6562\u4e0e\u679c\u65ad\u3001\u8bf4\u6211\u53ea\u662f\u628a\u559c\u6b22\u5bc4\u653e\u5728\u4f60\u90a3\u8fb9\u5374\u4ece\u6765\u6ca1\u6709\n\u60f3\u7ec6\u5fc3\u704c\u6e89\u3002\u4f46\u6211\u8981\u600e\u4e48\u80fd\u704c\u6e89\u5462\uff1f\u4f60\u5c31\u50cf\u662f\u4e00\u6c83\u5bcc\u9976\u7684\u571f\u5730\uff0c\u5927\u5bb6\u603b\u628a\u4ed6\u4eec\u7684\u7231\u57cb\u5728\u4f60\u7684\u5fc3\u7a9d\u3002\u5733\u6c34\u5f15\n\u6765\u7684\u6ecb\u6da6\u6765\u4e0d\u53ca\u6d12\u843d\u5728\u6211\u9700\u8981\u4f60\u7684\u90a3\u4e9b\u5c81\u6708\uff0c\u6211\u5411\u7740\u6709\u4f60\u5728\u7684\u9633\u5149\u8d8b\u53bb\uff0c\u5374\u5982\u4f0a\u85e4\u6da6\u4e8c\u7b14\u4e0b\u7684\u6f29\u6da1\uff0c\u53ea\u4e0d\n\u8fc7\u6210\u4e86\u8ba9\u4eba\u5bb3\u6015\u7684\u626d\u66f2\u602a\u72b6\u3002{...} \u6240\u4ee5\u8c22\u8c22\u6211\u66fe\u7ecf\u5728\u4f60\u5fc3\u91cc\u7684\u4f4d\u7f6e\u90a3\u4e48\u524d\u9762\u3001\u90a3\u4e48\u9760\u8fd1\u4f60\u5fc3\u7684\u5730\u65b9\u3002\nYou\u2019re gone, with the love we haven\u2019t had time to start yet. You say I am not brave and decisive enough, and you say I\njust place my love on your side but never want to irrigate it carefully. But how can I irrigate it? You are like a fertile\nland, where everyone always buries their love in your heart. The nourishment brought by the water in the canal did not\nfall on the years when I needed you. I moved towards the sunshine with you, but like a vortex in the pen of Runji Ito, it\nonly became a frightening twisted and strange shape {...} So thank you for being so in front of you and so close to your\nheart.\nEnglish Supervised Data\nText\nProblem:\nHuman rights are moral principles or norms, which describe certain standards of human behaviour{...} Military aircraft\ncan be either combat or non-combat: TurboJET is the brand name for the operations of the Hong Kong-headquartered\nShun Tak-China Travel Ship Management Limited, which was established from the joint venture between Shun Tak\nHoldings Limited and China Travel International Investment Hong Kong Limited in July 1999. It operates hydrofoil\nferry services in southern China. Question:use beechcraft starship\nA.military\nB.general aviation\nC.service\nAnswer:B.general aviation\nChinese Supervised Data\nPrompt\n\u95ee\u9898:{\u95ee\u9898\u548c\u5185\u5bb9} \u56de\u7b54:{\u56de\u7b54\u7684\u5185\u5bb9}\nQuestion:{the question} Answer:{the answer}\nText\n\u95ee\u9898\uff1a\n\u8bf7\u95ee\u4e0b\u9762\u63cf\u8ff0\u5c5e\u4e8e\u54ea\u4e00\u79cd\u4e8b\u4ef6\u7c7b\u578b\uff1f\u6587\u7ae0\uff1a\u6628\u665a\uff0c\u5728\u53f7\u79f0\u201c\u4e9a\u6d32\u7b2c\u4e00\u9b54\u9b3c\u4e3b\u573a\u201d\u7684\u5929\u6cb3\u4f53\u80b2\u573a\uff0c\u56fd\u8db3\u5728\u5360\u636e\n\u5929\u65f6\u5730\u5229\u4eba\u548c\u7684\u60c5\u51b5\u4e0b\uff0c\u534a\u573a\u5c31\u51fb\u57ae\u5bf9\u624b\uff0c\u72c2\u8f706\u7403{...} \u8fd9\u6837\u7684\u6218\u7ee9\u548c\u8868\u73b0\u4e5f\u5728\u8d5b\u540e\u5f15\u53d1\u4e86\u5de8\u5927\u7684\u4e89\u8bae\u3002\nA.\u80dc\u8d1f\nB.\u52a0\u606f\nC.\u878d\u8d44\nD.\u7206\u70b8\n\u56de\u7b54\uff1a\u7b54\u6848\uff1aA.\u80dc\u8d1f\nQuestion:\nMay I ask which type of event does the following description belong to? Article: Last night, at the Tianhe Stadium,\nwhich is known as the \"Number One Devil\u2019s Stadium in Asia,\" the Chinese football team occupied Under the favorable\nconditions of timing, location, and people, he defeated his opponent in half time and roared 6 goals {...} Such\nachievements and performances have also sparked great controversy after the game.\nA.Success or Failure\nB.Raise Interest Rates\nC.Financing\nD.Explosion\nAnswer: The answer is\uff1aA.Success or Failure\nCode\nPrompt\n\u02cb\u02cb\u02cb (sql / python / c / etc.) + {Code} \u02cb\u02cb\u02cb\nText\n\u02cb\u02cb\u02cb SQL\nALTER TABLE kooste.tamperewfs_luontorastit DROP CONSTRAINT tamperewfs_luontorastit_pk;\nALTER TABLE kooste.tamperewfs_luontorastit DROP CONSTRAINT tunnus_rasti_unique;\nALTER TABLE kooste.tamperewfs_luontorastit ADD COLUMN kooste_id bigint NOT NULL GENERATED\nALWAYS AS IDENTITY;...{Code~}\n\u02cb\u02cb\u02cb\nMetaMath\nPrompt\nQuestion/Q:{the question} Answer/A:{the answer}\nText\nQuestion:\nThe coffee shop sold 8 more than four times the number of lattes as it did teas. If 6 teas were sold how many lattes did\nthe coffee shop sell?\nAnswer:\nIf the coffee shop sold 6 teas, then the number of lattes sold is 8 more than four times the number of teas. Four times\nthe number of teas is 4 * 6 = 24. So, the number of lattes sold is 24 + 8 = 32. The answer is: 32\nB\nGeneration Cases\nThe following charts presents examples generated by Ziya2-13B with particular inputs.\nChinese Generation\n\u6a21\u578b\u8f93\u5165: \u6211\u56fd\u7684\u4e09\u7687\u4e94\u5e1d\u5206\u522b\u6307\u7684\u662f\u8c01\uff1f\nModel Input: Who are the \u201cThree Emperors and Five Sovereigns\u201d of our country?\nZiya2-13B Generation\n\"\u4e09\u7687\u4e94\u5e1d\"\u662f\u6307\u6211\u56fd\u53e4\u4ee3\u90e8\u843d\u9996\u9886\uff0c\u5176\u4e2d\"\u4e09\u7687\"\u4e00\u822c\u8ba4\u4e3a\u6307\u7684\u662f\u71e7\u4eba\u3001\u4f0f\u7fb2\u3001\u795e\u519c\uff1b\u800c\"\u4e94\u5e1d\"\u4e00\u822c\u6307\u7684\u662f\u9ec4\n\u5e1d\u3001\u989b\u987c\u3001\u5e1d\u55be\u3001\u5c27\u3001\u821c\u3002\nThe \"Three Emperors and Five Sovereigns\" refer to ancient tribal leaders in China, among which the \"Three Emperors\"\nare generally believed to refer to Suiren, Fuxi, and Shennong; The \"Five Sovereigns\" generally refer to Huang Di,\nZhuanxu, Sovereigns Ku, Yao, and Shun.\nEnglish Generation\nModel Input: Friends is a series of sitcoms produced by NBC in the United States,\nZiya2-13B Generation\nand created by David Crane and Marta Kauffman. The series premiered on September 22, 1994, and ended on May 6,\n2004, lasting ten seasons. With an ensemble cast starring Jennifer Aniston, Courteney Cox, Lisa Kudrow, Matt LeBlanc,\nMatthew Perry and David Schwimmer, the show revolves around six friends in their 20s and 30s who live in Manhattan,\nNew York City. The series was produced by Bright Kids Productions, in association with Warner Bros. Television. The\noriginal executive producers were Kevin S. Bright, Marta Kauffman, and David Crane. Friends had received both\ncritical acclaim and popular success throughout its original ten-season run on NBC; it received accolades such as\nwinning the Emmy Award for Outstanding Comedy Series in 2002, and was ranked No. 25 on TV Guide\u2019s 50 Greatest\nTV Shows of All Time in 2002.\nCode Generation\n\u6a21\u578b\u8f93\u5165: \u7ed9\u5b9a\u4e00\u4e2a\u6574\u6570n\uff0c\u8ba1\u7b97\u6240\u6709\u5c0f\u4e8e\u7b49\u4e8en \u7684\u975e\u8d1f\u6574\u6570\u4e2d\u6570\u5b571 \u51fa\u73b0\u7684\u4e2a\u6570\u3002\u4f8b\u5982\uff1a\n\u8f93\u5165\uff1an = 13\n\u8f93\u51fa\uff1a6\n\u5206\u6790\u5982\u4e0b\uff1a\u8f93\u5165n=13\uff0c\u5c0f\u4e8e\u7b49\u4e8e13\u975e\u8d1f\u6574\u6570\u91cc\u9762\u5305\u542b1\u7684\u6570\u5b57\u4e3a1\uff0c10\uff0c11\uff0c12\uff0c13\uff0c\u6240\u4ee5\u6570\u5b571\u5171\u51fa\u73b0\n\u4e866\u6b21\u3002\u4e0b\u9762\u662f\u5177\u4f53\u7684python\u4ee3\u7801\uff1a\u02cb\u02cb\u02cbpython\nModel Input: Given an integer n, calculate the number of occurrences of the number 1 in all non negative\nintegers less than or equal to n. For example:\nInput: n=13\nOutput: 6\nThe analysis is as follows: input n=13, and the number containing 1 in a non negative integer less than or equal to 13 is\n1, 10, 11, 12, 13, so the number 1 appears together Six times. The following is the specific Python code: \u02cb\u02cb\u02cb Python\nZiya2-13B Generation\nimport re\ndef countDigitOne( self , n: int) \u2212> int :\ncount = 0\nfor i in range(1, n + 1):\ns = str (i)\nfor x in s:\nif x == '1' :\ncount += 1\nreturn count\n\u02cb\u02cb\u02cb\n"
  },
  {
    "title": "CogVLM: Visual Expert for Pretrained Language Models",
    "link": "https://arxiv.org/pdf/2311.03079.pdf",
    "upvote": "16",
    "text": "CogVLM: Visual Expert for Pretrained Language Models\nWeihan Wang * 1 \u2021 Qingsong Lv * 2 Wenmeng Yu 2 Wenyi Hong 1 \u2021 Ji Qi 1 \u2021 Yan Wang 2 Junhui Ji 2\nZhuoyi Yang 1 \u2021 Lei Zhao 2 Xixuan Song 1 \u2021 Jiazheng Xu 1 \u2021 Keqin Chen 3 \u2021 Bin Xu 1 Juanzi Li 1 Yuxiao Dong 1\nMing Ding 2 Jie Tang 1\nAbstract\nWe introduce CogVLM, a powerful open-source\nvisual language foundation model.\nDifferent\nfrom the popular shallow alignment method\nwhich maps image features into the input space\nof language model, CogVLM bridges the gap\nbetween the frozen pretrained language model\nand image encoder by a trainable visual ex-\npert module in the attention and FFN layers.\nAs a result, CogVLM enables a deep fusion\nof vision language features without sacrificing\nany performance on NLP tasks. CogVLM-17B\nachieves state-of-the-art performance on 17 clas-\nsic cross-modal benchmarks, including 1) im-\nage captioning datasets: NoCaps, Flicker30k, 2)\nVQA datasets: OKVQA, TextVQA, OCRVQA,\nScienceQA, 3) LVLM benchmarks:\nMM-\nVet, MMBench, SEED-Bench, LLaVABench,\nPOPE, MMMU, MathVista, 4) visual grounding\ndatasets: RefCOCO, RefCOCO+, RefCOCOg,\nVisual7W. Codes and checkpoints are available at\nhttps://github.com/THUDM/CogVLM.\n1. Introduction\nVision language models are versatile and powerful. Many\nvision and cross-modality tasks can be formulated as next\ntoken prediction, e.g., image captioning (Agrawal et al.,\n2019), visual question answering (Antol et al., 2015), visual\ngrounding (Yu et al., 2016) and even segmentation (Chen\net al., 2022a). Useful abilities like in-context learning (Tsim-\npoukelli et al., 2021; Sun et al., 2023a; Alayrac et al., 2022)\nalso emerge along with the improvement of downstream\ntasks when scaling up VLMs. However, to train a large\nlanguage model is already non-trivial, and it is more chal-\nlenging to train a VLM from scratch with the same NLP\n*Equal contribution\n1Tsinghua University 2Zhipu AI \u2021Done\nas intern at Zhipu AI 3Beihang University.\nCorrespondence\nto:\nMing Ding <ming.ding@zhipuai.cn>, Jie Tang < ji-\netang@tsinghua.edu.cn>.\nLLaVA-Bench\nPOPE\nSEED\nMMBench\nMM-Vet\nNoCaps\nScienceQA\nOKVQA\nTextVQA\nVQAv2\nFlickr\nRefCOCOg\nRefCOCO+\nRefCOCO\nMathVista\nMMMU\n41.75\n59.5\n77.25\n84.62\n86.25\n87.88\n57.5\n65.0\n72.5\n45.5\n61.0\n76.5\n31.5\n41.0\n50.5\n122.0\n125.0\n128.0\n67.0\n79.0\n91.0\n51.5\n58.0\n64.5\n53.5\n62.0\n70.5\n60.75\n71.5\n82.25\n75.0\n85.0\n95.0\n80.0\n85.0\n90.0\n79.5\n84.0\n88.5\n86.25\n89.5\n92.75\n26.25\n30.5\n34.75\n30.25\n35.5\n40.75\nN/A\nShikra\nQwen-VL\nLLaVA-1.5\nInstructBLIP\nmPLUG-Owl2\nUnified-IO2\nEmu2\nCogVLM\nFigure 1. The performance of CogVLM on a broad range of multi-\nmodal tasks in comparison with existing approaches.\nperformance as well-trained pure language models like\nLLaMA2 (Touvron et al., 2023). Therefore, it is natural\nto investigate how to train a VLM from an off-the-shelf\npretrained language model.\nThe popular shallow alignment methods represented by In-\nstructBLIP (Li et al., 2023b) and MiniGPT-4 (Zhu et al.,\n2023) connect a frozen pretrained vision encoder and lan-\nguage model via a trainable Q-Former or a linear layer,\nmapping the image features into the input embedding space\nof the language model. This method converges rapidly, but\nits performance is noticeably inferior to that of LLaVA-\n1.5 with trainable language parameters, despite their model\nsizes and training datasets being almost identical.\nThe primary challenge in the performance of shallow align-\nment methods within VLMs can be attributed to the lack\nof deep fusion between visual and linguistic data. Shallow\nalignment methods struggle because they rely on \u2018frozen\u2019\nlanguage model weights, which are intrinsically trained to\nprocess text tokens. This presents a significant mismatch\n1\narXiv:2311.03079v2  [cs.CV]  4 Feb 2024\nCogVLM: Visual Expert for Pretrained Language Models\nReferring Expression Comprehension\n Q: Describe this image in detail.\nCogVLM: The image showcases a railway platform labeled \n'Platform 2'. A green train is approaching, with steam \nemanating from its engine. On the platform, there are \nseveral people, including a woman in a pink shirt, a child \nwith a backpack, and a man in a green shirt. There's a \nchalkboard sign on the platform that reads: 'Please Use The \nFootbridge To Cross The Line If You Require Assistance. \nPlease Ask The Station Staff.' In the background, there's a \nsignal post and some trees.\nDetailed Description\n Q: Can you explain this meme?\nCogVLM: This meme is a humorous take on the \nidea that certain drugs, like cocaine, are believed \nto have no side effects. The meme uses a \ufb01ctional \nmeeting of various mythical creatures, each with a \nunique and exaggerated appearance, to represent \nthe different effects of cocaine. The text at the top \nof the image, 'Publicly, we are still saying there \nare no side effects,' is a statement often made by \npharmaceutical companies or health organizations \nabout certain drugs. The creatures in the meeting, \nwith their various and often comical expressions \nand postures, humorously represent the different \n'side effects' of cocaine, suggesting that the drug \nhas a wide range of unintended effects.\nMeme Understanding\n Q: From the perspective of a photographer, \nanalyze why this picture was taken well?\nCogVLM: From a photographer's perspective, several \nelements contribute to the quality of this image. The \ncomposition is balanced, with the hot air balloon taking up \na signi\ufb01cant portion of the frame, yet still allowing the \nsurrounding environment to be visible. The use of lighting, \nespecially the soft glow from the balloon and the car's \nheadlights, creates a warm and inviting atmosphere. The \ndepth of \ufb01eld is shallow, focusing on the subjects in the \nforeground while softly blurring the background, drawing \nattention to the main elements of the scene.\nVisual Reasoning\n Q: What is the difference of largest and \nsmallest bar? Answer it step by step.\nCogVLM: The largest bar represents Iceland with a \npercentage of 100%. The smallest bar represents \nCambodia with a percentage of 52.4%. To \ufb01nd the \ndifference, subtract the smallest percentage from the \nlargest: 100% - 52.4% = 47.6%.\nChart Understanding & Reasoning\n Q: How many goals did the person in the \npicture score in the 2018 World Cup?\nCogVLM: The person in the picture is Cristiano Ronaldo. \nHe is the captain of the Portuguese team and has \nparticipated in the 2018 World Cup. In the 2018 World \nCup, Cristiano Ronaldo scored 4 goals.\nWorld Knowledge\n Q1: What did Musk say?\nCogVLM: Elon Musk in the tweet said: \"Fight Recap: I \njoked on X about \ufb01ghting Zuck, Zuck then said, 'Send ME \nLOCATION.' Italy graciously offered a Colosseum, but \nZuck declined. I suggested his home as a \"safe space,\" but \ntragically, he was already \"traveling.\" Is there anywhere he \nwill \ufb01ght?\"\n Q2: Do you think Zuck is really traveling?\nCogVLM: Based on the content of the tweet, it seems that \nZuck is not really traveling, but rather pretending to be \ntraveling in order to avoid the con\ufb02ict.\nOCR-Free Reasoning\nCogVLM: A piggy bank [[302,198,694,857]] with a \nsmile [[400,520,582,643]] on it is surrounded by coins \n[[028,820,986,997]], toy houses [[052,697,174,892;\n526,763,636,952;168,697,286,883;738,697,852,895]], \nand greenery.\n Q: Can you provide a description of the image and include \nthe coordinates [[x0,y0,x1,y1]] for each mentioned object?\nGrounding with Caption\nCogVLM: Find the \ufb02owers [[328,519,436,780]]. Check the \ngirl holding it, got the girl [[116,069,450,927]]. Check the \nclothes of it, got the clothes [[134,309,418,852]]. The \nclothes [[134,309,418,852]] are blue. So the answer is blue.\n Q1: What color are the clothes of the girl whose \nhands are holding \ufb02owers? Let's think step by step.\nCogVLM: Find the not(\ufb02owers) hands [[654,637,710,728]]. \nCheck the girl with it, got the girl [[526,109,880,946]]. \nCheck the jeans of it, got the jeans [[530,613,880,871]]. The \njeans [[530,613,880,871]] are gray. So the answer is gray.\n Q2: What color are the jeans of the girl whose hands \nare not holding \ufb02owers? Let's think step by step.\nGrounding Visual Question Answering\nFigure 2. Samples generated by CogVLM.\n2\nCogVLM: Visual Expert for Pretrained Language Models\nFigure 3. MMLU score and training loss over multimodal pre-\ntraining phase. When directly training the language part of the\nVLM using the LAION dataset, the model\u2019s score on the pure text\ndataset MMLU rapidly decreases, dropping to 24.9 at 2500 steps.\nFigure 4. The architecture of CogVLM. (a) The illustration about\nthe input, where an image is processed by a pretrained ViT and\nmapped into the same space as the text features. (b) The Trans-\nformer block in the language model. The image features have a\ndifferent QKV matrix and FFN. Only the purple parts are trainable.\nissue, as visual features lack a direct equivalent in the tex-\ntual input space. Consequently, when these visual features\nundergo multi-layer transformations, they tend to deviate\nfrom the expected input distribution of the deeper language\nmodel layers. This misalignment is particularly evident in\ntasks like image captioning, where the specificity of a task\n\u2013 such as writing style and caption length \u2013 can only be\nsuperficially encoded into visual features through shallow\nmethods.\nA common strategy, as seen in PaLI (Chen et al., 2022b)\nand Qwen-VL (Bai et al., 2023), involves direct training\nof LLM during the pre-training or supervised fine-tuning\n(SFT) phase. However, this approach can compromise the\nmodels\u2019 generalizability, particularly for tasks focused on\ntextual outputs. Conventionally, LLMs are pretrained on\nextensive text-only datasets (Raffel et al., 2020), leading to\na significant divergence in data distribution when compared\nto image-text pair datasets like LAION (Schuhmann et al.,\n2022) and COYO (Byeon et al., 2022). This shift often\nresults in catastrophic forgetting, a phenomenon where the\nmodel\u2019s proficiency in its original domain deteriorates. This\nissue is evident in Figure 3, which shows a marked decline in\nMMLU (Hendrycks et al., 2020) score as the model becomes\nmore attuned to the LAION dataset, thus validating our\nhypothesis. This trend is not isolated; similar effects have\nbeen observed in models like PaLM-E (Driess et al., 2023)\nand Flamingo (Alayrac et al., 2022). For instance, adapting\nan 8B parameter language model for VLM pretraining can\nlead to an 87.3% reduction in natural language generation\n(NLG) performance (Driess et al., 2023).\nThe discussion above raises an important question: is it\npossible to retain the NLP capabilities of the large language\nmodel while adding top-notch visual understanding abilities\nto it?\nCogVLM gives a \u201cyes\u201d answer. CogVLM instead adds\na trainable visual expert to the language model. In each\nlayer, the image features in the sequence use a new QKV\nmatrix and MLP layer with the text features. Visual ex-\npert doubles the number of parameters while keeping the\nFLOPs the same. Since all the parameters in the original lan-\nguage model are fixed, the behaviors are the same as in the\noriginal language model if the input sequence contains no\nimage. This inspiration arises from the comparison between\nP-Tuning (Liu et al., 2023f) and LoRA (Hu et al., 2021) in\nefficient finetuning, where p-tuning learns a task prefix em-\nbedding in the input while LoRA adapts the model weights\nin each layer via a low-rank matrix. As a result, LoRA\nperforms better and more stable. A similar phenomenon\nmight also exist in VLM, because in the shallow alignment\nmethods, the image features act like the prefix embedding\nin P-Tuning.\nOur contributions in this work are as follows:\n\u2022 We introduce the CogVLM model, which deeply inte-\ngrates visual and linguistic features while retaining the\nfull capabilities of a pretrained large language model.\nCogVLM-17B, trained from Vicuna-7B, achieves state-\nof-the-art across 17 classic cross-modal benchmarks.\n\u2022 Through extensive ablation studies, we validated the\neffectiveness of our proposed visual expert module and\nthe importance of deep fusion. We further delved into\nmultiple critical factors in multimodal pertaining, in-\ncluding the scale of visual encoder, variants of attention\nmask, the most impactful parameters in VLMs, and the\nnecessity of incorporating self-supervised image loss,\netc.\n3\nCogVLM: Visual Expert for Pretrained Language Models\n\u2022 We have made the weights of CogVLM and the dataset\nused in the SFT phase available to the public. We\nanticipate that the open sourcing of CogVLM will sig-\nnificantly contribute to the research and industrial ap-\nplication of visual understanding.\n2. Method\n2.1. Architecture\nCogVLM model comprises four fundamental components:\na vision transformer (ViT) encoder, an MLP adapter, a pre-\ntrained large language model (GPT), and a visual expert\nmodule. Figure 4 shows an overview of the CogVLM ar-\nchitecture. The components\u2019 design and implementation\ndetails are provided below:\nViT encoder. We utilize pretrained EVA2-CLIP-E (Sun\net al., 2023b) in CogVLM-17B. Note that the final layer of\nViT encoder is removed because it specializes in aggregating\nthe [CLS] features for contrastive learning.\nMLP adapter. To map the output of ViT into the same\nspace as the text features from word embedding, we use an\nMLP adapter, a two-layer MLP (SwiGLU (Shazeer, 2020)).\nFor implementation convenience, all image features share\nthe same position id in the language model.\nPretrained large language model. CogVLM\u2019s model de-\nsign is compatible with any off-the-shelf GPT-style pre-\ntrained large language model. Specifically, CogVLM-17B\nadopts Vicuna1.5-7B (Chiang et al., 2023) for further train-\ning. A causal mask is applied to all the attention operations,\nincluding the attention between image features.\nVisual expert module. We add a visual expert module to\neach layer to enable deep visual-language feature alignment.\nSpecifically, the visual expert module in each layer consists\nof a QKV matrix and an MLP in each layer. The shapes\nof the QKV matrix and MLP are identical to those in the\npretrained language model and initialized from them. The\nmotivation is that each attention head in the language model\ncaptures a certain aspect of semantic information, while a\ntrainable visual expert can transform the image features\nto align with the different heads, therefore enabling deep\nfusion.\nFormally, suppose that the input hidden states of an attention\nlayer are X \u2208 RB\u00d7H\u00d7(LI+LT )\u00d7D, where B is the batch\nsize, LI and LT are the lengths of image and text sequences,\nH is the number of attention heads, and D is the hidden\nsize. In the attention with visual expert, X is first split as\nimage hidden states XI and text hidden states XT , and the\nattention is computed as:\nAttention(X, WI, WT ) = softmax(Tril(QKT )\n\u221a\nD\n)V,\n(1)\nQ = concat(XIW Q\nI , XT W Q\nT ),\n(2)\nK = concat(XIW K\nI , XT W K\nT ),\n(3)\nV = concat(XIW V\nI , XT W V\nT ),\n(4)\nwhere WI, WT are the QKV matrices of the visual ex-\npert and original language model, and Tril(\u00b7) means lower-\ntriangular mask. The visual expert in FFN layers performs\nsimilarly,\nFFN(X) = concat(FFNI(XI), FFNT (XT )),\n(5)\nwhere FFNI and FFNT are the FFN of the visual expert and\noriginal language model.\nPosition embedding. In the RoPE within LLM, we allow\nall visual tokens to share a single position id, as they already\nencapsulate positional information when inputted into the\nViT. This approach mitigates the impact of remote atten-\nuation between tokens in the LLM. Given that an image\ncan occupy hundreds to thousands of tokens, and a typical\ninput sequence is structured as \u2018<image embed> query\u2019,\nusing conventional positional encoding would result in ex-\ncessively lengthy encoding sequences. Moreover, it would\nlead the query to focus more on the image sequences closer\nto it, namely the lower part of an image.\n2.2. Pretraining\nData. The image-text pairs for pretraining are all publicly\navailable, including LAION-2B and COYO-700M. After\nremoving the broken URLs, NSFW images, images with\nnoisy captions, images with political bias and images with\nan aspect ratio > 6 or < 1/6, about 1.5B images are left for\npretraining.\nWe also crafted a visual grounding dataset of 40M images.\nEach noun in the image caption is associated with bounding\nboxes to indicate the positions in the image. The construc-\ntion process basically follows (Peng et al.), which extracts\nnouns via spaCy (Honnibal & Johnson, 2015) and predicts\nthe bounding boxes using GLIPv2 (Zhang et al., 2022). The\nimage-text pairs are sampled from LAION-115M, a subset\nof LAION-400M filtered by (Li et al., 2023b). We filter and\nretain a subset of 40 million images to ensure that over 75%\nof images contain at least two bounding boxes.\nTraining. The first stage of pretraining is for image cap-\ntioning loss, i.e. next token prediction in the text part. We\ntrain the CogVLM-17B model on the 1.5B image-text pairs\nintroduced above for 120,000 iterations with a batch size\nof 8,192. The second stage of pretraining is a mixture of\nimage captioning and Referring Expression Comprehension\n(REC). REC is a task to predict the bounding box in the im-\nage given the text description of an object, which is trained\nin the form of VQA, i.e., Question: Where is the object?\nand Answer: [[x0, y0, x1, y1]]. Both x and y coordinates\n4\nCogVLM: Visual Expert for Pretrained Language Models\nrange from 000 to 999, meaning the normalized position\nin the image. We only consider the loss of the next token\nprediction in the \u201cAnswer\u201d part. We pretrain the second\nstage for 60,000 iterations with a batch size of 1,024 on the\ntext-image pairs and visual grounding datasets introduced\nabove. During the final 30,000 iterations, we change the\ninput resolution from 224 \u00d7 224 to 490 \u00d7 490. The total\nnumber of trainable parameters is 6.5B.\n2.3. Alignment\nIn the instruction alignment phase, we trained two gen-\neralist models: CogVLM-Chat and CogVLM-Grounding.\nCogVLM-Chat accepts natural language inputs and outputs,\nwhile CogVLM-Grounding accepts inputs and outputs with\nbounding boxes.\nCogVLM-Chat. In our study, we integrated data from a va-\nriety of open-source visual question-answering datasets, in-\ncluding VQAv2 (Antol et al., 2015), OKVQA (Marino et al.,\n2019), TextVQA (Singh et al., 2019), OCRVQA (Mishra\net al., 2019), ScienceQA (Lu et al., 2022), as well as\ndatasets formatted as multi-turn dialogues such as LLaVA-\nInstruct (Liu et al., 2023c), LRV-Instruction (Liu et al.,\n2023a), LLaVAR (Zhang et al., 2023). We then conducted\nunified instruction-supervised fine-tuning (SFT) across these\ndiverse datasets. The integrity and quality of SFT data are\ncrucial; notably, the LLaVA-Instruct dataset, initially gen-\nerated through a language-only GPT-4 pipeline, contained\ncertain inaccuracies. We meticulously corrected these errors\nthrough manual inspection and annotation to ensure data\nquality.\nVQA datasets typically feature concise, often one-word an-\nswers, contrasting with the dialogue datasets that provide\ndetailed responses with extensive reasoning. To accommo-\ndate this variability, we employed prompts formatted as\nQuestion: Short answer: for concise responses and Ques-\ntion: Answer: for extended discourse in the SFT phase.\nDuring training, the model underwent 6000 iterations with\na learning rate of 1e-5 and a batch size of 1024. To en-\nhance and ensure the stability of the training, we activated\nthe visual encoder\u2019s parameters and adjusted its learning\nrate to be one-tenth of that used for the remaining training\nparameters.\nCogVLM-Grounding. In order to endow our model with\nconsistent, interactive visual grounding capabilities, we col-\nlect a high-quality dataset covering 4 types of grounding\ndata: (1) Grounded Captioning (GC) - image captioning\ndatasets where each noun phrase within the caption is fol-\nlowed by the corresponding referential bounding boxes; (2)\nReferring Expression Generation (REG) - image-oriented\ndatasets that each bounding box in the image is annotated\nwith a descriptive textual expression that accurately charac-\nterizes and refers to the content within the specific region;\n(3) Referring Expression Comprehension (REC) - text-\noriented datasets that each textual description is annotated\nwith multiple referential links associating the phrases with\ncorresponding boxes; (4) Grounded Visual Question An-\nswering (GroundedVQA) - VQA-style datasets where the\nquestions may contain region references in a given image.\nThe sources of grounding data are all publicly available,\nincluding Flickr30K Entities (Plummer et al., 2015), Ref-\nCOCO (Kazemzadeh et al., 2014; Mao et al., 2016; Yu et al.,\n2016), Visual7W (Zhu et al., 2016), VisualGenome (Krishna\net al., 2017) and Grounded CoT-VQA (Chen et al., 2023a).\n[box] in this section is in the format of [[x0, y0, x1, y1]].\nIt is noteworthy that the curated datasets exhibit a versa-\ntility of visual grounding capabilities, and many datasets\ncan be adapted and repurposed across different tasks. For\ninstance, grounded captioning datasets can be reformulated\nto suit REG and REC tasks. Taking the example of \u201cA\nman [box1] and a woman [box2] are walking together.\u201d, this\ncan be reframed into question answering pairs like (\u201cDe-\nscribe this region [box2].\u201d, \u201cA woman.\u201d) and (\u201cWhere is\nthe man?\u201d, \u201c[box1]\u201d). Similarly, REC datasets can be trans-\nlated into REG tasks by switching the input and output, and\nvice versa. However, certain conversions might lead to am-\nbiguities. For example, when presented with the isolated\nquery \u201cWhere is another man?\u201d from the caption \u201cA man\n[box1] is running, while another man [box2] is looking.\u201d,\nthe distinction between [box1] and [box2] becomes unclear,\npotentially leading to errors.\n3. Experiments\nTo rigorously validate the superior performance and robust\ngeneralization of our base model, we conduct quantitative\nevaluations on an array of multi-modal benchmarks. These\nbenchmarks can be categorized into three broad areas cover-\ning a comprehensive range of measurement1:\n\u2022 Image Captioning. The main purpose of these tasks\nis to generate textual captions summarizing the major\ncontent of a given image. We utilize prominent datasets\nincluding NoCaps (Agrawal et al., 2019), COCO (Lin\net al., 2014), Flickr30K (Plummer et al., 2015), and\nTextCaps (Sidorov et al., 2020) for evaluation.\n\u2022 Visual Question Answering. The VQA tasks require\nmodels to answer questions that may focus on distinct\nvisual contents based on the given image. Our assess-\nment covers diverse datasets, including VQAv2 (An-\ntol et al., 2015), OKVQA (Marino et al., 2019),\nTextVQA (Singh et al., 2019), OCRVQA (Mishra et al.,\n2019) and ScienceQA (Lu et al., 2022).\n1Detailed summary of all benchmarks and corresponding met-\nrics are available at Appendix A.2.\n5\nCogVLM: Visual Expert for Pretrained Language Models\nTable 1. Performance on Image Captioning benchmarks. All tasks use CIDEr as the evaluation metric. OOD refers to out-of-domain\ntest set. Karp. refers to the Karpathy test split.\nMethod\nTrain\nData\nNoCaps val\nNoCaps test\nFlickr\nCOCO\nTextCaps\nOOD\noverall\nOOD\noverall\nKarp.\nKarp.\ntest\nHuman\n-\n95.7\n87.1\n91.6\n85.3\n-\n-\n125.1\nVinVL (Zhang et al., 2021)\n8.9M\n83.8\n94.3\n78.0\n92.5\n-\n130.8\n-\nSimVLM (Wang et al., 2021)\n1.8B\n115.2\n112.2\n109.5\n110.3\n-\n143.3\n-\nCoCa (Yu et al., 2022)\n4.8B\n-\n122.4\n-\n120.6\n-\n143.6\n-\nLEMON (Hu et al., 2022)\n2B\n120.2\n117.3\n110.1\n114.3\n-\n139.1\n-\nFlamingo (Alayrac et al., 2022)\n2.3B\n-\n-\n-\n-\n67.2\n138.1\n-\nPrismer (Liu et al., 2023d)\n12.7M\n113.5\n112.9\n-\n110.8\n-\n136.5\n-\nBLIP-2 (Li et al., 2023b)\n129M\n124.8\n121.6\n-\n-\n-\n144.5\n-\nInstructBLIP (Dai et al., 2023)\n129M\n-\n123.1\n-\n-\n82.4\n-\n-\nUniversalCap (Cornia et al., 2021)\n35M\n123.4\n122.1\n114.3\n119.3\n-\n143.4\n-\nGIT (Wang et al., 2022a)\n0.8B\n127.1\n125.5\n122.0\n123.4\n49.6\n144.8\n138.2\nGIT2 (Wang et al., 2022a)\n12.9B\n130.6\n126.9\n122.3\n124.8\n50.7\n145.0\n145.0\nQwen-VL (Bai et al., 2023)\n1.4B\n-\n121.4\n-\n-\n85.8\n-\n-\nPaLI-17B (Chen et al., 2022b)\n1.6B\n-\n127.0\n-\n124.4\n-\n149.1\n135.4\nPaLI-X-55B (Chen et al., 2023b)\n-\n-\n126.3\n-\n124.3\n-\n149.2\n147.0\nCogVLM (ours)\n1.5B\n132.6\n128.3\n128.0\n126.4\n94.9\n148.7\n144.9\n\u2022 LVLM Benchmarks. LVLM benchmarks are primar-\nily employed to assess the advanced capabilities of\nlarge multimodal models, such as object recognition\nand localization, OCR, visual description, and visual\nknowledge reasoning. We conduct multidimensional\nevaluations of the models on datasets including MM-\nVet (Yu et al., 2023), MMBench (Liu et al., 2023g),\nSEED-Bench (Li et al., 2023a), LLaVA-Bench (Liu\net al., 2023c), POPE (Li et al., 2023c), MMMU (Yue\net al., 2023) and MathVista (Lu et al., 2023).\n\u2022 Visual Grounding. Visual grounding involves a set\nof tasks that establish referential links between textual\nmentions in a sentence and specific regions in an im-\nage. We evaluate our model on the typical datasets,\nincluding Visual7w (Zhu et al., 2016), RefCOCO (Liu\net al., 2017), RefCOCO+, and RefCOCOg to ensure\ncompleteness.\n3.1. Image Captioning\nWe evaluate the image captioning capability of our pre-\ntrained base model on the aforementioned four benchmarks.\nIn a zero-shot evaluation on the Nocaps and Flickr datasets,\nwe assess the precision of our model in describing long-\ntail visual concepts. Additionally, we present results from\nfinetuning on the COCO and TextCaps datasets.\nThe detailed performance is shown in Table 1. Overall, our\nmodel achieves the SOTA or compatible performance across\nthe board. Specifically, on the NoCaps benchmark, our base\nmodel outperforms the previous best method, GIT2, across\nfour splits with a maximum of 5.7 points in the out-domain\nset while only consuming 10% of the pretraining data (1.5B\nvs 12.9B). On the Flickr benchmark, our model achieves a\nSOTA score of 94.9 surpassing the concurrently released\nQwen-VL model by 9.1 points. These results demonstrate\nthe remarkable capability and robustness of our pretrained\nmodel on the image captioning task. We also evaluate our\nmodel on the COCO (Lin et al., 2014) and TextCaps, where\nthe latter is specifically designed to integrate the textual in-\nformation of the given image into captions. Though training\nwithout the dedicated OCR data, encouragingly, our base\nmodel reveals a significant text-reading ability and obtains\na competitive performance with PaLI-X-55B, and outper-\nforms the previous best model of the same scale, PaLI-17B,\nby 9.1 points score.\n3.2. Visual Question Answering\nAs illustrated in Table 2, our CogVLM model demon-\nstrates outstanding performance and a significant lead\nover models of similar parameter scale across a variety\nof tasks, including daily-life image question-answering\ndataset VQAv2, text-intensive image question-answering\ndatasets such as TextVQA and OCRVQA, and knowledge-\ndemanding datasets like OKVQA and ScienceQA. This\nsuccess showcases the model\u2019s robust generalization capa-\nbilities and potential across diverse domains.\n3.3. LVLM Benchmarks\nOur findings, detailed in Table 2, demonstrate that CogVLM\nachieved state-of-the-art results in all 7 LVLM-benchmarks,\nmarkedly surpassing all other models.\nIt also outper-\nformed multimodal models that utilized larger language\nmodels, such as LLava1.5 with Vicuna-13B and Emu-2 with\n6\nCogVLM: Visual Expert for Pretrained Language Models\nTable 2. Generalist performance on VQA and LVLM benchmarks. * donates the dataset has been trained during SFT stage. We\ncompared with the latest state-of-the-art generalist models, including MiniGPT-4 (Zhu et al., 2023), IDEFICS-Instruct (Laurenc\u00b8on et al.,\n2023), OpenFlamingo (Awadalla et al., 2023), DreamLLM (Dong et al., 2023), InstructBLIP (Dai et al., 2023), Fuyu (Bavishi et al., 2023),\nQwen-VL (Bai et al., 2023), LLaVA-1.5 (Liu et al., 2023b), mPLUG-Owl2 (Ye et al., 2023), SPHINX (Lin et al., 2023b), Emu2 (Sun\net al., 2023a).\nMethod\nLLM\nVQA\nLVLM-Benchmark\nVQAv2 OKVQA TextVQA OCRVQA ScienceQA MM-Vet SEED MMBench LLaVA POPE MMMU MathVista\nMiniGPT-4\nVicuna-7B\n-\n-\n0.6\n-\n39.6\n22.1\n47.4\n23.0\n45.1\n-\n-\n23.1\nIDEFICS-Instruct LLaMA-65B\n37.4\n36.9\n32.9\n-\n61.8\n39.7\n53.2\n54.5\n56.9\n-\n-\n26.2\nOpenFlamingo\nMPT-7B\n53.0\n38.3\n28.3\n-\n44.8\n24.8\n42.7\n5.7\n34.2\n-\n26.3\n18.6\nDreamLLM\nVicuna-7B\n56.6\n44.3\n34.9\n-\n-\n35.9\n-\n49.9\n-\n-\n-\n-\nInstructBLIP\nVicuna-7B\n-\n-\n50.1\n-\n60.5\n26.2\n58.8\n33.9\n59.8\n53.8\n-\n25.3\nFuyu\nFuyu-8B\n74.2*\n60.6*\n-\n-\n-\n-\n-\n-\n-\n-\n27.4\n-\nQwen-VL-Chat\nQwen-7B\n78.2*\n56.6*\n61.5*\n70.5*\n68.8\n-\n65.4\n61.8\n67.7\n-\n32.9\n33.8\nLLaVA-1.5\nVicuna-7B\n78.5*\n-\n58.2*\n-\n66.8\n30.5\n58.6\n64.3\n60.7\n85.9\n-\n23.6\nmPLUG-Owl2\nLLaMA2-7B\n79.4*\n57.7*\n58.2*\n-\n68.7\n36.2\n64.1\n64.5\n25.0\n86.2\n32.1\n25.3\nUnified-IO2\nUIO-2XXL\n79.4*\n55.5*\n-\n-\n86.2*\n-\n65.6\n71.5\n-\n87.7\n-\n-\nLLaVA-1.5\nVicuna-13B\n80.0*\n-\n61.3*\n-\n71.6\n35.4\n61.6\n67.7\n64.6\n85.9\n33.6\n26.1\nSPHINX-2k\nLLaMA2 13B 80.7*\n62.6*\n61.2*\n67.8*\n70.6\n40.2\n71.6\n65.9\n-\n87.2\n32.9\n27.8\nEmu2-Chat\nLLaMA-33B\n84.9*\n64.8*\n66.6*\n-\n-\n48.5\n62.8\n63.6\n56.4\n-\n34.1\n-\nCogVLM-Chat\nVicuna-7B\n82.3*\n64.8*\n70.4*\n73.8*\n91.2*\n51.1\n72.5\n77.6\n77.8\n87.9\n41.1\n34.5\nTable 3. Results on Referring Expression Comprehension and Grounded Visual Question Answering.\nType\nModel\nRefCOCO\nRefCOCO+\nRefCOCOg\nVisual7W\nval\ntest-A\ntest-B\nval\ntest-A\ntest-B\nval\ntest\ntest\nGeneralist\nOFA-L* (Wang et al., 2022b)\n79.96\n83.67\n76.39\n68.29\n76.00\n61.75\n67.57\n67.58\n-\nVisionLLM-H (Wang et al., 2023b)\n-\n86.70\n-\n-\n-\n-\n-\n-\n-\nShikra-7B (Chen et al., 2023a)\n87.01\n90.61\n80.24\n81.60\n87.36\n72.12\n82.27\n82.19\n-\nShikra-13B (Chen et al., 2023a)\n87.83\n91.11\n81.81\n82.89\n87.79\n74.41\n82.64\n83.16\n85.33\nQwen-VL (Bai et al., 2023)\n89.36\n92.26\n85.34\n83.12\n88.25\n77.21\n85.58\n85.48\n-\nFerret-13B (You et al., 2023)\n89.48\n92.41\n84.36\n82.81\n88.14\n75.17\n85.83\n86.34\n-\nCogVLM-Grounding\n92.76\n94.75\n88.99\n88.68\n92.91\n83.39\n89.75\n90.79\n91.05\nSpecialist\nG-DINO-L (Liu et al., 2023e)\n90.56\n93.19\n88.24\n82.75\n88.95\n75.92\n86.13\n87.02\n-\nUNINEXT-H (Lin et al., 2023a)\n92.64\n94.33\n91.46\n85.24\n89.63\n79.79\n88.73\n89.37\n-\nONE-PEACE (Wang et al., 2023a)\n92.58\n94.18\n89.26\n88.77\n92.21\n83.23\n89.22\n89.27\n-\nLLAMA-33B, leading by 15.7 and 2.6 points on MM-vet,\n9.9 and 14.0 points on MMBench, respectively. Compared\nto IDEFICS-Instruct trained on LLaMA-65B, CogVLM\u2019s\nscores exceeded by 19.3, 23.1, and 20.9 points on Seed-\nBench, MMBench, and LLaVA-Bench, respectively. Fur-\nthermore, CogVLM achieved a score of 41.1 on the MMMU\ndataset, and also scored 87.9 on the hallucination assessment\ndataset POPE, along with 35.2 on the multimodal mathe-\nmatical reasoning benchmark MathVista. These impressive\nresults not only showcase its robust reasoning abilities and\nmulti-task generalization capabilities but also clearly demon-\nstrate that CogVLM is significantly outpacing other models\nin these domains. Notably, shallow fusion models such as\nInstructBLIP and MiniGPT-4 underperformed across most\nbenchmarks, despite InstructBLIP\u2019s extensive training on\ninstructional data, underscoring the necessity of deep fusion\nfor enhanced performance.\n3.4. Visual Grounding\nTable 3 shows the result on the standard visual grounding\nbenchmarks. We find that our generalist model achieves\nstate-of-the-art performance across the board, with a sig-\nnificant advantage over the previous or concurrent models.\nAs shown in the bottom part of Table 3, our model even\nsurpasses models that are specifically trained for individ-\nual tasks, achieving SOTA performance on 5 of 9 splits.\nFor instance, in the RefCOCO val subset, our model at-\ntains a score of 92.76, surpassing UNINEXT-H\u2019s 92.64; in\nthe RefCOCO+ test-A subset, it scores 92.91, exceeding\nONE-PEACE\u2019s 92.21; and in the RefCOCOg test subset, it\nachieves 90.79, outperforming UNINEXT-H\u2019s 89.27. These\nresults suggest a remarkable visual grounding capability of\nour model incorporating our training paradigm.\n7\nCogVLM: Visual Expert for Pretrained Language Models\nTable 4. Ablation studies for various components and training settings. VE refers to visual expert.\nAblated Aspects\nOriginal Setting\nAblated Setting\nTrainable\nCOCO\nNoCaps\nOKVQA\nTextVQA\nVQAv2\nparams\nCIDEr\u2191\nCIDEr\u2191\ntop1\u2191\ntop1\u2191\ntop1\u2191\nTuned parameters\nVE-full every layer\n+\nMLP Adapter\nMLP Adapter\n140M\n131.2\n111.5\n55.1\n40.7\n73.8\nLLM+MLP Adapter\n6.9B\n140.3\n118.5\n56.8\n44.7\n78.9\nVE-full every 4th layer\n1.7B\n138.7\n117.4\n58.9\n44.1\n77.6\nVE-FFN every layer\n4.4B\n140.0\n118.7\n58.2\n45.1\n78.6\nInit method\nFrom LLM\nRandom init\n6.6B\n138.0\n117.9\n55.9\n44.0\n79.1\nVisual attention mask\nCausal mask\nFull mask\n6.6B\n141.0\n117.2\n57.4\n45.1\n79.6\nImage SSL loss\n%\n!(clip feature)\n6.6B\n142.9\n119.8\n58.7\n45.9\n79.7\nVisual encoder\nEVA2-E\nEVA2-L\n6.6B\n141.4\n122.5\n59.2\n42.8\n79.0\nEMA\n!\n%\n6.6B\n143.1\n119.2\n57.1\n43.8\n79.4\nCogVLM (ours)\n\u2014\n\u2014\n6.6B\n142.8\n120.1\n59.3\n45.3\n80.0\n3.5. Ablation Study\nTo understand the impact of various components and set-\ntings on our model\u2019s performance, we conduct an extensive\nablation study for 6,000 iterations and a batch size of 8,192.\nTable 4 summarizes the results about the following aspects:\nModel structure and tuned parameters. To investigate\nthe effectiveness of CogVLM\u2019s model, we conduct ablation\nstudies on several structure variants and tuning strategies,\nincluding: 1) tuning only the MLP Adapter layer; 2) tuning\nall LLM parameters and the Adapter without adding visual\nexpert; 3) only adding visual expert at every 4th LLM layer;\nand 4) only add visual expert to FFNs at all layers.\nFrom the results, we can see that shallow vision-language\nalignment, i.e. only tuning the adapter layer (similar to the\nmethod used in BLIP-2), results in a significantly inferior\nperformance. Also, the performance of training the visual\nexpert is higher than that of training the LLM, especially on\nthe datasets that require external knowledge, even though\nthe training parameters are roughly the same. We also com-\npare with other variants of adding visual expert, including a.\ninserting an expert module every 4 layers and b. removing\nthe attention part from the expert. Both of them result in a\ncertain degree of performance decline, but within an accept-\nable range, which provides some guidance for balancing\ncomputational overhead and model performance.\nInitialization Method. As for visual expert\u2019s initializa-\ntion method, we compare initialization with weights from\nLLM to random initialization. Our results across various\ndatasets demonstrate that initialization with LLM\u2019s weights\nconsistently achieves superior performance. This indicates\nthat the transformer architecture pre-trained on language\ndata possesses a certain capability to process visual tokens.\nMoreover, it can serve as a more effective starting point for\nmultimodal pre-training initialization.\nVisual Attention Mask. We empirically find that using a\ncausal mask on visual tokens yields a better result in com-\nparison with a full mask. This is slightly counterintuitive, as\nusing a bidirectional attention mask allows access to more\ninformation than a causal mask. We hypothesize the possi-\nble explanation for this phenomenon is that the causal mask\nbetter fits the inherent structure of LLMs.\nImage SSL Loss. We also investigated the self-supervised\nlearning loss on image features, where each visual feature\npredicts the CLIP feature of the next position for visual self-\nsupervision. Align with the observation from PaLI-X (Chen\net al., 2023b), we find it brings no improvement on down-\nstream tasks, although we indeed observed improvements\nin small models in our early experiments.\nVisual Encoder. we substituted the 300M-parameter EVA2-\nL model for the 4.4B-parameter EVA2-E to investigate the\nimpact of visual encoder parameters on various tasks. The\nresults indicated that there was only a slight decrease in per-\nformance across most benchmarks. However, a notable ex-\nception was observed in the text-oriented dataset TextVQA,\nwhere we recorded a decline of 2.5.\nEMA. We utilize EMA (Exponential Moving Average) dur-\ning pretraining. The ablation results show that EMA often\nbrings improvements across various tasks compared to not\nusing it.\n4. Conclusion\nIn this paper, we introduce CogVLM, an open visual lan-\nguage foundation model. CogVLM shifts the paradigm\nfor VLM training from shallow alignment to deep fusion,\nachieving state-of-the-art performance on 17 classic multi-\nmodal benchmarks.\nThe VLM training is still in its infancy, and there are many\ndirections to explore, for example, better SFT alignment,\nRLHF and anti-hallucination. Since the previous famous\nVLMs are mostly closed-source, we believe CogVLM will\nbe a solid foundation for future multi-modal research.\n8\nCogVLM: Visual Expert for Pretrained Language Models\nReferences\nAgrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., John-\nson, M., Batra, D., Parikh, D., Lee, S., and Anderson, P.\nNocaps: Novel object captioning at scale. In Proceedings\nof the IEEE/CVF international conference on computer\nvision, pp. 8948\u20138957, 2019.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., et al. Flamingo: a visual language model for few-shot\nlearning. Advances in Neural Information Processing\nSystems, 35:23716\u201323736, 2022.\nAntol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D.,\nZitnick, C. L., and Parikh, D. Vqa: Visual question\nanswering. In Proceedings of the IEEE international\nconference on computer vision, pp. 2425\u20132433, 2015.\nAwadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y.,\nZhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S.,\nJitsev, J., Kornblith, S., Koh, P. W., Ilharco, G., Worts-\nman, M., and Schmidt, L.\nOpenflamingo: An open-\nsource framework for training large autoregressive vision-\nlanguage models.\narXiv preprint arXiv:2308.01390,\n2023.\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J.,\nZhou, C., and Zhou, J. Qwen-vl: A frontier large vision-\nlanguage model with versatile abilities. arXiv preprint\narXiv:2308.12966, 2023.\nBavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena,\nA., Somani, A., and Tas\u00b8\u0131rlar, S. Introducing our mul-\ntimodal models, 2023. URL https://www.adept.\nai/blog/fuyu-8b.\nByeon,\nM.,\nPark,\nB.,\nKim,\nH.,\nLee,\nS.,\nBaek,\nW., and Kim, S.\nCoyo-700m:\nImage-text pair\ndataset.\nhttps://github.com/kakaobrain/\ncoyo-dataset, 2022.\nCadene, R., Ben-Younes, H., Cord, M., and Thome, N.\nMurel: Multimodal relational reasoning for visual ques-\ntion answering. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pp.\n1989\u20131998, 2019.\nChen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and\nZhao, R. Shikra: Unleashing multimodal llm\u2019s referential\ndialogue magic. arXiv preprint arXiv:2306.15195, 2023a.\nChen, T., Li, L., Saxena, S., Hinton, G., and Fleet, D. J.\nA generalist framework for panoptic segmentation of\nimages and videos. arXiv preprint arXiv:2210.06366,\n2022a.\nChen, X., Wang, X., Changpinyo, S., Piergiovanni, A.,\nPadlewski, P., Salz, D., Goodman, S., Grycner, A.,\nMustafa, B., Beyer, L., et al.\nPali: A jointly-scaled\nmultilingual language-image model.\narXiv preprint\narXiv:2209.06794, 2022b.\nChen, X., Djolonga, J., Padlewski, P., Mustafa, B., Chang-\npinyo, S., Wu, J., Ruiz, C. R., Goodman, S., Wang, X.,\nTay, Y., et al. Pali-x: On scaling up a multilingual vision\nand language model. arXiv preprint arXiv:2305.18565,\n2023b.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\net al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023), 2023.\nCornia, M., Baraldi, L., Fiameni, G., and Cucchiara, R. Uni-\nversal captioner: Long-tail vision-and-language model\ntraining through content-style separation. arXiv preprint\narXiv:2111.12727, 1(2):4, 2021.\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,\nW., Li, B., Fung, P., and Hoi, S. Instructblip: Towards\ngeneral-purpose vision-language models with instruction\ntuning, 2023.\nDong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao,\nL., Sun, J., Zhou, H., Wei, H., et al. Dreamllm: Syn-\nergistic multimodal comprehension and creation. arXiv\npreprint arXiv:2309.11499, 2023.\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery,\nA., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T.,\net al. Palm-e: An embodied multimodal language model.\narXiv preprint arXiv:2303.03378, 2023.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\nM., Song, D., and Steinhardt, J.\nMeasuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\nHonnibal, M. and Johnson, M. An improved non-monotonic\ntransition system for dependency parsing. In Proceedings\nof the 2015 conference on empirical methods in natural\nlanguage processing, pp. 1373\u20131378, 2015.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation of\nlarge language models. arXiv preprint arXiv:2106.09685,\n2021.\nHu, X., Gan, Z., Wang, J., Yang, Z., Liu, Z., Lu, Y., and\nWang, L. Scaling up vision-language pre-training for\nimage captioning. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pp.\n17980\u201317989, 2022.\n9\nCogVLM: Visual Expert for Pretrained Language Models\nKafle, K. and Kanan, C. An analysis of visual question\nanswering algorithms. In Proceedings of the IEEE inter-\nnational conference on computer vision, pp. 1965\u20131973,\n2017.\nKazemzadeh, S., Ordonez, V., Matten, M., and Berg, T.\nReferitgame: Referring to objects in photographs of natu-\nral scenes. In Proceedings of the 2014 conference on em-\npirical methods in natural language processing (EMNLP),\npp. 787\u2013798, 2014.\nKrasin, I., Duerig, T., Alldrin, N., Ferrari, V., Abu-El-Haija,\nS., Kuznetsova, A., Rom, H., Uijlings, J., Popov, S., Veit,\nA., et al. Openimages: A public dataset for large-scale\nmulti-label and multi-class image classification. Dataset\navailable from https://github. com/openimages, 2(3):18,\n2017.\nKrishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,\nKravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,\nD. A., et al. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. In-\nternational journal of computer vision, 123:32\u201373, 2017.\nLaurenc\u00b8on, H., Saulnier, L., Tronchon, L., Bekman, S.,\nSingh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush,\nA. M., Kiela, D., Cord, M., and Sanh, V. Obelics: An\nopen web-scale filtered dataset of interleaved image-text\ndocuments, 2023.\nLi, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y.\nSeed-bench: Benchmarking multimodal llms with gener-\native comprehension. arXiv preprint arXiv:2307.16125,\n2023a.\nLi, J., Li, D., Savarese, S., and Hoi, S.\nBlip-2: Boot-\nstrapping language-image pre-training with frozen im-\nage encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023b.\nLi, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen,\nJ.-R.\nEvaluating object hallucination in large vision-\nlanguage models.\narXiv preprint arXiv:2305.10355,\n2023c.\nLin, F., Yuan, J., Wu, S., Wang, F., and Wang, Z. Uninext:\nExploring a unified architecture for vision recognition.\narXiv preprint arXiv:2304.13700, 2023a.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll\u00b4ar, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In Computer Vision\u2013ECCV\n2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part V 13, pp. 740\u2013\n755. Springer, 2014.\nLin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu,\nH., Lin, C., Shao, W., Chen, K., et al. Sphinx: The\njoint mixing of weights, tasks, and visual embeddings\nfor multi-modal large language models. arXiv preprint\narXiv:2311.07575, 2023b.\nLiu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., and Wang, L.\nAligning large multi-modal model with robust instruction\ntuning. arXiv preprint arXiv:2306.14565, 2023a.\nLiu, H., Li, C., Li, Y., and Lee, Y. J.\nImproved base-\nlines with visual instruction tuning.\narXiv preprint\narXiv:2310.03744, 2023b.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. arXiv preprint arXiv:2304.08485, 2023c.\nLiu, J., Wang, L., and Yang, M.-H. Referring expression\ngeneration and comprehension via attributes. In Proceed-\nings of the IEEE International Conference on Computer\nVision, pp. 4856\u20134864, 2017.\nLiu, S., Fan, L., Johns, E., Yu, Z., Xiao, C., and Anand-\nkumar, A. Prismer: A vision-language model with an\nensemble of experts. arXiv preprint arXiv:2303.02506,\n2023d.\nLiu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C.,\nYang, J., Su, H., Zhu, J., et al. Grounding dino: Marry-\ning dino with grounded pre-training for open-set object\ndetection. arXiv preprint arXiv:2303.05499, 2023e.\nLiu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z.,\nand Tang, J. Gpt understands, too. AI Open, 2023f.\nLiu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W.,\nYuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is\nyour multi-modal model an all-around player?\narXiv\npreprint arXiv:2307.06281, 2023g.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for\nscience question answering. Advances in Neural Infor-\nmation Processing Systems, 35:2507\u20132521, 2022.\nLu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi,\nH., Cheng, H., Chang, K.-W., Galley, M., and Gao,\nJ.\nMathvista: Evaluating mathematical reasoning of\nfoundation models in visual contexts. arXiv preprint\narXiv:2310.02255, 2023.\nMao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L.,\nand Murphy, K. Generation and comprehension of unam-\nbiguous object descriptions. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 11\u201320, 2016.\n10\nCogVLM: Visual Expert for Pretrained Language Models\nMarino, K., Rastegari, M., Farhadi, A., and Mottaghi, R.\nOk-vqa: A visual question answering benchmark requir-\ning external knowledge. In Proceedings of the IEEE/cvf\nconference on computer vision and pattern recognition,\npp. 3195\u20133204, 2019.\nMishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A.\nOcr-vqa: Visual question answering by reading text in\nimages. In 2019 international conference on document\nanalysis and recognition (ICDAR), pp. 947\u2013952. IEEE,\n2019.\nPeng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma,\nS., and Wei, F.\nKosmos-2: Grounding multimodal\nlarge language models to the world.\narXiv preprint\narXiv:2306.14824.\nPlummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C.,\nHockenmaier, J., and Lazebnik, S. Flickr30k entities:\nCollecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE\ninternational conference on computer vision, pp. 2641\u2013\n2649, 2015.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551, 2020.\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C.,\nWightman, R., Cherti, M., Coombes, T., Katta, A., Mullis,\nC., Wortsman, M., et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models.\nAdvances in Neural Information Processing Systems, 35:\n25278\u201325294, 2022.\nShazeer, N.\nGlu variants improve transformer.\narXiv\npreprint arXiv:2002.05202, 2020.\nShrestha, R., Kafle, K., and Kanan, C. Answer them all!\ntoward universal visual question answering models. In\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 10472\u201310481, 2019.\nSidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps:\na dataset for image captioning with reading comprehen-\nsion. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceed-\nings, Part II 16, pp. 742\u2013758. Springer, 2020.\nSingh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npp. 8317\u20138326, 2019.\nSun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z.,\nWang, Y., Rao, Y., Liu, J., Huang, T., et al. Generative\nmultimodal models are in-context learners. arXiv preprint\narXiv:2312.13286, 2023a.\nSun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Eva-\nclip: Improved training techniques for clip at scale. arXiv\npreprint arXiv:2303.15389, 2023b.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nTsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S.,\nVinyals, O., and Hill, F.\nMultimodal few-shot learn-\ning with frozen language models. Advances in Neural\nInformation Processing Systems, 34:200\u2013212, 2021.\nWang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu,\nZ., Liu, C., and Wang, L. Git: A generative image-to-\ntext transformer for vision and language. arXiv preprint\narXiv:2205.14100, 2022a.\nWang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J.,\nZhou, C., Zhou, J., and Yang, H. Ofa: Unifying architec-\ntures, tasks, and modalities through a simple sequence-\nto-sequence learning framework. In International Con-\nference on Machine Learning, pp. 23318\u201323340. PMLR,\n2022b.\nWang, P., Wang, S., Lin, J., Bai, S., Zhou, X., Zhou, J.,\nWang, X., and Zhou, C. One-peace: Exploring one gen-\neral representation model toward unlimited modalities.\narXiv preprint arXiv:2305.11172, 2023a.\nWang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G.,\nLuo, P., Lu, T., Zhou, J., Qiao, Y., et al. Visionllm: Large\nlanguage model is also an open-ended decoder for vision-\ncentric tasks. arXiv preprint arXiv:2305.11175, 2023b.\nWang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., and Cao,\nY. Simvlm: Simple visual language model pretraining\nwith weak supervision. arXiv preprint arXiv:2108.10904,\n2021.\nYe, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang,\nJ., Huang, F., and Zhou, J. mplug-owl2: Revolution-\nizing multi-modal large language model with modality\ncollaboration. arXiv preprint arXiv:2311.04257, 2023.\nYou, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang,\nZ., Cao, L., Chang, S.-F., and Yang, Y. Ferret: Refer\nand ground anything anywhere at any granularity. arXiv\npreprint arXiv:2310.07704, 2023.\n11\nCogVLM: Visual Expert for Pretrained Language Models\nYu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini,\nM., and Wu, Y. Coca: Contrastive captioners are image-\ntext foundation models. arXiv preprint arXiv:2205.01917,\n2022.\nYu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L.\nModeling context in referring expressions. In Computer\nVision\u2013ECCV 2016: 14th European Conference, Amster-\ndam, The Netherlands, October 11-14, 2016, Proceed-\nings, Part II 14, pp. 69\u201385. Springer, 2016.\nYu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang,\nX., and Wang, L.\nMm-vet: Evaluating large multi-\nmodal models for integrated capabilities. arXiv preprint\narXiv:2308.02490, 2023.\nYue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G.,\nStevens, S., Jiang, D., Ren, W., Sun, Y., et al. Mmmu:\nA massive multi-discipline multimodal understanding\nand reasoning benchmark for expert agi. arXiv preprint\narXiv:2311.16502, 2023.\nZhang, H., Zhang, P., Hu, X., Chen, Y.-C., Li, L., Dai, X.,\nWang, L., Yuan, L., Hwang, J.-N., and Gao, J. Glipv2:\nUnifying localization and vision-language understanding.\nAdvances in Neural Information Processing Systems, 35:\n36067\u201336080, 2022.\nZhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L.,\nChoi, Y., and Gao, J. Vinvl: Revisiting visual representa-\ntions in vision-language models. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pp. 5579\u20135588, 2021.\nZhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D.,\nand Sun, T. Llavar: Enhanced visual instruction tuning\nfor text-rich image understanding, 2023.\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\nMinigpt-4: Enhancing vision-language understanding\nwith advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\nZhu, Y., Groth, O., Bernstein, M., and Fei-Fei, L. Visual7w:\nGrounded question answering in images. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, pp. 4995\u20135004, 2016.\n12\nCogVLM: Visual Expert for Pretrained Language Models\nA. Appendix\nA.1. Details of Training Settings\nWe report the details of parameter settings during pre-training and multitask training in Table 5 and Table 6.\nTable 5. Hyperparameters for pre-training model.\nHyperparameters Stage 1\nStage 2\nTotal steps\n120, 000\n60, 000\nWarmup steps\n12, 000\n1, 200\nBatch size\n8, 192\n1, 024\nLearning rate\n1e\u22124\n1e\u22125\nLearning rate decay\nCosine\nWeight decay\n0.05\nDropout ratio\n0.1\nAdam \u03f5\n1e\u22128\nAdam \u03b2\n(0.9, 0.95)\nTextual encoder\nVicuna-1.5-7B\nVisual encoder\nEVA2-CLIP-E\nPatch size\n14\nInput resolution\n2242\n2242 \u2192 4902\nTable 6. Hyperparameters for multitask finetuning CogVLM.\nHyperparameters Multitask\nLearning rate\n1e\u22125\nTotal steps\n6,000\nBatch size\n1,024\nAdamW \u03f5\n1e\u22128\nAdamW \u03b2\n(0.9, 0.95)\nWeight decay\n0.1\nDropout ratio\n0.1\nInput resolution\n4902\nA.2. Details of Associated Datasets\nIn this section, we introduce the details of datasets and their use in our evaluation process for all associated benchmarks.\nA.2.1. IMAGE CAPTIONING\n\u2022 COCO (Lin et al., 2014) The Captions in COCO dataset are collected using Amazon\u2019s Mechanical Turk (AMT)\nworkers who are given instructions to control the quality. The dataset contains 330K images, where the train, validation\nand test sets contain 413,915 captions for 82,783 images, 202,520 captions for 40,504 images, and 379,249 captions\nfor 40,775 images respectively.\n\u2022 NoCaps (Agrawal et al., 2019). NoCaps is a large-scale benchmark for novel object captioning, containing nearly\n400 novel object classes compared to COCO. The validation and test set comprised of 4,500 and 10,600 images,\nrespectively, sourced from the Open Images (Krasin et al., 2017) and annotated with 11 human-generated captions per\n13\nCogVLM: Visual Expert for Pretrained Language Models\nTable 7. Summary of the evaluation benchmarks.\nTask\nDataset\nDescription\nSplit\nMetrics\nImage Caption\nNoCaps\nCaptioning of natural images.\nval\nCIDEr (\u2191)\nFlickr\nCaptioning of natural images.\nkarpathy-test\nCIDEr (\u2191)\nCOCO\nCaptioning of natural images.\nkarpathy-test\nCIDEr (\u2191)\nTextCaps\nCaptioning of natural images containing text.\ntest\nCIDEr (\u2191)\nGeneral VQA\nVQAv2\nVQA on natural images.\ntest-dev\nVQA Score(\u2191)\nOK-VQA\nVQA on natural images requiring outside knowledge.\nval\nVQA Score (\u2191)\nScienceQA\nMulti-choice VQA on a diverse set of science topics\ntest\nAccuracy (\u2191)\nTDIUC\nVQA on natural images with detailed question types.\nval\nVQA Score (\u2191)\nText-oriented VQA\nOCR-VQA\nVQA on images of book covers.\ntest\nEM (\u2191)\nTextVQA\nVQA on natural images containing text.\nval\nVQA Score (\u2191)\nLVLM Benchmarks\nMM-Vet\nOpen-ended VQA on a diverse set of topics\ntest\nGPT4 Score(\u2191)\nSEED-Bench\nMulti-choice VQA on a diverse set of topics\nIMG\nAccuracy (\u2191)\nMMBench\nMulti-choice VQA on a diverse set of topics\ntest\nAccuracy (\u2191)\nLLaVA-Bench\nOpen-ended VQA for testing instruction following abilities\nIn-the-Wild\nGPT4 Score(\u2191)\nPOPE\nMulti-choice VQA for testing hallucinations\noverall\nAccuracy (\u2191)\nMMMU\nVQA on a diverse set of topics\ntest\nAccuracy (\u2191)\nMathVista\nVQA for Measuring Mathematical Abilities\ntest-mini\nAccuracy (\u2191)\nGrounding\nRefCOCO\nRefer grounding on natural images.\noverall\nAccuracy (\u2191)\nRefCOCO+\nRefer grounding on natural images.\noverall\nAccuracy (\u2191)\nRefCOCOg\nRefer grounding on natural images.\noverall\nAccuracy (\u2191)\nVisual7W\nVQA with referential regions selection.\nval\nAccuracy (\u2191)\nimage, and each set is subdivided into three domains: \u201cin\u201d, \u201cnear\u201d, and \u201cout\u201d, with objects in the \u201cout-domain\u201d never\nappearing in the COCO dataset.\n\u2022 Flickr30K (Plummer et al., 2015). Flickr30K is a high-quality dataset consists of 31,783 images of everyday\nlife activities, envets and scenes (all harvested from the online website Flickr) and 158,915 captions (obtained via\ncrodsourcing). Each image in this dataset is described independently by five annotators who are not familiar with the\nspecific entities and circumstances depicted in them.\n\u2022 TextCaps (Sidorov et al., 2020) Textcaps is a dataset with 145k captions for 28k images. The design purpose of the\nTextCaps dataset is to effectively integrate textual information with visual context into captions, requiring the model to\nhave both excellent OCR capabilities and strong captioning abilities.\nA.2.2. GENERAL VQA\n\u2022 VQAv2 (Antol et al., 2015) VQAv2 encompasses over 200,000 images, paired with more than 1.1 million questions\nthat have collectively garnered over 11 million answers. Questions span various types, including yes/no, counting, and\nopen-ended queries.\n\u2022 OKVQA (Marino et al., 2019) The OK-VQA (Outside Knowledge Visual Question Answering) dataset is specifically\ndesigned to probe visual question answering capabilities that necessitate external knowledge or common sense beyond\nimage content. It has 14,055 open-ended questions and 5 ground truth answers per question.\n\u2022 ScienceQA (Lu et al., 2022) The ScienceQA dataset comprises 21,208 multimodal multiple-choice questions spanning\nthree diverse subjects: natural science, language science, and social science. Each question is annotated with\nexplanations linked to relevant lectures.\n\u2022 TDIUC (Shrestha et al., 2019) The TDIUC dataset features 1.6M questions across 170K images from MS COCO and\nVisual Genome. Categorized into 12 distinct question types, it ranges from basic tasks like identifying objects or colors\nto more advanced reasoning like counting or positional discernment.\n14\nCogVLM: Visual Expert for Pretrained Language Models\nA.2.3. TEXT-ORIENTED VQA\n\u2022 OCRVQA (Mishra et al., 2019) OCR-VQA consists of 207,572 book cover images with over 1 million question-answer\npairs.\n\u2022 TextVQA (Singh et al., 2019) TextVQA is a dataset with 45,336 questions on 28,408 images that challenges models to\ndetect, read, and reason about text within images to provide answers.\nA.3. LVLM Benchmarks\n\u2022 MM-Vet (Yu et al., 2023) MM-Vet defines six core VL capabilities and examines 16 integrations of interest derived\nfrom the combinations of these capabilities. It employs an evaluator based on LLMs for open-ended outputs, capable\nof assessing across different question types and answer styles, thus deriving a unified scoring metric.\n\u2022 SEED-Bench (Li et al., 2023a) SEED-Bench is a dataset comprising 19K multiple-choice questions with precise\nhuman annotations, covering 12 evaluation dimensions, including understanding of image and video modalities. It\nobtains accurate answer options through manual annotations, enabling objective and efficient assessment of model\nperformance.\n\u2022 MMBench (Liu et al., 2023g) MMBench comprises approximately 3000 multiple-choice questions, covering 20\ndifferent capability dimensions, aimed at evaluating various abilities of visual-language models. MMBench adopts a\nhierarchical capability dimension structure, including two high-level capability dimensions: perception and reasoning,\nas well as fine-grained capability dimensions such as object localization and attribute inference.\n\u2022 LLaVA-Bench (Liu et al., 2023c) LLaVA-Bench (In-the-Wild) is a benchmark dataset comprising 60 questions,\ndesigned to evaluate the multimodal instruction following capabilities of LMMs. It includes indoor and outdoor scenes,\nmemes, paintings, sketches, etc., and is equipped with highly detailed, manually curated descriptions and appropriate\nquestion selections.\n\u2022 POPE (Li et al., 2023c) The POPE dataset is a binary classification query dataset specifically designed to evaluate\nobject hallucination issues in LMMs. The random, popular, and adversarial subsets within the POPE dataset are\nconstructed through different sampling strategies, totaling 8,910 entries.\n\u2022 MMMU (Yue et al., 2023) The MMMU dataset is a large-scale, multidisciplinary multimodal understanding and\nreasoning benchmark set, containing 11.5K questions. It covers 6 major disciplines, 30 topics, and 183 subfields, with\nquestion types including multiple-choice and open-ended questions. The dataset includes 30 types of images, such\nas charts, tables, chemical structures, photographs, paintings, musical scores, etc., testing the multimodal perception\ncapabilities of models and their performance in expert-level tasks.\n\u2022 MathVista (Lu et al., 2023) MathVista is a new benchmark dataset that combines mathematical and visual understand-\ning, comprising 31 existing multimodal datasets and 3 newly created datasets, totaling 6141 examples. These datasets\nencompass a diverse range of mathematical reasoning abilities, including seven types: algebra, arithmetic, geometry,\nlogic, numerical common sense, science, and statistics. The goal is to comprehensively evaluate the capabilities of\nexisting foundational models in mathematical reasoning and visual understanding.\nA.3.1. GROUNDING\n\u2022 RefCOCO/RefCOCO+ (Liu et al., 2017) RefCOCO and RefCOCO+ evolved from the ReferItGame. Both subsets\nfocus on images with two or more similar objects. RefCOCO, with 142,209 expressions across 19,994 images, places\nno linguistic constraints. Conversely, RefCOCO+ emphasizes appearance-centric descriptions, omitting locational\nterms, and comprises 141,564 expressions over 19,992 images.\n\u2022 RefCOCOg (Mao et al., 2016) The RefCOCOg subset was amassed through Amazon Mechanical Turk, where workers\npenned natural referring expressions for objects in MSCOCO images; it boasts 85,474 referring expressions spanning\n26,711 images, each containing 2 to 4 objects of the same category.\n\u2022 Visual7W (Zhu et al., 2016). The Visual7W dataset is predominantly designed for VQA tasks, with a dedicated\nsubset crafted for grounded VQA. In this subset, models are presented with an image accompanied by a \u201cwhich\u201d-type\n15\nCogVLM: Visual Expert for Pretrained Language Models\nquestion, such as \u201cWhich is the small computer in the corner?\u201d. Participants are then given four bounding boxes within\nthe image, from which they must select the correct one as the answer. The grounded Visual7W part consists of 25,733\nimages and 188,068 questions.\n\u2022 Flickr30K-Entities (Plummer et al., 2015). The Flickr30K Entities dataset, a precursor in the realm of grounded\ncaptioning, encompasses a collection of 31,783 images accompanied by 158k captioning annotations. Every caption in\nthis dataset has been meticulously annotated such that each noun phrase is linked with a manually delineated referential\nbounding box. In total, there are 276k such annotated bounding boxes provided within this dataset.\n\u2022 VisualGenome (Krishna et al., 2017). The VisualGenome dataset stands as a cornerstone in understanding the\nmultifaceted relationships present within images. With a collection of over 100k images, each image is annotated\nin detail, capturing an average of 21 objects, 18 attributes, and 18 inter-object relationships. A unique aspect of this\ndataset is the alignment of objects, attributes, relationships, and region descriptions with standardized terminologies\nfrom WordNet. Specifically tailored for the REG and REC tasks, each annotated region in an image comes with a\ncorresponding descriptive text, making it a rich resource for image understanding and semantic modeling. We use the\nsubset with around 86k images and 3.6 million region-caption pairs for visual grounding.\n0\n20\n40\n60\n80\n100\nAbsurd\nAverage\nCounting\nObject Presence\nColor Attribute\nOther Attributes\nSport Recognition.\nObject Recognition\nPositional Reasoning\nScene Classification\nActivity Recognition\nUtility and Affordances\nSentiment Understanding\nPrevious SOTA\nCogVLM-17B\nFigure 5. Performance on TDIUC benchmark with fine-grained questions classes.\nB. Additional Fine-grained Experiments\nTo comprehensively investigate the proposed model on specific topics and question types, we further conduct extensive\nexperiments on a representative benchmark, TDIUC (Kafle & Kanan, 2017). We use the publicly available split of val set as\nevaluation data, and the VQA accuracy calculated from their official scripts as the evaluation metric.\nThe experimental results on TDIUC compare our model against the specialist SOTA method MUREL (Cadene et al., 2019)\nare shown in Figure 5. From the experimental result, we can see that our model consistently outperforms the previous model\non 12 specific question types, resulting in a 94.0 accuracy score compared to the previous SOTA of 88.2 on the overall\ndataset. These results demonstrate that our model exhibits comprehensive problem-solving skills on general VQA tasks.\nC. Computational Efficiency\nIn this section, we compare the computational efficiency of our model with other state-of-the-art models, considering both\npretraining and finetuning data from datasets such as VQAv2 and TextVQA. Owing to an optimized architecture and the\n16\nCogVLM: Visual Expert for Pretrained Language Models\nutilization of high-quality pretraining data, our model demonstrates a marked reduction in resource consumption during\ntraining relative to models with comparable parameter magnitudes.\nTable 8. Comparison of different models based on their computational efficiency. We use PFLOPS*days as metrics.\nModel\nPretraining Data\nPretraining compute\nVQAv2 finetuning\nTextVQA finetuning\nPaLI-3B\n1.6B\n56\n1.1\n0.2\nPaLI-17B\n1.6B\n453\n4.5\n0.9\nFlamingo-80B\n2.3B\n1381*\nN/A\nN/A\nGIT2-5.1B\n12.9B\n5513*\nN/A\nN/A\nCogVLM\n1.5B\n230.1\n1.2\n0.13\n17\n"
  },
  {
    "title": "Relax: Composable Abstractions for End-to-End Dynamic Machine Learning",
    "link": "https://arxiv.org/pdf/2311.02103.pdf",
    "upvote": "15",
    "text": "RELAX: COMPOSABLE ABSTRACTIONS FOR END-TO-END DYNAMIC\nMACHINE LEARNING\nRuihang Lai * 1 Junru Shao * 2 Siyuan Feng * 3 Steven S. Lyubomirsky * 2 Bohan Hou 1 Wuwei Lin 2 Zihao Ye 4\nHongyi Jin 1 Yuchen Jin 5 Jiawei Liu 6 Lesheng Jin 2 Yaxing Cai 2 Ziheng Jiang 7 Yong Wu 2\nSunghyun Park 2 Prakalp Srivastava 2 Jared Roesch 2 Todd C. Mowry 1 Tianqi Chen 1 2\nABSTRACT\nDynamic shape computations have become critical in modern machine learning workloads, especially in emerging\nlarge language models. The success of these models has driven demand for deploying them to a diverse set\nof backend environments. In this paper, we present Relax, a compiler abstraction for optimizing end-to-end\ndynamic machine learning workloads. Relax introduces first-class symbolic shape annotations to track dynamic\nshape computations globally across the program. It also introduces a cross-level abstraction that encapsulates\ncomputational graphs, loop-level tensor programs, and library calls in a single representation to enable cross-level\noptimizations. We build an end-to-end compilation framework using the proposed approach to optimize dynamic\nshape models. Experimental results on large language models show that Relax delivers performance competitive\nwith state-of-the-art hand-optimized systems across platforms and enables deployment of emerging dynamic\nmodels to a broader set of environments, including mobile phones, embedded devices, and web browsers.\n1\nINTRODUCTION\nMachine learning (ML) applications are now ubiquitous in\neveryday life and the broader economy. The arrival of GPT-\n4 and open-source large language models (LLMs) (Touvron\net al., 2023; Rozi`ere et al., 2023; Black et al., 2022; Vaswani\net al., 2017; Xu et al., 2023) has created promising opportu-\nnities for building even more powerful modern AI systems\nfor processing images and text. The success of these models\nhas also created growing demand to deploy these models\nto a diverse set of backend environments, including servers,\npersonal computers, vehicles, and mobile devices.\nMachine learning frameworks (Abadi et al., 2016; Paszke\net al., 2019) are responsible for deploying many of these\nmodels to diverse backends. Machine learning compil-\ners (Chen et al., 2018; Lattner et al., 2021; IREE Project,\n2019; Paszke et al., 2019) aim to reduce the gap between\nmodel and backends by ingesting model computations into\ncommon program abstractions, performing optimizations\nsuch as operator fusion, and generating high-performance\ncode on a diverse set of platforms. However, much engi-\nneering effort is required to support the tensor operators\n*Equal contribution\n1Carnegie Mellon University 2OctoML\n3Shanghai Jiao Tong University\n4University of Washington\n5Hyperbolic Labs\n6University of Illinois Urbana-Champaign\n7ByteDance.\nCorrespondence\nto:\nRuihang\nLai\n<rui-\nhangl@cs.cmu.edu>.\nin a model for different hardware backends, particularly\nsince most of these models make use of dynamic tensor\nshapes. Dynamic shapes contain variables that may depend\non program values, which makes it more difficult to perform\ncrucial optimizations like static memory planning. For ex-\nample, language models must handle variable-sized input\nmessages, KV-cache context length, vocabulary sizes, and\nother sources of shape dynamism.\nThere are many challenges in optimizing machine learning\nmodels with dynamic shapes. First, many traditional op-\ntimizations depend on static shape analysis. The missing\ninformation from dynamic dimensions can significantly af-\nfect the optimizations we can perform across operators and\nfunctions. Second, current ML compilers usually lower pro-\ngrams to hardware in a traditional single-shot lowering flow,\napplying one optimization after the other, typically rewrit-\ning the program into a lower-level representation. This\napproach often results in losing shape and other information\nbetween abstraction layers and makes it harder to perform\nincremental optimizations across boundaries.\nTo address these challenges, we introduce Relax, a compiler\nabstraction for end-to-end dynamic machine learning. We\nintroduce first-class symbolic shapes to track and represent\nthe relations of dynamic shape dimensions. Relax uses\nvariables to represent symbolic shape dimensions and uses a\n\u201cbest-effort\u201d approach to infer dynamic shapes across tensor\noperators and function calls statically when possible, using\na dynamic fallback otherwise. Relax also enables cross-\narXiv:2311.02103v1  [cs.LG]  1 Nov 2023\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\n@tensorir_function\ndef mm(X: Buffer((\"n\", 512) \"f32\"), W: Buffer((512, 1536), \"f32\"),\n       Y: Buffer((\"n\", 1536), \"f32\")):\n  n = sym_var()\n  for i, j, k in grid(n, 1536, 512):\n    with block():\n      with init():\n        Y[i, j] = 0\n      Y[i, j] += X[i, k] * W[k, j]\ndef main(x: Tensor((\"n\", 512), \"f32\"), w: Tensor((512, 1536), \"f32\")):\n  n = sym_var()\n  with dataflow(): \n    lv0: Tensor((n, 1536), \"f32\") = call_tir(mm, [x, w], Tensor((n, 1536),\"f32\"))\n    lv1: Tensor((n, 3, 8, 64), \"f32\") = reshape(lv0, shape(n, 3, 8, 64))\n    lv2: Tensor((n, 8, 64), \"f32\") = call_dps_library(\n      \"cutlass.attention\", [lv1], Tensor((n, 8, 64), \"f32\")\n    )\n    \u2026\nML models\nTarget Code\nML models\nOur Approach\nFirst-class symbolic shape\nannotations to enable\ndynamic shape aware \nanalysis and optimizations \nCross-level abstraction that encapsulates \nhigh-level computation, tensor program, \nlibraries and their interactions\nComputational Graph IR\nTensor Program IR\nTarget Code\nMulti-level ML Compilers\nMulti-level abstractions\nwith optimizations in\neach level\nsingle-shot \nlowering\nother optional layers of IRs\nLibraries \nComposable optimizations \nacross dynamic shape \naware computational graph, \ntensor program, and libraries \nFigure 1. Overview of our approach. We introduce first-class symbolic shape annotations to track dynamic shape computations globally\nacross the program. We also present a cross-level abstraction that encapsulates multiple abstraction layers and enables composable\ndynamic shape\u2013aware optimizations across them.\nlevel abstractions that encapsulate computational graphs,\nloop-level tensor programs, and library calls in the same\nlayer to enable analysis and optimizations across them. We\nalso introduce a collection of composable optimizations\nto enable dynamic shape-aware operator fusion, memory\noptimization, and tensor operator optimizations. Finally,\nwe build end-to-end compilation framework on top of these\nelements. This paper makes the following contributions:\n\u2022 We present a program abstraction with a first-class sym-\nbolic shape approach that tracks dynamic shape relations\nglobally across tensor operators and function calls.\n\u2022 We propose a design for cross-level abstractions to enable\noptimizations and analysis across the traditional levels of\nabstraction in ML frameworks, enabling dynamic shape-\naware optimizations across levels.\n\u2022 We build an end-to-end compilation framework to deploy\nemerging dynamic models to diverse hardware backends.\nExperimental results show that Relax compiles and opti-\nmizes emerging LLMs onto diverse hardware backends\nand delivers competitive performance to heavily opti-\nmized platform-specific solutions.\nAdditionally, Relax\nsupports LLMs on a broad set of devices and environ-\nments, including mobile phones, embedded devices, and\nweb browsers (through WebAssembly and WebGPU).\n2\nOVERVIEW\nThis section describes the key insights of our approach and\ngives an overview of the paper. Figure 1 summarizes our\noverall approach, focusing on two key elements.\nFirst, we observe that while machine learning workloads\ninvolve dynamic shape computations, we can perform a\nsubstantial amount of static analysis and optimizations by\nconsidering the relations between shapes. Thus, we intro-\nduce annotations that can track the shapes of intermedi-\nate computations through symbolic variables and symbolic\nshape computations. Our approach globally tracks these dy-\nnamic shape computations across function calls to represent\ndynamic shapes throughout the program.\nSecond, we observe that ML compilers often need to go\nthrough several abstraction levels to bring a machine learn-\ning model to a target platform. The typical layers include\ncomputational graphs (Abadi et al., 2016; Paszke et al.,\n2019), tensor programs (Feng et al., 2023; Tillet et al., 2019;\nAnsel, 2023), libraries (Chetlur et al., 2014; Thakkar et al.,\n2023), and other optional layers. Traditionally, ML compil-\ners focus on optimizations within each abstraction level and\ndo a uni-directional single-shot lowering to the next level.\nRelax brings computational graphs, tensor programs, and\nlibraries into a single cross-level abstraction. We design the\ninteraction of those components to enable cross-level inter-\nactions and partial lowering of programs within the same\nabstraction.\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\nAnnotation\ndef main(x: Tensor((\"n\", 4), \"f32\")) -> Tensor((\"n * 2\",), \"f32\"):\n  n = sym_var()\n  with dataflow(): \n    lv0: Tensor((n, 4), \"f32\") = call_tir(exp, [x], Tensor((n, 4), \"f32\"))\n    f0: Callable = subfunc\n    lv1: Tensor((n * 4,), \"f32\") = subfunc(lv0)\n    lv2: Tuple[\n      Tensor((n * 2,), \"f32\"), Tensor((n * 2,), \"f32\")\n    ] = split(lv1, sections=2)\n    lv3: Tensor((n * 2,), \"f32\") = lv2[0]\n    return lv3\ndef subfunc(x: Tensor((\"n\", 4), \"f32\")) -> Tensor((\"n * 4\",), \"f32\"):\n  n = sym_var()\n  with dataflow():\n    lv0: Tensor((n * 4,), \"f32\") = flatten(x)\n    lv1: Tensor((n * 4,), \"f32\") = relu(lv0)\n    return lv1\n@tensorir_function\ndef exp(X: Buffer((\"n\", 4) \"f32\"), Y: Buffer((\"n\", 4), \"f32\")):\n   ...\nDataflow block\nCross-function call\nCross-level function call\nFigure 2. Key elements of Relax abstraction.\nTable 1. Annotations in Relax with examples and explanations.\nAnnotation Examples\nExplanation\nObject\nObject\nAny runtime value \nShape\nShape([n, 4]) \nShape(ndim=2)\nSymbolic shape value (n, 4)\nShape with two dimensions\nTensor\nTensor((n, 4), \"f32\")\nTensor(ndim=None, dtype=\"f32\")\nTensor with symbolic shape (n, 4)\nTensor with unknown dimensions\nTuple\nTuple[Tensor((n, 4), \"f32\"), Object]\nTuple of a Tensor and an Object\nCallable\nCallable(\n  [Tensor((\"n\", 4), \"f32\")], \n  Tensor((\"n * 4\",), \"f32\")\n)\nFunction that takes a (n, 4) Tensor\nand returns a (n*4,) Tensor\nInstead of single-shot lowering, all optimizations in Relax\nare performed as composable dynamic shape\u2013aware trans-\nformations. This design allows us to incrementally optimize\nor partially lower portions of the computation using differ-\nent approaches, take analysis from other levels into account,\nand incorporate further optimizations that consider dynamic\nshape relations.\n3\nOUR APPROACH\n3.1\nLanguage Constructs\nRelax is an imperative compiler abstraction with first-class\nfunctions, focusing on operating on tensors at a high level\n(the \u201cgraph level,\u201d as most ML compilers refer to it). Relax\nprograms apply high-level operators to entire tensors and\ncan pass tensors (or tuples of tensors) among functions, or\ninvoke lower-level tensor programs for loop-level operations\non tensors. This subsection illustrates three main elements\nof Relax: structural annotations, dataflow blocks, and func-\ntion calls both within and across levels of abstractions.\nAnnotations. Each value in Relax is associated with an\nannotation that conveys structural information, similar to\na static type. Table 1 summarizes different annotations in\nRelax and their usage examples and explanations respec-\ntively. Annotations indicate at compile time the overall\ntypes (e.g., tensor, tuple) and additional information about\ndef symbolic_shape_fn(x: Tensor((\"n\", 2, 2), \"f32\")):\n    n, m = sym_var(), sym_var()\n    lv0: Tensor((n, 4), \"f32\") = reshape(x, shape(n, 4))\n    lv1: Tensor((n * 4,), \"f32\") = flatten(lv0)\n    lv2: Tensor(ndim=1, dtype=\"f32\") = unique(lv1)\n    lv3 = match_cast(lv2, Tensor((m,), \"f32\"))\n    lv4: Tensor((m,), \"f32\") = exp(lv3)\n    return lv4\ndef any_shape_fn(x: Tensor((?, 2, 2), \"f32\")):\n    n = get_shape_value(x, axis=0)\n    lv0: Tensor((?, 4), \"f32\") = reshape(x, (n, 4))\n    lv1: Tensor((?,), \"f32\") = flatten(lv0)\n    lv2: Tensor(?, \"f32\") = unique(lv1)\n    lv3: Tensor(?, \"f32\") = exp(lv2)\n    return lv3\nShape annotation with unknown ?  dimensions\nFirst-class symbolic shape annotation\nFigure 3. Comparison of first-class symbolic shape annotation with\nunknown dynamic shape annotation . First-class symbolic shape\nenables comprehensive symbolic analysis and facilitating advanced\ndynamic shape\u2013aware optimizations.\nvalues, such as the shape and data type of a tensor. Anno-\ntations form the basis for first-class symbolic shapes and\ncross-level abstractions.\nDataflow Blocks. A dataflow block in Relax demarcates a\nside effect\u2013free program (sub-)region without control flows,\ni.e., a straight-line sequence of pure operations, in order to\nsimplify program transformations. For example, when per-\nforming dead code elimination over Relax dataflow blocks,\none can safely remove unused operators without having to\nconsider whether this could affect the visible behavior of\nthe program by removing an operation with side effects.\nFunction Calls. Relax incorporates function calls that can\nbe within the same level of abstraction (i.e., allowing one\ngraph-level function to invoke another graph-level function)\nor across levels of abstraction, namely allowing graph-level\nfunctions to call tensor program functions and external li-\nbraries. Calling loop-level functions serves as a foundational\nelement for cross-level abstractions, as explored in detail in\n\u00a73.4. We use TensorIR (Feng et al., 2023) as the loop-level\ntensor program abstraction, though the same principle can\nbe used for other loop-level abstractions.\n3.2\nFirst-Class Symbolic Shape Abstraction\nThe shape of a tensor is very useful information in the\ncontext of ML frameworks, especially for memory planning.\nOftentimes, however, tensor shapes are unknown at compile\ntime and must be dealt with dynamically. One approach for\nreasoning about dynamic shape dimensions is to introduce\nan any (or unknown) value to represent dynamic dimensions,\nas in ONNX (Bai et al., 2019), Relay (Roesch, 2020), and\nsome MLIR dialects (Lattner et al., 2021). Unfortunately,\nthis approach fails to preserve potentially useful information,\nlike relations or constraints between shape dimensions (e.g.,\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\nif one tensor has dimension n, another may be 4n). Such\ninformation can be valuable for compiler optimizations,\nwhereas marking the dimension as any erases it entirely.\nWe instead introduce a first-class symbolic shape annota-\ntion (shown in Figure 3). A symbolic shape annotation de-\nscribes each shape dimension using a symbolic expression,\ncomprised of integer variables and constants. Consequently,\nfor models whose shapes are fully static, Relax shape an-\nnotations subsumes existing static shape\u2013based annotation.\nFor dynamic machine learning models with a mixed set of\ndynamic and static dimensions, Relax not only expresses the\nshape dimensions symbolically but tracks the symbolic re-\nlations over the dimensions. This symbolic shape relations\nhelp us apply more dynamic shape\u2013aware optimizations.\nFor example, we will know the total number of elements\nafter the flatten operator is 4n in Figure 3 and is the\nsame as the input, suggesting potential buffer reuses.\nBesides serving as annotations, a symbolic shape can also be\nused as a first-class value in the computation. For example,\nthe reshape operator in Figure 3 takes the symbolic shape\n(n, 4) as input and outputs a tensor value with the same\nshape. Symbolic shape expressions can also be used to\nconstruct arguments to tensor functions.\nIt is not always possible to track the shape relations at com-\npile time. We cannot deduce the output shape of data-\ndependent operators, e.g., unique in Figure 3, whose\noutput tensor shape depends on the runtime values of its\ninput. For such cases, we provide coarse-grained anno-\ntations (e.g., Shape(ndim=2) in Table 1, meaning that\nthe shape has two dimensions but both are unknown). To\nreason about such cases, we introduce a special construct\ncalled match cast that asserts a symbolic shape anno-\ntation for a value, allowing for the introduction of new\nsymbolic variables. The compiler inserts runtime checks\nfor each match cast, throwing an error if a constraint\nis violated. In our particular example in Figure 3, one can\nannotate the result of the exp operator with shape (m, ) af-\nter match cast. match cast can be inserted by front-\nends and compiler passes to suggest more specific symbolic\nshapes if possible and serves as a valuable tool for develop-\ners to indicate shape information within programs.\n3.3\nShape Annotation Deduction\nThe symbolic shape information in annotations serves as an\nimportant source of information for optimization passes. To\nbest use this information, Relax automatically tracks and\ndeduces symbolic shape annotations of intermediate values\nduring model construction and between passes.\nEach tensor operator has a registered shape deduction rule\nthat takes the input shape annotations and the input val-\nues (such as the case of reshape) and returns the output\ndef subfn(s: Shape([\"n\", \"m\"])) -> Tensor((\"n * m\",), \"f32\"):\n  ...\ndef cross_func_shape_deduce_example(\n  x: Tensor((\"n\",), \"f32\"),\n  y: Shape(ndim=2)\n):\n  n = sym_var() \n  f0: Callable([Shape([\"n\", \"m\"])], Tensor((\"n * m\",), \"f32\")) = subfn\n  lv0: Tensor((n * 4,), \"f32\") = f0(shape(n, 4))\n  lv1: Tensor((12,), \"f32\") = subfn(shape(3, 4))\n  lv2: Tensor(((n + 1) * 4,), \"f32\") = subfn(shape(n + 1, 4))\n  lv3: Tensor(ndim=1, dtype=\"f32\") = subfn(y)\n  ...\nFigure 4. Exemplary scenarios of dynamic shape deduction that\ninvolve cross-function calls. subfn contains a signature that\ntakes shape with symbolic variable n, m and returns an one dimen-\nsional Tensor that contains shape n \u2217 m. The annotations in f0,\nlv0-lv3 are deduced by the system.\nannotations. We then adopt a forward deduction method that\ndeduces the annotation of an expression based on its input\ncomponents. Forward deduction is simple and local, and\nwe can obtain annotations for temporary variables during\ncompiler passes. Additionally, when shapes cannot be in-\nferred automatically, the forward deduction is able to use the\nresults of a user-inserted match cast in order to continue\ninferring later annotations.\nWhile the symbolic shape provides useful information, it\nopens new challenges to design the annotation system and\nthe shape deduction\u2014it is not always possible to perfectly\ndeduce and check the symbolic constraints at compile time.\nDespite this, our goal is still to provide as much static in-\nformation as possible. We also recognize the importance\nof being able to propagate interprocedural shape relations\nglobally across sub-function calls to accommodate inter-\nmediate results of optimization passes like fusion, where\nthe sub-functions themselves contain dynamic inputs and\noutputs. Figure 4 exemplifies cross-function symbolic shape\ndeduction. Our system is able to take the symbolic relations\nat function subfn and propagate the shapes correctly in\nthe callers. We summarize the key design principles of our\nshape annotation and deduction as follows:\nIsolate symbolic relations at function boundaries. The\nfunction signatures in Relax give annotations for the param-\neters and the return value, allowing for the annotations of\nfunction calls to be inferred using only the function signa-\nture. This allows for functions to be used as first-class values\nwith the Callable annotation. For example, we can de-\nduce the return shapes of subfn calls in Figure 4 by only\nlooking at its signature. The function signature also serves\nas source to generate runtime checks, namely to ensure that\npassed arguments match the annotations on the function\nparameter and that the returned value matches the return\nannotation. These shape checks are lightweight and do not\nimpact the overall performance. Such dynamic checks are\nused as a fallback when it cannot be statically concluded\nwhether the shapes match at compile time, allowing for the\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\ndef fused_add_relu(\n  x: Tensor((\"n * 2\",), \"f32\"), \n  y: Tensor((\"n * 2\",), \"f32\"), \n  s: Shape([\"n\"])\n) -> Tensor((\"n * 2\",), \"f32\"):\n  lv0 = add(x, y)\n  lv1 = relu(lv0)\n  return lv1\ndef main(x: Tensor((\"n\", 2), \"f32\")):\n  n = sym_var() \n  lv0: Tensor((\"2 * n\",), \"f32\") = flatten(x)\n  lv1: Tensor((\"2 * n\",), \"f32\") = fused_add_relu(lv0, lv0, shape(n))\n  ...\nExtra symbolic shape \nparameter to pass in n\nParameter annotation can \nbe an expression\nCall into fused \nfunction\ndef main(x: Tensor((\"n\", 2), \"f32\")):\n  n = sym_var() \n  lv0: Tensor((\"2 * n\",), \"f32\") = flatten(x)\n  lv1: Tensor((\"2 * n\",), \"f32\") = add(lv0, lv0)\n  lv2: Tensor((\"2 * n\",), \"f32\") = relu(lv1)\n  ...\nBefore fusion\nAfter fusion\nRegions to \nfuse\nFigure 5. Example function signature that contains symbolic ex-\npressions caused by a result of operator fusion.\nlocal forward deduction to continue.\nBest-effort symbolic deduction while preserving static\ninformation. Relax performs best-effort shape deduction\nbased on the symbolic shape relations, using dynamic\nchecks as a fallback when symbolic inference is infeasi-\nble. When more specific information cannot be inferred,\nwe allow deduction steps to return coarser-grained an-\nnotations such as Tensor(ndim=1, dtype=\"f32\"),\nwhich still conveys some information at compile time (viz.,\nthat there is only one dimension). This allows automatic\ninference to succeed for common cases but also allows sup-\nport for more general cases, albeit with less information\navailable for optimizations. Coarse-grained annotations\nserve as a safety net for such purposes. Note that it is per-\nmitted to pass values with coarse-grained annotations (e.g.,\nShape(ndim=2)) to functions that contain more specific\nsymbolic annotations like Shape((n, m)), as the run-\ntime checks generated at the function boundary ensure that\nthese match.\nSupport symbolic expressions in parameter signatures.\nBesides symbolic variables, we also support arithmetic ex-\npressions in function parameter annotations. This capability\nis important to simplify operator fusion and other cross-\nfunction transformations in dynamic shape settings. Fig-\nure 5 provides an example of such a case. This example\nintends to fuse two intermediate operators, add and relu.\nThe problem is that both intermediate values contain an\nexpression 2 \u00d7 n. A na\u00a8\u0131ve approach would create a new\nfunction with parameters x and y that have shape (2n, ), but\nn is nowhere supplied. To overcome this problem, the oper-\nator fusion pass will pass an extra parameter s that is bound\nto the runtime value of n. Passing an extra symbolic argu-\nment is a common pattern we use when designing passes\n# Graph-level end-to-end dynamic ML model\ndef main(x: Tensor((\"n\", 128), \"f32\"), w: Tensor((128, 256), \"f32\")):\n  n = sym_var()\n  with dataflow(): \n    lv0: Tensor((n, 256), \"f32\") = call_tir(\n      mm, [x, w], Tensor((n, 256),\"f32\")\n    )\n    lv1: Tensor((n, 256), \"f32\") = relu(lv0)\n    lv2: Tensor((n, 256), \"f32\") = call_dps_library(\n      \"cutlass.rms_norm\", [lv1], Tensor((n, 256), \"f32\")\n    )\n    \u2026\n# Loop-level tensor programs\n@tensorir_function\ndef mm(X: Buffer((\"n\", 128) \"f32\"), W: Buffer((128, 256), \"f32\"),\n       Y: Buffer((\"n\", 256), \"f32\")):\n  n = sym_var()\n  for i, j, k in grid(n, 256, 128):\n    with block():\n      with init():\n        Y[i, j] = 0\n      Y[i, j] += X[i, k] * W[k, j]\ncall library \nfunctions from \ngraph level\ncall loop-level \ntensor programs \nfrom graph level\nFigure 6. Cross-level abstractions: Graph-level function calls and\ncommunicates with loop-level TensorIR using call tir, and\ninvokes library functions via call dps library.\ndef call_tir(tir_func, args, annotation, sym_args):\n  # Allocate output tensor\n  output = alloc_tensor(annotation.shape, annotation.dtype)\n  # Call low-level function in destination-passing style\n  tir_func(*args, output, *sym_args)\n  return output\nFigure 7. The semantics explanation of call tir.\nthat lift out function regions and recombine. This approach\nalso enables us to preserve complicated symbolic relation\nchecks (e.g., a shape of (n + 1, n/2)) at run time.\nThe above three principles strike a balance between the need\nfor symbolic information and the complexity of deduction\nand checking. They make global symbolic shape tracking\nfeasible for common machine learning use cases and bring\nopportunities for dynamic shape\u2013aware optimizations.\n3.4\nCross-Level Abstraction\nThis section describes the elements in Relax that enable\ncross-level abstractions. Our main goal is to design prim-\nitives that naturally represent and optimize interactions of\ncomputational graphs, tensor programs, and libraries.\nTo achieve this goal, we must reconcile the different char-\nacteristics of each abstraction. Specifically, computational\ngraph abstractions favor pure operators that return a new\ntensor after each operation. This design allows us to or-\nganize computations through directed acyclic graphs and\nperform effective graph rewriting without worrying about\nside effects. On the other hand, most tensor programs and\nlibraries deal with low-level computations and often mu-\ntate their inputs. For these cases, we introduce a require-\nment to pass in input and output memory explicitly, via the\ndestination-passing style (Shaikhha et al., 2017) (DPS). The\nDPS abstracts away memory management concerns from\nlow-level code, simplifying code generation.\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\nlv0\nPartial lowering \nAnalysis feedback\nCross-level transform\ncall_dps_library\ncall_tir\nhigh-level \noperators \ntemp workspace \nallocation in \ntensor program\nlv1\nlv2\ntensor \nprogram\n for i:\n   B[i] = max(A[i], 0)\nlv1 is an element-wise operator and is \ninvariant to scaling. \nReduces cost of annotating these \nproperties per op. Generalize support \nto more custom ops.\n ws = alloc()\n ...\n ws = param[2]\n ...\nalloc in graph-level\npassed as param to \ntensor program\nlv0\nlv1\nlv2\nlv0\nlv1\nlv2\nlv0\nlv1\nlv2\nlv0\nlv1\nlv2\nFigure 8. Examples of common optimization patterns that lever-\nages cross-level abstraction.\nWe introduce two primitives shown in Figure 6 to bridge the\nabstraction levels. First, we introduce call tir, which\nallows us to directly invoke a tensor function from the graph\nlevel. We design the semantics of call tir (shown in\nFigure 7) to map directly to a DPS call of the loop-level\nfunction. This approach allows us to assign high-level se-\nmantics to call tir nodes during graph-level transforma-\ntions and lower these into DPS calls and perform memory\nmanagement with lower-level function calls later.\nNote that call tir also takes the shape of the output as\nan input, as well as potentially other symbolic shape argu-\nments. This allows for shape information to be passed from\nthe graph level into loop-level tensor programs. Such shape\ninformation is crucial for optimizations on loop-level pro-\ngrams, like operator fusion. By flowing the symbolic shape\ninformation from the graph level to tensor programs, we can\nallow tensor programs to generate code that specializes to\nmost static dimensions and only uses dynamic dimensions\nwhen necessary (like dimension n in Figure 7).\nSecond, we introduce the call dps library primitive\nto allow direct calls into third-party low-level operator li-\nbraries from the graph level. In practice, it allows for great\nflexibility in prototyping, since external routines can be\neasily called from a Relax program. The convention of\ncall dps library mirrors those of call tir, except\nthat the callee is instead the name of a third-party function.\nThese functions are be supplied by a registration system and\nlinked in the final runnable module.\nBenefits of cross-level abstractions. Figure 8 summarizes\ncommon optimization patterns that are enabled by cross-\nlevel abstractions:\nPartial lowering: Instead of making all lowering decisions\nin a single shot, a pass can make dispatch decisions or\nloop-level optimizations for portions of the computations.\nThis pattern is helpful, for example, when we would like\nto replace the lowering decision of a single fused operator\nto different libraries; we can then pass the program to later\npasses to handle other operators.\nAnalysis feedback: We can analyze the loop patterns in the\ntensor program and automatically annotate their operator\nproperties. Compiler developers usually need to manually\nannotate these properties for each high-level operator in the\nsystem. By adopting cross-level abstraction and instead\nrelying on analysis-based properties, we can greatly reduce\nthe engineering cost of annotation on each high-level tensor\noperator for each new pass of interest.\nCross-level transforms: Sometimes optimization opportuni-\nties can only be discovered after we perform low-level opti-\nmizations. For example, an auto-scheduler can decide that a\ntensor program needs a temporal workspace during a tensor\nprogram optimization pass. We can jointly transform the ten-\nsor program and graph-level this case to lift that workspace\nallocation out, which allows the temporal workspace to also\nparticipate as part of global memory planning. In a similar\nspirit, we can lift out quantization and layout transforms in\ntensor programs to enable pre-computation.\nWhile these optimizations patterns are useful alone, the\nreal benefit comes when we compose them together. For\nexample, we can perform partial lowering to libraries, then\noptimize the rest via other techniques.\n3.5\nComposable Optimizations\nThis section describes a concrete set of optimizations that\nmake use of the proposed abstractions in an end-to-end\ncompilation framework.\nCross-level dynamic shape\u2013aware operator fusion. Op-\nerator fusion helps to bring multiple operators together and\nreduce the overall memory loading cost of operators. Fig-\nure 9 shows the general flow of how Relax performs operator\nfusion. A Relax program can contain tensor programs that\nrepresent customized operators, such as online rotary em-\nbedding written in loops. We first get analysis feedback\nto annotate the pattern kind of each tensor program. This\ninformation can be used by the FuseOps pass to group the\noperators into sub-functions that represent fusion groups. Fi-\nnally, we perform FuseTensorIR, which is a cross-level\ntransformation that jointly updates the tensor programs as\nwell as the graph-level calling site that combines multiple\ntensor programs in each fusion group into a single tensor\nprogram function. We need to make sure that all the above\nsteps handle symbolic shapes by tracking the symbolic vari-\nables and generate extra symbolic variable parameters to\nsupport symbolic expressions in parameter signatures, as\nwe discussed in \u00a73.3. The fused program can then be fed\ninto subsequent tensor operator optimizations.\nThe three fusion sub-steps bring great opportunities for\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\ndef main(x: Tensor((\"n\", 128), \"f32\"),\n         w: Tensor((128, 256), \"f32\")):\n  n = sym_var()\n  with dataflow(): \n    lv1: Tensor((n, 256), \"f32\") = call_tir(\n      fused_mm_relu, [x, w], Tensor((n, 256),\"f32\")\n    )\n    return lv1\n@tensorir_function\ndef fused_mm_relu(X: Buffer((\"n\", 128), \"f32\"),\n                  W: Buffer((128, 256), \"f32\"),\n                  Z: Buffer((\"n\", 256), \"f32\")):\n  n = sym_var()\n  Y = alloc_buffer((n, 256), \"f32\")\n  # matmul\n  for i, j, k in grid(n, 256, 128):\n    with block():\n      with init():\n        Y[i, j] = 0\n      Y[i, j] += X[i, k] * W[k, j]\n  # relu\n  for i, j in grid(n, 256):\n    with block():\n      Z[i, j] += max(Y[i, j], float32(0))\ndef main(x: Tensor((\"n\", 128), \"f32\"),\n         w: Tensor((128, 256), \"f32\")):\n  n = sym_var()\n  with dataflow(): \n    lv0: Tensor((n, 256), \"f32\") = call_tir(\n      mm, [x, w], Tensor((n, 256),\"f32\")\n    )\n    lv1: Tensor((n, 256), \"f32\") = relu(lv0)\n    return lv1\n@tensorir_function\ndef mm(X: Buffer((\"n\", 128) \"f32\"),\n       W: Buffer((128, 256), \"f32\"),\n       Y: Buffer((\"n\", 256), \"f32\")):\n  n = sym_var()\n  for i, j, k in grid(n, 256, 128):\n    with block():\n      with init():\n        Y[i, j] = 0\n      Y[i, j] += X[i, k] * W[k, j]\n@tensorir_function\ndef relu(X: Buffer((\"n\", 256) \"f32\"),\n         Y: Buffer((\"n\", 256), \"f32\")):\n  n = sym_var()\n  for i, j in grid(n, 256):\n    with block():\n      Y[i, j] += max(X[i, j], float32(0))\ndef main(x: Tensor((\"n\", 128), \"f32\"),\n         w: Tensor((128, 256), \"f32\")):\n  n = sym_var()\n  with dataflow(): \n    lv1: Tensor((n, 256), \"f32\") = fused_mm_relu(x)\n    return lv1\ndef fused_mm_relu(x: Tensor((\"n\", 128), \"f32\"),\n                  w: Tensor((128, 256), \"f32\")):\n  n = sym_var()\n  with dataflow(): \n    lv0: Tensor((n, 256), \"f32\") = call_tir(\n      mm, [x, w], Tensor((n, 256),\"f32\")\n    )\n    lv1: Tensor((n, 256), \"f32\") = relu(lv0)\n    return lv1\n@tensorir_function\ndef mm(X: Buffer((\"n\", 128) \"f32\"),\n       W: Buffer((128, 256), \"f32\"),\n       Y: Buffer((\"n\", 256), \"f32\")):\n  func_attr(\"compute_pattern\", \"OutputEwiseFusible\")\n  ...\n@tensorir_function\ndef relu(X: Buffer((\"n\", 256) \"f32\"),\n         Y: Buffer((\"n\", 256), \"f32\")):\n  func_attr(\"compute_pattern\", \"ElementWise\")\n  ...\nCompute pattern analysis\n+ FuseOps\n+ FuseTensorIR\nInitial Program\nFollow-up scheduling\nof TensorIR\nFigure 9. Dynamic shape\u2013aware operator fusion in Relax. Compute pattern analysis classifies each tensor program function to a pattern\nkind. FuseOps makes use of these pattern detects fusion patterns, constructs new sub-functions for detected regions and transforms\ndetected regions to cross-function calls. FuseTensorIR merges tensor functions into one replaces cross-function calls by call tir.\nx\n(2, n),\"f32\"\nlv0 = exp(x)\nlv1 = \ntranspose(lv0)\nlv2 = \nrelu(lv1)\nlv3 = \ntranspose(lv2)\nAllocate \n(2,n) for lv0\nAllocate \n(n,2) for lv1\nAllocate\n(n,2) for lv2\nAllocate \n(2,n) for lv3\nBefore memory planning:\nAfter memory planning:\nx\n(2, n),\"f32\"\nlv0 = exp(x)\nlv1 = \ntranspose(lv0)\nlv2 = \nrelu(lv1)\nlv3 = \ntranspose(lv2)\nAllocate 2*n*4 \nbytes for s0\nAllocate 2*n*4 \nbytes for s1\nInstantiate\nlv0:(2,n)\nfrom s0\nInstantiate\nlv2:(n,2)\nfrom s0\nlv1:(n,2)\nfrom s1\nlv3:(2,n)\nfrom s1\nFigure 10. Dynamic shape\u2013aware memory planning example. Be-\nfore planning, all four intermediate tensors are individually allo-\ncated. After memory planning, the intermediate tensors reuse two\nallocated storage chunks.\nfurther composition and customization. For example, we\ncan provide a rewrite that fuses new set of patterns that are\nnot covered by FuseOps (e.g., fusing all sub-operators in\nattention), and then pass the result to FuseOps to cover the\nremaining part. FuseTensorIR can finally consume the\nfusion group information from both customized fusion and\nstandard fusion. This approach allows us to quickly try out\nand compose different fusion passes, improving the overall\nproductivity of continuous compiler development.\nDynamic shape\u2013aware memory planning. Memory is a\nvery important resource in modern machine learning appli-\ncations. Most ML compilers can plan memory reuse by\ncomparing sizes of statically shaped tensors and allocating a\nfixed set of memory blocks ahead of time to reduce the run-\ntime memory allocation overhead. Normally, the compiler\ncannot take the same approach for compile-time unknown\ndef main(x: Tensor((\"n\", 128), \"f32\"), w: Tensor((128, 256), \"f32\")):\n  n = sym_var()\n  with dataflow(): \n    lv0: Tensor((n, 256), \"f32\") = call_tir(\n      mm, [x, w], Tensor((n, 256),\"f32\")\n    )\n    lv1: Tensor((n, 256), \"f32\") = rms_norm(lv0)\n    lv1: Tensor((n, 256), \"f32\") = call_dps_library(\n      \"cutlass.rms_norm\", [lv0], Tensor((n, 256), \"f32\")\n    )\n    ...\n@tensorir_function\ndef mm(X: Buffer((\"n\", 128) \"f32\"),\n       W: Buffer((128, 256), \"f32\"),\n       Y: Buffer((\"n\", 256), \"f32\")):\n  n = sym_var()\n  for i, j, k in grid(n, 256, 128):\n    with block():\n      with init():\n        Y[i, j] = 0\n      Y[i, j] += X[i, k] * W[k, j]\nI. Replacement from composable \npartial lowering passes  \nTensorIR function body\nProgram analysis and \ntransformations\nOptimized function body\nII. Loop-level TensorIR optimization\nupdate\nFigure 11. Tensor operator optimization examples.\nshapes and must rely on the runtime memory allocators.\nWith symbolic shape abstractions, however, we can analyze\nand compare tensor shapes and plan for their reuse accord-\ningly. Figure 10 shows how we can apply memory planning\nwith dynamic shapes. We can further combine memory\nplanning with other cross-level transformations. Specifi-\ncally, we can lift workspace allocations in tensor programs\nand then perform the global memory planning to further\nincrease reuse. Finally, we also take the upper bound of the\nsymbolic values (if they are known) and statically allocate\nadequate memory. This approach allows for creating a static\nmemory allocation plan ahead of time, even in the presence\nof dynamic shapes. Such predictable memory consumption\nestimation is crucial for deploying dynamic ML models on\nmemory-limited backends.\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\nRTX4090 7900XTX M2 Ultra\n0\n50\n100\n150\n200\nPerformance (tokens/s)\n61\n161\n117\n87\n204\n146\n89\nLlama2-7B\nRTX4090 7900XTX M2 Ultra\n0\n25\n50\n75\n100\n49\n96\n76\n53\n114\n82\n55\nLlama2-13B\nRTX4090 7900XTX M2 Ultra\n0\n10\n20\n30\n40\n35\n41\n36\n26\n45\n37\n24\nCodeLlama-34B\nHF Transformers\nllama.cpp\nRelax (Ours)\nFigure 12. Single-batch 4-bit Llama2 inference performance on NVIDIA RTX 4090, AMD 7900 XTX and Apple M2 Ultra. We omitted\nthe results of HF on AMD and Apple as it only supports 4-bit inference on NVIDIA GPUs. Relax brings competitive performance across\ndifferent backends with up to 1.25x speedup.\nTensor operator optimizations via partial lowering. Mod-\nern machine learning frameworks make use of two cate-\ngories of methods to optimize tensor operators. On one\nhand, we can offload compute to platform-specific oper-\nator libraries. We can also make use of tensor program\noptimizations and code generation. Most existing ML com-\npilation frameworks make the optimization decisions at the\nboundary between the graph level and lower levels. This\napproach makes it hard to compose different lowering ap-\nproaches. For instance, we want to use a new library for\ncertain operators, we need to carefully examine the existing\nlowering strategies and update accordingly. The complexity\nof the lowering layer grows as we try to incorporate more\napproaches such as different ways of auto-scheduling.\nRelax builds tensor operator optimizations via partial low-\nering (shown in Figure 11). We build passes that detects\nspecific patterns in the high-level computation, and partially\nlower detected regions to library calls. Additionally we\nbuild a set of dynamic shape\u2013aware loop-level schedule\nrules based on analysis to optimize tensor program func-\ntions. We can also include extra passes that apply the result\nof auto-tuning (Chen et al., 2019; Zheng et al., 2020; Shao\net al., 2022; Feng et al., 2023) to replace tensor programs.\nImportantly, all these transformations can be composed to-\ngether to get the best of the world. Our approach enables\nfast development and only requires implementing a single\nrelatively simple partial lowering pass to customize the be-\nhaviors of tensor operator optimizations.\n3.6\nBuilding a Runnable Module\nThis section outlines transforming an end-to-end dynamic\nML model in Relax into a runnable module after apply-\ning optimizations. Since first-class symbolic shapes are\nso crucial to Relax, a fundamental task of the lowering is\nto associate symbolic variables with concrete shape values\nand compute all symbolic expressions at run time. We cre-\nate an integer tensor on the host that stores runtime values\nof all symbolic expressions encountered. We also directly\nlower call tir and call dps library into function\ncalls of the corresponding target functions via DPS. At the\nstart of transformation, we populate the values of symbolic\nvariables in the tensor based on program inputs. We then\ngenerate tensor programs that load from the tensor, evaluate\nsymbolic expressions, and store results to corresponding lo-\ncations. Finally, we insert function calls to construct shape\ntuple objects when tensor shapes are needed as first-class\nvalues. After this transformation, we can erase all anno-\ntations, leaving a program comprised mainly of low-level\nfunction calls. At the end of the build, the calls are lowered\nto a sequence of virtual machine instructions, each of which\nis a call into a generated or builtin functions.\n4\nEVALUATION\nWe implement Relax on top of Apache TVM (Chen et al.,\n2018). Notably, the insights presented in this paper can also\nbenefit other ML compilation frameworks as well. This sec-\ntion provides evaluation to address the following questions:\n\u2022 Can Relax bring optimized performance on emerging\nlarge language model inference (\u00a74.1)?\n\u2022 Can Relax support a broad set of resource-constrained\nenvironments (\u00a74.2)?\n\u2022 What is the impact of operator fusion and partial library\nlowering (\u00a74.3)?\n\u2022 How well does Relax support static shapes (\u00a74.4)?\n4.1\nLarge Language Model Inference Evaluation\nThis section evaluates if Relax can bring optimized perfor-\nmance to end-to-end large language model inference in GPU\nenvironments. We evaluate our solutions on Llama2-7B,\nLlama2-13B and CodeLlama-34B on NVIDIA RTX 4090,\nAMD 7900 XTX and Apple M2 Ultra. We choose Hug-\ngingFace Transformers library (v4.33.3) (Wolf et al., 2020)\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\nTable 2. Inference performance (tokens/sec) of RedPajama-3B and\nLlama2-7B models on a broad set of mobile and edge devices.\nDevices\nBackend\nRP-3B\nLlama2-7B\niPhone 14 Pro Max\nMetal\n11.7\n7.0\nSamsung S23 Ultra\nOpenCL\n11.9\n7.3\nOrange Pi 5\nOpenCL\n5.0\n2.5\nSteam Deck\nVulkan\n20.1\n12.4\non PyTorch (v2.1) (Paszke et al., 2019) as a framework\nreference point. We also include llama.cpp (commit:\ne393259) (Gerganov, 2023) as a baseline. We use 4-bit\ninteger (int4) weight quantization and float16 activa-\ntions for all models. We measure the time of generating 256\ntokens with a single-batch 8-token prompt and compute the\ntokens/second generation performance.\nFigure 12 shows that Relax brings up to 1.25\u00d7 speedup\nto the baseline methods. In these cases, the baseline solu-\ntions contain specific GPU kernels for each platform. The\nsymbolic shape analysis enables Relax to generate tensor\nprograms that specialize to static dimensions and only bring\ndynamic dimensions when necessary. Additionally, it can\nautomatically perform operator fusion to reduce the over-\nhead of fine-grained operators. Finally, cross-level abstrac-\ntions enable us to seamlessly compose the graph and tensor\nprogram optimization across different hardware backends,\neliminating the need to manually write kernels for each GPU\nbackend. Indeed, note the different levels of support for each\nkind of GPU in the baselines. Relax can reduce the over-\nall engineering effort and bring more uniform competitive\nperformance across the GPU backends.\n4.2\nEvaluation on a Broader Set of Devices\nIn this section, we explore the feasibility of running emerg-\ning large language models on a broader set of less-supported\nplatforms. Specifically, we evaluate language model infer-\nence performance on a Samsung S23 Ultra with Qualcomm\nSnapdragon 8 Gen 2, iPhone 14 Pro Max, an Orange Pi 5\nwith ARM Mali GPU, and a Steam Deck equipped with an\nAMD APU. We run RedPajama-3B and Llama2-7B using\n4-bit quantization for most of the cases. We use 3-bit quan-\ntization for Llama2 on iPhone due to the 4GB VRAM limit\nrequired by iOS. To the best of our knowledge, Relax is\nthe first solution that enables GPU-accelerated LLM infer-\nence on these platforms. Without the dynamic shape\u2013aware\nmemory planning pass, these models are not even runnable\ndue to memory constraints.\nTable 2 displays the performance results. Relax provides a\ndecent token generation rate, which allows for a satisfying\ninteractive chat experience on these devices. One impor-\ntant characteristic of these devices is that they have limited\nVRAM capacity and significant GPU memory allocation\nLlama-2 7B\nLlama-2 13B\n0\n10\n20\n30\nPerformance (tokens/s)\n16\n8\n29\n15\n35\n30\nM1 Pro\nM2 Max\nRTX 4090\nFigure 13. Llama2 inference performance through WebGPU in\nGoogle Chrome, on different native platforms.\n7B\n13B\n34B\n0\n50\n100\n150\n200\nPerformance (tokens/s)\n145\n87\n38\n177\n100\n43\n204\n114\n45\nRelax w/o PLL & fusion\nRelax w/o PLL\nRelax\nFigure 14. Impact of partial library lowering (PLL) and operator\nfusion on Llama2 inference.\noverhead. Our dynamic shape\u2013aware memory planning\nhelps pre-allocate all the necessary memory at the start and\nkeeps it within the allocation budget.\nWe further evaluate Relax on WebGPU (Dakkak et al.,\n2016), a web API which enables native GPU executions\nin web browsers. WebGPU enables the deployment of mod-\nels locally on a web browser sandbox without further local\ndependencies. The results are shown in Figure 13. These re-\nsults show that Relax can enable deploying LLMs with this\nemerging backend and support web-native machine learning.\nNotably, WebGPU just shipped to Chrome earlier this year,\nand there is still some runtime overhead compared to native\nGPU runtimes. Nevertheless, it can still be very useful for\nusers seeking out-of-the-box in-browser solutions.\n4.3\nImpact of Partial Library Lowering and Fusion\nOne benefit of Relax\u2019s composable abstractions is that we\ncan flexibly combine optimizations including partial library\nlowering, operator fusion, and code generation. Figure 14\nshows the ablation study results by comparing the perfor-\nmance of Relax, Relax without partial library lowering, and\nRelax without both partial library lowering and fusion. Dy-\nnamic shape\u2013aware fusion helps contribute up to 18% of\nthe performance in the case without partial library lower-\ning. By incorporating CUTLASS fused attention and RMS\nnorm kernels in code generation, we obtain up to a 1.15\u00d7\nperformance improvement. Regarding libraries, our key\ntakeaway is that the machine learning compilation pipeline\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\nResNet50\nMobileNet\n0\n2\n4\n6\nNormalized Performance\n1.00\n1.00\n3.05\n7.04\n2.90\n6.49\nTorch\nTVM\nRelax (Ours)\nFigure 15. Normalized ResNet50 and MobileNet inference per-\nformance on NVIDIA RTX 4090. Relax performs similarly to\nspecialized static shape model compilers on these settings.\nalso needs continuous updates to keep up with the state of\nthe art. Having a composable cross-level abstraction helps\nus apply the latest improvements more easily, either through\nloop-level compiling techniques or external library calls.\n4.4\nStatically Shaped Model Evaluation\nThis section evaluates Relax on more traditional models\nwith static shapes. We evaluate our results on ResNet50\nand MobileNet inference in float32 on NVIDIA RTX\n4090. We pick PyTorch with compile mode and Apache\nTVM with static shape optimizations as reference points.\nFigure 15 shows that Relax has performance close to TVM\u2019s\nstatic graph runtime, which is tailored for static shapes.\nThese results show Relax can efficiently support models\nwith both static and dynamic shapes.\n5\nRELATED WORK\nVendor-optimized libraries like cuDNN (Chetlur et al.,\n2014), CUTLASS (Thakkar et al., 2023), MKL-DNN (Intel,\n2017), and MIOpen (Khan et al., 2019) are frequently used\nby deep learning frameworks (Abadi et al., 2016; Paszke\net al., 2019) to support tensor operators on various hardware\nbackends. The libraries are specific to each platform and\nhave large engineering development costs to cover the grow-\ning demand variants of operators, data formats, and layouts.\nRelax complements the use of such libraries by allowing\nthem to be used alongside loop-level code optimized with\ndynamic shape\u2013aware code generation. Frameworks that\nrely on libraries can potentially instead use Relax to choose\nbetween libraries or generated code.\nThe emerging demand for large language models has also\ninspired a set of frameworks (Wolf et al., 2020; Kwon et al.,\n2023; Gerganov, 2022; 2023) optimized for these particu-\nlar workloads. These frameworks usually rely on manual\noptimizations for each specific backend. They can leverage\nRelax to reduce the effort for supporting a broader set of\nworkloads and backends.\nThere has also been much work on abstractions for trans-\nforming and optimizing loop-level code for tensor operators.\nTriton (Tillet et al., 2019) and Graphene (Hagedorn et al.,\n2023) are abstractions that optimize tensorized programs\non GPU. DietCode (Zheng et al., 2022a), CoRA (Fegade\net al., 2022), and SparseTIR (Ye et al., 2023), focus on\ntensor program optimizations with shape dynamism and\nirregularity. Mosaic (Bansal et al., 2023) is a sparse com-\npiler combining library dispatch and sparse tensor program\noptimizations. Cortex (Fegade et al., 2021) enables ten-\nsor program optimizations for recursive computation. We\nuse TensorIR (Feng et al., 2023) as the tensor program ab-\nstraction in our cross-level design implementation, but we\ncould combine our approaches with other abstractions for\nloop-level programs to enable a broader spectrum of tensor\nprogram optimizations.\nMachine learning compilers are designed to represent and\noptimize end-to-end model computations. High-level com-\nputations are usually represented with computation graph\u2013\nstyle dialects. TVM (Chen et al., 2018)\u2019s Relay (Roesch\net al., 2018) and MLIR dialects (Lattner et al., 2021) usu-\nally represent dynamic dimensions as unknown and do not\ntrack dynamic shape relations. IREE (IREE Project, 2019)\nprovides end-to-end compilation with MLIR. Nimble (Shen\net al., 2021) leverages runtime bucketing to support dy-\nnamic operators. DISC (Zhu et al., 2021; Zheng et al.,\n2022b) enables shape as a first-class value but does not pro-\nvide symbolic shape tracking. We also notice a growing\ninterest in symbolic shape tracking concurrently with our\nwork. TorchInductor (Ansel, 2023) brings native symbolic\nshape support to the PyTorch compiler, focusing on kernel\ngeneration for TorchFX graphs (Reed et al., 2022) derived\nfrom TorchDynamo (Ansel, 2022). PyTorch compiler stores\na global symbolic variable table for traced subgraphs. It\nis synergistic with its JIT-focused design and avoids cross-\nfunction symbolic shape tracking. Axon (Collins & Grover,\n2022) is a functional language that considers shape deduc-\ntion in its types, applying a constraint solver to determine\nrelationships between shapes; unlike Relax, it does not de-\nscribe having a dynamic fallback for cases when shapes\ncannot be deduced statically. (Note that Relax could, in prin-\nciple, apply a similar constraint-solving approach, though\nit may be more expensive to use at compile time.) Addi-\ntionally, most existing ML compilers follow a multi-level\napproach, whereas Relax enables global symbolic shape\ntracking across functions via its cross-level abstractions.\nThe insights in Relax\u2019s support for dynamic shapes and\ncross-level optimizations can be used to improve these ML\ncompiler frameworks.\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\n6\nCONCLUSION\nWe introduce Relax, a composable abstraction for end-to-\nend dynamic machine learning. Our first-class symbolic\nshapes and cross-level abstractions enable composable opti-\nmizations of dynamic shape models and allow us to build\nan end-to-end framework that deploys dynamic models to\ndiverse backends. We hope this work will encourage addi-\ntional studies of dynamic shape\u2013aware program abstractions\nand highlight new possibilities for ML compilers.\nREFERENCES\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\nTensorflow: a system for large-scale machine learning.\nIn 12th USENIX symposium on operating systems design\nand implementation (OSDI 16), pp. 265\u2013283, 2016.\nAnsel, J. Torchdynamo, January 2022. URL https://\ngithub.com/pytorch/torchdynamo.\nAnsel, J.\nTorchinductor:\na pytorch-native compiler\nwith define-by-run ir and symbolic shapes, 2023.\nURL https://dev-discuss.pytorch.org/t/\n747.\nBai, J., Lu, F., Zhang, K., et al.\nOnnx: Open neural\nnetwork exchange. https://github.com/onnx/\nonnx, 2019.\nBansal, M., Hsu, O., Olukotun, K., and Kjolstad, F. Mosaic:\nAn interoperable compiler for tensor algebra. Proceed-\nings of the ACM on Programming Languages, 7(PLDI):\n394\u2013419, 2023.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. Gpt-neox-20b: An\nopen-source autoregressive language model, 2022.\nChen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen,\nH., Cowan, M., Wang, L., Hu, Y., Ceze, L., et al. Tvm:\nAn automated end-to-end optimizing compiler for deep\nlearning. In 13th USENIX Symposium on Operating Sys-\ntems Design and Implementation (OSDI 18), pp. 578\u2013594,\n2018.\nChen, T., Zheng, L., Yan, E., Jiang, Z., Moreau, T., Ceze, L.,\nGuestrin, C., and Krishnamurthy, A. Learning to optimize\ntensor programs, 2019.\nChetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran,\nJ., Catanzaro, B., and Shelhamer, E. cudnn: Efficient\nprimitives for deep learning, 2014.\nCollins, A. and Grover, V. Axon: A language for dynamic\nshapes in deep learning graphs, 2022.\nDakkak, A., Pearson, C., and Hwu, W.-m. Webgpu: A scal-\nable online development platform for gpu programming\ncourses. In 2016 IEEE International Parallel and Dis-\ntributed Processing Symposium Workshops (IPDPSW),\npp. 942\u2013949. IEEE, 2016.\nFegade, P., Chen, T., Gibbons, P., and Mowry, T. Cortex: A\ncompiler for recursive deep learning models. Proceedings\nof Machine Learning and Systems, 3:38\u201354, 2021.\nFegade, P., Chen, T., Gibbons, P., and Mowry, T.\nThe cora tensor compiler:\nCompilation for ragged\ntensors with minimal padding.\nIn Marculescu, D.,\nChi, Y., and Wu, C. (eds.), Proceedings of Machine\nLearning and Systems,\nvolume 4,\npp. 721\u2013747,\n2022.\nURL\nhttps://proceedings.mlsys.\norg/paper_files/paper/2022/file/\nafe8a4577080504b8bec07bbe4b2b9cc-Paper.\npdf.\nFeng, S., Hou, B., Jin, H., Lin, W., Shao, J., Lai, R., Ye, Z.,\nZheng, L., Yu, C. H., Yu, Y., et al. Tensorir: An abstrac-\ntion for automatic tensorized program optimization. In\nProceedings of the 28th ACM International Conference\non Architectural Support for Programming Languages\nand Operating Systems, Volume 2, pp. 804\u2013817, 2023.\nGerganov,\nG.\nggml.\nhttps://github.com/\nggerganov/ggml, 2022.\nGerganov, G.\nllama.cpp.\nhttps://github.com/\nggerganov/llama.cpp, 2023.\nHagedorn, B., Fan, B., Chen, H., Cecka, C., Garland, M.,\nand Grover, V. Graphene: An ir for optimized tensor\ncomputations on gpus. In Proceedings of the 28th ACM\nInternational Conference on Architectural Support for\nProgramming Languages and Operating Systems, Volume\n3, pp. 302\u2013313, 2023.\nIntel.\nIntel\u00ae math kernel library for deep learning\nnetworks, 2017. URL https://www.intel.com/\ncontent/www/us/en/developer/tools/\noneapi/onednn.html.\nIREE Project. IREE, sep 2019. URL https://github.\ncom/openxla/iree.\nKhan, J., Fultz, P., Tamazov, A., Lowell, D., Liu, C., Me-\nlesse, M., Nandhimandalam, M., Nasyrov, K., Perminov,\nI., Shah, T., Filippov, V., Zhang, J., Zhou, J., Natarajan,\nB., and Daga, M. Miopen: An open source library for\ndeep learning primitives, 2019.\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. arXiv preprint arXiv:2309.06180,\n2023.\nLattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis,\nA., Pienaar, J., Riddle, R., Shpeisman, T., Vasilache, N.,\nand Zinenko, O. Mlir: Scaling compiler infrastructure\nfor domain specific computation. In 2021 IEEE/ACM\nInternational Symposium on Code Generation and Opti-\nmization (CGO), pp. 2\u201314. IEEE, 2021.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information\nprocessing systems, 32, 2019.\nReed, J., DeVito, Z., He, H., Ussery, A., and Ansel,\nJ.\ntorch.fx: Practical program capture and transfor-\nmation for deep learning in python.\nIn Marculescu,\nD., Chi, Y., and Wu, C. (eds.), Proceedings of Ma-\nchine Learning and Systems, volume 4, pp. 638\u2013651,\n2022.\nURL\nhttps://proceedings.mlsys.\norg/paper_files/paper/2022/file/\n7c98f9c7ab2df90911da23f9ce72ed6e-Paper.\npdf.\nRoesch, J., Lyubomirsky, S., Weber, L., Pollock, J.,\nKirisame, M., Chen, T., and Tatlock, Z. Relay: a new\nIR for machine learning frameworks. In Proceedings\nof the 2nd ACM SIGPLAN International Workshop on\nMachine Learning and Programming Languages. ACM,\njun 2018. doi: 10.1145/3211346.3211348. URL https:\n//doi.org/10.1145%2F3211346.3211348.\nRoesch, J. G. Principled Optimization Of Dynamic Neural\nNetworks. PhD thesis, University of Washington, 2020.\nRozi`ere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan,\nX. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov,\nA., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C.,\nGrattafiori, A., Xiong, W., D\u00b4efossez, A., Copet, J., Azhar,\nF., Touvron, H., Martin, L., Usunier, N., Scialom, T., and\nSynnaeve, G. Code llama: Open foundation models for\ncode, 2023.\nShaikhha, A., Fitzgibbon, A., Peyton Jones, S., and Vytin-\niotis, D. Destination-passing style for efficient mem-\nory management.\nIn Proceedings of the 6th ACM\nSIGPLAN International Workshop on Functional High-\nPerformance Computing, FHPC 2017, pp. 12\u201323, New\nYork, NY, USA, 2017. Association for Computing\nMachinery.\nISBN 9781450351812.\ndoi: 10.1145/\n3122948.3122949.\nURL https://doi.org/10.\n1145/3122948.3122949.\nShao, J., Zhou, X., Feng, S., Hou, B., Lai, R., Jin, H.,\nLin, W., Masuda, M., Yu, C. H., and Chen, T. Tensor\nprogram optimization with probabilistic programs. Ad-\nvances in Neural Information Processing Systems, 35:\n35783\u201335796, 2022.\nShen, H., Roesch, J., Chen, Z., Chen, W., Wu, Y.,\nLi, M., Sharma, V., Tatlock, Z., and Wang, Y.\nNimble:\nEfficiently compiling dynamic neural net-\nworks for model inference.\nIn Smola, A., Dimakis,\nA., and Stoica, I. (eds.), Proceedings of Machine\nLearning and Systems,\nvolume 3,\npp. 208\u2013222,\n2021.\nURL\nhttps://proceedings.mlsys.\norg/paper_files/paper/2021/file/\n5b47430e24a5a1f9fe21f0e8eb814131-Paper.\npdf.\nThakkar, V., Ramani, P., Cecka, C., Shivam, A., Lu, H.,\nYan, E., Kosaian, J., Hoemmen, M., Wu, H., Kerr, A.,\nNicely, M., Merrill, D., Blasig, D., Qiao, F., Majcher, P.,\nSpringer, P., Hohnerbach, M., Wang, J., and Gupta, M.\nCUTLASS, jan 2023. URL https://github.com/\nNVIDIA/cutlass.\nTillet, P., Kung, H.-T., and Cox, D. Triton: an intermediate\nlanguage and compiler for tiled neural network computa-\ntions. In Proceedings of the 3rd ACM SIGPLAN Interna-\ntional Workshop on Machine Learning and Programming\nLanguages, pp. 10\u201319, 2019.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\nand Scialom, T. Llama 2: Open foundation and fine-tuned\nchat models, 2023.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I.\nAttention is all you need. In Guyon, I., Luxburg, U. V.,\nBengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,\nand Garnett, R. (eds.), Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc.,\n2017.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2017/file/\nRelax: Composable Abstractions for End-to-End Dynamic Machine Learning\n3f5ee243547dee91fbd053c1c4a845aa-Paper.\npdf.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., De-\nlangue, C., Moi, A., Cistac, P., Ma, C., Jernite, Y.,\nPlu, J., Xu, C., Le Scao, T., Gugger, S., Drame,\nM., Lhoest, Q., and Rush, A. M.\nTransformers:\nState-of-the-Art Natural Language Processing.\npp.\n38\u201345. Association for Computational Linguistics, Oc-\ntober 2020.\nURL https://www.aclweb.org/\nanthology/2020.emnlp-demos.6.\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao,\nC., and Jiang, D. Wizardlm: Empowering large language\nmodels to follow complex instructions. arXiv preprint\narXiv:2304.12244, 2023.\nYe, Z., Lai, R., Shao, J., Chen, T., and Ceze, L. Sparsetir:\nComposable abstractions for sparse compilation in deep\nlearning. In Proceedings of the 28th ACM International\nConference on Architectural Support for Programming\nLanguages and Operating Systems, Volume 3, pp. 660\u2013\n678, 2023.\nZheng, B., Jiang, Z., Yu, C. H., Shen, H., Fromm, J.,\nLiu, Y., Wang, Y., Ceze, L., Chen, T., and Pekhi-\nmenko, G.\nDietcode:\nAutomatic optimization for\ndynamic tensor programs.\nIn Marculescu, D., Chi,\nY.,\nand Wu,\nC. (eds.),\nProceedings of Machine\nLearning and Systems,\nvolume 4,\npp. 848\u2013863,\n2022a.\nURL https://proceedings.mlsys.\norg/paper_files/paper/2022/file/\nf89b79c9a28d4cae22ef9e557d9fa191-Paper.\npdf.\nZheng, L., Jia, C., Sun, M., Wu, Z., Yu, C. H., Haj-\nAli, A., Wang, Y., Yang, J., Zhuo, D., Sen, K., Gon-\nzalez, J. E., and Stoica, I.\nAnsor: Generating High-\nPerformance tensor programs for deep learning. In 14th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI 20), pp. 863\u2013879. USENIX As-\nsociation, November 2020. ISBN 978-1-939133-19-9.\nURL https://www.usenix.org/conference/\nosdi20/presentation/zheng.\nZheng, Z., Yang, X., Zhao, P., Long, G., Zhu, K., Zhu, F.,\nZhao, W., Liu, X., Yang, J., Zhai, J., et al. Astitch: en-\nabling a new multi-dimensional optimization space for\nmemory-intensive ml training and inference on modern\nsimt architectures. In Proceedings of the 27th ACM In-\nternational Conference on Architectural Support for Pro-\ngramming Languages and Operating Systems, pp. 359\u2013\n373, 2022b.\nZhu, K., Zhao, W., Zheng, Z., Guo, T., Zhao, P., Zhu, F.,\nBai, J., Yang, J., Liu, X., Diao, L., and Lin, W. Disc: A\ndynamic shape compiler for machine learning workloads,\n2021.\n"
  },
  {
    "title": "VR-NeRF: High-Fidelity Virtualized Walkable Spaces",
    "link": "https://arxiv.org/pdf/2311.02542.pdf",
    "upvote": "13",
    "text": "VR-NeRF: High-Fidelity Virtualized Walkable Spaces\nLinning Xu\nThe Chinese University of Hong Kong\nHong Kong\nMeta\nUSA\nVasu Agrawal\nMeta\nUSA\nWilliam Laney\nMeta\nUSA\nTony Garcia\nMeta\nUSA\nAayush Bansal\nMeta\nUSA\nChangil Kim\nMeta\nUSA\nSamuel Rota Bul\u00f2\nMeta\nSwitzerland\nLorenzo Porzi\nMeta\nSwitzerland\nPeter Kontschieder\nMeta\nSwitzerland\nAlja\u017e Bo\u017ei\u010d\nMeta\nSwitzerland\nDahua Lin\nThe Chinese University of Hong Kong\nHong Kong\nMichael Zollh\u00f6fer\nMeta\nUSA\nChristian Richardt\nMeta\nUSA\n22\u00d7 HDR Images\nEyeful Tower Captures\nWalkable\n  Spaces\nRendered Color at Varying Exposures\nRendered Color\nDepth\nEstimated Normal\nOccupancy Grid\nLOD Level\nVR-NeRF Modeling\nVR-NeRF Rendering\nTwo 2K\u00d72K Eyebufers\nFigure 1: VR-NeRF brings high-fidelity walkable spaces to real-time virtual reality. Our \u201cEyeful Tower\u201d camera rig captures\nspaces with high image resolution and dynamic range that approach the limits of the human visual system. We train high-\nfidelity neural radiance fields that exploit the high-dynamic range nature of our captured scenes and provide level-of-detail\nmip-mapping for efficient anti-aliasing. Our rendering backend leverages our accurate occupancy grid and a dynamic multi-GPU\nwork distribution scheme to achieve real-time frame rates on dual 2K\u00d72K eyebuffers for an immersive VR experience.\nABSTRACT\nWe present an end-to-end system for the high-fidelity capture,\nmodel reconstruction, and real-time rendering of walkable spaces in\nvirtual reality using neural radiance fields. To this end, we designed\nand built a custom multi-camera rig to densely capture walkable\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\n\u00a9 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0315-7/23/12.\nhttps://doi.org/10.1145/3610548.3618139\nspaces in high fidelity and with multi-view high dynamic range\nimages in unprecedented quality and density. We extend instant\nneural graphics primitives with a novel perceptual color space for\nlearning accurate HDR appearance, and an efficient mip-mapping\nmechanism for level-of-detail rendering with anti-aliasing, while\ncarefully optimizing the trade-off between quality and speed. Our\nmulti-GPU renderer enables high-fidelity volume rendering of our\nneural radiance field model at the full VR resolution of dual 2K\u00d72K\nat 36 Hz on our custom demo machine. We demonstrate the quality\nof our results on our challenging high-fidelity datasets, and com-\npare our method and datasets to existing baselines. We release our\ndataset on our project website: https://vr-nerf.github.io.\narXiv:2311.02542v1  [cs.CV]  5 Nov 2023\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nXu et al.\nKEYWORDS\nMulti-View Capture, Neural Radiance Fields, Novel-View Synthesis,\nHigh Dynamic Range Imaging, Real-Time\nACM Reference Format:\nLinning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal,\nChangil Kim, Samuel Rota Bul\u00f2, Lorenzo Porzi, Peter Kontschieder, Alja\u017e\nBo\u017ei\u010d, Dahua Lin, Michael Zollh\u00f6fer, and Christian Richardt. 2023. VR-\nNeRF: High-Fidelity Virtualized Walkable Spaces. In SIGGRAPH Asia 2023\nConference Papers (SA Conference Papers \u201923), December 12\u201315, 2023, Sydney,\nNSW, Australia. ACM, New York, NY, USA, 23 pages. https://doi.org/10.1145/\n3610548.3618139\n1\nINTRODUCTION\nThe advent of consumer virtual reality (VR) headsets has led to a\nproliferation of highly immersive visual media, including breath-\ntaking VR photography and video. However, existing approaches\nsupport either high-fidelity view synthesis with a small headbox of\nless than 1 m diameter [Broxton et al. 2020; Overbeck et al. 2018], or\nscene-scale free-viewpoint view synthesis of lower quality or fram-\nerate [Jang et al. 2022; Parra Pozo et al. 2019; Wu et al. 2022a]. In this\nwork, we present a comprehensive system designed to overcome\nthese limitations all the way from capture to rendering for high-\nfidelity free-viewpoint exploration of walkable, real-world static\nspaces in VR. Our contributions address the following challenges:\n(1) dense, high-fidelity capture of large-scale walkable spaces,\n(2) high-fidelity neural radiance field reconstruction, and\n(3) real-time rendering of our neural radiance fields in VR.\nHigh-fidelity view synthesis depends on high-quality, densely cap-\ntured multi-view images. While NeRF objects use 100s of views\n[Mildenhall et al. 2020] and light field captures around 1,000 views\nper location [Broxton et al. 2020], walkable scenes will need a min-\nimum of several thousand input views to provide enough spatial\ncoverage. Existing captures of walkable spaces tend to be hand-\nheld and usually comprise 100s of photos [e.g. Philip et al. 2021]\nor 1,000s of video frames [e.g. Knapitsch et al. 2017]. In both cases,\nthe space of camera poses is undersampled: photo sequences lack\nsufficient density, and videos move along a 1D subspace that fails to\nsample the 6D pose space sufficiently uniformly. High-fidelity view\nsynthesis also needs to reproduce the high dynamic range of the\nreal world, which existing methods do not. To this end, we designed\na custom camera rig that enables capturing walkable spaces in un-\nprecedented quality and density: our datasets contain thousands\nof 50 megapixel high dynamic range (HDR) images. Several of our\ndatasets exceed 100 gigapixels \u2013 two orders of magnitude more\nthan existing datasets [Flynn et al. 2019; Philip et al. 2021; Xu et al.\n2021].\nNeural radiance fields (NeRFs) have led to an explosion in high-\nquality novel-view synthesis techniques [Mildenhall et al. 2020;\nTewari et al. 2022]. However, existing methods do not support the\nsize, scale, and dynamic range of our high-fidelity datasets, even\nwhen downsampled to 2K resolution. We propose VR-NeRF, which\nis uniquely adapted to our high-quality datasets and supports real-\ntime VR rendering in full NeRF quality. Specifically, we introduce a\nnew perceptually based color space for representing high-dynamic\nrange radiance values of up to 10,000 cd/m2, allowing our model to\nlearn up to 22 stops1 of dynamic range (or 4,194,304:1). A second cru-\ncial component is a real-time-capable mip-mapping technique that\nsuppresses aliasing when observing objects at different distances\nusing level-of-detail rendering. We also developed a principled\npruning stage to obtain an accurate occupancy grid for speeding\nup rendering with a focus on improved geometry estimation.\nThe third and final stage of our end-to-end system is a custom\nmulti-GPU renderer that brings high-fidelity NeRF rendering into\nvirtual reality. On our custom-built demo machine, we can render\nour models at the full resolution of the Quest Pro VR headset, i.e.,\ntwo 2K\u00d72K eye buffers (~8 megapixel), at a consistent frame rate\nof 36 Hz, which results in a compelling VR experience that enables\nfree exploration of walkable spaces in high fidelity.\n2\nRELATED WORK\nKanade et al. [1995] coined the term \u201cVirtualized Reality\u201d to see\na previously recorded event from any perspective. Our goal is to\nvirtually walk through previously captured scenes at high fidelity\nin virtual reality. We, therefore, call our work Virtualized Walkable\nSpaces. There are three crucial components to enable high-fidelity\nvirtualized walkable spaces: (1) a mobile high-resolution multi-view\ncamera system to densely capture large-scale scenes; (2) an efficient\nneural representation to compactly and accurately encode a large-\nscale scene with high dynamic range and level of detail; and (3)\noptimized real-time rendering at VR resolution and frame rate.\nHigh-Resolution Multi-View Capture System. Capture systems\ncan vary from a single moving camera [Bertel et al. 2020; Davis\net al. 2012; Gortler et al. 1996; Hedman et al. 2016; Kim et al. 2013;\nKnapitsch et al. 2017; Levoy and Hanrahan 1996] to multi-camera\nrigs [Broxton et al. 2020; Flynn et al. 2019; Parra Pozo et al. 2019;\nWilburn et al. 2005] and synchronized camera arrays in big studios\n[Joo et al. 2019; Orts-Escolano et al. 2016]. Existing multi-view cap-\ntures are either limited to a small headbox [e.g. Overbeck et al. 2018;\nParra Pozo et al. 2019] or are sparsely captured [e.g. Knapitsch et al.\n2017; Yoon et al. 2020], which restricts freedom of motion. We built\na multi-camera rig that densely and efficiently captures a wide vari-\nety of walkable spaces to create large-scale multi-view datasets with\nhigh-resolution details (50 megapixels) and high dynamic range.\nLarge-scale Novel View Synthesis. Our focus is on real-time VR\nrendering of high-fidelity walkable spaces; recent surveys cover\nthe full range of scene representations [Richardt et al. 2020; Tewari\net al. 2022]. While mesh-based reconstructions [Straub et al. 2019;\nWhelan et al. 2018] are ideal for fast rendering, they tend to lack\nfine geometric detail. Image-based rendering [e.g. Hedman et al.\n2016] achieves more visual detail but struggles with reflective sur-\nfaces. Several follow-up methods use neural representations for\nexplicit reflection support [Philip et al. 2021; Wu et al. 2022a; Xu\net al. 2021] and achieve interactive frame rates. NeRFs [Mildenhall\net al. 2020] have become the de-facto standard neural representa-\ntion due to their versatility and ability to represent complex scenes\nwith high fidelity. They have been extended in multiple ways to\nrepresent large-scale scenes even at a city scale [Tancik et al. 2022;\nTurki et al. 2022; Xiangli et al. 2022; Xu et al. 2023; Zhang et al.\n2023]. High-resolution concerns have also been addressed [Jiang\n1One stop is a doubling or halving of the amount of light reaching the imaging sensor.\nVR-NeRF: High-Fidelity Virtualized Walkable Spaces\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\net al. 2023; Wang et al. 2022]. However, these methods do not sup-\nport level of detail and high dynamic range, which are required\nfor high-fidelity VR. LocalRF [Meuleman et al. 2023] and F2-NeRF\n[Wang et al. 2023] tackle large unbounded scenes, yet only support\nlimited view extrapolation and thus cannot provide fully immer-\nsive free-view exploration. Methods built on implicit surfaces, like\nsigned distance functions, tend to focus on high-quality 3D surface\nreconstruction rather than view synthesis [Li et al. 2023; Rosu and\nBehnke 2023; Yu et al. 2022; Zhu et al. 2023]. We build our model\non Instant-NGP (iNGP) [M\u00fcller et al. 2022], as it supports real-time\nrendering without model baking, and provides easily extensible\nmodel capacity via its hash grid. However, it lacks support for high-\nfidelity rendering of large-scale walkable spaces, such as level of\ndetail and perceptually based HDR support.\nHigh Dynamic Range (HDR). The human visual system supports\na significantly higher dynamic range than current camera or display\ntechnology [Reinhard et al. 2006]. When recreating highly realistic\nwalkable spaces, it is therefore important to accurately capture and\nrender the scene in HDR. RawNeRF learns linear radiance from raw\nsensor measurements using a weighted L2 loss that approximates a\ntonemapped loss [Mildenhall et al. 2022]. Several methods learn to\nreconstruct linear radiance from low dynamic range images using\ndifferentiable tonemapping models [Huang et al. 2022; Jun-Seong\net al. 2022; R\u00fcckert et al. 2022]. We train our HDR model directly\nusing HDR input images in a novel perceptually uniform color\nspace that does not require custom losses or tonemapping modules.\nLevel of Detail (LOD). Takikawa et al. [2021] and Barron et al.\u2019s\nMip-NeRF [2021] introduced the notion of level of detail into neural\nsigned distance and radiance fields, respectively, to reduce geomet-\nric and visual complexity, e.g. to minimize aliasing when viewing\nobjects from a distance. As Mip-NeRF\u2019s integrated positional en-\ncoding is incompatible with efficient grid-based NeRF approaches\nlike iNGP [M\u00fcller et al. 2022], Zip-NeRF [Barron et al. 2023] uses\nsupersampling as an approximation, but multi-second inference\ntimes still prevent real-time rendering. Aroudj et al. [2022] store the\nscene redundantly at multiple LOD levels in a sparse voxel octree.\nWe introduce an efficient LOD approach designed for iNGP that\nenables high-fidelity real-time VR rendering with anti-aliasing.\n3\nTHE \u201cEYEFUL TOWER\u201d CAPTURE RIG\nCapturing scenes with a hand-held camera quickly reaches limits:\ntaking hundreds of photos is tedious, achieving consistent coverage\nof viewpoints is difficult, and hand-held exposure bracketing is\ntricky due to camera shake. To capture real-world environments\nwith the highest visual fidelity in terms of spatial resolution and dy-\nnamic range, we designed, built, and refined a custom multi-camera\ncapture rig affectionately referred to as the Eyeful Tower. The design\nof our capture rig was guided by the following considerations:\n(1) Coverage: Place cameras for approximately uniform light\nfield capture, and parallelize data capture across cameras.\n(2) Fidelity: Match human visual perception in terms of acuity\nand high dynamic range.\n(3) Mobility: Allow single-person operation, and be usable with-\nout external power or network connection.\n(4) Rigidity: Support multi-exposure bracketing for high dy-\nnamic range (HDR) reconstruction without camera motion.\n(5) Storage: Record photos on-camera, so no server is needed.\nOffload all photos via a single network cable.\n3.1\nCapture Rig Design\nWe built our capture rig using extruded\naluminium around an 80\u00d780 cm base\nwith a 1.8 m vertical pole for 22 cameras\nthat are distributed on 7 levels with 3\ncameras each, plus one upward-facing\ncamera at the top (see right). A 1.5 kWh\nLi-ion battery powers cameras, a 24-\nport network switch, and a Raspberry\nPi controlling the cameras. We chose\nSony \ud835\udefc1 mirrorless cameras for their\nhigh-quality 50-megapixel raw images\nwith 14 stops of dynamic range. Please\nsee our supplement for details on the\nrig design and camera/lens choices.\n3.2\nCapture Process\n3.2.1\nDesirable Capture Density. Reproducing the appearance of\na static scene from any viewpoint in theory requires observations\nfor the entire 5D plenoptic function [Adelson and Bergen 1991].\nThe widely used NeRF synthetic dataset [Mildenhall et al. 2020] has\nviewpoints densely distributed on a hemisphere, which allows the\nrenderings to generalize continuously across the whole hemisphere\nof viewing directions. For scene-scale rendering, we are lacking\nsuch a densely captured dataset, which results in the limited ca-\npability to extrapolate novel viewpoints. However, this is critically\nimportant for virtual reality, where we want to deliver walkable\nspaces with 6-degrees-of-freedom allowable head movement.\n3.2.2\nCapture Procedure. We capture scenes by \u2018tiling\u2019 the avail-\nable floor area with rig positions that are spaced roughly 30 cm\napart. For complete captures, we capture forward- and backward-\nfacing views, while trying to stay at least 30 cm away from walls or\nobjects. Near walls, it is often sufficient to only capture the direction\nfacing away from the wall, as defocused close-ups of a wall usually\nadd little value. Before each capture, we also place scale bars (for\nautomatic scale estimation) and a Macbeth ColorChecker (for color\nverification and white balance) into the scene. During the capture,\nwe try to stay out of view of any camera, avoid moving any objects,\nsuch as chairs or carpets, and aim to minimize lighting changes\nand shadow casting.\n3.3\nData Preprocessing\n3.3.1\nHDR Image Merging. We use LibRaw 0.21 to debayer the\nraw images captured by our cameras to 16-bit linear TIFF images.\nWe then merge 9 different exposures into one high-dynamic range\nimage using a robustified version of Hanji et al.\u2019s Poisson photon\nnoise estimator [2020], which provides an unbiased estimate of\nscene radiance. We observed that the Sony \ud835\udefc1 raw image values\ndo not saturate as quickly as expected, which produces outliers\nthat can reduce the estimated radiance sufficiently to cause visible\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nXu et al.\ncolor changes. Therefore, we keep track of the minimum radiance\nestimate per pixel and color channel, so that we can ignore it when\nmerging the input exposures. In addition, we set fully saturated\npixels to the lowest radiance that saturates in all images.\n3.3.2\nCamera Calibration. We estimate camera poses and intrinsics\nusing Agisoft Metashape Pro 2.0 [Agisoft, LLC 2023], a professional\nphotogrammetry software that supports rig calibration, in which\nthe relative pose between cameras is constant across all positions\nof the capture rig within a scene. Metashape effectively handles our\nlarge-scale datasets with up to 6,300 photos at 50 megapixel reso-\nlution [Over et al. 2021]. It also automatically detects the markers\non our calibrated scale bars, such that camera poses are in metric\nspace for 1:1 scale rendering in VR.\n3.3.3\nCaptured Datasets. We captured multiple datasets using our\nEyeful Tower capture rig, which are summarized in Table 1. Our\ncaptures took between 5 minutes and 6 hours, depending on the\nscale and complexity of the scene, with an average speed of around\none minute per m2. The resulting datasets comprise 29\u2013303 billion\npixels, or rays, covering spaces of 6\u2013120 m2.\nTable 1: Statistics of scenes captured using our Eyeful Tower\nrig: We show the number of cameras, rig positions, and im-\nages, as well as the capture time, surface area, and the number\nof rays at full resolution (5,784\u00d78,660) and 1368\u00d72048 (\u20182K\u2019),\nour typical training and rendering image resolution.\nScene\nCameras\n# Pos.\n# Img.\nTime\nArea\nRays\nRays @ 2K\napartment\n22\n180\n3,960\n60 min\n55 m2\n190.6 B\n10.7 B\nkitchen\n19\n318\n6,024\n43 min\n54 m2\n302.7 B\n16.9 B\noffice1a\n9\n85\n765\n23 min\n20 m2\n29.1 B\n1.6 B\noffice1b\n22\n71\n1,562\n16 min\n20 m2\n78.2 B\n4.4 B\noffice2\n9\n233\n2,097\n39 min\n35 m2\n79.8 B\n4.5 B\noffice_view1\n22\n126\n2,772\n31 min\n18 m2\n138.9 B\n7.8 B\noffice_view2\n22\n67\n1,474\n10 min\n33 m2\n73.8 B\n4.1 B\nriverview\n22\n48\n1,008\n5 min\n6 m2\n52.9 B\n3.0 B\nseating_area\n9\n168\n1,512\n22 min\n16 m2\n55.9 B\n3.1 B\ntable\n9\n134\n1,206\n14 min\n24 m2\n45.2 B\n2.5 B\nworkshop\n9\n700\n6,300\n364 min\u2020 120 m2\n239.4 B\n13.4 B\n\u2020 Includes 121 minutes of capture time and 243 minutes of data offload mid-capture.\n4\nHIGH-FIDELITY NEURAL RADIANCE\nFIELDS\nVolume rendering using neural radiance fields is a compelling\nchoice for photorealistic scene representations due to the versatil-\nity of representing semi-transparent surfaces and finely detailed\nobjects while being suitable for delivering scene-scale rendering.\nAs our focus is on maximizing rendering fidelity in the available\ncompute budget, we use neural radiance fields as a foundation and\nleave alternative representations as future work. Instead of con-\nstructing a large, complex model with extra capacity to account\nfor various effects, our goal is to design a simple yet general model\nthat facilitates real-time VR rendering for large-scale scenes.\nWe therefore build on the Instant NGP architecture [M\u00fcller et al.\n2022] with its efficient and scalable multi-level hash encoding for\nfast rendering of large-scale static scenes. We make several contri-\nbutions to improve the visual fidelity of high-resolution room-scale\nrendering, including a perceptual color space that enables percep-\ntual optimization of high dynamic range images using a simple \ud835\udc3f1\nloss. We further introduce an efficient and effective level-of-detail\nscheme for anti-aliasing using multi-level hash grids. To faithfully\nrepresent unbounded areas, such as views through windows or long\ncorridors, we adopt a cubic space contraction based on the \ud835\udc3f\u221e norm\n[Wan et al. 2023], which is a good fit for grid-based representations.\nWe discuss implementation details and additional components that\ncontribute to the high quality of our view synthesis model in our\nsupplement.\n4.1\nPerceptual Modeling of High Dynamic\nRange\nOur Sony \ud835\udefc1 cameras capture raw images with a dynamic range\nof 14 stops (i.e., 14 bits of usable information). The 9-step exposure\nbracketing adds a further 8 stops, for a total of 22 stops of dynamic\nrange (see Figure 2). In other words, the brightest input pixel value\ncan be up to 4,194,304 times as bright as the darkest non-zero pixel\nvalue. Applying common image losses like \ud835\udc3f1 or \ud835\udc3f2 directly in linear\ncolor spaces of this range leads to poor results as the losses are\ndominated by errors in bright areas. For example, an error of 0.1 is\nsignificantly more noticeable at a base level of 0.1 (+100%) compared\nto 10 (only +1%), yet would be penalized the same. The solution is to\neither use a more complex loss function, such as RawNeRF\u2019s relative\nMSE [Mildenhall et al. 2022], or a carefully designed non-linear\nmapping to a perceptually uniform color space.\nHDR\nRAW\nJPG\n0\n5\n10\n15\n20\nluminance (linear)\n104\n102\n100\n106\npixel count (log)\nFigure 2: Comparison of the dynamic range of a JPEG photo\n(range 0~1) with the corresponding raw image (0~2) and the\nfull HDR image (0~145).\nOne such non-linear mapping is the Perceptual Quantizer (PQ)\ndeveloped by Dolby [Miller et al. 2013] and standardized by SMPTE\n[2014], which is the foundation of many consumer HDR image and\nvideo formats. PQ was designed to optimally encode the large lumi-\nnance range from 0 to 10,000 cd/m2 in 10\u201316 bits while minimizing\nvisible banding artifacts. This was achieved by approximating the\nintegral of just noticeable differences based on the contrast sen-\nsitivity function of the human visual system [Kunkel 2022]. The\nfunction\nPQ(\ud835\udc4c) =\n\u0012\ud835\udc501 + \ud835\udc502 \u00b7 \ud835\udc4c\ud835\udc5a1\n1 + \ud835\udc503 \u00b7 \ud835\udc4c\ud835\udc5a1\n\u0013\ud835\udc5a2\nwith constants\n(1)\n\ud835\udc5a1 = 1305\n8192, \ud835\udc5a2 = 2523\n32 , \ud835\udc501 = 107\n128, \ud835\udc502 = 2413\n128 , \ud835\udc503 = 2392\n128\n(2)\nmaps the input luminance \ud835\udc4c \u2208 [0, 10,000] cd/m2 to the \u2018PQ space\u2019\nin the unit range. For our experiments, we map linear color values\nof 1 to a luminance of 100 cd/m2 in order to allow a conversion to\nVR-NeRF: High-Fidelity Virtualized Walkable Spaces\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nthe PQ space. Operating in the unit range is also a natural fit for\nthe sigmoid activation function, which eases the learning of model\noutput distributions. Applying an \ud835\udc3f1 or \ud835\udc3f2 loss in the PQ space now\npenalizes errors according to human visual perception, and is able\nto produce colors in full high dynamic range. See Figure 7 for a\nsweep of different exposures at rendering time.\n4.2\nFeature Grid Mip-Mapping for\nLevel-of-Detail\nLevel-of-detail (LOD) rendering is desirable for large-scale scenes,\nas objects observed at different distances reveal varying levels of\ngeometric and texture detail. Single-LOD methods like NeRF or\niNGP can cause severe aliasing in highly textured objects seen at a\ndistance, while details seen in only a few views might be washed out\ndue to many overlapping distant views. Multiple levels of detail can\nreduce aliasing as the LOD level can be dynamically adjusted based\non the distance of objects from the viewer. In computer graphics,\ntexture LOD is usually implemented using mip-maps [Williams\n1983]. Mip-NeRF [Barron et al. 2021] introduced mip-mapping to\nNeRFs and Zip-NeRF [Barron et al. 2023] recently extended these\nideas to fast grid-based feature encodings, as used by iNGP. Un-\nfortunately, this approach is unsuitable for real-time rendering\n(1.1 FPS on 8\u00d7V100). Instead, we introduce a simple but effective\nmip-mapping scheme for grid-based feature encodings that enables\nlearning of continuous LOD while actively supporting real-time\nrendering.\n4.2.1\nFeature Grid Mip-Mapping. Multi-resolution feature grids\nare a natural fit for LOD rendering as they already represent features\nacross multiple scales. By considering a ray as a cone as in Mip-\nNeRF, and by comparing its cross section with the size of grid\nfeatures at each level, we can efficiently determine which feature\ngrid levels are theoretically resolvable at the ray level, and can down-\nweight or even ignore finer levels that would introduce aliasing.\nFor a specific ray, we start by calculating its base radius \ud835\udc5f at unit\ndistance along the ray. At a sample location, the pixel footprint is\nthen determined by multiplying the base radius with the metric\ndistance \ud835\udc61 along the ray as \u02c6\ud835\udc5f = \ud835\udc61 \u00b7\ud835\udc5f. For contracted spaces, Barron\net al. [2022, 2023] and Wang et al. [2023] consider the Jacobian JC of\nthe contraction function C(\u00b7) at the sample location x to calculate\nthe scale factor for variance or step size estimation. Similarly, we\ncould derive the contracted pixel radius via C(\u02c6\ud835\udc5f) = \u02c6\ud835\udc5f \u00b7 3\u221a\ufe01\ndet(JC(x)).\nIn practice, we compute the contracted pixel radius directly from\ncorresponding sample points on adjacent rays in the contracted\nspace. The optimal LOD level can then be calculated from the con-\nfiguration of the multi-resolution feature grid as follows. Suppose\nthe base resolution is \ud835\udc60 and the scale factor between levels is \ud835\udc53 . For\nthe \ud835\udc3fth level (with \ud835\udc3f = 0 being the base), each level has a grid voxel\nsize of (\ud835\udc60\ud835\udc53 \ud835\udc3f)\u22121. Based on the Nyquist\u2013Shannon sampling theorem,\nwe dampen features whose size is less than twice the footprint \u02c6\ud835\udc5f\nin the contracted coordinate space (see diagram in Figure 3). The\noptimal LOD level for a sample is therefore \ud835\udc3f\u2217 = \u2212 log\ud835\udc53 (2\ud835\udc60\u02c6\ud835\udc5f). For a\nL=k+1\nL=k\nMulti-level Hash Grids\nMasked features\n\u2026\nk\nk+1\n0\n1\n15\n14\n\u2026\n\u2026\n\u2026\nr1\nr2\nFigure 3: LOD Masking. Smaller sample footprints like \ud835\udc5f1\nreturn more features from the multi-level hash grid than\nlarger, more distant samples (\ud835\udc5f2).\npiecewise linear LOD transition, we use these per-level weights:\n\ud835\udc64\ud835\udc3f =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n1\n\ud835\udc3f \u2264 \u230a\ud835\udc3f\u2217\u230b\n\ud835\udc3f\u2217 \u2212 \u230a\ud835\udc3f\u2217\u230b\n\u230a\ud835\udc3f\u2217\u230b < \ud835\udc3f \u2264 \u2308\ud835\udc3f\u2217\u2309\n0\n\u2308\ud835\udc3f\u2217\u2309 < \ud835\udc3f\n(3)\nFor distant points, we only need to sample the features of the lowest\nfew grid levels, which reduces rendering time, while gradually re-\nvealing high-frequency features for closer points. Feature sampling\nof the finest hash grid layers is particularly expensive due to the\nhighly incoherent memory access patterns. Skipping these features\nresults in substantially faster rendering (Section 5).\n4.2.2\nLOD Bias. Similar to standard mip-mapping, we can option-\nally add an LOD bias \u0394\ud835\udc3f to the LOD \ud835\udc3f\u2217 used for querying and\nweighting the grid features. This continuously adjusts the sharp-\nness of details to balance between blurred and aliased rendering.\nIn fact, the LOD bias can be viewed as a unifying framework that\nencompasses coarse-to-fine training strategies [Lin et al. 2021; Park\net al. 2021; Yang et al. 2023]. Such progressive training approaches\nstart with low-frequency models and gradually increase the number\nof feature scales to improve details. This is equivalent to starting\nwith a large negative LOD bias, such that only low-frequency fea-\ntures are used, and annealing it towards zero during training. See\nFigure 8 for the visualized LOD bias sweep on the apartment\nscene.\n4.2.3\nDistance-aware Features. By mip-mapping grid features, we\nare effectively making the features used for radiance computation\ndistance-aware, as different features are used at different viewing\ndistances. This offers an additional degree of freedom to handle\ninconsistent data during the capture process, such as the distance-\ndependent shadows cast by the camera rig. Rig shadows are most\nprominent when the rig approaches walls or corners. With limited\ntraining views in these ambiguously captured locations, the model\nis likely to fake the shadows with incorrect geometry and/or appear-\nance. On the other hand, distance-aware features allow our model\nto learn distance-dependent appearance, which reduces visual arti-\nfacts. We further noticed that the mip-mapped features encourage\nthe model to better allocate model capacity for fine-grained details.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nXu et al.\n4.3\nOptimizing the Quality\u2013Speed Trade-off\nOur goal of high-fidelity real-time NeRF rendering requires some\nchallenging trade-offs between visual quality and rendering speed.\nFor example, while conditional latent codes and wider and deeper\nnetworks can improve rendering quality [Barron et al. 2023; M\u00fcller\net al. 2022], they come at a significant run-time cost. Similarly, using\na proposal network for sampling adds overhead at render time as\nmultiple networks need to be evaluated sequentially. To maximize\nrendering speed without model baking, we implement an explicit\nbinary occupancy grid for efficiently skipping free space and mini-\nmizing the number of sample points for which hash grid features\nneed to be queried and MLPs evaluated. An example grid is shown\nin Figure 9, along with the corresponding image results. While\noccupancy grids are widely-adopted acceleration structures [Chen\net al. 2022; Liu et al. 2020; M\u00fcller et al. 2022; Sun et al. 2022], we\npropose two novel extensions that help us prune more accurately.\n4.3.1\nCylinder Pruning. We initialize\nour binary occupancy grid based on the\nknown rig capture positions to carve\nout as much free space in the scene as\npossible. For this, we first approximate\nthe geometry of our Eyeful Tower cap-\nture rig as a cylinder. We then mark\nall occupancy grid voxels that are com-\npletely inside any such cylinder as free\nspace. This type of pruning has two key benefits: (1) it prevents\nthe model from cheating using floaters in front of cameras, which\nleads to more view-consistent models, and (2) it speeds up the early\nstages of training and thus helps improve convergence speed.\n4.3.2\nJoint History- and Grid-based Pruning. We explore a more\nconservative pruning strategy that combines pruning based on\ntraining history with dense grid sampling. History pruning keeps\ntrack of the maximum density observed for each voxel in the occu-\npancy grid during the training process. This only considers rays\nseen during training, so some parts of the scene may not be ob-\nserved. Grid-based pruning makes up for this by evaluating a dense\ncubic grid inside each voxel of the occupancy grid to estimate the\nmaximum density for each voxel. As the density depends on the\nstep size used in training, we use a worst-case estimate for this,\ni.e., the minimum step size possible inside each voxel based on the\nray from the closest camera. For our datasets, we start the pruning\nprocess after 100K iterations, when a relatively clean scene geom-\netry is obtained. Every thousand iterations, we prune grid voxels\nfor which both maximum densities fall below the current pruning\nthreshold (which we anneal linearly from zero to \ud835\udefc =0.2). We start\nwith a coarse occupancy grid of 1283 resolution, and upsample the\noccupancy grid at predefined iteration milestones to prune scenes\nmore accurately over time.\n5\nVR NERF RENDERING\nRendering a room-scale NeRF model in VR requires high resolution,\nhigh frame rates and low latency. Our target is native rendering\non a Meta Quest Pro VR headset, ideally dual 2K\u00d72K eyebuffers at\n72 FPS. We approach this task with a combination of hardware, soft-\nware, and model optimizations. Specifically, we present a custom\n(a) Color image\n(b) Static split: 49.3 FPS\n(c) Dynamic split: 73.6 FPS\nFigure 4: (a) In this example, we render a novel view using 3\nGPUs. (b) A static split distributes work equally (indicated\nby colors; brightness is proportional to #MLP evaluations).\n(c) Our dynamic work split achieves 49% higher FPS.\nmulti-GPU CUDA renderer with efficient in-register MLP evalu-\nation and automatic work distribution, a compute-efficient LOD\ntechnique (see Section 4.2), and a 20-GPU workstation for peak VR\nperformance.\nMLP evaluation is the most computationally expensive portion of\nmodel inference, and thus a prime candidate for optimization. Our\nMLP implementation is specialized for small iNGP-style networks\nby taking advantage of Nvidia\u2019s Tensor Cores and evaluating all\nlayers within registers. Inputs and outputs of the MLP are stored in\nshared memory while per-layer activations are stored in register-\nbacked arrays, with outputs from one layer being shuffled in an\narchitecture-dependent way to become the inputs to the next layer.\nThis limits memory traffic to just the input and output features,\nwhich are typically small (32 inputs, 16 bottleneck features, 3 output\ncolors) compared to the hidden layers (64 nodes), and the network\nweights, which are shared across the kernel and typically cached.\nThis structure also allows the MLP evaluation to be interleaved with\nray marching and hash grid sampling in a single kernel. This enables\nthe neural features to be passed to the networks without staging\nthrough global memory (which can suffer from capacity problems\nwith a large number of samples per ray) or across multiple kernels\n(which would incur extra launch and synchronization overhead).\nWe further adopt a dynamic work distribution strategy for im-\nproving the utilization of multiple GPUs compared to a static work\nsplit that would often be suboptimal as some rays take longer to\ncompute than others due to differences in pruning in different parts\nof the scene, as well as GPU caching and overhead effects. For every\nframe, we measure the throughput per GPU in rays per second, and\nassign contiguous rows to each GPU based on its ratio of the total\nthroughput. We use dampening for smoother convergence to an\noptimal distribution. Figure 4 demonstrates a 49% increase in FPS.\nEach GPU stores a separate copy of each scene (~700 MB VRAM).\nWe also built a custom 20-GPU rendering workstation to evaluate\nour walkable spaces at the highest possible fidelity in virtual reality.\nThis machine comprises a Dell R7515 server with an AMD Epyc\n7313P CPU and 256 GB of RAM, and is connected to 20 Nvidia A40\nGPUs via a PCIe switching solution from Liqid Inc., all in a 24U\nserver rack. We detail our design considerations in the supplement.\n6\nRESULTS AND EVALUATION\nFor our room-scale scenes, we use hash grid configurations with\n\ud835\udc3f=16 levels of two features, with a base resolution of 128 and scaling\nVR-NeRF: High-Fidelity Virtualized Walkable Spaces\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nTable 2: Quantitative comparison results on the Eyeful Tower\ntest set. All results are trained on 1K resolution images for\n110K iterations with 1024 samples per ray. We report the\naverage PSNR/SSIM/LPIPS both in sRGB and PQ color spaces.\nThe best results are highlighted. See the supplemental docu-\nment for the breakdown by individual dataset.\nMethods\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPQ-PSNR\u2191 PQ-SSIM\u2191 PQ-LPIPS\u2193\niNGP (our implementation)\n31.93\n0.918\n0.183\n37.39\n0.957\n0.133\nwith PQ color space\n32.47\n0.926\n0.170\n38.15\n0.962\n0.122\nwith PQ color space and LOD\n33.30\n0.930\n0.146\n38.95\n0.964\n0.108\nfactor of 1.4. Following iNGP, we use a 1-hidden-layer density MLP\nand a 2-hidden-layer color MLP, both 64 neurons wide. For each ray,\nwe sample 1024 points using exponential distances for integration.\nWe use the Adam optimizer [Kingma and Ba 2015] with \ud835\udefd1 = 0.9,\n\ud835\udefd2 = 0.99, \ud835\udf16 = 10\u221215, and a batch size 12,800 rays (256 random rays\nfrom 50 random images) for all our experiments. We use far-field\ncontraction for the subset of unbounded scenes. We use learning\nrate 0.01 for the hash grids and 0.005 for the remaining modules.\nWe discuss a series of additional techniques for per-scene quality\nimprovements in the supplement.\nFor fair evaluation, we hold out a fixed camera from the training\nset, which has the same number of frames as all other cameras. For\nablation experiments, we show results trained for 110K iterations on\n1K resolution Eyeful Tower datasets. The demo videos are produced\nby models trained on 2K resolution images and longer than 200K\niterations. In the supplement, we include additional results on the\nInria [Philip et al. 2021] and mip-NeRF360 datasets [Barron et al.\n2022], as well as ablations on pruning strategies.\n6.1\nComparative Evaluation\nTo model HDR images, iNGP [M\u00fcller et al. 2022] suggests using\nan exponential color activation for linear RGB space. RawNeRF\n[Mildenhall et al. 2022] further suggests using a weighted loss\nto prevent extremely bright areas from dominating. Table 2 and\nFigure 6 show the comparisons of our designed modules with iNGP\nbaselines: (1) the effectiveness of using PQ color space for HDR\nmodeling, and (2) the use of the LOD feature grid.\nWe choose the baseline of using iNGP with linear color space\nwith truncated exponential activation for the color network to\navoid the issue of exploding values weighted by the predicted color\nvalue, as practiced by Mildenhall et al. [2022]. \u201ciNGP with PQ color\nspace\u201d reflects our modification of directly training in the PQ color\nspace, and replaces the original exponential color activation with a\nsigmoid function. \u201ciNGP with PQ color space and LOD\u201d represents\nour core model of adopting mip-mapped grid features based on\nthe estimated LOD level for each queried sample point on the ray.\nWe report the standard PSNR/SSIM/LPIPS metrics in tonemapped\nsRGB space, and additionally report versions of these metrics in PQ\ncolor space for better evaluation on extremely bright and dark areas.\nMore results and analysis with supporting plots and visualizations\non each ablated module can be found in our supplement.\nPQ color space. Table 2 shows that the PQ color space consis-\ntently outperforms the linear color space for all test scenes and all\nmetrics. We noticed that during training, color predictions using\nthe exponential activation baseline continue to grow to excessively\nlarge values. This poses an ambiguity for predicting correct density\nvalues and their derived weights, which are multiplied with the\npoint color to obtain sample colors. Directly modeling colors in\nlinear RGB space poses additional challenges in regressing and in-\nterpolating accurate color values, especially when a large range of\nradiance is present. As shown in the second example in Figure 6, the\nbase model fails to model the color on the checkerboard correctly.\nMip-mapped features. The combination of LOD and PQ color\nspace further improves the rendering quality and leads to cleaner\ngeometries, as seen in Table 2 and Figure 6. This is critical for prun-\ning and VR rendering, where a good geometry is desired. Figure 6\nshows four scenarios where the mip-mapped features can help.\nThe first scene shows an annoying dark appearance baked into the\nrendered scene due to dynamic shadows from the rig in the training\ndata. These are modeled by the high-frequency levels and well ad-\ndressed by the distance-aware features. The second example shows\nboth cleaner geometry and appearance in the ambiguous space\nnear the whiteboard. The third example shows how LOD helps\neliminate aliasing for distant areas, especially when observing the\nscene from a wide angle. The last example shows how LOD can also\nreveal more detail compared to non-mip-mapped features, not just\na cleaner appearance. This is expected as the features correspond-\ning to high-resolution hash grids are specifically allocated to areas\nrich in fine details in the training views, which allows the model\nto automatically allocate more capacity for these parts. Note that\nwhile the quantitative metrics are similar to the baselines, the visual\nimprovements are easier to spot and critical to the high-fidelity\nrendering results that contribute to a pleasant VR experience.\n6.2\nPerformance Evaluation\nFigure 5 plots the rendering frame rate when using a varying num-\nber of A40 GPUs. Native VR rendering for a Meta Quest Pro headset\nrequires rendering two 2064\u00d72096 eyebuffers at 72 FPS. However,\nAsynchronous Spacewarp (ASW) [Beeler et al. 2016] can help close\nthis gap by reprojecting frames when they are rendered at least at\nhalf the native FPS, i.e., 36 (dashed line) instead of 72 (solid line). But\neven ASW fails if rendering is slower than that critical threshold.\nThus, we found the \u2018p99\u2019 metric (the 99th percentile of FPS) to be a\nbetter proxy for the quality of VR experience than mean FPS \u2014 as\nlong as the application is typically (i.e. 99%+ of the time) above 36\nFPS, ASW can deliver a smooth experience. Any slower, and the\nuser may notice stuttering frames, and experience motion sickness.\nAn off-the-shelf 3-GPU workstation is sufficient for reliable half-\nresolution VR rendering of half the scenes at 36 FPS (see Figure 5,\ntop right), which demonstrates the practicality of our method. For\nmaximum rendering speed and fidelity, we use our custom 20-\nGPU rendering workstation. All scenes but one achieve a p99 of 72\nFPS at half-resolution, and thus provide a smooth VR experience\neven without ASW. At full resolution (top row in Figure 5), 4 of\nthe 6 scenes shown in Figure 5 (bottom right) exceed the critical\nASW threshold of 36 FPS for a smooth, high-fidelity VR experience.\nInterestingly, halving the resolution only approximately doubles\nthe FPS, even though only 1/4 as many pixels are being rendered.\nThis may indicate a substantial amount of per-frame overhead (e.g.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nXu et al.\n1\n4\n8\n12\n16\n20\n0\n36\n72\n108\n144\nFrames per second\noffice_view1\noffice2\napartment\nseating_area\nriverview\nworkshop\n1\n4\n8\n12\n16\n20\n0\n36\n72\n108\n144\n1\n4\n8\n12\n16\n20\n0\n18\n36\n54\n72\nNumber of GPUs\nFrames per second\n1\n4\n8\n12\n16\n20\n0\n18\n36\n54\n72\nNumber of GPUs\nmean\u00b1std at 2\u00d71K\u00d71K\np99 at 2\u00d71K\u00d71K\nmean\u00b1std at 2\u00d72K\u00d72K\np99 at 2\u00d72K\u00d72K\nFigure 5: Runtime performance at half (top) and full (bottom)\nVR resolution (for a Meta Quest Pro) over a prerecorded cam-\nera trajectory. Left: Mean and standard deviation of frame\nrates. Right: The 99th percentile frame time (expressed as\nFPS) is indicative of the worst-case frame rate.\ndue to kernel launches, the VR compositor, or OpenGL display\npipeline) or insufficient parallelism available at lower resolutions.\n7\nDISCUSSION\nAggressive pruning. Like most pruning approaches, we threshold\ndensity for determining if a voxel is occupied or not. For bounded\nscenes with mostly solid surfaces, more aggressive pruning can be\napplied with a larger threshold (e.g.,\ud835\udefc =0.3) and finer grid resolution\n(e.g., 10243), which results in significantly faster rendering (see\n\u2018office_view1\u2019 in Figure 5). However, aggressive pruning does not\nwork well for complex real-world scenes, such as reflective surfaces,\ntransparent objects or unbounded scenes. This becomes particularly\napparent in VR, where over-pruned areas show box-like artifacts\nthat may not be easily seen in rendered 2D images or videos.\nDistance-aware features. Our level-of-detail feature weighting\nprovides our model the flexibility to reproduce distance-dependent\nappearance such as varying level of detail, or rig shadows. At the\nsame time, we observed that this reduces our model\u2019s ability to ex-\ntrapolate to unseen viewpoints or viewing distances, as feature vec-\ntors with unseen weighting may be used at render time. In particular,\ndensity can vary depending on distance, which is undesirable. We\nwork around this by pruning as much free-space as possible, so that\ndensity cannot suddenly appear when moving through free-space.\n8\nCONCLUSION\nWe presented VR-NeRF, the first holistic approach for capture, re-\nconstruction and rendering of high-fidelity walkable spaces in vir-\ntual reality. We made several key contributions across all stages of\nthe pipeline to achieve the significantly higher resolution, frame\nrate and visual fidelity required for comfortable VR viewing of\nneural radiance fields. We built a one-of-a-kind multi-camera rig\nthat captures thousands of uniformly distributed HDR photos of\na scene, integrated a novel perceptual color space for HDR model\noptimization, devised an efficient feature mip-mapping scheme for\nlevel-of-detail rendering, and implemented a multi-GPU renderer\nthat achieves comfortable VR viewing on our demo machine.\nACKNOWLEDGMENTS\nWe would like to thank Ada Lopaczynski, Autumn Trimble, Gadsden\nMerrill, Julia Buffalini, Kevyn McPhail, and Shukri Abdul Jalil for\ntheir exceptional technical support, and Alexandre Chapiro, Nathan\nMatsuda, Rafa\u0142 Mantiuk and Yaser Sheikh for helpful discussions.\nREFERENCES\nEdward H. Adelson and James R. Bergen. 1991. The Plenoptic Function and the\nElements of Early Vision. In Computational Models of Visual Processing. 3\u201320.\nAgisoft, LLC. 2023. Metashape 2.0.\nSamir Aroudj, Steven Lovegrove, Eddy Ilg, Tanner Schmidt, Michael Goesele, and\nRichard Newcombe. 2022. ERF: Explicit Radiance Field Reconstruction From Scratch.\n(2022). arXiv:2203.00051.\nJonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-\nBrualla, and Pratul P. Srinivasan. 2021. Mip-NeRF: A Multiscale Representation for\nAnti-Aliasing Neural Radiance Fields. In ICCV. doi: 10.1109/ICCV48922.2021.00580\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman.\n2022. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. In CVPR.\ndoi: 10.1109/CVPR52688.2022.00539\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman.\n2023. Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields. In ICCV.\nDean Beeler, Ed Hutchins, and Paul Pedriana. 2016. Asynchronous Spacewarp. https:\n//developer.oculus.com/blog/asynchronous-spacewarp/. Oculus Developer Blog.\nTobias Bertel, Mingze Yuan, Reuben Lindroos, and Christian Richardt. 2020. OmniPho-\ntos: Casual 360\u00b0 VR Photography. ACM Trans. Graph. 39, 6 (2020), 267:1\u201312. doi:\n10.1145/3414685.3417770\nMichael Broxton, John Flynn, Ryan Overbeck, Daniel Erickson, Peter Hedman, Matthew\nDuVall, Jason Dourgarian, Jay Busch, Matt Whalen, and Paul Debevec. 2020. Im-\nmersive Light Field Video with a Layered Mesh Representation. ACM Trans. Graph.\n39, 4 (2020), 86:1\u201315. doi: 10.1145/3386569.3392485\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. 2022. TensoRF:\nTensorial Radiance Fields. In ECCV. doi: 10.1007/978-3-031-19824-3_20\nAbe Davis, Marc Levoy, and Fr\u00e9do Durand. 2012. Unstructured Light Fields. Comput.\nGraph. Forum 31, 2 (2012), 305\u2013314. doi: 10.1111/j.1467-8659.2012.03009.x\nJohn Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan\nOverbeck, Noah Snavely, and Richard Tucker. 2019. DeepView: View Synthesis With\nLearned Gradient Descent. In CVPR. 2367\u20132376. doi: 10.1109/CVPR.2019.00247\nSteven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. 1996. The\nlumigraph. In SIGGRAPH. 43\u201354. doi: 10.1145/237170.237200\nParam Hanji, Fangcheng Zhong, and Rafa\u0142 K. Mantiuk. 2020. Noise-Aware Merg-\ning of High Dynamic Range Image Stacks without Camera Calibration. In ECCV\nWorkshops. doi: 10.1007/978-3-030-67070-2_23\nPeter Hedman, Tobias Ritschel, George Drettakis, and Gabriel Brostow. 2016. Scalable\nInside-Out Image-Based Rendering. ACM Trans. Graph. 35, 6 (2016), 231:1\u201311. doi:\n10.1145/2980179.2982420\nXin Huang, Qi Zhang, Ying Feng, Hongdong Li, Xuan Wang, and Qing Wang. 2022.\nHDR-NeRF: High Dynamic Range Neural Radiance Fields. In CVPR. doi: 10.1109/\nCVPR52688.2022.01785\nHyeonjoong Jang, Andr\u00e9as Meuleman, Dahyun Kang, Donggun Kim, Christian\nRichardt, and Min H. Kim. 2022. Egocentric Scene Reconstruction from an Omnidi-\nrectional Video. ACM Trans. Graph. 41, 4 (2022), 100:1\u201312. doi: 10.1145/3528223.\n3530074\nYifan Jiang, Peter Hedman, Ben Mildenhall, Dejia Xu, Jonathan T. Barron, Zhangyang\nWang, and Tianfan Xue. 2023. AligNeRF: High-Fidelity Neural Radiance Fields via\nAlignment-Aware Training. In CVPR. doi: 10.1109/CVPR52729.2023.00013\nHanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean Banerjee,\nTimothy Godisart, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara,\nand Yaser Sheikh. 2019. Panoptic Studio: A Massively Multiview System for Social\nInteraction Capture. TPAMI 41, 1 (2019), 190\u2013204. doi: 10.1109/TPAMI.2017.2782743\nKim Jun-Seong, Kim Yu-Ji, Moon Ye-Bin, and Tae-Hyun Oh. 2022. HDR-Plenoxels:\nSelf-Calibrating High Dynamic Range Radiance Fields. In ECCV. doi: 10.1007/978-\n3-031-19824-3_23\nVR-NeRF: High-Fidelity Virtualized Walkable Spaces\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nTakeo Kanade, P. J. Narayanan, and Peter W. Rander. 1995. Virtualized reality: concepts\nand early results. In ICCV Workshops. 69\u201376. doi: 10.1109/WVRS.1995.476854\nSing Bing Kang and Richard Weiss. 2000. Can We Calibrate a Camera Using an Image of\na Flat, Textureless Lambertian Surface?. In ECCV. doi: 10.1007/3-540-45053-X_41\nChangil Kim, Henning Zimmer, Yael Pritch, Alexander Sorkine-Hornung, and Markus\nGross. 2013. Scene reconstruction from high spatio-angular resolution light fields.\nACM Trans. Graph. 32, 4 (2013), 73:1\u201312. doi: 10.1145/2461912.2461926\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization.\nIn ICLR.\nArno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Tanks and\nTemples: Benchmarking Large-scale Scene Reconstruction. ACM Trans. Graph. 36,\n4 (2017), 78:1\u201313. doi: 10.1145/3072959.3073599\nTimo Kunkel. 2022. The Perceptual Quantizer: Design Considerations and Applications.\n(2022). Talk at ICC HDR Experts Day.\nMarc Levoy and Pat Hanrahan. 1996. Light field rendering. In SIGGRAPH. 31\u201342. doi:\n10.1145/237170.237199\nZhaoshuo Li, Thomas M\u00fcller, Alex Evans, Russell H. Taylor, Mathias Unberath, Ming-\nYu Liu, and Chen-Hsuan Lin. 2023. Neuralangelo: High-Fidelity Neural Surface\nReconstruction. In CVPR. 8456\u20138465. doi: 10.1109/CVPR52729.2023.00817\nChen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. 2021. BARF:\nBundle-Adjusting Neural Radiance Fields. In ICCV. doi: 10.1109/ICCV48922.2021.\n00569\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020.\nNeural Sparse Voxel Fields. In NeurIPS.\nSiwei Lyu. 2010. Estimating Vignetting Function from a Single Image for Image\nAuthentication. In ACM Workshop on Multimedia and Security. 3\u201312. doi: 10.1145/\n1854229.1854233\nRicardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey\nDosovitskiy, and Daniel Duckworth. 2021. NeRF in the Wild: Neural Radiance Fields\nfor Unconstrained Photo Collections. In CVPR. doi: 10.1109/CVPR46437.2021.00713\nAndreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang, Changil Kim, Min H. Kim,\nand Johannes Kopf. 2023. Progressively Optimized Local Radiance Fields for Robust\nView Synthesis. In CVPR. 16539\u201316548. doi: 10.1109/CVPR52729.2023.01587\nBen Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul Srinivasan, and\nJonathan T. Barron. 2022. NeRF in the Dark: High Dynamic Range View Syn-\nthesis from Noisy Raw Images. In CVPR. doi: 10.1109/CVPR52688.2022.01571\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance\nFields for View Synthesis. In ECCV. doi: 10.1007/978-3-030-58452-8_24\nScott Miller, Mahdi Nezamabadi, and Scott Daly. 2013. Perceptual Signal Coding for\nMore Efficient Usage of Bit Codes. SMPTE Motion Imaging Journal 122, 4 (2013),\n52\u201359. doi: 10.5594/j18290\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant\nNeural Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans.\nGraph. 41, 4 (2022), 102:1\u201315. doi: 10.1145/3528223.3530127\nSergio Orts-Escolano, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kow-\ndle, Yury Degtyarev, David Kim, Philip L. Davidson, Sameh Khamis, Mingsong\nDou, Vladimir Tankovich, Charles Loop, Qin Cai, Philip A. Chou, Sarah Mennicken,\nJulien Valentin, Vivek Pradeep, Shenlong Wang, Sing Bing Kang, Pushmeet Kohli,\nYuliya Lutchyn, Cem Keskin, and Shahram Izadi. 2016. Holoportation: Virtual 3D\nTeleportation in Real-time. In UIST. 741\u2013754. doi: 10.1145/2984511.2984517\nJin-Si R. Over, Andrew C. Ritchie, Christine J. Kranenburg, Jenna A. Brown, Daniel D.\nBuscombe, Tom Noble, Christopher R. Sherwood, Jonathan A. Warrick, and\nPhillipe A. Wernette. 2021. Processing coastal imagery with Agisoft Metashape\nProfessional Edition, version 1.6\u2014Structure from motion workflow documentation.\nOpen-File Report 2021-1039. U.S. Geological Survey. doi: 10.3133/ofr20211039\nRyan Styles Overbeck, Daniel Erickson, Daniel Evangelakos, Matt Pharr, and Paul\nDebevec. 2018. A System for Acquiring, Compressing, and Rendering Panoramic\nLight Field Stills for Virtual Reality. ACM Trans. Graph. 37, 6 (2018), 197:1\u201315. doi:\n10.1145/3272127.3275031\nKeunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman,\nSteven M. Seitz, and Ricardo-Martin Brualla. 2021. Nerfies: Deformable Neural\nRadiance Fields. In ICCV. doi: 10.1109/ICCV48922.2021.00581\nAlbert Parra Pozo, Michael Toksvig, Terry Filiba Schrager, Joyse Hsu, Uday Mathur,\nAlexander Sorkine-Hornung, Rick Szeliski, and Brian Cabral. 2019. An Integrated\n6DoF Video Camera and System Design. ACM Trans. Graph. 38, 6 (2019), 216:1\u201316.\ndoi: 10.1145/3355089.3356555\nJulien Philip, S\u00e9bastien Morgenthaler, Micha\u00ebl Gharbi, and George Drettakis. 2021.\nFree-viewpoint Indoor Neural Relighting from Multi-view Stereo. ACM Trans.\nGraph. 40, 5 (2021), 194:1\u201318. doi: 10.1145/3469842\nNasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Ham-\nprecht, Yoshua Bengio, and Aaron Courville. 2019. On the Spectral Bias of Neural\nNetworks. In ICML. 5301\u20135310.\nErik Reinhard, Greg Ward, Sumanta Pattanaik, and Paul Debevec. 2006. High Dynamic\nRange Imaging \u2013 Acquisition, Display and Image-Based Lighting. Morgan Kaufmann.\nChristian Richardt, James Tompkin, and Gordon Wetzstein. 2020. Capture, Recon-\nstruction, and Representation of the Visual Real World for Virtual Reality. In Real\nVR \u2013 Immersive Digital Reality: How to Import the Real World into Head-Mounted\nImmersive Displays. 3\u201332. doi: 10.1007/978-3-030-41816-8_1\nRadu Alexandru Rosu and Sven Behnke. 2023. PermutoSDF: Fast Multi-View Re-\nconstruction with Implicit Surfaces using Permutohedral Lattices. In CVPR. doi:\n10.1109/CVPR52729.2023.00818\nDarius R\u00fcckert, Linus Franke, and Marc Stamminger. 2022. ADOP: Approximate\nDifferentiable One-Pixel Point Rendering. ACM Trans. Graph. 41, 4 (2022), 99:1\u201314.\ndoi: 10.1145/3528223.3530122\nSMPTE. 2014. High Dynamic Range Electro-Optical Transfer Function of Mastering\nReference Displays. SMPTE Standard ST 2084:2014. Society of Motion Picture and\nTelevision Engineers. doi: 10.5594/SMPTE.ST2084.2014\nJulian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green,\nJakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei\nYan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon,\nNigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Mano-\nlis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven\nLovegrove, and Richard Newcombe. 2019. The Replica Dataset: A Digital Replica\nof Indoor Spaces. (2019). arXiv:1906.05797.\nCheng Sun, Min Sun, and Hwann-Tzong Chen. 2022. Direct Voxel Grid Optimization:\nSuper-fast Convergence for Radiance Fields Reconstruction. In CVPR. doi: 10.1109/\nCVPR52688.2022.00538\nTowaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek\nNowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. 2021. Neural\nGeometric Level of Detail: Real-Time Rendering With Implicit 3D Shapes. In CVPR.\n11358\u201311367. doi: 10.1109/CVPR46437.2021.01120\nMatthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall,\nPratul P. Srinivasan, Jonathan T. Barron, and Henrik Kretzschmar. 2022. Block-NeRF:\nScalable Large Scene Neural View Synthesis. In CVPR. doi: 10.1109/CVPR52688.\n2022.00807\nMatthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang,\nAlexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David Mcallister,\nJustin Kerr, and Angjoo Kanazawa. 2023. Nerfstudio: A Modular Framework for\nNeural Radiance Field Development. In SIGGRAPH Conference Proceedings. 72:1\u201312.\ndoi: 10.1145/3588432.3591516\nAyush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Yifan\nWang, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lom-\nbardi, Tomas Simon, Christian Theobalt, Matthias Niessner, Jonathan T. Barron, Gor-\ndon Wetzstein, Michael Zollh\u00f6fer, and Vladislav Golyanik. 2022. Advances in Neural\nRendering. Comput. Graph. Forum 41, 2 (2022), 703\u2013735. doi: 10.1111/cgf.14507\nHaithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. 2022. Mega-NeRF:\nScalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs. In CVPR.\n12922\u201312931. doi: 10.1109/CVPR52688.2022.01258\nZiyu Wan, Christian Richardt, Alja\u017e Bo\u017ei\u010d, Chao Li, Vijay Rengarajan, Seonghyeon\nNam, Xiaoyu Xiang, Tuotuo Li, Bo Zhu, Rakesh Ranjan, and Jing Liao. 2023. Learn-\ning Neural Duplex Radiance Fields for Real-Time View Synthesis. In CVPR. doi:\n10.1109/CVPR52729.2023.00803\nPeng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu, Taku Komura, Christian\nTheobalt, and Wenping Wang. 2023. F2-NeRF: Fast Neural Radiance Field Training\nwith Free Camera Trajectories. In CVPR. doi: 10.1109/CVPR52729.2023.00404\nZhongshu Wang, Lingzhi Li, Zhen Shen, Li Shen, and Liefeng Bo. 2022. 4K-NeRF: High\nFidelity Neural Radiance Fields at Ultra High Resolutions. (2022). arXiv:2212.04701.\nThomas Whelan, Michael Goesele, Steven J. Lovegrove, Julian Straub, Simon Green,\nRichard Szeliski, Steven Butterfield, Shobhit Verma, and Richard Newcombe. 2018.\nReconstructing Scenes with Mirror and Glass Surfaces. ACM Trans. Graph. 37, 4\n(2018), 102:1\u201311. doi: 10.1145/3197517.3201319\nBennett Wilburn, Neel Joshi, Vaibhav Vaish, Eino-Ville Talvala, Emilio Antunez, Adam\nBarth, Andrew Adams, Mark Horowitz, and Marc Levoy. 2005. High performance\nimaging using large camera arrays. ACM Trans. Graph. 24, 3 (2005), 765\u2013776. doi:\n10.1145/1073204.1073259\nLance Williams. 1983. Pyramidal Parametrics. Computer Graphics (Proceedings of\nSIGGRAPH) 17, 3 (1983), 1\u201311. doi: 10.1145/800059.801126\nTianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz\nOztireli. 2022b. D2NeRF: Self-Supervised Decoupling of Dynamic and Static Objects\nfrom a Monocular Video. In NeurIPS.\nXiuchao Wu, Jiamin Xu, Zihan Zhu, Hujun Bao, Qixing Huang, James Tompkin, and\nWeiwei Xu. 2022a. Scalable Neural Indoor Scene Rendering. ACM Trans. Graph. 41,\n4 (2022), 98:1\u201316. doi: 10.1145/3528223.3530153\nWenqi Xian, Alja\u017e Bo\u017ei\u010d, Noah Snavely, and Christoph Lassner. 2023. Neural Lens\nModeling. In CVPR. 8435\u20138445. doi: 10.1109/CVPR52729.2023.00815\nYuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian\nTheobalt, Bo Dai, and Dahua Lin. 2022. BungeeNeRF: Progressive Neural Ra-\ndiance Field for Extreme Multi-scale Scene Rendering. In ECCV. doi: 10.1007/978-\n3-031-19824-3_7\nJiamin Xu, Xiuchao Wu, Zihan Zhu, Qixing Huang, Yin Yang, Hujun Bao, and Weiwei\nXu. 2021. Scalable Image-Based Indoor Scene Rendering with Reflections. ACM\nTrans. Graph. 40, 4 (2021), 60:1\u201314. doi: 10.1145/3450626.3459849\nLinning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan, Nanxuan Zhao, Christian\nTheobalt, Bo Dai, and Dahua Lin. 2023. Grid-guided Neural Radiance Fields for\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nXu et al.\nLarge Urban Scenes. In CVPR. doi: 10.1109/CVPR52729.2023.00802\nJiawei Yang, Marco Pavone, and Yue Wang. 2023. FreeNeRF: Improving Few-shot\nNeural Rendering with Free Frequency Regularization. In CVPR.\ndoi: 10.1109/\nCVPR52729.2023.00798\nJae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. 2020. Novel\nView Synthesis of Dynamic Scenes with Globally Coherent Depths from a Monoc-\nular Camera. In CVPR. doi: 10.1109/CVPR42600.2020.00538\nZehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger.\n2022. MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface\nReconstruction. In NeurIPS.\nYuqi Zhang, Guanying Chen, and Shuguang Cui. 2023. GP-NeRF: Efficient Large-scale\nScene Representation with a Hybrid of High-resolution Grid and Plane Features.\n(2023). arXiv:2303.03003.\nJingsen Zhu, Yuchi Huo, Qi Ye, Fujun Luan, Jifan Li, Dianbing Xi, Lisha Wang, Rui\nTang, Wei Hua, Hujun Bao, and Rui Wang. 2023. I2-SDF: Intrinsic Indoor Scene\nReconstruction and Editing via Raytracing in Neural SDFs. In CVPR. doi: 10.1109/\nCVPR52729.2023.01202\nVR-NeRF: High-Fidelity Virtualized Walkable Spaces\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nGround Truth\niNGP\nwith PQ\nwith PQ + LOD\nSelected Patch\nPSNR 37.24\nPQ-PSNR 42.97\nPSNR 36.41\nPQ-PSNR 42.11\nPSNR 34.85\nPQ-PSNR 40.55\nSSIM 0.957\nPQ-SSIM 0.980\nSSIM 0.969\nPQ-SSIM 0.986\nSSIM 0.974\nPQ-SSIM 0.988\nPSNR 41.84\nPQ-PSNR 46.48\nPSNR 39.41\nPQ-PSNR 44.44\nPSNR 38.49 \nPQ-PSNR 43.23\nSSIM 0.983\nPQ-SSIM 0.990\nSSIM 0.987\nPQ-SSIM 0.993\nSSIM 0.990\nPQ-SSIM 0.994\n3\n2\nPSNR 30.77\nPQ-PSNR 36.07\nPSNR 29.72\nPQ-PSNR 34.88\nPSNR 29.60\nPQ-PSNR 34.82\nSSIM 0.898\nPQ-SSIM 0.933\nSSIM 0.898\nPQ-SSIM 0.933\nSSIM 0.908\nPQ-SSIM 0.940\nPSNR 29.07\nPQ-PSNR 36.23\nPSNR  27.48\nPQ-PSNR 34.74\nPSNR 26.04\nPQ-PSNR 33.25\nSSIM 0.824\nPQ-SSIM 0.906\nSSIM 0.847\nPQ-SSIM 0.922\nSSIM 0.849\nPQ-SSIM 0.923\n0\n1\n3\n2\n0\n1\n3\n2\n0\n1\n3\n2\n0\n1\n0\n1\n2\n3\n0\n1\n2\n3\n0\n1\n2\n3\n0\n1\n2\n3\nFigure 6: Qualitative comparisons between (1) iNGP, (2) iNGP with PQ color space, and (3) iNGP with PQ color space and LOD.\nThe four selected examples show the improvements over the baselines by adding PQ color space and LOD in combination, which\nleads to cleaner appearance and geometry finer details. The use of PQ color space stablizes the learning of correct radiance\nvalues, while the inclusion of LOD helps to learn a cleaner appearance and geometry that is robust to distance-dependent\nappearance variations. By dynamically allocating model capacity to the sampled points based on the needed level of detail, it\nfurther reveals more details over the ablated counterparts. (Images are white-balanced and tonemapped for better visualization.)\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nXu et al.\nFigure 7: Sweep of exposure values. For scenes with high dynamic range (e.g., bright outdoor views in the riverview scene),\none can freely adjust the exposure setting at render time by manipulating the tonemapping from PQ color space to sRGB space.\nLOD bias  \u20135\nLOD bias  \u20133\nLOD bias  +1\nLOD bias  \u20131\nLOD bias  0\nLOD bias  +3\nFigure 8: Sweep of LOD bias. We interpolate between a negative LOD bias of \u20135 and a positive LOD bias of +3 applied on top of\nthe original estimated LOD value for each sample point on the apartment scene. A negative LOD bias blurs the rendering by\nmasking out grid features representing high-frequency details, while a positive LOD bias helps reveal sharper details.\nGT\nGT\nRendered \nRGB\nRendered \nDepth\nEstimated \nNormal\nOccupancy \nGrid\nGT\nGT\nPred.\nPred.\nPred.\nPred.\nFigure 9: office2 results rendered at 4K resolution, trained with 400K iterations. Top row: from left to right, we show (1) rendered\nRGB image, (2) estimated depth map, (3) estimated normals, and (4) the occupancy grid. Bottom row: The highlighted patches\nreveal sufficient fine details.\nVR-NeRF: High-Fidelity Virtualized Walkable Spaces\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nA\nADDITIONAL CAPTURE DETAILS\nThe system and procedure described in Section 3 is the third it-\neration of our capture stack. The first version of the \u201crig\u201d was a\nsingle handheld DSLR camera, but achieving the desired capture\ndensity quickly became tedious. This was the primary motivation\nfor the design of the second version of the rig, v1, which featured 9\ncameras. While a substantial improvement, the low camera count\non v1 still required captures with the rig facing multiple directions.\nFortunately, v1 was designed with upgradability in mind, and we\nwere quickly able to add an additional 13 cameras. The result is\nthe current configuration, v2, which again simplified the capture\nprocedure and further increased our data quality.\nA.1\n\u201cRig\u201d v0 \u2014 Handheld DSLR\nCapture hardware. We began by performing handheld captures\nwith a single Canon 1D X Mark II camera, which can take 20 mega-\npixel photos. This was paired with a Canon EF 8\u201315mm f/4L Fisheye\nUSM lens, set to 8 mm focal length to ensure the highest possible\nfield of view, which minimized the number of images required to\nachieve high viewing direction coverage. A cellphone camera was\ninitially considered, but ultimately rejected due to the lack of in-\nterchangeable lens and insufficient field of view on the existing\nlenses. We also experimented with using a tripod for additional sta-\nbility during capture, but found it too cumbersome to continuously\nreposition it and thus removed it.\nCapture procedure. Each capture began by picking a reasonable\nISO, shutter speed, and f-stop which would be fixed for the scene\n\u2013 typically somewhere around ISO 1000, 1/40 seconds, and \ud835\udc53 /4.\nThe lens was set to its widest setting, at 8 mm. We then walked\nmultiple loops around the scene, taking an image at small steps,\ntypically about 30 cm, along each one. The camera would be held\nlevel horizontally, and its height would be increased each loop,\nstarting at approximately knee height and increasing in 20\u201350 cm\nintervals until the camera was above the head height of the person\nperforming the capture. This typically resulted in approximately\n100\u2013200 photos being captured for a single scene in 1\u20132 hours.\nA.2\nEyeful Tower v1 \u2014 9 fisheye cameras\nThe captures we performed with the single handheld camera were\nenough to get us started, but also clearly had some limitations. The\nbiggest, and most obvious, was the difficulty and time required\nto ensure sufficient scene coverage. We also observed over- and\nunder-exposure of different parts of our scenes, such as when look-\ning through windows (at the sun) or at shadows. We endeavored\nto address both of these issues with a rig that featured multiple\nrigidly mounted cameras, at differing heights, which could capture\nsimultaneously. Cameras at different heights would allow captures\nsimilar to the handheld setup with only a single pass of the scene,\nrather than one per height as before. The rigid mounting would\nalso enable an exposure bracket to be taken, enabling HDR images\nto be generated.\nCamera, lens, and exposure. The construction of a multi-camera\nrig gave us the opportunity to take a closer look at our camera\nand lens selection. After carefully considering options for both\nprofessional and machine vision cameras, we chose the Sony \ud835\udefc1\nv1\nv2\n9\u00d7 Sony Alpha 1 cameras\n8\u201315 mm fisheye lenses\n50 MP raw photos\n22\u00d7 Sony Alpha 1 cameras\n12\u201324 mm lenses\n50 MP raw photos\nThe Eyeful Tower\nFigure 10: The two versions of the Eyeful Tower camera rig.\nmirrorless interchangeable lens cameras for striking a good balance\nbetween resolution (50 megapixels), dynamic range (14 stops), ease-\nof-use (available analog triggers and Ethernet socket), as well as\nform factor (smaller than our previous Canon DSLR camera). We\ncontinued to use the same lens as before, via a Metabones EF-to-\nE-mount adapter, but this time zoomed to 12 mm to fill more of\nthe camera\u2019s sensor, as shown in Figure 11. We use ISO 500, the\ncamera\u2019s higher native ISO, for minimal imaging noise, and set the\naperture to \ud835\udc53 /8 with a focus distance of 1 m for a large depth of field.\nThe cameras are configured to take a 9-image exposure bracket,\n1 stop apart (\u20134 EV to +4 EV), as shown in Figure 13. The center\nexposure value is adjusted per-scene and is typically between 1/200\nand 1/60 of a second. RAW and JPEG images are stored redundantly\non the two SD cards in each camera.\nMechanical design and camera placement. We designed the cap-\nture rig using 80/20 extruded aluminium around an 80 \u00d7 80 cm base\nwith a 1.8 m vertical pole, allowing for substantial adjustability and\nexpandability. The pole held 7 camera brackets, each capable of\nsupporting a single camera, whose height could be adjusted, and\nwhose direction could be adjusted within 180 degrees horizontally.\nWe positioned the cameras left, forward, and right in an alternat-\ning fashion, as shown on the left side of Figure 10, attempting to\nmaximize scene coverage from a single rig position while allow-\ning sufficient space for the operator to not be visible in camera\nimages when standing behind the rig. One forward-facing bracket,\nat roughly eye height, was specially modified to support a second\ncamera that would be held out for validation. A final camera was\nadded at the top for ceiling coverage, which was otherwise not\npresent, for a total of 9 cameras on v1.\nElectrical design. At the base of the rig is a 1.5 kWh Li-ion bat-\ntery, capable of supplying regulated 12 V DC and 120 V AC power\nvia an integrated inverter. The 12 V bus is used to provide power\nto all cameras and a Raspberry Pi 4 via a 12 V to USB-PD adapter.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nXu et al.\nFigure 11: Example frames from the \u2018workshop\u2019 dataset captured using the 9 cameras in Eyeful Tower v1.\nFigure 12: Example frames from the \u2018apartment\u2019 dataset captured using the 22 cameras in Eyeful Tower v2.\n1/3200 s (\u20136 stops)\n1/1600 s (\u20135 stops)\n1/800 s (\u20134 stops)\n1/400 s (\u20133 stops)\n1/200 s (\u20132 stops)\n1/100 s (\u20131 stops)\n1/50 s (\u00b10 stops)\n1/25 s (+1 stops)\n1/13 s (+2 stops)\nFigure 13: To capture the high dynamic range (HDR) of the real world, we take nine exposures at increasing shutter speeds, and\nmerge these photos into a single HDR image. We can therefore reproduce the full dynamic range of input exposures, including\nthe brightest and darkest regions.\nVR-NeRF: High-Fidelity Virtualized Walkable Spaces\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nThe cameras are powered by internal Li-ion batteries, but are con-\ntinually trickle charged via USB. The 120 V AC output powers a\n24-port 1-Gigabit Ethernet switch that connects all the cameras to\nthe Raspberry Pi. The Pi runs custom capture software to enable\nformatting of all camera SD cards, camera parameter updates, and\nsimultaneous bracket triggering via the Sony Camera Remote SDK.\nThe switch also offers a 10GbE SFP+ port which is used to offload\ndata from the cameras to a PC for downstream processing. The\nbattery is able to power the entire system for 6 to 8 hours of use,\nand can be recharged via normal 120 V wall power overnight.\nCapture procedure. Cameras at multiple heights and viewing\ndirections simplifies the coverage problem during capture from 6D\nto 3D, as we now only need to tile the floor with the rig facing a few\ndirections. The general capture strategy is described in Section 3.2.2,\nwith example camera positions shown in Figure 14. For this initial\nversion of the rig, however, we needed to capture in four separate\norientations (facing forward, backward, left, and right, versus just\nforward and backward as described above), in order to ensure 360\nhorizontal degrees of coverage for each rig position at each height.\nA.3\nEyeful Tower v2 \u2014 22 rectilinear lenses\nThe additional cameras of Eyeful Tower v1 offered a substantial\nusability and data quality improvement over the single handheld\ncamera, but the sparse positioning still necessitated four passes of\nthe scene. The current version of the rig, v2, attempts to address\nthis by adding yet more cameras, increasing the total count to 22, as\nshown on the right in Figure 10. The lenses have also been replaced\nwith rectilinear Sony FE 12\u201324mm F/2.8 GM lenses, selected for\ntheir higher sharpness (2\u20133\u00d7 MTF50) and lower chromatic aberra-\ntion (less than half, in pixels). The camera and lens parameters are\nkept the same as before \u2014 12 mm zoom, ISO 500, \ud835\udc53 /8 aperture, and\n1 m focus distance. An example set of captured images is shown in\nFigure 12. The additional cameras more than compensate for the\nslightly lower per-camera FOV, and enable us to capture a scene in\ntwo passes (rig facing forward and backward) rather than the four\nthat v1 required.\nB\nMODEL IMPLEMENTATION DETAILS\nB.1\nVignetting and Lens Distortion\nVignetting effects are commonly present in wide-angle lenses, such\nas the fisheye and wide-angle lenses used in our capture system (see\nAppendix A). We parameterize the vignetting effect for each camera\nFigure 14: Visualized camera positions for the workshop\n(left) and apartment (right) scenes.\nusing the Kang and Weiss model [2000] with \ud835\udc3c\u2032 \u2248 (1 \u2212 \ud835\udefc\ud835\udc5f)\ud835\udc3c, where\nwe make \ud835\udefc and the principal point (\ud835\udc50\ud835\udc65,\ud835\udc50\ud835\udc66) used for computing\n\ud835\udc5f learnable parameters for each camera sensor. This allows the\nmodel to fit to the radial falloffs. Without modeling vignetting\n[Lyu 2010] explicitly, the model is likely to overfit on training\nviews by casting unwanted black floats everywhere in the air to\naccounting for the brightness decrease towards the edge of each\nimage frame. Figure 15 shows that modeling vignetting explicitly is\ncrucial for certain subset of data. In scenarios where the test camera\nhas unlearned vignetting parameters, one can optionally optimize\nthe vignetting model for the test camera lens before inference with\nall the other parameters fixed. The optimization can be done quickly\nin hundreds or thousands iterations. Note that we do not optimise\nfor lens distortions [Xian et al. 2023] in our current implementation,\nwhich is left as future work to further improve pixel-wise alignment\nand accuracy.\nwith vignetting\nPredicted vignetting\nwithout vignetting\nGT\nFigure 15: Ablation on vignetting effects. Without explicitly\nmodeling vignetting, the model has difficulty in explaining\naway the inconsistent appearance brought by different cam-\nera lenses. We found that this effect is easier to observe in\ntextureless areas, such as walls and floors in office_view1\n(first row) and office_view2 (second row), with noticeable\ndark floaters in front of cameras. The vignetting effect is\nfound more strongly in the Inria datasets [Philip et al. 2021],\nas shown in the bottom figure.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nXu et al.\nB.2\nLoss Design\nImage Loss. With PQ color space ranged in [0,1], we can directly\nuse \ud835\udc3f1/\ud835\udc3f2 for reconstruction loss metric without biasing towards\ncertain color range. We prioritize \ud835\udc3f1 loss in our experiments, as it\nis generally regarded as a more robust loss for outliers and provide\nsparser solutions compared to \ud835\udc3f2 loss. We also experimentally found\nthat \ud835\udc3f1 leads to sharper details compared to \ud835\udc3f2 loss.\nDepth Regularizations. NeRF reconstruction can be challenging\nfor textureless areas, such as a flat white wall or a featureless floors.\nFurthermore, the challenging reflections and shadows which caused\nthe abrupt change in the brightness can easily lead to incorrect\ngeometry where the view-dependent effects fail to capture the\nvariances. To address this issue, some approaches use additional\ngeometrical information [Yu et al. 2022], such as depth map guid-\nances. As the monocular depth predicted by off-the-shelf models\ncan only be used in relative scale, we instead resort to the recon-\nstructed mesh from Metashape during the pose estimation stage\nand project it to the training views as pseudo ground truth depth\nmap for supervision, which allows us to perform direct comparison\nin absolute scale. To avoid the misguidance from unreliable depth\nmap, we use depth loss only for the early stage of training, where\nthe incorrect geometry can get refined with image reconstruction\nloss only. Figure 16 shows the effects of using depth loss guidance\nin early training stages.\n(a) No depth loss\nPseudo GT Depth\nGT RGB\nPred. RGB\nPred. Depth\nPred. RGB\nPred. Depth\n(b) Depth loss\nfor 5k iterations\nFigure 16: Applying depth loss. The bottom-left image shows\nthe pseudo depth guidance obtained from Metashape\u2019s mesh\nreconstruction. Experimentally, we found that the depth loss\nconverges extremely fast in few hundreds or thousands itera-\ntions. The first row shows the results without depth supervi-\nsion, while the result shown in the second row is supervised\nby the depth loss for the first 5K iterations. The depth supervi-\nsion prevents model from cheating reflections with samples\nplaced behind the walls, leading to the inability to recover\nthe highlights.\nDistortion Loss. We adopt a simplified version of distortion loss\n[Barron et al. 2022] that encourages the sparseness of the sample\nweights along the ray without considering the compactness of sam-\nple intervals from the proposal network. In practice, we additionally\nconsider the depth variance loss applied on the inner world (regions\nunaffected by space warping) that encourages the weights to be\nconcentrated around the estimated depth. The depth variance is\ncalculated among samples cast by a single pixel. Figure 17 shows the\neffects of applying depth variance loss that learns flat wall without\nreflections. This trick is suggested to use during the separate prun-\ning stage, where the cleaner geometry is used for obtaining reliable\noccupancy grid only. This serves as an additional regularization for\nchallenging scenarios such as the highly reflective surfaces, planes\nwith detailed textures, where the depth variance could be relative\nlarge leading to incorrect geometry. We optionally add an \u201cempty\naround camera\u201d loss by placing random samples in the unit sphere\naround the cameras to avoid near-plane ambiguity, similar to the\nocclusion loss in FreeNeRF [Yang et al. 2023].\nOther Regularizations. Barron et al. [2023] recently proposed to\napply a weight-decay loss on the multi-level hash grid features to\nencourage a normal distribution of learned grid features. We found\nthat this loss can regularize the learning of grid features, leading to\nmore complete geometry and flat surfaces, yet at the cost of lower\nconvergence speed and occasional detail loss.\nB.3\nWeighted Sampling\nGiven the large number of rays used to be supervised during train-\ning, it is generally impractical to revisit each pixel multiple times\nNo depth\nvariance loss\nWith depth variance loss\nPred. RGB\nPred. RGB\nPred. Depth\nPred. Depth Variance\nPred. Depth\nPred. Depth Variance\nFigure 17: Applying depth variance loss. Depth variance loss\ncan be applied without ground-truth depth supervision. The\ndepth variance map on the top right shows a clear correla-\ntion between these reflection regions and the depth variance\nstatistics, which indicates its potential to suppress cheating\nof appearance changes via wrong geometry.\nVR-NeRF: High-Fidelity Virtualized Walkable Spaces\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nTable 3: Quantitative comparison results on the Eyeful Tower test set. All results are trained on 1K images for 110K iterations\nwith 1024 samples per ray. We report PSNR/SSIM/LPIPS both in sRGB and PQ color spaces. The best results are highlighted.\nFar-\niNGP (our implementation)\niNGP with PQ color space\niNGP with PQ color space and LOD\nScene\nfield\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193 PQ-PSNR\u2191 PQ-SSIM\u2191 PQ-LPIPS\u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193 PQ-PSNR\u2191 PQ-SSIM\u2191 PQ-LPIPS\u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193 PQ-PSNR\u2191 PQ-SSIM\u2191 PQ-LPIPS\u2193\napartment\n\u2713\n30.68\n0.903\n0.226\n35.16\n0.942\n0.203\n31.06\n0.910\n0.208\n35.52\n0.946\n0.186\n32.36\n0.915\n0.190\n36.61\n0.948\n0.173\nkitchen\n\u2713\n31.33\n0.925\n0.216\n36.43\n0.954\n0.166\n31.50\n0.928\n0.210\n36.67\n0.956\n0.161\n32.41\n0.932\n0.184\n37.53\n0.958\n0.146\noffice1a\n35.71\n0.972\n0.095\n41.75\n0.988\n0.055\n36.20\n0.974\n0.091\n42.43\n0.989\n0.052\n36.82\n0.976\n0.082\n43.05\n0.990\n0.048\noffice1b\n27.58\n0.880\n0.402\n33.38\n0.952\n0.248\n28.44\n0.895\n0.361\n34.59\n0.960\n0.216\n29.97\n0.914\n0.255\n36.16\n0.966\n0.162\noffice2\n39.84\n0.983\n0.053\n44.46\n0.986\n0.031\n40.26\n0.992\n0.046\n45.12\n0.993\n0.026\n40.71\n0.987\n0.037\n45.53\n0.994\n0.023\noffice_view1\n\u2713\n29.75\n0.890\n0.264\n35.14\n0.942\n0.192\n30.20\n0.897\n0.253\n35.63\n0.947\n0.181\n31.80\n0.901\n0.223\n37.09\n0.948\n0.167\noffice_view2\n\u2713\n27.19\n0.858\n0.259\n33.14\n0.921\n0.201\n27.64\n0.865\n0.259\n33.73\n0.926\n0.198\n28.08\n0.868\n0.230\n34.09\n0.927\n0.181\nriverview\n\u2713\n27.17\n0.864\n0.181\n34.09\n0.934\n0.135\n27.83\n0.869\n0.179\n34.77\n0.938\n0.134\n28.47\n0.863\n0.181\n35.51\n0.935\n0.135\nseating_area\n\u2713\n36.85\n0.968\n0.070\n41.93\n0.983\n0.050\n37.16\n0.970\n0.069\n42.26\n0.984\n0.049\n37.86\n0.974\n0.053\n42.92\n0.987\n0.035\ntable\n\u2713\n34.28\n0.952\n0.091\n40.37\n0.974\n0.067\n34.52\n0.954\n0.087\n40.78\n0.975\n0.064\n35.40\n0.962\n0.066\n41.59\n0.980\n0.046\nworkshop\n30.86\n0.907\n0.155\n36.54\n0.949\n0.115\n32.34\n0.937\n0.102\n38.12\n0.965\n0.074\n32.40\n0.940\n0.101\n38.31\n0.967\n0.073\nAverage\n31.93\n0.918\n0.183\n37.39\n0.957\n0.133\n32.47\n0.926\n0.170\n38.15\n0.962\n0.122\n33.30\n0.930\n0.146\n38.95\n0.964\n0.108\nwith limited training time. The training of NeRF is notorious for\nlong training time in order to bring finer details, which usually\nappear in the later stage of training due to the spectral bias of\nneural networks [Rahaman et al. 2019]. We also observed from\nthe computed error map that the errors usually dominate in high-\nfrequency details, while areas with relative uniform geometry and\nappearance have significant lower errors. One can either assign\nhigher loss weights to these pixels representing high-frequency de-\ntails or sampling them more often during the training. Practically,\nwe compute the Laplacian pyramid for each image during data\npreparation stage and use them as the indicator for important pixel\nsamples. Instead of using the directly obtained continuous value for\npixel weights, we threshold the importance samples by referencing\nto the 75% percentile of all pixels over the image. See Figure 18\nfor the construction of weighted map. For perspective images, we\nadditionally consider the effects of wide-angle field-of-view and\nassigning a sampling weight inverse to the actual radii of each pixel\nfootprint, which better matches the pixel coverage in 3D world.\nThe effectiveness of weighted samples can be seen in Figure 19.\nC\nADDITIONAL RESULTS AND ANALYSIS\nWe show a detailed breakdown of our complete quantitative abla-\ntion results across all 11 Eyeful Tower datasets in Table 3.\nPQ RGB\nBlurred\nPixel weights\nFigure 18: Derivation of pixel weights. We borrow the idea\nof Laplacian pyramid and derive the importance weight for\neach pixel to guide sampling.\nGT\nRandom rays\nWeighted rays\nFigure 19: Weighted samples. Effects of revealing finer details\nand geometry in early training stages (e.g., 2K iterations)\nwith weighted sampling. The sign of \u2018SHARP\u2019 in the scene is\nquickly revealed with weighted sampling.\nC.1\nLearning of linear RGB space\nFigure 20 visualize the converted PQ color space, where both very\nbright and dark regions are properly preserved in the converted\nspace. Figure 21 plots the RGB values produced by model linear\noutput layer, activation function, as well as the weighted value used\nfor final volumetric integration during the training process. It is\nclear to see that directly learning on linear RGB space with safe\nexponential function may cause the unstable training where the\nmaximum value predicted from model outputs keeps growing and\nPQ  color space\nsRGB\nPQ  color space\nsRGB\nFigure 20: Illustration of PQ color space. The highlighted\nregions represent extreme bright/dark areas which are prop-\nerly handled by the PQ conversion.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nXu et al.\nLinear Space + Exponential\nPQ Space + Sigmoid\n(a) Max RGB value from linear output layer\n \n(b) Max RGB value afer color activation\n(c) Max RGB value multiplied by weights\n(d) Validation PSNR\n4e+3\n3e+3\n2e+3\n1e+3\n0\n0\n100K\n0\n100K\n1.6e+38\n1.2e+38\n8e+37\n4e+37\n0\n40\n30\n20\n10\n0\n0\n100K\n32\n24\n20\n16\n0\n100K\n28\nFigure 21: RGB value analysis. These curves show: (a) the\nmaximum RGB value obtained from the linear output layer\nof the color network, (b) the maximum RGB value after the\ncolor activation function, (c) the point-wise color multiplied\nby the integrated density weight for final color composition,\nand (d) the validation PSNR showing the rendering fidelity.\nThe two blue lines represent: (1) the baseline using iNGP with\nlinear color space, and exponential activation (light blue),\nand (2) our PQ color space with sigmoid activation function\n(dark blue).\nfluctuated all the way. In contrast, we need not worry about all these\nissues in PQ RGB space as we are already work on the bounded\nPQ space in range [0,1], which gives pretty stabilized learning with\ncommonly used sigmoid activation function.\nC.2\nEffects of LOD\nFigure 22 shows the two ablated modules on Inria dataset. One\ncommonly observed advantage of using LOD feature is its improve-\nments on revealing fine details. We conjecture that by dynamically\nmasking out high-resolution grid features, the model encourages\nthese high-frequency features to only be used for rendering con-\ntents with close observations and fine details.\nC.3\nAblation on pruning strategy\nFigure 24 shows the comparison on different pruning strategies.\nAs \u2018history pruning\u2019 only considers stochastically sampled points\nvisited during the pruning period, it is unlikely to visit all the\nvoxels during the updating period, leading to numerous holes in the\nobtained occupancy grids. The quality of \u2018grid pruning\u2019 commonly\ndepends on the number of samples placed within each voxel. The\nestimation accuracy can get improved with increased number of\nsamples yet at the cost of large computation expenses. Furthermore,\nas these samples are usually evenly places for robustness, it can\nrarely matches with surface points, leading to box-like artifacts in\nthe obtained geometry. Our joint training combines the merits of\neach method and achieves accurate pruning results with limited\ncomputing budges (4 points for each voxel for grid pruning).\nExposure 0\nExposure \u20132\nGT\niNGP\nwith PQ\nwith PQ+LOD\nFigure 22: Ablation comparisons on raw Inria dataset [Philip\net al. 2021]. The highlighted patches clearly show that iNGP\nw/ PQ + LOD better preserves the geometry and details com-\npared to the ablated baselines. We also adjust the exposure\nvalues to adapt to the bright areas around the window.\nC.4\nMip-NeRF 360 & Inria datasets\nWe additionally tested on Mip-NeRF 360 dataset [Barron et al. 2022]\nwith our LOD and pruning designs. Figure 25 and Figure 26 show\nall the 9 scenes used in Barron et al. [2022]. The models are trained\nat 2K resolution, with properly recovered fine details and accurate\noccupancy grids. Philip et al. [2021] provide scenes with captured\nraw images. Figure 27 shows three scenes trained with HDR inputs\nand PQ color space.\n0\n100K\nValidation SSIM\nValidation PSNR\n0\n100K\n40\n30\n20\n1.0\n0.9\n0.7\n0.8\nFigure 23: 1K & 2K training. The learning curves show the\ncomparative validation results between training on 1K &\n2K resolution datasets. The black brackets include the pair\nof experiments for a same scene with 1K and 2K version.\nThe quantitative metrics are similar without significant drop\nwhen adapting to higher resolution images, which shows\nthe potential of using higher-resolution images for training\nsufficiently long.\nVR-NeRF: High-Fidelity Virtualized Walkable Spaces\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nJoint pruning\nHistory pruning\nGrid pruning\nRendered RGB\nRendered Depth\nEstimated Normal\nOccupancy Grid\nFigure 24: Ablation on pruning strategy. From top to bottom, we compare three alternative pruning strategies. \u2018Joint pruning\u2019\n(bottom) leverages the advantage of both \u2018history pruning\u2019 (top) and \u2018grid pruning\u2019 (middle) by placing important sample points\nobserved during training and also densely evaluating voxel grids with sufficient coverage. The obtained occupancy grid shown\non the right is clean and accurate, and the derived depth map and normal map indicate the well-preserved geometry compared\nto each individual strategy.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nXu et al.\nGround Truth\nOccupancy Grid\n2K Rendering\nPSNR: 29.47 \nSSIM: 0.869\nPSNR: 26.91 \nSSIM: 0.738\nPSNR: 26.05\nSSIM: 0.688\nPSNR: 31.27\nSSIM: 0.924\nPSNR: 31.15\nSSIM: 0.925\nFigure 25: Additional results on Mip-NeRF 360 scenes [Barron et al. 2022], trained on 2K resolution images for 50K iterations.\n(Best zoom in to investigate details.)\nVR-NeRF: High-Fidelity Virtualized Walkable Spaces\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nPSNR: 33.94\nSSIM: 0.910\nGround Truth\nOccupancy Grid\n2K Rendering\nPSNR: 26.06\nSSIM: 0.762\nPSNR: 23.69\n SSIM: 0.735\nPSNR: 33.01\nSSIM: 0.943\nPSNR: 23.69\nSSIM: 0.735\nFigure 26: Additional results on Mip-NeRF 360 scenes [Barron et al. 2022], trained on 2K resolution images for 50K iterations.\n(Best zoom in to investigate details)\nC.5\nHigh-resolution rendering\nFigure 23 shows the learning curve of 1K and 2K training results.\nThe validation PSNR for each 1K & 2K pair is generally close, with\nslight drop in SSIM metric. Figure 9 shows an example trained with\n4K resolution with fine-grained details.\nC.6\nHandling per-image variations\nTo explain per-image appearance variations, a latent code is com-\nmonly attached to each training image following the practice of\nMartin-Brualla et al. [2021]. One specialty of our captured data is\nthat instead of using per-image latent code, we can consider using\nper-frame latent code (shared by 22 cameras at a same time) as\na stronger regularization constraint. During our capture process,\nthe outdoor lighting conditions can change slightly, and the mov-\ning people and capture rig can cast annoying shadows sometimes.\nIt still remains an open question for us how to deal with these\nshadows effectively, as we found that using the interpolated latent\ncode (Figure 28) or modeling with shadow field [Wu et al. 2022b]\nexplicitly (Figure 29) can only lead to sub-optimal solutions. This\nbecomes an extreme challenging scenario when we only have few\nimages for each observation locations while most of them are cast\nby shadows.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nXu et al.\nGround Truth\nOccupancy Grid\nRendering\nPSNR: 31.68\nSSIM: 0.894\nPSNR: 31.53\nSSIM: 0.926\nPSNR: 34.54\nSSIM: 0.915\nFigure 27: Additional results on Inria scenes [Philip et al. 2021], trained on 1K resolution images for 100K iterations. (Best\nzoom in to investigate details)\nprevious latent code\nnext latent code\ninterpolated latent code\nFigure 28: Latent code condition. We try to use latent code\nto explain away shadow issues. The image is rendered at the\ninterpolate frame between two adjacent capture timestamps.\nFrom left to right we show results of using the learned latent\ncode from previous frame, interpolated latent code, and la-\ntent code from next frame. While showing the tendency of\nmoving the shadows smoothly (highlighted in white poly-\ngons), the overall appearance remains noisy with dark floats.\nC.7\nNerfstudio results (nerfacto)\nPure MLP-based NeRF methods have difficulty in scaling up due to\nslow training and limited model capacity. An alternative baseline\nGT\ndeducted shadow\npredicted shadow field\nFigure 29: Shadow Fields. We implemented a shadow field\n[Wu et al. 2022b] to explain shadows with per-point predicted\nattenuation. We found it helpful to use low-frequency grid\nfeature only for shadow field prediction. While the predicted\nshadow field looks reasonable in general, the accuracy is not\nsufficient to properly compensate for the affected appear-\nance.\nwe considered is the versatile nerfstudio tool [Tancik et al. 2023].\nWe test a subset of our dataset and use nerfstudio for training\nand evaluations. We use the integrated nerfacto model, trained on\n2K image for 100K iterations. The parameter setting for hash grid\nmatched with our model (128 \u00b7 1.415), and leave other components\nVR-NeRF: High-Fidelity Virtualized Walkable Spaces\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nTable 4: Test results on selected datasets using nerfstudio\n[Tancik et al. 2023]. PSNR/SSIM/LPIPS and FPS are reported\nhere.\nPSNR\nSSIM\nLPIPS\nFPS\nseating_area\n30.01\n0.891\n0.140\n0.517\nworkshop\n26.85\n0.849\n0.277\n0.570\noffice2\n26.60\n0.939\n0.127\n0.595\noffice_view1\n27.01\n0.804\n0.382\n0.374\nriverview\n26.66\n0.817\n0.260\n0.345\nOurs\nnerfacto\nGT\nnerfacto\nOurs\nGT\nOurs\nnerfacto\nOurs\nnerfacto\nFigure 30: Qualitative comparison between ours and ner-\nfacto. While the nerfacto model tends to produce scenes with\nsmooth geometry and visuals, our model renders finer de-\ntails in terms of correct color and high-frequency details, as\nvisible in the zoomed-in patches.\nwith default configurations. The poses are processed with the XML\ncamera pose file produced by Agisoft Metashape. We use the fish-\neye lens model for Eyeful Tower v1 images (with cropped black\nborders), and perspective model for Eyeful Tower v2 images, which\nare supported directly by nerfstudio.\nTable 4 and Figure 30 show the quantitative and qualitative\nresults for nerfstudio nerfactor model trained with sRGB spaces.\nWe found that they can handle far-field well in unbounded scenes\nand can capture most details in the scene, yet commonly miss the\ndetailed textures such as those on the carpets and floor. Note that\nour results shown in Figure 30 are trained on HDR and converted\nto sRGB space, where we can fairly compare the rendered visual\nquality.\nD\nADDITIONAL RENDERER DETAILS\nD.1\nDesign of our 20-GPU Workstation Machine\nThe design of our custom 20-GPU rendering workstation was driven\nby the following goals:\n\u2022 Single CPU. Our early experiments revealed higher stability\nfor the Oculus VR runtime with single-socket computers\nthan with dual-socket machines. Additionally, programming\nfor dual-socket machines requires special considerations, e.g.\nwhen crossing NUMA domains. To maximize stability and\nminimize programming difficulty, we require a single CPU.\n\u2022 16+ direct-attached dual-slot GPUs. The GPUs should be\navailable to programs just as the typical 2\u20134 are on work-\nstations, i.e., without network access or special cluster man-\nagement software, as a typical render farm would have. The\nGPUs should be approximately equivalent to desktop Nvidia\nRTX 3090 cards. This, combined with the previous goal,\nshould enable applications written for our multi-GPU work-\nstations to fully utilize the machine with no code changes.\n\u2022 Windows 10 OS. The Oculus VR stack only works on Win-\ndows operating systems. Using Windows 10 (instead of e.g.\nWindows 11 or Windows Server) allows the machine to more\nclosely match our development workstations.\n\u2022 Mobile and quiet. The machine should live inside a movable\nenclosure that can fit through a standard 32\" door, and be\nquieter than 55 dB within it. This enables the machine to be\ntaken to conferences and for demos to be given in the same\nroom.\nA thorough survey of commercial options found no solutions which\nsatisfy all above requirements. Many vendors offer workstations\nor servers with 8 GPUs, but nearly all use two CPU sockets. A few\nvendors offer 10\u201316 GPU servers, but these are typically limited\nto single-slot GPUs, and always use two sockets. Thus, we build\nour own solution. This system is housed in a USystems Edge 3\nsound-dampening rack that offers 30 dB of noise reduction.\nD.2\nEfficient Level-of-Detail Rendering\nSection 4.2 described the advantages that level-of-detail render-\ning can have on image quality. However, our LOD-based masking\nstrategy can also improve rendering performance. Kernel profiling\nmeasurements revealed that substantial time per frame is being\nspent waiting for features to be sampled from the multi-resolution\nhash grid, with the largest portion of the time being taken by the\nfinest feature layers. This is likely due to the hash grid storage\nthat leads to highly incoherent memory accesses. Coarser levels\nare stored within a dense linear array, and do not suffer as much\nfrom this issue, though the memory layout is still suboptimal for\nspatial coherence. Our LOD-based masking strategy removes the\nneed to sample many of these expensive hash-grid features, and\nthus decrease rendering time, as we\u2019re able to conditionally replace\nthe finest sampled levels with zeros.\n"
  },
  {
    "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs",
    "link": "https://arxiv.org/pdf/2311.02262.pdf",
    "upvote": "9",
    "text": "Preprint\nTELL YOUR MODEL WHERE TO ATTEND:\nPOST-HOC ATTENTION STEERING FOR LLMS\nQingru Zhang\u2020\u2217, Chandan Singh\u22c4, Liyuan Liu\u22c4, Xiaodong Liu\u22c4, Bin Yu\u2021,\nJianfeng Gao\u22c4, Tuo Zhao\u2020\n\u2020Georgia Institute of Technology\n\u2021University of California, Berkeley\n\u22c4Microsoft Research\n{qingru.zhang,tourzhao}@gatech.edu\nbinyu@berkeley.edu\n{chansingh,lucliu,xiaodl,jfgao}@microsoft.com\nABSTRACT\nIn human-written articles, we often leverage the subtleties of text style, such as\nbold and italics, to guide the attention of readers. These textual emphases are\nvital for the readers to grasp the conveyed information. When interacting with\nlarge language models (LLMs), we have a similar need \u2013 steering the model to pay\ncloser attention to user-specified information, e.g., an instruction. Existing methods,\nhowever, are constrained to process plain text and do not support such a mechanism.\nThis motivates us to introduce PASTA \u2013 Post-hoc Attention STeering Approach,\na method that allows LLMs to read text with user-specified emphasis marks. To\nthis end, PASTA identifies a small subset of attention heads and applies precise\nattention reweighting on them, directing the model attention to user-specified parts.\nLike prompting, PASTA is applied at inference time and does not require changing\nany model parameters. Experiments demonstrate that PASTA can substantially\nenhance an LLM\u2019s ability to follow user instructions or integrate new knowledge\nfrom user inputs, leading to a significant performance improvement on a variety of\ntasks, e.g., an average accuracy improvement of 22% for LLAMA-7B. Our code is\npublicly available at https://github.com/QingruZhang/PASTA.\n1\nINTRODUCTION\nThe advent of large language models (LLMs) has marked a significant milestone in natural language\nprocessing (NLP) and artificial intelligence (AI), showcasing exceptional performance across a wide\nrange of tasks (Vaswani et al., 2017; Brown et al., 2020a; OpenAI, 2023). Efforts to further refine\nthese models have been relentless, aiming to enable them to process and respond to natural and\nprogramming languages with human-like expertise (Stiennon et al., 2020; Yao et al., 2023).\nDespite their remarkable achievements, LLMs often encounter challenges in understanding their\ncontextual inputs during interactions with users (Shen et al., 2023; Lu et al., 2021). This difficulty\nbecomes particular evident when they are presented prompts1 containing extensive background\ncontexts or complex user instructions. Lengthy contexts can overwhelm LLMs, as their attention\nmodules, learned from data, are unable to fully capture crucial details (Liu et al., 2023). Complex\ninstructions can further inhibit the model from focusing on the user\u2019s intentions, resulting in undesired\noutputs (Wei et al., 2022). Additionally, for time-sensitive data, such as news articles, there can exist\nfactual knowledge within contexts, which contradicts with model prior beliefs induced from outdated\npre-training. As a result, a model may generate outputs conditioned on its pre-existing belief instead\nof attending to new facts within the contexts (Meng et al., 2022a;b; Mitchell et al., 2022; Hernandez\net al., 2023). All of these challenges contribute to LLMs struggling to comprehend user intentions.\nCompared to LLMs, human readers rarely struggle to understand the emphases of articles and\nintentions of writers. Writers often leverage a variety of text styles, such as bold and italics, to\nemphasize specific contents. This mechanism enables writers to direct and maintain the attention of\n\u2217Work is done during Qingru Zhang\u2019s internship at Microsoft Research.\n1We use prompts to refer to all LLM text inputs, including user instructions, and the other background\ninformation (which we refer to as context).\n1\narXiv:2311.02262v1  [cs.CL]  3 Nov 2023\nPreprint\nOriginal output:\nMary is a working professional\nHead 1\nLayer 1\nLayer 2\n\u2026\nLayer L\n\u2026\nSteered user input:\nMary is a doctor...*Return her \noccupation in json format*\nEmphasize attention score for \nselected token positions\n\u2026\nFor each selected head:\nHead 2\nHead h\nSteered output: \n{\u201cName\u201d: \u201cMary\u201d,  \u201cOccupation\u201d: \u201dDoctor\u201d}\nOriginal user input: \nMary is a doctor...Return her \noccupation in json format\nLLM\n\u2026\n\u2026\nPay attention to \n\u2018json format\u2019\nFigure 1: PASTA uses a user-specified part of the input to steer the model generation aligning with user\nintentions. PASTA modifies the attention scores generated during inference, by emphasizing the scores generated\nat token positions corresponding to the user-specified part of the context.\nhuman readers, ensuring that the intended information is accurately captured. In interactions between\nusers and LLMs, it is users also need to highlight specific information for the model. Consequently,\nmodel generation can be effectively biased in accordance with user guidance, thus addressing the\nchallenges mentioned earlier. This feature is particularly essential when designing user-AI interfaces,\nand can be frequently applied in extensive conversations between users and models. Existing methods,\nhowever, do not support such a mechanism. LLMs are inherently limited to processing plain texts,\ndevoid of any stylistic cues or emphasis markers (Brown et al., 2020b; Liu et al., 2021; Wei et al.,\n2022). Even when emphasis markers are added to prompts, state-of-the-art LLMs often struggle to\ndiscern weak signals from a couple of marker tokens (See evidence in Section 5.1).\nMotivated by the need to convey user emphasis, we introduce PASTA (Post-hoc Attention STeering\nApproach), a post-hoc method2 that enables users to highlight specific information, e.g., an instruction\nas in Figure 1, and steer models to interpret emphasized texts like human readers. Specifically,\nPASTA selects a small subset of attention heads and applies precise attention reweighting on them.\nAs illustrated in Figure 1, PASTA upweights the attention scores of the user-specified tokens while\ndownweighting the other tokens at specific attention heads. Our method is inspired by the observation\nthat attention modules exhibit various token-attending patterns across different heads (Michel et al.,\n2019; Voita et al., 2019; Clark et al., 2019). These attention patterns can be interpreted as encoding\ndiverse semantic or syntactic information, and altering them can substantially influence model\nbehaviors (Shi et al., 2023a; Hu et al., 2021b). Through steering attention modules, PASTA directs\nthe model to pay close attention to the user-specified parts and hence generate the desired output\naligning with the highlighted contents. Notably, PASTA is applied after training and does not require\nchanging any model parameters; PASTA only requires access to the attention scores of specific heads\nof an LLM.\nSince attention heads can serve different functions (Tenney et al., 2019; Deb et al., 2023), we introduce\nan efficient model profiling algorithm to identify which heads are effective for steering. Specifically,\nwe subsample small training sets from multiple tasks and evaluate the performance of attention\nsteering for each individual head across these tasks. PASTA selects the attention heads that, when\nsteered, generally improve the multi-task performance. We empirically observe that steering these\nheads not only benefits the existing tasks but also enhances the performance on unseen tasks. Notably,\nthe model profiling is performed only once for an LLM. The selected attention heads can be regarded\nas a model-level profile, effective for steering the LLM on unseen tasks.\nWe conduct experiments on diverse tasks to demonstrate the effectiveness of PASTA. Specifically,\nwe evaluate PASTA using GPT-J-6B (Wang & Komatsuzaki, 2021) and LLAMA-7B (Touvron\net al., 2023) on tasks that span complex instructions, lengthy contexts, and knowledge conflicts\nwithin contexts. The results demonstrate that PASTA consistently provides a significant performance\nimprovement over baseline prompting strategies. For example, PASTA achieve an average accuracy\nimprovement of 22% over few-shot prompting for LLAMA-7B across 4 challenging tasks.\n2Post-hoc means that our method does not update the model weights.\n2\nPreprint\n2\nBACKGROUND\nProblem description\nIn standard LLM prompting, we are given a pre-trained LLM and a text\nprompt x. In our setting, we additionally require (i) access to attention scores produced by attention\nmodules in the LLM3 and (ii) we are provided a user-specified subset of the prompt xg \u2282 x to be\nemphasized.\nAs in the example in Figure 1, x can be a string that ends in an instruction, such as Mary is a doctor\nbut used to be a nurse...Return her occupation in json format. If a user emphasizes the instruction, xg\ncan simply be the final instruction Return her occupation in json format. In evaluation datasets, we\nassume that the user-specified part of each example is already provided by enclosing at its both ends\nin some emphasis markers, like \u2018\u2217\u2019 marker in Markdown. Generating these well-structured data often\nincurs little overhead. For example, in the dataset tailored for evaluting model ability to follow user\ninstruction, we can simply mark the final instruction for every example, which are fixed and shared\nacross examples. When it comes to user-LLM interface, users can specify xg by enclosing it with the\nsame emphasis markers. xg can be specified flexibly. Namely, it need not be a continuous span, and\ncan be used to emphasize diverse information.\nMulti-Head Attention. A typical transformer model consists of L stacked layers, where each layer\ncontains two submodules: a multi-head attention (MHA) and a fully connected feed-forward network\n(FFN). Given the input X \u2208 Rn\u00d7d, MHA of the layer l performs the attention function in parallel H\nheads: MHA(l) (X) = Concat(H(l,1), ..., H(l,H))Wo where\nH(l,h) = A(l,h)V = Softmax\n\u0010\nQK\u22a4/\np\ndh\n\u0011\nV\n(1)\nwhere Q = XWqh, K = XWkh, V = XWvh and Wqh, Wkh, Wvh \u2208 Rd\u00d7dh are learnable\nprojection matrices of head h. dh is typically set to d/H. Specifically, denote the attention scores at\nthe head h of the l-th layer as A(l,h).\n3\nMETHOD\nPASTA (Algorithm 1) consists of two components: (i) post-hoc attention steering, which emphasizes\nthe user-specified parts of the input during inference, see Section 3.1 and (ii) multi-task model\nprofiling, which selects the effective attention heads for steering, see Section 3.2.\nAlgorithm 1 PASTA: Post-hoc Attention Steering Approach\nMulti-task model profiling (Section 3.2)\n1: Input: small training sets {D(i)}m\ni=1, the hyperparameters \u03b1, k;\n2: for 1 \u2264 i \u2264 m do\n3:\nfor 1 \u2264 l \u2264 L, 1 \u2264 h \u2264 H do\n4:\nEvaluate the model performance on D(i) when steering the head (l, h) by (2);\n5:\nReturn the evaluation result of steering (l, h) on D(i);\n6:\nend for\n7:\nCollect the steering results of all heads and return the task profiling R(i);\n8: end for\n9: Output: The attention head set H = \u2229m\ni=1R(i)\n1:k.\nInference-time steering (Section 3.1)\n1: Input: text inputs x, user-underlined segments G, coefficient \u03b1;\n2: Output: the model generations while steering every head (l, h) in H by (2).\n3.1\nPOST-HOC ATTENTION STEERING\nPASTA emphasizes the user-specified input subset by downweighting the attention scores of tokens\nthat are not specified by the user. Specifically, given the index set of highlighted input spans as G,\nPASTA emphasizes these user-specified tokens by an attention projection T :\nH(l,h) = T (A(l,h))V , where [T (A)]ij =\n\u001a\n\u03b1Aij/Ci\nif j \u2208 G\u2212\nAij/Ci\notherwise.\n(2)\n3We do not need access model weights nor intermediate outputs from the other modules like FFNs.\n3\nPreprint\nwhere 0 \u2264 \u03b1 < 1 is a scaling coefficient and G\u2212 = [n] \u2212 G is the index set of tokens that are not in\nG. The term Ci = P\nj\u2208G Aij + P\nj\u2208G\u2212 \u03b1Aij normalizes the scores so that they sum to one. The\nattention steering (2) is conducted during the inference time and does not require any training.\n(2) steers the model attention by scaling down the scores of tokens that are not highlighted by the user.\nWhen the coefficient \u03b1 is set very small, user-specified segments are highlighted given their increased\nattention scores after renormalization. Consequently, we can direct the model to concentrate more on\nthe user-specified tokens, biasing the generation to align with the specified contents.\nPASTA scales down the attention scores of non-specified tokens by \u03b1. As renormalization is followed,\nit is equivalent to scaling up the attention scores of user-specified tokens by 1/\u03b1. The reason of\nselecting (2) is that it can be more numerically stable compared to scaling up scores. Alternatively,\none can also scale the attention scores by adding a positive constant to the underlined tokens G.\nThe reason of we select multiplication in (2) instead of addition is that it preserves the difference\non attention magnitude among the highlighted tokens. As such, the steering operation only adjusts\noverall attention scales of two groups of tokens. In contrast, addition by a large constant to the\nhighlighted tokens results in their attention scores almost uniformly distributed, leading to unnecessary\ninformation loss and performance degeneration.\n3.2\nMULTI-TASK MODEL PROFILING\nEmpirically, we find that applying attention steering in (2) to all attention heads performs worse than\napplying it only to specific heads (see Section 5.3). It is important to specify the correct attention\nheads, given that different heads serve distinctive roles in encoding semantic/syntactic information.\nTo this end, we propose a multi-task model profiling algorithm to identify the effective attention heads\nfor steering. Specifically, given m tasks involving user emphases, we subsample a small training set\nD(i) (e.g., |D(i)| = 1000) from each task i. Then, we evaluate the performance of steering every\nindividual attention head (l, h) (1 \u2264 l \u2264 L, 1 \u2264 h \u2264 H) on each small subset D(i) (1 \u2264 i \u2264 m).\nFor every task i, we rank all of heads according to their steering performance on D(i) and regard\nthe ranking R(i) = [(l1, h1), (l2, h2), . . . ] as the profiling of task i. We then set the attention head\nset H for steering as the intersection of top-k performing heads, H = \u2229m\ni=1R(i)\n1:k (see Section 5.3\nfor alternative choices). Intuitively, we expect performance to improve as the number of tasks m\nincreases.\nLike attention steering, model profiling requires only access to attention scores, in addition to its\ninputs and outputs (model weights and gradients are not required). Importantly, this process needs to\nbe performed only once for a LLM, similar to finetuning. However, unlike finetuning, model steering\ndoes not modify model weights and, more importantly, generalizes to new tasks. The resulting head\nset H can be regarded as a model-level profile. Once it is determined, we can apply the attention\nsteering on H to both existing tasks and unseen tasks to enhance model contextual understanding and\nbenefit downstream performance.\n4\nEXPERIMENTAL SETUP\nEvaluation tasks and metrics.\nWe implement PASTA for two pre-trained models: GPT-J (6\nbillion parameters, (Wang & Komatsuzaki, 2021)) and LLaMA-7B (7 billion parameters, (Touvron\net al., 2023)). We evaluate the effectiveness of PASTA at (i) handling complex user instructions,\n(ii) interpreting lengthy contexts, and (iii) resolving in-context knowledge conflicts. For (i), we\nintroduce two new tasks: JSON formatting and Pronouns changing. For (ii) and (iii), we study Bias\nin Bios (De-Arteaga et al., 2019) and CounterFact (Meng et al., 2022a). For each task, we provide a\ndescription, describing which part of the input we emphasize, and what metrics we use for evaluation\n(see Appendix A for full dataset details).\n\u2022 JSON Formatting is a new task that evaluates an LLM\u2019s ability to produce outputs in a user-\ndesired format (JSON). This is an important usecase for LLMs when their output is being used in\na downstream process. This task utilizes the biographical data from BiasBios (described below)\nbut appends a different instruction to the end of contexts: answer the occupation of {person} and\ngenerate the answer as JSON format. The instruction prompts models to generate outputs in JSON\nformat.\n\u0093 We emphasize the final instruction\n4\nPreprint\nMetrics: (a) Format accuracy (F. Acc.) measures the accuracy at generating valid JSON. (b)\nPrediction accuracy (P. Acc.) measures the accuracy at generating the correct target in JSON\nvalues after loading the JSON-formatted generations.\n\u2022 Pronouns changing is a new task that evaluates an LLM\u2019s ability to follow a difficult user instruction.\nIt again uses the biographical contexts from BiasBios but instead instructs models to: substitute \u2018she\u2019\nand \u2018he\u2019 with \u2018they\u2019 and generate the occupation of {person} after changing pronouns.\n\u0093 We emphasize the final instruction.\nMetrics: (a) Accuracy evaluates the ratio that \u2018she/he\u2019 are successfully changed to \u2018they\u2019 in model\ngenerations. (b) All-changed accuracy (A. Acc.) is the ratio that models replace all corresponding\npronouns, i.e., changing she/he/her/him/hers/his to they/them/their/theirs.\n\u2022 CounterFact measures an LLM\u2019s ability to generate text consistent with a new fact. Each example\nconsists of (subject, relation, old target, new target), e.g., (Kevin Garnett, is a professional, basketball\nplayer, baseball player). We present the model both old and new facts following the prompt:\nPreviously, {old fact}, but currently, {new fact}. {question}. This change in facts over time often\nconfuses LLMs, resulting in random guesses on two of them when answering the {question}.\n\u0093 We emphasize the input span containing the new fact.\nMetrics: we evaluate metrics following (Meng et al., 2022a): (a) Efficacy score (ES) is the portion\nof cases for which the model has PLLM(new target) > PLLM(old target); (b) Paraphrase score\n(PS) is the same as ES but changes the {question} with a set of rephrased questions to assess the\ngeneralization\n\u2022 BiasBios consists of professional biographies of non-famous people, originally introduced to\ninvestigate gender bias in occupations. Each example includes biographical context and a label of\ntarget occupation. The first sentence mentions the person\u2019s occupation, and subsequent sentences\ndescribe the individual\u2019s career history but may not be directly related to the prediction, potentially\ndistracting the model attention. At the end of the context, we append the question: {person} has the\noccupation of\n.\n\u0093 We emphasize the first sentence, as it carries the most information about the occupation.\nMetrics: following (Hernandez et al., 2023), we compute Accuracy by checking whether the\nprobability assigned to the target occupation is the highest among the 28 candidate occupations.\nFor Pronouns changing, CounterFact, and BiasBios, we additionally measure Fluency as the average\nbi-gram and tri-gram entropy of generations, designed to be low for degenerated or repetitive\ntexts (Meng et al., 2022a). We filter out any results receiving a fluency below 3.0 (see full results\nincluding fluency in Appendix B).\nBaselines.\nWe compare PASTA to the following baselines:\n\u2022 Zero-shot prompting is the most common approach to interact with LLMs, in which a user feeds\nmodels a prompt containing background context and a user instruction or question.\n\u2022 Marked prompting alters the prompts used in zero-shot prompting by surrounding user-specified\ninput spans with emphasis markers, e.g. asterisks, as is done in markdown files for emphasis, or\nquotes, as is done in natural languages.\n\u2022 Few-shot prompting includes demonstrations (example inputs and target outputs) at the beginning\nof the prompt fed to the LLM. Few-shot prompting often improves performance in new tasks, but\nincreases the computational cost of inference due to the increased prompt length, particularly when\ndemonstrations are lengthy (Dong et al., 2023); here we use 3 demonstrations in context.\nPASTA settings\nWe study PASTA in 2 settings: multi-task and task-agnostic. In the multi-task\nsetting, the evaluation task j is included for profiling, whereas in the task-agnostic setting, the\nevaluation task is excluded (instead, we profile on the 3 datasets besides j). The multi-task setting\nimproves performance but requires labeled training samples for the task which is evaluated, which\ncan be difficult to obtain in practice.\nEmpirically, we find that PASTA is not sensitive to the scaling coefficient \u03b1 (see Section 5.3) and\nfix it to 0.01 in our experiments. We select 1000 training samples from each of the 4 tasks above\nfor model profiling. After model profiling, we select k from {300, 400, 500} for LLAMA-7B\n5\nPreprint\nTable 1: Main results of LLAMA-7B to demonstrate that PASTA can improve the model ability to (i) follow\nuser instruction (JSON Format and Prons. Changing); (ii) interpret contextual information (BiasBios); (iii)\nresolving knowledge conflicts (CounterFact). For all scores, higher is better. The best results are in bold.\nMethod\nJSON Format\nProns. Changing\nBiasBios\nCounterFact\nAll\nF. Acc / P. Acc\nAcc / A.Acc\nAcc\nES / PS\nAve.\nPrompting\nZero-shot\n60.00 / 54.94\n71.84 / 66.28\n87.36\n58.50 / 52.03\n67.29\n\u2217-marked\n18.55 / 12.71\n39.14 / 35.17\n90.62\n57.74 / 50.52\n49.38\n\u201c\u201d-marked\n4.56 / 4.20\n20.55 / 18.19\n89.82\n58.14 / 51.70\n42.15\nFew-shot\n84.85 / 73.58\n59.06 / 55.27\n88.79\n87.45 / 49.82\n73.45\nPASTA\nTask-agnostic\n88.16 / 49.08\n83.65 / 81.31\n93.54\n98.82 / 99.03\n85.89\nMulti-task\n96.64 / 85.09\n96.42 / 95.84\n95.28\n99.60 / 99.57\n95.46\nTable 2: Main results of GPT-J to demonstrate that PASTA can improve the model ability to (i) follow user\ninstruction (JSON Format and Prons. Changing); (ii) interpret contextual information (BiasBios); (iii) resolving\nknowledge conflicts (CounterFact). For all scores, higher is better. The best results are in bold.\nMethod\nJSON Format\nProns. Changing\nBiasBios\nCounterFact\nAll\nF. Acc / P. Acc\nAcc / A.Acc\nAcc\nES / PS\nAve.\nPrompting\nZero-shot\n28.83 / 25.09\n39.88 / 36.19\n72.76\n42.14 / 42.02\n44.96\n\u2217-marked\n4.44 / 4.10\n41.25 / 37.57\n74.14\n44.50 / 45.09\n40.63\n\u201c\u201d-marked\n8.81 / 5.62\n6.12 / 5.72\n78.64\n45.54 / 41.84\n33.87\nFew-shot\n84.15 / 72.65\n35.77 / 32.08\n72.98\n68.34 / 38.23\n59.65\nPASTA\nTask-agnostic\n46.68 / 34.71\n91.62 / 88.60\n80.84\n99.54 / 99.57\n77.80\nMulti-task\n91.50 / 18.63\n92.96 / 91.34\n94.96\n98.62 / 98.79\n85.22\nto have the number of steered heads |H| as {25, 53, 86}. We find that PASTA achieves the best\nperformance on LLAMA-7B when 50 \u2264 |H| \u2264 100, i.e., k = 400 or k = 500. For GPT-J, we select\nk from {250, 275, 300, 350} to have |H| as {52, 72, 111, 153}. For every task, we split data into\ntrain/validation/test sets following (Hernandez et al., 2023) (See Appendix A) and select |H| by cross\nvalidation. For all tasks, model outputs are generated with greedy search.\n5\nRESULTS\n5.1\nMAIN RESULT: PASTA IMPROVES MODEL GENERATION\nTables 1 and 2 present the main results for PASTA applied to LLAMA-7B and GPT-J respectively.\nFew-shot prompting is the strongest baseline, and task-agnostic PASTA outperforms it on the\nmain metric for each task for all settings except JSON Formatting with GPT-J. Multi-task PASTA\noutperforms all baselines across all settings.\nPASTA can improve LLM instruction following. The results from JSON Formatting and Pronouns\nChanging tasks indicate that, by highlighting the user instruction at the end of inputs, PASTA\neffectively steers models to focus on user intentions, thereby biasing their generation to fulfill specific\nrequirements or formats. For example, while GPT-J only achieves 39.9% of its zero-shot generations\ncomplying the user requirement on the Pronouns Changing task, PASTA yields a remarkable 53%\naccuracy improvement by emphasizing the instruction. Moreover, PASTA achieves an impressive\n96.64% format accuracy and 85.09% prediction accuracy when applied to LLAMA-7B on the\nJSON Formatting task. This performance exceeds that of few-shot prompting by 11%, even though\nfew-shot prompting explicitly provides the model with correct JSON examples through additional\ndemonstrations. Table 3 presents a few examples generated by LLAMA-7B when applying PASTA.\nPASTA can help models capture crucial contextual information. In the case of BiasBios and\nCounterFact tasks, we apply PASTA to emphasize specific context spans for LLMs. Consequently,\nthe models are guided to pay close attention to the specific contextual information or new facts within\ncontexts. The results from these two tasks illustrate that PASTA can direct the models to interpret\n6\nPreprint\nTable 3: Generation examples of LLAMA-7B on JSON Formatting and Pronouns Changing tasks.\nTask\nPrompt\nZero-shot Generation\nPASTA Generation\nJSON\nFormat\nWinnie is an American photographer\nliving in New York. Specialized in fash-\nion photography and portrait, she ap-\nplies her talent on both humans and ani-\nmals ...{instruction}\n\u2717\nWinnie is a fashion and portrait pho-\ntographer living in New York, special-\nized in fashion photography and portrait,\nwho applies her talent on both humans\nand animals.\n\u2713\n{\u201cname\u201d: \u201cWinnie\u201d, \u201coccupation\u201d:\n\u201cphotographer\u201d }\nProns.\nChange\nMary is an Associate Professor in the\nDepartment of Curriculum Instruction\nat St. John University; she holds a doc-\ntorate in Reading/Writing/Literacy from\nthe University of Pennsylvania. She re-\ncently published an article in \u201cUrban\nEducation\u201d with ...{instruction}\n\u2717\nMary is a professor in the Depart-\nment of Curriculum Instruction at St.\nJohn University; she holds a doctor-\nate in Reading/Writing/Literacy from\nthe University of Pennsylvania. She re-\ncently published an article in \u201cUrban\nEducation\u201d with . . .\n\u2713\nMary\nis\nan\nassociate\nprofes-\nsor in the department of curriculum\nand instruction at St. John\u2019s Univer-\nsity; they hold a doctorate in read-\ning/writing/literacy from the University\nof Pennsylvania. They recently pub-\nlished an article in . . .\nthe crucial information or resolve the knowledge conflicts within contexts, yielding significant\nimprovement in prediction performance of both tasks. For example, PASTA achieves a prediction\naccuracy of 94.96% for GPT-J on the BiasBios task, which is 16.32% higher than the best baseline.\nTables 1 and 2 also suggest that marked prompting, a baseline that highlights specific texts akin\nto human writers, struggles to effectively convey emphasis to LLMs. One possible reason is that\nthese emphasis markers rarely appear in the massive pre-training data. In contrast, few-shot prompt-\ning sometimes leads to improvements in model performance. However, a drawback of few-shot\nprompting is its instability, i.e. its performance exhibits high variance across different samples in the\ndemonstration (See Appendix C).\n5.2\nPASTA CAN MITIGATE THE SENSITIVITY OF PROMPTS\nTable 4: Results about sensitivity of model performance to prompt rephrasing on the JSON Formatting task.\nGiven rephrased instructions in prompt template, PASTA can imporve zero-shot performance for all prompts.\nInstruction\nMethod\nLLAMA-7B\nGPT-J\nAverage\nJSON Format\nF. Acc / P. Acc\nProns. Changing\nAcc / A. Acc\nJSON Format\nF. Acc / P. Acc\nProns. Changing\nAcc / A. Acc\nOriginal\nZero-shot\n60.0 / 54.9\n71.8 / 66.3\n28.8 / 25.1\n39.9 / 36.2\n47.9\nPASTA\n96.6 / 85.1\n96.4 / 95.8\n91.5 / 18.6\n93.0 / 91.3\n83.5\nShortened\nZero-shot\n36.0 / 32.4\n49.2 / 42.6\n25.4 / 17.1\n56.5 / 54.8\n39.3\nPASTA\n87.4 / 65.9\n89.0 / 86.9\n54.1 / 37.0\n94.0 / 93.7\n76.0\nRephrased\nZero-shot\n57.9 / 54.2\n82.3 / 79.6\n63.3 / 50.3\n76.0 / 72.8\n67.1\nPASTA\n97.1 / 87.1\n89.6 / 89.0\n77.5 / 68.1\n94.8 / 92.3\n86.9\nIt is well-known that the the performance of LLMs can be sensitive to minor changes in prompts,\nsuch as rephrasing and reformatting, even when these prompts convey the same meaning (Reynolds\n& McDonell, 2021; Liu et al., 2021). We find that PASTA can alleviate the sensitivity of model\nperformance to varying prompts. Specifically, Table 4 evaluates the performance of LLAMA-7B and\nGPT-J on JSON Formatting and Pronouns Changing task given different instructions in the prompt\ntemplate, all of which convey the same meaning (see precise prompts in Appendix A.1). The results\nshow that zero-shot performance is sensitive to different prompts and can significantly deteriorate\nwith poorly crafted templates. In contrast, PASTA consistently improves model performance over\nzero-shot prompting for all prompts, effectively mitigating sensitivity to variations in the prompts.\n5.3\nANALYSIS AND ABLATIONS\nIn this section, we investigate different hyperparameter choices and modeling decisions that affect\nthe performance of PASTA.\nModel profiling\nFigure 2 presents the results on the importance of model profiling introduced in\nSection 3.2. We compare PASTA when steering the selected heads versus other reasonable choices:\nsteering (i) all heads, (ii) entire layers, or (iii) individual heads on the JSON Formatting task (See\nAppendix C.2 for comparisons on the remaining tasks). Selecting heads via model profiling in\n7\nPreprint\n1\n4\n8\n12\n16\n20\n24\n28\n32\nLayer\n0\n25\n50\n75\n100\nF. Acc\nZero-shot\nPASTA\nSteer all heads\nSteer entire layer\nSteer single head\nFigure 2: The performance of LLAMA-7B on the JSON Formatting task when we steer (i) all heads\n(green); (ii) an entire layer (yellow); and (iii) an individual head within a layer (blue violin plot). The\nperformance varies dramatically across layers and across heads of a layer.\nPASTA (red line) significantly outperforms other approaches. Steering all heads (dashed green line)\ndegrades performance compared to the baseline zero-shot performance (dashed black line). This\nis likely because steering all heads over-amplifies the user-specified information at the expense of\nother essential information required for effective generation and prediction. Interestingly, we find\nthat the performance varies significantly when steering different layers (yellow) or heads (blue violin\nplot). As mentioned in Section 1, attention heads play distinct roles in encoding diverse semantic and\nsyntactic information (Tenney et al., 2019). When steering heads, which are appropriately involved\nin encoding of user-specified information, the model can be guided to capture and reinforce these\nspecific signals. Conversely, modifying the attention of unrelated heads not only fails to emphasize\nthe desired information but also interferes with their original functions, resulting in performance\ndeterioration. Therefore, it is important to identify the effective heads through model profiling prior\nto applying the steering.\nVarying strategies for selecting heads during profiling. As described in Sec. 5.3, our model profil-\ning selects the Intersection of the top-k performing heads to steer across multiple tasks. Alternatively,\nwhen evaluating on task j, we can select heads for steering with different strategies: (i) Task-specific \u2013\nsteer the top-k2 performing heads of only the task j, i.e., R(j)\n1:k2; or (ii) Union \u2013 the union of these\nheads across multiple tasks, i.e., \u222am\ni=1R(i)\n1:k2. Table 5 compares their performance. Using task-specific\nheads rather than intersection-selected heads sometimes yields improved performance, but requires\nselecting a different set of heads for each new task.\nTable 5: Varying head selection strategies between top task-specific heads, union across multiple\ntasks, and intersection (the default used in PASTA).\nPASTA\nJSON Format\nProns. Changing\nBiasBios\nCounterFact\nAll\nF. Acc / P. Acc\nAcc / A.Acc\nAcc\nES / PS\nAvg.\nLLAMA\nTask-specific\n95.56 / 86.83\n98.52 / 98.02\n97.62\n99.18 / 99.24\n96.57\nUnion\n88.42 / 74.49\n92.12 / 91.44\n96.36\n99.24 / 99.35\n92.22\nIntersection\n96.64 / 85.09\n96.42 / 95.84\n95.28\n99.60 / 99.57\n95.46\nGPT-J\nTask-specific\n85.71 / 79.39\n94.74 / 92.54\n97.64\n99.26 / 99.34\n93.29\nUnion\n72.61 / 64.89\n89.68 / 87.76\n95.56\n99.82 / 99.83\n88.21\nIntersection\n91.50 / 18.63\n92.96 / 91.34\n94.96\n98.62 / 98.79\n85.22\nVarying the number of heads to be steered. Figures 3a and 3b illustrate the performance of PASTA\nwhen steering different number of heads on two tasks. The results suggest that as more heads are\nincluded for steering, the model follows the user even more closely, achieving higher efficacy (JSON\nFormat Acc. and Pron. Change Acc.). However, at some point, this it results in a decrease in the\nmetrics reflecting the generation quality (JSON Pred. Acc and Fluency). Thus, there is a trade-off\nbetween emphasizing efficacy and generation quality, requiring choosing the number of heads during\nmodel profiling.\nVarying the scaling coefficient \u03b1.\nFigure 3c presents the performance of PASTA on two tasks\nwhen we change the scaling coefficient \u03b1. The results indicate that PASTA is fairly robust to this\nhyperparameter; in practice, we fix it as 0.01. Notice that setting \u03b1 to zero should be avoided, as this\n8\nPreprint\n2\n10\n25\n53\n86\n168\n294\n# Heads\n75\n80\n85\n90\n95\n100\nJSON Format Acc.\n0\n20\n40\n60\n80\nJSON Pred. Acc.\nJSON Format Acc.\nJSON Pred. Acc.\n(a) JSON Format\n2\n10\n25\n53\n86\n168\n294\n# Heads\n80\n85\n90\n95\n100\nPron. Change Acc.\n2\n3\n4\n5\n6\nFluency\nPron. Change Acc.\nFluency\n(b) Prons. Change\n0.05\n0.01 0.002 10\u22123 10\u22124\n0\n\u03b1\n20\n40\n60\n80\n100\nJSON Format Acc.\n80\n85\n90\n95\n100\nPron. Change Acc.\nJSON Format Acc.\nPron. Change Acc.\n(c) Varying \u03b1\nFigure 3: The performance of applying PASTA to LLAMA-7B on JSON Formating and Pronouns Changing\ntasks when varying the number of steered heads |H| (3a,3b); and changing the scaling coefficient \u03b1 (3c).\nleads to the complete removal of other crucial contexts at the steered heads, resulting in performance\ndegeneration.\n6\nRELATED WORK\nThe primary method for controlling LLMs has been through prompting, often yielding impressive\nimprovements in performance (Brown et al., 2020b; Liu et al., 2021; Wei et al., 2022) and spurring a\nline of work aiming to make prompting easier, e.g. (Strobelt et al., 2022; Bach et al., 2022; Shin et al.,\n2020; Deng et al., 2022; Singh et al., 2023b). However, LLMs remain extremely sensitive to nuances\nin prompts (Webson & Pavlick, 2021; Lu et al., 2021); PASTA complements these approaches by\nmaking it easier for a user to specify a prompt in difficult scenarios.\nAnother line of work aims to make LLMs more amenable to prompting by modifying them during\ntraining. Most prominent among these approaches are instruction finetuning (Wei et al., 2021; Chung\net al., 2022), Reinforcement Learning from Human Feedback (Ziegler et al., 2019; Ouyang et al.,\n2022), and other related methods, e.g. (Lee et al., 2023). There are also a few methods for directly\nspecifying which parts on an input are important during training, e.g. (Ross et al., 2017; Rieger\net al., 2019; Schramowski et al., 2020; Krishna et al., 2023). PASTA can be used in addition to these\napproaches to improve some aspects of model steerability (e.g. instruction following).\nPASTA is related to variety of methods for adapting to new tasks, including LoRA (Hu et al., 2021a),\nAdaLoRA (Zhang et al., 2023), QLoRA (Dettmers et al., 2023), and TOAST (Shi et al., 2023b).\nPASTA is also related to a variety of research on model editing, e.g. ROME (Meng et al., 2022a),\nMEMIT (Meng et al., 2022b), MEND (Mitchell et al., 2022), and REMEDI (Hernandez et al., 2023).\nUnlike these works, PASTA preserves an LLMs ability to transfer to new tasks using prompts and\nhuman-selected info, rather than using new labeled examples.\nFinally, PASTA is also motivated by works which have aimed to mechanistically understand attention\nscores (Zou et al., 2023), e.g. by studying them through feature importance (Jain & Wallace, 2019;\nWiegreffe & Pinter, 2019; Deb et al., 2023), through probing (Conneau et al., 2018; Liu & Avci,\n2019), through visualization (Karpathy et al., 2015; Olah et al., 2017), localizing knowledge (Meng\net al., 2022a; Dai et al., 2021), categorizing directions in representation space (Kim et al., 2017;\nSchwettmann et al., 2021), or through natural-language explanations (Bills et al., 2023; Singh et al.,\n2023a).\n7\nCONCLUSION\nIn this study, we propose PASTA, a novel approach aimed at enabling LLMs to move beyond the\nlimitations of plain text and effectively perceive user guidance embodied as highlighted parts of\nprompts. By making precise adjustments to attention scores in selected heads, PASTA directs the\nmodel\u2019s focus to the relevant context, mirroring the way humans benefit from textual cues. Unlike\ntraditional fine-tuning methods, PASTA is applied at inference time and requires neither parameter\nupdates nor gradient computation; PASTA requires only selecting which attention heads to apply the\nre-weighting to, a one-time profiling operation for a LLM. Experimental results show that PASTA\ncan significantly improve model performance on a variety of tasks. In the future, we plan to integrate\nPASTA with various other methods, such as few-shot in-context learning, aiming to highlight effective\nexamples to enhance its stability.\n9\nPreprint\nREFERENCES\nStephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma,\nTaewoon Kim, M Saiful Bari, Thibault Fevry, et al. Promptsource: An integrated development environment\nand repository for natural language prompts. arXiv preprint arXiv:2202.01279, 2022.\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan\nLeike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. URL\nhttps://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05.\n2023), 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\nChris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-\npher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\nLanguage models\nare few-shot learners.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.),\nAdvances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates,\nInc., 2020a. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020b.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\narXiv:2210.11416, 2022.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an\nanalysis of BERT\u2019s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Inter-\npreting Neural Networks for NLP, pp. 276\u2013286, Florence, Italy, August 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/W19-4828. URL https://aclanthology.org/W19-4828.\nAlexis Conneau, German Kruszewski, Guillaume Lample, Lo\u00a8\u0131c Barrault, and Marco Baroni. What you can cram\ninto a single vector: Probing sentence embeddings for linguistic properties. arXiv preprint arXiv:1805.01070,\n2018.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained\ntransformers. arXiv preprint arXiv:2104.08696, 2021.\nMaria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Choulde-\nchova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in bios: A case study of semantic\nrepresentation bias in a high-stakes setting. In proceedings of the Conference on Fairness, Accountability,\nand Transparency, pp. 120\u2013128, 2019.\nMayukh Deb, Bj\u00a8orn Deiseroth, Samuel Weinbach, Patrick Schramowski, and Kristian Kersting. Atman:\nUnderstanding transformer predictions through memory efficient attention manipulation. arXiv preprint\narXiv:2301.08110, 2023.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing,\nand Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint\narXiv:2205.12548, 2022.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized\nllms, 2023.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and\nZhifang Sui. A survey on in-context learning, 2023.\nEvan Hernandez, Belinda Z. Li, and Jacob Andreas. Inspecting and editing knowledge representations in\nlanguage models, 2023.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021a.\nJ. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv preprint abs:2106.09685, 2021b.\n10\nPreprint\nSarthak Jain and Byron C Wallace. Attention is not explanation. arXiv preprint arXiv:1902.10186, 2019.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. arXiv\npreprint arXiv:1506.02078, 2015.\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory Sayres.\nInterpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). arXiv\npreprint arXiv:1711.11279, 2017.\nSatyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju.\nPost hoc explanations of language models can improve language models. arXiv preprint arXiv:2305.11426,\n2023.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune,\nand Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv\npreprint arXiv:2309.00267, 2023.\nFrederick Liu and Besim Avci. Incorporating priors with feature attribution on text classification. arXiv preprint\narXiv:1906.08286, 2019.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\nLost in the middle: How language models use long contexts, 2023.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt,\nand predict: A systematic survey of prompting methods in natural language processing. arXiv preprint\narXiv:2107.13586, 2021.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts\nand where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786,\n2021.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in\ngpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022a.\nKevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a\ntransformer. arXiv preprint arXiv:2210.07229, 2022b.\nPaul Michel,\nOmer Levy,\nand Graham Neubig.\nAre sixteen heads really better than one?\nIn\nH.\nWallach,\nH.\nLarochelle,\nA.\nBeygelzimer,\nF.\nd'Alch\u00b4e-Buc,\nE.\nFox,\nand\nR.\nGarnett\n(eds.),\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems,\nvolume\n32.\nCurran\nAssociates,\nInc., 2019.\nURL https://proceedings.neurips.cc/paper_files/paper/2019/file/\n2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model editing at\nscale, 2022.\nChris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2(11):e7, 2017.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00a8opf, Edward Yang, Zachary\nDeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and\nSoumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M.\nWallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00b4e-Buc, Emily B. Fox, and Roman Garnett\n(eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024\u20138035,\n2019.\nLaria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot\nparadigm, 2021.\nLaura Rieger, Chandan Singh, W James Murdoch, and Bin Yu. Interpretations are useful: penalizing explanations\nto align neural networks with prior knowledge. arXiv preprint arXiv:1909.13584, 2019.\n11\nPreprint\nAndrew Slavin Ross, Michael C Hughes, and Finale Doshi-Velez. Right for the right reasons: Training\ndifferentiable models by constraining their explanations. arXiv preprint arXiv:1703.03717, 2017.\nPatrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Franziska Herbert, Xiaoting Shao,\nHans-Georg Luigs, Anne-Katrin Mahlein, and Kristian Kersting. Making deep neural networks right for the\nright scientific reasons by interacting with their explanations. Nature Machine Intelligence, 2(8):476\u2013486,\n2020.\nSarah Schwettmann, Evan Hernandez, David Bau, Samuel Klein, Jacob Andreas, and Antonio Torralba. Toward\na visual concept vocabulary for gan latent space. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 6804\u20136812, 2021.\nTianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and\nDeyi Xiong. Large language model alignment: A survey, 2023.\nBaifeng Shi, Siyu Gai, Trevor Darrell, and Xin Wang. Toast: Transfer learning via attention steering. arXiv\npreprint abs:2305.15542, 2023a.\nBaifeng Shi, Siyu Gai, Trevor Darrell, and Xin Wang. Refocusing is key to transfer learning. arXiv preprint\narXiv:2305.15542, 2023b.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting\nknowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980,\n2020.\nChandan Singh, Aliyah R Hsu, Richard Antonello, Shailee Jain, Alexander G Huth, Bin Yu, and Jianfeng Gao. Ex-\nplaining black box text modules in natural language with language models. arXiv preprint arXiv:2305.09863,\n2023a.\nChandan Singh, John X. Morris, Jyoti Aneja, Alexander M. Rush, and Jianfeng Gao. Explaining patterns in data\nwith language models via interpretable autoprompting, 2023b.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario\nAmodei, and Paul Christiano. Learning to summarize from human feedback. arXiv preprint abs:2009.01325,\n2020.\nHendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer, Hanspeter Pfister, and\nAlexander M. Rush. Interactive and visual prompt engineering for ad-hoc task adaptation with large language\nmodels, 2022.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4593\u20134601, Florence,\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1452. URL https:\n//aclanthology.org/P19-1452.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat\nmodels. arXiv preprint arXiv:2307.09288, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_\nfiles/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention:\nSpecialized heads do the heavy lifting, the rest can be pruned, July 2019. URL https://aclanthology.\norg/P19-1580.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:\n//github.com/kingoflolz/mesh-transformer-jax, May 2021.\nAlbert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts?\narXiv preprint arXiv:2109.01247, 2021.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,\nand Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n12\nPreprint\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information\nProcessing Systems, 35:24824\u201324837, 2022.\nSarah Wiegreffe and Yuval Pinter. Attention is not not explanation. arXiv preprint arXiv:1908.04626, 2019.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural\nlanguage processing. arXiv preprint arXiv:1910.03771, 2019.\nZhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad\nAwan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly\nSmith, L A Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, and Yuxiong He.\nDeepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales. arXiv preprint\nabs:2308.01320, 2023.\nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.\nAdaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on\nLearning Representations, 2023. URL https://openreview.net/forum?id=lq62uWRJjiY.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and\nGeoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593,\n2019.\nAndy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin,\nMantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang,\nAlex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks.\nRepresentation engineering: A top-down approach to ai transparency, 2023.\n13\nPreprint\nAPPENDIX\nA\nEXPERIMENTAL DETAILS\nWe implement all algorithms using PyTorch (Paszke et al., 2019) and Huggingface (Wolf et al.,\n2019) and run experiments on NVIDIA V100 GPUs and NVIDIA A6000 GPUs.\nTable 6 provides detailed statistics of datasets in our experiments.\nTable 6: Statistics of datasets.\nTask\nTrain\nValid\nTest\nCounterFact\n1000\n1000\n5000\nBiasBios\n1000\n1000\n5000\nJSON Formatting\n1000\n1000\n5000\nPronouns Changing\n1000\n1000\n5000\nA.1\nDETAILED PROMPT TEMPLATES OF EACH TASK\nFor each task, the prompt templates in our results are as follows:\n\u2022 JSON Formatting:\n\u2013 (Original) {context}. Answer the occupation of {person} and generate the answer as\njson format. Here is an example: {\u201cname\u201d: , \u201coccupation\u201d: ,}. Now generate the\nanswer.\n\u2013 (Shortened one in Section 5.2) {context}. Answer the occupation of {person} and\ngenerate the answer as json format.\n\u2013 (Rephrased one in Section 5.2) Answer the occupation of {person} and generate the\nanswer as json format. Here is an example: {\u201cname\u201d: , \u201coccupation\u201d: ,}. {context}.\nNow generate the answer.\n\u2022 Pronouns Changing:\n\u2013 (Original): {context}. For the aforementioned text, substitute \u2018she\u2019 and \u2018he\u2019 with \u2018they\u2019\nand generate the occupation of {person} after changing pronouns.\n\u2013 (Shortened one in Section 5.2): {context}. Change \u2018she\u2019 and \u2018he\u2019 with \u2018they\u2019 and\nanswer the occupation of {person} after replacing the pronouns\n\u2013 (Rephrased one in Section 5.2): {context}. For the aforementioned descriptions,\nreplace \u2018she\u2019 and \u2018he\u2019 with \u2018they\u2019 in the aformentioned text and generate the new text\nafter replacing the pronouns.\n\u2022 BiasBios: {context}. {person} has the occupation of.\n\u2022 CounterFact: Previously, {old fact}. Currently, {new fact}. {question}\nA.2\nTHE EVALUATION DETAILS OF PASTA\nTable 7 presents the number of heads to be steered by PASTA for LLAMA-7B and GPT-J-6B on\nevery task.\nTable 7: The number of heads to be steered by PASTA.\nTask\nLLAMA-7B\nGPT-J-6B\nJSON Formatting\n53\n153\nPronouns Changing\n86\n72\nBiasBios\n86\n111\nCounterFact\n86\n52\n14\nPreprint\nB\nEXTENDED RESULTS WITH FLUENCY\nIn this section, we include extended results, including fluency metrics. Fluency score is the average bi-\ngram and tri-gram entropy of generations, designed to be low for degenerated or repetitive texts (Meng\net al., 2022a). This metric can be regarded as the reference metric of generation quality. Typically,\nthe generations of language models are reliable as long as their fluency score is not too low. Here, we\nfilter out any results receiving a fluency score below 3.0. Table 8, 9 and 10 include all results and\nfluency evaluation.\nTable 8: Main results of LLAMA-7B to demonstrate that PASTA can improve the model ability to (i) follow\nuser instruction (JSON Format and Prons. Changing); (ii) interpret contextual information (BiasBios); (iii)\nresolving knowledge conflicts (CounterFact). For all scores, higher is better. The best results are in bold.\nMethod\nJSON Format\nProns. Changing\nBiasBios\nCounterFact\nF. Acc / P. Acc\nAcc / A.Acc / Flue.\nAcc / Flue.\nES / PS /Flue.\nPrompting\nZero-shot\n60.00 / 54.94\n71.84 / 66.28 / 6.10\n87.36 / 3.98\n58.50 / 52.03 / 4.96\n\u2217-marked\n18.55 / 12.71\n39.14 / 35.17 / 6.03\n90.62 / 3.89\n57.74 / 50.52 / 5.12\n\u201c\u201d-marked\n4.56 / 4.20\n20.55 / 18.19 / 5.13\n89.82 / 3.97\n58.14 / 51.70 / 5.13\nFew-shot\n84.85 / 73.58\n59.06 / 55.27 / 5.95\n88.79 / 4.19\n87.45 / 49.82 / 5.68\nPASTA\nTask-agnostic\n88.16 / 49.08\n83.65 / 81.31 / 4.62\n93.54 / 3.03\n98.82 / 99.03 / 4.78\nMulti-task\n96.64 / 85.09\n96.42 / 95.84 / 5.43\n95.28 / 4.05\n99.60 / 99.57 / 4.89\nTable 9: Main results of GPT-J to demonstrate that PASTA can improve the model ability to (i) follow user\ninstruction (JSON Format and Prons. Changing); (ii) interpret contextual information (BiasBios); (iii) resolving\nknowledge conflicts (CounterFact). For all scores, higher is better. The best results are in bold.\nMethod\nJSON Format\nProns. Changing\nBiasBios\nCounterFact\nF. Acc / P. Acc\nAcc / A.Acc / Flue.\nAcc / Flue.\nES / PS /Flue.\nPrompting\nZero-shot\n28.83 / 25.09\n39.88 / 36.19 / 5.91\n72.76 / 5.06\n42.14 / 42.02 / 5.01\n\u2217-marked\n4.44 / 4.10\n41.25 / 37.57 / 4.76\n74.14 / 5.01\n44.50 / 45.09 / 5.22\n\u201c\u201d-marked\n8.81 / 5.62\n6.12 / 5.72 / 5.43\n78.64 / 4.96\n45.54 / 41.84 / 5.16\nFew-shot\n84.15 / 72.65\n35.77 / 32.08 / 6.46\n72.98 / 4.82\n68.34 / 38.23 / 5.67\nPASTA\nTask-agnostic\n46.68 / 34.71\n91.62 / 88.60 / 3.00\n80.84 / 4.92\n99.54 / 99.57 / 5.11\nMulti-task\n91.50 / 18.63\n92.96 / 91.34 / 4.91\n94.96 / 4.87\n98.62 / 98.79 / 5.11\nTable 10: Varying head selection strategies between top top task-specific heads, union across multiple\ntasks, and intersection (the default used in PASTA).\nPASTA\nJSON Format\nProns. Changing\nBiasBios\nCounterFact\nF. Acc / P. Acc\nAcc / A.Acc / Flue.\nAcc / Flue.\nES / PS /Flue.\nLLAMA\nTask-specific\n95.56 / 86.83\n98.52 / 98.02 / 5.92\n97.62 / 4.18\n99.18 / 99.24 / 4.93\nunion\n88.42 / 74.49\n92.12 / 91.44 / 4.88\n96.36 / 4.13\n99.24 / 99.35 / 4.53\nintersection\n96.64 / 85.09\n96.42 / 95.84 / 5.43\n95.28 / 4.05\n99.60 / 99.57 / 4.89\nGPT-J\nTask-specific\n85.71 / 79.39\n94.74 / 92.54 / 5.07\n97.64 / 5.06\n99.26 / 99.34 / 4.94\nUnion\n72.61 / 64.89\n89.68 / 87.76 / 3.92\n95.56 / 5.02\n99.82 / 99.83 / 5.03\nIntersection\n91.50 / 18.63\n92.96 / 91.34 / 4.91\n94.96 / 4.87\n98.62 / 98.79 / 5.11\nC\nEXTENDED RESULTS\nC.1\nTHE VARIANCE OF FEW-SHOT PERFORMANCE\nFew-shot prompting sometimes leads to improvements in model performance. as explicitly providing\nthe examples in additional demonstrations. However, a drawback of few-shot prompting is its insta-\n15\nPreprint\nbility, meaning its performance exhibits high variance across different samples in the demonstratio.\nIn this section, we present the results to show that the performance of few-shot prompting displays\nhigh variance in terms of sampling different few-shot demonstrations.\nTable 11: The few-shot performance (Acc. / A. Acc. / Fluency) on the Pronouns Changing task.\nFew-shot examples\nLLAMA-7B\nGPT-J-6B\nDemonstration 1\n84.87 / 90.09 / 4.74\n43.82 / 40.36 / 6.43\nDemonstration 2\n57.24 / 53.98 / 6.22\n40.68 / 37.86 / 6.44\nDemonstration 3\n57.08 / 53.22 / 6.02\n33.13 / 29.21 / 6.48\nDemonstration 4\n52.26 / 48.30 / 6.42\n25.47 / 20.89 / 6.44\nDemonstration 5\n43.86 / 40.78 / 6.43\n11.90 / 8.63 / 6.51\nC.2\nMODEL PROFILING RESULTS\nIn this Section, we provide more results of the performance of LLAMA-7B on all of tasks when\nsteering: (i) all heads; (ii) entire layer; (iii) a individual head of a layer.\n1\n4\n8\n12\n16\n20\n24\n28\n32\nLayer\n20\n40\n60\n80\n100\nAcc\nZero-shot\nPASTA\nSteer all heads\nSteer entire layer\nSteer single head\nFigure 4: The performance of LLAMA-7B on Pronouns Changing task when we steer (i) all heads (green); (ii)\nentrie layer (yellow); and (iii) individual head with a layer (blue violin plot). The performance varies dramatically\nacross layers and across heads of a layer.\n1\n4\n8\n12\n16\n20\n24\n28\n32\nLayer\n60\n70\n80\n90\nAcc\nZero-shot\nPASTA\nSteer all heads\nSteer entire layer\nSteer single head\nFigure 5: The performance of LLAMA-7B on BiasBios task when we steer (i) all heads (green); (ii) entrie\nlayer (yellow); and (iii) individual head with a layer (blue violin plot). The performance varies dramatically\nacross layers and across heads of a layer.\n1\n4\n8\n12\n16\n20\n24\n28\n32\nLayer\n40\n60\n80\n100\nES\nZero-shot\nPASTA\nSteer all heads\nSteer entire layer\nSteer single head\nFigure 6: The performance of LLAMA-7B on CounterFact task when we steer (i) all heads (green); (ii) entrie\nlayer (yellow); and (iii) individual head with a layer (blue violin plot). The performance varies dramatically\nacross layers and across heads of a layer.\n16\n"
  },
  {
    "title": "LDM3D-VR: Latent Diffusion Model for 3D VR",
    "link": "https://arxiv.org/pdf/2311.03226.pdf",
    "upvote": "7",
    "text": "LDM3D-VR: Latent Diffusion Model for 3D VR\nGabriela Ben Melech Stan\nDiana Wofk\nEstelle Aflalo\nShao-Yen Tseng\nZhipeng Cai\nMichael Paulitsch\nVasudev Lal\nIntel Labs\n{gabriela.ben.melech.stan, diana.wofk, estelle.aflalo,\nshao-yen.tseng, zhipeng.cai, michael.paulitsch, vasudev.lal}@intel.com\nAbstract\nLatent diffusion models have proven to be state-of-the-art in the creation and\nmanipulation of visual outputs. However, as far as we know, the generation of\ndepth maps jointly with RGB is still limited. We introduce LDM3D-VR, a suite of\ndiffusion models targeting virtual reality development that includes LDM3D-pano\nand LDM3D-SR. These models enable the generation of panoramic RGBD based\non textual prompts and the upscaling of low-resolution inputs to high-resolution\nRGBD, respectively. Our models are fine-tuned from existing pretrained models\non datasets containing panoramic/high-resolution RGB images, depth maps and\ncaptions. Both models are evaluated in comparison to existing related methods.\n1\nIntroduction\nDiffusion models have brought about a shift in content creation by offering accessible models that\ngenerate RGB images from text prompts or produce high-resolution RGB images from low-resolution\ninputs. However, the generation of depth maps jointly with RGB images, often required for virtual\nreality (VR) content development, still presents a challenge. Panoramas created using conventional\nimage stitching algorithms exhibit certain drawbacks, such as artifacts and irregular shapes.\nLDM3D-VR builds upon the Latent Diffusion Model for 3D (LDM3D) [39] and explores RGBD\ngeneration for panoramic views. We also create a super-resolution model based on LDM3D that\njointly upscales an image alongside its corresponding depth map. To summarize, our contributions are\nthe following: (i) we introduce LDM3D-pano, which addresses the challenge of generating panorama\nviews jointly with their corresponding depth maps based on a text prompt (ii) we introduce LDM3D-\nSR that performs x4 upscaling and recovers high-resolution RGB and depth maps from low-resolution\ninputs (iii) we showcase this work through a demo accessible at https://huggingface.co/\nspaces/Intel/ldm3d. LDM3D-pano and LDM3d-SR are available at https://huggingface.\nco/Intel.\n2\nRelated work\nText-to-panorama. Text-to-panorama is an important task for creating VR environments. Early\napproaches achieve this goal through Generative Adversarial Networks (GANs) [24, 6]. Recent\nadvances in diffusion models have improved training stability and model generalization capacity\ncompared to GAN, enabling text-to-panorama works [3, 40, 18]. Some of these methods [40, 3]\nonly cover the left right rotations of panorama, i.e., without top down views. Others [18] cannot\ngenerate realistic panoramas due to the lack of training data. We propose LDM3D-pano, a novel\ndiffusion-based approach capable of producing, from an input text prompt, a realistic RGB panorama\nand its corresponding panorama depth map.\nSuper-resolution for images. Learning-based super-resolution (SR) has been extensively studied\nin the past decade. Following [10], initial methods adopted convolution neural networks (CNN)\nNeural Information Processing Systems (NeurIPS) 2023 Workshop on Diffusion Models.\narXiv:2311.03226v1  [cs.CV]  6 Nov 2023\nand proposed various methods to improve reconstruction quality [17, 16, 50, 23, 49]. GANs were\nlater introduced, which led to higher fidelity SR images [19, 42, 41]. Subsequent approaches then\nimproved SR performance through the use of attention [49, 8, 27] and transformer-based architectures\n[22, 26, 43, 25, 5]. Most recently, denoising diffusion probabilistic models [14] have demonstrated\nproficiency in image generation [29, 33] as well as image upscaling [35, 38, 12].\nSuper-resolution for depth. Enhancing the resolution of a depth map is also a widely-studied\nproblem. Naive pixel-level interpolations often yield noisy floating points at object boundaries.\nLearning-based approaches [44, 21, 13, 45, 51, 1] have emerged as promising alternatives.\nIn this work, we propose LDM3D-SR, a latent diffusion-based super-resolution model that can\nenhance the resolution of RGB and depth maps within the same architecture.\n3\nMethodology\n3.1\nLDM3D-pano\nLDM3D-pano extends LDM3D [39] to panoramic image generation. Key changes to the architecture\ninclude adjustments to the first and last Conv2d layers of the KL-autoencoder [11], enabling it to\nprocess a 4-channel input consisting of RGB concatenated with a single-channel depth map; we\ndenote this model as LDM3D-4c. The employed diffusion model is based on U-Net [32] operating in\na 64x128x4 latent space, following [31], with the incorporation of a CLIP text encoder [28] for text\nconditioning through cross-attention mapping on the U-Net layers.\nWe adopt a two-stage fine-tuning procedure, following [29, 39]. We first fine-tune the refined version\nof the KL-autoencoder in LDM3D-4c, using roughly 10k samples of size 256x256 sourced from\nLAION-400M [37], with depth map labels produced using the DPT-BEiT-L-512 [4].\nSubsequently, the U-Net backbone is fine-tuned based on Stable Diffusion (SD) v1.5 [29], employing\na subset of LAION Aesthetics 6+ [36] consisting of nearly 20k tuples (captions, 512x512-sized\nimages and depth maps produced using DPT-BEiT-L-512 [4]). We further fine-tune the U-Net on our\npanoramic dataset, comprised of High Dynamic Range (HDR) images\u2013originally in 4k resolution\u2013\nsourced from [47], [15]. These HDRIs are augmented into 512x1024 panoramic images utilizing\nthe methodology from [7], producing 7828 training images and 322 validation images. Panoramic\ndepth maps labels at 512x1024 resolution are produced using DPT-BEiT-L-512 [4]) Captions are\ngenerated using BLIP-2 [20]. Of the resulting captions, \u223c 70% start with \"360 view of\" and \u223c 4%\nwith \"panoramic view of,\" while the remaining captions do not feature a panorama-related mention.\n3.2\nLDM3D-SR\nLDM3D-SR specializes in super-resolution, utilizing the KL-AE previously developed for LDM3D-\n4c 3.1 to now encode low-resolution (LR) images into a 64x64x4 dimensional latent space. The\ndiffusion model used here is an adapted version of the U-Net referenced in 3.1, now modified to\nhave an 8-channel input. This change enables conditioning on LR latent via concatenation to the\nhigh-resolution (HR) latent during training, and to noise during inference. Text conditioning is also\nfacilitated using cross attention with a CLIP text encoder.\nWe finetune the U-Net in LDM3D-SR from SD-superres [2]. Training data consists of HR and LR\nsets with 261,045 samples each. For HR samples, we use a subset of LAION Aesthetics 6+ with\ntuples (captions, 512x512-sized images, and depth maps from DPT-BEiT-L-512 [4]). LR images\nare generated using a lightweight BSR-image-degradation method, introduced in [29] applied to the\nHR image. We explored three methods for generating LR depth maps: performing depth estimation\non the LR depth maps (LDM3D-SR-D), utilizing the original HR depth map for LR conditioning\n(LDM3D-SR-O), and applying bicubic degradation to the depth map (LDM3D-SR-B).\n4\nResults\n4.1\nPanoramic RGBD generation\nWe evaluate text-to-pano RGBD generation using the validation set of our dataset (see 3.1).\n2\nImage evaluation. We compare LDM3D-pano to Text2light LDR [7], a model that creates a text-\ndriven Low Dynamic Range panorama using a hierarchical approach for detail rendering, where global\ntext-scene alignment is followed by a local sampler to facilitate patch-based panorama synthesis.\nFor image quality assessment, we utilize Frechet Inception Distance (FID), Inception Score (IS),\nand CLIP similarity; these metrics are summarized in Table 1. LDM3D-pano achieves higher FID,\nand comparable IS and CLIPsim compared to Text2light. LDM3D-pano\u2019s higher FID may be due\nto its deficiency in local awareness and the absence of training in patch-based semantic coherence;\nfocusing on the overall, global context of the given text, potentially at the expense of finer, localized\ndetails. Nonetheless, by leveraging extensive text-to-image pretraining from [29, 39], LDM3D-pano\nhas the capacity to generate a diverse range of images, as is reflected by its marginally higher IS and\nCLIPsim scores, and in visualized samples in Figure 1.\nDepth evaluation. For panoramic depth evaluation, we compare LDM3D-pano to a baseline\nmonocular panorama depth estimation model: Joint_3D60_Fres model [46]. Since diffused RGBD\noutputs have no ground truth depth available, we use DPT-BEiT-L-512 [4] depth as reference.\nThe evaluated depth maps and the reference depth are all in disparity space and thus non-metric. We\nprimarily consider the mean absolute relative error (MARE). We fit depth estimates to the reference\nvia least-squares over 500 randomly sampled points. This aims to rescale and reshift the depth\nestimates to be more closely aligned with the reference. We then compute the MARE and summarize\nresults Table 2. As explained in Figure 2, we also report the MARE computed while excluding outlier\nsamples where error exceeds the 90th percentile. In both cases, LDM3D-pano achieves lower MARE\nthan the baseline panoramic depth estimation model.\nTable 1: Text-to-pano image metrics at 512x1024,\nevaluated on 332 samples from our validation set.\nMethod\nFID \u2193\nIS \u2191\nCLIPsim \u2191\nText2light[7]\n108.30\n4.646\u00b10.27\n27.083\u00b13.65\nLDM3D-pano\n118.07\n4.687\u00b10.50\n27.210\u00b13.24\nTable 2: Pano depth metrics at 512x1024.\nReference depth is from DPT-BEiT-L-512.\nMethod\nMARE \u2193\n\u226490%ile\nJoint_3D60[46]\n1.75\u00b12.87\n0.92\u00b10.87\nLDM3D-pano\n1.54\u00b12.55\n0.79\u00b10.77\nText2light [7] Image\nLDM3D-pano Image\nLDM3D-pano Depth\nJoint_3D60 [46] Baseline\nFigure 1: Qualitative comparison of text-to-panoramic RGBD gener-\nation at 512x1024. Images are compared with Text2light [7]. Depth\nmaps are compared to Joint_3D60[46]. Captions: top\u2013\"a 360 view of\na field with a few buildings in the distance,\" middle\u2013\"a 360 view of a\ncity street with a bridge,\" bottom\u2013\"a 360 view of a hospital room.\"\n0\n5\n10\n15\n0\n25\n50\n75\n100\n125\n150\n175\n200\nMean AbsRel Error Distribution (332 samples)\nBaseline percentile(MARE, 90) = 4.29\nLDM3D percentile(MARE, 90) = 4.55\nFigure 2: Error distribu-\ntion across evaluated sam-\nples shows a long tail.\nThis motivates removing\noutliers with error above\nthe 90th percentile.\n4.2\nHigh-resolution RGBD generation\nWe evaluate HR-RGBD generation using a subset from ImageNet-Val [9] composed of 2243 samples\nat 512x512 resolution. The LR validation set is constructed via bicubic downscaling of HR-RGBD.\nImage evaluation. In line with previous studies, we use reconstruction FID, IS, Peak Signal-to-Noise\nRatio (PSNR), and Structural Similarity Index Measure (SSIM) to evaluate the quality of HR images;\nthese metrics are summarized in Table 3. We compare LDM3D-SR against bicubic regression, SDx4\nsuper-resolution based on SDv2 [29], LDMx4 [30] and SD-superres based on SDv1 [2]. The latter\nis the model from which we fine-tune LDM3D-SR. Our findings reveal that LDM3D-SR achieves\nthe best FID and the second-best IS after SDx4. Conversely, bicubic regression attains the highest\n3\nscores on PSNR and SSIM. However, these two metrics tend to prefer blurriness over misaligned\nhigh-frequency details [34] and often contradict human perception [48]; this is supported by the\nsignificantly lower FID score of bicubic regression as well as by the visualization in Figure 3.\nIn comparing LDM3D-SR-D, LDM3D-SR-O, and LDM3D-SR-B, Table 3 also presents an ablation\nstudy on the optimal depth preprocessing method outlined in 3.2. Results reveal that employing\nbicubic degradation of the initial depth map is closely aligned with utilizing the original HR depth\nmap as conditioning. Conversely, utilizing a depth map calculated from the degraded image yields\ninferior results, likely due to low quality depth maps.\nDepth evaluation. Our depth evaluation protocol for LDM3D-SR closely follows that used for\nLDM3D-pano, as described in 4.1. Our baseline here is bicubic regression on depth. Table 3 reports\nthe MARE for bicubic regression and our LDM3D-SR methods. The error in bicubically-regressed\ndepth maps is predominantly along object boundaries that become blurred upon bicubic interpolation;\nsince edges account for a small fraction of scene content, the MARE for bicubic regression is\nparticularly low. Amongst the LDM3D-SR methods, -D exhibits the highest MARE while -O and\n-B both exhibit a lower MARE. We show additional visualizations of RGBD upscaling in Figure 4,\nwhere we observe high-resolution features in both the images and depth maps (the wings and antennae\nof the grasshopper, the threads of the screw). Lastly, Figure 5 does not indicate the presence of a tail\nin the error distribution, so we do not perform any outlier removal in this evaluation.\nTable 3: x4 upscaling from 128x128 to 512x512, evaluated on 2243 samples from ImageNet-Val\nMethod\nFID \u2193\nIS \u2191\nPSNR \u2191\nSSIM \u2191\nDepth MARE \u2193\nRegression, bicubic\n24.686\n60.135\u00b14.16\n26.424\u00b13.98\n0.716\u00b10.13\n0.0153\u00b10.0189\nSDx4[29]\n15.865\n61.103\u00b13.48\n24.528\u00b13.63\n0.631\u00b10.15\nN/A\nLDMx4[30]\n15.245\n60.060\u00b13.88\n25.511\u00b13.94\n0.686\u00b10.16\nN/A\nSD-superres[2]\n15.254\n59.789\u00b13.53\n23.878\u00b13.28\n0.642\u00b10.15\nN/A\nLDM3D-SR-D\n15.522\n59.736\u00b13.37\n24.113\u00b13.54\n0.659\u00b10.16\n0.0753\u00b10.0734\nLDM3D-SR-O\n14.793\n60.260\u00b13.53\n24.498\u00b13.59\n0.665\u00b10.16\n0.0530\u00b10.0496\nLDM3D-SR-B\n14.705\n60.371\u00b13.56\n24.479\u00b13.58\n0.665\u00b10.48\n0.0537\u00b10.0506\nLR Input\nBicubic\nSDx4[29]\nLDMx4[30]\nSD-superres[2] LDM3D-SR-B\nFigure 3: Qualitative comparison of x4 upscaling. Image sourced from ImageNet-Val.\nLR Image\nRGBD from LDM3D-SR-B\nBicubic Baseline\nReference Depth\nFigure 4:\nAdditional visualization of x4 RGBD upscaling with\nLDM3D-SR-B. Depth maps are compared to a bicubic regression\nbaseline against reference depth obtained using DPT-BEiT-L-512.\n0.0\n0.2\n0.4\n0.6\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nMean AbsRel Error Distribution (2243 samples)\nBaseline\nLDM3D\nFigure 5: Since there is no\nlong tail in the error dis-\ntribution across evaluated\nImageNet samples, no out-\nlier removal is performed.\n4\n5\nConclusion\nWe introduce LDM3D-pano and LDM3D-SR for 3D VR applications. LDM3D-pano competes\nwith panorama-specialized models by generating diverse high-quality panoramic images jointly with\npanoramic depth. LDM3D-SR focuses on RGBD upscaling, outperforming related image upscaling\nmethods while also generating high-resolution depth maps. Future work could combine these domains\nto generate high-resolution panorama RGBD to further enhance immersive VR experiences.\nReferences\n[1] Ido Ariav and Israel Cohen. Fully cross-attention transformer for guided depth super-resolution. Sensors,\n23(5):2723, 2023.\n[2] Justin Pinkney at Lambda Labs.\nSuper Resolution Lambda Labs.\nhttps://huggingface.co/\nlambdalabs/stable-diffusion-super-res. Accessed: 2023-05-30.\n[3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for\ncontrolled image generation, 2023.\n[4] Reiner Birkl, Diana Wofk, and Matthias M\u00fcller. Midas v3.1 \u2013 a model zoo for robust monocular relative\ndepth estimation. arXiv preprint arXiv:2307.14460, 2023.\n[5] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. Activating more pixels in image\nsuper-resolution transformer. In Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition\n(CVPR), pages 22367\u201322377, June 2023.\n[6] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Text2light: Zero-shot text-driven hdr panorama generation.\nACM Transactions on Graphics (TOG), 41(6):1\u201316, 2022.\n[7] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Text2light: Zero-shot text-driven hdr panorama generation,\n2023.\n[8] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for\nsingle image super-resolution. In Proc. of the IEEE/CVF conf. on computer vision and pattern recognition,\npages 11065\u201311074, 2019.\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE Conf. on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009.\n[10] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep\nconvolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2):295\u2013307,\n2015.\n[11] Patrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. Taming transformers for high-resolution image synthesis,\n2021.\n[12] Sicheng Gao, Xuhui Liu, Bohan Zeng, Sheng Xu, Yanjing Li, Xiaoyan Luo, Jianzhuang Liu, Xiantong\nZhen, and Baochang Zhang. Implicit diffusion models for continuous super-resolution. In Proc. of the\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), pages 10021\u201310030, June 2023.\n[13] Jiaxin Guo, Rong Xiong, Yongsheng Ou, Lin Wang, and Chao Liu. Depth image super-resolution via\ntwo-branch network. In Cognitive Systems and Information Processing: 6th International Conference,\nICCSIP 2021, Suzhou, China, November 20\u201321, 2021, Revised Selected Papers 6, pages 200\u2013212. Springer,\n2022.\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems, 33:6840\u20136851, 2020.\n[15] iHDRI. iHDRI.COM lighting for creatives. https://www.ihdri.com. Accessed: 2023-05-30.\n[16] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep\nconvolutional networks. In Proc. of the IEEE conf. on computer vision and pattern recognition, pages\n1646\u20131654, 2016.\n[17] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-recursive convolutional network for image\nsuper-resolution. In Proc. of the IEEE conf. on computer vision and pattern recognition, pages 1637\u20131645,\n2016.\n5\n[18] Blockade Lab. Blockade lab. https://www.blockadelabs.com/.\n[19] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-\nresolution using a generative adversarial network. In Proc. of the IEEE conf. on computer vision and\npattern recognition, pages 4681\u20134690, 2017.\n[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models, 2023.\n[21] Tao Li, Hongwei Lin, Xiucheng Dong, and Xiaohua Zhang. Depth image super-resolution using correlation-\ncontrolled color guidance and multi-scale symmetric network. Pattern Recognition, 107:107513, 2020.\n[22] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. SwinIR: Image\nrestoration using Swin Transformer. In Proc. of the IEEE/CVF int. conf. on computer vision, pages\n1833\u20131844, 2021.\n[23] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual\nnetworks for single image super-resolution. In Proc. of the IEEE conf. on computer vision and pattern\nrecognition workshops, pages 136\u2013144, 2017.\n[24] Chieh Hubert Lin, Hsin-Ying Lee, Yen-Chi Cheng, Sergey Tulyakov, and Ming-Hsuan Yang. Infinitygan:\nTowards infinite-pixel image synthesis. arXiv preprint arXiv:2104.03963, 2021.\n[25] Zhisheng Lu, Juncheng Li, Hong Liu, Chaoyan Huang, Linlin Zhang, and Tieyong Zeng. Transformer\nfor single image super-resolution. In Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern\nRecognition (CVPR) Workshops, pages 457\u2013466, June 2022.\n[26] Yiqun Mei, Yuchen Fan, and Yuqian Zhou. Image super-resolution with non-local sparse attention. In\nProc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, pages 3517\u20133526, 2021.\n[27] Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lianping Yang, Shuzhen Wang, Kaihao Zhang,\nXiaochun Cao, and Haifeng Shen. Single image super-resolution via a holistic attention network. In\nComputer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proc., Part\nXII 16, pages 191\u2013207. Springer, 2020.\n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision, 2021.\n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proc. of the IEEE/CVF conf. on computer vision and\npattern recognition, pages 10684\u201310695, 2022.\n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Latent dif-\nfusion models (LDM) for super-resolution \u2013 compvis/ldm-super-resolution-4x-openimages.\nhttps:\n//huggingface.co/CompVis/ldm-super-resolution-4x-openimages. Accessed: 2023-05-30.\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models, 2022.\n[32] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation, 2015.\n[33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. Advances in Neural Information Processing\nSystems, 35:36479\u201336494, 2022.\n[34] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi.\nImage super-resolution via iterative refinement, 2021.\n[35] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi.\nImage super-resolution via iterative refinement. IEEE Trans. on Pattern Analysis and Machine Intelligence,\n45(4):4713\u20134726, 2022.\n[36] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kun-\ndurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open\nlarge-scale dataset for training next generation image-text models, 2022.\n6\n[37] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400\nmillion image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[38] Shuyao Shang, Zhengyang Shan, Guangxing Liu, and Jinglin Zhang. Resdiff: Combining cnn and diffusion\nmodel for image super-resolution. arXiv preprint arXiv:2303.08714, 2023.\n[39] Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo,\nShao-Yen Tseng, Fabio Nonato, Matthias Muller, and Vasudev Lal. Ldm3d: Latent diffusion model for 3d,\n2023.\n[40] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa.\nMvdiffusion:\nEnabling holistic multi-view image generation with correspondence-aware diffusion. arXiv preprint\narXiv:2307.01097, 2023.\n[41] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-ESRGAN: Training real-world blind\nsuper-resolution with pure synthetic data. In Proc. of the IEEE/CVF int. conf. on computer vision, pages\n1905\u20131914, 2021.\n[42] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.\nESRGAN: Enhanced super-resolution generative adversarial networks. In Proc. of the European conf. on\ncomputer vision (ECCV) workshops, pages 0\u20130, 2018.\n[43] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer:\nA general u-shaped transformer for image restoration. In Proc. of the IEEE/CVF conf. on computer vision\nand pattern recognition, pages 17683\u201317693, 2022.\n[44] Jun Xie, Rogerio Schmidt Feris, and Ming-Ting Sun. Edge-guided single depth image super resolution.\nIEEE Trans. on Image Processing, 25(1):428\u2013438, 2015.\n[45] Yuxiang Yang, Qi Cao, Jing Zhang, and Dacheng Tao. CODON: on orchestrating cross-domain attentions\nfor depth super-resolution. Int. Journal of Computer Vision, 130(2):267\u2013284, 2022.\n[46] Ilwi Yun, Hyuk-Jae Lee, and Chae Eun Rhee. Improving 360 monocular depth estimation via non-local\ndense prediction transformer and joint supervised and self-supervised learning. In Proc. of the AAAI Conf.\non Artificial Intelligence, volume 36, pages 3224\u20133233, 2022.\n[47] Greg Zaal and Rob Tuytel et al. Poly Haven. https://polyhaven.com/. Accessed: 2023-05-30.\n[48] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric, 2018.\n[49] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution\nusing very deep residual channel attention networks. In Proc. of the European conf. on computer vision\n(ECCV), pages 286\u2013301, 2018.\n[50] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image\nsuper-resolution. In Proc. of the IEEE conf. on computer vision and pattern recognition, pages 2472\u20132481,\n2018.\n[51] Zixiang Zhao, Jiangshe Zhang, Shuang Xu, Zudi Lin, and Hanspeter Pfister. Discrete cosine transform\nnetwork for guided depth map super-resolution. In Proc. of the IEEE/CVF Conf. on Computer Vision and\nPattern Recognition, pages 5697\u20135707, 2022.\n7\n"
  },
  {
    "title": "CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding",
    "link": "https://arxiv.org/pdf/2311.03354.pdf",
    "upvote": "4",
    "text": "Preprint\nCOVLM: COMPOSING VISUAL ENTITIES AND RELA-\nTIONSHIPS IN LARGE LANGUAGE MODELS VIA COM-\nMUNICATIVE DECODING\nJunyan Li\nUMass Amherst\nDelin Chen\nWuhan University\nYining Hong\nUniversity of California, Los Angeles\nZhenfang Chen\nMIT-IBM Watson AI Lab\nPeihao Chen\nSouth China University of Technology\nYikang Shen\nMIT-IBM Watson AI Lab\nChuang Gan\nUMass Amherst and MIT-IBM Watson AI Lab\nABSTRACT\nA remarkable ability of human beings resides in compositional reasoning, i.e.,\nthe capacity to make \u201cinfinite use of finite means\u201d. However, current large vision-\nlanguage foundation models (VLMs) fall short of such compositional abilities due\nto their \u201cbag-of-words\u201d behaviors and inability to construct words that correctly\nrepresent visual entities and the relations among the entities. To this end, we\npropose CoVLM, which can guide the LLM to explicitly compose visual entities\nand relationships among the text and dynamically communicate with the vision\nencoder and detection network to achieve vision-language communicative decod-\ning. Specifically, we first devise a set of novel communication tokens for the\nLLM, for dynamic communication between the visual detection system and the\nlanguage system. A communication token is generated by the LLM following a\nvisual entity or a relation, to inform the detection network to propose regions that\nare relevant to the sentence generated so far. The proposed regions-of-interests\n(ROIs) are then fed back into the LLM for better language generation contin-\ngent on the relevant regions. The LLM is thus able to compose the visual enti-\nties and relationships through the communication tokens. The vision-to-language\nand language-to-vision communication are iteratively performed until the entire\nsentence is generated. Our framework seamlessly bridges the gap between vi-\nsual perception and LLMs and outperforms previous VLMs by a large margin on\ncompositional reasoning benchmarks (e.g., \u223c 20% in HICO-DET mAP, \u223c 14%\nin Cola top-1 accuracy, and \u223c 3% on ARO top-1 accuracy). We also achieve\ncompetitive performances on traditional vision-language tasks such as referring\nexpression comprehension and visual question answering 1.\n1\nINTRODUCTION\nA remarkable ability of human beings resides in compositional reasoning: the capacity to construct\nan endless number of novel combinations from a finite set of known components, i.e., \u201cinfinite use of\nfinite means\u201d (Chomsky, 1965; 1957; Montague, 1970). As depicted in Figure 1, for someone who\nhas never witnessed a scene where a person sits on a sulky, it\u2019s not hard to render this conclusion\nby combining the known components - \u201cman\u201d, \u201cis sitting on\u201d and \u201csulky\u201d. Compositionality is\nomnipresent in the language such that a sentence is made up of words like nouns (\u201cman\u201d) and\nverbs (\u201csit\u201d). It also exists ubiquitously in vision so that we could easily detect visual entities\nsuch as the person and the sulky, composed with relationships like \u201csit on\u201d. In fact, it\u2019s believed\nby cognitive scientists that the meaning of a sentence lies in the interaction between an utterance\n1Project page: https://vis-www.cs.umass.edu/CoVLM\n1\narXiv:2311.03354v1  [cs.CV]  6 Nov 2023\nPreprint\nCommunicative Decoding\nA man\nis sitting on\nVanilla Decoding\nA man is sitting on\nthe horse\nTraining Data\nA man is sitting on horse\ngrass\nSofa\ncar\nwheelchair\nground\nChair\nExisting VLM Models\nOur CoVLM\na sulky\nFigure 1: Comparison with existing VLMs. Previous models take in a whole image as input, impair-\ning the compositionality of VLMs. Our CoVLM inserts communication tokens into the LLM after\nvisual entities / relationships to enable the language-to-vision and vision-to-language communica-\ntion, improving compositionality to a large extent.\nand external situations that can be perceived - the meaning of a noun phrase is linked to a visual\nentity, and the meaning of a verb phrase is linked to a relational property (Janssen & Partee, 1997).\nFrom the meanings of the subject, verb phrase and object, the sentence is built in a systematic and\ncompositional way.\nCurrent Vision-Language Models (VLMs), however, tend to fall short of such compositional abili-\nties (Ma et al., 2023; Cascante-Bonilla et al., 2023; Doveh et al., 2022; Zhao et al., 2023). As noted\nby recent works, deficiency of compositionality in these VLMs is likely due to the hypothesis that\nthey behave like \u201cbag-of-words\u201d (Yuksekgonul et al., 2022) - that they merely memorize by rote the\nfrequent co-occurrences of words, but fail to construct words that could correctly represent objects\nand the relations between objects. In fact, previous works (Zhao et al., 2023; Cascante-Bonilla et al.,\n2023) have shown that VLMs struggle a lot when relationships are involved. We can also come to\nthis conclusion from Figure 1, in which the models utilize the shortcut learned from pre-training\nthat \u201ca man sits on a horse\u201d appears frequently and there\u2019s a man and a horse in the image, utterly\noverlooking the real object, sulky, that the person is sitting on.\nDelving into the architectures of these VLMs and how they infuse images into LLMs, we find\nthat these VLMs deviate from the way human beings perform compositional reasoning from sev-\neral perspectives. First, they feed one single image as a whole into LLMs and generate language\ndescriptions based on the holistic image embedding. This is inconsistent with object-centric repre-\nsentations in vision, through which the whole image can be constituted by visual entities and more\nimportantly, relationships between the entities. Second, these methods disregard the interaction be-\ntween the sentence parts and the ingredients in the images. The generation of a new word by the\nLLM is not linked to a specific visual entity or relationship but is contingent on previous words\nand holistic image feature instead. Although a series of works have been proposed to strengthen\nVLMs\u2019 compositional abilities (Doveh et al., 2023), they mainly probe the problem by proposing\nadditional datasets. However, as stated by recent analysis on compositionality (Doveh et al., 2022),\ncollecting specialized large-scale data to teach VL models the missing compositionality is imprac-\ntical, as finding specialized text-image pairs for each kind and possible value of the visual entities\nand their relations is rather expensive. In this paper, we approach the essence of this problem from\nthe perspective of model architecture, unveiling a compositional structure of LLM that can conduct\nstep-by-step communication with visual components and relationships.\nWe propose CoVLM: Composing Visual Entities and Relationships in Large Language Models,\nwhich guides the LLM to explicitly compose visual entities and relationships among the text, and\ndynamically communicate with the detection network to achieve vision-language communicative\ndecoding. Specifically, we devise a novel set of communication tokens for dynamic interaction and\ncommunication between the detection network and the LLM. Communication tokens are generated\nby the LLM, after the language tokens that denote visual entities or relationships. Upon the gen-\neration of communication tokens, a detection network is utilized to decode the regions relevant to\nthe generated language sequence so far, and propose several bounding box proposals. The features\nof relevant regions are then fed back to LLM by communication tokens, conditioned on which the\nLLM decodes the subsequent tokens. The bottom-up vision-to-language and top-down language-to-\nvision communicative decoding are iteratively performed until all words and tokens are generated.\nThe paradigm is shown on the right part of Figure 1.\n2\nPreprint\nWe first evaluate our CoVLM on compositional reasoning tasks, including predicting the object\nentity given the subject and the relationship (ARO (Yuksekgonul et al., 2022)), matching the correct\ncaptions describing the relation between two images with similar entities (Cola, (Ray et al., 2023)),\nand human-object interaction detection (HICO-DET, (Chao et al., 2015)). We outperform baseline\nVLMs by a large margin (e.g., \u223c 20% in HICO-DET mAP, \u223c 14% in Cola top-1 accuracy, and\n\u223c 3% on ARO top-1 accuracy). We also achieve competitive results on vision-language tasks such\nas referring expression comprehension and visual question answering.\n2\nRELATED WORKS\n2.1\nVISION-LANGUAGE MODEL (VLM)\nA proliferation of VLMs with remarkable commonsense reasoning abilities have been proposed\nrecently. Among them, Flamingo (Alayrac et al., 2022) employs cross-attention and perceiver sam-\npler to attend to visual contexts and enables visual context learning. BLIP2 (Li et al., 2023) uses a\nQFormer to attend to salient visual context for better language generation based on the visual con-\ntext. LLaVA (Liu et al., 2023a) performs image-text alignment first and then conducts instruction\nfinetuning. MiniGPT-4 (Zhu et al., 2023) aligns a frozen visual encoder with LLM using just one\nprojection layer. mPLUG-Owl (Ye et al., 2023) also involves a two-stage method for aligning image\nand text. There are also recent papers that push VLMs to the 3D domain (Hong et al., 2023).\nRecently, there has been a series of works that utilize LLMs for visual segmentation tasks. Specif-\nically, VisionLLM (Wang et al., 2023) uses an LLM-based decoder which makes predictions about\nbounding boxes and polygons given language instructions. DetGPT (Pi et al., 2023) is able to in-\nterpret human instruction, reason about the visual scene with common sense knowledge, and finally\noutput the objects of interest. GPT4RoI (Zhang et al., 2023) is capable of processing the user instruc-\ntions that contain interleaved sequences of language and spatial information. LISA (Lai et al., 2023)\nproposes the embedding-as-mask paradigm to unlock the segmentation capability. However, the\nvision-language communication of these VLMs is one-way and one-time, merely using language in-\nstructions to generate segmentations, or input segmented regions into the LLMs. KOSMOS-2 (Peng\net al., 2023) infuses location tokens after visual entities into the language generation process. How-\never, the communication is purely from the language system to the image for segmentation, while the\ngrounded visual regions are not fed back to the language system. Furthermore, none of these VLMs\ntackle the relations or compositionality in the language inputs. In this paper, we propose CoVLM\nwith a set of communication tokens for composing visual entities and relations and communicating\nbetween visual and language systems at each step.\n2.2\nCOMPOSITIONALITY IN VISION AND LANGUAGE\nCompositionality is a hallmark of human intelligence and plays an indispensable role in vision and\nlanguage. Previous works exploring the compositionality in vision and language cover a variety\nof tasks such as visual question answering (Agrawal et al., 2017), generation (Liu et al., 2023b),\nretrieval (Saito et al., 2023), planning (Ajay et al., 2023) and so on . A set of datasets have been\nproposed for examining the compositionality of vision-language models (Hudson & Manning, 2019;\nJohnson et al., 2016; Agrawal et al., 2017; Krishna et al., 2017; Ma et al., 2023). Specifically, the\nAttribution, Relation, and Order (ARO) benchmark (Yuksekgonul et al., 2022) is a benchmark to\nsystematically evaluate the ability of VLMs to understand different types of relationships, attributes,\nand order. Recently, VL-Checklist (Zhao et al., 2023) is a framework to evaluate VLM\u2019s abilities\nof recognizing objects, attributes, and relations. Cola (Ray et al., 2023) analyzes VLMs\u2019 compo-\nsitional ability in detail and propose a text-to-image retrieval benchmark to compose objects with\ntheir relations. Evaluation of VLMs on these benchmarks and metrics show current VLMs struggle\nwith compositionality. Furthermore, a set of works find it particularly frustrating for VLMs when\nrelationships are involved (Conwell & Ullman, 2022; Zhao et al., 2023). In this paper, we specially\nfocus on relational compositionality, with the help of the aforementioned datasets and metrics.\n3\nPreprint\nVision-Language Communication\nCLIP\n...\n<obj> A bread </obj> <visual> <box> is on the left of <previsual> <box> <obj> salad </obj>\nROI Pooling\nROI Pooling\nDetector\n(YOLO)\nObject\nDetector\nC\n/\n<visual> / <previsual> Features\nCoVLM\nFigure 2: Overview of our CoVLM framework. Our vision module consists of a CLIP encoder to\nencode the image, and an object detector which takes in the image together with language inputs\nto generate relevant regions. For language modelling, we insert a set of communication tokens into\nthe LLM, which can appear after a visual entity with a <visual> token or after a relationship\nwith a <previsual> token. The last hidden layer of the LLM is then sent to the object detector to\npropose regions relevant to the language inputs so far. This is termed as top down language-to-vision\ncommunication. Next, in vision-to-language communication, the features of the proposed regions\nare fed back to LLM via <box> or <prebox> token for further language generation.\n3\nCOVLM\nMost state-of-the-art Vision Language Models (VLMs) (e.g., LLaVA (Liu et al., 2023a),\nFlamingo (Alayrac et al., 2022), BLIP-2 (Li et al., 2023)) take an image and text prompt as inputs,\nand output a text sequence. Several recent VLMs (e.g., VisionLLM (Wang et al., 2023), LISA (Lai\net al., 2023), KOSMOS-2 (Peng et al., 2023)) enable a new ability to output segmentation masks\nbased on the text input. Specifically, KOSMOS-2 generates a location token denoting a discretized\nbounding box for each visual entity to ground the visual entity to the image. However, the communi-\ncation is purely from the LLM to the image for segmentation, while the grounded visual regions are\nnot fed back to the LLM. In addition, the location tokens are generated after the visual entities, thus\nfailing to assist in the process of generating word tokens based on grounded visual entities and rela-\ntions. In short, the cut-off between the vision module and the LLM deprives previous VLMs of the\ncrucial visual compositional ability. On the other hand, detection networks like Faster RCNN (Ren\net al., 2016) are able to generate region proposals and classify the proposals, but can not interact\nwith the language models.\nIn stark contrast to previous VLMs, our CoVLM stands out with its pioneering integration of de-\ntection networks into LLM to enable the seamless interaction between the vision module and the\nLLM, and compositionality over visual entities and relations. As shown in Figure 2, we first de-\nvise a set of special communication tokens for flexibly switch between the visual detection module\nand the LLM. For LLM, we use a pre-trained Pythia model (Biderman et al., 2023) that can handle\nlanguage tokens as inputs and outputs, as well as visual embeddings and special tokens which are\nmapped to the same embedding space as the language tokens, to constitute the LLM representations.\nThe vision module consists of an image encoder that produces features to feed into the LLM, and a\ndetection network that proposes region proposals that are relevant to previous language inputs. Top-\ndown language-to-vision communication is achieved by concatenating the last hidden state of the\nLLM-encoded features to the image embeddings, and input them into the detection network, which\nproposes relevant regions conditioned on the LLM representations. Bottom-up vision-to-language\ncommunication extracts the features of the relevant regions, and concatenates the features back into\nLLM for further language generation. In this way, the LLM is equipped with visual composionality.\nWe give the details below.\n3.1\nVISION MODULE\nOur vision module consists of two parts: image encoder and detection network.\n4\nPreprint\nImage Encoder. In this paper, we use the CLIP ViT-L model (Radford et al., 2021) for encoding the\nimage. We use a linear mapping layer to map the image embeddings to the same embedding space\nas the Pythia language embedding space. We then append the image embeddings to the beginning\nof the language sequence.\nDetection Network. Our detection network is similar to the YOLOX (Ge et al., 2021). The detection\nnetwork takes as inputs two things: 1) the image embeddings of the whole image (N \u00d7 N \u00d7 D,\nwhere N is the patch size and D is the embedding dim); 2) the last hidden state of the LLM so\nfar (1 \u00d7 D). The LLM embedding is expanded and concatenated to the same dim as the image\nembedding, yielding a final multi-modal embedding of size N \u00d7 N \u00d7 2D, and send to the detection\nnetwork. The detection network outputs N \u00d7N \u00d74 bounding boxes and N \u00d7N confidence scores.\nAfter non-maximum suppression, we keep a set of bounding boxes as regions of interest (ROIs). To\nextract the embeddings of one ROI, we extract the features of all patches that are covered by the\nROI, and average pool to yield a box embedding of size D. We choose the cropped image features\nof m bounding boxes with top scores.\n3.2\nLANGUAGE MODELS\nWe utilize the pre-trained Pythia model (Biderman et al., 2023) as the backbone of our LLM.\nIn addition to language tokens, we also devise a set of special communication tokens to facilitate\ncompositional vision-language modeling and communication, as is shown in Figure 2. We list the\nset of tokens below:\n\u2022 <obj>, </obj>: these two tokens enclose a set of language tokens referring to a visual entity\n\u2022 <visual>: this token is for switching to the vision module after a visual entity token v1 is\ncaptured by LLM, so the vision module could attend to the visual entity\n\u2022 <box>: this token receives the feedback from the vision module, concatenating the image fea-\ntures of detected v1 back into the LLM\n\u2022 <previsual>: this token is for switching to the vision module after a relation r to a previous\nvisual entity v1 is detected (and before the visual entity v2 that is in relation r to v1 is generated).\n\u2022 <prebox>: this token switches back from the vision module after potential regions of v2 are\ndetected, and concatenating the features to better generate the language description of v2.\nThe generation of communication tokens for visual entities and relations enable us to decompose the\nlanguage sequences into smaller components, where each component connects to the vision module,\nthus improving compositionality.\n3.3\nVISION-LANGUAGE COMMUNICATION\nThe dynamic interaction and communication between the vision module and the language model\ncan be iteratively performed through the special communication tokens introduced above.\nTop-Down Language-to-Vision Communication. Top-down communication is achieved by first\ngenerating the <visual> token or <previsual> token. After the token is generated, we sum-\nmarize the language information generated so far by taking the last hidden state of the LLM. This\ninformation gives the vision module a goal or task to attend to, just like the human visual system\n(Buschman & Miller, 2007). Contingent on the information so far, the vision module then uses the\ndetection network to propose several ROIs, and extracts the features of these ROIs.\nBottom-Up Vision-to-Language Communication. Bottom-up communication is achieved by gen-\nerating the <box> token or <prebox> token. The ROIs generated by the vision module are then\nfed back into the LLM to assist further language generation. For instance, if the prebox contains\nregions relevant to \u201ca bread is on the left of\u201d, the LLM is then capable of absorbing this information\nand generating \u201csalad\u201d.\n3.4\nMODEL PRE-TRAINING\nPre-training data. We create a large-scale grounded image-text dataset which consists of over 97M\nimage-text pairs from the pre-training data of BLIP-2 (Li et al., 2023). The images are from a variety\n5\nPreprint\nof datasets including COCO (Lin et al., 2014), CC3M (Sharma et al., 2018), CC12M (Changpinyo\net al., 2021), Visual Genome (Krishna et al., 2017), SBU (Ordonez et al., 2011) and a subset of\nLAION400M (Schuhmann et al., 2021). We apply a grounding pipeline to the image-text pair\nto associate the text spans in the caption to their corresponding visual entities in the image. The\npipeline consists of three steps:\nStep-1: Generating bounding-box-word pairs. We use GroundingDINO (Liu et al., 2023c) to\ndetect objects in the image and link the bounding box of the object to words in the text. We keep\nbounding boxes whose highest similarities are higher than 0.35 and extract the words whose similar-\nities are higher than 0.25 as the words that correspond to a bounding box. Non-maximum suppres-\nsion algorithm is applied to eliminate bounding boxes that have a high overlap with other bounding\nboxes linked to the same word.\nStep-2: Expanding grounded words to grounded expressions.\nIn practice, we observe that\nGroundingDINO often fail to link the whole referring expressions to an object in the image. For ex-\nample, for the expression \u201cman with a hat on his head\u201d, GroundingDINO will only link \u201cman\u201d to the\nperson in the image, but not the whole expression. This will limit the model\u2019s ability to understand\ncomplicated expressions. Inspired by KOSMOS-2 (Peng et al., 2023), we apply spaCy (Honnibal\net al., 2020) to obtain each word\u2019s dependency relation in the sentence, and expand a grounded word\nto a grounded expression by recursively traversing the dependency tree of that word and concatenate\neligible children words based on the linguistic rules.\nStep-3:\nAssigning bounding boxes to the special communication tokens.\nGiven the ex-\npressions and their associated bounding boxes in a grounded image-text pair, we can now\ninsert the special communication tokens into the text and assign the bounding boxes to\nthem.\nFor a given expression with a single bounding box, the resulted input sequence\nfor that expression is either in the form of \u201c<obj>expression</obj><visual><box>\u201d or\n\u201c<previsual><prebox><obj>expression</obj>\u201d depending on the position of the expres-\nsion in the sentence. If it is the first expression in the sentence, we use the form with a trailing\n<visual> token. Otherwise, we randomly select one from these two available forms.\nPre-training settings. We trained two models: CoVLM 1.4B and 2.8B, which uses Pythia-1.4B\nand Pythia-2.8B as the LLM respectively. Both of them use CLIP ViT-L/14 (Radford et al., 2021)\nas the image encoder. We load the huggingface checkpoint for these models, and fully fine-tune the\nwhole model during pre-training. More details can be found in Appendix.\n4\nEXPERIMENTS\n4.1\nEVALUATION ON COMPOSITIONAL REASONING TASKS\nWe aim to probe the model\u2019s ability to reason about entities with detailed attributes in an image and\nalso the relationships between two entities. All experiments are conducted in a zero-shot manner.\nDatasets and Metrics. We conduct experiments on ARO (Yuksekgonul et al., 2022), Cola (Ray\net al., 2023), and HICO-DET (Chao et al., 2015) datasets.\n\u2022 ARO contains 23,937 testing images. One entity pair in an image is annotated as a tuple (en-\ntity A, relation, entity B). Given entity A and relation, the model is required to predict entity B\nout of all candidate objects. We use top-1 and top-5 accuracy as evaluation metrics.\n\u2022 Cola contains 420 testing images, where each image is paired with a caption describing the\nrelation between two entities similar to ARO. The entity in Cola is described with more attribute\ndetails (e.g., texture, color, and size) which require the model to perform more fine-grained object\nrecognition. We conduct experiments on the multi-object part of this dataset, where two images\nwith similar entities and their corresponding captions are paired as a testing sample. The model\nis required to correctly match the corresponding caption for both two images. We report the\naccuracy following Cola (Ray et al., 2023).\n\u2022 HICO-DET contains 9,658 testing images, with 600 HOI categories constructed by 80 object\ncategories and 117 verb classes. Each instance of human-object interaction is represented as\na triplet, denoted as (subject, verb, object), accompanied by their respective bounding boxes.\nThe model is required to not only recognize the HOI categories but also localize the subject and\n6\nPreprint\nModel\nARO\nCola\nHICO-DET\nAccuracy\nAccuracy\nmAP\nTop-1\nTop-5\nTop-1\nRare\nNon-Rare\nFull\nVision-language Alignment Models\nCLIP (Radford et al., 2021)\n6.93\n21.12\n21.42\n-\n-\n-\nFLAVA (Singh et al., 2022)\n4.59\n12.76\n24.76\n-\n-\n-\nVision-language Generative Models\nOpenFlamingo3B (Awadalla et al., 2023)\n2.55\n7.11\n18.10\n-\n-\n-\nBLIP (Li et al., 2022)\n29.78\n54.18\n41.43\n-\n-\n-\nBLIP-2 ViT-L OPT2.7B (Li et al., 2023)\n29.73\n54.91\n35.71\n-\n-\n-\nKOSMOS-2 (Peng et al., 2023)\n19.88\n43.69\n30.48\n33.51\n17.83\n21.26\nCoVLM 1.4B\n32.46\n55.70\n44.29\n50.82\n35.47\n39.00\nTable 1: Compositional reasoning ability comparison with vision-language alignment models and\ngenerative models on three datasets. Visualization results are shown in Appendix and project page.\nobject. Following Chao et al. (2015), we report the mean AP (mAP) on 3 splits, namely a) Rare:\n138 HOI categories with less than 10 training instances, b) Non-Rare: the remaining 462 HOI\ncategories, and c) Full: all 600 HOI categories.\nBaselines. We mainly compare our CoVLM with two types of methods, namely the vision-language\nalignment model (i.e., CLIP (Radford et al., 2021), FLAVA (Singh et al., 2022)) and the vision-\nlanguage generative model (i.e., OpenFlamingo (Awadalla et al., 2023), BLIP-2 (Li et al., 2023) and\nKOSMOS-2 (Peng et al., 2023)). The vision-language alignment models learn a vision encoder and\na language encoder, which pull close the features of paired image and text while pushing away the\nunpaired one. The vision-language generative model takes as input the image and text and auto-\nregressively generates a sequence of text.\n4.1.1\nARO\nSetup. For our model and other vision-language generative models, we feed the model with the\nimage and text prompt \u201centity A relation\u201d, considering the model output as predicted entity B. For\nour model, we further insert a <visual> token after entity A and a <previsual> token after\nrelation to encourage the language model to better communicate with the visual branch. For vision-\nlanguage alignment models, we use all 890 candidates entity B to build 890 possible captions in the\nform of \u201centity A relation entity B\u201d and pick the top-1/top-5 captions that have the highest similarity\nscore with the image as the top-1/top-5 predictions.\nResults. In Table 1, our model achieves superior performance, outperforming all other vision-\nlanguage generative models and alignment models. It indicates that our model has a better ability\nto understand relations among visual entities in an image, and can better infer one visual entity\nusing information of the presence of other visual entities and their relations. We also notice that the\nalignment models perform worse in this task. We hypothesize this is because the alignment models\nare trained using contrastive learning, which makes them behave like bag-of-words (Yuksekgonul\net al., 2022). This makes them more easily to be misdirected by other objects in the image and\nproduce the wrong prediction, instead of using the relationship to infer the correct one.\n4.1.2\nCOLA\nSetup. For our model and other vision-language generative models, we calculate the perplexity\nscore between a caption and all candidate images and choose the image with lower perplexity as the\nprediction. Specifically, we feed the model with one image and a caption in the form of \u201centity a\nrelation entity b\u201d. We calculate the average perplexity over all text output. Notably, for our model,\nwe will insert a <visual> and <previsual> tokens after entity a and relation, respectively\nto encourage vision-language communication. For vision-language alignment models, we directly\nreport the results from Cola (Ray et al., 2023).\nResults. In Table 1, our CoVLM significantly outperforms both alignment and generative methods\nby a large margin. We attribute the performance to the <previsual> token which helps to retrieve\n7\nPreprint\nModel\nZero-shot\nmAP\nRare\nNon-Rare\nFull\nInteractNet (Gkioxari et al., 2018)\n%\n7.16\n10.77\n9.94\nCDN (Zhang et al., 2021)\n%\n27.39\n32.64\n31.44\nGEN-VLKT (Liao et al., 2022)\n%\n29.25\n35.10\n33.75\nRLIPv1-ParSe (Yuan et al., 2022)\n%\n26.85\n34.63\n32.84\nRLIPv2-ParSeDA (Yuan et al., 2023)\n%\n43.23\n45.64\n45.09\nRLIPv1-ParSe (Yuan et al., 2022)\n!\n15.08\n15.50\n15.40\nRLIPv2-ParSeDA (Yuan et al., 2023)\n!\n27.97\n21.90\n23.29\nCoVLM 1.4B\n!\n50.82\n35.47\n39.00\nTable 3: Comparison with task-specific methods on HICO-DET.\nthe visual information of the entity b for better describing its detailed attributes in text form, thus\nleading to lower complexity for the ground-truth caption.\nModel\nAcc.\nCLIP + MM-Pred (Ray et al., 2023)\n41.42\nCLIP + MM-Adapter (Ray et al., 2023)\n40.95\nFLAVA + MM-Pred (Ray et al., 2023)\n39.04\nFLAVA + MM-Adapter (Ray et al., 2023)\n40.47\nCoVLM 1.4B\n44.29\nTable 2: Comparisons with task-specific super-\nvised learning methods on Cola.\nAlso, the <visual> token helps to better lo-\ncalize entity a, allowing the model to better lo-\ncalize the area of entity b according to rela-\ntion. In Table 2, we also compare our zero-\nshot results with task-specific methods pro-\nposed in Ray et al. (2023) which fine-tunes\nCLIP (Radford et al., 2021) and FLAVA (Singh\net al., 2022) on the training data of Cola. Our\nmethod still achieves the best performance,\ndemonstrating the superiority of generalization\nand robustness of our model.\n4.1.3\nHICO-DET\nSetup. For our method and other generative models, we predict HOI (subject, verb, object) in two\nsteps: a) recognizing the interaction categories represented by verb, and b) localizing the subject and\nobject. To determine the existence of interaction, we manually build positive and negative phases\nwith verb, i.e., \u201cthe person is verb\u201d and \u201cthe person is not verb\u201d and calculate their perplexities\nusing the generative model. If the perplexity of the positive phase is lower than the negative one, we\nconsider this verb exists in the image. For all detected verb, we feed the model with the image and\ntext prompt \u201cthe person is verb\u201d to predict object and the location of the person and object. Notably,\nfor our CoVLM, we use the inserted <visual> and <previsual> token after the person and\nverb to predict the locations. Since the output of alignment methods does not contain the object\nlocation, we ignore these methods on the HICO-DET dataset.\nResults. Table 1 presents the zero-shot results on HICO-DET test set. Our model significantly\noutperforms KOSMOS-2 (Peng et al., 2023). We attribute the performance improvement to the\n<previsual> token that forces the model to communicate with the input image to localize the\narea of object and predict the text of object. In comparison, KOSMOS-2 only feeds the image\ninformation at the beginning of the input, and thus the model may suffer from language prior for\npredicting a wrong object. In Table 3, we also compare our zero-shot results with the task-specific\nsupervised learning methods. Our model achieves comparable results on the Non-Rare and Full sets.\nNotably, our zero-shot result exceeds all supervised learning methods in the Rare set, demonstrating\nthe generalization ability of our model.\n4.2\nEVALUATION ON VISION-LANGUAGE TASKS\nIn addition to compositional reasoning tasks, we also evaluate object localization and complex\nvision-language understanding abilities of our model. All experiments are conducted in a zero-shot\nmanner.\n8\nPreprint\nModel\nRefCOCOg\nRefCOCO+\nRefCOCO\nval\ntest\nval\ntestA\ntestB\nval\ntestA\ntestB\nGRILL (Jin et al., 2023)\n-\n47.50\n-\n-\n-\n-\n-\n-\nReCLIP (Subramanian et al., 2022)\n59.33\n59.01\n47.87\n50.10\n45.10\n45.78\n46.10\n47.07\nKOSMOS-2 (Peng et al., 2023)\n60.57\n61.65\n45.48\n50.73\n42.24\n52.32\n57.42\n47.26\nCoVLM 1.4B\n60.87\n61.91\n47.62\n50.93\n44.16\n48.19\n53.17\n43.18\nCoVLM 2.8B\n61.23\n62.33\n48.87\n52.51\n44.71\n49.32\n53.67\n44.49\nTable 4: Comparison of referring expression comprehension on three datasets.\n4.2.1\nREFERRING EXPRESSION COMPREHENSION\nSetup. This task requires a model to locate the bounding box of an object specified by language\ndescription. We follow KOSMOS-2 (Peng et al., 2023) to use three well-established benchmarks\nnamely RefCOCOg (Mao et al., 2016), RefCOCO+ (Yu et al., 2016) and RefCOCO (Yu et al.,\n2016). All these datasets use images from COCO (Lin et al., 2014). Both RefCOCO and RefCOCO+\nare annotated by a two-player game, where the spatial relations are excluded in RefCOCO+. The\nRefCOCOg contains longer referring expressions and spatial relations.\nFor our CoVLM, we feed the model with <obj>expression</obj><visual> and the\n<visual> will generate multiple bounding boxes with their confidence scores.\nInstead of\nchoosing the bounding box with the highest score as a prediction, we use <previsual> to\nfurther measure the alignment between the box and expression.\nSpecifically, we select the\nbounding box with the highest confidence score and subsequently choose additional bounding\nboxes from the remaining set whose confidence scores exceed a predefined threshold (0.5 times\nthe highest score in our case) as candidates.\nFor each candidate, we feed the model with\n<previsual><prebox><obj>expression</obj> and put the image feature of the bounding\nbox region into <prebox>. Then we calculate the perplexity of the expression for this bounding\nbox candidate and choose the one with the lowest perplexity as our final prediction.\nResults. In Table 4, our CoVLM 2.8B variant performs the best on both val and test sets of Ref-\nCOCOg and RefCOCO+, demonstrating its superior localization ability. On RefCOCO, we achieve\ncomparable performance with KOSMOS-2. Note that KOSMOS-2 has been instruction fine-tuned\non the data with the same form as the referring expression task, while our CoVLM is directly trans-\nferred to this new task without any form of instruction fine-tuning.\n4.2.2\nVISUAL QUESTION ANSWERING\nSetup. This task requires a model to answer questions about an image. Following BLIP-2 (Li et al.,\n2023), we evaluate on VQAv2 (Goyal et al., 2017) dataset and report the zero-shot accuracy on\ntest-dev set. We use the prompt \u201cQuestion: {question} Short Answer:\u201d.\nFigure 3: VQAv2 test-dev results.\nResults. In Figure 3, our CoVLM 2.8B model achieves bet-\nter performance compared with MetaLM (Hao et al., 2022),\nVLKD (Dai et al., 2022), OpenFlamingo3B (Awadalla et al.,\n2023), and KOSMOS-2 (Peng et al., 2023), and has a small\nmargin compared with Flamingo3B (Alayrac et al., 2022) and\nBLIP-2 ViT-L OPT2.7B Li et al. (2023). We hypothesize the\naccuracy margin may stem from the generative model gen-\nerating diverse answers that align conceptually with ground\ntruth, yet may not exactly match the annotation, affecting the\nevaluation. To get a better insight into how well our model\nperforms on VQAv2, we conduct a round of human evalua-\ntion for our CoVLM 2.8B model and BLIP-2 ViT-L OPT2.7B\nmodel on a randomly selected subset with 1000 samples. The\nhuman evaluation accuracy for our model is 57.11 while the\naccuracy for BLIP-2 is 56.62, suggesting that the performance\ngap between our model and BLIP-2 is negligible.\n9\nPreprint\n5\nCONCLUSION\nIn this paper we propose CoVLM: Composing Visual Entities and Relationships in Large Language\nModels, which guides the LLM to explicitly compose visual entities and relationships among the\ntext, and dynamically communicate with the detection networks to achieve vision-language com-\nmunicative decoding. We outperform previous VLMs by a large margin on compositional reasoning\nbenchmarks (e.g., \u223c 20% in HICO-DET mAP, \u223c 14% in Cola top-1 accuracy, and \u223c 3% in ARO\ntop-1 accuracy). We also achieve competitive performances on vision-language tasks such as re-\nferring expression comprehension and visual question answering. However, we do not cope much\nwith object-attribute compositionality and spatial event compositionality, which are crucial future\ndirections.\nREFERENCES\nAishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, and Devi Parikh. C-vqa: A compositional\nsplit of the visual question answering (vqa) v1.0 dataset, 2017.\nAnurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi Jaakkola, Josh Tenen-\nbaum, Leslie Kaelbling, Akash Srivastava, and Pulkit Agrawal. Compositional foundation models\nfor hierarchical planning, 2023.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-\nbastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski,\nRicardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.\nFlamingo: a vi-\nsual language model for few-shot learning.\nArXiv, abs/2204.14198, 2022.\nURL https:\n//api.semanticscholar.org/CorpusID:248476411.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei\nKoh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt.\nOpenflamingo: An open-\nsource framework for training large autoregressive vision-language models.\narXiv preprint\narXiv:2308.01390, 2023.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hal-\nlahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya\nSkowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language\nmodels across training and scaling, 2023.\nT. J. Buschman and Earl K. Miller.\nTop-down versus bottom-up control of attention in the\nprefrontal and posterior parietal cortices.\nScience, 315:1860 \u2013 1862, 2007.\nURL https:\n//api.semanticscholar.org/CorpusID:144021759.\nPaola Cascante-Bonilla, Khaled Shehada, James Seale Smith, Sivan Doveh, Donghyun Kim,\nRameswar Panda, G\u00a8ul Varol, Aude Oliva, Vicente Ordonez, Rogerio Feris, and Leonid Karlinsky.\nGoing beyond nouns with vision and language models using synthetic data, 2023.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.\nConceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3558\u20133568, 2021.\nYu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, and Jia Deng. Hico: A benchmark for recog-\nnizing human-object interactions in images. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), December 2015.\nNoam Chomsky. Bibliography, pp. 115\u2013120. De Gruyter Mouton, Berlin, Boston, 1957. ISBN\n9783112316009.\ndoi: doi:10.1515/9783112316009-014.\nURL https://doi.org/10.\n1515/9783112316009-014.\nNoam Chomsky.\nBibliography, pp. 253\u2013264.\nThe MIT Press, 50 edition, 1965.\nISBN\n9780262527408. URL http://www.jstor.org/stable/j.ctt17kk81z.10.\n10\nPreprint\nColin Conwell and Tomer David Ullman. Testing relational understanding in text-guided image\ngeneration. ArXiv, abs/2208.00005, 2022. URL https://api.semanticscholar.org/\nCorpusID:251224307.\nWenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. Enabling multimodal\ngeneration on clip via vision-language knowledge distillation. arXiv preprint arXiv:2203.06386,\n2022.\nSivan Doveh, Assaf Arbelle, Sivan Harary, Rameswar Panda, Roei Herzig, Eli Schwartz, Donghyun\nKim, Raja Giryes, Rog\u00b4erio Schmidt Feris, Shimon Ullman, and Leonid Karlinsky. Teaching\nstructured vision & language concepts to vision & language models. 2023 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 2657\u20132668, 2022.\nURL https:\n//api.semanticscholar.org/CorpusID:253734406.\nSivan Doveh, Assaf Arbelle, Sivan Harary, Roei Herzig, Donghyun Kim, Paola Cascante-Bonilla,\nAmit Alfassy, Rameswar Panda, Raja Giryes, Rog\u00b4erio Schmidt Feris, Shimon Ullman, and\nLeonid Karlinsky.\nDense and aligned captions (dac) promote compositional reasoning in vl\nmodels.\nArXiv, abs/2305.19595, 2023.\nURL https://api.semanticscholar.org/\nCorpusID:258987899.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text\nfor language modeling. arXiv preprint arXiv:2101.00027, 2020.\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in\n2021, 2021.\nGeorgia Gkioxari, Ross Girshick, Piotr Doll\u00b4ar, and Kaiming He. Detecting and recognizing human-\nobject interactions. In Proceedings of the IEEE conference on computer vision and pattern recog-\nnition, pp. 8359\u20138367, 2018.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pp. 6904\u20136913, 2017.\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and\nFuru Wei. Language models are general-purpose interfaces. arXiv preprint arXiv:2206.06336,\n2022.\nYining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang\nGan. 3d-llm: Injecting the 3d world into large language models, 2023.\nMatthew Honnibal, Ines Montani, Sofie Van Landeghem, Adriane Boyd, et al. spacy: Industrial-\nstrength natural language processing in python. 2020.\nDrew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning\nand compositional question answering, 2019.\nTheo M.V. Janssen and Barbara H. Partee.\nChapter 7 - compositionality.\nIn Johan van\nBenthem and Alice ter Meulen (eds.), Handbook of Logic and Language, pp. 417\u2013473.\nNorth-Holland, Amsterdam, 1997.\nISBN 978-0-444-81714-3.\ndoi: https://doi.org/10.1016/\nB978-044481714-3/50011-4.\nURL https://www.sciencedirect.com/science/\narticle/pii/B9780444817143500114.\nWoojeong Jin, Subhabrata Mukherjee, Yu Cheng, Yelong Shen, Weizhu Chen, Ahmed Hassan\nAwadallah, Damien Jose, and Xiang Ren. Grill: Grounded vision-language pre-training via align-\ning text and image regions. arXiv preprint arXiv:2305.14676, 2023.\nJustin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and\nRoss Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual\nreasoning, 2016.\n11\nPreprint\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting lan-\nguage and vision using crowdsourced dense image annotations. International journal of computer\nvision, 123:32\u201373, 2017.\nXin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Rea-\nsoning segmentation via large language model, 2023.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In International Conference\non Machine Learning, pp. 12888\u201312900. PMLR, 2022.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models, 2023.\nYue Liao, Aixi Zhang, Miao Lu, Yongliang Wang, Xiaobo Li, and Si Liu. Gen-vlkt: Simplify asso-\nciation and enhance interaction understanding for hoi detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 20123\u201320132, 2022.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pp. 740\u2013755. Springer, 2014.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023a.\nNan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, and Antonio Torralba. Unsupervised compo-\nsitional concepts discovery with text-to-image generative models, 2023b.\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection. arXiv preprint arXiv:2303.05499, 2023c.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nZixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna.\nCrepe: Can vision-language foundation models reason compositionally? In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10910\u201310921, 2023.\nJunhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\nGeneration and comprehension of unambiguous object descriptions. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 11\u201320, 2016.\nRichard Montague. Universal grammar. Theoria, 36(3):373\u2013398, 1970. doi: 10.1111/j.1755-2567.\n1970.tb00434.x.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million\ncaptioned photographs. Advances in neural information processing systems, 24, 2011.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.\nKosmos-2: Grounding multimodal large language models to the world, 2023.\nRenjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han,\nHang Xu, Lingpeng Kong, and Tong Zhang. Detgpt: Detect what you need via reasoning, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision, 2021.\nArijit Ray, Filip Radenovic, Abhimanyu Dubey, Bryan A. Plummer, Ranjay Krishna, and Kate\nSaenko. Cola: A benchmark for compositional text-to-image retrieval, 2023.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object\ndetection with region proposal networks, 2016.\n12\nPreprint\nKuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas\nPfister. Pic2word: Mapping pictures to words for zero-shot composed image retrieval, 2023.\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n2556\u20132565, 2018.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Mar-\ncus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n15638\u201315650, 2022.\nSanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, and Anna\nRohrbach. Reclip: A strong zero-shot baseline for referring expression comprehension. arXiv\npreprint arXiv:2204.05991, 2022.\nWenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong\nLu, Jie Zhou, Yu Qiao, and Jifeng Dai. Visionllm: Large language model is also an open-ended\ndecoder for vision-centric tasks, 2023.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen\nHu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian\nQi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with\nmultimodality, 2023.\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context\nin referring expressions. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amster-\ndam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 69\u201385. Springer, 2016.\nHangjie Yuan, Jianwen Jiang, Samuel Albanie, Tao Feng, Ziyuan Huang, Dong Ni, and Mingqian\nTang. Rlip: Relational language-image pre-training for human-object interaction detection. Ad-\nvances in Neural Information Processing Systems, 35:37416\u201337431, 2022.\nHangjie Yuan, Shiwei Zhang, Xiang Wang, Samuel Albanie, Yining Pan, Tao Feng, Jianwen Jiang,\nDong Ni, Yingya Zhang, and Deli Zhao. Rlipv2: Fast scaling of relational language-image pre-\ntraining. arXiv preprint arXiv:2308.09351, 2023.\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and\nwhy vision-language models behave like bags-of-words, and what to do about it? In The Eleventh\nInternational Conference on Learning Representations, 2022.\nAixi Zhang, Yue Liao, Si Liu, Miao Lu, Yongliang Wang, Chen Gao, and Xiaobo Li. Mining the\nbenefits of two-stage and one-stage hoi detection. Advances in Neural Information Processing\nSystems, 34:17209\u201317220, 2021.\nShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and\nPing Luo. Gpt4roi: Instruction tuning large language model on region-of-interest, 2023.\nTiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, and\nJianwei Yin. Vl-checklist: Evaluating pre-trained vision-language models with objects, attributes\nand relations, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models, 2023.\n13\nPreprint\nA\nAPPENDIX\nA.1\nPRE-TRAINING DETAILS\nApart from the grounded image-text pair dataset we created, we also use The Pile (Gao et al., 2020)\nas part of our pre-training dataset. The total pre-training loss consists of the language modeling loss\nand the detection loss, with a loss weight of 0.025 for the detection loss. We pre-train for 20k steps\nand use a batch size of 2,304 for grounded image-text data and a batch size of 2,304 for The Pile\ndata. AdamW (Loshchilov & Hutter, 2017) optimizer is employed with a learning rate of 1.0e\u22124\nand \u03b2 = (0.9, 0.999). We do not apply weight decay to the weights of LLM and CLIP, but apply a\nweight decay of 0.05 to the detection network.\nA.2\nCOMPOSITIONAL EVALUATION DATASET\nWe evaluate the compositional ability of the model on three compositional reasoning tasks. We\nillustrate these three tasks in Figure 4. For ARO dataset, given a subject entity and the relationship,\nthe model is required to predict the object entity according to the image information. For Cola\ndataset, the model is required to match a correct image to the caption by recognizing entities with\nvarious attributes and reasoning the relation between entities. For HICO-DET, the model is required\nto recognize all human-object interaction categories in an image and also localize both the human\nand the interacted object.\nARO\nCompositional Entity Prediction\nHICO-DET\nHuman-Object Interaction \nDetection\nCola\n Compositional Text-to-image Retrieval\n\u00fb\n\u00fc\nthe horse is eating the grass\n<person, straddle, horse>\ndecorative blue colorful painted large \noblong vase on black round table\nFigure 4: Explanation of three evaluated compositional reasoning tasks.\nA.3\nVISULIZATION OF COMPOSITIONAL REASONING RESULTS\nWe show the visualization results for our model, BLIP-2 and KOSMOS-2 on the three compositional\nreasoning tasks.\nFigure 5 shows the quantitative results for ARO. Compared to BLIP-2 and KOSMOS-2, we can rank\nthe ground truth object higher thanks to the <previsual> token to localize the object.\nFigure 6 shows the quantitative results for Cola. The presence of the <prebox> token can encode\nthe visual feature of the ROI, helping the model better understand the attribute of the objects due to\nits zoom-in effect. Also, relationship can be explicitly used by <previsual> token to better infer\nthe visual entity.\nFigure 7 shows the comparison of our model and KOSMOS-2 on HICO-DET task. In general our\nmodel gives more accurate bounding box for object localization. Also, when there are two identical\nobjects in the image, i.e., the bottom-right example in Figure 7, our model can take advantage of\nthe <previsual> token which can guide our model to pay attention to the region of the correct\nobject (the bench that the person is lying on), instead of the wrong object (the bench that behind the\nperson).\n14\nPreprint\nthe man is to the right of  [the fence] \nRanking in Candidate Outputs\nthe street is to the left of [the wall]\nBLIP2\n10\nKosmos-2 33\nOurs \n1\nRanking in Candidate Outputs\nBLIP2\n3\nKosmos-2 17\nOurs \n3\nthe chair is to the left of  [the pot]\nRanking in Candidate Outputs\nBLIP2\n12\nKosmos-2 27\nOurs \n1\nthe gym is to the right of [the trees]\nRanking in Candidate Outputs\nBLIP2\n53\nKosmos-2 43\nOurs \n0\nthe train is below [the steam]\nRanking in Candidate Outputs\nBLIP2\n29\nKosmos-2 11\nOurs \n4\nthe bathtub is near [the wallpaper]\nRanking in Candidate Outputs\nBLIP2\n62\nKosmos-2 23\nOurs \n0\nFigure 5: Quantitative results on ARO.\nFigure 6: Quantitative results on Cola.\nperson        washes        bus\nOurs\nperson         lies on        bench\nGT\nOurs\nperson        blows \ncake\nGT\nKosmos-2\nOurs\nGT\nKosmos-2\nperson         eats at        dining table\nOurs\nGT\nKosmos-2\nKosmos-2\nFigure 7: Quantitative results on HICO-DET.\n15\n"
  },
  {
    "title": "MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning",
    "link": "https://arxiv.org/pdf/2311.02303.pdf",
    "upvote": "4",
    "text": "MFTCODER: BOOSTING CODE LLMS WITH MULTITASK\nFINE-TUNING\nA PREPRINT\nBingchang Liu, Chaoyu Chen, Zi Gong, Cong Liao, Huan Wang, Zhichao Lei, Ming Liang,\nDajun Chen, Min Shen, Hailian Zhou, Wei Jiang, Hang Yu\u2217, Jianguo Li\u2217\nAnt Group, China\nNovember 7, 2023\nABSTRACT\nCode LLMs have emerged as a specialized research field, with remarkable studies dedicated to\nenhancing model\u2019s coding capabilities through fine-tuning on pre-trained models. Previous fine-\ntuning approaches were typically tailored to specific downstream tasks or scenarios, which meant\nseparate fine-tuning for each task, requiring extensive training resources and posing challenges in\nterms of deployment and maintenance. Furthermore, these approaches failed to leverage the inherent\ninterconnectedness among different code-related tasks. To overcome these limitations, we present\na multi-task fine-tuning framework, MFTCoder, that enables simultaneous and parallel fine-tuning\non multiple tasks. By incorporating various loss functions, we effectively address common chal-\nlenges in multi-task learning, such as data imbalance, varying difficulty levels, and inconsistent\nconvergence speeds. Extensive experiments have conclusively demonstrated that our multi-task\nfine-tuning approach outperforms both individual fine-tuning on single tasks and fine-tuning on\na mixed ensemble of tasks. Moreover, MFTCoder offers efficient training capabilities, including\nefficient data tokenization modes and PEFT fine-tuning, resulting in significantly improved speed\ncompared to traditional fine-tuning methods. MFTCoder seamlessly integrates with several main-\nstream open-source LLMs, such as CodeLLama and Qwen. Leveraging the CodeLLama foundation,\nour MFTCoder fine-tuned model, CODEFUSE-CODELLAMA-34B, achieves an impressive pass@1\nscore of 74.4% on the HumaneEval benchmark, surpassing GPT-4 performance (67%, zero-shot).\nMFTCoder is open-sourced at https://github.com/codefuse-ai/MFTCOder\nKeywords Large Language Model \u00b7 Code Generation \u00b7 Multi-task Learning\n1\nIntroduction\nThe paradigm-shifting emergence of ChatGPT2, powered by both GPT-3.5 and GPT-4 OpenAI (2023), has set ablaze\nthe landscape of research and development in the realm of large language models (LLMs). This breakthrough has\nfurther sparked the interest in leveraging LLMs for code understanding and generation, commonly referred to as Code\nLLMs. By pretraining on extensive code data sources such as the Github public data, these Code LLMs can acquire\ncomprehensive contextual representations that can be applied to various code-related tasks .\nWhile the pretraining stage of (Code) LLMs seek to ensure their generalizability to different downstream tasks, the\nsubsequent finetuning stage typically only adapt the (Code) LLMs to a specific task or a scenario. However, this\napproach overlooks two critical challenges. Firstly, it involves resource-intensive individual finetuning of large\nLLMs for each task, which hinders efficient deployment in production. Secondly, the interrelated nature of code\ndomain tasks suggests that joint finetuning can enhance performance compared to separate finetuning. It is\ntherefore imperative to conduct multitask finetuning, enabling simultaneous handling of all tasks while leveraging the\nstrengths of related tasks to enhance performance.\n\u2217Corresponding Author: {hyu.hugo, lijg.zero}@antgroup.com\n2https://openai.com/blog/chatgpt\narXiv:2311.02303v1  [cs.LG]  4 Nov 2023\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nAs an illuminating example, suppose we have two related tasks: code completion and code summarization. Code\ncompletion involves predicting the next line of code based on a partial code snippet, while code summarization aims\nto generate a concise human-readable summary of a given code snippet. Traditionally, separate models would be\nfine-tuned for each task, resulting in resource-intensive duplication. However, code completion and code summarization\nhave inherent connections. Completion of a code snippet relies on understanding the overall functionality and purpose,\nwhile generating an accurate summary requires comprehending the structure, dependencies, and intended functionality.\nBy employing multitask learning, a single model can be trained to jointly learn both tasks, leveraging shared knowledge\nand patterns, leading to improved performance on both tasks. The model understands the contextual dependencies\nbetween code elements, aiding in predicting the next snippet and generating informative summaries. Furthermore,\nmultitask learning offers additional benefits beyond individual task performance: the shared representation between\ntasks helps mitigate overfitting, promote better generalization, and enhance the model\u2019s ability to handle data scarcity\nfor specific tasks. If code completion has a larger training dataset than code summarization, the model can leverage the\nabundance of completion data to enhance performance in summarization, effectively addressing data scarcity challenges.\nMultitask learning even enables the model to handle unseen but related tasks without specific training data. Overall,\nmultitask learning allows models to jointly learn multiple related tasks, benefiting from shared knowledge, improving\nperformance, enhancing generalization, and handling data scarcity.\nDespite the importance of multitask learning for finetuning, only a handful of existing studies have explored this approach\nin the domain of NLP Raffel et al. (2023); Aghajanyan et al. (2021); Aribandi et al. (2022). These studies incorporate\nmulti-task data and merge it for large-scale model learning, without explicitly separating the tasks. Unfortunately, these\nstudies tend to prioritize tasks with larger sample sizes, disregarding tasks with smaller sample sizes. Furthermore, they\nfail to ensure equal convergence speed among tasks, leading to over-optimization of some tasks and under-optimization\nof others.\nIn this paper, we focus on multitask fine-tuing (MFT) of (Code) LLMs, in order to guarantee equitable attention to\ntasks with varying sample sizes and approximately similar optimization progress. In particular, our attention is on\nCode LLMs, as code domain tasks often exhibit correlations, and so we name our approach MFTCoder. We emphasize\nthat MFTcoder can be extended to an arbitrary set of related-NLP tasks in a straighforward manner. To enhance\nthe efficiency of MFTCoder, we incorporate parameter-efficient fine-tuning techniques, including LoRA Hu et al.\n(2021) and QLoRA Dettmers et al. (2023). Experimental results demonstrate that multi-task models trained using the\nMFT approach outperform those fine-tuned individually for each task or by merging data from multiple tasks. We\nfurther validate the effectiveness of MFTCoder on various baseline pretrained LLMs, such as Qwen Bai et al. (2023),\nBaichuan Baichuan (2023), Llama Touvron et al. (2023a), Llama 2 Touvron et al. (2023b), StarCoder Li et al. (2023a),\nCodeLLama Rozi\u00e8re et al. (2023), and CodeGeex2 Zheng et al. (2023). Remarkably, when applying MFTCoder to\nthe CodeLlama-34B-Python Rozi\u00e8re et al. (2023) base model, it achieves a pass@1 score of 74.4% on the humanEval\nevaluation dataset, even surpassing the performance of GPT-4 (67%, zero-shot) OpenAI (2023).\nThe main contributions of this paper can be summarized as follows:\n\u2022 We propose MFTCoder, a novel multitask finetuning approach for concurrently adapting LLMs to multiple code-\nrelated tasks. Our focus is on addressing the issues of data balance and convergence speed that commonly arise in\nprevious multitask finetuning methods.\n\u2022 We validate MFTCoder on various baseline pretrained models, including Qwen Bai et al. (2023), Baichuan Baichuan\n(2023), Llama Touvron et al. (2023a), Llama 2 Touvron et al. (2023b), StarCoder Li et al. (2023a), CodeLLama Rozi\u00e8re\net al. (2023), CodeFuse Di et al. (2023), and CodeGeex2 Zheng et al. (2023), demonstrating its compatibility with\ndifferent baseline models.\n\u2022 Extensive experiments show that the MFT approach outperforms individual fine-tuning for each task or data merging\nfrom multiple tasks. Notably, when implementing MFTCoder with the CodeLlama-34B-Python Rozi\u00e8re et al. (2023)\nbase model, it achieves an impressive pass@1 score of 74.4% on the humanEval evaluation dataset, surpassing the\nperformance of GPT-4 (67%, zero-shot) OpenAI (2023).\n2\nRelated Works\n2.1\nCode LLMs\nCoding capability serves as a critical criterion for evaluating general large language models (LLMs) in code-related\ntasks. Notable performance on the widely-used HumanEval dataset Chen et al. (2021), a benchmark for code generation,\nhas been observed across various models, including LaMDA Thoppilan et al. (2022), PaLM Chowdhery et al. (2022),\nPaLM 2 Anil et al. (2023), ChatGPT, and GPT-4 OpenAI (2023). In particular, GPT-4 has set a remarkable record of\n67.0% pass@1 score. However, their closed-source nature limits their availability and hinders further collaborative\n2\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nFigure 1: Overview of MFTCoder framework.\nadvancements. In contrast, recent open-source LLMs, including LLaMA Touvron et al. (2023a), LLaMA 2 Touvron et al.\n(2023b), Qwen Bai et al. (2023), and Phi-1.5 Li et al. (2023b), have demonstrated notable progress in code-related tasks,\nwith commentable scores of 23.7%, 29.9%, 32.3%, and 41.4% respectively. Despite this progress, their performance\nstill lags behind the state-of-the-art closed-source models.\nOn the other hand, LLMs specifically designed for code-related tasks, often referred to as code LLMs, have also\nundergone significant developments.\nAlongside closed-source Code LLMs such as Codex Chen et al. (2021),\nCode-Davinci Chen et al. (2021), AlphaCode Li et al. (2022), PaLM-Coder Chowdhery et al. (2022), and PanGu-\nCoder Christopoulou et al. (2022), open-source alternatives like including SantaCoder Allal et al. (2023), Phi-1.0 Gu-\nnasekar et al. (2023), CodeGeeX-2 Zheng et al. (2023), StarCoder Li et al. (2023a), Code LLaMA Rozi\u00e8re et al.\n(2023) have showcased competitive performance with their closed-source counterparts. Notably, CodeLLama-34B-\nPython Rozi\u00e8re et al. (2023) obtains a score of 53.7% on HumanEval. Apart from pretraining, another intriguing\napproach to further enhancing Code LLMs is instruction fine-tuning, as showcased by CodeT5+ Wang et al. (2023),\nPhi-1.0 Gunasekar et al. (2023), OctoPack Muennighoff et al. (2023), and WizardCoder Luo et al. (2023). By leveraging\ncarefully curated high-quality instruction datasets, these methods exhibit the potential of fine-tuning to enhance code\ngeneration capabilities.\n2.2\nMultitask Learning\nMultitask learning (MTL) Caruana (1997); Crawshaw (2020) is a potent approach in machine learning that holds\nsignificant promise for enhancing model performance and addressing diverse challenges Crawshaw (2020). By training\na single model on multiple related tasks, MTL enables the model to leverage shared knowledge and patterns, leading to\nenhanced generalization and improved accuracy. MTL methods can be categorized into two groups: hard parameter\nsharing Zhao et al. (2018); Liu et al. (2019b,a); Kendall et al. (2018); Liu et al. (2019c); Chen et al. (2018); Jean\net al. (2019) and soft parameter sharing Duong et al. (2015); Yang and Hospedales (2017); Long et al. (2017); Lee\net al. (2018); Sun et al. (2020); Pascal et al. (2021). Hard parameter sharing involves sharing model weights between\ntasks, while soft parameter sharing incorporates task-specific models with separate weights. In the context of large\nlanguage models (LLMs), hard parameter sharing is particularly relevant, since the large number of parameters in LLMs\nfacilitates their ability to handle multiple related tasks with a common set of parameters. As a result, optimizing an\nLLM to effectively tackle multiple tasks lies at the heart of MTL for LLMs. In recent years, notable advancements have\nbeen made in MTL techniques. Google introduced T5 Raffel et al. (2023) in 2020, where they explored the application\nof MTL techniques. Meta further introduced Mupper Aghajanyan et al. (2021) in 2021, which applies multi-task\nlearning between pretraining and fine-tuning, termed as pre-fine-tuning (PFT). They discovered that incorporating this\nstep enhances the performance of the pretrained model across various downstream tasks and significantly improves\nthe speed of fine-tuning. However, if the number of tasks in PFT is too small, it can have a negative impact on the\nmodel\u2019s performance. Therefore, it is recommended to have a minimum of 15 tasks for optimal results. Building upon\nT5, Google introduced ExT5 Aribandi et al. (2022), which increased the number of tasks to 107. They found that as\nlong as the number of tasks in pretraining is sufficiently large, even if there may be mutual interference among tasks,\nthe ultimate results are still remarkably good. Ultimately, ExT5 outperformed T5 across multiple metrics. It is worth\nnoting that these studies mainly focused on incorporating multi-task data and merging it for the large model to learn,\nwithout explicitly segregating the tasks. While these approaches have shown promising results, they tend to overlook\ndata imbalance and convergence speed issues that often arise in MTL. In this paper, we address these challenges and\npropose MFTCoder, a multitask finetuning approach for LLMs that tackles these problems effectively.\n3\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nFigure 2: Data Generation Approach for Code Exercises Datasets using Single-turn Conversation Scheme.\n3\nApproach\nIn this section, we will introduce our multi-task fine-tuning framework, MFTCoder 3, along with the design of its key\ncomponents.\n3.1\nMFT Framework\nMFTCoder aims to seamlessly adapt LLMs to diverse new scenarios while maximizing their performance within a\nspecific context. When applying MFTCoder to a new scenario, the initial step involves decomposing the scenario into\nsmaller tasks that correspond to targeted abilities. For instance, in the domain of code LLMs, the overarching objective\nof enhancing models\u2019 code capabilities can be further divided into specific tasks like code completion, text-to-code\ngeneration, unit test case generation, code repair, code debugging, and even cross-language translation. Our extensive\npractical experience has demonstrated that MFTCoder effectively handles multi-task scales ranging from single to\ndozens or even hundreds of tasks. Each task necessitates the collection and organization of fine-tuning datasets. However,\ndata collection for certain tasks can pose challenges. To overcome this, MFTCoder leverages Self-Instruct Wang\net al. (2022) techniques and Agents to generate instruction datasets. With the capability to concurrently fine-tune\nmultiple downstream tasks, MFTCoder effectively handles substantial volumes of fine-tuning data, ensuring efficient\ntraining. It incorporates two efficient data tokenization modes and implements PEFT (Parameter-Efficient Fine-Tuning)\ntechniques to enhance training efficiency. In the realm of multi-task learning, MFTCoder confronts the issue of task\nimbalances, encompassing imbalanced data distribution, varying task difficulties, and divergent convergence rates. To\nmitigate these challenges, MFTCoder introduces or adapts different loss functions to achieve task balance. Recognizing\nthat different large-scale models possess distinct strengths and capabilities, MFTCoder facilitates the selection of\nsuitable model architectures based on specific scenarios to achieve optimal performance. It has been adapted to popular\nLLMs, including LLama Touvron et al. (2023a), LLama 2 Touvron et al. (2023b), CodeLLama Rozi\u00e8re et al. (2023),\nQwen Bai et al. (2023), Baichuan 1/2 Baichuan (2023), ChatGLM 2 Du et al. (2022), CodeGeeX 2 Zheng et al. (2023),\nGPT-NEOX Black et al. (2022), CodeFuse Di et al. (2023), StarCoder Li et al. (2023a), AntLLM, and more. We\ncontinuously update and expand the compatibility with additional models.\nThe overall framework of MFTCoder is illustrated in Figure 1. In the subsequent sections, we will provide a more\ndetailed exploration of these components, including instruction datasets construction, efficient tokenization modes,\nPEFT fine-tuning and balanced loss functions.\n3.2\nInstruction Dataset Construction\nFor tasks with challenging data collection, We employ the Self-Instruct Wang et al. (2022) technique to generate\nfine-tuning data for downstream code-related tasks in MFTCoder. This involves providing customized prompts to\nGPT-3.5 or GPT-4 that clearly describe our instruction generation requirements, thereby generating instructional data.\nFurthermore, we drew inspiration from the Textbook approach employed in the PHI work Gunasekar et al. (2023),\nincorporating the self-instruct technique to generate Code Exercises datasets for downstream code-related tasks.\nIn terms of specific implementation, we have two options. One is the Agents multi-turn conversation approach achieved\nthrough Camel Li et al. (2023c), and the other is the single-turn conversation method by directly invoking the ChatGPT\n3https://github.com/codefuse-ai/MFTCoder\n4\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nAPI. In our multi-turn approach, we employ Camel to launch two agents, each assigned specific roles and task themes,\nfacilitating a dialogue between them to generate instructional data aligned with the given theme. For instance, when\ngenerating Python exercise data, we designate the roles of \u2019teacher\u2019 (simulating the user role of ChatGPT) and \u2019student\u2019\n(simulating the assistant role of ChatGPT) for the agents. The teacher\u2019s responsibility is to provide exercise instructions\nto the student, while the student\u2019s task is to offer corresponding solutions to those instructions. This iterative process\ncontinues, generating multiple exercise questions, until the task requirements are met or the maximum input length of\nChatGPT is reached. To accommodate ChatGPT\u2019s input length limitation, we cannot directly utilize a large question as\nthe task theme. For instance, when creating Python exercise questions to assess students\u2019 proficiency, we break down\nthe main theme into smaller Python knowledge points (e.g. binary search tree) and initiate separate Camel sessions for\neach knowledge point. For a concrete example, please refer to Appendix A.\nThe multi-turn approach provides high automation but can be costly due to the need for maintaining two agents, each\nmaking multi-turn calls to the ChatGPT API. To mitigate this, we propose a more cost-effective single-turn conversation\ngeneration approach, and the overall process is illustrated in Figure 2. We begin by creating an initial set of seeds, such\nas hundreds of Python knowledge points. These seeds are then combined with prepared fixed prompt templates to\ngenerate a set of patterned task prompts. To address the issue of reduced diversity caused by fixed templates and to\nensure accurate prompt descriptions, we utilize Camel\u2019s task prompt refinement feature to obtain precise and diverse\ntask prompts. Each task prompt is used to generate a set of instructions related to the corresponding seed (e.g. exercise\nproblems related to binary search trees). Using ChatGPT, we generate the corresponding solutions for the generated\ninstructions. Finally, we assemble and deduplicate the instructions with their respective solutions to obtain an exercise\ndataset. We have open-sourced a Python Code Exercises dataset 4 constructed using this approach.\n3.3\nEfficient Tokenization Modes\nTokenization is an essential step in the pre-training and fine-tuning of LLM models, where input and output texts are\nsplit into smaller units to be processed. It, along with the loss function, effectively defines how the data is utilized\nduring the training process, thus playing a crucial role in both the model\u2019s effectiveness and training efficiency. In the\ntypical SFT tokenization scheme, samples within the same batch are aligned to the maximum input length (seq-length)\nof the model with extra padding tokens, shown as Figure 3a. However, in practice, we have found that this approach\nresults in a high proportion of padding tokens. For example, when using the CodeFuse-13B Di et al. (2023) tokenizer to\nprocess 35 downstream tasks, the average proportion of padding tokens is 92.22% (with seq-length set to 4096). This\nmeans a significant number of tokens are used solely for alignment purposes, providing no value to the training process.\nThis results in lower training efficiency and wastage of offline tokenization storage space. To address this issue, we\nhave adopted and optimized two tokenization modes, namely dynamic padding and pack modes.\nIn dynamic padding mode, the micro batch window size of each GPU is determined by the maximum sample length in\nthe micro batch. Shorter samples are padded with additional padding tokens to match this size, as shown in Figure 3b.\nAlthough padding tokens do not affect the model\u2019s training effectiveness, they add computational overhead during\ntraining, impacting the training speed. Dynamic padding mode effectively reduces the proportion of padding tokens\nused, leading to faster training. In our experience, this approach can yield approximately a twofold speed improvement\ncompared to the traditional SFT tokenization mode (actual improvement depends on the dataset). It\u2019s important to note\nthat this mode is suitable for online tokenization scenarios only.\nWhile the dynamic padding mode reduces the micro batch window size, the pack mode, similar to Llama 2\u2019s SFT\ntokenization mode Touvron et al. (2023b), maximizes the utilization of the model\u2019s maximum input window length\n(seq-length). In the pack mode, multiple fine-tuning samples are sequentially packed into a window of seq-length,\nseparated by eos tokens, as shown in Figure 3c. In the figure, samples 1-4 of Figure 3a are combined and placed\nin one window after one another. If a sample cannot fit in the current window, it is placed in the next window with\npadding tokens filling the remaining space. For instance, in Figure 3c, sample 5 is placed in the second window with\npadding tokens, while sample 6 is accommodated in the third window. The pack mode, in comparison to the dynamic\npadding mode, offers even lower padding token ratio, resulting in improved training speed. Our practical experience\ndemonstrates that this approach reduces the average proportion of padding tokens to less than 10% among the 35\ntasks mentioned earlier, leading to a substantial boost in training speed while maintaining training effectiveness. It is\nimportant to highlight that MFTCoder supports both online and offline pack tokenization scenarios, serving not only the\nSFT phase but also the pre-training stages.\n4https://huggingface.co/datasets/codefuse-ai/CodeExercise-Python-27k\n5\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\n(a) Normal SFT Mode\n(b) Dynamic Padding Mode\n(c) Pack SFT Mode\nFigure 3: Illustration of the differences in sample organization within a batch between normal SFT, dynmaic padding and\nPack SFT tokenization modes. The light-colored squares in the figure represent the Prompt section of the samples, while\nthe dark-colored squares represent the Label section (participating in loss calculation). The blank squares represent\npadding section.\n3.4\nPEFT Efficient Fine-tuning\nThe prevalent large-scale models typically contain billions of parameters, while multi-task learning scenarios often\ninvolve numerous tasks, resulting in a substantial total number of fine-tuning samples. If we were to opt for full-fledged\nfine-tuning of these large models using a vast amount of data, two challenges would arise: firstly, the need for extensive\nstorage and computational resources; secondly, the potential risk of catastrophic forgetting during training. To address\nthese issues, MFTCoder incorporates the PEFT (Parameter-efficient fine-tuning) technique Houlsby et al. (2019),\nenabling efficient fine-tuning to be accomplished within a short timeframe and with minimal resource requirements.\nSpecifically, MFTCoder supports two PEFT methods: Lora (Large-scale Language Model Low-Rank Adaptation) Hu\net al. (2021) and QLora (Quantized Large-scale Language Model Low-Rank Adaptation) Dettmers et al. (2023). The\nfundamental concept of Lora is quite simple, as depicted in Figure 4. It involves adding an auxiliary branch to the\noriginal pretrained language model. During training, the parameters W \u2208 Rd\u00d7d of the original pretrained model remain\nfixed, while only the dimensional expansion matrix A \u2208 Rd\u00d7r and dimensional reduction matrix B \u2208 Rr\u00d7d within the\nauxiliary branch are trained. The matrix product BA is then added to the original model W, resulting in the newly\ntrained model. Due to the significantly smaller magnitude of r compared to d, the number of trainable parameters can\nbe dramatically reduced. Building upon LoRA, QLoRA incorporates a novel high-precision quantization technique\ncalled NF4 and dual quantization to quantize the pretrained model to 4 bits. It also introduces a small set of learnable\nlow-rank adapter weights. These weights are fine-tuned by optimizing the gradients through back-propagation of the\nquantized weights. As a result, QLoRA enables the fine-tuning of larger models using fewer GPU resources. As an\nexample, MFTCoder can fine-tune a 70B model on a single A100 with 80GB of VRAM.\n6\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nFigure 4: Visualizing the Essence of Lora\u2019s Basic Idea.\n3.5\nMultitask Fine-Tuning with Balanced Losses\nAs a multi-task learning framework, MFTCoder, as described in Section 2, faces a significant challenge of data\nimbalance, task heterogeneity, and varying convergence speeds. To address these challenges, MFTCoder incorporates a\nset of loss functions specifically designed to alleviate these imbalances.\nTo address the issue of data imbalance, we first ensure that all samples from all tasks are utilized exactly once within\na single epoch. To avoid the model favoring tasks with larger amounts of data, we introduce a weight assignment\nstrategy during loss computation. Specifically, we support two weight calculation schemes: one based on the number of\ntask samples and the other based on the number of valid tokens involved in the loss calculation. The former is more\nstraightforward, but it may perform poorly when dealing with tasks that have extreme differences in the number of valid\ntokens, such as binary classification tasks like \"yes\" or \"no\" answering or single-choice exam tasks. On the other hand,\nthe latter weight assignment scheme based on the actual number of valid tokens involved in the loss calculation can\nmitigate these issues. The specific formulation for weighted loss calculation is shown in Equation 1. In Equation 1, N\nrepresents the total number of tasks, Mi denotes the number of samples for the i-th task, Tij signifies the count of valid\ntokens (i.e., tokens involved in loss calculation) for the j-th sample of the i-th task, and tijk refers to the k-th valid token\nof the j-th sample for the i-th task.\nL(\u03b8) = min\n\u03b8\n1\nN\nN\nX\ni=1\nPMi\nj=1\nPTij\nk=1 \u2212 log(p\u03b8(tijk))\nPMi\nj=1 Tij\n(1)\nTo address the issue of task heterogeneity, we drew inspiration from the focal loss approach and incorporated it into\nMFTCoder. We implemented two different levels of focal loss functions to cater to different granularities. One operates\nat the sample level, as shown in Equation 2, while the other operates at the task level, as shown in Equation 3.\nL2(\u03b8) = min\n\u03b8\nPN\ni=1\nPMi\nj=1 \u2212\u03b1i \u2217 (1 \u2212 Pij)\u03b3 \u2217 Qij\nPN\ni=1 Mi\n, Pij = 1\nTij\nTij\nX\nk=1\nPijk, Qij = 1\nTij\nTij\nX\nk=1\nlog(Pijk)\n(2)\nL3(\u03b8) = min\n\u03b8\n1\nN\nN\nX\ni=1\n\u2212\u03b1i \u2217 (1 \u2212 Pi)\u03b3 \u2217 Qi, Pi =\n1\nMi\nMi\nX\nj=1\n1\nTij\nTij\nX\nk=1\nPijk, Qi =\n1\nMi\nMi\nX\nj=1\n1\nTij\nTij\nX\nk=1\nlog(Pijk)\n(3)\nTo address the issue of inconsistent convergence speeds, we drew inspiration from the FAMO Liu et al. (2023) approach\nand innovatively applied it to calculate the validation loss. Firstly, we assumed that each task, indexed by i, has its\nown original loss Li(\u03b8). In the t-th iteration, we updated the weights of each task based on the gradients of their\ncorresponding validation losses, aiming to maximize the weight wi for the task with the slowest convergence speed,\nshown as Equation 4. Here, gt represents the gradient of the weighted validation loss for all tasks, ci(\u03b1, gt) denotes the\nslope (gradient) of the validation loss for the i-th task, \u03b8t denotes the parameters of the network in the t-th iteration, \u03b1\nis the learning rate, and \u03f5 is a small constant to prevent division by zero. Furthermore, we would like to provide further\nexplanation on how we achieve balanced convergence. To ensure that tasks converge at a similar pace, we introduce a\n7\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nTable 1: Various experimental models and their corresponding training data.\nExperimental Model\nTask\nDesired Ability\n#Samples\n#Samples\nafter packing\nSFT-S-CODECOMPLETION\nCODE-COMPLETION\nCode Completion\n192,547\n18,811\nSFT-S-TEXT2CODE\nTEXT2CODE\nText-to-code Generation\n94,086\n14,399\nSFT-S-CODECOMMENT\nCODE-COMMENT\nCode Comments Generation\n645,711\n134,775\nSFT-S-CODETRANS\nCODE-TRANS\nCode Translation\n307,585\n71,573\nSFT-S-UNITTEST\nUNIT-TEST\nUnit test-case generation\n390,393\n77,681\nSFT-MIXED\nMix of the above 5 tasks\nAll of the above\n1,630,322\n317,239\nMFT-5TASKS\nThe above 5 tasks\nAll of the above\n1,630,322\n317,239\ndynamic balancing mechanism. At each iteration, we update the task-specific weights based on the gradients of their\nvalidation losses. This approach aims to give more importance to tasks with slower convergence speeds, allowing\nthem to have a larger influence on the overall optimization process. By dynamically adjusting the task weights, we\ncreate a balanced convergence scenario, where all tasks progress towards their optimal solutions at a similar rate. This\nmechanism effectively addresses the issue of disparate convergence speeds and enhances the overall stability and\nperformance of the MFTCoder framework.\nL4(\u03b8) = max\ngt min\ni\n1\n\u03b1ci(\u03b1, gt) \u2212 1\n2\u2225gt\u22252, gt =\nX\ni\nwi\nt\u2207Li(\u03b8t), ci(\u03b1, gt) = Li(\u03b8t) \u2212 Li(\u03b8t \u2212 \u03b1dt)\nLi(\u03b8t) + \u03f5\n(4)\nBy incorporating these different loss functions, MFTCoder effectively addresses the diverse requirements of various\nmultitask scenarios and alleviates the challenges of data imbalance, task heterogeneity, and inconsistent convergence\nspeeds typically encountered in existing large-scale MTL research. MFTCoder\u2019s flexible framework provides a robust\nsolution to these issues, empowering the development of more efficient and accurate multitask models.\n4\nEvaluation\nIn this section, we will conduct multiple sets of experiments using MFTCoder to validate the effectiveness and\nsuperiority of the MFT method. Specifically, we aim to address the following three research questions:\nRQ1: Does the MFT model, obtained by fine-tuning multiple tasks using MFT methodology, outperform the\nSFT-S(ingle) models, where each task is individually fine-tuned?\nRQ2: Does the MFT model outperform the SFT-Mixed model, where multiple tasks are combined and fine-tuned\nas one?\nRQ3: In terms of generalization to unseen tasks, does the MFT model outperform the SFT-Mixed model?\nNext, we will commence by presenting the experimental setup. Subsequently, we will showcase and delve into the\nexperimental results. Finally, we will culminate by summarizing and addressing the research questions raised in this\nsection.\n4.1\nEvaluation Setup\nTo address these three research questions, we selected 5 code-related downstream tasks and prepared the corresponding\nfine-tuning data, as shown in Table 1. Table 1 presents the desired enhancements (Column III) and the number of\nsamples (Column IV) for each task. For instance, the CODECOMPLETION-TASK aims to improve the model\u2019s code\ncompletion ability and includes 192,547 fine-tuning samples. The CODETRANS-TASK aims to enhance the model\u2019s code\ntranslation capability and consists of 307,585 fine-tuning samples. Thus, we trained 7 models (Column I), including\nindividual SFT-S-* models trained for each downstream task, a combined SFT-MIXED model for the 5 task data, and\nan MFT-5TASKS model trained using the MFT method.\nIn the experiment, all models were configured identically except for the training data. The base model for all models\nwas CodeLlama-13B-Python Rozi\u00e8re et al. (2023). Each model was trained using 16 A100 GPUs (with 80GB VRAM),\na micro batch size of 8, and a global batch size of 128. The Adam optimizer Kingma and Ba (2017) was used with an\n8\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\ninitial learning rate of 2e-4, and a minimum learning rate of 1e-5. We employed the QLora-INT4 mode of MFTCoder\nfor fine-tuning, with a consistent fine-tuning parameter proportion of 2.52%. The positions and initial values of the\ntrainable parameters were also the same. All models incorporate the Data-Balance Loss (i.e., Equation 1) and employ\npack tokenization mode. Notably, when there is only one task, this loss function aligns with the conventional loss\nemployed in standard GPT model pre-training. To determine the convergence point for each model, we trained them\nuntil the validation loss surpassed the loss from the current epoch for the next two consecutive epochs. This mechanism,\nknown as early-stopping strategy, was employed to ensure optimal convergence for each model.\n4.2\nEvaluation Datasets\nIn this paper, we utilized publicly available and representative code assessment benchmarks for comparative evaluation.\nThese benchmarks include:\n\u2022 HumanEval Chen et al. (2021) is a widely used Python code completion evaluation dataset, meticulously curated by\nresearchers at OpenAI.\n\u2022 HumanEval-X Zheng et al. (2023) is an extension of HumanEval, translated into multiple programming languages,\nenabling multi-language code completion evaluation.\n\u2022 DS-1000 Lai et al. (2022) focuses on assessing a model\u2019s ability to perform data science analysis using Python code,\ncovering essential libraries such as Numpy, Pandas, TensorFlow, Pytorch, Scipy, Sklearn, and Matplotlib.\n\u2022 MBPP Austin et al. (2021) comprises 1000 Python programming problems, constructed through crowdsourcing,\nprimarily targeting a model\u2019s proficiency in basic Python. In this study, we selected 500 problems with ID 11-510 from\nMBPP to evaluate the text-to-code generation capability, specifically generating code based on problem descriptions.\n\u2022 CodeFuseEval Di et al. (2023), building upon HumanEval and HumanEval-X, further extends the evaluation to\ninclude Chinese code completion (with Chinese docstrings), code translation, and unit test case generation capabilities,\nreferred to as CodeFuseEval-CN, CodeFuseEval-CodeTrans, and CodeFuseEval-UnitTest, respectively.\nThroughout these evaluation datasets, we employed \"pass@1\" as the evaluation metric in this paper.\n4.3\nEvaluation Results\nIn this section, we will showcase the evaluation results of seven trained models. For the SFT-S-* models, which\nwere trained individually for each task, we will focus on testing their specific target capabilities. For instance, we will\nexclusively evaluate the performance of the SFT-S-CODECOMPLETION model in the code completion task. On the\nother hand, for the SFT-MIXED and MFT-5TASKS models, we will assess their performance on each task and compare\nit with the corresponding SFT-S-* models. Specifically, we will conduct tests to evaluate the capabilities of code\ncompletion, text-to-code generation, code comment generation, code translation, and unit test case generation.\n4.3.1\nCode Completion\nFor code completion, we employed the HumanEval Chen et al. (2021) and HumanEval-X Zheng et al. (2023) evaluation\ndatasets to assess the model\u2019s performance. HumanEval is a widely-used benchmark dataset released by OpenAI\nspecifically designed to evaluate the Python code completion ability of large language models. HumanEval-X, on the\nother hand, is an expansion of HumanEval that enables the evaluation of large models\u2019 code completion performance\nacross various programming languages. Consistent with other studies, we employed the pass@1 metric as the evaluation\nmeasure.\nWe evaluated three models: SFT-S-CODECOMPLETION, SFT-MIXED, and MFT-5TASKS. The performance of these\nmodels on the HumanEval dataset is summarized in Table 2 (Column III). Results indicate that the MFT-5TASKS\nmodel, trained using the MFT approach, outperforms the other two models. It achieves a 2.44% higher performance\ncompared to the SFT-MIXED model, which was fine-tuned with mixed task data. It is worth noting that the SFT-MIXED\nmodel does not perform as well as the SFT-S-CODECOMPLETION model, which was trained individually for the\ncode-completion task.\nFurthermore, we conducted a multilingual evaluation on the HumanEval-X dataset for the three models, as presented in\nTable 3. The MFT-5TASKS model demonstrates superior performance in Java and Golang, while the SFT-MIXED\nmodel excels in C++ and JavaScript. Overall, the evaluation affirms that the MFT-5TASKS model outperforms the\nothers, with an average improvement of 1.22% over the SFT-MIXED model.\nOverall, in terms of code completion tasks, models trained using the MFT method outperform both individually\nfine-tuned models and models fine-tuned after combining multiple tasks.\n9\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nTable 2: Pass@1 performance on HumanEval (Code Completion) and MBPP (Text-to-Code Generation). We utilized\nthe greedy decoding strategy with zero-shot. The values of CodeLlama-Python-base are taken from Rozi\u00e8re et al.\n(2023).\nModel\nSize\nHumaneval\npass@1\nMBPP\npass@1\nAverage\nCodeLlama-Python-base Rozi\u00e8re et al. (2023)\n13B\n43.3%\n49.0%\n46.15%\nSFT-S-CODECOMPLETION\n13B\n59.76%\nNA\nNA\nSFT-S-TEXT2CODE\n13B\nNA\n54.2%\nNA\nSFT-MIXED\n13B\n57.93%\n53.6%\n55.765%\nMFT-5TASKS\n13B\n60.37%\n56.0%\n58.185%\nTable 3: Comparison of pass@1 Metric Performance on the multilingual HumanEval-X (zero-shot, greedy-decoding)\nTrained Model\nJava\nC++\nJavaScript\nGolang\nAverage\nCodeLlama-13B-Py-base\n43.3%\n41.46%\n34.76%\n38.41%\n29.27%\nSFT-S-CODECOMPLETION\n50.0%\n39.02%\n47.56%\n40.23%\n44.20%\nSFT-MIXED\n56.1%\n48.17%\n56.10%\n37.80%\n49.54%\nMFT-5TASKS\n57.32%\n46.34%\n54.27%\n45.12%\n50.76%\n4.3.2\nText-to-Code Generation\nTo evaluate the models\u2019 ability to generate code based on descriptions, we selected the MBPP Austin et al. (2021)\nevaluation dataset and used the pass@1 metric. MBPP is specifically designed to assess models\u2019 capacity to synthesize\nconcise Python programs from natural language descriptions.\nWe tested three models, namely SFT-S-TEXT2CODE, SFT-MIXED, and MFT-5TASKS, on the MBPP dataset,\nmeasuring their pass@1 performance as shown in Table 2 (Column IV). Among these models, MFT-5TASKS exhibited\nthe highest performance, surpassing the SFT-MIXED model by 2.4%. Similarly, in terms of the text-to-code generation\ntask, models fine-tuned after combining multiple tasks showed inferior performance compared to models fine-tuned\nspecifically for this individual task.\nOverall, in terms of text-to-code generation tasks, models trained using the MFT method outperform both\nindividually fine-tuned models and models fine-tuned after combining multiple tasks.\n4.3.3\nCode Comment Generation\nThe objective of the code comment generation task is to have models add necessary comments to the code without\nmodifying the input code itself. This includes both line comments and interface comments, making the code more\nreadable and user-friendly.\nTo assess this capability, we constructed an evaluation set based on 500 MBPP test questions (id xx-xx). For each\nquestion in the evaluation set, we had the SFT-S-CODECOMMENT, SFT-MIXED and MFT-5TASKS models generate\ncomments for it. Subsequently, we employed GPT-4 as the referee, which has been instructed with criteria for good\ncomments, to determine which model performed the best. If it was not possible to determine, the output was labeled as\nUNKNOWN. Finally, we counted the number of questions where each model was determined to perform the best and\ncalculated the corresponding proportions, shown in Table 4.\nIt can be observed that 38.8% of the questions were determined to be best performed by the MFT-5TASKS model,\nsurpassing the second-ranked SFT-MIXED by 7.4% and the third-ranked SFT-S-CODECOMMENT by 10.8%. Addition-\nally, 1.8% of the questions were marked as indeterminable by GPT-4. In summary, for this task, the models trained\nusing the MFT method exhibit the best performance.\n4.3.4\nCode Translation\nThe objective of the code translation task is to accurately and precisely translate a given code snippet implemented\nin the source language into an equivalent code snippet implemented in the target language while ensuring that both\nimplementations possess identical functionality. Here, we utilize the CODEFUSEEVAL 5 Di et al. (2023) evaluation\n5https://github.com/codefuse-ai/codefuse-evaluation\n10\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nTable 4: Performance Comparison of Three Models on Code Commenting Task. GPT-4 Determines the Best Performing\nModel for Each Question. This Table Presents the Proportion of Questions Where Each Model Performs the Best. In\nparticular, 1.8% of the evaluation cases were indeterminate for GPT-4 to determine the best-performing model.\nTraining Model\nBest identified by GPT-4\nSFT-S-CODECOMMENT\n28%\nSFT-MIXED\n31.4%\nMFT-5TASKS\n38.8%\nTable 5: Comparison of pass@1 Metric Performance on the CODEFUSEEVAL-CodeTranslation Di et al. (2023)\n(zero-shot, greedy-decoding)\nTraining Model\nPy2Java\nPy2C++\nJava2Py\nC++2Py\nJava2C++\nC++2Java\nAvg.\nSFT-S-CODETRANS\n59.52%\n57.40%\n70.73%\n62.20%\n67.07%\n62.80%\n63.29%\nSFT-MIXED\n80.16%\n71.20%\n67.68%\n72.56%\n65.85%\n82.31%\n73.29%\nMFT-5TASKS\n82.16%\n77.20%\n65.85%\n70.73%\n64.64%\n84.76%\n74.22%\ndatasets\u2019 code translation subset to support bidirectional translation between Java, Python, and C++. In order to evaluate\nthe accuracy and functional equivalence of the translation results, we employ test cases that are semantically equivalent\nto ones of the source program for each task. These test cases are used to verify whether the result code can run and pass\nsuccessfully, as indicated by the pass@1 criterion.\nThe test results of the three models are presented in Table 5. The MFT-5TASKS model performs the best in Python-\nto-Java, Python-to-C++, and C++-to-Java translations. The SFT-MIXED model excels in C++-to-Python translation,\nwhile the SFT-S-CODETRANS model performs the best in Java-to-Python and Java-to-C++ translations. Overall, the\nMFT-5TASKS model demonstrates superior performance, with an average improvement of 0.93% over SFT-MIXED\nand 10.9% over SFT-S-CODETRANS. This task also highlights the phenomenon that models trained using the\nMFT approach outperform the other two training methods.\n4.3.5\nUnit Test Case Generation\nThe task at hand is to generate unit test cases by training a model to produce a set of test cases for a given code snippet,\nsuch as a method or class, and verify if the provided code implementation is correct. We have opted to utilize the unittest\nsubset from the CODEFUSEEVAL Di et al. (2023) evaluation datasets as our test suite. We evaluate the test cases using\nthe pass@1 metric, which means that if a model generates test cases for a sample program and the sample program\npasses all the test cases, the count of correctly generated samples increases by 1. Similar to CodeLLama Rozi\u00e8re et al.\n(2023), we employ a greedy decoding strategy during the evaluation process.\nWe compared three models across Python, Java, and JavaScript for their test generation capabilities. The results in\nTable 6 indicate that the MFT-5TASKS model outperforms others in test generation for Python, with a 5.73% lead\nover the second-ranked SFT-MIXED model, and a significant 10.19% lead over the third-ranked SFT-S-UNITTEST\nmodel. In JavaScript, the MFT-5TASKS model also excels, with a 7.93% advantage over the other models. However, in\nJava, the MFT-5TASKS model performs 5.37% better than SFT-S-UNITTEST, but slightly lags behind SFT-MIXED\nby 5.44%. Overall, the MFT-5TASKS model consistently demonstrates the highest performance, with an average\nimprovement of 2.74% over SFT-MIXED, and a remarkable 7.83% improvement over SFT-S-UNITTEST.\nIn summary, training models using the MFT approach yielded better performance compared to models fine-tuned with\nthe same data mixed together. Furthermore, the performance of the MFT-trained models surpassed that of the model\nindividually fine-tuned for the UNIT-TEST task.\nTable 6: Comparison of pass@1 Metric Performance on the codefuseEVAL-TestcaseGeneration Di et al. (2023)\n(zero-shot, greedy-decoding)\nTrained Model\nPython\nJava\nJavaScript\nAverage\nSFT-S-UNITTEST\n33.76%\n32.43%\n41.46%\n35.88%\nSFT-MIXED\n38.22%\n43.24%\n41.46%\n40.97%\nMFT-5TASKS\n43.95%\n37.8%\n49.39%\n43.71%\n11\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nTable 7: Sample Text-to-SQL Test Cases from the Spider Yu et al. (2019b) dataset and Example Predictions by the\nSFT-Mixed Model and MFT-5Tasks Model.\nPROMPT:\n<|role_start|>system<|role_end|>You are a professional SQL engineer and you are writing SQL\nqueries for data query tasks.\n<|role_start|>human<|role_end|>What are the names of the heads who are born outside the\nCalifornia state?\nschema:head(head_ID|name|born_state|age\n<|role_start|>bot<|role_end|>\nLABEL:\nSELECT name FROM head WHERE born_state != \u2019California\u2019\nSFT-MIXED:\nYou can use the SQL SELECT statement to retrieve the names of the heads who are born outside\nthe California state. Here is a sample query:\n\u201c\u2018sql\nSELECT name\nFROM head\nWHERE born_state <> \u2019California\u2019;\n\u201c\u2018\nThis query selects the name column from the head table where the born_state column is not\nequal to \u2019California\u2019.\nMFT-5TASKS:\nSELECT name\nFROM head\nWHERE born_state <> \u2019California\u2019;\n4.3.6\nGeneralization on an Unseen Task\nIn addition to evaluating the performance of our models on tasks with training data to address RQ1 and RQ2, we also\nhave RQ3 to answer, which is whether the models trained using the MFT approach exhibit better generalization on\nunseen tasks compared to models trained by combining multiple datasets into a single SFT approach. To investigate\nthis, we selected the Text-to-SQL generation task as our testing target. The data for this task was not included in the\ntraining of the seven existing models. Furthermore, this task is code-related but distinctly different from the previous\nfive downstream tasks.\nWe have selected two evaluation metrics, BLEU score and logical accuracy of SQL statements. The BLEU score\nassesses the textual similarity between the generated outputs and the reference answers. The logical accuracy metric,\non the other hand, allows us to address the variations in SQL syntax that may occur. Specifically, Logical Accuracy\nmeasures the proportion of test samples in the dataset where the generated SQL statements are both syntactically correct\nand semantically equivalent to the reference answers.\nWe selected five representative text-to-SQL datasets, including WikiSQL Zhong et al. (2017), Spider Yu et al. (2019b),\nCSpider Min et al. (2019), CoSQL Yu et al. (2019a), and BirdSQL Li et al. (2023d), and randomly sampled 200\nexamples from each dataset for evaluation. The test case examples are shown in Table 7, where the first row demonstrates\nthe fine-tuned data format similar to OpenAI ChatML format 6. Using each sampled dataset, we tested the logical\naccuracy and BLEU score of the SFT-MIXED and MFT-5TASKS models, as shown in Table 8.\nAccording to Table 8, MFT-5TASKS outperforms SFT-MIXED in terms of BLEU scores on each dataset, averaging\n2.78 times higher. This indicates that the generated results of MFT-5TASKS exhibit higher similarity to the reference\nanswer texts. This similarity can also be observed in Table 7, where MFT-5TASKS produces cleaner results, while\nSFT-MIXED provides more explanations, which may be preferred in certain scenarios. Moreover, MFT-5TASKS\ndemonstrates better performance in terms of logical accuracy, achieving an overall accuracy that is 2.18 times higher\nthan SFT-MIXED model, and up to 4.67 times higher on the WikiSQL dataset. Numerically, MFT-5TASKS exhibits\nsuperior performance compared to SFT-MIXED, indicating stronger generalization of MFT-trained models on\nthe Text-to-SQL task, which is an unseen task during training.\n6https://github.com/openai/openai-python/blob/main/chatml.md\n12\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nTable 8: Comparison of generalization capabilities between MFT-5TASKS and SFT-MIXED on the Text-to-SQL task.\nThe evaluation metrics include SQL logical accuracy and BLEU score.\nTrained Model\nWIKISQL\nSPIDER\nCSPIDER\nCOSQL\nBiRDSQL\nAverage\nLogical Accuracy\nSFT-MIXED\n1.5%\n2.0%\n7.0%\n6.5%\n5.5%\n4.5%\nMFT-5TASKS\n7.0% (4.67x)\n4.5% (2.25x)\n16.5% (2.36x)\n10.5%(1.62x)\n10.5% (1.91x)\n9.8% (2.18x)\nBLEU\nSFT-MIXED\n0.032\n0.047\n0.025\n0.081\n0.026\n0.042\nMFT-5TASKS\n0.138\n0.138\n0.116\n0.119\n0.074\n0.117\n4.4\nEvaluation Summary\nWe selected five downstream tasks related to code and trained a total of seven models, including SFT-S-* models\nfine-tuned individually for each task, the SFT-MIXED model fine-tuned with a mixture of all task data, and the\nMFT-5TASKS model trained using the MFT method. We compared and tested the performance of each model in terms\nof their target capabilities. Additionally, we evaluated the generalization performance of the MFT method and the\nmixed SFT method on unseen tasks. The results can be summarized as follows:\ni Models trained with the MFT method outperformed those fine-tuned individually for each task, indicating a\npositive answer to RQ1.\nii Models trained with the MFT method outperformed those fine-tuned with a mixture of multiple tasks,\nproviding a positive answer to RQ2.\niii Models trained with the MFT method exhibit stronger generalization capabilities on new, unseen tasks\ncompared to the SFT models fine-tuned with a mixture of multiple task data.\n5\nApplication\nConsidering the outstanding performance of the MFT training method, we have leveraged our MFTCoder 3, developed\nbased on this approach, to fine-tune the existing mainstream open-source LLM models. e.g. QWen Bai et al. (2023),\nBaichuan Baichuan (2023), CodeGeex2 Zheng et al. (2023), Llama Touvron et al. (2023a), LLama2 Touvron et al.\n(2023b), CodeLLama Rozi\u00e8re et al. (2023), StarCoder Li et al. (2023a).\nMFTCoder supports Lora and QLora, which significantly reduces the number of model training parameters. Coupled\nwith dual quantization for model size compression, this ultimately leads to a substantial reduction in GPU memory\nrequirements. As a result, it becomes possible to fine-tune a 70B model on a single A100 GPU with ease. When\nfine-tuning these models using MFTCoder, we set the trainable parameters to be within the range of 0.1% to 5% of the\ntotal parameters. Through empirical evidence, we have found that as the proportion of trainable parameters increases,\nperformance improvement tends to plateau. In fact, we have observed that a trainable parameter proportion of less than\n5% is often sufficient to achieve performance levels close to that of full-scale fine-tuning.\nWhen fine-tuning these models, we configure them for multitasking with a range of 3-7 tasks. Depending on the model\nsize, we typically use Lora mode for models below 20B, and QLora mode for models larger than 20B. After fine-tuning,\nwe evaluate their performance in code completion and text-to-code generation tasks, measuring their performance\non HumanEval Chen et al. (2021) and MBPP Austin et al. (2021), as shown in Table 9 Column III and IV. We have\ncalculated the average improvement of MFT fine-tuning compared to the base models in terms of HumanEval and\nMBPP. As shown in column 5, the improvement ranges from 6.26% to 12.75%, with the improvements on HumanEval\nconsistently surpassing those on MBPP. Additionally, we have also evaluated the code completion performance of the\nMFTCoder fine-tuned models on the multilingual benchmark, HumanEval-X Zheng et al. (2023). The results of this\nevaluation are presented in Table 10. Notably, the fine-tuned CodeFuse-CodeLLama-Python-MFT (34B) achieved an\naverage pass@1 of 56.88% across four languages: Java, C++, JavaScript, and Golang.\nTable 9 also presents the performance of fine-tuned open-source models (e.g. OctoPack Muennighoff et al. (2023) and\nWizardCoder-Python Luo et al. (2023)) and representative closed-source models (e.g., Claude2 Anthropic (2023), GPT-\n4 OpenAI (2023)) on HumanEval and MBPP. It is worth noting that our fine-tuned model, CodeFuse-CodeLLama-\n34B 7, based on CodeLlama-34B-Python achieves a remarkable performance of 74.4% on HumanEval, surpassing\n7https://huggingface.co/codefuse-ai/CodeFuse-CodeLlama-34B\n13\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nTable 9: pass@1 performance on HumanEval Chen et al. (2021) (Code Completion) and MBPP Austin et al. (2021)\n(Text-to-Code Generation) after fine-tuning with MFTCoder across multiple mainstream open-source models. The\nCodeFuse-*-MFT models are evaluated using a combination of greedy decoding and zero-shot testing strategy, while\nthe metric values for the other models are taken from their respective papers, technical reports, or open-source project\nhomepages.\nModel\nSize\nHumaneval\npass@1\nMBPP\npass@1\nAverage\nOpen-source base models\nQWen-base Bai et al. (2023)\n14B\n32.3%\n40.8%\n36.55%\nLlama-base Touvron et al. (2023a)\n65B\n23.7%\n37.7%\n30.7%\nLlama2-base Touvron et al. (2023b)\n70B\n29.9%\n45.0%\n37.45%\nStarCoder-base Li et al. (2023a)\n15B\n33.6%\n52.7%\n43.15%\nCodeGeex2-base Zheng et al. (2023)\n6B\n35.9%\n42.4%\n39.15%\nCodeLlama-Python-base Rozi\u00e8re et al. (2023)\n13B\n43.3%\n49.0%\n46.15%\nCodeLlama-Python-base Rozi\u00e8re et al. (2023)\n34B\n53.7%\n56.2%\n54.95%\nMFT fine-tuned models\nCodeFuse-QWen-MFT 8\n14B\n48.78%\n43.8%\n46.29% (+9.74%)\nCodeFuse-Llama-MFT\n65B\n34.76%\n41.8%\n38.28% (+7.58)\nCodeFuse-Llama2-MFT\n70B\n40.85%\n40.8%\n40.83% (+3.38%)\nCodeFuse-StarCoder-MFT 9\n15B\n54.90%\n49.60%\n52.25% (+9.10%)\nCodeFuse-CodeGeex2-MFT\n6B\n45.12%\n46.2%\n45.66% (+6.51%)\nCodeFuse-CodeLlama-Python-MFT\n13B\n60.37%\n56.0%\n58.19% (+12.04%)\nCodeFuse-CodeLLama-Python-MFT 7\n34B\n74.4%\n61.0%\n67.70% (+12.75%)\nOpen-source fine-tuned models\nQWen-chat Bai et al. (2023)\n14B\n43.9%\n46.4%\n45.15%\nPHI-1 Gunasekar et al. (2023)\n1.3B\n50.6%\n55.5%\n53.05%\nOctoCoder Muennighoff et al. (2023)\n15B\n46.2%\nNA\nNA\nWizardCoder Luo et al. (2023)\n15B\n57.3%\n51.8%\n54.55%\nPhind-CodeLlama-v2 Phind (2023)\n34B\n71.95%\nNA\nNA\nWizardCoder-Python Luo et al. (2023)\n34B\n73.2%\n61.2%\n67.2%\nClosed-source models\nPanGu-Coder2 Shen et al. (2023)\n15B\n61.2%\nNA\nNA\nUnnatural CodeLlama Rozi\u00e8re et al. (2023)\n34B\n62.2%\n61.2%\n61.7%\nClaude2 Anthropic (2023)\nNA\n71.2%\nNA\nNA\nGPT-3.5 OpenAI (2023)\n175B\n48.1%\n52.2%\n50.15%\nGPT-4 (zero-shot) OpenAI (2023)\nNA\n67.00%\nNA\nNA\nall the listed models in the table, including GPT-4 (67.00%, zero-shot) OpenAI (2023). We also evaluated\nthe performance of the model on other benchmarks, including multilingual HUMANEVAL-X Zheng et al. (2023),\nMBPP Austin et al. (2021), DS-1000 Lai et al. (2022)and CODEFUSEEVAL Di et al. (2023), and compared it against\nGPT-3.5 and GPT-4, as shown in Figure 5. CodeFuse-CodeLLama-34B outperforms GPT-4 on CODEFUSEEVAL-\nUNITTEST and HUMANEVAL, matches its performance in code translation ability, but falls behind in Chinese code\ncompletion (CODEFUSEEVAL-CN), multi-language completion, data-science analysis (DS-1000), and text-to-code\ngeneration (MBPP) capabilities compared to GPT-4. However, it surpasses or equals GPT-3.5 on all evaluation datasets.\nThe input-output examples on each evaluation dataset can be found in Appendix C.\nFurthermore, we conducted an evaluation to assess the impact of fine-tuning the models with MFTCoder and code-\nrelated data on their performance in NLP tasks, as illustrated in Figure 6. Taking CODEFUSE-QWEN-14B as a case\nstudy, we compared it against the base model QWEN-14B and the official model QWEN-14B-CHAT fine-tuned by\nAlibaba Cloud on top of it. It is evident that CODEFUSE-QWEN-14B maintains its proficiency in NLP. In fact, it\nexhibits a slight enhancement in language, reasoning, and understanding abilities compared to the other two models.\nHowever, there is a minor decline in its examination ability when compared to the base model QWEN-14B and similar\nfindings are observed for the fine-tuned QWEN-14B-CHAT model.\n8https://huggingface.co/codefuse-ai/CodeFuse-QWen-14B\n9https://huggingface.co/codefuse-ai/CodeFuse-StarCoder-15B\n14\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nTable 10: pass@1 performance on Multi-lingual HumanEval-X (Zheng et al., 2023) after fine-tuning with MFTCoder\nacross multiple mainstream open-source models. The metric values marked with an asterisk (*) were obtained from the\nmodels\u2019 corresponding papers, technical reports, or open-source project homepages, while the remaining metric values\nwere evaluated using a combination of greedy decoding and zero-shot testing strategy.\nModel\nSize\nPython\nJava\nC++\nJavaScript\nGolang\nAvgerage\nQWen-base\n14B\n32.3%\u2217\n35.37%\n30.49%\n32.93%\n21.34%\n30.49%\nCodeFuse-QWen-MFT\n14B\n48.78%\n41.46%\n38.41%\n46.34%\n26.83%\n40.36%\nLlama-base\n65B\n23.7%\u2217\n29.26%\n20.73%\n23.78%\n18.9%\n23.27%\nCodeFuse-Llama-MFT\n65B\n34.76%\n37.2%\n29.88%\n32.93%\n23.78%\n31.71%\nLlama2-base\n70B\n29.9%\u2217\n39.02%\n31.10%\n35.98%\n23.78%\n31.96%\nCodeFuse-Llama2-MFT\n70B\n40.85%\n35.98%\n32.32%\n38.41%\n27.44%\n35.00%\nStarCoder-base\n15B\n33.6%\u2217\n34.15%\n25.61%\n22.56%\n22.56%\n29.48%\nCodeFuse-StarCoder-MFT\n15B\n54.9%\n47.56\n46.34%\n48.17%\n37.20%\n46.83%\nCodeGeex2-base\n6B\n35.9%\u2217\n30.8%\u2217\n29.3%\u2217\n32.2%\u2217\n22.5%\u2217\n30.14%\nCodeFuse-CodeGeex2-MFT\n6B\n45.12%\n45.73%\n37.2%\n37.2%\n28.05%\n38.66%\nCodeLlama-Python-base\n13B\n43.3%\u2217\n41.46%\n34.76%\n38.41%\n29.27%\n37.44%\nCodeFuse-CodeLlama-Python-MFT\n13B\n60.37%\n57.32%\n46.34%\n54.27%\n45.12%\n52.68%\nCodeLlama-34B-Python-base\n34B\n53.7%\u2217\n45.73%\n42.68%\n45.73%\n31.71%\n43.91%\nCodeFuse-CodeLLama-Python-MFT\n34B\n74.4%\n61.6%\n54.3%\n61.0%\n50.6%\n60.38%\nHumanEval\nCodeFuseEval-CN\nHumanEval-X\nMBPP\nCodeFuseEval-UnitTest\nDS1000\nCodeFuseEval-CodeTrans\n10 20 30 40 50 60 70 80\nCodeFuse-CodeLlama-34B\nGPT-3.5\nGPT-4\nFigure 5: Radar Chart of CodeFuse-CodeLlama-34B Model on HUMANEVAL, HUMANEVAL-X, MBPP, DS-1000,\nand CODEFUSEEVAL benchmarks compared to GPT-3.5 and GPT-4.\n6\nDiscussion\nDespite the superior performance of the MFT training method compared to the task data mixing-based SFT training\nmethod in the aforementioned experiments, it should be noted that the effectiveness of the MFT approach is highly\ndependent on the task-splitting strategy. Not all scenarios are suitable for being split into multiple tasks. For instance, in\nour practical experience, we found that splitting a task based on difficulty levels and training it using the MFT method\ndid not yield better results compared to the task-mixed SFT training method. Moreover, training code completion\ntasks as multiple tasks based on programming languages also did not outperform the mixed SFT method. Based on\nour practical experience, we have concluded that tasks with distinct main desired abilities are more suitable for task\n15\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nAFQMC\nCHID\nWic\nWSC\nCOPA\nCMNLI\nOCNLI\nAX-b\nAX-g\nRTE\nCSL\nC3\nEPRSTMT\nMMLU\nC-Eval\nARC-c\n0.2\n0.4\n0.6\n0.8\n1.0\nLanguage\nReasoning\nUnderstanding\nExamination\nCodeFuse-Qwen-14b\nQwen-14b\nQwen-14b-chat\nFigure 6: Performance comparison of CODEFUSE-QWEN-14B fine-tuned with MFTCoder and code-related data,\nQWEN-14B base model, and officially fine-tuned model QWEN-14B-CHAT on NLP evaluation datasets. Detailed data\ncan be found in Appendix D.\nsplitting and MFT training, whereas tasks with similar main training objectives are not well-suited for MFT training.\nWe plan to further investigate and establish more precise criteria for task delineation in future research.\nIn our task generalization experiments, we observed that models trained using the MFT method produced inference\nresults that were more similar to the reference answers and had more concise content. Conversely, inference results\ngenerated by the task-mixed SFT training method contained more Chain-of-Thought (CoT) information. In certain\nscenarios, the former approach was more preferred, such as in IDE plugins, while the latter approach was favored in\nother scenarios, such as web assistants. As a result, we cannot simply generalize that one method is better than the\nother. We are currently researching the reasons behind these performance differences.\nAs a multi-task learning method, MFT also faces a major challenge during the training process: inconsistent convergence\nspeeds among different tasks. For example, in the aforementioned experiments, the code completion task converged\nmuch faster than the unit test-case generation task (details can be found in Appendix B). This makes it difficult to find\nan optimal point that performs well on all tasks. The selected checkpoint either converges insufficiently on some tasks\nor overfits on others. To address this issue, we experimented with existing multi-task learning balancing optimization\nsolutions such as FAMO Liu et al. (2023). However, FAMO requires dual back-propagation in each iteration, resulting\nin training time being approximately doubled. Furthermore, the required number of epochs for convergence also\nincreases significantly, and the adjustability of the convergence speed is limited. Unfortunately, this exponentially\nincreased cost does not yield equivalent benefits. In response, we are currently developing a more optimal and adaptive\nmulti-task optimization balancing approach.\nFurthermore, even after balancing the convergence speeds of multiple tasks, where the same set of parameters is updated,\nit is still challenging to fundamentally eliminate the inherent conflicts in weight updates across different tasks. To\naddress this issue, we are currently exploring the utilization of MoE (Mixture of Experts) Chen et al. (2022) to achieve\nMFT.\n7\nConclusion\nThis paper introduces MFTCoder, a framework that supports multi-task fine-tuning, effectively addressing the challenges\nof data imbalance, varying difficulty levels, and inconsistent convergence speeds through the design of various loss\nfunctions. Experimental results demonstrate that this approach outperforms individual fine-tuning on each task or\nfine-tuning on a mixed ensemble of tasks. Additionally, MFTCoder facilitates efficient training, including efficient\ndata utilization and PEFT training. It also provides a high-quality instruction dataset construction solution. Leveraging\nMFTCoder for fine-tuning on the CodeLLama base, the CodeFuse-CodeLLama-34B model achieves an impressive\npass@1 score of 74.4% on the HumanEval dataset, surpassing the performance of GPT-4 (67%, zero-shot).\n16\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nReferences\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021. Muppet: Massive\nMulti-task Representations with Pre-Finetuning. arXiv:cs.CL/2101.11038\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff,\nMayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey\nSchoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc\u00c3a del\nR\u00c3o, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar,\nDavid Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried,\nArjun Guha, Harm de Vries, and Leandro von Werra. 2023. SantaCoder: don\u2019t reach for the stars! arXiv:cs.SE/2301.03988\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa,\nPaige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav\nMishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\nGustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin\nBrooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00c3\u00a9ment Crepy,\nShachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00c3az, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng,\nVlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi,\nLe Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao\nJia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li,\nYaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua\nMaynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat,\nMartin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy,\nBrennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine,\nDasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu,\nKelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny\nZhou, Slav Petrov, and Yonghui Wu. 2023. PaLM 2 Technical Report. arXiv:cs.CL/2305.10403\nAnthropic. 2023. Model Card and Evaluations for Claude Models.\nhttps://www-files.anthropic.com/production/\nimages/Model-Card-Claude-2.pdf\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran,\nDara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. 2022. ExT5: Towards Extreme Multi-Task\nScaling for Transfer Learning. arXiv:cs.CL/2111.10952\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael\nTerry, Quoc Le, et al. 2021. Program Synthesis with Large Language Models. arXiv preprint arXiv:2108.07732 (2021).\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui,\nLuo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang\nRen, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng\nXu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,\nXingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen\nTechnical Report. arXiv preprint arXiv:2309.16609 (2023).\nBaichuan. 2023. Baichuan 2: Open Large-scale Language Models. arXiv preprint arXiv:2309.10305 (2023). https://arxiv.\norg/abs/2309.10305\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle\nMcDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang,\nand Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. arXiv:cs.CL/2204.06745\nRich Caruana. 1997. Multitask learning. Machine learning 28 (1997), 41\u201375.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri\nBurda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry,\nPamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes,\nAriel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei,\nSam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. (2021).\narXiv:cs.LG/2107.03374\nZhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. 2018. Gradnorm: Gradient normalization for adaptive\nloss balancing in deep multitask networks. In International conference on machine learning. PMLR, 794\u2013803.\nZixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. 2022. Towards Understanding Mixture of Experts in Deep\nLearning. arXiv:cs.LG/2208.02813\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won\nChung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao,\n17\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nParker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa\nDev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David\nLuan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with\nPathways. arXiv:cs.CL/2204.02311\nFenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo\nShen, Lin Li, Hao Yu, Li Yan, Pingyi Zhou, Xin Wang, Yuchi Ma, Ignacio Iacobacci, Yasheng Wang, Guangtai Liang, Jiansheng\nWei, Xin Jiang, Qianxiang Wang, and Qun Liu. 2022. PanGu-Coder: Program Synthesis with Function-Level Language Modeling.\narXiv:cs.LG/2207.11280\nMichael Crawshaw. 2020. Multi-task learning with deep neural networks: A survey. arXiv preprint arXiv:2009.09796 (2020).\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs.\narXiv:cs.LG/2305.14314\nPeng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang\nFan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen\nLiu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao\nZhang, Yu Zhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, and Xianying Zhu. 2023. CodeFuse-13B: A Pretrained\nMulti-lingual Code Large Language Model. arXiv:cs.SE/2310.06266\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model\nPretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). 320\u2013335.\nLong Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Low resource dependency parsing: Cross-lingual parameter sharing\nin a neural network parser. In Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the\n7th international joint conference on natural language processing (volume 2: short papers). 845\u2013850.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero\nKauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks Are All You Need. arXiv preprint arXiv:2306.11644 (2023).\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan,\nand Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. arXiv:cs.LG/1902.00751\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA:\nLow-Rank Adaptation of Large Language Models. arXiv:cs.CL/2106.09685\nS\u00e9bastien Jean, Orhan Firat, and Melvin Johnson. 2019. Adaptive scheduling for multi-task learning. arXiv preprint arXiv:1909.06434\n(2019).\nAlex Kendall, Yarin Gal, and Roberto Cipolla. 2018. Multi-task learning using uncertainty to weigh losses for scene geometry and\nsemantics. In Proceedings of the IEEE conference on computer vision and pattern recognition. 7482\u20137491.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Optimization. arXiv:cs.LG/1412.6980\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang,\nand Tao Yu. 2022. DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation. ArXiv abs/2211.11501\n(2022).\nHae Beom Lee, Eunho Yang, and Sung Ju Hwang. 2018. Deep asymmetric multi-task feature learning. In International Conference\non Machine Learning. PMLR, 2956\u20132964.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023c. CAMEL: Communicative\nAgents for \"Mind\" Exploration of Large Scale Language Model Society. arXiv:cs.AI/2303.17760\nJinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo,\nXuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. 2023d. Can LLM\nAlready Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. arXiv:cs.CL/2305.03111\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher\nAkiki, Jia Li, Jenny Chim, et al. 2023a. StarCoder: may the source be with you! arXiv preprint arXiv:2305.06161 (2023).\nYuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023b. Textbooks Are All You\nNeed II: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 (2023).\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9 mi Leblond, Tom Eccles, James Keeling, Felix\nGimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen\nHuang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,\nPushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation with\nAlphaCode. Science 378, 6624 (dec 2022), 1092\u20131097. https://doi.org/10.1126/science.abq1158\nBo Liu, Yihao Feng, Peter Stone, and Qiang Liu. 2023. FAMO: Fast Adaptive Multitask Optimization. arXiv:cs.LG/2306.03792\n18\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nShikun Liu, Edward Johns, and Andrew J Davison. 2019b. End-to-end multi-task learning with attention. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. 1871\u20131880.\nShengchao Liu, Yingyu Liang, and Anthony Gitter. 2019c. Loss-balanced task weighting to reduce negative transfer in multi-task\nlearning. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 9977\u20139978.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019a. Multi-Task Deep Neural Networks for Natural Language\nUnderstanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 4487\u20134496.\nMingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S Yu. 2017. Learning multiple tasks with multilinear relationship\nnetworks. Advances in neural information processing systems 30 (2017).\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang.\n2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. arXiv preprint arXiv:2306.08568 (2023).\nQingkai Min, Yuefeng Shi, and Yue Zhang. 2019. A Pilot Study for Chinese SQL Semantic Parsing. arXiv:cs.CL/1909.13293\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro\nvon Werra, and Shayne Longpre. 2023. OctoPack: Instruction Tuning Code Large Language Models. arXiv:cs.CL/2308.07124\nOpenAI. 2023. GPT-4 Technical Report. arXiv:cs.CL/2303.08774\nLucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria A Zuluaga. 2021. Maximum roaming multi-task learning. In\nProceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 9331\u20139341.\nPhind. 2023. Phind-CodeLlama-34B-v2. https://huggingface.co/Phind/Phind-CodeLlama-34B-v2\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\n2023. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv:cs.LG/1910.10683\nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez,\nJ\u00e9r\u00e9my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023).\nBo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, et al.\n2023. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint arXiv:2307.14936 (2023).\nXimeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. 2020. Adashare: Learning what to share for efficient deep multi-task\nlearning. Advances in Neural Information Processing Systems 33 (2020), 8728\u20138740.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos,\nLeslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022).\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re,\nNaman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971 (2023).\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra,\nPrajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288 (2023).\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022.\nSelf-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 (2022).\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. 2023. CodeT5+: Open Code\nLarge Language Models for Code Understanding and Generation. arXiv:cs.CL/2305.07922\nYongxin Yang and Timothy Hospedales. 2017. Trace Norm Regularised Deep Multi-Task Learning. In 5th International Conference\non Learning Representations.\nTao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang,\nMichihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent\nZhang, Caiming Xiong, Richard Socher, Walter S Lasecki, and Dragomir Radev. 2019a. CoSQL: A Conversational Text-to-SQL\nChallenge Towards Cross-Domain Natural Language Interfaces to Databases. arXiv:cs.CL/1909.05378\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman,\nZilin Zhang, and Dragomir Radev. 2019b. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain\nSemantic Parsing and Text-to-SQL Task. arXiv:cs.CL/1809.08887\nXiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang, and Ying Wu. 2018. A modulation module for multi-task learning with\napplications in image retrieval. In Proceedings of the European Conference on Computer Vision (ECCV). 401\u2013416.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin\nYang, and Jie Tang. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X.\nIn KDD.\nVictor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2SQL: Generating Structured Queries from Natural Language using\nReinforcement Learning. CoRR abs/1709.00103 (2017).\n19\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nA\nCode Exercises Generation with Camel\nTable 11: Examples of Prompts for Code Exercises Generation using Camel\nOriginal task prompt:\nCreate 50 coding exercises to test and improve students\u2019 Python programming skills. The exercises must must must focus on Binary search. The difficulty of exercises\nmust be from easy to medium to hard.\nSpecified task prompt:\nA student will collaborate with a programming teacher to develop 50 Python coding exercises centered specifically on Binary search algorithms. The exercises should\nprogressively increase in difficulty, from easy to medium to hard, targeting various aspects of implementation and optimization techniques.\nFinal task prompt:\nA student will collaborate with a programming teacher to develop 50 Python coding exercises centered specifically on Binary search algorithms. The exercises should\nprogressively increase in difficulty, from easy to medium to hard, targeting various aspects of implementation and optimization techniques.\nAI Assistant sys message:\nSystemMessage(role_name=\u2019A student\u2019, role_type = <RoleType.ASSISTANT: \u2019assis-\ntant\u2019>, meta_dict={\u2019task\u2019: \u2019A student will assist a programming teacher in designing a\nseries of 50 Python coding exercises that emphasize the application and optimization\nof Bubble Sort algorithm. These exercises will span varying levels of difficulty,\nfrom easy to medium to hard, allowing students to strengthen their sorting skills\nprogressively.\u2019, \u2019assistant_role\u2019: \u2019A student\u2019, \u2019user_role\u2019: \u2019A programming teacher\u2019},\nrole=\u2019system\u2019, content=\u2019Never forget you are a A student and I am a A programming\nteacher. Never flip roles! Never instruct me!\nWe share a common interest in collaborating to successfully complete a task.\nYou must help me to complete the task.\nHere is the task: A student will assist a programming teacher in designing a series of\n50 Python coding exercises that emphasize the application and optimization of Bubble\nSort algorithm. These exercises will span varying levels of difficulty, from easy to\nmedium to hard, allowing students to strengthen their sorting skills progressively..\nNever forget our task!\nI must instruct you based on your expertise and my needs to complete the task.\nI must give you one instruction at a time.\nYou must write a specific solution that appropriately solves the requested instruction\nand explain your solutions.\nYou must decline my instruction honestly if you cannot perform the instruction due\nto physical, moral, legal reasons or your capability and explain the reasons.\nUnless I say the task is completed, you should always start with:\nSolution: <YOUR_SOLUTION>\n<YOUR_SOLUTION> should be very specific, include detailed explanations and\nprovide preferable detailed implementations and examples and lists for task-solving.\nAlways end <YOUR_SOLUTION> with: Next request.\u2019)\nAI User sys message:\nSystemMessage(role_name=\u2019A programming teacher\u2019, role_type = <RoleType.USER:\n\u2019user\u2019>, meta_dict = {\u2019task\u2019: \u2019A student will assist a programming teacher in designing\na series of 50 Python coding exercises that emphasize the application and optimization\nof Bubble Sort algorithm. These exercises will span varying levels of difficulty,\nfrom easy to medium to hard, allowing students to strengthen their sorting skills\nprogressively.\u2019, \u2019assistant_role\u2019: \u2019A student\u2019, \u2019user_role\u2019: \u2019A programming teacher\u2019},\nrole=\u2019system\u2019, content=\u2019Never forget you are a A programming teacher and I am a A\nstudent. Never flip roles! You will always instruct me.\nWe share a common interest in collaborating to successfully complete a task.\nI must help you to complete the task.\nHere is the task: A student will assist a programming teacher in designing a series of\n50 Python coding exercises that emphasize the application and optimization of Bubble\nSort algorithm. These exercises will span varying levels of difficulty, from easy to\nmedium to hard, allowing students to strengthen their sorting skills progressively..\nNever forget our task!\nYou must instruct me based on my expertise and your needs to solve the task ONLY\nin the following two ways:\n1. Instruct with a necessary input:\nInstruction: <YOUR_INSTRUCTION>\nInput: <YOUR_INPUT>\n2. Instruct without any input:\nInstruction: <YOUR_INSTRUCTION>\nInput: None\nThe \"Instruction\" describes a task or question. The paired \"Input\" provides further\ncontext or information for the requested \"Instruction\".\nYou must give me one instruction at a time.\nI must write a response that appropriately solves the requested instruction.\nI must decline your instruction honestly if I cannot perform the instruction due to\nphysical, moral, legal reasons or my capability and explain the reasons.\nYou should instruct me not ask me questions.\nNow you must start to instruct me using the two ways described above.\nDo not add anything else other than your instruction and the optional corresponding\ninput!\nKeep giving me instructions and necessary inputs until you think the task is completed.\nWhen the task is completed,\nyou must only reply with a single word\n<CAMEL_TASK_DONE>.\nNever say <CAMEL_TASK_DONE> unless my responses have solved your task.\u2019)\n20\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nB\nConvergence Speed of MFT Training Process\n3000\n4000\n5000\n6000\n7000\n8000\n9000 10000\n0.097\n0.098\n0.099\n0.100\n0.101\n0.102\n0.103\n(a) Code Comment Task\n3000\n4000\n5000\n6000\n7000\n8000\n9000 10000\n0.276\n0.278\n0.280\n0.282\n0.284\n(b) Code Completion Task\n3000\n4000\n5000\n6000\n7000\n8000\n9000 10000\n0.102\n0.104\n0.106\n0.108\n0.110\n0.112\n(c) Code Translation Task\n3000\n4000\n5000\n6000\n7000\n8000\n9000 10000\n0.575\n0.576\n0.577\n0.578\n0.579\n0.580\n0.581\n0.582\n(d) Text-to-Code Task\n3000\n4000\n5000\n6000\n7000\n8000\n9000 10000\n0.185\n0.190\n0.195\n0.200\n0.205\n0.210\n(e) Unit Test-case Generation Task\n3000\n4000\n5000\n6000\n7000\n8000\n9000\n10000\n0.184\n0.186\n0.188\n0.190\n0.192\n0.194\n(f) Overall of 5 tasks\nFigure 7: Validation Loss Convergence Speeds: A Comparative Analysis of 5 Code-related Downstream Tasks and\nOverall Training Progress using the data-balanced loss function.\n21\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nC\nExamples of codefuse-codellama-34b across various tasks\nC.1\nCode Completion on HumanEval\nPrompt:\n<|role_start|>human<|role_end|># language: Python\nfrom\ntyping\nimport\nList\ndef\nbelow_zero(operations: List[int]) -> bool:\n\"\"\" You\u2019re given a list of deposit\nand\nwithdrawal\noperations on a bank\naccount\nthat\nstarts\nwith\nzero\nbalance. Your task is to detect if at any point the\nbalance of\naccount\nfallls\nbelow zero , and\nat that\npoint\nfunction\nshould\nreturn\nTrue. Otherwise it should\nreturn\nFalse.\n>>> below_zero ([1, 2, 3])\nFalse\n>>> below_zero ([1, 2, -4, 5])\nTrue\n\"\"\"\n<|role_start|>bot<|role_end|>\nCodeFuse-CodeLLama-34B:\nbalance = 0\nfor op in operations:\nbalance += op\nif balance < 0:\nreturn\nTrue\nreturn\nFalse\nC.2\nMultilingual Code Completion on HumanEval-X\nPrompt (Java):\n<|role_start|>human<|role_end|>// language: Java\nimport\njava.util .*;\nimport\njava.lang .*;\nclass\nSolution {\n/**\nGiven a string s, count the number of uppercase\nvowels in even\nindices.\nFor\nexample:\ncountUpper (\" aBCdEf \") returns 1\ncountUpper (\" abcdefg \") returns 0\ncountUpper (\" dBBE \") returns 0\n*/\npublic int\ncountUpper(String s) {\n<|role_start|>bot<|role_end|>\n22\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nCodeFuse-CodeLLama-34B:\nint count = 0;\nfor (int i = 0; i < s.length (); i += 2) {\nchar c = s.charAt(i);\nif (Character.isUpperCase(c) && (c == \u2019A\u2019 || c == \u2019E\u2019 || c == \u2019I\u2019\n|| c == \u2019O\u2019 || c == \u2019U\u2019)) {\ncount ++;\n}\n}\nreturn\ncount;\n}\n}\nPrompt (Golang):\n<|role_start|>human<|role_end|>// language: Go\nimport (\n\"strings\"\n)\n// Create a function\nEncrypt\nthat\ntakes a string as an argument\nand\n// returns a string\nEncrypted\nwith the\nalphabet\nbeing\nrotated.\n// The\nalphabet\nshould be rotated in a manner\nsuch that the\nletters\n// shift\ndown by two\nmultiplied to two places.\n// For example:\n// Encrypt(\u2019hi \u2019) returns \u2019lm\u2019\n// Encrypt(\u2019asdfghjkl \u2019) returns \u2019ewhjklnop \u2019\n// Encrypt(\u2019gf \u2019) returns \u2019kj\u2019\n// Encrypt(\u2019et \u2019) returns \u2019ix\u2019\nfunc\nEncrypt(s string) string {\n<|role_start|>bot<|role_end|>\nCodeFuse-CodeLLama-34B:\nencrypted := make ([]byte , len(s))\nfor i, c := range s {\nif c >= \u2019a\u2019 && c <= \u2019z\u2019 {\nencrypted[i] = byte ((c-\u2019a\u2019+2) %26 + \u2019a\u2019) // shift\ndown two places\n} else if c >= \u2019A\u2019 && c <= \u2019Z\u2019 {\nencrypted[i] = byte ((c-\u2019A\u2019+2) %26 + \u2019A\u2019)\n} else {\nencrypted[i] = byte(c) // non -alphabetic\ncharacters\nremain the\nsame\n}\n}\nreturn\nstring(encrypted)\n}\n23\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nC.3\nCode Completion with Chinese prompts on CodeFuseEval-CN\nPrompt:\n<|role_start|>human<|role_end|># language: Python\n<|role_start|>bot<|role_end|>\nCodeFuse-CodeLLama-34B:\nencoded_message = \u2019\u2019\nvowels = {\u2019a\u2019: \u2019y\u2019, \u2019A\u2019: \u2019Y\u2019, \u2019e\u2019: \u2019c\u2019, \u2019E\u2019: \u2019C\u2019, \u2019i\u2019: \u2019g\u2019, \u2019I\u2019: \u2019G\u2019, \u2019o\u2019\n: \u2019m\u2019, \u2019O\u2019: \u2019M\u2019, \u2019u\u2019: \u2019s\u2019, \u2019U\u2019: \u2019S\u2019}\nfor letter in message:\nif letter.isalpha ():\nif letter in vowels:\nencoded_message += vowels[letter]\nelif\nletter.islower ():\nencoded_message += letter.upper ()\nelif\nletter.isupper ():\nencoded_message += letter.lower ()\nelse:\nencoded_message += letter\nreturn\nencoded_message\n}\nC.4\nText-to-Code on MBPP\nPrompt:\n<|role_start|>human<|role_end|> Write a function to find the number of subsequences having product smaller\nthan k for the given non negative array. Your code should satisfy these tests:\nassert\nno_of_subsequences ([1,2,3,4], 10) == 11\nassert\nno_of_subsequences ([4,8,7,2], 50) == 9\nassert\nno_of_subsequences ([5,6,7,8], 15) == 4\n<|role_start|>bot<|role_end|>\n24\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nCodeFuse-CodeLLama-34B:\ndef\nno_of_subsequences (arr , k):\nn = len(arr)\ndp = [0] * n\ndp[0] = 1\nfor i in range(1, n):\ndp[i] = dp[i-1]\nif arr[i] < k:\ndp[i] += 1\nfor j in range(i):\nif arr[j] < k//arr[i]:\ndp[i] += dp[j]\nreturn dp[n-1]\nimport\nunittest\nclass\nTestNoOfSubsequences (unittest.TestCase):\ndef\ntest_no_of_subsequences (self):\nself.assertEqual( no_of_subsequences ([1,2,3,4], 10), 111)\nself.assertEqual( no_of_subsequences ([4,8,7,2], 50), 9)\nself.assertEqual( no_of_subsequences ([5,6,7,8], 15), 4)\nif __name__ == \"__main__\":\nunittest.main ()\nC.5\nData Science Analysis on DS-1000\nPrompt:\nI\u2019m using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let\u2019s say it looks like this:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 1s correspond to the entries to this tensor, padded in front\nby 0s to a total length of 8. I.e. I want to create this tensor:\n[[0. 0. 0. 0. 1. 1. 1. 1.]\n[0. 0. 0. 0. 0. 1. 1. 1.]\n[0. 0. 0. 1. 1. 1. 1. 1.]\n[0. 0. 0. 0. 0. 0. 1. 1.]]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n25\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nCodeFuse-CodeLLama-34B:\nmax_length = tf.reduce_max(lengths)\n# Create a tensor of 1s and 0s\nmask = tf.sequence_mask(lengths , maxlen=max_length , dtype=tf.float32)\n# Pad the mask with 0s to a total\nlength of 8\npadded_mask = tf.pad(mask , [[0, 0], [0, 8 - max_length ]], constant_values =0)\nresult = padded_mask\nC.6\nCode Translation on CodeFuseEval-CodeTrans\nPrompt:\n<|role_start|>human<|role_end|>\n# Translate\nthe\nfollowing\nJava\nprogram to Python\n# Java\nimport\njava.util .*;\nimport\njava.lang .*;\nclass\nSolution {\npublic List <List <Integer >> getRow(List <List <Integer >> lst , int x) {\nList <List <Integer >> coords = new ArrayList <>();\nfor (int i = 0; i < lst.size (); i++) {\nList <List <Integer >> row = new ArrayList <>();\nfor (int j = lst.get(i).size () - 1; j >= 0; j--) {\nif (lst.get(i).get(j) == x) {\nrow.add(Arrays.asList(i, j));\n}\n}\ncoords.addAll(row);\n}\nreturn\ncoords;\n}\n}\n# Python\n<|role_start|>bot<|role_end|>\nCodeFuse-CodeLLama-34B:\ndef\nget_row(lst , x):\ncoords = []\nfor i in range(len(lst)):\nfor j in range(len(lst[i]) -1, -1,\n-1):\nif lst[i][j] == x:\ncoords.append ([i, j])\nreturn\ncoords\n26\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nC.7\nUnit test case Generation on CodeFuseEval-UnitTest\nPrompt:\n<|role_start|>human<|role_end|>\nWrite\nunit\ntests for the\nfollowing code , without\nproviding\nexplanations :\n\u2018\u2018\u2018\npublic\nclass\nExceptionUtils {\nprivate\nstatic\nfinal\nString []\nCAUSE_METHOD_NAMES = {\n\"getCause\",\n\" getNextException \",\n\" getTargetException \",\n\"getException\",\n\" getSourceException \",\n\"getRootCause\",\n\" getCausedByException \",\n\"getNested\",\n\" getLinkedException \",\n\" getNestedException \",\n\"getLinkedCause\",\n\"getThrowable\",\n};\npublic\nstatic\nThrowable\ngetCause(final\nThrowable\nthrowable , String []\nmethodNames) {\nif (throwable == null) {\nreturn\nnull;\n}\nif (methodNames == null) {\nfinal\nThrowable\ncause = throwable.getCause ();\nif (cause != null) {\nreturn\ncause;\n}\nmethodNames = CAUSE_METHOD_NAMES ;\n}\nreturn\nStream.of(methodNames).map(m -> getCauseUsingMethodName (\nthrowable , m)).filter(Objects :: nonNull).findFirst ().orElse(null);\n}\n}\n\u2018\u2018\u2018\n<|role_start|>bot<|role_end|>\n27\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nCodeFuse-CodeLLama-34B:\n\u2018\u2018\u2018\nimport org.junit.Assert;\nimport org.junit.Test;\nimport\njava.util.Objects;\nimport\njava.util.stream.Stream;\npublic\nclass\nExceptionUtilsTest {\n@Test\npublic\nvoid\ntestGetCause_NullThrowable () {\nThrowable\nthrowable = null;\nString [] methodNames = {\"getCause\"};\nThrowable\noutput = ExceptionUtils .getCause(throwable , methodNames);\nAssert.assertEquals(null , output);\n}\n@Test\npublic\nvoid\ntestGetCause_NullMethodNames () {\nThrowable\nthrowable = new\nRuntimeException (\"test\");\nString [] methodNames = null;\nThrowable\noutput = ExceptionUtils .getCause(throwable , methodNames);\nAssert.assertEquals(null , output);\n}\n@Test\npublic\nvoid\ntestGetCause_FoundCause () {\nThrowable\nthrowable = new\nRuntimeException (\"test\", new\nNullPointerException ());\nString [] methodNames = {\"getCause\"};\nThrowable\noutput = ExceptionUtils .getCause(throwable , methodNames);\nAssert.assertEquals( NullPointerException .class , output.getClass ());\n}\n@Test\npublic\nvoid\ntestGetCause_NotFoundCause () {\nThrowable\nthrowable = new\nRuntimeException (\"test\");\nString [] methodNames = {\"getCause\"};\nThrowable\noutput = ExceptionUtils .getCause(throwable , methodNames);\nAssert.assertEquals(null , output);\n}\n@Test\npublic\nvoid\ntestGetCause_DefaultMethodNames_FoundCause () {\nThrowable\nthrowable = new\nRuntimeException (\"test\", new\nNullPointerException ());\nThrowable\noutput = ExceptionUtils .getCause(throwable);\nAssert.assertEquals( NullPointerException .class , output.getClass ());\n}\n@Test\npublic\nvoid\ntestGetCause_DefaultMethodNames_NotFoundCause () {\nThrowable\nthrowable = new\nRuntimeException (\"test\");\nThrowable\noutput = ExceptionUtils .getCause(throwable);\nAssert.assertEquals(null , output);\n}\nprivate\nThrowable\ngetCauseUsingMethodName (Throwable\nthrowable , String\nmethodName) {\ntry {\nreturn (Throwable) throwable.getClass ().getMethod(methodName).\ninvoke(throwable);\n} catch (Exception e) {\nreturn\nnull;\n}\n}\n}\n\u2018\u2018\u2018\n28\nMFTCoder: Boosting Code LLMs with Multitask Fine-Tuning\nA PREPRINT\nD\nEvaluation results of CodeFuse-QWen-14B on NLP benchmark datasets\nTable 12: Comparisons of the performances of CodeFuse-QWen-14B, QWen-14B, and QWen-14b-chat on several\nNLP evaluation datasets. QWen-14B is a base model trained by Alibaba Cloud, QWen-14B-chat is a model fine-tuned\nby themselves on top of the QWen-14B base model, and CodeFuse-QWen-14B is a model fine-tuned by us using\nMFTCoder and code data.\nQWen-14B\nQWen-14B-chat\nCodeFuse-QWen-14B\nLANGUAGE\nAFQMC\n69.00%\n72.6%\n71.99%\nCHID\n84.7%\n72.3%\n84.42%\nWic\n50.9%\n50.5%\n55.02%\nWSC\n66.3%\n66.3%\n68.27%\nAverage\n67.73%\n65.43%\n69.93%\nREASONING\nCOPA\n93%\n89%\n95.00%\nCMNLI\n62.1%\n60.3%\n55.33%\nOCNLI\n58.2%\n53.8%\n49.22%\nAX-b\n49.5%\n58.5%\n67.84%\nAX-g\n80.9%\n52.5%\n82.87%\nRTE\n71.5%\n51.6%\n76.53%\nAverage\n69.20%\n60.95%\n71.13%\nUNDERSTANDING\nCSL\n54.4%\n55.6%\n68.50%\nC3\n90.8%\n91.7%\n91.01%\nEPRSTMT\n86.9%\n91.2%\n84.92%\nAverage\n77.37%\n79.50%\n81.48%\nEXAMINATION\nAX-b\n67.9%\n66.4%\n64.27%\nAX-g\n71.7%\n71.7%\n68.98%\nRTE\n84.4%\n80.3%\n84.75%\nAverage\n74.67%\n72.80%\n72.67%\n29\n"
  },
  {
    "title": "Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency",
    "link": "https://arxiv.org/pdf/2311.02772.pdf",
    "upvote": "3",
    "text": "arXiv:2311.02772v2  [cs.SD]  8 Feb 2024\nATTENTION OR CONVOLUTION: TRANSFORMER ENCODERS IN AUDIO LANGUAGE\nMODELS FOR INFERENCE EFFICIENCY\nSungho Jeon1\u2217, Ching-Feng Yeh2, Hakan Inan2, Wei-Ning Hsu2, Rashi Rungta2, Yashar Mehdad2, Daniel Bikel2\n1Heidelberg Institute of Theoretical Studies\n2Meta\nABSTRACT\nIn this paper, we show that a simple audio language model\ncan achieve comparable inference ef\ufb01ciency to more compli-\ncated pre-trained models with speech transformer encoders.\nThese speech transformers rely on mixing convolutional\nmodules with self-attention modules. They achieve state-of-\nthe-art performance on ASR with top ef\ufb01ciency.\nWe \ufb01rst\nshow that employing these speech transformers as an encoder\nsigni\ufb01cantly improves the ef\ufb01ciency of audio language mod-\nels as well. However, our study shows that we can achieve\ncomparable ef\ufb01ciency with advanced self-attention solely.\nWe demonstrate that this simpler approach is particularly\nbene\ufb01cial with a low-bit weight quantization technique of a\nneural network to improve ef\ufb01ciency. We hypothesize that it\nprevents propagating the errors between different quantized\nmodules compared to recent speech transformers mixing\nquantized convolution and the quantized self-attention mod-\nules. Our study suggests that we could pay attention to the\narchitecture of audio language language to improve their\ninference ef\ufb01ciency.\nIndex Terms\u2014 self-supervised pre-training, audio repre-\nsentation learning, ef\ufb01cient audio language models\n1. INTRODUCTION\nSelf-supervised audio language models exploit unlabeled\ndata to learn audio representations, agnostic to a speci\ufb01c task.\nThese representations are used for the target task by \ufb01ne-\ntuning instead of supervised learning with massive amounts\nof labeled data. This paradigm of pre-training and then \ufb01ne-\ntuning alleviates the dependency on abundant labeled data,\nand it lets us deploy our AI systems for diverse problems more\neasily. Following this paradigm, pre-training frameworks ex-\nploit rather complicated architectures [1]. These frameworks\nbring larger parameter counts with the concomitant higher\ncomputational cost of inference.\nHowever, there is a large gap between this high computa-\ntional inference cost and the requirements for deploying these\nmodels for on-device problems such as automatic speech\n*This work has done while Sungho Jeon was interning at Meta\nrecognition (ASR) for wearable devices. To narrow this gap,\nrecent work mostly investigates different con\ufb01gurations for\nthe components in the architecture, such as ef\ufb01cient con\ufb01g-\nurations for feature extractors [2] or subsampling of input\nsequences [3]. Another line of research investigates the ef-\n\ufb01cient transformer itself designed for the target audio task\nsolely.\nGulati et al. [4] introduce a transformer mixing a\nconvolutional and a self-attention module. Their supervised\nmodel performs better with a smaller model size on a stan-\ndard ASR dataset than pre-trained audio language models.\nInspired by this transformer, a more ef\ufb01cient model was\nintroduced [5]. These modern speech transformers bene\ufb01t\nfrom using convolutional modules in conjunction with self-\nattention modules.\nInterestingly, this approach is different from recent ef-\n\ufb01cient model architectures studied in other areas of AI. In\nthe area of Natural Language Processing (NLP), an ef\ufb01cient\ntransformer has been studied mainly by introducing more ef-\n\ufb01cient components of a vanilla transformer without mixing\nconvolutional modules [6]. In the area of Computer Vision,\nDosovitskiy et al. [7] focus on the training setup for the tasks\nof image recognition. They treat image patches in the same\nway as textual items are treated in NLP. This work shows that\na simple transformer can achieve comparable performance\nwith the state-of-the-art models mixing convolutional mod-\nules with self-attention modules. More recently, a similar ar-\ntifact is shown on the speech tasks as well [8]. However, this\nstudy focuses on the perspective of performance rather than\nthe perspective of ef\ufb01ciency.\nIn this work, we investigate the inference ef\ufb01ciency\ntrade-offs of the transformer encoder, employed in the self-\nsupervised audio language models. We \ufb01rst show that we can\nimprove the inference ef\ufb01ciency of audio language models\nby employing modern speech transformers \u2014Conformer and\nSqueezeformer\u2014 as their encoder. It improves their infer-\nence ef\ufb01ciency signi\ufb01cantly, with comparable performance\nat lower cost. However, our study shows that we can achieve\ncomparable ef\ufb01ciency with only ef\ufb01cient self-attention, with-\nout mixing convolution modules.\nOur evaluation shows that this approach is particularly\nbene\ufb01cial when we apply a quantization technique to improve\nef\ufb01ciency. This approach with quantization reduces 93.4% of\nCNN Feature Extractor\nCNN Feature Extractor\nTransformer Encoder\nAcoustic Unit Discovery System\n(e.g., K-means on MFCC)\nInput: Raw Waveform Audio\nPredict the pesudo labels \nof masked tokens\nEncoder Layer1\nEncoder Layern\nEncoder Layer2\nEncoder 1-1: Conformer \nEncoder 1-2: Squeezeformer \nFeed-Foward\nMulti-Head Self-Attention\nConvolution\nFeed-Foward\nMulti-Head Self-Attention\nConvolution\nFeed-Foward\nFeed-Foward\nMulti-Head Self-Attention\nFeed-Foward\nEncoder 2: Sparseformer Block\nSpar e s lf a t n io\nor\nz1\nz2\nz3\nz4\nzn\nx1\nx2\nMASK\nxn\nMASK\nHuBERT Framework\nFig. 1. HuBERT framework and two types of encoder candidates: Conformer or Squeezeformer vs. Sparseformer.\nthe storage size, more than 90% of the computational cost but\ndegrades the performance on the 10 downstream tasks such\nas increasing word error rate from 6.89% to 19.33% in ASR,\ncompared to the original pre-trained model without quantiza-\ntion. We hypothesize that a simple transformer prevents prop-\nagating errors between different quantized modules compared\nto modern speech transformers that mix modules of differing\ntypes.\n2. RELATED WORK\nOne line of related research investigates the ef\ufb01ciency of au-\ndio language models from the perspective of pre-processing\naudio data. Wu et al. [2] investigate the architecture variations\nof Wav2Vec 2.0 framework, a self-supervised pre-training\nframework [9], to examine the inference ef\ufb01ciency trade-offs.\nThey propose several techniques to improve the ef\ufb01ciency\nof Wav2Vec 2.0 framework.\nFor example, they introduce\nmore ef\ufb01cient con\ufb01gurations for the feature extractor and\ndownsampling input sequences linearly before their Trans-\nformer encoder. Following this work, Vyas et al. [3] propose\na stochastic approach to sub-sampling input sequences. Nev-\nertheless, there is little attention on the in\ufb02uence of their\nTransformer encoder in terms of ef\ufb01ciency.\nSince earlier studies of audio language models are based\non a vanilla Transformer, recent studies investigate the in-\n\ufb02uence of a more advanced Transformer encoder. Zhang et\nal. [10] employ Conformer into Wav2Vec 2.0, and this ap-\nproach improves the performance on audio downstream tasks.\nInstead of deploying Conformer, Chen et al. [11] propose a\nmasked speech denoising and a pre-training framework which\nemploys a Transformer encoder with relative positional en-\ncoding.\nTheir pre-trained model outperforms a HuBERT\nmodel. However, previous work mostly focuses on the per-\nspective of performance but there has been little interest on\nthe ef\ufb01ciency on this line of research.\nAnother line of studying the ef\ufb01ciency of neural networks\nis quantizing the components of neural networks [12]. Earlier\nstudies replace all full-precision weights of a neural network\nwith lower-precision weights. This approach drastically re-\nduces the memory size and inference time. However, quan-\ntizing the weights of a whole network can cause the propaga-\ntion of errors between modules, and the accumulated errors\ndegrade the performance signi\ufb01cantly in the end. To allevi-\nate this, diverse techniques have been introduced including a\npartial quantization of weights. More recently, a binarized\nTransformer is proposed, which employs a learnable scal-\ning method to the lower bits [13]. Yet et al. [14] show that\npre-trained audio models can bene\ufb01t from this work as well.\nFollowing this work, we investigate the inference ef\ufb01ciency\ntrade-offs with a quantization of neural network weights.\n3. MODEL ARCHITECTURE\n3.1. Self-Supervised Audio Pre-Training: HuBERT\nOur study is based on HuBERT [15] for a framework of\nself-supervised audio pre-training (Figure 1).\nThis frame-\nwork consists of three components: a feature extractor, a\ntransformer encoder, and an acoustic unit discovery mod-\nule. Following the Wav2Vec 2.0 architecture, a convolutional\nwaveform component is employed for a feature extractor\nwhich takes raw waveform inputs. It projects the input to\nvector representations. The transformer encoder consists of\nmultiple blocks, and it processes the input representations.\nThe acoustic unit discovery module produces the pseudo-\nlabels of input audio frames by clustering features, such\nas clustering MFCC features via k-means. Inspired by the\nBERT pre-training, non-masked audio representations are\nlearned to describe the masked tokens well by predicting\ntheir pseudo-labels.\n3.2. Encoder Candidate 1: Conformer / Squeezeformer\nConformer, the convolution-augmented transformer, consists\nof stacked layers of convolutional modules in conjunction\nEncoder in HuBERT\nEncoder Params\nPrec\nStorage\nFLOP\nBOP NonQ\nBOP BQ\nNVDA Est Time\nASR\u2193\nSD\u2193\n(+FastConv)\nL / D / H\n(MB)\n(Gs)\n(Gs)\n(Gs)\n(e-04, second)\n(WER, %)\n(DER, %)\nVanilla Trans\n12 / 786 / 12\nFP32\n184.42\n110.54\n1228.64\n-\n38.46\n7.06\n6.32\nConformer-S\n16 / 144 / 4\nFP32\n131.87\n22.10\n329.39\n-\n10.31\n8.56\n6.81\nSqueeze-XS\n16 / 144 / 4\nFP32\n132.04\n18.31\n272.88\n-\n8.54\n8.96\n9.18\nSparseformer-DN-S\n16 / 256 / 4\nFP32\n60.81\n26.05\n388.18\n-\n12.15\n8.44\n7.66\nSparseformer-SW-S\n8 / 512 / 4\nFP32\n117.18\n40.09\n597.43\n-\n18.70\n7.88\n6.56\nBQ-Vanilla Trans\n12 / 786 / 12\nFP32-W1A1\n25.23\n11.82\n172.83\n63.62\n5.53\n16.83\n7.62\nBQ-Conformer-S\n16 / 144 / 4\nFP32-W1A1\n12.88\n7.23\n103.44\n15.94\n3.27\n20.52\n11.11\nBQ-Squeeze-XS\n16 / 144 / 4\nFP32-W1A1\n13.05\n7.20\n104.05\n11.86\n3.28\n24.10\n13.42\nBQ-Sparseformer-DN-S\n16 / 256 / 4\nFP32-W1A1\n12.10\n7.35\n107.91\n10.70\n3.40\n19.33\n9.64\nBQ-Sparseformer-SW-S\n8 / 512 / 4\nFP32-W1A1\n19.71\n8.85\n130.38\n19.49\n4.12\n18.15\n8.39\nTable 1. Pro\ufb01ling Results (L: Layer Num, D: Dim, H: Head Num; BQ: BiT Quantization W1A1; NVDA Est Time: Estimated\ntime for their BOP based on the catalog of NVidia A100)\nwith self-attention modules. It achieves comparable perfor-\nmance on ASR with a smaller model size compared to a\nvanilla transformer.\nConformer is originally designed for\nASR, but it has been used widely for an ef\ufb01cient audio trans-\nformer in other speech tasks as well.\nKim et al. [5] redesign the Conformer architecture based\non their empirical study, with a new architecture they call\nSqueezeformer.\nThey investigate two aspects, at the mi-\ncro level and at the macro level. For the macro level, they\nintroduce subsampling of input audio sequences.\nFor the\nmicro-level, they introduce several modi\ufb01cations including\nre-ordering the modules in the transformer, changing activa-\ntion functions, and reducing the number of layer normaliza-\ntion modules.\n3.3. Encoder Candidate 2: Sparseformer\nLocal window attention has been studied to deal with long\ninput sequences. The full-attention matrix is sparsi\ufb01ed by at-\ntention patterns, which scales linearly for the input sequence\nlength. Following this, Sparseformer achieves similar per-\nformance with a vanilla transformer with signi\ufb01cantly fewer\noperations [6]. The key idea of Sparseformer is to subdivide a\nfull-attention computation into several sub-computations \ufb01rst,\nwhich applies a \ufb01xed-attention pattern as hyper parameters.\nThen these sub-computed outputs are used to approximate the\nfull-attention.\n3.4. Neural Quantization:\nRobustly Binarized Trans-\nformer\nLiu et al. [13] propose the robustly binarized transformer\n(BiT), which is a fully binarized transformer. They introduce\na two-set binarization scheme and an elastic binarization\nfunction which learns the mapping range of quantization in\nthe training. We employ this quantization technique to in-\nvestigate the in\ufb02uence of different transformer encoders with\nquantization. While Liu et al. [13] focus on quantizing a\ntransformer and linear/activation layers, we implement their\nquantization techniques for the convolutional layers as well\nto quantize Conformer and Squeezeformer. Yeh et al. [14]\ninvestigate the in\ufb02uence of different target bits for quantiz-\ning the HuBERT-base model with a vanilla transformer. We\nonly investigate the extreme bit of quantization, both 1 bit for\nweights and activation (W1A1).\n4. EXPERIMENTS\n4.1. Experimental Setup\nPre-training setup. We follow the pre-training setup of Hu-\nBERT. This pre-training is based on the Librispeech dataset,\nconsisting of 960 hours. We use 32 GPUs of NVidia A100\nwith a batch size of at most 36.5 seconds of audio per GPU.\nAll models are trained for 250k steps in the \ufb01rst phase, then\nthey are trained for 600k steps in the second phase. It takes\n8.5 hours for 100k steps on our setup.\nEvaluation Setup. We evaluate models in terms of compu-\ntational cost and performance on downstream tasks. We \ufb01rst\npro\ufb01le the computational cost of models using the DeepSpeed\nlibrary [16]. We examine a required storage (Storage), a num-\nber of \ufb02oating point operations (FLOP), a number of bit oper-\nations (BOP) [17], and an estimated time for their BOP based\non a catalog of NVidia A100 GPU.\nWe evaluate models on the 10 downstream tasks of SU-\nPERB [18]: automatic speech recognition (ASR), keyword\nspotting (KS), slot \ufb01lling (SF), speaker identi\ufb01cation (SID),\nphoneme recognition (PR), query by example (QbE), intent\nclassi\ufb01cation (IC), automatic speaker veri\ufb01cation (ASV),\nspeaker diarization (SD) and emotion recognition (ER).\nModel con\ufb01gurations. We employ the setup of baselines\u2019\nsmallest model, Conforemr-S and Squeezeformer-XS, re-\nspectively [4, 5] (Table 1). Following their shape of deep-\nnarrow architecture, we design Sparseformer-DN-S which\nrequires smaller computational cost than others in the quan-\ntized models\n4.2. Ef\ufb01ciency: Model Pro\ufb01ling and Downstream Tasks\nInference ef\ufb01ciency trade-offs. We \ufb01rst compare the com-\nputational cost of different encoders without quantization\nEncoder in HuBERT\nPrec\nSUPERB Tasks\n(+FastConv)\nASR\u2193\nKS\u2191\nSF\u2191\nSID\u2191\nPR\u2193\nQbE\u2191\nIC\u2191\nASV\u2193\nSD\u2193\nER\u2191\nVanilla Trans (Reported)\nFP16\n7.06\n96.62\n0.89\n53.67\n6.05\n6.91\n97.28\n5.30\n6.32\n65.00\nVanilla Trans (Our Setup)\nFP32\n6.89\n96.56\n0.89\n53.52\n5.82\n6.70\n97.86\n5.96\n6.63\n63.83\nConformer-S\nFP32\n8.56\n95.94\n0.87\n52.11\n9.42\n6.20\n92.88\n5.80\n6.81\n62.12\nSqueeze-XS\nFP32\n8.96\n96.13\n0.80\n35.35\n10.73\n6.14\n83.31\n7.86\n9.18\n58.75\nSparseformer-DN-S\nFP32\n8.44\n95.89\n0.88\n60.57\n7.99\n6.27\n96.02\n6.24\n7.66\n62.26\nSparseformer-SW-S\nFP32\n7.88\n93.25\n0.88\n61.86\n9.72\n4.92\n93.15\n6.98\n6.56\n65.90\nBQ-Vanilla Trans (Reported)\nFP16-W1A1\n15.96\n93.83\n0.78\n49.62\n22.96\n5.63\n93.01\n6.83\n7.62\n61.68\nBQ-Vanilla Trans (Our Setup)\nFP32-W1A1\n16.83\n94.77\n0.79\n40.15\n20.63\n5.57\n89.77\n9.13\n7.80\n60.74\nBQ-Conformer-S\nFP32-W1A1\n20.53\n92.44\n0.76\n24.98\n37.13\n5.18\n69.73\n9.86\n11.11\n56.41\nBQ-Squeeze-XS\nFP32-W1A1\n24.10\n92.79\n0.69\n18.56\n28.82\n5.08\n62.01\n11.97\n13.42\n57.57\nBQ-Sparseformer-DN-S\nFP32-W1A1\n19.33\n92.24\n0.79\n29.21\n33.17\n4.66\n71.34\n10.70\n9.64\n58.97\nBQ-Sparseformer-SW-S\nFP32-W1A1\n18.15\n94.03\n0.79\n38.92\n24.37\n6.09\n84.37\n9.19\n8.39\n61.39\nTable 2. Evaluation on SUPERB Tasks (BQ: BiT Neural Quantization, W1A1: 1 bit for Weights and Activation, Reported:\nReported in [14], Our Setup: Lower batch size (0.5M tokens < 1.2M tokens))\n(Table 1).\nA model employing Conformer-S shows lower\ncost than the baseline, 64% for the required storage and 74%\nin FLOP reductions compared to the HuBERT with vanilla\nTransformer. Since Squeezeformer has the same fundamental\narchitecture with Conformer, it shows similar pro\ufb01ling re-\nsults. Sparseformer-DN-S also shows comparable reductions\nfor their computational cost.\nNext, we evaluate these models on the 10 speech down-\nstream tasks of SUPERB (Table 2). We observe the ef\ufb01ciency\ntrade-offs for employing more ef\ufb01cient transformer on down-\nstream tasks. For example, it increases word error rate from\n6.89 to 8.44 in ASR.\nEf\ufb01ciency with quantization.\nWhen we apply 1-bit BiT\nquantization, our results show that these two types of en-\ncoders have different in\ufb02uences.\nThe quantized model\nemploying Spareformer (BQ-Sparseformer-DN-S) shows\nbetter performance than the quantized model employing\nConformer-S (BQ-Conformer-S) overall. It shows a lower\nword error rate on ASR (19.33 < 20.52) and a lower diariza-\ntion error rate on SD (9.64 < 11.11). Despite of the fact that\nBQ-Sparseformer-DN-S takes the smallest computational\ncost compared to the cost of BQ-Conformer-S: the 7.7%\nsmaller required storage and the 32.9% smaller BOP BQ.\nWe hypothesize that the quantized modules of different types\nin these speech transformers propagate errors.\nThen, the\naccumulated errors degrade performance more than a sim-\nple transformer encoder, consisting of self-attention modules\nonly.\nCompared to the baseline without quantization, employ-\ning Sparseformer with BiT quantization reduces 93.4% re-\nquired storage (184.42 \u2192 12.10), 93.4% of FLOP (110.54\n\u2192 7.35 ), and 90.3% of BOP (1228.64 \u2192 118.61). We es-\ntimate 91.1% runtime reduction in the theoretical maximum\nperformance of an NVidia A100 GPU. In return, it increases\nthe word error rate from 6.89% to 19.33%, and other tasks\nas well. Compared to the baseline with BiT quantization, it\nsaves 52.1% of required storage (25.23 \u2192 12.10), 37.8% of\nFLOP (11.82 \u2192 7.35), 50% of BOP (236.45 \u2192 118.61). As\nef\ufb01ciency trade-offs, it increases word error rate from 16.83%\nto 19.33%, and overall.\n4.3. Architecture Shape: Deep-Narrow vs. Shallow-Wide\nAshihara et al. [19] show that two different shapes of ar-\nchitectures have different advantages as speech tasks when\nthey investigate this issue with knowledge distillation. In-\nspired by this, we design a shallow-wide shape of Sparse-\nformer (Sparseformer-SW-S). It has half of the number of\nlayers but twice larger dimensions compared to the setup of\nSparseformer-DN-S. This model shows better performance,\nbut this shape of architecture brings higher computational\ncosts.\nOur pro\ufb01ler shows that the wide shape of the ar-\nchitecture causes a larger storage size due to the absolute\npositional encoding, employed in the framework. Each layer\nalso requires more matrix multiplication operations due to\nthe self-attention mechanism. Hence, it has disadvantages in\ncomputational cost to design more ef\ufb01cient models.\n5. CONCLUSIONS\nWe investigate the ef\ufb01ciency trade-offs of employing differ-\nent transformer encoders into the self-supervised framework\nof audio pre-training. Our experiments show that there are\ndecent ef\ufb01ciency trade-offs when we employ them. When\nwe apply a quantization technique, however, our results sug-\ngest that a simple transformer encoder employing only ef\ufb01-\ncient self-attention modules is more bene\ufb01cial than the recent\nspeech transformers blending modules of differing types.\n6. REFERENCES\n[1] Steffen Schneider, Alexei Baevski, Ronan Collobert,\nand Michael Auli, \u201cwav2vec: Unsupervised pre-training\nfor speech recognition,\u201d Proc. Interspeech, pp. 3465\u2013\n3469, 2019.\n[2] Felix Wu, Kwangyoun Kim, Jing Pan, Kyu J Han, Kil-\nian Q Weinberger, and Yoav Artzi,\n\u201cPerformance-\nef\ufb01ciency trade-offs in unsupervised pre-training for\nspeech recognition,\u201d in 2022 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2022, pp. 7667\u20137671.\n[3] Apoorv Vyas, Wei-Ning Hsu, Michael Auli, and Alexei\nBaevski, \u201cOn-demand compute reduction with stochas-\ntic wav2vec 2.0,\u201d Proc. Interspeech, 2022.\n[4] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki\nParmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,\nZhengdong Zhang, Yonghui Wu, et al.,\n\u201cConformer:\nConvolution-augmented transformer for speech recog-\nnition,\u201d Proc. Interspeech, pp. 5036\u20135040, 2020.\n[5] Sehoon Kim, Amir Gholami, Albert Eaton Shaw,\nNicholas Lee, Karttikeya Mangalam, Jitendra Malik,\nMichael W Mahoney, and Kurt Keutzer,\n\u201cSqueeze-\nformer: An ef\ufb01cient transformer for automatic speech\nrecognition,\u201d in Advances in Neural Information Pro-\ncessing Systems, 2022.\n[6] Rewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever,\n\u201cGenerating long sequences with sparse\ntransformers,\u201d arXiv preprint arXiv:1904.10509, 2019.\n[7] Alexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, et al.,\n\u201cAn image is\nworth 16x16 words: Transformers for image recogni-\ntion at scale,\u201d in International Conference on Learning\nRepresentations, 2021.\n[8] Yuan Gong, Cheng-I Lai, Yu-An Chung, and James\nGlass, \u201cSsast: Self-supervised audio spectrogram trans-\nformer,\u201d in Proceedings of the AAAI Conference on Ar-\nti\ufb01cial Intelligence, 2022, vol. 36, pp. 10699\u201310709.\n[9] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli,\n\u201cwav2vec 2.0: A framework for\nself-supervised learning of speech representations,\u201d Ad-\nvances in neural information processing systems, vol.\n33, pp. 12449\u201312460, 2020.\n[10] Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-\nCheng Chiu, Ruoming Pang, Quoc V. Le, and Yonghui\nWu, \u201cPushing the limits of semi-supervised learning for\nautomatic speech recognition,\u201d in NeuRIPS Workshop\non Self-Supervised Learning for Speech and Audio Pro-\ncessing, 2020.\n[11] Sanyuan Chen, Chengyi Wang,\nZhengyang Chen,\nYu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki\nKanda, Takuya Yoshioka, Xiong Xiao, et al., \u201cWavLM:\nLarge-scale self-supervised pre-training for full stack\nspeech processing,\u201d IEEE Journal of Selected Topics in\nSignal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022.\n[12] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran\nEl-Yaniv, and Yoshua Bengio, \u201cBinarized neural net-\nworks,\u201d Advances in neural information processing sys-\ntems, vol. 29, 2016.\n[13] Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott\nYih, Meng Li, Raghuraman Krishnamoorthi, and Yashar\nMehdad, \u201cBit: Robustly binarized multi-distilled trans-\nformer,\u201d in Advances in Neural Information Processing\nSystems, 2022.\n[14] Ching-Feng Yeh, Wei-Ning Hsu, Paden Tomasello, and\nAbdelrahman Mohamed, \u201cEf\ufb01cient speech representa-\ntion learning with low-bit quantization,\u201d arXiv preprint\narXiv:2301.00652, 2022.\n[15] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,\nKushal Lakhotia, Ruslan Salakhutdinov, and Abdelrah-\nman Mohamed, \u201cHubert: Self-supervised speech rep-\nresentation learning by masked prediction of hidden\nunits,\u201d IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 29, pp. 3451\u20133460, 2021.\n[16] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He,\n\u201cDeepspeed: System optimizations\nenable training deep learning models with over 100 bil-\nlion parameters,\u201d\nin Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Dis-\ncovery & Data Mining, 2020, pp. 3505\u20133506.\n[17] Mart Van Baalen, Christos Louizos, Markus Nagel,\nRana Ali Amjad, Ying Wang, Tijmen Blankevoort, and\nMax Welling,\n\u201cBayesian bits: Unifying quantization\nand pruning,\u201d Advances in neural information process-\ning systems, vol. 33, pp. 5741\u20135752, 2020.\n[18] Shu wen Yang, Po-Han Chi, Yung-Sung Chuang,\nCheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T.\nLiu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-\nHsien Huang, Wei-Cheng Tseng, Ko tik Lee, Da-Rong\nLiu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji\nWatanabe, Abdelrahman Mohamed, and Hung yi Lee,\n\u201cSUPERB: Speech Processing Universal PERformance\nBenchmark,\u201d\nin Proc. Interspeech, 2021, pp. 1194\u2013\n1198.\n[19] Takanori Ashihara, Takafumi Moriya, Kohei Matsuura,\nand Tomohiro Tanaka, \u201cDeep versus wide: An analy-\nsis of student architectures for task-agnostic knowledge\ndistillation of self-supervised speech models,\u201d pp. 411\u2013\n415, 2022.\n"
  },
  {
    "title": "Ultra-Long Sequence Distributed Transformer",
    "link": "https://arxiv.org/pdf/2311.02382.pdf",
    "upvote": "2",
    "text": "ULTRA-LONG SEQUENCE DISTRIBUTED TRANSFORMER\nXiao Wang 1 Isaac Lyngaas 1 Aristeidis Tsaris 1 Peng Chen 2 Sajal Dash 1 Mayanka Chandra Shekar 1\nTao Luo 3\nHong-Jun Yoon 1 Mohamed Wahib 4 John Gounley 1\nABSTRACT\nTransformer models trained on long sequences often achieve higher accuracy than short sequences. Unfortunately,\nconventional transformers struggle with long sequence training due to the overwhelming computation and memory\nrequirements. Existing methods for long sequence training offer limited speedup and memory reduction, and\nmay compromise accuracy. This paper presents a novel and efficient distributed training method, the Long\nShort-Sequence Transformer (LSS Transformer), for training transformer with long sequences. It distributes a\nlong sequence into segments among GPUs, with each GPU computing a partial self-attention for its segment.\nThen, it uses a fused communication and a novel double gradient averaging technique to avoid the need to\naggregate partial self-attention and minimize communication overhead. We evaluated the performance between\nLSS Transformer and the state-of-the-art Nvidia sequence parallelism on a Wikipedia enwik8 dataset. Results\nshow that our proposed method lead to 5.6x faster and 10.2x more memory-efficient implementation compared to\nstate-of-the-art sequence parallelism on 144 Nvidia V100 GPUs. Moreover, our algorithm scales to an extreme\nsequence length of 50,112 at 3,456 GPUs, achieving 161% super-linear parallel efficiency and a throughput of 32\npetaflops.\n1\nINTRODUCTION\nThe transformer is a powerful neural network architec-\nture widely used in natural language and image process-\ning (Vaswani et al., 2017). Its versatility is evidenced by\nits wide range of applications, including machine transla-\ntion (Wang et al., 2019), chatbots (Caldarini et al., 2022),\nspeech recognition (Dong et al., 2018), image caption-\ning (Yu et al., 2019), image segmentation (Valanarasu et al.,\n2021; Strudel et al., 2021), and classification (Chen et al.,\n2021b). The transformer achieves its impressive perfor-\nmance by recognizing that different input sequence tokens\nhave varying levels of importance to the final output predic-\ntion. The transformer captures the relationship between each\npair of input tokens using a process called \u201cself-attention\u201d.\nThis allows the transformer to generate highly accurate out-\nputs by focusing on the most relevant tokens in an input\nsequence while also paying attention to the overall context.\nThis approach has proven to be highly effective and makes\ntransformer a leading technology in artificial intelligence.\nWith long sequence training, transformer attends to many\nmore input tokens than a transformer trained with short\nsequences. Therefore, long sequence training often cap-\ntures more contextual information and leads to markedly\n1, Oak Ridge National Laboratory, US. 2, National Institute\nof Advanced Industrial Science and Technology, Japan. 3, Agency\nfor Science, Technology and Research, Singapore. Corresponding\nemail: wangx2@ornl.gov\nhigher prediction accuracy for many tasks with long-range\ndependencies, such as DNA sequence analysis (Zaheer\net al., 2020), long document summary (Beltagy et al., 2020)\nand image segmentation (Strudel et al., 2021). Unfortu-\nnately, transformer\u2019s memory footprint increases quadrat-\nically and computations increase cubically with longer se-\nquence lengths (Beltagy et al., 2020; Dao et al., 2022).\nTherefore, the transformer\u2019s sequence length is typically\ntruncated to no more than a couple thousand tokens due to\nruntime and memory constraints, despite longer sequences\nleading to higher accuracy.\nTo address this issue, there are three research approaches: hi-\nerarchical training, attention approximation, and distributed\nsequence parallelism. Hierarchical training involves training\nmultiple transformers at different levels of abstraction (Si\n& Roberts, 2021; Chen et al., 2022; 2021b; Yu et al., 2023).\nThe transformer at the lowest abstraction level trains on the\nshortest sequence segments. Then, the transformer at the\nnext higher level uses the previous level outputs as addi-\ntional input to train on longer segments. The process then\nrepeats until reaching the highest abstraction level. How-\never, training multiple transformers at different abstraction\nlevels significantly increases training time and memory foot-\nprint. In addition, hyper-parameter tuning is required for\nthe optimal model architecture at each abstraction level.\nThe approximation approach, in contrast, aims to reduce the\ncomputations and memory usage by approximating the self-\narXiv:2311.02382v2  [cs.DC]  8 Nov 2023\nUltra-Long Sequence Distributed Transformer\nTable 1. A summary for different long sequence training methods. Serial methods may need to be used on top of parallelism schemes. lx=\nsequence length; N= number of GPU workers; H= number of hierarchical levels; Z= number of non-zeros.\nApproach\nMethod\nAccuracy\nLoss\nSerial/\nDistributed\nMemory\n(per worker)\nCompute\n(per worker)\nMemory\nTotal\nCompute\nTotal\nComm.\nfreq. per layer\nPractical\nbottleneck\nHierarchical\nTraining\n(Chen et al., 2022; 2021b)\n(Si & Roberts, 2021; Yu et al., 2023)\nNo\nSerial\nO(l2\nxH)\nO(l3\nxH)\nN/A\nN/A\nN/A\nTrain multiple\nmodels\nAttention\nApproximation\n(Kitaev et al., 2020; Roy et al., 2021)\n(Child et al., 2019; Beltagy et al., 2020)\nYes\nSerial\nO(Z)\nO(Z3)\nN/A\nN/A\nN/A\nLow sparsity\nSequence\nParallel\nStraightforward Seq. Parallel\n(Li et al., 2021; 2023)\nNo\nDistributed\n(partial aggr.)\nO(l2\nx/N)\nO(l3\nx/N)\nO(l2\nx)\nO(l3\nx)\n\u2248 N 2\nFrequent\ncommunication\nNvidia Seq. Parallel\n(Korthikanti et al., 2022)\nDistributed\n(serial attention)\nO(l2\nx)\nO(l3\nx)\nO(l2\nx)\nO(l3\nx)\n8\nAttention not\ndistributed\nLSS Transformer (ours)\nDistributed\n(fully)\nO(l2\nx/N)\nO(l3\nx/N)\nO(l2\nx)\nO(l3\nx)\n2\n-\nattention operations through either sparse sampling (Child\net al., 2019; Kitaev et al., 2020; Roy et al., 2021; Belt-\nagy et al., 2020; Zaheer et al., 2020), low-rank approxima-\ntion (Choromanski et al., 2021; Katharopoulos et al., 2020),\ninfrequent self-attention updates (Ying et al., 2021; Rabe\n& Staats, 2022), or their combinations (Chen et al., 2021a).\nThese approximation methods can significantly reduce mem-\nory footprint and computations, and some can even reduce\nthe long sequence problem\u2019s quadratic complexity to linear\ncomplexity. However, approximation is a lossy information\ncompression technique that discards partial information for\nthe self-attention. Thereby, excessive approximation may\nlower accuracy especially for sequences with long-range\ndependency. Previous experiments demonstrate significant\naccuracy degradation when approximation compression ra-\ntio exceeds 70% (Shi et al., 2021).\nThe distributed sequence parallelism approach aims to\naddress the long sequence problem by distributing long\nsequences into contiguous sequence segments among\nGPUs (Li et al., 2021; 2023; Korthikanti et al., 2022; Jacobs\net al., 2023). The largest hurdle in distributed sequence\nparallelism lies in handling the self-attention, which is both\nthe most compute-intensive but also the most communicate-\nintensive step. Since each GPU\u2019s sequence segment has a\ncorrelation with every other GPU\u2019s segment, a straightfor-\nward distributed self-attention method, such as (Li et al.,\n2021), Deep-Speed Ulysses (Jacobs et al., 2023) and the\nmost recent LightSeq (Li et al., 2023), requires each GPU\nto communicate with every other GPU multiple times to cal-\nculate a partial self-attention score for its assigned segment,\nbefore aggregating them into the final self-attention output.\nConsequently, communication frequency tends to increase\nsignificantly in a quadratic growth rate with more sequence\nparallel GPUs, substantially impeding scalability.\nConversely, an alternative sequence parallel method from\nNvidia\u2019s Megatron-LM framework completely avoids self-\nattention dependency by performing sequential updates on\nthe core self-attention and feed-forward operations along\nthe sequence dimension, while parallelizing less compute-\nintensive but independent tasks, such as layer normaliza-\ntion and dropouts (Korthikanti et al., 2022). Although this\nmethod requires only 8 communications per attention layer,\nit misses the opportunity to parallelize the compute inten-\nsive self-attention. Consequently, it leads to a modest 29%\nspeedup compared to a sequential baseline with full back-\nward recomputation (Korthikanti et al., 2022).\nA summary of each approach\u2019s core idea and limitations is\nprovided in Table 1. To address their limitations, this paper\nintroduces the \u201cDistributed Long Short-Sequence Trans-\nformer\u201d (LSS Transformer). In contrast to existing methods,\nthe LSS Transformer (1) utilizes a single transformer for\ntraining; (2) attains remarkable speedup and memory re-\nduction; (3) has no approximation, thereby no accuracy\nloss; and (4) maintains a minimal communication overhead,\nrequiring only 2 communications per attention layer. It dis-\ntributes a long sequence into short segments among GPUs,\nwith each GPU computing a partial self-attention score for\nits segment in the context of the entire sequence. Then it\nuses a fused communication and a double gradient averag-\ning technique to avoid the need to aggregate partial self-\nattention score, minimize communication overhead while\npreserving data dependency.\nThis paper contributes in the following ways: (1) Introduc-\ning a general and innovative end-to-end sequence frame-\nwork for long sequence training. Operating at the attention\nlayer level, it remains agnostic to model sizes and varia-\ntions (encoder-only, decoder-only, etc.), making it univer-\nsally applicable without modifications. (2) Presenting a low\ncommunication overhead algorithm that uses fused commu-\nnication and gradient averaging to lower communication\nfrequency. (3) Presenting an integrative sequence and data\nparallelism algorithm that minimizes inter-GPU communi-\ncations within local communicative groups. Our assessment\non the Wikipedia enwik8 dataset demonstrates the superi-\nority of the LSS Transformer over state-of-the-art Nvidia\nsequence parallelism (Korthikanti et al., 2022), achieving 6x\nfaster training and 10x enhanced memory efficiency on 144\nNvidia V100 GPUs. Moreover, our algorithm scales remark-\nably to an extensive sequence length of 50,112 using 3,456\nGPUs, delivering 161% super-linear parallel efficiency and\na computation throughput of 32 petaflops.\nUltra-Long Sequence Distributed Transformer\nTable 2. Three distinct challenges for training transformer and their orthogonal levels of parallelism. Nd = data size; Nm = model size; lx\n= sequence length; B = batch size;\nChallenges\nLarge Dataset\nLarge Model Size\nLong Sequences\nComputational complexity\nO(Nd)\nO(Nm)\nO(l3\nx)\nMemory complexity\nO(B)\nO(Nm)\nO(l2\nx)\nParallelism\nData Parallel\nModel Parallel\nSequence Parallel\nSource of parallelism\nDistributed input data batches\nDistributed model parameters\nDistributed sequence segments\nDistributed memory\nNo\nYes\nYes\n2\nBACKGROUND AND MOTIVATION\n2.1\nOrthogonal Levels of Parallelism\nTraining a transformer model has three distinct computation\nchallenges: (1) large training dataset, (2) large model size\nand (3) long sequences. Table 2 offers a concise overview of\nthese challenges and their characteristics. As discussed ear-\nlier, increasing sequence length leads to a cubic growth rate\nfor computations and a quadratic growth rate for memory\nuse. In contrast, scaling model parameters or data batches\npresents linear computational and memory complexities.\nEach challenge relies on a unique parallelism. These paral-\nlelism levels are mostly orthogonal, address different chal-\nlenges and cannot replace each other. Data parallelism\naccelerates training on large datasets by distributing train-\ning batches. However, it does not distribute memory and\neach GPU has a copy of the entire model. Model paral-\nlelism achieves speedup and distributed memory for large\nmodel problem by distributing model parameters and their\ngradients. Sequence parallelism, in contrast, accelerates\ncomputations and distributes memory for long sequences\nby distributing sequence segments. This paper parallelizes\nin the sequence dimension to scale the sequence length by\ndistributing memory and compute cost, while also carefully\nassuring that the sequence parallelism does not interfere\nwith or hinders other forms of parallelization.\n2.2\nTransformer Architecture\nFigs. 1(i)-(iii) illustrate a standard transformer architecture\nfor both the decoder only (i.e., GPT) and the encoder only\n(BERT) models. A token embedding in Fig. 1(i) converts\na tokenized image or text input sequence x into an input\nvector. The input vector is then enhanced with embedded\npositional information.\nNext, the input vector x of size lx \u00d7 Em, with lx represent-\ning the sequence length and Em being the embedding size,\nundergoes layer normalization and random dropouts before\nbeing processed by the self-attention. The detailed steps\nfor self-attention are illustrated in Fig. 1(ii) with the goal of\nproducing a contextual embedding for each input token in\nrelation to the whole sequence. In the self-attention unit, the\ninput vector x is linearly transformed into query (Q), key\n(K), and value (V ) vectors, with all three vectors having\nthe same size as input x. Then self-attention computes the\nfollowing equation:\nE = softmax\n\u0010\nQKT /\np\ndk\n\u0011\nV = AwV .\n(1)\nThe self-attention score, denoted as Aw, has size lx \u00d7 lx\nand quantifies the correlation between each pair of tokens.\nThis correlation is calculated by taking the dot product be-\ntween Q and the transposed K. To stabilize gradients during\ntraining, the self-attention score Aw is further scaled by a\nconstant \u221adk, and is then normalized by a dropout and a\nSoftMax activation. The final self-attention output E is ob-\ntained by weighted averaging between value vector V and\nAw. The output embedding E is then linearly transformed\nbefore exiting the self-attention unit.\nThe contextual embedding output from self-attention is fur-\nther processed by a feed-forward unit, with detailed steps\nillustrated in Fig. 1(iii), residual connections, and layer\nnormalization to enhance training stability and prediction\naccuracy. These components, along with dropouts, form a\nsingle attention layer, outlined by a gray box in Fig. 1(i).\nThen the attention layers repeat itself by L times, where L\nis the number of attention layers. After undergoing final\nlayer normalization and linear transformation, the output y\nis compared to the target \u02dcy using cross-entropy loss, and all\nparameters are updated.\nIt\u2019s crucial to note that self-attention dominates both com-\nputation and memory requirements for training. With a size\nof lx \u00d7 lx, the self-attention score, Aw, has quadratic expan-\nsion in its size with increased sequence length. This leads\nto transformer\u2019s cubic computational and quadratic memory\ncomplexities with respect to sequence length.\n2.3\nState-of-the-Art Sequence Parallelism\nTo parallelize self-attention, the straightforward distributed\nself-attention method, such as (Li et al., 2021; 2023), par-\ntitions both the input vector x and its linearly transformed\nvectors Q, K and V into distributed segments among GPUs.\nFor example in a scenario with 3 GPUs, GPU 1 receives\nthe first segments of these vectors, namely x1, Q1, K1, and\nV1, and GPUs 2 and 3 receive the second and the third\nUltra-Long Sequence Distributed Transformer\nFigure 1. (i) A generic transformer with L attention layers outlined by a gray box. (ii) and (iii) the self-attention and the feed-forward\nunits, respectively. (iv) Forward pass for baseline sequence parallelism with 2 sequence distributed GPUs. Blue indicate distributed steps\nand green for sequential steps. (v) The 2 GPU example for the LSS Transformer\u2019s forward pass.\nsegments. To compute Eqn. (1), each GPU must receive\nevery other GPU\u2019s segment to compute partial self atten-\ntion scores. For example, GPU 1 needs to receive K2 and\nK3 from GPUs 2 and 3 before GPU 1 can compute partial\nself-attention scores Q1KT\n2 and Q1KT\n3 . Then, the partial\nself-attention scores are aggregated across GPUs into the\ncomplete self-attention score, which is then used in dot prod-\nucts with the distributed value vector V . Since each GPU\nmust communicate with every other GPU multiple times,\nthe communication frequency for the straightforward dis-\ntributed self-attention method increases quadratically with\nmore GPUs, significantly limiting its scalability.\nTo address this limitation, Nvidia\u2019s method (Korthikanti\net al., 2022), referred to as \u201cbaseline sequence parallelism\u201d\nin the rest of the paper, computes self-attention and feed-\nforward sequentially to avoid the quadratically increased\ncross-GPU communications (Korthikanti et al., 2022). How-\never, it independently parallelizes layer normalization and\ndropouts in the sequence dimension, as they lack such inter-\ntoken dependencies (Korthikanti et al., 2022). Note that\nthe term \u201csequential computations\u201d pertains to single GPU\ncomputations, while \u201cparallel computations\u201d refer to those\nperformed across GPUs in the sequence dimension. Al-\nthough self-attention and feed-forward are computed within\na single GPU, their computations are still vectorized through\nparallel threads within the GPU.\nFig. 1(iv) summarizes the baseline sequence parallelism\nusing an example of 2 GPUs. During a forward pass, po-\nsitional and token embeddings are computed sequentially\nand the output is scattered into contiguous segments among\nthe GPUs along the sequence dimension. Then in the at-\ntention layers, the feed-forward and the self-attention are\nsequentially updated by a single GPU, and are represented\nby green rectangles in the figure. All other steps are inde-\npendently updated with sequence parallelism, represented\nby blue rectangles. Gather and scatter communications are\nused before and after self-attention and feed-forward to en-\nsure sequential updates for them and independent parallel\nupdates for all other steps. Finally, the results from GPUs\nare gathered for a final layer normalization, linear trans-\nform, and cross-entropy loss evaluation. In the backward\npass, all steps are the same as the forward pass, except the\ngather communications in the forward pass is replaced by\nreduce-scatter for gradient synchronization among GPUs\nand scatter communications in the forward pass are replaced\nwith gather operations in the backward pass.\nDespite that the baseline sequence parallelism avoid the\nfrequent GPU communications in self-attention as in the\nstraightforward distributed method, it has two main limita-\ntions. First, the most compute-intensive self-attention and\nfeed-forward steps are sequential. Secondly, the commu-\nnication overhead is still significant with 8 global commu-\nnications per attention layer (4 in forward pass and 4 in\nbackward pass). As a result, the baseline sequence paral-\nlelism achieves only 3% speedup on a 22-billion-parameter\nmodel compared to a baseline without backward pass re-\ncomputation, and up to 29% speedup compared to a baseline\nwith backward recomputation (Korthikanti et al., 2022).\nUltra-Long Sequence Distributed Transformer\n3\nTHE DISTRIBUTED LONG\nSHORT-SEQUENCE TRANSFORMER\nTo achieve excellent scalability, the LSS Transformer must\ndistribute self-attention, but also overcome the quadratically\nincreased communication frequency. The LSS Transformer\nperforms sequence parallelism based on these principles:\nPrinciple 1: Independent Computations with Distributed\nMemory. Except for self-attention, all other computations\nlike layer normalization, residual connection, dropout, feed-\nforward, linear transform, positional, and token embed-\nding can be independently computed and distributed among\nGPUs without dependencies in the sequence dimension.\nMemory storage for these steps is also distributed in the\nsequence dimension in the same manner.\nThe feed-forward operation can be independently computed\nby row-distributing its linear transform multiplication along\nthe sequence length dimension, enabling independent GPU\ncomputations. Additionally, GeLu activation and dropout\noperate independently on each element of their input. For in-\nstance, for a sequence input x of dimension lx \u00d7 Em, where\nlx is sequence length and Em is embedding size, the first\nlinear transform step in the feed-forward unit, linear(x) in\nFig. 1(iii), multiplies x with linear transform model parame-\nters of size Em \u00d7 Dinner, where Dinner is the dimension\nof the feed-forward hidden layer. This matrix multiplication\ncan be row-distributed among GPUs without communica-\ntion when input x is distributed into sequence segments xi\nof size lx\nN \u00d7 Em, where N is the number of sequence par-\nallel GPUs. After independent element-wise operations on\nGeLu and dropout, the sequence distributed output is used\nas distributed input for the second linear transform mul-\ntiplication in the feed forward unit, again row-distributed\nwithout communication.\nFig. 1(v) depicts the LSS Transformer\u2019s forward pass, show-\ncasing a demonstration with 2 sequence parallel GPUs. Note\nthat while the figure exemplifies a specific scenario, the LSS\nTransformer\u2019s operate at the attention layer level, and is uni-\nversally adaptable to various model sizes and types without\nmodification. In this figure, input sequence x is scattered to\nx1 and x2 in the sequence dimension and each GPU receives\na segment. Principle 1 enables all subsequent operations to\nbe sequence distributed among GPUs and computed inde-\npendently. In addition, each operations\u2019 inputs, intermediate\noutputs and their associated gradients are also stored in dis-\ntributed memory across sequence-parallel GPUs, enabling\nexcellent memory footprint reduction. We use blue rectan-\ngles in the figure to represent these independent computation\nsteps. The self-attention is marked by a shaded blue rectan-\ngle to indicate that self-attention is distributed yet requires\ninter-GPU communications.\nPrinciple 2: Sequence Distributed Positional Embedding.\nThe positional embedding parameters are a lookup table and\neach row of the table represents a token\u2019s spatial position\nwithin the sequence. The number of rows of the lookup table\ncorresponds to the sequence length. Since each sequence-\ndistributed GPU receives contiguous sequence segments, the\nGPU performs lookup operations only on the corresponding\ncontiguous rows of the lookup table. This allows for row\ndistribution of the positional embeddings among the GPUs\nwithout dependency.\nPrinciple 3: Distributed Self-Attention with Fused Com-\nmunications. To parallelize self-attention, we use the fol-\nlowing math property to preserve data dependency while\nminimizing communication overhead. By distributing the\nquery vector, Q, in the sequence dimension among GPUs,\nwe compute the self-attention output, E, as the following\nconcatenation in the sequence dimension, and in parallel:\nE =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nsoftmax\n\u0010\nQ1KT / \u221adk\n\u0011\nV\nsoftmax\n\u0010\nQ2KT / \u221adk\n\u0011\nV\nsoftmax\n\u0010\nQ3KT / \u221adk\n\u0011\nV\n...\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n(2)\nwhere Qi is an lx\nN \u00d7 Em distributed query vector segment\nreceived by the ith GPU, where N is the number of GPUs.\nV and K are lx \u00d7 Em collected value and key vectors\nwithout distribution, having the same copy across all GPUs.\nsoftmax(QiKT /\u221adk) represents the ith GPU\u2019s partial self-\nattention score for its assigned segment, but in the context\nof the whole sequence.\nIn summary, the key idea is to distribute the query vec-\ntor in the sequence dimension among GPUs, while keep-\ning the value and key vectors collected.\nThen LSS\nTransformer computes an individual self-attention output,\nsoftmax(QiKT /\u221adk)V , for each GPU. Eqn. (2) shows that\nconcatenating GPUs\u2019 individual self-attention outputs is nu-\nmerically the same as directly computing self-attention E\nsequentially, since the concatenation is a simple row gather-\ning for E. Therefore, our proposed distributed self-attention\nmethod is an exact method without approximation, thereby\nno accuracy loss. Compared to the straightforward sequence\nparallelism with quadratically increased communication fre-\nquency, a significant advantage of Eqn. (2) is that it enables\ndistributed computing while requiring only 6 communica-\ntions per self-attention layer. The forward pass requires\n2 communications from gathering value and key vectors\nand 1 from self-attention concatenation. The backward pass\nrequires another 3 communications.\nTo further reduce communication overhead, we use a fused\ncommunications technique to reduce the communication\nfrequency from 6 communications per layer to 4. Fig. 2(i)\ndemonstrates the operations in the forward pass without the\nfused communication. An example sequence segment, xi\nUltra-Long Sequence Distributed Transformer\nFigure 2. (i) and (ii) show the difference without and with fused communications. (iii) shows distributed self-attention\u2019s forward pass\nwith fused communications. Note that the distributed self-attention outputs are not concatenated. (iv) LSS Transformer\u2019s Backward pass.\nModel parameters, except the positional embedding, are synchronized through gradient averaging. (v) The distributed self-attention\u2019s\nbackward pass with reduce-scatter.\nis linearly transformed into query, key and value segments.\nThen, two all-gather communications are independently op-\nerated on the key and value segments into the collected K\nand V . Fig. 2(ii) shows the fused communication operation\nin the forward pass, requiring only a single all-gather com-\nmunication. xi is linearly transformed into query segment\nQi. Meanwhile, xi is gathered into a temporary collected\nsequence x, before x is linearly transformed into the col-\nlected key and value vectors. The same technique is also\napplied to backward pass, reducing the total number of\ncommunications from 6 to 4 per attention layer.\nPrinciple 4: Gradient Averaging Technique to Synchro-\nnize GPUs and Avoid Concatenation. There are two issues\nfrom Principles 1 and 3. First, since sequence parallel GPU\ntrains on the same model parameters but using different\ninput sequence segments, the gradients for the model pa-\nrameters are different for each GPU. The second issue is\nthat the self-attention communication frequency needs to\nbe further reduced to achieve even better scalability and\nparallel efficiency.\nTo address both issues, we use a gradient averaging tech-\nnique to synchronize model parameters and avoid the con-\ncatenation for the GPUs\u2019 individual self-attention outputs.\nTherefore, communication frequency is reduced from 4 to\n2 per attention layer. Figs. 2(iii)-(v) use a 2 GPU exam-\nple to demonstrate how this gradient averaging technique\nis applied. In the forward pass for the self-attention in\nFig. 2(iii), a distributed query Qi is computed from the in-\nput sequence segment xi. Meanwhile, the self-attention\ninput segments are gathered among GPUs before computing\ncollected K and V vectors using a single all-gather fused\ncommunication, as explained before in Principle 3. Subse-\nquent computations and memory storage are all distributed\nand independently updated in the sequence dimension, pro-\nducing individual self-attention output for each GPU.\nThe individual self-attention outputs, however, are not con-\ncatenated across GPUs in Fig. 2(iii). Instead, the LSS\nTransformer allows each GPU to use its assigned sequence\nsegment and individual self-attention output to compute a\npartial cross-entropy loss and gradients in the backward\npass in Figs. 2(iv) and (v). Note that the backward pass\nin Fig. 2(v) uses reduce-scatter as the backward operation\nfor the all-gather in the forward pass. Finally, the averaged\ngradients are computed and used for synchronized model pa-\nrameter updates before training on the next data batch. One\nimportant technical detail to mention is that the averaged\ngradients are not computed for the positional embeddings,\nwhich are distributed parameters across GPUs and should\nnot be synchronized.\nTo understand why this gradient averaging technique can\navoid self-attention concatenation and synchronize model\nparameters at the same time, let us assume that the predicted\nsequence output from transformer is y and its true label\nis \u02dcy. The cross-entropy loss for the whole sequence, de-\nnoted as L(y, \u02dcy), equals the average of individual token\u2019s\nloss: L(y, \u02dcy) =\n1\nlx\nPlx\ni=1 L(yi, \u02dcyi), where lx is sequence\nlength. According to the gradient summation rule, the gradi-\nent of L(y, \u02dcy) with respect to model parameters, denoted as\n\u2207L(y, \u02dcy), equals the averaged gradient of each token\u2019s loss:\n\u2207L(y, \u02dcy) =\n1\nlx\nPlx\ni=1 \u2207L(yi, \u02dcyi). Therefore, there is no\nUltra-Long Sequence Distributed Transformer\nFigure 3. Integrated sequence and data parallelisms with double\ngradient averaging. The horizontal direction gradient averaging\nsynchronizes parameters without positional embeddings, and the\nvertical direction gradient averaging includes positional embed-\ndings.\nneed to concatenate individual self-attention outputs to com-\npute the loss and gradients for the whole sequence. Instead,\neach GPU uses its scattered individual self-attention output\nto compute a partial loss and gradient for each sequence\nsegment, before averaging each segment\u2019s gradients for a\nsynchronized model parameters update.\nBy avoiding the expensive concatenation operations in each\nattention layer, LSS Transformer lowers its communication\nfrequency to only twice per attention layer (one all-gather\nin forward pass and one reduce-scatter in backward pass)\nsince the gradient averaging occurs only once per data batch.\nThis results in much better scalability and reduced commu-\nnications compared to other sequence parallel methods.\n4\nINTEGRATED SEQUENCE & DATA\nPARALLELISM\nThe LSS Transformer\u2019s sequence parallelism has three lim-\nitations. First, it still requires 2 global inter-GPU commu-\nnications per attention layer, which degrades parallel effi-\nciency at many GPUs. Second, while sequence parallelism\ntackles the long sequence issue, it does not address compu-\ntation challenge for training large dataset. Three, sequence\nparallelism is only one source of parallelism. To scale to\na large supercomputer for training, the LSS Transformer\nneeds more sources of parallelism to achieve better scalabil-\nity. To address these issues, this section introduces a method\nto integrate the LSS Transformer\u2019s sequence parallelism\nwith data parallelism. With the integration, the parallel algo-\nrithm can (1) achieve better scalability; (2) simultaneously\ntackle long sequence and large dataset challenges; and (3)\nconstrain the self-attention communications among local\ncommunicative groups for reduced overhead.\nDespite that sequence and data parallelisms are mostly or-\nthogonal, one technical challenge to overcome is that both\nparallelisms require model parameter synchronization, but\namong GPUs in different communicative groups and com-\nmunicate in different ways. Sequence parallelism requires\nmodel parameter synchronization among sequence parallel\nGPUs, but excludes positional embedding parameters from\nsynchronization given that positional embeddings are dis-\ntributed in sequence dimension. Data parallelism requires\nmodel parameter synchronization among data parallel GPUs,\nbut must include positional embeddings given that data par-\nallel GPUs have the same copy of the positional embedding\nparameters, but train them with different data batches.\nTo address this issue, we use an innovative double gradient\naveraging technique to avoid synchronization conflicts for\npositional embeddings. Fig. 3. illustrates an example of how\nthe integrated sequence and data parallelism uses double\ngradient averaging. In this example, GPUs 1 and 2 process\na sequence x1 together using sequence parallelism, with\nthe first segment x1\n1 assigned to GPU 1 and the second\nsegment x1\n2 assigned to GPU 2. The positional embedding\nparameters are distributed in the same way with the first half\nPE1 assigned to GPU 1 and the second half PE2 assigned\nto GPU 2. Similarly, GPUs 3 and 4 handle a difference\nsequence x2 using sequence parallelism.\nAll GPUs that process the same sequence form a sequence\nparallel group and each group is shown as a horizontal pur-\nple box in Fig. 3. Each sequence parallel group\u2019s cross-GPU\ncommunications, shown as a horizontal purple arrow, are lo-\ncal and confined among GPUs in the same sequence parallel\ngroup. These communications involve a fused all-gather and\na reduce-scatter in each attention layer for computing dis-\ntributed self-attention. In addition, a gradient averaging is\nrequired once per data batch for model parameters synchro-\nnization and avoiding self-attention output concatenation,\nas discussed before in Sec. 3. The positional embedding\nparameters, however, are excluded from gradient averaging,\nas they are distributed across sequence parallel GPUs, and\nshould not be synchronized.\nMeanwhile, GPUs 1 and 3 are in a data parallel group,\nshown as a vertical red box in the figure, and GPUs 2 and 4\nare in another data parallel group. GPUs in the same data\nparallel group process sequence segments from different\ndata batches, but these sequence segments share the same\nspatial position within their sequences, thereby sharing the\nsame positional embedding. The only needed inter-GPU\ncommunications in the same data parallel group, shown as\nvertical red arrows in the figure, are gradient averaging to\nsynchronize parameters trained with different batches. Sim-\nilar to sequence parallel groups, the communication for the\ndata parallel groups is also localized and confined within\neach group. The gradient averaging for data parallelism,\nUltra-Long Sequence Distributed Transformer\nTable 3. The LSS Transformer and Nvidia baseline sequence parallelism\u2019s weak scaling experiment for the small model experiment, using\nonly 1 data parallel group.\n(a) LSS Transformer, data parallel group = 1\nNodes\n1\n6\n18\n54\n144\nGPUs\n6\n36\n108\n324\n864\nSequence Groups\n6\n36\n108\n324\n864\nSequence Length\n348\n2088\n6264\n18792\n50112\nPer GPU Mem. (GB)\n0.54\n1.01\n2.05\n5.94\n13.58\nFLOPS (x1012 flop/s)\n8\n189\n881\n3000\n8245\nSelf-Attn Comp Incr.\n1\n6\n18\n54\n144\nParallel Efficiency\n100%\n165%\n174%\n173%\n151%\n(b) Baseline, data parallel group = 1\nNodes\n1\n6\n18\nGPUs\n6\n36\n108\nSequence Groups\n6\n36\n108\nSequence Length\n348\n2088\n6264\nPer GPU Mem. (GB)\n0.94\n10.29\nOOM\nFLOPS (x1012 flop/s)\n5\n32\nOOM\nSelf-Attn Comp Incr.\n1\n6\nOOM\nParallel Efficiency\n100%\n42%\nOOM\nTable 4. The LSS Transformer and Nvidia baseline weak scaling for the small model experiment, using 4 data parallel groups.\n(a) LSS Transformer, data parallel groups = 4\nNodes\n4\n24\n72\n216\n576\nGPUs\n24\n144\n432\n1296\n3456\nSequence Groups\n6\n36\n108\n324\n864\nSequence Length\n348\n2088\n6264\n18792\n50112\nPer GPU Mem. (GB)\n0.54\n1.01\n2.11\n5.94\n13.58\nFLOPS (x1012 flop/s)\n28\n703\n3319\n10987\n32784\nSelf-Attn Comp Incr.\n1\n6\n18\n54\n144\nParallel Efficiency\n100%\n167%\n173%\n159%\n161%\n(b) Baseline, data parallel groups = 4\nNodes\n4\n24\n72\nGPUs\n24\n144\n432\nSequence Groups\n6\n36\n108\nSequence Length\n348\n2088\n6264\nPer GPU Memory (GB)\n0.94\n10.29\nOOM\nFLOPS (x1012 flop/s)\n18\n126\nOOM\nSelf-Attn Comp Incr.\n1\n6\nOOM\nParallel Efficiency\n100%\n42%\nOOM\nhowever, must include positional embeddings for synchro-\nnization, given that the training segments in the same data\nparallel group share the same positional embedding.\n5\nRESULTS\n5.1\nExperiment Setup\nDataset: enwik8 is a popular 100-million-byte Wikipedia\nXML character training dataset (Hutter et al., 2006; Beltagy\net al., 2020). Initially used as the testing dataset for the\nHutter Prize, the dataset can be downloaded at (Mahoney,\n2006) for public benchmark evaluation with a score board\navailable at (with Codes, 2022).\nComputing platform: Experiments were conducted on Oak\nRidge National Lab\u2019s Summit supercomputer, which has 6\nNVIDIA V100 GPUs and 2 POWER9 CPUs for each node.\nNodes are connected via Mellanox EDR 100G InfiniBand\nNon-blocking Fat Tree network. Each POWER9 CPU in the\nnode is densely-connected to 3 GPUs with Nvidia NVlinks,\nwhere each link has 100 GB/s bidirectional bandwidth, and\nthe two CPUs for each node are connected via an X bus\nwith 64 GB/s bidirectional bandwidth. Each CPU has 22\ncores (4 hardware threads for each) and 256 GB DRAM\nmemory. Each GPU has 80 streaming multiprocessors and\n16 GB memory. There are additional 54 \u201chigh memory\u201d\nnodes, which has 32 GB of memory per GPU.\nSoftware: The LSS Transformer is developed in PyTorch\nand will be made publicly available in the next revision.\n5.2\nSmall Model Experiment\nModel: All experiments in the small model experiment sub-\nsection trains a decoder-only transformer (GPT) with a 512\nembedding size, 6 attention layers and 8 multi-heads for a\ntotal of 20 million parameters. The input data batch size is\n4. We choose this model size for two reasons. First, this\nis the standard model for the enwik8 benchmark evalua-\ntion with an excellent bits-per-character accuracy score at\n1.0 (Beltagy et al., 2020; Al-Rfou et al., 2019; Sukhbaatar\net al., 2019). Second, choosing a small model size allows us\nto maximize memory usage and evaluate performance for\nscaling long sequences, instead of maximizing memory for\nstoring parameters for a large model.\nSequence Parallelism Weak Scaling: Tables 3(a) and 3(b)\nare the weak scaling performance comparison between the\nLSS Transformer and Nvidia baseline sequence parallelism.\nWe increased both the sequence lengths and the number of\nsequence parallel GPUs at the same rate, while keeping the\nnumber of data parallel group to be 1.\nThe first two rows of the tables indicate the number of\nnodes and GPUs, with 6 GPUs per node. The third row\nrepresents the number of sequence parallel groups, which is\nequal to the number of GPUs in this case as the number of\nUltra-Long Sequence Distributed Transformer\n(a) Scaling efficiency with and without fused\ncommunication and gradient averaging\n(b) Maximum sequence length\n(c) Runtimes breakdown for Table 3(a)\nFigure 4. (a) Scaling efficiency with and without fused communication and gradient averaging. (b) maximum sequence length at different\nnumbers of sequence parallel GPUs. (c) the runtime breakdown for the LSS Transformer\u2019s small model experiment weak scaling with\nonly 1 data parallel group.\nTable 5. Performance comparison between the two algorithms on the small model, using only 1 data parallel group. Sequence lengths\nincrease sub-linearly with a growth rate proportional to the square root of the number of GPUs.\n(a) LSS Transformer, data parallel group = 1\nNodes\n1\n6\n18\n54\n144\nGPUs\n6\n36\n108\n324\n864\nSequence Groups\n6\n36\n108\n324\n864\nSequence Length\n366\n900\n1512\n2592\n4320\nPer GPU Mem. (GB)\n0.53\n0.62\n0.74\n0.89\n1.15\nFLOPS (x1012 flop/s)\n8\n83\n266\n673\n1280\nSelf-Attn Comp Incr.\n1\n2\n4\n6\n10\nParallel Efficiency\n100%\n120%\n114%\n96%\n72%\n(b) Baseline, data parallel group = 1\nNodes\n1\n6\n18\nGPUs\n6\n36\n108\nSequence Groups\n6\n36\n108\nSequence Length\n366\n900\n1512\nPer GPU Mem. (GB)\n0.89\n2.55\n5.86\nFLOPS (x1012 flop/s)\n5\n28\n71\nSelf-Attn Comp Incr.\n1\n2\n4\nParallel Efficiency\n100%\n56%\n36%\ndata parallel group is 1. The fourth row shows the sequence\nlengths, increasing proportionally with the number of GPUs.\nThe fifth row displays the average per-GPU peak mem-\nory footprint in Gigabytes (GBs). At 6 nodes, the LSS\nTransformer\u2019s per-GPU memory footprint is 1.01 GB, and\nit scales to 144 nodes with 13.58 GB per GPU. Since Trans-\nformer has a quadratic memory complexity of O(l2\nx/N),\nwhere lx is sequence length and N is the number of GPUs,\nincreasing sequence length lx and number of GPUs N at\nthe same rate will still lead to a linear increase of memory.\nThis explains why LSS Transformer has a small memory\nfootprint at 1 node but increases to much larger memory\nfootprint at more nodes. In comparison, the baseline se-\nquence parallelism has a per-GPU memory footprint of\n10.29 GB at 6 nodes, over 10 times larger than that of the\nLSS Transformer at the same nodes. The baseline sequence\nparallelism cannot scale beyond 6 nodes due to memory\nconstraint, resulting in \u201dOOM\u201d (out of memory).\nThe sixth row represents the number of single-precision\nfloating point operations (FLOP) across all GPUs in a sec-\nond. The LSS Transformer achieves a computation through-\nput of 8 petaflops at 144 nodes with sequence length of\n50,112. In comparison, the baseline sequence parallelism is\n5.9 times slower at 6 nodes, achieving a throughput of 32\nteraflops, and cannot scale further due to memory constraint.\nThe seventh row shows the recorded per-GPU computations\nincrease for the self-attention unit relative to the per-GPU\ncomputations at 1 node. Since transformer has a cubic com-\nputation complexity, distributing computations across GPUs\nwill still lead to computation increase for weak scaling.\nThe eighth row represents parallel efficiencies, which is\nthe ratio between the actual speedup and the ideal speedup.\nThe LSS Transformer maintains a 151% super-linear par-\nallel efficiency at 144 nodes, while the baseline sequence\nparallelism\u2019s efficiency drops to 42% at only 6 nodes.\nIntegrated Sequence & Data Parallelism Weak Scaling:\nTo understand how the integration of data and sequence\nparallelisms accelerates training speed and reduces commu-\nnication overhead, Table 4(a) repeats the same experiment\nas Table 3(a), but with 4 data parallel groups. This means\nthat the total number of GPUs quadruples accordingly, but\nthe numbers of sequence parallel groups remain the same as\nbefore. By comparing the results between the 4 data parallel\ngroups and the single data parallel group in Tables 3 and 4,\nwe observe that the FLOPs throughput increases by almost\n4 times from 1 to 4 data parallel groups with 4 times more\nGPUs, achieving 32 petaflops at 3456 GPUs. This result\nindicates that the proposed local communication scheme en-\nUltra-Long Sequence Distributed Transformer\nables the integrated sequence and data parallelism with little\nadditional communication overhead and the integration of\nthese two parallelisms is an effective approach for achieving\nmore scalability and faster training.\nSuper-Linear Speedup: The LSS Transformer achieves\nsuper-linear scaling in Tables 3 and 4 due to two reasons.\nFirst, longer sequences result in increased work for each\nGPU due to the self-attention computation increase, leading\nto higher GPU utilization rates for longer sequences and\nsuper-linear scaling. Measured by PyTorch Cuda utiliza-\ntion report, GPU utilization rate for the LSS Transformer\nincreases from 33% at 1 node with a sequence length of 348\nto 83% at 6 nodes with a sequence length of 2,088. Sec-\nond, the LSS Transformer\u2019s low communication overhead\nsignificantly contributes to its excellent parallel efficiency.\nFig. 4(a) shows the scaling efficiency with and without the\nfused communication and gradient averaging techniques,\nwhich were both introduced in Sec. 3. At 864 GPUs and\nsequence length of 50,112, the scaling efficiency with both\ntechniques are 151%. Efficiency with gradient averaging but\nwithout fused communication is 147%, whereas efficiency\nwithout either technique is dropped to 118%.\nMaximal Sequence Length: Fig. 4(b) depicts the maximal\nsequence length when scaling the number of GPUs while\nmaximizing memory capacity. Each numbered pair in the\nfigure corresponds to the number of GPUs and the maximal\nsequence length. For example, (6, 0.59) indicates that 6\nGPUs can scale up to a maximal sequence length of 0.59 \u00d7\n104. We can observe that the maximal sequence length\nfollows a square root curve in the graph. Since transformer\nhas quadratic memory complexity with longer sequences,\nthe maximal sequence length increases asymptotically with\nsquare root functions as the total memory capacity grows.\nRuntime Breakdown. Fig. 4(c) illustrates the runtime break-\ndown for the weak scaling results presented in Table 3(a),\nfocusing on a single data parallel group. The blue bars\nrepresent the percentage of runtime spent on computations,\nwhile the orange bars indicate the percentage of runtime for\ncommunications. The grey bars denote the GPU waiting\ntimes. At 36 GPUs, the communication overhead accounts\nfor 27% of the total runtime. As the number of GPUs scales\nto 864 (144 nodes), the communication overhead becomes\n40% of the runtime.\nScaling Sub-Linearly Increased Sequence Lengths. One\nway to limit memory and computations increase is to in-\ncrease the sequence lengths at a sub-linear rate. Table 5\nrepeats the same scaling experiment as Table 3, but with\nthe sequence lengths increasing proportionally to the square\nroot of the number of GPUs. As a result, both memory\nand self-attention computations from rows 5 and 7 of the\ntable also increase at a rate asymptotic to square root func-\ntions. The memory footprint for the LSS-Transformer is\nonly at 1.15 GB per GPU when scaling to 864 GPUs, and\nthe self-attention computations is only 10 times more than\nthat for a single node. The LSS-Transformer\u2019s scaling re-\nmains highly efficient at 94% with 864 GPUs. In contrast,\nscaling efficiency for Nvidia\u2019s sequence parallelism drops to\n31% at 108 GPUs and cannot scale further due to memory\nconstraints.\n5.3\nLarge Model Experiment\nTable 6 repeats the same experiment as Table 5, but trains\na large 1.5-billion-parameter GPT model that has a 2048\nembedding size, 24 attention layers and 16 multi-heads.\nexperiments are run on the high-memory nodes of Summit\nwith 32 GBs of memory per GPU, and no model parallelism\nis used for this experiment. Since most of the memory\ncapacity is now used for storing model parameters instead\nof long sequences, we can notice that all runs in this table\nuse much more memory than the small model experiment.\nLSS Transformer maintains a high scaling efficiency of 92%\nat 108 GPUs and a smaller memory footprint compared to\nthe baseline parallelism. In contrast, the baseline parallelism\ncannot scale beyond a single node due to memory constraint\nan its FLOPs throughput is 2.3 times smaller than that for\nLSS Transformer at a single node.\n6\nCONCLUSION\nThis paper introduced the Long-Short Sequence Trans-\nformer (LSS Transformer), a novel algorithm and a general\nframework for distributing long sequence in transformer\nmodels. It uses a novel distributed self-attention mecha-\nnism, along with fused communication and double gradient\naveraging techniques, to achieve impressive speedups and\nmemory reduction with minimal communication overhead.\nIn conclusion, the LSS Transformer is a significant step for-\nward for addressing transformer\u2019s long sequence problem.\nWe believe that our approach provides an important contri-\nbution to the research field and enables ultra-long sequence\ntraining, especially to applications that benefit from long-\nrange token dependencies, such as DNA sequence analysis,\nlong document summary, and imaging applications.\nREFERENCES\nAl-Rfou,\nR.,\nChoe,\nD.,\nConstant,\nN.,\nGuo,\nM.,\nand Jones,\nL.\nCharacter-level language model-\ning with deeper self-attention.\nProceedings of the\nAAAI Conference on Artificial Intelligence, 33(01):\n3159\u20133166, July 2019.\ndoi:\n10.1609/aaai.v33i01.\n33013159. URL https://ojs.aaai.org/index.\nphp/AAAI/article/view/4182.\nBeltagy, I., Peters, M. E., and Cohan, A. Longformer: The\nUltra-Long Sequence Distributed Transformer\nTable 6. Performance comparison between the two algorithms on a large 1.5 billion model, using only 1 data parallel group, and without\nany model parallelism. Sequence lengths increase with a growth rate proportional to the square root of the number of GPUs.\n(a) LSS Transformer, data parallel group = 1\nNodes\n1\n6\n18\nGPUs\n6\n36\n108\nSequence Groups\n6\n36\n108\nSequence Length\n366\n900\n1512\nPer GPU Mem. (GB)\n21.67\n22.48\n23.34\nFLOPS (x1012 flop/s)\n52\n518\n2010\nSelf-Attn Comp Incr.\n1\n2\n4\nParallel Efficiency\n100%\n94%\n92%\n(b) Baseline, data parallel group = 1\nNodes\n1\n6\nGPUs\n6\n36\nSequence Groups\n6\n36\nSequence Length\n366\n900\nPer GPU Mem. (GB)\n25.28\nOOM\nFLOPS (x1012 flop/s)\n23\nOOM\nSelf-Attn Comp Incr.\n1\nOOM\nParallel Efficiency\n100%\nOOM\nlong-document transformer, 2020.\nCaldarini, G., Jaf, S., and McGarry, K. A literature survey\nof recent advances in chatbots. Information, 13(1):41,\n2022.\nChen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and R\u00b4e,\nC. Scatterbrain: Unifying sparse and low-rank attention.\nAdvances in Neural Information Processing Systems, 34:\n17413\u201317426, 2021a.\nChen, C.-F. R., Fan, Q., and Panda, R. Crossvit: Cross-\nattention multi-scale vision transformer for image classi-\nfication. In Proceedings of the IEEE/CVF international\nconference on computer vision, pp. 357\u2013366, New York,\nNY, USA, 2021b. IEEE.\nChen, R. J., Chen, C., Li, Y., Chen, T. Y., Trister, A. D.,\nKrishnan, R. G., and Mahmood, F.\nScaling vision\ntransformers to gigapixel images via hierarchical self-\nsupervised learning. In 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pp.\n16123\u201316134, New York, NY, USA, 2022. IEEE. doi:\n10.1109/CVPR52688.2022.01567.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Generat-\ning long sequences with sparse transformers, 2019. URL\nhttps://arxiv.org/abs/1904.10509.\nChoromanski, K., Likhosherstov, V., Dohan, D., Song, X.,\nGane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,\nA., Kaiser, L., Belanger, D., Colwell, L., and Weller, A.\nRethinking attention with performers. In The Interna-\ntional Conference on Learning Representations (ICLR),\nNew York, NY, USA, 2021. Association for Computing\nMachinery. doi: 10.48550/ARXIV.2009.14794. URL\nhttps://arxiv.org/abs/2009.14794.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C.\nFlashattention: Fast and memory-efficient exact atten-\ntion with io-awareness. In NeurIPS: Proceedings of the\n35th Neural Information Processing Systems Conference,\nNew York, NY, USA, 2022. Association for Computing\nMachinery. doi: 10.48550/ARXIV.2205.14135. URL\nhttps://arxiv.org/abs/2205.14135.\nDong, L., Xu, S., and Xu, B.\nSpeech-transformer: A\nno-recurrence sequence-to-sequence model for speech\nrecognition. In 2018 IEEE international conference on\nacoustics, speech and signal processing (ICASSP), pp.\n5884\u20135888, New York, NY, USA, 2018. IEEE.\nHutter, M., Mahoney, M., and Bowery, J. 500\u2019000 Cprize\nfor compressing human knowledge. http://prize.\nhutter1.net/, 2006. Accessed: 2023-03-05.\nJacobs, S. A., Tanaka, M., Zhang, C., Zhang, M., Song,\nS. L., Rajbhandari, S., and He, Y. Deepspeed ulysses:\nSystem optimizations for enabling training of extreme\nlong sequence transformer models, 2023.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transformers\nwith linear attention. In Proceedings of the 37th Inter-\nnational Conference on Machine Learning, ICML\u201920,\nNew York, NY, USA, 2020. Association for Computing\nMachinery.\nKitaev, N., Kaiser, L., and Levskaya, A. Reformer: The\nefficient transformer. In The International Conference\non Learning Representations (ICLR), New York, NY,\nUSA, 2020. Association for Computing Machinery. doi:\n10.48550/ARXIV.2001.04451. URL https://arxiv.\norg/abs/2001.04451.\nKorthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch,\nM., Shoeybi, M., and Catanzaro, B. Reducing activation\nrecomputation in large transformer models, 2022. URL\nhttps://arxiv.org/abs/2205.05198.\nLi, D., Shao, R., Xie, A., Xing, E. P., Gonzalez, J. E.,\nStoica, I., Ma, X., and Zhang, H. Lightseq: Sequence\nlevel parallelism for distributed training of long context\ntransformers, 2023.\nUltra-Long Sequence Distributed Transformer\nLi, S., Xue, F., Baranwal, C., Li, Y., and You, Y. Sequence\nparallelism: Long sequence training from system perspec-\ntive, 2021. URL https://arxiv.org/abs/2105.\n13120.\nMahoney,\nM.\nEnwik8\ntest\ndata.\nhttps://\nmattmahoney.net/dc/textdata.html,\n2006.\nAccessed: 2023-03-05.\nRabe, M. N. and Staats, C. Self-attention does not need\no(n2) memory, 2022.\nRoy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient\nContent-Based Sparse Attention with Routing Transform-\ners. Transactions of the Association for Computational\nLinguistics, 9:53\u201368, 02 2021. ISSN 2307-387X. doi:\n10.1162/tacl a 00353. URL https://doi.org/10.\n1162/tacl_a_00353.\nShi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., and\nKwok, J. T.\nSparsebert: Rethinking the importance\nanalysis in self-attention. In Proceedings of the 38th\nInternational Conference on Machine Learning, vol-\nume 139 of Proceedings of Machine Learning Research,\npp. 9547\u20139557, New York, NY, USA, 2021. PMLR.\nURL http://proceedings.mlr.press/v139/\nshi21a.html.\nSi, Y. and Roberts, K. Three-level hierarchical transformer\nnetworks for long-sequence and multiple clinical doc-\numents classification, 2021. URL https://arxiv.\norg/abs/2104.08444.\nStrudel, R., Garcia, R., Laptev, I., and Schmid, C. Seg-\nmenter: Transformer for semantic segmentation. In Pro-\nceedings of the IEEE/CVF international conference on\ncomputer vision, pp. 7262\u20137272, New York, NY, USA,\n2021. IEEE.\nSukhbaatar, S., Grave, E., Bojanowski, P., and Joulin,\nA.\nAdaptive attention span in transformers.\nIn Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pp. 331\u2013335, Flo-\nrence, Italy, July 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1032. URL https:\n//aclanthology.org/P19-1032.\nValanarasu, J. M. J., Oza, P., Hacihaliloglu, I., and Patel,\nV. M. Medical transformer: Gated axial-attention for med-\nical image segmentation. In Medical Image Computing\nand Computer Assisted Intervention\u2013MICCAI 2021: 24th\nInternational Conference, Strasbourg, France, September\n27\u2013October 1, 2021, Proceedings, Part I 24, pp. 36\u201346,\nNew York, NY, USA, 2021. Springer.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Proceedings of the 31st International\nConference on Neural Information Processing Systems,\nNIPS\u201917, pp. 6000\u20136010, Red Hook, NY, USA, 2017.\nCurran Associates Inc. ISBN 9781510860964.\nWang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F.,\nand Chao, L. S. Learning deep transformer models for\nmachine translation. In 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pp. 1810\u20131822,\nNew York, NY, USA, 2019. Association for Computing\nMachinery. doi: 10.48550/ARXIV.1906.01787. URL\nhttps://arxiv.org/abs/1906.01787.\nwith Codes,\nP.\nLanguage modelling on enwik8.\nhttps://paperswithcode.com/sota/\nlanguage-modelling-on-enwiki8,\n2022.\nAccessed: 2023-03-05.\nYing, C., Ke, G., He, D., and Liu, T.-Y. Lazyformer: Self\nattention with lazy update, 2021.\nYu, J., Li, J., Yu, Z., and Huang, Q. Multimodal transformer\nwith multi-view visual representation for image caption-\ning. IEEE Transactions on Circuits and Systems for Video\nTechnology, 30(12):4467\u20134480, 2019.\nYu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer,\nL., and Lewis, M. Megabyte: Predicting million-byte\nsequences with multiscale transformers, 2023.\nZaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al-\nberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,\nYang, L., et al. Big bird: Transformers for longer se-\nquences. Advances in Neural Information Processing\nSystems, 33:17283\u201317297, 2020.\n"
  },
  {
    "title": "Co-training and Co-distillation for Quality Improvement and Compression of Language Models",
    "link": "https://arxiv.org/pdf/2311.02849.pdf",
    "upvote": "2",
    "text": "Co-training and Co-distillation for\nQuality Improvement and Compression of Language Models\nHayeon Lee1\u2217 Rui Hou1 Jongpil Kim1\nDavis Liang2\nHongbo Zhang1\nSung Ju Hwang3 Alexander Min1\nMeta AI1\nAbridge AI2\nKAIST3\nhayeonlee@meta.com rayhou@meta.com jongpil@meta.com\ndavis@abridge.com hbzhang@meta.com sjhwang82@kaist.ac.kr alexmin@meta.com\nAbstract\nKnowledge Distillation (KD) compresses com-\nputationally expensive pre-trained language\nmodels (PLMs) by transferring their knowl-\nedge to smaller models, allowing their use\nin resource-constrained or real-time settings.\nHowever, most smaller models fail to surpass\nthe performance of the original larger model,\nresulting in sacrificing performance to improve\ninference speed. To address this issue, we pro-\npose Co-Training and Co-Distillation (CTCD),\na novel framework that improves performance\nand inference speed together by co-training two\nmodels while mutually distilling knowledge.\nThe CTCD framework successfully achieves this\nbased on two significant findings: 1) Distill-\ning knowledge from the smaller model to the\nlarger model during co-training improves the\nperformance of the larger model. 2) The en-\nhanced performance of the larger model further\nboosts the performance of the smaller model.\nThe CTCD framework shows promise as it can\nbe combined with existing techniques like ar-\nchitecture design or data augmentation, replac-\ning one-way KD methods, to achieve further\nperformance improvement. Extensive ablation\nstudies demonstrate the effectiveness of CTCD,\nand the small model distilled by CTCD outper-\nforms the original larger model by a significant\nmargin of 1.66 on the GLUE benchmark.\n1\nIntroduction\nIn recent years, the high computational cost of pre-\ntrained language models (PLMs) (Radford et al.,\n2019; Yang et al., 2019; Dai et al., 2019; Shoeybi\net al., 2019; Li et al., 2020; Brown et al., 2020)\nbecome a constraint in serving resource-limited\nor real-time applications. Knowledge Distillation\n(KD) (Hinton et al., 2015; Romero et al., 2015) is\na popular model compression technique to tackle\nthis issue, with a smaller model (student) learning\n(distilling) from a larger model (teacher).\n\u2217 Work done while interning at Meta AI.\nThe ideal scenario would involve compressing\nthe teacher to match the size of the small student\nwithout any performance drop. However, despite\nintense research in the area of KD (Turc et al.,\n2019; Tsai et al., 2019; Tang et al., 2019; Jiao et al.,\n2020; Sanh et al., 2019a; Sun et al., 2019; Wang\net al., 2020b,a), none of the existing methods have\nsuccessfully avoided performance degradation dur-\ning the distillation process. Some approaches have\nattempted to mitigate the performance gap by in-\ncorporating external factors. For instance, Jiao\net al. (2020) incorporates data augmentation, while\nWang et al. (2020b,a) focuses on designing student\narchitecture. However, these approaches have lim-\nitations as they only provide supplementary tech-\nniques rather than addressing the fundamental issue\nof performance loss in KD. This raises an interest-\ning question: Can we compress a model without\nscarifying the performance through KD?\nIn this work, we propose a novel framework\ncalled Co-Training and Co-Distillation (CTCD) for\nimproving the performance of a language model\nwhile compressing it through KD. CTCD involves\njointly training the teacher and the student models,\nallowing them to transfer knowledge to each other\nbidirectionally, from the teacher to the student and\nvice versa. Our work uncovers two key findings\nwithin the CTCD framework: Firstly, we demon-\nstrate that transferring knowledge from the smaller\nmodel to the larger model during co-training sig-\nnificantly improves the performance of the larger\nmodel. In other words, by employing knowledge\ndistillation (KD), we enhance the performance of\nthe teacher model compared to standalone training.\nThis is significantly different from conventional\none-way KD, where the teacher model cannot ben-\nefit from the distillation process since it is no longer\ntrained or distilled. Secondly, the improved perfor-\nmance of the larger model leads to further enhance-\nments in the performance of the smaller model\nthrough KD from the teacher model to the student\narXiv:2311.02849v2  [cs.CL]  7 Nov 2023\nFigure 1: Task-agnostic KD method The proposed framework are task-agnostic KD methods that perform KD during the\npre-training stage. This allows the distilled model to be deployed and fine-tuned on various downstream tasks.\nmodel. These novel findings enable the smaller\nmodel to surpass the independently trained larger\nmodel while maintaining its inference efficiency\nwithin the CTCD framework. Moreover, the CTCD\nframework can be combined orthogonally with ex-\nisting techniques for external factors such as stu-\ndent architecture design Wang et al. (2020b,a) or\ndata augmentation (Jiao et al., 2020). We achieve\nthis by replacing traditional one-way KD methods\nwith the proposed CTCD framework, which holds\npromise for significant performance improvements\nin language modeling tasks.\nIn addition to our CTCD framework, we introduce\na Community KD that utilizes CTCD framework\nwith the pre-trained teacher model by combining\nCTCD with conventional KD. Community KD dis-\ntills knowledge from two students to each other, as\nwell as from the pre-trained teacher to each student,\nduring the co-training of the students. We show\nthat distilling from the other student as well as the\npre-trained teacher is better than only one-way dis-\ntillation from the pre-trained teacher, as is done in\nconventional KD. Note that the inference cost on\na task is the same as in conventional KD, as we\ntake one of the students distilled by Community\nKD when deploying it to the task.\nWe validate the effectiveness and efficiency of\nour CTCD on the GLUE benchmark (Wang et al.,\n2019), which contains different language under-\nstanding tasks. As shown in Figure 1, We focus\non task-agnostic KD scenarios, where KD occurs\nduring the pre-training stage. Then, we fine-tune\nthe distilled PLMs on each downstream task and\nevaluate their performance. This is more challeng-\ning compared to task-specific KD, as it requires\nsubstantial computational resources to train PLMs\non a large text corpus, and the distilled knowledge\nshould be transferable across diverse downstream\ntasks. Our extensive ablation study revealed that\nthe larger model benefits from the distillation of the\nsmaller model, and the performance improvement\nof the larger model further enhances the perfor-\nmance of the smaller model. In our experiments,\nthe student model compressed by CTCD framework\nobtained 1.66 higher gain than the original large\nmodel trained using a stand-alone method, demon-\nstrating that our approach can improve model qual-\nity and inference efficiency concurrently.\nIn summary, our contributions are as follows:\n\u2022 We propose a novel knowledge distilla-\ntion framework called Co-Training and Co-\nDistillation (CTCD) framework to improve the\nperformance of models while compressing\nthem through KD.\n\u2022 Through our experiments, we demonstrate\nthat distilling knowledge from the smaller\nmodel to the larger model during co-training\nimproves the performance of the larger model.\n\u2022 Additionally, we highlight that the enhanced\nperformance of the larger model further boosts\nthe performance of the smaller model, result-\ning in improved performance for both models.\n\u2022 We provide valuable insights into adjusting\nloss weights and the length of the training\nphase for the effective application of the CTCD\nframework through extensive ablation study.\n2\nRelated Work\nOne-way Knowledge Distillation\nKnowledge\ndistillation (KD) (Hinton et al., 2015) is a model\ncompression technique in which a smaller student\nmodel distills knowledge from a larger, pre-trained\nteacher model. Recently, many researchers have\nexplored KD to address a high computational com-\nplexity of a language model resulting from its in-\ncreasing size. This includes task-agnostic KD meth-\nods for the pre-training stage (Sanh et al., 2019a;\nWang et al., 2020b,a), task-specific KD method for\nPrediction\nCo-distillation\nLarge \nText Corpus\nTeacher\n\u2026\nScratch\nStudent\n\u2026\nScratch\nPrediction\nCo-training\nteacher soft loss\nstudent soft loss\nTeacher\nSoft Label\nStudent \nSoft Label\nBack propagation\nLoss = teacher hard loss + teacher soft loss\nCo-training\nHard Label\nBack propagation\nLoss = student hard loss + student soft loss\nFigure 2: Co-Training and Co-Distillation (CTCD) During training, the teacher and student models can learn more effectively\nby comparing their prediction outputs not only against the ground truth but also against each other\u2019s predictions. We refer to\nthe former as a \"hard\" loss and the latter as a \"soft\" loss. For example, the student soft loss captures the distance between the\nstudent\u2019s prediction and the teacher\u2019s prediction, and vice versa. This co-training and co-distillation approach improve the\nperformance of the teacher model, which in turn benefits the performance of the student model.\nthe fine-tuning stage (Sun et al., 2019), and both\nof pre-training and fine-tuning stages (Jiao et al.,\n2020). Additionally, Wang et al. (2020b,a) have\nredesigned the architectures of student language\nmodels. However, the one-way KD can lead to a\nloss of knowledge, resulting in smaller models that\ngenerally have difficulty matching the performance\nof larger models, leading to performance degrada-\ntion. In contrast, our CTCD can improve the quality\nof both teacher and student models, making it pos-\nsible for the student to achieve the same quality as\nthe original teacher model.\nReversed Knowledge Distillation\nRecently, re-\nsearchers (Yuan et al., 2020; Qin et al., 2022;\nLee et al., 2023) have demonstrated that reversed\nKnowledge Distillation (reversed KD), which trans-\nfers knowledge from a smaller or poorer model to\na larger model, can improve the performance of the\nstudent model. In particular, Qin et al. (2022); Lee\net al. (2023) investigated the application of reversed\nKD in the PLMs, showing that a larger model can\nbenefit from a poorer and pre-trained model for\na specific downstream task. Inspired by the suc-\ncess of reversed KD, we design a co-distillation\nframework that includes reversed KD to improve\nthe performance of the teacher model by distilling\nknowledge from the smaller student model. Unlike\nexisting reversed KD methods, which are limited\nto improving the performance of the larger model,\nour proposed co-distillation framework can achieve\nboth performance improvement and model com-\npression, by showing a better-quality teacher leads\nto a better-quality student.\n3\nCo-training and Co-distillation\nWe first introduce the concepts of co-training and\nco-distillation briefly:\nCo-training\ntrains two (different-sized) models\n(e.g., a teacher and student) concurrently with the\ngoal of achieving similar model quality.\nCo-distillation\ntransfers knowledge in both di-\nrections between two models (e.g., a teacher and\nstudent), during co-training.\nFigure 2 illustrates how co-trained models learn\ntogether by comparing their prediction outputs\nagainst predictions from each other and to the hard\nlabels (or \u201cground truth\u201d). We refer to the former\nas a \u201csoft\u201d loss and the latter as a \u201chard\u201d loss. For\ninstance, the soft loss of the student model mea-\nsures the accuracy of the student\u2019s prediction by\nconsidering the teacher\u2019s prediction as a soft label,\nand vice versa.\nTask Formulation\nSuppose that we are given a\nclassification task with K classes. For each training\ninstance x and its ground truth label y, we denote\nthat the ground truth distribution over the labels is\nq(k|x) (q(k) for simplicity) where for each label\nk \u2208 {1...K}, q(y) = 1 and q(k) = 0 for all k \u0338= y.\nFor each x, the teacher model t\u03d5 parameterized by\n\u03d5 and the student model s\u03b8 parameterized by \u03b8\npredict the probability of each label k as p\u03c4\n\u03d5(k|x)\nand p\u03c4\n\u03b8(k|x), respectively as follows:\np\u03c4\n\u03d5(k|x) = f(zt) =\nexp(zt\nk/\u03c4)\nP K\ni=1 exp(zt\ni/\u03c4)\np\u03c4\n\u03b8(k|x) = f(zs) =\nexp(zs\nk/\u03c4)\nP K\ni=1 exp(zs\ni /\u03c4)\nwhere f is the softmax function, zt = {zt\ni}K\ni=1 =\nt\u03d5(x) is the output logit of the teacher model, zs =\n{zs\ni }K\ni=1 = s\u03b8(x) is the output logit of the student\nmodel, and \u03c4 is the temperature to soften p\u03d5(k)\nand p\u03b8(k).\nPrediction\nLarge \nText Corpus\nHard Label\nTeacher\n\u2026\nPre-trained\nStudent 1\n\u2026\nScratch\n\u2026\nScratch\nStudent 2\nsoft loss\nsoft loss\nTeacher\nSoft Label\nStudent 2 \nSoft Label\nCo-distillation\nStudent 1 \nSoft Label\nsoft loss\nhard loss\nhard loss\nsoft loss\nCo-training\nBack propagation\nLoss = hard loss + soft loss from student 2 + soft loss from teacher \nPrediction\nCo-training\nBack propagation\nLoss = hard loss + soft loss from student 2 + soft loss from teacher \nPrediction\nFigure 3: Community KD Different from conventional KD, where each student learns from the prediction of the pre-trained\nteacher only, the proposed approach learns each student from both prediction of the pre-trained teacher and prediction of another\nstudent during co-training. Note that since we take one of the pre-trained students to adapt it to downstream tasks, the inference\ncost is the same as the student training with the conventional KD.\nThe proposed objective LCTCD(\u03b8, \u03d5) consists\nof a normal KD objective LKD:t\u2192s to distill knowl-\nedge from the teacher model to the student model\nand a reversed KD objective LReKD:s\u2192t to distill\nknowledge from the student model to the teacher\nmodel, during their co-training.\nTeacher \u2192 Student\nThe normal KD objective\nLKD:t\u2192s(\u03b8, \u03d5) aims to train the student model by\nminimizing a weighted sum of the cross-entropy\nloss H(q, p\u03b8) between the ground truth q and\nstudent prediction p\u03b8 and Kullback-Leibler diver-\ngence (KL divergence) D(p\u03c4\n\u03d5, p\u03c4\n\u03b8) between the pre-\ndictions of the student and the teacher as follows:\nLKD(\u03b8, \u03d5) = \u03b1h \u00b7 H(q, p\u03b8) + \u03b1s \u00b7 D(p\u03c4\n\u03d5, p\u03c4\n\u03b8)\n(1)\nwhere\nH(q, p\u03b8) = \u2212\nK\nX\nk=1\nq(k) log(p\u03b8(k)),\nD(p\u03c4\n\u03d5, p\u03c4\n\u03b8) =\nK\nX\nk=1\np\u03c4\n\u03d5(k) \u00b7 log\np\u03c4\n\u03d5(k)\np\u03c4\n\u03b8(k) ,\n\u03b1h and \u03b1s are weighting hyper-parameter val-\nues for the cross-entropy loss and KL divergence,\nrespectively.\nWe regard the cross-entropy loss\nH(q, p\u03b8) as the hard loss for the student model,\nKL divergence D(p\u03c4\n\u03d5, p\u03c4\n\u03b8) as the soft loss for the\nstudent model, and following BERT (Devlin et al.,\n2019), H(q, p\u03b8) denotes the Masked Language\nModeling loss (MLM loss). In the KD objective,\nwe consider the teacher parameters \u03d5 as constant\nsince we only train the student parameters \u03b8 while\nthe teacher model t\u03d5 is fixed:\nLKD:t\u2192s(\u03b8, StopG(\u03d5))\n(2)\nwhere StopG(x) denotes that we do not compute\nthe gradient of x. In the conventional KD method,\nEquation (2) is the final objective to learn the stu-\ndent model only with the pre-trained teacher model.\nStudent \u2192 Teacher\nDifferent from such a one-\nway KD method, we introduce the reversed KD\nobjective LReKD:s\u2192t(StopG(\u03b8), \u03d5) to train the\nteacher model t\u03d5 as follows:\nLReKD:s\u2192t(StopG(\u03b8), \u03d5) =\n\u03b2h \u00b7 H(q, p\u03d5) + \u03b2s \u00b7 D(p\u03c4\n\u03b8, p\u03c4\n\u03d5)\n(3)\nwhere \u03b2h and \u03b2s are weighting hyper-parameter\nvalues of the hard loss H(q, p\u03d5) and soft loss\nD(p\u03c4\n\u03b8, p\u03c4\n\u03d5) for the teacher model, respectively. By\nminimizing KL divergence D(p\u03c4\n\u03b8, p\u03c4\n\u03d5) between\nthe predictions of the student model (p\u03c4\n\u03b8) and the\nteacher model (p\u03c4\n\u03d5), the teacher model learns from\nthe student model. In the reversed KD objective, we\nonly train the teacher model by applying StopG(x)\nto the gradient of the student parameters \u03b8.\nCo-training\nWith the Equations (2) and (3), we\nget the final objective LCTCD(\u03b8, \u03d5) as follows:\n\u03b8\u2217, \u03d5\u2217 = argmin\n\u03b8,\u03d5\nLCTCD(\u03b8, \u03d5) =\nLKD(\u03b8, StopG(\u03d5)) + LReKD(StopG(\u03b8), \u03d5)\n(4)\nAdapting to Downstream Task\nAfter model co-\ntraining/-distillation, the trained smaller (student)\nmodel s\u03b8\u2217 with trained parameter \u03b8\u2217 can be de-\nployed for multiple downstream tasks to improve\ninference efficiency. To fine-tune the model for\na specific downstream task, we adapt the trained\nparameter \u03b8\u2217 using the dataset for that task.\n4\nCommunity KD\nFurthermore, we introduce an advanced CTCD appli-\ncation named Community KD that can utilize CTCD\nframework with the pre-trained teacher model, as\nshown in Figure 3. It consists of a pre-trained\nteacher t\u03d5\u2217 with pre-trained parameters \u03d5\u2217 and\ntwo students s\u03b81 and s\u03b82 parameterized by \u03b81 and\n\u03b82, respectively. During the co-training of two stu-\ndents, each student learns from the hard labels, soft\nlabels generated from the pre-trained teacher pre-\ndictions, and soft labels generated from other stu-\ndent predictions. In other words, we conduct one-\nway knowledge distillation from the pre-trained\nteacher to each student by minimizing KL diver-\ngence between the teacher prediction and predic-\ntions of each student D(p\u03c4\n\u03b81, p\u03c4\n\u03d5\u2217) and D(p\u03c4\n\u03b82, p\u03c4\n\u03d5\u2217)\nand co-distillation between students in both direc-\ntions by minimizing LCTCD(\u03b81, \u03b82). The final\nobjective LCM(\u03b81, \u03b82, StopG(\u03d5\u2217)) is as follows:\n\u03b8\u2217\n1, \u03b8\u2217\n2 = argmin\n\u03b81,\u03b82\nLCM(\u03b81, \u03b82, StopG(\u03d5\u2217)) =\nLCTCD(\u03b81, \u03b82) + D(p\u03c4\n\u03b81, p\u03c4\n\u03d5\u2217) + D(p\u03c4\n\u03b82, p\u03c4\n\u03d5\u2217)\n(5)\nWe select one of the two students distilled by\nCommunity KD (\u03b8\u2217 = \u03b8\u2217\n1 or \u03b8\u2217 = \u03b8\u2217\n2) and fine-\ntune the selected single student s\u03b8\u2217 for downstream\ntasks, resulting that the inference cost does not in-\ncrease compared with the conventional KD method.\n5\nExperiment\nWe present a comprehensive analysis of the pro-\nposed CTCD method through empirical experiments.\nIn Section 5.1, we validate our CTCD method by\ncomparing the performance of small models dis-\ntilled by CTCD to the original large model on the\nGLUE benchmark (Wang et al., 2019). In Sec-\ntion 5.2, we analyze the impact of co-distillation\nby adjusting loss weights for the soft losses of a\nstudent and a teacher. In Section 5.3, we study the\nimpact of training length on CTCD method, allow-\ning us to determine the optimal training length for\nCTCD method. In Section 5.4, we demonstrate the\nefficacy of Community KD by comparing the per-\nformance of a model distilled by the Community\nKD to a model distilled by the one-way KD.\nImplementation details\nWe use a learning rate\nof 5e-4, linear warm-up of 5%, AdamW opti-\nmizer (Loshchilov and Hutter, 2019), and batch\nsize of 128 with A100 GPUs for pre-training. We\ntrain the teacher and student models from scratch\nFigure 4: Average performance on dev sets of GLUE\nbenchmark after training 10 epochs Learning from the stu-\ndent improves the performance of the teacher by an average\nof 1.88. Such improvement in teacher performance leads to\nimprovement in student quality from 77.46 to 77.94.\nfor 20 epochs in Section 5.1. To analyze the ef-\nfectiveness of CTCD method, we train the teacher\nand student models for 10 epochs and 20 epochs\nin Section 5.2 and Section 5.3, respectively. In\nSection 5.4, we train the models for 3 epochs after\nparameter remapping, which is the same as in con-\nventional one-way KD method (Sanh et al., 2019a).\nTraining Time\nFor the one-way distillation, we\nneed 1 GPU day to train the teacher for 10 epochs\nand 1.3 GPU days to distill knowledge from the\npre-trained teacher to the student for another 10\nepochs, which consumes a total of 2.3 days. For\nCTCD, it takes 3 GPU days to train both teacher\nand student models from scratch for 10 epochs.\nWe use the automatic mixed precision (AMP) of\nPyTorch (Paszke et al., 2019) to accelerate training\nfor all our models.\nDataset\nTo validate our CTCD method, we use a\nreduced dataset (30M) created by uniformly sam-\npling 1 out of every 4 sentences from the original\npre-training dataset (BookCorpus (Zhu et al., 2015)\n+ Wikipedia (Foundation)) used in the conventional\none-way KD method (Sanh et al., 2019a). We eval-\nuate our distilled models on the dev sets of the\nGLUE benchmark (Wang et al., 2019), which con-\nsists of nine sentence-level classification tasks. In\nSection 5.4, we use the original pre-training dataset\nto train Community KD method.\nModel Architecture\nWe use a 6-layer BERT (De-\nvlin et al., 2019) model as the teacher and a 4-layer\nBERT model as the student to analyze the effec-\ntiveness and efficiency of our CTCD method. In Sec-\ntion 5.4, we use a pre-trained BERT-base model\nas the teacher and a 6-layer BERT model as the\nstudent.\nPerformance\nGAP w/ Teacher\nOriginal Teacher\n78.06\n-\nStudent\nOne-way Distil. (10 epoch)\n77.46\n-0.60\nCTCD (10 epoch)\n77.94\n-0.12\nOne-way Distil. (20 epoch)\n78.39\n+0.33\nCTCD (20 epoch)\n79.12\n+1.66\nTable 1: Average performance on dev sets of GLUE benchmark The student distilled by CTCD significantly outperforms\nthe original teacher trained using the stand-alone method, achieving a higher gain of 1.66.\n(a) Teacher Performance\n(b) Student Performance\nFigure 5: Adjusting Loss Weight We investigate the impact of the distillation for the teacher model and student model by\nadjusting loss weights (\u03b1h, \u03b1s, \u03b2h, \u03b2s) for hard loss and soft loss. (a) We (co-)train the teacher model distilling knowledge from\nthe student by fixing \u03b1h : \u03b1s = 1 : 1 and varying \u03b2h : \u03b2s on the large text corpus. (b) We (co-)train the student model distilling\nknowledge from the teacher by fixing \u03b2h : \u03b2s = 1 : 1 and varying \u03b1h : \u03b1s on the large text corpus. Then we report the average\nperformance of each pre-trained model after fine-tuning it on downstream tasks (dev sets) of GLUE benchmark.\nFigure 6: Length of Training We pre-trained student models\nunder two different training lengths 10/20 epochs while dis-\ntilling knowledge from teacher models via ours or the conven-\ntional KD method. Then we adapt pre-trained student models\non CoLA task. With enough longer training (20 epoch), the\nstudent model distilled by ours significantly outperforms the\nstudent model distilled by the conventional KD method, with\na higher gain of 5.22.\n5.1\nCould Knowledge Distillation Help\nImprove Performance?\nIn Table 1 and Figure 4, we show the average per-\nformance of models trained using different methods\non a large text corpus and fine-tuned against the\nGLUE benchmark. Stand-alone trains a model\nwithout using any knowledge distillation.\nCo-\ntraining & One-way distillation trains teacher\nand student models together from scratch, with\nknowledge only flowing from the teacher to the\nstudent. Co-training & Co-distillation (Ours) is\nCTCD method, which trains both teacher and student\nmodels together from scratch and distills knowl-\nedge between each other in both directions. For\ndistillation methods, we set the weights of the\nhard losses for the teacher and student to 1. The\nweights of the soft losses are chosen from the set\n{0.5, 1, 2, 4}, and the results are reported with the\nbest-performing weights.\n1) Overall Results\nAs shown in Table 1, the stu-\ndent distilled by CTCD method significantly out-\nperforms the original teacher trained using the\nstand-alone method on the average performance of\nthe GLUE benchmark, achieving a higher gain of\n1.66. Furthermore, the student model distilled by\nour CTCD method outperforms the student distilled\nby the one-way distillation method on the aver-\nage performance of the GLUE benchmark. After\ntraining for 10 and 20 epochs, the student distilled\nby our CTCD method consistently has higher gains\nthan the student distilled by one-way distillation, as\n77.46 vs. 77.94 and 78.39 vs. 79.12, respectively.\n2) Does learning from a small and weak student\nprovide a performance benefit to the teacher?\nAs shown in Figure 4, the distillation of knowledge\nfrom the student model to the teacher model has\nbeen shown to significantly improve the quality of\nthe teacher model, with an average increase of 1.88\ncompared to teacher training methods that do not\nincorporate such distillation process (such as Stand-\nalone and Co-training & one-way distillation).\n3) Is the teacher\u2019s performance improvement\nreflected in the student\u2019s performance improve-\nment?\nFurthermore, we find that a better teacher\nleads to further performance improvement of the\nstudent from 77.46 to 77.94. The results demon-\nstrate that the distillation process successfully im-\nproves the performance of both the teacher and\nstudent. The student trained with our CTCD method\nachieves better performance than students trained\nwith the Stand-alone or Co-training & one-way dis-\ntillation methods, with an average improvement of\n0.88 and 0.47, respectively.\n5.2\nIn-depth Study 1: Adjusting Loss Weights\nIn this Section, we investigate the impact of distilla-\ntion on the performance of both the student and the\nteacher by adjusting loss weights (\u03b1h, \u03b1s, \u03b2h, \u03b2s)\nfor hard loss and soft loss. For example, setting\n\u03b1h : \u03b1s = 1 : 4 emphasizes learning from the\nteacher\u2019s knowledge (i.e., the soft loss for the stu-\ndent) 4\u00d7 more than the ground truth label distri-\nbution (i.e., the hard loss for the student), during\nco-training. This allows us to better understand the\neffect of distillation on each model and optimize\ntheir performance.\nTeacher side\nIn Figure 5(a), we co-train the\nteacher model using distillation to transfer knowl-\nedge from the student model. We fix the weighting\nvalues for the losses of the student to \u03b1h : \u03b1s = 1 :\n1 and vary the weighting values for the losses of\nthe teacher, \u03b2h : \u03b2s, while training on a large text\ncorpus. We then evaluate the average performance\nof the pre-trained teacher models on downstream\ntasks from the dev sets of GLUE benchmark.\nOur results show that co-training the teacher with\ndistillation outperforms training the teacher alone,\nregardless of the weighting values for the soft loss,\nby obtaining higher gains of 0.36, 0.77, 1.80, and\n1.02 for \u03b2h : \u03b2s = 1 : 1, 1 : 2, 1 : 4, and 4 : 4,\nrespectively. Additionally, we find that giving more\nweight to the soft loss of the teacher during training\n(\u03b1h : \u03b1s : \u03b2h : \u03b2s = 1 : 1 : 1 : 1 \u2192 1 : 1 : 1 :\n2 \u2192 1 : 1 : 1 : 4) leads to improved performance,\nwith an average score of 78.42 \u2192 78.83 \u2192 79.86.\nFurthermore, we observe that emphasizing only the\nsoft loss of the teacher (1 : 1 : 1 : 4) yields better\nperformance than emphasizing both the hard and\nsoft losses of the teacher (1 : 1 : 4 : 4), with an\naverage score of 79.86 vs. 79.09.\nStudent side\nWe find that the student model\u2019s\nperformance is not sensitive to the weighting values\nfor the hard and soft losses of the student, (\u03b1h : \u03b1s).\nRegardless of the chosen values, co-training the\nstudent with distillation consistently improves its\nperformance compared to training the student alone.\nFor instance, when we emphasize the soft loss of\nthe student by increasing the weighting value for\n(\u03b1s) as 1 : 1 : 1 : 1 \u2192 1 : 2 : 1 : 1 \u2192 1 : 4 : 1 : 1,\nwe observe similar levels of performance for the\nstudent model.\n5.3\nIn-depth Study 2: Length of Training\nWe studied the impact of co-training length on the\neffectiveness of the CTCD method (see Figure 6).\nWe find that longer training leads to improved per-\nformance, as demonstrated by our experiments us-\ning two different training lengths: 10 epochs and\n20 epochs. After pre-training the student models\nwith these different lengths, we adapted them to\nthe CoLA downstream tasks and evaluated their\nperformance using Matthew Correlation.\nResults\nBy increasing the (co-)training length\nfrom 10 epochs to 20 epochs, CTCD (Ours) signif-\nicantly improves the performance of the student\nmodel from 35.02 to 46.23, with a gain of 11.21.\nThis outperforms the conventional KD method,\nwhich only achieves a gain of 0.07 from 40.94 to\n41.01, despite a longer training time. The conven-\ntional KD method relies on a pre-trained teacher\nmodel to train the student model, which allows\nfor fast convergence but limits the learning of the\nstudent model. In contrast, the CTCD method al-\nlows for additional performance gains for both the\nteacher and student models by enabling them to\nlearn and grow together during co-training. This\ncan provide further benefits to the student model\u2019s\nperformance with longer co-training.\n5.4\nEfficacy of Community KD\nWe compare the proposed Community KD with\nthe conventional one-way KD method (Sanh et al.,\nDownstream Task\nMNLI\nQQP\nQNLI\nSST-2\nCoLA\nSTSB\nMRPC\nRTE\nAverage\nMetric\nAMP\nAcc.\nAcc.\nAcc.\nAcc.\nMatthew.\nPearson.\nSpear.\nF1\nAcc.\nAcc.\nAcc.\nDataset Size\n392.7k\n363.8k\n104.7k\n67.3k\n8.5k\n5.7k\n3.7k\n2.5k\nTeacher\nBERT (109M)\n84.17\n90.89\n90.68\n91.86\n57.54\n88.84\n88.56\n89.31\n85.04\n65.34\n83.23\nStudent\n(67M)\nOne-way KD (Sanh et al., 2019b)\nFP32\n81.93\n90.05\n87.72\n90.94\n52.03\n86.28\n86.07\n87.94\n82.59\n57.76\n80.33\nOurs: Student 1\nFP16\n81.88\n90.17\n88.24\n91.51\n54.82\n86.70\n86.49\n89.76\n85.29\n59.21\n81.40\nOurs: Student 2\nFP16\n81.34\n89.75\n88.37\n90.71\n56.08\n86.42\n86.44\n89.80\n85.29\n59.20\n81.34\nTable 2: Efficacy of Community KD The pre-trained BERT and 6-layer BERT is the teacher model and student architecture,\nrespectively, for both ours and the conventional one-way KD method. We fine-tune the distilled students on dev sets of GLUE\nbenchmark. We observe that learning from the soft knowledge of different student model improves performance over the\nconventional one-way KD method on most downstream tasks.\n2019a). To ensure a fair comparison, we use the\npre-trained BERT-base as the teacher model for\nboth methods and the 6-layer BERT as the stu-\ndent, which is the same architecture used in the\nconventional one-way KD method. As described\nin Section 4, we train two student models concur-\nrently and they learn from the pre-trained BERT,\nthe ground truth labels, and each other\u2019s knowledge.\nNote that since we fine-tune one of the two students\ndistilled by Community KD for downstream tasks,\nthe inference cost is the same as the conventional\none-way KD method. In Table 2, we report the\nresults of BERT and the conventional one-way KD\nmethod using checkpoints provided by Hugging\nFace (HuggingFace) and both students (Ours: Stu-\ndent 1 and Ours: Student 2) on the dev sets of the\nGLUE benchmark. We apply Automatic Mixed\nPrecision (AMP) (Paszke et al., 2019) to Commu-\nnity KD, which typically speeds up training but\nmay hurt performance.\nResults\nThe results presented in Table 2 shows\nthat Community KD, leads to improved perfor-\nmance on downstream tasks such as QQP, QNLI,\nSST-2, CoLA, STSB, MRPC, and RTE, even when\napplying quantization techniques. Specifically, the\naverage performance gain of the student model dis-\ntilled using our Community KD method is 1.04\n(1.2%) higher than that of the student model dis-\ntilled by the conventional one-way KD method.\nThis suggests that incorporating knowledge distil-\nlation from both a student model and a pre-trained\nteacher model is more effective than only using\nknowledge distillation from the pre-trained teacher\nmodel.\n6\nLimitations & Future Work\nLimitations\nThe proposed method co-train mod-\nels from scratch and may require a longer pre-\ntraining time than the conventional KD method.\nHowever, as we described in Section 5.3, when\nthe student model is trained long enough with its\nteacher, it can outperform the models trained with\nthe conventional KD on the downstream task. The\nproposed co-training method may increase the over-\nall training cost compared with one-way distilla-\ntion, and it may become a performance bottleneck\ndepending on training resource constraints. How-\never, note that CTCD can improve model quality\nwhile having the same inference cost as the one-\nway distillation on downstream tasks.\nFuture Work\n1) Architecture Sharing. Models\ncan share some of their architectures by reusing\nthe output of such architectures and updating them\ntogether during back-propagation. This may help\nreduce the additional computing and memory over-\nhead incurred by model co-training, while improv-\ning the model quality, especially for the student\nmodel. 2) Integration of Student Architecture\nDesign and Data Augmentation. Future research\ncan focus on effectively combining the CTCD frame-\nwork with student architecture design and data aug-\nmentation techniques. This integration provides a\npromising alternative to traditional one-way knowl-\nedge distillation methods, leading to significant\nimprovements in language modeling tasks.\n7\nConclusion\nThe size and complexity of pre-trained language\nmodels (PLMs) can hinder their practicality for\nonline downstream tasks. To address this, we in-\ntroduced a novel framework called co-training and\nco-distillation (CTCD). By training models of dif-\nferent sizes together and extracting inter-model\nknowledge in both directions, the proposed CTCD\nframework improves both model efficiency and per-\nformance. The proposed framework overcomes the\ntrade-off between efficiency and performance in\ntraditional one-way knowledge distillation meth-\nods. Notably, our compressed models achieved\nan impressive gain of 1.66 on the GLUE bench-\nmark, outperforming large models trained using\nstandalone methods.\nReferences\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978\u20132988, Florence, Italy. Asso-\nciation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171\u20134186.\nWikimedia Foundation. Wikimedia downloads.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nHuggingFace. Huggingface.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinyBERT: Distilling BERT for natural language un-\nderstanding. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 4163\u2013\n4174, Online. Association for Computational Lin-\nguistics.\nHayeon Lee, Rui Hou, Jongpil Kim, Davis Liang,\nSung Ju Hwang, and Alexander Min. 2023. A study\non knowledge distillation from weak teacher for scal-\ning up pre-trained language models. arXiv preprint\narXiv:2305.18239.\nChunyuan Li, Xiang Gao, Yuan Li, Xiujun Li, Baolin\nPeng, Yizhe Zhang, and Jianfeng Gao. 2020. Opti-\nmus: Organizing sentences via pre-trained modeling\nof a latent space. In EMNLP.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019. OpenRe-\nview.net.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Process-\ning Systems 32, pages 8024\u20138035. Curran Associates,\nInc.\nYujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han,\nZhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2022. Knowledge\ninheritance for pre-trained language models. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n3921\u20133937, Seattle, United States. Association for\nComputational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-\nhou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio. 2015. Fitnets: Hints for thin deep nets. In In\nProceedings of ICLR.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019a. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019b. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019.\nMegatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4323\u20134332, Hong Kong, China. Association for Com-\nputational Linguistics.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspecific knowledge from bert into simple neural net-\nworks. arXiv preprint arXiv:1903.12136.\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-\nvazhagan, Xin Li, and Amelia Archer. 2019. Small\nand practical BERT models for sequence labeling. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3632\u2013\n3636, Hong Kong, China. Association for Computa-\ntional Linguistics.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nThe impact of student initialization on knowledge\ndistillation. arXiv preprint arXiv:1908.08962, 13.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Interna-\ntional Conference on Learning Representations.\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\nand Furu Wei. 2020a.\nMinilmv2:\nMulti-head\nself-attention relation distillation for compress-\ning pretrained transformers.\narXiv preprint\narXiv:2012.15828.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020b. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. Advances in Neural In-\nformation Processing Systems, 33:5776\u20135788.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran Asso-\nciates, Inc.\nLi Yuan, Francis EH Tay, Guilin Li, Tao Wang, and\nJiashi Feng. 2020. Revisiting knowledge distillation\nvia label smoothing regularization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3903\u20133911.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE in-\nternational conference on computer vision, pages\n19\u201327.\n"
  },
  {
    "title": "Tailoring Self-Rationalizers with Multi-Reward Distillation",
    "link": "https://arxiv.org/pdf/2311.02805.pdf",
    "upvote": "2",
    "text": "TAILORING\nSELF-RATIONALIZERS\nWITH\nMULTI-\nREWARD DISTILLATION\nSahana Ramnath\u2665, Brihi Joshi\u2665, Skyler Hallinan\u2663, Ximing Lu\u2663, Liunian Harold Li\u2660\nAaron Chan\u2665, Jack Hessel\u2662, Yejin Choi\u2663\u2662, Xiang Ren\u2665\u2662\n\u2665University of Southern California \u2663University of Washington\n\u2660University of California Los Angeles \u2662Allen Institute for Artificial Intelligence\nsramnath@usc.edu\nABSTRACT\nLarge language models (LMs) are capable of generating free-text rationales\nto aid question answering.\nHowever, prior work 1) suggests that useful self-\nrationalization is emergent only at significant scales (e.g., 175B parameter GPT-\n3); and 2) focuses largely on downstream performance, ignoring the semantics of\nthe rationales themselves, e.g., are they faithful, true, and helpful for humans? In\nthis work, we enable small-scale LMs (\u223c200x smaller than GPT-3) to generate\nrationales that not only improve downstream task performance, but are also more\nplausible, consistent, and diverse, assessed both by automatic and human evalu-\nation. Our method, MARIO (Multi-rewArd RatIOnalization), is a multi-reward\nconditioned self-rationalization algorithm that optimizes multiple distinct proper-\nties like plausibility, diversity and consistency. Results on five difficult question-\nanswering datasets StrategyQA, QuaRel, OpenBookQA, NumerSense and QASC\nshow that not only does MARIO improve task accuracy, but it also improves the\nself-rationalization quality of small LMs across the aforementioned axes better\nthan a supervised fine-tuning (SFT) baseline. Extensive human evaluations con-\nfirm that MARIO rationales are preferred vs. SFT rationales, as well as qualitative\nimprovements in plausibility and consistency1.\n1\nINTRODUCTION\nQuestion: Malini is cutting bread with a bread \nknife which creates a smooth cut, while cutting cake \nwith a bread knife creates a rough cut. This means \nthat the ___ has less resistance (A) bread (B) cake\nAnswer: \n(A)\nGPT-3 Rationale: When \nsomething has less resistance, it is \neasier to cut through. Thus, the \nbread has less resistance, making it \neasier to cut with the bread knife.\nExisting \nRationalizers\nWhat we do\n(1) Large Size\n(2) Focus only on task \nperformance\nGPT-3\nMaRio Rationale: Less \nresistance implies ease in \ncutting through. So, it is easier \nto cut through bread.\n\ufffd\ufffd\n  SFT Rationale: Less resistance \nimplies ease in cutting through. So \nthe bread has a smooth cut as it is \nmore resistant.\n\ud83d\udc4e \n+ Plausibility \u2b06\n+Consistency \u2b06\n+Diversity \u2b06\nGPT-3\nRationale \nProperties \ud83c\udfc6\nFigure 1: Our proposed approach, MARIO. While existing self-rationalizing pipelines require\nexorbitantly large LMs that are used to primarily improve task performance, MARIO is a small LM\nthat is initially distilled from rationales generated by GPT-3, following by multi-reward training that\nimproves its rationale quality w.r.t three properties: plausibility, diversity and consistency. In the\nfigure above, MaRio rationale improves over the SFT rationale by improving over these properties,\nwhich is highlighted in red.\nIn recent years, there has been a surge of interest in using LMs to generate fluent, human-like, free-\ntext rationales that can explain their decision, called self-rationalizing LMs (Wiegreffe et al., 2021).\n1https://inklab.usc.edu/MaRio/\n1\narXiv:2311.02805v1  [cs.CL]  6 Nov 2023\nEarly approaches in self-rationalizing involved collecting human-written gold rationales and using\nthem as supervision for training LMs (Wiegreffe et al., 2021; Narang et al., 2020). Recently, with the\nadvent of large LMs, chain-of-thought prompting (Wei et al., 2022; Marasovic et al., 2022; Kojima\net al., 2022) has revolutionized the landscape of self-rationalization; now, with just a few well-\ndesigned prompts and demonstrations, LMs can generate explanations for their predictions. The\npresence of rationales can make LMs both more interpretable (Lertvittayakumjorn & Toni, 2021)\nand more usable (Joshi et al., 2023) from the perspective of users.\nHowever, prior work in self-rationalization has largely overlooked the quality of generated rationales\nthemselves; instead, their utility is justified by measuring downstream task performance (Wei et al.,\n2022; Zelikman et al., 2022). This is particularly problematic, as downstream models and users\nmay use these rationales as justifications for the predicted answer, which can further propagate these\nnegative quality outcomes (Atanasova et al., 2023; Joshi et al., 2023; Hovy & Prabhumoye, 2021).\nFurthermore, it is observed that rationalization comparable to human quality is only observed with at\na significant LM parameter scales (\u223c100B or more) (Wei et al., 2022). Despite some recent interest\nin using smaller LMs for rationalization (Chen et al., 2023b), it is still unclear if smaller LMs can\nbe used to generate similarly high-quality rationales.\nIn this work, we propose MARIO, a method that focuses on tailoring small-sized LMs (< 1B param-\neters) to be strong rationalizers both in terms of improved downstream performance, and in terms\nof desirable properties of the rationales themselves. Instead of relying on human rationale labelling\n(Wiegreffe et al., 2021), MARIO considers a setting where a small LM only has access to rewards\nthat measures factors underlying rationale quality, e.g. a trained LM that judges the plausibility of\na rationale and provides a numerical score. MARIO first starts with training a small LM to self-\nrationalize, with the help of GPT-3 (Brown et al., 2020) (TEXT-DAVINCI-003) generated rationales\nas initial supervision, which are shown to be of higher quality Sun et al. (2022). It then casts the\nproblem into a multi-reward conditioned rationale generation problem, where the LM is optimized\nto maximize quality rewards. In order to achieve this, MARIO extends QUARK proposed by Lu\net al. (2022) to a multi-reward setup, where generations from an LM are binned according reward\nvalues; the LM learns distributions conditioned on \u2018control-tokens\u2019 corresponding to every reward\nand high-quality generations can be obtained via conditioning on the highest-reward token.\nWe determine that high-quality rationales should have three necessary properties: plausibility\n(makes logical sense), diversity (is not repetitive) and consistency (supports the correct answer for\nthe instance). Generated rationales\u2019 rewards are assessed through automated metrics for each of the\nthree quality properties. We then evaluate MARIO on five question-answering datasets, and observe\nthat small LMs like T5-LARGE can be effectively trained to generate rationales that satisfy all of\nthe quality requirements, while also leading to improvements in task performance over supervised\nfine-tuned self-rationalizers (SFT). Via human evaluation, we also observe that rationales generated\nby MARIO are more preferred over those generated by SFT, across all datasets.\nWe note that tailoring small LMs with multiple quality rewards is a challenging task. Some of these\nissues include finding high-quality, stable rewards that can be effectively incorporated in a self-\nrationalizing pipeline. We also observed that a lot of additional desirable properties in rationales\n(like factuality and completeness) do not have reliable automated rewards. Furthermore, improving\ntask accuracy (which is the primary goal while generating rationales for a lot of these tasks) is\nchallenging in a multi-reward setup, and show that adding task accuracy as an additional reward term\nleads to the best configuration of MARIO. By using small LMs to generate high-quality rationales\nthat are also supported by human evaluations, we believe our findings can help guide future work in\nefficient, real-world situated methods in rationale generation and evaluation.\n2\nSELF-RATIONALIZATION\nThroughout this work, we refer to self-rationalizers as LMs that are trained or prompted to specif-\nically generate free-text rationales, along with their predictions.\nThese free-text rationales are\ntreated as explanations for their predictions. For the purpose of our experiments, we explore self-\nrationalization on the question answering (QA) task. Specifically, given a question, the LM must\nfirst generate a free-text rationale that explains the LM\u2019s reasoning process, followed by an answer\nto the given question. Table 1 shows examples of inputs and outputs by these self-rationalizing LMs\nfor five QA datasets: STRATEGYQA (Geva et al., 2021), QUAREL (Tafjord et al., 2019), OPEN-\n2\nTable 1: Sample Inputs and Outputs for Self-Rationalizing LMs. We use an I-RO setting for\nall our experiments. This table shows one example each from the training set of STRATEGYQA,\nOPENBOOKQA, QUAREL, NUMERSENSE and QASC. The rationales shown here are the ones\nsampled from GPT-3.\nSTRATEGYQA\nINPUT (I): Could someone in Tokyo take a taxi to the The Metropolitan Museum of\nArt?\nOUTPUT (RO): The Metropolitan Museum of Art is in New York City, USA. Tokyo,\nJapan is over 6,000 miles away. So the answer is no.\nOPENBOOKQA\nINPUT (I): Our only star provides us with energy that is (a) temporary (b) inferior (c)\nexpensive (d) reusable\nOUTPUT (RO): The energy from the sun is renewable and reusable. So the answer is\n(d).\nQUAREL\nINPUT (I): Cutting bread with a bread knife creates a smooth cut, while cutting cake\nwith a bread knife creates a rough cut. This means that the\nhas less resistance (A)\nbread (B) cake\nOUTPUT (RO): When something has less resistance, it is easier to cut through. Thus,\nthe bread has less resistance, making it easier to cut with the bread knife. So the answer\nis (A).\nNUMERSENSE\nINPUT (I): Fungi reproduce in <mask> ways. (A) no (B) zero (C) one (D) two (E)\nthree (F) four (G) five (H) six (I) seven (J) eight (K) nine (L) ten\nOUTPUT (RO): Fungi reproduce by sexual or asexual reproduction. So the answer is\n(D).\nQASC\nINPUT (I): Bees are necessary to (A) haploid plants (B) genetic diversity (C) spread\nflower seeds (D) prevent evolution (E) Reproduction (F) eat weeds (G) chew flowers\n(H) important habitats\nOUTPUT (RO): Bees are necessary to spread flower seeds. Bees pollinate flowers,\nwhich helps the flowers reproduce and spread their seeds. So the answer is (C).\nBOOKQA (Mihaylov et al., 2018), NUMERSENSE Lin et al. (2020) and QASC Khot et al. (2020).\nThese datasets were chosen over others which have existing human written rationales because all of\nthem require certain level of implicit or logical reasoning in order to arrive at the answer. As we\ndepict in the examples, we follow the I-RO format (Wiegreffe et al., 2021), wherein the input to the\nLM is the question, and the output is the joint generation of the rationale and the predicted answer.\nIn order to determine whether these generated rationales are of good quality, we focus on three\nproperties that are necessary for any rationale to have, agnostic of the task it is meant for. First,\nwe note that a rationale should be plausible. We define plausibility as the rationale making sense\non its own \u2013 whether it be common, logical or factual sense depending on the dataset at hand. For\nexample, if a rationale states \u2018Cows can fly\u2019, it is not plausible. Next, we identify that a rationale\nshould be diverse, where the rationale is clean and not repetitive. Lastly, we note that a rationale\nshould be consistent with the gold label for the input. Consistency is important to ensure that a\nrationale does not spew irrelevant information, and that it supports the gold answer. Furthermore,\nwe focus on consistency with respect to the gold label, as misleading rationales are unhelpful as\nboth LM justifications, and for human utility (Joshi et al., 2023). We formalise these properties\nas rewards in \u00a74 and while these are necessary properties for any rationale, we also discuss other\ngood-to-have properties in \u00a75.\nAll of these properties are agnostic of the actual prediction made by the LM. Since our self-\nrationalization setup generates a rationale first, followed by its prediction, we aim to generate ratio-\nnales with good quality, which should ideally improve the answer generated by the LM. Therefore,\nwe focus on improving self-rationalization along these three properties, as well as on task accuracy.\nAlong with the above rationale properties, we also consider task correctness as a necessary property\nof rationales, that they should try to improve over as a byproduct.\n3\nQuestion: Malini is cutting bread with a bread \nknife which creates a smooth cut, while cutting \ncake with a bread knife creates a rough cut. \nThis means that the ___ has less resistance (A) \nbread (B) cake\nRationale: When something \nhas less resistance, it is easier to \ncut through. Thus, the bread \nhas less resistance, making it \neasier to cut with the bread \nknife.\nGPT-3\nLess resistance implies ease \nin cutting through. So the \nbread has a smooth cut as it \nis less resistant.\nReward-based \nupdating\nDistillation\nSo the answer \nis (A).\n[P] [D] [C]\nPrefixed \nReward \nTokens\nRationale\nTask \nOutput\n+\nSingle Reward\n[Plausibility]\nMulti Reward - Canonical\nMulti Reward - Additive\n[Diversity]\n[Consistency]\n+\n[Plausibility]\n[Plausibility]\n[Diversity]\n[Plausibility]\n[Diversity]\n[Consistency]\n[Plausibility]\n\ufffd\ufffd\nFigure 2: MARIO pipeline. MARIO uses rationales generated by a larger LM like GPT-3 as\ninitial supervision, and uses rewards corresponding to three rationale properties: PLAUSIBILITY,\nDIVERSITY and CONSISTENCY, to improve self-rationalization of smaller LMs like T5-LARGE.\n3\nMARIO: OPTIMIZING FOR MULTIPLE REWARDS\nTo improve an LMs\u2019 rationalization across multiple properties, we leverage QUARK(Lu et al., 2022),\na reinforcement learning-like framework effective on tasks such as unlearning toxicity in genera-\ntions. We propose Multi-rewArd RatIOnalization (MARIO), an extension of QUARK to multiple\nrewards concurrently. We further propose two variants of MARIO: CANONICAL and ADDITIVE,\nthat explore different ways of using QUARK for in a multi-reward setup. Figure 2 shows a running\nexample of MARIO. Appendix B shows a second, more technical illustration of the same.\n3.1\nQUARK\nQUARK(Lu et al., 2022) is a reinforcement learning-like algorithm that trains LMs using unique\ncontrol tokens prepended to the generated text. The QUARK algorithm works iteratively: (1) sam-\npling the a pre-trained LM to generate more training data, (2) scoring the generated data using a\nchosen reward metric, and (3) using instance-level scores to sort and bin the data into a fixed num-\nber of bins, each of which correspond to a unique control token. During training, the LM learns\nto associate each control token with the quality (as determined by the reward metric) of the data it\nis assigned to. During inference, in order to obtain the best quality generations, QUARK samples\nthe LM using the control token corresponding to the highest reward measure. We provide a more\ndetailed description of how QUARK works in Appendix B.\n3.2\nCANONICAL MARIO\nSince QUARK is designed only for single reward optimizations, what if we are interested in improv-\ning the rationales along multiple rewards? In order to do this, we first propose a direct extension\nto the QUARK algorithm: instead of assigning each instance just one unique control token (which\ncorresponds to one specific property), we now assign each instance multiple unique control tokens\nat once; now, each control token corresponds to a different property we want to train the LM on. We\ncall this the CANONICAL MARIO. The order in which properties are represented in this method is\na design decision that we can choose, and we expand on this further in Appendix C.\n3.3\nADDITIVE MARIO\nWe now propose a step-by-step multi-reward QUARK: instead of training the LM on all the proper-\nties at the same time, we introduce them into training pipeline additively, in a predefined order of\nproperties. For example, say that we have properties P1, P2, P3, and that we want the LM to focus\non P3 first, before moving on to P1 and then later P2. In this method, we use multiple sets of control\n4\ntokens as in CANONICAL MARIO, but, we introduce each set of tokens into the training pipeline\nsuccessively. For example, we train the LM with only P3 for the first t steps, then we train the LM\non P3 and P1 from the t-th step to the 2t-th step, and then from the 2t-th step onwards, we train the\nLM on P3, P1 and P2. We call this method the ADDITIVE MARIO. Again, the order in which we\nadd the control tokens of different rewards to the training pipeline, and whether each new control\ntoken is added to the left or right of the existing control tokens are decision choices, and we expand\non it further in Appendix C.\n4\nEXPERIMENTS\n4.1\nDATASETS\nAs we mention in Section 2, we conduct experiments on five datasets: STRATEGYQA (Geva et al.,\n2021), QUAREL (Tafjord et al., 2019), OPENBOOKQA (Mihaylov et al., 2018), NUMERSENSE Lin\net al. (2020) and QASC Khot et al. (2020). All datasets have multi-choice questions (yes/no for\nSTRATEGYQA, a/b for QUAREL, a/b/c/d for OPENBOOKQA, a/b/-/l for NUMERSENSE, a/b/-/h for\nQASC), and the task is to generate a rationale followed by the predicted answer.\nWe report details of train, val and test splits in Appendix D. For all the datasets, we do not re-\nquire human-written gold rationales, and thus, we sample GPT-3 (TEXT-DAVINCI-003) Brown et al.\n(2020) for silver rationales for supervision. We use chain-of-thought prompts from prior works on\nthese datasets (refer Appendix H for the full prompts) and sample 5 rationale generations for each\ntraining set instance with a temperature of 0.92; however, for supervision we use only the instances\nwhere the answer predicted by GPT-3 is correct.\n4.2\nRATIONALE PROPERTY REWARDS AND TASK CORRECTNESS\nAs described in \u00a72, we determine three rationale quality properties that are used in MARIO. Here,\nwe formalize these properties with their implementations that we use as rewards within MARIO. We\nalso show demonstrative examples for each property in Table 2.\nLet Q be the input question, \u02c6R represent the LM\u2019s generated rationale, and O and \u02c6O represent the\ngold answer and the LM\u2019s predicted answer respectively. Below are the formulations of each reward:\n\u2022 PLAUSIBILITY via VERA (Liu et al., 2023): VERA is a trained commonsense statement\nverification T5-11B model, that provides a numerical score between 0 and 1, indicating\nthe plausibility of declarative sentences.\nPLAUSIBILITY( \u02c6R) = VERA( \u02c6R)\n(1)\nWe use the VERA release from HuggingFace 3.\n\u2022 CONSISTENCY via (Wiegreffe et al., 2021): Wiegreffe et al. (2021) provide a framework\nto evaluate the association between a rationale and a label with the help of two reference\nLMs that are trained to predict the answer with (MQR) and without (MQ) the rationale in\nthe input. More formally,\nCONSISTENCY( \u02c6R) = PMQR(O\u2223Q, \u02c6R) \u2212 PMQ(O\u2223Q)\n(2)\nCONSISTENCY is a numerical score between \u22121 and 1 that indicates the faithfulness of\nthe rationale towards the gold answer, like the implementation by Wiegreffe et al. (2021).\nHyperparameters and training guidelines for the LMs involved in generating the CONSIS-\nTENCY score is in Appendix E.\n\u2022 DIVERSITY via n-gram uniqueness in Li et al. (2022): Li et al. (2022) calculate diversity\nof generated text by determining the fraction of unique n-grams generated. DIVERSITY\nis a numerical score between 0 and 1 that indicates the lexical diversity of the rationale;\nthis metric also serves the purpose of ensuring that the rationale does not contain repeated\nphrases or sentences.\nDIVERSITY( \u02c6R) =\n4\n\u220f\nn=2\nunique n-grams( \u02c6R)\ntotal n-grams( \u02c6R)\n(3)\n2for NUMERSENSE we sampled just 1 rationale, since the generated rationales were deterministic\n3https://huggingface.co/liujch1998/vera\n5\n\u2022 TASK-CORRECTNESS: As our last reward, we add task correctness of the answer that is\ngenerated following the rationale. This is a binary 0/1 score, referring to the wrong and\nright predicted answer respectively.\nTASK-CORRECTNESS( \u02c6R) = 1[ \u02c6O = O]\n(4)\nWe evaluate and report the above four metrics for all our baselines and experiments. To simplify\ncomparisons, we also report an average normalized relative gain (NRG) (Chan et al., 2022) (Ap-\npendix E has a more detailed definition).\nTable 2: Demonstrative examples for the rationale properties of PLAUSIBILITY, CONSISTENCY\nand DIVERSITY.\nQUESTION\nMalini is cutting bread with a bread knife which creates a smooth cut, while cutting\ncake with a bread knife creates a rough cut. This means that the\nhas less resistance\n(A) bread (B) cake\nPLAUSIBILITY\n\u00f8 Less resistance implies that the item would be difficult to cut through. Therefore,\ncake has less resistance.\n\u00fb Less resistance implies ease in cutting through. So the bread has a smooth cut as it\nis less resistant.\nCONSISTENCY\n\u00f8 Less resistance implies ease in cutting through. So the cake has a smooth cut as it is\nless resistant.\n\u00fb Less resistance implies ease in cutting through. So the bread has a smooth cut as it\nis less resistant.\nDIVERSITY\n\u00f8 Less resistance implies ease in ease in ease in cutting through. Ease in cutting\nthrough. Answer is bread.\n\u00fb Less resistance implies ease in cutting through. So the bread has a smooth cut as it\nis less resistant.\n4.3\nHUMAN PREFERENCE EVALUATION\nWe first present human preference studies comparing rationales generated by MARIO and the su-\npervised fine-tuned baseline SFT for all five datasets.\nFor each instance, we ask three distinct annotators from a pool of qualified annotators to compare the\ntwo rationales across three settings, for a given question and correct answer pair: PLAUSIBILITY and\nCONSISTENCY, which are defined in the same manner as the rewards, and an overall PREFERENCE\nrating. PREFERENCE is meant to indicate that the annotators pick the rationale that they would find\nacceptable (Wiegreffe et al., 2022) for the given question. In Figure 3, we plot the % of instances\nwhere majority of annotators prefer only MARIO\u2019s rationales, only SFT\u2019s rationales, both or none.\nWe note human annotators prefer MARIO\u2019s only rationales for 83.15%, 75.3%, 71.49%, 67.44%\nand 66.6% of instances respectively for STRATEGYQA, QUAREL OPENBOOKQA, NUMERSENSE\nand QASC. Human annotators also find MARIO\u2019s rationales to be considerably more plausible and\nconsistent than SFT4. We use Amazon MTurk5 for all our human studies, and Appendix I provides\nfurther details on the same.\n4.4\nBASELINES VS. MARIO\nWe now empirically compare MARIO with SFT and three additional strong baseline models in terms\nof PLAUSIBILITY, CONSISTENCY, DIVERSITY and TASK ACCURACY (and their average NRG) .\nAll our baselines and MARIO are built on top of T5-LARGE LMs (0.7B).\n1. Supervised Fine-tuned Self-Rationalizer (SFT): A fine-tuned T5-LARGE, which serves as\nthe supervised learning baseline (we use the training data as described in \u00a74.1), trained to\ngenerate rationales and answers.\n4We do not perform human studies for DIVERSITY and TASK ACCURACY since they are auto-\nmatic/straightforward metrics\n5https://www.mturk.com/\n6\nFigure 3: Results of human studies comparing MARIO with SFT. Here, we plot the % of in-\nstances in the test set wherein annotators prefer MARIO, SFT, both or none, with respect to PREF-\nERENCE, PLAUSIBILITY and CONSISTENCY. We find that human annotators vastly prefer MARIO\u2019s\nrationales, and also find them to be much more plausible and consistent.\n2. Product of Rewards (PRODUCT): A multi-reward baseline where we consolidate the re-\nwards into a single representative metric by taking their product and apply QUARK. Ag-\ngregating several rewards into one is common in prior work and is often done through via\nproduct (Lu et al., 2023) or weighted average (Wu et al., 2023).\n3. Filtering rationales that lead to correct answers (FILT-ACC): This is a variant of STAR (Ze-\nlikman et al., 2022). We iteratively train and sample new training data from a T5-LARGE,\nsimilar to QUARK, but instead of using any control tokens, we filter out the instances which\nhave the wrong predicted label. We train this model with only cross-entropy loss.\n4. Multi-reward variant of FILT-ACC (FILT-ALL): Again, we iteratively train and sample\nnew training data from a T5-LARGE, and instead of using control tokens, we filter out the\ninstances which have the wrong predicted label and instances that fall under a specified\nthreshold value for PLAUSIBILITY, DIVERSITY and CONSISTENCY. The threshold value\nis tuned as a hyperparameter. We train this model with only cross-entropy loss.\nTable 3 shows the comparisons between MARIO and the baselines. For all five datasets, we note\nthat MARIO is the overall best setup as noted by both the individual metrics and the averaged\nNRG metric. ADDITIVE MARIO is found to be the best performing method for STRATEGYQA,\nand CANONICAL MARIO is found to be the best method for the other 4 datasets. We detail the\nhyperparameter configurations for all these models in Appendix E. It is important to note that not\nonly does the rationale get better (as seen via the rationale metrics), but the task accuracy also shows\na marked improvement over the baselines. We show some representative examples of rationales\ngenerated by training with MARIO, in comparison with those generated by SFT in Table 4. We also\nrelease the rationales generated by SFT and MARIO.6\n4.5\nREFERENCE LARGE LMS VS. MARIO\nWe now consider three strong reference Large LMs that are used in practice for self-rationalization\n(Wei et al., 2022): GPT-3 (Brown et al., 2020) (TEXT-DAVINCI-003, 175B), FLAN-T5 (Chung\net al., 2022) (in three sizes, L, XL, XXL) and LLAMA (Touvron et al., 2023) (in two sizes 7B,\n65B); we compare MARIO with them in terms of both average NRG, with respect to the size of\nthe underlying LMs (Figure 4) and individual metric scores (Table 10 in the appendix). All these\nreference LMs apart from FLAN-T5-L are orders of magnitude larger than our T5-LARGE LM\ntrained with MARIO; we include FLAN-T5-L in our comparison even though it\u2019s of the same size as\nMARIO because FLAN-T5-L is instruction-finetuned, and few-shot prompted to generate rationales,\nwith the same set of demonstrations used by other large LMs (shown in Appendix H).\nIdeally, we want a small-sized LM (for efficiency purposes) that achieves high performance, which\ncorresponds to the top-left portion of the graph in Figure 4. As such, to compare two LMs\u2019 per-\n6https://drive.google.com/drive/folders/1bWBxdiwce8US5y_\nG6d9-Eki7ObllpR80?usp=sharing\n7\nTable 3: Baselines vs. MARIO Results. For each dataset, the best averaged NRG (across TASK\nACCURACY, PLAUSIBILITY, DIVERSITY and CONSISTENCY) is highlighted in bold, and each\nbest individual metric is underlined. Cells marked with a * shows significant improvement for the\ncorresponding MARIO configuration over SFT (p < 0.05).\nMethod \u2192\nBaselines\nMARIO\nDataset \u2193\nMetric\nSFT\nPRODUCT\nFILT-ACC\nFILT-ALL\nCANONICAL\nADDITIVE\nSTRATEGYQA\nAcc.\n57.64\n62.01\n61.57\n61.35\n60.26\n65.07\nPlau.\n0.33\n0.35\n0.34\n0.36\n0.38\n0.39\u2217\nDiv.\n0.95\n0.92\n0.92\n0.94\n0.95\n0.97\u2217\nCons.\n-0.02\n0.00\n0.00\n0.00\n0.01\n0.04\u2217\nAvg. NRG\n58.66\n59.75\n59.39\n60.34\n60.94\n63.27\nQUAREL\nAcc.\n76.99\n79.53\n79.53\n76.45\n79.89\n78.99\nPlau.\n0.71\n0.72\n0.71\n0.73\n0.77\u2217\n0.75\nDiv.\n0.95\n0.95\n0.95\n0.95\n0.97\u2217\n0.97\nCons.\n0.18\n0.21\n0.20\n0.17\n0.19\n0.20\nAvg. NRG\n75.50\n76.71\n76.38\n75.74\n78.35\n77.75\nOPENBOOKQA\nAcc.\n63.65\n61.65\n65.86\n56.63\n66.06\n65.55\nPlau.\n0.53\n0.52\n0.55\n0.47\n0.55\n0.55\nDiv.\n0.98\n0.99\n0.99\n0.99\n0.99\u2217\n0.98\nCons.\n0.05\n0.07\n0.08\n0.01\n0.09\u2217\n0.09\nAvg. NRG\n66.79\n66.54\n68.47\n63.28\n68.64\n68.29\nNUMERSENSE\nAcc.\n46.23\n50.75\n51.76\n46.73\n55.28\n54.27\nPlau.\n0.60\n0.60\n0.61\n0.58\n0.63\u2217\n0.63\nDiv.\n1.00\n1.00\n1.00\n1.00\n1.00\n0.99\nCons.\n0.17\n0.20\n0.21\n0.16\n0.23\u2217\n0.23\nAvg. NRG\n66.18\n67.69\n68.32\n65.68\n69.95\n69.44\nQASC\nAcc.\n58.64\n57.88\n57.78\n57.02\n60.15\n59.61\nPlau.\n0.44\n0.43\n0.39\n0.42\n0.47\u2217\n0.47\nDiv.\n0.96\n0.95\n0.96\n0.96\n0.99\u2217\n0.99\nCons.\n0.19\n0.17\n0.17\n0.17\n0.19\n0.19\nAvg. NRG\n64.54\n63.60\n62.82\n63.38\n66.41\n66.28\nformance, the LM which is relatively to the left and to the top is practically a better choice. We\nnote that for QUAREL, MARIO results in an LM that is of a very small size (0.7B) but has a very\nhigh performance, almost equivalent to that of GPT-3. For NUMERSENSE, MARIO beats all mod-\nels except for FLAN-T5-XXL and GPT-3, and for QASC, MARIO beats all models except for\nFLAN-T5-XXL, LLAMA-65B and GPT-3. For OPENBOOKQA , we see that MARIO beats LMs\nsuch as FLAN-T5-L, FLAN-T5-XL and LLAMA-7B. For STRATEGYQA we see that our LM beats\nFLAN-T5-L, while performing only a little worse than FLAN-T5-XL.\nFigure 4: Reference Large LMs vs. MARIO Results: Here, we show the comparison of Avg.\nNRG values w.r.t the LM size (in the order of billion parameters) for all the datasets.\n8\nTable 4: Examples of Rationales: Here, we show rationales generated by MARIO and SFT for\nsome examples for the three datasets. We observe that sometimes, SFT generates longer rationales\nwhich are not consistent, or logically correct, which is fixed by MARIO.\nDataset\nExamples\nSTRATEGYQA\nQuestion: While on a liquid diet, are there some types of soup you cannot eat? Yes or No?\nMARIO Rationale: While on a liquid diet, there are some types of soup you cannot eat, such as cream of mushroom soup,\nwhich is a type of savory soup.\nSFT Rationale: While on a liquid diet, there are certain types of soups that are not suitable for a liquid diet such as a liquid diet.\nQuestion: Is a beard is moss that grows on a human? Yes or No?\nMARIO Rationale: A beard is a type of hair that grows on a human.\nSFT Rationale: A beard is a type of facial hair that grows from a root and is typically wavy to dark in color. It is not moss, but\na plant that provides shelter to the moss and helps it grow.\nQUAREL\nQuestion: A bus and a bicycle are both going the same direction. The bus is going 40mph faster than the bicycle. Will the bicycle\nin one hour cover (A) more distance (B) less distance\nMARIO Rationale: The bus is going faster than the bicycle. Thus, the bicycle will cover less distance in one hour.\nSFT Rationale: When something is going faster, it covers more distance. Thus, the bicycle will cover more distance in one hour\nbecause it is going faster than the bus.\nQuestion: Jim and Joe are riding bikes. Jim stops after fifteen minutes and takes a break. Joe keeps on riding. Who went farther?\n(A) Jim (B) Joe\nMARIO Rationale: When someone is riding for a longer period of time, they will go farther. Jim stopped after fifteen minutes\nand took a break, while Joe kept on riding. Thus, Joe went farther.\nSFT Rationale: When someone is moving, they will go farther than someone who is not. Thus, Joe went farther because he\nkept on riding.\nOPENBOOKQA\nQuestion: In order for crops to grow food safely, pesticides are used on them. When it floods, this causes t he what to be poisonous?\n(a) air (b) Corn (c) Runoff (d) farmers\nMARIO Rationale: When it floods, runoff from the crops is sprayed with pesticides, making them poisonous.\nSFT Rationale: When it floods, pesticides are sprayed onto crops, which can make them poisonous.\nQuestion: Plant growth may cause (a) an uptick in the number of leaves (b) a surge in leaf disease (c) a gradual decrease in leaves\n(d) a rapid decline of the leaves\nMARIO Rationale: Plant growth may cause an uptick in the number of leaves.\nSFT Rationale: Plant growth is a process of adding new leaves to the plant. This process can cause a gradual decrease in\nthe number of leaves.\nNUMERSENSE\nQuestion: People have no more than <mask> legs. (A) no (B) zero (C) one (D) two (E) three (F) four (G) five\n(H) six (I) seven (J) eight (K) nine (L) ten\nMARIO Rationale: People have two legs.\nSFT Rationale: People have four legs.\nQuestion: One in <mask> also means a quarter. (A) no (B) zero (C) one (D) two (E) three (F) four (G) five\n(H) six (I) seven (J) eight (K) nine (L) ten\nMARIO Rationale: One in four is a quarter.\nSFT Rationale: One in a quarter.\nQASC\nQuestion: What measures a meter? (A) meter stick (B) football field (C) tetraceratops (D) visibility (E) weather\n(F) U.S. customary units (G) horses (H) lamphreys\nMARIO Rationale:The metric system is based on the idea that a meter is equal to 100 centimeters.\nSFT Rationale: A meter is a unit of length. The most common unit of measurement used in the United States is the meter.\nQuestion: What can be used to navigate? (A) music (B) backbones (C) astrology (D) cilia (E) magic (F) sponges (G) astronomy (H) lamphreys\nMARIO Rationale: Cilia are tiny hairs on the surface of the skin that can be used to navigate.\nSFT Rationale: Navigating requires using a variety of tools and techniques. Some of these tools include compass, compass,\ncompass, compass, compass, compass, compass, compass, compass, compass, compass, compass, compass, compass, compass,\ncompass, compass, compass, compass, compass, compass, compass, compass, and com\n5\nDISCUSSION\n5.1\nPROPERTIES AND METRICS\nWhile the properties we have optimized for in this work are necessary properties, the question of\nwhat constitutes a complete set of properties for a rationale to be deemed as high-quality remains an\nopen problem (Joshi et al., 2023; Wiegreffe et al., 2022; Golovneva et al., 2022). Some contempo-\nrary works which work on other necessary rationale properties are REV Chen et al. (2023a) (nov-\nelty of information in rationale and faithfulness of rationale towards the predicted label), ROSCOE\nGolovneva et al. (2022) / ReCEval Prasad et al. (2023) (metrics that score rationales with respect\nto their step-by-step reasoning), LAS Hase et al. (2020) (faithfulness of rationale towards predicted\nlabels) etc. Further, there are also properties which do not have widespread implementations (to the\nbest of our knowledge) such as factual-correctness and completeness of rationales (existing com-\npleteness metrics depend upon the presence of gold rationales, which are not easily available and\nwhich mean that they cannot score any alternate reasoning to the answer). As future work, we hope\nto collect an extended set of properties and corresponding metrics, and apply MARIO to improve\nthem.\n5.2\nMULTI-REWARD HACKING\nAs additional experimentation with an alternate set of properties relevant to our chosen QA datasets,\nwe worked on a set of experiments focusing on factual-correctness and lexical diversity; specifically\n9\nfor STRATEGYQA which requires historical or factual correctness of the rationale (this is different\nfrom common-sense or logical correctness measured by PLAUSIBILITY, as explained in VERA(Liu\net al., 2023), and used in our work). We started off with an existing metric for fact verification\nLOREN (Chen et al., 2022) - while effective, we couldn\u2019t use this metric in practice since each score\nprediction required a Web API call, which is inefficient in our setting with training data generation\nand scoring, as done in MARIO. We tried a weaker metric, wherein we queried the rationale with a\nlarger LM, FLAN-T5-XXL and asked if the given rationale was factually correct or not (we took the\nprobability of \u2018yes\u2019 under yes/no). We noticed that applying QUARK/MARIO with this metric led to\nsome interesting reward hacking, as we show in the first two rows of Table 5. Trying to improve on\njust factuality caused the LM to start generating repetitive text, as an easy way of generating factual\nstatements. When we tried to counter this by training MARIO on factuality and lexical diversity\ntogether, the LM started generating incomplete rationales.\nTable 5: Reward Hacking observed in training using QUARK: We show examples of how if\nthe rewards used are very weak or incompatible with each other, they can lead to strange reward\nhacking where the score numerically increase, but the rationales become worse qualitatively.\nQuestion and Rationale\nErrors by other LMs\nQuestion: Can a snow leopard swim?\nRationale: The snow leopard is a cat. The snow leopard is a cat.\nT5-LARGE Repetition\nQuestion: Would someone on Venus be unlikely to experience hypothermia?\nRationale Hypothermia is a condition in which the body is unable to produce enough heat.\nT5-LARGE Incomplete\nQuestion: Did Cleopatra have ethnicity closer to Egyptians than Greeks?\nRationale: Cleopatra was a member of the Roman dynasty of Egypt.\nThe Roman dynasty of Egypt was a branch of the Roman Empire.\nThe Roman Empire was a branch of the Roman Empire.\nFLAN-T5-XL Repetition\nQuestion: Is Fiat Chrysler gaining a new overall corporate identity?\nRationale: Fiat Chrysler is a brand of automobiles. Fiat Chrysler is a brand of cars.\nFLAN-T5-XL Repetition\nWe further noted that this kind of repetitive generation is observed even in larger LMs which on\nsurface seem much better (as seen in rows 3 and 4 in Table 5). Therefore, we note that selecting\nstrong rewards, as well as careful qualitative investigation is extremely important to prevent this\nkind of reward hacking \u2013 where an increase in individual numerical reward scores do not guarantee\noverall qualitative improvements.\n5.3\nIS ACCURACY ENOUGH OF AN INDICATOR FOR RATIONALE QUALITY?\nAs we discuss in \u00a71, many contemporary works on self-rationalization ignore the independent qual-\nity of the generated rationale, and focus entirely on how the rationale can contribute to the task\nperformance. In this discussion, we analyze the reverse: if an LM is trained only with respect to\ntask performance, what does this mean for the rationale? We refer back to our main results, Ta-\nble 3; we specifically look at the rows SFT, FILT-ACC and MARIO. We first see that both FILT-ACC\nand SFT both improve upon the TASK ACCURACY on all five datasets, as intended. We then see\nthat for STRATEGYQA, QUAREL, NUMERSENSE and QASC, the average quality of the rationales\ngenerated by MARIO is decidedly better than the rationales generated by FILT-ACC, as seen by the\nvalues of the individual rationale quality metrics. For OPENBOOKQA, the analysis from just the\nmetrics is inconclusive; hence, we perform human studies comparing FILT-ACC and MARIO, in the\nsame manner as in \u00a74.3.\nWe find that human annotators prefer MARIO\u2019s rationales highly over that of FILT-ACC: for 69.65%\nof the questions, majority of the annotators prefer MARIO\u2019s rationales (as opposed to 22.88% of\npreference for FILT-ACC\u2019s rationales, and 7.46% preference for both). We further performed human\nstudies for PLAUSIBILITY and CONSISTENCY, and again, MARIO\u2019s rationales were found to be\ndistinctly better (PLAUSIBILITY: 49.5% preference for MARIO, 32.58% for SFT, 13.43% both,\n0.99% neither; CONSISTENCY: 48% preference for MARIO, 37.31% for SFT, 9.45% both, 2.48%\nneither). In conclusion, we find that optimizing for task performance does not naturally improve\nrationale performance, which further motivates the introduction of MARIO.\n10\n6\nCONCLUSION AND FUTURE WORK\nExisting self-rationalization LMs use rationales as a means for improving downstream task accuracy,\nwith the help of large-scale LMs. In this work, we propose MARIO, an algorithm that performs\nmulti-reward optimization of small self-rationalizing LMs to jointly improve the quality of their\nrationales as well as their task accuracy. We present strong experimental results on a small LM, T5-\nLARGE, over competitive baselines, on datasets STRATEGYQA, QUAREL OPENBOOKQA, NU-\nMERSENSE and QASC. In addition to a strong improvement in task accuracy, we see that rationales\nproduced by training an LM with our method are strongly preferred by human annotators. Lastly,\nwe discuss intricacies of reward-conditioned rationale generation for small LMs, issues faced with\nselecting appropriate rewards, as well as shortcuts taken by QUARK to improve reward scores that\ndo not translate well to qualitative improvement. As future work, we hope to extend our algorithm to\nimproving rationales along more dimensions like completeness, factuality as well as human utility.\nETHICAL CONSIDERATIONS\nLike any natural language generation system/algorithm, MARIO can unintentionally lead to toxic\nand harmful text; it is up to the user of the algorithm to use it responsibly, with non-harmful reward\nmetrics, to prevent the generation of biased and malicious outputs. As noted in McGuffie & New-\nhouse (2020), this is a deliberate misuse of text generation models, and we strongly denounce such\npractices.\nData.\nAll the datasets that we use in our work are released publicly for usage and have been duly\nattributed to their original authors.\nCrowdsourcing.\nAll our crowdworkers are from countries where English is the primary language.\nFor all our human studies, the task is setup in a manner that ensure that the annotators receive com-\npensation that is above minimum wage ($20/hour). Since we conduct extensive qualification tasks\nbefore annotations, crowdworkers that participate in the qualification are compensated more than the\ntask, given the time taken to read and understand task instructions and examples. Furthermore, we\nensure that we correspond with crowdworkers over email to address their queries. Crowdworkers\nhave also been given bonuses for flagging errors in the task, or consistently providing good-quality\nannotations.\nREPRODUCIBILITY\nFor all our experimental results and models, we report (1) the complete hyperparameter setting\nand any bounds explored (Appendix E) as well as the sizes and versions/pretrained-model links of\nall models used, (2) the time taken per experiment, and infrastructure used, (3) the mathematical\nequations (\u00a74.2, Appendix B) for all algorithms and metrics used, (4) descriptions of datasets, and\ndemonstrations used to sample rationales from GPT-3. All our codes and datasets are publicly re-\nleased at https://github.com/INK-USC/RationaleMultiRewardDistillation.\nREFERENCES\nPepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simon-\nsen, and Isabelle Augenstein. Faithfulness tests for natural language explanations. In Proceed-\nings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2:\nShort Papers), pp. 283\u2013294, Toronto, Canada, July 2023. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2023.acl-short.25. URL https://aclanthology.org/2023.\nacl-short.25.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav Kadavath, John\nKernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez,\nTristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario\nAmodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and\n11\nJared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human\nfeedback. ArXiv, abs/2204.05862, 2022. URL https://api.semanticscholar.org/\nCorpusID:248118878.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\nOana-Maria Camburu,\nTim Rockt\u00a8aschel,\nThomas Lukasiewicz,\nand Phil Blunsom.\ne-\nsnli:\nNatural language inference with natural language explanations.\nIn S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Ad-\nvances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.,\n2018.\nURL https://proceedings.neurips.cc/paper_files/paper/2018/\nfile/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf.\nAaron Chan, Maziar Sanjabi, Lambert Mathias, Liang Tan, Shaoliang Nie, Xiaochang Peng, Xiang\nRen, and Hamed Firooz. Unirex: A unified learning framework for language model rationale\nextraction. In International Conference on Machine Learning, pp. 2867\u20132889. PMLR, 2022.\nAaron Chan, Zhiyuan Zeng, Wyatt Lake, Brihi Joshi, Hanjie Chen, and Xiang Ren. Knife: Distilling\nreasoning knowledge from free-text rationales, 2023.\nHanjie Chen, Faeze Brahman, Xiang Ren, Yangfeng Ji, Yejin Choi, and Swabha Swayamdipta.\nREV: Information-theoretic evaluation of free-text rationales. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2007\u2013\n2030, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/\nv1/2023.acl-long.112. URL https://aclanthology.org/2023.acl-long.112.\nJiangjie Chen, Qiaoben Bao, Changzhi Sun, Xinbo Zhang, Jiaze Chen, Hao Zhou, Yanghua Xiao,\nand Lei Li. Loren: Logic-regularized reasoning for interpretable fact verification. In Proceedings\nof the AAAI Conference on Artificial Intelligence, volume 36, pp. 10482\u201310491, 2022.\nWei-Lin Chen, An-Zi Yen, Hen-Hsen Huang, Cheng-Kuang Wu, and Hsin-Hsi Chen. Zara: Improv-\ning few-shot self-rationalization for small language models, 2023b.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-\nels. arXiv preprint arXiv:2210.11416, 2022.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun\nShum, and T. Zhang. Raft: Reward ranked finetuning for generative foundation model align-\nment.\nArXiv, abs/2304.06767, 2023.\nURL https://api.semanticscholar.org/\nCorpusID:258170300.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy Liang, and Tatsunori Hashimoto.\nAlpacafarm: A simulation framework for\nmethods that learn from human feedback.\nArXiv, abs/2305.14387, 2023.\nURL https:\n//api.semanticscholar.org/CorpusID:258865545.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\nuse a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of\nthe Association for Computational Linguistics, 9:346\u2013361, 2021.\nOlga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-\nZarandi, and Asli Celikyilmaz. Roscoe: A suite of metrics for scoring step-by-step reasoning.\narXiv preprint arXiv:2212.07919, 2022.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Ab-\nhishek Sharma, Aditya Siddhant, Alexa Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey,\nA. Doucet, Orhan Firat, and Nando de Freitas.\nReinforced self-training (rest) for language\n12\nmodeling. ArXiv, abs/2308.08998, 2023. URL https://api.semanticscholar.org/\nCorpusID:261031028.\nPeter Hase, Shiyue Zhang, Harry Xie, and Mohit Bansal. Leakage-adjusted simulatability: Can\nmodels generate non-trivial explanations of their behavior in natural language? arXiv preprint\narXiv:2010.04119, 2020.\nDirk Hovy and Shrimai Prabhumoye. Five sources of bias in natural language processing. Language\nand Linguistics Compass, 15(8):e12432, 2021.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Rat-\nner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.\nDistilling step-by-step! outperform-\ning larger language models with less training data and smaller model sizes. In Findings of the\nAssociation for Computational Linguistics: ACL 2023, pp. 8003\u20138017, Toronto, Canada, July\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.507. URL\nhttps://aclanthology.org/2023.findings-acl.507.\nBrihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang,\nYejin Choi, and Xiang Ren. Are machine rationales (not) useful to humans? measuring and\nimproving human utility of free-text rationales. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 7103\u20137128, Toronto,\nCanada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.\n392. URL https://aclanthology.org/2023.acl-long.392.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras,\nand Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations.\narXiv preprint arXiv:2205.11822, 2022.\nTushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. Qasc: A dataset\nfor question answering via sentence composition. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pp. 8082\u20138090, 2020.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems,\n35:22199\u201322213, 2022.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners, 2023.\nPiyawat Lertvittayakumjorn and Francesca Toni. Explanation-based human debugging of NLP mod-\nels: A survey. Transactions of the Association for Computational Linguistics, 9:1508\u20131528, 2021.\ndoi: 10.1162/tacl a 00440. URL https://aclanthology.org/2021.tacl-1.90.\nLiunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. Symbolic\nchain-of-thought distillation: Small models can also \u201cthink\u201d step-by-step. ArXiv, abs/2306.14050,\n2023a. URL https://api.semanticscholar.org/CorpusID:259251773.\nLiunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. Sym-\nbolic chain-of-thought distillation: Small models can also \u201cthink\u201d step-by-step. In Proceedings\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 2665\u20132679, Toronto, Canada, July 2023b. Association for Computational Linguis-\ntics.\ndoi: 10.18653/v1/2023.acl-long.150.\nURL https://aclanthology.org/2023.\nacl-long.150.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization.\narXiv preprint arXiv:2210.15097, 2022.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making\nlanguage models better reasoners with step-aware verifier. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315\u2013\n5333, 2023c.\n13\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. Birds have four legs?! numersense:\nProbing numerical commonsense knowledge of pre-trained language models.\narXiv preprint\narXiv:2005.00683, 2020.\nJiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and\nYejin Choi. Rainier: Reinforced knowledge introspector for commonsense question answering.\narXiv preprint arXiv:2210.03078, 2022.\nJiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah A Smith, Yejin Choi, and Hannaneh Ha-\njishirzi. Vera: A general-purpose plausibility estimation model for commonsense statements.\narXiv preprint arXiv:2305.03695, 2023.\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am-\nmanabrolu, and Yejin Choi.\nQuark: Controllable text generation with reinforced unlearning.\nAdvances in neural information processing systems, 35:27591\u201327609, 2022.\nXiming Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Chandu, Abhilasha Ravichander,\nLianhui Qin, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, et al. Inference-time policy\nadapters (ipa): Tailoring extreme-scale lms without fine-tuning. arXiv preprint arXiv:2305.15065,\n2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad\nMajumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark.\nSelf-refine:\nItera-\ntive refinement with self-feedback.\nArXiv, abs/2303.17651, 2023.\nURL https://api.\nsemanticscholar.org/CorpusID:257900871.\nAna Marasovic, Iz Beltagy, Doug Downey, and Matthew Peters.\nFew-shot self-rationalization\nwith natural language prompts. In Findings of the Association for Computational Linguistics:\nNAACL 2022, pp. 410\u2013424, Seattle, United States, July 2022. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2022.findings-naacl.31. URL https://aclanthology.org/\n2022.findings-naacl.31.\nKris McGuffie and Alex Newhouse. The radicalization risks of gpt-3 and advanced neural language\nmodels. arXiv preprint arXiv:2009.06807, 2020.\nClara Meister, Elizabeth Salesky, and Ryan Cotterell.\nGeneralized entropy regularization or:\nThere\u2019s nothing special about label smoothing.\nIn Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, pp. 6870\u20136886, Online, July 2020. Asso-\nciation for Computational Linguistics.\ndoi: 10.18653/v1/2020.acl-main.615.\nURL https:\n//aclanthology.org/2020.acl-main.615.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,\n2018.\nSharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan.\nWt5?! training text-to-text models to explain their predictions. arXiv preprint arXiv:2004.14546,\n2020.\nArchiki Prasad, Swarnadeep Saha, Xiang Zhou, and Mohit Bansal. Receval: Evaluating reasoning\nchains via correctness and informativeness. arXiv preprint arXiv:2304.10703, 2023.\nDanish Pruthi, Rachit Bansal, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C.\nLipton, Graham Neubig, and William W. Cohen. Evaluating explanations: How much do ex-\nplanations from the teacher aid students?\nTransactions of the Association for Computational\nLinguistics, 10:359\u2013375, 2022. doi: 10.1162/tacl a 00465. URL https://aclanthology.\norg/2022.tacl-1.21.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself!\nleveraging language models for commonsense reasoning. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pp. 4932\u20134942, Florence, Italy, July\n2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1487. URL https:\n//aclanthology.org/P19-1487.\n14\nSwarnadeep Saha, Peter Hase, and Mohit Bansal.\nCan language models teach weaker agents?\nteacher explanations improve students via theory of mind, 2023.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\nProxi-\nmal policy optimization algorithms.\nArXiv, abs/1707.06347, 2017.\nURL https://api.\nsemanticscholar.org/CorpusID:28695052.\nJiao Sun, Swabha Swayamdipta, Jonathan May, and Xuezhe Ma. Investigating the benefits of free-\nform rationales. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp.\n5867\u20135882, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational\nLinguistics. doi: 10.18653/v1/2022.findings-emnlp.432. URL https://aclanthology.\norg/2022.findings-emnlp.432.\nOyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, and Ashish Sabharwal. Quarel: A dataset\nand models for answering questions about qualitative relationships. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 33, pp. 7063\u20137071, 2019.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nPeifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. Pinto: Faithful language\nreasoning using prompt-generated rationales. ArXiv, abs/2211.01562, 2022. URL https://\napi.semanticscholar.org/CorpusID:253265114.\nPeiFeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. PINTO: Faithful lan-\nguage reasoning using prompt-generated rationales.\nIn The Eleventh International Confer-\nence on Learning Representations, 2023a. URL https://openreview.net/forum?id=\nWBXbRs63oVu.\nPeifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. SCOTT: Self-\nconsistent chain-of-thought distillation. In Proceedings of the 61st Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Papers), pp. 5546\u20135558, Toronto, Canada,\nJuly 2023b. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-long.304.\nURL https://aclanthology.org/2023.acl-long.304.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\nSarah Wiegreffe, Ana Marasovi\u00b4c, and Noah A. Smith. Measuring association between labels and\nfree-text rationales. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 10266\u201310284, Online and Punta Cana, Dominican Republic, Novem-\nber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.804.\nURL https://aclanthology.org/2021.emnlp-main.804.\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi.\nReframing\nhuman-AI collaboration for generating free-text explanations. In Proceedings of the 2022 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pp. 632\u2013658, Seattle, United States, July 2022. Association for Compu-\ntational Linguistics. doi: 10.18653/v1/2022.naacl-main.47. URL https://aclanthology.\norg/2022.naacl-main.47.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A.\nSmith, Mari Ostendorf, and Hanna Hajishirzi.\nFine-grained human feedback gives better re-\nwards for language model training.\nArXiv, abs/2306.01693, 2023.\nURL https://api.\nsemanticscholar.org/CorpusID:259064099.\nEric Zelikman, Jesse Mu, Noah D Goodman, and Yuhuai Tony Wu. Star: Self-taught reasoner\nbootstrapping reasoning with reasoning. 2022.\n15\nA\nRELATED WORK\nSelf-rationalization and rationale-based distillation. There are two primary methods adopted by\nprior works for generating free-text rationales. The first set of approaches use gold human-written\nrationales to train a rationale generation model (Camburu et al., 2018; Narang et al., 2020; Wiegreffe\net al., 2021). The second set of approaches prompt large LMs with the help of curated templates\nwith or without demonstrations containing examples of rationale generation for the task at hand (Wei\net al., 2022; Kojima et al., 2023; Li et al., 2023c; Jung et al., 2022). Some approaches also leverage\nfew-shot training approaches with a handful of gold rationales (Marasovic et al., 2022; Chen et al.,\n2023b). Recent approaches also leverage rationales generated by large LMs to distill small LMs to\nbe better at the task or better rationalizers. (Pruthi et al., 2022; Li et al., 2023b; Chan et al., 2023;\nWang et al., 2023b; Saha et al., 2023; Hsieh et al., 2023)\nEvaluating free-text rationales. Existing works have conducted human and automatic evaluation\nof free-text rationales based on their association with predicted labels (Wiegreffe et al., 2021), ac-\nceptability (Wiegreffe et al., 2022), informativeness (Chen et al., 2023a), benefits and human utility\n(Sun et al., 2022; Joshi et al., 2023), simulatability (Rajani et al., 2019; Hase et al., 2020) and faith-\nfulness (Atanasova et al., 2023; Wang et al., 2023a) to name a few. Some recent works have also\nprovided frameworks to evaluate logical correctness of reasoning chains, that are similar to free-text\nrationales (Golovneva et al., 2022; Prasad et al., 2023).\nReward-conditioned text generation. Reinforcement learning has proven to be a reliable means\nto optimize language models towards a specific objective. One such example, proximal policy op-\ntimization (PPO) (Schulman et al., 2017), has been commonly used for a variety of tasks, spanning\ndetoxification (Wu et al., 2023; Lu et al., 2022), RLHF (Dubois et al., 2023; Bai et al., 2022), im-\nproving commonsense reasoning capabilities (Liu et al., 2022), and more. Adjacent to PPO, there\nare several other lighter-weight algorithms which condition the policy language model directly on\nthe reward without the need for a value function (Lu et al., 2022; Gulcehre et al., 2023; Dong et al.,\n2023; Lu et al., 2023; Zelikman et al., 2022). These methods rely on iterative, off-policy explo-\nrations at fixed intervals to continuously aggregate new trajectories to learn from. Another line of\nwork improves the reward directly through iterative refinement on a frozen policy model (Madaan\net al., 2023).\nB\nQUARK AND MARIO\nHere, we describe QUARK and MARIO in more technical detail (refer the top and bottom pipelines\nin Figure 5 respectively).\nQUARK begins training with a pretrained trained language model P0(t\u2223x); QUARK also requires\na reference language model Pref(t\u2223x) (which can be the same as P0, or different), and a reward\nfunction Rew(t, x) \u2192 R. Note that x = [x0, x1, . . . , xm\u22121] stands for the input text sequence, and\nt = [t0, t1, . . . , tn\u22121] stands for the output sequence generation.\nAs we explain in Section 3.1, QUARK operates in an iterative fashion:\n1. sampling P0 to generate more training data: Dnew = (x, tnew)\n2. scoring the generated data using Rew(x, t): D\n\u2032\nnew = (x, tnew, r)\n3. using these instance-level scores to sort and bin the data into a fixed number of\nbins [b1, b2, . . . , b5] each of which correspond to a unique control token:\nD\n\u2032\u2032\nnew\n=\n(x, tnew, r, b),\n4. Adding the now control-token attached data to the (growing) training data pool: D =\nD \u222a D\n\u2032\u2032\nnew\nDuring training, the model starts to associate each control token with its corresponding quality of\ndata (as given by Rew(x, t)), and to obtain the best quality generations during inference, QUARK\nsamples the trained P0 using the control token corresponding to the highest reward measure. QUARK\nis trained using the following training objectives:\n16\nLanguage Model\nCan a snow leopard swim?\nA snow leopard is a cat that lives in the snow.\nCats can swim. So the answer is yes.\nTOK2\nControl token on\nthe decoder side\nLanguage Model\nCan a snow leopard swim?\nA snow leopard is a cat that lives\nin the snow. Cats can swim. So\nthe answer is yes.\nControl tokens on\nthe decoder side\nSingle reward control tokens:\nTOK1, TOK2, TOK3, TOK4\nMulti-reward control tokens:\nR1: TOK1_A,B,C,C\nR2: TOK2_A,B,C,D\nR3: TOK3_A,B,C,D\nTOK1_B\nTOK2_D\nTOK3_A\nSingle-reward Quark\nMulti-reward Quark\nTOK1_B\nTOK1_B\nTOK2_D\nTOK1_B\nTOK2_D\nTOK3_A\nLanguage Model\nCan a snow leopard swim?\nA snow leopard is a cat that lives\nin the snow. Cats can swim. So\nthe answer is yes.\nCANONICAL MaRio\nADDITIVE MaRio\n+\n+\n+\nquar\nFigure 5: Optimizing properties with QUARK (top) and MARIO (bottom)\n\u2022 Reward-based learning using implicit reward signals based on control tokens (which are\nobtained by sorting the reward Rew(x, t) scores), as described above,\n\u2022 Language model objective using supervised/cross-entropy loss with respect to the target\ngeneration (as explained above, QUARK samples training data in an online manner from\nP0; however, if gold or silver offline training data is available, that can also be injected into\nthe training pipeline by scoring with Rew(x, t))\n\u2022 Stable text generation using the KL divergence penalty of P0\u2019s generation with respect to\nPref, and.\n\u2022 Entropy regularization of the generated text as in Meister et al. (2020)\nWe extend QUARK to MARIO by using multiple sets of control tokens, each corresponding to a\ndistinct reward/property, i.e., Rew1(x, t), Rew2(x, t), . . . , Rewk(x, t); the CANONICAL and AD-\nDITIVE methods use these control tokens either together, or in a step-by-step fashion as we explain\nin \u00a73.2, 3.3. The rest of the training follows the same iterative process and training objectives as\nQUARK.\nC\nORDER OF TOKENS\nAs we explain in the above two sections, the order of the control tokens corresponding to each\nreward we use in training our self-rationalizing LM is a design choice. Say for example, we have\nthree properties, along with control tokens corresponding to the task accuracy (as we do in this\npaper, refer \u00a74.2): this means that there are potentially 24 orders of these properties that we can\nuse in CANONICAL MARIO, and 48 possible variations that we can use for ADDITIVE MARIO (24\norders x 2 directions in which we can introduce the property to the training \u2013 left or right of the\nexisting control tokens, assuming we keep the direction of addition consistent throughout training).\nIt is impractical and inefficient to experiment with all these possible orders to pick the best possible\none. Hence, we propose a simple way of picking the order, based on the relative strengths of a\n(supervised-trained) self-rationalizing LM in each of these properties.\nFor example, say we have four reward metrics R1, R2, R3, R4, and we determine through a prede-\nfined method which property the LM is relatively stronger in (for example, say the LM is good at\ngenerating lexically diverse statements, but is only moderately good at grammar, is broadly bad at\ngenerating plausible statements, and even worse at producing concise rationales). For example, we\ndetermine the relative strength of rewards based on how good the supervised finetuned baseline SFT\n17\nis on a particular metric on the validation set, as opposed to the maximum and minimum value of\nthe metric itself.\nstrength(Ri) =\nmax(Ri) \u2212 ri\nmax(Ri) \u2212 min(Ri)\n(5)\nHere Ri refers to the reward, ri refers to the value the SFT has on the property Ri on the validation\nset, and max/min(Ri) refer to the maximum and minimum value taken by the reward metric Ri.\nFor example, let the relative order of the four reward metrics using the above approach is R2 < R1 <\nR4 < R3. Hence, we experiment with training the LM with the order R2, R1, R4, R3 if we want\nto allow the weaker rewards to improve on their own, before the stronger rewards are introduced\ninto the mix. Additionally, we can also use the opposite order R3, R4, R1, R2, so that the LM can\nquickly optimize on the stronger rewards and then try to be better with the weaker rewards.\nD\nDATASET SPLITS\n\u2022 For STRATEGYQA, since labels are not available for evaluation sets, we split the train set\ninto training, validation and test sets (taken from Joshi et al. (2023)), and report scores on\nthis test set.\n\u2022 For OPENBOOKQA and QUAREL, we use the provided training dataset, tuned on the val-\nidation set and report final performances on the test set7.\n\u2022 For NUMERSENSE, we use the train, validation and test sets as in the official GitHub8\nrelease.\n\u2022 For QASC, we split the original train set into train and validation (900 questions chosen\nrandomly for validation), and use the original validation set as the test set9.\nE\nHYPERPARAMETERS AND EVALUATION\nWe use T5-LARGE (0.7B parameters) for SFT and all our MARIO experiments, and we use T5-\nBASE for our CONSISTENCY models (as used in the original work Wiegreffe et al. (2021)) - we\nalways start training with the pretrained model from HuggingFace10. Tables 6, 7 and 8 show the\nhyperparameters used to train SFT, CONSISTENCY and MARIO respectively. Note that for our\nMARIO training, we use SFT as the reference model (Pref(t\u2223x) from Appendix B) for the KL\ndivergence penalty. Further, during inference, we always use greedy decoding. We run all our\nexperiments on NVIDIA Quadro RTX 8000 GPUs. For training SFT and CONSISTENCY models,\nwe use 1 GPU per experiment; for training MARIO, we use 2 GPUs per experiment - the first GPU\nto hold P0, Pref (notation from Appendix B), and the second GPU to hold the PLAUSIBILITY and\nCONSISTENCY reward models.\nFurthermore, we aggregate metrics using Normalized Relative Gain as mentioned in Chan et al.\n(2022). NRG of a metric value zi (corresponding to the general property Z) is formally defined as:\nNRG(zi) =\nzi \u2212 min(Z)\nmax(Z) \u2212 min(Z)\n(6)\nThe average NRG of a set of metrics (such as with the four metrics in this work) is a simple mathe-\nmatical average of their individual NRG\u2019s.\nFurther, for our statistical significance tests, are done using one-tailed independent t-tests (using\nscipy.stats.ttest ind).\n7https://huggingface.co/datasets/openbookqa,https://huggingface.co/\ndatasets/QuaRel\n8https://github.com/INK-USC/NumerSense/tree/main/data\n9https://huggingface.co/datasets/qasc\n10https://huggingface.co/t5-large,https://huggingface.co/t5-base\n18\nTable 6: SFT training details\nHyperparameter\nValue\nOptimizer\nAdam\nAdam epsilon\n1e-8\nAdam initial learning-rate\n3e-5\nLearning-rate scheduler\nlinear with warmup\nWarmup steps\n1000\nGradient clipping\n0.5\nTrain batch-size\n4 / 8\nTraining time\n\u223c 4 hours on 1 GPU\nTable 7: Training details for the MQR and MQ models used for CONSISTENCY\nHyperparameter\nValue\nOptimizer\nAdam\nAdam epsilon\n1e-8\nAdam initial learning-rate\n3e-5\nLearning-rate scheduler\nlinear with warmup\nWarmup steps\n1000\nGradient clipping\n0.5\nTrain batch-size\n4 / 32\nTraining time\n\u223c 4 hours on 1 GPU\nTable 8: QUARK and MARIO training details\nHyperparameter\nValue\nOptimizer\nAdam\nAdam epsilon\n1e-8\nAdam initial learning-rate\n3e-5\nLearning-rate scheduler\nlinear with warmup\nWarmup steps\n1000\nGradient clipping\n1.0\nGradient accumulation\n2 steps\nKL-divergence coef.\n0.05 / 0.1\nEntropy regularization coef.\n0.05 / 0.0\nSampling rate\n1 (QUAREL, NUMERSENSE, QASC)\nor 2 (STRATEGYQA, OPENBOOKQA)\nsamples for every train sample\nFrequency of exploration\nevery 300 (STRATEGYQA,\nQUAREL) / 4000 (OPENBOOKQA,\nNUMERSENSE, QASC) steps\nSampling strategy\nTop-p (0.7) sampling\nTemperature for sampling\n1.0\nNumber of distinct reward-bins\n5 for rationale metrics,\n2 for TASK ACCURACY\nTrain batch-size\n4\nTraining time\n\u223c 1 day on 2 GPUs\nOrder of rewards\nSTRATEGYQA: strongest to weakest,\nadd to right\nQUAREL: strongest to weakest,\nOPENBOOKQA: weakest to strongest,\nNUMERSENSE: weakest to strongest,\nQASC: strongest to weakest\n19\nF\nSINGLE REWARD EXPERIMENTS\nFor completeness of analysis, we present single-reward QUARK experiments, where we focus on\nimproving just one property. Table 9 shows results on the same. We first note that in most of the\ncases, MARIO achieves an equivalent or better improvement as compared to single-reward QUARK.\nFurther, we note that even if individually some properties are better when trained under single reward\nQUARK as compared to MARIO, MARIO is the only experiment where all the properties improve as\ncompared to the SFT baseline. We also see that sometimes, single reward QUARK leads to improve-\nment in other metrics as well; this could be because the metrics are positively correlated for that\ndataset. However, since we want to improve all metrics comprehensively, MARIO is a deterministic\nway to achieve the same. (Note: We don\u2019t run the experiment on NUMERSENSE DIVERSITY, since\nSFT already achieves the best possible value of 1.0).\nTable 9: QUARK experiments on improving single rewards. For each dataset, the best averaged\nNRG (across TASK ACCURACY, PLAUSIBILITY, DIVERSITY and CONSISTENCY) is highlighted\nin bold, and each best individual metric is underlined.\nMethod \u2192\nBaselines\nSingle Reward QUARK\nMARIO\nDataset \u2193\nMetric\nSFT\nPRODUCT\nAcc.\nPlau.\nDiv.\nCons.\nCANONICAL\nADDITIVE\nSTRATEGYQA\nAcc.\n57.64\n62.01\n61.57\n61.35\n59.17\n59.17\n60.26\n65.07\nPlau.\n0.33\n0.35\n0.36\n0.36\n0.36\n0.36\n0.38\n0.39\nDiv.\n0.95\n0.92\n0.92\n0.93\n0.96\n0.95\n0.95\n0.97\nCons.\n-0.02\n0.00\n-0.01\n0.01\n-0.04\n0.01\n0.01\n0.04\nAvg. NRG\n58.66\n59.75\n59.77\n60.21\n59.79\n60.17\n60.94\n63.27\nQUAREL\nAcc.\n76.99\n79.53\n81.88\n80.62\n78.99\n80.62\n79.89\n78.99\nPlau.\n0.71\n0.72\n0.74\n0.81\n0.71\n0.73\n0.77\n0.75\nDiv.\n0.95\n0.95\n0.95\n0.93\n0.97\n0.95\n0.97\n0.97\nCons.\n0.18\n0.21\n0.23\n0.20\n0.20\n0.22\n0.19\n0.20\nAvg. NRG\n75.50\n76.71\n78.1\n78.66\n77.0\n77.41\n78.35\n77.75\nOPENBOOKQA\nAcc.\n63.65\n61.65\n64.46\n61.65\n64.66\n66.27\n66.06\n65.55\nPlau.\n0.53\n0.52\n0.54\n0.53\n0.51\n0.54\n0.55\n0.55\nDiv.\n0.98\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\n0.98\nCons.\n0.05\n0.07\n0.09\n0.07\n0.07\n0.11\n0.09\n0.09\nAvg. NRG\n66.79\n66.54\n67.99\n66.79\n67.04\n68.69\n68.64\n68.29\nNUMERSENSE\nAcc.\n46.23\n50.75\n51.76\n50.75\n-\n54.27\n55.28\n54.27\nPlau.\n0.60\n0.60\n0.63\n0.63\n-\n0.61\n0.63\n0.63\nDiv.\n1.00\n1.00\n0.99\n1.00\n-\n1.00\n1.00\n0.99\nCons.\n0.17\n0.20\n0.21\n0.21\n-\n0.22\n0.23\n0.23\nAvg. NRG\n66.18\n67.69\n68.57\n68.56\n-\n69.07\n69.95\n69.44\nQASC\nAcc.\n58.64\n57.88\n58.21\n57.88\n58.1\n58.75\n60.15\n59.61\nPlau.\n0.44\n0.43\n0.42\n0.45\n0.40\n0.41\n0.47\n0.47\nDiv.\n0.96\n0.95\n0.96\n0.97\n0.98\n0.96\n0.99\n0.99\nCons.\n0.19\n0.17\n0.17\n0.17\n0.17\n0.20\n0.19\n0.19\nAvg. NRG\n64.54\n63.60\n63.68\n64.6\n63.65\n63.94\n66.41\n66.28\nG\nEXTENDED COMPARISON WITH FEW-SHOT LLMS\nIn Table 10, we present the detailed performance metrics of different reference LMs as opposed to\nMARIO. For QUAREL, MARIO beats all reference LLMs except for GPT-3 on all four metrics. For\nNUMERSENSE, MARIO beats all reference LLMs except for FLAN-T5-XXL and GPT-3 on all four\nmetrics. The results are more varied with STRATEGYQA, OPENBOOKQA and QASC; MARIO is\nbetter than the reference LLMs (apart from GPT-3) in the case of DIVERSITY for all three datasets,\nand in cases of varying comparisons with the reference LLMs (for example, MARIO is better at\nCONSISTENCY than FLAN-T5-L and LLAMA-7B for OPENBOOKQA). However, overall, we note\nthat our model still needs to go further with respect to PLAUSIBILITY and TASK ACCURACY. We\nnote that our method MARIO has done a significant job in bridging the gap between LMs such as\nthe ones discussed in this section, and much smaller LMs such as T5-LARGE. We also note for\nTASK ACCURACY, CONSISTENCY and DIVERSITY, MARIO beats FLAN-T5-L, a model of equal\nsize which has been trained with instruction fine-tuning for all 5 datasets (except for QASC and\n20\nCONSISTENCY); and for all datasets except for STRATEGYQA, MARIO also beats PLAUSIBILITY\nof FLAN-T5-L.\nTable 10: We compare MARIO with strong few-shot reference LMs: FLAN-T5, LLAMA and\nGPT-3. Apart from FLAN-T5-L (which we have included to show a model of equivalent size that\nhas been instruction finetuned), all these models are much bigger than our T5-LARGE trained with\nMARIO.\nMethod \u2192\nFLAN-T5\nLLAMA\nGPT-3\nMARIO (0.7B)\nDataset \u2193\nMetric\nL\nXL\nXXL\n7B\n65B\nT-D-003\nCANONICAL\nADDITIVE\nSTRATEGYQA\nAcc.\n54.59\n71.83\n70.52\n59.17\n72.27\n69.0\n60.26\n65.07\nPlau.\n0.49\n0.59\n0.64\n0.72\n0.70\n0.70\n0.38\n0.39\nDiv.\n0.88\n0.82\n0.86\n0.88\n0.93\n0.95\n0.95\n0.97\nCons.\n-0.01\n0.02\n0.05\n0.00\n0.06\n0.09\n0.01\n0.04\nAvg. NRG\n60.27\n65.96\n68.26\n67.29\n72.07\n72.13\n60.94\n63.27\nQUAREL\nAcc.\n77.36\n76.99\n77.54\n56.70\n76.27\n83.33\n79.89\n78.99\nPlau.\n0.60\n0.68\n0.70\n0.64\n0.70\n0.78\n0.77\n0.75\nDiv.\n0.93\n0.90\n0.92\n0.94\n0.96\n0.95\n0.97\n0.97\nCons.\n0.14\n0.13\n0.10\n0.00\n0.17\n0.23\n0.19\n0.20\nAvg. NRG\n71.84\n72.87\n73.64\n66.18\n75.19\n79.46\n78.35\n77.75\nOPENBOOKQA\nAcc.\n60.64\n72.49\n80.32\n40.76\n73.30\n85.94\n66.06\n65.66\nPlau.\n0.49\n0.59\n0.67\n0.66\n0.73\n0.74\n0.55\n0.55\nDiv.\n0.87\n0.84\n0.93\n0.95\n0.97\n0.99\n0.99\n0.98\nCons.\n0.05\n0.13\n0.22\n0.01\n0.16\n0.25\n0.09\n0.09\nAvg. NRG\n62.29\n68.00\n75.33\n63.07\n75.33\n80.36\n68.64\n68.29\nNUMERSENSE\nAcc.\n26.13\n48.24\n61.81\n17.59\n36.18\n74.37\n55.28\n54.27\nPlau.\n0.51\n0.65\n0.72\n0.62\n0.68\n0.76\n0.63\n0.63\nDiv.\n0.97\n0.92\n0.98\n0.98\n0.99\n1.00\n1.00\n0.99\nCons.\n0.03\n0.19\n0.35\n0.2\n0.36\n0.46\n0.23\n0.23\nAvg. NRG\n56.41\n66.19\n74.83\n59.40\n67.80\n80.84\n69.95\n69.44\nQASC\nAcc.\n61.02\n70.63\n74.84\n24.19\n75.59\n80.24\n60.15\n59.61\nPlau.\n0.44\n0.55\n0.63\n0.59\n0.71\n0.75\n0.47\n0.47\nDiv.\n0.78\n0.63\n0.89\n0.74\n0.98\n0.97\n0.99\n0.99\nCons.\n0.23\n0.32\n0.37\n0.10\n0.31\n0.38\n0.19\n0.19\nAvg. NRG\n61.13\n63.66\n73.84\n53.05\n77.52\n80.31\n66.41\n66.28\nH\nFEW-SHOT DEMONSTRATIONS\nWe include the full few-shot demonstrations used to prompt different models for five datasets in\nTables 11-15. For clarity, the rationalizations are highlighted.\nI\nCROWDSOURCING FOR HUMAN EVALUATIONS\nIn this section, we describe the MTurk experiment setup. Each MTurk annotator is paid above\nminimum wage. Since the dataset we used is carefully annotated by human, we can assure there is\nno toxic content and our experiment setup was submitted to IRB for ethical review. We limited our\nTurkers to English speaking nations - United States, Canada, Australia, New Zealand and United\nKingdom.\nTo ensure the quality of evaluation, we conduct a round of qualification tasks which include a small\nset of evaluations. Turkers need to finish the qualification task first and get results of it, then we will\nshow them the whole task.\nI.0.1\nWORKER SELECTION AND QUALITY CONTROL\nHere, we describe details about how workers are selected and how annotations are ensured to be\nclean. First, we employ multiple rounds of trials before deploying the actual task so as to get\nfeedback from annotators whether they understand the task correctly. This includes in-house tests,\n21\nTable 11: The complete prompt of rationalization for STRATEGYQA. Demonstration examples are\ncollected from Wei et al., 2022\nQ: Do hamsters provide food for any animals?\nHamsters are prey animals. Prey animals provide food for predators.\nA: So the answer is yes.\nQ: Could Brooke Shields succeed at University of Pennsylvania?\nBrooke Shields graduated from Princeton University. Princeton is ranked as the number 1 na-\ntional college by US news. University of Pennsylvania is ranked as number 6 national college\nby US news. Princeton only admits around 6 percent of applicants as of 2018. University of\nPennsylvania accepts around 9% of applicants as of 2018.\nA: So the answer is yes.\nQ: Yes or no: Hydrogen\u2019s atomic number squared exceeds number of Spice Girls?\nHydrogen is the first element and has an atomic number of one. To square a number, you mul-\ntiply it by itself. The Spice Girls has five members.\nA: So the answer is no.\nQ: Yes or no: Is it common to see frost during some college commencements?\nCollege commencement ceremonies often happen during the months of December, May, and\nsometimes June. Frost isn\u2019t uncommon to see during the month of December, as it is the win-\nter.\nA: So the answer is yes.\nQ: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\nThe War in Vietnam (1945-46) lasted around 6 months. The gestation period for a llama is 11\nmonths.\nA: So the answer is no.\nQ: Yes or no: Would a pear sink in water?\nThe density of a raw pear is about 0.59 g/cm\u02c63. The density of water is about 1 g/cm\u02c63. Objects\nonly sink if they are denser than the surrounding fluid.\nA: So the answer is no.\ntested via Amazon Turk Sandbox 11 and small batches tested on Turk. Second, we create a set of\nmedium to hard qualification tasks for verifying preference, plausibility and consistency annotations\nthat the annotators have to work on. These tasks are hand curated that cater certain parts of the\ninstruction \u2013 whether the annotators are reading the rationale correctly, or whether they are able\nto make appropriate connections between the rationale and the question. This weeds out a lot of\nannotators who do not understand the task or are cheating. We also weed out workers who are too\n\u2018fast\u2019 (completing the task in less than 5 seconds, which is indicative of potential slacking in the\ntask). Third, we constantly monitor task responses and feedback provided to annotators about their\ntask. We also collect feedback from them which we adapt in new versions of the task.\nThe final MTurk instructions and template that we land upon after the qualifications is shown in\nFigure 6 and 7\nJ\nLIMITATIONS\nMARIO demonstrates promising improvements on the self-rationalization capability of small lan-\nguage models; we note that using MARIO on a small LM like T5-LARGE leads to considerable\nbridging of the gap between the quality of its rationales versus the quality of rationales generated\nby much larger language models. However, we note that the results are still very much dependent\n11https://requester.mturk.com/developer/sandbox\n22\nTable 12: The complete prompt of rationalization for QUAREL. Demonstration examples are col-\nlected from Li et al., 2023a\nQ: Mike was snowboarding on the snow and hit a piece of ice. He went much faster on the ice\nbecause\nis smoother.\n(A) snow (B) ice\nA: When something is smoother, it is easier to slide on. Thus, he could go faster on the ice\nbecause ice is smoother. So the answer is: (B)\nQ: I could hear then boy that say close to me clear as day, however I could not hear the young\nlady sitting in the back of the room. Who am I able to hear louder\n(A) Boy (B) Lady\nA: When someone is close, it is easier to hear them. I also could not hear the young lady well.\nThus, I am able to hear the boy louder. So the answer is: (A)\nQ: I watched the snowflakes go from tiny specks in the sky to a nice size once they fell on my\nface. When did the snowflakes seem bigger?\n(A) in the sky (B) on my face\nA: When something is closer, it seems bigger. The snowflakes are closer when they are on my\nface. Thus, they seem bigger when they are on my face. So the answer is: (B)\nQ: When Tammy tried to slide the glass mixing bowl down the marble counter top to her mom,\nit came to a dead stop when it reached the wooden cutting board. The bowl came to a stop\nbecause the wooden cutting board has\n(A) more resistance or (B) less resistance\nA: When something has more resistance, it is harder to slide. Thus, the bowl came to a stop\nbecause the wooden cutting board has more resistance. So the answer is: (A)\nQ: Sarah walked through the city and saw a tourist attraction she wanted to visit. She had\nseveral blocks to go to get to it, and the attraction looked very small. As she got close to it\nthough, it towered over her. This is because when she was close to it the attraction looked\n(A) much bigger (B) much smaller\nA: When something is closer, it looks bigger. Thus, the attraction looked much bigger when\nshe was close to it. So the answer is: (A)\non the initially available data (since we heavily depend upon silver standard rationales generated\nby GPT-3 to give our model a warm start). Our method is also dependent upon the mathematical\nrewards that we use: as we discuss in Section 5, this is a very new and active area of research, and\nwe as a research community are still figuring out what properties we need, and how to efficiently\nimplement a good mathematical metric for them.\n23\nTable 13: The complete prompt of rationalization for OPENBOOKQA. Demonstration examples are\ncollected from Wang et al., 2022\nQ: The sun is responsible for\n(a) puppies learning new tricks (b) children growing up and getting old (c) flowers wilting in a\nvase (d) plants sprouting, blooming and wilting\nA: A plant requires sunlight for photosynthesis, which accumulates resources required for\nsprouting, blooming, and wilting. So the answer is: (d)\nQ: When standing miles away from Mount Rushmore\n(a) the mountains seem very close (b) the mountains are boring (c) the mountains look the\nsame as from up close (d) the mountains seem smaller than in photographs\nA: When an object is far away, it takes up less of your field of view, and so seems smaller than\nin the photographs. So the answer is: (d)\nQ: When food is reduced in the stomach\n(a) the mind needs time to digest (b) take a second to digest what I said (c) nutrients are being\ndeconstructed (d) reader\u2019s digest is a body of works\nA: The stomach is part of the digestive system. The breaking down of food into nutrients oc-\ncurs in the digestive system. So the answer is: (c)\nQ: Poison causes harm to which of the following?\n(a) a Tree (b) a robot (c) a house (d) a car\nA: A tree is a living thing. Poison causes harm to living things. So the answer is: (a)\nQ: A magnet will stick to\n(a) a belt buckle (b) a wooden table (c) a plastic cup (d) a paper plate\nA: A belt buckle is made of metal. If a magnet is attracted to a metal, then that magnet will\nstick to that metal. So the answer is: (a)\nQ: Deer are less safe in the woods because wolves\n(a) have fur (b) howl (c) have claws (d) have tails\nA: Claws are used by wolves to catch prey like deer. So the answer is: (c)\nQ: An electric car causes\n(a) more CO2 emissions (b) equal CO2 emissions (c) electric emissions (d) less CO2 emissions\nA: An electric car uses less gasoline than a regular car and thus causes less CO2 emissions. So\nthe answer is: (d)\n24\nTable 14: The complete prompt of rationalization for NumerSense. Demonstration examples are\ncollected from Liu et al., 2022\nQ: penguins have <mask> wings.\n(A) no (B) zero (C) one (D) two (E) three (F) four (G) five (H) six (I) seven (J) eight (K) nine\n(L) ten\nA: Birds have two wings. Penguin is a kind of bird. So the answer is (D).\nQ: a parallelogram has <mask> sides.\n(A) no (B) zero (C) one (D) two (E) three (F) four (G) five (H) six (I) seven (J) eight (K) nine\n(L) ten\nA: A rectangular is a parallelogram. A square is a parallelogram. So the answer is (F).\nQ: there are <mask> feet in a yard.\n(A) no (B) zero (C) one (D) two (E) three (F) four (G) five (H) six (I) seven (J) eight (K) nine\n(L) ten\nA: A yard is three feet. So the answer is (E).\nQ: water can exist in <mask> states.\n(A) no (B) zero (C) one (D) two (E) three (F) four (G) five (H) six (I) seven (J) eight (K) nine\n(L) ten\nA: There states for matter are solid, liquid, and gas. So the answer is (E).\nQ: a typical human being has <mask> limbs.\n(A) no (B) zero (C) one (D) two (E) three (F) four (G) five (H) six (I) seven (J) eight (K) nine\n(L) ten\nA: Human has two arms and two legs. So the answer is (F)\n25\nTable 15: The complete prompt of rationalization for QASC. Demonstration examples are collected\nfrom Wang et al., 2023a\nQ: How do you reduce pollution? (A) igniting fuel and oxidiser (B) transportation technology\n(C) wasting (D) not recycling (E) burning fossil fuels (F) converting electricity to heat (G)\nwater conservation (H) using less resources\nA: Conserving resources has a positive impact on the environment. Use of resources affects\nthe environment such as pollution. So the answer is: (H)\nQ: what will move to another area if their habitat will no longer support them? (A) density (B)\nBirds (C) squids (D) humans (E) clouds (F) gravity (G) cows (H) Whales\nA: If a habitat can no longer support animals then those animals will move to another area.\nCows are social animals. So the answer is: (G)\nQ: With the exception of allergies, what may cause a person to seek medical attention? (A)\nContact with latex (B) a tree falling (C) Organs within the body. (D) Contact with baby chicks\n(E) prolactin release (F) Contact with peanut butter (G) hypothyroidism (H) Contact with mi-\ncroorganisms\nA: Microorganisms can cause infections. Infections usually require medical treatment. So the\nanswer is: (H)\nQ: Lavender can induce (A) healing (B) energy (C) hormones (D) mutations (E) Heart rate (F)\ngrowth (G) symptoms (H) warmth\nA: Healing requires rest. Lavender induces restful sleep. So the answer is: (A)\nQ: what state is a liquid in when frozen? (A) vapor (B) dense (C) gas (D) cooled (E) steam (F)\nsolid (G) boiling (H) cold\nA: Freezing means changing from a liquid into a solid by reducing heat energy. Liquids freeze\nwhen they change to the solid state. So the answer is: (F)\nQ: what unites to form a diploid zygote? (A) plant reproduction (B) Most plants (C) orchids\n(D) sperm and ova (E) salt and pepper (F) predator and prey (G) honeybees (H) diploids and\nzygotes\nA: Gametes then unite in fertilization and form a diploid zygote. Collectively, the sperm and\nthe ova are also referred to as gametes. So the answer is: (D)\nQ: What absorbs all visible light? (A) apples (B) coal (C) Green (D) coral (E) skin (F) bamboo\n(G) glass (H) eyes\nA: If an object is black then that object absorbs all visible light. Light grains are quartz, Black\ngrains are coal. So the answer is: (B)\n26\nFigure 6: MTurk Instructions. We show these instructions to turkers, along with a sample HIT,\nand more examples that contain special cases of each of the annotation questions.\n27\nFigure 7: MTurk Template. Given a question and two explanations, we ask annotators to choose\nwhich explanation they prefer, followed by questions about their plausibility and consistency.\n28\n"
  },
  {
    "title": "Consistent4D: Consistent 360\u00b0 Dynamic Object Generation from Monocular Video",
    "link": "https://arxiv.org/pdf/2311.02848.pdf",
    "upvote": "1",
    "text": "CONSISTENT4D: CONSISTENT 360\u00b0 DYNAMIC OB-\nJECT GENERATION FROM MONOCULAR VIDEO\nYanqin Jiang1, Li Zhang3, Jin Gao1, Weimin Hu1, Yao Yao2 \f\n1CASIA, 2Nanjin University, 3Fudan University\njiangyanqin2021@ia.ac.cn, lizhangfd@fudan.edu.cn,\n{jin.gao, wmhu}@nlpr.ia.ac.cn, yaoyao@nju.edu.cn\nABSTRACT\nIn this paper, we present Consistent4D, a novel approach for generating 4D dy-\nnamic objects from uncalibrated monocular videos. Uniquely, we cast the 360-\ndegree dynamic object reconstruction as a 4D generation problem, eliminating\nthe need for tedious multi-view data collection and camera calibration. This is\nachieved by leveraging the object-level 3D-aware image diffusion model as the\nprimary supervision signal for training Dynamic Neural Radiance Fields (DyN-\neRF). Specifically, we propose a Cascade DyNeRF to facilitate stable convergence\nand temporal continuity under the supervision signal which is discrete along the\ntime axis. To achieve spatial and temporal consistency, we further introduce an\nInterpolation-driven Consistency Loss. It is optimized by minimizing the dis-\ncrepancy between rendered frames from DyNeRF and interpolated frames from\na pre-trained video interpolation model. Extensive experiments show that our\nConsistent4D can perform competitively to prior art alternatives, opening up new\npossibilities for 4D dynamic object generation from monocular videos, whilst\nalso demonstrating advantage for conventional text-to-3D generation tasks. Our\nproject page is https://consistent4d.github.io/.\nFigure 1: Video-to-4D results achieved by our method. We show the renderings of 2 objects at 2\nviewpoints and 3 timestamps.\n1\nINTRODUCTION\nPerceiving dynamic 3D information from visual observations is one of the fundamental yet chal-\nlenging problems in computer vision, which is the key to a broad range of downstream applica-\ntions e.g., virtual content creation, autonomous driving simulation, and medical image analysis.\nHowever, due to the high complexity nature of dynamic 3D signals, it is rather difficult to recover\nsuch information from a single monocular video observation. As a result, existing dynamic object\nreconstruction approaches usually take synchronized multi-view videos as inputs (Li et al.,\n2022b;a; Shao et al., 2023), or rely on training data containing effective multi-view cues (e.g., tele-\nporting cameras or quasi-static scenes (Li et al., 2021; Pumarola et al., 2021; Park et al., 2021b;a)).\nHowever, current reconstruction approaches often fail in reconstructing regions that were not ob-\nserved in input sequences (Gao et al., 2022a). Moreover, multi-view data capturing requires syn-\nchronized camera rigs and meticulous calibrations, which inevitably limit the methods to potential\nreal-world applications.\n1\narXiv:2311.02848v1  [cs.CV]  6 Nov 2023\nOn the other hand, given only a video clip of a dynamic object, humans are capable of depicting\nthe appearance, geometry, and movement of the object. This is achieved by prior knowledge of\nvisual appearance accumulated through human life. In contrast to the multi-view video setting, our\napproach favors such simple and practical input settings of a static monocular video. The static\nmonocular video offers several advantages: ease of collection for handheld cameras, minimizing the\nrisk of motion blur due to camera movement, and obviating the need for camera parameter estima-\ntion. As a static monocular video does not provide effective multi-view information for reconstruc-\ntion, we instead opt for the generation approaches which demonstrates diminished reliance on\nmulti-view information.\nIn this work, we present a novel video-to-4D generation approach termed Consistent4D. In this ap-\nproach, we represent dynamic objects through a specially designed Cascade DyNeRF and leverage\na pre-trained 2D diffusion model to regulate the DyNeRF optimization, inspired by recent advance-\nments in text-to-3D (Poole et al., 2023; Wang et al., 2022; Chen et al., 2023; Wang et al., 2023)\nand image-to-3D (Deng et al., 2022; Tang et al., 2023; Melas-Kyriazi et al., 2023) techniques. The\nchallenge is to achieve both spatial and temporal consistency. To tackle this challenge, we introduce\nan Interpolation-driven Consistency Loss (ICL), which leverages a pre-trained video interpolation\nmodel to provide spatiotemporally coherent supervision signals. Notably, the ICL loss not only\nenhances consistency in 4D generation but also mitigates multi-face issues in 3D generation. Fur-\nthermore, we train a lightweight video enhancer to enhance the video generated from dynamic NeRF\nas a post-processing step.\nWe have extensively evaluated our approach on both synthetic videos rendered from animated 3D\nmodel and in-the-wild videos collected from the Internet. To summarize, the contribution of this\nwork includes:\n\u2022 We propose a video-to-4D framework for dynamic object generation from a statically cap-\ntured monocular video. A specially designed Cascade DyNeRF is applied to represent the\nobject, optimized through the Score Distillation Sampling (SDS) loss by a pre-trained 2D\ndiffusion model. Moreover, for the comprehensiveness of the framework, we train a video\nenhancer to improve the rendering of 4D object as a post-processing step.\n\u2022 To address the challenge of maintaining temporal and spatial consistency in 4D generation\ntask, we introduce a novel Interpolation-driven Consistency Loss (ICL). The proposed ICL\nloss can significantly improve consistency, e.g., multi-face problem, in both video-to-4D\nand text-to-3D generation tasks.\n\u2022 We extensively evaluate our method on both synthetic and in-the-wild videos collected\nfrom the Internet, showing promising results for the new task of video-to-4D generation.\n2\nRELATED WORK\n3D Generation\n3D generation aims to generate 3D content conditionally or unconditionally. Early\nworks mainly make use of GAN, i.e., generate category-specific 3D objects or scenes from random\nnoise after learning the category prior (Schwarz et al., 2020; Chan et al., 2021; Gao et al., 2022b).\nRecently, general-purpose 3D generation has been enabled by text-to-image diffusion model pre-\ntrained on Internet-scale data, and it also becomes more controllable, e.g., controlled by text prompt\nor single image. The pioneer work of text-to-3D is DreamFusion (Poole et al., 2023), which proposes\nScore Distillation Sampling (SDS) loss to leverage the image diffusion model for neural radiance\nfield (NeRF) training. The following works (Lin et al., 2023; Chen et al., 2023; Wang et al., 2023)\nfurther enhance the visual quality of the generated object by using mesh representation, Variational\nScore Distillation, etc. However, the challenging problem, multi-face Janus problem, has not been\naddressed in the above works. Image is another popular condition for 3D generation. Different from\n3D reconstruction, which focuses on the reconstruction of visible regions from multi-view images,\n3D generation usually has only a single image and relies much on the image diffusion model to\ngenerate invisible regions of the object (Melas-Kyriazi et al., 2023; Tang et al., 2023; Liu et al.,\n2023). So many works simply translate the image to words using Lora and then exploit text-to-\n3D methods (Melas-Kyriazi et al., 2023; Tang et al., 2023; Seo et al., 2023). One exception is\nZero123 (Liu et al., 2023), which trains a 3D-aware image-to-image model using multi-view data\nand could generate a novel view of the object in the input image directly. Benefiting from multi-view\ndata training, the multi-face Janus problem has been alleviated to some extent in Zero123.\n2\n4D Reconstruction\n4D reconstruction, aka dynamic scene reconstruction, is a challenging task.\nSome early works focus on object-level reconstruction and adopt parametric shape models (Loper\net al., 2023; Vo et al., 2020) as representation. In recent years, dynamic neural radiance field become\npopular, and convenient dynamic scene reconstruction is enabled. These works can be classified\ninto two categories: a deformed scene is directly modeled as a NeRF in canonical space with a time-\ndependent deformation (Pumarola et al., 2021; Park et al., 2021a;b; Wu et al., 2022b; Tretschk et al.,\n2021) or time-varying NeRF in the world space (Gao et al., 2021; Li et al., 2021; Xian et al., 2021;\nFridovich-Keil et al., 2023; Cao & Johnson, 2023). Some of them require multi-view synchronized\ndata to reconstruct dynamic scenes, however, data collection and calibration is not convenient (Li\net al., 2022b; Shao et al., 2023). So, reconstruction from monocular videos gain attention. However,\nthose monocular methods either require teleporting camera or quaic-static scenes (Pumarola et al.,\n2021; Park et al., 2021a;b), which are not representative of daily life scenarios (Gao et al., 2022a).\n4D Generation\n4D generation extends 3D generation to space+time domain and thus is more\nchallenging. Early works are mainly category-specific and adopt parametric shape models (Zuffi\net al., 2017; 2018; Vo et al., 2020; Kocabas et al., 2020) as representation. They usually take images\nor videos as conditions and need category-specific 3D templates or per-category training from a\ncollection of images or videos (Ren et al., 2021; Wu et al., 2021; Yang et al., 2022; Wu et al.,\n2022a). Recently, one zero-shot category-agnostic work, text-to-4D (Singer et al., 2023), achieves\ngeneral-purpose dynamic scene generation from text prompt. It follows DreamFusion (Poole et al.,\n2023) and extends it to the time domain by proposing a three-stage training framework. However,\nthe quality of generated scenes is limited due to low-quality video diffusion models.\n3\nPRELIMINARIES\n3.1\nSCORE DISTILLATION SAMPLING FOR IMAGE-TO-3D\nScore Distillation Sampling (SDS) is first proposed in DreamFusion (Poole et al., 2023) for text-to-\n3D tasks. It enables the use of a 2D text-to-image diffusion model as a prior for optimization of a\nNeRF. We denote the NeRF parameters as \u03b8, text-to-image diffusion model as \u03d5, text prompt as \u03c1,\nthe rendering image and the reimage as x and z, the SDS loss is defined as:\n\u2207\u03b8LSDS (\u03d5, x) = E\u03c4,\u03f5\n\u0014\n\u03c9(t) (\u02c6\u03f5\u03b8 (zt; \u03c1, \u03c4) \u2212 \u03f5) \u2202x\n\u2202\u03b8\n\u0015\n,\n(1)\nwhere \u03c4 is timestamps in diffusion process, \u03f5 denotes noise, and \u03c9 is a weighted function. Intuitively,\nthis loss perturbs x with a random amount of noise corresponding to the timestep \u03c4, and estimates\nan update direction that follows the score function of the diffusion model to move to a higher density\nregion.\nBesides text-to-3D, SDS is also widely used in image-to-3D tasks. Zero123 (Liu et al., 2023) is one\nprominent representative. It proposes a viewpoint-conditioned image-to-image translation diffusion\nmodel fine-tuned from Stable Diffusion (Rombach et al., 2022), and exploits this 3D-aware image\ndiffusion model to optimize a NeRF using SDS loss. This image diffusion model takes one image,\ndenoted by Iin, and relative camera extrinsic between target view and input view, denoted by (R, T),\nas the input, and outputs the target view image. Compared with the original text-to-image diffusion\nmodel, text prompt in Equation 1 is not required in this model cause the authors translate the input\nimage and the relative camera extrinsic to text embeddings. Then Equation 1 could be re-written as:\n\u2207\u03b8LSDS (\u03d5, x) = E\u03c4,\u03f5\n\u0014\n\u03c9(t) (\u02c6\u03f5\u03b8 (zt; Iin, R, T, \u03c4) \u2212 \u03f5) \u2202x\n\u2202\u03b8\n\u0015\n,\n(2)\n3.2\nK-PLANES\nK-planes (Fridovich-Keil et al., 2023) is a simple and effective dynamic NeRF method which fac-\ntorizes a dynamic 3D volume into six feature planes (i.e., hex-plane), denoted as P = {Po}, where\no \u2208 {xy, yz, xz, xt, yt, zt}. The first three planes correspond to spatial dimensions, while the last\nthree planes capture spatiotemporal variations. Each of the planes is structured as a M \u00d7 M \u00d7 F\ntensor in the memory, where M represents the size of the plane and F is the feature size that en-\ncodes scene density and color information. Let t denote the timestamp of a video clip, given a point\n3\nDiffusion \nModel\nRendering\nSDS loss\nVideo\nInterpolation\nModule\nTemporal consistency\nSpatial consistency\nDiscriminator\nVideo Enhancer\nSR\nModel\nReal\nFake\nEnhanced \nvideo\nInput \nvideo\nImage loss\nTrainable\nFrozen\nForward\nBackward\n(a) Cascade DyNeRF\nXY\nYZ\nXZ\nXT\nYT\nZT\nXY\nYZ\nXZ\nXT\nYT\nZT\n(b) SDS and Interpolation Consistency Optimization\n(c) Post Video Enhancer\nFigure 2: Schematic illustration of Consistent4D. The framework consists of the optimization of a\nCascade DyNeRF and the training of a post video enhancer. The Cascade DyNeRF, which adopts\na residual learning strategy, is supervised by SDS loss from an image-to-image diffusion model.\nParticularly, a novel Interpolation-driven Consistency Loss is proposed to compensate for the spa-\ntiotemporal inconsistency brought by the image diffusion model. For post-processing, we train a\nlightweight cross-frame video enhancer using GAN to further improve the quality of the video ren-\ndered from DyNeRF.\np = (x, y, z, t) in the 4D space, we normalize the coordinate to the range [0, M) and subsequently\nproject it onto the six planes using the equation f(p)o = Po(\u03b9o(p)), where \u03b9o is the projection from\na space point p to a pixel on the o\u2019th plane. The plane feature f(p)o is extracted via bilinear in-\nterpolation. The six plane features are combined using the Hadamard product (i.e., element-wise\nmultiplication), to produce a final feature vector as follows:\nf(p) =\nY\no\u2208{xy,yz,xz,xt,yt,zt}\nf(p)o,\n(3)\nThen, the color and density of p is calculated as c(p) = c(f(p)) and d(p) = d(f(p)), where c and d\ndenotes mlps for color and density, respectively.\n4\nMETHOD\nIn this work, we target to generate a 360\u25e6 dynamic object from a statically captured monocular\nvideo. To achieve this goal, we develop a framework consisting of a DyNeRF and a video enhancer,\nsupervised by the pre-trained 2D diffusion model in Zero123 and a GAN, respectively. As shown in\nFigure 2, we first train a specially designed cascade DyNeRF using SDS loss and image reconstruc-\ntion loss. To guarantee spatial and temporal consistency, we propose a novel Interpolation-driven\nConsistency Loss as the extra regularization for the DyNeRF. For post-processing, we apply GAN\nto train a lightweight video enhancer, inspired by pix2pix (Isola et al., 2017).To summarize, taking\nan uncalibrated monocular video as the input, we obtain a DyNeRF from which we can render 360\u25e6\nview of the dynamic object, and the rendered results can be further enhanced by the video enhancer.\nIn the following sections, we will first introduce our design of the Cascade DyNeRF, and then\nillustrate the Interpolation-driven Consistency loss. At last, we will detail the video enhancer.\n4.1\nCASCADE DYNERF\nExisting DyNeRF methods mainly assume the supervision signals are temporally coherent, however,\nthis assumption does not hold in our task since our main supervision signal is from an image dif-\nfusion model. In order to minimize the impact of temporal discontinuity in the supervision signals,\nwe are prone to 4D representations which naturally guarantee a certain level of temporal continuity.\n4\nTherefore, we build our DyNeRF based on K-planes (Fridovich-Keil et al., 2023) which explots tem-\nporal interpolation, an operator naturally inclined to temporal smoothing. Empirically, reducing the\ntime resolution of spatiotemporal planes helps enhance temporal consistency, however, this results\nin over-smoothed renderings where finer details are lost. In contrast, increasing the time resolution\nleads to crisp renderings, but the continuity of images within the same time series diminishes. To\nachieve both temporal continuity and high image quality, we adjust the multi-scale technique in\nK-planes and introduce Cascade DyNeRF.\nLet us denote the scale index by s. In K-planes, multi-scale features are exploited by concatenation\nalong feature dimension, then the color and density could be calculated as:\nc(p) = c(concat({f(p)s)}S\ns=1), d(p) = d(concat({f(p)s)}S\ns=1),\n(4)\nwhere S is the number of scales. In our setting, the experiment results show simple concatenation\nis hard to balance between image quality and temporal consistency. So we propose to leverage the\ncascade architecture, which is commonly used in object detection algorithms (Cai & Vasconcelos,\n2018; Carion et al., 2020), to first output a coarse yet temporally coherent dynamic object from low-\nresolution planes and then let the high-resolution planes learn the residual between the coarse and\nfine results. That is, the color and density of the object after scale s is:\nc(p)s =\ns\nX\nk=1\nc(f(p)k), d(p)s =\ns\nX\nk=1\nd(f(p)k),\n(5)\nwhere k indicates the scale index. Note that losses are applied to the rendering results of each scale to\nguarantee that planes with higher resolution learn the residual between results from previous scales\nand the target object. In this way, we can improve temporal consistency without sacrificing much\nobject quality. But Cascade DyNeRF alone is not enough for spatiotemporal consistency, so we\nresort to extra regularization, please see in the next section.\n4.2\nINTERPOLATION-DRIVEN CONSISTENCY LOSS\nVideo generation methods usually train an inter-frame interpolation module to enhance the temporal\nconsistency between keyframes (Ho et al., 2022; Zhou et al., 2022; Blattmann et al., 2023). Inspired\nby this, we exploit a pre-trained light-weighted video interpolation model and propose Interpolation-\ndriven Consistency Loss to enhance the spatiotemporal consistency of the 4D generation.\nThe interpolation model adopted in this work is RIFE (Huang et al., 2022), which takes a pair of\nconsecutive images as well as the interpolation ratio \u03b3 (0 < \u03b3 < 1) as the input, and outputs the\ninterpolated image. In our case, we first render a batch of images that are either spatially continuous\nor temporally continuous, denoted by {x}J\nj=1, where J is the number of images in a batch. Let\nus denote the video interpolation model as \u03c8, the interpolated image as \u02c6x, then we calculate the\nInterpolation-driven Consistency Loss as:\n\u02c6xj = \u03c8(x1, xJ, \u03b3j),\nLICL =\nJ\u22121\nX\nj=2\n\u2225xj \u2212 \u02c6xj\u22252,\n(6)\nwhere \u03b3j = j\u22121\nJ\u22121, and 2 \u2264 j \u2264 J \u2212 1.\nThis simple yet effective loss enhances the continuity between frames thus improving the spatiotem-\nporal consistency in dynamic object generation by a large margin. Moreover, we find the spatial ver-\nsion of this loss alleviates the multi-face problem in 3D generation tasks as well. Please refer to the\nexperiment sections to see quantitative and qualitative results. The Interpolation-driven Consistency\nLoss and some other regularization losses are added with SDS loss in Equation 2, details of which\ncan be found in the experiment section.\n4.3\nCROSS-FRAME VIDEO ENHANCER\nSometimes image sequence rendered from the optimized DyNeRF suffers from artifacts, such as\nblurry edges, small floaters, and insufficient smoothness, especially when the object motion is abrupt\n5\nPistol\nGuppie\nCrocodile\nMonster\nSkull\nTrump\nAurorus\nAverage\nLPIPS\nCLIP\nLPIPS\nCLIP\nLPIPS\nCLIP\nLPIPS\nCLIP\nLPIPS\nCLIP\nLPIPS\nCLIP\nLPIPS\nCLIP\nLPIPS\nCLIP\nD-NeRF\n0.52\n0.66\n0.32\n0.76\n0.54\n0.61\n0.52\n0.79\n0.53\n0.72\n0.55\n0.60\n0.56\n0.66\n0.51\n0.68\nK-planes\n0.40\n0.74\n0.29\n0.75\n0.19\n0.75\n0.47\n0.73\n0.41\n0.72\n051\n0.66\n0.37\n0.67\n0.38\n0.72\nours\n0.10\n0.90\n0.12\n0.90\n0.12\n0.82\n0.18\n0.90\n0.17\n0.88\n0.23\n0.85\n0.17\n0.85\n0.16\n0.87\nTable 1: Video-to-4D quantitative comparison.\nor complex. To further improve the quality of rendered videos, we design a lightweight video en-\nhancer and optimize it via GAN, following pix2pix (Isola et al., 2017). The real images are obtained\nwith image-to-image technique (Meng et al., 2021) using a super-resolution diffusion model, and\nthe fake images are the rendered ones.\nTo better exploit video information, We add cross-frame attention to the UNet architecture in\npix2pix, i.e., each frame will query information from two adjacent frames. We believe this could\nenable better consistency and image quality. Denote the feature map before and after cross-frame-\nattention as F and F\u2032\nj, we have:\nF \u2032\nj = Attention(Qj, Kj, Vj),\nQj = flatten(Fj), Kj = Vj = flatten(concat(Fj\u22121, Fj+1),\n(7)\nwhere Q, K and V denotes query, key, and value in attention mechanism, and concat denotes the\nconcatenation along the width dimension.\nLoss for the generator and discriminator are the same as pix2pix.\n5\nEXPERIMENT\nWe have conducted extensive experiments to evaluate the proposed Consistent4D generator using\nboth synthetic data and in-the-wild data. The experimental setup, comparison with dynamic NeRF\nbaselines, and ablations are provided in the following sections.\n5.1\nIMPLEMENTATION DETAILS\nData Preparation For each input video, we initially segment the foreground object utilizing\nSAM (Kirillov et al., 2023) and subsequently sample 32 frames uniformly. The majority of the\ninput videos span approximately 2 seconds, with some variations extending to around 1 second or\nexceeding 5 seconds. For the ablation study of video sampling, please refer to the appendix A.3.\nTraining During SDS and interpolation consistency optimization, we utilize zero123-xl trained by\nDeitke et al. (2023) as the diffusion model for SDS loss. For Cascade DyNeRF, we set s = 2, i.e., we\nhave coarse-level and fine-level DyNeRFs.The spatial and temporal resolution of Cascade DyNeRF\nare configured to 50 and 8 for coarse-level, and 100 and 16 for fine-level, respectively. We first\ntrain DyNeRF with batch size 4 and resolution 64 for 5000 iterations. Then we decrease the batch\nsize to 1 and increase the resolution to 256 for the next 5000 iteration training. ICL is employed\nin the initial 5000 iterations with a probability of 25%, and we sample consecutive temporal frames\nat intervals of one frame and sample consecutive spatial frames at angular intervals of 5\u25e6-15\u25e6 in\nazimuth. SDS loss weight is set as 0.01 and reconstruction loss weight is set as 500. In addition\nto SDS and ICL, we also apply foreground mask loss, normal orientation loss, and 3D smoothness\nloss. The learning rate is set as 0.1 and the optimizer is Adam. In the post-video enhancing stage,\nwe train the video enhancer with a modified Unet architecture. The learning rate is set as 0.002, the\nbatch size is 16, and the training epoch is 200. The main optimization stage and the video enhancing\nstage cost about 2.5 hours and 15 minutes on a single V100 GPU. For details, please refer to the\nappendix A.4.\n5.2\nCOMPARISONS WITH OTHER METHODS\nTo date, few methods have been developed for 4D generation utilizing video obtained from a static\ncamera, so we only manage to compare our method with D-NeRF (Pumarola et al., 2021) and K-\nplanes (Fridovich-Keil et al., 2023).\n6\nInput video\nD-NeRF\nK-planes\nOurs\nGuppie\nAurorus\nTrump\nt = t1\nt = t2\nt = t1\nt = t2\nt = t1\nt = t2\nFigure 3: Comparison with dynamic NeRF methods. We render each dynamic object from a novel\nview at two timestamps.\nQuantitative Results To quantitatively evaluate the proposed video-4D generation method, we se-\nlect and download seven animated models, namely Pistol, Guppie, Crocodie, Monster, Skull, Trump,\nAurorus, from Sketchfab (ske, 2023) and render the multi-view videos by ourselves, as shown in Fig-\nure 3 and appendix A.2. We render one input view for scene generation and 4 testing views for our\nevaluation. The per-frame LPIPS (Zhang et al., 2018) score and the CLIP (Radford et al., 2021)\nsimilarity are computed between testing and rendered videos. We report the scores averaged over\nthe four testing views in Table. 1. Note that the commonly used PSNR and SSIM scores were not\napplied in our scenario as the pixel- and patch-wise similarities are too sensitive to the reconstruc-\ntion difference, which does not align with the generation quality usually perceived by humans. As\nshown in Table 1, our dynamic 3D generation produces the best quantitative results over the other\ntwo methods on both the LPIPS and CLIP scores, which well aligns with the qualitative comparisons\nshown in Figure 3.\nQualitative Results The outcomes of our method and those of dyNeRFs are illustrated in Figure 3.\nIt is observable that both D-NeRF and HyperNeRF methods struggle to achieve satisfactory results\nin novel views, owing to the absence of multi-view information in the training data. In contrast,\nleveraging the strengths of the generation model, our method proficiently generates a 360\u25e6 repre-\nsentation of the dynamic object. For additional results, please refer to the appendix A.1.\n5.3\nABLATIONS\nWe perform ablation studies for every component within our framework. For clarity, the video\nenhancer is excluded when conducting ablations for SDS and interpolation consistency optimization.\nCascade DyNeRF In Figure 4, we conduct an ablation study for Cascade DyNeRF. Specifically, we\nsubstitute Cascade DyNeRF with the original K-planes architecture, maintaining all other settings\nunchanged. In the absence of the cascade architecture, the training proves to be unstable, occa-\nsionally yielding incomplete or blurry objects, as demonstrated by the first and second objects in\nFigure 4. In some cases, while the model manages to generate a complete object, the moving parts\nof the object lack clarity, exemplified by the leg and beak of the bird. Conversely, the proposed\nCascade DyNeRF exhibits stable training, leading to relatively satisfactory generation results.\n7\nInput video\nOurs K-planes\nOurs Cas-DyNeRF\nNovel view 1\nNovel view 2\nt = t1\nt = t2\nt = t1\nt = t2\nt = t1\nt = t2\nOurs K-planes\nOurs Cas-DyNeRF\nFigure 4: Ablation of Cascade DyNeRF.\nw/o ICL\nw/ ICL\npreference rate(%)\n24.5\n75.5\n(a) Video-to-4D.\nw/o ICL\nw/ ICL\nsuccess rate(%)\n19.3\n28.6\n(b) Text-to-3D.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nUsers\n0\n20\n40\nSuccess Rate (%)\nw/o ICL\nw/ ICL\n(c) Details of Text-to-3D user study. Data is sorted.\nTable 2: User study of Interpolation-driven Consistency Loss.\nInterpolation-driven Consistency Loss The introduction of Interpolation-driven Consistency Loss\n(ICL) stands as a significant contribution of our work. Therefore, we conduct extensive experiments\nto investigate both its advantages and potential limitations. Figure 5a illustrates the ablation of both\nspatial and temporal Interpolation-driven Consistency Loss (ICL) in the video-to-4D task. Without\nICL, the objects generated exhibit spatial and temporal inconsistency, as evidenced by the multi-\nface/foot issue in the blue jay and T-rex, and the unnatural pose of the seagull. Additionally, color\ndiscrepancies, such as the black backside of the corgi, are also noticeable. Employing either spatial\nor temporal ICL mitigates the multi-face issue, and notably, the use of spatial ICL also alleviates the\ncolor defect problem. Utilizing both spatial and temporal ICL concurrently yields superior results.\nWe further perform a user study, depicted in Figure 2a, which includes results w/ and w/o ICL for 20\nobjects. For efficiency in evaluation, cases in which both methods fail are filtered out in this study.\n20 users participate in this evaluation, and the results unveiled a preference for results w/ ICL in\n75% of the cases.\nWe further explore whether ICL could alleviate multi-face problems for text-to-3D tasks. We com-\npared the success rate of DreamFusion implemented w/ and w/o the proposed ICL loss. For the sake\nof fairness and rigor, we collect all prompts related to animals from the official DreamFusion project\npage, totaling 230. 20 users are asked to participate in this non-cherry-pick user study, where\nwe establish three criteria for a successful generation: alignment with the text prompt,\nabsence of multi-face issues, and clarity in geometry and texture. We\nvisualize the statistics in Table 2b and Table 2c. The results show that although users have differ-\n8\nInput video\nTemporal ICL\nSpatial ICL\nICL\nw/o ICL\nt = t1\nt = t2\nt = t1\nt = t2\nt = t1\nt = t2\nt = t1\nt = t2\n(a) Video-to-4D. For each dynamic object, we render it from a novel view for two timestamps with textureless\nrendering for each timestamp. For clarity, we describe the input videos as follows (from left to right): blue jay\npecking, T-rex roaring, corgi smiling, seagull turning around.\na chimpanzee looking through a \ntelescope\nw/o ICL\nw/ ICL\na DSLR photo of a corgi \nstanding up drinking boba\na DSLR photo of a pig \nplaying a drum set\na red eyed tree frog, \nlow poly\n(b) Text-to-3D. For each 3D model, we render it from two views with a textureless rendering for each view and\nremove the background to focus on the actual 3D shape.\nFigure 5: Ablation of Interpolation-driven Consistency Loss.\nent understandings of successful generation, results w/ ICL always outperform results w/o it. For a\ncomprehensive understanding, qualitative comparisons are presented in Figure 5b, which indicates\nthe proposed technique effectively alleviates the multi-face Janus problem and thus promotes the\nsuccess rate. Implementation details about text-to-3D can be found in the appendix A.4, and we also\nanalyze the failure cases and limitations of ICL in A.5.\nCross-frame Video Enhancer In Figure 11 (see in appendix), we show the proposed cross-frame\nvideo enhancer could improve uneven color distribution and smooth out the rough edges, as shown\nin almost all figures, and remove some floaters, as indicated by the cat in the red and green box.\n6\nCONCLUSION\nWe introduce a novel video-to-4D framework, named Consistent4D, aimed at generating 360\u25e6 4D\nobjects from uncalibrated monocular videos captured by a stationary camera. Specifically, we first\noptimize a Cascade DyNeRF which is specially designed to facilitate stable training under the dis-\ncrete supervisory signals from an image-to-image diffusion model. More crucially, we introduce an\nInterpolation-driven Consistency Loss to enhance spatial and temporal consistency, the main chal-\nlenge in this task. For comprehensiveness, we train a lightweight video enhancer to rectify scattered\ncolor discrepancies and eliminate minor floating artifacts, as a post-processing step. Extensive ex-\n9\nperiments conducted on both synthetic and in-the-wild data demonstrate the effectiveness of our\nmethod.\nREFERENCES\nDeepfloyd. https://huggingface.co/DeepFloyd, 2023.\nSketchfab. https://sketchfab.com/3d-models, 2023.\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler,\nand Karsten Kreis.\nAlign your latents: High-resolution video synthesis with latent diffusion\nmodels. In CVPR, 2023.\nZhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In\nCVPR, 2018.\nAng Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. In arXiv preprint,\n2023.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\nEric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic\nimplicit generative adversarial networks for 3d-aware image synthesis. In CVPR, 2021.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and\nappearance for high-quality text-to-3d content creation. In arXiv preprint, 2023.\nMatt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan\nFan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of\n10m+ 3d objects. In arXiv preprint, 2023.\nCongyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir\nAnguelov, et al. Nerdi: Single-view nerf synthesis with language-guided diffusion as general\nimage priors. In arXiv preprint, 2022.\nSara Fridovich-Keil, Giacomo Meanti, Frederik Warburg, Benjamin Recht, and Angjoo Kanazawa.\nK-planes: Explicit radiance fields in space, time, and appearance. In arXiv preprint, 2023.\nChen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic\nmonocular video. In ICCV, 2021.\nHang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dy-\nnamic view synthesis: A reality check. In NeurIPS, 2022a.\nJun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan\nGojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned\nfrom images. In NeurIPS, 2022b.\nYuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-\nHao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified\nframework for 3d content generation. https://github.com/threestudio-project/\nthreestudio, 2023.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition\nvideo generation with diffusion models. In arXiv preprint, 2022.\nZhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate\nflow estimation for video frame interpolation. In ECCV, 2022.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with\nconditional adversarial networks. In CVPR, 2017.\n10\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In arXiv\npreprint, 2023.\nMuhammed Kocabas, Nikos Athanasiou, and Michael J Black. Vibe: Video inference for human\nbody pose and shape estimation. In CVPR, 2020.\nRuilong Li, Julian Tanke, Minh Vo, Michael Zollh\u00a8ofer, J\u00a8urgen Gall, Angjoo Kanazawa, and\nChristoph Lassner. Tava: Template-free animatable volumetric actors. In ECCV, 2022a.\nTianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim,\nTanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video\nsynthesis from multi-view video. In CVPR, 2022b.\nZhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time\nview synthesis of dynamic scenes. In CVPR, 2021.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In CVPR, 2023.\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object. In arXiv preprint, 2023.\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl:\nA skinned multi-person linear model. In Seminal Graphics Papers: Pushing the Boundaries,\nVolume 2, 2023.\nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Realfusion: 360 {\\deg}\nreconstruction of any object from a single image. In arXiv preprint, 2023.\nChenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2021.\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M\nSeitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In CVPR, 2021a.\nKeunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman,\nRicardo Martin-Brualla, and Steven M Seitz. Hypernerf: A higher-dimensional representation for\ntopologically varying neural radiance fields. In arXiv preprint, 2021b.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. In ICLR, 2023.\nAlbert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural\nradiance fields for dynamic scenes. In CVPR, 2021.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021.\nZhongzheng Ren, Xiaoming Zhao, and Alex Schwing. Class-agnostic reconstruction of dynamic\nobjects from videos. In NeurIPS, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, 2022.\nKatja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields\nfor 3d-aware image synthesis. In NeurIPS, 2020.\nJunyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa\nKim, Jiyoung Lee, and Seungryong Kim. Let 2d diffusion model know 3d-consistency for robust\ntext-to-3d generation. In arXiv preprint, 2023.\n11\nRuizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu. Tensor4d:\nEfficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering.\nIn\nCVPR, 2023.\nUriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman\nGoyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation.\nIn arXiv preprint, 2023.\nJunshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-\nit-3d: High-fidelity 3d creation from a single image with diffusion prior. In arXiv preprint, 2023.\nEdgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh\u00a8ofer, Christoph Lassner, and\nChristian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis\nof a dynamic scene from monocular video. In ICCV, 2021.\nDor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, and Pratul P Srini-\nvasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. In CVPR,\n2022.\nMinh Vo, Yaser Sheikh, and Srinivasa G Narasimhan. Spatiotemporal bundle adjustment for dy-\nnamic 3d human reconstruction in the wild. In IEEE TPAMI, 2020.\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian\nchaining: Lifting pretrained 2d diffusion models for 3d generation. In arXiv preprint, 2022.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In\narXiv preprint, 2023.\nShangzhe Wu, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Dove: Learning deformable\n3d objects by watching videos. In arXiv preprint, 2021.\nShangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Magicpony:\nLearning articulated 3d animals in the wild. In arXiv preprint, 2022a.\nTianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D \u03a5e 2\nnerf: Self-supervised decoupling of dynamic and static objects from a monocular video. In arXiv\npreprint, 2022b.\nWenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields\nfor free-viewpoint video. In CVPR, 2021.\nGengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, and Hanbyul Joo.\nBanmo: Building animatable 3d neural models from many casual videos. In CVPR, 2022.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In cvpr, 2018.\nDaquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo:\nEfficient video generation with latent diffusion models. In arXiv preprint, 2022.\nSilvia Zuffi, Angjoo Kanazawa, David W Jacobs, and Michael J Black. 3d menagerie: Modeling\nthe 3d shape and pose of animals. In CVPR, 2017.\nSilvia Zuffi, Angjoo Kanazawa, and Michael J. Black. Lions and tigers and bears: Capturing non-\nrigid, 3d, articulated shape from images. In CVPR, 2018.\n12\nA\nAPPENDIX\nA.1\nADDITIONAL VISUALIZATION RESULTS\nIn Figure 6, we present the result of our method on four in-the-wild videos. For clarity, we describe\nthe input videos as follows: robot dancing, squirrel feeding, toy-spiderman dancing, toy-rabbit\ndeforming. Due to limited space, the reviewers are strongly recommended to watch the video in the\nattached files to see various visualization results.\nA.2\nDATA USED IN VIDEO-TO-4D QUANTITATIVE EVALUATION\nSin three dynamic objects are shown in Figure 7, we only visualize the rest four here, as shown in\nFigure 7. The observation is similar to the results in the main paper.\nWe demonstrate the effectiveness of video enhancer in Figure 11. The analysis can be found in the\nmain paper.\nA.3\nTHE NUMBER OF FRAMES\nFor simplicity, we sample each input video to 32 frames in all experiments. However, we find input\nvideos without sampling sometimes give slightly better results, as shown in Figure 8.\nA.4\nIMPLEMENTATION DETAILS\nLoss Function in Video-to-4D Besides SDS loss LSDS, Interpolation-driven consistency loss\nLICL, we also apply reconstruction loss Lrec and mask loss Lm for the input view. 3D normal\nsmooth loss Ln and orientation loss Lori (Verbin et al., 2022) are also applied. Therefore, the final\nloss is calculated as L = \u03bb1LSDS +\u03bb2LICL +\u03bb3Lrec +\u03bb4Lm +\u03bb5Ln +\u03bb6Lori, where \u03bb1 = 0.1,\n\u03bb2 = 2500, \u03bb3 = 500, \u03bb4 = 50, \u03bb5 = 2.0, and \u03bb6 is initially 1 and increased to 20 linearly until\n5000 iterations. Note that the reconstruction loss and SDS loss are applied alternatively.\nVideo Enhancer For video enhancer architecture, we follow pix2pix (Isola et al., 2017) except\nfor that we modify the unet256 architecture to a light-weighted version, with only three up/down\nlayers and one cross-frame attention layer. The feature dimensions for the unet layers are set as 64,\n128, and 256. Besides, we inject a cross-attention layer in the inner layer of the unet to enable the\ncurrent frame to query information from adjacent frames. For real images, we use DeepFloyd-IF\nstage II (dee, 2023), which is a diffusion model for super-resolution tasks. The input image, i.e., the\nrendered image, is resized to 64 \u00d7 64 and the output resolution is 256 \u00d7 256. The prompt needed by\nthe diffusion model is manually set, i.e., we use the \u201da \u2217\u201d as the prompt, in which \u2217 is the category of\nthe dynamic object. For example, the prompts for dynamic objects in Figure 11 are a bird, a cat, a\nminions. The prompt cloud also be obtained from image or video caption models, or large language\nmodels.\nText-to-3D Details We choose Threestudio built by (Guo et al., 2023) as the codebase since it is\nthe best public implementation we could find. DeepFloy-IF (dee, 2023) is employed as the diffusion\nmodel, and all default tricks in Threestudio are utilized. The hyper-parameters for results w/ and\nw/o ICL, such as batch size and learning rate, are kept consistent between the implementations w/\nand w/o ICL, except for those related to ICL. We train the model for 5000 iterations, the first 1000\niterations with batch size 8 and resolution 64, and the rest 4000 with batch size 2 and resolution 256.\nThe learning rate is 0.01 and the optimizer is Adam, the same as the default setting in Threestudio.\nThe ICL loss is applied in the first 1000 iterations with probability 30% and weight 2000.\nA.5\nFAILURE CASES\nVideo-to-4D Since the video-to-4D task in this paper is very challenging, our method actually has\nmany failure cases. For example, we fail to generate the dynamic object when the motion is complex\nor abrupt, as shown in Figure 9. In Figure 9, the dog\u2019s tail disappears in the second image because\nthe tail is occluded in the input image when t = t2. The frog, which is jumping up and down fast,\ngets blurry when t = t1.\n13\nInput video\nNovel view 1\nt = t1\nt = t2\nt = t3\nt = t1\nt = t2\nt = t3\nt = t1\nt = t2\nt = t3\nNovel view 2\nt = t1\nt = t2\nt = t3\nFigure 6: Visualization results of our method. All four input videos are in-the-wild videos. The\nnovel views presented are 22.5\u25e6 and 112.5\u25e6 away from the input view, respectively. The results of\nour methods include RGB, normal map and depth map (from left to right).\n14\nIntpu video\nD-NeRF\nK-planes\nOurs\nCrocodile\nPistol\nSkull\nMonster\nFigure 7: Data and comparison results for video-to-4D quantitative evaluation.\nt = t1\nt = t2\nw/ sampling\nw/o sampling\nw/o sampling\nw/ sampling\nFigure 8: Ablation of video frame sampling. Videos w/ sampling contain 32 frames. Videos w/o\nsampling contain 72 and 39 frames for Aurorus and minions, respectively. The results of our methods\ninclude RGB and normal map (from left to right).\nt = t1\nt = t2\nt = t1\nt = t2\nInput video\nNovel View\nFigure 9: Video-to-4D failure cases.\na DSLR photo of a shiba inu\nplaying golf wearing tartan \ngolf clothes and hat\na zoomed out DSLR photo of a \nkangaroo sitting on a bench \nplaying the accordion\nw/o ICL\nw/ ICL\nFigure 10: Text-to-3D failure cases.\n15\nText-to-3D When applying ICL in text-to-3D, we find some multi-face cases that could not be\nalleviated, and we show them in Figure 10.\nInput video\nw/o video \nenhancer\nw/ video \nenhancer\nt = t1\nt = t2\nNovel view\nt = t1\nt = t2\nt = t1\nt = t2\nFigure 11: Ablation of Video enhancer. Please zoom in to view the details.\n16\n"
  }
]