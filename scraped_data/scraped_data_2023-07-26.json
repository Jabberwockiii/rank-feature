[
  {
    "title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition",
    "link": "https://arxiv.org/pdf/2307.13269.pdf",
    "upvote": "29",
    "text": "Preprint\nLORAHUB: EFFICIENT CROSS-TASK GENERALIZA-\nTION VIA DYNAMIC LORA COMPOSITION\nChengsong Huang\u2020\u00a7\u2217, Qian Liu\u2020\u2217, Bill Yuchen Lin\u2662\u2217, Tianyu Pang\u2020, Chao Du\u2020, Min Lin\u2020\n\u2020Sea AI Lab, Singapore\n\u00a7Washington University in St. Louis, MO, USA\n\u2662Allen Institute for AI, Seattle, WA, USA\nABSTRACT\nLow-rank adaptations (LoRA) are often employed to fine-tune large language\nmodels (LLMs) for new tasks. This paper investigates LoRA composability for\ncross-task generalization and introduces LoraHub, a simple framework devised\nfor the purposive assembly of LoRA modules trained on diverse given tasks, with\nthe objective of achieving adaptable performance on unseen tasks. With just a few\nexamples from a new task, LoraHub can fluidly combine multiple LoRA modules,\neliminating the need for human expertise and assumptions. Notably, the composi-\ntion requires neither additional model parameters nor gradients. Empirical results\non the Big-Bench Hard benchmark suggest that LoraHub, while not surpassing\nthe performance of in-context learning, offers a notable performance-efficiency\ntrade-off in few-shot scenarios by employing a significantly reduced number of\ntokens per example during inference. Notably, LoraHub establishes a better upper\nbound compared to in-context learning when paired with different demonstration\nexamples, demonstrating its potential for future development. Our vision is to\nestablish a platform for LoRA modules, empowering users to share their trained\nLoRA modules. This collaborative approach facilitates the seamless application of\nLoRA modules to novel tasks, contributing to an adaptive ecosystem. Our code is\navailable at github.com/sail-sg/lorahub, and all the pre-trained LoRA mod-\nules are released at huggingface.co/lorahub.\n1\nINTRODUCTION\nLLM\nLLM\nLORAHUB\nCompose\n\u2026\nLLM\n(b)\n(c)\n27.0\n37.5\n34.7\nUnseen Task Examples\nUnseen Task Test Input\nBIG-Bench Hard Performance\nzero-shot learning (a)\nfew-shot in-context learning (b)\nfew-shot LoraHub learning (c)\n(a)\nFigure 1: The illustration of zero-shot learning, few-shot in-context learning and few-shot LoraHub\nlearning (ours). Note that the Compose procedure is conducted per task rather than per example. Our\nmethod achieves similar inference throughput as zero-shot learning, yet approaches the performance\nof in-context learning on the BIG-Bench Hard (BBH) benchmark.\nRecent progress in natural language processing (NLP) has been largely fueled by large language\nmodels (LLMs) such as OpenAI GPT (Brown et al., 2020), FLAN-T5 (Chung et al., 2022), and\nLLaMA (Touvron et al., 2023). These models demonstrate top-tier performance across different\nNLP tasks. However, their enormous parameter size presents issues regarding computational effi-\nciency and memory usage during fine-tuning. To mitigate these challenges, Low-Rank Adaptation\n(LoRA) (Hu et al., 2022) has emerged as a parameter-efficient fine-tuning technique (Lester et al.,\n2021; He et al., 2022; An et al., 2022). By reducing memory demands and computational costs,\n\u2217The first three authors contributed equally to this work. Correspondence to Qian Liu at liuqian@sea.com.\n1\narXiv:2307.13269v2  [cs.CL]  18 Jan 2024\nPreprint\nit speeds up LLM training. LoRA achieves this by freezing the base model parameters (that is, an\nLLM) and training a lightweight module, which regularly delivers high performance on target tasks.\nWhile prior research has targeted the efficiency enhancement facilitated by LoRA, there is a dearth of\ninvestigation into the inherent modularity and composability of LoRA modules. Typically, previous\nmethods train LoRA modules to specialize in individual tasks. Yet, the intrinsic modularity of LoRA\nmodules presents an intriguing research question: Would it be possible to compose LoRA modules\nto generalize to novel tasks in an efficient manner? In this paper, we tap into the potential of LoRA\nmodularity for broad task generalization, going beyond single-task training to meticulously com-\npose LoRA modules for malleable performance on unknown tasks. Crucially, our method enables\nan automatic assembling of LoRA modules, eliminating dependency on manual design or human\nexpertise. With just a handful of examples from new tasks (e.g., 5), our approach can autonomously\ncompose compatible LoRA modules without human intrusion. We do not make assumptions about\nwhich LoRA modules trained on particular tasks can be combined, allowing for flexibility in amal-\ngamating any modules as long as they conform to the specification (e.g., using the same LLM). As\nour approach leverages several available LoRA modules, we refer to it as LoraHub and denote our\nlearning method as LoraHub learning.\nTo validate the efficiency of our proposed methods, we test our approaches using the widely recog-\nnized BBH benchmark with FLAN-T5 (Chung et al., 2022) serving as the base LLM. The results\nunderline the effectiveness of the LoRA module composition for unfamiliar tasks through a few-\nshot LoraHub learning process. Notably, our methodology achieves an average performance that\nclosely matches that of few-shot in-context learning, while demonstrating a superior upper bound,\nparticularly when using different demonstration examples. Additionally, our method substantially\nreduces the inference cost compared to in-context learning, eliminating the requirement of examples\nas inputs for the LLM. With fewer tokens per example during inference, our method significantly re-\nduces computational overhead and enables faster responses. It aligns with a broader research trend,\nwhere recent studies are actively exploring approaches to reduce the number of input tokens (Zhou\net al., 2023; Ge et al., 2023; Chevalier et al., 2023; Jiang et al., 2023a; Li et al., 2023; Jiang et al.,\n2023b). Our learning procedure is also notable for its computational efficiency, using a gradient-free\napproach to obtain the coefficients of LoRA modules and requiring only a handful of inference steps\nfor unseen tasks. For example, when applied to a new task in BBH, our methodology can deliver\nsuperior performance in less than a minute using a single A100 card.\nImportantly, LoraHub learning can feasibly be accomplished with a CPU-only machine, requiring\nproficiency solely for processing LLM inference. In our pursuit to democratize artificial intelligence,\nwe are taking an important step forward by envisioning the establishment of the LoRA platform. The\nplatform would serve as a marketplace where users can seamlessly share and access well-trained\nLoRA modules for diverse applications. LoRA providers have the flexibility to freely share or sell\ntheir modules on the platform without compromising data privacy. Users, equipped with CPU capa-\nbility, can leverage trained LoRA modules contributed by others through automated distribution and\ncomposition algorithms. This platform not only cultivates a repository of reusable LoRA modules\nwith a myriad of capabilities but also sets the stage for cooperative AI development. It empowers\nthe community to collectively enrich the LLM\u2019s capabilities through dynamic LoRA composition.\n2\nPROBLEM STATEMENT\nLarge Language Models\nWe assume that a large language model M\u03b8 is based on Transformer\narchitecture (Vaswani et al., 2017) and has been pre-trained on a large-scale text corpus. The model\narchitecture can be either encoder-decoder (Raffel et al., 2020) or decoder-only (Brown et al., 2020).\nAlso, M\u03b8 could also have been fine-tuned with a large set of instruction-following datasets such as\nFlan Colleciton (Longpre et al., 2023) and PromptSource (Bach et al., 2022).\nCross-Task Generalization\nIn real-world situations, users often desire an LLM to perform novel\ntasks that it has not encountered before \u2014 an ability widely known as cross-task generalization.\nGenerally, cross-task generalization falls into two categories: zero-shot learning (Mishra et al., 2022;\nSanh et al., 2022; Chung et al., 2022; OpenAI, 2022; Lin et al., 2022), which necessitates no labeled\nexamples of the new task, and few-shot learning (Ye et al., 2021; Min et al., 2022) which demands\na handful of labeled examples. Assume we have N distinct upstream tasks that the LLM has been\ntrained on, denoted as T = {T1, ..., TN}. Our paper primarily focuses on the latter category, where\n2\nPreprint\nCOMPOSE\nLoRA Tuning on Upstream Tasks\nCommonsense \nReasoning\nQuestion\nAnswering\nNatural Language \nInference\nQuestion \nGeneration\n\u2026\nLLM\nEvaluate the result of a \nBoolean expression: \nnot ( True ) and \n( True ) is False\n\ud835\udc64!\n\u00d7\n\ud835\udc64\"\n\u00d7\n+\n+\n+\n\ud835\udc64#\n\u00d7\n\ud835\udc64$\n\u00d7\n+ \u22ef\n\u2192\n. evaluate(\n)\nADAPT\nLoraHub Learning for Unseen Tasks\nBoolean Expressions\nK \u00d7\n\ud835\udc64%\n\u00d7\n\ud835\udc78\nFigure 2: Our method encompasses two stages: the COMPOSE stage and the ADAPT stage. During\nthe COMPOSE stage, existing LoRA modules are integrated into one unified module, employing a\nset of coefficients, denoted as w. In the ADAPT stage, the combined LoRA module is evaluated on a\nfew examples from the unseen task. Subsequently, a gradient-free algorithm is applied to refine w.\nAfter executing K iterations, a highly adapted combined LoRA module is produced, which can be\nincorporated with the LLM to perform the intended task.\nfor an unseen target task T \u2032 /\u2208 T, users can only provide a limited set of labeled examples, Q. Our\naim is to modify the model M\u03b8 to adapt it to task T \u2032 using only Q. An intuitive method would be to\nfine-tune the weights of M\u03b8 based on Q, yielding an updated model M\u03d5 with enhanced performance\non T \u2032. However, this approach is inefficient, time-consuming, and unstable when Q is small.\nLoRA Tuning\nLoRA (Hu et al., 2022), a parameter-efficient fine-tuning method, facilitates the\nadaptation of LLMs using lightweight modules, eliminating the need for fine-tuning the entire\nweights. LoRA tuning involves keeping the original model weights frozen while introducing train-\nable low-rank decomposition matrices as adapter modules into each layer of the model. Compared\nto the base LLM, this module possesses significantly fewer trainable parameters, paving the way\nfor rapid adaptation using minimal examples. As such, LoRA tuning presents a resource-efficient\ntechnique to quickly adapt LLMs for new tasks with restricted training data. However, traditional\nLoRA methods primarily concentrate on training and testing within the same tasks (Gema et al.,\n2023), rather than venturing into few-shot cross-task generalization.\n3\nMETHODOLOGY\nIn this section, we provide an overview of our proposed method. We then explain the LoRA tuning\nprocedure in detail. Last, we introduce the procedure of our LoraHub learning, which consists of\nthe COMPOSE stage and the ADAPT stage.\n3.1\nMETHOD OVERVIEW\nAs depicted in Figure 2, we initially train LoRA modules on a variety of upstream tasks. Specif-\nically, for N distinct upstream tasks, we separately train N LoRA modules, each represented as\nmi for task Ti \u2208 T. Subsequently, for a new task T \u2032 /\u2208 T, such as Boolean Expressions repre-\nsented in Figure 2, its examples Q are utilized to steer the LoraHub learning process. The LoraHub\nlearning encapsulates two main phases: the COMPOSE phase and the ADAPT phase. In the COM-\nPOSE phase, all available LoRA modules are combined into a single integrated module \u02c6m, using\n{w1, w2, . . . , wN} as coefficients. Each wi is a scalar value that can take on positive or negative\nvalues, and the combination can be done in different ways. During the ADAPT phase, the combined\nLoRA module \u02c6m is amalgamated with the LLM M\u03b8, and its performance on few-shot examples\nfrom the new task T \u2032 is assessed. A gradient-free algorithm is subsequently deployed to update w,\nenhancing \u02c6m\u2019s performance (e.g., loss) on the few-shot examples Q. Finally, after iterating through\nK steps, the optimum performing LoRA module is applied to the LLM M\u03b8, yielding the final LLM\nM\u03d5 = LoRA(M\u03b8, \u02c6m). This serves as an effectively adjusted model for the unseen task T \u2032, which\nwill then be deployed and not updated anymore.\n3\nPreprint\n3.2\nLORA TUNING ON UPSTREAM TASKS\nLoRA effectively minimizes the number of trainable parameters through the process of decomposing\nthe attention weight matrix update of the LLM, denoted as W0 \u2208 Rd\u00d7k, into low-rank matrices. In\nmore specific terms, LoRA exhibits the updated weight matrix in the form W0 + \u03b4W = W0 +\nAB, where A \u2208 Rd\u00d7r and B \u2208 Rr\u00d7k are trainable low-rank matrices with rank r, a dimension\nsignificantly smaller than those of d and k. In this context, the product AB defines the LoRA module\nm, as previously elaborated. By leveraging the low-rank decomposition, LoRA substantially reduces\nthe number of trainable parameters needed to adapt the weights of LLMs duriing fine-tuning.\n3.3\nCOMPOSE: ELEMENT-WISE COMPOSITION OF LORA MODULES\nWithin the COMPOSE stage, we implement an element-wise method to combine LoRA modules.\nThis process integrates the corresponding parameters of the LoRA modules, requiring the modules\nbeing combined to have the same rank r to properly align the structures. Given that mi = AiBi, the\ncombined LoRA module \u02c6m can be obtained by:\n\u02c6m = (w1A1 + w2A2 + \u00b7 \u00b7 \u00b7 + wNAN)(w1B1 + w2B2 + \u00b7 \u00b7 \u00b7 + wNBN).\n(1)\nNotbly, as we show in Sec. 5, combining too many LoRA modules at once can expand the search\nspace exponentially, which may destabilize the LoraHub learning process and prevent optimal per-\nformance. To mitigate this, we employ random selection to prune the candidate space, and more\nadvanced pre-filtering algorithms could be explored in the future.\n3.4\nADAPT: WEIGHT OPTIMIZATION VIA GRADIENT-FREE METHODS\nDuring the ADAPT stage, our goal is to modify the coefficients w to boost the model\u2019s perfor-\nmace on the examples from an unseen task. One might think of using gradient descent to optimize\nw, following standard backpropagation methods. However, this approach demands constructing a\nhypernetwork for all LoRA modules, similar to differentiable architecture search methods (Zhang\net al., 2019). Constructing these hypernetworks demands for substantial GPU memory and time,\nposing a challenge. Given that w consists of a relatively small number of parameters, we opted for\ngradient-free methods for optimization instead of gradient descent.\nInspired by previous work (Sun et al., 2022), we utilize a black-box optimization technique to find\nthe optimal w. The optimization process is steered by the cross-entropy loss, setting the goal to\nlocate the best set {w1, w2, . . . , wN} that reduces the loss L on the few-shot examples Q. Further-\nmore, we incorporate L1 regularization to penalize the sum of the absolute values of w, helping\nto prevent obtaining extreme values. Consequently, the final objective of LoraHub is to minimize\nL + \u03b1 \u00b7 PN\ni=1 |wi|, where \u03b1 serves as a hyperparameter.\nIn terms of the gradient-free method, we leverage Shiwa, a combinatorial optimization approach (Liu\net al., 2020). Shiwa offers a variety of algorithms and chooses the most suitable optimization algo-\nrithm for different circumstances. In most of the forthcoming experimental setups, we primarily\nemploy the Covariance Matrix Adaptive Evolution Strategies (CMA-ES) (Hansen & Ostermeier,\n1996). CMA-ES, as a stochastic and population-based optimization algorithm, offers versatility\nin addressing a broad spectrum of optimization challenges. It dynamically adjusts a search distri-\nbution, which is defined by a covariance matrix. During each iteration, CMA-ES systematically\nupdates both the mean and covariance of this distribution to optimize the target function. In our ap-\nplication, we employ this algorithm to mold the search space for w. Ultimately, we use it to identify\nthe optimal w by evaluating their performance on the few-shot examples from an unseen task.\n4\nEXPERIMENTAL RESULTS\nIn this section, we provide details on our main experiments. First, we give an overview of the\nexperimental setup and implementation details. Next, we present our findings along with the results.\n4\nPreprint\n4.1\nEXPERIMENTAL SETUP\nLarge Language Model\nIn our main experiments, we employ FLAN-T5 (Chung et al., 2022),\nparticularly FLAN-T5-large, as the base LLM. The model has shown impressive abilities to perform\nzero-shot and few-shot learning.\nCandidate LoRA Modules\nOur methodology requires a compendium of LoRA modules trained\non preceding tasks. For parity with FLAN, we adopt the tasks utilized to instruct FLAN-T5, thereby\nincorporating nearly 200 distinct tasks and their corresponding instructions 1. Following this, we\ntrained several LoRA modules as potential candidates. During each experimental sequence, we\nrandomly select 20 LoRA modules from them as the candidate for our LoraHub learning.\nDataset and evaluation\nOur method is evaluated using the Big-Bench Hard (BBH) benchmark, a\nwell-established standard that consists of multiple-choice questions from a variety of domains. The\nbenchmark consists of 27 different tasks, which are regarded to be challenging for language models.\nFor all tasks, we employ the exact match (EM) as our evaluation metric.\nBaseline Setup\nTo enhance the demonstration of our method\u2019s performance, we expanded\nour comparisons beyond the zero-shot and in-context learning settings.\nWe specifically chose\nthree representative gradient-based methods for comparison: full fine-tuning (FFT), LoRA tuning\n(LoRA) (Hu et al., 2022), and IA3 fine-tuning (IA3) (Liu et al., 2022). For all gradient-based meth-\nods, for a fair comparsion, we train for 40 epochs on the same three runs of 5 examples employed\nin our methods. In the case of FFT, a learning rate of 3e-5 is employed, whereas for IA3 and LoRA,\nwe adopt a learning rate of 2e-4. We report the performance of each method on the test set at the end\nof training (averaged over three runs) without any model selection to avoid potential selection bias.\n4.2\nMAIN RESULTS\nAs shown in Table 1, our experimental results demonstarte the superior efficacy of our method in\ncomparison to zero-shot learning while closely resembling the performance of in-context learning\n(ICL) in few-shot scenarios. This observation is derived from an average performance of three runs,\neach leveraging different few-shot examples. Importantly, our model utilizes an equivalent number\nof tokens as the zero-shot method, notably fewer than the count used by ICL. Although occasional\nperformance fluctuations, our method consistently outperforms zero-shot learning in most tasks. In\nthe era of LLMs, the input length is directly proportional to the inference cost, and thus LoraHub\u2019s\nability to economize on input tokens while approaching the peak performance grows increasingly\nsignificant. Moreover, as shown in Appendix Table 4, the upper bound performance of our method\nacross these runs can surpass ICL on 18 tasks, demonstrating its potential for future development.\nEven when compared to certain gradient-based optimization methods, our approach consistently\ndemonstrates competitive performance. For example, as depicted in Table 1, our method exhibits a\nnotable improvement of 3.1% on average in contrast to the promising IA3 method. Nevertheless, we\nacknowledge that our approach still falls behind LoRA tuning and full fine-tuning, especially in tasks\nthat exhibit significant deviation from the upstream task. Taking Dyck Languages as an example,\nboth LoraHub and ICL achieve only an average performance of nearly 1.0% on these tasks, while\nLoRA and FFT methods showcase impressive results with only 5 examples.\n4.3\nDISCUSSION\nLoraHub addresses the challenge of reducing inference costs by eliminating the need for processing\nadditional tokens, resulting in a noticeable reduction in overall inference expenses. However, it\nintroduces an inherent cost during the ADAPT stage, necessitating extra inference steps, such as\nthe 40 steps employed in our experiments. This introduces a trade-off between choosing the ICL\napproach and LoraHub, with the decision typically hinging on the nature of the situation.\nFor one-time ad-hoc tasks, the ICL approach should be more pragmatic due to LoraHub\u2019s additional\ninference step costs. In such scenarios, where immediate, single-use solutions are preferred, the sim-\nplicity and efficiency of ICL might outweigh the benefits of potential savings offered by LoraHub.\n1We released used the dataset at huggingface.co/datasets/lorahub/flanv2.\n5\nPreprint\nTable 1: Experimental results of zero-shot learning (Zero), few-shot in-context learning (ICL), IA3\nfine-tuning (IA3), LoRA tuning (LoRA), full fine-tuning (FFT) and our proposed few-shot LoraHub\nlearning (LoraHub) on the BBH benchmark with FLAN-T5-large as the base LLM. We denote\nalgorithmic tasks with the superscript \u00a7 following previous work (Wu et al., 2023b). Note that we\nemploy three runs, each leveraging different 5-shot examples per task, as demonstrations for all few-\nshot methods. The average performance of all methods is reported below, and the best performance\nof each few-shot method can be found in the Appendix A.\nTask\nZero\nICLavg\nIA3avg\nLoRAavg\nFFTavg\nLoraHubavg\nBoolean Expressions\n54.0\n59.6\n56.2\n56.0\n62.2\n55.5\nCausal Judgement\n57.5\n59.4\n60.2\n55.6\n57.5\n54.3\nDate Understanding\n15.3\n20.4\n20.0\n35.8\n59.3\n32.9\nDisambiguation\n0.0\n69.1\n0.0\n68.0\n68.2\n45.2\nDyck Languages\n1.3\n0.9\n4.2\n22.2\n19.5\n1.0\nFormal Fallacies\n51.3\n55.3\n51.5\n53.6\n54.0\n52.8\nGeometric Shapes\n6.7\n19.6\n14.7\n24\n31.1\n7.4\nHyperbaton\n6.7\n71.8\n49.3\n55.3\n77.3\n62.8\nLogical Deduction\u00a7\n(five objects)\n21.3\n39.1\n32.7\n40.0\n42.2\n36.1\nLogical Deduction\u00a7\n(seven objects)\n12.7\n40.7\n33.8\n37.3\n44.9\n36.8\nLogical Deduction\u00a7\n(three objects)\n0.0\n51.6\n8.5\n53.6\n52.9\n45.7\nMovie Recommendation\n62.7\n55.8\n61.8\n51.5\n66.0\n55.3\nMultistep Arithmetic\n0.7\n0.7\n0.7\n0.2\n0.0\n0.4\nNavigate\n47.3\n45.3\n46.2\n48.0\n48.0\n47.1\nObject Counting\n34.7\n32.4\n35.1\n38.7\n35.6\n33.7\nPenguins in a Table\n43.5\n41.3\n45.0\n36.2\n31.9\n35.9\nReasoning about Colored Objects\n32.0\n40.2\n40.7\n39.6\n37.6\n40.0\nRuin Names\n23.3\n19.3\n24.4\n37.8\n61.3\n24.4\nSalient Translation Error Detection\n37.3\n47.3\n37.1\n16.0\n16.2\n36.0\nSnarks\n50.0\n54.2\n53.9\n55.6\n66.7\n56.9\nSports Understanding\n56.0\n54.7\n55.1\n56.5\n54.0\n56.7\nTemporal Sequences\n16.7\n25.1\n18.2\n25.1\n37.8\n18.2\nTracking Shuffled Objects\u00a7\n(five objects)\n12.0\n12.0\n12.0\n13.8\n16.9\n12.3\nTracking Shuffled Objects\u00a7\n(seven objects)\n6.7\n6.7\n6.7\n10.0\n9.8\n7.7\nTracking Shuffled Objects\u00a7\n(three objects)\n24.7\n31.1\n30.7\n30.9\n32.0\n29.2\nWeb of Lies\n54.0\n53.8\n54.2\n52.7\n48.2\n50.1\nWord Sorting\n1.3\n0.5\n1.3\n4.9\n4.9\n1.1\nAvg Performance Per Task\n27.0\n37.3\n31.6\n37.7\n42.1\n34.7\nAvg Tokens Per Example\n111.6\n597.8\n111.6\n111.6\n111.6\n111.6\nGradient-based Training\nNo\nNo\nYes\nYes\nYes\nNo\nConversely, for recurring or similar tasks, LoraHub emerges as a compelling option. Despite the\nadded inference step cost, LoraHub\u2019s ability to efficiently handle repetitive tasks, often occurring\nthousands of times, while concurrently reducing overall expenses, positions it as a viable option in\nsuch kind of situations.\nIn summary, our intention is not to replace ICL, but to present LoraHub as a complementary strategy\nwith performance-efficiency trade-offs. Thus, we encourage a careful consideration of specific use\ncases and requirements when choosing between ICL and LoraHub, recognizing that the optimal\nsolution may vary based on the nature and frequency of the tasks at hand.\n6\nPreprint\nTable 2: The top five beneficial LoRA modules for BBH tasks and their associated upstream tasks,\nthe average weight values and the average performance on all BBH tasks.\nRank\nDataset: Task\nWeight\nPerf\nTask Description\n1\nWIQA: Last Process\n0.72\n28.1\nIdentifying the last step of a given process.\n2\nRACE: Is this the Right Answer\n0.68\n30.8\nDetermining if given answer is correct.\n3\nWIQA: First Process\n0.63\n28.1\nIdentifying the first step of a given process.\n4\nAdversarialQA: BiDAF\n0.61\n25.1\nAnswering question created by an\nadversarial model-in-the-loop.\n5\nWebQuestions: What is the Answer\n0.58\n27.0\nAnswering question based on information\nextracted from the web.\n5\nEXPERIMENTAL ANALYSIS\nIn this section, we thoroughly examine the characteristics of our proposed method and uncover\nseveral insightful findings. If not specified, we use FLAN-T5-large for all analysis.\nWhich LoRA modules are most effective for BBH tasks?\nWe hypothesized that the amalgamation of LoRA modules could incorporate skills and insights from\na variety of specific tasks. To evaluate this, we examined the extent of influence a single LoRA mod-\nule had amongst all tasks from the BBH benchmark. We measured the impact of each isolated task\nby calculating the average absolute weight. The top five modules, presented in Table 2, were found\nto have substantial influence, as indicated by their maximum average weights, which suggested that\nthey were notably more effective in cross-task transfer. Remarkably, a common feature among these\ntop five modules was their association with tasks requiring reading comprehension and reasoning\nskills\u2014attributes indicative of higher cognitive complexity. However, it is worth noting that none\nof the modules exhibited consistent improvement across all BBH tasks, as reflected in their average\nperformance on all BBH tasks, which did not show a significant improvement compared to the orig-\ninal FLAN-T5-large, except for the Rank 2. The results underscore the advantages of composing\ndiverse modules in LoraHub.\nHow effective is the gradient-free optimization method?\nTo assess the effectiveness of our gradient-free optimization method in correctly identifying the\nmost suitable LoRA module for a given downstream task, we carried out an empirical study using\nthe WikiTableQuestions (Pasupat & Liang, 2015) (WTQ) dataset. We strategically included a LoRA\nmodule that was specifically trained on the WTQ dataset into our pool of LoRA candidate modules,\nwhich originally stemmed from tasks exclusive to the Flan Collection. Subsequently, we designated\nWTQ as the targeted downstream task and computed the weights consistent with the methods em-\nployed in LoraHub learning. As an end result, the WTQ-specific LoRA module was awarded the\nhighest weight, exemplifying the algorithm\u2019s success in recognizing it as the most relevant. More-\nover, the combined LoRA module demonstrated marginal superiority over the WTQ LoRA module.\nThis underscores the claim that the gradient-free optimization method has the ability to proficiently\nselect the optimal upstream LoRA module for an unseen task.\nCan LoraHub work well on non-instruction-tuning models?\nIn previous investigations, we primarily focused on models with zero-shot capabilities that were\ntrained with instruction tuning. However, for models like T5 without zero-shot abilities, where\ntraining has a larger effect on parameters, it was unclear if LoraHub could still effectively manage\nand improve them. Our experiments show that although these models perform worse than FLAN-\nT5, LoraHub learning can still enable them to effectively generlize to unseen tasks. See Appendix B\nfor more details.\nWill the rank of LoRA modules impact the performance of LoraHub learning?\n7\nPreprint\nTable 3: The average performance of various methods across all tasks in the benchmark BBH.\nLoRA Retrieval\nLoraHub avg\nLoraHub best\n31.7\n34.7\n41.2\nThe parameter rank plays a crucial role in the LoRA framework, directly influencing the number\nof trainable parameters utilized during LoRA tuning. This prompts an intriguing question: does the\nvariation in rank values influence the outcomes observed within the LoraHub learning? Our analysis\nindicates that, for FLAN-T5, the choice of rank has minimal impact. However, for T5, it still exerts\nsome influence. Empirical findings reveal that, in comparison to rank values of 4 or 64, a rank value\nof 16 consistently demonstrates superior performance across different runs, both in terms of average\nand optimal values. Additional results are available in Appendix B.\nDoes more LoRA modules lead to better results?\nIn our main experiments, we randomly selected 20 LoRA modules for LoraHub learning. Therefore,\nwe conducted experiments to investigate the effect of using different numbers of LoRA modules.\nThe results demonstrate that as we increased the number of LoRA modules, the variance in perfor-\nmance increased. However, the maximum achievable performance also improved. More analysis on\nthe variance and the detailed results can be found in Appendix G.\nDoes composing LoRA modules extend beyond the single module\u2019s benefits?\nWe acknowledge the investigation of cross-task performance in prior work (Jang et al., 2023), which\ndelved into the capabilities of LoRA and proposed a novel method centered around LoRA module\nretrieval. In order to ensure a fair comparison, we conducted an experiment where we designed\na LoRA retrieval mechanism based on the loss derived from few-shot examples. Specifically, we\nranked all LoRA module candidates according to this loss and evaluated the best candidate on the\ntest set of the unseen task. As depicted in Table 3, the performance of LoRA retrieval is notably\nimpressive, positioning it as a strong baseline. However, in comparison to LoraHub, the performance\nof LoRA retrieval is relatively less favorable\n6\nRELATED WORK\nModel Merging\nOur method substantially draws on the concept of LoRA module composition,\nand thus, aligns with the significant thread of research in model merging. This research focus is\nbroadly categorized based on the ultimate objectives of model merging.\nThe first category focuses on merging entire models, and the goal is to combine individually trained\nmodels to approximate the performance benefits of model ensembling or multi-task learning. Prior\nworks (Matena & Raffel, 2021; Jin et al., 2023; Yadav et al., 2023; Wu et al., 2023a) operated under\nthe assumption of shared model architectures. For example, Matena & Raffel (2021) amalgamates\nmodels by approximating Gaussian posterior distributions garnered from Fisher information, while\nYadav et al. (2023) merges models via resolving model interferences. Another approach is merging\nmodels with different architectures. For instance, Ainsworth et al. (2023) configures weights of\ndifferent models prior to their merger. Following this objective, Stoica et al. (2023) merges models\noperating on varying tasks by identifying common features, without requiring additional training.\nUnlike these works, our work focuses on merging models for better cross-task generalization.\nThe second category most closely aligns with our research, stemming from a shared motivation of\nmodule composition. Various scholars have made advances in this line of research: Kingetsu et al.\n(2021) decomposes and recomposes modules on the basis of their functionality; Ilharco et al. (2023)\nproposes modulating model behavior using task vectors; Lv et al. (2023) amalgamates parameter-\nefficient modules weighted according to task similarity; Zhang et al. (2023) crafts modules by em-\nploying specific arithmetic operations; Sun et al. (2023) improves few-shot performance of unseen\ntasks by multi-task pre-training of prompts; Chronopoulou et al. (2023) averages adapter weights\nintended for transfer; Ponti et al. (2023) focuses on jointly learning adapters and a routing function\n8\nPreprint\nthat allocates skills to each task; and Muqeeth et al. (2023) concentrates on amalgamating experts\nin mixture of experts models; However, these methods generally necessitate multi-task training or\nhuman prior on module selection for the downstream task. In contrast, our method does not im-\npose any special training requirements and simply employs vanilla LoRA tuning. Additionally, the\nmodule selection for downstream tasks is entirely data-driven without human prior knowledge. This\ndesign gives the advantage of easily adding new LoRA modules for reuse, allowing our method to\nflexibly scale up the number of LoRA module candidates in the future.\nMixture of Experts\nThe Mixture of Experts (MoE) is an ensemble method, often visualized as\na collection of sub-modules, or \u201cexperts\u201d, each specializing in processing different types of input\ndata. Each expert in this system is controlled by a unique gating network, activated based on the\ndistinct nature of the input data. For every token in these input sequences, this network identifies\nand engages the most suitable experts to process the data. As a result, the performance is superior\ncompared to relying on a single, generic model for all types of input. This technique has proven\ninstrumental in numerous domains, such as natural language processing and computer vision (Ja-\ncobs et al., 1991; Shazeer et al., 2017; Du et al., 2022; Zhang et al., 2022; Wang et al., 2022; crumb,\n2023). Our methodology displays similarities to MoE, wherein upstream-trained LoRA modules\ncan be aligned with MoE\u2019s expert design. A noteworthy distinguishing factor is that our approach\nmechanism does not require any specialized manipulation of LoRAs during training while facili-\ntating dynamic LoRA module assembly at any scale, each pre-tuned to different tasks. In contrast,\nMoE mandates a predetermined count of experts during both the training and testing phases. Recent\nstudies on the interrelation between MoE and instruction tuning have demonstrated that the simul-\ntaneous application of both approaches enhances the effectiveness of each individually (Shen et al.,\n2023).\nCross-Task generalization\nRecent advancements like CrossFit (Ye et al., 2021), ExT5 (Aribandi\net al., 2022), FLAN (Wei et al., 2022), T0 (Sanh et al., 2022), InstructGPT (Ouyang et al., 2022),\nand ReCross (Lin et al., 2022) have been striving to foster a vastly multi-task model\u2019s generalization\nacross different tasks, very much aligned with the objectives of our research. Among this cohort,\nthe connections of CrossFit and ReCross with LoraHub are particularly noteworthy. The CrossFit\nframework (Ye et al., 2021) mandates a minimal number of labeled examples of the target task for\nfew-shot fine-tuning. However, its limitation lies in the application of task names as hard prefixes\nin templates, posing challenges in the task\u2019s generalization. On the other hand, while ReCross\nmitigates the need for labels in few-shot examples for retrieval, it necessitates a fine-tuning process\nusing the retrieved data. This procedure appears time-consuming when compared to LoraHub\u2019s\napproach. Through the deployment of few-shot labeled examples and a gradient-free optimization\nprocess, LoraHub facilitates an iterative update of weights to compose the LoRA modules. The\nresultant method is more efficient and cost-effective relative to previous work. Overall, LoraHub\noffers a more practical and viable solution to the optimization process.\n7\nCONCLUSION\nIn this work, we have introduced LoraHub, a strategic framework for composing LoRA modules\ntrained on diverse tasks in order to achieve adaptable performance on new tasks. Our approach\nenables the fluid combination of multiple LoRA modules using just a few examples from a novel\ntask, without requiring additional model parameters or human expertise. The empirical results on\nthe BBH benchmark demonstrate that LoraHub can effectively match the performance of in-context\nlearning in few-shot scenarios, removing the need for in-context examples during inference. Overall,\nour work shows the promise of strategic LoRA composability for rapidly adapting LLMs to diverse\ntasks. By fostering reuse and combination of LoRA modules, we can work towards more general\nand adaptable LLMs while minimizing training costs.\n8\nLIMITATIONS & FUTURE WORK\nPre-Filtering of LoRA Module Candidates\nWhile our method is successful in identifying and\nweighting relevant aspects from seen tasks to enhance unseen task performance, relying entirely\non the model to perform this search can lead to increased computational demands and potentially\n9\nPreprint\nunstable results. Incorporating a pre-filtering step to select only pertinent LoRA modules could\nexpedite and refine performance. Identifying an effective selection strategy warrants further study.\nMethod Applicability to Decoder-Only Models\nAll experiments for this study were executed\nusing the encoder-decoder architecture. We aspire to extrapolate this method to decoder-only models\nsuch as GPT (Brown et al., 2020), aiming to determine its applicability in such contexts.\nExploring Superior Optimization Methods\nThe use of a genetic algorithm for optimization in\nthis study raises the question of whether better optimization approaches exist that could provide\nsuperior gradient-free optimization with limited examples. Although the current method has shown\nadequate performance, there is still room for improvement.\nREFERENCES\nSamuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models mod-\nulo permutation symmetries. In The Eleventh International Conference on Learning Representa-\ntions, 2023.\nShengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and\nJian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. ArXiv\npreprint, 2022.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta,\nHonglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Prakash Gupta, Kai Hui, Sebastian\nRuder, and Donald Metzler. Ext5: Towards extreme multi-task scaling for transfer learning. In\nProc. of ICLR, 2022.\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Ab-\nheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, An-\ndrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason\nFries, Maged Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang,\nDragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. PromptSource: An integrated de-\nvelopment environment and repository for natural language prompts. In Proc. of ACL, 2022.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle,\nMarc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances\nin Neural Information Processing Systems 33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models\nto compress contexts. CoRR, abs/2305.14788, 2023. doi: 10.48550/ARXIV.2305.14788. URL\nhttps://doi.org/10.48550/arXiv.2305.14788.\nAlexandra Chronopoulou, Matthew Peters, Alexander Fraser, and Jesse Dodge.\nAdapterSoup:\nWeight averaging to improve generalization of pretrained language models. In Findings of the\nAssociation for Computational Linguistics: EACL 2023, 2023.\nHyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun\nDai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gau-\nrav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and\nJason Wei. Scaling instruction-finetuned language models. ArXiv preprint, 2022.\ncrumb.\nLlama-2,\nmixutre\nof\nlora.\nhttps://crumbly.medium.com/\nllama-2-molora-f5f909434711, 2023.\n10\nPreprint\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma,\nZongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kath-\nleen S. Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng\nChen, and Claire Cui. Glam: Efficient scaling of language models with mixture-of-experts. In\nKamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00b4ari, Gang Niu, and Sivan Sabato\n(eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,\nMaryland, USA, Proceedings of Machine Learning Research, 2022.\nTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context\ncompression in a large language model. CoRR, abs/2307.06945, 2023. doi: 10.48550/ARXIV.\n2307.06945. URL https://doi.org/10.48550/arXiv.2307.06945.\nAryo Pradipta Gema, Luke Daines, Pasquale Minervini, and Beatrice Alex. Parameter-efficient\nfine-tuning of llama for the clinical domain. ArXiv preprint, 2023.\nNikolaus Hansen and Andreas Ostermeier. Adapting arbitrary normal mutation distributions in evo-\nlution strategies: the covariance matrix adaptation. Proceedings of IEEE International Conference\non Evolutionary Computation, 1996.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a\nunified view of parameter-efficient transfer learning. In Proc. of ICLR, 2022.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In Proc. of ICLR, 2022.\nGabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi,\nand Ali Farhadi. Editing models with task arithmetic. In The Eleventh International Conference\non Learning Representations, 2023.\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures\nof local experts. Neural Computation, 1991.\nJoel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee,\nKyungjae Lee, and Minjoon Seo.\nExploring the benefits of training expert language models\nover instruction tuning. In International Conference on Machine Learning, 2023. URL https:\n//api.semanticscholar.org/CorpusID:256627673.\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing\nprompts for accelerated inference of large language models. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language Processing. Association for Computational\nLinguistics, December 2023a. URL https://arxiv.org/abs/2310.05736.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili\nQiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt com-\npression.\nCoRR, abs/2310.06839, 2023b.\ndoi: 10.48550/ARXIV.2310.06839.\nURL https:\n//doi.org/10.48550/arXiv.2310.06839.\nXisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion\nby merging weights of language models. In The Eleventh International Conference on Learning\nRepresentations, 2023.\nHiroaki Kingetsu, Kenichi Kobayashi, and Taiji Suzuki. Neural network module decomposition and\nrecomposition. ArXiv preprint, 2021.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. In Proc. of EMNLP, 2021.\nYucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance infer-\nence efficiency of large language models. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing. Association for Computational Linguistics, December\n2023. URL https://arxiv.org/abs/2310.06201.\n11\nPreprint\nBill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen Tian, and Xiang Ren. Unsupervised cross-task\ngeneralization via retrieval augmentation. In NeurIPS, 2022.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and\nColin Raffel.\nFew-shot parameter-efficient fine-tuning is better and cheaper than in-context\nlearning. ArXiv, abs/2205.05638, 2022. URL https://api.semanticscholar.org/CorpusID:\n248693283.\nJialin Liu, A. Moreau, Mike Preuss, Baptiste Rozi`ere, J\u00b4er\u00b4emy Rapin, Fabien Teytaud, and Olivier\nTeytaud. Versatile black-box optimization. Proceedings of the 2020 Genetic and Evolutionary\nComputation Conference, 2020.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V.\nLe, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods\nfor effective instruction tuning, 2023.\nXingtai Lv, Ning Ding, Yujia Qin, Zhiyuan Liu, and Maosong Sun. Parameter-efficient weight\nensembling facilitates task-level knowledge transfer. In Annual Meeting of the Association for\nComputational Linguistics, 2023.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: State-\nof-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft,\n2022.\nMichael Matena and Colin Raffel. Merging models with fisher-weighted averaging. ArXiv preprint,\n2021.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language Technologies, 2022.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization\nvia natural language crowdsourcing instructions. In Proc. of ACL, 2022.\nMohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing.\nArXiv preprint, 2023.\nOpenAI. ChatGPT. 2022. URL https://openai.com/blog/chatgpt.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel-\nton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan\nLeike, and Ryan J. Lowe. Training language models to follow instructions with human feedback.\nArXiv preprint, 2022.\nPanupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In\nProc. of ACL, 2015.\nEdoardo Maria Ponti, Alessandro Sordoni, Yoshua Bengio, and Siva Reddy. Combining parameter-\nefficient modules for task-level generalisation.\nIn Proceedings of the 17th Conference of the\nEuropean Chapter of the Association for Computational Linguistics, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 2020.\nJ. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https://GitHub.\ncom/FacebookResearch/Nevergrad, 2018.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V.\nNayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica,\nSheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,\n12\nPreprint\nJos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F\u00b4evry, Jason Alan Fries, Ryan Teehan,\nTeven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask\nprompted training enables zero-shot task generalization. In Proc. of ICLR, 2022.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\nIn Proc. of ICLR, 2017.\nSheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret\nZoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan\nLi, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou. Mixture-of-\nexperts meets instruction tuning:a winning combination for large language models, 2023.\nGeorge Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, and Judy Hoffman. Zipit! merging\nmodels from different tasks without training. arXiv, 2023.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning\nfor language-model-as-a-service.\nIn Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba\nSzepesv\u00b4ari, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learn-\ning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, Proceedings of Machine Learning\nResearch, 2022.\nTianxiang Sun, Zhengfu He, Qin Zhu, Xipeng Qiu, and Xuanjing Huang. Multitask pre-training of\nmodular prompt for Chinese few-shot learning. In Proc. of ACL, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models. ArXiv preprint, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\nGarnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017.\nYaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan\nAwadallah, and Jianfeng Gao. AdaMix: Mixture-of-adaptations for parameter-efficient model\ntuning. In Proc. of EMNLP, 2022.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In Proc. of\nICLR, 2022.\nChengyue Wu, Teng Wang, Yixiao Ge, Zeyu Lu, Ruisong Zhou, Ying Shan, and Ping Luo. \u03c0-\ntuning: Transferring multimodal foundation models with optimal multi-task interpolation. In An-\ndreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan\nScarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 37713\u2013\n37727. PMLR, 2023a. URL https://proceedings.mlr.press/v202/wu23t.html.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prab-\nhanjan Kambadur, David S. Rosenberg, and Gideon Mann. Bloomberggpt: A large language\nmodel for finance.\nCoRR, abs/2303.17564, 2023b.\ndoi: 10.48550/arXiv.2303.17564.\nURL\nhttps://doi.org/10.48550/arXiv.2303.17564.\nPrateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. TIES-merging: Re-\nsolving interference when merging models. In Thirty-seventh Conference on Neural Information\nProcessing Systems, 2023. URL https://openreview.net/forum?id=xtaX3WyCj1.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. CrossFit: A few-shot learning challenge for cross-task\ngeneralization in NLP. In Proc. of EMNLP, 2021.\n13\nPreprint\nChris Zhang, Mengye Ren, and Raquel Urtasun.\nGraph hypernetworks for neural architecture\nsearch. In Proc. of ICLR, 2019.\nFan Zhang, Duyu Tang, Yong Dai, Cong Zhou, Shuangzhi Wu, and Shuming Shi. Skillnet-nlu: A\nsparsely activated model for general-purpose natural language understanding, 2022.\nJinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules\nwith arithmetic operations. ArXiv preprint, 2023.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cotterell, and Mrinmaya Sachan.\nEfficient\nprompting via dynamic in-context learning. CoRR, abs/2305.11170, 2023. doi: 10.48550/ARXIV.\n2305.11170. URL https://doi.org/10.48550/arXiv.2305.11170.\n14\nPreprint\nA\nRESULT OF BEST RESULTS\nAs shown in Table 4, compared to gradient-based parameter-efficient training methods like LoRA\nand IA3, our approach demonstrates superior performance in terms of best results over experimental\nruns. While it exhibits a noticeable lag behind the fully fine-tuning (FFT) method, which updates\nall parameters during training, this observation suggests that our proposed method has a promising\nupper limit. We anticipate that future research efforts can contribute to accelerating the optimization\nspeed and further enhancing the efficacy of our approach.\nTable 4: Experimental results of several few-shot methods, including in-context learning (ICL), IA3\nfine-tuning (IA3), LoRA tuning (LoRA), full fine-tuning (FFT) and our LoraHub learning (LoraHub)\non the BBH benchmark with FLAN-T5-large as the base LLM. We denote algorithmic tasks with\nthe superscript \u00a7 following previous work (Wu et al., 2023b). Note that we use 5 examples per task\nas the demonstration for all methods. The best (best) performance is reported as the maximum value\nobtained across three runs.\nTask\nICLbest\nIA3best\nLoRAbest\nFFTbest\nLoraHubbest\nBoolean Expressions\n62.7\n58.0\n60.7\n65.3\n60.7\nCausal Judgement\n59.8\n62.1\n57.5\n60.9\n63.2\nDate Understanding\n21.3\n20.7\n40.7\n67.3\n45.3\nDisambiguation\n69.3\n0.0\n68.7\n70.7\n68.0\nDyck Languages\n2.0\n4.7\n25.3\n33.3\n2.7\nFormal Fallacies\n59.3\n52.0\n56.7\n56.0\n59.3\nGeometric Shapes\n20.0\n15.3\n28.7\n39.3\n18.7\nHyperbaton\n72.7\n49.3\n57.3\n82.0\n72.7\nLogical Deduction\u00a7\n(five objects)\n39.3\n32.7\n41.3\n43.3\n40.0\nLogical Deduction\u00a7\n(seven objects)\n42.0\n34.0\n42.7\n46.0\n46.0\nLogical Deduction\u00a7\n(three objects)\n52.7\n8.7\n56.7\n60.7\n52.7\nMovie Recommendation\n56.7\n62.0\n64.5\n70.7\n62.0\nMultistep Arithmetic\n0.7\n0.7\n0.7\n0.0\n1.3\nNavigate\n46.7\n47.3\n50.7\n50.0\n51.3\nObject Counting\n34.7\n35.3\n42.0\n38.0\n36.7\nPenguins in a Table\n43.5\n45.7\n41.3\n37.0\n47.8\nReasoning about Colored Objects\n41.3\n41.3\n40.7\n38.7\n44.7\nRuin Names\n20.7\n25.3\n42.0\n66.0\n28.7\nSalient Translation Error Detection\n48.0\n37.3\n17.3\n21.3\n42.7\nSnarks\n55.1\n56.4\n59.0\n69.2\n61.5\nSports Understanding\n56.7\n55.3\n58.7\n58.7\n62.7\nTemporal Sequences\n26.7\n18.7\n31.3\n48.7\n21.3\nTracking Shuffled Objects\u00a7\n(five objects)\n12.0\n12.0\n16.0\n20.0\n16.7\nTracking Shuffled Objects\u00a7\n(seven objects)\n6.7\n6.7\n12.0\n10.0\n15.3\nTracking Shuffled Objects\u00a7\n(three objects)\n31.3\n30.7\n32.0\n36.0\n31.3\nWeb of Lies\n54.0\n54.7\n55.3\n54.0\n57.3\nWord Sorting\n0.7\n1.3\n5.3\n6.0\n1.3\nBest Performance (Average)\n38.4\n32.1\n40.9\n46.2\n41.2\n15\nPreprint\nB\nRESULT OF NON-INSTRCUTION-TUNED MODELS\nTable 5: Comparsion among different ranks for few-shot LoraHub learning with the backbone T5-\nlarge (Raffel et al., 2020) on the BBH benchmark. Note that the T5-large model achieved 0.0% on\nall tasks under the zero-shot setting except Dyck Languages, where it scored 0.67%.\nTask \u2193\nRank \u2192\n4avg\n4best\n16avg\n16best\n64avg\n64best\nBoolean Expressions\n52.13\n57.33\n50.67\n58.00\n47.47\n58.00\nCausal Judgement\n52.41\n55.17\n49.66\n54.02\n50.80\n54.02\nDate Understanding\n0.40\n2.00\n14.40\n29.33\n4.53\n10.00\nDisambiguation\n10.00\n31.33\n26.93\n42.00\n1.73\n4.67\nDyck Languages\n0.40\n0.67\n0.40\n0.67\n0.40\n2.00\nFormal Fallacies\n48.40\n54.00\n46.93\n51.33\n46.93\n50.00\nGeometric Shapes\n0.00\n0.00\n6.53\n32.67\n1.47\n7.33\nHyperbaton\n30.13\n50.00\n39.07\n57.33\n32.93\n48.00\nLogical Deduction\u00a7\n(five objects)\n5.20\n14.67\n8.80\n19.33\n1.33\n6.67\nLogical Deduction\u00a7\n(seven objects)\n6.40\n17.33\n9.33\n19.33\n3.47\n16.00\nLogical Deduction\u00a7\n(three objects)\n14.40\n32.00\n21.73\n34.67\n6.93\n15.33\nMovie Recommendation\n7.07\n18.67\n7.87\n22.00\n1.20\n6.00\nMultistep Arithmetic two\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nNavigate\n49.60\n54.67\n52.27\n56.67\n49.87\n52.00\nObject Counting\n7.20\n18.00\n16.00\n21.33\n13.73\n26.67\nPenguins in a Table\n6.52\n13.04\n10.43\n17.39\n0.43\n2.17\nReasoning about Colored Objects\n6.27\n10.00\n5.07\n16.67\n0.53\n2.67\nRuin Names\n7.73\n13.33\n13.20\n28.00\n5.73\n15.33\nSalient Translation Error Detection\n0.00\n0.00\n1.73\n8.67\n0.00\n0.00\nSnarks\n21.28\n42.31\n49.49\n60.26\n16.15\n38.46\nSports Understanding\n46.53\n58.67\n46.80\n58.67\n46.53\n58.67\nTemporal Sequences\n3.07\n13.33\n6.53\n26.67\n2.40\n12.00\nTracking Shuffled Objects\u00a7\n(five objects)\n5.20\n14.00\n4.13\n9.33\n0.13\n0.67\nTracking Shuffled Objects\u00a7\n(seven objects)\n2.67\n10.00\n2.80\n14.00\n3.20\n8.00\nTracking Shuffled Objects\u00a7\n(three objects)\n3.73\n17.33\n16.27\n34.67\n5.87\n26.67\nWeb of Lies\n48.53\n54.00\n54.00\n56.00\n54.67\n57.33\nWord Sorting\n0.40\n0.67\n0.13\n0.67\n0.00\n0.00\nAverage Performance per Task\n16.14\n24.17\n20.78\n30.73\n14.76\n21.43\n16\nPreprint\nC\nRESULT OF LARGER MODEL\nTable 6: Experimental results of zero-shot learning (Zero) and our few-shot LoraHub learning (Lo-\nraHub) on the BBH benchmark with FLAN-T5-xl as the base LLM. Note that we use 5 examples per\ntask as the demonstration for both ICL and LoraHub. The average (avg) performance of LoraHub\nis computed over 5 runs with different random seeds, while the best (best) performance is reported\nas the maximum value obtained across these runs. We can see the trend of the results are similar to\nFLAN-T5-large.\nTask\nZero\nLoraHub avg\nLoraHub best\nBoolean Expressions\n52.0\n58.7\n63.3\nCausal Judgement\n62.1\n53.8\n59.8\nDate Understanding\n38.0\n37.6\n38.0\nDisambiguation Qa\n0.0\n20.5\n54.7\nDyck Languages\n1.3\n0.9\n2.0\nFormal Fallacies\n56.0\n56.0\n56.0\nGeometric Shapes\n8.7\n17.5\n28.0\nHyperbaton\n45.3\n53.5\n56.7\nLogical Deduction\u00a7\n(five objects)\n1.3\n42.7\n48.7\nLogical Deduction\u00a7\n(seven objects)\n8.7\n44.3\n50.0\nLogical Deduction\u00a7\n(three objects)\n0.7\n56.4\n61.3\nMovie Recommendation\n2.0\n62.8\n66.0\nMultistep Arithmetic Two\n0.0\n0.4\n0.7\nNavigate\n50.7\n50.7\n50.7\nObject Counting\n39.3\n40.7\n48.0\nPenguins In A Table\n17.4\n40.9\n45.7\nReasoning About Colored Objects\n46.7\n47.3\n50.7\nRuin Names\n18.0\n35.6\n44.7\nSalient Translation Error Detection\n44.7\n45.1\n48.7\nSnarks\n60.3\n60.8\n61.5\nSports Understanding\n56.7\n51.3\n53.3\nTemporal Sequences\n21.3\n21.5\n22.0\nTracking Shuffled Objects\u00a7\n(five objects)\n3.3\n9.9\n13.3\nTracking Shuffled Objects\u00a7\n(seven objects)\n5.3\n7.3\n8.7\nTracking Shuffled Objects\u00a7\n(three objects)\n7.3\n21.7\n31.3\nWeb Of Lies\n54.7\n47.1\n48.7\nWord Sorting\n1.3\n1.5\n2.0\nAverage Performance per Task\n25.8\n36.5\n41.3\n17\nPreprint\nD\nIMPROVING THE ROBUSTNESS OF LORAHUB\nIn order to enhance the robustness of LoraHub, we explored a straightforward approach in the se-\nlection of LoRA module candidates. Specifically, we first identified 20 LoRA module candidates\nwith the lowest loss on the few-shot examples. Our findings indicate a slight improvement in overall\nperformance after applying the pre-filtering startegy. Since the primary instability in our approach\narises from the selection of LoRA candidates. This method involves choosing a fixed set of LoRA\ncandidates to ensure the stability of our approach.\nTable 7: The experimental results of loss-based pre-filtering.\nTask\nLoraHubavg\nLoraHubfilter\nBoolean Expressions\n55.5\n60.00\nCausal Judgement\n54.3\n52.9\nDate Understanding\n32.9\n33.3\nDisambiguation\n45.2\n62.7\nDyck Languages\n1.0\n0.0\nFormal Fallacies\n52.8\n54.0\nGeometric Shapes\n7.4\n4.0\nHyperbaton\n62.8\n64.0\nLogical Deduction\u00a7\n(five objects)\n36.1\n37.3\nLogical Deduction\u00a7\n(seven objects)\n36.8\n22.0\nLogical Deduction\u00a7\n(three objects)\n45.7\n56.0\nMovie Recommendation\n55.3\n68.0\nMultistep Arithmetic\n0.4\n0.7\nNavigate\n47.1\n49.3\nObject Counting\n33.7\n38.7\nPenguins in a Table\n35.9\n37.0\nReasoning about Colored Objects\n40.0\n33.3\nRuin Names\n24.4\n22.0\nSalient Translation Error Detection\n36.0\n24.0\nSnarks\n56.9\n52.66\nSports Understanding\n56.7\n58.0\nTemporal Sequences\n18.2\n27.3\nTracking Shuffled Objects\u00a7\n(five objects)\n12.3\n11.3\nTracking Shuffled Objects\u00a7\n(seven objects)\n7.7\n8.0\nTracking Shuffled Objects\u00a7\n(three objects)\n29.2\n32.7\nWeb of Lies\n50.1\n46.0\nWord Sorting\n1.1\n1.3\nAvg Performance Per Task\n34.7\n35.4\n18\nPreprint\nE\nPERFORMANCE ON GENERAL IMPORTANT TASK\nIn our research, we have identified specific LoRA modules that exhibit significant impact when\nintegrated into merged LoRAs. Our focus lies in assessing the performance of the top five task-\nrelated LoRAs on the BBH benchmark. The results indicate that these top LoRAs perform similarly\nor even worse than zero-shot in most cases. Only one of them stands out as significantly better than\nzero-shot. However, it\u2019s worth noting that this performance is not as impressive as Lorahub. These\nfindings support the idea that the merging process can improve overall performance.\nTable 8: Detailed experimental results of top five LoRA modules shown in Table 2 on BBH tasks.\nTask\nWIQA: Last\nRACE: Right\nWIQA: First\nADQA\nWebQA\nBoolean Expressions\n52.67\n58.00\n52.67\n54.67\n53.33\nCausal Judgement\n55.17\n63.22\n55.17\n57.47\n57.47\nDate Understanding\n17.33\n19.33\n17.33\n16.67\n15.33\nDisambiguation\n0.00\n0.00\n0.00\n0.00\n0.00\nDyck Languages\n0.67\n0.67\n0.67\n1.33\n1.33\nFormal Fallacies\n51.33\n51.33\n51.33\n51.33\n51.33\nGeometric Shapes\n8.00\n13.33\n8.00\n6.67\n7.33\nHyperbaton\n16.67\n44.00\n16.67\n1.33\n6.00\nLogical Deduction\u00a7\n(five objects)\n23.33\n28.00\n23.33\n19.33\n20.67\nLogical Deduction\u00a7\n(seven objects)\n22.00\n26.00\n22.00\n10.67\n12.00\nLogical Deduction\u00a7\n(three objects)\n0.67\n9.33\n0.67\n0.00\n0.00\nMovie Recommendation\n63.33\n62.67\n63.33\n56.67\n63.33\nMultistep Arithmetic\n0.67\n0.67\n0.67\n0.67\n0.67\nNavigate\n47.33\n50.00\n47.33\n47.33\n47.33\nObject Counting\n34.67\n34.00\n34.67\n35.33\n35.33\nPenguins in a Table\n45.65\n41.30\n45.65\n39.13\n43.48\nReasoning about Colored Objects\n40.00\n37.33\n40.00\n31.33\n30.67\nRuin Names\n22.00\n21.33\n22.00\n17.33\n22.67\nSalient Translation Error Detection\n36.67\n34.67\n36.67\n32.67\n37.33\nSnarks\n52.56\n55.13\n52.56\n47.44\n52.56\nSports Understanding\n56.00\n58.67\n56.00\n55.33\n55.33\nTemporal Sequences\n16.67\n17.33\n16.67\n12.67\n17.33\nTracking Shuffled Objects\u00a7\n(five objects)\n12.00\n12.00\n12.00\n10.67\n12.00\nTracking Shuffled Objects\u00a7\n(seven objects)\n6.67\n6.67\n6.67\n6.67\n6.67\nTracking Shuffled Objects\u00a7\n(three objects)\n20.67\n30.67\n20.67\n10.67\n25.33\nWeb of Lies\n54.67\n54.00\n54.67\n54.00\n54.00\nWord Sorting\n1.33\n1.33\n1.33\n1.33\n1.33\nAvg Performance per Task\n28.10\n30.78\n28.10\n25.14\n27.04\n\u2206 FLAN-T5-large\n1.10\n3.78\n1.10\n-1.86\n0.04\n19\nPreprint\n5\n10\n50\n100\n45\n50\n55\n60\nboolean expressions\n5\n10\n50\n100\n40\n45\n50\n55\ncausal judgement\n5\n10\n50\n100\n20\n30\n40\n50\ndate understanding\n5\n10\n50\n100\n0\n20\n40\n60\ndisambiguation qa\n5\n10\n50\n100\n0\n5\n10\n15\ndyck languages\n5\n10\n50\n100\n50\n51\n52\n53\nformal fallacies\n5\n10\n50\n100\n10\n20\n30\ngeometric shapes\n5\n10\n50\n100\n20\n40\n60\nhyperbaton\n5\n10\n50\n100\n25\n30\n35\n40\n45\nlogical deduction five\n5\n10\n50\n100\n20\n30\n40\nlogical deduction seven\n5\n10\n50\n100\n0\n20\n40\n60\nlogical deduction three\n5\n10\n50\n100\n50\n55\n60\n65\nmovie recommendation\n5\n10\n50\n100\n0.0\n0.2\n0.4\n0.6\nmultistep arithmetic two\n5\n10\n50\n100\n42.5\n45.0\n47.5\n50.0\nnavigate\n5\n10\n50\n100\n25\n30\n35\n40\nobject counting\nFigure 3: The influence of number of LoRA modules on 15 tasks from BBH, and each box is\nobtained from 5 separate runs. The horizontal axis shows the number of LoRA modules to be\ncomposed in LoraHub learning.\nF\nIMPLEMENTATION DETAILS\nWe implemented LoRA tuning using the Huggingface PEFT library (Mangrulkar et al., 2022), with\nthe rank being set as 16. The gradient-free method was implemented using the open-source Nev-\nergrad optimization library (Rapin & Teytaud, 2018), with a constraint that the absolute value of\nLoRA weights should not exceed 1.5. Originally, all coefficients of LoRA modules were set at zero.\nIn our standard settings, we set the maximum number of iterations K as 40. The same 5 examples\nwere used during our LoraHub learning and the few-shot in-context learning. The hyperparameter \u03b1\nis set as 0.05. Regarding the hyperparameters for training candidate LoRA modules, we maintained\nconsistency across all modules, setting the batch size at 64, the learning rate at 1e \u2212 4, and the\nnumber of training epochs at 10.\nG\nINFLUENCE OF NUMBER OF LORA MODULES\nAs shown in Figure 3, with an increase in the number of LoRA module candidates, there is a corre-\nsponding increase in the performance variance. Based on our in-depth analysis, the primary source\nof variance is not related to gradient-free optimization algorithms but rather associated with the\nLoRA candidate modules. In other words, once the candidates are determined, random seeds have\nminimal impact on the final performance. Hence, we posit that the observed instability primar-\nily arises from the inherent challenge of balancing the quantity and quality of the LoRA module\ncandidates.\nH\nTHE IMPACT OF THRESHOLD\nIn this section, we omitted the threshold in our implementation, and the results are summarized in\nTable 9. Our observations indicate that the removal of the threshold had minimal impact on the\nmajority of tasks, underscoring the robustness of the gradient-free optimization algorithm itself in\nmost cases. The algorithm efficiently identified reasonable ranges even without specific upper and\nlower bounds. However, three tasks, namely Date Understanding, Disambiguation and Hyperbaton,\nexhibited notable effects. The resulting performance decline led to an average decrease of 1.2%\ncompared to the setting with threshold. This highlights the significance of establishing a reasonable\nthreshold to mitigate extreme scenarios.\n20\nPreprint\nTable 9: The comparsion between LoraHub and LoraHub without threshold.\nTask\nLoraHubavg with threshold\nLoraHubavg without threshold\nBoolean Expressions\n55.5\n54.0\nCausal Judgement\n54.3\n54.8\nDate Understanding\n32.9\n17.7\nDisambiguation\n45.2\n40.6\nDyck Languages\n1.0\n1.1\nFormal Fallacies\n52.8\n51.7\nGeometric Shapes\n7.4\n6.7\nHyperbaton\n62.8\n55.5\nLogical Deduction\u00a7\n(five objects)\n36.1\n36.5\nLogical Deduction\u00a7\n(seven objects)\n36.8\n35.6\nLogical Deduction\u00a7\n(three objects)\n45.7\n49.9\nMovie Recommendation\n55.3\n59.3\nMultistep Arithmetic\n0.4\n0.7\nNavigate\n47.1\n47.6\nObject Counting\n33.7\n34.7\nPenguins in a Table\n35.9\n33.8\nReasoning about Colored Objects\n40.0\n37.9\nRuin Names\n24.4\n24.0\nSalient Translation Error Detection\n36.0\n37.1\nSnarks\n56.9\n51.6\nSports Understanding\n56.7\n55.9\nTemporal Sequences\n18.2\n16.7\nTracking Shuffled Objects\u00a7\n(five objects)\n12.3\n12.3\nTracking Shuffled Objects\u00a7\n(seven objects)\n7.7\n8.5\nTracking Shuffled Objects\u00a7\n(three objects)\n29.2\n29.8\nWeb of Lies\n50.1\n50.3\nWord Sorting\n1.1\n1.3\nAvg Performance Per Task\n34.7\n33.5\n21\n"
  },
  {
    "title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
    "link": "https://arxiv.org/pdf/2307.13692.pdf",
    "upvote": "17",
    "text": "ARB: Advanced Reasoning Benchmark for Large\nLanguage Models\nTomohiro Sawada1,2,\u2217, Daniel Paleka1,3, Alexander Havrilla1,2, Pranav Tadepalli1,2, Paula Vidas1,\nAlexander Kranias1,2, John J. Nay4,5, Kshitij Gupta1,6, Aran Komatsuzaki1,2,\u2021\u2021\n1 DuckAI 2 Georgia Tech 3 ETH Z\u00fcrich 4 Nomos AI\n5 Stanford University Center for Legal Informatics 6 MILA\nAbstract\nLarge Language Models (LLMs) have demonstrated remarkable performance on\nvarious quantitative reasoning and knowledge benchmarks. However, many of\nthese benchmarks are losing utility as LLMs get increasingly high scores, despite\nnot yet reaching expert performance in these domains. We introduce ARB, a\nnovel benchmark composed of advanced reasoning problems in multiple fields.\nARB presents a more challenging test than prior benchmarks, featuring problems\nin mathematics, physics, biology, chemistry, and law. As a subset of ARB, we\nintroduce a challenging set of math and physics problems which require advanced\nsymbolic reasoning and domain knowledge. We evaluate recent models such as\nGPT-4 and Claude on ARB and demonstrate that current models score well below\n50% on more demanding tasks. In order to improve both automatic and assisted\nevaluation capabilities, we introduce a rubric-based evaluation approach, allowing\nGPT-4 to score its own intermediate reasoning steps. Further, we conduct a human\nevaluation of the symbolic subset of ARB, finding promising agreement between\nannotators and GPT-4 rubric evaluation scores.\n1\nIntroduction\nIn recent years, models such as GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdh-\nery et al., 2022], and Chinchilla [Hoffmann et al., 2022] have shown increasing performance across a\nwide variety of natural language tasks ranging from translation to reasoning [Bubeck et al., 2023,\nLaskar et al., 2023]. This rapid progress has been closely tracked and assessed by evaluating LLMs on\nbenchmarks, which test model capabilities on a set of standardized problems. The GLUE benchmark\n[Wang et al., 2019b] for language understanding was first released in April 2018; but models such\nas BERT [Devlin et al., 2019] and GPT-2 [Radford et al., 2019] in the following year were already\npowerful enough to necessitate the \u201cSuperGLUE\u201d benchmark [Wang et al., 2019a]. Since then, the\nrace between language models and benchmarks has increasingly favored the former.\nScaling up, model sizes and datasets alike, has led to rapid improvements on various natural language\ntasks on benchmarks like BIG-bench [Srivastava et al., 2022] and HELM [Liang et al., 2022]. Neural\nscaling laws [Kaplan et al., 2020, Caballero et al., 2023, Alabdulmohsin et al., 2022] have been used\nto predict the behavior of large scale models on various metrics. Nevertheless, LLM performance\noften increases unpredictably [Wei et al., 2022a], especially on tasks that require reasoning abilities.\nPredictions of performance on ML benchmarks often underestimate the rate of progress [Steinhardt,\n2022]. Since progress has been faster than anticipated, new benchmarks need to be more difficult.\n*Email: tsawada@gatech.edu.\n\u2021\u2021Email: contact@duckai.org.\narXiv:2307.13692v2  [cs.CL]  28 Jul 2023\nModels such as ChatGPT have shown the ability to pass entry-level examinations in fields such as\nlaw [Bommarito II and Katz, 2022], medicine [Kung et al., 2023], economics [Caplan, 2023], and\nmathematics [Shakarian et al., 2023]. Nevertheless, LLM understanding of many fields is reportedly\nshallow and unreliable [Shapira et al., 2023]. Expert reasoning in domains with specialized knowledge\nis essential for automated systems to augment skilled professionals [Noy and Zhang, 2023].\nIn this paper, we introduce a new benchmark dataset, ARB (Advanced Reasoning Benchmark),\ndesigned to evaluate expert reasoning abilities in mathematics, physics, chemistry, biology, and law.\nTo make the benchmark more challenging than previous benchmarks, we extract graduate-level tasks\nfrom resources intended for domain professionals. The performance of current models such as GPT-4\non the quantitative parts of ARB is very low using standard prompting methods.\nOur dataset offers improvements over existing benchmarks:\n\u2022 Hundreds of problems requiring expert reasoning in quantitative subjects, where LLMs are\nknown to underperform;\n\u2022 A large percentage of the problems are short-answer and open response questions, in contrast\nto the multiple-choice questions that dominated earlier benchmarks.\nIn addition, we propose an automated rubric-based method allowing self-evaluation of intermediate\nreasoning steps. While not currently a substitute for human evaluation, rubrics generated by GPT-4\nhave good coverage, and self-evaluation scores track human grading surprisingly well.\nWe provide the instructions to access the dataset in the supplementary material.\n2\nRelated Work\nImproving the reasoning capabilities of LLMs has been a subject of recent interest, with a particular\nfocus on advanced prompting techniques [Wei et al., 2022b, Kojima et al., 2023, Wang et al., 2023,\nYao et al., 2023, Nye et al., 2021]. Such techniques have seen increasingly successful applications\nin solving reasoning problems involving commonsense reasoning and mathematics, by promoting\nactive reasoning processes within the LLMs before yielding final answers.\nModel architectures such as Minerva [Lewkowycz et al., 2022] have exemplified the enhancement of\nreasoning capabilities through fine-tuning on extensive datasets covering math and reasoning tasks.\nThis has yielded improved performance across several benchmarks, including MATH [Hendrycks\net al., 2021], GSM8K [Cobbe et al., 2021], and MMLU [Hendrycks et al., 2020]. Concurrently, other\nlines of research [Li et al., 2023, Lightman et al., 2023, Cobbe et al., 2021] have investigated the\napplication of verification techniques to augment and enhance LLM performance.\nMost of the aforementioned work has typically evaluated techniques against math benchmarks\n(e.g., GSM8K [Cobbe et al., 2021], MATH [Hendrycks et al., 2021], SVAMP [Patel et al., 2021],\nASDiv [Miao et al., 2020], AQuA [Ling et al., 2017], MAWPS [Koncel-Kedziorski et al., 2016],\nMultiArith [Roy and Roth, 2016]) and commonsense reasoning tasks (e.g., CSQA [Talmor et al.,\n2018], StrategyQA [Geva et al., 2021], HotpotQA [Yang et al., 2018]). Recently, several new\nbenchmarks have been introduced for reasoning and planning tasks, such as the GPT-Planning\nBenchmark [Valmeekam et al., 2023], ALERT Reasoning Benchmark [Yu et al., 2022], JEEBench\n[Arora et al., 2023]), and [Gendron et al., 2023]. Additionally, comprehensive evaluation suites like\nthe Chain-of-Thought Hub [Fu et al., 2023] have been proposed.\nDespite their utility, existing benchmarks are limited in difficulty, represent a restricted range of\nreasoning challenges, and do not necessarily mirror real-world tasks demanding complex reasoning.\nMoreover, recent advancements such as Minerva [Lewkowycz et al., 2022] have revealed that these\nbenchmarks may not offer sufficient challenge.\nThe rapid progress in LLM capabilities has led many to explore using LLMs in the LLM evaluation\npipeline. Apart from using LLMs to generate evaluation tasks [Zhang et al., 2022, Perez et al., 2022],\nLLMs have increasingly been used as a proxy for human evaluation [Chiang and Lee, 2023, Liu et al.,\n2023, Fu et al., 2023, Kocmi and Federmann, 2023]. Useful LLM-based evaluation for alignment has\nbeen done using rubrics [Bai et al., 2022]. We explore the efficacy of rubrics for evaluation when\napplied to highly complex math and physics problems.\n2\n3\nBenchmark\nThe key considerations when building a machine learning benchmark are:\n\u2022 Difficulty. Most tasks have to be out of reach of current models; a benchmark where many\nmodels score over 95% is not useful for tracking differential AI development.\n\u2022 Usefulness. The tested skills should correlate with generally useful human skills.\n\u2022 Ease of evaluation. It should be straightforward for the model creators to compare the\nperformances of different models. The scores should be interpretable.\n\u2022 Minimizing data contamination. A consistent issue with popular benchmarks is that\nthe recent LLMs contain some tasks in their training data [OpenAI, 2023]. This leads to\noverestimation of true model capabilities.\n\u2022 Connection to general capabilities. If a model is trained on data similar to the benchmark,\nit is possible it achieves high performance without generalization or \u201cintelligence\u201d, failing to\nsolve novel tasks of similar difficulty [Chollet, 2019]. Conversely, problems should not be\npathological or overly adversarial, to avoid the dangers of underclaiming [Bowman, 2021].\n3.1\nFormatting\nThe benchmark consists of three types of questions: multiple choice, short answer, and open response,\nin descending order of proportion in the dataset.\n\u2022 Multiple choice questions consist of a question and four to five possible answers, and\nthe correct answer is the one that best answers the question. They were sourced from\nstandardized tests, such as the MCAT and bar exam prep, and make up a large proportion of\nthe dataset due to their ease of grading.\n\u2022 Short answer questions, on the other hand, ask for final answers in the format of a short\nphrase or mathematical expression. They were sourced from problem books such as Souza\nand Silva [2008], Gelca and Andreescu [2017], and physics book series Lim and Qiang\n[2001], Lim [2007], Lim [1998], Lim et al. [2019], and Lim [1996]. We generally avoided\nalgebraic expressions, because of technical difficulties in the grading process.\nA given algebraic expression may have several equivalent forms (e.g. nontrivial functional\nrelations for the functions appearing in the final answer), and a grading scheme which\naccounts for all possible variations across our entire dataset is not feasible. Moreover,\nphysics problems often require answers introducing new notation that is not explicitly\nmentioned in the problem statement.\n\u2022 Open response questions are more challenging: they consist of a question and a blank\nspace for the answer. They were sourced from problem books and exams, such as the\nHarvard PhD comprehensive exams in mathematics [Harvard University, 2021]. Such tasks\nrequire manual grading. These questions are aspirational in nature, as current systems (e.g.\nChatGPT) cannot produce satisfactory responses, even for the \u201celementary\u201d problems.\n3.2\nMathematics\nThis part of the dataset is the most diverse. It includes contest mathematics problems as well as\n\u201cuniversity mathematics\u201d (i.e. mathematics traditionally taught in universities at the undergraduate\nand beginning graduate level). The contest problems are sourced from Gelca and Andreescu [2017]\nand Brayman and Kukush [2018], and the university mathematics problems are sourced from Souza\nand Silva [2008] and Harvard University [2021]. The dataset does not include high school contest\nproblems because those are already present in other well-known benchmarks [Hendrycks et al., 2021].\nThe Putnam and Brayman books both contain official solutions, which we also include in the dataset.\nThis can be useful for fully automating the grading process, which we leave to future work.\nFor university mathematics, we pick Souza and Silva [2008] for its large selection of \u201cstandard\u201d\nundergraduate mathematics problems, as well as many problems suitable for the short answer portions.\nWe also select Harvard University [2021] because it covers topics that other collections of exams\nrarely not cover, such as representation theory of finite groups and algebraic topology.\n3\nTable 1: Types of problems in the benchmark by subject area.\nSubject\nAnswer Type\nNumber\nMathematics\nNumerical\n52\nSymbolic\n34\nProof-like\n19\nPhysics\nNumerical\n80\nNumerical (w/ image)\n18\nSymbolic\n18\nSymbolic (w/ image)\n13\nLaw\nMultiple Choice\n627\nMCAT (Reading)\nMultiple Choice\n165\nMCAT (Science)\nMultiple Choice\n144\nMultiple Choice (w/ image)\n37\n3.3\nPhysics\nThe physics problems are structured similarly as the math problems. The main difference is that some\nphysics problems contain figures, and there are more problems with numerical answers. The problems\nwere sourced from the Major American Universities PhD Qualifying Questions and Solutions series\n[Zhongguo-Kexue-Jishu-Daxue, 1990].\n3.4\nMCAT\nThe MCAT test contains multiple choice problems testing biology, psychology, chemistry, physics,\nand reading comprehension. The MCAT problems are sampled from the third edition of McGraw-\nHill Education 3 MCAT Practice Tests [Campbell et al., 2017] and cover both science and reading\nquestions. This book was chosen as very few of these problems appear in standard web-searchable\nsources, limiting contamination. As in the previous categories, we pick problems which are self-\ncontained. Because some MCAT science questions are accompanied by images, we accompany such\nquestions with corresponding image files.\n3.5\nLaw\nApplying law involves the application logical reasoning, in addition to grasping legal knowledge. This\nmakes assessments of legal skills an especially attractive type of language model benchmark, where\nwe are attempting to assess the reasoning and intelligence of these models. Furthermore, if the models\nbetter understand law, they can be more reliable and ultimately more useful in real-world applications,\npotentially even increasing the efficiency and transparency of governments more broadly.\nMost lawyers in the U.S. go to law school, graduate, then study for the Bar Examination, and then\nmust pass the bar before going on to practice law professionally. To evaluate legal understanding of\nthe models, we use an older Bar Examination practice set that, to the best of our knowledge, is not\navailable online in a way that could have led to its inclusion in training data for the language models\nthat we are assessing. The practice bar exam we administer to the various language models covers\nmost major areas of law and therefore it tests legal reasoning and broad U.S. legal knowledge.\n4\nEvaluation\nWe evaluate current LLMs on all text-only problems in our dataset. Other LLM benchmark papers do\nnot evaluate on multimodal tasks due to the lack of good multimodal models; we follow suit. Given\npublic communications about GPT-4 [OpenAI, 2023] and Gemini [Ghahramani, 2023], it is likely the\nphysics and MCAT image problems will be useful for testing multimodal LLMs soon.\n4\nModels\nWe evaluate ChatGPT (gpt3.5-turbo-0301), GPT 3.5 (text-davinci-003),\nGPT-4 with 8k context length (gpt-4-0314), and Claude (claude-v1.3-100k). We evaluate\nall question types using task-specific instructions and chain of thought. In chat models, we put the\ninstructions as the system prompt; otherwise we put them at the beginning of the prompt.\nIn all problem types, in order to extract the model\u2019s final answer, we instruct the model to write its\nfinal answer at the end of the response after the delimiter ANSWER: . We then parse the model\ngenerated final answer as the remaining text after the delimiter. The response is marked as incorrect if\nthe delimiter is not found. Due to the differences in evaluation for multiple choice versus open-ended\nresponses, we adopt multiple evaluation procedures.\nMultiple choice\nTo evaluate multiple choice questions, we can simply compare the extracted final\nanswer to the ground truth. A response is considered correct if the extracted choice matches the\nground truth choice. With appropriate prompting, all models output a parsable answer > 97% of the\ntime. We conduct a separate manual evaluation on a sampled subset of the questions to check that our\nparsing procedure is not mischaracterizing the true performance of the model.\nNumerical\nTo evaluate problems with a numerical final answer, we first extract the delimited model\nanswer as above. In the physics problems, many answers are in units; we prompt the model with\ninformation about the unit, and instruct it to fully simplify its answer and omit any units. However,\nsometimes the model forgets to do either or both, and so we apply a series of regexes to remove units.\nWe then attempt to parse the result into a mathematical expression using Python\u2019s SymPy library\n[Meurer et al., 2017]. If this parsing fails, the answer is marked as incorrect. Once parsed, we score a\nthe model answer as correct if |model_answer\u2212ground_truth|\nground_truth\n< 0.01.\nSymbolic\nProblems with symbolic answers are less structured and harder to parse. To do so,\nwe again leverage SymPy, first normalizing expressions to contain a default set of variable names\nand then checking for equivalence up to a permutation of the variables. However this approach is\nerror-prone and only works for the subset of symbolic responses in a function form. More advanced\nresponses, such as those containing set notation, require human evaluation.\nProof-like\nNatural language proofs cannot be evaluated automatically; the authors with training in\nmathematics grade the proofs. Further manual human evaluation requires a thorough inspection of\nthe intermediate reasoning steps. This makes evaluation expensive in practice.\nModel-based evaluation\nTo address the difficulties in developing automated metrics for evaluating\nmore advanced problems, we experiment with two model based approaches. First, we prompt\nChatGPT to grade the equivalence of two symbolic expressions with score options 0 when the totally\nincorrect, 0.5 when the symbolic expressions are nearly the same e.g. equivalent up to a constant, and\n1 when they are an exact match. Our prompting strategy can be found in the supplementary material.\nMore generally, we evaluate the capabilities of GPT-4 to grade intermediate reasoning chains via a\nrubric-based evaluation approach. For symbolic and proof-like problems, we few-shot prompt GPT-4\nto create a 10-point rubric. This is done by handwriting a small set of initial rubrics for proof-like\nproblems and prompting the model with these examples and the ground truth reference solution. The\nmodel assigns point values to intermediate steps using the reference solution as a guide. This process\nis illustrated in the supplementary material.\nWith model generated rubrics in hand, we then evaluate each question against its rubric. This is done\nby again prompting GPT-4 to go step by step through the model answer and assign partial credit\nbased on the rubric. This provides a denser automatic evaluation metric on increasingly unstructured\nanswers. As a nice side benefit, it makes human evaluation of complex symbolic questions much\neasier, significantly reducing the amount of time required per question.\n4.1\nResults\nWe now evaluate gpt-4, gpt-3.5-turbo, text-davinci-003, and claude-v1.3 on\nARB. The results for the mechanically scored subjects are in Figure 1.\n5\nFigure 1: Accuracy of models over automatically scored components of the ARB benchmark.\nNumerical questions are evaluated with a relative error threshold of 10\u22122.\nWe see models generally do quite well on the multiple choice Law and MCAT subsets, but struggle\nsignificantly on questions with numerical final answers. GPT-4 is the only model capable of reliably\nsimplifying complex expressions, but even GPT-4 struggles to reliably perform arithmetic and\nsymbolic manipulations over long contexts.\nOn the multiple-choice questions, the only model that cannot reliably follow the answer formatting\ninstructions is gpt-3.5-turbo. This happens for a variety of reasons, including the model refusing\nto answer or to commit to a single answer choice. On the Law benchmark, gpt-3.5-turbo does\nnot output a parsable answer around 25% of the time. The other models exhibit this failure in less\nthan 5% of multiple-choice questions, with GPT-4 being correctly parsed over 99% of the time.\nWe see a similarly low performance profile across models on symbolic problems, reported in Table 2.\nTable 2: Manually parsed scores for symbolic answer questions.\nMath Symbolic\nPhysics Symbolic\ngpt-4-0314\n18%\n28%\ngpt-3.5-turbo-0301\n12%\n6%\ntext-davinci-003\n3%\n6%\nclaude-v1.3-100k\n3%\n11%\n4.2\nWhat Kind of Errors Do LLMs Make?\nThe GPT-4 evaluation paper [Bubeck et al., 2023] classified errors GPT-4 makes in single-pass\nevaluation on GSM8K [Cobbe et al., 2021] and MATH [Hendrycks et al., 2021] into three types:\narithmetic mistakes, misunderstood statement, and wrong approach. We make a more fine-grained\nanalysis and extend it to math and physics problems in our dataset. The results are in Table 3.\nThe errors current LLMs make on the Mathematics part of ARB fall into five general types:\n\u2022 Misunderstanding / answering only a part of the question / misread problem;\n\u2022 Wrong approach: the model\u2019s early chain of thought does not guess the right approach;\n6\nTable 3: Mistakes on mathematics and physics problems in ARB, GPT-4.\nMisread\nWrong\nLogical error\nArithmetic\nCorrect\nCorrect\nproblem\napproach\nor hallucination\nmistake\nanswer\nreasoning\nMath Numerical\n0%\n25%\n88%\n48%\n3%\n3%\nMath Symbolic\n16%\n50%\n29%\n4%\n16%\n16%\nMath Proof-like\n5%\n50%\n72%\n16%\nn/a\n5%\nPhysics Numerical\n0%\n80%\n53%\n6%\n6%\n6%\nPhysics Symbolic\n0%\n37%\n68%\n31%\n28%\n12%\n\u2022 Logical errors: the model uses a false implication between two statements;\n\u2022 Hallucinating facts or theorems: the model confabulates a statement that is false in general,\nor not applicable in context;\n\u2022 Arithmetic/calculation error: the model multiplies incorrectly, omits a term in an expression,\ngives a wrong numerical value for a fraction, and other similar mistakes.\nWe grade GPT-4 using the above as a guideline. Our grading of the model\u2019s CoT answers is\nnot mutually exclusive; if the model both uses an approach that doesn\u2019t go anywhere and makes\na calculation error in it, we count it towards both categories. Note that the errors might not be\nindependent: arithmetic mistakes could be more or less frequent in wrong approach solutions as\nopposed to the solutions with correct idea. We notice that the model is likely to make incorrect\nsimplifications to get to some final answer in approaches that cannot work; this is expected, as\nprompting the model to produce a solution with a final answer often leads it to produce some final\nanswer by any means.\nWhen the model outputs a chain of implications, it is not always clear whether some false statement is\ndue to a logical error, or it is a straight-out confabulation. We merge those two error types in Table 3.\nSome problems ask for multiple things to be proven or calculated. Our graders gave the model a\nscore of 0.5 if it correctly derived at least half of the \"subproblems\" (for example, homology groups\nof a given manifold). With this more benevolent form of grading, the performance of GPT-4 on the\nProof-like problems jumps to 16%. Where applicable, slight discrepancy with automatic evaluation\nis also possible due to the error tolerance.\nWe note that many of the problems in Physics Symbolic have correct symbolic answers even when\nthere are flaws in the chain of thought reasoning of GPT-4. This is likely due to some kind of\nmemorization, although not necessarily from the same sources: see Table 12 for an example.\nIt is possible that our graders underestimate the rate of arithmetic mistakes in some cases, especially\nwhen the approach is clearly wrong, or it is not clear whether a given error is due to faulty reasoning\nor due to a missed term in the calculations.\nFor the larger subsets (see Table 1), we subsample the problems to between 20 and 40 per subject\narea; this is enough for a ballpark estimate of the frequency of different errors, and is not worth\nincreasing because attributing error types is inherently fuzzy.\n5\nModel-based Rubric Evaluation\nAs reasoning tasks increase in complexity, it gets harder to evaluate model performance. Symbolic\nfinal answers are in some cases difficult to grade automatically. Further, we are often more interested\nin the correctness of the reasoning used to produce the final answer; but evaluating intermediate\nreasoning steps requires expert human supervision. An ideal solution would be to use LLMs as\nevaluators based on a reference solution; unfortunately, there are major reliability issues.\nTo improve reliability, we propose generating rubrics as an important component of the evaluation\nprocess. The model generates the rubric from the reference solution, then evaluates any solution\nbased on the generated rubric. To aid rubric generation, we give few-shot examples of human-written\nrubrics to the rubric-generating model run. We study this approach by conducting a human evaluation\nof GPT-4 generated rubrics and the GPT-4 grading of its own solutions using the generated rubrics.\n7\nWe rate the quality of GPT-4 generated rubrics by hand in the first two rows of Table 4. Likert scores\nfrom 1-5 are assigned to both the coverage of the rubric, i.e. how well it captures key subproblems,\nand the point breakdown. Rubric quality scores are reported in Table 5 for symbolic and proof-like\nproblems. We find GPT-4 designs rubrics which cover the crucial solution steps well, but struggles to\nproperly allocate points to each step based on relative importance. However, it is much better than\nGPT-3.5-turbo, which tends to over-allocate points to only one or two solution steps.\nTable 4: Evaluations of rubric quality and GPT-4 rubric evaluation failure cases. Rubric coverage\nand rubric point spread are on a 1-5 Likert scale. Alternative solutions is the percentage of correct\nsolutions found not covered by the rubric. Extra/reduced credit track how often GPT-4 erroneously\nassigns or deducts points. Hallucinated rubric tracks how often GPT-4 assigns points by referring to a\nrubric item not actually present in the rubric.\nPhysics Symbolic\nMath Symbolic\nProof-like\nRubric coverage\n4.42\n4.26\n3.94\nRubric point spread\n4.16\n4.00\n4.06\nAlternative solutions\n5%\n2%\n0%\nExtra credit\n27%\n18%\n40%\nReduced credit\n11%\n12%\n5%\nHallucinated rubric\n0%\n15%\n0%\nThe obvious limitation of rubric scoring is the case of correct solutions not covered by the rubric. We\nfind that on our benchmark, GPT-4 rarely generates a fully or even mostly partially correct solution\nthat does not follow the rubric. Once done rating the model generated rubrics, we then manually\ngrade GPT-4\u2019s solutions according to each rubric and compare the result to GPT-4\u2019s evaluation. We\nalso annotate, for each problem, both whether GPT-4 assigns credit inappropriately or fails to assign\ncredit when it should.\nTable 5: Average scores (out of 10 points) when assigned by human annotators versus GPT-4.\nCorrelation is the Pearson correlation coefficient between the two scores, over all problems.\nPhysics Symbolic\nMath Symbolic\nProof-like\nHuman eval score\n5.00\n3.13\n2.65\nModel eval score\n5.05\n3.37\n3.8\nCorrelation\n0.91\n0.78\n0.82\nWe find a moderately high correlation between GPT-4\u2019s evaluation score and the manual score. In\nsome cases, the model, assigns an extra point or two when compared to the annotated rubric score.\nHowever, the self-eval score almost never deviates more than two points from the ground truth. The\nmain failure mode we detect is the assignment of partial credit to attempted solutions completely\noutside the problem rubric, where the human evaluation score is always zero. Taken together, we\nbelieve these results suggest that rubric-based evaluation is a promising automated evaluation method.\nHaving established rubric-based evaluation as a (imperfect) proxy for correctness, we now comment\non the GPT-4 performance graded by the rubric. Table 5 shows GPT-4 is best at generating correct\nintermediate reasoning steps for physics questions. Inspecting the model outputs suggests that GPT-4\nis good at recalling relevant and useful concepts in physics for solving the relevant problem; however,\nit can struggle with the mathematical manipulations required to solve the problem. The model is\nworse at recognizing the correct concepts and formulating an appropriate plan for the math questions,\nparticularly for proof-like problems.\n6\nLimitations and Conclusion\nIn this paper, we presented ARB, a novel benchmark for evaluating advanced reasoning capabilities\nin large language models. Our dataset is composed of various problems from the sciences and law,\n8\nsourced from graduate-level exams and professional resources. Despite advancements in current\nLLMs, their performance remains very low on the quantitative subjects, in ARB\u2019s tasks. We also\nproposed a rubric-based self-evaluation method, enabling LLMs to grade their own reasoning. This\nmethod is not yet reliable enough to replace human grading. We hope this method can be extended to\nmore reliable and cheap testing of complex model outputs.\nAs with all other benchmarks that are not created anew and kept secret, it is possible there is data\ncontamination. For example, the MCAT books are not available for free in most jurisdictions, but it\ncertainly possible that some model creators have trained on it anyway.\nFinally, the benchmark does not remotely cover all aspects of human ability; a model solving\nthis benchmark perfectly could still be much worse than most educated people in many aspects.\nNevertheless, we hope that increasing the difficulty standards helps the research community ground\nthe performance of increasingly powerful models more accurately.\nAcknowledgments and Disclosure of Funding\nWe thank Jeffrey Deng for developing and documenting the API, and building the project website.\nWe would also like to thank Raunak Chowdhuri for helpful comments, and Zhangir Azerbayev for\nuseful discussions early on in the project. TS is supported by NSF grant 1745583.\nReferences\nIbrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language\nand vision. Advances in Neural Information Processing Systems, 35:22300\u201322312, 2022.\nDaman Arora, Himanshu Gaurav Singh, and Mausam. Have LLMs advanced enough? A challenging problem\nsolving benchmark for Large Language Models, 2023.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,\nAnna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah,\nDanny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr,\nJared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael\nSellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson,\nSam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy\nTelleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben\nMann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI:\nHarmlessness from AI, feedback, 2022.\nBarbri. Barbri Practice Questions: Multistate Testing Practice Questions. Thomson/Bar/Bri, 2007. ISBN\n9780314174017.\nMichael Bommarito II and Daniel Martin Katz. GPT takes the bar exam. arXiv preprint arXiv:2212.14402,\n2022.\nSamuel R. Bowman. The dangers of underclaiming: Reasons for caution when reporting how NLP systems fail,\n2021.\nVolodymyr Brayman and A. G. Kukush. Undergraduate Mathematics Competitions (1995-2016): Taras\nShevchenko National University of Kyiv. Springer, 2018.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,\nYin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments\nwith GPT-4. arXiv preprint arXiv:2303.12712, 2023.\nEthan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws, 2023.\n9\nCandice McCloskey Campbell, Shaun Murphree, Jennifer M. Warner, Amy B. Wachholz, Kathy A. Zahler,\nand George J. Hademenos. McGraw-Hill Education 3 MCAT Practice Tests, Third Edition. McGraw-Hill\nEducation, Jan 2017. ISBN 1259859622.\nBryan Caplan. GPT retakes my midterm and gets an A, 2023. URL https://betonit.substack.com/\np/gpt-retakes-my-midterm-and-gets-an.\nCheng-Han Chiang and Hung-yi Lee. Can Large Language Models be an alternative to human evaluations?\narXiv e-prints, art. arXiv:2305.01937, may 2023. doi: 10.48550/arXiv.2305.01937.\nFran\u00e7ois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prab-\nhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\nIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with Pathways, 2022. URL\nhttps://arxiv.org/abs/2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and\nJohn Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL\nhttps://arxiv.org/abs/2110.14168.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, jun 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. GPTScore: Evaluate as you desire. arXiv e-prints,\nart. arXiv:2302.04166, feb 2023. doi: 10.48550/arXiv.2302.04166.\nYao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A continuous\neffort to measure large language models\u2019 reasoning performance, 2023.\nR\u02d8azvan Gelca and Titu Andreescu. Putnam and beyond. Springer, 2017.\nGa\u00ebl Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. Large language models are not abstract\nreasoners, 2023.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a\nlaptop? a question answering benchmark with implicit reasoning strategies, 2021.\nZoubin Ghahramani. Introducing PaLM 2, 2023. URL https://blog.google/technology/ai/\ngoogle-palm-2-ai-large-language-model.\nDepartment of Mathematics Harvard University. Qualifying examination for fall 2021, Aug 2021. URL\nhttps://www.math.harvard.edu/media/quals-F21_with_solutions.pdf.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding, 2020. URL https://arxiv.org/abs/2009.\n03300.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. Measuring mathematical problem solving with the MATH dataset. CoRR, abs/2103.03874, 2021.\nURL https://arxiv.org/abs/2103.03874.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie\nMillican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\nElsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.\nURL https://arxiv.org/abs/2203.15556.\n10\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\nDaniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. GPT-4 passes the bar exam.\nAvailable at SSRN 4389233, 2023.\nTom Kocmi and Christian Federmann. Large language models are state-of-the-art evaluators of translation\nquality. arXiv e-prints, art. arXiv:2302.14520, feb 2023. doi: 10.48550/arXiv.2302.14520.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language\nmodels are zero-shot reasoners, 2023.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A\nmath word problem repository. In Proceedings of the 2016 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, pages 1152\u20131157, San\nDiego, California, jun 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL\nhttps://aclanthology.org/N16-1136.\nTiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepa\u00f1o, Maria\nMadriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et al. Performance of ChatGPT on\nUSMLE: Potential for AI-assisted medical education using large language models. PLOS Digital Health, 2\n(2):e0000198, 2023.\nMd Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and\nJimmy Xiangji Huang. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets,\n2023.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy\nGur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL\nhttps://arxiv.org/abs/2206.14858.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making large\nlanguage models better reasoners with step-aware verifier, 2023.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,\nDeepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang,\nChristian Cosgrove, Christopher D. Manning, Christopher R\u00e9, Diana Acosta-Navas, Drew A. Hudson,\nEric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav\nSanthanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri\nChatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar,\nSurya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang,\nXuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models, 2022.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John\nSchulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step, 2023.\nSwee Cheng Lim, Choy Heng Lai, and Leong Chuan Kwek. Problems and solutions on optics. World Scientific,\n2019.\nYung-kuo Lim. Problems and solutions on thermodynamics and Statistical Mechanics. World Scientific, 1996.\nYung-kuo Lim. Problems and solutions in quantum mechanics: Major, American universities ph. D. qualifying\nquestions and, solutions. World Scientific, 1998.\nYung-kuo Lim. Problems and solutions on electromagnetism. World Scientific Pub. Co, 2007.\nYung-kuo Lim and Yuan-qi Qiang. Problems and solutions on Mechanics. World Scientif., 2001.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation:\nLearning, to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 158\u2013167, Vancouver,\nCanada, jul 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https:\n//aclanthology.org/P17-1015.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation\nusing GPT-4 with better human alignment. arXiv e-prints, art. arXiv:2303.16634, mar 2023. doi: 10.48550/\narXiv.2303.16634.\n11\nAaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ond\u02c7rej \u02c7Cert\u00edk, Sergey B. Kirpichev, Matthew Rocklin,\nAMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger,\nRichard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson, Fabian Pedregosa,\nMatthew J. Curry, Andy R. Terrel, \u0160t\u02c7ep\u00e1n Rou\u02c7cka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal, Robert\nCimrman, and Anthony Scopatz. Sympy: Symbolic computing in python. PeerJ Computer Science, 3:\ne103, jan 2017. ISSN 2376-5992. doi: 10.7717/peerj-cs.103. URL https://doi.org/10.7717/\npeerj-cs.103.\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing English\nmath word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 975\u2013984, Online, jul 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n2020.acl-main.92. URL https://aclanthology.org/2020.acl-main.92.\nShakked Noy and Whitney Zhang. Experimental evidence on the productivity effects of generative artificial\nintelligence. Available at SSRN 4375283, 2023.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show\nyour work: Scratchpads for intermediate computation with language models, 2021.\nOpenAI. GPT-4 technical report, 2023.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word\nproblems?, 2021.\nEthan Perez, Sam Ringer, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine\nOlsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written\nevaluations. arXiv preprint arXiv:2212.09251, 2022.\nMahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. Data Cards: Purposeful and Transparent Dataset\nDocumentation for Responsible AI. arXiv e-prints, art. arXiv:2204.01075, April 2022. doi: 10.48550/arXiv.\n2204.01075.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nSubhro Roy and Dan Roth. Solving general arithmetic word problems, 2016.\nPaulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu. An independent evaluation of\nChatGPT on mathematical word problems (MWP). arXiv preprint arXiv:2302.13814, 2023.\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and\nVered Shwartz. Clever Hans or Neural Theory of Mind? Stress testing social reasoning in large language\nmodels. arXiv preprint arXiv:2305.14763, 2023.\nPaulo N de Souza and Jorge N. Silva. Berkeley problems in Mathematics. Springer New York, 2008.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\ndoi: 10.48550/ARXIV.2206.04615. URL https://arxiv.org/abs/2206.04615.\nJacob Steinhardt. AI forecasting: One year in, 2022. URL https://bounded-regret.ghost.io/\nai-forecasting-one-year-in/.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering\nchallenge targeting commonsense knowledge. CoRR, abs/1811.00937, 2018. URL http://arxiv.org/\nabs/1811.00937.\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models\nstill can\u2019t plan (a benchmark for LLMs on planning and reasoning about change), 2023.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding\nsystems, 2019a.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding. 2019b. In the Proceedings\nof ICLR.\n12\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff\nDean, and William Fedus. Emergent abilities of large language models, 2022a.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. Chain of thought prompting elicits reasoning in large language models, 2022b. URL https:\n//arxiv.org/abs/2201.11903.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering, 2018.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models, 2023.\nPing Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, and Asli Celikyilmaz.\nALERT: Adapting language models to reasoning tasks, 2022.\nSarah Zhang, Reece Shuttleworth, Derek Austin, Yann Hicke, Leonard Tang, Sathwik Karnik, Darnell Granberry,\nand Iddo Drori. A dataset and benchmark for automatically answering and generating machine learning final\nexams. arXiv preprint arXiv:2206.05442, 2022.\nHefei Zhongguo-Kexue-Jishu-Daxue. Major American universities Ph. D. qualifying questions and solutions. 5.\nProblems and solutions on thermodynamics and statistical mechanics. World Scientific, 1990.\nA\nDatasheet\nWe present the data card, following the format proposed by Pushkarna et al. [2022].\nDataset Owners. contact@duckai.org.\nTable 6: Data overview.\nSubject\nTask Type\nSource\nMathematics\nContest problems\nGelca and Andreescu [2017], Brayman and Kukush [2018]\nUniversity math proof\nSouza and Silva [2008], Harvard University [2021]\nPhysics\nPhD qualifying exam\nZhongguo-Kexue-Jishu-Daxue [1990]\nLaw\nUS Law Standardized Exam\nBarbri [2007]\nMCAT\nReading comprehension\n[Campbell et al., 2017]\nMCAT\nCollege science\n[Campbell et al., 2017]\nDataset Overview. See Table 6.\nRisk and Mitigation. There is little risk associated with this dataset, as it is intended for benchmark-\ning reasoning capabilities of models, and it is too small to be used for advancing capabilities.\nMaintenance Status. Limited Maintenance. The data will not get major updates, but any technical\nissues will be addressed.\nMaintenance Plan. Any technical issues will be addressed.\n\u2022 Versioning. No new versions are planned.\n\u2022 Updates. Updates are limited to bug and error fixes.\n\u2022 Errors. Error handling will be considered case by case.\n\u2022 Feedback. contact@duckai.org.\n13\nExample: Typical Data Point. Each data point of the dataset consist of a pair of problem statement\nand ground truth solution. Table 7, Table 9 and Table 10 include problem statement and ground truth\nsolution of typical data points.\nSensitive Human Attributes. We have not found any sensitive human attributes in our dataset.\nData Distributions. Table Section 3.1 shows the number of problems for each subject area and\nanswer type. Text entries (problem statement, ground truth solution, ground truth answer) for all\ncategories are in LaTeX (although obviously, the non-quantitative subjects have very few mathematical\nexpressions).\nB\nDataset format\nThe benchmark dataset is available in .jsonl format, containing problem statements, ground truth\nsolutions, and final ground truth answers for each entry. We additionally include metadata such as\nsubject names and problem topics, where available.\nWe chose the four subject areas discussed earlier for several reasons. Primarily, the dataset focuses\non math and physics, as these subjects present more challenging problems than existing benchmarks.\nHowever, to ensure a comprehensive evaluation of models, we also included subjects like Law and\nMCAT. This inclusion allows for assessing model performance across a wider range of technical\ndomains, beyond the quantitative sciences.\nAlthough previous works have evaluated recent models on law [Katz et al., 2023], we draw upon\nthe established importance of broad benchmarks like SuperGLUE [Wang et al., 2019a]. Making\na benchmark more comprehensive expands the evaluation scope while enhancing the dataset\u2019s\nsignificance in the wider AI research context.\nC\nAccessing the dataset\nWe facilitate access to the dataset through API calls*. The problems can be accessed by the different\nsplits and subject areas.\nWe use an API to host the data (rather than making it publicly available on popular platforms such as\nHuggingFace or Github) out of concern for data contamination. Many models released in the last few\nyears have been trained on benchmark datasets OpenAI [2023], an approach that could inadvertently\ninflate the model\u2019s performance on the associated benchmark. There are several proposed strategies\nto mitigate this problem, including dataset poisoning and canary text methods.However, restricting\ndataset accessibility to web crawlers seemed to be the only way to ensure integrity of our dataset for\nfuture uses.\nD\nModel-based Rubric Eval\nTable 7 presents a GPT-4 generated rubric and self-evaluation for symbolic math questions. GPT-4\ndoes a good job breaking the problem into important sub-parts of the reference solution. Further, it\nensures a well-balanced point distribution among sub-problems, assigning similar point values to\neach sub-part. In contrast, less powerful models like GPT-3.5-turbo struggle to generate good rubrics,\nas they tend to under-specify sub-problems and allocate too many points to irrelevant steps.\nThe main downside on evaluating solutions only on the final answer is that the metric is not smooth;\na small mistake in the middle of the solution always leads to an incorrect final answer. Grading\nusing a rubric shows that GPT-4 generates some useful ideas, for example the correct formula for\nparameterizing an ellipse. The model also correctly identifies that the question\u2019s area is optimized by\nan isosceles triangle. Despite this, it is unable to correctly compute the final answer due to an earlier\nmistake in the response. This indicates that GPT-4 has some problem-solving abilities, but struggles\nto detect or recover from earlier errors in generation.\n*The API can be accessed through the following: https://arb.duckai.org/api/lib. The documentation can be\nfound here: https://app.swaggerhub.com/apis-docs/arb-dataset/arb-api/1.0.5\n14\nD.1\nUsing ChatGPT for Symbolic Evaluation\nUnlike GPT-4, GPT-3.5-turbo is not to write rubrics with good coverage of the reference solution\nand a fair point breakdown. Often the model will over-simplify the rubric and allocate far too many\npoints to non-essential parts of the problem. However, GPT-3.5-turbo does possess some ability to\nreason about complex symbolic expressions. Motivated by this, we asked the model to grade the\nfinal answers to symbolic math and physics problems. While much easier to grade than intermediate\nreasoning steps, more involved symbolic expressions still require human evaluation to compare\naccurately. Using cheap models like GPT-3.5-turbo to automate this symbolic equivalence boosts\nour abilities to evaluate models on this more complicated class of reasoning problems. We prompt\nGPT-3.5-turbo to compare extracted model generated symbolic final answers from GPT-4 to the\nreference answer and record results in Table 8.\nGPT-3.5-turbo is surprisingly good at not equating non-equivalent symbolic statements, achieving\na false positive rate of 0. However, this comes at of the model often not assigning credit when it\nshould, leading to an underestimate of the true performance of the graded answers on symbolic tasks.\nIn particular, GPT-3.5-turbo often fails when comparing expressions with more than 3 variables.\nFurthermore, we observe in several cases the student model GPT-4 is able to correctly generate the\nfinal answer, but does not present it in the expected format, making it impossible for GPT-3.5-turbo\nto correctly compare against the ground truth reference. These errors could potentially be avoided by\ngiving the model access to the entire student generated reasoning trace, but we expect this to be a\nminimal help and perhaps even harm performance.\nE\nInteresting Examples\nMath Example.\nOne common way GPT-4 gets numerical questions wrong is when it do some\nformal manipulations follows by a blackbox numerical calculation after which the model spits out an\n(incorrect) answer. (Table 10) The formal manipulation does not simplify the problem, and so all the\nwork of solving the problem is done in the final step of the calculation.\nMCAT Example.\nGPT-4 get\u2019s confused when meanings of words are implicit in prerequisite\nknowledge or contexts. In one example about DNA replication (Table 9), the model correctly\nidentifies that the radioactive thymine is present in the two strands of nucleotides from the original\nDNA, it fails to deduce that both of the resulting double helices are radioactive. This seems to be\nbecause the model confuses the word \"DNA\" with \"strands\" of the DNA. When looking at choice C,\nthe model (incorrectly) assumes that each of the four strands in the new double helices are radioactive,\nwhen it is clear from context that the choice is referring to the radioactive molecule being present\nsomewhere in each double helix (not necessarily in each strand). Because of this misconception, the\nmodel chooses D.\nLaw Example.\nAn unexpected mistake from GPT-4 in answering law questions is where the model\nreads too much into an answer choice. For example, GPT-4 incorrectly produced this answer and\nreasoning:\n\u201cB: This answer choice suggests that when two crossing offers are identical, one\nwill be treated as an offer and the other as an acceptance. This accurately reflects\nthe situation between Mom and Huck, as they both agreed on the same price.\u201d\nAnd made this final answer choice:\n\u201cYes, because when two crossing offers are identical in import, one will be treated\nas an offer and the other as an acceptance.\u201d\nThe error GPT-4 made is treating the statement in the answer choice (\u201cwhen two crossing offers are\nidentical, one will be treated as an offer and the other as an acceptance\u201d) as a fact, and then making a\nchoice based on that fact being true. Better prompting could likely precent errors of this type.\n15\nF\nMemorization\nWhile it is difficult to quantify potential problem leakage and memorization of similar problems, some\noutputs suggest this might be a relevant factor. In Table 12, GPT-3.5-turbo appears to be memorizing\nthe answer to this symbolic physics problem, given that it is getting the formula for capacitance C\ncorrect despite having the wrong reasoning. In contrast, GPT-4 stays faithful to its reasoning chain,\nresulting in a slightly inaccurate final answer. Note that the overall performance of all models we test\nis still somewhat low, hence not majorly affected by memorization of similar problems.\nG\nPrompts for Sections 4 and 5\nFor OpenAI chat models (gpt-4 and gpt-3.5-turbo), following best practices, we prepend\nthe system prompt. We do not use the system prompt for the other models due to there not being a\nwell-established way to do so, and out of concerns of hurting performance on the other models. We\nnote that omitting the system prompt should have little effect on the performance of the other models,\nas the directions of the system prompt are essentially restated in our user prompts.\nH\nChecklist Information.\nDataset Intended Uses. The ARB benchmark dataset, documented within the paper, is primarily\nintended for research purposes. We do not advocate for this dataset to train models that help students\ncheat on exams. We hope that the research community will use this benchmark to better assess\nreasoning capabilities of language models.\nAuthor Statement and License. We bear all responsibility in case of violation of rights. The ARB\ndataset is licensed under CC BY 4.0, and all helper code we release is under the MIT license. For all\nproblems originating in books listed in Section 3, following [Hendrycks et al., 2021], we abide by\nFair Use \u00a7107: \"the fair use of a copyrighted work, including such use by ... scholarship, or research,\nis not an infringement of copyright\", where fair use is determined by \"the purpose and character of the\nuse, including whether such use is of a commercial nature or is for nonprofit educational purposes\"\nand \"the effect of the use upon the potential market for or value of the copyrighted work\".\n16\nTable 7: Example model generated rubric and evaluation. Evaluation with a rubric reveals the model\nbegins with the model makes good partial progress but is unable to arrive at the correct final answer.\nProblem\nStatement\nFind the maximum area of all triangles that can be inscribed in an ellipse with semiaxes a\nand b, and describe the triangles that have maximum area.\nGround\nTruth\nSolution\nUsing the parameterization Using the parameterization\nx = a cos t, y = b sin t,\na triple of points on the ellipse is given by\n(a cos ti, b sin ti) ,\ni = 1, 2, 3.\nSo the area of an inscribed triangle is given by\n1\n2\n\f\f\f\f\f\n1\na cos t1\nb sin t1\n1\na cos t2\nb sin t2\n1\na cos t3\nb sin t3\n\f\f\f\f\f = ab\n2\n\f\f\f\f\f\n1\ncos t1\nsin t1\n1\ncos t2\nsin t2\n1\ncos t3\nsin t3\n\f\f\f\f\f\nwhich is ab times the area of a triangle inscribed in the unit circle. In the case of the circle,\namong all inscribed triangles with a given base 2w(0 < w \u2a7d 1), the one of maximum\narea is an isosceles triangle whose area equals\ng(w) = w\n\u0010\n1 +\np\n1 \u2212 w2\n\u0011\n.\nUsing elementary calculus one finds that the maximum of g on the interval 0 \u2a7d w \u2a7d\n1 occurs at w =\n\u221a\n3/2, corresponding to an equilateral triangle, and equals 3\n\u221a\n3/4.\nAlternatively, fixing one side of the triangle as the basis, we easily see that among all the\ninscribed triangles the one with the greatest area is isosceles because of the maximum\nheight, showing that the angle at the basis is the same. Fixing another side we see that the\ntriangle is indeed equilateral. Hence, the area is maximal when\nt2 = t1 + 2\u03c0\n3 and t3 = t2 + 2\u03c0\n3\nthat is, when the corresponding triangle inscribed in the unit circle is regular.\nFor the ellipse with semiaxes a, b, this corresponds to an inscribed triangle with maximum\narea equals 3ab\n\u221a\n3/4.\nModel\nGenerated\nRubric\n\u2022 Parameterize the ellipse using x = a cos t, y = b sin t (2 points).\n\u2022 Write the expression for the area of an inscribed triangle using the determinant\nformula (2 points):\nab\n2\n\f\f\f\f\f\n1\ncos t1\nsin t1\n1\ncos t2\nsin t2\n1\ncos t3\nsin t3\n\f\f\f\f\f\n(1)\n\u2022 Connect the problem to the maximum area of an inscribed triangle in the unit\ncircle (2 points).\n\u2022 Correctly identify that the maximum area occurs when the inscribed triangle is\nequilateral (2 points).\n\u2022 Find the maximum area of an inscribed triangle in the ellipse, and provide the\ncorrect answer (2 points):\n3ab\n\u221a\n3\n4\n(2)\n17\nTable 7 (continued)\nModel\nResponse\nTo maximize the area of the inscribed triangle, we can use the following approach:\n1. Parameterize the ellipse: The equation of an ellipse with semiaxes a and b is given\nby x2\na2 + y2\nb2 = 1. We can parameterize the ellipse using the parameter t as follows:\nx = a cos(t) and y = b sin(t), where 0 \u2264 t \u2264 2\u03c0.\n2.\nDetermine\nthe\nvertices\nof\nthe\ntriangle:\nLet\nA(a cos(t1), b sin(t1)),\nB(a cos(t2), b sin(t2)), and C(a cos(t3), b sin(t3)) be the vertices of the inscribed\ntriangle, where 0 \u2264 t1, t2, t3 \u2264 2\u03c0.\n3. Calculate the area of the triangle: Using the Shoelace Theorem, the area of the triangle\nis given by:\nArea = 1\n2\n\f\f\f\f\f\n3\nX\ni=1\nxiyi+1 \u2212\n3\nX\ni=1\nxi+1yi\n\f\f\f\f\f\nwhere (x4, y4) = (x1, y1).\nSubstituting the parameterized coordinates, we get:\nArea = 1\n2|a2 cos(t1) sin(t2) + a2 cos(t2) sin(t3)\n+a2 cos(t3) sin(t1) \u2212 a2 cos(t2) sin(t1) \u2212 a2 cos(t3) sin(t2) \u2212 a2 cos(t1) sin(t3)|\nSimplifying the expression, we get:\nArea = a2\n2 | cos(t1) sin(t2) \u2212 cos(t2) sin(t1)+\ncos(t2) sin(t3) \u2212 cos(t3) sin(t2) + cos(t3) sin(t1) \u2212 cos(t1) sin(t3)|\nUsing the sine difference identity, sin(A \u2212 B) = sin(A) cos(B) \u2212 cos(A) sin(B), we can\nrewrite the area as:\nArea = a2\n2 |sin(t1 \u2212 t2) + sin(t2 \u2212 t3) + sin(t3 \u2212 t1)|\n4. Maximize the area: Notice that the maximum value of each sine term is 1, and this\noccurs when the angles t1, t2, t3 are equally spaced around the ellipse, i.e., when the\ntriangle is equilateral. In this case, the area becomes:\nArea = a2\n2 |1 + 1 + 1| = 3a2\n2\n5. Relate the maximum area to the semiaxes: Since the triangle is equilateral, the ratio of\nthe area of the triangle to the area of the ellipse is constant. The area of the ellipse is given\nby \u03c0ab. Thus, we can write:\nAreamax\n\u03c0ab\n=\n3a2\n2\n\u03c0ab\nSolving for the maximum area, we get:\nAreamax = 3a2\n2\n\u00b7 \u03c0ab = 3\u03c0a3b\n2\nThus, the maximum area of an inscribed triangle in an ellipse with semiaxes a and b is\n3\u03c0a3b\n2\n, and the triangles with maximum area are equilateral.\nANSWER: 3\u03c0a3b\n2\n18\nTable 7 (continued)\nRubric\nBased\nEvaluation\n1. The student parameterized the ellipse correctly using x = a cos(t) and y = b sin(t). (2\npoints)\n2. The student used the Shoelace Theorem to find the area of the triangle, but did not use\nthe determinant formula as specified in the rubric. (0 points)\n3. The student did not explicitly connect the problem to the maximum area of an inscribed\ntriangle in the unit circle. (0 points)\n4. The student correctly identified that the maximum area occurs when the inscribed\ntriangle is equilateral. (2 points)\n5. The student found the maximum area of the inscribed triangle in the ellipse, but their\nanswer is incorrect. The correct answer is 3ab\n\u221a\n3\n4\n, not 3\u03c0a3b\n2\n. (0 points)\nRubric Score: 4 points\nTable 8: Performance of GPT-3.5-turbo on symbolic equivalence versus human ground truth. The\nmodel achieves a false positive rate of 0, but allows many false negatives. This suggests there are\nmany cases in which the model does not recognize equivalent expressions.\nPhysics Symbolic\nMath Symbolic\nTrue positives\n3\n4\nFalse positives\n0\n0\nTrue negatives\n9\n22\nFalse negatives\n6\n8\nAccuracy\n0.67\n0.76\n19\nTable 9: Problem, solution and GPT-4 response for a question from the MCAT part of the benchmark.\nThe model mistakes two copies of a DNA with the two amino acid strands in a DNA.\nProblem\nStatement\nIf you replaced the thymine in a double helix with radioactive thymine and you allowed\nDNA replication to occur once, which of the following results would you expect? Choices:\nA: The DNA would not be radioactive in either of the double helices.\nB: The mRNA made from the double helices would be radioactive.\nC: The DNA in each of the double helices would be radioactive.\nD: The DNA in one of the double helices would be radioactive but not in the other one.\nGround\nTruth\nSolution\nThe correct answer is C. During semiconservative replication, the DNA helix unwinds so\nthat both strands of DNA can serve as a template. Both strands are copied, producing two\ndouble helices. Each one consists of a template strand and a newly synthesized strand.\nModel\nResponse\nFirst, let\u2019s consider the process of DNA replication.\nDNA replication is a semi-\nconservative process, which means that each new double helix formed contains one\noriginal strand and one newly synthesized strand. In this case, the original strand contains\nradioactive thymine.\nNow let\u2019s analyze the answer choices:\nA: The DNA would not be radioactive in either of the double helices.\nThis choice is incorrect because we know that the original strand contains radioactive\nthymine, and since replication is semi-conservative, each new double helix will have one\noriginal strand with radioactive thymine.\nB: The mRNA made from the double helices would be radioactive.\nThis choice is unrelated to the question since it concerns mRNA instead of the DNA\nreplication process. Therefore, we can rule out this choice.\nC: The DNA in each of the double helices would be radioactive.\nThis choice suggests that both strands in each new double helix would be radioactive.\nHowever, we know that only the original strand contains radioactive thymine, and the\nnewly synthesized strand does not. Thus, this choice is not correct.\nD: The DNA in one of the double helices would be radioactive but not in the other one.\nThis choice aligns with the concept of semi-conservative replication. Each new double\nhelix contains one original strand (with radioactive thymine) and one newly synthesized\nstrand (without radioactive thymine).\nTherefore, the correct answer is:\nANSWER: D\n20\nTable 10: Problem, solution and GPT-4 sesponse for a question from the Math Numerical part of\nthe benchmark. The model often returns a numerical answer after doing some internal black-box\nnumerical calculation, which is rarely correct.\nProblem\nStatement\nCompute\nZ \u03c0\n0\nx sin x\n1 + sin2 xdx.\nGround\nTruth\nSolution\nWe use the example from the introduction for the particular function f(x) =\nx\n1+x2 to\ntransform the integral into\n\u03c0\nZ\n\u03c0\n2\n0\nsin x\n1 + sin2 xdx.\nThis is the same as\n\u03c0\nZ\n\u03c0\n2\n0\n\u2212 d(cos x)\n2 \u2212 cos2 x,\nwhich with the substitution t = cos x becomes\n\u03c0\nZ 1\n0\n1\n2 \u2212 t2 dt =\n\u03c0\n2\n\u221a\n2 ln\n\u221a\n2 + t\n\u221a\n2 \u2212 t\n\f\f\f\f\f\n1\n0\n=\n\u03c0\n2\n\u221a\n2 ln\n\u221a\n2 + 1\n\u221a\n2 \u2212 1\nModel\nResponse\nTo compute the integral\nZ \u03c0\n0\nx sin x\n1 + sin2 xdx,\nwe can use integration by parts. Let u = x and dv =\nsin x\n1 + sin2 xdx. Then, we have\ndu = dx and v =\nZ\nsin x\n1 + sin2 xdx. To find the integral of v, we can use the substitution\nmethod. Let t = sin x, then dt = cos xdx. The integral becomes:\nZ\nt\n1 + t2 dt = 1\n2\nZ\n2t\n1 + t2 dt = 1\n2 ln(1 + t2) + C\nNow, we can substitute back t = sin x, and we get:\nv = 1\n2 ln(1 + sin2 x) + C\nNow, we can apply integration by parts:\nZ \u03c0\n0\nx sin x\n1 + sin2 xdx = uv\n\f\f\u03c0\n0 \u2212\nZ \u03c0\n0\nvdu =\n\u0014\nx \u00b7 1\n2 ln(1 + sin2 x)\n\u0015\u03c0\n0\n\u2212\nZ \u03c0\n0\n1\n2ln(1+sin2 x)dx\nThe first term is:\n\u0014\nx \u00b7 1\n2 ln(1 + sin2 x)\n\u0015\u03c0\n0\n= \u03c0 \u00b7 1\n2 ln(1) \u2212 0 \u00b7 1\n2 ln(1 + sin2 0) = 0\nNow, we need to compute the second term:\n\u2212\nZ \u03c0\n0\n1\n2 ln(1 + sin2 x)dx\nUnfortunately, this integral does not have a closed-form solution in terms of elementary\nfunctions. However, it is possible to compute its numerical value using numerical integra-\ntion methods, such as the trapezoidal rule, Simpson\u2019s rule, or Gaussian quadrature.\nUsing numerical integration, we can approximate the value of the integral to be approxi-\nmately \u22121.9348. Therefore, the final answer is: ANSWER: \u22121.9348.\n21\nTable 11: Problem, solution and GPT-4 response for a question from the law part of the benchmark.\nProblem\nStatement\nMom, a wealthy woman, wished to buy her son, Sylvester, an expensive Rolls-Royce for a\nwedding present. She visited Dealership, a Rolls-Royce dealer, several times, looked at\nmany cars, and discussed possible sales prices with Huck, a salesman. On May 15, after\nmuch discussion, Huck and Mom signed a writing that stated, \"If we can agree on price on\nor before June 1, Mom agrees to buy and Huck, for Dealership, agrees to sell one yellow\nRolls-Royce Silver Streak, serial number XO857623.\" On May 20, Mom dispatched a\nletter to Huck stating, \"I will buy the Rolls-Royce for 150, 000.\" On the same day, Huck\ndispatched a letter to Mom, stating, \"I will sell the Rolls-Royce for 150, 000.\" Has a valid\ncontract been formed between Dealership and Mom?\nAnswer Candidates:\nA.) Yes, because the May 15 writing constitutes a contract with a missing price term, and\nthat term was filled by the crossing offers.\nB.) Yes, because when two crossing offers are identical in import, one will be treated as\nan offer and the other as an acceptance.\nC.) No, because there were two crossing offers and no acceptance; hence there was no\nmutual assent.\nD.) No, but the result would be different if Mom were a merchant.\nGround\nTruth\nSolution\nC\nModel\nResponse\nFirst, let\u2019s analyze the May 15 writing. The writing states that if the price can be agreed\nupon by June 1, Mom and Huck will enter into a contract for the purchase of the specific\nRolls-Royce. At this point, there is no agreement for the price, so there is no valid contract\nformed yet.\nNext, let\u2019s examine the crossing offers on May 20. Mom sent a letter stating she would\nbuy the car for $150,000, and Huck sent a letter stating he would sell the car for $150,000.\nBoth letters were dispatched on the same day, and they agreed on the same price. Since\nthe price was one of the main terms that needed to be agreed upon, and both parties agreed\non the same price, this could be seen as one party making an offer and the other accepting.\nThus, a valid contract could be formed.\nNow, let\u2019s look at the answer choices:\nA: This answer choice argues that the May 15 writing is a contract with a missing price\nterm. However, the May 15 writing itself stated that an agreement would only be formed\nif the price was agreed upon. Therefore, this answer choice is incorrect.\nB: This answer choice suggests that when two crossing offers are identical, one will be\ntreated as an offer and the other as an acceptance. This accurately reflects the situation\nbetween Mom and Huck, as they both agreed on the same price.\nC: This answer choice argues that there was no mutual assent because there were two\ncrossing offers and no acceptance. However, since both offers agreed on the same price,\none could be seen as an offer and the other as an acceptance, forming a valid contract.\nD: This answer choice states that the result would be different if Mom were a merchant.\nHowever, the fact that Mom is not a merchant does not affect the formation of a valid\ncontract in this situation.\nGiven the analysis of the answer choices, the correct answer is:\nANSWER: B\n22\nTable 12: Example of memorization in ARB. GPT-3.5-turbo\u2019s response indicates memorization of\nthe correct expression. GPT-4 commits to its reasoning better, resulting in the wrong final answer.\nProblem\nA cylindrical capacitor of length L consists of an inner conductor wire of radius\na, a thin outer conducting shell of radius b. The space in between is filled with\nnonconducting material of dielectric constant \u03b5.\nFind the capacitance.\nReference\nSolution\nSupposing that the charge per unit length of the inner wire is \u2212\u03bb and using cylindrical\ncoordinates (r, \u03c6, z), we find the electric field intensity in the capacitor by Gauss\u2019\ntheorem to be\nE = \u2212\n\u03bb\n2\u03c0\u03b5rer =\n\u2212Q\n2\u03c0\u03b5Lrer.\nThe potential difference between the inner and outer capacitors is\nV = \u2212\nZ b\na\nE \u00b7 dr =\n\u03bb\n2\u03c0\u03b5 ln\n\u0012 b\na\n\u0013\n.\nHence the capacitance is\nC = \u03bbL\nV\n= 2\u03c0\u03b5L\nln\n\u0000 b\na\n\u0001\ngpt-3.5\n-turbo\nSolution\nThe capacitance of a cylindrical capacitor is given by the equation:\nC =\n2\u03c0\u03b5L\nln(b/a)\nwhere \u03b5 is the dielectric constant, L is the length of the capacitor, a is the radius of\nthe inner conductor, and b is the radius of the outer conductor.\nTherefore, the capacitance of the given cylindrical capacitor is:\nC =\n2\u03c0\u03b5L\nln(b/a) =\n2\u03c0\u03b5L\nln(b) \u2212 ln(a)\nANSWER:\n2\u03c0\u03b5L\nln(b)\u2212ln(a)\n23\nTable 12 (continued)\ngpt-4\nSolution\nTo find the capacitance of the cylindrical capacitor, we will use the general formula\nfor capacitance: C = Q\nV , where Q is the charge stored in the capacitor and V is the\npotential difference between the inner conductor wire and the outer conducting shell.\nFirst, let\u2019s find the electric field E between the two conductors. We can use Gauss\u2019s\nlaw, which states that the electric flux through a closed surface is equal to the charge\nenclosed by the surface divided by the permittivity of free space \u03b50:\nI\n\u20d7E \u00b7 d \u20d7A = Qenclosed\n\u03b50\n.\nWe will choose a cylindrical Gaussian surface of radius r and length L, where\na < r < b. The electric field is radial, so the electric flux through the curved surface\nof the cylinder is:\nI\n\u20d7E \u00b7 d \u20d7A = E\nI\ndA = E(2\u03c0rL).\nThe charge enclosed by the Gaussian surface is the charge on the inner conductor\nwire, which is given by its linear charge density \u03bb multiplied by the length of the\nGaussian surface, L: Qenclosed = \u03bbL.\nNow we can use Gauss\u2019s law to find the electric field:\nE(2\u03c0rL) = \u03bbL\n\u03b50\n\u21d2 E =\n\u03bb\n2\u03c0r\u03b50\n.\nHowever, we need to account for the dielectric material in between the conductors.\nThe electric field in a dielectric material is related to the electric field in vacuum by\nthe dielectric constant \u03b5:\nEdielectric = E\n\u03b5 .\nSo, the electric field in the dielectric material is:\nEdielectric =\n\u03bb\n2\u03c0r\u03b50\u03b5.\nNow we can find the potential difference between the inner conductor wire and the\nouter conducting shell by integrating the electric field along the radial direction:\nV = \u2212\nZ b\na\nEdielectricdr = \u2212\nZ b\na\n\u03bb\n2\u03c0r\u03b50\u03b5dr.\nIntegrating, we get:\nV = \u2212\n\u03bb\n2\u03c0\u03b50\u03b5 ln( b\na).\nFinally, we can find the capacitance using the formula C = Q\nV :\nC =\n\u03bbL\n\u2212\n\u03bb\n2\u03c0\u03b50\u03b5 ln( b\na).\nSimplifying, we get:\nANSWER: C = 2\u03c0\u03b50\u03b5L\nln( b\na ) .\n24\nTable 13: Prompt used for multiple-choice MCAT and Law problems.\nSystem\nYou are a top graduate student taking an open-ended qualifying exam. Your final\nanswer should always be in the last line of your response, preceded by ANSWER:.\nUser\nYou are a top graduate student taking a qualifying exam. Below you will find a\nmultiple choice question.\nQuestion: {Problem_Statement}\nAnswer Choices: {Answer_Choices}\nNow it is time to choose an answer. Think carefully and go step by step.\nMake sure to justify all your work. Your final answer should be one of A,B,C,D,...\ngiven at the end of your work and preceded by ANSWER:. For example, if you\nthink the answer is B, the last line of your answer should be ANSWER: B\nSolution:\nTable 14: Prompt used for numerical problems.\nSystem\nYou are a top graduate student taking an open-ended qualifying exam. Your final\nanswer should always be in the last line of your response, preceded by ANSWER:.\nUser\nYou are a top graduate student taking an open-ended qualifying exam. Below you\nwill find a question requiring you to compute a numerical value.\nQuestion: {Problem_Statement}\nNow it is time to give your answer. Think carefully and go step by step. Make sure\nto justify all your work. Please simplify all expressions as much as possible and do\nnot leave any variables in your final answer.\nYour final answer should NOT contain units and should be given at the end of your\nwork and preceded by ANSWER:\nFor example, if you think the answer is 2.4 meters, the last line of your answer\nshould be ANSWER: 2.4.\nSolution:\n25\nTable 15: Prompt used for symbolic problems.\nSystem\nYou are a top graduate student taking an open-ended qualifying exam. Your final\nanswer should always be in the last line of your response, preceded by ANSWER:.\nUser\nYou are a top graduate student taking an open-ended qualifying exam. Below you\nwill find a question requiring you to give a symbolic answer.\nQuestion: {Problem_Statement}\nNow it is time to give your answer. Think carefully and go step by step. Make sure\nto justify all your work.\nYour final answer should NOT contain units and should be given at the end of your\nwork and preceded by ANSWER:\nFor example, if you think the answer is x \u2217 y, the last line of your answer should be\nANSWER: x \u2217 y\nSolution:\nTable 16: Prompt used for proof-like problems.\nSystem\nYou are a top graduate student taking an open-ended qualifying exam. Your final\nanswer should always be in the last line of your response, preceded by ANSWER:.\nUser\nYou are a top graduate student taking an open-ended qualifying exam. Below you\nwill find a question requiring you to prove the given statement.\nQuestion: {Problem_Statement}\nNow it is time to give your answer. Think carefully and go step by step. Make sure\nto justify all your work.\nSolution:\n26\nTable 17: Prompt used for GPT-3.5-turbo symbolic evaluation.\nSystem\nYou are a top professor grading an open-ended qualifying exam.\nUser\nProblem Statement: Give a solution to the differential equation y\u2032\u2032 = \u2212y\nReference Answer: y(t) = cos(t) or y(t) = sin(t)\nModel Answer: y(x) = sin(x)\nScore: The correct answer is either y(t) = cos(t) or y(t) = sin(t). The model\ngave y(x) = sin(x). Since the function variable was not specified, the model\nmatches one of the reference answers. GRADE: 1\nProblem Statement: {Problem_Statement}\nReference Answer: {Final_Answer}\nModel Answer: {model_final_answer}\nNow it is time to grade the model answer. If the solution is incorrect give GRADE:\n0. If the solution is nearly correct up to a constant give GRADE: 0.5. If the solution\nis correct give GRADE: 1. Before coming to a final grade think think carefully and\ngo step by step. DO NOT TRY TO SOLVE THE PROBLEM. If a variable name is\nnot specified and the reference answer and the model answer are the same up to the\nname of a variable give a score of GRADE: 1. For example if the reference answer\nis\nf(x) = x2\nand the model answer is\nf(y) = y2\ngive a score of GRADE: 1.\nScore: The reference answer\nTable 18: Prompt used for GPT-4 rubric self-evaluation.\nSystem\nYou are a top professor grading an open-ended qualifying exam.\nUser\nProblem Statement: {Problem_Statement}\nRubric: {rubric}\nStudent Answer: {response}\nNow it is time to grade the student answer. Make sure to check each point of the\nrubric step by step. And make sure to print the total number of earned points at the\nend of your grading. For example, if the student earned 8 points, print Rubric Score:\n8 points\nRubric Evaluation:\n27\nTable 19: Prompt used for GPT-4 rubric design.\nSystem\nYou are a top professor grading an open-ended qualifying exam.\nUser\nProblem Statement: {Sample_Problem_Statement}\nReference Solution: {Sample_Reference_Solution}\nRubric: {Handcrafted_Rubric}\nProblem Statement: { Problem_Statement }\nReference Solution: { Solution }\nNow it is time to write the rubric. Make sure to think carefully and go step by step,\nbreaking down the problem into multiple parts. The total number of possible points\nshould sum to 10.\nRubric:\n28\n"
  },
  {
    "title": "Predicting Code Coverage without Execution",
    "link": "https://arxiv.org/pdf/2307.13383.pdf",
    "upvote": "8",
    "text": "Predicting Code Coverage without Execution\nMichele Tufano, Shubham Chandel, Anisha Agarwal, Neel Sundaresan, Colin Clement\nMicrosoft\nRedmond, WA, USA\n{mitufano, schandel, anisagarwal, neels, coclement}@microsoft.com\nAbstract\nCode coverage is a widely used metric for quan-\ntifying the extent to which program elements,\nsuch as statements or branches, are executed\nduring testing. Calculating code coverage is\nresource-intensive, requiring code building and\nexecution with additional overhead for the in-\nstrumentation. Furthermore, computing cover-\nage of any snippet of code requires the whole\nprogram context. Using Machine Learning to\namortize this expensive process could lower\nthe cost of code coverage by requiring only\nthe source code context, and the task of code\ncoverage prediction can be a novel benchmark\nfor judging the ability of models to understand\ncode.\nWe propose a novel benchmark task\ncalled Code Coverage Prediction for Large Lan-\nguage Models (LLMs). We formalize this task\nto evaluate the capability of LLMs in under-\nstanding code execution by determining which\nlines of a method are executed by a given test\ncase and inputs. We curate and release a dataset\nwe call COVERAGEEVAL by executing tests\nand code from the HumanEval dataset and col-\nlecting code coverage information. We report\nthe performance of four state-of-the-art LLMs\nused for code-related tasks, including OpenAI\u2019s\nGPT-4 and GPT-3.5-Turbo, Google\u2019s BARD,\nand Anthropic\u2019s Claude, on the Code Coverage\nPrediction task. Finally, we argue that code cov-\nerage as a metric and pre-training data source\nare valuable for overall LLM performance on\nsoftware engineering tasks.\n1\nIntroduction\nSoftware testing is an essential part of the soft-\nware life-cycle which aims at detecting bugs in a\nprogram prior to shipping new versions. Code cov-\nerage is a widely used metric which estimates the\nquality of testing, providing some confidence that\nthe system will operate conforming to the specified\nrequirements. Several standards require a specific\nlevel of code coverage for software systems before\nthey are allowed to be deployed.\nFocal Method {m}\npublic String foo(int x){\nif(x == 0){\nreturn \"zero\";\n} else if(x > 0){\nreturn \"positive\";\n} else {\nreturn \"negative\";\n}\nreturn \"impossible\";}\nTest Case {t}\npublic void testFoo() {\nString res = foo(2);\nAssert.isEqual(\"positive\", res);}\nCoverage-Annotated Method {cov(m, t)}\n>\npublic String foo(int x){\n>\nif(x == 0){\n!\nreturn \"zero\";\n>\n} else if(x > 0){\n>\nreturn \"positive\";\n!\n} else {\n!\nreturn \"negative\";\n!\n}\n-\nreturn \"impossible\";}\nFigure 1: Given a focal method m, that is a method\nunder test, and a test case t covering that method, the\ncode coverage obtained by t on m can be represented\nas the coverage-annotated method cov(m, t), where >\nrepresents executed statements, ! represents statements\nnot executed, and - represents unreachable code.\nFor example, coverage is one of the metrics con-\nsidered by the Federal Aviation Administration\n(FAA) for safety certification of avionic equipment,\nas documented in DO-178B (Johnson, 1998) and\nDO-178C (Rierson, 2017). Test coverage is also a\nrequirement in the automotive safety standard ISO\n26262 Road Vehicles - Functional Safety (Palin\net al., 2011).\nGiven a focal method m, which is executed di-\nrectly by the test case t, code coverage measures\nthe number of statements that have been executed\n(i.e., covered) by the test t. Figure 1 shows an ex-\nample of a focal method m (method under test)\ntested by t. The coverage obtained by t on m\narXiv:2307.13383v1  [cs.SE]  25 Jul 2023\nis represented in the coverage-annotated method\ncov(m, t), where executed statements are marked\nwith > while missed (i.e., uncovered statements)\nwith ! and unreachable code (i.e., dead code) with\n- . From this representation, several quantitative\ncoverage metrics can be computed, such as func-\ntional, statement, branch, and path coverage.\nCode coverage is computed by instrumenting the\ncode and running the test suite while monitoring\nthe code execution. This process is expensive, since\nit requires building and executing code, especially\nfor large software projects or when code coverage\nis computed multiple times. Additionally, it is not\npossible to measure code coverage for a snippet of\ncode without the availability of the entire program\nwhich contains the given snippet. This situation\nhappens when only partial code is available, for\nexample within a commit log/diff, or when only\npartial code is transmitted to a server, for security\nand/or networking reasons.\nWhile Large Language Models (LLMs) have\ngained prominence in code-related tasks and\ndemonstrated impressive results in areas such as\ncode generation and test generation, it remains un-\nclear to what extent these models truly understand\ncode execution (Liu et al., 2023). The task of ac-\ncurately determining which lines of a method are\nexecuted based on a given test case and its inputs\nrequires a deep understanding of the underlying\ncode execution dynamics. This motivates the need\nfor a dedicated task, referred to as Code Coverage\nPrediction, which specifically evaluates the capa-\nbility of LLMs in comprehending code execution.\nFurther, a model capable of this task is indepen-\ndently useful as it can amortize the expensive code\ncoverage computation process, or function in cases\nwhere normal code coverage is not possible to com-\npute.\nIn this paper we formalize the Code Coverage\nPrediction task, with the primary objective of evalu-\nating the capability of LLMs in understanding code\nexecution by accurately determining which lines of\na method are executed based on a given test case.\nTo facilitate evaluation, we have curated a compre-\nhensive dataset named COVERAGEEVAL, consist-\ning of coverage-annotated methods. This dataset is\ncreated by executing tests and code from the Hu-\nmanEval dataset, allowing us to collect valuable\ncode coverage information. We have organized\nand made this curated dataset available on GitHub,\nenabling researchers to explore and advance code\ncoverage prediction techniques and LLM code un-\nderstanding.\nWe evaluate the performance of four state-of-the-\nart LLMs widely employed for code-related tasks:\nOpenAI\u2019s GPT-4 and GPT-3.5, Google\u2019s BARD,\nand Anthropic\u2019s Claude. Our ultimate goal is to\ngain insights into the capabilities of LLMs in pre-\ndicting code coverage, offering a promising alter-\nnative to execution-based coverage measurement\nin various scenarios. This approach proves advan-\ntageous when the costs associated with program\nbuilding and execution are prohibitive, when code\ncoverage needs to be invoked multiple times, when\nonly code snippets are available (e.g., in server-side\nscenarios), or when errors in the project prevent\ncomplete builds. Additionally, this task introduces\na novel metric for assessing code understanding\nand serves as a valuable (pre-)training objective.\nBy training models to excel in this task, we be-\nlieve we can enhance their overall performance on\ncode-related tasks.\nThis paper makes the following contributions:\n\u2022 Code Coverage Prediction Task: We propose\na novel task to assess the capability of LLMs\nin understanding code execution by accurately\npredicting executed lines of a method based\non a given test case and inputs.\n\u2022 Evaluation of State-of-the-Art LLMs: We eval-\nuate four prominent LLMs (GPT-4, GPT-3.5,\nBARD, and Claude) on the Code Coverage\nPrediction task, providing insights into their\nperformance and understanding of code exe-\ncution.\n\u2022 Curated Dataset: We curate a comprehen-\nsive dataset (COVERAGEEVAL) of coverage-\nannotated methods and test cases, derived\nfrom the HumanEval dataset. This dataset\nis openly available on GitHub1 (Microsoft,\n2023) enabling further research and advance-\nment in code coverage prediction techniques.\n2\nBackground\nCode coverage is a measure of the degree to which\na test suite exercises a software system (Ivankovi\u00b4c\net al., 2019). Code coverage is commonly com-\nputed by means of instrumentation. This technique\ninserts instrumentation code in various locations\nwithin the code or binaries of the program under\n1https://github.com/microsoft/coverage-eval\ntest, in order to monitor its execution. This inserted\ncode provides counters to record which function\nor statement of the program have been executed\nby the test suite. Inserting these additional state-\nments within the original code leads to execution\noverhead, which can be significant especially for\nlarge software programs (Tikir and Hollingsworth,\n2002).\nThe most common coverage metric is computed\nat statement level, where statement refers to a syn-\ntactic unit of code (e.g., assignment, invocation,\nassertion), often matching a single line of code.\nThe coverage indicates whether a statement has\nbeen executed or not, and aggregated metrics can\nbe computed at function/program level to measure\nthe amount of statements covered by a test suite.\nIn the example in Figure 1, the test case t executes\nfour statements in m, which constitutes \u223c 44%\nstatement coverage for the method m.\nGiven statement coverage information, other\ncoverage criteria and metrics can be obtained by\nmeans of static analysis. Statement coverage infor-\nmation regarding control structure (e.g., if-else\nand case statements) can be used to compute\nbranch coverage, which measure how many log-\nical branches in the program have been executed.\nIn the example in Figure 1 only one branch is ex-\necuted (i.e., else if (x > 0) ), while the other\ntwo branches are missed by the test case t.\nIn the remainder of this paper we will focus on\nstatement coverage, from which other coverage\ncriteria can be obtained.\n3\nCode Coverage Prediction Task\nGiven a method under test (focal method) m,\ncomposed of n statements Sm = s1, s2, . . . , sn,\nand a test case t which exercises the method m,\nthe coverage-annotated focal method cov(m, t) is\ncomposed of a sequence of n statements St\nm =\ns\u2217\n1, s\u2217\n2, . . . , s\u2217\nn, where each statement s\u2217\ni repre-\nsents the coverage-annotated statement of si in\nm.\nSpecifically, s\u2217\ni is marked with one of the\nthree possible coverage symbols c \u2208 {>, !, \u2212},\nwhere the symbol > identifies statements that have\nbeen executed by t, the symbol ! identifies state-\nments that have been missed by t, and the sym-\nbol \u2212 identifies statements that are unreachable.\nThis defines a sequence of n coverage symbols\nCt\nm = c1, c2, . . . , cn, where ci \u2208 {>, !, \u2212}.\nWe define the Code Coverage Prediction Task as\nthe problem of predicting the coverage-annotated\nsequence of statements St\nm given the focal method\nm and a test case t. Formally, this problem can be\ndefined in terms of inputs and expected output:\nInput\n\u2022 Focal Method: m\n\u2022 Test Case: t\nOutput\n\u2022 St\nm = s\u2217\n1, s\u2217\n2, . . . , s\u2217\nn\nor\n\u2022 Ct\nm = c1, c2, . . . , cn\nSpecifically, the output can be either the\ncoverage-annotated sequence of statements St\nm,\nor the sequence of coverage symbols Ct\nm, which\ncan then combined with the original sequence of\nstatements Sm = s1, s2, . . . , sn, to obtain the\ncoverage-annotated sequence of statements St\nm =\ns\u2217\n1, s\u2217\n2, . . . , s\u2217\nn comprising the coverage cov(m, t).\nThis final step is performed by aligning the two\nsequences and obtaining s\u2217\ni = ci + si, where the +\noperation refers to string concatenation.\nLet us take as example the focal method m and\ntest case t in Figure 1. The model is expected to\npredict either the coverage-annotated sequence of\nstatements St\nm or the sequence of coverage sym-\nbols: > > ! > > ! ! ! -.\n3.1\nCoverage Prediction for Pre-Training\nWe propose that the code coverage prediction task\nintroduced in our paper can serve as a valuable pre-\ntraining task for LLMs focused on code generation.\nWhile current pre-training tasks, such as Masked\nLanguage Modeling (MLM) help models under-\nstand code syntax and semantics by analyzing vast\namounts of raw text representing code, our pro-\nposed task enables the model to learn about code\nexecution, which is not technically discoverable by\nsource code text alone.\nTo accomplish this pre-training, we suggest\naugmenting the training data with extensive cov-\nerage logs obtained from Continuous Integra-\ntion/Continuous Deployment (CI/CD) pipelines.\nThese logs contain valuable information about code\ncoverage from regression tests executed during pull\nrequests or commits.\nBy exposing the models to these coverage logs\nduring pre-training, they can learn to associate test\ncases and inputs with the specific lines of code that\nare executed. This pre-training approach enhances\nthe models\u2019 understanding of how different parts\nof the code are exercised by various test scenar-\nios. Consequently, the models can acquire a deeper\ncomprehension of the relationships between inputs,\ntests, and code execution, leading to improved code\ngeneration capabilities.\nIntegrating coverage prediction as a pre-training\ntask could enable models to learn from real-world\ntest scenarios, capturing the nuances of code execu-\ntion in practical settings. This real-world exposure\nshould enhances the models\u2019 ability to generate\ncode that aligns with actual testing practices.\nFurthermore, incorporating coverage prediction\nas a pre-training task opens up possibilities for\ntransfer learning. Models pre-trained on coverage\nprediction can be fine-tuned on downstream tasks,\nsuch as bug detection or test case generation, where\nunderstanding code execution is crucial. The mod-\nels\u2019 pre-existing knowledge of code coverage can\nprovide a solid foundation for these related tasks,\npotentially improving their overall performance.\n4\nCOVERAGEEVAL Dataset\nIn addition to proposing the code coverage predic-\ntion task, this paper also introduces COVERAGEE-\nVAL, a dataset specifically designed for evaluating\nLLMs on this task. This section outlines the pro-\ncess of curating this dataset, which begins with\nthe HumanEval dataset (Chen et al., 2021). By\nexecuting test cases from the HumanEval dataset,\nwe gather code coverage information. To create\nCOVERAGEEVAL, we parse the code coverage logs\ngenerated during the execution of the test cases.\nThis parsing step enables us to extract the relevant\ncoverage annotations. We then carefully structure\nand export the dataset in a format that facilitates its\nuse and evaluation by researchers and practitioners\nalike.\nBy curating this dataset, we aim to provide a\nstandardized benchmark for evaluating LLMs on\nthe code coverage prediction task. The availabil-\nity of COVERAGEEVAL enables researchers to ex-\nplore and advance code understanding, fostering\ninnovation and enabling the development of more\neffective models.\n4.1\nHumanEval\nThe HumanEval dataset consists of 164 hand-\nwritten problems and their code solutions, where\neach problem is a programming task involving\nlanguage comprehension, reasoning, algorithms\nand/or simple mathematics (Chen et al., 2021).\nEach code solution in the dataset includes a func-\ntion signature, a docstring containing the problem\ndescription, a function body, and several unit tests.\nWe extend the HumanEval dataset to include cov-\nerage, calculated using the function body and the\nrespective unit tests.\n4.2\nCoverage Analysis\nIn this section, we describe the steps taken to ana-\nlyze the code coverage on the HumanEval dataset\nand create our COVERAGEEVAL dataset.\nEach code solution in the HumanEval dataset\nis accompanied by a single test case, which in-\ncludes multiple asserts designed to test the cor-\nrectness of the code solution based on the given\nproblem\u2019s functional requirements. These asserts\ncover various inputs, scenarios, and code state-\nments/branches. To enhance the dataset and in-\ncrease the complexity of each data point, we split\nthe single test case into multiple test cases, each\ncontaining a single assert. This splitting process\nallows us to generate additional method-test pairs,\nas well as making each data point more challenging.\nThe original test case may cover most of the lines\nand branches in the method, but each individual\nassert covers only a subset of them.\nBy performing this split, we create a more di-\nverse set of method-test pairs within the dataset.\nEach individual test case invokes the focal method\nonce and covers a subset of the statements and\nbranches within the method. This enables us to\nevaluate the LLMs\u2019 ability to predict code coverage\nat a more granular level, going beyond the overall\ncoverage of the method. It also adds complexity\nto the task, as predicting coverage for each assert\nrequires a deeper understanding of the code and its\npotential execution paths.\nSubsequently, we execute the extracted test cases\nindividually with pytest. During the execution,\nwe also enable the coverage computation using\ncoverage.py. To do so, we run the following com-\nmand: coverage run -m pytest <test_name>\nwhere <test_name> is each individual test in the\ndataset.\nNext, for each test case t, we analyze the cor-\nresponding coverage report obtained by the test\nexecution in order to extract the annotated cover-\nage cov(m, t). The coverage report marks each\nsource code line in the file with coverage informa-\ntion, specifying whether the statement has been\nexecuted or not.\nWe automatically parse this report and extract\nthe corresponding annotated coverage cov(m, t).\nAt the end of this process, we obtained a dataset\nwhere each data point is formed by a triplet d =\n{m, t, cov(m, t)}.\n4.3\nData Format\nThe COVERAGEEVAL dataset maintains the struc-\nture of the HumanEval dataset, with the addition\nof coverage information for each test. Each record\ncorresponds to a unique problem and contains the\nfollowing fields:\n\u2022 Problem ID: A unique ID for the problem\n\u2022 Problem: The name of the method written to\nsolve the problem\n\u2022 Method: The method contents, including a\nfunction signature, a docstring with the details\nof the problem, and the function body.\n\u2022 Tests: A list of unit tests for the problem. Each\nitem in the list includes the unique ID of the\ntest and the code of the test. We have also\nadded coverage information for each test in\nthe following two forms:\n1. Coverage: The code of the method, with\neach line annotated with > , ! or - for\ncode that is executed, missed or unreach-\nable by the given test.\n2. Coverage Sequence:\nA list of equal\nlength to the number of lines in the\nmethod, where each value in the list is\n> , ! or - , depending on the status of\nthe respective line of code in the method.\nFigure 3 (Appendix) shows a sample record from\nthe COVERAGEEVAL dataset. COVERAGEEVAL\nis available to the public via GitHub (Microsoft,\n2023).\nTable 1 reports the statistics for the COVER-\nAGEEVAL dataset in terms of number of problems,\ncode solutions, tests, and coverage symbols. The\ndiscrepancy between number of problems and so-\nlutions is explained by the fact that some problems\nhave multiple solutions. It is also worth noting that\nwhile our dataset currently does not contain any un-\nreachable code (-), we have proactively considered\nthe potential presence of unreachable code while\ndesigning the task.\nProblems\nSolutions\nTests\nCoverage Symbols\nExecuted (>)\nMissed (!)\nUnreachable (-)\n158\n164\n1160\n20037\n1734\n0\nTable 1: COVERAGEEVAL statistics.\n5\nEvaluating LLMs\nIn this section, we present our evaluation of state-\nof-the-art Language Models (LLMs) for the pro-\nposed task of Code Coverage Prediction. We se-\nlected four highly regarded LLMs that are not only\npopular for code generation but also widely used\nfor other Natural Language (NL) tasks. The LLMs\nwe employed for this evaluation are OpenAI\u2019s GPT-\n4 and GPT-3.5, Google\u2019s BARD, and Anthropic\u2019s\nClaude.\nGPT-3.5 (Brown et al., 2020) and GPT-4 (Ope-\nnAI, 2023) are large language models developed\nby OpenAI which are Transformer-style models\n(Vaswani et al., 2017) pre-trained to predict the\nnext token in a document. Both models were then\nfine-tuned using Reinforcement Learning from Hu-\nman Feedback (RLHF) (Christiano et al., 2017).\nGPT-4 improves over the predecessor by accept-\ning as input both images and text (multimodal\nmodel) and producing text as output. BARD is\na conversational AI developed by Google based\non LaMDA(Thoppilan et al., 2022) a Transformer-\nbased language models trained on dialogue (Adi-\nwardana et al., 2020). Anthropic Claude is a 52-\nbillion-parameter LLM developed by Anthropic.\nClaude was pretrained on a large text corpus and\nfinetuned with \"RL from AI Feedback\" (RLAIF),\nwhere AI feedback are steered by a small set of\nprinciples drawn from a \"constitution\" defined by\nhumans (Bai et al., 2022).\n5.1\nExperimental Design\nWhen evaluating the LLMs on the code coverage\nprediction task, we designed the experiments to\nassess their performance on non-trivial coverage\nsequences while progressively providing more in-\nformation and examples.\nFirst,\nwe filtered out data points d\n=\n{m, t, cov(m, t)} where the coverage sequence is\ntrivial consisting exclusively of the symbol > .\nThese cases represent methods with no branches\nor where the test case covers every statement in\nthe focal method. Although these data points are\nincluded in the COVERAGEEVAL dataset, we ex-\ncluded them from this specific evaluation. The sub-\nset of data points containing only trivial symbols is\nreported in our online appendix. It\u2019s important to\nnote that no data points in the dataset has a cover-\nage sequence consisting solely of ! or - symbols.\nAfter this filtering step, we were left with 478 data\npoints on which we evaluated the LLMs.\nThe prompt used to evaluate the LLMs was de-\nsigned to include the following sections:\n\u2022 System NL prompt: a prompt providing a\nnatural language description of the task, aimed\nat conveying the task to the LLM.\n\u2022 Examples: zero, one, or multiple examples of\nthe task.\n\u2022 Focal Method m and Test Case t.\nIn terms of the System NL prompt, our evalua-\ntion involved experimenting with various prompts\nand descriptions. We achieved the most favorable\noutcomes by utilizing a system prompt that emu-\nlates a terminal environment (e.g., python terminal).\nWithin this prompt, we instructed the LLM to gen-\nerate the code coverage output based on a given test\ncase and method. For OpenAI models, we included\nthis prompt in the specific system prompt section,\nwhile for BARD and Claude, we incorporated it as\nthe initial part of the prompt.\nTo comprehensively assess the LLMs\u2019 perfor-\nmance, we conducted evaluations using different\nnumbers of examples for the code coverage predic-\ntion task. Specifically, we employed zero-shot, one-\nshot, and multi-shot prompting approaches. This\nallowed us to examine the impact of example avail-\nability on the models\u2019 performance and their ability\nto generalize the task across various methods.\nWhen selecting examples for evaluating cover-\nage on a particular method mi, we took care to\nprevent data leakage and encourage the LLMs to\ngeneralize their predictions to other methods. To\nachieve this, we randomly sampled a data point\n{mj, t, cov(m, t)} where mj \u0338= mi when provid-\ning examples.\nFinally, the prompt provides a focal method m\nand a corresponding test case t for which we ex-\npected the model to predict the code coverage. Fig-\nure 2 shows an example of the prompt we designed.\nInference is performed on all the LLMs with\ntemperature and topp set to 0, and generating one\nsample.\nSystem NL Prompt\nYou are a terminal. Instruction:\nWhen user runs:\ncoverage run -m\npytest code.py\nthen you'll cat the file code.py,\nwith each line starting with either of the two symbols below:\n> if the line is executed\n! is the line is not executed\nExample output:\n> line1\n! line2\n> line3\n...\n> linen\nYou job is to figure out which line will be executed\ngiven different test cases.\nExamples\n(anaconda3-2020.11) cat code.py\ndef split_words(txt):\n...\n(anaconda3-2020.11) cat test.py\ndef test():\nassert split_words(\"Hello,world!\") == [\"Hello\",\"world!\"]\nassert True\n(anaconda3-2020.11) coverage run -m\npytest test.py\n> def split_words(txt):\n>\nif \" \" in txt:\n!\nreturn txt.split()\n>\nelif \",\" in txt:\n>\nreturn txt.replace(',',' ').split()\n!\nelse:\n...\nFocal Method m + Test Case t\n(anaconda3-2020.11) cat code.py\ndef <focal_method>\n...\n(anaconda3-2020.11) cat test.py\ndef test():\n...\n(anaconda3-2020.11) coverage run -m\npytest test.py\nFigure 2: Code Coverage Prediction Task Prompt: (i)\nSystem NL Prompt instruct the LLM to operate as in a\nterminal environment; (ii) zero, one, or multiple exam-\nples of the coverage prediction task may be shown; (iii)\nthe current focal method m and test case t are provided\n5.2\nEvaluation Metrics\nIn this section we describe the evaluation metrics.\nGiven the method m, the test case t, and the se-\nquence of coverage symbols Ct\nm = c1, c2, . . . , cn,\nwhere ci \u2208 {>, !, \u2212}, the model generates a\npredicted sequence of coverage symbols \u02c6Ct\nm =\n\u02c6c1, \u02c6c2, . . . , \u02c6cn. We consider the following metrics\nto evaluate the performances of our proposed ap-\nproach.\nModel\nzero-shot\none-shot\nmulti-shot\nMatch\nStmt\nBranch\nMatch\nStmt\nBranch\nMatch\nStmt\nBranch\nOpenAI GPT-4 (gpt-4)\n25.75\n84.47\n20.16\n22.85\n90.71\n22.65\n30.04\n90.5\n22.5\nOpenAI GPT-3.5 (gpt-3.5-turbo)\n0\n39.87\n8.33\n8.17\n76.53\n17.17\n11.03\n82.29\n17.9\nGoogle BARD (text-bison-001)\n0\n81.27\n17.21\n1.87\n86.93\n19.63\n21.56\n85.66\n20.52\nAnthropic Claude (claude-1.3)\n3.9\n84.47\n20.07\n4.83\n83.21\n19.16\n6.88\n55.7\n12.23\nTable 2: LLMs performances on the Code Coverage Prediction Task. The table reports the percentages of predicted\ncoverage sequences that match the ground truth (Match), the percentage of correct coverage symbols for statements\n(Stmt), and specifically for branches (Branch). Evaluation performed for zero-shot, one-shot, and multi-shot.\n5.2.1\nPerfect Sequence Match\nThe perfect sequence match metric counts the num-\nber of times that the predicted sequence \u02c6Ct\nm exactly\nmatches (symbol-by-symbol) the target coverage\nsequence Ct\nm. This represents the case where the\nmodel predicts the coverage with perfect accuracy\nfor all the statements and branches.\n5.2.2\nStatement Correctness\nThe statement correctness metric measures the per-\ncentage of statements for which the execution pre-\ndiction is correct. This is equivalent to the per-\ncentage of symbols in the predicted sequence that\nmatch the target sequence.\n5.2.3\nBranch Correctness\nThe branch correctness metric measures the per-\ncentage of branch-specific statements for which\nthe execution prediction is correct. The branch\ncorrectness only considers the symbols associated\nwith branch statements. It measures the percentage\nof symbols in the predicted sequence (associated\nwith branches) that match the symbols in the target\nsequence.\n6\nResults\nTable 2 presents the performance of different LLMs\non the Code Coverage Prediction task. The table\nshowcases the percentage of predicted coverage\nsequences that match the ground trught (Match),\nthe percentage of correct coverage symbols for all\nthe statements (Stmt), and the percentage of correct\ncoverage symbols when only considering branch\nstatements (Branch). Evaluation performances are\ncomputed using zero-shot, one-shot, and multi-shot\nprompting.\nOpenAI GPT-4 demonstrates the highest perfor-\nmance on this task, achieving 24.75% exact match\nwith zero-shot prompting and improving to 30%\nwith multi-shot prompting, where up to 6 exam-\nples are provided in the prompt. Notably, the other\nLLMs achieve low exact matches with zero-shot\nprompting (between 0 and 4%), suggesting that\nthese foundational models may not have been ex-\nposed to coverage logs during their training or\nthat. The second best-performing model is Google\nBARD, with an exact sequence match reaching\n21.5% with multi-shot prompting.\nRegarding the percentage of correct coverage\nstatements (see Stmt), most models demonstrate\nimprovement as more examples are included in\nthe prompt. OpenAI GPT-4 obtain the overall best\nscores between 84% and 90% of statement correct-\nness.\nWhen considering only statements involved in\nbranches (e.g., if-else, while), it becomes evi-\ndent that there is a significant drop in correct pre-\ndictions. In fact, the best performing model, Ope-\nnAI GPT-4, accurately predicts a modest 22% of\nthese symbols when one- and multi-shot is used\nfor prompting. It is important to note that this sub-\nset of statements, which are intricately connected\nto branches, presents a greater challenge for eval-\nuation because the LLM must reason about the\nboolean conditions that determine which branch\nis covered. Consequently, accurately predicting\ncoverage symbols within this context requires the\nmodel to possess a profound understanding of the\nconditional logic that guides program execution.\nDespite the surprisingly strong results of Ope-\nnAI GPT-4 on the Code Coverage Prediction task,\nit should be noted that the model still fails to gener-\nate the correct coverage for more than 70% of the\nmethod-test pairs in the COVERAGEEVAL dataset.\nThis emphasizes that LLMs have a long way to go\nin developing a deep understanding of code execu-\ntion.\nWe believe that in order to enhance code gen-\neration results, these LLMs should gain a com-\nprehensive understanding of code execution under\ndifferent inputs and test cases. Therefore, we assert\nthat our dataset and proposed task can contribute\nto the advancement of LLMs towards this goal.\n7\nDiscussion& Applications\nLLMs trained to excel on the Code Coverage Pre-\ndiction task could offer a promising alternative to\ntraditional execution-based code coverage measure-\nment in various scenarios. In this section, we dis-\ncuss several use case scenarios where this approach\ncan be valuable and beneficial.\n7.1\nExpensive Build & Execution\nFor large software projects with millions of lines\nof code and numerous dependencies, the build and\nexecution process can be time-consuming and ex-\npensive. In such cases, developers may want to\nanalyze the code coverage obtained by newly writ-\nten tests without waiting for the lengthy build phase.\nBy leveraging LLMs trained on the Code Coverage\nPrediction task, developers can predict the coverage\nobtained by the new tests on existing methods with-\nout the need to build the entire project or execute\nthe tests. This enables developers to quickly as-\nsess whether additional tests are required to cover\nmissed lines or branches in the methods, saving\nvaluable time and resources.\n7.2\nLimited Code Availability\nTraditional code coverage computation requires the\ncomplete source code of the codebase to be avail-\nable for instrumentation and execution. However,\nthere are scenarios where only a partial view of the\ncode is accessible, making code coverage computa-\ntion impossible using traditional methods.\nIn cases where limited code availability poses a\nchallenge, the Code Coverage Prediction approach\ncan be employed. For example, when utilizing an\nAI code generation service from an IDE, develop-\ners may transmit only a partial view of the code\nto the server where the AI model resides. In this\nscenario, the server can use the proposed approach\nto predict the code coverage of the AI-generated\ntest cases on the given method. This enables esti-\nmation of the code coverage without the need for\nthe entire codebase, addressing privacy concerns\nand network limitations. The predicted code cover-\nage can then be used to make informed decisions,\nsuch as generating additional tests if coverage is\ninsufficient or transmitting the generated tests to\nthe user if coverage is satisfactory.\n7.3\nLive Coverage\nLive Unit Testing, integrated into various IDEs, al-\nlows developers to receive real-time feedback on\nthe impact of code changes on existing tests and\nidentifies whether newly added or modified code\nis covered by existing tests. In this scenario, the\nCode Coverage Prediction approach can be applied\nby replacing the actual execution of test cases with\nan AI inference call to predict the coverage on the\nmodified or newly added methods. This provides\ndevelopers with immediate feedback on code cov-\nerage without the need for executing the entire test\nsuite. By utilizing LLM-based models for code\ncoverage prediction, developers can streamline the\ntesting process and receive timely insights into the\ncoverage of their code changes.\n8\nConclusion\nIn this paper, we introduced the novel task of Code\nCoverage Prediction, which aims to assess the ca-\npabilities of Large Language Models (LLMs) in\nunderstanding code execution by accurately pre-\ndicting the lines of code that are executed based\non given test cases.\nWe curated a comprehen-\nsive dataset named COVERAGEEVAL, consisting of\ncoverage-annotated methods derived from the Hu-\nmanEval dataset. This dataset enables researchers\nto explore and advance code coverage prediction\ntechniques and LLM code understanding.\nWe evaluated the performance of four state-of-\nthe-art LLMs, namely OpenAI\u2019s GPT-4 and GPT-\n3.5, Google\u2019s BARD, and Anthropic\u2019s Claude, on\nthe Code Coverage Prediction task. The results\ndemonstrated that GPT-4 achieved the highest per-\nformance, with 10.46% exact match with zero-shot\nprompting and 24.48% with multi-shot prompting.\nHowever, none of the models, including GPT-4,\nachieved high accuracy in predicting code cover-\nage, indicating that LLMs still have a long way\nto go in developing a deep understanding of code\nexecution.\nThe Code Coverage Prediction task serves as a\nvaluable metric for assessing code understanding\nand can potentially contribute to the enhancement\nof LLMs\u2019 overall performance on code-related\ntasks. By training models to excel in this task,\nwe can improve their ability to comprehend code\nexecution dynamics, which is crucial for tasks such\nas code generation and test generation.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain chat-\nbot. arXiv preprint arXiv:2001.09977.\nYuntao Bai,\nSaurav Kadavath,\nSandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen,\nAnna Goldie,\nAzalia Mirhoseini,\nCameron McKinnon, et al. 2022.\nConstitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nlarge language models trained on code.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. Ad-\nvances in neural information processing systems, 30.\nMarko Ivankovi\u00b4c, Goran Petrovi\u00b4c, Ren\u00e9 Just, and Gor-\ndon Fraser. 2019. Code coverage at google. In Pro-\nceedings of the 2019 27th ACM Joint Meeting on\nEuropean Software Engineering Conference and Sym-\nposium on the Foundations of Software Engineering,\npages 955\u2013963.\nLeslie A Johnson. 1998. Do-178b. Software Considera-\ntions in Airborne Systems and Equipment Certifica-\ntion, Crosstalk Magazine.\nChenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang,\nAlexey Svyatkovskiy, Shengyu Fu, Neel Sundare-\nsan, and Nan Duan. 2023.\nCode execution\nwith pre-trained language models. arXiv preprint\narXiv:2305.05383.\nMicrosoft. 2023.\nCoverage-eval.\nhttps://github.\ncom/microsoft/coverage-eval.\nOpenAI. 2023. Gpt-4 technical report.\nRob Palin, David Ward, Ibrahim Habli, and Roger Riv-\nett. 2011. Iso 26262 safety cases: Compliance and\nassurance.\nLeanna Rierson. 2017. Developing safety-critical soft-\nware: a practical guide for aviation software and\nDO-178C compliance. CRC Press.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nMustafa M Tikir and Jeffrey K Hollingsworth. 2002.\nEfficient instrumentation for code coverage test-\ning. ACM SIGSOFT Software Engineering Notes,\n27(4):86\u201396.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998\u20136008.\nA\nCOVERAGEEVAL Example\nProblem: rounded_avg\nID = 104\ndef rounded_avg(n, m):\n\"\"\"SYNTH You are given two positive integers n and m,\nand your task is to compute the average of the integers\nfrom n through m (including n and m).\nRound the answer to the nearest integer\nand convert that to binary.\nIf n is greater than m, return -1.\nExample:\nrounded_avg(1, 5) => \"0b11\"\nrounded_avg(7, 5) => -1\nrounded_avg(10, 20) => \"0b1111\"\nrounded_avg(20, 33) => \"0b11010\"\n\"\"\"\nif m < n:\nreturn -1\nsummation = 0\nfor i in range(n, m+1):\nsummation += i\nreturn bin(round(summation/(m - n + 1)))\nTest Cases\ndef test_658():\nassert rounded_avg(185,546) == \"0b101101110\"\nassert True\ndef test_659():\nassert rounded_avg(362,496) == \"0b110101101\"\nassert True\ndef test_660():\nassert rounded_avg(560,851) == \"0b1011000010\"\nassert True\nCoverage-Annotated Method {test_id = 660}\n>\ndef rounded_avg(n, m):\n>\nif m < n:\n!\nreturn -1\n>\nsummation = 0\n>\nfor i in range(n, m+1):\n>\nsummation += i\n>\nreturn bin(round(summation/(m - n + 1)))\nCoverage Sequence {test_id = 660}\n[ > ,\n> ,\n! ,\n> ,\n> ,\n> ,\n> ]\nFigure 3: Example record from the COVERAGEEVAL\ndataset. This record is for the rounded_avg problem.\nWe have shown 3 of the unit tests, as well as sample\ncoverage annotation data from one unit test. where >\nrepresents executed statements, ! missing statements,\nand - unreachable code.\nB\nDeployed Systems\nWe deploy our approach in two systems covering\nsome of the use cases described in the paper.\nB.1\nSystem A - Live Coverage\nFigure 4 shows the deployment of System A, which\nprovides live coverage prediction for developers\ndirectly into their IDE. System A supports the sce-\nnario where a developer is writing tests for a given\nmethod (e.g., Fibonacci(n)) in their codebase.\nSystem A provides live coverage information (bot-\ntom of Figure 4) where lines covered by the tests\nare marked with > and highlighted in green and\nthe line missed are marked with ! and highlighted\nin red.\nThe benefits provided by System A are the fol-\nlowing: (i) no need to build the entire codebase;\n(ii) no need to execute the tests; (iii) live and\nlightweight coverage prediction.\nB.2\nSystem B - Test Generation with\nCoverage\nFigure 5 shows the deployment of System B, which\nprovides Test Suites with a coverage guarantee.\nSystem B supports the scenario where a developer\nis requesting test cases for a given method and\nwould like to obtain a certain degree of coverage\non the method under test. Once the method is\ntransmitted to the Test Generation Service, the Test\nGeneration Model (i.e., an AI-based test generation\ntool or any other tool) outputs a first batch of test\ncase candidates. The Coverage Prediction Model\nanalyzes these tests and the method under test, and\npredicts the coverage that these tests achieve on\nthe method. If the coverage is satisfactory (w.r.t.\na given criteria and threshold) the tests are trans-\nmitted to the IDE and shown to the developer. If\nthe tests do not meet the criteria in terms of cover-\nage, the Test Generation Service requests additional\ntests from the Test Generation Model (optionally,\nproviding the specific lines/branches which still\nneed to be covered).\nThe benefits provided by System B are the fol-\nlowing: (i) automated test generation with cover-\nage guarantees; (ii) lightweight generation without\nneed of build and test execution on the user side.\nCoverage Prediction \nModel\nMethod\nTests\nCoverage Annotated Method\nUser Space\nServer Space\nFigure 4: System A - Live Coverage\nCoverage Prediction \nModel\nUser Space\nServer Space\nTest Generation \nModel\nSatisfactory \nCoverage?\nRequest \nMore Tests\nGenerated \nTests\nYES\nNO\nMethod\nto be Tested\nFigure 5: System B - Test Generation with Coverage\n"
  },
  {
    "title": "Strivec: Sparse Tri-Vector Radiance Fields",
    "link": "https://arxiv.org/pdf/2307.13226.pdf",
    "upvote": "5",
    "text": "Strivec: Sparse Tri-Vector Radiance Fields\nQuankai Gao\u22171\nQiangeng Xu\u22171\nHao Su2\nUlrich Neumann1\nZexiang Xu3\n1University of Southern California\n2UC San Diego\n3Adobe Research\n{quankaig,qiangenx,uneumann}@usc.edu\nhaosu@ucsd.edu\nzexu@adobe.com\nAbstract\nWe propose Strivec, a novel neural representation that\nmodels a 3D scene as a radiance field with sparsely dis-\ntributed and compactly factorized local tensor feature grids.\nOur approach leverages tensor decomposition, following\nthe recent work TensoRF [7], to model the tensor grids. In\ncontrast to TensoRF which uses a global tensor and focuses\non their vector-matrix decomposition, we propose to uti-\nlize a cloud of local tensors and apply the classic CANDE-\nCOMP/PARAFAC (CP) decomposition [5] to factorize each\ntensor into triple vectors that express local feature distribu-\ntions along spatial axes and compactly encode a local neu-\nral field. We also apply multi-scale tensor grids to discover\nthe geometry and appearance commonalities and exploit\nspatial coherence with the tri-vector factorization at mul-\ntiple local scales. The final radiance field properties are re-\ngressed by aggregating neural features from multiple local\ntensors across all scales. Our tri-vector tensors are sparsely\ndistributed around the actual scene surface, discovered by a\nfast coarse reconstruction, leveraging the sparsity of a 3D\nscene. We demonstrate that our model can achieve better\nrendering quality while using significantly fewer parame-\nters than previous methods, including TensoRF and Instant-\nNGP [27].\n1. Introduction\nRepresenting 3D scenes as radiance fields [26] has en-\nabled photo-realistic rendering quality and emerged as a\npopular design choice in 3D vision and graphics appli-\ncations.\nWhile many methods [31, 46, 3] (following\nNeRF [26]) purely use MLPs to represent neural fields, re-\ncent works, like TensoRF [7] and Instant-NGP [27], have\ndemonstrated the advantages of using shared global feature\nencoding for radiance field modeling, in terms of speed,\ncompactness, and quality. However, these methods share\nand assign neural features uniformly in a scene (with ten-\nsor factors or hash tables), assuming the scene content is\n\u2217Equal contribution.\nCode and results: https://github.com/Zerg-Overmind/Strivec\nTensoRF-VM-192\nINGP\nOurs-48\nReference\n0\n5\n10\n15\n20\n25\n30\n31\n32\n33\n34\nNumber of Parameters\nPSNR(dB)\nTensoRF-CP-384\nNeRF\nTensoRF-VM-48\nINGP\nTensoRF-VM-96\nTensoRF-VM-192\nPoint-NeRF\nOurs-48\nOurs-24\nFigure 1: We compare with previous methods in terms of\nrendering quality (PSNR) and model capacity (number of\nparameters) on the NeRF Synthetic dataset on the bottom.\nOur method and TensoRF are shown with different model\nsizes. Our approach consistently achieve better rendering\nquality with fewer model parameters than TensoRF, as well\nas other methods like iNGP. On the top, we show one ex-\nample of visual comparisons of the mic scene that has chal-\nlenging fine-grained geometric structures, where our ap-\nproach captures most of the details and is the closest to\nthe reference.\nNote that the results of NeRF and Point-\nNeRF use 200k optimization steps while the rest use only\n30k steps.\nequally complex over the entire space, which can be inef-\nficient (requiring high model capacity) to accurately model\nintricate local scene details (see Fig.1).\nWe aim to accurately and compactly model a 3D scene\nand reproduce the complex local details. To this end, we\npropose Strivec, a novel neural scene representation that\nutilizes sparsely distributed and compactly factorized local\ntensor grids to model a volumetric radiance field for high-\narXiv:2307.13226v2  [cs.CV]  24 Aug 2023\nquality novel view synthesis. As shown in Fig.1, our ap-\nproach is able to accurately model the complex scene struc-\ntures that are not recovered well by previous methods. More\nimportantly, our superior rendering quality is achieved with\nmuch less model capacity.\nIn particular, we base our model on TensoRF [7], a recent\napproach that leverages tensor factorization in radiance field\nmodeling. It is fast, compact, and of high rendering quality.\nTensoRF applies CP and vector-matrix (VM) decomposi-\ntion techniques to factorize a field into vectors and matri-\nces and model the entire scene as a global factorized ten-\nsor. Instead of a single global tensor, we leverage a sparse\nset of multiple small local tensors distributed around the\nscene surface for more efficient scene modeling. Specifi-\ncally, each of our tensors represents a local radiance field in-\nside its local bounding box and is compactly modeled with\nfactorized triple vectors based on the CP decomposition.\nNote that the global CP decomposition in TensoRF has\nled to a highly compact model but cannot achieve compa-\nrable rendering quality to their VM decomposition. This\nis because a tri-vector CP component is rank-one, while a\nglobal feature grid of an entire 3D scene is often complex\nand of high rank, requiring a large (impractical) number of\nCP components for high accuracy. TensoRF addresses this\nby introducing matrix factors in their VM decomposition,\nessentially increasing the rank of each tensor component.\nOur model instead consists of multiple small tensor grids,\nexploiting local spatial commonalities in a scene. Com-\npared to a global tensor, our local tensor is less complex and\nof much lower rank, thus effectively reducing the required\nnumber of CP components (per tensor) and enabling prac-\ntical high-quality radiance field reconstruction with highly\ncompact tri-vector factors. Our local tri-vector tensors can\nlead to superior rendering quality and compactness over\nTensoRF\u2019s VM model (see Fig. 1). We also observe that our\nlocal tensors are generally more robust than a global tensor\nagainst the orientation of spatial axes (which can affect the\nrank of a tensor and thus affects the quality; see Fig. 2).\nImportantly, adopting local tensors (instead of a global\none) also brings us the flexibility to allocate neural features\naccording to the actual scene distribution, enabling more\nefficient scene modeling and better usage of model param-\neters than a global representation. To do so, we pre-acquire\ncoarse scene geometry \u2013 that can be easily achieved via\na fast RGB\u03c3 volume reconstruction (like DVGO [36]) or\nmulti-view stereo (like Point-NeRF [43]) \u2013 to directly dis-\ntribute local tensors around the actual scene surface, leading\nto a sparse scene representation that avoids unnecessarily\nmodeling the empty scene space. Note that while previous\nmethods have also leveraged sparse representations (with\nvoxels [22, 45] or points [43]) of radiance fields, their lo-\ncal features are modeled and optimized independently. Our\nmodel instead correlates a group of local features inside a\nlocal box and compactly express them with triple vectors,\nuniquely exploiting the local spatial coherence along axes\nand imposing local low-rank priors in the feature encoding\nvia tensor factorization. Moreover, unlike previous sparse\nrepresentations that only use a single-scale feature grid or\npoint cloud, we distribute multi-scale local tensors to effec-\ntively model the scene geometry and appearance at multiple\nscales in a hierarchical manner. In particular, for an arbi-\ntrary 3D location, we aggregate the neural features from its\nneighboring tri-vector components at all scales and decode\nthe volume density and view-dependent color from the ag-\ngregated features for radiance field rendering.\nOur approach takes the best of previous local and global\nradiance field representations. Compared with global repre-\nsentations like TensoRF and Instant-NGP, our model takes\nadvantage of the sparsity of a scene more directly; com-\npared with local representations like Plenoxels and Point-\nNeRF, our model makes use of the local smoothness and\ncoherence of scene geometry and appearance. As shown in\nour experimental results on both synthetic and real datasets,\nour model is able to achieve state-of-the-art rendering qual-\nity on these datasets, outperforming previous methods, in-\ncluding TensoRF and Instant-NGP, while using significantly\nfewer model parameters, demonstrating the superior repre-\nsentational power of our model.\n2. Related Work\nScene representations.\nTo represent a 3D scene, tradi-\ntional and learning-based methods have studied various rep-\nresentations, such as depth map [16, 21], mesh [18, 40, 34],\npoint cloud [32, 1, 39] and implicit function [10, 25, 28, 44].\nIn recent years, continuous neural field representations\nstand out in various 3D tasks such as single-view 3D recon-\nstruction [42, 14], surface completion [11, 30], multi-view\nreconstruction [28] and novel view synthesis [26, 24]. Com-\npared with traditional discrete representations, a continuous\nfield have no limitation on spatial resolution, e.g., volume\nresolution or the number of points. It can also naturally be\nrepresented by neural networks, such as an MLP, which are\nknown for approximating complex functions well.\nNeural field representations.\nSpecifically, NeRF [26]\nrepresents a 3D scene as a radiance field with a global co-\nordinate MLP, which models geometry, lighting and tex-\nture information jointly, leading to photo-realistic render-\ning quality in novel view synthesis. Apart from its advan-\ntage, purely MLP-based NeRF models [3, 38] in general\nsuffer from inefficiency [2] when modeling highly complex\nor large-scale scenes, due to limited model capacity, slow\noptimization speed, and the cost of modeling empty space.\nTo model radiance fields more efficiently, recent works\nhave explored combining neural fields with various tradi-\ntional 3D representations, including voxels [22, 45, 36, 48]\nand points [43].\nLow-rank representations such as tri-\nplane [6, 13] and tensor decomposition [7, 29] have also\nbeen studied. In particular, DVGO [36] and Plenoxels [45]\nrespectively use dense and sparse voxels with neural fea-\ntures for radiance field modeling.\nWhile being efficient\nto optimize, these localized feature grid-based represen-\ntations lead to a large model size and can face overfit-\nting issues when the features are of very high resolution.\nConsequently, DVGO can also work with a low-resolution\ngrid and Plenoxels requires additional spatial regularization\nterms. On the other hand, recent works have adopted global\nfeature encoding to express a high-resolution feature grid,\nincluding Instant-NGP [27] that hashes spatial features into\nmulti-scale hash tables and TensoRF [7] that factorizes a\nfeature grid into vector and matrix factors. These global fea-\nture encoding methods exploit the spatial correlation across\nthe entire scene space, leading to fast and compact recon-\nstruction and surpassing previous MLP-based or grid-based\nrepresentations on rendering quality. However, similar to\nNeRF, such global representation can also be limited by its\nmodel capacity when representing highly complex or large-\nscale content.\nOur approach instead combines local and global repre-\nsentations. Our tri-vector fields are sparsely distributed in\nthe scene, similar to local representations (like plenoxels\nand Point-NeRF); meanwhile, features in each field are rep-\nresented by tri-vector components shared across the local\nregion as done in TensoRF, exploiting spatial feature com-\nmonalities. Our model leverages both spatial sparsity and\ncoherence, leading to much higher compactness and better\nreconstruction quality than previous local and global repre-\nsentations (see Tab. 1).\nRelevant to our work, previous methods, such as Kilo-\nNeRF [33] and BlockNeRF [37] have also utilized multiple\nlocal MLPs to represent a scene. Specifically, KiloNeRF fo-\ncuses and speeding up NeRF and their rendering quality is\nsacrificed; BlockNeRF essentially uses multiple NeRFs to\nincrease the total model capacity. Instead of pure MLPs, our\nwork is built upon tensor factorization-based feature encod-\ning as done in TensoRF [7], and we in fact achieve superior\nrendering quality while decreasing the model capacity.\n3. Sparse Tri-Vector Field Representation\nWe now present our novel radiance field representation.\nIn essence, our model consists of a cloud of small local tri-\nvector tensors at multiple scales, designed to leverage both\nsparsity and multi-scale spatial coherence (see Fig. 2).\nLet T = {\u03c4n|n = 1, ..., N} denote a cloud of tri-vector\ntensors. Each local tensor \u03c4 is located at p, covering a local\ncuboid space \u03c9 with an edge length of l. This cloud of tri-\nvector tensors represents a radiance field for the 3D space:\n\u2126 =\nN\n[\nn=1\n\u03c9n.\n(1)\nHere, each tensor \u03c4 encodes a local multi-channel fea-\nture grid that includes a (single-channel) density grid A\u03c3\nand a (multi-channel) appearance grid Ac, similar to the\ntensor grid in TensoRF [7]. In contrast to using a single\nglobal tensor in TensoRF [7], we model the volume den-\nsity and view-dependent colors with multiple local tensors.\nIn particular, for an arbitrary location \u03c7 \u2208 \u2126, we select M\nnearest tensors that cover \u03c7. Across the selected tensors, we\naggregate the extracted density and appearance features re-\ncovered by their tri-vector factors for radiance field property\nregression, where the volume density \u03c3 is directly obtained\nafter the aggregation and the view-dependent color c is re-\ngressed by a small MLP \u03c8 along with the viewing direction\nd. The continuous radiance field can be expressed as:\n\u03c3\u03c7, c\u03c7 = A\u03c3({G\u03c3(\u03c7)}), \u03c8(Ac({Gc(\u03c7)}), d).\n(2)\n3.1. Local tri-vector tensors.\nWe apply the classic Canonical polyadic (CP) decompo-\nsition [5] to model our local tensors with tri-vector compo-\nnents.\nCP decomposition.\nCP decomposition factorizes a M di-\nmension tensor \u03c4 \u2208 RI1\u00d7I2\u00d7...\u00d7IM into a linear combina-\ntion of R rank-1 tensors:\n\u03c4 =\nR\nX\nr=1\n\u03bbrv0\nr \u2297 v1\nr \u2297 ..., \u2297vM\nr ,\n(3)\nwhere \u2297 denotes outer product; the weighting factor \u03bbr can\nbe absorbed into vectors {v0\nr, ..., vM\nr }.\nDensity and appearance tensors.\nIn our case of mod-\neling a 3D radiance field, we set the geometry grid G\u03c3 \u2208\nRI\u00d7J\u00d7K as a 3D tensor. And the multi-channel appearance\ngrid Gc \u2208 RI\u00d7J\u00d7K\u00d7P corresponds to a 4D tensor. The\nfourth appearance mode is of lower dimension (compared\nwith the spatial modes), representing the final dimension of\nthe features sent to the MLP decoder network.\nAccording to Eqn.3, we factorize each tensor\u2019s feature\ngrids, G\u03c3 and Gc, by CP decomposition:\nG\u03c3 =\nR\u03c3\nX\nr=1\nA\u03c3,r =\nR\u03c3\nX\nr=1\nvX\n\u03c3,r \u2297 vY\n\u03c3,r \u2297 vZ\n\u03c3,r,\n(4)\nGc =\nRc\nX\nr=1\nAc,r \u2297 br =\nRc\nX\nr=1\nvX\nc,r \u2297 vY\nc,r \u2297 vZ\nc,r \u2297 br, (5)\n\ud835\udc65\nScale 1\nScale \ud835\udc60\nTensor Factorization\n...\nNeighbor \nAggregation\n\ud835\udca2\ud835\udc4e\ud835\udc50, \ud835\udca2\ud835\udc4e\ud835\udf0e\n\ud835\udca2\ud835\udc4f\n\ud835\udc50, \ud835\udca2\ud835\udc4f\n\ud835\udf0e\n\ud835\udc531\n\ud835\udc50, \ud835\udc531\n\ud835\udf0e\n\ud835\udc53\ud835\udc60\ud835\udc50, \ud835\udc53\ud835\udc60\ud835\udf0e\nDecoding\na\nb\n...\n\ud835\udc50\ud835\udc65\n\ud835\udf0e\ud835\udc65\n\ud835\udc53\ud835\udf0e\n\ud835\udc53\ud835\udc50\nMultiscale \nAggregation\n...\nFigure 2: Overview of our Sparse Tri-Vector Radiance Fields. We distribute our local tensors based on a coarse geometry\nestimated by a fast RGB\u03c3 volume reconstruction as done in DVGO [36]. Here, we show our model running under S = 2\nscales. Each local tensor is factorized as axis-aligned triple based on CP decomposition. For any shading point \u03c7, we extract\nand evaluate features in each local tensor, according to the factorization (Sec. 3.1). Then, we aggregate these features among\nnearby tensors (Sec. 3.2) and across different scales (Sec. 3.3). Finally, the density and color are decoded (Sec. 3.4) and used\nby volume rendering (Sec.4).\nHere R\u03c3 and Rc denote numbers of component; A\u03c3,r and\nAc,r are the component tensors that are factorized spatially;\nvX\n\u03c3,r, ..., vX\nc,r, ... are the 1D vectors with resolution I, J, K,\nmodeling scene geometry and appearance along X, Y, Z\naxis; R\u03c3 and Rc are the component numbers; br expresses\nthe feature dimension.\nAs done in TensoRF [7], we stack all feature-mode vec-\ntors br as columns together, which ends up a P \u00d7Rc appear-\nance matrix B. This matrix models the appearance feature\nvariations of the tensor and functions like a appearance dic-\ntionary. Note that naively following CP decomposition like\nTensoRF will assign a different appearance matrix for every\nlocal tensor. Instead, we propose to utilize a global appear-\nance matrix Bc shared across the entire cloud of local ten-\nsors, leading to a global appearance dictionary that explains\nthe color correlations across scene. This further improves\nboth the computational efficiency and model compactness\nof our model.\nTherefore, each of our local tensors is represented by\ntheir unique local tri-vector factors vX\nr , vY\nr , vZ\nr .\nFeature evaluation.\nTo achieve a continuous field, we\nconsider trilinear interpolation when evaluating the tensor\ngrid features. For a location \u03c7, we first compute its relative\nposition \u02dc\u03c7 to the selected tensor located at p:\n\u02dcx, \u02dcy, \u02dcz = x \u2212 px, y \u2212 py, z \u2212 pz.\n(6)\nThen, for example, to get A\u03c3,r at (\u02dcx, \u02dcy, \u02dcz), we can com-\npute and trilinearly interpolate eight A\u03c3,r on the corners.\nAs mentioned in [7], applying linear interpolation on each\nvector first is mathematically equivalent and can reduce the\ncomputation cost. Under the rule of outer product, we have\nAr,i,j,k = vX\nr,ivY\nr,jvZ\nr,k, then the interpolated density fea-\ntures at location \u03c7 are:\nG\u03c3(\u03c7) =\nX\nr\nvX\n\u03c3,r(\u02dcx)vY\n\u03c3,r(\u02dcy)vZ\n\u03c3,r(\u02dcz) =\nX\nr\nA\u03c3,r(\u02dc\u03c7), (7)\nwhere vX\n\u03c3,r(\u02dcx) is vX\n\u03c3,r\u2019s linearly interpolated value at (\u02dcx)\nalong its X axis. Here, G\u03c3(\u03c7) is a scalar.\nSimilarly, the interpolated appearance features can be\ncomputed as:\nGc(\u03c7) =\nX\nr\nvX\nc,r(\u02dcx)vY\nc,r(\u02dcy)vZ\nc,r(\u02dcz)br\n(8)\n=\nX\nr\nAc,r(\u02dc\u03c7)br\n(9)\n= B \u00b7 (\u2295[Ac,r]r),\n(10)\nwhere \u201c\u2295\u201d denotes concatenation, \u201c\u00b7\u201d denotes dot product.\nThe appearance feature Gc(\u03c7) \u2208 RP is a vector.\n3.2. Feature aggregation.\nWe propose to aggregate the features from M neigh-\nboring tensors to jointly model the volume density and ap-\npearance for each 3D location \u03c7. In particular, inspired by\nPoint-NeRF, we leverage an inverse distance-based weight-\ning function to directly aggregate the multi-tensor features.\nSpecifically, this weight can be expressed by\nwm =\n1\n\u2225pm \u2212 \u03c7\u2225.\n(11)\nWith this weight function, we directly obtain the density\nfeature via the weighted sum:\nf \u03c3(\u03c7) =\n1\nP wm\nM\nX\nm=1\nwmG\u03c3\nm(\u03c7).\n(12)\nSimilarly, the appearance feature aggregation can be ex-\npressed in a similar way, while using the shared appearance\nmatrix (as described in Sec. 3.1) across local tensors:\nf c(\u03c7) =\n1\nP wm\nM\nX\nm=1\nwmGc\nm(\u03c7)\n(13)\n=\n1\nP wm\nM\nX\nm=1\nwmBc \u00b7 (\u2295[Ac,r]r)\n(14)\n=\n1\nP wm\nBc \u00b7 (\nM\nX\nm=1\nwm(\u2295[Ac,r]r)).\n(15)\nNote that owing to sharing the appearance matrix across\ntensors, we reduce the computational complexity from\nO(M \u00b7 P \u00b7 Rc) in Eqn.14, to O((M + P) \u00b7 Rc) in Eqn.15.\n3.3. Multi-scale tri-vector fields.\nComplex 3D scenes often contain multi-frequency ge-\nometry and appearance details. This motivates us to build\nmulti-scale tensor clouds to discover the local geometry and\nappearance commonalities at multiple scales. Our final ra-\ndiance field is modeled by multiple tri-vector tensor clouds\nat S different scales. Different clouds consist of tensors\nwith different resolutions.\nTo regress the density and appearance at a location \u03c7,\nwe gather the density and appearance features from a set of\ntensor clouds that cover \u03c7, {Ts|1 \u2264 s \u2264 S, \u03c7 \u2208 \u2126s}. Please\nnote that tensor clouds of certain scales might not cover the\nlocation, so that \u2225{Ts}\u2225 \u2264 S. We simply compute the mean\nfeatures across these scales:\nf \u03c3(\u03c7) =\n1\n\u2225{Ts}\u2225\nX\ns\nf \u03c3\ns (\u03c7),\n(16)\nf c(\u03c7) =\n1\n\u2225{Ts}\u2225\nX\ns\nf c\ns(\u03c7).\n(17)\nNote that f \u03c3(\u03c7) and f c(\u03c7) are the final density and ap-\npearance features we aggregate across multiple scales and\nmultiple neighboring tensors.\n3.4. Decoding.\nWe apply softplus activation on the density feature\nf \u03c3(\u03c7) to obtain the final volume density and regress the\nview-dependent color by sending the appearance feature\nf c(\u03c7) and the viewing direction d to the MLP decoder \u03c8.\n4. Rendering and Reconstruction\nVolume Rendering\nWe evaluates each pixel\u2019s color with\nphysically-based volume rendering via differentiable ray\nmarching.\nFollowing NeRF [26], we sample Q shading\npoints at {\u03c7q | q = 1, ..., Q} along the ray, and accumu-\nlate radiance by density:\nc =\nQ\nX\nq=1\nTq(1 \u2212 exp(\u2212\u03c3q\u03b4q))cq,\nTq = exp(\u2212\nq\u22121\nX\nt=1\n\u03c3t\u03b4t).\n(18)\n\u03c3q and cq are the density and radiance at shading points; \u03b4t\nis the marching distance per step; T denotes transmittance.\nDistributing local tensors.\nFirst of all, to better leverage\nthe sparsity of a scene, we first obtain a geometric prior that\nroughly covers the scene geometry. The geometric prior\ncan be in any commonly-used form, e.g., point cloud, oc-\ncupancy grid, octree, or mesh vertices. Then we can uni-\nformly distribute tensors in the spatial neighborhood of the\ngeometry. For a multi-scale model, each of the scale will\nbe distributed independently. For most of our results, we\nquickly optimize a coarse RGBA volume from the multi-\nview images and use the optimized occupancy grid as the\ngeometry prior, as done in DVGO [36], which finishes in\nseconds.\nTo maintain training stability and speed, each tensor \u03c4\u2019s\nposition p and coverage \u03c9 is fixed once determined. We also\ninitialize the 3(R\u03c3 + Rc) vectors (vX\n\u03c3,r, ..., vX\nc,r, ...) of each\ntensor by normal distribution. For each scale s, a P \u00d7 Rc\nappearance matrix Bc\ns is shared by all tri-vector tensors of\nthat scale. Specifically, \u201cBc\u00b7()\u201d in Eqn.15 can be efficiently\nimplemented as a fully-connected neural layer. Therefore,\nBc for each scale and a global appearance MLP \u03c8 will be\nimplemented as neural networks and initialized by default\nmethods [15].\nOptimization and objectives.\nGiven a set of multi-view\nRGB images with camera poses, the sparse tri-vector radi-\nance field is per-scene optimized to reconstruct the radiance\nfields, under the supervision of the ground truth pixel col-\nors. Following the volume rendering equation 18, the L2\nrendering loss can be past back to the global MLP and ag-\ngregated features, then, all the way to the the appearance\nmatrices and the feature vectors of local tensors.\nWe apply a rendering loss to supervise the reconstruc-\ntion and also apply an L1 regularization loss on density fea-\nture vectors v\u03c3,r to promote geometry sparsity and to avoid\nMethod\nBatchSize\nSteps\nTime\u2193\n# Param.(M)\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPSV gg \u2193\nLPIPSAlex \u2193\nNeRF[24]\n4096\n300k\n35.0h\n1.25\n31.01\n0.947\n0.081\n-\nPlenoxels[45]\n5000\n128k\n11.4m\n194.50\n31.71\n0.958\n0.049\n-\nDVGO[36]\n5000\n30k\n15.0m\n153.00\n31.95\n0.960\n0.053\n0.035\nPoint-NeRF200k[43]\n4096\n200k\n5.5h\n27.74\n33.31\n0.962 *\n0.049\n0.027\nInstantNGP[27]\n10k-85k\n30k\n3.9m\n11.64\n32.59\n0.960\n-\n-\nTensoRF-CP[7]\n4096\n30k\n25.2m\n0.98\n31.56\n0.949\n0.076\n0.041\nTensoRF-VM[7]\n4096\n30k\n17.4m\n17.95\n33.14\n0.963\n0.047\n0.027\nOurs-24\n4096\n30k\n34.3m\n7.07\n33.24\n0.963\n0.046\n0.026\nOurs-48\n4096\n30k\n35.7m\n13.52\n33.55\n0.965\n0.044\n0.025\nTable 1: Comparisons of our method with other radiance-based models [24, 41, 22, 43, 7, 27] on the Synthetic-NeRF dataset\n[24]. Ours-24 is the one with 24 components while Ours-48 is the one with 48 components. We report the corresponding\nrendering quality (PSNR, SSIM, and LPIPS), model capacity (number of parameters), and training time, batch size and steps.\nOur model achieves the best rendering quality with a compact model size. We report PointNeRF\u2019s updated SSIM.\noverfitting as done in TensoRF [7]:\nLr = \u2225C \u2212 \u02dcC\u22252\n2,\n(19)\nLL1 = 1\nN\nR\u03c3\nX\nr\n\u2225v\u03c3,r\u2225,\n(20)\nwhere \u02dcC is the ground truth pixel color, \u2225v\u03c3,r\u2225 is the sum\nof absolute values of elements on density vectors, and N is\nthe total number of elements. The total loss is:\nL = Lr + \u03b1LL1.\n(21)\nWe set the weight of the sparsity term \u03b1 as 1e\u22125 by default.\n5. Implementation\nTo obtain coarse scene geometry, we modify the coarse\ndensity estimation introduced in [36] and get a 1003 occu-\npancy volume in 30 seconds. We can skip this step if there\nexists available geometric data, e.g., the meshes in Scan-\nNet [12], or point clouds from multiview stereo. According\nto the experiments, our method is not very sensitive to the\ninitial geometry. Please refer to Appendix.B. for more de-\ntails. We set the default number of scales to 3. In a scene\nbox of [-1,1], we rasterize the scene geometry (occuppied\nvoxels centers or points) onto 3 grids with different voxel\nsizes, e.g., 0.43, 0.23, 0.13. For each grid, we distribute tri-\nvector tensors at the center of its occupied voxels. The ten-\nsor spatial coverage of these 3 scales is 0.63, 0.33, 0.153,\nrespectively. For each scale, we query M = 4 nearby ten-\nsors. Following [36], our feature decoding network \u03c8 is a\n2-layer MLP with 128 channels. For each scale, its appear-\nance matrix Bc is implemented by a single linear layer of\n27 channels.\nWe implement the framework in PyTorch [17] with cus-\ntomized CUDA operations. During optimization, we adopt\nthe coarse to fine strategy in [7], linearly up-sample the\nvectors\u2019 dimension (I, J, K) from 29 to 121 for scale one,\n15 to 61 for scale two, and 7 to 31 for scale three. The\nup-sampling happens at step 2000, 3000, 4000, 5500, and\n7000. We use the Adam optimizer [19] with initial learning\nrates of 0.02 for vectors and 0.001 for networks. On a single\n3090 RTX GPU, we train each model for 30000 steps while\neach batch contains 4096 rays. Please find more details in\nthe supplemental materials.\n6. Experiments\n6.1. Evaluation on the NeRF Synthetic Dataset.\nWe first evaluate our method on the Synthetic-NeRF\ndataset [24] and the quantitative results compared with\nother methods are reported in Tab.1, including NeRF [26],\nPlenoxels [45], DVGO [36], Point-NeRF [43], iNGP [27],\nand TensoRF [7]. We report our models of two different\nmodel sizes with different numbers of components; both\nsettings are with the same 3 scales of local tensors.\nOur approach achieves the best averaged PSNR,\nLPIPSV gg and LPIPSAlex in all the methods, leading to\nsuperior visual quality as shown in Fig. 3 Meanwhile, our\nhigh rendering quality is achieved with a compact model\nsize. When compared with local voxel-based representa-\ntions, such as Plenoxels and DVGO, our approach are sig-\nnificantly better.\nOn the other hand, global featuring encoding-based\nmethods, like iNGP and TensoRF, are known for their high\ncompactness and can achieve higher rendering quality than\nlocal voxel-based methods. Nonetheless, our method can\nstill outperform them. Note that, even our smaller model\n(Ours-24) leads to better rendering quality than both iNGP\nand TensoRF that leverage global feature encoding, while\nour model uses significantly fewer parameters (about 60%\nand 40% of the size of iNGP and TensoRF). This clearly\ndemonstrates the high visual quality and high compatness\nof our model with our sparsely distributed tri-vector tensors.\nIn all the baseline methods, Point-NeRF is able to\nachieve relatively higher rendering quality than others.\nOurs-48\nINGP\nReference\nTensoRF-VM-192\nPoint-NeRF\nFigure 3: Qualitative comparisons on the NeRF Synthetic dataset [26].\nHowever, this is enabled by optimizing their model for 300k\nsteps with a long period of 5.5 hours.\nIn contrast, our\nmethod achieves higher quality with significantly fewer op-\ntimization steps (only 30k) and optimization time (about 36\nmin). As expected, our model is slower to optimize than\nTensoRF due to the additional costs of multi-tensor aggre-\ngation. However, though speed is not our focus in this work,\nour model can still converge quickly and lead to signifi-\ncantly faster reconstruction than MLP-based methods, such\nas NeRF, as well as Point-NeRF that is point-based.\nPerformance w.r.t.\nrotation.\nWe observe that tensor\nfactorization-based methods can be sensitive to the orien-\ntation of the coordinate frames, since axis-aligned features\nare used; in essence, this is because the rank of a sparse ten-\nsor is sensitive to rotation, as shown in Fig. 4. Especially,\nthis can benefit reconstruction on synthetic synthetic scenes\nwhere the objects are perfectly aligned with the axes, e.g.\nthe lego scene in the NeRF synthetic data. However, we\nfind that our method based on local tensors are more robust\nagainst the orientation of the axes than a global TensoRF. In\nparticular, we compare our method with TensoRF in Tab.2\nwith different degrees of rotation around the Z axis on two\nscenes, lego (which is strongly aligned with axes) and chair\n(which is less aligned and thus less affected ) . As shown in\nthe table, while both methods are affected by the rotation,\nour method has much smaller drops of PSNRs.\n6.2. Evaluation on the real datasets.\nThe ScanNet dataset.\nWe evaluate our method on the real\ndataset, ScanNet [12] with the two scenes selected by NSVF\n[22], and compare with other methods.\nWe follow the\nsame experiment setting as done in NSVF [22], using the\nprovided mesh to distribute our tensors, and optimize our\nFigure 4: A toy example to illustrate the TensoRF-CP with\nglobal decomposition in (left) axis-aligned and (right) non-\naxis-aligned situations. The bottom shows the grid values.\nIn axis-aligned case, only 1 component is needed to rep-\nresent the scene (vector bases recover grid values by outer\nproduct). In non-axis-aligned case, however, 3 components\nare needed because the rank of matrix changes from 1 to 3\nafter scene rotation. While our design with local low-rank\ntensors can alleviate this issue.\nchair / lego\nrot 0\u25e6\nrot 5\u25e6\nrot 45\u25e6\nTensoRF-CP\n33.60 / 34.50\n32.90 / 29.79\n32.50 / 28.57\nTensoRF-VM\n35.76 / 36.46\n34.91 /32.53\n34.55 / 32.31\nOurs-48\n35.88 / 36.52\n35.72 / 35.37\n35.64 / 34.97\nTable 2: Comparisons of our method with TensoRF [7] on\nthe chair and lego scenes of Synthetic-NeRF dataset [24]\nwhen considering rotation of different angles around z-axis.\nmodel, TensoRF for the same 100k steps for fair compar-\nisons. Please note that Point-NeRF uses all scanned depth\nimages as initial geometry instead of meshes. Therefore, we\nOurs\nTensoRF-VM\nReference\nPoint-NeRF\nFigure 5: Qualitative comparisons on the ScanNet dataset.\nAverage over Scene 101 and Scene 241\nPSNR \u2191\nSSIM \u2191 LPIPSAlex \u2193# Param.(M) \u2193\nSRN [35]\n18.25\n0.592\n0.586\n-\nNeRF [24]\n22.99\n0.620\n0.369\n-\nNSVF [22]\n25.48\n0.688\n0.301\n-\nPoint-NeRF[43]\n25.92\n0.891\n0.273\n159.01\nTensoRF-CP[7]\n27.54\n0.751\n0.328\n0.97\nTensoRF-VM[7]\n28.61\n0.787\n0.290\n49.92\nOurs-48\n29.05\n0.792\n0.243\n12.82\nTable 3: Quantitative comparison on two scenes in the\nScanNet dataset [12]. Point-NeRF, TensoRF-CP, TensoRF-\nVM and Ours-48 are optimized for 100k steps.\nalso obtain the results of Point-NeRF 100k steps from the\nauthors, using the provided mesh for fairness. We find the\nScannet data has many holes in the provided mesh geome-\ntry, while methods, such as NSVF and Point-NeRF, require\naccurate initial geometry; though Point-NeRF can poten-\ntially fix them with it point growing technique as shown in\ntheir original paper, it is not able to fully address them in\n100k step optimization and lead to holes in their final ren-\ndering. Our approach instead does not require very accu-\nrate coarse geometry, since our local tensors cover relatively\nlarge regions. We show the quantitative results in Tab. 3 and\nqualitative results in Fig. 5. Note that our method can also\nperform well on real scenes, achieving the highest perfor-\nmance in terms of PSNR and LPIPSAlex, while using the\nsecond smallest model size (the smallest one is TensoRF-\nCP100k). Our visual quality is also higher than the compar-\nison methods.\nThe Tanks and Temples dataset.\nWe also evaluate our\nmethod on another real dataset, Tanks and Temples [12]\nwith the 5 object scenes selected by NSVF [22]. We us-\ning the very coarse geometries estimated by DVGO[36]\nto distribute our tensors. We follow the same experiment\nsetting as done in TensoRF [7], optimizing our model for\nthe same 30k steps for fair comparisons. As is shown in\nTab. 3, our method outperforms other methods in terms of\nPSNR, SSIM and LPIPSAlex, while using the second small-\nest model size.\nScale\nPSNR \u2191 SSIM \u2191 # Param.(M) \u2193 Time \u2193\nSingle(0.6)\n32.22\n0.957\n1.75\n18.22m\nSingle(0.3)\n32.73\n0.961\n4.15\n21.31m\nSingle(0.15)\n31.96\n0.952\n10.20\n28.55m\nMulti(0.6, 0.3)\n33.11\n0.963\n6.20\n30.12m\nMulti(0.6, 0.3, 0.15)\n33.55\n0.965\n13.52\n35.70m\nTable 4:\nAblation under different scale settings on\nSynthetic-NeRF dataset. We select 3 scales of tensors with\ncube sizes of 0.6, 0.3, and 0.15.\n6.3. Ablation study\nWe analyze our model in terms of different scales in Ta-\nble.4, while keeping the number of components the same\n(here we use 48). The scale here is the size of our local\ntensors of each axis. We considered 3 different scales, i.e.,\n0.6, 0.3, and 0.15 respectively as single-scale settings and\nsome of their combinations as multi-scale settings. Note\nthat even with a single scale (0.3), the performance of our\nmethod can be comparable with some other methods such\nas iNGP [27] while ours have less than half of the model\nsize. When increasing the number of scales or decreasing\nthe size of local tensors, our model size will also increase.\nIn general, there is a trade-off of our method between scales\nand computational consumption (time and size).\nUsually, a smaller scale can lead to better performance,\nthough our method with a scale of 0.15 does not strictly\nfollow because we don\u2019t have high-quality input geome-\ntry to place these local tensors with a very small size. In\nfact, according to our per-scene breakdown results on the\nSynthetic-NeRF dataset (please refer to our supplemental\nmaterial), single-scale(0.075) can achieve higher perfor-\nmance than single-scale(0.3) and single-scale(0.15) on most\nscenes, except for ship because it has many thin structures\nthat our coarse reconstruction does not cover.\nPSNR \u2191 SSIM \u2191 LPIPSAlex \u2193 # Param.(M) \u2193\nNeRF [26]\n25.78\n0.864\n0.198\n-\nNSVF [22]\n28.40\n0.900\n0.153\n-\nTensoRF-CP30k[7]\n27.59\n0.897\n0.144\n0.97\nTensoRF-VM30k[7]\n28.56\n0.920\n0.125\n49.92\nOurs-4830k\n28.70\n0.922\n0.113\n14.11\nTable 5: Quantitative comparison on scenes in the Tanks\nand Temples dataset [20] selected in NSVF [22]. TensoRF-\nCP, TensoRF-VM and Ours-48 are optimized for 30k steps.\nWe also compare our method with a variant that uses\nvector-matrix (VM) decomposition [7] in each local tensor\ninstead of CP decomposition. Please refer to Appendix.A.\nfor more details. Also, we can achieve a higher training\nand inference speed without a significant performance drop,\nwhich we refer to Appendix.E.\n7. Conclusion\nIn this work, we have presented a novel approach\nfor high-quality neural scene reconstruction and photo-\nrealistic novel view synthesis.\nWe propose a novel ten-\nsor factorization-based scene representation, which lever-\nages CP decomposition to compactly model a 3D scene as\na sparse set of multi-scale tri-vector tensors that express lo-\ncal radiance fields. Our representation leverages both spar-\nsity and spatial local coherence, and leads to accurate and\nefficient modeling of complex scene geometry and appear-\nance. We demonstrate that the sparse tri-vector radiance\nfields can achieve superior rendering quality than previous\nstate-of-the-art neural representations, including TensoRF\nand iNGP, while using significantly fewer parameters.\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas Guibas. Learning representations and generative\nmodels for 3D point clouds. In International Conference on\nMachine Learning (ICML), pages 40\u201349, 2018. 2\n[2] Relja Arandjelovi\u00b4c and Andrew Zisserman.\nNerf in de-\ntail: Learning to sample for view synthesis. arXiv preprint\narXiv:2106.05264, 2021. 2\n[3] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages 5855\u2013\n5864, 2021. 1, 2\n[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5470\u20135479, 2022. 13\n[5] J Douglas Carroll and Jih-Jie Chang. Analysis of individual\ndifferences in multidimensional scaling via an n-way gener-\nalization of \u201ceckart-young\u201d decomposition. Psychometrika,\n35(3):283\u2013319, 1970. 1, 3\n[6] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 16123\u201316133, 2022.\n3\n[7] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision (ECCV), pages 333\u2013350.\nSpringer, 2022. 1, 2, 3, 4, 6, 7, 8, 9, 12, 13, 14, 15\n[8] Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su,\nand Andreas Geiger. Factor fields: A unified framework for\nneural fields and beyond, 2023. 13\n[9] Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su,\nand Andreas Geiger. Factor fields: A unified framework for\nneural fields and beyond. arXiv preprint arXiv:2302.01226,\n2023. 14\n[10] Zhiqin Chen and Hao Zhang. Learning implicit fields for\ngenerative shape modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2019. 2\n[11] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll.\nImplicit functions in feature space for 3d shape reconstruc-\ntion and completion.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 6970\u20136981, 2020. 2\n[12] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes.\nIn\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5828\u20135839, 2017.\n6, 7, 8\n[13] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 12479\u201312488,\n2023. 3\n[14] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna,\nand Thomas Funkhouser. Local deep implicit functions for\n3d shape. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n4857\u20134866, 2020. 2\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into rectifiers: Surpassing human-level per-\nformance on imagenet classification. In Proceedings of the\nIEEE international conference on computer vision (ICCV),\npages 1026\u20131034, 2015. 5\n[16] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra\nAhuja, and Jia-Bin Huang. Deepmvs: Learning multi-view\nstereopsis. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 2821\u2013\n2830, 2018. 2\n[17] Sagar Imambi, Kolla Bhanu Prakash, and GR Kanagachi-\ndambaresan. Pytorch. Programming with TensorFlow: Solu-\ntion for Edge Computing Applications, pages 87\u2013104, 2021.\n6\n[18] Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and\nJitendra Malik. Learning category-specific mesh reconstruc-\ntion from image collections.\nIn European Conference on\nComputer Vision (ECCV), 2018. 2\n[19] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 6\n[20] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\nKoltun. Tanks and temples: Benchmarking large-scale scene\nreconstruction. ACM Transactions on Graphics, 36(4), 2017.\n9, 13, 15\n[21] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid.\nLearning depth from single monocular images using deep\nconvolutional neural fields.\nIEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 38(10):2024\u20132039,\n2016. 2\n[22] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua,\nand Christian Theobalt. Neural sparse voxel fields. arXiv\npreprint arXiv:2007.11571, 2020. 2, 6, 7, 8, 9, 14, 15\n[23] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: Learning dynamic renderable volumes from images.\narXiv preprint arXiv:1906.07751, 2019. 15\n[24] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,\nJonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. Nerf in the wild: Neural radiance fields for uncon-\nstrained photo collections. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 7210\u20137219, 2021. 2, 6, 7, 8\n[25] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019. 2\n[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view\nsynthesis.\nIn European Conference on Computer Vision\n(ECCV), pages 405\u2013421. Springer, 2020. 1, 2, 5, 6, 7, 9,\n12, 13, 14, 15\n[27] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics,\n41(4):1\u201315, 2022. 1, 3, 6, 8, 12, 13, 14\n[28] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\nAndreas Geiger. Differentiable volumetric rendering: Learn-\ning implicit 3d representations without 3d supervision. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 2020. 2\n[29] Anton Obukhov, Mikhail Usvyatsov, Christos Sakaridis,\nKonrad Schindler, and Luc Van Gool. Tt-nf: Tensor train\nneural fields. arXiv preprint arXiv:2209.15529, 2022. 3\n[30] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 165\u2013174, 2019.\n2\n[31] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), pages 5865\u20135874, 2021. 1\n[32] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classification\nand segmentation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2017. 2\n[33] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance fields with\nthousands of tiny mlps. arXiv preprint arXiv:2103.13744,\n2021. 3\n[34] Jonathan Richard Shewchuk. Tetrahedral mesh generation\nby delaunay refinement. In Proceedings of the fourteenth\nannual symposium on Computational geometry, pages 86\u2013\n95, 1998. 2\n[35] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wet-\nzstein.\nScene representation networks:\nContinuous 3d-\nstructure-aware neural scene representations. arXiv preprint\narXiv:1906.01618, 2019. 8\n[36] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n5459\u20135469, 2022. 2, 3, 4, 5, 6, 8, 13\n[37] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-\nhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,\nand Henrik Kretzschmar. Block-nerf: Scalable large scene\nneural view synthesis.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 8248\u20138258, 2022. 3\n[38] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Struc-\ntured view-dependent appearance for neural radiance fields.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 5481\u20135490.\nIEEE, 2022. 2\n[39] Jinglu Wang, Bo Sun, and Yan Lu. MVPnet: Multi-view\npoint regression networks for 3D object reconstruction from\na single image. Proceedings of the AAAI Conference on Ar-\ntificial Intelligence (AAAI), 2019. 2\n[40] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei\nLiu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh\nmodels from single RGB images. In European Conference\non Computer Vision (ECCV), 2018. 2\n[41] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini-\nvasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-\nBrualla, Noah Snavely, and Thomas Funkhouser.\nIbrnet:\nLearning multi-view image-based rendering. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2021. 6\n[42] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir\nMech, and Ulrich Neumann. Disn: Deep implicit surface\nnetwork for high-quality single-view 3d reconstruction. Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n32, 2019. 2\n[43] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\nPoint-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5438\u20135448, 2022. 2, 3, 6, 8,\n13, 14\n[44] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan\nAtzmon, Basri Ronen, and Yaron Lipman. Multiview neu-\nral surface reconstruction by disentangling geometry and ap-\npearance. In Proc. NeurIPS, 2020. 2\n[45] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa.\nPlenox-\nels: Radiance fields without neural networks. arXiv preprint\narXiv:2112.05131, 2021. 2, 3, 6\n[46] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\nfields. arXiv preprint arXiv:2010.07492, 2020. 1\n[47] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018. 14\n[48] Xiaoshuai Zhang, Sai Bi, Kalyan Sunkavalli, Hao Su, and\nZexiang Xu.\nNerfusion: Fusing radiance fields for large-\nscale scene reconstruction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 5449\u20135458, 2022. 2\nAppendix\nA. Ablation Studies on Tensor Factorization\nStrategies\n# Comp PSNR \u2191 SSIM \u2191 # Param.(M) \u2193\nMulti(0.6, 0.3, 0.15)\n24\n33.24\n0.963\n7.07\nSingle(0.3)\n96\n33.02\n0.963\n9.15\nVM-Cloud (0.3)\n6\n32.59\n0.959\n11.36\nVM-Cloud (0.3)\n12\n32.99\n0.962\n21.64\nTable 6: (a) Comparisons on our method pairing with dif-\nferent factorization strategies, e.g., CP decomposition and\nvector-matrix (VM) decomposition (row 2 vs 3,4). The lo-\ncal tensors\u2019 edge lengths are all set as 0.3. (b) We also com-\npare a single-scale model with a multi-scale model (row 1\nvs 2). We evaluate these settings on the NeRF Synthetic\ndataset [26] and evaluate them with both rendering quality\nand model capacity (#Param. denotes the number of param-\neters).\nOther than CP decomposition, TensoRF [7] also pro-\nposes vector-matrix (VM) decomposition, which factorizes\na 3D tensor as the summation of vector-matrix bases. Each\nbasis is the outer product of a matrix along a plane, e.g., the\nXY plane, and a vector along an orthogonal direction, e.g.,\nthe Z axis. For comparison, we also explore to replace our\ntri-vector representation with the vector-matrix representa-\ntion for each local tensor. Tab. 6 shows that the single-scale\ntri-vector cloud can outperform the vector-matrix cloud rep-\nresentation with less model capacity.\nIt is not a surprise that our tri-vector cloud representa-\ntion achieves more compactness. It applies more compres-\nsion by factorizing each component of a 3D tensor, with\na space complexity of O(IJK), into three vectors, with a\nspace complexity of O(I + J + K). On the other hand,\nvector-matrix cloud representation factorizes it into three\nvectors and three matrices, which have a space complex-\nity of O(IJ + JK + IK). Even if we reduce the number\nof components, the vector-matrix clouds still require more\nspace than our tri-vector representations.\nIn terms of quality, since our method exploits the spa-\ntial sparsity of natural scenes, we only need to factorize\neach local space independently instead of the entire scene\ntogether. The more compact tri-vector representation can\nbenefit from the appearance coherence in local space and\nresult in better performance. In TensoRF [7], since the en-\ntire space is factorized all at once, the radiance information\nis, in general, less coherent across locations and the CP de-\ncomposition will lead to a shortage of rank.\nB. Ablation Studies on Multi-scale Models\nIn Tab.6, we also compare our multi-scale tri-vector ra-\ndiance fields with the single-scale strategy. In our default\nmodel, we have three scales, composed of tensors with\nlengths 0.15, 0.3, and 0.6, respectively. Similar to the find-\nings in iNGP [27], our multi-scale models provide more\nsmoothness and lead to a better rendering quality than their\nsingle-scale counterparts. The multi-scale model with 24\ncomponents (row 1) can already outperform the single-scale\nmodel (row 2), which has more parameters.\nC. Ablation Studies on the Number of Tensor\nComponents\nWe conduct experiments on the NeRF Synthetic\ndataset [26] to show the relationship between rendering per-\nformance and the number of tensor components. In Tab.7,\nwe compare our multi-scale models with 12, 24, 48, and 96\nappearance components, respectively. In general, more ten-\nsor components will lead to better performance. We also ob-\nserve that the benefit of adding more components becomes\nmarginal when the number reaches 48. We speculate that\nit is harder to learn high-frequency details even though the\nmodel\u2019s capacity can hold high-rank information. Improve-\nment in this aspect can be a promising future direction.\nD. Ablation Studies on Initial Geometry\nWe emphasize that our superior quality stems from our\nnovel scene representation rather than the initial geome-\ntry. The initial geometry is simply acquired from a low-\nres RGBA volume reconstruction, which is coarse and only\nused to roughly prune empty space.\nWe show in Fig. 6 that our approach performs robustly\nwith various choices of these geometry structures and con-\nsistently achieves high PSNRs, even with a much worse\nearly-stopped RGBA reconstruction. This showcases the\nkey to our superior quality is our Strivec model itself.\nIn particular, the self-bootstrap geometry is generated\npurely from our own model with 8 coarse tri-vectors with-\nout existing modules in previous work. Moreover, we can\nalso further prune unoccupied tensors during training but\nwe find this leads to similar quality (0.03db difference) and\nunnecessary extra (+22%) training time. We instead choose\nto use one single initial geometry to prune empty space in\nimplementation for its simplicity and efficiency.\nPSNR\u2191\nSSIM\u2191\nLPIPSV gg \u2193\nLPIPSAlex \u2193\n# Param.(M)\u2193\nOurs-12\n32.94\n0.961\n0.049\n0.028\n4.87\nOurs-24\n33.24\n0.963\n0.046\n0.026\n7.07\nOurs-48\n33.55\n0.965\n0.044\n0.025\n13.52\nOurs-96\n33.59\n0.965\n0.043\n0.024\n21.01\nTable 7: Ablation study on the number of tensor components. We use the same setting as our default model but only change\nthe number of components in each variant. These variants are evaluated on the NeRF Synthetic dataset [26].\nMVS\nSelf-Bootstrap\nRGBA\nRGBA (Early Stop)\n36.56\n36.54\n36.54\n36.51\nPSNR\nInit Method\nFigure 6: Our quality with initial geometry by different\nmethods.\nE. Speed v.s. Performance\nThough speed is not our focus, here, if we reduce the\nnumber of scales from 3 to 2 and TopK from 4 to 2 (i.e.,\nMulti(0.6, 0.3) with TopK=2), and Strivec becomes faster\nthan CP and close to VM, while still having competitive\nquality (see Ours-48(fast) in Tab.8). The fewer ranks of our\ntensor and the less number of TopK to be find for each sam-\nple point along a ray lead to less computation, and thus, ac-\nceleration. To conclude, Strivec is capable to improve qual-\nity, training time and compactness all together with proper\nhyper-parameters.\nTrain(s)\u2193 Inference(s/it)\u2193 #Params.(M)\u2193 PSNR\u2191\nTensoRF-CP\n1914\n2.01\n0.98\n31.56\nTensoRF-VM\n915\n1.60\n17.95\n33.14\nOurs-48(fast)\n959\n1.67\n6.20\n33.09\nTable 8: Comparison on NeRF Synthetic dataset [26]. We\ncompare the average training time (s), inference time (s/it),\nthe number of parameters (M) and PSNR.\nF. Per-scene Breakdown Results of the NeRF\nSynthetic Dataset\nWe show the per-scene detailed quantitative results of\nthe comparisons on the NeRF Synthetic dataset [26] in Tab.\n10 and qualitative comparisons in our video. With com-\npact model capacity, our method outperforms state-of-the-\nart methods [26, 27, 43, 7] and achieves the best PSNRs,\nand LPIPSs in most of the scenes. We report two versions\nof iNGP [27]. Specifically, iNGP-dark100k is reported in the\noriginal paper. According to issue #745 in iNGP\u2019s official\nrepo, the method uses a random color background in train-\ning and dark background in testing. The number of itera-\ntions, 100k, is referenced to its initial code base release. We\nalso refer to the results reported in [8] as iNGP-white30k,\nsince the authors use a white background in both training\nand testing for 30k iterations, which has the same setting as\ngarden\nroom\nModel Size(avg)\nDVGO\n24.32\n28.35\n5.1GB\nOurs-48\n24.13\n28.11\n12.6MB\nTable 9: Results on the Mip-NeRF 360 dataset.\nours and many other compared methods. Please refer to is-\nsue #745 and #1266 in iNGP\u2019s official repo for more details.\nG. The Tanks and Temples Dataset\nWe show the qualitative comparison between our Strivec\nand TensoRF-VM [7] on the Tanks and Temples dataset [20]\nin Fig.7. Similar to the procedures on the NeRF Synthetic\ndataset, we build the coarse scene geometry within 30 sec-\nonds to place our local tensors. The quantitative results are\nreported in Tab.11.\nH. Mip-NeRF360 Dataset\nWe evaluate our method on two scenes (one indoor scene\nand one outdoor scene) of Mip-NeRF360 dataset [4]. Note\nthat we only use the scene warping scheme the same as\nDVGO [36] and Mip-NeRF360 [4] and keeping other com-\nponents (i.e., positional encoding, point sampling, etc.) the\nsame as TensoRF [7]. The qualitative and quantitative re-\nsults are shown in Fig. 8 and Tab. 9 , respectively. Here,\nwe use only two scales in implementation to show our com-\npactness and scalability.\nNeRF Synthetic\nChair\nDrums\nLego\nMic\nMaterials\nShip\nHotdog\nFicus\nPSNR\u2191\nNeRF [26]\n33.00\n25.01\n32.54\n32.91\n29.62\n28.65\n36.18\n30.13\nNSVF [22]\n33.19\n25.18\n32.54\n34.27\n32.68\n27.93\n37.14\n31.23\nPoint-NeRF20k [43]\n32.50\n25.03\n32.40\n32.31\n28.11\n28.13\n34.53\n32.67\nPoint-NeRF200k [43]\n35.40\n26.06\n35.04\n35.95\n29.61\n30.97\n37.30\n36.13\niNGP-dark100k [27]\n35.00\n26.02\n36.39\n36.22\n29.78\n31.10\n37.40\n33.51\niNGP-white30k [27, 9]\n35.42\n24.24\n34.82\n35.98\n28.99\n30.72\n37.45\n32.09\nTensoRF-CP [7]-38430k\n33.60\n25.17\n34.05\n33.77\n30.10\n28.84\n36.24\n30.72\nTensoRF-VM [7]-19230k\n35.76\n26.01\n36.46\n34.61\n30.12\n30.77\n37.41\n33.99\nOurs-1230k\n35.21\n25.96\n35.60\n35.29\n29.54\n30.64\n37.03\n34.21\nOurs-2430k\n35.60\n26.16\n36.05\n35.81\n29.79\n30.89\n37.24\n34.37\nOurs-4830k\n35.88\n26.20\n36.52\n36.65\n29.90\n31.13\n37.63\n34.47\nSSIM\u2191\nNeRF\n0.967\n0.925\n0.961\n0.980\n0.949\n0.856\n0.974\n0.964\nNSVF\n0.968\n0.931\n0.960\n0.987\n0.973\n0.854\n0.980\n0.973\nPoint-NeRF20k\n0.981\n0.944\n0.980\n0.986\n0.959\n0.916\n0.983\n0.986\nPoint-NeRF200k\n0.991\n0.954\n0.988\n0.994\n0.971\n0.942\n0.991\n0.993\niNGP-white30k\n0.985\n0.924\n0.979\n0.990\n0.945\n0.892\n0.982\n0.977\nTensoRF-CP-38430k\n0.973\n0.921\n0.971\n0.983\n0.950\n0.857\n0.975\n0.965\nTensoRF-VM-19230k\n0.985\n0.937\n0.983\n0.988\n0.952\n0.895\n0.982\n0.982\nOurs-1230k\n0.983\n0.937\n0.980\n0.989\n0.948\n0.888\n0.981\n0.983\nOurs-2430k\n0.984\n0.940\n0.982\n0.990\n0.952\n0.893\n0.982\n0.984\nOurs-4830k\n0.985\n0.940\n0.984\n0.992\n0.953\n0.899\n0.983\n0.985\nLPIPSV gg \u2193\nNeRF\n0.046\n0.091\n0.050\n0.028\n0.063\n0.206\n0.121\n0.044\nPoint-NeRF20k\n0.051\n0.103\n0.054\n0.039\n0.102\n0.181\n0.074\n0.043\nPoint-NeRF200k\n0.023\n0.078\n0.024\n0.014\n0.072\n0.124\n0.037\n0.022\niNGP-white30k\n0.022\n0.092\n0.025\n0.017\n0.069\n0.137\n0.037\n0.026\nTensoRF-CP-38430k\n0.044\n0.114\n0.038\n0.035\n0.068\n0.196\n0.052\n0.058\nTensoRF-VM-19230k\n0.022\n0.073\n0.018\n0.015\n0.058\n0.138\n0.032\n0.022\nOurs-1230k\n0.025\n0.070\n0.022\n0.015\n0.062\n0.145\n0.033\n0.022\nOurs-2430k\n0.022\n0.067\n0.020\n0.013\n0.058\n0.141\n0.031\n0.021\nOurs-4830k\n0.021\n0.064\n0.017\n0.011\n0.056\n0.138\n0.029\n0.018\nLPIPSAlex \u2193\nNSVF\n0.043\n0.069\n0.029\n0.010\n0.021\n0.162\n0.025\n0.017\nPoint-NeRF20k\n0.027\n0.057\n0.022\n0.024\n0.076\n0.127\n0.044\n0.022\nPoint-NeRF200k\n0.010\n0.055\n0.011\n0.007\n0.041\n0.070\n0.016\n0.009\niNGP-white30k\n0.022\n0.093\n0.025\n0.017\n0.069\n0.140\n0.037\n0.026\nTensoRF-CP-38430k\n0.022\n0.069\n0.014\n0.018\n0.031\n0.130\n0.024\n0.024\nTensoRF-VM-19230k\n0.010\n0.051\n0.007\n0.009\n0.026\n0.085\n0.013\n0.012\nOurs-1230k\n0.011\n0.051\n0.009\n0.007\n0.027\n0.092\n0.015\n0.013\nOurs-2430k\n0.010\n0.049\n0.008\n0.006\n0.024\n0.087\n0.014\n0.012\nOurs-4830k\n0.009\n0.048\n0.007\n0.005\n0.023\n0.086\n0.012\n0.011\nTable 10: Detailed breakdown of quantitative metrics on individual scenes in the NeRF Synthetic [26] for our method and\nbaselines. All scores are averaged over the testing images. The subscripts are the number of iterations of the models. NeRF\nonly [26] reports the LPIPSV gg [47] while NSVF only reports LPIPSAlex.\nFigure 7: Qualitative comparison on the Tanks and Temples dataset. Top: ours. Bottom: TensoRF-VM.\nTanks & Tamples\nIgnatius\nTruck\nBarn\nCaterpillar\nFamily\nMean\nPSNR \u2191\nNV [23]\n26.54\n21.71\n20.82\n20.71\n28.72\n23.70\nNeRF [26]\n25.43\n25.36\n24.05\n23.75\n30.29\n25.78\nNSVF [22]\n27.91\n26.92\n27.16\n26.44\n33.58\n28.40\nTensoRF-CP[7]\n27.86\n26.25\n26.74\n24.73\n32.39\n27.59\nTensoRF-VM[7]\n28.34\n27.14\n27.22\n26.19\n33.92\n28.56\nOurs-48\n28.39\n27.32\n28.09\n26.58\n33.13\n28.70\nSSIM \u2191\nNV [23]\n0.992\n0.793\n0.721\n0.819\n0.916\n0.848\nNeRF [26]\n0.920\n0.860\n0.750\n0.860\n0.932\n0.864\nNSVF [22]\n0.930\n0.895\n0.823\n0.900\n0.954\n0.900\nTensoRF-CP[7]\n0.934\n0.885\n0.839\n0.879\n0.948\n0.897\nTensoRF-VM[7]\n0.948\n0.914\n0.864\n0.912\n0.965\n0.920\nOurs-48\n0.948\n0.915\n0.884\n0.917\n0.957\n0.924\nLPIPSAlex \u2193\nNV [23]\n0.117\n0.312\n0.479\n0.280\n0.111\n0.260\nNeRF [26]\n0.111\n0.192\n0.395\n0.196\n0.098\n0.198\nNSVF [22]\n0.106\n0.148\n0.307\n0.141\n0.063\n0.153\nTensoRF-CP[7]\n0.089\n0.154\n0.237\n0.176\n0.063\n0.144\nTensoRF-VM[7]\n0.081\n0.129\n0.217\n0.139\n0.057\n0.125\nOurs-48\n0.083\n0.123\n0.167\n0.125\n0.065\n0.113\nLPIPSV gg \u2193\nTensoRF-CP[7]\n0.106\n0.202\n0.283\n0.227\n0.088\n0.181\nTensoRF-VM[7]\n0.078\n0.145\n0.252\n0.159\n0.064\n0.140\nOurs-48\n0.083\n0.150\n0.216\n0.154\n0.078\n0.136\nTable 11: Quantity comparison on five scenes in the Tanks and Temples dataset [20] selected in NSVF [22]. NV, NeRF, and\nNSVF have not reported their LPIPSV gg\nFigure 8: Qualitative results on Mip-NeRF360 dataset.\nFigure 9: Visualization of local tensors (single scale) on initial geometry.\n"
  },
  {
    "title": "Contrastive Example-Based Control",
    "link": "https://arxiv.org/pdf/2307.13101.pdf",
    "upvote": "4",
    "text": "Proceedings of Machine Learning Research vol 211:1\u201318, 2023\n5th Annual Conference on Learning for Dynamics and Control\nContrastive Example-Based Control\nKyle Hatch 1\nKHATCH@STANFORD.EDU\nBenjamin Eysenbach 2\nBEYSENBA@CS.CMU.EDU\nRafael Rafailov 1\nRAFAILOV@STANFORD.EDU\nTianhe Yu 1\nTIANHEYU@CS.STANFORD.EDU\nRuslan Salakhutdinov 2\nRSALAKHU@CS.CMU.EDU\nSergey Levine 3\nSVLEVINE@EECS.BERKELEY.EDU\nChelsea Finn 1\nCBFINN@CS.STANFORD.EDU\n1Department of Computer Science, Stanford University\n2Machine Learning Department, Carnegie Mellon University\n3Department of Electrical Engineering and Computer Sciences, UC Berkeley\nEditors: N. Matni, M. Morari, G. J. Pappas\nAbstract\nWhile many real-world problems that might benefit from reinforcement learning, these prob-\nlems rarely fit into the MDP mold: interacting with the environment is often expensive and spec-\nifying reward functions is challenging. Motivated by these challenges, prior work has developed\ndata-driven approaches that learn entirely from samples from the transition dynamics and examples\nof high-return states. These methods typically learn a reward function from high-return states, use\nthat reward function to label the transitions, and then apply an offline RL algorithm to these tran-\nsitions. While these methods can achieve good results on many tasks, they can be complex, often\nrequiring regularization and temporal difference updates. In this paper, we propose a method for\noffline, example-based control that learns an implicit model of multi-step transitions, rather than\na reward function. We show that this implicit model can represent the Q-values for the example-\nbased control problem. Across a range of state-based and image-based offline control tasks, our\nmethod outperforms baselines that use learned reward functions; additional experiments demon-\nstrate improved robustness and scaling with dataset size.1\nKeywords: reinforcement learning, offline RL, robot learning, reward learning, contrastive learn-\ning, model-based reinforcement learning, example-based control, reward-free learning\n1. Introduction\nReinforcement learning is typically framed as the problem of maximizing a given reward function.\nHowever, in many real-world situations, it is more natural for users to define what they want an\nagent to do with examples of successful outcomes (Fu et al., 2018b; Zolna et al., 2020a; Xu and\nDenil, 2019; Eysenbach et al., 2021). For example, a user that wants their robot to pack laundry into\na washing machine might provide multiple examples of states where the laundry has been packed\ncorrectly. This problem setting is often seen as a variant of inverse reinforcement learning (Fu et al.,\n2018b), where the aim is to learn only from examples of successful outcomes, rather than from\n1. Videos of our method are available on the project website: https://sites.google.com/view/laeo-rl.\nCode is released at: https://github.com/khatch31/laeo.\n\u00a9 2023 K. Hatch, B. Eysenbach, R. Rafailov, T. Yu, R. Salakhutdinov, S. Levine & C. Finn.\narXiv:2307.13101v1  [cs.LG]  24 Jul 2023\nCONTRASTIVE EXAMPLE-BASED CONTROL\nexpert demonstrations. To solve this problem, the agent must both figure out what constitutes task\nsuccess (i.e., what the examples have in common) and how to achieve such successful outcomes.\nIn this paper, our aim is to address this problem setting in the case where the agent must learn\nfrom offline data without trial and error. Instead, the agent must infer the outcomes of potential\nactions from the provided data, while also relating these inferred outcomes to the success examples.\nWe will refer to this problem of offline RL with success examples as offline example-based control.\nMost prior approaches involve two steps: first learning a reward function, and second combining\nit with an RL method to recover a policy (Fu et al., 2018b; Zolna et al., 2020a; Xu and Denil, 2019).\nWhile such approaches can achieve excellent results when provided sufficient data (Kalashnikov\net al., 2021; Zolna et al., 2020a), learning the reward function is challenging when the number of\nsuccess examples is small (Li et al., 2021; Zolna et al., 2020a). Moreover, these prior approaches\nare relatively complex (e.g., they use temporal difference learning) and have many hyperparameters.\nOur aim is to provide a simple and scalable approach that avoids the challenges of reward\nlearning. The main idea will be learning a certain type of dynamics model. Then, using that model\nto predict the probabilities of reaching each of the success examples, we will be able to estimate the\nQ-values for every state and action. Note that this approach does not use an offline RL algorithm as\na subroutine. The key design decision is the model type; we will use an implicit model of the time-\naveraged future (precisely, the discounted state occupancy measure). This decision means that our\nmodel reasons across multiple time steps but will not output high-dimensional observations (only\na scalar number). A limitation of this approach is that it will correspond to a single step of policy\nimprovement: the dynamics model corresponds to the dynamics of the behavioral policy, not of the\nreward-maximizing policy. While this means that our method is not guaranteed to yield the optimal\npolicy, our experiments nevertheless show that our approach outperforms multi-step RL methods.\nThe main contribution of this paper is an offline RL method (LAEO) that learns a policy from\nexamples of high-reward states. The key idea behind LAEO is an implicit dynamics model, which\nrepresents the probability of reaching states at some point in the future. We use this model to esti-\nmate the probability of reaching examples of high-return states. LAEO is simpler yet more effective\nthan prior approaches based on reward classifiers. Our experiments demonstrate that LAEO can\nsuccessfully solve offline RL problems from examples of high-return states on four state-based and\ntwo image-based manipulation tasks. Our experiments show that LAEO is more robust to occlu-\nsions and also exhibits better scaling with dataset size than prior methods. We show that LAEO can\nwork in example-based control settings in which goal-conditioned RL methods fail. Additionally,\nwe show that the dynamics model learned by LAEO can generalize to multiple different tasks, being\nused to solve tasks that are not explicitly represented in the training data.\n2. Related Work\nReward learning.\nTo overcome the challenge of hand-engineering reward functions for RL, prior\nmethods either use supervised learning or adversarial training to learn a policy that matches the ex-\npert behavior given by the demonstration (imitation learning) (Pomerleau, 1988; Ross et al., 2011;\nHo and Ermon, 2016; Spencer et al., 2021) or learn a reward function from demonstrations and opti-\nmize the policy with the learned reward through trial and error (inverse RL) (Ng and Russell, 2000;\nAbbeel and Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Fu et al., 2018a).\nHowever, providing full demonstrations complete with agent actions is often difficult, therefore, re-\ncent works have focused on the setting where only a set of user-specified goal states or human videos\n2\nCONTRASTIVE EXAMPLE-BASED CONTROL\nare available (Fu et al., 2018b; Singh et al., 2019; Kalashnikov et al., 2021; Xie et al., 2018; Eysen-\nbach et al., 2021; Chen et al., 2021). These reward learning approaches have shown successes in\nreal-world robotic manipulation tasks from high-dimensional imageinputs (Finn et al., 2016; Singh\net al., 2019; Zhu et al., 2020; Chen et al., 2021). Nevertheless, to combat covariate shift that could\nlead the policy to drift away from the expert distribution, these methods usually require significant\nonline interaction. Unlike these works that study online settings, we consider learning visuomotor\nskills from offline datasets.\nOffline RL.\nOffline RL (Ernst et al., 2005; Riedmiller, 2005; Lange et al., 2012; Levine et al.,\n2020) studies the problem of learning a policy from a static dataset without online data collection\nin the environment, which has shown promising results in robotic manipulation (Kalashnikov et al.,\n2018; Mandlekar et al., 2020; Rafailov et al., 2021; Singh et al., 2020; Julian et al., 2020; Kalash-\nnikov et al., 2021). Prior offline RL methods focus on the challenge of distribution shift between the\noffline training data and deployment using a variety of techniques, such as policy constraints (Fu-\njimoto et al., 2018; Liu et al., 2020; Jaques et al., 2019; Wu et al., 2019; Zhou et al., 2020; Kumar\net al., 2019; Siegel et al., 2020; Peng et al., 2019; Fujimoto and Gu, 2021; Ghasemipour et al.,\n2021), conservative Q-functions (Kumar et al., 2020; Kostrikov et al., 2021; Yu et al., 2021; Sinha\nand Garg, 2021), and penalizing out-of-distribution states generated by learned dynamics mod-\nels (Kidambi et al., 2020; Yu et al., 2020b; Matsushima et al., 2020; Argenson and Dulac-Arnold,\n2020; Swazinna et al., 2020; Rafailov et al., 2021; Lee et al., 2021; Yu et al., 2021).\nWhile these prior works successfully address the issue of distribution shift, they still require\nreward annotations for the offline data. Practical approaches have used manual reward sketching to\ntrain a reward model (Cabi et al., 2019; Konyushkova et al., 2020; Rafailov et al., 2021) or heuristic\nreward functions (Yu et al., 2022). Others have considered offline learning from demonstrations,\nwithout access to a predefined reward function (Mandlekar et al., 2020; Zolna et al., 2020a; Xu\net al., 2022; Jarboui and Perchet, 2021), however they rely on high-quality demonstration data. In\ncontrast, our method: (1) addresses distributional shift induced by both the learned policy and the\nreward function in a principled way, (2) only requires user-provided goal states and (3) does not\nrequire expert-quality data, resulting in an effective and practical offline reward learning scheme.\n3. Learning to Achieve Examples Offline\nOffline RL methods typically require regularization, and our method will employ regularization in\ntwo ways. First, we regularize the policy with an additional behavioral cloning term, which penal-\nizes the policy for sampling out-of-distribution actions. Second, our method uses the Q-function for\nthe behavioral policy, so it performs one (not many) step of policy improvement. These regularizers\nmean that our approach is not guaranteed to yield the optimal policy.\n3.1. Preliminaries\nWe assume that an agent interacts with an MDP with states s \u2208 S, actions a, a state-only reward\nfunction r(s) \u2265 0, initial state distribution p0(s0) and dynamics p(st+1 | st, at). We use \u03c4 =\n(s0, a0, s1, a1, \u00b7 \u00b7 \u00b7 ) to denote an infinite-length trajectory. The likelihood of a trajectory under a\npolicy \u03c0(a | s) is \u03c0(\u03c4) = p0(s0) Q\u221e\nt=0 p(st+1 | st, at)\u03c0(at | st). The objective is to learn a policy\n\u03c0(a | s) that maximizes the expected, \u03b3-discounted sum of rewards: max\u03c0 E\u03c0(\u03c4)\n\u0002P\u221e\nt=0 \u03b3tr(st)\n\u0003\n.\nWe define the Q-function for policy \u03c0 as the expected discounted sum of returns, conditioned on an\n3\nCONTRASTIVE EXAMPLE-BASED CONTROL\ninitial state and action:\nQ\u03c0(s, a) \u225c E\u03c0(\u03c4)\n\" \u221e\nX\nt=0\n\u03b3tr(st)\n\f\f\f\f\ns0=s\na0=a\n#\n.\n(1)\nWe will focus on the offline (i.e., batch RL) setting. Instead of learning by interacting with the\nenvironment (i.e., via trial and error), the RL agent will receive as input a dataset of trajectories\nD\u03c4 = {\u03c4 \u223c \u03b2(\u03c4)} collected by a behavioral policy \u03b2(a | s). We will use Q\u03b2(s, a) to denote the\nQ-function of the behavioral policy.\nSpecifying the reward function.\nIn many real-world applications, specifying and measuring a\nscalar reward function is challenging, but providing examples of good states (i.e., those which would\nreceive high rewards) is straightforward. Thus, we follow prior work (Fu et al., 2018b; Zolna et al.,\n2020a; Eysenbach et al., 2021; Xu and Denil, 2019; Zolna et al., 2020b) in assuming that the agent\ndoes not observe scalar rewards (i.e., D\u03c4 does not contain reward information). Instead, the agent\nreceives as input a dataset D\u2217 = {s\u2217} of high-reward states s\u2217 \u2208 S. These high-reward states are\nexamples of good outcomes, which the agent would like to achieve. The high-reward states are not\nlabeled with their specific reward value.\nTo make the control problem well defined, we must relate these success examples to the reward\nfunction. We do this by assuming that the frequency of each success example is proportional to its\nreward: good states are more likely to appear (and be duplicated) as success examples.\nAssumption 1\nLet p\u03c4(s) be the empirical probability density of state s in the trajectory dataset,\nand let p\u2217(s) as the empirical probability density of state s under the high-reward state dataset. We\nassume that there exists a positive constant c such that r(s) = c p\u2217(s)\np\u03c4(s) for all states s.\nThis is the same assumption as Eysenbach et al. (2021). This assumption is important because it\nshows how example-based control is universal: for any reward function, we can specify the corre-\nsponding example-based problem by constructing a dataset of success examples that are sampled\naccording to their rewards. We assumed that rewards are non-negative so that these sampling prob-\nabilities are positive.\nThis assumption can also be read in reverse. When a user constructs a dataset of success ex-\namples in an arbitrary fashion, they are implicitly defining a reward function. In the tabular setting,\nthe (implicit) reward function for state s is the count of the times s occurs in the dataset of success\nexamples. Compared with goal-conditioned RL (Kaelbling, 1993), defining tasks via success exam-\nples is more general. By identifying what all the success examples have in common (e.g., laundry\nis folded), the RL agent can learn what is necessary to solve the task and what is irrelevant (e.g., the\ncolor of the clothes in the laundry). We now can define our problem statement as follows:\nDefinition 1 In the offline example-based control problem, a learning algorithm receives as input\na dataset of trajectories D\u03c4 = {\u03c4} and a dataset of successful outcomes D\u2217 = {s} satisfying\nAssumption 1. The aim is to output a policy that maximizes the RL objective (Eq. 3.1).\nThis problem setting is appealing because it mirrors many practical RL applications: a user has\naccess to historical data from past experience, but collecting new experience is prohibitively expen-\nsive. Moreover, this problem setting can mitigate the challenges of reward function design. Rather\nthan having to implement a reward function and add instruments to measure the corresponding com-\nponents, the users need only provide a handful of observations that solved the task. This problem\n4\nCONTRASTIVE EXAMPLE-BASED CONTROL\nsetting is similar to imitation learning, in the sense that the only inputs are data. However, unlike\nimitation learning, in this problem setting the high-reward states are not labeled with actions, and\nthese high-reward states may not necessarily contain entire trajectories.\nFigure 1: Our method will use con-\ntrastive learning to predict which states\nmight occur at some point in the future.\nOur method will estimate the discounted state occupancy\nmeasure,\np\u03b2(st+ = s | s0, a0) \u225c (1 \u2212 \u03b3)\n\u221e\nX\nt=0\n\u03b3tp\u03c0\nt (st = s | s0, a0),\n(2)\nwhere p\u03b2\nt (st | s, a) is the probability of policy \u03b2(a | s) visiting\nstate st after exactly t time steps. Unlike the transition function\np(st+1 | st, at), the discounted state occupancy measure indi-\ncates the probability of visiting a state at any point in the future,\nnot just at the immediate next time step. In tabular settings, this\ndistribution corresponds to the successor representations (Dayan,\n1993). To handle continuous settings, we will use the contrastive\napproach from recent work (Mazoure et al., 2020; Eysenbach\net al., 2022). We will learn a function f(s, a, sf) \u2208 R takes as input an initial state-action pair\nas well as a candidate future state, and outputs a score estimating the likelihood that sf is a real\nfuture state. The loss function is a standard contrastive learning loss(e.g., Ma and Collins (2018)),\nwhere positive examples are triplets of a state, action, and future state:\nmax\nf\nL(f; D\u03c4) \u225c Ep(s,a),sf\u223cp\u03b2(st+|s,a) [log \u03c3(f(s, a, sf))]+Ep(s,a),sf\u223cp(s) [log(1 \u2212 \u03c3(f(s, a, sf)))] ,\nwhere \u03c3(\u00b7) is the sigmoid function. At optimality, the implicit dynamics model encodes the dis-\ncounted state occupancy measure:\nf\u2217(s, a, sf) = log p\u03b2(st+ = sf | s, a) \u2212 log p\u03c4(sf).\n(3)\nWe visualize this implicit dynamics model in Fig. 1. Note that this dynamics model is policy\ndependent. Because it is trained with data collected from one policy (\u03b2(a | s)), it will correspond\nto the probability that that policy visits states in the future. Because of this, our method will result\nin estimating the value function for the behavioral policy (akin to 1-step RL (Brandfonbrener et al.,\n2021)), and will not perform multiple steps of policy improvement. Intuitively, the training of\nthis implicit model resembles hindsight relabeling (Kaelbling, 1993; Andrychowicz et al., 2017).\nHowever, it is generally unclear how to use hindsight relabeling for single-task problems. Despite\nbeing a single-task method, our method will be able to make use of hindsight relabeling to train the\ndynamics model.\n3.2. Deriving Our Method\nThe key idea behind out method is that this implicit dynamics model can be used to represent the\nQ-values for the example-based problem, up to a constant. The proof is in Appendix A.\nLemma 2 Assume that the implicit dynamics model is learned without errors. Then the Q-function\nfor the data collection policy \u03b2(a | s) can be expressed in terms of this implicit dynamics model:\nQ\u03b2(s, a) =\nc\n1 \u2212 \u03b3 Ep\u2217(s\u2217)\nh\nef(s,a,s\u2217)i\n.\n(4)\n5\nCONTRASTIVE EXAMPLE-BASED CONTROL\nSo, after learning the implicit dynamics model, we can estimate the Q-values by averaging\nthis model\u2019s predictions across the success examples. We will update the policy using Q-values\nestimated in this manner, plus a regularization term:\nmin\n\u03c0 L(\u03c0; f, D\u2217) \u225c \u2212(1 \u2212 \u03bb)E\u03c0(a|s)p(s),s\u2217\u223cD\u2217\nh\nef(s,a,s\u2217)i\n\u2212 \u03bbEs,a\u223cD\u03c4 [log \u03c0(a | s)] .\n(5)\nIn our experiments, we use a weak regularization coefficient of \u03bb = 0.5.\nFigure 2: If the state-action representation \u03d5(s, a) is close\nto the representation of a high-return state \u03c8(s), then the\npolicy is likely to visit that state.\nOur method estimates\nQ-values by combining the distances to all the high-return\nstates (Eq. 1).\nIt is worth comparing this approach to prior\nmethods based on learned reward functions (Xu\nand Denil, 2019; Fu et al., 2018b; Zolna et al.,\n2020a). Those methods learn a reward func-\ntion from the success examples, and use that\nlearned reward function to synthetically label\nthe dataset of trajectories. Both approaches can\nbe interpreted as learning a function on one of\nthe datasets and then applying that function to\nthe other dataset. Because it is easier to fit a\nfunction when given large quantities of data,\nwe predict that our approach will outperform\nthe learned reward function approach when the\nnumber of success examples is small, relative\nto the number of unlabeled trajectories. Other\nprior methods (Eysenbach et al., 2021; Reddy\net al., 2020) avoid learning reward functions by\nproposing TD update rules that are applied to both the unlabeled transitions and the high-return\nstates. However, because these methods have yet to be adapted to the offline RL setting, we will\nfocus our comparisons on the reward-learning methods.\n3.3. A Geometric Perspective\nBefore presenting the complete RL algorithm, we provide a geometric perspective on the repre-\nsentations learned by our method. Our implicit models learns a representation of state-action pairs\n\u03d5(s, a) as well as a representation of future states \u03c8(s). One way that our method can optimize these\nrepresentations is by treating \u03d5(s, a) as a prediction for the future representations.2 Each of the high-\nreturn states can be mapped to the same representation space. To determine whether a state-action\npair has a large or small Q-value, we can simply see whether the predicted representation \u03d5(s, a)\nis close to the representations of any of the success examples. Our method learns these representa-\ntions so that the Q-values are directly related to the Euclidean distances3 from each success example.\nThus, our method can be interpreted as learning a representation space such that estimating Q-values\ncorresponds to simple geometric operations (kernel smoothing with an RBF kernel (Hastie et al.,\n2009, Chpt. 6)) on the learned representations. While the example-based control problem is more\ngeneral than goal-conditioned RL (see Sec. 3.1), we can recover goal-conditioned RL as a special\ncase by using a single success example.\n2. Our method can also learn the opposite, where \u03c8(s) is a prediction for the previous representations.\n3. When representations are normalized, the dot product is equivalent to the Euclidean norm. We find that unnormalized\nfeatures work better in our experiments.\n6\nCONTRASTIVE EXAMPLE-BASED CONTROL\n3.4. A Complete Algorithm\nWe now build a complete offline RL algorithm based on these Q-functions. We will call our method\nLEARNING TO ACHIEVE EXAMPLES OFFLINE (LAEO). Our algorithm will resemble one-step RL\nmethods, but differ in how the Q-function is trained. After learning the implicit dynamics model\n(and, hence, Q-function) we will optimize the policy. The objective for the policy is maximizing\n(log) Q-values plus a regularization term, which penalizes sampling unseen actions:4\nmax\n\u03c0\n(1 \u2212 \u03bb) log E\u03c0(a|s)p\u03c4(s) [Q(s, a)] + \u03bbE(s,a)\u223cp\u03c4(s,a) [log \u03c0(a | s)]\n= (1 \u2212 \u03bb) log E\u03c0(a|s),s\u2217\u223cp\u2217(s)\nh\nef(s,a,s\u2217)i\n+ \u03bbE(s,a)\u223cp\u03c4(s,a) [log \u03c0(a | s)] .\n(6)\nAlgorithm 1 Learning to Achieve Examples Offline\n1: Inputs:\ndataset of trajectories D\n= {\u03c4},\ndataset of high-return states D\u2217 = {s}.\n2: Learn the model via contrastive learning:\nf \u2190\narg minf L(f; D\u03c4)\n\u25b7 Eq. 5\n3: Learn the policy: \u03c0 \u2190 arg min\u03c0 L(\u03c0; f, D\u2217)\n\u25b7 Eq. 6\n4: return policy \u03c0(a | s)\nAs noted above, this is a one-step RL\nmethod: it updates the policy to maximize the\nQ-values of the behavioral policy. Performing\njust a single step of policy improvement can be\nviewed as a form of regularization in RL, in the\nsame spirit as early stopping is a form of reg-\nularization in supervised learning. Prior work\nhas found that one-step RL methods can per-\nform well in the offline RL setting. Because our method performs only a single step of policy\nimprovement, we are not guaranteed that it will converge to the reward-maximizing policy. We\nsummarize the complete algorithm in Alg. 1.\n4. Experiments\nFigure 3:\nBenchmark tasks:\nWe evaluate the performance of LAEO\non six simulated manipulation tasks,\ntwo of which use pixel observations\n(FetchReach-image and FetchPush-image) and four of which use low-\ndimensional states (FetchReach, FetchPush, SawyerWindowOpen, and\nSawyerDrawerClose ).\nOur experiments test whether\nLAEO can effectively solve\noffline RL tasks that are\nspecified by examples of\nhigh-return states,\nrather\nthan via scalar reward func-\ntions.\nWe study when\nour approach outperforms\nprior approaches based on\nlearned reward functions.\nWe look not only at the per-\nformance relative to baselines on state-based and image-based tasks, but also how that performance\ndepends on the size and composition of the input datasets. Additional experiments study how LAEO\nperforms when provided with varying numbers of success observations and whether our method\ncan solve partially observed tasks. We include full hyperparameters and implementation details in\nAppendix B. Code is available at https://github.com/khatch31/laeo. Videos of our\nmethod are available at https://sites.google.com/view/laeo-rl.\n4. For all experiments except Fig. 8, we apply Jensen\u2019s inequality to the first term, using E\u03c0(a|s),s\u2217\u223cp\u2217(s)[f(s, a, s\u2217)].\n7\nCONTRASTIVE EXAMPLE-BASED CONTROL\nFigure 4: Benchmark comparison: LAEO matches or outperforms prior example-based offline RL methods on state\nand image-based tasks, including those that learn a separate reward function (ORIL, PURL).The gap in performance is\nmost significant on the FetchPush and FetchPush-image tasks, which involve more complicated dynamics than\nthe other tasks, suggesting that LAEO may outperform model free reward-learning approaches on tasks with complicated\ndynamics. LAEO also outperforms BC on all of the tasks, highlighting LAEO\u2019s ability to learn a policy that outperforms\nthe behavior policy on non-demonstration datasets.\nBaselines.\nOur main point of comparison will be prior methods that use learned reward functions:\nORIL (Zolna et al., 2020a) and PURL (Xu and Denil, 2019). The main difference between these\nmethods is the loss function used to train reward function: ORIL uses binary cross entropy loss while\nPURL uses a positive-unlabeled loss (Xu and Denil, 2019). Note that the ORIL paper also reports\nresults using a positive-unlabeled loss, but for the sake of clarity we simply refer to it as PURL. After\nlearning the reward function, each of these methods applies an off-the-shelf RL algorithm. We will\nimplement all baselines using the TD3+BC (Fujimoto and Gu, 2021) offline RL algorithm. These\noffline RL methods achieve good performance on tasks specified via reward functions (Kostrikov\net al., 2021; Brandfonbrener et al., 2021; Fujimoto and Gu, 2021). We also include Behavioral\nCloning (BC) results.\nBenchmark comparison.\nWe start by comparing the performance of LAEO to these baselines on\nsix manipulation tasks. FetchReach and FetchPush are two manipulation tasks from Plappert\net al. (2018) that use state-based observations. FetchReach-image and FetchPush-image\nare the same tasks but with image-based observations. SawyerWindowOpen and Sawyer-\nDrawerClose are two manipulation tasks from Yu et al. (2020a). For each of these tasks, we\ncollect a dataset of medium quality by training an online agent from Eysenbach et al. (2022) and\nrolling out multiple checkpoints during the course of training. The resulting datasets have success\nrates between 45% \u2212 50%. We report results after 500, 000 training gradient steps (or 250, 000\nsteps, if the task success rates have converged by that point).\nWe report results in Fig. 4. We observe that LAEO, PURL, and ORIL perform similarly on\nFetchReach and FetchReach-image. This is likely because these are relatively easy tasks,\nand each of these methods is able to achieve a high success rate. Note that all of these methods\nsignificantly outperform BC, indicating that they are able to learn better policies than the mode\nbehavior policies represented in the datasets. On SawyerDrawerClose, all methods, including\nBC, achieve near perfect success rates, likely due to the simplicity of this task. On FetchPush,\nFetchPush-image, and SawyerWindowOpen, LAEO outperforms all of the baselines by a\n8\nCONTRASTIVE EXAMPLE-BASED CONTROL\nsignificant margin. Recall that the main difference between LAEO and PURL/ORIL is by learn-\ning a dynamics model, rather than the reward function. These experiments suggest that for tasks\nwith more complex dynamics, learning a dynamics model can achieve better performance than is\nachieved by model-free reward classifier methods.\nFigure 5: Data quality.\nLAEO continues to match or\noutperform reward classifier based methods on datasets that\ncontain a low percentage of successful trajectories.\nVarying the input data.\nOur next experi-\nment studies how the dataset composition af-\nfects LAEO and the baselines.\nOn each of\nthree tasks, we generate a low-quality dataset\nby rolling out multiple checkpoints from a\npartially trained agent from Eysenbach et al.\n(2022). In comparison to the medium-quality\ndatasets collected earlier, which have success\nrates between 45% \u2212 50%, these low quality\ndatasets have success rates between 8% \u2212 12%.\nWe will denote these low quality datasets with the \u201cHard\u201d suffix. Fig. 5 shows that LAEO continues\nto outperform baselines on these lower-quality datasets.\nOur next experiments study how varying the number of high-return example states and the num-\nber of reward-free trajectories affects performance. As noted in the Sec. 1, we conjecture that our\nmethod will be especially beneficial relative to reward-learning approaches in settings with very\nfew high-return example states. In Fig. 6 (left), we vary the number of high-return example states\non FetchPush -image, holding the number of unlabeled trajectories constant. We observe that\nLAEO maintains achieves the same performance with 1 success example as with 200 success ex-\namples. In contrast, ORIL\u2019s performance decreases as the number of high-return example states\ndecreases. In Fig. 6 (right), we vary the number of unlabeled trajectories, holding the number of\nhigh-return example states constant at 200. We test the performance of LAEO vs. ORIL on three\ndifferent dataset sizes on FetchPush-image, roughly corresponding to three different orders\nof magnitude: the 0.1\u00d7 dataset contains 3, 966 trajectories, the 1\u00d7 dataset contains 31, 271 tra-\njectories, and the 10\u00d7 dataset contains 300, 578 trajectories. We observe that LAEO continues to\nsee performance gains as number of unlabeled trajectories increases, whereas ORIL\u2019s performance\nplateaus. Taken together these results suggest that, in comparison to reward classifier based meth-\nods, LAEO needs less human supervision and is more effective at leveraging large quantities of\nunlabeled data.\nFigure 6: Effect of dataset size: (Left) The most competitive baseline (ORIL) achieves better performance when\ngiven more examples of high-return states, likely because it makes it easier to learn ORIL\u2019s reward classifier. LAEO,\nwhich does not require learning a reward classifier, consistently achieves high success rates. (Right) LAEO continues to\nimprove when trained with more reward-free trajectories, while ORIL\u2019s performance plateaus.\n9\nCONTRASTIVE EXAMPLE-BASED CONTROL\nFigure 7:\nPartial observability.\nLAEO continues to\nsolve the FetchPush-image manipulation task in a set-\nting where the new camera placement causes partial observ-\nability. This camera angle causes the block to be hidden from\nview by the gripper when the gripper reaches down to push\nthe block.\nPartial Observability.\nWe also test the per-\nformance of LAEO on a partially-observed\ntask.\nWe modify the camera position in the\nFetchPush-image so that the block is oc-\ncluded whenever the end effector is moved to\ntouch the block.\nWhile such partial observ-\nability can stymie temporal difference meth-\nods (Whitehead and Ballard, 1991), we pre-\ndict that LAEO might continue to solve this\ntask because it does not rely on temporal dif-\nference learning. The results, shown in Fig. 7,\nconfirm this prediction. On this partially ob-\nservable task, we compare the performance of\nLAEO with that of ORIL, the best performing baseline on the fully observable tasks. On the par-\ntially observable task, LAEO achieves a success rate of 51.9%, versus 33.9% for ORIL.\nFigure 8: Comparison with goal-conditioned RL. LAEO\nsolves manipulation tasks at multiple different locations\nwithout being provided with a goal-state at test time.\nComparison to Goal-Conditioned RL.\nOne\nof the key advantages of example-based con-\ntrol, relative to goal-conditioned RL, is that the\npolicy can identify common patterns in the suc-\ncess examples to solve tasks in scenarios where\nit has never before seen a success example. In\nsettings such as robotics, this can be an issue\nsince acquiring a goal state to provide to the\nagent requires already solving the desired task\nin the first place.\nWe test this capability in\na variant of the SawyerDrawerClose envi-\nronment. For training, the drawer\u2019s X position\nis chosen as one of five fixed locations. Then, we evaluate the policy learned by LAEO on three\ntypes of environments: In Distribution: the drawer\u2019s X position is one of the five locations from\ntraining; Interpolation: The drawer\u2019s X position is between some of the locations seen during train-\ning; Extrapolation: The drawer\u2019s X position is outside the range of X positions seen during training.\nWe compare to a goal-conditioned policy learned via contrastive RL, where actions are extracted by\naveraging over the (training) success examples: \u03c0(a | s) = Es\u2217\u223cp\u2217(s)[\u03c0(a | s, g = s\u2217)].\nThe results, shown in Fig. 8, show that LAEO consistently outperforms this goal-conditioned\nbaseline. As expected, the performance is highest for the In Distribution environments and lowest\nfor the Extrapolation environments. Taken together, these experiments show that LAEO can learn\nto reach multiple different goal locations without access to goal states during test time.\nMultitask Critic.\nWe explore whether a LAEO dynamics network trained on data from one\ntask can be used to solve other downstream tasks. We create a simple multitask environment by\ndefining several different tasks that can be solved in the SawyerDrawerClose environment:\nClose, Half-closed, Open, Reach-near, Reach-medium, and Reach-far. We then\nuse a trained critic network from the previous set of experiments (Comparison to Goal-Conditioned\nRL), condition it on a success example from a downstream task, and select actions by using cross\nentropy method (CEM) optimization. By using CEM optimization, we do not need to train a sepa-\n10\nCONTRASTIVE EXAMPLE-BASED CONTROL\nrate policy network for each of the tasks. See Appendix C for implementation details and for details\nof the multitask drawer environment.\nFigure 9: Multitask Critic: Cross entropy method (CEM)\noptimization over the LAEO dynamics model trained only\non the data from the drawer close task is able to solve six\ndifferent tasks. Randomly sampling actions from the action\nspace results in a 0% success rate across all of the six tasks\n(not shown for clarity).\nCEM over the LAEO critic achieves non-\nzero success rates on all six tasks, despite only\nbeing trained on data from the Close task (see\nFigure 9). In contrast, randomly sampling ac-\ntions from the action space achieves a 0% suc-\ncess rate on all of the tasks. Results are aver-\naged across eight random seeds. This suggests\nthat a single LAEO critic can be leveraged to\nsolve multiple downstream tasks, as long as the\ndynamics required to solve those tasks are rep-\nresented in the training data. Note that since\nwe condition the critic network on a single goal\nexample, these experiments can be interpreted\nfrom a goal-conditioned perspective as well as\nan example-based control perspective. In fu-\nture work, we aim to explore the multitask ca-\npabilities of the LAEO dynamics model in an\nexample-based control setting at a larger scale.\nThis will involve training on larger, more diverse datasets as well as conditioning the critic network\non multiple success examples for a single task (as done in the Comparison to Goal-Conditioned RL\nexperiments).\n5. Conclusion\nIn this paper, we present an RL algorithm aimed at settings where data collection and reward specifi-\ncation are difficult. Our method learns from a combination of high-return states and reward-free tra-\njectories, integrating these two types of information to learn reward-maximizing policies. Whereas\nprior methods perform this integration by learning a reward function and then applying an off-the-\nshelf RL algorithm, ours learns an implicit dynamics model. Not only is our method simpler (no\nadditional RL algorithm required!), but also it achieves higher success rates than prior methods.\nWhile our experiments only start to study the ability of contrastive-based methods to scale\nto high-dimensional observations, we conjecture that methods like LAEO may be particularly amenable\nto such problems because the method for learning the representations (contrastive learning) resem-\nbles prior representation learning methods (Mazoure et al., 2020; Nair et al., 2022). Scaling this\nmethod to very large offline datasets is an important direction for future work.\n6. Acknowledgments\nBE is supported by the Fannie and John Hertz Foundation and the NSF GRFP (DGE2140739).\nReferences\nPieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In\nProceedings of the twenty-first international conference on Machine learning, page 1, 2004.\n11\nCONTRASTIVE EXAMPLE-BASED CONTROL\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. arXiv\npreprint arXiv:1707.01495, 2017.\nArthur Argenson and Gabriel Dulac-Arnold.\nModel-based offline planning.\narXiv preprint\narXiv:2008.05556, 2020.\nDavid Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-\npolicy evaluation. Advances in Neural Information Processing Systems, 34:4933\u20134946, 2021.\nSerkan Cabi, Sergio G\u00b4omez Colmenarejo, Alexander Novikov, Ksenia Konyushkova, Scott Reed,\nRae Jeong, Konrad Zo\u0142na, Yusuf Aytar, David Budden, Mel Vecerik, et al. A framework for\ndata-driven robotics. arXiv preprint arXiv:1909.12200, 2019.\nAnnie S Chen, Suraj Nair, and Chelsea Finn. Learning generalizable robotic reward functions from\u201d\nin-the-wild\u201d human videos. arXiv preprint arXiv:2103.16817, 2021.\nPeter Dayan. Improving generalization for temporal difference learning: The successor representa-\ntion. Neural computation, 5(4):613\u2013624, 1993.\nDamien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.\nJournal of Machine Learning Research, 6:503\u2013556, 2005.\nBenjamin Eysenbach, Sergey Levine, and Ruslan Salakhutdinov. Replacing rewards with examples:\nExample-based policy search via recursive classification, 2021.\nBenjamin Eysenbach, Tianjun Zhang, Ruslan Salakhutdinov, and Sergey Levine. Contrastive learn-\ning as goal-conditioned reinforcement learning. arXiv preprint arXiv:2206.07568, 2022.\nChelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control\nvia policy optimization. In International conference on machine learning, pages 49\u201358. PMLR,\n2016.\nJustin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse rein-\nforcement learning. International Conference on Learning Representations, 2018a.\nJustin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with\nevents: A general framework for data-driven reward definition. arXiv preprint arXiv:1805.11686,\n2018b.\nScott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.\narXiv preprint arXiv:2106.06860, 2021.\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without\nexploration. arXiv preprint arXiv:1812.02900, 2018.\nSeyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-\nmax q-learning operator for simple yet effective offline and online rl. In International Conference\non Machine Learning, pages 3682\u20133691. PMLR, 2021.\n12\nCONTRASTIVE EXAMPLE-BASED CONTROL\nTrevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The elements of\nstatistical learning: data mining, inference, and prediction, volume 2. Springer, 2009.\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Conference on Neural\nInformation Processing Systems, 2016.\nMatt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara\nNorman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, et al. Acme: A research\nframework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020.\nNatasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah\nJones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of\nimplicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.\nFiras Jarboui and Vianney Perchet. Offline inverse reinforcement learning, 2021. URL https:\n//arxiv.org/abs/2106.05068.\nRyan Julian, Benjamin Swanson, Gaurav S Sukhatme, Sergey Levine, Chelsea Finn, and Karol\nHausman. Efficient adaptation for end-to-end vision-based robotic manipulation. arXiv preprint\narXiv:2004.10190, 2020.\nLeslie Pack Kaelbling. Learning to achieve goals. In IJCAI, pages 1094\u20131099. Citeseer, 1993.\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre\nQuillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforce-\nment learning for vision-based robotic manipulation. In Conference on Robot Learning, pages\n651\u2013673. PMLR, 2018.\nDmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski,\nChelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic re-\ninforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021.\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-\nbased offline reinforcement learning. arXiv preprint arXiv:2005.05951, 2020.\nKsenia Konyushkova, Konrad Zolna, Yusuf Aytar, Alexander Novikov, Scott Reed, Serkan Cabi,\nand Nando de Freitas. Semi-supervised reward learning for offline reinforcement learning. Offline\nReinforcement Learning Workshop at Neural Information Processing Systems, 2020.\nIlya Kostrikov, Jonathan Tompson, Rob Fergus, and Ofir Nachum. Offline reinforcement learning\nwith fisher divergence critic regularization. arXiv preprint arXiv:2103.08050, 2021.\nAviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy\nq-learning via bootstrapping error reduction.\nIn Advances in Neural Information Processing\nSystems, pages 11761\u201311771, 2019.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline\nreinforcement learning. arXiv preprint arXiv:2006.04779, 2020.\nSascha Lange, Thomas Gabel, and Martin A. Riedmiller. Batch reinforcement learning. In Rein-\nforcement Learning, volume 12. Springer, 2012.\n13\nCONTRASTIVE EXAMPLE-BASED CONTROL\nByung-Jun Lee, Jongmin Lee, and Kee-Eung Kim. Representation balancing offline model-based\nreinforcement learning. In International Conference on Learning Representations, 2021. URL\nhttps://openreview.net/forum?id=QpNz8r_Ri2Y.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-\nrial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\nKevin Li, Abhishek Gupta, Ashwin Reddy, Vitchyr H Pong, Aurick Zhou, Justin Yu, and Sergey\nLevine.\nMural: Meta-learning uncertainty-aware rewards for outcome-driven reinforcement\nlearning. In International Conference on Machine Learning, pages 6346\u20136356. PMLR, 2021.\nYao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch reinforce-\nment learning without great exploration. arXiv preprint arXiv:2007.08202, 2020.\nZhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional\nmodels: Consistency and statistical efficiency. arXiv preprint arXiv:1809.01812, 2018.\nAjay Mandlekar, Fabio Ramos, Byron Boots, Silvio Savarese, Li Fei-Fei, Animesh Garg, and Dieter\nFox. Iris: Implicit reinforcement without interaction at scale for learning control from offline\nrobot manipulation data. In 2020 IEEE International Conference on Robotics and Automation\n(ICRA), pages 4414\u20134420. IEEE, 2020.\nTatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-\nefficient reinforcement learning via model-based offline optimization.\narXiv preprint\narXiv:2006.03647, 2020.\nBogdan Mazoure, Remi Tachet des Combes, Thang Long Doan, Philip Bachman, and R Devon\nHjelm. Deep reinforcement and infomax learning. Advances in Neural Information Processing\nSystems, 33:3686\u20133698, 2020.\nSuraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A uni-\nversal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.\nAndrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In Proceedings\nof the Seventeenth International Conference on Machine Learning, ICML \u201900, 2000.\nXue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:\nSimple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\nMatthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow-\nell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce-\nment learning: Challenging robotics environments and request for research.\narXiv preprint\narXiv:1802.09464, 2018.\nDean A Pomerleau. Alvinn: an autonomous land vehicle in a neural network. In Proceedings of the\n1st International Conference on Neural Information Processing Systems, pages 305\u2013313, 1988.\nRafael Rafailov, Tianhe Yu, A. Rajeswaran, and Chelsea Finn. Offline reinforcement learning from\nimages with latent space models. Learning for Decision Making and Control (L4DC), 2021.\n14\nCONTRASTIVE EXAMPLE-BASED CONTROL\nNathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum margin planning. In\nProceedings of the 23rd International Conference on Machine Learning, ICML \u201906, 2006.\nSiddharth Reddy, Anca D. Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement\nlearning with sparse rewards. International Conference on Learning Representations, 2020.\nMartin Riedmiller. Neural fitted q iteration\u2013first experiences with a data efficient neural reinforce-\nment learning method. In European Conference on Machine Learning, pages 317\u2013328. Springer,\n2005.\nStephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. A reduction of imitation learning and\nstructured prediction to no-regret online learning. AISTATS, 2011.\nNoah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-\nunert, Thomas Lampe, Roland Hafner, and Martin Riedmiller. Keep doing what worked: Be-\nhavioral modelling priors for offline reinforcement learning. arXiv preprint arXiv:2002.08396,\n2020.\nAvi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, and Sergey Levine. End-to-end robotic\nreinforcement learning without reward engineering. arXiv preprint arXiv:1904.07854, 2019.\nAvi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine.\nCog:\nConnecting new skills to past experience with offline reinforcement learning. arXiv preprint\narXiv:2010.14500, 2020.\nSamarth Sinha and Animesh Garg. S4rl: Surprisingly simple self-supervision for offline reinforce-\nment learning. arXiv preprint arXiv:2103.06326, 2021.\nJonathan Spencer, Sanjiban Choudhury, Arun Venkatraman, Brian Ziebart, and J. Andrew Bagnell.\nFeedback in imitation learning: The three regimes of covariate shift. ArXiv Preprint, 2021.\nPhillip Swazinna, Steffen Udluft, and Thomas Runkler. Overcoming model bias for robust offline\ndeep reinforcement learning. arXiv preprint arXiv:2008.05533, 2020.\nSteven D Whitehead and Dana H Ballard. Learning to perceive and act by trial and error. Machine\nLearning, 7(1):45\u201383, 1991.\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.\narXiv preprint arXiv:1911.11361, 2019.\nAnnie Xie, Avi Singh, Sergey Levine, and Chelsea Finn. Few-shot goal inference for visuomotor\nlearning and planning. In Conference on Robot Learning, pages 40\u201352. PMLR, 2018.\nDanfei Xu and Misha Denil. Positive-unlabeled reward learning. arXiv preprint arXiv:1911.00459,\n2019.\nHaoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin. Discriminator-weighted offline imita-\ntion learning from suboptimal demonstrations. International Conference on Machine Learning,\n2022.\n15\nCONTRASTIVE EXAMPLE-BASED CONTROL\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey\nLevine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learn-\ning. In Conference on Robot Learning, pages 1094\u20131100. PMLR, 2020a.\nTianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea\nFinn, and Tengyu Ma.\nMopo:\nModel-based offline policy optimization.\narXiv preprint\narXiv:2005.13239, 2020b.\nTianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.\nCombo: Conservative offline model-based policy optimization. arXiv preprint arXiv:2102.08363,\n2021.\nTianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and Sergey Levine.\nHow to leverage unlabeled data in offline reinforcement learning. International Conference on\nMachine Learning, 2022.\nWenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforce-\nment learning. arXiv preprint arXiv:2011.07213, 2020.\nHenry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Ku-\nmar, and Sergey Levine. The ingredients of real-world robotic reinforcement learning. arXiv\npreprint arXiv:2004.12570, 2020.\nBrian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse\nreinforcement learning. In Aaai, volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008.\nKonrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre, Ziyu Wang, Yusuf Ay-\ntar, Misha Denil, Nando de Freitas, and Scott Reed. Offline learning from demonstrations and\nunlabeled experience. arXiv preprint arXiv:2011.13885, 2020a.\nKonrad Zolna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarejo, David Budden, Serkan\nCabi, Misha Denil, Nando de Freitas, and Ziyu Wang. Task-relevant adversarial imitation learn-\ning. Conference on Robot Learning, 2020b.\n16\nCONTRASTIVE EXAMPLE-BASED CONTROL\nAppendix A. Proofs\nThe proof follows by substituting Assumption 1 into the definition of Q-values (Eq. 1):\nProof\nQ\u03b2(s, a) = E\u03b2(\u03c4)\n\" \u221e\nX\nt=0\n\u03b3tr(st)\n\f\f\f\f\ns0=s\na0=a\n#\n=\n1\n1 \u2212 \u03b3\nZ\np\u03b2(st+ = s\u2217 | s, a)r(s\u2217)ds\u2217\n=\n1\n1 \u2212 \u03b3\nZ\np\u03b2(st+ = s\u2217 | s, a)c p\u2217(s\u2217)\np\u03c4(s\u2217)ds\u2217\n=\nc\n1 \u2212 \u03b3\nZ\np\u2217(s\u2217)ef(s,a,s\u2217)ds\u2217 =\nc\n1 \u2212 \u03b3 Es\u2217\u223cp\u2217(s)\nh\nef(s,a,s\u2217)i\n.\nAppendix B. Experimental Details\nWe implemented our method and all baselines using the ACME framework (Hoffman et al., 2020).\n\u2022 Batch size: 1024 for state based experiments, 256 for image based experiments\n\u2022 Training iterations: 250, 000 if task success rates had converged by that point, otherwise\n500, 000\n\u2022 Representation dimension: 256\n\u2022 Reward learning loss (for baselines): binary cross entropy (for ORIL) and positive unlabeled\n(for PURL)\n\u2022 Critic architecture: Two-layer MLP with hidden sizes of 1024. ReLU activations used be-\ntween layers.\n\u2022 Reward function architecture (for baselines): Two-layer MLP with hidden sizes of 1024.\nReLU activations used between layers.\n\u2022 Actor learning rate: 3 \u00d7 10\u22124\n\u2022 Critic learning rate: 3 \u00d7 10\u22124\n\u2022 Reward learning rate (for baselines): 1 \u00d7 10\u22124\n\u2022 \u03bb for behavioral cloning weight in policy loss term: 0.5\n\u2022 \u03b7 for PU loss: 0.5\n\u2022 Size of offline datasets: Each dataset on Fetch tasks the consists of approximately 4, 000\ntrajectories of length 50, except for the FetchPush-image dataset, which consists of ap-\nproximately 40, 000 trajectories. Each dataset on the Sawyer tasks consists of approximately\n4, 000 trajectories of length 200.\n17\nCONTRASTIVE EXAMPLE-BASED CONTROL\nFigure 10: Multitask Drawer Environment.: We apply cross entropy method (CEM) optimization on the LAEO\ndynamics model trained only on the data from the drawer close task to solve six different tasks: close, half-closed, open,\nreach (near), reach (medium), and reach (far). .\nAppendix C. Multitask Critic Experiments\nThe Half-closed task requires the agent to push the drawer from an open position into a halfway\nclosed position. The Open task requires the agent to pull the drawer from a closed position into an\nopen position. The Close task is the same as in the original SawyerDrawerClose environment,\nand requires the agent to push the drawer from an opened position into a closed position. The three\nreaching tasks, Reach-near, Reach-medium, Reach-far, require the agent to reach the end-\neffector to a three different target positions. The tasks are visualized in Figure 10.\nFor these experiments, we load the final checkpoint of a critic network from the previous set\nof experiments (Comparison to Goal-Conditioned RL), and select actions by using cross entropy\nmethod (CEM) optimization on the critic network. By using CEM optimization, we do not need to\ntrain a separate policy network for each of the tasks. Since the LAEO dynamics model is a multistep\ndynamics model (meaning that the model predicts whether a goal state will be reached sometime\nin the future and not just at the subsequent timestep) we are able to use CEM directly with the\ndynamics model. Specifically, for each task, we collect a success example using a scripted policy\nfrom Yu et al. (2020a). Then, at each environment timestep t, we condition the LAEO dynamics\nmodel on the success example, and then run CEM to choose an action that maximizes the output of\nthe dynamics network. At each timestep, we perform 10 iterations of CEM, using a population size\nof 10, 000 and an elite population size of 2, 000. Results are averaged across eight random seeds.\n18\n"
  }
]