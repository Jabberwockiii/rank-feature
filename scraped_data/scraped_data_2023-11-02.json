[
  {
    "title": "Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling",
    "link": "https://arxiv.org/pdf/2311.00430.pdf",
    "upvote": "49",
    "text": "DISTIL-WHISPER: ROBUST KNOWLEDGE\nDISTILLATION VIA LARGE-SCALE\nPSEUDO LABELLING\nSanchit Gandhi, Patrick von Platen & Alexander M. Rush\nHugging Face\n{sanchit, patrick, sasha}@huggingface.co\nABSTRACT\nAs the size of pre-trained speech recognition models increases, running these large\nmodels in low-latency or resource-constrained environments becomes challeng-\ning. In this work, we leverage pseudo-labelling to assemble a large-scale open-\nsource dataset which we use to distill the Whisper model into a smaller variant,\ncalled Distil-Whisper. Using a simple word error rate (WER) heuristic, we select\nonly the highest quality pseudo-labels for training. The distilled model is 5.8 times\nfaster with 51% fewer parameters, while performing to within 1% WER on out-\nof-distribution test data in a zero-shot transfer setting. Distil-Whisper maintains\nthe robustness of the Whisper model to difficult acoustic conditions, while being\nless prone to hallucination errors on long-form audio. Distil-Whisper is designed\nto be paired with Whisper for speculative decoding, yielding a 2 times speed-up\nwhile mathematically ensuring the same outputs as the original model. To facil-\nitate further research in this domain, we make our training code, inference code\nand models publicly accessible.\n1\nINTRODUCTION\nIn recent years, Automatic Speech Recognition (ASR) systems have surpassed human-level accu-\nracy in many academic benchmarks (Amodei et al., 2016; Baevski et al., 2020; Zhang et al., 2020),\nenabling a wide range of applications from transcription services to voice assistants (Aks\u00a8enova et al.,\n2021). Whisper (Radford et al., 2022), a 1.5 billion parameter sequence-to-sequence (Seq2Seq)\ntransformer model (Vaswani et al., 2017) pre-trained on 680,000 hours of weakly supervised speech\nrecognition data, demonstrates a strong ability to generalise to many different datasets and domains\n(Gandhi et al., 2022). However, the ever-increasing size of pre-trained ASR models poses challenges\nwhen deploying these systems in low-latency settings or resource-constrained hardware (He et al.,\n2018; Zhang et al., 2022).\nRecent efforts in natural language processing (NLP) have demonstrated promising advancements in\ncompressing transformer-based models. Knowledge distillation (KD) has successfully been applied\nto reduce the size of models such as BERT (Devlin et al., 2019), without a significant performance\nloss on non-generative classification tasks (Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020).\nInspired by machine translation methods, pseudo-labelling (PL) approaches (Kim & Rush, 2016)\nhave also been explored for Seq2Seq summarisation (Shleifer & Rush, 2020), demonstrating the\npotential for substantial compression of Seq2Seq models on generative tasks. In the audio domain,\nKD has shown promising results for audio classification (Peng et al., 2021; Chang et al., 2021).\nHowever, similar results have not yet been achieved for the more difficult task of speech recognition.\nIn this paper, we apply distillation to the Whisper model in the context of Seq2Seq ASR. We address\nthe challenge of maintaining robustness to different acoustic conditions through our construction of\na large-scale open-source dataset covering 10 distinct domains. By pseudo-labelling the data, we\nensure consistent transcription formatting across the dataset and provide sequence-level distillation\nsignal. We propose a simple word error rate (WER) based heuristic for filtering the pseudo-labelled\ndata and demonstrate that it is an effective method for ensuring good downstream performance of\nthe distilled model.\n1\narXiv:2311.00430v1  [cs.CL]  1 Nov 2023\nWe demonstrate that Distil-Whisper maintains the robustness of Whisper to different audio domains\nand noisy acoustic conditions. We measure this by evaluating the distilled models on four out-\nof-distribution test sets spanning multiple audio domains. The best model performs to within 1%\nWER of original Whisper checkpoint, while being 5.8 times faster with 51% fewer parameters. On\nlong-form evaluation, the distilled model outperforms Whisper by 0.1% WER. We show that this\nperformance gain is due to a lower propensity to hallucinate than the original Whisper model.\nBy sharing the same encoder weights as Whisper, Distil-Whisper can be used efficiently as an as-\nsistant model to Whisper for speculative decoding (Leviathan et al., 2023), for which we achieve a\n2 times improvement in inference speed with only an 8% increase to parameter count. Speculative\ndecoding algorithmically ensures that predictions of the main model are unchanged, meaning it can\nbe used as a drop-in replacement for existing speech recognition pipelines using Whisper.\nOur work suggests that large-scale pseudo-labelling of speech data has been under-explored and\nthat it provides a promising technique for KD. To serve as a basis for further research on distillation\nfor speech recognition, we release training code, inference code and models under https://\ngithub.com/huggingface/distil-whisper.\n2\nRELATED WORK\nIn the NLP domain, model distillation has demonstrated substantial promise in reducing model size\nand computational requirements with minimal degradation to performance. Sanh et al. (2019) use a\nweighted average of the KD loss and the traditional cross entropy data loss to train DistilBERT, a 6\nlayer distilled version of BERT (Devlin et al., 2019), that achieves a 40% decrease in model size, a\n60% increase in speed, and a 97% preservation of language under-standing capabilities on the GLUE\nbenchmark (Wang et al., 2019). Shleifer & Rush (2020) extend the DistilBERT methodology the\nSeq2Seq setting, by initialising the student decoder from maximally spaced layers of the teacher de-\ncoder and incorporating intermediate hidden-states into the KD loss function. The resulting model,\nDistilBART, outperforms the original model on the XSUM and CNN/Daily Mail datasets (Narayan\net al., 2018; See et al., 2017), with 37% model compression and a 48% increase in speed. Du et al.\n(2023) demonstrate that while distilled models perform well on in-distribution (ID) evaluation data,\nthey perform significantly worse than their pre-trained counterparts on out-of-distribution (OOD)\ntest sets. By training on a diverse, large-scale pseudo-labelled dataset, we preserve the robustness to\ndifferent acoustic conditions, demonstrated by an ability to generalise to OOD test data.\nKD has also been applied to the ASR task, albeit with a focus on encoder-only models. Peng\net al. (2021) apply KD to the Wav2Vec 2.0 model (Baevski et al., 2020), achieving 79% model\ncompression and 59% increase in speed. However, these gains come at the expense of a 6.9%\nincrease to WER on the LibriSpeech corpus (Panayotov et al., 2015). Chang et al. (2021) apply a\nsimilar method to the HuBERT model (Hsu et al., 2021), and too report a 7.0% WER increase. Pang\net al. (2018) attempt to distill LAS Chan et al. (2016), an early Seq2Seq ASR model, but find their\nbest distilled model performs 2.2% WER worse than its larger counterpart. This paper focuses on\nKD of Seq2Seq models, with substantial model compression but also preserving WER performance\non OOD test data.\nPrevious studies involving distilling the Whisper model have predominantly been centered around\nreducing model size and memory footprint. Shao et al. (2023) applied KD in combination with\nQuantisation Aware Training (QAT) (Jacob et al., 2017), demonstrating that significant parameter\nreduction is possible with only marginal performance decrement. However, the student model is\ntrained and tested on a small corpus of ID data, giving no measure of its ability to generalise to\nOOD data, and thus its robustness to different acoustic conditions (Geirhos et al., 2020; Radford\net al., 2022). Furthermore, this work did not consider optimising the model for latency. This paper\nseeks to distill the Whisper model to achieve significant model compression, jointly with latency\nimprovements and WER performance on OOD test data. We also evaluate the distilled models\u2019\nrobustness to noisy audio conditions.\n2\nTable 1: Dimensionality details of the pre-trained Whisper checkpoints.\nModel\nLayers\nWidth\nHeads\nParameters / M\ntiny.en\n4\n384\n6\n39\nbase.en\n6\n512\n8\n74\nsmall.en\n12\n768\n12\n244\nmedium.en\n24\n1024\n16\n769\nlarge-v2\n32\n1280\n20\n1550\n3\nBACKGROUND\nWhisper (Radford et al., 2022) is a sequence-to-sequence (Seq2Seq) transformer model (Vaswani\net al., 2017) pre-trained on 680,000 hours of noisy speech recognition data web-scraped from the in-\nternet. When scaled to this quantity of data, Whisper yields competitive results with fully supervised\nsystems, but in a zero-shot setting without the need for any fine-tuning.\nWhisper is composed of a transformer-based encoder (Enc) and decoder (Dec). Assume we have an\ninput speech signal comprised of T feature vectors X1:T = {x1, . . . , xT } and a target transcription\ny1:N = {y1, . . . , yN} of N tokens in the standard speech recognition setting. The encoder FEnc is\ntrained to map X1:T to a sequence of hidden-state vectors H1:M:\nFEnc : X1:T \u2192 H1:M\n(1)\nThe sequence length of the hidden-states M is typically half than that of the input speech feature\nsequence T by action of the convolutional layers in the encoder stem that downsample the input.\nThe decoder auto-regressively predicts a probability distribution for the next token yi, conditional\non all previous tokens y<i and the encoder hidden-states H1:M:\nP (yi|y<i, H1:M)\n(2)\nTo train the Whisper model, we assume a dataset where each example (X1:T , y1:N) is an (audio,\ntext) pair. The model is trained using the standard cross-entropy (CE) loss, where the model is\ntrained to predict an instance class by maximising the estimated probability of the target class labels:\nLCE = \u2212\nN\nX\ni=1\nP (yi|y<i, H1:M)\n(3)\nThere are five variants of the Whisper model summarised in Table 1. The models share the same\nSeq2Seq architecture but have different dimensionality. For all model sizes, the encoder and decoder\nhave the same width, heads and number of layers in the transformer blocks. The first version of the\nWhisper paper introduced a large-v1 checkpoint, which was subsequently re-trained with regulari-\nsation more training epochs to give an improved large-v2 version (Radford et al., 2022). As both\nmodels share the same dimensions, we present results for the large-v2 model in this paper.\n4\nROBUST KNOWLEDGE-DISTILLATION\n4.1\nKNOWLEDGE DISTILLATION\nKnowledge distillation (KD) (Hinton et al., 2015) is a compression technique in which a smaller\nstudent model is trained to reproduce the behaviour of a larger teacher one. Compared to minimising\nthe CE loss between the student model\u2019s predictions and the training labels, KD allows the student\nmodel to learn from the full predictive distribution of possible next tokens in a given context, rather\nthan just maximising the probability of the next target token in the training data.\n3\nShrink and Fine-Tune\nThe most basic distillation method involves shrinking the teacher model to\na smaller student size, and training the student on the CE objective in Equation 3. Following Shleifer\n& Rush (2020), we perform layer-based compression by initialising the student model by copying\nthe weights from maximally spaced layers of the teacher model. For example, when initialising a 2-\nlayer student model from a 32-layer teacher model, we copy the 1st and 32nd layers from the teacher\nto the student. Given the simplicity and effectiveness of this strategy in the Seq2Seq summarisation\nsetting (Shleifer & Rush, 2020; Li et al., 2022), we use it for all distillation methods.\nPseudo Labelling\nIn the pseudo-label setting (Kim & Rush, 2016), we replace the ground truth\ntext transcription y1:N with the teacher\u2019s generation \u02c6y1:N \u2032 for the corresponding input audio X1:T :\nLP L = \u2212\nN \u2032\nX\ni=1\nP (yi|\u02c6y<i, H1:M)\n(4)\nThis form of distillation can be viewed as \u201csequence-level\u201d KD, where knowledge is transferred\nfrom the teacher model to the student model across a sequence of generated pseudo-labels (Kim &\nRush, 2016).\nKullback-Leibler Divergence\nIn the KL Divergence (Kullback & Leibler, 1951) setting, the full\nprobability distribution of the student model Pi is trained to match the full distribution of the teacher\nmodel Qi by minimising the KL divergence over the entire set of next possible tokens at position i:\nLKL =\nN\nX\ni=1\nKL (Qi, Pi)\n(5)\nThis can be viewed as \u201cword-level\u201d KD, where knowledge is transferred from the teacher model to\nthe student model via the logits over the possible tokens (Kim & Rush, 2016). The KL Divergence\nis attractive since it provides information over all classes and has less variance in gradients than the\nCE loss (Hinton et al., 2015).\nObjective\nThe final KD training objective is a weighted sum of the KL and PL terms:\nLKD = \u03b1KLLKL + \u03b1P LLP L\n(6)\nwhere \u03b1KL and \u03b1P L are scalar weights for the KL and loss terms respectively. Following (Shleifer\n& Rush, 2020), we set \u03b1KL = 0.8 and \u03b1P L = 1.0.\n4.2\nPSEUDO-LABEL SELECTION: WER THRESHOLD\nThe pseudo-labels generated by the Whisper model are subject to transcription errors and halluci-\nnations (Bain et al., 2023; Zhang et al., 2023). To ensure we only train on accurate pseudo-labels,\nwe propose a simple heuristic to filter our pseudo-labelled training data. For each training sample,\nwe normalise the ground truth labels y1:N and the Whisper generated pseudo-labels \u02c6y1:N \u2032 using the\nWhisper English normaliser (Radford et al., 2022). We compute the word error rate (WER) between\nthe normalised ground truth and normalised psuedo-labels, and discard any samples that exceed a\ngiven WER threshold \u03bb:\nWER (norm (y1:N) , norm (\u02c6y1:N \u2032)) > \u03bb\n(7)\nWe tune the value of \u03bb on our validation sets, and demonstrate in Section 9.1 that this simple filtering\nmethod improves transcription quality and downstream model performance.\n4\n5\nCHUNKED LONG-FORM TRANSCRIPTION\nWhisper models have a fixed receptive field corresponding to 30-seconds of input audio and cannot\nprocess longer audio inputs at once. Most academic datasets comprise of short utterances less than\n30-seconds in duration, and so this is not a problem. However, real-world applications such as\nmeeting transcriptions typically require transcribing long audio files of many minutes or hours.\nThe original Whisper paper presents a long-form transcription algorithm that sequentially tran-\nscribes 30-second segments of audio and shifts the sliding window according to the timestamps\npredicted by the model. This auto-regressive algorithm requires both beam-search and temperature\nfallback to ensure accurate long-form transcription (Radford et al., 2022).\nWe use an alternative strategy, first proposed by Patry (2022), in which the long audio file is chunked\ninto smaller segments with a small overlap between adjacent segments. The model is run over each\nchunk and the inferred text is joined at the strides by finding the longest common sequence between\noverlaps. This stride enables accurate transcription across chunks without having to transcribe them\nsequentially. We observe that this algorithm only requires greedy decoding to reliably transcribe\nlong audio files. Furthermore, this algorithm is semi auto-regressive in the sense that the chunks\ncan be transcribed in any order, provided adjacent chunks are subsequently joined correctly at their\nboundaries. This allows chunks to be transcribed in parallel through batching. In practice, this\nyields up to 9 times improvements in inference speed compared to sequential transcription over long\naudio files. In this work, use the chunked long-form transcription algorithm when evaluating both\nthe Whisper and Distil-Whisper models.\n6\nSPECULATIVE DECODING\nSpeculative decoding (SD) (Leviathan et al., 2023) is a method for accelerating the inference of auto-\nregressive transformer models by employing a faster assistant model. The assistant model generates\na sequence of candidate tokens, all of which are verified by the main model in a single forward\npass. By generating with the faster assistant model and only performing validation forward passes\nwith the main model, the decoding process is sped-up significantly. The i-th candidate token from\nthe assistant model \u02c6yi is only kept if all previous candidate tokens \u02c6y<i match the validation tokens\npredicted by the main model. Consequently, speculative decoding ensures that the generated output\nexactly matches the sequence of tokens that would be generated by the main model, making it a\nnatural replacement for existing inference pipelines that use the main model.\nDistilSpec (Zhou et al., 2023) proposes using a knowledge-distilled student model as the assistant to\nbetter align the distribution of the assistant model with the main one. We apply the same principal\nhere, using Distil-Whisper as the assistant to Whisper.\n7\nEXPERIMENTAL SETUP\n7.1\nDATA\nInspired by SpeechStew (Chan et al., 2021), we assemble a large corpus of ASR training data for\nlarge-scale KD through a combination of nine publicly available speech recognition datasets. An\noverview of the datasets is presented in Table 2, with additional details in Appendix A.1. The\ncombined dataset contains 21,170 hours of speech data, encompassing over 18,260 speakers and 10\ndistinct domains. We load and pre-process all datasets in the Hugging Face Datasets library (Lhoest\net al., 2021), streaming the data from the Hugging Face Hub1.\nWe generate pseudo-labels for our training data with the Whisper large-v2 checkpoint, using the\nFlax Whisper implementation in the Hugging Face Transformers library (Heek et al., 2020; Wolf\net al., 2020). We found there to be little difference in the downstream performance of the distilled\nmodel after pseudo-labelling using either greedy or beam-search, and so we opted to pseudo-label\nthe training data with greedy decoding for its faster inference speed.\n1Training\ndatasets:\nhttps://huggingface.co/collections/distil-whisper/\ntraining-datasets-6538d05c69721489d1db1e49\n5\nTable 2: Summary of the open-source datasets used for training. For some datasets, the number of\nspeakers cannot be reliably retrieved. We denote these entries as \u201cunknown\u201d.\nDataset\nSize / h\nSpeakers\nDomain\nLicence\nPeople\u2019s Speech\n12,000\nunknown\nGovernment, interviews\nCC-BY-SA-4.0\nGigaSpeech\n2,500\nunknown\nAudiobook, podcast, YouTube\napache-2.0\nCommon Voice 13\n2,400\nunknown\nNarrated Wikipedia\nCC0-1.0\nFisher\n1,960\n11,900\nTelephone conversations\nLDC\nLibriSpeech\n960\n2,480\nAudiobooks\nCC-BY-4.0\nVoxPopuli\n540\n1,310\nEuropean Parliament\nCC0\nTED-LIUM\n450\n2,030\nTED talks\nCC-BY-NC-ND 3.0\nSwitchBoard\n260\n540\nTelephone conversations\nLDC\nAMI\n100\nunknown\nMeetings\nCC-BY-4.0\nTotal\n21,170\n18,260+\nFigure 1: Architecture of the Distil-Whisper model. The encoder (shown in green) is entirely\ncopied from the teacher to the student and frozen during training. The student\u2019s decoder consists\nof only two decoder layers, which are initialised from the first and last decoder layer of the teacher\n(shown in red). All other decoder layers of the teacher are discarded. The model is trained on a\nweighted sum of the KL divergence and PL loss terms.\n7.2\nTRAINING\nWe initialise the student models by copying the entire encoder from the teacher and freeze it dur-\ning training. We distill 2-layer decoder checkpoints from the medium.en and large-v2 models by\ncopying the first and last decoder layers, which we refer to as distil-medium.en and distil-large-\nv2 respectively. The dimensionality details of the distilled models are shown in Table 3, with the\narchitecture and training objective summarised in Figure 1.\nWe train with a batch size of 256 for a total of 80,000 optimisation steps, which amounts to eight\nepochs of training. Since we only train for eight epochs, the risk of over-fitting is low, and so we do\nnot use any data augmentation or regularisation techniques. Instead, we rely on the diversity of our\ndataset to ensure model generalisation and robustness, the same premise used in training the original\nWhisper model (Radford et al., 2022). Refer to Appendix B.1 for full details of our training set-up.\n6\nTable 3: Dimensionality details of Distil-Whisper checkpoints.\nModel\nEnc. Layers\nDec. Layers\nWidth\nHeads\nParams. / M\ndistil-medium.en\n24\n2\n1024\n16\n394\ndistil-large-v2\n32\n2\n1280\n20\n756\nTable 4: Summary of the OOD datasets used for short and long-form evaluation.\nDataset\nSize / h\nSpeakers\nDomain\nLicence\nShort-Form\nCHiME-4\n7\n87\nNews broadcast\nLDC\nEarnings-22\n115\nunknown\nFinancial meetings\nCC-BY-SA-4.0\nFLEURS\n2\n3\nNarrated Wikipedia\nCC-BY-4.0\nSPGISpeech\n100\nunknown\nFinancial meetings\nUser Agreement\nLong-Form\nEarnings-21\n39\nunknown\nFinancial meetings\nCC-BY-SA-4.0\nEarnings-22\n115\nunknown\nFinancial meetings\nCC-BY-SA-4.0\nMeanwhile\n1\n1\nTV show\nUser Agreement\nRev 16\n16\nunknown\nPodcasat\nCC-BY-4.0\n7.3\nSHORT-FORM EVALUATION\nThe objective of Distil-Whisper is to compress the original Whisper model into a smaller, faster vari-\nant of the model that retains its robustness to different acoustic conditions (speakers, speaking styles\nand domains). To investigate this capability, we employ a broad collection of speech recognition\ndatasets to examine whether Distil-Whisper can effectively generalise across datasets and domains.\nWe evaluate the Distil-Whisper model on a total of 15 short-form datasets. The first 11 of these\ndatasets are the corresponding test splits for the training data used to distil the model. These test\nsplits are in distribution (ID) with the training data. The remaining four datasets are used as test\nsets only, where Distil-Whisper is assessed in a zero-shot setting, that is without the use of the\ncorresponding training data, thereby measuring the model\u2019s ability to generalise to out of distribution\n(OOD) datasets. An overview of the OOD evaluation datasets is presented in Table 4. Full details\nof the evaluation datasets are provided in Appendix A.2.\nWe examine both overall robustness, that is the average performance over all datasets, and effective\nrobustness (Taori et al., 2020), which measures the difference in expected performance between a\nreference dataset that is ID, and one or more datasets that are OOD. A model with high effective\nrobustness does better on OOD datasets as a function of performance on the reference dataset. A\nmodel with ideal effective robustness performs equally well on all datasets. In our experiments,\nwe use GigaSpeech (Chen et al., 2021) as the reference dataset, owing to the fact it contains web-\nscraped data from audiobooks, podcasts and YouTube videos, and is such ID with both the pre-\ntrained Whisper training data and the distilled Whisper train set.\nWe evaluate the noise robustness of the Distil-Whisper models, the original Whisper models, and\neight other LibriSpeech-trained models by measuring the WER on the LibriSpeech test-clean dataset\nwith increasing amounts of noise applied to the input audio. The LibriSpeech dataset is an ideal\nchoice of dataset since it has a high signal-to-noise ratio (SNR), and thus enablers evaluation over\na large range of SNRs as the amount of noise is increased. We add either white noise or pub noise\nfrom the Audio Degradation Toolbox (Mauch & Ewert, 2013). The pub noise simulates a naturally\nnoisy environment, with ambient sounds and indistinguishable conversations characteristic of a busy\nrestaurant or pub. The level of additive noise is determined based on the signal power of individual\ninstances, and corresponds to a specified SNR.\n7\nTable 5: Distil-Whisper retains the WER performance of the Whisper model but with faster\ninference speed. Average WER results over the four OOD short-form test sets and the four OOD\nlong-form test sets. Relative latency is the inference time relative to the large-v2 checkpoint. For\nshort-form evaluation, the batch size is set to 1. For long-form evaluation, the chunked long-form\ntranscription algorithm is used with a batch size of 16.\nModel\nParams / M\nShort Form\nLong Form\nRel. Latency\nAvg. WER\nRel. Latency\nAvg. WER\ntiny.en\n39\n6.1\n18.9\n5.4\n18.9\nbase.en\n74\n4.9\n14.3\n4.3\n15.7\nsmall.en\n244\n2.6\n10.8\n2.2\n14.7\nmedium.en\n769\n1.4\n9.5\n1.3\n12.3\nlarge-v2\n1550\n1.0\n9.1\n1.0\n11.7\ndistil-medium.en\n394\n6.8\n11.1\n8.5\n12.4\ndistil-large-v2\n756\n5.8\n10.1\n5.8\n11.6\n7.4\nLONG-FORM EVALUATION\nWe evaluate the long-form transcription performance of the Distil-Whisper model on four OOD\ndatasets comprising different lengths and acoustic conditions, in order to cover the broadest possible\ndistribution of data. An overview of the long-form datasets is presented in Table 4. Full details about\nthe long-form datasets are provided in Appendix A.3.\nThe Whisper model demonstrates a susceptibility to hallucinate, characterised by either the repeti-\ntive generation of identical sequences, or predicting passages of text not spoken in the audio input\n(Bain et al., 2023; Zhang et al., 2023). These hallucinations errors are most prevalent in long-form\naudio transcription, particularly when the audio contains large amounts of silence between spoken\nutterances. To quantify the amount of repetition and hallucination in the predicted transcriptions, we\nmeasure the number of repeated 5-gram word duplicates (5-Dup.) and the insertion error rate (IER)\nover the four OOD long-form datasets. We also report the substitution error rate (SER) and deletion\nerror rate (DER) to quantify the frequency of substitutions and deletions in the transcriptions.\n8\nRESULTS\n8.1\nSHORT-FORM EVALUATION\nTable 5 reports the average WER scores over the four OOD short-form test sets for the Whisper and\nDistil-Whisper checkpoints. For a detailed breakdown of results on a per-dataset basis, refer to Ap-\npendix C. Of the two distilled models, the distil-large-v2 model achieves the lowest overall average\nWER of 10.1%. It is one percentage point higher than the large-v2 baseline, with 5.8 times faster\ninference speed and fewer than half the parameters. The inference speed is comparable to the tiny.en\nWhisper checkpoint, but with an 8.8% WER advantage. The distil-medium.en model is on average\n2.0% WER higher than the large-v2 model, with 6.8x faster inference and 75% model compression.\nThese findings highlight that Distil-Whisper retains the overall robustness of the Whisper model,\nwith comparable WER performance averaged over multiple OOD datasets, but with significantly\nfaster inference speed and reduced parameter count.\nTable 6 compares the effective robustness of large-v2 to distil-large-v2. The models have very close\nperformance on the reference distribution, performing to within 2% relative WER. The distilled\nmodel improves upon the pre-trained baseline for the SPGISpeech dataset by 12.8% relative, but\nperforms worse on the three other OOD datasets. Compared to the teacher model, the distilled model\nachieves an overall WER increase of 0.8% absolute (or 10.7% relative). The narrow performance\ngap indicates that Distil-Whisper has comparable effective robustness to the original Whisper model.\n8\nTable 6: Effective robustness across various datasets. WER results for one ID reference dataset\nand four OOD datasets. The relative error rate (RER) is shown on the right, giving the per-dataset\neffective robustness scores. The macro-average results are shown in the bottom row.\nDataset\nlarge-v2\ndistil-large-v2\nRER\nGigaSpeech\n10.7\n10.5\n-2.0\nCHIME-4\n11.8\n14.0\n18.4\nEarnings-22\n16.6\n16.9\n1.6\nFLEURS\n4.2\n6.3\n48.2\nSPGISpeech\n3.8\n3.3\n-12.8\nAverage\n9.4\n10.2\n10.7\nTable 7: Comparison of long-form transcription algorithms. Average WER results over the four\nOOD long-form test sets for the sequential and chunked long-form algorithms. Relative latency is\nthe inference time relative to the large-v2 model with sequential long-form decoding. The chunked\ntranscription results are reported using a batch size of 16.\nModel\nAlgorithm\nRel. Latency\nAvg. OOD WER\nlarge-v2\nSequential\n1.0\n10.4\nlarge-v2\nChunked\n9.9\n11.7\ndistil-large-v2\nChunked\n57.5\n11.6\n8.2\nLONG-FORM EVALUATION\nWe compare the long-form transcription performance of Distil-Whisper to the pre-trained Whisper\nmodels on the four OOD long-form test sets. Table 5 reports the relative latency for a batch size of\n16, as well as the macro-average WER. The per-dataset WER scores are provided in Appendix C.\nThe results show that distil-large-v2 outperforms or equals large-v2 on four of the five test sets, with\nan average WER that is 0.1% lower with 5.8 times faster batched inference speed. It is only on the\nMeanwhile dataset that the Distil-Whisper model performs worse, which contains recordings from\na single speaker with a high-frequency of uncommon words.\nTable 7 compares the performance of the Whisper long-form sequential algorithm to the Distil-\nWhisper chunked one. The large-v2 model with the chunked algorithm yields a 9.9 times speed-up\ncompared to the sequential one, with a 1.3% increase to average OOD WER. This demonstrates the\ninference speed gain that is achieved through batching. Using the distilled model in combination\nwith the chunked algorithm provides further improvements: the distil-large-v2 model is 57.5 times\nfaster than the baseline large-v2 implementation, while performing to within 1.2% WER.\n8.3\nROBUSTNESS TO ADDITIVE NOISE\nFigure 2 shows how WER performance degrades as the intensity of additive noise increases on the\nLibriSpeech test-clean dataset. Of the 14 models we compare to, eight are pre-trained and/or fine-\ntuned on LibriSpeech. There are many models that outperform the Distil-Whisper models under low\nnoise (40 dB SNR), including the Whisper checkpoints and LibriSpeech trained models. However,\nas the noise becomes more intensive, the WERs of the Distil-Whisper checkpoints degrade less\nseverely than the LibriSpeech trained models and approach those of Whisper, especially when the\nadditive pub noise decreases below 10 dB. Since we copy the full encoder and freeze it during\ntraining, the student and teacher models share the same encoder. Thus, they show similar robustness\nto noise, particularly under more natural distribution shifts like pub noise.\n9\nFigure 2: Effect of noise on WER performance. WER on LibriSpeech test-clean as a function of\nSNR under additive white noise (left) and pub noise (right).\n8.4\nROBUSTNESS TO HALLUCINATIONS\nTable 8 reports the number of repeated 5-gram word duplicates (5-Dup.) and insertion error rate\n(IER) metrics averaged over the four long-form test sets. In addition to the Whisper and Distil-\nWhisper models, we report the results for the official Wav2Vec 2.0 large model fine-tuned on 960\nhours of LibriSpeech data. This checkpoint provides a comparison between the Whisper Seq2Seq\narchitecture and a CTC based one (Graves et al., 2006). A CTC model should be less prone to\nhallucination errors given it is an encoder-only architecture with a linear head over the vocabulary.\nThe distil-large-v2 model has 1.3 times fewer repetition errors than Whisper large-v2. It also obtains\nthe lowest average IER, improving on Whisper by 1.2% absolute. This indicates that the amount of\nhallucination is improved in Distil-Whisper compared to the original Whisper model. The average\ndeletion error rate (DER) is comparable for both large-v2 and distil-large-v2, performing to within\n0.3% DER. However, the substitution error rate (SER) is 1.4% higher for distil-large-v2, indicating\nthat the distilled models are subject to more substitution errors. Overall, the reduction in IER out-\nweighs the increase to SER, and Distil-Whisper returns the lowest WER of all the models. While\nthe wav2vec 2.0 model underperforms in its average WER score, we find that it is far less prone to\nrepetition errors compared to both Whisper and Distil-Whisper. Further work is needed to reduce\nrepetition errors in Seq2Seq ASR models.\n8.5\nSPECULATIVE DECODING\nTable 9 reports the relative latency of the medium.en and large-v2 models with speculative decoding.\nWe compare the latency using either the smallest Whisper checkpoints or the Distil-Whisper models\nas the assistant. Since the outputs of the original Whisper models are obtained exactly, we report the\nrelative latency only. The distilled student models are initialised by copying and freezing the entire\nencoder from the teacher, meaning they use exactly the same encoder as main Whisper models.\nTherefore, when running SD, the encoder can be shared between the main and assistant models, and\nonly the distilled decoder layers have to be loaded in addition to the main model. This results in just\nan 8% increase to parameter count when using distil-large-v2 as the assistant to large-v2.\n10\nTable 8: Detailed long-form error rates. Average number of repeated 5-gram word duplicates\n(5-Dup.) and insertion error rate (IER) over the four long-form test sets. Shown also are the average\nsubstitution error rate (SER), deletion error rate (DER) and word error rate (WER) metrics.\nModel\n5-Dup.\nIER\nSER\nDER\nWER\nwav2vec2-large-960h\n7971\n4.8\n18.9\n4.6\n28.3\ntiny.en\n23313\n5.1\n8.9\n4.8\n18.9\nbase.en\n22719\n4.3\n6.6\n4.8\n15.7\nsmall.en\n26377\n3.3\n5.0\n6.5\n14.7\nmedium.en\n23549\n3.5\n4.2\n4.6\n12.3\nlarge-v2\n23792\n3.3\n3.9\n4.5\n11.7\ndistil-medium.en\n18918\n2.5\n5.6\n4.4\n12.4\ndistil-large-v2\n18503\n2.1\n5.3\n4.2\n11.6\nTable 9: Impact of speculative decoding. Relative latency of medium.en and large-v2 using Whis-\nper and Distil-Whisper assistant models. The relative latency is computed relative to the large-v2\nmodel without speculative decoding for a batch size of 1.\nModel\nParams / M\nRel. Latency\nmedium.en\n769\n1.4\nwith tiny.en\n808\n2.7\nwith distil-medium.en\n856\n3.3\nlarge-v2\n1550\n1.0\nwith tiny\n1589\n2.1\nwith distil-large-v2\n1669\n2.0\nSpeculative decoding with the distil-large-v2 assistant yields a 2.0 times improvement to inference\nspeed over large-v2 alone. This is comparable to using the tiny model as the assistant. For the\nmedium.en model, using distil-medium.en as an assistant provides a 2.4 times speed-up. This is\ngreater than using the tiny.en checkpoint as an assistant, which is only 2.0 times faster. Overall,\nspeculative decoding provides significant speed-ups to latency while mathematically ensuring the\nsame outputs, making it a natural replacement for existing Whisper pipelines.\n9\nANALYSIS\n9.1\nWER THRESHOLD\nDuring training, we filter pseudo-labelled data where the normalised WER between the Whisper-\ngenerated pseudo-labels and the ground truth labels exceeds a given threshold \u03bb. To investigate the\neffect of this threshold on the performance of the distilled model, we train a series of distil-large-v2\ncheckpoints for 10,000 training steps (or two epochs) on a range of threshold values. Table 10 shows\nthe average WER performance of the trained models. Setting the threshold too high allows mis-\ntranscribed or hallucinated transcriptions to enter the training set. Setting the WER threshold low\nretains only the most accurate Whisper-generated pseudo-labels, but also only the easiest samples\n(i.e. those with very low WER) and discards a larger proportion of the training data. We found\nthat a WER threshold of 10% provides a good trade-off between these opposing factors. Had we\ntrained for longer, the effect of using a higher quantity of training data might have become more\npronounced, thus favouring higher thresholds. Using a WER threshold to filter pseudo-labelled data\nmay compensate for the decreased transcription accuracy of the Whisper-generated labels predicted\nwith greedy decoding as opposed to beam-search. We find it is an effective strategy for improving\nthe performance of Seq2Seq ASR systems trained on pseudo-labelled data.\n11\nTable 10: The WER threshold is an effective filter for PL data. Average WER of the distil-large-\nv2 checkpoint on the 11 ID and three OOD validation sets as the WER threshold \u03bb is reduced.\n\u03bb\nData Filtered / %\nAvg. ID WER\nAvg. OOD WER\nAvg. WER\n100\n0.0\n14.8\n9.1\n13.4\n80\n6.2\n13.5\n7.5\n12.1\n40\n11.9\n13.3\n7.4\n12.0\n20\n24.2\n13.1\n7.3\n11.7\n15\n32.0\n13.0\n7.4\n11.7\n10\n45.4\n12.6\n7.4\n11.4\n5\n60.3\n12.6\n7.3\n11.4\nTable 11: Performance improves with increasing dataset size. Average WER of the distil-large-v2\ncheckpoint on the 11 ID and three OOD validation sets as the amount of training data is increased.\nSize / h\nProportion / %\nAvg. ID WER\nAvg. OOD WER\nAvg. WER\n435\n2\n17.1\n13.8\n16.4\n871\n4\n15.1\n10.5\n14.0\n1,742\n8\n14.0\n9.2\n12.9\n3,483\n16\n13.3\n7.8\n12.0\n6,966\n32\n13.0\n7.7\n11.8\n13,933\n64\n12.8\n7.4\n11.6\n21,770\n100\n12.6\n7.4\n11.4\n9.2\nDATASET SCALING\nTo study the amount of data is required to distil Whisper, we trained a series of distil-large-v2 models\non subsampled versions of our dataset. Table 11 shows the average WER performance of the distil-\nlarge-v2 model for each of the dataset proportions. All increases in dataset size result in improved\nperformance on the ID validation sets. On the OOD validation sets, performance improves rapidly\nfrom 435 to 3,483 hours, and then slows down significantly between 3,483 hours and 13,933 hours.\nUsing the full dataset of 21,770 hours \u2013 a further 1.6 times increase in size \u2013 results in no further\nimprovement to OOD WER. This suggests that there are diminishing gains increasing the amount\nof pseudo-labelled training data above 13,933 hours, or 2% of the Whisper pre-training data.\n9.3\nMODEL SIZE\nTable 12 shows the latency and WER performance for 16, 8, 4 and 2 decoder-layer models. The 16-\nlayer decoder model largely retains the WER performance of the 32-layer teacher model, performing\nto within 0.1% OOD WER with a 1.9 times improvement in latency. As the number of decoder layers\nis reduced further, the average OOD WER increases more significantly. The maximum increase is\n2.1% for the 2-layer decoder model, however this configuration is 5.8 times faster than the teacher\nmodel. These results highlight the trade-off between latency and performance as the number of\ndecoder layers is reduced.\nIn distilling only the decoder, the parameter reduction is limited to 51%. To further reduce the\nparameter count, we can jointly distil the encoder and decoder. Table 12 compares the performance\nof a full 32-layer encoder student model to a reduced 16-layer one, both with 2-layer decoders.\nWhen the encoder is reduced to 16-layers, the parameter count decreases by an additional 19%.\nHowever, the OOD WER performance degrades by 3.1% absolute. This suggests that having a deep\nencoder is paramount for maintaining strong WER performance of distilled Seq2Seq ASR models.\n12\nTable 12: Trade-off between latency and WER performance with decreasing model size. Aver-\nage WER over the 11 ID and three OOD validation sets as the number of encoder and decoder layers\nin the large-v2 checkpoint are reduced. The first row corresponds to the teacher checkpoint large-v2.\nThe following rows correspond to the distilled models, which are trained for 10,000 optimisation\nsteps (or two epochs).\nEnc\nDec\nParams / M\nRel. Latency\nID WER\nOOD WER\nAvg. WER\n32\n32\n1543\n1.0\n12.8\n5.3\n11.1\n32\n16\n1124\n1.9\n11.3\n5.4\n9.9\n32\n8\n914\n3.0\n11.6\n6.0\n10.3\n32\n4\n809\n4.3\n12.0\n6.5\n10.8\n32\n2\n756\n5.8\n12.6\n7.4\n11.4\n16\n2\n441\n8.6\n16.0\n10.5\n14.7\n10\nCONCLUSION\nWe introduce Distil-Whisper, a distilled version of Whisper that is 49% smaller, 5.8 times faster,\nand within 1% WER performance on OOD short-form audio. On OOD long-form audio, Distil-\nWhisper outperforms Whisper, due to fewer hallucinations and repetitions. We show that large-scale\npseudo-labelling is an effective strategy for distilling ASR models, in particular when combined our\nWER threshold filter. We further demonstrate that Distil-Whisper can be used in combination with\nWhisper using speculative decoding to obtain the same outputs as the original model with 2 times\nfaster inference.\n11\nACKNOWLEGEMENTS\nWe thank Nicolas Patry and Arthur Zucker for their implementation of the chunked long-form tran-\nscription algorithm in Transformers and Jo\u02dcao Gante for the implementation of speculative decoding.\nWe gratefully acknowledge the support of Google\u2019s TPU Research Cloud (TRC) program for pro-\nviding Cloud TPU resources for this research.\nREFERENCES\nAl\u00a8ena Aks\u00a8enova, Daan van Esch, James Flynn, and Pavel Golik. How Might We Create Better\nBenchmarks for Speech Recognition?\nIn Proceedings of the 1st Workshop on Benchmarking:\nPast, Present and Future, pp. 22\u201334, Online, August 2021. Association for Computational Lin-\nguistics.\ndoi: 10.18653/v1/2021.bppf-1.4.\nURL https://aclanthology.org/2021.\nbppf-1.4.\nDario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl\nCase, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, Jie Chen, Jingdong Chen,\nZhijie Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Ke Ding, Niandong Du, Erich\nElsen, Jesse Engel, Weiwei Fang, Linxi Fan, Christopher Fougner, Liang Gao, Caixia Gong,\nAwni Hannun, Tony Han, Lappi Johannes, Bing Jiang, Cai Ju, Billy Jun, Patrick LeGresley,\nLibby Lin, Junjie Liu, Yang Liu, Weigao Li, Xiangang Li, Dongpeng Ma, Sharan Narang, Andrew\nNg, Sherjil Ozair, Yiping Peng, Ryan Prenger, Sheng Qian, Zongfeng Quan, Jonathan Raiman,\nVinay Rao, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Kavya Srinet, Anuroop Sriram,\nHaiyuan Tang, Liliang Tang, Chong Wang, Jidong Wang, Kaifu Wang, Yi Wang, Zhijian Wang,\nZhiqian Wang, Shuang Wu, Likai Wei, Bo Xiao, Wen Xie, Yan Xie, Dani Yogatama, Bin Yuan,\nJun Zhan, and Zhenyao Zhu. Deep Speech 2 : End-to-End Speech Recognition in English and\nMandarin. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd\nInternational Conference on Machine Learning, volume 48 of Proceedings of Machine Learning\nResearch, pp. 173\u2013182, New York, New York, USA, 20\u201322 Jun 2016. PMLR. URL https:\n//proceedings.mlr.press/v48/amodei16.html.\n13\nRosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty,\nReuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber.\nCommon Voice: A\nMassively-Multilingual Speech Corpus. In Proceedings of the 12th Language Resources and\nEvaluation Conference, pp. 4218\u20134222, Marseille, France, May 2020. European Language\nResources Association.\nISBN 979-10-95546-34-4.\nURL https://www.aclweb.org/\nanthology/2020.lrec-1.520.\nIgor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky,\nDavid Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Claudio Fantacci, Jonathan Godwin,\nChris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski,\nThomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza Merzic,\nVladimir Mikulik, Tamara Norman, John Quan, George Papamakarios, Roman Ring, Francisco\nRuiz, Alvaro Sanchez, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan,\nLuyu Wang, Wojciech Stokowiec, and Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL\nhttp://github.com/deepmind.\nAlexei Baevski,\nYuhao Zhou,\nAbdelrahman Mohamed,\nand Michael Auli.\nwav2vec\n2.0:\nA\nFramework\nfor\nSelf-Supervised\nLearning\nof\nSpeech\nRepresentations.\nIn\nH. Larochelle,\nM. Ranzato,\nR. Hadsell,\nM.F. Balcan,\nand H. Lin (eds.),\nAdvances\nin Neural Information Processing Systems, volume 33, pp. 12449\u201312460. Curran Asso-\nciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf.\nMax Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. WhisperX: Time-Accurate Speech\nTranscription of Long-Form Audio. In Proc. INTERSPEECH 2023, pp. 4489\u20134493, 2023. doi:\n10.21437/Interspeech.2023-78.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:\n//github.com/google/jax.\nJean Carletta. Unleashing the killer corpus: experiences in creating the multi-everything AMI Meet-\ning Corpus. Language Resources and Evaluation, 41(2):181\u2013190, 2007. ISSN 1574-020X. doi:\n10.1007/s10579-007-9040-x.\nWilliam Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural\nnetwork for large vocabulary conversational speech recognition.\nIn 2016 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4960\u20134964, 2016. doi:\n10.1109/ICASSP.2016.7472621.\nWilliam Chan, Daniel Park, Chris Lee, Yu Zhang, Quoc Le, and Mohammad Norouzi. SpeechStew:\nSimply Mix All Available Speech Recognition Data to Train One Large Neural Network. arXiv\ne-prints, art. arXiv:2104.02133, April 2021.\nHeng-Jui Chang, Shu wen Yang, and Hung yi Lee. Distilhubert: Speech Representation Learning by\nLayer-Wise Distillation of Hidden-Unit Bert. ICASSP 2022 - 2022 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pp. 7087\u20137091, 2021. URL https:\n//api.semanticscholar.org/CorpusID:238354153.\nGuoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su,\nDaniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe, Shuai-\njiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Yujun Wang, Zhao You, and\nZhiyong Yan. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Tran-\nscribed Audio. arXiv e-prints, art. arXiv:2106.06909, June 2021.\nChristopher Cieri, David Miller, and Kevin Walker. The Fisher Corpus: a Resource for the Next\nGenerations of Speech-to-Text. In Proceedings of the Fourth International Conference on Lan-\nguage Resources and Evaluation (LREC\u201904), Lisbon, Portugal, May 2004a. European Language\nResources Association (ELRA).\nURL http://www.lrec-conf.org/proceedings/\nlrec2004/pdf/767.pdf.\n14\nChristopher Cieri et al. Fisher English Training Speech Part 1 Speech LDC2004S13. Web Down-\nload. Linguistic Data Consortium, 2004b.\nChristopher Cieri et al.\nFisher English Training Speech Part 1 Transcripts LDC2004T19. Web\nDownload. Linguistic Data Consortium, 2004c.\nChristopher Cieri et al. Fisher English Training Speech Part 2 Speech LDC2005S13. Web Down-\nload. Linguistic Data Consortium, 2005a.\nChristopher Cieri et al.\nFisher English Training Speech Part 2 Transcripts LDC2005T19. Web\nDownload. Linguistic Data Consortium, 2005b.\nAlexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason\nRiesa, Clara Rivera, and Ankur Bapna. FLEURS: Few-shot Learning Evaluation of Universal\nRepresentations of Speech. arXiv e-prints, art. arXiv:2205.12446, May 2022. doi: 10.48550/\narXiv.2205.12446.\nTri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. arXiv\ne-prints, art. arXiv:2307.08691, July 2023. doi: 10.48550/arXiv.2307.08691.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. FlashAttention: Fast and\nMemory-Efficient Exact Attention with IO-Awareness. arXiv e-prints, art. arXiv:2205.14135,\nMay 2022. doi: 10.48550/arXiv.2205.14135.\nMiguel Del Rio, Natalie Delworth, Ryan Westerman, Michelle Huang, Nishchal Bhandari, Joseph\nPalakapilly, Quinten McNamara, Joshua Dong, Piotr \u02d9Zelasko, and Miguel Jett\u00b4e. Earnings-21: A\nPractical Benchmark for ASR in the Wild. In Proc. Interspeech 2021, pp. 3465\u20133469, 2021. doi:\n10.21437/Interspeech.2021-1915.\nMiguel Del Rio, Peter Ha, Quinten McNamara, Corey Miller, and Shipra Chandra. Earnings-22: A\nPractical Benchmark for Accents in the Wild. arXiv e-prints, art. arXiv:2203.15591, March 2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Min-\nnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL\nhttps://aclanthology.org/N19-1423.\nMengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad Shokouhi, Xia Hu, and Ahmed Hassan\nAwadallah.\nRobustness Challenges in Model Distillation and Pruning for Natural Language\nUnderstanding. In Proceedings of the 17th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics, pp. 1766\u20131778, Dubrovnik, Croatia, May 2023. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.129. URL https:\n//aclanthology.org/2023.eacl-main.129.\nDaniel Galvez, Greg Diamos, Juan Torres, Keith Achorn, Juan Cer\u00b4on, Anjali Gopi, David\nKanter, Max Lam, Mark Mazumder, and Vijay Janapa Reddi.\nThe People\u2019s Speech:\nA Large-Scale Diverse English Speech Recognition Dataset for Commercial Usage.\nIn\nJ. Vanschoren and S. Yeung (eds.), Proceedings of the Neural Information Processing\nSystems Track on Datasets and Benchmarks, volume 1. Curran, 2021.\nURL https:\n//datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/\n2021/file/202cb962ac59075b964b07152d234b70-Paper-round1.pdf.\nSanchit Gandhi, Patrick von Platen, and Alexander M. Rush. ESB: A Benchmark For Multi-Domain\nEnd-to-End Speech Recognition. arXiv e-prints, art. arXiv:2210.13352, October 2022. doi: 10.\n48550/arXiv.2210.13352.\nJohn S. Garofolo et al. CSR-I (WSJ0) Complete LDC93S6A. Web Download. Linguistic Data\nConsortium, 1993.\n15\nRobert Geirhos, J\u00a8orn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A. Wichmann.\nShortcut learning in deep neural networks.\nNa-\nture Machine Intelligence, 2(11):665\u2013673, Nov 2020.\nISSN 2522-5839.\ndoi:\n10.1038/\ns42256-020-00257-z. URL https://doi.org/10.1038/s42256-020-00257-z.\nJ.J. Godfrey, E.C. Holliman, and J. McDaniel.\nSWITCHBOARD: telephone speech corpus for\nresearch and development. In [Proceedings] ICASSP-92: 1992 IEEE International Conference\non Acoustics, Speech, and Signal Processing, volume 1, pp. 517\u2013520 vol.1, 1992. doi: 10.1109/\nICASSP.1992.225858.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, San-\njana Krishnan, Marc\u2019Aurelio Ranzato, Francisco Guzman, and Angela Fan. The FLORES-101\nEvaluation Benchmark for Low-Resource and Multilingual Machine Translation. arXiv e-prints,\nart. arXiv:2106.03193, June 2021. doi: 10.48550/arXiv.2106.03193.\nAlex Graves, Santiago Fern\u00b4andez, Faustino Gomez, and J\u00a8urgen Schmidhuber. Connectionist Tem-\nporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks. In\nProceedings of the 23rd International Conference on Machine Learning, ICML \u201906, pp. 369\u2013376,\nNew York, NY, USA, 2006. Association for Computing Machinery. ISBN 1595933832. doi:\n10.1145/1143844.1143891. URL https://doi.org/10.1145/1143844.1143891.\nAndreas Griewank and Andrea Walther. Algorithm 799: Revolve: An Implementation of Check-\npointing for the Reverse or Adjoint Mode of Computational Differentiation.\nACM Trans.\nMath. Softw., 26(1):19\u201345, mar 2000. ISSN 0098-3500. doi: 10.1145/347837.347846. URL\nhttps://doi.org/10.1145/347837.347846.\nYanzhang He, Tara N. Sainath, Rohit Prabhavalkar, Ian McGraw, Raziel Alvarez, Ding Zhao, David\nRybach, Anjuli Kannan, Yonghui Wu, Ruoming Pang, Qiao Liang, Deepti Bhatia, Yuan Shang-\nguan, Bo Li, Golan Pundak, Khe Chai Sim, Tom Bagby, Shuo-yiin Chang, Kanishka Rao, and\nAlexander Gruenstein. Streaming End-to-end Speech Recognition For Mobile Devices. arXiv\ne-prints, art. arXiv:1811.06621, November 2018. doi: 10.48550/arXiv.1811.06621.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas\nSteiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL\nhttp://github.com/google/flax.\nFranc\u00b8ois Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko, and Yannick Est`eve.\nTED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adapta-\ntion. In Speech and Computer, pp. 198\u2013208. Springer International Publishing, 2018.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network. arXiv\ne-prints, art. arXiv:1503.02531, March 2015. doi: 10.48550/arXiv.1503.02531.\nJeremy Howard and Sebastian Ruder. Universal Language Model Fine-tuning for Text Classifica-\ntion. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 328\u2013339, Melbourne, Australia, July 2018. Association for Com-\nputational Linguistics. doi: 10.18653/v1/P18-1031. URL https://aclanthology.org/\nP18-1031.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed.\nHuBERT: Self-Supervised Speech Representation Learning by\nMasked Prediction of Hidden Units.\narXiv e-prints, art. arXiv:2106.07447, June 2021.\ndoi:\n10.48550/arXiv.2106.07447.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,\nHartwig Adam, and Dmitry Kalenichenko. Quantization and Training of Neural Networks for\nEfficient Integer-Arithmetic-Only Inference. arXiv e-prints, art. arXiv:1712.05877, December\n2017. doi: 10.48550/arXiv.1712.05877.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun\nLiu. TinyBERT: Distilling BERT for Natural Language Understanding. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020, pp. 4163\u20134174, Online, November 2020.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.372. URL\nhttps://aclanthology.org/2020.findings-emnlp.372.\n16\nNorman P. Jouppi, Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James Laudon, Cliff\nYoung, and David Patterson. A Domain-Specific Supercomputer for Training Deep Neural Net-\nworks. Commun. ACM, 63(7):67\u201378, jun 2020. ISSN 0001-0782. doi: 10.1145/3360307. URL\nhttps://doi.org/10.1145/3360307.\nYoon Kim and Alexander M. Rush. Sequence-Level Knowledge Distillation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural Language Processing, pp. 1317\u20131327, Austin,\nTexas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1139.\nURL https://aclanthology.org/D16-1139.\nS. Kullback and R. A. Leibler.\nOn information and sufficiency.\nThe Annals of Mathematical\nStatistics, 22(1):79\u201386, 1951. ISSN 00034851. URL http://www.jstor.org/stable/\n2236703.\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast Inference from Transformers via Spec-\nulative Decoding.\nIn Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engel-\nhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Con-\nference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,\npp. 19274\u201319286. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/\nv202/leviathan23a.html.\nQuentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen,\nSuraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario\n\u02c7Sa\u02c7sko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Can-\nwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Cl\u00b4ement\nDelangue, Th\u00b4eo Matussi`ere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer,\nVictor Mustar, Franc\u00b8ois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A Community\nLibrary for Natural Language Processing. In Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing: System Demonstrations, pp. 175\u2013184, Online and\nPunta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\nURL https://aclanthology.org/2021.emnlp-demo.21.\nZheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati, Parminder Bhatia, Andrew Arnold, Bing\nXiang, and Dan Roth. DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation\nand Quantization. In Proceedings of the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pp. 203\u2013211, Dublin, Ireland, May 2022. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.22. URL https:\n//aclanthology.org/2022.acl-short.22.\nLinguistic Data Consortium. 2000 HUB5 English Evaluation Transcripts LDC2002T43. Web Down-\nload. Linguistic Data Consortium, 2002.\nIlya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In International Con-\nference on Learning Representations, 2019.\nURL https://openreview.net/forum?\nid=Bkg6RiCqY7.\nMatthias Mauch and Sebastian Ewert. The Audio Degradation Toolbox and its Application to Ro-\nbustness Evaluation.\nIn Proceedings of the 14th International Society for Music Information\nRetrieval Conference (ISMIR 2013), Curitiba, Brazil, 2013. accepted.\nAbid Mohsin.\nPodcast transcription benchmark (part 1).\nhttps://www.rev.ai/blog/\npodcast-transcription-benchmark-part-1/, 2019. Accessed: 25 Oct., 2023.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\u2019t Give Me the Details, Just the Summary!\nTopic-Aware Convolutional Neural Networks for Extreme Summarization. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1797\u20131807,\nBrussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206.\nPatrick K. O\u2019Neill, Vitaly Lavrukhin, Somshubra Majumdar, Vahid Noroozi, Yuekai Zhang, Oleksii\nKuchaiev, Jagadeesh Balam, Yuliya Dovzhenko, Keenan Freyberg, Michael D. Shulman, Boris\n17\nGinsburg, Shinji Watanabe, and Georg Kucsko. SPGISpeech: 5,000 Hours of Transcribed Finan-\ncial Audio for Fully Formatted End-to-End Speech Recognition. In Proc. Interspeech 2021, pp.\n1434\u20131438, 2021. doi: 10.21437/Interspeech.2021-1860.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR\ncorpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 5206\u20135210, 2015.\ndoi: 10.1109/ICASSP.2015.\n7178964.\nRuoming Pang, Tara Sainath, Rohit Prabhavalkar, Suyog Gupta, Yonghui Wu, Shuyuan Zhang, and\nChung-Cheng Chiu. Compression of End-to-End Models. In Proc. Interspeech 2018, pp. 27\u201331,\n2018. doi: 10.21437/Interspeech.2018-1025.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the Difficulty of Training Recurrent Neu-\nral Networks. In Proceedings of the 30th International Conference on International Conference\non Machine Learning - Volume 28, ICML\u201913, pp. III\u20131310\u2013III\u20131318. JMLR.org, 2013.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance\nDeep Learning Library.\nIn H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc,\nE. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.\n8024\u20138035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\nNicolas Patry. Making automatic speech recognition work on large files with Wav2Vec2 in Trans-\nformers. https://huggingface.co/blog/asr-chunking, 2022. Accessed: 25 Oct.,\n2023.\nZilun Peng, Akshay Budhkar, Ilana Tuil, Jason Levy, Parinaz Sobhani, Raphael Cohen, and Jumana\nNassour. Shrinking Bigfoot: Reducing wav2vec 2.0 footprint. In Proceedings of the Second\nWorkshop on Simple and Efficient Natural Language Processing, pp. 134\u2013141, Virtual, November\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.sustainlp-1.14. URL\nhttps://aclanthology.org/2021.sustainlp-1.14.\nDaniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel,\nMirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer,\nand Karel Vesely. The Kaldi Speech Recognition Toolkit. In IEEE 2011 Workshop on Automatic\nSpeech Recognition and Understanding. IEEE Signal Processing Society, December 2011. IEEE\nCatalog No.: CFP11SRW-USB.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya\nSutskever. Robust Speech Recognition via Large-Scale Weak Supervision. arXiv e-prints, art.\narXiv:2212.04356, December 2022. doi: 10.48550/arXiv.2212.04356.\nSteve Renals, Thomas Hain, and Herve Bourlard. Recognition and understanding of meetings the\nAMI and AMIDA projects. In 2007 IEEE Workshop on Automatic Speech Recognition and Un-\nderstanding (ASRU), pp. 238\u2013247, 2007. doi: 10.1109/ASRU.2007.4430116.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv e-prints, art. arXiv:1910.01108, October\n2019. doi: 10.48550/arXiv.1910.01108.\nTal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran, Yi Tay, and\nDonald Metzler. Confident Adaptive Language Modeling. arXiv e-prints, art. arXiv:2207.07061,\nJuly 2022. doi: 10.48550/arXiv.2207.07061.\nAbigail See, Peter J. Liu, and Christopher D. Manning. Get To The Point: Summarization with\nPointer-Generator Networks. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 1073\u20131083, Vancouver, Canada, July\n2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1099. URL https:\n//aclanthology.org/P17-1099.\n18\nHang Shao, Wei Wang, Bei Liu, Xun Gong, Haoyu Wang, and Yanmin Qian. Whisper-KDQ: A\nLightweight Whisper via Guided Knowledge Distillation and Quantization for Efficient ASR.\narXiv e-prints, art. arXiv:2305.10788, May 2023. doi: 10.48550/arXiv.2305.10788.\nSam Shleifer and Alexander M. Rush. Pre-trained Summarization Distillation. arXiv e-prints, art.\narXiv:2010.13002, October 2020. doi: 10.48550/arXiv.2010.13002.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\nMo-\nbileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics, pp. 2158\u20132170, On-\nline, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.195.\nURL https://aclanthology.org/2020.acl-main.195.\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig\nSchmidt.\nMeasuring Robustness to Natural Distribution Shifts in Image Classification.\nIn\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-\nral Information Processing Systems, volume 33, pp. 18583\u201318599. Curran Associates, Inc.,\n2020.\nURL https://proceedings.neurips.cc/paper_files/paper/2020/\nfile/d8330f857a17c53d217014ee776bfd50-Paper.pdf.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141 ukasz Kaiser,\nand Illia Polosukhin.\nAttention is All you Need.\nIn I. Guyon,\nU. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-\nciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nEmmanuel Vincent, Shinji Watanabe, Aditya Arie Nugraha, Jon Barker, and Ricard Marxer. An\nAnalysis of Environment, Microphone and Data Simulation Mismatches in Robust Speech\nRecognition.\nComput. Speech Lang., 46(C):535\u2013557, nov 2017.\nISSN 0885-2308.\ndoi:\n10.1016/j.csl.2016.11.005. URL https://doi.org/10.1016/j.csl.2016.11.005.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?\nid=rJ4km2R5t7.\nChanghan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary\nWilliamson, Juan Pino, and Emmanuel Dupoux. VoxPopuli: A Large-Scale Multilingual Speech\nCorpus for Representation Learning, Semi-Supervised Learning and Interpretation. In Proceed-\nings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp.\n993\u20131003, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n2021.acl-long.80. URL https://aclanthology.org/2021.acl-long.80.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-Art Nat-\nural Language Processing.\nIn Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations, pp. 38\u201345, Online, October 2020. As-\nsociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\n2020.emnlp-demos.6.\nYu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V.\nLe, and Yonghui Wu. Pushing the Limits of Semi-Supervised Learning for Automatic Speech\nRecognition. arXiv e-prints, art. arXiv:2010.10504, October 2020. doi: 10.48550/arXiv.2010.\n10504.\nYu Zhang, Daniel S. Park, Wei Han, James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong\nXu, Yanping Huang, Shibo Wang, Zongwei Zhou, Bo Li, Min Ma, William Chan, Jiahui Yu,\n19\nYongqiang Wang, Liangliang Cao, Khe Chai Sim, Bhuvana Ramabhadran, Tara N. Sainath,\nFranc\u00b8oise Beaufays, Zhifeng Chen, Quoc V. Le, Chung-Cheng Chiu, Ruoming Pang, and Yonghui\nWu. BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic\nSpeech Recognition. IEEE Journal of Selected Topics in Signal Processing, 16(6):1519\u20131532,\n2022. doi: 10.1109/JSTSP.2022.3182537.\nYu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen,\nBo Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prab-\nhavalkar, Daniel S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor\nStrohman, Bhuvana Ramabhadran, Tara Sainath, Pedro Moreno, Chung-Cheng Chiu, Johan\nSchalkwyk, Franc\u00b8oise Beaufays, and Yonghui Wu. Google USM: Scaling Automatic Speech\nRecognition Beyond 100 Languages. arXiv e-prints, art. arXiv:2303.01037, March 2023. doi:\n10.48550/arXiv.2303.01037.\nYongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh,\nSanjiv Kumar, Jean-Franc\u00b8ois Kagy, and Rishabh Agarwal. DistillSpec: Improving Speculative\nDecoding via Knowledge Distillation. arXiv e-prints, art. arXiv:2310.08461, October 2023. doi:\n10.48550/arXiv.2310.08461.\nA\nADDITIONAL DATASET DETAILS\nA.1\nTRAINING DATA\nQuantitative and qualitative information about the training datasets is displayed in Tables 13 and 14\nrespectively. A detailed description of the training datasets is presented below.\nPeople\u2019s Speech (Galvez et al., 2021) is a large-scale English speech recognition dataset which\nassembles audio-transcription pairs sourced from the internet. The dataset covers multiple sources,\nincluding interviews, radio and finance. We use the \u201cclean\u201d subset of the dataset, with approximately\n12,000 hours of training data and corresponding validation and test splits.\nGigaSpeech (Chen et al., 2021) is a multi-domain speech recognition corpus curated from audio-\nbooks, podcasts and YouTube. It contains both narrated and spontaneous speech over a range of\ncontent material, including science, arts and sports. We use the large subset (2,500 hours) to train\nand the standard validation and test splits.\nCommon Voice (Ardila et al., 2020) is a collection of open-license, crowd-source speech datasets\nwhere contributors record themselves narrating text from Wikipedia in various languages. Given its\ncrowd-sourced approach, the dataset exhibits significant diversity in audio quality and speakers. The\nrecorded audio often contains challenges such as background noise, accent variations, hesitations,\nand incorporation of non-native words. We use the English subset of version 13.0 (16-3-2023), with\napproximately 2,400 hours and the canonical data splits.\nTable 13: Quantitative statistics of the training datasets. The mean audio length is quoted in\nseconds, and the mean transcription length in number of words.\nDataset\nSize / h\nSpeakers\nMean Audio (s)\nMean Text (words)\nPeople\u2019s Speech\n12,000\nunknown\n13.8\n38.5\nCommon Voice 13\n3,000\nunknown\n5.6\n31.9\nGigaSpeech\n2,500\nunknown\n4.0\n12.9\nFisher\n1,960\n11,900\n3.3\n10.1\nLibriSpeech\n960\n2,480\n12.1\n32.9\nVoxPopuli\n540\n1,310\n10.3\n26.1\nTED-LIUM\n450\n2,030\n6.1\n18.3\nSwitchBoard\n260\n540\n4.8\n8.3\nAMI\n100\nunknown\n2.6\n7.3\nTotal\n21,770\n18,260+\n7.1\n19.8\n20\nTable 14: Qualitative statistics of the training datasets. The speaking styles are narrated (N),\noratory (O) or spontaneous (S), or a combination of them.\nDataset\nDomain\nRec. Cond.\nStyle\nLicence\nPeople\u2019s Speech\nInternet Archive\nClose-talk mic.\nN, O, S\nCC-BY-SA-4.0\nCommon Voice 13\nNarrated Wikipedia\nClose-talk mic.\nN\nCC0-1.0\nGigaSpeech\nAudiobook, podcast, YouTube\nClose-talk mic.\nN, S\napache-2.0\nFisher\nTelephone conversations\nTelephone\nS\nLDC\nLibriSpeech\nAudiobooks\nClose-talk mic.\nN\nCC-BY-4.0\nVoxPopuli\nEuropean Parliament\nClose-talk mic.\nO\nCC0\nTED-LIUM\nTED talks\nClose-talk mic.\nO\nCC-BY-NC-ND 3.0\nSwitchBoard\nTelephone conversations\nTelephone\nS\nLDC\nAMI\nMeetings\nHeadset\nS\nCC-BY-4.0\nFisher (Cieri et al., 2004a) is a corpus of two-sided conversational telephone calls amongst speakers\nfrom the United States. We combine Part 1 (Cieri et al., 2004b;c) and Part 2 (Cieri et al., 2005a;b)\nof the dataset to give 1,960 hours of training data.\nLibriSpeech (Panayotov et al., 2015) is a standard dataset for training and evaluating academic\nspeech models. It is comprised of 960 hours of narrated audiobooks sourced from the LibriVox2\nproject. The audiobook domain provides high-quality recording conditions, with little to no back-\nground noise. We use the standard split of train, validation (dev-clean, dev-other) and test sets\n(test-clean, test-other).\nVoxPopuli (Wang et al., 2021) is a large-scale multilingual speech datasets consisting of European\nParliament event recordings from 2009-2020. The speech is oratory and from the political domain,\nwith mostly non-native speakers. We use the English subset with approximately 550 hours and\ndataset splits provided therein.\nTED-LIUM (Hernandez et al., 2018) is a collection of English-language TED Talk conference\nvideos. The talks span a variety of cultural, political, and academic themes. We use the Release 3\nedition of the training set with approximately 450 hours and the legacy distribution of validation and\ntest data.\nSwitchBoard (Godfrey et al., 1992) is a 260 hour corpus of two-sided conversational telephone calls\namongst speakers from the United States. We partition 5% of the SwitchBoard corpus to form the\nvalidation split The test sets are the Hub5Eval2000 (Linguistic Data Consortium, 2002) data with\ntwo subsets: SwitchBoard and CallHome.\nAMI (Carletta, 2007; Renals et al., 2007) consists of 100 hours of meeting recordings captured using\nmultiple recording streams in parallel. The corpus is manually annotated to provide the ground truth\ntranscriptions. Individual samples of the AMI dataset contain very large audio files between 10\nand 60 minutes in duration. We segment the audio samples according the the Kaldi (Povey et al.,\n2011) recipe for AMI3 to yield utterance of suitable length for training ASR systems. This involves\nsplitting samples longer than 30 words at the time-stamps for punctuation to yield shorter utterances.\nWe use the individual headset microphone (AMI IHM) and single distant microphone (AMI SDM)\nversions of the dataset, with the train, validation and test sets provided therein.\nA.2\nSHORT-FORM EVALUATION DATA\nA detailed description of the short-form evaluation datasets is provided below.\nCHiME-4 (Vincent et al., 2017) comprises of narrated samples from the Wall Street Journal corpus\n(Garofolo et al., 1993). Recordings are performed in noisy environments using a 6-channel tablet\n2LibriVox: https://librivox.org/\n3AMI Kaldi recipe:\nhttps://github.com/kaldi-asr/kaldi/tree/master/egs/ami/\ns5b\n21\nbased microphone array. We use the official 1-channel validation and test sets for evaluating our\nmodels.\nEarnings-22 (Del Rio et al., 2022) is a 119-hour test set of earnings calls recorded by global com-\npanies. The dataset was developed with the intention of assembling a diverse range of speakers and\naccents speaking in the context of real-world financial meetings.\nThe Earnings-22 dataset contains audio recordings upwards of 10-minutes in duration. To create\na short-form evaluation dataset, we segment these files into shorter samples up to 20-seconds in\nlength. We first predict timestamps for the long audio files using the official wav2vec 2.0 base +\n4-gram model (Baevski et al., 2020) fine-tuned on the LibriSpeech (Baevski et al., 2020) dataset.\nWe then split samples at the predicted timestamps for punctuation. If the samples are still longer\nthan 20-seconds, we split them again at the longest silence in the utterance.\nFLEURS (Few-shot Learning Evaluation of Universal Representations of Speech) (Conneau et al.,\n2022) is a small-scale corpus for evaluating speech recognition systems in 102 languages. The\ntranscription data is taken from the FLoRes-101 dataset (Goyal et al., 2021), a machine translation\ncorpus with 3001 samples of English text each translated to 101 other languages. To assemble the\nFLEURS dataset, up to three native speakers are recorded narrating the sentence translations in their\nnative language. The recorded audio data is paired with the sentence transcriptions, thus yielding\na multilingual speech recognition corpus. We use the English-US (en us) subset with 1 hour of\nvalidation and 2 hours of test data.\nSPGISpeech (O\u2019Neill et al., 2021) is an English speech recognition dataset comprising of company\nearnings calls that have been manually transcribed by S&P Global, Inc. We evaluate our models on\nthe official validation and test splits, each of which is 100 hours.\nA.3\nLONG-FORM EVALUATION DATA\nA detailed description of the long-form evaluation datasets is presented below.\nEarnings-21 (Del Rio et al., 2021) is a 39-hour corpus of company earnings calls over various\nfinancial sections.\nEarnings-22 (Del Rio et al., 2022) is a 119-hour test set of earnings calls recorded by global com-\npanies. The dataset was developed with the intention of assembling a diverse range of speakers and\naccents speaking in the context of real-world financial meetings.\nMeanwhile (Radford et al., 2022) is a collection of 64 segments taken from The Late Show with\nStephen Colbert. The transcriptions are taken from the closed-caption data for each video and cor-\nrected with manual inspection. The YouTube URLs are provided by the Whisper authors, along with\nthe segment start-end times4.\nRev 16 (Mohsin, 2019) is a set of 30 podcast recordings that are commonly used to benchmark ASR\nsystems in production settings. We follow the Whisper authors in evaluating on a subset of 16 of the\n30 files with IDs:\n3 4 9 10 11 14 17 18 20 21 23 24 26 27 29 32\nB\nEXPERIMENTAL SET-UP\nB.1\nTRAINING\nWe train the models using the JAX and Flax neural network libraries (Bradbury et al., 2018; Heek\net al., 2020). We use data parallelism across TPU v4-8 accelerators (Jouppi et al., 2020), with\nbfloat16 precision and gradient checkpointing (Griewank & Walther, 2000). We use a softmax\ntemperature of 2.0 to smooth the distributions of the student and teacher models when computing\nthe KL divergence loss (Hinton et al., 2015). Models are trained with an Optax implementation\nof the AdamW optimiser (Babuschkin et al., 2020; Loshchilov & Hutter, 2019) and gradient norm\n4Meanwhile\nmetadata:\nhttps://github.com/openai/whisper/blob/main/data/\nmeanwhile.json\n22\nclipping (Pascanu et al., 2013). We train for a total of 80,000 optimisation steps, equivalent to eight\nepochs of training. We use the slanted triangular learning rate (STLR) (Howard & Ruder, 2018)\nschedule, linearly increasing the learning rate from zero to a maximum of 1e-4 over the first 500\nsteps, and then linearly decaying it to zero. If the student encoder is the same as the teacher encoder,\nwe freeze its parameters and only run it forward once. Back propagation is then only run through\nthe distilled decoder. Table 15 summarises the training hyperparemters.\nTable 15: Distil-Whisper training hyperparameters.\nHyperparameter\nValue\nDevice\nTPU v4-8\nUpdates\n80,000\nBatch size\n256\nWarmup steps\n500\nLR schedule\nLinear decay\nPrecision\nbfloat16\nKL softmax temperature\n2.0\nMax grad norm\n1.0\nOptimizer\nAdamW\n\u03b21\n0.9\n\u03b22\n0.999\n\u03f5\n10-8\nWeight decay\n0.0\nTimestamp probability\n50%\nB.2\nEVALUATION\nWe evaluate all models in JAX on TPU v4-8 with greedy decoding unless specified otherwise. We\nnormalise text using the Whisper English normaliser (Radford et al., 2022), which standardises text\nby removing or converting specific words, symbols, numeric expressions, and managing whitespace\nand spellings, in an attempt to only penalise a system when an error is caused by actually mistran-\nscribing a word, and not by formatting differences. We measure transcription accuracy using the\nWER metric.\nDuring training, we evaluate the intermediate checkpoints every 5k training steps on the 13 vali-\ndation sets. We select the checkpoint with the best macro-average performance over the validation\nsplits for final evaluation on the test splits.\nFor latency measurements, we evaluate the models in PyTorch (Paszke et al., 2019) using a single\nA100 40GB GPU in float16 precision. Specifically, we measure the total time taken to decode 256\nsamples from each of the four OOD test sets over batch sizes in the set {1, 4, 16}. Batch size 1\nlatency corresponds to short-form evaluation, where the models are evaluated without timestamp\nprediction. Batch sizes 4 and 16 correspond to long-form transcription, where the chunked long-\nform transcription algorithm is used. The Whisper models are evaluated with timestamp prediction\nand the Distil-Whisper models without. These configurations resulted in the best WER scores on\nthe TED-LIUM long-form validation set.\nUsing the inference speed measurements, we compute the ratio of the inference time of Distil-\nWhisper to the Whisper large-v2 checkpoint, giving a figure for relative latency. We record all\nlatency measurements using Flash Attention 2 (Dao, 2023), since it is a general inference optimisa-\ntion for modern GPU hardware in production. In Section D.5, we show the effect of Flash Attention\n2 on the latency of Whisper and Distil-Whisper.\n23\nC\nEVALUATION RESULTS\nTable 16: Per-dataset WER scores over the 15 short-form test sets. The macro-average WER scores\nare shown for the 11 ID datasets, four OOD datasets, and an overall average over all 15 test sets.\nDataset\ntiny.en\nbase.en\nsmall.en\nmedium.en\nlarge-v2\ndistil-medium.en\ndistil-large-v2\nAMI IHM\n22.9\n19.9\n17.4\n16.4\n16.9\n16.1\n14.7\nAMI SDM\n50.0\n45.2\n38.1\n37.0\n36.5\n35.7\n33.9\nCall Home\n23.8\n20.3\n19.0\n16.0\n17.5\n15.1\n13.5\nCommon Voice 13\n28.9\n21.4\n15.3\n12.3\n10.4\n15.3\n12.9\nGigaSpeech\n13.5\n12.1\n11.0\n10.8\n10.7\n11.2\n10.5\nLibriSpeech clean\n5.9\n4.4\n3.3\n3.1\n3.2\n3.9\n3.6\nLibriSpeech other\n14.1\n10.4\n7.4\n6.1\n5.6\n8.0\n6.9\nPeople\u2019s Speech\n26.4\n22.2\n19.3\n18.6\n18.6\n18.4\n16.5\nSwitchBoard\n17.7\n15.6\n15.3\n14.0\n14.2\n11.7\n11.2\nTED-LIUM\n11.8\n10.9\n10.1\n11.5\n12.0\n10.1\n9.6\nVoxpopuli\n11.3\n9.6\n8.3\n7.9\n7.3\n8.8\n8.0\nCHIME-4\n32.7\n24.1\n15.7\n12.7\n11.8\n15.1\n14.0\nEarnings-22\n25.8\n21.2\n17.9\n17.0\n16.6\n18.4\n16.9\nFLEURS\n11.2\n7.5\n5.9\n4.9\n4.2\n6.9\n6.3\nSPGISpeech\n5.8\n4.2\n3.6\n3.4\n3.8\n3.8\n3.3\nID Average\n20.6\n17.5\n15.0\n14.0\n13.9\n14.0\n12.8\nOOD Average\n18.9\n14.3\n10.8\n9.5\n9.1\n11.1\n10.1\nAverage\n20.1\n16.6\n13.8\n12.8\n12.6\n13.2\n12.1\nTable 17: Per-dataset WER scores over the five long-form test sets. The macro-average WER scores\nare shown for the one ID dataset, four OOD datasets, and an overall average over all five test sets.\nDataset\ntiny.en\nbase.en\nsmall.en\nmedium.en\nlarge-v2\ndistil-medium.en\ndistil-large-v2\nTED-LIUM\n6.4\n5.6\n5.8\n4.3\n4.4\n3.8\n3.7\nEarnings 21\n17.5\n14.5\n15.1\n12.3\n11.8\n11.6\n11.2\nEarnings 22\n24.1\n19.4\n20.6\n15.6\n15.1\n16.3\n15.1\nMeanwhile\n16.4\n13.4\n8.7\n7.9\n6.3\n8.9\n7.8\nRev 16\n17.4\n15.4\n14.5\n13.2\n13.6\n13.0\n12.2\nID Average\n6.4\n5.6\n5.8\n4.3\n4.4\n3.8\n3.7\nOOD Average\n18.9\n15.7\n14.7\n12.3\n11.7\n12.4\n11.6\nAverage\n16.4\n13.7\n12.9\n10.7\n10.2\n10.7\n10.0\n24\nTable 18: Per-dataset WER scores for the sequential and chunked long-form transcription algo-\nrithms. The macro-average WER scores are shown for the one ID dataset, four OOD datasets, and\nan overall average over all five test sets.\nDataset\nlarge-v2\nlarge-v2\ndistil-large-v2\nsequential\nchunked\nchunked\nTED-LIUM\n4.0\n4.4\n3.7\nEarnings 21\n10.7\n11.8\n11.2\nEarnings 22\n14.0\n15.1\n15.1\nMeanwhile\n5.2\n6.3\n7.8\nRev 16\n11.7\n13.6\n12.2\nID Average\n4.0\n4.4\n3.7\nOOD Average\n10.4\n11.7\n11.6\nAverage\n9.1\n10.2\n10.0\nTable 19: Per-dataset repeated 5-gram word duplicates (5-Dup.), insertion error rate (IER), substi-\ntution error rate (SER), deletion error rate (DER) and word error rate (WER) for the five long-form\ndatasets. An average is shown for the ID dataset (TED-LIUM), the four OOD datasets, and an\noverall average.\nDataset\nMetric\nwav2vec2-large-960h\ntiny.en\nbase.en\nsmall.en\nmedium.en\nlarge-v2\ndistil-medium.en\ndistil-large-v2\nTED-LIUM\n5-Dup.\n157\n522\n557\n549\n452\n542\n283\n270\nIER\n1.7\n2.1\n2.1\n1.8\n1.4\n1.8\n0.6\n0.5\nSER\n6.0\n2.2\n1.6\n1.2\n1.0\n0.9\n1.4\n1.3\nDER\n1.9\n2.2\n1.9\n2.7\n2.0\n1.8\n1.8\n1.8\nWER\n9.6\n6.4\n5.6\n5.8\n4.3\n4.4\n3.8\n3.7\nEarnings-21\n5-Dup.\n7938\n19294\n19629\n20611\n21014\n21559\n16912\n16797\nIER\n5.2\n4.1\n3.2\n3.0\n3.0\n3.0\n2.0\n1.7\nSER\n20.9\n8.2\n6.0\n4.6\n4.0\n3.9\n5.0\n4.7\nDER\n4.3\n5.3\n5.3\n7.5\n5.3\n4.9\n4.5\n4.7\nWER\n30.4\n17.5\n14.5\n15.1\n12.3\n11.8\n11.6\n11.2\nEarnings-22\n5-Dup.\n20869\n65599\n63041\n77122\n64977\n65419\n52475\n50949\nIER\n8.5\n6.5\n4.8\n5.3\n3.8\n3.9\n3.7\n3.0\nSER\n26.8\n11.6\n8.5\n6.9\n5.9\n5.5\n7.3\n6.7\nDER\n4.8\n6.0\n6.0\n8.4\n6.0\n5.7\n5.3\n5.4\nWER\n40.1\n24.1\n19.4\n20.6\n15.6\n15.1\n16.3\n15.1\nMeanwhile\n5-Dup.\n858\n1379\n1406\n1292\n1485\n1464\n1236\n1225\nIER\n1.5\n5.7\n5.2\n1.4\n3.6\n3.0\n1.4\n1.0\nSER\n11.9\n9.1\n6.7\n4.2\n3.2\n2.4\n5.5\n5.4\nDER\n3.0\n1.6\n1.5\n3.1\n1.0\n0.9\n2.1\n1.4\nWER\n16.4\n16.4\n13.4\n8.7\n7.9\n6.3\n8.9\n7.8\nRev 16\n5-Dup.\n2220\n6981\n6800\n6483\n6719\n6724\n5047\n5040\nIER\n4.2\n4.2\n3.8\n3.3\n3.4\n3.5\n2.8\n2.7\nSER\n16.1\n6.8\n5.3\n4.2\n3.8\n3.7\n4.6\n4.2\nDER\n6.1\n6.4\n6.3\n7.0\n6.0\n6.4\n5.6\n5.3\nWER\n26.4\n17.4\n15.4\n14.5\n13.2\n13.6\n13.0\n12.2\nID Average\n5-Dup.\n157\n587\n671\n548\n574\n752\n281\n270\nIER\n1.7\n2.4\n2.6\n1.9\n1.9\n2.7\n0.6\n0.5\nSER\n6.0\n2.1\n1.6\n1.1\n1.0\n0.9\n1.4\n1.3\nDER\n1.9\n2.2\n1.9\n2.0\n2.0\n1.7\n1.8\n1.8\nWER\n9.6\n6.8\n6.0\n4.9\n4.8\n5.3\n3.8\n3.7\nOOD Average\n5-Dup.\n7971\n23313\n22719\n26377\n23549\n23792\n18918\n18503\nIER\n4.8\n5.1\n4.3\n3.3\n3.5\n3.3\n2.5\n2.1\nSER\n18.9\n8.9\n6.6\n5.0\n4.2\n3.9\n5.6\n5.3\nDER\n4.6\n4.8\n4.8\n6.5\n4.6\n4.5\n4.4\n4.2\nWER\n28.3\n18.9\n15.7\n14.7\n12.3\n11.7\n12.4\n11.6\nAverage\n5-Dup.\n6408\n18755\n18287\n21211\n18929\n19142\n15191\n14856\nIER\n4.2\n4.5\n3.8\n3.0\n3.0\n3.0\n2.1\n1.8\nSER\n16.4\n7.6\n5.6\n4.2\n3.6\n3.3\n4.8\n4.5\nDER\n4.0\n4.3\n4.2\n5.7\n4.1\n4.0\n3.8\n3.7\nWER\n24.6\n16.4\n13.7\n12.9\n10.7\n10.2\n10.7\n10.0\n25\nD\nADDITIONAL ANALYSIS\nD.1\nEARLY EXIT\nEarly exit is a paradigm for dynamically controlling the number of decoder layers used at inference\ntime. It is based on the reasoning that the same amount of computation may not be required for\nevery input to achieve adequate performance, depending on whether the input is easy or hard.\nInstead of making a prediction based on the hidden-representation of the final decoder layer, early\nexiting makes a prediction based on some intermediate layer. For each decoder layer l, we compute\na confidence score ci[l] for the i-th token. We also define an early-exit threshold \u03b1i[l]. If our\nconfidence score exceeds this threshold (ci[l] > \u03b1i[l]), we exit early and greedily predict the most\nprobably token. Otherwise, we continue to the next layer and repeat.\nConfident Adaptive Language Modeling (CALM) (Schuster et al., 2022) proposes using a softmax\ndifference as the confidence score. The decoder hidden-state for the l-th layer dl\ni is mapped to the\nlogit space using the word-embedding matrix W . We then take the softmax of these logits to get\nthe token probabilities from the i-th decoder layer:\nP\n\u0000yi|y<i, H1:M, dl\ni\n\u0001\n= softmax\n\u0000W dl\ni\n\u0001\n(8)\nThe confidence score ci[l] is defined as the difference between the top-2 most probable predictions.\nIf this difference is greater than the threshold \u03b1i[l], the model is confident of its predictions, and we\ncan terminate decoding early.\nTo gauge how many decoder layers can be skipped with early exit, we benchmarked the performance\nof the Whisper medium.en model on 100 samples form the LibriSpeech test-clean dataset. As the\ndataset with the lowest WER performance on short-form evaluation (see Table 16), it provides an\nupper-bound for the number of decoder layers that can be skipped, since the model should be most\nconfident. We attempted setting the early-exit threshold automatically using the textual consistency\nformulation from CALM, which guarantees that the model will perform to within a certain tolerance\nof the full model with specified probability, but found it skipped close to zero layers for almost all\nexamples. Instead, we swept over a set of values of the threshold, recording the WER performance\nand number of decoder layers used.\nTable 20 shows the average number of decoder layers utilised by the medium.en model as the early-\nexit threshold is reduced. The medium.en model has a total of 24 decoder layers, of which the last\n3 are skipped almost immediately. However, the WER penalty is significant even for just a 3-layer\nreduction. As we reduce the threshold, the number of layers skipped does not reduce significantly,\nbut the WER penalty continues to inflate. Setting the threshold to 0.9750 results in an average of\n3 skipped decoder layers, yielding an inference speed-up of 1.1 times. However, it also causes an\nincrease in WER from 2.3% to 3.4% This suggests that there is high-utilisation of the first 21 decoder\nlayers in the pre-trained Whisper model, and that the final 3 layers are necessary for ensuring high\ntranscription accuracy. We leave finding effective early exit schemes for Seq2Seq ASR models as\nfuture work.\nD.2\nDISTILLATION OBJECTIVE\nThe knowledge distillation (KD) objective proposed in Section 4.1 is a weighted average of the\nKullback-Leibler (KL) divergence and pseudo-label (PL) terms:\nLKD = \u03b1KLLKL + \u03b1P LLP L\n(9)\nThe typical setting in layer-based compression is that the dimensionality of the student model\nmatches that of the teacher model. This means the student and teacher layers output the same\nshape of hidden-states. Thus, we can introduce a mean-square error (MSE) term to encourage the\nstudent\u2019s hidden layer outputs to match those of the teacher:\n26\nTable 20: Early-exit performance. WER on 100 examples from the LibriSpeech test-clean dataset\nas the early-exit threshold is varied. The latency results are computed relative to the medium.en\nmodel with full utilisation of the 24 decoder layers.\nThreshold\nAvg. Dec Layers\nRel. Latency\nWER\n1.0000\n24.0\n1.0\n2.3\n0.9875\n21.2\n1.1\n2.8\n0.9750\n21.0\n1.1\n3.4\n0.9625\n20.8\n1.1\n3.5\n0.9500\n20.7\n1.2\n3.6\n0.9375\n20.6\n1.2\n3.7\n0.9250\n20.5\n1.2\n4.3\nLMSE =\nN\nX\ni=1\nL\u2032\nX\nl=1\nMSE\n\u0010\nHS\nl , HT\n\u03d5(l)\n\u0011\n(10)\nwhere HS\nl is the hidden-state output from the l-th layer of the student model S, \u03d5 (l) maps the l-th\nstudent layer to the corresponding teacher layer it is trained to emulate, and HT\n\u03d5(l) is the hidden-\nstate output from layer \u03d5 (l) of the teacher model T. The mapping \u03d5 follows the settings from\nShleifer & Rush (2020), where it is selected such that each decoder layer is trained to behave like\nmaximally spaced teacher layers. For example, given a 2-layer student model initialised from a\n32-layer teacher model, we choose pairings in \u03d5 such that each student decoder layer is taught to\nbehave like 16 teacher layers. Thus, student layer 1\u2019s hidden-states are paired to teacher layer 16,\nand student layer 2\u2019s hidden-states paired to teacher layer 32:\n\u03d5(l) =\n\u001a16\nif l = 1\n32\nif l = 2\n(11)\nA more general KD training objective is then a weighted sum of the KL, PL and MSE terms:\nLKD = \u03b1KLLKL + \u03b1P LLP L + \u03b1MSELMSE\n(12)\nwhere \u03b1KL, \u03b1P L and \u03b1MSE are scalar weights for the KL, PL and MSE loss terms respectively.\nFollowing (Shleifer & Rush, 2020), we set \u03b1KL = 0.8 and \u03b1P L = 1.0, and tune the value of \u03b1MSE\non our validation set.\nTo quantify the performance gain obtained by incorporating each KD term, we train distil-large-v2\ncheckpoints for 10,000 training steps (two epochs) on a three combinations of KD objectives: (i)\nPL, (ii) PL + KD, and (iii) PL + KD + MSE. Training on PL alone is equivalent to shrink and\nfine-tune (SFT), but with the ground truth labels replaced by the PL generated ones. In all cases,\nWhisper-generated pseudo-labels are used as the ground truth labels during training.\nTable 21 displays the average WER across the 11 short-form in-distribution (ID) validation sets and\nthe three out-of-distribution (OOD) validation sets for each KD combination. The addition of the\nKL-divergence term yields an OOD word error rate (WER) that is 0.3% absolute lower compared\nto just PL. This suggests that the additional information transferred from the teacher to the student\nduring KD is beneficial over training on PL alone. Incorporating the MSE loss term had a negligible\neffect on the average WER performance of the distilled model. This indicates that there is sufficient\ntraining signal from the PL and KL loss terms. The MSE loss requires that the hidden-states for\neach layer are recorded and kept in memory. This added a significant overhead when training the\nmodel in JAX, which resulted in a decrease to the maximum possible batch size. By only using\nthe PL and KL objectives and training with a higher throughput, we achieved better results within a\nspecified time interval compared to using the MSE loss, and thus opted for this configuration for our\n27\nTable 21: Impact of the distillation objective. Average WER of the distil-large-v2 checkpoint over\nthe 11 ID and three OOD validation sets for the three possible training objectives: pseudo-labels\n(PL), KL-divergence (KL) and mean-square error (MSE).\nObjective\nAvg. ID WER\nAvg. OOD WER\nAvg. WER\nPL\n12.8\n7.7\n11.6\nPL + KL\n12.6\n7.4\n11.4\nPL + KL + MSE\n12.6\n7.3\n11.4\nTable 22: Impact of speculative decoding with batching. Relative latency of the medium.en and\nlarge-v2 models using either the tiny Whisper or Distil-Whisper assistant models. The assistant\nmodels used are shown below the main ones. The relative latency is computed relative to the large-\nv2 model without speculative decoding for each batch size.\nModel\nParams / M\nBatch Size\n1\n4\n16\nmedium.en\n769\n1.4\n1.3\n1.5\nw\\ tiny.en\n808\n2.7\n1.8\n1.2\nw\\ distil-medium.en\n856\n3.3\n2.2\n1.3\nlarge-v2\n1550\n1.0\n1.0\n1.0\nw\\ tiny\n1589\n2.1\n1.3\n0.8\nw\\ distil-large-v2\n1669\n2.0\n1.3\n0.8\nexperiments. Therefore, our final KD training objective is a weighted sum of the PL and KL terms\nonly.\nD.3\nBATCHED SPECULATIVE DECODING\nTable 22 reports the relative latency of the medium.en and large-v2 models both with and with-\nout speculative decoding at various batch sizes. For a batch size of 1, speculative decoding with\nthe distil-large-v2 assistant yields a 2.0 times increase to inference speed over the large-v2 alone.\nThis speed-up is comparable to using the tiny model as the assistant. For the medium.en model,\nusing distil-medium.en as the assistant provides a 2.4 times speed-up. This outperforms the tiny.en\nassistant, which is only 2.0 times faster. A similar trend holds for a batch size of 4, albeit with\nsmaller improvements to relative latency. For a batch size of 16, speculative decoding performs\nworse than using the main model by itself. For batched speculative decoding, all candidate tokens in\nacross the batch must match the validation tokens for the tokens to be accepted. If any token in the\nbatch at a given position does not agree, all candidate tokens that precede the position are discarded.\nConsequently, speculative decoding favours lower batch sizes, where it provides significant latency\nimprovements while ensuring the same outputs as the original model.\nD.4\nSTRATEGIES FOR RELIABLE LONG-FORM TRANSCRIPTION\nTranscribing long-form audio relies on the accurate prediction of multiple chunks of audio in paral-\nlel. Since long-form audio typically contains instances of long pauses between spoken utterances,\nthe Whisper model has a higher propensity to hallucinate compared to short-form audio. To combat\nthis, the hyper-parameters of the chunked long-form transcription algorithm can be optimised to\nhelp avoid failure cases. These tuned hyper-parameters are applied in the long-form transcription\nresults reported in Section 8.2.\nTable 23 shows the WER performance on the long-form TED-LIUM validation set as the chunk\nlength of the audio segments is decreased for the large-v2 and distil-large-v2 models. For Whisper,\na chunk length of 30-seconds is optimal, whereas Distil-Whisper performs best with a chunk length\nof 15-seconds, giving the best overall performance on the validation set with 4.1% WER. Whisper\n28\nTable 23: Effect of chunk length on the chunked long-form algorithm. WER performance on the\nlong-form TED-LIUM validation set as the chunk length of the long-form transcription algorithm is\nreduced.\nChunk Length / s\nlarge-v2\ndistil-large-v2\n30\n4.8\n7.4\n25\n5.3\n5.7\n20\n6.5\n5.0\n15\n6.5\n4.2\n10\n10.0\n4.3\nis pre-trained on 30-seconds audio samples, whereas the mean sample length in the Distil-Whisper\ntraining set is 7.1-seconds (see Appendix A.1 for details). This suggests that the chunk length should\nbe selected based on the distribution of audio data the model is trained on.\nIt is worth noting that the long-form WERs of Whisper and Distil-Whisper on the TED-LIUM val-\nidation set are 2.4% and 3.0% WER lower than their short-form performance on the same dataset.\nThis performance gain demonstrates that the long-form algorithm used by Distil-Whisper is an ef-\nfective approach for transcribing long audio files with strong WER performance.\nD.5\nFLASH ATTENTION 2\nFlash Attention (FA) (Dao et al., 2022) addresses the slow and memory-intensive nature of trans-\nformers models on long sequences by proposing an IO-aware exact attention algorithm. It uses tiling\nto minimise memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip\nmemory (SRAM), resulting in faster inference and the ability to handle longer sequence lengths with\nimproved model performance. Flash Attention 2 (FA2) (Dao, 2023) further refines this approach,\noptimising GPU work partitioning to achieve even greater computational efficiency.\nTo demonstrate the effect of FA2 on the Whisper and Distil-Whisper models, we benchmark the\ninference speed over 256 examples from each of the four OOD test sets. We report the latency as\nthe real-time factor (RTF), defined as the ratio of the inference time to the audio duration. In or-\nder to generalise to multiple hardware, we report the inference time on a 40GB A100 GPU, which\nis typically used in industry, as well as a 16GB T4 GPU, a typical consumer-grade GPU. The re-\nsults serve as look-up tables for practitioners wishing to determine the best trade-off between WER\nperformance and latency for various batch sizes and hardware.\nTable 24 reports the RTF for batch sizes 1, 4 and 16 on a 40GB A100 GPU. For the base attention\nimplementation, distil-large-v2 is faster than base.en at batch sizes 1 and 4, and marginally slower\nat batch size 16. It remains faster than small.en at batch size 16, with an average OOD WER\nthat is 0.7% lower. For all batch sizes, distil-large-v2 is at least 3.3 times faster than large-v2,\nwhile performing to within 1% WER. This highlights that distil-large-v2 can be used as a drop-in\nreplacement for base.en at low batch sizes and small.en at higher ones, with 3.3% and 0.7% WER\nimprovements respectively. Similarly, distil-medium.en is faster than tiny.en at batch sizes 1 and 4,\nand faster than base.en at batch size 16, while performing 3.2% better on OOD test data.\nIncorporating FA2 benefits Distil-Whisper more than Whisper at higher batch sizes: the distil-large-\nv2 model is 31% faster with FA2 at batch size 16, compared to 22% for large-v2. Furthermore,\nthe distil-medium.en checkpoint is as fast as tiny.en at batch size 16, with an average OOD WER\nthat is 7.8% lower. FA2 has a greater improvement on the encoder than the decoder, where the\nmemory is re-allocated at each decoding step. Since Distil-Whisper consists of 32 encoder layers\nand only 2 decoder layers, it improves the inference time significantly more than Whisper, which\nhas 32 encoder and 32 decoder layers. This suggests that FA2 should always be incorporated for\nDistil-Whisper when operating at higher batch sizes. Using a static key/value cache would result in\na more significant speed-up to the inference time of the decoder. We leave this as future works.\nTable 25 reports the RTF on a 16GB T4 GPU. Since FA2 is not currently supported on T4, we report\nthe RTF using the original FA implementation. For the base attention implementation, distil-large-v2\nis faster than small.en at batch size 1. However, the distilled models follow the trend of medium.en\n29\nTable 24: Real time factor (RFT) with and without FA2 for batch sizes 1, 4 and 16. Inference speed\nis measured on a 40GB A100 GPU with PyTorch 2.0. RTF is expressed in 10-3.\nModel\nAvg. OOD WER\nBase\nFlash Attention 2\n1\n4\n16\n1\n4\n16\ntiny.en\n18.9\n22.7\n8.7\n3.1\n21.4\n8.3\n2.9\nbase.en\n14.3\n27.9\n11.7\n4.6\n30.5\n10.7\n3.4\nsmall.en\n10.8\n59.1\n22.3\n8.4\n50.7\n18.3\n6.3\nmedium.en\n9.5\n99.4\n43.1\n15.1\n89.3\n37.1\n11.2\nlarge-v2\n9.1\n137.2\n54.9\n21.3\n121.9\n48.4\n16.6\ndistil-medium.en\n11.1\n19.2\n7.5\n4.3\n20.4\n7.1\n2.9\ndistil-large-v2\n10.1\n25.1\n9.9\n6.5\n24.2\n8.9\n4.4\nTable 25: Real time factor (RFT) with and without FA for batch sizes 1, 4 and 16. Inference speed\nis measured on a 16GB T4 GPU with PyTorch 2.0. RTF is expressed in 10-3.\nModel\nAvg. OOD WER\nBase\nFlash Attention\n1\n4\n16\n1\n4\n16\ntiny.en\n18.9\n20.0\n7.2\n2.6\n21.2\n6.9\n2.5\nbase.en\n14.3\n26.1\n9.5\n4.2\n26.1\n9.6\n3.7\nsmall.en\n10.8\n48.3\n19.0\n9.9\n42.6\n16.8\n8.0\nmedium.en\n9.5\n89.9\n44.6\n26.5\n66.2\n33.8\n18.5\nlarge-v2\n9.1\n129.0\n74.5\n47.5\n100.6\n52.0\n33.8\ndistil-medium.en\n11.1\n23.0\n18.1\n17.2\n19.0\n12.1\n10.4\ndistil-large-v2\n10.1\n38.0\n31.8\n31.2\n27.4\n20.8\n20.1\nand large-v2 at higher batch sizes, where the percentage decreases to RTF are much lower than those\nof the smaller pre-trained Whisper checkpoints, such as base.en and small.en. At batch size 4 and\n16, distil-large-v2 is slower than small.en. The distil-medium.en model is faster than small.en at\nbatch size 4, but slower at 16.\nThis performance pattern can be attributed to the Distil-Whisper architecture: since we copy the\nentire encoder from the original Whisper medium.en and large-v2 models, the distil-medium.en and\ndistil-large-v2 models require more memory than small.en, especially at higher batch sizes. As the\nmemory on a T4 GPU increases, the throughput saturates, resulting in diminishing RTF benefits\nat batch sizes of 4 and 16. This trend is also observed for the medium.en and large-v2 Whisper\ncheckpoints.\nThe latency improvement obtained using FA is significant for both Whisper and Distil-Whisper. At\nbatch size 1, distil-large-v2 is comparable to base.en, while distil-medium.en is faster than tiny.en.\nHowever, the memory savings are not enough to offset the effects of the T4 GPU at higher batch\nsizes; distil-large-v2 is slower than small.en at batch size 4 and 16, and distil-medium.en slower than\nbase.en.\nOverall, a T4 GPU may be adequate for operating Whisper and Distil-Whisper models at a batch\nsize of 1. For batch sizes beyond this, there is a notable performance stagnation on a T4, and higher\nmemory A100 GPUs are preferential.\n30\n"
  },
  {
    "title": "LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing",
    "link": "https://arxiv.org/pdf/2311.00571.pdf",
    "upvote": "39",
    "text": "LLaVA-Interactive: An All-in-One Demo for\nImage Chat, Segmentation, Generation and Editing\nWei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, Chunyuan Li\nMicrosoft Research, Redmond\n{wchen,irinasp,jianwyan,jfgao,chunyl}@microsoft.com\nhttps://llava-vl.github.io/llava-interactive/\nAbstract\nLLaVA-Interactive is a research prototype for multimodal human-AI interaction.\nThe system can have multi-turn dialogues with human users by taking multimodal\nuser inputs and generating multimodal responses. Importantly, LLaVA-Interactive\ngoes beyond language prompt, where visual prompt is enabled to align human\nintents in the interaction. The development of LLaVA-Interactive is extremely\ncost-efficient as the system combines three multimodal skills of pre-built AI models\nwithout additional model training: visual chat of LLaVA [13], image segmentation\nfrom SEEM [31], and image generation and editing from GLIGEN [11]. A diverse\nset of application scenarios is presented to demonstrate the promises of LLaVA-\nInteractive and to inspire future research in multimodal interactive systems.\n1\nIntroduction\nThe rapid advancement of large language models (LLMs) [17, 5] has revolutionized chatbot systems.\nAs an example, OpenAI\u2019s ChatGPT [16] has demonstrated unprecedented levels of intelligence for\nhuman-AI interaction. The success of ChatGPT on language tasks has inspired the community to\nanticipate expanding the success to the multimodal space to eventually develop general-purpose\nmultimodal AI agents [9]. The release of GPT-4V [18] is a big stride towards the goal. Although\nGPT-4V demonstrates many impressive AI skills [27, 15], building multimodal conversational AI\nagents based solely on GPT-4V remains challenging for the open-source research community for two\nreasons. (i) GPT-4V is primarily a language-based human-AI interaction system, where user-input\nimages mainly provide visual contexts for text input and the system can only produce responses in\ntext. (ii) The details of model training and system architecture are not disclosed.\nTo mitigate the challenges, we present LLaVA-Interactive, an open-source research prototype sys-\ntem which can have multi-turn dialogues with human users by taking multimodal user inputs and\ngenerating multimodal responses. LLaVA-Interactive combines three multimodal skills of pre-built\nAI models without additional model training: visual chat of LLaVA [13], image segmentation from\nSEEM [30], and image generation and editing from GLIGEN [11]. We hope that LLaVA-Interactive is\ncomplementary to GPT-4V for the development of future multimodal AI agents as LLaVA-Interactive\nprovides a more extensible framework by supporting richer visual prompting and being open source.\n\u2022 Visual Prompting. LLaVA-Interactive supports flexible language-vision human-AI interactions\nby allowing human users to use diverse visual prompts, such as drawing strokes, drag and drop\nor bounding boxes, to express user intents for completing sophisticated multimodal tasks that\ninvolve image segmentation, generation and editing. As a result, we find that compared to\nstandalone LMMs, such as GPT-4V or LLaVA, LLaVA-Interactive can better follow user intents\nand generate more engaged human-machine interaction experiences.\nPreprint.\narXiv:2311.00571v1  [cs.CV]  1 Nov 2023\n\u2022 Open-source. We make our system and code base publicly available to facilitate future improve-\nments in the community.\nIn the rest of this paper, Section 2 reviews related work. Section 3 describes the interface, workflow,\nand AI skills of LLaVA-Interactive. Section 4 presents a case study of developing an AI agent to\nassist photographic artists using LLaVA-Interactive. Section 5 presents a preliminary evaluation of\nLLaVA-Interactive.\n2\nRelated Works\nLMM with Visual Output and Interaction.\nMost existing LMMs are developed to support visual\nchat \u2013 image understanding and reasoning. There are several exploratory studies to enable LMM to\nsupport image output such as image generation/editing and segmentation, demonstrated in GILL [8],\nCM3leon [29], Emu [20], DreamLLM [3], Kosmos-G [19] and MGIE [4]. The idea is generalized\nto other modalities such as video and audio in NextGPT [26]. In contrast to model training to\nenable image output, another line of research is to prompt engineer LLM for multimodal tool use\nsuch as Visual ChatGPT [25], X-GPT [30], MM-REACT [28], VisProg [7], and ViperGPT [21],\nwhere expert vision models with image output are activated in the inference time without any model\ntraining. Both research lines have demonstrated the extended capabilities with image output in\nLLMs. Similar to them, LLaVA-Interactive also supports image generation/editing and segmentation.\nLLaVA-Interactive is different from existing works in two aspects: (i) LLaVA-Interactive is cheap in\ndevelopment, as it is a synergy of the inference stages of three models. There is no model training,\nand no prompt engineering of LLM. (ii) Importantly, LLaVA-Interactive emphasizes the support of\nvisual interaction, where a user can draw strokes to specify the human intent in segmentation and\ngeneration/editing, a unique capability that existing visual assistant systems do not have.\nUnified Multimodal Modeling.\nInspired by the success of a single unified language model Chat-\nGPT for language tasks, it is of great promise to build a general-purpose assistant with a single\nmultimodal foundation model to complete more complex tasks [9]. While the development of a\nunified LMM for all vision-language tasks is still exploratory [30, 10, 6, 2, 14, 23, 24], it is be-\nlieved that this direction has great promise in unlocking new application scenarios. We present\nLLaVA-Interactive as a demo illustration of the potential of this research direction, including bridging\nvision and language tasks, completing a wide range of tasks in the wild, and providing a multimodal\npromptable user interface [9].\n3\nLLaVA-Interactive\nInterface.\nFigure 1 visualizes the user interface of LLaVA-Interactive. The overall interface layout\nis illustrated in (a), which consists of three panels annotated in different colors to ease the presentation.\nThe top-left panel in purple maintains the up-to-date image and accepts visual prompts such as user\nstrokes if necessary, the right panel in green is a language-based chat interface that accepts user\nquestions about the image, and responds in natural language. The lower-left section, highlighted in\nblue, represents the visual interaction interface, which comprises three tabs. Each tab is distinguished\nby its unique function and is displayed within a red rounded rectangle.\nTo illustrate how humans can interact with LLaVA-Interactive using visual prompts, we provide one\nexample for each tab in the sub-figures of Figure 1. (b) Remove or Change Objects. For an image,\nthe user draws a stroke on the object of interest. After clicking the \u201cSegment\u201d button, the object\nsegmentation mask is provided, e.g., the pier is highlighted in the magenta mask in this example.\nBy dragging the mask out of the image, and clicking the \u201cGenerate\u201d button, an edited image that\nremoves the object is generated, e.g., the pier is removed in this example. (c) Inpaint New Objects.\nTo add objects with precise size and position in the image, the user could specify the object spatial\nconfigurations using bounding boxes. Each drawing corresponds a minimum size box that contains\nthe stroke. The semantic concepts of the objects are provided in the grounding instruction (separated\nby a semicolon). By clicking the \u201cGenerate\u201d button, the desired objects are inpainted in the input\nimage, e.g., a boat a duck are added in the lake, and a bird is added on the sky in this example.\n(d) Generate New Image. To generate a new completely new image with precise object spatial\nlayouts, one may specify the object layout using bounding boxes on \u201cSketch Pad\u201d, and provide\nthe image-level caption as the language instruction. By clicking \u201cGenerate\u201d button, a new image\n2\n(a) The user interface layout. It consists of three panels, annotated in different colored for illustration.\n(b) Remove or Change Objects\n(c) Inpaint New Objects\n(d) Generate New Image\nFigure 1: The user interface of LLaVA-Interactive. (a) The overall user interface layout with\nthree main panels, among which the visual interaction panel consists of three tabs based on their\nfunctionalities. They are shown in magenta rounded rectangles, and detailed in (b,c,d), whose image\noutput after applying the visual interaction in each tab is shown at the bottom, respectively.\n3\nFigure 2: The workflow of LLaVA-Interactive.\nthat contains desired scene layout is generated. In this example, a new visual scene is generated to\nvisualize the semantics of a boat on the lake, with mountains in the background. At any given time,\nusers can effortlessly switch among the three visual interaction tabs to fulfill their intended visual\ncreation requirements iteratively.\nWorkflow.\nFigure 2 provides a workflow of LLaVA-Interactive. We describe the typical visual\ncreation process as below:\n(1) Image Input: To start, an image is required. The user can either upload an image or generate\none by providing a language caption and drawing bounding boxes to establish the spatial\narrangement of objects. Once the image is prepared, it can be interacted with through one of\nthree methods: chat, segmentation, or editing.\n(2) Visual Chat: Users can ask questions about the image, such as seeking suggestions for revisions.\nBased on the editing recommendations, objects can be removed or added using Steps 3 or 4,\nrespectively.\n(3) Interactive Segmentation: Users can create an object mask using either a stroke drawing or a text\nprompt. To remove the mask, drag it out of the image, and the background will automatically\nbe filled. Alternatively, the mask can be moved to a different location. To replace the mask with\na new object, provide a text prompt for the mask.\n(4) Grounded Editing: Users can directly place new objects on the image by drawing bounding\nboxes and associating the corresponding concepts with the intended objects.\n(5) Multi-turn Interaction: By repeating Steps 2, 3, or 4, users can iteratively refine their visual\ncreations.\nCapability Comparisons\nBased on LLaVA that allows image input for visual chat only, LLaVA-\nInteractive extends the capabilities to support visual interaction such as user-drawn strokes and\nbounding boxes, as well as visual image generation/editing. Please see the comparisons of the\ncapabilities below:\n3.1\nBehind the Scenes: Individual Models\nLLaVA-Interactive is an all-in-one demo that connects three LV models in one interactive session for\nimage chat, segmentation and generation/editing, which can complete more complex tasks than a\nsingle model alone. As a background, we briefly describe the individual models for those who are\ninterested in the key techniques:\n4\nSystem\nVisual Input\nVisual Output\nVisual Interaction\nLLaVA [13] / GPT-4V [18]\n\u2713\nLLaVA-Interactive\n\u2713\n\u2713\n\u2713\nTable 1: Comparison with existing multimodal systems. The empty cells indicate inapplicable.\n\u2022 LLaVA [13]: Large Language and Vision Assistant, the first open-source alternative to GPT-4V.\nIt is an end-to-end trained large multimodal model that combines CLIP vision encoder and\nVicuna for general-purpose visual understanding and reasoning, achieving impressive chat\ncapabilities mimicking the spirits of the GPT-4V. The recent LLaVA-1.5 [12] is considered in\nLLaVA-Interactive.\n\u2022 SEEM [31]: Segment Everything Everywhere with Multi-modal prompts all at once. SEEM\nallows users to easily segment an image using prompts of different types including visual\nprompts (points, marks, boxes, scribbles) and language prompts. It can also work with any\ncombination of prompts or generalize to custom prompts.\n\u2022 GLIGEN [11]: Grounded-Language-to-Image Generation, an open-source model that extends\nthe functionality of existing pre-trained text-to-image diffusion models by enabling them to also\nbe conditioned on visual prompts such as bounding boxes.\n3.2\nDevelopment Challenges\nLLaVA-Interactive is a system-level demo synergy. It showcases the ability to create general-purpose\nassistants/agents by leveraging existing model checkpoints, eliminating the need for additional\nmodel training. While the training requirements for AI models are minimal, the development of\nLLaVA-Interactive presented various technical challenges that we addressed along the way. Firstly,\nwe encountered difficulties with the GLIGEN inpainting model, which lacked the capability to fill\nbackground holes. As a solution, we incorporated the use of LaMA [22] for background filling.\nSecondly, Gradio, the framework used, lacked comprehensive support for user interaction, such\nas drag-and-drop functionality. To overcome this, we developed a new Gradio Image component\ntool that enabled the desired interaction. Additionally, integrating multiple complex projects and\nmodels posed a complexity challenge, which we addressed through experimentation, a streamlined\nUI layout, and an efficient data sharing scheme. Lastly, managing different package requirements and\ndependencies proved challenging, leading us to run separate web services for different models, such\nas LaMA.\n4\nCase Study: Multimodal Interactive Creation for Photographic Artists\nTo illustrate the enhanced user interaction experience and application scenarios made possible by\nLLaVA-Interactive, we present a case study focused on its utilization as a general-purpose assistant for\nphotographic artists. Figure 3 showcases a multi-turn multimodal interactive image editing process.\nThe left side of the figure displays the interaction type and functionality for each turn, while the right\nside provides a detailed depiction of the interaction process. The language-based dialogue between\nthe user and assistant is presented as text, with the user\u2019s visual prompts shown in magenta text and\nthe resulting image editing outcomes displayed as processed images.\nWe describe the interaction process for this case study in Figure 3, which can be used as an example\nfor the presentation of more application scenarios in Section 5.\n1 A user first uploads a lake scenery image, and asks for suggestions to improve the visual\nappearance. The assistant suggests to remove the wooden pier extending out into the water.\n2 Following the suggestions, the user turns to the \u0003\n\u0002\n\u0000\u0001\nRemove and Change Object tab, chooses the\n\u0003\u0002\n\u0000\u0001\nStroke mode, draws a stroke on the pier of the image, and clicks \u0003\n\u0002\n\u0000\u0001\nSegment. The segmentation\nmask of the pier is shown, presented in magenta color in this example. The user can further\ndrag the mask out of the image, and click Generate, after which an updated image of the clear\nlake without the pier is shown.\n5\nAssistant: The part of the picture that might not be visually appealing to the viewer is the wooden pier \nextending out into the water. The pier appears to be old and worn, which could make it less attractive \ncompared to a well-maintained or modern pier. Additionally, the pier's proximity to the water might \nnot be appealing to some viewers who prefer to stay on land or have a fear of water. However, the \noverall scene, including the calm lake, the mountains in the background, and the trees, can be visually \nappealing to many people.\nUser: What part of the picture isn't visually appealing to the viewer?\nUser: [Stroke on the pier]\nAssistant:\nUser: [Drag the mask  to remove] [ Generate]\nUser:  Now, what part of the picture isn't visually appealing to the viewer?\nAssistant: The part of the picture that is not visually appealing to the viewer is the dock. The dock is located in the middle of the \nlake, surrounded by trees and mountains. However, it appears to be old and dilapidated, which detracts from the overall beauty of \nthe scene. The presence of the dock in such a picturesque setting with a calm lake, mountains, and trees creates a contrast between \nthe natural beauty and the man-made structure, which might not be as appealing to the viewer as the other elements in the image.\nUser: [Referral text = Dock] [Segment]\nUser: [Drag the mask to remove] [Generate]\nAssistant: To improve the visual appeal of the picture, you can consider adding more elements to create a sense of depth and \nbalance. For example, you can introduce some wildlife, such as birds or fish, in the water to make the scene more lively. \nAdditionally, you can add more trees or vegetation around the lake to create a more natural and serene atmosphere. You can also \nexperiment with different lighting conditions, such as sunrise or sunset, to enhance the overall mood and aesthetic of the image. \nFinally, you can play with the composition by adjusting the position of the trees or the horizon line to create a more visually \npleasing balance between the foreground and the background.\nUser: I removed the dock. What else can I do to improve the visual appeal of the picture?\nAssistant: The final product is beautiful and captures the essence of the scene. The reflection of the sunset on the lake water \nadds depth and visual interest to the image. The combination of the sunset, mountains, and trees creates a serene and \npicturesque atmosphere. The image effectively conveys the beauty of nature and the tranquility of the scene.\nUser: [Referral text = Sky] [Segment]\nUser: [Grounding text = sunset scene]\nAssistant: Reflections on the water refer to the light from the sunset that is reflected off the surface of the lake. As the sun sets, \nits warm colors cast a glow on the water, creating a beautiful and captivating effect. The reflections can create a sense of depth \nand dimension in the image, making the scene more visually appealing and realistic.\nUser: I really like the idea of a sunset and changed the sky to a sunset scene. Does it look good to you?\nAssistant: As an AI, I don't have personal preferences or opinions, but I can tell you that the image you have created with the \nsunset sky looks visually appealing. The combination of the sunset and the mountain lake creates a serene and picturesque \nscene that can evoke a sense of tranquility and beauty. The colors of the sunset and the reflections on the water add depth and \ninterest to the image, making it an attractive piece of art.\nUser: What do you mean by the reflections on the water?\nUser: [Referral text = lake surface] [Segment]\nUser: [Grounding text = reflection of the sunset scene in the sky] [Generate]\nUser: Thank you for mentioning the reflection. I added the reflection of the sunset. What do you think of my final product?\nVisual Chat \nfor Editing \nSuggestion\nVisual Prompt \nfor \nSegmentation \nand Removal\nVisual Chat \nfor Editing \nSuggestion\nVisual Chat \nfor Editing \nSuggestion\nVisual Chat \nfor Editing \nSuggestion\nVisual Chat \nfor Editing \nSuggestion\nText Prompt \nfor \nSegmentation \nand Removal\nText Prompt \nfor \nSegmentation \nand Editing\nText Prompt \nfor \nSegmentation \nand Editing\n1\n2\n3\n4\n5\n6\n7\n8\n9\nFigure 3: The use case study of LLaVA-Interactive as a photographic artist. The multi-turn multimodal\ninteractive image editing process is illustrated. (Left) The interaction type/functionality for each turn;\n(Right) The detailed interaction process.\n6\n3 Based on the updated image, the user asks for further suggestions to improve the visual\nappearance. The assistant suggests to remove the small dock in the middle of the lake.\n4 Following the suggestions, the user turns to the \u0003\n\u0002\n\u0000\u0001\nRemove and Change Object tab again.\nThis time, the user utilizes the text prompt mode to segment the object, by choosing \u0003\u0002\n\u0000\u0001\nText,\ntyping the \u201cdock\u201d in the \u0003\n\u0002\n\u0000\u0001\nEnter referring text box, and clicking the \u0003\n\u0002\n\u0000\u0001\nSegment button. The\nsegmentation mask of the dock is shown, presented in magenta color. Following the same\ndrag-and-generate procedure, the highlighted dock is removed from the image.\n5 The user seeks further suggestions to improve the visual appeal of the picture, and the assistant\nrecommends several concrete options, among which adding a sunset to enhance the overall\naesthetic of the image is mentioned.\n6 The user utilizes the text prompt mode again to select the sky region.\nTo re-\nplace\nthe\nselected\nmask\nwith\na\nnew\nobject,\nthe\nuser\nputs\n\u201csunset\nscene\u201d\nin\n\u0003\n\u0002\n\u0000\u0001\nEnter grounding text for generating a new image box, and click \u0003\u0002\n\u0000\u0001\nGenerate. A\nnew image with a sunset in the sky is shown.\n7 The user asks for further editing suggestions, and finds that the reflection of the sunset on the\nwater can make the image more appealing and realistic.\n8 By performing a similar text prompt based segmentation and replacement procedure on the\nwater, the reflection of the sunset scene in the sky is shown on the lake of the image.\n9 The user asks for comments on the final product. The assistant considers this final version can\neffectively convey the beauty of nature.\n5\nPreliminary Evaluation with More Application Scenarios\n5.1\nIterative Co-Creation of Visual Scene and Description\nPlease see the user-assistant interaction in Figure 4 and 5. Joint creation of a visual scene and its\nassociated text from scratch can be useful for content creators. In this scenario, the user can generate a\npeaceful and relaxing outdoor scene from text, and specify spatial layout by drawing boxes. Once the\nimage is generated, the users can ask the assistant to describe and promote the image using text. The\nvisual image creation in one shot can be imperfect, and can be iteratively improved, e.g., removing\nthe gray shore and adding a white goose using a visual prompt in this example. For the final image,\nthe user can ask the assistant to produce a poem in both English and Chinese to promote the image.\nIf necessary, the user can also ask if the synthsized image is reasonable or not, e.g., the size of the\nswan compared with the boat. It can be seen the text description and response of LLaVA-Interactive\nis often coherent with edited images.\n5.2\nGraphic Design for a Halloween Poster\nPlease see the user-assistant interaction in Figure 6, 7 and 8. Designing visually appealing posters\nand gift cards for occasions like Halloween demands imaginative concepts and striking aesthetics.\nFor instance, when crafting a Halloween poster, the user may request the AI assistant to offer a range\nof ideas, and then choose one to be transformed into an image. To refine the generated image, the\nuser may seek further suggestions, such as incorporating a bat, swapping the scarecrow with a ghost,\nremoving a smaller scarecrow, adding a skeleton, and substituting pumpkins with a spider web. After\nmaking these adjustments, the user can request feedback, and the assistant affirms that the design\neffectively captures the spirit of Halloween.\n5.3\nFashion Design for Kid\u2019s Clothing\nPlease see the user-assistant interaction in Figure 9 and 10. Imagine one day when a user sees his\nChinese nephew\u2019s sweatshirt with designed English text and a picture on the back, and wants to\npersonalize the design while the sweatshirt is being worn. To first show his nephew what is written\non the sweatshirt and the meaning of the text. The user asks the related questions, and the assistant is\nable to correctly answer both. This simple application can be widely used to recognize text on various\nclothes on the Chinese market, where most users have less knowledge about designed English text.\n7\nWith LLaVA-Interactive, the user can further edit the picture by following his nephew\u2019s personalized\nrequests to add a blue hat and sun glasses. The assistant can make comments on the new picture,\nsaying that \u201c the design features a cartoon bear wearing sunglasses and a hat, which could be appealing\nto children who enjoy animals or have a sense of humor.\u201d This encouraging comment can boost the\nkid\u2019s confidence in his design skills.\nThe assistant also mentions the city \u201cDenver\u201d in the comments, which the assistant guesses that it\ncould be the child\u2019s hometown or a place they enjoy visiting. Following the comments, the user\nwants to design a new picture with a representative natural scene of Denver. To this end, the user\nfirst removes the bear and only leaves the background; after that, the user creates a new scene by\nspecifying the spatial layout of objects \u201clake; boat; tent; snow mountains\u201d. With the new image,\nthe assistant believes that it is a great choice for a kid\u2019s clothing, as it combines both aesthetics and\nimagination.\n5.4\nFood Preparation\nDinner Preparation for a Romantic Date\nPlease see the user-assistant interaction in Figure 11.\nIndividuals often enjoy capturing photos of appetizing meals they consume. When planning a\nromantic dinner, individuals typically dedicate time and consideration to crafting the perfect dish,\ncomplemented by wine and flowers. Nonetheless, they may still feel apprehensive about whether the\ndinner is adequately prepared or if any enhancements could be made.\nIn this situation, we suggest utilizing LLaVA-Interactive to obtain valuable advice and recommenda-\ntions. The assistant expresses enthusiasm about the dinner while also offering specific ideas, such as\nincorporating salads to enhance the main dishes and using candles to establish a warm and intimate\nambiance. By implementing these suggestions, the user can modify the image to develop various\nvirtual dinner options and submit them to the assistant for evaluation. Once the ideal solution is\ndetermined and receives positive feedback from the assistant, the user can also request guidance on\nappropriate dating etiquette for this specific dinner.\nFood Preparation Recipe\nPlease see the user-assistant interaction in Figure 12 and 13. In another\nexample involving meal preparation using available ingredients, the user may inquire about the\nnecessary ingredients and cooking instructions. The user might also decide to change some elements,\nsuch as replacing butter with rice. Upon asking the same question again, updated ingredients and\ninstruction lists will be provided with the revised cooking tips.\n5.5\nVisual Content Creation and Story Telling\nPlease see the user-assistant interaction in Figure 14. The process of visual storytelling usually de-\nmands creativity and considerable time investment, as it involves the development of both compelling\nimages and imaginative text. Occasionally, adjustments to the visuals may be necessary to ensure they\nare in harmony with the overall scene. In Figure 14, LLaVA-Interactive is able to provide detailed\ndescriptions and a magical story for kids.\nThe user may ask for possible edits on the image for a more whimsical story. Several more playful\nand imaginative elements are suggested, including glowing mushrooms and oversize instruments. By\nfollowing the idea, the user inpaints the new objects of intended spatial configuration in the image,\ne.g., a growing mushroom in front of the first character on the left, and an oversize drum for the\nfourth character who is playing a drum.\n5.6\nEducation\nScientific Education.\nPlease see the user-assistant interaction in Figure 15 and 16. In order to\nengage children in learning scientific concepts, it is effective to present the information using visual\nimagery and familiar themes, such as cartoon characters. For instance, a child living in the Seattle\narea may be drawn to an image featuring the Space Needle and a dinosaur set against a pink sky,\ncreating a lively scene. Such an image would capture the child\u2019s interest due to its recognizable\nelements. The child might then inquire about the T-rex, the reason behind the pink sky, the color of\nthe sky during midday, and why it changes. Additionally, the T-rex could be replaced with various\n8\nrobots, prompting the child to ask about their functions and concepts. By using familiar and visually\nappealing elements, learning about science can become an enjoyable experience for children.\nCartoon Reading Education.\nPlease see the user-assistant interaction in Figure 17. To improve\none\u2019s ability to interpret cartoons, it is crucial to understand that different details within an image can\ncommunicate various meanings. For example, when examining an editorial cartoon featuring a man\nwearing clothing labeled \"PRESS\" and bound by a ball and chain, the assistant might explain that the\nimage metaphorically represents the obstacles journalists encounter in their quest for truth and the\nsignificance of safeguarding their freedom of expression. However, if the \"PRESS\" label is removed\nand the same question is asked, the responses could vary greatly. The cartoon may still convey a\nstrong visual message about the challenges and pressures the man faces, but the interpretations can\ndiffer considerably without the context provided by the \"PRESS\" label.\n5.7\nInterior Design\nInterior Design: Large Living Room.\nPlease see the user-assistant interaction in Figure 18 and 19.\nInterior design can be progressively enhanced through a trial-and-error process. Initially, the user\ncreates an image based on a text description prompt and seeks advice on how to modernize the\ndesign. Suggestions might include incorporating minimalist elements, adopting contemporary styles,\nadding potted plants, and displaying modern art on the walls. The user can then modify the design by\nsegmenting objects and making adjustments using stroke and text prompts. As a result, these new\nelements come together to create a more comfortable and updated living room design.\nInterior Design: Small Living Room.\nPlease see the user-assistant interaction in Figure 20 and\n21. In a different living room design scenario, the assistant offers an extensive list of improvements.\nThe user utilizes the interactive segmentation ability of the AI to select the sofa and table, updating\nthem to a modern style. Additionally, a potted plant is incorporated using the grounded inpainting\nskill. The user might also inquire about specific objects, such as the position of the TV or replacing\nthe ceiling lamp. Based on the suggestions, these objects can be modified using segmentation and\ninpainting skills to achieve the desired outcome.\n5.8\nIdentification of Unusual and Risky Items\nPlease see the user-assistant interaction in Figure 22. It is an important aspect of safety and security\nto detect the unusual, abnormal and risky items in images. The process typically involves analyzing\nimages to detect any objects or patterns that may pose a risk or deviate from the norm. We use the\npopular extreme ironing image as an example to illustrate this use scenario. A typical question is to\nreport what is unusual in the image. We further ask about the potential dangers. LLaVA-Interactive is\nable to correctly respond to this due to the use of the underlying LLaVA model. The image appears to\nbe unusual because of the co-existence of the moving car and the man doing ironing. We ablate this\nby removing one element at each time. By removing the person, the ironing activity is not reported.\nInstead, the presentation of a chair on a moving car becomes the key unusual element. By replacing\nthe moving taxi with flowers, the moving car is not reported. Instead, the assistant perceives the\nunusual aspect as that the man is riding a bicycle that is carrying a large, colorful bouquet of flowers.\nThis is possible as the riding bicycle might be hiding behind the flowers. This analysis-by-synthesis\napproach can be effective in examining visual scenes to identify anomalies.\n6\nConclusions and Future Work\nIn this paper, we have introduced LLaVA-Interactive, a research demo prototype that showcases\nthe practical applications of large multimodal models featuring visual input, output, and interaction.\nLLaVA-Interactive is cost-effective in system development since it combines three pre-trained\nmultimodal models of complementary skills using web services, without requiring additional model\ntraining: LLaVA for visual chat, SEEM for interactive image segmentation, and GLIGEN for\ngrounded image generation and editing. At the system level, compared with other systems, LLaVA-\nInteractive is a fully vision-language multimodal system in terms of input, output, and interaction,\nparticularly unique in supporting visual prompts for image segmentation and generation/editing. Our\ninitial assessment of LLaVA-Interactive across a wide range of real-world application scenarios has\n9\ndemonstrated its excellent ability to perform new, complex tasks. We hope this will inspire further\nresearch into multimodal foundation models.\nWe identify several potential avenues for future research: (i) The abilities of LLaVA-Interactive are re-\nstricted by the performance limits of the utilized pre-trained models. Enhancing LLaVA-Interactive\u2019s\nspecific skill could be achieved by replacing the module with a superior model variant or creating\nan improved individual model, such as LLaVA, SEEM, and GLIGEN. System development and\nindividual model development can be de-coupled, allowing for a plug-and-play approach to system\nserving. We also hope to extend the system development framework by incorporating additional\nfeatures like Instruct Pix2Pix [1] for image-level editing. (ii) Since LLaVA-Interactive is a composite\nof individual models, its capacity during each inference is determined by the existing abilities of those\nmodels. While more complex tasks can be accomplished through iterative activation of current skills\nfor combined skills, no new skill emerges at each inference by interpolating in the neural network\u2019s\nhidden space. We encourage the community to develop multimodal foundation models with more\nunified modeling, allowing new capabilities to emerge through latent task composition.\nAcknowledgments\nThe authors would like to express their gratitude to the MSR Central Engineering Team for their\nsupport. We appreciate the efforts of all authors who contributed to the individual pre-trained models,\nmaking LLaVA-Interactive a reality. Special thanks go to Matt Mazzola for creating the informative\ndemo instruction video and managing the demo serving, as well as Swadheen Shukla and Lars Liden\nfor their valuable input and insightful discussions.\nReferences\n[1] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow\nimage editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18392\u201318402, 2023.\n[2] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey Hinton. A\nunified sequence interface for vision tasks. arXiv preprint arXiv:2206.07669, 2022.\n[3] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jian-\njian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension\nand creation. arXiv preprint arXiv:2309.11499, 2023.\n[4] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guid-\ning instruction-based image editing via multimodal large language models. arXiv preprint\narXiv:2309.17102, 2023.\n[5] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan\nLu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction\nmodel. arXiv preprint arXiv:2304.15010, 2023.\n[6] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and Derek Hoiem. Towards general\npurpose vision systems: An end-to-end task-agnostic vision-language architecture. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n16399\u201316409, June 2022.\n[7] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training. arXiv preprint arXiv:2211.11559, 2022.\n[8] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal\nlanguage models. arXiv preprint arXiv:2305.17216, 2023.\n[9] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng\nGao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv\npreprint arXiv:2309.10020, 2023.\n10\n[10] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang,\nYu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for\nlarge-scale vision and vision-language tasks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 2691\u20132700, 2023.\n[11] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan\nLi, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint\narXiv:2301.07093, 2023.\n[12] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual\ninstruction tuning. arXiv preprint arXiv:2310.03744, 2023.\n[13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[14] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.\nUnified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint\narXiv:2206.08916, 2022.\n[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao\nCheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical\nreasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.\n[16] OpenAI. ChatGPT. https://openai.com/blog/chatgpt/, 2023.\n[17] OpenAI. Gpt-4 technical report, 2023.\n[18] OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_\nCard.pdf, 2023.\n[19] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-\ng: Generating images in context with multimodal large language models. arXiv preprint\narXiv:2310.02992, 2023.\n[20] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv\npreprint arXiv:2307.05222, 2023.\n[21] D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution\nfor reasoning. arXiv preprint arXiv:2303.08128, 2023.\n[22] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii\nAshukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lem-\npitsky. Resolution-robust large mask inpainting with fourier convolutions. arXiv preprint\narXiv:2109.07161, 2021.\n[23] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou,\nJingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a\nsimple sequence-to-sequence learning framework. In ICML, 2022.\n[24] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang.\nSeggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\n[25] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671, 2023.\n[26] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any\nmultimodal llm. arXiv preprint arXiv:2309.05519, 2023.\n[27] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\narXiv:2309.17421, 2023.\n11\n[28] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for\nmultimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.\n[29] Lili Yu and et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning.\n2023.\n[30] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat\nBehl, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao.\nGeneralized decoding for pixel, image, and language. In CVPR, 2023.\n[31] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee.\nSegment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n12\nIterative Visual Generation: Description and Editing (1/2)\nUser\n(Generate New Image) [Language instruction = \u201ca boat on a lake, with mountains in the\nbackground\u201d; Draw boxes for Grounding instruction \u201cboat; lake; mountains\u201d] [Generate]\nAssistant\nUser\nPlease describe the image and promote the scenery.\nAssistant\nThe image features a serene scene of a blue boat floating on a calm lake, surrounded by a\nbeautiful mountain range. The boat is positioned near the shore, with the mountains in the\nbackground, creating a picturesque and tranquil atmosphere. The reflection of the mountains\nand the boat can be seen on the water\u2019s surface, enhancing the overall beauty of the scene. This\nidyllic setting is perfect for those seeking relaxation and a connection with nature.\nUser\n(Remove and Change Object) [Visual prompt by drawing a stroke on the shore][Generate]\nAssistant\nUser\n(Inpaint New Objects) [Draw boxes for Grounding instruction \u201cwhite goose\u201d] [Generate]\nAssistant\nFigure 4: Iterative visual creation: image generation, description and editing (1/2). Refer to the\nscenario discussions in Section 5.1.\n13\nIterative Visual Generation: Description and Editing (2/2)\nUser\nwrite a poem about the image?\nAssistant\nA serene scene unfolds, A blue boat rests on the water, A swan swims by, gracefully, In the\ncalm lake, a peaceful sight.\nThe mountains rise in the distance, A majestic backdrop to the scene, The boat and swan coexist,\nIn this tranquil, picturesque setting.\nThe water\u2019s surface is still, A reflection of the sky, The boat and swan share the space, In\nharmony, they glide.\nThe scene evokes a sense of calm, A moment of respite from the world, A peaceful coexistence,\nBetween the boat, swan, and the mountains.\nUser\n\u5173\u4e8e\u8fd9\u4e00\u666f\u8272\uff0c\u5199\u4e00\u9996\u6563\u6587\u8bd7\nAssistant\n\u5728\u8fd9\u4e00\u534e\u4e3d\u7684\u666f\u8272\u4e2d\uff0c\u4e00\u8258\u5c0f\u8239\u5728\u6e56\u9762\u4e0a\u6f02\u6d6e\uff0c\u4e00\u53ea\u4f18\u7f8e\u7684\u5929\u9e45\u5728\u6c34\u4e2d\u6e38\u6cf3\uff0c\u5929\u7a7a\u5728\n\u6c34\u9762\u4e0a\u5982\u753b\u822c\u53cd\u5c04\uff0c\u5929\u5c71\u5728\u8fdc\u65b9\u8038\u7acb\uff0c\u5b9b\u5982\u4e00\u5e45\u4e30\u6536\u7684\u753b\u5377\u3002\n\u8fd9\u4e00\u534e\u4e3d\u7684\u666f\u8272\uff0c\u5982\u8bd7\u5982\u753b\uff0c\u6e56\u6cca\u7684\u5b81\u9759\uff0c\u8239\u548c\u5929\u9e45\u7684\u81ea\u7531\uff0c\u5929\u5c71\u7684\u58ee\u4e3d\uff0c\u90fd\u878d\u5165\u4e86\n\u8fd9\u4e00\u7f8e\u4e3d\u7684\u753b\u5377\u3002\n\u8fd9\u4e00\u534e\u4e3d\u7684\u666f\u8272\uff0c\u65e2\u6709\u5b81\u9759\uff0c\u4e5f\u6709\u81ea\u7531\uff0c\u4e5f\u6709\u58ee\u4e3d\uff0c\u5b83\u8ba9\u6211\u4eec\u611f\u53d7\u5230\uff0c\u5927\u81ea\u7136\u7684\u7f8e\u5999\n\u4e0e\u795e\u79d8\u3002\n\u8fd9\u4e00\u534e\u4e3d\u7684\u666f\u8272\uff0c\u662f\u4e00\u9996\u8d5e\u7f8e\u5927\u81ea\u7136\u7684\u8bd7\uff0c\u662f\u4e00\u5e45\u63cf\u7ed8\u81ea\u7136\u7f8e\u666f\u7684\u753b\u5377\uff0c\u662f\u4e00\u6bb5\u611f\u53d7\n\u81ea\u7136\u7684\u6563\u6587\uff0c\u5b83\u90fd\u662f\u4e00\u9996\u8d5e\u7f8e\u81ea\u7136\u7684\u8d5e\u6b4c\u3002\nUser\nis the swan too big?\nAssistant\nIn the image, the swan appears to be relatively large compared to the boat and the surrounding\nenvironment. However, it is not necessarily too big, as swans can vary in size depending on the\nspecies and individual. It is possible that the swan is simply larger than what one might expect\nin a typical lake or river setting. Nonetheless, the presence of a large swan in the image adds to\nthe overall beauty and tranquility of the scene.\nUser\nPlease describe the image and promote the scenery\nAssistant\nThe image features a serene and picturesque scene, with a boat floating on a body of water, such\nas a lake or river. A large swan is swimming nearby, adding a touch of grace and beauty to\nthe landscape. In the distance, majestic mountains rise up, providing a stunning backdrop to\nthe scene. The combination of the boat, swan, and mountains creates a sense of harmony and\ntranquility, inviting viewers to appreciate the natural beauty of the environment.\nThis idyllic setting is perfect for those seeking a peaceful escape from the hustle and bustle\nof everyday life. Whether it\u2019s for leisurely boating, birdwatching, or simply taking in the\nbreathtaking views, this scenery offers a unique and memorable experience for anyone who\nvisits. The image captures the essence of nature\u2019s beauty and the simple pleasures that can be\nfound in the great outdoors.\nFigure 5: Iterative visual creation: image generation, description and editing (2/2). Refer to the\nscenario discussions in Section 5.1.\n14\nGraphic Design for a Halloween Poster. (1/3)\nUser\nI\u2019d like to design a poster for Halloween. Can you give me some ideas?\nAssistant\nSure! Here are a few ideas for a Halloween poster:\n\u2022 A spooky scene with a full moon, a graveyard, and some ghosts or other supernatural\ncreatures.\n\u2022 A pumpkin patch with carved pumpkins and a scarecrow.\n\u2022 A haunted house with a creepy clown or other scary characters lurking inside.\n\u2022 A group of trick-or-treaters out on Halloween night, with a mix of cute and spooky\nelements.\n\u2022 A witch\u2019s cauldron with ingredients for a potion, such as bats, spiders, and ghostly\neyeballs.\n\u2022 A haunted forest with glowing eyes and twisted trees.\n\u2022 A spooky lab with a mad scientist and his monstrous creations.\n\u2022 A graveyard at night with glowing headstones and a ghostly figure.\n\u2022 A haunted mansion with a ghostly family and their spooky guests.\n\u2022 A group of skeletons having a party, with skulls and bones as decorations.\nI hope these ideas give you some inspiration for your Halloween poster!\nUser\n(Generate New Image) [Language Instruction = \"A pumpkin patch with carved pumpkins\nand a scarecrow\"] [Generate]\nAssistant\nUser\nWhat do you think of this picture I generated? Do you have any ideas to improve?\nAssistant\nThe image features a group of scarecrows dressed in Halloween costumes, standing in a field\nwith pumpkins. There are several pumpkins scattered throughout the scene, with some placed\ncloser to the scarecrows and others further away. The scarecrows are positioned in various\nposes, creating a lively and festive atmosphere. The scene is set against a blue sky, which adds\na sense of depth and contrast to the image. The combination of scarecrows, pumpkins, and the\nblue sky creates a visually appealing and spooky Halloween scene. To improve the image, one\ncould consider adding more elements to the scene, such as a few more pumpkins in different\nsizes and positions, or incorporating some additional Halloween-themed decorations like bats,\nspiders, or ghostly figures. This would create a more dynamic and engaging visual experience\nfor the viewer.\nFigure 6: Graphic Design for a Halloween Poster (1/3). Refer to the scenario discussions in\nSection 5.2.\n15\nGraphic Design for a Halloween Poster. (2/3)\nUser\n(Inpaint New Objects)[Draw box for Grounding Instruction = \"bat\"][Generate]\nAssistant\nUser\nWhat do you think of the bat I added? Do I need more of them for this Halloween poster?\nAssistant\nThe bat you added to the Halloween poster is a nice touch, but it might not be enough to create a\nmore spooky atmosphere. To enhance the poster\u2019s Halloween theme, you could consider adding\nmore bats, spiders, or other scary elements such as ghosts, skeletons, or cobwebs. This would\ncreate a more visually engaging and thrilling poster that captures the essence of Halloween.\nUser\n(Change\nor\nRemove\nObjects)\n[Visual\nprompt\nby\nstroking\non\none\nscare-\ncrow][Segment][Grounding Instruction = \"Halloween ghost\"][Generate]\nAssistant\nFigure 7: Graphic Design for a Halloween Poster (2/3). Refer to the scenario discussions in\nSection 5.2.\n16\nGraphic Design for a Halloween Poster. (3/3)\nUser\n(Remove\nor\nChange\nObjects)[Visual\nprompt\nby\nstroking\non\nthe\nsmall\nscare-\ncrow][Segment][Visual prompt by dragging out of picture to remove] [Generate]\nAssistant\nUser\n(Inpaint Objects) [Draw box for Grounding Instruction = \u201cHalloween skeleton\u201d][Generate]\nAssistant\nUser\n(Inpaint Objects) [Clear to accept][Draw box for Grounding Instruction = \"Halloween\nspider web\"][Generate]\nAssistant\nUser\nI added a skeleton, a ghost and a spider web to my Halloween poster like you suggested. I am\npretty happy with my final product. What do you think?\nAssistant\nI think your Halloween poster looks great! The addition of the skeleton, ghost, and spider web\nadds a spooky and festive touch to the poster. The combination of the pumpkins, skeleton, and\nspider web creates a visually appealing and fun design that captures the essence of Halloween.\nWell done!\nFigure 8: Graphic Design for a Halloween Poster (3/3). Refer to the scenario discussions in\nSection 5.2.\n17\nFashion Design for Kid\u2019s Clothing (1/2)\nUser\nwhat is written in the image?\nAssistant\nThe image features a sweatshirt with the words \"You\u2019re making me crazy\" written on it.\nUser\n\u8fd9\u662f\u4ec0\u4e48\u610f\u601d\uff1f\nAssistant\n\u8fd9\u53e5\u8bdd\u7684\u610f\u601d\u662f\uff1a\u4f60\u8ba9\u6211\u89c9\u5f97\u75af\u72c2\u3002\nUser\n(Inpaint New Objects) [Draw boxes for Grounding Instruction = \u201cblue hat; sun glasses\u201d]\n[Generate]\nAssistant\nUser\nhow do you think about the picture design as a kid cloth?\nAssistant\nWhen designing a picture for a kid\u2019s cloth, it\u2019s important to consider the age range and interests\nof the child who will be wearing it. For younger children, simple shapes, bright colors, and\ncharacters from popular children\u2019s shows or movies can be effective. For older children, more\ncomplex designs, patterns, and popular trends in fashion can be considered. It\u2019s also important\nto ensure that the design is age-appropriate and does not contain any inappropriate or offensive\nimagery. Additionally, the design should be durable and able to withstand washing and wear. In\nthe case of the image, the design features a cartoon bear wearing sunglasses and a hat, which\ncould be appealing to children who enjoy animals or have a sense of humor. The sweatshirt is\nalso personalized with the name \"Denver\" on the back, which could be a way to show pride in\nthe child\u2019s hometown or a place they enjoy visiting.\nUser\n(Remove and Change Objects) [Referral Text = \u201cbear and glasses\u201d, visual prompt by\ndrawing on \u201chand\u201d, then remove them by dragging the segmentation masks out of the image]\n[Generate]\nAssistant\nFigure 9: Fashion Design for Kid\u2019s Clothing (1/2). Refer to the scenario discussions in Section 5.3.\n18\nFashion Design for Kid\u2019s Clothing (2/2)\nUser\n(Inpaint New Objects) [Draw boxes for Language Instruction = \u201cboat; lake; snow moun-\ntain; tent\u201d] [Generate]\nAssistant\nUser\nhow do think about the current picture design for a kid cloth?\nAssistant\nThe current picture design for the kid\u2019s cloth is quite appealing and imaginative. The scene\nof two tents on a lake, surrounded by mountains, creates a sense of adventure and outdoor\nexploration, which can be both entertaining and inspiring for a child. The vibrant colors and\ndetailed illustration make the design stand out and can spark a child\u2019s imagination, making it\na fun and creative choice for a kid\u2019s cloth. The design is visually engaging and can capture\na child\u2019s attention, making it an attractive option for a child\u2019s clothing. Overall, the current\npicture design is a great choice for a kid\u2019s cloth, as it combines both aesthetics and imagination,\ncreating a visually appealing and engaging piece of clothing.\nFigure 10: Fashion Design for Kid\u2019s Clothing (2/2). Refer to the scenario discussions in Section 5.3.\n19\nDinner Preparation\nUser\nI just prepared a dinner for my girlfriend, how do you think of it? is there anything l should add or\nremove to improve the dinner?\nAssistant\nThe dinner you prepared looks delicious and well-presented. The table is set with a variety of\nfood, including bowls of Asian cuisine, a bottle of wine, and wine glasses. There are also vases\nwith flowers, adding a touch of elegance to the setting. The table is set with chairs, and the overall\npresentation is inviting.\nAs for improvements, it would be great to have some side dishes or salads to complement the\nmain courses. You could also consider adding some dipping sauces or condiments to enhance the\nflavors of the dishes. Additionally, you could add some soft lighting or candles to create a more\nintimate and cozy atmosphere. Finally, you could play some background music to set the mood for\na romantic dinner.\nUser\n(Inpaint New Objects) [Draw boxes for Grounding Instruction = \u201csalads; candles\u201d] [Generate]\nAssistant\nUser\nsalads and candles are added now. is the current dinner good?\nAssistant\nThe current dinner setup appears to be quite appealing and well-prepared. The table is set with a\nvariety of dishes, including bowls of food, a bottle of wine, and a lit candle. The presence of salads\nand candles adds to the ambiance and makes the dinner more enjoyable. The combination of food,\nwine, and candlelight creates a warm and inviting atmosphere, which is ideal for a pleasant dining\nexperience.\nUser\nThis image is the home dinner I made; can you give dating behavior tips for this particular dinner?\nAssistant\nHere are some dating behavior tips for a home-cooked dinner like the one in the image:\nPrepare the meal: Show your date that you put effort into making the meal by preparing it yourself.\nThis can be a fun and creative way to spend time together. Set the mood: Create a cozy and\nintimate atmosphere at home. Light some candles, play some soft music, and set the table with\nnice plates and cutlery. Communicate: Use this opportunity to catch up and have a meaningful\nconversation with your date. Ask them about their day, share your thoughts and feelings, and\nengage in deep conversation. Be attentive: Pay attention to your date\u2019s needs and preferences.\nOffer to refill their drink, ask if they would like more food, and make sure they are comfortable.\nShow appreciation: Express your gratitude for your date\u2019s company and the meal. Compliment\ntheir outfit or tell them how much you enjoyed the food. Be yourself: Be authentic and relaxed.\nDon\u2019t try to impress your date with a fancy meal or extravagant gesture. Instead, focus on spending\nquality time together and enjoying each other\u2019s company. Have fun: Finally, have fun and enjoy\nthe evening! Laugh, share stories, and create memories together. In the image, there are several\nbowls of food, a bottle of wine, and a lit candle on the table. This suggests that the meal is a special\noccasion, and the couple is enjoying each other\u2019s company in a cozy and intimate setting.\nFigure 11: Dinner preparation for a date. Refer to the scenario discussions in Section 5.4.\n20\nRecipe (1/2)\nUser\nI have those products except the greens and would like to cook a meal. What could I cook?\nCould you please give me the recipe.\nAssistant\nCertainly! You can make a delicious roast beef and vegetable dinner using the ingredients you\nhave. Here\u2019s a simple recipe to get you started:\nIngredients:\n\u2022 1 lb roast beef (sliced)\n\u2022 4-6 medium carrots (peeled and chopped)\n\u2022 2-3 medium potatoes (peeled and chopped)\n\u2022 1/2 cup butter\n\u2022 Salt and pepper to taste\n\u2022 Optional: 1 tsp dried herbs (such as rosemary, thyme, or parsley)\nInstructions:\n1. Preheat your oven to 350\u00b0F (175\u00b0C).\n2. In a large roasting pan, place the sliced roast beef. Season with salt and pepper, and\nadd the optional dried herbs if desired.\n3. Arrange the chopped carrots and potatoes around the roast beef, ensuring they are\nevenly distributed.\n4. Drizzle the butter over the vegetables and roast beef, making sure it is evenly coated.\n5. Cover the roasting pan with aluminum foil and place it in the preheated oven.\n6. Roast the beef and vegetables for about 1 hour, or until the meat is cooked through and\nthe vegetables are tender.\n7. Remove the roasting pan from the oven and carefully remove the foil.\n8. Use a spatula or serving spoon to serve the roast beef and vegetables onto individual\nplates.\n9. Enjoy your delicious roast beef and vegetable dinner!\nThis recipe is a simple and hearty meal that can be enjoyed by the whole family. You can also\ncustomize the vegetables and herbs to suit your preferences, and adjust the cooking time as\nneeded to achieve the desired level of doneness.\nUser\n(Remove\nor\nChange\nObjects)\n[Referral Text\n=\n\"butter\"][Segment][Grounding\nInstruction = \"rice\"][Generate]\nAssistant\nFigure 12: Recipe (1/2). Refer to the scenario discussions in Section 5.4.\n21\nRecipe (2/2)\nUser\nI updated the picture adding rice because it is my favorite food. Could you please give me a\nnew recipe including rice.\nAssistant\nSure, here\u2019s a new recipe that includes rice:\nIngredients:\n1. 1 lb. beef (sliced into thin strips)\n2. 4 carrots (sliced)\n3. 2 potatoes (peeled and cubed)\n4. 1 cup white rice\n5. 1/4 cup chopped onion\n6. 2 cloves garlic, minced\n7. 1/4 cup soy sauce\n8. 2 tbsp. olive oil\n9. 1 tsp. sesame oil\n10. 1 tsp. cornstarch\n11. Salt and pepper to taste\n12. Chopped green onions for garnish\nInstructions:\n1. In a small bowl, whisk together the soy sauce, olive oil, sesame oil, and cornstarch.\nSet aside.\n2. Heat a large skillet or wok over medium-high heat. Add the beef and cook until\nbrowned, about 5-7 minutes. Remove the beef from the skillet and set aside.\n3. In the same skillet, add the onion and garlic and cook until the onion is translucent,\nabout 2-3 minutes.\n4. Add the carrots and potatoes to the skillet and cook for another 5-7 minutes, or until\nthe vegetables are tender but still crisp.\n5. Return the beef to the skillet and add the soy sauce mixture. Stir to combine and cook\nfor another 2-3 minutes, or until the sauce has thickened.\n6. Add the rice to the skillet and stir to combine with the beef and vegetables. Cook for\nanother 2-3 minutes, or until the rice is heated through.\n7. Season with salt and pepper to taste.\n8. Serve the beef and rice mixture over a bed of rice. Garnish with chopped green onions.\nEnjoy your delicious and hearty rice dish!\nFigure 13: Recipe (2/2). Refer to the scenario discussions in Section 5.4.\n22\nVisual Content Creation and Story Telling\nUser\ncan you describe the image?\nAssistant\nThe image features a group of five cartoon characters, each dressed as a different type of tree,\nplaying musical instruments in a forest setting. They are positioned in a line, with one character on\nthe left, two in the middle, and two on the right side of the scene. The characters are playing various\ninstruments, including a violin, a guitar, and a banjo.\nThe forest setting is enhanced by the presence of trees in the background, with one large tree on the\nleft side and another on the right side of the image. The scene appears to be a lively and whimsical\nrepresentation of a musical performance in a natural environment.\nUser\nplease write a story based on the image?\nAssistant\nOnce upon a time, in a magical forest, a group of five tree-themed characters gathered to perform a\nmusical concert. Each character was dressed in a unique costume representing their respective tree\nspecies, and they played various musical instruments with great enthusiasm.\nThe first character, a tall and slender tree, played the violin with grace and precision. The second\ncharacter, a sturdy and broad tree, strummed the guitar with a powerful and rhythmic beat. The third\ncharacter, a delicate and elegant tree, skillfully plucked the banjo, adding a touch of melody to the\nperformance. The fourth character, a small and twisted tree, played the drums with a lively and\nenergetic flair. Lastly, the fifth character, a short and bushy tree, danced and sang, captivating the\naudience with their charming presence.\nAs the concert progressed, the forest came alive with the music, and the audience of animals and\nfairies gathered around to enjoy the performance. The moon, which was full and bright, illuminated\nthe scene, casting a warm and magical glow over the entire forest.\nThe concert was a great success, and the tree-themed characters became known as the most talented\nand entertaining musicians in the land. They continued to perform together, spreading joy and\nhappiness through their music, and the forest became a place of wonder and enchantment for all\nwho visited.\nUser\nif necessary, please provide suggestion to edit the image for a more whimsical story?\nAssistant\nTo make the image more whimsical, one could consider adding more playful and imaginative\nelements to the scene. For example, the characters could be posed in a more exaggerated and\nhumorous manner, with oversized instruments or exaggerated facial expressions. The forest setting\ncould be further enhanced by adding more colorful and fantastical elements, such as glowing\nmushrooms, sparkling flowers, or even a magical waterfall.\nAdditionally, the characters could be interacting with each other in a more playful and lighthearted\nmanner, such as dancing, laughing, or even engaging in a friendly competition to see who can play\nthe best instrument. The overall atmosphere of the image could be further enhanced by adding\nmore whimsical details, such as a flying bird or a playful squirrel, to create a more imaginative and\ndelightful scene.\nUser\n(Inpaint New Objects) [Draw boxes for Grounding Instruction = \u201cglowing mushrooms; drums\u201d\n] [Generate]\nAssistant\nFigure 14: Visual content creation and story telling. Refer to the scenario discussions in Section 5.5.\n23\nScientific Education (1/2)\nUser\n(Generate New Image) [Draw box for Grounding Instruction = \"the space needle; di-\nnosaur\"][Language Instruction = \"the space needle and dinosaur\"] [Generate]\nAssistant\nUser\nCan you tell me what is in this picture?\nAssistant\nThe image features a large green dinosaur statue standing next to a tall tower, possibly the Space\nNeedle. The dinosaur appears to be a T-Rex, and it is positioned in front of the tower, creating\nan interesting contrast between the two structures. The scene is set against a backdrop of a pink\nsky, adding a unique and vibrant atmosphere to the image.\nUser\nWhat is a T-rex?\nAssistant\nA T-Rex, short for Tyrannosaurus Rex, is a large carnivorous dinosaur that lived during the Late\nCretaceous period, around 68 to 65 million years ago. It is one of the most well-known and\niconic dinosaurs due to its massive size, powerful jaws, and distinctive appearance. T-Rex had\na small head, large eyes, and a long, sturdy tail. It was a dominant predator in its ecosystem,\nfeeding on a variety of prey, including other dinosaurs, such as Triceratops and Edmontosaurus.\nT-Rex is often depicted in popular culture, including movies, television shows, and video games,\nas a fearsome and relentless predator. However, in reality, it is believed that T-Rex had a\nrelatively slow and lumbering gait, and it may have been more of a scavenger than an active\nhunter. Despite its fearsome reputation, T-Rex was not the largest land predator that ever existed;\nthat title belongs to the Spinosaurus, a dinosaur from the same time period.\nUser\nWhy is the sky pink?\nAssistant\nThe sky is pink due to the presence of clouds in the atmosphere, which are reflecting the sunlight.\nThis phenomenon occurs when sunlight passes through the Earth\u2019s atmosphere and interacts\nwith the molecules in the air, causing them to scatter the light. The scattering process is more\npronounced for shorter wavelengths, such as blue and violet, than for longer wavelengths, like\nred and orange. As a result, the sky appears pink during sunset or sunrise, when the sun is low\nin the sky and the light has to travel through more of the Earth\u2019s atmosphere.\nUser\nIf I want this picture to represent midday, not during sunset or sunrise, what color should the\nsky be?\nAssistant\nIf you want the picture to represent midday, the sky should be blue. During midday, the sun\nis at its highest point in the sky, and the light has to travel through the least amount of the\nEarth\u2019s atmosphere. This results in the scattering of shorter wavelengths, such as blue and violet,\nbeing less pronounced, allowing the longer wavelengths, like red and orange, to dominate the\nsky\u2019s color. In this case, the sky would appear blue, providing a more natural and accurate\nrepresentation of the time of day.\nUser\n(Change and Remove Objects) [Referral Text = \"T-rex\"][Segment]\nAssistant\nFigure 15: Scientific Education (1/2). Refer to the scenario discussions in Section 5.6.\n24\nScientific Education (2/2)\nUser\nI want to replace the dinosaur with something else. Any suggestions?\nAssistant\nCertainly! If you would like to replace the dinosaur with something else, you could consider\nthe following options: 1. A giant robot or mech: This could be a futuristic or retro-inspired\ndesign, adding a sense of wonder and excitement to the scene. 2. A massive sculpture of a\nfamous historical figure: This could be a person from history, mythology, or even a fictional\ncharacter, such as a superhero or a popular movie character. 3. A giant, intricate artwork or\nmural: This could be a colorful, abstract, or representational piece that adds visual interest\nand artistic flair to the area. 4. A large, interactive exhibit or installation: This could be an\nimmersive experience that engages visitors, such as a virtual reality exhibit, an interactive art\npiece, or a themed playground. Ultimately, the choice of replacing the dinosaur with something\nelse depends on the context, the desired atmosphere, and the preferences of the local community\nor visitors.\nUser\n(Change and Remove Objects) [Grounding Instruction = \"a giant robot\"][Generate]\nAssistant\nUser\nWhat kind of robot is that?\nAssistant\nThe image features a large robotic crane, which is a type of construction equipment used for\nlifting and moving heavy objects.\nUser\n(Change and Remove Objects) [Generate]\nAssistant\nUser\nWhat kind of robot is that?\nAssistant\nThe robot in the image is a large, green, metallic robot with a humanoid shape.\nUser\nWhat is this type of robot used for?\nAssistant\nThe large green robot is a mechanical robot, which is typically used for tasks that require\nstrength, precision, or the ability to perform repetitive actions. These robots can be found in\nvarious industries, such as manufacturing, construction, and even entertainment.\nUser\nThank you so much for the education.\nAssistant\nYou\u2019re welcome! I\u2019m glad I could help. If you have any more questions, feel free to ask.\nFigure 16: Scientific Education. (2/2) Refer to the scenario discussions in Section 5.6.\n25\nEducation: Cartoon Reading.\nUser\nUser\nWhat does this editorial cartoon mean?\nAssistant\nThe editorial cartoon depicts a man in a suit, who appears to be a reporter or a news anchor,\nbeing tied up with a ball and chain. This image symbolizes the idea of being restricted or\nlimited in one\u2019s freedom of expression or movement, possibly due to censorship or other forms\nof control. The man\u2019s profession as a reporter or news anchor suggests that the cartoon might\nbe commenting on the challenges faced by journalists in certain situations, where they may be\nsubjected to restrictions or limitations on their ability to report the news freely and accurately.\nThe image serves as a metaphor for the challenges faced by journalists in their pursuit of truth\nand the importance of protecting their freedom of expression.\nUser\n(Change and Remove Objects) [Visual prompt by stroking on on \"tie\"][Segment][Drag to\nremove][Generate]\nAssistant\nUser\nWhat does this editorial cartoon mean?\nAssistant\nThe editorial cartoon features a man in a suit and tie, who is being pulled by a giant rubber band.\nThe rubber band is attached to the man\u2019s waist, and it appears to be stretched to its limit. The\nimage is a black and white drawing, which adds to the dramatic effect of the scene.The meaning\nof this editorial cartoon could be interpreted in various ways, depending on the context in which\nit was created. It could symbolize the challenges or burdens that the man might be facing, such\nas financial or professional responsibilities. The rubber band could represent the pressure or\nstress that the man is experiencing, and the fact that it is stretched to its limit might indicate that\nhe is struggling to cope with these challenges. Alternatively, the image could also be interpreted\nas a humorous representation of the man\u2019s size or the consequences of overeating. The cartoon\nmight be commenting on the man\u2019s weight or the societal expectations of professionalism and\nappearance. In any case, the cartoon is a powerful visual representation of the challenges and\npressures that the man might be facing, and it encourages the viewer to reflect on the underlying\nmessage and its relevance to their own experiences or the broader context in which the cartoon\nwas created.\nFigure 17: Education: Cartoon Reading. Refer to the scenario discussions in Section 5.6.\n26\nInterior Design: Large Living Room (1/2)\nUser\n(Generate New Image) [Language Instruction = \"Living room with minimum furniture\nand nice lighting\"][Generate]\nAssistant\nUser\nWhat do you think about this room. How could I improve it to make it more modern?\nAssistant\nThe room appears to be a cozy living room with a couch, a coffee table, a TV, and a potted\nplant. To make it more modern, you could consider updating the furniture and decor to reflect\ncontemporary styles. For example, you could replace the couch and coffee table with sleek,\nminimalist designs, and add a stylish rug to the floor. Additionally, you could update the TV\nto a slim, flat-screen model and consider adding some modern artwork or decorative items on\nthe walls. To enhance the room\u2019s ambiance, you could also introduce more potted plants or\ngreenery, and consider using a combination of neutral and warm colors for the walls, curtains,\nand other textiles. Finally, adding some ambient lighting, such as floor lamps or table lamps,\ncan create a more inviting and modern atmosphere in the living room.\nUser\n(Remove or Change Object) [Referral text = \"sofa\"] [Segment][Grounding Instruction\n= \"Contemporary light sofa\"] [Generate]\nAssistant\nUser\n(Remove or Change Object) [Referral text = \"coffee table\"] [Segment][Grounding\nInstruction = \"contemporary coffee table\"] [Generate]\nAssistant\nFigure 18: Interior Design: Large Living Room (1/2). Refer to the scenario discussions in Section 5.7.\n27\nInterior Design: Large Living Room (2/2)\nUser\n(Remove or Change Object) [Visual prompt by stroking on chair] [Segment][Grounding\nInstruction = \"Potted plant\"] [Generate]\nAssistant\nUser\n(Inpaint New Objects) [Draw box for [Grounding Instruction = \"Art work by Gustav\nKlimt\"] [Generate]\nAssistant\nUser\nI have replaced the couch, coffee table, added potted plant and added modern painting to this\nroom. I started to like it. What do you think?\nAssistant\nIt sounds like you have made some great improvements to the living room, and the addition\nof the modern painting, sleek furniture, and contemporary decor has likely transformed the\nspace into a more modern and stylish environment. The potted plant and rug also add to\nthe overall aesthetic, creating a warm and inviting atmosphere. It\u2019s always great to hear that\nsomeone is pleased with their living space, as it reflects their personal taste and preferences. The\ncombination of these elements has likely resulted in a more comfortable and visually appealing\nliving room that you can enjoy spending time in.\nFigure 19: Interior Design: Large Living Room (2/2). Refer to the scenario discussions in Section 5.7.\n28\nInterior Design: Small Living Room (1/2)\nUser\n(Generate New Image) [Language Instruction = \"Living room with sofa and good light-\ning\"][Generate]\nAssistant\nUser\nHow could I improve this room to make it more modern and cozy?\nAssistant To make the room more modern and cozy, you can consider the following improvements:\n1. Upgrade the furniture: Replace the couch and chair with more contemporary designs,\nsuch as sleek lines and neutral colors. You can also add a stylish coffee table and side\ntables to complement the seating area.\n2. Update the lighting: Replace the current lighting fixtures with more modern and energy-\nefficient options, such as LED lights or smart lighting systems. You can also add floor\nlamps or table lamps to create a warm and inviting atmosphere.\n3. Enhance the window treatments: Replace the current window treatments with more\nmodern and functional options, such as curtains, blinds, or shades. You can also consider\nadding a window seat or a bench to create a cozy seating area near the window.\n4. Add decorative elements: Introduce more decorative elements, such as artwork, plants, or\ndecorative pillows, to make the room feel more personalized and welcoming.\n5. Incorporate technology: Add a smart TV or a sound system to the room to create a more\nmodern and entertainment-focused space.\nBy implementing these changes, you can transform the living room into a more modern and cozy\nspace that reflects your personal style and preferences.\nUser\n(Remove or Change Object) [Referral Text = \"sofa\"] [Segment][Grounding Instruction\n=\"Contemporary sofa\"] [Generate]\nAssistant\nUser\n(Remove or Change Object) [Referral Text = \"White table\"] [Segment][Grounding\nInstruction = \"modern coffee table\"] [Generate]\nAssistant\nUser\n(Insert New Objects) [Draw box for Grounding Instruction = \"Potted plant\"] [Generate]\nAssistant\nFigure 20: Interior Design: Small Living Room (1/2). Refer to the scenario discussions in Section 5.7.\n29\nInterior Design: Small Living Room (2/2)\nUser\nI\u2019ve changed some furniture in the room. Do you think it is good idea to lower the TV?\nAssistant\nYes, lowering the TV in the living room is a good idea. In the image, the TV is currently\nmounted on the wall at a higher position. Lowering the TV to a more comfortable viewing\nheight can improve the overall viewing experience for the people in the room. This can help\nreduce strain on the neck and eyes, especially when watching the TV from the couch or other\nseating areas. Additionally, lowering the TV can make it easier for people to see the screen,\nespecially if they are sitting at a lower angle or distance from the TV.\nUser\n(Remove or Change Object) [Referral Text = \"TV\"] [Segment][Drag segmented object to a\nlower position][Grounding Instruction = \"TV\"] [Generate]\nAssistant\nUser\nI would like to replace the ceiling lamp. Do you think it is good idea?\nAssistant\nYes, it could be a good idea to replace the ceiling lamp in the living room. The current lamp\nappears to be old and has a yellowish hue, which might not provide the desired lighting effect\nor ambiance. Replacing it with a modern, energy-efficient lamp or a more visually appealing\ndesign can enhance the overall aesthetics of the living room and create a more comfortable\nand inviting atmosphere. Additionally, upgrading the lighting can also improve the room\u2019s\nfunctionality and energy efficiency.\nUser\n(Remove\nor\nChange\nObject)\n[Referral Text\n=\n\"Lamp\"]\n[Segment][Grounding\nInstruction = = \"contemporary chandelier\"] [Generate]\nAssistant\nFigure 21: Interior Design: Small Living Room (2/2). Refer to the scenario discussions in Section 5.7.\n30\nIdentification of Unusual and Risky Items.\nUser\nUser\nwhat is unusual about this image? is it dangerous?\nAssistant\nThe unusual aspect of this image is that a man is ironing clothes while standing on the back of a\nmoving car. This is dangerous because ironing clothes typically requires a stable surface and\nproper safety measures, such as using an ironing board and wearing appropriate protective gear.\nStanding on the back of a moving car is not only unstable but also poses a significant risk to the\nman\u2019s safety, as he could lose his balance and fall off the car, potentially causing severe injuries.\nAdditionally, the man\u2019s actions could distract the driver, increasing the risk of an accident.\nUser\n(Remove or Change Object) [Referral Text = \"person\"][Visual prompt by stroking on \"leg\"]\n[Segment][Drag segmented objects to remove] [Generate]\nAssistant\nUser\nwhat is unusual about this image? is it dangerous?\nAssistant\nThe unusual aspect of this image is that a yellow SUV is driving down the street with a chair\nstrapped to its roof. This is not a typical sight, as it is not common for people to transport\nfurniture, such as chairs, on the roof of their vehicles while driving. The situation could be\npotentially dangerous, as the chair might not be securely fastened, and it could fall off the\nvehicle while in motion, causing accidents or damage to the chair and other vehicles on the road.\nAdditionally, the driver\u2019s visibility might be obstructed by the chair, making it difficult to see\nother vehicles or pedestrians, which could also lead to accidents.\nUser\n(Remove or Change Object) [Visual prompt by stroking on \"yellow taxi\"] [Seg-\nment][Grounding Instruction = \"flowers\"] [Generate]\nAssistant\nUser\nwhat is unusual about this image? is it dangerous?\nAssistant\nThe unusual aspect of this image is the man riding a bicycle that is carrying a large, colorful\nfloral arrangement, which appears to be a giant bouquet of flowers. This is not a common sight,\nas people typically do not transport such large and heavy objects on bicycles. The man is also\nwearing a yellow shirt, which adds to the uniqueness of the scene. The presence of a car in the\nbackground further emphasizes the contrast between the man\u2019s bicycle and the typical mode of\ntransportation seen on the street. The combination of the man\u2019s attire, the bicycle, and the giant\nbouquet of flowers creates an interesting and eye-catching image.\nFigure 22: Identification of Unusual and Dangerous Items. Refer to the scenario discussions in\nSection 5.8.\n31\n"
  },
  {
    "title": "Controllable Music Production with Diffusion Models and Guidance Gradients",
    "link": "https://arxiv.org/pdf/2311.00613.pdf",
    "upvote": "23",
    "text": "Controllable Music Production with Diffusion Models\nand Guidance Gradients\nMark Levy\nApple\nmark_levy@apple.com\nBruno Di Giorgi\nApple\nbdigiorgi@apple.com\nFloris Weers\nApple\nfloris_weers@apple.com\nAngelos Katharopoulos\nApple\na_katharopoulos@apple.com\nTom Nickson\nApple\ntnickson@apple.com\nAbstract\nWe demonstrate how conditional generation from diffusion models can be used to\ntackle a variety of realistic tasks in the production of music in 44.1kHz stereo audio\nwith sampling-time guidance. The scenarios we consider include continuation,\ninpainting and regeneration of musical audio, the creation of smooth transitions be-\ntween two different music tracks, and the transfer of desired stylistic characteristics\nto existing audio clips. We achieve this by applying guidance at sampling time in a\nsimple framework that supports both reconstruction and classification losses, or\nany combination of the two. This approach ensures that generated audio can match\nits surrounding context, or conform to a class distribution or latent representation\nspecified relative to any suitable pre-trained classifier or embedding model. Audio\nsamples are available at https://machinelearning.apple.com/research/controllable-\nmusic.\n1\nIntroduction\nRecent work has shown great progress in addressing the challenging problem of generating musical\naudio with high enough quality for real world applications. Language modelling approaches such as\nMusicLM and MusicGen [1, 2] tackle the problem of sequence length by working with compressed,\ntokenised representations originally developed for efficient audio encoding, and by cascading coarse\nand fine models to achieve realistic sounding audio at up to 32kHz. Diffusion models such as Mo\u00fbsai\nand Noise2Music [3, 4] also show promising results, again using a cascade of models. These systems\nfocus on conditional generation from descriptions of the desired content, such as \u201ca calming violin\nmelody backed by a distorted guitar riff\u201d. This allows the creation of an impressive variety of\nsounds but limits control over the musical output to generalised concepts, and conditioning relies\non suitable paired data being available at training time. In our work, we scale up waveform and\nlatent diffusion to reach high audio quality, and then explore some of the approaches to creative\nediting that become possible with a pre-trained diffusion model. Noting the similarity between the\nreconstruction guidance of [5] and the classifier guidance first introduced in [7], we combine both in a\nsingle framework, allowing us to tackle a wide range of useful music production tasks where control\nis provided in the form of example audio. Conditioning on audio prompts provides intuitive and\nfine-grained control over the musical characteristics of generated output, while applying conditioning\nwith guidance gradients at sampling time removes the requirement to have paired data when training\nour diffusion models. By analogy with previous similar work on controllable image modification, we\nsee huge potential for music production incorporating a diffusion model as a generative prior, and our\nwork only scratches the surface of possible methods and applications.\nPreprint. Under review.\narXiv:2311.00613v2  [cs.SD]  5 Dec 2023\n2\nRelated work\n2.1\nCreative editing with diffusion models\nDiffusion models for high resolution images now provide the basis for a wide range of methods\nfor creative editing of photos and videos. Inpainting can be implemented most simply by replacing\nthe estimate of unmasked regions required during sampling with the original pixel values [8]. This\napproach can be improved by fine-tuning with masking, or even training from scratch with a suitable\nmasking strategy [9, 10]. Individual elements of an image can be edited starting from a rough\nguide, where for example part or all of the desired final image is provided in the form of a sketch\nor shape mask [11]. If a paired dataset of sketch and image examples is available, then a separate\ncontrol network can be fine-tuned to provide conditioning at sampling time [12]. With models\ntrained to generate images conditioned on the embedding of a text prompt, semantic editing can be\nimplemented by modifying the prompt and generating pixels within an explicitly specified region, or\nby manipulating cross-attention maps between text and image [13\u201315].\nA related line of research uses diffusion models to solve inverse problems, where the aim is to\nreconstruct a complex signal as precisely as possible from a degraded version [16, 5, 10]. While\nthe motivation for these methods is reconstruction of missing or corrupted data, they can easily be\napplied to creative settings, where we wish to regenerate some part of an original signal.\n2.2\nDiffusion models for musical audio\nDiffusion models have been applied to music by fine-tuning a pre-trained image model to generate\naudio spectrograms by treating them directly as images [17, 18]. Most recent work focuses on\ndiffusion models trained directly on audio signals to ensure higher quality results. CRASH works\ndirectly in the waveform domain to generate short drum hits at 44.1kHz [19]. Noise2Music trains a\ncascade of diffusion models to generate 30 second clips of 24kHz audio, conditioned on text prompts\nlearned from a large paired dataset of music and synthetic pseudo-labels [4]. Mo\u00fbsai introduces a\nlatent diffusion model for audio, using an independently trained diffusion autoencoder to compress\nspectrograms and then generating in the resulting latent domain [20, 3]. CQTDiff applies previous\nwork on inverse problems to audio reconstruction with a diffusion model operating in the Constant-Q\ntransform domain, including inpainting short sections of piano music recorded at 22kHz [21].\n3\nConditional generation with guidance gradients\nSimilarly to [22], we formalise conditional sampling as a multi-objective optimization problem:\nmax\nx\nJ1(x) = pdata(x)\nmin\nx\nJ2(x) = d(y, A(x))\n(1)\nwhere x \u2208 Rn is the sample, A \u2208 Rn \u2192 V is a non-linear, differentiable measurement operator,\ny \u2208 V is the measured output and d is a distance function V\u00d7V \u2192 R+. J1 is maximised for samples\nthat have high probability under the data distribution pdata and is solved using the reverse process of a\ndiffusion model. J2 is minimized for samples that are consistent with the measurement y.\nDiffusion models solve the first sub-problem with an iterative algorithm, starting from noisy xT \u223c\nN(0, I) and refining the sample xt at every iteration, with t decreasing from T to 0. It is therefore a\nnatural choice to solve the second problem by including a gradient descent step xt := xt\u2212\u03be\u2207xtJ2(xt)\nat each iteration of the denoising process, where \u03be is the step size. This strategy can be interpreted as\nthe alternation of unconditional updates and projections towards the measurement subspace [5].\nDepending on A, specific variations can be applied. When A is defined for noiseless inputs only, a\ndifferent \u201cdenoising\u201d measurement operator A\u2032 can be used instead. A\u2032 is obtained as the composition\nof the original measurement operator A and the differentiable denoising operator of the diffusion\nmodel \u02c6x0(xt), which estimates the noiseless sample at each iteration. When A is linear A(x) =\nAx with A \u2208 Rm\u00d7n having full row rank, we can apply a data consistency step xt := xt +\nAT (AAT )\u22121(y\u2212Axt) at the end of each iteration, which exactly projects xt onto the measurement\nsubspace y = Ax. The resulting algorithms are provided in Section B of the appendix.\n2\nUnconditional\nInfill/Regeneration\nContinuation\nTransitions\nCl. Guidance\nModel\nSampling\nSteps\nFAD\u2193\nKLD\u2193\nFAD\u2193\nKLD\u2193\nMR\u2193\nFAD\u2193\nKLD\u2193\nMR\u2193\nRealism\u2191\nFAD\u2193\nKLD\u2193\nLatent\nDDIM\n50\n1.13\n0.33\n0.45/0.53\n0.19/0.19\n2.68/1.76\n0.49\n0.22\n3.11\n0.95\u00b10.12\n1.22\n0.30\nDDPM\n50\n1.10\n0.34\n0.39/0.42\n0.17/0.17\n2.63/2.30\n0.50\n0.18\n3.00\n0.95\u00b10.12\n1.31\n0.21\n500\n0.72\n0.28\n0.55/0.49\n0.18/0.19\n2.63/1.72\n0.46\n0.20\n3.03\n0.94\u00b10.10\n0.96\n0.08\nWaveform\nDDPM\n50\n8.42\n1.72\n1.36/0.47\n0.36/0.25\n1.67/2.87\n2.73\n0.93\n4.42\n1.05\u00b10.17\n8.68\n1.57\n500\n4.35\n1.35\n0.56/0.58\n0.28/0.27\n2.84/2.69\n0.77\n0.72\n3.82\n1.00\u00b10.14\n6.28\n0.30\nCQTDiff\nDDPM\n50\n1.98/ \u2013\n0.42/ \u2013\n3.10/ \u2013\nVAE\n0.47\n0.13\n0.51\n0.15\n1.48\n0.40\n0.12\n1.48\n0.96\u00b10.09\n0.47\n0.13\nTest Set\n0.23\n0.04\n0.25\n0.08\n0.00\n0.18\n0.07\n0.00\n1.00\u00b10.00\n0.23\n0.04\nTable 1: Experimental results\n4\nExperiments\n4.1\nModel architectures\nWe train two classes of diffusion models, a waveform model and a latent diffusion model. Our\nwaveform model is a one-dimensional Unet [23] with 440M parameters. Our latent diffusion model\ncomprises a Variational Autoencoder (VAE) with a downsampling ratio of 128 in the time dimension\nand a transformer diffusion model [24] with 1B parameters. See Section C of the appendix for full\ndetails. As a baseline we also train a CQT diffusion model [21], using the authors\u2019 reference code.\n4.2\nDataset and metrics\nWe train all our diffusion models on the Free Music Archive dataset [25], which consists of 100k\ntracks totalling 8k hours of music. We hold out 80 hours as a test set.\nTo evaluate the quality of the generated audio we compute the Fr\u00e9chet Audio Distance (FAD) [26] in\nthe VGGish [27] embedding space; and the KL Divergence [28] in the AudioSet class space [29],\nusing the Patchout classifier [30]. Both metrics are computed with respect to reference statistics\ncomputed on the training set.\n4.3\nCreative applications\nWe evaluate our models in a set of creative applications described below (further details in Section\nB.2 of the appendix). Results are shown in Table 1.\nUnconditional generation: We sample 7k five second audio clips starting from Gaussian noise. As a\nreference we include the scores obtained by evaluating the clips in the test set and their reconstruction\nusing the VAE, which serves as an upper bound for the Latent model.\nContinuation: We take the first 2.4s from each test set example, and use the model to reconstruct a\npossible continuation up to 6s. We include the mel-reconstruction distance [31] (\u201cMR\u201d in Table 1) as\na measure of consistency between the generated and the original continuation.\nInfill / Regeneration: An audio segment with duration equal to 6s is extracted from each test set\nexample. In the infill task, the middle 2 seconds of each segment are masked out and then generated\nusing the model. In the regeneration task the original audio is partially noised using the forward\nprocess, which ensures that basic rhythmic structure is maintained while other details are obscured,\nand used as the starting sample instead of Gaussian noise. In both cases the left and right contexts are\nused to condition the generation as done in the continuation task.\nTransitions: The transition task is a variant of the regeneration task: we start with two different\ntracks on the left and the right sides, and a 2.5s constant-power crossfade between them in the middle\nsegment to be regenerated. Ideally we want the regenerated section to sound musical even when\nthe raw crossfade contains rhythmic and harmonic clashes. We evaluate this with the realism score\nintroduced in [32], normalised track-by-track by the score of the constant-power crossfade. We\nevaluate this task over a set of 100 transitions between randomly extracted pairs of tracks, with the\nreference manifold computed by projecting the training set on the Patchout classifier\u2019s embedding\nspace [30].\n3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative transition time\n1\n2\n3\n4\nMel-spectrogram distance\nsignal: gain-crossfade\nsignal: VAE\nmodel: Latent\nmodel: Waveform\nFigure 1: Average mel-spectrogram dis-\ntance of generated transitions with respect\nto the first track, over the duration of the\ntransition. The distance increases linearly\nfor all methods, implying smooth transi-\ntions. The ML-generated transitions start\nat non-zero distance because of the imper-\nfect reconstruction.\nIn\ufb01ll\nRegenerate\nContinuation\nTransition\nGuidance\n0\n100\n200\n300\nNumber of wins\nOriginal\nLatent\nWaveform\nCQTDi\ufb00\nFigure 2: Subjective evaluation\nresults.\nWe show the number\nof wins in head-to-head prefer-\nence comparisons between sam-\nples generated by each class of\nmodel and also the original i.e.\nthe prompt audio itself.\nWe illustrate transition smoothness in Fig. 1, which shows the average mel-reconstruction error\nrelative to the first track over the duration of the transition.\nClassifier guidance: A common technique to improve the realism of conditionally generated samples\nis to use pre-trained classifiers to compute the gradients. We use the gradient of the L2 loss on the\nembedding space of the Patchout classifier [30] to guide our generation.\n4.4\nSubjective evaluation\nWe run a blind pairwise comparison test where we present the rater two samples generated by two mod-\nels (or one sample vs the reference audio) for the same audio prompt and creative task. The raters are\nasked to choose the preferred sample from each pair based on perceived quality. We also encourage the\nreader to listen to the samples available at https://machinelearning.apple.com/research/controllable-\nmusic.\n4.5\nDiscussion\nResults in Table 1 show that, regardless of the sampling method, the model operating in a latent space\ngenerally produces musical audio that is higher quality and closer to the original than the waveform\ndiffusion model. This general trend confirms that cascading generation is beneficial, as previously\nsuggested in [4]. Both models improve over infilling with the baseline CQTDiff model by a large\nmargin. We summarise the results of the subject evaluation in Fig. 2. These confirm the superiority\nof our models over the baseline and suggest that the latent representation gives an advantage in tasks\nwhere a data consistency step is unavailable (guidance) or weaker (continuation, where we have only\na one-sided prompt).\n5\nConclusions\nIn this paper we explored a simple framework that enables different applications of diffusion gen-\nerative models in the context of high-fidelity music production. Applications such as continuation,\ninpainting, regeneration, transition and latent conditioning have been evaluated for two model ar-\nchitectures showing the relative importance of architectural and sampling choices for the different\ntasks.\n4\nReferences\n[1] Andrea Agostinelli, Timo I. Denk, Zal\u00e1n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour,\nand Christian Frank. MusicLM: Generating music from text, 2023.\n[2] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and\nAlexandre D\u00e9fossez. Simple and controllable music generation, 2023.\n[3] Flavio Schneider, Zhijing Jin, and Bernhard Sch\u00f6lkopf. Mo\u00fbsai: Text-to-music generation with\nlong-context latent diffusion, 2023.\n[4] Qingqing Huang, Daniel S. Park, Tao Wang, Timo I. Denk, Andy Ly, Nanxin Chen, Zhengdong\nZhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, Jesse Engel, Quoc V. Le, William Chan,\nZhifeng Chen, and Wei Han. Noise2Music: Text-conditioned music generation with diffusion\nmodels, 2023.\n[5] Hyungjin Chung, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye.\nDiffusion posterior sampling for general noisy inverse problems, 2023.\n[6] Eloi Moliner, Jaakko Lehtinen, and Vesa V\u00e4lim\u00e4ki. Solving Audio Inverse Problems with a\nDiffusion Model, 2022.\n[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis.\nAdvances in Neural Information Processing Systems, 34:8780\u20138794, 2021.\n[8] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data\ndistribution, 2020.\n[9] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[10] Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alexandros G. Dimakis, and\nAdam Klivans. Ambient diffusion: Learning clean distributions from corrupted data, 2023.\n[11] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano\nErmon. SDEdit: Guided image synthesis and editing with stochastic differential equations.\narXiv preprint arXiv:2108.01073, 2021.\n[12] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels, 2023.\n[13] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of\nnatural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18208\u201318218, 2022.\n[14] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,\nand Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007\u20136017,\n2023.\n[15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\n[16] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration\nmodels. Advances in Neural Information Processing Systems, 35:23593\u201323606, 2022.\n[17] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, June\n2022.\n5\n[18] Seth Forsgren and Hayk Martiros. Riffusion - Stable diffusion for real-time music generation.\n2022.\n[19] Simon Rouard and Ga\u00ebtan Hadjeres. CRASH: Raw Audio Score-based Generative Modeling\nfor Controllable High-resolution Drum Sound Synthesis, June 2021. arXiv:2106.07431 [cs,\neess].\n[20] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn.\nDiffusion autoencoders: Toward a meaningful and decodable representation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10619\u201310629,\n2022.\n[21] Eloi Moliner, Jaakko Lehtinen, and Vesa V\u00e4lim\u00e4ki. CQT-Diff: Solving Audio Inverse Problems\nwith a Diffusion Model, November 2022. arXiv:2210.15228 [cs, eess].\n[22] Hyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated MRI. Medical\nimage analysis, 80:102479, 2022.\n[23] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks\nfor biomedical image segmentation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9,\n2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.\n[24] William Peebles and Saining Xie. Scalable Diffusion Models with Transformers, 2022.\n[25] Micha\u00ebl Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. Fma: A dataset\nfor music analysis. In 18th International Society for Music Information Retrieval Conference,\nnumber CONF, 2017.\n[26] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fr\u00e9chet audio distance:\nA metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466, 2018.\n[27] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Chan-\nning Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. CNN architectures\nfor large-scale audio classification. In 2017 IEEE International Conference on Acoustics, Speech\nand Signal Processing, pages 131\u2013135. IEEE, 2017.\n[28] Vladimir Iashin and Esa Rahtu. Taming visually guided sound generation. arXiv preprint\narXiv:2110.08791, 2021.\n[29] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Chan-\nning Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled\ndataset for audio events. In 2017 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 776\u2013780, 2017.\n[30] Khaled Koutini, Jan Schl\u00fcter, Hamid Eghbal-zadeh, and Gerhard Widmer. Efficient training of\naudio transformers with patchout. In Interspeech 2022. ISCA, sep 2022.\n[31] Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, and Bryan Pardo. VampNet: Music\ngeneration via masked acoustic token modeling, 2023.\n[32] Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved\nprecision and recall metric for assessing generative models. Advances in Neural Information\nProcessing Systems, 32, 2019.\n[33] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya\nSutskever. Jukebox: A Generative Model for Music, April 2020. arXiv:2005.00341 [cs, eess,\nstat].\n[34] Curtis Hawthorne, Andrew Jaegle, C\u02d8at\u02d8alina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz\nMalinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, Hannah Sheahan,\nNeil Zeghidour, Jean-Baptiste Alayrac, Jo\u00e3o Carreira, and Jesse Engel. General-purpose,\nlong-context autoregressive modeling with perceiver ar, 2022.\n6\n[35] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and\nYonghui Wu. w2v-BERT: Combining contrastive learning and masked language modeling for\nself-supervised speech pre-training, 2021.\n[36] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. MaskGIT: Masked\ngenerative image transformer, 2022.\n[37] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\nIn International Conference on Learning Representations, 2021.\n[38] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. In\nInternational Conference on Learning Representations, 2020.\n[39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In\nInternational Conference on Learning Representations, 2020.\n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nStableDiffusion: High-Resolution Image Synthesis with Latent Diffusion Models, April 2022.\narXiv:2112.10752 [cs].\n[41] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining\nXie. A ConvNet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 2022.\n[42] Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. Parallel wavegan: A fast waveform gen-\neration model based on generative adversarial networks with multi-resolution spectrogram. In\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 6199\u20136203. IEEE, 2020.\n[43] Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High Fidelity Neural Audio\nCompression, October 2022. arXiv:2210.13438 [cs, eess, stat].\n[44] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position repre-\nsentations. arXiv preprint arXiv:1803.02155, 2018.\n7\nA\nOther related work\nA.1\nLanguage models for music generation\nWe focus in this paper on diffusion models, but there is a growing body of parallel work on language\nmodelling for musical audio, operating in the discrete token space of VQ-VAEs. Jukebox uses a\ncascade of three transformers trained with metadata and song lyrics as well as encoded audio, to\nenable generation conditioned on song texts and artist identity [33]. PerceiverAR and MusicGen\nmodel interleaved sequences of audio tokens from the multiple residual VQ-VAE codebooks of\nneural codecs, while MusicLM extends this approach by conditioning on \u201csemantic\" tokens from a\npre-trained first stage model which models audio at a lower sample rate [34, 1, 2, 35]. MusicLM is\nconditioned on text captions and also on melody, by concatenating automatically extracted melody\ntokens with the text tokens, and this supports generation of samples conditioned on a provided melody\nas well as a text prompt.\nIn contrast to these autoregressive models, VampNet introduces a language model trained on a masked\ntoken prediction objective which supports inpainting, and also employs an efficient iterative sampling\nscheme to generate interleaved codebook tokens [31, 36]. The authors experiment with enforcing the\nrhythmic consistency of generated sections by retaining tokens around the expected beat positions in\nthe region to be inpainted.\nB\nConditional generation\nB.1\nDiffusion models\nA Gaussian denoising diffusion probabilistic model (DDPM) defines a forward corruption process\nthrough time, which gradually applies noise to real data x0 \u223c pdata\nq(xt|x0) = N(xt; \u03b1tx0, \u03c32\nt I),\nwhere \u03b1t and \u03c3t are constants defining a noise schedule with monotonically decreasing signal-to-\nnoise ratio \u03b1t/\u03c3t, such that xT \u223c N(0, I). We use a cosine schedule with continuous t \u2208 [0, 1],\n\u03b1t = cos(\u03c0t/2) and \u03b12\nt + \u03c32\nt = 1. We can sample noised data directly with xt = \u03b1tx0 + \u03c3t\u03f5t and\n\u03f5t \u223c N(0, I).\nDDPMs are trained to learn a reverse process p\u03b8(xs|xt) = N(\u00b5\u03b8(xt, t), \u03c3t\u2192s) where \u03c3t\u2192s =\n(\u03c32\ns/\u03c32\nt )(1 \u2212 \u03b12\nt /\u03b12\ns) are constants computed from the noise schedule and \u00b5\u03b8 is the output of\na denoising model that estimates x0 or equivalently \u03f5t. We choose the so called v prediction\nparameterization for our estimator, learning to predict a combination of the added noise and the\noriginal signal vt = \u03b1t\u03f5t \u2212 \u03c3tx0 [37]. We train our model to minimise a reconstruction loss\nL(\u03b8) = Et\u223cU(0,1),x0\u223cpdata,\u03f5\u223cN (0,I)\nh\r\rvt \u2212 v\u03b8(xt, t)\n\r\r2i\n.\nSampling from a DDPM starts with Gaussian noise xT and uses p\u03b8(xs|xt) to produce gradually\nless noisy samples until reaching a final sample x0. An important result from [38] is that the output\nof the denoising model is equivalent to estimating an evidence lower bound on the score function\n\u2207xt log p(xt), which enables the use of samplers derived from denoising score matching, such as\nDDIM [39].\nB.2\nSampling with guidance gradients\nWith reference to Sect. 3, Algorithms 1 and 2 illustrate the DDPM and DDIM iterative methods\nfor conditional generation. We model different conditional generation tasks by varying specific\nparameters of the algorithm: the measurement operator A, the distance function d and the starting\nsample xT , as shown in Table 2.\n8\nTask\nA(x)\nd(y, A(x))\ny,\nxT\nContinuation\nA(x) = Ax,\nA = AL\n\u2225y \u2212 Ax\u22251\ny = A\u00afx\nxT = AT y + (I \u2212 AT A)z\nInfill\nA(x) = Ax,\nA =\n\"\nAL\nAR\n#\nas Continuation\nas Continuation\nRegenerate\nas Infill\nas Continuation\ny = A\u00afx\nxT = AT y + (I \u2212 AT A)(kz + (1 \u2212 k)\u00afx)\nTransition\nas Infill\nas Continuation\n\u00afx =\n\uf8ee\n\uf8ef\uf8f0\nAL\u00afxL\nFout\u00afxL + Fin\u00afxR\nAR\u00afxR\n\uf8f9\n\uf8fa\uf8fb\ny = A\u00afx\nxT = AT y + (I \u2212 AT A)(kz + (1 \u2212 k)\u00afx)\nFout = [0, diag(fout), 0]\nFin = [0, diag(fin), 0]\nEmbedder guidance\nA \u2208 Rn \u2192 Rm\n\u2225y \u2212 Ax\u22252\ny = A(\u00afx),\nxT = z\nClassifier guidance\nA(x)i = p(ci|x) \u2208 [0, 1]m\nBCE(y, A(x))\ny \u2208 [0, 1]m,\nxT = z\nTable 2: Task specific parameters. AL = [ICL, 0] \u2208 RCL\u00d7n, AR = [0, ICR] \u2208 RCR\u00d7n; CL and CR\nare the sample lengths of the left and right contexts respectively, CL + CR < n; \u00afx, \u00afxL, \u00afxR are target\nsignals; z is a noise signal z \u223c N(0, I), k \u2208 [0, 1] is a scalar coefficient that regulates the amount of\nnoise in the infill region of the initial sample for the regenerate and transition tasks, BCE stands for\nBinary Cross-Entropy. In the transition task fout and fout represent fade out and fade in coefficients of\na constant-power cross-fade. The audio channel dimension is not considered for simplicity.\nAlgorithm 1 Sampling with guidance gradients\nRequire: initial sample xT , measurement y, distance function d, measurement operator A(\u00b7),\ngradient step size \u03be, N timesteps ti in (T, 0)\nfor i = N to 1 do\nt \u2190 ti, s \u2190 ti\u22121\n\u02c6x0 \u2190 \u03b1txt \u2212 \u03c3tv\u03b8(xt, t)\n\u03f5 \u223c N(0, I)\nxs \u2190 \u03b1s\n\u03c32\nt\n(1 \u2212 \u03b12\nt\n\u03b12s\n)\u02c6x0 + \u03b1t\u03c32\ns\n\u03b1s\u03c32\nt\nxt + \u03c3t\u2192s\u03f5\nxs \u2190 xs \u2212 \u03be\u2207xtd(y, A(xt))\n\u25b7 Alternatively A(\u02c6x0)\n\u25b7 Apply data consistency step if possible (see Sect. 3)\nend for\nreturn x0\nAlgorithm 2 DDIM with guidance gradients\nRequire: initial sample xT , measurement y, distance function d, measurement operator A(\u00b7),\ngradient step size \u03be, N timesteps ti in (T, 0)\nfor i = N to 1 do\nt \u2190 ti, s \u2190 ti\u22121\n\u02c6x0 \u2190 \u03b1txt \u2212 \u03c3tv\u03b8(xt, t)\n\u02c6\u03f5t \u2190 (xt \u2212 \u03b1t\u02c6x0)/\u03c3t\n\u02c6\u03f5t \u2190 \u02c6\u03f5t \u2212 \u03be\u03c3t\u2207xtd(y \u2212 A(xt))\n\u25b7 Alternatively A(\u02c6x0)\nxs \u2190 \u03b1s\u02c6x0 +\np\n(1 \u2212 \u03b12s)\u02c6\u03f5t\n\u25b7 Apply data consistency step if possible (see Sect. 3)\nend for\nreturn x0\n9\nC\nModel architecture and training details\nC.1\nModels\nOur waveform diffusion model uses a similar Unet implementation to [40] to generate 11.9 seconds\nof 44.1kHz stereo audio. To deal efficiently with the large input size we \u201cfold\u201d the 2-dimensional\nstereo input into the channel dimension and \u201cunfold\u201d the output. We minimize possible aliasing\nartifacts by using overlapping windows with frame size 32 and hop size 16, and multiplying with a\nhamming window function before aggregating.\nOur latent diffusion model comprises a Variational Autoencoder (VAE) to extract the latent repre-\nsentations and a transformer diffusion model to generate 5.9 seconds of audio. Contrary to previous\nworks on images, we use a much higher downsampling ratio for our VAE to make the input size\ntractable for the transformer. In particular, our VAE consists of 8 layers in the encoder and decoder.\nAfter every layer but the first one we use a 2\u00d7 down- or upsampling operation respectively, resulting\nin a latent sequence 128\u00d7 smaller than the input. The layers are implemented as 1D ConvNext [41]\nlayers, which we observed perform better than the traditional dilated convolutions. Finally, we train\nour VAE using frequency losses as well as L1 and L2 losses [42]. To improve the fidelity of the\naudio reconstructed from the latent space, we finetune the decoder while keeping the encoder fixed\nusing a multi-scale frequency based discriminator for a small number of steps (similar to [43]). Our\ntransformer model for latent diffusion follows [24] closely, the main difference being the addition of\na relative positional encoding [44]. We use 32 layers with 1536 feature dimensions resulting in 1B\nparameters.\nC.2\nTraining\nThe models are trained with the AdamW optimizer, \u03b21 = 0.9, \u03b22 = 0.999, and no weight decay.\nA cosine learning rate schedule with a warmup of 5000 steps is used at the beginning of training.\nTraining uses fp16 mixed precision and distributed data parallelism. The waveform model is trained\nfor 600k steps with a batch size of 384 on 24 A100 GPUs, and the latent model for 300k steps with a\nbatch size of 96 on 8 A100 GPUs.\nC.3\nSampling parameters\nWe set the guidance gradient step size \u03be differently for latent (\u03be = 3 \u00d7 10\u22122) and waveform\n(\u03be = 3 \u00d7 10\u22123) models, based on informal qualitative evaluation. The waveform models appeared\nmore sensitive to \u03be and degraded to generating noisy signals for higher values \u03be > 3 \u00d7 10\u22122. We\nkept the step size fixed throughout the sampling process. We created initial samples for the regenerate\nand transition tasks using a noise mixing coefficient k = 0.85.\n10\n"
  },
  {
    "title": "De-Diffusion Makes Text a Strong Cross-Modal Interface",
    "link": "https://arxiv.org/pdf/2311.00618.pdf",
    "upvote": "21",
    "text": "De-Diffusion Makes Text a Strong Cross-Modal Interface\nChen Wei1,2\nChenxi Liu1\nSiyuan Qiao1\nZhishuai Zhang1\nAlan Yuille2\nJiahui Yu1\n1Google DeepMind\n2Johns Hopkins University\n(iii) Multi-Modal Few-Shot Learning with LLMs\n(ii) Multi-Modal Dialogue\nWhat is unusual \nabout this image?\nIt is unusual to see a \ncorgi riding a bicycle, \nespecially in a busy \ncity like New York\u2026\nBard\nStable Diffusion\nMidjourney\nImagen\n\u201ca photograph \ncloseup dog \ncorgi wearing \nwhite hat corgi \nyellow glasses\u2026\u201d\nPrompt\n\u201ca photograph closeup dog corgi wearing white hat corgi \nyellow glasses atop on red handle bicycle street nyc \ncorgi smiling wearing an orange fedora smile yellow \nsunglasses people car multiple screen buildings street\u2026\u201d\nencoder\ndecoder\n(text2img\ndiffusion)\nDe-Diffusion\nYellow sunglasses.\nPaLM 2\nQ: What is the tabby \ncat wearing?\nA: Red sweater.\n\u201ca photograph \ncloseup dog \ncorgi wearing \nwhite hat corgi \nyellow glasses\u2026\u201d\n\u201ca photograph a \ncat wearing \nreddish red wool \nsweater with \nglasses\u2026\u201d\nQ: What is the corgi \ndog wearing?\nA:\nPrompt\n(i) Transferable Prompts for Text-to-Image Generation\nFigure 1. De-Diffusion is an autoencoder whose decoder is a pre-trained text-to-image diffusion model. It encodes an input image into a\npiece of information-rich text, which mixes comprehensive semantic concepts present in the image to be a \u201cscrambled caption\u201d. We group\nsemantics by color for illustration. De-Diffusion text can act as a flexible interface between different modalities, for example, enabling\ndiverse vision-language applications including: (i) providing transferable prompts for different text-to-image tools, (ii) enabling text-only\nchatbots, e.g., Bard [1], to engage in multi-modal dialogue, and (iii) injecting image context into off-the-shelf large language models\n(LLMs), e.g., PaLM 2 [5], to perform open-ended visual question answering by prompting the LLM with few-shot examples.\nAbstract\nWe demonstrate text as a strong cross-modal interface.\nRather than relying on deep embeddings to connect image\nand language as the interface representation, our approach\nrepresents an image as text, from which we enjoy the in-\nterpretability and flexibility inherent to natural language.\nWe employ an autoencoder that uses a pre-trained text-to-\nimage diffusion model for decoding. The encoder is trained\nto transform an input image into text, which is then fed into\nthe fixed text-to-image diffusion decoder to reconstruct the\noriginal input \u2013 a process we term De-Diffusion. Exper-\niments validate both the precision and comprehensiveness\nof De-Diffusion text representing images, such that it can\nbe readily ingested by off-the-shelf text-to-image tools and\nLLMs for diverse multi-modal tasks. For example, a single\nDe-Diffusion model can generalize to provide transferable\nprompts for different text-to-image tools, and also achieves\na new state of the art on open-ended vision-language tasks\nby simply prompting large language models with few-shot\nexamples. Project page: dediffusion.github.io.\n1\narXiv:2311.00618v1  [cs.CV]  1 Nov 2023\n1. Introduction\nWe have witnessed LLM-powered products such as\nChatGPT taking over the world by storm. Nowadays many\npeople are convinced of the benefits that LLMs can bring\nin understanding natural language conversations and as-\nsisting humans in creative tasks.\nHowever, what is the\npath forward?\nOne clear direction and trend is towards\nmulti-modality, allowing the model to understand additional\nmodalities such as image, video, and audio. GPT-4 [60]\nis a multi-modal model with impressive image understand-\ning capabilities, and has recently rolled out to the public\ntogether with audio-processing capabilities. Gemini is also\n\u201cmulti-modal from day one\u201d [2]. Multi-modal models like\nthese have a fundamental design choice to make, i.e., how\ndifferent modalities should communicate and connect? In\nthe context of this work, we rephrase the question as: what\nis the cross-modal interface?\nWe argue that a good cross-modal interface should at\nleast possess the following two properties: (1) content pre-\nserving, i.e., signals from the original modality can be re-\nconstructed from the interface representation to a high de-\ngree; (2) semantically meaningful, i.e., the interface repre-\nsentation contains useful abstractions of the raw signals, so\nthat understanding and reasoning can be performed more\neasily. Balancing these two properties is challenging, and\nin fact they can often be in contention with each other. For\nexample, the raw signals from the original modality satisfy\ncontent preserving perfectly, but are lacking on the seman-\ntically meaningful front.\nEver since the deep learning era [19,32,33,46], deep em-\nbeddings have been the go-to choice as cross-modal inter-\nface. They can be good at preserving image pixels if trained\nas an autoencoder [33], and can also be semantically mean-\ningful, with the most recent exemplar being CLIP [63]. In\nthis paper, we do not argue that deep embeddings are a bad\ncross-modal interface per se, but instead convey the idea\nthat according to our experiments, text can be a strong alter-\nnative cross-modal interface.\nIf we consider the relationship between the speech and\ntext for a quick second, text has always been so natural of\na cross-modal interface that we do not typically think of\nit as such. Converting the speech audio to text well pre-\nserves the content such that we can reconstruct the speech\naudio with the mature text-to-speech technique. We are also\nconfident that the transcribed text contains all the semantics\ninformation, in other words, semantically meaningful. By\nanalogy, we can also \u201ctranscribe\u201d an image into text, which\nhas the more familiar name of image captioning. But when\nwe compare typical image captions against the two proper-\nties of cross-modal interface, they do not preserve content\nwell but only capture the most salient semantic concepts. In\nother words, image captions are more about precision than\ncomprehensiveness [13,88], and it is hard to answer any and\nall visual questions from the short captions (e.g., Fig. 6).\nWhile image captions do not make an ideal interface rep-\nresentation, we argue that precise and comprehensive text,\nif attainable, remains a promising option, both intuitively\nand practically.\nIntuitively, humans rely on language to\narticulate our physical surroundings, engage in reasoning,\nand deliver solutions. In other words, we constantly \u201ctran-\nscribe\u201d information about the external world into language\nand use it as an interface for higher-level cognition [16,24].\nPractically, text is the native input domain for LLMs. Using\ntext as the interface can avoid the need for adaptive train-\ning often required with deep embeddings [4, 49]. Given\nthat training and adapting top-performing LLMs can be pro-\nhibitively expensive [4, 5, 60], text provides a modular de-\nsign that opens up more possibilities. The question is, how\ncan we attain precise and comprehensive text of images?\nWe resort to the classic autoencoding for a solution [33].\nUnlike common autoencoders, we utilize a pre-trained text-\nto-image diffusion model as the decoder, and naturally, with\ntext as the latent space. The encoder is trained to transform\nan input image into text, which is then fed into the text-\nto-image diffusion model for decoding. To minimize the\nreconstruct error, the latent text, though often mixing se-\nmantic concepts together to be a \u201cscrambled caption\u201d of the\ninput image, has to be both precise and comprehensive. No\nextra supervision is used other than images themselves.\nRecent generative text-to-image models excel at convert-\ning arbitrary rich text of, e.g., tens of words, to highly de-\ntailed images that closely follow the prompts [59,65,68,71,\n91]. This essentially suggests the remarkable capability of\nthese generative models to process complex text into visu-\nally coherent outputs. By employing one of these genera-\ntive text-to-image models as the decoder, the optimized en-\ncoder explores the wide latent space of text and unpacks the\nenormous visual-language knowledge encapsulated within\nthe generative model, embodying a foundational paradigm\nknown as Analysis by Synthesis [7,10,94].\nWe show De-Diffusion text extensively captures seman-\ntic concepts in images, and, when used as text prompts,\nenables diverse vision-language applications (Fig. 1). De-\nDiffusion text can generalize to be a transferable prompt\nfor different text-to-image tools. Evaluated quantitatively\nby reconstruction FID [31], De-Diffusion text significantly\noutperforms human-annotated captions as prompts to a\nthird-party text-to-image model [68]. De-Diffusion text also\nenables off-the-shelf LLMs to conduct open-ended vision-\nlanguage tasks by simply prompting LLMs with few-shot\ntask-specific examples.\nWe highlight De-Diffusion out-\nperforms Flamingo [4] on open-ended few-shot VQA [6]\nwith 100\u00d7 fewer learnable weights and without using inter-\nleaved image-text supervision. The results demonstrate De-\nDiffusion text effectively interconnects both human inter-\npretations and various off-the-shelf models across domains.\n2\n2. Related Work\nAutoencoding is a classical approach for learning repre-\nsentations [33,70]. It uses an encoder to map the input into\na compressed, meaningful representation, and a decoder to\nreconstruct the input from this representation to be as close\nas possible to the original. This simple autoencoding con-\ncept underpins many unsupervised representation learning\nalgorithms across domains [20, 30, 34, 45, 82].\nBy forc-\ning the model to compress then reconstruct the input, au-\ntoencoders discover useful structural representations of the\ndata. For example, Neural De-Rendering [87] is a gener-\nalized autoencoder that utilizes a deterministic rendering\nfunction as the decoder and maps images into structured\nand disentangled scene descriptions. Inspired by its name\n\u201cde-rendering\u201d, we name our approach \u201cDe-Diffusion\u201d.\nA specific type of autoencoder, VQ-VAE [67,79] or dis-\ncrete VAE [66], is designed to learn discrete, structured\nrepresentations in the latent space. This can be especially\nuseful for modeling data with categorical or symbolic at-\ntributes. These methods are now widely adopted in multi-\nmodal models to tokenize images [22, 66, 68, 91]. How-\never, VQ-VAE\u2019s latent space is hidden and often entangled,\nrequiring adaptive fine-tuning for downstream tasks. De-\nDiffusion also utilizes a discrete latent space. In contrast,\nwe directly encode images into a sequence of text, which is\ndirectly interpretable.\nSPAE [92] and LQAE [54] are two recent approaches\nthat encode images into the vocabulary space of a fixed\nLLM. They jointly learn the encoder and decoder from\nscratch.\nConsequently, although the latent space is dis-\ncrete text, it tends to act as a \u201ccipher code\u201d that only the\nco-trained decoder can interpret. This limits generalization\nto human understanding and off-the-shelf LLMs and text-\nto-image models. In contrast, De-Diffusion utilizes a pre-\ntrained text-to-image diffusion model as the decoder, ob-\ntaining interpretable text as the latent representation.\nHow many words is an image worth? The adage \u201ca pic-\nture is worth a thousand words\u201d means that still images\ncan convey complex and sometimes multiple ideas more\neffectively than a mere verbal description. Indeed, a sin-\ngle image can tell a story that would take many words\nto explain.\nThe question, how many words is an im-\nage worth, is constantly explored by the computer vision\ncommunity [23, 25, 26, 52].\nFor example, \u201cAn image is\nworth 16\u00d716 words\u201d, or ViT [21], proposes to take the im-\nage patches as tokens (words) and process these tokens by\nTransformers [80], which has become one of the standard\nvision backbones now. In this sense, our work can also been\nseen as \u201cAn image is worth 75 words\u201d, for we encode input\nimages into a sequence of 75 tokens.\nSeveral prior works also explore to use text to represent\nimages [9, 89] and combine with with LLMs. However,\nthese works rely on multiple captioning and classification\nmodels, whose outputs are concatenated to be the text rep-\nresentation. Their performance is heavily dependent on the\ncaptioning and classification models, and we demonstrate\nin \u00a7 4 that even human-annotation captions can lack the ex-\ntensive details covered in De-Diffusion text.\nVision-language models. The breakthrough in NLP [11,\n20, 37, 38, 60, 64, 85], especially their abilities to perform\nfew-shot learning, has inspired a large body of vision-\nlanguage work.\nA family of vision-language models is\nbased on contrastive learning [29], where images and text\nare projected in to a same embedding space [41, 43, 50, 61,\n63, 90, 95]. De-Diffusion differs from contrastive models\nas we encode image as text, instead of deep embeddings.\nAnother family of vision-language models fuses vision and\nlanguage models by jointly training them with large-scale\nimage-text data [4,14,47,53,58,66,90,93]. In contrast, De-\nDiffusion takes a modular design with text as the represen-\ntation, bypassing the heavy cost image-text data collection\nand jointly training large-scale vision and language models.\n3. Method\n3.1. De-Diffusion for Text Representation\nAutoencoder. Autoencoding is one of the classical meth-\nods for representation learning [33, 70]. An autoencoder\nfirst encodes an input x into a latent representation z, then\ndecodes z back to \u02dcx for reconstruction. Both the encoder\nand the decoder are optimized so that the reconstructed in-\nput \u02dcx is as similar as possible to the original input x. By do-\ning so, the compressed representation z preserves the infor-\nmation in the input. Since no more supervision is required\nexcept the input itself, autoencoding is an unsupervised ap-\nproach without the heavy burden of human annotation.\nText as the latent representation. While autoencoders can\nlearn compressed representations z that preserve useful in-\nformation, it is difficult to use the latent z for downstream\ntasks without any additional training, let alone direct human\ninterpretation. In this work, we propose to encode the input\nimage into text. Practically, the encoder compresses each\nimage into a sequence of BPE-encoded text tokens [73],\nwhere each token can take on a discrete value from the vo-\ncabulary. To faithfully reconstruct the image from the latent\ntext, the text must precisely and comprehensively capture\nthe semantic concepts present in the image, making a inter-\nface representation, in contrast to image captions that only\nfocus on the most visually salient information.\nText-to-image diffusion as the decoder. One potential\nconcern is that the encoder might still encrypt the images\ninto a cipher code that only the decoder can decipher, mak-\ning human interpretation challenging. This is particularly\nlikely when the encoder and the decoder are jointly trained.\n3\ninput\noutput\ntext\ngumbel softmax\nattn. \npooler\nimage\nback\nbone\n/\nimage-to-text\nencoder\nU-Net\nCLIP\ntext\nmodel\ntext-to-image diffusion\ndecoder\npixel reconstruction\nFigure 2. Architecture of De-Diffusion. The overall structure is an autoencoder, with (i) a pre-trained text-to-image diffusion model as\nthe decoder, (ii) text as the latent representation, and (iii) a image-to-text encoder consisting of a image backbone and an attentional pooler.\nLock and unlock denote frozen and learnable weights, respectively. We use Gumbel-softmax [42,56] for discrete text tokens.\nTo mitigate this concern [54], we introduce a pre-trained\ntext-to-image diffusion model as the decoder, and dub our\nmethod as \u201cDe-Diffusion\u201d.\nText-to-image diffusion models, as the name suggested,\nlearn the relationship between text and images from a large\ndataset of image-text pairs and excel at converting texts into\nhighly detailed images. They already establish the projec-\ntion from descriptive text to image, and we unpack this en-\ncapsulated knowledge by employing a frozen text-to-image\ndiffusion model as the decoder. As illustrated in Fig. 2, the\ntext-to-image diffusion model consists of a CLIP text en-\ncoder [63] and a U-Net [69], and the codebook is then nat-\nurally the vocabulary of the CLIP text encoder.\nWhen training De-Diffusion, we freeze the parameters of\nthe text-to-image diffusion decoder. In each mini-batch, we\nexpose the decoder with one randomly sampled noise level\nfor each sample. This resembles the training procedure for\ndiffusion models [35], except the parameters are fixed and\nthe text conditions are outputs of the image-to-text encoder\ninstead of the training data.\nImage-to-text encoder. The encoder maps the input image\ninto text. It starts with an image backbone that extracts im-\nage features, followed by an attentional pooler [39,90] that\nturns the features into output text tokens. The image back-\nbone can be a pre-trained and frozen model that excels at\nimage feature extraction. It can also be randomly initial-\nized, supervised by the reconstruction objective during De-\nDiffusion training. We ablate the two choices in Tab. 4d.\nThe attentional pooler projects n learnable queries to n\ntext tokens by a few Transformer blocks [80]. Each Trans-\nformer block consists of a self-attention layer over all the\nqueries, a cross-attention layer to gather features from the\nimage backbone, and an MLP layer. After the Transformer\nblocks, a linear layer projects the queries to discrete text to-\nkens from the vocabulary of CLIP text encoder, in order to\nconnect to the diffusion decoder. The n queries are posi-\ntional sensitive, meaning that each query corresponds to a\nspecific position in the CLIP text encoder. The n output text\ntokens, together with the special tokens [SOS] and [EOS], are\nthen fed into the diffusion decoder. We ablate the effect of\nn, the number of text tokens, in Tab. 4a.\nOptimization. Same as other autoencoders, the training\nobjective of De-Diffusion is to minimize the reconstruc-\ntion error between the input image and the reconstruction\nfrom the pre-trained diffusion model. Specifically, both the\nloss function and the noise variance schedule strictly follow\nthose of the pre-trained diffusion model [35]. The training\ndata of De-Diffusion only includes images, without human\nannotations or paired text descriptions.\nOur model can be viewed as a special discrete autoen-\ncoder with discrete text tokens as the latent.\nSimilar to\nother discrete autoencoders [66, 67, 79], we use Gumbel-\nsoftmax [42, 56] as the continuous relaxation to back-\npropagate the gradients from the decoder through the dis-\ncrete latent. The relaxation becomes tight as the tempera-\nture \u03c4 \u2192 0. We find that an annealing schedule of tempera-\nture \u03c4 is important for stable training.\nTo increase the information density and readability, we\nexclude all the punctuation in the vocabulary, which ac-\ncounts for around 6% of the original vocabulary of CLIP\ntext encoder. As a result, only word tokens and number to-\nkens are allowed. We ablation this design choice in Tab. 4b.\n3.2. Implementation Details\nText-to-image diffusion model. The text-to-image diffu-\nsion model used for De-Diffusion training is based on Ima-\ngen [71]. The U-Net has 600M parameters with an embed-\nding dimension of 256 and input resolution of 64\u00d764. The\ntext encoder is from OpenCLIP ViT-H/14 [15]. The training\ndata is WebLI [14], an image-language dataset built from\npublic web images and texts. We use v-prediction as the\nobjective [72], a batch size of 2048, and train for 3M steps.\nFor reference, this text-to-diffusion model achieves an FID\nof 5.37 on 30K 64\u00d764 MS-COCO 2014 validation images.\nImage backbone and attentional pooler. We utilize a pre-\ntrained CoCa ViT-L model with input resolution 288\u00d7288\nas the image backbone, and freeze it during De-Diffusion\ntraining [21, 90]. This CoCa model is pre-trained on JFT-\n3B [76] and ALIGN datasets [43]. Our attentional pooler\nis equipped with 75 queries, in addition to the [SOS] and\n[EOS] tokens to fully utilize the 77 context length defined\nby CLIP text encoder [15,63]. The attention pooler has five\nTransformer blocks which are always randomly initialized.\n4\n2\n4\n6\n8\nguidance scale\n6\n10\n14\n18\nFID\nDe-Diffusion\nHuman\nBLIP-2\nPaLI-X\nFigure 3. Evaluating different captioning methods by text-to-\nimage reconstruction. The text-to-image model is a pre-trained\nStable Diffusion v2-base model [68].\nWe report FID (\u2193) on\n30K MS-COCO (2014) validation split with 256\u00d7256 images.\nDe-Diffusion obtains better FID than human-annotated captions,\nBLIP-2 [49] (fine-tuned on MS-COCO), and PaLI-X [12] (a multi-\ntask captioning model). Numerical results are provided in Tab. 6.\nTraining of De-Diffusion. The De-Diffusion training data\nalso comes from WebLI [14], while only the images but not\nthe text are used. The broad domain coverage of WebLI en-\nables zero-shot and few-shot evaluations of De-Diffusion on\ndownstream applications in the next section (\u00a7 4). For mem-\nory efficiency, we use the Adafactor optimizer [74] with\n\u03b21 = 0.9, \u03b22 = 0.999 and a decoupled weight decay ratio of\n0.01. We train with a batch size of 2048 for 500K steps, tak-\ning around 2.5 days on 64 TPUv4 chips. The learning rate\nstarts at 3e-4 and is annealed to 3e-6 with cosine decay [55],\nalong with a 10K step warmup [27]. The Gumbel-softmax\ntemperature begins from 2.0 and is exponentially annealed\nto 0.3 through the entire schedule, which we find is suf-\nficient to close the gap between the continuous relaxation\nduring training and the discrete inference.\n4. Experiments and Applications\nIn this section, we introduce several applications of De-\nDiffusion text, ranging from transferable prompts for text-\nto-image tools and few-shot vision-language understanding.\nTo demonstrate the versatility of De-Diffusion text across\ndifferent tasks and domains \u2013 that is, its ability to serve as a\nstrong cross-modal interface \u2013 all the applications use text\nfrom a single De-Diffusion model detailed in \u00a7 3.2.\n4.1. Transferable Text-to-Image Prompt\nSince De-Diffusion encodes an input image into text and\ndecode it by a text-to-image diffusion model, it is trivial\nfor De-Diffusion text to serve as a prompt suggestion to re-\nconstruct an image by this specific text-to-image diffusion\ndecoder. Furthermore, we demonstrate that De-Diffusion\ntext is transferable to other unseen decoders, i.e., text-to-\nimage tools, such as Imagen [71], Stable Diffusion [68]\nand Midjourney [3]. This suggests that De-Diffusion text\nis not over-fitted to a single text-to-image decoder but gen-\neralizable across different text-to-image frameworks, which\nis crucial to make a cross-model interface.\nQuantitative evaluation. We quantitatively evaluate the\nability of De-Diffusion text to transfer to other text-to-\nimage diffusion models and compare with traditional cap-\ntioning methods. To do this, we develop a benchmark that\nuses a third-party pre-trained text-to-image model to recon-\nstruct an image from either De-Diffusion text or captions.\nSpecifically, we first obtain De-Diffusion text and captions\nfor a given image. Both are then input into the third-party\ntext-to-image model to synthesize the corresponding image.\nWe compare the synthesized image to the original. Text\ncontaining more precise and comprehensive descriptions al-\nlows the model to produce images more similar to the orig-\ninal. By evaluating the similarity between original and syn-\nthesized images, our benchmark quantifies the precision and\ncomprehensiveness of different methods.\nWe use the pre-trained Stable Diffusion v2-base [68] as\na generic text-to-image generator, whose weights and train-\ning data are oblivious to both De-Diffusion and caption-\ning methods.\nWe measure the similarity between origi-\nnal and synthesized 256\u00d7256 images using FID (Frechet\nInception Distance) [31] on 30K images from MS-COCO\n2014 validation split [13]. Image generation utilizes dif-\nferent classifier-free guidance [36] scales from 1.5 to 8.0,\nalong with 50 steps of DDIM sampling [75].\nWe evaluate De-Diffusion, human captions and two\nstate-of-the-art image captioning methods, plotted in Fig. 3:\n(i) Human-annotated captions from MS-COCO provide\na strong FID baseline of 8.08 at guidance scale 3.0. We syn-\nthesize new images using the longest of the five annotated\ncaptions, which we find works best. Other options to utilize\nhuman captions are discussed in Appendix A.\n(ii) BLIP-2 refers to its ViT-g OPT 2.7B variant [49],\nwhich is fine-tuned on MS-COCO. As one of the state-of-\nthe-art captioning methods, BLIP-2\u2019s FID curve is close to\nthat of human-annotated captions.\n(iii) PaLI-X [12] performs fine-tuning on multiple cap-\ntion datasets, instead of solely on MS-COCO. As a result,\nits FID curve is higher than that of BLIP-2.\n(iv) De-Diffusion is trained with solely web images, but\nnot MS-COCO images or any human-annotated captioning\ndata. It has an indirect access to noisy web image-language\npairs through the pre-trained diffusion model.\nHowever,\nDe-Diffusion achieves the lowest FID of 6.43 at guidance\n3.0, significantly better than the human-annotated captions.\nThese results indicate that De-Diffusion text precisely\nand comprehensively verbalizes image details, allowing it\nto effectively transfer to other text-to-image tools.\n5\nQualitative evaluation. Our visualizations in Figs. 6 and 7\ndemonstrate that De-Diffusion text is more comprehen-\nsive than human-annotated captions. Images are from MS-\nCOCO 2014 validation split and we test with three promi-\nnent text-to-image tools including Stable Diffusion XL [62],\nMidjourney [3], and Imagen [71].\nThe results show that De-Diffusion text covers fine-\ngrained semantic aspects ranging from objects and their po-\nsitional relationships, human attributes, backgrounds, to ac-\ntion subcategories. In contrast, human-annotated captions\noften neglect fine-grained semantic details, leading to high\nvariance in the generated images across text-to-image tools.\nWhile the descriptions in human captions are precise, De-\nDiffusion text much more comprehensively enumerates key\nobjects, their attributes, their relationships and background\ncontexts.\nThis comprehension allows cross-tool text-to-\nimage reconstruction with De-Diffusion.\nFigs. 8 and 9 visualize text-to-image reconstruction with\nDe-Diffusion text on synthetic images from other text-to-\nimage tools Ideogram1 and Lexica2. We provide the syn-\nthetic links of these images in Appendix E. Fig. 8 shows De-\nDiffusion can provide fine-grained descriptions for complex\nand diverse synthetic images besides photographic images\nin MS-COCO (Figs. 6 and 7). The prompts also transfer\nacross different text-to-image models. Fig. 9 further high-\nlights the ability of De-Diffusion to articulate diverse image\ntypes and explicitly name the genre such as \u201ccg wallpaper\u201d,\n\u201cwatercolor painting\u201d, \u201cetching logo\u201d, and a plain image of\na black circle. These results suggest that De-Diffusion can\nbe applied to provide cross-tool prompt inspiration for user-\nuploaded images to explore new vocabulary and aesthetics.\n4.2. Multi-Modal Few-Shot Learner\nWe next show that De-Diffusion can convert an off-\nthe-shelf LLM, which is never trained on vision-language\ndata, to perform open-ended vision-language task by sim-\nply prompting the LLM with few-shot examples, and no\nadaptive training is required.\nLLMs exhibit surprising generalization ability with few-\nshot learning, adapting to new tasks from just a few an-\nnotated task-specific examples without any further train-\ning [11]. However, these powerful models are limited to\ntext. Since then, methods have emerged to enable multi-\nmodal capabilities by encoding images into the word em-\nbedding space [63,78] or training a new module to connect\nvision and language embeddings [4,49]. However, these ap-\nproaches have downsides \u2013 not only would they introduce\nprohibitively heavy computational costs due to joint train-\ning with enormous language models like 540B PaLM [17],\nbut the visual embeddings also bind to a specific language\nmodel such that changing the language model requires re-\n1https://ideogram.ai\n2https://lexica.art\ntrainable\nVQAv2OKVQACOCO\nmethods\nLLM\nparams. shot test-dev\nval\ntest\nBLIP-2 ViT-g [49]\nFlanT5XXL\n108M\n0\n65.0\u2020\n45.9\u2020\n-\nLENS [9]\nFlanT5XXL\n0\n0\n62.6\n43.3\n-\nAnyMAL ViT-G [58] Llama270B\n-\n0\n64.2\n42.6\n95.9\nPICa-Full [89]\nGPT-3\n0\n16\n56.1\n48.0\n-\nOpenFlamingo-9B [8]MPT7B\n-\n0\n52.7\n37.8\n79.5\nOpenFlamingo-9B [8]MPT7B\n-\n4\n54.8\n40.1\n89.0\nOpenFlamingo-9B [8]MPT7B\n-\n32\n53.3\n42.4\n99.5\nIDEFICS-80B [47]\nLlama65B\n14B\n0\n60.0\n45.2\n91.8\nIDEFICS-80B [47]\nLlama65B\n14B\n4\n63.6\n52.4\n110.3\nIDEFICS-80B [47]\nLlama65B\n14B\n32\n65.9\n57.8\n116.6\nFlamingo-9B [4]\nChinchilla7B\n2B\n0\n51.8\n44.7\n79.4\nFlamingo-9B [4]\nChinchilla7B\n2B\n4\n56.3\n49.3\n93.1\nFlamingo-9B [4]\nChinchilla7B\n2B\n32\n60.4\n51.0\n106.3\nFlamingo-80B [4]\nChinchilla70B 10B\n0\n56.3\n50.6\n84.3\nFlamingo-80B [4]\nChinchilla70B 10B\n4\n63.1\n57.4\n103.2\nFlamingo-80B [4]\nChinchilla70B 10B\n32\n67.6\n57.8\n113.8\nDe-Diffusion ViT-L\nPaLM 2-S\n135M\n0\n63.9\n51.4\n63.4\nDe-Diffusion ViT-L\nPaLM 2-S\n135M\n4\n64.0\n53.5\n87.1\nDe-Diffusion ViT-L\nPaLM 2-S\n135M\n32\n63.1\n53.3\n92.0\nDe-Diffusion ViT-L\nPaLM 2-L\n135M\n0\n67.2\n57.0\n88.5\nDe-Diffusion ViT-L\nPaLM 2-L\n135M\n4\n67.9\n58.2\n100.3\nDe-Diffusion ViT-L\nPaLM 2-L\n135M\n32\n68.4\n60.6\n103.7\nTable 1. Vision-language few-shot learning. We report VQA\naccuracy [6] for visual question answering on VQAv2 [28] and\nOKVQA [57] in the open-ended setting, and CIDEr [81] for MS-\nCOCO image captioning [13]. The Bold denotes the top perfor-\nmance and the underlined denotes the second-best in each column.\n\u2020 in-domain COCO images are used for training.\ntraining.\nThis limits the flexibility of these multi-modal\nmodels to keep pace with rapid progress in LLMs.\nUnlike previous methods based on deep embeddings,\nDe-Diffusion encodes images into text that any language\nmodel can readily comprehend. This allows off-the-shelf\nlanguage models to ground images by simply interleav-\ning task instructions and De-Diffusion text in any order,\nas Fig. 1 shows.\nUsing text as a cross-modal interface,\nDe-Diffusion empowers off-the-shelf language models with\nmulti-modal abilities. We next demonstrate that this mod-\nular approach achieves state-of-the-art performance on dif-\nferent multi-modal few-shot learning benchmarks, thanks to\nthe comprehensive image context provided by De-Diffusion\ntext, and seamless integration with advanced reasoning abil-\nities provided by the LLMs.\nMulti-modal few-shot learning. We follow the evalua-\ntion protocol of Flamingo [4] to assess few-shot learn-\ning on three vision-language tasks including VQAv2 [28],\n6\n(a) reference image\n[LLM prompt] Answer the question given the context.\nImage context: a colvonvscocam blog closeup of young rear man looking carrying head teddybear wearing a red\nsweater it in white barriers amidst between a a wooden polcoping a rails opposite blurry except a woman people\nsitting right off white cabinets lit white windows and sill approximately wearing hair hair burgundy and black\ntabletop backpack approximately wallets blush brown hair hair tallinn salzburg church church church backpack\ncloseup closeup hair hair cubic\nImage question: What toy is this? Short answer:\n[LLM completion] teddy bear.\n[GT answers] stuffed animal, teddy bear\n(b) reference image\n[LLM prompt] Answer the question given the context.\nImage context: a colcandidenverlanticcloseup former recent train train parked traditionenclosed metrotram in a\nred livery it on railroad platform containing wearing a a yellowpolsurround a knob beside platform near a under\nplatform shelter right there and roof shadows and platform and tracks etc wore worn worn maumaroon brown\nwhite stripes markings contentworn yellow yellow stripes train pretoria namibia railway platform train operator\nworn brown windows platform platform\nImage question: What other big vehicle is often painted about the same shade as this vehicle? Short answer:\n[LLM completion] fire truck.\n[GT answers] firetruck, fire truck\nFigure 4. VQA with an off-the-shelf LLM, where De-Diffusion text of the reference image is inserted after \u201cImage context\u201d in the LLM\nprompt. The LLM then completes the prompt to answer the visual question. De-Diffusion text provides abundant visual details, e.g., teddy\nbear in (a) and red livery of the train in (b). We use PaLM 2-L [5] as the LLM. Samples are from OKVQA [57].\nOKVQA [57] and MS-COCO caption [13]. De-Diffusion\ntext for the support images is interleaved along with their\nquestions, answers, and captions to form prompts for the\nLLMs. The LLM\u2019s completion is considered a correct an-\nswer only if it exactly matches the ground truth. More de-\ntails are in Appendix B. Results are shown in Tab. 1.\nThanks to the modular nature of De-Diffusion text, we\nare able to couple the same set of De-Diffusion text with dif-\nferent language models, PaLM 2-S and PaLM 2-L [5] with-\nout multi-modal training. The performance of De-Diffusion\ntext paired with PaLM 2-L increases from zero-shot to 32-\nshot setup on all three tasks. However, when coupled with\nPaLM 2-S, the 32-shot performance slightly decreases on\ntwo VQA benchmarks compared to using four shots. We\nhypothesize this is because smaller language models like\nPaLM 2-S benefit less from long context [86], e.g., the\naround 3600-token prompts for 32 shots.\nDe-Diffusion text paired with PaLM 2-L matches other\nmethods on MS-COCO captioning, and establishes new\nstate-of-the-art results on two VQA benchmarks for all\nzero-shot, 4-shot, and 32-shot settings.\nMeanwhile, De-\nDiffusion training is also more lightweight in both data and\ncomputation. Data-wise, De-Diffusion only requires im-\nages, unlike Flamingo and its followups [4,8,47] which use\nmassive interleaved web text and images, or BLIP-2 [49]\nwhich needs human annotations. Computation-wise, De-\nDiffusion not only uses far fewer parameters (135M in De-\nDiffusion vs. 10B in Flamingo-80B), but its training also\ndoes not involve inference with frozen LLMs like 70B-\nparameter Chinchilla [37] in Flamingo. Instead, it only re-\nquires frozen 600M U-Net and CLIP text encoder (\u00a7 3.2).\nVQAv2\nOKVQA\nmethods\n0-shot 4-shot 32-shot 0-shot 4-shot 32-shot\nBLIP-2 OPT2.7b caption [49] 63.1\n63.0\n62.8\n58.5\n57.6\n59.1\nHuman caption [13]\n63.1\n63.2\n63.6\n59.0\n58.9\n60.1\nDe-Diffusion ViT-L\n65.2\n66.0\n66.2\n57.0\n58.2\n60.6\nTable 2. Compare to other captions on the val split of VQAv2\nand OKVQA. BLIP-2 represents the top captioning model. Hu-\nman captions are from MS-COCO annotations. PaLM 2-L is used.\nOur results suggest that LLMs, without any multi-modal\ntraining, can make grounded inferences for vision-language\ntasks using just text descriptions of images. The benefits of\nlanguage models are more pronounced on challenging sit-\nuations requiring reasoning and commonsense knowledge,\nsuch as d Outside Knowledge VQA (OKVQA) [57]. As\nthe examples in Fig. 4 show, LLMs can answer non-trivial\nvisual questions that demand both De-Diffusion image con-\ntext and commonsense knowledge.\nOn the other hand, the results suggest that De-Diffusion\ntext comprehensively captures semantic elements in im-\nages, which language models can leverage to perform VQA\nand captioning. In Tab. 2, De-Diffusion text outperforms\nthe state-of-the-art captioning method BLIP-2 [49] and even\nhuman-annotated captions on VQAv2 [28]. It also surpasses\nboth on OKVQA [57] given 32 shots. This suggests the\nstrength of De-Diffusion text in providing rich context.\nOpen-ended one-shot classification. We follow the proto-\ncol from Frozen [78] to evaluate open-ended one-shot im-\nage classification on miniImageNet [83]. We interleave De-\nDiffusion text for the support images along with their real\nclass names as prompts for the LLM. The text generated by\nthe LLM is used as the prediction.\n7\nreference image\n[Chatbot prompt] Image context: <De-Diffusion text>. What is amusing about this image?\n[ChatGPT-3.5] The amusing aspect of the image seems to be the juxtaposition and contrast between the serene\nand peaceful scene of the Labrador retriever peacefully resting on the slate footpath, and the busy, crowded, and\nsomewhat chaotic background of a bustling street in Milan.\n[Bard] The amusing thing about the image is the juxtaposition of the sleeping dog and the busy street. The dog\nis completely relaxed and oblivious to the hustle and bustle around it. The contrast between the two creates a\nsense of humor.\nFigure 5. Multi-modal dialogue with off-the-shelf text-only chatbots, where De-Diffusion text is inserted after \u201cImage context\u201d in the\ntext prompt for ChatGPT-3.5 and Bard. Full <De-Diffusion text> of this reference image is in Fig. 6.\nmethods\nLLM\nw/o induction w/ induction\nFrozen [78]\nFrozen [78]\n1.7\n65.0\nLQAE [54]\nGPT3.5 [11]\n1.5\n68.7\nSPAEPaLM [92]\nPaLM 2-L [5]\n32.2\n85.4\nDe-Diffusion\nLlama270B [77]\n60.8\n95.0\nDe-Diffusion\nPaLM 2-S [5]\n79.2\n98.1\nDe-Diffusion\nPaLM 2-L [5]\n78.9\n99.3\n(a) 2-way Classification\nmethods\nLLM\nw/o induction w/ induction\nP>M>F [40]\n-\n95.3\nFrozen [78]\nFrozen [78]\n0.9\n33.8\nLQAE [54]\nGPT3.5 [11]\n1.0\n45.9\nSPAEPaLM [92]\nPaLM 2-L [5]\n23.6\n67.0\nDe-Diffusion\nLlama270B [77]\n64.8\n87.9\nDe-Diffusion\nPaLM 2-S [5]\n66.4\n88.6\nDe-Diffusion\nPaLM 2-L [5]\n71.8\n97.0\n(b) 5-way Classification\nTable 3. Open-ended one-shot classification on miniImageNet,\nwhere only the exact class names predicted by the LLM are con-\nsidered correct. Task induction is introductory text explaining the\nclassification task and providing expected class names at the start\nof the prompt. Previous best in the closed form is de-emphasized.\nWe evaluate in an open-ended fashion, where only\ngenerating the exact class name is considered correct.\nThere is also an option of task induction, which is in-\ntroductory text explaining the classification task and pro-\nviding expected class names at the beginning of the\nprompt, e.g., \u201cClassify the context into dog\nor cat.\u201d More details are in Appendix B.1.\nThe results are shown in Tab. 3. Task induction largely\nincreases performance because it helps the language model\nto generate the exact class names required for open-ended\nevaluation. With three different LLMs, LLaMA-70B [77],\nPaLM 2-S and PaLM 2-L [5], De-Diffusion significantly\noutperforms previous methods on both 2-way and 5-way\nclassification.\nPaLM 2-L inference with task induction\nachieves 97.0% accuracy, even surpassing the previous\nclosed-form state-of-the-art of 95.3% systematically. These\nresults suggest De-Diffusion excels at verbalizing class\nnames of main objects in images.\n4.3. Multi-modal Dialogue\nChatbots such as ChatGPT-3.5 [60] and Bard [1] are\nLLM-based models that engage users with conversational\ninteractions. They have demonstrated impressive advances\nin natural language understanding, generation, and conver-\nsational capabilities. These chatbots can engage in remark-\nably human-like dialogue, answer follow-up questions, and\nperform helpful tasks. However, as language models, they\nlack grounding in the visual world. In Fig. 5, we demon-\nstrate that De-Diffusion text can provide this missing visual\ngrounding. By incorporating De-Diffusion descriptions of\nimages into the conversational context, chatbots can lever-\nage the rich visual details captured in the text. This allows\nthem to answer challenging questions that require complex\nreasoning and commonsense knowledge. Furthermore, we\nfind De-Diffusion text transfers across different chatbots.\nWe explore more combinations in Appendix D.\n4.4. Ablation\nIn this section, we ablate different design choices of De-\nDiffusion. By default, the encoder is a frozen CoCa pre-\ntrained ViT-Large model, and we train De-Diffusion for\n300K steps. For text-to-image reconstruction, we use FID\non Stable Diffusion v2.0-base, the same setting as Fig. 3,\nreporting the lowest FID across guidance scales. For few-\nshot learning, we use 5-way 1-shot classification accuracy\non miniImageNet with task induction, identical to Tab. 3b.\nNumber of tokens. De-Diffusion text by default uses up\nall 75 tokens from the CLIP text encoder context.\nIn\nTab. 4a, we show performance using 5, 15, and 45 tokens.\nWith more tokens, reconstruction with Stable Diffusion im-\nproves, with FID decreasing from 9.19 to 6.43. This aligns\nwith our intuition that longer text descriptions as prompts\nlead to better text-to-image reconstruction. Interestingly,\nfew-shot classification accuracy decreases from 97.8% to\n97.0% with longer text. This suggests when context length\nis limited, De-Diffusion prioritizes the most salient seman-\ntic concepts, usually the image classes. This aligns with\nthe training objective of De-Diffusion to find the most rep-\nresentative text latent to minimize reconstruction error of\nautoencoding. With longer context, De-Diffusion text in-\ncludes more comprehensive but subtle concepts beyond the\nclasses, important for reconstruction but not classification.\n8\ntokens FID\u2193 acc.\n5\n9.19\n97.8\n15\n7.42\n97.6\n45\n6.95\n97.0\n75\n6.43\n97.0\n(a) Number of tokens.\npunctuation FID\u2193 acc.\n\u2713\n6.85\n96.8\n\u00d7\n6.43\n97.0\n(b) Excluding punctuation.\nblocks FID\u2193 acc.\n3\n6.85\n96.6\n5\n6.43\n97.0\n9\n6.76\n93.1\n(c) Pooler depth.\narch.\ninit.\n# steps FID\u2193 acc.\nViT-Base\nCoCa\n300K\n6.84\n92.6\nViT-Large CoCa\n300K\n6.43\n97.0\nViT-Large\nrand\n300K\n14.6\n67.2\nViT-Large\nrand\n500K\n11.0\n72.2\n(d) Image backbone.\nTable 4. De-Diffusion ablation experiments. We evaluate text-to-image reconstruction FID (\u2193) on MS-COCO (2014) validation split\nusing 256\u00d7256 images with Stable Diffusion v2-base. We report the best FID across guidance scales. We also report open-ended 5-way\n1-shot classification accuracy on miniImageNet. Default settings are marked in gray .\nExcluding punctuation. We use the 49K token vocabulary\nof CLIP as the codebook of latent representations. This\nnaturally results from using the CLIP text encoder for the\ntext-to-image diffusion model. However, we exclude punc-\ntuation from the vocabulary, which accounts for around 6%\nof the original tokens. By excluding these, we can devote\nmore of the limited 75 latent tokens to content words, allow-\ning more semantic concepts to be expressed. In Tab. 4b, we\nvary these choices. Excluding punctuation improves recon-\nstruction FID on Stable Diffusion from 6.85 to 6.43, sug-\ngesting better transferability of De-Diffusion text to other\ntext-to-image models, likely due to the use of more content\nwords. On the other hand, few-shot accuracy on miniIm-\nageNet only drops 0.2%, showing punctuation has a small\ninfluence on few-shot learning ability when using LLMs.\nPooler depth. Tab. 4c varies the depth, i.e., number of\nTransformer blocks, in the attentional pooler of the image-\nto-text encoder.\nToo few layers may limit its ability to\ncapture all the necessary semantics. But too many layers\ncould overfit to the specific text-to-image diffusion model\nand hurt generalizability. Experiments suggest that with as\nfew as three Transformer blocks, the attentional pooler can\neffectively transform image features from the pre-trained\nCoCa backbone into De-Diffusion text. With five Trans-\nformer blocks, we obtain the best performance on both re-\nconstruction FID with Stable Diffusion and few-shot accu-\nracy on miniImageNet. This implies that the pre-trained\nCoCa backbone provides effective image features for image\nto text encoding, which we examine next.\nImage backbone. Tab. 4d varies different image backbone\narchitectures. Increasing the frozen pre-trained CoCa back-\nbone size from ViT-Base to ViT-Large largely improves per-\nformance, reducing reconstruction FID from 6.84 to 6.43,\nand improving few-shot classification accuracy from 92.6%\nto 97.0%. We also explore a randomly initialized backbone\noptimized by the De-Diffusion objective. With 300K train-\ning steps, this obtains an FID of 14.6 and few-shot accuracy\nof 67.2%. Performance increases with a longer 500K sched-\nule, as expected for generative model training. Though still\nbehind pre-trained CoCa backbones, training from scratch\nachieves 72.2% few-shot accuracy on miniImageNet, sur-\npassing prior methods like SPAE with PaLM 2-L at 67.0%\ntraining data\nMS-COCO FID\u2193\nminiImageNet acc.\nWebLI [14]\n6.43\n97.0\nImageNet-1K [19]\n6.93\n97.2\nMS-COCO [51]\n7.53\n85.8\nTable 5. De-Diffusion ablation on training data. Settings are the\nsame as those in Tab. 4. Default setting is in gray .\ndespite. This highlights the promise of using a pre-trained\ngenerative model as supervision to train an image backbone\nfrom scratch. By learning to synthesize inputs, such a back-\nbone is encouraged to captures all factors of variation, align-\ning with the principle of analysis by synthesis.\nTraining data. We explore using different training images\nfor De-Diffusion, such as web images in WebLI [14] and the\ntraining splits of ImageNet-1K [19] and MS-COCO (2014).\nResults are shown in Tab. 5. By default, we use WebLI,\nwhich covers diverse domains, including multi-object pho-\ntos like MS-COCO and single-object photos like ImageNet.\nConsequently, WebLI training obtains strong MS-COCO\nreconstruction FID of 6.43 and 97.0% few-shot classifica-\ntion accuracy on miniImageNet. When training on Ima-\ngeNet, a subset of which miniImageNet is derived from, we\nachieve an even higher 97.2% few-shot accuracy. This sug-\ngests in-domain training images benefit De-Diffusion. In\ncontrast, training with only MS-COCO images hurts both\nreconstruction and few-shot performance, likely because its\ndataset is too small at only 83K images.\n5. Conclusion\nWe propose De-Diffusion, an autoencoder whose latent\nis text representation. By employing a pre-trained text-to-\nimage diffusion model as the decoder, we obtain content-\npreserving and semantically meaningful textual descrip-\ntions for the input images. We then apply De-Diffusion\ntext into text-to-image reconstruction, where De-Diffusion\ntext surpasses human-annotated captions, and combine with\nadvanced LLMs to perform multi-modal few-shot learn-\ning, where we surpass large-scale vision-language models.\nOur results suggest that text representation, like how it con-\nnects human perception and cognition, can serve as a strong\ncross-modal interface for multi-modal tasks.\n9\nOriginal Image\nStable Diffusion XL\nMidjourney\nImagen\n[De-Diffusion Text] an landsapiccinemageneric photograph dog labrador aus white creamy labrador retriever\nlying lying resting threshold lying an onto slate footpath pathway street milan ositalian retristreet stil\nrelating called an cream dog shown sleeping sleeping beside near an blue left bicycle bicycle left\ncrowded street left tyre and umbrella blurry beige brown monochrome left left towards and sitting among\npeople street gray walls alley mostly brown buildings street blur street pathway street\nobject positional relationships\n[GT Caption] A white dog is sleeping on a street and a bicycle.\n[De-Diffusion Text] an attribumontagjagsinfo closeup woman giraffe wearing white sheer sheer blouse\nlong eved olive pants standing an on terracotta fencing balcony tanzania tanzania osdaria jens keynes\npresented yet description an blond female shown holding lovingly embraced holds an shadows animal\ngiraffe head when smile smile animal ear blonde neck abadbrown brown purple consist though among\nwooden plants among plants animals shady trees trees either trees rainforest shadows blouse holistic zoo\nhuman attributes\n[GT Caption] A woman standing with by a giraffe at a fence, and feeding it, with trees and shrubs behind.\nFigure 6. Text-to-image reconstruction with De-Diffusion text and ground-truth captions. The original images are from MS-COCO\n2014 val split. We highlight different visual aspects in green.\n10\nOriginal Image\nStable Diffusion XL\nMidjourney\nImagen\n[De-Diffusion Text] an davilishlishblog closeup berries jar through largerefrerefrejar glass jar eachother glass\non an on peach hardwood closeup glass homemade osmixed glass jar called relating called an oranges fruit\nshown slices eachother containing relating an orange orange slices slices between black grapes open chunks\norange oranges and berry blackblueberry consist though towards pink closeup facing that background pink\nwall background pink pink wall wall closeup chia grapes recipe\nbackgrounds\n[GT Caption] A jar filled with different types of fruit on a table.\n[De-Diffusion Text] an attribusphostavpix person ski skis whose white red dres dres black helmet red pants\nriding an on snow ski ski austria austria oscompete ski ski resembrelating description an ski person shown\naction ripping speeds on an a ski stick poles with wearing markings black wheels ilitgoggles silver pink\nwhite green consist though towards smoky blur though snow blur smoky smoky but but winter background\ncompete compete olympic championship skis\naction subcategories\n[GT Caption] A helmeted and goggled skier leans to get around an obstacle.\nFigure 7. Text-to-image reconstruction with De-Diffusion text and ground-truth captions. The original images are from MS-COCO\n2014 val split. We highlight different visual aspects in green.\n11\nOriginal Image\nStable Diffusion XL\nMidjourney\nImagen\n[De-Diffusion Text] an disponsphographic provided graphic a lion jpg grey grey known lion serious face closeup face though an behind\nblack splatbackdrop realism britanniosanimal animal animal creativeexhibiting called an a lion shown looking frontal upwards towards an\nblack splatsplatblot with with dripping copper eyed copper markings blackandbronze grey monochrome overlooking scattered with black\nsplatbehind splatchaos beige background background overcast beige background closeup eyebrow deviantart realism poster\n[De-Diffusion Text] an artjvdigitally sart rendering glass apple largelargeblue glass shape with apple with leaf on an on olive lders ashore\nbeach britanniosfuturistic apple apple creativeexhibiting called an glass fruit shown glass incorporating with with an icy icy iceberg iceberg\nwith water boiling with leaf moody sky olive green teal teal placed though on blur waves behind ocean waves grey moody sky grey grey\nbackground dusk blur blur blur montage\n[De-Diffusion Text] an disponsphocgi provided painting a owl beautifully white blue intricate owl closeup beak closeup beak near an near\nwhite written realism visionary legendosfantasy animal bird presented description called an animal owl shown beak looking showcasing\nwearing an gold elaborate winged ears with url text orange lenses orange lenses darker orange blue blues also front on gold stamp on blurry\nfont warm blur on white peach background fps eyebrow fantasy deviantart simulation\n[De-Diffusion Text] an artrhadigitally sart illustration woman face wearing colorful colorful paints face painted head pink lipstick though an\namong colourful confetti confetti realism pinup osjanumonroe monroe resembrelating called an face woman shown face smelling upwards\nmultiple an colorful florals roses hats above many paints with earrings turmeric makeup brightly orange red pink wth scattered among yellow\noranges flying flying butterflies teal background on teal blue background lips eyebrow hadid cg poster\nFigure 8. Text-to-image reconstruction with De-Diffusion text. Original images are synthetic and we provide their source in Appendix E.\n12\nOriginal Image\nStable Diffusion XL\nMidjourney\nImagen\n[De-Diffusion Text] an illustration envcesarpixels wallpaper colorful swirl numerous colorful colorful curved sails bent curved orange angular\nconsist an among colorful curved curved modernist futuristic osfuturistic futuristic cave resembrelating called an colorful swirl shown folded\ncurved resembresemban teal curved curved ribbons while teal colorful colorful swirl orange lines orange orange purple purple consist numerous\nbetween orange lines among curved swoop purple stripes but gray purple siding modernist modernist modernist cg wallpaper\n[De-Diffusion Text] an artapiccgi sart painting watercolor mountain consisting blackandwhite misty huge mountain towering mound and\nstick beside an beside a wetland wetland mountainfuturistic osfuturistic futuristic mound shown exhibiting see an misty pond shown\nhillside alongside alongside with an black black stems poles with dripping dripping with dripping atmospheric mist silver monochrome grey\nmonochrome wth foreground towards white background aside alps peaks white peaks mountains beige beige background sunlight reflections\nfantasy watercolor painting\n[De-Diffusion Text] an illustration albuetching vscocam illustration intricate insect heavily black intricate intricate insect insect crest intricate\ncrest on an behind lit circular moon intricate folkosintricate insect insect forma exhibiting called an intricate insect shown frontal frontal\nsurrounded amongst an lit many crescent moons besides scattered stars and stars and moons pastgold beige navy amongst beside among\nand crescent beside and crescent navy stars on dark navy background night stars bohemian etching logo\n[De-Diffusion Text] an anomicomkppixels tfsimple circle consisting white blk wire hoop circular hoop black wire between an into black wire\nhoop minimal midcentury osminimal minimalist hoop creativesimilar called an white circle shown portrait frontal closeup resemban white\nwhite circular circle with simple simple simple hoop white circle ilitwhi black monochrome transportently between simple circle simple simple\nframe white isobackground isowhite background minimalist minimalist minimalist line decal\nFigure 9. Text-to-image reconstruction with De-Diffusion text. We highlight the types of images in green. Original images are synthetic\nand we provide their source in Appendix E.\n13\nReferences\n[1] Bard. hhttps://bard.google.com/chat/. 1, 8\n[2] Google I/O 2023:\nMaking AI more helpful for every-\none.\nhttps://blog.google/technology/ai/\ngoogle-io-2023-keynote-sundar-pichai. 2\n[3] Midjourney.\nhttps : / / www . midjourney . com /\nhome/. 5, 6\n[4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. In NeurIPS,\n2022. 2, 3, 6, 7, 18\n[5] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023. 1,\n2, 7, 8\n[6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVQA: Visual question answering. In ICCV, 2015. 2, 6\n[7] Bishnu S Atal and Suzanne L Hanauer. Speech analysis and\nsynthesis by linear prediction of the speech wave. The jour-\nnal of the acoustical society of America, 1971. 2\n[8] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\nSamir Gadre, Shiori Sagawa, et al. OpenFlamingo: An open-\nsource framework for training large autoregressive vision-\nlanguage models. arXiv preprint arXiv:2308.01390, 2023.\n6, 7\n[9] William Berrios, Gautam Mittal, Tristan Thrush, Douwe\nKiela, and Amanpreet Singh. Towards language models that\ncan see: Computer vision through the lens of natural lan-\nguage. arXiv preprint arXiv:2306.16410, 2023. 3, 6\n[10] Thomas G Bever and David Poeppel. Analysis by synthe-\nsis: a (re-) emerging program of research for language and\nvision. Biolinguistics, 2010. 2\n[11] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In NeurIPS, 2020. 3, 6, 8\n[12] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-\nbastian Goodman, Xiao Wang, Yi Tay, et al.\nPaLI-X: on\nscaling up a multilingual vision and language model. arXiv\npreprint arXiv:2305.18565, 2023. 5, 18\n[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick.\nMicrosoft COCO captions: Data collection and evaluation\nserver. arXiv preprint arXiv:1504.00325, 2015. 2, 5, 6, 7, 18\n[14] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\nGrycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,\nGaurav Mishra, Linting Xue, Ashish V Thapliyal, James\nBradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,\nBurcu Karagol Ayan, Carlos Riquelme Ruiz, Andreas Peter\nSteiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. PaLI: A jointly-scaled multilingual language-\nimage model. In ICLR, 2023. 3, 4, 5, 9\n[15] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-\ning laws for contrastive language-image learning. In CVPR,\n2023. 4\n[16] Noam Chomsky. Language and mind. Cambridge University\nPress, 2006. 2\n[17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. PaLM: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 6\n[18] Kevin Clark and Priyank Jaini.\nText-to-image diffu-\nsion models are zero-shot classifiers.\narXiv preprint\narXiv:2303.15233, 2023. 19\n[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2, 9\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL, 2019. 3\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021. 3, 4, 19\n[22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\n2021. 3\n[23] Sanja Fidler, Abhishek Sharma, and Raquel Urtasun. A sen-\ntence is worth a thousand pixels. In CVPR, 2013. 3\n[24] Jerry A Fodor. The language of thought, volume 5. Harvard\nuniversity press, 1975. 2\n[25] Aviv Gabbay, Niv Cohen, and Yedid Hoshen. An image is\nworth more than a thousand words: Towards disentangle-\nment in the wild. NeurIPS, 2021. 3\n[26] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 3\n[27] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He.\nAccurate, large mini-\nbatch SGD: Training imagenet in 1 hour.\narXiv preprint\narXiv:1706.02677, 2017. 5\n[28] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the V in VQA Matter: Ele-\n14\nvating the Role of Image Understanding in Visual Question\nAnswering. In CVPR, 2017. 6, 7, 18\n[29] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension-\nality reduction by learning an invariant mapping. In CVPR,\n2006. 3\n[30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00e1r, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR, 2022. 3\n[31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. NeurIPS, 2017. 2, 5, 18\n[32] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A\nfast learning algorithm for deep belief nets. Neural compu-\ntation, 2006. 2\n[33] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing\nthe dimensionality of data with neural networks. Science,\n2006. 2, 3\n[34] Geoffrey E Hinton and Richard Zemel. Autoencoders, min-\nimum description length and helmholtz free energy.\nIn\nNeurIPS, 1993. 3\n[35] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 4\n[36] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[37] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de\nLas Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, et al. An empirical analysis of compute-optimal large\nlanguage model training. In NeurIPs, 2022. 3, 7\n[38] Jeremy Howard and Sebastian Ruder. Universal language\nmodel fine-tuning for text classification.\narXiv preprint\narXiv:1801.06146, 2018. 3\n[39] Shell Xu Hu, Da Li, Jan St\u00fchmer, Minyoung Kim, and Tim-\nothy M Hospedales. Pushing the limits of simple pipelines\nfor few-shot learning: External data and fine-tuning make a\ndifference. In CVPR, 2022. 4\n[40] Shell Xu Hu, Da Li, Jan St\u00fchmer, Minyoung Kim, and Tim-\nothy M Hospedales. Pushing the limits of simple pipelines\nfor few-shot learning: External data and fine-tuning make a\ndifference. In CVPR, 2022. 8\n[41] Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen,\nSneha Kudugunta, Chao Jia, Yinfei Yang, and Jason\nBaldridge. Mural: multimodal, multitask retrieval across lan-\nguages. arXiv preprint arXiv:2109.05125, 2021. 3\n[42] Eric Jang, Shixiang Gu, and Ben Poole. Categorical repa-\nrameterization with gumbel-softmax. In ICLR, 2017. 4\n[43] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. In ICML, 2021. 3, 4\n[44] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-\nments for generating image descriptions. In CVPR, 2015. 18\n[45] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. In ICLR, 2014. 3\n[46] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In NeurIPS, 2012. 2\n[47] Hugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Sid-\ndharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.\nOBELISC: An open web-scale filtered dataset of interleaved\nimage-text documents.\narXiv preprint arXiv:2306.16527,\n2023. 3, 6, 7\n[48] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis\nBrown, and Deepak Pathak. Your diffusion model is secretly\na zero-shot classifier.\narXiv preprint arXiv:2303.16203,\n2023. 19\n[49] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2:\nbootstrapping language-image pre-training with\nfrozen image encoders and large language models. In ICML,\n2023. 2, 5, 6, 7, 18\n[50] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.\nAlign before fuse: Vision and language representation learn-\ning with momentum distillation. NeurIPS, 2021. 3\n[51] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, 2014. 9\n[52] Anthony Z Liu, Lajanugen Logeswaran, Sungryull Sohn,\nand Honglak Lee.\nA picture is worth a thousand\nwords: Language models plan from pixels. arXiv preprint\narXiv:2303.09031, 2023. 3\n[53] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 3\n[54] Hao Liu, Wilson Yan, and Pieter Abbeel. Language quan-\ntized autoencoders: Towards unsupervised text-image align-\nment. arXiv preprint arXiv:2302.00902, 2023. 3, 4, 8\n[55] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradi-\nent descent with warm restarts. In ICLR, 2017. 5\n[56] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The\nconcrete distribution: A continuous relaxation of discrete\nrandom variables. In ICLR, 2017. 4\n[57] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. OK-VQA: A Visual Question Answer-\ning Benchmark Requiring External Knowledge. In CVPR,\n2019. 6, 7, 18\n[58] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar\nNagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh,\nPrakash Murugesan, Peyman Heidari, Yue Liu, Kavya\nSrinet, Babak Damavandi, and Anuj Kumar.\nAnyMAL:\nAn efficient and scalable any-modality augmented language\nmodel. arXiv preprint arXiv:2309.16058, 2023. 3, 6\n[59] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. In ICML,\n2022. 2\n[60] OpenAI.\nGPT-4 Technical Report.\narXiv preprint\narXiv:2303.08774, 2023. 2, 3, 8\n[61] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi,\nHanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen,\nMinh-Thang Luong, Yonghui Wu, et al. Combined scaling\nfor zero-shot transfer learning. Neurocomputing, 2023. 3\n15\n[62] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 6\n[63] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In ICML, 2021.\n2, 3, 4, 6\n[64] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. JMLR, 2020. 3\n[65] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with CLIP latents. arXiv preprint arXiv:2204.06125,\n2022. 2, 21\n[66] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In ICML, 2021. 3, 4\n[67] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gen-\nerating diverse high-fidelity images with VQ-VAE-2.\nIn\nNeurIPS, 2019. 3, 4\n[68] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 2, 3, 5\n[69] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional networks for biomedical image segmen-\ntation. In MICCAI, 2015. 4\n[70] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams,\net al. Learning internal representations by error propagation.\nParallel Distributed Processing, 1986. 3\n[71] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding.\nIn\nNeurIPS, 2022. 2, 4, 5, 6\n[72] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In ICLR, 2022. 4\n[73] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural\nmachine translation of rare words with subword units. arXiv\npreprint arXiv:1508.07909, 2015. 3\n[74] Noam Shazeer and Mitchell Stern.\nAdafactor: Adaptive\nlearning rates with sublinear memory cost. In ICML, 2018.\n5\n[75] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2021. 5\n[76] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In ICCV, 2017. 4\n[77] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,\nAnastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,\nNaejin Kong, Harshith Goka, Kiwoong Park, and Victor\nLempitsky.\nResolution-robust large mask inpainting with\nfourier convolutions. In CVPR, 2022. 8\n[78] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-\nlami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot\nlearning with frozen language models. NeurIPS, 2021. 6, 7,\n8\n[79] Aaron\nvan\nden\nOord,\nOriol\nVinyals,\nand\nKoray\nKavukcuoglu.\nNeural discrete representation learning.\nIn NeurIPS, 2017. 3, 4\n[80] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 3,\n4\n[81] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. CIDEr: Consensus-based image description evalu-\nation. In CVPR, 2015. 6\n[82] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua\nBengio,\nPierre-Antoine\nManzagol,\nand\nL\u00e9on\nBottou.\nStacked denoising autoencoders: Learning useful represen-\ntations in a deep network with a local denoising criterion.\nJMLR, 2010. 3\n[83] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning.\nNeurIPS, 2016. 7\n[84] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao\nLi, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan\nYuille, and Christoph Feichtenhofer.\nDiffusion models as\nmasked autoencoders. In ICCV, 2023. 19\n[85] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\nQuoc V Le. Finetuned language models are zero-shot learn-\ners. arXiv preprint arXiv:2109.01652, 2021. 3\n[86] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson,\nYifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny\nZhou, et al. Larger language models do in-context learning\ndifferently. arXiv preprint arXiv:2303.03846, 2023. 7\n[87] Jiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli. Neu-\nral scene de-rendering. In CVPR, 2017. 3\n[88] Lingxi Xie, Xiaopeng Zhang, Longhui Wei, Jianlong Chang,\nand Qi Tian. What is considered complete for visual recog-\nnition? arXiv preprint arXiv:2105.13978, 2021. 2\n[89] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yu-\nmao Lu, Zicheng Liu, and Lijuan Wang. An empirical study\nof GPT-3 for few-shot knowledge-based VQA.\nIn AAAI,\n2022. 3, 6\n[90] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. TMLR, 2022.\n3, 4\n[91] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and\nYonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation. arXiv preprint arXiv:2206.10789,\n2022. 2, 3\n[92] Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolf-\ngang Macherey, Yanping Huang, David A Ross, Irfan Essa,\nYonatan Bisk, Ming-Hsuan Yang, et al.\nSpae: Semantic\n16\npyramid autoencoder for multimodal generation with frozen\nllms. arXiv preprint arXiv:2306.17842, 2023. 3, 8\n[93] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, et al. Scaling autoregressive multi-\nmodal models: Pretraining and instruction tuning.\narXiv\npreprint arXiv:2309.02591, 2023. 3\n[94] Alan Yuille and Daniel Kersten. Vision as bayesian infer-\nence: analysis by synthesis?\nTrends in cognitive sciences,\n2006. 2\n[95] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,\nDaniel Keysers, Alexander Kolesnikov, and Lucas Beyer.\nLiT: Zero-shot transfer with locked-image text tuning. In\nCVPR, 2022. 3\n17\nA. Transferable Text-to-Image Prompt\nmethod\n1.5\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\nPaLI-X [12]\n9.68 8.50 10.16 12.38 14.27 15.81 16.81 17.76\nBLIP-2 [49]\n10.66 8.46 8.92 10.40 11.84 12.93 13.81 14.77\nCOCO longest 10.68 8.14 8.08 9.28 10.62 11.62 12.61 13.38\nCOCO random 10.79 8.38 8.65 10.09 11.57 12.41 13.66 14.37\nCOCO concat. 12.40 9.48 8.96 10.03 11.37 12.39 13.29 14.10\nDe-Diffusion\n11.51 8.15 6.63 7.12\n7.85\n8.65\n9.36 10.02\nTable 6. Evaluating different captioning methods by text-to-\nimage reconstruction.\nWe report FID (\u2193) with classifier-free\nguidance scales from 1.5 to 8.0. Best FID of each method is bold.\nWe use the pre-trained Stable Diffusion v2-base model3\nas the generic text-to-image generator. We measure the sim-\nilarity between original and synthesized 256\u00d7256 images\nusing FID [31] on 30K images from MS-COCO 2014 val-\nidation split. Image generation utilizes 50 steps of DDIM\nsampling, and different classifier-free guidance scales from\n1.5 to 8.0. We report the results in Tab. 6.\nPaLI-X refers to its variant that is multi-task finetuned\non multiple image caption benchmarks. The model obtains\n147.3 CIDEr [12] on image captioning on Karpathy test\nsplit [44]. BLIP-2 [49] refers to its ViT-g OPT2.7B variant,\nwith 145.8 CIDEr captioning performance.\nFor human-annotated captions, we take advantage of the\nfive caption annotations provided for each image in MS-\nCOCO [13]. We evaluate with three different variants. In\nCOCO longest, we select the longest captions of the five\ncaptions as the prompt for text-to-image generation.\nIn\nCOCO random, we randomly sample one from the five. In\nCOCO concat., we concatenate all five captions in to a long\nsentence. As in Tab. 6, COCO longest obtains the best re-\nconstruction FID, which is the one illustrated in Fig. 3.\nB. Multi-Modal Few-Shot Learner\nB.1. Few-Shot LLM Prompts\nPrompts for LLMs in the multi-modal few-shot learn-\ning experiments are built by interleaving De-Diffusion\ntext of support set images, denoted as <De-Diffusion\ntext>, and their corresponding answers, which are fol-\nlowed by De-Diffusion text of the query image. We ran-\ndomly sample the support set from the training split. The\nLLM\u2019s completion is considered a correct answer only if it\nexactly matches the ground truth.\nFew-shot VQA. On VQA tasks including VQAv2 [28] and\nOKVQA [57], an example two-shot prompt is:\n3https : / / huggingface . co / stabilityai / stable -\ndiffusion-2-base\nAnswer the question given the context.\nImage context: <De-Diffusion text>\nImage question: Is the train moving? Short\nanswer: yes$\nImage context: <De-Diffusion text>\nImage question: What sport is this? Short\nanswer: skiing$\nImage context: <De-Diffusion text>\nImage question: Where is he looking? Short\nanswer:\nWe take LLM\u2019s output before $ as the prediction.\nFew-shot captioning. On MS-COCO captioning [13], an\nexample two-shot prompt with two shots is:\nMS COCO image captioning.\nImage context: <De-Diffusion text>\nMS COCO image caption: a man with a red\nhelmet on a small moped on a dirt road$\nImage context: <De-Diffusion text>\nMS COCO image caption: a man is standing\nnext to a window wearing a hat$\nImage context: <De-Diffusion text>\nMS COCO image caption:\nWe take LLM\u2019s output before $ as the prediction.\nFew-shot classification. For a 2-way 1-shot classification\non miniImageNet between class lion and vase, the prompt\nwith task induction is:\nClassify the context into \"lion\" or \"vase\".\nContext: <De-Diffusion text>\nClassification: lion.\nContext: <De-Diffusion text>\nClassification: vase.\nContext: <De-Diffusion text>\nClassification:\nWe take LLM\u2019s output before period as the prediction.\nIn the case without induction, we remove the first sentence.\nB.2. Zero-Shot Generalization\nZero-shot prompt. Following Flamingo [4], we build the\nprompts with several pseudo samples from the downstream\ntasks, where we remove De-Diffusion text of the support\nset images and only keep their corresponding answers. We\ntake the pseudo samples as a form of prompt engineering,\nfor example, to teach the model to end the answers with the\nsymbol $. An example zero-shot VQA prompt with two\npseudo samples is:\n18\nAnswer the question given the context.\nImage context:\nImage question: Is the train moving? Short\nanswer: yes$\nImage context:\nImage question: What sport is this? Short\nanswer: skiing$\nImage context: <De-Diffusion text>\nImage question: Where is he looking? Short\nanswer:\nWe take LLM\u2019s output before $ as the prediction.\n# pseudo\n4-shot\n0-shot\npseudo qry\nsample\n0\n4\n8\n16\n4\n8\n16\n32\n32\nVQAv2 65.6 65.9 66.1 66.0 64.8 64.9 65.1 65.2\n43.4\nOKVQA 57.1 57.7 57.8 58.2 56.0 55.9 56.7 57.0\n36.3\nTable 7. Effectiveness of pseudo samples for 4-shot and 0-shot\nVQA tasks. We experiment with 4-shot support with another 0, 4,\n8, and 16 pseudo samples in the prompts, and 0-shot situation with\nanother 4, 8, 16, 32 pseudo samples. VQAv2 is evaluated on the\nvalidation split and OKVQA is on the test split. Best results are\nbold. Results reported in Tabs. 1 and 2 are in gray . Pseudo qry\ndenotes the situation where the query\u2019s context is also left blank.\nEffectiveness of pseudo samples on VQA. We quantita-\ntively evaluate the effectiveness of pseudo samples. Results\nin Tab. 7 are obtained by a PaLM 2-L. The 4-shot situa-\ntion can work alone without any pseudo samples and still\nachieves decent results, and it benefits from more pseudo\nsamples. On the other hand, our method can not work with-\nout any pseudo samples in the zero-shot setting, where the\ncompletion of LLMs can be in any format so that it can\nnot evaluated by the exact-match evaluation protocol. The\nzero-shot setting also benefits from more pseudo samples.\nWe further evaluate a case where both the support sam-\nples and the query are pseudo. In other words, the query\u2019s\nimage context is also left blank as the support samples, and\nonly the question itself is kept. In this case, LLMs tend\nto complete the answer by a reasonable guess based on\nthe commonsense knowledge. For example, a yes or no\nanswer for the question Is the train moving?, or\nbaseball for the question What sport is this?.\nAnd we obtain 43.4 for VQAv2 and 36.3 for OKVQA. We\nbelieve these numbers set a bottom line performance for\nthese VQA tasks, which an advanced LLM can obtain with-\nout any visual cues.\nC. Ablation\nEffectiveness of De-Diffusion text substring. By default,\nDe-Diffusion text consists of 75 tokens to use up CLIP text\n15\n30\n45\n60\n75\nsubstring length\n27\n80\n85\n95\n90\n95\n5-way 1-shot accuracy (%)\nFigure 10.\nEffectiveness of De-Diffusion substring.\nWe ex-\ntract different lengths of the prefix substrings of De-Diffusion text\nand use the substrings for open-ended 5-shot 1-way miniImageNet\nclassification. Task induction is used.\nencoder\u2019s context length, which are decoded to be a long\ntext string. Here, we evaluate which part of the string con-\ntains most information. Specifically, we extract different\nlengths of their prefix substrings, from short to long, and\nuse the substring for open-ended 5-shot 1-way miniIma-\ngeNet classification. Task induction is used. The results\nare plotted in Fig. 10. With longer prefix, the few-shot clas-\nsification accuracy increases. The first a few tokens are less\ninformative, obtaining a lower accuracy, and the prefix of\n15 tokens starts to retain most of the full few-shot classifi-\ncation performance, with accuracy around 95%. In practice,\nwe found De-Diffusion text often starts with the style of the\nimages as in Fig. 9, which could reflect the common cases\nin the image-text training data of the CLIP text encoder and\nthe text-to-image diffusion model.\nA toy example on ImageNet. In Tab. 4d we explore the\ncase where the image backbone is randomly initialized and\ntrained with the reconstruction objective. We further ex-\nplore a similar toy example on ImageNet, where the de-\ncoder is a 128\u00d7128 class-conditioned ImageNet generative\ndiffusion model, and the encoder is a randomly initialized\nViT-Base [21]. The class-conditioned ImageNet model ob-\ntains an FID of 3.82. The latent space, in this case, is a\ndiscrete prediction of the class label, assuming values of\n[ 0 , 1 , . . . , 999 ] to reflect 1000 classes in ImageNet, which\nis identical to a typical classification model. We train the\nmodel for a long 500K-step schedule with batch size of\n2048. No augmentation is used. As a result, the model\nobtains 47.7% accuracy on ImageNet classification. These\nresults, together with other methods that use diffusion mod-\nels for classification [18, 48, 84], provide another aspect of\nthe potential of generative models for image classification,\nand in the long run, image recognition.\n19\nStep 1: Obtaining De-Diffusion text.\n[De-Diffusion text of image A] a colrejolossoils painting of transformer robot robot standing wearing dusk\nred robot in a blue armor it across blue waves amidst towards a a yellow pollens a swirl behind viewed\nbetween a colorful swirl swirl beside colorful yellow sunset cloudy colourful hills colourful valleys smh\nwearing gogh gogh bered red blue blue blue painting presented red red red psorirobot robson capcom\nmodernist gicpainting painting painting blue painting abstract mural\n[De-Diffusion text of image B] a colstavgmbmagewallpapers of deer deer animal standing standing wearing\ndeer deer on a water water it on a river accompanied asting a a orangepolreflection a water among blur\ndespite a green trees trees also but yellow autumn misty among forests autumnal fir trunk besides snow\nwinter shutterorange teal teal darkness forest featuring bered auburn auburn majeantlers bavholistic fantasy\nforest deer wallpapers wallpapers wide reflections forest forest\nStep 2: Conversations with a chatbot.\n[Prompt] Image A: <De-Diffusion text of image A>. Image B: <De-Diffusion text of image B>.\nDescribe a new image with image A\u2019s main object in image B\u2019s scene and style and translate the description to a prompt.\n[ChatGPT] Show a dusk red transformer robot standing tall beside a river in a holistic fantasy forest. Surround the robot with green trees\ntouched by yellow autumn mist, and the reflections of the robot in the water touched by orange hues. The backdrop should feature yellow\nautumn leaves, snow-touched fir trunks, and capture a blend of modernist and fantasy aesthetics.\n[Prompt] Describe a new image with image B\u2019s main object in image A\u2019s scene and style and translate the description to a prompt.\n[ChatGPT] Depict a deer amidst swirling blue waves with a colorful sunset behind it. Surround the deer with abstract modernist blue and red\nswirls, with yellow pollens illuminating the scene. The backdrop should feature colorful hills and valleys in the style of a Van Gogh painting,\ndominated by rich red and blue hues, merging the abstract mural feel with the serenity of nature.\nStep 3: Image generation with text-to-image tools.\nA + B (Stable Diffusion XL)\nA + B (Midjourney)\nB + A (Stable Diffusion XL)\nB + A (Midjourney)\nTable 8. Text-based image blending. Step 1: Obtain De-Diffusion text to represent image A and B. Step 2: Ask ChatGPT to describe\na new image with image A\u2019s main object in image B\u2019 background, and vice versa. Step 3: Use ChatGPT generated prompts for Stable\nDiffusion XL and Midjourney to get the blended new images. Original images are synthetic with links in E.\n20\nD. Text-Based Image Blending\nBlending two images by interpolating their deep embed-\ndings is often explored as a type of image manipulation\n(e.g., [65]). In this work, we encode images as text. There-\nfore, we showcase a novel text-based image blending as in\nTab. 8. Specifically, we use De-Diffusion text to represent\ntwo images and ask ChatGPT to describe a new image mix-\nture. With this description from ChatGPT as the prompt,\nwe generate new images as the blended results with differ-\nent text-to-image tools.\nThe new images are not as similar to each other as in\nthe samples of text-to-image reconstruction (Figs. 6 and 7),\nlikely because the ChatGPT-generated description is not as\nprecise and extensive as De-Diffusion text. However, each\nof them can be taken as a reasonable blending result, cap-\nturing the main object of \u201ctransformer robot\u201d and \u201cdear\u201d in\nthe foreground, and \u201cautumn forest\u201d and \u201cVan Gogh style\nswirling\u201d in the background. These results again demon-\nstrate the possibility of text as an alternative cross-modal\ninterface to deep embeddings.\nE. Source of Synthetic Images\nWe use ClipDrop4 to generate all the images of Stable\nDiffusion XL v1.0, and Midjourney v5.25 for all the images\nof Midjourney. We summarize the links the the synthetic\nimages we used as the original as follows:\nFig. 1\nhttps : / / imagen . research . google / main _ gallery _\nimages/a-photo-of-a-corgi-dog-riding-a-bike-in-\ntimes-square.jpg\nFig. 8 (a)\nhttps://ideogram.ai/g/hF8ZIUScTA-_NWrXdYS40Q/2\nFig. 8 (b)\nhttps://ideogram.ai/g/FCDbFJXNRyGX_0jACq8ppw/0\nFig. 8 (c)\nhttps : / / lexica . art / prompt / d0128f70 - be78 - 40fb -\nb629-2d5488d62259\nFig. 8 (d)\nhttps://ideogram.ai/g/KGtfj-JrRAuwxWlDYI-qpA/2\nFig. 9 (a)\nhttps : / / lexica . art / prompt / 32486251 - 00bf - 47fd -\n8190-01481ff76ec9\nFig. 9 (b)\nhttps : / / lexica . art / prompt / 9085bca0 - 2eb5 - 46d0 -\n9a52-277b8d76091a\nFig. 9 (c)\nhttps://ideogram.ai/g/mBsmE04ZTZS0dKAta33bpQ/3\nTab. 8 (a)\nhttps : / / lexica . art / prompt / 60217aa0 - f27c - 43ed -\na783-20bbc45d672c\nTab. 8 (b)\nhttps : / / lexica . art / prompt / 46e1bc73 - daeb - 4216 -\na2fb-ee09fb4db603\n4https://clipdrop.co/stable-diffusion\n5https : / / docs . midjourney . com / docs / model -\nversions\nF. Acknowledgement\nWe thank Nanxin Chen, Jason Baldridge and Yonghui\nWu for valuable feedback and support.\n21\n"
  },
  {
    "title": "The Generative AI Paradox: \"What It Can Create, It May Not Understand\"",
    "link": "https://arxiv.org/pdf/2311.00059.pdf",
    "upvote": "17",
    "text": "Preprint\nTHE GENERATIVE AI PARADOX:\n\u201cWhat It Can Create, It May Not Understand\u201d\nPeter West1\u2217 Ximing Lu1,2\u2217\nNouha Dziri2\u2217\nFaeze Brahman1,2\u2217\nLinjie Li1\u2217\nJena D. Hwang2\nLiwei Jiang1,2\nJillian Fisher1\nAbhilasha Ravichander2\nKhyathi Raghavi Chandu2\nBenjamin Newman1\nPang Wei Koh1\nAllyson Ettinger2\nYejin Choi1,2\n1University of Washington\n2Allen Institute for Artificial Intelligence\n{pawest,linjli}cs.washington.edu\n{ximinglu,nouhad,faezeb}allenai.org\nABSTRACT\nThe recent wave of generative AI has sparked unprecedented global attention,\nwith both excitement and concern over potentially superhuman levels of artifi-\ncial intelligence: models now take only seconds to produce outputs that would\nchallenge or exceed the capabilities even of expert humans. At the same time,\nmodels still show basic errors in understanding that would not be expected even\nin non-expert humans. This presents us with an apparent paradox: how do we rec-\noncile seemingly superhuman capabilities with the persistence of errors that few\nhumans would make? In this work, we posit that this tension reflects a divergence\nin the configuration of intelligence in today\u2019s generative models relative to intel-\nligence in humans. Specifically, we propose and test the Generative AI Paradox\nhypothesis: generative models, having been trained directly to reproduce expert-\nlike outputs, acquire generative capabilities that are not contingent upon\u2014and can\ntherefore exceed\u2014their ability to understand those same types of outputs. This\ncontrasts with humans, for whom basic understanding almost always precedes the\nability to generate expert-level outputs. We test this hypothesis through controlled\nexperiments analyzing generation vs. understanding in generative models, across\nboth language and image modalities. Our results show that although models can\noutperform humans in generation, they consistently fall short of human capabili-\nties in measures of understanding, showing weaker correlation between generation\nand understanding performance, and more brittleness to adversarial inputs. Our\nfindings support the hypothesis that models\u2019 generative capability may not be con-\ntingent upon understanding capability, and call for caution in interpreting artificial\nintelligence by analogy to human intelligence.\n1\nINTRODUCTION\n\u201cWhat I cannot create, I do not understand.\u201d \u2013 Richard Feynman\nThe recent wave of generative AI, from ChatGPT to GPT4 to DALL-E 2/3 to Midjourney, has\nsparked unprecedented global attention\u2014with equal parts excitement about the expansive potential\napplications, and deep concern about the dangers of \u201cintelligence\u201d that seems even to exceed that of\nhumans. Indeed, in both language and visual domains, current generative models take only seconds\nto produce outputs that could challenge experts with years of skill and knowledge, providing com-\npelling motivation for claims that models have surpassed human intelligence (Bubeck et al., 2023;\nSurameery & Shakor, 2023). At the same time, probing of models\u2019 outputs continues to uncover ba-\nsic errors in understanding that would be unexpected even for non-expert humans (Dziri et al., 2023;\nArkoudas, 2023; Qin et al., 2023). This presents us with an apparent paradox: how do we reconcile\nthe seemingly superhuman capabilities of these models with the persistent presence of fundamental\nerrors that most humans could correct?\n\u2217 First co-authors.\n1\narXiv:2311.00059v1  [cs.AI]  31 Oct 2023\nPreprint\nThe story only mentions  \none design being sketched [\u2026]\nB. Go to the bar\nWrite a two paragraph story  \nabout 3 people building a house\na blue backpack  \nand a red orange\nAustin had a rough day at work  \nand decided to go to the bar.  \n Austin had a drink that night.  \nWhy did Austin do this?\n[\u2026] Finally, after years of saving \nmoney and sketching designs, they \ndecided to build their dream house. \nBob with his architectural expertise, \nsketched a breathtaking design of a \ntwo storied country house.\nAustin likely went to the bar and had \na drink because he had a rough day \nat work and wanted to relax, unwind, \nor temporarily escape from the stress \nand frustrations he experienced \nduring the day.\nSelect the best answer from the  \nchoices provided:\nHow many total designs were  \nsketched in the story?\nWhich image matches the caption?\nExpected answer:  Many\nA. Settle down \nB. Go to the bar  \nC. Order a drink\n[selected]\nBased on the image you sent, I can say  \nthat it is not a mountain goat. [\u2026]\na mountain goat stands  \non top of a rock on a hill\nIs this a mountain goat?\nExpected answer:  Yes\nGeneration\nUnderstanding\nSELECTIVE\nINTERROGATIVE\nA.\nB.\nC.\nD.\nLanguage Modality\nVision Modality\n(SELECTIVE SETTING)\n(INTERROGATIVE SETTING)\n(SELECTIVE SETTING)\n(INTERROGATIVE SETTING)\nFigure 1: Generative AI in language and vision can produce high-quality generations. Paradoxically, however,\nmodels have trouble demonstrating selective (A,C) or interrogative (B,D) understanding of these modalities.\nWe posit that this tension arises because the configuration of capabilities in today\u2019s generative mod-\nels diverges from the configuration of intelligence in humans. Specifically, in this work we propose\nand test the Generative AI Paradox hypothesis: generative models, having been trained directly to\nreproduce expert-like outputs, acquire generative capabilities that are not contingent upon\u2014and can\ntherefore exceed\u2014their ability to understand those same types of outputs. This contrasts with hu-\nmans, for whom basic understanding nearly always serves as a prerequisite to the ability to generate\nexpert-level outputs (Gobet, 2017; Alexander, 2003; Berliner, 1994).\nWe test this hypothesis through controlled experiments analyzing generation and understanding ca-\npabilities in generative models, across language and visual modalities. We conceptualize \u201cunder-\nstanding\u201d relative to generation via two angles: 1) given a generative task, to what extent can models\nselect correct responses in a discriminative version of that same task? and 2) given a correct gener-\nated response, to what extent can models answer questions about the content and appropriateness of\nthat response? This results in two experimental settings, selective and interrogative, respectively.\nThough our results show variation across tasks and modalities, a number of clear trends emerge. In\nselective evaluation, models often match or even outperform humans on generative task settings, but\nthey fall short of human performance in discriminative (understanding) settings. Further analysis\nshows that discrimination performance is more tightly linked to generation performance in humans\nthan in GPT4, and human discrimination performance is also more robust to adversarial inputs, with\nthe model-human discrimination gap increasing with task difficulty. Similarly, in interrogative eval-\nuation, though models can generate high-quality outputs across tasks, we observe frequent errors in\nmodels\u2019 ability to answer questions about those same generations, with model understanding perfor-\nmance again underperforming human understanding. We discuss a number of potential reasons for\nthis divergence in capability configurations for generative models versus humans, including model\ntraining objectives, and size and nature of input.\nOur findings have a number of broader implications. First, the implication that existing conceptual-\nizations of intelligence, as derived from experience with humans, may not be able to be extrapolated\nto artificial intelligence\u2014although AI capabilities in many ways appear to mimic or exceed human\nintelligence, the contours of the capability landscape may diverge fundamentally from expected pat-\nterns in human cognition. On the flip side, our findings advise caution when studying generative\nmodels for insights into human intelligence and cognition, as seemingly expert human-like outputs\nmay belie non-human-like mechanisms. Overall, the generative AI paradox encourages studying\nmodels as an intriguing counterpoint to human intelligence, rather than as a parallel.\n2\nTHE GENERATIVE AI PARADOX\nWe begin by outlining the Generative AI Paradox and an experimental design to test it.\n2\nPreprint\n2.1\nOPERATIONAL DEFINITIONS\nFigure 1 offers examples of the seemingly paradoxical behavior of generative models. In language\n(column B), GPT4 is able to generate a compelling story about 3 friends building a house, but when\npressed on details of its own generated story, fails to correctly answer a simple question: GPT4 as-\nserts that only one design was sketched in the story despite writing about years \u201csketching designs\u201d.\nIn vision (column C), a generator produces a correct image beyond average human capabilities, yet\nthe understanding model is unable to single out that correct generation against plausible alternatives,\ndespite selection being the seemingly \u201ceasier\u201d task. In both cases, models meet or exceed human\ngeneration abilities but lag in understanding.\nObservations such as these motivate the Generative AI Paradox:\nGenerative models seem to acquire generation abilities more effectively than un-\nderstanding, in contrast to human intelligence where generation is usually harder.\nTesting this hypothesis requires an operational definition of each aspect of the paradox. First, what\nit means for generation to be \u201cmore effective\u201d than understanding for a given model and task t, with\nhuman intelligence as a baseline. Taking g and u to be some performance measures of generation\nand understanding, we formally state the Generative AI Paradox hypothesis as:\ng(human, t) = g(model, t) =\u21d2 u(human, t) \u2212 u(model, t) > \u03f5\n(1)\nPut simply, the hypothesis holds for a task t if a human who achieves the same generation per-\nformance g as a model would be expected to achieve significantly (> \u03f5 for a reasonably large \u03f5)\nhigher understanding performance u than models do. Stated another way, models perform worse on\nunderstanding than we would expect of humans with similarly strong generative capabilities.\nGeneration is straightforward to operationally define: given a task input (question/prompt), gen-\neration is the production of observable content to satisfy that input. Thus, performance g can be\nevaluated automatically or by humans (e.g. style, correctness, preference). While understanding is\nnot defined by some observable output, it can be tested by explicitly defining its effects.Thus, we\nmeasure performance u by asking the following questions:\n1. Selective evaluation. For a given task, which can be responded to generatively, to what extent can\nmodels also select accurate answers among a provided candidate set in a discriminative version\nof that same task? A common example of this is multiple choice question answering, which\nis one of the most common ways to examine both human understanding and natural language\nunderstanding in language models (Wang et al., 2019). (Figure 1, columns A, C)\n2. Interrogative evaluation. For a given generated model output, to what extent can models accu-\nrately respond to questions about the content and appropriateness of that output? This is akin to\nan oral examination in education (Sabin et al., 2021). (Figure 1, columns B, D )\nThese definitions of understanding provide us with a blueprint for evaluating the Generative AI\nParadox, allowing us to test whether Hypothesis 1 holds across modalities, tasks, and models.\n2.2\nEXPERIMENTAL OVERVIEW\nHere, we provide a high-level road map for experiments informed by the definitions above. We\npropose 2 sub-hypotheses to test across experimental settings, and provide cross-experiment details.\n2.2.1\nHYPOTHESES\nEvaluating whether Hypothesis 1 holds for a given task requires establishing a human baseline,\nspecifically, the understanding performance we expect from a human with the same generation ca-\npabilities as the model. We define how such a baseline is established for both kinds of understanding\nabove, resulting in 2 sub-hypotheses.\nSelective evaluation.\nHere, we explicitly measure human generation and understanding perfor-\nmance to establish a baseline. We say Hypothesis 1 holds if models underperform in understanding\n3\nPreprint\nGenerative\nGPT4\nGPT3.5 Human\nDiscriminative\nGPT4\nGPT3.5 Human\nCSQA\nSocialIQA\nHellaSwag\nPIQA\n\u03b1-NLI/\u03b1-NLG\nCommonsense\nSummarization\nDialogue\nNLI\nConv. QA\nQA\nXSUM\nTopioca\nRACE\nWaNLI\n\u03b4-NLI\nFaithDial\nDREAM\nMutual+\nCSQA\nSocialIQA\nPIQA\n\u03b1-NLI/\u03b1-NLG\nFigure 2: Discriminative and generative performance of GPT3.5 and GPT4 vs Humans. Models outperform\nhumans in generation but underperform them in discrimination for most of the cases. The scatter plot in the\nbottom right summarizes GPT4\u2019s performance vs. human performance (using the hard negatives from Section\n3.2 to measure discriminative accuracy for XSUM and FaithDial); each point represents a different task. Hu-\nmans have a larger positive slope between their discrimination and generation abilities compared to GPT4.\ncompared to humans with equivalent generation performance (or lower generation performance, as-\nsuming that if humans matched model generation they would do even better at understanding. The\nsub-hypothesis is simply:\nsub-hypothesis 1: models meet or exceed humans at generation while lagging at discrimination.\nInterrogative evaluation.\nFor the human baseline here, we assume that humans can answer sim-\nple questions of understanding about their own generations. For a given task input, we test how\naccurate models are at answering questions on AI generated outputs and as the human baseline, as-\nsume near-perfect accuracy on such questions for their own generations. The sub-hypothesis in this\ncase is:\nsub-hypothesis 2: models struggle to answer simple questions about generated content, which hu-\nmans could answer for their own generations.\n2.2.2\nMODELS AND EXPERIMENTS\nWe focus our study on the strongest current generative models, i.e., those driving interest and con-\ncern among experts and the public. We investigate language and vision, modalities where recent\nimpressive progress has been made. For language, we use GPT4 and GPT3.5 as both generation\nand understanding models. In the vision domain, the strongest generators and understanding models\nare typically separate. We use Midjourney (Inc., 2023) to generate, CLIP (Radford et al., 2021) and\nOpenCLIP (Ilharco et al., 2021) as understanding models for selective evaluation, and BLIP-2 (Li\net al., 2023), BingChat (Microsoft, 2023), and Bard (Google, 2023) for interrogative evaluation.\nWe conduct experiments across both sub-hypotheses, investigating tasks with selective evaluation\nof understanding (sub-hypothesis 1) in \u00a73 and investigating tasks with interrogative evaluation of\nunderstanding (sub-hypothesis 2) in \u00a74. Both sections include both language and vision tasks.\n3\nCAN MODELS DISCRIMINATE WHEN THEY CAN GENERATE?\nFirst, in our selective evaluation, we conduct a side-by-side performance analysis on generative\nand discriminative variants of tasks to assess models\u2019 generation and understanding capabilities in\n4\nPreprint\nMidjourney (generative)\nCLIP (discriminative)\nOpenCLIP (discriminative)\nHuman (discriminative)\nFigure 3: Model and human performance under the generative and discriminative settings on the vision modal-\nity. We observe models fall short of human accuracy in discriminative performance, and their generative accu-\nracy also lags behind their discriminative accuracy.\nGPT4 (discriminative)\nOpenCLIP (discriminative)\nHuman (discriminative)\nLanguage\nVision\nFigure 4: Model vs. human performance across varying levels of answer difficulty on discriminative tasks.\nlanguage and vision modalities. We compare this generative and discriminative performance to that\nof humans. For our tasks we draw on diverse source benchmarks, detailed below:\nLanguage benchmarks.\nFor dialogue, we explore two open-ended datasets\u2014Mutual+ (Cui\net al., 2020) and DREAM (Sun et al., 2019), and a document-grounded benchmark, Faithdial (Dziri\net al., 2022). These tasks require generating coherent continuations based on conversation history\n(faithful to the document in grounded dialogue). For reading comprehension, we include Topioca\n(Adlakha et al. 2022; conversational QA) and RACE (Lai et al. 2017; factual QA). For summariza-\ntion, we consider XSUM (Narayan et al., 2018). We also include the commonsense benchmarks\nCommonSenseQA (Talmor et al., 2019), SocialIQA (Sap et al., 2019), HellaSwag (Zellers et al.,\n2019), PIQA (Seo et al., 2018), and \u03b1NLG/\u03b1NLI (Bhagavatula et al., 2020). Lastly, we consider\nthe natural language inference tasks WaNLI (Liu et al., 2022) and \u03b4-NLI (Rudinger et al., 2020).\nVision benchmarks.\nFor image generation, we source text prompts from four benchmarks: these\nrange from descriptions of natural scenes, (likely in-domain for the model) to out-of-distribution\nscenes with specific attributes and relationships that rarely exist in real images. Prompts are sourced\nfrom: COCO (Lin et al., 2014), PaintSkill (Cho et al., 2022), DrawBench (Saharia et al., 2022)\nand T2ICompBench (Huang et al., 2023). More dataset details are in \u00a7A.2.\nExperimental setup.\nFor each task and modality, we consider two settings: i) generative: we\nprompt models to generate a response given task-specific inputs (e.g., dialogue history, document,\nimage caption), and ii) discriminative: we require task-specific models to select the correct answer\nfrom a set of candidates, using existing candidates where available and otherwise generating options.\nFor the generative setting, we conduct human evaluations using Amazon Mechanical Turk (AMT) to\njudge the correctness of responses (i.e, text or image) and report percentage of successful responses\nsatisfying task requirements. For the discriminative setting, we report the accuracy of choosing the\nground-truth response among the candidate options. To establish a human performance baseline,\nwe ask workers to perform all discriminative tasks and evaluate the correctness of the ground-truth\nresponses for each task.1 Details of AMT annotations and instructions are in \u00a7D.\n3.1\nGENERATIVE AND DISCRIMINATIVE CAPABILITIES IN MODELS VS. HUMANS\nLanguage.\nFigure 2 presents a comparison of GPT3.5, GPT4, and human generative and discrim-\ninative performances. We see that for 10 of the 13 datasets, Sub-hypothesis 1 is supported in at\nleast one model, with models outperforming humans in generation but underperforming humans in\ndiscrimination. For 7 of the 13 datasets, this sub-hypothesis is supported in both models.\n1Ground-truth responses were initially written by humans for the language tasks, while ground-truth images\nare generated by Midjourney.\n5\nPreprint\nVision.\nIt is not practical to ask humans to produce detailed images as we do with vision models,\nbut we assume that an average human could not achieve the stylistic quality of models like Midjour-\nney and thus assume human generation performance is lower. Therefore, we only compare models\u2019\ngenerative and discriminative accuracy to humans\u2019 discriminative accuracy. Similar to the language\ndomain, Figure 3 shows that CLIP and OpenCLIP2 fall short of human accuracy in discriminative\nperformance. Assuming human generation is worse, this agrees with sub-hypothesis 1: Vision AI\nexceeds average humans at generation but lags at understanding.\n3.2\nMODELS FALL FURTHER SHORT OF HUMAN PERFORMANCE WITH HARDER\nDISCRIMINATION TASKS\nWe take a closer look at the gap in discriminative performance between humans and models by\nmanipulating the difficulty of the negative candidates. Two types of negatives are considered: i)\nHard negatives: challenging examples that deter models from relying on data biases and artifacts\nto produce an answer. These negatives are wrong in subtle and challenging ways; recognizing\nthem may require profound understanding of the task. ii) Easy negatives: these candidates are\nsemantically distant from the topic of the question, providing a clear contrast to the correct answer.3\nFigure 4 (left) shows the comparison between GPT4 and humans4. Notably, as the complexity of\nthe candidate answers increases, model performance gradually declines. For instance, in the XSUM\ntask, GPT4 achieves 100% accuracy when selecting the correct answer from easy negatives, but\nthis drops to 19% when confronted with hard negatives. XSUM exhibits a substantial difference\nin performance compared to FaithDial. Upon inspection, we observe that models tend to make\nthe most mistakes in discrimination tasks when the responses are lengthy and challenging, such\nas summarizing lengthy documents. In contrast, humans can maintain a consistently high level of\naccuracy across different levels of difficulty.\nCSQA\nSocialIQA\nHellaSwag\nPIQA\naNLG\nFaithDial\nDREAM\nMutual+\nWaNLI\n-NLI\nXSUM\nTopioca\nRACE\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT4\nHuman\nBoth\nFigure 5: Human\u2019s preference scores between\nhuman-generated vs. GPT4-generated responses\nFigure 4 (right) shows the discriminative perfor-\nmance of OpenCLIP, in comparison to humans,\nacross difficulty levels. Consistent with the language\nresults, and even more robustly across tasks, we\nsee that while humans show versatile performance\nacross hard and easy negative settings, model per-\nformance drops substantially when confronted with\nhard negatives (from 100% to \u223c69%). Overall, these\nresults highlight that humans have the ability to dis-\ncern correct answers even when faced with challeng-\ning or adversarial examples, but we see that this ca-\npability is not as robust in LMs. This discrepancy\nraises questions about the true extent of these mod-\nels\u2019 understanding.\n3.3\nMODEL GENERATIONS ARE PREFERRED OVER HUMAN GENERATIONS\nTo better understand the gap between humans and language models, we asked AMT workers to\nprovide their preferences between machine and human-generated answers in the language-related\ntasks, along with a rationale for their choices5. While both sets of responses score high in correctness\n(Figure 2), Figure 5 shows a notable trend: workers often favor responses from GPT4 over those\ngenerated by humans. The same applies for GPT3.5 (Figure 8 in \u00a7B.3). The rationales provided by\nhumans often indicate a preference for GPT4 due to longer response length, more elegant writing\nstyle, and being more informative, while human choice is preferred for brevity and conciseness\n(Figure 9 in \u00a7C). This makes the divergence in capabilities\u2013with models excelling in relative terms\nat generation and humans at understanding-based tasks\u2013even more apparent.\n2We report the best results on CLIP (clip-vit-large-patch14) and OpenCLIP (CLIP-ViT-bigG\n-14-laion2B-39B-b160k), more results can be found in \u00a7B.3.\n3See \u00a7B.2 for details about the negative candidates construction. For the language domain, hard negatives\nare constructed only for tasks that are originally generative in nature (i.e., FaithDial and XSUM).\n4The same trend also applies for GPT3.5.\n5See Figure 9 in \u00a7 B.3 for details.\n6\nPreprint\nGenerative\nQA\nGPT4\nMidjourney\nGPT4\nBard\nBLIP-2\nHuman\nBingChat\nLanguage\nVision\nFigure 6: Models vs. human performance on language/visual QA based on model generated texts/images.\n4\nCAN MODELS UNDERSTAND WHAT MODELS GENERATE?\nIn the previous section, we showed that models often excel at generating accurate answers while\nlagging behind humans in the discriminative task. Now, in our interrogative evaluation, we investi-\ngate to what extent models can demonstrate meaningful understanding of generations\u2014something\nhumans are highly capable of\u2014by directly asking models questions about generated content.\nLanguage experimental setup.\nIn language, we first prompt models to generate a paragraph us-\ning task-specific background information. Then using its generation as context, we ask the model\nmultiple-choice questions about its own generated information.6 For example, for XSUM (Narayan\net al., 2018) (summarization) we prompt the model to generate an article based on a ground-truth\nsummary, and then ask the model to select the best summary (same choices as \u00a73) for the generated\narticle. For Mutual+ (Cui et al., 2020) (dialogue), the model generates the conversation history that\nleads to a given dialogue, and then is asked to choose the best dialogue continuing that history. In\nHellaSwag (Zellers et al., 2019) (commonsense), the model generates the context preceding a given\nsentence and then selects the most fitting continuation for that generated context. We only perform\nselective evaluation on the correct generations verified by humans.\nWe use zero-shot GPT3.5 and GPT4 for all of the evaluations, both generating and question an-\nswering. We report the model generation performance, the selection performance based on content\ngenerated by the model, and human selection performance using the model\u2019s generated content. As\nan implicit baseline, we assume that humans can answer such questions about their own generations\nwith high accuracy, and so refrain from the complex process of eliciting these human generations.\nVision experimental setup.\nWe conduct interrogative evaluation on image understanding models\nvia visual question answering in an open-ended setting. We consider TIFAv1.0 (Hu et al., 2023) as\nthe evaluation benchmark, with text prompts from COCO, PaintSkill, DrawBench and Parti (Yu\net al., 2022). TIFAv1.0 includes questions automatically generated by a language model, only con-\ncerning the content specified in the text prompt (e.g., about existence/attributes of an object and\nrelative position between objects). We first ask Midjourney to generate images, based on the text\nprompts. Then, we interrogate the understanding models (e.g., BLIP-2) with answerable questions\n(verified by AMT workers) about the generated images. AMT is used to collect human responses,\nand judge the correctness of human/model outputs. See \u00a7C.1 for more details.\nResults.\nResults for the language modality are shown in Figure 6 (left). We observe that while\nthe models excel at generation, they make frequent errors in answering questions about their own\ngenerations, indicating failures in understanding. Humans, who we assume could not generate such\ntext at the same speed or scale, consistently achieve higher accuracy in QA compared to the model,\ndespite the fact that questions are about the model\u2019s own output. As stated in sub-hypothesis 2,\nwe expect humans would achieve even higher accuracy for their own generations. We note that the\nhumans in this study are not experts; producing text as sophisticated as the model\u2019s output could be a\nsignificant challenge. We anticipate that the performance gap in understanding one\u2019s own generation\nwould widen even more when comparing the model to human experts, who are likely to answer such\nquestions with near-perfect accuracy.\n6Unlike \u00a73, questions here are about the generation, rather than taking the generation as a potential answer.\n7\nPreprint\nFigure 6 (right) shows the interrogative results in the visual modality.7 We see that image under-\nstanding models still fall short of human accuracy in answering simple questions about elements\nin the generated images. At the same time, state-of-the-art image generation models can generate\nimages at a quality and speed beyond most average humans (who we expect will have trouble gen-\nerating comparable realistic images), indicating a relative gap between generation (stronger) and\nunderstanding (weaker) in vision AI compared to humans. Surprisingly, the performance gap be-\ntween models and humans is smaller for simpler models than advanced multimodal LLMs (i.e.,\nBard and BingChat), which have some intriguing visual understanding abilities, but still struggle to\nanswer simple questions about generated images.\n5\nDISCUSSION\nAssessing the generative AI paradox.\nBroadly, we find significant experimental evidence of the\nGenerative AI Paradox: though models can regularly outperform humans in text and image genera-\ntion, they fall short of human performance in discriminative versions of generative tasks, and when\nanswering questions about generated content. Furthermore, our analyses show that discrimination\nperformance is more tightly linked to generation performance in humans than in GPT4, and that\nhuman discrimination performance is also more robust to challenging inputs. These trends vary\nacross tasks and modalities, but in general our results robustly support the hypothesis that generative\ncapability can outstrip understanding capability in models, especially compared with humans.\nProposed explanations and points of future study.\nGiven the above evidence in support of the\nGenerative AI Paradox, the next question is: what factors could lead to models that excel at gen-\neration even when they cannot demonstrate strong understanding? We propose some hypotheses\nbelow, and encourage future work to explore this question.\nGenerative AI is defined by the generative learning objective, explicitly encouraging reconstruc-\ntion/generation of the training distribution, while only implicitly encouraging understanding if it\nfurthers this goal. Human learning, while not completely understood, likely diverges from this by\nencouraging behavior beyond pure reconstruction of stimuli.\nAlthough we often query generative models as if they were individuals, they typically model a\nmedium (e.g. text over many authors in language models). Providing context may push models\ncloser to emulating a specific individual (Andreas, 2022), but they tend towards behavior that looks\ndistributionally correct rather than individually correct, prioritizing stylistic and document-wide\nfeatures over details necessary for understanding tasks. Training on many documents (e.g. huge\nswaths of internet text) also contrasts with humans: it would take an average human reader e.g.\nover 32 years just to read all the pages of Wikipedia (contributors; Brysbaert, 2019). This obvious\ndiscrepancy in not only quantity, but also diversity of knowledge could encourage models to use\nexisting solutions to problems, which they have seen already, whereas humans have not and therefore\nneed to exercise understanding and reasoning to answer the same questions correctly.\nEvolutionary and economic pressures can affect the way that AI develops. For instance, popular lan-\nguage model architectures have shown a preference for languages like English (Ravfogel et al., 2019)\nwhich has seen the most attention in NLP (Bender, 2019) and thus the most reward for improvement.\nSimilar pressures could encourage architectures, training paradigms, and other decisions that favor\ngeneration over understanding, as generation is harder for humans and thus more useful/valuable.\nLimitations.\nDataset/benchmark contamination is a potential limitation with proprietary models,\nbut this should have similar effects on generation and discriminative evaluation in \u00a73, and our eval-\nuation in \u00a74 uses novel generations which would not be seen at training time. Also, we focus on a\nsmall set of the most popular/widely used models. Future work should investigate a wider range of\nmodels, including smaller or weaker models, for which we hypothesize the paradox may be even\nmore pronounced as we often saw with GPT3.5 vs GPT4 (\u00a73).\nWhile our evaluation of human performance is focused, future work can explore more extensive\ncomparisons between model and human performance. We also advocate for adopting comparison to\nhumans as a widespread practice, to carefully judge when model capabilities extrapolate with human\n7We report performance of BingChat, Bard and the best BLIP-2 model (BLIP2-flan-t5-xxl) on two\nsubsets, more results can be found in \u00a7C.2\n8\nPreprint\ncapabilities, and when they do not. Finally, we only investigate one divergence between humans and\nmodels. Proposing and testing other points of divergence between artificial and natural intelligence\nexceeds our scope but will be imperative to calm concerns and calibrate excitement.\n6\nRELATED WORK\nGenerative paradoxes in large language model behavior.\nPrior work paradoxically employs\nlarge language models to improve their own generations, finding that models successfully identify\nmistakes (despite these mistakes being generated by the models themselves). Madaan et al. (2023)\nprompt models to critique and improve their own generations. Agrawal et al. (2023) find that models\ncan identify hallucinated content in their own generations, and Gero et al. (2023) show that models\ncan identify erroneously omitted elements in generated in clinical extraction data.\nInconsistencies in large language models.\nPast work suggests that large language models (LMs)\nlack a robust concept representation. Dziri et al. (2023) show that strong models often struggle at\nsolving basic tasks like multiplication. Elazar et al. (2021) and Ravichander et al. (2020) show that\nLMs make inconsistent predictions when prompted with similar statements. Ribeiro et al. (2019)\nfind that QA systems often generate contradictory answers. Kassner & Sch\u00a8utze (2020) and Ettinger\n(2020) find that models can generate correct facts but also their negations. Jang et al. (2022) con-\nstruct a benchmark showing large LMs often make inconsistent predictions. Berglund et al. (2023)\ndemonstrate that while models can correctly recognize factual knowledge present in their training\ndata, they fail to make inferences related to those facts.\nGenerative models and human cognitive mechanisms.\nWhile the reasoning mechanism of mod-\nels is unknown, prior work has investigated if models possess similar competencies with humans.\nStojni\u00b4c et al. (2023) evaluate commonsense psychology, finding that while infants can reason about\nthe causes of actions by an agent, models are not capable cannot emulating this. Sap et al. (2022) find\nthat language models fail to demonstrate Theory-of-Mind. Storks et al. (2021) and Bisk et al. (2020)\nshow discrepancies between human and model capacities in physical commonsense reasoning.\n7\nCONCLUSIONS\nIn this work, we propose the Generative AI Paradox hypothesis, which posits that impressive gener-\nation abilities in generative models, by contrast to humans, may not be contingent upon commensu-\nrate understanding capabilities. We test this through controlled experiments in language and vision\nmodalities, and though our results show variation depending on task and modality, we find robust\nsupport for this hypothesis. Our findings have a number of broader implications. In particular,\nthey imply that existing conceptualizations of intelligence, as derived from experience with humans,\nmay not be applicable to artificial intelligence\u2014although AI capabilities may resemble human in-\ntelligence, the capability landscape may diverge in fundamental ways from expected patterns based\non humans. Overall, the generative AI paradox suggests that the study of models may serve as an\nintriguing counterpoint to human intelligence, rather than a parallel.\nREPRODUCIBILITY\nWe include a simple description of overall details in \u00a72, as well as experiment-specific details like\ndatasets used and evaluation setup at the beginning of each experiment section, \u00a73 and \u00a7C. These\ndescriptions are relatively brief, and we include more extensive information in the appendix. For\ninstance, we include more detail on models, model settings, and datasets in \u00a7A. We also include\nmore experimental details and further experiments that can be useful for work comparing to and\nreproducing our results in \u00a7B and \u00a7C. Finally, we include more extensive information about our\nhuman evaluation templates in \u00a7D. All datasets and models we use here are public or can be accessed\nthrough public interfaces.\n9\nPreprint\nETHICS STATEMENT\nOur work is conducted using existing benchmarks and models, and does not introduce new data,\nmethodology, or models with significant risk of harm. All experiments we conduct would be con-\nsidered analysis of existing resources, particularly in terms of the performance of models. We con-\nduct human studies, with appropriate IRB exemptions. Based on our estimates of the time for task\ncompletion, we ensure workers are paid at least $15 USD per hour. We strive to not conduct any\nexperiments that introduce additional bias, harm, or reduction in diversity, either through the way\nour research is conducted or its effects. We acknowledge that our work is primarily concerned with\ncertain aspects of performance and does not specifically measure concepts such as bias or toxicity.\nREFERENCES\nVaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de Vries, and Siva Reddy. Topi-\nocqa: Open-domain conversational question answering with topic switching. Transactions of the\nAssociation for Computational Linguistics, 10:468\u2013483, 2022.\nAyush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when they\u2019re\nhallucinating references? arXiv preprint arXiv:2305.18248, 2023.\nPatricia A Alexander. The development of expertise: The journey from acclimation to proficiency.\nEducational researcher, 32(8):10\u201314, 2003.\nJacob Andreas. Models of meaning?\nThe 11th Joint Conference on Lexical and Computational\nSemantics at NAACL, 2022.\nKonstantine Arkoudas. Gpt-4 can\u2019t reason. arXiv preprint arXiv:2308.03762, 2023.\nEmily\nBender.\nHigh\nresource\nlanguages\nvs\nlow\nresource\nlan-\nguages.\nThe\nGradient,\n2019.\nURL\nhttps://thegradient.pub/\nthe-benderrule-on-naming-the-languages-we-study-and-why-it-matters/\n#fn4.\nLukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Kor-\nbak, and Owain Evans. The reversal curse: Llms trained on\u201da is b\u201dfail to learn\u201db is a\u201d. 2023.\nURL https://api.semanticscholar.org/CorpusID:262083829.\nDavid C Berliner. Expertise: The wonder of exemplary performances. Creating powerful thinking\nin teachers and students, pp. 161\u2013186, 1994.\nChandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman,\nHannah Rashkin, Doug Downey, Wen tau Yih, and Yejin Choi.\nAbductive commonsense\nreasoning.\nIn International Conference on Learning Representations, 2020.\nURL https:\n//openreview.net/forum?id=Byg1v1HKDB.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 7432\u20137439, 2020.\nMarc Brysbaert. How many words do we read per minute? a review and meta-analysis of reading\nrate. Journal of Memory and Language, 109:104047, 2019. ISSN 0749-596X. doi: https://doi.\norg/10.1016/j.jml.2019.104047.\nURL https://www.sciencedirect.com/science/\narticle/pii/S0749596X19300786.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.\nSparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\nJaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social\nbiases of text-to-image generative transformers. 2022.\n10\nPreprint\nWikipedia\ncontributors.\nWikipedia:size\nof\nwikipedia\n-\nwikipedia.\nURL\nhttps:\n//en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia#:\u02dc:text=As%\n20of%2022%20September%202023,of%20all%20pages%20on%20Wikipedia.\nLeyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming Zhou. MuTual: A dataset for multi-turn\ndialogue reasoning. In Proceedings of the 58th Annual Meeting of the Association for Compu-\ntational Linguistics, pp. 1406\u20131416, Online, July 2020. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2020.acl-main.130. URL https://aclanthology.org/2020.\nacl-main.130.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning, 2023.\nNouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M Ponti, and Siva\nReddy. FaithDial: A Faithful Benchmark for Information-Seeking Dialogue. Transactions of the\nAssociation for Computational Linguistics, 10:1473\u20131490, 12 2022. doi: 10.1162/tacl a 00529.\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West,\nChandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and fate: Limits of transformers\non compositionality. arXiv preprint arXiv:2305.18654, 2023.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich\nSch\u00a8utze, and Yoav Goldberg. Measuring and improving consistency in pretrained language mod-\nels. Transactions of the Association for Computational Linguistics, 9:1012\u20131031, 2021. doi:\n10.1162/tacl a 00410. URL https://aclanthology.org/2021.tacl-1.60.\nAllyson Ettinger. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for Computational Linguistics, 8:34\u201348, Jan\n2020. ISSN 2307-387X. doi: 10.1162/tacl a 00298. URL http://dx.doi.org/10.1162/\ntacl_a_00298.\nZelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel Galley, Jianfeng Gao, and\nHoifung Poon. Self-verification improves few-shot clinical information extraction. arXiv preprint\narXiv:2306.00024, 2023.\nFernand Gobet. Understanding expertise: A multi-disciplinary approach. Bloomsbury Publishing,\n2017.\nGoogle. Bard. https://bard.google.com, 2023. Accessed before: 2023-09-28.\nYushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A\nSmith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question an-\nswering. arXiv preprint arXiv:2303.11897, 2023.\nKaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu.\nT2i-compbench: A com-\nprehensive benchmark for open-world compositional text-to-image generation. arXiv preprint\narXiv:2307.06350, 2023.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori,\nAchal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali\nFarhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/\nzenodo.5143773. If you use this software, please cite it as below.\nMidjourney Inc. Midjourney. https://midjourney.com, 2023. Accessed before: 2023-09-\n28.\nMyeongjun Jang, Deuk Sin Kwon, and Thomas Lukasiewicz.\nBECEL: Benchmark for consis-\ntency evaluation of language models. In Proceedings of the 29th International Conference on\nComputational Linguistics, pp. 3680\u20133696, Gyeongju, Republic of Korea, October 2022. Inter-\nnational Committee on Computational Linguistics. URL https://aclanthology.org/\n2022.coling-1.324.\n11\nPreprint\nNora Kassner and Hinrich Sch\u00a8utze. Negated and misprimed probes for pretrained language models:\nBirds can talk, but cannot fly. Association for Computational Linguistics, 2020.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.\nRACE: Large-scale\nReAding comprehension dataset from examinations.\nIn Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing, pp. 785\u2013794, Copenhagen, Denmark,\nSeptember 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL\nhttps://aclanthology.org/D17-1082.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In ICML, 2022.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-\nimage pre-training with frozen image encoders and large language models.\narXiv preprint\narXiv:2301.12597, 2023.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. WANLI: Worker and AI collabo-\nration for natural language inference dataset creation. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pp. 6826\u20136847, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.508.\nURL https://aclanthology.org/2022.findings-emnlp.508.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nMicrosoft. Bingchat. https://bing.com/chat, 2023. Accessed before: 2023-09-28.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\u2019t give me the details, just the sum-\nmary! topic-aware convolutional neural networks for extreme summarization. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1797\u20131807,\nBrussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206.\nOpenAI. ChatGPT: Optimizing language models for dialogue, 2022.\nOpenAI. GPT-4 technical report, 2023. URL https://arxiv.org/pdf/2303.08774.pdf.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi\nYang. Is chatgpt a general-purpose natural language processing task solver?\narXiv preprint\narXiv:2302.06476, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nShauli Ravfogel, Yoav Goldberg, and Tal Linzen. Studying the inductive biases of rnns with syn-\nthetic variations of natural languages. In North American Chapter of the Association for Com-\nputational Linguistics, 2019. URL https://api.semanticscholar.org/CorpusID:\n80628431.\nAbhilasha Ravichander, Eduard Hovy, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Che-\nung. On the systematicity of probing contextualized word representations: The case of hypernymy\nin BERT. In Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,\npp. 88\u2013102, Barcelona, Spain (Online), December 2020. Association for Computational Linguis-\ntics. URL https://aclanthology.org/2020.starsem-1.10.\n12\nPreprint\nMarco Tulio Ribeiro, Carlos Guestrin, and Sameer Singh. Are red roses red? evaluating consistency\nof question-answering models. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 6174\u20136184, Florence, Italy, July 2019. Association for Com-\nputational Linguistics. doi: 10.18653/v1/P19-1621. URL https://aclanthology.org/\nP19-1621.\nRachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ro-\nnan Le Bras, Noah A. Smith, and Yejin Choi. Thinking like a skeptic: Defeasible inference\nin natural language.\nIn Findings of the Association for Computational Linguistics: EMNLP\n2020, pp. 4661\u20134675, Online, November 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.findings-emnlp.418. URL https://aclanthology.org/2020.\nfindings-emnlp.418.\nMihaela Sabin, Karen H Jin, and Adrienne Smith.\nOral exams in shift to remote learning.\nIn\nProceedings of the 52nd ACM Technical Symposium on Computer Science Education, pp. 666\u2013\n672, 2021.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Com-\nmonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pp. 4463\u20134473, Hong Kong, China, Novem-\nber 2019. Association for Computational Linguistics.\ndoi: 10.18653/v1/D19-1454.\nURL\nhttps://aclanthology.org/D19-1454.\nMaarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. Neural theory-of-mind? on the limits of\nsocial intelligence in large lms. arXiv preprint arXiv:2210.13312, 2022.\nMinjoon Seo, Tom Kwiatkowski, Ankur P Parikh, Ali Farhadi, and Hannaneh Hajishirzi. Phrase-\nindexed question answering: A new challenge for scalable document comprehension. In EMNLP,\n2018.\nGala Stojni\u00b4c, Kanishk Gandhi, Shannon Yasuda, Brenden M Lake, and Moira R Dillon. Common-\nsense psychology in human infants and machines. Cognition, 235:105406, 2023.\nShane Storks, Qiaozi Gao, Yichi Zhang, and Joyce Chai. Tiered reasoning for intuitive physics: To-\nward verifiable commonsense language understanding. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2021, pp. 4902\u20134918, Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.422.\nURL https://aclanthology.org/2021.findings-emnlp.422.\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. DREAM: A challenge\ndata set and models for dialogue-based reading comprehension. Transactions of the Association\nfor Computational Linguistics, 7:217\u2013231, 2019. doi: 10.1162/tacl a 00264. URL https:\n//aclanthology.org/Q19-1014.\nNigar M Shafiq Surameery and Mohammed Y Shakor. Use chat gpt to solve programming bugs.\nInternational Journal of Information Technology & Computer Engineering (IJITC) ISSN: 2455-\n5290, 3(01):17\u201322, 2023.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pp. 4149\u20134158, Minneapolis, Min-\nnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL\nhttps://aclanthology.org/N19-1421.\n13\nPreprint\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nInternational Conference on Learning Representations, 2019. URL https://openreview.\nnet/forum?id=rJ4km2R5t7.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers:\nState-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\nShunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, and Karthik Narasimhan. COLLIE:\nSystematic construction of constrained text generation tasks, 2023.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\nHellaSwag: Can a\nmachine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Associ-\nation for Computational Linguistics, pp. 4791\u20134800, Florence, Italy, July 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1472. URL https://www.aclweb.org/\nanthology/P19-1472.\n14\nPreprint\nA\nMODELS AND DATASETS\nA.1\nMODELS\nFor the language domain, We evaluate the performance of 2 LLMs: GPT4 (gpt-4) (OpenAI, 2023)\nand GPT3.5 (GPT3.5-turbo) (OpenAI, 2022). During inference, we set nucleus sampling p to\n0.7 and temperature to 1. For each task, we evaluate the performance of each model on 500 test\nexamples.\nFor the vision domain, we choose the strongest model available to us (i.e., Midjourney (Inc., 2023))\nas the image generator. In practice, Midjourney generates 4 images for each text prompt. For image\nunderstanding, we evaluate a wide spectrum of models, including variations of CLIP (Radford et al.,\n2021), OpenClip (Ilharco et al., 2021) for selective evaluation, and BLIP (Li et al., 2022), BLIP-\n2 (Li et al., 2023), Instruct-BLIP (Dai et al., 2023), Bard (Google, 2023) and BingChat (Google,\n2023) for interrogative evaluation. For all open-source models, we adopt the implementation and\nmodel weights available on HuggingFace (Wolf et al., 2019).\nA.2\nDATASETS\nVision.\nFor selective evaluation, we source text prompts from 4 datasets, COCO (Lin et al., 2014),\nPaintskill (Cho et al., 2022), DrawBench (Saharia et al., 2022) and T2ICompBench (Huang et al.,\n2023). COCO prompts are human-written captions on real images. PaintSkill features text prompts\nthat examine image generation on specific object categories, object counts and spatial relations be-\ntween objects. DrawBench additionally test for long-form text, rare words, and challenging prompts.\nT2ICompBench is designed to test models on open-world, compositional text-to-image generation,\nwith text prompts covering 3 categories, attribute binding, object relationships, and complex com-\npositions. For interrogative evaluation, we consider TIFAv1.0 (Hu et al., 2023) as the evaluation\nbenchmark. The text prompts in TIFAv1.0 are originally from COCO, Paintskill, DrawBench and\nParti (Yu et al., 2022). For each text prompt, TIFAv1.0 includes questions automatically generated\nby a language model, only concerning the content specified in the text prompt (e.g., about exis-\ntence/attributes of an object and relative position between objects).\nB\nCAN MODELS DISCRIMINATE WHEN THEY CAN GENERATE?\nB.1\nSETUP\nVision.\nWe follow the setup on language tasks and consider two settings on each dataset for eval-\nuation: i) generative: we prompt Midjourney to generate images given the text descriptions, and ii)\ndiscriminative: we require the image understanding models to select the image, that better matches\nthe text description, from two candidates. For the generative setting, we conduct human evaluations\non AMT to judge whether the generated image matches the text prompt. In total, we randomly\nsample 100 text prompts per dataset. As Midjourney generates 4 images for each text prompt and\nusers of Midjourney in practice would pick the best image among the four, we report the success\nrate per prompt as the evaluation metric. For the discriminative setting, we construct the candidates\nwith a negative image for each positive image from the successful generations verified by human\nworkers of a given prompt. We report accuracy as the evaluation metric. Human performance on\ndiscriminative setting is measured by comparing the majority of 3 human responses to a 4th one.\nB.2\nNEGATIVE EXAMPLE CONSTRUCTION\nLanguage.\nTo construct the negative examples for the FaithDial and XSUM datasets, we explore\nthree corruptions processes:\n1. Easy negatives: we compile responses that are unrelated to the information provided in the\nknowledge snippet K (such as a dialogue or summary document). For a given context, we\nrandomly select a gold response that was based on a different K.\n2. Moderately hard negatives: we perturb the groundtruth answer by replacing up to two\nentities with entities randomly chosen from the training data.\n15\nPreprint\nFigure 7: GPT3.5 vs. Humans. Humans\nshow a larger positive correlation between\ntheir discrimination and generation abilities\ncompared to GPT3.5.\nCSQA\nSocialIQA\nHellaSwag\nPIQA\naNLG\nFaithDial\nDREAM\nMutual+\nWaNLI\n-NLI\nXSUM\nTopioca\nRACE\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nChatGPT\nHuman\nBoth\nFigure 8:\nQuality scores of human-generated re-\nsponses vs. GPT3.5 response scores\n3. Hard negatives: To generate examples that are likely hallucinations but sufficiently chal-\nlenging to distinguish from correct answers, we directly perturb the knowledge spans K\nand then feed them to GPT4. We replace up to two entities in the original K with entities\nof the same type from the same document to avoid easy-to-detect off-topic entities. The re-\nsponse generated by GPT4 will be a hallucination, containing subtle alterations that render\nit incorrect when compared to the groundtruth response.\nVision.\nTo examine the discriminative performance gap of image understanding models across dif-\nferent difficulty levels, we similarly construct hard and easy negatives in the image space to evaluate\nimage understanding models: i) Hard negative: a negative image that is generated based on the\nsame text prompt as the positive image, such that it is semantically close to the text prompt, but con-\ntains subtle mistakes identifiable by humans. ii) Easy negative: a negative image that is randomly\nsampled from the successful generations of a different prompt in the same dataset (as the positive\nimage), such that it is semantically distant from the positive image and can be easily distinguishable.\nFor both cases, we use AMT to verify the negative samples and only retain the ones with agreeable\njudgments among 3 workers. In the end, we have 345 instances with hard negatives, including 52,\n72, 100 and 42 instances for COCO, PaintSkill, CompBench and DrawBench, respectively; and\n372 instances with easy negatives, comprising 82, 72, 100 and 42 instances for COCO, PaintSkill,\nCompBench and DrawBench, respectively.\nB.3\nADDITIONAL RESULTS\nLanguage.\nIn Figure 7, we show humans exhibit a larger positive correlation between their dis-\ncrimination and generation abilities compared to GPT3.5. Figure 8 illustrates that workers often fa-\nvor responses from GPT3.5 over those generated by humans. Figure 9 shows the rationales provided\nby humans on their preferences for GPT4 responses compared to groundtruth human responses.\nVision.\nWe include additional results from different model variants of CLIP and OpenCLIP in\nTable 1. These models consistently fall short of human accuracy in discriminative performance. In\nTable 2, we observe the gap between model and human performance becomes larger as the difficulty\nlevel of the task increases with easy and hard negatives.\nC\nCAN MODELS UNDERSTAND WHAT MODELS GENERATE?\nC.1\nEXPERIMENTAL SETUP.\nLanguage\nWe additionally explore constrained generation in which models are given lexical con-\nstraints for generation. In the constrained setting, we use a compositional task that covers diverse\ngeneration levels, i.e., word, sentence, paragraph, and passage: COLLIE-v1 (Yao et al., 2023), which\ncontains 2,080 constraint instances across 13 different task types shown in Appendix Table 3. We\n16\nPreprint\nTable 1: Additional results on selective evaluation for vision modality.\nCOCO\nPaintSkill\nT2ICompBench\nDrawBench\nMidjourney (Generative)\n85.00%\n80.41%\n71.15%\n62.63%\nDiscriminative\nHuman\n92.86%\n99.30%\n97.00%\n100.00%\nCLIP\nclip-vit-base-patch16\n79.81%\n75.00%\n79.00%\n76.19%\nclip-vit-base-patch32\n83.66%\n72.39%\n77.50%\n77.38%\nclip-vit-large-patch14\n85.58%\n81.95%\n84.50%\n78.57%\nclip-vit-large-patch14-336\n87.50%\n78.47%\n81.50%\n76.19%\nOpenCLIP\nCLIP-ViT-bigG-14-laion2B-39B-b160k\n81.73%\n85.28%\n84.50%\n84.53%\nCLIP-VIT-g-14-laion2B-s12B-b42k\n82.70%\n85.41%\n83.50%\n77.38%\nCLIP-VIT-g-14-laion2B-s34B-b88K\n83.66%\n81.25%\n88.00%\n79.76%\nCLIP-ViT-H-14-laion2B-s32B-b79K\n82.69%\n85.41%\n83.50%\n77.38%\nTable 2: Additional results on model vs. human performance across varying levels of answer diffi-\nculty for vision tasks.\nCOCO\nPaintSkill\nT2ICompBench\nDrawBench\nHard\nEasy\nHard\nEasy\nHard\nEasy\nHard\nEasy\nHuman\n85.71%\n100%\n98.61%\n100%\n94.00%\n100%\n100%\n100%\nCLIP\nclip-vit-base-patch16\n59.62%\n100%\n50.00%\n100%\n58.00%\n100%\n52.38%\n100%\nclip-vit-base-patch32\n67.31%\n100%\n45.83%\n98.95%\n55.00%\n100%\n54.76%\n100%\nclip-vit-large-patch14\n71.15%\n100%\n63.89%\n100%\n69.00%\n100%\n57.14%\n100%\nclip-vit-large-patch14-336\n75.00%\n100%\n56.94%\n100%\n63.00%\n100%\n52.38%\n100%\nOpenCLIP\nCLIP-ViT-bigG-14-laion2B-39B-b160k\n63.46%\n100%\n70.83%\n99.74%\n69.00%\n100%\n69.05%\n100%\nCLIP-VIT-g-14-laion2B-s12B-b42k\n65.39%\n100%\n70.83%\n100%\n67.00%\n100%\n54.76%\n100%\nClip-VIT-g-14-laion2B-s34B-b88K\n67.31%\n100%\n62.50%\n100%\n76.00%\n100%\n59.52%\n100%\nCLIP-ViT-H-14-laion2B-s32B-b79K\n65.38%\n100%\n70.83%\n100%\n67.00%\n100%\n54.76%\n100%\nTable 3: Example of tasks in Collie Benchmark covering several generation levels including word,\nsentence, paragraph and passage.\nTask\nExample\nword01\nGenerate a word with at least 15 letters.\nword02\nGenerate a word with 10 letters, where letter 1 is \u2018s\u2019, letter 3 is \u2018r\u2019, letter 9 is \u2018e\u2019.\nword03\nGenerate a word with at most 10 letters and ends with \u2018r\u2019.\nsent01\nPlease generate a sentence with exactly 82 characters. Include whitespace into your character\ncount.\nsent02\nGenerate a sentence with 10 words, where word 3 is \u201csoft\u201d and word 7 is \u201cbeach\u201d and word 10\nis \u201cmath\u201d.\nsent03\nGenerate a sentence with at least 20 words, and each word less than six characters.\nsent04\nGenerate a sentence but be sure to include the words \u201csoft\u201d, \u201cbeach\u201d and \u201cmath\u201d.\npara01\nGenerate a paragraph where each sentence begins with the word \u201csoft\u201d.\npara02\nGenerate a paragraph with at least 4 sentences, but do not use the words \u201cthe\u201d, \u201cand\u201d or \u201cof\u201d.\npara03\nGenerate a paragraph with exactly 4 sentences, each with between 10 and 15 words.\npara04\nGenerate a paragraph with at least 3 sentences, each with at least 15 words.\npara05\nGenerate a paragraph with 2 sentences that end in \u201cmath\u201d and \u201crock\u201d respectively.\npass01\nGenerate a passage with 2 paragraphs, each ending in \u201cI sit.\u201d and \u201cI cry.\u201d respectively.\n17\nPreprint\nFigure 9: AMT Workers rationals on their preferences for GPT4 responses compared to groundtruth\nhuman responses.\n18\nPreprint\nParagraph\nSentence\nPassage\nWord\nGen. Acc.\nDisc. Acc. (overall)\nDisc. Acc. (incorrect gen.)\nDisc. Acc. (correct gen.)\nword01\nword02\nword03\nsent01\nsent02\nsent04\nsent03\npara01\npara02\npara04\npara03\npara05\npass01\nFigure 10: GPT4 Generative Constraint Satisfaction on Collie along with discriminative accuracy\non its Generations.\ngenerate outputs for 50 examples per task. We then ask models about their generations, specifically\nquerying about whether the generations satisfy the given constraints.\nVision.\nFor interrogative evaluation on vision modality, we randomly sample 25 prompts from\neach subset of TIFAv1.0, resulting in 100 prompts in total. For evaluation of image understanding\nmodels, we include all answerable questions on the generated images (verified by AMT workers)\nfrom the original dataset, and collect the groundtruth answers on this questions from human anno-\ntators. Note that even when the generated image does not strictly align with the text prompt, we still\ninclude the image-question pairs that are considered answerable by human annotators to interrogate\nunderstanding models. In the end, we gather 1,871 image-question pairs, with 533, 482, 422 and\n434 instances on COCO, Paintskill, DrawBench and Parti subset, respectively. Human performance\nis measured by comparing the majority of 3 human responses and the 4th one.\nC.2\nADDITIONAL RESULTS\nLanguage\nWe report on the constrained setting. Figure 10 shows GPT4\u2019s constraint satisfaction\nrate across 13 tasks in Collie . Certain tasks with simple constraints such as word01 (generating a\nword with a minimum number of letters), and sent04 (generating a sentence containing three spe-\ncific words) are less challenging for models. However, we observe a significant drop in performance\nwhen posed with arbitrary position constraints and strict counting requirements (e.g., sent02, gen-\nerating a sentence with x words, where the 3rd word is A, and the 7th word is B, ...), suggesting that\ncurrent models cannot handle generations when faced with rich compositional constraints. Unlike\nthe open-ended setting, we find models are often better at answering questions about their genera-\ntions than generating. We propose that more precise constraints like this are easier to judge (trivial\nfor humans) while being much harder for models to exercise flexible generation over.\nVision\nTable 4 shows the full results from different model variants of BLIP, BLIP-2, Instruct-BLIP,\nBard and BingChat on all 4 subsets of TIFAv1.0. Note that Bard and BingChat can occasionally\nrefuse to answer the question, when the image contains people. The results from these models are\non a subset when they can provide a reasonable answer. The model performance is consistently\nlower than human performance, acorss different models.\n19\nPreprint\nTable 4: More results on interrogative evaluation for vision modality.\nCOCO\nPaintSkill\nDrawBench\nParti\nMidjourney (Generative)\n84.00%\n52.00%\n72.00%\n84.00%\nQuestiong Answering\nHuman\n95.88%\n97.72%\n96.32%\n96.83%\nblip-vqa-base\n89.68%\n83.82%\n82.23%\n84.56%\nblip-vqa-capfilt-large\n89.68%\n83.82%\n82.23%\n84.56%\nBLIP2-flan-t5-xl\n89.49%\n83.20%\n81.52%\n85.25%\nBLIP2-flan-t5-xxl\n91.18%\n88.59%\n85.31%\n90.56%\nBLIP2-opt-2.7b\n81.99%\n82.16%\n72.75%\n79.03%\nBLIP2-opt-6.7b\n84.05%\n77.39%\n72.99%\n75.81%\ninstructblip-flan-t5-xl\n88.56%\n81.54%\n81.99%\n88.25%\ninstructblip-flan-t5-xxl\n91.93%\n84.02%\n84.83%\n88.02%\ninstructblip-vicuna-7b\n92.50%\n83.20%\n81.75%\n87.10%\ninstructblip-vicuna-13b\n88.74%\n80.71%\n78.67%\n76.04%\nBard\n74.02%\n66.28%\n56.33%\n59.42%\nBingChat\n80.49%\n87.20%\n80.68%\n87.20%\nC.3\nQUALITATIVE EXPERIMENTS\nHere, we include a small-scale study we conducted on GPT-4, of the model\u2019s ability to answer\nquestions about stories it generates. Prompts are constructed by the paper authors, and questions\nare constructed by hand to allow probing of specific details of the generated content not specifically\ndepending on the prompt. We specifically focus on simple questions that the model nonetheless gets\nwrong. These examples are in Tables 5, 6, 7.\nD\nHUMAN ANNOTATION ON AMT\nAll human annotations were conducted on the Amazon Mechanical Turk (AMT). Through a paid\nqualification round, we qualify 130 best performing workers that consistently provide conscientious,\nhigh-quality annotations. This project paid the Mturk workers between $15-25 per hour in median\npay depending on the difficulty of the task. We report on the pairwise agreement rate8: the agreement\nlevels range from 90-97% over the datasets.\nHuman Discrimination Evaluation.\nFor the language modality, we obtain human discrimination\nnumbers by prompting the AMT worker with the appropriate context and question for the given task,\nand ask them to choose the correct response among a list of choices. For vision modality, the set\nup is the same with one exception: the workers are asked to choose the best matching image for the\ncaption. Each examples were annotated by 3 workers and majority vote was taken for the reported\nresults. An example of discriminative human evaluation is found in Figure 11.\nGeneration Evaluation.\nFor model generation evaluation in the language modality, the worker\nis given the context, question, and the model generation and is asked to say if the generation is\nan acceptable response. In the vision modality, the same evaluation is conducted by promting the\nworker with a caption (the input prompt) and the generated image and asked if the image matches the\ncaption. Each examples were annotated by 3 workers and majority vote was taken for the reported\nresults. Evaluation of groundtruth also uses this template. Template used is found in Figure 12.\nComparative Evaluation.\nFor language modality only, we conduct the comparative evaluation.\nThe worker is prompted with the appropriate context and question, and given model generation and\n8Our data is skewed to a single label. This inbalance in the label affects kappa, and therefore it is not a\nsuitable measure for agreement for the present work.\n20\nPreprint\nTable 5: Qualitative questioning of GPT-4 about its own generated stories\nPrompt\nWrite a two paragraph story about 3 people building a house\nGPT-4\nOnce upon a time, in the heart of the serene countryside, lived three close friends: Bob, an architect; Allen,\na handyman; and Chris, a carpenter. They were always dreaming about creating something exceptional\nand finally, after years of saving money and sketching designs, they decided to build their dream house.\nBob, with his architectural expertise, sketched a breathtaking design of a two-storied country house with a\nwooden porch. Allen collected all the necessary tools, and Chris with his woodworking skills, pledged to\nmake every single piece of furniture unique.\nWeeks turned into months, and the three friends tirelessly worked day and night to build the house. Bob\nand Chris constructed the wooden framework, walls, and roof; Allen managed the plumbing and electri-\ncal fittings. During their breaks, they would sit together on the raw framework, overlooking the rolling\ngreen hills, discussing the interior decoration and envisaging the fulfilment of their dream. Despite the\nchallenges, disagreements, and exhaustion, they never gave up and encouraged one another to keep going.\nAfter several months of strenuous efforts, the house was finally built to match their grand blueprint. It was\na testament to the trio\u2019s diligent work, creativity, and friendship, and they named it \u2019The Bond House\u2019.\nQuestion\nHow many times did Bob sketch the house?\nGPT-4\nThe story mentions that Bob sketched the house once.\nError\nWe know Bob sketched many designs over the years.\nQuestion\nWas the very first sketch of the house two stories?\nGPT-4\nYes, the very first sketch of the house was two stories.\nError\nWe know Bob sketched designs before the first one described, so we actually don\u2019t know about the very\nfirst sketch.\nQuestion\nWas plumbing added before or after the walls and roof were constructed?\nGPT-4\nThe story suggests that the plumbing was added after the walls and roof were constructed.\nError\nPlumbing is typically added before the walls, as plumbing is inside of the walls.\nQuestion\nWhat did they likely do on the last break before the house was done?\nGPT-4\nThey likely sat together on the raw framework, overlooking the rolling green hills, discussing the interior\ndecoration and envisaging the fulfilment of their dream.\nError\nThe raw framework was unlikely to be exposed by the time they took their last break.\ngroundtruth answer, asked to choose which is the preferred answer. In this setup, they are also asked\nto choose from 3-5 checklist that seeks to ascertain the rationale for the choice (e.g., \u201cThe response\nis shorter and so more to the point\u201d or \u201cThe style is better\u201d). Template used is found in Figure 13.\n21\nPreprint\nHuman Writing.\nFor vision modality only, we conduct a writing task where the worker is\nprompted with a generated image and a question, and asked to briefly answer the question. Each ex-\namples were annotated by 2 workers: 1 to establish groundtruth answers for generated images based\non majority vote (expert adjudication was used in case of disagreement) and 1 to gauge human\nperformance on the task. Template used is found in Figure 14.\nFigure 11: Human discrimination template.\nFigure 12: Model generation evaluation template.\n22\nPreprint\nFigure 13: Comparative evaluation template.\n23\nPreprint\nTable 6: Qualitative questioning of GPT-4 about its own generated stories\nPrompt\nWrite a 5 paragraph story about 3 friends finding treasure\nGPT-4\nOnce upon a time, in the small, peaceful town of Verdi, three friends- Tom, Max, and Sara lived. Tom was\nthe most adventurous and curious in the group. Max, the tallest among them, was the most athletic and\nhelped Tom in their ventures. Sara, the most sensible, constantly discouraged their dangerous adventures\nbut couldn\u2019t resist tagging along. One day, Tom, investigating an old myth about hidden treasure, decided\nto persuade his friends to join him in his hunt.\nThe trio cautiously made their way through the dense woods that bordered the town in the direction of the\nhill as stated in the legend. Thick webs of strong vines and thorny bushes were a challenge, but Max\u2019s\nstrength and Sara\u2019s whittled down hiking stick bore a path for them. As they reached the clearing at the\ntop of the hill, they saw a decrepit stone structure, obscured by overgrown foliage and moss. The friends\ncouldn\u2019t contain their excitement \u2013 it was very likely the location of the treasure according to the myth.\nInside the crumbling stone structure, there were ancient carvings and paintings along the dusty walls.\nAlthough time-worn, they could still make out the drawings. The walls told the story of a mighty king and\nhis beloved, resplendent treasure. Sara, with her keen eyes, discovered a hidden passage in the corner of the\nroom. The friends entered the passage with bated breath, their flashlight rays cutting through the darkness.\nA dusty chest lay quietly in the musty air, the very image of the treasure chest Tom had described from the\nlegend. Anticipation crackled in the air as they all held their breath while Tom painstakingly opened the\nheavy lid.\nThe sight that greeted them was beyond their wildest dreams. The chest was filled with gold, silver, and\nprecious gems of every color. There were ornate arm bands, necklaces, and coins - glimmering in the\ntorchlight, waiting for centuries to be discovered. They looked at each other, their faces mirroring the\nshock and ecstasy of their incredible discovery.\nThe trio didn\u2019t let their triumph consume their logic. They understood such a treasure was part of their\ntown\u2019s history, and stealing it would be morally wrong. Keeping their excitement at bay, they informed\nthe town authorities about their finding. The town celebrated the discovery, but the greatest joy was in the\nhearts of Tom, Max, and Sara. While they may not have gained materialistic wealth, their bond strength-\nened, and they discovered the true treasure of friendship and adventure. This adventure was a tale they\nwould reminisce fondly about, well into their old age. The friends garnered respect and admiration from\nthe townsfolk, but they remained humble, attributing their discovery more to luck than skill. Their friend-\nship had always been cherished, but now it was legendary. The tale of their adventure became a cherished\nlore, passed down generations, speaking of a time long ago, when three friends discovered a hidden treasure\nbut chose their friendship and integrity over greed.\nQuestion\nDid Sara discourage their adventure to find the treasure?\nGPT-4\nYes, Sara initially discouraged their adventure to find the treasure.\nError\nThe story does not say this explicitly.\nQuestion\nWho would a reader expect the trio to inform about the treasure once they found it?\nGPT-4\nA reader would expect the trio to inform the town authorities about the treasure once they found it.\nError\nThis is actually a surprising part of the story. Generally, a reader would expect them to keep the treasure.\nQuestion\nWhat is the most surprising part of the story?\nGPT-4\nThe most surprising part of the story is when the friends, after finding the treasure, decide not to keep it for\nthemselves but instead inform the town authorities about their discovery.\nError\nAlthough this is a correct answer, it contradicts what the model responded to the previous question.\n24\nPreprint\nTable 7: Qualitative questioning of GPT-4 about its own generated stories\nPrompt\nWrite a 2 paragraph story about 3 scientists building a space ship\nGPT-4\nOnce upon a time in the outskirts of London, three brilliant scientists, Dr. Benjamin, Dr. Amelia, and\nDr. Max, dreamt of launching a spaceship to enter the vast, uncharted universe. Their burning ambition\nand unparalleled intellect dumped them in countless sleepless nights, where they poured over blueprints,\nmathematical equations, and stacks of astronomical data. The trio envisioned building a state-of-the-art\nspaceship powered by a revolutionary plasma propulsion technology. Not only would their ship be faster\nthan any mankind had ever built, but it would also have the capability of sustaining life for centuries,\nhinting at the prospect of interstellar travel.\nAfter years of relentless effort and dedication, their dream finally transformed into a magnificent spaceship\nnamed Exodus. It was unlike anything the world had ever seen before. Spanning more than two hun-\ndred feet in length, the ship looked like a cosmic shark decked with arrays of communication antennae\nand numerous high-tech observational instruments. The generous interior boasted a miniature ecosystem,\nadvanced cryogenic facilities, and fully automated systems designed to withstand the extremities of deep\nspace. As the world applauded the triumphant spectacle, the three scientists stood smiling, gazing at their\nincredible creation and dreaming of the extraordinary journeys it would soon embark upon.\nQuestion\nDid the trio pour over blueprints, mathematical equations, and stacks of astronomical data before or after\ndeciding to build a state-of-the-art spaceship?\nGPT-4\nThe trio poured over blueprints, mathematical equations, and stacks of astronomical data before deciding\nto build a state-of-the-art spaceship.\nError\nThe story actually seems to imply that they did this because they wanted to build a state-of-the-art space-\nship, which would imply it happened after.\nQuestion\nWhy did the trio pour over blueprints, mathematical equations, and stacks of astronomical data?\nGPT-4\nThe trio poured over blueprints, mathematical equations, and stacks of astronomical data because they\nwere planning to build a state-of-the-art spaceship for interstellar travel.\nError\nWhile correct, this contradicts the previous answer.\nQuestion\nCan we say anything about how long the three brilliant scientists knew each other before Exodus was built?\nGPT-4\nThe story does not provide information on how long the three brilliant scientists knew each other before\nExodus was built.\nError\nWe can say at least \u201cyears\u201d because the trio were working together on the ship for years.\n25\nPreprint\nFigure 14: Human writing template\n26\n"
  },
  {
    "title": "Text Rendering Strategies for Pixel Language Models",
    "link": "https://arxiv.org/pdf/2311.00522.pdf",
    "upvote": "10",
    "text": "Text Rendering Strategies for Pixel Language Models\nJonas F. Lotz\u2020,\u2021 Elizabeth Salesky\u22c6 Phillip Rust\u2020 Desmond Elliott\u2020\n\u2020Department of Computer Science, University of Copenhagen\n\u2021ROCKWOOL Foundation Research Unit\n\u22c6Johns Hopkins University\njonasf.lotz@di.ku.dk\nAbstract\nPixel-based language models process text ren-\ndered as images, which allows them to han-\ndle any script, making them a promising ap-\nproach to open vocabulary language modelling.\nHowever, recent approaches use text renderers\nthat produce a large set of almost-equivalent\ninput patches, which may prove sub-optimal\nfor downstream tasks, due to redundancy in\nthe input representations. In this paper, we in-\nvestigate four approaches to rendering text in\nthe PIXEL model (Rust et al., 2023), and find\nthat simple character bigram rendering brings\nimproved performance on sentence-level tasks\nwithout compromising performance on token-\nlevel or multilingual tasks. This new rendering\nstrategy also makes it possible to train a more\ncompact model with only 22M parameters that\nperforms on par with the original 86M param-\neter model. Our analyses show that character\nbigram rendering leads to a consistently better\nmodel but with an anisotropic patch embedding\nspace, driven by a patch frequency bias, high-\nlighting the connections between image patch-\nand tokenization-based language models.\n1\nIntroduction\nThere is a growing movement in NLP towards\ntokenization-free methods (Clark et al., 2022; Xue\net al., 2022; Yu et al., 2023) including pixel-based\nrepresentations of text (Salesky et al., 2021, 2023;\nRust et al., 2023; Tschannen et al., 2023). It has\nbeen shown that these tokenization-free methods\ncan readily handle unseen languages and that they\nare more robust to noise attacks than tokenization-\nbased models. In addition, pixel-based approaches\ncan effectively exploit visual similarities between\ncharacters and scripts because they allow for com-\nplete parameter sharing across all inputs, making\nthem a promising direction for multilingual NLP.\nPrevious work on pixel-based models segments\nthe rendered text into either consecutive patches\n(Rust et al., 2023; Tschannen et al., 2023) or with\n(a) Continuous rendering (CONTINUOUS):\n(b) Structured rendering (BIGRAMS):\n(c) Structured rendering (MONO):\n(d) Structured rendering (WORDS):\nFigure 1: Examples of rendering strategies for the sen-\ntence \u201cI must be growing small again.\u201d from Carroll\n(1865). Black patches mark the end of a sequence, fol-\nlowing Rust et al. (2023).\na sliding window (Salesky et al., 2021, 2023) as\nin speech processing. Although the proposed ap-\nproaches have the appealing properties of yielding\ncompact and transferable representations, they also\nresult in a very large input space because there\nis no unique way to represent lexical units. As a\nconsequence, pixel-based models could observe\na new set of image representations with every\nnew sentence, which adds redundancy in the input\nspace and is sub-optimal for developing contextual\nlanguage representations.\nWe refer to these\nunstructured rendering strategies as CONTINUOUS\nand illustrate the point qualitatively in Figure 1\nand Figure 2, and quantitatively in Figure 3. In this\nwork, we ask whether structuring the input, which\nleads to more frequent parameter updates through\nnow-unique word representations, would enable\npixel-based models to develop a deeper understand-\ning of context and semantics. We then propose\nrendering strategies structured around providing\nthe model with a compressed input space.\nWe demonstrate how enforcing a BIGRAMS-\nstructured rendering strategy leads to both a\nmore capable and data-efficient model:\nwhen\nevaluated on semantic sentence-level tasks, we\nfind that a 22M parameters model performs\narXiv:2311.00522v1  [cs.CL]  1 Nov 2023\n(a) Most frequent patches with CONTINUOUS rendering:\n(b) Most frequent patches with BIGRAMS rendering:\nFigure 2: A continuous rendering strategy results in\nmany uniquely-valued image patches for similar inputs,\nwhile structured rendering (here, BIGRAMS) regularises\nand compresses the potential input space.\ncompetitively with the unstructured original at\n86M parameters, and that scaling back up to\n86M parameters narrows the performance gap to\nBERT (Devlin et al., 2019) trained on the same\ndata.\nIn subsequent analyses, we find that the\nadded input structure provokes a clear visual token\nfrequency bias in the learned embedding space.\nWhile also found in BERT, frequency biases have\nbeen shown to degrade the quality of embedding\nspaces when word representations are not only\ndetermined by semantic relations but also by the\nnumber of model updates (Gong et al., 2018;\nGao et al., 2019; Fuster Baggetto and Fresno,\n2022). We show that frequent words have more\ncontext-specific representations than infrequent\nwords, especially in the upper layers.\nFinally,\nwe show that PIXEL models acquire a non-trivial\nsemantic understanding during pretraining, but that\ntheir sentence representations are easily influenced\nby this frequency bias. We release all models1 and\ncode2 for pretraining and finetuning.\n2\nBackground: modelling text as images\nWe build upon the general-purpose language en-\ncoder framework presented in Rust et al. (2023):\nPIXEL is a text autoencoder which builds on the\nMasked Autoencoding Vision Transformer (ViT-\nMAE; He et al., 2021) and is similarly pretrained\nwith a masked reconstruction objective. However,\ninstead of patches from natural images of objects\n(Deng et al., 2009), the patches now contain im-\nages of text. To go from text to images of text,\nPIXEL relies on a rendering library (PangoCairo)3\nto produce a sequence-level image which is sliced\ninto image patches of size 16 \u00d7 16 pixels. The\nsequence-length maximum of 529 patches approxi-\nmately equals the memory requirements of BERT,\n1https://huggingface.co/Team-PIXEL\n2https://github.com/xplip/pixel/tree/\nTextRenderingStrategies\n3https://docs.gtk.org/PangoCairo\n0k\n2k\n4k\n6k\n8k\n10k\nSentences observed\n0k\n50k\n100k\n150k\n200k\n250k\n# unique patches\ncontinuous\nbigrams\nmono\nwords\nFigure 3: Number of unique image patches observed\nas a function of training data sequences. Structured\nrendering results in greater representational efficiency.\nthe closest benchmark for PIXEL. By using the\nGoogle Noto font family which supports the major-\nity of Unicode codepoints,4 the renderer supports\nall languages that can currently be typeset.\nBefore the first layer of the PIXEL model, image\npatches are linearly projected to obtain a sequence\nof patch \u2018embeddings\u2019. During pretraining, 25% of\nembeddings are masked in spans of up to 6 patches\nand only the unmasked patches with a prepended\nCLS embedding are passed through the encoder. Af-\nter replacing the masked embeddings amidst the\nencoder outputs, relying on fixed sinusoidal posi-\ntion embeddings for ordering information, the de-\ncoder predicts the pixel values of solely the masked\npatches. To later finetune the encoder on a classi-\nfication task, the decoder can be replaced with a\ntask-specific head and the masking ratio set to 0%.\n3\nStructured rendering\nPreviously proposed approaches to rendering\ntext as images render full sequences of text and\nsegment into either consecutive patches (Rust\net al., 2023; Tschannen et al., 2023) or with a\nsliding window (Salesky et al., 2021, 2023). These\nCONTINUOUS strategies result in a significant\nnumber of uniquely-valued patches, many of\nwhich may be observed only once during training.\nWe depict this redundancy in Figure 2 and quantify\nit in Figure 3, showing how similar text inputs\nresult in unique visual representations.\nWe compare four rendering strategies: the orig-\ninal unstructured (CONTINUOUS), and three struc-\ntured (WORDS, MONO, BIGRAMS), as depicted\nin Figure 1. To render WORDS we separate seg-\n4https://fonts.google.com/noto\nments with additional whitespace5 such that new\nsegments begin at the beginning of the next im-\nage patch, regulating possible spatial variation. BI-\nGRAMS, rendering two characters per image patch,\nis chosen to be widely applicable, without knowl-\nedge of word or morphemic segmentation (Mielke\net al., 2021; Keren et al., 2022). More specifically\u2014\nconsider the word pairs \u27e8\u201cgrow\u201d, \u201cgrowing\u201d\u27e9 and\n\u27e8\u201cgrowing\u201d, \u201cwalking\u201d\u27e9\u2014the BIGRAMS renderer\nwill produce an overlap of image patches (under-\nlined) for both pairs while the same extent is not\nguaranteed with WORDS-level rendering as it is reg-\nulated by character width. The choice of character\n(n = 2)-grams is motivated by what generally fits\nwithin a 16 \u00d7 16 pixels image patch in the setup\nfrom Rust et al. (2023).\nMONO instead applies\nmonospaced fonts where each character is a fixed\nwidth; depending on font size, this may result in\ncharacter bigram patches without breaks within\ncharacters, but this is not guaranteed. The main\ndifference between BIGRAMS and MONO is that\nMONO simply slides across the sentence, two char-\nacters at the time, yielding two ways to represent a\nword whereas BIGRAMS renders the words and then\npads with whitespace, ensuring unique inputs.6\nAs seen in Figure 3, the structured rendering\nstrategies result in a greatly compressed input\nspace as measured by the number of unique im-\nage patches processed by the model, but Figure 1\nreveals that it comes at the cost of longer sequence\nlengths. While the rendering strategies we propose\nwere not specifically designed for English, they\nmay not equally generalise to other languages or\nscripts. We further discuss the representational effi-\nciencies of these strategies in \u00a7 A.1 and limitations\nto generalisability under Limitations.\n4\nModel scale variants\nRecall from Figure 3 that CONTINUOUS rendering\nproduces a significantly larger set of unique image\npatches compared to other approaches. A conse-\nquence of this is that models must learn to encode\nmany\nalmost-identical\nvisual\nrepresentations,\nwhich may be wasteful, both in terms of parameters\nand training efficiency. Therefore, we hypothesise\nthat PIXEL models that operate over fewer unique\nimage patches can be scaled down without sacrific-\n5We render whitespace at minimum 3 pixels wide, sometimes\nresulting in a blank patch between tokens in structured inputs.\n6As an example, \u201cbe\u201d in Figure 1 is split into 2 image patches\nwith MONO rendering. Depending on the context, it could\nalso be represented in a single image patch.\nModel\nEncL-DecL\nHid\nMLP\nAtt\n|\u03b8|\nBASE\n12-8\n768\n3072\n12\n86M\nSMALL\n12-4\n384\n1536\n6\n22M\nTINY\n12-2\n192\n768\n3\n5.5M\nTable 1: Details of PIXEL model scale variants.\ning performance. While \u201cBase\u201d models and larger\nones are widely used for their strong performance,\nproven scaling laws (Touvron et al., 2021; Zhai\net al., 2021) enable greater experimentation and\nmodel development at smaller scale (Ivgi et al.,\n2022), which is both more environmentally friendly\n(Strubell et al., 2019; Bender et al., 2021; Hersh-\ncovich et al., 2022) and facilitates contributions\nwith limited computational resources.\nWith this in mind, we propose two smaller\narchitectures which we will compare across down-\nstream tasks in \u00a7 5. Our BASE model architecture\nis directly adopted from ViT (Dosovitskiy et al.,\n2021) and PIXEL, and we add two more compact\nSMALL and TINY model variants, as described in\nTable 1. The configurations of the smaller models\nare based on the ViT variants presented in Zhai\net al. (2021). Following the scaling experiments in\nHe et al. (2021), indicating that shallow decoders\nof as small as 2 layers can be sufficient for\nViT-MAEs, we apply a scheme of halving the\nnumber of decoder layers at every scale reduction.\n5\nExperiments\nWe pretrain SMALL models with the proposed ren-\ndering strategies. The models are then evaluated\non dependency parsing (UDP) with data from Uni-\nversal Dependencies v2.10 treebanks (Zeman et al.,\n2022; Nivre et al., 2020) and GLUE (Wang et al.,\n2018), exploring the models\u2019 capabilities at syn-\ntactic processing on the word level and semantic\nprocessing on the sentence level.\n5.1\nPretraining\nWe pretrain all models on the English Wikipedia\nand Bookcorpus (Zhu et al., 2015) data used by\nRust et al. (2023) for direct comparison with PIXEL\nand BERT, which results in \u223c16.8M training ex-\namples. We follow the suggested hyperparameters\nused for PIXEL with the exception of batch size.\nThe smaller architectures of SMALL and TINY al-\nlow for larger batch sizes, which we double from\n256 examples to 512 and 1024, respectively. We\nthen halve the number of pretraining steps accord-\nStructure\nScale\nUDP\nGLUE\nUDP\nGLUE\nTyDiQA-GoldP\nRenderer\nAvg.\nAvg.\nVariant\n|\u03b8|\nAvg.\n\u2206\u00b5\nAvg.\n\u2206\u00b5\nAvg.\n\u2206\u00b5\nCONTINUOUS\n76.2\n71.0\nTINY\n5.5M\n72.0\n\u22120.3\n66.5\n+12.7\n41.6\n+4.9\nBIGRAMS\n76.1\n75.4\nSMALL\n22M\n76.1\n\u22120.1\n75.4\n+4.4\n50.8\n+2.0\nMONO\n75.9\n74.4\nBASE\n86M\n75.5\n\u22120.6\n78.0\n+3.9\n52.8\n+0.5\nWORDS\n76.6\n74.7\nBERT\n110M\n50.5\n\u2014\n80.0\n\u2014\n51.5\n\u2014\nTable 2: Structure (left): averaged results for SMALL-models comparing downstream performance on UDP and\nGLUE following the different rendering strategies. Scale (right): averaged results across model scales using the\nBIGRAMS rendering structure. \u2206\u00b5 is the difference in average performance between BIGRAMS and CONTINUOUS\nrendering for a given model scale. BERT results are marked in grey to visually distinguish from pixel-based models.\ningly from 1M to 500k and 250k in order to train\nfor the same number of epochs as PIXEL (\u223c16\nepochs, but varying slightly due to differing se-\nquence lengths per rendering strategy).\nPretraining BASE takes 8 days on 8 \u00d7 40GB\nNvidia A100 GPUs, while in comparison, pretrain-\ning SMALL takes less than 48 hours on 8 \u00d7 40GB\nNvidia A100 GPUs, and TINY less than 24\nhours. Loss trajectories for the different rendering\nstrategies are in line with their representational\nefficiency (Figure 3), indicating that structured\nrendering may make the masked reconstruction\ntask more data-efficient, achieving a low loss in\nfewer steps (see \u00a7 A.2: Figure 10).\n5.2\nFinetuning\nTo finetune our models for classification tasks we\nreplace the decoder used for pretraining with a\ntask-specific classification head. We do not search\nfor more optimal hyperparameters than those used\nfor PIXEL with the exception of the learning rate;\nwe find that the more compact architectures often\nbenefit from a slightly higher learning rate.7\nWe follow the same protocol during finetuning\nas done for PIXEL: for word-level tasks we obtain\nthe rendered image patch indices for every word\nand as a consequence, the CONTINUOUS strategy\nbecomes identical to the WORDS structure when\nfinetuning on UDP. \u00a7 6.1 further investigates the\nconsequence of a mismatch between how the data\nis structured during pretraining and finetuning.\nWhen finetuning on GLUE the structure follows\nwhat was seen during pretraining for all rendering\nstrategies. Reported performances for BERT and\nPIXEL are taken from Rust et al. (2023).\n7We search the space {1e\u22125, 3e\u22125, 5e\u22125, 7e\u22125, 9e\u22125}\nand report the average over 3 seeds.\n5.3\nRendering strategies\nWe present averaged results comparing the render-\ning strategies in the left part of Table 2. Detailed\nresults for each downstream task are presented in\nTable 4 and Table 5 in the appendix. For UDP\nwe find that the WORDS structure slightly outper-\nforms BIGRAMS and MONO on this word-level task.\nWhen comparing the WORDS and CONTINUOUS\nstrategies we get a first hint as to the importance of\nincluding structure during pretraining as well, keep-\ning in mind that the rendering structure is the same\nfor both strategies when finetuning on UDP. For\nGLUE we see a large increase in performance when\nrendering with any structure and especially BI-\nGRAMS. We attribute the difference in performance\nbetween BIGRAMS and MONO to the unique word\nrepresentations with BIGRAMS, as discussed in \u00a7 3.\nWe find that BIGRAMS is the best performing\nstructure on average, even slightly outperforming\nthe 86M parameters PIXEL (average UDP: 76.1;\naverage GLUE:\n74.1) with only \u00bc its model\nparameters. We provide an investigation into the\nmechanisms that enable this improved performance\non GLUE in \u00a7 6.4. Next we pretrain TINY and\nBASE model variants with BIGRAMS rendering to\nevaluate performance at different model scales.\n5.4\nModel scaling\nThe right part of Table 2 compares the different\nmodel scales all following a BIGRAMS rendering\nstrategy. Detailed results are likewise presented\nin Table 4, Table 5, and Table 6 in the appendix.\nWe find that the TINY configuration performs\ncompetitively on the word-level tasks considering\nits only 5.5M parameters, but has a larger gap\nup to SMALL and BASE on the sentence-level\nGLUE tasks. SMALL proves to be a good trade-off\nbetween scale and performance where it is not\nfar behind BASE on GLUE and even slightly\noutperforms on UDP.8 BASE comes a step closer\nto closing the gap in performance up to BERT on\nGLUE. Comparing to the performance following\na CONTINUOUS rendering strategy, summarised\nas the difference in average performance (\u2206\u00b5), it\nis clear that the more compact the model size, the\ngreater the benefit from structured rendering.\nTo verify that BIGRAMS rendering does not\ndegrade the performance on multilingual sentence-\nlevel tasks across different scripts and morpholo-\ngies, we also include results on TyDiQA-GoldP\n(Clark et al., 2020).9 Again we find that SMALL\nperforms competitively considering its size.\n6\nAblations and supplementary analyses\nIn this section we investigate how BIGRAMS\nrendering changes the model compared to CON-\nTINUOUS. For clarity in what follows, we refer\nto the BASE model with BIGRAMS rendering from\n\u00a7 5.4 as BASE-BIGRAMS and keep referring to the\noriginal model from Rust et al. (2023) as PIXEL.\n6.1\nWhen does rendering structure matter?\nHaving established that a structured rendering strat-\negy leads to improved downstream performance,\nwe further investigate when it is needed: is it suf-\nficient to finetune with structure or does the model\ndevelop strategy-specific features during pretrain-\ning?\nWe analyze this by comparing rendering\nstrategies between pretraining and finetuning.\nThe results in Table 3 for GLUE show that a mis-\nmatch leads to lower downstream performance for\nboth strategies, with BIGRAMS \u2192 CONTINUOUS\nbeing the most harmful, perhaps unsurprisingly.\nThis result does not align with the finding for\nUDP in \u00a7 5.3 where CONTINUOUS overcomes\nthe change to WORDS-structured rendering.\nIt\nmay indicate that the lower-level UDP tasks are\neasier for PIXEL-based models than the high-level\nGLUE tasks (Lauscher et al., 2020). This is in\nline with the relatively good performance for\nTINY-BIGRAMS on UDP.\nTo emphasize the increase in performance\non semantic tasks with BIGRAMS rendering, we\n8We expect that BASE could prevail and would benefit from a\nwider search for optimal hyperparameters during finetuning.\n9With the CONTINUOUS rendering strategy, answer spans are\nextracted such that the answer may include leading or trailing\ncharacters when there is no exact mapping from a word to an\nimage patch index. Therefore, we did not include TyDiQA-\nGoldP in the comparison in \u00a7 5.3. More details can be found\nin Rust et al. (2023). We discuss limitations to answer span\nextraction with BIGRAMS rendering in \u00a7 A.4.\nRENDERER\nGLUE\nPretraining\nFinetuning\nAvg.\nBIGRAMS\nBIGRAMS\n75.4\nCONTINUOUS\nCONTINUOUS\n71.0\nCONTINUOUS\nBIGRAMS\n61.1\nBIGRAMS\nCONTINUOUS\n53.0\nTable 3: Rendering strategy combinations between pre-\ntraining and finetuning with SMALL models. For GLUE,\nmatching pretraining structure is most effective.\ndemonstrate that BASE-BIGRAMS outperforms\nPIXEL by 3.6 points on average on MasakhaNER\n(Adelani et al., 2021), a named entity recognition\nbenchmark for 10 African languages. This further\nillustrates the potential of PIXEL-based models\nfor modelling low-resource languages. Detailed\nresults are presented in Table 7 in the appendix. We\nnext turn our attention to how BIGRAMS rendering\nenables better performance on semantic tasks.\n6.2\nContextual representations\nThe extent to which language models capture se-\nmantic information is partly determined by their\nability to contextualise text (Peters et al., 2018).\nWe therefore analyse how capable BASE-BIGRAMS\nis at producing contextualised word representations.\nWe use the Words in Context dataset (WiC; Pile-\nhvar and Camacho-Collados, 2019) of sentences\nthat contain target words (noun or verb) in either\na similar (True) or different (False) context across\nsentence pairs.10 We compute the mean hidden\nstate output over all tokens associated with the\ntarget word to obtain a representation. We infer\nthat there is contextualisation if the model gener-\nates representations of a target word from different\ncontexts with a low cosine similarity compared to\ntarget words in similar contexts. We report this\nindication of contextuality for each layer of the\nmodel, including the input layer, to better under-\nstand the properties of the different layers. Similar-\nities between randomly chosen words from random\nexamples (Random) are included as a baseline.11\nFigure 4a plots the resulting distributions of\nsimilarities. We see that representations of target\nwords from similar contexts have a higher cosine\nsimilarity than from different contexts, though with\n10Target words are not necessarily identical across sentence\npairs and can vary e.g. in conjugation or number.\n11It is not possible to obtain an exact mapping from words to\nneat image patch indices following the CONTINUOUS ren-\ndering strategy so we do not present this analysis for PIXEL.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nLayer depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine similarity\nSimilar context\nTrue\nFalse\nRandom\n(a) BASE-BIGRAMS\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nLayer depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) BERT\nFigure 4: Distributions of cosine similarities for verbs and nouns from the WiC dataset across model layers 0-12,\nlayer 0 being the input layer. Every example presents a target word in either a similar or different context across\na sentence pair. The representation of the target word is computed as the mean hidden state output over the\ncorresponding tokens. We generally see that BASE-BIGRAMS encodes target words in a similar context as more\nsimilar. The median cosine similarity between random words from random sentences are shown as a baseline.\na considerable overlap, and higher for different con-\ntexts than for random. When comparing to BERT in\nFigure 4b, there is a clear difference in the similar-\nity compared to random words. The difference in\nsimilarity between similar and random words grad-\nually increases throughout the BASE-BIGRAMS\nmodel, until the final layers, whereas the difference\nsteadily decreases throughout the model for BERT.\nGiven the shared image patch embedding layer\nin PIXEL-based models, random words are more\nsimilar to each other at the input layer when\nmodelled as images than entries in a vocabulary.\nTaken together, these plots suggest that a PIXEL-\nbased language model is capable of forming con-\ntextualised word representations and that these are\nmore context-specific in upper layers, though not\nas fine-grained as seen for BERT.\n6.3\nToken frequency and similarity\nThe degree of cosine similarity between random\nwords observed in Figure 4a encourages us to\nassess the isotropic nature of the model (Etha-\nyarajh, 2019; Rajaee and Pilehvar, 2021). The\nhigh cosine similarities suggest that the word\nrepresentations are not evenly distributed with\nrespect to direction in the embedding space, but\ninstead appear to be anisotropic. When learned\nvector representations populate a narrow cone in\nthe embedding space, this geometric alignment\nleads to an overestimation of their similarity (Gao\net al., 2019), which is not an expected property of\nan expressive word embedding space (Arora et al.,\n2016; Mu and Viswanath, 2018).12\nRecent work has shown that Transformer-based\nlanguage models can develop a representation bias\ndriven by token frequency, where low-frequency to-\nkens are clustered together in the embedding space,\nleading to anisotropy in the model (Gao et al.,\n2019; Fuster Baggetto and Fresno, 2022; Jiang\net al., 2022). This bias leads to poor word contex-\ntualisation because the learned vector positions of\nlow frequency words have not moved far from their\nrandom initialisation. Thus, their embeddings are\nnot sufficiently distinct from unrelated words with\nsimilarly low token frequency (Gong et al., 2018;\nCai et al., 2021). Tokens with a higher frequency,\nand thus more parameter updates, can move further\nin the embedding space from their initialisation\nand become more semantically meaningful. Conse-\nquently, we hypothesise that compressing the input\nspace in the form of structured rendering allows the\nmodel to build more contextualised word represen-\ntations through more frequent parameter updates.\nWe investigate this by sampling inputs that were\nseen during pretraining with high and low fre-\nquency. Specifically, we take the 100 most fre-\n12Following Cai et al. (2021) this global estimate of ansiotropy\ndoes not rule out the possibility of distinct and locally\nisotropic clusters in the embedding space. Ding et al. (2022)\nshow that isotropy calibration methods (Gao et al., 2019;\nWang et al., 2020; Li et al., 2020) do not lead to consistent\nimprovements on downstream tasks when models already\nbenefit from local isotropy. We leave this direction for\nPIXEL to future research.\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine similarity\n0\n1\n2\n3\nDensity\nComparing\nHigh, High\nLow, Low\nHigh, Low\n(a) Words in isolation, PIXEL\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine similarity\n0\n2\n4\n6\n(b) Words in isolation, BASE-BIGRAMS\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine similarity\n0\n2\n4\n6\n(c) Words in context, BASE-BIGRAMS\nFigure 5: Distributions of cosine similarities within samples of high-frequency words (High), low-frequency words\n(Low), or between the two samples. Rendering with BIGRAMS structure leads to less directionally aligned vector\nrepresentations of frequent words that have seen more updates during pretraining compared to infrequent words.\nquently occurring words from the Wikipedia cor-\npus that was seen during pretraining and 100 words\nthat occur around 1000 times (rank \u2248 50k).13 We\nfirst render each word from the two frequency sam-\nples in isolation. We then include a comparison to\nwords in context across 100 unique sentences per\nword with BASE-BIGRAMS.14\nWe plot the distributions of cosine similarities\nbetween representations from the last encoder\nlayer, where we expect embeddings from both\nmodels to be contextualised. Comparing the plots\nfrom the two rendering strategies, summarised in\nFigure 5, the effect of pretraining with a smaller\nset of unique tokens becomes clear: for PIXEL\nthe distribution appears as mixtures with a larger\ndistribution mass at higher values of cosine\nsimilarity from comparing high-frequency words\nto other high-frequency (excluding self-similarity\nfor now) than when comparing low-frequency\nto other low-frequency. For BASE-BIGRAMS the\nfrequent words both in isolation and in-context are\nless directionally aligned with each other compared\nto the infrequent, which is in line with the represen-\ntation degeneration problem from Gao et al. (2019)\nand more frequent updates leading to better con-\ntextualisation. Figure 6 visualises the in-context\nrepresentations in 2 dimensions using t-SNE\n(van der Maaten and Hinton, 2008) and provides\nan additional indication of more frequent words\nhaving less locally compact representations.15\nWe expect that in-context representations from\nPIXEL also qualitatively resembles Figure 5a\nbut cannot easily demonstrate this due to the\n13Excluding punctuation and numbers.\n14Recall from \u00a7 6.2 that the CONTINUOUS rendering strategy\nby design makes an exact mapping from words in a sentence\nto neat image patch indices unattainable.\n15Plotting the first 2 singular values from a singular value\ndecomposition gives the same qualitative indications.\naforementioned challenges in aligning patch\nembeddings with CONTINUOUS rendering.\n6.4\nFrequency bias and semantic modelling\nWhile there is less evidence of representation\ndegeneration with CONTINUOUS rendering, it is\nlikely that the poorer performance on GLUE in\n\u00a7 5.4 is caused by PIXEL seeing too many different\npatches too few times. This is a direct consequence\nof the multitude of ways that similar inputs can be\nrendered by the CONTINUOUS approach. However,\nthe drop in performance when mismatching the\nrendering strategies in \u00a7 6.1 for CONTINUOUS \u2192\nBIGRAMS demonstrates that the model has devel-\noped a set of strategy-specific expectations and\nfeatures that are not easily updated. In fact, the new\nrendering strategy for finetuning introduces a set of\npatches that likely never escape the low-frequency\ndomain and therefore remain poorly contextualised.\nSigns of a token frequency bias has also been\nfound in BERT (Fuster Baggetto and Fresno, 2022).\nWe lastly assess the connection between\nvisual token frequency and downstream semantic\nperformance. With BERT, high-frequency words\nhave the most context-specific representations\n(Ethayarajh, 2019), and upper-layer representations\nof low-frequency words are influenced more by\ntheir context than frequent words (Voita et al.,\n2019). Following Ethayarajh (2019), we see that\nthis applies to BASE-BIGRAMS as well (illustrated\nin Figure 7 and discussed in greater detail in\n\u00a7 A.5). We expect that sentences that only vary in\nbeing cased or uncased would result in different\nrepresentations when lowercase appears more\nfrequently (for most words). This demonstrates the\nimpact of observed token frequency on semantic\nmodelling and is in line with observed biases in\nBERT\u2019s embedding space (Jiang et al., 2022).\n200\n0\n200\n200\n0\n200\nHigh freq.\nLow freq.\nFigure 6: t-SNE plot of the output\nembeddings\nof\nhigh-\nand\nlow-\nfrequency words in context from\nBASE-BIGRAMS.\nLow-frequency\nwords cluster tightly in this space.\n0\n5\n10\nLayer depth\n0.0\n0.5\n1.0\nCosine similarity\nSimilarity\nSelf\nHigh freq.\nLow freq.\nIntra-sentence\nHigh freq.\nLow freq.\nFigure 7: Self- and intra-sentence\nsimilarity\nfrom\nBASE-BIGRAMS.\nHigh-frequency words are the most\ncontext-specific;\nlow-frequency\nwords are influenced by their context.\n0\n5\n10\nLayer depth\n0.0\n0.5\n1.0\nSpearman's correlation\nContinuous\nBigrams\nBigrams, uncased\nFigure 8: Evaluation performance\non STS-B. Uncased sentences yield\nbetter performance than the original\nwith BASE-BIGRAMS; the effect is\nless clear for PIXEL (not shown).\nWe rely on the Semantic Textual Similarity\nBenchmark (STS-B; Cer et al., 2017) also found in\nGLUE for this assessment. We measure the cosine\nsimilarity between sentence representations16 and\nplot its correlation with the gold standard similarity\nscores as the measure of performance. Figure 8\nproves that both CONTINUOUS and BIGRAMS ren-\ndering during pretraining lead to non-trivial seman-\ntic modelling capabilties. At peak performance,\naround the middle layers, the increase from sim-\nply ensuring that all words are uncased is roughly\nthe same as the increase from PIXEL to BASE-\nBIGRAMS. This resembles how frequent and in-\nfrequent tokens have unequal influence on their\ncontext in BERT (Voita et al., 2019).\nSeeing that BASE-BIGRAMS exhibits similar\nrepresentational traits to that of BERT, future work\ncould aim for more semantically capable PIXEL-\nbased models by generalising advances found for\ntokenizer-based models (Gao et al., 2021).\n7\nRelated work\nRecent work on pixel-based language modelling\nhas demonstrated how visual language under-\nstanding can be achieved through pixels only (Lee\net al., 2022), observed that the visual similarity of\nlanguages plays an important role in cross-lingual\ntransfer (Rahman et al., 2023), and shown how\nunifying the modalities for text and images allow\na single encoder to perform multimodal tasks\n(Tschannen et al., 2023).\nBy relying on bytes\ndirectly, the unification of modalities can be taken\neven further (Jaegle et al., 2021; Horton et al.,\n2023; Yu et al., 2023). The work most closely\n16Mean hidden state output across all tokens in a sentence,\nexcluding the CLS token and black end-of-sequence token.\nrelated to ours, after Rust et al. (2023), is the work\non machine translation with pixel representations\n(Salesky et al., 2021, 2023). A detailed discussion\nof previous pixel-based approaches can be found in\nRust et al. (2023, \u00a7 5). Where PIXEL laid the foun-\ndation for general-purpose language encoding with\npixel-based representations, this work takes the\nfirst step towards hypothesis-driven improvements\nwithout adding additional data (Yang et al., 2019)\nor scaling up the model (Conneau and Lample,\n2019).\nThough it is possible that competitive\nperformance could be achieved by a model with\nCONTINUOUS rendering by pretraining on more\ndata for more steps (Liu et al., 2019).\nOur addition of BIGRAMS structure resembles\nthe addition of optional but hugely beneficial\n(n = 4)-grams in the character-based CANINE\nmodel (Clark et al., 2022). While character-level\nn-gram models (Wieting et al., 2016; Bojanowski\net al., 2017) have been succeeded by Transformer-\nbased language models, character-level features\nremain valuable as they are less sparse and more\nrobust to misspellings than word n-grams, and\nremain useful for especially morphologically rich\nlanguages (Garrette and Baldridge, 2013; Kulmizev\net al., 2017). Previous work have hypothesised that\ncharacter-level models would be more suitable than\nsubword-based for modelling morphologically-rich\nlanguages (Tsarfaty et al., 2020; Keren et al.,\n2022), but a semantically capable design has\nproven non-obvious (Ma et al., 2020; Keren et al.,\n2022; Nzeyimana and Niyongabo Rubungo, 2022;\nSun et al., 2023).\nWe see potential for future\nwork with pixel-based language models exploring\nappropriate strategies for learning morphological\npatterns (Klein and Tsarfaty, 2020; Seker and\nTsarfaty, 2020; Soulos et al., 2021).\n8\nConclusion\nWe evaluate four text rendering strategies to ad-\ndress the problem of redundancy in the input space\nof PIXEL-based language models. Consequently,\nmore frequent parameter updates lead to better\ncontextualised language representations. We find\nthat rendering two characters per image patch\n(BIGRAMS) is a good trade-off between efficiency\nand generalisability, resulting in substantial im-\nprovements on downstream semantic and sentence-\nlevel tasks; contributing to open-vocabulary NLP\nwith limited computational resources.\nFurther analyses reveal how the added ren-\ndering structure provokes clear representational\nsimilarities to what has been found in BERT.\nWe see potential in future work generalising\nimprovements\nfound\nfor\ntokenization-based\nmasked language models to PIXEL-based masked\nlanguage models. Furthermore, considering that\nthe Vision Transformer has also been applied\nto speech modelling (Huang et al., 2022), and\nthat patch representation has been suggested to\nbe a critical component for the success of ViTs\n(Trockman and Kolter, 2023), we see potential for\nimage patches as the basis for unifying modalities.\nLimitations\nWhile the rendering strategies we propose here are\nwell-suited to English, not all equally generalise to\nother languages or scripts. WORDS rendering relies\non word boundaries which may not be readily avail-\nable or well-defined for many languages which do\nnot mark word or sentence boundaries with whites-\npace such as Thai or polysynthetic languages such\nas Inuktitut. MONO and BIGRAMS are more gen-\neral approaches, but may affect the rendering of\npositional characters such as diacritics or correct\ncontextual forms based on where boundaries are\ncreated. For both approaches, it may be necessary\nto modulate font size across languages to ensure\ncharacter pairs fit into a single patch, especially\nwhen rendering with diacritics. MONO provides\nfurther representational efficiency compared to BI-\nGRAMS by fixing character width, but comes at\nthe cost of more limited language coverage; many\nscripts cannot be made fixed-width and fewer than\n10 have mono fonts available. CONTINUOUS ren-\ndering provides a more general approach which\nmust be balanced with learning efficiency.\nAcknowledgements\nJonas F. Lotz is funded by the ROCKWOOL Foun-\ndation (grant 1242). Elizabeth Salesky is supported\nby the Apple Scholars in AI/ML fellowship. Phillip\nRust is funded by the Novo Nordisk Foundation\n(grant NNF 20SA0066568). This work was sup-\nported by a research grant (VIL53122) from VIL-\nLUM FONDEN.\nReferences\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neu-\nbig, Daniel D\u2019souza, Julia Kreutzer, Constantine Lig-\nnos, Chester Palen-Michel, Happy Buzaaba, Shruti\nRijhwani, Sebastian Ruder, Stephen Mayhew, Is-\nrael Abebe Azime, Shamsuddeen H. Muhammad,\nChris Chinenye Emezue, Joyce Nakatumba-Nabende,\nPerez Ogayo, Aremu Anuoluwapo, Catherine Gitau,\nDerguene Mbaye, Jesujoba Alabi, Seid Muhie Yi-\nmam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani,\nRubungo Andre Niyongabo, Jonathan Mukiibi, Ver-\nrah Otiende, Iroro Orife, Davis David, Samba Ngom,\nTosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi,\nGerald Muriuki, Emmanuel Anebi, Chiamaka Chuk-\nwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel\nOyerinde, Clemencia Siro, Tobius Saul Bateesa,\nTemilola Oloyede, Yvonne Wambui, Victor Akin-\node, Deborah Nabagereka, Maurice Katusiime, Ayo-\ndele Awokoya, Mouhamadane MBOUP, Dibora Ge-\nbreyohannes, Henok Tilaye, Kelechi Nwaike, De-\ngaga Wolde, Abdoulaye Faye, Blessing Sibanda, Ore-\nvaoghene Ahia, Bonaventure F. P. Dossou, Kelechi\nOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Sa-\nlomey Osei. 2021.\nMasakhaNER: Named entity\nrecognition for African languages.\nTransactions\nof the Association for Computational Linguistics,\n9:1116\u20131131.\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2016. A latent variable model\napproach to PMI-based word embeddings. Transac-\ntions of the Association for Computational Linguis-\ntics, 4:385\u2013399.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big?\nIn Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT \u201921, page 610\u2013623, New York, NY,\nUSA. Association for Computing Machinery.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135\u2013146.\nXingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth\nChurch. 2021. Isotropy in the contextual embedding\nspace: Clusters and manifolds. In International Con-\nference on Learning Representations.\nLewis Carroll. 1865. Alice\u2019s Adventures in Wonderland.\nMacmillan.\nDaniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation.\nIn Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancouver,\nCanada. Association for Computational Linguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454\u2013470.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2022.\nCanine: Pre-training an efficient\ntokenization-free encoder for language representa-\ntion. Transactions of the Association for Computa-\ntional Linguistics, 10:73\u201391.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li,\nand Li Fei-Fei. 2009.\nImageNet: A Large-Scale\nHierarchical Image Database. In CVPR09.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYue Ding, Karolis Martinkus, Damian Pascual, Si-\nmon Clematide, and Roger Wattenhofer. 2022. On\nisotropy calibration of transformer models. In Pro-\nceedings of the Third Workshop on Insights from Neg-\native Results in NLP, pages 1\u20139, Dublin, Ireland.\nAssociation for Computational Linguistics.\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk Weissenborn,\nXiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021.\nAn image\nis worth 16x16 words:\nTransformers for image\nrecognition at scale. In International Conference on\nLearning Representations.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55\u201365,\nHong Kong, China. Association for Computational\nLinguistics.\nAlejandro Fuster Baggetto and Victor Fresno. 2022. Is\nanisotropy really the cause of BERT embeddings\nnot being semantic? In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n4271\u20134281, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and\nTieyan Liu. 2019. Representation degeneration prob-\nlem in training natural language generation models.\nIn International Conference on Learning Representa-\ntions.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894\u20136910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nDan Garrette and Jason Baldridge. 2013. Learning a\npart-of-speech tagger from two hours of annotation.\nIn Proceedings of the 2013 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 138\u2013147, Atlanta, Georgia. Association for\nComputational Linguistics.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2018. Frage: Frequency-agnostic\nword representation. arXiv preprint.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-\notr Doll\u2019ar, and Ross B. Girshick. 2021. Masked\nautoencoders are scalable vision learners.\n2022\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 15979\u201315988.\nDaniel Hershcovich, Nicolas Webersinke, Mathias\nKraus, Julia Bingler, and Markus Leippold. 2022.\nTowards climate awareness in NLP research. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2480\u2013\n2494, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nMaxwell Horton, Sachin Mehta, Ali Farhadi, and Mo-\nhammad Rastegari. 2023. Bytes are all you need:\nTransformers operating directly on file bytes. arXiv\npreprint.\nPo-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski,\nMichael Auli, Wojciech Galuba, Florian Metze, and\nChristoph Feichtenhofer. 2022.\nMasked autoen-\ncoders that listen. In NeurIPS.\nMaor Ivgi, Yair Carmon, and Jonathan Berant. 2022.\nScaling laws under the microscope: Predicting trans-\nformer performance from small scale experiments.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 7354\u20137371, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste\nAlayrac, Carl Doersch, Catalin Ionescu, David Ding,\nSkanda Koppula, Andrew Brock, Evan Shelhamer,\nOlivier J. H\u2019enaff, Matthew M. Botvinick, Andrew\nZisserman, Oriol Vinyals, and Jo\u00e3o Carreira. 2021.\nPerceiver io: A general architecture for structured\ninputs & outputs. arXiv preprint.\nTing Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang,\nDeqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen\nHuang, Denvy Deng, and Qi Zhang. 2022. Prompt-\nBERT: Improving BERT sentence embeddings with\nprompts. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 8826\u20138837, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nOmri Keren, Tal Avinari, Reut Tsarfaty, and Omer\nLevy. 2022. Breaking character: Are subwords good\nenough for mrls after all? arXiv preprint.\nStav Klein and Reut Tsarfaty. 2020. Getting the ##life\nout of living: How adequate are word-pieces for mod-\nelling complex morphology? In Proceedings of the\n17th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 204\u2013209, Online. Association for Computa-\ntional Linguistics.\nArtur Kulmizev, Bo Blankers, Johannes Bjerva, Malvina\nNissim, Gertjan van Noord, Barbara Plank, and Mar-\ntijn Wieling. 2017. The power of character n-grams\nin native language identification. In Proceedings of\nthe 12th Workshop on Innovative Use of NLP for\nBuilding Educational Applications, pages 382\u2013389,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli\u00b4c, and\nGoran Glava\u0161. 2020. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual Transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483\u20134499, On-\nline. Association for Computational Linguistics.\nKenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu,\nFangyu Liu, Julian Martin Eisenschlos, Urvashi\nKhandelwal, Peter Shaw, Ming-Wei Chang, and\nKristina Toutanova. 2022. Pix2struct: Screenshot\nparsing as pretraining for visual language understand-\ning. arXiv preprint.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119\u20139130, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint.\nWentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin\nWang, and Guoping Hu. 2020. CharBERT: Character-\naware pre-trained language model. In Proceedings\nof the 28th International Conference on Computa-\ntional Linguistics, pages 39\u201350, Barcelona, Spain\n(Online). International Committee on Computational\nLinguistics.\nSabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky,\nColin Raffel, Manan Dey, Matthias Gall\u00e9, Arun Raja,\nChenglei Si, Wilson Y. Lee, Beno\u00eet Sagot, and Sam-\nson Tan. 2021. Between words and characters: A\nbrief history of open-vocabulary modeling and tok-\nenization in nlp. arXiv preprint.\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-top:\nSimple and effective postprocessing for word repre-\nsentations. In International Conference on Learning\nRepresentations.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji\u02c7c, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 4034\u20134043, Marseille,\nFrance. European Language Resources Association.\nAntoine Nzeyimana and Andre Niyongabo Rubungo.\n2022.\nKinyaBERT: a morphology-aware Kin-\nyarwanda language model. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5347\u20135363, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227\u20132237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. WiC: the word-in-context dataset for evalu-\nating context-sensitive meaning representations. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1267\u20131273,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nMd Mushfiqur Rahman, Fardin Ahsan Sakib, Fahim\nFaisal, and Antonios Anastasopoulos. 2023. To token\nor not to token: A comparative study of text repre-\nsentations for cross-lingual transfer. arXiv preprint.\nSara Rajaee and Mohammad Taher Pilehvar. 2021. A\ncluster-based approach for improving isotropy in con-\ntextual embedding space. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 575\u2013584, Online. Association\nfor Computational Linguistics.\nPhillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Eliza-\nbeth Salesky, Miryam de Lhoneux, and Desmond El-\nliott. 2023. Language modelling with pixels. ICLR.\nElizabeth Salesky, David Etter, and Matt Post. 2021.\nRobust open-vocabulary translation from visual text\nrepresentations. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 7235\u20137252, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nElizabeth Salesky, Neha Verma, Philipp Koehn, and\nMatt Post. 2023. Multilingual pixel representations\nfor translation and effective cross-lingual transfer.\narXiv preprint.\nAmit Seker and Reut Tsarfaty. 2020. A pointer net-\nwork architecture for joint morphological segmen-\ntation and tagging. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4368\u20134378, Online. Association for Computational\nLinguistics.\nPaul Soulos, Sudha Rao, Caitlin Smith, Eric Rosen,\nAsli Celikyilmaz, R. Thomas McCoy, Yichen Jiang,\nColeman Haley, Roland Fernandez, Hamid Palangi,\nJianfeng Gao, and Paul Smolensky. 2021. Structural\nbiases for improving transformers on translation into\nmorphologically rich languages. In Proceedings of\nthe 4th Workshop on Technologies for MT of Low\nResource Languages (LoResMT2021), pages 52\u201367,\nVirtual. Association for Machine Translation in the\nAmericas.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645\u20133650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nJimin Sun, Patrick Fernandes, Xinyi Wang, and Gra-\nham Neubig. 2023. A multi-dimensional evaluation\nof tokenizer-free multilingual pretrained models. In\nFindings of the Association for Computational Lin-\nguistics: EACL 2023, pages 1725\u20131735, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9-\ngou. 2021. Training data-efficient image transform-\ners & distillation through attention. In ICML, pages\n10347\u201310357.\nAsher Trockman and J Zico Kolter. 2023. Patches are\nall you need? Transactions on Machine Learning\nResearch. Featured Certification.\nReut Tsarfaty, Dan Bareket, Stav Klein, and Amit Seker.\n2020. From SPMRL to NMRL: What did we learn\n(and unlearn) in a decade of parsing morphologically-\nrich languages (MRLs)? In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7396\u20137408, Online. Association\nfor Computational Linguistics.\nMichael Tschannen, Basil Mustafa, and Neil Houlsby.\n2023. Image-and-language understanding from pix-\nels only. 2023 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR).\nLaurens van der Maaten and Geoffrey E. Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research, 9:2579\u20132605.\nElena Voita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396\u20134406, Hong Kong,\nChina. Association for Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353\u2013355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nLingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu,\nGuangtao Wang, and Quanquan Gu. 2020. Improv-\ning neural language generation with spectrum control.\nIn International Conference on Learning Representa-\ntions.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2016. Charagram: Embedding words and\nsentences via character n-grams. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1504\u20131515, Austin,\nTexas. Association for Computational Linguistics.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. ByT5: Towards a token-free\nfuture with pre-trained byte-to-byte models. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:291\u2013306.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran Asso-\nciates, Inc.\nLili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Agha-\njanyan, Luke Zettlemoyer, and Mike Lewis. 2023.\nMegabyte: Predicting million-byte sequences with\nmultiscale transformers. arXiv preprint.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, Elia\nAckermann, No\u00ebmi Aepli, Hamid Aghaei, \u017deljko\nAgi\u00b4c, Amir Ahmadi, Lars Ahrenberg, Chika Kennedy\nAjede, Gabriel\u02d9e Aleksandravi\u02c7ci\u00afut\u02d9e, Ika Alfina, Avner\nAlgom, Erik Andersen, Lene Antonsen, Katya\nAplonova, Angelina Aquino, Carolina Aragon,\nGlyd Aranes, Maria Jesus Aranzabe, Bilge Nas\nAr\u0131can, \u007fH\u00f3runn Arnard\u00f3ttir, Gashaw Arutie, Jes-\nsica Naraiswari Arwidarasti, Masayuki Asahara,\nDeniz Baran Aslan, Cengiz Asmazo\u02d8glu, Luma\nAteyah, Furkan Atmaca, Mohammed Attia, Aitz-\niber Atutxa, Liesbeth Augustinus, Elena Badmaeva,\nKeerthana Balasubramani, Miguel Ballesteros, Esha\nBanerjee, Sebastian Bank, Verginica Barbu Mititelu,\nStarka\u00f0ur Barkarson, Rodolfo Basile, Victoria Bas-\nmov, Colin Batchelor, John Bauer, Seyyit Talha Bedir,\nKepa Bengoetxea, Yifat Ben Moshe, G\u00f6zde Berk,\nYevgeni Berzak, Irshad Ahmad Bhat, Riyaz Ah-\nmad Bhat, Erica Biagetti, Eckhard Bick, Agn\u02d9e\nBielinskien\u02d9e, Krist\u00edn Bjarnad\u00f3ttir, Rogier Blok-\nland, Victoria Bobicev, Lo\u00efc Boizou, Emanuel\nBorges V\u00f6lker, Carl B\u00f6rstell, Cristina Bosco, Gosse\nBouma, Sam Bowman, Adriane Boyd, Anouck Brag-\ngaar, Kristina Brokait\u02d9e, Aljoscha Burchardt, Marie\nCandito, Bernard Caron, Gauthier Caron, Lauren\nCassidy, Tatiana Cavalcanti, G\u00fcl\u00b8sen Cebiro\u02d8glu Ery-\ni\u02d8git, Flavio Massimiliano Cecchini, Giuseppe G. A.\nCelano, Slavom\u00edr \u02c7C\u00e9pl\u00f6, Neslihan Cesur, Savas\nCetin, \u00d6zlem \u00c7etino\u02d8glu, Fabricio Chalub, Shweta\nChauhan, Ethan Chi, Taishi Chika, Yongseok Cho,\nJinho Choi, Jayeol Chun, Juyeon Chung, Alessan-\ndra T. Cignarella, Silvie Cinkov\u00e1, Aur\u00e9lie Collomb,\n\u00c7a\u02d8gr\u0131 \u00c7\u00f6ltekin, Miriam Connor, Daniela Corbetta,\nMarine Courtin, Mihaela Cristescu, Philemon Daniel,\nElizabeth Davidson, Mathieu Dehouck, Martina\nde Laurentiis, Marie-Catherine de Marneffe, Vale-\nria de Paiva, Mehmet Oguz Derin, Elvis de Souza,\nArantza Diaz de Ilarraza, Carly Dickerson, Arawinda\nDinakaramani, Elisa Di Nuovo, Bamba Dione, Pe-\nter Dirix, Kaja Dobrovoljc, Timothy Dozat, Kira\nDroganova, Puneet Dwivedi, Hanne Eckhoff, Sandra\nEiche, Marhaba Eli, Ali Elkahky, Binyam Ephrem,\nOlga Erina, Toma\u017e Erjavec, Aline Etienne, Wograine\nEvelyn, Sidney Facundes, Rich\u00e1rd Farkas, Feder-\nica Favero, Jannatul Ferdaousi, Mar\u00edlia Fernanda,\nHector Fernandez Alcalde, Jennifer Foster, Cl\u00e1u-\ndia Freitas, Kazunori Fujita, Katar\u00edna Gajdo\u0161ov\u00e1,\nDaniel Galbraith, Federica Gamba, Marcos Gar-\ncia, Moa G\u00e4rdenfors, Sebastian Garza, Fabr\u00edcio Fer-\nraz Gerardi, Kim Gerdes, Filip Ginter, Gustavo\nGodoy, Iakes Goenaga, Koldo Gojenola, Memduh\nG\u00f6k\u0131rmak, Yoav Goldberg, Xavier G\u00f3mez Guino-\nvart, Berta Gonz\u00e1lez Saavedra, Bernadeta Grici\u00afut\u02d9e,\nMatias Grioni, Lo\u00efc Grobol, Normunds Gr\u00afuz\u00af\u0131tis,\nBruno Guillaume, C\u00e9line Guillot-Barbance, Tunga\nG\u00fcng\u00f6r, Nizar Habash, Hinrik Hafsteinsson, Jan Ha-\nji\u02c7c, Jan Haji\u02c7c jr., Mika H\u00e4m\u00e4l\u00e4inen, Linh H\u00e0 M\u02dcy, Na-\nRae Han, Muhammad Yudistira Hanifmuti, Takahiro\nHarada, Sam Hardwick, Kim Harris, Dag Haug,\nJohannes Heinecke, Oliver Hellwig, Felix Hennig,\nBarbora Hladk\u00e1, Jaroslava Hlav\u00e1\u02c7cov\u00e1, Florinel Hoci-\nung, Petter Hohle, Jena Hwang, Takumi Ikeda, An-\nton Karl Ingason, Radu Ion, Elena Irimia, O. l\u00e1j\u00edd\u00e9\nIshola, Kaoru Ito, Siratun Jannat, Tom\u00e1\u0161 Jel\u00ednek,\nApoorva Jha, Anders Johannsen, Hildur J\u00f3nsd\u00f3ttir,\nFredrik J\u00f8rgensen, Markus Juutinen, Sarveswaran\nK, H\u00fcner Ka\u00b8s\u0131kara, Andre Kaasen, Nadezhda\nKabaeva, Sylvain Kahane, Hiroshi Kanayama, Jenna\nKanerva, Neslihan Kara, Ritv\u00e1n Karah\u00f3\u02c7ga, Boris\nKatz, Tolga Kayadelen, Jessica Kenney, V\u00e1clava\nKettnerov\u00e1, Jesse Kirchner, Elena Klementieva,\nElena Klyachko, Arne K\u00f6hn, Abdullatif K\u00f6ksal,\nKamil Kopacewicz, Timo Korkiakangas, Mehmet\nK\u00f6se, Natalia Kotsyba, Jolanta Kovalevskait\u02d9e, Si-\nmon Krek, Parameswari Krishnamurthy, Sandra\nK\u00fcbler, O\u02d8guzhan Kuyruk\u00e7u, Asl\u0131 Kuzgun, Sooky-\noung Kwak, Veronika Laippala, Lucia Lam, Lorenzo\nLambertino, Tatiana Lando, Septina Dian Larasati,\nAlexei Lavrentiev, John Lee, Phuong L\u00ea H`\u00f4ng,\nAlessandro Lenci, Saran Lertpradit, Herman Leung,\nMaria Levina, Cheuk Ying Li, Josie Li, Keying Li,\nYuan Li, KyungTae Lim, Bruna Lima Padovani, Kris-\nter Lind\u00e9n, Nikola Ljube\u02c7si\u00b4c, Olga Loginova, Ste-\nfano Lusito, Andry Luthfi, Mikko Luukko, Olga\nLyashevskaya, Teresa Lynn, Vivien Macketanz,\nMenel Mahamdi, Jean Maillard, Aibek Makazhanov,\nMichael Mandl, Christopher Manning, Ruli Ma-\nnurung, B\u00a8us\u00b8ra Mars\u00b8an, C\u0103t\u0103lina M\u0103r\u0103nduc, David\nMare\u02c7cek, Katrin Marheinecke, Stella Markantonatou,\nH\u00e9ctor Mart\u00ednez Alonso, Lorena Mart\u00edn Rodr\u00edguez,\nAndr\u00e9 Martins, Jan Ma\u02c7sek, Hiroshi Matsuda, Yuji\nMatsumoto, Alessandro Mazzei, Ryan McDonald,\nSarah McGuinness, Gustavo Mendonc\u00b8a, Tatiana\nMerzhevich, Niko Miekka, Karina Mischenkova,\nMargarita Misirpashayeva, Anna Missil\u00a8a, C\u0103t\u0103lin\nMititelu, Maria Mitrofan, Yusuke Miyao, AmirHos-\nsein Mojiri Foroushani, Judit Moln\u00e1r, Amirsaeid\nMoloodi, Simonetta Montemagni, Amir More, Laura\nMoreno Romero, Giovanni Moretti, Keiko Sophie\nMori, Shinsuke Mori, Tomohiko Morioka, Shigeki\nMoro, Bjartur Mortensen, Bohdan Moskalevskyi,\nKadri Muischnek, Robert Munro, Yugo Murawaki,\nKaili M\u00a8u\u00a8urisep, Pinkey Nainwani, Mariam Nakhl\u00e9,\nJuan Ignacio Navarro Hor\u02dcniacek, Anna Nedoluzhko,\nGunta Ne\u02c7spore-B\u00aferzkalne, Manuela Nevaci, Luong\nNguy\u02dc\u00ean Th\u1ecb, Huy`\u00ean Nguy\u02dc\u00ean Th\u1ecb Minh, Yoshihiro\nNikaido, Vitaly Nikolaev, Rattima Nitisaroj, Alireza\nNourian, Hanna Nurmi, Stina Ojala, Atul Kr. Ojha,\nAd\u00e9day\u00f2 Ol\u00fa\u00f2kun, Mai Omura, Emeka Onwueg-\nbuzia, Noam Ordan, Petya Osenova, Robert \u00a8Ostling,\nLilja \u00d8vrelid, S\u00b8aziye Bet\u00a8ul \u00a8Ozates\u00b8, Merve \u00a8Ozc\u00b8elik,\nArzucan \u00a8Ozg\u00a8ur, Balk\u0131z \u00a8Ozt\u00a8urk Bas\u00b8aran, Teresa Pac-\ncosi, Alessio Palmero Aprosio, Hyunji Hayley Park,\nNiko Partanen, Elena Pascual, Marco Passarotti, Ag-\nnieszka Patejuk, Guilherme Paulino-Passos, Giulia\nPedonese, Angelika Peljak-\u0141api\u00b4nska, Siyao Peng,\nCenel-Augusto Perez, Natalia Perkova, Guy Perrier,\nSlav Petrov, Daria Petrova, Andrea Peverelli, Jason\nPhelan, Jussi Piitulainen, Tommi A Pirinen, Emily\nPitler, Barbara Plank, Thierry Poibeau, Larisa Pono-\nmareva, Martin Popel, Lauma Pretkalnin\u00b8a, Sophie\nPr\u00e9vost, Prokopis Prokopidis, Adam Przepi\u00f3rkowski,\nTiina Puolakainen, Sampo Pyysalo, Peng Qi, An-\ndriela R\u00a8a\u00a8abis, Alexandre Rademaker, Mizanur Ra-\nhoman, Taraka Rama, Loganathan Ramasamy, Carlos\nRamisch, Fam Rashel, Mohammad Sadegh Rasooli,\nVinit Ravishankar, Livy Real, Petru Rebeja, Siva\nReddy, Mathilde Regnault, Georg Rehm, Ivan Ri-\nabov, Michael Rie\u00dfler, Erika Rimkut\u02d9e, Larissa Ri-\nnaldi, Laura Rituma, Putri Rizqiyah, Luisa Rocha,\nEir\u00edkur R\u00a8ognvaldsson, Mykhailo Romanenko, Rudolf\nRosa, Valentin Ros,ca, Davide Rovati, Ben Ro-\nzonoyer, Olga Rudina, Jack Rueter, Kristj\u00e1n R\u00fa-\nnarsson, Shoval Sadde, Pegah Safari, Beno\u02c6\u0131t Sagot,\nAleksi Sahala, Shadi Saleh, Alessio Salomoni,\nTanja Samard\u02c7zi\u00b4c, Stephanie Samson, Manuela San-\nguinetti, Ezgi San\u0131yar, Dage S\u00a8arg, Baiba Saul\u00af\u0131te,\nYanin Sawanakunanon, Shefali Saxena, Kevin Scan-\nnell, Salvatore Scarlata, Nathan Schneider, Sebastian\nSchuster, Lane Schwartz, Djam\u00e9 Seddah, Wolfgang\nSeeker, Mojgan Seraji, Syeda Shahzadi, Mo Shen,\nAtsuko Shimada, Hiroyuki Shirasu, Yana Shishk-\nina, Muh Shohibussirri, Dmitry Sichinava, Janine\nSiewert, Einar Freyr Sigur\u00f0sson, Aline Silveira,\nNatalia Silveira, Maria Simi, Radu Simionescu,\nKatalin Simk\u00f3, M\u00e1ria \u0160imkov\u00e1, Kiril Simov, Maria\nSkachedubova, Aaron Smith, Isabela Soares-Bastos,\nShafi Sourov, Carolyn Spadine, Rachele Sprugnoli,\nVivian Stamou, Stein\u007fh\u00f3r Steingr\u00edmsson, Antonio\nStella, Milan Straka, Emmett Strickland, Jana Str-\nnadov\u00e1, Alane Suhr, Yogi Lesmana Sulestio, Umut\nSulubacak, Shingo Suzuki, Daniel Swanson, Zsolt\nSz\u00e1nt\u00f3, Chihiro Taguchi, Dima Taji, Yuta Takahashi,\nFabio Tamburini, Mary Ann C. Tan, Takaaki Tanaka,\nDipta Tanaya, Mirko Tavoni, Samson Tella, Is-\nabelle Tellier, Marinella Testori, Guillaume Thomas,\nSara Tonelli, Liisi Torga, Marsida Toska, Trond\nTrosterud, Anna Trukhina, Reut Tsarfaty, Utku T\u00fcrk,\nFrancis Tyers, Sumire Uematsu, Roman Untilov,\nZde\u02c7nka Ure\u0161ov\u00e1, Larraitz Uria, Hans Uszkoreit, An-\ndrius Utka, Elena Vagnoni, Sowmya Vajjala, Rob\nvan der Goot, Martine Vanhove, Daniel van Niek-\nerk, Gertjan van Noord, Viktor Varga, Uliana Ve-\ndenina, Eric Villemonte de la Clergerie, Veronika\nVincze, Natalia Vlasova, Aya Wakasa, Joel C. Wal-\nlenberg, Lars Wallin, Abigail Walsh, Jing Xian Wang,\nJonathan North Washington, Maximilan Wendt,\nPaul Widmer, Shira Wigderson, Sri Hartati Wi-\njono, Seyi Williams, Mats Wir\u00e9n, Christian Wit-\ntern, Tsegay Woldemariam, Tak-sum Wong, Alina\nWr\u00f3blewska, Mary Yako, Kayo Yamashita, Naoki\nYamazaki, Chunxiao Yan, Koichi Yasuoka, Marat M.\nYavrumyan, Arife Bet\u00fcl Yenice, Olcay Taner Y\u0131ld\u0131z,\nZhuoran Yu, Arlisa Yuliawati, Zden\u02c7ek \u017dabokrtsk\u00fd,\nShorouq Zahra, Amir Zeldes, He Zhou, Hanzhi Zhu,\nAnna Zhuravleva, and Rayan Ziane. 2022. Universal\ndependencies 2.10. LINDAT/CLARIAH-CZ digital\nlibrary at the Institute of Formal and Applied Linguis-\ntics (\u00daFAL), Faculty of Mathematics and Physics,\nCharles University.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby,\nand Lucas Beyer. 2021. Scaling vision transformers.\n2022 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 1204\u20131213.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\nA\nAppendix\nA.1\nRepresentational efficiency\n0\n500\n1000\n1500\nLength [Patches]\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nProportion\nRenderer\ncontinuous\nbigrams\nmono\nwords\nFigure 9: Distributions of sequence lengths (in patches) resulting from different rendering strategies.\nAs seen in Figure 1, structured rendering compresses the input space by reducing the positions characters\nmay be observed in. This dramatically affects the number of unique inputs observed in a fixed number of\nsequences, as quantified in Figure 3. Concretely, the 10 most frequently observed image patches after\nprocessing 100,000 sequences from English Wikipedia are shown in Figure 2; with continuous rendering\nall are positional variants of the same subword, while with structured rendering each represents different\nwords or morphemes. However, instituting word- or subword-level structure with whitespace padding\nincreases sequence lengths compared to unstructured rendering as quantified in Figure 9.\nA.2\nPretraining loss curves\n0k\n100k\n200k\n300k\n400k\n500k\nTraining steps\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nLoss\nRenderer\ncontinuous\nbigrams\nmono\nwords\nFigure 10: Pretraining loss for SMALL models with different rendering strategies, indicating that structured rendering\nmay make the masked reconstruction task more data efficient, reaching a low loss in fewer steps.\nA.3\nDetailed experimental results\nENG\nARA\nCOP\nHIN\nJPN\nKOR\nTAM\nVIE\nZHO\nAVG\nBERT\n90.6\n77.7\n13.0\n75.9\n73.8\n30.2\n15.2\n49.4\n28.8\n50.5\nPIXEL\n88.7\n77.3\n83.5\n89.2\n90.7\n78.5\n52.6\n50.5\n73.7\n76.1\nTINY-CONTINUOUS\n78.9\n74.6\n80.0\n87.9\n89.9\n75.1\n48.3\n46.2\n69.5\n72.3\nStructure\nSMALL-CONTINUOUS\n87.2\n77.2\n83.4\n88.9\n91.0\n78.8\n53.8\n51.9\n73.5\n76.2\nSMALL-BIGRAMS\n87.9\n75.4\n84.1\n88.9\n90.8\n79.4\n53.9\n50.9\n73.9\n76.1\nSMALL-MONO\n88.3\n76.8\n83.4\n88.9\n91.0\n79.0\n50.5\n51.3\n73.8\n75.9\nSMALL-WORDS\n88.0\n77.2\n83.9\n89.3\n91.2\n78.7\n53.7\n53.3\n74.2\n76.6\nScale\nTINY-BIGRAMS\n82.9\n70.6\n79.1\n86.2\n90.0\n76.2\n44.9\n47.6\n69.8\n72.0\nSMALL-BIGRAMS\n87.9\n75.4\n84.1\n88.9\n90.8\n79.4\n53.9\n50.9\n73.9\n76.1\nBASE-BIGRAMS\n89.6\n77.7\n81.4\n88.6\n90.8\n78.1\n49.8\n49.4\n73.9\n75.5\nTable 4: Test set LAS results for dependency parsing on a selection of Universal Dependencies treebanks (UDP).\nMNLI-M/MM\nQQP\nQNLI\nSST-2\nCOLA\nSTS-B\nMRPC\nRTE\nWNLI\nAVG\nBERT\n84.0 / 84.2\n87.6\n91.0\n92.6\n60.3\n88.8\n90.2\n69.5\n51.8\n80.0\nPIXEL\n78.1 / 78.9\n84.5\n87.8\n89.6\n38.4\n81.1\n88.2\n60.5\n53.8\n74.1\nTINY-CONTINUOUS\n36.7 / 37.0\n76.6\n72.9\n87.2\n2.1\n25.1\n82.4\n58.5\n59.2\n53.8\nStructure\nSMALL-CONTINUOUS\n72.2 / 73.6\n84.8\n86.2\n88.3\n19.1\n81.7\n84.6\n61.4\n57.7\n71.0\nSMALL-BIGRAMS\n77.3 / 78.1\n85.7\n87.8\n90.4\n42.3\n84.3\n87.8\n63.5\n56.3\n75.4\nSMALL-MONO\n77.4 / 77.6\n84.7\n86.8\n89.4\n42.3\n82.4\n86.9\n57.5\n58.9\n74.4\nSMALL-WORDS\n76.7 / 77.3\n84.5\n86.6\n89.9\n44.6\n80.5\n87.4\n62.8\n56.3\n74.7\nScale\nTINY-BIGRAMS\n60.8 / 61.9\n79.6\n81.7\n87.2\n15.6\n77.9\n83.0\n59.4\n57.7\n66.5\nSMALL-BIGRAMS\n77.3 / 78.1\n85.7\n87.8\n90.4\n42.3\n84.3\n87.8\n63.5\n56.3\n75.4\nBASE-BIGRAMS\n81.1 / 81.4\n87.6\n89.7\n90.4\n53.3\n86.6\n90.2\n63.5\n56.3\n78.0\nTable 5: Validation set performance on GLUE. The reported metrics are F1 score for QQP and MRPC, Matthew\u2019s\ncorrelation for COLA, Spearman\u2019s \u03c1 for STS-B, and accuracy for the rest.\nENG\nARA\nBEN\nFIN\nIND\nKOR\nRUS\nSWA\nTEL\nAVG\nBERT\n68.5\n58.0\n43.2\n58.3\n67.1\n12.4\n53.2\n71.3\n48.2\n51.5\nPIXEL\n59.6\n57.3\n36.3\n57.1\n63.6\n26.1\n50.5\n65.9\n61.7\n52.3\nTINY-CONTINUOUS\n42.6\n45.0\n12.4\n45.3\n48.1\n13.2\n36.7\n46.8\n45.7\n36.6\nSMALL-CONTINUOUS\n57.1\n53.3\n20.3\n57.5\n62.9\n22.3\n51.1\n65.3\n58.1\n48.8\nScale\nTINY-BIGRAMS\n43.3\n45.5\n19.0\n50.3\n48.2\n14.9\n45.4\n52.7\n56.4\n41.6\nSMALL-BIGRAMS\n50.8\n53.2\n37.1\n59.1\n57.5\n20.1\n52.8\n62.4\n64.2\n50.8\nBASE-BIGRAMS\n53.8\n53.1\n46.5\n59.6\n60.3\n18.8\n54.1\n64.1\n65.7\n52.8\nTable 6: Validation set F1 scores for TyDiQA-GoldP. Average (AVG) scores exclude ENG (Clark et al., 2020). With\nsome rendering structures, answer span extraction adversely affects results (see discussion at \u00a7 A.4).\nAMH\nHAU\nIBO\nKIN\nLUG\nLUO\nPCM\nSWA\nWOL\nYOR\nAVG\nBERT\n0\n86.6\n83.5\n72.0\n78.4\n73.2\n87.0\n83.3\n62.2\n73.8\n62.7\nPIXEL\n47.7\n82.4\n79.9\n64.2\n76.5\n66.6\n78.7\n79.8\n59.7\n70.7\n70.6\nBASE-BIGRAMS\n50.1\n85.6\n82.2\n68.4\n78.4\n72.5\n82.8\n82.4\n64.4\n74.8\n74.2\nTable 7: Test set F1 scores on MasakhaNER (Adelani et al., 2021). We follow the implementation of Rust et al.\n(2023) and render each word at the start of a new image patch.\nA.4\nTyDiQa-GoldP\nThe CONTINUOUS rendering strategy used for PIXEL, in which words often overlap in an image patch,\nleads to extracted answer spans that potentially include leading or trailing characters that should not be\npart of the answer. BIGRAMS rendering adressess this issue by yielding clear word boundaries in the\ninput representations.\nHowever, the BIGRAMS rendering strategy poses new challenges to extracting answer spans for TyDiQA-\nGoldP. While the task is simplified compared to the primary task by removing language tracks that lack\nwhitespace,17 we find that a surprisingly high number of \u201cwords\u201d are a string of comma-separated words\nor concatenations of characters and letters that should be delimited by whitespace. By design we consider\nand render these as one unit when we only split by whitespace. An example of a single \u201cunit\u201d from the\ntraining split highlights this issue more clearly: \u201coikeudet[1]L\u00e4\u00e4ni[1]1Vilna523,0501387Vilnan\u201d18 where\nthe expected answer is \u201cVilna\u201d and highlighted in bold. In such an instance, a PIXEL BIGRAMS model\nwill predict the whole unit, resulting in a lower performance. Furthermore, some of these \u201cwords\u201d in\nthe training data are more than a thousand characters long and therefore do not fit within the maximum\nsequence length of 529 patches.\n17https://github.com/google-research-datasets/tydiqa/blob/master/gold_passage_baseline/README.md\n18id = finnish-1438027099681899178-6\nA.5\nMeasuring self-similarity and intra-sentence similarity\nWe follow Ethayarajh (2019) and measure the degree of self-similarity and intra-sentence similarity for\nthe words in the two frequency samples from \u00a7 6.3. Self-similarity is computed as the cosine similarity\nbetween the same word in different sentences and a high degree therefore indicates that representations\nvary little across contexts. For intra-sentence similarity we compute the cosine similarity between a word\nrepresentation and the sentence representation (mean hidden state output across all tokens excluding the\nCLS token and black end-of-sequence token).19 This captures how aligned the representation of a word\nis with the sentence as a whole. If a word has both a low degree of self-similarity and intra-sentence\nsimilarity, we infer that the word has a context-specific representation that is still distinct from the other\nwords in that sentence. If self-similarity is low but intra-sentence similarity is high, this alludes to the\nword simply being contextualised by aligning its representation with the other words in that sentence. We\nsummarise these two measures in Figure 7 and find that, just like in Figure 4a, the upper layers produce\nmore context-specific representations as seen by the lower self-similarity, and that high-frequency words\nare the most context-specific. This is in line with Ethayarajh (2019) who finds that stopwords, being some\nof the most frequently observed words in the pretraining data, have some of the most context-specific\nrepresentations. The measure of intra-sentence similarity reveals that the contextualised representation\nof low-frequency words is more similar to that of its context, with high-frequency words having more\nnuance where words do not necessarily mean the same just because they appear in the same sentence.\n19Ethayarajh (2019) average over every word-sentence combination for a given sentence, not just a single word.\n"
  },
  {
    "title": "ChatCoder: Chat-based Refine Requirement Improves LLMs' Code Generation",
    "link": "https://arxiv.org/pdf/2311.00272.pdf",
    "upvote": "8",
    "text": "ChatCoder: Chat-based Refine Requirement Improves LLMs\u2019\nCode Generation\nZejun Wang\nKey Lab of HCST (PKU), MOE; SCS\nBeijing, China\nJia Li\nKey Lab of HCST (PKU), MOE; SCS\nBeijing, China\nGe Li\nKey Lab of HCST (PKU), MOE; SCS\nBeijing, China\nZhi Jin\nKey Lab of HCST (PKU), MOE; SCS\nBeijing, China\nABSTRACT\nLarge language models have shown good performances in generat-\ning code to meet human requirements. However, human require-\nments expressed in natural languages can be vague, incomplete,\nand ambiguous, leading large language models to misunderstand\nhuman requirements and make mistakes. Worse, it is difficult for a\nhuman user to refine the requirement. To help human users refine\ntheir requirements and improve large language models\u2019 code gen-\neration performances, we propose ChatCoder: a method to refine\nthe requirements via chatting with large language models. We de-\nsign a chat scheme in which the large language models will guide\nthe human users to refine their expression of requirements to be\nmore precise, unambiguous, and complete than before. Experiments\nshow that ChatCoder has improved existing large language models\u2019\nperformance by a large margin. Besides, ChatCoder has the advan-\ntage over refine-based methods and LLMs fine-tuned via human\nresponse.\nCCS CONCEPTS\n\u2022 Computer systems organization \u2192 Embedded systems; Re-\ndundancy; Robotics; \u2022 Networks \u2192 Network reliability.\nKEYWORDS\ncode generation, refine requirement, large language model, human\ninteraction\nACM Reference Format:\nZejun Wang, Jia Li, Ge Li, and Zhi Jin. 2018. ChatCoder: Chat-based Refine\nRequirement Improves LLMs\u2019 Code Generation. In Woodstock \u201918: ACM\nSymposium on Neural Gaze Detection, June 03\u201305, 2018, Woodstock, NY.\nACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX\n1\nINTRODUCTION\nLarge language models(LLMs) have performed well in code gen-\neration. Given human problem descriptions expressed in natural\nlanguage, LLMs can generate corresponding code to meet human\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\n\u00a9 2018 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06...$15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nrequirements. Not only do the well-known close-source LLMs for\nbusiness show the ability to generate code with high quality (e.g.,\nGPT-4[13] pass 67% of the tests in HumanEval[4]), but also the\nrecent open-source LLMs have reported their good results on code\ngeneration (e.g., Gunasekar et al. have designed an open-source\nLLM called phi-1[6] which has passed 50.6% of the tests in Hu-\nmanEval). Thus, applying LLMs to assist human programmers in\ntheir everyday coding tasks is promising.\nHowever, human\u2019s poor requirement expressions in natural lan-\nguage restrict LLMs\u2019 ability to generate better programs. Human\nexpressions can be vague, incomplete, and ambiguous. These low-\nquality requirement expressions mislead large language models to\ngenerate the wrong code. We raise an example from the sanitized-\nMBPP dataset[2] in Figure 1 to illustrate the issue, which is thought\nunambiguous by the authors. Suppose that we want gpt-3.5-turbo\nto write a function to find the largest negative number from the\ngiven list. Based on the original requirement, the large language\nmodel generates a program which can extract the negative numbers\nwith the largest actual value correctly. However, the authors of\nsanitized-MBPP think that the \u2019largest negative number\u2019 means the\nlargest absolute value. Thus the large language model generates\nthe wrong code due to the bad expression \u2019largest\u2019.\nThe problem can be solved via requirements refinement. Re-\nquirements refinement is the process of revealing the underlying\ndependencies and hidden structures[11]. With more details revealed,\nincomplete information will be filled up during requirement re-\nfinements, and the ambiguities will be clarified. In our example\nillustrated in Figure 1, we can simply reveal the hidden structure\nof \u2019the largest\u2019 as \u2019the largest absolute value\u2019 to the large language\nmodel. With the refined requirement, the large language model\ngenerated the code that fulfilled the MBPP\u2019s authors\u2019 expectations.\nRequirements refinement asks for the collaboration of human\nusers and large language models. In the context of requirement\nengineering, requirements refinement is performed by a series of\ninteractions between the software supplier (the coder) and the\nsoftware customer (the user). The software supplier analyzes the\ninitial expression of the customer\u2019s requirements and raises the\npoints of refinement. The software customers need to respond to the\npoints based on which the supplier can finish a round of refinement.\nNeither the software customer nor the software supplier is qualified\nto perform requirements refinement by themselves. According to\nIEEE Std 830-1998[1], customers usually do not understand the\nsoftware design and development process well enough to write\na usable one. Suppliers usually do not understand the customer\u2019s\narXiv:2311.00272v1  [cs.SE]  1 Nov 2023\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nproblem and field of endeavour well enough to specify requirements\nfor a satisfactory system. In the scenario of asking LLMs to generate\nprograms to fulfil human requirements, the human user of LLM is\nthe customer, and the LLM itself is the supplier. To let the supplier\nLLM produce code that better fulfils the user\u2019s requirements via\nrequirements refinement, we need to develop a method for humans\nand LLMs to collaborate.\nWe propose ChatCoder, a new method for code generation with\nlarge language models through requirements refinement via chat.\nIt is a concise dialogue framework that assists LLMs and humans\u2019\ncollaboration on requirements refinement via chatting. The key\nproblem is how to chat with the large language model. Our solu-\ntion, ChatCoder, has a novel chatting schema designed inspired\nby IEEE Recommended Practice for Software Requirements Specifi-\ncations(IEEE SRS)[1]. This paper mainly discusses method-level\ncode generation. Referring to the contents of software requirement\nspecifications raised by IEEE SRS covering every corner of a soft-\nware\u2019s life cycle, we raise six angles covering the development of a\nmethod and provide the angles to large language models to analyze\nthe requirement specifications. Then the large language models\nlead the human user to refine the requirements based on its anal-\nysis by adding information, correcting mistakes, giving examples,\nand answering questions. The whole process is in the chat form.\nIn this paper, we test ChatCoder on the HumanEval dataset and\nSanitized-MBPP dataset, and the test results show that the refined\nrequirements with ChatCoder improve the LLM\u2019s code generation\nperformances by a large margin, at an average of the percentage of\n10. The results show that ChatCoder\u2019s refinement is effective and\nefficient.\nOur contribution is summarized as follows:\n\u2022 We find and raise the problem that human\u2019s poor require-\nment expressions in natural language limit LLMs\u2019 ability to\ngenerate better programs.\n\u2022 We point out the necessity to ask for the collaboration of\nhumans and large language models.\n\u2022 We raise ChatCoder, a dialogue framework effectively as-\nsisting human and LLM\u2019s collaboration on requirements\nrefinement for better code generation.\n2\nBACKGROUND\n2.1\nLarge Language Model for Code Generation\nLarge language models are currently pre-trained Transformer-based\nlanguage models with at least tens of billions of parameters. The\nfirst well-known large language model is GPT-3 [3] proposed by\nOpenAI, and GPT-3 presented its extraordinary code generation\nability. Following GPT-3, a series of business-oriented close-source\nlarge language models have been proposed, e.g. GPT-3.5 and GPT-4,\nwhose code generation abilities improve day by day. Besides, several\nopen-source large language models for code-related tasks have\nbeen published, e.g., StarCoder[9], CodeT5+[14]. WizardCoder[12]\netc. They have been proven to have comparable code generation\ncapabilities with the close-source large language models.\nThe current way of applying large language models in code\ngeneration is via prompting techniques. A prompt is a formatted\ntext wrapping the user\u2019s original instruction for the large language\nmodel. Then the prompt is sent to the large language model as\ninput to get the large language model\u2019s response. Given the user\u2019s\ndescription of a programming task, properly designed prompts will\nmake it easier for large language models to generate the correct\ncorresponding code. For example, Li et al. [8]propose that providing\nexamples closely related to the programming tasks can help large\nlanguage models to generate better code. Jiang et al. [7]propose\nthat appending the text that encourages the LLMs to decompose the\nprogramming task helps large language models solve complicated\nproblems. In this paper, our proposed method can be categorized\nas prompt engineering as well.\n2.2\nRequirements Refinement\nRequirements refinement is both a process of deriving specifica-\ntions and a necessary means towards preparing architecture designs.\nDuring requirements refinement, the design of requirement spec-\nifications should reveal its underlying dependencies and hidden\nstructure. Requirements refinement is the start from requirements\nto implementation design. It is important because many users in\npractice do not understand what functions they want precisely at\nthe beginning of a software project[10]. With requirements refine-\nment, the users and software suppliers can agree on what function\nthe user truly needs.\nPrevious studies of requirements refinement focus on providing\na formal method for the software supplier to analyse and refine\nthe software customer\u2019s requirements. Liu [11] raises a hierarchi-\ncal framework from the business level to the component level to\nrefactor the customer\u2019s requirements. Darimont and Lamsweerde\n[5] propose formal refinement patterns for goal-driven require-\nments elaboration via KAOS. Liu[10] proposes to use the SOFL\nlanguage to describe the refinement process and raises the model\nof successive refinements in which the requirements refinement is\na process from coarse to fine with a loop back. Jong et al. propose\nto use nondeterminism and parameterised specifications to support\nstep-wise specifications and have the specifications written and\nanalysed using the language and proof checker of PVS.\nRequirements refinement requires collaboration with both the\nsoftware provider and the software user. On the one hand, require-\nment analysing methods, e.g., the formal approaches mentioned\nabove, can not achieve complete and accurate themselves. Domain\nknowledge and the customer\u2019s personal quality (e.g., the ability to\nexpress oneself clearly) are essential in requirements refinement. On\nthe other hand, the users can not refine their requirements by them-\nselves since they may not understand the software design enough,\nleading to the phenomenon that their proposed requirements may\nnot fulfil what they actually want. So what is important is to\nfind a friendly way of interaction: the software provider find the\npoints in the user\u2019s requirement which need refinement, then the\nprovider asks for the software user\u2019s comments on the refinement\nin the way that the software user can understand and give proper\nanswers.\n3\nMETHODOLOGY\n3.1\nOverall Structure of ChatCoder\nChatCoder is code generation method through requirements refine-\nment via a dialogue framework designed for the communication\nChatCoder: Chat-based Refine Requirement Improves LLMs\u2019 Code Generation\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\n#MBPP/443\nWrite a python function to find the largest\nnegative number from the given list.\ndef largest_neg(list1):\n    if len(list1) == 0:\n        return None\n    max_neg = None\n    for num in list1:\n        if num < 0:\n            if max_neg is None or num > max_neg:\n                max_neg = num\n    return max_neg\n# User's Real Intention\n>>> largest_neg([1,2,3,-4,-6]) -> -6\n>>> largest_neg([1,2,3,-8,-9]) -> -9\n#MBPP/443 Refined Requirements\nWrite a python function to find the largest\nnegative number from the given list.\n largest negative number: The largest\nnegative number refers to the negative\nnumber with the largest absolute value in\nthe given list.\nRefine the concept of \"largest\"\ndef largest_neg(list1):\n    largest = None\n    for num in list1:\n        if num < 0:\n            if largest is None or abs(num) >\nabs(largest):\n                largest = num\n    return largest\nOrignal Requirement\nRefined Requirement\nlargest:\nlargest actual value?\nlargest abstract value?\nCode generated before refinement\nCode generated after refinement\nFigure 1: Example of Refinement Improving Code Generation Performance\nbetween a large language model and its user to refine the require-\nments. Within the framework, a large language model can analyse\nthe arguments to refine the user\u2019s original requirement expression,\nthen return the arguments back to the users in a way that human\nusers can easily understand and give responses.\nThe overall structure of ChatCoder is a two-round dialogue illus-\ntrated in Fig 2. The first round is Paraphrase and Extend. Since the\nhuman user\u2019s expression of requirements can be vague, incomplete\nand ambitious, ChatCoder uses prompts to ask the large language\nmodel to paraphrase the user\u2019s original requirements from sev-\neral angles that complete requirement specifications must be clear.\nFor the missing or ambitious arguments which require refinement,\nChatCoder asks the large language model to extend them with its\nassumptions gained from its training data. Human users need to\nreview the refined specifications and correct the mistakes within.\nThe second round is Going-deep and Loop-back. In this round, Chat-\nCoder requires large language models to ask the human users about\ntheir confusion about the refined specifications in Paraphrase and\nExtend for losing information and further refinement. Human users\nneed to answer the questions and loop back to correct the refined\nspecifications when the users find the large language model\u2019s ques-\ntions are raised based on wrong requirement specifications. After\nthe two rounds of refinement, the refined requirement is obtained\nand then sent to large language models to get the user\u2019s desired\nprograms.\nIn the following paragraphs, we will explain the design of each\nround in detail.\n3.2\nParaphrase and Extend\nThe large language model is asked to paraphrase the user\u2019s ini-\ntial requirement expression in this round. The paraphrase is per-\nformed by extending the initial requirement on the preset angles\nextracted from existing research of requirement engineering. Chat-\nCoder wraps the instruction to paraphrase the user\u2019s requirement\nand the angles used for extension in a prompt to order the large\nlanguage model to perform the paraphrase. Then the prompt is\nsent to the large language model for its response. The format of the\nprompt is presented in Figure 3.\nThe angles selected are based on the environment of applying\nChatCoder. Since this paper mainly discusses generating method-\nlevel programs, the angles for ChatCoder are all about method-level\nrequirements refinement. In particular, the ChatCoder in this paper\nhas five angles for the Paraphrase and Extend round, inspired by\nIEEE Recommended Practice for Software Requirements Specifications:\n\u2022 Key Concepts This angle asks the large language model to\nextract and explain the key concepts involved in the user\u2019s\nrequirements, including objects and actions. By extending\nthis angle, the user and the large language model can align\ntheir understanding of the key concepts, setting a firm basis\nfor further discussion.\n\u2022 Method Purpose This angle asks the large language model\nto paraphrase the function provided by the method to be\nimplemented. In this angle, the large language model will\ndescribe the transformation for the input and the changes\nof the running states in a more detailed way. The LLM\u2019s\ndescription reflects its ongoing implementation based on the\nLLM\u2019s understanding of the initial requirement expression\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\ninitial requirement\nspecifications\nKey Concept\nMethod\nPurpose\nInput\nRequirements\nOutput\nRequirements\nEdge Cases\nExceptions\nand Errors\nGenerate by\nParaphase & Extend\nKey Concepts\nInput\nOutput\nEage Cases\ngoing deep\ngoing deep\ngoing deep\ngoing deep\ngoing deep\ngoing deep\nGenerate\nReview and Edit\nReview and Edit\nLoop-back to\nCorrect Mistakes\nLarge language model\nHuman user\nHuman user\nPhase 1: \nParaphrase and Extend\nPhase 2:\nGoing-deep and Loop-back\nRefined Requierment Specifications\nFigure 2: Overall Structure of the ChatCoder Dialogue Framework\nand its inference for the incomplete expression, revealing\nthe error and incompleteness of the requirement expression.\n\u2022 Input Requirements This angle asks the large language\nmodel to extend the requirements for the method\u2019s inputs,\nincluding the parameters\u2019 types, actual meaning, boundaries\nand properties. Explaining the meanings is another chance\nfor the LLM and the user to align their understanding of\nthe requirements. The type, boundary and property are eas-\nily missing but play important roles in the design of the\nalgorithm.\n\u2022 Output Requirements This angle asks the large language\nmodel to extend the requirements for the method outputs,\nincluding the types, the meaning and the format. Explaining\nthe meanings is another chance for the LLM and the user\nto align their understanding of the requirements. While a\nmethod may serve other methods, its returning type and\nformat matter but can be missing, e.g., the decimals to reserve\nfor a floating-point output number.\n\u2022 Edge Cases This angle asks the large language model to\nextend possible edge cases and solutions. Since a method can\nrun in complicated outer environments, the input and the\nglobal variable states may not fulfil the method\u2019s running\npreconditions. So properly handling edge cases is necessary\nfor a robust method implementation but can be easily ignored\nby software customers.\n\u2022 Exceptions and Errors This angle asks the large language\nmodel to extend the solutions for possible exceptions and\nerrors during the method\u2019s execution. Like edge cases, han-\ndling exceptions and errors are necessary but can be easily\nmissed by the users because of their unprofessional soft-\nware design. The large language model must analyse, raise\nsolutions and wait for the users\u2019 review.\nThe human user is supposed to review the large language model\u2019s\nresponse to the instructions for refining requirements. For the key\nconcepts and method purpose, the human user is requested to\ncorrect the mistakes made by the large language model. For the\ninput and output requirements, the human user is requested to\ncorrect the mistakes for the meanings and review whether the\nlarge language model\u2019s inference on the input and output formats\nmeets the real needs. For the edge cases, exceptions and errors, the\nusers are requested to review whether they can occur and whether\nthe large language model\u2019s proposed solution is satisfactory. If the\nhuman user encounters an expression that is difficult to understand\nand rewrite, the user can directly delete the expression.\nOur design of Paraphrase and Extend is an effective and efficient\nway for the large language model and the human user to commu-\nnicate for requirements refinement. First, our instructions for the\nlarge language model are in natural language. Compared with the\nformal language designed for human coders to analyse the require-\nments for refinement, large language models are more familiar with\nnatural languages since most of their training data is in natural\nlanguages. Second, the angles mentioned in the instructions cover\nChatCoder: Chat-based Refine Requirement Improves LLMs\u2019 Code Generation\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nmany reasons humans and AI programmers make mistakes. Refin-\ning the requirements from these angles helps reduce programming\nmistakes. Third, it is easy for human users to read, understand\nand modify the refined requirements, thanks to the LLM\u2019s string\nexpression power. Most of the refined specifications are generated\nby the large language model. All the work left for human users is\nonly to make modifications, which is a small workload compared to\ngenerating the whole refined specifications, not to say that human\nusers may not know what to write for the refinement.\n3.3\nGoing-deep and Loop-back\nIn this round, the large language model is asked to going-deep: to\nfurther refine the requirements based on the specifications obtained\nin Paraphrase and Extend; the human user is requested to loop back:\ncheck for possible errors in the reviews and the errors corrected.\nGoing-deep The large language model is asked to raise questions\nin the angles based on the existing specifications obtained in Para-\nphrase and Extend. The instruction for the large language model is\nalso wrapped in a prompt, presented in Figure 3. We design Going-\ndeep to refine the requirements further because the large language\nmodel is a black box, and it is hard to say we have used up its\npotential to refine requirements through Paraphrase and Extend.\nIn this round, we let the large language model ask questions in\na free form for what confused the most about the requirements,\nthen give possible answers based on its observations or assump-\ntions. Suppose the large language model keeps raising questions\nwhich are answered in the specifications. In that case, we regard the\nspecifications are complete enough for the large language model to\ngenerate corresponding programs.\nLoop-back The user is asked to review the questions and an-\nswers generated by the large language model in Going-deep and\ncorrect the wrong answers for further refinement. The user may\nfind that the large language model raises wrong questions, e.g., it\nasks whether the output list should be sorted. However, the desired\noutput is an integer. In this scenario, the user must \"Loop-back\":\nreview the specifications in Paraphrase and Extend to look for the\nwrong expressions leading to the wrong questions, then have them\ncorrected. Loop-back is important because it is difficult to guarantee\nthat the users never make mistakes.\nAfter Going-deep and Loop-back, the user will have the updated\nspecifications from Paraphrase and Extend and the further refined\nspecifications from Going-deep. Then these refined specifications\nare appended to the original expression of requirements and sent\nto the large language model to get the large language model\u2019s\ngenerated programs.\n4\nEXPERIMENTS\n4.1\nExperiment Settings\nDatasets: We select three datasets for our experiments:\n\u2022 Sanitized-MBPP A well-known and widely-used dataset[2].\nIts test set contains 257 programming questions which stan-\ndalone Python methods can solve. We choose this dataset for\nthe following reasons. First, its task descriptions are short,\nwhich means they are more likely to be incomplete and am-\nbiguous than longer descriptions, so we can find out whether\nChatCoder can let LLMs analyze the points within each de-\nscription for refinement. Second, the authors of Sanitized-\nMBPP claimed that these task descriptions are manually\nchecked for disambiguation. It provided a chance to vali-\ndate whether ChatCoder can make LLM analyze the task\ndescription from a different angle from the dataset\u2019s authors.\n\u2022 HumanEval A well-known and widely-used dataset[4]. It has\n164 programming questions to be solved by Python programs.\nWe chose this dataset because its task descriptions are longer\nand more complicated than those of Sanitized-MBPP, from\nwhich we want to evaluate whether ChatCoder can still find\nthe points for refinement and keep improving LLMs\u2019 code\ngeneration performances.\nBaselines: We select four baselines for our experiments:\n\u2022 gpt-3.5-turbo. The latest version of the gpt-3.5-turbo family,\na family of closed-source large language models published by\nOpenAI. It is powerful enough and easy to access, leaving the\ntime long enough for anyone to reproduce our experiments\nbefore it is deprecated.\n\u2022 gpt-4. The newest generation of the closed-source large lan-\nguage model, published by OpenAI, performs extraordinarily\nwell on code generation.\nGeneration Configurations For HumanEval, we perform greedy\ngeneration, which means the generation is zero-shot, and the sam-\npling is performed only once with a temperature of 0. For Sanitized-\nMBPP, we perform 3-shot generation. For each task, we sample 20\nprograms with top_p=0.2 when evaluating models for gpt-3.5-turbo.\nAs for GPT-4, because there is a calling rate limit and the calling\nfee is high, it is difficult and expensive to sample 20 programs for a\nprogramming task. So we sample one program for a programming\ntask with temperature 0 like HumanEval. The version of GPT-4 is\ngpt-4-0613. The version of gpt-3.5-turbo is gpt-3.5-turbo-0613. For\na fair comparison, we rerun all the baselines with the same prompts\nand our generation configuration rather than copy the results from\nthe original papers.\nMetrics We report the test pass rate[4]. For HumanEval and\nSanitized-MBPP on GPT-4, we report pass@1. We report pass@1,\npass@2, pass@5, and pass@10 for the other settings.\n4.2\nResearch Questions\nTo evaluate our proposed ChatCoder, we raise and investigate the\nfollowing research questions:\n\u2022 1) How does ChatCoder perform compared with existing\ncode generation models?\n\u2022 2) Is ChatCoder an efficient method for LLM and human\nusers to communicate for requirement refinement?\n\u2022 3) How much improvement is brought by human involve-\nment in ChatCoder?\n4.3\nRQ1: Code Generation Performances\nRQ1 is to evaluate ChatCoder\u2019s overall code generation perfor-\nmances compared with the baselines. Our results are reported in\nTable 1. When investigating RQ1, we try ChatCoder with GPT-4\nand gpt-3.5-turbo-0613. We performed Paraphrase and Extend and\nGoing-deep and Loop-back on gpt-3.5-turbo-0613 and obtained the\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nPrompt for the LLM: Paraphase and Extend\nExample Output from Large Language Model\nHere is a programming task:\n{task}\nPlease help refine the requirement specification following the steps:\n1. List and clarify the definitions of the key concepts\n2. Explain the purpose of the method to be implemented with your understandings\nin details\n3. Analyze each parameter's type, meanings, format, properties and restrictions.\n4. Analyze the return values' types, meanings, format, properties and restrictions.\n5. Raise possible edge cases and solutions\n5. Raise possible exceptions and errors and the solutions.\ne.g. \nHere is a programming task:\ndef filter_by_prefix(strings: List[str], prefix: str) -> List[str]:\n    \"\"\" Filter an input list of strings only for ones that start with a given prefix.\n    >>> filter_by_prefix([], 'a')\n    []\n    >>> filter_by_prefix(['abc', 'bcd', 'cde', 'array'], 'a')\n    ['abc', 'array']\n    \"\"\"\nPlease help refine the requirement specification following the steps:\n1. List and clarify the definitions of the key concepts\n2. Explain the purpose of the method to be implemented with your understandings\nin details\n3. Analyze each parameter's type, meanings, format, properties and restrictions.\n4. Analyze the return values' types, meanings, format, properties and restrictions.\n5. Raise possible edge cases and solutions\n5. Raise possible exceptions and errors and the solutions.\n1. Key concepts:\n- `strings`: A list of strings that will be filtered.\n- `prefix`: A string that will be used to filter the list of strings.\n2. Purpose of the method:\nThe purpose of the `filter_by_prefix` method is to filter an input list of strings and\nreturn a new list that only contains strings starting with a given prefix.\n3. Parameter analysis:\n- `strings`: This parameter is of type `List[str]`, which means it expects a list of strings\nas input.\n- `prefix`: This parameter is of type `str`, which means it expects a string as input. The\nprefix is the string that will be used to filter the list of strings.\n4. Return value analysis:\nThe return value of the `filter_by_prefix` method is a new list of strings that only\ncontains strings starting with the given prefix. The return type is `List[str]`.\n5. Possible edge cases and solutions:\n- If the input list `strings` is empty, the method should return an empty list.\n- If none of the strings in the input list `strings` start with the given prefix, the\nmethod should return an empty list.\n6. Possible exceptions and errors:\n- There are no specific exceptions or errors mentioned in the provided code.\nHowever, some possible exceptions that could occur are:\n  - If the `strings` parameter is not a list, a `TypeError` should be raised.\n  - If the `prefix` parameter is not a string, a `TypeError` should be raised.\n  - If any of the strings in the `strings` list are not of type `str`, a `TypeError` should be\nraised.\nTo handle these exceptions, appropriate type checks and error handling should be\nimplemented in the method.\nPrompt for LLM: Going-deep and Loop-back\nExample Output from Large Language Model\nHere is a programming task:\n{the task: with the refinement from previous step aggregated}\nBased on the given task descriptions, please:\nRaise up to 5 question about the requirements that you concern the most. \n    - Each questions should be followed by an answer based on your\nunderstanding of the task or your guess. \n    - You can raise questions from the following aspects: [Key Concept]\n[Input Requirement] [Output Requirement] [Edge Case], and any other\ncategories you concern.\n    - Don't ask questions that have answers in the task description.\n1. [Input Requirement] Can the input list `strings` contain duplicate strings?\n   - the input list can contain duplicate strings.\n2. [Output Requirement] Should the returned list of strings be in the same order as\nthe original list?\n   - Yes. the returned list of strings should be in the same order as the original list\n3. [Edge Case] What should be the behavior if the input list `strings` is None?\n   - raise a `TypeError` since it expects a list of strings as input.\n4. [Edge Case] What should be the behavior if the input prefix is an empty string?\n   - the method should return an empty list since no strings in the input list will start\nwith an empty string.\n5. [Edge Case] What should be the behavior if the input prefix is longer than any\nstring in the input list?\n   - the method should return an empty list since no strings in the input list will start\nwith a prefix longer than themselves.\nFigure 3: Prompts for Large Language Models and Example Outputs\nrefined requirement specifications. Then, we feed these refined\nrequirement specifications to GPT-4 and gpt-3.5-turbo-0613 to get\ntheir generated code and test the pass rates. We obtained the refined\nrequirement specifications from gpt-3.5-turbo-0613 because it is\nan LLM with the ability to perform requirement analysis and is\neasy to access. Compared with gpt-3.5-turbo-0613, GPT-4 is both\nexpensive and strict with access.\nAccording to Table 1, ChatCoder successfully helps large lan-\nguage models improve their generated program\u2019s execution ac-\ncuracy through the refined requirements by a large margin. For\nexample, for gpt-3.5-turbo, its pass@1 on Sanitized-MBPP is im-\nproved from 57.04% to 71.25%, and the margin is the percentage of\n14. Compared horizontally, for both gpt-3.5-turbo and gpt-4, the\nperformance improvements on Sanitized-MBPP is more prominent\nthan those on HumanEval, which is because the task descriptions\nof Sanitized-MBPP are single sentences and method signatures,\nmuch more simple than the task descriptions of HumanEval. Thus\nthe information for code generation of Sanitized-MBPP is far less\nsufficient than the information of HumanEval. As a result, when\nChatCoder brings the refined requirement specifications full of\nadditional information, the code generation performance on MBPP\nis more prominent than the improvement on HumanEval.\n4.4\nRQ2: Communication Efficiency Evaluation\nWe evaluate whether ChatCoder is an efficient way for large lan-\nguage models and humans to communicate for requirements re-\nfinement. The key of ChatCoder is the constraints, i.e., the angles\nprovided for the large language models to analyse the initial ex-\npression of requirements for refinement and the instructions we\ndesigned to convey LLMs the angles. So we compare ChatCoder\nwith two other ways of communicating with the large language\nChatCoder: Chat-based Refine Requirement Improves LLMs\u2019 Code Generation\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nTable 1: Code Generation Performances\nHumanEval\nSanitized-MBPP\npass@1\npass@1\npass@2\npass@5\npass@10\ngpt-3.5-turbo\n70.12%\n57.04%\n58.17%\n59.13%\n59.75%\ngpt-4\n81.10%\n66.15%\n-\n-\n-\nChatCoder(gpt-3.5-turbo)\n79.87%\n71.25%\n73.23%\n75.18%\n76.25%\nChatCoder(gpt-4)\n90.24%\n76.65%\n-\n-\nTable 2: Communication Efficiency Comparison\nHumanEval\nSanitized-MBPP\npass@1\npass@1\npass@2\npass@5\npass@10\ngpt-3.5-turbo\n70.12%\n56.95%\n58.16%\n59.48%\n60.48%\nFree Paraphrase\n78.05%\n64.61%\n65.47%\n66.17%\n66.68%\nFree QA\n71.95%\n66.47%\n68.82%\n70.91%\n72.00%\nChatCoder\n79.87%\n71.25%\n73.23%\n75.18%\n76.25%\nmodel: 1) Free Paraphrase: We let the large language model para-\nphrase the original programming task without giving any angles\nand ask the human user to have it edited and corrected for cogni-\ntion alignment; 2) Free QA: We let the large language model to\nask human users questions about their confusion about the origi-\nnal programming task and collect the human users\u2019 responses. All\nthese experiments are conducted based on gpt-3.5-turbo-0613. The\nresults are presented on Table 2\nAccording to Table 2, all three communication methods with\nLLMs for requirements refinement help the LLM improve its code\ngeneration results. This finding points out that any form of re-\nquirements refinement is useful and important in applying LLMs to\ngenerate code. Compared with ChatCoder, Free Paraphrase and Free\nQA do not instruct the LLM to perform certain kinds of refinement,\nleading to lower improvements. With careful inspection, we find\nthat the additional contents generated by the LLM for requirements\nrefinement surround our proposed analysis angles spontaneously.\nHowever, due to lacking explicit instructions, the refinement can\nnot cover all the points covered by ChatCoder. So explicitly instruct-\ning the LLM with the angles for refining requirements is important\nfor ChatCoder. Designing better instructions to order the LLM to\nrefine requirements is part of our future work.\n4.5\nRQ3: Human Intervention Evaluation\nWe evaluate how important human intervention is to ChatCoder.\nThis experiment is to prove the argument that requirements refine-\nment should involve the participation of both software provider\nand software supplier, in this paper, the human user and the large\nlanguage model.\nWe evaluate the human intervention by comparing it with ask-\ning the large language model to paraphrase and generate further\nquestions without human\u2019s edit and correction, referred to as \u2019Auto-\nRefine\u2019 in the following description. We compare the LLM\u2019s code\ngeneration performances of Auto-Refine and our ChatCoder. All\nexperiments are conducted on gpt-3.5-turbo-0613. The results are\npresented in Table 3\nIt is not surprising that Self-Refine hurts the LLM\u2019s code gen-\neration performances. Since ChatCoder utilizes requirements re-\nfinement to improve the large language model\u2019s code generation\nperformance, human intervention is necessary and can not be ne-\nglected. The process of ChatCoder is to reveal the inner structure\nof the requirements from the given angles, which are not expressed\nexplicitly, even with ambiguity. The answer to solving the ambigu-\nity is known only by the human user. But Auto-Refine just guesses\nan answer based on the large language model\u2019s training data, rep-\nresenting how most people understand the requirement. Suppose\nthe large language model\u2019s guess or explanation of the requirement\nis wrong without human edits. In that case, the large language\nmodel will generate code following the wrong understanding of\nrequirements and give up the other possible understandings. Thus,\nAuto-Refine hurts the LLM\u2019s code generation performances.\n5\nDISCUSSION\n5.1\nCase Study\nThis section raises several real test cases illustrating how ChatCoder\nhelps LLMs generate code with refined requirements. Due to the\npage limit, we select three cases from MBPP covering refinement\nabout the input, the output and the purpose since they influence the\nfunctional requirements directly. In contrast, edge cases and excep-\ntions influence the robustness, requiring more space to illustrate.\nWe put the cases in Figure 4.\n\u2022 MBPP/91 This task asks the coder to write a method checking\nif a string is presented in any string as a substring within a\nlist. Due to the word \u2019if\u2019, we know the output of this method\nshould be of judgement. However, the large language model\nmisunderstands the task and returns a list of words. Because\nChatCoder asks the large language model to analyze the\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nTable 3: Human Intervention Evaluation\nHumanEval\nSanitized-MBPP\npass@1\npass@1\npass@2\npass@5\npass@10\ngpt-3.5-turbo\n70.12%\n56.95%\n58.16%\n59.48%\n60.48%\nAuto-Refine\n68.90%\n52.82%\n54.77%\n56.30%\n57.12%\nChatCoder\n79.87%\n71.25%\n73.23%\n75.18%\n76.25%\noutput, the output requirement is refined, indicating that\nthe method should return a boolean value. The large lan-\nguage model generates the correct code based on the refined\nrequirement.\n\u2022 MBPP/118 This task asks the coder to write a method con-\nverting a string to a list. The large language model misunder-\nstands the purpose of the method. The string should be split\ninto words. However, the LLM thinks the method should be\nsplit into characters. The purpose of this method expressed\nby the original requirement is incomplete. ChatCoder asks\nthe LLM to analyze the purpose of the method, and the LLM\nreturns that the method should split the string by characters.\nThe human user reviews the refined expression and corrects\nthis mistake. With the corrected refined requirement, the\nlarge language generates the correct code.\n\u2022 MBPP/307 This task asks the coder to write a method to get a\ncolon of a tuple. However, the expression is incomplete. The\nmeaning of the parameters is missing, requiring refinement.\nWithout refinement, the large language model thinks \u2019m\u2019\nand \u2019n\u2019 are some indexes, leading to generating the wrong\ncode. ChatCoder asks the LLM to analyze the meaning of\neach input parameter. The LLM responds that \u2019m\u2019 and \u2019n\u2019\nare the index of the colon, which is wrong. The human user\nreviews the refined specification and corrects the meaning\nthat \u2019m\u2019 is the index of the colon and \u2019n\u2019 is the value to be\nappended to the colon. The large language model generates\nthe correct code with the corrected refined requirement.\n5.2\nSavings of Human Labor Costs\nCompared with performing requirements refinement with require-\nment engineers, ChatCoder asks the large language model to gen-\nerate most of the text. At the same time, human users just need\nto review and edit, saving lots of human labour. This section will\nanalyze how much human labour costs are saved.\nWe evaluate the savings of the human labour costs by calculating\nhow many tokens in the final refined requirement specifications\nare from humans. The statistics are shown in Figure 5. From Figure\n5, we can see that tokens from human users take only a tiny pro-\nportion of the refined specifications. To boost the code generation\nperformance, the users need to review the text, delete anything\nthey do not like, and input, on average, ten tokens due to the help\nof ChatCoder.\n5.3\nRelevance and Completeness\nWe need to evaluate whether the improvement is due to Chat-\nCoder\u2019s refined requirements and whether the users think Chat-\nCoder\u2019s refined requirement specifications fulfil their needs well.\nThus we invited three people outside the research group to give\nscores on ten randomly selected ChatCoder\u2019s refined requirements\nabout the \u2018relevance\u2019 and \u2018completeness\u2019. The results are depicted\nin Figure 6. We ask the testers to compare the requirements before\nand after refinement and the code generated before and after the\nrequirement refinement. Then we ask them to give a score (1-5) to\njudge whether the refinement relates to the improvement of the\ngenerated code (The real score, 1 for unrelated and 5 for directly\nrelated). Besides, we ask them to give a score (1-5) to judge whether\nthe refinement makes them clearer about the user\u2019s requirements\n(The comp score, 1 for getting confused and 5 for getting clear). We\ncalculate the average scores with error bars and have the results\ndepicted in Figure 6.\nThrough Figure 6, we find that all testers agree that the refined\nrequirements help the large language model generate better code\nand help themselves better understand the requirements. However,\ncompared with the real score, the confidence that people get clearer\nabout the problems is slightly weaker. That is because people judge\nthe quality of the code partially based on the execution test results.\nHowever, execution tests are not perfect. The program passing\ncertain test cases may not really fulfil the user\u2019s requirements. So\nChatCoder still needs to be improved to refine the requirements\nbetter to fulfil the user\u2019s true needs.\n5.4\nThreats to Validations\nThere are a few threats to our methods\n\u2022 1) The user quality. The reviews and edits are performed\nby the volunteer professional programmers in our research\ngroup. So they deeply understand large language models,\nprogramming languages and algorithms. However, it can not\nbe guaranteed that every user of large language models is\nas good at these things as our researchers. So finding some\nway to test ChatCoder for ordinary users of large language\nmodels is on our future work list.\n\u2022 2) The dataset. We use the datasets, HumanEval and Sanitized-\nMBPP in this paper to align the other research in this field.\nHowever, there is a flaw: these datasets do not really come\nfrom \u2019our requirements\u2019 and are too simple compared with\nreal-world programs. One reasonable but difficult-to-realize\nsolution is to recruit a group of full-time programming work-\ners to evaluate the effect of ChatCoder in their real-world\nChatCoder: Chat-based Refine Requirement Improves LLMs\u2019 Code Generation\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nTask ID\nRequirement\nCode\nMBPP/91\n(Output)\nOriginal Requirment:\ndef find_substring(str1, sub_str):\n   \"\"\"\n   Write a function to check if a substring is present in\na given list of string values.\n   \"\"\"\nOriginal Code:\ndef find_substring(str1, sub_str):\n    result = []\n    for string in str1:\n        if sub_str in string:\n            result.append(string)\n    return result\nRefined Requirement:\n...\n4. Return value analysis:\n    The return value of the method should be a boolean\nindicating whether the substring is present in any of\nthe strings within the list. It should be of type bool\n...\nCode with ChatCoder\ndef find_substring(str1, sub_str):\n    # ... a few edge case check...\n    if sub_str == \"\":\n        return True\n    for string in str1:\n        if sub_str in string:\n            return True\n    return False\nTest Case: \n>>> find_substring([\"red\", \"black\", \"white\", \"green\", \"orange\"],\"ange\")\n   True\nTask ID\nRequirement\nCode\nMBPP/118\n(Purpose)\nOriginal Requirment:\ndef string_to_list(string):\n    \"\"\"\n    [link text](https:// [link text](https:// [link text]\n(https://)))write a function to convert a string to a list\n   \"\"\"\nOriginal Code:\ndef string_to_list(string):\n    return list(string)\nRefined Requirement:\n...\n    2. Purpose of the method:\n    The purpose of the method is to convert a given\nstring into a list. This means that each word in the\nstring will be an individual item in the resulting list.\n...\nCode with ChatCoder\ndef string_to_list(string):\n   # ... some doc string\n    return string.split()\nTest Case: \n>>> string_to_list(\"python programming\")\n    ['python','programming']\nTask ID\nRequirement\nCode\nMBPP/307\n(Input)\nOriginal Requirment:\ndef colon_tuplex(tuplex,m,n):\n  \"\"\"\n  Write a function to get a colon of a tuple.\n  \"\"\"\nOriginal Code:\ndef colon_tuplex(tuplex, m, n):\n    return tuplex[m:n]\nRefined Requirement:\n...\n    3. Parameter analysis:\n    - `tuplex`: The input tuple from which the colon\nneeds to be extracted. It can contain elements of any\ntype.\n    - `m`: the index of the colon, i.e., the empty list.\n    - `n`: the value to be append to the colon\n...\nCode with ChatCoder\ndef colon_tuplex(tuplex, m, n):\n   # Case Check...\n    new_tuple = list(tuplex)\n    new_tuple[m].append(n)\n    return tuple(new_tuple)\nTest Case: \n>>> >>> colon_tuplex((\"HELLO\", 5, [], True) ,2,50)\n  (\"HELLO\", 5, [50], True)\nFigure 4: Case Study\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nmax\n76\nmedian\n4\naverage\n9.93\nStatistics of \nTokens from Human\nHistogram of tokens from humans\nx-axis: numbers of tokens from human\ny-axis: number of tasks\nComparison of tokens from llms and humans for each task\nFigure 5: Statistics of Human Labor Savings\nFigure 6: Human Evaluation Score\njob. Finding a more applicable way of evaluating ChatCoder\nis one of our future work.\n\u2022 3) The length. The refined requirement specifications ob-\ntained by ChatCoder are a bit long. The long text brings\ntwo problems. First, it is a heavy burden for the human user\nto review and edit, leading to a high opportunity to make\nmistakes. Second, we find through our observation that the\ncurrent large language models have difficulty in coping with\nlong text: they can ignore the logic dependency of two dis-\ntanced terms. One of our future works is compressing the\nrefined requirement specifications and preserving all the\nnecessary information.\n6\nCONCLUSION\nWe propose ChatCoder, an effective method to improve large lan-\nguage models\u2019 code generation performances by requirement re-\nfinement via chat. We design a two-round dialogue framework to\nguide the large language model, refine the original requirements\nthrough five angles, and go deeper. Then we ask the human users\nto review and edit the generated refined requirement specifica-\ntions. We apply ChatCoder to the famous large language models:\ngpt-4 and gpt-3.5-turbo and prove that ChatCoder improves their\ncode generation ability by a large margin. Besides, we prove that\nChatCoder is an efficient way of communicating with the LLMs\nfor requirements refinement, and human intervention is needed in\nrequirements refinement.\nREFERENCES\n[1] 1998. IEEE Recommended Practice for Software Requirements Specifications.\nIEEE Std 830-1998 (1998), 1\u201340. https://doi.org/10.1109/IEEESTD.1998.88286\n[2] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk\nMichalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le,\nand Charles Sutton. 2021. Program Synthesis with Large Language Models. CoRR\nabs/2108.07732 (2021). arXiv:2108.07732 https://arxiv.org/abs/2108.07732\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\nChris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nIn Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-\nzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,\nInc., 1877\u20131901.\nhttps://proceedings.neurips.cc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\nOliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-\ntios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shan-\ntanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei,\nSam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large\nLanguage Models Trained on Code. (2021). arXiv:2107.03374 [cs.LG]\n[5] E. de Jong, J. van de Pol, and J. Hooman. 2000. Refinement in requirements\nspecification and analysis: a case study. In Proceedings Seventh IEEE International\nConference and Workshop on the Engineering of Computer-Based Systems (ECBS\n2000). 290\u2013298. https://doi.org/10.1109/ECBS.2000.839888\n[6] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa,\nOlli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien\nBubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023.\nTextbooks Are All You Need. arXiv:2306.11644 [cs.CL]\n[7] Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. 2023. Self-\nplanning Code Generation with Large Language Model. CoRR abs/2303.06689\n(2023). https://doi.org/10.48550/arXiv.2303.06689 arXiv:2303.06689\n[8] Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2023. Towards Enhancing\nIn-Context Learning for Code Generation. CoRR abs/2303.17780 (2023). https:\n//doi.org/10.48550/arXiv.2303.17780 arXiv:2303.17780\n[9] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,\nChenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu,\nEvgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig\nDavaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier,\nNicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,\nBenjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason\nStillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,\nZhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh,\nSasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero,\nTony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan\nEbert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Ander-\nson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry\nBahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf,\nArjun Guha, Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the\nsource be with you! arXiv:2305.06161 [cs.CL]\n[10] Shaoying Liu. 2002. Capturing complete and accurate requirements by refinement.\nIn Eighth IEEE International Conference on Engineering of Complex Computer\nSystems, 2002. Proceedings. 57\u201367. https://doi.org/10.1109/ICECCS.2002.1181498\n[11] WenQian Liu. 2008. A requirements refinement framework. In Proceedings of\nthe 2008 ACM Symposium on Applied Computing (SAC), Fortaleza, Ceara, Brazil,\nMarch 16-20, 2008, Roger L. Wainwright and Hisham Haddad (Eds.). ACM, 658\u2013\n659. https://doi.org/10.1145/1363686.1363844\n[12] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu,\nChongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder: Em-\npowering Code Large Language Models with Evol-Instruct. CoRR abs/2306.08568\n(2023). https://doi.org/10.48550/arXiv.2306.08568 arXiv:2306.08568\nChatCoder: Chat-based Refine Requirement Improves LLMs\u2019 Code Generation\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\n[13] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023).\nhttps:\n//doi.org/10.48550/arXiv.2303.08774 arXiv:2303.08774\n[14] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li,\nand Steven C. H. Hoi. 2023. CodeT5+: Open Code Large Language Models\nfor Code Understanding and Generation. CoRR abs/2305.07922 (2023). https:\n//doi.org/10.48550/arXiv.2305.07922 arXiv:2305.07922\n"
  },
  {
    "title": "AMSP: Super-Scaling LLM Training via Advanced Model States Partitioning",
    "link": "https://arxiv.org/pdf/2311.00257.pdf",
    "upvote": "8",
    "text": "AMSP: Reducing Communication Overhead of\nZeRO for Efficient LLM Training\nQiaoling Chen\u2217\u2020\u2021, Qinghao Hu \u2217\u2020\u2021, Guoteng Wang \u2021, Yingtong Xiong \u2021, Ting Huang \u00a7, Xun Chen \u00a7\nYang Gao \u2021, Hang Yan \u2021, Yonggang Wen \u2020, Tianwei Zhang \u2020, Peng Sun \u2021\u00a7\n\u2217 S-Lab, NTU, \u2020 Nanyang Technological University, \u2021 Shanghai AI Laboratory \u00a7 SenseTime\nAbstract\u2014Training large language models (LLMs) encounters\nchallenges in GPU memory consumption due to the high memory\nrequirements of model states. The widely used Zero Redundancy\nOptimizer (ZeRO) addresses this issue through strategic shard-\ning but introduces communication challenges at scale. To tackle\nthis problem, we propose AMSP, a system designed to optimize\nZeRO for scalable LLM training. AMSP incorporates three flexi-\nble sharding strategies: Full-Replica, Full-Sharding, and Partial-\nSharding, and allows each component within the model states (Pa-\nrameters, Gradients, Optimizer States) to independently choose\na sharding strategy as well as the device mesh. We conduct a\nthorough analysis of communication costs, formulating an op-\ntimization problem to discover the optimal sharding strategy.\nAdditionally, AMSP optimizes distributed LLM training by ef-\nficiently overlapping communication with computation. Evalua-\ntions demonstrate up to 52% Model FLOPs Utilization (MFU)\nwhen training the LLaMA-based model on 1024 GPUs, resulting\nin a 1.56 times improvement in training throughput compared\nto newly proposed systems like MiCS and ZeRO++.\nI. INTRODUCTION\nLarge Language Models (LLMs) have demonstrated excep-\ntional performance in various tasks, with the relationship be-\ntween model size and performance often following a power-\nlaw relationship. Despite the prevailing trend of training giant\nmodels like GPT-3 with 175 billion parameters, recent stud-\nies indicate that optimal performance may be achieved with\nsmaller models trained on larger datasets [1]. Emerging LLMs\nlike LLaMA [2], featuring 7 billion to 30 billion parameters.\nTraining LLMs significantly demands on GPU memory, pri-\nmarily due to the substantial memory consumption of model\nstates, encompassing parameters (P), gradients (G), and op-\ntimizer states (OS). Additional memory is allocated for ac-\ntivations and temporary buffers. For instance, when training\nLLaMA-7B, a substantial 112GB of memory is required for\nmodel states, surpassing the capacity of an 80GB NVIDIA\nA100 GPU. To address this challenge, ZeRO, implemented in\nDeepspeed [3] and PyTorch FSDP [4], introduces a sharding\nstrategy to alleviate redundant memory allocations. ZeRO-\n1 distributes optimizer states across GPUs, ZeRO-2 further\nshards gradients and ZeRO-3 extends this approach to param-\neters, gradients, and optimizer states. This strategic sharding\noptimizes memory usage, enabling efficient training of large\nmodels within GPU constraints. ZeRO could work in cooper-\nation with 3D parallelism [5] and has become widely adopted\nin distributed LLMs training.\nZeRO heavily relies on collective communication for effec-\ntive model states management, introducing challenges in large-\nscale LLM training due to the substantial transmission cost.\nIn our experiments, training a LLaMA-7B model on 8 GPUs\nusing ZeRO-1 achieves a model FLOPs utilization (MFU) of\n63%, but scaling to 1024 GPUs with the same batch size\nresults in a significant performance reduction, with the MFU\ndropping to 36%. The costly communication of ZeRO can be\nattributed to three primary factors: 1) a significant bandwidth\ndiscrepancy between inter-node and intra-node networks, 2)\nan increase in collective communication latency as the com-\nmunication scale grows, and 3) the use of a small micro-batch\nsize per GPU on numerous GPUs, exacerbating the compute-\nto-communication ratio imbalance.\nSeveral approaches have been proposed to reduce the com-\nmunication overhead of ZeRO with improved memory utiliza-\ntion. ZeRO++ [6] achieves this by maintaining a secondary\nparameters shard within small subgroups, effectively reducing\ncommunication latency when collecting them. MiCS [7] shards\nall model states components within subgroups and replicates\nthem across subgroups, reducing communication scale and\nconsequently reducing communication latency, leading to en-\nhanced training performance. Despite these efforts, when scal-\ning LLM training to a large extent, ZeRO++ and MiCS exhibit\nsuboptimal speedup ratios due to two factors. Firstly, the in-\nflexible model states sharding mechanism results in suboptimal\ncommunication costs. This limitation is evident in the case of\nMiCS, where scaling LLaMA-7B training from 8 GPUs to\n1024 GPUs leads to a significant decrease in model training\nperformance, even falling below the efficiency of ZeRO-1.\nSecondly, the inefficiency of the overlap mechanism poses a\nchallenge. For instance, an efficient implementation of MiCS\nwith a streamlined communication-computation overlap can\noutperform DeepSpeed-MiCS by a factor of 2\u00d7 during the\ntraining of LLaMA-13B on 1024 GPUs.\nWe propose AMSP for reducing the communication over-\nhead of ZeRO for training LLMs at scale. To achieve this goal,\nAMSP incorporates three flexible sharding strategies\u2014Full-\nReplica, Full-Sharding, and Partial-Sharding\u2014allowing each\ncomponent within the model states (P, G, OS) to indepen-\ndently choose a sharding strategy. The introduced sharding\nfactors (s0\np \u00d7 s1\np, s0\ng \u00d7 s1\ng, s0\nos \u00d7 s1\nos) control the number of\nGPUs and the device mesh over which the tensors are sharded.\nGiven this flexibility, we analyze the memory consumption\n1\narXiv:2311.00257v2  [cs.DC]  13 Mar 2024\nand communication costs for each sharding dimension. Then,\nwe formulate an optimization problem aimed at discovering\noptimal sharding factors that minimize communication costs\nwhile adhering to the constraint of GPU memory capacity.\nAMSP implements an execution engine tailored for training\nLLMs, incorporating these flexible sharding factors to achieve\noptimized communication efficiency during training.\nAMSP further optimizes distributed LLM training by effi-\nciently overlapping communication with computation. When\nparameters sharding is enabled, AMSP employs a strategy to\nprefetch parameters for the next layer using AllGather dur-\ning the forward pass, while simultaneously performing cur-\nrent layer computations. In the backward pass, AMSP strate-\ngically schedules ReduceScatter operations for gradient\nsynchronization within each parameters sharding subgroup,\navoiding conflicts and ensuring that computations continue\nwithout waiting for communications to finish. Additionally,\nwith activation re-computation, AMSP carefully manages the\nadditional forward computation in the backward pass, retaining\nprefetched parameters for immediate use. These overlapping\nstrategies collectively reduce GPU idle time and significantly\nenhance the training performance of LLMs.\nExtensive evaluations show a significant system perfor-\nmance of AMSP on training LLaMA-based models. On 1024\nNvidia Ampere GPUs, the MFU of AMSP is 51%, 52%, and\n42% on LLaMA-7B, LLaMA-13B, and LLaMA-30B training.\nIn comparison, MiCS demonstrates lower MFU values at 35%,\n33%, and 29% for the same models, ZeRO++ shows the least\nMFU among the three, with MFU rates at merely 4%, 6%, and\n5% for the 7B, 13B, and 30B models, respectively. Compared\nto MiCS and ZeRO++, AMSP improves the training throughput\nby a factor of 1.4 \u2212 12.7 on 1024 GPUs for training LLaMA-\nbased models. AMSP1 has been used for training InternLM [8]\non thousands of GPUs. Our efforts also encompass an exhaus-\ntive study characterizing a six-month development workload\ntrace of LLM collected from our GPU datacenter [9].\nII. BACKGROUND\nWe provide a brief introduction to the essential background\nof LLM training and the associated challenges to improve\nperformance. Table I gives notations used in this work.\nA. LLM Architecture\nLLMs like GPT-3 [10] and LLaMA [2] widely adopt\nthe Transformer [11] architecture with multiple layers. Each\nTransformer layer comprises a list of modules, such as linear,\nmulti-head-attention (MHA), and norm modules. The input\nand output dimensions for each Transformer layer are denoted\nas B \u00d7 S \u00d7 H, where B represents the micro-batch size, S\nindicates the sequence length, and H is the hidden dimension.\nThe relationship between the model size of LLMs and their\nperformance is typically governed by a power-law relationship.\nWhile there has been a trend to train giant models like GPT-\n3 with 175B parameters, existing studies suggest that opti-\nmal model performance may be attained with smaller models\n1Please visit https://github.com/InternLM/InternEvo to access the system.\nTABLE I\nNOTATIONS USED IN THIS WORK.\nNotation\nMeaning\nD\nMemory consumption of a GPU.\nT\nTime consumption.\n\u03a6\nModel parameters count during training.\nN\nTotal number of GPU nodes used for training.\nR\nNumber of GPUs per computational node.\nB\nMicro-batch size (sequences per micro-batch).\nM\nNumber of micro-batches.\nL\nNumber of layers of the model.\nK\nNumber of modules within a layer of the model.\nsdp, stp, spp\nSize of data, tensor and pipeline parallelism.\nsp, sg, sos\nSharding factors of parameters, gradients and model states.\nTABLE II\nPOPULAR LLMS AND THEIR PARAMETERS COUNT.\nModel\n# Parameters\nModel\n# Parameters\nGPT-3\n175B\nBLOOM\n175B\nLLaMA\n7B, 13B, 33B, 65B\nMistral\n7B\nLLaMA2\n7B, 13B, 70B\nInternLM2\n7B, 20B\nCerebras-GPT\n1.3B, 2.7B, 6.7B, 13B\nBaichuan2\n7B, 13B\ntrained on larger datasets [1]. As illustrated in Table II, recently\nintroduced LLMs like LLaMA and InternLM typically feature\n7B to 30B parameters.\nB. Distributed LLM Training\nEfficiently training LLMs at scale in GPU clusters involves\nutilizing 3D parallelism. Data Parallelism (DP) divides input\ndata into chunks, distributing them across GPUs, where each\nGPU independently computes gradients, later synchronized\nthrough AllReduce communication [12]. Tensor Parallelism\n(TP) distributes parameters across GPUs along specific dimen-\nsions for parallel training. Megatron-LM employs TP to parti-\ntion linear layers along the row or column dimension, integrat-\ning collective communication operations for consistent results\n[5]. Pipeline Parallelism (PP) evenly divides a model\u2019s Trans-\nformer layers into multiple stages, distributing them across\nGPUs. A scheduler splits an input batch into micro-batches,\nalternating between forward and backward computations [13]\n[14]. Two consecutive pipeline stages exchange intermediate\ndata through point-to-point communication.\nC. ZeRO\nTraining LLMs results in significant memory consumption,\nlargely due to the occupation of GPU memory by model states,\nwhich comprise tensors containing parameters (P), gradients\n(G), and optimizer states (OS). The remaining memory is allo-\ncated to activations and temporary buffers. In the context of a\nmodel with \u03a6 parameters, employing mixed precision training\nalongside the Adam optimizer [15], it necessitates 2\u03a6, 2\u03a6, and\n12\u03a6 bytes of GPU memory for P, G and OS, respectively. As\nan illustrative example, the LLaMA-7B model requires 112GB\nof memory for its model states, exceeding the memory capac-\nity of an NVIDIA A100 GPU (80GB). As shown in Figure\n2\nFig. 1. Overview of GPU memory allocation for model states with different\nstrategies. ZeRO-1 and ZeRO-3 significantly reduce memory consumption for\nmodel states compared to standard data parallelism. MiCS and ZeRO++ are\nproposed to mitigate communication overhead, particularly cross-node com-\nmunication time, in comparison to the ZeRO approach.\n1, ZeRO reduces redundant memory usage for model training\nby sharding model states [16].\nZeRO-1 splits optimizer states across GPUs (sos >1). In the\ntraining phase, each GPU independently computes gradients\nthrough forward and backward computations, which are then\nsynchronized across sdp GPUs using AllReduce. Each GPU\nupdates specific portions of the model parameters. The most\nrecent model parameters for a GPU are gathered from other\nGPUs using the AllGather operation. ZeRO-2 extends this\napproach by further sharding gradients across GPUs (sg =\nsos > 1). Each GPU retains only the gradients corresponding\nto its optimizer states segment after the reduction operation.\nZeRO-3, also implemented in FSDP [4], employs the shard-\ning strategy on model parameters, gradients, and optimizer\nstates (sp = sg = sos > 1). Before each forward and back-\nward computation, individual GPUs execute the AllGather\noperation to assemble the complete set of model parameters\nand subsequently discard them post-computation. The syn-\nchronization of gradients across GPUs is achieved through\nReduce-Scatter. Each GPU updates its corresponding\nshard of model parameters using the maintained optimizer\nstates and gradients at the end of each step.\nIII. CHALLENGES AND MOTIVATION\nZeRO has gained extensive adoption across various training\nframeworks, such as DeepSpeed [3], FSDP [4], and Colos-\nsalAI [17], owing to its user-friendly interface and scalability\nacross hundreds of GPUs. Despite leveraging high-bandwidth\nRDMA networks, challenges emerge in the form of poor Qual-\nity of Service (QoS) during distributed LLM training on large-\nscale GPU clusters. This can be mainly attributed to significant\ncommunication overhead.\nA. High Communication Overhead\nZeRO necessitates extensive usage of collective communica-\ntion for managing parameters and gradients. The transmission\n8\n16 32 64 128256512\n(a) Model States\n0\n20\n40\nMemory (GB)\nzero1\nzero2\nzero3\n8\n16 32 64 128256512\n(b) Computation\n0\n101\n102\nTime (s)\n8\n16 32 64 128256512\n(c) Communication\n0\n5\n10\nLatency (ms)\nall_reduce\nall_gather\nreduce_scatter\nFig. 2.\nMicro-benchmark of training LLaMA-7B across a scale of GPUs,\nranging from 8 to 512, while maintaining a global batch size of 4M tokens.\nThe micro-batch size B is consistently set to 1 in all tests. Panel (a) illustrates\nthe GPU memory consumption of model states. Panel (b) depicts the time\ntaken for forward and backward computations. Panel (c) presents the latency\nof three communication operations with a fixed message size of 256MB.\ncost across large-scale clusters presents a challenge, as it can-\nnot be easily mitigated through computation-communication\noverlapping. When training a LLaMA-7B model on 8 GPUs\nusing ZeRO-1, the model FLOPs utilization (MFU) attains\n63% in our test-bed. Scaling the training to 1024 GPUs with\nthe same batch size results in a notable performance reduction,\nwith the MFU dropping to 36%. Similarly, scaling LLaMA-\n13B training from 8 GPUs to 1024 GPUs with ZeRO-3 leads\nto a substantial MFU reduction from 47% to 4%.\nThree main factors contribute to the costly communications\nfor large-scale LLM training with ZeRO. Firstly, there exists\na notable discrepancy between inter-node network bandwidth\nand intra-node NVLINK bandwidth. High-performance DGX-\nA100 nodes offer 600GB/s intra-node bidirectional bandwidth\nper GPU and provide 400GB/s inter-node bidirectional band-\nwidth per node. The bandwidth ratio between intra-node and\ninter-node measures at 2 in our test-bed. Secondly, the latency\nof collective communication operations demonstrates a posi-\ntive correlation with communication scale [18] [19] [20] and\nillustrated in Figure 2(c). Figure 3 further illustrates a reduc-\ntion in the effective bandwidth of communication operations\nutilized by ZeRO, scaling from 8 GPUs to 512 GPUs. Thirdly,\nthe global batch size limitation for convergence efficiency im-\nposes the use of a very small batch size per GPU when training\non numerous GPUs. As depicted in 2 (b), the computation\ntime of the LLaMA-7B model training linearly reduces from\n8 GPUs to 512 GPUs while maintaining a consistent 4M\nbatch size. This reduction adversely affects the compute-to-\ncommunication ratio, leading to a communication bottleneck.\nB. Trade-off between Communication and Memory\nA trade-off exists between memory utilization and commu-\nnication cost in distributed LLM training. Initially, the com-\nmunication cost can be effectively reduced by diminishing the\ncommunication scale. This involves limiting communications\nto a smaller group of GPUs, potentially within the same node,\nwhich mitigates the overall communication cost. In addition,\nas depicted in Figure 2 (a), scaling ZeRO to a large scale does\nnot yield substantial memory savings compared to a smaller\nsize. Consequently, various approaches have been proposed to\nreduce communication overhead with higher memory usage.\nZeRO++ [6] keeps a secondary shard of parameters while\nsharding other model states across the cluster (sp =sg =sos =\n3\n1M 64M 256M 1G\n(a) AllGather\n0\n500\n1000\n1500\nEffective BW (Gb/s)\n1M 64M 256M 1G\n(b) ReduceScatter\n1M 64M 256M 1G\n(c) AllReduce\n1M 64M 256M 1G\n(d) Broadcast\n8x1 GPUs\n8x16 GPUs\n8x2 GPUs\n8x32 GPUs\n8x4 GPUs\n8x64 GPUs\n8x8 GPUs\nFig. 3. Performance evaluation of collective communication operations using\nNCCL. The assessment is conducted with varying message sizes (in bytes).\nGPU nodes are linked using 4 Mellanox Infiniband HDR NICs (800 Gbps\nbandwidth in total). The notation 8\u00d7A GPUs indicates that the tests were\nconducted on A nodes, with each node housing 8 NVIDIA Ampere GPUs\n(A800) connected by NVLINK.\nsdp), as shown in Figure 1. In the forward phase, it collects\nparameters across all GPUs and maintains a secondary shard\nof parameters within a small subgroup of GPUs, potentially\nwithin the same node. During the backward phase, it collects\nparameters from this secondary shard. Additionally, ZeRO++\nuses quantization to compress parameters and gradients, effec-\ntively reducing inter-node communication size. Note that we\nwould not enable configurations related to the quantization of\nZeRO++ to ensure consistent model quality.\nMiCS [7] and FSDP [4] facilitate the sharding of model\nstates within a subgroup and replicate them across subgroups\n(sp =sg =sos <sdp), as shown in Figure 1. These approaches\nemploy AllGather to collect parameters within a subgroup\nfor both forward and backward computation and synchronize\ngradients across the cluster using ReduceScatter. Conse-\nquently, MiCS and FSDP contribute to improved training per-\nformance by effectively reducing the communication scale. It\nis crucial to configure an appropriate subgroup size to prevent\nOut-Of-Memory (OOM) errors.\nC. Motivation\nDespite efforts to reduce communication costs, ZeRO++\nand MiCS still exhibit poor speedup ratios when scaling LLM\ntraining to a large scale. This is attributed to their inflexible\nmodel states sharding mechanism, requiring sp = sg = sos \u2264\nsdp in all cases. Such a configuration may not be optimal for\ntraining LLMs with diverse model sizes and hyper-parameters.\nIn addition, the inefficiency of the overlap mechanism in\nZeRO++ and MiCS also poses a challenge. For instance, when\nscaling LLaMA-7B training from 8 GPUs to 1024 GPUs with\nMiCS, MFU decreases from 50% to 35% in our test-bed.\nIn this scenario, MiCS even exhibits lower performance than\nZeRO-1, highlighting the drawbacks of the inflexible model\nstates sharding mechanism.\nIn this study, the three components of model states (P, G,\nOS) are sharded into independent subgroups and replicated\nacross these subgroups, following the condition sp \u2264sdp, sg \u2264\nsdp, sos \u2264sdp. This flexibility allows us to fine-tune the trade-\noff between communication and GPU memory by configuring\nsp, sg, sos. By doing so, we may achieve minimal communica-\ntion cost for distributed LLM training through individualized\nconfiguration of the communication scale on P, G, and OS,\nwhile respecting GPU memory constraints.\nTaking LLaMA-7B as an illustrative example, we adopt the\nconfiguration of sp = sg = 1, sos = 8 for training. In this\nsetting, each GPU retains a complete copy of parameters and\ngradients, while each node stores a full copy of optimiza-\ntion states. During training, gradients are synchronized across\nclusters using AllReduce, and each GPU obtains the latest\nparameters within the same node through AllGather at the\nend of each step. In our test-bed, scaling LLaMA-7B training\nfrom 8 GPUs to 1024 GPUs with this configuration results in\nan acceptable MFU reduction from 64% to 51%.\nIV. MODEL STATES SHARDING AND ANALYSIS\nIn this section, we assume that there is no tensor parallelism\nor pipeline parallelism during the training, which implies that\nstp = spp = 1. This simplification allows us to focus on the\nimpact of the discussed sharding strategies on communication\nand memory aspects.\nA. Performance Model of Collective Communication\nThe \u03b1 \u2212 \u03b2 cost model [21] is widely employed to char-\nacterize the performance of collective communication [22].\nTaking the example of a ring-based AllReduce on p GPUs,\nwhere the input size is v, and the physical bandwidth between\ntwo GPUs is w, the input is evenly split into p chunks. In\nthe first stage, each chunk undergoes p \u2212 1 rounds of reduc-\ntion to each GPU, constituting a ReduceScatter operation\nwith a time complexity of trs = (p \u2212 1)(\u03b1 +\nv\nw\u00d7p), where\n\u03b1 denotes the latency per transmission. Then, each reduced\nchunk at every GPU is broadcast to other GPUs, constituting\nan AllGather operation with the same time complexity as\nReduceScatter. The overall time complexity of the ring-\nbased AllReduce is given by tar = 2(p \u2212 1)(\u03b1 +\nv\nw\u00d7p).\nHowever, predicting collective communication time with\nhigh accuracy using the \u03b1 \u2212 \u03b2 cost model is challenging\nin certain scenarios. First, in addition to the ring algorithm,\nNCCL introduces new communication algorithms like Tree\n[23], Collnet, CollnetDirect, and CollnetChain. Consequently,\na single cost model struggles to formulate the communication\ntime for all these algorithms. Second, In-Network Aggregation\nsolutions are widely implemented in production GPU clusters.\nThese solutions offload AllReduce onto network switches\nto accelerate and scale distributed training [24] [25] [26].\nIn this work, we adopt a straightforward yet effective\nprofiling-based approach to model the performance of collec-\ntive communication. Specifically, we utilize\nt(o, v, p0 \u00d7 p1) = v/w(o, v, p0 \u00d7 p1)\nto evaluate the time consumption of a collective communi-\ncation operator (o) with a given data size (v) and a spec-\nified participant GPU device mesh (p0 \u00d7 p1, where p0 de-\nnotes the number of GPUs in a node, and p1 is the num-\nber of nodes). Here, w(o, v, p0 \u00d7 p1) represents the effective\nbandwidth obtained through performance profiling on the tar-\nget GPU cluster in advance, as illustrated in Figure 3. In\n4\nFig. 4. Optimizing model states sharding through the dependency rule. In this instance, when sp = sg = 2, there\u2019s no need to set sos = 1 as it would store\nredundant optimized states, incurring additional communication costs.\ncases where v is not profiled, we employ the interpolation\nmethod to predict the effective bandwidth and communica-\ntion time. This work focuses on four key collective commu-\nnication operations: AllReduce(AR), AllGather(AG),\nReduceScatter(RS) and Broadcast(BC).\nB. Flexible Model States Sharding with Dependency Rule\nWe adopt three sharding strategies, namely Full-Replica,\nFull-Sharding, and Partial-Sharding, and provide the flexibil-\nity for each of the three components within the model states\n(P, G, and OS) to independently select a sharding strategy.\nTo encapsulate these strategies, we introduce sharding factors\nsp = s0\np \u00d7 s1\np, sg = s0\ng \u00d7 s1\ng and sos = s0\nos \u00d7 s1\nos, representing\nthe number of GPUs and the device mesh over which the\ntensors of P, G, and OS are sharded, respectively. Setting the\nfactor to 1 implies full replication of the tensor, simplifying\nto vanilla data parallelism if all components (P, G, and OS)\nchoose the Full-Replica strategy. Conversely, setting the factor\nequal to the DP size results in complete tensor sharding, with\neach GPU holding 1/sdp of the tensor. For instance, in ZeRO-\n3, all components (P, G, and OS) choose the Full-Sharding\nstrategy. Partial-Sharding emerges when the factor falls be-\ntween 1 and sdp, indicating tensor sharding across a subgroup\nof GPUs and replication across subgroups.\nA dependency rule is crucial when flexibly sharding the\nmodel states to avoid unnecessary data movement and storage.\nThroughout the training, the framework employs local param-\neters for gradient computation and synchronized gradients for\nupdating local optimizer states. If a GPU oversees extra gra-\ndients or optimizer states unrelated to its local parameters,\nlaunching additional communication becomes necessary. This\nincurs significant and avoidable expenses. Figure 4 illustrates\nan instance of this scenario, highlighting the impact when\nsetting sp = sg = 2, sos = 1. Before independently sharding\nP, G, and OS, we establish the following constraints:\nR \u2265 s0\ndp \u2265 s0\nos \u2265 s0\ng \u2265 s0\np,\nN \u2265 s1\ndp \u2265 s1\nos \u2265 s1\ng \u2265 s1\np,\nwhere s0\ndp and s1\ndp is the device mesh of DP ranks, R denotes\nthe GPU count per node, and N is the node number. As shown\nin Figure 4, adhering to the dependency avoids unnecessary\ndata movement and storage.\nC. Communication Time Analysis\nIn this subsection, we analyze the communication cost asso-\nciated with individually partitioning parameters, gradients, and\noptimizer states. Figure 5 provides an overview of the inserted\ncollective communication operations for each component.\n1) Parameters Sharding: When s0\np \u00d7 s1\np = sp > 1, the \u03a6\nparameters of a model are split into sp shards, with each GPU\nmanaging one shard. As shown in Figure 5(a), during each\nforward and backward pass of every micro-batch in a step,\nthe training system orchestrates the collection of parameters\nshards from other GPUs to reconstruct the complete set of\nmodel weights required for computations. This is achieved\nusing AllGather on sp GPUs. In each micro-batch of a\nstep, after the gradients are generated during the backward\nphase, the training framework launches ReduceScatter to\naggregate and distribute gradients across sp GPUs. The train-\ning framework performs AllGather and ReduceScatter\nat the granularity of a module within a Transformer layer. The\ninput size for i-th module of a layer is 2\u03a6i (using FP16). The\ncommunication time attributable to parameters sharding for M\nmicro-batches of a step is given by:\nTp = ML\nK\nX\ni=0\n\u00002t(AG, 2\u03a6i, s0\np \u00d7 s1\np) + t(RS, 2\u03a6i, s0\np \u00d7 s1\np)\n\u0001\n,\nwhere L denotes the number of layers and K is the number\nof modules of a layer.\nTaking Figure 4 (a) as an example, when s0\np \u00d7 s1\np = 2 \u00d7 1,\nAllGather and ReduceScatter are executed within the\nsame node. parameters sharding allows for overlapped com-\nmunication with computation. During the forward or backward\ncomputation of a module, it is feasible to execute AllGather\nand ReduceScatter for the subsequent module.\n2) Optimizer States Sharding: When s0\nos \u00d7 s1\nos = sos > 1,\na total of sos GPUs collectively possess a complete duplicate\nof optimizer states. Following parameters sharding with sp,\neach parameters is stored and replicated across sdp/sp GPUs.\nIn this configuration, optimizer states may exhibit redundancy,\nwith sdp/sp replicas distributed across the cluster. To reduce\nthis redundancy, we introduce a solution by allowing sos > sp,\naffording flexibility to reduce redundancy. In this scenario, the\noptimizer states for \u03a6/sp parameters are shared by sos/sp\n5\nFig. 5. Analysis of inserted collective communication operations when individually sharding parameters, gradients, and optimizer states.\nFig. 6. Sharding scheme for optimizer states. (a) shards each optimizer states\ntensor into multiple devices along with the corresponding optimizer states.\n(b) distributes each tensor of optimizer states in its entirety.\nGPUs, forming an optimizer states sharding subgroup. Illus-\ntrated in Figure 4 (b), GPU-0 and GPU-2 share common pa-\nrameters shards but maintain distinct optimizer states shards,\nforming an optimizer states sharding subgroup.\nAfter the backward pass of the last micro-batch in a step,\neach GPU updates \u03a6/sos parameters based on the optimizer\nstates. Before the update phase, each GPU should gather and\naggregate gradients for parameters within its optimizer states.\nTo optimize this process, we employ AllReduce on gradi-\nents across sdp/sp GPUs sharing the same set of parameters\n(in the amount of \u03a6/sp), as illustrated in Figure 5 (c). In\nFigure 4 (b), we execute AllReduce on GPU-0/2/4/6. Given\nthat sos > sp, each GPU receives additional gradients not\nmanaged by its optimizer states. To resolve this issue, we\nemploy a select & drop mechanism, enabling each GPU to\nexclusively select the necessary gradients from the output of\nthe AllReduce.\nFollowing the completion of parameters updates, it is es-\nsential to spread updated parameters among GPUs within the\nsame optimizer states sharding subgroup. For example, In\nFigure 4 (b), GPU-0 should send its updated parameters to\nGPU-2. The choice of the collective communication primitive\nrelies on the sharding scheme employed for optimizer states.\nTypically, two mechanisms govern the sharding of optimizer\nstates across GPUs, as illustrated in Figure 6. 1) The intra-\ntensor approach involves evenly splitting a single parameters\nalong with its states into multiple shards, which are then\ndistributed to different GPUs. In this scenario, AllGather\nproves effective, ensuring that each GPU receives all updated\nvalues for parameters stored in its local memory. 2) The inter-\ntensor approach distributes each parameters and its states as\na whole across devices, using a greedy algorithm to balance\nGPU memory usage. In this case, direct usage of AllGather\nis not feasible, as each GPU may not have an identical number\nof updated parameters. To address this, the updated parameters\ncan be spread by broadcasting each shard separately using an\nNCCL group call.\nWhile both sharding schemes remain compatible with mix-\nprecision training using FP16, the inter-tensor approach is\nrecommended for FP8 training [27]. This preference is pivotal\nsince the distribution of per-tensor scaling factors becomes\nimperative when dealing with FP8 shards. Consequently, we\nadopt the inter-tensor approach in this work. Following the\noptimizer update stage, a series of Broadcast operations are\ninitiated on sos/sp GPUs to disseminate updated parameters.\nThe training system launches 2\u03a6/U AllReduce opera-\ntions with a specified bucket size U to synchronize gradi-\nents for a model with \u03a6 trainable parameters (utilizing FP16).\nThe AllReduce communication time attributed to optimizer\nstates sharding for a given step is expressed as:\nT 0\nos = 2\u03a6\nUsp\n \nt(AR, U,\ns0\ndp\ns0p\n\u00d7\ns1\ndp\ns1p\n)\n!\n.\nFor disseminating updated parameters, the training system uti-\nlizes a group of Broadcast operations. Each Broadcast\noperation processes an input of size 2\u03a6/sos on average, and\nthe system executes sos/sp such operations. The Broadcast\ncommunication time attributable to optimizer states sharding\nduring a step is given by:\nT 1\nos = sos\nsp\n\u0012\nt(BC, 2\u03a6\nsos\n, s0\nos\ns0p\n\u00d7 s1\nos\ns1p\n)\n\u0013\n.\noptimizer states sharding facilitates the potential for over-\nlapped communication and computation. During the backward\ncomputation of the i-th layer, we can perform AllReduce\non gradients generated on layer i + 1. Simultaneously, during\nthe forward computation of the i-th layer, it is also possible to\nbroadcast the latest parameters (updated in the previous step)\nfor the next layer.\n3) Gradients Sharding: When s0\ng \u00d7 s1\ng = sg > 1, a total\nof sg GPUs collectively hold a complete copy of gradients\ngenerated at each micro-batch of every step. As depicted in\nFigure 4, if sg = sp, each GPU retains \u03a6/sp gradients, ac-\ncumulating them at every micro-batch based on the parameter\nsharding mechanism. In this work, we introduce the flexibility\n6\nto set sg > sp to conserve GPU memory. For simplicity, we\nimpose the following constraints on the selection of sg:\nsg \u2208 {sp, sos}.\nIn the scenario where sg > sp, each GPU initiates an\nAllReduce operation on sg/sp GPUs to aggregate and dis-\ntribute gradients in every micro-batch, excluding the last one.\nFollowing the aggregation, each GPU retains only the gradi-\nents allocated to it, discarding the surplus. For instance, in\nFigure 4 (c), GPU-0 and GPU-2 can employ such a select\n& drop mechanism to shard gradients. It is noteworthy that\nalternative approaches might leverage ReduceScatter to\nachieve similar outcomes. However, the inter-tensor approach\nin sharding optimizer states leads to uneven shard sizes per\nGPU, making ReduceScatter less suitable. Thus, we opt\nfor AllReduce on gradients, preserving only the relevant\nones. Assuming AllReduce is executed with bucket size U,\nthe communication time attributable to gradients sharding of\na step can be expressed as:\nTg = (M \u2212 1) 2\u03a6\nUsp\n \nt(AR, U, s0\ng\ns0p\n\u00d7 s1\ng\ns1p\n)\n!\n.\nGradients sharding can also overlap communication with com-\nputation. During the backward computation for i-th layer, we\ncan concurrently execute AllReduce for (i + 1)-th layer.\n4) Summary: Based on the aforementioned analysis, we\ncan conclude that the single-step communication time of dis-\ntributed LLM training with a flexible model states sharding\nstrategy is the sum of three components:\nTcomm(s0\np, s1\np, s0\ng, s1\ng, s0\nos, s1\nos) = Tp + Tg + T 0\nos + T 1\nos.\nD. GPU Memory Consumption Analysis\nIn the context of mixed-precision training with the Adam\noptimizer, the GPU memory consumed by model states during\ntraining can be expressed as the sum of memory allocated for\nsharded parameters, gradients, and optimization states:\nDmodelstate(s0\np, s1\np, s0\ng, s1\ng, s0\nos, s1\nos) = 2\u03a6\ns0ps1p\n+ 2\u03a6\ns0gs1g\n+ 12\u03a6\ns0oss1os\n.\nAdditionally, the total GPU memory consumption, Dtotal, can\nbe encapsulated by:\nDtotal = Dmodelstate + Dactivation + Dtmp,\nwhere Dactivation is the memory consumed by activations dur-\ning training, and Dtmp denotes the temporary memory used\nby communication buffers or other transient variables. Existing\nmethodologies [5] [16] for analyzing and predicting activation\nmemory usage are seamlessly integrated into our present study.\nV. SYSTEM DESIGN & COMMUNICATION OVERLAP\nTo reduce the communication overhead of ZeRO for effi-\ncient LLM training, we introduce AMSP. It leverages an ex-\npanded model states sharding space and is adept at identifying\nthe most communication-efficient factors. We focus on how\nAMSP systematically optimizes the training performance with\na flexible model states sharding strategy.\nFig. 7. Overview of AMSP architecture and workflow. The Planner identifies\nthe optimal solution for model states sharding. The Executor executes LLM\ntraining using the selected strategy, and enhances communication performance\nthrough overlap and placement optimization.\nA. System Architecture\nFigure 7 illustrates the two components of AMSP: the Plan-\nner and the Executor. (1) The Planner identifies the optimal\nsolution for model states sharding. This component integrates\nthree modules: the Pre-Filter, narrowing the search space based\non specific rules; the Communication-Profiler, offering pre-\ndictions for collective communication time; and the Solver,\nconstructed by a memory and communication cost model to\nidentify the optimal strategy. (2) The Executor is accountable\nfor executing LLM training using the selected strategy. This\ncomponent integrates two essential modules to enhance com-\nmunication efficiency. The Communication Overlap strategy\noffers a fine-grained overlap for computation and communica-\ntion. Moreover, AMSP employs the topology-aware Commu-\nnication Placement strategy to reduce network communication\nacross spine switches, enhancing overall efficiency.\nWorkflow. 1 AMSP begins by having users define LLM\narchitecture, specifying metadata such as layer number and\nsequence length, along with hyper-parameters like micro-batch\nsize and micro-batch number. Users also provide settings for\nthe training cluster, including the total number of GPUs and\nGPU memory capacity.\n2\nThe Planner eliminates certain\nstrategies that may incur additional communication costs, re-\nsulting in a set of alternative strategies. The Communication-\nProfiler, operating offline, provides communication time data,\naiding the Planner in estimating step time for these alterna-\ntives. 3 Using an optimization problem solver, the Planner\nidentifies the optimal strategy. 4 Subsequently, the Executor\nruns the training job using the chosen strategy, enhanced by\nCommunication Overlap and topology-aware Communication\nPlacement strategy.\nAMSP utilizes real-system profiling to ascertain the effec-\ntive bandwidth of three used collective communication oper-\nations (i.e., AllGather, ReduceScatter, AllReduce\nand Broadcast) across diverse communication sizes and\ndevice meshes. Consequently, AMSP estimates the communi-\ncation latency induced by model states sharding.\n7\nFig. 8.\nExample of sharding model states within the same node. In (a)\nassigning s0\np = 1, s1\np = 2 results in increased cross-node communication\ncaused by AllGather and ReduceScatter. In (b) with sos = 2, setting\ns0\nos = 1, s1\nos = 2 creates cross-node Broadcast.\nB. Execution Planner\nThe Execution Planner generates the optimal combination\nstrategy for the input model with the provided hardware infor-\nmation. AMSP formulates an optimization problem to search\nfor the optimal {s0\np, s1\np, s0\ng, s1\ng, s0\nos, s1\nos} by minimizing the sum\nof communication costs subject to memory constraints. The\ninteger programming problem is defined as follows:\nMinimize\nTcomm(s0\np, s1\np, s0\ng, s1\ng, s0\nos, s1\nos)\n(1)\nSubject to\nDtotal \u2264 GPU_Memory_Capacity\n(2)\n1 \u2264 s0\np \u2264 s0\ng \u2264 s0\nos \u2264 s0\ndp \u2264 R\n(3)\n1 \u2264 s1\np \u2264 s1\ng \u2264 s1\nos \u2264 s1\ndp \u2264 N\n(4)\ns0\ni \u00d7 k = s0\ndp, k \u2208 Z,\ni \u2208 {p, g, os}\n(5)\ns1\nj \u00d7 k = s1\ndp, k \u2208 Z,\nj \u2208 {p, g, os}\n(6)\ns0\ni = s0\ndp,\nif s1\ni > 1,\ni \u2208 {p, g, os}\n(7)\nThis problem minimizes the communication cost of LLM\ntraining with respect to the GPU memory capacity (Equa-\ntion 2) and the dependency rules outlined in Section IV-B\n(Equation 3, 4). Instead of exhaustively iterating through all\npossible solutions for {s0\np, s1\np, s0\ng, s1\ng, s0\nos, s1\nos}, we optimize the\nefficiency of assignment strategy exploration by incorporating\ntwo filters. Firstly, s0\ndp should be divisible by s0\np,g,os (Equation\n5), ensuring the participation of all GPUs within a node in\nthe training process. Additionally, s1\ndp should be divisible by\ns1\np,g,os (Equation 6), allowing for the utilization of all nodes\nin the training. Secondly, to minimize cross-node communi-\ncation (Equation 7), a priority is placed on employing fewer\nnodes when sharding P, G, and OS independently. For in-\nstance, in Figure 8(a), selecting (s0\np = 1, s1\np = 2) necessitates\nlaunching AllGather and ReduceScatter on two nodes\nFig. 9. Overlapping strategy for parameters sharding and corresponding con-\ntrol hooks. (a) illustrates the concurrent execution of AllGather for pre-\nfetching parameters with the forward computation. (b) presents the overlap of\nAllGather and ReduceScatter with backward computation. (c) demon-\nstrates the communiction-computation overlap strategy during the backward\nphase when activation recomputation is enabled.\nevery micro-batch, while opting for (s0\np = 2, s1\np = 1) results\nin reduced cross-node communication costs. In Figure 8(b),\nsetting (s0\nos = 1, s1\nos = 2) induces cross-node AllGather\nfor spreading updated parameters per step, whereas (s0\nos =\n2, s1\nos = 1) confines this communication within a node. Based\non these filters, AMSP can efficiently employ a brute-force\nsearch method to obtain the optimal solution, effectively nav-\nigating the solution space with reduced complexity.\nC. Computation-Communication Overlap\nAMSP uses specific hooks of PyTorch 2.1, as detailed in\nTable III, to facilitate the necessary NCCL communications for\nsharding P, G, and OS. Timely initiation of these operations\nis important for ensuring both correctness and efficiency.\nTABLE III\nHOOKS USED BY AMSP\nModule\nHook Name\ntorch.nn.modules\nregister_forward_hook\ntorch.nn.modules\nregister_forward_pre_hook\ntorch.nn.modules\nregister_full_backward_hook\ntorch.nn.modules\nregister_full_backward_pre_hook\ntorch.Tensor\nregister_hook\n1) Overlap for Parameters Sharding: As shown in Figure\n9 (a), prior to initiating the forward computation in a layer, a\nsequence of AllGather operations is launched to proactively\nfetch parameters for each module (such as linear modules) of\nthe subsequent layer. This strategic prefetching enables AMSP\nto seamlessly overlap the computation of the current layer with\ncommunication tasks for the next layer, enhancing overall ef-\nficiency. To coordinate this process for each module, AMSP\nleverages register_forward_pre_hook to await the\n8\ncompletion of the corresponding AllGather. Upon conclud-\ning the forward computation for a module, AMSP releases gath-\nered parameters, triggered by register_forward_hook.\nIn the backward pass, each module within a layer com-\nputes gradients for both its weights (GradWeight) and its\ninput (GradInput). After the computation of GradWeight,\nAMSP initiates ReduceScatter on it for gradients distri-\nbution and synchronization. In scenarios without activation\nrecomputation, launching all AllGather operations simulta-\nneously to fetch parameters of modules in the next layer is not\noptimal. This is because such operations would obstruct the\nexecution of ReduceScatter, leading to sub-optimal train-\ning performance. To address this challenge, we adopt a more\nstrategic approach by fetching parameters of only the next\nmodule, triggered by register_backward_pre_hook.\nConsequently, as shown in Figure 9 (b), AMSP efficiently over-\nlaps AllGather with the computation of GradWeight.\nAMSP also overlaps ReduceScatter with the computa-\ntion of GradInput. Furthermore, we decouple the life-cycle\nof ReduceScatter from the backward function. In cases\nwhere the ReduceScatter operation is not completed by\nthe time GradInput computation concludes, AMSP seam-\nlessly proceeds to the backward computation of the next mod-\nule instead of causing unnecessary blocking.\nAs we decouple the life-cycle of ReduceScatter from\nthe backward function, the completion of the backward func-\ntion does not guarantee the availability of the true gradients for\nthe corresponding weights. Therefore, an additional post-hook\nis applied to the AccumulateGrad of each parameter, ensuring\nthe completion of the corresponding ReduceScatter oper-\nation before utilizing these gradients for subsequent commu-\nnication or computation tasks. Examples of such tasks include\nlaunching AllReduce operations on this data, facilitating\ngradient synchronization across data parallelism ranks.\nIn Figure 9 (c), with the activation recomputation mecha-\nnism enabled, AMSP requires an additional forward computa-\ntion for a layer during the backward pass. At the beginning of\nthis secondary forward pass for a layer, AMSP initiates a se-\nries of AllGather operations to proactively fetch parameters\nfor the subsequent layer. Importantly, in contrast to the first\nforward phase, AMSP retains the gathered parameters after the\nsecondary forward computation for subsequent gradients com-\nputation. Following the computation of GradWeight, AMSP\nproceeds to initiate a ReduceScatter operation on it. AMSP\nefficiently overlaps both AllGather and ReduceScatter\noperations with computation during the backward phase.\n2) Overlap for Optimizer States Sharding: During the back-\nward phase of the last micro-batch, each GPU aggregates\ngradients for parameters within its optimizer states by using\nAllReduce across data parallelism ranks. To optimize this\nprocess, we implement a bucketing strategy where a parameter\nis placed in a bucket after its backward computation [12]. Once\na bucket is full, all gradients of the parameters within it are flat-\ntened into a contiguous buffer. Then, we execute AllReduce\non this buffer without blocking the backward computation of\nthe remaining layers. This approach is triggered by hooks on\nFig. 10. Examples of communication placement. When s1\np,g,os > 1, (a) de-\npicts an instance where groups of s1\np,g,os nodes are organized under the same\nspine switch, facilitating intra-switch communication, which is the preferred\napproach by AMSP. (b) showcases a scenario in which the groups of s1\np,g,os\nnodes are distributed across different spine switches, necessitating extensive\ncross-switch communication.\nthe AccumulateGrad of parameters.\nWe utilize the asynchronous communication mechanism of\nNCCL to overlap parameters broadcast with the forward com-\nputation of the next step. It is crucial to ensure that all param-\neters have updated values before being used for both compu-\ntation and communication. We implement two optimizations\nto address this challenge. Firstly, we register a hook for each\nmodule by using register_forward_pre_hook, ensur-\ning that the parameters to be used are the most recent ones be-\nfore any computation or communication takes place. Secondly,\nwe manage the order of forward computation to align with\nthe sequence of parameters broadcast. This synchronization\nensures that the parameters used in the forward computation\nare the ones that have been successfully broadcasted, avoiding\nunnecessary blocking during the forward phase.\n3) Overlap for Gradients Sharding: In the case of sg > sp,\nthe system initiates AllReduce operations on sg/sp GPUs\nfor gradients aggregation and distribution in each micro-batch\nexcluding the final one. To further enhance efficiency, a buck-\neting strategy, similar to the one employed for optimizer states\nsharding, is leveraged. The AllReduce communication pro-\ncess seamlessly overlaps with the backward computation, fa-\ncilitated through hooks integrated into the AccumulateGrad\nof parameters. Following the AllReduce, a GPU rank only\nretains gradients pertinent to it, releasing other gradients to\nconserve GPU memory.\nD. Communication Placement\nTo further improve training performance, AMSP tries to min-\nimize communication traffic across spine-switches within the\nleaf-spine network architecture, which is commonly used in\ncurrent GPU data centers. Typically, GPU nodes are connected\nto leaf-switches, and these leaf-switches are interconnected\nby spine-switches. When GPUs are not under the same leaf\nswitch, communication has to go through spine switches, lead-\ning to increased latency and potential network congestion. In\nthis study, when s1\np,g,os > 1, AMSP aims to optimize collective\ncommunications by utilizing fewer leaf switches as illustrated\nin Figure 10 (a), instead of incurring extensive cross-switch\n9\n8\n32\n128\n512\n1024\n(a) LLaMA-7B\n0\n0.2\n0.4\n0.6\n0.8\nMFU\n8\n32\n128\n512\n1024\n(b) LLaMA-13B\n0\n0.2\n0.4\n0.6\n0.8\nMFU\n8\n32\n128\n512\n1024\n(c) LLaMA-30B\n0\n0.2\n0.4\n0.6\n0.8\nMFU\noom\noom\nZeRO-1\nZeRO-3\nZeRO++\nMiCS\nOur Work\nFig. 11. End-to-end evaluation results (MFU) of training LLaMA-based Mod-\nels from 8 GPUs to 1024 GPUs.\nTABLE IV\nSTRATEGIES USED FOR SHARDING P , G AND OS.\nModel\nApproach1\ns0\np\ns1\np\ns0\ng\ns1\ng\ns0\nos\ns1\nos\nLLaMA-7B\nOur Work\n1\n1\n1\n1\n8\n1\nZeRO-1\n1\n1\n1\n1\nR\nN\nZeRO-3\nR\nN\nR\nN\nR\nN\nMiCS\n8\n1\n8\n1\n8\n1\nZeRO++2\nR\nN\nR\nN\nR\nN\nLLaMA-13B\nOur Work\n4\n1\n4\n1\n8\n1\nZeRO-3\nR\nN\nR\nN\nR\nN\nMiCS\n8\n1\n8\n1\n8\n1\nZeRO++2\nR\nN\nR\nN\nR\nN\nLLaMA-30B\nOur Work\n8\n1\n8\n1\n8\n4\nZeRO-3\nR\nN\nR\nN\nR\nN\nMiCS\n8\n2\n8\n2\n8\n2\nZeRO++2\nR\nN\nR\nN\nR\nN\n1 We set s0\ndp = R, s1\ndp = N in this experiment.\n2 ZeRO++ uses s0\np = 8, s1\np = 1 to shard secondary parameters.\ncommunication as shown in Figure 10 (b). For instance, with\n(s0\np = 8, s1\np = 4), AMSP strategically groups four nodes under\nthe same leaf switches to perform collective communications\nlike AllGather and ReduceScatter. This strategic ap-\nproach aims to minimize inter-switch communication latency,\nthereby reducing the additional communication overhead in-\ntroduced by the sharding of model states.\nVI. EVALUATION\nA. Experimental Setup\n1) Implementation: We use an iterative solver to dynam-\nically optimize communication costs based on the provided\nconfiguration. To uphold comparable computational perfor-\nmance, AMSP incorporates FlashAttention-v2 [28] and adopts\nmixed-precision training with BF16, aligning with baseline\n8\n32\n128\n512\n1024\n(a) LLaMA-7B\n0\n1000\n2000\n3000\n4000\n5000\nTGS\n8\n32\n128\n512\n1024\n(b) LLaMA-13B\n0\n500\n1000\n1500\n2000\n2500\nTGS\n8\n32\n128\n512\n1024\n(c) LLaMA-30B\n0\n400\n800\n1200\nTGS\noom\noom\nZeRO-1\nZeRO-3\nZeRO++\nMiCS\nOur Work\nFig. 12. End-to-end evaluation results (TGS) of training LLaMA-based Mod-\nels from 8 GPUs to 1024 GPUs.\nsystems. We also introduce a user-friendly interface enabling\nusers to customize the sharding of P, G, and OS through\npredefined configurations or leverage the integrated solver to\nautomatically determine the optimal sharding strategy.\n2) Testbed: We evaluate the training performance of three\npopular LLMs: LLaMA-7B, LLaMA-13B, and LLaMA-30B.\nThe training is conducted on a dedicated cluster with 128 GPU\nservers. Each server is equipped with 8 GPUs and 128 CPU\ncores, resulting in a total of 1024 NVIDIA Ampere GPUs\n(A800). Each GPU is outfitted with 80GB of memory, in-\nterconnected through NVLink within a node, and inter-node\ncommunication is facilitated by 4 Mellanox HDR InfiniBand\nwithout SHARP.\n3) Baselines & Evaluation Metrics: We conduct a compre-\nhensive benchmark of AMSP, comparing it against DeepSpeed-\nZeRO1, DeepSpeed-ZeRO3 [16], DeepSpeed-ZeRO++ [6],\nand DeepSpeed-MiCS [7]. Our evaluation focuses on key per-\nformance metrics, including Model FLOPs Utilization (MFU)2\n[29] and Tokens per GPU per Second (TGS). The sequence\nlength is held constant at 4096 tokens in all experiments. The\nsequence length is fixed at 4096 tokens. Micro-batch size is\nconfigured to 1 sequence with 4096 tokens, while the global-\nbatch size is set to 4 million tokens. The micro-batch number\nis 128 with 8 GPUs (i.e. M = 128); however, training with\n1024 GPUs reduces the micro-batch number to 1 (i.e. M = 1).\nSince the core objective of AMSP is reducing communication\noverhead in ZeRO, we adopt s0\ndp = R, s1\ndp = N across all\nexperiments, excluding tensor or pipeline parallelism.\n2We calculate FLOPs and MFU using the formula in Megatron-LM. As de-\ntailed in [28], while the FLOPs due to attention should be halved, with causal\nmask, only approximately half the number of elements in attention needs\ncomputation. For consistency, we adhere to the literature formula (without\ndividing attention FLOPs by 2) as in FlashAttention and many other libraries.\n10\n8\n32\n128\n512\n1024\n(a) LLaMA-7B\n0\n20\n40\n60\n80\nMemory(GB)\n8\n32\n128\n512\n1024\n(b) LLaMA-13B\n0\n20\n40\n60\n80\nMemory(GB)\n8\n32\n128\n512\n1024\n(c) LLaMA-30B\n0\n20\n40\n60\n80\nMemory(GB)\noom\noom\nZeRO-1\nZeRO-3\nZeRO++\nMiCS\nOur Work\nFig. 13. Peak Memory of training LLaMA-7B/13/30B from 8 to 1024 GPUs.\n4) System Configurations: Table IV presents the configu-\nrations utilized in AMSP and the baselines. AMSP maintains\na uniform set of configurations when scaling training from 8\nGPUs to 1024 GPUs. In ZeRO++, the secondary shard num-\nber of the parameters is tuned for optimal performance, with\n(s0\np = 8, s1\np = 1), and quantization is not enabled. Activation\nrecomputation is applied during the training of LLaMA-30B,\nwhile it is disabled for LLaMA-7B and LLaMA-13B. The\ncommunication-computation overlap configurations are con-\nsistently enabled in all baselines. To ensure a fair comparison\nwith baselines, we disabled the overlap between Broadcast\nand forward computation during the end-to-end system eval-\nuations, as these baselines do not provide this function.\nB. End-to-End System Evaluation\n1) Scalability Performance: Figure 11 illustrates the MFU\nduring the training of models of varying sizes with different\nGPU number, while Figure 12 provides corresponding TGS\nresults. AMSP exhibits higher performance across all cases\nthan the basesline systems. Specifically, it achieves 51%, 52%,\nand 42% MFU when training LLaMA-7B, LLaMA-13B, and\nLLaMA-30B models with 1024 GPUs, respectively.\nWhen training LLaMA-7B with 8 GPUs, ZeRO-1 exhibits\na very similar MFU to AMSP. The observation indicates that\nboth systems achieve comparable computation efficiency. This\nsimilarity arises from the fact that they share the same commu-\nnication cost. Importantly, this result underscores that AMSP,\ndespite introducing innovative communication optimizations,\nmaintains a comparable level of computation efficiency with\nbaseline systems, given the commonality in utilizing the same\ncomputation engine, such as FlashAttention.\nAs the GPU number increases, AMSP demonstrates a com-\nparatively stable decrease in MFU and TGS across LLaMA-\n7B, LLaMA-13B and LLaMA-30B models when compared to\nFig. 14. Trace segment for the LLaMA-7B model training on 32 GPUs with\na micro-batch size of 4096 tokens using DeepSpeed-ZeRO3/MiCS/ZeRO++.\nThis trace captures the backward phase of the last micro-batch within a step.\nother baselines. Expanding from 8 to 1024 GPUs, AMSP ex-\nperiences a modest 15% reduction in MFU, while ZeRO-3 can\nexhibit reductions of up to 88%. The decrease in AMSP\u2019s MFU\nis attributed to the reduced computational load per GPU as the\nnumber of GPUs grows, leading to a higher communication-\nto-compute ratio. Notably, the MiCS approach, which employs\na subgroup communication strategy, also exhibits a relatively\ngentle downward trend, similar to AMSP. However, due to\nits limited array of configuration options, MiCS consistently\nmaintains an MFU below that of AMSP. Zero++, while of-\nten outperforming ZeRO-3, faces challenges such as out-of-\nmemory (OOM) issues. For instance, when training LLaMA-\n30B on 32 GPUs, Zero++ encountered an OOM situation,\nwhereas other methods achieved an MFU of around 40%.\nWhen training LLaMA-7B on 1024 GPUs, AMSP achieves\nMFU 51%, surpassing other baselines. ZeRO-1 follows closely\nwith 36% MFU, while MiCS ranks third at 35%. ZeRO-3 and\nZeRO++ lag significantly behind, achieving approximately 4%\nMFU. In comparison to ZeRO-1, AMSP effectively constrains\nthe Broadcast operation, which is used to disseminate up-\ndated parameters to other GPUs at the end of each step, involv-\ning only 8 GPUs. This strategic approach minimizes commu-\nnication overhead. On the other hand, MiCS also reduces the\ncommunication scale of AllGather and ReduceScatter\nwithin a node for parameters fetch and gradients distribution,\nyet it generates more traffic than AMSP. Consequently, MiCS\nexhibits lower performer compared to ZeRO-1 when training\nLLaMA-7B on 1024 GPUs. For LLaMA-13B and LLaMA-\n30B training on 1024 GPUs, AMSP maintains its leading po-\nsition with MFU values of 51% and 43%, respectively. MiCS\nfollows with 33% for LLaMA-13B and 29% for LLaMA-30B.\n11\nFig. 15. Training trace segment for LLaMA-7B, LLaMA-13B, and LLaMA-30B models using AMSP. This trace encompasses both the forward and backward\nphases of a training step. LLaMA-7B and LLaMA-13B are trained with a micro-batch size of 4096 tokens and a micro-batch number of 2 on 32 GPUs, while\nLLaMA-30B utilizes 64 GPUs for training.\n2) GPU Memory Analysis: Figure 13 illustrates the max-\nimum allocated memory during training for various systems.\nZeRO-3 stands out as the most memory-efficient, primarily\ndue to its aggressive splitting of model states across all GPUs.\nThe memory consumption of ZeRO-3 becomes stable in large-\nscale training. For instance, when scaling from 512 GPUs to\n1024 GPUs, the memory consumption for training LLaMA-\n30B changes from 16GB to 15GB with ZeRO-3. Both ZeRO++\nand MiCS demonstrate enhanced training performance at the\nexpense of higher memory consumption. MiCS, in particular,\nprioritizes redundant storage to optimize communication effi-\nciency, resulting in approximately double the memory usage\ncompared to ZeRO-3 for the same models on 1024 GPUs. This\ntrade-off highlights the strategic use of memory resources to\nachieve superior training outcomes with these approaches.\nAMSP consistently exhibits high memory consumption, par-\nticularly noticeable when comparing it to MiCS. For instance,\nwhen training the 7B model on 1024 GPUs, AMSP\u2019s memory\nfootprint is twice that of MiCS, despite achieving a more\nefficient utilization of memory. Although AMSP and MiCS\ndemonstrate similar memory consumption for the 13B and 30B\nmodels, their memory utilization compositions vary. AMSP\nattains higher training efficiency by dynamically managing\nthe memory allocation of parameters, gradients and optimizer\nstates. This dynamic allocation strategy allows AMSP to op-\ntimize memory usage effectively, contributing to its superior\ntraining efficiency despite the higher memory footprint.\nC. Performance Gap Analysis\nOur investigation reveals that the superior training perfor-\nmance of AMSP can be attributed to two crucial optimiza-\ntions: a flexible sharding strategy for parameters, gradients,\nand optimizer states, and an advanced methodology for or-\nchestrating the overlap between communication and com-\n12\n8\n32\n128\n512\n1024\n(a) LLaMA-7B\n0\n0.2\n0.4\n0.6\n0.8\nMFU\n8\n32\n128\n512\n1024\n(b) LLaMA-13B\n0\n0.2\n0.4\n0.6\n0.8\nMFU\n8\n32\n128\n512\n1024\n(c) LLaMA-30B\n0\n0.2\n0.4\n0.6\n0.8\nMFU\noom\nDeepSpeed (MiCS Config)\nOurs (MiCS Config)\nOurs (Optimal Config)\nFig. 16. Effects (MFU) of Sharding Strategy in training LLaMA-based Mod-\nels from 8 GPUs to 1024 GPUs.\nputation. Our findings underscore that existing implemen-\ntations of ZeRO-3, MiCS, and ZeRO++ within the Deep-\nSpeed framework encounter significant hurdles in achiev-\ning effective overlap between communication and compu-\ntation, particularly during the backward phases. This chal-\nlenge is notably evident in the inefficient utilization of idle\ncompute resources during the ReduceScatter commu-\nnications in ZeRO-3 and ZeRO++, as well as the com-\nbined ReduceScatter and AllReduce communications\nin MiCS, as depicted in Figure 14. In this figure, we present a\ntrace segment for the LLaMA-7B model training on 32 GPUs\nwith a micro-batch size of 4096 tokens using DeepSpeed-\nZeRO3/MiCS/ZeRO++. This trace captures the backward\nphase of the last micro-batch within a step. It reveals instances\nof computation resource bubbles during the backward pass.\nEven when the communication-computation overlap setting\nis enabled, DeepSpeed-ZeRO3/MiCS/ZeRO++ fails to effec-\ntively overlap ReduceScatter with computation. Addition-\nally, in DeepSpeed-MiCS, the ReduceScatter operation\nalso blocks the concurrent execution of AllReduce. We\nshown the complete trace in Appendix A.\nAMSP excels in orchestrating the seamless overlap of com-\nmunications with computation, leading to a substantial im-\nprovement in computing resource utilization, as depicted in\nFigure 15. During the forward pass, as shown in Figure 15\n(b) and (c), AMSP facilitates overlap by concurrently com-\nputing each layer alongside the AllGather communica-\ntions necessary for acquiring the subsequent layer\u2019s param-\neters. During the backward phase, AMSP skillfully overlaps\nAllReduce and ReduceScatter communications with\ncomputation. Moreover, AllReduce can be executed con-\ncurrently with AllGather and ReduceScatter. Addi-\ntionally, AMSP synchronizes the broadcasting of updated pa-\n8\n32\n128\n512\n1024\n(a) LLaMA-7B\n0\n1000\n2000\n3000\n4000\n5000\nTGS\n8\n32\n128\n512\n1024\n(b) LLaMA-13B\n0\n500\n1000\n1500\n2000\n2500\nTGS\n8\n32\n128\n512\n1024\n(c) LLaMA-30B\n0\n400\n800\n1200\nTGS\noom\nDeepSpeed (MiCS Config)\nOurs (MiCS Config)\nOurs (Optimal Config)\nFig. 17. Effects (TGS) of Sharding Strategy in training LLaMA-based Models\nfrom 8 GPUs to 1024 GPUs.\nrameters with the forward computation of the next step, as\nillustrated in Figure 15 (a).\nD. Ablation Study\nWe present ablation experiments to substantiate the effec-\ntiveness of the flexible sharding strategy in AMSP,\n1) Analysis of Sharding Strategy: We conducted experi-\nments to validate the effectiveness of our sharding strategy\nby comparing our Planner-based optimal configurations with\nMiCS\u2019s rule-based config under the same execution engine.\nFigure 16 illustrates the MFU results, while Figure 17 pro-\nvides corresponding TGS results. Our observations reveal that\nwith a 1024-GPU setup, the optimal configuration for AMSP\nat model sizes of 7B, 13B, and 30B yields MFU values that\nare 1.3x, 1.1x, and 1.13x higher, respectively, compared to\nthe MiCS configuration. Moreover, implementing MiCS under\nAMSP results in higher performance compared to the imple-\nmentation on DeepSpeed. This improvement can be attributed\nto our optimizations for communication-computation overlap.\n2) Analysis of Overlap: We conducted a series of experi-\nments to assess the efficacy of our approach in communication-\ncomputation overlap. Our investigation involved systematically\ndeactivating overlap optimizations in the following sequence:\ninitially, we disabled the overlap of Broadcast operations\nfor spreading updated parameters. Subsequently, we turned\noff the overlap of AllReduce for gradient synchroniza-\ntion. Finally, we deactivated the overlap of AllGather and\nReduceScatter in parameter sharding, resulting in a state\nof no overlap during the training process. The experiments\nwere conducted using 64 GPUs, and we varied the compu-\ntational loads by adjusting the micro-batch number M. The\nresults of these experiments, showcasing the effects of overlap,\nare presented in Table V.\n13\nTABLE V\nEFFECTS OF OVERLAP STRATEGY ON TRAINING LLAMA-BASED MODELS WITH MICRO-BATCH SIZES FROM 1 TO 8 USING 64 GPUS. (TGS AND MFU)\nOverlap Strategy\nTGS\nMFU\nM = 1\nM = 2\nM = 4\nM = 8\nM = 1\nM = 2\nM = 4\nM = 8\n7B\nOverlap AllGather/ReduceScatter+AllReduce+Broadcast\n3525\n4119\n4416\n4503\n0.57\n0.62\n0.65\n0.67\nOverlap AllGather/ReduceScatter+AllReduce\n3346\n3991\n4329\n4438\n0.54\n0.60\n0.64\n0.65\nOverlap AllGather/ReduceScatter\n2812\n3609\n4070\n4286\n0.45\n0.54\n0.60\n0.63\nNo Overlap\n2812\n3609\n4070\n4286\n0.45\n0.54\n0.60\n0.63\n13B\nOverlap AllGather/ReduceScatter+AllReduce+Broadcast\n1918\n2082\n2183\n2230\n0.53\n0.58\n0.61\n0.62\nOverlap AllGather/ReduceScatter+AllReduce\n1895\n1920\n2160\n2184\n0.53\n0.54\n0.60\n0.61\nOverlap AllGather/ReduceScatter\n1630\n1782\n2033\n2126\n0.45\n0.49\n0.56\n0.59\nNo Overlap\n1527\n1552\n1608\n1666\n0.42\n0.43\n0.44\n0.46\n30B\nOverlap AllGather/ReduceScatter+AllReduce+Broadcast\n825\n856\n872\n880\n0.48\n0.50\n0.50\n0.51\nOverlap AllGather/ReduceScatter+AllReduce\n759\n814\n847\n859\n0.44\n0.47\n0.49\n0.50\nOverlap AllGather/ReduceScatter\n651\n740\n814\n842\n0.37\n0.43\n0.47\n0.49\nNo Overlap\n557\n654\n678\n719\n0.33\n0.38\n0.39\n0.42\nIn large-scale distributed LLM training, particularly with\n1024 GPUs, the limitations imposed by the global batch size\nnecessitate setting the micro-batch number M to 1. Under such\nconditions, the impact of our overlap optimizations becomes\nsignificantly more pronounced. For instance, in our experi-\nments with a 7B model, disabling the overlap of Broadcast\nleads to a 5% drop in MFU. Further disabling the overlap opti-\nmization for AllReduce causes an additional 16% decline in\nMFU. When investigating scenarios with sp > 1 and sos > 1,\noverlaps for Broadcast, AllReduce, AllGather, and\nReduceScatter significantly contribute to MFU improve-\nments. For the 13B model, communication overlap could en-\nhance the overall system performance by a factor of 1.25\ncompared to the case without any overlap setting. Meanwhile,\nfor the 30B model, the improvement is even more substantial,\nreaching a factor of 1.48. These values demonstrate the essen-\ntial nature of overlap optimizations in boosting the efficiency\nof LLM training.\nVII. RELATED WORK\nModel parallelism and 3D parallelism. Model parallelism is\nrepresented by two approaches: tensor parallelism and pipeline\nparallelism. Tensor parallelism [5] involves partitioning spe-\ncific layer weights and introducing additional AllReduce com-\nmunication. Pipeline parallelism [13], [14], [30], [31] divides\nthe layers of the model horizontally among each rank. Recent\ninnovations have proposed methods that autonomously discern\nparallelism approaches by intricately melding both data and\nmodel parallelism for distinct operators within the model. To\nillustrate, solutions like Alpa [32], OptCNN [33], FlexFlow\n[34], [35], and TensorOpt [36] incorporate both data and tensor\nparallelism. These leverage a variety of search algorithms to\nrefine and enhance the execution of blueprints. However, while\nthese automated parallelism solutions focus on optimizing the\nsharding and placement strategies for the optimal operators\nwithin the computational graph, they overlook strategies re-\nlated to the orthogonal placement of the model states.\nLarge-scale communication optimization. Some works [4],\n[18], [19], [37] try to overlap communication with computa-\ntion to mitigate communication costs. ZeRO++ and Espresso\n[38] utilize quantization and compression techniques to reduce\ncommunication volume, albeit at the expense of precision.\nDEAR [39] aggregates multiple small communications using\nfixed-size buffers to reduce communication overheads. Hetu\n[40] leverages hierarchical all-to-all to minimize inter-node\ncommunication volume under poor inter-node communication.\nSimilarly, Hybrid AllReduce [41] attempts to decompose a\nsingle collective communication primitive into a combination\nof multiple subgroup communications.\nVIII. CONCLUSION\nWe propose AMSP to address the communication challenge\nof distributed LLM training at scale with ZeRO. The proposed\nAMSP introduces a novel approach by incorporating flexible\nsharding strategies\u2014Full-Replica, Full-Sharding, and Partial-\nSharding\u2014for each component within the model states (Pa-\nrameters, Gradients, and Optimizer States). The introduced\nsharding factors (s0\np \u00d7 s1\np, s0\ng \u00d7 s1\ng, s0\nos \u00d7 s1\nos) control GPU and\ndevice mesh sharding. Analyzing memory and communication\ncosts for each dimension, AMSP formulates an optimization\nproblem to find factors optimizing communication costs under\nmemory constraints. Additionally, it implements an execution\nengine tailored for LLM training, and the customized commu-\nnication and computation overlap strategy, incorporating these\nflexible sharding factors to achieve optimized communication\nefficiency during training. Compared to MiCS and ZeRO++,\nAMSP improves the training throughput by 1.4 \u2212 12.7.\nREFERENCES\n[1] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\nE. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark,\nT. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc,\nA. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and\nL. Sifre, \u201cTraining compute-optimal large language models,\u201d CoRR, vol.\nabs/2203.15556, 2022.\n[2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,\nA. Joulin, E. Grave, and G. Lample, \u201cLlama: Open and efficient foun-\ndation language models,\u201d CoRR, vol. abs/2302.13971, 2023.\n14\n[3] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, \u201cDeepspeed: System\noptimizations enable training deep learning models with over 100 billion\nparameters,\u201d in Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, 2020, pp. 3505\u2013\n3506.\n[4] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright,\nH. Shojanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu,\nB. Nguyen, G. Chauhan, Y. Hao, and S. Li, \u201cPytorch fsdp: Experiences\non scaling fully sharded data parallel,\u201d CoRR, vol. abs/2304.11277, 2023.\n[5] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. A.\nKorthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro,\nA. Phanishayee, and M. Zaharia, \u201cEfficient large-scale language model\ntraining on gpu clusters using megatron-lm,\u201d CoRR, vol. abs/2104.04473,\n2021.\n[6] G. Wang, H. Qin, S. A. Jacobs, C. Holmes, S. Rajbhandari, O. Ruwase,\nF. Yan, L. Yang, and Y. He, \u201cZero++: Extremely efficient collective\ncommunication for giant model training,\u201d CoRR, vol. abs/2306.10209,\n2023.\n[7] Z. Zhang, S. Zheng, Y. Wang, J. Chiu, G. Karypis, T. Chilimbi, M. Li,\nand X. Jin, \u201cMics: near-linear scaling for training gigantic model on\npublic cloud,\u201d Proceedings of the VLDB Endowment, vol. 16, p. 37\u201350,\n2022.\n[8] I.\nContributors,\n\u201cInternlm,\u201d\nhttps://github.com/InternLM/InternLM,\n2023.\n[9] Q. Hu, Z. Ye, Z. Wang, G. Wang, M. Zhang, Q. Chen, P. Sun, D. Lin,\nX. Wang, Y. Luo et al., \u201cCharacterization of large language model devel-\nopment in the datacenter,\u201d in USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI\u201924), 2024.\n[10] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nVoss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, \u201cLanguage models are few-shot learners,\u201d in Proceed-\nings of the 34th International Conference on Neural Information Pro-\ncessing Systems, ser. NIPS\u201920.\nCurran Associates Inc., 2020.\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n\u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d vol. 30, 2017.\n[12] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke,\nJ. Smith, B. Vaughan, P. Damania, and S. Chintala, \u201cPytorch dis-\ntributed: Experiences on accelerating data parallel training,\u201d CoRR, vol.\nabs/2006.15704, 2020.\n[13] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee,\nJ. Ngiam, Q. V. Le, Y. Wu et al., \u201cGpipe: Efficient training of giant\nneural networks using pipeline parallelism,\u201d vol. 32, 2019.\n[14] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur,\nG. R. Ganger, P. B. Gibbons, and M. Zaharia, \u201cPipedream: Generalized\npipeline parallelism for dnn training,\u201d in Proceedings of the 27th ACM\nSymposium on Operating Systems Principles, 2019, pp. 1\u201315.\n[15] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d\nCoRR, vol. abs/1412.6980, 2017.\n[16] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, \u201cZero: Memory\noptimizations toward training trillion parameter models,\u201d in SC20: In-\nternational Conference for High Performance Computing, Networking,\nStorage and Analysis.\nIEEE, 2020, pp. 1\u201316.\n[17] S. Li, H. Liu, Z. Bian, J. Fang, H. Huang, Y. Liu, B. Wang, and\nY. You, \u201cColossal-ai: A unified deep learning system for large-scale\nparallel training,\u201d in Proceedings of the 52nd International Conference\non Parallel Processing, 2023, pp. 766\u2013775.\n[18] Y. Peng, Y. Zhu, Y. Chen, Y. Bao, B. Yi, C. Lan, C. Wu, and C. Guo, \u201cA\ngeneric communication scheduler for distributed dnn training accelera-\ntion,\u201d in Proceedings of the 27th ACM Symposium on Operating Systems\nPrinciples, 2019, pp. 16\u201329.\n[19] P. Sun, Y. Wen, R. Han, W. Feng, and S. Yan, \u201cGradientflow: Optimiz-\ning network performance for large-scale distributed dnn training,\u201d IEEE\nTransactions on Big Data, vol. 8, no. 2, pp. 495\u2013507, 2019.\n[20] P. Sun, Y. Wen, T. N. B. Duong, and S. Yan, \u201cTimed dataflow: Reducing\ncommunication overhead for distributed machine learning systems,\u201d in\n2016 IEEE 22nd International Conference on Parallel and Distributed\nSystems (ICPADS).\nIEEE, 2016, pp. 1110\u20131117.\n[21] R. Thakur and W. D. Gropp, \u201cImproving the performance of collective\noperations in mpich,\u201d in European Parallel Virtual Machine/Message\nPassing Interface Users\u2019 Group Meeting.\nSpringer, 2003, pp. 257\u2013267.\n[22] L. Zhang, S. Shi, X. Chu, W. Wang, B. Li, and C. Liu, \u201cDear: Acceler-\nating distributed deep learning with fine-grained all-reduce pipelining,\u201d\nin 2023 IEEE 43rd International Conference on Distributed Computing\nSystems (ICDCS).\nIEEE, 2023, pp. 142\u2013153.\n[23] P. Sanders, J. Speck, and J. L. Tr\u00a8aff, \u201cTwo-tree algorithms for full\nbandwidth broadcast, reduction and scan,\u201d Parallel Computing, vol. 35,\nno. 12, pp. 581\u2013594, 2009.\n[24] R. L. Graham, L. Levi, D. Burredy, G. Bloch, G. Shainer, D. Cho,\nG. Elias, D. Klein, J. Ladd, O. Maor et al., \u201cScalable hierarchical\naggregation and reduction protocol (sharp) tm streaming-aggregation\nhardware design and evaluation,\u201d in High Performance Computing: 35th\nInternational Conference, ISC High Performance 2020, Frankfurt/Main,\nGermany, June 22\u201325, 2020, Proceedings 35.\nSpringer, 2020, pp. 41\u2013\n59.\n[25] Z. Li, J. Huang, Y. Li, A. Xu, S. Zhou, J. Liu, and J. Wang, \u201cA2tp:\nAggregator-aware in-network aggregation for multi-tenant learning,\u201d in\nProceedings of the Eighteenth European Conference on Computer Sys-\ntems, 2023, pp. 639\u2013653.\n[26] S. Liu, Q. Wang, J. Zhang, W. Wu, Q. Lin, Y. Liu, M. Xu, M. Canini,\nR. C. Cheung, and J. He, \u201cIn-network aggregation with transport trans-\nparency for distributed training,\u201d in Proceedings of the 28th ACM In-\nternational Conference on Architectural Support for Programming Lan-\nguages and Operating Systems, Volume 3, 2023, pp. 376\u2013391.\n[27] H. Peng, K. Wu, Y. Wei, G. Zhao, Y. Yang, Z. Liu, Y. Xiong, Z. Yang,\nB. Ni, J. Hu et al., \u201cFp8-lm: Training fp8 large language models,\u201d arXiv\npreprint arXiv:2310.18313, 2023.\n[28] T. Dao, \u201cFlashattention-2: Faster attention with better parallelism and\nwork partitioning,\u201d arXiv preprint arXiv:2307.08691, 2023.\n[29] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., \u201cPalm: Scal-\ning language modeling with pathways,\u201d Journal of Machine Learning\nResearch, vol. 24, no. 240, pp. 1\u2013113, 2023.\n[30] S. Fan, Y. Rong, C. Meng, Z. Cao, S. Wang, Z. Zheng, C. Wu, G. Long,\nJ. Yang, L. Xia et al., \u201cDapple: A pipelined data parallel approach\nfor training large models,\u201d in Proceedings of the 26th ACM SIGPLAN\nSymposium on Principles and Practice of Parallel Programming, 2021,\npp. 431\u2013445.\n[31] B. Yang, J. Zhang, J. Li, C. R\u00b4e, C. R. Aberger, and C. D. Sa,\n\u201cPipemare: Asynchronous pipeline parallel dnn training,\u201d CoRR, vol.\nabs/1910.05124, 2020.\n[32] L. Zheng, Z. Li, H. Zhang, Y. Zhuang, Z. Chen, Y. Huang, Y. Wang,\nY. Xu, D. Zhuo, E. P. Xing, J. E. Gonzalez, and I. Stoica, \u201cAlpa:\nAutomating inter- and intra-operator parallelism for distributed deep\nlearning,\u201d CoRR, vol. abs/2201.12023, 2022.\n[33] Z. Jia, S. Lin, C. R. Qi, and A. Aiken, \u201cExploring hidden di-\nmensions in parallelizing convolutional neural networks,\u201d CoRR, vol.\nabs/1802.04924, 2018.\n[34] Z. Jia, M. Zaharia, and A. Aiken, \u201cBeyond data and model parallelism\nfor deep neural networks,\u201d CoRR, vol. abs/1807.05358, 2018.\n[35] C. Unger, Z. Jia, W. Wu, S. Lin, M. Baines, C. E. Q. Narvaez, V. Ra-\nmakrishnaiah, N. Prajapati, P. McCormick, J. Mohd-Yusof et al., \u201cUnity:\nAccelerating {DNN} training through joint optimization of algebraic\ntransformations and parallelization,\u201d in 16th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 22), 2022, pp.\n267\u2013284.\n[36] Z. Cai, K. Ma, X. Yan, Y. Wu, Y. Huang, J. Cheng, T. Su, and F. Yu,\n\u201cTensoropt: Exploring the tradeoffs in distributed dnn training with auto-\nparallelism,\u201d CoRR, vol. abs/2004.10856, 2020.\n[37] A. Jayarajan, J. Wei, G. Gibson, A. Fedorova, and G. Pekhimenko,\n\u201cPriority-based parameter propagation for distributed DNN training,\u201d\nCoRR, vol. abs/1905.03960, 2019.\n[38] Z. Wang, H. Lin, Y. Zhu, and T. E. Ng, \u201cHi-speed dnn training with\nespresso: Unleashing the full potential of gradient compression with\nnear-optimal usage strategies,\u201d in Proceedings of the Eighteenth Euro-\npean Conference on Computer Systems, 2023, pp. 867\u2013882.\n[39] L. Zhang, S. Shi, X. Chu, W. Wang, B. Li, and C. Liu, \u201cDear: Acceler-\nating distributed deep learning with fine-grained all-reduce pipelining,\u201d\nCoRR, vol. abs/2302.12445, 2023.\n[40] X. Nie, P. Zhao, X. Miao, T. Zhao, and B. Cui, \u201cHetumoe: An efficient\ntrillion-scale mixture-of-expert distributed training system,\u201d CoRR, vol.\nabs/2203.14685, 2022.\n[41] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, \u201cDeepspeed: System\noptimizations enable training deep learning models with over 100 billion\nparameters,\u201d pp. 3505\u20133506, 2020.\n15\nFig. 18. Training trace of LLaMA-7B on 32 NVIDIA A800 GPUs with a micro-batch size of 4096 tokens and micro-batch number of 2.\nAPPENDIX\nA. Trace Analysis (7B)\nWe present the training traces during a single step for the\nLLaMA-7B, LLaMA-13B, and LLaMA-30B models under the\nDeepSpeed framework configurations of ZeRO-3, MiCS, and\nZeRO++, as well as under the AMSP framework with its op-\ntimal configuration, in Figure 18, Figure 19, and Figure 20,\nrespectively. For this analysis, we utilized a micro batch size of\n4096 tokens and a micro batch number of 2 across 32 GPUs\nfor both the 7B and 13B models, while the training of the\n30B model was conducted on 64 GPUs. Each GPU is equipped\nwith 80GB of memory, interconnected through NVLink within\na node. Inter-node communication is facilitated by 4 Mellanox\nHDR InfiniBand connections without SHARP.\n16\nFig. 19. Training trace of LLaMA-13B on 32 NVIDIA A800 GPUs with a micro-batch size of 4096 tokens and micro-batch number of 2.\nFig. 20. Training trace of LLaMA-30B on 64 NVIDIA A800 GPUs with a micro-batch size of 4096 tokens and micro-batch number of 2.\n17\n"
  },
  {
    "title": "Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?",
    "link": "https://arxiv.org/pdf/2311.00047.pdf",
    "upvote": "7",
    "text": "Grounding Visual Illusions in Language:\nDo Vision-Language Models Perceive Illusions Like Humans?\nYichi Zhang\nUniversity of Michigan\nzhangyic@umich.edu\nJiayi Pan\u2217\nUniversity of California, Berkeley\njiayipan@berkeley.edu\nYuchen Zhou\u2217\nThe New School\nzhoua926@newschool.edu\nRui Pan\nNorthwestern University\nrui.pan@kellogg.northwestern.edu\nJoyce Chai\nUniversity of Michigan\nchaijy@umich.edu\nAbstract\nVision-Language Models (VLMs) are trained\non vast amounts of data captured by humans\nemulating our understanding of the world.\nHowever, known as visual illusions, human\u2019s\nperception of reality isn\u2019t always faithful to\nthe physical world. This raises a key question:\ndo VLMs have the similar kind of illusions as\nhumans do, or do they faithfully learn to rep-\nresent reality? To investigate this question, we\nbuild a dataset containing five types of visual\nillusions and formulate four tasks to examine\nvisual illusions in state-of-the-art VLMs. Our\nfindings have shown that although the overall\nalignment is low, larger models are closer to\nhuman perception and more susceptible to vi-\nsual illusions. Our dataset and initial findings\nwill promote a better understanding of visual\nillusions in humans and machines and provide\na stepping stone for future computational mod-\nels that can better align humans and machines\nin perceiving and communicating about the\nshared visual world. The code and data are\navailable at github.com/vl-illusion/dataset.\n1\nIntroduction\nIt\u2019s well established that human perceptual systems\nare susceptible to visual illusions, which are de-\nfined as \u201cconsistent and persistent discrepancies\nbetween a physical state of affairs and its represen-\ntation in consciousness\u201d (Day, 1984). Figure 1\nshows a well-known example - the checker shadow\nillusion (Adelson, 1995). Here, a cylinder on the\nchecker board creates a shadow on the board. Hu-\nman viewers are directed to look at the two squares\nA and B as shown in Figure 1(a). To most normal-\nsighted people, they will perceive that square A is\n\u2217Work done while the author was an undergraduate re-\nsearch assistant at the University of Michigan.\n(a)\n(b)\nFigure 1: The checker shadow illusion (Adelson, 1995).\ndarker than square B. However, the reality is, the\ncolor pixels of A and B are exactly the same, as\nshown in Figure 1(b). This example demonstrates\nthat while the physical attributes of A and B are the\nsame, from humans\u2019 eyes, they may look different,\nwhich may further influence how language is used\nto describe these objects.\nMotivated by human visual illusion phenomena,\nrecent years have seen an increasing amount of\nwork in machine visual illusions (Gomez-Villa\net al., 2019, 2020; Hirsch and Tal, 2020; Sun and\nDekel, 2021; Lonnqvist et al., 2021). These pre-\nvious works were solely based on vision, for ex-\nample, evaluating how the internal representation\nfrom a computer vision model can be used as a\nproxy of stimulus compared to human\u2019s stimulus\nshift under illusions. Most previous experiments\nwere conducted in a case-by-case manner, without\naddressing general behaviors through a systematic\ninvestigation.\nDifferent from these previous works, this paper\nstudies visual illusion from a new angle, in the\ncontext of language communication. Language\ncomprehension and language production are tightly\nlinked to how we perceive the visual world. Back to\nFigure 1(a), when two people are observing the fig-\nure together, due to their likely shared illusion, the\narXiv:2311.00047v1  [cs.AI]  31 Oct 2023\nexpression \u201cthe darker square\u201d will lead to the same\nreference of square A. But when a human com-\nmunicates with a machine, will the machine also\ninterpret \u201cthe darker square\u201d as square A? Given\nthe rise of large vision-language models (VLM),\nit\u2019s important to understand whether these VLM\nmodels have a similar tendency to visual illusions,\nand to what degree they may align with human vi-\nsion. The answers to these questions will further\nimpact the alignment of the grounded language\ncommunication between humans and machines.\nTo address these questions, we created a new\nvisual illusion dataset covering five different cate-\ngories from the cognitive literature. Based on the\ndataset, we created a benchmark, Grounding Vi-\nsual Illusion in Language (GVIL), which consists\nof four subtasks: Same-Difference Question An-\nswering (SameDiffQA), Referential Question An-\nswering (RefQA), Attribute Question Answering\n(AttrQA), and Referential Localization (RefLoc) to\nassess machines\u2019 alignment with the human under\nvisual illusions.\nWe specifically evaluated four state-of-the-art\nvision-language models: Unified-IO (Lu et al.,\n2022), OFA (Wang et al., 2022), LLaVA (Liu et al.,\n2023) and InstructBLIP (Dai et al., 2023). Our re-\nsults have shown that these four models mostly do\nnot align with human vision illusions, especially\nfor QA-based tasks. However, for the RefLoc task,\nthese models (especially ones with larger parame-\nters) have demonstrated an impressive alignment\nwith humans.\nTo our knowledge, this is the first work that takes\nlanguage into consideration to study machine vi-\nsual illusion. There are two main contributions of\nthis work. First, this investigation provides an ini-\ntial understanding of the alignment between human\nand machine visual illusions. Such understanding\nwill enable techniques for a better communicative\ngrounding in situated language communication and\nhelp develop more reliable embodied agents in the\nfuture. Second, unlike using internal representa-\ntions to explain illusions, language can be used\nas a proxy to demonstrate whether and how ma-\nchine illusions match or mis-match with the hu-\nman illusion. This benchmark will not only facili-\ntate computational models for better human agent\nalignment, but also provide tools for scientific un-\nderstanding of visual illusion in both humans and\nmachines.\n2\nRelated Work\nHuman Visual Illusion\nVisual illusions in hu-\nmans are instances where human subjective per-\nceived properties, such as color or size, deviates\nfrom their true physical characteristics (Carbon,\n2014). This underscores the fact that the human\nbrain doesn\u2019t perfectly replicate physical features;\nrather, it integrates contextual information and prior\nknowledge to form the perceptual experiences (Car-\nbon, 2014).\nVisual illusions can affect human behavior in\nmultiple ways. Research has shown that human ac-\ntion cannot resist visual illusions (Gentilucci et al.,\n1996; Franz, 2001; Carey, 2001), so is language\ncomprehension and language production. Such\nfindings catalyze inquiries regarding the capabil-\nity of models to comprehend language instructions\nbased on human perceptions and align them with\nhuman intent.\nMachine Visual Illusion.\nPrevious studies have\nsignificantly advanced our ability to examine visual\nillusions by providing systematic data and tools.\nThese efforts include the introduction of tools for\ncalculating and generating illusory images system-\natically (Hirsch and Tal, 2020; Fan and Zeng, 2023),\nthe development of open-source software with a\nparametric framework for controlled illusion gen-\neration (Makowski et al., 2021), and the proposal\nof a framework synthesizing new visual illusions\nusing automatic differentiation techniques (Gomez-\nVilla et al., 2022). With the goal of evaluating\nmachine visual illusions, prior research (Gomez-\nVilla et al., 2019, 2020; Afifi and Brown, 2019;\nBenjamin et al., 2019) has also demonstrated that\nconvolutional neural networks trained on ImageNet\nor low-level vision tasks can be misled by certain\nvisual illusions, similar to human responses. These\nworks have formed a foundation for scalable and\nreproducible research on machine illusions.\nUnlike prior research focusing exclusively on\nvision models, our study introduces a novel and\nunique angle by presenting the first dataset offering\nnatural language annotations for the evaluation of\nmachine-processed visual illusions. This work in-\ntends to bridge the current gap and facilitate future\nevaluations of vision-language models concerning\ntheir alignment with human visual illusions. This\nnovel perspective illuminates future improvements\nin human-machine alignment and promotes the cru-\ncial role of human language as the interaction inter-\nface with machines.\nFoundation Vision-Language Models.\nRecent\nadvancements in foundational vision-language\nmodels (VLMs) have shown impressive results\nacross a broad spectrum of tasks (OpenAI, 2023;\nWang et al., 2022; Lu et al., 2022; Alayrac et al.,\n2022; Radford et al., 2021). These models, acting\nas user interfaces, interact with users through both\nlanguage and visuals, necessitating a deep under-\nstanding of human intent and an alignment with hu-\nman values to make them more useful. While pre-\nvious research has primarily focused on language-\nbased uni-modal alignment problems (Ouyang\net al., 2022; Kosinski, 2023), our work offers a\nfresh perspective. Centered on the intersection of\nVLM\u2019s perception capability and human cognitive\nbiases, we investigate to what degree they can un-\nderstand humans and align with human intentions\nunder visual illusions.\n3\nThe Grounding Visual Illusion in\nLanguage (GVIL) Benchmark\nTo facilitate our investigation, we created a bench-\nmark for evaluating machine visual illusions in the\ncontext of grounded communication. This bench-\nmark is built upon a set of images with visual illu-\nsions. Each image consists of two objects which\nmay look different to humans but are actually iden-\ntical in their pixels. This setup has two advantages.\nFirst, the definition of illusion is clear and non-\nambiguous, thus it is easy to measure whether the\nmachine has a similar illusion as humans. Sec-\nondly, the multi-object setup naturally supports the\nevaluation of language grounding, such as evalu-\nating whether the machine can select the object\nan expression grounds to under the illusion (i.e.,\nsquare A is what \"the darker square\" grounds to in\nFigure1(a)).\n3.1\nData Collection\nOur dataset encapsulates five distinct types of illu-\nsions, each reflecting different elements of human\nphysiological and cognitive processes (Gregory,\n1997; Kitaoka, 2010; Robinson, 2013). Table 1 dis-\nplays a sample of each illusion type, along with a\ndetailed description.\nThese illusions can be categorized into two broad\nareas: color and geometric illusions. For color illu-\nsions, we adopt the classifications of color con-\nstancy, assimilation, and simultaneous contrast\n(MacEvoy, 2005). In terms of geometric illusions,\nColor Constancy\nThe red ship on the left still looks red\nafter applying a blue filter, the blue\nship on the right still looks blue af-\nter applying a red filter, even though\nthe RGB colors of both ships are the\nsame.\nColor Assimilation\nThe two circles have the same\ncolor, while the one on the\nleft looks red (due to its neigh-\nbor/foreground) and the one\non the right looks orange.\nColor Contrast\nThe two grey circles have the\nsame color, while the one on\nthe left looks lighter and the\none on the right looks darker.\nGeometrical Relativity\nThe two orange circles have\nthe same size, while the one on\nthe left looks smaller and the\none on the right looks bigger.\nGeometrical Perspective\nThe two people have the same\nheight, while the one on the\nleft looks shorter and the one\non the right looks taller.\nTable 1: Example illusion from each category and the\ncorresponding explanations.\nwe only included distortions among the four cat-\negories in Robinson\u2019s illusion classification in or-\nder to fix the need for a comparative assessment.\nThe illusions we used to generate our data include\nDelboeuf (Delboeuf, 1865), Ebbinghaus, and Jas-\ntrow illusions (Jastrow, 1892) for relativity, and\nM\u00fcller-Lyer (M\u00fcller-Lyer, 1889) and Ponzo illu-\nsions (Ponzo, 1911) for perspective distortion. The\nfollowing explanations give an overview of the hu-\nman perception phenomena underlying each cate-\ngory:\n\u2022 Color Constancy refers to phenomenon where\nthe color of an object remains constant perceptu-\nally, despite changes in lighting conditions.\n\u2022 Color Assimilation shows how two adjacent\ncolor patches influence each other\u2019s perceptual\nappearance, reducing the perceived color differ-\nence.\n\u2022 Color Contrast. The perceived color of an ob-\nject shifts opposite to the color of its surround-\nings\nFigure 2: Data augmentation examples for the Ebbing-\nhaus illusion.\n\u2022 Geometrical Relativity refers to the distortion\nin the perceived shape or size of an object due to\nthe influence of surrounding oobjects.\n\u2022 Geometrical Perspective reflects the tendency\nof our visual system to perceive perceptually dis-\ntant objects as larger than nearby ones of the\nsame size.\nFor each illusion type, we first collected several\nroot images from the literature (Todorovi\u00b4c, 2020)\nand online resources1. We manually identify at-\ntributes that can be changed without impacting the\neffect of illusion (e.g., the color of objects in geo-\nmetric illusions, or the position of objects in color\nillusions), and edit them to create more illusion\ninstances of the same type, to enrich the number of\nimages in our dataset. We show some augmentation\nexamples in Figure 2.\nThe statistics of our dataset is shown in Table 2.\nNote that since this dataset is only used for the eval-\nuation purpose, i.e., to assess machine\u2019s alignment\nwith human in visual illusion, we chose quality\nover quantity. The dataset is modest in size as each\ninstance is carefully selected (or augmented) based\non cognitive literature. Nonetheless, our infras-\ntructure is also designed to foster the continuous\ndevelopment and augmentation of this dataset, al-\nlowing for community contributions, for instance.\nIt will become an important resource to not only\nsupport a better understanding of machine/human\nvisual illusion, but also facilitate the adaptation of\ncomputational models to visual illusions.\n3.2\nBenchmark Tasks\nWe define four vision-language tasks targeting dif-\nferent model capabilities.\nSame-Different Question Answering (SameDif-\nfQA) aims at evaluating the ability of recognizing\n1https://michaelbach.de/ot/\nCategory\n#Root\n#Image\n#Instance\nColor Constancy\n3\n6\n96\nColor Assimilation\n5\n34\n544\nColor Contrast\n3\n30\n480\nGeometrical Relativity\n3\n20\n320\nGeometrical Perspective\n2\n10\n160\nTotal\n16\n100\n1600\nTable 2: Dataset statistics.\nIMG1 (illusion-free)\nIMG2 (illusion-induced)\nQ: Are the two balls the same color or different?\nDifferent\nSame\nDifferent\nSame\nHumanlike\nSame\nSame\nNo-Illusion\n\u00b7\u00b7\u00b7\nDifferent\nN/A\nIMG2:\nIMG1:\nFigure 3: Illustration of the SameDiffQA setup. For each\ninstance, the model is asked about its perception of an\nobject property across two images, one illusion-free and\none illusion-induced. For valid illusion evaluation, the\nmodel must initially identify identical properties in the\nillusion-free image.\nillusions. As shown in Figure 3, each question\nconcerns a pair of images (IMG1 and IMG2). One\nimage (IMG1) is illusion-free where two objects\n(blue balls) are identical in color. The other image\n(IMG2) is induced with an effect of illusion where\ntwo balls appear in different colors (blue on the left\nand green on the right) although their pixels are the\nsame as in IMG1. The model is tasked to answer\nwhether two objects are the same color for each of\nthe images. From a human\u2019s perspective, the an-\nswer would be \u201cSame\u201d to IMG1 and \u201cDifferent\u201d to\nIMG2 due to the visual illusion. If the model gives\nthe answer \u2018Same\u201d to IMG1 and \u201cDifferent\u201d to IMG2,\nthen the answers align with human\u2019s answers and\ntherefore the model is considered \u201chuman-like\". If\nthe model gives \u201cSame\u201d to both images, it implies\nthat the model is faithful to reality and does not per-\nceive the same illusion as humans do. If the model\nanswers \u201cDifferent\u201d to IMG1, it means it lacks ba-\nsic ability to correctly perceive reality and these\ncases are considered not applicable to our illusion\nevaluation.\nWhile SameDiffQA focuses on the detection of\nthe presence of illusions, we design three tasks to\nexamine how well do machines align with humans\nwhen communication happens under the influence\nof illusions. Since it is reported that models tend\nto take shortcut by giving an answer purely based\non the text question without looking at the image\n(Goyal et al., 2017), we propose a paired test to\nreduce the evaluation bias. As shown in Figure 4,\neach instance comes with two images: one original\nillusion image (IMG1) and one image IMG2 that flips\nthe objects from the original image (IMG1) in a way\nthat will also invert the answer to the question.\nSpecifically, we evaluate the following three as-\npects:\nReferential Question Answering (RefQA) tests\nthe human-likeness of referring to objects under\nthe illusion. In the question, the object of interest\nis referred to by a property affected by the illusion,\nand the machine is asked to select the object from\ntwo options, e.g., select either left or right for the\nball that looks blue, in IMG1 and IMG2.\nAttribute Question Answering (AttrQA) tests the\nhuman-likeness to describe the attribute of objects\nunder the illusion. The question is about describing\na visual attribute (e.g. color) influenced by the\nillusion of a selected object, and we provide two\nanswer options to select from.\nReferential Localization (RefLoc) tests the\nhuman-likeness of localizing the referred object un-\nder the illusion. Given a referential expression that\nmakes sense to humans under the effect of illusion\n(but may not be faithful to reality), the model needs\nto predict a bounding box for the object the expres-\nsion is referring to. For each referential query, we\nconsider the machine\u2019s response to be humanlike\nonly when the pair of responses from the two im-\nages both match with those from human\u2019s. This\nenforces that a humanlike response from machines\nhas to be grounded in the illusion image.\nTo create this benchmark, we annotate the col-\nlected illusion images with natural language ques-\ntions and the corresponding answers that humans\nwill give under illusions. To support the study of\nlanguage grounding, we also annotate the referring\nexpressions for each of the objects with the cor-\nresponding bounding box, where the expressions\nare formed under illusions. We provide several\nparaphrases for all the language annotations to help\nthe evaluation be more robust to models that are\nsensitive to language forms. All the images and\nIMG1:             | IMG2:           .\nIMG1 (original)\nIMG1: Left | IMG2: Right\n\u00b7\u00b7\u00b7 | \u00b7\u00b7\u00b7\nHumanlike\nWhich ball looks blue, left or right?\nIs the ball on the left blue or green?\nLocalize the blue ball in the image.  \n\u00b7\u00b7\u00b7 | \u00b7\u00b7\u00b7\nIMG2 (flipped)\nIMG1: Blue  | IMG2: Green\nRefQA \nAttrQA\nRefLoc\nUnlike\n\u00b7\u00b7\u00b7 | \u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7 | \u00b7\u00b7\u00b7\nFigure 4: Illustration of the RefQA, AttrQA and RefLoc\nsetups. We flip the illusion image wherein the grounding\noutcome should also be inverted, to create a pair of\nimages for each test. Model success requires accurate\nidentification in both original and flipped versions to\nalign with human responses. Matching human answers\nsignals the model\u2019s capability to interpret illusions in a\nhumanlike way, while a mismatch indicates otherwise.\ncorresponding annotations are verified by at least\nthree human annotators from our team.\n4\nExperimental Setup\nVision-Language Models.\nTo be evaluated on\nall of the four tasks in GVIL, the model has to be\nequipped with the visual question-answering skill\nand the object localization skill simultaneously.\nAmong a few candidates, we choose two state-of-\nthe-art models, the Unified-IO (Lu et al., 2022)\nand OFA (Wang et al., 2022), both of which are\ntrained on a wide range of vision-language tasks,\nand achieve impressive performance with a strong\nzero-shot inference capability on unseen data. Ad-\nditionally, we select two recent works that adapt\nlarge language models to understand visual images:\nthe LLaVA (Liu et al., 2023) and InstructBLIP (Dai\net al., 2023). These models are interesting to in-\nspect as they have shown a highly conceptual under-\nstanding of the nuance in images, such as the capa-\nbility of interpreting jokes, which may also be use-\nful in interpreting illusions. For each of the afore-\nTiny (33M)\nBase (93M)\nLarge (470M)\nHuge (930M)\n0\n20\n40\n60\n80\nOFA\n4.2\n43.0\n52.8\n19.5\n36.544.0\n14.518.2\n67.2\n15.0\n34.0\n51.0\nVicuna-7B\nVicuna-13B\n0\n20\n40\n60\n80\nLLaVA\n3.2\n50.546.2\n1.5\n48.849.8\nSmall (71M)\nBase (241M)\nLarge (776M)\nXL (2925M)\n0\n20\n40\n60\n80\nUnified-IO\n8.2\n55.2\n36.5\n8.2 15.5\n76.2\n28.825.8\n45.5\n35.5\n8.5\n56.0\nVicuna-7B\nVicuna-13B\n0\n20\n40\n60\n80\nInstructBLIP\n22.8\n55.8\n21.5\n26.830.2\n43.0\nHumanlike\nNo-Illusion\nN/A\nFigure 5: Results of SameDiffQA. The number shows the percentage of the answers. Each cluster represents the\ndistribution over humanlike, no-illusion and N/A answers from a model. The green and red line correspond to the\nlinear regression of humanlike rate and no-illusion rate across all the model sizes. Except for OFA-Large, Unified-\nIO-Large, InstructBLIP-13B, the differences between the humanlike rate and the no-illusion rate are statistically\nsignificant P < 0.005. Details are in Table 4 Appendix A.\nmentioned models, there exists a range of variants\nin different sizes: OFA-{Tiny, Base, Large, Huge},\nUnified-IO-{Small, Base, Large, XL}, LLaVA-\n{Vicuna-7B, Vicuna-13B}, InstructBLIP-{Vicuna-\n7B, Vicuna-13B}. This allows us to study the im-\npact of size variations on model\u2019s understanding of\nvisual illusions.\nMetrics.\nThrough the experiments, we keep track\nof the Humanlike rate to measure the alignment be-\ntween humans and VLMs, which is the percentage\nof examples where the machine gives exactly the\nsame answers as humans. For the SameDiffQA task,\nwe also compute the No-Illusion rate, which cor-\nresponds to the percentage of examples where the\nmachine consistently considers the objects as the\nsame under both illusion and illusion-free settings.\nFor examples where the model fails to identify the\nobjects as the same in the illusion-free image or\nproduces nonsense answers to the questions, we\nmark them as Not Applicable (N/A) and exclude\nthem from the illusion recognition assessment.\n5\nResults Analysis\nFrom our experiments, we are interested in investi-\ngating the following research questions:\n\u2022 RQ1: to what extent do VLMs recognize the\npresence of illusions similar to humans?\n\u2022 RQ2: how much do VLMs align with humans\nwhen communication happens under the influ-\nence of illusions?\n\u2022 RQ3: does the degree of alignment between\nVLMs and human responses vary across dif-\nferent categories of illusions?\nWe highlight several of our findings across this\nthree questions in below.\n5.1\nIllusion Recognition\nThe results of SameDiffQA are shown in Figure 5.\nRelative proportions of \"humanlike,\" \"no-illusion,\"\nand \"not applicable (N/A)\" responses are repre-\nsented as green, orange, and grey bars respectively\nfor each model, which all together account for\n100%. First of all, we notice a large percentage\nof responses, across all models, fall under the N/A\ncategory. This suggests that these models often\ncannot even tell that the objects are identical in the\nillusion-free image, underscoring the need for im-\nprovement in standard vision-language reasoning\ncapabilities beyond the scope of illusion contexts.\nGiven the high proportion of N/A responses, one\nmight question the benchmark\u2019s adequacy in reli-\nably reflecting a model\u2019s tendency towards either\n\"humanlike\" or \"no-illusion\". Excluding the N/A\nresponses, we employed a \u03c72-test and found that 9\nout of 12 models would reject the null hypothesis\nwhich posits that the \"humanlike\" or \"no-illusion\"\nresponses are uniformly distributed. In other words,\nthese models do not behave randomly. Refer to Ap-\npendix A for more details. Such findings indicate\nthat, despite certain limitations in their capabili-\nties, our dataset and experimental design effectively\nOFA\nUnified-IO\nLLaVA\nI-BLIP\n0\n3\n6\n9\n12\n15\nRefQA\n0.5\n7.5\n14.0\n12.0\n2.8 3.8\n7.8\n9.8\n5.2 5.2\n2.0 1.2\nOFA\nUnified-IO\nLLaVA\nI-BLIP\n0\n3\n6\n9\n12\n15\nAttrQA\n0.5\n3.0\n9.0\n11.0\n2.2 3.5\n11.2\n9.5\n2.2 2.0\n2.0 1.8\nOFA\nUnified-IO\n0\n10\n20\n30\n40\n50\nRefLoc\n6.8\n18.2\n39.2\n34.8\n16.0\n30.8\n32.0\n44.5\nFigure 6: Humanlike rate on RefQA, AttrQA and RefLoc.\nEach bar represents a different model size, arranged in\nascending order from left to right. Note that LLaVA and\nInstructBLIP cannot predict object bounding boxes thus\ndo not have the RefLoc results.\nTask\nModel\nPearson coeff.\np-value\nSameDiffQA\nOFA\n0.689\n0.311\nUnifiedIO\n0.940\n0.059*\nRefQA\nOFA\n0.946\n0.054*\nUnifiedIO\n0.977\n0.022**\nAttrQA\nOFA\n0.957\n0.043**\nUnifiedIO\n0.853\n0.146\nRefLoc\nOFA\n0.933\n0.066*\nUnifiedIO\n0.960\n0.039**\nTable 3: Pearson\u2019s correlation analysis between the hu-\nmanlike rate and model size. Statistically significant\nresults with p < 0.05 and p < 0.1 are marked with **\nand *, respectively.\ngauge illusion recognition in the assessed VLMs.\nWhen examining cases where responses are ap-\nplicable for testing illusion recognition, we observe\nthat the majority of models are more likely to fail\nin recognizing illusions (35.4% on average) than\nproducing humanlike responses (15.6% on aver-\nage). This discrepancy is most pronounced in the\ncase of InstructBLIP, where the model predomi-\nnantly offers \u2019no-illusion\u2019 answers. Conversely, the\nUnified-IO XL model stands out as the only model\nexhibiting a significant propensity towards human-\nlike illusion recognition. A further investigation of\nthe underlying reason that causes this discrepancy\nwould be interesting further work.\nTo illustrate how the scores evolve with model\nsize, we plot regression lines of \u201chumanlike\"\n(green) and \u201cno-illusion\" (red) rates, respectively.\n0\n20\n40\n60\n80\nOFA Huge\n8.3\n4.2\n0.0\n0.0\n9.6\n12.5\n8.1\n36.8\n23.3\n12.5\n20.8\n21.7\n7.5\n5.0\n3.8\n41.2\n27.5\n27.5\n12.5\n75.0\nSameDiffQA\nRefQA\nAttrQA\nRefLoc\nConstancy\nAssimilation\nContrast\nRelativity\nPerspective\n0\n20\n40\n60\n80\nUnified-IO XL\n20.8\n8.3\n4.2\n33.3\n29.4\n8.8\n9.6\n44.9\n35.0\n5.8\n11.7\n42.5\n50.0\n2.5\n8.8\n35.0\n37.5\n40.0\n7.5\n75.0\nSameDiffQA\nRefQA\nAttrQA\nRefLoc\n0\n20\n40\n60\n80\nLLaVA 13B\n0.0\n4.2\n0.0\n1.5\n2.9\n1.5\n1.7\n10.0\n2.5\n2.5\n5.0\n1.2\n0.0\n0.0\n5.0\nSameDiffQA\nRefQA\nAttrQA\n0\n20\n40\n60\n80\nInstructBLIP 13B\n33.3\n8.3\n0.0\n19.9\n0.0\n2.9\n25.8\n2.5\n1.7\n31.2\n0.0\n0.0\n40.0\n0.0\n2.5\nSameDiffQA\nRefQA\nAttrQA\nFigure 7: Humanlike rates of the largest model of each\nfamily, with finer-grained human-likeness scores on\neach illusion category.\nAn emerging trend reveals that \u201chumanlike\" scores\ntend to increase as the model scales, whereas \"no-\nillusion\" responses tend to decline. This finding\nsuggests a positive correlation between model scale\nand human-machine alignment under illusions. We\nhypothesize that this observed trend could be at-\ntributed to the enhanced pattern-recognition capa-\nbilities of larger models. These models, arguably,\nare better suited to discern and assimilate patterns\npresent in data generated by humans, which may\nhave been shaped by the influence of illusions. Con-\nsequently, it\u2019s plausible to assume that these models\nare progressing towards a more humanlike compre-\nhension of illusions.\n5.2\nCommunication Under Illusion\nThe results of RefQA, AttrQA, and RefLoc exper-\niments are shown in Figure 6, offering insights\ninto the alignment between machine and human\nresponses under the influence of visual illusions.\nWe find that all the VLMs encounter significant\nchallenges in responding to questions presented un-\nder the influence of visual illusions in both RefQA\nand AttrQA. As a result, the models obtain a maxi-\nmum humanlike response rate of only 14.0% and\n11.2% for RefQA and AttrQA, respectively. Inter-\nestingly, models exhibit much stronger alignment\nin the localization task, with the highest alignment\nof 44.5% achieved by Unified-IO XL. This indi-\ncates that the learned object localization skill aligns\nTiny\nBase\nLarge\nHuge\n0\n20\n40\n60\n19.9\n36.0\n53.5\n47.4\n19.9\n33.4\n39.4\n43.3\n19.9\n35.2\n48.7\n46.2\n20.0\n34.3\n45.1\n44.8\nAttn. on the humanlike choice under illusion\nAttn. on the counter-humanlike choice under illusion\nAttn. on the humanlike choice w/o illusion\nAttn. on the counter-humanlike choice w/o illusion\nFigure 8: Attention weight distribution of OFA models for the RefLoc task.\nFigure 9: Visualization of the attention maps generated\nby the OFA-Large model for the RefLoc task. In each\nrow, the input image is shown on the left, and the atten-\ntion map for the referential expression \"smaller orange\nball\" is shown on the right. The attention maps surround-\ning the object of interest are highlighted for enhanced\nvisibility.\nbetter with humans under illusions compared to the\nvisual question answering skills. Research into the\nunderlying reason behind this difference might be\nan interesting future direction.\nNotably, we find a positive correlation between\nscaling up models and the increase of human-\nlike rate across different models and tasks, which\nechoes our earlier observations from the SameD-\niffQA experiment. To verify the statistical signifi-\ncance, we conducted Pearson\u2019s correlation analysis\nfor OFA and UnifiedIO models2. As shown in Ta-\nble 3, 6 of the 8 tested scenarios showed significant\nor moderately significant positive correlations, with\nPearson coefficients exceeding 0.9. Such results un-\nderscore the potential of larger models to enhance\nthe human-machine alignment of responses across\ndifferent tasks and illusion contexts.\n2InstructBLIP and LLaVA were excluded since at least\nthree data points are needed for the test.\n5.3\nDelving into Illusion Categories\nWe provide a more granular analysis by examin-\ning each type of illusion, presenting the human-\nlike rates for each category in Figure 7. The re-\nsults depicted here are sourced from the largest\nmodel within each model family, namely Unified-\nIO Huge, OFA Huge, LLaVA Vicuna-13B, and\nInstructBLIP Vicuna-13B. Our observation reveals\nthat the perspective category demonstrates the high-\nest degree of alignment between machines and hu-\nmans. On the contrary, color constancy illusions\nemerge as the category with the least congruity in\nrelation to human responses.\n5.4\nUnderstanding the Cause of Illusions\nTo gain insight into model predictions under the in-\nfluence of illusions, we analyze the attention distri-\nbutions of OFA models in the RefLoc task. Specif-\nically, we compute the attention weight from the\nlocalization query (e.g., \"the smaller orange ball\")\nto the object representation of either a \"humanlike\"\nor \"counter-humanlike\" perception under illusions.\nAs depicted by the dark blue and light blue bars\nin Figure 8, as the model size increases, attention\nweights lean more towards the humanlike selec-\ntion. This trend is consistent with the humanlike\nrate observed for the RefLoc task in Figure 6. To\ndetermine if this bias stems from the illusion, we\nalso calculate attention weights for images without\nthe illusion inducer (represented by orange bars).\nThese weights are nearly equally distributed across\nboth objects, suggesting that the illusion does in-\ndeed influence the model\u2019s attention and biases its\npredictions similarly to human perceptions.\nFigure 9 shows an example using the attention\nvisualization tool (Aflalo et al., 2022). The first\nimage displays the original illusion image, with\ntwo orange balls of identical size while the left\nball seems smaller. The second image is devoid\nof the illusion inducer, while the third image arti-\nficially enlarges the right orange ball. Attention\nmaps corresponding to the \"smaller orange ball\"\nquery3 are shown adjacent to each image. In the\noriginal illusion, the model predominantly focuses\non the left ball, aligning with human observations.\nWithout the illusion inducer, the query becomes\nambiguous, leading to a dispersed attention map.\nHowever, when an actual size difference is present,\nthe model\u2019s attention decisively shifts to the cor-\nrectly perceived smaller ball on the left. A compar-\nison of these attention maps highlights that while\nillusions can steer the model\u2019s attention similarly\nto humans, its effect is less pronounced than when\na real disparity exists.\n6\nDiscussion and Conclusion\nWe introduce GVIL, the first dataset facilitating a\nsystematic evaluation of machine visual illusion via\nnatural language. Evaluating four distinct series\nof state-of-the-art vision-language model families\nacross varying scales, we observe a notable align-\nment between these models and human perceptions\nduring object localization in the context of illusions.\nInterestingly, this alignment tends to strengthen\nwith increased model size. Conversely, many mod-\nels face challenges in mirroring human perspec-\ntives within visual question-answering tasks. Our\npreliminary observations underscore the need for\nfurther discussions in two directions:\nAssessment of Vision-Language Models in the\nRealm of Visual Illusions.\nVision-language\nmodels have demonstrated commendable prowess\nin both visual and language understanding. Yet, a\nnotable gap persists in assessing their performance\nin the presence of visual illusions.\nGiven that\nsuch illusions are intrinsic to human perception,\noverlooking this facet may contribute to misalign-\nment between human and AI interpretations during\nreal-world engagements. While our study unveils\ncertain trends, like the correlation between model\nsize and human-model alignment, making defini-\ntive assertions is non-trivial due to the discrepancy\nin model architectures and their training datasets.\nThrough GVIL, we aspire to catalyze further re-\nsearch that addresses visual illusion in VLMs.\n3We use the second last layer of the OFA large model, as\nthe overall attention score of this layer is the highest. Atten-\ntions from all the heads are averaged.\nGaining Insights from Illusions.\nExploring the\neffects of visual illusions can offer fresh per-\nspectives to comprehend the intricacies of vision-\nlanguage models. Visual illusion, in some way, is\nsimilar to various types of values shared by our\nhuman society, but not shared among today\u2019s AI\nmodels. Given the rapidly growing applications\nof large AI models, it\u2019s important to identify and\nunderstand various aspects of alignment between\nthese models and humans. Vision illusion is only\nan example among many possibilities for future\nstudies.\nLimitations\nThis work is only the initial attempt to the question\nand there are many limitations which we think of as\nexciting future research directions. First of all, al-\nthough our experiments yields some interesting em-\npirical findings, it is not clear why different forms\nof tasks (e.g., QA-based tasks vs. RefLoc) lead to a\nstark contrast in the results. As these findings may\nhave implications in future technology that adapt\nto visual illusions, more in-depth understanding of\nthese behaviors will be needed in the future. Sec-\nond, our benchmark is currently small in size. It\nlays an infrastructure for this work. Future efforts\nto collect more data to form a centralized reposi-\ntory will be desired for studying visual illusions in\nboth humans and machines. Third, our investiga-\ntion is only based on a manually collected dataset\nfor our intellectual curiosity. The construction of\nthis dataset has the limitations that the effect of\nvisual illusions are not validated by a wider range\nof human subjects other than the authors. While\nit has motivation in improving situated language\ncommunication with embodied agents in the physi-\ncal world, how visual illusions play in perceiving\nand communicating about the real physical world\nremains an interesting question.\nEthics Statement\nThe data are collected and annotated by the au-\nthors without the involvement of any other human\nsubject. Data samples are selected from a wide\nliterature search on the subject of visual illusions.\nAcknowledgements\nThis work was supported by NSF IIS-1949634 and\nthe DARPA PTG program HR00112220003. The\nauthors would like to thank the anonymous review-\ners for their valuable comments and suggestions.\nReferences\nEdward H Adelson. 1995. Checkershadow illusion.\nMahmoud Afifi and Michael S Brown. 2019. What else\ncan fool deep learning? addressing color constancy\nerrors on deep neural network performance. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision, pages 243\u2013252.\nEstelle Aflalo, Meng Du, Shao-Yen Tseng, Yongfei\nLiu, Chenfei Wu, Nan Duan, and Vasudev Lal. 2022.\nVl-interpret: An interactive visualization tool for\ninterpreting vision-language transformers. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 21406\u201321415.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\net al. 2022. Flamingo: a visual language model for\nfew-shot learning. arXiv preprint arXiv:2204.14198.\nA Benjamin, Cheng Qiu, Ling-Qi Zhang, K Kording,\nand A Stocker. 2019. Shared visual illusions between\nhumans and artificial neural networks. In 2019 Con-\nference on Cognitive Computational Neuroscience,\nvolume 10, pages 2019\u20131299.\nClaus-Christian Carbon. 2014. Understanding human\nperception by human-made illusions. Frontiers in\nhuman neuroscience, 8:566.\nDavid P Carey. 2001. Do action systems resist visual\nillusions? Trends in cognitive sciences, 5(3):109\u2013\n113.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning.\narXiv preprint\narXiv:2305.06500.\nRH Day. 1984. The nature of, perceptual illusions. In-\nterdisciplinary Science Reviews, 9(1):47\u201358.\nFranz Joseph Delboeuf. 1865. Note sur certaines illu-\nsions d\u2019optique: Essai d\u2019une th\u00e9orie psychophysique\nde la maniere dont l\u2019oeil appr\u00e9cie les distances et les\nangles. Bulletins de l\u2019Acad\u00e9mie Royale des Sciences,\n19:195\u2013216.\nJinyu Fan and Yi Zeng. 2023. Challenging deep learn-\ning models with image distortion based on the abut-\nting grating illusion. Patterns, 4(3).\nVolker H Franz. 2001. Action does not resist visual\nillusions. Trends in cognitive sciences, 5(11):457\u2013\n459.\nMaurizio Gentilucci, Sergio Chieffi, Elena Daprati,\nM Cristina Saetti, and Ivan Toni. 1996. Visual illu-\nsion and action. Neuropsychologia, 34(5):369\u2013376.\nAlex Gomez-Villa, Adri\u00e1n Mart\u00edn, Javier Vazquez-\nCorral, Marcelo Bertalm\u00edo, and Jes\u00fas Malo. 2022.\nOn the synthesis of visual illusions using deep gener-\native models. Journal of Vision, 22(8):2\u20132.\nAlexander Gomez-Villa, Adrian Martin, Javier Vazquez-\nCorral, and Marcelo Bertalm\u00edo. 2019. Convolutional\nneural networks can be deceived by visual illusions.\nIn Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 12309\u2013\n12317.\nAlexander Gomez-Villa, Adrian Mart\u00edn, Javier Vazquez-\nCorral, Marcelo Bertalm\u00edo, and Jes\u00fas Malo. 2020.\nColor illusions also deceive cnns for low-level vision\ntasks: Analysis and implications. Vision Research,\n176:156\u2013174.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 6904\u20136913.\nRichard L Gregory. 1997. Visual illusions classified.\nTrends in cognitive sciences, 1(5):190\u2013194.\nElad Hirsch and Ayellet Tal. 2020. Color visual illu-\nsions: A statistics-based computational model. Ad-\nvances in Neural Information Processing Systems,\n33:9447\u20139458.\nJoseph Jastrow. 1892. Studies from the laboratory of ex-\nperimental psychology of the university of wisconsin.\nii. The American Journal of Psychology, 4(3):381\u2013\n428.\nAkiyoshi Kitaoka. 2010. A brief classification of colour\nillusions. Colour: Design & Creativity, 5(3):1\u20139.\nMichal Kosinski. 2023. Theory of mind may have spon-\ntaneously emerged in large language models. arXiv\npreprint arXiv:2302.02083.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nBen Lonnqvist, Alban Bornet, Adrien Doerig, and\nMichael H Herzog. 2021. A comparative biology ap-\nproach to dnn modeling of vision: A focus on differ-\nences, not similarities. Journal of Vision, 21(10):17\u2013\n17.\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh\nMottaghi, and Aniruddha Kembhavi. 2022. Unified-\nio: A unified model for vision, language, and multi-\nmodal tasks. arXiv preprint arXiv:2206.08916.\nBruce MacEvoy. 2005. Handprint : Color vision.\nDominique Makowski,\nZen J Lau,\nTam Pham,\nW Paul Boyce, and SH Annabel Chen. 2021. A para-\nmetric framework to generate visual illusions using\npython. Perception, 50(11):950\u2013965.\nFC M\u00fcller-Lyer. 1889. Optische urteilst\u00e4uschungen.\nArchiv f\u00fcr Physiologie Suppl, pages 263\u2013270.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nMario Ponzo. 1911. Intorno ad alcune illusioni nel\ncampo delle sensazioni tattili sull. Archives Itali-\nennes de Biologie.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748\u20138763. PMLR.\nJames Outram Robinson. 2013. The psychology of vi-\nsual illusion. Courier Corporation.\nEric D Sun and Ron Dekel. 2021. Imagenet-trained\ndeep neural networks exhibit illusion-like response\nto the scintillating grid. Journal of Vision, 21(11):15\u2013\n15.\nDejan Todorovi\u00b4c. 2020. What are visual illusions? Per-\nception, 49(11):1128\u20131199.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022. Ofa: Unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning, pages\n23318\u201323340. PMLR.\nA\nStatistical Analysis for Illusion\nRecognition\nWe conducted a statistical analysis to rigorously\nvalidate that our experimental setup is able to reveal\na model\u2019s inclination towards either \u201chumanlike\" or\n\u201cno-illusion\", notwithstanding the high prevalence\nof N/A samples. Specifically, we applied a \u03c72-test\nto the model predictions omitting the N/A samples.\nThe null hypothesis posits that the humanlike and\nno-illusion samples are uniformly distributed, i.e.,\nthe model behaviors randomly. As shown in Table\n4, a significant proportion of the models reject the\nnull hypothesis. Out of the 12 models tested, 8\nmodels rejected the null hypothesis with a p-value\n< 0.001, and 9 models rejected the null hypothe-\nsis with a p-value < 0.01. These figures strongly\nsuggest that most models perform better than what\nwould be expected by chance alone, which is a\npiece of strong evidence that our dataset and exper-\nimental setup can support the evaluation of illusion\nrecognition for the tested VLMs.\nModel\n#HL\n#NI\n\u03c72\np-value\nOFA-Tiny\n17\n172\n129.45\n<0.001***\nOFA-Base\n78\n146\n20.64\n<0.001***\nOFA-Large\n58\n73\n1.72\n0.190\nOFA-Huge\n60\n136\n29.47\n<0.001***\nUnifiedIO-Small\n33\n221\n139.15\n<0.001***\nUnifiedIO-Base\n33\n62\n9.38\n0.002**\nUnifiedIO-Large\n115\n103\n0.66\n0.416\nUnifiedIO-XL\n142\n34\n66.27\n<0.001***\nLLaVA-7B\n13\n202\n12.89\n<0.001***\nLLaVA-13B\n6\n195\n177.72\n<0.001***\nInstructBLIP-7B\n91\n223\n55.49\n<0.001***\nInstructBLIP-13B\n107\n121\n0.86\n0.354\nTable 4: Chi-square test for the SameDiff Task (Figure\n5). #HL and #NI denote the number of humanlike illu-\nsionary answers and no-illusion answers, respectively.\nStatistically significant results with p < 0.001 and p <\n0.05 are marked with *** and **. respectively.\n"
  },
  {
    "title": "ChipNeMo: Domain-Adapted LLMs for Chip Design",
    "link": "https://arxiv.org/pdf/2311.00176.pdf",
    "upvote": "7",
    "text": "ChipNeMo: Domain-Adapted LLMs for Chip Design\nMingjie Liu * 1 Teodor-Dumitru Ene * 1 Robert Kirby * 1 Chris Cheng * 1 Nathaniel Pinckney * 1\nRongjian Liang * 1 Jonah Alben 1 Himyanshu Anand 1 Sanmitra Banerjee 1 Ismet Bayraktaroglu 1\nBonita Bhaskaran 1 Bryan Catanzaro 1 Arjun Chaudhuri 1 Sharon Clay 1 Bill Dally 1 Laura Dang 1\nParikshit Deshpande 1 Siddhanth Dhodhi 1 Sameer Halepete 1 Eric Hill 1 Jiashang Hu 1 Sumit Jain 1\nAnkit Jindal 1 Brucek Khailany 1 George Kokai 1 Kishor Kunal 1 Xiaowei Li 1 Charley Lind 1 Hao Liu 1\nStuart Oberman 1 Sujeet Omar 1 Ghasem Pasandi 1 Sreedhar Pratty 1 Jonathan Raiman 1 Ambar Sarkar 1\nZhengjiang Shao 1 Hanfei Sun 1 Pratik P Suthar 1 Varun Tej 1 Walker Turner 1 Kaizhe Xu 1 Haoxing Ren 1\nAbstract\nChipNeMo aims to explore the applications of\nlarge language models (LLMs) for industrial chip\ndesign.\nInstead of directly deploying off-the-\nshelf commercial or open-source LLMs, we in-\nstead adopt the following domain adaptation tech-\nniques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models.\nWe evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities.\nIn particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.\n1. Introduction\nOver the last few decades, Electronic Design Automation\n(EDA) algorithms and tools have provided huge gains in\nchip design productivity. Coupled with the exponential\nincreases in transistor densities provided by Moore\u2019s law,\nEDA has enabled the development of feature-rich complex\nSoC designs with billions of transistors. More recently, re-\n*Equal contribution 1NVIDIA.\nsearchers have been exploring ways to apply AI to EDA al-\ngorithms and the chip design process to further improve chip\ndesign productivity (Khailany et al., 2020; Ren & Fojtik,\n2021; Roy et al., 2021). However, many time-consuming\nchip design tasks that involve interfacing with natural lan-\nguages or programming languages still have not been auto-\nmated. The latest advancements in commercial (ChatGPT,\nBard, etc.) and open-source (Vicuna (Chiang et al., 2023),\nLLaMA2 (Touvron et al., 2023), etc.) large language mod-\nels (LLM) provide an unprecedented opportunity to help\nautomate these language-related chip design tasks. Indeed,\nearly academic research (Thakur et al., 2023; Blocklove\net al., 2023; He et al., 2023) has explored applications of\nLLMs for generating Register Transfer Level (RTL) code\nthat can perform simple tasks in small design modules as\nwell as generating scripts for EDA tools.\nWe believe that LLMs have the potential to help chip de-\nsign productivity by using generative AI to automate many\nlanguage-related chip design tasks such as code generation,\nresponses to engineering questions via a natural language\ninterface, analysis and report generation, and bug triage. In\nthis study, we focus on three specific LLM applications: an\nengineering assistant chatbot for GPU ASIC and Architec-\nture design engineers, which understands internal hardware\ndesigns and is capable of explaining complex design top-\nics; EDA scripts generation for two domain specific tools\nbased on Python and Tcl for VLSI timing analysis tasks\nspecified in English; bug summarization and analysis as\npart of an internal bug and issue tracking system.\nAlthough general-purpose LLMs trained on vast amounts of\ninternet data exhibit remarkable capabilities in generative AI\ntasks across diverse domains (as demonstrated in (Bubeck\net al., 2023)), recent work such as BloombergGPT (Wu et al.,\n2023) and BioMedLLM (Venigalla et al., 2022) demonstrate\nthat domain-specific LLM models can outperform a gen-\neral purpose model on domain-specific tasks. In the hard-\nware design domain, (Thakur et al., 2023; Liu et al., 2023)\nshowed that open-source LLMs (CodeGen (Nijkamp et al.,\n1\narXiv:2311.00176v4  [cs.CL]  7 Mar 2024\nChipNeMo: Domain-Adapted LLMs for Chip Design\n2\nDomain-Adaptive\nPretraining\n24B tokens of chip \ndesign docs/code\nThousands GPU hrs\nModel\nAlignment\n56K/128K \n(SteerLM/SFT) insts\n+ 1.4K task insts\n100+ GPU hrs\nFoundation Models\nLLaMA2 \n(7B, 13B, 70B) \nChipNeMo \nChat Models\n(7B, 13B, 70B)\nChipNeMo \nFoundation Models\n(7B, 13B, 70B)\nPretraining\nTrillions tokens of \ninternet data\n105 \u2013 106 GPU hrs\nFigure 1: ChipNeMo Training Flow\n2023)) fine-tuned on additional Verilog data can outperform\nstate-of-art OpenAI GPT-3.5 models. Customizing LLMs\nin this manner also avoids security risks associated with\nsending proprietary chip design data to third party LLMs\nvia APIs. However, it would be prohibitively expensive to\ntrain domain-specific models for every domain from scratch,\nsince this often requires millions of GPU training hours. To\ncost-effectively train domain-specific models, we instead\npropose to combine the following techniques: Domain-\nAdaptive Pre-Training (DAPT) (Gururangan et al., 2020) of\nfoundation models with domain-adapted tokenizers, model\nalignment using general and domain-specific instructions,\nand retrieval-augmented generation (RAG) (Lewis et al.,\n2021b) with a trained domain-adapted retrieval model.\nAs shown in Figure 1, our approach is to start with a base\nfoundational model and apply DAPT followed by model\nalignment. DAPT, also known as continued pretraining with\nin-domain data, has been shown to be effective in areas such\nas biomedical and computer science publications, news, and\nreviews. In our case, we construct our domain-specific pre-\ntraining dataset from a collection of proprietary hardware-\nrelated code (e.g. software, RTL, verification testbenches,\netc.) and natural language datasets (e.g. hardware specifi-\ncations, documentation, etc.). We clean up and preprocess\nthe raw dataset, then continued-pretrain a foundation model\nwith the domain-specific data. We call the resulting model a\nChipNeMo foundation model. DAPT is done on a fraction\nof the tokens used in pre-training, and is much cheaper, only\nrequiring roughly 1.5% of the pretraining compute.\nLLM tokenizers convert text into sequences of tokens for\ntraining and inference. A domain-adapted tokenizer im-\nproves the tokenization efficiency by tailoring rules and\npatterns for domain-specific terms such as keywords com-\nmonly found in RTL. For DAPT, we cannot retrain a new\ndomain-specific tokenizer from scratch, since it would make\nthe foundation model invalid. Instead of restricting Chip-\nNeMo to the pre-trained general-purpose tokenizer used\nby the foundation model, we instead adapt the pre-trained\ntokenizer to our chip design dataset, only adding new tokens\nfor domain-specific terms.\nChipNeMo foundation models are completion models which\nrequire model alignment to adapt to tasks such as chat.\nWe use largely publicly available general-purpose chat in-\nstruction datasets for multi-turn chat together with a small\namount of domain-specific instruction datasets to perform\nalignment on the ChipNeMo foundation model, which pro-\nduces the ChipNeMo chat model. We observe that align-\nment with a general purpose chat instruction dataset is\nadequate to align the ChipNeMo foundation models with\nqueries in the chip design domain. We also added a small\namount of task-specific instruction data, which further im-\nproves the alignment. We trained multiple ChipNeMo foun-\ndation and chat models based on variants of LLaMA2 mod-\nels used as the base foundation model.\nTo improve performance on the engineering assistant chat-\nbot application, we also leverage Retrieval Augmented Gen-\neration (RAG). RAG is an open-book approach for giving\nLLMs precise context for user queries. It retrieves rele-\nvant in-domain knowledge from its data store to augment\nthe response generation given a user query. This method\nshows significant improvement in grounding the model to\nthe context of a particular question. Crucially we observed\nsignificant improvements in retrieval hit rate when finetun-\ning a pretrained retrieval model with domain data. This led\nto even further improvements in model quality.\nOur results show that domain-adaptive pretraining was the\nprimary technique driving enhanced performance in domain-\nspecific tasks. We highlight the following contributions and\nfindings for adapting LLMs to the chip design domain:\n\u2022 We demonstrate domain-adapted LLM effectiveness on\nthree use-cases: an engineering assistant chatbot, EDA\ntool script generation, and bug summarization and anal-\nysis. We achieve a score of 6.0 on a 7 point Likert scale\nfor engineering assistant chatbot based on expert eval-\nuations, more than 70% correctness on the generation\nof simple EDA scripts, and expert evaluation ratings\nabove 5 on a 7 point scale for summarizations and\nassignment identification tasks.\n\u2022 Domain-adapted ChipNeMo models dramatically out-\nperforms all vanilla LLMs evaluated on both multiple-\nchoice domain-specific AutoEval benchmarks and hu-\nman evaluations for applications.\n\u2022 Using the SteerLM alignment method (Dong et al.,\n2023) over traditional SFT improves human evaluation\nscores for the engineering assistant chatbot by 0.62\npoints on a 7 point Likert scale.\n\u2022 SFT on an additional 1.4K domain-specific instruc-\ntions significantly improves the model\u2019s proficiency at\ngenerating correct EDA tool scripts by 18%.\n\u2022 Domain-adaptive tokenization reduce domain data to-\nken count by up to 3.3% without hurting effectiveness\non applications.\n\u2022 Fine-tuning our ChipNeMo retrieval model with\n2\nChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus\nof domain data, model alignment to domain specific tasks,\nand retrieval-augmented generation with a fine-tuned re-\ntrieval model. We illustrate the details of each technique in\nthis section.\n2.1. Domain-Adaptive Tokenization\nWhen adapting a pre-trained tokenizer, the main goals\nare to improve tokenization efficiency on domain-specific\ndata, maintain language model performance on general\ndatasets, and minimize the effort for retraining/fine-tuning.\nTo achieve this, we developed the following approach:\n1. Train a tokenizer from scratch using domain-specific\ndata.\n2. From the vocabulary of the new tokenizer, identifying\ntokens that are absent in the general-purpose tokenizer\nand are rarely found in general-purpose datasets.\n3. Expand the general-purpose tokenizer with the newly\nidentified tokens at Step 2.\n4. Initialize model embeddings of the new tokens by uti-\nlizing the general-purpose tokenizer.\nSpecifically for Step 4, when a new token is encountered,\nit is first re-tokenized using the original pretrained general-\npurpose tokenizer. The LLM\u2019s token embedding for the new\ntoken is determined by averaging the embeddings of the\ntokens generated by the general-purpose tokenizer (Koto\net al., 2021). The LLM\u2019s final output layer weights for the\nnew tokens are initialized to zero.\nStep 2 helps maintain the performance of the pre-trained\nLLM on general datasets by selectively introducing new\ntokens that are infrequently encountered in general-purpose\ndatasets. Step 4 reduces the effort required for retraining or\nfinetuning the LLM via initialization of the embeddings of\nnew tokens guided by the general-purpose tokenizer.\n2.2. Domain Adaptive Pretraining\nIn our study, we apply DAPT on pretrained foundation base\nmodels: LLaMA2 7B/13B/70B. Each DAPT model is ini-\ntialized using the weights of their corresponding pretrained\nfoundational base models. We name our domain-adapted\nmodels ChipNeMo. We employ tokenizer augmentation\nas depicted in Section 2.1 and initialize embedding weight\naccordingly (Koto et al., 2021). We conduct further pre-\ntraining on domain-specific data by employing the standard\nautoregressive language modeling objective. All model\ntraining procedures are conducted using the NVIDIA NeMo\nframework (Kuchaiev et al., 2019), incorporating techniques\nsuch as tensor parallelism (Shoeybi et al., 2019) and flash\nattention (Dao et al., 2022) for enhanced efficiency.\nOur models undergo a consistent training regimen with\nsimilar configurations. A small learning rate of 5 \u00b7 10\u22126\nis employed, and training is facilitated using the Adam\noptimizer, without the use of learning rate schedulers. The\nglobal batch size is set at 256, and a context window of 4096\ntokens is applied, resulting in an effective batch size of 1M\ntokens. The total number of training steps is set to 23,200,\nequating to roughly 1 epoch of the data blend.\nFigure 2: Smoothed Training Loss for ChipNeMo with Tokenizer\nAugmentation.\nFigure 2 illustrates the training loss of ChipNeMo under\nthe specified hyperparameters. We do observe spikes in the\ntraining loss. In contrast to the hypothesis in (Chowdhery\net al., 2022), we postulate that in our scenario, these spikes\ncan be attributed to \u201cbad data\u201d since these irregularities\nseem to consistently occur in similar training steps for the\nsame model, even across different model sizes. We chose\nnot to address this issue, as these anomalies did not appear\nto significantly impede subsequent training steps (with no\nnoticeable degradation in validation loss), possibly due to\n3\nChipNeMo: Domain-Adapted LLMs for Chip Design\nour application of a low learning rate.\nWe refer readers to Appendix for details on the training data\ncollection process A.2, training data blend A.3, and imple-\nmentation details and ablation studies on domain-adaptive\npretraining A.6.\n2.3. Model Alignment\nAfter DAPT, we perform model alignment. We specifically\nleverage two alignment techniques: supervised fine-tuning\n(SFT) and SteerLM (Dong et al., 2023). We adopt the iden-\ntical hyperparameter training configuration as DAPT for all\nmodels, with the exception of using a reduced global batch\nsize of 128. We employ an autoregressive optimization ob-\njective, implementing a strategy where losses associated\nwith tokens originating from the system and user prompts\nare masked (Touvron et al., 2023). This approach ensures\nthat during backpropagation, our focus is exclusively di-\nrected towards the optimization of answer tokens.\nWe combined our domain alignment dataset, consisting\nof approximately 1.4k samples, with larger general chat\ndatasets. For SFT, we blended the domain instructional\ndata with 128k commercial-viable chat data and then per-\nformed fine-tuning for a single epoch after random shuffling.\nWe conducted experiments involving augmentation of the\ndomain-specific SFT dataset for more than one epoch. How-\never, it became apparent that the model rapidly exhibited\nsigns of overfitting when presented with in-domain ques-\ntions, often repeating irrelevant answers from the domain\nSFT dataset. For SteerLM, we closely followed the steps\noutlined in (Wang et al., 2023). We first trained an attribute\nmodel instantiated with LLaMA2-13B model on the Help-\nSteer and OASST datasets. We then used the attribute model\nto label all attributes for OASST data and our domain in-\nstructional data. Finally, we conducted attribute-conditioned\nfine-tuning and also masked the attribute labels and trained\non ChipNeMo models for 2 epochs. We refer readers to\nAppendix A.4 for details on the alignment datasets and A.7\non implementations details.\nWe also experimented with DAPT directly on a chat aligned\nmodel, such as the LLaMA2-Chat model. We found that\nDAPT significantly degraded the model\u2019s alignment, mak-\ning the resulting model useless for downstream tasks.\n2.4. Domain-Adapted Retrieval Model\nIt is well known that LLMs can generate inaccurate text,\nso-called hallucination (Ji et al., 2023). Although the phe-\nnomenon is not completely understood, we still must miti-\ngate hallucinations since they are particularly problematic\nin an engineering assistant chatbot context, where accu-\nracy is critical. Our proposal is to leverage the retrieval\naugmented generation (RAG) method. RAG tries to re-\ntrieve relevant passages from a database to be included in\nthe prompt together with the question, which grounds the\nLLM to produce more accurate answers. We find that using\na domain adapted language model for RAG significantly\nimproves answer quality on our domain specific questions.\nAlso, we find that fine-tuning an off-the-shelf unsupervised\npre-trained dense retrieval model with a modest amount\nof domain specific training data significantly improves re-\ntrieval accuracy. Our domain-adapted RAG implementation\ndiagram is illustrated on Figure 3.\nFigure 3: RAG Implementation Variations\nWe created our domain adapted retrieval model by fine-\ntuning the e5 small unsupervised model (Wang et al., 2022)\nwith 3000 domain specific auto-generated samples using the\nTevatron framework (Gao et al., 2022). We refer readers to\nthe details on the sample generation and training process in\nAppendix A.8.\nEven with the significant gains that come with fine-tuning a\nretrieval model, the fact remains that retrieval still struggles\nwith queries that do not map directly to passages in the\ndocument corpus or require more context not present in\nthe passage. Unfortunately, these queries are also more\nrepresentative of queries that will be asked by engineers in\nreal situations. Combining retrieval with a domain adapted\nlanguage model is one way to address this issue.\n3. Evaluations\nWe evaluate our training methodology and application per-\nformance in this section. We study our 7B, 13B, and 70B\nmodels in the training methodology evaluation, and only our\nChipNeMo-70B model using SteerLM for model alignment\nin the application performance evaluation. For compari-\nson, we also evaluate two baseline chat models: LLaMA2-\n70B-Chat and GPT-4. LLaMA2-70B-Chat is the publicly\nreleased LLaMA2-Chat model trained with RLHF and is\nconsidered to be the state-of-the-art open-source chat model,\nwhile GPT-4 is considered to be the state-of-the-art propri-\netary chat model.\n4\nChipNeMo: Domain-Adapted LLMs for Chip Design\nFigure 4: Domain-Adapted ChipNeMo Tokenizer Improvements.\n3.1. Domain-Adaptive Tokenization\nWe adapt the LLaMA2 tokenizer (containing 32K tokens) to\nchip design datasets using the previously outlined four-step\nprocess. Approximately 9K new tokens are added to the\nLLaMA2 tokenizer. The adapted tokenizers can improve\ntokenization efficiency by 1.6% to 3.3% across various chip\ndesign datasets as shown in Figure 4. We observe no obvious\nchanges to tokenizer efficiency on public data. Importantly,\nwe have not observed significant decline in the LLM\u2019s accu-\nracy on public benchmarks when using the domain-adapted\ntokenizers even prior to DAPT.\n3.2. Domain Adaptive Pretraining\nFigure 5: Chip Domain Benchmark Result for ChipNeMo.\nFigure 5 presents the outcomes for ChipNeMo models on\nthe AutoEval benchmark for chip design domain (detailed\nin Appendix A.5). Results on open domain academic bench-\nmark results are presented in Appendix A.6. Our research\nfindings can be summarized as follows:\n\u2022 DAPT exerts a substantial positive impact on tasks\nwithin the domain itself. This effect is manifested in\nsignificant improvements in internal design knowledge\nas well as general circuit design knowledge.\n\u2022 DAPT models exhibit a slight degradation in perfor-\nmance on open-domain academic benchmarks.\n\u2022 The use of larger and more performant foundational\nmodels yields better zero-shot results on domain-\nspecific tasks. Furthermore, the employment of su-\nperior base models results in enhanced domain models\npost-DAPT, leading to heightened performance on in-\ndomain tasks.\n\u2022 Improvements attributed to DAPT with in-domain\ntasks exhibit a positive correlation with model size,\nwith larger models demonstrating more pronounced\nenhancements in domain-specific task performance.\n3.3. Training Ablation Studies\nFor our ablation studies, we conducted multiple rounds of\ndomain adaptive pre-training. We provide brief summaries\nand refer to the Appendix A.6 for details.\nThe differences between training with the augmented tok-\nenizer and the original tokenizer appeared to be negligible.\nWe thus primarily attribute the accuracy degradation on\nopen-domain academic benchmarks to domain data. More-\nover, the removal of the public dataset only slightly re-\ngressed on most tasks including academic benchmarks.\nIn our exploration, we experimented with employing a larger\nlearning rate, as in CodeLLaMA (Rozi`ere et al., 2023). We\nobserved large spikes in training loss at the initial training\nsteps. Although this approach eventually led to improved\ntraining and validation loss, we noted substantial degrada-\ntions across all domain-specific and academic benchmarks,\nexcept on coding. We hypothesize that a smaller learning\nrate played a dual role, facilitating the distillation of domain\nknowledge through DAPT while maintaining a balance that\ndid not veer too far from the base model, thus preserving\ngeneral natural language capabilities.\nWe also explored the application of Parameter Efficient\nFine-Tuning (PEFT) in the context of Domain-Adaptive\nPre-training (DAPT). In this pursuit, we conducted two ex-\nperiments involving the incorporation of LoRA adapters (Hu\net al., 2021), introducing additional parameters of 26.4 mil-\nlion (small) and 211.2 million (large) respectively. In both\ninstances, our findings revealed a significant accuracy gap\non in-domain tasks when compared to the full-parameter\nDAPT approach. Furthermore, when contrasting the out-\ncomes between small and large PEFT models, we observed\na marginal enhancement on in-domain task accuracy, with\nlarge adapter exhibiting a slight improvement.\n3.4. Training Cost\nAll models have undergone training using 128 A100 GPUs.\nWe estimate the costs associated with domain adaptive pre-\ntraining for ChipNeMo as illustrated in Table 1. It is worth\nnoting that DAPT accounts for less than 1.5% of the overall\n5\nChipNeMo: Domain-Adapted LLMs for Chip Design\ncost of pretraining a foundational model from scratch.\nModel Size\nPretraining\nDAPT\nSFT\n7B\n184,320\n2,620\n90\n13B\n368,640\n4,940\n160\n70B\n1,720,320\n20,500\n840\nTable 1: Training cost of LLaMA2 models in A100 GPU hours.\nPretraining cost from (Touvron et al., 2023).\n3.5. RAG and Engineering Assistant Chatbot\nWe created a benchmark to evaluate the performance of\ndesign chat assistance, which uses the RAG method. This\nbenchmark includes 88 questions in three categories: archi-\ntecture/design/verification specifications (Specs), testbench\nregression documentation (Testbench), and build infrastruc-\nture documentation (Build). For each question, we specify\nthe golden answer as well as the paragraphs in the design\ndocument that contains the relevant knowledge for the an-\nswer. These questions are created by designers manually\nbased on a set of design documents as the data store for\nretrieval. It includes about 1.8K documents, which were\nsegmented into 67K passages, each about 512 characters.\nFirst, we compare our domain adapted retrieval model with\nSentence Transformer (Reimers & Gurevych, 2019) and\ne5 small unsupervised (Wang et al., 2022) on each category.\nEach model fetches its top 8 passages from the data store.\nAs shown in Figure 6, our domain-adapted model performed\n2x better than the original e5 small unsupervised model and\n30% better than sentence transformer.\nFigure 6: Retrieval Model Accuracy Comparison\nThe queries in the Specs category are derived directly from\npassages in the documents, so their answers are often nicely\ncontained in a concise passage and clearly address the query.\nOn the other hand, the queries of the Testbench and Build\ncategories are not directly derived from passages, so their\nanswers were often not as apparent in the fetched passages\nand required more context (see Appendix A.8 for detailed\nexamples). This significantly contributes to the difference\nin retrieval quality between the categories.\nFigure 7: Human Evaluation of Different Models. Model Only\nrepresents results without RAG. RAG (hit)/(miss) only include\nquestions whose retrieved passages hit/miss their ideal context,\nRAG (avg) includes all questions. 7 point Likert scale.\nWe conducted evaluation of multiple ChipNeMo models\nand LLaMA2 models with and without RAG. The results\nwere then scored by human evaluators on a 7 point Likert\nscale and shown in Figure 7. We highlight the following:\n\u2022 ChipNeMo-70B-Steer outperforms GPT-4 in all cate-\ngories, including both RAG misses and hits.\n\u2022 ChipNeMo-70B-Steer\noutperforms\nsimilar\nsized\nLLaMA2-70b-Chat in model-only and RAG evalua-\ntions by 3.31 and 1.81, respectively.\nOur results indicate that RAG significantly boosts human\nscores. RAG improves ChipNeMo-70B-Steer, GPT-4, and\nLLaMA2-70b-Chat by 0.56, 1.68, and 2.05, respectively.\nEven when RAG misses, scores are generally higher than\nwithout using retrieval. The inclusion of relevant in-domain\ncontext still led to improved performance, as retrieval is not\na strictly binary outcome. Furthermore, while ChipNeMo-\n70B-SFT outperforms GPT4 by a large margin through\ntraditional supervised fine-tuning, applying SteerLM meth-\nods (Wang et al., 2023) leads to further elevated chatbot\nratings. We refer readers to the complete evaluation results\nin Appendix A.9.\n3.6. EDA Script Generation\nIn order to evaluate our model on the EDA script generation\ntask, we created two different types of benchmarks. The first\nis a set of \u201cEasy\u201d and \u201cMedium\u201d difficulty tasks (1-4 line\nsolutions) that can be evaluated without human intervention\nby comparing with a golden response or comparing the\ngenerated output after code execution. The second set of\ntasks \u201cHard\u201d come from real use case scenarios that our\nengineers chose. These tasks are much harder requiring\nmultiple API calls and understanding relationship between\n6\nChipNeMo: Domain-Adapted LLMs for Chip Design\ndifferent VLSI objects. Because these are hard to evaluate\nin an automated way (with current model performance), we\nhad human engineers judge the correctness between 0-10.\nWe evaluate the model on two tools, one is a fully in-house\nPython based tool and the other is a Tcl based EDA tool\nwith limited public data. The size of these benchmarks are\ndescribed in Table 2. Work is ongoing to both increase the\nsize and scope for these benchmarks to allow us to further\nassess and improve these models.\nEvaluation Benchmark Name\nSize\nPython Tool - Automatic (Easy)\n146\nPython Tool - Automatic (Medium)\n28\nPython Tool - Human (Hard)\n25\nTcl Tool - Automatic (Easy)\n708\nTcl Tool - Automatic (Medium)\n27\nTcl Tool - Human (Hard)\n25\nTable 2: EDA Script Generation Evaluation Benchmarks\nThe comparative performance of our models on these eval-\nuations are shown in Figures 8 and 9. Figure 8 shows the\nresults on automated \u201ceasy\u201d and \u201cmedium\u201d benchmarks\nwhere we check for fully accurate code. For \u201cHard\u201d bench-\nmarks in Figure 9 we check for partial correctness of the\ncode, which is evaluated by a human user on a 0-10 scale.\nChipNeMo-70B-Steer performs significantly better than off-\nthe-shelf GPT-4 and LLaMA2-70B-Chat model.\nFigure 8: EDA Script Generation Evaluation Results, Pass@5\nAs seen in Figure 8, models like GPT-4 and LLaMA2-70B-\nChat have close to zero accuracy for the Python tool where\nthe domain knowledge related to APIs of the tool are neces-\nsary. This shows the importance of DAPT. Without DAPT,\nthe model had little to no understanding of the underlying\nAPIs and performed poorly on both automatic and human\nevaluated benchmarks. Our aligned model further improved\nthe results of DAPT because our domain instructional data\nhelps guide the model to present the final script in the most\nuseful manner. An ablation study on inclusion of domain\ninstructional data for model alignment and the application\nof retrieval is provided in Appendix A.9.\nFigure 9: EDA Script Generation Evaluation Results, Single Gen-\neration (temperature=0), Human Evaluated 0-10 Point Scale.\nOur non-domain models performed better on our Tcl tool\nthan the Python tool, but the trend for our domain trained\nmodel was the opposite. We suspect this was due to the\nproprietary nature of our Python tool. It was difficult for\ngeneral LLMs to perform well on our Python tool bench-\nmark without knowledge of the APIs. Since ChipNeMo is\ntrained with domain data, the inherent python coding ability\nof the base model allows ChipNeMo-70B-Steer to perform\nbetter. This again highlights the importance of DAPT for\nlow-volume or proprietary programming languages.\nFigure 10: Bug Summarization and Analysis Evaluation Results, 7\npoint Likert scale.\n3.7. Bug Summarization and Analysis\nTo evaluate our models on bug summarization and analysis\nwe have a hold out set of 30 bugs which are ideal candidates\nfor summarization. This includes having a long comment\nhistory or other data which makes the bugs hard for a human\nto quickly summarize. As described in Appendix A.10.3\nthe long length of each individual bug requires the LLM to\nperform hierarchical summarization.\nWe study three separate sub-tasks: summarization focused\non technical details, summarization focused on manage-\nrial details, and a post-summarization recommendation of\n7\nChipNeMo: Domain-Adapted LLMs for Chip Design\ntask assignnment. Participants are tasked with rating the\nmodel\u2019s performance on a 7-point Likert scale for each of\nthese three assignments. The results can be found in Fig-\nure 10. Although the GPT-4 model excels in all three tasks,\noutperforming both our ChipNeMo-70B-Steer model and\nthe LLaMA2-70B-Chat model, ChipNeMo-70B-Steer does\nexhibit enhancements compared to the off-the-shelf LLaMA\nmodel of equivalent size. We attribute the comparatively\nlower improvements in summarization tasks resulting from\nour domain-adaptation to the limited necessity for domain-\nspecific knowledge in summarization compared to other\nuse-cases.\n4. Related Works\nMany domains have a significant amount of proprietary data\nwhich can be used to train a domain-specific LLM. One ap-\nproach is to train a domain specific foundation model from\nscratch, e.g., BloombergGPT(Wu et al., 2023) for finance,\nBioMedLLM(Venigalla et al., 2022) for biomed, and Galac-\ntica(Taylor et al., 2022) for science. These models were\nusually trained on more than 100B tokens of raw domain\ndata. The second approach is domain-adaptive pretraining\n(DAPT) (Gururangan et al., 2020) which continues to train\na pretrained foundation model on additional raw domain\ndata. It shows slight performance boost on domain-specific\ntasks in domains such as biomedical, computer science pub-\nlications, news, and reviews. In one example, (Lewkowycz\net al., 2022) continued-pretrained a foundation model on\ntechnical content datasets and achieved state-of-the-art per-\nformance on many quantitative reasoning tasks.\nRetrieval Augmented Generation (RAG) helps ground the\nLLM to generate accurate information and to extract up-to-\ndate information to improve knowledge-intensive NLP tasks\n(Lewis et al., 2021a). It is observed that smaller models with\nRAG can outperform larger models without RAG (Borgeaud\net al., 2022). Retrieval methods include sparse retrieval\nmethods such as TF-IDF or BM25(Robertson & Zaragoza,\n2009), which analyze word statistic information and find\nmatching documents with a high dimensional sparse vec-\ntor. Dense retrieval methods such as (Karpukhin et al.,\n2020; Izacard et al., 2022a) find matching documents on\nan embedding space generated by a retrieval model pre-\ntrained on a large corpus with or without fine-tuning on a\nretrieval dataset. The retrieval model can be trained stan-\ndalone (Karpukhin et al., 2020; Izacard et al., 2022a; Shi\net al., 2023) or jointly with language models (Izacard et al.,\n2022b; Borgeaud et al., 2022). In addition, it has been shown\nthat off-the-shelf general purpose retrievers can improve a\nbaseline language model significantly without further fine-\ntuning (Ram et al., 2023). RAG is also proposed to perform\ncode generation tasks (Zhou et al., 2023) by retrieving from\ncoding documents.\nFoundation models are completion models, which have lim-\nited chat and instruction following capabilities. Therefore, a\nmodel alignment process is applied to the foundation models\nto train a corresponding chat model. Instruction fine-tuning\n(Wei et al., 2022) and reinforcement learning from human\nfeedback (RLHF) (Ouyang et al., 2022) are two common\nmodel alignment techniques. Instruction fine-tuning further\ntrains a foundation model using instructions datasets. RLHF\nleverages human feedback to label a dataset to train a re-\nward model and applies reinforcement learning to further\nimprove models given the trained reward model. RLHF is\nusually more complex and resource hungry than instruction\nfine-tuning. Therefore, recent studies also propose to reduce\nthis overhead with simpler methods such as DPO (Rafailov\net al., 2023) and SteerLM (Dong et al., 2023).\nResearchers have started to apply LLM to chip design prob-\nlems. Early works such as Dave (Pearce et al., 2020) first\nexplored the possibility of generating Verilog from En-\nglish with a language model (GPT-2). Following that work,\n(Thakur et al., 2023) showed that fine-tuned open-source\nLLMs (CodeGen) on Verilog datasets collected from GitHub\nand Verilog textbooks outperformed state-of-the-art OpenAI\nmodels such as code-davinci-002 on 17 Verilog questions.\n(Liu et al., 2023) proposed a benchmark with more than\n150 problems and demonstrated that the Verilog code gen-\neration capability of pretrained language models could be\nimproved with supervised fine-tuning by bootstrapping with\nLLM generated synthetic problem-code pairs. Chip-Chat\n(Blocklove et al., 2023) experimented with conversational\nflows to design and verify a 8-bit accumulator-based micro-\nprocessor with GPT-4 and GPT-3.5. Their findings showed\nthat although GPT-4 produced relatively high-quality codes,\nit still does not perform well enough at understanding and\nfixing the errors. ChipEDA (He et al., 2023) proposed to use\nLLMs to generate EDA tools scripts. It also demonstrated\nthat fine-tuned LLaMA2 70B model outperforms GPT-4\nmodel on this task.\n5. Conclusions\nWe explored domain-adapted approaches to improve LLM\nperformance for industrial chip design tasks. Our results\nshow that domain-adaptive pretrained models, such as the\n7B, 13B, and 70B variants of ChipNeMo, achieve simi-\nlar or better results than their base LLaMA2 models with\nonly 1.5% additional pretraining compute cost. Our largest\ntrained model, ChipNeMo-70B, also surpasses the much\nmore powerful GPT-4 on two of our use cases, engineering\nassistant chatbot and EDA scripts generation, while show-\ning competitive performance on bug summarization and\nanalysis. Our future work will focus on further improving\nChipNeMo models and methods for production use.\n8\nChipNeMo: Domain-Adapted LLMs for Chip Design\nReferences\nBlocklove, J., Garg, S., Karri, R., and Pearce, H. Chip-chat:\nChallenges and opportunities in conversational hardware\ndesign, 2023.\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,\nE., Millican, K., van den Driessche, G., Lespiau, J.-B.,\nDamoc, B., Clark, A., de Las Casas, D., Guy, A., Menick,\nJ., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones,\nC., Cassirer, A., Brock, A., Paganini, M., Irving, G.,\nVinyals, O., Osindero, S., Simonyan, K., Rae, J. W., Elsen,\nE., and Sifre, L. Improving language models by retrieving\nfrom trillions of tokens, 2022.\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lund-\nberg, S., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang,\nY. Sparks of artificial general intelligence: Early experi-\nments with gpt-4, 2023.\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,\nH. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,\nBrockman, G., Ray, A., Puri, R., Krueger, G., Petrov,\nM., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,\nS., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-\nian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D.,\nPlappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A.,\nGuss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,\nJ., Babuschkin, I., Balaji, S., Jain, S., Saunders, W.,\nHesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,\nV., Morikawa, E., Radford, A., Knight, M., Brundage,\nM., Murati, M., Mayer, K., Welinder, P., McGrew, B.,\nAmodei, D., McCandlish, S., Sutskever, I., and Zaremba,\nW. Evaluating large language models trained on code,\n2021.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\nStoica, I., and Xing, E. P.\nVicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023.\nURL https://lmsys.org/blog/\n2023-03-30-vicuna/.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,\nN., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,\nJ., Petrov, S., and Fiedel, N. Palm: Scaling language\nmodeling with pathways, 2022.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge,\n2018.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. FlashAt-\ntention: Fast and memory-efficient exact attention with\nIO-awareness. In Advances in Neural Information Pro-\ncessing Systems, 2022.\nDong, Y., Wang, Z., Sreedhar, M. N., Wu, X., and Kuchaiev,\nO.\nSteerlm:\nAttribute conditioned sft as an (user-\nsteerable) alternative to rlhf, 2023.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\nPresser, S., and Leahy, C. The pile: An 800gb dataset of\ndiverse text for language modeling, 2020.\nGao, L., Ma, X., Lin, J., and Callan, J. Tevatron: An efficient\nand flexible toolkit for dense retrieval, 2022.\nGao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai,\nY., Sun, J., Guo, Q., Wang, M., and Wang, H. Retrieval-\naugmented generation for large language models: A sur-\nvey, 2024.\nGururangan, S., Marasovi\u00b4c, A., Swayamdipta, S., Lo, K.,\nBeltagy, I., Downey, D., and Smith, N. A. Don\u2019t stop\npretraining: Adapt language models to domains and tasks,\n2020.\nHe, Z., Wu, H., Zhang, X., Yao, X., Zheng, S., Zheng, H.,\nand Yu, B. Chateda: A large language model powered\nautonomous agent for eda, 2023.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\nSong, D., and Steinhardt, J. Measuring massive multitask\nlanguage understanding, 2021.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., and Chen, W. Lora: Low-rank adaptation of large\nlanguage models. CoRR, abs/2106.09685, 2021. URL\nhttps://arxiv.org/abs/2106.09685.\nIzacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski,\nP., Joulin, A., and Grave, E. Unsupervised dense infor-\nmation retrieval with contrastive learning, 2022a.\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,\nF., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S.,\nand Grave, E. Atlas: Few-shot learning with retrieval\naugmented language models, 2022b.\n9\nChipNeMo: Domain-Adapted LLMs for Chip Design\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii,\nE., Bang, Y. J., Madotto, A., and Fung, P.\nSurvey\nof hallucination in natural language generation. ACM\nComput. Surv., 55(12), mar 2023.\nISSN 0360-0300.\ndoi: 10.1145/3571730. URL https://doi.org/10.\n1145/3571730.\nKarpukhin, V., O\u02d8guz, B., Min, S., Lewis, P., Wu, L., Edunov,\nS., Chen, D., and tau Yih, W. Dense passage retrieval for\nopen-domain question answering, 2020.\nKhailany, B., Ren, H., Dai, S., Godil, S., Keller, B., Kirby,\nR., Klinefelter, A., Venkatesan, R., Zhang, Y., Catanzaro,\nB., and Dally, W. J. Accelerating chip design with ma-\nchine learning. IEEE Micro, 40(6):23\u201332, 2020. doi:\n10.1109/MM.2020.3026231.\nKocetkov, D., Li, R., Allal, L. B., Li, J., Mou, C., Ferrandis,\nC. M., Jernite, Y., Mitchell, M., Hughes, S., Wolf, T.,\nBahdanau, D., von Werra, L., and de Vries, H. The stack:\n3 tb of permissively licensed source code, 2022.\nKoto, F., Lau, J. H., and Baldwin, T. IndoBERTweet: A\npretrained language model for Indonesian Twitter with\neffective domain-specific vocabulary initialization. In\nProceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 10660\u201310668,\nNovember 2021.\nKuchaiev, O., Li, J., Nguyen, H., Hrinchuk, O., Leary, R.,\nGinsburg, B., Kriman, S., Beliaev, S., Lavrukhin, V.,\nCook, J., Castonguay, P., Popova, M., Huang, J., and\nCohen, J. M. Nemo: a toolkit for building ai applications\nusing neural modules, 2019.\nK\u00a8opf, A., Kilcher, Y., von R\u00a8utte, D., Anagnostidis, S., Tam,\nZ.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O.,\nNagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A.,\nMaguire, A., Schuhmann, C., Nguyen, H., and Mattick,\nA. Openassistant conversations \u2013 democratizing large\nlanguage model alignment, 2023.\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. Race:\nLarge-scale reading comprehension dataset from exami-\nnations, 2017.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin,\nV., Goyal, N., K\u00a8uttler, H., Lewis, M., tau Yih, W.,\nRockt\u00a8aschel, T., Riedel, S., and Kiela, D.\nRetrieval-\naugmented generation for knowledge-intensive nlp tasks,\n2021a.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin,\nV., Goyal, N., K\u00a8uttler, H., Lewis, M., tau Yih, W.,\nRockt\u00a8aschel, T., Riedel, S., and Kiela, D.\nRetrieval-\naugmented generation for knowledge-intensive nlp tasks,\n2021b.\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E.,\nMichalewski, H., Ramasesh, V., Slone, A., Anil, C.,\nSchlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-\nAri, G., and Misra, V. Solving quantitative reasoning\nproblems with language models, 2022.\nLiu, M., Pinckney, N., Khailany, B., and Ren, H. Verilo-\ngEval: evaluating large language models for verilog code\ngeneration. In 2023 IEEE/ACM International Conference\non Computer-Aided Design (ICCAD), 2023.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. Codegen: An open large\nlanguage model for code with multi-turn program synthe-\nsis. ICLR, 2023.\nOpenAI, :, Achiam, J., Adler, S., Agarwal, S., Ahmad, L.,\nAkkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J.,\nAltman, S., Anadkat, S., Avila, R., Babuschkin, I., Bal-\naji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M.,\nBelgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G.,\nBerner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman,\nA.-L., Brockman, G., Brooks, T., Brundage, M., Button,\nK., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson,\nC., Carmichael, R., Chan, B., Chang, C., Chantzis, F.,\nChen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess,\nB., Cho, C., Chu, C., Chung, H. W., Cummings, D., Cur-\nrier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N.,\nDeville, D., Dhar, A., Dohan, D., Dowling, S., Dunning,\nS., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus,\nL., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L.,\nGeorges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G.,\nGontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S.,\nGreene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy, C., Han,\nJ., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse,\nC., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B.,\nHsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S.,\nJang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S.,\nJonn, B., Jun, H., Kaftan, T., \u0141ukasz Kaiser, Kamali, A.,\nKanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L.,\nKim, J. W., Kim, C., Kim, Y., Kirchner, H., Kiros, J.,\nKnight, M., Kokotajlo, D., \u0141ukasz Kondraciuk, Kondrich,\nA., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V.,\nLampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D.,\nLi, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T.,\nLowe, R., Lue, P., Makanju, A., Malfacini, K., Manning,\nS., Markov, T., Markovski, Y., Martin, B., Mayer, K.,\nMayne, A., McGrew, B., McKinney, S. M., McLeavey, C.,\nMcMillan, P., McNeil, J., Medina, D., Mehta, A., Menick,\nJ., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V.,\nMorikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O.,\nM\u00b4ely, D., Nair, A., Nakano, R., Nayak, R., Neelakantan,\nA., Ngo, R., Noh, H., Ouyang, L., O\u2019Keefe, C., Pachocki,\nJ., Paino, A., Palermo, J., Pantuliano, A., Parascandolo,\nG., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng,\n10\nChipNeMo: Domain-Adapted LLMs for Chip Design\nA., Perelman, A., de Avila Belbute Peres, F., Petrov, M.,\nde Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M.,\nPong, V., Powell, T., Power, A., Power, B., Proehl, E.,\nPuri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C.,\nReal, F., Rimbach, K., Ross, C., Rotsted, B., Roussez,\nH., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S.,\nSastry, G., Schmidt, H., Schnurr, D., Schulman, J., Sel-\nsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker,\nS., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin,\nJ., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Stau-\ndacher, N., Such, F. P., Summers, N., Sutskever, I., Tang,\nJ., Tezak, N., Thompson, M., Tillet, P., Tootoonchian,\nA., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J.\nF. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright,\nC., Wang, J. J., Wang, A., Wang, B., Ward, J., Wei, J.,\nWeinmann, C., Welihinda, A., Welinder, P., Weng, J.,\nWeng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich,\nS., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M.,\nXiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba,\nW., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng,\nT., Zhuang, J., Zhuk, W., and Zoph, B. Gpt-4 technical\nreport, 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,\nC. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K.,\nRay, A., Schulman, J., Hilton, J., Kelton, F., Miller, L.,\nSimens, M., Askell, A., Welinder, P., Christiano, P., Leike,\nJ., and Lowe, R. Training language models to follow\ninstructions with human feedback, 2022.\nPearce, H., Tan, B., and Karri, R.\nDave:\nDeriv-\ning automatically verilog from english.\nIn Proceed-\nings of the 2020 ACM/IEEE Workshop on Machine\nLearning for CAD, MLCAD \u201920, pp. 27\u201332, New\nYork, NY, USA, 2020. Association for Computing\nMachinery.\nISBN 9781450375191.\ndoi: 10.1145/\n3380446.3430634.\nURL https://doi.org/10.\n1145/3380446.3430634.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,\nC. D., and Finn, C. Direct preference optimization: Your\nlanguage model is secretly a reward model, 2023.\nRam, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua,\nA., Leyton-Brown, K., and Shoham, Y.\nIn-context\nretrieval-augmented language models, 2023.\nReimers, N. and Gurevych, I. Sentence-bert: Sentence\nembeddings using siamese bert-networks. In Proceed-\nings of the 2019 Conference on Empirical Methods in\nNatural Language Processing. Association for Compu-\ntational Linguistics, 11 2019. URL http://arxiv.\norg/abs/1908.10084.\nRen, H. and Fojtik, M. Invited- nvcell: Standard cell lay-\nout in advanced technology nodes with reinforcement\nlearning. In 2021 58th ACM/IEEE Design Automation\nConference (DAC), 2021.\nRichardson, L. Beautiful soup documentation. April, 2007.\nRobertson, S. and Zaragoza, H. The probabilistic relevance\nframework: Bm25 and beyond. Found. Trends Inf. Retr.,\n3(4):333\u2013389, apr 2009. ISSN 1554-0669. doi: 10.1561/\n1500000019. URL https://doi.org/10.1561/\n1500000019.\nRoy, R., Raiman, J., Kant, N., Elkin, I., Kirby, R., Siu, M.,\nOberman, S., Godil, S., and Catanzaro, B. PrefixRL:\nOptimization of parallel prefix circuits using deep rein-\nforcement learning. In 2021 58th ACM/IEEE Design\nAutomation Conference (DAC), 2021.\nRozi`ere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan,\nX. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov,\nA., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C.,\nGrattafiori, A., Xiong, W., D\u00b4efossez, A., Copet, J., Azhar,\nF., Touvron, H., Martin, L., Usunier, N., Scialom, T., and\nSynnaeve, G. Code llama: Open foundation models for\ncode, 2023.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale. arXiv preprint arXiv:1907.10641, 2019.\nSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,\nAlyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja,\nA., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma,\nS. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N.,\nDatta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica,\nM., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang,\nT., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry,\nT., Fries, J. A., Teehan, R., Bers, T., Biderman, S., Gao,\nL., Wolf, T., and Rush, A. M. Multitask prompted training\nenables zero-shot task generalization, 2022.\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis,\nM., Zettlemoyer, L., and tau Yih, W. Replug: Retrieval-\naugmented black-box language models, 2023.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\nJ., and Catanzaro, B.\nMegatron-lm: Training multi-\nbillion parameter language models using model paral-\nlelism. arXiv preprint arXiv:1909.08053, 2019.\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn,\nA., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R.\nGalactica: A large language model for science, 2022.\nThakur, S., Ahmad, B., Fan, Z., Pearce, H., Tan, B., Karri,\nR., Dolan-Gavitt, B., and Garg, S. Benchmarking large\nlanguage models for automated verilog rtl code gener-\nation. In 2023 Design, Automation & Test in Europe\nConference & Exhibition (DATE), pp. 1\u20136, 2023. doi:\n10.23919/DATE56975.2023.10137086.\n11\nChipNeMo: Domain-Adapted LLMs for Chip Design\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\nand Scialom, T. Llama 2: Open foundation and fine-tuned\nchat models, 2023.\nVenigalla, A., Frankle, J., and Carbin, M. BioMedLM:\na domain-specific large language model for biomedical\ntext, 2022. URL https://www.mosaicml.com/\nblog/introducing-pubmed-gpt.\nWang, L., Yang, N., Huang, X., Jiao, B., Yang, L.,\nJiang, D., Majumder, R., and Wei, F. Text embeddings\nby weakly-supervised contrastive pre-training.\narXiv\npreprint arXiv:2212.03533, 2022.\nWang, Z., Dong, Y., Zeng, J., Adams, V., Sreedhar, M. N.,\nEgert, D., Delalleau, O., Scowcroft, J. P., Kant, N.,\nSwope, A., and Kuchaiev, O. Helpsteer: Multi-attribute\nhelpfulness dataset for steerlm, 2023.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned language\nmodels are zero-shot learners, 2022.\nWu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M.,\nGehrmann, S., Kambadur, P., Rosenberg, D., and Mann,\nG. Bloomberggpt: A large language model for finance,\n2023.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. Hellaswag: Can a machine really finish your sen-\ntence? In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 2019.\nZhou, S., Alon, U., Xu, F. F., Wang, Z., Jiang, Z., and\nNeubig, G. Docprompting: Generating code by retrieving\nthe docs, 2023.\n12\nChipNeMo: Domain-Adapted LLMs for Chip Design\nA. Appendix\nA.1. Contributions\nMingjie Liu conducted DAPT and model alignment.\nTeodor-Dumitru Ene, Robert Kirby developed inference\nand application evaluation infrastructure.\nChris Cheng developed RAG framework.\nNathaniel Pinckney collected and prepared data sets for\ntraining.\nRongjian Liang developed custom tokenizers.\nWalker Turner, Charley Lind, George Kokai developed a\ngeneral circuit design knowledge benchmark.\nSiddhanth Dhodhi, Ismet Bayraktaroglu, Himyanshu\nAnand, Eric Hill designed engineering assistant chatbot,\nprovided domain instruction datasets, evaluation bench-\nmarks, and conducted evaluation.\nParikshit Deshpande, Zhengjiang Shao, Kaizhe Xu, Ji-\nashang Hu, Laura Dang, Xiaowei Li, Hao Liu, Ambar\nSarkar developed engineering assistant chatbot application.\nSreedhar Pratty, Kishor Kunal, Ghasem Pasandi, Varun\nTej, Sumit Jain, Sujeet Omar, Pratik P Suthar, Hanfei\nSun developed EDA scripts generation application, pro-\nvided domain instruction datasets and evaluation bench-\nmarks.\nBonita Bhaskaran, Arjun Chaudhuri, Sanmitra Baner-\njee, Ghasem Pasandi developed bug summarization and\nanalysis application, provided domain instruction datasets\nand evaluation benchmarks.\nBrucek Khailany, Stuart Oberman, Sharon Clay,\nSameer Halepete, Jonathan Raiman, Bryan Catanzaro,\nJonah Alben, Bill Dally advised from AI research and\nhardware engineering perspectives.\nHaoxing Ren designed and led the research.\nA.2. Data Collection Process\nCollection was implemented with a set of shell and Python\nscripts, designed to identify relevant design data and doc-\numentation, convert them to plain text if applicable, filter\nthem using basic quality metrics, compute a checksum for\nprecise file deduplication, and compress them for storage.\nThe collection flow did not use off-the-shelf LLM-specific\nscraping and collection scripts, as we aimed to minimize\nspace requirements through in-situ data collection of inter-\nnal data sources (both networked file systems and internal\nweb applications). For file system-based collection, data\nwas kept in-place while being filtered for quality, instead of\nstoring additional sets of raw data locally.\nThe design and verification data collection encompassed a\nvariety of source files, including Verilog and VHDL (RTL\nand netlists), C++, Spice, Tcl, various scripting languages,\nand build-related configuration files. Data from internal\nweb services were gathered through both REST API calls\nand conventional crawling, with HTML formatting being\nremoved using the open-source BeautifulSoup(Richardson,\n2007) Python library in both instances to minimize inad-\nvertent removal of coding examples, at the cost of intro-\nducing more boiler plate navigation bars and other HTML\npage elements. Our data collection flow supported conven-\ntional documentation formats, including .docx, .pptx, and\n.pdf, using readily available Python conversion libraries and\nopen-source tools.\nAs most internal data is believe to be of high quality, min-\nimal filtering was applied: line count filtering was used to\nensure that exceedingly large or small files were excluded,\nand files were sorted into broad categories of manually writ-\nten versus tool-generated.\nA.3. Training Data\nDuring Domain-Adaptive Pre-Training (DAPT), we assem-\nble a dataset from a combination of proprietary chip design\nspecific data sources and publicly available datasets.\nChip Design Datasets:\nOur internal dataset consists of\na diverse range of text sources pertinent to chip design,\nspanning design, verification, infrastructure, and internal\ndocumentation. Table 3 provides a breakdown of the data\ncollected after filtering, and the corresponding number of to-\nkens using the LLaMA2 tokenizer. We construct the dataset\nby gathering all relevant internal data, then filtering by file\ntype, based on filename extensions and distinguishing be-\ntween machine-generated and human-written content. Al-\nthough we evaluated on three specific use cases, we did not\nspecifically limit the dataset to sources known to be rele-\nvant to these use cases since we believed that incorporating\nadditional domain knowledge would improve performance.\nAfter collection, cleaning, and filtering, the internal data\ntraining corpus has 23.1 billion tokens. Further details of\nthe data collection process are covered in Appendix A.2.\nPublic Datasets: We augment the chip design specific\ndata with a sample of publicly available data from various\nsources, a common practice in the development of founda-\ntional large language models. Our approach was to reuse\npublic training data from other language models, with the\nstipulation that it must be publicly accessible and compatible\nwith open sourcing. These datasets exhibit a high degree of\ncorrelation with the pretraining data used in LLaMA2 (Tou-\nvron et al., 2023), with the intention of preserving general\nknowledge and natural language capabilities during DAPT.\nThe public datasets used by ChipNeMo can be categorized\ninto two groups, natural language and code. For the natural\n13\nChipNeMo: Domain-Adapted LLMs for Chip Design\nData Source Type\nData\nData\nTraining\nTraining\nPercentage (%)\nTokens (B)\nPercentage (%)\nTokens (B)\nBug Summary\n9.5%\n2.4\n10.0%\n2.4\nDesign Source\n47.0%\n11.9\n24.5%\n5.9\nDocumentation\n17.8%\n4.5\n34.0%\n8.2\nVerification\n9.1%\n2.3\n10.4%\n2.5\nOther\n7.9%\n2.0\n12.0%\n2.9\nWikipedia\n5.9%\n1.5\n6.2%\n1.5\nGithub\n2.8%\n0.7\n3.0%\n0.7\nTotal\n100.0%\n25.3\n100.0%\n24.1\nTable 3: Breakdown of Data by Source. Token count measured with original LLaMA2 tokenizer.\nlanguage component, we draw from Wikipedia data (Gao\net al., 2020), as it is widely regarded for its high data quality.\nFor code, we leverage GitHub data (Kocetkov et al., 2022),\nfocusing on programming languages also present in our in-\nternal data chip design dataset such as C++, Python, and\nVerilog. To ensure that the overall dataset is representative\nof pre-training distributions, we perform a sub-sampling\noperation that results in approximately 9.2% of the total\ntraining tokens being sampled from these public datasets,\nwith a balanced representation of natural language and code.\nData Blend: A significant proportion of the domain data we\ngathered is comprised of unannotated code from diverse ori-\ngins. In an effort to enhance the model\u2019s comprehension of\ndomain-specific knowledge, we conducted downsampling\nof code data while concurrently upsampling natural lan-\nguage data, specifically design documentation, over a span\nof 2 to 4 training epochs. We also increased the representa-\ntion of data that we deemed more pertinent to downstream\napplications, such as human-written EDA tool scripts. Fur-\nthermore, we incorporated publicly available domain data\nfor 1 epoch. Details of the token distribution for training are\nshown in Table 3.\nA.4. Alignment Data\nDuring Supervised Fine-Tuning (SFT), we employ a general\nchat SFT instruction dataset that is accessible for commer-\ncial use. The dataset is comprised largely of publicly avail-\nable instruction following datasets including OASST (K\u00a8opf\net al., 2023), FLAN (Wei et al., 2022), P3 (Sanh et al.,\n2022) and a small amount of a broad domain proprietary\ndataset comprising various topics such as brainstorming,\nopen-ended question answering, rewriting, summarization\netc. It\u2019s important to note that the SFT instruction data we\ndiscuss here is focused on general natural language tasks\nand does not contain any information or tasks related to the\ndownstream use cases in chip design. In total, this dataset\ncomprises 128,000 training samples.\nFor SteerLM (Dong et al., 2023) we closely follow the imple-\nmentations in (Wang et al., 2023). The attribute training data\nonly contains public available data from HelpSteer (Wang\net al., 2023) and OASST (K\u00a8opf et al., 2023). For the models\nattribute-conditioned finetuning,we only used the OASST\ndata comprised of 56,000 training samples.\nAdditionally, we meticulously assembled a domain-specific\ninstruction dataset for aligning the model to downstream\nuse cases. These examples have been meticulously crafted\nby subject matter experts and are formatted as single-turn\nquestions and answers. Table 4 depicts the quantity of our\ndomain-specific instruction dataset. It\u2019s worth noting that\nthe total number of training samples in the domain-specific\ninstruction dataset is quite small when compared to the\nextensive amount of generative chat instruction data.\nDomain Source\nNumber of Samples\nDesign Knowledge\n302\nEDA Script Generation\n480\nBug summarization and analysis\n648\nTotal\n1430\nTable 4: Breakdown of Domain Alignment Data.\nA.5. Domain Evaluation Benchmarks\nIn order to quickly and quantitatively assess the accuracy of\nvarious models, we established evaluation criteria structured\nas multiple-choice question-and-answer formats for each\nuse case, designed to closely align with established bench-\nmarks, such as MMLU (Hendrycks et al., 2021). In the\nprocess of formulating these multiple-choice questions, col-\nlaboration with domain experts was pivotal. The goal was\nto ensure that each question included at least one complex\nanswer choice, thereby posing a challenge to individuals\nwith limited domain expertise. Careful attention was also\ngiven to prevent any inadvertent contamination of the ques-\ntions with data from our domain-specific alignment data.\nIn addition to the per-use-case benchmarks, an additional\nbenchmark was created for general circuit design knowl-\nedge, covering both analog and digital design topics. The\nnumber of multiple-choice questions for evaluation bench-\nmark are shown in Table 5.\nWhen we report results on the above benchmarks, we take\naverage results obtained from five distinct runs to mitigate\n14\nChipNeMo: Domain-Adapted LLMs for Chip Design\nDomain Source\nNumber of Questions\nDesign Knowledge (Design)\n94\nEDA Script Generation (Scripting)\n74\nBug Summarization and Analysis (Bugs)\n70\nOpen Domain Circuit Design (Circuits)\n227\nTable 5: Domain-specific Evaluation Benchmark.\nthe effects of variance and noise in the testing process. Each\niteration employs a set of 5-shot examples, with variations\nintroduced across each individual runs.\nIn addition to these domain-specific evaluation bench-\nmarks, we also include commonly-used publicly available\nLLM academic benchmarks. Furthermore, we measure\nthe model\u2019s code generation capabilities, by evaluating Hu-\nmanEval (Chen et al., 2021) for Python and VerilogEval (Liu\net al., 2023) for Verilog.\nA.6. Domain Adaptive Pretraining (DAPT)\nIn this section we present detailed results on our domain\nadaptive pretrained models. We also detail our ablation\nexperiments on domain adaptive pretraining.\nDAPT Hyperparameters: Details presented in Table 6.\nHyperparameters\nValue\nContext Window\n4096\nGlobal Batch Size\n256 (128)\nOptimizer\ndistributed fused adam\nWeight Decay\n0.01\nBetas\n0.9, 0.95 (0.9, 0.98)\nLearning Rate\n5 \u00b7 10\u22126\nScheduler\nNone\nTable 6: DAPT and SteerLM/SFT hyperparameters, SteerLM/SFT\nvalues shown in parenthesis (if differs from DAPT).\nAuto Eval Results: We present detailed results on auto\nevaluation benchmarks in Table 7 and Table 8. For simplic-\nity, in the remainders of the section we present aggregated\nbenchmark results for ablation studies:\n\u2022 Chip: We report average results on in-domain Design,\nScripting, Bugs, and Circuits benchmarks from Table 5\n(5-shot).\n\u2022 MMLU: We report the overall results on MMLU (5-\nshot) (Hendrycks et al., 2021) a popular aggregated\nbenchmark on a wide variety of subjects.\n\u2022 Reasoning: We report average results on popular pub-\nlic benchmarks on common sense reasoning (0-shot),\nincluding Winogrande (Sakaguchi et al., 2019), hel-\nlaswag (Zellers et al., 2019), ARC-easy (Clark et al.,\n2018), and RACE-High (Lai et al., 2017).\n\u2022 Code:\nWe\nreport\naverage\npass-rate\nof\ncod-\ning benchmarks with greedy decoding,\ninclud-\ning HumanEval (Chen et al., 2021), VerilogEval-\nMachine\n(Liu\net\nal.,\n2023),\nand\nVerilogEval-\nHuman (Liu et al., 2023).\nDomain-Adaptive Tokenization: We experimented with\nDAPT using the original LLaMA2 tokenizer and the domain-\nadapted tokenizer as described in Section 2.1. Figure 11\ndepicts smoothed training loss for ChipNeMo with the orig-\ninal unmodified tokenizer. When compared with Figure 2,\nwe observe that the domain-adapted tokenizer has larger\ntraining loss upon initialization, due to added tokens never\nbeing observed during foundation model pretraining. Simi-\nlar training loss is achieved for DAPT with 1 epoch.\nTable 9 presents aggregated auto evaluation benchmark re-\nsults. We note that careful tokenizer adaptations and weight\ninitialization only slightly impacts model performance on\ngeneral academic benchmarks.\nDAPT significantly im-\nproved domain benchmarks with any tokenizer, including\nVerilog coding (no major difference in HumanEval). We\nconclude that domain-adapting the tokenizer comes with\nthe benefit of improved tokenization and training efficiency\nwith no degradation on the models general language and\ndomain capabilities.\nFigure 11: Smoothed Training Loss with Original LLaMA2 Tok-\nenizer.\nPublic Datasets Mix-in: As introduced in Section A.3 we\nincluded public data in DAPT, sampled from commonly-\nused public datasets for foundation model pre-training.\nWe primarily hoped that mixing in public data such as\nWikipedia in DAPT could help \u201ccorrect\u201d disturbances\nbrought by domain-adapted tokenizer and improve general\nnatural language capabilities of models. We conducted an-\nother round of DAPT with domain-adapted tokenizer using\nonly the domain data, training for the same number of steps\nequating to roughly 1.1 epoch of the data. We found that\npublic data mix-in slightly improves results. We present\ndetailed results in Table 10.\nLearning Rate:\nWe experimented with employing a\nlarger learning rate, inspired by the approach used in\nCodeLLaMA (Rozi`ere et al., 2023). We use similar training\nhyperparameters as in Table 11. We use a cosine schedule\nwith 200 warm-up steps, and set the final learning rate to\n15\nChipNeMo: Domain-Adapted LLMs for Chip Design\nModel\nDesign\nScripting\nBugs\nCircuits\nMMLU\nWinogrande\nhellaswag\nARC-e\nRACE-H\nLLaMA2-7B\n41.1\n42.0\n42.2\n47.9\n45.7\n68.9\n75.6\n73.5\n46.2\nChipNeMo-7B\n57.5\n49.3\n42.8\n49.5\n44.6\n67.4\n76.3\n73.7\n46.2\nLLaMA2-13B\n43.6\n49.6\n39.7\n55.5\n55.4\n72.1\n79.3\n76.3\n46.7\nChipNeMo-13B\n67.9\n56.3\n50.1\n56.8\n53.4\n71.1\n80.3\n76.7\n46.1\nLLaMA2-70B\n52.3\n64.9\n56.9\n67.0\n68.6\n77.6\n83.6\n79.6\n48.9\nChipNeMo-70B\n76.6\n73.9\n65.8\n71.7\n69.4\n78.0\n85.1\n80.9\n48.9\nGPT-3.5\n51.7\n66.7\n52.0\n66.5\n70.0\u2217\n81.6\u2217\n85.5\u2217\n85.2\u2217\n-\nGPT-4\n58.4\n77.4\n63.4\n79.0\n86.4\u2217\n87.5\u2217\n95.3\u2217\n96.3\u2217\n-\nTable 7: Auto Evaluation Results. We report academic benchmark results for LLaMA2 using proprietary evaluation methods. ChipNeMo\nmodels trained with domain-adapted tokenizer. * results from (OpenAI et al., 2023).\nModel\nHumanEval\nVerilogEval-\nHuman\nVerilogEval-\nMachine\nLLaMA2-7B\n14.0\n3.8\n24.5\nChipNeMo-7B\n12.2\n8.3\n28.7\nLLaMA2-13B\n17.1\n9.0\n30.8\nChipNeMo-13B\n17.7\n22.4\n43.4\nLLaMA2-70B\n28.0\n30.8\n51.0\nChipNeMo-70B\n30.5\n27.6\n53.8\nGPT-3.5\n48.1\u2217\n26.7\u2020\n46.7\u2020\nGPT-4\n67.0\u2217\n43.5\u2020\n60.0\u2020\nTable 8: Coding Evaluation Results. Showing pass-rate with\ngreedy decoding. We report results for LLaMA2 using propri-\netary evaluation methods. ChipNeMo models trained with domain-\nadapted tokenizer. *, \u2020 results from (OpenAI et al., 2023; Liu et al.,\n2023).\nModel\nTokenizer\nDAPT\nChip\nMMLU\nReason\nCode\n7B\nOri.\nNo\n43.4\n45.7\n66.1\n14.1\n7B\nDpt.\nNo\n42.7\n44.6\n65.9\n13.9\n7B\nOri.\nYes\n51.2\n44.8\n65.7\n17.6\n7B\nDpt.\nYes\n49.8\n44.6\n65.8\n16.4\n13B\nOri.\nNo\n47.1\n55.4\n68.6\n18.9\n13B\nDpt.\nNo\n46.0\n55.1\n68.6\n18.4\n13B\nOri.\nYes\n57.7\n54.0\n68.4\n27.2\n13B\nDpt.\nYes\n57.8\n53.4\n68.5\n27.8\nTable 9: Evaluation Results on ChipNeMo models with Different\nTokenizers. Dpt. indicate domain-adapted tokenizer and Ori.\nindicate using LLaMA2 original tokenizer. Using augmented\ntokenizer without DAPT corresponds to the model initialization as\nin Section 2.1.\nbe 1/30th of the peak learning rate of 3 \u00b7 10\u22124. We use the\nsame batch size and number of training steps as DAPT.\nFigure 12 shows the training loss for ChipNeMo-7B with\naugmented tokenizers including public dataset mix-in. We\nobserved large spikes in training loss at the initial training\nsteps with the final training loss for 7B models to even be\nbetter than 13B original DAPT hyperparameters. However,\nwe note substantial degradation across natural language\nbenchmarks as shown in Table 12, including in-domain chip\ndesign. Coding capabilities improved as consistent with the\nfindings of (Rozi`ere et al., 2023).\nWe highlight that our case differs from that in (Rozi`ere\net al., 2023). Although we also conduct \u201ccontinued pretrain-\nPublic\nChip\nMMLU\nReason\nCode\nNo\n56.9\n53.0\n67.5\n24.1\nYes\n57.8\n53.4\n68.5\n27.8\nTable 10: Ablation on Public Dataset Mix-in with ChipNeMo-13B.\nPublic data mix-in slightly improves results.\nHyperparameters\nValue\nContext Window\n4096\nGlobal Batch Size\n256\nOptimizer\ndistributed fused adam\nWeight Decay\n0.01\nBetas\n0.9, 0.95\nLearning Rate (lr)\n3 \u00b7 10\u22124\nScheduler\nCosineAnnealing\nWarmup Steps\n200\nmin lr\n1 \u00b7 10\u22125\nTable 11: Training Hyperparameters with Larger Learning Rate.\nWe adopt similar parameter as to (Rozi`ere et al., 2023).\ning\u201d initializing from pretrained checkpoints, we preferably\nwant the model to maintain high degrees of performance\non general capabilities, while distilling domain dataset in-\nformation and knowledge (unseen in model pretraining)\ninto model weights. In contrast, (Rozi`ere et al., 2023) use\npublicly available code data that predominantly lacks natu-\nral language elements, emphasizing their primary focus on\ncoding-related tasks. We hypothesize that a smaller learning\nrate played a dual role for domain adaptation, facilitating\nthe distillation of domain knowledge through DAPT while\nmaintaining a balance that did not veer too far from the base\nmodel, thus preserving general natural language capabilities\nwhile significantly improving performance on in-domain\ntasks.\nParameter Efficient Fine-Tuning (PEFT): Parameter effi-\ncient fine-tuning freezes the pre-trained model weights and\ninjects trainable parameters in smaller adapter models for ef-\nficient fine-tuning of downstream tasks. We explore the use\nof PEFT in DAPT using Low-Rank Adaptation (LoRA) (Hu\net al., 2021). Since our transformer layer implementation\nfuses KQV into a single projection, we add LoRA adapters\nfor a single Low-Rank projection for each self attention\n16\nChipNeMo: Domain-Adapted LLMs for Chip Design\nFigure 12: Smoothed Training Loss with Larger Learning Rate. We\ninclude loss curves of suggested hyperparameters for comparison.\nLearning\nRate\nChip\nMMLU\nReason\nCode\n5 \u00b7 10\u22126\n49.8\n44.6\n65.8\n16.4\n3 \u00b7 10\u22124\n25.5\n26.6\n49.8\n18.1\nTable 12: Ablation on Learning Rate with ChipNeMo-7B. A larger\nlearning rate significantly degrades performance on all language\nrelated tasks but slightly improves coding.\nlayer in combined fashion. We experiment on LLaMA2-\n13B models with the original LLaMA2 tokenizer, using the\nsame DAPT training setups in Table 6. We ran two experi-\nments, introducing additional trainable parameters of 26.4\nmillion (small) and 211.2 million (large) respectively.\nFigure 13 shows the training loss curves of LoRA models\nand compares with full parameter training. For both LoRA\nmodels, the loss quickly converges and stops decreasing\nbeyond a certain point. Table 13 reports the evaluation\nresults on LoRA models. Both LoRA models significantly\nunderperforms full parameter training on in-domain chip\ndesign tasks. LoRA models improve in chip design tasks\ncompared to their non-DAPT counterparts, with the larger\nmodel exhibiting slightly better (but non significant) results.\nParameters\nChip\nMMLU\nReason\nCode\nNone\n47.1\n55.4\n68.6\n18.9\n26.4M\n49.0\n55.0\n68.2\n13.0\n211.2M\n49.6\n54.2\n68.6\n15.3\n13B\n57.7\n54.0\n68.4\n27.2\nTable 13: Evaluation Results on LoRA Models. First column\nindicate number of trainable parameters. None indicates LLaMA2-\n13B model without DAPT. 13B indicates full parameter training.\nA.7. Model Alignment\nFor standard supervised-finetuning (SFT) we used the fol-\nlowing structured template:\n<extra_id_0>System\\n{system}\n<extra_id_1>User\\n{user_utterance}\nFigure 13: Smoothed Training Loss of LoRA (Hu et al., 2021).\n13B corresponds to full parameter DAPT.\n<extra_id_1>Assistant\\n{chipnemo_response}\n...\nFor SteerLM we follow the steps in (Wang et al., 2023) and\napply attribute labeling to our domain data:\n1. We trained a \u201dgeneral\u201d attribute scoring model. We\nonly used HelpSteer and OASST attribute labeled data\nwithout any domain data with weights initialized from\nLLaMA2-13B.\n2. We scored domain data (1.4k samples) with the at-\ntribute scoring model in Step 1.\n3. We mixed OASST data (56k samples) with domain\ndata (1.4k samples) for 2 epochs.\n4. We conduct attribute-conditioned fine-tuning on Chip-\nNeMo models.\nFigure 14: Attribute Scores for SteerLM.\nFigure 14 depicts the attribute scores (and their standard\ndeviation) labeled by the general attribute scoring model.\nThe attribute scoring model could generalize well to unseen\ndomain data on attributes such as toxicity, humor, creativity,\nverbosity, but had slightly lower scores for domain data\non metrics such as quality, helpfulness, correctness. This\nleaves room of improvement for the attribute scoring model\n17\nChipNeMo: Domain-Adapted LLMs for Chip Design\non domain data, which could be improved by possibly ini-\ntializing the attribute models from ChipNeMo (as respect\nto LLaMA2) and adding attribute labeled domain data. For\nour experiments we used the attribute labels as-is.\nAdditionally, we conducted an additional alignment us-\ning solely the general chat dataset, excluding any domain-\nspecific alignment data. For clarity, we designate all our\nChipNeMo models as follows:\n\u2022 ChipNeMo-SFTG: Models fine-tuned with general\nchat data exclusively using standard SFT.\n\u2022 ChipNeMo-SFT: Models fine-tuned with both domain\nand general chat data using standard SFT;\n\u2022 ChipNeMo-SteerG: Models fine-tuned with general\nchat data exclusively using SteerLM.\n\u2022 ChipNeMo-Steer: Models fine-tuned with both do-\nmain and general chat data using SteerLM;\nA.8. Domain-Adaptive Retrieval Model\nManually generating training samples is very effort inten-\nsive, so we elected to implement a process to generate them\nautomatically. Since we are using contrastive learning to\nfine-tune our model, each sample requires a set of both\npositive passages and negative passages, particularly hard\nnegatives to maximize the accuracy.\nFigure 15: Sample Generation For Retrieval Model Training\nA.8.1. DATASET SAMPLING PROCEDURE\nFigure 15 describes the steps taken to generate a sample:\n1. Randomly select a passage from the document corpus\n2. Use a language model (Vicuna) to generate a valid\nquery from the passage\n3. Use a pre-existing retrieval model (sentence trans-\nformer) to fetch the top-N passages from the document\ncorpus for the query where each passage is a potential\nhard-negative\n4. It is possible that some of the fetched passages are\nactually positive, so use the same language model to\nfilter out the positive passages\n5. If there are not enough negative passages after this\nfiltering process, supplement with random passages\nfrom the corpus\nFor our initial research we used Vicuna (Chiang et al., 2023)\nand Sentence Transformer (Reimers & Gurevych, 2019);\nhowever, they can easily be replaced with LLaMA2 (Tou-\nvron et al., 2023) and BM25 (Robertson & Zaragoza, 2009)\nrespectively to produce a retrieval model that is commer-\ncially viable.\nA.8.2. HIT QUALITY COMPARISON\nNot all hits are created equal. The passage in the Spec ex-\nample below clearly and completely answers its query. The\npassage in the Build example contains the answer; however,\nmore context is required to answer the query.\nSpec Example: Hit passage clearly answers the query.\nQuery:\nWhat is the XXX buffer in the YYY used for?\nHit Passage:\nSince the memory subsystem is partitioned, YYY may\nreceive read responses whose order is different from their\nrequests. ZZZ requires strict in-order per-thread processing,\nthus YYY reorders the read responses to the request order\nbefore sending them back to ZZZ. YYY re-orders the\nread responses in a read return reorder buffer (XXX) to\nguarantee the ordering.\nAnswer: re-orders the read responses.\nBuild Example: Additional information is required to fully\nanswer the query. Such as: What is a DL? How do we know\nArch-Build-Hotseat-XXX is a DL?\nQuery:\nWhat is the support DL for XXX build issues?\nHit Passage:\nTree Setup\nWorking in the XXX Mainline explains initial tree setup\nand build steps\nBuild\nArch-Build-Hotseat-XXX - Hotseat support for XXX build\nissues YYY build failures\nA.9. Additional Evaluation Results and Discussions\nTable 14 shows the evaluation data for all models on the engi-\nneering assistant chatbot application. It should be noted that\nadding domain-specific instructional data to our SFT train-\ning led to only marginal improvement on the engineering\n18\nChipNeMo: Domain-Adapted LLMs for Chip Design\nassistant task. However, including this data in our SteerLM\ntraining degraded the quality of our model\u2019s responses. We\nbelieve this is due to insufficient adaptation of our data to\nthe SteerLM labeling system. Future work will see closer\ncompatibility between general-purpose and domain-specific\ninstructional data labeling, from which we expect to see\nimprovements in the behavior of the model as a chatbot.\nThe evaluation results for all models on the EDA script\ngeneration task are presented in Table 15. In addition to\nthe comparison with off-the-shelf models discussed in the\nmain section of the paper 3.6, an ablation study was con-\nducted to assess the significance of SteerLM training against\nChipNeMo-70B-SFT and ChipNeMo-70B-Steer models.\nThe observations suggest that SteerLM training helps on\nthe \u201chard\u201d benchmark, thus showcasing its effectiveness in\ngenerating relevant results for real world use cases.\nFigure 16 and Figure 17 depict the comparison among\nLLaMA2-70B-Steer, ChipNeMo-70B-SteerG model, and\nChipNeMo-70B-Steer models. The results reveal a signifi-\ncant enhancement achieved solely through DAPT training,\nemphasizing the crucial role of domain knowledge. More-\nover, a substantial improvement is observed when contrast-\ning the chat model\u2019s performance with and without domain\ninstructional data. This emphasizes that the model\u2019s capac-\nity to produce accurate answers can be enhanced through\nimproved alignment with domain instructional data. These\nresults show the importance of both DAPT and model align-\nment for domain specific applications.\nThe impact of RAG for generating EDA scripts was also\nstudied. The retrieved data consisted of specific APIs related\nto the questions along with a description of the API. This\nhelped in improving the accuracy of \u201ceasy\u201d and \u201cmedium\u201d\ndifficulty benchmark which depend heavily on the API\nknowledge. On the \u201chard\u201d benchmark, a degradation in\nthe accuracy is noticed as compared to non-RAG model.\nThis showcases the difficulty of coupling existing retrieval\ntechniques with non-natural language tasks such as code\ngeneration. While a wide variety of retrieval techniques\nhave been proven to work for natural language tasks, there\nare comparatively fewer publications focused on retrieval-\naugment code generation (Gao et al., 2024). This empha-\nsizes the importance of DAPT, especially in data regimes\nwhere there are insufficient quality examples and explana-\ntions to readily apply retrieval.\nThe evaluation results for the bug summarization and anal-\nysis task are presented in Table 16. Minor improvements\nare observed with DAPT, evident when comparing the non-\nDAPT LLaMA2-70B-SteerG model to its DAPT counter-\npart, ChipNeMo-70B-SteerG. When comparing SteerLM\nto traditional SFT training, a slight enhancement in sum-\nmarization task performance is noted, while no significant\ndifference is observed in task assignment.\nAdditionally, we investigated GPT-4 models in two versions.\nIn the first approach, we utilized the conventional evaluation\nprocedure employed for our other models, assuming limi-\ntations based on a comparable context window size. This\nprocedure included breaking down substantial bugs into\nsmaller segments and employing hierarchical summariza-\ntion, as previously described. In the second approach, we\nsuccessfully fitted each of our test bugs entirely within the\n32k context size of GPT-4. The results of this ablation study\nsuggest that our hierarchical summarization has minimal\nimpact on response quality.\nFigure 16: EDA Script Generation Domain Data Ablation, Pass@5\nFigure 17: EDA Script Generation Domain Data Ablation, Single\nGeneration (temperature=0), Human Evaluated 0-10\nA.10. Chip Design Applications\nWe conducted a survey of potential LLM applications within\nour design teams and categorized them into four buckets:\ncode generation, question & answer, analysis and report-\ning, and triage. Code generation refers to LLM generating\ndesign code, testbenches, assertions, internal tools scripts,\netc.; Q & A refers to an LLM answering questions about\ndesigns, tools, infrastructures, etc.; Analysis and report-\ning refers to an LLM analyzing data and providing reports;\ntriage refers to an LLM helping debug design or tool prob-\nlems given logs and reports. We selected one key applica-\ntion from each category to study in this work, except for\n19\nChipNeMo: Domain-Adapted LLMs for Chip Design\nModel\nRAG\nHit\nMiss\nAvg.\nGPT-4\nNo\n-\n-\n2.84\nLLaMA2-70B-Chat\nNo\n-\n-\n1.81\nLLaMA2-70B-SteerG\nNo\n-\n-\n1.99\nChipNeMo-70B-SteerG\nNo\n-\n-\n5.12\nChipNeMo-70B-Steer\nNo\n-\n-\n4.80\nChipNeMo-70B-SFTG\nNo\n-\n-\n4.68\nChipNeMo-70B-SFT\nNo\n-\n-\n4.89\nGPT-4\nYes\n4.77\n4.04\n4.52\nLLaMA2-70B-Chat\nYes\n4.18\n3.22\n3.86\nLLaMA2-70B-SteerG\nYes\n3.68\n3.00\n3.46\nChipNeMo-70B-SteerG\nYes\n6.02\n4.78\n5.68\nChipNeMo-70B-Steer\nYes\n5.58\n4.39\n5.26\nChipNeMo-70B-SFTG\nYes\n5.18\n4.79\n5.06\nChipNeMo-70B-SFT\nYes\n5.39\n4.17\n5.06\nTable 14: Engineering Assistant Chatbot Human Evaluation. Eval-\nuated with 7-point Likert Scale.\nthe triage category which we leave for further research.\nThe motivation and technical details of each application are\ngiven below.\nA.10.1. ENGINEERING ASSISTANT CHATBOT\nThis application aims to help design engineers with an-\nswers to their architecture, design, verification, and build\nquestions, which could significantly improve their overall\nproductivity without impacting the productivity of others. It\nis observed that design engineers often enjoy brainstorming,\ndesigning hardware, and writing code, but can be slowed\ndown waiting for answers on design knowledge they lack.\nDesign productivity can also be enhanced by avoiding hav-\ning engineers write code based on mistaken assumptions or\ndebugging code that they are unfamiliar with. Internal stud-\nies have shown that up to 60% of a typical chip designer\u2019s\ntime is spent in debug or checklist related tasks across a\nrange of topics including design specifications, testbench\nconstruction, architecture definition, and tools or infrastruc-\nture. Experts on these issues are often spread around the\nglobe in a multinational company, such that it is not always\nconvenient to find immediate help. Therefore, an engineer-\ning assistant chatbot based on knowledge extracted from\ninternal design documents, code, any recorded data about\ndesigns and technical communications such as emails and\ncorporate instant communications, etc. could help signifi-\ncantly improve design productivity. We implemented this\napplication with the domain-adapted RAG method men-\ntioned in Section 2.4.\nA.10.2. EDA SCRIPT GENERATION\nAnother common task in an industrial chip design flow is\nwriting EDA scripts to accomplish a variety of tasks such\nas design implementation, introspection and transformation.\nThese scripts often leverage both tool-specific and custom\nFigure 18: LLM script generator integration with EDA tools\ninternal script libraries. Learning these libraries, navigating\ntool documentation, and writing and debugging these scripts,\ncan take up a significant amount of engineering time.\nLLMs have proven adept at small scale code generation on\na wide array of tasks (Rozi`ere et al., 2023) and therefore\ncustomizing these models to accelerate engineer produc-\ntivity in this domain specific task is a natural fit. In this\nwork we focus on generating two different types of scripts\nfrom natural language task descriptions. The first are scripts\nwhich leverage an internal python library for design editing\nand analysis. The second are Tcl scripts that use the com-\nmand interface provided by a leading industrial static timing\nanalysis tool.\nIn order to build our domain-specific fine-tuning dataset for\nthis task, production scripts for both tools were collected\nfrom design experts. We observed that our DAPT models\ncan generate reasonable inline comments for the code. This\nenabled us to use these models to improve the quality of\ncollected scripts by generating additional inline comments.\nHuman experts later verified and corrected these comments\nand created an associated prompt. These prompts and code\npairs make up the data used for model alignment as dis-\ncussed in A.4.\nTo provide and collect feedback in the most meaningful\nway, we spent significant effort building the flow shown in\nFig. 18 where engineers can both query the model and run\ngenerated code through the same interface. This allows us\nto be confident in the correctness of generated code as well\nas provide accurate feedback by allowing engineers to see\nhow many corrections they might need to get a functioning\nscript. We support this integration by establishing interactive\nconnections to tool servers.\nAdditionally, we provide a user feedback form, allowing\nus to compare different models and glean valuable insights\nfrom user feedback. This valuable information can aid us in\nfurther refining our models.\n20\nChipNeMo: Domain-Adapted LLMs for Chip Design\nTool1 (Python)\nTool2 (Tcl)\nModel\nEasy\nMedium\nHard\nEasy\nMedium\nHard\n(Automatic)\n(Automatic)\n(Human)\n(Automatic)\n(Automatic)\n(Human)\nGPT-4\n0%\n0%\n0.0\n20%\n52%\n1.1\nLLaMA2-70B-Chat\n0%\n0%\n0.1\n7%\n4%\n0.0\nLLaMA2-70B-SteerG\n0%\n0%\n0\n0%\n11%\n0.2\nChipNeMo-70B-SteerG\n19%\n11%\n1.4\n29%\n52%\n2.4\nChipNeMo-70B-SFT\n61%\n29%\n3.4\n27%\n74%\n1.9\nChipNeMo-70B-Steer\n49%\n32%\n3.6\n45%\n56%\n2.9\nChipNeMo-70B-Steer (w/RAG)\n77%\n36%\n2.3\n84%\n85%\n0.8\nTable 15: EDA Script Generation Evaluation.\nAutomatic Evaluation Scored Pass@5.\nHuman Evaluation Scored 0-10 on a Single Generation (temperature = 0).\nModel\nTechnical Summary\nManagerial Summary\nTask Assignment\nGPT-4\n6.30\n6.25\n6.00\nGPT-4 (32k, No Chunks)\n6.14\n6.45\n5.78\nLLaMA2-70B-Chat\n4.35\n4.95\n5.00\nLLaMA2-70B-SteerG\n4.95\n5.35\n4.73\nChipNeMo-70B-SteerG\n5.00\n5.35\n5.45\nChipNeMo-70B-Steer\n5.05\n5.25\n4.27\nChipNeMo-70B-SFT\n4.50\n5.15\n5.45\nTable 16: Bug Summarization and Analysis Evaluation. Likert scale 1-7.\nA.10.3. BUG SUMMARIZATION AND ANALYSIS\nTracking the reporting, triage, debug and resolution of vari-\nous features and bugs across stages of the production flow\nis a time-consuming process. Engineering managers spend\na lot of time reviewing internal issue tracking databases\nto build understanding of the state of the project and help\nspeed their execution. Therefore, a tool that is able to look\nat all supporting information and quickly summarize both\ntechnical and managerial data as well as suggest next steps\nwould boost team productivity. We focus on using LLMs to\ngenerate three different outputs - one focused on technical\ndetails, one on managerial details and one recommending\ntask assignment.\nTo study these tasks we used an internal bug database. This\ndatabase is used for bug reporting, tracking and resolution as\nwell as general task and feature tracking across the company.\nWe expect ChipNeMo models to perform well on this task as\na large amount of bug data was included in the DAPT dataset.\nAdditionally, we built a domain-specific SFT dataset for this\ntask that includes examples of the bug summarizing and\ntask assignment tasks.\nOften, bug descriptions contain large snippets of log files\nor code dumps along with long comment histories. In such\ncases, the bug text is too large for our LLM context windows.\nTo work around this, we implemented two solutions. First,\nwe found and replaced long path names with shorter aliases\nto allow the model to associate paths that occur in multiple\nplaces in the bug without needing to process the entire string.\nSecond, we split the summarization task into an incremen-\ntal task where the model is tasked with accumulating data\nacross multiple summary and bug data chunks. We use a\nhierarchical approach where the bug is first separated into\nchunks that fit into the context window. Those chunks are\nthen summarized and the summaries are accumulated then\nseparated into chunks. This process is repeated until the\nentire set of summaries fits into a single context window and\na single summary is generated. We use this same approach\nindependent of the LLM used for summarization.\n21\nChipNeMo: Domain-Adapted LLMs for Chip Design\nA.11. Examples\nA.11.1. ENGINEERING ASSISTANT CHATBOT\nQuestion:\nWrite a System Verilog assert to check if signal A is true\nwithin 5 cycles of signal B being true\nAnswer:\nSure, here is the System Verilog assert to check if signal\nA is true within 5 cycles of signal B being true:\nassert property (@(posedge clk) disable iff\n(\u02dcrst_n)\n,\u2192\n(signal_B && !signal_A) |-> ##[1:5]\nsignal_A\n,\u2192\n);\nThis assert checks that if signal B is true and signal A\nis false, then signal A will become true within the next\n5 cycles. The \u2018##[1:5]\u2018 syntax is used to specify a delay\nrange from 1 to 5 cycles. The \u2018disable iff (\u223crst n)\u2018 clause\nensures that the assert is not triggered during reset.\nQuestion:\nwhy do we add retiming stages?\nAnswer:\nRetiming stages are added to improve the timing of the\ndesign and to account for the physical distance between\ndifferent parts of the chip. By adding retiming stages,\nwe can reduce the amount of time it takes for signals\nto travel between different parts of the chip, which can\nhelp to improve the overall performance of the design.\nAdditionally, retiming stages can also help to reduce the\namount of noise and interference that can occur on the\nsignals, which can further improve the performance of the\ndesign.\nQuestion:\nHow do I run multiple seeds of random tests using testgen\nframework?\nAnswer:\nYou can run multiple seeds of random tests using the\nTestgen framework by using the rand tgen target. This\ntarget allows you to specify a variety of options for the\nstopping criteria of random runs using the same Testgen\ntestlist(s).\nnvmk -c . rand_tgen TESTLIST=my_testlist\nMON=1 TGEN.LOOP=50\n,\u2192\nThis command will run 50 iterations of the my testlist\ntestlist, each with a different seed. You can specify\nthe number of iterations and the seed mode using the\nTGEN.LOOP and SEED options, respectively.\nA.11.2. EDA SCRIPT GENERATION\nSome function names and commands are obfuscated.\nQuestion:\nwrite a code in TOOL1 to print the number of flop cells\nin the given rectangle (0,0,100,100)\nAnswer:\n# Sure, here is a TOOL1 code to print the\nnumber of flop cells in the given\nrectangle (0,0,100,100):\n,\u2192\n,\u2192\n# This code gets the top_ref of the design\ntop_ref = env().get_top_reference()\n# This code gets the cells of the rectangle\nin the top_ref\n,\u2192\ncells =\nenv().layout().cells_in_rectangle(\ntop_ref, rectangle(0, 0, 100, 100),\nhierarchical)\n,\u2192\n,\u2192\n,\u2192\n# This code gets the cells that are flop\ncells\n,\u2192\nflop_cells = cells.type(\"is_flop_cell\")\n# This code prints the number of flop cells\nprint(flop_cells.size())\nQuestion:\nWrite a TOOL2 tcl proc to find intersecting logic from\npins \u2019$pinA\u2019 to pin \u2019$pinB\u2019.\nAnswer:\n#Sure, here is the code:\nproc myfunc {pinA pinB} {\n# Get all the pins in the fanout of the\ngiven pin\n,\u2192\nset fanout_pins [fanout -from $pinA]\n# Get all the pins in the fanout of the\ngiven pin\n,\u2192\nset fanin_pins [fanin -to $pinB]\n# Get the intersection of the two\ncollections\n,\u2192\nset intersection [common_logic\n$fanout_pins $fanin_pins]\n,\u2192\n# Return the intersection\nreturn $intersection\n}\n22\nChipNeMo: Domain-Adapted LLMs for Chip Design\nA.11.3. BUG SUMMARY AND ANALYSIS\nUsernames, chip names and paths are obfuscated.\nPrompt:\nBugId : 002\nSynopsis : Request power/current measurement on fabrics using fabric test module\nModule : Silicon - DFT\nDescription :\nHi all,\nWe want to evaluate silicon current (power) consumed by fabric macros in a chip. This will help us correlate this power to\npre-Si power estimates.\nHere are the conditions we want to look at for this data:\nFabric test module activity: 0/25/50/75/100%\nCorners:\ntypical low voltage\ntypical high voltage\nWe want to isolate fabric current (power) as much as possible, so we want to enable only the logic needed for fabric test\nmodules and the fabrics themselves.\nIf this information can be broken down per module (there are 6 in the chip), that will help.\nSeverity : 5-Performance\nPriority : 5-Normal\nComments :\n#1 : Author : User A\nIs this a requirement only for current or for future chips as well ?\n#2 : Author : User B\nThere will likely be interest in future chips to perform this exercise as we push the boundaries of fabric power density for\nbetter area.\n#3 : Author : User A\nWhat is needed in this bug ?\n@User B It is not clear.\nCan you list the specific needs with timelines and priority\nTechnical Summary:\nIn the internal bug management system, a bug has been filed regarding the power and current measurements of the fabrics in\nthe latest chip. The request is to evaluate the silicon current (power) consumed by the fabric macros in the latest chip and\nto correlate this power to pre-Si power estimates. The conditions for the measurements are the fabric test module activity\nat 0/25/50/75/100% and the corners are typical low voltage and typical high voltage. The data needs to be broken down per\nmodule (there are 6 in the latest chip). Only the logic needed for fabric test modules and the fabrics themselves need to be\nenabled.\nManagerial Summary:\nUser A asks User B if the need for evaluating the silicon current of fabric macros in a chip is only for that specific chip or\nfor future ones as well. User B replies that there will likely be interest in future chips to perform this exercise as they push\nthe boundaries of fabric power density for better area. User A then asks what is needed in this bug and asks for specific\nneeds with timelines and priority.\nTask Assignment:\nUser B\n23\n"
  }
]