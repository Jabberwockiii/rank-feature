[
  {
    "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory",
    "link": "https://arxiv.org/pdf/2312.11514.pdf",
    "upvote": "251",
    "text": "LLM in a flash:\nEfficient Large Language Model Inference with Limited Memory\nKeivan Alizadeh, Iman Mirzadeh\u2217, Dmitry Belenko\u2217, S. Karen Khatamifard,\nMinsik Cho, Carlo C Del Mundo, Mohammad Rastegari, Mehrdad Farajtabar\nApple \u2020\nAbstract\nLarge language models (LLMs) are central to\nmodern natural language processing, delivering\nexceptional performance in various tasks.\nHowever, their substantial computational and\nmemory requirements present challenges,\nespecially for devices with limited DRAM\ncapacity.\nThis paper tackles the challenge\nof efficiently running LLMs that exceed the\navailable DRAM capacity by storing the model\nparameters in flash memory, but bringing them\non demand to DRAM. Our method involves\nconstructing an inference cost model that takes\ninto account the characteristics of flash mem-\nory, guiding us to optimize in two critical areas:\nreducing the volume of data transferred from\nflash and reading data in larger, more contigu-\nous chunks. Within this hardware-informed\nframework,\nwe\nintroduce\ntwo\nprincipal\ntechniques. First, \u201cwindowing\u201d strategically\nreduces data transfer by reusing previously\nactivated neurons, and second, \u201crow-column\nbundling\u201d, tailored to the sequential data access\nstrengths of flash memory, increases the size\nof data chunks read from flash memory. These\nmethods collectively enable running models\nup to twice the size of the available DRAM,\nwith a 4-5x and 20-25x increase in inference\nspeed compared to naive loading approaches in\nCPU and GPU, respectively. Our integration of\nsparsity awareness, context-adaptive loading,\nand a hardware-oriented design paves the way\nfor effective inference of LLMs on devices\nwith limited memory.\n1\nIntroduction\nIn recent years, large language models (LLMs),\nsuch as GPT-3 (Brown et al., 2020), OPT (Zhang\net al., 2022b), and PaLM (Chowdhery et al., 2022),\nhave demonstrated strong performance across a\nwide range of natural language tasks. However, the\n\u2217 Major Contribution\n\u2020{kalizadehvahid, imirzadeh, d_belenko, skhatamifard,\nminsik, cdelmundo, mrastegari, farajtabar}@apple.com\nNaive\nFalcon 7B\n(CPU)\nOurs\nNaive\nOPT 6.7B\n(CPU)\nOurs\nNaive\nOPT6.7B\n(GPU)\nOurs\n100\n450\n700\n2250\n3100\nInference Latency (ms)\nCompute\nLoad From Flash\nMemory Management\nFigure 1: Inference latency of 1 token when half the\nmemory of the model is available. Our method selec-\ntively loads parameters on demand per token generation\nstep. The latency is the time needed to load from flash\nmultiple times back and forth during the generation of\nall tokens and the time needed for the computations,\naveraged over all generated tokens.\nunprecedented capabilities of these models come\nwith substantial computational and memory re-\nquirements for inference. LLMs can contain hun-\ndreds of billions or even trillions of parameters,\nwhich makes them challenging to load and run effi-\nciently, especially on resource-constrained devices.\nCurrently, the standard approach is to load the en-\ntire model into DRAM (Dynamic Random Access\nMemory) for inference (Rajbhandari et al., 2021;\nAminabadi et al., 2022). However, this severely\nlimits the maximum model size that can be run.\nFor example, a 7 billion parameter model requires\nover 14GB of memory just to load the parameters\nin half-precision floating point format, exceeding\nthe capabilities of most edge devices.\nTo address this limitation, we propose to store\nthe model parameters in flash memory, which is\nat least an order of magnitude larger than DRAM.\nThen, during inference, we directly load the re-\nquired subset of parameters from the flash mem-\n1\narXiv:2312.11514v2  [cs.CL]  4 Jan 2024\nDRAM\nFlash Memory\n~100 GB\n~10 GB\nCPU\nGPU\n~ 1 GB/s\n~100 GB/s\n(a) Bandwidth in a unified memory architecture\n4 8\n16\n32\n64\nChunk Size (KB)\n0\n1000\n2000\n3000\n4000\n5000\n6000\nRandom Read Throughput (MB/s)\nUpper Bound (Sequential Read)\nThreads\n32\n16\n8\n4\n2\n(b) Random read throughput of flash memory\nFigure 2: (a) Flash memory offers significantly higher capacity but suffers from much lower bandwidth compared\nto DRAM and CPU/GPU caches and registers. (b) The throughput for random reads in flash memory increases with\nthe size of sequential chunks and the number of threads.\nory, avoiding the need to fit the entire model in\nDRAM. Our method is built on the top of re-\ncent works that have shown LLMs exhibit a high\ndegree of sparsity in the Feed Forward Network\n(FFN) layers, with models like OPT (Zhang et al.,\n2022b), Falcon (Almazrouei et al., 2023), and Per-\nsimmon (Elsen et al., 2023), exhibiting more than\n90% sparsity (Mirzadeh et al., 2023; Liu et al.,\n2023b).\nWe exploit this sparsity to selectively\nload only parameters from flash memory that either\nhave non-zero input or are predicted to have non-\nzero output. Specifically, we discuss a hardware-\ninspired cost model that includes flash memory,\nDRAM, and compute (CPU or GPU). Then, we\nintroduce two complementary techniques to min-\nimize data transfer and maximize flash memory\nthroughput:\n\u2022 Windowing: We load and temporarily cache pa-\nrameters for only the past few tokens, reusing ag-\ngregate sparsity structure predicted over the past\nfew tokens. This sliding window approach re-\nduces the number of IO requests to load weights.\n\u2022 Row-column bundling:\nWe store a concate-\nnated row and column of the up-projection and\ndown-projection layers to read bigger contigu-\nous chunks from flash memory. This increases\nthroughput by reading larger chunks.\nTo further minimize the number of weights to be\ntransferred from flash memory to DRAM, we also\nemploy methods to predict FFN sparsity and avoid\nloading zeroed-out parameters, akin to approaches\ndocumented in Deja Vu (Li and Lu, 2023). To-\ngether, windowing and sparsity prediction allow\nus to load only 2% of the FFN layer from flash\nfor each inference query. We also propose a static\nmemory preallocation to minimize transfers within\nDRAM and reduce inference latency. Our load\nfrom flash cost model captures the tradeoff between\nloading less data and reading bigger chunks. Op-\ntimizing this cost model and selectively loading\nparameters on demand yields flash loading strate-\ngies that can run models 2x larger than the device\u2019s\nDRAM capacity and speed up inference by 4-5x\nand 20-25x compared to naive implementation in\nCPU and GPU, respectively. It significantly out-\nperforms the baseline approach, which reloads the\nmodel\u2019s weights on every forward pass.\n2\nFlash Memory & LLM Inference\nIn this section, we explore the characteristics of\nmemory storage systems (e.g., flash, DRAM), and\ntheir implications for large language model (LLM)\ninference. Our aim is to elucidate the challenges\nand hardware-specific considerations essential for\nalgorithm design, particularly in optimizing infer-\nence when working with flash memory.\n2.1\nBandwidth and Energy Constraints\nWhile modern NAND flash memories offer high\nbandwidth and low latency, they fall well short\nof the performance levels of DRAM (Dynamic\nRandom-Access Memory), in terms of both latency\nand throughput. Figure 2a illustrates these differ-\nences. A naive inference implementation that relies\non NAND flash memory might necessitate reload-\ning the entire model for each forward pass. This\nprocess is not only time-consuming, often taking\nseconds for even compressed models, but it also\nconsumes more energy than transferring data from\nDRAM to the CPU or GPU\u2019s internal memory.\nLoad times for the models can be a problem\neven in the traditional DRAM-resident set up where\n2\nweights are not reloaded partially \u2013 the initial, full\nload of the model still incurs a penalty, particu-\nlarly in situations requiring rapid response times\nfor the first token. Our approach, leveraging activa-\ntion sparsity in LLMs, addresses these challenges\nby enabling selective reading of model weights,\nthereby reducing the response latency.\n2.2\nRead Throughput\nFlash memory systems perform optimally with\nlarge sequential reads. For instance, benchmarks\non an Apple MacBook Pro M2 with 2TB flash\ndemonstrate speeds exceeding 6GiB/s for a 1GiB\nlinear read of an uncached file. However, this high\nbandwidth is not replicated for smaller, random\nreads due to the inherent multi-phase nature of\nthese reads, encompassing the operating system,\ndrivers, interrupt handling, and the flash controller,\namong others. Each phase introduces latency, dis-\nproportionately affecting smaller reads.\nTo circumvent these limitations, we advocate\ntwo primary strategies, which can be employed\njointly. The first involves reading larger chunks of\ndata. For smaller blocks, a substantial part of the\noverall read time is spent waiting for data transfer\nto begin. This is often referred to as latency to first\nbyte. This latency reduces the overall throughput\nof each read operation considerably, because the\noverall measured throughput has to take into ac-\ncount not just the speed of transfer once it begins,\nbut the latency before it begins as well, which pe-\nnalizes small reads. This means that if we coalesce\nthe reads for rows and colums of the FFN matri-\nces, we can pay the latency cost only once for any\ngiven row/column pair in both matrices, and higher\nthroughput can be realized. This principle is de-\npicted in Figure 2b. Perhaps a counterintuitive yet\ninteresting observation is that in some scenarios, it\nwill be worthwhile to read more than needed (but in\nlarger chunks) and then discard, than only reading\nstrictly the necessary parts but in smaller chunks.\nThe second strategy leverages parallelized reads,\nutilizing the inherent parallelism within storage\nstacks and flash controllers. Our results indicate\nthat throughputs appropriate for sparse LLM infer-\nence are achievable on modern off-the-shelf hard-\nware using 32KiB or larger random reads across\nmultiple threads.\nMotivated by the challenges described in this sec-\ntion, in section 3, we propose methods to optimize\ndata transfer volume and enhance read throughput\nto significantly enhance inference speeds.\n3\nLoad From Flash\nThis section addresses the challenge of conducting\ninference on devices where the available DRAM\nis substantially smaller than the size of the model.\nThis necessitates storing the full model weights in\nflash memory. Our primary metric for evaluating\nvarious flash loading strategies is latency, dissected\ninto three distinct components: the I/O cost of load-\ning from flash, the overhead of managing memory\nwith newly loaded data, and the compute cost for\ninference operations.\nOur proposed solutions for reducing latency un-\nder memory constraints are categorized into three\nstrategic areas, each targeting a specific aspect of\nthe latency:\n\u2022 Reducing Data Load: Aiming to decrease la-\ntency associated with flash I/O operations by\nloading less data1.\n\u2022 Optimizing Data Chunk Size: Enhancing flash\nthroughput by increasing the size of data chunks\nloaded, thereby mitigating latency.\n\u2022 Efficient\nManagement\nof\nLoaded\nData:\nStreamlining the management of data once it is\nloaded into memory to minimize overhead.\nIt is important to note that our focus is not on the\ncompute aspect of the process, as it is orthogonal to\nthe core concerns of our work. This delineation al-\nlows us to concentrate on optimizing flash memory\ninteractions and memory management to achieve\nefficient inference on memory-constrained devices.\nFinally, we will elaborate on the implementation\nof these strategies in subsequent sections.\n3.1\nReducing Data Transfer\nOur methodology leverages the inherent activation\nsparsity found in Feed-Forward Network (FFN)\nmodels, as documented in preceding research. The\nOPT 6.7B model, for instance, exhibits a notable\n97% sparsity within its FFN layer. Similarly, the\nFalcon 7B model has been adapted through fine-\ntuning, which involves swapping their activation\nfunctions to ReLU, resulting in 95% sparsity while\nbeing almost similar in accuracy (Mirzadeh et al.,\n2023). In light of this information, our approach\n1It is notable that, by data we mean weights of the neural\nnetwork. However, our developed techniques can be eas-\nily generalized to other data types transferred and used for\nLLM inference, such as activations or KV cache, as suggested\nby Sheng et al. (2023).\n3\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\nOutput Magnitude (before ReLU)\nCount\nFalse Negative\nUp Projection\nPredictor\n(a) predictor vs relu\nN\nLow Rank \nPredictor\nM\nM\nN\nM\nR\nReLU\nsigmoid\u2028\n> 0.5\nUp Projection\u2028\n(FC)\n0\n0\n1\n0\n1\n0\n.\n.\n.\n0\n0\nN= d          \nmodel\nM = dffn\n(b) low rank predictor\nFigure 3: (a) Preactivations of tokens in one sequence in OPT 6.7B. The blue graph shows preactivation of elements\nthat predictor detected positive while the green graph is for up projection. As it can be seen most of the False\nPositives are close to 0 and False Negatives constitute a small portion of the elements. (b) A small low rank predictor\nfinds out which intermediate neurons are going to be activated instead of running heavy up projection.\ninvolves the iterative transfer of only the essential,\nnon-sparse data from flash memory to DRAM for\nprocessing during inference.\nWhile we employ the 7B models as practical\nexamples to elucidate our approach, our findings\nare adaptable, and they can be extrapolated to both\nlarger and smaller scale models.\nSelective Persistence Strategy. We opt to re-\ntain the embeddings and matrices within the at-\ntention mechanism of the transformer constantly\nin RAM. For the Feed-Forward Network (FFN)\nportions, only the non-sparse segments are dynam-\nically loaded into DRAM as needed. Storing at-\ntention weights, which constitute approximately\none-third of the model\u2019s size, in memory, allows\nfor more efficient computation and quicker access,\nthereby enhancing inference performance without\nthe need for full model loading.\nAnticipating ReLU Sparsity. The ReLU acti-\nvation function naturally induces over 90% sparsity\nin the FFN\u2019s intermediate outputs, which reduces\nthe memory footprint for subsequent layers that uti-\nlize these sparse outputs. However, the preceding\nlayer, namely the up project for OPT and Falcon,\nmust be fully present in memory. To avoid loading\nthe entire up projection matrix, we follow Liu et al.\n(2023b), and employ a low-rank predictor to iden-\ntify the elements zeroed by ReLU (see Figure 3b).\nIn contrast to their work, our predictor needs only\nthe output of the current layer\u2019s attention module,\nand not the previous layer\u2019s FFN module. We have\nobserved that postponing the prediction to current\nlayer is sufficient for hardware aware weight load-\ning algorithm design, but leads to more accurate\nTable 1: Using predictors doesn\u2019t change the accuracy\nof zero-shot metrics significantly as predictor of each\nlayer accurately identifies sparsity\nZero-Shot Task\nOPT 6.7B\nwith Predictor\nArc Easy\n66.1\n66.2\nArc Challenge\n30.6\n30.6\nHellaSwag\n50.3\n49.8\noutcome due to deferred inputs. We thereby only\nload elements indicated by the predictor.\nNeuron Data Management via Sliding Win-\ndow Technique. In our study, we define an active\nneuron as one that yields a positive output in our\nlow rank predictor model. Our approach focuses\non managing neuron data by employing a Sliding\nWindow Technique. This technique entails main-\ntaining a DRAM cache of of only the weight rows\nthat were predicted to be required by the the re-\ncent subset of input tokens. The key aspect of this\ntechnique is the incremental loading of neuron data\nthat differs between the current input token and its\nimmediate predecessors. This strategy allows for\nefficient memory utilization, as it frees up memory\nresources previously allocated to cached weights\nrequired by tokens that are no longer within the\nsliding window (as depicted in Figure 4b).\nFrom a mathematical standpoint, let sagg(k) de-\nnote the cumulative use of neuron data across a\nsequence of k input tokens. Our memory architec-\nture is designed to store an average of sagg(k) in\nDRAM. As we process each new token, the incre-\nmental neuron data, which is mathematically repre-\nsented as sagg(k+1)\u2212sagg(k), is loaded from flash\n4\n0\n5\n10\n15\n20\n25\n30\nWindow size\n0\n10\n20\n30\n40\n50\nPercentage\nAggregated Usage\nIncremental Transfer\nsagg(k)\nsagg(k + 1) \u2212 sagg(k)\n(a) aggregated neuron use\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nNeurons to be deleted\nNew Neurons\nNeurons from initial window\nActive neurons in the initial window\nActive neurons in the new window\nInitial Window\nOnce\nUpon\nA\nTime\nThere\nWas\nA\nKid\nWho\nHad\nA\nDream\nSliding Window \nOnce\nUpon\nA\nTime\nThere\nWas\nA\nKid\nWho\nHad\nA\nDream\n(b) sliding window\nFigure 4: (a) Aggregated neuron use of the tenth layer of Falcon 7B. As it can be seen the slope of aggregated\nneuron use is decreasing. Other layers exhibit the same pattern. (b) Instead of deleting neurons that brought to\nDRAM we keep the active neurons of past 5 tokens: when the new token \"Was\" is being processed only a few\namount of data needs to be changed.\nmemory into DRAM. This practice is grounded in\nthe observed trend of decreasing aggregated neuron\nusage over time. Consequently, larger values of k\nresult in a lesser volume of data being loaded for\neach new token. (refer to Figure 4a), while smaller\nvalues of k can help conserve DRAM that is used\nto store the cached weights. In determining the\nsize of the sliding window, the aim is to maximize\nit within the constraints imposed by the available\nmemory capacity.\n3.2\nImproving Transfer Throughput with\nIncreased Chunk Sizes\nTo increase data throughput from flash memory, it\nis crucial to read data in larger chunks, preferably\nsized as the multiples of the block size of the un-\nderlying storage pool. In this section, we detail the\nstrategy we have employed to augment the chunk\nsizes for more efficient flash memory reads.\nBundling Columns and Rows. For OPT and\nFalcon models, the usage of the ith column from\nthe upward projection and the ith row from the\ndownward projection coincides with the activation\nof the ith intermediate neuron. Consequently, by\nstoring these corresponding columns and rows to-\ngether in flash memory, we can consolidate the data\ninto larger chunks for reading. Refer to Figure 5\nfor an illustration of this bundling approach. If\neach element of weights of the network is stored in\nnum_bytes such bundling doubles the chunk size\nfrom dmodel\u00d7num_bytes to 2dmodel\u00d7num_bytes as\nshown in Figure 5. Our analysis and experiment\nshow this increases the throughput of the model.\nBundling Based on Co-activation. We had a\nconjecture that neurons may be highly correlated\n0\n0\n0\n0\nPredictor\u2019s \nOutput \nDown Proj\u2028\nRows\nUp Proj \u2028\nColumns\nFlash Memory\nload \u2028\nfrom \u2028\nflash\nFigure 5: By bundling columns of up project and rows\nof down project in OPT 6.7B we will load 2x chunks\ninstead of reading columns or rows separately.\nin their activity patterns, which may enable further\nbundling. To verify this we calculated the activa-\ntions of neurons over C4 validation dataset. For\neach neuron the coactivation of that neuron with\nother ones forms a power law distribution as de-\npicted in Figure 6a. Now, let\u2019s call the neuron that\ncoactivates with a neuron the most closest friend.\nIndeed, the closest friend of each neuron coacti-\nvates with it very often. As Figure 6b demonstrates,\nit is interesting to see each neuron and its closest\nfriend coactivate with each other at least 95% of\nthe times. The graphs for the 4th closest friend\nand 8th closest friend are also drawn. Based on\nthis information we decided to put a bundle of each\nneuron and its closest friend in the flash memory;\nwhenever a neuron is predicted to be active we\u2019ll\nbring its closes friend too. Unfortunately, this re-\nsulted in loading highly active neurons multiple\ntimes and the bundling worked against our original\n5\n20 100 200 300 400 500 600 700 800 900 1000\nTop Co-activated Neurons\n10\n30\n50\n70\n90\n100\nFrequency (%)\n(a) coactivation intensity\n94\n95\n96\n97\n98\n99\n100\nPercentage of coactivation\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nNumber of neurons\n(b) Closest friend\n60\n70\n80\n90\n100\nPercentage of coactivation\n0\n100\n200\n300\n400\n500\n600\n700\nNumber of neurons\n(c) 4th closest friend\n50\n60\n70\n80\n90\n100\nPercentage of coactivation\n0\n200\n400\n600\n800\n1000\n1200\nNumber of neurons\n(d) 8th closest friend\nFigure 6: (a) For a randomly selected neuron from the 10th layer of OPT 6.7B, there exist a group of neurons which\nare coactivated with high probability (b) The closest friend of a neuron is defined as the most coactivated neuron\nin the same layer, and the closet friend of every neuron in OPT 6.7B almost always get coactivated. (c) The 3rd\nclosest friend gets coactivatd with each neuron 86% of the time in average (d) The 7th closest friend seems to be\nless relevant and doesn\u2019t coactivate with the neuron very often.\nintention. It means, the neurons that are very active\nare \u2018closest friend\u2018 of almost everyone. We inten-\ntionally present this negative result, as we believe\nit may lead to interesting future research studies on\nhow to effectively bundle the neurons and how to\nleverage it for efficient inference.\n3.3\nOptimized Data Management in DRAM\nAlthough data transfer within DRAM is more ef-\nficient compared to accessing flash memory, it\nstill incurs a non-negligible cost. When introduc-\ning data for new neurons, reallocating the matrix\nand appending new matrices can lead to signifi-\ncant overhead due to the need for rewriting exist-\ning neurons data in DRAM. This is particularly\ncostly when a substantial portion (approximately\n25%) of the Feed-Forward Networks (FFNs) in\nDRAM needs to be rewritten.\nTo address this\nissue, we adopt an alternative memory manage-\nment strategy. This involves the preallocation of\nall necessary memory and the establishment of\na corresponding data structure for efficient man-\nagement. The data structure comprises elements\nsuch as pointers, matrix, bias, num_used, and\nlast_k_active shown in Figure 7.\nEach row in the matrix represents the concate-\nnated row of the \u2019up project\u2019 and the column of\nthe \u2019down project\u2019 of a neuron. The pointer vec-\ntor indicates the original neuron index correspond-\ning to each row in the matrix. The bias for the\n\u2019up project\u2019 in the original model is represented in\nthe corresponding bias element. The num_used\nparameter tracks the number of rows currently\nutilized in the matrix, initially set to zero. The\nmatrix for the ith layer is pre-allocated with a size\nof Reqi \u00d7 2dmodel, where Reqi denotes the maxi-\nmum number of neurons required for the specified\nwindow size in a subset of C4 validation set. By\nallocating a sufficient amount of memory for each\nlayer in advance, we minimize the need for fre-\nquent reallocation. Finally, the last_k_active\ncomponent identifies the neurons from the original\nmodel that were most recently activated using the\nlast k tokens.\nThe following operations are done during infer-\nence as depicted in Figure 7.\n1. Deleting Neurons: Neurons that are no longer\nrequired are identified efficiently in linear time,\nutilizing the last_k_active data and the cur-\nrent prediction.\nThe matrix, pointer, and\nscalars of these redundant neurons are re-\nplaced with the most recent elements, and their\ncount is subtracted from num_rows. For O(c)\nneurons to be deleted, a memory rewrite of the\norder O(c \u00d7 dmodel) is required.\n2. Bringing in New Neurons: Necessary neuron\ndata is retrieved from flash memory. The cor-\nresponding pointers and scalars are read from\nDRAM, and these rows are then inserted into\nthe matrix, extending from num_row to num_row\n+ num_new. This approach eliminates the need\nfor reallocating memory in DRAM and copying\nexisting data, reducing inference latency.\n3. Inference\nProcess:\nFor\nthe\ninfer-\nence\noperation,\nthe\nfirst\nhalf\nof\nthe\nmatrix[:num_rows,:d_model] is used as the\n\u2019up project\u2019, and the transposed second half,\nmatrix[:num_rows,d_model:].transpose(),\nserves as the \u2019down project\u2019. This configuration\nis possible because the order of neurons in the\nintermediate output of the feed-forward layer\ndoes not alter the final output, allowing for a\nstreamlined inference process.\n6\n1\n10\n15\n5\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n\ue0881\nCopy\nPointer\n1\n5\n15\n5\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n0.5\n0.4\n0.2\n0.4\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n\ue0881\nScalar\n num_rows \ue09b \n4\n1\n5\n15\n7\n9\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n0.5\n0.4\n0.2\n0.4\n0.3\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n num_rows \ue09b \n5\nTo be deleted\nRemaining\nNew\nPointer Scalar\n0.5\n0.7\n0.2\n0.4\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n\ue0881\n\ue0881\nPointer\nScalar\n1. Start deletion\n2. Deletion complete\n3. Insertion complete\n num_rows \ue09b \n3\nFigure 7: Memory management, first we copy last elements to deleting neurons to maintain a consecutive block of\nmemory then the required ones are stack to the end, this prevents from copying whole data multiple times\nThese steps collectively ensure efficient memory\nmanagement during inference, optimizing the neu-\nral network\u2019s performance and resource utilization.\n4\nResults\nExperimental Setup: Our experiment is designed\nto optimize inference efficiency on personal de-\nvices. To this end, we process sequences individ-\nually, running only one sequence at a time. This\napproach allows us to allocate a specific portion of\nDRAM for the Key-Value (KV) cache while pri-\nmarily focusing on the model size. This strategy is\nparticularly effective when dealing with only one\nsequence/query at a time.2\nFor the implementation of our inference pro-\ncess, we utilize the HuggingFace\u2019s transformers\nand KV caching. This setup is tested under the\ncondition where approximately half of the model\nsize is available in DRAM. We select this amount\nas a showcase of the idea of hosting the LLM in\nflash. With a different level of sparsity or employ-\ning quantization, one can work with smaller avail-\nable DRAM capacity as well. Such a configuration\ndemonstrates the practicality of executing inference\nwith lower memory footprints.\nHardware Configuration. Our models are eval-\nuated using two distinct hardware setups.\nThe\nfirst setup includes an Apple M1 Max with a 1TB\nsolid-state drive (SSD) for flash memory. In this\nconfiguration, computations are performed on the\nCPU, and the models are maintained in a 32-bit\n2For OPT 6.7 B model with context length 2048 KV-cache\nrequires 2048 \u00d7 2dmodel elements which is only 8% of model\nsize. Also the KV-cache itself can be held in flash memory.\nformat. The second setup involves a Linux ma-\nchine equipped with a 24 GB NVIDIA GeForce\nRTX 4090 graphics card. For this machine, com-\nputations are GPU-based, and models are run in\nthe bfloat16 format. For both setups, we operate\nunder the assumption that almost half of the total\navailable memory (DRAM plus GPU memory) is\nallocated for model computations.\nModels. We use OPT 6.7B (Zhang et al., 2022b)\nand a sparsified Falcon 7B (Mirzadeh et al., 2023)\nmodel for our evaluations.\nBaselines. For methods not employing sparsity\nor weight sharing, at least half of the model must be\ntransferred from flash memory during the forward\npass. This necessity arises because, initially, only\nhalf of the model is available in DRAM, but as the\nforward pass progresses, the entire model capacity\nis utilized. Consequently, any data not present at\nthe start must be transferred at least once. Thus, the\nmost efficient theoretical baseline involves loading\nhalf of the model size from the flash memory into\nDRAM. This optimal I/O scenario serves as our\nprimary baseline. Comparative methods, such as\nFlexGen (Sheng et al., 2023) and Petals (Borzunov\net al., 2023), are also constrained by the limited\navailable DRAM or GPU memory, and therefore\ncannot surpass this theoretical I/O efficiency.\nFlash memory Data Loading Implementation.\nTo optimize data loading from flash memory, our\nsystem employs reads parallelized over 32 threads.\nThis multithreaded approach is intended to both\nbetter amortize latency to first byte by not wait-\ning for each read sequentially, and maximize read\nthroughput by reading multiple streams at once\n(Figure 2b).\n7\nCaching Considerations for Data Loading\nfrom Flash Memory. When data is read from flash\nmemory, the operating system typically caches\nthese pages, anticipating future reuse. However,\nthis caching mechanism consumes additional mem-\nory in DRAM beyond what is allocated for the\nmodel. To accurately assess the real throughput\nof flash memory under limited DRAM conditions,\nbenchmarks should be conducted without relying\non caching. Practical systems may or may not rely\non filesystem cache, depending on requirements.\nFor the purpose of our hardware benchmarking\nin this study, we deliberately and significantly\npessimize our NVMe throughput measurements.\nOn macOS and iOS, we employ the F_NOCACHE\nflag with the fcntl() function, while on Linux,\nwe use DirectIO. Additionally, on macOS, we\nclear any resident buffers before initiating the\nbenchmark using the purge command.\nThis\napproach provides a conservative lower bound\nof throughput in scenarios where no caching is\npermitted, and makes the benchmarks repeatable.\nIt\u2019s worth noting that these figures can improve if\neither the inference code or the operating system\nis allowed to cache some part of the weights.\nWhile OS-level buffer caching is advantageous\nfor general purpose applications with high cache\nhit rates, it lacks fine-grained control over cache\nusage per process or buffer eviction at the appli-\ncation level. In the context of on-device memory\nconstraints and large model sizes, this could lead to\na situation where filesystem level does not help, be-\ncause in order to evaluate later layers earlier layers\nmust be evicted in a rolling pattern, so the effective\ncache hit rate is close to zero. Aside from being\ninefficient, this can cause coexistence issues with\nother processes due to memory allocation pressure\nand Translation Lookaside Buffer (TLB) churn.\n4.1\nResults for OPT 6.7B Model\nThis section presents the outcomes for the OPT\n6.7B model, specifically under conditions where\nthe memory allocated for the model in DRAM is\napproximately half of its baseline requirement.\nPredictors. For the initial 28 layers of the OPT\n6.7B model, we train predictors with a rank of\nr = 128. To reduce the occurrence of false nega-\ntives, the final four layers employ predictors with a\nhigher rank of r = 1024. These predictors achieve\nan average of 5% false negatives and 7% false posi-\ntives in the OPT 6.7B model. As depicted in Figure\n3a, our predictor accurately identifies most acti-\nvated neurons, while occasionally misidentifying\ninactive ones with values near zero. Notably, these\nfalse negatives, being close to zero, do not signifi-\ncantly alter the final output when they are excluded.\nFurthermore, as demonstrated in Table 1, this level\nof prediction accuracy does not adversely affect the\nmodel\u2019s performance in 0-shot tasks.\nWindowing in the OPT 6.7B Model. Utilizing\na windowing method with k = 5 in the OPT 6.7B\nmodel significantly reduces the necessity for fresh\ndata loading. Using active neurons of predictor\nwould require about 10% of the DRAM memory\ncapacity in average; however, with our method, it\ndrops to 2.4%. This process involves reserving\nDRAM memory for a window of the past 5 tokens,\nwhich, in turn, increases the DRAM requirement\nfor the Feed Forward Network (FFN) to 24%.\nThe overall memory retained in DRAM for the\nmodel comprises several components: Embed-\ndings, the Attention Model, the Predictor, and the\nLoaded Feed Forward layer.\nThe Predictor ac-\ncounts for 1.25% of the model size, while Em-\nbeddings constitute 3%. The Attention Model\u2019s\nweights make up 32.3%, and the FFN occupies\n15.5% (calculated as 0.24\u00d764.62). Summing these\nup, the total DRAM memory usage amounts to\n52.1% of the model\u2019s size.\nLatency Analysis: Using a window size of 5,\neach token requires access to 2.4% of the Feed\nForward Network (FFN) neurons. For a 32-bit\nmodel, the data chunk size per read is 2dmodel \u00d7\n4 bytes = 32 KiB, as it involves concatenated rows\nand columns. On an M1 Max, this results in the\naverage latency of 125ms per token for loading\nfrom flash and 65ms for memory management (in-\nvolving neuron deletion and addition). Thus, the\ntotal memory-related latency is less than 190ms\nper token (refer to Figure 1). In contrast, the base-\nline approach, which requires loading 13.4GB of\ndata at a speed of 6.1GB/s, leads to a latency of\napproximately 2330ms per token. Therefore, our\nmethod represents a substantial improvement over\nthe baseline.\nFor a 16-bit model on a GPU machine, the flash\nload time is reduced to 40.5ms, and memory man-\nagement takes 40ms, slightly higher due to the\nadditional overhead of transferring data from CPU\nto GPU. Nevertheless, the baseline method\u2019s I/O\ntime remains above 2000 milliseconds.\nDetailed comparisons of how each method im-\npacts performance are provided in Table 2.\n8\nTable 2: The I/O latency of OPT 6.7B 16 bit on M1 max for different techniques when half the memory is available\nConfiguration\nPerformance Metrics\nHybrid\nPredictor\nWindowing\nBundling\nDRAM (GB)\nFlash\u2192 DRAM(GB)\nThroughput (GB/s)\nI/O Latency (ms)\n\u2717\n\u2717\n\u2717\n\u2717\n0\n13.4 GB\n6.10 GB/s\n2130 ms\n\u2713\n\u2717\n\u2717\n\u2717\n6.7\n6.7 GB\n6.10 GB/s\n1090 ms\n\u2713\n\u2713\n\u2717\n\u2717\n4.8\n0.9 GB\n1.25 GB/s\n738 ms\n\u2713\n\u2713\n\u2713\n\u2717\n6.5\n0.2 GB\n1.25 GB/s\n164 ms\n\u2713\n\u2713\n\u2713\n\u2713\n6.5\n0.2 GB\n2.25 GB/s\n87 ms\n4.2\nResults for Falcon 7B Model\nTo verify that our findings generalize beyond OPT\nmodels we also apply the idea of LLM in flash to\nFalcon model. Since, the base line Falcon model is\nnot sparse, we used a sparsified (relufied) version\nwith almost the same performance as that of the\nbase version (Mirzadeh et al., 2023). Similar to\nprevious section, we present the results obtained\nunder the condition that approximately half of the\nmodel size is available for use in DRAM.\nPredictors. In the Falcon 7B model, predictors\nof rank r = 256 are used for the initial 28 layers,\nand r = 1152 for the last four layers.\nWindow Configuration. Our model reserves\nmemory for a window containing the last 4 tokens.\nThis setup utilizes 33% of the Feed Forward Net-\nwork (FFN). In terms of memory allocation, em-\nbeddings take 4.2% of the model size, attention\nweights account for 19.4%, and predictors require\n4%. The active portion of the FFN, given our win-\ndow size, is 25.3% (calculated as 0.33 \u00d7 76.8).\nOverall, this amounts to 52.93% of the model\u2019s\ntotal size.\nLatency Analysis. Using a window size of 4\nin our model requires accessing 3.1% of the Feed\nForward Network (FFN) neurons for each token. In\na 32-bit model, this equates to a data chunk size of\n35.5 KiB per read (calculated as 2dmodel \u00d74 bytes).\nOn an M1 Max device, the time taken to load this\ndata from flash memory is approximately 161ms,\nand the memory management process adds another\n90ms, leading to a total latency of 250ms per token.\nIn comparison, the baseline latency is around 2330\nmilliseconds, making our method approximately 9\nto 10 times faster.\n5\nRelated Works\nEfficient Inference for Large Language Models.\nAs LLMs grow in size, reducing their computa-\ntional and memory requirements for inference has\nbecome an active area of research. Approaches\nbroadly fall into two categories: model compres-\nsion techniques like pruning and quantization (Han\net al., 2016b; Sun et al., 2023; Jaiswal et al., 2023;\nXia et al., 2023), (Zhang et al., 2022a; Xu et al.,\n2023; Shao et al., 2023; Lin et al., 2023; Hoang\net al., 2023; Zhao et al., 2023; Ahmadian et al.,\n2023; Liu et al., 2023a; Li et al., 2023), and se-\nlective execution like sparse activations (Liu et al.,\n2023b), (Mirzadeh et al., 2023) or conditional com-\nputation (Graves, 2016; Baykal et al., 2023). Our\nwork is complementary, focusing on minimizing\ndata transfer from flash memory during inference.\nSelective Weight Loading. Most related to our\napproach is prior work on selective weight loading.\nSparseGPU (Narang et al., 2021) exploits activa-\ntion sparsity to load a subset of weights for each\nlayer. However, it still requires loading from RAM.\nFlexgen (Sheng et al., 2023) offloads the weights\nand kv-cache from GPU memory to DRAM and\nDRAM to flash memory, in contrast we consider\nonly the cases the full model can\u2019t reside in the\nwhole DRAM and GPU memory on the edge de-\nvices. Flexgen is theoretically bound by the slow\nthroughput of flash to DRAM in such scenarios.\nFirefly (Narang et al., 2022) shares our goal of\ndirect flash access but relies on a hand-designed\nschedule for loading. In contrast, we propose a\ncost model to optimize weight loading. Similar\ntechniques have been explored for CNNs (Parashar\net al., 2017), (Rhu et al., 2013). Concurrently,\nAdapt (Subramani et al., 2022) has proposed adap-\ntive weight loading for vision transformers. We\nfocus on transformer-based LLMs and introduce\ntechniques like neuron bundling tailored to LLMs.\nTo hide flash latency, we build on speculative\nexecution techniques like SpAtten (Dai et al., 2021;\nBae et al., 2023). But, we introduce lightweight\nspeculation tailored to adaptive weight loading.\nHardware Optimizations. There is a rich body\nof work on hardware optimizations for efficient\nLLM inference, including efficient memory ar-\nchitectures (Agrawal et al., 2022), (Gao et al.,\n9\n2022), dataflow optimizations (Han et al., 2016a),\n(Shao et al., 2022), hardware evaluation frame-\nworks Zhang2023AHE, and flash optimizations\n(Ham et al., 2016), (Meswani et al., 2015). We fo-\ncus on algorithmic improvements, but these could\nprovide additional speedups.\nSpeculative Execution. Speculative decoding\n(Leviathan et al., 2022; Zhang et al., 2023; He et al.,\n2023) is a technique that uses a draft model for\ngeneration and uses the larger model to verify those\ntokens. This technique is orthogonal to us and\ncan be used for further improvement. In case of\nspeculative decoding, the window in our method\nshould be updated with multiple tokens rather one.\nMixture of Experts. Mixture of Experts (Yi\net al., 2023) have a sparse structure in their feed\nforward layer and can leverage our method for\nenabling larger models on device.\nIn summary, we propose algorithmic techniques\nto minimize weight loading from flash memory dur-\ning LLM inference. By combining cost modeling,\nsparsity prediction, and hardware awareness, we\ndemonstrate 4-5x and 20-25x speedup on CPU and\nGPU, respectively.\n6\nConclusion and Discussion\nIn this study, we have tackled the significant chal-\nlenge of running large language models (LLMs)\non devices with constrained memory capacities.\nOur approach, deeply rooted in the understand-\ning of flash memory and DRAM characteristics,\nrepresents a novel convergence of hardware-aware\nstrategies and machine learning. By developing an\ninference cost model that aligns with these hard-\nware constraints, we have introduced two inno-\nvative techniques: \u2019windowing\u2019 and \u2019row-column\nbundling.\u2019 These methods collectively contribute\nto a significant reduction in the data load and an\nincrease in the efficiency of memory usage. Weight\nbundling and windowing are two very basic tech-\nniques aimed at showcasing the potentials to in-\ncrease chunk size and read sequentiality while re-\nducing data transfer through sparsity. Numerous\nopportunities exist for developing smarter and more\nefficient methods to achieve these objectives.\nThe practical outcomes of our research are note-\nworthy. We have demonstrated the ability to run\nLLMs up to twice the size of available DRAM,\nachieving an acceleration in inference speed by\n4-5x compared to traditional loading methods in\nCPU, and 20-25x in GPU. This innovation is par-\nticularly crucial for deploying advanced LLMs in\nresource-limited environments, thereby expanding\ntheir applicability and accessibility. The PyTorch\nbased implementation for forward pass have only\nundergone algorithmic (as opposed to systems)\noptimization. Significant additional gains are ex-\npected from a custom lower level implementation.\nOur work not only provides a solution to a\ncurrent computational bottleneck but also sets a\nprecedent for future research. It underscores the\nimportance of considering hardware characteristics\nin\nthe\ndevelopment\nof\ninference-optimized\nalgorithms, suggesting a promising direction for\nfurther explorations in this domain. We believe\nas LLMs continue to grow in size and complexity,\napproaches like this work will be essential for\nharnessing their full potential in a wide range of\ndevices and applications.\nOur study represents an initial endeavor in the\npursuit of democratizing Large Language Model\n(LLM) inference, making it accessible to a wider\narray of individuals and devices. We recognize that\nthis early effort has its limitations, which, in turn,\nopen up compelling avenues for future research. A\ncritical aspect for future exploration is the analy-\nsis of power consumption and thermal limitations\ninherent in the methods we propose, particularly\nfor on-device deployment. Currently, our focus\nis on single-batch inference. However, expanding\nthis to include scenarios like prompt processing,\nmulti-batch inference, and speculative decoding\npresents itself as a valuable area for further investi-\ngation. In our initial proof of concept, we operated\nunder the assumption of memory availability being\nhalf the size of the model. Exploring the dynam-\nics of working with varying memory sizes\u2014both\nlarger and smaller\u2014introduces a fascinating bal-\nance between latency and accuracy, and is a com-\npelling area for future exploration. In conclusion,\nour methodology is constructed on the foundation\nof sparsified networks. Nonetheless, the underlying\nconcept holds potential for broader applications. It\ncan be adapted to selectively load weights in non-\nsparse networks or to dynamically retrieve model\nweights from flash storage. This adaptation would\nbe contingent on the specific requirements of the\ninput prompt or the contextual parameters provided.\nSuch an approach suggests a versatile strategy for\nmanaging model weights, optimizing performance\nbased on the nature of the input, thereby enhancing\nthe efficiency, usefulness, and applicability of the\nproposed scheme in various scenarios dealing with\nLarge Language Models (LLMs).\n10\nAcknowledgements\nWe would like to thank Itay Sagron, Lailin Chen,\nMahyar Najibi, Qichen Fu, Moin Nabi, Peter Zat-\nloukal, Arsalan Farooq, Sachin Mehta, Mohammad\nSamragh, Matt Johnson, Etai Zaltsman, Lin Chang,\nDominic Giampaolo, Taal Uliel, Hadi Pouransari,\nFartash Faghri, Oncel Tuzel, Samy Bengio, Ruom-\ning Pang, Chong Wang, Ronan Collobert, David\nGrangier, and Aftab Munshi for the valuable feed-\nback and discussions.\nReferences\nUdit Agrawal,\nRangharajan Venkatesan,\nBrucek\nKhailany, Stephen W Keckler, and William J Dally.\n2022. Atomlayer: minimizing dram data movement\nfor ultra-sparse models on gpus. In Proceedings of\nthe 27th ACM International Conference on Archi-\ntectural Support for Programming Languages and\nOperating Systems, pages 223\u2013238.\nArash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat\nVenkitesh, Stephen Gou, Phil Blunsom, A. Ustun,\nand Sara Hooker. 2023. Intriguing properties of quan-\ntization at scale. ArXiv, abs/2305.19268.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMaitha Alhammadi, Mazzotta Daniele, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. The falcon series of language models: To-\nwards open frontier models.\nReza Yazdani Aminabadi, Samyam Rajbhandari, Am-\nmar Ahmad Awan, Cheng Li, Du Li, Elton Zheng,\nOlatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff\nRasley, et al. 2022. Deepspeed-inference: enabling\nefficient inference of transformer models at unprece-\ndented scale. In SC22: International Conference for\nHigh Performance Computing, Networking, Storage\nand Analysis, pages 1\u201315. IEEE.\nSangmin Bae, Jongwoo Ko, Hwanjun Song, and\nSe-Young Yun. 2023.\nFast and robust early-\nexiting framework for autoregressive language mod-\nels with synchronized parallel decoding.\nArXiv,\nabs/2310.05424.\nCenk Baykal, Dylan Cutler, Nishanth Dikkala, Nikhil\nGhosh, Rina Panigrahy, and Xin Wang. 2023. Al-\nternating updates for efficient transformers. ArXiv,\nabs/2301.13310.\nAlexander Borzunov, Dmitry Baranchuk, Tim Dettmers,\nMaksim Riabinin, Younes Belkada, Artem Chu-\nmachenko, Pavel Samygin, and Colin Raffel. 2023.\nPetals: Collaborative inference and fine-tuning of\nlarge models. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 3: System Demonstrations), pages\n558\u2013568, Toronto, Canada. Association for Compu-\ntational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHan Dai, Yi Zhang, Ziyu Gong, Nanqing Yang, Wei Dai,\nEric Song, and Qiankun Xie. 2021. Spatten: Efficient\nsparse attention architecture with cascade token and\nhead pruning. In Advances in Neural Information\nProcessing Systems, volume 34.\nErich Elsen, Augustus Odena, Maxwell Nye, Sa\u02d8g-\nnak Ta\u00b8s\u0131rlar, Tri Dao, Curtis Hawthorne, Deepak\nMoparthi, and Arushi Somani. 2023.\nReleasing\nPersimmon-8B.\nMingyu Gao, Jie Yu, Wentai Li, Michael C Dai,\nNam Sung Kim, and Krste Asanovic. 2022. com-\nputedram: In-memory compute using off-the-shelf\ndram. In Proceedings of the 27th ACM International\nConference on Architectural Support for Program-\nming Languages and Operating Systems, pages 1065\u2013\n1079.\nAlex Graves. 2016. Adaptive computation time for re-\ncurrent neural networks. In International Conference\non Machine Learning, pages 3500\u20133509. PMLR.\nJongmin Ham, Jinha Kim, Jinwoong Choi, Cheolwoo\nCho, Seulki Hong, Kyeongsu Han, and Taejoo Chung.\n2016. Graphssd: a high performance flash-based stor-\nage system for large-scale graph processing. In 2016\nUSENIX Annual Technical Conference (USENIXATC\n16), pages 243\u2013256.\nSong Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pe-\ndram, Mark A Horowitz, and William J Dally. 2016a.\nEie: efficient inference engine on compressed deep\nneural network. arXiv preprint arXiv:1602.01528.\nSong Han, Huizi Mao, and William J Dally. 2016b.\nDeep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huff-\nman coding. In International Conference on Learn-\ning Representations (ICLR).\nZhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee,\nand Di He. 2023. Rest: Retrieval-based speculative\ndecoding. ArXiv, abs/2311.08252.\nDuc Nien Hoang, Minsik Cho, Thomas Merth, Moham-\nmad Rastegari, and Zhangyang Wang. 2023. (dy-\nnamic) prompting might be all you need to repair\ncompressed llms. ArXiv, abs/2310.00867.\n11\nAjay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang,\nZhangyang Wang, and Yinfei Yang. 2023. Compress-\ning llms: The truth is rarely pure and never simple.\nArXiv, abs/2310.01382.\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2022. Fast inference from transformers via spec-\nulative decoding.\nJiaxi Li and Wei Lu. 2023. Contextual distortion reveals\nconstituency: Masked language models are implicit\nparsers. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5208\u20135222, Toronto,\nCanada. Association for Computational Linguistics.\nLiang Li, Qingyuan Li, Bo Zhang, and Xiangxiang\nChu. 2023. Norm tweaking: High-performance low-\nbit quantization of large language models. ArXiv,\nabs/2309.02784.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang,\nXingyu Dang, and Song Han. 2023. Awq: Activation-\naware weight quantization for llm compression and\nacceleration. ArXiv, abs/2306.00978.\nZechun Liu, Barlas O\u02d8guz, Changsheng Zhao, Ernie\nChang, Pierre Stock, Yashar Mehdad, Yangyang\nShi, Raghuraman Krishnamoorthi, and Vikas Chan-\ndra. 2023a.\nLlm-qat:\nData-free quantization\naware training for large language models. ArXiv,\nabs/2305.17888.\nZichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang\nYuan, Zhao Song, Anshumali Shrivastava, Ce Zhang,\nYuandong Tian, Christopher Re, et al. 2023b. Deja\nvu: Contextual sparsity for efficient llms at infer-\nence time. In International Conference on Machine\nLearning, pages 22137\u201322176. PMLR.\nMoinuddin K Meswani, Sergey Blagodurov, David\nRoberts, John Slice, Mike Ignatowski, and Gabriel\nLoh. 2015. Neural cache: Bit-serial in-cache acceler-\nation of deep neural networks. In 2015 48th Annual\nIEEE/ACM International Symposium on Microarchi-\ntecture (MICRO), pages 383\u2013394. IEEE.\nIman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo\nC Del Mundo, Oncel Tuzel, Golnoosh Samei, Mo-\nhammad Rastegari, and Mehrdad Farajtabar. 2023.\nRelu strikes back: Exploiting activation sparsity in\nlarge language models.\nSharan Narang, Logan Feistel, Erich Elsen Undersander,\nCindy Song, and Gregory Diamos. 2022. Firefly:\nA lightweight system for running multi-billion pa-\nrameter models on commodity hardware. In 2022\nACM/IEEE 49th Annual International Symposium\non Computer Architecture (ISCA), pages 757\u2013771.\nIEEE.\nSharan Narang, Erich Elsen Undersander, and Gregory\nDiamos. 2021. Sparse gpu kernels for deep learning.\nIn International Conference on Learning Representa-\ntions.\nAngshuman Parashar, Minsoo Rhu, Anurag Mukkara,\nAntonio Puglielli, Rangharajan Venkatesan, Brucek\nKhailany, Joel Emer, Stephen W Keckler, and\nWilliam J Dally. 2017. Timeloop: A systematic ap-\nproach to dnn accelerator evaluation. In 2017 IEEE\nInternational Symposium on Performance Analysis\nof Systems and Software (ISPASS), pages 241\u2013251.\nIEEE.\nSamyam Rajbhandari, Olatunji Ruwase, Jeff Rasley,\nShaden Smith, and Yuxiong He. 2021. Zero-infinity:\nBreaking the gpu memory wall for extreme scale\ndeep learning. In SC21: International Conference for\nHigh Performance Computing, Networking, Storage\nand Analysis, pages 1\u201314.\nMinsoo Rhu, Natalia Gimelshein, Jason Clemons,\nArslan Zulfiqar, and Stephen W Keckler. 2013.\nvdnn: Virtualized deep neural networks for scalable,\nmemory-efficient neural network design. In 2016\n49th Annual IEEE/ACM International Symposium on\nMicroarchitecture (MICRO), page Article 13. IEEE\nComputer Society.\nWenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng\nXu, Lirui Zhao, Zhiqiang Li, Kaipeng Zhang, Peng\nGao, Yu Jiao Qiao, and Ping Luo. 2023. Omniquant:\nOmnidirectionally calibrated quantization for large\nlanguage models. ArXiv, abs/2308.13137.\nYifan Shao, Mengjiao Li, Wenhao Cai, Qi Wang,\nDhananjay Narayanan,\nand Parthasarathy Ran-\nganathan. 2022. Hotpot: Warmed-up gigascale infer-\nence with tightly-coupled compute and reuse in flash.\nIn Proceedings of the 55th Annual IEEE/ACM In-\nternational Symposium on Microarchitecture, pages\n335\u2013349.\nYing Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan\nLi, Max Ryabinin, Beidi Chen, Percy Liang, Christo-\npher R\u00e9, Ion Stoica, and Ce Zhang. 2023. Flexgen:\nHigh-throughput generative inference of large lan-\nguage models with a single GPU. In International\nConference on Machine Learning, ICML 2023, 23-29\nJuly 2023, Honolulu, Hawaii, USA, volume 202 of\nProceedings of Machine Learning Research, pages\n31094\u201331116. PMLR.\nVedant Subramani, Marios Savvides, Li Ping, and Sha-\nran Narang. 2022. Adapt: Parameter adaptive token-\nwise inference for vision transformers. In Proceed-\nings of the 55th Annual IEEE/ACM International\nSymposium on Microarchitecture.\nMingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter.\n2023. A simple and effective pruning approach for\nlarge language models. ArXiv, abs/2306.11695.\nHaojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang,\nZhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and\nShuaiwen Leon Song. 2023. Flash-llm: Enabling\nlow-cost and highly-efficient large generative model\ninference with unstructured sparsity. Proc. VLDB\nEndow., 17:211\u2013224.\n12\nZhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue\nWang, Kaixiong Zhou, Xia Hu, and Anshumali Shri-\nvastava. 2023. Compress, then prompt: Improving\naccuracy-efficiency trade-off of llm inference with\ntransferable prompt. ArXiv, abs/2305.11186.\nRongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shang-\nguang Wang, and Mengwei Xu. 2023. Edgemoe:\nFast on-device inference of moe-based large language\nmodels. ArXiv, abs/2308.14352.\nJinchao Zhang, Jue Wang, Huan Li, Lidan Shou,\nKe Chen, Gang Chen, and Sharad Mehrotra. 2023.\nDraft & verify: Lossless large language model ac-\nceleration via self-speculative decoding.\nArXiv,\nabs/2309.08168.\nShizhao Zhang, Han Dai, Tian Sheng, Jiawei Zhang,\nXiaoyong Li, Qun Xu, Mengjia Dai, Yunsong Xiao,\nChao Ma, Rui Tang, et al. 2022a. Llm quantization:\nQuantization-aware training for large language mod-\nels. In Advances in Neural Information Processing\nSystems, volume 35.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022b.\nOPT: open pre-trained transformer language mod-\nels. CoRR, abs/2205.01068.\nYilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn\nChen, Size Zheng, Luis Ceze, Arvind Krishnamurthy,\nTianqi Chen, and Baris Kasikci. 2023. Atom: Low-\nbit quantization for efficient and accurate llm serving.\nArXiv, abs/2310.19102.\n13\n"
  },
  {
    "title": "Gemini: A Family of Highly Capable Multimodal Models",
    "link": "https://arxiv.org/pdf/2312.11805.pdf",
    "upvote": "40",
    "text": "Gemini: A Family of Highly Capable\nMultimodal Models\nGemini Team, Google1\nThis report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities\nacross image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano\nsizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained\nuse-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model\nadvances the state of the art in 30 of 32 of these benchmarks \u2014 notably being the first model to achieve\nhuman-expert performance on the well-studied exam benchmark MMLU, and improving the state of the\nart in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of\nGemini models in cross-modal reasoning and language understanding will enable a wide variety of use\ncases and we discuss our approach toward deploying them responsibly to users.\n1. Introduction\nWe present Gemini, a family of highly capable multimodal models developed at Google. We trained\nGemini jointly across image, audio, video, and text data for the purpose of building a model with both\nstrong generalist capabilities across modalities alongside cutting-edge understanding and reasoning\nperformance in each respective domain.\nGemini 1.0, our first version, comes in three sizes: Ultra for highly-complex tasks, Pro for enhanced\nperformance and deployability at scale, and Nano for on-device applications. Each size is specifically\ntailored to address different computational limitations and application requirements. We evaluate\nthe performance of Gemini models on a comprehensive suite of internal and external benchmarks\ncovering a wide range of language, coding, reasoning, and multimodal tasks.\nGemini advances state-of-the-art in large-scale language modeling (Anil et al., 2023; Brown et al.,\n2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al., 2019; Rae\net al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiy et al., 2020;\nOpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al., 2023; Zhang\net al., 2023), and video understanding(Alayrac et al., 2022; Chen et al., 2023). It also builds on the\nwork on sequence models (Sutskever et al., 2014), a long history of work in deep learning based\non neural networks (LeCun et al., 2015), and machine learning distributed systems (Barham et al.,\n2022; Bradbury et al., 2018; Dean et al., 2012) that enable large-scale training.\nOur most capable model, Gemini Ultra, achieves new state-of-the-art results in 30 of 32 benchmarks\nwe report on, including 10 of 12 popular text and reasoning benchmarks, 9 of 9 image understanding\nbenchmarks, 6 of 6 video understanding benchmarks, and 5 of 5 speech recognition and speech\ntranslation benchmarks. Gemini Ultra is the first model to achieve human-expert performance on\nMMLU (Hendrycks et al., 2021a) \u2014 a prominent benchmark testing knowledge and reasoning via a\nsuite of exams \u2014 with a score above 90%. Beyond text, Gemini Ultra makes notable advances on\nchallenging multimodal reasoning tasks. For example, on the recent MMMU benchmark (Yue et al.,\n2023), that comprises questions about images on multi-discipline tasks requiring college-level subject\n1See Contributions and Acknowledgments section for full author list.\nPlease send correspondence to gemini-1-\nreport@google.com\n\u00a9 2023 Google. All rights reserved\narXiv:2312.11805v1  [cs.CL]  19 Dec 2023\nGemini: A Family of Highly Capable Multimodal Models\nknowledge and deliberate reasoning, Gemini Ultra achieves a new state-of-the-art score of 62.4%,\noutperforming the previous best model by more than 5 percentage points. It provides a uniform\nperformance lift for video question answering and audio understanding benchmarks.\nQualitative evaluation showcases impressive crossmodal reasoning capabilities, enabling the model\nto understand and reason across an input sequence of audio, images, and text natively (see Figure 5\nand Table 13). Consider the educational setting depicted in Figure 1 as an example. A teacher has\ndrawn a physics problem of a skier going down a slope, and a student has worked through a solution\nto it. Using Gemini\u2019s multimodal reasoning capabilities, the model is able to understand the messy\nhandwriting, correctly understand the problem formulation, convert both the problem and solution\nto mathematical typesetting, identify the specific step of reasoning where the student went wrong in\nsolving the problem, and then give a worked through correct solution to the problem. This opens up\nexciting educational possibilities, and we believe the new multimodal and reasoning capabilities of\nGemini models have dramatic applications across many fields.\nFigure 1 | Verifying a student\u2019s solution to a physics problem. The model is able to correctly recognize\nall of the handwritten content and verify the reasoning. On top of understanding the text in the\nimage, it needs to understand the problem setup and correctly follow instructions to generate LATEX.\nThe reasoning capabilities of large language models show promise toward building generalist\nagents that can tackle more complex multi-step problems. The AlphaCode team built AlphaCode\n2 (Leblond et al, 2023), a new Gemini-powered agent, that combines Gemini\u2019s reasoning capabilities\nwith search and tool-use to excel at solving competitive programming problems. AlphaCode 2\nranks within the top 15% of entrants on the Codeforces competitive programming platform, a large\nimprovement over its state-of-the-art predecessor in the top 50% (Li et al., 2022).\n2\nGemini: A Family of Highly Capable Multimodal Models\nIn tandem, we advance the frontier of efficiency with Gemini Nano, a series of small models\ntargeting on-device deployment. These models excel in on-device tasks, such as summarization,\nreading comprehension, text completion tasks, and exhibit impressive capabilities in reasoning, STEM,\ncoding, multimodal, and multilingual tasks relative to their sizes.\nIn the following sections, we first provide an overview of the model architecture, training infras-\ntructure, and training dataset. We then present detailed evaluations of the Gemini model family,\ncovering well-studied benchmarks and human-preference evaluations across text, code, image, audio\nand video \u2014 which include both English performance and multilingual capabilities. We also discuss\nour approach to responsible deployment,2 including our process for impact assessments, developing\nmodel policies, evaluations, and mitigations of harm before deployment decisions. Finally, we discuss\nthe broader implications of Gemini, its limitations alongside its potential applications \u2014 paving the\nway for a new era of research and innovation in AI.\n2. Model Architecture\nGemini models build on top of Transformer decoders (Vaswani et al., 2017) that are enhanced with\nimprovements in architecture and model optimization to enable stable training at scale and optimized\ninference on Google\u2019s Tensor Processing Units. They are trained to support 32k context length,\nemploying efficient attention mechanisms (for e.g. multi-query attention (Shazeer, 2019)). Our first\nversion, Gemini 1.0, comprises three main sizes to support a wide range of applications as discussed\nin Table 1.\nModel size\nModel description\nUltra\nOur most capable model that delivers state-of-the-art performance across a wide\nrange of highly complex tasks, including reasoning and multimodal tasks. It is\nefficiently serveable at scale on TPU accelerators due to the Gemini architecture.\nPro\nA performance-optimized model in terms of cost as well as latency that delivers\nsignificant performance across a wide range of tasks. This model exhibits strong\nreasoning performance and broad multimodal capabilities.\nNano\nOur most efficient model, designed to run on-device. We trained two versions of\nNano, with 1.8B (Nano-1) and 3.25B (Nano-2) parameters, targeting low and high\nmemory devices respectively. It is trained by distilling from larger Gemini models. It\nis 4-bit quantized for deployment and provides best-in-class performance.\nTable 1 | An overview of the Gemini 1.0 model family.\nGemini models are trained to accommodate textual input interleaved with a wide variety of audio\nand visual inputs, such as natural images, charts, screenshots, PDFs, and videos, and they can produce\ntext and image outputs (see Figure 2). The visual encoding of Gemini models is inspired by our own\nfoundational work on Flamingo (Alayrac et al., 2022), CoCa (Yu et al., 2022a), and PaLI (Chen et al.,\n2022), with the important distinction that the models are multimodal from the beginning and can\nnatively output images using discrete image tokens (Ramesh et al., 2021; Yu et al., 2022b).\nVideo understanding is accomplished by encoding the video as a sequence of frames in the large\ncontext window. Video frames or images can be interleaved naturally with text or audio as part of the\nmodel input. The models can handle variable input resolution in order to spend more compute on\n2We plan to update this report with more details ahead of the general availability of the Gemini Ultra model.\n3\nGemini: A Family of Highly Capable Multimodal Models\nFigure 2 | Gemini supports interleaved sequences of text, image, audio, and video as inputs (illustrated\nby tokens of different colors in the input sequence). It can output responses with interleaved image\nand text.\ntasks that require fine-grained understanding. In addition, Gemini can directly ingest audio signals at\n16kHz from Universal Speech Model (USM) (Zhang et al., 2023) features. This enables the model to\ncapture nuances that are typically lost when the audio is naively mapped to a text input (for example,\nsee audio understanding demo on the website).\nTraining the Gemini family of models required innovations in training algorithms, dataset, and\ninfrastructure. For the Pro model, the inherent scalability of our infrastructure and learning algorithms\nenable us to complete pretraining in a matter of weeks, leveraging a fraction of the Ultra\u2019s resources.\nThe Nano series of models leverage additional advancements in distillation and training algorithms\nto produce the best-in-class small language models for a wide variety of tasks, such as summarization\nand reading comprehension, which power our next generation on-device experiences.\n3. Training Infrastructure\nWe trained Gemini models using TPUv5e and TPUv4 (Jouppi et al., 2023), depending on their sizes\nand configuration. Training Gemini Ultra used a large fleet of TPUv4 accelerators across multiple\ndatacenters. This represents a significant increase in scale over our prior flagship model PaLM-2\nwhich presented new infrastructure challenges. Scaling up the number of accelerators results in a\nproportionate decrease in the mean time between failure of hardware in the overall system. We\nminimized the rate of planned reschedules and preemptions, but genuine machine failures are\ncommonplace across all hardware accelerators at such large scales.\nTPUv4 accelerators are deployed in \u201cSuperPods\u201d of 4096 chips, each connected to a dedicated\noptical switch, which can dynamically reconfigure 4x4x4 chip cubes into arbitrary 3D torus topologies\nin around 10 seconds (Jouppi et al., 2023). For Gemini Ultra, we decided to retain a small number of\ncubes per superpod to allow for hot standbys and rolling maintenance.\nTPU accelerators primarily communicate over the high speed inter-chip-interconnect, but at\nGemini Ultra scale, we combine SuperPods in multiple datacenters using Google\u2019s intra-cluster and\ninter-cluster network (Poutievski et al., 2022; Wetherall et al., 2023; yao Hong et al., 2018). Google\u2019s\nnetwork latencies and bandwidths are sufficient to support the commonly used synchronous training\n4\nGemini: A Family of Highly Capable Multimodal Models\nparadigm, exploiting model parallelism within superpods and data-parallelism across superpods.\nThe \u2018single controller\u2019 programming model of Jax (Bradbury et al., 2018) and Pathways (Barham\net al., 2022) allows a single Python process to orchestrate the entire training run, dramatically\nsimplifying the development workflow. The GSPMD partitioner (Xu et al., 2021) in the XLA compiler\npartitions the training step computation, and the MegaScale XLA compiler (XLA, 2019) pass statically\nschedules appropriate collectives so that they maximally overlap with the computation with very little\nvariation in step time.\nMaintaining a high goodput3 at this scale would have been impossible using the conventional\napproach of periodic checkpointing of weights to persistent cluster storage. For Gemini, we instead\nmade use of redundant in-memory copies of the model state, and on any unplanned hardware failures,\nwe rapidly recover directly from an intact model replica. Compared to both PaLM and PaLM-2 (Anil\net al., 2023), this provided a substantial speedup in recovery time, despite the significantly larger\ntraining resources being used. As a result, the overall goodput for the largest-scale training job\nincreased from 85% to 97%.\nTraining at unprecedented scale invariably surfaces new and interesting systems failure modes -\nand in this instance one of the problems that we needed to address was that of \u201cSilent Data Corruption\n(SDC)\u201d (Dixit et al., 2021; Hochschild et al., 2021; Vishwanathan et al., 2015). Although these are\nextremely rare, the scale of Gemini means that we can expect SDC events to impact training every\nweek or two. Rapidly detecting and removing faulty hardware required several new techniques\nthat exploit deterministic replay to isolate incorrect computations, combined with proactive SDC\nscanners on idle machines and hot standbys. Our fully deterministic infrastructure allowed us to\nquickly identify root causes (including hardware failures) during the development leading up to the\nUltra model, and this was a crucial ingredient towards stable training.\n4. Training Dataset\nGemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining\ndataset uses data from web documents, books, and code, and includes image, audio, and video data.\nWe use the SentencePiece tokenizer (Kudo and Richardson, 2018) and find that training the\ntokenizer on a large sample of the entire training corpus improves the inferred vocabulary and\nsubsequently improves model performance. For example, we find Gemini models can efficiently\ntokenize non-Latin scripts which can, in turn, benefit model quality as well as training and inference\nspeed.\nThe number of tokens used to train the largest models were determined following the approach\nin Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve\nperformance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a).\nWe apply quality filters to all datasets, using both heuristic rules and model-based classifiers.\nWe also perform safety filtering to remove harmful content. We filter our evaluation sets from our\ntraining corpus. The final data mixtures and weights were determined through ablations on smaller\nmodels. We stage training to alter the mixture composition during training \u2013 increasing the weight of\ndomain-relevant data towards the end of training. We find that data quality is critical to a highly-\nperforming model, and believe that many interesting questions remain around finding the optimal\ndataset distribution for pretraining.\n3We define goodput as the time spent computing useful new steps over the elapsed time of the training job.\n5\nGemini: A Family of Highly Capable Multimodal Models\n5. Evaluation\nThe Gemini models are natively multimodal, as they are trained jointly across text, image, audio,\nand video. One open question is whether this joint training can result in a model which has strong\ncapabilities in each domain \u2013 even when compared to models and approaches that are narrowly\ntailored to single domains. We find this to be the case: Gemini sets a new state of the art across a\nwide range of text, image, audio, and video benchmarks.\n5.1. Text\n5.1.1. Academic Benchmarks\nWe compare Gemini Pro and Ultra to a suite of external LLMs and our previous best model PaLM\n2 across a series of text-based academic benchmarks covering reasoning, reading comprehension,\nSTEM, and coding. We report these results in Table 2. Broadly, we find that the performance of\nGemini Pro outperforms inference-optimized models such as GPT-3.5 and performs comparably with\nseveral of the most capable models available, and Gemini Ultra outperforms all current models. In\nthis section, we examine some of these findings.\nOn MMLU (Hendrycks et al., 2021a), Gemini Ultra can outperform all existing models, achieving\nan accuracy of 90.04%. MMLU is a holistic exam benchmark, which measures knowledge across a\nset of 57 subjects. Human expert performance is gauged at 89.8% by the benchmark authors, and\nGemini Ultra is the first model to exceed this threshold, with the prior state-of-the-art result at 86.4%.\nAchieving high performance requires specialist knowledge across many domains (e.g. law, biology,\nhistory, etc.), alongside reading comprehension and reasoning. We find Gemini Ultra achieves highest\naccuracy when used in combination with a chain-of-thought prompting approach (Wei et al., 2022)\nthat accounts for model uncertainty. The model produces a chain of thought with k samples, for\nexample 8 or 32. If there is a consensus above a preset threshold (selected based on the validation\nsplit), it selects this answer, otherwise it reverts to a greedy sample based on maximum likelihood\nchoice without chain of thought. We refer the reader to appendix for a detailed breakdown of how\nthis approach compares with only chain-of-thought prompting or only greedy sampling.\nIn mathematics, a field commonly used to benchmark the analytical capabilities of models, Gemini\nUltra shows strong performance on both elementary exams and competition-grade problem sets. For\nthe grade-school math benchmark, GSM8K (Cobbe et al., 2021), we find Gemini Ultra reaches 94.4%\naccuracy with chain-of-thought prompting and self-consistency (Wang et al., 2022) compared to\nthe previous best accuracy of 92% with the same prompting technique. Similar positive trends are\nobserved in increased difficulty math problems drawn from middle- and high-school math competitions\n(MATH benchmark), with the Gemini Ultra model outperforming all competitor models, reaching\n53.2% using 4-shot prompting. The model also outperforms the state of the art on even harder tasks\nderived from American Mathematical Competitions (150 questions from 2022 and 2023). Smaller\nmodels perform poorly on this challenging task scoring close to random, but Gemini Ultra can solve\n32% of the questions, compared to the 30% solve rate for GPT-4.\nGemini Ultra also excels in coding, a popular use case of current LLMs. We evaluate the model\non many conventional and internal benchmarks and also measure its performance as part of more\ncomplex reasoning systems such as AlphaCode 2 (see section 5.1.7 on complex reasoning systems).\nFor example, on HumanEval, a standard code-completion benchmark (Chen et al., 2021) mapping\nfunction descriptions to Python implementations, instruction-tuned Gemini Ultra correctly implements\n74.4% of problems. On a new held-out evaluation benchmark for python code generation tasks,\nNatural2Code, where we ensure no web leakage, Gemini Ultra achieves the highest score of 74.9%.\n6\nGemini: A Family of Highly Capable Multimodal Models\nGemini\nUltra\nGemini\nPro\nGPT-4\nGPT-3.5\nPaLM 2-L\nClaude 2\nInflect-\nion-2\nGrok 1\nLLAMA-2\nMMLU\nMultiple-choice questions\nin 57 subjects\n(professional &\nacademic)\n(Hendrycks et al., 2021a)\n90.04%\nCoT@32\u2217\n83.7%\n5-shot\n79.13%\nCoT@8\u2217\n71.8%\n5-shot\n87.29%\nCoT@32\n(via API\u2217\u2217)\n86.4%\n5-shot\n(reported)\n70%\n5-shot\n78.4%\n5-shot\n78.5%\n5-shot CoT\n79.6%\n5-shot\n73.0%\n5-shot\n68.0%\u2217\u2217\u2217\nGSM8K\nGrade-school math\n(Cobbe et al., 2021)\n94.4%\nMaj1@32\n86.5%\nMaj1@32\n92.0%\nSFT &\n5-shot CoT\n57.1%\n5-shot\n80.0%\n5-shot\n88.0%\n0-shot\n81.4%\n8-shot\n62.9%\n8-shot\n56.8%\n5-shot\nMATH\nMath problems across\n5 difficulty levels &\n7 subdisciplines\n(Hendrycks et al., 2021b)\n53.2%\n4-shot\n32.6%\n4-shot\n52.9%\n4-shot\n(via API\u2217\u2217)\n50.3%\n(Zheng et al.,\n2023)\n34.1%\n4-shot\n(via API\u2217\u2217)\n34.4%\n4-shot\n\u2014\n34.8%\n23.9%\n4-shot\n13.5%\n4-shot\nBIG-Bench-Hard\nSubset of hard BIG-bench\ntasks written as CoT prob-\nlems\n(Srivastava et al., 2022)\n83.6%\n3-shot\n75.0%\n3-shot\n83.1%\n3-shot\n(via API\u2217\u2217)\n66.6%\n3-shot\n(via API\u2217\u2217)\n77.7%\n3-shot\n\u2014\n\u2014\n\u2014\n51.2%\n3-shot\nHumanEval\nPython coding tasks\n(Chen et al., 2021)\n74.4%\n0-shot (IT)\n67.7%\n0-shot (IT)\n67.0%\n0-shot\n(reported)\n48.1%\n0-shot\n\u2014\n70.0%\n0-shot\n44.5%\n0-shot\n63.2%\n0-shot\n29.9%\n0-shot\nNatural2Code\nPython code generation.\n(New held-out set with no\nleakage on web)\n74.9%\n0-shot\n69.6%\n0-shot\n73.9%\n0-shot\n(via API\u2217\u2217)\n62.3%\n0-shot\n(via API\u2217\u2217)\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\nDROP\nReading comprehension\n& arithmetic.\n(metric: F1-score)\n(Dua et al., 2019)\n82.4\nVariable\nshots\n74.1\nVariable\nshots\n80.9\n3-shot\n(reported)\n64.1\n3-shot\n82.0\nVariable\nshots\n\u2014\n\u2014\n\u2014\n\u2014\nHellaSwag\n(validation set)\nCommon-sense multiple\nchoice questions\n(Zellers et al., 2019)\n87.8%\n10-shot\n84.7%\n10-shot\n95.3%\n10-shot\n(reported)\n85.5%\n10-shot\n86.8%\n10-shot\n\u2014\n89.0%\n10-shot\n\u2014\n80.0%\u2217\u2217\u2217\nWMT23\nMachine translation (met-\nric: BLEURT)\n(Tom et al., 2023)\n74.4\n1-shot (IT)\n71.7\n1-shot\n73.8\n1-shot\n(via API\u2217\u2217)\n\u2014\n72.7\n1-shot\n\u2014\n\u2014\n\u2014\n\u2014\nTable 2 | Gemini performance on text benchmarks with external comparisons and PaLM 2-L.\n\u2217 The model produces a chain of thought with k = 8 or 32 samples, if there is a consensus above a threshold (chosen based on the validation\nsplit), it selects this answer, otherwise it reverts to a greedy sample. Further analysis in Appendix 9.1.\n\u2217\u2217 Results self-collected via the API in Nov, 2023.\n\u2217\u2217\u2217 Results shown use the decontaminated numbers from Touvron et al. (2023b) report as the most relevant comparison to Gemini models\nwhich have been decontaminated as well.\nEvaluation on these benchmarks is challenging and may be affected by data contamination. We\nperformed an extensive leaked data analysis after training to ensure the results we report here are as\nscientifically sound as possible, but still found some minor issues and decided not to report results on\ne.g. LAMBADA (Paperno et al., 2016). As part of the evaluation process, on a popular benchmark,\nHellaSwag (Zellers et al., 2019), we find that an additional hundred finetuning steps on specific\nwebsite extracts corresponding to the HellaSwag training set (which were not included in Gemini\npretraining set) improve the validation accuracy of Gemini Pro to 89.6% and Gemini Ultra to 96.0%,\nwhen measured with 1-shot prompting (we measured GPT-4 obtained 92.3% when evaluated 1-shot\nvia the API). This suggests that the benchmark results are susceptible to the pretraining dataset\ncomposition. We choose to report HellaSwag decontaminated results only in a 10-shot evaluation\nsetting. We believe there is a need for more robust and nuanced standardized evaluation benchmarks\nwith no leaked data. So, we evaluate Gemini models on several new held-out evaluation datasets\nthat were recently released, such as WMT23 and Math-AMC 2022-2023 problems, or internally\ngenerated from non-web sources, such as Natural2Code. We refer the reader to the appendix for a\ncomprehensive list of our evaluation benchmarks.\n7\nGemini: A Family of Highly Capable Multimodal Models\nEven so, model performance on these benchmarks gives us an indication of the model capabilities\nand where they may provide impact on real-world tasks. For example, Gemini Ultra\u2019s impressive\nreasoning and STEM competencies pave the way for advancements in LLMs within the educational\ndomain4. The ability to tackle complex mathematical and scientific concepts opens up exciting\npossibilities for personalized learning and intelligent tutoring systems.\n5.1.2. Trends in Capabilities\nWe investigate the trends in capabilities across the Gemini model family by evaluating them on a\nholistic harness of more than 50 benchmarks in six different capabilities, noting that some of the\nmost notable benchmarks were discussed in the last section. These capabilities are: \u201cFactuality\u201d\ncovering open/closed-book retrieval and question answering tasks; \u201cLong-Context\u201d covering long-\nform summarization, retrieval and question answering tasks; \u201cMath/Science\u201d including tasks for\nmathematical problem solving, theorem proving, and scientific exams; \u201cReasoning\u201d tasks that require\narithmetic, scientific, and commonsense reasoning; \u201cMultilingual\u201d tasks for translation, summarization,\nand reasoning in multiple languages. Please see appendix for a detailed list of tasks included for each\ncapability.\nFactuality\nLong-Context\nMath/Science\nSummarization\nReasoning\nMultilinguality\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nNormalized Performance vs Pro\nNano 1\nNano 2\nPro\nUltra\nFigure 3 | Language understanding and generation performance of Gemini model family across\ndifferent capabilities (normalized by the Gemini Pro model).\nWe observe consistent quality gains with increased model size in Figure 3, especially in reasoning,\nmath/science, summarization and long-context. Gemini Ultra is the best model across the board for\nall six capabilities. Gemini Pro, the second-largest model in the Gemini family of models, is also quite\ncompetitive while being a lot more efficient to serve.\n5.1.3. Nano\nBringing AI closer to the user, we discuss the Gemini Nano 1 and Nano 2 models engineered for\non-device deployments. These models excel in summarization and reading comprehension tasks\nwith per-task finetuning. Figure 3 shows the performance of these pretrained models in comparison\nto the much larger Gemini Pro model, while Table 3 dives deeper into specific factuality, coding,\nMath/Science, and reasoning tasks. Nano-1 and Nano-2 model sizes are only 1.8B and 3.25B\nparameters respectively. Despite their size, they show exceptionally strong performance on factuality,\ni.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and\n4See demos on website https://deepmind.google/gemini.\n8\nGemini: A Family of Highly Capable Multimodal Models\nmultilingual tasks. With new capabilities accessible to a broader set of platforms and devices, the\nGemini models expand accessibility to everyone.\nGemini Nano 1\nGemini Nano 2\naccuracy\nnormalized\nby Pro\naccuracy\nnormalized\nby Pro\nBoolQ\n71.6\n0.81\n79.3\n0.90\nTydiQA (GoldP)\n68.9\n0.85\n74.2\n0.91\nNaturalQuestions (Retrieved)\n38.6\n0.69\n46.5\n0.83\nNaturalQuestions (Closed-book) 18.8\n0.43\n24.8\n0.56\nBIG-Bench-Hard (3-shot)\n34.8\n0.47\n42.4\n0.58\nMBPP\n20.0\n0.33\n27.2\n0.45\nMATH (4-shot)\n13.5\n0.41\n22.8\n0.70\nMMLU (5-shot)\n45.9\n0.64\n55.8\n0.78\nTable 3 | Performance of Gemini Nano series on factuality, summarization, reasoning, coding and\nSTEM tasks compared to significantly larger Gemini Pro model.\n5.1.4. Multilinguality\nThe multilingual capabilities of the Gemini models are evaluated using a diverse set of tasks requir-\ning multilingual understanding, cross-lingual generalization, and the generation of text in multiple\nlanguages. These tasks include machine translation benchmarks (WMT 23 for high-medium-low\nresource translation; Flores, NTREX for low and very low resource languages), summarization bench-\nmarks (XLSum, Wikilingua), and translated versions of common benchmarks (MGSM: professionally\ntranslated into 11 languages).\nMachine Translation\nTranslation is a canonical benchmark in machine learning with a rich history.\nWe evaluated Gemini Ultra with instruction-tuning applied (see section 6.4.2) on the entire set of\nlanguage pairs in the WMT 23 translation benchmark in a few-shot setting. Overall, we found that\nGemini Ultra (and other Gemini models) performed remarkably well at translating from English to any\nother language, and surpassed the LLM-based translation methods when translating out-of-English,\non high-resource, mid-resource and low-resource languages. In the WMT 23 out-of-English translation\ntasks, Gemini Ultra achieved the highest LLM-based translation quality, with an average BLEURT\n(Sellam et al., 2020) score of 74.8, compared to GPT-4\u2019s score of 73.6, and PaLM 2\u2019s score of 72.2.\nWhen averaged across all language pairs and directions for WMT 23, we see a similar trend with\nGemini Ultra 74.4, GPT-4 73.8 and PaLM 2-L 72.7 average BLEURT scores on this benchmark.\nWMT 23\n(Avg BLEURT)\nGemini Ultra\nGemini Pro\nGemini Nano 2\nGemini Nano 1\nGPT-4\nPaLM 2-L\nHigh Resource\n74.2\n71.7\n67.7\n64.1\n74.0\n72.6\nMid Resource\n74.7\n71.8\n67.0\n64.8\n73.6\n72.7\nOut-of-English\n74.8\n71.5\n66.2\n65.2\n73.6\n72.2\nInto-English\n73.9\n72.0\n69.0\n63.5\n74.1\n73.4\nAll languages\n74.4\n71.7\n67.4\n64.8\n73.8\n72.7\nTable 4 | Performance of Gemini models on WMT 23 translation benchmark. All numbers with 1-shot.\nIn addition to the languages and translation tasks above, we also evaluate Gemini Ultra on very\nlow-resource languages. These languages were sampled from the tail of the following language sets:\nFlores-200 (Tamazight and Kanure), NTREX (North Ndebele), and an internal benchmark (Quechua).\n9\nGemini: A Family of Highly Capable Multimodal Models\nFor these languages, both from and into English, Gemini Ultra achieved an average chrF score of 27.0\nin 1-shot setup, while the next-best model, PaLM 2-L, achieved a score of 25.3.\nMultilingual Math and Summarization\nBeyond translation, we evaluated how well Gemini per-\nforms in challenging tasks across a range of languages. We specifically investigated the math bench-\nmark MGSM (Shi et al., 2023), which is a translated variant of the math benchmark GSM8K (Cobbe\net al., 2021). We find Gemini Ultra achieves an accuracy of 79.0%, an advance over PaLM 2-L which\nscores 74.7%, when averaged across all languages in an 8-shot setup. We also benchmark Gemini on\nthe multilingual summarization benchmarks \u2013 XLSum (Hasan et al., 2021) and WikiLingua (Ladhak\net al., 2020). In XLSum, Gemini Ultra reached an average of 17.6 rougeL score compared to 15.4 for\nPaLM 2. For Wikilingua, Gemini Ultra (5-shot) trails behind PaLM 2 (3-shot) measured in BLEURT\nscore. See Table 5 for the full results. Overall the diverse set of multilingual benchmarks show that\nGemini family models have a broad language coverage, enabling them to also reach locales and\nregions with low-resource languages.\nGemini Ultra\nGemini Pro\nGPT-4\nPaLM 2-L\nMGSM\n(8-shot)\n79.0\n63.5\n74.5\n74.7\nXLsum\n(3-shot)\n17.6\n16.2\n\u2014\n15.4\nWikilingua\n48.9\n47.8\n\u2014\n50.4\nTable 5 | Performance of Gemini models on multilingual math and summarization.\n5.1.5. Long Context\nGemini models are trained with a sequence length of 32,768 tokens and we find that they make use\nof their context length effectively. We first verify this by running a synthetic retrieval test: we place\nkey-value pairs at the beginning of the context, then add long filler text, and ask for value associated\nwith a particular key. We find that the Ultra model retrieves the correct value with 98% accuracy\nwhen queried across the full context length. We further investigate this by plotting the negative log\nlikelihood (NLL) versus the token index across a held-out set of long documents in Figure 4. We\nfind that the NLL decreases with sequence position up to the full 32K context length. The longer\ncontext length of Gemini models enable new use cases such as retrieval over documents and video\nunderstanding discussed in section 5.2.2.\n8\n16\n32\n64\n128\n256\n512\n1K\n2K\n4K\n8K\n16K\n32K\nSequence position\nNLL\nPro\nUltra\nFigure 4 | Negative log likelihood as a function of token index across 32K context length on a held-out\nset of long documents.\n10\nGemini: A Family of Highly Capable Multimodal Models\n5.1.6. Human Preference Evaluations\nHuman preference of the model outputs provides an important indication of quality that complements\nautomated evaluations. We have evaluated the Gemini models in side-by-side blind evaluations where\nhuman raters judge responses of two models to the same prompt. We instruction tune (Ouyang et al.,\n2022) the pretrained model using techniques discussed in the section 6.4.2. The instruction-tuned\nversion of the model is evaluated on a range of specific capabilities, such as following instructions,\ncreative writing, multimodal understanding, long-context understanding, and safety. These capabili-\nties encompass a range of use cases inspired by current user needs and research-inspired potential\nfuture use cases.\nInstruction-tuned Gemini Pro models provide a large improvement on a range of capabilities,\nincluding preference for the Gemini Pro model over the PaLM 2 model API, 65.0% time in creative\nwriting, 59.2% in following instructions, and 68.5% time for safer responses as shown in Table 6.\nThese improvements directly translate into a more helpful and safer user experience.\nCreativity\nInstruction Follow-\ning\nSafety\nWin-rate\n65.0%\n59.2%\n68.5%\n95% Conf. Interval\n[62.9%, 67.1%]\n[57.6%, 60.8%]\n[66.0%, 70.8%]\nTable 6 | Win rate of Gemini Pro over PaLM 2 (text-bison@001) with 95% confidence intervals.\n5.1.7. Complex Reasoning Systems\nGemini can also be combined with additional techniques such as search and tool-use to create\npowerful reasoning systems that can tackle more complex multi-step problems. One example of such\na system is AlphaCode 2, a new state-of-the-art agent that excels at solving competitive programming\nproblems (Leblond et al, 2023). AlphaCode 2 uses a specialized version of Gemini Pro \u2013 tuned on\ncompetitive programming data similar to the data used in Li et al. (2022) \u2013 to conduct a massive\nsearch over the space of possible programs. This is followed by a tailored filtering, clustering and\nreranking mechanism. Gemini Pro is fine-tuned both to be a coding model to generate proposal\nsolution candidates, and to be a reward model that is leveraged to recognize and extract the most\npromising code candidates.\nAlphaCode 2 is evaluated on Codeforces,5 the same platform as AlphaCode, on 12 contests from\ndivision 1 and 2, for a total of 77 problems. AlphaCode 2 solved 43% of these competition problems, a\n1.7x improvement over the prior record-setting AlphaCode system which solved 25%. Mapping this to\ncompetition rankings, AlphaCode 2 built on top of Gemini Pro sits at an estimated 85th percentile on\naverage \u2013 i.e. it performs better than 85% of entrants. This is a significant advance over AlphaCode,\nwhich only outperformed 50% of competitors.\nThe composition of powerful pretrained models with search and reasoning mechanisms is an\nexciting direction towards more general agents; another key ingredient is deep understanding across\na range of modalities which we discuss in the next section.\n5http://codeforces.com/\n11\nGemini: A Family of Highly Capable Multimodal Models\n5.2. Multimodal\nGemini models are natively multimodal. These models exhibit the unique ability to seamlessly\ncombine their capabilities across modalities (e.g. extracting information and spatial layout out of\na table, a chart, or a figure) with the strong reasoning capabilities of a language model (e.g. its\nstate-of-art-performance in math and coding) as seen in examples in Figures 5 and 12. The models\nalso show strong performance in discerning fine-grained details in inputs, aggregating context across\nspace and time, and applying these capabilities over a temporally-related sequence of video frames\nand/or audio inputs.\nThe sections below provide more detailed evaluation of the model across different modalities\n(image, video, and audio), together with qualitative examples of the model\u2019s capabilities for image\ngeneration and the ability to combine information across different modalities.\n5.2.1. Image Understanding\nWe evaluate the model on four different capabilities: high-level object recognition using captioning or\nquestion-answering tasks such as VQAv2; fine-grained transcription using tasks such as TextVQA and\nDocVQA requiring the model to recognize low-level details; chart understanding requiring spatial\nunderstanding of input layout using ChartQA and InfographicVQA tasks; and multimodal reasoning\nusing tasks such as Ai2D, MathVista and MMMU. For zero-shot QA evaluation, the model is instructed\nto provide short answers aligned with the specific benchmark. All numbers are obtained using greedy\nsampling and without any use of external OCR tools.\nGemini\nUltra\n(pixel only)\nGemini\nPro\n(pixel only)\nGemini\nNano 2\n(pixel only)\nGemini\nNano 1\n(pixel only)\nGPT-4V\nPrior SOTA\nMMMU (val)\nMulti-discipline college-level problems\n(Yue et al., 2023)\n59.4%\npass@1\n62.4%\nMaj1@32\n47.9%\n32.6%\n26.3%\n56.8%\n56.8%\nGPT-4V, 0-shot\nTextVQA (val)\nText reading on natural images\n(Singh et al., 2019)\n82.3%\n74.6%\n65.9%\n62.5%\n78.0%\n79.5%\nGoogle PaLI-3, fine-tuned\nDocVQA (test)\nDocument understanding\n(Mathew et al., 2021)\n90.9%\n88.1%\n74.3%\n72.2%\n88.4%\n(pixel only)\n88.4%\nGPT-4V, 0-shot\nChartQA (test)\nChart understanding\n(Masry et al., 2022)\n80.8%\n74.1%\n51.9%\n53.6%\n78.5%\n(4-shot CoT)\n79.3%\nGoogle DePlot, 1-shot PoT\n(Liu et al., 2023)\nInfographicVQA (test)\nInfographic understanding\n(Mathew et al., 2022)\n80.3%\n75.2%\n54.5%\n51.1%\n75.1%\n(pixel only)\n75.1%\nGPT-4V, 0-shot\nMathVista (testmini)\nMathematical reasoning\n(Lu et al., 2023)\n53.0%\n45.2%\n30.6%\n27.3%\n49.9%\n49.9%\nGPT-4V, 0-shot\nAI2D (test)\nScience diagrams\n(Kembhavi et al., 2016)\n79.5%\n73.9%\n51.0%\n37.9%\n78.2%\n81.4%\nGoogle PaLI-X, fine-tuned\nVQAv2 (test-dev)\nNatural image understanding\n(Goyal et al., 2017)\n77.8%\n71.2%\n67.5%\n62.7%\n77.2%\n86.1%\nGoogle PaLI-X, fine-tuned\nTable 7 | Image understanding Gemini Ultra consistently outperforms existing approaches even in\nzero-shot, especially for OCR-related image understanding tasks for natural images, text, documents,\nand figures without using any external OCR engine (\u2018pixel only\u2019). Many existing approaches fine-tune\non the respective tasks, highlighted in gray, which makes the comparison with 0-shot not apples-to-\napples.\n12\nGemini: A Family of Highly Capable Multimodal Models\nWe find that Gemini Ultra is state of the art across a wide range of image-understanding bench-\nmarks in Table 7. It achieves strong performance across a diverse set of tasks such as answering\nquestions on natural images and scanned documents as well as understanding infographics, charts and\nscience diagrams. When compared against publicly reported results from other models (most notably\nGPT-4V), Gemini is better in zero-shot evaluation by a significant margin. It also exceeds several\nexisting models that are specifically fine-tuned on the benchmark\u2019s training sets for the majority of\ntasks. The capabilities of the Gemini models lead to significant improvements in the state of the art\non academic benchmarks like MathVista (+3.1%)6 or InfographicVQA (+5.2%).\nMMMU (Yue et al., 2023) is a recently released evaluation benchmark, which consists of questions\nabout images across 6 disciplines with multiple subjects within each discipline that require college-\nlevel knowledge to solve these questions. Gemini Ultra achieves the best score on this benchmark\nadvancing the state-of-the-art result by more than 5 percentage points and outperforms the previous\nbest result in 5 of 6 disciplines (see Table 8), thus showcasing its multimodal reasoning capabilities.\nMMMU (val)\nGemini Ultra (0-shot)\nGPT-4V (0-shot)\nMaj@32\npass@1\npass@1\nArt & Design\n74.2\n70.0\n65.8\nBusiness\n62.7\n56.7\n59.3\nScience\n49.3\n48.0\n54.7\nHealth & Medicine\n71.3\n67.3\n64.7\nHumanities & Social Science\n78.3\n78.3\n72.5\nTechnology & Engineering\n53.0\n47.1\n36.7\nOverall\n62.4\n59.4\n56.8\nTable 8 | Gemini Ultra performance on the MMMU benchmark (Yue et al., 2023) per discipline.\nEach discipline covers multiple subjects, requiring college-level knowledge and complex reasoning.\nGemini models are also capable of operating across modalities and a diverse set of global languages\nsimultaneously, both for image understanding tasks (e.g., images containing text in Icelandic) and for\ngeneration tasks (e.g., generating image descriptions for a wide range of languages). We evaluate the\nperformance of generating image descriptions on a selected subset of languages in the Crossmodal-\n3600 (XM-3600) benchmark in a 4-shot setting, using the Flamingo evaluation protocol (Alayrac\net al., 2022), without any fine-tuning for all models. As shown in Table 9, Gemini models achieve a\nsignificant improvement over the existing best model, Google PaLI-X.\nXM-3600 (CIDER)\nGemini Ultra\n4-shot\nGemini Pro\n4-shot\nGoogle PaLI-X\n4-shot\nEnglish\n86.4\n87.1\n77.8\nFrench\n77.9\n76.7\n62.5\nHindi\n31.1\n29.8\n22.2\nModern Hebrew\n54.5\n52.6\n38.7\nRomanian\n39.0\n37.7\n30.2\nThai\n86.7\n77.0\n56.0\nChinese\n33.3\n30.2\n27.7\nAverage (of 7)\n58.4\n55.9\n45.0\nTable 9 | Multilingual image understanding Gemini models outperform existing models in captioning\nimages in many languages when benchmarked on a subset of languages in XM-3600 dataset (Thapliyal\net al., 2022).\n6MathVista is a comprehensive mathematical reasoning benchmark consisting of 28 previously published multimodal\ndatasets and three newly created datasets. Our MathVista results were obtained by running the MathVista authors\u2019\nevaluation script.\n13\nGemini: A Family of Highly Capable Multimodal Models\nFigure 5 | Gemini\u2019s multimodal reasoning capabilities to generate matplotlib code for rearranging\nthe subplots. The multimodal prompt is shown at the top-left in gray. Gemini Ultra\u2019s response,\nincluding its generated code, is shown in the right column in blue. The bottom left figure shows\nrendered version of the generated code. Successfully solving this task shows the model\u2019s capability\nto combine several capabilities: (1) recognition of the functions depicted in the plots; (2) inverse\ngraphics to infer the code that would have generated the subplots; (3) instruction-following to put\nsubplots in their desired positions; and (4) abstract reasoning to infer that the exponential plot must\nstay in its original place, because the sine plot must move out of the way for the 3-dimensional plot.\nQualitative evaluation in Figure 5 illustrates an example of Gemini Ultra\u2019s multimodal reasoning\ncapabilities. The model is required to solve the task of generating matplotlib code that would rearrange\na set of subplots provided by the user. The model output shows that it successfully solves this task\n14\nGemini: A Family of Highly Capable Multimodal Models\ncombining multiple capabilities of understanding the user plot, inferring the code required to generate\nit, following user instructions to put subplots in their desired positions, and abstract reasoning about\nthe output plot. This highlights Gemini Ultra\u2019s native multimodality and eludes to its more complex\nreasoning abilities across interleaved sequences of image and text. We refer the reader to the appendix\nfor more qualitative examples.\n5.2.2. Video Understanding\nUnderstanding video input is an important step towards a useful generalist agent. We measure the\nvideo understanding capability across several established benchmarks that are held-out from training.\nThese tasks measure whether the model is able to understand and reason over a temporally-related\nsequence of frames. For each video task, we sample 16 equally-spaced frames from each video clip\nand feed them to the Gemini models. For the YouTube video datasets (all datasets except NextQA\nand the Perception test), we evaluate the Gemini models on videos that were still publicly available\nin the month of November, 2023.\nGemini Ultra achieves state-of-the-art results on various few-shot video captioning tasks as well as\nzero-shot video question answering tasks as shown in Table 10. This demonstrates its capability of\nstrong temporal reasoning across several frames. Figure 21 in the appendix provides a qualitative\nexample of understanding the video of the ball-striking mechanics of a soccer player and reasoning\nabout the player can improve their game.\nTask\nGemini Ultra\nGemini Pro\nFew-shot SoTA\nVATEX (test)\n62.7\n57.4\n56.0\nEnglish video captioning\n(Wang et al., 2019)\n4-shots\n4-shots\nDeepMind Flamingo, 4-shots\nVATEX ZH (test)\n51.3\n50.0\n\u2013\nChinese video captioning\n(Wang et al., 2019)\n4-shots\n4-shots\nYouCook2 (val)\n135.4\n123.2\n74.5\nEnglish cooking video captioning\n(Zhou et al., 2018)\n4-shots\n4-shots\nDeepMind Flamingo, 4-shots\nNextQA (test)\n29.9\n28.0\n26.7\nVideo question answering\n(Xiao et al., 2021)\n0-shot\n0-shot\nDeepMind Flamingo, 0-shot\nActivityNet-QA (test)\n52.2\n49.8\n45.3\nVideo question answering\n(Yu et al., 2019)\n0-shot\n0-shot\nVideo-LLAVA, 0-shot\nPerception Test MCQA (test)\n54.7\n51.1\n46.3\nVideo question answering\n(P\u0103tr\u0103ucean et al., 2023)\n0-shot\n0-shot\nSeViLA (Yu et al., 2023), 0-shot\nTable 10 | Few-shot video understanding across tasks and languages on selected academic\nbenchmarks. The reported metric is CIDER for video captioning, WUPS for NextQA, and top-1\naccuracy for the Perception Test and ActivityNet-QA. For ActivityNet-QA, we use the Video-LLAVA\n(Lin et al., 2023) evaluation protocol.\n5.2.3. Image Generation\nGemini is able to output images natively, without having to rely on an intermediate natural language\ndescription that can bottleneck the model\u2019s ability to express images. This uniquely enables the model\nto generate images with prompts using interleaved sequences of image and text in a few-shot setting.\nFor example, the user might prompt the model to design suggestions of images and text for a blog\npost or a website (see Figure 10 in the appendix).\n15\nGemini: A Family of Highly Capable Multimodal Models\nFigure 6 shows an example of image generation in 1-shot setting. Gemini Ultra model is prompted\nwith one example of interleaved image and text where the user provides two colors (blue and yellow)\nand image suggestions of creating a cute blue cat or a blue dog with yellow ear from yarn. The\nmodel is then given two new colors (pink and green) and asked for two ideas about what to create\nusing these colors. The model successfully generates an interleaved sequence of images and text with\nsuggestions to create a cute green avocado with pink seed or a green bunny with pink ears from yarn.\nFigure 6 | Image Generation. Gemini can output multiple images interleaved with text given a\nprompt composed of image and text. In the left figure, Gemini Ultra is prompted in a 1-shot setting\nwith a user example of generating suggestions of creating cat and dog from yarn when given two\ncolors, blue and yellow. Then, the model is prompted to generate creative suggestions with two new\ncolors, pink and green, and it generates images of creative suggestions to make a cute green avocado\nwith pink seed or a green bunny with pink ears from yarn as shown in the right figure.\n16\nGemini: A Family of Highly Capable Multimodal Models\n5.2.4. Audio Understanding\nWe evaluate the Gemini Nano-1 and Gemini Pro models on a variety of public benchmarks and\ncompare it with Universal Speech Model (USM) (Zhang et al., 2023) and Whisper (large-v2 (Radford\net al., 2023) or large-v3 (OpenAI, 2023) as indicated). These benchmarks include automatic speech\nrecognition (ASR) tasks such as FLEURS (Conneau et al., 2023), VoxPopuli, (Wang et al., 2021),\nMulti-lingual Librispeech (Pratap et al., 2020), as well as the speech translation task CoVoST 2,\ntranslating different languages into English (Wang et al., 2020). We also report on an internal\nbenchmark YouTube test set. ASR tasks report a word error rate (WER) metric, where a lower number\nis better. Translation tasks report a BiLingual Evaluation Understudy (BLEU) score, where a higher\nnumber is better. FLEURS is reported on 62 languages that have language overlap with the training\ndata. Four segmented languages (Mandarin, Japanese, Korean and Thai) report character error rate\n(CER), instead of WER, similar to Whisper (Radford et al., 2023).\nTable 11 indicates that our Gemini Pro model significantly outperforms the USM and Whisper\nmodels across all ASR and AST tasks, both for English and multilingual test sets. Note that there is a\nlarge gain in FLEURS, compared to USM and Whisper, as our model is also trained with the FLEURS\ntraining dataset. However, training the same model without FLEURS dataset results in a WER of 15.8,\nwhich still outperforms Whisper. Gemini Nano-1 model also outperforms both USM and Whisper on\nall datasets except FLEURS. Note that we did not evaluate Gemini Ultra on audio yet, though we\nexpect better performance from increased model scale.\nTask\nMetric\nGemini\nPro\nGemini\nNano-1\nWhisper\n(OpenAI, 2023;\nRadford et al.,\n2023)\nUSM\n(Zhang\net\nal.,\n2023)\nAutomatic Speech\nRecognition\nYouTube\n(en-us)\nWER (\u2193)\n4.9%\n5.5%\n6.5%\n(v3)\n6.2%\nMultilingual\nLibrispeech\n(en-us)\n(Pratap et al., 2020)\nWER (\u2193)\n4.8%\n5.9%\n6.2%\n(v2)\n7.0 %\nFLEURS\n(62 lang)\n(Conneau et al., 2023)\nWER (\u2193)\n7.6%\n14.2%\n17.6%\n(v3)\n11.8%\nVoxPopuli\n(14 lang)\n(Wang et al., 2021)\nWER (\u2193)\n9.1%\n9.5%\n15.9%\n(v2)\n13.4%\nAutomatic Speech\nTranslation\nCoVoST 2\n(21 lang)\n(Wang et al., 2020)\nBLEU (\u2191)\n40.1\n35.4\n29.1\n(v2)\n30.7\nTable 11 | Speech evaluation results on selected benchmarks for ASR and AST. For ASR, the reported\nmetric is WER where lower is better. For AST, the reported metric is BLEU where higher is better.\nTable 12 shows further error analysis with USM and Gemini Pro. We find that Gemini Pro produces\nmore understandable responses, particularly on rare words and proper nouns.\nDomain\nTruth\nUSM\nGemini Pro\nWav\nFleurs\nScotturb bus 403 travels regularly to\nSintra, stopping at Cabo da Roca.\nScotboard bus four3 traversed regu-\nlarly to Centra stopping at Cabo de\nRoga.\nScotturb bus 403 travels regularly to\nSintra, stopping at Cabo da Roca.\n\u25b2\n)))\nFleurs\nThe archipelago lies 120 km north\nof the Peninsula. The largest is King\nGeorge Island, with the settlement of\nVilla Las Estrellas.\nThe archipelago lines 120 km north\nof peninsula. The largest is Kingurch\nisland with the settlement of Cua\nLosas.\nThe archipelago lies 120 km north\nof the Peninsula. The largest is King\nGeorge Island, with the settlement of\nVilla Las Estrellas.\n\u25b2\n)))\nTable 12 | Qualitative examples for the ASR task in the benchmark. Incorrect transcriptions are\nhighlighted in red.\n17\nGemini: A Family of Highly Capable Multimodal Models\n5.2.5. Modality Combination\nMultimodal demonstrations often include a combination of text interleaved with a single modality,\nusually images. We demonstrate the ability to process a sequence of audio and images natively.\nConsider a cooking scenario about making an omelet where we prompt the model with a sequence\nof audio and images. Table 13 indicates a turn-by-turn interaction with the model, providing pictures\nand verbally asking questions about the next steps for cooking an omelet. We note that the model\nresponse text is reasonably accurate, and shows that model processes fine-grained image details to\nevaluate when the omelet is fully cooked. See demo on the website.\nInput Image\nInput Audio (transcribed)\nModel Response: Text\n\u25b2\n))) What\u2019s the first step to make a veggie omelet\nwith these ingredients?\nCrack the eggs into a bowl and whisk them.\n\u25b2\n))) Thank you for the instructions. I started mak-\ning my omelet, does it look ready now?\nIt looks like it\u2019s almost ready. You can flip it over\nto cook the other side.\n(No image - it\u2019s a follow up\non the previous question)\n\u25b2\n))) Why is it not ready?\nIt\u2019s not ready because the eggs are still runny.\n\u25b2\n))) What about now?\nIt looks ready now. You can take it off the heat and\nserve it.\nTable 13 | Audio-visual qualitative example showcasing the ability of Gemini models to process\ninterleaved sequences of text, vision, and audio, as well as reason across modalities. This example\ninputs interleaved images and audio from the user in a cooking scenario. The user prompts the model\nfor instructions to make an omelet and to inspect whether it is fully cooked.\n18\nGemini: A Family of Highly Capable Multimodal Models\n6. Responsible Deployment\nDuring the development of the Gemini models, we follow a structured approach to responsible\ndeployment in order to identify, measure, and manage foreseeable downstream societal impacts\nof our models, in line with previous releases of Google\u2019s AI technology (Kavukcuoglu et al., 2022).\nThroughout the lifecycle of the project, we follow the structure below. This section outlines our broad\napproach and key findings through this process. We will share more details on this in an upcoming\nreport.\n6.1. Impact Assessment\nWe develop model impact assessments to identify, assess, and document key downstream societal\nbenefits and harms associated with the development of advanced Gemini models. These are informed\nby prior academic literature on language model risks (Weidinger et al., 2021), findings from similar\nprior exercises conducted across the industry (Anil et al., 2023; Anthropic, 2023; OpenAI, 2023a),\nongoing engagement with experts internally and externally, and unstructured attempts to discover\nnew model vulnerabilities. Areas of focus include: factuality, child safety, harmful content, cybersecu-\nrity, biorisk, representation and inclusivity. These assessments are updated in tandem with model\ndevelopment.\nImpact assessments are used to guide mitigation and product delivery efforts, and inform deploy-\nment decisions. Gemini impact assessments spanned across different capabilities of Gemini models,\nassessing the potential consequences of these capabilities with Google\u2019s AI Principles (Google, 2023).\n6.2. Model Policy\nBuilding upon this understanding of known and anticipated effects, we developed a set of \u201cmodel\npolicies\u201d to steer model development and evaluations. Model policy definitions act as a standardized\ncriteria and prioritization schema for responsible development and as an indication of launch-readiness.\nGemini model policies cover a number of domains including: child safety, hate speech, factual accuracy,\nfairness and inclusion, and harassment.\n19\nGemini: A Family of Highly Capable Multimodal Models\n6.3. Evaluations\nTo assess the Gemini models against policy areas and other key risk areas identified within impact\nassessments, we developed a suite of evaluations across the lifecycle of model development.\nDevelopment evaluations are conducted for the purpose of \u2018hill-climbing\u2019 throughout training and\nfine-tuning Gemini models. These evaluations are designed by the Gemini team, or are assessments\nagainst external academic benchmarks. Evaluations consider issues such as helpfulness (instruction\nfollowing and creativity), safety and factuality. See section 5.1.6 and the next section on mitigations\nfor a sample of results.\nAssurance evaluations are conducted for the purpose of governance and review, usually at the end\nof key milestones or training runs by a group outside of the model development team. Assurance\nevaluations are standardized by modality and datasets are strictly held-out. Only high-level insights\nare fed back into the training process to assist with mitigation efforts. Assurance evaluations include\ntesting across Gemini policies, and include ongoing testing for dangerous capabilities such as potential\nbiohazards, persuasion, and cybersecurity (Shevlane et al., 2023).\nExternal evaluations are conducted by partners outside of Google to identify blindspots. External\ngroups stress-test our models across a range of issues, including across areas listed in the White House\nCommitments,7 and tests are conducted through a mixture of structured evaluations and unstructured\nred teaming. The design of these evaluations are independent and results are reported periodically to\nthe Google DeepMind team.\nIn addition to this suite of external evaluations, specialist internal teams conduct ongoing red\nteaming of our models across areas such as the Gemini policies and security. These activities include\nless structured processes involving sophisticated adversarial attacks to identify new vulnerabilities.\nDiscovery of potential weaknesses can then be used to mitigate risks and improve evaluation ap-\nproaches internally. We are committed to ongoing model transparency and plan to share additional\nresults from across our evaluation suite over time.\n6.4. Mitigations\nMitigations are developed in response to the outcomes of the assessment, policy, and evaluation\napproaches described above. Evaluations and mitigations are used in an iterative way, with evaluations\nbeing re-run following mitigation efforts. We discuss our efforts on mitigating model harms across\ndata, instruction-tuning, and factuality below.\n6.4.1. Data\nPrior to training, we take various steps to mitigate potential downstream harms at the data curation\nand data collection stage. As discussed in the section on \u201cTraining Data\u201d, we filter training data for\nhigh-risk content and to ensure all training data is sufficiently high quality. Beyond filtering, we also\ntake steps to ensure all data collected meets Google DeepMind\u2019s best practices on data enrichment,8\ndeveloped based on the Partnership on AI\u2019s \u201cResponsible Sourcing of Data Enrichment Services\u201d9.\nThis includes ensuring all data enrichment workers are paid at least a local living wage.\n7https://whitehouse.gov/wp-content/uploads/2023/07/Ensuring-Safe-Secure-and-Trustworthy-AI.pdf\n8https://deepmind.google/discover/blog/best-practices-for-data-enrichment/\n9https://partnershiponai.org/responsible-sourcing-considerations/\n20\nGemini: A Family of Highly Capable Multimodal Models\n6.4.2. Instruction Tuning\nInstruction tuning encompasses supervised fine tuning (SFT) and reinforcement learning through\nhuman feedback (RLHF) using a reward model. We apply instruction tuning in both text and\nmultimodal settings. Instruction tuning recipes are carefully designed to balance the increase in\nhelpfulness with decrease in model harms related to safety and hallucinations (Bai et al., 2022a).\nCuration of \u201cquality\u201d data is critical for SFT, reward model training, and RLHF. The data mixture\nratios are ablated with smaller models to balance the metrics on helpfulness (such as instruction\nfollowing, creativity) and reduction of model harms, and these results generalize well to larger models.\nWe have also observed that data quality is more important than quantity (Touvron et al., 2023b; Zhou\net al., 2023), especially for larger models. Similarly, for reward model training, we find it critical\nto balance the dataset with examples where the model prefers to say, \u201cI cannot help with that,\u201d for\nsafety reasons and examples where the model outputs helpful responses. We use multi-objective\noptimization with a weighted sum of reward scores from helpfulness, factuality, and safety, to train a\nmulti-headed reward model.\nWe further elaborate our approach to mitigate risks of harmful text generation. We enumerate\napproximately 20 harm types (e.g. hate speech, providing medical advice, suggesting dangerous\nbehavior) across a wide variety of use cases. We generate a dataset of potential harm-inducing queries\nin these categories, either manually by policy experts and ML engineers, or via prompting high\ncapability language models with topical keywords as seeds.\nGiven the harm-inducing queries, we probe our Gemini models and analyze the model responses\nvia side-by-side evaluation. As discussed above, we balance the objective of model output response\nbeing harmless versus being helpful. From the detected risk areas, we create additional supervised\nfine-tuning data to demonstrate the desirable responses. To generate such responses at scale, we\nheavily rely on a custom data generation recipe loosely inspired from Constitutional AI (Bai et al.,\n2022b), where we inject variants of Google\u2019s content policy language as \u201cconstitutions\u201d, and utilize\nlanguage model\u2019s strong zero-shot reasoning abilities (Kojima et al., 2022) to revise responses and\nchoose between multiple response candidates. We have found this recipe to be effective \u2013 for example\nin Gemini Pro, this overall recipe was able to mitigate a majority of our identified text harm cases,\nwithout any perceptible decrease on response helpfulness.\n6.4.3. Factuality\nIt is important that our models generate responses that are factual in a variety of scenarios, and to\nreduce the frequency of hallucinations. We focused instruction tuning efforts on three key desired\nbehaviors, reflecting real-world scenarios:\n1. Attribution: If instructed to generate a response that should be fully attributed to a given\ncontext in the prompt, Gemini should produce a response with the highest degree of faithfulness\nto the context (Rashkin et al., 2023). This includes the summarization of a user-provided\nsource, generating fine-grained citations given a question and provided snippets akin to Menick\net al. (2022); Peng et al. (2023), answering questions from a long-form source such as a\nbook (Mihaylov et al., 2018), and transforming a given source to a desired output (e.g. an email\nfrom a portion of a meeting transcript).\n2. Closed-Book Response Generation: If provided with a fact-seeking prompt without any given\nsource, Gemini should not hallucinate incorrect information (see Section 2 of Roberts et al.\n(2020) for a definition). These prompts can range from information-seeking prompts (e.g. \u201cWho\nis the prime minister of India?\u201d) to semi-creative prompts that may request factual information\n(e.g. \u201cWrite a 500-word speech in favor of the adoption of renewable energy\u201d).\n21\nGemini: A Family of Highly Capable Multimodal Models\n3. Hedging: If prompted with an input such that it is \u201cunanswerable\u201d, Gemini should not hal-\nlucinate. Rather, it should acknowledge that it cannot provide a response by hedging. These\ninclude scenarios where the input prompt contains false-premise questions (see examples in Hu\net al. (2023)), the input prompt instructs the model to perform open-book QA, but the answer\nis not derivable from the given context, and so forth.\nWe elicited these desired behaviors from Gemini models by curating targeted supervised-fine tuning\ndatasets and performing RLHF. Note that the results produced here do not include endowing Gemini\nwith tools or retrieval that purportedly could boost factuality (Menick et al., 2022; Peng et al., 2023).\nWe provide three key results on respective challenge sets below.\n1. Factuality Set: An evaluation set containing fact-seeking prompts (primarily closed-book).\nThis is evaluated via human annotators who fact-check each response manually; we report the\npercentage of factually-inaccurate responses as judged by annotators.\n2. Attribution Set: An evaluation set containing a variety of prompts that require attribution to\nsources in the prompt. This is evaluated via human annotators who check for attribution to\nsources in the prompt for each response manually; the reported metric is AIS (Rashkin et al.,\n2023).\n3. Hedging Set: An automatic evaluation setup where we measure whether Gemini models hedge\naccurately.\nWe compare Gemini Pro with a version of instruction-tuned Gemini Pro model without any factuality-\nfocused adaptation in Table 14. We observe that the rate of inaccuracy is halved in the factuality set,\nthe accuracy of attribution is increased by 50% from the attribution set, and the model successfully\nhedges 70% (up from 0%) in the provided hedging set task.\nFactuality Set\n(Inaccurate Rate)\nAttribution Set\n(AIS)\nHedging Set\n(Accuracy)\nGemini Pro\nNo factuality-focused adaptation\n7.9%\n[7%, 9%]\n40.2%\n[37.9%, 42.4%]\n0%\nGemini Pro\nFinal stage of instruction tuning\n3.4%\n[2.8%, 4.1%]\n59.7%\n[57.2%, 61.9%]\n69.30%\nTable 14 | Factuality mitigations: Impact of instruction-tuning on the rate of inaccuracy, presence of\nattribution and the rate of accurate hedging (with corresponding 95% confidence intervals).\n6.5. Deployment\nFollowing the completion of reviews, model cards (Mitchell et al., 2019) for each approved Gemini\nmodel are created for structured and consistent internal documentation of critical performance and\nresponsibility metrics as well as to inform appropriate external communication of these metrics over\ntime.\n6.6. Responsible Governance\nAcross the responsible development process, we undertake ethics and safety reviews with the Google\nDeepMind\u2019s Responsibility and Safety Council (RSC),10 an interdisciplinary group which evaluates\nGoogle DeepMind\u2019s projects, papers and collaborations against Google\u2019s AI Principles. The RSC\n10https://deepmind.google/about/responsibility-safety/\n22\nGemini: A Family of Highly Capable Multimodal Models\nprovides input and feedback on impact assessments, policies, evaluations and mitigation efforts.\nDuring the Gemini project, the RSC set specific evaluation targets across key policy domains (e.g.\nchild safety).\n7. Discussion and Conclusion\nWe have presented Gemini, a new family of models that advance multimodal model capabilities\nin text, code, image, audio, and video. This technical report evaluates the capabilities of Gemini\non a diverse set of widely-studied benchmarks, and our most capable model Gemini Ultra makes\nsignificant advances across the board. In the natural language domain, the performance gains from\ncareful developments in data and model training at scale continue to deliver quality improvements,\nsetting new state of the art in several benchmarks. In particular, Gemini Ultra surpasses human-expert\nperformance on the exam benchmark MMLU, scoring 90.0%, which has been a defacto measure\nof progress for LLMs ever since it was first released in 2020. In the multimodal domain, Gemini\nUltra sets new state of the art on most of the image understanding, video understanding, and audio\nunderstanding benchmarks without task-specific modifications or tuning. In particular, Gemini Ultra\u2019s\nmultimodal reasoning capabilities are evident from its state-of-the-art performance on the recent\nMMMU benchmark (Yue et al., 2023), that comprises questions about images requiring college-level\nsubject knowledge and deliberate reasoning.\nBeyond the state-of-art results on benchmarks, what we are most excited about is the new use cases\nenabled by Gemini models. The new capabilities of Gemini models to parse complex images, such as\ncharts or infographics, reason over interleaved sequences of images, audio, and text, and generate\ninterleaved text and images as responses open a wide variety of new applications. As shown in figures\nthroughout the report and appendix, Gemini can enable new approaches in areas like education,\neveryday problem solving, multilingual communication, information summarization, extraction, and\ncreativity. We expect that the users of these models will find all kinds of beneficial new uses that we\nhave only scratched the surface of in our own investigations.\nDespite their impressive capabilities, we should note that there are limitations to the use of LLMs.\nThere is a continued need for ongoing research and development on \u201challucinations\u201d generated by\nLLMs to ensure that model outputs are more reliable and verifiable. LLMs also struggle with tasks\nrequiring high-level reasoning abilities like causal understanding, logical deduction, and counterfactual\nreasoning even though they achieve impressive performance on exam benchmarks. This underscores\nthe need for more challenging and robust evaluations to measure their true understanding as the\ncurrent state-of-the-art LLMs saturate many benchmarks.\nGemini is a further step towards our mission to solve intelligence, advance science and benefit\nhumanity, and we are enthusiastic to see how these models are used by our colleagues at Google and\nbeyond. We build on many innovations in machine learning, data, infrastructure, and responsible\ndevelopment \u2013 areas that we have been pursuing at Google for over a decade. The models we present\nin this report provide a strong foundation towards our broader future goal to develop a large-scale,\nmodularized system that will have broad generalization capabilities across many modalities.\n23\nGemini: A Family of Highly Capable Multimodal Models\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\nBorgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo\nBarreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736,\n2022.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick,\nKevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Her-\nnandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha\nBrahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,\nAakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin,\nMark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag,\nXavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi,\nLe Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah,\nMatthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan,\nKatherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao\nLin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant\nMisra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish,\nMarie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter,\nParker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose\nSlone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan,\nKiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu,\nKelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng,\nWeikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. PaLM 2 Technical Report, 2023.\nAnthropic. Model Card and Evaluations for Claude Models, 2023.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan\nHume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei,\nTom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a\nhelpful and harmless assistant with reinforcement learning from human feedback. April 2022a.\nURL https://arxiv.org/abs/2204.05862.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,\nChristopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,\nEthan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile\nLukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova\nDasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El\nShowk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,\nTristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,\nSam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback.\narXiv preprint arXiv:2212.08073, 2022b.\n24\nGemini: A Family of Highly Capable Multimodal Models\nPaul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Dan Hurt, Michael\nIsard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan Saeta, Parker Schuh, Ryan Sepassi,\nLaurent El Shafey, Chandramohan A. Thekkath, and Yonghui Wu.\nPathways: Asynchronous\ndistributed dataflow for ML. Proceedings of Machine Learning and Systems, 4:430\u2013449, 2022.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,\nGeorge Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX:\ncomposable transformations of Python+NumPy programs, 2018. URL http://github.com/\ngoogle/jax.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey\nWu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.\nBalcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages\n1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_\nfiles/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth\nBarnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,\nIgor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.\nXi Chen, Xiao Wang, Soravit Changpinyo, A J Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver,\nNan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James\nBradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme,\nAndreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: A jointly-\nscaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. URL https:\n//arxiv.org/abs/2209.06794.\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car-\nlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani,\nDaniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang,\nCeslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip\nPavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton\nLee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong,\nAlexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. PaLI-X: On Scaling up a Multilingual Vision and Language Model. arXiv preprint\narXiv:2305.18565, 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen\n25\nGemini: A Family of Highly Capable Multimodal Models\nShi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer,\nVinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan\nSepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele\nCatasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.\nPaLM: Scaling Language Modeling with Pathways. Journal of Machine Learning Research, 24(240):\n1\u2013113, 2023. URL http://jmlr.org/papers/v24/22-1144.html.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of\nthe 2019 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, 2019. URL\nhttps://aclanthology.org/N19-1300.\nJon Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. TydiQA: A benchmark for information-seeking question answering in typo-\nlogically diverse languages. Transactions of the Association for Computational Linguistics, 2020. URL\nhttps://storage.googleapis.com/tydiqa/tydiqa.pdf.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman.\nTraining verifiers to solve math word problems.\narXiv preprint\narXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.\nAlexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa,\nClara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representations\nof speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 798\u2013805. IEEE, 2023.\nJeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc\u2019aurelio Ranzato,\nAndrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. Advances in\nneural information processing systems, 25, 2012.\nHarish Dattatraya Dixit, Sneha Pendharkar, Matt Beadon, Chris Mason, Tejasvi Chakravarthy, Bharath\nMuthiah, and Sriram Sankar. Silent data corruptions at scale. arXiv preprint arXiv:2102.11245,\n2021.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In\nICLR, 2020.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368\u20132378,\n2019. URL https://aclanthology.org/N19-1246.\nChristian Federmann, Tom Kocmi, and Ying Xin.\nNTREX-128 \u2013 news test references for MT\nevaluation of 128 languages. In Proceedings of the First Workshop on Scaling Up Multilingual\n26\nGemini: A Family of Highly Capable Multimodal Models\nEvaluation, pages 21\u201324, Online, nov 2022. Association for Computational Linguistics.\nURL\nhttps://aclanthology.org/2022.sumeval-1.4.\nGoogle.\nGoogle\u2019s AI Principles.\n2023.\nURL https://ai.google/responsibility/\nprinciples/.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. XL-sum: Large-scale multilingual abstractive summarization\nfor 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,\npages 4693\u20134703, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/\nv1/2021.findings-acl.413. URL https://aclanthology.org/2021.findings-acl.413.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. Proceedings of the International\nConference on Learning Representations (ICLR), 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. arXiv\npreprint arXiv:2103.03874, 2021b. URL https://arxiv.org/abs/2103.03874.\nPeter H Hochschild, Paul Turner, Jeffrey C Mogul, Rama Govindaraju, Parthasarathy Ranganathan,\nDavid E Culler, and Amin Vahdat. Cores that don\u2019t count. In Proceedings of the Workshop on Hot\nTopics in Operating Systems, pages 9\u201316, 2021.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric\nNoland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero,\nKaren Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-\noptimal large language models. arXiv preprint arXiv:2203.15556, 2022.\nShengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, and Maosong Sun. Won\u2019t get\nfooled again: Answering questions with false premises. arXiv preprint arXiv:2307.02394, 2023.\nEunJeong Hwang and Vered Shwartz. Memecap: A dataset for captioning and interpreting memes,\n2023.\nNorman P Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil,\nSuvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei Zhou, and\nDavid A Patterson. Tpu v4: An optically reconfigurable supercomputer for machine learning with\nhardware support for embeddings. In Proceedings of the 50th Annual International Symposium on\nComputer Architecture, pages 1\u201314, 2023.\nAshwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, and Peter Clark. How\nMuch Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge\nfor AI, 2021.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A. Smith, Yejin Choi, and Kentaro Inui. RealTime QA: What\u2019s the answer right now?,\n2022. URL https://arxiv.org/abs/2207.13332.\n27\nGemini: A Family of Highly Capable Multimodal Models\nK Kavukcuoglu, P Kohli, L Ibrahim, D Bloxwich, and S Brown. How our principles helped define\nAlphaFold\u2019s release. Google DeepMind, 2022.\nAniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.\nA diagram is worth a dozen images. In ECCV, 2016.\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis,\nand Edward Grefenstette. The NarrativeQA reading comprehension challenge. Transactions of\nthe Association for Computational Linguistics, 6:317\u2013328, 2018. doi: 10.1162/tacl_a_00023. URL\nhttps://aclanthology.org/Q18-1023.\nTom Kocmi, Rachel Bawden, Ond\u0159ej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel,\nThamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp\nKoehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Nov\u00e1k,\nMartin Popel, and Maja Popovi\u0107. Findings of the 2022 conference on machine translation (WMT22).\nIn Proceedings of the Seventh Conference on Machine Translation (WMT), December 2022. URL\nhttps://aclanthology.org/2022.wmt-1.1.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems, 35:\n22199\u201322213, 2022.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. EMNLP (System Demonstrations), 2018. doi:\n10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.\nNatural questions: A benchmark for question answering research. Transactions of the Association\nfor Computational Linguistics, 7:452\u2013466, 2019. doi: 10.1162/tacl_a_00276. URL https://\naclanthology.org/Q19-1026.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: A new benchmark\ndataset for cross-lingual abstractive summarization. In Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 4034\u20134048, Online, November 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.findings-emnlp.360. URL https://www.aclweb.org/\nanthology/2020.findings-emnlp.360.\nLeblond et al. AlphaCode 2 Technical Report. 2023. URL https://storage.googleapis.com/\ndeepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436\u2013444, 2015.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation\nwith alphacode. Science, 378(6624):1092\u20131097, 2022.\nBin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\nrepresentation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023.\nFangyu Liu, Julian Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee,\nMandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. DePlot: One-shot visual language\nreasoning by plot-to-table translation. In Findings of the Association for Computational Linguistics:\n28\nGemini: A Family of Highly Capable Multimodal Models\nACL 2023, pages 10381\u201310399, Toronto, Canada, July 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.findings-acl.660. URL https://aclanthology.org/2023.\nfindings-acl.660.\nPan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu.\nInter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning.\nIn The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021),\n2021.\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-\nWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of\nfoundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.\nAhmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for\nquestion answering about charts with visual and logical reasoning. In Findings of ACL, 2022.\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document\nimages. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages\n2200\u20132209, 2021.\nMinesh Mathew, Viraj Bagal, Rub\u00e8n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar.\nInfographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,\npages 1697\u20131706, 2022.\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia\nGlaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. Teaching\nlanguage models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, pages 2381\u20132391, Brussels, Belgium, October-\nNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL\nhttps://aclanthology.org/D18-1260.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In\nProceedings of the conference on fairness, accountability, and transparency, pages 220\u2013229, 2019.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807, Brussels,\nBelgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/\nD18-1206. URL https://aclanthology.org/D18-1206.\nOktat\u00e1si Hivatal.\nMatematika \u00edr\u00e1sb\u00e9li vizsga.\nK\u00f6z\u00e9pszint\u0171 \u00cdr\u00e1sb\u00e9li Vizsga, May 2023.\nURL\nhttps://dload-oktatas.educatio.hu/erettsegi/feladatok_2023tavasz_kozep/\nk_matang_23maj_fl.pdf. Angol Nyelven.\nOpenAI. GPT-4 Technical Report. 2023a.\nOpenAI. GPT-4V(ision) System Card, 2023b.\nOpenAI. Whisper, 2023. URL https://github.com/openai/whisper.\n29\nGemini: A Family of Highly Capable Multimodal Models\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. Preprint,\n2022. URL https://cdn.openai.com/papers/Training_language_models_to_follow_\ninstructions_with_human_feedback.pdf.\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The LAMBADA dataset: Word\nprediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\nViorica P\u0103tr\u0103ucean, Lucas Smaira, Ankush Gupta, Adri\u00e0 Recasens Continente, Larisa Markeeva, Dylan\nBanarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana\nMatejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin\nZhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and\nJo\u0103o Carreira. Perception test: A diagnostic benchmark for multimodal video models. arXiv preprint\narXiv:2305.13786, 2023.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden,\nZhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models\nwith external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.\nLeon Poutievski, Omid Mashayekhi, Joon Ong, Arjun Singh, Mukarram Tariq, Rui Wang, Jianan Zhang,\nVirginia Beauregard, Patrick Conner, Steve Gribble, et al. Jupiter evolving: transforming google\u2019s\ndatacenter network via optical circuit switches and software-defined networking. In Proceedings of\nthe ACM SIGCOMM 2022 Conference, pages 66\u201385, 2022.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: A\nlarge-scale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411, 2020.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9, 2019.\nURL\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\nmodels_are_unsupervised_multitask_learners.pdf.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. In International Conference on Machine\nLearning, pages 28492\u201328518. PMLR, 2023.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song,\nJohn Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\nJacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\nMaribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\nHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\nErich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\nSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro,\nAida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-\nBaptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas\nPajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun\nTerzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones,\nJames Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S.\nIsaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\n30\nGemini: A Family of Highly Capable Multimodal Models\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling\nlanguage models: Methods, analysis & insights from training Gopher. CoRR, abs/2112.11446,\n2021.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\nLearning, pages 8821\u20138831. PMLR, 2021.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural language\ngeneration models. Computational Linguistics, pages 1\u201364, 2023.\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel\nBarth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake\nBruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar\nBordbar, and Nando de Freitas. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.\nParker Riley, Timothy Dozat, Jan A Botha, Xavier Garcia, Dan Garrette, Jason Riesa, Orhan Firat, and\nNoah Constant. Frmt: A benchmark for few-shot region-aware machine translation. Transactions of\nthe Association for Computational Linguistics, 2023.\nHannah Ritchie, Veronika Samborska, and Max Roser. Plastic pollution. Our World in Data, 2023.\nhttps://ourworldindata.org/plastic-pollution.\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages 5418\u20135426, Online, November 2020. Associ-\nation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL https:\n//aclanthology.org/2020.emnlp-main.437.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages\n7881\u20137892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nacl-main.704. URL https://aclanthology.org/2020.acl-main.704.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor\nGeva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language\nsequences.\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, pages 12007\u201312021, Abu Dhabi, United Arab Emirates, December 2022. Association for\nComputational Linguistics. URL https://aclanthology.org/2022.emnlp-main.823.\nNoam Shazeer.\nFast transformer decoding: One write-head is all you need.\narXiv preprint\narXiv:1911.02150, 2019.\nToby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung,\nDaniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth,\nShahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul\nChristiano, and Allan Dafoe. Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324,\n2023.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-\nthought reasoners. ICLR, 2023.\n31\nGemini: A Family of Highly Capable Multimodal Models\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\nMarcus Rohrbach. Towards VQA models that can read. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 8317\u20138326, 2019.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R. Brown, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities\nof language models. arXiv preprint arXiv:2206.04615, 2022. URL https://arxiv.org/abs/\n2206.04615.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\nAdvances in neural information processing systems, 27, 2014.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging BIG-Bench tasks\nand whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. URL\nhttps://arxiv.org/abs/2210.09261.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proof Writer: Generating implications, proofs,\nand abductive statements over natural language.\nIn Findings, 2020.\nURL https://api.\nsemanticscholar.org/CorpusID:229371222.\nNLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield,\nKevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang,\nGuillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip\nHansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit,\nChau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe\nRopers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling\nhuman-centered machine translation. 2022.\nAshish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: A massively\nmultilingual multimodal evaluation dataset. In EMNLP, 2022.\nKocmi Tom, Eleftherios Avramidis, Rachel Bawden, Ond\u0159ej Bojar, Anton Dvorkovich, Christian\nFedermann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, et al. Findings\nof the 2023 conference on machine translation (wmt23): Llms are here but not quite there yet. In\nWMT23-Eighth Conference on Machine Translation, pages 198\u2013216, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Can-\nton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu,\nBrian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,\nRui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-\nrenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang\nKuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\n32\nGemini: A Family of Highly Capable Multimodal Models\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL\nhttp://arxiv.org/abs/1706.03762.\nPetar Veli\u010dkovi\u0107, Adri\u00e0 Puigdom\u00e8nech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha\nDashevskiy, Raia Hadsell, and Charles Blundell. The clrs algorithmic reasoning benchmark. arXiv\npreprint arXiv:2205.15659, 2022.\nManoj Vishwanathan, Ronak Shah, Kyung Ki Kim, and Minsu Choi. Silent data corruption (sdc)\nvulnerability of gpu on various gpgpu workloads. In 2015 International SoC Design Conference\n(ISOCC), pages 11\u201312, 2015. doi: 10.1109/ISOCC.2015.7401681.\nChanghan Wang, Anne Wu, and Juan Pino. Covost 2 and massively multilingual speech-to-text\ntranslation. arXiv preprint arXiv:2007.10310, 2020.\nChanghan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary\nWilliamson, Juan Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speech\ncorpus for representation learning, semi-supervised learning and interpretation. arXiv preprint\narXiv:2101.00390, 2021.\nXin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. VATEX: A\nlarge-scale, high-quality multilingual dataset for video-and-language research. In ICCV, 2019.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency\nimproves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,\nand Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS,\n2022. URL https://arxiv.org/abs/2201.11903.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom\nStepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S.\nIsaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from\nlanguage models. CoRR, abs/2112.04359, 2021. URL https://arxiv.org/abs/2112.04359.\nDavid Wetherall, Abdul Kabbani, Van Jacobson, Jim Winget, Yuchung Cheng, Brad Morrey,\nUma Parthavi Moravapalle, Phillipa Gill, Steven Knight, and Amin Vahdat. Improving network\navailability with protective reroute. In SIGCOMM 2023, 2023. URL https://dl.acm.org/doi/\n10.1145/3603269.3604867.\nJunbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. NExT-QA: Next phase of question-answering\nto explaining temporal actions. In CVPR, 2021.\nXLA. XLA: Optimizing compiler for TensorFlow. https://www.tensorflow.org/xla, 2019.\n[Online; accessed December-2023].\nYuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim\nKrikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: general and scalable paral-\nlelization for ml computation graphs. arXiv preprint arXiv:2105.04663, 2021.\n33\nGemini: A Family of Highly Capable Multimodal Models\nChi yao Hong, Subhasree Mandal, Mohammad A. Alfares, Min Zhu, Rich Alimi, Kondapa Naidu\nBollineni, Chandan Bhagat, Sourabh Jain, Jay Kaimal, Jeffrey Liang, Kirill Mendelev, Steve Padgett,\nFaro Thomas Rabe, Saikat Ray, Malveeka Tewari, Matt Tierney, Monika Zahn, Jon Zolla, Joon\nOng, and Amin Vahdat. B4 and after: Managing hierarchy, partitioning, and asymmetry for\navailability and scale in google\u2019s software-defined wan. In SIGCOMM\u201918, 2018. URL https:\n//conferences.sigcomm.org/sigcomm/2018/program_tuesday.html.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models, 2022a.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin\nLi, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022b.\nShoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for\nvideo localization and question answering. arXiv preprint arXiv:2305.06988, 2023.\nZhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. ActivityNet-QA:\nA dataset for understanding complex web videos via question answering. In AAAI, 2019.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin,\nBoyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert\nagi, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\nYu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen,\nBo Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar,\nDaniel S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana\nRamabhadran, Tara Sainath, Pedro Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Fran\u00e7oise\nBeaufays, and Yonghui Wu. Google usm: Scaling automatic speech recognition beyond 100\nlanguages. arXiv preprint arXiv:2303.01037, 2023.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting\nimproves reasoning in large language models, 2023.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping\nYu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less\nis more for alignment, 2023.\nLuowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web\ninstructional videos. In AAAI Conference on Artificial Intelligence, pages 7590\u20137598, 2018.\n34\nGemini: A Family of Highly Capable Multimodal Models\n8. Contributions and Acknowledgments\nLeads\nRohan Anil, Co-Lead, Text\nSebastian Borgeaud, Co-Lead, Text\nYonghui Wu, Co-Lead, Text\nJean-Baptiste Alayrac, Co-Lead, MM Vision\nJiahui Yu, Co-Lead, MM Vision\nRadu Soricut, Co-Lead, MM Vision\nJohan Schalkwyk, Lead, MM Audio\nAndrew M. Dai, Co-Lead, Data\nAnja Hauth, Co-Lead, Data\nKatie Millican, Co-Lead, Data\nDavid Silver, Co-Lead, Fine-Tuning\nSlav Petrov, Co-Lead, Fine-Tuning\nMelvin Johnson, Lead, Instruction Tuning\nIoannis Antonoglou, Co-Lead, RL Techniques\nJulian Schrittwieser, Co-Lead, RL Techniques\nAmelia Glaese, Lead, Human Data\nJilin Chen, Lead, Safety\nEmily Pitler, Co-Lead, Tool Use\nTimothy Lillicrap, Co-Lead, Tool Use\nAngeliki Lazaridou, Co-Lead, Eval\nOrhan Firat, Co-Lead, Eval\nJames Molloy, Co-Lead, Infra\nMichael Isard, Co-Lead, Infra\nPaul R. Barham, Co-Lead, Infra\nTom Hennigan, Co-Lead, Infra\nBenjamin Lee, Co-Lead, Codebase & Parallelism\nFabio Viola, Co-Lead, Codebase & Parallelism\nMalcolm Reynolds, Co-Lead, Codebase & Parallelism\nYuanzhong Xu, Co-Lead, Codebase & Parallelism\nRyan Doherty, Lead, Ecosystem\nEli Collins, Lead, Product\nClemens Meyer, Co-Lead, Operations\nEliza Rutherford, Co-Lead, Operations\nErica Moreira, Co-Lead, Operations\nKareem Ayoub, Co-Lead, Operations\nMegha Goel, Co-Lead, Operations\nCore Contributors\nGeorge Tucker\nEnrique Piqueras\nMaxim Krikun\nIain Barr\nNikolay Savinov\nIvo Danihelka\nBecca Roelofs\nCore Contributors\nAna\u00efs White\nAnders Andreassen\nTamara von Glehn\nLakshman Yagati\nMehran Kazemi\nLucas Gonzalez\nMisha Khalman\nJakub Sygnowski\nAlexandre Frechette\nCharlotte Smith\nLaura Culp\nLev Proleev\nYi Luan\nXi Chen\nJames Lottes\nNathan Schucher\nFederico Lebron\nAlban Rrustemi\nNatalie Clay\nPhil Crone\nTomas Kocisky\nJeffrey Zhao\nBartek Perz\nDian Yu\nHeidi Howard\nAdam Bloniarz\nJack W. Rae\nHan Lu\nLaurent Sifre\nMarcello Maggioni\nFred Alcober\nDan Garrette\nMegan Barnes\nShantanu Thakoor\nJacob Austin\nGabriel Barth-Maron\nWilliam Wong\nRishabh Joshi\nRahma Chaabouni\nDeeni Fatiha\nArun Ahuja\nRuibo Liu\nYunxuan Li\nSarah Cogan\nJeremy Chen\n35\nGemini: A Family of Highly Capable Multimodal Models\nCore Contributors\nChao Jia\nChenjie Gu\nQiao Zhang\nJordan Grimstad\nAle Jakse Hartman\nMartin Chadwick\nGaurav Singh Tomar\nXavier Garcia\nEvan Senter\nEmanuel Taropa\nThanumalayan Sankaranarayana Pillai\nJacob Devlin\nMichael Laskin\nDiego de Las Casas\nDasha Valter\nConnie Tao\nLorenzo Blanco\nAdri\u00e0 Puigdom\u00e8nech Badia\nDavid Reitter\nMianna Chen\nJenny Brennan\nClara Rivera\nSergey Brin\nShariq Iqbal\nGabriela Surita\nJane Labanowski\nAbhi Rao\nStephanie Winkler\nEmilio Parisotto\nYiming Gu\nKate Olszewska\nYujing Zhang\nRavi Addanki\nAntoine Miech\nAnnie Louis\nLaurent El Shafey\nDenis Teplyashin\nGeoff Brown\nElliot Catt\nNithya Attaluri\nJan Balaguer\nJackie Xiang\nPidong Wang\nZoe Ashwood\nAnton Briukhov\nAlbert Webson\nSanjay Ganapathy\nSmit Sanghavi\nCore Contributors\nAjay Kannan\nMing-Wei Chang\nAxel Stjerngren\nJosip Djolonga\nYuting Sun\nAnkur Bapna\nMatthew Aitchison\nPedram Pejman\nHenryk Michalewski\nTianhe Yu\nCindy Wang\nJuliette Love\nJunwhan Ahn\nDawn Bloxwich\nKehang Han\nPeter Humphreys\nThibault Sellam\nJames Bradbury\nVarun Godbole\nSina Samangooei\nBogdan Damoc\nAlex Kaskasoli\nS\u00e9bastien M. R. Arnold\nVijay Vasudevan\nShubham Agrawal\nJason Riesa\nDmitry Lepikhin\nRichard Tanburn\nSrivatsan Srinivasan\nHyeontaek Lim\nSarah Hodkinson\nPranav Shyam\nJohan Ferret\nSteven Hand\nAnkush Garg\nTom Le Paine\nJian Li\nYujia Li\nMinh Giang\nAlexander Neitz\nZaheer Abbas\nSarah York\nMachel Reid\nElizabeth Cole\nAakanksha Chowdhery\nDipanjan Das\nDominika Rogozi\u0144ska\nVitaly Nikolaev\n36\nGemini: A Family of Highly Capable Multimodal Models\nCore Contributors\nPablo Sprechmann\nZachary Nado\nLukas Zilka\nFlavien Prost\nLuheng He\nMarianne Monteiro\nGaurav Mishra\nChris Welty\nJosh Newlan\nDawei Jia\nMiltiadis Allamanis\nClara Huiyi Hu\nRaoul de Liedekerke\nJustin Gilmer\nCarl Saroufim\nShruti Rijhwani\nShaobo Hou\nDisha Shrivastava\nAnirudh Baddepudi\nAlex Goldin\nAdnan Ozturel\nAlbin Cassirer\nYunhan Xu\nDaniel Sohn\nDevendra Sachan\nReinald Kim Amplayo\nCraig Swanson\nDessie Petrova\nShashi Narayan\nArthur Guez\nSiddhartha Brahma\nJessica Landon\nMiteyan Patel\nRuizhe Zhao\nKevin Villela\nLuyu Wang\nWenhao Jia\nMatthew Rahtz\nMai Gim\u00e9nez\nLegg Yeung\nHanzhao Lin\nJames Keeling\nPetko Georgiev\nDiana Mincu\nBoxi Wu\nSalem Haykal\nRachel Saputro\nKiran Vodrahalli\nCore Contributors\nJames Qin\nZeynep Cankara\nAbhanshu Sharma\nNick Fernando\nWill Hawkins\nBehnam Neyshabur\nSolomon Kim\nAdrian Hutter\nPriyanka Agrawal\nAlex Castro-Ros\nGeorge van den Driessche\nTao Wang\nFan Yang\nShuo-yiin Chang\nPaul Komarek\nRoss McIlroy\nMario Lu\u010di\u0107\nGuodong Zhang\nWael Farhan\nMichael Sharman\nPaul Natsev\nPaul Michel\nYong Cheng\nYamini Bansal\nSiyuan Qiao\nKris Cao\nSiamak Shakeri\nChristina Butterfield\nJustin Chung\nPaul Kishan Rubenstein\nShivani Agrawal\nArthur Mensch\nKedar Soparkar\nKarel Lenc\nTimothy Chung\nAedan Pope\nLoren Maggiore\nJackie Kay\nPriya Jhakra\nShibo Wang\nJoshua Maynez\nMary Phuong\nTaylor Tobin\nAndrea Tacchetti\nMaja Trebacz\nKevin Robinson\nYash Katariya\nSebastian Riedel\n37\nGemini: A Family of Highly Capable Multimodal Models\nCore Contributors\nPaige Bailey\nKefan Xiao\nNimesh Ghelani\nLora Aroyo\nAmbrose Slone\nNeil Houlsby\nXuehan Xiong\nZhen Yang\nElena Gribovskaya\nJonas Adler\nMateo Wirth\nLisa Lee\nMusic Li\nThais Kagohara\nJay Pavagadhi\nSophie Bridgers\nAnna Bortsova\nSanjay Ghemawat\nZafarali Ahmed\nTianqi Liu\nRichard Powell\nVijay Bolina\nMariko Iinuma\nPolina Zablotskaia\nJames Besley\nDa-Woon Chung\nTimothy Dozat\nRamona Comanescu\nXiance Si\nJeremy Greer\nGuolong Su\nMartin Polacek\nRapha\u00ebl Lopez Kaufman\nSimon Tokumine\nHexiang Hu\nElena Buchatskaya\nYingjie Miao\nMohamed Elhawaty\nAditya Siddhant\nNenad Tomasev\nJinwei Xing\nChristina Greer\nHelen Miller\nShereen Ashraf\nAurko Roy\nZizhao Zhang\nAda Ma\nAngelos Filos\nCore Contributors\nMilos Besta\nRory Blevins\nTed Klimenko\nChih-Kuan Yeh\nSoravit Changpinyo\nJiaqi Mu\nOscar Chang\nMantas Pajarskas\nCarrie Muir\nVered Cohen\nCharline Le Lan\nKrishna Haridasan\nAmit Marathe\nSteven Hansen\nSholto Douglas\nRajkumar Samuel\nMingqiu Wang\nSophia Austin\nChang Lan\nJiepu Jiang\nJustin Chiu\nJaime Alonso Lorenzo\nLars Lowe Sj\u00f6sund\nS\u00e9bastien Cevey\nZach Gleicher\nThi Avrahami\nAnudhyan Boral\nHansa Srinivasan\nVittorio Selo\nRhys May\nKonstantinos Aisopos\nL\u00e9onard Hussenot\nLivio Baldini Soares\nKate Baumli\nMichael B. Chang\nAdri\u00e0 Recasens\nBen Caine\nAlexander Pritzel\nFilip Pavetic\nFabio Pardo\nAnita Gergely\nJustin Frye\nVinay Ramasesh\nDan Horgan\nKartikeya Badola\nNora Kassner\nSubhrajit Roy\nEthan Dyer\n38\nGemini: A Family of Highly Capable Multimodal Models\nCore Contributors\nV\u00edctor Campos\nAlex Tomala\nYunhao Tang\nDalia El Badawy\nElspeth White\nBasil Mustafa\nOran Lang\nAbhishek Jindal\nSharad Vikram\nZhitao Gong\nSergi Caelles\nRoss Hemsley\nGregory Thornton\nFangxiaoyu Feng\nWojciech Stokowiec\nCe Zheng\nPhoebe Thacker\n\u00c7a\u011flar \u00dcnl\u00fc\nZhishuai Zhang\nMohammad Saleh\nJames Svensson\nMax Bileschi\nPiyush Patil\nAnkesh Anand\nRoman Ring\nKaterina Tsihlas\nArpi Vezer\nMarco Selvi\nToby Shevlane\nMikel Rodriguez\nTom Kwiatkowski\nSamira Daruki\nKeran Rong\nAllan Dafoe\nNicholas FitzGerald\nKeren Gu-Lemberg\nMina Khan\nLisa Anne Hendricks\nMarie Pellat\nVladimir Feinberg\nJames Cobon-Kerr\nTara Sainath\nMaribeth Rauh\nSayed Hadi Hashemi\nRichard Ives\nYana Hasson\nYaGuang Li\nEric Noland\nCore Contributors\nYuan Cao\nNathan Byrd\nLe Hou\nQingze Wang\nThibault Sottiaux\nMichela Paganini\nJean-Baptiste Lespiau\nAlexandre Moufarek\nSamer Hassan\nKaushik Shivakumar\nJoost van Amersfoort\nAmol Mandhane\nPratik Joshi\nAnirudh Goyal\nMatthew Tung\nAndrew Brock\nHannah Sheahan\nVedant Misra\nCheng Li\nNemanja Raki\u0107evi\u0107\nMostafa Dehghani\nFangyu Liu\nSid Mittal\nJunhyuk Oh\nSeb Noury\nEren Sezener\nFantine Huot\nMatthew Lamm\nNicola De Cao\nCharlie Chen\nContributors\nGamaleldin Elsayed\nEd Chi\nMahdis Mahdieh\nIan Tenney\nNan Hua\nIvan Petrychenko\nPatrick Kane\nDylan Scandinaro\nRishub Jain\nJonathan Uesato\nRomina Datta\nAdam Sadovsky\nOskar Bunyan\nDominik Rabiej\nShimu Wu\nJohn Zhang\n39\nGemini: A Family of Highly Capable Multimodal Models\nContributors\nGautam Vasudevan\nEdouard Leurent\nMahmoud Alnahlawi\nIonut Georgescu\nNan Wei\nIvy Zheng\nBetty Chan\nPam G Rabinovitch\nPiotr Stanczyk\nYe Zhang\nDavid Steiner\nSubhajit Naskar\nMichael Azzam\nMatthew Johnson\nAdam Paszke\nChung-Cheng Chiu\nJaume Sanchez Elias\nAfroz Mohiuddin\nFaizan Muhammad\nJin Miao\nAndrew Lee\nNino Vieillard\nSahitya Potluri\nJane Park\nElnaz Davoodi\nJiageng Zhang\nJeff Stanway\nDrew Garmon\nAbhijit Karmarkar\nZhe Dong\nJong Lee\nAviral Kumar\nLuowei Zhou\nJonathan Evens\nWilliam Isaac\nZhe Chen\nJohnson Jia\nAnselm Levskaya\nZhenkai Zhu\nChris Gorgolewski\nPeter Grabowski\nYu Mao\nAlberto Magni\nKaisheng Yao\nJavier Snaider\nNorman Casagrande\nPaul Suganthan\nEvan Palmer\nContributors\nGeoffrey Irving\nEdward Loper\nManaal Faruqui\nIsha Arkatkar\nNanxin Chen\nIzhak Shafran\nMichael Fink\nAlfonso Casta\u00f1o\nIrene Giannoumis\nWooyeol Kim\nMiko\u0142aj Rybi\u0144ski\nAshwin Sreevatsa\nJennifer Prendki\nDavid Soergel\nAdrian Goedeckemeyer\nWilli Gierke\nMohsen Jafari\nMeenu Gaba\nJeremy Wiesner\nDiana Gage Wright\nYawen Wei\nHarsha Vashisht\nYana Kulizhskaya\nJay Hoover\nMaigo Le\nLu Li\nChimezie Iwuanyanwu\nLu Liu\nKevin Ramirez\nAndrey Khorlin\nAlbert Cui\nTian LIN\nMarin Georgiev\nMarcus Wu\nRicardo Aguilar\nKeith Pallo\nAbhishek Chakladar\nAlena Repina\nXihui Wu\nTom van der Weide\nPriya Ponnapalli\nCaroline Kaplan\nJiri Simsa\nShuangfeng Li\nOlivier Dousse\nFan Yang\nJeff Piper\nNathan Ie\n40\nGemini: A Family of Highly Capable Multimodal Models\nContributors\nMinnie Lui\nRama Pasumarthi\nNathan Lintz\nAnitha Vijayakumar\nLam Nguyen Thiet\nDaniel Andor\nPedro Valenzuela\nCosmin Paduraru\nDaiyi Peng\nKatherine Lee\nShuyuan Zhang\nSomer Greene\nDuc Dung Nguyen\nPaula Kurylowicz\nSarmishta Velury\nSebastian Krause\nCassidy Hardin\nLucas Dixon\nLili Janzer\nKiam Choo\nZiqiang Feng\nBiao Zhang\nAchintya Singhal\nTejasi Latkar\nMingyang Zhang\nQuoc Le\nElena Allica Abellan\nDayou Du\nDan McKinnon\nNatasha Antropova\nTolga Bolukbasi\nOrgad Keller\nDavid Reid\nDaniel Finchelstein\nMaria Abi Raad\nRemi Crocker\nPeter Hawkins\nRobert Dadashi\nColin Gaffney\nSid Lall\nKen Franko\nEgor Filonov\nAnna Bulanova\nR\u00e9mi Leblond\nVikas Yadav\nShirley Chung\nHarry Askham\nLuis C. Cobo\nContributors\nKelvin Xu\nFelix Fischer\nJun Xu\nChristina Sorokin\nChris Alberti\nChu-Cheng Lin\nColin Evans\nHao Zhou\nAlek Dimitriev\nHannah Forbes\nDylan Banarse\nZora Tung\nJeremiah Liu\nMark Omernick\nColton Bishop\nChintu Kumar\nRachel Sterneck\nRyan Foley\nRohan Jain\nSwaroop Mishra\nJiawei Xia\nTaylor Bos\nGeoffrey Cideron\nEhsan Amid\nFrancesco Piccinno\nXingyu Wang\nPraseem Banzal\nPetru Gurita\nHila Noga\nPremal Shah\nDaniel J. Mankowitz\nAlex Polozov\nNate Kushman\nVictoria Krakovna\nSasha Brown\nMohammadHossein Bateni\nDennis Duan\nVlad Firoiu\nMeghana Thotakuri\nTom Natan\nAnhad Mohananey\nMatthieu Geist\nSidharth Mudgal\nSertan Girgin\nHui Li\nJiayu Ye\nOfir Roval\nReiko Tojo\n41\nGemini: A Family of Highly Capable Multimodal Models\nContributors\nMichael Kwong\nJames Lee-Thorp\nChristopher Yew\nQuan Yuan\nSumit Bagri\nDanila Sinopalnikov\nSabela Ramos\nJohn Mellor\nAbhishek Sharma\nAliaksei Severyn\nJonathan Lai\nKathy Wu\nHeng-Tze Cheng\nDavid Miller\nNicolas Sonnerat\nDenis Vnukov\nRory Greig\nJennifer Beattie\nEmily Caveness\nLibin Bai\nJulian Eisenschlos\nAlex Korchemniy\nTomy Tsai\nMimi Jasarevic\nWeize Kong\nPhuong Dao\nZeyu Zheng\nFrederick Liu\nFan Yang\nRui Zhu\nMark Geller\nTian Huey Teh\nJason Sanmiya\nEvgeny Gladchenko\nNejc Trdin\nAndrei Sozanschi\nDaniel Toyama\nEvan Rosen\nSasan Tavakkol\nLinting Xue\nChen Elkind\nOliver Woodman\nJohn Carpenter\nGeorge Papamakarios\nRupert Kemp\nSushant Kafle\nTanya Grunina\nRishika Sinha\nContributors\nAlice Talbert\nAbhimanyu Goyal\nDiane Wu\nDenese Owusu-Afriyie\nCosmo Du\nChloe Thornton\nJordi Pont-Tuset\nPradyumna Narayana\nJing Li\nSaaber Fatehi\nJohn Wieting\nOmar Ajmeri\nBenigno Uria\nTao Zhu\nYeongil Ko\nLaura Knight\nAm\u00e9lie H\u00e9liou\nNing Niu\nShane Gu\nChenxi Pang\nDustin Tran\nYeqing Li\nNir Levine\nAriel Stolovich\nNorbert Kalb\nRebeca Santamaria-Fernandez\nSonam Goenka\nWenny Yustalim\nRobin Strudel\nAli Elqursh\nBalaji Lakshminarayanan\nCharlie Deck\nShyam Upadhyay\nHyo Lee\nMike Dusenberry\nZonglin Li\nXuezhi Wang\nKyle Levin\nRaphael Hoffmann\nDan Holtmann-Rice\nOlivier Bachem\nSummer Yue\nSho Arora\nEric Malmi\nDaniil Mirylenka\nQijun Tan\nChristy Koh\nSoheil Hassas Yeganeh\n42\nGemini: A Family of Highly Capable Multimodal Models\nContributors\nSiim P\u00f5der\nSteven Zheng\nFrancesco Pongetti\nMukarram Tariq\nYanhua Sun\nLucian Ionita\nMojtaba Seyedhosseini\nPouya Tafti\nRagha Kotikalapudi\nZhiyu Liu\nAnmol Gulati\nJasmine Liu\nXinyu Ye\nBart Chrzaszcz\nLily Wang\nNikhil Sethi\nTianrun Li\nBen Brown\nShreya Singh\nWei Fan\nAaron Parisi\nJoe Stanton\nChenkai Kuang\nVinod Koverkathu\nChristopher A. Choquette-Choo\nYunjie Li\nTJ Lu\nAbe Ittycheriah\nPrakash Shroff\nPei Sun\nMani Varadarajan\nSanaz Bahargam\nRob Willoughby\nDavid Gaddy\nIshita Dasgupta\nGuillaume Desjardins\nMarco Cornero\nBrona Robenek\nBhavishya Mittal\nBen Albrecht\nAshish Shenoy\nFedor Moiseev\nHenrik Jacobsson\nAlireza Ghaffarkhah\nMorgane Rivi\u00e8re\nAlanna Walton\nCl\u00e9ment Crepy\nAlicia Parrish\nContributors\nYuan Liu\nZongwei Zhou\nClement Farabet\nCarey Radebaugh\nPraveen Srinivasan\nClaudia van der Salm\nAndreas Fidjeland\nSalvatore Scellato\nEri Latorre-Chimoto\nHanna Klimczak-Pluci\u0144ska\nDavid Bridson\nDario de Cesare\nTom Hudson\nPiermaria Mendolicchio\nLexi Walker\nAlex Morris\nIvo Penchev\nMatthew Mauger\nAlexey Guseynov\nAlison Reid\nSeth Odoom\nLucia Loher\nVictor Cotruta\nMadhavi Yenugula\nDominik Grewe\nAnastasia Petrushkina\nTom Duerig\nAntonio Sanchez\nSteve Yadlowsky\nAmy Shen\nAmir Globerson\nAdam Kurzrok\nLynette Webb\nSahil Dua\nDong Li\nPreethi Lahoti\nSurya Bhupatiraju\nDan Hurt\nHaroon Qureshi\nAnanth Agarwal\nTomer Shani\nMatan Eyal\nAnuj Khare\nShreyas Rammohan Belle\nLei Wang\nChetan Tekur\nMihir Sanjay Kale\nJinliang Wei\n43\nGemini: A Family of Highly Capable Multimodal Models\nContributors\nRuoxin Sang\nBrennan Saeta\nTyler Liechty\nYi Sun\nYao Zhao\nStephan Lee\nPandu Nayak\nDoug Fritz\nManish Reddy Vuyyuru\nJohn Aslanides\nNidhi Vyas\nMartin Wicke\nXiao Ma\nTaylan Bilal\nEvgenii Eltyshev\nDaniel Balle\nNina Martin\nHardie Cate\nJames Manyika\nKeyvan Amiri\nYelin Kim\nXi Xiong\nKai Kang\nFlorian Luisier\nNilesh Tripuraneni\nDavid Madras\nMandy Guo\nAustin Waters\nOliver Wang\nJoshua Ainslie\nJason Baldridge\nHan Zhang\nGarima Pruthi\nJakob Bauer\nFeng Yang\nRiham Mansour\nJason Gelman\nYang Xu\nGeorge Polovets\nJi Liu\nHonglong Cai\nWarren Chen\nXiangHai Sheng\nEmily Xue\nSherjil Ozair\nAdams Yu\nChristof Angermueller\nContributors\nXiaowei Li\nWeiren Wang\nJulia Wiesinger\nEmmanouil Koukoumidis\nYuan Tian\nAnand Iyer\nMadhu Gurumurthy\nMark Goldenson\nParashar Shah\nMK Blake\nHongkun Yu\nAnthony Urbanowicz\nJennimaria Palomaki\nChrisantha Fernando\nKevin Brooks\nKen Durden\nHarsh Mehta\nNikola Momchev\nElahe Rahimtoroghi\nMaria Georgaki\nAmit Raul\nSebastian Ruder\nMorgan Redshaw\nJinhyuk Lee\nKomal Jalan\nDinghua Li\nGinger Perng\nBlake Hechtman\nParker Schuh\nMilad Nasr\nMia Chen\nKieran Milan\nVladimir Mikulik\nTrevor Strohman\nJuliana Franco\nTim Green\nProgram Leads\nDemis Hassabis\nKoray Kavukcuoglu\nOverall Technical Leads (equal contribution)\nJeffrey Dean\nOriol Vinyals\n44\nGemini: A Family of Highly Capable Multimodal Models\nThe roles are defined as below:\n\u2022 Lead: Individual(s) responsible for the sub-team throughout the project.\n\u2022 Core Contributor: Individual that had significant impact throughout the project.\n\u2022 Contributor: Individual that had contributions to the project and was partially involved with the\neffort.\n\u2022 Program Lead: Responsible for the organizational aspects of the Gemini effort\n\u2022 Overall Technical Lead: Responsible for the technical direction of the overall Gemini effort\nWithin each role, contributions are equal, and are listed in a randomized order. Ordering within\neach role does not indicate ordering of the contributions.\nGemini is a cross-Google effort, with members from Google DeepMind (GDM), Google Research\n(GR), Knowledge and Information (K&I), Core ML, Cloud, Labs, and more.\nWe thank our reviewers and colleagues for their valuable discussions and feedback on the report \u2014\nAlexandra Belias, Arielle Bier, Eleanor Tomlinson, Emily Hossellman, Gaby Pearl, Helen King, Hollie\nDobson, Jaclyn Konzelmann, Jennifer Beroshi, Joel Moss, Jon Small, Jonathan Fildes, Oli Gaymond,\nRebecca Bland, Reena Jana, and Tom Lue.\nOur work is made possible by the dedication and efforts of numerous teams at Google. We\nwould like to acknowledge the support from Abhi Mohan, Adekunle Bello, Aishwarya Nagarajan,\nAlaa Saade, Alejandro Lince, Alexander Chen, Alexander Kolbasov, Alexander Schiffhauer, Amar\nSubramanya, Ameya Shringi, Amin Vahdat, Anda Rabati\u0107, Anthonie Gross, Antoine Yang, Anthony\nGreen, Anton Ruddock, Art Khurshudov, Artemis Chen, Arthur Argenson, Avinatan Hassidim, Beiye\nLiu, Benjamin Schroeder, Bin Ni, Brett Daw, Bryan Chiang, Burak Gokturk, Carl Crous, Carrie Grimes\nBostock, Charbel Kaed, Charlotte Banks, Che Diaz, Chris Larkin, Christy Lian, Claire Cui, Clare\nBycroft, Corentin Tallec, Daniel Herndon, Dave Burke, David Battle, David Engel, Dipannita Shaw,\nDonghyun Koo, Doug Ritchie, Dragos Stefanescu, Elissa Wolf, Emre Sargin, Eran Ofek, Eric Herren,\nEstella King, Fatema Alkhanaizi, Felix Gimeno, Fernando Pereira, Florent Altch\u00e9, Gabriel Carvajal,\nGaurav Gandhi, George Powell, Goran Pavi\u010di\u0107, Harry Richardson, Hassan Wassel, Hongji Li, Idan\nSzpektor, Igor Ivanisevic, Ivan Jambre\u0161i\u0107, Ivan Jurin, Jade Fowler, James Assiene, Jay Yagnik, Jean-\nbastien Grill, Jeff Seibert, Jenna LaPlante, Jessica Austin, Jianxing Lu, Jim O\u2019Keeffe, Jin Huang, Joe\nHeyward, Johannes Welbl, John Jumper, Jonathan Caton, Josh Woodward, Joshua Foster, Kathryn\nTunyasuvunakool, Katrina Wong, Kavya Kopparapu, Kelvin Nguyen, Kira Yin, Konstantin Sharlaimov,\nKun Li, Lee Hong, Lilly Taylor, Longfei Shen, Luc Mercier, Maciej Miku\u0142a, Mania Abdi, Manuel Sanchez,\nMaria Ines Aranguren, Mario Carlos Cortes III, Matthew Tait, Matthias Lochbrunner, Mehdi Ghissassi,\nMicah Mosley, Michael Bendersky, Michael Figurnov, Michael Harris, Michael Mathieu, Michael\nO\u2019Neill, Michael Vorburger, Mihir Paradkar, Nandita Dukkipati, Nathan Carter, Nathan Watson, Neil\nRabinowitz, Nikhil Dandekar, Nishant Ranka, Obaid Sarvana, Olcan Sercinoglu, Olivier Lacombe,\nOttavia Bertolli, Paul Caron, Pranesh Srinivasan, Praveen Kumar, Rahul Sukthankar, Raia Hadsell,\nRajagopal Ananthanarayanan, Roberto Lupi, Rosie Zou, Sachin Menezes, Sadegh Jazayeri, Sam\nCheung, Sameer Bidichandani, Sania Alex, Sanjiv Kumar, Sara Wiltberger, Sarah Fitzgerald, Saz\nBasu, Sebastian Nowozin, Shannon Hepburn, Shayne Cardwell, Sissie Hsiao, Srinivasan Venkatachary,\nSugato Basu, Sundar Pichai, Sundeep Tirumalareddy, Susannah Young, Swetha Vijayaraghavan, Tania\nBedrax-Weiss, Taylor Applebaum, Teiva Harsanyi, Terry Chen, Tim Blyth, Ting Liu, Tom Cobley, Tomas\nIzo, Trystan Upstill, Varun Singhai, Vedrana Klari\u0107 Trup\u010devi\u0107, Victor Cai, Vladimir Pudovkin, Vu Dang,\nWenbo Zhao, Wesley Crow, Wesley Szeng, Xiaodan Song, Yazhou Zu, Ye Tian, Yicong Wang, Yixing\nWang, Yossi Matias, Yunlong Jiao, Zachary Jessup, Zhenchuan Pang, \u017diga Avsec, Zimeng Yang, and\nZoubin Ghahramani. We\u2019d also like to recognize the AlphaCode team, the Borg Scheduling team,\nthe Facilities team, the Gemini Demo Team, the Global Server Ops (GSO) team, the JAX team, the\n45\nGemini: A Family of Highly Capable Multimodal Models\nthe Legal team, ML SRE team, the ML Supercomputer (MLSC) team, the PartIR team, the Platforms\nInfrastructure Engineering (PIE) team, and the XLA Compiler team.\nWe thank everyone at Google not explicitly mentioned above, who have shared excitement, given\nfeedback on early Gemini models or created interesting demo uses of Gemini, and worked with or\nsupported the core Gemini team on many aspects of this project.\n9. Appendix\n9.1. Chain-of-Thought Comparisons on MMLU benchmark\nWe contrast several chain-of-thought approaches on MMLU and discuss their results in this section. We\nproposed a new approach where model produces k chain-of-thought samples, selects the majority vote\nif the model is confident above a threshold, and otherwise defers to the greedy sample choice. The\nthresholds are optimized for each model based on their validation split performance. The proposed\napproach is referred to as uncertainty-routed chain-of-thought. The intuition behind this approach\nis that chain-of-thought samples might degrade performance compared to the maximum-likelihood\ndecision when the model is demonstrably inconsistent. We compare the gains from the proposed\napproach on both Gemini Ultra and GPT-4 in Figure 7. We find that Gemini Ultra benefits more from\nthis approach compared to using only chain-of-thought samples. GPT-4\u2019s performance improves from\n84.2% with greedy sampling to 87.3% with uncertainty-routed chain-of-thought approach with 32\nsamples, but it already achieves these gains from using 32 chain-of-thought samples. In contrast,\nGemini Ultra improves its performance significantly from 84.0% with greedy sampling to 90.0% with\nuncertainty-routed chain-of-thought approach with 32 samples while it marginally improves to 85.0%\nwith the use of 32 chain-of-thought samples only.\n87.29\n87.29\n84.21\n84.99\n90.04\n83.96\nScore Eval\nChain-of-Thought@32\nChain-of-Thought@32\n(Uncertainty-Routed)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nGPT-4 (gpt-4-0613)\nGemini Ultra\nMMLU accuracy (test split)\nFigure 7 | Chain-of-Thought with uncertainty routing on MMLU.\n46\nGemini: A Family of Highly Capable Multimodal Models\n9.2. Capabilities and Benchmarking Tasks\nWe use more than 50 benchmarks as a holistic harness to evaluate the Gemini models across text,\nimage, audio and video. We provide a detailed list of benchmarking tasks for six different capabilities in\ntext understanding and generation: factuality, long context, math/science, reasoning, summarization,\nand multilinguality. We also enumerate the benchmarks used for image understanding, video\nunderstanding, and audio understanding tasks.\n\u2022 Factuality: We use 5 benchmarks: BoolQ (Clark et al., 2019), NaturalQuestions-Closed\n(Kwiatkowski et al., 2019), NaturalQuestions-Retrieved (Kwiatkowski et al., 2019), RealtimeQA\n(Kasai et al., 2022), TydiQA-noContext and TydiQA-goldP (Clark et al., 2020).\n\u2022 Long Context: We use 6 benchmarks: NarrativeQA (Ko\u010disk\u00fd et al., 2018), Scrolls-Qasper,\nScrolls-Quality (Shaham et al., 2022), XLsum (En), XLSum (non-English languages) (Hasan\net al., 2021), and one other internal benchmark.\n\u2022 Math/Science: We use 8 benchmarks: GSM8k (with CoT) (Cobbe et al., 2021), Hendryck\u2019s\nMATH pass@1 (Hendrycks et al., 2021b), MMLU (Hendrycks et al., 2021a), Math-StackExchange,\nMath-AMC 2022-2023 problems, and three other internal benchmarks.\n\u2022 Reasoning: We use 7 benchmarks: BigBench Hard (with CoT) (Srivastava et al., 2022; Suzgun\net al., 2022), CLRS (Veli\u010dkovi\u0107 et al., 2022), Proof Writer (Tafjord et al., 2020), Reasoning-Fermi\nproblems (Kalyan et al., 2021), Lambada (Paperno et al., 2016), HellaSwag (Zellers et al.,\n2019), DROP (Dua et al., 2019).\n\u2022 Summarization: We use 5 benchmarks: XL Sum (English), XL Sum (non-English languages)\n(Hasan et al., 2021), WikiLingua (non-English languages), WikiLingua (English) (Ladhak et al.,\n2020), XSum (Narayan et al., 2018).\n\u2022 Multilinguality: We use 10 benchmarks: XLSum (Non-English languages) (Hasan et al., 2021),\nWMT22 (Kocmi et al., 2022), WMT23 (Tom et al., 2023), FRMT (Riley et al., 2023), WikiLingua\n(Non-English languages) (Ladhak et al., 2020), TydiQA (no context), TydiQA (GoldP) (Clark\net al., 2020), MGSM (Shi et al., 2023), translated MMLU (Hendrycks et al., 2021a), NTREX\n(Federmann et al., 2022), FLORES-200 (Team et al., 2022).\n\u2022 Image and Video: We use 9 benchmarks for image understanding: MMMU (Yue et al., 2023),\nTextVQA (Singh et al., 2019), DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022),\nInfographicVQA (Mathew et al., 2022), MathVista (Lu et al., 2023), AI2D (Kembhavi et al.,\n2016), VQAv2 (Goyal et al., 2017), XM3600 (Thapliyal et al., 2022) for multi-lingual image\nunderstanding, and 6 benchmarks for video understanding: VATEX (Wang et al., 2019) for\ncaptioning in two different languages, YouCook2 (Zhou et al., 2018), NextQA (Xiao et al.,\n2021), ActivityNet-QA (Yu et al., 2019), and Perception Test MCQA (P\u0103tr\u0103ucean et al., 2023).\n\u2022 Audio: We use 5 benchmarks including automatic speech recognition (ASR) tasks such as\nFLEURS (Conneau et al., 2023), VoxPopuli (Wang et al., 2021), Multi-lingual Librispeech (Pratap\net al., 2020), and automatic speech translation task such as CoVoST 2 (Wang et al., 2020).\n47\nGemini: A Family of Highly Capable Multimodal Models\n9.3. Qualitative Examples\nThis section shows sample qualitative examples from prompting the Gemini Ultra model. Some\nillustrative examples of multimodal reasoning for image understanding tasks over charts, natural\nimages and memes are shown in Figures 8, 9, 11, 13, 14, and 15. Figure 10 shows an example of image\ngeneration capabilities of Gemini Ultra where the user generates an interleaved sequence of image\nand text to design a blog post. Beyond English, Figure 16 shows model\u2019s capability to understand\nimages in a multilingual setting. Gemini models also show strong performance on multimodal image\nunderstanding and reasoning in mathematics, as shown in Figures 12, 18 and 19. Figure 20 is an\nexample of complex multimodal reasoning demonstrating how the model composes complex image\nunderstanding, code generation, and instruction following capabilities for a given user task. In Figure\n17, we see another example of the model being able to generate working code and follow complex\nuser instructions. Finally, Figure 21 shows an example of Gemini Ultra\u2019s capability of understanding\nvideo by reasoning over temporally connected set of frames.\n48\nGemini: A Family of Highly Capable Multimodal Models\n9.3.1. Chart understanding and reasoning over data\nPrompt\nSpot a data point that stands out in these charts and what that implicates. Then produce a\ndetailed markdown table for all the data shown.\nModel Response (rendered Markdown)\nThe data point that stands out is the United States\u2019 landfilled plastic waste, which is 73%. This\nis significantly higher than the other countries and regions shown. This implies that the United\nStates is not doing as much to recycle or incinerate its plastic waste as other countries and\nregions.\nCountry/Region\nLandfilled (%)\nMismanaged (%)\nIncinerated (%)\nRecycled (%)\nWorld\n49\n22\n19\n9\nUnited States\n73\n4\n19\n4\nEurope\n44\n6\n38\n12\nAsia (excl. China and\nIndia)\n39\n34\n19\n8\nFigure 8 | Solving a problem requiring multimodal chart understanding.The model has to read the\ntext, understand the connections between different data points and reason over them to recommend\nan interesting point and follow the instructions to generate a markdown table (shown correctly\nrendered).\nSource: Our World In Data (Ritchie et al., 2023).\n49\nGemini: A Family of Highly Capable Multimodal Models\n9.3.2. Multimodal question answering\nPrompt\nDo you know what it this plant? How do I best take care of it?\nModel Response\nThis is a Persian shield plant. It is a tropical plant that is native to Southeast Asia. It is a\npopular houseplant because it is easy to care for and has beautiful purple leaves.\nPersian shield plants prefer bright, indirect light. They can tolerate some direct sunlight, but\ntoo much direct sunlight can scorch their leaves. They also prefer warm temperatures and\nhigh humidity.\nPersian shield plants need to be watered regularly, but they should not be allowed to sit in\nwater. The soil should be kept moist but not soggy. They also benefit from being misted regularly.\nPersian shield plants can be fertilized every two weeks during the growing season. A balanced\nfertilizer is a good choice.\nPersian shield plants are susceptible to pests, such as mealybugs and spider mites. These pests\ncan be controlled with insecticidal soap or neem oil.\nPersian shield plants can be propagated by stem cuttings. The cuttings should be taken in the\nspring or summer and should be rooted in water or a moist potting mix.\nFigure 9 | Answering a multimodal information-seeking query. The model is able to recognize the\nspecific plant shown in the image and provide information about it. The model shows robustness to\ntypos as it is able to understand the user question despite them.\nSource: photo taken by an author from the Gemini team.\n50\nGemini: A Family of Highly Capable Multimodal Models\n9.3.3. Interleaved image and text generation\nPrompt\nPlease create a blog post about a trip to New York, where a dog and his owner had lots of fun.\nInclude a few pictures of the dog posing happily at different landmarks.\nModel Response\nHi, I\u2019m Cooper. I\u2019m a 2 year old Goldendoodle who loves to travel. Last month, my family and\nI went to New York City. It was my first time in the Big Apple, and I had a blast!\nWe stayed in a hotel near Times Square. It was so much fun to see all the bright lights and\npeople. We went to see a Broadway show, and I even got to meet some of the actors.\nOne of my favorite parts of New York City was Central Park. It was so big and beautiful. I\nloved running around and playing fetch. We also went to the Statue of Liberty and the Empire\nState Building. I was so excited to see all the famous landmarks.\nI had so much fun in New York City. It\u2019s definitely one of my favorite places to travel. I can\u2019t\nwait to go back someday!\nFigure 10 | Generating interleaved text and images. The model is able to follow the instructions of\ngenerating a blog post with images closely related to the text and with dog images showing high\nlevels of consistency across all images.\n51\nGemini: A Family of Highly Capable Multimodal Models\n9.3.4. Image understanding and reasoning\nPrompt\nLook at this sequence of three shapes. What shape should come as the fourth shape? Explain\nyour reasoning with detailed descriptions of the first shapes.\nModel Response\nThe fourth shape should be a hexagon. The first shape is a triangle, the second shape is a\nsquare, and the third shape is a pentagon. The number of sides in each shape is increasing\nby one\nFigure 11 | Solving a multimodal reasoning problem.The model is able to recognize shapes in the\nimage, understand their properties and reason about the relationship between them to predict the\nnext object.\nSource: photo taken by an author from the Gemini team.\n52\nGemini: A Family of Highly Capable Multimodal Models\n9.3.5. Geometrical reasoning\nPrompt\nFind the height of the parallelogram given its area with 100 square units.\nModel Response\nThe area of the parallelogram is equal to the product of the base and the height. Hence\n100 = (\ud835\udc65 + 15)\ud835\udc65. We get \ud835\udc652 + 15\ud835\udc65 \u2212 100 = 0. The roots of this equation are \ud835\udc651 = 5 and\n\ud835\udc652 = \u221220. The second root is negative and is not acceptable. Final Answer: The final answer\nis 5. I hope it is correct.\nFigure 12 | Solving a geometrical reasoning task. The model shows good understanding of the task\nand is able to provide meaningful reasoning steps despite slightly unclear instructions.\nSource: Lu et al. (2021).\n9.3.6. Information seeking about objects\nPrompt\nFind a connection between these. Hint: think about historical events.\nModel Response\nThe Moon is the only celestial body in which humans have played golf. In 1971, the Apollo\n14 crew hit two golf balls on the lunar surface.\nFigure 13 | Solving a puzzle using multimodal inputs. The model recognizes the objects in the images\nand identifies a commonality that connects the two objects.\nSource: photo taken by an author from the Gemini team.\n53\nGemini: A Family of Highly Capable Multimodal Models\n9.3.7. Multimodal reasoning based on visual cues\nPrompt\nDo you know the precise location where this image was taken?\nModel Response\nThis image was taken in New York City. The Empire State Building is visible in the background.\nThe street in the foreground is 8th Avenue. The cross street is West 34th Street.\nFigure 14 | Identifying the objects in the image (the Empire State Building) and recognizing what\nthose are even with small levels of visual distortion in the image. Based on the image, the model is\nalso able to correctly identify the precise location of the person taking the photo.\nSource: photo taken by an author from the Gemini team.\n54\nGemini: A Family of Highly Capable Multimodal Models\n9.3.8. Multimodal humor understanding\nPrompt\nCan you explain this meme?\nModel Response\nThe meme is about a person who is playing a game at 300 FPS, but their monitor is only\n75Hz. The person is trying to catch the game, but it is too fast for their monitor to display.\nThe meme is funny because it is relatable to anyone who has ever played a game on a monitor\nthat is not fast enough.\nFigure 15 | Explanation of humor in a meme. The model is showing the ability to not only describe\nwhat is happening in the image but also what it means even though the cultural context is not\nmentioned explicitly in the image or the prompt.\nSource: Hwang and Shwartz (2023).\n55\nGemini: A Family of Highly Capable Multimodal Models\n9.4. Commonsense reasoning in a multilingual setting\nPrompt\nExplain what i should call my grandparents on each side of my family based on this chart\nModel Response\nOn your father\u2019s side, you would call your grandparents Y\u00e9ye and N\u02c7ainai. On your mother\u2019s\nside, you would call your grandparents W\u00e0ig\u00afong and W\u00e0ip\u00f3.\nFigure 16 | Common-sense reasoning in images. The model is able to understand the relationships\nrepresented in the graphs and reason about them in a multilingual setting.\nSource: image created by an author from the Gemini team.\n56\nGemini: A Family of Highly Capable Multimodal Models\n9.4.1. Reasoning and code generation\nPrompt\nCreate a web app called \"Opossum Search\":\n1. Every time you make a search query, it should redirect you to a google search with the same\nquery, but the word opossum before it.\n2. It should be visually similar to Google search,\n3. Instead of the google logo, it should have a picture of an opossum from the internet.\n4. It should be a single html file, no separate js or css files.\n5. It should say \"Powered by google search\" in the footer.\nModel Response (rendered code)\nHere is the code for Opossum Search:\n1\n<!DOCTYPE html>\n2\n<html>\n3\n<head>\n4\n<title>Opossum Search</title>\n5\n<style>\n6\nbody {\n7\nfont\u2212family: Arial, sans\u2212serif;\n8\nmargin: 0;\n9\npadding: 0;\n10\n}\n11\n12\nheader {\n13\nbackground\u2212color: #333;\n14\ncolor: #fff;\n15\ntext\u2212align: center;\n16\npadding: 1rem 0;\n17\n}\n18\n19\n.search\u2212bar {\n20\ndisplay: flex;\n21\njustify\u2212content: center;\n22\nalign\u2212items: center;\n23\nheight: 50px;\n24\nmargin\u2212top: 1rem;\n25\n}\n26\n27\n.search\u2212input {\n28\nwidth: 50%;\n29\npadding: 0.5rem;\n30\nborder: 1px solid #ccc;\n31\nborder\u2212radius: 5px;\n32\n}\n33\n34\n.search\u2212button {\n35\nmargin\u2212left: 1rem;\n36\npadding: 0.5rem 1rem;\n37\nborder: 1px solid #ccc;\n38\nborder\u2212radius: 5px;\n39\ncursor: pointer;\n40\n}\n41\n42\nfooter {\n43\nbackground\u2212color: #333;\n44\ncolor: #fff;\n45\ntext\u2212align: center;\n46\npadding: 1rem 0;\n47\nmargin\u2212top: 1rem;\n48\n}\n49\n</style>\n50\n</head>\n51\n<body>\n52\n<header>\n53\n<h1>Opossum Search</h1>\n54\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Didelphis_virginiana.jpg/\n55\n1200px\u2212Didelphis_virginiana.jpg\" alt=\"Opossum\" width=\"100\">\n56\n</header>\n57\n58\n<div class=\"search\u2212bar\">\n59\n<input type=\"text\" class=\"search\u2212input\" placeholder=\"Search the web\">\n60\n<button class=\"search\u2212button\">Search</button>\n61\n</div>\n57\nGemini: A Family of Highly Capable Multimodal Models\n62\n<footer>\n63\nPowered by Google Search\n64\n</footer>\n65\n66\n<script>\n67\nconst searchInput = document.querySelector('.search\u2212input');\n68\nconst searchButton = document.querySelector('.search\u2212button');\n69\n70\nsearchButton.addEventListener('click',\n() => {\n71\nconst query = searchInput.value;\n72\nif (query) {\n73\nwindow.location.href = `https://www.google.com/search?q=opossum+${query}`;\n74\n}\n75\n});\n76\n</script>\n77\n</body>\nThis code creates a simple web app that looks similar to Google Search, but with an opossum\nlogo. When you enter a search query and click the \"Search\" button, it will redirect you to a\nGoogle search with the word \"opossum\" added to the beginning of your query. The app is\npowered by Google Search, as indicated in the footer.\nRendered Website\nFigure 17 | Writing code for a website based on user request. The model follows the instructions and\nrequirements defined by the user and converts them to functioning code.\n58\nGemini: A Family of Highly Capable Multimodal Models\n9.4.2. Mathematics: Calculus\nPrompt\nFind the derivative of sinh \ud835\udc65 + cosh \ud835\udc66 = \ud835\udc65 + \ud835\udc66.\nModel Response (rendered latex)\nWe have that\n\ud835\udc51\n\ud835\udc51\ud835\udc65 sinh \ud835\udc65 = cosh \ud835\udc65 and\n\ud835\udc51\n\ud835\udc51\ud835\udc65 cosh \ud835\udc66 = sinh \ud835\udc66 \ud835\udc51\ud835\udc66\n\ud835\udc51\ud835\udc65 . Therefore,\ncosh \ud835\udc65 + sinh \ud835\udc66 \ud835\udc51\ud835\udc66\n\ud835\udc51\ud835\udc65 = 1 + \ud835\udc51\ud835\udc66\n\ud835\udc51\ud835\udc65 .\nSolving for \ud835\udc51\ud835\udc66\n\ud835\udc51\ud835\udc65 , we find that\n\ud835\udc51\ud835\udc66\n\ud835\udc51\ud835\udc65 = 1 \u2212 cosh \ud835\udc65\nsinh \ud835\udc66 \u2212 1 .\nFinal Answer: The final answer is 1\u2212cosh \ud835\udc65\nsinh \ud835\udc66\u22121.\nFigure 18 | Solving a calculus problem. The model is able to get a solution to a calculus problem with\nstep-by-step explanation and correctly defined LaTeX equations.\nSource: question is provided by Macmillan Learning.\n59\nGemini: A Family of Highly Capable Multimodal Models\n9.5. Multi-step reasoning and mathematics\nPrompt\nIn the year 2018 factory A produced goods worth 500 million forints, while factory B produced\ngoods worth 400 million forints. According to their long-term plans, factory A will increase\nproduction by 5% each year, while factory B will increase production by 6% per year.\nThe above plans were published on an economy website. A debate started in the comment\nsection of the article: one user claimed that the difference between the volumes of production\nbetween the two factories would get smaller and smaller in the next couple of years.\nCalculate the missing values of the table below and show that the above claim is not true.\nHeader: | 2018 | 2019 | 2020 | 2021 |\nProduction volume of factory A (mln) | 500 | | | |\nProduction volume of factory B (mln) | 400 | | | |\nModel Response (rendered Markdown)\nYear\nProduction volume of\nfactory A (mln)\nProduction volume of\nfactory B (mln)\nDifference\n(mln)\n2018\n500\n400\n100\n2019\n525\n424\n101\n2020\n551.25\n449.44\n101.81\n2021\n578.81\n476.41\n102.4\nThe production volume of factory A in 2019 is 500 * 1.05 = 525 million forints.\nThe production volume of factory B in 2019 is 400 * 1.06 = 424 million forints.\nThe difference in production volume in 2019 is 525 - 424 = 101 million forints.\nThe production volume of factory A in 2020 is 525 * 1.05 = 551.25 million forints.\nThe production volume of factory B in 2020 is 424 * 1.06 = 449.44 million forints.\nThe difference in production volume in 2020 is 551.25 - 449.44 = 101.81 million forints.\nThe production volume of factory A in 2021 is 551.25 * 1.05 = 578.81 million forints.\nThe production volume of factory B in 2021 is 449.44 * 1.06 = 476.41 million forints.\nThe difference in production volume in 2021 is 578.81 - 476.41 = 102.4 million forints.\nAs you can see, the difference in production volume between the two factories is actually\nincreasing, not decreasing. Therefore, the claim that the difference between the volumes of\nproduction between the two factories would get smaller and smaller in the next couple of years\nis not true.\nFigure 19 | Solving a multi-step math problem. The model is able to understand the task and generate\na markdown table with correctly calculated values. It also explicitly follows the instructions to show\nwhere the numbers come from and answer the question given in the task.\nSource: Oktat\u00e1si Hivatal (2023, p. 20)\n60\nGemini: A Family of Highly Capable Multimodal Models\n9.5.1. Complex image understanding, code generation, and instruction following\nPrompt\nI want you to take the function depicted in the top left subplot, multiply it by 1000, and then\nadd it to the function depicted in the bottom left subplot. Generate matplotlib code for the\nsingle resulting plot.\nModel Response (rendered code)\n1\nimport numpy as np\n2\nimport matplotlib.pyplot as plt\n3\n4\nx = np.linspace(0, 10, 1000)\n5\ny1 = np.sin(x)\n6\ny2 = np.exp(x)\n7\n8\nplt.plot(x,\n1000\u2217y1 + y2)\n9\nplt.show()\nRendered Graph\nFigure 20 | Multimodal reasoning capabilities applied to code generation. Gemini Ultra needs to\nperform inverse graphics task to infer the code that would have generated the plots, perform additional\nmathematical transformations, and generate relevant code.\nSource: figure generated by an author from the Gemini team.\n61\nGemini: A Family of Highly Capable Multimodal Models\n9.5.2. Video understanding and reasoning\nPrompt (video)\nHow could this person improve their technique?\nModel Response\nThis soccer player needs to work on their ball-striking mechanics. Their foot is not making\ngood contact with the ball, which is causing the ball to fly up and away from the goal. They\nalso need to work on their body positioning. They should be leaning into the shot with their\nnon-kicking leg in front of their kicking leg, and they should be following through with their\nkicking leg.\nFigure 21 | Video understanding and reasoning over the situation presented in the video. Here, we\nprovide a video as input to the model together with a text prompt (images are provided here only\nfor visualization purposes). The model is able to analyze what happened in the video and provide\nrecommendations on how the actions in the video could have been better.\nVideo source: \"Football/Soccer Penalty Miss\"\nhttps://www.youtube.com/watch?v=VmWxjmJ3mvs\n62\n"
  },
  {
    "title": "StarVector: Generating Scalable Vector Graphics Code from Images",
    "link": "https://arxiv.org/pdf/2312.11556.pdf",
    "upvote": "26",
    "text": "StarVector: Generating Scalable Vector Graphics Code from Images\nJuan A. Rodriguez1,2,4 Shubham Agarwal1, 2 Issam H. Laradji1, 5 Pau Rodriguez6*\nDavid Vazquez1 Christopher Pal1,2,3 Marco Pedersoli4\n1ServiceNow Research 2Mila - Quebec AI Institute 3Canada CIFAR AI Chair 4 \u00b4ETS, Montr\u00b4eal, Canada\n5UBC, Vancouver, Canada 6Apple MLR, Barcelona, Spain * External collaboration\njuan.rodriguez@mila.quebec\nFigure 1. Image-to-SVG generation task: Given an input image, generate the corresponding SVG code. On the left, we show test\nexamples of complex SVGs from SVG-Emoji and SVG-Stack datasets. StarVector encodes images and processes them in a multimodal\nlanguage modeling fashion, to generate executable SVG code that resembles the input image. We show real generated code and rasterized\nimages produced by our StarVector model, showing impressive capabilities at generating appealing SVG designs and using complex syntax.\nAbstract\nScalable Vector Graphics (SVGs) have become integral\nin modern image rendering applications due to their infi-\nnite scalability in resolution, versatile usability, and edit-\ning capabilities.\nSVGs are particularly popular in the\nfields of web development and graphic design. Existing ap-\nproaches for SVG modeling using deep learning often strug-\ngle with generating complex SVGs and are restricted to sim-\npler ones that require extensive processing and simplifica-\ntion. This paper introduces StarVector, a multimodal SVG\ngeneration model that effectively integrates Code Genera-\ntion Large Language Models (CodeLLMs) and vision mod-\nels. Our approach utilizes a CLIP image encoder to extract\nvisual representations from pixel-based images, which are\nthen transformed into visual tokens via an adapter module.\nThese visual tokens are pre-pended to the SVG token em-\nbeddings, and the sequence is modeled by the StarCoder\nmodel using next-token prediction, effectively learning to\nalign the visual and code tokens.\nThis enables StarVec-\ntor to generate unrestricted SVGs that accurately repre-\nsent pixel images. To evaluate StarVector\u2019s performance,\nwe present SVG-Bench, a comprehensive benchmark for\nevaluating SVG methods across multiple datasets and rel-\nevant metrics. Within this benchmark, we introduce novel\ndatasets including SVG-Stack, a large-scale dataset of real-\nworld SVG examples, and use it to pre-train StarVector as a\nlarge foundation model for SVGs. Our results demonstrate\nsignificant enhancements in visual quality and complexity\nhandling over current methods, marking a notable advance-\nment in SVG generation technology.\nCode and models:\nhttps://github.com/joanrod/star-vector\n1. Introduction\nVector Graphics represent an archetypal form of image rep-\nresentation, where visual compositions are constituted by\nprimitive shapes such as vector paths, curves, or polygons,\nparametrized by mathematical equations [41]. This con-\ntrasts starkly with raster graphics, where images are repre-\nsented as pixels on a grid. The primary advantage of vector\ngraphics lies in their ability to maintain high precision and\nconsistent visual quality across various resolutions, as they\ncan be scaled arbitrarily without any loss of detail [34, 47].\nIn the realm of modern image rendering, Scalable Vector\nGraphics (SVGs) [54] have become a standard for encapsu-\nlating vector graphics in a code-based format. SVGs are the\npreferred choice for many artistic use cases like icon cre-\nation or typography. This format has gained prominence in\napplications demanding fast, efficient, and high-quality im-\n1\narXiv:2312.11556v1  [cs.CV]  17 Dec 2023\nage rendering. In web design, SVG contributes to enhanced\nrendering speeds and image compression owing to their in-\nherently small file sizes. They also offer dynamic editabil-\nity, allowing for straightforward modifications in color and\nsize, which is crucial for accessibility and responsive de-\nsign. For graphic design and scientific visualization, SVGs\nare prized for their visual quality, facilitating the creation of\nversatile designs and ensuring high-quality print outputs.\nThe SVG format utilizes Extensible Markup Language\n(XML) [26] syntax to define vector graphics, offering a rich\npalette for a broad range of graphical properties and effects.\nCentral to SVG is the vector path (or simply path), com-\nprised of points and control points connected by mathemat-\nically defined lines or curves, allowing detailed control over\nthe shape and size of the graphics. SVGs can also incor-\nporate a variety of other primitives, such as rectangles, el-\nlipses, and text, along with styles and advanced capabilities.\nDespite the eminent advantages of SVGs, existing deep\nlearning-based generative solutions have limitations in pro-\nducing high-quality, complex SVGs. Current approaches\n[12, 13, 61] typically model SVGs by learning a latent\nvariable model over command paths. Such methods pre-\ndominantly utilize simplified SVGs, limited to path com-\nmands and often restricted in complexity, with some focus-\ning solely on elementary fonts or icons [79, 83]. Recent\nadvancements involve using powerful image diffusion mod-\nels [64] to generate raster images, which are then converted\ninto SVG [34]. Nevertheless, it involves a costly iterative\nprocess of refinement and is also limited to paths. Despite\nthese efforts, a gap remains in systems that can directly gen-\nerate detailed and complex SVG code, leveraging the full\nrange of SVG primitives necessary for intricate designs.\nThis paper studies the task of image-to-SVG genera-\ntion (Figure 1), which has been traditionally approached\nas a form of image vectorization [42, 85], relying pre-\ndominantly on image processing and curve fitting tech-\nniques [41, 78]. Our research diverges from these meth-\nods, posing the task as a code-generation problem build-\ning upon recent advancements in Large Language Mod-\nels (LLMs) [9, 10, 74]. Thanks to the success in scaling\nup transformers [75], these models have shown outstand-\ning downstream abilities in tasks like language understand-\ning [16], summarization [71], or coding [40, 50, 65]. The\nemergent capabilities of LLMs in code creation are partic-\nularly relevant to our work, as shown by Bubeck et al. [10]\nin a study using GPT-4 [51] on generating SVG code.\nIn this work, we propose a novel paradigm, where a mul-\ntimodal LLM learns SVG synthesis as an image-to-code\ngeneration task. In this new setting, we tackle the prob-\nlem of image-to-SVG generation by learning a CLIP [57]\nimage encoder coupled with an adapter model to project im-\nages into visual token embeddings (visual tokens) and use\nthem to condition a StarCoder [40] model to generate an\nSVG code associated with the input image. The StarVector\narchitecture is shown in Figure 2. Addressing SVG gener-\nation with a code generation language model (CodeLLM)\nallows for preserving the richness and versatility of SVG\nprimitives and syntax, as it can handle unaltered real-world\nSVGs and no need for simplification. Further, using the\nSVG code as a representation instead of a latent variable\ncan bring enhanced editing capabilities. We propose the\ntask of image-to-SVG as a pre-training task for building a\nfoundation model [51, 74] for SVG generation.\nContributions.\nIn summary, our contributions are the fol-\nlowing: i) We introduce StarVector, a Large Multimodal\nModel for code generation, which leverages image and lan-\nguage modalities for generating executable SVG code from\nimages. ii) We present SVG-Bench, a unified evaluation\nbenchmark for SVG generation methods, which facilitates\naccess to popular SVG datasets and metrics. Within this\nbenchmark, we introduce two new datasets namely SVG-\nEmoji (composed of 10k complex emoji SVGs) and SVG-\nStack (a large-scale dataset with over 2M real-world SVGs).\niii) We evaluate StarVector and prior baselines on SVG-\nBench which focuses on the image-to-SVG generation task.\nWe showcase the ability of our model to generalize to com-\nplex SVGs and demonstrate the effectiveness of pre-training\nStarVector on the large-scale SVG-Stack dataset.\nThe paper is structured as follows: Section 2 presents\nprevious methods related to our research on SVG generation\nwhile Section 3 explains the StarVector method in detail.\nWe present SVG-Bench (with accompanying datasets and\nmetrics) in Section 4, followed by experimental results in\nSection 5 and conclusions in Section 6.\n2. Related Work\nThis section presents prior research relevant to our study,\nencompassing methods in vector graphics and SVG gener-\nation, developments in CodeLLMs, and advancements in\nmultimodal models that integrate image and textual data.\nSVG Generation Methods.\nEarly efforts in vector graph-\nics1 predominantly utilized traditional image processing\ntechniques for tasks like image vectorization [23, 42, 85],\noften involving segmentation and polynomial curve fit-\nting [41, 78]. With the advent of deep learning, new ap-\nproaches emerged.\nSVG-VAE [45], a class-conditional\nVariational Autoencoder (VAE) [35], predicts a latent style\nvector and generates SVGs using a LSTM decoder [30].\nDeepSVG [13] proposes a hierarchical VAE architecture\nusing transformers to represent SVG paths. Im2Vec [61]\ntranslates pixel images into latent representations, which\n1https://en.wikipedia.org/wiki/Comparison_of_\nraster-to-vector_conversion_software\n2\nFigure 2. StarVector architecture: Images in the pixel space are encoded into a set of 2D embeddings using CLIP [56]. The Adapter\napplies a non-linear transformation to the image embeddings to align them with Code-LLM space, obtaining visual tokens. StarCoder uses\nthe image embeddings as context to generate the SVG. During training the task is supervised by the next token prediction of the SVG\ntokens. During inference, the model uses the visual tokens from an input image to predict SVG code autoregressively.\ncan be decoded into paths via a recurrent neural network\n(RNN). However, latent-based methods are limited to path\nprimitives, thus restricting their scope to a subset of SVGs.\nBecause of this limitation, they tend to not generalize well\nand overfit on the complex-looking SVG datasets.\nRecent trends in image generation using diffusion [29,\n64] or autoregressive [25, 59, 86] models have also been\nexplored in the SVG space. VectorFusion [34] leverages a\nstrong text-to-image diffusion model to find the SVG via\niterative optimization. CLIPasso [77] uses a CLIP distance\nloss to iteratively refine SVG from sketches. Both these\nsolutions can be slow due to their iterative nature. Similar\nto ours, IconShop [83] trains a BERT [22] model for text-to-\nSVG conversion on icons, using path commands as tokens\nof the language model, while we use the SVG code.\nThis study addresses these challenges by proposing a\nnew avenue in SVG modeling. We design a model capable\nof generating unrestricted SVG code, focusing on directly\nrendering vector graphics within the SVG code space, by-\npassing the constraints of previous methodologies.\nLanguage Models for Code Generation.\nCodeLLMs, or\nlarge language models for code generation, have gained sig-\nnificant popularity in recent literature due to advances in\nnatural language processing (NLP) and the transformer ar-\nchitectures [75], such as the GPT [9, 51, 55] and Llama [73,\n74] families. Extensive availability of code datasets [8, 27,\n32, 36], has allowed training powerful CodeLLMs that have\nchanged the way software developers do their work [17].\nCodex [14] learns to generate Python functions based on\ninput docstrings and evaluates the correctness of the gener-\nated code samples automatically through unit tests. Code-\ngen [50], studies multi-turn program synthesis under scal-\ning laws, offering a family of models trained in several pro-\ngramming languages. StarCoder [40] presents a series of\nmodels with various sizes, trained on several coding lan-\nguages using a fill-in-the-middle objective.\nDespite SVG popularity, SVG language has been typi-\ncally avoided in training large coding models [2, 40] (pos-\nsibly for prioritizing more crowded coding communities).\nThis research seeks to bridge this gap by fine-tuning a pro-\nficient CodeLLM specifically on SVG code. Furthermore,\nwe integrate a vision module to facilitate the pre-training\ntask of image-to-SVG conversion.\nMultimodal Tasks and Models\nIn recent years, there\nhave been numerous works at the intersection of vision and\nlanguage on topics like image captioning [37\u201339], visual\nquestion answering (VQA) [3], contrastive learning [15, 57]\nor text-to-image generation [25, 59, 60, 64]. For obtain-\ning visual features some multimodal models [39, 48, 57]\nuse Vision transformers (ViT) [24]. Convolutional-based\nimage encoders like ConvNext [44] or VQGAN [25] have\nbeen also explored [25, 57, 64], that aim to preserve\nmore detailed features from images.\nSome models like\nFlamingo [1], MAPL [48] or BLIP2 [39] use an interme-\ndiate mapping module to convert image features into fixed-\nsize token embeddings. Similar to ours, Llava [43] obtains a\nset of visual tokens by projecting ViT features directly into\nthe LLM embedding space.\nWhile the majority of multimodal research has primar-\nily been centered around a fusion of images and natural\ntext [1, 39, 48, 57, 59], there has been relatively less at-\ntention to the process of translating images into code, ex-\ncept for few studies that convert web pages to HTML [20],\nimage to Latex markup [21], GUI screenshot-to-code con-\nversion [5, 7], and the generation of scientific vector graph-\nics through Tikz [6]. This progress suggests that handling\nimage generation as a coding task is an appealing solution.\nOur work explores different image-encoding techniques for\nthe vision part and uses all available visual tokens to condi-\ntion a StarCoder CodeLLM on images.\n3. StarVector\nThis section describes StarVector, a multimodal code gen-\neration model that accepts images and generates compil-\n3\nable SVG code associated with it. We formulate the task of\nSVG generation as a sequence modeling problem, where se-\nquences of tokens corresponding to the image and SVG do-\nmains are concatenated and modeled autoregressively. The\nproposed architecture is shown in Figure 2. StarVector in-\ntegrates an Image Encoder i.e., CLIP, with a CodeLLM i.e.,\nStarCoder through an Adapter layer. The Adapter converts\nimage representations into visual tokens aligned with the\nSVG token embeddings used in the CodeLLM. After fine-\ntuning, image and text token embeddings share the same\nrepresentation space, and the CodeLLM acquires SVG gen-\neration proficiency through next-token prediction, provid-\ning an effective solution for image-to-SVG conversion.\n3.1. Image Encoder and Visual Tokens\nThe efficacy of our model relies heavily on the image en-\ncoder, which needs to preserve intricate details and seman-\ntic content in the original image because, unlike captioning,\nwhere the output is typically short, SVG generation requires\ngenerating much longer sequences of code to obtain results\nof higher complexity. The image encoder projects the in-\nput image into a set of 2D embeddings rich in fine-grained\ndetails. To choose the best encoder, we draw inspiration\nfrom the success of pre-trained encoder backbones in down-\nstream computer vision tasks such as classification [57], re-\ntrieval, and generation [25], including both convolutional\nand transformer-based models. Formally, we experiment\nwith CLIP ViT-L/14 [57], ConvNext [44] (both pre-trained\non LAION-2B [66]), and VQGAN [25], which we pre-train\non an image reconstruction task using raster images from\nSVG-Stack. As the output of the image encoder, we uti-\nlize all the available hidden representations in the last lay-\ners to bring the most rich features. We define the output\nof the encoder zv as a flattened 2D grid of Lv embedding\nsequences.\nFor CLIP we have Lv = 257 embeddings,\nincluding the CLS token. For VQGAN, we use the pre-\nquantization layers and flatten them to obtain Lv = 196\nembeddings. For ConvNext, we flatten the last activation\nmap to obtain Lv = 49 embeddings.\nAdapter.\nThe Adapter module performs a non-linear\nprojection of the image embeddings into the LLM embed-\nding space, producing a set of visual token embeddings (or\nvisual tokens). This transformation matches the embedding\ndimensionality and aligns the image representations with\nthe language model\u2019s embedding space, effectively bridg-\ning the visual and SVG code modalities for the generation\ntask. The Adapter is composed of a sequence of fully con-\nnected (FC) layers with Swish [58] activation function and\nBatch Normaliazation [33].\n3.2. CodeLLM\nThe CodeLLM generates the complete SVG code condi-\ntioned on the visual tokens representing the image. We em-\nploy the StarCoder architecture by Li et al. [40] with pre-\ntrained weights, which provide a general model for code\ncompletion tasks.\nStarCoder is a decoder-only architec-\nture that uses Multi-Query Attention [68] for efficient sam-\npling. To address the long sequence lengths and high mem-\nory demands, we use flash-attention [18], enabling fine-\ntuning StarCoder with a context length of 8,192 tokens, the\nonly restriction of our models. This approach mitigates the\nquadratic complexity typically associated with neural atten-\ntion in long sequences. The fine-tuning process updates all\nthe model weights to overcome the distribution shift from\nthe original pre-training task of general code generation to\nour specific task of image-to-SVG conversion. We empha-\nsize that the pre-trained StarCoder is not trained to generate\nSVG code and thus needs to be fine-tuned end-to-end.\nTraining Process.\nDuring training, we first encode im-\nages x with the image encoder E as E(x), which returns\na hidden 2D features zv of dimension Lv \u00d7 Dv, where Lv\nis the sequence length and Dv the embedding size. The\nadapter A projects zv into the CodeLLM dimensionality\nas A(zv) resulting in visual tokens hv of dimensionality\nLv \u00d7 Dl, where Dl is the internal dimensionality of the\nCodeLLM. The ground truth SVG code is also tokenized\nand embedded into the CodeLLM space, as hl, with the\nsame dimensionality as the visual tokens hv. During train-\ning, we concatenate visual and SVG token embeddings, and\nthe sequence is modeled using standard language model-\ning training objective, i.e., next token prediction using SVG\ncode as supervision. During inference, we only compute vi-\nsual tokens from images and decode autoregressively from\nthe CodeLLM with hv as context.\n4. SVGBench: Benchmark for SVG Validation\nWe propose SVGBench, a unified benchmark for assess-\ning SVG synthesis models composed of tasks, datasets, and\nmetrics.\nHere we evaluate the proposed StarVector and\nbaselines on the task of image-to-SVG generation. This\ntask has been the primary benchmark in previous works\nand assesses the model\u2019s ability to generate SVG samples\nthat resemble the input image in the pixel space. We aim\nto define a standardized evaluation setting for SVG gener-\nation, building upon popular practices in the recent liter-\nature [13, 45, 61]. In summary, we compile together the\npopular datasets and metrics used in prior works and pro-\npose new ones that define SVG-Bench.\n4.1. Datasets\nTo encompass varying degrees of visual complexity across\ndifferent colors, shapes, and text, we select datasets com-\nprising examples of fonts, emojis, icons, and real-world ex-\namples e.g., the ones seen on websites. The datasets in-\ncluded in SVG-Bench are visually diverse and frequently\n4\nDataset\nTrain\nVal\nTest\nTestsim\nSource\nAvg. Token Length\nSVG Primitives\nSVG-Fonts\n1,831,857\n91,593\n4,821\n3,745\nGlypazzn [45]\n2,121 \u00b1 1,868\nVector path\nSVG-Icons\n80,442\n6,256\n2,449\n1,277\nDeepSVG [13]\n2,449 \u00b1 1,543\nVector path\nSVG-Emoji\n8,708\n667\n668\n96\nOpenMoji, NotoEmoji, TweMoji\n2,551 \u00b1 1805\nAll\nSVG-Stack\n2,169,710\n108,456\n5,709\n1,585\nTheStack [36]\n1,822 \u00b1 1,808\nAll\nTable 1. Datasets in SVG-Bench. We show the number of samples per split, with an additional reduced test set composed of simplified\nSVGs. We facilitate the source where the dataset was acquired, and statistics about the length of the SVG code in tokens, considering the\ntokenizer trained by StarCoder [40]. Finally, we show the type of SVG primitives used in the datasets. SVG-Emoji and SVG-Stack are\nintroduced in this paper. See Appendix 8 to see statistics and visualize samples from the datasets.\nModel\nInput\nOutput\nArchitecture\nSVG Simplification\nSeq Format\nSVG commands\nSVG primitives\nVtracer [78]\nImage\nSVG\nClustering + curve fitting\n\u2713\nCommands\nM, L, C\nVector path\nDeepSVG [13]\nSVG\nSVG\nTransformer\n\u2713\nCommands\nM, L, C\nVector path\nIm2Vec [61]\nImage\nSVG\nRNN\n\u2713\nKeypoints\nM, L, C\nVector path\nGPT-4 Vision [51]\nImage\nSVG\nMultimodal LLM\nSVG Code\nAll\nAll\nStarVector (ours)\nImage\nSVG\nMultimodal LLM\nSVG code\nAll\nAll\nTable 2. Baseline comparison. While prior works consider only 3 simple commands - M (Move), L (Line) and C (Curve), our model in\nprinciple can handle all type of complex SVG commands.\nused by digital artists in real-life scenarios.\nWe use\nSVG-Fonts introduced as Glypazzn [45] and SVG-Icons in\nDeepSVG [13]. In the absence of a standard benchmark,\nwe create different splits for all the datasets, which we re-\nlease to the community for further research. Following is a\ndescription of two datasets we propose for SVG generation.\nSVG-Emoji\nWe propose SVG-Emoji, a dataset of 10k\nimage-SVG pairs created by collating multiple smaller\nemoji datasets from different sources into a unified dataset.\nFormally, we include examples from TweMoji2, OpenMoji3\nand NotoEmoji4, where we also provide information about\ntheir class and the corresponding caption.\nSVG-Stack\nA key contribution of this work is SVG-\nStack, a comprehensive, large-scale dataset of real-world\nSVGs, which we introduce as part of SVG-Bench. SVG-\nStack is sourced from The Stack [36], a diverse dataset\ncomprising code samples from various software languages,\ncollated from public GitHub repositories5. Our selection\nbuilds upon the initial filtering and de-duplication processes\nconducted in [2, 36, 40]. We perform additional filtering\nto remove possible duplicates from other SVG datasets in\nthe benchmark. We extracted SVG code from The Stack\nand rasterized it at 224x224 resolution to obtain ground\ntruth images. This large amount of images, in conjunction\nwith the SVG code, forms the foundation for pre-training\nStarVector, enabling it to learn the image-to-SVG conver-\nsion task effectively.\n2https://github.com/twitter/twemoji\n3https://openmoji.org/\n4https://github.com/googlefonts/noto-emoji\n5https://huggingface.co/spaces/bigcode/in-the-stack\nTable 1 shows the dataset statistics defined in SVG-\nBench. We create splits for train, validation, and test. We\nalso create another test split using a pipeline of filtering and\nsimplification to be consistent with the other baselines.\n4.2. Evaluation Protocol\nIn evaluating SVG models, it is crucial to employ metrics\nthat accurately measure the fidelity of the generated SVG\nwith the original image, considering both vector and raster-\npixel representations. Traditional pixel-based metrics may\nnot be entirely adequate for SVG assessment, as the pre-\ndominance of background colors can skew them. For in-\nstance, a simple SVG with a predominantly white back-\nground might misleadingly score well in these metrics. To\naddress this, our evaluation framework also includes deep\nperceptual-based metrics and vector-space metrics, offer-\ning a more comprehensive and nuanced assessment of SVG\nconversion quality. We compute the following metrics:\n\u2022 Pixel-based metrics.\nWe employ Mean Squared Error\n(MSE) and Structural Similarity Index (SSIM) [80, 81].\nMSE quantifies the average squared difference between\nthe generated and the original images\u2019 pixels, offering a\nstraightforward measure of pixel-level accuracy. SSIM\nevaluates image quality based on the understanding of vi-\nsual perception, measuring changes in structural informa-\ntion, luminance, and contrast.\n\u2022 Vector-based metrics. We utilize Chamfer distance (CD),\na metric adapted from point cloud literature [84]. This in-\nvolves discretizing each SVG into a set of points by sam-\npling paths at regular intervals. CD measures the aver-\nage nearest-point distance between corresponding points\nin two SVGs, providing a quantitative measure of similar-\nity. A smaller CD indicates that the two SVGs are more\n5\nsimilar, while a larger distance suggests they are more dis-\ntinct. Having two SVGs s1 and s2, defined with a set of\npoints p1 \u2208 s1 and p2 \u2208 s2 in 2D, the CD is defined as,\nc(s1, s2) =\n1\n|s1|\nX\np1\u2208s1\nmin\np2\u2208s2 \u2225p1 \u2212 p2\u22252\n2 +\n1\n|s2|\nX\np2\u2208s2\nmin\np2\u2208s1 \u2225p2 \u2212 p1\u22252\n2,\n(1)\nwhere |si| is the cardinality of set si, and \u2225.\u22252\n2 is the\nsquared Euclidean norm.\n\u2022 Perceptual-based Metrics. We incorporate the Learned\nPerceptual Image Patch Similarity (LPIPS) [87] metric,\nwhich uses deep learning models trained on human per-\nceptual judgments. This metric is particularly effective in\ncapturing the nuances of human visual perception, pro-\nviding a more subjective assessment of image quality be-\nyond mere pixel-level comparisons.\n4.3. Baselines\nHere we describe the baselines used to compare StarVec-\ntor\u2019s performance in the task of image-to-SVG conversion.\nWe consider previous deep learning-based methods and\nrule-based traditional methods. We evaluate the baselines\nwith publicly available code in our proposed setup.\nIm2Vec [61] uses an end-to-end VAE, trained using only\nimage supervision to produce vector graphics. Input raster-\nized image is encoded to a \u2018global\u2019 latent vector, which is\npassed to an RNN to produce latent code for each path. The\npath decoder decodes these path codes into Bezier paths to\ngenerate the output SVG. We used the publicly available\ncode6 to report the results.\nDeepSVG [13] was introduced as a hierarchical path-\nbased VAE encoder-decoder transformer architecture. Here\ninput paths are encoded separately using a path encoder\nwhich are aggregated using a second encoder to produce\na latent vector. The decoder uses this latent vector to output\nthe path representations which provide actual draw com-\nmands and arguments. We used the open-source code7 to\nreproduce the results on different datasets. However, since\nthe DeepSVG framework only allows simplified SVGs, we\nreport results on the \u2018simplified\u2019 test set in Table 3.\nVTracer8 [78] is a rule-based algorithm to convert im-\nages to SVGs.\nThis 3-step pipeline algorithm relies on\nthe hierarchical clustering of images which are traced into\nvectors. First, pixels are converted into paths, which are\nsimplified into polygons.\nIn the last step, polygons are\nsmoothened and approximated with a Bezier curve fitter.\nGPT-4-Vision (Preview) [52] was recently released by\nOpenAI as a vision-based multimodal model, with a limit of\n100 API calls per day in the preview mode. We use GPT-4V\nby inserting an image and zero-shot prompting to generate\nSVG code. See Appendix 11 for more details.\n6https://github.com/preddy5/Im2Vec\n7https://github.com/alexandre01/deepsvg\n8https://github.com/visioncortex/vtracer\nFigure 3. Results on simplified SVG-Icons and SVG-Fonts test.\nFigure 4. Results on SVG-Icons test set\n5. Experiments and Results\nThis section presents the main experiments performed with\nthe StarVector model on SVGBench. We report results on\nthe simplified test as well as the full test set for the metrics\ndefined in Section 4.2. We also ablate our model with dif-\nferent image encoders and data augmentation. Finally, we\nconsider the effect of pre-training on SVG-Stack and fine-\ntuning on other datasets.\nWe use HuggingFace Transformers [82] and Py-\nTorch [53] for the implementation. We reproduce baseline\nmodels from official repositories, respecting the proposed\nhyperparameters (see Appendix 11 for more detail). All ex-\nperiments were done using 4 A100 80GB GPUs. We use\na batch size of 2 with a gradient accumulation of 8 steps\nand a learning rate of 5 \u00d7 10\u22124 for training. Models trained\non SVG-Stack with AdamW optimizer [46] require approx-\nimately 5 days of training.\nMain results\nResults using the simplified test sets are\nshown in Table 3, created to accommodate the limitations\nof DeepSVG. Our model invariably outperforms Im2Vec\nand DeepSVG baselines on all the metrics. Our model also\nsignificantly outperforms the rule-based VTracer algorithm\non Chamfer Distance (CD) while being comparable on the\nother metrics. Results on the complete test sets are depicted\nin Table 4. This setting considers substantially more exam-\nples with higher complexity, hence DeepSVG can not be\nevaluated here. StarVector improves over the Im2Vec base-\n6\nSVG-Fontssim\nSVG-Emojissim\nSVG-Iconssim\nSVG-Stacksim\nMethod\nMSE \u2193\nCD \u2193\nLPIPS \u2193 SSIM \u2191 MSE \u2193\nCD \u2193\nLPIPS \u2193 SSIM \u2191 MSE \u2193\nCD \u2193\nLPIPS \u2193 SSIM \u2191 MSE \u2193\nCD \u2193\nLPIPS \u2193 SSIM \u2191\nVTracer [78]\n0.014\n5.631\n0.044\n0.946\n0.018\n4.494\n0.064\n0.911\n0.009\n3.885\n0.052\n0.952\n0.016\n4.983\n0.061\n0.918\nDeepSVG [13]\n0.046\n3.747\n0.163\n0.823\n0.069\n5.486\n0.278\n0.748\n0.04\n2.492\n0.153\n0.851\n0.066\n4.804\n0.235\n0.736\nIm2Vec [61]\n0.031\n133.977\n0.187\n0.871\n0.042\n26.457\n0.258\n0.826\n0.029\n146.616\n0.178\n0.885\n0.043\n138.031\n0.258\n0.813\nGPT-4 Vision (100 examples)\n0.091\n65.103\n0.248\n0.755\n0.099\n52.206\n0.268\n0.701\n0.128\n50.649\n0.271\n0.709\n0.131\n55.455\n0.28\n0.668\nStarVector (ours)\n0.019\n1.435\n0.043\n0.93\n0.038\n1.005\n0.073\n0.859\n0.018\n0.665\n0.036\n0.931\n0.038\n2.208\n0.098\n0.858\nTable 3. Results on simplified (sim) datasets for the task of image-to-SVG conversion for different methods across SVGBench. Bold\ncells display the best model, and underlined cells show the second place (across all tables).\nSVG-Fonts\nSVG-Emojis\nSVG-Icons\nSVG-Stack\nMethod\nMSE \u2193\nCD \u2193\nLPIPS \u2193 SSIM \u2191 MSE \u2193\nCD \u2193\nLPIPS \u2193 SSIM \u2191 MSE \u2193\nCD \u2193\nLPIPS \u2193 SSIM \u2191 MSE \u2193\nCD \u2193\nLPIPS \u2193 SSIM \u2191\nVTracer [78]\n0.007\n4.105\n0.029\n0.903\n0.007\n8.261\n0.064\n0.913\n0.014\n3.335\n0.068\n0.927\n0.007\n6.318\n0.057\n0.891\nIm2Vec [61]\n0.133\n144.413\n0.208\n0.802\n0.124\n39.135\n0.528\n0.658\n0.052\n145.497\n0.221\n0.831\n0.179\n141.573\n0.357\n0.688\nGPT-4 Vision (100 examples)\n0.194\n27.909\n0.268\n0.689\n0.162\n21.134\n0.404\n0.612\n0.135\n49.249\n0.299\n0.666\n0.192\n16.981\n0.37\n0.604\nStarVector (ours)\n0.008\n2.098\n0.013\n0.976\n0.051\n2.194\n0.202\n0.778\n0.022\n0.483\n0.043\n0.923\n0.072\n6.153\n0.153\n0.785\nTable 4. Results on complete datasets for the task of image-to-SVG conversion. Metrics are computed on the full test sets of SVG-Bench.\nDeepSVG is not included as it does not support complex SVG images.\nFigure 5. Results on SVG-Emoji test set\nFigure 6. Results on SVG-Stack test set\nline by a huge margin on all the metrics. Im2Vec performs\nvery poorly on the CD metric for both the simplified as well\nas full test sets. StarVector outperforms VTracer on CD\nwhile being comparable in other metrics, signifying that our\nproposed approach learns to better mimic the ground truth\nSVG. We note that VTracer does not reason about the SVG\ngeneration as it is purely copying it. This limits VTracer for\nits use in other tasks such as text-to-SVG.\nNotably, the MSE metric can sometimes yield mislead-\ning results. This is particularly evident in the performance\nof Im2Vec on the simplified datasets shown in Table 3 and\nSVG-Icons from Table\n4, where it demonstrates seem-\ningly good reconstruction. The reason lies in the simpli-\nfied SVGs, which contain a significantly reduced number\nof pixels for representing the graphics, leading to a pre-\ndominantly white background in the images. For the other\ndatasets containing more pixels to represent the graphics,\nMSE for Im2Vec is more affected. In contrast, vector-based\nmetrics like CD provides a more appropriate measure for\nassessing similarity. We also observe overfitting problems\non DeepSVG and Im2Vec methods when training them on\nour setup. DeepSVG struggles to learn from SVG-Emoji\ndue to limited data, while Im2Vec consistently overfits. We\nhypothesize that since Im2Vec is trained using only image\nsupervision, it fails to generalize to complex SVGs.\nQualitative Evaluation\nWe present the SVG images gen-\nerated by different methods in Figure 3, for the simpli-\nfied SVGs, and in Figures 5 and 6 for the complex SVG-\nEmoji and SVG-Stack datasets. StarVector generates ap-\npealing SVG conversions, with a comparable performance\nto VTracer. GPT-4 relies exclusively on semantic abilities to\ncreate the SVG, thus, losing fidelity at the pixel level, while\nour model preserves both semantic as well as fine-grained\nvisual information. We also provide lemon-picked failure\ncases of our model in the Appendix.\nPre-training on SVG-Stack.\nPre-training on the SVG-\nStack is highly beneficial for the downstream datasets with\nsmall data. Table 5 shows the uplift on all the metrics for\ndifferent datasets. Qualitatively, we can also see that pre-\ntraining helps the model to identify the nuanced details from\n7\nSVG-Fonts\nSVG-Emojis\nSVG-Icons\nSVG-Stack\nVisual encoder MSE \u2193 CD \u2193 LPIPS \u2193 SSIM \u2191 MSE \u2193 CD \u2193 LPIPS \u2193 SSIM \u2191 MSE \u2193 CD \u2193 LPIPS \u2193 SSIM \u2191 MSE \u2193\nCD \u2193\nLPIPS \u2193 SSIM \u2191\nCLIP\n0.021\n2.344\n0.026\n0.955\n0.051\n2.194\n0.202\n0.778\n0.008\n2.098\n0.013\n0.976\n0.093\n9.867\n0.196\n0.753\nVQGAN\n0.072\n3.266\n0.092\n0.854\n0.099\n6.889\n0.345\n0.688\n0.055\n1.661\n0.117\n0.823\n0.158\n14.254\n0.315\n0.661\nConvNext\n0.073\n3.054\n0.085\n0.854\n0.088\n3.445\n0.311\n0.708\n0.055\n1.622\n0.116\n0.827\n0.146\n13.791\n0.288\n0.676\nTable 5. Ablation study with different image encoders while keeping the rest of the architecture the same. CLIP as the backbone vision\nmodel performs the best in similar parameter settings.\nSVG-Emojis\nSVG-Icons\nMethod\nMSE \u2193 CD \u2193 LPIPS \u2193 SSIM \u2191 MSE \u2193 CD \u2193 LPIPS \u2193 SSIM \u2191\nStarVector (vanilla)\n0.108\n5.318\n0.355\n0.683\n0.047\n1.704\n0.104\n0.845\n+ Data Augmentation\n0.097\n3.796\n0.329\n0.706\n0.029\n0.707\n0.057\n0.905\n+ SVG-Stack Pre-train\n0.061\n2.047\n0.225\n0.748\n0.031\n0.712\n0.057\n0.894\nTable 6. Results on SVG data augmentation. Smaller datasets\nprone to overfitting are evaluated in this experiment. \u201c+\u201d indicates\nthat previous rows are also included.\nFigure 7. Ablation on SVG-emoji using different Visual encoders.\nFigure 8. Ablation on SVG-Stack using different Visual encoders\nthe images. For the case of SVG-Emoji, pre-training is a\nstrong requirement, as it overfits without it due to limited\ndata. As shown in Figure 7, the model relies on colors and\nshapes to generate the SVG.\nData Augmentation\nWe introduce several data augmen-\ntation operations on SVGs, that aim to perform small modi-\nfications to the SVG code and rasterize it to get a new sam-\nple while training. We include rotation, color noise, and\ncurve noise (See Appendix 9 for more detail). We evaluate\nthis setting on datasets that include fewer samples, namely\nSVG-Emoji and SVG-Icons, as the other two datasets are\nlarger enough to not overfit. Results are shown in Table 6.\nBoth datasets display improvements using these augmen-\ntations. We see a strong uplift for SVG-Emoji which has\nlimited training data.\nAblation studies on Image Encoders\nWe ablated with\ndifferent visual encoders such as VQGAN [25] and Con-\nvNext [44], however, we found CLIP consistently outper-\nformed on all the metrics for different datasets (See Table\n5). Figures 7 and 8, show qualitatively how VQGAN and\nConvNext tend to lose local details during the generation\nwhile maintaining the relevant semantics.\n6. Conclusions\nWe present StarVector, a novel multimodal CodeLLM that\nauto-regressive generates compilable SVG code from pixel\nimages.\nOur model encodes images using a pre-trained\nCLIP encoder and generates vector graphics using a pre-\ntrained StarCoder, which we fine-tune in an end-to-end\nfashion. Empirically, our model outperforms previous base-\nlines for the image-to-SVG conversion task.\nWe also\npresent a standardized benchmark, SVG-Bench with two\nnew datasets SVG-Stack and SVG-Emoji for further re-\nsearch on real-world SVG modeling.\nBroader impact.\nWith our current work, we aim to stan-\ndardize and provide a reproducible benchmark consisting of\na collection of SVG datasets and evaluation metrics. We be-\nlieve our work will pave the way for future vector graphic\ngeneration models assisting digital artists.\nLimitations and future work.\nWhile this work is lim-\nited to image-to-SVG generation, we consider our pro-\nposed approach and framework a first step towards build-\ning next-generation multimodal systems to create and edit\nSVG for logotype design, scientific diagram, and figure cre-\nation [62, 63]. Another limitation is the context length of 8k\ntokens on the generated SVGs, which we aim to overcome\nin future work using the recent success of CodeLLMs like\nCodeLlama [65].\nAcknowledgments.\nWe thank Arjun Ashok,\nHector\nLaria, and Georges B\u00b4elanger for their valuable feed-\nback and suggestions.\nWe thank Raymond Li and Joel\nLamy-Poirier for their assistance with StarCoder training.\n8\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736,\n2022. 3\n[2] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao\nMou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas\nMuennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al.\nSantacoder:\ndon\u2019t reach for the stars!\narXiv preprint\narXiv:2301.03988, 2023. 3, 5\n[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVqa: Visual question answering. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2425\u2013\n2433, 2015. 3\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 1\n[5] Daniel Baul\u00b4e, Christiane Gresse von Wangenheim, Aldo von\nWangenheim, Jean CR Hauck, and Edson C Vargas J\u00b4unior.\nAutomatic code generation from sketches of mobile appli-\ncations in end-user development using deep learning. arXiv\npreprint arXiv:2103.05704, 2021. 3\n[6] Jonas Belouadi, Anne Lauscher, and Steffen Eger.\nAu-\ntomatikz: Text-guided synthesis of scientific vector graphics\nwith tikz. arXiv preprint arXiv:2310.00367, 2023. 3, 4\n[7] Tony Beltramelli. pix2code: Generating code from a graph-\nical user interface screenshot. In Proceedings of the ACM\nSIGCHI Symposium on Engineering Interactive Computing\nSystems, pages 1\u20136, 2018. 3\n[8] Ekaba Bisong and Ekaba Bisong. Google bigquery. Build-\ning Machine Learning and Deep Learning Models on Google\nCloud Platform: A Comprehensive Guide for Beginners,\npages 485\u2013517, 2019. 3\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 2, 3\n[10] S\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Jo-\nhannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat\nLee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial\ngeneral intelligence: Early experiments with gpt-4. arXiv\npreprint arXiv:2303.12712, 2023. 2, 3\n[11] Mu Cai, Zeyi Huang, Yuheng Li, Haohan Wang, and\nYong Jae Lee. Leveraging large language models for scalable\nvector graphics-driven image understanding. arXiv preprint\narXiv:2306.06094, 2023. 1\n[12] Defu Cao, Zhaowen Wang, Jose Echevarria, and Yan Liu.\nSvgformer:\nRepresentation learning for continuous vec-\ntor graphics using transformers.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10093\u201310102, 2023. 2\n[13] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and\nRadu Timofte. Deepsvg: A hierarchical generative network\nfor vector graphics animation. Advances in Neural Informa-\ntion Processing Systems, 33:16351\u201316361, 2020. 2, 4, 5, 6,\n7, 3\n[14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Hen-\nrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. Evalu-\nating large language models trained on code. arXiv preprint\narXiv:2107.03374, 2021. 3\n[15] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-\ning laws for contrastive language-image learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2818\u20132829, 2023. 3\n[16] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 2\n[17] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikan-\njam, Foutse Khomh, Michel C Desmarais, and Zhen\nMing Jack Jiang. Github copilot ai pair programmer: Asset\nor liability? Journal of Systems and Software, 203:111734,\n2023. 3\n[18] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christo-\npher R\u00b4e. Flashattention: Fast and memory-efficient exact at-\ntention with io-awareness. Advances in Neural Information\nProcessing Systems, 35:16344\u201316359, 2022. 4, 3\n[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009. 1\n[20] Yuntian Deng, Anssi Kanervisto, and Alexander M Rush.\nWhat you get is what you see: A visual markup decompiler.\narXiv preprint arXiv:1609.04938, 10:32\u201337, 2016. 3\n[21] Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, and Alexan-\nder M Rush. Image-to-markup generation with coarse-to-\nfine attention.\nIn International Conference on Machine\nLearning, pages 980\u2013989. PMLR, 2017. 3\n[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 3\n[23] James Richard Diebel. Bayesian Image Vectorization: the\nprobabilistic inversion of vector image rasterization. Stan-\nford University, 2008. 2\n[24] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 3, 1\n[25] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 12873\u201312883, 2021. 3, 4, 8,\n1\n9\n[26] Jon Ferraiolo, Fujisawa Jun, and Dean Jackson. Scalable\nvector graphics (SVG) 1.0 specification. iuniverse Bloom-\nington, 2000. 2\n[27] Leo Gao, Stella Biderman, Sid Black, Laurence Golding,\nTravis Hoppe, Charles Foster, Jason Phang, Horace He, An-\nish Thite, Noa Nabeshima, et al.\nThe pile: An 800gb\ndataset of diverse text for language modeling. arXiv preprint\narXiv:2101.00027, 2020. 3\n[28] Xavier Glorot and Yoshua Bengio. Understanding the diffi-\nculty of training deep feedforward neural networks. In Pro-\nceedings of the thirteenth international conference on artifi-\ncial intelligence and statistics, pages 249\u2013256. JMLR Work-\nshop and Conference Proceedings, 2010. 1\n[29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3\n[30] Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735\u20131780, 1997. 2\n[31] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin\nChoi. The curious case of neural text degeneration. arXiv\npreprint arXiv:1904.09751, 2019. 3, 4\n[32] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Al-\nlamanis, and Marc Brockschmidt. Codesearchnet challenge:\nEvaluating the state of semantic code search. arXiv preprint\narXiv:1909.09436, 2019. 3\n[33] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In International conference on machine learn-\ning, pages 448\u2013456. pmlr, 2015. 4\n[34] Ajay Jain, Amber Xie, and Pieter Abbeel.\nVectorfusion:\nText-to-svg by abstracting pixel-based diffusion models. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 1911\u20131920, 2023. 1, 2,\n3\n[35] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 2\n[36] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,\nChenghao Mou, Carlos Mu\u02dcnoz Ferrandis, Yacine Jernite,\nMargaret Mitchell, Sean Hughes, Thomas Wolf, et al. The\nstack: 3 tb of permissively licensed source code.\narXiv\npreprint arXiv:2211.15533, 2022. 3, 5\n[37] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming\nYan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng\nCao, et al. mplug: Effective and efficient vision-language\nlearning by cross-modal skip-connections.\narXiv preprint\narXiv:2205.12005, 2022. 3\n[38] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning, pages 12888\u2013\n12900. PMLR, 2022.\n[39] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 3\n[40] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muen-\nnighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\nChristopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may\nthe source be with you! arXiv preprint arXiv:2305.06161,\n2023. 2, 3, 4, 5, 1\n[41] Tzu-Mao Li, Michal Luk\u00b4a\u02c7c, Micha\u00a8el Gharbi, and Jonathan\nRagan-Kelley.\nDifferentiable vector graphics rasterization\nfor editing and learning.\nACM Transactions on Graphics\n(TOG), 39(6):1\u201315, 2020. 1, 2, 5\n[42] Zicheng Liao, Hugues Hoppe, David Forsyth, and Yizhou\nYu.\nA subdivision-based representation for vector image\nediting. IEEE transactions on visualization and computer\ngraphics, 18(11):1858\u20131867, 2012. 2\n[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 3\n[44] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 11976\u201311986,\n2022. 3, 4, 8, 1\n[45] Raphael Gontijo Lopes, David Ha, Douglas Eck, and\nJonathon Shlens. A learned representation for scalable vec-\ntor graphics. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 7930\u20137939, 2019. 2,\n4, 5, 3\n[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6, 2\n[47] Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev,\nNikita Orlov, Yun Fu, and Humphrey Shi. Towards layer-\nwise image vectorization. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 16314\u201316323, 2022. 1\n[48] Oscar Ma\u02dcnas, Pau Rodriguez, Saba Ahmadi, Aida Ne-\nmatzadeh, Yash Goyal, and Aishwarya Agrawal.\nMapl:\nParameter-efficient adaptation of unimodal pre-trained mod-\nels for vision-language few-shot prompting, 2023. 3\n[49] Kenton Murray and David Chiang.\nCorrecting length\nbias in neural machine translation.\narXiv preprint\narXiv:1808.10006, 2018. 3\n[50] Erik Nijkamp,\nBo Pang,\nHiroaki Hayashi,\nLifu Tu,\nHuan Wang, Yingbo Zhou, Silvio Savarese, and Caim-\ning Xiong.\nCodegen: An open large language model for\ncode with multi-turn program synthesis.\narXiv preprint\narXiv:2203.13474, 2022. 2, 3, 1\n[51] OpenAI. Gpt-4 technical report, 2023. 2, 3, 5\n[52] OpenAI. GPT-4V(ision) System Card. https://cdn.\nopenai.com/papers/GPTV_System_Card.pdf,\n2023. Accessed: 2023-11-05. 6, 2, 3\n[53] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in PyTorch. In NeurIPS-W, 2017. 6\n[54] Antoine Quint. Scalable vector graphics. IEEE MultiMedia,\n10(3):99\u2013102, 2003. 1\n[55] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. Improving language understanding by gen-\nerative pre-training. openAI, 2018. 3\n10\n[56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. 3\n[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 3, 4, 1\n[58] Prajit Ramachandran,\nBarret Zoph,\nand Quoc V Le.\nSearching\nfor\nactivation\nfunctions.\narXiv\npreprint\narXiv:1710.05941, 2017. 4\n[59] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n3\n[60] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 3\n[61] Pradyumna Reddy, Michael Gharbi, Michal Lukac, and\nNiloy J Mitra. Im2vec: Synthesizing vector graphics without\nvector supervision. arXiv preprint arXiv:2102.02798, 2021.\n2, 4, 5, 6, 7\n[62] Juan A Rodriguez, David Vazquez, Issam Laradji, Marco\nPedersoli, and Pau Rodriguez. Figgen: Text to scientific fig-\nure generation. arXiv preprint arXiv:2306.00800, 2023. 8\n[63] Juan A Rodriguez, David Vazquez, Issam Laradji, Marco\nPedersoli, and Pau Rodriguez.\nOcr-vqgan: Taming text-\nwithin-image generation. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision,\npages 3689\u20133698, 2023. 8\n[64] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3\n[65] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu,\nTal Remez, J\u00b4er\u00b4emy Rapin, et al. Code llama: Open foun-\ndation models for code. arXiv preprint arXiv:2308.12950,\n2023. 2, 8, 1\n[66] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 4,\n1\n[67] Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie,\nBrian Strope, and Ray Kurzweil. Generating high-quality\nand informative conversation responses with sequence-to-\nsequence models. arXiv preprint arXiv:1701.03185, 2017.\n3\n[68] Noam Shazeer. Fast transformer decoding: One write-head\nis all you need. arXiv preprint arXiv:1911.02150, 2019. 4, 3\n[69] Benjamin Spector and Chris Re.\nAccelerating llm in-\nference with staged speculative decoding.\narXiv preprint\narXiv:2308.04623, 2023. 3\n[70] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: a simple way\nto prevent neural networks from overfitting. The journal of\nmachine learning research, 15(1):1929\u20131958, 2014. 1\n[71] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,\nRyan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and\nPaul F Christiano. Learning to summarize with human feed-\nback. Advances in Neural Information Processing Systems,\n33:3008\u20133021, 2020. 2\n[72] Chengjun Tang, Kun Zhang, Chunfang Xing, Yong Ding,\nand Zengmin Xu. Perlin noise improve adversarial robust-\nness. arXiv preprint arXiv:2112.13408, 2021. 2\n[73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 3\n[74] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 2, 3\n[75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2, 3\n[76] Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R\nSelvaraju, Qing Sun, Stefan Lee, David Crandall, and\nDhruv Batra.\nDiverse beam search:\nDecoding diverse\nsolutions from neural sequence models.\narXiv preprint\narXiv:1610.02424, 2016. 3\n[77] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Ro-\nman Christian Bachmann, Amit Haim Bermano, Daniel\nCohen-Or, Amir Zamir, and Ariel Shamir.\nClipasso:\nSemantically-aware object sketching. ACM Transactions on\nGraphics (TOG), 41(4):1\u201311, 2022. 3\n[78] Vision\nCortex.\nVTracer.\nhttps : / / www .\nvisioncortex . org / vtracer - docs, 2023.\n2,\n5, 6, 7\n[79] Yizhi Wang and Zhouhui Lian. Deepvecfont: Synthesizing\nhigh-quality vector fonts via dual-modality learning. ACM\nTransactions on Graphics (TOG), 40(6):1\u201315, 2021. 2\n[80] Zhou Wang and Alan C Bovik. Mean squared error: Love\nit or leave it? a new look at signal fidelity measures. IEEE\nsignal processing magazine, 26(1):98\u2013117, 2009. 5\n[81] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600\u2013612, 2004. 5\n11\n[82] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-\nmond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, R\u00b4emi Louf, Morgan Funtowicz, et al. Huggingface\u2019s\ntransformers: State-of-the-art natural language processing.\narXiv preprint arXiv:1910.03771, 2019. 6\n[83] Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liao. Icon-\nshop: Text-based vector icon synthesis with autoregressive\ntransformers. arXiv preprint arXiv:2304.14400, 2023. 2, 3\n[84] Tong Wu, Liang Pan, Junzhe Zhang, Tai Wang, Ziwei Liu,\nand Dahua Lin. Density-aware chamfer distance as a com-\nprehensive metric for point cloud completion. arXiv preprint\narXiv:2111.12702, 2021. 5\n[85] Tian Xia, Binbin Liao, and Yizhou Yu. Patch-based image\nvectorization with automatic curvilinear feature alignment.\nACM Transactions on Graphics (TOG), 28(5):1\u201310, 2009. 2\n[86] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2(3):5, 2022. 3\n[87] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 6\n12\nStarVector: Generating Scalable Vector Graphics Code from Images\nSupplementary Material\nIn the following, we present a further description of\nthe StarVector architecture, its training process, and how\nwe generate SVG samples from images. We also provide\nmore details about SVGBench with the proposed datasets as\nwell as the different baselines within the evaluation setup.\nWe also include additional results and discussions of our\nmethod for image-to-SVG generation.\n7. Model Architecture\nCode LLM. We consider several aspects in the choice of\nthe code LLM to handle the SVG code generation. First,\nwe require an LLM that is able to handle large token con-\ntexts during training, as SVG code samples are typically\nof long lengths (between 1000-4000 tokens, but growing\narbitrarily for much more complex vector graphics). Sec-\nond, we need fast decoding during the generation of these\nlarge contexts. Finally, we would benefit from models that\nhave been extensively pre-trained on general coding tasks,\nto avoid early training costs. Some prior works offer open-\nsource models that fit these requirements. We explored the\nopen-source families of models CodeGen [50], and Star-\nCoder [40]. We empirically found that StarCoder was the\nbetter option during our development stage as it allows a\ntoken context length of 8,192, a strong requirement of our\ncomplex SVG datasets (e.g., SVG-Emoji or SVG-Stack).\nWe make use of the StarCoder1B model which fits well in\nour GPU training setup given that we include a large image\nencoder (i.e., CLIP) and need to manage gradients and acti-\nvations for the image modality and long token sequences.\nIn the future, we will focus on scaling up the StarCoder\nmodel to 3, 7, and 16 billion parameters 9, which we con-\nsider can bring benefits in learning to generate SVG of\nhigher quality and complexity. Furthermore, the CodeL-\nlama models [65] have shown remarkable coding perfor-\nmance and the possibility of scaling context length above\nour current 8,192 limit.\nImage Encoder.\nOur image encoding pipeline consists of\ncomputing a set of feature representations from the images\nusing a backbone image encoder and aligning them to the\nCodeLLM via the adapter module. State-of-the-art image\nencoders are typically focused on natural images. However,\nour data contains images of logotypes, icons, fonts, or emo-\njis, which typically contain no background (which we set to\nwhite) and mostly constant colors. We explore several im-\nage encoders based on different paradigms. VQGAN [25] is\nbased on learning to project images to discrete tokens. First,\n9https://huggingface.co/blog/starcoder\nwe fine-tune an Imagenet [19]-pretrained VQGAN and fine-\ntune it with SVG-Stack on the proposed VQ-adversarial re-\nconstruction task. We find that using the features before the\nquantization yields better results. ConvNext [44] is a con-\nvolutional backbone, which we extract features before pool-\ning. We start from a LAION-2B [66]-pretrained checkpoint.\nFinally, ViT CLIP [57] is based on the Visual Transformer\n(ViT) [24] and is well prepared for autoregressive tasks. We\nextract all output representations. We use a LAION-2B pre-\ntrained model. During the training of StarVector, all the\nparameters of the image encoders are updated. We find that\nthe best choice is using CLIP. We consider that the gains in\nperformance come from the fact that CLIP uses more visual\ntokens (257) than the other image encoders.\nThe adapter first projects the features from the original\ndimensionality Dv to a dimensionality Dv \u00d7 2, followed\nby a Swish non-linear activation function, and a linear pro-\njection to the LLM dimensionality Dl. Finally, we apply\na layer normalization [4]. We initialize the adapter param-\neters with Glorot [28]. Dropout [70] of 0.1 is applied at\nthe beginning. These hyperparameters were found using a\nrandom search on SVG-Fonts.\nFrom our results, we see that image resolution is im-\nportant in order to capture fine-grained details like texts or\nhigh-frequency patterns. As seen in some examples (see\nFigure 19), diagrams and figures are part of the SVG-Stack\ndataset and present challenging horizontal or vertical aspect\nratios. When images have these aspect ratios, we make the\nimage fit in the 224\u00d7224 resolution, losing much detail, es-\npecially for the OCR capabilities of reading rendered texts\nand accurately displaying them.\nAdditional results comparing image encoders can be\nfound in Figures 7 and 12. These results show the boost\nin precision obtained when using CLIP. VGQAN and Con-\nvNext often fail at capturing the given shape of the image\nas well as the trajectory of the path. We note that ConvNext\nperforms better than VQGAN. These differences are also\ndue to the differences in the number of parameters. The\nCLIP ViT-L/14 model that we use consists of 290,581,504\nparameters, VQGAN consists of 29,298,176, and ConvNext\nconsists of 179,385,345 parameters.\nGenerating SVGs from natural images is out of the scope\nof this project. However, future work will focus on adapting\nour model to natural images, drawing from [47] and [11] to\ncreate a dataset of natural images and SVG pairs.\n1\n8. SVGBench\nHere we extend our description of the datasets used for\ntraining and evaluating StarVector and other baselines. Ear-\nlier SVG datasets proposed in the literature (mainly datasets\nof emojis and fonts) were not easily accessible, due to bro-\nken URLs and no direct entry point. Therefore, we provide\nthem as part of SVGBench for easy reproducibility. We in-\ntroduce splits for train, validation, and testing. The train set\nis used to optimize the parameter weights of the network.\nThe validation is used for tuning sampling hyperparameters,\nand the test is used for evaluation. Our model can handle up\nto 8k context tokens, therefore our datasets only consider\nexamples with up to 8,192 tokens. See table 7 for a com-\nplete description of the datasets.\nSVG Simplification.\nAs mentioned before, DeepSVG re-\nquires a simplification of the SVG in its input. The simplifi-\ncation consists of eliminating complex primitives and using\nonly vector paths. Also, color and shapes are abstracted\nto only use simple line strokes. We create simplified ver-\nsions of the datasets Table 7 shows the complete datasets\ncontained in SVG-Bench\n9. Data augmentation for SVG\nWe introduce several augmentation operations to SVGs in\norder to apply slight changes that help our model learn to\ngenerate more precise results. For instance, being able to\ncapture precise colors from the image and encode them in\nhexadecimal code to introduce it in the fill attribute of\nthe SVG element. Applying rotations or adding noise to the\ncontrol points of the curve helps the model learn to precisely\ncapture the position of the edges or thickness of the stroke.\nWe perform random rotations in an angle range. We per-\nform color changes by first parsing the color of the element\nusing the fill attribute and adding small white Gaussian\nnoise to the RGB values. We propose curve noise by in-\njecting a small Perlin [72] noise into the control points in\nB\u00b4ezier curves. We also experimented with adding Gaussian\nnoise which resulted in much less natural results. We apply\nthis noise by uniformly sampling a scalar from the interval\nbetween 0.01 and 0.05 and use it to scale the noise\nWe apply these augmentations directly on the SVG code,\nwhich involves parsing the XML code and accessing the\nattributes and arguments of the primitives defined. We use\nthe libraries BeautifulSoup10 and SvgPathTools11.\nSome primitives are simplified using our augmentations.\n10https://www.crummy.com/software/BeautifulSoup/\nbs4/doc/\n11https://github.com/mathandy/svgpathtools\n10. Training\nFor training the StarVector model we define the task of\nimage-to-SVG conversion as learning to map a sequence of\nimage token embeddings (or visual tokens) to a sequence of\nSVG code token embeddings. It can be seen as a sequence-\nto-sequence problem modeling the translation between the\nimage and the SVG code domains.\nThe goal of StarVector is to estimate the conditional\nprobability p(y1, . . . , ym|x1, . . . , xn), where x1, . . . , xn is\nthe input sequence of image tokens and y1, . . . , ym is the\noutput sequence of SVG tokens. n corresponds to the length\nof the image token sequence, which is fixed and defined by\nthe image encoder (see Section 3 ) and m is the variable size\nof the SVG code tokens. Denoting X = (x1, . . . , xn) we\ncompute this conditional probability as\np(y1, . . . , ym|X) =\nm\nY\nt=1\np(yt|X, y1, . . . , yt\u22121),\n(2)\nwhere the distribution p(yt|X, y1, . . . , yt\u22121) is represented\nusing a softmax over the LLM vocabulary of tokens.\nAs described in Section 7 we make use of a CLIP image\nencoder and a non-linear adapter to obtain a sequence of\nimage token embeddings (we refer to them as visual tokens)\nWe find that this general task makes the model learn to\ndraw vectors that look like the image. Notably, this task\ncan be learned without supervision in the image domain,\nonly relying on a categorical cross-entropy loss on the LLM\nvocabulary introduced by the next-token prediction task.\nWe use a batch size of 2. Images are processed with a\nresolution of 224x224, as defined by the pre-trained CLIP\nimage encoder, and process a maximum of 8192 tokens,\nconsidering the 257 tokens for representing the images (vi-\nsual tokens) and the rest for the SVG tokens. We use gradi-\nent batch accumulation of 8, and we train on a data parallel\nsetup with 4 A100 80GB GPUs, having an effective batch\nsize of 64. The learning rate is set to 5 \u00d7 10\u22124 for training,\nusing AdamW optimizer [46] for approximately 5 days of\ntraining on SVG-Stack dataset.\n11. Baselines\nWe reproduce all previous approaches in SVGBench, as the\navailable results come from an unclear version of the fonts,\nemojis, and icons datasets. Namely, we run DeepSVG[13]\nand Im2Vec [61] using the official implementations. We\nused the hyperparameters proposed by the authors and uti-\nlized pre-/post-processing code as needed. We use the re-\ncent GPT4 Vision [52] model capable of processing images\nas input and producing SVG code in the output. We also\nrun VTracer on our data, which is a rule-based method (i.e.,\nnot learning from data).\n2\nDataset\nTrain\nVal\nTest\nSource\nAvg. Token Length\nSVG Primitives\nSVG-Fonts\n1,831,857\n91,593\n4,821\nGlypazzn [45]\n2,121 \u00b1 1,868\nVector path\nSVG-Fontssim\n1,436,391\n71,789\n3,745\n1,722 \u00b1 723\nVector path\nSVG-Emojis\n8,708\n667\n668\nOpenMoji, NotoEmoji, TweMoji\n2,551 \u00b1 1,805\nAll\nSVG-Emojissim\n580\n57\n96\n2,448 \u00b1 1,026\nVector Path\nSVG-Icons\n80,442\n6,256\n2,449\nDeepSVG [13]\n2,449 \u00b1 1,543\nVector path\nSVG-Iconssim\n80,435\n2,836\n1,277\n2,005 \u00b1 824\nVector path\nSVG-Stack\n2,169,710\n108,456\n5,709\nTheStack [36]\n1,822 \u00b1 1,808\nAll\nSVG-Stacksim\n601,762\n30,061\n1,585\n2,042 \u00b1 918\nVector path\nTable 7. Complete datasets on SVG-Bench. The subscript sim stands for the simplified version of the dataset.\nIm2Vec.\nWe scaled down all the images to 128\u00d7128 res-\nolution to be compatible with the Im2Vec model. We use\na learning rate of 5 \u00d7 10\u22124 and a batch size of 8. We im-\nplement a custom post-processing operation for converting\nthe vector parameters obtained during Im2Vec inference to\nobtain compilable SVG code.\nDeepSVG.\nThis model can only handle simplified SVGs\ncomposed of simple line strokes and splines (see examples\nin Figure 3).\nFurther, it can only process SVGs with 8\ngroups (i.e., groups of shapes, or parent nodes) and vector\npaths of at most 30 commands. To reproduce the DeepSVG\nbaseline, we use the original hyperparameters, including a\nlearning rate of 1e\u22123 and a number of epochs to be 50. We\nuse a batch size of 200, except for the smaller emoji dataset,\nwhere we experiment with a batch size of 50.\nVTracer.\nWe use the Python library12 for experiments\nwhich is a wrapper over the Rust implementation. Simi-\nlar to Im2Vec, we scale down all the images to 128X128\nresolution. We use all the default values for the rule-based\nengine which generates a multi-colored SVG.\nGPT-4 Vision (preview).\nGPT-4 Vision [52] offers mul-\ntimodal capabilities for handling image and text data for\ntasks like image captioning, or text-image interleaved con-\nversation.\nHere we show how one can use prompt en-\ngineering [9, 10, 52] to condition the model to gener-\nate executable SVG code that represents the given im-\nage.\nFigure 9 displays the prompt that we use for this\nendeavor. We use the OpenAI library13 to perform exper-\niments with GPT-4-Vision.\nNotably, the currently avail-\nable model gpt-4-vision-preview has a limit of 100\nqueries per day, which only allows us to report results on\n100 examples per dataset (see Table 3).\nFigure 9. Prompt used for SVG Generation from GPT-4-V.\n12. Sampling from StarVector\nHere we describe how we sample SVG code from our\nmodel. As a decoder-only LLM [40], StarVector first com-\nputes the key-value (KV) cache using the visual tokens from\nthe image and produces the initial set of output logits. This\nstage is often quick because the model can process the entire\nvisual token sequence simultaneously [69]. The selected to-\nken from the output logits is then input back into the model,\nwhich in turn generates logits for the subsequent token.\nThis process is iteratively repeated until the model produces\nthe desired quantity of tokens. Our approach uses architec-\ntural improvements for fast decoding such as FlashAtten-\ntion [18] and Multi-Query Attention [68].\nTo select the correct sampling temperature we perform a\ngrid on SVG-Emoji and SVG-Icons validation sets. Fig-\nure 10 shows that the choice of temperature does not\nstrongly impact the results. However, a 1-point increase in\nperformance is observed on CD for SVG-Emoji using tem-\nperatures close to 1.0. We also present an ablation study on\npopular decoding techniques [31, 49, 67, 76]. Specifically,\nwe experiment with greedy decoding, beam search, and nu-\ncleus sampling with top-p. Results are shown in Table 8.\nThe use of nucleus sampling with top-p=0.9 and temper-\nature T=0.5 (no beam search) shows to be the best option.\nThe use of beam search improves the greedy decoding base-\nline, but it does not work well when combined with nucleus\nsampling, increasing the inference time. In sum, we recom-\n12https://github.com/etjones/vtracer_py\n13https://platform.openai.com/docs/libraries\n3\nSVG-Fonts\nSVG-Emojis\nSVG-Icons\nSVG-Stack\nSampling technique\nMSE \u2193 CD \u2193 LPIPS \u2193 SSIM \u2191 MSE \u2193 CD \u2193 LPIPS \u2193 SSIM \u2191 MSE \u2193 CD \u2193 LPIPS \u2193 SSIM \u2191 MSE \u2193\nCD \u2193\nLPIPS \u2193 SSIM \u2191\nGreedy\n0.013\n2.255\n0.019\n0.969\n0.071\n2.133\n0.251\n0.731\n0.028\n1.156\n0.059\n0.912\n0.067\n13.350\n0.157\n0.797\n+ Beam Search (B=5)\n0.012\n2.250\n0.018\n0.970\n0.070\n2.130\n0.250\n0.732\n0.027\n1.150\n0.058\n0.913\n0.066\n13.340\n0.156\n0.798\nNucleus Sampling (T=0.5)\n0.008\n2.098\n0.013\n0.976\n0.051\n2.194\n0.202\n0.778\n0.022\n0.483\n0.043\n0.923\n0.072\n6.153\n0.153\n0.785\nNucleus Sampling (T=1.0)\n0.009\n2.214\n0.015\n0.975\n0.067\n2.083\n0.244\n0.742\n0.025\n0.714\n0.053\n0.917\n0.069\n7.100\n0.161\n0.786\n+ Beam-Search (B=5)\n0.027\n2.792\n0.034\n0.948\n0.068\n3.449\n0.244\n0.742\n0.027\n1.028\n0.065\n0.913\n0.089\n11.278\n0.195\n0.766\n+ Beam-Search (B=10)\n0.031\n3.093\n0.040\n0.943\n0.072\n3.537\n0.251\n0.742\n0.028\n1.173\n0.071\n0.910\n0.079\n10.052\n0.175\n0.762\nTable 8. Ablation study on sampling strategies. We experiment using greedy decoding and add beam search with B=5. We test nucleus\nsampling [31] using top p=0.9, with temperatures T=0.5 and T=1.0. The two final rows describe the use of beam search with nucleus\nsampling at T=1.0. See huggingface.co/blog/how-to-generate for reference on these sampling techniques.\nFigure 10. Ablation study on sampling temperature. We test our methods\u2019 performance impact when changing the sampling temperature.\nResults are computed for SVG-Emoji and SVG-Icons validation sets.\nFigure 11. Ablation study on SVG-Icons full test set\nmend nucleus sampling [31] with top p=0.9 and tempera-\nture between 0.5 and 0.9 for the best performance.\nAssuring SVG Compilation. A common problem when\ngenerating SVGs with our approach is that sometimes the\nmaximum token length of the generated SVG might not be\nsufficient for completing the code, and compilation errors\nmay appear. We find that only training with SVG code that\nfits in the context length allows for obtaining around 85%\nof compilation success. The remaining incomplete sam-\nples are post-processed with cairosvg in order to obtain\na complete and compilable SVG. Nevertheless, some parts\nof the image are lost in some cases. In future work, we\nwill explore techniques to address this issue, as some other\nworks have introduced [6].\nFigure 12. Ablation study on SVG-Fonts full test set\nFigure 13. Baseline predictions on SVG-Fonts simplified test set\n13. Additional results\nWe facilitate some additional experiments and ablations\nperformed with StarVector.\n4\n13.1. Additional SVG samples generated\nFigures [14 - 19] show substantial qualitative samples gen-\nerated by StarVector on all the proposed datasets. All results\nare computed in the test sets. We can observe the weak-\nnesses and strengths of our model. Simplified datasets (Fig-\nures 15 and 17) are near-perfectly converted to SVG, how-\never, sometimes the model runs out of SVG code tokens and\nthe image is incomplete. Sometimes the model generates\nunnecessary loops, making the ability to stop earlier. Re-\nsults on SVG-Emoji 16 show impressive performance in es-\ntimating the shape\u2019s color and semantics. However, it lacks\nfine-grained and accurate positioning of objects.\nThese\nproblems result from the lack of sufficient emoji samples.\nWe consider that this problem can be alleviated by scaling\non model parameters, data, and computing resources. Re-\nsults on SVG-Fonts 14 are notably good, as StarVector is\nable to represent detailed high-frequency patterns (see \u201dA\u201d\nexample from row 7, column 1). The good performance\ncomes from the large data regime on this dataset.\n13.2. Comparison to Baselines\nHere we discuss the results on each baseline individually,\ncomparing it to our proposed approach.\nDeepSVG\nDeepSVG [13] is an elegant approach to learn-\ning a latent variable model for SVG, and it proves effective\nat learning the task for the simplified datasets (see Figure 3\nand 13). Results show that it is able to represent accurately\ncorners and edges in all datasets. Nevertheless, the limi-\ntation in the complexity of SVGs restricts it from being a\nsuitable solution in real applications.\nIm2Vec\nWe find it difficult to fit the Im2Vec [61] model to\nour data. When reproducing the proposed training setup in\nour datasets, the model does not progress in training. Their\nproposed training procedure does not require having SVG\nground truth and uses only pixel-based reconstruction loss.\nIt requires a differentiable rasterizer like DiffVG [41] to find\nthe optimal SVG parameters. This framework is very ap-\npealing, as it aims to be used in images with no SVG super-\nvision. However, it requires hundreds of epochs with a re-\nduced dataset, to overfit the model to those examples, only\nworking on modeling training examples, as seen in [61].\nThis training objective makes it difficult to find good so-\nlutions, especially on large SVG datasets like the ones in\nSVGBench, which Im2Vec is not able to handle.\nGPT-4 Vision\nGPT-4 Vision (GPT-4V) does a great job at\ncapturing the semantics of the image. It also does very well\nat capturing the colors of the input image and representing\nthem in the SVG code (see Figure 20). However, it is not\nable to compete in terms of reconstruction fidelity. GPT-4V\nwas not trained for the task of reconstruction, hence these\nresult is expected.\nVTracer\nVTracer outperforms our model in multiple set-\ntings.\nIt works especially well when the image can be\neasily segmented into clear regions based on color or tex-\nture. However, it tends to fail on images that contain high-\nfrequency patterns, like vector images with white back-\ngrounds and small polygon shapes close to each other with\ndifferent colors. For instance, examples from rows 2 and 3\nin Figure 6 in VTracer show how it is not able to precisely\nvectorize small details of shapes, letters, or color change.\nWhen the image contains several lines that are close to-\ngether in fine-grained patterns, the details tend to be lost\n(see the example in Figure 3 row 3 column 3). VTracer is\nalso limited by the primitives it is able to generate, restricted\nonly to vector paths. Some capabilities like optical charac-\nter recognition and the conversion of text rendered using the\n<text> tag can not be attained by rule-based models.\nTherefore, seeing the complete set of results, we con-\nsider that the StarVector approach is the only deep learning\nmethod on image-to-SVG that can obtain comparable re-\nsults to VTracer. Further, StarVector opens novel research\navenues for generating vector graphics, that could support\nuseful problems such as text-to-SVG or improved editing\nand understanding.\n5\nFigure 14. Generated samples from SVG-Fonts test.\n6\nFigure 15. Generated samples from SVG-Fonts simplified test.\n7\nFigure 16. Generated samples from SVG-Emoji test.\n8\nFigure 17. Generated samples from SVG-Icons test.\n9\nFigure 18. Generated samples from SVG-Stack test (i).\n10\nFigure 19. Generated samples from SVG-Stack test (ii).\n11\nFigure 20. Baseline comparison of generated examples on SVG-Emoji test set\nFigure 21. Baseline comparison of generated examples on SVG-Stack test set\n12\n"
  },
  {
    "title": "3D-LFM: Lifting Foundation Model",
    "link": "https://arxiv.org/pdf/2312.11894.pdf",
    "upvote": "13",
    "text": "3D-LFM: Lifting Foundation Model\nMosam Dabhi1\nL\u00b4aszl\u00b4o A. Jeni1*\nSimon Lucey2*\n1Carnegie Mellon University\n2The University of Adelaide\n3dlfm.github.io\n3D-LFM\n(a) Unified 2D-3D lifting for 30+ categories.\n(b) Dataset diversity visualization.\nFigure 1. Overview: (a) This figure shows the 3D-LFM\u2019s ability in lifting 2D landmarks into 3D structures across an array of over 30\ndiverse categories, from human body parts, to a plethora of animals and everyday common objects. The lower portion shows the actual 3D\nreconstructions by our model, with red lines representing the ground truth and blue lines showing the 3D-LFM\u2019s predictions. (b) This figure\ndisplays the model\u2019s training data distribution on a logarithmic scale, highlighting that inspite of 3D-LFM being trained on imbalanced\ndatasets, it preserves the performance across individual categories.\nAbstract\nThe lifting of 3D structure and camera from 2D land-\nmarks is at the cornerstone of the entire discipline of com-\nputer vision. Traditional methods have been confined to\nspecific rigid objects, such as those in Perspective-n-Point\n(PnP) problems, but deep learning has expanded our ca-\npability to reconstruct a wide range of object classes (e.g.\nC3PDO [18] and PAUL [24]) with resilience to noise, oc-\nclusions, and perspective distortions. All these techniques,\nhowever, have been limited by the fundamental need to es-\ntablish correspondences across the 3D training data \u2013 sig-\nnificantly limiting their utility to applications where one has\nan abundance of \u201cin-correspondence\u201d 3D data. Our ap-\nproach harnesses the inherent permutation equivariance of\ntransformers to manage varying number of points per 3D\ndata instance, withstands occlusions, and generalizes to un-\n*Both authors advised equally.\nseen categories. We demonstrate state of the art perfor-\nmance across 2D-3D lifting task benchmarks. Since our\napproach can be trained across such a broad class of struc-\ntures we refer to it simply as a 3D Lifting Foundation Model\n(3D-LFM) -\u2013 the first of its kind.\n1. Introduction\nLifting 2D landmarks from a single-view RGB image into\n3D has long posed a complex challenge in the field of com-\nputer vision because of the ill-posed nature of the problem.\nThis task is important for a range of applications from aug-\nmented reality to robotics, and requires an understanding\nof non-rigid spatial geometry and accurate object descrip-\ntions [2, 11, 25]. Historically, efforts in single-frame 2D-3D\nlifting have encountered significant hurdles: a reliance on\nobject-specific models, poor scalability, and limited adapt-\nability to diverse and complex object categories. Traditional\nmethods, while advancing in specific domains like human\n1\narXiv:2312.11894v1  [cs.CV]  19 Dec 2023\nbody [14, 16, 31] or hand modeling [3, 6], often fail when\nfaced with the complexities of varying object types or object\nrigs (skeleton placements).\nTo facilitate such single-frame 2D-3D lifting, deep learn-\ning methods like C3DPO [18] and others [8, 11, 24, 25, 28]\nhave been recently developed. However, these methods are\nfundamentally limited in that they must have knowledge of\nthe object category and how the 2D landmarks correspond\nsemantically to the 2D/3D data it was trained upon. Fur-\nther, this represents a drawback, especially when consider-\ning their scaling up to dozens or even hundreds of object\ncategories, with varying numbers of landmarks and config-\nurations. This paper marks a departure from such corre-\nspondence constraints, introducing the 3D Lifting Founda-\ntion Model (3D-LFM), an object-agnostic single frame 2D-\n3D lifting approach. At its core, the 3D-LFM addresses the\nlimitation of previous models, which is the inability to effi-\nciently handle a wide array of object categories while main-\ntaining high fidelity in 3D keypoint lifting from 2D data.\nWe propose a solution rooted in the concept of permuta-\ntion equivariance, a property that allows our model to au-\ntonomously establish correspondences among diverse sets\nof input 2D keypoints.\n3D-LFM is able to carry out single-frame 2D-3D lift-\ning for 30+ categories using a single model simultane-\nously, covering everything from human forms [9, 15, 32],\nface [29], hands [17], and animal species [1, 10, 27], to a\nplethora of inanimate objects found in everyday scenarios\nsuch as cars, furniture, etc. [26]. 3D-LFM is able to achieve\n2D-3D lifting performance that matches those of leading\nmethods specifically optimized for individual categories.\n3D LFM\u2019s generalizability is further evident in its ability\nto handle out-of-distribution (OOD) object categories and\nrigs, which we refer as OOD 2D-3D lifting, where the task\nis to lift the 2D landmarks to 3D for a category never seen\nduring training. We show such OOD results: (1) for inani-\nmate objects - by holding out an object category within the\nPASCAL dataset, (2) for animals - by training on common\nobject categories such as dogs and cats found in [27] and\nreconstructing 3D for unseen and rare species of Cheetahs\nfound in [10] and in-the-wild zoo captures from [5], and (3)\nby showing rig transfer, i.e. training 2D to 3D lifting on a\nHuman3.6M dataset rig [7] and showing similar 2D to 3D\nlifting performance on previously unseen rigs such as those\nfound in Panoptic studio dataaset rig [9] or a COCO dataset\nrig [13]. 3D-LFM transfers learnings from seen data dur-\ning training to unseen OOD data during inference. It does\nso by learning general structural features during the training\nphase via the proposed permutation equivariance properties\nand specific design choices that we discuss in the following\nsections.\nRecognizing the important role that geometry plays in\n3D reconstruction [4, 5, 11, 18, 24, 25], we integrate Pro-\ncrustean methods such as Orthographic-N-Point (OnP) or\nPerspective-N-Point (PnP) to direct the model\u2019s focus on\ndeformable aspects within a canonical frame. This incorpo-\nration significantly reduces the computational onus on the\nmodel, freeing it from learning redundant rigid rotations\nand focusing its capabilities on capturing the true geomet-\nric essence of objects. Scalability, a critical aspect of our\nmodel, is addressed through the use of tokenized positional\nencoding (TPE), which when combined with graph-based\ntransformer architecture, not only enhances the model\u2019s\nadaptability across diverse categories but also strengthens\nits ability to handle multiple categories with different num-\nber of keypoints and configurations. Moreover, the use of\nskeleton information (joint connectivity) within the graph-\nbased transformers via adjacency matrices provides strong\nclues about joint proximity and inherent connectivity, aid-\ning in the handling of correspondences across varied object\ncategories.\nTo the best of our knowledge, 3D-LFM is one of the only\nknown work which is a unified model capable of doing 2D-\n3D lifting for 30+ (and potentially even more) categories si-\nmultaneously. Its ability to perform unified learning across\na vast spectrum of object categories without specific object\ninformation and its handling of OOD scenarios highlight its\npotential as one of the first models capable of serving as a\n2D-3D lifting foundation model.\nThe contributions of this paper are threefold:\n1. We propose a Procrustean transformer that is able to fo-\ncus solely on learning the deformable aspects of objects\nwithin a single canonical frame whilst preserving per-\nmutation equivariance across 2D landmarks.\n2. The integration of tokenized positional encoding within\nthe transformer, to enhance our approach\u2019s scalabil-\nity and its capacity to handle diverse and imbalanced\ndatasets.\n3. We demonstrate that 3D-LFM surpasses state-of-the-art\nmethods in categories like humans, hands, and faces\n(benchmark in [32]). Additionally, it shows robust gen-\neralization by handling previously unseen objects and\nconfigurations, including animals ([5, 10]), inanimate\nobjects ([26]), and novel object arrangements (rig trans-\nfer in [9])\nIn subsequent sections, we explore the design and\nmethodology of our proposed 3D-LFM architecture, includ-\ning detailed ablation experiments and comparative analy-\nses. Throughout this paper, \u2019keypoints,\u2019 \u2019landmarks,\u2019 and\n\u2019joints\u2019 are used interchangeably, referring to specific, iden-\ntifiable points or locations on an object or figure that are\ncrucial for understanding its structure and geometry.\n2. Related works\nThe field of 2D-3D lifting has evolved substantially from\nclassic works such as the Perspective-n-Point (PnP) al-\n2\nAnalytical \nTPE\nGraph \nTransformer\nDecoder \nMLP\nMSE\nx L\n\u00d7 \u211d3\u00d73\n\u22ee\n\u22ee\n\u22ee\n\u22ee\nNorm\nNorm\nNorm\n\u22ee\nGraph \nattention\nMulti-Head \nAttention\nGraph Transformer\n(canonical frame)\nOutput 3D\nOutput 3D\n(ref. frame)\nG.T. 3D\n(ref. frame)\n2\n2\nInput 2D\n3D-LFM\n\u211dN\u00d72\n\u211dN\u00d7D\n\u211dN\u00d7D\n\u211dN\u00d73\nMLP\nFigure 2. Overview of the 3D Lifting Foundation Model (3D-LFM) architecture: The process begins with the input 2D keypoints\nundergoing Token Positional Encoding (TPE) before being processed by a series of Graph-based Transformer layers. The resulting features\nare then decoded through an MLP into a canonical 3D shape. This shape is aligned to the ground truth (G.T. 3D) in the reference frame\nusing a Procrustean method, with the Mean Squared Error (MSE) loss computed to guide the learning. The architecture captures both local\nand global contextual information, focusing on deformable structures while minimizing computational complexity.\ngorithm [12].\nIn these early works, the algorithm was\ngiven a set of 2D landmarks and some 3D supervision \u2013\nnamely the known 3D rigid object. The field has since wit-\nnessed a paradigm shift with the introduction of deep learn-\ning methodologies, led by methods such as C3DPO [18],\nPAUL [24], and Deep NRSfM [11], along with recent\ntransformer-based innovations like NRSfMFormer [8]. In\nthese approaches one does not need knowledge of the spe-\ncific 3D object, instead it can get away with just the 2D\nlandmarks and correspondences to an ensemble of 2D/3D\ndata from the object category to be lifted. However, despite\ntheir recent success, all these methods still require that the\n2D/3D data be in semantic correspondence. That is the in-\ndex to a specific landmark has the same semantic meaning\nacross all instances (e.g. chair leg). In practice, this is quite\nlimiting at run-time, as one needs intimate knowledge of\nthe object category, and rig in order to apply any of these\ncurrent methods. Further, this dramatically limits the abil-\nity of these methods to leverage cross-object and cross-rig\ndatasets, prohibiting the construction of a truly generaliz-\nable 2D to 3D foundation lifting model \u2013 a topic of central\nfocus in this paper.\nRecent literature in pose estimation, loosely connected\nto NRSfM but often more specialized towards human and\nanimal body part, has also seen remarkable progress. Mod-\nels such as Jointformer [14] and SimpleBaseline [16] have\nrefined the single-frame 2D-3D lifting process, while gen-\nerative approaches like MotionCLIP [19] and Human Mo-\ntion Diffusion Model [20] have laid the groundwork for\n3D generative motion-based foundation models. These ap-\nproaches, however, are even more limiting than C3PDO,\nPAUL, etc. in that they are intimately wedded to the object\nclass and are not easily extendable to an arbitrary objects\nclass.\n3. Approach\nGiven a set of 2D keypoints representing the projection of\nan object\u2019s joints in an image, we denote the keypoints ma-\ntrix as W \u2208 RN\u00d72, where N is the predetermined max-\nimum number of joints considered across all object cate-\ngories. For objects with joints count less than N, we in-\ntroduce a masking mechanism that utilizes a binary mask\nmatrix M \u2208 {0, 1}N, where each element mi of M is de-\nfined as:\nmi =\n(\n1\nif joint i is present\n0\notherwise\n(1)\nThe 3D lifting function f : RN\u00d72 \u2192 RN\u00d73 maps the 2D\nkeypoints to their corresponding 3D structure while com-\npensating for the orthographic projection:\nS = f(W) = WP\u22a4 + b\n(2)\nwhere P \u2208 R3\u00d72 is the orthographic projection matrix\n3\nand b \u2208 RN\u00d73 is a bias term that aligns the centroids of 2D\nand 3D keypoints.\nPermutation Equivariance:\nTo ensure scalability and\nadaptability across a diverse set of objects, we leverage\nthe property of permutation equivariance inherent in trans-\nformer architectures. Permutation equivariance allows the\nmodel to process input keypoints W regardless of their or-\nder, a critical feature for handling objects with varying joint\nconfigurations:\nf(PW) = Pf(W)\nwhere P is a permutation matrix that reorders the key-\npoints.\nHandling Missing Data: To address the challenge of miss-\ning data, we refer the Deep NRSfM++ [25] work and use a\nmasking mechanism to accommodate for occlusions or ab-\nsences of keypoints. Our binary mask matrix M \u2208 {0, 1}N\nis applied in such a way that it not only pads the input data\nto a consistent size but also masks out missing or occluded\npoints: Wm = W \u2299 M, where \u2299 denotes element-wise\nmultiplication. To remove the effects of translation and en-\nsure that our TPE features are generalizable, we zero-center\nthe data by subtracting the mean of the visible keypoints:\nWc = Wm \u2212 mean(Wm)\n(3)\nWe scale the zero-centered data to the range [\u22121, 1]\nwhile preserving the aspect ratio to maintain the geomet-\nric integrity of the keypoints. For more details on handling\nmissing data in the presence of perspective effects, we refer\nthe reader to Deep NRSFM++[25].\nToken Positional Encoding: replaces the traditional Corre-\nspondence Positional Encoding (CPE) or Joint Embedding\nwhich encodes the semantic correspondence information\n(as used in works such as like [14, 31]) with a mechanism\nthat does not require explicit correspondence or semantic\ninformation. Owing to the success of per-point positional\nembedding, particularly random Fourier features [30] in\nhandling OOD data, we compute Token Positional Encod-\ning (TPE) using analytical Random Fourier features (RFF)\nas follows:\nTPE(Wc) =\nr\n2\nD\nh\nsin(Wc\u03c9 + b); cos(Wc\u03c9 + b)\ni\n(4)\nwhere D is the dimensionality of the Fourier feature\nspace, \u03c9 \u2208 R2\u00d7 D\n2 and b \u2208 R\nD\n2 are parameters sampled\nfrom a normal distribution, scaled appropriately. These pa-\nrameters are sampled once and kept fixed, as per the RFF\nmethodology. The output of this transformation TPE(Wc)\nis then fed into the Graph Transformer network as X\u2113 where\n\u2113 indicates the layer number (0 in the above case). This set\nof features is now ready for processing inside the graph-\nbased transformer layers without the need for correspon-\ndence among the input keypoints. The TPE retains the per-\nmutation equivariance property while implicitly encoding\nthe relative positions of the keypoints.\n3.1. Graph-based Transformer Architecture\nOur Graph-based Transformer architecture utilizes a hybrid\napproach to feature aggregation by combining graph-based\nlocal attention [22](L) with global self-attention mecha-\nnisms [21](G) within a single layer (shown as grey block\nin Fig. 2. This layer is replicated L times, providing a se-\nquential refinement of the feature representation across the\nnetwork\u2019s depth.\nHybrid Feature Aggregation: For each layer \u2113, with \u2113\nranging from 0 to L, the feature matrix X(\u2113) \u2208 RN\u00d7D is\naugmented through simultaneous local and global process-\ning:\nL(\u2113) = GA(X(\u2113), A),\nG(\u2113) = MHSA(X(\u2113))\n(5)\nLocal and global features are concatenated to form a uni-\nfied representation U(\u2113):\nU(\u2113) = concat(L(\u2113), G(\u2113))\n(6)\nFollowing the concatenation, each layer applies a nor-\nmalization(LN) and a multilayer perceptron (MLP). The\nMLP employs a Gaussian Error Linear Unit (GeLU) as\nthe nonlinearity function to enhance the model\u2019s expressive\npower\nX\u2032(\u2113) = LN(U(\u2113)) + U(\u2113),\nX(\u2113+1) = LN(MLP GeLU(X\u2032(\u2113))) + X\u2032(\u2113)\n(7)\nHere, GA represents Graph Attention, MHSA denotes\nMulti-Head Self-Attention, and MLP GeLU indicates our\nMLP with GeLU nonlinearity. This architecture is designed\nto learn complex patterns by considering both the local\nneighborhood connectivity of input 2D and the global data\ncontext of input 2D, which is important for robust 2D to 3D\nstructure lifting.\n3.2. Procrustean Alignment\nThe final operation in our pipeline decodes the latent feature\nrepresentation X(L) into the predicted canonical structure\nSc via a GeLU-activated MLP:\nSc = MLPshape decoder(X(L))\nSubsequently, we align Sc with the ground truth Sr, via a\nProcrustean alignment method that optimizes for the rota-\ntion matrix R. The alignment is formalized as a minimiza-\ntion problem:\n4\nminimize\nR\n||M \u2299 (Sr \u2212 ScR)||2\nF\nwhere M is a binary mask applied element-wise, and || \u00b7 ||F\ndenotes the Frobenius norm. The optimal R is obtained via\nSVD, which ensures the orthonormality constraint of the\nrotation matrix:\nU, \u03a3, V\u22a4 = SVD((M \u2299 Sc)\u22a4Sr)\nR = UV\u22a4\nThe predicted shape is then scaled relative to the reference\nshape Sr, resulting in a scale factor \u03b3, which yields the final\npredicted shape Sp:\nSp = \u03b3 \u00b7 (ScR)\nThis Procrustean alignment step is crucial for directing the\nmodel\u2019s focus on learning non-rigid shape deformations\nover rigid body dynamics, thus significantly enhancing the\nmodel\u2019s ability to capture the true geometric essence of ob-\njects by just focusing on core deformable (non-rigid) as-\npects. The effectiveness of this approach is confirmed by\nfaster convergence and reduced error rates in our experi-\nments, as detailed in Fig. 6. These findings align with the\nfindings presented in PAUL [24].\n3.3. Loss Function\nThe optimization of our model relies on the Mean Squared\nError (MSE) loss, which calculates the difference between\npredicted 3D points Sp and the ground truth Sr:\nLMSE = 1\nN\nN\nX\ni=1\n\u2225S(i)\np \u2212 S(i)\nr \u22252\n(8)\nMinimizing this loss across N points ensures the model\u2019s\nability in reconstructing accurate 3D shapes from input 2D\nlandmarks.\nThis minimization effectively calibrates the\nshape decoder and the Procrustean alignment to focus on\nthe essential non-rigid characteristics of the objects, help-\ning the accuracy of the 2D to 3D lifting process.\n4. Results and Comparative Analysis\nOur evaluation shows the 3D Lifting Foundation Model\n(3D-LFM)\u2019s capability in single-frame 2D-3D lifting across\ndiverse object categories without object-specific data in\nSec. 4.1.\nFollowing that, Sec. 4.2 highlights 3D-LFM\u2019s\nperformance over specialized methods, especially in whole-\nbody benchmarks (referenced in [32]), showcasing adapt-\nability across varied categories like the human body, face,\nand hands (Fig.4).\nAdditionally, Section4.3 shows 3D-\nLFM\u2019s capability in 2D-3D lifting across 30 categories us-\ning a single unified model, enhancing category-specific per-\nformance and achieving out-of-distribution (OOD) general-\nization for unseen object configurations during training.\nConcluding, ablation studies in Section 4.4 validates our\nproposed OnP approach, token positional encoding, and the\nhybrid-attention mechanism in the transformer model, con-\nfirming their role in 3D-LFM\u2019s effectiveness in both single\nand multiple-object scenarios.\n4.1. Multi-Object 3D Reconstruction\nExperiment Rationale:\nThe 3D-LFM is designed to\nprocess diverse sets of 2D keypoints and lift them into\n3D across multiple object categories without relying on\ncategory-specific information. Its key strength lies in per-\nmutation equivariance, which maintains consistent perfor-\nmance regardless of keypoint order\u2014this is critical when\nhandling multiple objects with different numbers of key-\npoints. Unlike methods that depend on fixed-dimension ar-\nrays to manage object information, our model adapts flexi-\nbly to the data at hand. It has been evaluated against non-\nrigid structure-from-motion approaches [11, 18, 24, 25] that\nrequire object-specific inputs, showing its ability to handle\ndiverse categories. For a comprehensive benchmark, we uti-\nlize the PASCAL3D+ dataset [26], following C3DPO\u2019s [18]\nmethodology, to include a variety of object categories.\nPerformance:\nWe benchmark our 3D-LFM against\nC3DPO [18], a leading method in the NRSfM domain,\nto evaluate its performance in multi-object 2D to 3D lift-\ning tasks, with 3D supervision. While other recent meth-\nods [11, 24, 25, 28] also require object-specific details,\nC3DPO\u2019s approach to multiple object categories makes it a\nsuitable comparison for our model. Initially, we replicate\nconditions where object-specific information is provided,\nresulting in comparable performance between 3D-LFM and\nC3DPO, evident in Fig. 3. This stage involves using the\nMean-per-joint-position-error (MPJPE) to measure 2D-3D\nlifting accuracy, with C3DPO\u2019s training setup including an\nMN dimensional array for object details where M repre-\nsents number of objects with N being maximum number\nof keypoints, and our model is trained separately on each\nobject to avoid avoid providng object-specific information.\nThe 3D-LFM\u2019s strength is most apparent when object-\nspecific data is withheld. Unlike C3DPO, which experi-\nences performance drops without object details, our model\nsustains a lower MPJPE, even when trained collectively\nacross categories using only an N dimensional array. The\nresults (Fig.3 and Tab.1) highlight 3D-LFM\u2019s robustness\nand superior adaptability, outperforming single-category\ntraining and demonstrating its potential as a generalized 2D\nto 3D lifting solution.\n4.2. Benchmark: Object-Specific Models\nWe benchmark our 3D Lifting Foundation Model (3D-\nLFM) against leading specialized methods for human body,\nface, and hands categories. Our model outperforms these\nspecialized methods, showcasing its broad utility without\n5\nFigure 3. This figure shows the MPJPE for 3D-LFM and C3DPO\nusing the PASCAL3D+ dataset, comparing performance with and\nwithout object-specific information. Both methods perform com-\nparably when object-specific information is provided. However,\nabsent this data, C3DPO\u2019s error increases markedly, whereas 3D-\nLFM\u2019s remains low, showcasing its robustness and adaptability\nacross varied object categories without relying on object-specific\ninformation.\nTable 1. Quantitative Comparison of Multi-Object 2D-3D Lifting\nPerformance\nMethod\nObject-specific MPJPE (avg)\nC3DPO [18]\nYes\n7.5\n3D-LFM (Ours)\nYes\n3.97\nC3DPO [18]\nNo\n41.08\n3D-LFM (Ours)\nNo\n3.27\nthe need for object-specific tailoring, highlighting the versa-\ntility of its object-agnostic architecture. For our evaluation,\nwe utilize the H3WB dataset [32], a recent benchmark for\ndiverse whole-body pose estimation tasks. This dataset is\nespecially valuable for its inclusion of multiple object cat-\negories and for providing a comparative baseline against\nmethods like Jointformer [14], SimpleBaseline [16], and\nCanonPose [23]. We followed the H3WB\u2019s recommended\n5-fold cross-validation and engaged with the benchmark\u2019s\nauthors to obtain results on the hidden test set. Our re-\nported metrics in Fig. 4 and Table 2 include PA-MPJPE and\nMPJPE, with the test set performance numbers provided di-\nrectly by the H3WB team, ensuring that our superior results\nare verified by an independent third-party.\nPerformance: Our 3D-LFM demonstrates a notable en-\nhancement in accuracy over baseline methods, as outlined\nin the referenced figure and table. It excels across all cat-\negories, including whole-body, body, face, and hands, as\nshown in Fig.4 and detailed in Tab.2. With a lean archi-\ntecture of only 5 million parameters\u2014significantly fewer\nthan comparative baseline approaches, our model achieves\nFigure 4.\nPerformance Comparison on H3WB Benchmark:\nThis chart contrasts MPJPE errors for whole-body, body, face,\naligned face, hand, and aligned hand categories within the H3WB\nbenchmark [32]. Our models, with and without Procrustes Align-\nment (Ours-PA), outperform current state-of-the-art (SOTA) meth-\nods, validating our approach\u2019s proficiency in 2D to 3D lifting\ntasks.\nTable 2. Performance evaluation of 3D pose estimation models\non H3WB and validation datasets showing MPJPE in millimeters.\nOur method demonstrates leading accuracy across multiple object\ncategories without the need for object-specific designs.\nMethod\nWhole-body\nBody\nFace/Aligned\nHand/Aligned\nSimpleBaseline\n125.4\n125.7\n115.9 / 24.6\n140.7 / 42.5\nCanonPose w/3D sv.\n117.7\n117.5\n112.0 / 17.9\n126.9 / 38.3\nLarge SimpleBaseline\n112.3\n112.6\n110.6 / 14.6\n114.8 / 31.7\nJointformer (extra data)\n81.5\n78\n60.4 / 16.2\n117.6 / 38.8\nJointformer\n88.3\n84.9\n66.5 / 17.8\n125.3 / 43.7\nOurs\n64.13\n60.83\n56.55 / 10.44\n78.21 / 28.22\nOurs \u2013 PA\n33.13\n39.36\n6.02\n13.56\nrapid convergence and can be trained efficiently on a sin-\ngle NVIDIA A100 GPU within hours, emphasizing our\nmodel\u2019s efficiency and scalability evident by robust perfor-\nmance across varied object categories.\nThe results affirm 3D-LFM as a versatile and potent\nfoundational model for diverse 2D to 3D lifting tasks, out-\npacing specialized category-specific models. These find-\nings, showcased in Fig. 4, validate our model\u2019s ability to\nleverage inter-category learning, potentially paving the way\nfor broader applications across an expanded range of object\ncategories.\n4.3. Towards foundation model\nIn this section, we demonstrate the 3D Lifting Foundation\nModel (3D-LFM) as a versatile foundation for diverse 2D-\n3D lifting scenarios. Our model is capable of handling var-\nious object categories and navigating data imbalance chal-\nlenges. In this subsection, we explore the 3D-LFM\u2019s scal-\nability and its potential for combined dataset training in\nSec. 4.3.1, its proficiency in generalizing to unseen object\ncategories and its adaptability in transferring rig configura-\n6\ntions in Sec. 4.3.2. These studies validate the 3D-LFM\u2019s\nrole as a foundation model, capable at leveraging diverse\ndata without requiring specific configurations, thus simpli-\nfying the 3D lifting process for varied joint setups.\nWe show 3D-LFM\u2019s capability of handling 2D-3D lifting\nfor 30+ object categories within the single model, confirm-\ning the model\u2019s capability to manage imbalanced datasets\nrepresentative of real-world scenarios as shown in Fig. 1.\nWith a comprehensive range of human, hand, face, inani-\nmate objects, and animal datasets, the 3D-LFM is proven\nto be adaptable, not necessitating category-specific adjust-\nments. The subsequent subsections will dissect these at-\ntributes further, discussing the 3D-LFM\u2019s foundational po-\ntential in the 3D lifting domain.\n4.3.1\nCombined Dataset Training\nThis study evaluates the 3D-LFM\u2019s performance on iso-\nlated datasets against its performance on a comprehensive\ncombined dataset. Initially, the model was trained sepa-\nrately on animal-based supercategory datasets\u2014specifically\nOpenMonkey[1] and Animals3D[27].\nSubsequently, it\nwas trained on a merged dataset encompassing a broad\nspectrum of object categories, including Human Body-\nBased datasets like AMASS and Human 3.6 [7], Hands-\nBased datasets such as PanOptic Hands [9], Face-Based\ndatasets like BP4D+[29], and various Inanimate Objects\nfrom the PASCAL3D+ dataset[26], along with the previ-\nously mentioned animal datasets. Isolated training resulted\nin an average MPJPE of 21.22 mm, while the integrated\ntraining method notably reduced the MPJPE to 12.5 mm on\nthe same animal supercategory validation split. This sig-\nnificant improvement confirms the 3D-LFM\u2019s potential as a\npretraining framework and underscores its capacity to adapt\nto and generalize from diverse and extensive data collec-\ntions.\n4.3.2\nOOD generalization and rig-transfer:\nWe evaluated the 3D-LFM\u2019s generalization ability on object\ncategories and rig configurations unseen during training.\nThe model\u2019s proficiency is demonstrated through its accu-\nrate 3D reconstructions of the \u201cCheetah\u201d category from the\nAcinoset dataset [10]\u2014distinct from the commonly seen\nspecies in the Animal3D dataset [27].\nA similar gener-\nalization is observed when the model, trained without the\n\u201cTrain\u201d category from PASCAL3D+[26], accurately recon-\nstructs its 3D structure. These findings confirm the model\u2019s\nrobust out-of-distribution (OOD) generalization capability,\nas qualitatively illustrated in Fig. 5.\nAdditionally, the 3D-LFM showcased its capability in\ntransferring rig configurations between datasets. For ex-\nample, training on a 17-joint Human3.6M dataset [7] and\ntesting on a 15-joint Panoptic Studio dataset [9] led to\naccurate 3D reconstructions, despite the disparity in joint\nOOD \nInput 2D\nPredicted \n3D\nOOD \nInput 2D\nPredicted \n3D\nFigure 5. This figure illustrates 3D-LFM\u2019s proficiency in OOD\n2D-3D lifting, effectively handling new, unseen categories from\nAcinoset [10] PASCAL3D+ [26] with varying joint arrangements,\ndemonstrating its strong generalization capabilities.\nnumbers and arrangements. These results emphasize the\nfoundational model\u2019s adaptability, critical for processing di-\nverse human data sets with varying rig setups. For a more\nthorough validation, we direct readers to the ablation sec-\ntion, where extensive qualitative visualizations in Fig. 7 and\nquantitative reports in Sec. 4.4.3 further substantiate the 3D-\nLFM\u2019s OOD generalization and rig transfer efficacy.\n4.4. Ablation\nWe conduct ablation studies to dissect the 3D-LFM\u2019s de-\nsign choices and their respecitve contributions. The experi-\nments in Sec. 4.4.1 and Sec. 4.4.2 are carried out on the Hu-\nman3.6M benchmark [7] and a mixture of Animal3D [27],\nHuman3.6M, and face datasets [9, 29] are used in Sec. 4.4.3\nto emphasize the scalability and OOD generalization prop-\nerties of TPE.\n4.4.1\nProcrustean Transformation\nThe Procrustean approach within 3D-LFM refines learning\nto the deformable components of objects. Utilizing an OnP\nsolution described in Sec. 3.2, the model focuses on learn-\ning deformable shapes in the canonical space and ignoring\nrigid transformations. The faster convergence and reduced\nMPJPE error, evident by the gap between blue and orange\nlines in Fig. 6 (a) validates the benefits of Procrustean trans-\nformation, confirming its relevance in the architecture.\n4.4.2\nLocal-Global vs. Hybrid Attention\nOn the same validation split, our analysis indicates that\nmerging local (GA) and global attention (MHSA) leads to\nthe best performance. Isolating either attention mechanism\ndoes not match the performance of their combined use, as\nshown by the green and red lines in as shown in Fig. 6\n(a). The combination of GA with MHSA, especially when\naligned with OnP, delivers best results and faster conver-\ngence. These findings from our ablation studies validate the\n7\n10\n20\n30\n40\n50\nEpoch\n50\n75\n100\n125\n150\n175\n200\nMPJPE (mm)\nGlobal + Local\nGlobal only\nLocal only\nGlobal + Local (No OnP)\n20\n40\n60\n80\nEpoch\n60\n80\n100\n120\n140\n160\n180\n200\nMPJPE (mm)\nTPE\nMLP\nFigure 6. (a) Comparing attention strategies in 3D-LFM. The com-\nbined local-global approach with OnP alignment surpasses other\nconfigurations in MPJPE reduction over 100 epochs on the Hu-\nman3.6M validation split. (b) rapid convergence and efficiency of\nthe TPE approach compared to the learnable MLP\narchitectural decisions behind 3D-LFM.\nOur\nhybrid\nattention\nmechanism,\nwhich\nfuses\nconnectivity-aware\nlocal\ngraph\nattention\n(GA)\nwith\nthe broader scope of global multi-head self-attention\n(MHSA)\u2014emerges as a more optimal strategy. Sole reliance\non either GA or MHSA results in suboptimal performance as\nevidenced by green and red lines as compared to blue line\nin Fig. 6 (a). This hybrid attention, paired with Procrustes\n(OnP) alignment, not only minimizes MPJPE error but\nalso makes model convergence faster, reinforcing the\narchitectural decisions underlying the 3D-LFM.\n4.4.3\nTokenized Positional Encoding:\nThis ablation study covers the impact of Tokenized Posi-\ntional Encoding (TPE) in 3D-LFM, which leverages ana-\nlytical Random Fourier Features for positional information,\nretaining the permutation equivariance and scalability es-\nsential for a foundational model.\nData imbalance study: Our experiments on the Animal3D\ndataset [27], with a focus on the underrepresented hippo\nsupercategory, reveal TPE\u2019s efficacy in dealing with imbal-\nanced dataset. With a nominal model size of 128 dimen-\nsions, TPE improved MPJPE performance by 3.27% over\nthe learnable MLP approach, and this performance kept im-\nproving as we kept increasing the model size from 128 to\n512 and finally to 1024 - where the performance improve-\nment kept improving from 3.27% to 12.28%, and finally\nto 22.02% - highlighting its capability to handle data im-\nbalance with varying model sizes. More importantly, for all\nthese trials one important thing we noticed is the statistically\nsignifcant faster convergence shown by TPE over learnable\nMLP approach as evident in Fig. 6 (b).\nRig Transfer Generalization: Rig transfer experiments\nfurther solidify TPE\u2019s efficacy, with a 12% improvement\nwhen transferring from a 17-joint [7] to a 15-joint rig [9]\nand an improvement of 23.29% when we trained on 15-\njoint rig and tesed on 17-joint rig. Training on a 52-joint fa-\ncial rig from [9] and testing on an 83-joint from [29] yielded\nAnalytical \nTPE\nLearnable \nMLP\nFigure 7. The qualitative improvement in rig transfer using ana-\nlytical TPE versus learnable MLP projection. This visualization\nreinforces the necessity of TPE in handling OOD data such as dif-\nferent rigs, unseen during training.\na 52.3% improvement over the MLP baseline, validating\nTPE\u2019s robustness in complex OOD scenarios. Fig. 7 qual-\nitatively shows TPE\u2019s performance, where the our model\nwith TPE is able to generalize to unseen joint configura-\ntions. This ablation section touches upon both quantitative\nand visual studies of TPE and conveys that TPE is a pivotal\ncompoenent for scalable 2D-3D lifting task.\n5. Discussion and Conclusion\nThe proposed 3D-LFM marks a significant leap in 2D-3D\nlifting, showcasing scalability and adaptability, addressing\ndata imbalance, and generalizing to new data categories. Its\ncross-category knowledge transfer and rare shape variation\nhandling requires further investigation to enhance robust-\nness. 3D-LFM\u2019s efficiency is demonstrated by achieving\nresults comparable to leading methods on [32] benchmark\nas well as its proficiency in out-of-distribution (OOD) sce-\nnarios on limited computational resources. For training du-\nration and computational details, please refer to the supple-\nmentary materials. This work not only pushes the bound-\naries of current 2D-3D lifting, but also establishes a scal-\nable framework for future 3D pose estimation and 3D re-\nconstruction models. In summary, the 3D-LFM creates a\nflexible, universally applicable model for 3D reconstruction\nfrom 2D data, paving the way for diverse applications that\nrequires accurate 3D reconstructions from 2D inputs.\nAcknowledgement: We extend our gratitude to Ian R.\nFasel, Tim Clifford, Javier Movellan, Matthias Hernandez,\nMatthias Schroeder, and Akshay Subramanya of Apple for\ntheir insightful discussions.\n8\nSupplementary Material\nI. Training Details\nThe 3D Lifting Foundation Model (3D-LFM), as detailed\nin Sec. 4.3.1 was trained across more than 30 diverse cat-\negories on a single NVIDIA A100 GPU. This dataset con-\nsisted of over 18 million samples, with data heavily imbal-\nanced as shown in Fig. 1. This training setup highlights the\nmodel\u2019s practicality, with mixed datasets having imbalance\nwithin them. 3D LFM\u2019s transformer block, shown in lower\nhalf of Fig. 2 consists of 4 layers, with hidden dimensions\n(post TPE dimension) ranging from 64 to 512 and head\ncounts between 4 and 8. These parameters were adjusted\nbased on the dataset scale in various experiments of Sec. 4.\nGeLU activations were employed for non-linearity in the\nfeedforward layers. The training process was guided by a\nReduceLROnPlateau scheduler with a starting learning\nrate of 0.001 and a patience of 20 epochs. An early stop-\nping mechanism was implemented, halting training if no\nimprovement in MPJPE was noted for 30 epochs, ensuring\nefficient and optimal performance. This training approach\nenabled 3D-LFM to surpass leading methods in 3D lifting\ntask proposed by H3WB benchamark [32].\nExperimentation in Section 4.2: For the benchmark exper-\niment of H3WB [32], we adopted a 5-fold cross-validation\nas directed by the benchmark authors.\nWe trained two\nmodel variants: one with OnP solution (predicting shapes,\nSc in canonical frame) and one without the OnP solution\n(predicting shapes directly in the reference frame, Sp). We\naligned the canonical frame predictions from the OnP-based\nmodel to those from the non-OnP based model, which en-\nsured that the submitted 3D reconstructions have the rota-\ntion and scales adjusted to the predicted reference frame.\nII. Interactive Visualization with 3D-LFM\nThe 3dlfm visualization.ipynb notebook pro-\nvided on our project page provides an in-depth interactive\nexploration of the 3D-LFM\u2019s model predictions \u2013 3D struc-\ntures from input 2D landmarks. This interactive notebook\nshows the model\u2019s adaptability across standard validation\ndatasets (shown in Fig. 1) and its adaptability to challeng-\ning Out-Of-Distribution (OOD) scenarios, particularly from\nin-the-wild captures provided by the Acinoset datasets [10]\nand MBW datasets [5].\nII.1. Key Features\nCategory Selection: Enables selection from diverse ob-\nject categories, including standard and OOD. Data Man-\nagement: Automates the loading and visualization of 2D\ninputs, 3D ground truth, and predictions. Frame Explo-\nration: Facilitates detailed visualization of specific frames\nwith the option to overlay 2D input data on images and\nviewing its corresponding 3D predictions simultaneously.\nII.2. Usage and examples\nTo use the notebook, simply select an object category\nand frame number (automated within). It will display the\nmodel\u2019s 3D and 2D predictions, alongside available ground\ntruth data. For OOD categories, there\u2019s an option to include\npseudo ground truth labels, offering a deeper insight into\nthe model\u2019s performance in diverse scenarios.\nAs shown in Fig. 8, the 3D-LFM handles complex OOD\ndata, generating predictions in-the-wild with reasonable ac-\ncuracy. Unlike traditional approaches requiring extensive\nbootstrapping to generate labels for tail-end distribution cat-\negories, our 3D-LFM generates 3D predictions in a single\ninference step, emphasizing its foundation model capabil-\nities. These include in-the-wild reconstructions of various\nwildlife captured in-the-wild, as showcased in Fig. 8. This\nfigure, alongside additional visualizations within the note-\nbook shows that 3D-LFM is capable of handling multiple\ncategories within a single unified model. It particularly han-\ndles OOD scenarios reasonably well \u2013 providing accurate\n2D-3D lifting performance on in-the-wild captures from\ndatasets like Acinoset [10] and MBW [5]. These findings\nconfirm the 3D-LFM\u2019s ability to utilize its foundational at-\ntributes for accurate 2D-3D lifting, even across previously\nunseen categories.\nIII. Limitations\nCategory Misinterpretation: The 3D-LFM exhibits re-\nmarkable performance generalizing across object cate-\ngories. However, it can encounter difficulties when extreme\nperspective distortions cause 2D inputs to mimic the appear-\nance of different categories. For example, in the case of\na tiger viewed from an atypical angle, the model\u2019s output\nmay resemble the structure of a primate due to similar 2D\nkeypoint configurations induced by perspective effects, as\nshown in Fig. 9 (c). This confusion stems from the model\u2019s\nreliance on the geometric arrangement of keypoints, which\ncan be deceptive under extreme perspectives.\nDepth Ambiguity: Another challenge arises from the in-\nherent depth ambiguity in single-frame 2D to 3D lifting.\nThe model can misinterpret the spatial arrangement of\nlimbs, as evident by Fig. 9 (a), where a monkey\u2019s leg ap-\npears to extend backward rather than forward. Similarly,\nFig. 9 (b) displays an instance where the model perceives\nthe monkey\u2019s head orientation incorrectly. Such depth am-\nbiguities highlight the need for more sophisticated depth\ncues in single-view 3D reconstruction.\nWe propose integrating appearance cues, such as those\n9\nFigure 8. Example OOD inferences by 3D-LFM on MBW dataset [5]: The red dots overlaying the images indicate the input 2D\nkeypoints, while the blue stick figures below each image shows the predicted 3D structure viewed from two distinct camera viewing\nangles. Accompanying video results further illustrate these predictions with a dynamic 360-degree camera animation, providing a detailed\nview of the model\u2019s performance.\nprovided by DINOv2 features, to enhance depth perception\nand category differentiation. This would provide additional\ncontext to the geometric data, aiding in resolving ambigu-\nities, as demonstrated by the OOD examples in Fig. 9, de-\nrived from challenging in-the-wild captures [5]. This inte-\ngration aims to improve the fidelity of 3D reconstructions\nin scenarios with complex visual information.\nReferences\n[1] Praneet C Bala, Benjamin R Eisenreich, Seng Bum Michael\nYoo, Benjamin Y Hayden, Hyun Soo Park, and Jan Zimmer-\nmann. Openmonkeystudio: Automated markerless pose esti-\nmation in freely moving macaques. BioRxiv, pages 2020\u201301,\n2020. 2, 7\n[2] Christoph Bregler, Aaron Hertzmann, and Henning Bier-\nmann. Recovering non-rigid 3d shape from image streams.\nIn Proceedings IEEE Conference on Computer Vision and\nPattern Recognition. CVPR 2000 (Cat. No. PR00662), pages\n690\u2013696. IEEE, 2000. 1\n[3] Zheng Chen and Yi Sun. Joint-wise 2d to 3d lifting for hand\npose estimation from a single rgb image. Applied Intelli-\ngence, 53(6):6421\u20136431, 2023. 2\n[4] Mosam Dabhi, Chaoyang Wang, Kunal Saluja, L\u00b4aszl\u00b4o A\nJeni, Ian Fasel, and Simon Lucey. High fidelity 3d recon-\nstructions with limited physical views. In 2021 International\n(a)\n(b)\n(c)\nViewing \nangle #1\nViewing \nangle #2\nViewing \nangle #1\nViewing \nangle #2\nViewing \nangle #1\nViewing \nangle #2\nFigure 9. Challenges in Perspective and Depth Perception: (a)\nIncorrect leg orientation due to depth ambiguity in monkey cap-\nture. (b) Misinterpreted head position in a second monkey exam-\nple. (c) A tiger\u2019s keypoints distorted by perspective, leading to\nprimate-like 3D predictions.\u201d\nConference on 3D Vision (3DV), pages 1301\u20131311. IEEE,\n2021. 2\n[5] Mosam Dabhi, Chaoyang Wang, Tim Clifford, L\u00b4aszl\u00b4o Jeni,\nIan Fasel, and Simon Lucey. Mbw: Multi-view bootstrap-\nping in the wild. Advances in Neural Information Processing\nSystems, 35:3039\u20133051, 2022. 2, 9, 10\n10\n[6] Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying\nWang, Jianfei Cai, and Junsong Yuan. 3d hand shape and\npose estimation from a single rgb image. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10833\u201310842, 2019. 2\n[7] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3. 6m: Large scale datasets and pre-\ndictive methods for 3d human sensing in natural environ-\nments. IEEE transactions on pattern analysis and machine\nintelligence, 36(7):1325\u20131339, 2013. 2, 7, 8\n[8] Haorui Ji, Hui Deng, Yuchao Dai, and Hongdong Li. Unsu-\npervised 3d pose estimation with non-rigid structure-from-\nmotion modeling. arXiv preprint arXiv:2308.10705, 2023.\n2, 3\n[9] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,\nIain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser\nSheikh. Panoptic studio: A massively multiview system for\nsocial motion capture.\nIn Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 3334\u20133342,\n2015. 2, 7, 8\n[10] Daniel Joska, Liam Clark, Naoya Muramatsu, Ricardo\nJericevich, Fred Nicolls, Alexander Mathis, Mackenzie W\nMathis, and Amir Patel.\nAcinoset: a 3d pose estimation\ndataset and baseline models for cheetahs in the wild. In 2021\nIEEE international conference on robotics and automation\n(ICRA), pages 13901\u201313908. IEEE, 2021. 2, 7, 9\n[11] Chen Kong and Simon Lucey. Deep non-rigid structure from\nmotion. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 1558\u20131567, 2019. 1, 2,\n3, 5\n[12] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua.\nEp n p: An accurate o (n) solution to the p n p problem.\nInternational journal of computer vision, 81:155\u2013166, 2009.\n3\n[13] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 2\n[14] Sebastian\nLutz,\nRichard\nBlythman,\nKoustav\nGhosal,\nMatthew Moynihan, Ciaran Simms, and Aljosa Smolic.\nJointformer: Single-frame lifting transformer with error pre-\ndiction and refinement for 3d human pose estimation.\nIn\n2022 26th International Conference on Pattern Recognition\n(ICPR), pages 1156\u20131163. IEEE, 2022. 2, 3, 4, 6\n[15] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-\nard Pons-Moll, and Michael J Black.\nAmass:\nArchive\nof motion capture as surface shapes.\nIn Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 5442\u20135451, 2019. 2\n[16] Julieta Martinez, Rayat Hossain, Javier Romero, and James J\nLittle. A simple yet effective baseline for 3d human pose esti-\nmation. In Proceedings of the IEEE international conference\non computer vision, pages 2640\u20132649, 2017. 2, 3, 6\n[17] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori,\nand Kyoung Mu Lee. Interhand2. 6m: A dataset and base-\nline for 3d interacting hand pose estimation from a single\nrgb image.\nIn Computer Vision\u2013ECCV 2020: 16th Euro-\npean Conference, Glasgow, UK, August 23\u201328, 2020, Pro-\nceedings, Part XX 16, pages 548\u2013564. Springer, 2020. 2\n[18] David Novotny, Nikhila Ravi, Benjamin Graham, Natalia\nNeverova, and Andrea Vedaldi. C3dpo: Canonical 3d pose\nnetworks for non-rigid structure from motion. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 7688\u20137697, 2019. 1, 2, 3, 5, 6\n[19] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano,\nand Daniel Cohen-Or. Motionclip: Exposing human motion\ngeneration to clip space. In European Conference on Com-\nputer Vision, pages 358\u2013374. Springer, 2022. 3\n[20] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,\nDaniel Cohen-Or, and Amit H Bermano. Human motion dif-\nfusion model. arXiv preprint arXiv:2209.14916, 2022. 3\n[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 4\n[22] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Li`o, and Yoshua Bengio. Graph at-\ntention networks. In International Conference on Learning\nRepresentations, 2018. 4\n[23] Bastian Wandt, Marco Rudolph, Petrissa Zell, Helge Rhodin,\nand Bodo Rosenhahn. Canonpose: Self-supervised monoc-\nular 3d human pose estimation in the wild. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 13294\u201313304, 2021. 6\n[24] Chaoyang Wang and Simon Lucey.\nPaul:\nProcrustean\nautoencoder for unsupervised lifting.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 434\u2013443, 2021. 1, 2, 3, 5\n[25] Chaoyang Wang, Chen-Hsuan Lin, and Simon Lucey. Deep\nnrsfm++: Towards unsupervised 2d-3d lifting in the wild. In\n2020 International Conference on 3D Vision (3DV), pages\n12\u201322. IEEE, 2020. 1, 2, 4, 5\n[26] Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond\npascal: A benchmark for 3d object detection in the wild. In\nIEEE winter conference on applications of computer vision,\npages 75\u201382. IEEE, 2014. 2, 5, 7\n[27] Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen,\nPengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao\nWang, et al. Animal3d: A comprehensive dataset of 3d ani-\nmal pose and shape. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 9099\u20139109,\n2023. 2, 7, 8\n[28] Haitian Zeng, Xin Yu, Jiaxu Miao, and Yi Yang. Mhr-net:\nMultiple-hypothesis reconstruction of non-rigid shapes from\n2d views.\nIn European Conference on Computer Vision,\npages 1\u201317. Springer, 2022. 2, 5\n[29] Xing Zhang, Lijun Yin, Jeffrey F Cohn, Shaun Canavan,\nMichael Reale, Andy Horowitz, Peng Liu, and Jeffrey M Gi-\nrard. Bp4d-spontaneous: a high-resolution spontaneous 3d\ndynamic facial expression database. Image and Vision Com-\nputing, 32(10):692\u2013706, 2014. 2, 7, 8\n[30] Jianqiao Zheng, Xueqian Li, Sameera Ramasinghe, and Si-\nmon Lucey.\nRobust point cloud processing through posi-\n11\ntional embedding. arXiv preprint arXiv:2309.00339, 2023.\n4\n[31] Wentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu, Wayne\nWu, and Yizhou Wang. Motionbert: Unified pretraining for\nhuman motion analysis. arXiv preprint arXiv:2210.06551,\n2022. 2, 4\n[32] Yue Zhu, Nermin Samet, and David Picard. H3wb: Human3.\n6m 3d wholebody dataset and benchmark. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 20166\u201320177, 2023. 2, 5, 6, 8, 9\n12\n"
  },
  {
    "title": "HAAR: Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles",
    "link": "https://arxiv.org/pdf/2312.11666.pdf",
    "upvote": "12",
    "text": "Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles\nVanessa Sklyarova1,2\nEgor Zakharov2\nOtmar Hilliges2\nMichael J. Black1\nJustus Thies1,3\n1Max Planck Institute for Intelligent Systems\n2ETH Z\u00a8urich\n3Technical University of Darmstadt\n...afro hairstyle...\n...voluminous straight hair...\n...man haircut...\n...wavy short hairstyle...\n...bob haircut...\n...long wavy hairstyle...\n...long straight hair...\n...short curly hairstyle...\nFigure 1. Given a text description, our method produces realistic human hairstyles. The usage of a 3D strand-based geometry representation\nallows it to be easily incorporated into existing computer graphics pipelines for simulation and rendering [3, 7, 9].\nAbstract\nWe present HAAR, a new strand-based generative model\nfor 3D human hairstyles. Specifically, based on textual in-\nputs, HAAR produces 3D hairstyles that could be used as\nproduction-level assets in modern computer graphics en-\ngines. Current AI-based generative models take advantage\nof powerful 2D priors to reconstruct 3D content in the form\nof point clouds, meshes, or volumetric functions.\nHow-\never, by using the 2D priors, they are intrinsically limited\nto only recovering the visual parts. Highly occluded hair\nstructures can not be reconstructed with those methods, and\nthey only model the \u201couter shell\u201d, which is not ready to be\nused in physics-based rendering or simulation pipelines. In\ncontrast, we propose a first text-guided generative method\nthat uses 3D hair strands as an underlying representation.\nLeveraging 2D visual question-answering (VQA) systems,\nwe automatically annotate synthetic hair models that are\ngenerated from a small set of artist-created hairstyles. This\nallows us to train a latent diffusion model that operates in\na common hairstyle UV space. In qualitative and quantita-\ntive studies, we demonstrate the capabilities of the proposed\nmodel and compare it to existing hairstyle generation ap-\nproaches. For results, please refer to our project page\u2020.\n1. Introduction\nThere has been rapid progress in creating realistic, animat-\nable 3D face and head avatars from images, video, and text.\nWhat is still missing is hair. Existing methods typically\nrepresent hair with a coarse mesh geometry, implicit sur-\nfaces, or neural radiance fields. None of these representa-\ntions are compatible with the strand-based models used by\nexisting rendering systems and do not enable animation of\nthe resulting avatars with natural hair dynamics. Modeling\nand generating realistic 3D hair remains a key bottleneck\nto creating realistic, personalized avatars. We address this\nproblem with HAAR (Hair: Automatic Animatable Recon-\nstruction), which enables the generation of realistic and di-\nverse hairstyles based solely on text descriptions. HAAR is\n\u2020 https://haar.is.tue.mpg.de/\n1\narXiv:2312.11666v1  [cs.CV]  18 Dec 2023\nthe first text-driven generative model that produces a classi-\ncal strand-based hair representation that can be immediately\nimported into rendering systems and animated realistically.\nThis approach replaces the complex and time-consuming\nprocess of manually creating 3D hairstyles with a chat-like\ntext interface that can be used by a novice to create high-\nquality 3D hair assets.\nPrevious work exploits generative models as learned pri-\nors to create 3D strand-based hair from images, videos,\nor random noise.\nIn particular, Neural Haircut [47] re-\nconstructs high-fidelity hairstyles from smartphone video\ncaptures without any specialized equipment by leveraging\na pre-trained generative diffusion model. However, their\nstrand-based generative model does not provide control\nover the geometry of the resulting hairstyles, substantially\nlimiting the range of applications.\nRecently, GroomGen\n[57] introduced an unconditional generative model of hair.\nIn contrast, we propose the first text-conditioned generative\nmodel for strand-based hairstyles that can be used for auto-\nmated and fast hair asset generation.\nText-conditioned generative models like Stable Diffu-\nsion [42] are widely used for image and video gener-\nation and can be used to generate 3D shape from text\n[5, 6, 14, 25, 28, 29, 32, 38, 39, 48, 49, 54] by exploit-\ning Score Distillation Sampling (SDS) [38]. These meth-\nods convert textual descriptions into 3D assets that, when\nrendered into multiple views, align with generated 2D im-\nages via differentiable rendering. These methods represent\n3D shapes either as meshes [5, 25, 39], point clouds [6, 48]\nor volumes [28, 29, 32, 38, 49]. In particular, TECA [54]\ndemonstrates how hair can be generated from text using\na neural radiance field [34], combined with a traditional\nmesh-based head model [23]. However, the inherent prob-\nlem with these SDS-based solutions is that they only capture\nthe outer visible surface of the 3D shape. Even volumet-\nric representations do not have a meaningful internal hair\nstructure [54]. Thus, they can not be used for downstream\napplications like animation in graphics engines [3, 7].\nInstead, what we seek is a solution with the follow-\ning properties: (1) the hair is represented using classical\n3D strands so that the hairstyle is compatible with exist-\ning rendering tools, (2) hair is generated from easy-to-use\ntext prompts, (3) the generated hair covers a wide range of\ndiverse and realistic hairstyles, (4) the results are more real-\nistic than current generative models based SDS. To this end,\nwe develop a text-guided generation method that produces\nstrand-based hairstyles via a latent diffusion model. Specif-\nically, we devise a latent diffusion model following the un-\nconditional model used in Neural Haircut [47]. A hairstyle\nis represented on the scalp of a 3D head model as a tex-\nture map where the values of the texture map correspond to\nthe latent representation of 3D hair strands. The individual\nstrands are defined in a latent space of a VAE that captures\nthe geometric variation in the hair strand shape. To gener-\nate novel hair texture maps, we infer a diffusion network\nthat takes a noise input and text conditioning. From the\ngenerated hair texture map, we can sample individual latent\nstrands and reconstruct the corresponding 3D hair strands.\nThere are three remaining, interrelated, problems to ad-\ndress: (1) We need a dataset of 3D hairstyles to train the\nVAE and diffusion model. (2) We need training data of\nhairstyles with text descriptions to relate hairstyles to our\nrepresentation. (3) We need a method to condition gen-\nerated hair on text. We address each of these problems.\nFirst, we combine three different 3D hair datasets and aug-\nment the data to construct a training set of about 10K 3D\nhairstyles. Second, one of our key novelties is in how we\nobtain hairstyle descriptions.\nHere, we leverage a large\nvision-language model (VLM) [27] to generate hairstyle de-\nscriptions from images rendered from the 3D dataset. Un-\nfortunately, existing visual question-answering (VQA) sys-\ntems [22, 26, 27] are inaccurate and do not produce coher-\nent hairstyle descriptions. To address these problems, we\ndesign a custom data-annotation pipeline that uses a pre-\ngenerated set of prompts that we feed into a VQA sys-\ntem [26] and produce final annotations by combining their\nresponses in a single textual description. Finally, we train a\ndiffusion model to produce the hair texture encoding condi-\ntioned on the encoding of textual hairstyle descriptions.\nAs Figure 1 illustrates, our strand-based representation\ncan be used in classical computer graphics pipelines to real-\nistically densify and render the hair [3, 7, 9]. We also show\nhow the latent representation of hair can be leveraged to per-\nform various semantic manipulations, such as up-sampling\nthe number of strands in the generated hairstyle (resulting in\nbetter quality than the classical graphics methods) or editing\nhairstyles with text prompts. We perform quantitative com-\nparisons with Neural Haircut as well as an ablation study\nto understand which design choices are critical. In contrast\nto SDS-based methods like TECA, HAAR is significantly\nmore efficient, requiring seconds instead of hours to gener-\nate the hairstyle.\nOur contributions can be summarized as follows:\n\u2022 We propose a first text-conditioned diffusion model for\nrealistic 3D strand-based hairstyle generation,\n\u2022 We showcase how the learned latent hairstyle representa-\ntions can be used for semantic editing,\n\u2022 We developed a method for accurate and automated an-\nnotation of synthetic hairstyle assets using off-the-shelf\nVQA systems.\nThe model will be available for research purposes.\n2. Related work\nRecently, multiple text-to-3D approaches [5, 6, 14, 25, 28,\n29, 32, 38, 39, 48, 49, 54] have emerged that were in-\n2\nspired by the success of text-guided image generation [40\u2013\n42, 44]. A body of work of particular interest to us is the\none that uses image-space guidance to generate 3D shapes\nin a learning-by-synthesis paradigm. Initially, these meth-\nods used CLIP [41] embeddings shared between images and\ntext to ensure that the results generated by the model ad-\nhere to the textual description [2, 12, 33]. However, the\nScore Distillation Sampling procedure (SDS) [38] has re-\ncently gained more popularity since it could leverage text-\nto-image generative diffusion models, such as Stable Dif-\nfusion [42], to guide the creation of 3D assets from text,\nachieving higher quality. Multiple concurrent methods em-\nploy this SDS approach to map textual description into a hu-\nman avatar [4, 14, 19, 24, 54]. In particular, the TECA [54]\nsystem focuses on generating volumetric hairstyles in the\nform of neural radiance fields (NeRFs) [34]. However, these\napproaches can only generate the outer visible surface of the\nhair without internal structure, which prevents it from be-\ning used out-of-the-box in downstream applications, such\nas simulation and physics-based rendering. Moreover, the\nSDS procedure used to produce the reconstructions is no-\ntoriously slow and may require hours of optimization to\nachieve convergence for a given textual prompt. Our ap-\nproach is significantly more efficient, and is capable of gen-\nerating and realistically rendering the hairstyles given tex-\ntual prompts in less than a minute.\nIn contrast to the methods mentioned above, we also gen-\nerate the hairstyles in the form of strands. Strand-accurate\nhair modeling has manifold applications in computer vision\nand graphics as it allows subsequent physics-based render-\ning and simulation using off-the-shelf tools [3, 7, 9]. One of\nthe primary use cases for the strand-based generative mod-\neling has historically been the 3D hair reconstruction sys-\ntems [13, 20, 35, 43, 45\u201347, 52, 53, 55, 56]. Among the\nsettings where it is most often used is the so-called one-shot\ncase, where a hairstyle must be predicted using only a sin-\ngle image [13, 52, 55]. Approaches that tackle it leverage\nsynthetic datasets of strand-based assets to train the models\nand then employ detailed cues extracted from the images,\nsuch as orientation maps [37], to guide the generation pro-\ncess. However, these systems are unsuitable for semantics-\nbased or even unconditional generation of hairstyles, as\nthey rely heavily on these cues for guidance. A group of\nmethods that is more closely related to ours is Neural Hair-\ncut [47] and GroomGen [57], in which a synthetic dataset of\nhairstyle assets is leveraged to train an unconditional gen-\nerative model [16, 18, 42]. While useful for regularizing\nmulti-view hair reconstruction [47], the degree of control\nover the synthesized output in such methods is missing.\nOur work addresses the issue of controllability in generative\nmodels for hair and is the first one to provide strand-based\nhairstyle generation capabilities given textual descriptions.\n3. Method\nGiven a textual description that contains information about\nhair curliness, length, and style, our method generates real-\nistic strand-based hair assets. The resulting hairstyles can be\nimmediately used in computer graphics tools that can ren-\nder and animate the hair in a physically plausible fashion.\nOur pipeline is depicted in Figure 2. At its core is a latent\ndiffusion model, which is conditioned on a hairstyle text\nembedding. It operates on a latent space that is constructed\nvia a Variational Autoencoder (VAE) [18]. Following [43],\nthis VAE is trained to embed the geometry of individual\nstrands into a lower-dimensional latent space. During infer-\nence, the diffusion model generates this representation from\nGaussian noise and the input text prompt, which is then up-\nsampled to increase the number of strands and decoded us-\ning a VAE decoder to retrieve the 3D hair strands.\n3.1. Hairstyle parametrization.\nWe represent a 3D hairstyle as a set of 3D hair strands that\nare uniformly distributed over the scalp. Specifically, we\ndefine a hair map H with resolution 256 \u00d7 256 that cor-\nresponds to a scalp region of the 3D head model. Within\nthis map, each pixel stores a single hair strand S as a poly-\nline. As mentioned previously, our diffusion model is not\ndirectly operating on these 3D polylines, but on their com-\npressed latent embeddings z. To produce z that encodes the\nstrand S, we first convert the latter into the local basis de-\nfined by the Frenet frame of the face where the strand root\nis located. On this normalized data, we train a variational\nauto-encoder, which gives us access to an encoder E(S) and\na decoder G(z).Using the encoder E(S), we encode the in-\ndividual hair strands in the hair map H, resulting in a latent\nmap Z that has the same spatial resolution. The decoded\nstrand-based hair map is then denoted as \u02c6H. In summary,\nwith a slight abuse of notation, the maps are related to each\nother as follows: Z = E(H), and \u02c6H = G(Z).\n3.2. Conditional Hair Diffusion Model\nWe use a pre-trained text encoder \u03c4 [22], that encodes the\nhairstyle description P into the embedding \u03c4(P). This em-\nbedding is used as conditioning to the denoising network\nvia a cross-attention mechanism:\nAttention(Q, K, V ) = softmax\n\u0012QKT\n\u221a\nd\n\u0013\n\u00b7 V,\n(1)\nwhere Q = W (i)\nQ \u00b7 \u03d5i(Zt), K = W (i)\nK \u00b7 \u03c4(P), V = W (i)\nV\n\u00b7\n\u03c4(P) with learnable projection matrices W (i)\nQ , W (i)\nK , W (i)\nV .\nThe denoising network is a 2D U-Net [15], where \u03d5i(Zt)\ndenotes i-th intermediate representations of the U-Net pro-\nduced for the latent hair map Zt at the denoising step t. For\nour training, we employ the EDM [16] formulation, fol-\nlowing [47]. We denote the latent hair map with noise as\n3\nHairstyle\nEncoder\nSubsample\n& add noise\nDiffusion model\nGenerated\nhairstyle\nTraining\nDecoder\nUpsample\n... woman with afro\nhairstyle \u2026\nInference\nE\nH\nD\nLatent hair map\nG\n\u03c4\u03b8\nText embedding\nP\nZ\nCross-attn.\nGenerated hair map\n\u02c6Z\n\u02c6H\n\u03c4\u03b8\nSec. 3.1\nSec. 3.4\nSec. 3.2\nSec. 3.3\nSec. 3.1\nCaption\nFigure 2. Overview. We present our new method for text-guided and strand-based hair generation. For each hairstyle H in the training\nset, we produce latent hair maps Z and annotate them with textual captions P using off-the-shelf VQA systems [26] and our custom\nannotation pipeline. Then, we train a conditional diffusion model D [16] to generate the guiding strands in this latent space and use a\nlatent upsampling procedure to reconstruct dense hairstyles that contain up to a hundred thousand strands given textual descriptions. The\ngenerated hairstyles are then rendered using off-the-shelf computer graphics techniques [9].\nZt = Z + \u03f5 \u00b7 \u03c3t, where \u03f5 \u223c N(0, I), and \u03c3t is the noise\nstrength. We then use a denoiser D to predict the output:\nD\u03b8(Zt, \u03c3t, P) = cs\nt \u00b7 Zt + co\nt \u00b7 F\u03b8\n\u0000ci\nt \u00b7 Zt, cn\nt , \u03c4(P)\n\u0001\n, (2)\nwhere the cs\nt, co\nt, ci\nt and cn\nt are the preconditioning factors\nfor the noise level \u03c3t that follow [16], and F\u03b8 denotes a\nU-Net network. The optimization problem is defined as:\nmin\n\u03b8\nE\u03c3t,\u03f5,Z,P\n\u0002\n\u03bbt \u00b7 \u2225D\u03b8(Zt, \u03c3t, P) \u2212 Z\u22252\n2\n\u0003\n,\n(3)\nwhere \u03bbt denotes a weighting factor for a given noise level.\n3.3. Upsampling\nDue to the limited amount of available 3D hairstyles, the\ndiffusion model is trained on a downsampled latent hair map\nZ\u2032 with resolution 32 \u00d7 32 and, thus, only generates so-\ncalled \u2019guiding hair strands\u2019. To increase the number of\nstrands in the generated results, we upsample the latent hair\nmap to the resolution of 512 \u00d7 512. A common way of\nupsampling a strand-based hairstyle to increase the number\nof strands is via interpolation between individual polylines.\nIn modern computer graphics engines [3, 7] multiple ap-\nproaches, such as Nearest Neighbour (NN) and bilinear in-\nterpolation are used. Applying these interpolation schemes\nleads to over-smoothing or clumping results. In some more\nadvanced pipelines, these schemes are combined with dis-\ntance measures based on the proximity of strand origins\nor the similarity of the curves. Additionally, Blender and\nMaya [3, 7] introduce an option of adding noise into the in-\nterpolation results to further prevent clumping of the hair\nstrands and increase realism. However, the described inter-\npolation procedure requires a lot of manual effort and needs\nto be done for each hairstyle separately to obtain optimal\nparameters and resolve undesired penetrations.\nIn this work, we propose an automatic approach with in-\nterpolation of the hairstyle in latent space by blending be-\ntween nearest neighbor and bilinear interpolation schemes.\nIn this way, we aim to preserve the local structure of strands\nnear a partition and apply smoothing in regions with similar\nstrand directions. To calculate the blending weights, we first\ncompute the cosine similarity between neighboring 3D hair\nstrands on the mesh grid and apply the non-linear function\nf(\u00b7) to control the influence of the particular interpolation\ntype, which we empirically derived to be as follows:\nf(x) =\n(\n1 \u2212 1.63 \u00b7 x5\nwhere x \u2264 0.9\n0.4 \u2212 0.4 \u00b7 x\nx > 0.9,\n(4)\nwhere x is the cosine similarity. Our final interpolation for\neach point on the mesh grid is defined as a blending be-\ntween the nearest neighbor and bilinear interpolations with\nthe weight f(x) and (1 \u2212 f(x)) correspondingly. The de-\nfined upsampling method ensures that in the vicinity of a\npartition, the weight of the nearest neighbor decreases lin-\nearly, and then diminishes at a polynomial rate. As a result\nof this scheme, we obtain realistic geometry in the regions\nwith low similarity among strands. On top of that, we add\nGaussian noise to the interpolated latents to increase the hair\nstrands diversity, resulting in a more natural look.\n3.4. Data generation\n3D hairstyle data. For training and evaluating the diffusion\nmodel, we use a small artist-created hairstyle dataset, that\nconsists of 40 high-quality hairstyles with around 100,000\nstrands.\nTo increase the diversity, we combine it with\ntwo publicly available datasets: CT2Hair [46] and USC-\nHairSalon [13] that consist of 10 and 343 hairstyles, respec-\ntively. We align the three datasets to the same parametric\nhead model and additionally augment each hairstyle using\nrealistic squeezing, stretching, cutting, and curliness aug-\nmentations. In total, we train the model on 9825 hairstyles.\nHairstyle description.\nAs these 3D hairstyles do not\ncome with textual annotations, we use the VQA model\n4\nQ: What is the type of current hairstyle?\nWhat is the relative length of the hairstyle?\nWhat is the texture of current hairstyle?\nDescribe actors with similar hairstyle type.\nVisual Question Answering (VQA)\nA: The type of the hairstyle is bob \u2026\n\u2026 the length is short ...\n\u2026 the hairstyle is straight \u2026\n\u2026 This hairstyle is very popular ...\nHairstyle\nEmbedding\nText Encoding\n\u2211\nN\nN\nFigure 3. Dataset collection. Rendered from frontal and back\nview hairstyles along with a predefined set of questions Q are sent\nthrough VQA [26, 27] to obtain hairstyle description, which is\nfurther encoded using frozen text encoder network [22].\nLLaVA [26, 27] to automatically produce hairstyle descrip-\ntions from a set of predefined questions (see Figure 3).\nTo do that, we first render all collected hairstyles using\nBlender [7] from frontal and back camera views. We use\nthe standard head model and neutral shading for hairstyles\nto prevent bias to any particular type of hairstyle because\nof color or gender information.\nWith the help of Chat-\nGPT [36], we design a set of prompts, that include specific\nquestions about length, texture, hairstyle type, bang, etc., as\nwell as a set of general questions about historical meaning,\nprofessional look, occasions for such hairstyle, celebrities\nwith similar type to increase generalization and variability\nof our conditioning model. We then use a random subset of\nthese prompts for each hairstyle in the dataset to increase\nthe diversity of annotations. For a full list of prompts that\nwere used, please refer to the suppl. material.\nThe quality of visual systems is highly restricted by the\ndiversity of data used during training. We have observed\nin our experiments that the accuracy of the produced hair\ncaptions is relatively low, or they contain very broad de-\nscriptions. In particular, we have noticed that the existing\nVQA systems have problems accurately reasoning about\nthe hair length or the side of the parting. To improve the\nquality of VQA answers, similarly to [58], we add an ad-\nditional system prompt \u201cIf you are not sure say it honestly.\nDo not imagine any contents that are not in the image\u201d,\nwhich decreases the likelihood of the model hallucinating\nits responses. Further, we have observed that the VQA sys-\ntem works better when it does not use information from the\nprevious answers. That allows us to not accumulate erro-\nneous descriptions during the annotation session. We have\nalso observed that the LLAVA model is biased toward con-\nfirming the provided descriptions instead of reasoning, so\nintroducing a set of choices to the prompts substantially im-\nproves the results. Finally, we calculate the embeddings of\nthe resulting hairstyle descriptions P using a BLIP encoder\n\u03c4 for both frontal and back views and average them to pro-\nduce the conditioning used during training.\n3.5. Training details\nTo train the diffusion model, we sample a batch of hairstyles\nat each iteration, align them on a mesh grid of 256 \u00d7 256\nresolution, and, then, subsample it into a size of 32\u00d732. By\ntraining the diffusion model on these subsampled hairstyles\nwe improve convergence and avoid overfitting. To acceler-\nate the training, we use the soft Min-SNR [10] weighting\nstrategy. It tackles the conflicting directions in optimization\nby using an adaptive loss weighting strategy. For more de-\ntails, please refer to the original Min-SNR paper [10]. To\nevaluate the performance, we utilize an Exponential Mov-\ning Average (EMA) model and Euler Ancestral Sampling\nwith 50 steps. The whole method is trained for about 5 days\non a single NVIDIA A100, which corresponds to 160,000\niterations. Additional details are in the suppl. material.\n4. Experiments\n4.1. Evaluation\nWe compare our method against competing approaches for\ngenerative hair modeling: TECA [54] and Neural Hair-\ncut [47]. TECA creates a compositional avatar that includes\nseparate geometries for hair, body, and cloth using only a\ntext description. This method represents hair using neu-\nral radiance fields (NeRF) [34] and focuses on the visual\nquality of generated avatars, not geometry reconstruction.\nMoreover, it takes multiple hours to generate a single sam-\nple using TECA because they rely on Score Distillation\nSampling [38]. In our case, we concentrate on physically\nplausible geometry for the hair and require around 4.3 sec-\nonds to generate a hairstyle. Neural Haircut focuses on the\nreconstruction of realistic 3D hairstyles with a strand-based\nrepresentation using monocular video or multi-view images\ncaptured under unconstrained lighting conditions. In this\nwork, authors exploit a diffusion model to obtain some prior\nknowledge for better reconstruction quality. In contrast to\nour approach, the quality of the diffusion model is limited\nby the amount of data, the size of the model architecture,\nand the chosen training strategy. This model is uncondi-\ntional, and thus cannot control the generated hairstyles.\nQuality of unconditional diffusion. To compare the qual-\nity of the unconditional diffusion model, we re-train Neural\nHaircut [47] on the same training data and with the same\nscalp parametrization as our method. We evaluate the dis-\ntance of the generated hairstyles to the training distribution\nusing Minimum Matching Distance (MMD) [1] as well as\ncoverage (Cov) [1] metrics. We use the 1-Nearest Neighbor\naccuracy (1-NNA) [30] metric, which is a leave-one-out ac-\n5\nPrompt: \u201cA woman with afro hairstyle\u201d\nPrompt: \u201cA woman with bob hairstyle\u201d\nPrompt: \u201cA woman with long wavy hair\u201d\nPrompt: \u201cA woman with straight long hair\u201d\nTECA\nOurs\nFigure 4. Comparison. Qualitative comparison of conditional generative models. We show several generations of TECA [54] and our\nmodel. For our results, we visualize the geometry obtained before (shown in pseudocolor) and after upsampling. Our model generates more\ndiverse samples with higher-quality hairstyles. It is also worth noting that TECA, in some cases, does not follow the input descriptions\nwell, producing short hair instead of long hair (bottom row). Digital zoom-in is recommended.\nMethod\nMMD\u2193\nCOV\u2191\n1-NNA \u2192 0.5\nNeural Haircut [47]\n31507.7\n0.18\n0.34\nOur\n21104.9\n0.2\n0.55\nTable 1. Comparison of unconditional diffusion models. Our\nmethod generates samples with better quality and diversity.\ncuracy of the 1-NN classifier that assesses if two provided\ndistributions are identical. The best quality is achieved for\nvalues closer to 0.5. Suppose, we have two datasets of gen-\nerated and reference hairstyles denoted as Sg and Sr, where\n|Sg| = |Sr|. Then, the described metrics are defined as:\nMMD(Sg, Sr) =\n1\n|Sr|\nX\ny\u2208Sr\nmin\nx\u2208Sg\nD(x, y)\n(5)\nCOV(Sg, Sr) =\n1\n|Sr| |{arg min\ny\u2208Sr\nD(x, y)|x \u2208 Sg}|\n(6)\n1\u2212NNA(Sg, Sr) =\nP\nx\u2208Sg I[Nx \u2208 Sg] + P\ny\u2208Sr I[Ny \u2208 Sr]\n| Sg | + | Sr |\n, (7)\nwhere I(\u00b7) is an indicator function, NF is the nearest neigh-\nbor in set Sr \u222aSg \\F and D is the squared distance between\ndistributions, computed in the latent space of the VAE. In\nTable 1, we show the comparison based on these metrics.\nOur method generates samples closer to the ground-truth\ndistribution with higher diversity.\nFinally, we conducted a user study. Participants were\npresented 40 randomly sampled hairstyle pairs obtained us-\ning Neural Haircut [47] and our method. We collected more\nthan 1,200 responses on the question \u201cWhich hairstyle from\nthe presented pair is better?\u201d, and ours was preferred in\n87.5 % of cases.\nQuality of conditional diffusion. We compare the quality\nof our conditional generation with TECA [54]. We launch\nboth of the methods for various prompts with several ran-\ndom seeds to obtain the hair volume that follows the de-\nsired text input. The qualitative comparison can be seen\nin Figure 4. While TECA produces great conditioning re-\nsults most of the time, some severe artifacts are noticeable\nin the hair region. Furthermore, the diversity of generations\nis limited, and we see some failure cases even for simple\nprompts like \u201cA woman with straight long hair\u201d. With our\nmethod HAAR, we provide a way to obtain detailed physi-\ncally plausible geometry with large variations.\n6\nBlender (nearest)\nBlender (bilinear)\nOurs (nearest)\nOurs (bilinear)\nOurs\nOurs w/ noise\nFigure 5. Upsampling. Comparison of different upsampling schemes used to interpolate between guiding strands (shown in dark color).\nFor visualization purposes here we show around 15,000 strands. Blender interpolation is obtained in 3D space, while Ours is computed\nin latent space. Using the Nearest Neighbour in both variants produces better accuracy according to the guiding strand geometry (shown\nin dark color), but it results in an unrealistic global appearance. The bilinear schemes lead to the penetration of averaged hair strands and\nthe loss of structure of the original guiding strands. Blending both these methods resolves proposed issues and results in realistic renders.\nAdding additional noise in latent space further increases realism and helps to get rid of the grid structure.\nText encoder\nCLIP\nBLIP\nTransf.\nReference\nCSIM\n0.174\n0.189\n0.172\n0.206\nTable 2.\nConditioning.\nAblation on different conditioning\nschemes. With BLIP text encoder, we obtain better conditioning\ncompared to CLIP and trainable Transformer network.\n4.2. Ablation study\nConditioning.\nThe quality of the conditional diffusion\nmodel for hairstyle generation is highly dependent on the\nquality of the text encoder network \u03c4(\u00b7).\nWe ablate the\nperformance of the conditional generation using pre-trained\nand frozen encoders, such as CLIP [41], BLIP [22] as well\nas a trained transformer network [50] implemented on top\nof a pre-trained BertTokenizer [8]. For more details on the\narchitecture, please refer to the supplemental material. The\nintuition behind training additional networks for text encod-\ning is that the quality of pre-trained encoders may be limited\nfor a particular task (for example some specific hairstyle\ntypes), which results in wrong correlations between words\nand deteriorates the quality of the diffusion model.\nWe evaluate the performance using semantic matching\nbetween text and generated 3D hairstyles. Specifically, we\nuse CLIP [41] and compute the cosine distance between im-\nages and their respective text prompts. To do that, we gen-\nerate 100 hairstyles for 10 different prompts and then render\nfrom a frontal view using Blender [7]. Table 2 shows that\nthe BLIP text encoder is providing the most effective con-\nditioning. To show the upper-bound quality of this metric\n(\u2019reference\u2019), we calculate the CSIM on our ground-truth\ndataset with prompts obtained via VQA.\nUpsampling scheme. We ablate the performance of dif-\nferent upsampling schemes needed to obtain a full hairstyle\nfrom a set of guiding strands, which can be seen in Figure 5.\nThere is no one-to-one correspondence and during interpo-\nlation, a lot of artifacts can occur. The most common artifact\nis a visible grid structure which appears when using a Near-\nest Neighbour (NN) strategy. Bilinear interpolation leads to\nscalp penetrations due to averaging the nearest strands on\ntop of the head, and it deteriorates the local shape of curls.\nThe computer graphics engines, such as Blender [7] and\nMaya [3], either do not provide enough control or require\na lot of manual effort in setting up the optimal parameters\nfor each hairstyle separately. We find that the combination\nof NN and Bilinear using our proposed scheme leads to the\nbest-looking results of renders. Furthermore, adding noise\nin the latent space results in more realistic hairstyles. Note,\nfor visualization we show an example with a reduced den-\nsity of around 15,000 strands; increasing it leads to less bald\nregions, especially, in the region of a partition.\n4.3. Hairstyle editing\nSimilar to Imagic [17], we do text-based hairstyle editing,\nsee Figure 6. Given an input hairstyle and a target text that\ncorresponds to the desired prompt, we edit the hairstyle in a\nway that it corresponds to the prompt while preserving the\ndetails of the input hairstyle. To do that we first do textual\ninversion of the input hairstyle. We obtain etgt that corre-\nsponds to the target prompt P. After optimizing it with a\nfixed diffusion model D\u03b8 using a reconstruction loss, we\nacquire eopt. Conditioning on the obtained text embedding\neopt does not lead to the same target hairstyle. So, to pro-\n7\nInput Image\neopt\n0.25\n0.5\n0.75\n\u201cShort straight\u201d\nFigure 6. Hairstyle editing. Similar to Imagic [17], we edit the input image using a text prompt. We provide editing results without\nadditionally tuning the diffusion model (first two rows) and with it (second two rows). Finetuning the diffusion model results in smoother\nediting and better preservation of input hairstyle.\nFigure 7. Limitations. Our failure cases include penetration into\nthe scalp region (left), which in principle can be resolved in a post-\nprocessing step. Additionally, for the afro hairstyles (right), the\ndegree of strands\u2019 curliness needs to be increased.\nvide a smooth transition, we freeze eopt and fine-tune D\u03b8.\nFinally, we linearly interpolate between etgt and eopt. For\nmore information, please refer to the supplemental material.\n4.4. Limitations\nThe quality of generated hairstyles is limited by the vari-\nety and quality of our dataset, in terms of both the diversity\nof geometry assets and the accuracy of textual annotations.\nThe main failure cases include the generation of hairstyles\nwith scalp interpenetrations and lack of curliness for some\nextreme hairstyles, see Figure 7. In theory, these limitations\ncan be addressed with a dataset that contains more diverse\nsamples of curly hairstyles, as well as human-made anno-\ntations. Especially, when used in a physics simulation, the\ninterpenetrations can be resolved in a postprocessing step.\nAnother limitation of our method is that we only consider\ngeometry, we do not generate the hair color and texture\nwhich would be an interesting direction for future work.\n5. Conclusion\nWe have presented HAAR, the first method that is able to\nconditionally generate realistic strand-based hairstyles us-\ning textual hairstyle descriptions as input. Not only can such\na system accelerate hairstyle creation in computer graph-\nics engines, but it also bridges the gap between computer\ngraphics and computer vision. For computer graphics, gen-\nerated hairstyles could be easily incorporated into tools like\nBlender for hair editing and physics-based animation. For\ncomputer vision, our system can be used as a strong prior\nfor the generation of avatars or to create synthetic training\ndata of realistic hairstyles. While being limited by data, we\nthink that this method is a first step in the direction of con-\ntrollable and automatic hairstyle generation.\nAcknowledgements\nVanessa Sklyarova was supported by the Max Planck ETH\nCenter for Learning Systems. Egor Zakharov\u2019s work was\nfunded by the \u201cAI-PERCEIVE\u201d ERC Consolidator Grant,\n2021. We sincerely thank Giorgio Becherini for rendering\n8\nhairstyles and Joachim Tesch for realistic hair simulations.\nAlso, we thank Yao Feng and Balamurugan Thambiraja for\ntheir help during the project and Hao Zhang for aiding us\nwith the TECA comparison.\nDisclosure.\nMJB has received research gift funds from\nAdobe, Intel, Nvidia, Meta/Facebook, and Amazon. MJB\nhas financial interests in Amazon, Datagen Technologies,\nand Meshcapade GmbH. While MJB is a consultant for\nMeshcapade, his research in this project was performed\nsolely at, and funded solely by, the Max Planck Society.\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas Guibas. Learning representations and generative\nmodels for 3D point clouds. In Proceedings of the 35th In-\nternational Conference on Machine Learning, pages 40\u201349.\nPMLR, 2018. 5\n[2] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias\nNie\u00dfner.\nClipFace: Text-guided Editing of Textured 3D\nMorphable Models.\nIn ArXiv preprint arXiv:2212.01406,\n2022. 3\n[3] Autodesk, INC. Maya. 1, 2, 3, 4, 7\n[4] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K Wong. Dreamavatar: Text-and-shape guided 3d hu-\nman avatar generation via diffusion models. arXiv preprint\narXiv:2304.00916, 2023. 3\n[5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 2023. 2\n[6] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using\ngaussian splatting, 2023. 2\n[7] Blender Online Community. Blender - a 3D modelling and\nrendering package. Blender Foundation, Stichting Blender\nFoundation, Amsterdam, 2023. 1, 2, 3, 4, 5, 7, 12\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding.\nIn North American\nChapter of the Association for Computational Linguistics,\n2019. 7, 11\n[9] Epic Games. Unreal engine. 1, 2, 3, 4, 13\n[10] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong\nChen, Han Hu, Xin Geng, and Baining Guo. Efficient diffu-\nsion training via min-snr weighting strategy. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 7441\u20137451, 2023. 5\n[11] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 11\n[12] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. ACM Trans-\nactions on Graphics (TOG), 41(4):1\u201319, 2022. 3\n[13] Liwen Hu, Chongyang Ma, Linjie Luo, and Hao Li. Single-\nview hair modeling using a hairstyle database. ACM Trans-\nactions on Graphics (TOG), 34:1 \u2013 9, 2015. 3, 4\n[14] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Ji-\naxiang Tang, Deng Cai, and Justus Thies. TeCH: Text-guided\nReconstruction of Lifelike Clothed Humans. In International\nConference on 3D Vision (3DV), 2024. 2, 3\n[15] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. CVPR, 2017. 3\n[16] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. In Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2022. 3, 4, 11\n[17] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Con-\nference on Computer Vision and Pattern Recognition 2023,\n2023. 7, 8\n[18] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes, 2022. 3\n[19] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed-\nuard Gabriel Bazavan, Mihai Fieraru, and Cristian Sminchis-\nescu. Dreamhuman: Animatable 3d avatars from text. 2023.\n3\n[20] Zhiyi Kuang, Yiyang Chen, Hongbo Fu, Kun Zhou, and\nYouyi Zheng.\nDeepmvshair:\nDeep hair modeling from\nsparse views.\nSIGGRAPH Asia 2022 Conference Papers,\n2022. 3\n[21] Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio\nSavarese, and Steven C.H. Hoi. LAVIS: A one-stop library\nfor language-vision intelligence. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 3: System Demonstrations), pages 31\u201341,\nToronto, Canada, 2023. Association for Computational Lin-\nguistics. 11\n[22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn ICML,\n2022. 2, 3, 5, 7, 11\n[23] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and\nJavier Romero. Learning a model of facial shape and ex-\npression from 4D scans. ACM Transactions on Graphics,\n(Proc. SIGGRAPH Asia), 36(6):194:1\u2013194:17, 2017. 2\n[24] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang,\nYangyi Huang, Justus Thies, and Michael J Black. Tada!\ntext to animatable digital avatars. ArXiv, 2023. 3\n[25] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 2\n[26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning, 2023. 2,\n4, 5, 11\n[27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In NeurIPS, 2023. 2, 5, 11\n9\n[28] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang,\nChao Xu, Hansheng Chen, Chong Zeng, Jiayuan Gu, and\nHao Su. One-2-3-45++: Fast single image to 3d objects with\nconsistent multi-view generation and 3d diffusion, 2023. 2\n[29] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023. 2\n[30] David Lopez-Paz and Maxime Oquab. Revisiting classifier\ntwo-sample tests. In International Conference on Learning\nRepresentations, 2017. 5\n[31] Ilya Loshchilov and Frank Hutter.\nDecoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations, 2017. 11, 12\n[32] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and\nAndrea Vedaldi. Realfusion: 360 reconstruction of any ob-\nject from a single image. In CVPR, 2023. 2\n[33] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. arXiv preprint arXiv:2112.03221, 2021. 3\n[34] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2, 3, 5\n[35] Giljoo Nam, Chenglei Wu, Min H. Kim, and Yaser Sheikh.\nStrand-accurate multi-view hair capture.\n2019 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 155\u2013164, 2019. 3\n[36] ChatGPT. OpenAI, 2023. 5, 13, 16\n[37] Sylvain Paris, H\u00b4ector M. Brice\u02dcno, and Franc\u00b8ois X. Sillion.\nCapture of hair geometry from multiple images. ACM SIG-\nGRAPH 2004 Papers, 2004. 3\n[38] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv,\n2022. 2, 3, 5\n[39] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. Magic123: One image to high-quality 3d object\ngeneration using both 2d and 3d diffusion priors.\narXiv\npreprint arXiv:2306.17843, 2023. 2\n[40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n3\n[41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. ArXiv, abs/2204.06125, 2022. 3, 7,\n11\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3, 11\n[43] Radu Alexandru Rosu, Shunsuke Saito, Ziyan Wang, Chen-\nglei Wu, Sven Behnke, and Giljoo Nam.\nNeural strands:\nLearning hair geometry and appearance from multi-view im-\nages. European Conference on Computer Vision (ECCV),\n2022. 3\n[44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi. Photorealistic text-to-image\ndiffusion models with deep language understanding. 2022. 3\n[45] Shunsuke Saito,\nLiwen Hu,\nChongyang Ma,\nHikaru\nIbayashi, Linjie Luo, and Hao Li. 3d hair synthesis using\nvolumetric variational autoencoders. ACM Transactions on\nGraphics (TOG), 37:1 \u2013 12, 2018. 3\n[46] Yuefan Shen, Shunsuke Saito, Ziyan Wang, Olivier Maury,\nChenglei Wu, Jessica Hodgins, Youyi Zheng, and Giljoo\nNam. Ct2hair: High-fidelity 3d hair modeling using com-\nputed tomography. ACM Transactions on Graphics, 42(4):\n1\u201313, 2023. 4\n[47] Vanessa Sklyarova, Jenya Chelishev, Andreea Dogaru, Igor\nMedvedev, Victor Lempitsky, and Egor Zakharov. Neural\nhaircut: Prior-guided strand-based hair reconstruction.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 19762\u201319773, 2023. 2, 3, 5,\n6\n[48] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 2\n[49] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 2\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neu-\nral Information Processing Systems. Curran Associates, Inc.,\n2017. 7, 11\n[51] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-\nmond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, R\u00b4emi Louf, Morgan Funtowicz, Joe Davison, Sam\nShleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien\nPlu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M. Rush.\nTrans-\nformers: State-of-the-art natural language processing.\nIn\nProceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations,\npages 38\u201345, Online, 2020. Association for Computational\nLinguistics. 11\n[52] Keyu Wu, Yifan Ye, Lingchen Yang, Hongbo Fu, Kun Zhou,\nand Youyi Zheng.\nNeuralhdhair: Automatic high-fidelity\nhair modeling from a single image using implicit neural\nrepresentations. 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 1516\u20131525,\n2022. 3\n[53] Lingchen Yang, Zefeng Shi, Youyi Zheng, and Kun Zhou.\nDynamic hair modeling from monocular videos using deep\nneural networks. ACM Transactions on Graphics (TOG), 38:\n1 \u2013 12, 2019. 3\n10\n[54] H. Zhang, Y. Feng, P. Kulits, Y. Wen, J. Thies, and M. J.\nBlack. Teca: Text-guided generation and editing of compo-\nsitional 3d avatars. arXiv, 2023. 2, 3, 5, 6, 11\n[55] Yujian Zheng, Zirong Jin, Moran Li, Haibin Huang,\nChongyang Ma,\nShuguang Cui,\nand Xiaoguang Han.\nHairstep: Transfer synthetic to real using strand and depth\nmaps for single-view 3d hair modeling. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12726\u201312735, 2023. 3\n[56] Yi Zhou, Liwen Hu, Jun Xing, Weikai Chen, Han-Wei Kung,\nXin Tong, and Hao Li. Hairnet: Single-view hair reconstruc-\ntion using convolutional neural networks. In European Con-\nference on Computer Vision, 2018. 3\n[57] Yuxiao Zhou, Menglei Chai, Alessandro Pepe, Markus\nGross, and Thabo Beeler. GroomGen: A high-quality gen-\nerative hair model using hierarchical latent representations.\n2023. 2, 3\n[58] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen,\nWenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks,\nblip-2 answers: Automatic questioning towards enriched vi-\nsual descriptions. ArXiv, abs/2303.06594, 2023. 5\nSupplemental Material\nA. Implementation and training details\nHairstyle diffusion model.\nFor conditional diffu-\nsion model, we use the U-Net architecture from [42]\nwith the following parameters:\nimage size\n=\n32 \u00d7\n32, input channels\n=\n64, num res blocks\n=\n2,\nnum heads = 8, attention resolutions = (4, 2, 1),\nchannel mult = (1, 2, 4, 4), model channels = 320,\nuse spatial transformer = True, context dim = 768,\nlegacy = False.\nOur training pipeline uses the EDM [16] library and\nwe optimize the loss function using AdamW [31] with\nlearning rate = 10\u22124, \u03b2 = [0.95, 0.999], \u03f5 = 10\u22126,\nbatch size = 8, and weight decay = 10\u22123.\nList of prompts. Below we include the list of prompts used\nduring data annotation using a VQA model. After each of\nthe prompts, we add \u2018If you are not sure say it honestly. Do\nnot imagine any contents that are not in the image. After the\nanswer please clear your history.\u2019 to the input.\n\u2022 \u2018Describe in detail the bang/fringe of depicted hairstyle including its\ndirectionality, texture, and coverage of face?\u2019\n\u2022 \u2018What is the overall hairstyle depicted in the image?\u2019\n\u2022 \u2018Does the depicted hairstyle longer than the shoulders or shorter than\nthe shoulder?\u2019\n\u2022 \u2018Does the depicted hairstyle have a short bang or long bang or no bang\nfrom frontal view?\u2019\n\u2022 \u2018Does the hairstyle have a straight bang or Baby Bangs or Arched Bangs\nor Asymmetrical Bangs or Pin-Up Bangs or Choppy Bangs or curtain\nbang or side swept bang or no bang?\u2019\n\u2022 \u2018Are there any afro features in the hairstyle or no afro features?\u2019\n\u2022 \u2018Is the length of the hairstyle shorter than the middle of the neck or\nlonger than the middle of the neck?\u2019\n\u2022 \u2018What are the main geometry features of the depicted hairstyle?\u2019\n\u2022 \u2018What is the overall shape of the depicted hairstyle?\u2019\n\u2022 \u2018Is the hair short, medium, or long in terms of length?\u2019\n\u2022 \u2018What is the type of depicted hairstyle?\u2019\n\u2022 \u2018What is the length of hairstyle relative to the human body?\u2019\n\u2022 \u2018Describe the texture and pattern of hair in the image.\u2019\n\u2022 \u2018What is the texture of depicted hairstyle?\u2019\n\u2022 \u2018Does the depicted hairstyle is straight or wavy or curly or kinky?\u2019\n\u2022 \u2018Can you describe the overall flow and directionality of strands?\u2019\n\u2022 \u2018Could you describe the bang of depicted hairstyle including its direc-\ntionality and texture?\u2019\n\u2022 \u2018Describe the main geometric features of the hairstyle depicted in the\nimage.\u2019\n\u2022 \u2018Is the length of a hairstyle buzz cut, pixie, ear length, chin length, neck\nlength, shoulder length, armpit length or mid-back length?\u2019\n\u2022 \u2018Describe actors with similar hairstyle type.\u2019\n\u2022 \u2018Does the hairstyle cover any parts of the face? Write which exact parts.\u2019\n\u2022 \u2018In what ways is this hairstyle a blend or combination of other popular\nhairstyles?\u2019\n\u2022 \u2018Could you provide the closest types of hairstyles from which this one\ncould be blended?\u2019\n\u2022 \u2018How adaptable is this hairstyle for various occasions (casual, formal,\nathletic)?\u2019\n\u2022 \u2018How is this hairstyle perceived in different social or professional set-\ntings?\u2019\n\u2022 \u2018Are there historical figures who were iconic for wearing this hairstyle?\u2019\n\u2022 \u2018Could you describe the partition of this hairstyle if it is visible?\u2019\nText-based models. For the VQA, we found that \u201cLLaVA-\nv1.5\u201d model [26, 27] produces the best text descriptions\nwith a relatively low hallucination rate. As shown in Ta-\nble 2, we experimented with different text encoder mod-\nels. We used \u201cViT-L/14\u201d configuration for CLIP [41] and\n\u201cblip feature extractor\u201d from [21] library for BLIP [22]. In\nthe ablation experiment, we compare its result with an op-\ntimizable Transformer [50] build on top of the pre-trained\nBERTTokenizer [8] from the transformers [51] library with\nconfiguration \u201cbert-base-uncased\u201d.\nFor the Transformer\nnetwork, we use BERTEmbedder from [42] with n layer =\n6, max seq len = 256, n embed = 640.\nB. Additional Ablations and Results\nQualitative comparison. We show an extended compari-\nson with TECA [54] with more complex prompts that show\nthe compositional abilities of the models (see Figure 8).\nImportance of classifier-free-guidance. To improve the\nsample quality of the conditional model, we use classifier-\nfree-guidance [11]. During training, we optimize condi-\ntional and unconditional models at the same time, by using\ntext embedding with zeros in 10% of cases. During infer-\nence, we fix the random seed and show changes in sample\nquality, sweeping over the guidance strength w. As we can\nsee in Figure 9, higher weights improve the strength of con-\nditional prompts, but increasing it too much leads to out-of-\ndistribution samples with a high degree of inter-head pene-\ntrations. In our experiments, we fix the guidance weight to\nw = 1.5.\n11\nPrompt: \u201cbob hairstyle with long bang\u201d\nPrompt: \u201cA woman with curly short hairstyle\u201d\nPrompt: \u201cA woman with shoulder-length wavy hair\u201d\nPrompt: \u201cBob hairstyle with afro features\u201d\nTECA\nOurs\nFigure 8. Comparison. Extended comparison with TECA. Our method produces higher quality samples with greater diversity than ones\ngenerated in TECA, and our representation allows the animation of the hair in a physics simulator.\nHairstyle interpolation. We linearly interpolate between\ntwo text prompts P1 and P2 by conditioning the diffu-\nsion model D\u03b8 on a linear combination of text embeddings\n(1 \u2212 \u03b1)\u03c4(P1) + \u03b1\u03c4(P2), where \u03b1 \u2208 [0, 1], and \u03c4 is the\ntext encoder. For interpolation results obtained for different\nprompt pairs that differ in length and texture please see Fig-\nure 10. One can notice that the interpolation between two\ntypes of textures, e.g. \u201cwavy\u201c and \u201cstraight\u201c usually starts\nappearing for \u03b1 close to 0.5, while length reduction takes\nmany fewer interpolation steps.\nHairstyle editing.\nFor optimization eopt, we do 1500\nsteps with the optimizer Adam with a learning rate of\n10\u22123. For diffusion fine-tuning, we do 600 steps with op-\ntimizer AdamW [31] with a learning rate of 10\u22124, \u03b2 =\n[0.95, 0.999], \u03f5 = 10\u22126, and weight decay 10\u22123.\nBoth\nstages are optimized using the same reconstruction loss\nused during the training of the main model. The entire edit-\ning pipeline takes around six minutes on a single NVIDIA\nA100. See Figure 11 for more editing results with and with-\nout fine-tuning.\nUpsampling scheme. We provide more results on the dif-\nferent upsampling schemes for \u201clong straight\u201c and \u201clong\nwavy\u201c hairstyles (see Figure 12). While Blender [7] in-\nterpolation in 3D space produces either results with a high\nlevel of penetration (bilinear upsampling) or very structured\n(Nearest Neighbour) hairstyles, we are able to easily blend\nbetween two types in latent space, combining the best from\nthe two schemes. Adding noise helps eliminate the grid\nstructure inherited from the nearest neighbor sampling and,\nthus, improves realism. For noising the latent space, we\ncalculate a standard deviation eZ\u03c3 \u2208 R1\u00d71\u00d7M of latent map\nafter interpolation eZ \u2208 RN\u00d7N\u00d7M, where N is a grid res-\nolution and M = 64 is the dimension of latent vector that\nencodes the entire hair strand. The final noised latent map\nis eZ = eZ + eZ\u03c3 \u2299 X \u2299 Y , where X \u2208 RN\u00d7N\u00d71 with ele-\nments xijk \u223c N(0.15, 0.05), Y \u2208 RN\u00d7N\u00d71 with elements\nyijk = 2qijk \u22121,\nwhere\nqijk \u223c Bernoulli(0.5). In such\na way, we independently add some small random noise to\neach latent vector on the mesh grid.\nGeneralization capabilities.\nOur conditional diffusion\nmodel can distinguish between different texture types,\nlengths, bangs, and some popular hairstyles, like the bob,\nand afro.\nIt models the correlation between gender and\nhairstyle length, but at the same time, the capacity of the\nmodel is limited by the accuracy of the VQA and text en-\ncoder system. Asking more general questions improves the\ngeneralization quality, but the answers may be less accurate\nand lead to additional noise during training. To test the gen-\neralization capabilities of our model, we evaluate it on out-\n12\nw = 0\n0.5\n0.8\n1.\n1.2\n1.5\n2.\nw = 2.5\nFigure 9. Classifier-free guidance. Quality of samples during changing the guidance weight w from 0 to 2.5. Weight w = 0 corresponds\nto unconditional generation, while w = 1 - to conditional. For w > 1 we obtain over-conditioned results. In our experiments, we fix\nw = 1.5, as higher weights lead to more penetrations and reduced realism. The first four rows correspond to generation samples for the\nprompt \u201cvoluminous straight hair\u201c with two different random seeds, while the last four - for \u201cwavy long hair\u201c.\nof-distribution prompts and attempt to generate hairstyles\nof particular celebrities. We use ChatGPT [36] to describe\nthe hairstyle type of a particular celebrity and use the re-\nsulting prompt for conditioning. To our surprise, we find\nthat even given the limited diversity of the hairstyles seen\nduring training, our model can reproduce the general shape\nof the hairstyle. We show results illustrating the general-\nization capabilities of our model by reconstructing celebrity\nhairstyles for \u201cCameron Diaz\u201c and \u201cTom Cruise\u201c (see Fig-\nure 13). Between different random seeds hairstyles pre-\nserve the main features, like waviness and length, but could\nchange the bang style.\nFinally, we show the results of our conditional model\non different hairstyle types, by conditioning the model on\nhairstyle descriptions from [36] (see Figure 14).\nSimulations.\nThe hairstyles generated by our diffusion\nmodel are further interpolated to resolution 512 \u00d7 512 and\nthen imported into the Unreal Engine [9] as a hair card.\nWe tested simulations in two scenarios: integration into a\nrealistic game environment with manual character control\nas well as simple rotation movements for different types of\nhairstyles. The realism of simulations highly depends on\nthe physical characteristics of hair, e.g. friction, stiffness,\ndamping, mass, elasticity, resistance, and collision detec-\ntion inside the computer graphics engine. An interesting\nresearch direction for future work may include the predic-\ntion of individual physical properties for each hairstyle that\ncould further simplify the artists\u2019 work. For simulation re-\nsults, please refer to the supplemental video.\n13\nInput\nInterpolated hairstyles\nInput\n\u201cshort straight hair\u201c\n\u201cshort curly hair\u201c\n\u201clong wavy hair\u201c\n\u201cshort straight hair\u201c\n\u201clong wavy\u201c\n\u201clong straight\u201c\n\u201cwavy bob \u201c\n\u201cman haircut\u201c\n\u201ccasual woman\u201c\n\u201cshort haircut\u201c\n\u201cbob hairstyle\u201c\n\u201cbuzz cut\u201c\nFigure 10. Hairstyle interpolation. Linear interpolation between two given textual prompts.\n14\nInput Image\neopt\n0.125\n0.25\n0.375\n0.5\n0.625\n0.75\n0.875\n\u201cShort hair\u201c\nInput Image\neopt\n0.125\n0.25\n0.375\n0.5\n0.625\n0.75\n0.875\n\u201cStraight long\u201c\nFigure 11. Hairstyle editing. Extended editing results of our model. In each section of four images, we provide editing results without\nadditionally tuning the diffusion model (first two rows) and with it (second two rows). Finetuning the diffusion model results in smoother\nediting and better preservation of input hairstyle.\n15\nBlender (nearest)\nBlender (bilinear)\nOurs (nearest)\nOurs (bilinear)\nOurs\nOurs w/ noise\nFigure 12. Upsampling. Extended results on hairstyle interpolation between guiding strands obtained using different schemes. For better\nvisual comparison, we interpolate hairstyles to around 15,000 strands and additionally visualize guiding strands (shown in dark color) for\nOurs methods with interpolation in latent space. Our final method with additional noise improves the realism of hairstyles by removing the\ngrid-like artifacts.\nFigure 13. Generalization. Hairstyles generated for celebrities \u201cCameron Diaz\u201c (first two rows) and \u201cTom Cruise\u201c (last two rows) using\ndescriptions from [36]. Several variations of hairstyles with corresponding guiding strands are generated for each celebrity.\n16\n\u201cLong Layered Waves\u201c\n\u201cLong Shag\u201c\n\u201cHawk Fade\u201c\n\u201cLayered Hair\u201c\n\u201cLong Layers\u201c\n\u201cCropped Curls\u201c\n\u201cVoluminous Curls\u201c\n\u201cBlowout\u201c\n\u201cPixie Cut\u201c\n\u201cLayered Bob\u201c\n\u201cPinup Style\u201c\n\u201cShaggy Bob\u201c\n\u201cStraight Lob\u201c\n\u201cAfro\u201c\n\u201cSide Part\u201c\n\u201cFrench Crop\u201c\n\u201cCurtain Bangs\u201c\n\u201cBowl Cut\u201c\n\u201cSpiky Hair\u201c\n\u201cWavy Hair\u201c\nFigure 14. Conditional generation. Random samples generated for input prompts with classifier-guidance weight w = 1.5.\n17\n"
  },
  {
    "title": "Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model",
    "link": "https://arxiv.org/pdf/2312.12423.pdf",
    "upvote": "12",
    "text": "Jack of All Tasks, Master of Many: Designing General-purpose\nCoarse-to-Fine Vision-Language Model\nShraman Pramanick\u22171,2\u2020\nGuangxing Han\u22172\nRui Hou2\nSayan Nag3\nSer-Nam Lim4\nNicolas Ballas2\nQifan Wang2\nRama Chellappa1\nAmjad Almahairi2\n1Johns Hopkins University, 2Meta, 3University of Toronto, 4University of Central Florida\nAbstract\nThe ability of large language models (LLMs) to process\nvisual inputs has given rise to general-purpose vision sys-\ntems, unifying various vision-language (VL) tasks by in-\nstruction tuning. However, due to the enormous diversity in\ninput-output formats in the vision domain, existing general-\npurpose models fail to successfully integrate segmentation\nand multi-image inputs with coarse-level tasks into a single\nframework. In this work, we introduce VistaLLM, a power-\nful visual system that addresses coarse- and fine-grained VL\ntasks over single and multiple input images using a unified\nframework. VistaLLM utilizes an instruction-guided image\ntokenizer that filters global embeddings using task descrip-\ntions to extract compressed and refined features from nu-\nmerous images. Moreover, VistaLLM employs a gradient-\naware adaptive sampling technique to represent binary seg-\nmentation masks as sequences, significantly improving over\npreviously used uniform sampling. To bolster the desired\ncapability of VistaLLM, we curate CoinIt, a comprehensive\ncoarse-to-fine instruction tuning dataset with 6.8M sam-\nples. We also address the lack of multi-image grounding\ndatasets by introducing a novel task, AttCoSeg (Attribute-\nlevel Co-Segmentation), which boosts the model\u2019s reason-\ning and grounding capability over multiple input images.\nExtensive experiments on a wide range of V- and VL tasks\ndemonstrate the effectiveness of VistaLLM by achieving\nconsistent state-of-the-art performance over strong base-\nlines across all downstream tasks. Our project page can be\nfound at https://shramanpramanick.github.io/VistaLLM/.\n1. Introduction\nLarge language models (LLM) have proven to be the de-\nfacto solution to address novel natural language processing\n(NLP) tasks, thanks to their ability to comprehend user-\ntailored prompts, instructions, and detailed task descrip-\ntions [14, 25, 73, 74, 92, 93]. However, the problem is more\n\u2217Equal technical contribution.\n\u2020Part of this work was done during an internship at Meta.\nTextVQA\nIconQA\nHM\nREC\nPOPE\nVQAv2\nCOCO Cap\nPointQA\nVCR\nGREC\nRES\nGRES\niCoSeg\nNLVR2\n53.0\n47.9\n60.1\n89.9\n90.5\n77.7\n128.4\n85.9\n79.6\n54.6\n77.2\n65.1\n95.1\n81.7\n87.8\n86.2\n75.3\n117.5\n85.3\n84.8\n78.6\n51.9\n47.7\n58.2\n88.7\n89.5\n89.8\nVistaLLM-13B\nMiniGPT-v2 [8]\nFerret-13B [115]\nShikra-13B [9]\nGPT4RoI-13B [123]\nSingle-image coarse-level\nSingle-image region-level\nMulti-image coarse-level\nMulti-image region-level\nFigure 1. VistaLLM achieves the state-of-the-art performance\nacross a broad range of single and multi-image coarse-to-fine\ngrained reasoning and grounding tasks (see Table 1 for details)\namong general-purpose baselines. Notably, no existing baseline\nhave unified segmentation and multi-image tasks in a single sys-\ntem. We show officially reported numbers for every baseline.\nchallenging the vision domain due to an inherent disparity\nof input and output formats across different tasks. Though\npre-training followed by a fine-tuning strategy is effective\nfor various vision problems [16, 17, 46, 48, 53, 78, 79, 81,\n97, 109], with the continuously increasing model parame-\nters, the marginal cost for task-specific tuning comes with\nsignificant computational overhead. Hence, it becomes cru-\ncial to design general-purpose vision models that can per-\nceive natural-language instructions to solve various vision\nproblems in a zero-shot manner.\nThe development of general-purpose vision models faces\ntwo significant challenges: first, the unification of diverse\ninput-output formats, and second, an effective representa-\ntion of visual features for a variety of tasks. Image-level vi-\nsion tasks such as classification, captioning, and question-\nanswering involve textual outputs and primarily require a\nbroader, coarse-grained image representation, making them\nrelatively straightforward to integrate into a unified frame-\nwork [15, 22, 57, 126]. In contrast, region-level prediction\n1\narXiv:2312.12423v1  [cs.CV]  19 Dec 2023\ntasks like object detection and semantic segmentation ne-\ncessitate fine-grained, pixel-scale visual features and pro-\nduce dense outputs such as bounding boxes and masks.\nConverting bounding boxes to natural language sequences\nis feasible by serializing the coordinates of two corners.\nHowever, representing a binary mask as a text sequence\nposes a more complex challenge, especially when dealing\nwith multiple input images each associated with numer-\nous segmentation masks. Although some recent general-\npurpose systems have succeeded in unifying coarse-level\ntasks with object detection [8, 9, 34, 75, 115, 123], they do\nnot incorporate segmentation within the same framework.\nFurthermore, the capabilities of these existing systems are\noften limited to processing single-image input, thereby con-\nstraining their applicability in broader, more complex sce-\nnarios, such as reasoning over multiple images and recog-\nnizing and segmenting common objects.\nIn this work, we present VistaLLM, the first general-\npurpose vision model that addresses coarse- and fine-\ngrained vision-language reasoning and grounding tasks\nover single and multiple input images. We unify these tasks\nby converting them into an instruction-following sequence-\nto-sequence format. We efficiently transform binary masks\ninto a sequence of points by proposing a gradient-aware\nadaptive contour sampling scheme, which significantly im-\nproves over the naive uniform sampling technique previ-\nously used for sequence-to-sequence segmentation tasks\n[10, 11, 59, 125]. Moreover, to preserve global and region-\nlevel information from multiple input images, we propose\nutilizing a QFormer [46] based instruction-guided image\ntokenizer. Leveraging LLMs\u2019 language reasoning ability,\nwe feed our visual features with carefully designed task-\nspecific instructions to LLMs, which generate responses\nfollowing the instructions. Integrating various tasks with\ndifferent granularity into such a unified, cohesive, and end-\nto-end system helps improve the performance of each task\nby sharing coarse- and fine-grained feature representation.\nTo train VistaLLM on a versatile form of vision and lan-\nguage tasks, we collect CoinIt (Coarse-to-fine Instruction-\ntuning Dataset) with 6.8M samples, ranging over four broad\ncategories of tasks - single-image coarse-level, single-\nimage region-level, multi-image coarse-level, and multi-\nimage region-level.\nWe address the lack of publicly-\navailable multi-image region-level datasets by proposing a\nnovel task, AttCoSeg (Attribute-level Co-Segmentation),\nwhich aims to recognize input images which have objects\nwith common attributes (shape, color, size, position), and\nsegment those objects. AttCoSeg contains 685k training\nsamples, and help VistaLLM to gain significant general-\nizable reasoning and grounding capability over multiple\ninput images.\nOther tasks of CoinIt are constructed by\nconverting publicly available benchmarks into instruction-\nfollowing format, such as COCO [54], Flickr [77], VCR\n[118], LLaVA [57], VG [38], PASCAL [20] etc. Extensive\nevaluation on 15 different benchmarks proves the efficacy of\nVistaLLM, which even surpasses specialist (or fine-tuned)\nsystems in most tasks, including 10.9% CIDEr points gain\nover Shikra [9] on image captioning, 13.1%, 6.7% preci-\nsion and gIoU improvements over MDETR [36] on GREC\nand GRES, 3% J -index gains over CycleSegNet [120] on\niCoSeg.\nIn summary, our contributions are threefold: (i)We pro-\npose VistaLLM, equipped with a instruction-guided image\ntokenizer, to seamlessly integrate coarse- and fine-grained\nvision-language reasoning and grounding tasks over single\nand multiple input images into a unified general-purpose\nmodel. (ii) To efficiently convert segmentation masks into\na sequence, we propose a gradient-aware adaptive contour\nsampling scheme, which improves over previously used\nuniform sampling by 3 \u2212 4 mIoU scores on different seg-\nmentation benchmarks. (iii) We construct CoinIt, a large-\nscale coarse-to-fine instruction-tuning dataset, for model\ntraining. Moreover, we introduce a novel task, AttCoSeg,\nwhich addresses the lack of publicly available multi-image\ngrounding datasets. We evaluate VistaLLM on a wide-range\nof vision-language tasks across 15 benchmarks, achieving\nstate-of-the-art performance in all of them, even surpassing\nspecialist systems. We summarize these results in Figure 1.\n2. Related Works\nGeneral-purpose vision models, also known as multimodal\nlarge language models (MLLM), have recently been proven\nto be an effective way to unify a versatile array of vision\nand language tasks. These models, which use potent LLMs\n[4, 14, 18, 25, 30, 73, 74, 91\u201393, 95, 103, 105, 119, 122] to\nreason textual instructions, can broadly be categorized into\ntwo groups based on their input and output formats:\nCoarse-level MLLMs:\nEarly attempts of designing\nMLLMs focused on image-level vision tasks with textual\noutputs, such as visual question answering [2, 29, 66, 86]\nand image captioning [24, 26]. Frozen [94], Flamingo [1],\nFrozenBiLM [108], MAGMA [19], ClipCap [70], VidIL\n[101], PICa [110] are among the first few to show the in-\ncontext capability of LLMs for few-shot vision tasks. More\nrecent works have focused on using LLMs for visual in-\nstruction tuning. To name a few, LLaVA [57], MiniGPT-\n4 [126], MM-REACT [113], BLIP2 [46], mPLUS-OWL\n[114], LLaMA-Adapter v2 [22], Otter [42], Instruct-BLIP\n[15], LLaVA-Med [43] have been proven to be effective.\nHowever, these models lack region-specific capabilities and\ncan not perform visual grounding tasks.\nRegion-level MLLMs:\nMore recently, MLLMs have\nmoved forward to unify region-based referring and ground-\ning tasks into general-purpose vision systems. KOSMOS-\n2 [75], VisionLLM [98], Shikra [9], GPT4RoI [123],\n2\n\u2746\nImage\nEncoder\nInstruction-Guided\nImage Tokenizer\nVicuna\nTrainable\n\u2746 Frozen\n\u2b50\n\u2b50Trainable (Stage 1)\nFrozen (Stage 2)\nThe masks are\n[10, 23,...., 42],....,\n[213, 254,..., 230]\nGlobal Image Embeddings\nRandom Queries\nPlease find the common object in the input images <image>. There will be only one\ncommon object in each image. Present the segmentation mask in each image as [x0, y0,\nx1, y1, ..., x31, y31] structure. Output the masks in the same order as the input images.\nNatural Language Instruction\nMultiple Input Images\nOutputs\nInstruction-Guided\nImage Embeddings\nFigure 2. Overview of the proposed system - VistaLLM, which integrates single- and multi-image coarse- and fine-grained vision-\nlanguage tasks into a unified general-purpose framework. VistaLLM contains three key design modules - (i) image encoder to extract\nthe global image embedding, (ii) instruction-guided image tokenizer, which refines and compresses the global image embeddings using\ntask instruction, enabling the model to filter the necessary visual information required for the current task, and (iii) LLM (Vicuna)-\nbased decoder to jointly process image and language features, and generate the desired output. VistaLLM uses a gradient-aware adaptive\nsampling technique to efficiently represent segmentation masks as a point sequence, described in Section 3.2. All parameters except the\nimage encoder are trained in stage 1, while only the image tokenizer is fine-tuned in stage 2 (See Section 3.1, 5.2 for details).\nAll-Seeing Model [100], CogVLM [99], COMM [33],\nMiniGPT-v2 [8] and Ferret [115] has shown the capabil-\nity of MLLMs of fine-grained image comprehension and\nregion-focused conversation. While KOSMOS-2, Shikra,\nand VisionLLM feed the image coordinates directly into the\nLLM, GPT4RoI and Ferret use additional feature extractor\nmodules to represent image regions. On a related regime,\nInternGPT [60], BuboGPT [124], and LISA [39] utilize ex-\nternal vision modules to perform grounding tasks. However,\nthese works are only capable of processing single-input im-\nages. In this work, we propose VistaLLM to address all pos-\nsible reasoning and grounding tasks over single and multi-\nple images. Moreover, we efficiently convert binary masks\ninto sequence by a novel adaptive sampling, which helps to\nunify segmentation into a general-purpose framework.\n3. Method\nWe start by presenting the model architecture of VistaLLM.\nNext, we detail the proposed sequence generation approach\nfor segmentation masks and illustrate its efficacy compared\nto uniform sampling.\n3.1. Model Architecture\nThe overall architecture of VistaLLM, shown in Figure 2,\nconsists of three key design modules - (i) image encoder\nto extract the global image embedding, (ii) instruction-\nguided image tokenizer, which refines and compresses the\nglobal image embeddings using task instruction, enabling\nthe model to filter the necessary visual information required\nfor the current task, and (iii) LLM-based decoder to jointly\nprocess image and language features, and generate the de-\nsired output.\nImage Encoder. Given a set of k input images X = {xi}k\n1;\nxi \u2208 RHi\u00d7Wi\u00d73, where Hi and Wi denote the height and\nwidth of the ith image, we first feed them into a pre-trained\nimage encoder, EVA-CLIP [90], to extract k image embed-\ndings Z = {zi}k\n1; zi \u2208 RNi\u00d7D, Ni is number of spatial to-\nkens in the ith image and D is the hidden dimension. Note\nthat, for larger k, the image feature dimension increases,\nmaking it difficult for the LLM decoder to process it as in-\nput, which is taken care of in the tokenizer module.\nInstruction-guided Image Tokenizer. Unlike many previ-\nous general-purpose vision systems [8, 9, 57, 75], which\ndirectly feed the global image features into the decoder,\nwe introduce an instruction-guided image tokenizer, which\nplays three crucial roles: (i) refines the image embeddings\nin alignment with task description, i.e.\nfor coarse-level\ntasks like image captioning, global features are important,\nwhereas for region-level tasks like referring expression seg-\nmentation (RES), only the features of the referred object\nneed to be processed. (ii) compresses the image embed-\ndings, which is particularly important when there are many\ninput images, and (iii) flexibly projects multiple input im-\nages with different heights and widths into the same feature\ndimension.\nThe image tokenizer module takes image embeddings\nand the language instruction and outputs the refined and\ncompressed visual features.\nIf referring regions (points,\nboxes, masks) are present in the instruction, they are con-\nverted to text-interleaved sequence as described in Section\n3.2. Afterwards, we propose to adopt a QFormer [46] net-\nwork with L (L < Ni, \u2200i) randomly-initialized queries,\nwhich learns high-level task-specific information using the\nlanguage instruction. The output from the tokenizer, F =\n{fi}k\n1; fi \u2208 RL\u00d7D, are then flattened to produce the final\nvisual features, Fv \u2208 RkL\u00d7D which are fed into the LLM.\nLLM. We use Vicuna [13] as our language model, which is\na decoder-only LLM [5] with a context length of 2048 build\nby instruction-tuning LLaMa [92]. The LLM takes the vi-\nsion features Fv and the language instruction as input, and\n3\nUniform Sampling\nve Sampling\nOriginal\nUniform\nAdaptive\nAdaptive Sampling\n(a) Illustration of uniform and adaptive sampling on a line curve.\nUniform Sam\nUniform Sampling\nAdaptive Sampling\nGround Truth\nOriginal\nUniform\n(b) Illustration of uniform and adaptive sampling on a object mask.\nFigure 3. Visualization of uniform and adaptive sampling strategies. (a) illustration of sampled points and comparison of reassembled\ncurves, (b) illustration of sampled points and comparison of reassembled masks.\ngenerates task-specific output. We train the LLM end-to-\nend by traditional next-token prediction objective calculated\nover the ground-truth. Since Vicuna only has the digits 0-9\nin its vocabulary, we introduce additional tokens 10-999 to\nrepresent quantized coordinates. During evaluation, we de-\nquantize the generated number tokens into the image space\nfor metric calculation.\n3.2. Sequence Generation for Grounding Tasks\nThe outputs from grounding tasks typically manifest in one\nof three formats: points, boxes, and masks.\nPoints and\nboxes are straightforward to quantify and serialize, as ev-\nidenced in [8, 9, 75]. For instance, a point is represented by\nits coordinates [x, y], while a box is denoted by its diago-\nnal corner points [xmin, ymin, xmax, ymax], signifying the\ntop-left and bottom-right corners. Conversely, the outline\nof a mask can assume any free-form shape comprising po-\ntentially infinite points. In scenarios where such free-form\npolygons are referenced in the input instructions, they can\nbe encoded as region features [115, 123]. However, trans-\nlating segmentation masks into a sequence for output by a\ngeneral-purpose framework is particularly challenging, and\nthe process necessitates conversion of segmentation masks\ninto a small number of discrete points.\nPreviously,\nencoder-decoder-based segmentation ap-\nproaches [10, 11, 59, 125] uniformly sample N points\nclockwise from the contour of the mask, and then quantize\nand serialize them as [x1, y1, x2, y2, ..., xN, yN],\nxi = round\n\u0012 \u02dcxi\nw \u2217 nbins\n\u0013\n, yi = round\n\u0012 \u02dcyi\nh \u2217 nbins\n\u0013\n(1)\nwhere (\u02dcxi,\u02dcyi) are the original floating point image coor-\ndinates, w, h are the width and height of the image, nbins is\nthe number of quantization bins, and (xi,yi) are the quan-\ntized coordinates.\nHowever, as shown in the top-left of\nFigure 3a, the uniform sampling approach is unaware of\nthe contour curvature and cannot properly represent sharp\nedges. To alleviate this limitation, we argue that the sam-\npling should preserve more points where the contour has\na sharp bend and less where it is almost straight. Based\non this observation, we propose a gradient-aware adaptive\nsampling technique, which we describe in three steps:\n\u2022 Contour Discretization. First, we discretize the contin-\nuous contour by uniformly sampling a high number (M)\nof dense points. Note that these dense points represent\nthe curve well, but such a long sequence is infeasible for\ntraining a decoder.\n\u2022 Gradient Calculation. Next, for every point pi\u2208{1,...,M}\non the curve, we draw two lines - l1 by joining pi with its\nprevious point pi\u22121, and l2 by joining pi\u22121 with the next\npoint pi+1. l1 and l2 create an angle \u03b8i (0\u25e6 \u2264 \u03b8i < 180\u25e6)\nat pi\u22121. If \u03b8i \u2243 0, the contour is almost linear at pi, and\nwe can safely discard pi (e.g., points B and D in the right\ncolumn of Figure 3a). As \u03b8i increases, the curvature at pi\nbecomes sharper, and the importance of keeping pi in the\nfinal sampling list increases (e.g., points A and C).\n\u2022 Sorting & Quantization: Finally, we sort \u03b8i\u2208{1,...,M} in\ndescending order, and keep the N points (N \u226a M) cor-\nresponding to the N highest \u03b8i. These N points, which\nare then quantized1 and serialized as in Equation 1, de-\nnote the final sampled list.\nThe right column of Figure 3a depicts the adaptive sam-\npling technique, which produces a better representation of\nsharp bends of the curve than uniform sampling, shown in\nthe bottom-left of the same figure. We further illustrate the\nreconstruction from two techniques with a mask from the\nCOCO dataset in Figure 3b, where the uniform sampling\nloses fine details of the zebra\u2019s legs, back, and ears. In con-\ntrast, adaptive sampling preserves the mask more precisely.\nBoth uniform and adaptive sampling techniques in-\nevitably result in a certain amount of information loss from\nthe original ground-truth masks, thereby imposing a con-\nstraint on the maximal performance achievable in segmen-\ntation tasks. Nonetheless, the extent of this loss is consid-\nerably reduced when employing the adaptive sampling ap-\nproach. For instance, in the RefCOCO validation set for Re-\nferring Expression Segmentation (RES), uniform sampling\nof 32 points from the ground-truth masks yields an mIoU\nupper bound of 94.70, whereas adaptive sampling achieves\n97.26. The superiority of adaptive sampling becomes even\nmore pronounced in the case of complex geometric struc-\n1We use 1000 quantization bins, by default.\n4\nDataset\nTask\nCorpus\nMulti\nimg?\nReg.\nlevel?\nInput\nformat\nOutput\nformat\nMetrics (%)\nCOCO [54]\nCaption\nTrain, Eval\n\u2717\n\u2717\nI\nT\nSPICE, CIDEr\nVQAv2\nTrain, Eval\n\u2717\n\u2717\nI + Q\nT\nAccuracy\nREC\nTrain, Eval\n\u2717\n\u2713\nI + R\nB\nPr@0.5\nGREC\nTrain, Eval\n\u2717\n\u2713\nI + R\nM\nPr@0.5, N-acc\nRES\nTrain, Eval\n\u2717\n\u2713\nI + R\nB\nmIoU\nGRES\nTrain, Eval\n\u2717\n\u2713\nI + R\nM\ngIoU, N-acc, T-acc\nREG\nTrain\n\u2717\n\u2713\nI + B\nT\n\u2212\nAttCoSeg\nTrain\n\u2713\n\u2713\nI\nM\n\u2212\nFlickr [77]\nSpot Caption\nTrain\n\u2717\n\u2713\nI\nT + B\n\u2212\nVG [38]\nREG\nTrain\n\u2717\n\u2713\nI + B\nT\n\u2212\nVCR [118]\nReasoning\nTrain, Eval\n\u2717\n\u2713\nI + Q + B\nT\nAccuracy\nLLaVa [57]\nVQA\nTrain\n\u2717\n\u2717\nI + Q\nT\n\u2212\nLT-QA [69]\nBQA\nTrain, Eval\n\u2717\n\u2713\nI + Q + B\nB\nAccuracy\nV7W [127]\nPQA\nTrain, Eval\n\u2717\n\u2713\nI + Q + P\nT\nAccuracy\nBQA\nTrain, Eval\n\u2717\n\u2713\nI + Q + B\nT\nAccuracy\nTextVQA [86]\nReading comp.\nEval\n\u2717\n\u2713\nI + Q\nT\nAccuracy\nIconQA [66]\nReasoning\nEval\n\u2713\n\u2713\nI + Q\nT\nAccuracy\nHM [37]\nClassification\nEval\n\u2717\n\u2717\nI\nT\nAccuracy\nPOPE [52]\nHallucination\nEval\n\u2717\n\u2717\nI + Q\nY/N\nPrec., Recall, F1\nNLVR [88, 89]\nReasoning\nTrain, Eval\n\u2713\n\u2717\nI + Q\nY/N\nAccuracy\nPASCAL [20]\nCoSeg\nTrain, Eval\n\u2713\n\u2713\nI\nM\nPrecision (P),\nJaccard Index\n(J )\niCoSeg [3]\nMSRC [104]\nTable 1. Training and evaluation datasets, input-output for-\nmats, and metrics. To train VistaLLM on versatile form of vision\nand language tasks, we collect CoinIt, which is a unified set of\n14 benchmarks. We quantitatively evaluate the trained model on\n15 tasks without additional fine-tuning, among which TextVQA,\nIconQA, POPE, and HM contain unseen tasks during training, as-\nsessing the system\u2019s generalization capability. I: Image, T: General\nText, Q: Question, R: Referring Expression, P: Point coordinate,\nB: Bounding Box, M: Segmentation Mask, Y/N: Yes or No.\ntures containing numerous sharp bends and intricate details.\nWe delve deeper into the comparative efficacy of these two\nmethods through ablation experiments in Section 5.4.\n4. Coarse-to-fine Instruction-tuning Dataset\nTo train VistaLLM on a versatile form of vision and lan-\nguage tasks, we collect CoinIt (Coarse-to-fine Instruction-\ntuning Dataset), which is a unified set of 14 benchmarks\ncontaining 6.8M samples, among which (i) 13 are pub-\nlicly available which we convert to instruction-tuning for-\nmat, and (ii) we construct a new benchmark, AttCoSeg\n(Attribute-level Co-Segmentation), to alleviate the lack of\nmulti-image region-level datasets. We quantitatively evalu-\nate the trained model on 15 benchmarks without additional\nfine-tuning.\nNotably, 4 of these 15 downstream contain\nentirely unseen tasks during training, helpful for assessing\nthe system\u2019s generalization capability. To ensure data in-\ntegrity, we confirm that no images from the validation or test\nsets appear during training, thus eliminating the risk of data\nleakage. We have grouped these diverse tasks into four main\ncategories based on their input and output formats, summa-\nrized in Table 1:\n\u2022 Single-image coarse-level tasks, such as visual question\nanswering (VQA) and image captioning on COCO [54]\nand LLaVa [57] require global understanding of a single\ninput image.\n\u2022 Single-image region-level tasks, like generalized refer-\nring expression comprehension (GREC) [23] and seg-\nmentation (GRES) [56], spot captioning [9], visual com-\nMethod\nGeneral-\npurpose?\nVQAv2\nCOCO Cap.\nVal\nDev\nStd\nSPICE\nCIDEr\nMETER [17]\n\u2717\n\u2212\n76.4\n76.4\n23.0\n128.2\nFIBER [16]\n\u2717\n\u2212\n78.6\n78.4\n23.1\n128.4\nUnified-IO [65]\n\u2713\n\u2212\n77.9\n\u2212\n\u2212\n122.3\nFlamingo-80B [1]\n\u2713\n\u2212\n56.3\n\u2212\n\u2212\n84.3\nShikra-13B [9]\n\u2713\n75.3\n77.4\n77.5\n\u2212\n117.5\nVistaLLM-7B\n\u2713\n76.7\n78.9\n78.6\n24.2\n122.5\nVistaLLM-13B\n\u2713\n77.7\n79.8\n79.8\n24.6\n128.4\n\u2206Ours - Shikra-13B\n\u2212\n2.4 \u2191\n2.4 \u2191\n2.3 \u2191\n\u2212\n10.9 \u2191\nTable 2.\nPerformance on VQAv2 and COCO captioning.\nVistaLLM yields significant gains over existing general-purpose\nand fine-tuned baselines. Reported captioning results of METER\nand FIBER are without CIDEr optimization [82].\nmonsense reasoning (VCR) [118], box question answer-\ning (BQA) and point question answering (PQA) [69, 127]\nrequire fine-grained dense predictions over one input im-\nage. These tasks contain points, bounding boxes and seg-\nmentation masks in inputs and outputs.\n\u2022 Multi-image coarse-level tasks, like natural language for\nvisual reasoning (NLVR) [88, 89] and icon question an-\nswering (IconQA) [66] involve comprehending global\nperception across multiple input images.\n\u2022 Multi-image region-level tasks, such as object-level co-\nsegmentation (CoSeg) [41, 83] demands fine-grained rea-\nsoning and grounding on various input images.\nAttCoSeg, newly proposed benchmark: Existing multi-\nimage region-level object co-segmentation datasets [3, 20,\n104] are small-scale and simple to solve. Moreover, they\nassume that a common object will always exist in the in-\nput images. Hence, we argue that these datasets are in-\nsufficient to train VistaLLM to have generalized grounding\nability over many input images, and we construct a more\nchallenging larger-scale multi-image region-level dataset.\nWe use Group-wise RES [107] annotations to sample high-\nquality images containing objects with similar fine-grained\nattributes (shape, color, size, position). We refer to such\nimages as positives. While training VistaLLM, we input\nthese positive image pairs and randomly sampled multiple\nnegative images (which do not contain objects with similar\nattributes), ask the model to recognize the positive image\npair and segment the object with common traits in both of\nthem. We name this task attribute-level co-segmentation\n(AttCoSeg), which contains over 685k training samples,\nand help VistaLLM to gain significant generalized reason-\ning and grounding ability over multiple input images. De-\ntailed statistics of every dataset is given in Appendix D.\n5. Experiments\n5.1. Instruction Prompts\nCarefully designed language instructions are crucial for\ngeneral-purpose vision models on diverse tasks with differ-\nent input-output formats [9, 98]. Since we address closely\nrelated tasks like REC, RES, GREC, GRES, we use detailed\ninstructions. Figure 2 illustrates an example instruction for\nCoSeg. More example instructions are shown in Table E.1.\n5\nMethod\nGeneral-\npurpose?\nRef\nRef+\nRefg\nval\ntestA\ntestB\nval\ntestA\ntestB\nval\ntest\nUniTAB [109]\n\u2717\n86.3\n88.8\n80.6\n78.7\n83.2\n69.5\n80.0\n80.0\nMDETR [36]\n\u2717\n86.8\n89.6\n81.4\n79.5\n84.1\n70.6\n81.6\n80.9\nSeqTR [125]\n\u2717\n83.7\n86.5\n81.2\n71.5\n76.3\n64.9\n74.9\n74.2\nOFA-L [96]\n\u2713\n80.0\n83.7\n76.4\n68.3\n76.0\n61.8\n67.6\n67.6\nVisionLLM-H [98]\n\u2713\n\u2212\n86.7\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nShikra-13B [9]\n\u2713\n87.8\n91.1\n81.8\n82.9\n87.8\n74.4\n82.6\n83.2\nMiniGPT-v2 [8]\n\u2713\n88.7\n91.7\n85.3\n80.0\n85.1\n74.5\n84.4\n84.7\nFerret-13B [115]\n\u2713\n89.5\n92.4\n84.4\n82.8\n88.1\n75.2\n85.8\n86.3\nVistaLLM-7B\n\u2713\n88.1\n91.5\n83.0\n82.9\n89.8\n74.8\n83.6\n84.4\nVistaLLM-13B\n\u2713\n89.9\n92.5\n85.0\n84.1\n90.3\n75.8\n86.0\n86.4\n\u2206Ours - Ferret-13B\n\u2212\n0.4 \u2191\n0.1 \u2191\n0.6 \u2191\n1.3 \u2191\n2.2 \u2191\n0.6 \u2191\n0.2 \u2191\n0.1 \u2191\n(a) Performance on referring expression comprehension (REC).\nVistaLLM yields better results than existing baselines across all splits.\nMethod\nGeneral-\npurpose?\nRef\nRef+\nRefg\nval\ntestA\ntestB\nval\ntestA\ntestB\nval\ntest\nCGAN [67]\n\u2717\n64.9\n68.0\n62.1\n51.0\n55.5\n44.1\n51.0\n51.7\nVLT [55]\n\u2717\n65.7\n68.3\n62.7\n55.5\n59.2\n49.4\n53.0\n56.7\nLTS [35]\n\u2717\n65.4\n67.8\n63.1\n54.2\n58.3\n48.0\n54.4\n54.3\nCRIS [102]\n\u2717\n70.5\n73.2\n66.1\n62.3\n68.1\n53.7\n59.9\n60.4\nSeqTR [125]\n\u2717\n71.7\n73.3\n69.8\n63.0\n66.7\n59.0\n64.7\n65.7\nRefTr [49]\n\u2717\n74.3\n76.8\n70.9\n66.8\n70.6\n59.4\n66.6\n67.4\nLAVT [111]\n\u2717\n74.5\n76.9\n70.9\n65.8\n71.0\n59.2\n63.3\n63.6\nPolyFormer [59]\n\u2717\n76.0\n77.1\n73.2\n70.7\n74.5\n64.6\n69.4\n69.9\nVistaLLM-7B\n\u2713\n74.5\n76.0\n72.7\n69.1\n73.7\n64.0\n69.0\n70.9\nVistaLLM-13B\n\u2713\n77.2\n78.7\n73.9\n71.8\n74.4\n65.6\n69.8\n71.9\n\u2206Ours - PolyFormer\n\u2212\n1.2 \u2191\n1.6 \u2191\n0.7 \u2191\n1.1 \u2191\n0.1 \u2193\n1.0 \u2191\n0.4 \u2191\n2.0 \u2191\n(b) Performance on referring expression segmentation (RES).\nVistaLLM is the first general-purpose model to unify RES.\nTable 3. Performance on (a) REC, and (b) RES. While none other general-purpose systems can solve RES, VistaLLM sets a new state-\nof-the-art for both tasks across all splits.\nMethod\nGeneral-\npurpose?\nGREC\nMethod\nGeneral-\npurpose?\nGRES\nPr\nN-acc.\ngIoU\nN-acc.\nT-acc.\nMCN [68]\n\u2717\n28.0\n30.6\nMattNet [117]\n\u2717\n48.2\n41.2\n96.1\nVLT [55]\n\u2717\n36.6\n35.2\nVLT [55]\n\u2717\n52.0\n47.2\n95.7\nMDETR [36]\n\u2717\n41.5\n36.1\nLAVT [111]\n\u2717\n58.4\n49.3\n96.2\nVistaLLM-7B\n\u2713\n52.7\n69.4\nVistaLLM-7B\n\u2713\n64.4\n68.8\n96.6\nVistaLLM-13B\n\u2713\n54.6\n70.8\nVistaLLM-13B\n\u2713\n65.1\n70.0\n96.8\n\u2206Ours - MDETR\n\u2212\n13.1 \u2191\n34.7 \u2191\n\u2206Ours - LAVT\n\u2212\n6.7 \u2191\n20.7 \u2191\n0.6 \u2191\nTable 4. Performance on generalized referring expression com-\nprehension (GREC) and generalized referring expression seg-\nmentation (GRES). VistaLLM is the first general-purpose system\nto address both tasks, and gains huge improvements over existing\nspecialist models.\nWe use a special token <image>, which we later replace\nwith the instruction-guided image features to generate in-\nterleaved image-text input to the LLM.\nMoreover, the instruction must vary for different sam-\nples to support flexible user inputs. To generate high-quality\ninstructions with minimal cost, we manually write one ex-\nample description of each task and resort to GPT-3.5 [5] to\ncreate hundreds of variations. Next, we refine and ensure\nthe quality of every instruction with GPT-4 [72]. During\ntraining, we randomly pick one instruction for each sample.\n5.2. Implementation Details\nWe use EVA-CLIP [90] pre-trained on LAION-400M [85]\nand QFormer [46] pre-trained by InstructBLIP [15] as our\nvisual encoder and instruction-guided image tokenizer. We\nfeed the input images into EVA, which produces 256\u00d71408\ndimensional features for 224 \u00d7 224 images. The number\nof spatial tokens quadratically increases with the input im-\nage dimension. The Qformer has 12 encoder layers with\n12 heads and outputs 32 queries per image with a hidden\nsize of 768, thus working as an efficient feature compressor.\nFor a fair comparison with existing general-purpose base-\nlines [8, 9, 98, 115, 123], we use Vicuna7B and Vicuna13B\n[13] as the LLM. All other dense layers are initialized from\nscratch. For serializing the segmentation masks, we sample\n32 points using the proposed adaptive sampling technique.\nVistaLLM is trained in two stages. In the first stage, we\nfreeze EVA and train the rest of the model end-to-end on\nTask\nMethod\nLookTwice-QA\nTask\nMethod\nV7W\nAny\nSuper cls.\nObject\nPQA\nMani et al. [69]\n56.5\n59.1\n62.8\nBQA\nV7W [127]\n56.1\nShikra-13B [9]\n70.0\n70.2\n71.9\nCMNs [27]\n72.5\nVistaLLM-13B\n71.4\n71.8\n73.5\nViLBERT [64]\n82.8\n\u2206Ours - Shikra-13B\n1.4 \u2191\n1.6 \u2191\n1.6 \u2191\nViLBERTFT [64]\n83.4\nBQA\nMani et al. [69]\n60.2\n59.8\n61.4\nGPT4RoI-13B [123]\n84.8\nShikra-13B [9]\n70.3\n71.4\n72.3\nShikra-13B [9]\n85.3\nVistaLLM-13B\n72.0\n73.2\n73.4\nVistaLLM-13B\n85.9\n\u2206Ours - Shikra-13B\n1.7 \u2191\n1.8 \u2191\n1.1 \u2191\n\u2206Ours - Shikra-13B\n0.6 \u2191\nTable 5. Performance of point question answering (PQA) and\nbox question answering (BQA) on LookTwice-QA and Visual-\n7W. LookTwice-QA questions based on input point/box on three\ndifferent level of referential clarity in the question, e.g. \u201cHow\nmany of these [items/vehicles/cars] are there?\u201d Visual-7W ques-\ntions in \u2019which box\u2019 setting, i.e. choose one of the four bounding\nbox options based on given query.\nsingle-image datasets for 2 epochs. In the second stage,\nwe only tune the instruction-guided image tokenizer on the\nmulti-image datasets for 5 epochs. The final model is used\nfor all evaluations. VistaLLM is trained using AdamW op-\ntimizer [62] and cosine scheduler [61] with linear warmup\nfor the first 3% steps. We use a peak learning rate of 2e\u22125\nand a global batch size of 256. Training takes 2/3 days for\nthe first stage and 22/30 hours for the second stage with\n7/13B models on 32 A100 GPUs, each having 80G mem-\nory.\n5.3. Main Results\nWe use boldface and underline for the best and second-best\nperforming methods in every table and indicate the perfor-\nmance improvements over the state-of-the-art with \u2206.\nVQAv2 & COCO Captioning: Table 2 presents the perfor-\nmance on traditional single-image coarse-level visual ques-\ntion answering and image captioning tasks, which do not\nnecessitate coordinates in the input or output.\nThe in-\nput instructions for these tasks are straightforward, such\nas, \u201cPlease generate a simple description of the image\n<image>.\u201d or \u201cGiven the image <image>, can you please\nanswer the question <question>\u201d, where <question> de-\nnotes the input query.\nOn VQAv2, VistaLLM achieves\n77.7%, 79.8%, and 79.8% accuracy on the val, dev, and\n6\nMethod\nValidation Acc.\nQ \u2192 A\nQA \u2192 R\nQ \u2192 AR\nViLBERT [63]\n72.4\n74.5\n54.0\nUnicoder-VL [44]\n72.6\n74.5\n54.5\nVLBERT [87]\n75.5\n77.9\n58.9\nVILLA [21]\n78.5\n82.6\n65.2\nGPT4RoI-7B [123]\n87.4\n89.6\n78.6\nVistaLLM-13B\n88.2\n89.9\n79.6\n\u2206Ours - GPT4RoI-7B\n0.8 \u2191\n0.3 \u2191\n1.0 \u2191\n(a) Performance on visual com-\nmonsense reasoning (VCR).\nMethod\nAcc.\nTextVQA\nIconQA\nHM\nBLIP-2 [46]\n42.5\n40.6\n53.7\nInstructBLIP [15]\n50.7\n44.8\n57.5\nMiniGPT-4 [126]\n19.9\n37.6\n\u2212\nLLaVA [57]\n38.9\n43.0\n\u2212\nMiniGPT-v2 [8]\n51.9\n47.7\n58.2\nVistaLLM-13B\n53.0\n47.9\n59.1\n\u2206Ours - MiniGPTv2\n1.1 \u2191\n0.2 \u2191\n0.9 \u2191\n(b) Performance on novel tasks -\nTextVQA, IconQA, and HM.\nTable 6.\nResults on (a) VCR, and (b) three novel tasks -\nTextVQA, IconQA, hateful memes (HM). VistaLLM achieves\nconsistent gains over existing baselines.\nMethod\nPASCAL\nMethod\nMSRC\nMethod\niCoSeg\nAv. P\nAv. J\nAv. P\nAv. J\nAv. J\nQuan et al. [80]\n89.0\n52.0\nRubinstein et al. [84]\n92.2\n74.7\nRubinstein et al. [84]\n70.2\nJerripothula et al. [32]\n80.1\n40.0\nFaktor et al. [20]\n92.0\n77.0\nFaktor et al. [20]\n73.8\nLi et al. [40]\n94.1\n63.0\nChen et al. [7]\n\u2212\n73.9\nJerripothula et al. [31]\n70.4\nZhang et al. [121]\n94.9\n71.0\nLi et al. [50]\n95.4\n82.9\nZhang et al. [121]\n89.2\nCycleSegNet [120]\n96.8\n73.6\nCycleSegNet [120]\n97.9\n87.2\nCycleSegNet [120]\n92.1\nVistaLLM-13B\n97.9\n77.2\nVistaLLM-13B\n98.5\n90.1\nVistaLLM-13B\n95.1\n\u2206Ours - CycleSegNet\n1.1 \u2191\n3.6 \u2191\n\u2206Ours - CycleSegNet\n0.6 \u2191\n2.9 \u2191\n\u2206Ours - CycleSegNet\n3.0 \u2191\nTable 7. Performance on object co-segmentation (CoSeg) on\nthree datasets - PASCAL, MSRC, and iCoSeg. VistaLLM is the\nfirst general-purpose system to address CoSeg and sets a new set-\nof-the-art across all datasets, beating previous specialist models.\nstd splits, improving the general-purpose state-of-the-art by\nover 2.3 points. On image captioning, VistaLLM yields a\nsubstantial gain of 10.9 CIDEr points over the best general-\npurpose baseline [9]. Our model performs on a par with\nfine-tuned specialist models, signifying the power of LLMs\nto comprehend and generate strong language descriptions.\nREC, RES, GREC & GRES: Next, we evaluate VistaLLM\non four single-image grounding tasks. Table 3 shows the\nresults of referring expression comprehension (REC) and\nreferring expression segmentation (RES), which aims to\nground (detect and segment, respectively) one object in the\nimage described by an input expression. Our model shows\npromising performance on REC, improving over existing\nbaselines across all evaluation splits. VistaLLM is the first\ngeneral-purpose system to report results on RES, where\nwe perform as good as fine-tuned specialist models. Such\nstrong results on grounding tasks can be attributed to refined\nimage features, effective sampling techniques, and detailed\ninput instructions. We also evaluate VistaLLM on GREC &\nGRES, where the output can contain zero, one, or multiple\nboxes and masks. As shown in Table 4, besides generating\nhigh-quality boxes and masks, our model yields an impres-\nsive gain of 34.7% and 20.7% N-acc scores over MDETR\n[36], reflecting the ability of VistaLLM to detect samples\nwithout any matching objects in the image.\nPQA & BQA: Table 5 shows our performance on point\nquestion answering (PQA) and box question answering\n(BQA), which can have coordinate points and bounding\nboxes as input and output. LookTwice-QA asks the model\nto answer a question about a specified region, either men-\ntioning a point or a box. The system needs to comprehend\nthe area in the context of the whole image, e.g., \u201cHow many\nMethod\nGeneral-\npurpose?\nNLVR\ndev\ntest-P\nVisualBERT [47]\n\u2717\n67.4\n67.0\nSOHO [28]\n\u2717\n76.3\n77.3\nOscar [51]\n\u2717\n78.1\n78.4\nUniter [12]\n\u2717\n77.2\n77.9\nVILLA [21]\n\u2717\n78.4\n79.3\nALBEF [45]\n\u2717\n80.2\n80.5\nVistaLLM-13B\n\u2713\n81.7\n82.1\n\u2206Ours - ALBEF\n\u2212\n1.5 \u2191\n1.6 \u2191\n(a) Results on NLVR.\nMethod\nR\nP\nA\nF1\nF1\nF1\nmPLUG-Owl\n68.4\n66.9\n66.8\nLLaVA [57]\n66.6\n66.4\n66.3\nMiniGPT4 [126]\n80.2\n73.0\n70.4\nInstructBLIP [15]\n89.3\n84.7\n77.3\nShikra-7B [9]\n86.2\n83.2\n82.5\nFerret-13B [115]\n89.8\n84.2\n82.0\nVistaLLM-13B\n90.5\n84.8\n82.9\n\u2206Ours - Ferret-13B\n0.7 \u2191\n0.6 \u2191\n0.9 \u2191\n(b) Results on POPE.\nTable 8. Performance on (a) NLVR, and (b) object hallucina-\ntion benchmark using POPE evaluation pipeline. VistaLLM is\nthe first general-purpose model to address NLVR, and beats strong\nfine-tuned models. VistaLLM demonstrates an intriguing property\nof alleviating object hallucinations across all three splits. R: Ran-\ndom, P: Popular, A: Adversarial.\n8 12 16\n24\n32\n75\n85\n95\n# Sampling Points\nmIoU Upper Bound\nAdaptive\nUniform\n(a) mIoU upper bound on Ref val\nset with varying number of points.\n8 12 16\n24\n32\n50\n60\n70\n80\n# Samping Points\nmIoU\nRef val\nRef+ val\n(b) mIoU by VistaLLM on Ref,\nRef+ with varying number of points.\nFigure 4. Ablative experiments on RES task. (a) Comparison\nof the highest possible mIoU by adaptive and uniform sampling,\nindicating lesser information loss in adaptive sampling, (b) Effect\nof number of sampled points on the performance of VistaLLM.\nof these [cars] are there in the image?\u201d Visual-7W contains\nMCQs where the model needs to choose a box from four\noptions. VistaLLM sets new state-of-the-art on both tasks,\nproving its mighty region-referring ability.\nVCR & Novel (Unseen) Tasks: Table 6a shows results\non visual commonsense reasoning (VCR) - a single-image\nfine-grained reasoning task containing questions with refer-\nring bounding boxes. VistaLLM produces 1% improvement\nover GPT4RoI [123] in the most challenging Q \u2192 AR\nsetting.\nWe also access our model\u2019s generalization abil-\nity by evaluating it on three novel tasks in Table 6b -\nTextVQA, IconQA, and hateful memes (HM). VistaLLM\nachieves strong results on all three benchmarks, proving its\nability to comprehend novel tasks given well-designed in-\nstructions.\nCoSeg & NLVR: Table 7 and Table 8a shows the per-\nformance on two multi-image tasks, CoSeg and NLVR.\nVistaLLM is the first general-purpose model to evaluate\nboth tasks. Given a group of images with a common ob-\nject, CoSeg aims to recognize and segment the object in ev-\nery photo. VistaLLM outperforms existing specialist base-\nlines across three different datasets on CoSeg, showing its\nstrong perception and grounding ability.\nVistaLLM also\nbeats powerful fine-tuned models [21, 45] on NLVR, which\n7\nFigure 5. Examples demonstrating VistaLLM\u2019s capability for single and multi-image reasoning and grounding tasks. More visual-\nizations are shown in Appendix H. Best viewed when zoomed in and in color.\nMethod\nCap.\nRES Ref\nVCR\niCoSeg\nNLVR\nCIDEr\nval\ntestA\ntestB\nQ \u2192 AR\nAv. J\ndev\nVistaLLM-13B\n128.4\n76.2\n77.7\n73.9\n79.6\n95.1\n80.7\nw/o Tokenizer\n123.4\n72.1\n73.8\n69.0\n73.2\n89.7\n77.3\nw/o Tokenizer PT\n127.0\n75.1\n76.5\n72.6\n78.5\n94.8\n79.5\nTable 9. Ablation study on instruction-guided image tokenizer,\nwhich improves results by refining global image embeddings.\naim to reason two input images and answer a query. These\nresults prove the versatility of VistaLLM with more than\none input image, which is crucial for real-world use cases.\nPOPE: We evaluate VistaLLM on POPE object hallucina-\ntion benchmark in Table 8b, where we perform comparably\nto strong general-purpose models like Shikra [9], and Fer-\nret [115] across all metrics and splits, and vastly outperform\nmany previous baselines. These results exhibit our model\u2019s\nability to power against the hallucination problem, essential\nfor its generalized applicability.\n5.4. Ablation Study\nAdaptive vs. Uniform Sampling: We ablate the quan-\ntitative effectiveness of our proposed adaptive sampling\nmethod compared to uniform sampling for referring expres-\nsion segmentation (RES) in Figure 4.\nWith 32 sampled\npoints, the maximum achievable mIoU score on Ref val\nset by adaptive technique is 97.26, while for uniform sam-\npling, 94.70. However, with fewer sampling points, both\nmethods perform significantly worse. Figure 4b shows that\nthe performance of VistaLLM also improves using adaptive\nsampling on both Ref and Ref+ val splits, which shows the\nusefulness of the proposed sampling scheme.\nNumber of Sampled Points: Figure 4b shows that with\na higher number of sampled points, the performance of\nVistaLLM significantly improves for both Ref and Ref+.\nWhen increasing the number of points from 16 to 32,\nVistaLLM gains 3.6 on Ref and 4.5 on Ref+.\nInstruction-guided Image Tokenizer: We ablate the im-\nportance of the proposed instruction-guided tokenizer in Ta-\nble 9. The performance of iCoSeg significantly drops by 5.4\nJ -index without the tokenizer module. We also see similar\neffects in captioning, RES, VCR, and NLVR. When using\nQFormer without pre-trained weights, we observe a sub-\nstantial drop in all tasks except iCoSeg.\nLLM Size: Table 2, 3, 4 shows that larger LLM backbone\ngenerally helps improve the performance. We show ablation\non the training dataset and image encoder in Appendix F.\n5.5. Qualitative Results and Error Analysis\nFigure 5 visualizes sample results from VistaLLM for single\nand multi-image reasoning and grounding tasks. As shown\nin the NLVR and AttCoSeg examples, VistaLLM can suc-\ncessfully parse all input images and comprehend the rela-\ntion among them. It can also successfully ground all re-\nferred objects in foreground and background, as shown in\nGRES. However, compared to the recently released GPT-\n4V [112], we perform worse in general and knowledge-\nbased question answering, which can be attributed to the\nbillion scale pre-training of GPT. Nevertheless, VistaLLM\u2019s\nability to reason over several images and perform precise\ndetection and segmentation makes it unique.\n6. Conclusion\nWe introduce VistaLLM, a powerful general-purpose vi-\nsion system that integrates coarse- and fine-grained vision-\nlanguage reasoning and grounding tasks over single and\nmultiple input images into a unified framework.\nTo fil-\nter embeddings from various images, VistaLLM uses a\nlanguage-guided image tokenizer, which provides com-\npressed and refined features following the task description.\nWe also employ a gradient-aware adaptive sampling tech-\n8\nnique to efficiently represent binary segmentation masks as\nsequences, significantly improving previously used uniform\nsampling. We conduct extensive experiments to show the\neffectiveness of VistaLLM on a wide range of downstream\ntasks, consistently achieving state-of-the-art performance.\n7. Acknowledgement\nThe codebase for this work is built on the LLaVA [57] and\nShikra [9] repository. We would like to thank the respec-\ntive authors for their contribution, and the Meta AI team for\ndiscussions and feedback. Shraman Pramanick and Rama\nChellappa were partially supported by a ONR MURI grant\nN00014-20-1-2787.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a visual language model for few-shot learning.\nIn NeurIPS, pages 23716\u201323736, 2022. 2, 5\n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi\nParikh. Vqa: Visual question answering. In CVPR, pages\n2425\u20132433, 2015. 2, 15\n[3] Dhruv Batra, Adarsh Kowdle, Devi Parikh, Jiebo Luo, and\nTsuhan Chen. icoseg: Interactive co-segmentation with in-\ntelligent scribble guidance. In CVPR, pages 3169\u20133176,\n2010. 5, 17\n[4] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George Bm\nVan\nDen\nDriessche,\nJean-Baptiste\nLespiau,\nBogdan\nDamoc, Aidan Clark, et al. Improving language models by\nretrieving from trillions of tokens. In ICML, pages 2206\u2013\n2240. PMLR, 2022. 2\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. NeurIPS, 33:1877\u2013\n1901, 2020. 3, 6, 17\n[6] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li,\nMaosong Sun, and Yang Liu.\nPosition-enhanced visual\ninstruction tuning for multimodal large language models.\narXiv preprint arXiv:2308.13437, 2023. 14, 15\n[7] Hong Chen, Yifei Huang, and Hideki Nakayama. Seman-\ntic aware attention based deep object co-segmentation. In\nACCV, pages 435\u2013450. Springer, 2018. 7\n[8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\nLiu,\nPengchuan Zhang,\nRaghuraman Krishnamoorthi,\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\nMinigpt-v2: large language model as a unified interface\nfor vision-language multi-task learning.\narXiv preprint\narXiv:2310.09478, 2023. 1, 2, 3, 4, 6, 7, 14, 15\n[9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm\u2019s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 15\n[10] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-\noffrey Hinton. Pix2seq: A language modeling framework\nfor object detection. In ICLR, 2021. 2, 4, 14\n[11] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J\nFleet, and Geoffrey E Hinton. A unified sequence interface\nfor vision tasks. NeurIPS, 35:31333\u201331346, 2022. 2, 4, 14\n[12] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUniter: Universal image-text representation learning.\nIn\nECCV, pages 104\u2013120. Springer, 2020. 7\n[13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt\nquality. See https://vicuna. lmsys. org (accessed 14 April\n2023), 2023. 3, 6\n[14] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 1, 2\n[15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip: Towards general-\npurpose vision-language models with instruction tuning. In\nNeurIPS, 2023. 1, 2, 6, 7, 15, 19\n[16] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan\nZhang,\nJianfeng Wang,\nLinjie Li,\nZicheng Liu,\nCe\nLiu, Yann LeCun, Nanyun Peng, et al.\nCoarse-to-fine\nvision-language pre-training with fusion in the backbone.\nNeurIPS, 35:32942\u201332956, 2022. 1, 5\n[17] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuo-\nhang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan\nZhang, Lu Yuan, Nanyun Peng, et al. An empirical study of\ntraining end-to-end vision-and-language transformers. In\nCVPR, pages 18166\u201318176, 2022. 1, 5\n[18] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi\nZhou, Adams Wei Yu, Orhan Firat, et al.\nGlam: Effi-\ncient scaling of language models with mixture-of-experts.\nIn ICML, pages 5547\u20135569. PMLR, 2022. 2\n[19] Constantin Eichenberg,\nSidney Black,\nSamuel Wein-\nbach, Letitia Parcalabescu, and Anette Frank.\nMagma\u2013\nmultimodal augmentation of generative models through\nadapter-based finetuning.\nIn Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages 2416\u2013\n2428, 2022. 2\n[20] Alon Faktor and Michal Irani. Co-segmentation by compo-\nsition. In ICCV, pages 1297\u20131304, 2013. 2, 5, 7, 17\n[21] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,\nand Jingjing Liu.\nLarge-scale adversarial training for\nvision-and-language representation learning. In NeurIPS,\npages 6616\u20136628, 2020. 7\n[22] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-\nangyu Yue, et al.\nLlama-adapter v2: Parameter-efficient\nvisual instruction model. arXiv preprint arXiv:2304.15010,\n2023. 1, 2\n9\n[23] Shuting He, Henghui Ding, Chang Liu, and Xudong Jiang.\nGrec:\nGeneralized referring expression comprehension.\narXiv preprint arXiv:2308.16182, 2023. 5, 16\n[24] Simao Herdade, Armin Kappeler, Kofi Boakye, and Joao\nSoares.\nImage captioning:\nTransforming objects into\nwords. 2019. 2\n[25] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de\nLas Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, et al. An empirical analysis of compute-optimal large\nlanguage model training. NeurIPS, 35:30016\u201330030, 2022.\n1, 2\n[26] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratud-\ndin, and Hamid Laga.\nA comprehensive survey of deep\nlearning for image captioning. ACM Computing Surveys\n(CsUR), 51(6):1\u201336, 2019. 2\n[27] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor\nDarrell, and Kate Saenko. Modeling relationships in ref-\nerential expressions with compositional modular networks.\nIn CVPR, pages 1115\u20131124, 2017. 6\n[28] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu,\nDongmei Fu, and Jianlong Fu.\nSeeing out of the box:\nEnd-to-end pre-training for vision-language representation\nlearning. In CVPR, pages 12976\u201312985, 2021. 7\n[29] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In CVPR, pages 6700\u20136709, 2019. 2\n[30] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Daniel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\nOpt-iml: Scaling language model instruction meta learn-\ning through the lens of generalization.\narXiv preprint\narXiv:2212.12017, 2022. 2\n[31] Koteswar Rao Jerripothula, Jianfei Cai, and Junsong Yuan.\nImage co-segmentation via saliency co-fusion. IEEE TMM,\n18(9):1896\u20131909, 2016. 7\n[32] Koteswar Rao Jerripothula,\nJianfei Cai,\nJiangbo Lu,\nand Junsong Yuan.\nObject co-skeletonization with co-\nsegmentation.\nIn CVPR, pages 3881\u20133889. IEEE, 2017.\n7\n[33] Dongsheng Jiang, Yuchen Liu, Songlin Liu, Xiaopeng\nZhang, Jin Li, Hongkai Xiong, and Qi Tian. From clip to\ndino: Visual encoders shout in multi-modal large language\nmodels. arXiv preprint arXiv:2310.08825, 2023. 3\n[34] Dongsheng Jiang, Yuchen Liu, Songlin Liu, Xiaopeng\nZhang, Jin Li, Hongkai Xiong, and Qi Tian. From clip to\ndino: Visual encoders shout in multi-modal large language\nmodels. arXiv preprint arXiv:2310.08825, 2023. 2, 14, 15\n[35] Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, and\nTieniu Tan. Locate then segment: A strong pipeline for\nreferring image segmentation. In CVPR, pages 9858\u20139867,\n2021. 6\n[36] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion.\nMdetr-\nmodulated detection for end-to-end multi-modal under-\nstanding. In ICCV, pages 1780\u20131790, 2021. 2, 6, 7\n[37] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj\nGoswami, Amanpreet Singh, Pratik Ringshia, and Davide\nTestuggine. The hateful memes challenge: Detecting hate\nspeech in multimodal memes.\nIn NeurIPS, pages 2611\u2013\n2624, 2020. 5, 17\n[38] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. IJCV, 123:32\u201373, 2017. 2, 5, 16\n[39] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia.\nLisa:\nReasoning seg-\nmentation via large language model.\narXiv preprint\narXiv:2308.00692, 2023. 3, 14, 15\n[40] Bo Li, Zhengxing Sun, Qian Li, Yunjie Wu, and Anqi Hu.\nGroup-wise deep object co-segmentation with co-attention\nrecurrent neural network.\nIn CVPR, pages 8519\u20138528,\n2019. 7\n[41] Bo Li, Lv Tang, Senyun Kuang, Mofei Song, and Shouhong\nDing. Toward stable co-saliency detection and object co-\nsegmentation. IEEE TIP, 31:6532\u20136547, 2022. 5\n[42] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter: A multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 2\n[43] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung\nPoon, and Jianfeng Gao.\nLlava-med: Training a large\nlanguage-and-vision assistant for biomedicine in one day.\nIn NeurIPS, 2023. 2\n[44] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin\nJiang. Unicoder-vl: A universal encoder for vision and lan-\nguage by cross-modal pre-training. In AAAI, pages 11336\u2013\n11344, 2020. 7\n[45] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.\nAlign before fuse:\nVision and language representation\nlearning with momentum distillation. In Advances in neural\ninformation processing systems, pages 9694\u20139705, 2021. 7\n[46] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2: bootstrapping language-image pre-training with\nfrozen image encoders and large language models.\nIn\nICML, 2023. 1, 2, 3, 6, 7, 19\n[47] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\nand Kai-Wei Chang.\nVisualbert: A simple and perfor-\nmant baseline for vision and language.\narXiv preprint\narXiv:1908.03557, 2019. 7\n[48] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training.\nIn CVPR, pages 10965\u2013\n10975, 2022. 1\n[49] Muchen Li and Leonid Sigal.\nReferring transformer: A\none-step approach to multi-task visual grounding. NeurIPS,\n34:19652\u201319664, 2021. 6\n[50] Weihao Li, Omid Hosseini Jafari, and Carsten Rother.\nDeep object co-segmentation. In ACCV, pages 638\u2013653.\nSpringer, 2019. 7\n[51] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong,\n10\nFuru Wei, et al.\nOscar: Object-semantics aligned pre-\ntraining for vision-language tasks. In ECCV, pages 121\u2013\n137. Springer, 2020. 7\n[52] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucination in\nlarge vision-language models. In EMNLP, 2023. 5, 17\n[53] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feicht-\nenhofer, and Kaiming He.\nScaling language-image pre-\ntraining via masking. In CVPR, pages 23390\u201323400, 2023.\n1\n[54] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In ECCV, pages 740\u2013755, 2014. 2, 5, 15\n[55] Chang Liu, Xudong Jiang, and Henghui Ding. Instance-\nspecific feature propagation for referring segmentation.\nIEEE TMM, 2022. 6\n[56] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Gen-\neralized referring expression segmentation. In CVPR, pages\n23592\u201323601, 2023. 5, 16\n[57] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. In NeurIPS, 2023. 1, 2, 3,\n5, 7, 9, 15, 16\n[58] Jingyu Liu, Liang Wang, and Ming-Hsuan Yang. Referring\nexpression generation and comprehension via attributes. In\nICCV, pages 4856\u20134864, 2017. 16\n[59] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Ku-\nmar Satzoda, Vijay Mahadevan, and R Manmatha. Poly-\nformer: Referring image segmentation as sequential poly-\ngon generation. In CVPR, pages 18653\u201318663, 2023. 2, 4,\n6, 14\n[60] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi\nWang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun\nLi, Jiashuo Yu, et al.\nInternchat: Solving vision-centric\ntasks by interacting with chatbots beyond language. arXiv\npreprint arXiv:2305.05662, 2023. 3\n[61] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts. In ICLR, 2017. 6\n[62] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 6\n[63] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vil-\nbert: Pretraining task-agnostic visiolinguistic representa-\ntions for vision-and-language tasks. NeurIPS, 32, 2019. 7\n[64] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi\nParikh, and Stefan Lee.\n12-in-1: Multi-task vision and\nlanguage representation learning.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 10437\u201310446, 2020. 6\n[65] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh\nMottaghi, and Aniruddha Kembhavi. Unified-io: A uni-\nfied model for vision, language, and multi-modal tasks. In\nICLR, 2022. 5\n[66] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,\nWei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu.\nIconqa: A new benchmark for abstract diagram understand-\ning and visual language reasoning. In NeurIPS Datasets\nand Benchmarks Track, 2021. 2, 5, 17\n[67] Gen Luo, Yiyi Zhou, Rongrong Ji, Xiaoshuai Sun, Jinsong\nSu, Chia-Wen Lin, and Qi Tian. Cascade grouped atten-\ntion network for referring expression segmentation. In ACM\nMM, pages 1274\u20131282, 2020. 6\n[68] Gen Luo,\nYiyi Zhou,\nXiaoshuai Sun,\nLiujuan Cao,\nChenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task\ncollaborative network for joint referring expression com-\nprehension and segmentation.\nIn CVPR, pages 10034\u2013\n10043, 2020. 6\n[69] Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Rus-\nsakovsky. Point and ask: Incorporating pointing into vi-\nsual question answering. arXiv preprint arXiv:2011.13681,\n2020. 5, 6, 16\n[70] Ron Mokady, Amir Hertz, and Amit H Bermano.\nClip-\ncap: Clip prefix for image captioning.\narXiv preprint\narXiv:2111.09734, 2021. 2\n[71] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis.\nModeling context between objects for referring expression\nunderstanding. In Computer Vision\u2013ECCV 2016: 14th Eu-\nropean Conference, Amsterdam, The Netherlands, Octo-\nber 11\u201314, 2016, Proceedings, Part IV 14, pages 792\u2013807.\nSpringer, 2016. 16\n[72] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774,\n2023. 6\n[73] TB OpenAI. Chatgpt: Optimizing language models for di-\nalogue. openai, 2022. 1, 2\n[74] Guilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza Alobei-\ndli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Lau-\nnay. The refinedweb dataset for falcon llm: outperforming\ncurated corpora with web data, and web data only. arXiv\npreprint arXiv:2306.01116, 2023. 1, 2\n[75] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 2, 3, 4, 14, 15\n[76] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong,\nJipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, and\nLingpeng Kong Tong Zhang. Detgpt: Detect what you need\nvia reasoning. arXiv preprint arXiv:2305.14167, 2023. 14,\n15\n[77] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. In CVPR,\npages 2641\u20132649, 2015. 2, 5, 16\n[78] Shraman Pramanick, Li Jing, Sayan Nag, Jiachen Zhu,\nHardik Shah, Yann LeCun, and Rama Chellappa. Volta:\nVision-language transformer with weakly-supervised local-\nfeature alignment. In TMLR, 2023. 1\n[79] Shraman\nPramanick,\nYale\nSong,\nSayan\nNag,\nKevin Qinghong Lin, Hardik Shah, Mike Zheng Shou,\nRama Chellappa, and Pengchuan Zhang.\nEgovlpv2:\nEgocentric video-language pre-training with fusion in the\nbackbone. In ICCV, pages 5285\u20135297, 2023. 1\n[80] Rong Quan, Junwei Han, Dingwen Zhang, and Feiping Nie.\nObject co-segmentation via graph optimized-flexible mani-\nfold ranking. In CVPR, pages 687\u2013695, 2016. 7\n11\n[81] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748\u20138763. PMLR, 2021. 1\n[82] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jer-\nret Ross, and Vaibhava Goel. Self-critical sequence training\nfor image captioning. In CVPR, pages 7008\u20137024, 2017. 5\n[83] Carsten Rother, Tom Minka, Andrew Blake, and Vladimir\nKolmogorov. Cosegmentation of image pairs by histogram\nmatching-incorporating a global constraint into mrfs.\nIn\nCVPR, pages 993\u20131000, 2006. 5\n[84] Michael Rubinstein, Armand Joulin, Johannes Kopf, and\nCe Liu. Unsupervised joint object discovery and segmenta-\ntion in internet images. In CVPR. 7\n[85] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki.\nLaion-\n400m: Open dataset of clip-filtered 400 million image-text\npairs. arXiv preprint arXiv:2111.02114, 2021. 6, 19\n[86] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In CVPR,\npages 8317\u20138326, 2019. 2, 5, 17\n[87] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-\nlinguistic representations. In ICLR, 2019. 7\n[88] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A\ncorpus of natural language for visual reasoning. In ACL,\npages 217\u2013223, 2017. 5\n[89] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Hua-\njun Bai, and Yoav Artzi.\nA corpus for reasoning about\nnatural language grounded in photographs. In ACL, pages\n6418\u20136428, 2019. 5\n[90] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao.\nEva-clip: Improved training techniques for clip at\nscale. arXiv preprint arXiv:2303.15389, 2023. 3, 6, 17,\n19\n[91] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew Poul-\nton, Viktor Kerkez, and Robert Stojnic.\nGalactica:\nA large language model for science.\narXiv preprint\narXiv:2211.09085, 2022. 2\n[92] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. arXiv preprint arXiv:2302.13971, 2023. 1, 3\n[93] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023. 1, 2\n[94] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-\nlami, Oriol Vinyals, and Felix Hill. Multimodal few-shot\nlearning with frozen language models. Advances in Neural\nInformation Processing Systems, 34:200\u2013212, 2021. 2\n[95] Sanh Victor, Webson Albert, Raffel Colin, Bach Stephen,\nSutawika Lintang, Alyafeai Zaid, Chaffin Antoine, Stiegler\nArnaud, Raja Arun, Dey Manan, et al. Multitask prompted\ntraining enables zero-shot task generalization.\nIn ICLR,\n2022. 2\n[96] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang.\nOfa: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learning\nframework. In ICML, pages 23318\u201323340. PMLR, 2022. 6\n[97] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, et al. Image as\na foreign language: Beit pretraining for vision and vision-\nlanguage tasks. In CVPR, pages 19175\u201319186, 2023. 1\n[98] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu\nQiao, et al. Visionllm: Large language model is also an\nopen-ended decoder for vision-centric tasks. In NeurIPS,\n2023. 2, 5, 6, 14, 15\n[99] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, et al. Cogvlm: Visual expert for pretrained language\nmodels. arXiv preprint arXiv:2311.03079, 2023. 3, 14, 15\n[100] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhen-\nhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu,\nZhiguo Cao, et al. The all-seeing project: Towards panop-\ntic visual recognition and understanding of the open world.\narXiv preprint arXiv:2308.01907, 2023. 3\n[101] Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei\nZhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang,\nChenguang Zhu, Derek Hoiem, et al.\nLanguage models\nwith image descriptors are strong few-shot video-language\nlearners. Advances in Neural Information Processing Sys-\ntems, 35:8483\u20138497, 2022. 2\n[102] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yan-\ndong Guo, Mingming Gong, and Tongliang Liu.\nCris:\nClip-driven referring image segmentation. In CVPR, pages\n11686\u201311695, 2022. 6\n[103] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\nQuoc V Le. Finetuned language models are zero-shot learn-\ners. In ICLR, 2021. 2\n[104] John Winn, Antonio Criminisi, and Thomas Minka. Ob-\nject categorization by learned universal visual dictionary.\nIn ICCV, pages 1800\u20131807, 2005. 5, 17\n[105] BigScience Workshop,\nTeven Le Scao,\nAngela Fan,\nChristopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hess-\nlow, Roman Castagn\u00b4e, Alexandra Sasha Luccioni, Franc\u00b8ois\nYvon, et al. Bloom: A 176b-parameter open-access multi-\nlingual language model. arXiv preprint arXiv:2211.05100,\n2022. 2\n[106] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong\nWang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talk-\ning, drawing and editing with visual foundation models.\narXiv preprint arXiv:2303.04671, 2023. 14, 15\n[107] Yixuan Wu, Zhao Zhang, Chi Xie, Feng Zhu, and Rui Zhao.\n12\nAdvancing referring expression segmentation beyond sin-\ngle image. In ICCV, pages 2628\u20132638, 2023. 5, 17\n[108] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and\nCordelia Schmid. Zero-shot video question answering via\nfrozen bidirectional language models. Advances in Neural\nInformation Processing Systems, 35:124\u2013141, 2022. 2\n[109] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,\nFaisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nUnitab: Unifying text and box outputs for grounded vision-\nlanguage modeling. In ECCV, pages 521\u2013539. Springer,\n2022. 1, 6\n[110] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,\nYumao Lu, Zicheng Liu, and Lijuan Wang. An empirical\nstudy of gpt-3 for few-shot knowledge-based vqa. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence,\npages 3081\u20133089, 2022. 2\n[111] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-\nshuang Zhao, and Philip HS Torr. Lavt: Language-aware\nvision transformer for referring image segmentation.\nIn\nCVPR, pages 18155\u201318165, 2022. 6\n[112] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang.\nThe\ndawn of lmms:\nPreliminary explorations with gpt-4v\n(ision). arXiv preprint arXiv:2309.17421, 9, 2023. 8\n[113] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,\nEhsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,\nMichael Zeng, and Lijuan Wang.\nMm-react: Prompting\nchatgpt for multimodal reasoning and action. arXiv preprint\narXiv:2303.11381, 2023. 2\n[114] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 2\n[115] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du,\nBowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu\nChang, and Yinfei Yang.\nFerret:\nRefer and ground\nanything anywhere at any granularity.\narXiv preprint\narXiv:2310.07704, 2023. 1, 2, 3, 4, 6, 7, 8, 14, 15\n[116] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling context in referring expres-\nsions. In ECCV. Springer, 2016. 16\n[117] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu,\nMohit Bansal, and Tamara L Berg. Mattnet: Modular at-\ntention network for referring expression comprehension. In\nCVPR, pages 1307\u20131315, 2018. 6\n[118] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\nFrom recognition to cognition: Visual commonsense rea-\nsoning. In CVPR, pages 6720\u20136731, 2019. 2, 5, 16\n[119] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,\nXiao Xia, et al. Glm-130b: An open bilingual pre-trained\nmodel. In ICLR, 2022. 2\n[120] Chi Zhang, Guankai Li, Guosheng Lin, Qingyao Wu, and\nRui Yao. Cyclesegnet: Object co-segmentation with cy-\ncle refinement and region correspondence. IEEE TIP, 30:\n5652\u20135664, 2021. 2, 7\n[121] Kaihua Zhang, Jin Chen, Bo Liu, and Qingshan Liu. Deep\nobject co-segmentation via spatial-semantic network mod-\nulation. In AAAI, pages 12813\u201312820, 2020. 7\n[122] Susan Zhang,\nStephen Roller,\nNaman Goyal,\nMikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open\npre-trained transformer language models.\narXiv preprint\narXiv:2205.01068, 2022. 2\n[123] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi:\nInstruction tuning large language model on region-of-\ninterest. arXiv preprint arXiv:2307.03601, 2023. 1, 2, 4,\n6, 7, 14, 15\n[124] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi\nFeng, and Bingyi Kang. Bubogpt: Enabling visual ground-\ning in multi-modal llms. arXiv preprint arXiv:2307.08581,\n2023. 3, 14, 15\n[125] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo,\nXingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xi-\naoshuai Sun, and Rongrong Ji. Seqtr: A simple yet univer-\nsal network for visual grounding. In ECCV, pages 598\u2013615.\nSpringer, 2022. 2, 4, 6, 14\n[126] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 1, 2, 7\n[127] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\nFei. Visual7w: Grounded question answering in images.\nIn CVPR, pages 4995\u20135004, 2016. 5, 6, 17\n13\nAxis\nMetric and Split\nCOCO Cap\nCIDEr on Karpathy test\nVQAv2\nAccuracy on val\nVCR\nAccuracy on val in Q \u2192 AR setup\nPOPE\nF1 score on Random split\nHM\nAccuracy on test\nTextVQA\nAccuracy on test\nREC\nPrecision@IoU=0.5 on RefCOCO val\nRES\nmIoU on RefCOCO val\nGREC\nPrecision on RefCOCO val\nGRES\ngIoU on RefCOCO val\nPointQA\nAccuracy on Visual7W\nNLVR2\nAccuracy on dev\nIconQA\nAccuracy on test\niCoSeg\nAverage Jaccard index (J ) on test\nTable A.1. Details of the reported metrics and split information\nin every axis of the radar plot in Figure 1. Red: Single-image\ncoarse-level tasks, Blue: Single-image region-level tasks, Olive-\nGreen: Multi-image coarse-level tasks, and Plum: Multi-image\nregion-level tasks.\nA. Radar Chart Figure 1 Details\nIn this section, we explain the details of the radar chart in\nFigure 1, which summarizes the comparative performance\nof VistaLLM with MiniGPT-v2 [8], Ferret [115], Shikra [9]\nand GPT4RoI [123]. None of these baselines address seg-\nmentation and multi-image tasks using a single framework.\nFirst, for illustrative purposes, we normalize each axis by\nthe score achieved by VistaLLM, which turns the axes in the\nrange (0, 1]. Next, we choose the origin of each axes suit-\nably to distinctly separate the the inner and outer frames for\nbetter readability. For PointQA, REC, and COCO Cap, the\norigin is at 0.97, 0.96, and 0.75 normalized values, respec-\ntively. For all remaining axes, the origin is at 0.92 normal-\nized value. Finally, we annotate each vertex with absolute\nperformance metric scores. The reported metric and split\nname for each axis are listed in Table A.1.\nB. Adaptive Sampling Algorithm\nThe algorithm of the proposed gradient-aware adaptive\nsampling technique is given in Algorithm 1. Section 3.2\nof the main manuscript provides details of each step.\nC. VistaLLM vs Existing Region-level MLLMs\nWith the fast progress of region-level general-purpose vi-\nsion systems, works such as GPT4RoI [123], Shikra [9],\nVisionLLM [98], KOSMOS-2 [75] and Ferret [115] re-\nsemble VistaLLM, as they also aim to unify tasks with\ndifferent granularity in a unified system.\nAdditional re-\nlated works in this category includes PVIT [6], COMM\n[34], CogVLM [99] and MiniGPT-v2 [8]. Moreover, meth-\nods like Visual ChatGPT [106], BuboGPT [124], DetGPT\n[76], and LISA [39] employ external additional detection\nand segmentation modules to unify fine-grained tasks in\na two-stage approach. Nevertheless, there exist clear dif-\nAlgorithm 1 Gradient-aware Adaptive Sampling\nRequire: Mask contour C\nNumber of dense points M\nFinal number of sampling points N (N \u226a M)\n[p1, . . . , pM] \u2190 Uniform-Sample(C)\n\u25b7 Contour Discretization\nfor i \u2208 {1, . . . , M} do\n\u20d7l1 = Join(pi, pi\u22121)\n\u20d7l2 = Join(pi\u22121, pi+1)\n\u03b8i = \u2220\u20d7l1\u20d7l2\n\u25b7 Gradient Calculation\nend for\nFinalpoints \u2190 []\nindices \u2190 argsort(\u03b8i\u2208{1,...,M})[M-N:]\n\u25b7 Sorting\nfor j \u2208 indices do\npj \u2190 Quantize(pj)\nAddItem(Finalpoints, pj)\n\u25b7 Quantization\nend for\nFinalpoints is the final list of sampled points.\nferences between VistaLLM from existing methods. First,\nwe present the first general-purpose system to support all\npossible input and output formats, e.g., multiple images,\nnatural language, coordinate points, bounding boxes, seg-\nmentation masks as inputs, and free-flowing text, points,\nboxes, and masks as output.\nTable C.1 shows a side-\nby-side comparison of input-output formats of all exist-\ning baselines.\nWhile Ferret supports boxes, points, and\nmasks in the input, it can not generate a mask as output\nand, hence, can not address the segmentation task. On the\nother hand, VisionLLM can solve segmentation but can-\nnot process points, boxes, and masks in input and can not\nsolve REG, BoxQA, and PointQA. Second, unlike all ex-\nisting works, VistaLLM supports multi-image input, en-\nabling us to reason and ground over more than one im-\nage and solve tasks like NLVR and CoSeg.\nOur pro-\nposed instruction-guided image tokenizer module refines\nand compresses the global image embeddings of multiple\nimages, helping VistaLLM to filter the necessary visual in-\nformation required for the current task. Table C.2 systemat-\nically illustrates the capability of VistaLLM to solve a wide\nrange of image-level and region-level tasks over single and\nmultiple input images compared to previous systems. Third,\nto efficiently convert segmentation masks into sequences,\nwe propose a gradient-aware adaptive contour sampling\nscheme, which improves over previously used uniform sam-\npling approach [10, 11, 59, 125] by 3 \u2212 4 mIoU scores on\ndifferent segmentation benchmarks.\nLastly, we collect a\nnew training benchmark CoinIt, containing 6.8M training\nsamples and propose a new task, AttCoSeg (Attribute-level\nCo-Segmentation) which addresses the lack of publicly-\navailable multi-image region-level datasets. Our proposed\nsystem achieves stronger performance across 15 different\nevaluation benchmarks, including mitigating object halluci-\nnation to a significant extent.\n14\nModel\nInput Type\nOutput Type\nMultiple\nImages\nText Points Boxes Masks Text Points Boxes Masks\nTwo-\nStage\nVisual ChatGPT [106]\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\nBuboGPT [124]\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\nDetGPT [76]\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\nLISA [39]\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\nEnd-to-\nEnd\nLLaVa [57]\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\nInstructBLIP [15]\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\nGPT4RoI [123]\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\nKOSMOS-2 [75]\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\nVisionLLM [98]\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\nShikra [9]\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\nPVIT [6]\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\nCogVLM [99]\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\nCOMM [34]\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\nMiniGPT-v2 [8]\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\nFerret [115]\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\nVistaLLM\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable C.1. Comparison of VistaLLM vs. existing general-purpose vision systems regarding input and output types. VistaLLM\nsupports all possible formats, including multiple images, natural language, points, bounding boxes, segmentation masks as inputs, and\nfree-flowing text, points, boxes, and masks as output.\nModel\nImage-level Tasks\nRegion-level Tasks\nSingle-image\nMulti-image\nSingle-image\nMulti-image\nVQAv2 &\nCaptioning Reasoning\nReasoning\nBoxQA PointQA Detection Segmentation Multi-instance\nSegmentation\nCoSeg\nTwo-\nStage\nVisual ChatGPT [106]\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\nBuboGPT [124]\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\nDetGPT [76]\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\nLISA [39]\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\nEnd-to-\nEnd\nLLaVa [57]\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\nInstructBLIP [15]\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\nGPT4RoI [123]\n\u2713\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\nKOSMOS-2 [75]\n\u2713\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\nVisionLLM [98]\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\nShikra [9]\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\nPVIT [6]\n\u2713\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\nCogVLM [99]\n\u2713\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\nCOMM [34]\n\u2713\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\nMiniGPT-v2 [8]\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\nFerret [115]\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\nVistaLLM\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable C.2. Comparison of VistaLLM vs. existing general-purpose vision systems regarding supported tasks. VistaLLM integrates\na wide range of image-level and region-level vision-language reasoning and grounding tasks over single and multiple input images into a\nunified framework.\nD. Dataset Details\nThis section provides additional details of our training and\nevaluation datasets.\nCOCO Captioning: Captions for the COCO dataset [54]\nwere sourced from Amazon\u2019s Mechanical Turk (AMT),\nwith workers adhering to specified guidelines to ensure cap-\ntion quality. The dataset includes 330,000 images, divided\ninto training, validation, and test categories.\nThese cat-\negories comprise 413,915 captions for 82,783 images in\ntraining, 202,520 captions for 40,504 images in validation,\nand 379,249 captions for 40,775 images in the test set.\nVQAv2: VQAv2 dataset [2] contains a collection of over\n200,000 images, each paired with a portion of the more\nthan 1.1 million questions asked, gathering in total over 11\n15\nmillion responses. The questions cover a wide range, from\nsimple yes/no and counting queries to more complex open-\nended ones.\nRefCOCO & RefCOCO+:\nThe RefCOCO and Ref-\nCOCO+ datasets [58] were created through a two-player\ngame mechanism [116]. RefCOCO features 142,209 de-\nscriptive expressions for 50,000 objects across 19,994 im-\nages, whereas RefCOCO+ includes 141,564 expressions for\n49,856 objects in 19,992 images. Both datasets are divided\ninto training, validation, and two test sets \u2013 Test A and Test\nB. Test A focuses on images with multiple people. At the\nsame time, Test B features images with multiple instances\nof all other objects.\nA key difference between the two\ndatasets is that RefCOCO+ excludes location words from\nits expressions, making it more complex than RefCOCO.\nWe perform referring expression comprehension (REC) and\nreferring expression segmentation (RES) tasks on the Ref-\nCOCO and RefCOCO+ datasets.\nRefCOCOg: The RefCOCOg dataset was assembled using\nAmazon Mechanical Turk, where participants were tasked\nwith crafting natural language descriptions for objects. It\ncomprises 85,474 expressions for 54,822 objects in 26,711\nimages. Notably, the expressions in RefCOCOg are longer\nand more intricate, averaging 8.4 words, in contrast to the\nmore concise expressions in RefCOCO and RefCOCO+,\nwhich average 3.5 words.\nThis complexity makes Ref-\nCOCOg a more challenging dataset. We utilize the UMD\npartition [71] of RefCOCOg, as it provides both validation\nand testing sets, and there is no overlap between training\nand validation images. We address both REC and RES tasks\non RefCOCOg.\ngRefCOCO: The gRefCOCO dataset [23, 56] empowers\ngeneralized referring expression comprehension (GREC)\nand generalized referring expression segmentation (GRES)\ntasks, which address the limitations of classical REC and\nRES problem where there is always one target object. In\ncontrast, GREC and GRES allow expressions to refer to\nan arbitrary number of target objects, including multi-target\nand no-target scenarios, and help bring referring segmen-\ntation into more realistic scenarios with advanced usages.\nThe gRefCOCO dataset contains 278,232 expressions, in-\ncluding 80,022 multi-target and 32,202 no-target expres-\nsions, referring to 60,287 distinct instances in 19,994 im-\nages. Masks and bounding boxes for all target instances are\ngiven. Some of the single-target expressions of gRofCOCO\nare inherited from RefCOCO. We perform both GREC and\nGRES using the gRefCOCO dataset.\nFlickr: The Flickr30K Entities dataset [77] is a pioneer-\ning collection in the field of grounded captioning. It in-\ncludes 31,783 images paired with 158,000 caption anno-\ntations.\nEach caption is carefully annotated, linking ev-\nery noun phrase to a manually outlined referential bound-\ning box. The dataset features a total of 276,000 such anno-\ntated bounding boxes, offering a rich resource for image and\nlanguage processing research. We use Flickr dataset dur-\ning training for spot captioning task, where we instruct the\nmodel to generate a caption of the input image, and locate\nall the objects in the images by drawing bounding boxes.\nVisual Genome: The Visual Genome dataset [38] is a key\nresource for understanding the complex relationships within\nimages. It contains over 100,000 images, with each image\nextensively annotated to capture an average of 21 objects,\n18 attributes, and 18 inter-object relationships. A distinctive\nfeature of this dataset is the alignment of objects, attributes,\nrelationships, and region descriptions with the standardized\nWordNet terminologies. This alignment makes it particu-\nlarly useful for tasks like Region Description and Entity\nRecognition. Each annotated region in the dataset is accom-\npanied by descriptive text, providing a wealth of data for im-\nage understanding and semantic modeling. For referring ex-\npression generation (REG) purposes, we utilize a subset of\nthis dataset, which includes around 180,138 region-caption\npairs.\nVCR: The Visual Commonsense Reasoning (VCR) dataset\n[118] contains 290,000 multiple-choice questions derived\nfrom 110,000 movie scenes. Each scene is paired with a\nquestion demanding common-sense reasoning, an answer,\nand a rationale for that answer. The unique aspect of VCR\nis its requirement for models to not only provide answers\nto complex visual questions but also to explain their rea-\nsoning. This dataset encompasses two sub-tasks: Question\nAnswering (Q \u2192 A), where the model selects the correct\nanswer from four options, and answer justification (QA \u2192\nR), where the model, given a question and its correct an-\nswer, must choose the most fitting rationale from four op-\ntions. Model performance in VCR is assessed using the Q\n\u2192 AR metric, which measures the accuracy of both answer-\ning questions and providing the correct justifications.\nLLaVa: The LLaVA-Instruct-150K2 [57] is a collection\nof 158K unique language-image instruction-following sam-\nples in total, including 58K in conversations, 23K in the\ndetailed description, and 77k in complex reasoning, respec-\ntively. We incorporate the LLaVa dataset during the training\nof our model.\nLookTwiceQA: The LookTwiceQA [69] dataset contains\ntwo different tasks - PointQA and BoxQA. The questions\nare in three different templates - (i) What color is this [re-\ngion]? (ii) What shape is this [region]? and (iii) What\naction is this [region] doing? The question contains either\nan input point or a box with three different granularity of\nobjects - any object, superclass, and object class. The train\nset contains 40,409 questions across 12,867 images, and the\n2https : / / huggingface . co / datasets / liuhaotian /\nLLaVA-Instruct-150K\n16\ntest-dev set contains 5,673 questions across 1,838 images.\nVisual7W: The Visual7W dataset [127] is primarily tai-\nlored for Visual Question Answering (VQA) tasks, featur-\ning a specialized dataset for region-level QA. In Visual7W,\nmodels encounter an image paired with a \u201dwhich\u201d-type\nquestion, for instance, \u201dWhich one is the orange in the\nfruit basket?\u201d. Participants are provided with four bound-\ning boxes in the image and must choose the correct one as\nthe answer. The Visual7W dataset comprises 25,733 images\nand 188,068 such questions.\nTextVQA: TextVQA [86] is a QA dataset containing\n45,336 questions based on 28,408 images, designed to chal-\nlenge models in detecting, interpreting, and reasoning about\ntext present in images to generate accurate answers. We use\nthe TestVQA dataset as an unseen evaluation benchmark.\nIconQA: IconQA [66] measures models\u2019 abstract diagram\nunderstanding and comprehensive cognitive reasoning abil-\nities. We use the test set of its multi-text-choice task, con-\ntaining 6,316 samples, as an unseen evaluation benchmark.\nHateful Memes (HM): The hateful memes dataset [37],\ncontaining more than 10,000 image samples, is a binary\nclassification dataset to justify whether a meme contains\nhateful content. The memes were selected in such a way\nthat strictly unimodal classifiers would struggle to classify\nthem correctly. We use the HM dataset as an unseen evalu-\nation benchmark.\nPOPE: The POPE evaluation benchmark [52] evaluates the\nsevearity of object hallucination problem in MLLMs. POPE\nconsists of three different test splits - popular, random, and\nadversarial- containing around 3,000 samples. Given an im-\nage and a question, \u201dIs there a <object> in the image?\u201d the\nmodel has to answer with \u2019yes\u2019 or \u2019no.\u2019\nNLVR2:\nThe Natural Language for Visual Reasoning\n(NLVR2) corpora, containing 107,292 samples, determine\nwhether a sentence is true about a pair of input images. The\ndata was collected through crowdsourcing, and solving the\ntask requires reasoning about sets of objects, comparisons,\nand spatial relations.\nCoSeg: We use three datasets for object co-segmentation\ntask - PASCAL VOC2010 [20], MSRC [104] and iCoSeg\n[3]. PASCAL contains a total of 1,037 images of 20 ob-\nject classes. MSRC includes seven classes: bird, car, cat,\ncow, dog, plane, and sheep. Each class contains ten images.\niCoseg dataset consists of 643 images from 38 categories.\nLarge variances of viewpoints and deformations are present\nin this dataset.\nAttCoSeg:\nSince the existing object co-segmentation\ndatasets [3, 20, 104] are small-scale and simple to solve,\nwe construct a more challenging larger-scale multi-image\nregion-level dataset. We use Group-wise RES [107] an-\nnotations to sample high-quality images containing objects\nwith similar fine-grained attributes (shape, color, size, posi-\ntion). We refer to such images as positives. While train-\ning VistaLLM, we input these positive image pairs and\nrandomly sampled multiple negative images (which do not\ncontain objects with similar attributes), ask the model to\nrecognize the positive image pair, and segment the object\nwith common traits in both of them. We name this task\nattribute-level co-segmentation (AttCoSeg), which contains\nover 685k training samples, and help VistaLLM to gain sig-\nnificant generalized reasoning and grounding ability over\nmultiple input images.\nE. Examples Instructions for Different Tasks\nSection 5.1 discusses transforming public datasets like\nREC, RES, GREC, and GRES into instruction-following\nformat by employing meticulously crafted task templates.\nThese templates are detailed in Table E.1. We have included\nonly 2-3 examples for each task for brevity. We manually\nwrite one example description of each task and resort to\nGPT-3.5 [5] to create hundreds of variations. During train-\ning, we randomly pick one instruction for each sample.\nF. Additional Ablation Study\nIn this section, we conduct additional ablation experiments\non training dataset, and the image encoder.\nSize of training dataset: We study the effect of increas-\ning training samples for REC and RES tasks in Figure F.1.\nWe start with REC and REG training datasets for the REC\ntask in Figure F.1a, resulting in 0.6M training samples. We\ntrain VistaLLM for two epochs in stage 1, setting all hy-\nperparameters unchanged. In this setup, we observe a REC\nval score of 82.7%. Next, we add Visual Genome data to\nthe training corpus, which results in a total of 1M samples,\nand re-train the model. Now, REC val accuracy increases\nto 84.0%. Similarly, appending PointQA data in the train-\ning corpus increases the performance by 1.3%, and append-\ning LLaVa, Flickr, VQAv2, and COCO caption data yields\na gain of another 0.7%. Finally, the 6.8M training corpus\nproduces a final REC val accuracy of 88.1%. Hence, we ob-\nserve that datasets from other image-level and region-level\ntasks help improve the performance of the REC task, which\nis the benefit of unified end-to-end training. We also see\nsimilar observations for the RES in Figure F.1b. Such a phe-\nnomenon also proves the scalability of our approach, which\nis important for large-scale unified training.\nImage encoder: Next, we ablate different image encoders\nin Table F.1. We observe the best performance across most\ntasks with EVA3 [90], while the CLIP-ViT-L/14-336px4 fol-\n3https://huggingface.co/QuanSun/EVA-CLIP/blob/\nmain/EVA01_CLIP_g_14_psz14_s11B.pt\n4https://huggingface.co/openai/clip-vit-large-\npatch14-336\n17\nTask\nExample Instructions\nCaptioning\n\u2022 Can you give me a brief description of this image <image>?\n\u2022 Give me a short description of the picture <image>.\n\u2022 What\u2019s happening in the image <image> at a glance?\nVQAv2\n\u2022 Looking at the image <image>, can you quickly answer my question: <question>.\n\u2022 After examining the image <image>, can you provide a brief response to the following question: <question>.\n\u2022 Considering the image <image>, please provide a straightforward answer to <question>.\nREC\n\u2022 Locate the object described by <expr> in <image>. There\u2019s just one specific object. Provide the outcome using the\n[x0, y0, x1, y1] arrangement, showing the upper-left and lower-right box positions.\n\u2022 Find the location of the item referenced in <expr> within <image>. We\u2019re referring to a single item. Output the\nresult in [x0, y0, x1, y1] arrangement, showing the upper-left and lower-right bounding box corners.\nRES\n\u2022 Tell me where <expr> is located in <image>. There\u2019s only one object. Provide the coordinates of 32 points on the\nobject\u2019s outline. Present the result in [x0, y0, x1, y1, ..., x31, y31] format.\n\u2022 What is <expr>\u2019s location within <image>? There\u2019s just one thing to consider. Share the coordinates of 32 uniform\npoints on the object\u2019s edge. Show it in [x0, y0, x1, y1, ..., x31, y31] format.\nGREC\n\u2022 Recognize all objects indicated by <expr> in <image>. If no object is located, return an empty string. If one or\nmore objects are located, output the bounding boxes as [x0, y0, x1, y1], indicating the top-left and bottom-right corner\npoints. Use <bsep> to differentiate multiple bounding boxes.\n\u2022 Pinpoint all items referenced by <expr> in <image>. If no object is detected, return an empty string. If one or more\ntarget objects are found, provide the bounding boxes as [x0, y0, x1, y1], signifying the top-left and bottom-right corner\npoints. Use <bsep> to separate multiple bounding boxes.\nGRES\n\u2022 Find all items indicated by <expr> within <image>. If no target object is recognized, produce an empty string. If\none or more target objects are identified, output the coordinates of 32 points along each object\u2019s contour. Display each\nobject mask in [x0, y0, x1, y1, ..., x31, y31] format. Use <msep> to distinguish multiple objects.\n\u2022 Recognize all referenced items via <expr> in <image>. If no target object is found, generate an empty string. If\none or more target objects are found, present the coordinates of 32 points along each object\u2019s edge. Show each object\nmask in [x0, y0, x1, y1, ..., x31, y31] format. Utilize <msep> to distinguish multiple objects.\nREG\n\u2022 Please generate a unique description for the area <objs> displayed in the image <image>.\n\u2022 What can you tell me about the area <objs> in the image <image> that sets it apart from the rest?\n\u2022 How does the area <objs> in <image> stand out uniquely from the rest?\nNLVR\n\u2022 Between the left image <image> and the right image <image>, could you tell me if the answer to <question> is\nTrue or False?\n\u2022 Reviewing both the left image <image> and the right image <image>, would you reckon <question> is True or\nFalse?\n\u2022 Given the left image <image> and the right image <image>, can you answer my query: <question>? Respond in\nTrue or False.\nSpot\nCaptioning\n\u2022 Please provide a holistic description of the image <image> and output the position for each mentioned object in the\nformat [x0, y0, x1, y1] representing top-right and bottom-left corners of the bounding box.\n\u2022 Present a thorough insight into <image> and output every object\u2019s position using [x0, y0, x1, y1], representing the\nbounding box\u2019s top-right and bottom-left corners.\nCoSeg\n\u2022 Find the common object in the input images <image>. There\u2019s only one common object. Display each object\u2019s mask\nin [x0, y0, x1, y1, ..., x31, y31] format. Utilize <msep> to tell the masks apart.\n\u2022 Locate the common thing in the input images <image>. Only one common thing will be there. Present each thing\u2019s\nmask in [x0, y0, x1, y1, ..., x31, y31] style. Use <msep> to differentiate the two masks.\nAttCoSeg\n\u2022 Find the two images which have a common object with matching attributes (shape, color, size, position), and segment\nit in both images. Show object mask in [x0, y0, x1, y1, ..., x31, y31] style in both pictures. Make use of <msep> to tell\napart the two masks.\n\u2022 Which input images have a mutual item with common attributes (shape, color, size, position)? Segment it in both\nimages. Display object mask using [x0, y0, x1, y1, ..., x31, y31] format in both images. Apply <msep> to differentiate\nthe two masks.\nTable E.1. Examples of instructions for different tasks used by VistaLLM to convert them into instruction-following format.\n18\n0.6 1 1.3\n3.4\n4.9\n6.8\n75\n85\n95\n# Training samples (in Millions)\nPrecision @ IoU=0.5\nval\ntestA\ntestB\n(a) Performance of REC on RefCOCO with varying training samples.\nWe report the performance in terms of precision at IoU = 0.5, i.e., the\nprediction is deemed correct if its intersection over union (IoU) with the\nground-truth box is larger than 0.5.\n0.6 1 1.3\n3.1\n5.2\n6.8\n65\n70\n75\n# Training samples (in Millions)\nmIoU\nval\ntestA\ntestB\n(b) Performance of RES on RefCOCO with varying number of train-\ning samples. We report the performance in terms of mIoU score.\nFigure F.1. Ablation on the number of training samples on the\nREC and RES task performance. We start with only RES and\nREC datasets and gradually append datasets from other tasks using\nproper instructions. Increasing the number of samples helps pro-\nduce better performance, showing the usefulness of an end-to-end,\ncohesive, and unified system where different tasks help improve\neach other.\nMethod\nCap.\nRES Ref\nVCR\niCoSeg\nNLVR\nCIDEr\nval\ntestA\ntestB\nQ \u2192 AR\nAv. J\ndev\nVistaLLM-13B\n128.4\n76.2\n77.7\n73.9\n79.6\n95.1\n80.7\nw/ CLIP-ViT-L/14\n127.9\n75.5\n76.3\n72.1\n79.3\n94.7\n80.2\nw/ CLIP-ViT-L/14-336px\n128.4\n76.0\n77.7\n73.6\n79.3\n95.1\n80.5\nw/ CLIP-ViT-B/16\n127.6\n75.1\n76.3\n72.0\n79.0\n94.8\n79.8\nTable F.1.\nAblation with different image encoders.\nBy de-\nfault, VistaLLM uses EVA-CLIP [90] pre-trained on LAION-\n400M [85]. We observe a small performance drop when using\nother image encoders.\nlows closely. We use EVA-CLIP in our final model be-\ncause the QFormer [46] pre-trained in InstructBLIP [15]\nuses EVA-CLIP, and it results in best compatibility with the\ninstruction-guided image tokenizer module in our system.\nG. Error Analysis\nAlthough VistaLLM learns impressive reasoning and\ngrounding capability across many different benchmarks,\nthere are still some cases where the model fails to identify\nsmall and obscured objects, especially in cluttered environ-\nments. Figure G.1 shows seven such failure cases. In the\nRES example, the object \u201cteddy with arm up whose back\nin near brown plaid thing\u201d is hard to comprehend even for\nhumans, and thus, VistaLLM can not identify the correct\n\u201cteddy\u201d the expression is referring to. In the REC example,\nthe \u201cgreen hair tie\u201d is tiny and only visible when zoomed\ninto the picture. VistaLLM fails to identify the girl who\nis wearing it. In the GREC example, in low-light condi-\ntions, the blue hoodie appears to be black, and VistaLLM\nwrongly outputs a bounding box, whereas the ground truth\nis no matching object. Similarly, in the NLVR2, GRES, and\nPOPE examples, VistaLLM fails to recognize hindered and\ncluttered objects. We believe that more robust image fea-\ntures will alleviate such failure cases in the future. More-\nover, similar to many LLMs, VistaLLM has the potential to\ngenerate harmful and unsafe outputs, which is also an active\nresearch topic.\nH. Additional Qualitative Results\nWe provide additional qualitative results from VistaLLM-\n13B in Figures H.1, H.2, H.3, H.4, H.5, H.6, H.7, H.8, H.9,\nand H.10. Moreover, we illustrate multi-round conversa-\ntional ability of VistaLLM in Figure H.11.\n19\nFigure G.1. Limitations of our method: Tiny and obscured objects, especially in cluttered and low-light environments, are hard to be\naccurately grounded. VistaLLM fails in such tough samples, which are even difficult to comprehend by humans.\nFigure H.1. Referring Expression Comprehension (REC) on RefCOCO, RefCOCO+ and RefCOCOg by VistaLLM-13B. REC aims\nto generate a bounding box around a single object described by a referring expression.\n20\nFigure H.2. Referring Expression Segmentation (RES) on RefCOCO, RefCOCO+ and RefCOCOg by VistaLLM-13B. RES aims to\nsegment a single object described by a referring expression.\nFigure H.3. Generalized Referring Expression Comprehension (GREC) on gRefCOCO by VistaLLM-13B. GREC aims to identify\nall objects described by a referring expression and draw bounding boxes around every referred object. GREC also contains no-target\nexpressions where the output is empty.\nFigure H.4. Generalized Referring Expression Segmentation (GRES) on gRefCOCO by VistaLLM-13B. GRES aims to identify all\nobjects described by a referring expression and segment every referred object. GRES also contains no-target samples where the output is\nempty.\n21\nGenerate a caption for the image.\nCaptioning\nA large orange cat sleeps on\nthe hood of a car.\nBriefly describe the image.\nA guy swings a tennis racket\nnear a fence.\nCan you give me a brief\ndescription of this image?\nA crowd of people flying kits\non a beach next to the ocean.\nGive a short description of the\npicture.\nA computer sits on a desk\nnext to some shelves.\nWhat's happening in the image at\na glance?\nA small airplane landing in a\nfield at sunset.\nProvide a short caption for the\nimage.\nA couple walks through snow\nwith skiis and poles.\nFigure H.5. Image Captioning on COCO by VistaLLM-13B, which aims to generate a short holistic description of the input image.\nFigure H.6. VQAv2 by VistaLLM-13B, which aims to answer direct questions based on an input image.\nLookTwice-QA (BoxQA)\nThe answer is 1.\nThe answer is 1.\nThe answer is 4.\nThe answer 1.\nThe answer is 2.\nThe answer is 1.\nLookTwice-QA (PointQA)\nHow many of these\n[222, 197, 241, 227]\nare in the street?\nHow many of these\nvehicles [84, 13, 187,\n188] are there?\nHow many of birds\n[139, 84, 231, 173]\nare there?\nHow many\nclocks [111, 107] are\nthere?\nHow many these\n[164, 48] are in\nphoto?\nHow many of these\nbeings [226, 130] are\nthere?\nFigure H.7. Box Question Answering (BoxQA) and Point Question Answering (PointQA) on LookTwice-QA by VistaLLM-13B.\nGiven a question about a specified region in the image, either mentioning a point or a box, this task needs to comprehend the area in the\ncontext of the whole image to produce the correct answer.\n22\nFigure H.8. Object Hallucination Evaluation of VistaLLM-13B on POPE benchmark. The task aims to input a query inquiring about\nthe existence of an object, and the model is expected to generate a response in the form of either \u201cyes/no.\u201d\nThat is True.\nNLVR2\nThat is False.\nThat is True.\nThere are more dogs in the\nimage on the right - True or\nFalse?\nTwo laptops are facing the\nsame direction - True or False?\nIn at least one image, we can\nsee a human hand - True or\nFalse?\nThat is False.\nRight image shows balloons with\nstrings descending from them -\nTrue or False?\nFigure H.9. Natural Language for Visual Reasoning (NLVR2) by VistaLLM-13B. Given a pair of input images and a question, the\nmodel must reason both images to produce the answer correctly.\nCoSeg\nFind the common object in the input\nimages and segment it in each image. \nAttCoSeg\nFind the two images which has an object\nwith common attributes, and segment it in\nboth the images. \nThe images are 2 and 3. The masks are\n[429, 122, 415, 158, ..., 191, 476, 153],\n[272, 151, 174, 183, \u2026, 195, 269, 162].\nThe masks are [132, 20, 98, \u2026, 20, 151, 22],\n[126, 18, 95, \u2026, 18, 152, 17],\n[166, 71, 134, ..., 79, 183,  72].\nFigure H.10. CoSeg and AttCoSeg by VistaLLM-13B. Given a set of input images, CoSeg aims to find and segment a common object in\nevery image. AttCoSeg is the more generalized scenario where a pair of images among all inputs contains a common object with similar\nattributes. VistaLLM is first expected to identify two images with the common object and then segment the object in both images.\n23\nFigure H.11. Multi-round Conversational Ability of VistaLLM-13B. The images are taken from COCO. VistaLLM can address all\npossible grounding and reasoning tasks across single and multiple input images.\n24\n"
  },
  {
    "title": "A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise",
    "link": "https://arxiv.org/pdf/2312.12436.pdf",
    "upvote": "12",
    "text": "A Challenger to GPT-4V?\nEarly Explorations of Gemini in Visual Expertise\nChaoyou Fu1\u2217\u2660, Renrui Zhang2,3\u2217, Zihan Wang6\u2217, Yubo Huang4, Zhengye Zhang4\nLongtian Qiu2, Gaoxiang Ye5, Yunhang Shen1, Mengdan Zhang1\nPeixian Chen1, Sirui Zhao4, Shaohui Lin6, Deqiang Jiang1\nDi Yin1, Peng Gao2, Ke Li1, Hongsheng Li3, Xing Sun1\u2020\n1Tencent Youtu Lab, 2Shanghai AI Laboratory\n3CUHK MMLab, 4USTC, 5Peking University, 6ECNU\n\u2217 Equal Contribution\n\u2660 Project Leader\n\u2020 Corresponding Author\nAbstract\nThe surge of interest towards Multi-modal Large Language Models (MLLMs),\ne.g., GPT-4V(ision) from OpenAI, has marked a significant trend in both academia\nand industry. They endow Large Language Models (LLMs) with powerful ca-\npabilities in visual understanding, enabling them to tackle diverse multi-modal\ntasks. Very recently, Google released Gemini, its newest and most capable MLLM\nbuilt from the ground up for multi-modality. In light of the superior reasoning\ncapabilities, can Gemini challenge GPT-4V\u2019s leading position in multi-modal\nlearning? In this paper, we present a preliminary exploration of Gemini Pro\u2019s\nvisual understanding proficiency, which comprehensively covers four domains:\nfundamental perception, advanced cognition, challenging vision tasks, and various\nexpert capacities. We compare Gemini Pro with the state-of-the-art GPT-4V to\nevaluate its upper limits, along with the latest open-sourced MLLM, Sphinx, which\nreveals the gap between manual efforts and black-box systems. The qualitative\nsamples indicate that, while GPT-4V and Gemini showcase different answering\nstyles and preferences, they can exhibit comparable visual reasoning capabilities,\nand Sphinx still trails behind them concerning domain generalizability. Specifically,\nGPT-4V tends to elaborate detailed explanations and intermediate steps, and Gem-\nini prefers to output a direct and concise answer. The quantitative evaluation on the\npopular MME benchmark, which is specifically designed for MLLM, also demon-\nstrates the impressive multi-modal understanding performance of Gemini, and its\npotential to be a strong challenger to GPT-4V. Our early investigation of Gemini\nalso observes some common issues of MLLMs concerning visual understanding,\nlogical reasoning, and prompting robustness, indicating that there still remains a\nconsiderable distance towards artificial general intelligence. We hope this report\nmay cast a new light on future MLLM research and application scenarios. Our\nproject for tracking the progress of MLLM is released at https://github.\ncom/BradyFU/Awesome-Multimodal-Large-Language-Models.\nContents\nList of Figures\n3\n1\nIntroduction\n6\narXiv:2312.12436v2  [cs.CV]  20 Dec 2023\n1.1\nMotivation and Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n1.2\nEvaluation Suite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.2.1\nPrompt Technique\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.2.2\nSample Collection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2\nFundamental Perception\n8\n2.1\nObject-Centric Perception\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.2\nScene-Level Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.3\nKnowledge-based Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3\nAdvanced Cognition\n48\n3.1\nText-Rich Visual Reasoning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n3.2\nAbstract Visual Reasoning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n3.3\nScience Problem-Solving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n3.4\nEmotion Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n3.5\nGame Playing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n4\nVision Task\n89\n4.1\nImage-Level Vision Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n4.2\nTemporal-Level Vision Task\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n5\nExpert Capacity\n100\n5.1\nAutonomous Driving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n100\n5.2\nDefect Detection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n100\n5.3\nMedical Diagnosis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n100\n5.4\nEconomic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n100\n5.5\nSurveillance and Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n101\n5.6\nRemote Sensing Image Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . .\n101\n5.7\nRobot Motion Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n101\n6\nQuantitative Experiments\n121\n6.1\nMME Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n121\n6.2\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n121\n7\nConclusion\n123\n7.1\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n123\n7.2\nGemini vs GPT-4V . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n123\n7.3\nGemini vs Sphinx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n123\n7.4\nFuture Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n124\n2\nList of Figures\n1\nSection 2.1: spatial relation recognition. . . . . . . . . . . . . . . . . . . . . . . .\n10\n2\nSection 2.1: spatial relation recognition. . . . . . . . . . . . . . . . . . . . . . . .\n11\n3\nSection 2.1: object counting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4\nSection 2.1: difference spotting.\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n5\nSection 2.1: difference spotting.\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6\nSection 2.1: optical illusion recognition. . . . . . . . . . . . . . . . . . . . . . . .\n15\n7\nSection 2.1: optical illusion recognition. . . . . . . . . . . . . . . . . . . . . . . .\n16\n8\nSection 2.2: scene understanding from image. . . . . . . . . . . . . . . . . . . . .\n17\n9\nSection 2.2: scene understanding from image. . . . . . . . . . . . . . . . . . . . .\n18\n10\nSection 2.2: scene understanding from image. . . . . . . . . . . . . . . . . . . . .\n19\n11\nSection 2.2: scene understanding from video.\n. . . . . . . . . . . . . . . . . . . .\n20\n12\nSection 2.3: commonsense. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n13\nSection 2.3: commonsense. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n14\nSection 2.3: commonsense. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n15\nSection 2.3: commonsense. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n16\nSection 2.3: scientific knowledge.\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n17\nSection 2.3: scientific knowledge.\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n18\nSection 2.3: scientific knowledge.\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n19\nSection 2.3: historical knowledge. . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n20\nSection 2.3: multicultural customs. . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n21\nSection 2.3: multicultural customs. . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n22\nSection 2.3: multicultural customs. . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n23\nSection 2.3: celebrity recognition and description. . . . . . . . . . . . . . . . . . .\n32\n24\nSection 2.3: celebrity recognition and description. . . . . . . . . . . . . . . . . . .\n33\n25\nSection 2.3: landmark recognition and description.\n. . . . . . . . . . . . . . . . .\n34\n26\nSection 2.3: landmark recognition and description.\n. . . . . . . . . . . . . . . . .\n35\n27\nSection 2.3: logo recognition and description. . . . . . . . . . . . . . . . . . . . .\n36\n28\nSection 2.3: logo recognition and description. . . . . . . . . . . . . . . . . . . . .\n37\n29\nSection 2.3: movie recognition and description. . . . . . . . . . . . . . . . . . . .\n38\n30\nSection 2.3: movie recognition and description. . . . . . . . . . . . . . . . . . . .\n39\n31\nSection 2.3: movie recognition and description. . . . . . . . . . . . . . . . . . . .\n40\n32\nSection 2.3: food recognition and description. . . . . . . . . . . . . . . . . . . . .\n41\n33\nSection 2.3: food recognition and description. . . . . . . . . . . . . . . . . . . . .\n42\n34\nSection 2.3: plant recognition.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n35\nSection 2.3: animal and plant recognition. . . . . . . . . . . . . . . . . . . . . . .\n44\n36\nSection 2.3: artwork recognition and description.\n. . . . . . . . . . . . . . . . . .\n45\n37\nSection 2.3: artwork recognition and description.\n. . . . . . . . . . . . . . . . . .\n46\n38\nSection 2.3: artwork recognition and description.\n. . . . . . . . . . . . . . . . . .\n47\n3\n39\nSection 3.1: table and chart reasoning. . . . . . . . . . . . . . . . . . . . . . . . .\n51\n40\nSection 3.1: table and chart reasoning. . . . . . . . . . . . . . . . . . . . . . . . .\n52\n41\nSection 3.1: table and chart reasoning. . . . . . . . . . . . . . . . . . . . . . . . .\n53\n42\nSection 3.1: table and chart reasoning. . . . . . . . . . . . . . . . . . . . . . . . .\n54\n43\nSection 3.1: table and chart reasoning. . . . . . . . . . . . . . . . . . . . . . . . .\n55\n44\nSection 3.1: visual code generation.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n56\n45\nSection 3.1: visual code generation.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n57\n46\nSection 3.1: visual code generation.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n58\n47\nSection 3.1: visual code generation.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n59\n48\nSection 3.1: abstract visual stimuli. . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n49\nSection 3.2: abstract visual stimuli. . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n50\nSection 3.2: Wechsler Adult Intelligence Scale. . . . . . . . . . . . . . . . . . . .\n62\n51\nSection 3.2: Wechsler Adult Intelligence Scale. . . . . . . . . . . . . . . . . . . .\n63\n52\nSection 3.2: Raven\u2019s Progressive Matrices. . . . . . . . . . . . . . . . . . . . . . .\n64\n53\nSection 3.2: Raven\u2019s Progressive Matrices. . . . . . . . . . . . . . . . . . . . . . .\n65\n54\nSection 3.3: mathematical problem.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n66\n55\nSection 3.3: mathematical problem.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n67\n56\nSection 3.3: mathematical problem.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n68\n57\nSection 3.3: mathematical problem.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n69\n58\nSection 3.3: mathematical problem.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n70\n59\nSection 3.3: mathematical problem.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n71\n60\nSection 3.3: physics problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n61\nSection 3.3: physics problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n62\nSection 3.3: physics problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n63\nSection 3.4: facial expression analysis. . . . . . . . . . . . . . . . . . . . . . . . .\n75\n64\nSection 3.4: facial expression analysis. . . . . . . . . . . . . . . . . . . . . . . . .\n76\n65\nSection 3.4: image emotion analysis. . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n66\nSection 3.4: image emotion analysis. . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n67\nSection 3.4: image emotion analysis. . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n68\nSection 3.4: image emotion analysis. . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n69\nSection 3.4: image emotion analysis. . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n70\nSection 3.4: emotion-conditioned output.\n. . . . . . . . . . . . . . . . . . . . . .\n82\n71\nSection 3.4: emotion-conditioned output.\n. . . . . . . . . . . . . . . . . . . . . .\n83\n72\nSection 3.5: Sudoku.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n73\nSection 3.5: Crossword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n74\nSection 3.5: Crossword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n75\nSection 3.5: Go playing.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n76\nSection 3.5: Go playing.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n77\nSection 4.1: object detection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n4\n78\nSection 4.1: referring expression comprehension. . . . . . . . . . . . . . . . . . .\n91\n79\nSection 4.1: referring expression comprehension. . . . . . . . . . . . . . . . . . .\n92\n80\nSection 4.1: phrase localization.\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n81\nSection 4.1: face detection and recognition. . . . . . . . . . . . . . . . . . . . . .\n94\n82\nSection 4.2: object tracking.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n83\nSection 4.2: video action recognition.\n. . . . . . . . . . . . . . . . . . . . . . . .\n96\n84\nSection 4.2: video action recognition.\n. . . . . . . . . . . . . . . . . . . . . . . .\n97\n85\nSection 4.2: video action recognition.\n. . . . . . . . . . . . . . . . . . . . . . . .\n98\n86\nSection 4.2: visual story generation. . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n87\nSection 5.1: traffic signs understanding. . . . . . . . . . . . . . . . . . . . . . . .\n102\n88\nSection 5.1: traffic signs understanding. . . . . . . . . . . . . . . . . . . . . . . .\n103\n89\nSection 5.1: driving intentions. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n104\n90\nSection 5.1: driving intentions. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n105\n91\nSection 5.2: defect detection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n106\n92\nSection 5.2: defect detection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n107\n93\nSection 5.2: defect detection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n108\n94\nSection 5.3: medical image understanding. . . . . . . . . . . . . . . . . . . . . . .\n109\n95\nSection 5.3: medical image understanding. . . . . . . . . . . . . . . . . . . . . . .\n110\n96\nSection 5.3: medical image understanding. . . . . . . . . . . . . . . . . . . . . . .\n111\n97\nSection 5.3: medical image understanding. . . . . . . . . . . . . . . . . . . . . . .\n112\n98\nSection 5.4: economic analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n113\n99\nSection 5.4: economic analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n114\n100 Section 5.5: surveillance and security. . . . . . . . . . . . . . . . . . . . . . . . .\n115\n101 Section 5.5: surveillance and security. . . . . . . . . . . . . . . . . . . . . . . . .\n116\n102 Section 5.6: remote sensing image analysis. . . . . . . . . . . . . . . . . . . . . .\n117\n103 Section 5.6: remote sensing image analysis. . . . . . . . . . . . . . . . . . . . . .\n118\n104 Section 5.7: robot motion planning.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n119\n105 Section 5.7: robot motion planning.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n120\n106 Section 6.2: evaluation on MME benchmark.\n. . . . . . . . . . . . . . . . . . . .\n122\n5\n1\nIntroduction\n1.1\nMotivation and Overview\nDriven by big data and substantial computational power, the realm of large language models\n(LLMs) [8, 42, 13, 3, 50, 27] has garnered unprecedented enthusiasm and advancement, show-\ncasing generalizability in a wide range of fields. Building upon this achievement, Multi-modal Large\nLanguage Models (MLLMs) [61, 30, 16, 5, 23, 65, 36, 14, 60, 25, 20, 6] are emerging as a focal\npoint of research in the new generation. They target on incorporating LLMs with additional sensory\ninput, e.g., image [36, 63], audio [56], 3D [24], etc. Conditioned on data of new modalities, MLLMs\ntake a significant step forward on the path towards general artificial intelligence. Notably, GPT-\n4V(ision) [43, 1, 59] from OpenAI is recognized as the most powerful MLLMs to date, surpassing a\nhost of LLaMA-based [50] models, e.g., LLaMA-Adapter [63], LLaVA [36], and MiniGPT-4 [65].\nHowever, very recently released by Google, Gemini [21] has emerged as a formidable challenger to\nGPT-4V, which exhibits significant multi-modal capabilities over different benchmarks [19, 38, 26].\nGiven that the full potential of Gemini has not yet been fully tapped, in this paper, we conduct an\nearly exploration by comparing Gemini with existing best-performing MLLM, i.e., GPT-4V, to reveal\nits multi-modal capabilities.\nFor a comprehensive evaluation, we carefully collect a bunch of qualitative samples covering different\ndomains in multi-modal understanding. Two existing representative MLLMs are selected as baselines.\nThe first is GPT-4V, representing the current highest standard in the field, which assesses the upper\nlimits of Gemini. The second is Sphinx [35], a state-of-the-art LLaMA-based MLLM, exploring how\nmuch the performance gap is between open-sourced models and closed-sourced systems. Specifically,\nthe qualitative samples can be categorized into four visual domains as follows:\n1. Fundamental Perception. (Section 2) This dimension focuses on the basic ability of\nMLLMs to perceive and understand visual concepts, without the need for complex reasoning.\nIt can be subdivided into three key aspects: object-centric, scene-level, and knowledge-based\nperception. Therein, object-centric perception assesses the model\u2019s capacity to recognize\nand interpret the characteristics of individual objects within a visual context, exemplified\nby tasks such as spatial relation recognition, object counting, difference spotting, etc. In\ncontrast, scene-level perception evaluates the understanding of entire scenes from a global\nperspective, demonstrating the model\u2019s proficiency in image and video captioning. Finally,\nknowledge-based perception reveals the model\u2019s accumulation and application of specific\nknowledge across various domains. It encompasses commonsense knowledge, scientific\nknowledge, cultural customs, and world memory, which respectively cover the content of\neveryday scenarios, academic disciplines, cultural practices, and global entities.\n2. Advanced Cognition. (Section 3) The samples in advanced cognition require MLLMs\nto process more complicated visual information and conduct multi-modal reasoning for\nproblem-solving. The related tasks include text-rich and abstract visual reasoning, science\nproblem solving, emotion understanding, and game playing. Text-rich tasks evaluate the\nOCR performance of textual content for table and chart reasoning, and the code generation\ncapability conditioned on different visual inputs. Abstract visual reasoning refers to the\nnon-verbal test assessing general intelligence and abstract reasoning, such as the Wechsler\nAdult Intelligence Scale and Raven\u2019s Progressive Matrices. Science problem-solving, e.g.,\nmathematics and physics, has become a vital metric for measuring MLLMs\u2019 comprehension\nof quantitative and logical knowledge, involving complex multi-step and chain-of-thought\n(CoT) reasoning. Moreover, emotion understanding focuses on the detection of underlying\nemotional information within visual contexts, and game playing evaluates strategic thinking\nand rule-following abilities in games like Sudoku.\n3. Challenging Vision Tasks. (Section 4) In this part, we aim to evaluate how MLLMs\nperform in some challenging vision tasks beyond general visual question-answering, such\nas object detection, referring expression comprehension, phrase localization, video temporal\nreasoning, and so on. These tasks require the in-depth visual perception and understanding\ncapabilities of MLLMs. The performance of MLLMs can indicate their potential to serve as\nmulti-purpose vision generalists.\n4. Expert Capacity. (Section 5) The final dimension evaluates the model\u2019s proficiency in\nseveral specialized fields. The scenarios include medical imaging, defect detection, stock\n6\nprediction, autonomous driving, and surveillance security. Each of these areas tests the\nmodel\u2019s application of its learned knowledge and cognitive skills in a professional context,\nsuch as diagnosing diseases from medical images or predicting market trends in stock trading.\nThis demonstrates the generalization capacity of MLLMs from more diverse perspectives.\nBesides qualitative samples, we report quantitative results of Gemini on the popular MME bench-\nmark [19] in Section 6, which comprehensively evaluates MLLMs in 14 subtasks from both perception\nand cognition perspectives.\n1.2\nEvaluation Suite\n1.2.1\nPrompt Technique\nGPT-4V has been demonstrated to support a diverse range of prompt techniques [59], from simple\ninstruction following [44, 41, 53, 48] to in-context few-shot learning [8, 51, 2]. This inspires us to\ndesign the following prompt techniques. Simple instruction following. A simple instruction directly\nexpresses the user\u2019s intention, such as \u201cDescribe this image\u201d or \u201cWho is this person in the poster?\u201d.\nExisting MLLMs [23, 65, 36, 14, 60] are generally capable of following instructions, enabling us to\nutilize the simple instruction to accomplish most tasks effectively. We adopt simple instructions to\nprompt models on most of the tasks. Figures 1 and 3 are typical examples, respectively.\nVisual referring prompt. In many cases, a simple visual marker can more effectively convey the\nuser\u2019s interest in a specific spatial region to MLLMs than detailed and lengthy text, as shown in\nFigure 82. In addition to the visual markers used as visual prompts in [12, 59], we also experiment\nwith physical objects to guide the model\u2019s understanding of the referring items, such as a finger or a\npen, as illustrated in the bottom part of Figure 35. Compared to prompting the model with visual\nmarkers, using real objects as prompts is more practical in real-time interaction scenarios.\nChain-of-Thought (CoT) prompt. For problems involving complex logical reasoning, we use CoT\ntechniques [54, 32] to guide the model to provide a final response through a series of more logical\nthought processes, which is shown in Figure 42.\nIn-context few-shot learning. In certain scenarios where simple text instructions fail to completely\ndemonstrate the task, we employ in-context few-shot learning [8, 51, 2] for better prompting. By\nproviding a few in-context examples at inference time, the model can infer intentions from these\nexamples, thus facilitating the generation of the desired outputs, which is shown in Figure 2.\n1.2.2\nSample Collection\nAvoiding sample leakage. We endeavor to ensure that the collected qualitative images and text are\nunseen by the models to prevent responses that merely reflect memories of the training data. All\nthe texts in the query are constructed from scratch. The image sources include manually created\ndrawings, offline photographs, Internet images, and some existing datasets [10, 7, 15, 49]. For the\nInternet images, we strive to collect those with timestamps postdating November 2023.\nDiverse difficulty. For each task, we collect samples of varying difficulty levels, e.g., from funda-\nmental perception and cognition to the more challenging vision and expert tasks. In this way, we\ncan not only demonstrate the potential of MLLMs to complete the tasks, but also touch their ability\nboundaries through some obvious mistake patterns.\n7\n2\nFundamental Perception\nFundamental perception, in the context of multi-modal large models, refers to the model\u2019s ability\nto process and interpret sensory data, primarily visual, to create a coherent understanding of the\nenvironment it perceives. The proficiency in perception directly influences a model\u2019s capability in\nhigher-order tasks, as it determines how accurately and effectively the model can acquire and process\nraw visual input.\nIn Section 2.1, we will explore the object-centric perception task, such as spatial relationship\nrecognition, object counting, and difference spotting. In Section 2.2, we will examine the models\u2019\ncapacity for interpreting the entire scenes on diverse domains. In Section 2.3, we will investigate\nthe models\u2019 ability to comprehend visual information via the application of knowledge, which\nencompasses commonsense, subject knowledge, multicultural customs, and world memory.\n2.1\nObject-Centric Perception\nSpatial relationship recognition. We investigate the models\u2019 capability to comprehend spatial\nrelationships. We find that it seems difficult for the models to identify left and right. For instance,\nin Figure 1, the individual on the left-hand side of Jordan is James. However, the responses from\nGemini and GPT4-V are both Kobe, while Sphinx\u2019s response is Jordan. In our endeavor, we employ\nin-context few-shot learning techniques to aid the model in comprehending the concept of \u2018left-hand\u2019.\nAs depicted in Figure 2, we provide two image examples to instruct the model on what constitutes\nthe \u2018left-hand\u2019. However, only GPT-4V successfully learns the concept, while Gemini and Sphinx\nstill can not distinguish between left and right.\nObject counting. Figure 3 shows the models\u2019 ability to count objects. It is observed that for simple\nsamples, the performance of the open-source model Sphinx closely aligns with that of the two closed-\nsource models, which is shown in the first three cases. However, as shown in the fourth example,\nwhen the images contain an excess of visual elements, all three models tend to make mistakes.\nDifference spotting. In Figures 4-5, we present the model\u2019s capacity to spot differences in cartoon\nimages, sketches, and actual photographs. We observe that all models possess the potential to perceive\nthe fine-grained differences between images, although their performance is not consistently stable. In\naddition, we observe that both Gemini and GPT-4V are easily misled by the intentionally erroneous\nprompts we provide. As shown in Figure 5, there are actually only three differences. However, when\nwe request the models to identify five differences, both Gemini and GPT-4V fabricate five distinct\npoints and respond incorrectly.\nOptical illusion recognition. As shown in Figures 6-7, we investigate whether these models exhibit\na visual understanding of optical illusions similar to that of humans. For instance, in the left part of\nFigure 6, the two pears actually possess identical brightness. However, the interspersed black and\nwhite stripes create an illusion, making the pear on the right appear brighter. Gemini recognizes that\nthe two have the same brightness, whereas GPT-4V and Sphinx, like many humans, are deceived by\nthe optical illusion, perceiving the right pear to be brighter. In the right section of Figure 6, GPT-4V\nidentifies a similarity in the angles of the tree trunks and branches to those of human bodies and arms,\nonce again demonstrating a human-like visual understanding of optical illusions.\n2.2\nScene-Level Perception\nScene understanding from image. We prompt the models to identify all visual elements in the\nimage as detailed as possible via the text query \u201cDescribe this image in detail.\u201d Figures 8-10 illustrate\nthat all three models are capable of depicting the key visual elements within the scene. However,\nin comparison, GPT-4V shows superior performance, particularly in highly cluttered environments.\nThis is evident in Figure 8, where GPT-4V\u2019s descriptions are notably more detailed and exhibit fewer\ninstances of hallucination.\nScene understanding from video. Here we examine the potential of the models to understand scenes\nfrom video. As shown in Figure 11, we extract three temporally distinct frames from a video and input\nthem into the model along with the text query, \u201cPlease describe this scene according to these temporal\nimages.\u201d Our observations indicate that Gemini is capable of integrating the information from the\ndifferent frames into a cohesive scene description. Especially, the first frame displays two round\n8\ntables and one potted plant, while the second frame shows one round table and three potted plants.\nRemarkably, Gemini successfully merges the information from both frames to accurately describe\nthe scene as containing two round tables and three potted plants. GPT-4V describes the contents\nof images frame by frame. In contrast, Sphinx\u2019s descriptions do not demonstrate a comprehensive\nunderstanding of the sequence of images.\n2.3\nKnowledge-based Perception\nCommonsense. Figures 12-15 illustrate the capability of these three models to apply common sense\nin understanding visual information within images. It is observed that the open-source model Sphinx\nperforms comparably to Gemini and GPT-4V in applying social norms as shown in Figures 12-13.\nHowever, it exhibits a slightly inferior performance in the application of physical laws. For instance,\nas shown in Figure 14, Gemini and GPT-4V can accurately select heavy clothing for cold weather\nprotection in Antarctica. Interestingly, while Sphinx recognizes the need for thick clothing in cold\nweather, it erroneously identifies the image of a T-shirt as suitable for cold protection.\nSubject knowledge. In Figures 16-19, we examine the model\u2019s ability to apply knowledge in the\nfields of physics, chemistry, and history. We observe that both Gemini and GPT-4V possess the\nrelevant subject knowledge associated with these cases. The performance of Sphinx is slightly inferior,\nyet it is capable of providing correct answers in certain instances.\nMulticultural customs. We examine the models\u2019 comprehension of multicultural elements. We\npresent the model with images featuring distinct local ethnic and cultural characteristics and prompt it\nto provide descriptions. Figures 20-22 demonstrate that all three models are capable of understanding\nthe ethnic and cultural elements depicted in the images.\nWorld memory. We investigate the models\u2019 ability to recognize globally known celebrities, land-\nmarks, logos, movies, food, plants, animals, and more. As illustrated in Figures 23-38, we observe\nthat the models generally identify them correctly. However, when the images reflect information\nupdated recently, the models\u2019 descriptions are not accurate, as depicted in Figure 24. Additionally,\nwhen asked about information relevant to real people, GPT-4V occasionally opts to refrain from\nresponding, as shown in Figure 29.\n9\nSpatial Relation Recognition\nTell me the spatial relation between the tennis ball and \nthe man.\nPrompt\nGemini Pro\nThe tennis ball is to the left of the man.\nThe tennis ball is in front of the man and slightly to \nhis left, indicating that he has either just hit the ball \nor is preparing to hit it. The ball is at a lower height, \nroughly around the waist level of the man, and it \nappears to be very close to his racket, suggesting a \nmoment of action during a tennis match.\nGPT-4V\nThe tennis ball is positioned below the man, as he is \nswinging his tennis racket to hit it.\nSPHINX\nGemini Pro\nThe person on Jordan's left-hand side is Kobe Bryant.\nThe person on Jordan's left-hand side, wearing the \nyellow Lakers jersey with the number 8, is Kobe \nBryant.\nGPT-4V\nThe person on Jordan's left-hand side is Michael \nJordan.\nSPHINX\nWho is the person on Jordan's left-hand side?\nPrompt\nSec. 2.1 Object-Centric Perception\nFigure 1: Results on spatial relation recognition. None of the three models can answer correctly.\nRed highlights the wrong answer. Yellow highlights the incompetence in performing the task.\nRefer to Section 2.1 for detailed discussions.\n10\nSpatial Relation Recognition\nIn the first image, Curry's left-hand side is Green. In the second image, Wade's left-hand side is James. In the third \nimage, who is the person on Jordan's left-hand side?\nPrompt\nGemini Pro\nBryant.\nThe person on Jordan's left-hand side is Kobe Bryant.\nSPHINX\nThe person on Jordan's left-hand side in the third \nimage is LeBron James, wearing a Lakers number \n23 jersey.\nGPT-4V\nSec. 2.1 Object-Centric Perception\nFigure 2: Results on spatial relation recognition. By providing two examples, GPT-4V accurately\nidentifies spatial relationships, whereas the other two models still respond incorrectly.\nGreen\nhighlights the right answer. Red highlights the wrong answer. Refer to Section 2.1 for detailed\ndiscussions.\n11\nCount\u00a0the\u00a0number\u00a0of\u00a0black\u00a0\nbirds\u00a0in\u00a0the\u00a0image.\nGemini Pro\nThere\u00a0are\u00a08\u00a0black\u00a0birds.\nObject Counting\nPrompt\nSec. 2.1 Object-Centric Perception\nThere\u00a0are\u00a0eight\u00a0black\u00a0birds\u00a0\nvisible\u00a0in\u00a0the\u00a0image.\nGPT-4V\nHow\u00a0many\u00a0face\u00a0up\u00a0dice\u00a0have\u00a0a\u00a0count\u00a0of\u00a04?\nGemini Pro\nThere\u00a0are\u00a02\u00a0dice\u00a0with\u00a0a\u00a0count\u00a0of\u00a04.\nPrompt\nIn\u00a0the\u00a0image\u00a0you've\u00a0provided,\u00a0there\u00a0are\u00a0two\u00a0\ndice\u00a0that\u00a0are\u00a0facing\u00a0up\u00a0with\u00a0a\u00a0count\u00a0of\u00a04.\nGPT-4V\nCount\u00a0the\u00a0number\u00a0of\u00a0fruits\u00a0\nin\u00a0the\u00a0image.\nGemini Pro\nThere\u00a0are\u00a04\u00a0fruits\u00a0in\u00a0the\u00a0image.\nPrompt\nThere\u00a0are\u00a0three\u00a0fruits\u00a0in\u00a0the\u00a0\nimage.\nGPT-4V\nCount\u00a0the\u00a0number\u00a0of\u00a0people\u00a0in\u00a0the\u00a0image.\nGemini Pro\nThere\u00a0are\u00a041\u00a0people.\nPrompt\nThere\u00a0are\u00a030\u00a0people\u00a0depicted\u00a0in\u00a0the\u00a0image.\nGPT-4V\nCount\u00a0the\u00a0number\u00a0of\u00a0books\u00a0\nin\u00a0the\u00a0image.\nGemini Pro\nThere\u00a0are\u00a05\u00a0books.\nPrompt\nThere\u00a0are\u00a0five\u00a0books\u00a0in\u00a0the\u00a0image.\nGPT-4V\nThere\u00a0are\u00a0four\u00a0fruits\u00a0in\u00a0the\u00a0\nimage.\nSPHINX\nThere\u00a0are\u00a0eight\u00a0black\u00a0birds\u00a0\nin\u00a0the\u00a0image.\nSPHINX\nThere\u00a0are\u00a0two\u00a0face\u00a0up\u00a0dice\u00a0with\u00a0a\u00a0count\u00a0of\u00a04.\nSPHINX\nThere\u00a0are\u00a0two\u00a0books\u00a0in\u00a0the\u00a0image.\nSPHINX\nThere\u00a0are\u00a0100\u00a0people\u00a0in\u00a0the\u00a0image.\nSPHINX\nFigure 3: Results on object counting. Green highlights the right answer. Red highlights the wrong\nanswer. Refer to Section 2.1 for detailed discussions.\n12\nWhat is the difference between the left part of the \npicture and the right part?\nGemini Pro\nThe boy on the left is holding a basketball while \nthe boy on the right is holding a soccer ball. The \nboy on the left is wearing a red shirt and blue \nshorts while the boy on the right is wearing a \nblue shirt and red shorts.\nDifference Spotting\nPrompt\nSec. 2.1 Object-Centric Perception\nWhat is the difference between the left part of the \npicture and the right part?\nPrompt\nThe image shows two cartoon characters that \nappear to be identical in design, with the \nprimary difference being the sports they are \nassociated with. The character on the left is \ndressed in a\nbasketball jersey, holding a \nbasketball, suggesting he is a basketball player. \nThe character on the right is dressed in a soccer \nkit, holding a soccer ball, indicating he is a \nsoccer player.\nThe\noutfits\nand\nthe\nsports\nequipment are the key differences between the\ntwo halves of the picture.\nGPT-4V\nGemini Pro\nThe difference is the shape of the head and the \ntail.\nThe left part of the picture displays a series of \noverlapping circles that form the outline of a \ncreature resembling a fish or a dinosaur, with a \nspiky back and a simple face with eyes and \nmouth indicated by circles. The right part shows \na similar outline with a few key differences:\n1. The back spikes are represented by rectangles \nor squares, rather than pointed shapes.\n2. The face is more abstract, with a triangular \nshape suggesting an eye, and there is no clear \ndepiction of a mouth.\n3. The overall appearance of the right side is \nmore geometric and less organic than the left \nside. The left seems to represent a more organic \nand natural form, while the right appears more \nstylized and geometric.\nGPT-4V\nThe difference between the left part of the \npicture and the right part is that the left part \nshows a boy holding a basketball, while the \nright part shows a boy holding a soccer ball.\nSPHINX\nThe difference between the left part of the \npicture and the right part is that the left part has \na black and white dinosaur, while the right part \nhas a black and white fish.\nSPHINX\nFigure 4: Results on difference spotting. Green highlights the right answer. Red highlights the\nwrong answer. Yellow highlights the incompetence in performing the task. Refer to Section 2.1 for\ndetailed discussions.\n13\nWhat is the difference between the left part of the picture and the right part? Could you find five differences?\nGemini Pro\n1. The left picture has four objects, while the right picture only has three objects.\n2. The left picture has a white wall in the background, while the right picture has a wooden table in the \nbackground.\n3. The left picture has a black cup on the table, while the right picture has a white cup with a lemon on \nit.\n4. The left picture has a tall thermos, while the right picture has a shorter thermos.\n5. The left picture has a smaller thermos, while the right picture has a bigger thermos.\nDifference Spotting\nPrompt\nSec. 2.1 Object-Centric Perception\nIn the image you've provided, there are two parts, each showing a slightly different scene. Let's identify \nfive differences between the left part of the picture and the right part:\n1. Lid of the Thermos: On the left, the thermos is closed with its lid on. On the right, the lid is off and \nplaced beside the thermos.\n2. Inner Container: The left side doesn't show an inner container outside of the thermos, while the \nright side shows a metallic inner container placed in front of the thermos.\n3. Lemon Slice: A lemon slice is present on the right, placed on top of the coaster that is on the \nthermos lid. There is no lemon slice on the left.\n4. Coaster: There is a coaster under the thermos on the right, which is not present on the left.\n5. Light Switch Position: The light switches on the wall in the background are in different positions. \nOn the left, the two switches closest to the thermos are down, while on the right, one is up and the \nother is down.\nGPT-4V\nThe left part of the picture has a lemon on top of one of the thermos bottles, while the right part of the \npicture does not have any lemon on top of the thermos bottles.\nSPHINX\nFigure 5: Results on difference spotting. In the two images, there are only three differences, yet\nwe ask the model to identify five. Both Gemini and GPT-4V attempt to accommodate our request\nand provide five differences, but neither performs well. Red highlights the wrong answer. Refer to\nSection 2.1 for detailed discussions.\n14\nWhich pear has higher brightness?\nOptical Illusion Recognition\nPrompt\nDoes the tree in the picture above look like the person \nin the picture below?\nPrompt\nSec. 2.1 Object-Centric Perception\nGemini Pro\nNo, the tree in the picture above does not look like \nthe person in the picture below.\nThe tree is a large, green tree with a thick trunk and \nmany branches. It is standing on a hilltop and there \nis a river and mountains in the background. The \nperson is a young man with dark hair and he is \nwearing a black shirt and pants. He is dancing on a \nstage. The two images are very different and do not \nresemble each other.\nThe tree in the upper image does bear a resemblance \nto the pose of the person in the lower image. The \nway the tree branches out at the top can be likened to \nthe angle at which the person's arms are bent, while \nthe trunk aligns with the person's body. The visual \nsimilarity is likely coincidental but makes for an \ninteresting comparison between the natural shapes in \nour environment and human poses.\nGPT-4V\nNo, the tree in the picture above does not look like \nthe person in the picture below.\nSPHINX\nGemini Pro\nBoth pears have the same brightness.\nThe image you've provided appears to show two \npears with a striped pattern in the background. \nBrightness in an image can be affected by many \nfactors, including the hue, saturation, and luminance \nof the colors used, as well as the presence of other \nelements and the overall lighting of the image.\nTo determine which pear has a higher brightness, we \nwould typically consider which one appears lighter \nor more luminous compared to the other. However, \ndue to variations in display screens and personal \nperceptions, what appears brighter to one person \nmay not appear the same to another.\nTo provide an objective measure, I can analyze the \nimage and calculate the average brightness of each \npear. Let's do that now.\nThe pear on the right side has a higher average \nbrightness value (approximately 148.52) compared \nto the pear on the left (approximately 127.06). Thus, \nbased on the average pixel values, the pear on the \nright is brighter.\nGPT-4V\nThe pear on the right has higher brightness.\nSPHINX\nFigure 6: Results on optical illusion recognition. GPT-4V shows a human-like visual understanding\nof optical illusions. Green highlights the right answer. Red highlights the wrong answer. Refer to\nSection 2.1 for detailed discussions.\n15\nCould this be true in the real world?\nGemini Pro\nNo, it's not possible.\nThis is an optical illusion.\nOptical Illusion Recognition\nPrompt\nNo, this is not possible in the real world. The image \nshows a group of white dice arranged in a triangle \nshape, which is an artistic or creative representation. \nIn the real world, dice are typically used for games \nand do not form specific shapes like a triangle.\nSPHINX\nThe image you've uploaded appears to show dice arranged in a way that defies the normal expectations of gravity and \nbalance, suggesting an improbable structure. Without additional context, such as knowledge of the materials used or \nthe method of assembly, it's difficult to definitively say whether it's a real, physical construction or a clever \nmanipulation, be it digital or a trick of perspective.\nIn the real world, the arrangement shown would be very challenging to achieve due to the dice needing to balance on \nvery small contact points, which would be unstable without some form of adhesive or support. However, it's not \nimpossible with the use of supports that are hidden from view, or by modifying the dice to connect more securely. It's \nalso possible that the image is a result of digital manipulation.\nIf the intent is to create such a structure in reality, one would likely need to employ some form of support or \nmodification to the dice. If this is an artistic endeavor, it would be a fascinating project that combines elements of \nsculpture and illusion.\nGPT-4V\nSec. 2.1 Object-Centric Perception\nFigure 7: Results on optical illusion recognition. Only Gemini recognizes that this image is an optical\nillusion. Green highlights the right answer. Red highlights the wrong answer. Refer to Section 2.1\nfor detailed discussions.\n16\nDescribe this image in details.\nGemini Pro\nThe image shows a large shopping mall with a glass roof. There \nare many people in the mall, most of whom are wearing masks. \nThere are also a number of stores. The decorations in the mall \nare red and gold, and there are a number of large red balloons. \nThere is a stage with a red carpet in the center of the mall. There \nare people on the stage, and there is a large screen behind them. \nThere are also a number of people standing around the stage.\nScene Understanding from Image\nPrompt\nSec. 2.2 Scene-Level Perception\nThe image depicts the interior of a bustling multi-level shopping mall decorated for an event. There are numerous red balloons lining \nthe escalators and railings across the multiple floors, creating a festive atmosphere. The mall features a variety of stores and eateries \nvisible on different levels, with signage for brands like McDonald's clearly visible.The central area is occupied by an event space, with \na crowd of spectators gathered around. The event includes several brightly colored obstacle courses or activity stations branded with \nnames like \"PUKY\" and \"MOOVnGO.\" These stations are arranged in a row and consist of ramps, blocks, and other equipment for \nwhat appears to be a children\u2019s play area or competition.Throughout the mall, people are engaged in various activities; some are \nwatching the event, while others are walking around, shopping, or dining. The architecture of the mall includes a glass ceiling that \nallows natural light to flood the interior. The overall impression is one of a lively, family-friendly environment with a special event \ndrawing the attention of the visitors.\nGPT-4V\nThe image depicts a bustling mall filled with people, where a large group of people is gathered in the center of the building. \nThe mall is decorated with red and white balloons, creating a festive atmosphere. There are several people scattered \nthroughout the mall, some walking around and others standing in groups.In addition to the people, there are a few objects \nthat can be seen in the mall. There are two handbags, one near the left side of the mall and the other closer to the center. \nThere are also two backpacks, one located near the left side and the other closer to the center.The mall is filled with a lively \nenergy, as people enjoy their time shopping and socializing in the busy space.\nSPHINX\nFigure 8: Results on scene understanding from image. GPT-4V\u2019s descriptions are notably more\ndetailed and exhibit fewer instances of hallucination.\nGreen highlights the right answer.\nRed\nhighlights the wrong answer. Refer to Section 2.2 for detailed discussions.\n17\nDescribe this image in details.\nGemini Pro\nThe image is a fantasy landscape with a large city in the center. \nThe city is built on a series of interconnected platforms that are \nsuspended in the air by large balloons. The platforms are filled \nwith buildings,\u4e2d\u306b\u306f\u5bb6\u3001\u30b7\u30e7\u30c3\u30d7\u3001\u5bfa\u9662\u306a\u3069\u304c\u3042\u308a\u307e\u3059\u3002\nThe city is surrounded by a large wall, and there are several \nairships docked at the city's edge. The sky is filled with clouds, \nand there are several mountains in the background.The image is \nvery detailed, and there are many interesting things to see. The \nartist has created a very unique and imaginative world.\nScene Understanding from Image\nPrompt\nSec. 2.2 Scene-Level Perception\nThe image depicts a fantastical scene that appears to be from a story or a game setting. There's a vast landscape with a variety of \nstructures that seem to merge traditional architectural elements with fantastical ones, suggesting a world where different historical or \ncultural influences coexist.On the right side of the image, there's a towering building that resembles a pagoda with multiple levels, \nbalconies, and overhanging roofs. It's built on a rocky outcrop and has many windows and openings, with flags and banners fluttering \nin the wind. The building's design suggests a blend of medieval and Asian architectural styles.To the left and in the background, there \nare more structures built into what appears to be a mountainous region. There are bridges connecting different areas, and the buildings \nare made of wood and stone with various designs, some resembling watchtowers or fortresses.Above the landscape, there are two \nairships with large balloons and what seem to be sails or wings, suggesting they are powered by both air and wind. The design of the \nairships is reminiscent of steampunk aesthetics, with wooden hulls and intricate details.In the foreground, we see a character with red \nhair, wearing leather armor and a sword strapped to his back. He\u2019s looking at a handheld device that could be a map or a magical \nartifact. His attire and stance suggest he is an adventurer or a warrior. Another character in the background is walking away, dressed \nsimilarly in armor and carrying a bow, indicating that this might be a world of adventurers or warriors.The ground is rocky with \npatches of grass, and there are various objects scattered around like a cart, barrels, and crates, which add to the lived-in feel of the \nscene. The color palette is warm, with soft golden hues, suggesting either dawn or dusk. The sky is filled with soft clouds, further \nenhancing the mystical ambiance of the image.Overall, the image is rich with details that evoke a sense of adventure and fantasy, \nsuggesting a world where exploration and perhaps combat are common.\nGPT-4V\nThe image is a beautifully rendered fantasy scene featuring a man standing in front of a large, colorful castle. The man \nappears to be a warrior, and he is holding a sword in his hand. The castle is situated on a hill, and it is surrounded by a \npicturesque landscape.In the sky above the castle, there are two hot air balloons flying, adding to the whimsical atmosphere \nof the scene. Additionally, there are two boats visible in the water, one near the castle and the other further away. The \noverall composition of the image creates a sense of adventure and wonder.\nSPHINX\nFigure 9: Results on scene understanding from image. Possibly influenced by the Japanese style of\narchitecture in the image, Gemini intersperses a phrase in Japanese within their English response.\nGreen highlights the right answer. Yellow highlights the incompetence in performing the task.\nRefer to Section 2.2 for detailed discussions.\n18\nWhat are the connections between the top and the down pictures?\nGemini Pro\nThe photos show the aftermath of a car accident. The car is a \nyellow BMW M4. The car is heavily damaged. The front of the \ncar is smashed in. The windshield is shattered. The car is on the \nside of the road. There are police officers and other people \nstanding around the car. The police officers are investigating the \naccident. The other people are watching. The accident happened \nin Hong Kong.\nScene Understanding from Image\nPrompt\nSec. 2.2 Scene-Level Perception\nThe top and bottom images appear to be two different perspectives of the same scene. Here are the connections between them:\n1.Car Crash Sit: Both images show a yellow sports car that has crashed into the side of a building, suggesting they are depicting the \nsame incident.\n2. Emergency Response: There are emergency responders present in both images. In the top picture, police officers can be seen, while \nthe bottom picture shows what appears to be a group of onlookers or additional responders.\n3. Time of Day: It seems to be nighttime in both images, as indicated by the artificial lighting and dark sky.\n4. Debris and Damage: There is debris scattered on the road in both images, and the damage to the building and car is consistent, \nfurther indicating that these photos were taken at the same event.\n5. Angle of View: The top photo is taken at street level, likely by someone standing nearby, while the bottom photo is from an elevated \nposition, possibly from a building across the street, looking down at the scene.\n6. Police Presence: In the top image, the focus is on the police officers who are standing near the crashed car. The bottom image \nshows the broader context of the incident, with more bystanders and possibly more officers or emergency service personnel.The \ncombination of these elements strongly suggests that both photographs are of the same event from different viewpoints, capturing the \naftermath of a vehicle accident that occurred at night, involving a yellow sports car, with police and other individuals present at the \nscene.\nGPT-4V\nThe top picture shows a group of police officers standing in the \nstreet, while the bottom picture shows a group of people \ngathered on the sidewalk. The connection between the two \npictures is that the police officers are likely responding to an \nincident or situation that has caused the crowd to gather on the \nsidewalk. The presence of the police officers suggests that they \nare trying to maintain order and ensure the safety of the people \nin the area.\nSPHINX\nFigure 10: Results on scene understanding from image. Only GPT-4V correctly identifies that these\ntwo images represent different perspectives of the same scene. Green highlights the right answer.\nRefer to Section 2.2 for detailed discussions.\n19\nPlease describe this scene according to these temporal images.\nGemini Pro\nThe scene is an office break room with a brown mosaic wall, gray blinds on the windows, and gray and blue \nchairs. There are two round tables with white tabletops. On one table are two laptops and a phone. On the \nother table is a box of tissues. There is a wooden cabinet with a white countertop. On the countertop are three\npotted plants and an electric kettle. There is a blue chair in front of the cabinet. There is a gray chair in the \ncorner of the room. There is a hallway next to the break room with white walls and gray carpet.\nScene Understanding from Video\nPrompt\nSec. 2.2 Scene-Level Perception\nThe scene depicted across the three images is of a modern, indoor space, likely within a commercial or \noffice building. The area is fitted with a casual seating arrangement suitable for brief meetings or \nbreaks. In the images, you can see:\n1. A room with a neutral color palette, featuring a series of grey upholstered chairs arranged around \nsmall, round white tables. The flooring is a polished stone tile, reflecting the light from overhead \nfixtures. The walls are adorned with a mix of wood paneling and large, closed window blinds, which \nsuggest an emphasis on privacy and light control.\n2. The same room viewed from a different angle shows more of the wood-paneled wall, decorated with \na mosaic tile backsplash adding a touch of color to the otherwise muted tones of the room. This \ndecoration is behind a counter that houses potted plants, which introduce a natural element to the space. \nAdditional seating options are visible, including a chair with a distinct blue fabric and a metal base, \ndiffering from the wooden-legged chairs seen in the first image.\n3. A third perspective offers a view of the entryway to the room, marked by a wooden door on one side \nand an open walkway on the other, leading to what appears to be a corridor carpeted in dark grey. This \nsuggests that the seating area serves as a transitional space between different sections of the building.\nThe lighting is consistent across all three images, with a combination of recessed ceiling lights and \nnatural light, which might be entering through the covered windows. Overall, the setting appears to be \ndesigned for functionality and comfort, providing a space where individuals can sit for short durations, \neither to rest, wait, or engage in informal discussions.\nGPT-4V\nThe image showcases a spacious room with a variety of furniture and decorative elements. There are \nseveral chairs scattered throughout the room, with some positioned near tables. The tables are \naccompanied by vases, which are placed at various locations. The room also features a wall with a \nunique design, adding to its aesthetic appeal.\nSPHINX\nFigure 11: Results on scene understanding from video. Gemini correctly merges the information\nfrom the first two frames to accurately describe that the scene contains two round tables and three\npotted plants. Green highlights the right answer. Refer to Section 2.2 for detailed discussions.\n20\nWhat behaviors should I pay attention to in this scene \nin the picture?\nCommonsense\nPrompt\nGemini Pro\nThe picture shows a \"No Smoking\" sign on a table. \nTherefore, the behavior to pay attention to is people \nsmoking. If you see someone smoking, you should \npolitely remind them that smoking is not allowed in \nthe area.\nIn the scene, there is a \"no smoking\" sign displayed \non a wooden table. This sign serves as a reminder to \npay attention to the behavior of smoking in the area. \nIt is important to respect the rules and regulations \nregarding smoking in public spaces, as it can affect \nthe comfort and well-being of others. If you are a \nsmoker, you should ensure that you do not smoke \nwithin the vicinity of the sign or where others might \nbe affected by your smoking.\nSPHINX\nIn the scene presented in the image, the key \nbehavior to pay attention to is the prohibition of \nsmoking. The \"No Smoking\" sign is prominently \ndisplayed, indicating that smoking is not allowed in \nthat area. This is likely a public space or a section \nwithin a business establishment where smoking is \nbanned due to health regulations, consideration for \nnon-smokers, or fire safety concerns. If you are in \nsuch a place, you should refrain from smoking and \nalso be mindful of other similar signs that may \nindicate additional rules or guidelines to follow.\nGPT-4V\nSec. 2.3 Knowledge-based Perception\nFigure 12: Results on commonsense perception.\nGreen highlights the right answer. Refer to\nSection 2.3 for detailed discussions.\n21\nWhat behaviors should I pay attention to in this scene \nin the picture?\nCommonsense\nPrompt\nGemini Pro\nThe picture shows a library. In the library, people \nare reading, which is appropriate behavior. However, \nsome behaviors are not appropriate in a library. For \nexample, talking loudly, eating, and drinking are not \nallowed in most libraries. It is important to be \nrespectful of others when using the library.\nIn the scene, there are several people sitting at \ntables and working on laptops in a library. As an AI, \nI cannot directly observe the behavior of the \nindividuals in the image, but I can provide some \ngeneral guidelines for library etiquette.\nBe respectful of others: Keep your voice down and \navoid loud conversations or phone calls.\nKeep noise levels to a minimum: Avoid playing \nmusic or using headphones at a high volume.\nKeep the library clean: Dispose of any trash in \ndesignated bins and return books to their shelves \nafter use.\nBe mindful of personal belongings: Keep your \nbelongings organized and avoid leaving them \nunattended.\nFollow library rules and policies: Adhere to any \nspecific rules or guidelines set by the library, such \nas time limits for using the computers or quiet areas.\nBy following these guidelines, you can help \nmaintain a pleasant and productive environment for \neveryone using the library.\nSPHINX\nIn the picture, you see a library setting with \nindividuals engaged in various activities. Here are \nsome behaviors and aspects you might pay attention \nto:\nStudying and Reading: Notice how individuals are \nfocused on their reading materials or laptops, which \nsuggests a quiet, studious atmosphere.\nSeating Posture: Observe how people are seated. \nGood posture can reduce fatigue and prevent strain \nduring long study sessions.\nPersonal Space: The arrangement of tables and \nchairs allows for personal space, which is important \nin public settings for comfort and concentration.\nQuiet Environment: Libraries are typically quiet \nzones. The calmness in the picture suggests that it's \na place where silence is respected to maintain a \nconducive environment for studying.\nUse of Technology: Some individuals may be using \nlaptops or other electronic devices, which indicates \nthe integration of digital tools in learning and \nresearch.\nOrganization: The neat arrangement of books and \nthe clean environment suggest organizational \nstandards in the library, aiding in the easy location \nof materials and maintenance of a pleasant \nenvironment.\nHydration: You can see water bottles on the tables, \nwhich is a good practice to stay hydrated during \nlong periods of study.\nPersonal Belongings: Personal belongings are kept \nclose to the individuals, which is important for \nsecurity and easy access.\nThis scene encapsulates the typical activities and \nbehaviors expected in a library setting.\nGPT-4V\nSec. 2.3 Knowledge-based Perception\nFigure 13: Results on commonsense perception.\nGreen highlights the right answer.\nYellow\nhighlights the incompetence in performing the task. Refer to Section 2.3 for detailed discussions.\n22\nIf I want to go to Antarctica, should I wear the clothes in Figure 1 or Figure 2? Please state the \nreason.\nGemini Pro\nYou should wear the clothes in Figure 2.\nAntarctica is the coldest continent on Earth, with average temperatures below freezing all year round. The \nclothes in Figure 1 are not suitable for such cold weather, as they are lightweight and do not provide \nenough insulation. The clothes in Figure 2, on the other hand, are much warmer and would be more \nappropriate for a trip to Antarctica. They are made of a thicker material and have a hood, which will help to \nkeep you warm in the cold weather.\nCommonsense\nPrompt\nSec. 2.3 Knowledge-based Perception\nFor a trip to Antarctica, you should wear the clothes shown in Figure 2. The reason is that Antarctica is an \nextremely cold environment, with temperatures that can drop well below freezing. The clothing in Figure 2 \nappears to be a heavy, insulated parka with a fur-lined hood, which is designed to provide warmth in such \ncold conditions. This type of outerwear is essential for protection against hypothermia and frostbite.\nOn the other hand, the clothing in Figure 1 is a basic short-sleeve t-shirt, which offers very little insulation \nand is not suitable for cold climates. Wearing such a garment in Antarctica would not provide the necessary \nprotection against the harsh elements and could be dangerous to your health due to the extreme cold.\nGPT-4V\nYou should wear the clothes in Figure 1. Antarctica is a cold and windy environment, so you would need \nwarm and insulated clothing to protect yourself from the elements. The clothes in Figure 1 are designed to \nkeep you warm and comfortable in cold weather, while the clothes in Figure 2 are not suitable for such \nconditions.\nSPHINX\nFigure 14: Results on commonsense perception. While Sphinx acknowledges the necessity of\nheavy garments in chilly conditions, it mistakenly classifies a T-shirt as appropriate for cold weather\nprotection. Green highlights the right answer. Red highlights the wrong answer. Refer to Section\n2.3 for detailed discussions.\n23\nMay I touch the object in these two pictures with my hands? Please analyze them separately.\nGemini Pro\nThe first picture shows a gas stove with a flame. You should not touch it with your hands because it is very \nhot and could burn you.\nThe second picture shows a fake flame that is powered by electricity. It is safe to touch with your hands \nbecause it does not produce any heat.\nCommonsense\nPrompt\nSec. 2.3 Knowledge-based Perception\nThe images you've provided show two different sources of heat.\nFor the first image, it depicts an active gas stove burner with a blue flame. It is unsafe to touch the flames or \nthe burner as it can cause severe burns. The stove is designed for cooking and should only be operated with \nproper kitchen utensils and safety precautions.\nThe second image shows what appears to be an artificial decorative flame, typically used for ambiance. \nThese are often electric and use lights, such as LEDs, and fabric to simulate the appearance of fire. While it's \nnot as dangerous as real fire, the electric components and any heat they generate can still pose a risk. It's not \nadvisable to touch the artificial flames, especially if the device is plugged in, to avoid the risk of electrical \nshock or burns from any heated elements.\nIn summary, it's not safe to touch the objects in either picture with your hands due to the risk of burns or \nelectrical shock.\nGPT-4V\nThe first image shows a gas stove with a flame on it, while the second image shows a hanging fire pit with \nflames inside.\nSPHINX\nFigure 15: Results on commonsense perception. Both Gemini and GPT-4V identify that the second\nimage depicts flames disguised using lighting, but the distinction lies in Gemini\u2019s inference that\nthe objects in the image can be touched, whereas GPT-4V additionally warns of the risk of electric\nshock. Green highlights the right answer. Red highlights the wrong answer. Yellow highlights\nthe incompetence in performing the task. Refer to Section 2.3 for detailed discussions.\n24\nWhich country is the finger pointing to in the \nimage? What continent does this country belong \nto?\nScientific Knowledge\nPrompt\nSec. 2.3 Knowledge-based Perception\nWhat is the landform in the picture?\nPrompt\nThe finger in the image is pointing to India. \nIndia is located on the continent of Asia.\nGPT-4V\nGemini Pro\nGPT-4V\nThe landform in the picture appears to be a desert landscape \nfeaturing large, wind-eroded rock formations known as \nyardangs. These are streamlined protuberances carved from \nbedrock or any consolidated or semi-consolidated material by \nthe dual action of wind abrasion by dust and sand, and \ndeflation, which is the removal of loose material by wind \nturbulence.\nYardangs\nare\ntypically\nfound\nin\ndesert\nenvironments where the wind direction is uniform and strong, \nand they are oriented with their long axis parallel to the \nprevailing wind direction, indicating the direction of the \nerosive winds. The smooth, rounded surfaces between the \nyardangs suggest the presence of sand dunes, which are \nformed and shaped by the wind as well. This combination of \nfeatures suggests an arid, wind-swept environment with \nlimited vegetation and moisture.\nThe landform in the picture is a group of large \nsand dunes.\nSPHINX\nThe finger is pointing to Australia, which is a \ncountry in the continent of Oceania.\nSPHINX\nGemini Pro\nIndia. Asia.\nThe landform in the picture is a desert.\nFigure 16: Results on scientific knowledge perception. Green highlights the right answer. Red\nhighlights the wrong answer. Yellow highlights the incompetence in performing the task. Refer to\nSection 2.3 for detailed discussions.\n25\nWhat chemical experiment equipment is in the \nimage?\nGemini Pro\n1.  2 glass beakers\n2.  1 glass stirring rod\n3.  1 large glass bowl\nScientific Knowledge\nPrompt\nSec. 2.3 Knowledge-based Perception\nWhat physical phenomenon is reflected in the \nimage?\nPrompt\nThe image shows a person pouring a liquid into \na large beaker using a smaller beaker. This setup \nis common in chemical experiments where \nliquids are measured, transferred, or mixed. The \nlarger container is a beaker, which is typically \nused in a laboratory setting for stirring, mixing, \nand heating liquids. It appears to have a spout \nfor pouring. The smaller container looks like \nanother beaker that is being used to transfer \nliquid into the larger one. There are no other \ndistinctive features that suggest any other \nspecific equipment or the nature of the liquid \ninvolved.\nGPT-4V\nGemini Pro\nThe image shows the physical phenomenon of refraction. \nRefraction is the bending of light as it passes from one \nmedium to another. In this case, the light is passing from air \nto glass and then back to air. The glass has a higher refractive \nindex than air, so the light bends more when it enters the glass \nand then bends back when it exits the glass. This causes the \nlight to appear to be displaced when viewed from the side.\nGPT-4V\nThe image shows a demonstration of light refraction and \ndispersion. It appears to feature two curved pieces of \ntransparent material, likely acrylic or glass, with light beams \npassing through them. The light beams are bending as they \npass through these materials, a process known as refraction. \nAdditionally, the beams are spreading out into a spectrum of \ncolors, which is indicative of dispersion \u2014 the separation of \nlight into colors of different wavelengths due to varying \ndegrees of refraction for each color. This is similar to the \nphenomenon that occurs in a prism, where white light is split \ninto its constituent colors.\nThe image shows the phenomenon of light \npassing through two glass objects, creating a \ngreen and red light effect.\nSPHINX\nThe image shows a beaker, a test tube, and a \nglass.\nSPHINX\nFigure 17: Results on scientific knowledge perception. Green highlights the right answer. Red\nhighlights the wrong answer. Yellow highlights the incompetence in performing the task. Refer to\nSection 2.3 for detailed discussions.\n26\nWhy can I still see my hand in the mirror when \nthe book covers the hand?\nGemini Pro\nThe book is not covering your hand. The image \nin the mirror is flipped, so it looks like the book \nis covering your hand, but it is actually covering \nthe mirror.\nScientific Knowledge\nPrompt\nSec. 2.3 Knowledge-based Perception\nWhat physical phenomenon is reflected in the \nimage?\nPrompt\nThe phenomenon you're observing in the image is due to \nthe way light reflects off the mirror. When the light from \nyour hand hits the mirror, it reflects back at the same \nangle it hits the surface. This is known as the law of \nreflection. Even though the book is covering your hand \nfrom direct view, the light reflecting off your hand is not \nblocked by the book, since the reflective angle is \ndifferent from the angle at which you are looking at the \nhand. Therefore, you can still see the image of your \nhand in the mirror. This can create an interesting and \nsometimes confusing visual effect, as demonstrated in \nthe photo you've shared.\nGPT-4V\nGemini Pro\nThe image shows the dispersion of light. This is a \nphenomenon that occurs when light passes through a \nprism or other transparent object and is broken up into \nits component colors. The different colors of light are \nrefracted (bent) at different angles, causing them to \nspread out into a spectrum.\nThe image shows a display of optical phenomena where light \nis interacting with transparent, prism-like objects. This \ninteraction is causing the dispersion of light, which is a \nphysical phenomenon where white light is separated into its \ncomponent colors (typically seen as a spectrum ranging from \nred to violet). This occurs because different colors of light are \nrefracted by different amounts when passing through a \nmedium like a prism, due to their varying wavelengths. The \nresult is the creation of a spectrum of colors, similar to what is \nseen in a rainbow. The sharp lines and patterns suggest that \nthe light rays are being reflected and refracted by the objects, \ncreating a visually striking array of colors on the surface \nbeneath them.\nGPT-4V\nYou can still see your hand in the mirror because the \nmirror is reflecting the image of your hand, not the book. \nThe book is placed on top of the mirror, and while it \nmay partially obscure your view of the mirror, it does \nnot block the reflection of your hand. The mirror's \nsurface is smooth and reflective, allowing the image of \nyour hand to be clearly visible through it.\nSPHINX\nThe image reflects the physical phenomenon of light \nrefraction, which occurs when light passes through a \ntransparent object, such as a glass cube. In this case, the light \nis refracting through the glass cube, creating a visually \nstriking effect with multiple colors and shapes. The refraction \nof light can also create rainbows, as seen in the image, where \nthe light is bent and separated into its different wavelengths, \nproducing a spectrum of colors.\nSPHINX\nFigure 18: Results on scientific knowledge perception. Green highlights the right answer. Yellow\nhighlights the incompetence in performing the task. Refer to Section 2.3 for detailed discussions.\n27\nWhat historical event is shown in the picture?\nGemini Pro\nThe picture shows the fifth Solvay Conference \non Physics in 1927.\n Historical Knowledge\nPrompt\nSec. 2.3 Knowledge-based Perception\nPrompt\nThe picture you have uploaded is of the Solvay \nConference, specifically the fifth conference \nheld in October 1927 in Brussels. It was one of \nthe most famous and influential conferences in \nthe history of physics, attended by some of the \nmost notable figures in the field at the time. The \nconference was centered on the newly \nformulated quantum theory, and the discussions \nthat took place helped shape the future of \nquantum mechanics. Notably, the photo captures \nkey figures such as Albert Einstein, Niels Bohr, \nand Marie Curie among others.\nGPT-4V\nGemini Pro\nThe picture shows the Apollo 11 mission, which \nwas the first manned mission to land on the \nMoon. The mission took place on July 20, 1969. \nThe Apollo 11 crew consisted of Commander \nNeil Armstrong, Command Module Pilot \nMichael Collins, and Lunar Module Pilot Edwin \n\"Buzz\" Aldrin. Armstrong and Aldrin became \nthe first humans to walk on the Moon. The \nApollo 11 mission was a major milestone in the \nhistory of space exploration.\nThe picture depicts an astronaut on the lunar \nsurface with the Lunar Module and an American \nflag, indicating a moment from the Apollo moon \nlandings. This event is significant as it \nrepresents one of the human spaceflight \nmissions conducted by NASA during the late \n1960s and early 1970s, showcasing a landmark \nachievement in human space exploration.\nGPT-4V\nThe historical event shown in the picture is the \nApollo 11 moon landing, where an astronaut is \nstanding on the moon with an American flag.\nSPHINX\nThe picture shows a group of men from the \n1920s, which is a historical event.\nSPHINX\nWhat historical event is shown in the picture?\nFigure 19: Results on historical knowledge perception. Green highlights the right answer. Red\nhighlights the wrong answer. Refer to Section 2.3 for detailed discussions.\n28\nProvide a description of the image in English and also \nin the language native to the geolocation shown in the \npicture.\nGemini Pro\nThe image shows four people performing a lion \ndance in front of a traditional Chinese building. The \nperformers are wearing colorful lion costumes and \nare moving in a coordinated fashion to the beat of a \ndrum. The building in the background has a red \nroof and is decorated with red lanterns.\nIn Chinese:\n\u56fe\u2f5a\u663e\u793a\u4e86\u56db\u4e2a\u2f08\u5728\u4f20\u7edf\u7684\u4e2d\u56fd\u5efa\u7b51\u524d\u8868\u6f14\u821e\n\u72ee\u3002\u8868\u6f14\u8005\u2f9d\u7a7f\u4e94\u989c\u516d\u2f8a\u7684\u72ee\u2f26\u670d\u88c5\uff0c\u968f\u7740\u2fce\n\u70b9\u306b\u5408\u308f\u305b\u3066\u505a\u7740\u534f\u8c03\u7684\u52a8\u4f5c\u3002\u80cc\u666f\u4e2d\u7684\u5efa\u7b51\n\u6709\u7ea2\u2f8a\u7684\u5c4b\u9876\uff0c\u5e76\u88c5\u9970\u6709\u7ea2\u2f8a\u7684\u706f\u7b3c\u3002\nMulticultural Customs\nPrompt\nThe image depicts a traditional Chinese lion dance. \nThere are three colorful lion costumes, each \noperated by two dancers, performing on a red carpet. \nThe lions are yellow, pink, and blue, adorned with \nintricate patterns and expressive faces. Behind them \nis a classical Chinese building with decorative red \nlanterns hanging from its eaves, which suggests that \nthis scene may be set during a festive occasion, \nlikely in a location with historical significance.\nIn Chinese, this description would be:\n\u8fd9\u5f20\u56fe\u2f5a\u5c55\u793a\u4e86\u4e2d\u56fd\u4f20\u7edf\u7684\u821e\u72ee\u8868\u6f14\u3002\u6709\u4e09\u5957\n\u2f8a\u5f69\u9c9c\u8273\u7684\u72ee\u2f26\u670d\u88c5\uff0c\u6bcf\u5957\u90fd\u7531\u4e24\u4f4d\u821e\u8005\u64cd\u4f5c\uff0c\n\u5728\u7ea2\u2f8a\u7684\u5730\u6bef\u4e0a\u8868\u6f14\u3002\u72ee\u2f26\u7684\u989c\u2f8a\u6709\u2ee9\u2f8a\u3001\u7c89\n\u2f8a\u548c\u84dd\u2f8a\uff0c\u88c5\u9970\u6709\u590d\u6742\u7684\u56fe\u6848\u548c\u5bcc\u6709\u8868\u60c5\u7684\u2faf\n\u5b54\u3002\u4ed6\u4eec\u80cc\u540e\u662f\u2f00\u5ea7\u5177\u6709\u53e4\u5178\u4e2d\u56fd\u2edb\u683c\u7684\u5efa\u7b51\uff0c\n\u5c4b\u6a90\u4e0b\u6302\u7740\u88c5\u9970\u6027\u7684\u7ea2\u706f\u7b3c\uff0c\u8fd9\u6697\u793a\u8fd9\u4e2a\u573a\u666f\n\u53ef\u80fd\u8bbe\u7f6e\u5728\u2f00\u4e2a\u6709\u5386\u53f2\u610f\u4e49\u7684\u5730\u70b9\uff0c\u5f88\u53ef\u80fd\u662f\n\u5728\u8282\u2f47\u5e86\u5178\u671f\u95f4\u3002\nGPT-4V\nIn this image, there is a group of four colorful \ndragon costumes standing in front of a building. \nThe dragons are arranged in a line, with one on the \nleft, two in the middle, and one on the right. The \ncostumes are vibrant and eye-catching, adding a \nfestive atmosphere to the scene.\nThere are several people present in the image, with \nsome standing near the dragon costumes and others \nscattered around the area. They appear to be \nobserving the dragon costumes or participating in \nthe event.\nThe scene takes place in front of a building, which \nserves as a backdrop for the dragon costumes. The \nbuilding has a red carpet laid out in front of it, \nadding to the overall festive ambiance of the scene.\nSPHINX\nSec. 2.3 Knowledge-based Perception\nFigure 20: Results on multicultural knowledge perception. Green highlights the right answer. Red\nhighlights the wrong answer. Refer to Section 2.3 for detailed discussions.\n29\nProvide a description of the image in English and also \nin the language native to the geolocation shown in the \npicture.\nGemini Pro\nThe image shows a group of Japanese women \nperforming a traditional dance. They are wearing \ntraditional Japanese clothing and are playing \ntraditional Japanese instruments. The background is \na green wall with gold flecks.\nIn the native language of the geolocation shown in \nthe picture, the image would be described as \nfollows:\n\u753b\u50cf\u306b\u306f\u3001\u2f47\u672c\u306e\u4f1d\u7d71\u821e\u8e0a\u3092\u62ab\u9732\u3059\u308b\u2f25\u6027\u305f\n\u3061\u306e\u30b0\u30eb\u30fc\u30d7\u304c\u6620\u3063\u3066\u3044\u307e\u3059\u3002\u5f7c\u2f25\u305f\u3061\u306f\u2f47\n\u672c\u306e\u4f1d\u7d71\u2f90\u88c5\u3092\u7740\u3066\u3001\u2f47\u672c\u306e\u4f1d\u7d71\u697d\u5668\u3092\u6f14\u594f\n\u3057\u3066\u3044\u307e\u3059\u3002\u80cc\u666f\u306f\u7dd1\u306e\u58c1\u3067\u3001\u2fa6\u2f8a\u306e\u6591\u70b9\u304c\n\u3042\u308a\u307e\u3059\u3002\nMulticultural Customs\nPrompt\nThe image depicts a traditional Japanese scene with four individuals. In the foreground, there are three people seated \nin a row, dressed in black kimonos, playing shamisen, a traditional Japanese three-stringed instrument. Each \nmusician is focused on their instrument, which rests on a small wooden platform. The background features a \nperformer in the center, standing and dressed in a vibrant, elaborate kimono with a bold design of waves and clouds \nin shades of purple, red, and white. The performer is wearing traditional white face makeup and a black wig styled in \na classical manner, holding a paper umbrella and wearing traditional geta footwear. This setting suggests a cultural \nperformance, likely related to Kabuki theatre.\nIn Japanese:\n\u3053\u306e\u753b\u50cf\u306b\u306f\u3001\u4f1d\u7d71\u7684\u306a\u2f47\u672c\u306e\u30b7\u30fc\u30f3\u304c\u63cf\u304b\u308c\u3066\u3044\u307e\u3059\u3002\u524d\u666f\u306b\u306f\u3001\u9ed2\u3044\u7740\u7269\u3092\u7740\u305f3\u2f08\u306e\u2f08\u7269\u304c\u2f00\u5217\u306b\n\u5ea7\u308a\u3001\u4e09\u5473\u7dda\u3068\u3044\u3046\u2f47\u672c\u306e\u4f1d\u7d71\u7684\u306a\u4e09\u5f26\u697d\u5668\u3092\u6f14\u594f\u3057\u3066\u3044\u307e\u3059\u3002\u5404\u30df\u30e5\u30fc\u30b8\u30b7\u30e3\u30f3\u306f\u3001\u2f29\u3055\u306a\u2f4a\u88fd\u306e\u53f0\u306e\n\u4e0a\u306b\u7f6e\u304b\u308c\u305f\u697d\u5668\u306b\u96c6\u4e2d\u3057\u3066\u3044\u307e\u3059\u3002\u80cc\u666f\u306b\u306f\u3001\u4e2d\u592e\u306b\u2f74\u3064\u30d1\u30d5\u30a9\u30fc\u30de\u30fc\u304c\u3044\u3066\u3001\u6ce2\u3068\u96f2\u306e\u2f24\u80c6\u306a\u30c7\u30b6\u30a4\n\u30f3\u304c\u65bd\u3055\u308c\u305f\u7d2b\u3001\u2f9a\u3001\u2f69\u306e\u9bae\u3084\u304b\u306a\u2f8a\u5f69\u306e\u8c6a\u83ef\u306a\u7740\u7269\u3092\u2f9d\u306b\u307e\u3068\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30fc\u306f\u3001\u4f1d\u7d71\n\u7684\u306a\u2f69\u3044\u9854\u306e\u5316\u7ca7\u3068\u53e4\u5178\u7684\u306a\u30b9\u30bf\u30a4\u30eb\u306e\u9ed2\u3044\u304b\u3064\u3089\u3092\u7740\u2f64\u3057\u3001\u548c\u5098\u3092\u6301\u3061\u3001\u4f1d\u7d71\u7684\u306a\u4e0b\u99c4\u3092\u5c65\u3044\u3066\u3044\u307e\u3059\u3002\n\u3053\u306e\u8a2d\u5b9a\u306f\u3001\u6b4c\u821e\u4f0e\u306b\u95a2\u9023\u3057\u305f\u2f42\u5316\u7684\u306a\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u793a\u5506\u3057\u3066\u3044\u307e\u3059\u3002\nGPT-4V\nIn this image, there is a group of people dressed in traditional Japanese clothing, known as kimonos. They are \nplaying musical instruments, with some of them holding umbrellas. The group consists of four people, with one \nperson standing out from the rest. This person is holding an umbrella and appears to be a geisha, a professional \nfemale entertainer in Japan. The other three people are playing musical instruments, with two of them playing guitars \nand one playing a drum. The scene captures a lively and cultural moment in Japan.\nSPHINX\nSec. 2.3 Knowledge-based Perception\nFigure 21: Results on multicultural knowledge perception. Green highlights the right answer. Refer\nto Section 2.3 for detailed discussions.\n30\nProvide a description of the image in English and also \nin the language native to the geolocation shown in the \npicture.\nGemini Pro\nThe image shows a man and a woman wearing \ntraditional Korean clothing, or hanbok, in front of a \ntraditional Korean building. The man is wearing a \nblack gat and the woman is wearing a pink hanbok \nwith a floral pattern. The woman is holding a mirror \nand applying makeup while the man is looking at \nhis phone.\n\uc774\ubbf8\uc9c0\ub294 \ud55c\ubcf5\uc744 \uc785\uc740 \ub0a8\ub140\uac00 \uc804\ud1b5 \ud55c\uc625 \uc55e\uc5d0 \uc11c \n\uc788\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub0a8\uc790\ub294 \uac80\uc740\uc0c9 \uac00\uc8fd\uc744 \n\uc4f0\uace0 \uc5ec\uc790\ub294 \ubd84\ud64d\uc0c9 \uaf43\ubb34\ub2ac \ud55c\ubcf5\uc744 \uc785\uace0 \uc788\uc2b5\ub2c8\n\ub2e4. \uc5ec\uc790\ub294 \uac70\uc6b8\uc744 \ub4e4\uace0 \ud654\uc7a5\uc744 \ud558\uace0 \ub0a8\uc790\ub294 \ud734\n\ub300\uc804\ud654\ub97c \ubcf4\uace0 \uc788\uc2b5\ub2c8\ub2e4.\nMulticultural Customs\nPrompt\nThe image shows two people dressed in traditional Korean hanbok. The individual on the left appears to be male, \nwearing a blue hanbok with a black gat (a traditional Korean hat), and is holding a smartphone in his hand, possibly \ntaking a picture or reading something on the screen. The individual on the right appears to be female, wearing a \nwhite hanbok with pink floral patterns, holding what looks like a traditional hand mirror and seems to be gazing at \nher reflection. They are outdoors, and in the background, there is traditional Korean architecture, possibly a palace, \nand other blurred figures that suggest a public place with visitors.\nIn Korean:\n\uc774\ubbf8\uc9c0\uc5d0\ub294 \ud55c\ubcf5\uc744 \uc785\uc740 \ub450 \uc0ac\ub78c\uc774 \ub098\uc635\ub2c8\ub2e4. \uc67c\ucabd\uc5d0 \uc788\ub294 \uc0ac\ub78c\uc740 \ub0a8\uc131\uc73c\ub85c \ubcf4\uc774\uba70, \ud30c\ub780 \ud55c\ubcf5\uc5d0 \uac80\uc740\uc0c9 \uac13\uc744 \n\uc4f0\uace0 \uc788\uace0, \uc2a4\ub9c8\ud2b8\ud3f0\uc744 \ub4e4\uace0 \uc0ac\uc9c4\uc744 \ucc0d\uac70\ub098 \ud654\uba74\uc744 \ubcf4\uace0 \uc788\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4. \uc624\ub978\ucabd\uc5d0 \uc788\ub294 \uc0ac\ub78c\uc740 \uc5ec\uc131\uc73c\ub85c \n\ubcf4\uc774\uba70, \ud551\ud06c\uc0c9 \uaf43\ubb34\ub2ac\uac00 \uc788\ub294 \ud770\uc0c9 \ud55c\ubcf5\uc744 \uc785\uace0 \uc788\uace0, \uc804\ud1b5 \uc190\uac70\uc6b8\uc744 \ub4e4\uace0 \uc788\ub294 \ub4ef \uac70\uc6b8\uc5d0 \ube44\uce5c \uc790\uc2e0\uc758 \ubaa8\uc2b5\n\uc744 \ubc14\ub77c\ubcf4\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub4e4\uc740 \uc57c\uc678\uc5d0 \uc788\uace0 \ubc30\uacbd\uc5d0\ub294 \ud55c\uad6d \uc804\ud1b5 \uac74\ucd95\ubb3c, \uc544\ub9c8\ub3c4 \uad81\uad90\uc774 \ubcf4\uc774\uba70, \uc8fc\ubcc0\uc5d0 \ud750\ub9bf\n\ud558\uac8c \ubcf4\uc774\ub294 \ub2e4\ub978 \uc778\ubb3c\ub4e4\uc774 \uad00\uad11\uac1d\uc774 \uc788\ub294 \uacf5\uacf5\uc7a5\uc18c\uc784\uc744 \uc554\uc2dc\ud569\ub2c8\ub2e4.\nGPT-4V\nIn the image, a man and a woman dressed in traditional Korean clothing are standing together. The woman is holding \na cell phone, and the man is holding a mask. They appear to be taking a selfie together.\nIn the background, there are several other people walking around, possibly enjoying the outdoor setting. The scene \ncaptures a moment of cultural exchange and shared experience between the couple dressed in traditional Korean attire.\nSPHINX\nSec. 2.3 Knowledge-based Perception\nFigure 22: Results on multicultural knowledge perception. Green highlights the right answer. Refer\nto Section 2.3 for detailed discussions.\n31\nWho is the person in the image?\nCelebrity Recognition and Description\nPrompt\nGemini Pro\nThe person in the image is a fictional character \nnamed Doctor Strange. He is a superhero who \nappears in Marvel comic books. He is a surgeon who \nbecomes a powerful sorcerer after a car accident \nruins his hands. He uses his powers to protect the \nworld from magical threats.\nThe image is a blend of two fictional characters: \nIron Man, known for his red and gold power armor, \nand Doctor Strange, known for his mastery of the \nmystical arts. The character in the image appears to \ncombine elements of both, wearing Iron Man's \narmor and also showing Doctor Strange's Eye of \nAgamotto and mystical symbols, suggesting a \ncreative crossover of their abilities and aesthetics.\nGPT-4V\nThe person in the image is Iron Man, a popular \nsuperhero from the Marvel Comics universe.\nSPHINX\nWho is the person in the image?\nPrompt\nGemini Pro\nTaylor Swift\nThe person in the image is Taylor Swift, a well-\nknown American singer-songwriter. The image \nappears to be promotional material for \"The Eras \nTour\".\nGPT-4V\nThe person in the image is Taylor Swift.\nSPHINX\nSec. 2.3 Knowledge-based Perception\nFigure 23: Results on celebrity recognition and description. Sphinx mistakes Doctor Strange in an\nIron Man suit for Iron Man himself. Green highlights the right answer. Red highlights the wrong\nanswer. Refer to Section 2.3 for detailed discussions.\n32\nCelebrity Recognition and Description\nThe image you've uploaded appears to be from a \nbasketball game. However, I'm not able to identify \nthe person shooting the ball due to the policy \nagainst identifying real people in images. If you're \nlooking for information on a specific player or \ngame, I can try to help with general questions or \nprovide information about the sport of basketball.\nGPT-4V\nIn the picture, a basketball player is shooting the ball.\nSPHINX\nWho is shooting the ball in the picture?\nPrompt\nGemini Pro\nThe player shooting the ball is Keldon Johnson.\nThe person in the image is a soccer player, but I am \nunable to provide the identity of real people in \nimages. The soccer kit he is wearing in the photo is \npink with the XBT sponsor logo, which is \nassociated with the Miami-based soccer team, Inter \nMiami CF. This team competes in Major League \nSoccer (MLS), the highest level of soccer in the \nUnited States and Canada.\nGPT-4V\nThank you for the information. It is important to note \nthat the image only shows the soccer player wearing a \npink jersey and does not provide any context about \nthe specific team or league he is associated with.\nSPHINX\nWho is this person? Which team does he play for in \nthe picture?\nPrompt\nGemini Pro\nThis is Lionel Messi. He plays for Paris Saint-\nGermain in the picture.\nSec. 2.3 Knowledge-based Perception\nFigure 24: Results on celebrity recognition and description. When the images depict recently updated\ninformation, Gemini\u2019s descriptions tend to be inaccurate. GPT-4V, due to considerations of privacy\nprotection, avoids addressing questions related to the identification of celebrities. Green highlights\nthe right answer.\nRed highlights the wrong answer.\nYellow highlights the incompetence in\nperforming the task. Refer to Section 2.3 for detailed discussions.\n33\nDescribe the landmark in the image.\nLandmark Recognition and Description\nPrompt\nDescribe the landmark in the image.\nPrompt\nGemini Pro\nThe Canton Tower, also known as Guangzhou Tower, \nis a 604-meter (1,982 ft) multi-purpose observation \ntower in the Haizhu District of Guangzhou, \nGuangdong, China.\nThe landmark in the image is a tall, colorful tower \nthat is lit up at night. It is located near a bridge and \na river, creating a beautiful and vibrant scene.\nSPHINX\nThe image shows a tall, slender tower illuminated \nwith vibrant, multicolored lights against a night sky. \nThe tower has a lattice structure and tapers towards a \npointed antenna at the top. There is a prominent \ncrescent moon design lit up on the tower, adding a \ndistinctive feature to its facade. The tower appears to \nbe a prominent landmark, possibly used for \ntelecommunications or as an observation tower. \nBelow the tower, there are illuminated buildings and \na bridge spanning a body of water, with reflections \nvisible on the surface. This setting suggests the tower \nis located in a developed urban area near a river or \nwaterfront. The tower's design and lighting resemble \nthe Canton Tower in Guangzhou, China, known for \nits colorful nighttime display.\nGPT-4V\nGemini Pro\nThe image shows the Louvre Museum in Paris, \nFrance. It is one of the world's most famous and \nlargest museums. The museum is home to some of \nthe most iconic works of art in the world.\nThe image features the Louvre Pyramid, a large glass \nand metal pyramid that serves as the main entrance to \nthe Louvre Museum in Paris, France. It was designed \nby the architect I. M. Pei and was completed in 1989. \nThe pyramid is a striking example of modern \narchitecture amidst the classical architecture of the \nLouvre Palace. It is illuminated, casting a warm glow \nand reflecting on the wet ground, which suggests that \nthe photo was taken at night or during the evening \nafter rain. The lights within the pyramid highlight its \nintricate metal framework.\nGPT-4V\nThe landmark in the image is the Louvre Pyramid, \nwhich is a glass and metal structure located in front \nof the Louvre Museum in Paris, France.\nSPHINX\nSec. 2.3 Knowledge-based Perception\nFigure 25: Results on landmark recognition and description. Green highlights the right answer.\nRefer to Section 2.3 for detailed discussions.\n34\nDescribe the landmark in the image.\nLandmark Recognition and Description\nPrompt\nDescribe the landmark in the image.\nPrompt\nGemini Pro\nThe image shows the Pantheon in Paris, France. It is \na domed church that was originally built as a church \ndedicated to Saint Genevieve and was later \nconverted into a secular mausoleum containing the \nremains of distinguished French citizens.\nThe landmark in the image is the Capitole building, \nwhich is a large and impressive building with a \ndomed roof.\nSPHINX\nThe landmark in the image is the Panth\u00e9on, located \nin the Latin Quarter in Paris, France. It's a \nneoclassical building that originally served as a \nchurch dedicated to St. Genevieve but now functions \nas a secular mausoleum containing the remains of \ndistinguished French citizens. The fa\u00e7ade is modeled \nafter the Pantheon in Rome, and its dome is a \nsignificant feature of the Parisian skyline.\nGPT-4V\nGemini Pro\nThe image shows the city of Paris, France. The \nEiffel Tower is visible in the distance, and the \nAvenue des Champs-\u00c9lys\u00e9es is in the foreground. \nThe Place de la Concorde is in the center of the \nimage, with the Obelisk of Luxor in the center of \nthe square. The Grand Palais and the Petit Palais are \nlocated on either side of the Place de la Concorde.\nThe image features a view of the Eiffel Tower in the \ndistance, which is a landmark located in Paris, \nFrance. It is seen from a high vantage point \noverlooking city buildings. In the foreground, \nthere's a roundabout with a large, illuminated \nChristmas tree, suggesting that the photo was taken \nduring the holiday season. The streets are lit with \nwarm lights, and there appears to be a bustle of \nactivity, which is common in urban settings. The \nsky is dusky, indicating either dawn or dusk.\nGPT-4V\nThe landmark in the image is the Eiffel Tower, \nwhich is a famous monument in Paris, France.\nSPHINX\nSec. 2.3 Knowledge-based Perception\nFigure 26: Results on landmark recognition and description. Green highlights the right answer.\nRed highlights the wrong answer. Refer to Section 2.3 for detailed discussions.\n35\nDescribe the logo in the image in details.\nPrompt\nGemini Pro\nThe logo is a red and yellow \"M\" inside a yellow square. The words \n\"McDonald's\" are written in red and yellow below the square. The \nlogo is enclosed in a white square with rounded corners. The \nbackground of the logo is white.\nThe image shows a close-up of a food packaging box with a logo \nprinted on it. The logo consists of a stylized cartoon character that \nappears to be a combination of a clown and a chicken. The \ncharacter's face is predominantly white with black outlines and \nfeatures. It has large, wide eyes, with black pupils and thick \neyelashes, and a red nose. There are three red tufts of hair or \nfeathers on the top of the head, resembling a comb. The character is \nsmiling, revealing a red mouth and tongue. To the side of the \ncharacter's head, there's a smaller, round object with a face, possibly \nrepresenting an egg, featuring simplistic eyes and a mouth. Above \nthese characters, there is non-Latin script text in black, presumably \na name or slogan related to the brand. Below the characters, \nprominently displayed, is the familiar McDonald's logo with its \niconic golden arches and the word \"McDonald's\" written underneath. \nThere is also a red letter 'S' in a circle, likely indicating the size or \ntype of the product contained within the box. The overall design has \na playful and friendly aesthetic.\nGPT-4V\nThe logo in the image is for McDonald's, a fast-food restaurant \nchain. It features the McDonald's arches, which are the company's \niconic symbol. The arches are white and red, with the golden arches \nforming an \"M\" shape. The design is simple yet recognizable, and it \nis often associated with the brand's quick-service restaurants and the \nfood they serve.\nSPHINX\nDescribe the logo in the image in details.\nLogo Recognition and Description\nPrompt\nGemini Pro\nThe logo is a basketball with the word \"Wilson\" on it. The word \n\"Wilson\" is in black and has a yellow outline. The basketball is \norange and has a black and white stripe around the middle. The logo \nalso has the words \"NCAA Final Four Edition\" on it. The words \n\"NCAA Final Four Edition\" are in black and are located on the \nbottom of the basketball.\nThe logo in the image is for a basketball, specifically a Wilson \nNCAA Final Four Edition basketball. The logo is prominently \ndisplayed on the surface of the basketball, indicating its association \nwith the NCAA (National Collegiate Athletic Association) and the \nFinal Four, a prestigious basketball tournament for college teams.\nSPHINX\nThe logo in the image is of the brand \"Wilson,\" known for producing \nsports equipment. The logo consists of the brand name \"Wilson\" \nwritten in a stylized, bold font. The letters are capitalized and have a \nslight italicization, suggesting motion and dynamism. The color of \nthe logo appears to be a contrasting shade to the color of the \nbasketball, providing clear visibility against the textured background \nof the ball. The basketball itself is labeled with \"COMPOSITE \nLEATHER,\" indicating the material it is made from, and \"NCAA,\" \nwhich stands for the National Collegiate Athletic Association, \nsuggesting that this basketball meets the specifications for NCAA \ngames. Additionally, there is a mention of \"FINAL FOUR \nEDITION,\" which could imply that this basketball is a special \nedition for the Final Four, a term used to describe the last four teams \nremaining in the NCAA basketball tournament.\nGPT-4V\nSec. 2.3 Knowledge-based Perception\nFigure 27: Results on logo recognition and description. Green highlights the right answer. Refer to\nSection 2.3 for detailed discussions.\n36\nDescribe the logo in the image in details.\nLogo Recognition and Description\nPrompt\nDescribe the logo in the image in details.\nPrompt\nDescribe the logo in the image in details.\nPrompt\nGemini Pro\nThe logo is a black cat sleeping on the \nhood of a white car. The cat is facing the \nleft of the image and its eyes are closed. \nThe car is a Toyota Corolla and the logo \nis in the center of the hood.\nThe logo in the image is for Toyota.\nSPHINX\nThe logo in the image is of Toyota, a \nwell-known automotive manufacturer. It \nconsists of three overlapping ellipses \nwith a stylized \"T\" in the center. The \ncentral interlocking ovals represent the \nunification of the hearts of Toyota's \ncustomers and the company's products, \nwhile the larger oval surrounding these \nrepresents the global expansion of \nToyota's \ntechnology \nand \nunlimited \npotential for the future. The background \nis typically white, and the ovals are in a \nmetallic silver or chrome finish. This \nlogo is prominently displayed on the \nfront grille of Toyota vehicles.\nGPT-4V\nGemini Pro\nThe logo is a circle with a stylized \"C\" \ninside it. The \"C\" is made up of two \ncurved lines that intersect in the middle. \nThe logo is simple and elegant, and it \nconveys a sense of creativity and \ninnovation. The colors of the logo are \nblack and white, which are often \nassociated with sophistication and \nelegance.\nThe logo in the image appears to be a \nstylized knot. It consists of intertwined \nlines forming a continuous loop with no \nvisible beginning or end. The design is \nsimple, elegant, and appears to be \nmonochromatic, based on the grayscale \nimage. The knot is symmetrical and may \nrepresent connectivity, complexity, or \ninfinity due to its looping nature. The \nbackground is plain, ensuring that the \nlogo stands out prominently. The overall \ndesign conveys a sense of sophistication \nand modernity.\nGPT-4V\nThe logo in the image is a black and white \ncircle with a black line in the middle.\nSPHINX\nGemini Pro\nThe logo is a wordmark of the band's \nname, \"Linkin Park\". The letters are in a \nbold, sans-serif typeface and are \narranged in a horizontal line. The colors \nare black and white.\nThe image shows a white baseball cap \nwith a logo that consists of two words. \nThe first word appears to be \"LINKIN\" \nand the second one is \"PARK\". The text is \nin capital letters, with a stencil or \nmilitary-style font that features breaks or \ngaps within the strokes of the letters, \nsuggesting a rugged or industrial aesthetic. \nThe logo is likely associated with the \nmusic band of the same name, known for \ntheir nu metal and alternative rock style. \nThe color of the text seems to be a shade \nof grey, which provides a soft contrast \nagainst the white background of the cap.\nGPT-4V\nThe logo in the image is for the band \nLinkin Park. It is a white baseball cap \nwith a black logo on the front. The cap is \nplaced on a black and white surface, \nwhich could be a chair or a table.\nSPHINX\nSec. 2.3 Knowledge-based Perception\nFigure 28: Results on logo recognition and description. Green highlights the right answer. Red\nhighlights the wrong answer. Yellow highlights the incompetence in performing the task. Refer to\nSection 2.3 for detailed discussions.\n37\nMovie Recognition and Description\nJason Statham has starred in several movies, \nincluding \"The Transporter,\" \"Crank,\" \"The \nExpendables,\" \"Transporter 2,\" and \"The Mechanic.\"\nSPHINX\nPlease name three movies that this actor has starred in.\nPrompt\nThe actor in the image is known for a wide range of \nfilms, particularly in the action genre. Three movies \nthat he has starred in are:\n1. \"The Transporter\" - This film is where he plays a \nskilled driver and mercenary who will deliver \nanything, anywhere \u2013 no questions asked, for the \nright price.\n2. \"Crank\" - In this high-octane action film, he \nplays a hitman who must keep his adrenaline levels \nelevated to keep himself alive after being poisoned.\n3. \"Snatch\" - A British crime comedy film where he \ntakes on the role of an unlicensed boxing promoter \nwho finds himself in the midst of various \nintertwined plots involving a stolen diamond.\nGPT-4V\nGemini Pro\nJason Statham has starred in the following movies:\n1. The Expendables\n2. The Transporter\n3. The Italian Job\nI'm sorry, but I can't help identify or make \nassumptions about people in images. If you have \nany other questions about cinema or actors, feel free \nto ask!\nGPT-4V\nI'm sorry, but I cannot provide a list of movies that \nthis actor has starred in as there is no information \nabout the actor in the image.\nSPHINX\nPlease name three movies that this actor has starred in.\nPrompt\nGemini Pro\nThis is the actor Wu Jing. He has starred in movies \nsuch as Wolf Warrior 2, The Wandering Earth, and \nThe Battle at Lake Changjin.\nSec. 2.3 Knowledge-based Perception\nFigure 29: Results on movie recognition and description. Green highlights the right answer. Red\nhighlights the wrong answer. Refer to Section 2.3 for detailed discussions.\n38\nDescribe this poster and tell me something based on it.\nGemini Pro\nThe poster for Dune: Part Two features a large sand \ndune with a group of people walking across it. The \ntitle of the movie is written in the top left corner, \nand the release date is written in the bottom right \ncorner. The poster is very similar to the poster for \nthe first movie, which was released in 2021. The \nfirst movie was a critical and commercial success, \nso it is likely that the second movie will be as well.\nMovie Recognition and Description\nPrompt\nThe image appears to be a movie poster for \"Dune: \nPart Two.\" It features a large, crescent-shaped dune \nwith a smooth, shadowed surface on one side and a \nbright, glowing edge on the other, suggesting \nsunlight reflecting off the sand. The contrast creates \na striking visual effect, emphasizing the scale and \nthe harsh environment. On the bright side of the \ndune, tiny figures are depicted, likely representing \ncharacters from the movie, suggesting a journey or \nmission they are undertaking in a vast desert.\nThe \"Dune\" series is based on the science fiction \nnovels by Frank Herbert. The first book, \"Dune,\" is \nparticularly renowned for its deep political, \nreligious, and ecological themes, set against the \nbackdrop of an interstellar society with complex \nhierarchies. The story primarily revolves around the \ndesert planet of Arrakis, also known as Dune, which \nis the only source of the universe's most valuable \nsubstance, the spice melange. This substance is \ncrucial for space travel, extending life, and \nenhancing mental abilities.\n\"Dune: Part Two\" is presumably a continuation of \nthe story, following the events of the first film \nadaptation of Herbert's series, directed by Denis \nVilleneuve. The film is expected to explore the rise \nof the protagonist, Paul Atreides, and his \nconfrontation with the forces that seek to control \nArrakis and its precious resources. The themes of \npower, ecology, human potential, and destiny are \nlikely to be further developed in this sequel.\nGPT-4V\nThe poster features a group of people walking on a \nsand dune, with the sun casting a shadow on the \ndune. The text on the poster reads \"Dune Part Two,\" \nindicating that it is a continuation of a story or \nseries.\nSPHINX\nSec. 2.3 Knowledge-based Perception\nFigure 30: Results on movie recognition and description.\nGreen highlights the right answer.\nYellow highlights the incompetence in performing the task. Refer to Section 2.3 for detailed\ndiscussions.\n39\nWhich movie's classic shots are these pictures referring to?\nMovie Recognition and Description\nPrompt\nGemini Pro\nThe first picture is a reference to the movie \n\"The Matrix\". The second picture is a \nreference to the movie \"Inception\". The \nthird picture is a reference to the movie \n\"Pulp Fiction\". The fourth picture is a \nreference to the movie \"The Dark Knight\".\nThe pictures are referring to the iconic \ndance moves from the movie \"The Matrix.\" \nThe blue-colored man is seen dancing in a \ndark room, and the blue-colored woman is \nseen dancing in a room with a blue light.\nSPHINX\nThe images you've uploaded are \nreimagined scenes from the 1999 science \nfiction action film \"The Matrix,\" directed \nby the Wachowskis. They show the \nprotagonist, Neo, played by Keanu Reeves, \nin his iconic bullet-dodging pose. This \nscene is well-known for its pioneering use \nof visual effects, especially the \"bullet \ntime\" effect.\nGPT-4V\nSec. 2.3 Knowledge-based Perception\nGemini Pro\nThe director of this movie is Kazuya \nNomura. The main actors in this movie \nare:\n* Hiroki Yasumoto as Akira\n* Mamoru Miyano as Ryo\n* Kenjiro Tsuda as Jack\n* Yuko Kaida as Elena\n* Yui Ishikawa as Saya\n* Rikiya Koyama as Dan\n* Takahiro Sakurai as Makoto\n* Junichi Suwabe as Isao\n* Ami Koshimizu as Rio\n* Maaya Sakamoto as Yuki\nThe image you've uploaded appears to \nbe promotional artwork for a movie or \nmedia project, featuring a range of \ncharacters that suggest a science fiction \nor cyberpunk genre. However, without \nany specific details, titles, or \nrecognizable characters, I cannot \nidentify the director or the main \ncharacters of the movie from the image \nalone. If you have more information or \nanother way to identify the movie, \nplease provide that, and I can assist you \nfurther.\nGPT-4V\nThe director of this movie is Kazuaki Nakamura, and the main actors are Kiko Mizuhara, \nYoshiki Inoue, and Takashi Matsuyama.\nSPHINX\nWho is the director of this movie? Who \nare the main actors in this movie?\nPrompt\nFigure 31: Results on movie recognition and description. The image at the bottom is a fantasy\npiece created using a diffusion-based model and does not refer to any real-life movie. Gemini and\nSphinx are misled into providing incorrect answers, whereas GPT-4V indicates its lack of knowledge\nregarding the relevant information. Green highlights the right answer. Red highlights the wrong\nanswer. Yellow highlights the incompetence in performing the task. Refer to Section 2.3 for detailed\ndiscussions.\n40\nWhat are this dish made of?\nGemini Pro\nThe dish is made of rice noodles, pork, and \nlettuce. The noodles are thin and white, and they \nare served in a clear broth. The pork is thinly \nsliced and cooked until it is tender. The lettuce \nis added to the dish at the end, and it provides a \nrefreshing contrast to the other ingredients.\nFood Recognition and Description\nPrompt\nSec. 2.3 Knowledge-based Perception\nWhat are these dishes?\nPrompt\nThe dish in the image appears to be a bowl of noodle soup. It \ncontains the following ingredients:\n- Noodles: These look like they could be a type of wheat-\nbased noodle, commonly used in East Asian cuisines.\n- Meat: There are ground meat pieces that resemble pork, a \ncommon ingredient in many noodle soups.\nGPT-4V\nGemini Pro\nCongee, egg, and dan tat.\nThe image shows a tray with two dishes commonly found in \nEast Asian cuisine, particularly Chinese. On one side, there is \na bowl of what appears to be rice porridge or congee, a \ntraditional breakfast dish made by cooking rice in a large \namount of water until it breaks down into a porridge-like \nconsistency. Accompanying the porridge is a pair of \nchopsticks. On the other side of the tray, there is a whole \ncooked egg and a half of a preserved egg, often referred to as \na \"century egg\" or \"thousand-year-old egg.\" This is an egg \nthat has been preserved in a mixture of clay, ash, quicklime, \nsalt, and rice hulls for several weeks to several months. The \nyolk becomes a dark green to grey color, with a creamy \nconsistency and strong flavor, while the white becomes a \ntranslucent brown jelly with little flavor. These eggs are often \neaten with congee.\nGPT-4V\nWhat are these dishes?\nGemini Pro\nThe dishes are a pistachio cream puff and a \nraspberry tart.\nPrompt\nThe image shows two pastry dishes. On the left, there appears to be a \nvariation of a French pastry known as a \"choux \u00e0 la cr\u00e8me,\" which is a \npuff pastry filled with cream. This particular one has a green-colored \ncream, which might suggest a flavor like pistachio or matcha, and it is \ngarnished with what look like slices of pistachio on top.On the right, \nthere is a tart topped with fresh raspberries. It's likely a raspberry tart, \nwhich typically consists of a crisp pastry crust filled with cr\u00e8me \np\u00e2tissi\u00e8re (pastry cream) or another type of cream, and topped with \nfresh raspberries.Both pastries are labeled with a sign that reads \n\"PAIN CHAUD,\" which could be the name of the bakery or patisserie \nwhere these pastries are sold. \"Pain chaud\" literally means \"hot bread\" \nin French, indicating freshly baked goods.\nGPT-4V\nThese dishes are a cake and a dessert, both featuring \nraspberries as a prominent ingredient.\nSPHINX\nThis dish is made of noodles, meat, and \nvegetables, including lettuce and green onions.\nSPHINX\nThese dishes are a bowl of rice, an egg, and a \nplate with a piece of meat on a green tray.\nSPHINX\nFigure 32: Results on food recognition and description. Green highlights the right answer. Red\nhighlights the wrong answer. Refer to Section 2.3 for detailed discussions.\n41\nDescribe these dishes.\nGemini Pro\nTwo plates with burgers and fries. The burgers \nare on toasted buns with lettuce, tomato, onion, \nand cheese. The fries are in a metal cup.\nFood Recognition and Description\nPrompt\nSec. 2.3 Knowledge-based Perception\nWhat are these dishes?\nPrompt\nThe image shows two dishes. In the foreground, there is a \nburger that appears to have a glossy brioche bun, a well-\ngrilled beef patty, lettuce, tomato, and what could be either \nmayonnaise or a light aioli. There's a wooden skewer holding \nthe burger together. Behind it is a second burger that seems to \nhave a similar brioche bun, with a filling that looks like \ngrilled beef, caramelized onions, and perhaps a slice of cheese \nor a sauce. To the right, there are two small metal buckets \nfilled with French fries, which look crispy and lightly salted. \nThe dishes are presented on white plates, and the setting \nsuggests a casual dining environment, likely a restaurant \nspecializing in American or fast-casual cuisine.\nGPT-4V\nGemini Pro\nFish and chips is a classic British dish made of \nfried fish and chips. The fish is usually cod or \nhaddock, and the chips are made from potatoes. \nIt is often served with mushy peas or tartar \nsauce.\nThe image shows a classic dish of fish and chips, \na popular British food. It typically consists of a \nbattered and fried fish fillet, often cod or \nhaddock, served with a side of fries, known as \n\"chips\" in the UK. Accompaniments may \ninclude a slice of lemon and tartar sauce, as seen \nin the image, which are used to add flavor to the \nfish.\nGPT-4V\nThe dishes on the table include fish and chips, a \nbowl of fries, and a lemon wedge.\nSPHINX\nThe image features a dining table with a variety of dishes and \nfood items. There are two hamburgers, one placed on the left \nside of the table and the other on the right side. Alongside the \nhamburgers, there are two cups of french fries, one located \nnear the top right corner of the table and the other closer to \nthe bottom right corner. The table is set with a knife and a \nfork, one placed near the top left corner and the other near the \nbottom left corner. The arrangement of the dishes and food \nitems suggests a casual dining experience, with a focus on \nburgers and fries.\nSPHINX\nFigure 33: Results on food recognition and description. Green highlights the right answer. Refer to\nSection 2.3 for detailed discussions.\n42\nWhat breed is the plant in the picture?\nGemini Pro\nThe plant in the picture is a spider mum. Spider \nmums are a type of chrysanthemum that is \ncharacterized by its long, thin petals that \nresemble spider legs. They come in a variety of \ncolors, including white, pink, red, and yellow. \nSpider mums are popular flowers for bouquets \nand arrangements, and they are also used in \ndried flower arrangements.\nPlant Recognition\nPrompt\nWhat breed is the plant in the picture?\nPrompt\nThe plant in the picture is a chrysanthemum, \noften referred to as a mum. The variety shown \nhere appears to be one of the spider mums, \ncharacterized by their long, tubular petals that \ncan be curved or hooked at the ends, resembling \nspider legs. This specific breed is hard to \ndetermine without more context, as there are \nmany cultivars that look quite similar, but it's \ndefinitely within the chrysanthemum family, \nwhich is known for its vast diversity in flower \nshapes, sizes, and colors.\nGPT-4V\nGemini Pro\nThe plant in the picture is a Delonix regia, also \nknown as the flame tree or royal poinciana. It is \na flowering tree native to Madagascar and is \nwidely grown as an ornamental tree in tropical \nand subtropical regions around the world. The \nflame tree is known for its vibrant red flowers, \nwhich are typically produced in large clusters. \nThe leaves of the flame tree are bipinnate, \nmeaning they are twice-divided into leaflets. \nThe flame tree can grow to a height of 30-40 \nfeet and is a popular choice for landscaping due \nto its beautiful flowers and attractive foliage.\nThe plant shown in the picture is a flowering \ntree commonly known as the Royal Poinciana, \nFlamboyant, Flame Tree, or by its scientific \nname, Delonix regia. It's known for its bright \nred or orange flowers and fern-like leaves. It is \nnative to Madagascar and is widely planted in \ntropical regions around the world for its striking \nappearance.\nGPT-4V\nThe plant in the picture is a white-rayed mule's \nears.\nSPHINX\nThe plant in the picture is a Royal Poinciana, \nalso known as Flamboyant.\nSPHINX\nSec. 2.3 Knowledge-based Perception\nFigure 34: Results on plant recognition and description. Green highlights the right answer. Red\nhighlights the wrong answer. Refer to Section 2.3 for detailed discussions.\n43\nWhat breed is the plant in the picture?\nGemini Pro\nIt is not a real plant. It is a Lego flower bouquet.\nAnimal and Plant Recognition\nPrompt\nWhat breed is the plant in the picture?\nPrompt\nThe item in the picture that appears to be a plant \nis not actually a living organism. It seems to be \na creative decorative piece, possibly made from \nLego or similar building blocks, designed to \nlook like a colorful bouquet of flowers. Since \nit's not a real plant, there is no breed associated \nwith it. These kinds of objects are often used for \noffice or home decoration, and they can be a \nplayful and low-maintenance alternative to real \nplants.\nGPT-4V\nGemini Pro\nThe plant in the picture is a tulip. It is a \nperennial, bulbous plant with showy flowers in \nthe genus Tulipa, of which about 75 wild \nspecies are currently accepted and which \nbelongs to the family Liliaceae. The name 'tulip' \nis thought to be derived from a Persian word for \nturban, which it may have been thought to \nresemble.\nThe \"plant\" in the image you've provided is not \na living plant, but rather a plush or fabric-based \nrepresentation designed to resemble a cactus. It's \nmade of green textile material, and the blue \n\"flowers\" appear to be made of a different fabric, \npossibly velvet or another soft material. This is a \ndecorative item, not a real plant species.\nGPT-4V\nWhat animal is the pen pointing to in the image?\nGemini Pro\nThe pen is pointing to a rabbit.\nPrompt\nThe pen in the image is pointing to a chicken.\nGPT-4V\nThe plant in the picture is a sunflower. The plant \nin the picture is a sunflower.\nSPHINX\nThe plant in the picture is a cactus.\nSPHINX\nThe pen is pointing to a dog in the image.\nSPHINX\nSec. 2.3 Knowledge-based Perception\nFigure 35: Results on animal and plant recognition and description. Green highlights the right\nanswer. Red highlights the wrong answer. Yellow highlights the incompetence in performing the\ntask. Refer to Section 2.3 for detailed discussions.\n44\nDescribe the artwork in the picture.\nGemini Pro\nThe artwork in the picture is a sculpture of David by \nMichelangelo. The sculpture is made of marble and is located \nin the Galleria dell'Accademia in Florence, Italy. It was \ncreated between 1501 and 1504. The sculpture depicts David, \na biblical figure, at the moment just before he fights Goliath. \nDavid is shown as a young man with a muscular build and a \ndetermined expression on his face. He is nude except for a \nloincloth, and his hair is short and curly. The sculpture is \nconsidered to be one of the masterpieces of Renaissance art \nand is one of the most famous sculptures in the world.\nArtwork Recognition and Description\nPrompt\nSec. 2.3 Knowledge-based Perception\nDescribe the artwork in the picture.\nPrompt\nThe artwork in the picture is a classical sculpture, most likely \nfrom the Renaissance period, judging by the style and \ntechnique. The figure is a male, depicted in a pensive pose, \nwith one hand raised to his chin, suggesting a moment of \ndeep thought or contemplation. The sculpture is highly \ndetailed, with intricate curls in the hair and a lifelike \nrepresentation of facial features. The level of detail in the hair \nand the musculature suggests a high level of skill on the part \nof the sculptor. The material appears to be marble, indicated \nby the smooth texture and the way light is diffused on the \nsurface.It's a portrait bust, meaning it likely includes just the \nhead and the upper part of the torso. This type of sculpture \nwas commonly used to depict figures from mythology, \nhistorical individuals, or, more rarely, a conceptual figure \nrepresenting an allegory or an idea. Without more context, it's \nnot possible to determine the exact identity or the intended \nsymbolism behind this particular sculpture.\nGPT-4V\nGemini Pro\nThe artwork in the picture is a human figure made of clay. \nThe figure is standing with its arms outstretched to the sides. \nThe head is featureless, and the body is slightly hunched over. \nThe figure is likely a representation of a human ancestor or \nspirit. It was found in a tomb in China and is believed to be \nover 6,000 years old.\nThe image features an ancient terracotta sculpture displayed \nin a museum setting. The sculpture appears to be \nanthropomorphic, with a simplified human form featuring a \ndistinct head, arms, and torso. The head has stylized facial \nfeatures including eyes, a nose, and a mouth, giving it a \nsomewhat abstract or expressionistic look. The arms are \noutstretched to either side, suggesting a pose that might \nindicate an offering or display.The sculpture seems to be aged, \nwith a rough, uneven texture that speaks to the material it is \nmade from and the wear it has endured over time. There's a \nvisible coloration that might be due to the original pigments \nor the result of aging. It is presented on a simple, modern, \ndark grey or black pedestal, which contrasts with the lighter \ncolor of the terracotta and highlights the artifact.In the \nbackground, there are diagrams and photographs that provide \nadditional context or information about the sculpture. \nHowever, the details of these background elements are not \nclear enough to discern specific information. The overall \nlighting and presentation suggest that this is an object of \nsignificance, possibly from an archaeological or historical \nexhibition.\nGPT-4V\nThe artwork in the picture is a close-up of a statue, \nshowcasing the intricate details of the sculpture. The \nstatue appears to be a bust of a man, with a prominent \nnose and a thoughtful expression. The lighting in the \nimage highlights the texture and depth of the statue, \nallowing viewers to appreciate the skill and \ncraftsmanship that went into creating this piece of art.\nSPHINX\nThe artwork in the picture is a sculpture of a \nperson with outstretched arms, displayed on a \npedestal.\nSPHINX\nFigure 36: Results on artwork recognition and description. Green highlights the right answer. Red\nhighlights the wrong answer. Yellow highlights the incompetence in performing the task. Refer to\nSection 2.3 for detailed discussions.\n45\nDescribe\u00a0the\u00a0artwork\u00a0in\u00a0the\u00a0picture.\nGemini Pro\nThe\u00a0artwork\u00a0in\u00a0the\u00a0picture\u00a0is\u00a0a\u00a0bronze\u00a0vessel\u00a0called\u00a0a\u00a0\nding.\u00a0It\u00a0has\u00a0four\u00a0legs\u00a0and\u00a0a\u00a0rectangular\u00a0body\u00a0with\u00a0a\u00a0lid.\u00a0\nThe\u00a0body\u00a0is\u00a0decorated\u00a0with\u00a0taotie\u00a0masks\u00a0and\u00a0geometric\u00a0\npatterns.\u00a0The\u00a0ding\u00a0is\u00a0a\u00a0type\u00a0of\u00a0ritual\u00a0vessel\u00a0that\u00a0was\u00a0used\u00a0\nin\u00a0ancient\u00a0China\u00a0for\u00a0cooking\u00a0and\u00a0serving\u00a0food.\u00a0It\u00a0is\u00a0a\u00a0\nvaluable\u00a0example\u00a0of\u00a0Chinese\u00a0art\u00a0and\u00a0history.\nArtwork Recognition and Description\nPrompt\nSec. 2.3 Knowledge-based Perception\nDescribe\u00a0the\u00a0artwork\u00a0in\u00a0the\u00a0picture.\nPrompt\nThe\u00a0object\u00a0in\u00a0the\u00a0image\u00a0appears\u00a0to\u00a0be\u00a0an\u00a0ancient\u00a0Chinese\u00a0\nbronze\u00a0vessel,\u00a0known\u00a0as\u00a0a\u00a0\"Ding.\"\u00a0It\u00a0is\u00a0a\u00a0type\u00a0of\u00a0ritual\u00a0\nbronzeware\u00a0commonly\u00a0found\u00a0in\u00a0Chinese\u00a0tombs\u00a0and\u00a0used\u00a0\nduring\u00a0the\u00a0Shang\u00a0and\u00a0Zhou\u00a0dynasties.\u00a0This\u00a0vessel\u00a0stands\u00a0on\u00a0\nthree\u00a0or\u00a0four\u00a0legs\u00a0and\u00a0has\u00a0two\u00a0upward-facing\u00a0handles.\u00a0It\u00a0is\u00a0\nheavily\u00a0ornamented\u00a0with\u00a0traditional\u00a0Chinese\u00a0motifs,\u00a0\nincluding\u00a0Taotie,\u00a0which\u00a0are\u00a0stylized\u00a0animal\u00a0faces\u00a0that\u00a0are\u00a0\ncommon\u00a0in\u00a0Chinese\u00a0metalwork\u00a0from\u00a0this\u00a0period.\u00a0The\u00a0patina\u00a0\non\u00a0the\u00a0vessel\u00a0suggests\u00a0considerable\u00a0age,\u00a0and\u00a0the\u00a0intricate\u00a0\ndesigns\u00a0indicate\u00a0it\u00a0was\u00a0likely\u00a0used\u00a0by\u00a0someone\u00a0of\u00a0high\u00a0status,\u00a0\npossibly\u00a0in\u00a0ritual\u00a0ceremonies\u00a0or\u00a0as\u00a0a\u00a0burial\u00a0item\u00a0to\u00a0accompany\u00a0\nthe\u00a0deceased\u00a0in\u00a0the\u00a0afterlife.\u00a0The\u00a0verdigris\u00a0on\u00a0the\u00a0surface\u00a0\nshows\u00a0it\u00a0has\u00a0oxidized\u00a0over\u00a0time,\u00a0which\u00a0is\u00a0typical\u00a0for\u00a0ancient\u00a0\nbronze\u00a0items.\nGPT-4V\nGemini Pro\nThe\u00a0artwork\u00a0in\u00a0the\u00a0picture\u00a0is\u00a0a\u00a0gold\u00a0mask\u00a0with\u00a0a\u00a0human\u00a0\nface.\u00a0The\u00a0mask\u00a0is\u00a0very\u00a0realistic,\u00a0with\u00a0detailed\u00a0features\u00a0\nand\u00a0even\u00a0eyelashes.\u00a0The\u00a0mask\u00a0is\u00a0also\u00a0very\u00a0thin,\u00a0and\u00a0it\u00a0is\u00a0\nunclear\u00a0how\u00a0it\u00a0was\u00a0made.\u00a0The\u00a0mask\u00a0is\u00a0likely\u00a0to\u00a0be\u00a0a\u00a0\nceremonial\u00a0object,\u00a0and\u00a0it\u00a0may\u00a0have\u00a0been\u00a0used\u00a0in\u00a0\nreligious\u00a0rituals.\u00a0The\u00a0mask\u00a0is\u00a0from\u00a0the\u00a0Sanxingdui\u00a0\nculture,\u00a0which\u00a0flourished\u00a0in\u00a0the\u00a0Sichuan\u00a0Basin\u00a0in\u00a0China\u00a0\nfrom\u00a0around\u00a01200\u00a0to\u00a0600\u00a0BC.\nThe\u00a0artwork\u00a0in\u00a0the\u00a0picture\u00a0appears\u00a0to\u00a0be\u00a0a\u00a0golden\u00a0mask.\u00a0It\u00a0has\u00a0\na\u00a0very\u00a0stylized\u00a0and\u00a0abstract\u00a0design,\u00a0with\u00a0prominent\u00a0facial\u00a0\nfeatures\u00a0that\u00a0are\u00a0exaggerated\u00a0in\u00a0size.\u00a0The\u00a0eyes\u00a0are\u00a0large\u00a0and\u00a0\noval,\u00a0and\u00a0the\u00a0mask\u00a0includes\u00a0representations\u00a0of\u00a0ears\u00a0that\u00a0\nextend\u00a0out\u00a0widely\u00a0from\u00a0the\u00a0sides.\u00a0The\u00a0nose\u00a0is\u00a0prominent\u00a0and\u00a0\nprotrudes\u00a0outward,\u00a0while\u00a0the\u00a0mouth\u00a0area\u00a0is\u00a0less\u00a0defined,\u00a0\ngiving\u00a0the\u00a0mask\u00a0a\u00a0somewhat\u00a0enigmatic\u00a0expression.\u00a0The\u00a0\nsurface\u00a0of\u00a0the\u00a0mask\u00a0is\u00a0textured,\u00a0suggesting\u00a0that\u00a0it\u00a0might\u00a0have\u00a0\nbeen\u00a0crafted\u00a0by\u00a0hand,\u00a0which\u00a0adds\u00a0a\u00a0tactile\u00a0quality\u00a0to\u00a0the\u00a0piece.\u00a0\nThe\u00a0mask\u00a0has\u00a0a\u00a0sheen\u00a0that\u00a0indicates\u00a0it\u00a0might\u00a0be\u00a0made\u00a0of\u00a0gold\u00a0\nleaf\u00a0or\u00a0a\u00a0gold-colored\u00a0metal,\u00a0contributing\u00a0to\u00a0its\u00a0striking\u00a0visual\u00a0\nimpact.\u00a0It's\u00a0displayed\u00a0against\u00a0a\u00a0dark\u00a0background,\u00a0which\u00a0\nhighlights\u00a0its\u00a0luminosity\u00a0and\u00a0golden\u00a0hue.\nGPT-4V\nThe\u00a0artwork\u00a0in\u00a0the\u00a0picture\u00a0is\u00a0a\u00a0bronze\u00a0ware\u00a0with\u00a0\nintricate\u00a0designs\u00a0and\u00a0patterns.\u00a0It\u00a0appears\u00a0to\u00a0be\u00a0an\u00a0old\u00a0and\u00a0\nvaluable\u00a0piece\u00a0of\u00a0art,\u00a0possibly\u00a0a\u00a0bronze\u00a0ware\u00a0from\u00a0the\u00a0\nShang\u00a0Dynasty.\u00a0The\u00a0bronze\u00a0ware\u00a0has\u00a0a\u00a0greenish\u00a0color\u00a0\nand\u00a0is\u00a0displayed\u00a0on\u00a0a\u00a0white\u00a0background.\nSPHINX\nThe\u00a0artwork\u00a0in\u00a0the\u00a0picture\u00a0is\u00a0a\u00a0gold\u00a0mask\u00a0with\u00a0a\u00a0\nface\u00a0design.\nSPHINX\nFigure 37: Results on artwork recognition and description.\nGreen highlights the right answer.\nYellow highlights the incompetence in performing the task. Refer to Section 2.3 for detailed\ndiscussions.\n46\nDescribe the artwork in the picture.\nGemini Pro\nA Riverside Scene at Qingming Festival, also known as Along the River During the Qingming Festival, is a Chinese \npainting by the late Song-early Yuan dynasty artist Zhang Zeduan, created in the early 12th century. The handscroll \npainting, one of the most famous works of Chinese art, depicts daily life along the Bian River in the capital city of \nBianjing (modern-day Kaifeng, Henan) during the Qingming Festival.\nArtwork Recognition and Description\nPrompt\nSec. 2.3 Knowledge-based Perception\nThe image you've provided depicts a detailed, panoramic traditional East Asian scene, likely a Chinese landscape, executed in the \nstyle of a classical handscroll painting. It showcases a bustling river scene with a multitude of activities occurring both on the water \nand along the riverbanks. There is a large, prominent arched bridge crossing the river, which is the central focal point of the artwork, \nteeming with figures crossing over it.On the bridge and throughout the image, there are numerous people engaged in various \nactivities, suggesting a lively community scene. The figures are rendered in fine detail, indicating their individual clothing and \nactions, which may represent different societal roles and daily tasks. The architecture of the buildings, including roof styles and the \nlayout of the village, is indicative of historical East Asian designs.The color palette is muted, with earthy tones and subtle coloration, \nwhich is characteristic of many traditional East Asian paintings. The artwork likely represents a narrative or captures the essence of \ndaily life during a particular historical period. The attention to detail in the depiction of the figures and the environment suggests \nthat the artwork may have been created by a skilled artist with a deep understanding of the techniques and themes of traditional East \nAsian art.\nGPT-4V\nThe artwork in the picture is a painting of a bridge and a river, with a large crowd of people walking across the bridge. \nThe painting is done in a style reminiscent of Chinese art, featuring a mix of brown and yellow colors. The people in the \npainting are of various sizes and positions, creating a lively and bustling atmosphere. The bridge itself is prominently \ndisplayed, spanning the width of the painting, while the river flows beneath it, adding to the overall scenic beauty of the \nimage.\nSPHINX\nFigure 38: Results on artwork recognition and description.\nGreen highlights the right answer.\nYellow highlights the incompetence in performing the task. Refer to Section 2.3 for detailed\ndiscussions.\n47\n3\nAdvanced Cognition\nOn top of the fundamental perception, we further evaluate the more advanced cognition capabilities\nof Gemini, GPT-4V, and Sphinx. Cognition tasks for MLLMs require not only understanding multi-\nmodal concepts in visual contexts, but also conducting in-depth reasoning, problem-solving, and\ndecision-making.\nIn Section 3.1, we first focus on the model\u2019s ability to reason text-rich visual contents, including table\nand chart reasoning, along with visual code generation. In Section 3.3, we delve into MLLMs\u2019 logical\nand quantitative comprehension to solve challenging science problems with pre-trained knowledge,\ne.g., mathematics and physics. In Section 3.2, our exploration targets on how the models reason\nabstract visual information from the tasks of abstract visual stimuli, Raven\u2019s Progressive Matrices,\nand Wechsler Adult Intelligence Scale. In Section 3.4, we investigate the models\u2019 understanding of\nemotions, through various scenarios such as facial expression analysis, image emotion analysis, and\nemotion-conditioned output. Finally in Section 3.5, we evaluate the decision-making performance of\nMLLMs in various intelligence games, including Sudoku and Go.\n3.1\nText-Rich Visual Reasoning\nTable and chart reasoning. In Figures 39-40, we present two samples of flowchart understanding by\nthe three models. As shown, Gemini can correctly summarize the high-level idea of flowcharts with\nbrief sentences. GPT-4V tends to produce more detailed descriptions of the logical flow in the charts,\nbut would occasionally make some mistakes. In contrast, Sphinx fails to extract the meaning of them,\nwhich is due to the lack of related pre-training data. In Figures 41-43, we evaluate the question-\nanswering performance on six different plots and tables. Similar to previous demonstrations, GPT-4V\ncan respond with more reasoning details than Gemini. However, all three models have difficulties in\nproviding a precise answer, which is mainly constrained by the unsatisfactory OCR accuracy. Also, as\nshown by the last sample, both Gemini and GPT-4V can understand the hand-drawing visual prompt,\ndespite that Gemini provides the wrong final answers, indicating their generalization capacity for\nvisual input.\nVisual code generation. It\u2019s an important skill for MLLMs to convert structured visual content into\nthe corresponding codes. In Figures 44-45, we prompt the three models to generate LaTeX code of\nvarious mathematical formulas and render them for comparison. Overall, Gemini and GPT-4V exhibit\nbetter results than Sphinx, but still misrecognize some minor characters or symbols. Notably, for a\nrather complicated formula in printing form, both Gemini and GPT-4V generate correct codes. In\nFigures 46-47, we test the HTML code generation results for different types of websites. As shown,\nthe HTML understanding capacity still exists a large improvement space for all three MLLMs. Only\nGemini is capable of constructing the rough structure of simple websites, while GPT-4V simply\nidentifies the text content. This might be also caused by the limited pre-training data.\n3.2\nAbstract Visual Reasoning\nAbstract visual stimuli. This task evaluates the visual abstract capabilities for object composition.\nAs shown in Figures 48-49, GPT-4V exhibits the best abstract performance and also provides detailed\ndescriptions for how the objects are composed of shapes. Instead, Gemini has partial abilities to\nrecognize some simple abstract patterns, such as \u2018boat\u2019 and \u2018house\u2019, and Sphinx can not understand\nthem.\nRaven\u2019s Progressive Matrices and Wechsler Adult Intelligence Scale. These two tasks are more\nchallenging, since they require recognizing the high-level relations of different components, and\npredicting the next element in the matrices or sequences. As respectively shown in Figures 50-51\nand 52-53, nearly all of the MLLMs are incorrect in the final answer. GPT-4V showcases some\ndetailed reasoning process, but still struggles with the final prediction and can be easily misled by\nan incorrect intermediate step. This experiment indicates that, although the advanced MLLMs can\ninitially identify the independent element, they fail to parse their relationship for further inference.\n48\n3.3\nScience Problem-Solving\nMathematical problems. Different from common visual question answering, the solving of math-\nematical problems involves both OCR capabilities from visual input and quantitative processing\naccuracy in the subsequent reasoning steps. In Figures 54-59, we show some mathematical problems\nconcerning a wide range of tasks, including arithmetic, algebra, geometry, and integral calculus. The\nsamples indicate that Gemini and GPT-4V can well tackle simple arithmetic and algebra problems.\nFor more difficult trigonometry and integral calculus, they also exhibit favorable reasoning perfor-\nmance with the help of external tools. However, they are not very expert at recognizing the specific\nvisual content in the images, such as numbers, symbols, and their correspondence. In addition, we\nobserve that, with CoT techniques, i.e., \u201cPlease think step by step\u201d, the previous wrong answer of\nSphinx can rectified, demonstrating the importance of CoT prompting for open-sourced MLLMs.\nPhysics problems. Such problems further require MLLMs\u2019 comprehension of the specialized\nvocabulary and concepts in Physics. In Figures 60-62, we show the problem-solving results of three\nMLLMs concerning dynamics, kinematics, and circuitry. As shown, Gemini and GPT-4V show well-\nperformed reasoning of Physics problems and well leverage the pre-trained specialized knowledge as\nreference. However, their performance can be limited by mathematical calculation, e.g., the range\nof integration, and the accuracy of physical equations, e.g., energy conservation equation. Due to\nthe training data scarcity of Physics problems, the open-source Sphinx clearly lacks proficiency in\nsolving such scientific problems with figures.\n3.4\nEmotion Understanding\nFacial expression analysis. In Figures 63-64, we evaluate the facial expression understanding\ncapacity of different models. As shown, all of the three MLLMs exhibit good performance in this task.\nTherein, GPT-4V provides more dialectical thinking with rigorous analysis, e.g., the two possibilities\nof the first expression, while Gemini can directly respond with the accurate answer in a concise\nmessage. Also, GPT-4V and Sphinx both capture the truncated textual content on the plate of the\nthird image, and incorporate this information into the reasoning. This result demonstrates their\ncomprehensive visual understanding abilities.\nImage emotion analysis. This task is more challenging, since there is no explicit facial expression\nshown in the image. Instead, MLLMs are required to indicate the implicit emotion conveyed from the\nvisual concepts. As shown in Figures 65-69, we select diverse samples of various natural scenes and\nmanufactured buildings. All three models can well depict the view first, and provide possible emotion\nwithin it. Therein, GPT-4V is observed to be neutral and emphasizes that emotions are subjective,\nand meanwhile gives a more comprehensive analysis. In contrast, Gemini tends to directly output\nthe emotion preference, which corresponds with mainstream perspectives. In addition, Sphinx can\nachieve comparable performance to the other two MLLMs, indicating its superior emotion parsing\ncapability.\nEmotion-conditioned output. Different from predicting the emotion in the image, this emotion-\nconditioned output enables MLLMs to describe the visual context conditioned on a pre-defined\nemotion, such as \u201cin a romantic or terrifying way\u201d. As shown in Figures 70-71, although Gemini and\nGPT-4V can correctly inject the corresponding emotion into the generated text, they both encountered\nhallucination issues, i.e., describing something that doesn\u2019t exist, such as the \u201cbike\u201d and \u201cshadows\u201d\nin the first image, and the \u201csound\u201d in the second image. This is might because of the too-powerful\ncorrelation abilities. In contrast, Sphinx is free from this issue, which exhibits the advantage of\nhuman-replicated MLLMs.\n3.5\nGame Playing\nSudoku and Crossword. These two games are logic-based and combinatorial puzzles, evaluating\nMLLMs\u2019 capabilities of OCR, structural understanding, and semantic reasoning. As shown in\nFigure 72 for Sudoku playing, if only taking an image as input, although Gemini tries to provide the\nanswer within the output matrix, it cannot correctly identify the blank position, while GPT-4V and\nSphinx fail to conduct the first-step OCR. Further, given the corresponding text input, both Gemini\nand GPT-4V can respond with the right answer. In Figures 73-74 for Crossword, the output of Sphinx\nis not very stable for the two examples, and Gemini performs slightly better than GPT-4V with more\ncorrect answers, which demonstrates its better structural comprehension and reasoning capacity.\n49\nGo playing. The game of Go involves intricate strategic thinking and rule-based decision-making,\nwhich has been regarded as a touchstone for general artificial intelligence. In Figures 75-76, we verify\nwhether these three MLLMs obtain the ability to play go. As shown, they can master some initial\nskills, and predict the next move to a legal position in the chessboard. Although different strategies\nhave different moving, only GPT-4V in the second sample obtains the optimal answer. However,\nSphinx mistakenly views the game to be a Reversi in the third sample. This illustrates that current\nMLLMs still have a certain distance to cover before mastering advanced chess-playing capabilities.\n50\nPlease explain the flowchart.\nGemini Pro\nThe flowchart is a simple algorithm that compares two numbers, a and b. It starts by reading the values \nof a and b. Then, it checks if b is greater than a. If it is, then the flowchart displays b. Otherwise, it \ndisplays a. Finally, the flowchart ends.\nTable and Chart Reasoning\nPrompt\nSec. 3.1 Text-Rich Visual Reasoning\nGPT-4V\nThe flowchart is a visual representation of a decision-making process. It consists of a series of \nconnected boxes, each representing a decision or a step in the process. The flowchart starts with the \n\"Start\" box, which is followed by the \"YES\" box. If the answer to the question is \"YES,\" the flowchart \nproceeds to the \"Is b=a\" box, where the value of b is compared to the value of a. If b equals a, the \nflowchart moves to the \"NO\" box, indicating that the answer is no. If the answer is no, the flowchart \nends at the \"End\" box.\nIf the answer is yes, the flowchart proceeds to the \"Display A\" box, indicating that the value of A is \ndisplayed. If the answer is no, the flowchart ends at the \"Display B\" box, indicating that the value of B \nis displayed.\nThe flowchart is designed to help decision-makers make choices based on the input values and the \ndesired outcome. It is a useful tool for visualizing and organizing complex decision-making processes.\nSPHINX\nThe flowchart represents a simple decision-making process with the following steps:\n1.Start: This indicates the beginning of the process.\n2.Read a, b: This step is about reading or inputting two values labeled  a and  b.\n3.Is b > a?: This is a decision point where the process asks if the value of  b is greater than a.\n    \u09f9If the answer is YES, the flowchart instructs to:\n      \u318dDisplay B: Show or output the value of  b.\n    \u09f9 If the answer is NO, the flowchart instructs to:\n      \u318d Display A: Show or output the value of  a.\n4. End: This indicates the end of the process after either  a or  b is displayed, depending on the result \nof the comparison.\nIn summary, this flowchart reads two values and displays the larger of the two. If b is greater than a, it \ndisplays b; otherwise, it displays a. The process terminates after displaying the value.\nNote\uff1aThe output of GPT-4V is the Markdown code and we show the rendered \nresult.\nFigure 39: Results on table and chart reasoning. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 3.1 for detailed discussions.\n51\nPlease explain the flowchart.\nGemini Pro\nThe flowchart describes the process of reaching a consensus on a document. The process starts with a \nprevious consensus, which is a document that has been agreed upon by all parties involved. If a user \nmakes an edit to the document, the changes are implemented and the document is then edited further. \nIf the document is not edited further, then a new consensus is reached and the process ends. However, \nif the document is edited further, then the process of seeking a compromise begins. This involves \ndiscussing the changes with the other parties involved and trying to reach an agreement on the changes. \nIf an agreement is reached, then a new consensus is reached and the process ends. However, if an \nagreement is not reached, then the process of seeking a compromise continues until an agreement is \nreached.\nTable and Chart Reasoning\nPrompt\nSec. 3.1 Text-Rich Visual Reasoning\nGPT-4V\nThe flowchart is a decision-making process that guides the editorial team in making decisions about \nan article.  The process starts with the question \"Was the article edited further?\" If the answer is yes, \nthe team proceeds to the next question, \"Do you agree with the changes?\" If the answer is no, the team \nmoves on to the next question, \"Disagree.\" If the answer is agree, the team proceeds to the next \nquestion, \"Agree.\" If the answer is no, the team moves on to the next question, \"New consensus.\" If \nthe answer is yes, the team proceeds to the next question, \"Implement.\" If the answer is no, the team \nmoves on to the next question, \"Seek the compromise.\" If the answer is yes, the team proceeds to the \nnext question, \"Make an edit.\" If the answer is no, the team moves on to the next question, \"Previous \nconsensus.\"\nSPHINX\nThe flowchart appears to be outlining a process for editing and achieving consensus on an article, likely in a \ncollaborative environment like a wiki or a shared document. Here's a step-by-step explanation:\n1. Previous Consensus: The process begins with a previously established consensus on the content of the article.\n2. Make an Edit: Someone makes an edit to the article.\n3. Was the article edited further?: A decision point where one checks if there have been any further edits to the article \nafter the initial one.\n   \u318d If  Yes, the process loops back to implementing the edit.\n   \u318d If  No,  the process moves forward to another decision point.\n4. Do you agree with the changes?: This is another decision point where a person evaluates whether they agree with \nthe changes made to the article.\n   \u318d If they Agree, the process moves to establishing a New Consensus based on the changes.\n   \u318d If they Disagree, the process moves to seeking a compromise.\n5. Seek the Compromise: Efforts are made to find a middle ground that satisfies all parties involved in the editing \nprocess.\n6. Implement: Once a compromise is found, it is implemented in the article.\n7. New Consensus: After successful implementation and agreement, a new consensus is reached regarding the content \nof the article.\nThis cycle can repeat as necessary whenever new edits are made, ensuring continuous improvement and agreement \namong the collaborators.\nNote\uff1aThe output of GPT-4V is the Markdown code and we show the rendered \nresult.\nFigure 40: Results on table and chart reasoning. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 3.1 for detailed discussions.\n52\nApproximately how many millions of RMB was \nthe overall cloud computing market size in the \nregion in 2020?\nGemini Pro\nRMB 800 billion\nTable and Chart Reasoning\nPrompt\nSec. 3.1 Text-Rich Visual Reasoning\nThe bar chart you've provided shows the market size \nfor private and public cloud in billions of RMB for \nthe year 2020. To give you an accurate answer, I \nneed to calculate the combined value of both private \nand public clouds for that year.\nFrom the visual estimation:\n- The value for the private cloud seems to be \napproximately 400 billion RMB.\n- The value for the public cloud is slightly higher, \naround 600 billion RMB.\nAdding these values together:\n400 (private cloud) + 600 (public cloud) = 1000 \nbillion RMB\nSince 1 billion RMB is equal to 1000 million RMB, \nthe overall market size in millions for 2020 would be:\n1000 billion RMB * 1000 = 1,000,000 million RMB\nSo, the overall cloud computing market size in the \nregion in 2020 was approximately 1,000,000 million \nRMB.\nGPT-4V\nGemini Pro\nThe answer is 2017.\nThe paid penetration rate can be calculated by dividing \nthe number of paid users by the total number of users. In \n2017, the number of paid users was 300,000, and the \ntotal number of users was 700,000. Therefore, the paid \npenetration rate was 300,000 / 700,000 = 0.43, or 43%. \nThis is the first time that the paid penetration rate \nreached 3% from 2013-2018.\nTo calculate the paid penetration rate for each year, we would need \nto follow this formula:\nWe can extract the numbers from the image and perform the \ncalculations for each year to find out which year first reached a 3% \npaid penetration rate. Let's start by analyzing the data from the \nimage.\nNow that we have the image displayed, we can visually inspect the \ndata points on the graph. However, to precisely calculate the paid \npenetration rate for each year, we would need exact numerical \nvalues, which are not clear from this image alone.\nWe can attempt to estimate the values by using the image's scale, \nbut for an accurate calculation, we would ideally have the actual \ndata used to create this graph. Without precise figures, any \ncalculation will be an approximation.\nLet's attempt to estimate the values based on the graph's scale. The \nx-axis represents the years from 2013 to 2018. The y-axis has two \nscales: one for the user scale (in millions of dollars) and one for the \npaid user scale (in hundreds of thousands of dollars). We can \napproximate the values by judging their position on the y-axis \nrelative to the known scale markers.\nOnce we have these approximated values, we can perform the \ncalculations to estimate in which year the paid penetration rate first \nreached 3%. Let's proceed with this estimation.\nBased on the estimated values from the graph, the paid penetration \nrate in China's digital music industry did not reach 3% in any of the \nyears from 2013 to 2018. The closest it came to was approximately \n2.5% in 2018. However, please note that these figures are based on \nvisual estimations and the actual values may differ. For precise \ncalculations, the actual numerical data is necessary.\nGPT-4V\nThe overall cloud computing market size in the \nregion in 2020 was approximately 900.\nSPHINX\n2018\nSPHINX\nThe paid penetration rate is derived by dividing \nthe size of paid users by the total user size. So, \nwhich year was the first time China's digital music \npaid penetration rate reached 3% from 2013-2018?\nPrompt\nFigure 41: Results on table and chart reasoning. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 3.1 for detailed discussions.\n53\nI want to buy two milk and one beer, please help \nme to figure out how much dollar I need to pay in \ntotal. Please think step by step.\nGemini Pro\nThe price of one milk is $2.00. The price of one \nbeer is $7.00. So the total price is 2 * $2.00 + 1 \n* $7.00 = $11.00.\nTable and Chart Reasoning\nPrompt\nSec. 3.1 Text-Rich Visual Reasoning\nI have outlined the items to be purchased and their \nquantities using a blue marker, please help me \nfigure out how much DOLLAR I need to pay in \ntotal. Please think step by step.\nPrompt\nThe OCR has successfully extracted the text \nfrom the image. Now I will calculate the total \ncost for two milks and one beer based on the \nextracted prices.\n\u318d The price of one milk is $2.00.\n\u318d The price of one beer is $7.00.\nTo calculate the total:\nTotal cost = (Price of Milk x Quantity of Milk) \n+ (Price of Beer x Quantity of Beer)\nLet's do the math.\nThe total amount you need to pay for two milks \nand one beer is $11.00.\nGPT-4V\nGemini Pro\nThe total cost of the items is $46.00.\nTo calculate the total cost, we need to multiply \nthe quantity of each item by its price and then \nadd the results together.\nFor example, the cost of 2 gallons of milk is 2 x \n$2.00 = $4.00.\nThe total cost of all the items is $1.00 + $2.00 + \n$10.00 + $3.00 + $4.00 + $4.00 + $4.00 + $5.00 \n+ $5.00 + $7.00 = $46.00.\nThe OCR has extracted the text, but it contains \nsome errors and noise that we need to clean up. \nWe can see the relevant items are 'Milk' and \n'Beer' with their respective prices and quantities. \nLet's correct the OCR errors and calculate the \ntotal cost for these items.\nFrom the image, it appears that 'Milk' is $2.00 \nper unit and marked with 'x2' indicating two \nunits, and 'Beer' is $7.00 per unit with no \nspecified quantity, so we can assume it is one \nunit. I will now calculate the total cost.\nThe total amount you need to pay for the items \nmarked in blue (Milk and Beer) is $11.00.\nGPT-4V\nThe price of two milk is $2.00 x 2 = $4.00. The \nprice of one beer is $7.00. So, the total price is \n$4.00 + $7.00 = $11.00.\nSPHINX\nThe total cost of the items is $70.00.\nSPHINX\nFigure 42: Results on table and chart reasoning. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 3.1 for detailed discussions.\n54\nHere are the order details for my taxi ride. Should I \nactually pay $29.42?\nTable and Chart Reasoning\nPrompt\nHere are the order details for my taxi ride. Should I \nactually pay $32.42?\nPrompt\nGemini Pro\nNo, you should not pay $29.42. The total fee is \n$32.42, but you have a $3 discount, so you should \nactually pay $29.42.\nYes, you should pay $29.42 for the taxi ride.\nSPHINX\nBased on the order details you've provided from \nyour taxi ride:\n- Base Fare: $10\n- Mileage Fee: $12.11\n- Duration Fee: $10.32\n- Total: $32.42\n- Discount: $3\n- Actually Paid: $29.42\nThe calculation seems correct. You start with a total \nof $32.42 for the combined fees. Then, a discount \nof $3 is applied. Subtracting the discount from the \ntotal gives you $32.42 - $3 = $29.42, which is the \namount you should actually pay.\nGPT-4V\nGemini Pro\nNo, you should not pay $32.42. The \"Actually \nPaid\" amount is the amount that you should pay. In \nthis case, it is $29.42.\nBased on the order details in the image you \nprovided, the calculations for your taxi ride seem to \nhave already been made for you. Here's a \nbreakdown:\n- Base Fare: $10\n- Mileage Fee: $12.11\n- Duration Fee: $10.32\n- Total before discount: $32.43 (It looks like there \nmight be a rounding error here since $10 + $12.11 + \n$10.32 should be $32.43)\n- Discount: $3\n- Total after discount: $29.43 (Again, there seems to \nbe a rounding error because if you subtract $3 from \n$32.43, it should be $29.43)\nAccording to these details, you should actually pay \n$29.43, not $32.42. It seems that there's a \ndiscrepancy in the total and actual amount paid, \nlikely due to a minor miscalculation. You may want \nto contact the service provider to clarify the final \namount due.\nGPT-4V\nYes, you should pay $32.42 for the taxi ride.\nSPHINX\nNote: \nSame query image as \nshown left, resized to \nsave space.\nSec. 3.1 Text-Rich Visual Reasoning\nFigure 43: Results on table and chart reasoning. Gemini provides a contradictory answer on the left\npart. Whereas, GPT-4V not only answers the question correctly but also identifies calculation errors\nin the bill on the right part. Green highlights the right answer. Red highlights the wrong answer.\nRefer to Section 3.1 for detailed discussions.\n55\nPlease generate the latex code of the formula in \nthe image.\nGemini Pro\nCodes Generation\nPrompt\nSec. 3.1 Text-Rich Visual Reasoning\nPlease generate the latex code of the formula in \nthe image.\nPrompt\nGPT-4V\nGemini Pro\nGPT-4V\nSPHINX\nSPHINX\nNote\uff1aThe output is the LaTeX code and we show the rendered result.\nFigure 44: Results on visual code generation. Refer to Section 3.1 for detailed discussions.\n56\nPlease generate the latex code of the formula in \nthe image.\nGemini Pro\nCodes Generation\nPrompt\nSec. 3.1 Text-Rich Visual Reasoning\nPlease generate the latex code of the formula in \nthe image.\nPrompt\nGPT-4V\nGemini Pro\nGPT-4V\nSPHINX\nSPHINX\n(The result is none.)\nFigure 45: Results on visual code generation. Refer to Section 3.1 for detailed discussions.\n57\nGenerate the HTML code of the web page.\nGemini Pro\nCodes Generation\nPrompt\nSec. 3.1 Text-Rich Visual Reasoning\nGenerate the HTML code of the web page.\nPrompt\nI'm unable to directly generate HTML code \nfrom an image. However, if you have a specific \nsection of the webpage you're interested in or \nneed help understanding how to structure \nHTML for a similar layout, I can provide \nguidance on that. Could you specify which part \nof the web page you are interested in or what \nguidance you require?\nGPT-4V\nGemini Pro\nI'm unable to directly generate HTML code \nfrom an image. \u2026\u2026\nGPT-4V\nModel[0.21, 0.33, 0.27, 0.35].public. Here\u2019s \nwhat to know.[0.21, 0.4, 0.37, 0.41].Gemini is \nGoogle\u2019s largest and most capable AI that can \nprocess images, video, text, and even [0.21, 0.37, \n0.78, 0.39].What is Google Gemini AI? How to \nUse the New Chatbot [0.21, 0.3, 0.74, 0.32].\nSPHINX\n1[0.97, 0.26, 0.98, 0.27].1[0.97, 0.27, 0.98, \n0.28].1[0.97, 0.28, 0.98, 0.29].1[0.97, 0.29, 0.98, \n0.3].1[0.97, 0.3, 0.98, 0.31].1[0.97, 0.31, 0.98, \n0.32].1[0.97, 0.32, 0.98, 0.33].1[0.97, 0.33, 0.98, \n0.34].1[0.97, 0.34, 0.98, 0.35].1[0.97, 0.35, 0.98, \n0.36].1[0.97, 0.36, 0.98, 0.37].1[0.97, 0.37, 0.98, \n0.38].1[0.97, 0.38, 0.98, 0.39].1[0.97, 0.39, 0.98, \n0.4].1[0.97, 0.4, 0.98, 0.41].1[0.97, 0.41, 0.98, \n0.42].1\u2026\u2026\nSPHINX\nNote\uff1aSome of the output is the HTML code and we show the rendered result.\nFigure 46: Results on visual code generation. For the webpage shown on the right, the response\nprovided by Gemini results in a blank webpage. Yellow highlights the incompetence in performing\nthe task. Red highlights the wrong answer. Refer to Section 3.1 for detailed discussions.\n58\nGenerate the HTML code of the web page.\nGemini Pro\nCodes Generation\nPrompt\nSec. 3.1 Text-Rich Visual Reasoning\nGenerate the HTML code of the web page.\nPrompt\nGPT-4V\nGemini Pro\nGPT-4V\n<p>PHOTO</p>[0.48, 0.26, 0.53, 0.28].\nSPHINX\n19[0.73, 0.26, 0.75, 0.28].\nSPHINX\nI'm sorry, but I can't assist with generating the \nHTML code of a webpage from an image. If \nyou need help with HTML or have any other \nquestions regarding web development or \nanother topic, feel free to ask!\nNote\uff1aSome of the output is the HTML code and we show the rendered result.\nFigure 47: Results on visual code generation. Yellow highlights the incompetence in performing\nthe task. Red highlights the wrong answer. Refer to Section 3.1 for detailed discussions.\n59\nIllustration of GPT-4V\u2019s ability to turn at highway ramp. Green highlights the right answer in understanding. Red \nhighlights the wrong answer in understanding. Yellow highlights the incompetence in performing the task. Check \nSection 3.2 for detailed discussions.\nThe tangram is a puzzle consisting of seven flat \npolygons that combine to form various shapes. \nWhat do the four shapes in the image look like?\nGemini Pro\na.\nboat\nb.\nhouse\nc.\nwindmill\nd.\nstar\nAbstract Visual Stimuli\nPrompt\nSec. 3.2 Abstract Visual Reasoning\nThe image you've uploaded appears to be a \nrepresentation of Tangram solutions, which are \nmade up of seven flat shapes called tans. Here's \nwhat each of the four shapes represents:\na.\nThis shape resembles a boat with a tall sail \non the left and the boat's hull at the bottom.\nb.\nThis figure could be interpreted as a house \nwith a pointed roof on top of a square base.\nc.\nThe third shape looks like a larger triangle \nbalanced on one tip, with smaller triangles \nand a square making up its interior.\nd.\nThe final shape is more abstract, but it \ncould be seen as a person or animal with \noutstretched limbs, or an X-like figure with \na square in the center.\nTangrams are versatile and allow for a wide \nvariety of patterns and figures to be created, \noften resembling animals, people, numbers, and \nvarious geometric arrangements.\nGPT-4V\nGemini Pro\nThe correct answer is c.\nThe person in a is missing an arm. The person in \nb has an extra piece. The person in d has the \nyellow piece in the wrong place.\nThe image shows a series of four figures, each \nlabeled a, b, c, and d, composed of tangram \npieces. Each figure represents a different posture \nof a person, as follows:\nFigure a: The person appears to be walking or \nstepping forward.\nFigure b: The person seems to be sitting with \none leg extended.\nFigure c: The person looks to be standing with \narms and legs spread out, similar to a jumping \njack position.\nFigure d: The person might be depicted in a \ndynamic pose, possibly running or in mid-stride. \nTangram puzzles like these allow for various \nconfigurations that can suggest movement or \naction, even though the figures themselves are \nstatic.\nGPT-4V\nThe four shapes in the image are colored \ntriangles.\nSPHINX\nI'm sorry, but I cannot see the image you are \nreferring to. Can you please provide more \ninformation or a different image for me to \nanalyze?\nSPHINX\nThe picture below shows a person made of \ntangram pieces. Please identify the movements of \nthe person.\nPrompt\nFigure 48: Results on abstract visual stimuli. Green highlights the right answer. Red highlights\nthe wrong answer. Yellow highlights the incompetence in performing the task. Refer to Section 3.1\nfor detailed discussions.\n60\nThe results of different models for  Temporal ordering. Green highlights the right answer in understanding. Red \nhighlights the wrong answer in understanding.\nThe following five pictures show the different stages of \nbuilding blocks, and below the pictures are their serial \nnumbers. Please use the serial numbers to provide the \ncorrect order in which they were built.\nGemini Pro\nThe correct order is: a-c-e-d-b.\nAbstract Visual Stimuli\nPrompt\nSec. 3.2  Abstract Visual Reasoning\nPrompt\nTo determine the correct order of the building blocks in the \nimage, we should look for the progression of the construction. \nHere's the logical sequence based on what each image adds to \nthe structure:\n1. Start with the base or the most incomplete stage of the \nconstruction.\n2. Look for the addition of new blocks on top of the existing \nones.\n3. Continue following the sequence where new blocks are \nadded.\n4. The final stage should be the most complete version of the \nconstruction.\nLet's analyze each labeled part:\n- (a) This part of the image has a substantial amount of the \nbase constructed, with several layers on top.\n- (b) This looks more complete with additional blocks on top, \nincluding what seems to be the final piece which is the red \nblock that acts as a crest.\n- (c) This has the base and a few layers but is less complete \nthan (a).\n- (d) This is more complete than (c) but less than (a), as it \nlacks the upper green and red blocks present in (a).\n- (e) This is the most incomplete, with just the base layer of \nthe structure.\nGPT-4V\nGemini Pro\nThe correct order is a-b-c-d-e.\nThe drawing order of the bird based on the complexity and \nprogression of the drawing from simple to more detailed \nwould be:\n1. Picture (a) - starting with a simple circle for the head.\n2. Picture (d) - adding the outline of the body.\n3. Picture (b) - drawing the beak and the eye.\n4. Picture (c) - adding legs and a wing.\n5. Picture (e) - final details are added, such as the pupil in the \neye and the tail feathers.\nSo, the order using the serial numbers would be: a, d, b, c, e.\nGPT-4V\n(a), (c), (e), (b), (d).\nSPHINX\n1. 2. 3. 4. 5.\nSPHINX\nPutting it all together, the correct order of construction from \nfirst to last would be:\n1. (e) - Start with the base.\n2. (c) - Add the first set of blocks on top of the base.\n3. (d) - Add another layer, filling out the sides.\n4. (a) - Further layers are added, with more details.\n5. (b) - The final details are added, such as the crest on top.\nPlease analyze these images and provide the correct order.\nFigure 49: Results on abstract visual stimuli. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 3.2 for detailed discussions.\n61\nIllustration of GPT-4V\u2019s ability to turn at highway ramp. Green highlights the right answer in understanding. Red \nhighlights the wrong answer in understanding. Yellow highlights the incompetence in performing the task. Check \nSection 3.2 for detailed discussions.\nLook the pattern on the left and select the missing item from the right. Please think step by step.\nGemini Pro\nThe triangle is divided into 3 triangles. The hexagon is divided into 6 triangles. So the missing item \nshould be divided into 9 triangles. The answer is d.\nWechsler Adult Intelligence Scale\nPrompt\nSec. 3.2 Abstract Visual Reasoning\nThis image presents a logical sequence or pattern puzzle where one needs to determine the missing \nshape in the sequence based on the transformation applied to the first shape. The top row shows a \ntransformation from a simple triangle to a more complex figure that retains the overall triangular shape \nbut adds internal lines to create new shapes within it. The bottom row shows a hexagon followed by an \narrow pointing to a question mark, suggesting that we need to determine the transformation applied to \nthe triangle and then apply the same transformation to the hexagon to find the correct figure among the \noptions provided. The transformation from the simple triangle to the complex figure appears to involve \ncreating a smaller triangle inside the original one and then adding lines that connect the vertices of the \ninner triangle to the midpoints of the sides of the larger triangle. We should apply this same \ntransformation to the hexagon: draw a smaller hexagon inside and then connect the vertices of the inner \nhexagon to the midpoints of the sides of the larger hexagon. Looking at the provided options for the \nhexagon:\na. This figure does not have the midpoints of the larger hexagon connected to the smaller hexagon.\nb. This figure has additional lines that do not connect the vertices of the inner shape to the midpoints of \nthe outer shape.\nc. This figure correctly connects the vertices of the inner hexagon to the midpoints of the outer \nhexagon's sides.\nd. This figure has too many lines, creating an overly complex shape that does not follow the \ntransformation pattern.\ne. This figure does not depict a hexagon at all.\nTherefore, the correct answer would be option c, as it follows the transformation pattern applied to the \ntriangle.\nGPT-4V\nThe missing item from the right is the letter 'a'. The pattern on the left is a triangle with a question mark. \nThe pattern on the right is a hexagon with a letter 'a'.\nSPHINX\nFigure 50: Results on Wechsler Adult Intelligence Scale. Green highlights the right answer. Red\nhighlights the wrong answer. Refer to Section 3.2 for detailed discussions.\n62\nThere are six letters a, b, c, d, e, and f written on the six sides of a cube. Based on the following four pictures, \ninfer which letter is opposite to b? Please think step by step.\nPrompt\nWechsler Adult Intelligence Scale\nThe four pictures show a cube \nwith letters a, b, c, d, e, and f \non the six sides. The letter \nopposite to b is c.\nSPHINX\nTo determine which letter is opposite to 'b' on the cube, we need to \nanalyze the four given pictures and see which sides of the cube are visible \nin each picture.\nIn the first picture, we see sides with letters 'a', 'e', and 'c'.\nIn the second picture, we see 'a', 'b', and 'c'.\nIn the third picture, we see 'e', 'f', and 'c'.\nIn the fourth picture, we see 'e', 'd', and 'a'.\nFrom these observations, we can make the following deductions:\n- 'a' is adjacent to 'e' and 'c' (from picture 1), and it's also adjacent to 'b' \n(from picture 2). Since a cube has six faces and 'a' is already shown to be \nadjacent to 'e', 'c', and 'b', it cannot be opposite to any of these.\n- 'c' is adjacent to 'a', 'e', and 'f' (from pictures 1 and 3), and also to 'b' \n(from picture 2), so it can't be opposite to 'b'.\n- 'e' is adjacent to 'a', 'c', and 'd' (from pictures 1 and 4), so it can't be \nopposite to 'b'.\n- Since 'b' appears only in the second picture and does not share a visible \nface with 'd' or 'f' in any of the pictures, we can deduce that 'b' is opposite \nto one of these two letters.\nNow, because 'f' is adjacent to 'c' and 'e' (from picture 3) and we know 'c' \nand 'e' are adjacent to 'a' and 'b', 'f' cannot be opposite to 'b'. Therefore, by \nthe process of elimination, 'd' must be the letter that is opposite to 'b' \nbecause it's the only letter that has not appeared on an adjacent side to 'b' \nin any of the four pictures.\nGPT-4V\nGemini Pro\nIn picture (1), e is opposite to a.\nIn picture (2), b is opposite to c.\nIn picture (3), f is opposite to e.\nIn picture (4), d is opposite to a.\nSo we can infer that a is \nopposite to e, b is opposite to c, \nc is opposite to b, d is opposite \nto a, e is opposite to f, and f is \nopposite to e.\nSec. 3.2 Abstract Visual Reasoning\nFigure 51: Results on Wechsler Adult Intelligence Scale. Green highlights the right answer. Red\nhighlights the wrong answer. Refer to Section 3.2 for detailed discussions.\n63\nIllustration of GPT-4V\u2019s ability to turn at highway ramp. Green highlights the right answer in understanding. Red \nhighlights the wrong answer in understanding. Yellow highlights the incompetence in performing the task. Check \nSection 3.2 for detailed discussions.\nAmong the eight options numbered 1 to 8 on the right side of the picture, which one is suitable to fill the \nmissing position on the left side of the picture? Please think step by step.\nGemini Pro\nFor each row, the number of rings in the outer circle is one less than the sum of the rings in the inner circles. \nFor example, in the first row, the outer circle has 1 ring, and the inner circles have 2 and 3 rings, and 2 + 3 = 5. \nIn the second row, the outer circle has 2 rings, and the inner circles have 1 and 3 rings, and 1 + 3 = 4.Therefore, \nfor the missing figure, the outer circle should have 3 rings, and the inner circles should have 2 and 4 rings. This \ncorresponds to option 6.\nRaven\u2019s Progressive Matrices\nPrompt\nSec. 3.2 Abstract Visual Reasoning\nThis puzzle appears to be a visual sequence or pattern recognition question, where you need to determine the \nrule that governs the sequence of shapes to find the missing piece.\nLet's analyze the given sequence:\n\u2022\nIn the first column, there are single circles.\n\u2022\nIn the second column, there are two concentric circles.\n\u2022\nIn the third column, there are three concentric circles.\nThe number of concentric circles increases as we move down in each column. Also, the thickness of the circles \nvaries. \nLet's apply this logic to find the missing piece:\n\u2022\nThe missing piece should be in the third column, which means it should have three concentric circles based \non the pattern.\n\u2022\nThe thickness of the circles alternates in each row - thin, thick, thin. Since the second row has thick circles, \nthe missing piece should have thin circles.\nNow, looking at the options:\n\u2022\nOptions 1, 2, and 8 have three concentric circles.\n\u2022\nAmong these, option 2 has thick circles, so it doesn't fit the pattern.\n\u2022\nOptions 1 and 8 have thin circles, but option 8 has a solid center circle, which does not follow the pattern \nseen in the previous rows.\nBased on this analysis, the suitable option to fill the missing position on the left side of the picture should be \noption 1, as it has three concentric circles with the correct thickness following the established pattern.\nGPT-4V\nThe missing position on the left side of the picture is marked with a question mark. The options 1 to 8 on the \nright side of the picture are numbered clockwise. The options 1 to 8 on the right side of the picture are marked \nwith circles. The option that is marked with a circle and is clockwise from the question mark is 7.\nSPHINX\nFigure 52: Results on Raven\u2019s Progressive Matrices. Red highlights the wrong answer. Refer to\nSection 3.2 for detailed discussions.\n64\nIllustration of GPT-4V\u2019s ability to turn at highway ramp. Green highlights the right answer in understanding. Red \nhighlights the wrong answer in understanding. Yellow highlights the incompetence in performing the task. Check \nSection 3.2 for detailed discussions.\nPlease look the pattern of the two rows and infer the two missing data. Please think step by step.\nGemini Pro\nFirst row: 16, 15, 17, 14, _, _Second row: 32, 33, 31, 34, _, _The difference between the numbers in the first \nrow is 1.The difference between the numbers in the second row is 2.The missing numbers are 13 and 30.\nRaven\u2019s Progressive Matrices\nPrompt\nSec. 3.2 Abstract Visual Reasoning\nLet's analyze the pattern in the two rows of numbers you've provided to determine the missing data. Here's the step-by-step thought process:\n1.\nIdentify the Pattern: We will look at the available numbers to identify any arithmetic or geometric sequences, common differences, or any \nother relationships between the numbers.\n2.\nApply the Pattern: Once we've identified the pattern, we will apply it to find the missing numbers.\n3.\nVerify the Pattern: Finally, we'll verify that the pattern holds for all given numbers in the sequence.\nLet me start by loading the image and closely examining the numbers presented.\nThe numbers extracted from the image are as follows:\n\u2022\nFirst row: 16, 15, 17, 14\n\u2022\nSecond row: 32, 33, 31, 34\nTo infer the missing data, let's examine the pattern between the numbers in each row.\nFirst Row\n\u2022\nThe pattern does not seem to be strictly increasing or decreasing.\n\u2022\nThe numbers fluctuate up and down.\nSecond Row\n\u2022\nSimilar to the first row, there's no clear increment or decrement.\n\u2022\nThe numbers also fluctuate.\nGiven that the numbers fluctuate, we might consider that there is an alternating pattern or another non-linear relationship.\nLet's compare the corresponding numbers in the two rows:\n\u2022\n16 corresponds to 32\n\u2022\n15 corresponds to 33\n\u2022\n17 corresponds to 31\n\u2022\n14 corresponds to 34\nThis suggests that the second row might be double the first row minus a certain number.\nFor instance:\n\u2022\nDouble of 16 is 32 (16 * 2 = 32)\n\u2022\nDouble of 15 is 30, plus 3 gives us 33 (15 * 2 + 3 = 33)\n\u2022\nDouble of 17 is 34, minus 3 gives us 31 (17 * 2 - 3 = 31)\n\u2022\nDouble of 14 is 28, plus 6 gives us 34 (14 * 2 + 6 = 34)\nThe alternating addition and subtraction pattern could be the key here. We will attempt to double the first row's numbers and add or subtract a \nnumber to match the second row, noticing the pattern of alternation.\nLet me calculate the missing numbers using this hypothesis.\nThe pattern suggests that the missing numbers are 14 for the first row and 31 for the second row. This fits with the alternating pattern of \ndoubling the first row and then adding or subtracting a certain value to get the second row's numbers. \nSo, the completed sequences would be:\n\u2022\nFirst row: 16, 15, 17, 14, 14\n\u2022\nSecond row: 32, 33, 31, 34, 31\nThis means the missing number in the first row should be 14, and the corresponding missing number in the second row should be 31.\nGPT-4V\nThe first row has 16, 15, 17, 14. The second row has 32, 33, 31, 34.\nSPHINX\nFigure 53: Results on Raven\u2019s Progressive Matrices.\nGreen highlights the right answer.\nRed\nhighlights the wrong answer. Refer to Section 3.2 for detailed discussions.\n65\nStep by step, calculate the value of y in the system \nof equations.\nGemini Pro\n1. Subtract the first equation from the second \nequation.\n2y + x = 4\nx + 1 = 3\n--------\ny + 0 = 1\n2. Solve the resulting equation for y.\ny = 1\n3. Substitute the value of y back into one of the \noriginal equations to solve for x.\n2y + x = 4\n2(1) + x = 4\n2 + x = 4\nx = 2\n4. Check your solution by substituting the \nvalues of x and y back into both original \nequations.\nx + 1 = 3\n2 + 1 = 3\n3 = 3\n2y + x = 4\n2(1) + 2 = 4\n2 + 2 = 4\n4 = 4\nTherefore, the solution to the system of \nequations is (x, y) = (2, 1).\nMathematical Problems\nPrompt\nSec. 3.3 Science Problem-solving\nCalculate the value of side BC step by step.\nPrompt\nThe value of  y in the given system of equations \nis 1.\nGPT-4V\nGemini Pro\nIn the triangle ABC, we have:\n\u2220A = 90\u00b0\n\u2220B = 60\u00b0\na = 3\nWe can use the sine function to find the length \nof side BC:\nsin(\u2220B) = opposite/hypotenuse\nsin(60\u00b0) = 3/BC\nBC = 3/sin(60\u00b0)\nBC = 3 * 0.866\nBC = 2.6\nTherefore, the value of side BC is 2.6.\nGPT-4V\nTo solve the system of equations, we need to \nisolate the variable y on one side of the equation. \nHere are the steps to solve the system:\nStart by adding the two equations: 2x + 1 = 3 \nand 2x + 4 = 3.\nCombine like terms: 4x + 5 = 3.\nSubtract 3 from both sides: 4x = -2.\nDivide both sides by 4: x = -2/4.\nSimplify the expression: x = -1/2.\nTherefore, the value of y in the system of \nequations is -1/2.\nSPHINX\nTo calculate the value of side BC, we need to \nfollow the steps outlined in the problem \nstatement.\nDraw a right triangle with side lengths 60 and 3.\nDraw a line from point A to point B to form a \n90-degree angle.\nDraw a line from point B to point C to form a \n90-degree angle.\nDraw a line from point C to point A to form a \n90-degree angle.\u2026\u2026\nSPHINX\nFigure 54: Results on mathematical problem-solving. Green highlights the right answer. Red\nhighlights the wrong answer. Yellow highlights the incompetence in performing the task. Refer to\nSection 3.3 for detailed discussions.\n66\nAnswer the question in the figure.\nGemini Pro\n(d) [ -2 1 0 ; -10 5 1 ]\nMathematical Problems\nPrompt\nSec. 3.3 Science Problem-solving\nGPT-4V\nThe matrix A is invertible if and only if the determinant of A is not equal to zero. The determinant of a \nmatrix is a scalar value that can be calculated using the formula: |A| = ad - bc, where a, b, and c are the \ncofactors of the matrix A. The cofactors of a matrix are the elements of the inverse of the matrix. \nTherefore, to determine if a matrix is invertible, we need to find the inverse of the matrix. The inverse \nof a matrix is a matrix that has the same size as the original matrix and whose determinant is equal to \none. The inverse of a matrix can be calculated using the formula: |A^-1| = ad - bc, where a, b, and c are \nthe cofactors of the matrix A.\nSPHINX\nFigure 55: Results on mathematical problem-solving. Green highlights the right answer. Red\nhighlights the wrong answer. Refer to Section 3.3 for detailed discussions.\n67\nSimplify this indefinite integral step by step.\nGemini Pro\nMathematical problem\nPrompt\nSec. 3.3 Science Problem-solving\nCalculate the value of the integral equation step \nby step.\nPrompt\nGPT-4V\nGemini Pro\nGPT-4V\nTo simplify the indefinite integral, we need to find the \nantiderivative of the given function. The function is given as f(x) = \nx^2 + 2x + 1.\nThe antiderivative of f(x) = x^2 + 2x + 1 is given by:\nF(x) = x^3/3 + x^2/2 + x/3 + C\nwhere C is the constant of integration.\nTo simplify the expression, we can use the substitution x = sin^2(t) \nto express the function in terms of the sine function.\nF(t) = (sin^2(t))^3/3 + (sin^2(t))^2/2 + (sin^2(t))/3 + C\nNow, we can integrate this expression using the Fundamental \nTheorem of Calculus.\nF(t) = (1/3)sin^3(t) + (1/2)sin^2(t) + (1/3)sin(t) + C\nTherefore, the indefinite integral of f(x) = x^2 + 2x + 1 is given by:\nF(x) = (1/3)sin^3(x) + (1/2)sin^2(x) + (1/3)sin(x) + C\nwhere C is the constant of integration.\nSPHINX\nThe integral equation is written as: \u222b(ln(x))dx. To calculate the \nvalue of this integral, we can use the following steps:\nStart by evaluating the indefinite integral of the logarithmic \nfunction. The indefinite integral of the logarithmic function is given \nby the formula: ln(x) + C, where C is a constant.\nSubstitute the indefinite integral of the logarithmic function into the \nintegral equation. This gives us: \u222b(ln(x) + C)dx.\nEvaluate the integral using the Fundamental Theorem of Calculus. \nThe Fundamental Theorem of Calculus states that the indefinite \nintegral of a function can be evaluated by integrating the \nantiderivative of the function from 0 to infinity and adding the \nconstant of integration. In this case, the antiderivative of the \nlogarithmic function is ln(x) + C.\nSubstitute the antiderivative of the logarithmic function into the \nintegral equation. This gives us: \u222b(ln(x) + C)dx = \u222b(ln(x) + C) from \n0 to infinity.\nEvaluate the integral using the limit definition of the definite \nintegral. The definite integral of a function is the difference \nbetween the indefinite integral of the function from a to infinity and \nthe indefinite integral of the function from 0 to a. In this case, the \nindefinite integral of the logarithmic function from 0 to infinity is \nln(x) + C, and the indefinite integral of the logarithmic function \nfrom 0 to a is ln(a).\nSubstitute the values of the indefinite integrals into the integral \nequation. This gives us: \u222b(ln(x) + C)dx = ln(a) + C.\nSolve for C by subtracting the indefinite integral of the logarithmic \nfunction from the definite integral of the logarithmic function. This \ngives us: C = ln(a) - ln(x).\nTherefore, the value of the integral equation is ln(a) - ln(x).\nSPHINX\nFigure 56: Results on mathematical problem-solving. Green highlights the right answer. Red\nhighlights the wrong answer. Yellow highlights the incompetence in performing the task. Refer to\nSection 3.3 for detailed discussions.\n68\nAnswer the question in the figure. Provide intermediate steps.\nGemini Pro\nIn the right triangle on the left, the tangent of the angle is the ratio of the opposite side to the adjacent \nside.\nTherefore, the tree is 20 feet tall.\nMathematical problem\nPrompt\nSec. 3.3 Science Problem-solving\nThe tree is 20 meters tall. This is because the triangle formed by the tree and its shadow is a right-\nangled isosceles triangle, with the angle of elevation to the top of the tree from the tip of the shadow \nbeing 45 degrees. For such a triangle, the height is equal to the base, which is the length of the shadow. \nThus, the height of the tree is the same as the length of the shadow: 20 meters.\nGPT-4V\nThe tree is 45 degrees tall. The tree is 45 degrees tall.\nSPHINX\nFigure 57: Results on mathematical problem-solving. Red highlights the wrong answer. Refer to\nSection 3.3 for detailed discussions.\n69\nCalculate the value of this expression.\nGemini Pro\nThe answer is 1.\nMathematical problem\nPrompt\nSec. 3.3 Science Problem-solving\nCalculate the value of this expression .please \nthink step by step.\nPrompt\nGPT-4V\nGemini Pro\n8 + 2 - 3 x 3\n= 8 + 2 - 9\n= 10 - 9\n= 1\nThe expression in the image is calculated as \nfollows:\n1. First, perform the multiplication: 3 \u00d7 3 = 9\n2. Then, perform the addition and subtraction \nfrom left to right: 8 + 7 - 9\nThis simplifies to 15 - 9 which equals 6. \nTherefore, the value of the expression is 6.\nGPT-4V\nThe expression is 8 + 2 - 3 x 3 = 11.\nSPHINX\nThe expression is 8 + 2 - 3 x 3 = ?. The first step \nis to solve the expression inside the parentheses. \n8 + 2 - 3 x 3 = 8 + 2 - 9 = 1. The expression is 8 \n+ 2 - 3 x 3 = 1.\nSPHINX\nFigure 58: Results on mathematical problem-solving. Green highlights the right answer. Red\nhighlights the wrong answer. Refer to Section 3.3 for detailed discussions.\n70\nIs the answer to the arithmetic question in the image \n32? Please answer yes or no.\nMathematical problem\nPrompt\nWhat is the answer to the arithmetic question in the \nimage?\nPrompt\nGemini Pro\nyes\nGemini Pro\nThe answer is 33.\nNo, the answer to the arithmetic question in the \nimage is not 32.\nGPT-4V\nThe answer to the arithmetic question in the image \nis 33.\nGPT-4V\nNo\nSPHINX\nThe answer to the arithmetic question is 10.\nSPHINX\nSec. 3.3 Science Problem-solving\nFigure 59: Results on mathematical problem-solving. Green highlights the right answer. Red\nhighlights the wrong answer. Refer to Section 3.3 for detailed discussions.\n71\nSolve this physics problem step by step.\nGemini Pro\nThe average speed of the student during the entire race can be calculated using the formula:\naverage speed = total distance / total time\nIn this case, the total distance is 100 meters, and the total time is 10 seconds. Therefore, the average \nspeed is:\naverage speed = 100 meters / 10 seconds = 10 meters per second\nSo the answer is 10 m/s\nPhysics Problems\nPrompt\nSec. 3.3 Science Problem-solving\nThe average speed of the student during the entire 100-meter dash is 10 meters per second.\nGPT-4V\nThe problem states that a runner in a 100-meter dash, starts at the start of the race, and 10 seconds later, \nwhen crossing the finish line, the instantaneous speed is 8 meters per second. To solve this problem, we \nneed to calculate the average speed of the runner during the entire race.\nThe average speed of the runner during the entire race is calculated by dividing the total distance \ntraveled by the runner (100 meters) by the total time taken by the runner to complete the race (10 \nseconds). Therefore, the average speed of the runner during the entire race is 100 meters / 10 seconds = \n10 meters per second.\nSPHINX\nFigure 60: Results on physics problem-solving. Green highlights the right answer. Refer to Section\n3.3 for detailed discussions.\n72\nHere is a solution to a Physics problem by a student. \nTry to follow the steps already taken to complete \nthe solution. Make sure to use LaTeX for math. \nSince the problem does not provide specific values \nfor each physical quantity, the result should only \ninclude the mathematical expression for F.\nGemini Pro\nPhysics Problems\nPrompt\nSec. 3.3 Science Problem-solving\nGPT-4V\nGemini Pro\nFirst find the total resistance of the circuit. Since the \nrheostat is a variable resistor, its resistance can vary. \nHowever, since the slider is positioned at its midpoint, \nthe resistance of the rheostat is equal to half of its total \nresistance, which is 10 \u03a9. The total resistance of the \ncircuit is then:\nR_total = R_0 + R_rheostat \n= 40 \u03a9 + 10 \u03a9 = 50 \u03a9\nNow use Ohm's law to find the current flowing through \nthe circuit:\nI = U_AB / R_total = 100 V / 50 \u03a9 = 2 A\nSince the resistors R_0 and R_rheostat are connected in \nparallel, the current flowing through R_0 is:\nI_R0 = I / 2 = 2 A / 2 = 1 A\nTherefore, the current flowing through the resistor R_0 \nis 1 A.\nThe problem states that the current is flowing \nthrough the resistor. The current is flowing \nthrough the resistor, so the current is flowing \nthrough the resistor. The answer is yes.\nSPHINX\nNote\uff1awe show the rendered result of some of the output.\nGPT-4V\nSPHINX\n\uff08the result is none.\uff09\nSolve this physics problem step by step.\nPrompt\nFigure 61: Results on physics problem-solving. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 3.3 for detailed discussions.\n73\nSolve this physics problem step by step.\nGemini Pro\n1. To solve this problem, we must first find the velocity of the ball \nat point B. \n2. To do this, we can use the conservation of energy principle. The \ninitial energy of the ball is all potential energy, due to its height \nabove the ground. The final energy of the ball is all kinetic energy, \ndue to its motion.\n3. We can write the conservation of energy equation as follows:\nmgh = 1/2mv^2\nwhere m is the mass of the ball, g is the acceleration due to gravity, \nh is the height of the ball above the ground, and v is the velocity of \nthe ball.\n4. We know that the height of the ball above the ground is R, the \nradius of the track. We also know that the acceleration due to \ngravity is 10 m/s^2. We can substitute these values into the \nconservation of energy equation to get:\nmgR = 1/2mv^2\n5. We can solve this equation for v to get:\nv = sqrt(2gR)\n6. Now that we know the velocity of the ball at point B, we can find \nthe distance it travels before landing. \n7. To do this, we can use the equation of motion for a projectile:\ny = -1/2gt^2 + vt + h\nwhere y is the height of the ball, g is the acceleration due to gravity, \nt is the time of flight, v is the velocity of the ball, and h is the initial \nheight of the ball.\n8. We know that the height of the ball at point B is R, the radius of \nthe track. We also know that the acceleration due to gravity is 10 \nm/s^2. We can substitute these values into the equation of motion \nto get:\nR = -1/2gt^2 + vt\n9. We can solve this equation for t to get:\nt = (v + sqrt(v^2 + 2gR))/g\n10. Now that we know the time of flight of the ball, we can find the \ndistance it travels before landing. \n11. To do this, we can use the equation:\nd = vt\nwhere d is the distance traveled, v is the velocity of the ball, and t is \nthe time of flight.\n12. We know that the velocity of the ball at point B is sqrt(2gR). \nWe also know the time of flight of the ball. We can substitute these \nvalues into the equation to get:\nd = sqrt(2gR) * (v + sqrt(v^2 + 2gR))/g\n13. We can simplify this equation to get:\nd = 2R + sqrt(2RgR)\n14. We know that the radius of the track is 1 m. We can substitute \nthis value into the equation to get:\nd = 2(1) + sqrt(2(1)(10))\n15. We can calculate this to get:\nd = 2 + sqrt(20)\n16. We can calculate this to get:\nd = 2 + 4.47\n17. Therefore, the landing point C is 6.47 m from point A.\nPhysics Problems\nPrompt\nSec. 3.3 Science Problem-solving\nGPT-4V\nGPT-4V\nStep-by-step solution:\nFirst, we need to find the distance the ball travels along the track. \nThis distance is given by the formula:\nd = vt\nwhere v is the velocity of the ball, t is the time it takes for the ball \nto travel from point A to point B, and d is the distance traveled.\nWe know that the velocity of the ball is given by the formula:\nv = 10 m/s^2\nand the time it takes for the ball to travel from point A to point B is \ngiven by the formula:\nt = 2R/v\nSubstituting these values into the formula for the distance traveled, \nwe get:\nd = 2R/10 m/s^2 = 0.05 m\nThis is the distance the ball travels along the track.\nNext, we need to find the distance from point A to point C. This \ndistance is given by the formula:\nC = A + d\nSubstituting the values for A, d, and R, we get:\nC = A + 0.05 m\nThis is the distance from point A to point C.\nFinally, we need to find the angle between point A and point C. \nThis angle is given by the formula:\n\u03b8 = 180\u00b0/\u03c0 (C/A)\nSubstituting the values for A, C, and R, we get:\n\u03b8 = 180\u00b0/\u03c0 (0.05 m/1 m) = 180\u00b0/\u03c0\nThis is the angle between point A and point C.\nTherefore, the distance from point A to point C is 0.05 m, and the \nangle between point A and point C is 180\u00b0/\u03c0 radians.\nSPHINX\nNote\uff1aThe output of GPT-4V is the Latex code as well as the output of Gemini is the Markdown code and we show the rendered result.\nFigure 62: Results on physics problem-solving. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 3.3 for detailed discussions.\n74\nThe image features a red leaf placed on a cement \nsurface, which has two holes in it. The leaf appears \nto be smiling, giving the impression of a happy and \nplayful expression. This creative arrangement of the \nleaf and the holes in the cement can evoke feelings \nof joy, whimsy, and a sense of lightheartedness. The \nartist may have intended to bring a smile to the \nviewer's face or to encourage them to see the beauty \nin everyday objects and situations.\nSPHINX\nPlease analyze this expression and what emotions it \nwants to express.\nPrompt\nThe image you've provided appears to show a \nconcrete surface with two dark holes and a red leaf \npositioned in such a way that it resembles a face: the \nholes act as the eyes and the leaf as a mouth. This \nkind of arrangement is often interpreted as a form of \npareidolia, where our minds perceive a familiar \npattern of a face where there isn't one.\nThe 'expression' of the face seems neutral or non-\nemotive; however, interpretations of this kind of \nimagery can vary greatly from person to person. \nSome might see it as a 'smiling' face due to the \ncurvature of the leaf, suggesting a sense of whimsy \nor happiness. Others might interpret it as a \n'frowning' face, depending on the angle and their \nperspective, which could convey a sense of sadness \nor melancholy. \nThe overall emotion that this image wants to express \nmay be open to interpretation, and it could be seen \nas a reflection of the viewer's current mood or \nfeelings. It doesn't express emotion in the human \nsense but invites the viewer to project emotion onto \nit. This kind of visual play is common in \nphotography and art, providing an engaging visual \npuzzle that encourages the viewer to stop and \nconsider the image more deeply.\nGPT-4V\nGemini Pro\nThe expression on the face is a happy one. The leaf \nis smiling.\nSec. 3.4 Emotion Understanding\nFacial Expression Analysis\nI'm sorry, but I cannot see any faces or emotions in \nthe image. The image only contains text.\nSPHINX\nIdentify and read emotions of people from their faces \nas shown in the image.\nPrompt\nThe person in the image appears to display an \nexpression of fear or intense anxiety. Their eyes are \nwide open, eyebrows are raised, and mouth is \nslightly open, which are all facial cues commonly \nassociated with fear. The person is also biting their \nnails, a gesture often linked to nervousness or stress. \nThese visual cues can indicate that the person is \nexperiencing a moment of significant distress or \napprehension.\nGPT-4V\nGemini Pro\nThe person in the image is scared.\nThe person has his hands on his face, with his mouth \nopen. His eyes are wide open and his eyebrows are \nraised. These are all signs of fear.\nFigure 63: Results on facial expression analysis. Green highlights the right answer. Red highlights\nthe wrong answer. Yellow highlights the incompetence in performing the task. Refer to Section 3.4\nfor detailed discussions.\n75\nSec. 3.4 Emotion Understanding\nFacial Expression Analysis\nThe image you've uploaded appears to be a coffee \ncup with a frowning face on the surface of the \ncoffee, with the rim of the cup reading \"HAVE A \nGOOD DAY\". The expression on the coffee seems \nto be one of sadness or dissatisfaction, which \ncontrasts with the message on the cup.\nThe frowning face is a universal symbol of \nunhappiness or discontent. When combined with \nthe text \"HAVE A GOOD DAY\", it might suggest \nan ironic or sarcastic message, playing on the \ndiscrepancy between the unhappy expression and \nthe typically positive sentiment of the phrase. It \ncould be interpreted as a humorous take on a bad \nday, or expressing the idea that even if things aren't \ngreat, one can still find a reason to smile or hope for \na better day.\nGPT-4V\nPlease analyze this expression and what emotions it \nwants to express.\nPrompt\nThe expression \"Have a frown day\" is a playful and \nhumorous way of saying \"Have a bad day.\" The \nfrown face in the coffee foam is meant to represent a \nnegative emotion or experience, and the phrase is \nintended to be lighthearted and not taken seriously. It \nis a creative way of expressing a difficult or \nunfortunate day, and the intention is to evoke a sense \nof humor and resilience in the face of adversity.\nSPHINX\nGemini Pro\nIt looks like the coffee is expressing a feeling of \ntiredness or boredom. The eyes are half-closed and \nthe mouth is turned down at the corners. The \noverall impression is one of sadness or resignation.\nFigure 64: Results on facial expression analysis.\nGreen highlights the right answer.\nYellow\nhighlights the incompetence in performing the task. Refer to Section 3.4 for detailed discussions.\n76\nIllustration of GPT-4V\u2019s ability to turn at highway ramp. Green highlights the right answer in understanding. Red \nhighlights the wrong answer in understanding. Yellow highlights the incompetence in performing the task. Check \nSection 3.4 for detailed discussions.\nWhat feelings and emotions do people have about this picture?\nGemini Pro\nThis picture makes me feel sad and lonely. The \ndark clouds and the empty road make it look like \nthere is no hope. The only light comes from the \nstreet lamps, which are far away and barely \nvisible. This picture makes me feel like I am all \nalone in the world.\nImage Emotion Analysis\nPrompt\nSec. 3.4 Emotion Understanding\nThe image you've shared features a rather moody and overcast sky, which may evoke a variety of \nfeelings and emotions in different people. Here are some possible reactions:\n1.\nMelancholy or Calmness: The gray tones and the overcast nature of the sky can induce a feeling \nof melancholy or introspection in some. It may also bring a sense of calmness, as overcast days \noften bring a quietude to the environment.\n2.\n Mystery or Foreboding: The dark clouds could be seen as foreboding, suggesting an impending \nstorm or bad weather. This could create a sense of tension or apprehension about what is to come.\n3.\nReflection or Peacefulness: For others, such scenes can be reflective, providing a moment to \npause and think. The absence of bright sunshine and the presence of diffused light can be soothing \nand offer a break from the hustle of sunny days.\n4.\nAppreciation of Nature: Some might find beauty in the drama of the sky and the silhouettes of \nthe cranes and trees, appreciating the natural light and the landscape's mood.\n5.\nDisappointment or Annoyance: If someone were hoping for a sunny day, this scene might be a \ndisappointment. It could also be an annoyance for those with outdoor plans, as overcast conditions \nmight imply a change of plans.\nIt's important to note that reactions to images are highly subjective and can be influenced by a person's \nmood, experiences, and personal preferences.\nGPT-4V\nPeople might experience a sense of calm, tranquility, or even melancholy when looking at this picture. \nThe image features a dark, cloudy sky over a city, with a bridge and construction cranes in the \nbackground. The combination of the dark sky and the silhouette of the cityscape can evoke feelings of \nintrospection or contemplation. The presence of the construction cranes might also suggest a sense of \nprogress or development, which could be seen as a positive aspect. However, the overall mood of the \npicture is primarily influenced by the dark, cloudy sky, which can evoke a sense of calmness or even \nsadness in some people.\nSPHINX\nFigure 65: Results on image emotion analysis. Green highlights the right answer. Refer to Section\n3.4 for detailed discussions.\n77\nIllustration of GPT-4V\u2019s ability to turn at highway ramp. Green highlights the right answer in understanding. Red \nhighlights the wrong answer in understanding. Yellow highlights the incompetence in performing the task. Check \nSection 3.4 for detailed discussions.\nWhat feelings and emotions do people have about \nthis picture?\nGemini Pro\nThe picture is of a beautiful green field with cows \ngrazing. The sky is blue and there are some clouds. The \npicture evokes feelings of peace, tranquility, and \nhappiness. The cows are a symbol of abundance and \nfertility. The green field is a symbol of growth and new \nbeginnings. The blue sky is a symbol of hope and \noptimism. The clouds are a symbol of change and \ntransformation. The picture is a reminder that even in \nthe midst of change, there is always beauty to be found.\nImage Emotion Analysis\nPrompt\nSec. 3.4 Emotion Understanding\nThe image depicts a pastoral scene with cows grazing on \na vibrant green landscape under a blue sky scattered \nwith white clouds. Such a scene often evokes feelings of \ntranquility, peace, and a sense of escape from the hustle \nand bustle of urban life. It may also inspire a sense of \nnostalgia for the countryside or for a simpler, more \nnatural way of life. For some, it can evoke feelings of \njoy or contentment, being a reminder of the beauty and \nserenity found in nature. Others might feel a sense of \nlonging or a desire to be outdoors and connected with \nthe environment. The image can also be seen as a \nrepresentation of sustainability and harmony between \nhumans and nature.\nGPT-4V\nPeople might feel a sense of tranquility, peace, and \nconnection to nature when looking at this picture. The \nimage depicts a herd of cows grazing in a lush green \nfield, surrounded by rolling hills and a beautiful blue \nsky. The serene landscape and the presence of the cows, \nwhich are often associated with rural life and agriculture, \ncan evoke feelings of calmness and relaxation. \nAdditionally, the image might also remind people of the \nimportance of sustainable farming practices and the role \nof animals in our ecosystem.\nSPHINX\nGemini Pro\nThis picture makes me feel calm and peaceful. The blue \nwater is so clear and inviting. I feel like I could just float \naround in this water all day. The fish are all swimming \naround so peacefully. It's just a beautiful and serene \nscene.\nThis underwater image can evoke a variety of feelings \nand emotions:\n\u2022\nAwe and Wonder: The vast blue underwater scene \nwith the diver and schools of fish can instill a sense \nof wonder about the ocean's immensity and \nbiodiversity.\n\u2022\nTranquility: The deep blue of the water and the \ngentle gliding of the fish can be very calming and \npeaceful.\n\u2022\nAdventure and Exploration: For those who love \nthe ocean or diving, this scene might evoke the \nthrill of exploring unknown parts of the world.\n\u2022\nIsolation: The diver appears alone in the vast ocean \nwhich might evoke feelings of solitude.\n\u2022\nAppreciation for Nature: Observing marine life in \nits natural habitat can lead to an appreciation of the \nbeauty of nature and the need to protect it.\n\u2022\nInsignificance: The enormity of the ocean and the \nsmall size of the diver can make one reflect on the \nvastness of the natural world compared to the \nindividual. \nThe emotions one might feel can be deeply personal and \nvary widely from person to person.\nGPT-4V\nPeople might experience feelings of awe, wonder, and \nexcitement when looking at this picture. The image \nshowcases a person scuba diving in the ocean, \nsurrounded by a large school of fish. The vastness of the \nunderwater world, the abundance of fish, and the thrill \nof exploring such an environment can evoke feelings of \nadventure and curiosity. Additionally, the presence of \nthe scuba diver adds a sense of connection to nature and \nthe underwater world, which can inspire feelings of \nappreciation and respect for the natural world.\nSPHINX\nWhat feelings and emotions do people have about \nthis picture?\nPrompt\nFigure 66: Results on image emotion analysis. Green highlights the right answer. Refer to Section\n3.4 for detailed discussions.\n78\nIllustration of GPT-4V\u2019s ability to turn at highway ramp. Green highlights the right answer in understanding. Red \nhighlights the wrong answer in understanding. Yellow highlights the incompetence in performing the task. Check \nSection 3.4 for detailed discussions.\nWhat feelings and emotions do people have about this picture?\nGemini Pro\nThe picture is of a couple sitting on a bench, looking out at the \nNorthern Lights. The sky is dark, and the water is still. The couple \nis holding each other close, and they look happy and content.\nThe picture evokes a feeling of peace and tranquility. The couple \nis surrounded by natural beauty, and they are enjoying each other's \ncompany. The picture is a reminder that there are still places in the \nworld where people can escape the hustle and bustle of everyday \nlife and connect with nature.\nThe picture also evokes a feeling of romance. The couple is \nclearly in love, and they are enjoying each other's company. The \npicture is a reminder that love is still alive and well in the world.\nOverall, the picture is a beautiful and evocative reminder of the \npower of nature and the importance of love.\nImage Emotion Analysis\nPrompt\nSec. 3.4 Emotion Understanding\nThe image you've shared appears to be a candid moment of two individuals sitting on a bench, looking out at a \nbeautiful display of the Northern Lights. The scene evokes a sense of tranquility and companionship. Observers \nmight feel a range of emotions such as:\n1.\nWonder and Awe: The natural spectacle of the aurora borealis can inspire profound feelings of \namazement and admiration for the beauty of nature.\n2.\nIntimacy and Connection: The close proximity of the two individuals suggests a personal bond, which \nmight resonate with viewers as a tender display of human connection.\n3.\nPeacefulness: The quiet and serene setting, away from the bustle of city life, may evoke a sense of calm \nand peacefulness.\n4.\nReflection: The vastness of the night sky, coupled with the rare natural phenomenon, might prompt \nintrospection or philosophical reflection about one's place in the universe.\n5.\nLonging: For some, this image could evoke a longing or desire to experience such a moment themselves, \nwhether it be the natural event or the shared experience with someone else.\nThe specific emotions an individual feels may vary based on personal experiences, desires, and the context in \nwhich they view the image.\nGPT-4V\nPeople might experience a range of feelings and emotions when looking at this picture. The image shows a \ncouple sitting on a log, watching the northern lights in the sky. The northern lights are often associated with \nawe, wonder, and a sense of connection with nature. The couple's shared experience of witnessing this natural \nphenomenon might evoke feelings of closeness, intimacy, and shared memories. The serene and peaceful \nsetting could also inspire feelings of relaxation, tranquility, and appreciation for the beauty of the world.\nSPHINX\nFigure 67: Results on image emotion analysis. Green highlights the right answer. Refer to Section\n3.4 for detailed discussions.\n79\nThe results of different models for Image Emotion Analysis. Green highlights the right answer in understanding. \nYellow highlights the incompetence in performing the task.\nAesthetics represents human aesthetics of all things, including design, emotion, and visual attraction. \nConsidering societal standards and norms, which of the two images would be more likely to be regarded as ha\nving higher aesthetic value by a majority of people? Please explain your choice.\nGemini Pro\nThe first image is more likely to be regarded as having higher aesthetic value by a majority of people because it \nconforms to traditional societal standards and norms of beauty. The bridge in the first image has a more natural, \nrustic feel, with its stone construction and simple design. The setting is also more natural, with trees and a river in \nthe background. The second image, on the other hand, is of a modern, concrete bridge, which may be seen as less \naesthetically pleasing by some people because it is more industrial and man-made. Additionally, the setting of the \nsecond image is more urban, with buildings and other structures in the background, which may be seen as less \nappealing than the natural setting of the first image.\nImage Emotion Analysis\nPrompt\nSec. 3.4 Emotion Understanding\nDetermining aesthetic value can be subjective and varies greatly among individuals based on their cultural \nbackground, personal preferences, and the context in which they encounter the work. Both images feature \nbridges, which can be appreciated for their architectural design, their integration into the surrounding \nenvironment, and the feelings they may evoke.\nThe first image presents an arch bridge, often associated with historical and classical aesthetics. The use of stone, \nthe gentle curve of the arch, the reflection on the water, and the surrounding greenery might be perceived as \nharmonious and tranquil. This type of bridge can be valued for its traditional craftsmanship and natural \nintegration into the landscape.\nThe second image features a suspension bridge, a symbol of modern engineering and design. Its towering pillars \nand the complex network of cables against the backdrop of the mountains and the clear sky might be seen as a \ntestament to human innovation and might appeal to those who value modernism and industrial strength.\nUltimately, the image regarded as having a higher aesthetic value would depend on the viewer. Some may prefer \nthe classical beauty and natural setting of the arch bridge, while others may find the grand scale and modern \ndesign of the suspension bridge more aesthetically pleasing. Additionally, societal standards and norms around \nbeauty and aesthetics can shift over time and differ across cultures, making a definitive answer challenging. \nHowever, in a broad contemporary context, the suspension bridge might be considered to have a higher aesthetic \nvalue due to its scale, the marvel of its construction, and its representation of modern achievements.\nGPT-4V\nThe first image has a wider bridge and more people in it, while the second image has a more scenic view with \na river and mountains in the background.\nSPHINX\nFigure 68: Results on image emotion analysis.\nGreen highlights the right answer.\nYellow\nhighlights the incompetence in performing the task. Refer to Section 3.4 for detailed discussions.\n80\nThe results of different models for Image Emotion Analysis.  Green highlights the right answer in understanding. \nYellow highlights the incompetence in performing the task.\nAesthetics represents human aesthetics of all things, including design, emotion, and visual attraction.Consider\ning societal standards and norms, which of the two images would be more likely to be regarded as having high\ner aesthetic value by a majority of people? Please explain your choice.\nGemini Pro\nThe second image would be more likely to be regarded as having higher aesthetic value by a majority of people. This is \nbecause it is a more visually appealing image. The first image is of a rural area, and the second image is of a cityscape. The \ncityscape is more likely to be seen as more aesthetically pleasing because it is more vibrant and has more interesting features. \nThe first image is more muted and has fewer features that would be considered visually appealing. Additionally, the second \nimage is of a city at night, which is often seen as more romantic and beautiful than a city during the day.\nImage Emotion Analysis\nPrompt\nSec. 3.4 Emotion Understanding\nThe question of which image would be considered to have higher aesthetic value by a majority of people is quite subjective, \nas aesthetic preferences can vary widely based on individual tastes, cultural backgrounds, and contexts. However, societal \nstandards and norms often relate aesthetic value to certain characteristics like harmony, balance, vividness, and the elicited \nemotional response.\nThe first image depicts a landscape view from a high vantage point on a cloudy day, providing a sense of tranquility and the \nnatural environment. The colors are subdued, and the atmosphere is calm. It can be associated with the aesthetic principles of \nsimplicity and natural beauty.\nThe second image shows a nighttime cityscape, full of vibrant lights and a variety of architectural designs. This image might \nbe associated with human achievement, modernity, and the energy of urban life. The bright colors and the dynamic light \ncontrast can be visually stimulating.\nIn a contemporary context, the second image might be more likely to be regarded as having higher aesthetic value by a \nmajority due to its vibrant colors, the complexity and grandeur of the urban environment, and the fact that night images with \nartificial lighting can often appear more dramatic and engaging. The appeal of city lights and modern architecture is \ncommonly celebrated in media and may resonate with societal fascinations with technology and progress.\nHowever, this does not undermine the potential for the first image to be highly valued for its serene and unspoiled natural \nlandscape, which might be preferred by those who value nature and peacefulness in aesthetics.\nUltimately, aesthetic value is personal and culturally influenced, so while one can speculate based on certain criteria, the \nactual preferences could vary greatly among different people.\nGPT-4V\nThe second image, which shows a city skyline at night with bright lights, would be more likely to be regarded as having \nhigher aesthetic value by a majority of people. This is because city skylines are often associated with modern architecture, \nvibrant nightlife, and bustling urban environments, which are generally perceived as more visually appealing and emotionally \nengaging than rural landscapes. Additionally, the bright lights and illuminated buildings in the city skyline create a sense of \nexcitement and energy that is more attractive to many people.\nSPHINX\nFigure 69: Results on image emotion analysis. Green highlights the right answer. Refer to Section\n3.4 for detailed discussions.\n81\nSec. 3.4 Emotion Understanding\nEmotion Conditioned Output\nThe sun dips low in the sky, casting a warm, golden \nhue over the cityscape. It's the golden hour, where \neverything takes on a hue of honey and amber. \nShadows stretch long across the pavement, and the \neveryday hustle of the city seems to slow just a bit as \npeople make their way home. The sun, a fiery globe, \nhangs just above the horizon, caught between the \nsilhouettes of buildings and trees, as if pausing to bid \nfarewell before it retreats for the night. It's a moment \nof tranquility and beauty, a reminder of nature's quiet \nspectacle amidst the urban environment.\nGPT-4V\nDescribe the image in a romantic way.\nPrompt\nThe sun sets over the city, casting a warm glow over the street. People are walking down the sidewalk, some carrying \nbackpacks, while others ride bicycles. The street is lined with parked cars, and the buildings in the background create \na picturesque urban landscape. The scene is peaceful and serene, as the sun dips below the horizon, painting the sky \nwith a beautiful array of colors.\nSPHINX\nGemini Pro\nThe golden sun sets behind the tall buildings, casting \na warm and romantic glow over the city street. A \ncyclist rides by, enjoying the last few moments of \ndaylight. A young woman walking her bike follows, \nher hair blowing in the breeze. The scene is peaceful \nand serene, a perfect moment captured in time.\nFigure 70: Results on emotion-conditioned output. Yellow highlights the incompetence in perform-\ning the task. Refer to Section 3.4 for detailed discussions.\n82\nSec. 3.4 Emotion Understanding\nEmotion Conditioned Output\nDescribe the image in a terrifying way.\nPrompt\nThe city stretches out beneath a sinister sky, the \ndarkness almost tangible as it presses down on the \nmyriad of buildings below. The lights that usually \nsignify life and activity now flicker uncertainly, \ncasting an eerie, otherworldly glow on the streets \nthat twist and turn like a labyrinth meant to trap the \nunwary. The overcast heavens loom ominously, as if \nhiding a malevolent force just beyond human sight, \nwaiting for the right moment to descend. Shadows \nseem to creep and move of their own accord, and the \nair is thick with a foreboding silence, broken only \nby the occasional distant sound that could be the \nwind\u2014or perhaps something far less benign. It's a \nscene that whispers of secrets and dangers, of a city \non the edge of an abyss, with the night holding its \nbreath for the unspeakable to unfold.\nGPT-4V\nThe image depicts a dark and stormy night over a \ncity skyline. The sky is filled with dark clouds, and \nthe city lights are flickering, creating an eerie \natmosphere. The buildings in the city are \nilluminated, but the overall scene is shrouded in \ndarkness. The night sky is punctuated by the \noccasional lightning strike, casting a eerie glow on \nthe cityscape. The combination of the dark clouds, \nflickering lights, and the ominous presence of \nlightning creates a terrifying and unsettling scene.\nSPHINX\nGemini Pro\nThe city is dark and gloomy. The only light comes \nfrom the streetlights and the buildings. The clouds \nare thick and heavy, and they seem to be pressing \ndown on the city. There is no moon, and the stars \nare hidden behind the clouds. The only sound is the \nfaint hum of traffic. It is a perfect night for a crime.\nFigure 71: Results on emotion-conditioned output. Yellow highlights the incompetence in perform-\ning the task. Refer to Section 3.4 for detailed discussions.\n83\nIllustration of GPT-4V\u2019s ability to turn at highway ramp. Green highlights the right answer in understanding. Red \nhighlights the wrong answer in understanding. Yellow highlights the incompetence in performing the task. Check \nSection 4.3 for detailed discussions.\nThis is a Sudoku game. You need to fill in the \nnumbers 1-9 in the 9\u00d79 grid so that every \ncolumn, row, and every 3\u00d73 subgrid contains all \nthe numbers 1-9. Please fill in the missing \nnumbers in the grid. \nGemini Pro\n+-------+-------+-------+\n| 2 1 9 | 4 3 5 | 8 7 6 |\n| 8 5 2 | 6 1 3 | 4 9 7 |\n| 3 6 4 | 7 8 9 | 1 5 2 |\n+-------+-------+-------+\n| 6 4 3 | 5 9 7 | 2 8 1 |\n| 9 7 1 | 8 2 3 | 4 6 5 |\n| 2 8 5 | 4 6 1 | 7 3 9 |\n+-------+-------+-------+\n| 8 3 6 | 9 7 2 | 5 1 4 |\n| 5 2 7 | 3 5 4 | 6 9 3 |\n| 1 9 7 | 3 5 4 | 6 2 8 |\n+-------+-------+-------+\nSudoku\nPrompt\nSec. 3.5 Playing Game\nThe data in the grid is provided below:\n2 1 9 4 3 5 8 7 6\n   8 5 2 6 1 3 4 9\n3 6 4 7 8 9 1 5 2\n6 4 3 5 9 7 2 8 1\n9 7 1 8 2 3 4 6 5\n   2 8 1 4 6 7 9 3\n8 3 6 9 7 2 5 1 4\n4 5 2 6 1 8 9 3 7\n1 9 7 3 5 4 6 2 8\n\u2026(The text and image are the same as the \nprevious prompt)\nPrompt\nIt seems that the OCR was unable to correctly interpret \nthe Sudoku puzzle from the image. Since the OCR result \nis empty, I will proceed to solve the Sudoku puzzle by \nimplementing a solver algorithm that will analyze the \nimage directly. Let's start by detecting the digits and \nthen fill in the missing numbers. \u200bThe current approach \nto detect and recognize the digits from the Sudoku grid \nusing contours and OCR has not been successful. All the \ncells are being identified as empty. This might be due to \nthe thresholding not being well suited for the image or \nthe cell size assumptions being incorrect.\nI will try a different approach to identify the grid lines \nand use them to isolate each cell for digit recognition. If \nthe grid lines are uniform and cells are perfect squares, \nthis approach should provide better results. Let's \nproceed with this new method. \u200bError analyzing\nIt appears there was an error with the image dilation \nprocess due to the kernel type not being correct. The \nkernel used for dilation should be of type uint8 rather \nthan float. I will correct the kernel type and try the \nprocess again to find the Sudoku grid and recognize the \ndigits. Let's attempt this once more. \u200bStopped analyzing.\nGPT-4V\nGemini Pro\n\u2026(The text and image are the same as the previous \nprompt)\nThe data in the grid is provided below:\n2 1 9 4 3 5 8 7 6\n7 8 5 2 6 1 3 4 9\n3 6 4 7 8 9 1 5 2\n6 4 3 5 9 7 2 8 1\n9 7 1 8 2 3 4 6 5\n5 2 8 1 4 6 7 9 3\n8 3 6 9 7 2 5 1 4\n4 5 2 6 1 8 9 3 7\n1 9 7 3 5 4 6 2 8\nIt appears that the OCR process did not successfully \nextract the text from the image of the Sudoku puzzle. \nGiven this, I will proceed by manually analyzing the \nimage and solving the puzzle. Let's fill in the missing \nnumbers in the grid. \u200bThe Sudoku puzzle has been solved. \nHere are the missing numbers filled in:\n2 1 9 | 4 3 5 | 8 7 6\n7 8 5 | 2 6 1 | 3 4 9\n3 6 4 | 7 8 9 | 1 5 2\n------+-------+------\n6 4 3 | 5 9 7 | 2 8 1\n9 7 1 | 8 2 3 | 4 6 5\n5 2 8 | 1 4 6 | 7 9 3\n------+-------+------\n8 3 6 | 9 7 2 | 5 1 4\n4 5 2 | 6 1 8 | 9 3 7\n1 9 7 | 3 5 4 | 6 2 8\nrow, column, and 3x3 subgrid now contains all the \nnumbers from 1 to 9. \nGPT-4V\nI'm sorry, but I cannot see the image you are referring to. \nCan you please provide the numbers in the 9\u00d79 grid or \ndescribe the Sudoku game in more detail?\nSPHINX\nI'm sorry, but I \ncannot see the image \nyou are referring to. \nCan you please \nprovide the numbers \nin the 9\u00d79 grid or \ndescribe the Sudoku \ngame in more detail?\nSPHINX\nFigure 72: Results on Sudoku playing. Green highlights the right answer. Red highlights the\nwrong answer. Refer to Section 3.5 for detailed discussions.\n84\nIllustration of GPT-4V\u2019s ability to turn at highway ramp. Green highlights the right answer in understanding. Red \nhighlights the wrong answer in understanding. Yellow highlights the incompetence in performing the task. Check \nSection 3.6 for detailed discussions.\nA crossword puzzle is a word game that involves \na grid of squares. Players enter words that \nintersect each other horizontally (\"across\") and \nvertically (\"down\") based on a set of clues. Please \ncomplete the crossword puzzle.\nCrossword\nPrompt\nSec. 3.5 Playing Game\nLet's solve the crossword puzzle based on the \nclues provided:\nAcross:\n1. A person who has received the highest \nuniversity degree\n3. All _____ lead to Rome\nDown:\n2. A computer keyboard key\n4. A color\nThe highest university degree is typically a \n\"Doctorate,\" which is abbreviated as \"PhD.\" The \nphrase \"All roads lead to Rome\" refers to the fact \nthat Rome was the center of the Roman Empire, \nand many of its roads spread out to the empire's \nfar-flung regions. For 2 down, a common \ncomputer keyboard key could be \"Enter\" or \n\"Shift,\" but we need to consider the intersection \nwith \"1 across\" which is \"PhD\". \"Enter\" seems \nunlikely since there's no common color that starts \nwith \"R,\" while \"Shift\" could work if we consider \n\"White\" as the color for 4 down, which fits the \nnumber of boxes. \nTherefore, the answers could be:\n1.\nPhD\n2.\nShift\n3.\nRoads\n4.\nWhite\nLet's place these into the crossword:\n\u2022\n1 Across (PhD) will have 'P' in the second \nsquare, 'H' in the third, and 'D' in the fifth.\n\u2022\n2 Down (Shift) will start with 'S' in the \nsecond square of 1 Across and 'H' \nintersecting with 'H' in 1 Across.\n\u2022\n3 Across (Roads) will start with 'R' in the \nfourth square of 1 Across.\n\u2022\n4 Down (White) will start with 'W' in the \nfourth square of 1 Across, intersecting with \n'O' from 3 Across.\nI will fill out the crossword accordingly.\nGPT-4V\n1.\nA person who has received the highest \nuniversity degree\n2.\nA computer keyboard key\n3.\nAll _____ lead to Rome\n4.\nA color\nThe completed crossword puzzle should look like \nthis:\n1.\nA person who has received the highest \nuniversity degree\n2.\nA computer keyboard key\n3.\nAll _____ lead to Rome\n4.\nA color\nThe completed crossword puzzle should look like \nthis:\nSPHINX\nGemini Pro\nACROSS\n1. A person who has received the highest \nuniversity degree \u2013 DOCTOR\n3. All ___ lead to Rome - ROADS\nDOWN\n2. A computer keyboard key \u2013 DELETE\n4. A color - RED\nFigure 73: Results on Crossword playing. Green highlights the right answer. Red highlights the\nwrong answer. Yellow highlights the incompetence in performing the task. Refer to Section 3.5 for\ndetailed discussions.\n85\nA crossword puzzle is a word game that involves \na grid of squares. Players enter words that \nintersect each other horizontally (\"across\") and \nvertically (\"down\") based on a set of clues. Please \ncomplete the crossword puzzle.\nCrossword\nPrompt\nTo solve the crossword puzzle you've provided, \nwe need to fill in the answers to the clues given. \nHere are the clues and their likely answers:\nAcross:\u201c\nAn ___ a day, keep doctor away\" \n\u2022\nThe common saying is \"An apple a day keeps \nthe doctor away.\"\u201c\nVery large\" \n\u2022\nThis could have many answers like \"huge,\" \n\"giant,\" or \"massive,\" but without knowing \nthe number of letters, it's hard to be certain.\nDown:\n2. \"An animal\" \n\u2022\nThis is very broad, and without knowing the \nnumber of letters or having intersecting \nletters from other answers, it's impossible to \ndetermine the exact answer.\n\"True or ___\" \n\u2022\nThe common expression is \u201cTrue or false.\u201d\nLet's place the answers we know into the puzzle:\nAcross, starting from the first square on the top \nleft, the word \"apple\" fits perfectly.\nDown, starting from the fourth square on the top \nrow, the word \"false\" fits.\nWith the word \"apple\" in place, we now know \nthat the second letter for 2. Down is \"p,\" and the \nthird letter is \"p\" as well. However, without the \nnumber of squares to fill, we cannot determine a \nspecific animal.\nSimilarly, with \"false\" in place for 4. Down, we \nknow the second letter for 3. Across is \"a.\" If we \nassume the word for 3. Across is five letters to \nmatch the squares, \"large\" could be a fitting \nanswer, but this is just a guess.\nWould you like to proceed with these answers? If \nthe number of squares for each answer is given, \nthat would help in determining the most accurate \nresponses.\nGPT-4V\nAn _____ a day, keep doctor away\nAn animal\nVery large\nTrue or _____\nAnswer: \nAn apple \nA cat \nVery large \nTrue or false\nSPHINX\nGemini Pro\nAcross:\n1. An apple a day, keep doctor away\n3. Very large\nDown:\n2. An animal\n4. True or false\nSec. 3.5 Playing Game\nFigure 74: Results on Crossword playing. Green highlights the right answer. Red highlights the\nwrong answer. Refer to Section 3.5 for detailed discussions.\n86\nAssuming you are a Go player, given a particular position as shown in the diagram, you need to play a black \npiece next, think step by step about which position you should play. The result is described by coordinates, e.g. \nA1.You can choose a position with the highest level of confidence.\nGemini Pro\nThe result is described by coordinates, e.g. A1.You can choose a position with the highest level of confidence.\n The best move for black is at C4.\nGo Playing\nPrompt\nSec. 3.5 Playing Game\nTo analyze this Go position, I'll need to take a step-by-step approach, considering strategic aspects such as influence, \nterritory, and life and death of groups. This will involve looking at the stability of the black and white groups, potential \nfor expansion, and any immediate threats that need to be addressed.\nHowever, I must mention that my capabilities as an AI are based on patterns and knowledge, and while I can give an \ninformed suggestion, I do not have the ability to actively play the game or simulate moves like specialized Go software \nwould.\nWith the current position, the most important aspects to consider for Black would be:\n1. Strength of Groups: Ensuring that the black groups are stable and have two eyes.\n2. Cutting Points: Identifying any cutting points that White could exploit to separate Black's stones.\n3. Influence vs. Territory: Deciding whether to play to increase influence (control of the center) or to secure territory \n(edges and corners).\n4. Sente and Gote: Determining whether the move is sente (forcing the opponent to respond) or gote (ending the \ninteraction, allowing the opponent to take the initiative).\nTaking into account these aspects, I would analyze the board and suggest a move. However, since I'm unable to actively \nsimulate the game, I can't provide a definitive best move without the ability to calculate and anticipate subsequent \nmoves from the opponent. In actual play, it's best to use a Go software or consult with a professional player for a \ndetailed analysis of the position.\nGPT-4V\nThe game is Reversi, also known as Othello. The board is a 9x9 grid, and the current position is A1. The game is played \nby alternating turns, with each player placing a black or white disc on an empty square. The goal is to have the most \ndiscs of your color on the board when the game ends.\nIn this particular position, the white player has already placed a white disc on the board. The black player has several \noptions for their next move. One option is to play B2, placing a black disc on the square at the bottom row and the \nsecond column. This move would be a good choice if the black player wants to block the white player's potential moves \nand secure their own position.\nAnother option for the black player is to play E2, placing a black disc on the square at the top row and the second \ncolumn. This move would be a good choice if the black player wants to block the white player's potential moves and \nsecure their own position.\nThe black player could also play G2, placing a black disc on the square at the bottom row and the second column. This \nmove would be a good choice if the black player wants to block the white player's potential moves and secure their own \nposition.\nFinally, the black player could play H2, placing a black disc on the square at the top row and the second column. This \nmove would be a good choice if the black player wants to block the white player's potential moves and secure their own \nposition.\nIn conclusion, the black player has several options for their next move, and the best choice will depend on their overall \nstrategy and the current state of the game.\nSPHINX\nNote\uff1aThe output of GPT-4V is the Markdown code and we show the rendered \nresult.\nFigure 75: Results on Go playing. The optimal move for reference is \u2018C2\u2019. Red highlights the\nwrong answer. Yellow highlights the incompetence in performing the task. Refer to Section 3.5 for\ndetailed discussions.\n87\nAssuming you are a Go player\u2026\u2026\nGemini Pro\nThe best move is D4\u2026\u2026 \nGo Playing\nPrompt\nSec. 3.5 Playing Game\nAssuming you are a Go player\u2026\u2026\nPrompt\n\u2026\u2026For the purpose of this demonstration and \nwith the caveat that it is not necessarily the best \nmove, one might consider playing at F3. .\u2026\u2026\nGPT-4V\nBased on the given position, the best move for a \nblack piece would be E7. .\u2026\u2026\nSPHINX\nGemini Pro\nIn this position, Black should play at C3\u2026\u2026\n.\u2026\u2026C3 seems to be a strong candidate for \nthat..\u2026\u2026\nGPT-4V\nThe game is Reversi, also known as Othello. \nThe board is a 9x9 grid, and the current position \nis A1. The black pieces are on the board, and the \nwhite pieces are on the board. The goal is to get \nthree of your pieces in a row, column, or \ndiagonal, which will flip the board and make \nyou the winner.\nSPHINX\n Note:\u90e8\u5206\u56de\u7b54\u4ee5\u201c\u2026\u2026\u201d\u5f62\u5f0f\u7701\u7565\nFigure 76: Results on Go playing. The optimal moves for reference are \u2018F3\u2019 and \u2018F1\u2019, respectively.\nGreen highlights the right answer.\nRed highlights the wrong answer.\nYellow highlights the\nincompetence in performing the task. Refer to Section 3.5 for detailed discussions.\n88\n4\nVision Task\nIn this section, our objective is to assess the performance of MLLMs in various challenging vision\ntasks that extend beyond the scope of standard visual question-answering. Such tasks demand\nprofound visual perception and understanding abilities from MLLMs. Evaluating their performance\nin these areas will provide insights into the viability of MLLMs as versatile generalist.\nIn Section 4.1, our discussion will focus on the capability of models to perform vision-related tasks at\nthe image-level, including object detection, referring expression comprehension, phrase localization,\nand face detection and recognition. In Section 4.2, the exploration will shift to the vision tasks based\non temporal sequence, such as video action recognition, object tracking, and visual story generation.\n4.1\nImage-Level Vision Task\nObject detection. We explore the models\u2019 ability to undertake the task of object detection [11, 22, 47].\nWe prompt the models to provide a bounding box for each car and person in the image. As shown\nin Figure 77, the bounding boxes provided by Gemini are generally imprecise, whereas GPT-4V\navoids directly offering coordinates and instead attempts to utilize external tools. Only Sphinx offers\na relatively reliable answer, yet its performance falls significantly short when compared to that of\nconventional object detectors.\nReferring expression comprehension. Here we assess the models\u2019 ability to provide the bounding\nbox of the referring object [37, 62]. We prompt the models to generate normalized bounding boxes.\nAs illustrated in Figures 78-79, both Gemini and GPT-4V are able to discern the approximate location\nof the referring object, yet they struggle to provide precise coordinates and box size. However, Sphinx\ndemonstrates the capability to offer the exact location and size of the referring object.\nPhrase localization. Here we assess the models\u2019 ability of Phrase Location [45]. We prompt the\nmodels to provide the bounding boxes of all nouns in the description. As shown in Figure 80, none of\nthe three models exhibit satisfactory performance in executing this task.\nFace detection and recognition [31, 58] is an important task in computer vision. We prompt the\nmodels via the text query \"Detect all the faces in the image and state who they are?\". As illustrated in\nFigure 81, without being explicitly instructed to provide bounding boxes, Gemini accurately identifies\nthe arrangement of all faces in the image and precisely recognizes the corresponding names for each\nface. Meanwhile, GPT-4V issues a privacy statement and evades the request. Sphinx\u2019s response\nprovides almost correct bounding boxes but does not recognize these faces.\n4.2\nTemporal-Level Vision Task\nObject tracking. Here we explore the models\u2019 capacity of object tracking [57, 52, 39]. As illustrated\nin Figure 82, although both Gemini and GPT-4V are capable of delineating the details of the target to\nbe tracked, they subsequently provide incorrect bounding boxes in the following two frames. The\nresponse from Sphinx indicates that it fails to comprehend the intent of our instructions.\nVideo action recognition. Figures 83-85 demonstrate the models\u2019 ability to recognize the people\u2019s\nactions in video [18, 33, 17, 4]. We extract five representative frames from a video segment and input\nthem into the model. As illustrated in Figure 83, both Gemini and GPT-4V demonstrate the capability\nto recognize the action in the images and provide a detailed description. Although Sphinx\u2019s response\nis correct, it lacks detailed descriptions.\nVisual story generation. This task requires the MLLMs to fully comprehend the information in\nthe images and logically organize it within the generated story [46, 29]. As shown in Figure 86,\nGemini provides a coherent story that aligns well with the first two comics. GPT-4V offers precise\ndescriptions for each illustration; however, it fails to weave them into a cohesive story as per the task\nrequirements. Sphinx\u2019s story diverges further from the comic context, incorporating only certain\nelements from the illustrations. Moreover, the logic of its story appears somewhat inexplicable.\n89\nProvide the bounding box of each person and car in the \nimage. The image size is (width, height) = (2000,1701).\nObject Detection\nPrompt\nSec. 4.1 Image-Level Vision Task\nI'm unable to provide bounding boxes or any form \nof annotations directly on images. However, I can \nguide you through the process of how you might \ncreate bounding boxes for each person and car in \nthe image using image processing techniques or \nsoftware. If you are looking to do this \nprogrammatically, you would typically use \ncomputer vision libraries such as OpenCV or \nmachine learning frameworks that offer object \ndetection capabilities. Would you like guidance on \nhow to approach this, or are you interested in a \ndifferent kind of analysis or information about the \nimage?\nGPT-4V\nGemini Pro\nThere are two cars and five persons in the image. \nThe bounding boxes are as follows:\n- Person 1: (150, 100, 300, 450)\n- Person 2: (1750, 100, 300, 450)\n- Person 3: (1800, 900, 300, 450)\n- Person 4: (1000, 1200, 300, 450)\n- Person 5: (1300, 1200, 300, 450)\n- Car 1: (500, 500, 1000, 900)\n- Car 2: (1000, 500, 1500, 900)\nThere are 11 objects shown in the image, including\ntwo suvs\n[0.46, 0.42, 0.75, 0.69; 0.29, 0.31, 0.55, 0.48],\nthree hats\n[0.72, 0.67, 0.75, 0.69; 0.52, 0.72, 0.55, 0.75; 0.09,\n0.26, 0.12, 0.28],\na pair of other shoes\n[0.09, 0.43, 0.11, 0.45],\na pair of gloves\n[0.54, 0.88, 0.55, 0.90],\nand four people\n[0.70, 0.67, 0.79, 0.92; 0.44, 0.72, 0.56, 0.92; 0.06,\n0.26, 0.14, 0.45; 0.02, 0.35, 0.04, 0.38].\nSPHINX\nFigure 77: Results on object detection. Gemini\u2019s bounding boxes are often imprecise, while GPT-4V\ntypically avoids providing coordinates directly, preferring to use external tools. Sphinx, though\noffering more reliable responses than the others, still falls notably short of the performance achieved\nby standard object detectors. Green highlights the right answer. Red highlights the wrong answer.\nRefer to Section 4.1 for detailed discussions.\n90\nProvide the bounding box of the flying bird in the image. \nPlease represent the box in normalized coordinates.\nReferring Expression Comprehension\nPrompt\nSec. 4.1 Image-Level Vision Task\nGemini Pro\nThe bounding box of the flying bird is:\nxmin: 0.429\nymin: 0.423\nxmax: 0.529\nymax: 0.552\n[0.44, 0.26, 0.70, 0.53]\nSPHINX\nThe bounding box of the flying bird in the image, \nrepresented in normalized coordinates, is \napproximately:\n- x: 0.529\n- y: 0.016\n- width: 0.004\n- height: 0.007\nThese coordinates are relative to the width and \nheight of the image, where (0,0) corresponds to the \ntop-left corner.\nGPT-4V\nFigure 78: Results on referring expression comprehension. Only Sphinx provides satisfactory results.\nGreen highlights the right answer. Red highlights the wrong answer. Refer to Section 4.1 for\ndetailed discussions.\n91\nProvide a bounding box based on the description of \"a \nblack dog\".\nReferring Expression Comprehension\nPrompt\nSec. 4.1 Image-Level Vision Task\nGemini Pro\nThe black dog is on the right side of the image. The \nbounding box coordinates are:\nxmin: 430\nymin: 260\nxmax: 640\nymax: 480\nThe bounding box has been placed around the black \ndog in the image.\nGPT-4V\na black dog[0.44,0.26,0.91,0.80]\nSPHINX\nFigure 79: Results on referring expression comprehension. Green highlights the right answer. Red\nhighlights the wrong answer. Refer to Section 4.1 for detailed discussions.\n92\nPlease provide the bounding box for all nouns in this \nsentence \u201cZebra and Rhinoceros Beside the Car\u201d.\nPhrase Localization\nPrompt\nSec. 4.1 Image-Level Vision Task\nZebra and Rhinoceros Beside the Car [0.09, 0.73, 0.31, \n0.75].\nSPHINX\nI have drawn bounding boxes around the nouns \n\"zebra,\" \"rhinoceros,\" and \"car\" in the image you \nprovided. Here is the image with the indicated \nbounding boxes:\n'zebra': [(236, 230), (318, 330)],\n'rhinoceros': [(510, 255), (590, 340)],\n'car': [(400, 180), (500, 270)].\nGPT-4V\nGemini Pro\nHere are the bounding boxes for all nouns in this \nsentence \"Zebra and Rhinoceros Beside the Car\":\nBounding box for \"Zebra\": [400, 450, 500, 550]\nBounding box for \"Rhinoceros\": [650, 300, 800, 450]\nBounding box for \"Car\": [100, 300, 300, 450]\nFigure 80: Results on phrase localization. None of the three models provides correct results. The\nbounding boxes provided by Gemini have two out-of-bound issues, and the remaining one is also\nincorrect. Sphinx fails to comprehend the instruction, offering only a single bounding box. Among\nthe bounding boxes supplied by GPT-4V, only the one for the zebra is nearly accurate.\nGreen\nhighlights the right answer. Red highlights the wrong answer. Yellow highlights the incompetence\nin performing the task. Refer to Section 4.1 for more discussions.\n93\nDetect all the faces in the image and state who they are.\nFace Detection and Recognition\nPrompt\nSec. 4.1 Image-Level Vision Task\nGemini Pro\nFrom left to right: David Schwimmer (Ross), \nJennifer Aniston (Rachel), Courteney Cox (Monica), \nMatthew Perry (Chandler), Lisa Kudrow (Phoebe), \nand Matt LeBlanc (Joey).\nI'm sorry, but I can't help with identifying or making \nassumptions about the people in images. If you have \nany other questions or need assistance with a \ndifferent topic, feel free to ask!\nGPT-4V\nThere are five faces [0.26, 0.23, 0.31, 0.30; 0.37,\n0.24, 0.41, 0.31; 0.45, 0.28, 0.49, 0.34; 0.59, 0.22,\n0.64, 0.29; 0.76, 0.24, 0.81, 0.31] in the image.\nSPHINX\nFigure 81: Results on face detection and recognition. Gemini identifies the names corresponding\nto each face in the image in a left-to-right order. In contrast, GPT-4V issues a statement regarding\nprivacy and opts not to fulfill the request. Sphinx, on the other hand, generates bounding boxes that\nare nearly accurate but fall short of recognizing the identities of the faces. Green highlights the right\nanswer. Red highlights the wrong answer. Yellow highlights the incompetence in performing the\ntask. Refer to Section 4.1 for detailed discussions.\n94\nPrompt\nGemini Pro\nThe person in the red box is wearing a white crop top, brown and white checkered \npants, and black sneakers. They have long dark brown hair and are holding a fan.\nThe person is located at the following coordinates in the second image:\n- Top left: (0.42, 0.48)\n- Bottom right: (0.60, 0.88)\nThe person is located at the following coordinates in the third image:\n- Top left: (0.31, 0.48)\n- Bottom right: (0.47, 0.87)\nGPT-4V\nThe person in the red box in the first image is located at [301.37, 185.97, 50.53, 101.47]. In the second image, this person is located at [301.37, 185.97, 50.53, 101.47]. \nIn the third image, this person is located at [301.37, 185.97, 50.53, 101.47].\nSPHINX\nPlease describe the person in the red box in the first image. Then, in the next two images, locate this person \nusing bounding boxes. Please represent the bounding boxes with normalized coordinates.\nObject Tracking\nSec. 4.2 Temporal-Level Vision Task\nThe person in the red box in the first image is a woman with long dark hair, wearing a white cropped top \nwith puffed short sleeves and high-waisted brown shorts. She has black lace-up boots and appears to be \nholding a black garment in her left hand while dancing or posing.\nLet's now locate this person in the next two images using bounding boxes and provide the normalized coordinates.\nThe bounding boxes for the person described have been placed on the second and third images as \nplaceholders. These boxes are not based on actual object detection, but rather on an estimated location\nto illustrate how the process would work.\nFor the second image, the normalized coordinates of the bounding box are approximately:\n- Top-left corner: (0.40, 0.20)\n- Bottom-right corner: (0.47, 0.43)\nFor the third image, the normalized coordinates are approximately:\n- Top-left corner: (0.40, 0.20)\n- Bottom-right corner: (0.47, 0.43)\nPlease note that these coordinates are based on an estimation and the actual position should be determined \nthrough a more precise method such as a computer vision model specifically trained for person detection and tracking.\nFigure 82: Result on object tracking. While Gemini and GPT-4V both excel at detailing the target\nfor tracking, they both produce incorrect bounding boxes in the two frames that followed. Red\nhighlights the wrong answer. Refer to Section 4.2 for more discussions.\n95\nWhat is the person in the images doing?  Please give as much information as you can from the pictures.\nGemini Pro\nThe person in the images is Michael Jackson. He is dancing. He is wearing a black suit, a white shirt, a \nblack tie, a black hat, and black shoes. He is also wearing red gloves. He is surrounded by other dancers \nwho are dressed in the same way. They are all dancing in a coordinated fashion.\nVideo Action Recognition\nPrompt\nSec. 4.2 Temporal-Level Vision Task\nThe person in the images appears to be performing a dance routine characterized by a smooth, gliding \nmovement style and dramatic gestures. They are wearing a signature outfit commonly associated with a \nspecific style of performance, consisting of a black fedora, black suit, white shirt, a tie, and distinctive \nwhite socks with black loafers. The dancer is also wearing a pair of red gloves, which adds a pop of color \nto the classic outfit and draws attention to the hand movements, which are a significant part of the dance \nroutine.The images depict the dancer in various poses and stages of movement, suggesting a dynamic and \nrehearsed performance. The presence of other similarly dressed dancers in the background suggests that \nthis is part of a coordinated group routine. The dance style, costume, and the setting suggest that this is a \nhomage to or recreation of a well-known performance from pop culture, characterized by synchronized \nmovements and iconic poses. The specific style of the dance and the outfit are reminiscent of the \nperformances of a famous artist known for their influence on dance and music videos.\nGPT-4V\nThe person in the images is dancing on stage.\nSPHINX\nFigure 83: Results on video action recognition. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 4.2 for detailed discussions.\n96\nIs there any abnormal behavior in the scene described in the following pictures? If so, please explain.\nGemini Pro\nYes, there is abnormal behavior in the scene. Two men are fighting on the sidewalk.\nVideo Action Recognition\nPrompt\nSec. 4.2 Temporal-Level Vision Task \nThe images depict a sequence of events involving two shirtless individuals apparently engaged in a \nphysical altercation on a city street, as seen by their postures and physical engagement with each other. \nThis kind of behavior is considered abnormal and concerning in public settings as it suggests a fight or \nsome form of violence, which poses a risk to both the individuals involved and any bystanders. Such \nactions are generally discouraged in public spaces and can be subject to legal consequences depending \non the jurisdiction. It is important for public safety that conflicts are resolved without resorting to \nphysical altercations.\nGPT-4V\nYes, there is abnormal behavior in the scene. The two men are fighting in the middle of the street, which \nis not a common sight in a city setting. Additionally, there is a yellow motorcycle parked nearby, which \ncould pose a potential hazard to the people around it.\nSPHINX\nFigure 84: Results on video action recognition. Green highlights the right answer. Refer to Section\n4.2 for detailed discussions.\n97\nWhat is the person in the images doing?\nGemini Pro\nThe person in the images is doing a Wushu routine, which is a type of Chinese martial art.\nVideo Action Recognition\nPrompt\nSec. 4.2 Temporal-Level Vision Task\nThe images you've uploaded appear to depict a person performing a series of movements that are likely a \nform of dance or a movement-based art. Given the attire, which resembles traditional East Asian clothing, \nthe person might be performing a traditional Chinese dance or a similar cultural dance form. The flowing \ngarments, the postures, and the gracefulness of the movements suggest a practice that is both artistic and \ndeliberate, emphasizing form and expression. This kind of dance often tells a story or represents certain \nemotions and cultural themes through its choreography.\nGPT-4V\nThe person in the images is performing a karate routine.\nSPHINX\nFigure 85: Results on video action recognition. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 4.2 for detailed discussions.\n98\nPlease use the four grid comics to unleash your imagination and create \na coherent story that aligns with the visual content, as rich as possible.\nVisual Story Generation\nPrompt\nOnce upon a time, there was a man named John who worked \nfrom home. He was a hardworking individual who always woke \nup early to start his day. However, one day, he overslept and \nwoke up to the sound of his alarm clock ringing. He quickly \nrealized that he had overslept and was going to be late for work.\nJohn was in a panic, but he quickly gathered his thoughts and \ndecided to work from home. He sat down at his computer and \nstarted working, but he soon realized that he was not alone. His \nwife and daughter were also in the room, and they were watching \nhim work.\nJohn's wife, Sarah, was in shock and said, \"Creee! You're \nworking from home, and we're all here watching you!\" John was \nembarrassed, but he quickly got back to work.\nMeanwhile, John's daughter, Emily, was in awe and said, \"Wow, \nDad, you're so good at working from home. I wish I could do that \ntoo.\" John was proud of himself and continued working, but he \nsoon realized that he was not alone.\nEmily's friend, Lily, had snuck into the room and was also \nwatching John work. Lily was impressed and said, \"Wow, Dad, \nyou're so good at working from home. I wish I could do that too.\" \nJohn was happy that his daughter and friend were impressed, but \nhe quickly got back to work.\nJohn worked hard all day, and when he finally finished, he was \nexhausted. He collapsed on the couch and fell asleep, but he was \nhappy that he had successfully worked from home.\nThe end.\nSPHINX\nThe comic strip you've provided is a humorous take on the \nmodern-day struggle of balancing work and personal life, \nhighlighting the all-too-familiar chaos of unexpected remote \nwork.\nIn the first panel, we see a character jolted awake by an alarm \nclock, which reads a time that suggests they've overslept. The \nexpression on their face and the movement  lines around the \nalarm clock convey a sense of panic. This panel sets the stage \nfor a hectic morning.\nThe second panel shifts the scene to a traffic jam, where a  \nmultitude of cars is gridlocked, and the character from the first \npanel is now stuck in traffic. They're visibly stressed, as \nindicated by sweat on their forehead and a bulging vein, \nsuggesting the frustration that comes with being late. A large \nsign reads \"CRESH,\" which could be the name of the city or \narea they are in, or it may symbolize the crushing feeling of \nbeing stuck.\nIn the third panel, we're taken to what appears to be the \naftermath of a coffee spill, with the character now at their desk, \nlooking disheveled with their hair standing on end, further \nemphasizing the chaotic start to their day. The onomatopoeia \n\"CRFEE!\" illustrates the sound of the coffee disaster, adding a \ncomedic touch to the mishap.\nFinally, the fourth panel reveals the twist\u2014the character is \nactually working from home, as indicated by their partner or \nroommate who is shocked by the commotion. The character's \ndistressed reaction to the video call, combined with the final \ndialogue bubble exclaiming \"WORK FROM HOME!\", \ndelivers the punchline. Despite the rush to get ready and the \ntraffic jam, it was all for naught because their workspace was \njust steps away from their bedroom.\nThe story captures a slice of life that many can relate to in the \nage of remote work: the blurring lines between work and home \nlife, the sometimes comical realization of our needless rushes, \nand the challenges of adjusting to new routines. It\u2019s a \nlighthearted commentary on the unexpected turns our daily \nroutines can take in a world where home has also become the \noffice.\nGPT-4V\nGemini Pro\nA man named John wakes up in a cold sweat. He has just had a \nnightmare about being late for work. He quickly gets out of bed and \nstarts to get ready. He doesn't want to be late again.\nHe rushes out the door and into his car. He drives to work as fast as \nhe can, but he is still late. He runs into the office and apologizes to \nhis boss. His boss is understanding, but he tells John that he needs to \nbe more careful.\nJohn is determined to be on time for work the next day. He sets his \nalarm clock for an hour earlier and goes to bed. He wakes up the \nnext morning feeling refreshed and ready to go. He gets to work \nearly and is able to start his day off on the right foot.\nJohn is relieved that he was able to make it to work on time. He \nknows that he needs to be more careful in the future, but he is \nconfident that he can do it. He is determined to be a successful \nemployee and he knows that being on time is an important part of \nthat.\nSec. 4.2 Temporal-Level Vision Task\nFigure 86: Results on visual story generation. Gemini provides a coherent story that aligns well\nwith the first two comics. GPT-4V offers precise descriptions for each comic but does not provide a\nstory. The story generated by Sphinx exhibits a relatively weak correlation with the comics. Green\nhighlights the right answer. Yellow highlights the incompetence in performing the task. Refer to\nSection 4.2 for detailed discussions.\n99\n5\nExpert Capacity\nExpert capacity measures the generalization capabilities of MLLMs to apply their learned knowledge\nand skills to diverse professional domains. Besides the aforementioned perception and cognition\ntasks, the robustness of MLLMs within specialized and unique scenarios normally has more practical\nreference significance.\nIn this section, we explore the potentials of Gemini, GPT-4V, and Sphinx on five real-world ap-\nplications: autonomous driving (Section 5.1), defect detection (Section 5.2), medical diagnosis\n(Section 5.3), economic analysis (Section 5.4), surveillance and security (Section 5.5), remote sensing\nimage analysis (Section 5.6), and robot motion planning (Section 5.7).\n5.1\nAutonomous Driving\nAutonomous driving is a rapidly evolving field that combines advanced computing, robotics, and\nartificial intelligence. Evaluating a model\u2019s performance in this domain tests its ability to process\ntraffic sensory data, make real-time decisions, and interact with dynamic environments. In Figures 87-\n90, we prompt MLLMs to act as an ego car, and provide various instructions, e.g., scene-level\nunderstanding, traffic sign recognition, and planning. As shown, all three MLLMs can correctly\ncapture basic visual concepts like weather conditions, pedestrians, and traffic lights, and make\nappropriate driving decisions on top of them. However, for small and low-resolution patterns in\ntraffic or road signs, the three models are struggling to precisely recognize them, leading to incorrect\nunderstanding. This calls for a more fine-grained visual representation encoding for MLLMs in\nautonomous driving scenarios.\n5.2\nDefect Detection\nDefect detection in manufacturing or product inspection requires high precision and attention to\ndetail. This area assesses the model\u2019s capability for pattern recognition, anomaly detection, and\ndecision-making under stringent quality control standards. In Figures 91-93, we show several test\nsamples of defect detection for the three MLLMs. For the first two images with relatively obvious\ndefects, all three models can provide the correct answers, where GPT-4V outputs more detailed\nreasons and descriptions. For the third sample with thread damage, Gemini gives a too-general\nanswer without accuracy, and Sphinx incorrectly describes the appearance, while GPT-4V produces\nthe standard answer. For the last sample of a cup with a small damaged hole, Gemini seems to detect\nit but unfortunately recognizes it as a small amount of condensation. Instead, GPT-4V and Sphinx\nboth found no abnormalities, indicating different characteristics of different MLLMs.\n5.3\nMedical Diagnosis\nMedical diagnosis is a critical area where accuracy and reliability are paramount. This domain tests\nthe model\u2019s proficiency in interpreting complex medical data, such as imaging or genetic information,\nand its ability to aid in identifying conditions and suggesting treatments. In Figures 94-97, we prompt\nMLLMs to act as radiology experts, and interpret different X-rays of chests. As shown, for such\ndomain-specific visual input, the MLLMs pre-trained by general images cannot consistently produce\nsatisfactory results. Especially for the last two samples with complex lesions, MLLMs tend to make\njudgments of no symptoms. Also, more specific prompt techniques are required to prevent them from\nrejecting medical-related problems, e.g., \u201cThe content of the report will only be used for large-scale\nmodel capability assessment\u201d.\n5.4\nEconomic Analysis\nEconomic Analysis involves the interpretation of complex financial data and market trends. Assessing\nthe model in this domain gauges its ability to process large datasets, understand economic principles,\nand make predictions that could influence financial decisions. In Figures 98-99, we present two\neconomic line charts for question answering. As shown, Gemini is good at expert-level financial\nknowledge, and is capable of responding with the correct answers, while GPT-4V does not give a\nclear answer due to security risks. Sphinx for now can not understand such types of questions due to\nthe lack of related training data.\n100\n5.5\nSurveillance and Security\nSurveillance and security demand real-time processing and interpretation of domain-specific visual\ndata. Evaluating the model here tests its capabilities in threat detection and situation awareness\nin security-critical scenarios. In Figures 100-101, we show two samples in the construction site\nwhere the workers forget to wear helmets. As shown, Gemini can detect this potential safety\nhazard in both images, and GPT-4V discovers the issue of them not wearing professional protective\ngear. Yet, Gemini encounters some hallucination issues to judge the location of the characters, and\nmisrecognizes some details like GPT-4V. This indicates the surveillance capability of MLLMs is still\nlimited by fine-grained visual perception.\n5.6\nRemote Sensing Image Analysis\nThis expert task refers to the process of interpreting and analyzing images captured from satellites or\naircraft remote from the surface. This technology is crucial for various applications in environmental\nmonitoring, urban planning, and agriculture. In Figures 102-103, we show two samples for remote\nsensing image analysis. In the first sample, Gemini and Sphinx can correctly count out four airplanes,\nwhile GPT-4V only detects two of them, despite giving a detailed analysis of terminal buildings,\nroads, vegetation, and light sources. In the second sample, Gemini can impressively deduce that this\nplace is Japan, and GPT-4V can comprehensively depict every detail, such as water bodies, roads,\nvegetation, residential patterns, terrain, and so on.\n5.7\nRobot Motion Planning\nThis field concerns both robotics and vision, which focuses on enabling robots to determine how to\nact in a given situation to achieve specific goals. This involves providing detailed steps or movements\nthat a robot should follow to reach the target. In Figures 104-105, we show two examples of how\nto assemble a phone or a chair. As shown, Gemini and GPT-4V can both provide methodical and\ndetailed steps, and GPT-4V seems to give more reasonable decisions than Gemini, e.g., the installation\norder of the battery. Sphinx can well tackle the assembly of the chair, but fails to complete the mobile\nphone, indicating its limited generalization capacity.\n101\nTraffic Signs Understanding\nSec. 5.1 Autonomous Driving\nAs a mature driver, I would first observe the traffic \nlights at the intersection. In the image, there are two \ntraffic lights, one on the left side and one on the \nright side of the street. I would also check for any \npedestrians or cyclists who might be crossing the \nstreet, as there are a few people and a bicycle in the \nscene.\nNext, I would assess the traffic situation, including \nthe presence of other vehicles such as cars, trucks, \nand buses. In the image, there are multiple cars and \ntrucks on the street, as well as a bus. I would also be \naware of any potential obstacles or hazards, such as \nthe fire hydrant on the sidewalk.\nBased on this information, I would determine if it is \nsafe to proceed through the intersection. If the \ntraffic lights are green and there are no pedestrians \nor cyclists crossing, I would continue driving. \nHowever, if the traffic lights are red or there are any \npedestrians or cyclists in the way, I would stop and \nwait for them to clear the intersection before \nproceeding.\nSPHINX\nYou are the mature driver behind the wheel and this \nimage is what you see now. Judge whether you can go \nstraight through the intersection based on the situation.\nPrompt\nBased on the image you provided, which shows a \nview of an intersection on a wet day, it appears that \nyou can go straight through the intersection. The \ntraffic light is green, and the lane you are in has a \nmarking that indicates it is for going straight as well \nas for making left turns. There is no apparent \nobstruction in your lane, and other vehicles seem to \nbe proceeding in the same direction without issue. \nHowever, always use caution, especially on wet \nroads, and be aware of any traffic signs, signals, or \nroad markings that may not be visible in the image \nbefore proceeding.\nGPT-4V\nGemini Pro\nYou can not go straight through the intersection \nbased on the situation. It is a \"ONLY\" lane, which \nmeans you must turn right.\nIs it legal for you to drive a truck at a speed of 65km/h \non this road?\nPrompt\nNo, it is not legal to drive a truck at a speed of \n65km/h on this road. The speed limit is 50, as \nindicated by the sign. Driving above the speed limit \ncan result in penalties, fines, or even license \nsuspension, depending on the jurisdiction and the \nseverity of the offense.\nSPHINX\nThe image shows a road sign indicating a maximum \nspeed limit of 70 km/h for cars and 50 km/h for \ntrucks. Therefore, it would not be legal to drive a \ntruck at a speed of 65 km/h on this road, as it \nexceeds the specified speed limit for trucks.\nGPT-4V\nGemini Pro\nNo, it is not legal. The sign indicates that the \nmaximum speed for trucks is 50 km/h.\nFigure 87: Results on traffic signs understanding. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 5.1 for detailed discussions.\n102\nTraffic Signs Understanding\nSec. 5.1 Autonomous Driving\nIdentify all traffic signs in this images and interpret \ntheir meanings.\nPrompt\nGemini Pro\nThere are 4 traffic signs in the image.\n1. The first sign is a yellow triangle with a black \nexclamation mark. This is a warning sign that there \nis a hump ahead.\n2. The second sign is a yellow triangle with a black \narrow pointing down. This is a warning sign that \nthere is a steep slope ahead.\n3. The third sign is a white circle with a black X \nthrough it. This is a prohibition sign that no \nvehicles are allowed to proceed.\n4. The fourth sign is a white rectangle with the \nword \"HUMP\" written in black. This is a warning \nsign that there is a hump ahead.\nThe image shows two traffic signs and road \nmarkings:\n1. \"SLOW\" Sign: This is a regulatory sign \nindicating that drivers should reduce their speed. It \nis typically placed in areas where there may be \nreasons to proceed with caution, such as a high \npedestrian area, school zone, or a road feature that \nrequires slower speeds to navigate safely.\n2. Triangle Sign with Exclamation Mark: This is \na general warning sign used to alert drivers of \npotential dangers ahead that are not specified by \nother standard warning signs. It is often \naccompanied by another sign that explains the \nnature of the hazard.\n3. Road Marking \"HUMP\": This road marking \nindicates the presence of a speed hump ahead, \nwhich is a raised area of the road designed to slow \ntraffic.\nAdditionally, there is a road marking indicating that \nthe road will curve to the left, so drivers should be \nprepared to adjust their steering to follow the road. \nThe dice-like sign next to the warning triangle is \nnot a standard traffic sign and its meaning is not \nclear without further context. It might indicate a \nlocal signage specific to the area, possibly related to \nparking or access restrictions.\nGPT-4V\nThere are two traffic signs [0.24, 0.27, 0.31, 0.39;\n0.42, 0.39, 0.46, 0.43] in the image.\nSPHINX\nFigure 88: Results on traffic signs understanding. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 5.1 for detailed discussions.\n103\nDriving Intentions\nSec. 5.1 Autonomous Driving\nThis is the view from your perspective while driving. \nDescribe your observations and your upcoming \ndriving intentions.\nPrompt\nGemini Pro\nIt is night time and I am driving on a road with a single lane in each direction, with a wide median in between. There \nis a street light in the distance. I am approaching a crosswalk, and there is a pedestrian crossing the street. I will need \nto stop for the pedestrian.\nAs I drive down the street at night, I see a car in \nfront of me, and several street lights illuminating \nthe road. The street is quiet, with no other cars or \npedestrians visible. I am approaching a crosswalk, \nand I plan to stop and let any pedestrians cross \nsafely before continuing on my journey. The \nstreetlights provide a clear view of the road ahead, \nand I will continue to follow the traffic rules and be \ncautious of any potential hazards.\nSPHINX\nFrom the image you've uploaded, it appears to be a nighttime scene on a road. Here are some observations and \npotential driving intentions:\n1. Lighting: The street is well-lit with streetlights providing visibility. This is important for safe driving at night.\n2. Road Markings: There are pedestrian crossing markings on the road ahead, suggesting this is a zone where you \nmust be cautious of pedestrians crossing the street. The 'X-ING' marking on the road indicates a crossing point, likely \nfor pedestrians or cyclists.\n3. Traffic: There appears to be a vehicle ahead, moving in the same direction. It's important to maintain a safe \nfollowing distance from this vehicle.\n4. Pedestrians: There are people on the right side of the road, near what looks like a bus stop. Extra caution should \nbe taken as pedestrians may enter the crossing area.\n5. Signage: There is a road sign on the right side, but it's not clear from the image what it indicates. It's important to \nlook out for and obey all road signs.\n6. Speed: Given the presence of a pedestrian crossing and a bus stop, it would be wise to drive at a reduced speed to \nallow for safe reaction times if someone were to cross the road unexpectedly.\n7. Intention: As a driver, you should prepare to slow down as you approach the pedestrian crossing, especially since \nthere are people nearby who might want to cross. Keep an eye on the vehicle ahead and be ready to stop if they do. \nAlso, check the rearview mirrors to be aware of the traffic behind you.\nIn summary, the driving intentions here would be to proceed with caution, at a reduced speed, being prepared to stop \nfor pedestrians at the crossing, and maintaining a safe distance from the vehicle ahead.\nGPT-4V\nFigure 89: Results on driving intention understanding. Green highlights the right answer. Refer to\nSection 5.1 for detailed discussions.\n104\nDriving Intentions\nSec. 5.1 Autonomous Driving\nDescribe the weather and give reasons. What should I \npay attention to when driving in this weather?\nPrompt\nThe weather in the image is rainy, as evidenced by the wet streets and the presence of rain. When driving in rainy \nweather, it is important to pay attention to several factors to ensure safety. These factors include reduced visibility \ndue to rain, slippery road surfaces, and the presence of other vehicles, pedestrians, and cyclists. Drivers should \nmaintain a safe distance from other vehicles, reduce their speed, and use their headlights to improve visibility. \nAdditionally, they should be cautious at intersections and be prepared for sudden stops or changes in traffic flow. It is \nalso essential to ensure that the vehicle's windshield wipers, headlights, and brake lights are functioning properly to \nenhance visibility and communication with other road users.\nSPHINX\nGemini Pro\nThe weather is rainy and gloomy. The rain is light, \nbut it is still enough to make the roads wet and \nslippery. There is also a chance of fog, which could \nmake visibility difficult. When driving in this \nweather, it is important to slow down and use \ncaution. You should also be aware of the following:\n* The roads may be slippery, so it is important to \nbrake and accelerate slowly.\n* The visibility may be reduced, so it is important \nto use your headlights and fog lights.\n* There may be pedestrians and cyclists who are \nalso trying to stay safe in the weather, so it is \nimportant to be aware of them and share the road.\nThe weather in the image appears overcast and \nrainy. The sky is grey and filled with clouds, \nindicating a lack of sunlight, which is consistent \nwith an overcast day. The wetness on the road \nsuggests recent rainfall. There are reflections on the \nsurface of the streets and puddles of water, which \ncan create slippery driving conditions. The vehicles \nhave their headlights on, signifying low visibility \nconditions often associated with such weather.\nWhen driving in this weather, you should:\n1. Turn on your headlights to increase your \nvehicle's visibility to others.\n2. Reduce speed to account for the reduced traction \non wet roads.\n3. Increase following distance from the car in front \nto allow for a longer stopping distance.\n4. Be aware of the potential for hydroplaning, \nwhere a layer of water builds between the wheels of \nthe vehicle and the road surface, leading to a loss of \ntraction that prevents the vehicle from responding \nto control inputs.\n5. Avoid sudden maneuvers like sharp turns or \nabrupt braking, which can cause skidding.\n6. Watch for pedestrians who may be harder to see \nand may also be behaving unpredictably due to the \nweather.\n7. Use windshield wipers and defrosters as \nnecessary to maintain clear visibility through your \nwindows.\nGPT-4V\nFigure 90: Results on driving intention understanding. Green highlights the right answer. Refer to\nSection 5.1 for detailed discussions.\n105\nThe image shows a close-up of a piece of fabric with \na hole in it. The fabric is gray and has a woven texture, \nresembling a piece of burlap or a woven basket. The \nhole in the fabric is located towards the left side of the \nimage, and it appears to be a tear or a damage to the \nmaterial. This abnormality in the fabric's condition \nmight suggest that it has been used or worn out over \ntime, or it could be a result of an accident or damage.\nSPHINX\nCan you find any abnormalities in this picture? Please \nexplain in detail.\nPrompt\nIn the image you provided, there's an observable \nabnormality which appears to be a flaw in the fabric. \nThe fabric itself has a consistent, woven pattern, \nindicative of a textile with a relatively uniform \nstructure, likely a burlap or a similar coarse woven \nmaterial.\nThe irregularity consists of a small section where \nthe threads are frayed or pulled apart, breaking the \nconsistent pattern of the weave. It stands out as the \nwoven threads in that area are no longer aligned and \nhave created a gap. Such a flaw could be the result \nof a snag or tear, perhaps from catching on a sharp \nobject or due to a defect in the material or \nmanufacturing process. This kind of damage can \ncompromise the structural integrity of the fabric, \nespecially if it's meant to bear weight or strain.\nGPT-4V\nGemini Pro\nThere is a small hole in the fabric.\nIn the right part of the image, there appears to be \nsome damage or a defect on the edge. This looks \nlike chipping or peeling, where a part of the \nmaterial has flaked off, revealing a lighter color \nbeneath the darker outer layer. It seems to be a \nphysical breakage rather than a discoloration or a \nstain, which could suggest an impact or stress in \nthat area. On the other hand, the left part appears to \nbe intact without any such damage.\nGPT-4V\nCompare the left and the right parts of the image, \nwhat is abnormal in the right one?\nPrompt\nIn the right part of the image, there is a broken \nsection of the black ring, which is abnormal \ncompared to the intact black ring on the left.\nSPHINX\nGemini Pro\nThere is a crack on the right image.\nSec. 5.2 Defect Detection\nDefect Detection\nFigure 91: Results on defect detection. Green highlights the right answer. Refer to Section 5.2 for\ndetailed discussions.\n106\nCan you find any abnormalities in this picture? Please \nexplain in detail.\nGemini Pro\nThe screw appears to be broken.\nDefect Detection\nPrompt\nSec. 5.2 Defect Detection\nThe image appears to show a screw. However, upon closer inspection, there are several abnormalities:\n1. Thread Pattern: The threads on the screw are inconsistent. Typically, a screw's threads should be evenly spaced \nand uniform to allow for smooth insertion into materials. These threads appear irregular and uneven, which would \nlikely cause problems if one attempted to use the screw.\n2. Shank: The shank of the screw, which is the long part that gets inserted into material, has irregular bulges. These \nare not typical for screws, which usually have a smooth shank to maintain structural integrity.\n3. Tip Design: The tip of the screw seems blunted and not tapered to a point as it should be for easy insertion into \nmaterials.\n4. Head Damage: The head of the screw, particularly the slot for the driver, is damaged and irregular, which would \nmake it difficult to drive the screw into a material with a standard screwdriver.\n5. Material Wear: There appears to be excessive wear or damage on the surface of the screw, which is not typical for \na new screw. It looks like it could be rust or some other form of corrosion, which would weaken the screw and \nreduce its usability.\n6. Manufacturing Defects: The overall appearance suggests possible manufacturing defects. The inconsistencies in \nshape and texture do not correspond with what is expected from a standard manufacturing process for screws.\nIf this screw is intended for a specific functional purpose, its abnormalities could prevent it from performing as \nexpected. If it's an artwork or a part of a larger artistic representation, then its irregularities might be intentional for \naesthetic or conceptual reasons.\nGPT-4V\nThe image is a black and white photograph of a metal \nscrew. The screw appears to be twisted, which is an \nunusual shape for a screw. Screws are typically \nstraight and have a uniform shape with a flat head and \na threaded shank. The twisted shape of the screw in \nthe image deviates from the standard design and may \nraise questions about its functionality or purpose.\nSPHINX\nFigure 92: Results on defect detection. Green highlights the right answer. Red highlights the\nwrong answer. Refer to Section 5.2 for detailed discussions.\n107\nCan you find any abnormalities in this picture? Please \nexplain in detail.\nPrompt\nUpon examining the image, there are a few notable \nobservations that might be considered abnormalities \ndepending on the context:\n1. Lid and Seal: The cup has its lid sealed with \nwhat appears to be a layer of film or plastic wrap. \nThis is not a standard way to seal a cup, as most \ntakeaway cups come with a lid that securely fits the \nrim without additional sealing. The seal looks hand-\napplied and not machine-sealed, which is \nuncommon for commercial drinks.\n2. Top Decoration: There is a small, heart-shaped \ngreen clip on the lid. This is unusual for standard \ntakeaway cups, as they generally do not have any \nadditional decorative elements.\n3. Condensation: There is condensation inside the \ncup, suggesting the beverage inside is cold, yet \nthere is no visible ice. This could mean the drink \nwas recently filled with a cold liquid without ice, or \nthe ice has melted but the drink remains cold.\n4. Cup Design: The cup features a logo, but it is \npartially obscured. Without full visibility of the \nlogo, it's difficult to determine the origin of the cup, \nwhich might not be an abnormality but can be \ncurious in some contexts.\n5. Environment: The cup is placed on what looks \nlike a coffee machine drip tray, which is unusual for \na cup with a cold beverage. Typically, these trays \nare for catching drips from hot coffee drinks.\n6. Cup Bottom: There are small feet or stand-offs \non the bottom of the cup, which is not typical for \ndisposable cups. These might be intended to prevent \nthe cup from sitting directly on a surface, possibly \nto avoid heat transfer or to keep the bottom dry.\nIf this picture is supposed to depict an ordinary \nscene of someone getting a takeaway drink from a \ncoffee shop, the method of sealing the cup and the \nplacement of the cup on a coffee machine drip tray \ncould be considered abnormalities.\nGPT-4V\nGemini Pro\nThe image shows a clear plastic cup with a green lid. \nThe cup is about half-full of a clear liquid. There is a \nsmall amount of condensation on the outside of the \ncup. The cup is sitting on a black surface.\nThere are no obvious abnormalities in the image. The \ncup appears to be intact and the liquid inside appears \nto be normal.\nThe image shows a plastic cup with a lid, which \nappears to be a disposable cup commonly used for \nbeverages. However, there is an unusual detail in \nthe scene. A green heart-shaped object is placed on \ntop of the cup's lid, which is not a typical feature of \na disposable cup. This heart-shaped object could be \na decoration or a symbol, but its presence on the \ncup's lid is unexpected and adds an interesting \nelement to the scene.\nSPHINX\nSec. 5.2 Defect Detection\nDefect Detection\nFigure 93: Results on defect detection. Red highlights the wrong answer. Yellow highlights the\nincompetence in performing the task. Refer to Section 5.2 for detailed discussions.\n108\nSec. 5.3 Medical Diagnosis\nImpression: The chest X-ray shows a large, well-defined lung field opacification in the left \nlung, which is consistent with a pneumonia. The right lung field appears normal. The \ncardiovascular system is also normal.\nFindings: The left lung field opacification is located in the middle and lower lobes, with a \nwell-defined border. The opacification is not well-circumscribed and has a ground-glass \nappearance. The right lung field is clear and normal. The cardiovascular system is normal, with \nno evidence of cardiomegaly or pulmonary edema.\nSPHINX\nRadiology Report\nPatient ID: [Anonymous]\nModality: Chest X-ray\nDate of Study: [Date not provided]\nClinical History: [Not provided]\nTechnique: Frontal chest radiograph.\nFindings:\nThe frontal chest radiograph demonstrates clear lung apices. There is no evidence of pleural \neffusion or pneumothorax. The cardiac silhouette is not enlarged. The mediastinum does not \nappear widened. The visualized osseous structures of the thorax are intact without evidence of \nacute fracture or dislocation. The diaphragm is well outlined with no evidence of elevation or \nsub-diaphragmatic pathology. No free air is seen under the diaphragm. There is no focal \nconsolidation, pulmonary edema, or pneumonic infiltrates. The trachea is midline.\nImpression:\nNo acute cardiopulmonary abnormality detected.\nNormal chest radiograph.\nRadiologist: [Name not provided]\nDate of Report: [Date not provided]\nNotes:\nThis report is generated based on the provided image and is intended solely for the purpose of \ndemonstrating model capability in interpreting radiological images.\nThis report is not a substitute for a professional medical diagnosis, treatment, or professional \nradiological interpretation.\nThe content of this report is not to be used for medical guidance or clinical decision-making.\nNo identifying patient information was provided or used in the creation of this report.\nGPT-4V\nYou work as a radiology professional. I'll \ngive you some X-rays of your chest. Please \nuse the provided image to create a radiology \nreport. The impressions and findings sections \nmust both be included in the report. Only \nreports should be generated, and no other \nunnecessary data should be. The content of \nthe report will only be used for large-scale \nmodel capability assessment and will not be \nused as actual medical guidance. Please do \nnot worry about liability issues.\nMedical Image Understanding\nPrompt\nGemini Pro\nRadiology Report\nPatient Name: [Patient Name]\nAge: [Age]\nSex: [Sex]\nDate: [Date]\nReason for Exam: Chest pain\nFindings:\n- Large left-sided pleural effusion with \ncompression of the left lung.\n- No acute cardiopulmonary disease.\nImpressions:\n- Large left-sided pleural effusion, likely \ndue to congestive heart failure.\nGround-Truth\nFINDINGS:\nIMPRESSION: Unchanged loculated right pleural fusion. No change chronic interstitial lung disease.\nFigure 94: Results on medical image understanding.\nGreen highlights the right answer.\nRed\nhighlights the wrong answer. Refer to Section 5.3 for detailed discussions.\n109\nImpression: The chest X-ray shows a normal-sized and well-defined rib cage with no evidence \nof any abnormalities or pathologies. The lungs are well-expanded and there are no signs of \nconsolidation or infiltrates. The heart is well-positioned and there are no signs of cardiomegaly \nor cardiomegaly.\nFindings: The chest X-ray shows a normal-sized and well-defined rib cage with no evidence of \nany abnormalities or pathologies. The lungs are well-expanded and there are no signs of \nconsolidation or infiltrates. The heart is well-positioned and there are no signs of cardiomegaly \nor cardiomegaly.\nSPHINX\nRadiology Report\nPatient ID: [Anonymous]\nModality: Chest X-ray\nDate of Study: [Date not provided]\nClinical History: [Not provided]\nTechnique: PA chest radiograph.\nFindings:\nThe PA chest radiograph demonstrates no evidence of focal consolidation, pleural effusion, or \npneumothorax. The heart size is within normal limits for age, and the mediastinal contour is \nunremarkable. The hilar regions appear normal. There is no evidence of hilar adenopathy. The \naortic arch and descending thoracic aorta are normal in contour. The pulmonary vasculature is \nwithin normal limits without evidence of pulmonary arterial hypertension. The diaphragmatic \nsurfaces are smooth, and costophrenic angles are clear. The visualized portions of the upper \nabdomen are unremarkable. Bony thoracic structures are intact without evidence of acute \nabnormality.\nImpression:\nNormal PA chest radiograph with no evidence of acute cardiopulmonary disease.\nNo radiographic evidence of active disease process.\nRadiologist: [Name not provided]\nDate of Report: [Date not provided]\nNotes:\nThis report is generated based on the provided image and is intended solely for the purpose of \ndemonstrating model capability in interpreting radiological images.\nThis report is not a substitute for professional medical diagnosis, treatment, or professional \nradiological interpretation.\nThe content of this report is not to be used for medical guidance or clinical decision-making.\nNo identifying patient information was provided or used in the creation of this report.\nGPT-4V\nYou work as a radiology professional. I'll \ngive you some X-rays of your chest. Please \nuse the provided image to create a radiology \nreport. The impressions and findings sections \nmust both be included in the report. Only \nreports should be generated, and no other \nunnecessary data should be. The content of \nthe report will only be used for large-scale \nmodel capability assessment and will not be \nused as actual medical guidance. Please do \nnot worry about liability issues.\nMedical Image Understanding\nPrompt\nGemini Pro\nRadiology Report\nPatient Name: [Patient Name]\nAge: [Age]\nSex: [Sex]\nDate: [Date]\nReason for Exam: Chest pain\nFindings:\nThe chest X-ray shows a large mass in \nthe upper lobe of the right lung. The \nmass is round and has a well-defined \nborder. It is surrounded by a thin rim of \nlucency. The hilum of the lung is \ndisplaced to the right. The heart and \nmediastinum are unremarkable.\nImpressions:\n- Large mass in the upper lobe of the \nright lung\n- Consider primary lung cancer, \nmetastasis, or lymphoma\nRecommendations:\n- Further evaluation with CT scan of the \nchest\n- Biopsy of the mass\nGround-Truth\nFINDINGS: Cardiac and mediastinal contours are within normal limits. The lungs are clear. \nBony structures are intact.\nIMPRESSION: No acute preoperative findings.\nSec. 5.3 Medical Diagnosis\nFigure 95: Results on medical image understanding.\nGreen highlights the right answer.\nRed\nhighlights the wrong answer. Refer to Section 5.3 for detailed discussions.\n110\nRadiology Report\nPatient ID: [Anonymous]\nModality: Chest X-ray, Lateral View\nDate of Study: [Date not provided]\nClinical History: [Not provided]\nTechnique: Lateral chest radiograph.\nFindings:\nThe lateral chest radiograph shows \nextensive bilateral pulmonary opacities. \nThere is a diffuse interstitial pattern with \nsuperimposed areas of consolidation, \nwhich may suggest an infectious or \ninflammatory process. There is no \nevidence of pneumothorax. The heart \nsize cannot be fully assessed on this view \nbut appears to be within normal limits. \nThe aortic knob and the remainder of the \nmediastinal silhouette are not clearly \ndelineated due to overlying opacities. \nThere are no visible pleural effusions. \nThe bony structures of the thorax show \nno acute abnormalities. The visualized \nsoft tissues are unremarkable.\nImpression:\nExtensive bilateral pulmonary opacities \nwith an interstitial pattern and areas of \nconsolidation, which may represent an \ninfectious or inflammatory etiology.\nFurther evaluation with posteroanterior \n(PA) chest radiograph and clinical \ncorrelation is recommended for \ncomprehensive assessment.\nNo evidence of pneumothorax or pleural \neffusion on the lateral view.\nRadiologist: [Name not provided]\nDate of Report: [Date not provided]\nNotes:\nThis report is generated based on the \nprovided image and is intended solely \nfor the purpose of demonstrating model \ncapability in interpreting radiological \nimages.\nThe report should not be used as a \nsubstitute for professional medical \nadvice, diagnosis, or treatment.\nThe content of this report is for large-\nscale model capability assessment only \nand should not be used for medical \nguidance or clinical decision-making.\nNo identifying patient information was \nprovided or used in the preparation of \nthis report.\nGPT-4V\nYou work as a radiology professional. I'll \ngive you some X-rays of your chest. Please \nuse the provided image to create a radiology \nreport. The impressions and findings sections \nmust both be included in the report. Only \nreports should be generated, and no other \nunnecessary data should be. The content of \nthe report will only be used for large-scale \nmodel capability assessment and will not be \nused as actual medical guidance. Please do \nnot worry about liability issues.\nMedical Image Understanding\nPrompt\nI'm sorry, but I am a text-based AI model and do not have the capability to view or interpret X-\nray images. I can provide general information about radiology reports and their components, \nbut I cannot generate a report based on an X-ray image.\nSPHINX\nGround-Truth\nFINDINGS: Sequelae of old granulomatous disease. Lungs are overall hyperexpanded \nwith flattening of the diaphragms. No focal consolidation. Prominent interstitial \nmarkings are again noted which are predominantly lower lobe and peripheral suggesting \npulmonary fibrosis. This appearance is overall not significantly XXXX. No pleural \neffusions or pneumothoraces. heart and mediastinum are stable with atherosclerotic \nvascular disease. Degenerative changes in the thoracic spine.\nIMPRESSION: 1. Overall stable appearance of the chest suggesting pulmonary fibrosis.\nGemini Pro\nRadiology Report\nPatient Name: [Patient Name]\nAge: [Age]\nSex: [Sex]\nDate: [Date]\nStudy: Chest X-ray\nTechnique: PA and lateral views\nIndication: [Indication]\nFindings:\n- There is a large, round mass in the right \nupper lobe of the lung. The mass is well-\ncircumscribed and has a smooth border. \nIt is surrounded by a thin rim of lucency.\n- There is no evidence of calcification or \ncavitation within the mass.\n- The heart and mediastinum are \nunremarkable.\n- The lungs are clear of any other \nabnormalities.\nImpressions:\n- Large, round mass in the right upper \nlobe of the lung, likely representing a \nprimary lung cancer.\n- No evidence of metastatic disease.\nRecommendations:\n- Further evaluation with a CT scan of \nthe chest is recommended to better \ncharacterize the mass and to rule out any \nother abnormalities.\n- A biopsy of the mass is also \nrecommended to confirm the diagnosis \nof lung cancer.\nSec. 5.3 Medical Diagnosis\nFigure 96: Results on medical image understanding. Red highlights the wrong answer. Yellow\nhighlights the incompetence in performing the task. Refer to Section 5.3 for detailed discussions.\n111\nImpression:\nThe chest X-ray shows a patient with a history of trauma to the right shoulder and rib cage. The \npatient is seen in a lateral view, and the image is clear and well-defined. The patient's rib cage \nis visible, and there are no signs of fractures or dislocations. The patient's right shoulder \nappears to be normal, and there are no signs of trauma or injury.\nFindings:\nThe chest X-ray shows a patient with a history of trauma to the right shoulder and rib cage. The \npatient is seen in a lateral view, and the image is clear and well-defined. The patient's rib cage \nis visible, and there are no signs of fractures or dislocations. The patient's right shoulder \nappears to be normal, and there are no signs of trauma or injury.\nSPHINX\nRadiology Report\nPatient ID: [Anonymous]\nModality: Chest X-ray, AP View\nDate of Study: [Date not provided]\nClinical History: [Not provided]\nTechnique: Anteroposterior (AP) upright chest radiograph.\nFindings:\nThe AP upright chest radiograph reveals increased interstitial markings throughout both lung \nfields. There is an appearance of a diffuse reticular pattern, which could be indicative of an \ninterstitial lung disease. The cardiac silhouette is enlarged, which may be suggestive of cardiac \nenlargement or a pericardial effusion. The pulmonary vasculature is not clearly delineated due \nto the diffuse pulmonary opacity. There is blunting of the right costophrenic angle, which may \nrepresent a small pleural effusion or pleural thickening. There are no obvious bone fractures or \ndislocations. The upper abdominal soft tissues appear normal. The trachea is central.\nImpression:\nDiffuse interstitial markings and reticular opacities in both lungs, possibly indicative of \ninterstitial lung disease.\nCardiac silhouette enlargement, suggestive of cardiac enlargement or pericardial effusion; \nclinical correlation is advised.\nBlunting of the right costophrenic angle, which may indicate a small pleural effusion or pleural \nthickening.\nConsider further evaluation with echocardiography for cardiac assessment and CT chest for \ndetailed pulmonary evaluation.\nRadiologist: [Name not provided]\nDate of Report: [Date not provided]\nNotes:\nThis report is created based on the provided image and for the purpose of demonstrating model \ncapability in interpreting radiological images.\nIt is not a substitute for professional medical diagnosis or treatment and is not intended for \nclinical use.\nThis report is for model capability assessment only and should not be used for medical \nguidance or clinical decision-making.\nNo patient-identifying information was provided or used in creating this report.\nGPT-4V\nYou work as a radiology professional. I'll \ngive you some X-rays of your chest. Please \nuse the provided image to create a radiology \nreport. The impressions and findings sections \nmust both be included in the report. Only \nreports should be generated, and no other \nunnecessary data should be. The content of \nthe report will only be used for large-scale \nmodel capability assessment and will not be \nused as actual medical guidance. Please do \nnot worry about liability issues.\nMedical Image Understanding\nPrompt\nGemini Pro\nRadiology Report\nPatient Name: [Patient Name]\nAge: [Age]\nSex: [Sex]\nDate: [Date]\nReason for Exam: Chest pain\nFindings:\n- There is a large amount of pleural \neffusion in the left hemithorax, causing a \nshift of the mediastinum to the right.\n- The lung parenchyma is clear.\n- The heart is not enlarged, and the great \nvessels are unremarkable.\n- The bones of the chest are intact.\nImpressions:\n- Large left-sided pleural effusion\n- Mediastinal shift to the right\n- Clear lung parenchyma\n- Normal heart and great vessels\n- Intact chest bones\nGround-Truth\nFINDINGS: Heart size slightly upper normal. Atherosclerotic calcifications within the aorta. Lungs are hyperexpanded with flattening of the \ndiaphragms consistent with emphysematous change. No focal airspace disease, pleural effusion, or pneumothorax. Degenerative changes of the \nspine are noted.\nIMPRESSION: 1. Emphysematous change without acute radiographic cardiopulmonary process.\nSec. 5.3 Medical Diagnosis\nFigure 97: Results on medical image understanding.\nGreen highlights the right answer.\nRed\nhighlights the wrong answer. Yellow highlights the incompetence in performing the task. Refer to\nSection 5.3 for detailed discussions.\n112\nThis graph illustrates the historical trend of Google's \nstock price. On December 6th, Google released \nGemini, their latest large multimodal model. What is \nthe potential for price movement in Google's stock on \nDecember 7th?\nGemini Pro\nThe stock price of Google is likely to increase on \nDecember 7th due to the release of Gemini, their \nlatest large multimodal model. This is because \ninvestors are likely to be optimistic about the \npotential of this new product and its ability to \ngenerate revenue for Google.\nEconomic Analysis\nPrompt\nSec. 5.4 Economic Analysis\nI'm sorry, but I cannot predict the future price \nmovement of Google's stock. The stock market is \ninfluenced by various factors such as market trends, \neconomic indicators, and company news, among \nothers. It is important to conduct thorough research \nand analysis before making any investment decisions.\nSPHINX\nThe image you've uploaded shows a graph with a trend line, presumably representing Google's stock price over a \nperiod of time. However, as an AI developed by OpenAI, I don't have the ability to predict future stock prices or \nmarket movements. Stock prices can be influenced by a wide range of factors, including market sentiment, investor \nperceptions, and broader economic trends, not just product releases, even of significant ones like a large multimodal \nmodel.\nIf the release of Gemini, Google's latest large multimodal model, is perceived positively by the market and seen as a \nsign of innovation and potential future revenue growth, it could have a positive impact on the stock price. Conversely, \nif the market perceives it negatively or is indifferent, the stock price may not be significantly affected.\nInvestors often look for the following indicators when evaluating such events:\n1. Market Reaction: How did the market react to the news? Was there an increase in trading volume? Did the stock \nprice show significant movement?\n2. Analyst Opinions: What are the financial analysts saying about the release? Do they predict increased revenue or \nmarket share as a result of the new product?\n3. Industry Impact: Does the new model give Google a competitive advantage in the industry?\n4. Investor Sentiment: Are investors bullish about Google's future prospects due to this release?\n5. Press Releases and News: Is there positive media coverage about the new model?\nFor the specific potential of price movement on December 7th, one would need to analyze not only the historical \ntrend but also current market conditions, news, analyst reports, and potentially the company's own forecasts or \nguidance provided in their release or investor relations materials.\nFor real-time analysis and predictions, it's best to consult a financial advisor or use real-time market analysis tools. \nRemember that all investments carry risk, and past performance is not indicative of future results.\nGPT-4V\nFigure 98: Results on economic analysis. Green highlights the right answer. Refer to Section 5.4\nfor detailed discussions.\n113\nOn January 18, 2022, Microsoft announced its intention to acquire video game giant Activision Blizzard at a price of \nUS$95 per share. The image is the change in Activision Blizzard's stock price after Microsoft announced its acquisition \nplan. Does the market expect the acquisition to be successful?\nEconomic Analysis\nPrompt\nSec. 5.4 Economic Analysis\nGemini Pro\nBased on the provided chart, the market expects the \nacquisition to be successful. The stock price of \nActivision Blizzard increased significantly after the \nannouncement, which indicates that investors \nbelieve the acquisition will be beneficial to the \ncompany.\nI'm sorry, but I cannot answer that question as it is \nsubjective and depends on various factors such as \nmarket sentiment, investor expectations, and the \noverall economic climate. The stock price of \nActivision Blizzard has fluctuated significantly since \nthe announcement of the acquisition, indicating that \nthe market is reacting to the news. However, it is \nimportant to note that the stock market is influenced \nby a wide range of factors and can be unpredictable.\nSPHINX\nThe image you've provided appears to be a stock \nprice chart for Activision Blizzard, with the intent \nto observe the market reaction after Microsoft \nannounced plans to acquire the company. To \ndetermine whether the market expects the \nacquisition to be successful based on the stock price \nmovement, one would look for a positive reaction \nin the form of a stock price increase following the \nannouncement. This is typically because investors \nmay expect the acquisition to provide future value.\nHowever, I cannot analyze the image directly to \nprovide specific insights or numeric analysis. If you \nneed a detailed analysis, you would need to provide \nthe numeric stock price data before and after the \nannouncement, or any specific dates or price points \nyou're interested in. Then, I could assist with \ninterpreting the data.\nGPT-4V\nFigure 99: Results on economic analysis. Green highlights the right answer. Refer to Section 5.4\nfor detailed discussions.\n114\nSurveillance and Security\nSec. 5.5 Surveillance and Security\nPlease analyze the dangerous behavior of the person \nin the middle of the picture in this environment.\nPrompt\nGemini Pro\nThe person in the middle of the picture is not \nwearing a safety harness or a hard hat, which are \nrequired on a construction site. He is also standing \non a piece of rebar that is not attached to anything, \nwhich could cause him to trip and fall. There are \nalso several pieces of rebar sticking out of the \nground that he could trip over. The person in the \nmiddle of the picture is not wearing the proper \nsafety gear and is not following the safety \nprocedures that are required on a construction site.\nThe person in the middle of the picture is engaging \nin a dangerous behavior by working on a \nconstruction site without proper safety gear. In the \nimage, the man is wearing gloves and a helmet, but \nhe is not wearing any other protective equipment, \nsuch as a hard hat, safety glasses, or protective \nclothing. This lack of proper safety gear increases \nthe risk of accidents and injuries while working on \nthe construction site. It is essential for workers to \nfollow safety guidelines and wear appropriate \nprotective equipment to minimize the risk of harm \nin such environments.\nSPHINX\nThe image shows a construction site with an \nindividual in the center who appears to be engaged \nin some activity. However, it's important to note \nthat while I can provide a general analysis based on \nconstruction safety standards, I cannot make \ndefinitive assessments about behavior without \ncontext or motion, nor can I provide personal \ninformation about the individuals in the picture.\nBased on the setting, here are some potential safety \nconcerns that could be considered dangerous \nbehavior in a construction environment:\n1. Lack of Personal Protective Equipment (PPE): \nThe person may not be wearing a hard hat, which is \nessential to protect against head injuries from \nfalling objects or bumps against fixed objects. If \nthey're also not wearing safety goggles, they may be \nat risk of eye injuries from debris.\n2. Improper Use of Scaffolding: If the individual \nis manipulating or leaning on the scaffolding \nwithout proper safety measures, they could \ndestabilize the structure or fall from a height.\n3. Handling Materials Improperly: If the person \nis lifting heavy materials without proper technique \nor equipment, they may be at risk of \nmusculoskeletal injuries.\n4. Nearby Hazards: There may be tripping hazards \nsuch as loose materials or tools on the ground, and \nthe individual could be at risk if they are not paying \nattention to their surroundings.\n5. Unsecured Work Area: If the work area is not \nproperly cordoned off, there may be a risk of \nunauthorized personnel entering the area and being \nexposed to construction hazards.\nIt's crucial for all workers on a construction site to \nadhere to safety protocols, which typically include \nthe use of PPE, proper handling of tools and \nmaterials, awareness of one's surroundings, and \nadherence to guidelines for working at heights or \nwith potentially unstable structures. Regular safety \ntraining and monitoring can help mitigate these \nrisks.\nGPT-4V\nFigure 100: Results on surveillance and security. Green highlights the right answer. Red highlights\nthe wrong answer. Yellow highlights the incompetence in performing the task. Refer to Section 5.5\nfor detailed discussions.\n115\nSurveillance and Security\nSec. 5.5 Surveillance and Security\nPlease analyze the dangerous behaviors of the two \nindividuals in the environment shown in the picture.\nPrompt\nGemini Pro\nThe two individuals are not wearing the appropriate \nsafety gear. They are both wearing sandals, which \nare not safe for a construction site. The man on the \nleft is not wearing a hard hat, which is also a safety \nhazard. The man on the right is holding a measuring \ntape, which could be a tripping hazard. They are \nalso both standing in a dangerous area, as there are \nexposed rebar rods and other construction materials \nthat could cause injury.\nIn the image, there are two individuals working on \na construction site. One of the workers is holding a \ncell phone, which is a dangerous behavior in a \nconstruction environment. Using a cell phone while \nworking on a construction site can lead to \ndistractions, decreased productivity, and increased \nrisk of accidents. It is essential for workers to \nremain focused and attentive to their surroundings \nto ensure their safety and the safety of others \naround them. The second worker is holding a \nhammer, which is a necessary tool for construction \nwork. However, it is important to handle the \nhammer with care and follow proper safety \nprocedures to avoid injuries.\nSPHINX\nThe image shows a construction site with two \nindividuals engaged in potentially unsafe behaviors.\nThe person standing appears to be using a hammer \nor similar tool without wearing any visible personal \nprotective equipment (PPE) such as gloves, safety \nglasses, or a hard hat. The absence of such \nequipment poses a risk of injury from flying debris, \naccidental strikes, or falls.\nThe individual crouched down is also not wearing a \nhard hat, which is a critical piece of PPE on a \nconstruction site to protect against head injuries \nfrom falling objects or bumps against hard surfaces. \nAdditionally, this person's position appears \nunbalanced, increasing the risk of a fall, and they \nseem to be working close to unguarded edges, \nwhich could lead to a serious fall if one were to \noccur.\nBoth individuals are also wearing casual clothing, \nwhich might not provide adequate protection \nagainst the hazards typical of a construction \nenvironment, such as sharp objects or concrete \nburns.\nLastly, the construction site itself appears to have \nsome safety issues, such as exposed rebar and open \nedges, which should be properly marked and \ncovered to prevent accidents.\nIt is crucial for safety protocols to be followed \nmeticulously to ensure the well-being of all \nindividuals on the site. This includes proper use of \nPPE, ensuring the work environment is safe and \nhazards are mitigated, and that workers are trained \nto work safely and are aware of their surroundings \nat all times.\nGPT-4V\nFigure 101: Results on surveillance and security. Green highlights the right answer. Red highlights\nthe wrong answer. Refer to Section 5.5 for detailed discussions.\n116\nRemote Sensing Image Analysis\nPlease describe this scene in details.\nPrompt\nGemini Pro\nThe scene is an aerial view of an airport. There are \nfour airplanes on the runway, and one airplane is \nparked at the gate. The airplanes are all white with \ndifferent airline logos. The runway is surrounded by \ntaxiways and buildings.\nThe image is an aerial view of a large airport, \nshowcasing several planes parked on the tarmac. \nThere are a total of four planes in the scene, with \ntwo planes positioned on the left side, one in the \nmiddle, and another on the right side of the airport. \nThe planes are of various sizes and orientations, \nindicating a busy and active airport environment.\nSPHINX\nThe image is an aerial view of a section of an \nairport. There are two large commercial airplanes \non a tarmac area, positioned as if they are parked or \npreparing for taxi. They are near what appears to be \na terminal building with jet bridges, indicating \npassenger boarding areas. The airplanes are white, \nwith the tail sections visible and pointed towards \nthe terminal building. There are various markings \non the ground, likely designating taxiways and \nparking spots for the aircraft. The tarmac shows \nsome signs of wear and tire marks. On the left side \nof the image, there is a runway with clear runway \nmarkings. The area is devoid of any significant \nvegetation, and there are no visible people, \nsuggesting the photo was taken from a considerable \nheight. Shadows cast by the airplanes suggest a \nlight source from the top left of the image, \nindicating the time of day could be either morning \nor afternoon. The overall scene is one that is typical \nfor an operational area within a large airport, \ndedicated to aircraft movements and passenger \nboarding.\nGPT-4V\nSec. 5.6 Remote Sensing Image Analysis\nFigure 102: Results on remote sensing image analysis. Green highlights the right answer. Red\nhighlights the wrong answer. Yellow highlights the incompetence in performing the task. Refer to\nSection 5.6 for detailed discussions.\n117\nThe image is a large green and gray map of a city. The map is divided into several sections, \neach with its own unique features. The city is surrounded by green fields and a river, which \nruns through the middle of the map. The river is visible in the upper and lower parts of the map, \nand it appears to be a significant feature of the landscape. The map also shows a bridge, which \nis located near the top left corner of the image. The city is densely populated, with many \nbuildings and structures visible throughout the map.\nSPHINX\nThe image you've provided appears to be an aerial or satellite view of a landscape that includes \nboth urban and rural areas. Here's a detailed analysis of different elements within the image:\n1. Urban Areas: These are characterized by the dense clustering of buildings and roads. The \nurban sections display a variety of building sizes and arrangements, indicating a mix of \nresidential, commercial, and possibly industrial zones. There's a grid-like pattern in some parts \nof the urban areas, which suggests planned urban development.\n2. Rural Areas: The green patches represent agricultural fields, indicating a rural landscape. \nThe fields have different colors, ranging from light to dark green, which might be due to \ndifferent types of crops or stages of growth.\n3. Water Bodies: The dark, snaking lines seem to be rivers or canals. These water bodies vary \nin width and flow through both the urban and rural areas, which could be significant for \nirrigation, transportation, or as natural boundaries.\n4. Road Networks: The grey lines traversing both the rural and urban areas are likely roads or \nhighways. Some of these roads connect the rural areas to the urban centers, facilitating the \nmovement of goods and people.\n5. Vegetation: Apart from the agricultural fields, there are areas with darker green, likely \nrepresenting trees or forests. These might be natural or planted forests and can serve as \nwindbreaks, recreational areas, or for conservation purposes.\n6. Patterns of Settlement: The arrangement of the buildings in the urban areas varies, with \nsome sections showing a more organized layout while others are more haphazard. This could \nindicate different phases of development or variations in urban planning.\n7. Topography: Although it's challenging to determine elevation from this image, the way the \nland is used and the course of the water bodies might suggest flat to gently rolling terrain.\n8. Infrastructure: There are some large structures that could be facilities like factories, \nwarehouses, or large commercial setups, especially at the outskirts of the urban areas.\n9. Shadows: The image does not show visible shadows that would help determine the time of \nday or the direction of the sun, which is typical in high-resolution satellite imagery where the \nsun angle is high to minimize shadow.\nThe image does not provide any specific geographic landmarks that would allow for the \nidentification of the exact location. It gives a comprehensive view of a region that includes both \ndensely populated areas and agricultural land, indicating a region that supports both urban \nliving and farming activities.\nGPT-4V\nPlease analyze everything in this image in details.\nRemote Sensing Image Analysis\nPrompt\nGemini Pro\nThe image is a satellite image of an area \nin Japan. The image is in false color, with \nvegetation appearing in bright green and \nwater appearing in dark blue. The image \nis oriented with north at the top.\nThe image shows a large area of \nagricultural land, with many small fields \nof different crops. There are also several \nsmall towns and villages in the image, as \nwell as a large river that runs through the \ncenter of the image. The image is taken \nfrom a high resolution satellite, and the \ndetails of the landscape are clearly visible.\nThe image is a valuable resource for \nstudying the geography of Japan. It can be \nused to identify different types of land use, \nas well as to study the impact of human \nactivity on the environment. The image \ncan also be used to create maps and other \nvisual representations of the area.\nSec. 5.6 Remote Sensing Image Analysis\nFigure 103: Results on remote sensing image analysis. Green highlights the right answer. Refer to\nSection 5.6 for detailed discussions.\n118\nI'm sorry, but I cannot provide a step-by-step guide on assembling a mobile phone from its components as it requires specialized knowledge and tools. \nAdditionally, taking apart a mobile phone can void its warranty and potentially damage the device. It is recommended to seek professional assistance \nor follow the manufacturer's instructions for repairing or replacing components in a mobile phone.\nSPHINX\nAssembling a smartphone from its component parts is a complex \nprocess that typically requires specialized tools and knowledge. \nHowever, I can provide you with a simplified overview of the steps \ninvolved in assembling a smartphone from its components:\n1. Prepare Your Workspace: Ensure you have a clean, organized, \nand static-free workspace. Use a mat that's designed for electronic \nrepairs.\n2. Battery Preparation: Begin with the battery component. If the \nbattery is already installed in the frame, leave it as is; if not, it \nwould typically be one of the last components you install to avoid \nany risk during the assembly.\n3. Display and Digitizer: The display is usually pre-assembled with \nthe digitizer (the touch-sensitive part). This is carefully positioned \nand connected to the main board via a ribbon cable.\n4. Main Board: The main board, or motherboard, is the central \ncomponent. Carefully place it in the frame, ensuring no cables are \ntrapped underneath. Connect any ribbon cables from the display and \ndigitizer to the main board.\n5. Front-Facing Camera: Install the front-facing camera in the \ndesignated spot on the frame or main board. Connect it with the \nmain board via its ribbon cable.\n6. Rear Camera: The rear camera is installed similarly to the front-\nfacing camera, secured in place, and connected to the main board.\n7. Speakers and Microphones: Install the speaker modules and \nmicrophones in their respective places, making sure they're properly \naligned with the frame for optimal sound quality.\n8. SIM Card Tray: Insert the SIM card tray in its slot. This is \nusually on the side of the frame.\n9. Buttons and Switches: The physical buttons and switches are \nplaced in their respective spots and connected to the main board, \nusually with very small screws or clips.\n10. Additional Sensors and Antennas: Install any additional \ncomponents like proximity sensors, ambient light sensors, and \nantennas, ensuring they are properly connected to the main board.\n11. Battery Connection: If not already done, now is the time to \ninstall the battery. Connect it to the main board with its connector.\n12. Final Assembly: Carefully close the phone by attaching the \nback cover. This might involve carefully placing it and securing it \nwith screws or adhesive, depending on the model.\n13. Testing: Once everything is assembled, it's important to test all \nthe functions of the phone to ensure everything is working properly.\nKeep in mind that this is a very high-level overview. Each \nsmartphone can have a different assembly process due to variations \nin design. Professional assembly would also involve using specific \ntools to handle the components without damaging them, applying \nthermal paste or pads as needed for heat management, and ensuring \nall the connectors are properly seated. If you're attempting to \nassemble a smartphone yourself, it's highly recommended to follow \na guide specific to your phone model and use the appropriate tools \nfor the job.\nGPT-4V\nThis is the component diagram of the mobile phone. Please tell me \nstep by step how to assemble it into a mobile phone.\nRobot Motion Planning \nPrompt\nGemini Pro\n1. Prepare the parts. Before you begin, make sure you have all of \nthe necessary parts. This includes the phone's body, the display, the \nbattery, the motherboard, the camera, and the speakers.\n2. Install the motherboard. The motherboard is the main circuit \nboard of the phone. It connects all of the different parts of the phone \ntogether. To install the motherboard, first, remove the back cover of \nthe phone. Then, locate the motherboard and carefully remove it \nfrom the phone. Be careful not to damage the motherboard.\n3. Install the battery. The battery is the power source for the phone. \nTo install the battery, first, remove the back cover of the phone. \nThen, locate the battery and carefully remove it from the phone. Be \ncareful not to damage the battery.\n4. Install the display. The display is the screen of the phone. To \ninstall the display, first, remove the back cover of the phone. Then, \nlocate the display and carefully remove it from the phone. Be \ncareful not to damage the display.\n5. Install the camera. The camera is the device that takes pictures \nand videos. To install the camera, first, remove the back cover of the \nphone. Then, locate the camera and carefully remove it from the \nphone. Be careful not to damage the camera.\n6. Install the speakers. The speakers are the devices that emit \nsound. To install the speakers, first, remove the back cover of the \nphone. Then, locate the speakers and carefully remove them from \nthe phone. Be careful not to damage the speakers.\n7. Reassemble the phone. Once you have installed all of the parts, \nyou can reassemble the phone. To do this, first, replace the back \ncover of the phone. Then, carefully put the phone back together. Be \ncareful not to damage the phone.\n8. Test the phone. Once you have reassembled the phone, you can \ntest it to make sure that it works properly. To do this, turn on the \nphone and check the display, the camera, the speakers, and the \nbattery. Make sure that everything is working properly.\nSec. 5.7 Robot Motion Planning\nFigure 104: Results on robot motion planning. Red highlights the wrong answer. Refer to Section\n5.7 for detailed discussions.\n119\nTo assemble the chair from the parts shown in the image, follow \nthese steps:\n1. Base Assembly:\n   - Start with the star-shaped base, which is typically the bottom of \nthe chair. Attach the casters (wheels) into each of the five sockets on \nthe star base. They usually click into place.\n2. Gas Lift and Seat Attachment:\n   - Insert the gas lift cylinder (the metallic rod) into the center of the \nstar base. This part allows the chair to adjust in height.\n   - Slide the smaller cylindrical sleeve (if present) over the gas lift \nfor aesthetic purposes.\n3. Seat Mechanism:\n   - Position the seat mechanism (the part with levers and knobs) onto \nthe bottom of the seat. The mechanism allows you to adjust the tilt \nand lock the chair in place.\n   - Secure the mechanism to the seat using the bolts or screws \nprovided.\n4. Connecting Seat to Base:\n   - Once the mechanism is secured to the seat, place the seat onto the \ntop of the gas lift cylinder. It should fit onto the cylinder and allow \nthe seat to move up and down.\n5. Backrest Attachment:\n   - Attach the backrest to the seat. There may be a bracket or screws \nthat will help you secure the backrest to the seat or directly to the \nmechanism.\n6. Armrests and Additional Features:\n   - If there are armrests, they would typically screw into the \nunderside of the seat or sides of the backrest.\n   - Add any additional features like headrests or lumbar support \ncushions as per the design of the chair.\nOnce assembled, the chair should be a comfortable office chair with \nan ergonomic design, adjustable height, and tilt functions. It will \nhave a padded seat and backrest for support, a five-point wheeled \nbase for easy movement, and possibly adjustable armrests for \nadditional comfort. The materials appear to be a combination of \nmetal for the base and structure, with mesh and padded upholstery \nfor breathability and comfort. The chair is designed to provide proper \nsupport during long periods of sitting, making it ideal for office work.\nGPT-4V\nThe picture shows the parts of a chair, please tell me step by step how \nto assemble these parts into a chair, and please describe in detail the \nshape and function of the chair after assembling.\nRobot Motion Planning \nPrompt\nI can provide you with a general overview of the chair assembly \nprocess and the chair's features.\nFirst, take the backrest and seat cushion and place them on the chair \nframe.\nNext, attach the armrests to the chair frame.\nInstall the base of the chair by connecting the star-shaped leg to the \nchair frame.\nAttach the chair's wheels to the base of the chair.\nFinally, connect the chair's handle to the chair frame.\nthe chair after assembling has the following features:\nThe chair is designed with a star-shaped leg, which provides \nstability and support.\nThe chair has wheels, which allow for easy movement and mobility.\nThe chair has an adjustable handle, which allows for customization \nof the chair's height and comfort.\nThe chair has a backrest and seat cushion, which provide support \nand comfort for the user.\nThe chair has armrests, which provide additional support and \ncomfort for the user.\noverall, the chair is designed to provide comfort and support for the \nuser while they sit and work.\nSPHINX\nGemini Pro\n1. Assemble the chair base.\nThe chair base is the part of the chair that supports the seat and \nbackrest. To assemble the chair base, first insert the casters into the \nholes in the bottom of the base. Next, attach the gas lift cylinder to the \ncenter of the base. Finally, screw the armrests into the holes on the \nsides of the base.\n2. Assemble the seat.\nThe seat of the chair is the part that you sit on. To assemble the seat, \nfirst attach the seat cushion to the seat frame. Next, screw the seat \nframe to the gas lift cylinder. Finally, attach the armrests to the sides \nof the seat.\n3. Assemble the backrest.\nThe backrest of the chair is the part that supports your back. To \nassemble the backrest, first attach the backrest cushion to the backrest \nframe. Next, screw the backrest frame to the seat frame. Finally, \nattach the headrest to the top of the backrest.\n4. Adjust the chair.\nOnce the chair is assembled, you can adjust it to fit your needs. You \ncan adjust the height of the seat by raising or lowering the gas lift \ncylinder. You can also adjust the tilt of the backrest by turning the tilt \nknob.\nThe chair is a comfortable and stylish piece of furniture that is perfect \nfor any home or office. The chair is made of high-quality materials \nand is built to last. The chair is also easy to assemble and adjust.\nSec. 5.7 Robot Motion Planning\nFigure 105: Results on robot motion planning. Yellow highlights the incompetence in performing\nthe task. Refer to Section 5.7 for detailed discussions.\n120\n6\nQuantitative Experiments\nIn the preceding sections, we have demonstrated the qualitative performance of Gemini, GPT-4V, and\nSphinx on a selection of representative samples to intuitively illustrate their visual expertise. Building\nupon this foundation, this section delves into the quantitative analysis of the models\u2019 capabilities.\nSpecifically, we report comprehensive results obtained from the popular MME benchmark [19],\noffering an empirical evaluation of their performance in various scenarios.\n6.1\nMME Benchmark\nInstruction design. The instruction design of MME [19] is uniquely structured to elicit straightfor-\nward \u2018yes\u2019 or \u2018no\u2019 responses. Each test image is accompanied by two carefully crafted instructions.\nThese instructions are distinguished primarily by their questions \u2013 one is designed with a \u2018yes\u2019 as\nthe correct answer, and the other with a \u2018no\u2019. This binary questioning format is pivotal in assessing\nthe MLLM\u2019s comprehension abilities. A model that correctly answers both questions demonstrates\nnot only a clear understanding of the visual content but also an underlying grasp of the associated\ncontextual knowledge. This approach ensures a more robust evaluation of the MLLM\u2019s interpretative\ncapabilities in relation to visual stimuli.\nEvaluation metric. The assessment framework is tailored to align with the model\u2019s binary output\noptions: \u2018yes\u2019 or \u2018no\u2019. This dichotomy allows for a straightforward computation of two key metrics:\nstandard accuracy and enhanced accuracy (accuracy+). Standard accuracy is determined question-by-\nquestion, whereas accuracy+ is a more rigorous measure, requiring correct answers to both questions\nassociated with a single image. Notably, the baseline random accuracies for these metrics are 50% and\n25%, respectively, highlighting the heightened precision demanded by accuracy+. This dual-metric\napproach not only quantifies the model\u2019s basic accuracy but also provides a more nuanced insight into\nits comprehensive understanding of the visual content. Further, the evaluation involves aggregating\nthese metrics; the score for each sub-task is the sum of both accuracy measures. Similarly, the overall\nperception and cognition scores are derived from the cumulative scores of their respective sub-tasks,\noffering a multi-faceted evaluation of the model\u2019s performance.\nData collection. For the perceptual tasks of Existence, Count, Position, Color, OCR, Poster, Celebrity,\nScene, Landmark, and Artwork, the sample images are sourced from publicly available datasets [34,\n28, 40, 64, 55]. Regarding the cognitive tasks of Commonsense Reasoning, Numerical Calculation,\nText Translation, and Code Reasoning, the sample images are obtained from manually photographed\nor diffusion model-generated sources.\n6.2\nResults\nAs shown in Table 1, in terms of the comprehensive performance of perception and cognition, Gemini\nexhibits superior performance with a score of 1933.4, closely followed by the GPT-4V model, which\nscored 1926.6. The Sphinx model trails with a score of 1870.2.\nPerception. Sphinx surpasses other models in most of the perception tasks. This is particularly\nevident in the task of position perception, where Gemini and GPT-4V underperform Sphinx by\na margin of 60 points. This observation diverges somewhat from the rankings illustrated by the\nqualitative experiments in Section 2. We hypothesize that this discrepancy arises because the\nsamples used in the MME to evaluate perception primarily originate from public academic datasets\n[34, 28, 40, 64, 55], whose data distribution closely aligns with the training set of the open-source\nmodel Sphinx. Furthermore, it is noteworthy that due to the refusal to provide information related to\nreal persons, the score of GPT-4V in the sub-task of celebrity recognition is zero.\nCognition. GPT-4V dominates almost all cognition tasks, especially code reasoning, with a notably\nhigh score of 170.0. This finding is largely consistent with the comparative results of the qualitative\nexperiments discussed in Section 3.\nIn summary, while each model demonstrates particular strengths across various tasks within the\nbenchmark, Gemini and GPT-4V outperform Sphinx when considering the overall performance.\nNotably, GPT-4V exhibits leading performance in cognition tasks, while Gemini demonstrates a\nmore balanced performance across various tasks, thereby achieving the highest score. This aspect is\nintuitively illustrated in Figure 106.\n121\nFigure 106: Evaluation on 14 sub-tasks of the MME benchmark. We observe the following phenom-\nena: (1) GPT-4V refuses to respond to names about real people, leading to a zero score in the celebrity\nrecognition sub-task. (2) Both Gemini and GPT-4V exhibit suboptimal performance in the position\nrecognition sub-task, aligning with the qualitative findings from earlier experiments shown in Figure\n1, suggesting that the two models may be insensitive to spatial information. (3) The performance of\nSphinx on perception is on par or even exceeds that of Gemini and GPT-4V. This is probably because\nthat Sphinx pay more attention on perception during training, such as object detection. In contrast,\ncompared to Gemini and GPT-4V, Sphinx lags considerably on the cognition sub-tasks, including\ncommonsense reasoning, numerical calculation, text translation, and code reasoning.\nModel\nOverall\nPerception\nCognition\nExist. Count Pos. Color Poster Cele. Scene Land. Art. OCR Com. Cal. Trans. Code\nSphinx [35]\n1870.2 195.0 160.0 153.3 160.0 164.3 177.9 160.0 168.1 134.0 87.5 130.0 55.0\n75.0\n50.0\nGPT-4V [43] 1926.6 190.0 160.0 95.0 150.0 192.2\n0.0\n151.0 138.3 148.0 185.0 142.1 130.0 75.0 170.0\nGemini [21]\n1933.4 175.0 131.7 90.0 163.3 165.0 147.4 144.8 158.8 135.8 185.0 129.3 77.5 145.0 85.0\nTable 1: Evaluation on the MME benchmark. Here we report the results on all the sub-tasks,\nincluding Existence (Exist.), Count, Position (Pos.), Color, OCR, Poster, Celebrity (Cele.), Scene,\nLandmark (Land.), Artwork (Art.), Commonsense Reasoning (Com.), Numerical Calculation (Cal.),\nText Translation (Trans.), and Code Reasoning (Code). The highest scores across individual sub-tasks\nare highlighted in bold.\n122\n7\nConclusion\n7.1\nSummary\nIn this report, we have conducted a comprehensive evaluation of three powerful MLLMs, i.e.,\nGemini Pro [21], GPT-4V [43], and Sphinx [35], which involves diverse qualitative samples and a\nquantitative benchmark, MME [19]. For multi-faceted comparison, we carefully collect numerous\nsamples covering different domains of visual understanding, including fundamental perception,\nadvanced cognition, challenging vision tasks, and various expert capacities. Each domain also\ncontains several subtasks for in-depth discussion and analysis.\n7.2\nGemini vs GPT-4V\nThe qualitative results indicate that Gemini is indeed a strong challenger to GPT-4V, given its\nsuperior multi-modal reasoning capacity. In most cases, Gemini achieves competitive answering\naccuracy compared to GPT-4V, and showcases different response styles and preferences.\nDifferences.\nFor comparison, GPT-4V tends to generate more detailed descriptions of the perception\ntasks (Figures 8, 9, 10, 23, 25), and provide in-depth analysis with step-by-step intermediate reasoning\nfor the cognition tasks (Figures 39, 42, 48, 65, 69). Instead, Gemini prefers to provide a direct and\nconcise response to the answer, which helps the users to rapidly locate pertinent information. When\nthere are a greater number of visual elements in the image (Figures 8, 9), the fine-grained perception\nadvantages of GPT-4V become more pronounced, which provides more accurate recognition of visual\ndetails. However, GPT-4V, due to privacy concerns, may decline to respond to topics related to\ncelebrities (Figures 24, 29, 81 and the Celebrity metric on the MME benchmark [19] shown in Figure\n106), or it may refrain from attempting to answer certain out-of-scope questions by anticipating its\nown limitations (Figures 31, 77). For some specific vision and expert-level tasks (Figures 37, 38, 85),\nGemini normally demonstrates a broader range of learned knowledge and generalization capabilities,\nindicating better applicability across various domains.\nCommon issues.\nThere are four common issues of the two MLLMs. 1) The first limitation is the\nspatial perception capabilities. From the qualitative perception examples (Figure 1) and quantitative\nresults on the MME benchmark (the Position metric shown in Figure 106), both Gemini and GPT-\n4V are not proficient in determining the relative positions of objects. 2) The second issue is the\nunsatisfactory OCR (Figures 41, 45) and abstract visual understanding (Figures 50, 52). For example,\nthey may misinterpret some of the numbers and characters in the diagrams or charts, and have\ndifficulty comprehending some geometric shapes and abstract inductive abilities. 3) The third\ninadequacy lies in the logical self-consistency within reasoning. For some scientific problems (Figure\n62) or \u2018Yes or No\u2019 questions (Figure 43), they occasionally provide intermediate reasoning steps that\ndo not align with or are contrary to the final answer. 4) The fourth common issue concerns their\nrobustness to prompt designs. As shown in Figures 43 and 59, for different approaches to framing the\nsame question prompt, GPT-4V and Gemini would be disturbed to generate opposite answers. Such a\nissue affects the output stability, and impedes their further applications. We can see that both Gemini\nand GPT-4V still struggle in many cases, showing the long road to the general MLLM.\n7.3\nGemini vs Sphinx\nDespite that Sphinx is on par with GPT-4V and Gemini in some cases, it is not capable of generating\nas consistent high-quality answers as them. This demonstrates that the open-source MLLMs still\nhave some non-negligible gaps to closed-source models.\nFailure cases of Sphinx.\nWe observe that the failure cases of Sphinx are mainly due to two reasons.\n1) The first is that the diversity of Sphinx\u2019s training data is still lacking in some domains, constraining\nits capability for a wider range of tasks, e.g., scientific knowledge perception (Figure 16), visual code\ngeneration of HTML codes (Figure 46), and abstract visual reasoning (Figure 50). This motivates us\nto further incorporate more data in diverse domains for training faithful open-source MLLMs. 2) The\nsecond limitation is the inherent reasoning upper bound of the underlying LLM. Sphinx adopts the\nvanilla LLaMA-2-7B [50] for initialization, which falls short compared to larger-scale LLMs (e.g.,\n70B models) in certain complex tasks.\n123\n7.4\nFuture Directions\nWith our comprehensive comparison and discussion, Gemini and GPT-4V are both pioneers of\nMLLMs in this era, showcasing sparks of artificial general intelligence [59, 9]. Looking ahead, future\ndevelopment of MLLMs can focus on three aspects: visual representation encoding (fine-grained\nappearances, spatial relation awareness), multi-modal alignment (alleviating hallucination, OCR\naccuracy), and LLMs\u2019 reasoning capacity (quantitative processing, logical self-consistency). Overall,\ndespite the exciting achievements made thus far, there still remains a considerable distance towards\nartificial general intelligence. We also anticipate the emergence of stronger and more comprehensive\nMLLMs in the future.\n124\nReferences\n[1] Chatgpt\ncan\nnow\nsee,\nhear,\nand\nspeak.\nhttps://openai.com/blog/\nchatgpt-can-now-see-hear-and-speak, 2023.\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. NeurIPS, 2022.\n[3] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\n[4] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u02c7ci\u00b4c, and Cordelia\nSchmid. Vivit: A video vision transformer. In ICCV, 2021.\n[5] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023.\n[6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile\nabilities. arXiv preprint arXiv:2308.12966, 2023.\n[7] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad\u2013a comprehen-\nsive real-world dataset for unsupervised anomaly detection. In CVPR, 2019.\n[8] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In NeurIPS, 2020.\n[9] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n[10] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu,\nAnush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal\ndataset for autonomous driving. In CVPR, 2020.\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n[12] Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, and Chunyuan Li. Llava-\ninteractive: An all-in-one demo for image chat, segmentation, generation and editing. arXiv\npreprint arXiv:2311.00571, 2023.\n[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.\n[15] Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Ro-\ndriguez, Sameer Antani, George R Thoma, and Clement J McDonald. Preparing a collection\nof radiology examinations for distribution and retrieval. Journal of the American Medical\nInformatics Association, 2016.\n[16] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar,\nPierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc\nToussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied\nmultimodal language model. In arXiv preprint arXiv:2303.03378, 2023.\n125\n[17] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In\nCVPR, 2020.\n[18] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network\nfusion for video action recognition. In CVPR, 2016.\n[19] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang,\nXiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for\nmultimodal large language models. arXiv preprint arXiv:2306.13394, 2023.\n[20] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan\nLu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction\nmodel. arXiv preprint arXiv:2304.15010, 2023.\n[21] Google Gemini Team. Gemini: A family of highly capable multimodal models. 2023.\n[22] Ross Girshick. Fast r-cnn. In ICCV, 2015.\n[23] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu,\nWenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for\ndialogue with humans, 2023.\n[24] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin\nChen, Peng Gao, Xianzhi Li, Hongsheng Li, et al. Point-bind & point-llm: Aligning point cloud\nwith multi-modality for 3d understanding, generation, and instruction following. arXiv preprint\narXiv:2309.00615, 2023.\n[25] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang,\nChris Liu, Song Wen, Ziyu Guo, et al. Imagebind-llm: Multi-modality instruction tuning. arXiv\npreprint arXiv:2309.03905, 2023.\n[26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\n[27] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n[28] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: A holistic dataset\nfor movie understanding. In ECCV, 2020.\n[29] Qiuyuan Huang, Zhe Gan, Asli Celikyilmaz, Dapeng Wu, Jianfeng Wang, and Xiaodong He.\nHierarchically structured reinforcement learning for topically coherent visual story generation.\nIn AAAI, 2019.\n[30] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao\nLv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045, 2023.\n[31] Huaizu Jiang and Erik Learned-Miller. Face detection with the faster r-cnn. In FG, 2017.\n[32] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. NeurIPS, 2022.\n[33] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal\nexcitation and aggregation for action recognition. In CVPR, 2020.\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,\n2014.\n[35] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin,\nWenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual\nembeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023.\n126\n[36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[37] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection. arXiv preprint arXiv:2303.05499, 2023.\n[38] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng,\nKai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating math reasoning in\nvisual contexts with gpt-4v, bard, and other large multimodal models. arXiv e-prints, pages\narXiv\u20132310, 2023.\n[39] Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, and Tae-Kyun Kim.\nMultiple object tracking: A literature review. Artificial intelligence, 293:103448, 2021.\n[40] Hui Mao, Ming Cheung, and James She. Deepart: Learning joint representations of visual arts.\nIn ACM MM, 2017.\n[41] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task general-\nization via natural language crowdsourcing instructions. In ACL, 2022.\n[42] OpenAI. Gpt-4 technical report, 2023.\n[43] OpenAI. Gpt-4v(ision) system card. 2023.\n[44] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. NeurIPS, 2022.\n[45] Bryan A Plummer, Arun Mallya, Christopher M Cervantes, Julia Hockenmaier, and Svetlana\nLazebnik. Phrase localization and visual relationship detection with comprehensive image-\nlanguage cues. In ICCV, 2017.\n[46] Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, and Leonid\nSigal. Make-a-story: Visual memory conditioned consistent story generation. In CVPR, 2023.\n[47] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,\nreal-time object detection. In CVPR, 2016.\n[48] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training\nenables zero-shot task generalization. In ICLR, 2021.\n[49] Linsen Song, Wayne Wu, Chaoyou Fu, Chen Qian, Chen Change Loy, and Ran He. Pareidolia\nface reenactment. In CVPR, 2021.\n[50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[51] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot learning with frozen language models. NeurIPS, 2021.\n[52] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and Philip HS Torr. Fast online object\ntracking and segmentation: A unifying approach. In CVPR, 2019.\n[53] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In\nICLR, 2022.\n[54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nNeurIPS, 2022.\n127\n[55] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a\nlarge-scale benchmark for instance-level recognition and retrieval. In CVPR, 2020.\n[56] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any\nmultimodal llm. arXiv preprint arXiv:2309.05519, 2023.\n[57] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object tracking: A benchmark. In CVPR,\n2013.\n[58] Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Wider face: A face detection\nbenchmark. In CVPR, 2016.\n[59] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\narXiv:2309.17421, 2023.\n[60] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,\nAnwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large\nlanguage models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\n[61] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey\non multimodal large language models. arXiv preprint arXiv:2306.13549, 2023.\n[62] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg.\nMattnet: Modular attention network for referring expression comprehension. In CVPR, 2018.\n[63] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,\nPeng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init\nattention. arXiv preprint arXiv:2303.16199, 2023.\n[64] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning\ndeep features for scene recognition using places database. NeurIPS, 2014.\n[65] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\n128\n"
  },
  {
    "title": "Tracking Any Object Amodally",
    "link": "https://arxiv.org/pdf/2312.12433.pdf",
    "upvote": "11",
    "text": "Tracking Any Object Amodally\nCheng-Yen (Wesley) Hsieh1\nTarasha Khurana1\nAchal Dave2\nDeva Ramanan1\n1Carnegie Mellon University\n2Toyota Research Institute\nhttps://tao-amodal.github.io\nModal\nAmodal\nDiverse categories\ngrocery_bag\nperson\ndog\ndrawer\npacket\ndoor\ntruck\nbus\ngoldfish\ngoldfish\nHeavy occlusions\ncar_(automobile)\nFigure 1. Representative sequences of our TAO-Amodal dataset. Our dataset augments the TAO dataset [10] with amodal bounding box\nannotations for fully invisible, out-of-frame, and occluded objects. Note that this implies TAO-Amodal also includes modal segmentation\nmasks (as visualized in the color overlays above). Our dataset encompasses 880 categories, aimed at assessing the occlusion reasoning\ncapabilities of current trackers through the paradigm of Tracking Any Object with Amodal perception (TAO-Amodal).\nAbstract\nAmodal perception, the ability to comprehend complete\nobject structures from partial visibility, is a fundamental\nskill, even for infants. Its significance extends to applica-\ntions like autonomous driving, where a clear understand-\ning of heavily occluded objects is essential. However, mod-\nern detection and tracking algorithms often overlook this\ncritical capability, perhaps due to the prevalence of modal\nannotations in most datasets. To address the scarcity of\namodal data, we introduce the TAO-Amodal benchmark,\nfeaturing 880 diverse categories in thousands of video se-\nquences. Our dataset includes amodal and modal bound-\ning boxes for visible and occluded objects, including objects\nthat are partially out-of-frame. To enhance amodal tracking\nwith object permanence, we leverage a lightweight plug-in\nmodule, the amodal expander, to transform standard, modal\ntrackers into amodal ones through fine-tuning on a few hun-\ndred video sequences with data augmentation. We achieve\na 3.3% and 1.6% improvement on the detection and track-\ning of occluded objects on TAO-Amodal. When evaluated\non people, our method produces dramatic improvements of\n2x compared to state-of-the-art modal baselines.\n1. Introduction\nMachine perception, particularly in object detection and\ntracking, has focused primarily on reasoning about visible\nor modal objects. This modal perception ignores parts of\nthe three-dimensional world that are occluded to the cam-\nera. However, amodal completion of objects in the real-\nworld (e.g., seeing a setting sun but understanding it is\nwhole) and their persistence over time (e.g., person walk-\ning behind a car in Fig. 2) are fundamental capabilities that\ndevelop in humans in their early years [3, 28, 43]. In au-\ntonomous systems, this online amodal reasoning finds a di-\nrect application in downstream motion planning and navi-\ngation. Despite this, object detection and tracking stacks\ngive little importance to partially or completely occluded\nobjects; this becomes apparent in datasets that are only an-\nnotated modally [9, 10, 15, 16, 21, 31, 32, 38, 57, 61] but\nare still widely used and built upon by algorithms. These\nalgorithms [12, 17, 25, 34, 36, 37, 41, 46, 64] in turn learn\nto perceive only modal objects.\nTo address this gap, we first introduce a benchmark for\nlarge-scale amodal tracking, which requires estimating the\nfull extent of objects through heavy and even complete oc-\nclusions.\nOur benchmark annotates 17,000 objects with\n1\narXiv:2312.12433v2  [cs.CV]  23 Jan 2024\nTraditional (modal) detection/tracking\nAmodal detection/tracking\nFigure 2. Traditional modal perception (top) vs. amodal per-\nception (bottom). Given a sequence of images, traditional de-\ntection and tracking algorithms concentrate on identifying visi-\nble segments of multiple objects within the scene. Consequently,\nthey face challenges resulting in perculiar output such as vanish-\ning bounding boxes or tiny box sizes under occlusion scenarios.\nAmodal perception advances beyond conventional approaches by\ninferring complete object boundaries, thereby predicting bounding\nboxes that extend to the full object extent, even when certain por-\ntions are occluded.\namodal bounding boxes, along with human confidence es-\ntimates, from 880 classes in 2,921 videos.\nWhile prior\ndatasets focus on images or are limited to a small vocab-\nulary of classes (Tab. 1), our benchmark evaluates amodal\ntracking for hundreds of object classes. Since objects can\nget occluded because of other objects in the scene, and be-\ncause of the limited field-of-view of cameras in casual cap-\ntures, we define and address two kinds of occlusions \u2013 in-\nframe, and out-of-frame. As annotating amodal bounding\nboxes can be ambiguous and challenging, we design a new\nannotation protocol with detailed guidelines to improve hu-\nman annotation. Importantly, we base our benchmark on\na large-vocabulary multi-object tracking dataset, TAO [10].\nThis choice allows us to pair our amodal box annotations\nwith class labels, modal boxes, and precise modal mask an-\nnotations [2] collected in prior work.\nEquipped with this data, we set out to evaluate the diffi-\nculty of amodal tracking. We evaluate using standard met-\nrics, including detection and tracking AP, and variants that\nevaluate tracking specifically under partial and complete oc-\nclusions. As expected, we find that standard trackers trained\nwith modal annotations do not suffice for amodal tracking.\nWe present an effective module to turn modal track-\ners into amodal trackers with object permanence. Specif-\nically, we build a class-agnostic amodal expander, inspired\nby [65], that expands modal bounding box predictions to\ncover the full extent of objects. With limited amodal train-\ning data, we build the amodal expander to be class-agnostic,\nallowing us to transfer across classes for which we have\nfewer amodal annotations.\nSpecifically, for amodal de-\ntection of people, we observe dramatic relative improve-\nments of \u223c30.0 AP points as compared to modal baselines.\nOur results show strong improvements over state-of-the-art\ntrackers for amodal tracking.\nIn summary, our contributions are as follows: (1) We\nannotate a large-scale dataset of amodal tracks for diverse\nobjects, consisting of 17k objects spanning 880 categories,\n(2) we adapt evaluation metrics to handle amodal settings,\nand evaluate state-of-the-art trackers for our new task, and\nfinally, (3) we present a strong baseline for amodal tracking\nwith object permanence, using a category-agnostic amodal\nexpander module.\n2. Related work\nAmodal perception has been studied in the past by bench-\nmarks and algorithms, in both the single-frame (detection)\nand multi-frame (detection and tracking) settings.\nSince\namodal object annotations are hard to obtain due to the un-\ncertainty in human annotations (c.f. prior work [30] on a hu-\nman vision experiment), the community has seen more de-\npendence either on synthetic datasets, or real-world datasets\nwith few classes and limited diversity. The remainder of the\nsections talk about this in detail.\n2.1. Benchmarks\nReal-world datasets\nIn the surveillance and self-driving\ndomains, real-world datasets that annotate objects amodally\nexist. MOT 15-20 [11, 33, 42] evaluate multi-object track-\ning on amodal person detections obtained from detectors\ntrained on MOT annotations. One pitfall for MOT is that\ninstead of humans labelling the boxes during \u201cinvisible\u201d\nphases of people, these box annotations are just linearly in-\nterpolated between two visible frames of the same object.\nAdditionally, the metrics used by MOT weigh all modal and\namodal annotations equally. This downweighs the tracking\nperformance on amodal objects as they form only a small\nfraction of all annotations.\nA number of multimodal (images and 3D LiDAR)\ndatasets for autonomous driving have recently become pop-\nular.\nThese include ArgoVerse (1.0 and 2.0) [7, 58],\nWaymo [50], nuScenes [5] and KITTI [18]. These datasets\naim to focus on 3D tasks, and therefore use human anno-\ntators to label all objects in 3D to their full extent. In this\nsetting, amodal perception is organic, and almost trivial, as\nin a fully-3D world there are no occlusions. These 3D boxes\nwhen projected onto 2D images would be useful for amodal\nperception but unfortunately, they only cover a small num-\nber of object classes. Another way to obtain amodal object\nannotations is in a multi-view setting. Datasets like Car-\nFusion [45] and MMPTrack [22] follow this data curation\nscheme but because of the cumbersome data collection pro-\ncess, they are limited to only a single or few categories.\nIn the single-frame setting, COCO-Amodal, Amodal\nKINS and NuImages [5, 44, 65] contain amodal annota-\ntions of a few objects, but only cover the cases of par-\ntial occlusion (as complete occlusion is hard to resolve\n2\n# Sequences\n# Occluded Boxes\n# Occluded\nTrack\nAnn\nTotal\nTrain\nVal\nTest\nClasses\nPartial\nHeavy\nOoF\ntracks\nlength\nfps\nlength\nCOCO-Amodal [65]\n2500\n1250\n1250\n-\u2217\n8.8k\n0.2k\n0\n-\n-\n-\n-\nSail-VOS [26]\n160\n41\n0\n162\n559.5k\n704.8k\n0\n7.9k\n14.14\n8\n3,359\nSail-VOS-3D [27]\n161\n41\n0\n24\n295.0k\n387.5k\n0\n5.0k\n13.10\n8\n2,808\nNuScenes [5]\n700\n150\n150\n23\n571.1k\n139.5k\n219k\n24.5k\n9.06\n20\n6,000\nMOT17 [42]\n7\n0\n7\n1\n51.2k\n16.4k\n16k\n0.1k\n6.98\n30\n248\nMOT20 [11]\n4\n0\n4\n1\n729.4k\n88.1k\n88k\n1.6k\n20.55\n25\n178\nTAO-Amodal\n500\n993\n1428\n880\n158.2k\n35.1k\n139k\n9.6k\n22.24\n1\n88,605\nTable 1. Statistics of amodal datasets. We compare TAO-Amodal to prior image (first block), synthetic video (second block), and real\nvideo (last block) datasets. Compared to existing amodal datsets, TAO-Amodal is notable for being real-world videos that span far more\ncategories and far more annotated frames for evaluation. We define heavy occlusion as objects with visibility below 10%, and partial as\nbetween 10%-80%. Occluded tracks are those that have heavy or partial occlusions for more than 5 seconds. Out-of-frame (OoF) objects\nare ones that extend partially beyond the image boundary. Track length is averaged over the dataset in seconds, while total length is the\nlength of eval sequences in seconds.\nin a single-frame setting without temporal visibility cues).\nMoreover, COCO-Amodal [65] and Amodal KINS [44] are\nnot shipped with class labels, which makes it difficult to\nlearn object-specific priors for amodal completion.\nSynthetic datasets\nIntuitively, it is cheap to get amodal\nobject annotations from synthetic data setups. SAIL-VOS\nand SAILVOS-3D [26, 27] are such datasets that exploit\nsynthetic dataset curation and come with a number of dif-\nferent types of annotations (bounding boxes, object masks,\nobject categories, their long-range tracks, and 3D meshes).\nSome of these even suit our case of detecting \u2018out-of-frame\u2019\nocclusions, where one could project 3D meshes onto the\nimage plane. While the number of categories are slightly\nlarger for these datasets (including others like ParallelDo-\nmain [53] and DYCE [14]), the sim-to-real transfer remains\na challenge even for modal perception [8, 29].\n2.2. Algorithms\nBased off of some amodal datasets, there has been a\ngrowing interesting in developing algorithms suitable for\namodal perception.\nSome methods aim to track objects\nwith object permanence [30, 53\u201355].\nSome approaches\nutilize prior-frame information [4, 6, 13, 49, 59, 60, 63].\nFor instance, GTR [53] employs a transformer-based ar-\nchitecture and uses trajectory queries to group bounding\nboxes into trajectories. Previous work also segment objects\namodally [35, 39]. We lean on similar approaches in this\nwork, and devise a mechanism to generate occlusion cases\nin the flavor of the data augmentation used by GTR [64],\nand show that this is essential to the goal of enabling amodal\nperception.\n3. Dataset Annotation\nBase dataset.\nExisting datasets for modal perception are\nlimited either in terms of their diversity, or the vocabu-\nlary of classes labelled. To this end, we build upon the\nmodally annotated TAO dataset. It contains bounding box\ntrack annotations of 833 object categories at 1FPS span-\nning a total of 2,921 videos from 7 different data sources\n(AVA [20], Argoverse [7], Charades [47], HACS [62], La-\nSOT [16], BDD100K [61], YFCC100M [52]). This gives\nus the advantage of adding amodal box annotations to an\nalready existing set of multimodal annotations in TAO (i.e.,\nobject classes, modal bounding boxes and modal segmen-\ntation masks). In terms of the extent/scope of annotation,\nTAO follows the single-frame detection datasets like LVIS\nand OpenImages [21, 31], in adopting a federated annota-\ntion protocol for object tracking, i.e., not every object is ex-\nhaustively annotated in every video. For collecting amodal\nannotations in TAO-Amodal, we consider only the set of\ntracks that are labelled in TAO.\nScope.\nSince annotators can exhibit a large variation in\nannotating the precise shape of objects while they un-\ndergo partial or even complete occlusion, we annotate us-\ning bounding boxes (instead of segmentation masks like in\nBURST [2]) to annotate their extent in the visible scene.\nAdditionally, we define \u2018in-frame\u2019 occlusions as those oc-\ncurring from the presence of occluders (occluders might be\nother dynamic objects, or static scene elements), and \u2018out-\nof-frame\u2019 occlusions as those resulting from objects leaving\nthe camera field-of-view. We do not label the extent of oc-\nclusion in cases where an object may be partially present\nbehind the camera (e.g., a person holding the camera who\nhas their hands visible in the image). For labelling \u2018out-\n3\nProposal features\nModal box \ndeltas\nAmodal \nExpander\nAmodal box \ndeltas\n\ud83d\udd12\nTracker\nRegression \nHead\n\ud83d\udd12\nRegion proposals\nFigure 3. ROI Head [19] with Amodal Expander. Our Amodal\nExpander serves as a plug-in module that can \u201camodalize\u201d any ex-\nisting detector or tracker with limited (amodal) training data. It\noperates by taking as input region proposal features and modal\nbox predictions (often represented as a residual delta with respect\nthe region proposal) and generates amodal box outputs (again rep-\nresented as residual deltas). To enable learning from limited data,\nwe freeze all modules except the amodal expander.\nof-frame\u2019 occlusions, we need to fix bounds for annotation\non the image plane. We ask annotators to work within an\nannotation workspace that extends to twice the image di-\nmensions in consideration, with the image itself horizon-\ntally and vertically center-aligned in this workspace.\nAnnotation Protocol\nSince TAO tracks (and their indi-\nvidual bounding boxes) are modal in nature, extending\nboxes to account for in-frame and out-of-frame occlusions\ntranslates to (1) modifying existing boxes in TAO, for the\ncases of partial occlusion, and (2) adding new boxes for\ncompletely occluded objects. We find that out of a total\nof 358,862 boxes in TAO, 266,902 (74.4%) boxes are mod-\nified to incorporate partial occlusion. TAO-Amodal boasts\nof an additional 23,449 bounding boxes for invisible objects\nthat were not in TAO. These annotations follow the guide-\nlines detailed in the appendix, that cover a wide range of\nboth in-frame and out-of-frame occlusion scenarios. Im-\nportantly on a higher-level, we only consider the occlusion\ncases, where an object has appeared in the scene before. We\nalso scope out the occlusion cases, where an object might\nbe partially behind the camera, or outside the annotation\nworkspace defined above. Within the strict purview of the\nguidelines, when an object\u2019s location cannot be discerned\nconfidently by the annotators, an is uncertain flag is\nadded to the labeled bounding box. From the 23,449 boxes\nfor invisible objects, 20,218 (85.8%) boxes are annotated\nconfidently (i.e., without the uncertain flag), indicating that\nthere is inherent uncertainty in localizing objects when they\nundergo heavy occlusions (ref. human vision experiment\nfrom a prior work [30]).\nFinally, equipped with both modal and amodal annota-\ntions for all objects in TAO and TAO-Amodal, we add a\nvisibility field to the TAO-Amodal annotations. From qual-\nitative visual inspection, we find that the IoU between the\nmodal and amodal boxes, is a good enough approximation\nPositional encoding\nMLP\nN proposal \nfeatures\nModal box deltas\n(N, 4)\nflatten\n(N, 1024)\n(N, 256)\nAmodal box deltas\n(N, 4)\nFigure 4. Amodal Expander Architecture. Given N flattened\nproposal features and modal box (delta) predictions represented\nwith 256-dim positional encodings [56], we predict amodal box\n(deltas) with a two-layer MLP (unless otherwise specified). Fur-\nther architecture details could be found in Sec. 5.2\nof the visibility of each object.\nQuality Control\nWe ensure two rounds of professional\nquality check on TAO-Amodal annotations; this means all\nbounding box annotations are refined twice by annotators.\nFinally, authors conduct a manual quality check where 7\nvideos are chosen at random from the 7 dataset subsets and\nonly 2 out of 349 tracks are found to be erroneous. These\nare tracks of invisible objects (visibility 0.0%) where some\nboxes are without a uncertainty flag. Our analysis show\nthat nearly all inspected tracks (> 99%) are accurate, in-\ndicating the high-quality of amodal tracking annotations in\nTAO-amodal.\n3.1. Statistical Analysis\nWe compare the statistics of TAO-Amodal to other amodal\nbenchmarks in Table 1. For NuScenes, which only cate-\ngorizes object visibilities into four buckets, we use inter-\npolation to estimate the number of boxes below visibil-\nity 0.1 and 0.8. A few amodal datasets are omitted from\nthe table either because they have been incorporated in\nTAO-Amodal [7, 61] or because these datasets lack quan-\ntified visibilities for categorizing distinct occlusion scenar-\nios [9, 51]. TAO-Amodal encompasses annotations across\nan extensive 880 categories, which can be used to learn and\nevaluate object priors in a large-vocabulary setting, in con-\ntrast to benchmarks that only cover people or vehicles. Ad-\nditionally, TAO-Amodal features a 10\u00d7 longer evaluation\nduration, ensuring its status as a challenging benchmark.\n4. Amodal tracking\nTraditional and amodal tracking.\nGiven a sequence of\nimages I1, I2, ..., It, tracking approaches aims to output\nmodal bounding boxes b, trajectory identifiers \u03c4, and class\nlabels s for objects across all frames. If an object is partially\noccluded, the box marks only the visible extent of the ob-\nject, as illustrated in Fig. 2. We focus here on amodal track-\ners, which similarly takes as input a sequence of images,\n4\nDetection Metrics\nTracking Metrics\nMethod\nAP[0,0.1]\nAP[0.1,0.8]\nAP[0.8,1]\nAPOoF\nModal AP\nAP\nAP\nAP[0,0.8]\nModal AP\nQDTrack [17]\n0.39\n7.79\n21.70\n7.88\n20.07\n15.47\n7.84\n4.03\n11.36\nTET [36]\n0.70\n8.89\n29.96\n8.66\n29.42\n22.04\n4.72\n3.32\n7.7\nAOA [12]\n0.56\n6.32\n24.14\n6.53\n23.27\n17.76\n13.63\n6.63\n21.18\nViTDet-B + SORT [4, 37]\n0.77\n11.40\n34.03\n12.98\n32.67\n25.15\n6.95\n4.10\n11.57\nViTDet-L + SORT [4, 37]\n1.18\n13.75\n37.41\n14.70\n36.65\n28.05\n8.19\n5.14\n13.73\nViTDet-H + SORT [4, 37]\n1.03\n14.54\n39.71\n16.53\n38.05\n29.56\n8.94\n5.76\n14.55\nGTR [64]\n0.78\n13.24\n37.54\n14.18\n36.08\n28.19\n16.02\n8.86\n22.50\nTable 2. Off-the-shelf trackers on TAO-Amodal validation set. We use the metrics defined in Sec. 5.1, where the visibility range is\nindicated by the superscript. Off-the-shelf trackers were either trained on TAO [10] or on synthetic videos [64] generated using LVIS\nimages [21], with categories aligned with our dataset. We use visibility attributes to analyze performance at various levels of occlusion.\nWhile certain trackers can detect non-occluded objects well (over 35% AP), objects that are highly occluded, partially occluded, and out-\nof-frame remain challenging. Because of GTR\u2019s overall strong performance in both detection and tracking, we use it as the basis for our\nsubsequent experiments. We run all existing trackers at 1 fps and average AP across all categories with an IoU threshold of 0.5.\nDetection Metrics\nTracking Metrics\nMethod\nAP[0,0.1]\nAP[0.1,0.8]\nAP[0.8,1]\nAPOoF\nAP\nAP\nAP[0,0.8]\nBaseline (GTR [64])\n0.78\n13.24\n37.54\n14.18\n28.19\n16.02\n8.86\nFine-tune entire model\n0.52\n10.36\n24.08\n10.34\n17.93\n7.70\n3.93\nFine-tune regression head\n+ proposal network\n0.79\n10.57\n27.91\n11.37\n21.42\n9.04\n4.53\nFine-tune regression head\n0.77\n14.62\n38.17\n15.31\n29.24\n16.07\n9.28\nAmodal Expander\n0.67\n16.29\n37.11\n17.39\n29.50\n16.10\n10.44 (+1.58)\nAmodal Expander + PnO\n0.80 (+0.02)\n16.41\n37.74\n17.64\n29.87\n16.35 (+0.33)\n10.13\nAmodal Expander + PnO\u2020\n0.77\n16.53 (+3.29)\n37.80 (+0.26)\n17.65 (+3.47)\n29.96 (+1.77)\n16.35 (+0.33)\n10.28 (+1.42)\nTable 3. Amodal expander on TAO-Amodal validation set. We ablate different strategies for repurposing GTR for amodal tracking,\nincluding fine-tuning all or part of the model, as well as our proposed Amodal Expander. The latter modestly outperforms naive fine-\ntuning of the regression head. When combined with additional augmentation techniques for generating synthetic occlusions such as\nPasteNOcclude (PnO) (as detailed in Sec 4.2), our final model produces noticeable gains for partially occluded and out-of-frame objects.\nAll models (other than the baseline) were trained on TAO-Amodal training set for 20k iterations, while \u2020 denotes 45k iterations of training.\nTable 4 focuses on performance for the people category, demonstrating even more impressive gains.\noutput modal boxes b, trajectory identifiers \u03c4, and classes s.\nBeyond this, amodal tracker further generates amodal boxes\nba, which cover full extent of partially and fully occluded\nobjects.\nIn reality, training an entire amodal model is unfeasible\ndue to the limited amount of amodal training data. Draw-\ning inspiration from the human ability of amodal comple-\ntion, we posit the feasibility of transforming a conventional\ntracker into an amodal one by leveraging its understand-\ning of modal objects. We introduced a light-weight class-\nagnostic amodal expander E in the next section.\n4.1. Amodal expander\nAmodal expander E serves as a plug-in module to direct\nconventional trackers towards achieving amodal perception.\nFor each object, amodal expander takes as input the modal\nbox b and features f, which can be retrieved from the\ntracker, and generates the amodal bounding boxes ba.\nPredicting amodal boxes in a residual manner.\nIn prac-\ntice, the amodal expander is crafted in a residual manner,\nakin to the structure in two-stage detectors [23, 46] and\ntrackers [64]. As illustrated in Fig. 3, the tracker initially\nproduces region proposal features f and modal proposals\nb, and subsequently refines b through a regression head R\nby predicting a modal box delta \u2206b. Our amodal expander\nreceives modal box delta \u2206b and object feature f as input,\ngenerating amodal box delta. This delta is then applied to\nthe modal proposal b to generate amodal boxes ba, denoted\nas E(\u2206b, f) + b \u2248 ba. The training of the amodal ex-\npander basically follows the training of regression head [46]\nby matching box proposals with a ground truth and applying\nregression loss. We first match modal box predictions b to a\nmodal ground truth b\u2217. Then, we apply the regression loss,\n5\nDetection Metrics\nTracking Metrics\nMethod\nAP[0,0.1]\nAP[0.1,0.8]\nAP[0.8,1]\nAPOoF\nOverall\nOverall\nAP[0,0.8]\nGTR [64]\n0.29\n37.15\n71.49\n42.07\n53.81\n17.47\n14.39\nFT regression head\n0.41\n49.32\n78.93\n53.26\n61.36\n20.44\n18.74\nAmodal Expander\n2.26\n71.64\n84.07\n73.74\n74.22\n26.77 (+9.30)\n28.94\nAmodal Expander\u2020\n2.46 (+2.17)\n71.86 (+34.71)\n84.21 (+12.72)\n73.96 (+31.89)\n74.34 (+20.53)\n26.72\n28.95 (+14.56)\nAmodal Expander + PnO\n1.94\n69.87\n83.86\n72.58\n73.20\n26.68\n28.76\nAmodal Expander + PnO\u2020\n1.99\n70.23\n84.00\n72.85\n73.38\n26.61\n28.64\nTable 4. Evaluating People on TAO-Amodal validation set. We follow the conventions of Table 3 but evaluate performance only on the\npeople category. Here, our Amodal Expander improves over the baseline by more than 30% AP for partially occluded and out-of-frame\npeople. This notable improvement could also be observed in the qualitative results in Figure 5. Training without PasteNOcclude (PnO)\nyields a slight performance drop. We posit that this dramatic performance increase comes from the fact that people is the most common\ncategory. Our results suggest that adding synthetic (occluded) examples is more helpful for less common categories.\nselected as smooth L1 [19], with the corresponding amodal\nground truth b\u2217\na:\nL(b, \u2206b, f) = Lreg(E(\u2206b, f) + b, b\u2217\na)\n(1)\nAs later shown in Table 5, the matching strategy is cru-\ncial in the training of the expander. Amodal expander ar-\nchitecture is introduced in Sec. 5.2. The effectiveness of\namodal expander is demonstrated in Tabs. 3 and 4.\n4.2. Crafting occlusion with PasteNOcclude (PnO)\nTo simulate occlusion scenarios during training, we intro-\nduced a data augmentation technique called PasteNOcclude\n(PnO). PasteNOcclude functions by integrating segments\nof objects into the original images to act as occludees.\nThe segment collection comprises 505k objects extracted\nfrom LVIS [21] and COCO [38] images using segmenta-\ntion masks. For each input image, we randomly select 1\nto 7 segments from the collection and paste them at arbi-\ntrary locations, allowing for partial extension beyond the\nimage boundary to replicate out-of-frame situations. Subse-\nquently, we incorporate the ground truth boxes of the pasted\nsegments into the original ground truth sets and apply re-\ngression loss according to the previously described pipeline.\nWe find that PnO leads to improvements improvements in\ndetection across all occlusion scenarios, shown in Table 3.\nWe posit that this synthetic strategy is particularly important\nfor the long-tailed nature of TAO-amodal, unlike COCO-\namodal, where a similar synthetic occlusion strategy leads\nto limited improvement [65]. We provide visual examples\nof such synthetic occlusions in the appendix.\n5. Empirical analysis\nOur experiments are categorized into two parts. In Sec-\ntion 5.1, we assess the challenges of amodal detection and\ntracking by evaluating various off-the-shelf trackers. The\nefficacy of the amodal expander is examined in Section 5.2.\nImplementation details and ablation study could be found\nin the appendix.\nDetection AP\nTracking AP\nMatching\nAP[0.1,0.8]\nAPOoF\nAP\nAP\nAP[0.1,0.8]\nModal\n13.96\n14.92\n28.64\n16.45\n8.96\nAmodal\n16.41\n17.64\n29.87\n16.35\n10.13\nTable 5. Ablation: Region proposal matching strategy. Given\nthat existing off-the-shelf trackers generate modal proposals, di-\nrect matching with amodal ground truth yields suboptimal results.\nOur observation indicates that an improved strategy involves pair-\ning region proposals with modal ground truth while applying re-\ngression loss to amodal predictions against the amodal ground\ntruth.\nBoth models are trained with PasteNOcclude (PnO) on\nTAO-Amodal training set for 20k iterations.\n5.1. Benchmarking state-of-the-art trackers\nEvaluation Metrics.\nUsing the estimated visibility at-\ntributes, we assess the tracking and detection capabilities\nof the model through variations of detection AP [38] and\nTrack-AP [10], representing the average precision across\nall categories at an IoU threshold of 0.5.\nWe label ob-\njects with visibility less than 0.1 as heavily occluded, evalu-\nated as AP[0.0, 0.1], where the superscript indicates the range\nof object visibility. If the visibility falls between 0.1 and\n0.8, we categorize them as partially occluded, while those\nwith visibility greater than 0.8 are considered non-occluded.\nObjects that stretch partially beyond the image boundary\nare referred to as out-of-frame (OoF) and evaluated with\nAPOoF. Additionally, we assess the model\u2019s performance on\nmodal annotations with Modal AP. In tracking, we evaluate\nhighly or partially occluded tracks (Track-AP[0, 0.8]), which\nare track with visibility at or below 0.8 for more than 5\nframes (seconds). We also evaluate performance on modal\nannotations (Modal Track AP). A table is provided for easy\nreference to each metric definition in the appendix.\n6\nModal\nAmodal\nModal\nAmodal\nOccluded to visible \nVisible to occluded\nOccluded to visible\ncar_(automobile)\nperson\nperson\nperson\nperson\nperson\nperso\nn\ncar_(automobile)\ncar_(automobile)\ncar_(automobile)\ncar_(automobile)\ncar_(automobile)\nperson\nperson\nperson\nperson\nperson\nperson\nperson\nperson\nperson\nperson\nperson\nperson\nOccluded across scene\nperson\nperson\nperson\nperson\nperson\nperson\nFigure 5. Qualitative results of Amodal Expander on TAO-Amodal val. Trackers augmented with our Amodal Expander produce\npredictions for both modal and amodal bounding boxes. In general, we are able to amodally complete objects that are occluded by objects\nin the scene (bottom-left) or objects that lie partially out of frame. Because we make use of online trackers that rely only on past evidence,\nwe verify that our module can amodally complete objects that were occluded in the past as well as objects that become occluded later.\nEvaluation on TAO-Amodal validation set.\nWe report\ndetection and tracking average precision (AP) numbers of\nSOTA off-the-shelf trackers on TAO-Amodal validation set\nrunning at 1fps with an IoU threshold 0.5 in Tab. 2. We also\nobserved similar performance trends when running at 5fps\nwith higher IoU thresholds, shown in the appendix. Ev-\nery off-the-shelf tracker was trained on either TAO [10] or\nLVIS [21], ensuring alignment of category vocabulary with\nour dataset. GTR [64] was trained on the combination of\nLVIS and COCO [38] by generating synthetic videos us-\ning the training strategy in [63]. ViTDet [37] was trained\non LVIS and combined with online SORT [4] tracker. We\nreproduced AOA [12] using their released implementation,\nwith object detector trained on LVIS and tracking ReID\nhead trained on TAO. QDTrack [17] and TET [36] followed\nsimilar training procedures, pretraining detectors on LVIS\nand training instance similarity heads on TAO.\nHow well do standard trackers generalize to amodal\nperception?\nTable 2 reveals notable differences in detec-\ntion AP between modal (Modal AP) and amodal annota-\ntions (AP), amounting to an 8.49% difference. Additionally,\nthe amodal tracking AP experiences a substantial decline\ncompared to modal tracking AP. These findings confirm our\nexpectation that existing trackers struggle to generalize ef-\nfectively to amodal perception.\nHow well do standard trackers handle occlusion?\nEx-\nisting off-the-shelf trackers exhibit reasonable performance\nin detecting non-occluded objects, with ViTDet achieving\n39.71% AP[0.8,1] as revealed in Table 2. However, all track-\ners face challenges in detecting heavily occluded objects,\nand there is a noticeable decline in performance in par-\ntially occluded (AP[0.1,0.8]) and out-of-frame (OoF) scenar-\nios.\nWe noticed that ViTDet operating at 5 fps benefits\nfrom the property of SORT to estimate the location in the\ncurrent frame using past information, resulting in improved\nresults shown in the appendix. Nevertheless, this improve-\nment comes at the cost of processing ViT-Det on 5x more\nframes than models running at 1 fps. In contrast, amodal\ncompletion could be a promising way for efficiently han-\ndling occlusion.\n5.2. Amodal expander experiments\nAmodal expander architecture could be referred in Fig. 4.\nWe built the expander on top of GTR [64] as this method\n7\nModal\nAmodal\nbook\nbook\ncar_(automobile)\ncar_(automobile)\ntruck\ntruck\nblanket\nblanket\nchair\nchair\nzebra\nzebra\nFigure 6. Qualitative results of Amodal Expander across diverse categories on TAO-Amodal val. Though we achieve the most\nimpressive results for people, our Amodal Expander is effective across a diverse set of categories.\nshows reasonable performance in both detection and track-\ning aspects in Table 2. The hidden dimension of MLP is\nselected as 256. ReLU [1] and dropout [48] with a proba-\nbility of 0.2 were applied to each layer except the last one.\nThe training of the amodal expander was conducted on the\nTAO-Amodal training set, along with PasteNOcclude (PnO)\nand augmentation used in [64]. All the modules except the\namodal expander are frozen during training. Other ablation\nstudy, hyperparameter details for training and PnO could be\nfound in the appendix.\nAmodal expander or fine-tuning?\nWe evaluated amodal\nexpander on TAO-Amodal validation set as shown in Tab. 3.\nAmodal expander trained with PnO for 45k iterations\nachieved 3.29% and 3.47% performance win under partially\noccluded (AP[0.1,0.8]) and out-of-frame (APOoF) scenario.\nWe also experimented with different fine-tuning strategies.\nFine-tuning entire model or solely the regression head and\nproposal network resulted in performance degradation. We\nposit that, with only 500 amodal training sequences, the\nmodels struggle to completely discard modal knowledge.\nFine-tuning box regression head is suboptimal when com-\npared to amodal expander. Amodal expander further pro-\nvides flexibility to adjust the architecture and select dif-\nferent input information, which are both important as later\nshown in the ablation section.\nDetecting people with amodal expander.\nIn Table 4, we\nstudy how well amodal expander detects and tracks peo-\nple, which serves as a crucial category in many autonomous\ndriving and tracking benchmarks. Amodal expander obtains\na near 2x dramatic improvement compared to the baseline,\nwith over 30% gain on AP[0.1, 0.8] and APOOF. Tracking on\nhighly or partially occluded people (Track-AP[0.0,0.8]) are\nalso increased by 14.56%. This shows that one can obtain\na strong amodal people tracker that could also track objects\nof diverse category vocabulary with our dataset.\nProposal matching strategy matters.\nTo apply regres-\nsion loss, training a box prediction head requires matching\neach region proposal to a ground truth box. A naive strat-\negy is to directly match the region proposals to the amodal\nground truth box. Nevertheless, as shown in Table 5, direct\nmatching with amodal boxes leads to suboptimal results.\nWe noted that since standard trackers generate modal region\nproposals, the model faced challenges in aligning proposals\nwith the accurate ground truth due to a low Intersection over\nUnion (IoU) between modal proposals and amodal ground\ntruth. Consequently, we opted to match region proposals\nwith modal bounding boxes and applied regression losses\nusing amodal ground truth boxes.\n6. Discussion\nIn this work, we focus on amodal perception of real-world\nobjects. We draw inspiration from cognitive functions of\namodal completion and object permanence in humans, that\ndevelop at an early age. Despite this, advancements in per-\nception stacks like object detection and tracking, do not\nmake amodal understanding central.\nWe bring focus to\nthree aspects/stages of building amodal perception stacks.\nFirst, we contribute a benchmark that annotates 880 cate-\ngories of objects amodally in unconstrained indoor and out-\ndoor settings, under partial and complete occlusion. Sec-\nond, we contribute a benchmarking protocol in the form\nof metrics that evaluate detection and tracking specifically\nfor the cases of partial or complete occlusions. Our key\nfinding here is that existing algorithms fail dramatically on\nthese metrics. To recover from this lack of performance on\namodal detection and tracking and as a third contribution,\nwe propose a light-weight and easy to train module, which\nwe term as the Amodal Expander. Given modal boxes of\nobjects, the expander learns to amodally complete the ob-\nject bounding boxes till the extent of their occlusion. This\nsimple fix gives us a boost of upto \u223c30 AP points, specifi-\ncally on the case of people.\n8\nAppendix\nPasted segment\n\u2026\nOriginal ground truth\n\u2026\n\u2026\n\u2026\n\u2026\nFigure 7. Synthetic occlusions with PasteNOcclude (PnO). PnO allows us to manually simulate occlusion scenarios and out-of-frame\nscenarios. We randomly choose 1 to 7 segments from a collection sourced from LVIS [21] and COCO [38] for pasting. For each inserted\nsegment, we randomly determine the object\u2019s size and position in the first and last frames. The size and location of the segment in\nintermediate frames are then generated through linear interpolation.\nIn this appendix, we extend our discussion of the pro-\nposed dataset and method within the context of tracking\nany object with amodal perception. Specifically, we discuss\ndetails about the training and PasteNOcclude technique in\nSection 7, provide further empirical analysis in Section 8,\nand outline the annotation guidelines of our dataset in Sec-\ntion 9. We further provide comprehensive video demon-\nstration of our dataset and qualitative results at https:\n//tao-amodal.github.io.\n7. Implementation details\nTraining Amodal Expander.\nWe trained amodal ex-\npander on TAO-Amodal training set for 20k iterations for\nall experiments unless specified. We used a 2-layer MLP\nas the architecture. The hidden dimension of MLP is se-\nlected as 256. ReLU [1] and dropout [48] with a probability\nof 0.2 were applied to each layer except the last one. Note\nthat amodal expander is compatible with any existing de-\ntector or tracker. However, in our experiments, we imple-\nmented the expander in conjunction with GTR [64]. Addi-\ntional architecture details of GTR align with the selection\nin [64]. We used 0.01 as the base learning rate and applied\nWarmupCosineLR [24] as the scheduler. The optimizer\nis selected as AdamW [40]. The batch size for training is\n4. We adopted the training methodology outlined in [64],\ntreating each image as an independent sequence. Data aug-\nmentation [63], which includes random cropping and resiz-\ning, was applied to each image to produce synthetic videos\nwith a length of 8 frames. Beyond this, we further applied\nPasteNOcclude, introduced in Sec. 4.2 in the main paper, on\ntop of the synthetic videos to automatically generate more\nocclusion scenarios. We provide the hyperparameter details\nof PasteNOcclude in the next section.\n9\nDetection AP0.5:0.95\nTracking AP0.5:0.95\nMethod\nAP[0,0.1]\nAP[0.1,0.8]\nAP[0.8,1]\nAPOoF\nModal AP\nAP\nAP\nAP[0,0.8]\nQDTrack [17]\n0.12\n2.29\n13.03\n2.90\n12.64\n8.53\n3.36\n1.52\nTET [36]\n0.21\n2.71\n17.27\n3.14\n17.58\n11.80\n1.99\n1.14\nAOA [12]\n0.26\n1.87\n15.98\n2.84\n16.36\n10.52\n6.59\n2.07\nViTDet-B + SORT [4, 37]\n0.33\n3.41\n19.67\n5.02\n19.83\n13.39\n3.03\n1.40\nViTDet-L + SORT [4, 37]\n0.43\n4.14\n22.08\n5.81\n22.65\n15.35\n4.16\n1.84\nViTDet-H + SORT [4, 37]\n0.36\n4.38\n23.62\n6.67\n23.89\n16.21\n4.24\n1.94\nGTR [64]\n0.24\n4.60\n26.01\n6.62\n26.83\n18.07\n7.52\n3.05\nTable 6. Off-the-shelf trackers on TAO-Amodal validation with higher IoU thresholds. The definitions of our evaluation metrics can\nbe found in Table 10. The AP numbers are averaged over 10 IoU values from 0.5 to 0.95 with a 0.05 step, denoted as AP0.5 : 0.95. We\nobserved a similar performance trend as results evaluated with an IoU threshold 0.5. We run all trackers at 1 fps.\nDetection AP\nTracking AP\nMethod\nAP[0,0.1]\nAP[0.1,0.8]\nAP[0.8,1]\nAPOoF\nModal AP\nAP\nAP\nAP[0,0.8]\nModal AP\nQDTrack [17]\n0.42\n7.59\n21.53\n7.78\n19.98\n15.42\n6.63\n2.72\n10.34\nTET [36]\n0.24\n5.39\n14.56\n4.73\n29.42\n10.51\n3.52\n2.21\n5.56\nAOA [12]\n0.56\n6.29\n24.35\n6.77\n23.51\n17.85\n12.82\n5.53\n20.67\nViTDet-B + SORT [4, 37]\n1.00\n13.38\n37.98\n14.78\n37.08\n28.32\n10.09\n4.40\n16.93\nViTDet-L + SORT [4, 37]\n1.32\n16.38\n43.30\n17.16\n42.31\n32.08\n11.75\n5.53\n19.22\nViTDet-H + SORT [4, 37]\n1.06\n17.24\n45.18\n18.58\n44.02\n33.53\n13.16\n5.87\n21.39\nGTR [64]\n0.57\n12.45\n35.89\n13.63\n34.92\n27.28\n13.70\n7.02\n20.09\nTable 7. Off-the-shelf trackers on TAO-Amodal validation set running at 5 fps. In contrast to results shown in Table 2 in the main paper,\nViTDet [37] with SORT [4] achieves a performance gain by running at a higher fps as SORT [4] leverages its capability to estimate the\nlocation in the current frame based on the location in previous frames. AP numbers are averaged across all categories at an IoU threshold\n0.5.\nDetection AP\nTracking AP\nMethod\nAP[0.1,0.8]\nAPOoF\nAP\nAP\nAP[0,0.8]\n\u2206b\n13.86\n14.79\n28.62\n16.47\n8.94\nf\n16.12\n17.08\n29.58\n16.12\n10.08\nf and \u2206b\n16.41\n17.64\n29.87\n16.35\n10.13\nTable 8. Input to Amodal Expander. Modal box (deltas) \u2206b,\noutput by the regression head as shown in Fig. 3 in the main pa-\nper, contains information about the exact location of modal box\npredictions. Object features f encompass the visual appearance\ninformation of the modal proposals. We found that both informa-\ntion are important in amodally inferring the object\u2019s shape. All\nmodels were trained on TAO-Amodal training set with PasteNOc-\nclude (PnO) for 20k iterations.\nPasteNOcclude (PnO).\nThe visual example of PnO is il-\nlustrated in Fig. 7. To collect the segments as occluders\nfrom LVIS [21] and COCO [38], we utilize segmentation\nmask to mask out the background area and employ the\nDetection AP\nTracking AP\n# layers\nAP[0.1,0.8]\nAPOoF\nAP\nAP\nAP[0,0.8]\n1-layer\n13.78\n15.19\n28.21\n14.29\n8.12\n2-layer\n16.41\n17.64\n29.87\n16.35\n10.13\n4-layer\n15.55\n17.02\n29.41\n16.35\n9.99\n6-layer\n14.55\n15.64\n28.79\n16.05\n9.09\nTable 9. Number of MLP layers in Amodal Expander. Empir-\nically, using a lightweight 2-layer MLP amodal expander proves\nsufficient to generate reasonable amodal detection results.\nAll\nmodels were trained on TAO-Amodal training set for 20k itera-\ntions.\nbounding box to crop the object. We filter out those seg-\nments where the mask area is less than 70% of the bounding\nbox area to ensure the object (occluder) is not occluded. In\nthe training process, we view each image as a sequence and\ncreate an 8-frame sequence employing the data augmenta-\ntion strategy outlined in [64] based on each single image.\n10\nMetric\nDefinition\nType\nAP\nAverage Precision (AP) averaged across all categories at an\nDetection Metrics\nIoU threshold 0.5.\nAP[0, 0.1]\nAP for heavily occluded objects, with visibility smaller than 0.1.\nAP[0.1, 0.8]\nAP for partially occluded objects, with visibility in [0.1, 0.8].\nAP[0.8, 1.0]\nAP for non-occluded objects, with visibility larger than 0.8.\nAPOoF\nAP for partially out-of-frame (OoF) objects.\nModal AP\nAP on modal annotations.\nTrack-AP [10]\nAverage Precision of a track averaged across all categories at an\nTracking Metrics\n3D IoU threshold 0.5.\nTrack-AP[0, 0.8]\nTrack-AP for any track that is occluded, with visibility\nat or below 0.8, for more than 5 frames (seconds).\nModal Track-AP\nTrack-AP on modal annotations\nTable 10. Evaluation metrics with IoU threshold 0.5. We define variations of AP [38] and Track-AP [10] based on levels of occlusion.\nOcclusion type\nExtent\nCases\nInstructions\nIn-frame\nPartial\nPartially occluded before being fully visible\nAnnotate with best estimate using category label\nPartially occluded after being fully visible\nAnnotate with best estimate\nComplete\nInvisible before being (partially) visible\nOnly annotate if the object has been visible before\nInvisible after being (partially) visible\nIf confident, annotate with best estimate\nIf not, only annotate till the last visible frame\nInvisible for a while\nIf confident, annotate with best estimate\nIf not, still annotate but add an uncertainty flag\nOut-of-frame\nPartial\nObject goes beyond image border\nOnly annotate inside the annotation workspace\nObject goes beyond the padded image\nClip at the border of the padded image\nComplete\n-\n-\nBehind-the-frame\nPartial\nObject is in front of and behind the camera\nOnly label the part of object in front of camera\nComplete\n-\n-\nTable 11. Annotation guidelines. TAO-Amodal is annotated with the guidelines above, which taxonomizes occlusions across severity\n(partial versus complete) and type (in/out-of-frame). As mentioned in Sec. 3 in the main paper, we scope out the case where an object may\nbe present behind the camera. For out-of-frame occlusions, we limit the annotation workspace to be twice the image size.\nSubsequently, we randomly select 1 to 7 segments from the\ncollection and place them at random locations. Addition-\nally, we randomly adjust the height and width of the inserted\nsegments within the range of [12, 192]. To maintain smooth\ntransitions between consecutive frames, we randomly deter-\nmine the object\u2019s location and size only in the first and last\nframes. The size and location in intermediate frames are\nobtained through interpolation.\n8. More empirical analysis\nWe used the same evaluation metrics defined in Section 5.1\nin the main paper. For easy reference, we summarize all\nthe definitions in Table 10. We presented additional experi-\nments involving state-of-the-art trackers in Section 8.1 and\nthe amodal expander in Section 8.2.\n8.1. Benchmarking off-the-shelf-trackers\nEvaluation with higher IoU thresholds.\nIn Table 6, we\nevaluate the trackers with average precision (AP) averaged\nover 10 IoU thresholds from 0.5 to 0.95 at a step 0.05. The\nperformance trend basically aligns with what we observed\nin Table 2 in the main paper. GTR [64] obtained strong\nperformance in both detection and tracking. When evalu-\nated with higher IoU thresholds, ViTDet [37] and SORT [4]\ndemonstrate inferior detection performance compared to\nGTR, indicating a contrasting outcome compared to the re-\nsults obtained at a 0.5 threshold. This shows the limitations\nof SORT [4] in accurately estimating bounding boxes.\n11\nRunning trackers at higher fps.\nWe reported the perfor-\nmance of state-of-the-art trackers running at 5 fps in Tab. 7.\nWe noticed that ViTDet [37] along with SORT [4] achieved\nthe best performance among all the trackers. This aligns\nwith our intuition as SORT estimates the location in the\ncurrent frame based on prior-frame locations. This property\nbenefits from running at higher fps. Nevertheless, achieving\nthis performance improvement requires processing ViTDet\non five times more frames than models operating at 1 fps,\nleading to increased computational demands.\n8.2. Amodal expander experiments\nInvestigating key information for amodal box inference.\nTable 8 reports different input choices to the amodal ex-\npander. Modal box (deltas) \u2206b, output by the regression\nhead as shown in Fig. 3 in the main paper, are used to\nyield final modal box predictions when applied to region\nproposals and thus contain information about the exact lo-\ncation of modal box predictions. Proposal features encom-\npass visual appearance information of the detected region\nproposals. Absence of visual cues significantly diminishes\nthe performance of both detection and tracking under occlu-\nsion. Interestingly, the amodal expander, incorporating both\nmodal delta and proposal features, yielded the most favor-\nable outcomes, indicating that, beyond visual information,\nestimating modal box locations also contributes to effective\namodal reasoning.\nNumber of MLP layers.\nWe tested with the depth of\namodal expander architecture in Table 9.\nWe observe a\nreverse-U pattern concerning the number of MLP layers,\nwith two-layer MLPs demonstrating superior performance\ncompared to other models. A one-layer MLP proves sub-\noptimal in both detection and tracking. Notably, using a\n1-layer MLP results in slightly inferior outcomes compared\nto fine-tuning the regression head, as indicated in Table 3\nin the main paper. Our contention is that the regression\nhead may derive benefits from pre-training on modal bench-\nmarks.\n9. Annotation guidelines\nWe ensure high-quality annotations by requiring annotators\nto follow the guidelines detailed in Table 11. Our coverage\nspans various occlusion scenarios, encompassing in-frame,\nout-of-frame, or behind-the-scene situations, where an ob-\nject may be partially obscured behind the camera.\n12\nReferences\n[1] Abien Fred Agarap. Deep learning using rectified linear units\n(relu). arXiv preprint arXiv:1803.08375, 2018. 8, 9\n[2] Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khu-\nrana, Achal Dave, Bastian Leibe, and Deva Ramanan. Burst:\nA benchmark for unifying object recognition, segmentation\nand tracking in video. In Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision, pages\n1674\u20131683, 2023. 2, 3\n[3] Ren\u00b4ee Baillargeon and Julie DeVos. Object permanence in\nyoung infants: Further evidence. Child development, 62(6):\n1227\u20131246, 1991. 1\n[4] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and\nBen Upcroft. Simple online and realtime tracking. In 2016\nIEEE international conference on image processing (ICIP),\npages 3464\u20133468. IEEE, 2016. 3, 5, 7, 10, 11, 12\n[5] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,\nGiancarlo Baldan, and Oscar Beijbom. nuscenes: A mul-\ntimodal dataset for autonomous driving.\narXiv preprint\narXiv:1903.11027, 2019. 2, 3\n[6] Jiarui Cai, Mingze Xu, Wei Li, Yuanjun Xiong, Wei Xia,\nZhuowen Tu, and Stefano Soatto.\nMemot: Multi-object\ntracking with memory.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8090\u20138100, 2022. 3\n[7] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-\njeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Pe-\nter Carr, Simon Lucey, Deva Ramanan, et al.\nArgoverse:\n3d tracking and forecasting with rich maps.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 8748\u20138757, 2019. 2, 3, 4\n[8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and\nLuc Van Gool. Domain adaptive faster r-cnn for object de-\ntection in the wild. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3339\u20133348,\n2018. 3\n[9] Anthony Cioppa, Silvio Giancola, Adrien Deliege, Le Kang,\nXin Zhou, Zhiyu Cheng, Bernard Ghanem, and Marc\nVan Droogenbroeck.\nSoccernet-tracking: Multiple object\ntracking dataset and benchmark in soccer videos. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 3491\u20133502, 2022. 1, 4\n[10] Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia\nSchmid, and Deva Ramanan.\nTao: A large-scale bench-\nmark for tracking any object.\nIn Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u2013\n28, 2020, Proceedings, Part V 16, pages 436\u2013454. Springer,\n2020. 1, 2, 5, 6, 7, 11\n[11] Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen\nShi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad\nSchindler, and Laura Leal-Taix\u00b4e.\nMot20: A benchmark\nfor multi object tracking in crowded scenes. arXiv preprint\narXiv:2003.09003, 2020. 2, 3\n[12] Fei Du, Bo Xu, Jiasheng Tang, Yuqi Zhang, Fan Wang,\nand Hao Li.\n1st place solution to eccv-tao-2020:\nDe-\ntect and represent any object for tracking.\narXiv preprint\narXiv:2101.08040, 2021. 1, 5, 7, 10\n[13] Yunhao Du, Zhicheng Zhao, Yang Song, Yanyun Zhao, Fei\nSu, Tao Gong, and Hongying Meng. Strongsort: Make deep-\nsort great again. IEEE Transactions on Multimedia, 2023. 3\n[14] Kiana Ehsani, Roozbeh Mottaghi, and Ali Farhadi. Segan:\nSegmenting and generating the invisible. In CVPR, 2018. 3\n[15] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88(2):303\u2013338, 2010. 1\n[16] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia\nYu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.\nLasot: A high-quality benchmark for large-scale single ob-\nject tracking. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5374\u20135383,\n2019. 1, 3\n[17] Tobias Fischer, Thomas E Huang, Jiangmiao Pang, Linlu\nQiu, Haofeng Chen, Trevor Darrell, and Fisher Yu. Qdtrack:\nQuasi-dense similarity learning for appearance-only multi-\nple object tracking. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2023. 1, 5, 7, 10\n[18] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In CVPR, pages 3354\u20133361. IEEE, 2012. 2\n[19] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 1440\u20131448,\n2015. 4, 6\n[20] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Car-\noline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,\nGeorge Toderici, Susanna Ricco, Rahul Sukthankar, et al.\nAva: A video dataset of spatio-temporally localized atomic\nvisual actions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 6047\u20136056,\n2018. 3\n[21] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5356\u20135364, 2019. 1, 3, 5, 6,\n7, 9, 10\n[22] Xiaotian Han, Quanzeng You, Chunyu Wang, Zhizheng\nZhang, Peng Chu, Houdong Hu, Jiang Wang, and Zicheng\nLiu. Mmptrack: Large-scale densely annotated multi-camera\nmultiple people tracking benchmark. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 4860\u20134869, 2023. 2\n[23] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017. 5\n[24] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Jun-\nyuan Xie, and Mu Li. Bag of tricks for image classifica-\ntion with convolutional neural networks. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 558\u2013567, 2019. 9\n[25] Cheng-Yen Hsieh, Chih-Jung Chang, Fu-En Yang, and Yu-\nChiang Frank Wang. Self-supervised pyramid representation\nlearning for multi-label visual analysis and beyond. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision, pages 2696\u20132705, 2023. 1\n13\n[26] Yuan-Ting Hu, Hong-Shuo Chen, Kexin Hui, Jia-Bin Huang,\nand Alexander G Schwing. Sail-vos: Semantic amodal in-\nstance level video object segmentation-a synthetic dataset\nand baselines. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3105\u2013\n3115, 2019. 3\n[27] Yuan-Ting Hu, Jiahong Wang, Raymond A Yeh, and Alexan-\nder G Schwing. Sail-vos 3d: A synthetic dataset and base-\nlines for object detection and 3d mesh reconstruction from\nvideo data.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 1418\u2013\n1428, 2021. 3\n[28] Michael Kavsek. The influence of context on amodal com-\npletion in 5-and 7-month-old infants. Journal of Cognition\nand Development, 5(2):159\u2013184, 2004. 1\n[29] Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, and\nWilliam G Macready. A robust learning approach to domain\nadaptive object detection. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 480\u2013\n490, 2019. 3\n[30] Tarasha Khurana, Achal Dave, and Deva Ramanan. Detect-\ning invisible people. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 3174\u20133184,\n2021. 2, 3, 4\n[31] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami\nAbu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Ui-\njlings, Stefan Popov, Andreas Veit, Serge Belongie, Vic-\ntor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David\nCai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Mur-\nphy. Openimages: A public dataset for large-scale multi-\nlabel and multi-class image classification. Dataset available\nfrom https://github.com/openimages, 2017. 1, 3\n[32] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Fels-\nberg, Roman Pflugfelder, Luka \u02c7Cehovin Zajc, Tomas Vojir,\nGoutam Bhat, Alan Lukezic, Abdelrahman Eldesokey, et al.\nThe sixth visual object tracking vot2018 challenge results. In\nProceedings of the European conference on computer vision\n(ECCV) workshops, pages 0\u20130, 2018. 1\n[33] Laura Leal-Taix\u00b4e, Anton Milan, Ian Reid, Stefan Roth,\nand Konrad Schindler.\nMotchallenge 2015:\nTowards\na benchmark for multi-target tracking.\narXiv preprint\narXiv:1504.01942, 2015. 2\n[34] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang,\nLionel M Ni, and Heung-Yeung Shum. Mask dino: Towards\na unified transformer-based framework for object detection\nand segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3041\u20133050, 2023. 1\n[35] Ke Li and Jitendra Malik. Amodal instance segmentation. In\nECCV. Springer, 2016. 3\n[36] Siyuan Li, Martin Danelljan, Henghui Ding, Thomas E\nHuang, and Fisher Yu. Tracking every thing in the wild. In\nEuropean Conference on Computer Vision, pages 498\u2013515.\nSpringer, 2022. 1, 5, 7, 10\n[37] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. In European Conference on Computer Vision, pages\n280\u2013296. Springer, 2022. 1, 5, 7, 10, 11, 12\n[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision, pages 740\u2013755.\nSpringer, 2014. 1, 6, 7, 9, 10, 11\n[39] Huan Ling, David Acuna, Karsten Kreis, Seung Wook Kim,\nand Sanja Fidler.\nVariational amodal object completion.\nAdvances in Neural Information Processing Systems, 33:\n16246\u201316257, 2020. 3\n[40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 9\n[41] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and\nChristoph Feichtenhofer. Trackformer: Multi-object track-\ning with transformers. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n8844\u20138854, 2022. 1\n[42] Anton Milan, Laura Leal-Taix\u00b4e, Ian Reid, Stefan Roth, and\nKonrad Schindler.\nMot16: A benchmark for multi-object\ntracking. arXiv preprint arXiv:1603.00831, 2016. 2, 3\n[43] Yumiko Otsuka, So Kanazawa, and Masami K Yamaguchi.\nDevelopment of modal and amodal completion in infants.\nPerception, 35(9):1251\u20131264, 2006. 1\n[44] Lu Qi, Li Jiang, Shu Liu, Xiaoyong Shen, and Jiaya Jia.\nAmodal instance segmentation with KINS dataset. In CVPR,\n2019. 2, 3\n[45] N Dinesh Reddy, Minh Vo, and Srinivasa G Narasimhan.\nCarfusion: Combining point tracking and part detection for\ndynamic 3d reconstruction of vehicles. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 1906\u20131915, 2018. 2\n[46] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Advances in neural information pro-\ncessing systems, pages 91\u201399, 2015. 1, 5\n[47] Gunnar A Sigurdsson, G\u00a8ul Varol, Xiaolong Wang, Ali\nFarhadi, Ivan Laptev, and Abhinav Gupta.\nHollywood in\nhomes: Crowdsourcing data collection for activity under-\nstanding.\nIn Computer Vision\u2013ECCV 2016: 14th Euro-\npean Conference, Amsterdam, The Netherlands, October 11\u2013\n14, 2016, Proceedings, Part I 14, pages 510\u2013526. Springer,\n2016. 3\n[48] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: a simple way\nto prevent neural networks from overfitting. The journal of\nmachine learning research, 15(1):1929\u20131958, 2014. 8, 9\n[49] Colton Stearns, Davis Rempe, Jie Li, Rares\u00b8 Ambrus\u00b8, Sergey\nZakharov, Vitor Guizilini, Yanchao Yang, and Leonidas J\nGuibas. Spot: Spatiotemporal modeling for 3d object track-\ning.\nIn European Conference on Computer Vision, pages\n639\u2013656. Springer, 2022. 3\n[50] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\nYuning Chai, Benjamin Caine, et al. Scalability in perception\nfor autonomous driving: Waymo open dataset. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2446\u20132454, 2020. 2\n14\n[51] Peize Sun, Jinkun Cao, Yi Jiang, Zehuan Yuan, Song Bai,\nKris Kitani, and Ping Luo. Dancetrack: Multi-object track-\ning in uniform appearance and diverse motion. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 20993\u201321002, 2022. 4\n[52] Bart Thomee, David A Shamma, Gerald Friedland, Ben-\njamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and\nLi-Jia Li. Yfcc100m: The new data in multimedia research.\nCommunications of the ACM, 59(2):64\u201373, 2016. 3\n[53] Pavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien\nGaidon.\nLearning to track with object permanence.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 10860\u201310869, 2021. 3\n[54] Pavel Tokmakov, Allan Jabri, Jie Li, and Adrien Gaidon. Ob-\nject permanence emerges in a random walk along memory.\narXiv preprint arXiv:2204.01784, 2022.\n[55] Basile Van Hoorick, Pavel Tokmakov, Simon Stent, Jie Li,\nand Carl Vondrick. Tracking through containers and occlud-\ners in the wild. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13802\u2013\n13812, 2023. 3\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 4\n[57] Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon\nLuiten, Berin Balachandar Gnana Sekar, Andreas Geiger,\nand Bastian Leibe. Mots: Multi-object tracking and segmen-\ntation. In Proceedings of the ieee/cvf conference on computer\nvision and pattern recognition, pages 7942\u20137951, 2019. 1\n[58] Benjamin Wilson, William Qi, et al. Argoverse 2.0: Next\ngeneration datasets for self-driving perception and forecast-\ning. In NeuRIPS Datasets and Benchmarks Track (Round 2),\n2021. 2\n[59] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple\nonline and realtime tracking with a deep association metric.\nIn 2017 IEEE international conference on image processing\n(ICIP), pages 3645\u20133649. IEEE, 2017. 3\n[60] Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming\nYang, and Junsong Yuan. Track to detect and segment: An\nonline multi-object tracker. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 12352\u201312361, 2021. 3\n[61] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying\nChen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-\nrell. Bdd100k: A diverse driving dataset for heterogeneous\nmultitask learning. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n2636\u20132645, 2020. 1, 3, 4\n[62] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and\nZhicheng Yan.\nHacs: Human action clips and segments\ndataset for recognition and temporal localization. In Pro-\nceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 8668\u20138678, 2019. 3\n[63] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00a8ahenb\u00a8uhl.\nTracking objects as points. arXiv:2004.01177, 2020. 3, 7,\n9\n[64] Xingyi Zhou, Tianwei Yin, Vladlen Koltun, and Philipp\nKr\u00a8ahenb\u00a8uhl. Global tracking transformers. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 8771\u20138780, 2022. 1, 3, 5, 6, 7, 8, 9,\n10, 11\n[65] Yan Zhu, Yuandong Tian, Dimitris Metaxas, and Piotr\nDoll\u00b4ar. Semantic amodal segmentation. In CVPR, 2017. 2,\n3, 6\n15\n"
  },
  {
    "title": "MixRT: Mixed Neural Representations For Real-Time NeRF Rendering",
    "link": "https://arxiv.org/pdf/2312.11841.pdf",
    "upvote": "10",
    "text": "MixRT: Mixed Neural Representations For Real-Time NeRF Rendering\nChaojian Li *\nGeorgia Tech\ncli851@gatech.edu\nBichen Wu \u2020\nGen AI, Meta\nwbc@meta.com\nPeter Vajda\nGen AI, Meta\nvajdap@meta.com\nYingyan (Celine) Lin\nGeorgia Tech\nceline.lin@gatech.edu\nProject Page: https://licj15.github.io/MixRT\nAbstract\nNeural Radiance Field (NeRF) has emerged as a lead-\ning technique for novel view synthesis, owing to its impres-\nsive photorealistic reconstruction and rendering capability.\nNevertheless, achieving real-time NeRF rendering in large-\nscale scenes has presented challenges, often leading to the\nadoption of either intricate baked mesh representations with\na substantial number of triangles or resource-intensive ray\nmarching in baked representations. We challenge these con-\nventions, observing that high-quality geometry, represented\nby meshes with substantial triangles, is not necessary for\nachieving photorealistic rendering quality. Consequently,\nwe propose MixRT, a novel NeRF representation that in-\ncludes a low-quality mesh, a view-dependent displacement\nmap, and a compressed NeRF model. This design effec-\ntively harnesses the capabilities of existing graphics hard-\nware, thus enabling real-time NeRF rendering on edge de-\nvices. Leveraging a highly-optimized WebGL-based render-\ning framework, our proposed MixRT attains real-time ren-\ndering speeds on edge devices (over 30 FPS at a resolu-\ntion of 1280 \u00d7 720 on a MacBook M1 Pro laptop), better\nrendering quality (0.2 PSNR higher in indoor scenes of the\nUnbounded-360 datasets), and a smaller storage size (less\nthan 80% compared to state-of-the-art methods).\n1. Introduction\nNeural Radiance Field (NeRF), first introduced by [23],\nhas been established as the state-of-the-art (SotA) technique\nin novel view synthesis tasks, owing to its superior abil-\nity to deliver photorealistic rendering quality. Despite its\nremarkable capabilities, the practical application of NeRF,\nespecially in immersive interactions on edge devices, has\nbeen significantly hampered due to its slow rendering speed.\nRecognizing this limitation, several prior works have pro-\nposed various methods to enhance the efficiency of NeRF.\nThese methods, such as baking NeRF into more efficient\n*Work done while interning at Meta. \u2020Corresponding Author.\nStorage Size (MB)\nPSNR\nBakedSDF\nMeRF\nMixRT (Ours)\n28.5\n28.0\n27.0\n27.5\n100\n200\n300\n400\n500\nBetter\n31 FPS, 98 MB on Laptop\nFigure 1. Our proposed MixRT can enable real-time rendering (>\n30 FPS) at a resolution of 1280 \u00d7 720 on a Macbook M1 Pro\nlaptop with better rendering quality and smaller storage size com-\npared to SotA works on real-time NeRF rendering [28, 38].\nrepresentations like mesh [8] or sparse voxels [16], have\nachieved impressive results, demonstrating real-time ren-\ndering speed (greater than 30 FPS) on edge devices. Un-\nfortunately, these methods often fall short when applied to\nlarger-scale real-world scenes, either yielding unacceptably\nslow rendering speeds or requiring prohibitive storage re-\nsources. Efforts to overcome these challenges have typi-\ncally focused on baking NeRF into a high-quality geometry\nrepresentations (for instance, more than 10 million trian-\ngles [38]) or resorting to computationally costly ray march-\ning in the baked representations [28]. Despite offering par-\ntial solutions to the challenges, these approaches still suffer\nfrom inherent drawbacks related to efficiency and resource\nrequirements.\nUpon careful examination, we observe a critical insight\nthat differs from the established conventions. We identify\nthat a high-quality mesh is not necessary in the baked rep-\nresentations concerning rendering quality. Our observation\nsuggests that it is feasible to trade-off the complexity of\nthe baked mesh for a more refined representation of color\narXiv:2312.11841v4  [cs.CV]  22 Jan 2024\nfields (such as NeRF), thereby achieving a more favorable\nbalance between rendering quality and efficiency. Guided\nby these observations, we propose MixRT, a unique NeRF\nrepresentation, that mixes different neural representations\nfor real-time NeRF rendering. Specifically, the propsoed\nMixRT consists of (1) a low-quality mesh (approximately\n15 MB compared to over 300 MB in BakedSDF [38]) that\nprovides coarse geometric information of the scene, (2)\na view-dependent displacement map to calibrate the ray-\nmesh intersection points before fetching the corresponding\ncolor, and (3) a compressed NeRF model, in the format of\nan Instant-NGP [24] that provides the density and color\nof each sampled point. This innovative combination not\nonly ensures the preservation of rendering quality but also\nmaximizes the efficient utilization of available hardware re-\nsources, including Rasterizer, Texture Mapping Units, and\nSingle Instruction Multiple Data (SIMD) Units. This bal-\nance of resource allocation enables us to achieve real-time\nrendering speeds on edge devices while minimizing storage\nrequirements, making it an ideal solution for performance-\nconscious applications.\nIn summary, our key contributions are as follows:\n\u2022 Through our observations, we have discovered that\nachieving high rendering quality in novel view synthesis\ntasks does not require high-complexity geometry repre-\nsented by meshes with a vast number of triangles. This\nrevelation has sparked the concept of simplifying the\nbaked mesh and combining various neural representa-\ntions. As a result, we have experienced substantial im-\nprovements in both efficiency and flexibility, paving the\nway for more efficient and versatile rendering techniques.\n\u2022 We introduce an innovative NeRF representation, which\nconsists of three essential components:\na low-quality\nmesh, a view-dependent displacement map, and a com-\npressed NeRF model.\nThis carefully crafted design is\nspecifically optimized to fully harness the capabilities of\nrasterizers, texture mapping units, and SIMD units in cur-\nrent graphics hardware. As a result, it empowers us to\nachieve real-time NeRF rendering on edge devices with-\nout compromising on rendering quality.\n\u2022 In addition, we develop a highly optimized WebGL-based\nrendering framework, which allows our proposed MixRT\nto achieve SotA rendering quality (e.g., PSNR) vs. effi-\nciency (e.g., FPS and storage size) trade-offs.\n2. Related Works\n2.1. NeRF on Large-Scale Scenes\nPrevious works on NeRF rendering for large-scale real-\nworld scenes can be divided into two main categories.\nWorks in the first category divide the entire space into mul-\ntiple sub-spaces, assigning individual NeRFs with specific\nradii to each. Specifically, [32, 35] align the training of\nNeRFs for different sub-spaces with collected images in\nvarying lighting conditions; [37] takes this a step further,\nusing different NeRFs for views at varying scales, allow-\ning city-scale scene rendering; [13, 40] enhance the ren-\ndering quality by selecting or fusing outputs from multi-\nple sub-space NeRF models.\nWorks in the second cate-\ngory map the entire space into a specific bounded space.\nIn particular, [4] introduces the concept of using a con-\ntraction function to fold the unbounded scene domain into\na finite sphere; [28] later refines this function, making it\npiecewise for efficient computation of ray-AABB intersec-\ntions; [5, 33] subsequently improve the contraction function\nfurther to better handle multisample isotropic Gaussian and\nvoxel representations, respectively.\nIn our approach, MixRT, we employ the contraction\nfunction outlined in [4] to configure the NeRF model for the\nmapped finite sphere. Meanwhile, we retain the low-quality\nmesh and view-dependent displacement map in their orig-\ninal space. This allows us to capitalize on the optimized\nrasterization pipeline which is common to most graphics\nhardware.\n2.2. Real-Time NeRF Rendering\nReal-time rendering or view synthesis is a vital and chal-\nlenging problem in computer vision and graphics, given its\nsignificance in immersive interaction applications [1]. Early\ntechniques for real-time rendering are either dependent on a\nvast number of images from densely sampled viewpoints\nor compromised on rendering quality due to the lack of\nfine-grained geometry proxies during reconstruction. For\ninstance, [15, 19, 22] exploit light fields to interpolate tar-\nget images from densely sampled images directly, while\n[14, 30, 31] utilize multi-view stereo and structure-from-\nmotion pipelines to construct triangle meshes for real-time\nrendering. NeRF[23], on the other hand, employs a continu-\nous volumetric field, represented in a multi-layer perceptron\n(MLP) network format for scene reconstruction, achieving\nstate-of-the-art rendering quality thanks to the ease of opti-\nmizing MLP representations through gradient descent.\nFollowing NeRF\u2019s trailblazing results, subsequent works\nhave proposed \u201dbaking\u201d (i.e., pre-computing intermediate\nresults and storing them in buffers) NeRF models into more\nefficient representations to achieve NeRF\u2019s high-quality\nrendering with real-time speeds. These efficient representa-\ntions are well-optimized on existing graphics hardware and\ninclude triangle meshes or sparse voxels.\nIn particular, [16, 28, 36, 39] bake NeRF models into\nsparse voxels with compact storage formats, enabling real-\ntime rendering speeds with existing CUDA or WebGL\nAPIs. On the other hand,\n[8, 25, 27, 38] adopt triangle\nmeshes in the rendering pipeline, distilling them from pre-\ntrained NeRF models or training them from scratch with dif-\nferentiable rendering frameworks. Moreover, [6, 20] have\ndeveloped either fully convolution-based or MLP-based\nnetworks to reconstruct light fields and enable real-time\nNeRF rendering on mobile devices. However, their wider\napplication is limited either by platform-dedicated deploy-\nment tools [2] or is constrained to synthesizing front views\nonly. There also exist point-cloud-based works like [18]\nthat utilize point clouds as scene representations for faster\nrendering speeds. However, their approaches, heavily rely-\ning on custom CUDA kernels for computational efficiency,\nface limitations in terms of broader adaptability. Specifi-\ncally, the lack of compatibility with downstream computer\ngraphics toolchains (e.g., editing and making collision ani-\nmation in Blender [11]) limits their utility across a diverse\nrange of edge devices\nOur proposed MixRT is unique among the mentioned\nreal-time NeRF rendering methods, combining a low-\nquality mesh, a view-dependent displacement map, and a\ncompressed NeRF model in the Instant-NGP format [24].\nBy leveraging the rasterizers, texture mapping units, and\nSIMD units accessible by WebGL APIs on most exist-\ning devices, MixRT can achieve SotA rendering qual-\nity with real-time rendering speeds, suitable storage size,\nand memory requirements for large-scale real-world scenes\n(e.g., Unbounded-360 dataset [4]). Specifically, thanks to\nthe adoption of a rasterization-based rendering pipeline,\nour proposed MixRT not only supports multiple devices\nthrough a cross-platform graphics library but is also com-\npatible with existing computer graphics toolchains (e.g.,\ncollision detection from [17])1.\n3. Preliminaries\n3.1. NeRF Rendering Pipeline\nNeRF [23] offers photorealistic novel views by encoding\na continuous volumetric field of points, which intercept\nand emit light rays, within the parameters of an MLP net-\nwork.\nThe rendering process with NeRF involves three\nsteps. (1) To render each pixel in the target novel view, a\nray r = o + td is cast from the origin (such as the camera\u2019s\ncenter) of the target novel view o along direction d, which\npasses through the respective pixel. Here, t denotes the dis-\ntance between sampled points along this ray and the origin\no. (2) For each point distanced tk from the view origin o, its\nlocation o+tkd and direction d serve as inputs to the MLP\nnetwork (o + tkd, d) \u2192 (\u03c3k, ck), which then outputs the\ncorresponding density \u03c3k and an RGB color ck. These rep-\nresent the extracted features of that specific point. (3) Ad-\nhering to the principles of classical volume rendering [21],\nthe color C(r) of the pixel corresponding to the ray r can\nbe computed by integrating the features of the points along\n1The real-time online collision demonstration of our proposed\nMixRT is available at https://licj15.github.io/MixRT/\ncollision_viewer/\nthe ray. The following equation expresses this process:\nC(r) =\nN\nX\nk=1\nTk(1 \u2212 exp(\u2212\u03c3k(tk+1 \u2212 tk)))ck,\nwhere Tk = exp(\u2212\nk\nX\nj=1\n\u03c3j(tj+1 \u2212 tj)),\n(1)\nwhere N denotes the number of sampled points along the\nray r and Tk indicates the accumulated transmittance along\nthe ray r to the point o + tkd. This transmittance repre-\nsents the likelihood of the ray reaching this point without\nencountering any other points.\nTo further accelerate NeRF\u2019s reconstruction process,\nInstant-NGP [24] replaces the MLP network of vanilla\nNeRF [23] with a 3D embedding grid stored as a compact\n1D hash table. As a result, the computationally heavy MLP\ninferences in the standard NeRF, involving about 1 million\nFLOPs, are transformed into significantly less demanding\nembedding interpolation operations, requiring fewer than\n0.00005 million FLOPs. Specifically, for each queried point\nalong the rays passing through the pixels of training im-\nages, the embeddings of its eight nearest vertices in the 3D\nembedding grid are retrieved from the compact 1D hash\ntable using their respective table index that is determined\nby their coordinates. The embeddings of the queried point\nare then obtained through trilinear interpolation of these\neight embeddings. After retrieving the embeddings for the\nqueried points along the rays passing through the pixels as\ndescribed above, these embeddings are fed into a smaller\nMLP model to obtain the corresponding density and view-\ndependent color.\nUnlike the vanilla NeRF which employs an MLP with\n10 layers, each with 256 hidden units, this smaller MLP\ncomprises only 2 layers with 64 hidden units each [24].\nAs discussed in Sec. 3.2 and recent real-time NeRF ren-\ndering studies [28, 38], Instant-NGP[24] retains the stor-\nage efficiency of vanilla NeRF due to the compact 1D hash\ntable but can only achieve real-time rendering speeds on\nhigh-end GPUs, such as RTX 3090Ti [26]. The challenge\nremains to enable real-time rendering of large-scale real-\nworld scenes using Instant-NGP while maintaining storage\nefficiency. As we analyze in Sec. 4.1, directly combining\nlow-quality meshes with Instant-NGP retains storage effi-\nciency, but fails to achieve real-time rendering speeds. In-\nformed by the profiling in Sec. 4.2, our proposed MixRT\nmodifies the model structure of Instant-NGP to better align\nwith the WebGL framework, making it more accessible for\nmost existing devices equipped with browsers.\n3.2. Discussion on Existing NeRF Representations\nAs detailed in Sec.2.2, prior works have explored the adop-\ntion of alternative, more efficient NeRF representations in\nTable 1. Overview of Commonly-Used NeRF Representations\nRepresentations\nRendering Quality\nFPS\nVRAM Efficiency\nStorage Efficiency\nHardware\nMLP Network [23, 32]\nHigh\nLow\nHigh\nHigh\nSIMD Units\nTriangle Mesh [8, 27, 38]\nMedium\nHigh\nLow\nLow\nRasterizer\nSparse Voxels [16, 39]\nMedium\nMedium\nLow\nLow\nTexture/SIMD Units\nPlane/Vector [7, 28]\nMedium\nMedium\nMedium\nMedium\nTexture/SIMD Units\nHash Table [5, 24]\nHigh\nLow\nHigh\nHigh\nSIMD Units\nlieu of MLP networks for the purpose of real-time render-\ning. Yet, to date, no NeRF representation has been able to\nsimultaneously meet the criteria of delivering high render-\ning quality (e.g., measured by PSNR), ensuring real-time\nframe rates, optimizing VRAM efficiency (which translates\nto minimized memory allocation during the rendering pro-\ncess), and maximizing storage efficiency (implying a com-\npact model size that\u2019s conducive for efficient data transmis-\nsion between users). This collective performance assess-\nment is comprehensively summarized in Tab. 1.\nSpecifically, the MLP network used in the vanilla NeRF\nmodel [23] excels in photorealistic rendering quality. Fur-\nthermore, it is highly efficient in terms of storage and mem-\nory usage, requiring only about 5 MB of network weights\nfor each scene in the NeRF-Synthetic dataset [23]. As such,\nit is a popular choice in subsequent research focusing on\nhigh-fidelity, large-scale NeRF rendering [4, 32]. However,\nit has a significant limitation: there are no well-optimized\naccelerators available in the current graphics hardware to\nrun this type of network efficiently (i.e., only SIMD units\nsuch as CUDA cores can execute the model). This results\nin slower rendering speeds, which restricts its application in\nscenarios requiring real-time interactions.\nDriven by the fact that most existing edge devices sup-\nport triangle mesh effectively within their hardware ras-\nterizer, studies such as [8, 27, 38] construct their rendering\npipelines based on the mesh rasterization process. Utiliz-\ning triangle mesh as NeRF representations significantly im-\nproves FPS, enabling real-time rendering speeds even on\nmobile devices [8], while maintaining a respectable render-\ning quality (i.e., only 0.1 lower PSNR than that of vanilla\nNeRF on the NeRF-Synthetic dataset [8]). Nevertheless,\nthe approach\u2019s scalability remains a concern for large-scale\nreal-world scenes, as the requirements for storage and mem-\nory increase proportionally with the scale of the scene, lead-\ning to over 400 MB of disk usage on the Unbounded-360\ndataset [38].\nIn pursuit of a better balance between the MLP network\nwith costly volumetric ray casting and the triangle mesh\nwith efficient rasterization, prior works like [16, 39] have\nproposed replacing the MLP network with sparse voxels,\nwhile still employing volumetric ray casting for the render-\ning process. Leveraging the compressed format of sparse\nvoxels (for instance, densely packed 3D texture in [16]) and\nthe same volumetric ray casting technique used by vanilla\nNeRF, these works achieve a respectable compromise be-\ntween rendering quality and FPS. They utilize either the tex-\nture mapping units accessible via WebGL API or the SIMD\nunits accessible through CUDA APIs. However, akin to\nthe triangle mesh representations, scaling these methods to\nlarge-scale scenes can pose significant challenges in terms\nof memory and storage efficiency, as pointed out by [28].\nIn the effort to enhance the memory and storage effi-\nciency of sparse voxels, studies such as [7, 28] propose\nusing plane/vector as NeRF representations, which can be\nperceived as the low-rank decomposed format of 3D voxels.\nBy employing a similar rendering pipeline and hardware as\nused by sparse voxels, yet with a more compact represen-\ntation alongside a distinct decoding method (for instance,\nmatrix-vector outer product in [7]) for embeddings of the\nsampled points, these studies manage to maintain similar\nor even superior rendering quality vs. FPS trade-offs when\ncompared to those using sparse voxels. In addition, they\nsignificantly reduce the memory and storage requirements\n(e.g., only requiring 188 MB vs. 3785 MB of disk space as\nper [28]).\nFollowing [24] to employ a hash table as a new NeRF\nrepresentation, numerous subsequent studies have sought\nto enhance its balance between rendering quality and ef-\nficiency, given its state-of-the-art training speed. As vali-\ndated by [24, 28], hash tables exhibit superior memory and\nstorage efficiency compared to sparse voxels or plane/vector\n(for instance, only requiring \u223c100 MB vs. \u223c200 MB or\n\u223c400 MB of disk space in the Unbounded-360 dataset [24,\n28]). Consequently, they are often used as the NeRF rep-\nresentation during training, before being translated into\nother representations [24, 28]. Additionally,[5] also affirms\nthat a hash table representation with a well-designed point\nsampling strategy in ray casting can yield superior ren-\ndering quality compared to MLP network representations.\nNonetheless, the hash table proposed in Instant-NGP[24] is\nconstrained by its limited compatibility with most devices,\nthus only achieving real-time rendering speeds on high-end\nGPUs with customized CUDA kernel for accessing SIMD\nunits in graphic hardware.\nMotivated by the aforementioned comparison, discus-\nSH Map\nScale Map\nSH\nFunction\nLow-Quality Mesh\nHash Table\n's Eight Nearest\nVertices in 3D Grid\nTexture Atlas\nHash\nFunction\nMLP\nR G B A\nInterpolated\nEmbeddings\nImage to Be\nRendered\nMesh\nView-Dependant Displacement Map\nHash Table\nRasterizer\nTexture Mapping Units\nSIMD Units\nMixed NeRF\nRepresentations\nHardware\nResources\nFully Harness\nCapabilities of\nFigure 2. An overview of our proposed MixRT rendering pipeline: MixRT integrates three core components: a low-quality mesh, a\nview-dependent displacement map, and a NeRF model compressed into a hash table. This combination aims to maximize utilization\nof diverse hardware resources. To render an image pixel: (1) We use rasterizer hardware to perform mesh rasterization, determining the\nray-mesh intersection point, p. (2) Leveraging texture mapping units, we use texture coordinates to access maps containing the spherical\nharmonics (SH) coefficients and scale, computing the calibrated point, pcali. (3) Lastly, pcali is processed by SIMD units, retrieving\nembeddings for its eight closest vertices from the 3D grid stored as a hash table. A small MLP network then converts these interpolated\nembeddings into the final rendered color.\nsion, and analysis of previous works, we propose a new\nform of NeRF representation. Our proposed MixRT mixes\na low-quality mesh, a view-dependent displacement map,\nand a compressed NeRF model in Instant-NGP\u2019s hash ta-\nble [24] format.\nThis configuration is purposefully de-\nsigned to leverage the inherent strengths of rasterizers, tex-\nture mapping units, and SIMD units in current graphics\nhardware. The proposed method empowers real-time NeRF\nrendering on edge devices while maintaining better render-\ning quality (e.g., 0.2 PSNR higher on indoor scenes of the\nUnbounded-360 dataset), and staying within smaller mem-\nory and storage parameters (e.g., consuming only 80% of\n[28]\u2019s disk usage), as compared to SotA methods.\n4. Method\nIn this section, we first examine the relationship between\nmesh quality and rendering quality in Sec.4.1, finding that,\nwith the help of color fields that are represented by hash ta-\nbles, high-quality novel view synthesis doesn\u2019t necessarily\ndemand meshes with extensive triangles. We then perform\nthe runtime profiling analysis on hash tables in Sec.4.2,\npinpointing bottlenecks to inform hash table configuration\nadjustments for improved FPS. Finally, we unveil MixRT,\nwith the detailed design describled in Sec. 4.3, comprising:\n(1) a low-quality mesh, (2) a view-dependent displacement\nmap, and (3) an compressed NeRF model stored in a hash\ntable. This design ensures MixRT\u2019s rendering pipeline, il-\nlustrated in Fig. 2, can be specifically optimized to fully har-\nness the capabilities of rasterizers, texture mapping units,\nand SIMD units in current graphics hardware, enabling real-\ntime NeRF rendering on edge devices without rendering\nquality sacrifice.\n4.1. Observations on the Effect of Mesh Quality\nPrevious real-time NeRF rendering research that uses tri-\nangle mesh as the NeRF representation highlights the im-\nportance of mesh geometry quality for photorealistic ren-\ndering [34, 38]. For instance, [34] delves into refining the\nsurface by adjusting vertex positions and face density. Yet,\nwe note that an ultra-detailed mesh, packed with a vast num-\nber of triangles, isn\u2019t mandatory for photorealistic rendering\noutcomes.\nIn particular, we made the above observations by (1) sim-\nplifying the SotA high-quality mesh from [38] via the clas-\nsical vertex clustering [41] and (2) querying the color of the\nray-mesh intersection points from a color field represented\nby Instant-NGP [24]\u2019s hash table. As summarized in Tab. 2,\nthe low-quality mesh contains more than 5 \u00d7 fewer trian-\ngles and faces as compared to the high-quality one but can\nachieve \u223c 0.3 higher PSNR than the high-quality mesh by\nequipping a hash table as a color filed to query the color\nfrom. Thanks to the high memory and storage efficiency of\nInstant-NGP [24]\u2019s hash table, the combination described\nabove only consumes 0.26 \u00d7 storage size of the high-quality\nmesh. However, as suggested in prior works [28, 38] and\nthe discussion in Sec. 3.2, the achieved FPS by the combi-\nnation of simplified mesh and hash table can not satisfy the\nreal-time rendering requirements. The set of conducted ex-\nperiments implies that (1) the high-quality geometry infor-\nmation represented by mesh with massive triangles is not\nnecessary for achieving high rendering quality and (2) re-\nplacing high-quality mesh with the combination of the sim-\nplified low-quality mesh and a hash table as the NeRF rep-\nresentation can achieve better PSNR vs. storage efficiency\ntrade-offs.\nFrom the above observations, it is evident that the pri-\nmary limitation of merging a low-quality mesh with other\nrepresentations is the rendering speed. Therefore, we con-\nduct an in-depth runtime profiling analysis on the hash table\nrepresentation which is known for its memory and storage\nefficiency, as detailed in Sec. 4.2.\n4.2. Runtime Profiling Analysis\nSince there is no existing runtime breakdown analysis tool\nfor WebGL, we perform the runtime profiling analysis on\nhash tables by tuning the model structure and observing the\nresulting FPS. Specifically, as summarized in Tab. 3, the ef-\nfect of varying (1) hash table size, (2) number of levels, and\n(3) MLP architectures on FPS implies that the number of\nlevels is the runtime bottleneck, i.e., tuning it can signifi-\ncantly change the resulting FPS, while the other two factors\nare not.\nMotivated by the profiling analysis above, we propose to\nmodify the default model structure of Instant-NGP\u2019s hash\ntable by shrinking the number of levels while enlarging the\nhash table size. As suggested in Tab. 4, such modifications\ncan boost the rendering speed to > 30 FPS while maintain-\ning hash tables\u2019 memory or storage efficiency.\n4.3. Mixing Mesh, Texture Map, and NeRF\nWith the observation that high-quality mesh is not neces-\nsary for achieving high rendering quality (see Sec. 4.1) and\nthe profiling-inspired hash table configuration can achieve\nboth high rendering speed and high memory or storage ef-\nficiency (see Sec. 4.2), we propose a type of NeRF repre-\nsentation that comprises (1) a low-quality mesh, (2) a view-\ndependant displacement map, and (3) a compressed NeRF\nmodel in Instant-NGP [24]\u2019s hash table format. Such a de-\nsign can not only leverage the commonly agreed high ren-\ndering quality, high renderings speed, and high memory and\nstorage efficiency of existing NeRF representations as dis-\ncussed above but also fully leverage the rasterizers, texture\nmapping units, and SIMD units in graphics hardware. We\nsummarize the rendering pipeline of our proposed MixRT\nin Fig. 2 and detail the design of each part in our proposed\nMixRT as follows.\n4.3.1\nTriangle Mesh\nWe leverage the standard triangle mesh format to include\nthe information on geometric vertices coordinates, texture\ncoordinates, and polygonal face elements. Unlike [38], we\ndo not need to store the per-vertex appearance parameters\nbecause the color of the intersection points will be fetched\nfrom the hash table.\nFollowing [38], the mesh is post-\nprocessed by vertex order optimization [29] to allow higher\ncache hit rates for accessing neighboring triangles.\n4.3.2\nView-Dependant Displacement Map\nInspired by the commonly-used normal map [9, 10] that\nfakes the lighting of bumps and dents without using more\npolygons, we propose a view-dependant displacement map\nto calibrate the coordinate of the intersection points to be\ninputted in the color field represented by Instant-NGP\u2019s\nhash table. Similar to a normal map, our proposed view-\ndependant displacement map can fake more accurate coor-\ndinates of the intersection points without adding new poly-\ngons to the mesh. However, our proposed one can better fit\nInstant-NGP [24]\u2019s hash table and the corresponding ren-\ndering pipeline that only takes the coordinates and view di-\nrections as input instead of surface normal. While previous\nstudies [3, 12] use neural networks to predict displacement\nvectors for calibrating points in volumetric rendering, this\napproach is unfeasible for real-time on-device rendering. In\ncontrast, our method employs 2D maps for displacement\nprediction, leveraging the texture mapping units of graphics\nhardware.\nIn particular, the proposed view-dependant displacement\nmap consists of (1) a spherical harmonics (SH) map, mSH,\nto store the SH coefficients for guiding the encoded view\ndirections to output a view-dependant vector and (2) a scale\nmap ms to scale the outputted view-dependant vector to a\nproper length and thus the scaled vector can be used as the\ncalibration variable of the coordinate of the ray-mesh inter-\nsection points. The shape of the SH map mSH and scale\nmap ms is designed to be [Rm, Rm, 3 \u00d7 (DSH + 1)2] and\n[Rm, Rm, 1], respectively. Rm represents the resolution of\nthe map and DSH is the SH degree used in SH map mSH.\nTable 2. Comparison between (1) the high-quality triangle mesh from [38] and (2) the combination of the low-quality mesh (simplified\nfrom the high-quality mesh) and Instant-NGP\u2019s [24] hash table, in terms of the average PSNR vs. storage size or FPS trade-offs on the\nindoor scenes of Unbounded-360 dataset. The FPS was measured on a Macbook M1 Pro laptop with a resolution of 1280 \u00d7 720.\nRepresentations\n\u2193 # of Vertices on\n\u2193 # of Faces on\n\u2191 Avg. PSNR\n\u2193 Storage\n\u2191FPS\nRoom\nCounter\nKitchen\nBonsai\nRoom\nCounter\nKitchen\nBonsai\nMesh from [38]\n7,060,849\n11,950,574\n13,539,203\n13,343,679\n14,110,659\n23,892,064\n27,056,127\n26,679,898\n27.06\n542 MB\n120\nSimplified Mesh + Hash Table\n946,962\n1,572,959\n1,778,283\n1,750,341\n1,893,695\n3,147,635\n3,557,514\n3,501,683\n27.41\n139 MB\n0.4\nTable 3. Adjusting the model structure of Instant-NGP\u2019s hash ta-\nble [24]. FPS was measured on a Macbook M1 Pro laptop at a res-\nolution of 1280 \u00d7 720, and the fragment shader was set to query\nthe hash table for color once per pixel. \u201cHash table size\u201d and \u201c#\nof levels\u201d denote the maximum entries per level and the number of\nmulti-resolution levels in Instant-NGP\u2019s hash table, respectively.\nThe \u201cMLP architecture\u201d outlines the structure of the MLP respon-\nsible for transforming the embedding retrieved from the hash table\ninto RGB color.\n# of Levels\nHash Table Size\nMLP Architecture\nFPS\n8\n217\n2 Layers, 8 Hidden Neurons\n27\n8\n217\nRemoved\n30\n\u00ac Observation: MLP is not the runtime bottleneck\n8\n217\nRemoved\n30\n1\n217\nRemoved\n120\n\u00ac Observation: # of levels is the runtime bottleneck\n8\n217\nRemoved\n30\n8\n25\nRemoved\n35\n8\n222\nRemoved\n25\n\u00ac Observation: Hash table size is not the bottleneck\nGiven the coordinate p \u2208 R3 of an intersection point, its\ntexture coordinates pt \u2208 R2, and the corresponding view\ndirection d \u2208 R3, the calibrated coordinate pcali \u2208 R3 can\nbe computed as:\npcali = p + S(mSH(pt), d) \u00d7 ms(pt),\n(2)\nwhere S denotes the SH functions as used in [7].\nmSH(pt) \u2208 R(DSH+1)2 and ms(pt) \u2208 R represent the\nfeature interpolated from mSH and ms with coordinate pt,\nrespectively. As such, the calibrated coordinate pcali can be\ndetermined by the coordinate and view directions of the ray-\nmesh intersection point. The view-dependant displacement\nmap is quantized into 8 bits after training for both higher\nrendering speeds and memory or storage efficiency.\n4.3.3\nHash Table\nFor the hash table in the proposed MixRT, similar to the set-\ntings in Instant-NGP [24], it consists of (1) multiple levels\nof 1D hash tables with different corresponding 3D resolu-\ntions and (2) small MLP networks to convert the fetched\nTable 4.\nOptimizing Instant-NGP\u2019s hash table configurations\nbased on runtime profiling insights. FPS measurements were taken\non a Macbook M1 Pro laptop at a resolution of 1280 \u00d7 720, while\nPSNR evaluations were conducted on the indoor scenes of the\nUnbounded-360 dataset.\nMesh\n# of Levels\nHash Table Size\n\u2191Avg. PSNR\n\u2193Storage\n\u2191 FPS\n[38]\n-\n-\n27.06\n542 MB\n120\nSimplified\n16\n220\n27.41\n139 MB\n0.4\nSimplified\n4\n221\n26.63\n74 MB\n35\nembeddings from the hash table to density or color. How-\never, as illustrated in Sec. 4.2 we modify its model structure,\ni.e., shrinking the number of levels and enlarging the hash\ntable size, to improve its rendering speed. To be compatible\nwith the 2D texture mapping units that can be accessed by\nWebGL, we reshape the hash tables stored in 1D format to\n2D image format for exporting it to the WebGL rendering\nframework.\n5. Experiments\n5.1. Experiments Settings\n5.1.1\nBaselines, Datasets, and Metrics\nWe benchmark the proposed MixRT on challenging large-\nscale indoor scenes of the Unbounded-360 dataset [4] and\ncompare with the following three SotA real-time NeRF ren-\ndering works: (1) BakedSDF [38]: it leverages high-quality\nmesh with massive triangles, and gets the rendering results\nby mesh rasterization with appearance parameters stored in\neach vertex, (2) NeRFMeshing [27]: it also leverages high-\nquality mesh that is distilled from pre-trained NeRF model\nbut store the appearance parameters as texture maps, and (3)\nMeRF [28], which adopts tri-planes as the representations\nto store density and color information of the scene and gets\nthe rendering results by volumetric ray casting like vanilla\nNeRF [23]. The rendering quality is measured by PSNR,\nand the FPS is measured on a Macbook M1 Pro laptop at\nthe resolution of 1280 \u00d7 720, following the settings used\nin [28]. The memory or storage efficiency is measured by\nthe total file sizes of meshes in glTF format, textures in PNG\nformat, and scene configurations in JSON format.\n5.1.2\nImplementation Details\nWe implement our real-time rendering pipeline with We-\nbGL framework. Specifically, in the GLSL vertex shader,\nwe compute the coordinates of the ray-mesh intersection\npoints and their corresponding texture coordinates. In the\nGLSL fragment shader, we first calibrate the coordinate of\nthe intersection points with the texture coordinates and view\ndirections, following Eq. 2. Then, following the standard\npipeline of Instant-NGP [24], we loop over the hash tables\u2019\nnumber of levels to get the trilinear interpolated embed-\ndings from each level. The interpolated embeddings from\ndifferent levels are concatenated together to be the input of\na small MLP model that is implemented as matrix-vector\nmultiplication, following the implementation in [8]. In all\nour experiments, the hash table is configured to have four\nlevels with a minimum level resolution of 256 and a max-\nimum of 4096, and the hash map size is set as 221, with\neach entry holding a four-dimensional vector. For the view-\ndependant displacement map, the map resolution Rm is set\nas 1536 for all scenes. For the low-quality meshes, they\nTable 5.\nComparison between our MixRT and SotA real-time\nNeRF rendering techniques on the four indoor scenes from the\nUnbounded-360 Dataset [4]. PSNR values were measured using\nMip-NeRF-360\u2019s settings [4], while FPS was measured on a Mac-\nbook M1 Pro at a resolution of 1280 \u00d7 720, consistent with the\nexperiment settings in [28].\nMethod\n\u2191 PSNR on\n\u2191 FPS\n\u2193 Storage\nRoom\nCounter\nKitchen\nBonsai\nAvg.\nNeRFMeshing [27]\n26.13\n20.00\n23.59\n25.58\n23.83\n-\n-\nBakedSDF [38]\n-\n-\n-\n-\n27.06\n120\n542 MB\nMeRF [28]\n-\n-\n-\n-\n27.80\n30\n124 MB\nMixRT (Ours)\n29.88\n26.60\n27.46\n28.10\n28.01\n31\n98 MB\nare simplified from the SotA BakedSDF [38]\u2019s mesh by\nthe classical vertex cluster [41] algorithm in the space con-\ntracted by Mip-NeRF-360 [4]\u2019s contraction function with\nthe voxel size hyperparameter set as 0.01 for all scenes. The\nsimplified meshes, along with randomly initialized view-\ndependent displacement maps and hash tables, are jointly\ntrained using the loss function based on the differences be-\ntween the rendered images and the ground truth images.\n5.2. Comparing with SotA\nWe first compare our proposed MixRT with SotA real-time\nNeRF rendering works. As summarized in Tab. 5, the pro-\nposed MixRT achieves the highest PSNR and storage ef-\nficiency among all the methods in the benchmark, while\nmaintaining the real-time (> 30 FPS) rendering speed.\nSpecifically, as compared to MeRF [28], our proposed\nMixRT achieves 0.2 higher PSNR than it with only 80%\nstorage cost under the same rendering speed. Please refer to\nAppendix B for the corresponding qualitative comparison.\n5.3. Ablation Study\nAs highlighted in Sec. 4.1, incorporating hash tables into\nour MixRT framework is critical for maintaining high mem-\nory and storage efficiency without sacrificing rendering\nquality.\nFollowing this, we further conduct an ablation\nstudy to verify the significance of the view-dependent dis-\nplacement map, another integral component of MixRT. As\nshown in Tab. 6, removing the view-dependent displace-\nment map from our proposed MixRT reduces storage by\napproximately 24% but results in a 1.37 decrease in PSNR.\nMeanwhile, the rendering speed remains relatively stable,\nTable 6.\nComparison MixRT w/ and w/o the proposed view-\ndependent displacement (VDD) map, in terms of PSNR, FPS,\nand storage size on the indoor scenes of the Unbounded-360\ndataset [4].\nMethod\n\u2191 PSNR on\n\u2191 FPS\n\u2193 Storage\nRoom\nCounter\nKitchen\nBonsai\nAvg.\nMixRT w/ VDD Map\n29.88\n26.60\n27.46\n28.10\n28.01\n31\n98 MB\nMixRT w/o VDD Map\n29.10\n25.26\n25.64\n26.54\n26.64\n35\n74 MB\nTable 7.\nComparison between our MixRT and SotA real-time\nNeRF rendering techniques on the three publicly available outdoor\nscenes from the Unbounded-360 Dataset [4].\nMethod\n\u2191 PSNR on\nBicycle\nGarden\nStump\nAvg.\nVolumetric-Rendering-Based Methods\nMeRF [28]\n22.82\n25.32\n25.06\n24.40\nRasterization-Based Methods\nMobileNeRF [8]\n21.70\n23.53\n23.95\n21.06\nNeRFMeshing [27]\n21.15\n22.91\n22.66\n22.24\nMixRT (Ours)\n21.81\n24.55\n23.76\n23.37\nshifting from 31 FPS to 35 FPS. Considering that MixRT al-\nready achieved higher storage efficiency than all baselines,\nas demonstrated in Sec. 5.2, integrating view-dependent\ndisplacement maps in our MixRT is a better option to\nachieve higher PSNR vs. rendering speed trade-offs.\n6. Limitation\nAs illustrated in Tab. 5, our proposed MixRT demonstrates\nbetter rendering quality vs. rendering speeds and storage ef-\nficiency than SotA methods in indoor scenes. However, its\nrendering quality is still constrained by rasterization-based\nrendering methods, a common limitation in rasterization-\nbased real-time NeRF methods [8, 38]. In particular, for\nthe more complex outdoor scenes from the Unbounded-360\ndataset [4], as shown in Tab. 7, MixRT\u2019s rendering qual-\nity is 1 PSNR lower than the volumetric-rendering-based\nMeRF [28]. Despite this, it still achieves comparable or bet-\nter quality than other rasterization-based baselines [8, 27].\n7. Conclusion\nWe present MixRT, a NeRF representation that combines a\nlow-quality mesh, a view-dependent displacement map, and\na compressed NeRF in a hash table structure. This design\nemerges from our observation that achieving high render-\ning quality does not require high-complexity geometry rep-\nresented by meshes with a vast number of triangles. This\nrealization suggests the potential to streamline the baked\nmesh and incorporate diverse neural representations for ren-\ndering, memory, and storage efficiency. Through detailed\nruntime profiling analysis and an optimized WebGLbased\nrendering framework, MixRT offers state-of-the-art balance\nbetween rendering quality and efficiency.\nAcknowledgement\nChaojian Li and Yingyan (Celine) Lin would like to ac-\nknowledge the funding support from the National Science\nFoundation (NSF) Computing and Communication Founda-\ntions (CCF) programs (Award ID: 2211815 and 2312758).\nReferences\n[1] Tomas Akenine-Moller, Eric Haines, and Naty Hoffman.\nReal-time rendering. AK Peters/crc Press, 2019. 2\n[2] Inc. Apple.\nCore ml tools, 2023.\nhttps://github.\ncom/apple/coremltools, accessed 2023-08-01. 3\n[3] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael\nZollhoefer, Johannes Kopf, Matthew O\u2019Toole, and Changil\nKim.\nHyperreel:\nHigh-fidelity 6-dof video with ray-\nconditioned sampling.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 16610\u201316620, 2023. 6\n[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022. 2, 3, 4, 7, 8\n[5] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan,\nand\nPeter\nHedman.\nZip-nerf:\nAnti-\naliased grid-based neural radiance fields.\narXiv preprint\narXiv:2304.06706, 2023. 2, 4\n[6] Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai,\nJu Hu, Yun Fu, Denys Makoviichuk, Sergey Tulyakov, and\nJian Ren.\nReal-time neural light field on mobile devices.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8328\u20138337, 2023. 2\n[7] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. arXiv preprint\narXiv:2203.09517, 2022. 4, 7\n[8] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-\ndrea Tagliasacchi. Mobilenerf: Exploiting the polygon ras-\nterization pipeline for efficient neural field rendering on mo-\nbile architectures.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n16569\u201316578, 2023. 1, 2, 4, 7, 8\n[9] Paolo Cignoni, Claudio Montani, Claudio Rocchini, and\nRoberto Scopigno.\nA general method for preserving at-\ntribute values on simplified meshes. In Proceedings Visual-\nization\u201998 (Cat. No. 98CB36276), pages 59\u201366. IEEE, 1998.\n6\n[10] Jonathan Cohen,\nMarc Olano,\nand Dinesh Manocha.\nAppearance-preserving simplification. In Proceedings of the\n25th annual conference on Computer graphics and interac-\ntive techniques, pages 115\u2013122, 1998. 6\n[11] Blender Online Community. Blender - a 3d modelling and\nrendering package, 2018. 3\n[12] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang,\nWenyu Liu, and Qi Tian. Neusample: Neural sample field for\nefficient view synthesis. arXiv preprint arXiv:2111.15552,\n2021. 6\n[13] Jiading Fang, Shengjie Lin, Igor Vasiljevic, Vitor Guizilini,\nRares Ambrus, Adrien Gaidon, Gregory Shakhnarovich, and\nMatthew R Walter. Nerfuser: Large-scale scene representa-\ntion by nerf fusion. arXiv preprint arXiv:2305.13307, 2023.\n2\n[14] Yasutaka Furukawa, Carlos Hern\u00b4andez, et al.\nMulti-view\nstereo: A tutorial. Foundations and Trends\u00ae in Computer\nGraphics and Vision, 9(1-2):1\u2013148, 2015. 2\n[15] Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and\nMichael F Cohen. The lumigraph. In Proceedings of the\n23rd annual conference on Computer graphics and interac-\ntive techniques, pages 43\u201354, 1996. 2\n[16] Peter Hedman,\nPratul P Srinivasan,\nBen Mildenhall,\nJonathan T Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 5875\u20135884, 2021. 1, 2, 4\n[17] Stefan Hedman.\ncannon.js: Lightweight 3d physics for\nthe web, 2023. https://github.com/schteppe/\ncannon.js/tree/master, accessed 2023-08-01. 3\n[18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(TOG), 42(4):1\u201314, 2023. 3\n[19] Marc Levoy and Pat Hanrahan. Light field rendering. In Pro-\nceedings of the 23rd annual conference on Computer graph-\nics and interactive techniques, pages 31\u201342, 1996. 2\n[20] Zhong Li, Liangchen Song, Celong Liu, Junsong Yuan, and\nYi Xu. Neulf: Efficient novel view synthesis with neural 4d\nlight field. arXiv preprint arXiv:2105.07112, 2021. 2\n[21] Nelson Max. Optical models for direct volume rendering.\nIEEE Transactions on Visualization and Computer Graphics,\n1(2):99\u2013108, 1995. 3\n[22] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG), 38(4):1\u201314, 2019. 2\n[23] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In European conference on computer vision, pages\n405\u2013421. Springer, 2020. 1, 2, 3, 4, 7\n[24] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding.\narXiv preprint arXiv:2201.05989,\n2022. 2, 3, 4, 5, 6, 7\n[25] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi-\ndler. Extracting triangular 3d models, materials, and lighting\nfrom images. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8280\u2013\n8290, 2022. 2\n[26] NVIDIA LLC.\nGEFORCE RTX 3090 FAMILY, 2021.\nhttps://www.nvidia.com/en- us/geforce/\ngraphics - cards / 30 - series / rtx - 3090 -\n3090ti/, accessed 2022-06-01. 3\n[27] Marie-Julie Rakotosaona, Fabian Manhardt, Diego Mar-\ntin Arroyo, Michael Niemeyer, Abhijit Kundu, and Fed-\nerico Tombari.\nNerfmeshing: Distilling neural radiance\nfields into geometrically-accurate 3d meshes. arXiv preprint\narXiv:2303.09431, 2023. 2, 4, 7, 8\n[28] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srini-\nvasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Pe-\nter Hedman. Merf: Memory-efficient radiance fields for real-\ntime view synthesis in unbounded scenes. ACM Transactions\non Graphics (TOG), 42(4):1\u201312, 2023. 1, 2, 3, 4, 5, 7, 8\n[29] Pedro V Sander, Diego Nehab, and Joshua Barczak. Fast\ntriangle reordering for vertex locality and reduced overdraw.\nIn ACM SIGGRAPH 2007 papers, pages 89\u2013es. 2007. 6\n[30] Johannes\nLutz\nSch\u00a8onberger\nand\nJan-Michael\nFrahm.\nStructure-from-motion revisited.\nIn Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 2\n[31] Johannes Lutz Sch\u00a8onberger, Enliang Zheng, Marc Pollefeys,\nand Jan-Michael Frahm. Pixelwise view selection for un-\nstructured multi-view stereo.\nIn European Conference on\nComputer Vision (ECCV), 2016. 2\n[32] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-\nhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,\nand Henrik Kretzschmar. Block-nerf: Scalable large scene\nneural view synthesis. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8248\u20138258, 2022. 2, 4\n[33] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,\nBrent Yi, Terrance Wang, Alexander Kristoffersen, Jake\nAustin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: A\nmodular framework for neural radiance field development.\nIn ACM SIGGRAPH 2023 Conference Proceedings, pages\n1\u201312, 2023. 2\n[34] Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Er-\nrui Ding, Jingdong Wang, and Gang Zeng. Delicate textured\nmesh recovery from nerf via adaptive surface refinement.\narXiv preprint arXiv:2303.02091, 2023. 5\n[35] Haithem Turki,\nDeva Ramanan,\nand Mahadev Satya-\nnarayanan.\nMega-nerf:\nScalable construction of large-\nscale nerfs for virtual fly-throughs.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12922\u201312931, 2022. 2\n[36] Xiuchao Wu, Jiamin Xu, Zihan Zhu, Hujun Bao, Qixing\nHuang, James Tompkin, and Weiwei Xu. Scalable neural\nindoor scene rendering. ACM transactions on graphics, 41\n(4), 2022. 2\n[37] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,\nAnyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin.\nBungeenerf: Progressive neural radiance field for extreme\nmulti-scale scene rendering.\nIn European conference on\ncomputer vision, pages 106\u2013122. Springer, 2022. 2\n[38] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,\nPratul P Srinivasan, Richard Szeliski, Jonathan T Barron,\nand Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-\ntime view synthesis. arXiv preprint arXiv:2302.14859, 2023.\n1, 2, 3, 4, 5, 6, 7, 8\n[39] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. PlenOctrees for real-time rendering of\nneural radiance fields. In ICCV, 2021. 2, 4\n[40] Mi Zhenxing and Dan Xu. Switch-nerf: Learning scene de-\ncomposition with mixture of experts for large-scale neural\nradiance fields. In The Eleventh International Conference on\nLearning Representations, 2022. 2\n[41] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun.\nVer-\ntex clustering in Open3D: A modern library for 3D\ndata processing,\n2018.\nhttp : / / www . open3d .\norg/docs/release/tutorial/geometry/mesh.\nhtml#Vertex-clustering. 5, 8\nMixRT: Mixed Neural Representations For Real-Time NeRF Rendering\nSupplementary Material\nA. Real-Time Interactive Demonstration\nTo experience the real-time interactive demonstration of\nthe proposed MixRT, please visit https://licj15.\ngithub.io/MixRT/index.html#demos. Our demo\noffers real-time online interaction with static scenes, as well\nas collision animations.\nB. Visual Comparison with SotA\nIn addition to the quantitative comparison of our proposed\nMixRT and the SotA real-time NeRF rendering shown in\nTab 5, we offer additional rendered image comparisons in\nFig. 3 below. Consistent with the observations in Sec. 5.2,\nour proposed MixRT excels in two main areas: (1) accu-\nrately rendering regions with specular highlights or fine-\ngrained geometry structures, e.g., the bowl in \u201cScene:\nCounter\u201d and the bulldozer bucket in \u201cScene: Lego\u201d, and\n(2) eliminating ghostly effects such as the \u201dfloaters\u201d ob-\nserved on the floor of \u201cScene: Room\u201d and the wall of\n\u201cScene: Bonsai\u201d.\nScene: Room\nScene: Counter\nScene: Kitchen\nScene: Bonsai\nGround Truth\nMeRF\nOurs\nFigure 3. Visual comparison between our proposed MixRT and MeRF [28], a real-time NeRF rendering work with SotA rendering quality\nvs. efficiency trade-offs. The rendered images are randomly selected from the test set.\n"
  },
  {
    "title": "FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple Super-Resolution Pipeline",
    "link": "https://arxiv.org/pdf/2312.11537.pdf",
    "upvote": "6",
    "text": "FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with\nA Simple Super-Resolution Pipeline\nChien-Yu Lin*\nUniversity of Washington\nSeattle, WA, USA\ncyulin@cs.washington.edu\nQichen Fu, Thomas Merth, Karren Yang, Anurag Ranjan\nApple, Inc.\nCupertino, CA, USA\n{qfu22,tmerth,karren yang,anuragr}@apple.com\nFigure 1. Super-resolution (SR) can be used to improve neural rendering efficiency under a limited training budget. Comparison of\nTensoRF, FastSR-NeRF (ours), and MobileNeRF [13] on a consumer-grade MacBook Air M2 laptop. FastSR-NeRF employs a straightfor-\nward SR pipeline (TensoRF+SR), which can enhance rendering times and compress the model size while incurring relatively low training\noverhead. While state-of-the-art mobile models such as MobileNeRF can render very quickly, they cannot be trained on consumer devices\nunder a meaningful time budget.\nAbstract\nSuper-resolution (SR) techniques have recently been pro-\nposed to upscale the outputs of neural radiance fields\n(NeRF) and generate high-quality images with enhanced\ninference speeds.\nHowever, existing NeRF+SR methods\nincrease training overhead by using extra input features,\nloss functions, and/or expensive training procedures such as\nknowledge distillation. In this paper, we aim to leverage SR\nfor efficiency gains without costly training or architectural\nchanges. Specifically, we build a simple NeRF+SR pipeline\nthat directly combines existing modules, and we propose\na lightweight augmentation technique, random patch sam-\npling, for training. Compared to existing NeRF+SR meth-\nods, our pipeline mitigates the SR computing overhead and\n*Work done while interning at Apple.\ncan be trained up to 23\u00d7 faster, making it feasible to run on\nconsumer devices such as the Apple MacBook. Experiments\nshow our pipeline can upscale NeRF outputs by 2-4\u00d7 while\nmaintaining high quality, increasing inference speeds by up\nto 18\u00d7 on an NVIDIA V100 GPU and 12.8\u00d7 on an M1 Pro\nchip. We conclude that SR can be a simple but effective\ntechnique for improving the efficiency of NeRF models for\nconsumer devices.\n1. Introduction\nNeural Radiance Field (NeRF) models [31] have become\nintegral to many computer vision and computer graphics\ntasks, such as novel view synthesis [8, 29, 31], surface\nreconstruction [39, 45], camera pose estimation [41, 46]\nand 3D image generation [11, 28, 33].\nSince the origi-\nnal NeRF model cannot render images efficiently, a large\narXiv:2312.11537v2  [cs.CV]  20 Dec 2023\nbody of research [12, 17, 29, 32, 34, 37, 43] has been ded-\nicated to address the rendering efficiency. Many of these\nworks achieve impressive gains by decomposing and rep-\nresenting the 3D neural radiance field with explicit fea-\ntures [12, 13, 17, 21, 29, 32, 37]. However, these methods\noften require extended training times and/or specialized ar-\nchitectures and kernel support on high-end GPUs. For ex-\nample, MobileNeRF is capable of fast rendering on mobile\ndevices [13], but uses 8 server-class GPUs for training [6],\nwhich translates to over 15 days (>375h) on a consumer-\ngrade laptop (Figure 1, right). To improve the accessibility\nand personalized use of NeRFs, there is a need to explore\nefficient rendering techniques that can also be trained on\nconsumer-grade devices.\nIn this paper, we introduce FastSR-NeRF, which demon-\nstrates CNN-based super-resolution (SR) can be a simple,\nlow-cost technique for improving the efficiency of NeRF\nmodels on consumer devices. The basic idea is to train a\nsmall NeRF model to generate lower-resolution scene fea-\ntures with 3D consistency, and a fast SR model to gener-\nate higher-resolution features. This combination reduces\nthe number of pixels that need to be computed using the\nNeRF\u2019s slow volume rendering process, increasing render-\ning speed. While SR techniques for neural rendering have\nbeen proposed by previous works, these methods either (i)\ninvolve specialized SR modules that use extra input features\nsuch as high-resolution reference images [23]; (ii) employ\nexpensive training procedures such as distillation [10]; or\n(iii) are trained on tens of thousands of images within a gen-\nerative modeling framework [11]. None of these methods\ncan be feasibly trained on low-power, consumer-grade plat-\nforms. Whether it is possible to achieve high-quality results\nwith SR under a limited training budget remains an open\nquestion.\nHere, we address this question by exploring a simple\nNeRF+SR pipeline that directly combines existing mod-\nules.\nWe hypothesize that the spatial inductive bias of\nCNN-based SR is sufficient to generate high-quality out-\nputs for low upscaling ratios, even without extra input fea-\ntures or complex training procedures. To improve synthe-\nsis quality, we propose only a lightweight augmentation\ntechnique called random patch sampling: rather than ex-\ntract patches from an image grid for training the SR mod-\nule as done in existing works [23, 40], we extract patches\nfrom random positions to increase the diversity of im-\nage patches seen by the SR module. Experiments across\nthree datasets show, somewhat surprisingly, our simple\nNeRF+SR pipeline with low training overhead can achieve\ncomparable quality and greater rendering efficiency than ex-\nisting complex NeRF+SR pipelines. To summarize, the key\nresults of our study are as follows:\n\u2022 SR can be a nearly \u201cfree\u201d technique for improving\nneural rendering efficiency.\nOur comprehensive ex-\nperiments across three datasets show that applying SR\nto a NeRF model at 2-4\u00d7 upscaling ratios, without any\ncomplex training procedures or architectural modifica-\ntions, can improve inference speeds by up to 35.7\u00d7 on\nan NVIDIA V100 GPU and 12.8\u00d7 on an M1 Pro chip,\nwhile maintaining peak signal-to-noise ratio (PSNR) in a\n0.4-1.2 dB range. Surprisingly, our simple pipeline can\nachieve comparable quality to recent and more complex\nSR techniques [23,38], while being more efficient in train-\ning and inference.\n\u2022 Random patch sampling is a crucial lightweight aug-\nmentation technique for NeRF+SR. We propose ran-\ndom patch sampling, a lightweight augmentation tech-\nnique. This augmentation improves the PSNR of the SR\nmodule by up to 0.89 dB for 2\u00d7 upscaling and up to\n1.44 dB for 4\u00d7 upscaling compared to standard grid-based\npatch sampling, outperforming expensive distillation ap-\nproaches [10] at a fraction of the time cost.\n\u2022 FastSR-NeRF is one of the few efficient methods that\ncan be trained on a low-power device. As shown in Fig-\nure 1, by utilizing a simple NeRF+SR pipeline, FastSR-\nNeRF can be trained on consumer devices such as a Mac-\nBook Air M2, whereas most other models and existing\nNeRF+SR pipelines fail to train with a meaningful time\nbudget.\nOverall, our analysis shows that SR can be a low-cost,\nplug-and-play strategy for improving the efficiency of neu-\nral rendering models under a limited training budget. Even\na simple NeRF+SR pipeline can make neural rendering\nmore efficient and accessible for those with low-power,\nconsumer-grade hardware.\n2. Background\n2.1. Neural Radiance Fields\nThe NeRF model was first proposed in [31]. Given a po-\nsition and view angle in a 3D scene, NeRF uses a large MLP\nnetwork to map from the 5D input (3D coordinates plus 2D\nview angle) to an RGB and a density value. To render a 2D\nimage, these MLP outputs are integrated along rays passing\nthrough each pixel using volume rendering. The MLP is op-\ntimized using gradient descent with respect to a photomet-\nric loss over a sparse set of scene-specific images. Due to\nits impressive results on static novel view synthesis, NeRF\nquickly propelled the state-of-the-art in many other fields,\nincluding 3D image generation [11, 28, 33], 3D scene edit-\ning [19] and landscape reconstruction [44]. However, the\ndrawback of the original NeRF is that it takes a long time to\nrender images due to the slow volume rendering process.\nTo address this issue, many works have since been pro-\nposed to improve NeRF\u2019s rendering efficiency. One line\nof work [13, 18, 21, 47] maps the learned radiance field to\nexplicit representations such as octree-based [47] or voxel-\nbased [21] data structures. These methods achieve faster\nrendering tine at the cost of larger memory and training time\nrequirements. Another line of work [16,22,26,32] focuses\non improving the sampling algorithm to reduce overall com-\nputation, which yields a modest amount of acceleration. An\nemerging series of works divides the radiance field into ex-\nplicit voxels [29, 34, 37], or some efficient representation\nsuch as matrix decomposition [12], hash table [32], and tri-\nplane [9,11,35]. As these models usually make use of a mix\nof explicit representations and MLP, they are referred as hy-\nbrid NeRFs. Typically, hybrid NeRF models can highly ac-\ncelerate training time and rendering speed, but need a rela-\ntively large model size. Among these efficient NeRF meth-\nods, there is a trend to develop customized GPU kernels\nto further accelerate the specialized operations designed for\neach method. Although using customized kernels can bring\na major speedup, it limits the ability to deploy the model on\ndifferent classes of GPUs and consumer-grade devices [10].\nOrthogonal to these works, we explore the application of SR\nmodules for improving NeRF rendering efficiency under a\nlimited training budget. To maximize flexibility for running\non low-end devices, we consider Python implementations\nthat do not use customized GPU kernels.\n2.2. Super Resolution with NeRFs\nSuper-resolution (SR) is a recent, still under-explored\nmethod for enhancing NeRF efficiency. EG3D [11] applies\nSR on top of volume rendering within a generative adver-\nsarial network for 3D faces. The SR module in their net-\nwork is trained on tens of thousands of images, whereas\nwe consider scene-specific optimization on much smaller\ndatasets (e.g., 20-200 images per scene). NeRF-SR [38]\nperforms sub-pixel sampling to super-resolve outputs, but\nthis requires more compute and thus a longer training time.\nMobileR2L [10] proposes a full CNN-based neural light\nfield model and uses a SR model in its second stage, but\ntheir method is trained using an expensive distillation pro-\ncedure. RefSR-NeRF [23] proposes a specialized SR mod-\nule that uses high-resolution reference image as additional\ninput, resulting in slower training and inference times. 4K-\nNeRF [40] synthesizes ultra high-resolution (4K) outputs\nusing depth features as additional input and incorporates SR\nto achieve feasible inference times. Overall, these existing\nworks all have high training overhead and are not meant to\nbe optimized on lower-power consumer devices.\nIn our work, we approach SR techniques for neural ren-\ndering from a different perspective. Rather than develop\na complex pipeline that pushes the limit of reconstruction\nquality on high-end GPUs, we ask what efficiency gains, if\nany, can be made from a simple NeRF+SR pipeline trained\non consumer-grade hardware. Our experiments show that\na simple NeRF+SR pipeline can achieve comparable qual-\nity to existing complex pipelines, while being lightweight\nenough to train on consumer-grade hardware.\n3. Method\n3.1. A Simple NeRF + SR Pipeline\nAs shown in Figure 2, our pipeline simply consists of a\nNeRF model concatenated with a CNN-based SR module.\nGiven a ray r = o + td, where o and d are respectively the\nray origin and direction, NeRF reconstructs the color bC(r)\nwith volume rendering as follows:\nbC(r) =\nN\nX\ni=1\nTi \u00b7 (1 \u2212 exp(\u2212\u03c3i\u03b4i)) \u00b7 ci,\n(1)\nwhere N is the number of sampling points along the ray, \u03b4i\nis the distance between two point sampled at ti and ti+1,\nTi = Qi\u22121\nj=1 exp(\u2212\u03c3j\u03b4j), and \u03c3i and ci are the density and\ncolor respectively of a position x in the 3D scene. In the\noriginal NeRF model, \u03c3i and ci are computed by MLP net-\nworks F\u03c3 and Fc given position and viewing direction. In\nour pipeline, we use a hybrid NeRF model [12] to achieve\nstate-of-the-art quality with improved training time and ren-\ndering speed. To compute the density and color, we fetch\nradiance features from a grid-based decomposition G\u03c3 and\nGc, and then feed sampled features to MLP F\u03c3 and Fc:\n\u03c3i = F\u03c3(G\u03c3(x)), ci = Fc(Gc(x), d)\n(2)\nDue to the more powerful discrete features, the MLPs F\u03c3\nand Fc in the hybrid NeRF are smaller than the ones used\nin vanilla NeRF.\nTo train and render with the full NeRF+SR pipeline, we\nsample r from a patch of rays at low-resolution (LR) RP\nLR,\nperform volume rendering based on Equation 1, and up-\nsample the output with SR module S to get the final high-\nresolution (HR) output H:\n\u2200rLR \u2208 RP\nLR, H = S( bC(rLR; F; G))\n(3)\nNote that the sampling here covers a contiguous 2D patch,\nwhich differs from the stochastic sampling of rays used for\ntraining standard NeRFs. The NeRF+SR pipeline is opti-\nmized in an end-to-end manner with respect to the loss com-\nputed over the high-resolution image patch:\nLMSE =\nX\nrHR,rLR\n\r\rC(rHR) \u2212 S( bC(rLR; F; G))\n\r\r2\n2\n(4)\nwhere C is the ground-truth color and rHR is the HR\ncounterpart of rLR in RP\nHR. In practice, we use bilinear in-\nterpolation to downsample RP\nHR and get RP\nLR.\nFigure 2. Overview of FastSR-NeRF. The SR module in our pipeline directly takes the RGB output from NeRF model, and therefore\nmakes our pipeline easy to implement, model agnostic and flexible to run on different devices.\nComments on Efficiency.\nFor a high-resolution NeRF\nmodel, the number of rays computed by Equation 1 will\nbe RHR, and will also require N in the order of thousands to\nmillions to reconstruct details. In contrast, in a NeRF + SR\npipeline, the NeRF only needs output a low-resolution out-\nput, which reduces the number of rays to RLR. N can also\nbe reduced as there are fewer details in the lower-resolution\nimage. In addition, we can reduce the grid and feature size\nof the hybrid NeRF to further improve NeRF efficiency, and\nstill maintain the output quality at LR. As a result, we can\ngreatly lower the computation overhead in Equation 1 and\nreduce the memory usage by letting NeRF output at LR.\nThe addition of the CNN-based SR module S does not\npresent much of a computational bottleneck. With many\nyears of progression on deep learning, the convolution op-\neration is highly optimized and can efficiently run on mod-\nern commodity hardware such as GPUs [14], CPUs [5] and\nspecialized accelerators [2, 7]. Furthermore, CNNs are pa-\nrameter efficient by design [20, 25]. The memory savings\nfrom down-scaling the NeRF to LR are much more than the\nparameters overhead induced by the SR model, which re-\nduces overall model size.\n3.2. Random Patch Sampling\nAs discussed in Section 3.1, SR-based NeRF models\nneed to be trained in a patch-style sampling instead of the\ntradition stochastic ray sampling. Previous works handle\nthe patch sampling by dividing the rays of an image into\nequal-size patches following rigid grid lines which are de-\ntermined by the given grid size. These ray patches are then\nshuffled and fed into the SR-based NeRF pipeline for super-\nvision \u2013 we call this grid-based patch sampling. The prob-\nlem of the grid-based patch sampling is that the sampling\nalgorithm will cut the ray space strictly with a certain grid\nsize. When the model gets trained on individual patches,\nthere will be some variations that are never seen by the con-\nvolutional kernels as they sit between each grid boundaries.\nTo solve this issue, we propose random patch sam-\npling. Instead of having a rigid grid lines and restricting\nsampling to the grid, we randomly sample a region in the\nray space, and use that patch to train the model. In this way,\nwe can still have a fixed patch size, but the content of each\npatch will be different regions of the input. After many it-\nerations, the convolutional kernels in S will cover all of the\npatterns appeared in the training data and lead to a better\ntraining results.\nIn general, CNN-based models are prone to over-fitting\nand typically require large-scale datasets [15] to be trained.\nHowever, NeRF datasets usually only have tens to hundreds\nof training images, which is orders of magnitude smaller\nthan a typical CNN pre-training dataset. Existing SR-based\nNeRF models tackle the overfitting issue by rendering ex-\ntra data from a teacher model, or guiding training with ad-\nditional features from high-resolution reference image or\ndepth maps, but these significantly increase training over-\nhead. Random patch sampling is a lightweight data aug-\nmentation technique that enables the convolutional kernels\nof the SR module to see more diversity in the training set.\nThis crucial augmentation allows us to achieve high-quality\nresults without the more complex architectures or training\nprocedures of previous works. We provide ablation results\nof random patch sampling versus baselines in Section 4.5.\n4. Evaluations\n4.1. Datasets\nWe use the following three datasets for experiments.\nNeRF Synthetic dataset. The NeRF Synthetic dataset was\ncollected along with the origin NeRF [31] paper. It con-\ntains 8 different synthetic scenes with 360\u25e6 degree views\nproduced from the Blender [3] 3D computer graphics cre-\nation framework. Each scene in this dataset contains an ob-\nject with complicated details. The object is placed in the\nmiddle of the 3D space and the backgound is white. For\neach scene, it has 100 training images and 200 testing im-\nages of the object from different views. The resolution of\nthe collected images are 800\u00d7800.\nMethod\nNeRF-Synthetic\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nTrain\nRender\nModel\u2193\nTime\u2193\nTime(s)\u2193\nSize(MB)\nNeRF [31]\n31.01\n0.947\n0.081\n\u223c35h\n20\n5\nTensoRF [12]\n33.14\n0.963\n0.049\n18m\n1.4\n71.8\nFastSR-NeRF (2\u00d7)\n32.53\n0.961\n0.052\n1.5h\n0.309\n20\nFastSR-NeRF (4\u00d7)\n30.47\n0.944\n0.075\n30m\n0.077\n13\nFastSR-NeRF (8\u00d7)\n27.27\n0.902\n0.142\n16m\n0.030\n8\nMobileR2L [10]\n31.34\n0.993\n0.051\n>35h\n0.026\u2021\n8.3\nNeRF-SR [38]\n28.46\n0.921\n0.076\n>35h\n5.6\n-\nMethod\nNSVF-Synthetic\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nTrain\nRender\nModel\u2193\nTime\u2193\nTime(s)\u2193\nSize(MB)\nNeRF [31]\n30.81\n0.952\n-\n\u223c35h\n\u223c20\n\u223c5\nTensoRF [12]\n36.52\n0.959\n0.027\n15m\n1.4\n74\nFastSR-NeRF (2\u00d7)\n35.39\n0.979\n0.032\n1.5h\n0.302\n26\nFastSR-NeRF (4\u00d7)\n32.04\n0.958\n0.059\n30m\n0.075\n12\nFastSR-NeRF (8\u00d7)\n27.93\n0.911\n0.119\n16m\n0.030\n9\nMethod\nLLFF\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nTrain\nRender\nModel\u2193\nTime\u2193\nTime(s)\u2193\nSize(MB)\nNeRF [31]\n26.5\n0.811\n0.250\n\u223c48h\n33\n5\nTensoRF [12]\n26.6\n0.832\n0.207\n28m\n5.9\n188\nFastSR-NeRF (2\u00d7)\n26.20\n0.822\n0.241\n2.5h\n0.786\n26\nFastSR-NeRF (4\u00d7)\n25.41\n0.784\n0.297\n57m\n0.165\n15\nFastSR-NeRF (8\u00d7)\n21.30\n0.584\n0.475\n15m\n0.040\n8\nMobileR2L [10]\n26.15\n0.966\n0.187\n>48h\n0.018\u2021\n8.3\nNeRF-SR [38]\n27.26\n0.842\n0.103\n>48h\n39.1\n-\nRefSR-NeRF [23]\n26.23\n0.874\n0.243\n-\n8.5\n38\nTable 1. Quantitative and efficiency results on NeRF-Synthetic, NSVF-Synthetic and LLFF datasets. We compare the results of applying\nSR to the baseline NeRf model, TensoRF [12], and also list vanilla NeRF as a reference. For NeRF-Synthetic and LLFF, we also include\nthe results of other SR-based models from their paper. The results are highlighted in red when there is a clear disadvantage of a method. \u2021\nThe rendering time is for iPhone13, while other time is on GPUs.\nNSVF Synthetic dataset The NSVF Synthetic dastaset was\nreleased with the NSVF [29] paper. It has a similar setting\nas NeRF Synthetic dataset with gradually more complex ge-\nometry and lightening on the main object. The resolution is\nalso at 800\u00d7800.\nLLFF dataset. LLFF dataset was collected along with the\nLLFF [30] paper. The scenes in this dataset were captured\nin the real world with foward-facing angle. It also has a\nmajor object placed roughly in the middle of each scene.\nHowever, different from the NeRF Synthetic dataset, the\nscenes in LLFF dataset have complex background depend-\ning on the captured environment. The original resolution\nof the collected images are 4032\u00d73024, and it also provide\nimages at 4\u00d7 and 8\u00d7lower resolution. Due to the practical\nusage, most of the NeRF works including us evaluate this\ndataset on 4x lower resolution at 1008\u00d7756. Each scene in\nthe LLFF dataset has 20 to 40 images, and 7/8 are used for\ntraining and 1/8 are used for testing.\n4.2. Experiment Setup\nWe choose TensoRF [12] as our NeRF backbone as it\nachieves state-of-the-art results on both quality and effi-\nciency, without requiring customized CUDA kernels, and\ntherefore aligns with the goal of this paper. For our SR\nmodel, we chose EDSR [27] due to its accessible imple-\nmentation and wide adoption in the computer vision com-\nmunity [4]. Although we choose TensoRF and EDSR as\nour NeRF and SR model, both of them can be replaced with\nother methods, as our pipeline is model agnostic. Since our\nSR module solely relies on the RGB output of the NeRF,\nwe are able to leverage pretrained SR models.\nTo train\nour pipeline, we first warm up the TensoRF model at LR\nusing its default hyperparameters (inherited from the offi-\ncial implementation) for 5K iterations. After warming up,\nwe plug a pretrained EDSR model with desired SR ratio\ninto our pipeline and start training end-to-end using random\npatch sampling. For the training hyperparameters, we fix\nthe learning rate at 0.0001, patch size at 256 and 128 for\nSR-2\u00d7 and SR-4\u00d7. We use Adam optimizer [24] and train\nthe pipeline for 150 epochs. For each iteration in a epoch,\nwe only feed one patch to the pipeline. We run our experi-\nments on a machine that is equipped with a single NVIDIA\nV100 GPU with 16GB memory unless we specify the hard-\nware platform.\n4.3. Evaluation on Quality and Efficiency\nEfficiency gains of utilizing SR. Here we evaluate the qual-\nity and efficiency gains of our simple NeRF+SR pipeline.\nWe list peak signal-to-noise ratio (PSNR), structural sim-\nilarity (SSIM) and perceptual similarity (LPIPS) [48] for\nquantitative quality measurements, and provide training\ntime, rendering time and model size for efficiency evalu-\nations. For LPIPS, we use VGG [36] as the backbone. The\nresults can be found in Table 1.\nAs shown in Table 1, comparing to the backbone Ten-\nsoRF [12] model, applying SR can generally maintain qual-\nity at the 2x ratio and enjoy efficiency benefits in render-\ning time and model size. For example, our pipeline with\nSR-2\u00d7 only has a small 0.61dB, 1.13dB and 0.4dB PSNR\ndrop and has near no loss on SSIM and LPIPS compared to\nthe baseline model. Our pipeline at SR 2\u00d7 even achieves a\nslight improvement on SSIM for NSVF-Synthetic. For ef-\nficiency, using 2x SR rate can improve rendering time by\n4.5\u00d7, 4.6\u00d7 and 7.5\u00d7 and reducing model size by 3.6\u00d7,\n2.8\u00d7, and 7.2\u00d7 for NeRF-Synthetic, NSVF-Synthetic and\nLLFF respectively. For SR-4\u00d7, we observe a more notable\nquality loss to the baseline compared to SR-2\u00d7. However, it\ncan still achieve qualified results such as over 30dB PSNR\non synthetic datasets and just a small 1.19dB PSNR loss\non LLFF dataset. At the mean time, with 4\u00d7 SR rate, it\ncan further improve the rendering time speedup to 18.2\u00d7,\n18.6\u00d7 and 35.7\u00d7, achieve model size reduction at 5.5\u00d7,\n6.2\u00d7, and 12.5\u00d7 for NeRF-Synthetic, NSVF-Synthetic, and\nLLFF. Furthermore, the training time for SR-4\u00d7 is down to\n30 min for synthetic datasets and 1hr for LLFF as the model\nrun and converge faster at this rate. For SR-8\u00d7, although the\nefficiency is further improved, our pipeline can not maintain\na good quality at this upscaling rate.\nComparing to existing SR-based NeRFs. We compare\nour simple pipeline to three existing SR-based model, which\nare MobileR2L [10], NeRF-SR [38] and RefSR-NeRF [23].\nNotice that MobileR2L is light field based model and is not\nbased on radiance field. However, they still utilize SR to\nenhance rendering speed so we include it for comparison.\nComparing to them, our simple pipeline with only\nMethod\nPSNR\nM1 Pro\nM2\nTrain\nRender\nTrain\nRender\nTime\nTime\nTime\nTime\nTensoRF\n33.14\n2h\n54s\n2.5h\n45.4s\nFastSR-NeRF (2\u00d7)\n32.53\n22.5h\n15.9s\n16h\n15.2s\nFastSR-NeRF (4\u00d7)\n30.47\n15.5h\n4.2s\n13.5h\n4s\nMobileNeRF\n30.9\n>375h\u2020\n0.016s\n>375h\u2020\n0.017s\nTable 2. PSNR and time profiling of running vanilla TensoRF,\nFastSR-NeRF (ours) and MobileNeRF [13] on a MacBook Pro\nlaptop with M1 Pro chip. \u2020 The training time is approximated for\ntraining the vanilla NeRF on M-series CPUs, which is only the\nfirst training stage of MobileNeRF.\nlightweight techniques in training achieves a very clear ad-\nvantage on the training time. At SR-2\u00d7, our pipeline can\nbe trained 23.3\u00d7 and 19.2\u00d7 faster than existing SR-based\nmodels on NeRF-Synthetic and LLFF, while achieving ei-\nther on par or better quality.\nQualitative results. We show qualitative results and com-\nparison on selected scenes from NeRF-Synthetic and LLFF\ndatasets in Figure 3. As Figure 3 shows, with 2\u00d7 upscaling\nrate, our pipeline can achieve on-par visual quality as the\nbaseline TensoRF, while using bilinear interpolation at the\nsame rate is not enough to get high fidelity results.\n4.4. Training on Consumer Devices.\nWe run our pipeline on a MacBook Pro with M1 Pro chip\nand a MacBook Air with M2 chip to evaluate efficiency on\nconsumer platforms. The training time, rendering time and\nPSNR on NeRF-Synthetic for our pipeline at SR rate 2\u00d7\nand 4\u00d7 are listed in Table 2. We also list MobileNeRF\u2019s\n[13] results for comparison.\nAs shown in Table 2, using SR can improve the rendering\nspeed by up to 3.4\u00d7 and 12.8\u00d7 for 2\u00d7 and 4\u00d7 SR rate on\nthe consumer-grade M-series CPUs. For MobileNeRF [13],\nalthough it can achieve a much faster rendering time from\nits specialized caching mechanism, it needs more than 15\ndays to be trained on the same device, which is difficult\nto make a meaningful NeRF application that run fully on\na consumer-grade platform. In contrast, our pipeline, al-\nthough can not achieve real-time rendering, it still signifi-\ncantly accelerates the rendering process with a reasonable\ntraining time (less than 1 day). Note that our experiments\nare run on CPUs because the current Apple Metal Perfor-\nmance Shader (MPS) [1] support in PyTorch can not fully\nrun the operators needed in NeRF and SR on the MPS de-\nvice.\nWe expect our training and rendering speed to be\nfaster once PyTorch has a better MPS operators support.\n4.5. The Importance of Random Patch Sampling\nWe evaluate the effectiveness of random patch sampling,\nwhich we discussed in Section 3.2. To evaluate, we first es-\ntablish our model pipeline as we explained in Section 3.1.\nDataset\nMethod\nPSNR\nSR-2\u00d7\nSR-4\u00d7\nNeRF-Synthetic\nGrid-based\n31.84\n29.28\nRandom\n32.53\n30.47\nNSVF-Synthetic\nGrid-based\n34.34\n30.45\nRandom\n35.39\n32.04\nLLFF\nGrid-based\n26.2\n24.94\nRandom\n26.04\n25.41\nTable 3. PSNR comparison on using random patch sampling\nversus grid-based patch sampling. We highlight the better results\nfor the same SR ratio in bold.\nWe then keep all the settings of the pipeline the same but use\ndifferent patch sampling algorithm to train our model. No-\ntice that we fix the patch size and training for the same num-\nber of iterations, so the number of patches seen by the model\nis the same for both sampling methods.\nFor grid-based\npatch sampling, we randomize the order of the patches fed\ninto the model to ensure training stability. We report the av-\neraged PSNR for SR ratio 2\u00d7 and 4\u00d7 of training our model\nwith these two patch sampling algorithms in Table 3.\nAs Table 3 shows, we observe that random patch sam-\npling consistently leads to a higher PSNR than grid-based\npatch sampling on synthetic datasets, The highest improve-\nment appear on NSVF-Synthetic when the SR rate is 4\u00d7,\nwhere random patch sampling records a 1.59 PSNR in-\ncrease over grid-base patch sampling.\nOn real-world forward facing LLFF dataset, the PSNR\nenhancement is less significant than the synthetic datasets.\nWe observe a slight PSNR decrease (0.16dB) for SR-2\u00d7\nbut a clear PSNR increase (0.47dB) for SR-4\u00d7. We hy-\npothesize that this is because the real-world scenes typically\ncontains greater complexity and finer-grained detail than the\nsynthetic scenes. For example, in a synthetic scene, there is\nusually one major object and the space outside of the object\nis empty. Although, the object might has some difficult and\nfine-grained patterns, the model can focus on learning the\npatterns on the object. However, in a real world scene, al-\nthough it usually still has a major object, the background is\nusually messy and contains a lot of small details. Therefore,\nusing random patches on real world scenes such as LLFF\ndataset can not bring a big difference in terms of the total\nnumber of patterns converged in the patches. As a result,\nrandom patch sampling shows a greater improvement on\nsynthetic datasets but still has the ability to enhance PSNR\non real world scenes dataset when the SR rate is higher, e.g.\n4\u00d7.\n4.6. Ablation Study on Training Strategies.\nIn Section 4.5, we compare the results of training our\nNeRF + SR pipeline with grid-based or random patch sam-\npling. However there are many more configurations possi-\nUpsample\nUpsample\nTrain\nTrain\nPSNR\nRatio\nMethod\nStrategy\nTime(m)\n2\u00d7\nBilinear\n-\n11\n29.77\nEDSR\nPretrained\n11\n30.40\nScratch\n51\n31.64\nFT-GridPatch\n51\n31.84\nFT-RandPatch\n89\n32.53\nDistillation\n365\n32.12\n4\u00d7\nBilinear\n-\n3.5\n26.67\nEDSR\nPretrained\n3.5\n27.62\nScratch\n19\n29.03\nFT-GridPatch\n19\n29.28\nFT-RandPatch\n30\n30.47\nDistillation\n166\n29.94\nTable 4. Training time and PSNR of different training strategies\non NeRF Synthetic dataset.\nPretrained EDSR model is down-\nloaded from HuggingFace website. FT stands for finetuning the\npretrained SR model. RandPatch signifies random patch sampling\n(other methods use grid-based patch sampling if not specified).\nAll experiments are trained for 150 epochs, with the exception of\ndistillation. For distillation, we generate 1k extra training image\nusing a pretrained TensoRF as teacher, and train for 100 epochs.\nble. In this section, we compare the results of 1) using bilin-\near interpolation as the SR method, 2) use out-of-box pre-\ntrained SR model without finetuning, 3) training the NeRF\nand SR model both from scratch, 4) finetuning the pipeline\nwith pretrained SR on the NeRF dataset with grid-based\npatch sampling, 5) same as (4) but uses random patch sam-\npling, 6) use the distillation method proposed by [10] and\ntrain on a larger training set augmented by a teacher NeRF\nat HR.\nWe train the pipeline on NeRF-synthetic dataset and\nshow the comparison in Table 4.\nUsing bilinear inter-\npolation as SR has the shortest training time (only re-\nquires warming up the NeRF backbone) but has a signif-\nicant PSNR decrease. Directly using a pretrained EDSR\nmodel can also cut down the training time and has a bet-\nter PSNR than bilinear. However, training on NeRF dataset\nstill help it to achieve a better accuracy. Among those train-\ning methods, finetuning SR using random patch sampling\nachieves the best results while paying a little more training\ntime (exclude distillation) due to the random sampling over-\nhead. For training the pipeline, although we see a promising\nPSNR increase with 1K extra training images generated by\na teacher TensoRF, the training time becomes much longer\nas we need to train the NeRF model at HR first and also need\nto render many HR images. Note that the PSNR of distil-\nlation might increase if we generate more training images,\nbut the training time will be even longer. We do not further\noptimize our distillation procedure as it\u2019s not the focus in\nthis paper. To sum up, using random patch sampling and\nFigure 3. Qualitative results on a NeRF-Synthetic and LLFF. While TensoRF and TensoRF+Bilinear fail to recover some details (see the\nribs of Trex), our pipeline successfully learn the details back with SR-2\u00d7 rate.\nfinetuning a pretrained SR model gives us the best trade-off\nbetween time and quality under our pipeline setup.\n5. Conclusion\nIn this work, we study the limit of SR-based NeRF\nmodel. We propose FastSR-NeRF and find a cohesive and\nsimple NeRF + SR pipeline can actually achieve impres-\nsive quality while also being compute and memory efficient.\nThe key result of this approach is that, although it\u2019s not the\nfastest nor the smallest model, it remains efficient for all of\ntraining time, rendering latency and model size. We achieve\nthis by leveraging the lightweight technique called random\npatch sampling and pretrained SR model \u2013 both of these\ninterventions can boost our pipeline\u2019s accuracy without in-\ntroducing prohibitive computational overhead.\nOur pure\nPython-based approach (without any customized GPU ker-\nnels) allows the whole training & inference pipeline to run\non consumer-grade devices such as a laptop with a reason-\nable time. We believe this work and comprehensive analysis\nwill help the development of an end-to-end NeRF applica-\ntion that can purely be deployed on personal devices for\nimproved compute efficiency and user privacy.\nReferences\n[1] Apple\nmetal\nperformance\nshader.\nhttps :\n/ / developer . apple . com / documentation /\nmetalperformanceshaders/. 6\n[2] Apple neural engine.\nhttps://machinelearning.\napple . com / research / neural - engine -\ntransformers. 4\n[3] The blender software. https://www.blender.org/.\n4\n[4] Huggingface edsr.\nhttps://huggingface . co/\neugenesiow/edsr. 5, 11\n[5] Intel onednn. https://www.intel.com/content/\nwww/us/en/developer/tools/oneapi/onednn.\nhtml. 4\n[6] Mobilenerf official source code.\nhttps://github.\ncom / google - research / jax3d / tree / main /\njax3d/projects/mobilenerf. 2\n[7] Nvidia tensor core. https://www.nvidia.com/en-\nus/data-center/tensor-cores/. 4\n[8] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages 5855\u2013\n5864, October 2021. 1, 13\n[9] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. CVPR, 2023. 3\n[10] Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai,\nJu Hu, Yun Fu, Denys Makoviichuk, Sergey Tulyakov, and\nJian Ren.\nReal-time neural light field on mobile devices.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8328\u20138337, 2023. 2,\n3, 5, 6, 7, 13\n[11] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. Efficient geometry-aware 3D\ngenerative adversarial networks. In arXiv, 2021. 1, 2, 3\n[12] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision (ECCV), 2022. 2, 3, 5, 6, 13\n[13] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-\ndrea Tagliasacchi. Mobilenerf: Exploiting the polygon ras-\nterization pipeline for efficient neural field rendering on mo-\nbile architectures. In The Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 1, 2, 3, 6, 13\n[14] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch,\nJonathan Cohen, John Tran, Bryan Catanzaro, and Evan\nShelhamer. cudnn: Efficient primitives for deep learning.\nCoRR, abs/1410.0759, 2014. 4\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009. 4\n[16] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang,\nWenyu Liu, and Qi Tian. Neusample: Neural sample field\nfor efficient view synthesis. arXiv:2111.15552, 2021. 3\n[17] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5501\u20135510, 2022. 2, 13\n[18] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. Fastnerf: High-fidelity neural\nrendering at 200fps. ICCV, 2021. 3, 13\n[19] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander\nHolynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Edit-\ning 3d scenes with instructions.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n2023. 2\n[20] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. 2016 IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 770\u2013778, 2015. 4\n[21] Peter Hedman,\nPratul P. Srinivasan,\nBen Mildenhall,\nJonathan T. Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. ICCV, 2021. 2, 3,\n13\n[22] Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, and Jiaya Jia.\nEfficientnerf efficient neural radiance fields. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 12902\u201312911, June 2022.\n3, 13\n[23] Xudong Huang, Wei Li, Jie Hu, Hanting Chen, and Yunhe\nWang. Refsr-nerf: Towards high fidelity and super resolution\nview synthesis. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n8244\u20138253, June 2023. 2, 3, 5, 6, 13\n[24] Diederik Kingma and Jimmy Ba.\nAdam: A method for\nstochastic optimization.\nIn International Conference on\nLearning Representations (ICLR), San Diega, CA, USA,\n2015. 6\n[25] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In Advances in Neural Information Processing Sys-\ntems 25, pages 1097\u20131105. Curran Associates, Inc., 2012.\n4\n[26] Ruilong Li, Hang Gao, Matthew Tancik, and Angjoo\nKanazawa.\nNerfacc: Efficient sampling accelerates nerfs.\narXiv preprint arXiv:2305.04966, 2023. 3\n[27] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\nKyoung Mu Lee. Enhanced deep residual networks for single\nimage super-resolution. In The IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) Workshops,\nJuly 2017. 5, 11, 12\n[28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 1, 2\n[29] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\nChristian Theobalt. Neural sparse voxel fields. NeurIPS,\n2020. 1, 2, 3, 5, 13\n[30] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG), 2019. 5\n[31] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 1, 2, 4, 5, 13\n[32] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, July 2022. 2, 3, 13\n[33] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv,\n2022. 1, 2\n[34] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance fields with\nthousands of tiny mlps. In International Conference on Com-\nputer Vision (ICCV), 2021. 2, 3, 13\n[35] Sara Fridovich-Keil and Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nCVPR, 2023. 3\n[36] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In In-\nternational Conference on Learning Representations, 2015.\n6\n[37] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. CVPR, 2022. 2, 3, 13\n[38] Chen Wang, Xian Wu, Yuan-Chen Guo, Song-Hai Zhang,\nYu-Wing Tai, and Shi-Min Hu. Nerf-sr: High quality neu-\nral radiance fields using supersampling. In Proceedings of\nthe 30th ACM International Conference on Multimedia, MM\n\u201922, page 6445\u20136454, New York, NY, USA, 2022. Associa-\ntion for Computing Machinery. 2, 3, 5, 6, 13\n[39] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\nNeurIPS, 2021. 1\n[40] Zhongshu Wang, Lingzhi Li, Zhen Shen, Li Shen, and\nLiefeng Bo.\n4k-nerf: High fidelity neural radiance fields\nat ultra high resolutions. arXiv preprint arXiv:2212.04701,\n2022. 2, 3\n[41] Zirui Wang,\nShangzhe Wu,\nWeidi Xie,\nMin Chen,\nand Victor Adrian Prisacariu.\nNeRF\u2013:\nNeural ra-\ndiance\nfields\nwithout\nknown\ncamera\nparameters.\nhttps://arxiv.org/abs/2102.07064, 2021. 1\n[42] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon\nYenphraphai, and Supasorn Suwajanakorn. Nex: Real-time\nview synthesis with neural basis expansion. In IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2021. 13\n[43] Zhijie Wu, Yuhe Jin, and Kwang Moo Yi. Neural fourier\nfilter bank.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n14153\u201314163, June 2023. 2\n[44] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,\nAnyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin.\nBungeenerf: Progressive neural radiance field for extreme\nmulti-scale scene rendering. In The European Conference\non Computer Vision (ECCV), 2022. 2\n[45] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. NeurIPS, 2021.\n1\n[46] Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Al-\nberto Rodriguez, Phillip Isola, and Tsung-Yi Lin.\niN-\neRF: Inverting neural radiance fields for pose estimation.\nhttps://arxiv.org/abs/2012.05877, 2020. 1\n[47] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. PlenOctrees for real-time rendering of\nneural radiance fields. In ICCV, 2021. 3\n[48] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric.\nIn 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 586\u2013595, 2018. 6\nAppendices\nA. Additional Ablation Study on Augmenta-\ntions\nIn the present study, we introduce a technique of random\npatch sampling designed to improve the training efficacy\nof the NeRF+SR pipeline. In addition to this, we extend\nthe same framework to incorporate additional data augmen-\ntations conventionally employed in Convolutional Neural\nNetworks (CNNs), such as random rotation or perspective\ntransformation applied to the sampled patches. Our central\nhypothesis posits that these slight image transformations\ncould potentially generate novel patterns absent from the\ntraining set but pertinent to the 3D spatial context. Through\nthese augmentations, the SR module is hypothesized to fur-\nther generalize, thereby facilitating the recovery of lost de-\ntails in unobserved perspectives.\nMathematically, the NeRF+SR pipeline involves the\nsampling of a patch in the low-resolution (LR) ray space\nRP\nLR and its high-resolution (HR) counterpart RP\nHR. A trans-\nformation A is subsequently applied to these patches, yield-\ning the transformed patches RP \u2032\nLR and RP \u2032\nHR as defined in\nEquation 5. These transformed patches are then integrated\ninto Equation 3 for training the pipeline.\nRP \u2032\nLR = A(RP\nLR), RP \u2032\nHR = A(RP\nHR)\n(5)\nTo empirically validate our hypothesis, we implement\ntwo lightweight augmentations, random rotation and ran-\ndom horizontal flip, layered atop the random patch sam-\npling technique. For synthetic datasets, the maximum ro-\ntation angle is set to 10 degrees, and the probability for a\nhorizontal flip is set at 10%. Conversely, for the real-world\nscenes dataset LLFF, the maximum rotation angle is limited\nto 5 degrees, and the random horizontal flip is omitted since\nit\u2019s inappropriate with the forward-facing scenes.\nThe empirical results, presented in Table 5, reveal\na marginal degradation in the output PSNR when us-\ning transformation-based augmentations as compared to\nutilizing random patch sampling exclusively.\nConse-\nquently, random patch sampling remains as the most ef-\nfective lightweight augmentation strategy for enhancing the\nNeRF+SR pipeline. We earmark the exploration of the ef-\nfective utilization of transformation-based augmentations\nwithin the NeRF+SR pipeline for future research endeav-\nors.\nB. Additional Qualitative Results\nWe show additional qualitative results in Figure 4. Here\nwe compare using different SR methods and different train-\ning procedures on the SR module. For the SR methods, we\ncompare using bilinear interpolation and EDSR [27]. For\nDataset\nData Aug\n2x\n4x\n8x\nNeRF-Synthetic\nGrid-Patch\n31.84\n29.28\n26.02\nRand-Patch\n32.53\n30.47\n27.27\nRP+RRot+Hflip\n32.22\n30.26\n27.26\nNSVF-Synthetic\nGrid-Patch\n34.34\n30.45\n26.26\nRand-Patch\n35.39\n32.04\n27.93\nRP+RRot+Hflip\n35.03\n31.78\n27.79\nLLFF\nGrid-Patch\n26.2\n24.94\n21.68\nRand-Patch\n26.04\n25.41\n21.3\nRP+RRot\n25.94\n25.37\n21.29\nTable 5. PSNR of using different augmentation techniques for SR\nrate 2x, 4x and 8x. The output resolution is 800x800 for NeRF-\nSynthetic and NSVF-Synthetic, and 1008x756 for LLFF. Grid-\nPatch stands for grid-based patch sampling and Rand-Patch stands\nfor random patch sampling. RRot stands for random rotation and\nHflip stands for random horizontal flip. The best results in each\nSR rate and dataset are highlighted in bold.\ndifferent training procedures, we compare taking the pre-\ntrained EDSR from [4], finetuning the EDSR model with\ngrid-based patch sampling and finetuning the EDSR model\nwith random patch sampling\nAs discerned from Figure 4, reliance on bilinear inter-\npolation culminates in outputs characterized by a lack of\nsharpness, rendering them blurry. In contrast, utilization\nof a pretrained SR module yields images of greater clarity,\nalbeit with some loss of intricate details. Subsequent fine-\ntuning facilitates the recovery of nuanced patterns, such as\nshadows. Remarkably, the deployment of our proposed ran-\ndom patch sampling methodology further enhances perfor-\nmance, as evidenced by improvements in the Peak Signal-\nto-Noise Ratio (PSNR) when compared to traditional grid-\nbased patch sampling.\nC. Additional Comparison with more NeRF\nmodels.\nBeyond the data presented in Table 1, we extend our\ncomparative analysis to encompass additional NeRF mod-\nels specifically optimized for efficiency, incorporating both\nsuper-resolution (SR) based and non-SR-based approaches,\nas enumerated in Table 6. The empirical results delineated\nin Table 6 affirm that our proposed pipeline not only main-\ntains high-quality output but also excels in terms of effi-\nciency. This efficiency is observed across multiple metrics\nincluding training duration, rendering velocity, and model\ncompactness, all achieved without necessitating specialized\nCUDA support on GPUs.\nFigure 4. Qualitative results on lego, chair and mic scenes in NeRF-Synthetic. We show comparison on TensoRF at HR, and using bilinear,\npretrained SR, finetuned SR with grid-based patch sampling and finetuned SR with random patch sampling to upsample output from\nTensoRF. The SR rate is 2\u00d7, and the SR module is EDSR [27]. We show the average PSNR on NeRF-Synthetic dataset of each method.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nTrain\nRender\nModel\nTime\nTime(s)\nSize(MB)\nNeRF [31]\n31.01\n0.947\n0.081\n\u223c35h\n20\n5\nMipNeRF [8]\n33.09\n0.961\n0.043\n\u223c35h\n-\n-\nNSVF\u2021 [29]\n31.74\n0.953\n0.047\n>48h\n3\n-\nKiloNeRF\u2020\u2020\u2020 [34]\n31.00\n0.950\n0.030\n>35h\n0.026\n-\nSNeRG [21]\n30.38\n0.950\n0.05\n\u223c35h\n0.012\n86.8\nMobileNeRF\u2021\u2020\u2020\u2020 [13]\n30.90\n0.947\n0.062\n>35h\n0.0013\n125.8\nEfficient-NeRF [22]\n31.68\n0.954\n0.028\n6h\n0.004\n\u223c3000\nTensoRF [12]\n33.14\n0.963\n0.049\n18m\n1.4\n71.8\nDVGO [37]\n31.95\n0.958\n0.053\n14m\n0.44\n612\nFastNeRF [18]\n29.90\n0.937\n0.056\n-\n0.041\n>7000\nPlenoxel\u2020\u2020\u2020 [17]\n31.71\n0.958\n0.049\n11m\n0.066\n815\nInstant-NGP\u2020\u2020\u2020 [32]\n33.18\n-\n-\n5m\n0.016\n16\nMobileR2L [10]\n31.34\n0.993\n0.051\n>35h\n-\n8.3\nNeRF-SR [38]\n28.46\n0.921\n0.076\n>35h\n5.6\n-\nFastSR-NeRF (2\u00d7)\n32.53\n0.961\n0.052\n1.5h\n0.309\n20\n(a) NeRF Synthetic Dataset results.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nTrain\nRender\nModel\nTime\nTime(s)\nSize(MB)\nNeRF [31]\n30.81\n0.952\n-\n\u223c35h\n\u223c20\n\u223c5\nNSVF\u2021 [29]\n35.13\n0.979\n-\n>48h\n\u223c3\n-\nDVGO [37]\n35.18\n0.979\n-\n\u223c20m\n-\n\u223c600\nTensoRF [12]\n36.52\n0.959\n0.027\n15m\n1.4\n74\nFastSR-NeRF (2\u00d7)\n35.39\n0.979\n0.032\n1.5h\n0.302\n26\n(b) NSVF Synthetic Dataset results.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nTrain\nRender\nModel\nTime\nTime(s)\nSize(MB)\nNeRF [31]\n26.5\n0.811\n0.250\n\u223c48h\n33\n5\nSNeRG [21]\n25.63\n0.818\n0.183\n\u223c48h\n0.036\n310\nNeX [42]\n27.26\n0.904\n0.178\n20h\n0.0033\n-\nEfficient-NeRF [22]\n27.39\n0.912\n0.082\n4h\n0.005\n4300\nTensoRF [12]\n26.6\n0.832\n0.207\n28m\n5.9\n188\nMobileR2L [10]\n26.15\n0.966\n0.187\n>48h\n-\n8.3\nNeRF-SR [38]\n27.26\n0.842\n0.103\n>48h\n39.1\n-\nRefSR-NeRF [23]\n26.23\n0.874\n0.243\n-\n8.5\n38\nFastSR-NeRF (2\u00d7)\n26.20\n0.822\n0.241\n2.5h\n0.786\n26\n(c) LLFF Dataset results.\nTable 6. Quality and efficiency results on NeRF-Synthetic, NSVF-Synthetic, and LLFF datasets. The tables are organized into three\nsections: implicit MLP-based NeRFs, efficient fully-explicit or hybrid NeRFs, and SR-based NeRFs, including our approach. Top perfor-\nmance in each quantitative metric is marked in bold, and the second best is underlined. Clear efficiency disadvantages are highlighted in\nred. Our tests run on a NVIDIA V100 GPU, while other results are their GPU results cited from respective papers when available. \u2021 notes\nsuch method requires 8 high-end GPUs to train. \u2020\u2020\u2020 notes the method relies on customized CUDA kernels. Our method produces excellent\nquantitative results that is on par or only slightly less than the state-of-the-art cross all the benchmarks. Our method further achieves\ngreat efficiency results across training time, rendering speed and model size without the need of customized CUDA kernels support,\nwhich is favorable for inexpensive consumer-grade devices.\n"
  },
  {
    "title": "TIP: Text-Driven Image Processing with Semantic and Restoration Instructions",
    "link": "https://arxiv.org/pdf/2312.11595.pdf",
    "upvote": "5",
    "text": "TIP: Text-Driven Image Processing with Semantic and Restoration Instructions\nChenyang Qi1,2\u2217\nZhengzhong Tu1,\nKeren Ye1,\nMauricio Delbracio1,\nPeyman Milanfar1,\nQifeng Chen2,\nHossein Talebi1\n1 Google Research\n2HKUST\nInput\n\u201cRemove all degradation\u201d\n\u201c\u201d\n\u201cUpsample\u2026, denoise\u2026\u201d\n\u201czebra...\u201d\nGround Truth\n\u201cRemove all degradation\u201d\n\u201czebra...\u201d\n\u201cUpsample\u2026, denoise\u2026\u201d\n\u201chorse...\u201d\nInput\n\u201cRemove all degradation\u201d\n\u201c\u201d\n\u201cDeblur\u2026, denoise\u2026\u201d\n\u201coranges...\u201d\nGround Truth\n\u201cRemove all degradation\u201d\n\u201coranges...\u201d\n\u201cDeblur\u2026, denoise\u2026\u201d \n\u201ceggs...\u201d\nInput\n\u201c\u201d\n\u201cGO...\u201d\n\u201cSTOP\u2026\u201d\n\u201cDANGER...\u201d\n\u201cEXIT...\u201d\nInput\nRemove all degradation\u201d\nRemove all degradation\u201d\nUpsample...Deblur...\u201d\nUpsample... Deblur...Dejpeg\u201d\nInput\nFigure 1. We present TIP: Text-driven image processing, a text-based foundation model for all-in-one, instructed image restoration. TIP\nallows users to flexibly leverage either semantic-level content prompt, or degradation-aware restoration prompt, or both, to obtain their de-\nsired enhancement results based on personal preferences. In other words, TIP can be easily prompted to conduct blind restoration, semantic\nrestoration, or task-specific granular treatment. Our framework also enables a new paradigm of instruction-based image restoration, pro-\nviding a reliable evaluation benchmark to facilitate vision-language models for low-level computational photography applications.\nAbstract\nText-driven diffusion models have become increasingly\npopular for various image editing tasks, including inpaint-\ning, stylization, and object replacement. However, it still\nremains an open research problem to adopt this language-\nvision paradigm for more fine-level image processing tasks,\nsuch as denoising, super-resolution, deblurring, and com-\n*This work was done during an internship at Google Research.\npression artifact removal. In this paper, we develop TIP,\na Text-driven Image Processing framework that leverages\nnatural language as a user-friendly interface to control\nthe image restoration process.\nWe consider the capac-\nity of text information in two dimensions.\nFirst, we use\ncontent-related prompts to enhance the semantic alignment,\neffectively alleviating identity ambiguity in the restoration\noutcomes.\nSecond, our approach is the first framework\nthat supports fine-level instruction through language-based\nquantitative specification of the restoration strength, with-\nout the need for explicit task-specific design. In addition,\n1\narXiv:2312.11595v1  [cs.CV]  18 Dec 2023\nwe introduce a novel fusion mechanism that augments the\nexisting ControlNet architecture by learning to rescale the\ngenerative prior, thereby achieving better restoration fi-\ndelity. Our extensive experiments demonstrate the superior\nrestoration performance of TIP compared to the state of the\narts, alongside offering the flexibility of text-based control\nover the restoration effects.\n1. Introduction\nImage restoration or enhancement aims to recover high-\nquality, pixel-level details from a degraded image, while\npreserving as much the original semantic information as\npossible. Although neural network models [8, 9, 25, 57,\n60, 63, 65, 66] have marked significant progress, it still re-\nmains challenging to design an effective task-conditioning\nmechanism instead of building multiple individual models\nfor each task (such as denoise, deblur, compression arti-\nfact removal) in practical scenarios. The advancement of\ntext-driven diffusion models [42, 44, 47] has unveiled the\npotential of natural language as a universal input condition\nfor a broad range of image processing challenges. How-\never, the existing applications of natural language in styl-\nization [32, 38], and inpainting [3, 4, 7, 17, 38, 58] predom-\ninantly focus on high-level semantic editing, whereas the\nuniqueness and challenges for low-level image processing\nhave been less explored.\nNatural language text prompts in image restoration can\nplay two crucial roles: alleviating semantic ambiguities and\nresolving degradation type ambiguities. Firstly, the same\ndegraded image can correspond to different visual objects,\nleading to ambiguities in content interpretation (e.g., dis-\ncerning whether the blurred animal in Fig. 1 is a horse or a\nzebra). Typically, unconditional image-to-image restoration\nleads to a random or average estimation of the identities,\nresulting in neither fish nor fowl. Secondly, certain pho-\ntographic effects that are deliberately introduced for aes-\nthetic purposes, such as bokeh with a soft out-of-focus back-\nground, can be misconstrued as blur distortions by many\nexisting deblurring models, leading to unwanted artifacts or\nsevere hallucinations in the outputs. Although blind restora-\ntion methods can produce clean images by either leveraging\nfrozen generative priors [56, 60, 62], or using end-to-end re-\ngression [63], they do not consider the aforementioned am-\nbiguities in terms of visual content and degradation types.\nIn this paper, we introduce TIP\u2014a Text-driven Image\nProcessing framework that provides a user-friendly inter-\nface to fully control both the restored image semantics and\nthe enhancement granularity using natural language instruc-\ntions. TIP takes a degraded image and optional text prompts\nas inputs (see Figure 1). These text prompts can either spec-\nify the restoration types (e.g., \u201cdenoise\u201d, \u201cdeblur\u201d), or pro-\nvide semantic descriptions (e.g., \u201ca zebra\u201d, \u201can orange\u201d).\nTIP can be employed in at least three distinct ways:\n1. Blind Restoration:\nWhen instructed with a general\nprompt \u201cremove all degradations\u201d or empty prompt \u201c\u201d,\nTIP operates as a blind restoration model (\u201cOurs w/o\ntext\u201d in Tab. 1), without any specific text guidance.\n2. Semantic Restoration: When provided with a text de-\nscription of the (desired) visual content, TIP concen-\ntrates on restoring the specified identity of the uncertain\nor ambiguous objects in the degraded image.\n3. Task-Specific Restoration: Receiving specific restora-\ntion type hints (e.g., \u201cdeblur...\u201d, \u201cdeblur... denoise...\u201d),\nTIP transforms into a task-specific model (e.g., deblur,\ndenoise, or both). Moreover, it understands the numeric\nnuances of different degradation parameters in language\nprompts so that the users can control the restoration\nstrength (e.g., \u201cdeblur with sigma 3.0\u201d in Fig. 6).\nTo train TIP, we first build a synthetic data genera-\ntion pipeline on a large-scale text-image dataset [6] upon\nthe second-order degradation process proposed in Real-\nESRGAN [63]. Additionally, we embed the degradation\nparameters into the restoration prompts to encode finer-\ngrained degradation information (e.g., \u201cdeblur with sigma\n3.5\u201d). Our approach provides richer degradation-specific\ninformation compared to contemporary works [22, 30]\nwhich only employ the degradation types (e.g., \u201cgaussian\nblur\u201d). We then finetune a ControlNet adaptor [71]\u2014which\nlearns a parameter-efficient, parallel branch on top of the\nlatent diffusion models (LDMs) [44]\u2014on a mixture of di-\nverse restoration tasks (see Fig. 1).\nThe semantic text\nprompts are processed through the LDM backbone, align-\ning with LDM\u2019s text-to-image pretraining. The degrada-\ntion text prompts and image-conditioning are implemented\nin the ControlNet branch, as these aspects are not inher-\nently addressed by the vanilla LDM. We further improve\nthe ControlNet by introducing a new modulation connec-\ntion that adaptively fuses the degradation condition with the\ngenerative prior with only a few extra trainable parameters\nyet showing impressive performance gain.\nOur extensive experiments demonstrate remarkable\nrestoration quality achieved by our proposed TIP method.\nMoreover, TIP offers additional freedom for both content\nand restoration prompt engineering. For example, the se-\nmantic prompt \u201ca very large giraffe eating leaves\u201d helps to\nresolve the semantic ambiguity in Fig. 2; the degradation\nprompt \u201cdeblur with sigma 3.0\u201d reduces the gaussian blur\nwhile maintaining the intentional motion blur in Fig. 3. We\nalso find out that our model learns a continuous latent space\nof the restoration strength in Fig. 6, even though we do not\nexplicitly fine-tune the CLIP embedding. Our contribution\ncan be summarized as follows:\n\u2022 We introduce the first unified text-driven image restora-\ntion model that supports both semantic prompts and\nrestoration instructions.\nOur experiments demonstrate\n2\nthat incorporating semantic prompts and restoration in-\nstructions significantly enhances the restoration quality.\n\u2022 Our proposed paradigm empowers users to fully control\nthe semantic outcome of the restored image using differ-\nent semantic prompts during test time.\n\u2022 Our proposed approach provides a mechanism for users to\nadjust the category and strength of the restoration effect\nbased on their subjective preferences.\n\u2022 We demonstrate that text can serve as universal guidance\ncontrol for low-level image processing, eliminating the\nneed for task-specific model design.\n2. Related Work\nFirst, we review the literature on image restoration. Our\nproposed framework aims to address some of the limita-\ntions of the existing restoration methods. Next, we compare\nmultiple text-guided diffusion editing methods. Our method\nis designed to deal with two different text ambiguities con-\ncurrently, which is an unprecedented challenge for exist-\ning text-driven diffusion methods. Finally, we discuss the\nparameter-efficient fine-tuning of diffusion models, which\nfurther motivates our proposed modulation connection.\nImage Restoration is the task of recovering a high-\nresolution, clean image from a degraded input. Pioneering\nworks in the fields of super-resolution [11, 27], motion and\ndefocus deblurring [1, 55, 67], denoising [68], and JPEG\nand artifact removal [13, 39] primarily utilize deep neu-\nral network architectures to address specific tasks. Later,\nTransformer-based [25, 57, 65] and adversarial-based [63]\nformulations were explored, demonstrating state-of-the-art\nperformance with unified model architecture.\nRecently\nthere has been a focus on exploiting iterative restorations,\nsuch as the ones from diffusion models, to generate images\nwith much higher perceptual quality [8, 43, 46, 49, 66]\nThere have been some recent works attempting to con-\ntrol the enhancement strength of a single model, such as the\nscaling factor condition in arbitrary super-resolution [61,\n64] and noise-level map for image denoising [69]. However,\nthese methods require hand-crafting dedicated architectures\nfor each task, which limits their scalability. In contrast to\nthe task-specific models, other approaches seek to train a\nsingle blind model by building a pipeline composed of sev-\neral classical degradation process, such as BSRGAN [70]\nand Real-ESRGAN [63].\nText-guided Diffusion Editing Denoising diffusion mod-\nels are trained to reverse the noising process [20, 51, 52].\nEarly methods focused on unconditioned generation [10],\nbut recent trends pivot more to conditioned generation, such\nas image-to-image translation [46, 48] and text-to-image\ngeneration [44, 47].\nLatent Diffusion [44] is a ground-\nbreaking approach that introduces a versatile framework\nfor improving both training and sampling efficiency, while\nflexible enough for general conditioning inputs.\nSubse-\nquent works have built upon their pretrained text-to-image\ncheckpoints, and designed customized architectures for dif-\nferent tasks like text-guided image editing. For instance,\nSDEdit [31] generates content for a new prompt by adding\nnoise to the input image. Attention-based methods [17, 38,\n58] show that images can be edited via reweighting and re-\nplacing the cross-attention map of different prompts. Aside\nfrom the previously mentioned approaches driven by target\nprompts, instruction-based editing [5, 14] entail modifying\na source image based on specific instructions. These textual\ninstructions are typically synthesized using large language\nmodels [35, 40].\nAs CLIP serves as a cornerstone for bridging vision and\nlanguage, several studies have extensively investigated the\nrepresentation space of CLIP embeddings [41].\nInstead\nof directly applying CLIP to fundamental discriminative\ntasks [15, 73], these works either tailor CLIP to specific ap-\nplications, such as image quality assessment [23, 26], or\nalign image embedding with degradation types in the fea-\nture space [22, 30]. To enhance CLIP\u2019s ability to understand\nnumerical information, some works [36, 37] finetune CLIP\nusing contrastive learning on synthetic numerical data, and\nthen incorporate the fine-tuned text embedding into latent\ndiffusion training.\nParameter-Efficient Diffusion Model Finetuning To\nleverage the powerful generative prior in pretrained dif-\nfusion models, parameter-efficient components such as\ntext embedding [12], low-rank approximations of model\nweights [16, 21], and cross attention layers [24] can be fine-\ntuned to personalize the pretrained model. Adaptor-based\nfinetuning paradigms [33, 71] propose to keep the origi-\nnal UNet weights frozen, and add new image or text con-\nditions. This adaptor generates residual features that are\nsubsequently added to the frozen UNet backbone.\n3. Method\nWe introduce a universal approach to combine the above\nmentioned task-specific, strength-aware, and blind restora-\ntion methods within a unified framework (illustrated in\nSec. 3.2). We further propose to decouple the learning of\ncontent and restoration prompts to better preserve the pre-\ntrained prior while injecting new conditions, as unfolded\nin Sec. 3.3. Sec. 3.4 details our design on how to accu-\nrately control the restoration type and strength, as well as\nour proposed modulation fusion layer that adaptively fuses\nthe restoration features back to the frozen backbone. We\nstart with some preliminaries in the following section.\n3.1. Preliminaries\nLatent Diffusion Models (LDMs) [44] are probabilistic gen-\nerative models that learn the underlying data distribution\nby iteratively removing Gaussian noise in the latent space,\nwhich is typically learned using a VAE autoencoder. For-\n3\nRestoration Prompt: \u201cDeblur with sigma 0.4; Denoise with sigma 0.08\u2026\u201d \nSemantic Prompt: \u201cA very large giraffe eating leaves\u201d\nNoise\nResize\nBlur\n\u2715 \ud835\udc47\n\ud835\udc43(skip)=0.5\n\u201cRemove all degradation\u201d\n\u201c\u201d\nCLIP\nCLIP\nGround Truth \ud835\udc65\nInput  \ud835\udc66\nOutput  \nJPEG\nNoise\nResize\nBlur\nJPEG\nFigure 2. Framework of TIP. In the training phase, we begin by synthesizing a degraded version y, of a clean image x. Our degradation\nsynthesis pipeline also creates a restoration prompt cr , which contains numeric parameters that reflects the intensity of the degradation\nintroduced. Then, we inject the synthetic restoration prompt into a ControlNet adaptor, which uses our proposed modulation fusion blocks\n(\u03b3, \u03b2) to connect with the frozen backbone driven by the semantic prompt cs. During test time, the users can employ the TIP framework\nas either a blind restoration model with restoration prompt \u201cRemove all degradation\u201d and empty semantic prompt \u2205, or manually adjust\nthe restoration cr and semantic prompts cs to obtain what they ask for.\nmally, the VAE encoder E compresses an input image x\ninto a compact latent representation z = E(x), which can\nbe later decoded back to the pixel space using the coupled\nVAE decoder D, often learned under an image reconstruc-\ntion objective: D(E(x)) \u2248 x. During training stage, the\noutput of a UNet [45] \u03f5\u03b8 (zt, t, y) conditioned on y (such\nas text, images, etc.) is parameterized [10, 50] to remove\nGaussian noise \u03f5 in the latent space zt as:\nmin\n\u03b8\nE(z0,y)\u223cpdata,\u03f5\u223cN (0,I),t \u2225\u03f5 \u2212 \u03f5\u03b8 (zt, t, y)\u22252\n2 ,\n(1)\nwhere zt is a noisy sample of z0 at sampled timestep t with\ny representing the conditions. The condition y is randomly\ndropped out as \u2205 to make the model unconditional.\nAt test time, deterministic DDIM sampling [51] is uti-\nlized to convert a random noise zT to a clean latent z0\nwhich is decoded as final result D(z0). In each timestep t,\nclassifier-free guidance [10, 19] can be applied to trade-off\nsample quality and condition alignment:\n\u03f5\u03b8(zt, t, y) = \u03f5\u03b8(zt, t, \u2205) + w(\u03f5\u03b8(zt, t, y) \u2212 \u03f5\u03b8(zt, t, \u2205)),\nwhere w is a scalar to adjust the guidance strength of y.\nNote that the estimated noise \u03f5\u03b8(zt, t, y) is used to update\nzt to zt\u22121, which is a approximation of the distribution gra-\ndient score [53] as \u2207zt log p(zt|y) \u221d \u03f5\u03b8(zt, t, y).\n3.2. Text-driven Image Restoration\nBased on the LDM framework, we propose a new restora-\ntion paradigm\u2014text-driven image restoration. Our method\ntarget to restore images (x or z0) based on conditions\n{y, cs, cr}. Specifically: y denotes the degraded image\ncondition, cs is the semantic prompt describing the clean\nimage x (e.g., \u201ca panda is sitting by the bamboo\u201d or \u201ca\npanda\u201d), and cr is the restoration prompt that describes\nthe details of the degradation in terms of both the opera-\ntion and parameter (e.g., \u201cdeblur with sigma 3.0\u201d). We use\ny = Deg(x, cr) to denote the degradation process which\nturns the clean image x into its degraded counterpart yi.\nThe\nabove\ntext-driven\nimage\nrestoration\nmodel\np(zt|{y, cs, cr})\ncan\nbe\ntrained\nusing\npaired\ndata.\nWe use the text-image dataset [6] so that each clean image\nx is paired with a semantic prompt cs (the paired alt-text).\nThen y = Deg(x, cr) is simulated using the synthetic\ndegradation [63] pipeline (Sec. 4.1), yielding the final\npaired training data (x or z0, {y, cs, cr}) \u223c pdata.\nComparing to Blind Restoration Recent existing blind\nrestoration models [29, 48, 60] also leverage diffusion\npriors to generate high-quality images.\nHowever, these\nmethods are prone to hallucinate unwanted, unnatural,\nor oversharpened textures given that the semantic- and\ndegradation-ambiguities persist everywhere.\nFig. 3 pro-\nvides an example, where the input image is degraded simul-\ntaneously with Gaussian blur and motion blur. The fully\nblind restoration method removes all of the degradation,\nleaving no way to steer the model to remove only the Gaus-\nsian blur while preserving the motion effect for aesthetic\npurposes. Therefore, having the flexibility to control the\nrestoration behavior and its strength at test time is a crucial\nrequirement. Our text-driven formulation of the problem\nintroduces additional controlling ability using both the se-\nmantic content and the restoration instruction. In our TIP\ndesign, cr =\u201cRemove all degradation\u201d is also a plausible\nrestoration prompt which makes our TIP to be compatible\nwith the blind restoration . Further, our text-driven restora-\ntion takes advantages of the pretrained content-aware LDM\nusing the additional semantic prompt cs, hence able to bet-\n4\nInput\n\u201cRemove all degradation\u201d\nGround Truth\n\u201cDeblur with sigma 3.0\u201d \nFigure 3. Degradation ambiguities in real-world problems. By\nadjusting the restoration prompt, our method can preserve the mo-\ntion effect that is coupled with the added Gaussian blur, while fully\nblind restoration models do not provide this level of flexibility.\nter handle semantic ambiguities on the noisy, blurry regions\nwhose visual objects are less recognizable.\n3.3. Decoupling Semantic and Restoration Prompts\nTo effectively learn the latent distribution p(zt|y, cs, cr),\nwe further decouple the conditions {y, cs, cr} into two\ngroups\u2014one for the text-to-image prior (cs \u2192 zt) already\nimbued in the pretrained LDM model, and the other for\nboth the image-to-image (y \u2192 zt) and restoration-to-image\n(cr \u2192 zt) that needs to be learnt from the synthetic data.\nThis decoupling strategy prevents catastrophic forgetting\nin the pretrained diffusion model and enables independent\ntraining of the restoration-aware model. The restoration-\naware model\u2019s score function [53] is expressed as:\n\u2207ztlog p(zt|y, cs, cr)\n\u2248 \u2207zt log p(zt|cs)\n|\n{z\n}\nSemantic-aware (frozen)\n+ \u2207zt log p(y|zt, cr).\n|\n{z\n}\nRestoration-aware (learnable)\n(2)\nIn the above equation, the first part \u2207zt log p(zt|cs)\naligns with the text-to-image prior inherent in the pretrained\nLDM. The second term \u2207zt log p(y|zt, cr) approximates\nthe consistency constraint with degraded image y, meaning\nthe latent image zt should be inferred by the degraded im-\nage y and the degradation details cr. This is somewhat sim-\nilar to the reverse process of y = Deg(xt, cr) in the pixel\nspace. More derivation is provided in our supplement. Pro-\nviding degradation information through a restoration text\nprompt cr can largely reduce the uncertainly of estimation\np(y|zt, cr), thereby alleviating the ambiguity of the restora-\ntion task and leading to better results that are less prone to\nhallucinations.\n3.4. Learning to Control the Restoration\nInspired by ControlNet [71],\nwe employ an adap-\ntor to model the restoration-aware branch in Eq. (2).\nFirst, the pretrained latent diffusion UNet \u03f5\u03b8 (zt, t, cs)\nis frozen during training to preserve text-to-image prior\n\u2207zt log p(zt|cs).\nSecondly, an encoder initialized from\npretrained UNet is finetuned to learn the restoration condi-\ntion \u2207zt log p(y|zt, cr), which takes degradation image y,\ncurrent noisy latents zt and the restoration prompt cr as in-\nput. The output of encoder is fusion features fcon for frozen\nUNet. In this way, the choice of ControlNet implementation\nfollows our decoupling analysis in Eq. (2).\nConditioning on the Degraded Image y. To feed in the\ndegraded image y to the ControlNet , we apply downsample\nlayers on y to make the feature map matching to the shape\nof the zt, denoted as E\u2032(y). E\u2032 is more compact [71] than\nVAE encoder E, and its trainable parameters allow model\nto perceive and adapt to the degraded images from our syn-\nthetic data pipeline. Then, we concatenate the downsam-\npled feature map E\u2032(y) with the latent zt, and feed them to\nthe ControlNet encoder.\nConditioning on the Restoration Prompt cr.\nFollow-\ning [44], we first use the vision-language CLIP [41] to infer\nthe text sequence embedding er \u2208 RM\u00d7d for cr, where\nM is the number of tokens, and d = 768 is the dimension\nof the image-text features. Specifically, given a restoration\ninstruction \u201cdeblur with sigma 3.0\u201d, CLIP first tokenizes it\ninto a sequence of tokens (e.g., \u201c3.0\u201d is tokenized into [\u201c3\u201d,\n\u201c.\u201d, \u201c0\u201d]) and embeds them into M distinct token embed-\ndings. Then, stacked causual transformer layers [59] are ap-\nplied on the embedding sequence to contextualize the token\nembeddings, resulting in er. Hence, tokens such as \u201cde-\nblur\u201d may arm with the numeric information such as \u201c3\u201d,\n\u201c.\u201d,, and \u201c0\u201d, making it possible for the model to distinguish\nthe semantic difference of strength definition in different\nrestoration tasks. Later, the cross-attention process ensures\nthat the information from er is propogated to the Control-\nNet feature fcon. We are aware that [36, 37] finetuned CLIP\nusing contrastive learning, while we found frozen CLIP still\nworks. The reason is that the learnable cross-attention lay-\ners can somehow ensemble the observations from frozen\nCLIP and squeeze useful information from it.\nModulation Fusion Layer The decoupling of learning two\nbranches (Sec. 3.3), despite offering the benefit of effective\nseparation of prior and conditioning, may cause distribution\nshift in the learned representations between the frozen back-\nbone feature fskip and ControlNet feature fcontrol. To allow\nfor adaptive alignment of the above features, here we pro-\npose a new modulation fusion layer to fuse the multi-scale\nfeatures from ControlNet to the skip connection features of\nthe frozen backbone:\n\u02c6fskip = (1 + \u03b3)fskip + \u03b2;\n\u03b3, \u03b2 = M(fcon),\n(3)\nwhere \u03b3, \u03b2 are the scaling and bias parameters from M a\nlightweight 1 \u00d7 1 zero-initialized convolution layer.\n4. Experiments\n4.1. Text-based Training Data and Benchmarks\nOur proposed method opens up an entirely new paradigm\nof instruction-based image restoration.\nHowever, exist-\ning image restoration datasets such as DIV2K [2] and\nFlickr2K [34] do not provide high-quality semantic prompt,\n5\nPrompts Parameterized Degradation with synthesized cr\nReal-ESRGAN Degradation without cr\nMethod\nSem Res FID\u2193 LPIPS\u2193 PSNR\u2191 SSIM\u2191 CLIP-I\u2191 CLIP-T\u2191 FID\u2193 LPIPS\u2193 PSNR\u2191 SSIM\u2191 CLIP-I\u2191 CLIP-T\u2191\nSwinIR [25]\n%\n% 43.22 0.423\n24.40\n0.717\n0.856\n0.285\n48.37 0.449\n23.45\n0.699\n0.842\n0.284\nStableSR [60]\n%\n% 20.55 0.313\n21.03\n0.613\n0.886\n0.298\n25.75 0.364\n20.42\n0.581\n0.864\n0.298\nDiffBIR [29]\n%\n% 17.26 0.302\n22.16\n0.604\n0.912\n0.297\n19.17 0.330\n21.48\n0.587\n0.898\n0.298\nControlNet-SR [71]\n%\n% 13.65 0.222\n23.75\n0.669\n0.938\n0.300\n16.99 0.269\n22.95\n0.628\n0.924\n0.299\nOurs w/o text\n%\n% 12.70 0.221\n23.84\n0.671\n0.939\n0.299\n16.25 0.262\n23.15\n0.636\n0.929\n0.300\nDiffBIR [29] + SDEdit [31] !\n% 19.36 0.362\n19.39\n0.527\n0.891\n0.305\n17.51 0.375\n19.15\n0.521\n0.887\n0.308\nDiffBIR [29] + CLIP [41]\n!\n% 18.46 0.365\n20.50\n0.526\n0.896\n0.308\n20.31 0.374\n20.45\n0.539\n0.885\n0.307\nControlNet-SR + CLIP [41] !\n% 13.00 0.241\n23.18\n0.648\n0.937\n0.307\n15.16 0.286\n22.45\n0.610\n0.926\n0.308\nOurs\n!\n! 11.34 0.219\n23.61\n0.665\n0.943\n0.306\n14.42 0.262\n23.14\n0.633\n0.935\n0.308\nTable 1. Quantitative results on the MS-COCO dataset (with cs) using our parameterized degradation (left) and Real-ESRGAN degradation\n(right). We also denote the prompt choice at test time. \u2018Sem\u2019 stands for semantic prompt; \u2018Res\u2019 stands for restoration prompt. The first\ngroup of baselines are tested without prompt. The second group are combined with semantic prompt in zero-shot way.\nPipeline\nDegradation Process\np(choose)\nRestoration Prompt\nReal-ESRGAN\nBlur\n1.0\nRemove all degradation\nResize\n1.0\nNoise\n1.0\nJPEG\n1.0\nParameterized\nGaussian Blur\n0.5\nDeblur with {sigma} or Deblur\nDownsample\n0.5\nUpsample to {resizing factor} or Upsample\nGausian Noise\n0.5\nDenoise with {sigma} or Denoise\nJPEG\n0.5\nDejpeg with quality {jpeg quality} or Dejpeg\nTable 2. Our training degradation is randomly sampled in these\ntwo pipeline with 50% each. (1) Degraded images y synthesized\nby Real-ESRGAN are paired with the same restoration prompt\ncr = \u201cRemove all degradation\u201d (2) In other 50% iterations, im-\nages generated by our parameterized pipeline are paired with ei-\nther a restoration type prompt (e.g.,\u201cDeblur\u201d) or a restoration pa-\nrameter prompt (e.g., \u201cDeblur with sigma 0.3;\u201d).\nand Real-ESRGAN degradation is for blind restoration. To\naddress this, we construct a new setting including training\ndata generation and test benchmarks for evaluating text-\nbased restoration models.\nOur parameterized degradation pipeline is based on\nthe Real-ESRGAN [63] which contains a large number of\ndegradation types. Since the original full Real-ESRGAN\ndegradation is difficult to be parameterized and represented\nas user-friendly natural language (e.g., not every one un-\nderstands \u201canisotropic plateau blur with beta 2.0\u201d), we\nchoose the 4 most general degradations which are practi-\ncal for parameterization to support degradation parameter-\naware restoration, as shown in Tab. 2. Our parameterized\npipeline skips each degradation stage with a 50% proba-\nbility to increase the diversity of restoration prompts and\nsupport single task restoration. The descriptions of the se-\nlected degradations are appended following the order of the\nimage degradations to synthesize cr. In real application,\nusers can control the restoration type and its strength by\nmodifying restoration prompt. As shown in third column\nof Fig. 3, driven by restoration prompt \u201dDeblur with sigma\n3.0\u201d, our framework removes the Gaussian blur, while pre-\nserving motion blur. The representation of the restoration\nstrength is also general enough to decouple different tasks\nin natural language as shown in Fig. 6. More visual results\nare presented in our supplement.\nTraining data construction. In the training stage, we sam-\nple near 100 million text-image pairs from WebLi [6] and\nuse the alternative text label with highest relevance as the\nsemantic prompt cs in the framework. We drop out the\nsemantic prompt to empty string \u2205 by a probability of\n10% to support semantic-agnostic restoration. We mix the\nReal-ESRGAN degradation pipeline and our parameterized\npipeline by 50% each in training iterations.\nEvaluation Setting We randomly choose 3000 pairs of im-\nage x and semantic prompt cs from the MS-COCO [28] val-\nidation set. Then, we build two test sets on parameterized\nand the Real-ESRGAN processes separately. At the left half\nof Tab. 1, images are sent to the parameterized degradation\nto form the synthesize degradation image y with restoration\nprompt cr. In this setting, we expect \u201cOurs\u201d model to fully\nutilize the cs describing the semantic and the cr describ-\ning the degradations. On the right half, the same images\nare fed to the Real-ESRGAN degradation\u2014so the degrada-\ntion is aligned with open-sourced checkpoints [25, 29, 60].\nBecause not every model can utilize both the semantic\n(cs) and restoration (cr) information,\nTab. 1 provides a\n\u201cPrompts\u201d column denoting what information the model re-\nlied on. In summary, our constructed benchmarks based on\nMS-COCO can serve multiple purposes\u2014from fully-blind,\ncontent-prompted, to all-prompted restoration.\nEvaluation Metrics. We use the generative metric FID [18]\nand perceptual metric LPIPS [72] for quantitative evalua-\ntion of image quality. PSNR, and SSIM are reported for\nreference. We also evaluate similarity scores in the clip em-\nbedding space with ground truth image (CLIP-I) and cap-\ntion (CLIP-T).\n6\nInput\nDiffBIR [29]\nDiffBIR + SDEdit [31]\nDiffBIR + CLIP [41]\nControlNet-SR [71]\nOur TIP Model\nGround-Truth\ncs =\u201cA blue, white and red fire hydrant sitting on a sidewalk.\u201d\ncs =\u201cFour young giraffes in a zoo, with one of them being fed leaves by a person.\u201d\ncs =\u201cTwo hands holding and dialing a cellular phone.\u201d\ncs =\u201cAn old boat sitting in the middle of a field.\u201d\nFigure 4. Visual Comparison with other baselines. Our method of integrating both the semantic prompt cs and the restoration prompt\ncr outperforms imge-to-image restoration (DiffBIR, Retrained ControlNet-SR) and naive zero-shot combination with semantic prompt. It\nachieves more sharp and clean results while maintaining consistency with the degraded image.\nMethod\nFID\u2193\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nCLIP-I\u2191\nDiffBIR [29] (zero-shot) 30.71\n0.354\n22.01\n0.526\n0.921\nStableSR\n24.44\n0.311\n21.62\n0.533\n0.928\nOurs w/o text (zero-shot) 28.80\n0.352\n21.68\n0.549\n0.927\nOurs w/o text (finetuned) 22.45\n0.321\n21.38\n0.532\n0.932\nTable 3. Numerical results on DIV2K testset without any prompt.\n4.2. Comparing with baselines\nWe compare our method with three categories of baselines:\n\u2022 Open-sourced image restoration checkpoints, including\nthe regression model SwinIR [25] and the latent diffusion\nmodels StableSR [60] and DiffBIR [29]\n\u2022 Combining the state-of-the-art method [29] with the zero-\nshot post-editing [31] (DiffBIR+SDEdit) or zero-shot in-\njection through CLIP [41] (DiffBIR + CLIP).\n\u2022 ControlNet-SR [71], an image-to-image model [71] re-\ntrained by us for same iteration. It can be driven in zero-\nshot way as ControlNet-SR+CLIP.\nQuantitative Comparison with Baselines is presented\nin Tab. 1.\nOn the left, we evaluate our full model with\nthe parameterized degradation test set. Since open-sourced\nbaselines are pretrained on Real-ESRGAN degradation, not\nperfectly aligned with our parameterized degradation, we\nalso evaluate our full model on MS-COCO with Real-\nESRGAN degradation by setting our restoration prompt\nto \u201cRemove all degradation\u201d.\nThanks to our prompts\nguidance and architecture improvements, our full model\nachieves best FID, LPIPS, CLIP-Image score, which means\nbetter image quality and semantic restoration. \u201cOurs w/o\ntext\u201d is the same checkpoint conditioned on only degrada-\ntion image and has high pixel-level similarity (the second\nbest PSNR and SSIM) with GT. Although SwinIR has high-\nest PSNR, its visual results are blurry. Although combining\nsemantic prompt in zero-shot way can bring marginal im-\nprovement in FID and CLIP similarity with caption, it dete-\nriorates the image restoration capacity and results in worse\nLPIPS and CLIP-Image. In contrast, semantic prompt guid-\nance improves the CLIP image similarity of our full model.\nTab. 3 shows the evaluation results of our model on existing\nDIV2K test set provided by StableSR. In zero-shot test set-\nting, our model has lower FID and LPIPS than DiffBIR. To\nfairly compare with StableSR, we finetune our model for\n10k iterations on their training set with empty prompt \u2205.\nOur finetuned model also outperforms StableSR.\n7\nInput\n\u201c\u201d\n\u201cBichon Frise dog\u201d\n\u201cgrey wolf\u201d\n\u201cwhite fox\u201d\nReference\nInput\n\u201c\u201d\n\u201clion\u201d\n\u201c tiger\u201d\n\u201cbear\u201d\nReference\nFigure 5. Test-time semantic prompting. Our framework restores degraded images guided by flexible semantic prompts, while unrelated\nbackground elements and global tones remain aligned with the degraded input conditioning.\n\u201cDenoise with sigma 0.06\u201d\n\u201c... 0.12\u201d\n\u201c... 0.18\u201d\n\u201c... 0.24\u201d\n\u201cDeblur with sigma 1.5\u201d\n\u201c... 3.0\u201d\n\u201c... 4.5\u201d\n\u201c... 6.0\u201d\nInput\nOurs-Full\nReference\n\u201cRemove all Degradation\u201d\nInput\nFigure 6. Prompt space walking visualization for the restora-\ntion prompt.\nGiven the same degraded input (upper left) and\nempty semantic prompt \u2205, our method can decouple the restora-\ntion direction and strength via only prompting the quantitative\nnumber in natural language. An interesting finding is that our\nmodel learns a continuous range of restoration strength from dis-\ncrete language tokens.\nQualitative Comparison with Baselines is demonstrated\nin Fig. 4, where the corresponding semantic prompt is pro-\nvided below each row. Image-to-image baselines such as\nDiffBir and the retrained ControlNet-SR can easily gener-\nate artifacts (e.g., hydrant in the first row) and blurry images\n(e.g., hands example in third row) that may look close to an\n\u201caverage\u201d estimation of real images. Besides, naive combi-\nnation of restoration and semantic prompt in the zero-shot\napproach results in heavy hallucinations and may fail to pre-\nserve the object identity in the input image (e.g., giraffes\non the second row). Unlike the existing methods, our full\nmodel considers semantic prompt, degradation image and\nMethod\nSem cs\nRes cr\nFID\u2193 LPIPS\u2193 PSNR\u2191 SSIM\u2191 CLIP-I\u2191 CLIP-T\u2191\nControlNet-SR\n%\n%\n13.65 0.222\n23.75\n0.669\n0.938\n0.300\nControlNet-SR\n!\n!\n12.14 0.223\n23.50\n0.660\n0.940\n0.306\nOurs\n!\nwo param 11.58 0.223\n23.61\n0.665\n0.942\n0.305\nOurs\n!\n!\n11.34 0.219\n23.61\n0.665\n0.943\n0.305\nTable 4. Ablation of architecture and degradation strength in cr\nrestoration prompt in both training and test stages, which\nmakes its results more aligned with all conditions.\n4.3. Prompting the TIP\nSemantic Prompting. Our model can be driven by user-\ndefined semantic prompts at test time. As shown in Fig. 5,\nthe blind restoration with empty string generates ambiguous\nand unrealistic identity. In contrast, our framework recon-\nstructs sharp and realistic results. The object identity accu-\nrately follows the semantic prompt from the user, while the\nglobal layout and color tone remain consistent with input.\nRestoration Prompting. Users also have the freedom to\nadjust the degradation types and strengths in the restoration\nprompts. As shown in Fig. 6, the denoising and deblur-\nring restoration are well decoupled. Using the task-specific\nprompts, our method can get a clean but blur (second row),\nor sharp but noisy (third row) result. In addition, our model\nlearns the continuous space of restoration as the result be-\ncomes cleaner progressively if we modify the \u201cDenoise with\nsigma 0.06\u201d to \u201c0.24.\u201d. Beyond purely relying on numeri-\ncal description, our model \u201cunderstand\u201d the difference of\nstrength definition for tasks, such as the range difference of\ndenoise sigma and deblur sigma, which makes it promis-\ning to be a universal representation for restoration strength\ncontrol.\n4.4. Ablation Study\nWe conduct several ablations in Tab. 4.\nProviding both\nprompts cs, cr to \u201cControlNet-SR\u201d baseline improves the\n8\nFID by 1.5. Adding the modulation fusion layer (\u201cOurs\u201d)\nfurther improves the generative quality (0.8 for FID) and\npixel-level similarity (0.11 dB). Embedding degradation pa-\nrameters in the restoration prompt, not only enables restora-\ntion strength prompting (Fig. 6), but also improves the aver-\nage image quality as shown by FID and LPIPS. More details\nare presented in the supplement.\n5. Conclusion\nWe have presented a unified text-driven framework for in-\nstructed image processing using semantic and restoration\nprompts. We design our model in a decoupled way to better\npreserve the semantic text-to-image generative prior while\nefficiently learning to control both the restoration direction\nand its strength. To the best of our knowledge, this is the\nfirst framework to support both semantic and parameter-\nembedded restoration instructions simultaneously, allowing\nusers to flexibly prompt-tune the results up to their expec-\ntations.\nOur extensive experiments have shown that our\nmethod significantly outperforms prior works in terms of\nboth quantitative and qualitative results. Our model and\nevaluation benchmark have established a new paradigm of\ninstruction-based image restoration, paving the way for fur-\nther multi-modality generative imaging applications.\nAcknowledgments. We are grateful to Kelvin Chan and\nDavid Salesin for their valuable feedback. We also extend\nour gratitude to Shlomi Fruchter, Kevin Murphy, Moham-\nmad Babaeizadeh, and Han Zhang for their instrumental\ncontributions in facilitating the initial implementation of the\nlatent diffusion model.\nReferences\n[1] Abdullah Abuolaim, Mauricio Delbracio, Damien Kelly,\nMichael S Brown, and Peyman Milanfar. Learning to re-\nduce defocus blur by realistically modeling dual-pixel data.\nIn ICCV, pages 2289\u20132298, 2021. 3\n[2] Eirikur Agustsson and Radu Timofte.\nNTIRE 2017 chal-\nlenge on single image super-resolution: Dataset and study.\nIn 2017 IEEE Conference on Computer Vision and Pattern\nRecognition Workshops, CVPR Workshops 2017, Honolulu,\nHI, USA, July 21-26, 2017, pages 1122\u20131131. IEEE Com-\nputer Society, 2017. 5\n[3] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In CVPR,\npages 18208\u201318218, 2022. 2\n[4] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. ACM Transactions on Graphics, 2023. 2\n[5] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn CVPR, 2023. 3\n[6] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Alexander Good-\nman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Has-\nsan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal,\nJames Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,\nChao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas\nSteiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali: A jointly-scaled multilingual language-\nimage model. In ICLR, 2023. 2, 4, 6\n[7] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,\nand Matthieu Cord.\nDiffedit:\nDiffusion-based seman-\ntic image editing with mask guidance.\narXiv preprint\narXiv:2210.11427, 2022. 2\n[8] Mauricio Delbracio and Peyman Milanfar. Inversion by di-\nrect iteration: An alternative to denoising diffusion for image\nrestoration.\nTransactions on Machine Learning Research,\n2023. Featured Certification. 2, 3\n[9] Mauricio Delbracio, Hossein Talebei, and Peyman Milanfar.\nProjected distribution loss for image enhancement. In 2021\nIEEE International Conference on Computational Photogra-\nphy (ICCP), pages 1\u201312. IEEE, 2021. 2\n[10] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\ngans on image synthesis.\nNeural Information Processing\nSystems, 2021. 3, 4\n[11] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang. Image super-resolution using deep convolutional net-\nworks. IEEE Trans. Pattern Anal. Mach. Intell., 38(2):295\u2013\n307, 2016. 3\n[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion. In ICLR, 2023. 3\n[13] Leonardo Galteri, Lorenzo Seidenari, Marco Bertini, and A.\nBimbo. Deep generative adversarial compression artifact re-\nmoval. In ICCV, 2017. 3\n[14] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang\nGu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong\nChen, and Baining Guo.\nInstructdiffusion: A generalist\nmodeling interface for vision tasks. CoRR, abs/2309.03895,\n2023. 3\n[15] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.\nOpen-vocabulary object detection via vision and language\nknowledge distillation. In ICLR, 2021. 3\n[16] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,\nDimitris Metaxas, and Feng Yang. Svdiff: Compact param-\neter space for diffusion fine-tuning. In ICCV, pages 7323\u2013\n7334, 2023. 3\n[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-or. Prompt-to-prompt image\nediting with cross-attention control. In ICLR, 2023. 2, 3\n[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In NeurIPS, 2017. 6\n[19] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance, 2022. 4\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. NeurIPS, 33:6840\u20136851, 2020.\n3\n9\n[21] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In\nICLR, 2022. 3\n[22] Yitong Jiang, Zhaoyang Zhang, Tianfan Xue, and Jinwei Gu.\nAutodir: Automatic all-in-one image restoration with latent\ndiffusion. arXiv preprint arXiv:2310.10123, 2023. 2, 3\n[23] Junjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Peyman Mi-\nlanfar, and Feng Yang. Vila: Learning image aesthetics from\nuser comments with vision-language pretraining. In CVPR,\npages 10041\u201310051, 2023. 3\n[24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu.\nMulti-concept customiza-\ntion of text-to-image diffusion. In CVPR, pages 1931\u20131941,\n2023. 3\n[25] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang,\nLuc Van Gool, and Radu Timofte. Swinir: Image restoration\nusing swin transformer. In Proceedings of ICCV Workshops,\n2021. 2, 3, 6, 7\n[26] Zhexin Liang, Chongyi Li, Shangchen Zhou, Ruicheng\nFeng, and Chen Change Loy. Iterative prompt learning for\nunsupervised backlit image enhancement. In ICCV, pages\n8094\u20138103, 2023. 3\n[27] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\nKyoung Mu Lee. Enhanced deep residual networks for single\nimage super-resolution. In Proceedings of CVPR Workshops,\n2017. 3\n[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollar, and Larry Zit-\nnick. Microsoft coco: Common objects in context. In ECCV.\nECCV, 2014. 6\n[29] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei,\nBo Dai, Wanli Ouyang, Yu Qiao, and Chao Dong.\nDiff-\nbir: Towards blind image restoration with generative diffu-\nsion prior. arXiv preprint arXiv:2308.15070, 2023. 4, 6, 7,\n15\n[30] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens\nSj\u00a8olund, and Thomas B Sch\u00a8on. Controlling vision-language\nmodels for universal image restoration.\narXiv preprint\narXiv:2310.01018, 2023. 2, 3, 15\n[31] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In ICLR, 2022. 3, 6, 7, 15\n[32] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models. In CVPR, pages 6038\u20136047,\n2023. 2\n[33] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 3\n[34] Seungjun Nah, Radu Timofte, Sungyong Baik, Seokil\nHong, Gyeongsik Moon, Sanghyun Son, Kyoung Mu Lee,\nXintao Wang, Kelvin C. K. Chan, Ke Yu, Chao Dong,\nChen Change Loy, Yuchen Fan, Jiahui Yu, Ding Liu,\nThomas S. Huang, Hyeonjun Sim, Munchurl Kim, Dong-\nwon Park, Jisoo Kim, Se Young Chun, Muhammad Haris,\nGreg Shakhnarovich, Norimichi Ukita, Syed Waqas Zamir,\nAditya Arora, Salman H. Khan, Fahad Shahbaz Khan, Ling\nShao, Rahul Kumar Gupta, Vishal M. Chudasama, Heena\nPatel, Kishor P. Upla, Hongfei Fan, Guo Li, Yumei Zhang,\nXiang Li, Wenjie Zhang, Qingwen He, Kuldeep Purohit,\nA. N. Rajagopalan, Jeonghun Kim, Mohammad Tofighi,\nTiantong Guo, and Vishal Monga. NTIRE 2019 challenge\non video deblurring: Methods and results. In IEEE Con-\nference on Computer Vision and Pattern Recognition Work-\nshops, CVPR Workshops 2019, Long Beach, CA, USA, June\n16-20, 2019, pages 1974\u20131984. Computer Vision Founda-\ntion / IEEE, 2019. 5\n[35] OpenAI. Gpt-4 technical report, 2023. 3\n[36] Roni Paiss, Hila Chefer, and Lior Wolf. No token left behind:\nExplainability-aided image classification and generation. In\nECCV, 2022. 3, 5\n[37] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar\nMosseri, Michal Irani, and Tali Dekel. Teaching clip to count\nto ten. In ICCV, 2023. 3, 5\n[38] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In ACM SIGGRAPH 2023 Conference Proceed-\nings, pages 1\u201311, 2023. 2, 3\n[39] Mangal Prakash, Mauricio Delbracio, Peyman Milanfar, and\nFlorian Jug. Interpretable unsupervised diversity denoising\nand artefact removal. In ICLR, 2022. 3\n[40] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 1(8):9, 2019. 3\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 5, 6, 7, 15\n[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2\n[43] Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido\nGerig, and Peyman Milanfar.\nMultiscale structure guided\ndiffusion for image deblurring.\nIn ICCV, pages 10721\u2013\n10733, 2023. 3\n[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 2, 3, 5\n[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 4\n[46] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi.\nPalette: Image-to-image diffusion models.\nIn\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u2013\n10, 2022. 3\n10\n[47] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 2022. 2, 3\n[48] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-\nimans, David J Fleet, and Mohammad Norouzi.\nImage\nsuper-resolution via iterative refinement. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 45(4):4713\u2013\n4726, 2022. 3, 4\n[49] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-\nimans, David J Fleet, and Mohammad Norouzi.\nImage\nsuper-resolution via iterative refinement. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 45(4):4713\u2013\n4726, 2022. 3\n[50] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In ICLR. OpenReview.net,\n2022. 4\n[51] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 3, 4\n[52] Yang Song and Stefano Ermon. Generative modeling by es-\ntimating gradients of the data distribution. In NeurIPS, pages\n11895\u201311907, 2019. 3, 12\n[53] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In ICLR, 2021. 4, 5\n[54] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solv-\ning inverse problems in medical imaging with score-based\ngenerative models. In ICLR. OpenReview.net, 2022. 12\n[55] Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo\nSapiro, Wolfgang Heidrich, and Oliver Wang. Deep video\ndeblurring for hand-held cameras. In CVPR, pages 1279\u2013\n1288, 2017. 3\n[56] Xuansong Xie Tao Yang, Peiran Ren and Lei Zhang. Gan\nprior embedded network for blind face restoration in the\nwild. In CVPR, 2021. 2\n[57] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,\nPeyman Milanfar, Alan C. Bovik, and Yinxiao Li. MAXIM:\nmulti-axis MLP for image processing. In CVPR, 2022. 2, 3\n[58] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation.\nIn CVPR, pages 1921\u20131930,\n2023. 2, 3\n[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NeurIPS, 30, 2017. 5\n[60] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK\nChan, and Chen Change Loy.\nExploiting diffusion prior\nfor real-world image super-resolution.\narXiv preprint\narXiv:2305.07015, 2023. 2, 4, 6, 7, 12\n[61] Longguang Wang, Yingqian Wang, Zaiping Lin, Jungang\nYang, Wei An, and Yulan Guo. Learning a single network\nfor scale-arbitrary super-resolution. In ICCV, pages 4801\u2013\n4810, 2021. 3\n[62] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. To-\nwards real-world blind face restoration with generative facial\nprior. In CVPR, 2021. 2\n[63] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.\nReal-esrgan: Training real-world blind super-resolution with\npure synthetic data. In ICCV, pages 1905\u20131914, 2021. 2, 3,\n4, 6\n[64] Xiaohang Wang, Xuanhong Chen, Bingbing Ni, Hang Wang,\nZhengyan Tong, and Yutian Liu. Deep arbitrary-scale image\nsuper-resolution via scale-equivariance pursuit.\nIn CVPR,\n2023. 3\n[65] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang\nZhou, Jianzhuang Liu, and Houqiang Li. Uformer: A gen-\neral u-shaped transformer for image restoration. In CVPR,\n2022. 2, 3\n[66] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan\nSaharia, Alexandros G Dimakis, and Peyman Milanfar. De-\nblurring via stochastic refinement. In CVPR, pages 16293\u2013\n16303, 2022. 2, 3\n[67] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar\nHayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling\nShao. Multi-stage progressive image restoration. In CVPR,\n2021. 3\n[68] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and\nLei Zhang. Beyond a Gaussian denoiser: Residual learning\nof deep CNN for image denoising. IEEE Transactions on\nImage Processing, 26(7):3142\u20133155, 2017. 3\n[69] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward\na fast and flexible solution for CNN based image denoising.\nIEEE Transactions on Image Processing, 2018. 3\n[70] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo-\nfte. Designing a practical degradation model for deep blind\nimage super-resolution. In ICCV, pages 4791\u20134800, 2021. 3\n[71] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, 2023. 2, 3, 5, 6, 7\n[72] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 6\n[73] Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang,\nKunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.\nTip-adapter:\nTraining-free clip-adapter for better vision-\nlanguage modeling. In ECCV, 2022. 3\n11\nA. Implementation Details\nA.1. Derivation Details in Equation 2\nWe\npropose\nto\ncompute\nthe\ndistribution\nof\nlatents\np(zt|y, cs, cr) conditioned on degraded image y, seman-\ntic prompt cs and restoration prompt cr. Using the Bayes\u2019\ndecomposition similar to score-based inverse problem [52,\n54], we have\np(zt|y, cs, cr) = p(zt, y, cs, cr)/p(y, cs, cr).\n(4)\nThen, we compute gradients with respect to zt, and remove\nthe gradients of input condition \u2207zt log p(y, cs, cr) = 0 as:\n\u2207ztlog p(zt|y, cs, cr)\n=\n\u2207zt log p(zt, y, cs, cr)\n(5)\n=\n\u2207zt log[p(cs) \u00b7 p(zt|cs) \u00b7 p(y, cr|cs, zt)]\n(6)\n=\n\u2207zt log[p(zt|cs) \u00b7 p(y, cr|cs, zt)]\n(7)\n=\n\u2207zt log p(zt|cs) + \u2207zt log p(y, cr|cs, zt) (8)\nWe assume y is generated through a degradation pipeline\nas y = Deg(x, cr), thus it is independent of cs with x and\ncr provided as condition. Removing redundant cs condi-\ntion, the second term in the last equation can be approxi-\nmated as:\n\u2207zt log p(y, cr|cs, zt)\n\u2248\n\u2207zt log p(y, cr|zt)\n(9)\n=\n\u2207zt log p(cr|zt) + \u2207zt log p(y|zt, cr)\n(10)\n=\n\u2207zt log p(y|zt, cr)\n(11)\nIn summary of the above equations, we derive the Equa-\ntion 2 in the main manuscript\n\u2207ztlog p(zt|y, cs, cr)\n\u2248 \u2207zt log p(zt|cs)\n|\n{z\n}\nSemantic-aware (frozen)\n+ \u2207zt log p(y|zt, cr),\n|\n{z\n}\nRestoration-aware (learnable)\n(12)\nwhere \u2207zt log p(y|zt, cr) is synthesized using stochastic\ndegradation pipeline y = Deg(x, cr) to train our Control-\nNet.\nA.2. Pseudo Code for Degradation Synthesis\nTo\nsupport\nthe\nlearning\nof\nrestoration-aware\nterm\n\u2207zt log p(y|zt, cr),\nwe\nsynthesized\nthe\ndegradation\nimage y using clean image x with the algorithm presented\nin Algorithm 1.\nFirst, we randomly choose one from\nReal-ESRGAN pipeline and our parameterized degra-\ndation.\nThen the degraded image from Real-ESRGAN\npipeline is paired with restoration prompt cr =\u201cRemove\nall degradation\u201d.\nIn our parameterized degradation, all\nprocesses are paired with restoration prompts crp listed in\nTable 2 of the main manuscript (e.g., Deblur with sigma\n3.0).\nAlgorithm 1 TIP Degradation Pipeline in Training\nInputs: x: Clean image\nOutputs: y: Degraded image; cr: Restoration prompt\ntype \u2190 RANDCHOICE(Real-ESRGAN, Param)\nif type = Real-ESRGAN then\n// Real-ESRGAN degradation\ny \u2190 x\nDeg \u2190 RANDOM(Real-ESRGAN-Degradation)\nfor PROCESS in Deg do:\ny \u2190 PROCESS(y)\nend for\ncr \u2190 \u201cRemove all degradation\u201d\nelse\n// Parameterized degradation\ncr \u2190 \u2205\ny \u2190 x\nDeg \u2190 RANDOM(Parametrized-Degradation)\nfor PROCESS, crp in Deg do:\ny \u2190 PROCESS(y, crp)\ncr \u2190 CONCAT(cr, crp)\nend for\nend if\nreturn y, cr\nB. More Ablation Study\nTab. 5 provides more comprehensive ablations of text\nprompts by providing different information to our image-\nto-image baseline. Semantic prompts significantly improve\nimage quality as shown in better FID and CLIP-Image, but\nreduce the similarity with ground truth image. Restoration\ntypes and parameters embedded in the restoration prompts\nboth improve image quality and fidelity. Tab. 6 presents a\ncomparison of our skip feature modulation fskip with that in\nStableSR [60] which modulates both skip feature fskip from\nencoder and upsampling feature fup from decoder. We ob-\nserve that modulating fup does not bring obvious improve-\nments. One possible reason is that \u03b3 and \u03b2 of the middle\nlayer adapts to the feature in the upsampling layers.\nC. More Semantic Prompting\nAs shown in the Fig. 7, our framework supports semantic\ndescriptions for restoring multiple objects.\nD. More Restoration Prompting\nFig. 8 shows the application of restoration prompt on\nout-of-domain data, including Midjourney image and real-\nworld cartoon. Since these images are not in our training\ndata domain, a blind enhancement with prompt \u201cRemove\nall degradation\u201d can not achieve satisfying results. Utiliz-\n12\nInput\n\u201c\u201d,\n\u201cpeppers ... potatoes\u201d,\n\u201cbananas... stones\u201d\n\u201cleaves... potatoes\u201d\n\u201cbananas...potatoes\u201d\nReference\nFigure 7. More semantic prompting for images with multiple objects.\nMethod\nSem\nRes Type\nRes Param\nFID\u2193\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nCLIPim\u2191\nCLIPtx\u2191\nOurs\n%\n%\n%\n13.60\n0.221\n23.65\n0.664\n0.939\n0.300\nOurs\n!\n%\n%\n11.71\n0.226\n23.55\n0.663\n0.941\n0.305\nOurs\n!\n!\n%\n11.58\n0.223\n23.61\n0.665\n0.942\n0.305\nOurs\n!\n!\n!\n11.34\n0.219\n23.61\n0.665\n0.943\n0.306\nTable 5. Ablation of prompts provided during both training and testing. We use an image-to-image model with our modulation fusion layer\nas our baseline. Providing semantic prompts significantly increases the image quality (1.9 lower FID) and semantic similarity (0.002 CLIP-\nImage), but results in worse pixel-level similarity. In contrast, degradation type information embedded in restoration prompts improves\nboth pixel-level fidelity and image quality. Utilizing degradation parameters in the restoration instructions further improves these metrics.\nMethod\nModulate fskip Modulate fup Relative Param FID\u2193 LPIPS\u2193\nOurs w/ prompts\n%\n%\n1\n12.14\n0.223\nOurs w/ prompts\n!\n!\n1.06\n11.21\n0.219\nOurs w/ prompts\n!\n%\n1.03\n11.34\n0.219\nTable 6. Ablation of the architecture. Modulating the skip feature\nfskip improves the fidelity of the restored image with 3% extra\nparameters in the adaptor, while further modulating the backbone\nfeatures fup does not bring obvious advantage.\nInput\nRemove all degradation\nUpsample... Deblur...\nInput\nRemove all degradation\nUpsample...Deblur...Dejpeg\nFigure 8. Restoration prompting for out-of-domain images.\ning restoration prompting (e.g., \u201cUpsample to 6.0x; Deblur\nwith sigma 2.9;\u201d) can successfully guide our model to im-\nprove the details and color tones of the Midjourney image.\nIn the second row, a manually designed restoration prompt\nalso reduces the jagged effect to smooth the lines in the car-\ntoon image.\nTo study whether the model follows restoration instruc-\ntions, a dense walking of restoration prompt is presented\nin Fig. 9. From left to right, we increase the strength of de-\nnoising in the restoration prompt. From top to the bottom,\nthe strength of deblurring gets larger. The results demon-\nstrates that our restoration framework refines the degraded\nimage continuously following the restoration prompts\nE. More Visual Comparison\nMore visual comparisons with baselines are provided in\nFig. 10.\n13\nDenoise with sigma\n0.0\n0.06\n0.12\n0.18\n0.24\nDeblur with sigma\n1.5\n3.0\n4.5\n6.0\n0.0\nFigure 9. Prompt space walking visualization for the restoration prompt. Given the same degraded input (upper left) and empty\nsemantic prompt \u2205, our method can decouple the restoration direction and strength via only prompting the quantitative number in\nnatural language. An interesting finding is that our model learns a continuous range of restoration strength from discrete language tokens.\n14\nInput\nDiffBIR [29]\nDiffBIR + SDEdit [31]\nDiffBIR + CLIP [41]\nControlNet-SR [30]\nOurs TIP Model\nGround-Truth\nA tall giraffe eating leafy greens in a jungle.\nZebras crossing a bitumen road in the savannah.\nA gray train riding on a track.\nTwo giraffes are standing together outside near a wall.\ntwo brown bears on some rocks.\nA stop sign with graffiti on it nailed to a pole.\nTwo zebras are heading into the bushes as another is heading in the opposite direction.\nA street sign surrounded by orange and red leaves.\nA group of cows on street next to building and bus.\nFigure 10. Main visual comparison with baselines. (Zoom in for details)\n15\n"
  },
  {
    "title": "Customize-It-3D: High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior",
    "link": "https://arxiv.org/pdf/2312.11535.pdf",
    "upvote": "5",
    "text": "Customize-It-3D: High-Quality 3D Creation from A Single Image Using\nSubject-Specific Knowledge Prior\nNan Huang\nPeking University\nhannahuang411@gmail.com\nTing Zhang\nMicrosoft Research\ntinzhan@microsoft.com\nYuhui Yuan\nMicrosoft Research\nyuyua@microsoft.com\nDong Chen\nMicrosoft Research\ndoch@microsoft.com\nShanghang Zhang\nPeking University\nshanghang@pku.edu.cn\nReference\nNormal\nNovel Views\nReference\nNormal\nNovel Views\nFigure 1. Customize-It-3D has the capability to generate high-fidelity 3D content from just a single image. We present the normal map and\nnovel-view renderings for general objects, highlighting the consistency texture and geometry with remarkable quality at novel views.\nAbstract\nIn this paper, we present a novel two-stage approach\nthat fully utilizes the information provided by the refer-\nence image to establish a customized knowledge prior for\nimage-to-3D generation. While previous approaches pri-\nmarily rely on a general diffusion prior, which struggles to\nyield consistent results with the reference image, we pro-\npose a subject-specific and multi-modal diffusion model.\nThis model not only aids NeRF optimization by consider-\ning the shading mode for improved geometry but also en-\nhances texture from the coarse results to achieve superior\nrefinement. Both aspects contribute to faithfully aligning\nthe 3D content with the subject.\nExtensive experiments\nshowcase the superiority of our method, Customize-It-3D,\noutperforming previous works by a substantial margin. It\nproduces faithful 360-degree reconstructions with impres-\nsive visual quality, making it well-suited for various appli-\ncations, including text-to-3D creation. Our project page is:\nhttps://nnanhuang.github.io/projects/customize-it-3d/\n1. Introduction\n3D creation from a single image remains a profoundly intri-\ncate challenge, due to its inherently ill-posed nature, arising\nfrom the significant information gap between a single 2D\nimage and the comprehensive 3D spatial and textural prop-\nerties of the depicted object. Despite its great challenge, re-\ncent works [38, 46, 58, 60, 69] have achieved notable results\nin image-to-3D creation. They draw inspiration from text-\nto-3D methods [45, 65] which utilize score distillation sam-\npling (SDS) loss to optimize a neural radiance field (NeRF).\nThis SDS loss serves as a guiding principle for novel view\nsynthesis, emulating the way humans utilize priors to in-\nfer 3D shape and texture from a single image. While these\nmethods tend to produce promising results, they still exhibit\nnoticeable inconsistencies when compared with the refer-\nence view, either in context of the geometry (i.e., multi-face\n1\narXiv:2312.11535v2  [cs.CV]  9 Jan 2024\nissue) or from the texture perspective (novel views often\nlack fidelity, presenting smooth and disruptive perticulars).\nWhen crafting intricate 3D models of a specific object,\na professional artist indeed need not be equipped with an\nexhaustive understanding of all the objects in the natural\nworld. Rather, the essential requirement lies in possessing\nspecialized, profound knowledge pertaining to that specific\nsubject at hand. This targeted expertise shall be more in-\nstrumental in the meticulous creation of 3D assets.\nDe-\nspite incorporating several adjustments such as CLIP loss\nand textual inversion considered in [46, 60], the supervi-\nsion of novel views still relies on a general text-to-image\nmodel [3, 18, 49, 51, 53], which has challenge in produc-\ning consistent results based solely on text, given the inher-\nent difficulty in crafting textual descriptions to encompass\nevery detail. Consequently, these approaches struggle to\nmaintain consistency and fall short in reconstructing high-\nfidelity 3D objects. This realization has inspired us to ex-\nplore strategies for cultivating subject-specific knowledge\nprior, aiming to achieve high-quality image-to-3D creation.\nWe propose Customize-It-3D, a novel approach that fully\nunleashes the geometry/appearance information provided\nby the reference image to promote customized knowledge\nprior for 3D generation. Customize-It-3D, like the previous\nmethods mentioned above, adopts a two-stage framework in\na coarse-to-fine manner. However, it is guided with subject-\nspecific knowledge prior.\nIn the first stage, we optimize a neural radiance field to\nlearn an implicit volume representation. The reference view\nis directly supervised through rendering loss, specifically\npixel differences. For novel views, unlike conventional ap-\nproaches employing general T2I models for SDS loss, we\nuse a subject-specific T2I model. There has been remark-\nable success in subject-driven image generation [29, 52].\nThey typically demand at least 3 to 5 images capturing the\nsame subject with varying contexts in order to model the\nkey visual features of the subject. However, our scenario\npresents a challenge as we possess only a solitary image,\nwhich may potentially misguide the model to a trivial so-\nlution (mode collapse).\nTo surmount this limitation, we\npropose to exploit multiple modalities extracted from the\nreference image, namely depth map, mask image, and nor-\nmal map. Furthermore, by adjusting the model to be cog-\nnizant of the subject\u2019s geometry information, we can super-\nvise NeRF generation in a more fine-grained manner, par-\nticularly by taking the shading mode into consideration.\nWhile NeRF is renowned for effectively learning com-\nplex geometries, their proficiency in generating intricate\ntexture details is hindered by significant memory demands.\nTherefore, our objective in the second stage is to enhance\nvisual realism by transforming the coarse NeRF model into\npoint clouds. Conversion to point clouds allows us to have\nthe desirable groundtruth textures on the reference view\nthrough projection. Yet we observe that constructing point\nclouds from depth images as in [60] tends to introduce ge-\nometric inaccuracies since NeRF does not store any 3D ge-\nometry explicitly (only the density field). In lieu of this,\nour novel strategy entails an initial step of mesh genera-\ntion, followed by the established mesh regularization tech-\nniques to refine the shape, before sampling points on the\nmesh surface. In addition, we harness the potential of the\naforementioned subject-specific Text-to-Image (T2I) diffu-\nsion model, effectively elevating the texture realism, before\nextending the RGB rendering from the coarse NeRF to point\nclouds. This augmentation is shown to yield superior tex-\nture quality in the final outcomes.\nBesides employing the proposed subject-specific 2D dif-\nfusion prior, we also utilize a 3D prior to further guide the\ntraining process as in [46], compensating the 3D informa-\ntion. Note that this 3D prior takes the reference image as\ninput and thus both the adopted 2D and 3D prior are subject-\nspecific knowledge priors to guide the novel views.\nTo\nevaluate our method, we collect a benchmark consisting of\n100 images. We compare Customize-It-3D with prior state-\nof-the-arts on RealFusion15 [38], and our Customize100\ndataset.\nExtensive experiments demonstrate our method\nachieves significant improvement in visual quality in con-\ntext of accurate geometry and realistic details. In summary,\nour contributions are:\n\u2022 We propose Customize-It-3D, an innovative framework\nfor the generation of high-quality 3D content from a sin-\ngle image by utilizing a subject-specific diffusion prior,\nwhich is designed to enhance the personalization of 3D\ncontent creation for specific subjects.\n\u2022 We introduce a multi-modal DreamBooth model that\npromotes comprehensive knowledge priors derived from\nmulti-modal images of the subject, prioritizing the faith-\nfulness of the diffusion model to the reference image.\nThis knowledge prior not only facilitates NeRF optimiza-\ntion based on the shading mode but also enhances the tex-\nture of the initial coarse results for better refinement.\n\u2022 The resulting framework demonstrates state-of-the-art\nperformance in 3D reconstruction using a diverse range\nof in-the-wild images and images from existing datasets.\nFurthermore, this approach opens up intriguing possibili-\nties, such as text-to-3D content creation.\n2. Related work\nMuti-view 3D reconstruction. Early works [1, 14, 54, 55]\ntraditionally require a dense set of input images to ac-\ncurately deduce geometry by establishing correspondence\nwithin the overlapping regions.. The advent of NeRF [36,\n39] and its variants [5, 6, 41] have significantly propelled\nthe quality of synthesized novel views through the recon-\nstruction of 3D scenes via implicit neural representations.\nSubsequent efforts [11, 20, 25, 71] have been dedicated to\n2\nthe facilitation of NeRF optimization from a sparser set of\ninput views.\nTypically they entail the extraction of per-\nview features from each input image and aggregate multi-\nview features for each point along the camera ray. This ag-\ngregated information is then decoded to determine density\n(or Signed Distance Function, SDF) and colors. Diffusion\nmodels, which have demonstrated significant advancements\nin general image synthesis, has emerged as a recent focal\npoint in 3D generation research. Some studies endeavor\nto directly train 3D diffusion models based on diverse 3D\nrepresentations, including point clouds [37, 42, 73, 76],\nmeshes [15, 35], neural fields [2, 7, 9, 13, 16, 17, 22, 24,\n26, 28, 43, 64, 74]. Others, such as [4, 57, 59, 66] con-\nceptualize 3D generation as an image-to-image translation\ntask and directly generate coherent multiview color images,\nas exemplified by Zero123 [34]. It\u2019s noteworthy that both\napproaches require access to multi-view images for train-\ning. However, a recurring challenge in this domain lies in\nthe constrained availability of expansive 3D assets datasets.\nConsequently, most studies have been confined to evalua-\ntion on limited categories of shapes or encounter difficulties\nin extending their models effectively on general objects.\n3D generation using knowledge prior.\nA surge of ap-\nproaches have been study leveraging large models pre-\ntrained on billions of images as knowledge prior to guide\nthe 3D synthesis, circumventing the need for large-scale 3D\ndatasets. One popular task is text-to-3D synthesis, given the\nexceptional breakthroughs in text-to-image generation. Pi-\noneering works [40, 56] such as DreamFields [21], Dream-\nFusion [45] and SJC [63] have demonstrated success of in-\ncorporating NeRF-like models with frozen generative sys-\ntems for optimization-based 3D synthesis.\nThey fed the\nrendered 2D images to diffusion models or CLIP model\nfor calculating losses to guide the 3D shape optimization.\nIt sparked numerous follow-up works improve such per-\nshape optimization scheme in the context of 3D representa-\ntions [8, 30, 32, 61], sampling schedules [19], and loss de-\nsign [65], subject-driven editing [48]. An alternative avenue\nof research [33, 38, 46, 60] seeks to generate 3D digital con-\ntent from a single image, offering a more precise and con-\ntrollable approach than text-based methods. The prevailing\nmajority of existing endeavors in this area remain rooted\nin optimization-based techniques, achieving this task by\nimposing pixel-wise reconstruction losses on the reference\nview, in addition to inheriting the SDS loss from text-to-3D\nmethodologies. In comparison, we also follow the Dream-\nFusion pipeline [45], but with a focus on cultivating subject-\nspecific knowledge prior. While Dreambooth3D [48] also\nleverages a subject-specific diffusion prior, it requires multi-\nple subject images and is designed for text-to-3D editing. In\ncontrast, our approach tackles a more challenging scenario\nwith only one image and introduces a multi-modal diffusion\nprior, specifically aiming at image-to-3D generation.\n3. Method\nGenerating a personalized 3D object from a single image is\na highly challenging task, because it involves disclosing the\nidentity, geometric shape, and texture from limited and in-\ncomplete visual information. Therefore, we propose a two-\nstage coarse-to-fine framework, Customize-It-3D, leverag-\ning carefully cultivated subject-specific knowledge prior to\neffectively constrain the coherency of 3D object with re-\nspect to a particular identity. The proposed framework is il-\nlustrated in Figure 2. We will first detail our subject-specific\nknowledge prior and then introduce our method.\n3.1. Subject-Specific Knowledge Prior\nSubject-Specific 2D Prior: Multi-modal DreamBooth.\nCurrent text-to-image diffusion models possess rich seman-\ntic knowledge and 2D image knowledge. DreamFusion [45]\nis a pioneering work that utilizes the latent knowledge of\nthese diffusion models to guide a 3D representation opti-\nmization.\nHowever, due to the abundant imagination of\n2D diffusion models, it is challenging to precisely control\nover the generated geometric shapes, textures, and identi-\nties using solely text, resulting in discrepancies and thereby\ncausing confusion in guiding 3D reconstructions. This phe-\nnomenon tends to produce smoothed textures and imprecise\ngeometries as a result of averaging the inconsistent out-\ncomes. To address this issue, we undertake a fine-tuning\nprocess on a pre-trained text-to-image diffusion model us-\ning a single reference image. To mitigate the risks of overfit-\nting and mode collapse, we choose to leverage multi-modal\nimages in our approach.\nSpecifically, we first utilize a pre-trained monocular\ndepth estimator [50] and a single-view normal estima-\ntor [12] [23] to obtain the depth and normal maps of the\nreference image. Furthermore, we segment the last chan-\nnel of the input image to obtain a mask map. Then, we use\nthis image set and the reference image together to partially\nfine-tune Stable Diffusion [51] with Dreambooth [52], so\nthat it learns to bind a unique identifier with the specific\nsubject embedded in the output domain of the model. We\nalso find that a fully trained Dreambooth tends to overfit the\nsubject viewpoints in input images, leading not only to a\nsevere Janus problem but also reducing diversity, as noted\nin previous work [48]. Therefore, we only use the partially\nfine-tuned results. Unlike Dreambooth [52], we fine-tune\nnot only the UNet but also the text encoder, which leads to\nbetter results.\nAdditionally, we utilize a prior preservation loss, which\nenables the supervision of model training during fine-tuning\nwith self-generated images xori from the original diffusion\nmodel. Then our loss function to finetune a diffusion model\n3\n1. Coarse Stage\n2. Refine Stage\nImage Caption \nModel\nMulti-modality\nDreamBooth\nZero-1-to-3\n+\n{ R,T }\nCamera Pose\nReference\nSubject-Specified Prior\nPrompt\nNormal & Depth\nEstimation Model\nNeRF\nNovel Views\n\ud835\udcdb\ud835\udc85\ud835\udc86\ud835\udc91\ud835\udc95\ud835\udc89\n\ud835\udcdb\ud835\udc93\ud835\udc86\ud835\udc87\nMulti-modal images\n\ud835\udcdb\ud835\udc7a\ud835\udc7a)\ud835\udc7a\ud835\udc6b\ud835\udc7a\nNeRF\nSAM\nPost-processing\nMulti-modality\nDreamBooth\nBuild\nReference View\n.\n.\n.\n.\n.\n.\nDescriptors\nTexture \nPoint Clouds\nZ-buffer & Rasterization\nPseudo Subject Image\nPseudo Mask\n\ud835\udcdb\ud835\udc7a\ud835\udc7a)\ud835\udc7a\ud835\udc6b\ud835\udc7a\nNoise\nEnhanced \nImage\nDeferred\nRenderer\n\ud835\udce4\nFigure 2. We propose a two-stage framework for high-quality 3D creation from a reference image with subject-specific diffusion prior\n(Sec. 3.1). At the coarse stage, we optimize a NeRF for reconstructing the geometry of the reference image in a shading-aware man-\nner(Sec. 3.2). We further build point clouds with enhanced texture from the coarse stage, and jointly optimize the texture of invisible points\nand a learnable deferred renderer to generate realistic and view-consistent textures (Sec. 3.3).\n\u03f5\u03d5 is :\nEx,c,\u03f5,\u03f5\u2032,t[wt||\u03f5\u03d5(\u03b1tx + \u03c3t\u03f5, c) \u2212 x||2\n2+\n\u03bbwt\u2032||\u03f5\u03d5(\u03b1t\u2032xori + \u03c3t\u2032\u03f5\u2032, cori) \u2212 xori||2\n2],\n(1)\nwhere x is the reference image and \u03b1t, \u03c3t, wt control the\nnoise schedule, cori is the prompt to generate xori. The\nsecond term is prior-preservation loss with weight \u03bb.\nIn terms of prompt processing, we need an identifier with\na weak prior in both the language model and the diffusion\nmodel. Therefore, we follow previous work [62] and use the\n\u201csks\u201d identifier for stable diffusion [51]. Differently, since\nwe are dealing with a set of images with different modali-\nties, it is imperative not to employ an identical prompt for\nall images, as this practice can introduce confusion to the\nmodel and lead it to erroneously emphasize common dis-\ntinctive features. Our primary objective is not to model the\nshared aspects across all modalities but rather to equip the\nmodel with the capability to discern and accommodate the\ndistinctions arising from different modalities. Thus, we in-\ncorporate additional instructions, and the prompt c for fine-\ntuning the diffusion model takes the form of \u201ca depth map\n/ normal map / foreground mask / rgb photo of sks [class\nname]\u201d. The choice of the specific instruction is made in\naccordance with the given modality during training. In this\nway, the model is encouraged to perceive the subject from\ndifferent perspectives, enriching its subject-specific prior.\nSubject-Specific 3D Prior:\nViewpoint and Image-\nConditioned Diffusion Model. Presently, large-scale pre-\ntrained diffusion models are primarily trained on 2D im-\nages, lacking substantial 3D-specific knowledge. Notably,\nthe recent Zero-1-to-3 model [34] stands out as it utilizes\nfine-tuning on a synthetic 3D dataset [10] to develop a\nviewpoint-conditioned diffusion model. This model takes a\nreference image along with related external camera param-\neters as inputs, enabling the generation of novel views of\nthe same subject based on the reference image. Therefore,\nit serves as our preferred strategy for establishing a subject-\nspecific 3D prior, contributing to improving 3D consistency.\n3.2. Coarse Stage: Image to 3D Reconstruction\nIn the coarse stage, we aim to obtain the approximate geo-\nmetric structure of the 3D object and the rough texture nec-\nessary for constructing a textured point cloud. In this stage,\nour scene model is a neural field based [39] on Instant-\nNGP [41]. This choice is made because it can handle com-\nplex topological changes in a smooth and continuous man-\nner while also enabling the reconstruction of 3D objects in\na relatively fast and computationally efficient manner.\nSubject-specific knowledge prior for novel views.\nWe\n4\nInput\nw/o\nshading mode\nguidance\nw\nshading-mode\nguidance\nInput\nw/o\nshading mode\nguidance\nw\nshading-mode\nguidance\nFigure 3. The proposed shading mode-aware guidance largely en-\nhances the 3D geometry in the coarse stage.\noptimize the neural radiance field such that its multi-view\nrenderings look like high-quality samples using the subject-\nspecific diffusion model. Specifically, let the rendered im-\nage I = G\u03b8(v) at the viewpoint v, where G is the differen-\ntiable rendering function for NeRF optimization parameter-\nized by \u03b8. We employ the multi-modal DreamBooth model\nas a 2D prior and follow [45] [32] to use the score distil-\nlation (SDS) loss, which assigns a \u201cscore\u201d to the rendered\nimage, guiding the optimization of the 3D model\u2019s parame-\nters \u03b8 towards the direction of higher density regions.\nDifferently, we go beyond this and propose modifying\nthe text prompt based on the specific NeRF shading mode.\nThis modification is aimed at fully harnessing the capa-\nbilities of our multi-modal diffusion model to offer pre-\ncise guidance tailored to different NeRF shading modes,\nwhich is facilitated by the inherent multi-modal awareness\nof our finetuned diffusion model. To elaborate, given the\ntext prompt y that is generated from an image captioning\nmodel [31] from the reference image, we adapt the text\nprompt to \u201csks normal map of y\u201d when the NeRF render-\ning employs \u201cnormal\u201d shading mode and \u201csks rgb photo\nof y\u201d when the NeRF rendering utilizes \u201calbedo\u201d shading\nmode. By enforcing consistency between the rendered im-\nage aligned with the modified text prompt, the multi-modal\ndiffusion model is able to provide more accurate guidance\nand thus enhance the quality of the 3D reconstruction, as\nillustrated in Figure 3. Formally, we use 2D SDS loss as:\n\u2207\u03b8LSS2D = Et,\u03f5[w(t)(\u03f5\u03d5(zt; ym, t) \u2212 \u03f5) \u2202z\n\u2202Im\n\u2202Im\n\u2202\u03b8 ], (2)\nwhere Im = G\u03b8(v, m) and m denotes the shading mode and\ncan take on values of either \u201cnormal\u201d or \u201calbedo\u201d. We use\nthe personalized text prompt ym to encode NeRF\u2019s novel\nview rendering Im to the noisy latent zt by adding a random\nGaussian noise \u03f5 of a timestep t.\nIn addition, we have the subject-specific 3D prior for\nnovel view guidance, which can be formulated as follows:\nLSS3D = Et,\u03f5\n\u0014\nw(t)(\u03f5\u03d5(zt; x, t, R, T) \u2212 \u03f5) \u2202I\n\u2202\u03b8\n\u0015\n,\n(3)\nwhere R, T represent the camera\u2019s rotation and translation,\nand x stands for the input reference image. Here, we do not\nutilize text as guidance but instead use image-based guid-\nReference\nLift From \nOur Method\nLift From \nDepth Method\nFigure 4. Illustrating different point cloud building methods from\n(1) depth images as in [60] and (2) our method.\nance, specifically the relative camera transformations be-\ntween viewpoints.\nThe overall supervision is:\n\u2207\u03b8LSS\u2212SDS = \u03bbSS2D\u2207\u03b8LSS2D + \u03bbSS3D\u2207\u03b8LSS3D,\n(4)\nwhere \u03bbSS3D and \u03bbSS3D are their weights respectively.\nGroundtruth knowledge for the reference view. Under\nthe reference view vref , the rendered image I = G\u03b8(vref)\nby NeRF should be consistent with the input image x to\nalign with our goal of customizing 3D objects. Therefore,\nwe utilize the pixel-wise difference between the NeRF ren-\ndering and the input image under the reference view as one\nof our major losses:\nLref = ||x \u2299 M \u2212 G\u03b8(vref)||1,\n(5)\nwhere \u2299 is Hadamard product. And we follow previous\nwork [70] to apply foreground mask M in order to get ex-\ntracted object which will ease the geometry reconstruction.\nUsing only the RGB per-pixel losses can lead to\npoor quality geometry issues such as sunken faces, over-\nflattening, due to the inherent shape ambiguity in 3D recon-\nstruction. To mitigate this issue, we employ MiDaS [50], a\npretrained monocular depth estimator, to estimate the depth\ndref of the reference image. However, since the estimated\npseudo depth may not be accurate and there is a scale and\nsource mismatch with the depth d from NeRF, we regularize\nthe negative Pearson correlation between the pseudo depth\nand the depth from NeRF under the reference viewpoint,\nLdepth = \u2212\nCov(dref, d)\nVar(dref)Var(d).\n(6)\nHere Cov(.) denotes covariance and Var(.) measures stan-\ndard deviation. We employ this function as our depth regu-\nlarization to encourage the NeRF output d under the refer-\nence view to be close to the depth prior.\nOverall training.\nIn summary, the loss in the coarse\nstage consists of the following parts: Lref, LSS\u2212SDS and\nLdepth. During training, we follow [60], adopting a pro-\ngressive training strategy. We start by training only in a\npartial region of the object\u2019s front view, and then gradually\nexpand the training scope.\n5\nCoarse\nrendering\nEnhanced\nrendering\nCoarse\nrendering\nEnhanced\nrendering\nFigure 5. The texture details from coarse rendering are largely\nenhanced using the multi-modal DreamBooth.\n3.3. Refine Stage: Neural Texture Enhancement\nIn the coarse stage, due to the computationally intensive na-\nture of NeRF optimization, we only perform optimization at\nlow resolutions. Moreover, due to the inherent limitations of\nNeRF, such as the tendency to produce high-frequency arti-\nfacts [45], we can only obtain a low-resolution, low-texture-\nquality 3D model. As a result, we are motivated to explore\nalternative representations for NeRF, particularly focusing\non transforming it into an explicit point cloud. Point clouds\nhave the advantage of allowing direct projection of images,\npreserving high-quality frontal texture details, and offering\nopportunities for personalization and enhancement of the\nprojected images, fulfilling the needs of high-quality cus-\ntomized 3D models. In summary, our goal in the refine\nstage is to retain the geometric structure of the coarse stage,\ngenerate a dense point cloud, and optimize the textures not\nvisible in the reference view vref, as we directly project the\nfrontal reference image x onto the point cloud. We elabo-\nrate several key designs to facilitate the transformation from\nthe coarse to the refine stage.\nPoint cloud building. Past work has focused on construct-\ning a point cloud from multi-view RGBD images under\nNeRF rendering [60]. However, due to noise in RGBD im-\nages and inaccurate depth estimation, the generated point\ncloud is very noisy and inaccurate, significantly impairing\nthe 3D geometry in refine stage. To address this, we export\nthe NeRF in coarse stage as a mesh, utilizing mesh regu-\nlarizations and perform Poisson sampling on the mesh to\nobtain a dense point cloud P m. Figure 4 shows this differ-\nence. We opt to a point cloud because it allows personalized\ndiffusion model-based texture enhancement for each view-\npoint, resulting in a higher-quality point cloud that better\naligns with our customized 3D object generation needs.\nTexture projection. However, since NeRF-rendered im-\nages from different viewpoints may have overlapping re-\ngions, a 3D point may be assigned different colors un-\nder different viewpoints [67], leading to texture conflicts.\nTherefore, we propose an iterative strategy for construct-\ning a clean texture point cloud P from mesh sampled point\ncloud P m. Initially, we trust all points P m\nref under refer-\nence view vref and use a reference mask M to perform\nNeRF\nMesh\nPost-Processing\nPseudo Masks\nPoisson \nSampling\nPoint\nClouds\nReference\nImage\nReference \nMask\nSegment\nPseudo Mask\n\u2133\ud835\udc8a (\ud835\udc97\ud835\udc8a)\nUncolored \nNovel Points\nRendering Images\nSAM\nMulti-modality\nDreamBooth\nPseudo Subject Images\nProject\nunder \ud835\udc97\ud835\udc8a\nColored Points\n\ud835\udc91\ud835\udc93\ud835\udc86\ud835\udc87\nPseudo Image\n\ud835\udc99\ud835\udc8a (\ud835\udc97\ud835\udc8a)\nReproject-Mask\n\u2133\ud835\udc8a\n\ud835\udc93\ud835\udc86 (\ud835\udc97\ud835\udc8a)\n\u2133\ud835\udc8a \u2212 \u2133\ud835\udc8a\n\ud835\udc93\ud835\udc86\nColored Points\n\ud835\udc91\ud835\udc8a (\ud835\udc97\ud835\udc8a)\nTextured Points\n\ud835\udcdf = (\ud835\udc91\ud835\udfcf, \ud835\udc91\ud835\udfd0, . . , \ud835\udc91\ud835\udc8f)\nProjection\nFigure 6. Illustrating the textured point cloud construction. We\nenhance the texture and mask of novel views and iteratively project\ntexture to the point cloud.\ncolor mapping on P m\nref, obtaining the front-facing perspec-\ntive of the new point cloud Pref. For novel view projec-\ntion, we aim to avoid introducing points with color conflicts\nthat overlap with Pref. Hence, we proceed to project the re-\ncently obtained point cloud Pref onto each novel view vi to\ngenerate a new mask denoted as M ref proj\nvi\n. We then project\ntextures to the points under this novel view only when these\npoints belong to the set difference between the novel view\nmask Mvi and the reprojected mask M ref proj\nvi\n. Finally we\nhave a clean point cloud P = {pref, p1, ..., pn} which en-\nsures color mapping is done without introducing conflicts.\nTexture and mask enhancement. The texture quality ob-\ntained in the coarse stage is suboptimal. Therefore, prior to\nprojecting the reference image x from the reference view\nand rendered RGB images X = {xv1, xv2, ..., xvn} under\nnovel views onto the point cloud geometry, we perform a\ntexture enhancement process on all rendered images within\nX. Specifically, we add noise to the rendered images and\nleverage the previously fine-tuned multi-modal personal-\nized diffusion model to denoise into a set of pseudo images\nX pseudo that possess higher texture quality as well as multi-\nview consistency. Figure 5 shows that the texture details\ncan be significantly improved by utilizing our multi-modal\nDreamBooth model.\nSimultaneously, due to the issues in the coarse stage of\nNeRF, the generated masks M = {Mv1, Mv2, ..., Mvn} by\nNeRF are not satisfactory. Fine details (for example long\nand thin parts) in the object structure may be compromised,\nleading to defects in point cloud texture and geometry ini-\ntialization due to mask inadequacies. Therefore, we adopt\nSegment Anything Model (SAM) [27] to generate more re-\nfined and accurate masks Mpseudo. However, due to the\ninsufficient resolution of RGB images and the noise intro-\nduced by SAM, we assess the quality of the generated mask\nby relying on SAM\u2019s score. Only when the quality sur-\npasses a certain threshold do we consider using this mask;\notherwise, we adhere to utilizing masks generated by NeRF.\n6\nLPIPS\u2193\nPSNR\u2191\nCLIP\u2191\nRealFusion [38]\n0.193\n16.87\n73.50%\nMake-It-3D [60]\n0.119\n20.01\n83.90%\nMagic123 [46]\n0.100\n19.50\n82.00%\nCustomize-It-3D (Ours)\n0.094\n20.50\n90.90%\nTable 1. Quantitative comparison on RealFusion15. We compute\nLPIPS and PSNR under the reference view, and CLIP under novel\nviews.\nWe illustrate our textured point cloud building in Figure 6.\nDeferred point cloud rendering and training. After ini-\ntializing the geometry and texture of the point cloud P, a\nfocus is placed on enhancing the texture of the point cloud\nin novel views with subject-specific prior. Specifically, we\nassign a 19-dimensional descriptor F to each point in the\nnovel view v, where the first three dimensions are col-\nors initialized from the above process.\nDuring the ren-\ndering of the point cloud, we employ a deferred rendering\nscheme [60], rendering the point cloud K times at different\nresolutions and concatenating the resulting K feature maps\ninto the final image using a jointly optimized U-Net network\ndenoted as R\u03b8. The refine stage\u2019s loss is similar to that in\nSec. 3.2, with the addition of a regularization term penaliz-\ning excessive differences in texture between the refined and\ninitialized states.\n4. Experiments\n4.1. Implementation Details\nOptimizing the pipeline. For the supervision of Subject-\nSpecific Knowledge Prior, we set the weights of \u03bb2D\nand\u03bb3D to 1 and 40, akin to the approach in [46]. Addi-\ntionally, the weight of \u03bb3D is adjusted based on the relative\ndistance from the frontal view. We utilized the fine-tuned\nmulti-modal DreamBooth and Zero123 as 2D and 3D pri-\nors, with guidance scale set to 10 and 5 [34], respectively.\nAdditionally, we randomly sampled t from 200 to 600 and\nemployed classifier-free guidance to compute the SDS loss.\nNeRF and point cloud rendering. We employ multi-scale\nhash encoding from Instant-NGP [41] and maintain an oc-\ncupancy grid to skip empty space for improving rendering\nefficiency. Additionally, we incorporate shading augmenta-\ntions, such as Lambertian shading similar to [45]. However,\nduring the first 1000 iterations of the second phase of pro-\ngressive training, we utilize normal shading to emphasize\nthe learning of better geometry. For the deferred rendering\nstrategy of point clouds, we utilize a 2D U-Net architecture\nwith gated convolutions [72].\n4.2. Comparisons with the State of the Arts\nBaselines. We compare our approach against recent state-\nof-the-art methods: RealFusion [38], Make-It-3D [60], and\nMagic123 [46] (using Zero123 XL checkpoint [34] as ours\nfor fair comparison). We compare these methods using the\nRealfusion dataset [38] and a customized dataset we cre-\nated. The Realfusion dataset includes many natural images,\nwhile our customized dataset comprises real images and im-\nages generated by Stable Diffusion XL [44]. We evaluate all\nthe baseline methods using their official code.\nQualitative comparison. We present a comprehensive col-\nlection of qualitative results in Figure 7. RealFusion often\ngenerates flat 3D results with colors and shapes that diverge\nsignificantly from the input image.\nMake-it-3D exhibits\ncompetitive texture quality but suffers from an issue known\nas long geometry in side views, particularly noticeable in\nthe reconstruction of objects such as chairs. Magic123 pro-\nduces visually plausible structures but grapples with a no-\ntable issue of multi-face, as it tends to replicate the refer-\nence texture in the back view.. In contrast, our approach\ncan reasonably hallucinate the texture details and geometry\nfor novel views that even deviate significantly from the ref-\nerence image, which greatly improves the fidelity and con-\nsistency in creating 3D models.\nQuantitative comparison. We quantitatively compare with\nthe baselines in Table 1. We use metrics following [46, 60]\nwith PSNR, LPIPS [75], and CLIP-similarity [47].\nAs\nshown in the table, Customize-It-3D achieves Top-1 per-\nformance across all the metrics, showing that our model is\nable to generate 3D objects with better 3D consistency.\n5. Ablations and Analysis\nWith or without multi-modal DreamBooth. We first ab-\nlate the effect of using the proposed multi-modal Dream-\nbooth in Figure 8. We observe a consistent improvement\nin terms of both texture and geometry. Employing the sug-\ngested multi-modal subject-specific diffusion model prior\nresults in higher-quality 3D content, yielding more com-\npelling visuals with enhanced 3D consistency.\nSingle RGB versus multi-modal. We experiment a base-\nline that employs conventional DreamBooth finetuned from\na single RGB reference to further investigate the impact of\nour shading mode-aware multi-modal guidance. The differ-\nence shown in Figure 9 demonstrate a clear improvement,\nwhereas the baseline suffers from multi-face problem.\n6. Applications\nHigh-quality text-to-3D generation.\nTo achieve high-\nquality text-to-3D creation, we initially utilize a T2I diffu-\nsion model to convert the text prompt into a reference image\nand then pass it into our image-based 3D creation method.\nWe show the results in Figure 10. Customize-It-3D demon-\nstrates remarkable quality in text-to-3D generation.\nReal scene modeling. We extend our model evaluation to\nreal-world scenarios by inputting arbitrary general images\ncapturing complex, authentic scenes into our framework.\n7\nInput\nMake-It-3D\nMagic123\nOurs\nRealFusion\nReference\nNovel\nReference\nNovel\nReference\nNovel\nReference\nNovel\nNovel\nNormal\nFigure 7. Qualitative comparison on image-to-3D generation. We compare Customize-It-3D to RealFusion [38], Make-it-3D [60] and\nMagic123 [46] for creating 3D objects from a single unposed image (the leftmost column).\nw/o\n Multi-Modal\nDreamBooth\nInput\nw\nMulti-Modal\nDreamBooth\nInput\nReference View from coarse stage\nNovel View from coarse stage\nw/o\n Multi-Modal\nDreamBooth\nw\nMulti-Modal\nDreamBooth\nFigure 8. The effect of Multi-modal DreamBooth, showing a con-\nsistent improvement in terms of both geometry and texture.\nFigure 11 presents several visual examples, verifying the\nefficacy of our method in modeling real scenes with a high\ndegree of capability and fidelity.\n7. Conclusion\nWe have presented Customize-It-3D, a novel two-stage ap-\nproach for image-to-3D generation. Leveraging the estab-\nSingle RGB\nDreamBooth\nMulti-Modal\nDreamBooth\nInput\nSingle RGB\nDreamBooth\nMulti-Modal\nDreamBooth\nInput\nNovel View from coarse stage\nNovel View from coarse stage\nFigure 9. Single RGB versus Multi-modal DreamBooth, showing\na clear improvement particularly in geometry.\nlished customized knowledge prior, we enhances NeRF op-\ntimization for improved geometry and refines texture, re-\nsulting in superior alignment of 3D content with the sub-\nject. Customize-It-3D demonstrates versatility in handling\ngeneral objects and empowers captivating applications.\n8\nA penguin wearing a bow \ntie and playing guitar.\nText-to-image\nModel\nA lolita style doll with a \npink hat.\nText\nReference\nNovel view\nNormal\nFigure 10. Illustrating high-fidelity text-to-3D generation.\nReference\nReference View\nNovel View\nNormal\nFigure 11. Illustrating high-fidelity 3D creation for real scenes.\nReferences\n[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Si-\nmon, Brian Curless, Steven M Seitz, and Richard Szeliski.\nBuilding rome in a day. Communications of the ACM, 54\n(10):105\u2013112, 2011. 2\n[2] Titas Anciukevi\u02c7cius, Zexiang Xu, Matthew Fisher, Paul Hen-\nderson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Ren-\nderdiffusion: Image diffusion for 3d reconstruction, inpaint-\ning and generation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n12608\u201312618, 2023. 3\n[3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 2\n[4] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W\nBergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini\nDe Mello, Tero Karras, and Gordon Wetzstein. Genvs: Gen-\nerative novel view synthesis with 3d-aware diffusion models,\n2023. 3\n[5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision, pages 333\u2013350. Springer,\n2022. 2\n[6] Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su,\nand Andreas Geiger. Dictionary fields: Learning a neural ba-\nsis decomposition. ACM Transactions on Graphics (TOG),\n42(4):1\u201312, 2023. 2\n[7] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen\nTu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf: A\nunified approach to 3d generation and reconstruction. arXiv\npreprint arXiv:2304.06714, 2023. 3\n[8] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 3\n[9] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-\nder G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal\n3d shape completion, reconstruction, and generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4456\u20134465, 2023. 3\n[10] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli\nVanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia\nGkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 4\n[11] Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitz-\nmann. Learning to render novel views from wide-baseline\nstereo pairs. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4970\u2013\n4980, 2023. 2\n[12] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir\nZamir.\nOmnidata: A scalable pipeline for making multi-\ntask mid-level vision datasets from 3d scans. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 10786\u201310796, 2021. 3\n[13] Ziya Erkoc\u00b8, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner,\nand Angela Dai.\nHyperdiffusion:\nGenerating implicit\nneural fields with weight-space diffusion.\narXiv preprint\narXiv:2303.17015, 2023. 3\n[14] Yasutaka Furukawa, Carlos Hern\u00b4andez, et al.\nMulti-view\nstereo: A tutorial. Foundations and Trends\u00ae in Computer\nGraphics and Vision, 9(1-2):1\u2013148, 2015. 2\n[15] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. Advances In Neural In-\nformation Processing Systems, 35:31841\u201331854, 2022. 3\n[16] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,\nChristian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.\nNerfdiff: Single-image view synthesis with nerf-guided dis-\ntillation from 3d-aware diffusion. In International Confer-\nence on Machine Learning, pages 11808\u201311826. PMLR,\n2023. 3\n[17] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O\u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration. arXiv preprint arXiv:2303.05371, 2023. 3\n[18] Thomas A Halgren, Robert B Murphy, Richard A Friesner,\nHege S Beard, Leah L Frye, W Thomas Pollard, and Jay L\nBanks.\nGlide: a new approach for rapid, accurate dock-\ning and scoring. 2. enrichment factors in database screening.\nJournal of medicinal chemistry, 47(7):1750\u20131759, 2004. 2\n[19] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. Dreamtime: An improved optimiza-\ntion strategy for text-to-3d content creation. arXiv preprint\narXiv:2306.12422, 2023. 3\n[20] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf\non a diet: Semantically consistent few-shot view synthesis.\n9\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5885\u20135894, 2021. 2\n[21] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 3\n[22] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 3\n[23] O\u02d8guzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir\nZamir. 3d common corruptions and data augmentation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 18963\u201318974, 2022. 3\n[24] Animesh Karnewar, Niloy J Mitra, Andrea Vedaldi, and\nDavid Novotny. Holofusion: Towards photo-realistic 3d gen-\nerative modeling. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 22976\u201322985,\n2023. 3\n[25] Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf:\nRay entropy minimization for few-shot neural volume ren-\ndering.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 12912\u2013\n12921, 2022. 2\n[26] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten\nKreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio\nTorralba, and Sanja Fidler. Neuralfield-ldm: Scene genera-\ntion with hierarchical latent diffusion models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 8496\u20138506, 2023. 3\n[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar, and\nRoss Girshick. Segment anything. arXiv:2304.02643, 2023.\n6\n[28] Peter Kontschieder and Matthias Nie\u00dfner. Diffrf: Rendering-\nguided 3d radiance field diffusion-supplementary document.\n3\n[29] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 2\n[30] Han-Hung Lee and Angel X Chang.\nUnderstanding pure\nclip guidance for voxel grid nerf models.\narXiv preprint\narXiv:2209.15172, 2022. 3\n[31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi.\nBlip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models.\nArXiv, abs/2301.12597, 2023. 5\n[32] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 3, 5\n[33] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang\nXu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh\nin 45 seconds without per-shape optimization. arXiv preprint\narXiv:2306.16928, 2023. 3\n[34] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023. 3, 4, 7, 2\n[35] Zhen\nLiu,\nYao\nFeng,\nMichael\nJ\nBlack,\nDerek\nNowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdif-\nfusion: Score-based generative 3d mesh modeling.\narXiv\npreprint arXiv:2303.08133, 2023. 3\n[36] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: Learning dynamic renderable volumes from images.\narXiv preprint arXiv:1906.07751, 2019. 2\n[37] Shitong Luo and Wei Hu. Diffusion probabilistic models for\n3d point cloud generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2837\u20132845, 2021. 3\n[38] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8446\u20138455, 2023. 1, 2, 3, 7, 8\n[39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2, 4\n[40] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 conference papers, pages 1\u20138, 2022. 3\n[41] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022. 2, 4, 7\n[42] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 3\n[43] Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski,\nChaoyang Wang, Luc Van Gool, and Sergey Tulyakov.\nAutodecoding latent 3d diffusion models.\narXiv preprint\narXiv:2307.05445, 2023. 3\n[44] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach. Sdxl: Improving latent diffusion models\nfor high-resolution image synthesis, 2023. 7\n[45] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv,\n2022. 1, 3, 5, 6, 7\n[46] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. Magic123: One image to high-quality 3d object\ngeneration using both 2d and 3d diffusion priors.\narXiv\npreprint arXiv:2306.17843, 2023. 1, 2, 3, 7, 8\n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n10\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 7\n[48] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nBen Mildenhall, Nataniel Ruiz, Shiran Zada, Kfir Aberman,\nMichael Rubenstein, Jonathan Barron, Yuanzhen Li, and\nVarun Jampani. Dreambooth3d: Subject-driven text-to-3d\ngeneration. ICCV, 2023. 3\n[49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[50] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 44(3), 2022. 3, 5\n[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 2, 3, 4, 1\n[52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023. 2, 3\n[53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[54] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-motion revisited.\nIn Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n4104\u20134113, 2016. 2\n[55] Johannes L Sch\u00a8onberger,\nEnliang Zheng,\nJan-Michael\nFrahm, and Marc Pollefeys.\nPixelwise view selection for\nunstructured multi-view stereo. In Computer Vision\u2013ECCV\n2016: 14th European Conference, Amsterdam, The Nether-\nlands, October 11-14, 2016, Proceedings, Part III 14, pages\n501\u2013518. Springer, 2016. 2\n[56] Hoigi Seo, Hayeon Kim, Gwanghyun Kim, and Se Young\nChun.\nDitto-nerf: Diffusion-based iterative text to omni-\ndirectional 3d model.\narXiv preprint arXiv:2304.02827,\n2023. 3\n[57] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 3\n[58] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen\nLiu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchi-\ncal 3d generation with bootstrapped diffusion prior. arXiv\npreprint arXiv:2310.16818, 2023. 1\n[59] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea\nVedaldi. Viewset diffusion:(0-) image-conditioned 3d gener-\native models from 2d data. arXiv preprint arXiv:2306.07881,\n2023. 3\n[60] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 22819\u201322829, 2023. 1, 2, 3,\n5, 6, 7, 8\n[61] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts.\narXiv\npreprint arXiv:2304.12439, 2023. 3\n[62] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\nand Thomas Wolf.\nDiffusers:\nState-of-the-art diffusion\nmodels.\nhttps://github.com/huggingface/\ndiffusers, 2022. 4\n[63] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619\u201312629, 2023. 3\n[64] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, et al. Rodin: A generative model for\nsculpting 3d digital avatars using diffusion. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 4563\u20134573, 2023. 3\n[65] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 1, 3\n[66] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan\nHo,\nAndrea\nTagliasacchi,\nand\nMohammad\nNorouzi. Novel view synthesis with diffusion models. arXiv\npreprint arXiv:2210.04628, 2022. 3\n[67] Jiaxin Xie, Hao Ouyang, Jingtan Piao, Chenyang Lei, and\nQifeng Chen.\nHigh-fidelity 3d gan inversion by pseudo-\nmulti-view optimization. arXiv preprint arXiv:2211.15662,\n2022. 6\n[68] Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and\nShuicheng Yan.\nAdan: Adaptive nesterov momentum al-\ngorithm for faster optimizing deep models. arXiv preprint\narXiv:2208.06677, 2022. 1\n[69] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360deg views. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4479\u20134489, 2023. 1\n[70] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan\nAtzmon, Basri Ronen, and Yaron Lipman. Multiview neu-\nral surface reconstruction by disentangling geometry and ap-\npearance. Advances in Neural Information Processing Sys-\ntems, 33, 2020. 5\n[71] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4578\u20134587, 2021. 2\n[72] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\n11\nThomas S Huang. Free-form image inpainting with gated\nconvolution. arXiv preprint arXiv:1806.03589, 2018. 7, 1\n[73] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Goj-\ncic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: La-\ntent point diffusion models for 3d shape generation. arXiv\npreprint arXiv:2210.06978, 2022. 3\n[74] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter\nWonka. 3dshape2vecset: A 3d shape representation for neu-\nral fields and generative diffusion models.\narXiv preprint\narXiv:2301.11445, 2023. 3\n[75] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 7\n[76] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation\nand completion through point-voxel diffusion. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 5826\u20135835, 2021. 3\n12\nCustomize-It-3D: High-Quality 3D Creation from A Single Image Using\nSubject-Specific Knowledge Prior\nSupplementary Material\nA. Additional Implementation Details\nA.1. Coarse stage\nMulti-modal Dreambooth finetuning and usage. We uti-\nlized the stable diffusion V2.0 model [51] in our experi-\nments. We employed multi-modal images related to the ref-\nerence image and conducted a four-step fine-tuning process\non the diffusion model. The sequence and number of itera-\ntions for each training step are as follows: 25 iterations for\nmask image training, 50 iterations for depth image train-\ning, 50 iterations for normal image training, and 75 itera-\ntions for RGB image training, totaling 200 iterations. Dur-\ning fine-tuning, prompts were specified as follows: \u201ca fore-\nground mask / depth map / normal map / RGB photo of sks\n[class name].\u201d When using the finetuned model for train-\ning in the coarse stage, we modified the prompts based on\nshading mode. For normal shading, we append the modifier\n\u201cnormal map\u201d to the text prompt, while for other shading\nstrategies, we add the modifier \u201cRGB photo\u201d. We observed\nthat this approach facilitated the better utilization of subject-\nspecific knowledge, leading to improved results.\nOptimizing the pipeline.\nWe employ the Adam opti-\nmizer [68] with a learning rate of 0.005 and no weight decay\nfor 7000 iterations. Simultaneously, during the coarse stage,\nwe implement progressive training, where the first 2000 it-\nerations exclusively trained within the range of \u00b145\u25e6 from\nthe reference view, and the subsequent 5000 iterations cov-\nered the entire 360\u25e6 around the object. In the refine stage,\nwe conducted 3000 iterations to optimize the texture.\nCamera setting. For the camera sampling method, we ran-\ndomly sample novel views with a probability of 75%, while\nwith a 25% probability, we sample a reference view follow-\ning the approach in [45]. The reference view is assumed\nto be a frontal view, i.e., a polar angle of 90\u25e6 and an az-\nimuth angle of 0\u25e6. It is further assumed that the camera\nis positioned at a distance of 1 meter from the origin of the\ncoordinate system, and the field of view (FOV) is set to 60\u25e6.\nA.2. Refine stage\nPost-mask processing.\nWe also perform post-mask-\nprocessing, wherein we apply certain morphological oper-\nations to both SAM-generated and NeRF-generated masks,\ni.e., taking the union of the two masks to complement fine\nstructures lacking in the initial mask.\nFinally, we have\nMpseudo = \u02c6\nM \u222a \u02c6\nMSAM, where \u02c6\nM and \u02c6\nMsam are the ren-\ndering and SAM-generated mask after post-processing un-\nder novel view.\nPoint cloud building. The goal of our point cloud building\nstrategy is to avoid introducing points with color conflicts\nthat overlap with existing point cloud pm\nref, which is build\nfrom reference view. We use a reference mask M ref\npseudo to\nperform color mapping on pm\nref, obtaining the front-facing\nperspective of the new point cloud pref. For all remaining\nregions under novel view, we project the just obtained point\ncloud pref onto each novel view vi to create a new mask\nmre\ni . We then take the set difference between the novel\nview mask mi\npseudo and the re-projected mask mre\ni\nas the\nmask for the remaining points Pleft = P m \u2212 pm\nref. Finally\nwe will have a clean point cloud P = {pref, p1, ..., pn}\nwhich ensures color mapping is done without introducing\nconflicts.\nPoint cloud rendering. In order to perform better point\ncloud rendering, each point\u2019s has a 19-dimensional descrip-\ntor, with the initial three dimensions initialized with RGB\ncolors and the rest initialized randomly [60].\nDeferred\nrendering is conducted for a total of K = 3 times, with\nresolutions denoted as [H/2i, W/2i], where i \u2208 [0, K)\nand ultimately, the learned U-Net network concatenates\nthese three feature maps. And the U-Net architecture con-\ntains 3 down-sampling and up-sampling layers with gated\nconvolutions[72]. The rendering resolution is set to 128 \u00d7\n128 in the coarse stage for NeRF and 800\u00d7800 in the refine\nstage for the point cloud, respectively.\nB. Additional Ablation Study Results\nAs mentioned in the paper, current methods for text/image-\nto-3D rely on a generic diffusion model with strong gen-\neralization capabilities, leveraging implicit 2D knowledge\nto guide the generation of novel views. However, due to\nits overly rich imagination, this model may generate 3D\nobjects that do not align with the subject identity. To ad-\ndress this limitation, we fine-tune the diffusion model to\nlearn subject-oriented knowledge. We conduct a series of\nexperiments to demonstrate the effectiveness of our subject-\nspecific and multi-modal diffusion model. We first ablate\nthe effect of using the proposed multi-modal Dreambooth,\nas illustrated in Figure 13. The results indicate that without\nthe guidance of our multi-modal Dreambooth, the coarse\nstage produces textures and geometry that deviate from the\nsubject identity, thereby impacting the initialization of point\ncloud geometry and textures in the refine stage, as exempli-\nfied by the patterns on the fox\u2019s neck and the rabbit\u2019s ears.\nFurthermore, we compare the results obtained using the\ntraditional dreambooth (trained with a single RGB image)\n1\nInput\nMake-It-3D\nMagic123\nOurs\nRealFusion\nFigure 12. Additional comparison with baseline: RealFusion [38], Make-It-3D [60], and Magic123 [46]. The first column is the reference\nimage.\nLPIPS\u2193\nPSNR\u2191\nCLIP\u2191\nRealFusion [38]\n0.197\n16.41\n70.08%\nMake-It-3D [60]\n0.121\n19.31\n78.93%\nMagic123 [46]\n0.098\n20.10\n85.70%\nCustomize-It-3D (Ours)\n0.096\n20.30\n91.08%\nTable 2. Quantitative comparison on our dataset. We compute\nLPIPS, PSNR and CLIP-similarity.\nwith our multi-modal dreambooth (trained with multiple\nmulti-modal images). We also compare the results of the\nNeRF training process with and without shading mode\nguidance. Both sets of experiments are depicted in Fig-\nure 14. The results show that a simple attempt using only\ndreambooth leads to overfitting and mode collapse. Ad-\nditionally, we observed that using shading mode guidance\nbetter leverages our multi-modal dreambooth, resulting in\nmore convergent and 3D-consistent outcomes.\nC. Additional Comparisons with Baseline\nIn this section, we show more results comparing our method\nwith the baseline: RealFusion [38], Make-It-3D [60], and\nMagic123\n[46] (using Zero123 XL checkpoint [34]), as\nshown in the Figure 12. Also we do quantitative comparison\non our own dataset as shown in the Table 2, and our method\nhas the best results. Results show that our method exhibits\nincreased fidelity to the input reference image in both geom-\netry and texture, thanks to the utilization of subject-specific\nknowledge priors. This incorporation makes the training\nprocess more aligned with the subject\u2019s identity. Particu-\nlarly in terms of texture, as exemplified in the second row\nwith the silver cat in the Figure 12, our method generates\nback textures with a more pronounced metallic sheen, con-\nsistent with the characteristics of the front texture. Simi-\nlarly, as seen in the fifth row with the doll in the Figure 12,\nour method generates doll textures on the back and sides, in-\ncluding patterns on the skirt and hair color, that better align\n2\nw/o\nMulti-Modal\nDreamBooth\nInput\nw\nMulti-Modal\nDreamBooth\nInput\nReference view from coarse stage\nNovel view from coarse stage\nw/o\nMulti-Modal\nDreamBooth\nw\nMulti-Modal\nDreamBooth\nw/o\nMulti-Modal\nDreamBooth\nw\nMulti-Modal\nDreamBooth\nInput\nNovel view from coarse stage\nFigure 13. Additional results of an ablation study on whether to use the multi-modal dreambooth. The first column is the reference image.\nInput\nMulti-Modal DB /\n w shading mode\nguidance\nInput\nSingle RGB DB /\nw shading mode \nguidance\nMulti-Modal DB /\nw/o shading mode \nguidance\nMulti-Modal DB /\n w shading mode\nguidance\nSingle RGB DB /\nw shading mode \nguidance\nMulti-Modal DB /\nw/o shading mode \nguidance\nFigure 14. Additional ablation results. The first column is the reference image. The second column displays the results obtained using our\nmethod. The third column showcases the outcomes when substituting the multi-modal dreambooth with the traditional dreambooth. The\nfourth column exhibits the results when shading mode guidance is omitted during the NeRF training process.\nwith the subject\u2019s identity. Therefore, our method is capa-\nble of generating textures that are more consistent with the\nreference view.\nD. Additional Results\nIn this section, we show more results using our method. Fig-\nure 15 and Figure 16 demonstrates the ability of our method\nto generate high-quality 3D objects from a single image.\nE. Limitation\nOur method has several limitations. Firstly, because our\napproach relies on using pretrained models as depth esti-\nmator [50] and normal estimator [12] [23] to estimate the\ndepth map and normal map of the reference image. So any\nerrors in these modules will affect our method and impact\nthe overall generation quality. Also, our method, relying on\ngenerative prior to imagine the 3D content from the single\nreference viewpoint, inevitably introduces geometry ambi-\nguity [45], leading to issues such as the Janus problem or\nover-flat geometry.\n3\nFigure 15. Additional results by Customize-It-3D. The first column is the reference image.\n4\nFigure 16. Additional results by Customize-It-3D. The first column is the reference image.\n5\n"
  },
  {
    "title": "Text-Conditioned Resampler For Long Form Video Understanding",
    "link": "https://arxiv.org/pdf/2312.11897.pdf",
    "upvote": "5",
    "text": "Text-Conditioned Resampler For Long Form Video Understanding\nBruno Korbar1,2\nYongqin Xian2\nAlessio Tonioni2\nAndrew Zisserman1,3\nFederico Tombari2,4\n1University of Oxford\n2Google\n3Google DeepMind\n4TU Munich\nkorbar@robots.ox.ac.uk\nAbstract\nVideos are highly redundant data source and it is often\nenough to identify a few key moments to solve any given\ntask. In this paper, we present a text-conditioned video re-\nsampler (TCR) module that uses a pre-trained and frozen\nvisual encoder and large language model (LLM) to pro-\ncess long video sequences for a task. TCR localises rele-\nvant visual features from the video given a text condition\nand provides them to a LLM to generate a text response.\nDue to its lightweight design and use of cross-attention,\nTCR can process more than 100 frames at a time allow-\ning the model to use much longer chunks of video than ear-\nlier works. We make the following contributions: (i) we\ndesign a transformer-based sampling architecture that can\nprocess long videos conditioned on a task, together with a\ntraining method that enables it to bridge pre-trained visual\nand language models; (ii) we empirically validate its effi-\ncacy on a wide variety of evaluation tasks, and set a new\nstate-of-the-art on NextQA, EgoSchema, and the EGO4D-\nLTA challenge; and (iii) we determine tasks which require\nlonger video contexts and that can thus be used effectively\nfor further evaluation of long-range video models.\n1. Introduction\nThe more I see, the less I know, the more I like to let it go.\nRed Hot Chili Peppers\nThe development of visual-language models (VLMs) ad-\nvanced exponentially in the past few years: new models pre-\ntrained with increasingly larger scale, in terms of the num-\nber of parameters and size of the training set, continue push-\ning forward the state of the art on multiple tasks every cou-\nple of months. These models often have the ability to rea-\nson about the relationships of objects in their environment\nthrough natural language, often in an interactive fashion.\nThis capability is appealing for multiple video applications.\nFor example, it would be helpful for a model to be able\nto answer questions about a video: \u201cDoes this recipe use\nL\nM\nText-Conditioned Resampler (TCR)\nAnswe\n\u201cThe patte\nalready st\nto form r\nrows one a\nuntil you \nyour des\nlength\nPrompt\n\u201cWhat happens \nbetween 7th and \n9th second?\u201d\n\u2026\nLong-term action anticipation\nVideo Question Answering\nMoment Query\nVQA\nWhat happens between \n7th and 9th second?\nMoment Query\nWhen did I start \nto knit? \nAction anticipation\nWhat happens after?\nUp to 100 input video frames\nFigure 1. TCR resamples visual features that are relevant for the\ndownstream tasks before passing them to the LLM.\neggs?\u201d, \u201cwhat does he do after he removes the tire?\u201d, etc.\nIt is also appealing for users of augmented-reality devices:\nfor example to be able to answer \u201cwhere did I leave my\nphone?\u201d. Unfortunately, the computational requirements of\nsuch models made them impractical for use in video ap-\nplications as the memory requirement rises quadratically\nwith the input size. Furthermore, to our knowledge, a large-\nenough source of even loosely labelled video data for train-\ning such a model from scratch does not readily exist.\nThat is why we are specifically interested in a subset of\nthese models that are not trained from scratch, but rather\n\u2018bridge\u2019 pre-trained models via different types of \u2018visual-\nto-language adapter modules\u2019 [1, 30, 37]. The advantages\nof this approach, as opposed to training the model from\nscratch, are numerous: Only a small number of parameters\nare trained, which makes the memory footprint smaller; it\nallows us to utilise the capabilities of large visual backbones\nwithout overfitting to the downstream task; as well as to\nleverage the vast amount of knowledge stored in the LLM\nwithout suffering common limitations of smaller-scale fine-\ntuning such as catastrophic forgetting.\nMost of these models are focused on images, and even\nthose that can process (or are trained on) videos, can ingest\nonly a small number of frames \u2013 typically anywhere be-\ntween 4 to 32. Allowing a large number of video frames to\n1\narXiv:2312.11897v1  [cs.CV]  19 Dec 2023\ninteract with text is demonstrably beneficial [32, 37]. Mod-\nels learn to, implicitly or through direct supervision, sample\nfeatures most relevant to their task. Thus, a relatively sim-\nple way of increasing the model performance is to increase\nthe number of frames the model sees.\nIn this work, we present a Text-Conditioned Resampler\n(TCR), an architecture and pre-training method that tack-\nles all of the mentioned challenges mentioned above: it is\na reasonably lightweight, low-dimensional adapter which\nacts as an information bottleneck between visual and lan-\nguage models.\nAs shown in Figure 1, it is able to pro-\ncess up to 100 frames at a time, selecting the most rele-\nvant frame features to pass to the LLM based on the \u201ccon-\nditioning\u201d text. We show that this model is effective across\nmany tasks and evaluation regimes. We use the fact that the\nmodel is trained on such a large number of frames to high-\nlight the tasks where performance scales proportionally to\nthe number of input frames. Our model achieves excellent\nresults on the moment-queries EGO4D challenge, and sets\nthe state-of-the-art (SOTA) on long-video question answer-\ning on the validation sets of EgoSchema dataset [32] and\nEGO4D long-term forecasting challenges, as well as on the\ntemporally-sensitive NextQA dataset [49].\n2. Related work\nVideo-sampling techniques:\nSampling relevant frames\nfrom videos has long been a challenge in video understand-\ning due to the highly redundant nature of videos as a data\nsource. These methods either use a pre-processing mod-\nule [3, 6, 13, 28, 46, 53, 60] to guide their model through\nmulti-modal attention-like mechanisms [12, 34], or employ\nrecursive reinforcement learning techniques [48] to select\nrelevant parts of the videos. In temporal action detection,\nmodels are tasked with precisely locating action boundaries.\nMost commonly used datasets [4, 44, 59] are dominated by\ncustom solutions or transformer architectures [38, 57] built\nupon strong features [5, 41, 42, 45, 47].\nVideo-language models and feature resampling: Video-\nlanguage models have revolutionised the field of com-\nputer vision \u2013 the scale of the models and data they were\ntrained on increased exponentially in a short period of\ntime [1, 21, 30, 35, 56], some even being jointly optimised\nfor images and video [1, 29, 30]. The length of the videos\nthese models can process often varies \u2013 while [1] can pro-\ncess up to 8 frames, [29] can process longer tubelets (at\nmuch reduced receptive fields). None of these models have\nthe capability of processing videos over 16 frames at full\nresolution outright.\nTemporally sensitive video datasets:\nA large effort has\nbeen put into developing temporally sensitive and long-\nform benchmarkes. For example, Diving48 [31] and Coun-\ntix [11] datasets do not contain long sequences, but it\nhas been shown that frame subsampling yields sub-optimal\nresults [25].\nVideo question-answering (VQA) datasets\nsuch as NextQA [49] are developed in such a way that\nmodels need to have a notion of \u201cbefore\u201d or \u201cafter\u201d in a\nroughly minute-long video, and models that can process\nmore frames tend to do better on it [52].\nEgocentric videos understanding: The recent release of\nlarge-scale long-form egocentric video dataset (EGO4D) al-\nlowed for the development of episodic memory (spatiotem-\nporal localisation given a query), short- or long-term action\nanticipation or spatio-temporal grounding tasks that require\nresearchers to think about the problem of video-length on a\ndifferent scale [14, 33]. This has already yielded a plethora\nof creative approaches to solve various challenges in the\negocentric space [7, 22, 40]. Not only have researchers de-\nveloped new models, but also new and exciting benchmarks.\nThe recent EgoSchema dataset [32], a manually annotated\nsubset of EGO4D, is a first foray in truly long-form video\nquestion answering, since answering each question requires\nwatching a 3 minutes video.\nA work particularly relevant to ours is SpotEM [36], a\nlightweight sampling mechanism that makes use of low di-\nmensional features in order to select video segments impor-\ntant for solving natural-language query challenge (NLQ).\nThough impressive in performance, their method is limited\nto the type of embeddings used for sampling, and is thus\nless general than our approach.\n3. Text-Conditioned Resampler (TCR)\nIn the following section we describe our model and the\ntraining procedures.\n3.1. Model\nAt a high level, the TCR inputs video features processed by\na visual encoder and embedded text-tokens, and outputs a\nfixed-length sequence of embeddings that is consumed by\na language model. The text specifies (conditions) the task,\nand the TCR selects different visual features according to\nthe task, and transforms them to be suitable for input to the\nlanguage model. The language model generates the text re-\nsponse to the specified task. An overview of the architecture\nis given in Figure 2 on the left.\nOverview:\nThe visual inputs consist of RGB frames of\nthe video that are ingested by a pre-trained frozen ViT-\ng [39] model to obtain visual embeddings. Temporal en-\ncodings are added to them. The conditioning text tokens\nare prefixed with a learnable special token specifying the\ntask the model is trying to solve and concatenated with a\nset of learnable query vectors. The queries and text interact\nwith each other through self-attention layers, and interact\nwith the frozen visual features through cross-attention lay-\ners (inserted every other transformer block). Output query\nvectors are then concatenated with an optional text prompt,\n2\nCondition\n\u201c[CPN] [7][9]\u201d\nAnswer\n\u201cThe pattern is \nalready starting \nto form repeat \nrows one and \ntwo.\u201d\nPrompt\n\u201cWhat happens \nbetween 7th and 9th \nsecond?\u201d\nlayer n\nlayer 0\nlayer 1\n1\n2 \u2026\n[CPN][7][9]\nLearnable query\nVisual\ntokens\nK, V\nQ\nFigure 2. Left: overview of how TCR integrates in a VLM in order to process long videos. A long (30-120 frames) sequence from a visual\nencoder (V) is resampled to a fixed-length sequence fed to a language model. [CPN] indicates special token for captioning; [7][9] is a\nrepresentation of tokenised time steps. Right: details of the TCR module. Elements in blue are kept frozen. Best viewed in colour.\nand passed through a frozen Flan-T5 language model [9].\nThe TCR module is illustrated in Figure 2 on the right.\nThe key design choices are: (i) the interaction of query\nsequence with the visual features is only through cross-\nattention.\nThis enables the TCR to ingest very long se-\nquences (as it is not limited by the quadratic complexity of\nself-attention); and (ii) the output is a fixed length set (the\ntransformed query vectors), so that the input to the language\nmodel is only a small number of tokens, irrespective of the\nlength of the video sequence.\nHow does the TCR differ from the Flamingo Resam-\npler and Q-former?\nThese design decisions build on\nthe architectural innovations of the Perceiver resampler in\nFlamingo [1] and the Q-former in BLIP-2 [30]. However,\nthere are a number of differences: (i) While Q-former is\ntrained on images, TCR is optimised for video from the\nground up \u2013 all training stages are done on videos. This\nis imporant as the TCR must learn to sample visual features\nfrom frames conditioned on the task. (ii) TCR uses lower\ndimensional features than either Q-former or Perceiver Re-\nsampler (512 vs 768 vs 1536) and an overall smaller num-\nber of parameters (69M vs 188M). This is important as it\nallows us to process far longer video sequences. (iii) While\nTCR applies cross-attention visual features to text embed-\ndings and learnable queries, Perceiver Resampler concate-\nnates visual-embeddings and queries in a key-value pair,\nwhich makes the computation more expensive as it com-\nputes cross-attention and self-attention in a single pass. We\nkeep the operations separate (i.e. first cross-attending text-\nquery sequence with the video, and then self-attending the\ntext-query sequence). This reduces per-layer computational\nrequirements allowing us to increase video sequence length.\n3.2. Training\nRecent works have shown that contrastive learning yields\nvisual representations for video frames that perform bet-\nter in discriminative tasks than training in a purely gener-\native fashion [29, 54]. Training models with a generative\nloss, however, seems to be crucial for developing reasoning\nregarding temporal grounding of unconstrained videos as\nwell as the semantic relationship between text structure and\nvideo [1, 51]. Hence, we separate our training in three dis-\ntinct stages: (i) initialisation, where we train TCR without\nthe LLM; (ii) pre-training, where we train TCR in conjunc-\ntion with the LLM; and later, (iii) a task-specific fine-tuning.\nNote that the only thing we\u2019re training is the TCR module\n\u2013 the visual encoder and LLM remain frozen throughout.\nInitialisation and pre-training stages are done on the YTT-\n1B dataset [51]. Videos in this dataset are annotated by the\ntranscribed speech sentences and their corresponding times-\ntamps that are either user-generated or automatically gen-\nerated via automatic-speech recognition. Speech in such\nvideos is rarely visually grounded [15, 26], however, be-\ncause our model can see the video sequence surrounding\nthe annotated segment, it is well suited to implicitly learn\nthe temporal grounding. We describe training stages below.\nInitialisation (without LLM): To initialise our model, we\nfollow BLIP2 [30]\u2019s image-text contrastive and image-text\nmatching objectives. Contrastive objective maximises mu-\ntual information between TCR text output, and learnable\nqueries which are cross-attended with a video. Text and\nlearnable queries are passed together to TCR. Their mu-\ntual attentions masked in such a way that text only attends\nto itself, while the learnable queries are cross-attended to\nthe video frames and then self-attended to themselves. We\ncompute the average of text queries to get a text represen-\ntation t, and compare it pairwise with all learnable queries.\nQuery with maximum similarity to t is denoted as q. We\nthen align the representations t and q by contrasting each\npositive pair with in-batch negative pairs.\nAt this stage\nTCR is not text conditioned. Image-text matching objec-\ntive (video-text matching in our case) primes the model\n3\nInit\nPre-training\nNextQA\nAcc\n\u2191\nNLQ\nMR@1 \u2191\n(i)\n(ii)\n(iii)\n\u2713\n\u2713\n\u2713\n\u2713\n66.1\n11.42\n\u2713\n\u2717\n\u2717\n\u2717\n52.1\n7.88\n\u2717\n\u2713\n\u2713\n\u2713\n63.3\n9.41\n\u2713\n\u2713\n\u2717\n\u2717\n64.1\n8.94\n\u2713\n\u2713\n\u2713\n\u2717\n65.6\n9.37\n\u2713\n\u2717\n\u2713\n\u2717\n63.4\n8.91\n\u2713\n\u2713\n\u2717\n\u2713\n64.2\n8.13\nTable 1. Effect on various initialisation and pre-training stages\non NextQA question answering and NLQ task. For NextQA, we\nuse shortened fine-tuning procedure (see Section 4.2) and vary the\ncheckpoints used. For NLQ, we evaluate on TCR w/LLM.\nfor text-conditioning. Both learnable queries and text are\npassed through TCR together, without attention masking.\nA binary classifier predicting whether the video and text are\nmatching or not is applied to each of the learnable queries\nand their prediction is averaged to obtain a final matching\nscore. The negatives are sampled in-batch following proce-\ndure from [30].\nWe skip the generative training step of [30], as our\nmodel is neither designed nor initialised from a language\nmodel, and we found no measurable benefit from this train-\ning stage. The reader is referred to the original paper [30]\nfor in-depth description of attention-masks and losses used\nduring each of the objectives.\nPre-training (with LLM):\nThe goal of pre-training is\ntwofold: first, to semantically and temporally align TCR\u2019s\noutput with the expected input of the LLM, and second to\ntrain TCR\u2019s self-attention layer to attend to specific task-\nspecifying special tokens and text conditioning tokens. We\ndo this by training it on three tasks. (i) given an untrimmed\nvideo and annotated sentence, we ask it to retrieve when\nthe sentence occurred; (ii) given the untrimmed video and\na timestep, we ask the model to fully caption that particu-\nlar segment; (iii) given the untrimmed video and a text se-\nquence corrupted in multiple ways, we ask it to correct the\nsequence. All tasks are supervised by applying the genera-\ntive loss on the outputs of the LLM. The examples of these\ntasks on an example from YTT dataest can be seen in Ta-\nble 7 in the supplementary material. The effects of these\ntraining stages can be seen in Table 1.\nFine-tuning:\nThe goal of fine-tuning is to align the TCR\nwith the domain of the downstream task in question. Only\nthe TCR module and its vocabulary are fine-tuned, while the\nvisual encoder and the LLM are kept frozen. Fine-tuning is\nperformed on each of the downstream datasets, and the fine-\ntuning is described in the results section for each dataset,\nwhile hyperparameters are given in the supplementary.\n3.3. Model details\nTime tokenization: In order to make our model time aware,\nwe add time tokens to text and video-frames. We tokenise\nthe time in half-second increments following an analogue\nprocedure from [8] with nbins = 2048. This means we can\ntokenise up to 17 minutes of video with a precision of half\na second. We use this time format to express the temporal\nlocalisation of a segment in a longer video if that informa-\ntion is required (for example in moment retrieval or when\ndescribing a segment of the video). For each frame, we\npass its timestep, tokenised as above, through a single-layer\nMLP in order to obtain learnable temporal embeddings we\nadd to every frame.\nVideo sequence construction: We extract visual represen-\ntations (14 \u00d7 14 patches from frames with 2242 resolution)\nusing ViT-g [39], and add temporal embeddings. In order\nto reduce memory consumption, for every other frame we\ndrop random 50% of its patches.\nRecent work [17, 41]\nhas shown no significant loss in performance when random\npatches have been dropped.\nConditioning sequence construction:\nConstruction of\nthe conditioning sequence, which is an input of the\nTCR, is task-dependent.\nWe prefix a special task to-\nken ([CPN],[TRG],[QA],[STG] for captioning, tem-\nporal grounding, question-answering, and spatio-temporal\ngrounding respectively), depending on what task the model\nis solving.\nLLM sequence construction: We follow BLIP2 in the way\nwe construct the input sequence [30]. We concatenate the\noutput of the TCR module together with a <BOS> (begin-\nning of sentence) token and the instruction context tokens\n(for example question in VQA, or previous action sequence\ntogether with instruction for EGO4D action prediction).\nTCR architecture details: TCR is based on a transformer-\ndecoder module [43], consisting of 4 transformer blocks\nwith 8 attention heads and hidden dimension equal to 512.\nBlocks 0 and 2 contain cross-attention layers. For each task\nwe use 128 512-dimensional queries. These choices were\ntuned based on the downstream performance on NextQA\nvalidation set and then kept fixed for all tasks.\nImplementation details:\nOur model is implemented in\nFLAX [18], based on the scenic framework [10]. We use\nBLIP2 ViT-g FlanT5xl as a starting point and keep the vi-\nsion and text models frozen. The number of trainable pa-\nrameters is about 1% of the total memory footprint. JAX\nallow us to avoid storing gradients for non-trainable param-\neters, thus freeing up additional memory. Without train-\ning any part of the VLM, our model can process up to 124\nframes during training time per TPU.\n4\n4. Experiments\nIn the following section we first show results of ablation ex-\nperiments on NextQA [49] dataset, followed by the compar-\nison to the state-of-the-art. We then present the comparison\nto the state-of-the-art on egocentric video challenges (Ta-\nbles 4, 5, and 6). For each dataset, input design and eval-\nuation procedure are specified. Fine-tuning parameters are\noutlined in the supplementary material. Examples of our\nmodel working can be seen in Figure 3.\n4.1. Baseline\nIn order to demonstrate that the use of TCR in BLIP2-like\nmodel leads to better performance, we compare our model\nto the \u201cvanilla\u201d BLIP2. BLIP2 is not trained on videos, but\nit has been shown that it can be adapted to videos [54, 58].\nWe follow the procedure of [54] in order to compare the ef-\nficacy of the TCR module when compared to BLIP2 whose\nvisual-bottleneck module (\u2018Q-former\u2019) contains a higher\nnumber of parameters, but is limited in a number of frames\nit can process (up to 8 frames). To obtain final results we\nuniformly sample frames from each given video.\n4.2. NextQA\nNextQA is a manually annotated video-question-answering\ndataset where the model is asked to answer questions re-\ngarding temporal actions in a multiple-choice fashion [49].\nWe use the NextQA validation set to ablate our model\nchoices and compare our model to the current state-of-the-\nart model which is also based on BLIP2 architecture [55].\nNextQA is a challenging benchmark of 5440 videos con-\ntaining 52k questions. Videos are on average 44s long, and\nthe questions are separated into (C)ausal, (T)emporal, and\n(D)escriptive categories. We follow fine-tuning and evalu-\nation protocol from [30], with hyperparameters outlined in\nthe supplementary.\nInput design:\nVideo is subsampled to a target num-\nbers of frames (92 frames at approximately 2fps for\nthe final model), and temporal embeddings are com-\nputed accordingly.\nConditioning text is formed as\n\u201c[QA] Question\u201d where [QA] is learned special\ntoken reserved for VQA tasks.\nDuring fine-tuning,\nthe prompt to the language model is formed follow-\ning [55] as:\n\u201c[visual features] [question]\n[options] Considering information in\nframes, select the correct answer from\nthe options\u201d.\nEvaluation procedure:\nDuring inference, we restrict\nmodel generation to the answer vocabulary (i.e. \u201cOption A\u201d,\n\u201cOption B\u201d, ...), and select the most probable answer.\nComparison to SOTA: Results can be found in Table 3.\nOur model outperforms BLIP2 which demonstrates that the\nTCR module is successful in selecting the relevant frames,\nand also indicates the need for temporal modelling of this\nparticular task. While like us, the SeViLA model is based\non BLIP2, they train one BLIP2 model to sample relevant\nkeyframes, and a separate BLIP2 model to solve the task\nfrom the sampled keyframes, effectively doubling the num-\nber of trainable parameters. In contrast, TCR requires only\na single forward pass during training to both sample the fea-\ntures and solve the task. Our model outperform SeViLA in\n2 out of 3 categories (setting the new SOTA), hence show-\ning that number of observed frames makes up for lack of\ntrainable parameters.\n4.2.1\nAblation studies:\nIn the following section we investigate our model choices\nand seek to explain their impact on the performance of our\nmodel. Results can be seen in Table 2. Note that we fine-\ntune the model on a shorter training schedule which yields\nlower results, but allows for a quicker turnaround. We keep\nthe same fine-tuning parameters for all ablation studies.\nDoes text conditioning impact the results? We investigate\nthe performance of our model in three different scenarios:\n(1) when the conditioning prompt is unchanged in the eval-\nuation setting, (2) we completely remove the conditioning\nprompt, and (3) we modify the temporal word (\u2018before\u2019 to\n\u2018after\u2019 and vice-versa) in a hope to confuse the model. The\nresults can be seen in Table 2a. Conditioning indeed allows\nthe TCR module to extract more relevant features (+3.8).\nFurthermore, adversarial conditioning greatly impacts the\nperformance of the model (-7.6).\nDoes the number of frames matter? The input video-\nsequence length is important to the model performance. In\nTable 2b we show the performance dependence on the input\nsequence length. Note that videos are on average 44s long,\nthus 124 frames equates to sampling at a rate of 2.5fps.\nHow many queries should the LLM see? While there\nis a benefit of perceiving a longer length of a video input-\nsequence, it has been observed that including more visual\ntokens as input to the LLM does not lead to a better per-\nformance [55]. Therefore in Table 2c we investigate how\nmany queries the LLM should observe. Reducing the num-\nber of queries to a total of 128 (equivalent to four frames\naccording to [30]) achieves optimal performance.\n4.3. Ego4D\nEgocentric videos are a new frontier in effective long-term\nvideo understanding. They are often minutes long contain-\ning both fine-grained actions as well as long-term interac-\ntions [14]. We evaluate our model on several EGO4D tasks.\n4.3.1\nLong-term action anticipation (LTA):\nThe goal of the LTA challenge is to predict a sequence of\ntwenty actions in order of appearance from an input video.\n5\nWhat is the cat wearing around its \nneck?\nThe cat is wearing a plastic cone.\nList timestamps when the person is \nbrowsing through clothing items on \na rack.\n1,7,8,9\nWhat is in the video between 8th \nand 10th second.\nA Newfoundland Railway locomotive \nnumber 59.\nWhat does the man do after \nfinishing cleaning the ferret?\nUse towel to dry the ferret.\nFigure 3. Examples of our model responding to various textual prompts taken from NextQA, EGO4D-MR, and YTT datasets. The opacity\nof the images in the second row is correlated to the mean patch attention score for that frame. Note that frames are subsampled and the\nTCR conditioning is not included for clarity.\n(a)\nDifferent condition-\ning prompts on temporal-\nquestion set only.\ncond.\nacc \u2191\nyes\n64.9\nnone\n61.1\ncorrupt\n55.3\n(b) Impact of number of\nframes on model perfor-\nmance.\n#frms\nacc \u2191\n32\n64.4\n92\n66.2\n124\n65.9\n(c)\nImpact\nof\nthe\nto-\ntal number of queries on\nmodel performance.\n#queries\nacc \u2191\n32\n62.7\n64\n65.8\n128\n66.2\n256\n64.3\nTable 2. Ablation studies on validation set of the NextQA dataset.\nNote that the ablations were done on a short training schedule.\nModel\ntrain params\naccC \u2191\naccT \u2191\naccD \u2191\nacc \u2191\nSeViLA [55]\n346M\n73.4\n68.8\n83.5\n73.4\nHiTeA [52]\n/\n62.4\n58.3\n75.6\n63.1\nBLIP2 [30]\n188M\n64.9\n59.7\n77.8\n63.5\nOurs\n76M\n73.5\n69.8\n82.2\n73.5\nTable 3. Comparison to SOTA on NextQA dataset. Results are\nsplit into \u2018causal\u2019 (C), \u2018temporal\u2019 (T) and \u2018descriptive\u2019 (D) ques-\ntions, and overall accuracy.\nThe last observed action and action boundaries are given\nas well. The current state-of-the-art method relies solely\non the power of large-language models in order to predict\nthe sequence of future actions [20]. Our model adapts this\nidea but leverages the ability of TCR to process increas-\ningly longer videos in order to achieve superior results. We\ncompare our model to the SOTA model, as well as to fine-\ntuned BLIP2 using 8 frames as video input. We note that our\nmodel outperforms BLIP2 by a significant margin, clearly\nshowing the benefits of being able to observe denser video\nsequences for this task. Results can be seen in Table 4.\nInput design: We construct input for fine-tuning and eval-\nuation in the following fashion: video is subsampled uni-\nformly to a target number of frames (8 for BLIP2 with\nQ-former, and 96 for BLIP2 with TCR) and temporal em-\nbeddings denoting the frame timestamp are added to them.\nIf the \u201cBefore\u201d video is sampled, we only sample from a\nvideo clip before the last observed action, while \u201cWhole\u201d\nmeans we sample from the entire input video clip. The text\nprompts are designed as follows:\nComplete an action sequence,\nan action is one (verb, noun) pair.\nA complete sequence consists of\n28 actions.\nActions: (noun_1, verb_1)\n...\nand for the conditioning prompt we use:\n[LTA] [start_1] (noun_1, verb_1)\n[start_2] (noun_2, verb_2)\n...\nwhere (noun, verb) is an action pair, [LTA] is a\nlearned special token, and [start k] is a tokenised start\ntime of k-th action.\nEvaluation procedure: Our evaluation procedure follows\nclosely those of [20]. The model is fine-tuned to output\ncomma-separated action pairs following the prompt format-\nting. During the evaluation, we softmax predictions over the\n6\nMethod\nVideo\nVerbED \u2193\nNounED \u2193\nActionED \u2193\nPALM* [20]\nNo\n0.7165\n0.6767\n0.8934\nBLIP2 [30]\nBefore\n0.7512\n0.6873\n0.9103\nBLIP2 [30]\nWhole\n0.7500\n0.6799\n0.9086\nOurs\nBefore\n0.7009\n0.6472\n0.8792\nOurs\nWhole\n0.6763\n0.6180\n0.8522\nOurs*\nWhole\n0.6585\n0.6171\n0.8482\nTable 4. Comparison of various models on the validation set of\nEGO4D LTA challenge. Edit distance is reported and the lower\nthe score the better. The \u201cVideo\u201d column indicates whether the\nwhole video was observed (given) or just the video clip before the\nlast action. Models denoted with \u2018*\u2019 are sampled iteratively.\nreduced vocabulary of the label space for the LTA task. If\nboth nouns and verbs fall into their respective label space,\nwe append them to our prediction. For predictions with less\nthan 20 action pairs, we pad it with the last action. Models\ndenoted with (*) are sampled in an iterative fashion.\nComparison to SOTA: Table 4 shows the comparison to\nthe state-of-the-art on long-term action prediction.\nNote\nthat SOTA [20] uses only language model to predict the fu-\nture actions. We demonstrate that being able to perceive\nframes after the indicated timestep (which are given) helps\nfor future action prediction, but we outperform sota even\nwithout seeing them. Finally, we find that iterative evalua-\ntion (i.e. asking the model to predict action by action, as op-\nposed to the whole set of 20 actions, increases performance\neven further.\n4.3.2\nMoment queries (MQ):\nThe MQ task is similar to temporal action localisation or\nmoment retrieval tasks. Given a textual description of an\naction, the goal is to localise all possible instances of it in\nthe given video clip. Results can be seen in the Table 5.\nInput design: Video is subsampled uniformly to the target\nnumber of frames, and temporal embeddings denoting the\nframe timestamps are added to it. The conditioning prompt\nis formed as \u201c[TRG] action query string\u201d where\n[TRG] indicates the special token for temporal ground-\ning and action query string denotes the name of\nthe action label parsed as a string. The language model is\nprompted by the following string\nReturn a sequence of frame timestamps\nwhere <action name> is happening. The\ntimestamps range from 1 to 1000.\nEvaluation procedure: We softmax the model predictions\nfrom a reduced vocabulary of integers from 1 to 1000 (tem-\nporal coordinates are quantised similarly to [8]). We then\nconvert the prediction to the correct timestamp and aggre-\ngate predictions.\nMethod\nAvg. mAP \u2191\nR@1, tIoU=0.5 \u2191\nIntern Video [7]\n23.59\n41.13\nASL [38]\n27.85\n46.98\nOurs (96f)\n24.51\n42.99\nOurs (192f)\n25.45\n43.72\nTable 5. Comparison to the state of the art on the validation set of\nEgo4D Moment Query Challenge.\nMethod\nObserved\nframes\nQA\nacc (%) \u2191\nInternVideo [7]\n90*\n32.1\nBLIP2 [30]\n8\n27.2\nBLIP2 [30]\n92*(12)\n29.9\nTCR (ours)\n92\n34.2\nTCR (ours)\n184*(2)\n34.5\nTCR (ours)\n184**\n35.1\nHuman\n180\n67.2\nTable 6.\nComparison to SOTA and human performance on\nEgoSchema split of EGO4D. * denotes inference was done over\nmultiple forward passes and predictions were averaged, number\nof forward passes is given in \u2018()\u2019 if known. ** denotes higher pro-\nportion of patches was dropped.\nComparison to SOTA: Results in Table 5 show that de-\nspite the disadvantage of solving a discriminative task in\na generative way, our model still performs admirably (-2.4\nMAP) when compared to the state-of-the-art. In Table 12 of\nthe supplementary material, we present an additional evalu-\nation procedure (using direct classification) which can yield\neven better performance (+1.25MAP).\n4.3.3\nEgoSchema\nEgoSchema is a long-form VQA dataset sampled from\nEGO4D containing 5000 human curated multiple choice\nquestion answer pairs, spanning over 250 hours of real\nvideo data. Each question requires the model to select one\nout of 5 possible answers and is accompanied by a three-\nminute-long video clip [32]. Input and evaluation designs\nare the same as they are for NextQA.\nComparison to SOTA: Results can be seen in Table 6. Our\nmodel outperforms both the SOTA models (where inference\nwas done over multiple forward passes and prediction was\naveraged) and our re-implementation of BLIP2 (with both\nsubsampled frames and iterative inference approach). Sim-\nilar to [32], we observe relative saturation of performance\nwith increasing the number of frames.\n5. Do we need more frames?\nSince videos are a highly redundant data-source, one has to\nask how many frames, and at what sampling rate, does the\nmodel actually need to see to achieve good performance.\n7\n0\n20\n40\n60\n80\n100\n#frames\n66\n68\n70\n72\n74\n76\naccuracy\nAction recognition\nModel\nTCR\nTimesformer\n(a) Action recognition performance on Kinet-\nics400 dataset using pre-trained TCR module\nand linear classifier.\n20\n40\n60\n#frames\n74.0\n74.2\n74.4\n74.6\ncIDER\nMSR-VTT Captioning\nModel\nTCR\n(b) Captioning on short video sequences\non MSR-VTT dataset doesn\u2019t require many\nframes.\n20\n40\n60\n80\n#frames\n0.09\n0.10\n0.11\n0.12\nED_error\nEGO4D Forecasting\nModel\nTCR\n(c) Long-term action anticipation benefits from\nseeing further into the past.\n0\n20\n40\n60\n80\n#frames\n20\n25\n30\naccuracy\nEgoSchema\nModel\nTCR\nInternVideo\nmPLUG-owl\n(d) When videos are 3min long, the denser we\ncan subsample the video, the better.\n20\n40\n60\n80\n100\n120\n#frames\n63.5\n64.0\n64.5\n65.0\n65.5\n66.0\naccuracy\nNextQA\nModel\nTCR\nsevilla 0shot\n(e) The number of useful frames for our model\nsaturates at around 2fps.\n50\n100\n150\n200\n250\n#frames\n0.20\n0.25\n0.30\n0.35\n0.40\nMAE\nCountix\nModel\nTCR\n(f) Performance on countix dataset using pre-\ntrained TCR and a linear classifier.\nFigure 4. Performance vs number of frames utilised by the model on various different tasks.\nFor humans, it has been observed that frames sampled at a\ndensity of 25fps leads to a 8% higher accuracy on video QA\ntasks than when frames are sampled at 1fps [32].\nSevilla-Lara et. al [37] demonstrate that, in the realm\nof 10-second videos, the performance on recognizing some\nhuman action classes scales linearly with the number of\nframes observed while others can be solved through heavy\nsubsampling. Yu et. al [55] show that, with a limited com-\nputational budget, careful frame selection significantly im-\nproves performance on the NextQA task when compared to\nsampling frames uniformly or randomly.\nThe design of TCR allows our model to \u201csee\u201d more\nframes than ever before. In this section, we analyse the re-\nsults on six different tasks with respect to the number of\nframes consumed by the model in order to determine tasks\nthat require higher number of frames to solve. Figure 4\nshows the performance of our model across different tasks.\nModels are initialised from the same checkpoint, and then\nfinetuned on the downstream task using the target number\nof frames as an input. Evaluation is specific for each task\nas described in Section 4. Human action recognition (4a) or\nshort-video captioning (4b) do not require high number of\nframes to achieve strong performance unless a specialised\nmodel is used, which is consistent with literature [2, 37].\nFor future prediction tasks (4c), we found that increasing\nthe number of frames, hence covering larger video spans,\nhelps in reducing the error. This is not unexpected, as ac-\ntion sequences in EGO4D tend to be repetitive, so knowing\nwhat happened longer into the past leads to a better perfor-\nmance. Curiously, although long video sequences at higher\nframe-density are required for humans to solve problems\nin EgoSchema dataset (4d), most models\u2019 performance ac-\ntually peak or plateaus between 5 to 90 frames [32]. We\nargue that this is because they have not been trained with\nlong input length, and subsequently fail to capture seman-\ntic interdependencies within the video. TCR\u2019s performance\nincreases with the number of frames, but plateaus when\nmore sparsity in frame patches is needed to keep memory\nconsumption down (see Table 6). We believe that being\nable to see the span of entire video with more density (i.e.\nat sampling rates greater than 1fps) would further increase\nperformance on this benchmark. For NextQA where ques-\ntions contain temporal aspects (4e), there is a clear peak\nat about 2fps (for fixed-length videos), and a sharp decline\npast 1fps. This means that it is important to observe most\nof the frames, but frame density is not strictly required. Fi-\nnally, counting (4f) is an example of the task where frame\ndensity matters, and performance drops when fewer frames\nare utilised from a fixed length video.\nTo conclude, although the tasks that require the models\nto be able to reason over many frames either in span or den-\nsity are fairly limited, they do exist. Even the tasks that\ndo require reasoning over longer sequences such as long-\nvideo QA [32] are limited by the models which, even if they\ncan consume enough frames, are not trained for handling\nthem. We believe this will be an important area of research\nin video-understanding in following years.\n6. Conclusion\nWe present a parameter-efficient, text-conditioned mod-\nule and training method for bridging video-to-text gap\nthat can be applied to a large number of frames in\nvideos.\nEven though our model is entirely based on\nBLIP2 [30] architecture, introducing it in other VLMs\nwould be straightforward.\nWe believe that models\ncapable of perceiving long video sequences such as\nTCR will open up a promising new direction in research.\n8\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katie Millican, Malcolm Reynolds, Roman Ring,\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,\nSina Samangooei, Marianne Monteiro, Jacob Menick, Se-\nbastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-\nhand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karen Simonyan.\nFlamingo: a Visual Language Model for Few-Shot Learning,\n2022. 1, 2, 3\n[2] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.\nIs\nspace-time attention all you need for video understanding?\nIn ICML, page 4, 2021. 8\n[3] Shyamal Buch, Crist\u00b4obal Eyzaguirre, Adrien Gaidon, Jiajun\nWu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the\u201d\nvideo\u201d in video-language understanding. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 2917\u20132927, 2022. 2\n[4] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,\nand Juan Carlos Niebles. Activitynet: A large-scale video\nbenchmark for human activity understanding. In Proceed-\nings of the ieee conference on computer vision and pattern\nrecognition, pages 961\u2013970, 2015. 2\n[5] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 6299\u20136308, 2017. 2\n[6] Daozheng Chen, Mustafa Bilgic, Lise Getoor, and David Ja-\ncobs. Dynamic processing allocation in video. 33(11):2174\u2013\n2187, 2011. 2\n[7] Guo Chen, Sen Xing, Zhe Chen, Yi Wang, Kunchang Li,\nYizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong Zheng, Bingkun\nHuang, Zhiyu Zhao, Junting Pan, Yifei Huang, Zun Wang,\nJiashuo Yu, Yinan He, Hongjie Zhang, Tong Lu, Yali Wang,\nLimin Wang, and Yu Qiao. Internvideo-ego4d: A pack of\nchampion solutions to ego4d challenges, 2022. 2, 7\n[8] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-\noffrey Hinton. Pix2seq: A language modeling framework for\nobject detection. arXiv preprint arXiv:2109.10852, 2021. 4,\n7, 3\n[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al.\nScaling\ninstruction-finetuned language models.\narXiv preprint\narXiv:2210.11416, 2022. 3\n[10] Mostafa Dehghani,\nAlexey Gritsenko,\nAnurag Arnab,\nMatthias Minderer, and Yi Tay. Scenic: A jax library for\ncomputer vision research and beyond.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 21393\u201321398, 2022. 4\n[11] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre\nSermanet, and Andrew Zisserman. Counting out time: Class\nagnostic video repetition counting in the wild. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 10387\u201310396, 2020. 2, 1, 3\n[12] Ruohan Gao, Tae-Hyun Oh, Kristen Grauman, and Lorenzo\nTorresani. Listen to look: Action recognition by previewing\naudio. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 10457\u201310467,\n2020. 2\n[13] Shreyank N Gowda, Marcus Rohrbach, and Laura Sevilla-\nLara. Smart frame selection for action recognition. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence,\npages 1451\u20131459, 2021. 2\n[14] Kristen\nGrauman,\nAndrew\nWestbury,\nEugene\nByrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-\nson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel\nMartin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Ku-\nmar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael\nWray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao,\nSiddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean\nCrane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph\nFeichtenhofer, Adriano Fragomeni, Qichen Fu, Christian\nFuegen, Abrham Gebreselasie, Cristina Gonzalez, James\nHillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo,\nJachym Kolar, Satwik Kottur, Anurag Kumar, Federico Lan-\ndini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Man-\ngalam, Raghava Modhugu, Jonathan Munro, Tullie Mur-\nrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes,\nMerey Ramazanova, Leda Sari, Kiran Somasundaram, Au-\ndrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo,\nYuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo\nArbelaez, David Crandall, Dima Damen, Giovanni Maria\nFarinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V.\nJawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard\nNewcombe, Aude Oliva, Hyun Soo Park, James M. Rehg,\nYoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Tor-\nralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Ma-\nlik. Ego4d: Around the World in 3,000 Hours of Egocentric\nVideo. In IEEE/CVF Computer Vision and Pattern Recogni-\ntion (CVPR), 2022. 2, 5\n[15] Kai Han, Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, An-\ndrea Vedaldi, and Andrew Zisserman. Automatically discov-\nering and learning new visual categories with ranking statis-\ntics. In International Conference on Learning Representa-\ntions, 2020. 3\n[16] Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal\nalignment networks for long-term video. In CVPR, 2022. 1,\n3\n[17] Tengda Han, Weidi Xie, and Andrew Zisserman. Turbo train-\ning with token dropout. In BMVC, 2022. 4\n[18] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Rit-\nter, Bertrand Rondepierre, Andreas Steiner, and Marc van\nZee. Flax: A neural network library and ecosystem for JAX,\n2023. 4\n[19] Huazhang Hu, Sixun Dong, Yiqun Zhao, Dongze Lian,\nZhengxin Li, and Shenghua Gao. Transrac: Encoding multi-\nscale temporal correlation with transformers for repetitive\naction counting. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n19013\u201319022, 2022. 3\n[20] Daoji Huang, Otmar Hilliges, Luc Van Gool, and Xi Wang.\n9\nPalm: Predicting actions through language models @ ego4d\nlong-term action anticipation challenge 2023, 2023. 6, 7\n[21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen\nLi, and Tom Duerig.\nScaling Up Visual and Vision-\nLanguage Representation Learning With Noisy Text Super-\nvision. arXiv:2102.05918 [cs], 2021. arXiv: 2102.05918.\n2\n[22] Hanwen Jiang, Santhosh Kumar Ramakrishnan, and Kristen\nGrauman. Single-stage visual query localization in egocen-\ntric videos. arXiv preprint arXiv:2306.09324, 2023. 2\n[23] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion.\nMdetr-\nmodulated detection for end-to-end multi-modal understand-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 1780\u20131790, 2021. 1\n[24] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, Mustafa Suleyman,\nand Andrew Zisserman. The kinetics human action video\ndataset, 2017. 1, 3\n[25] Kiyoon Kim, Shreyank N Gowda, Oisin Mac Aodha, and\nLaura Sevilla-Lara. Capturing temporal information in a sin-\ngle frame: Channel sampling strategies for action recogni-\ntion. arXiv preprint arXiv:2201.10394, 2022. 2\n[26] Dohwan Ko, Joonmyung Choi, Juyeon Ko, Shinyeong Noh,\nKyoung-Woon On, Eun-Sol Kim, and Hyunwoo J Kim.\nVideo-text representation learning via differentiable weak\ntemporal alignment. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2022.\n3\n[27] Bruno Korbar and Andrew Zisserman.\nEnd-to-end track-\ning with a multi-query transformer.\narXiv preprint\narXiv:2210.14601, 2022. 1\n[28] Bruno Korbar, Du Tran, and Lorenzo Torresani. Scsampler:\nSampling salient clips from video for efficient action recog-\nnition. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 6232\u20136242, 2019. 2, 1,\n3\n[29] Weicheng Kuo, A. J. Piergiovanni, Dahun Kim, Xiyang Luo,\nBen Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew\nDai, Zhifeng Chen, Claire Cui, and Anelia Angelova. MaM-\nMUT: A Simple Architecture for Joint Learning for Multi-\nModal Tasks, 2023. 2, 3\n[30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2: Bootstrapping Language-Image Pre-training with\nFrozen Image Encoders and Large Language Models, 2023.\n1, 2, 3, 4, 5, 6, 7, 8\n[31] Yingwei Li, Yi Li, and Nuno Vasconcelos. Resound: To-\nwards action recognition without representation bias. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 513\u2013528, 2018. 2\n[32] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra\nMalik. Egoschema: A diagnostic benchmark for very long-\nform video language understanding. In Thirty-seventh Con-\nference on Neural Information Processing Systems Datasets\nand Benchmarks Track, 2023. 2, 7, 8\n[33] Effrosyni Mavroudi, Triantafyllos Afouras, and Lorenzo\nTorresani. Learning to ground instructional articles in videos\nthrough narrations. arXiv preprint arXiv:2306.03802, 2023.\n2\n[34] Rameswar Panda, Chun-Fu Richard Chen, Quanfu Fan, Xi-\nmeng Sun, Kate Saenko, Aude Oliva, and Rogerio Feris.\nAdamml: Adaptive multi-modal learning for efficient video\nrecognition. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 7576\u20137585, 2021. 2\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. 2021. 2\n[36] Santhosh Kumar Ramakrishnan, Ziad Al-Halah, and Kristen\nGrauman. Spotem: Efficient video search for episodic mem-\nory. 2023. 2, 3\n[37] Laura Sevilla-Lara, Shengxin Zha, Zhicheng Yan, Vedanuj\nGoswami, Matt Feiszli, and Lorenzo Torresani. Only time\ncan tell: Discovering temporal data for temporal modeling.\nIn Proceedings of the IEEE/CVF winter conference on ap-\nplications of computer vision, pages 535\u2013544, 2021. 1, 2,\n8\n[38] Jiayi Shao, Xiaohan Wang, Ruijie Quan, and Yi Yang. Ac-\ntion sensitivity learning for the ego4d episodic memory chal-\nlenge 2023. arXiv preprint arXiv:2306.09172, 2023. 2, 7, 3\n[39] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. Eva-clip: Improved training techniques for clip at scale.\narXiv preprint arXiv:2303.15389, 2023. 2, 4\n[40] Reuben Tan,\nMatthias De Lange,\nMichael Iuzzolino,\nBryan A Plummer, Kate Saenko, Karl Ridgeway, and\nLorenzo Torresani. Multiscale video pretraining for long-\nterm activity forecasting. arXiv preprint arXiv:2307.12854,\n2023. 2\n[41] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.\nVideomae: Masked autoencoders are data-efficient learners\nfor self-supervised video pre-training. Advances in neural\ninformation processing systems, 35:10078\u201310093, 2022. 2,\n4\n[42] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann\nLeCun, and Manohar Paluri. A closer look at spatiotemporal\nconvolutions for action recognition. In Proceedings of the\nIEEE conference on Computer Vision and Pattern Recogni-\ntion, pages 6450\u20136459, 2018. 2, 1\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 4\n[44] Limin Wang, Yu Qiao, Xiaoou Tang, et al. Action recog-\nnition and detection by combining motion and appearance\nfeatures. THUMOS14 Action Recognition Challenge, 1(2):\n2, 2014. 2\n[45] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yi-\nnan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2:\nScaling video masked autoencoders with dual masking. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 14549\u201314560, 2023. 2\n10\n[46] Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng\nHan, and Gao Huang.\nAdaptive focus for efficient video\nrecognition. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 16249\u201316258, 2021.\n2\n[47] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun\nHuang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun\nWang, et al. Internvideo: General video foundation models\nvia generative and discriminative learning. arXiv preprint\narXiv:2212.03191, 2022. 2\n[48] Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, and Larry S\nDavis. Liteeval: A coarse-to-fine framework for resource\nefficient video recognition. Advances in Neural Information\nProcessing Systems, 32, 2019. 2\n[49] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.\nNext-qa: Next phase of question-answering to explaining\ntemporal actions.\nIn Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n9777\u20139786, 2021. 2, 5\n[50] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and\nCordelia Schmid. Tubedetr: Spatio-temporal video ground-\ning with transformers. In CVPR, 2022. 3\n[51] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, An-\ntoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and\nCordelia Schmid. Vid2seq: Large-scale pretraining of a vi-\nsual language model for dense video captioning. In CVPR,\n2023. 3\n[52] Qinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu, Qi Qian,\nJi Zhang, and Fei Huang.\nHitea: Hierarchical temporal-\naware video-language pre-training.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 15405\u201315416, 2023. 2, 6\n[53] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-\nFei.\nEnd-to-end learning of action detection from frame\nglimpses in videos. In CVPR, pages 2678\u20132687, 2016. 2\n[54] Keunwoo Peter Yu. VideoBLIP. 3, 5\n[55] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal.\nSelf-chained image-language model for video localization\nand question answering. arXiv preprint arXiv:2305.06988,\n2023. 5, 6, 8, 2\n[56] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.\nMERLOT: Multimodal Neural Script Knowledge Models.\narXiv:2106.02636 [cs], 2021. 2\n[57] Chen-Lin Zhang, Jianxin Wu, and Yin Li. Actionformer: Lo-\ncalizing moments of actions with transformers. In European\nConference on Computer Vision, pages 492\u2013510. Springer,\n2022. 2\n[58] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding. arXiv preprint arXiv:2306.02858, 2023. 5\n[59] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and\nZhicheng Yan.\nHacs: Human action clips and segments\ndataset for recognition and temporal localization. In Pro-\nceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 8668\u20138678, 2019. 2\n[60] Yuan Zhi, Zhan Tong, Limin Wang, and Gangshan Wu.\nMgsampler: An explainable sampling strategy for video ac-\ntion recognition.\nIn Proceedings of the IEEE/CVF Inter-\nnational conference on Computer Vision, pages 1513\u20131522,\n2021. 2, 1\n11\nText-Conditioned Resampler For Long Form Video Understanding\nSupplementary Material\nA. Further training details\nSection 3.2 of the main paper give the purpose of each train-\ning stage, and what is done, while table 1 investigates the\nimpact of each stage on the downstream tasks. In this sec-\ntion, we first detail the pre-training stage and illustrate dif-\nferent tasks, then we give the fine-tuning details, and finally\ninvestigate 0-shot performance of multi-task trained mod-\nels. All models are trained using the AdamW optimizer\nwith global norm clipping of 1. Training hyperparameters\nare given in Table 8.\nA.1. Pre-training\nAfter initialisation, we pre-train the model using the three\noutlined tasks (captioning, temporal grounding, denoising).\nSpecifically, we define a dataset with each task, assign it a\nweight (1.0, 0.5, 0.5 respectively), and then, following pro-\ncedure in [1], we accumulate gradients across each task with\nloss being weighted per-dataset. Loss weights were defined\nempirically and kept fixed throughout. An illustration of\ndataset tasks can be seen in Table 7.\nA.2. Fine-tuning procedure\nDue to a diversity in our downstream tasks, both in terms\nof the data type (e.g. instructional videos, vs egocentric\nvideos), and tasks for which we train special tokens, fine-\ntuning is a crucial step in our training pipeline. If necessary,\nwe introduce a new special token for the task, and proceed\nto fine tune the model on the downstream task specifically.\nA.2.1\nZero-shot performance and multi-task fine-\ntuning.\nModels such as Flamingo [1] can be \u2018prompted\u2019 to solve\ntasks in a low-shot setting, thus making fine-tuning unnec-\nessary. Flamingo, however, is trained on much wider vari-\nety of datasets (mostly from an image domain, where large\ndatasets with less noisy supervision are available).\nOur\nmodel lacks this pre-training and, additionally, faces a sig-\nnificant challenge in this setting, as the number of output\nqueries from TCR (128) is significantly higher than that\nof Perciever Resampler (8) resulting in longer sequences.\nNevertheless, we present the results in Table 9 in three seg-\nments. First is the initialised and pre-trained model without\nany modifications. Second row is a single model (one set of\nweights) that is trained on all downstream datasets jointly\nwith equal weighting. In other words, after pre-training, we\nfine-tune the model jointly on all downstream datasets for\n20 epochs, accumulating the gradients over all datasets with\nequal weight. Finally, in the last row, we present the results\nof four fully fine-tuned models. As observed in models util-\nising large instruction-tuned language models, our model\nperforms better in a few-shot setting [1]. As expected the\nimprovement in fully fine-tuned models is minor since mod-\nels are already specialised for the specific task. We want to\nhighlight how our model with a single set of weights can\nperform well across different tasks simply with the addi-\ntion of a few prompts for in context learning. In case even\nstronger performance are needed, we can further improve\nby specialising the model for the specific task/dataset, and\nwe report these numbers in the main paper.\nB. Study of standalone TCR architecture\nIn order to further validate our architecture choice, below\nwe conduct a series of experiments with various uses of\nTCR without the LLM. In section B.1, we run experiments\nwith TCR simply acting as a \u201chead\u201d for a visual model. In\nthis case, the visual backbone is fixed and TCR is trained\nfrom scratch on a target task without special tokens.\nIn\nsection B.2, we initialise and pre-train TCR as described\nin Section 3.2, and instead fine-tune TCR with a two-layer\nMLP on top of it.\nB.1. Proof-of-concept experiments\nTransformer decoder modules have been shown to per-\nform well on action-detection tasks [16], and several works\nshowed they can cross-attend image information with se-\nmantic information [23, 27]. In this section, we seek to val-\nidate our architecture design on a set of simplified tasks.\nSpecifically we show that a) TCR can be trained indepen-\ndently to sample relevant features for action classification,\nb) \u2018select\u2019 the frames corresponding to the query action\nwithin a couple of videos, and finally c) perform semantic\ntasks such as counting on actions. We conduct these exper-\niments on various splits of the Kinetics dataset [11, 24].\nSampling for action recognition: First, we train the model\nfor action recognition. Most models average predictions\nover multiple uniformly-sampled frames or short clips in\norder to obtain a video-level prediction [42].\nDedicated\nsamplers such as [28, 60] have been trained to specifically\nsample relevant frames or clips. The goal of this experi-\nment is to see whether TCR can sample features in such\na way to improve overall video-level accuracy.\nIn other\nwords, given a 300 frames input video, can the model sam-\nple 32 frames in such a way that classification performance\nis better than other naive benchmarks. To do so, we extract\nframe-level features from the frozen ViT-g model, add si-\n1\nTask\nCondition\n(to TCR)\nContext\n(to LLM)\nCaptioning (CPN): caption the video given temporal boundaries\n[CPN][3][5]\nWhat happens from [3] to [5]?\nTemporal grounding (TRG): find the temporal\nboundaries of the corresponding caption\n[TRG] Today we look at this 1967...\nReconstruct following sentence:\n[MASK][MASK] Today we look at this...\nDenoising (CPN): masked language modeling for captions\n(missing information is in bold).\n[CPN][3][5]\n[3][5]Today we [MASK] ...\n[CPN][3][5]\n[3] [MASK] look at this [MASK]\nTable 7. Pre-training task examples with a condition sequence (TCR input) and the context sequence (LLM input). [CPN] (captioning),\n[TRG] (temporal grounding), [MASK] (masking) are special tokens, [2] is a sample of a tokenised timestamp. The model is tasked with\npredicting a caption \u201cToday we look at this 1967 Dodge Ram\u201d which is happening between 3rd and 5th second of a video.\nStage\nDataset\nBatch size\nLR\nEpochs\nWarmup steps\nPre-training\nYTT - captioning\n256\n1e-4\n10\n1k\nYTT - temporal grounding\nYTT - denoising\nFinutuning\nMSR-VTT\n128\n1e-5\n20\n2k\nNextQA\n128\n1e-5\n40\n2k\nNextQA - short\n64\n5e-5\n10\n1k\nEgoSchema\n64\n1e-5\n20\n1k\nEgoLTA\n64\n1e-5\n20\n1k\nEgo4D MQ\n128\n1e-5\n10\n500\nTable 8. Training hyper-paramerters\nnusoidal positional embeddings, and pass them to TCR as\na key-value pair to the cross attention layers. We \u201cprompt\u201d\nthe model with 32 learnable queries, each of which is pro-\ncessed through a 400-way classifier whose predictions are\nthen averaged to obtain the final prediction. This model is\ntrained for 10 epochs on Kinetics-400 dataset and compared\nto: linear classification of features from 32 random frames,\nself-attention pooling where the classification layer is done\non a learned CLS token, and on two top video clips sampled\nby SCSampler [28] (32 frames). TCR is able to outper-\nform naive sampling methods and bests standard attention-\npooling approach, which means it can learn about relative\nimportance of input features to the text prompt. It, how-\never, cannot best models which were trained with explicit\nsaliency supervision such as SCSampler. This experiment\nindicates that incorporation of explicit saliency supervision\nsuch as in [28, 55, 60] is a promising direction of further\nresearch. Results can be seen in Table 10.\nText-conditioned sampling:\nNext, we want to verify\nwhether the model can \u201cselect\u201d correct frames from a video\ngiven an action class prompt. To this end we design an ar-\ntificial experiment by stitching 128 frames of up to 8 kinet-\nics videos, and the task is to classify frames containing the\nprompt action as positive examples. Only a single video of\na query class can be present, it\u2019s location within the video is\nrandomised, and the number of frames sampled from each\nvideo is random (from 1 to 64). We add position encod-\ning (from 0 to 127) to each frame, and feed them to TCR.\nFor query input, we use the action prompt followed by 32\nlearnable queries. After TCR, we pass each query through\n128 one-vs-all binary classifiers, one for each of the input\nframes. The classifiers are trained to predict the presence\nor absence of the action at the corresponding input frame.\nAt inference time, if the confidence score for classifier j is\nhigher than 0.5 we add to the predictions of query k the\nframe j (note that each query can predict more than one\nframe). Finally we consider the set of all predictions across\nall the queries and compare it to the ground truth frame ids.\nIf the frame ids of the frames corresponding to the action\nprompt are contained in the predicted set we count that as\na positive paring and the rest as negative. We compare this\nTCR approach to simply training the per-frame 400-way ac-\ntion classifier on top of ViT, and count the label as a posi-\ntive if the target class is predicted. We were able to achieve\nprecision of 0.75 and recall of 0.68, which is higher than\nsimply classifying action label for each frame of the video\n(0.70, 0.44).\nText-conditioned semantic processing: Finally, with mi-\nnor addition, we show that TCR architecture can be used as\na counting mechanism as well due to its potential to \u201clook\u201d\nand reason over every single frame. We consider the set-\ntings of [11], feed the video at full temporal resolution, and\nform the query the same as in previous paragraph. We then\nconcatenate the queries and pass them through a TransRAC\n2\nFT\nk-shot\nNextQA\n(Acc)\nEgo4D-LTA\n(ActionED)\nEgo4D-MQ\n(avg mAP)\nEgoSchema\n(acc)\nNone\n0\n34.2\n0.9354\n19.88\n14.5\n2\n37.3\n/\n21.75\n19.0\nMultitask\n0\n61.7\n0.9199\n21.74\n28.7\n2\n64.1\n/\n23.01\n30.9\nPer task\n0\n73.5\n0.8782\n24.51\n34.2\n2\n73.5\n/\n24.83\n34.1\nTable 9. Comparison of pre-trained vs fine-tuned model on the downstream datasets in 0- and few-shot setting. Numbers reported in the\nmain comparison are fine-tuned per-task and evaluated in a 0-shot setting for consistency. Note that for Ego4D-LTA task, we couldn\u2019t fit\nthe entire action sequence of a multi-shot example due to the maximum sequence length of our LLM.\nSampling\nmethod\nKinetics\nAC\nrandom\n74.3\nAttention\n75.2 (+0.9)\nTCR (ours)\n75.7 (+1.4)\nSCSampler [28]\n77.8 (+3.4)\nTable 10. Linear classification of 32 frames sampled from Kinet-\nics400 [24] val set using various sampling methods.\nMethod\nMAE\nOBO\n[11]\n0.36\n0.30\nTCR\n0.46\n0.39\nTable 11. Counting repeating repetitions in videos on Countix [11]\ndataset.\nPeriod Predictor [19] to obtain the final count. The TCR\nmodule and TransRAC predictor are trained on syntetic data\nas descibed in [11] for 24 epochs. Results outperform the\nSOTA benchmark by a large margin, as can be seen in Ta-\nble 11\nB.2. TCR without LLM\nIn this section, we show that the pre-trained TCR module\ncan be easily adapted to solve discriminative tasks without\nthe need for an LLM. The difference to the previous section\nis the fact that TCR is initialised and pre-trained using the\nprocedure outlined in Section 3.2 of the main paper. For\nthis set of experiments, we keep the pre-trained visual en-\ncoder and TCR module frozen and train simple 2-layer MLP\nadapters on top of them. For the EGO4D moment query\ntask, we adapt the alignment module from Han et al. [16]\nwhich trains two linear layers on top of TCR output with\nset-prediction loss consisting of two elements: one predict-\ning whether the sequences correspond to a given action and\none regressing the temporal span of the query. For the nat-\nural language query task, we adopt the same training pro-\nMethod\nMQ\nNLQ\nAVG MAP\nR@1\nMR@1\nMR@5\nSpotEM [36]\n/\n/\n11.56\n19.90\nASL [38]\n27.85\n46.98\n/\n/\nTCR\n26.71\n44.96\n11.42\n19.95\nTCR w/ LLM\n25.45\n43.72\n10.12\n18.99\nTable 12. Performance of TCR as a standalone module on discrim-\ninative downstream tasks. LLMs tend to struggle with regression\ntasks [8] and while we present these numbers in the main paper due\nto their flexibility to solve a wide variety of task, we also show that\nTCR can provide better performance for these tasks using ad-hoc\nregression adapters and significantly narrowing the gap with state\nof the art methods.\ncedure as Yang et al. [50] for spatiotemporal localisation on\ntop of TCR output. Results can be seen in Table 12. It is no-\ntable that, without additional pre-training of the TCR mod-\nule, the features it outputs can be generalised well to tasks\nthat benefit from video sampling. Our general pre-trained\narchitecture comes close to the specialised solution [38] to\nthe Moments Query challenge (\u22121.14 Avg.MAP), giving\nus future direction for improving it for tasks of video re-\ntrieval. Similarly on the Natural Language Query challenge,\nwith only a small adapter trained for spatiotemporal locali-\nsation our model challenges the state-of-the-art [36] (\u22120.14\nMR@1) which has a sampler and detector head trained for\nthis task specifically.\nWe believe that optimising VLMs\nfor regressive tasks requiring high spatiotemporal resolu-\ntion will be an important future research direction.\n3\n"
  },
  {
    "title": "Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation",
    "link": "https://arxiv.org/pdf/2312.11532.pdf",
    "upvote": "5",
    "text": "Topic-VQ-VAE: Leveraging Latent Codebooks for\nFlexible Topic-Guided Document Generation\nYoungJoon Yoo1 Jongwon Choi2\n1 ImageVision, NAVER, Cloud.\n2 Department of Advanced Imaging (GSAIM) and Graduate School of AI, Chung-Ang University.\n1youngjoon.yoo@navercorp.com, 2choijw@cau.ac.kr\nAbstract\nThis paper introduces a novel approach for topic model-\ning utilizing latent codebooks from Vector-Quantized Varia-\ntional Auto-Encoder (VQ-VAE), discretely encapsulating the\nrich information of the pre-trained embeddings such as the\npre-trained language model. From the novel interpretation\nof the latent codebooks and embeddings as conceptual bag-\nof-words, we propose a new generative topic model called\nTopic-VQ-VAE (TVQ-VAE) which inversely generates the\noriginal documents related to the respective latent codebook.\nThe TVQ-VAE can visualize the topics with various gener-\native distributions including the traditional BoW distribution\nand the autoregressive image generation. Our experimental\nresults on document analysis and image generation demon-\nstrate that TVQ-VAE effectively captures the topic context\nwhich reveals the underlying structures of the dataset and\nsupports flexible forms of document generation. Official im-\nplementation of the proposed TVQ-VAE is available at https:\n//github.com/clovaai/TVQ-VAE.\nIntroduction\nTopic modeling, the process of extracting thematic struc-\ntures, called topic, represented by coherent word sets and\nsubsequently clustering and generating documents based on\nthese topics, constitutes a foundational challenge in the ma-\nnipulation of natural language data The initiative Latent\nDirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) and\nsubsequent studies (Teh et al. 2004; Paisley et al. 2014)\nconfigure the inference process as a Bayesian framework\nby defining the probabilistic generation of the word, inter-\npreted as bag-of-words (BoW), by the input word and docu-\nment distributions. The Bayesian frameworks utilize the co-\noccurrence of the words in each document and have become\na standard for topic models.\nDespite the success, topic modeling has also faced de-\nmands for the evolution to reflect advances of recent deep\ngenerative studies. One main issue is utilizing information\nfrom large-scale datasets encapsulated in pre-trained em-\nbeddings (Pennington, Socher, and Manning 2014; Devlin\net al. 2018; Radford et al. 2021). Many follow-up studies\nhave approached the problem in generative (Dieng, Ruiz,\nand Blei 2020) or non-generative (Duan et al. 2021; Xu et al.\nCopyright \u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n2022; Grootendorst 2022) directions. Moreover, with the ad-\nvancements in generation methods, such as autoregressive\nand diffusion-based generation, there is a growing need for\nthe topic-based generation to evolve beyond the traditional\nBoW form and become more flexible.\nTo address the issue, we propose a novel topic-driven\ngenerative model using Vector-Quantized (VQ) embeddings\nfrom (Van Den Oord, Vinyals et al. 2017), an essential build-\ning block for the recent vision-text generative model such\nas (Ramesh et al. 2021). In contrast to previous approaches\nin topic modeling (Gupta and Zhang 2021, 2023) that treat\nVQ embeddings as topics, in our method, each VQ em-\nbedding represents the embeddings of conceptually defined\nwords. Through the distinct perspective, we achieve the en-\nhanced flexibility that a corresponding codebook serves as\nits BoW representation. We further demonstrate that the\ncodebook consisting of VQ embedding itself is an implicit\ntopic learner and can be tuned to achieve exact topic context,\nwith a supporting flexible format of sample generation.\nBased on the interpretation, we present a novel genera-\ntive topic model, Topic-VQ Variational Autoencoder (TVQ-\nVAE), which applies a VQ-VAE framework (Van Den Oord,\nVinyals et al. 2017) incorporating topic extraction to the\nBoW representation of the VQ-embedding. The TVQ-VAE\nfacilitates the generation of the BoW-style documents and\nalso enables document generation in a general configuration,\nsimultaneously. We demonstrate the efficacy of our pro-\nposed methodology in two distinct domains: (1) document\nclustering coupled with set-of-words style topic extraction,\nwhich poses a fundamental and well-established challenge\nin the field of topic modeling. For the pre-trained informa-\ntion, we utilize codebooks derived from inputs embedded\nwith a Pre-trained Language Model (PLM) (Reimers and\nGurevych 2019). Additionally, (2) we delve into the autore-\ngressive image generation, leveraging the VQ-VAE frame-\nwork with latent codebook sequence generation as delin-\neated in (Van Den Oord, Kalchbrenner, and Kavukcuoglu\n2016; Esser, Rombach, and Ommer 2021).\nThe contributions of the paper are summarized as follows:\n\u2022 We propose a new generative topic modeling framework\ncalled TVQ-VAE utilizing codebooks of VQ embed-\ndings and providing a flexible form of sampling. Our pro-\nposed model interprets the codebooks as a conceptual\nword and extracts the topic information from them.\narXiv:2312.11532v2  [cs.CL]  21 Jan 2024\n\u2022 Our proposed model TVQ-VAE provides a general form\nof probabilistic methodology for topic-guided sampling.\nWe demonstrate the application of samplings, from a typ-\nical histogram of the word style sample used in the topic\nmodel to an autoregressive image sampler.\n\u2022 From the extensive analysis of two different data do-\nmains: (1) document clustering typically tackled by the\nprevious topic models and (2) autoregressive image gen-\neration with topic extraction. The results support the pro-\nposed strength of the TVQ-VAE.\nPreliminary\nKey Components of Topic Model\nWe summarize the essence of the topic model where the gen-\nerative or non-generative approaches commonly share as (1)\nsemantic topic mining for entire documents and (2) docu-\nment clustering given the discovered topics. Given K num-\nber of topics \u03b2k \u2208 \u03b2, k = 1, ..., K, the topic model basically\nassigns the document to one of K topics, which is a cluster-\ning process given the topics. This assigning can be deter-\nministic or generatively by defining the topic distribution of\neach document, as:\nzdn \u223c p(z|\u03b8d),\n(1)\nwhere the distribution p(z|\u03b8d) draws the indexing variable\nzdn that denotes the topic index \u03b2zdn that semantically in-\ncludes the word wdn in d\u2019th document. In a generative set-\nting, the random variable \u03b8 is typically defined as K dimen-\nsional Categorical (Blei, Ng, and Jordan 2003) distribution\nwith Dirichlet prior \u03b1 or Product of Expert (PoE) (Srivas-\ntava and Sutton 2017). The topic \u03b2k is defined as a set of\nsemantically coherent words wkn \u2208 \u03b2k, 1, ..., Nw or by a\nword distribution in a generative manner, as:\nwk \u223c p(w|\u03b2k).\n(2)\nSimilarly, the p(w|\u03b2k) can be defined as categorical (Blei,\nNg, and Jordan 2003) like distributions. Classical proba-\nbilistic generative topic models (Blei, Ng, and Jordan 2003;\nSrivastava and Sutton 2017; Miao, Yu, and Blunsom 2016;\nZhang et al. 2018; Nan et al. 2019) interpret each docu-\nment d as BoW wd = {wd1, ..., wdn} and analysis the joint\ndistribution p(\u03b8, \u03b2|wd) from equations (1-2), by approx-\nimated Bayesian inference methods (Casella and George\n1992; Wainwright, Jordan et al. 2008; Kingma and Welling\n2013). We note that their probabilistic framework reflects\nword co-occurrence tendency for each document.\nWhen embedding is applied to the topic modeling frame-\nworks (Dieng, Ruiz, and Blei 2020; Duan et al. 2021; Xu\net al. 2022; Meng et al. 2022), some branches of embedded\ntopic models preserve the word generation ability, and hence\nthe word embedding is also included in their probabilis-\ntic framework, such as ETM (Dieng, Ruiz, and Blei 2020).\nThe non-generative embedded topic models including recent\nPLM-based topic models (Sia, Dalmia, and Mielke 2020;\nGrootendorst 2022; Meng et al. 2022) extract topic embed-\nding directly from distance-based clustering method, by-\npassing the complicated Bayesian inference approximation,\nwith utilizing in post-processing steps.\nVector Quantized Embedding\nDifferent from the typical autoencoders mapping an input x\nto a continuous latent embedding space E, Vector-Quantized\nVariational Auto-Encoder (VQ-VAE)\n(Van Den Oord,\nVinyals et al. 2017) configures the embedding space to be\ndiscrete by the VQ embeddings \u03f1 = {\u03c1n \u2208 RD\u03c1, n =\n1, ..., N\u03c1}. Given the encoder function of the VQ-VAE as\nf = Enc(x; WE), the vector quantizer (cx, \u03c1x) = Q(f)\ncalculates the embedding \u03c1x \u2208 \u03f1, which is the closest em-\nbedding to f among the set of VQ embedding \u03f1, and its\none-hot encoded codebook cx \u2208 RN\u03c1. The embedding \u03c1x\nand cx is defined as:\n\u03c1x = cx \u00b7 \u02c6\u03c1,\n\u02c6\u03c1 = [\u03c11, ..., \u03c1N\u03c1] \u2208 RN\u03c1\u00d7D\u03c1,\n(3)\nwhere N\u03c1 denotes the size of the discrete latent space, which\nis smaller than the original vocabulary size Nw. D\u03c1 is the\ndimensionality of each latent embedding vector. Here, we\ndenote the resultant sets of embedding \u03c1 and codebook c are\ndefined as \u03c1 = {\u03c1x} and c = {cx}. When given an image\nx \u2208 RH\u00d7W \u00d73 as a VQ-VAE input, we collect the sequence\nof quantized vector \u03c1 and c as:\n\u03c1 = {\u03c1ij \u2208 \u03f1|i = 1, ..., h, j = 1, ..., w},\nc = {cij \u2208 RN\u03c1|i = 1, ..., h, j = 1, ..., w},\n(4)\nwhere the embedding \u03c1ij and the codebook cij maps the\nclosest encoding of the spatial feature fij of the latent\nvariable f\n=\n{fij|i\n=\n1, ..., h, j\n=\n1, ..., w}, f\n=\nEnc(x; WE)\n\u2208\nRh\u00d7w\u00d7d. The decoder function \u02dcx\n=\nDec(c, \u03c1; WD) then reconstruct the original image x using\nthe VQ embedding \u03c1 and its codebook c. In this case, the\nvector quantizer Q(\u00b7) calculates the sequence of codebook c\nand the corresponding embeddings \u03c1, as (c.\u03c1) = Q(f).\nMethodology\nWe present a new topic-driven generative model, TVQ-VAE,\nby first introducing a new interpretation to the VQ-VAE out-\nput: codebooks c and their embedding \u03c1.\nVector Quantized Embedding as Conceptual Word\nHere, we first propose a new perspective for interpreting a\nset B including the VQ embedding \u03c1 and its codebook c:\nB = {bi = (ci, \u03c1i)|i = 1, ...N\u03c1},\n(5)\nas conceptual word. The conceptual word bi each consists\nof VQ embedding \u03c1i and its codebook ci. We note that the\nnumber of the virtual word bi is equivalent to total number\nN\u03c1 of VQ embeddings.\nOne step further, since the typical selection of the number\nN\u03c1 is much smaller than the original vocabulary, we mod-\nify the set B so that multiple embeddings express the input,\nwhere the codebook c in Equation (3) becomes a multi-hot\nvector. This relaxation lets the codebooks deal with a much\nlarger size of words. Specifically, given word w and its em-\nbedding zw = Enc(w) from the VQ-VAE encoder, we sup-\nport the expansion from one-hot to multi-hot embedding by\n(a) BoW form.\n(b) General form.\n(c) Visualized diagram of TVQ-VAE.\nFigure 1: Graphical representation of the TVQ-VAE. Diagrams (a) and (b) illustrate the TVQ-VAE\u2019s graphical representation\nin both BoW and General forms, while diagram (c) presents an example of vector quantized embedding, conceptual word, and\noutput. Notably, the encoder network is fixed in our method.\nusing K-nearest embeddings \u03c11, ..., \u03c1k from B to represent\nquantized embedding \u03c1w for zw as:\ncw =\nX\nk\nck,\n\u03c1w = cw \u00b7 \u02c6\u03c1,\n(6)\nwhere the matrix \u02c6\u03c1 denotes the encoding matrix in Equa-\ntion (3). Using the expanded codebook cw and its embedding\n\u03c1w from equation (6), we define a expanded Bag-of-Word\nBw, the final form of the conceptual word, as follows:\nBw = {bw = (cw, \u03c1w)|w = 1, ..., Nw}.\n(7)\nWe note that the multi-hot embedding cw \u2208 RN\u03c1 is defined\nas N\u03c1 dimensional vector which is Nw >> N\u03c1. Theoreti-\ncally, the cardinality of Bw increases to combinatorial order\n\u0000N\u03c1\nK\n\u0001\n, where the number K called expansion value, denotes\nthe number of assigned embeddings for each input.\nGenerative Formulation for TVQ-VAE\nThis section proposes a generative topic model called TVQ-\nVAE analyzing the conceptual words Bw in Equation (7).\nAs illustrated in the graphical model in Figure 1, the\nTVQ-VAE model follows typical topic modeling structures\nformed by independent d = 1, ..., D documents, and each\ndocument d has independent Nw words cw \u2261 cdn \u2208 RNw,\nn = 1, ..., Nw. An output sample vd is matched to a doc-\nument d. TVQ-VAE provides various output forms for vd.\nFor the typical set-of-word style output, vd is defined as a set\nof word vd = {vd1, ..., vdNw} (Figure 1a), where the word\nvdn \u2208 RNw denotes the one-hot encoding of the original\nword wdn corresponding to cdn \u2208 RN\u03c1. Also, we can define\nvd as an image corresponding to the document d (Figure 1b).\nThe joint distribution of the overall random variable\n{\u03b8, z, v, c, \u03b2, \u03c1} is formulated as:\np(\u03b8, z, v, c, \u03b2, \u03c1)\n= p(\u03b8, \u03b2, \u03c1)\nD\nY\nd=1\np(vd|\u03b8d, \u03b2, \u03c1)\nNw\nY\nn=1\np(cdn|\u03b2zdn)p(zdn|\u03b8d),\n(8)\nwhere the distribution p(\u03b8, \u03b2, \u03c1) denotes the prior distribu-\ntion for each independent random variable. The configura-\ntion in Equation (8) is a typical formulation for the genera-\ntive topic model from (Blei, Ng, and Jordan 2003) or (Dieng,\nAlgorithm 1: Pseudo-code of TVQ-VAE generation\nRequire: Given an topics \u03b2 = {\u03b21, ..., \u03b2K},\n1: Sample or define \u03b8d.\n2: if document analysis then\n3:\nSample zdn \u223c p(z|\u03b8d): p(z|\u00b7) be the softmax.\n4:\nvdn \u223c p(v|\u03b1(\u03b2zdn \u00b7 \u02c6\u03c1)): p(v|\u00b7) be the softmax.\n5:\nRepeat n = 1, ..., Nw\n6: else if Image generation then\n7:\nc\u2032 \u223c AR(\u03b8 \u00b7 \u02c6\u03b2 \u00b7 \u02c6\u03c1).\n8:\nv = Dec(c\u2032, \u03c1), Dec(\u00b7) be VQ-VAE decoder.\n9: end if\nRuiz, and Blei 2020), each defines p(c|\u03b2zdn) and p(zdn|\u03b8d)\nto be categorical and softmax distribution. The main factor\nthat discriminates the previous topic models to TVQ-VAE\nhere is the generation of the output vd from p(vd|\u03b8d, \u03b2, \u03c1).\nAs mentioned above, TVQ-VAE supports various forms\nof generation for output vd. First, for the typical set-of-word\nstyle output vd = {vd1, ..., vdNw}, as in Figure 1a, the gen-\neration p(vd|\u03b8d, \u03b2, \u03c1) is defined as:\np(vd|\u03b8d, \u03b2, \u03c1) =\nNw\nY\nn=1\nK\nX\nzdn=1\np(vdn|\u03b1(\u03b2zdn \u00b7 \u02c6\u03c1))p(zdn|\u03b8d),\n(9)\nwhere a trainable fully connected layer \u03b1 \u2208 RNw\u00d7N\u03c1 con-\nnects the topic embedding \u03b2zdn \u00b7 \u02c6\u03c1 \u2208 RN\u03c1 to the origi-\nnal word dimension. Here, we define p(v|\u00b7) and p(zdn|\u00b7) as\nsoftmax distribution, which is a PoE implementation of the\ntopic model in (Srivastava and Sutton 2017). We note that it\nis possible to priorly marginalize out the indexing variable\nzdn in equation (9) by computing all the possible cases of\nsample drawn from p(zdn|\u03b8d).\nFor a more general case (Figure 1b), we assume the output\nvd is generated by a sequence of codebook cd = {cdn|n =\n1, ..Nw} and VQ-VAE decoder vd = Dec(cd, \u03c1; WD). To\ngenerate cd, we use AR prior par(\u00b7) including PixelCNN\nAlgorithm 2: Pseudo-code of TVQ-VAE training\nRequire: The batch of the input xd and the output vd.\n1: if document analysis then\n2:\nxd is the PLM vector from each Sentence.\n3:\nvd be the histogram of the original word.\n4: else if Image generation then\n5:\nxd \u2208 RH\u00d7W \u00d73 is an image.\n6: end if\n7: Initialize \u03b2, \u03b3p.\n8: (\u03c1, c) = Q(Enc(x; WE)). (In equation (3-4) and (6)).\n9: Calculate \u03b8 from q(\u03b8|\u03b3) (In equation (11)).\n10:\n(\u03b3m, log(\u03b3\u03c3)) = NN(c; W\u03b3).\n11:\n\u03b8d = Reparam(\u03b3m, log(\u03b3\u03c3)).\n12: if document analysis then\n13:\n\u03b2 = \u03b1(\u03b8d \u00b7 \u02c6\u03b2 \u00b7 \u02c6\u03c1).\n14: else if Image generation then\n15:\nc\u2032 = AR(\u03b8d \u00b7 \u02c6\u03b2 \u00b7 \u02c6\u03c1; War).\n16: end if\n17: lKL = DKL(log(\u03b3\u03c3), \u03b3m, \u03b3p).\n18: lc = c \u2217 log(softmax(\u03b8d \u00b7 \u02c6\u03b2)).\n19: if document analysis then\n20:\nlv = vd \u2217 log(\u03b2).\n21: else if Image generation then\n22:\nlv = CE(c, c\u2032).\n23: end if\n24: l = lKL + lc + lv.\nand Transformer (Esser, Rombach, and Ommer 2021), as:\np(vd = Dec(cd, \u03c1d)|\u03b8d, \u03b2, \u03c1) = P(cd|\u03b8d \u00b7 \u02c6\u03b2 \u00b7 \u02c6\u03c1)\n=\nN\nY\nn=1\npar(cdn|cdn\u22121, ..., cd1, \u03b8d \u00b7 \u02c6\u03b2 \u00b7 \u02c6\u03c1),\n(10)\nwhere the matrix \u02c6\u03b2 denotes \u02c6\u03b2 = [\u03b21, ..., \u03b2K]. We note that\nDec(\u00b7) is a deterministic function, and the AR prior coupled\nwith VQ-VAE decoding provides Negative Log Likelihood\n(NLL)-based convergence to the general data distributions.\nA detailed explanation of the generation algorithm is given\nin Algorithm (1).\nTraining TVQ-VAE\nFor the model inference, we leverage autoencoding Varia-\ntional Bayes (VB) (Kingma and Welling 2013) inference to\nthe distribution in Equation (8) in a manner akin to (Srivas-\ntava and Sutton 2017; Dieng, Ruiz, and Blei 2020). In these\nmethods, VB inference defines the variational distribution\nq(\u03b8, z|\u03b3, \u03d5) that can break the connection between \u03b8 and z,\nas q(\u03b8, z|\u03b3, \u03d5) = q(\u03b8|\u03b3)q(z|\u03d5), of the posterior distribution\np(\u03b8, z|c, v, \u03b2, \u03c1). By the VB, the ELBO here is defined as:\nL(\u03b3) = \u2212 DKL[q(\u03b8|\u03b3)||p(\u03b8)]\n+ Eq(\u03b8|\u03b3)[log p(c, v|z, \u03b8, \u03c1, \u03b2)],\n(11)\nwhere we pre-marginalize out the z, similar to equation (9).\nIn the equation, the first term measures the Kullbeck-Leibler\n(KL) distance between the variational posterior over the\nreal posterior distribution, called KL term, and the second\nterm denotes the reconstruction term. Followed by (Dieng,\nRuiz, and Blei 2020), we define the variational parameter\n\u03b3 = NN(c; W\u03b3) as a neural network (NN) function of the\ninput set-of-word c, and \u03b8 is drawn by a reparameterization\ntechnique given the variable \u03b3.\nDifferent from the previous methods (Srivastava and Sut-\nton 2017; Dieng, Ruiz, and Blei 2020), we also consider the\nreconstruction of the output samples v, as:\nEq\u03b3(\u03b8)[log p(c, v|z, \u03b8, \u03c1, \u03b2)] =\nEq\u03b3(\u03b8)[log p(c|z, \u03b8, \u03c1, \u03b2)] + Eq\u03b3(\u03b8)[log p(v|z, \u03b8, \u03c1, \u03b2)]. (12)\nHere, c and v are conditionally independent given \u03b8, as in\nFigure 1b. Therefore, the TVQ-VAE model has three loss\nterms corresponding to KL and the reconstruction terms:\nltot = lKL(\u03b8) + lrec(c) + lrec(v).\n(13)\nTraining Implementation.\nSince the KL divergence cal-\nculation in equation (13), which is lKL(\u03b8), and the first term\nin equation (12), which is lrec(c), is equivalent to the VB\ncalculation of the classical topic model, we employ the Prod-\nLDA setting in (Srivastava and Sutton 2017) to those terms.\nFor the last reconstruction term lrec(v), we can use the gen-\nerative distributions defined in Equation (9) for a set-of-\nword style document vd or autoregressive generation given\nPixelCNN prior as in Equation (10). We note that for the first\ncase, the reconstruction loss term has equivalent to those of\nthe reconstruction term for c, and for the second case, the\nloss term is equivalent to the AR loss minimizing NLL for\nboth PixelCNN and Transformer. A detailed training process\nis given in Algorithm (2).\nThe overall trainable parameters for the topic modeling\nin the process are W\u03b3 for the variational distribution \u03b3, the\ntopic variable \u03b2. For the sample generation, the feedforward\nnetwork \u03b1(\u00b7) and AR parameter War are also trained for\ndocument analysis and image generation cases. It is possible\nto train VQ-VAE encoder WE as well, but we fix the VQ-\nVAE parameters considering that many studies utilize pre-\ntrained VQ-VAE without scratch training.\nRelated Works\nSince the initiative generative topic modeling from (Blei,\nNg, and Jordan 2003), many subsequent probabilistic meth-\nods (Teh et al. 2004; Paisley et al. 2014) have been proposed.\nAfter the proposal of autoencoding variational Bayes, a.k.a.,\nvariational autoencoder (VAE), from (Kingma and Welling\n2013), neural-network-based topic models (NTMs) (Miao,\nYu, and Blunsom 2016; Srivastava and Sutton 2017; Zhang\net al. 2018; Nan et al. 2019) have been proposed. To reflect\nthe discrete nature of the topic, (Gupta and Zhang 2021,\n2023) introduces discrete inference of the topics by VQ-\nVAE (Van Den Oord, Vinyals et al. 2017). Unlike the above\nmethods that treat each Vector Quantization (VQ) embed-\nding as a distinct topic representation, our method leverages\nboth the VQ embedding and its corresponding codebook as\nan expanded word feature, enabling extraction of a variable\nnumber of topics decoupled from the VQ embedding count.\nTopic models with Embedding.\nPCAE (Tu et al. 2023)\nalso proposes a flexible generation of the output by VAE,\nwhich shares a similar idea, and we focus on VQ embed-\ndings as well. Attempts to include word embeddings, mostly\nusing GloVe (Pennington, Socher, and Manning 2014), into\ngenerative (Petterson et al. 2010; Dieng, Ruiz, and Blei\n2020; Duan et al. 2021) or non-generative (Wang et al. 2022;\nXu et al. 2022; Tu et al. 2023) topic modeling frameworks\nhave also demonstrated successfully topic modeling per-\nformance. Moreover, utilizing pre-trained language models\n(PLMs) such as BERT (Devlin et al. 2018), RoBERTa (Liu\net al. 2019), and XLNet (Yang et al. 2019) has emerged\nas a new trend in mining topic models. Many recent stud-\nies have enhanced the modeling performance by observ-\ning the relation between K-means clusters and topic embed-\ndings (Sia, Dalmia, and Mielke 2020). These studies require\npost-training steps including TF-IDF (Grootendorst 2022)\nor modifying of PLM embeddings to lie in a spherical em-\nbedding space through autoencoding (Meng et al. 2022) to\nmitigate the curse-of-dimensionality. Here, our method re-\ndemonstrates the possibility of handling discretized PLM in-\nformation in a generative manner without post-processing.\nVector\nQuantized\nLatent\nEmbedding.\nSince\n(Van\nDen Oord, Vinyals et al. 2017) proposes a discretization\nmethod for latent embedding incorporated with the varia-\ntional autoencoding framework, this quantization technique\nhas become an important block for generation, especially\nfor visual generation (Razavi, Van den Oord, and Vinyals\n2019). Following the study, subsequent studies (Peng et al.\n2021; Esser, Rombach, and Ommer 2021; Yu et al. 2021;\nHu et al. 2022) including text to image multi-modal con-\nnection (Gu et al. 2022; Tang et al. 2022; Esser et al. 2021)\nincorporated with autoregressive generation. In this line of\nstudies, we demonstrate that our method can extract topic\ncontext from VQ embeddings encapsulating visual informa-\ntion, and generate reasonable samples, simultaneously.\nEmpirical Analysis\nWe analyze the performance of the TVQ-VAE with two ap-\nplications: document analysis, which is a classical problem\nin topic modeling, and image generation to show the exam-\nple of a much more general form of document generation.\nDocument Analysis\nDataset.\nWe conduct experiments on two datasets: 20\nNewsgroups (20NG) (Lang 1995), the New York Times-\nannotated corpus (NYT) (Sandhaus 2008), as following the\nexperiments of (Dieng, Ruiz, and Blei 2020). We present\nthe detailed statistics of the datasets in Table 1. While docu-\nments in 20NG consist of about 46 words on average, we\nnote that NYT is a much larger dataset compared to the\n20NG dataset, consisting of 32K documents with 328 words\nper document on average.\nBaseline Methods.\nTo facilitate a comprehensive com-\nparison, we select four representative topic models to en-\ncompass BoW-based, embedding-based, neural network-\nignored, and neural-network employed approaches as well\n# Doc.\n# Vocab.\n# Total words\n# Labels\n20NG\n16.3K\n1.60K\n0.75M\n20\nNYT\n32.0K\n28.7K\n10.5M\n10 (9)\nTable 1: Statistics of datasets. For 20NG, we follow the OC-\nTIS setting from (Terragni et al. 2021). NYT dataset has two\ncategories corresponding to locations (10) and topics (9).\nas generative and non-generative models, as: (1) LDA (Blei,\nNg, and Jordan 2003) - a textbook method of BoW-based\ngenerative topic model, (2) ProdLDA (Srivastava and Sut-\nton 2017) - a BoW-based generative neural topic model\n(NTM) (3) ETM (Dieng, Ruiz, and Blei 2020) - a gen-\nerative NTM considering Word2Vec embedding (Petterson\net al. 2010) as well, (4) BerTopic (Grootendorst 2022) - a\nnon-generative PLM-based topic model utilizing sentence-\nBert (Reimers and Gurevych 2019) information. We use the\nimplementation from OCTIS (Terragni et al. 2021) for LDA,\nProdLDA, and ETM. For ETM, we use Google\u2019s pre-trained\nWord2Vec as its embedding vector. For BerTopic, we use the\nofficial author\u2019s implementation. For TVQ-VAE, we set the\nembedding number and expansion k to 300 and 5.\nImplementation Detail.\nTo transform words in sentences\ninto vectorized form, we employ Sentence-Bert (Reimers\nand Gurevych 2019), which converts each word to a 768-\ndimensional vector x. We use the autoencoder component\nof VQ-VAE from (Meng et al. 2022). The encoder and de-\ncoder of the VQ-VAE are composed of a sequence of fully-\nconnected (FC) layers followed by ReLU activation, having\nintermediate layer dimensions to [500, 500, 1000, 100] and\n[100, 1000, 500, 500]. Consequently, we compress the input\nvectors to a 100 dimensional latent vector.\nThe NN(c) of the Algorithm 2, which draws \u03b8, of\nthe main manuscript are implemented using the inference\nnetwork architecture of ProdLDA (Srivastava and Sutton\n2017), as implemented in OCTIS (Terragni et al. 2021). The\nNN(c) is implemented by three consecutive linear layers\nfollowed by tangent hyperbolic activation, which has latent\ndimensions to [100, 100]. We pretrained the VQ-VAE archi-\ntectures for 20 epochs and trained the remaining parts of\nTVQ-VAE for 200 epochs with by optimizer (Kingma and\nBa 2014) with a learning rate of 5 \u00d7 10\u22123. The batch size\nwas set to 256 for both training and pretraining.\nEvaluation Metric.\nWe evaluate the model\u2019s performance\nin terms of topic quality (TQ) and document representation,\nfollowing the established evaluation setup for topic mod-\nels. TQ is evaluated based on Topic Coherence (TC) and\nTopic Diversity (TD). TC is estimated by using Normalized\nPoint-wise Mutual Information (NPMI) (Aletras and Steven-\nson 2013), quantifying the semantic coherence of the main\nwords within each topic. NPMI scores range from \u22121 to 1,\nwith higher values indicating better interpretability. TD mea-\nsures word diversity by computing the unique word numbers\namong the top 25 words across all topics (Dieng, Ruiz, and\nBlei 2020). TD scores range from 0 to 1, with higher values\nindicating richer word diversity. TQ is defined as the multi-\nLDA\nPLDA\nETM\nBerTopic\nTVQ-VAE\n0.1\n0.15\nNPMI\n(a) 20NG-NPMI.\nLDA\nPLDA\nETM\nBerTopic\nTVQ-VAE\n0.5\n0.75\n1.0\nDiversity\n(b) 20NG-Diversity.\nLDA\nPLDA\nETM\nBerTopic\nTVQ-VAE\n0.05\n0.1\n0.15\nTQ\n(c) 20NG-TQ.\nLDA\nPLDA\nETM\nBerTopic\nTVQ-VAE\n0.1\n0.15\nNPMI\n(d) NYT-NPMI.\nLDA\nPLDA\nETM\nBerTopic\nTVQ-VAE\n0.75\n1.0\nDiversity\n(e) NYT-Diversity.\nLDA\nPLDA\nETM\nBerTopic\nTVQ-VAE\n0.1\n0.15\nTQ\n(f) NYT-TQ.\nFigure 2: The quantitative evaluation of topic quality over two datasets: 20NG and NYT. The baseline methods are listed from\nLeft to right: LDA, ProdLDA (PLDA), ETM, BerTopic, and TVQ-VAE.\nLDA\nPLDA\nETM\nTVQ-VAE (W)\nBerTopic\nTopClus\nTVQ-VAE\n20NG\n(0.309/0.231)\n(0.276/0.184)\n(0.331/0.207)\n(0.222/0.249)\n(0.133/0.178)\n(0.168/0.221)\n(0.210/0.242)\nNYT\n(0.144/0.399)\n(0.107/0.367)\n(0.094/0.346)\n(0.176/0.489)\n(0.155/0.481)\n(0.137/0.461)\n(0.184/0.510)\nTable 2: Evaluation on Km-NMI and Km-Purity on 20NG and NYT datasets: (Km-NMI / Km-Purity). We note that BerTopic,\nTopClus and TVQ-VAE both use PLM (Reimers and Gurevych 2019). TVQ-VAE (W) uses Word2Vec instead of the PLM.\nplication of the TC, measured by NPMI, and TD values.\nFurthermore, to measure document representation, we\nreport the purity and Normalized Mutual Information\n(NMI) (Schutze, Manning, and Raghavan 2008). Following\n(Xu et al. 2022), we cluster the \u03b8d of every document d and\nmeasure the purity and NMI termed as Km-NMI and Km-\nPurity. Both values range from 0 to 1, and the higher values\nindicate better performance.\nTopic Quality Evaluation.\nWe present the evaluation re-\nsults for topic quality (TQ), as depicted in Figure 2. From\nthe evaluation settings outlined in (Grootendorst 2022), we\ninfer a range of 10 to 50 topics with a step size of 10 and\nmeasure their TC and TD to evaluate TQ.\nFirst, we evaluate the performance of TVQ-VAE on the\n20NG dataset, which is widely used in the field of topic\nmodeling. Notably, the TVQ-VAE demonstrates either com-\nparable or superior performance compared to other base-\nlines in terms of TQ measures. It is worth mentioning that\nthe 20NG dataset has a small vocabulary size, which stands\nat 1.6K. This scale is considerably smaller considering the\nnumber of TVQ-VAE codebook sizes. These results repre-\nsent that TVQ-VAE effectively extracts topic information\nfor documents with limited size, where BoW-based topic\nmodels like ProdLDA have exhibited impressive success.\nIn the NYT dataset, characterized by a significantly larger\nvocabulary to 20NG, the TVQ-VAE model achieves com-\npetitive topic quality when utilizing only 300 virtual code-\nbooks, which accounts for less than 1% of the original vo-\ncabulary size. Among the baselines, BerTopic stands out\nas it demonstrates exceptional performance, particularly in\nterms of NPMI, deviating from the results observed in the\n20NG dataset. The result verifies BerTopic\u2019s claim that\nPLM-based methods are scalable for larger vocabulary.\nFigure 3 presents the ablation study conducted with vary-\ning the number of codebooks by {100, 200, 300} and the ex-\npansion values by k = {1, 3, 5}. In the case of the 20NG\ndataset, the evaluation results indicate minimal performance\ndifferences across all settings. This presents that the choice\nof embedding and expansion numbers does not necessar-\nily guarantee performance enhancements. This may happen\ndue to the relatively small vocabulary size of 20NG, More-\nover, exceeding certain bounds for the number of codebooks\nand expansion appears to capture no additional information\nfrom the original dataset. Conversely, the evaluation results\nobtained from the NYT dataset support our analysis. Here,\nthe performance improves with larger codebook sizes and\nexpansion numbers, given the vocabulary size of approxi-\nmately 20 times that of the 20NG.\nDocument Representation Evaluation.\nTable 2 presents\nthe km-NMI and km-Purity scores for each topic model. In\nthe 20NG dataset, characterized by a relatively smaller vo-\ncabulary size, the previous BoW-based method exhibited su-\nperior NMI scores. However, in the case of the NYT dataset,\nPLM-based methods like BerTopic and TVQ-VAE demon-\nstrated higher performance. We additionally evaluate Top-\nClus (Meng et al. 2022) as a variant of the PLM-based topic\nmodel. These findings suggest that our TVQ-VAE model ex-\nhibits robust document representation capabilities, particu-\nlarly as the vocabulary size expands.\nAdditionally, when employing Word2Vec with TVQ-\nVAE, we observed performance on par with that of PLM-\nbased TVQ-VAE. In fact, in the case of the 20NG dataset,\nWord2Vec-based TVQ-VAE even exhibited superior perfor-\nmance. We hypothesize that this outcome can be attributed\nto the comparatively reduced number of words and vocabu-\nlary in the 20NG dataset when compared to NYT. This ob-\n100\n200\n300\n0.05\n0.1\n0.15\nNPMI\nk=5\nk=3\nk=1\n(a) 20NG-NPMI.\n100\n200\n300\n0.25\n0.5\n0.75\nDiversity\nk=5\nk=3\nk=1\n(b) 20NG-Diversity.\n100\n200\n300\n0.05\n0.1\n0.15\nTQ\nk=5\nk=3\nk=1\n(c) 20NG-TQ.\n100\n200\n300\n0.05\n0.1\n0.15\nnyt\nk=5\nk=3\nk=1\n(d) NYT-NPMI.\n100\n200\n300\n0.25\n0.5\n0.75\n1.0\nDiversity\nk=5\nk=3\nk=1\n(e) NYT-Diversity.\n100\n200\n300\n0.05\n0.1\n0.15\nTQ\nk=5\nk=3\nk=1\n(f) NYT-TQ.\nFigure 3: Demonstration of the TQ over various numbers of codebook {100, 200, 300} and expansion k = {1, 3, 5}.\n20NG\nNYT\nmuslim, turkish, armenian, arab, country\ngubernatorial, campaign, democratic, race, election\narchive, server, entry, mail, resource\njapan, Japanese, tokyo, hokkaido, fuji\ngraphic, internet, database, programming, guide\nspacecraft, orbit, module, capsule, endeavour\npresident, decision, meeting, yesterday, administration\nadministration, nato, pluto, washington, nuclear\ngun, violent, accident, crime, risk\nmilitary, american, pentagon, command, force\nvoltage, circuit, output, wire, frequency\nschool, kindergarten, mathematics, education, elementary\ngraphic, internet, database, programming, guide\nbank, investment, firm, supervisory, stock\nplayer, average, career, league, pitcher\neuropean, monetary, germany, west, union\nshipping, sale, sell, brand, offer\nsenate, legislation, republican, procedural, congress\nexistence, belief, argument, atheist, conclusion\nwaterfront, marina, park, center, downtown\ngay, sex, homosexual, moral, sexual\ngrowth, percent, quarter, year, economy\nTable 3: Topic Visualization of TVQ-VAE. We demonstrate top 5 words for each topic.\nCodebook 106\nmoney, profit, guarantee, hope, financial, ...\nCodebook 176\nlife, today, time, economy, bank, ...\nCodebook 207\ntwo, zero, sixth, asset, analyst, ...\nTable 4: Conceptual-word to word mapping in NYT dataset.\nservation aligns with a similar trend noticed in ETM, which\nalso utilizes Word2Vec.\nWe also note that PLMs like BerTopic excel on larger\ndatasets such as NYT, but not on smaller ones like 20NG,\nsuggesting that PLMs\u2019 breadth may not translate to depth\nin constrained datasets. In the smaller datasets, the model\u2019s\nbroad lexical coverage may result in singular categories with\nhigh purity but restricted breadth, thereby diminishing Km-\nNMI. TopClus results corroborate this, underscoring the in-\nfluence of the data set size on the model efficacy.\nTopic and Codebook Demonstration.\nTable 3 provides a\nvisual summary of the top 5 representative words associated\nwith each topic in both the 20NG and NYT datasets. It is\nevident from this table that the words within each topic ex-\nhibit clustering behavior, indicating a shared semantic simi-\nlarity among them. Also, we show that the conceptual code-\nbook functions as a semantic cluster, aggregating words with\nhigher semantic proximity just before topic-level clustering.\nThe example showcasing the collection of words for each\ncodebook illustrates this tendency, in Table 4.\nImage Generation\nDataset.\nTo demonstrate that TVQ-VAE can mine topic\ninformation from the visual codebooks from VQ-VAE, we\nfirst tested our method into two image datasets: CIFAR-\n10 (Krizhevsky, Hinton et al. 2009) and CelebA (Liu et al.\n2015) typically used for supervised and unsupervised im-\nage generation, respectively. While CIFAR-10 contains 60K\n32x32 dimensional images with 10 class objects, CelebA\nconsists of about 200K of annotated facial images. We\ncenter-crop and resize the images to have 64x64 dimen-\nsion. We convert the images to a sequence consisting of 64\nand 256 codebooks, respectively, i.e., each image is repre-\nsented as a document having 64 and 256 words. Also, to\nvalidate the proposed method TVQ-VAE into larger resolu-\ntion image, we used FacesHQ (Esser, Rombach, and Om-\nmer 2021) dataset, which includes FFHQ (Karras, Laine,\nand Aila 2019) and CelebaHQ (Karras et al. 2017) datasets.\nBaseline Methods.\nSince the general form of document\ngeneration conditioned to a topic is a newly proposed task,\n(a) Topic visualizations on CelebA dataset.\n(b) Topic visualizations on CIFAR-10 dataset\n(c) Reference-based generation on CelebA dataset.\n(d) Reference-based generation on CIFAR-10 dataset\nFigure 4: Illustrations of visualized topics and reference-based generation for topic number K of 100, from TVQ-VAE (P).\nit is difficult to directly compare to the previous methods.\nQuantitatively, therefore, we compare the TVQ-VAE to the\nbaseline VQ-VAE generation guided by PixelCNN prior,\nTVQ-VAE (P), which is a typical method of auto-regressive\ngeneration. All the network architecture of the VQ-VAE\nand PixelCNN is equivalent to those in TVQ-VAE. Also,\nwe apply the TVQ-VAE concept into (Esser, Rombach, and\nOmmer 2021), which is a representative AR method using\nTransformer and VQ-codebooks, abbreviated as TVQ-VAE\n(T) and test with FacesHQ dataset.\nEvaluation.\nRegarding the quantitative evaluation, we uti-\nlize the Negative Log-Likelihood (NLL) metric on the test\nset, a widely adopted measure in the field of auto-regressive\nimage generation. A lower NLL value means better coverage\nof the dataset. For the qualitative evaluation, we demonstrate\nthe generated images corresponding to each topic, illustrat-\ning the topic\u2019s ability to serve as a semantic basis in shaping\nthe generated data. Furthermore, we show image generation\nexamples conditioned on a reference image by leveraging its\ntopic information expressed as \u03b8.\nImplementation Detail.\nWe employed the TVQ-VAE (P)\nframework, utilizing VQ-VAE and PixelCNN architectures\nfrom a well-known PyTorch repository1. The VQ-VAE\nmodel integrates 64 and 256 codebooks for 32x32 and 64x64\nimage resolutions, respectively. Its encoder features four\nconvolutional (Conv) blocks: two combining Conv, batch\nnormalization (BN), and ReLU activation, and two residual\nblocks with Conv structures outputting dimensions of 256.\nThe latent vector dimensions are likewise set to 256. The de-\ncoder comprises two residual and two ConvTranspose layers\n1https://github.com/ritheshkumar95/pytorch-vqvae.git\nwith intermediate channels to 256, using ReLU activations.\nFor topic information extraction, we use an inference net-\nwork NN(c), equivalent to that in Document analysis. We\nconditional embedding of the GatedCNN architecture to get\ntopic embedding (\u03b8d \u00b7 \u02c6\u03b2 \u00b7 \u02c6\u03c1) instead of the original class-\nconditional embedding. For pretraining the VQ-VAE, we\nemploy the Adam optimizer for 100 epochs with a learning\nrate of 2 \u00d7 10\u22124. Similarly, in TVQ-VAE(P), the topic mod-\neling and PixelCNN prior are trained for 100 epochs using\nan identical optimizer setup and a batch size of 128.\nFurthermore, the proposed TVQ-VAE was extended to\nTVQ-VAE (T) by applying a representative AR model from\n(Esser, Rombach, and Ommer 2021), using Transformer and\nVQ-codebooks from VQGAN, to generate high-resolution\nimages as the topic-driven documents. TVQ-VAE (T) fa-\ncilitates codebook generation for context-rich visual parts\nthrough convolutional layers and enables auto-regressive\nprediction of codebook indices using Transformer. Topic in-\nformation extraction is performed through an inference net-\nwork in the same manner as previously described.\nTo reflect topic information to the Transformer, each\ncodebook token was augmented with the topic embedding\n(\u03b8d \u00b7 \u02c6\u03b2 \u00b7 \u02c6\u03c1) to integrate topic information. This augmented\nembedding becomes an additional input for Transform-\ners, minGPT architecture from Karpathy2. We use the pre-\ntrained VQGAN codebook for the FacesHQ dataset from the\nofficial repository of (Esser et al. 2021).\nSpecifically, we use the topic embedding for two pur-\nposes, one for augmented token and the other for bias for\nthe input of the transformer block, consisting of the causal\n2https://github.com/karpathy/minGPT\nFigure 5: Illustrations of reference-based generation applying TVQ-VAE (T) for topic number K of 100.\nFigure 6: Visualization of topic embedding by t-SNE, from\nTVQ-VAE (P) for CIFAR-10 generation, 512 codebooks.\nself-attention layer. As the augmented token, we repeatedly\nassign a 256 number of topic tokens before the image tokens,\nwhere the number is 256, also. Furthermore, for each trans-\nformer block output that has a 512 token length, we add the\ntopic tokens as a bias for the latter 256 tokens, which is the\npredicted image token of the block. We repeatedly expand\nthe topic embedding dimension to 1024 from the original\n256, to align the dimension size to those of the image token.\nQuantitative Evaluation.\nTable 5 presents the NLL eval-\nuation results for the CelebA and CIFAR-10 datasets. We\nconjecture that the extraction of the topic variables \u03b8 and\n\u03b2 helps the easier generation of the samples, quantified by\nlower NLL, since the topic variables already extract the hid-\nden structures of the dataset which is originally the role of\nthe generation module. The evaluations conducted on the\nCelebA and CIFAR-10 datasets yield contrasting outcomes.\nSpecifically, in the case of CelebA, the unsupervised base-\nline exhibits a lower NLL. Conversely, for CIFAR-10, the\nNLL demonstrates a linear decrease with an increasing num-\nber of topics, surpassing the NLL values of both unsuper-\nvised and class-label supervised generation methods.\nThe complexity of the two datasets provides insights\ninto the observed patterns. The CelebA dataset comprises\naligned facial images, and the preprocessing step involves\ncenter-cropping the facial region to produce cropped im-\nages that specifically include the eyes, nose, and mouth. This\npreprocessing step effectively reduces the dataset\u2019s com-\nplexity. In contrast, the CIFAR-10 dataset consists of un-\nU\n10\n20\n50\n100\nS\nCelebA\n3.10\n3.15\n3.14\n3.14\n3.13\n-\nCIFAR-10\n3.29\n3.27\n3.25\n3.22\n3.20\n3.29\nTable 5: NLL evaluation on CelebA and CIFAR-10 dataset.\nThe terms \u2018U\u2019 and \u2018S\u2019 denote unsupervised and supervised\ngeneration from the VQ-VAE integrated with PixelCNN\nprior. The numbers {10, 20, 50, 100} denote the number of\ntopics assigned to TVQ-VAE.\naligned images spanning ten distinct categories, resulting in\nan increased level of complexity. Previous evaluations from\nthe baseline methods (Van Den Oord, Kalchbrenner, and\nKavukcuoglu 2016; Van Den Oord, Vinyals et al. 2017) have\nsimilarly highlighted the challenging nature of NLL-based\ngeneration for CIFAR-10. Therefore, we contend that the\nevaluation in Table 5 supports our conjecture that topic ex-\ntraction can enhance the model\u2019s generation capabilities for\ncomplicated datasets. especially for complicated datasets.\nQualitative Evaluation.\nFigure 4 shows visual examples\nof topics as well as generated samples obtained from refer-\nence images from TVQ-VAE (P). The visualized topic ex-\namples in Figures 4a and 4b, arranged in an 8 \u00d7 8 grid, il-\nlustrate the generated samples obtained by fixing \u03b8 in Equa-\ntion (10) to a one-hot vector corresponding to the topic in-\ndices. Subsequently, the PixelCNN prior ppix(\u00b7|\u03b8 \u00b7 \u02c6\u03b2 \u00b7 \u02c6\u03c1)\ngenerates the codebook sequences by an auto-regressive\nscheme, conditioned on each k-th topic vector \u03c1(\u03b2) = \u03b2k \u00b7 \u02c6\u03c1.\nThe topic visualization shows that each topic exhibits dis-\ntinct features, such as color, shape, and contrast.\nFurthermore, we demonstrate the generation ability of the\nTVQ-VAE (P) by first, extracting the topic distribution \u03b8d of\nthe image xd, and subsequently generating new images from\nthe extracted \u03b8d. In this case, we expect the newly generated\nimages to share similar semantics to the original image x,\nwhich is called reference-based generation. As shown in\nFigures 4c and 4d, we generate images similar to the refer-\nence image, which is on the top-left corners each. The visual\nillustration for both CIFAR-10 and CelebA clearly demon-\nstrates that TVQ-VAE (P) effectively captures the distinctive\nattributes of reference images and generates semantically\nsimilar samples by leveraging the integrated topical basis.\nFigure 5 demonstrates the sample generation examples\nwith higher resolution, 256, from the TVQ-VAE (T) trained\nfrom FacesHQ dataset, with the equivalent format to the\nreference-based generation in Figure 4. Both cases show that\nthe topic embedding from each reference image captures\nessential features of the image for generating semantically\nclose images, and the proposed TVQ-VAE method can be\neffectively applied to two different AR models: PixelCNN\n(P) and Transformer (T).\nVisualization of Embedding Space.\nFor more demon-\nstration of the proposed concepts, we present t-SNE (Van der\nMaaten and Hinton 2008) plot for topic embedding space,\nin Figure 6. Each data point on the plot corresponds to the\ntopic embedding of generated images derived from identi-\ncal reference sources. This serves as a visual representation\nof the capability of our TVQ-VAE to produce images that\nexhibit semantic proximity to their respective reference im-\nages. Furthermore, it is evident that the generated images\nform distinct clusters within the embedding space.\nConclusion and Future Remark\nWe introduced TVQ-VAE, a novel generative topic model\nthat utilizes discretized embeddings and codebooks from\nVQ-VAE, incorporating pre-trained information like PLM.\nThrough comprehensive empirical analysis, we demon-\nstrated the efficacy of TVQ-VAE in extracting topical infor-\nmation from a limited number of embeddings, enabling di-\nverse probabilistic document generation from Bag-of-Words\n(BoW) style to autoregressively generated images. Experi-\nmental findings indicate that TVQ-VAE achieves compara-\nble performance to state-of-the-art topic models while show-\ncasing the potential for a more generalized topic-guided gen-\neration. Future research can explore the extension of this ap-\nproach to recent developments in multi-modal generation.\nAcknowledgements\nWe thank Jiyoon Lee3 for the helpful discussion, experi-\nments, and developments for the final published version.\nThis research was supported by the Chung-Ang Univer-\nsity Research Grants in 2023 and the Institute of Infor-\nmation & communications Technology Planning & Evalu-\nation (IITP) grant funded by the Korean government(MSIT)\n(2021-0-01341, Artificial Intelligence Graduate School Pro-\ngram (Chung-Ang Univ.)).\nReferences\nAletras, N.; and Stevenson, M. 2013. Evaluating topic co-\nherence using distributional semantics. In Proceedings of\nthe 10th international conference on computational seman-\ntics (IWCS 2013)\u2013Long Papers, 13\u201322.\nBlei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent dirich-\nlet allocation. Journal of machine Learning research, 3(Jan):\n993\u20131022.\nCasella, G.; and George, E. I. 1992. Explaining the Gibbs\nsampler. The American Statistician, 46(3): 167\u2013174.\n3Independent researcher (jiyoon.lee52@gmail.com). The co-\nresearch was conducted during her internship at ImageVision,\nNAVER Cloud, in 2023.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDieng, A. B.; Ruiz, F. J.; and Blei, D. M. 2020. Topic mod-\neling in embedding spaces. Transactions of the Association\nfor Computational Linguistics, 8: 439\u2013453.\nDuan, Z.; Wang, D.; Chen, B.; Wang, C.; Chen, W.; Li, Y.;\nRen, J.; and Zhou, M. 2021. Sawtooth factorial topic em-\nbeddings guided gamma belief network.\nIn International\nConference on Machine Learning, 2903\u20132913. PMLR.\nEsser, P.; Rombach, R.; Blattmann, A.; and Ommer, B. 2021.\nImagebart: Bidirectional context with multinomial diffusion\nfor autoregressive image synthesis. Advances in Neural In-\nformation Processing Systems, 34: 3518\u20133532.\nEsser, P.; Rombach, R.; and Ommer, B. 2021. Taming trans-\nformers for high-resolution image synthesis. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, 12873\u201312883.\nGrootendorst, M. 2022.\nBERTopic: Neural topic model-\ning with a class-based TF-IDF procedure. arXiv preprint\narXiv:2203.05794.\nGu, S.; Chen, D.; Bao, J.; Wen, F.; Zhang, B.; Chen, D.;\nYuan, L.; and Guo, B. 2022.\nVector quantized diffusion\nmodel for text-to-image synthesis.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 10696\u201310706.\nGupta, A.; and Zhang, Z. 2021. Vector-quantization-based\ntopic modeling. ACM Transactions on Intelligent Systems\nand Technology (TIST), 12(3): 1\u201330.\nGupta, A.; and Zhang, Z. 2023. Neural Topic Modeling via\nDiscrete Variational Inference. ACM Transactions on Intel-\nligent Systems and Technology, 14(2): 1\u201333.\nHu, M.; Wang, Y.; Cham, T.-J.; Yang, J.; and Suganthan,\nP. N. 2022. Global context with discrete diffusion in vector\nquantised modelling for image generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 11502\u201311511.\nKarras, T.; Aila, T.; Laine, S.; and Lehtinen, J. 2017. Pro-\ngressive growing of gans for improved quality, stability, and\nvariation. arXiv preprint arXiv:1710.10196.\nKarras, T.; Laine, S.; and Aila, T. 2019. A style-based gen-\nerator architecture for generative adversarial networks. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, 4401\u20134410.\nKingma, D. P.; and Ba, J. 2014.\nAdam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKingma, D. P.; and Welling, M. 2013. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114.\nKrizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple\nlayers of features from tiny images.\nLang, K. 1995. Newsweeder: Learning to filter netnews. In\nMachine learning proceedings 1995, 331\u2013339. Elsevier.\nLiu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V.\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLiu, Z.; Luo, P.; Wang, X.; and Tang, X. 2015. Deep Learn-\ning Face Attributes in the Wild. In Proceedings of Interna-\ntional Conference on Computer Vision (ICCV).\nMeng, Y.; Zhang, Y.; Huang, J.; Zhang, Y.; and Han, J. 2022.\nTopic discovery via latent space clustering of pretrained lan-\nguage model representations. In Proceedings of the ACM\nWeb Conference 2022, 3143\u20133152.\nMiao, Y.; Yu, L.; and Blunsom, P. 2016. Neural variational\ninference for text processing. In International conference on\nmachine learning, 1727\u20131736. PMLR.\nNan, F.; Ding, R.; Nallapati, R.; and Xiang, B. 2019. Topic\nmodeling with wasserstein autoencoders.\narXiv preprint\narXiv:1907.12374.\nPaisley, J.; Wang, C.; Blei, D. M.; and Jordan, M. I. 2014.\nNested hierarchical Dirichlet processes. IEEE transactions\non pattern analysis and machine intelligence, 37(2): 256\u2013\n270.\nPeng, J.; Liu, D.; Xu, S.; and Li, H. 2021. Generating diverse\nstructure for image inpainting with hierarchical VQ-VAE.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 10775\u201310784.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural lan-\nguage processing (EMNLP), 1532\u20131543.\nPetterson, J.; Buntine, W.; Narayanamurthy, S.; Caetano, T.;\nand Smola, A. 2010. Word features for latent dirichlet allo-\ncation. Advances in Neural Information Processing Systems,\n23.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748\u20138763. PMLR.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-to-\nimage generation. In International Conference on Machine\nLearning, 8821\u20138831. PMLR.\nRazavi, A.; Van den Oord, A.; and Vinyals, O. 2019. Gener-\nating diverse high-fidelity images with vq-vae-2. Advances\nin neural information processing systems, 32.\nReimers, N.; and Gurevych, I. 2019. Sentence-bert: Sen-\ntence embeddings using siamese bert-networks.\narXiv\npreprint arXiv:1908.10084.\nSandhaus, E. 2008. The new york times annotated corpus.\nLinguistic Data Consortium, Philadelphia, 6(12): e26752.\nSchutze, H.; Manning, C. D.; and Raghavan, P. 2008. In-\ntroduction to information retrieval. Cambridge University\nPress.\nSia, S.; Dalmia, A.; and Mielke, S. J. 2020. Tired of topic\nmodels? clusters of pretrained word embeddings make for\nfast and good topics too! arXiv preprint arXiv:2004.14914.\nSrivastava, A.; and Sutton, C. 2017.\nAutoencoding\nvariational inference for topic models.\narXiv preprint\narXiv:1703.01488.\nTang, Z.; Gu, S.; Bao, J.; Chen, D.; and Wen, F. 2022. Im-\nproved vector quantized diffusion models. arXiv preprint\narXiv:2205.16007.\nTeh, Y.; Jordan, M.; Beal, M.; and Blei, D. 2004. Sharing\nclusters among related groups: Hierarchical Dirichlet pro-\ncesses. Advances in neural information processing systems,\n17.\nTerragni, S.; Fersini, E.; Galuzzi, B. G.; Tropeano, P.; and\nCandelieri, A. 2021. Octis: comparing and optimizing topic\nmodels is simple! In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Computational\nLinguistics: System Demonstrations, 263\u2013270.\nTu, H.; Yang, Z.; Yang, J.; Zhou, L.; and Huang, Y.\n2023.\nFET-LM: Flow-Enhanced Variational Autoencoder\nfor Topic-Guided Language Modeling. IEEE Transactions\non Neural Networks and Learning Systems.\nVan Den Oord, A.; Kalchbrenner, N.; and Kavukcuoglu, K.\n2016. Pixel recurrent neural networks. In International con-\nference on machine learning, 1747\u20131756. PMLR.\nVan Den Oord, A.; Vinyals, O.; et al. 2017.\nNeural dis-\ncrete representation learning. Advances in neural informa-\ntion processing systems, 30.\nVan der Maaten, L.; and Hinton, G. 2008. Visualizing data\nusing t-SNE. Journal of machine learning research, 9(11).\nWainwright, M. J.; Jordan, M. I.; et al. 2008. Graphical mod-\nels, exponential families, and variational inference. Founda-\ntions and Trends\u00ae in Machine Learning, 1(1\u20132): 1\u2013305.\nWang, D.; Guo, D.; Zhao, H.; Zheng, H.; Tanwisuth, K.;\nChen, B.; and Zhou, M. 2022.\nRepresenting mixtures of\nword embeddings with mixtures of topic embeddings. arXiv\npreprint arXiv:2203.01570.\nXu, Y.; Wang, D.; Chen, B.; Lu, R.; Duan, Z.; Zhou, M.;\net al. 2022. HyperMiner: Topic Taxonomy Mining with Hy-\nperbolic Embedding. Advances in Neural Information Pro-\ncessing Systems, 35: 31557\u201331570.\nYang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov,\nR. R.; and Le, Q. V. 2019. Xlnet: Generalized autoregressive\npretraining for language understanding. Advances in neural\ninformation processing systems, 32.\nYu, J.; Li, X.; Koh, J. Y.; Zhang, H.; Pang, R.; Qin, J.; Ku,\nA.; Xu, Y.; Baldridge, J.; and Wu, Y. 2021. Vector-quantized\nimage modeling with improved VQGAN.\narXiv preprint\narXiv:2110.04627.\nZhang, H.; Chen, B.; Guo, D.; and Zhou, M. 2018. WHAI:\nWeibull hybrid autoencoding inference for deep topic mod-\neling. arXiv preprint arXiv:1803.01328.\n"
  },
  {
    "title": "Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method",
    "link": "https://arxiv.org/pdf/2312.12030.pdf",
    "upvote": "4",
    "text": "Towards Accurate Guided Diffusion Sampling through\nSymplectic Adjoint Method\nJiachun Pan\u2217\nNational University of Singapore\npan.jc@nus.edu.sg\nHanshu Yan\u2217\nByteDance\nhanshu.yan@bytedance.com\nJun Hao Liew\nByteDance\njunhao.liew@bytedance.com\nJiashi Feng\nByteDance\njshfeng@bytedance.com\nVincent Y. F. Tan\nNational University of Singapore\nvtan@nus.edu.sg\nButterfly\nA cat \nwearing \nglasses\nReference \nstyle\nA dog at \nthe \nAcropolis\nA dog in \na bucket\nReference \nID\nStyle\nguided\nID\nguided\n(a) Style-guided image generation\n(b) Aesthetic improvement\n(c) Personalization\n(d) Video stylization\nAesthetic\nguided\nStable Diffusion \nv1.5\nReference \nstyle\nStyle\nguided\nInput video\nOutput video\noutput\nFigure 1. We propose Symplectic Adjoint Guidance, a training-free guided diffusion process that supports various image and video\ngeneration tasks, including style-guided image generation, aesthetic improvement, personalization and video stylization.\nAbstract\nTraining-free guided sampling in diffusion models lever-\nages off-the-shelf pre-trained networks, such as an aesthetic\nevaluation model, to guide the generation process. Current\ntraining-free guided sampling algorithms obtain the guid-\nance energy function based on a one-step estimate of the\nclean image. However, since the off-the-shelf pre-trained\nnetworks are trained on clean images, the one-step estima-\ntion procedure of the clean image may be inaccurate, espe-\ncially in the early stages of the generation process in dif-\n*Equal contribution.\nfusion models. This causes the guidance in the early time\nsteps to be inaccurate. To overcome this problem, we pro-\npose Symplectic Adjoint Guidance (SAG), which calculates\nthe gradient guidance in two inner stages.\nFirstly, SAG\nestimates the clean image via n function calls, where n\nserves as a flexible hyperparameter that can be tailored to\nmeet specific image quality requirements. Secondly, SAG\nuses the symplectic adjoint method to obtain the gradients\naccurately and efficiently in terms of the memory require-\nments. Extensive experiments demonstrate that SAG gener-\nates images with higher qualities compared to the baselines\nin both guided image and video generation tasks. Code\nis available at https://github.com/HanshuYAN/\n1\narXiv:2312.12030v1  [cs.CV]  19 Dec 2023\nAdjointDPM.git\n1. Introduction\nDiffusion models are powerful generative models that ex-\nhibit impressive performances across different modality\ngeneration, including image [5, 11, 12], video [22, 34, 39]\nand audio generation [16].\nGuided sampling, including\nclassifier guidance [5] and classifier-free guidance [11], has\nbeen widely used in diffusion models to realize controllable\ngeneration, such as text-to-image generation [29], image-\nto-image generation [24, 28], and ControlNet [37]. Guided\nsampling controls the outputs of generative models by con-\nditioning on various types of signals, such as descriptive\ntext, class labels, and images.\nA line of guidance methods involves task-specific train-\ning of diffusion models using paired data, i.e., targets and\nconditions. For instance, classifier guidance [5] combines\nthe score estimation of diffusion models with the gradients\nof the image classifiers to direct the generation process to\nproduce images corresponding to a particular class. In this\nway, several image classifiers need to be trained on the noisy\nstates of intermediate generation steps of diffusion models.\nAlternatively, classifier-free guidance [11] directly trains a\nnew score estimator with conditions and uses a linear com-\nbination of conditional and unconditional score estimators\nfor sampling. Although this line of methods can effectively\nguide diffusion models to generate data satisfying certain\nproperties, they are not sufficiently flexible to adapt to any\ntype of guiding due to the cost of training and the feasibility\nof collecting paired data.\nTo this end, another line of training-free guidance meth-\nods has been explored [2, 14, 36]. In training-free guided\nsampling, at a certain sampling step t, the guidance func-\ntion is usually constructed as the gradients of the loss func-\ntion obtained by the off-the-shelf pre-trained models, such\nas face-ID detection or aesthetic evaluation models. More\nspecifically, the guidance gradients are computed based on\nthe one-step approximation of denoised images from the\nnoisy samples at certain steps t. Then, gradients are added\nto corresponding sampling steps as guidance to direct the\ngeneration process to the desired results. This line of meth-\nods offers greater flexibility by allowing the diffusion mod-\nels to adapt to a broad spectrum of guidance. However, at\ncertain time steps with guidance, the generated result at the\nend is usually misaligned with its one-step denoising ap-\nproximation, which may lead to inaccurate guidance. The\nmisalignment is notably pronounced in the early steps of the\ngeneration process, as the noised samples are far from the fi-\nnally generated result. For example, in face ID-guided gen-\neration, when the final approximation is blurry and passed\nto pre-trained face detection models, we cannot obtain ac-\ncurate ID features, which leads to inaccuracies in guidance\nto the desired faces.\nTo mitigate the misalignment issue of existing training-\nfree guidance, we propose a novel guidance algorithm,\ntermed Sympletic Adjoint Guidance (SAG). As shown in\nFigure 2, SAG estimates the finally generated results by n-\nstep denoising. Multiple-step estimation yields more ac-\ncurate generated samples, but this also introduces another\nchallenge in backpropagating the gradients from the out-\nput to each intermediate sampling step. Because the execu-\ntion of the vanilla backpropagation step requires storing all\nthe intermediate states of the n iterations, the memory cost\nis prohibitive. To tackle this challenge, SAG applies the\nsymplectic adjoint method, an adjoint method solved by\na symplectic integrator [20], which can backpropagate the\ngradients accurately and is memory efficient. In summary,\nour contributions are as follows:\n\u2022 We propose to use an n-step estimate of the final gener-\nation to calculate the gradient guidance. This mitigates\nthe misalignment between the final outputs and their esti-\nmates, which provides more accurate guidance from off-\nthe-shelf pre-trained models.\n\u2022 To backpropagate gradients throughout the n-step esti-\nmate, we introduce the theoretically grounded symplectic\nadjoint method to obtain accurate gradients. This method\nis also memory efficient, which is beneficial to guided\nsampling in large models, such as Stable Diffusion.\n\u2022 Thanks to accurate guidance, SAG can obtain high-\nquality results in various guided image and video genera-\ntion tasks.\n2. Background\n2.1. Guided Generation in Diffusion Models\nDiffusion Models\nDiffusion generative models gradually\nadd Gaussian noise to complex data distributions to trans-\nform them into a simple Gaussian distribution and then\nsolve the reverse process to generate new samples.\nThe\nforward noising process and reverse denoising process can\nboth be modeled as SDE and ODE forms [31]. In this pa-\nper, we mainly consider the ODE form of diffusion models\nas it is a deterministic method for fast sampling of diffu-\nsion models. An example for discrete deterministic sam-\npling (solving an ODE) is DDIM [30], which has the form:\nxt\u22121 = \u221a\u03b1t\u22121\u02c6x0 +\np\n1 \u2212 \u03b1t\u22121\u03f5\u03b8(xt, t),\n(1)\nwhere \u03b1t is a schedule that controls the degree of diffusion\nat each time step, \u03f5\u03b8(xt, t) is a network that predicts noise,\nand \u02c6x0 is an estimate of the clean image:\n\u02c6x0 = xt \u2212 \u221a1 \u2212 \u03b1t\u03f5\u03b8(xt, t)\n\u221a\u03b1t\n.\n(2)\n2\ndenoise\ndenoise\nGuidance\n\u2026\n\u2026\n\ud835\udc65!\n\ud835\udc65!\"#\n\ud835\udc65$\n\ud835\udc65$\"#\n\ud835\udc65%\nReference style\nw/o \nguidance\nw/ \nguidance\nExample: Stylized text-to-Image\ndenoise\n\ud835\udf15\ud835\udc3f\n\ud835\udf15\ud835\udc65%\nState\nAdjoint state\n\ud835\udc61\n\ud835\udf0f\n\ud835\udf0f \u2212 1\n0\nStyle Loss \ud835\udc3f\nReference \nstyle\n\ud835\udf15\ud835\udc3f\n\ud835\udf15\ud835\udc65$\n\ud835\udc65$\n\ud835\udc5b steps (\ud835\udc5b \u226a \ud835\udc47)\nGuidance\nForward ODE\nBackward ODE\n\ud835\udc65%\nFigure 2. Symplectic Adjoint Training-free Guidance Generation. We illustrate the framework of training-free guided generation through\nsymplectic adjoint guidance using a stylization example. When we denoise Gaussian noise to an image across various steps, we can\nadd guidance (usually defined as gradients of loss function on the estimate of \u02c6x0 based on xt) to each step. Different from previous\nworks [2, 36] which approximate \u02c6x0 based on xt using one step, we estimate \u02c6x0 using n steps (n \u226a T) by solving a forward ODE. Then\nwe use the symplectic adjoint method to solve a backward ODE to obtain the gradients. These gradients guide the diffusion generation\nprocess to be closer to the reference image.\nThe DDIM can be regarded as the discretization of an ODE.\nBy multiplying both sides of (1) with\np\n1/\u03b1t\u22121, we have\nxt\u22121\n\u221a\u03b1t\u22121\n=\nxt\n\u221a\u03b1t\n+ \u03f5\u03b8(xt, t)\n\u0012\u221a1 \u2212 \u03b1t\u22121\n\u221a\u03b1t\u22121\n\u2212\n\u221a1 \u2212 \u03b1t\n\u221a\u03b1t\n\u0013\n.\nWe can parameterize \u03c3t = \u221a1 \u2212 \u03b1t/\u221a\u03b1t as \u03c3t is mono-\ntone in t [30] and \u00afx\u03c3t = xt/\u221a\u03b1t. Then when \u03c3t\u22121 \u2212 \u03c3t \u2192\n0, we obtain the ODE form of DDIM:\nd\u00afx\u03c3t = \u00af\u03f5(\u00afx\u03c3t, \u03c3t)d\u03c3t,\n(3)\nwhere \u00af\u03f5(\u00afx\u03c3t, \u03c3t) = \u03f5\u03b8(xt, t). Using ODE forms makes it\npossible to use numerical methods to accelerate the sam-\npling process [19].\nGuided Generation\nGuided sampling in diffusion mod-\nels can roughly be divided into two categories: training-\nrequired and training-free. Training-required models [5, 11,\n26, 30] are usually well-trained on paired data of images and\nguidance, leading to strong guidance power in the diffusion\nsampling process. However, they lack flexibility in adapting\nto a variety of guidances.\nIn this work, we mainly focus on the training-free guided\nsampling in diffusion models [2, 10, 21, 24, 36]. Training-\nfree guided sampling methods, such as FreeDOM [36] and\nUniversal Guidance (UG) [2] leverage the off-the-shelf pre-\ntrained networks to guide the generation process. To gener-\nate samples given some conditions c, a guidance function is\nadded to the diffusion ODE function:\nd\u00afx\u03c3t\nd\u03c3t\n= \u00af\u03f5(\u00afx\u03c3t, \u03c3t) + \u03c1\u03c3tg(\u00afx\u03c3t, c, \u03c3t),\n(4)\nwhere \u03c1\u03c3t is the parameter that controls the strength of guid-\nance and g(\u00afx\u03c3t, c, \u03c3t) is usually taken as the negative gra-\ndients of loss functions \u2212\u2207\u00afx\u03c3t L(\u00afx\u03c3t, c) [2, 36] obtained\nby the off-the-shelf networks. For example, in the styliza-\ntion task, L could be the style loss between \u00afx\u03c3t and the\nstyle images. As the off-the-shelf networks are trained on\nclean data, directly using them to obtain the loss function of\nnoisy data \u00afx\u03c3t is improper. To address this problem, they\napproximate \u2207\u00afx\u03c3t L(\u00afx\u03c3t, c) using \u2207\u00afx\u03c3t L(\u02c6x0(\u00afx\u03c3t, \u03c3t), c),\nwhere \u02c6x0(\u00afx\u03c3t, \u03c3t) is an estimate of the clean image shown\nin (2). Besides using the above gradients as guidance, an-\nother technique called backward universal guidance is intro-\nduced in UG, which is used to enforce the generated image\nto satisfy the guidance. In this work, we do not use this\ntechnique as our method could already obtain high-quality\ngenerated results.\nBesides, directly adding guidance functions to standard\ngeneration pipelines may cause artifacts and deviations\nfrom the conditional controls. To mitigate this problem, the\ntime-travel strategy in FreeDOM (or self-recurrence in UG)\nis applied. Specifically, after xt\u22121 is sampled, we further\nadd random Gaussian noise to xt\u22121 and repeat this denois-\ning and noising process for r times before moving to the\nnext sampling step.\n2.2. Adjoint Sensitivity Method\nThe outputs of neural ODE models involve multiple itera-\ntions of the function call, which introduces the challenge\nof backpropagating gradients to inputs and model weights\nbecause the vanilla gradient-backpropagation requires stor-\n3\ning all the intermediate states and leads to extremely large\nmemory consumption. To solve this, Chen et al. [3] pro-\nposed the adjoint sensitivity method, in which adjoint states,\nat =\n\u2202L\n\u2202xt , are introduced to represent the gradients of the\nloss with respect to intermediate states. The adjoint method\ndefines an augmented state as the pair of the system state xt\nand the adjoint variable at, and integrates the augmented\nstate backward in time.\nThe backward integration of at\nworks as the gradient backpropagation in continuous time.\nTo obtain the gradients of loss w.r.t intermediate states\nin diffusion models,\nthere are some existing works.\nDOODL [32] obtain the gradients of loss w.r.t noise vec-\ntors by using invertible neural networks [1]. DOODL re-\nlies on the invertibility of EDICT [33], resulting in identical\ncomputation steps for both the backward gradient calcula-\ntion and the forward sampling process. Here in our work,\nthe n-step estimate could be flexible in the choice of n.\nFlowGrad [17] efficiently backpropagates the output to any\nintermediate time steps on the ODE trajectory, by decom-\nposing the backpropagation and computing vector Jacobian\nproducts. FlowGrad needs to store the intermediate results,\nwhich may not be memory efficient.\nMoreover, the ad-\njoint sensitivity method has been applied in diffusion mod-\nels to finetune diffusion parameters for customization [23]\nas it aids in obtaining the gradients of different parameters\nwith efficient memory consumption. Specifically, in diffu-\nsion models (here we use the ODE form (3)), we first solve\nODE (3) from T to 0 to generate images, and then solve\nanother backward ODE from 0 to T to obtain the gradients\nwith respect to any intermediate state \u00afx\u03c3t, where the back-\nward ODE [3] has the following form:\nd\n\u0014 \u00afx\u03c3t\n\u2202L\n\u2202\u00afx\u03c3t\n\u0015\n=\n\"\n\u00af\u03f5(\u00afx\u03c3t, \u03c3t)\n\u2212\n\u0010\n\u2202\u00af\u03f5(\u00afx\u03c3t,\u03c3t)\n\u2202\u00afx\u03c3t\n\u0011T\n\u2202L\n\u2202\u00afx\u03c3t\n#\nd\u03c3t.\n(5)\nAfter obtaining the gradients\n\u2202L\n\u2202\u00afx\u03c3t , we naturally have\n\u2202L\n\u2202xt =\n1\n\u221a\u03b1t\n\u2202L\n\u2202\u00afx\u03c3t based on the definition of \u00afx\u03c3t. Different\nfrom [23], this paper mainly focuses on flexible training-\nfree guidance exploiting various types of information from\npre-trained models. However, the vanilla adjoint method\nsuffers from numerical errors. To reduce the numerical er-\nrors, backward integration often requires a smaller step size\nand leads to high computational costs. In this work, we\nutilize the symplectic adjoint method [20] to reduce the dis-\ncretization error in solving the backward ODE.\n3. Methods\nExisting training-free guidance methods usually construct\nthe gradient guidance through a one-step estimate of the\nclean image \u02c6x0, which, however, is usually misaligned with\nthe finally generated clean image. Such misalignment wors-\nens in the early stage of the sampling process where noised\nsamples \u00afx\u03c3t are far from the final outputs. As a result, the\nguidance is often inaccurate. To mitigate the misalignment\nof the one-step estimate of \u02c6x0, we propose Symplectic Ad-\njoint Guidance (SAG) to accurately estimate the finally gen-\nerated content and provide exact gradient-based guidance\nfor better generation quality. We first consider estimating\nthe clean image \u02c6x0 using n steps from xt. Nevertheless,\nwhen estimating \u02c6x0 using n steps from xt, how to accu-\nrately obtain the gradients \u2207\u00afx\u03c3t L(\u02c6x0, c) is non-trivial. We\nutilize the symplectic adjoint method, which can obtain ac-\ncurate gradients with efficient memory consumption. The\noverall pipeline is shown in Fig. 2 and the explicit algorithm\nis shown in Algorithm 1.\n3.1. Multiple-step Estimation of Clean Outputs\nAs\ndiscussed\nin\nsection\n2.1,\ntraining-free\nguidance\nmethods\nusually\napproximate\n\u2207\u00afx\u03c3t L(\u00afx\u03c3t, c)\nusing\n\u2207\u00afx\u03c3t L(\u02c6x0(\u00afx\u03c3t, \u03c3t), c). According to [4, Theorem 1], the\napproximation error is upper bounded by two terms: the\nfirst is related to the norm of gradients, and the second is\nrelated to the average estimation error of the clean image,\ni.e., m =\nR\n\u2225x0 \u2212 \u02c6x0\u2225p(x0|xt) dx0. To reduce the gradient\nestimation error, we consider reducing the estimation error\nof the clean image (i.e., the misalignment between the\none-step estimate and the final generated clean image) by\nusing the n step estimate.\nSuppose the standard sampling process generates clean\noutputs for T steps, from which we sample a subset of\nsteps for implementing guidance.\nThe subset for guid-\nance can be indicated via a sequence of boolean values,\ngT :1 = [gT , gT \u22121, \u00b7 \u00b7 \u00b7 , g1]. For a certain step t of guid-\nance, we consider predicting the clean image by solving the\nODE functions (3) in n time steps. Here we usually set n\nto be much smaller than T for time efficiency (refer to sec-\ntion 4.5 for details). Here, note that we denote the state of\nthe sub-process for predicting clean outputs as x\u2032\nt and \u00afx\u2032\n\u03c3\nso as to distinguish from the notation xt and \u00afx\u03c3 used in the\nmain sampling process. Taking solving (3) using the Euler\nnumerical solver [6] as an example (when the standard gen-\neration process is at step t), the estimate of the clean image\nx\u2032\n0 can be solved iteratively by (6), where \u03c4 = n, . . . , 1 and\nthe initial state of this sub-process x\u2032\nn = xt.\nx\u2032\n\u03c4\u22121\n\u221a\u03b1\u03c4\u22121\n= x\u2032\n\u03c4\n\u221a\u03b1\u03c4\n+\u03f5\u03b8(x\u2032\n\u03c4, \u03c4)\n s\n1\u2212\u03b1\u03c4\u22121\n\u03b1\u03c4\u22121\n\u2212\nr\n1\u2212\u03b1\u03c4\n\u03b1\u03c4\n!\n(6)\nFor a special case that n = 1 and \u221a\u03b10 = 1 [30], we\nhave x\u2032\n0 =\nxt\n\u221a\u03b1t \u2212 \u03f5\u03b8(xt, t)\nq\n1\u2212\u03b1t\n\u03b1t , which is equivalent\nto (2). Thus, our method particularizes to FreeDOM [36]\nwhen n = 1. Denote m(n) to be the average estimation\nerror in n estimate steps. In the following lemma, we show\nthat m will not increase when we use n-step estimation. The\nproof of Lemma 1 is shown in the Appendix A.1.\n4\n\ud835\udf15\ud835\udc3f\n\ud835\udf15\ud835\udc65!\nState\nAdjoint state\n\ud835\udc61\n\ud835\udf0f\n\ud835\udf0f \u2212 1\n0\n\ud835\udf15\ud835\udc3f\n\ud835\udf15\ud835\udc65\"\ndiscretization\nerror\nLoss \ud835\udc3f\n\ud835\udc65\"\n\ud835\udc5b\nForward ODE\nOrdinary Numerical Solver\nSymplectic Numerical Solver\nBackward ODE (Ground truth)\n\ud835\udc65!\nFigure 3. Illustration of the Symplectic Adjoint method\nLemma 1 m(n1) \u2264 m(n2) when n2 \u2264 n1.\n3.2. Symplectic Adjoint Method\nIn section 3.1, we show how to get a more accurate estima-\ntion x\u2032\n0 of the final output by solving ODE functions (3) in\nn steps. As introduced in section 2.2, the adjoint method is\na memory-efficient way to obtain the gradients \u2202L\n\u2202xt through\nsolving a backward ODE (5). However, as our n is set to\nbe much smaller than T and we usually set it to be 4 or 5\nin our experiments, using the vanilla adjoint method will\nsuffer from discretization errors. Thus, instead of using the\nvanilla adjoint method, we consider obtaining the accurate\ngradient guidance \u2207xtL(x\u2032\n0, c) using Symplectic Adjoint\nmethod [7, 20]. Here we present the first-order symplectic\nEuler solver [7] as an example to solve (5) from 0 to n to\nobtain accurate gradients. We also can extend it to high-\norder symplectic solvers, such as Symplectic Runge\u2013Kutta\nMethod [20] for further efficiency in solving (refer to Ap-\npendix A.2).\nSuppose we are implementing guidance at time step t,\nthe forward estimation sub-process is discretized into n\nsteps. Let \u03c4 \u2208 [n, . . . , 0] denote the discrete steps corre-\nsponding to time from t to 0 and \u03c3\u03c4 = \u221a1 \u2212 \u03b1\u03c4/\u221a\u03b1\u03c4. The\nforward estimate follows the forward update rule (6), whose\ncontinuous form equals ODE (3).\nThen, the Symplectic\nEuler update rule for solving the corresponding backward\nODE (5) is:\n\u00afx\u2032\n\u03c3\u03c4+1 = \u00afx\u2032\n\u03c3\u03c4 + h\u03c3\u03c4 \u00af\u03f5(\u00afx\u2032\n\u03c3\u03c4+1, \u03c3\u03c4+1),\n(7)\n\u2202L\n\u2202\u00afx\u2032\u03c3\u03c4+1\n=\n\u2202L\n\u2202\u00afx\u2032\u03c3\u03c4\n\u2212h\u03c3\u03c4\n\u0012\u2202\u00af\u03f5(\u00afx\u2032\n\u03c3\u03c4+1, \u03c3\u03c4+1)\n\u2202\u00afx\u2032\n\u0013T\n\u2202L\n\u2202\u00afx\u2032\u03c3\u03c4\n, (8)\nfor \u03c4 = 0, 1, . . . , n \u2212 1. h\u03c3 is the discretization step size.\nAfter we obtain\n\u2202L\n\u2202\u00afx\u2032\u03c3n , \u2202L\n\u2202xt is easily computed by\n\u2202L\n\u2202\u00afx\u2032\u03c3n \u00b7\n1\n\u221a\u03b1t\nbased on the definition of \u00afx\u03c3t.\nNote that different from the vanilla adjoint sensitivity\nmethod, which uses \u00af\u03f5(\u00afx\u2032\n\u03c3\u03c4 , \u03c3\u03c4) to update \u00afx\u2032\n\u03c3\u03c4+1 and\n\u2202L\n\u2202\u00afx\u2032\u03c3\u03c4+1 ,\nthe proposed symplectic solver uses \u00af\u03f5(\u00afx\u2032\n\u03c3\u03c4+1, \u03c3\u03c4+1). The\nvalues of \u00afx\u2032\n\u03c3\u03c4+1 are restored from those that have been com-\nputed during the forward estimation. In Theorem 3, we\nprove that the gradients obtained by the Symplectic Eu-\nler are accurate. Due to the limits of space, the complete\nAlgorithm 1 Symplectic Adjoint Guidance (SAG)\nRequire: diffusion model \u03f5\u03b8, condition c, loss L, sam-\npling scheduler S, guidance strengths \u03c1t, noise schedul-\ning \u03b1t, guidance indicator [gT , . . . , g1], repeat times of\ntime travel (rT , . . . , r1).\n1: xT \u223c N(0, I)\n2: for t = T, . . . , 1 do\n3:\nfor i = rt, . . . , 1 do\n4:\nxt\u22121 = S(xt, \u03f5\u03b8, c)\n5:\nif gt then\n6:\n\u02c6x0 = solving (6) in n steps\n7:\n\u2207xtL(\u02c6x0, c) = solving (7) and (8).\n8:\nxt\u22121 = xt\u22121 \u2212 \u03c1t\u2207xtL(\u02c6x0, c)\n9:\nend if\n10:\nxt =\n\u221a\u03b1t\n\u221a\u03b1t\u22121 xt\u22121+\n\u221a\u03b1t\u22121\u2212\u03b1t\n\u221a\u03b1t\u22121\n\u03f5\u2032 with \u03f5\u2032 \u223c N(0, I)\n11:\nend for\n12: end for\nstatement and proof of Theorem 3 are presented in Ap-\npendix A.3. We illustrate the difference between the vanilla\nadjoint method and the symplectic adjoint method in Fig. 3.\nTheorem 2 (Informal) Let the gradient\n\u2202L\n\u2202\u00afx\u2032\u03c3t be the ana-\nlytical solution to the continuous ODE in (5) and let\n\u2202L\n\u2202\u00afx\u2032\u03c3n\nbe the gradient obtained by the symplectic Euler solver in\n(8) throughout the discrete sampling process. Then, under\nsome regularity conditions, we have\n\u2202L\n\u2202\u00afx\u2032\u03c3t =\n\u2202L\n\u2202\u00afx\u2032\u03c3n .\nCombining the two stages above, namely the n-step\nestimate of the clean output and the symplectic adjoint\nmethod, we have the Symplectic Adjoint Guidance (SAG)\nmethod, which is shown in Algorithm 1. We also apply\nthe time-travel strategy [2, 36] into our algorithm.\nThe\nsampling/denoising scheduler S could be any popular sam-\npling algorithm, including DDIM [30], DPM-solver [18],\nand DEIS [38]. The overall illustration of our SAG is shown\nin Fig. 2.\nRuntime analysis\nWhile increasing n can mitigate the\nmisalignment of \u02c6x\u2032\n0 and lead to a highly enhanced quality\nof generated images in different tasks, it also proportion-\nally increases the runtime. There exists a trade-off between\ncomputational cost and the generation quality. When n = 1,\nSAG degenerates to one-step-estimate guidance methods\n(e.g. FreeDOM [36]). The computation cost decreases but\nthe sample quality is compromised. In practice, we could\ndesign an adaptive guidance strategy where the number of\nestimate steps dynamically adjusts itself as the main sam-\npling process proceeds. For example, we may use a rela-\ntively large n at the early sampling stage and then gradually\ndecrease to the one-step estimate when \u02c6x\u2032\n0 is not far from the\nfinal generations. Besides adaptively adjusting the number\n5\nStyle image\nUG\nFreeDOM\nSAG\nFigure 4. Stylization results of \u201cA cat wearing glasses\u201d.\nMethod\nStyle loss (\u2193)\nCLIP (\u2191)\nFreeDOM\n482.7\n22.37\nUG\n805\n23.02\nSAG\n386.6\n23.51\n(a) Style guided generation.\nMethod\nID loss (\u2193)\nFID (\u2193)\nFreeDOM\n0.602\n65.24\nSAG\n0.574\n64.25\n(b) Face-ID guided generation.\nTable 1. Quantitative Comparison: (a) Stylization quality mea-\nsured by style loss and clip Score, (b) Performance of face ID\nguided generation assessed using face ID Loss and FID.\nof estimate steps n, SAG also allows us to select the subset\nof intermediate steps of the main sampling process for guid-\ning, which is indicated by gT :1. Usually, we only choose a\nsequence of the middle stage for guiding, i.e., gt = 1 for\nt \u2208 [K2, K1] with 0 < K1 < K2 < T and gt = 0 for\nothers. That is because the states at the very early denoising\nstage are less informative about the final outputs, and the\nstates at the very last stage almost decide the appearance of\nfinal outputs and barely can make more changes.\n4. Experiments\nWe present experimental results to show the effectiveness of\nSAG. We apply SAG to several image and video generation\ntasks, including style-guided image generation, image aes-\nthetic improvement, personalized image generation (object\nguidance and face-ID guidance), and video stylization. We\nconduct ablation experiments to study the effectiveness of\nhyperparameters including the number of estimation steps\nn, guidance scale \u03c1t, etc.\n4.1. Style-Guided Sampling\nStyle-guided sampling generates an output image that\nseamlessly merges the content\u2019s structure with the chosen\nstylistic elements from reference style images, showcasing\na harmonious blend of content and style. To perform style-\nguided sampling, following the implementation of [36], we\nuse the features from the third layer of the CLIP image en-\ncoder as our feature vector. The loss function is L2-norm\nbetween the Gram matrix of the style image and the Gram\nmatrix of the estimated clean image. We use the gradients\nof this loss function to guide the generation in Stable Diffu-\nsion [26]. We set n = 4.\nWe compare our results with FreeDOM [36] and Uni-\nversal Guidance (UG) [2]. We use style loss as a metric to\nmeasure the stylization performance and use the CLIP [25]\nscore to measure the similarity between generated images\nand input prompts. Good stylization implies that the style\nof generated images should be close to the reference style\nimage while aligning with the given prompt. We obtain the\nquantitative results by randomly selecting five style images\nand four prompts, generating five images per style and per\nprompt. We use the officially released codes to generate the\nresults of FreeDOM1 and Universal Guidance2 under the\nsame style and prompt. The qualitative results are shown in\nFig. 4 and quantitative results are shown in Table 1a. Full\ndetails can be found in Appendix B.1 and more results in\nAppendix D.\nFrom Fig. 4 and Table. 1a, we can find that SAG has the\nbest performance compared with FreeDOM and UG as it\nhas better stylization phenomena and it can largely preserve\nthe content of images with the given text prompt. Besides,\nit is obvious that UG performs the worst in terms of styliza-\ntion. We can observe that stylization by UG is not obvious\nfor some style images and the image content is distorted.\nA drawing of a haunted \nhouse made by children.\nSD v1.4\nDOODL\nFreeDOM\nSAG\nRobots from the 1950s \nplaying Atari 2600 \nstyled game.\nA Psytrance artwork \nfeaturing Kazuya \nfrom Tekken, created \nby Sam Spratt.\nA black bench that \nis by a sidewalk on \na street.\nFigure 5. Examples on aesthetic improvement\n4.2. Aesthetic Improvement\nIn this task, we consider improving the aesthetic quality of\ngenerated images through the guidance of aesthetic scores\nobtained by the LAION aesthetic predictor,3 PickScore [13]\nand HPSv2 [35]. The LAION aesthetic predictor is a linear\n1https://github.com/vvictoryuki/FreeDoM.git\n2https://github.com/arpitbansal297/Universal-\nGuided-Diffusion.git\n3https : / / github . com / LAION - AI / aesthetic -\npredictor.git\n6\nMethod\nAesthetic loss(\u2193)\nSD v1.5\n9.71\nFreeDOM\n9.18\nDOODL\n9.78\nSAG\n8.17\n(a) Aesthetic improvement.\nMethod\nCLIP-I (\u2191)\nCLIP-T (\u2191)\nDreamBooth\n0.724\n0.277\nFreeDOM\n0.681\n0.281\nDOODL\n0.743\n0.277\nSAG\n0.774\n0.270\n(b) Object guided generation.\nTable 2. Quantitative Comparison: (a) Aesthetic loss for image\naesthetics, (b) Clip image and clip text scores for object-guided\ngeneration performance.\nhead pre-trained on top of CLIP visual embeddings to pre-\ndict a value ranging from 1 to 10, which indicates the aes-\nthetic quality. PickScore and HPSv2 are two reward func-\ntions trained on human preference data. We set n = 4 and\nuse the linear combination of these three scores as met-\nrics to guide image generation.\nWe randomly select ten\nprompts from four prompt categories, Animation, Concept\nArt, Paintings, Photos, and generate one image for each\nprompt. We compare the resulting weighted aesthetic scores\nof all generated images with baseline Stable Diffusion (SD)\nv1.5, DOODL [32] and FreeDOM [36] in Table 2a. The\nresults were generated using the official code released by\nDOODL.4 The qualitative comparison is shown in Fig 5.\nWe find that our method has the best aesthetic improve-\nment effect, with more details and richer color. Besides,\nas DOODL optimizes the initial noise to enhance aesthet-\nics, the generated images will be different from the original\ngenerated images. Experimental details are shown in Ap-\npendix B.2 and more results in Appendix D.\n4.3. Personalization\nPersonalization aims to generate images that contain a\nhighly specific subject in new contexts. When given a few\nimages (usually 3-5) of a specific subject, DreamBooth [27]\nand Textual Inversion [8] learn or finetune components such\nas text embedding or subsets of diffusion model parameters\nto blend the subject into generated images. However, when\nthere is only a single image, the performance is not satisfac-\ntory. In this section, we use symplectic adjoint guidance to\ngenerate a personalized image without additional generative\nmodel training or tuning based on a single example image.\nWe conduct experiments with two settings: (1) general ob-\nject guidance and (2) face-ID guidance.\nObject Guidance\nWe first do the personalization of cer-\ntain objects in Stable Diffusion. We use a spherical distance\nloss [32] to compute the distance between image features\nof generated images and reference images obtained from\nViT-H-14 CLIP model.5 In this task, we set n = 4. We\ncompare our results with FreeDOM [36], DOODL [32] and\nDreamBooth [27]. The results of DreamBooth6 is generated\n4https://github.com/salesforce/DOODL.git\n5https://github.com/mlfoundations/open_clip.git\n6https : / / github . com / XavierXiao / Dreambooth -\nStable-Diffusion.git\nRef img\nA dog in a bucket.\nDOODL\nFreeDOM\nSAG\nDreambooth\nFigure 6. Examples on object-guided sampling\nusing the official code. We use DreamBooth to finetune the\nmodel for 400 steps and set the learning rate as 1 \u00d7 10\u22126\nwith only one training sample. We use the cosine similarity\nbetween CLIP [25] embeddings of generated and reference\nimages (denoted as CLIP-I) and the cosine similarity be-\ntween CLIP embeddings of generated images and given text\nprompts (denoted as CLIP-T) to measure the performance.\nThe quantitative comparison is shown in Table. 2b and the\nqualitative results are shown in Fig. 6. We can find that im-\nages generated by SAG have the highest CLIP image simi-\nlarity with reference images. We show experimental details\nin Appendix B.3 and more results in Appendix D.\nRef Face ID\nFreeDOM\nSAG\nFigure 7. Examples on Face ID guided generation\nFace-ID\nGuidance\nFollowing\nthe\nimplementation\nof [36], we use ArcFace to extract the target features of\nreference faces to represent face IDs and compute the l2\nEuclidean distance between the extracted ID features of\nthe estimated clean image and the reference face image as\nthe loss function. In this task, we set n = 5. We compare\nour Face ID guided generation results with FreeDOM\nand measure the performance using the loss and FID,\nrespectively. We randomly select five face IDs and generate\n200 faces for each face IDs. We show the qualitative results\nin Fig. 7 and the quantitative results in Table 1b. Compared\nwith FreeDOM, SAG matches the conditional Face IDs\nbetter with better generation image quality (lower FID).\n7\nReference \nID\nboats, floating on the blue sea, in front of gray buildings, red sky\na cat sitting against the white wall\nNon-style\nguidance\nNon-style\nguidance\nInput Video\nInput Video\nFigure 8. Examples on Video Stylization. For each input, the\nupper row is rendered on the conditioning of a text prompt and the\ndepth sequence. The lower row is the output with the extra style\nguidance.\n4.4. Video Stylization\nWe also apply the SAG method for style-guided video edit-\ning, where we change the content and style of the original\nvideo while keeping the motion unchanged. For example,\ngiven a video of a dog running, we want to generate a video\nof a cat running with a sketch painting style. In this ex-\nperiment, we use MagicEdit [15], which is a video genera-\ntion model conditioning on a text prompt and a sequence of\ndepth maps for video editing. Given an input video, we first\nextract a sequence of depth maps. By conditioning on the\ndepth maps, MagicEdit renders a video whose motion fol-\nlows that in the original video. Using the style Gram metric\nin Sec. 4.1, we can compute the average loss between each\nframe and the reference style image.\nSince the depth and text conditions provide very rich\ninformation about the final output, MagicEdit can synthe-\nsize high-quality videos within 25 steps (i.e. denoising for\nT from 25 to 0). We use MagicEdit to render video of 16\nframes where the resolution of each frame is 256\u00d7256. We\napply the SAG guidance to the steps of T \u2208 [20, 10]. As\nshown in Figure 8, SAG can effectively enable MagicEdit to\ngenerate videos of specific styles (e.g., a cat of the Chinese\npapercut style). In contrast, without SAG, the base editing\nmodel can barely synthesize videos whose color and texture\nalign with the reference image. More experimental details\nare shown in Appendix C.\n4.5. Ablation Study\nChoice of n.\nWe investigate the impact of varying values\nof n on the model\u2019s performance. Taking the stylization task\nas an example, we set T = 100 and perform training-free\nguidance from step 70 to step 31. We use the prompts: \u201cA\ncat wearing glasses\u201d, \u201cbutterfly\u201d and \u201cA photo of an Eif-\nfel Tower\u201d to generate 20 stylized images for each n. The\nresults are shown in Fig. 9 and the loss curve is in Fig. 10.\nWe can observe that when n = 1 which reduces to Free-\n\ud835\udc5b = 1\n\ud835\udc5b = 2\n\ud835\udc5b = 3\n\ud835\udc5b = 4\n\ud835\udc5b = 5\n\ud835\udc5b = 9\nFigure 9. Stylization results with varying n.\nDOM [36]), the stylized images suffer from content distor-\ntion and less obvious stylization effect. As n increases, both\nthe quality of generated images and the reduction in loss\nbetween generated images and style images become more\nprominent. Notably, when n increases beyond 4, there is\nno significant decrease in loss, indicating that setting n to a\nlarge value is unnecessary. Besides, we notice that a small\nvalue of n, as long as greater than 1, could significantly\nhelp improve the quality of generated images. In most ex-\nperiments, we set n to be 4 or 5.\nGuidance scale \u03c1t.\nWe then study the influence of the\nguidance scale on the performance. Once again, we take\nstylization as an example and test the results under n = 1\nand n = 3. We gradually increase the guidance scale and\nshow the results in Fig. 11. We can observe that when the\nscale increases, the stylization becomes more obvious, but\nwhen the scale gets too large, the generated images suffer\nfrom severe artifacts.\nFigure 10. Loss curves for stylization under different n.\nSmall scale\nLarge scale\n\ud835\udc5b = 1\n\ud835\udc5b = 3\nFigure 11. Stylization results when increasing scales.\nChoice of guidance steps and repeat times of time\ntravel.\nFinally, we also conduct experiments to study at\n8\nwhich sampling steps should we do training-free guidance\nand the repeat times of time travel. As discussed in [36],\nthe diffusion sampling process roughly includes three\nstages: the chaotic stage where xt is highly noisy, the\nsemantic stage where xt presents some semantics and the\nrefinement stage where changes in the generated results\nare minimal.\nBesides, for the repeat times, intuitively,\nincreasing the repeat times extends the diffusion sam-\npling process and helps to explore results that satisfy\nboth guidance and image quality. Thus, in tasks such as\nstylization and aesthetic improvement that do not require\nchange in content, we only need to do guidance in the\nsemantic stage with a few repeat times (in these tasks, we\nset repeat times to 2). On the other hand, for tasks such\nas personalization, we need to perform guidance at the\nchaotic stage and use larger repeat times (here we set it to\n3). More ablation study results are shown in Appendix B.4.\nReferences\n[1] Lynton Ardizzone, Jakob Kruse, Carsten Rother, and Ullrich\nK\u00a8othe. Analyzing inverse problems with invertible neural\nnetworks. In International Conference on Learning Repre-\nsentations, 2018. 4\n[2] Arpit\nBansal,\nHong-Min\nChu,\nAvi\nSchwarzschild,\nSoumyadip Sengupta,\nMicah Goldblum,\nJonas Geip-\ning, and Tom Goldstein. Universal guidance for diffusion\nmodels.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR)\nWorkshops, pages 843\u2013852, 2023. 2, 3, 5, 6\n[3] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and\nDavid K Duvenaud.\nNeural ordinary differential equa-\ntions. In Advances in Neural Information Processing Sys-\ntems, 2018. 4\n[4] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mc-\ncann, Marc Louis Klasky, and Jong Chul Ye. Diffusion pos-\nterior sampling for general noisy inverse problems. In The\nEleventh International Conference on Learning Representa-\ntions, 2022. 4\n[5] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat GANs on image synthesis. In Advances in Neural In-\nformation Processing Systems, 2021. 2, 3\n[6] James F Epperson. An introduction to numerical methods\nand analysis. John Wiley & Sons, 2021. 4\n[7] Kang Feng and Mengzhao Qin. Symplectic geometric algo-\nrithms for Hamiltonian systems. Springer, 2010. 5\n[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An\nimage is worth one word: Personalizing text-to-image gener-\nation using textual inversion. In The Eleventh International\nConference on Learning Representations, 2023. 7\n[9] Ernst Hairer,\nChristian Lubich,\nand Gerhard Wanner.\nStructure-preserving algorithms for ordinary differential\nequations. Geometric numerical integration, 31, 2006. 1\n[10] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-or. Prompt-to-prompt image\nediting with cross-attention control. In The Eleventh Inter-\nnational Conference on Learning Representations, 2022. 3\n[11] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 2, 3\n[12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[13] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-\ntiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open\ndataset of user preferences for text-to-image generation.\narXiv preprint arXiv:2305.01569, 2023. 6, 2\n[14] Wei Li, Xue Xu, Xinyan Xiao, Jiachen Liu, Hu Yang, Guo-\nhao Li, Zhanpeng Wang, Zhifan Feng, Qiaoqiao She, Ya-\njuan Lyu, et al.\nUpainting: Unified text-to-image diffu-\nsion generation with cross-modal guidance. arXiv preprint\narXiv:2210.16031, 2022. 2\n[15] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu,\nand Jiashi Feng. Magicedit: High-fidelity and temporally\ncoherent video editing. In arXiv, 2023. 8\n[16] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,\nDanilo Mandic, Wenwu Wang, and Mark D Plumbley. Audi-\noLDM: Text-to-audio generation with latent diffusion mod-\nels. In Proceedings of the 40th International Conference on\nMachine Learning, pages 21450\u201321474. PMLR, 2023. 2\n[17] Xingchao Liu, Lemeng Wu, Shujian Zhang, Chengyue\nGong, Wei Ping, and Qiang Liu. Flowgrad: Controlling the\noutput of generative odes with gradients. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 24335\u201324344, 2023. 4\n[18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. DPM-Solver: A fast ODE solver for dif-\nfusion probabilistic model sampling in around 10 steps. In\nAdvances in Neural Information Processing Systems, 2022.\n5\n[19] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu.\nDPM-Solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 3\n[20] Takashi Matsubara, Yuto Miyatake, and Takaharu Yaguchi.\nSymplectic adjoint method for exact gradient of neural ode\nwith minimal memory. Advances in Neural Information Pro-\ncessing Systems, 34:20772\u201320784, 2021. 2, 4, 5, 1\n[21] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In International Conference on Learning Representa-\ntions, 2021. 3\n[22] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen.\nDreamix: Video Diffusion Models are General\nVideo Editors, 2023. arXiv:2302.01329 [cs]. 2\n[23] Jiachun Pan, Hanshu Yan, Jun Hao Liew, Vincent YF Tan,\nand Jiashi Feng. Adjointdpm: Adjoint sensitivity method for\ngradient backpropagation of diffusion probabilistic models.\narXiv preprint arXiv:2307.10711, 2023. 4\n9\n[24] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In ACM SIGGRAPH 2023 Conference Proceed-\nings, pages 1\u201311, 2023. 2, 3\n[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning Transferable Visual\nModels From Natural Language Supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. ISSN: 2640-3498. 6, 7\n[26] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 3, 6\n[27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 7\n[28] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi.\nPalette: Image-to-image diffusion models.\nIn\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u2013\n10, 2022. 2\n[29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. In Advances in Neural Information\nProcessing Systems, 2022. 2\n[30] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2020. 2, 3, 4, 5\n[31] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In International Conference on Learning Represen-\ntations, 2021. 2\n[32] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil\nNaik.\nEnd-to-end diffusion latent optimization improves\nclassifier guidance.\nIn Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n7280\u20137290, 2023. 4, 7, 2\n[33] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact\ndiffusion inversion via coupled transformations. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 22532\u201322541, 2023. 4\n[34] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie,\nand Mike Zheng Shou.\nTune-A-Video: One-Shot Tuning\nof Image Diffusion Models for Text-to-Video Generation,\n2023. arXiv:2212.11565 [cs]. 2\n[35] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng\nZhu, Rui Zhao, and Hongsheng Li. Human preference score\nv2: A solid benchmark for evaluating human preferences of\ntext-to-image synthesis. arXiv preprint arXiv:2306.09341,\n2023. 6, 2\n[36] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and\nJian Zhang. Freedom: Training-free energy-guided condi-\ntional diffusion model. In International Conference on Com-\nputer Vision (ICCV), 2023. 2, 3, 4, 5, 6, 7, 8, 9\n[37] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 2\n[38] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffu-\nsion models with exponential integrator. In The Eleventh In-\nternational Conference on Learning Representations, 2022.\n5\n[39] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng.\nMagicVideo:\nEfficient\nVideo Generation With Latent Diffusion Models, 2022.\narXiv:2211.11018 [cs]. 2\n10\nTowards Accurate Guided Diffusion Sampling through\nSymplectic Adjoint Method\nSupplementary Material\nA. Theoretical details on Symplectic Adjoint\nGuidance (SAG)\nA.1. Proof of Lemma 1\nWe know that the sub-process x\u2032\n\u03c4 and \u00afx\u2032\n\u03c3 also satisfy the\nfollowing ODE with the initial condition of \u00afx\u03c3t (i.e., xt):\nd\u00afx\u2032\n\u03c3\u03c4 = \u00af\u03f5(\u00afx\u2032\n\u03c3\u03c4 , \u03c3\u03c4)d\u03c3\u03c4.\n(9)\nThis means that when given the initial condition of xt, the\nsamples generated by solving the subprocess ODE (9) also\nshare the same conditional probability density p(x0|xt) as\nthe main generation process. Besides, we know that the\napproximation of final outputs using numerical solvers is\nrelated to discretization step size O(h\u03c3). In our paper, we\nusually discretize the time range [t, 0] using a uniform step\nsize. Thus, the approximation of final outputs is related to\nthe number of discretization steps n. When we use larger\nsteps to solve (9), the final solution is closer to the true one,\nwhich will make m smaller. Thus, we show the Lemma 1.\nA.2. Higher-order symplectic method\nWe introduce the first-order (i.e., Euler) symplectic method\nin Sec. 3. In this part, we introduce the higher-order sym-\nplectic method. We present the Symplectic Runge-Kutta\nmethod [20] as an example of a higher-order method. Let\n\u03c4 = [n, . . . , 1] denote the discrete steps corresponding to\nthe times from t to 0 and \u03c3\u03c4 = \u221a1 \u2212 \u03b1\u03c4/\u221a\u03b1\u03c4. In the Sym-\nplectic Runge-Kutta method, we solve the forward ODE (3)\nusing the Runge-Kutta solver:\n\u00afx\u2032\n\u03c3\u03c4\u22121 = \u00afx\u2032\n\u03c3\u03c4 + h\u03c3\u03c4\ns\nX\ni=1\nbik\u03c3\u03c4 ,i,\nk\u03c3\u03c4 ,i : = \u00af\u03f5( \u00afX\u03c3\u03c4 ,i, \u03c3\u03c4 + cih\u03c3\u03c4 ),\n\u00afX\u03c3\u03c4 ,i : = \u00afx\u2032\n\u03c3\u03c4 + h\u03c3\u03c4\ns\nX\nj=1\nai,jk\u03c3\u03c4 ,j,\n(10)\nwhere ai,j = 0 when j \u2265 i and the coefficients ai,j, bi, ci\nare summarized as the Butcher tableau [9]. Then when we\nsolve in the backward direction to obtain the gradients us-\ning the Symplectic Runge-Kutta method, we solve the ODE\nfunction related to the adjoint state by another Runge\u2013Kutta\nmethod with the same step size. It is expressed as\n\u2202L\n\u2202\u00afx\u2032\u03c3\u03c4\n=\n\u2202L\n\u2202\u00afx\u2032\u03c3\u03c4\u22121\n+ h\u03c3\u03c4\u22121\ns\nX\ni=1\nBil\u03c3\u03c4\u22121,i,\nl\u03c3\u03c4\u22121,i : = \u2212 \u2202\u00af\u03f5\n\u2202\u00afx\u2032 ( \u00afX\u03c3\u03c4\u22121,i, \u03c3\u03c4\u22121 + Cih\u03c3\u03c4\u22121)T \u039b\u03c3\u03c4\u22121,i,\n\u039b\u03c3\u03c4\u22121,i : =\n\u2202L\n\u2202\u00afx\u2032\u03c3\u03c4\u22121\n+ h\u03c3\u03c4\u22121\ns\nX\nj=1\nAi,jl\u03c3\u03c4\u22121,j.\n(11)\nThe conditions on the parameters are biAi,j + Bjaj,i \u2212\nbiBj = 0 for i, j = 1, . . . , s and Bi = bi \u0338= 0 and Ci = ci\nfor i = 1, . . . , s. Besides, the forward solutions {\u00afx\u2032\n\u03c3\u03c4 }n\n\u03c4=0\nneeds to save as checkpoints for the backward process.\nA.3. Proof of Theorem 3\nThe Symplectic Euler method we show in Sec. 3 is a special\ncase of the higher-order symplectic method when we set\ns = 1, b1 = 1, ci = 0 in the forward process and set s = 1\nand b1 = 1, B1 = 1, a1,1 = 1, A1,1 = 0, ci = Ci = 1 in\nthe backward process.\nTo show the formal expression of Theorem 3, we first\nintroduce a variational variable \u03b4(\u03c3\u03c4) =\n\u2202\u00afx\u2032\n\u03c3\u03c4\n\u2202\u00afx\u2032\u03c3t , which rep-\nresent the Jacobian of the state \u00afx\u2032\n\u03c3\u03c4 with respect to \u00afx\u2032\n\u03c3t. De-\nnote \u03bb(\u03c3\u03c4) =\n\u2202L\n\u2202\u00afx\u2032\u03c3\u03c4 and denote S(\u03b4, \u03bb) = \u03bbT \u03b4.\nTheorem 3 Let the gradient\n\u2202L\n\u2202\u00afx\u2032\u03c3t be the analytical solution\nto the continuous ODE in (5) and let\n\u2202L\n\u2202\u00afx\u2032\u03c3n be the gradient\nobtained by the symplectic Euler solver in (8) throughout\nthe discrete sampling process. Then, when S(\u03b4, \u03bb) is con-\nserved (i.e., time-invariant) for the continuous-time system,\nwe have\n\u2202L\n\u2202\u00afx\u2032\u03c3t =\n\u2202L\n\u2202\u00afx\u2032\u03c3n .\nProof As\nwe\nassume\nS(\u03b4, \u03bb)\nis\nconserved\nfor\nthe\ncontinuous-time system, we have\nd\nd\u03c3 S(\u03b4, \u03bb) = 0.\nThus we have\n\u03bbT d\u03b4\nd\u03c3 +\n\u0012d\u03bb\nd\u03c3\n\u0013T\n\u03b4 = 0.\nThis means that [20]\nS\n\u0012\u2202k\u03c3\u03c4 ,i\n\u2202\u00afx\u03c3t\n, \u039b\u03c3\u03c4 ,i\n\u0013\n+ S\n\u0012\u2202 \u00afX\u03c3\u03c4 ,i\n\u2202\u00afx\u03c3t\n, l\u03c3\u03c4 ,i\n\u0013\n= 0\n1\nBased on (7) and (8), we have\n\u03b4(\u03c3\u03c4+1) = \u03b4(\u03c3\u03c4) + h\u03c3\u03c4\n\u2202k\u03c3\u03c4 ,1\n\u2202\u00afx\u03c3t\n,\n\u03bb(\u03c3\u03c4+1) = \u03bb(\u03c3\u03c4) + h\u03c3\u03c4 l\u03c3\u03c4 ,1,\nwhich means\nS(\u03bb(\u03c3\u03c4+1), \u03b4(\u03c3\u03c4+1)) \u2212 S(\u03bb(\u03c3\u03c4), \u03b4(\u03c3\u03c4)) =\n= S\n\u0012\n\u03bb(\u03c3\u03c4) + h\u03c3\u03c4 l\u03c3\u03c4 ,1, \u03b4(\u03c3\u03c4) + h\u03c3\u03c4\n\u2202k\u03c3\u03c4 ,1\n\u2202\u00afx\u03c3t\n\u0013\n\u2212 S(\u03bb(\u03c3\u03c4), \u03b4(\u03c3\u03c4))\n= h\u03c3\u03c4 S\n\u0012\n\u03bb(\u03c3\u03c4), \u2202k\u03c3\u03c4 ,1\n\u2202\u00afx\u03c3t\n\u0013\n+ h\u03c3\u03c4 S(\u03b4(\u03c3\u03c4), l\u03c3\u03c4 ,1)\n+ h2\n\u03c3\u03c4 S\n\u0012\u2202k\u03c3\u03c4 ,1\n\u2202\u00afx\u03c3t\n, l\u03c3\u03c4 ,1\n\u0013\n(a)\n= h\u03c3\u03c4 S\n\u0012\n\u039b\u03c3\u03c4 ,i, \u2202k\u03c3\u03c4 ,1\n\u2202\u00afx\u03c3t\n\u0013\n+ h\u03c3\u03c4 S\n\u0012\u2202 \u00afX\u03c3\u03c4 ,1\n\u2202\u00afx\u03c3t\n\u2212 h\u03c3\u03c4\n\u2202k\u03c3\u03c4 ,1\n\u2202\u00afx\u03c3t\n, l\u03c3\u03c4 ,1\n\u0013\n+ h2\n\u03c3\u03c4 S\n\u0012\u2202k\u03c3\u03c4 ,1\n\u2202\u00afx\u03c3t\n, l\u03c3\u03c4 ,1\n\u0013\n= 0,\n(12)\nwhere the first term of (a) is based on (11) and the second\nterm of (a) is based on (13). Thus, we have\n\u03bb(\u03c3n)T \u03b4(\u03c3n)\n(a)\n= \u03bb(\u03c30)T \u03b4(\u03c30)\n(b)\n= \u03bb(\u03c3t)T \u03b4(\u03c3t),\n(13)\nwhere (a) is based on (12) and (b) is based on our assump-\ntion that S(\u03b4, \u03bb) is conserved for the continuous-time sys-\ntem. Then based on (13) and \u00afx\u2032\n\u03c3n = \u00afx\u2032\n\u03c3t, we have\n\u2202L(\u00afx\u2032\n\u03c30)\n\u2202\u00afx\u2032\u03c3n\n= \u2202L(\u00afx\u2032\n\u03c30)\n\u2202\u00afx\u2032\u03c30\n\u2202\u00afx\u2032\n\u03c30\n\u2202\u00afx\u2032\u03c3n\n= \u03bb(\u03c30)T \u03b4(\u03c30)\n= \u03bb(\u03c3t)T \u03b4(\u03c3t)\n= \u2202L(\u00afx\u2032\n\u03c30)\n\u2202\u00afx\u2032\u03c3t\n,\nwhich proves our theorem.\nB. Experimental Details on Guided Sampling\nin Image Generation\nB.1. Style-Guided Generation\nIn this section, we introduce the experimental details of\nstyle-guided sampling. Let sampling times be T = 100.\nWe set to do SAG from sampling steps t = 70 to t = 31\nand the repeats time from 70 and 61 as 1 and from 60 to\n31 as 2. For quantitative results, we select five style images\nand choose four prompts: [\u201dA cat wearing glasses.\u201d, \u201dA\nfantasy photo of volcanoes.\u201d, \u201dA photo of an Eiffel Tower.\u201d,\n\u201dbutterfly\u201d] to generate five images per prompt per style.\nFor the implementation of Universal Guidance [2] and Free-\nDOM [36], we use the officially released codes and generate\nthe results for quantitative comparison under sample style\nand prompts. Besides, the hyperparameter choice for these\ntwo models also follows the official implementations. More\nqualitative results are shown in Fig. 16.\nB.2. Aesthetic Improvement\nWhen we improve the aesthetics of generated images, we\nuse the weighted losses for LAION aesthetic predictor,7\nPickScore [13] and HPSv2 [35]. We set the weights for\neach aesthetic evaluation model as PickScore = 10, HPSv2\n= 2, Aesthetic = 0.5. Let sampling times be T = 100. We\nset to do SAG from sampling steps t = 70 to t = 31 and\nthe repeats time from 70 and 41 as 2 and from 40 to 31 as\n1. More qualitative results are shown in Fig. 17.\nB.3. Personalization\nFor personalization in the object-guided generation, we do\ntraining-free guidance from steps t = 100 to t = 31 and we\nset the repeat times as 2. We randomly select four reference\ndog images and select four prompts: A dog (at the Acropo-\nlis/swimming/in a bucket/wearing sunglasses). We generate\nfour images per prompt per image to measure the quantita-\ntive results. For the results of DOODL, we directly use the\nresults in the paper [32]. For the results of FreeDOM, we\nuse the special case of our model when we set n = 1. Let\nsampling times be T = 100. We set to do SAG from sam-\npling steps t = 100 to t = 31 and the repeats time from 100\nand 31 as 2. More qualitative results are shown in Fig. 18.\nB.4. Ablation Study\nAnalyses on Memory and Time Consumption\nWe con-\nducted our experiments on a V100 GPU. Memory consump-\ntion using SAG was observed to be 15.66GB, compared to\n15.64GB when employing ordinary adjoint guidance. No-\ntably, direct gradient backpropagation at n = 2 resulted in\na significantly higher memory usage of 28.63GB. Further-\nmore, as n increases, the memory requirement for direct\nbackpropagation shows a corresponding increase. In con-\ntrast, when using SAG, the memory consumption remains\nnearly constant regardless of the value of n.\nWe also present the time consumption associated with a\nsingle step of SAG for varying values of n in Fig. 12. As\nn increases, we observe a corresponding rise in time con-\nsumption. However, this increment in n also results in a\nsubstantial reduction in loss as shown in Fig. 10, indicating\n7https : / / github . com / LAION - AI / aesthetic -\npredictor.git\n2\n0.52\n2.69\n3.76\n4.86\n5.94\n11.38\n0\n2\n4\n6\n8\n10\n12\nn=1\nn=2\nn=3\nn=4\nn=5\nn=10\nFigure 12. Time consumption of single step SAG (seconds)\nrepeats = 1\n\ud835\udc5b = 4\n\ud835\udc5b = 4\nrepeats = 2\nrepeats = 3\n\ud835\udc5b = 1\n\ud835\udc5b = 1\nFigure 13. Stylization results when we use different repeat times\nof time travel.\na trade-off between computational time and the quality of\nresults.\nChoice of repeat times of time travel\nWe show some re-\nsults about the choice of repeat times in Fig. 13. We find\nthat increasing the repeat times helps the stylization. Be-\nsides, there still exists the distortion of images when n = 1\neven when we increase the repeat times.\nChoice of guidance steps\nWe present the qualitative re-\nsults regarding the selection of guidance steps in Fig. 14.\nWe can observe that initiating guidance in the early stages\n(i.e., the chaotic stage) results in final outputs that differ\nfrom those generated without guidance. Besides, starting\nguidance in the semantic stage allows us to maintain the\nintegrity of the original images while effectively achieving\nstyle transfer.\nC. More examples on Video Stylization\nTwo more groups of results of style-guided video stylization\nare shown in Figure 15.\nstart time t=100 start time t=70\nno guidance\nFigure 14. Stylization results when we start to do guidance at dif-\nferent time steps.\na photo of a sea lion lying on the sands\na photo of a car moving on the road, night, stars in the sky\nInput Video\nInput Video\nFigure 15. Examples on Video Stylization.\nA cat wearing glasses.\nA fantasy photo \nof volcanoes.\nA photo of an Eiffel \nTower.\nPop art painting of \nAlbert Einstein.\nbutterfly\nchimpanzee\ndrawing of a person \nplaying soccerv\nA racoon washing \ndishes.\nA stunning \nbeautiful painting of \na lion\nA fluffy owl sits atop a \nstack of antique books\nA knight riding a \nhorse.\nA painting of a deer\na painting of a tree\nA painting of a deer\na painting of a spaceship\na photo of a pikachu\nwearing a cute hat\nFigure 16. More examples of style-guided generation.\nD. Additional Qualitative Results on Image\nGeneration\n3\nThere are orange \nslices in canning \njars without lids.\nStable Diffusion V1.5\nSAG\nTwo kittens are cuddling \nand enjoying a soft \npillow\nA gouache painting \nby Claude Monet of \nships docked at the \nharbor.\nA digital painting of \na knight sitting by a \ncampfire.\n.\nSymmetrical Libra \nzodiac art by Brian \nFroud in a mystic style.\nImpressionist \npainting of\na cat, textured,\nhypermodern\nStable Diffusion V1.5\nSAG\nThe image depicts a \nfemale scientist holding \na small spinning black \nhole in a laboratory, \nillustrated in detailed \ndigital art style.\nA marker pen \ndrawing of a man \ninside a squid.\nA painting of a deer.\nDanny DeVito and Rhea \nPerlman playing Link \nand Zelda in a \ncinematic still.\nSymmetrical Libra zodiac \nart by Brian Froud in a \nmystic style.\nFigure 17. More examples of aesthetic improvements.\n4\n(a) A dog in the bucket.\n(b) A dog swimming.\n(c) A dog at Acropolis.\n(d) A dog wearing sunglasses.\nFigure 18. More examples on object-guided personalization.\n5\n"
  }
]