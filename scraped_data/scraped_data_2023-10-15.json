[
  {
    "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
    "link": "https://arxiv.org/pdf/2310.08491.pdf",
    "upvote": "48",
    "text": "PROMETHEUS: INDUCING FINE-GRAINED\nEVALUATION CAPABILITY IN LANGUAGE MODELS\nSeungone Kim1,2\u2217\u2020\nJamin Shin2,3\u2217\u2020\nYejin Cho1\u2217\u2020\nJoel Jang4\nShayne Longpre5\nHwaran Lee2,3\nSangdoo Yun2,3\nSeongjin Shin3\nSungdong Kim1,2,3\nJames Thorne1\nMinjoon Seo1\u2020\n1KAIST AI\n2NAVER AI Lab\n3NAVER Cloud\n4University of Washington\n5MIT\n{seungone, yejin cho, minjoon}@kaist.ac.kr\njamin.shin@outlook.com\nABSTRACT\nRecently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-\n4) as an evaluator for long-form responses has become the de facto standard.\nHowever, for practitioners with large-scale evaluation tasks and custom criteria\nin consideration (e.g., child-readability), using proprietary LLMs as an evalua-\ntor is unreliable due to the closed-source nature, uncontrolled versioning, and\nprohibitive costs. In this work, we propose PROMETHEUS, a fully open-source\nLLM that is on par with GPT-4\u2019s evaluation capabilities when the appropri-\nate reference materials (reference answer, score rubric) are accompanied. We\nfirst construct the FEEDBACK COLLECTION, a new dataset that consists of 1K\nfine-grained score rubrics, 20K instructions, and 100K responses and language\nfeedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train\nPROMETHEUS, a 13B evaluator LLM that can assess any given long-form text\nbased on customized score rubric provided by the user. Experimental results show\nthat PROMETHEUS scores a Pearson correlation of 0.897 with human evaluators\nwhen evaluating with 45 customized score rubrics, which is on par with GPT-4\n(0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring cor-\nrelation with GPT-4 with 1222 customized score rubrics across four benchmarks\n(MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends,\nbolstering PROMETHEUS\u2019s capability as an evaluator LLM. Lastly, PROMETHEUS\nachieves the highest accuracy on two human preference benchmarks (HHH Align-\nment & MT Bench Human Judgment) compared to open-sourced reward models\nexplicitly trained on human preference datasets, highlighting its potential as an\nuniversal reward model. We open-source our code, dataset, and model 1.\n1\nINTRODUCTION\nEvaluating the quality of machine-generated text has been a long-standing challenge in Natural\nLanguage Processing (NLP) and remains especially essential in the era of Large Language Models\n(LLMs) to understand their properties and behaviors (Liang et al., 2022; Chang et al., 2023; Zhong\net al., 2023; Chia et al., 2023; Holtzman et al., 2023). Human evaluation has consistently been the\npredominant method, for its inherent reliability and capacity to assess nuanced and subjective di-\nmensions in texts. In many situations, humans can naturally discern the most important factors of\nassessment, such as brevity, creativity, tone, and cultural sensitivities. On the other hand, conven-\ntional automated evaluation metrics (e.g., BLEU, ROUGE) cannot capture the depth and granularity\nof human evaluation (Papineni et al., 2002; Lin, 2004b; Zhang et al., 2019; Krishna et al., 2021).\nApplying LLMs (e.g. GPT-4) as an evaluator has received substantial attention due to its potential\nparity with human evaluation (Chiang & yi Lee, 2023; Dubois et al., 2023; Li et al., 2023; Liu et al.,\n\u2217denotes equal contribution. Work was done while Seungone was interning at NAVER AI Lab.\n\u2020Corresponding authors\n1https://kaistai.github.io/prometheus/\n1\narXiv:2310.08491v2  [cs.CL]  9 Mar 2024\nLLM User\nLLM Developer\nFine-grained Evaluation\nwith User-Defined Score Rubrics\nScore of a\nspecific domain or task\nScore based on\nHelpfulness/Harmlessness\nScore based on the\nCustomized Score Criteria\nOur Proposed Approach\nVicuna Bench,\nMT Bench,\nAlpacaFarm,\n...\nVicuna Bench,\nMT Bench,\nAlpacaFarm,\n...\n+\nMMLU,\nBig Bench,\nMATH,\nHumanEval,\n...\nConventional\u2028\nLLM Evaluation\nCoarse-Grained\nEvaluation\nGPT-4\nEvaluation\nAccuracy,\nEM, Rouge\nPrometheus\nEvaluation\nOnly\ndiagnoses about a\nspecific\ndomain or task\nOnly\ndiagnoses about the\ngeneral preference \nof the public\nClose-source Nature\n\nUncontrolled \nVersioning\n\nProhibitive Costs\nFully Open-source\n\nReproducible\nEvaluation\n\nInexpensive Costs\nGPT-4\nEvaluation\nWhich LLM is the\nmost humorous one \nout there?\nIs the LLM I\u2019m \ndeveloping inspiring \nwhile being culturally\nsensitive? \n<Instruction Set>\nScore Rubric: Does the \nresponse demonstrate \ncultural sensitivity\u009d\n\u00a8 Score 1: ~~~\n      ..\u008b\n\u00a8 Score 5: ~~~\n\nReference Answer:\n...\nFigure 1: Compared to conventional, coarse-grained LLM evaluation, we propose a fine-grained\napproach that takes user-defined score rubrics as input.\n2023; Peng et al., 2023; Zheng et al., 2023; Ye et al., 2023b; Min et al., 2023). Initial investigations\nand observations indicate that, when aptly prompted, LLMs can emulate the fineness of human\nevaluations. However, while the merits of using proprietary LLMs as an evaluation tool are evident,\nthere exist some critical disadvantages:\n1. Closed-source Nature: The proprietary nature of LLMs brings transparency concerns as\ninternal workings are not disclosed to the broader academic community. Such a lack of\ntransparency hinders collective academic efforts to refine or enhance its evaluation capa-\nbilities. Furthermore, this places fair evaluation, a core tenet in academia, under control of\nfor-profit entity and raises concerns about neutrality and autonomy.\n2. Uncontrolled Versioning: Proprietary models undergo version updates that are often be-\nyond the users\u2019 purview or control (Pozzobon et al., 2023). This introduces a reproducibil-\nity challenge. As reproducibility is a cornerstone of scientific inquiry, any inconsistency\nstemming from version changes can undermine the robustness of research findings that\ndepend on specific versions of the model, especially in the context of evaluation.\n3. Prohibitive Costs: Financial constraints associated with LLM APIs are not trivial. For\nexample, evaluating four LLMs variants across four sizes (ranging from 7B to 65B) using\nGPT-4 on 1000 evaluation instances can cost over $2000. Such scaling costs can be pro-\nhibitive, especially for academic institutions or researchers operating on limited budgets.\n2\nDespite these limitations, proprietary LLMs such as GPT-4 are able to evaluate scores based on\ncustomized score rubrics. Specifically, current resources are confined to generic, single-dimensional\nevaluation metrics that are either too domain/task-specific (e.g. EM, Rouge) or coarse-grained (e.g.\nhelpfulness/harmlessness (Dubois et al., 2023; Chiang et al., 2023; Liu et al., 2023) as shown in left-\nside of Figure 1. For instance, AlpacaFarm\u2019s (Dubois et al., 2023) prompt gives a single definition\nof preference, asking the model to choose the model response that is generally preferred. However,\nresponse preferences are subject to variation based on specific applications and values. In real-world\nscenarios, users may be interested in customized rubric such as \u201cWhich LLM generates responses\nthat are playful and humorous\u201d or \u201cWhich LLM answers with particularly care for cultural sensitiv-\nities?\u201d Yet, in our initial experiments, we observe that even the largest open-source LLM (70B) is\ninsufficient to evaluate based on a customized score rubric compared to proprietary LLMs.\nTo this end, we propose PROMETHEUS, a 13B LM that aims to induce fine-grained evaluation ca-\npability of GPT-4, while being open-source, reproducible, and inexpensive. We first create the\nFEEDBACK COLLECTION, a new dataset that is crafted to encapsulate diverse and fine-grained user\nassessment score rubric that represent realistic user demands (example shown in Figure 2). We de-\nsign the FEEDBACK COLLECTION with the aforementioned consideration in mind, encompassing\nthousands of unique preference criteria encoded by a user-injected score rubric. Unlike prior feed-\nback datasets (Ye et al., 2023a; Wang et al., 2023a), it uses custom, not generic preference score\nrubric, to train models to flexibly generalize to practical and diverse evaluation preferences. Also, to\nbest of our knowledge, we are first to explore the importance of including various reference materials\n\u2013 particularly the \u2018Reference Answers\u2019 \u2013 to effectively induce fine-grained evaluation capability.\nWe use the FEEDBACK COLLECTION to fine-tune Llama-2-Chat-13B in creating PROMETHEUS.\nOn 45 customized score rubrics sampled across three test sets (MT Bench, Vicuna Bench, Feed-\nback Bench), PROMETHEUS obtains a Pearson correlation of 0.897 with human evaluators, which\nis similar with GPT-4 (0.882), and has a significant gap with GPT-3.5-Turbo (0.392).\nUnex-\npectely, when asking human evaluators to choose a feedback with better quality in a pairwise set-\nting, PROMETHEUS was preferred over GPT-4 in 58.67% of the time, while greatly outperformed\nGPT-3.5-Turbo with a 79.57% win rate. Also, when measuring the Pearson correlation with GPT-4\nevaluation across 1222 customized score rubrics across 4 test sets (MT Bench, Vicuna Bench, Feed-\nback Bench, Flask Eval), PROMETHEUS showed higher correlation compared to GPT-3.5-Turbo and\nLlama-2-Chat 70B. Lastly, when testing on 2 unseen human preference datasets (MT Bench Human\nJudgments, HHH Alignment), PROMETHEUS outperforms two state-of-the-art reward models and\nGPT-3.5-Turbo, highlighting its potential as an universal reward model.\nOur contributions are summarized as follows:\n\u2022 We introduce the FEEDBACK COLLECTION dataset specifically designed to train an eval-\nuator LM. Compared to previous feedback datasets, it includes customized scoring rubrics\nand reference answers in addition to the instructions, responses, and feedback.\n\u2022 We train PROMETHEUS, the first open-source LLM specialized for fine-grained evalua-\ntion that can generalize to diverse, real-world scoring rubrics beyond a single-dimensional\npreference such as helpfulness and harmlessness.\n\u2022 We conduct extensive experiments showing that by appending reference materials (refer-\nence answers, fine-grained score rubrics) and fine-tuning on feedback, we can induce eval-\nuation capability into language models. PROMETHEUS shows high correlation with human\nevaluation, GPT-4 evaluation in absolute scoring settings, and also shows high accuracy in\nranking scoring settings.\n2\nRELATED WORK\nReference-based text evaluation\nPreviously, model-free scores that evaluate machine-generated\ntext based on a golden candidate reference such as BLEU (Papineni et al., 2002) and ROUGE (Lin,\n2004a) scores were used as the dominant approach. However, Krishna et al. (2021) reported limi-\ntations in reference-based metrics, such as ROUGE, observing that they are not reliable for evalua-\ntion. In recent years, model-based approaches have been widely adopted such as BERTScore (Zhang\net al., 2019), BLEURT (Sellam et al., 2020), and BARTScore (Yuan et al., 2021) which are able to\ncapture semantic information rather than only evaluating on lexical components.\n3\nPrometheus\nDoes the response consider regulations\nand compliance requirements?\n\n\n[Score 1] The response disregard \nregulations and compliance requirements.\n\n[Score 2] The response shows minimal \nawareness of regulatory compliance, \nmaking several errors.\n\n[Score 3] The response recognizes certain \nregulatory requirements but lacks depth.\n\n[Score 4] The response mostly considers\nregulatory compliance, \nwith only minor oversights.\n\n[Score 5] The response perfectly \naccounts for and integrates \nregulatory compliance requirements.\n\nTitle: Online Banking Service FinTech\nStartup Business Strategy\n\nExecutive Summary:\nOur startup, \u201cDigitalBank\u201d, aims to\nrevolutionize the traditional banking\nindustry by providing an innovative ...\nCustomized Score Rubric\nEstablishing a FinTech startup that offers\nonline banking services requires a\ncomprehensive strategy to tackle \nvarious challenges inherent to the industry \nand capitalize on market opportunities.\n..\u008d\n\u0086\u008d Market Research: ..\u008d\n\u0082\u008d Regulatory and Compliance: ..\u008d\n\u0081\u008d Product Development: ..\u008d\n\u00a0\u008d Service Differentiation: ..\u008d\n\u007f\u008d Go-to-Market Strategy: ..\u008d\n\u009f\u008d Growth and Expansion: ..\u008d\n\u009b\u008d Sustainability and Trust: ...\n\nThese items are important aspects to\nconsider when starting a Fintech startup\nthat offers online banking services.\nReference Answer\nResponse to Evaluate\nDevelop me a business strategy for\ncreating a FinTech startup \nthat offers online banking services.\nInstruction\nThe response provides \na comprehensive overview.\n...\nOverall, it includes a detailed\nplan of how to start a FinTech\nstartup by presenting an\nimaginary startup \u201cDigitalBank\u201d.\n...\nHowever, it lacks a deeper dive\ninto regulatory compliance.\n...\nFeedback\nScore\n3 out of 5\nFigure 2: The individual components of the FEEDBACK COLLECTION. By adding the appropriate\nreference materials (Score Rubric and Reference Answer) and training on GPT-4\u2019s feedback, we\nshow that we could obtain a strong open-source evaluator LM.\nLLM-based text evaluation\nRecent work has used GPT-4 or a fine-tuned critique LLM as an\nevaluator along a single dimension of \u201cpreference\u201d (Chiang & yi Lee, 2023; Li et al., 2023; Dubois\net al., 2023; Zheng et al., 2023; Liu et al., 2023). For instance, AlpacaFarm (Dubois et al., 2023) asks\nthe model to select \u201cwhich response is better based on your judgment and based on your own pref-\nerence\u201d Another example is recent work that showed ChatGPT can outperform crowd-workers for\ntext-annotation tasks (Gilardi et al., 2023; Chiang & yi Lee, 2023). Wang et al. (2023b) introduced\nPandaLM, a fine-tuned LLM to evaluate the generated text and explain its reliability on various\npreference datasets. Similarly, Ye et al. (2023a) and Wang et al. (2023a) create critique LLMs.\nHowever, the correct preference is often subjective and depends on applications, cultures, and ob-\njectives, where degrees of brevity, formality, honesty, creativity, and political tone, among many\nother potentially desirable traits that may vary (Chiang & yi Lee, 2023). While GPT-4 is unreliable\ndue to its close-source nature, uncontrolled versioning, and prohibitive costs, it was the only option\nexplored for fine-grained and customized evaluation based on the score rubric (Ye et al., 2023b). On\nthe contrary, we train, to best of our knowledge, the first evaluator sensitive to thousands of unique\npreference criteria, and show it significantly outperforms uni-dimensional preference evaluators in\na number of realistic settings. Most importantly, compared to previous work, we strongly argue\nthe importance of appending reference materials (score rubric and reference answer) in addition to\nfine-tuning on the feedback in order to effectively induce fine-grained evaluation capability.\n3\nTHE FEEDBACK COLLECTION DATASET\nWhile previous work has demonstrated that fine-tuning on feedback is effective for improving LMs\nto function as a critique (Ye et al., 2023a; Wang et al., 2023a), the datasets used in previous work\nare not directly applicable for improving LMs to function as a fine-grained evaluator. We thus\nintroduce the FEEDBACK COLLECTION, a new dataset for the sole purpose of fine-tuning an open-\nsourced evaluator LLM. Our 4 main considerations during dataset construction are: (1) including as\nmany reference materials (i.e. reference answer, and scoring rubric) as possible, (2) maintaining a\nuniform length among the reference answers for each score (1 to 5) to prevent undesired length bias,\n(3) maintaining a uniform score distribution to prevent undesired decision bias, and (4) limiting the\nscope of our instructions and responses to realistic situations where a user is interacting with a LLM.\n4\nTable 1: Information about our training dataset FEEDBACK COLLECTION. Note that there are 20\ninstructions accompanied for each score rubric, leading to a total number of 20K. Also, there is a\nscore 1-5 response and feedback for each instruction, leading to a total number of 100K.\nEvaluation Mode\nData\n# Score Rubrics\n# Instructions & Reference Answer\n# Responses & Feedback\nAbsolute Evaluation\nFEEDBACK COLLECTION\n1K (Fine-grained &\nCustomized)\nTotal 20K\n(20 for each score rubric)\nTotal 100K(5 for each instruction;\n20K for each score within 1-5)\nTaking these into consideration, we construct each instance within the FEEDBACK COLLECTION\nto encompass four components for the input (instruction, response to evaluate, customized score\nrubric, reference answer) and two components in the output (feedback, score). An example of an\ninstance is shown in Figure 2 and the number of each component is shown in Table 1.\nThe four components for the input are as follows:\n1. Instruction: An instruction that a user would prompt to an arbitrary LLM.\n2. Response to Evaluate: A response to the instruction that the evaluator LM has to evaluate.\n3. Customized Score Rubric: A specification of novel criteria decided by the user. The eval-\nuator should focus on this aspect during evaluation. The rubric consists of (1) a description\nof the criteria and (2) a description of each scoring decision (1 to 5).\n4. Reference Answer: A reference answer that would receive a score of 5. Instead of re-\nquiring the evaluator LM to solve the instruction, it enables the evaluator to use the mutual\ninformation between the reference answer and the response to make a scoring decision.\nThe two components for the output are as follows:\n1. Feedback: A rationale of why the provided response would receive a particular score. This\nis analogous to Chain-of-Thoughts (CoT), making the evaluation process interpretable.\n2. Score: An integer score for the provided response that ranges from 1 to 5.\nEach instance has an accompanying scoring rubric and reference answer upon the instruction in\norder to include as much reference material as possible. Also, we include an equal number of 20K\ninstances for each score in the range of 1 to 5, preventing undesired decision bias while training the\nevaluator LLM. A detailed analysis of the FEEDBACK COLLECTION dataset is in Appendix A.\n3.1\nDATASET CONSTRUCTION PROCESS\nWe construct a large-scale FEEDBACK COLLECTION dataset by prompting GPT-4. Specifically,\nthe collection process consists of (1) the curation of 50 initial seed rubrics, (2) the expansion of\n1K new score rubrics through GPT-4, (3) the augmentation of realistic instructions, and (4) the\naugmentation of the remaining components in the training instances (i.e. responses including the\nreference answers, feedback, and scores). Figure 3 shows the overall augmentation process.\nStep 1: Creation of the Seed Rubrics\nWe begin with the creation of a foundational seed dataset\nof scoring rubrics. Each author curates a detailed and fine-grained scoring rubric that each personnel\nconsiders pivotal in evaluating outputs from LLMs. This results in an initial batch of 50 seed rubrics.\nStep 2: Augmenting the Seed Rubrics with GPT-4\nUsing GPT-4 and our initial seed rubrics, we\nexpand the score rubrics from the initial 50 to a more robust and diverse set of 1000 score rubrics.\nSpecifically, by sampling 4 random score rubrics from the initial seed, we use them as demonstra-\ntions for in-context learning (ICL), and prompt GPT-4 to brainstorm a new novel score rubric. Also,\nwe prompt GPT-4 to paraphrase the newly generated rubrics in order to ensure PROMETHEUS could\ngeneralize to the similar score rubric that uses different words. We iterate the brainstorming \u2192\nparaphrasing process for 10 rounds. The detailed prompt used for this procedure is in Appendix G.\nStep 3: Crafting Novel Instructions related to the Score Rubrics\nWith a comprehensive dataset\nof 1000 rubrics at our disposal, the subsequent challenge was to craft pertinent training instances.\n5\nAdaptive \nCommunication\nAmbiguity \nNavigation\nGlobal Cultural \nAwareness\nEmotional Intelligence \nin Communication\nSpecialized Language \nMastery\nCultural Sensitivity & \nRespect\nInnovative Problem-\nSolving\nError & Misinterpretation \nManagement\nIndustry & Technical\n Language Proficiency\nSafe & Engaging Content Delivery\nTone & Style Modulation\nContextual Language \nAdaptation\nEmotionally Attuned Responses\nProactive \nClarification Seeking\nAmbiguous Input Clarification\nUnclear Query Handling\nSensitivity to Cultural \nDifferences\nRespect for Cultural \nDiversity\nUnderstanding of Global \nTraditions\nEmotionally Considerate\nCommunication\nRecognition & Acknowledgment \nof Emotions\nEmpathetic & Supportive Responses\nPrecise Technical \nLanguage Use\nTechnical Vocabulary \nInterpretation\nSector-Specific Terminology\nNuanced Cultural Understanding\nAvoidance of Cultural Stereotypes\nSocietal Manners & Etiquette\nConflict Resolution \nStrategies\nCreative Solution Crafting\nNovel Idea Generation\nError Identification & Correction\nMisinterpretation Mitigation\nEffective Misconception Handling\nAccurate Industry \nTerminology Use\nSpecialized Language Fluency\nTechnical Term Mastery\nChild Safety Promotion\nClear & Concise Information Provision\nEngagement Enhancement Strategies\nEmotionally Attuned Responses\nTechnical Term\nMastery\nNovel Idea \nGeneration\n...\n2\n1\n3\n4\nSeed \nScore Rubrics\nFeedback Collection Instance Augmentation\nFeedback Collection\nScore Rubrics (1K)\nIs the answer written \nprofessionally and formally, \nso that I could \nsend it to my boss?\n\n[Score 1] The answer lacks\nany sense of professionalism \nand is informal.\n\n...\n\n[Score 5] The answer is \ncompletely professional \nand suitable for a format setting.\nI am an entry level employee at\na multinational corporation and\nI have been asked to write a\nreport on the current trends\nin our industry.\n\nI am unsure how to structure\nthe report and level of formality\nrequired.The report will be read \nby my immediate supervisor, \nthe regional manager, \nand potentially the CEO.\n\nCan you give me a sample of\nhow the report \nshould be written?\nDoes the response utilize\nappropriate professional jargon\nand terminology suited for an\nacademic or expert audience?\n\n[Score 1] The response misuses\nterms or avoids professional\nlanguage entirely.\n...\n\n[Score 5] The response\nperfectly utilizes professional\nterms, ensuring accuracy and\ncomprehensibility for experts.\nIs the answer explained like\na formal proof?\n\n[Score 1] The answer lacks any\nstructure resembling a\nformal proof.\n\n...\n\n[Score 5] The answer is\nstructured and explained\nexactly like a formal proof.\nCustomized Score Rubric\nInstruction\nReference Answer (Score 5) &\nResponses to Evaluate\nHere\u2019s a plan you can follow.\n\nFirst, do research on all the\nlatest stuff in your industry.\nThen, create an outline to\nkeep your thoughts organized.\nIn the report, include a title page,\nexecutive summary, introduction,\nbody, conclusion, and references.\n\nKeep your language formal, but\ndon\u2019t be too stiff. Use headings,\nsubheadings, and add charts or\ngraphs. Here\u2019s an example:\n...\nThe response provides a helpful\nguide to approaching the task,\nbut it could be more professional\nin tone and phrasing.\n\nSome sentences feel too casual\nand informal for a report to be\nread by the supervisor or CEO.\n\nIt does, however, provide a clear\noutline and an example\nparagraph that gives a sense of\nhow the report should be written.\nSo, the overall score is 3.\nFeedback\nScore\nFigure 3: Overview of the augmentation process of the FEEDBACK COLLECTION. The keywords\nincluded within the score rubrics of the FEEDBACK COLLECTION is also displayed.\nFor example, a score rubric asking \u201cIs it formal enough to send to my boss\u201d is not related to a\nmath problem. Considering the need for a set of instructions closely related to the score rubrics, we\nprompt GPT-4 to generate 20K unique instructions that are highly relevant to the given score rubric.\nStep 4: Crafting Training Instances\nLastly, we sequentially generate a response to evaluate and\ncorresponding feedback by prompting GPT-4 to generate each component that will get a score of i\n(1 \u2264 i \u2264 5). This leads to 20 instructions for each score rubric, and 5 responses & feedback for each\ninstruction. To eliminate the effect of decision bias when fine-tuning our evaluator LM, we generate\nan equal number of 20K responses for each score. Note that for the response with a score of 5, we\ngenerate two distinctive responses so we could use one of them as an input (reference answer).\n3.2\nFINE-TUNING AN EVALUATOR LM\nUsing the FEEDBACK COLLECTION dataset, we fine-tune Llama-2-Chat (7B & 13B) and obtain\nPROMETHEUS to induce fine-grained evaluation capability.\nSimilar to Chain-of-Thought Fine-\ntuning (Ho et al., 2022; Kim et al., 2023a), we fine-tune to sequentially generate the feedback and\nthen the score. We highlight that it is important to include a phrase such as \u2018[RESULT]\u2019 in be-\ntween the feedback and the score to prevent degeneration during inference. We include the details\nof fine-tuning, inference, and ablation experiments (reference materials, base model) in Appendix C.\n6\nTable 2: Information about the datasets we use to test evaulator LMs. Note that FEEDBACK BENCH\nis a dataset that is crafted with the exact same procedure as the FEEDBACK COLLECTION as ex-\nplained in Section 3.1. We include additional analysis of FEEDBACK BENCH in Appendix B. Sim-\nulated GPT-4 \u2020 denotes GPT-4 prompted to write a score of i (1 \u2264 i \u2264 5) during augmentation.\nEvaluation Mode\nEvaluation Data\nSource / Types of Score Rubric\nResponse LMs\nAbsolute Evaluation\nFEEDBACK BENCH (Seen Rubric)\n1K Machine Generated\nSimluated GPT-4 \u2020\nFEEDBACK BENCH (Unseen Rubric)\n50 Hand Crafted\nVicuna Bench\n80 Hand Crafted\nWizardLM, Vicuna,\nLlama2-Chat, ChatGPT\nMT Bench\nFlask Eval\nLogical Thinking (3), Background Knowledge (2)\nProblem Handling (4), User Alignment (3)\nAlpaca, Vicuna, Bard, ChatGPT\nRanking Evaluation\nMT Bench Human Judgments\nHuman Preference (Helpfulness)\nAlpaca, Llama, Vicuna, ChatGPT,\nClaude-v1, GPT-4\nHHH Alignment\nHelpfulness, Harmlessness,\nHonesty, Other\nHuman Annotation\n4\nEXPERIMENTAL SETTING: EVALUATING AN EVALUATOR LM\nIn this section, we explain our experiment setting, including the list of experiments, metrics, and\nbaselines that we use to evaluate fine-grained evaluation capabilities of an evaluator LLM. Compared\nto measuring the instruction-following capability of a LLM, it is not straightforward to directly\nmeasure the capability to evaluate. Therefore, we use human evaluation and GPT-4 evaluation as\na standard and measure how similarly our evaluator model and baselines could closely simulate\nthem. We mainly employ two types of evaluation methods: Absolute Grading and Ranking Grading.\nDetailed information on the datasets used for the experiment is included in Table 2.\n4.1\nLIST OF EXPERIMENTS AND METRICS\nAbsolute Grading\nWe first test in an Absolute Grading setting, where the evaluator LM should\ngenerate a feedback and score within the range of 1 to 5 given an instruction, a response to evaluate,\nand reference materials (as shown in Figure 2). Absolute Grading is challenging compared to Rank-\ning Grading since the evaluator LM does not have access to an opponent to compare with and it is\nrequired to provide a score solely based on its internal decision. Yet, it is more practical for users\nsince it relieves the need to prepare an opponent to compare with during evaluation.\nWe mainly conduct three experiments in this setting: (1) measuring the correlation with human\nevaluators (Section 5.1), (2) comparing the quality of the feedback using human evaluation (Sec-\ntion 5.1), and (3) measuring the correlation with GPT-4 evaluation (Section 5.2). For the experiments\nthat measure the correlation, we use 3 different correlation metrics: Pearson, Kdendall-Tau, and\nSpearman. For measuring the quality of the generated feedback, we conduct a pairwise compari-\nson between the feedback generated by PROMETHEUS, GPT-3.5-Turbo, and GPT-4, asking human\nevaluators to choose which has better quality and why they thought so. Specifically, we recruited 9\ncrowdsource workers and split them into three groups: PROMETHEUS vs GPT-4, PROMETHEUS vs\nChatGPT, and GPT-4 vs ChatGPT. The annotators are asked to answer the following three questions:\n1. What score would you give to the response based on the given score rubric?\n2. Among the two Feedback, which is better for critiquing the given response?\n3. Why did you reject that particular feedback?\nWe use the following four benchmarks to measure the correlation with human evaluation and GPT-4\nevaluation. Note that FEEDBACK BENCH is a dataset generated with the same procedure as the\nFEEDBACK COLLECTION, and is divided into two subsets (Seen Rubric and Unseen Rubric).\n\u2022 FEEDBACK BENCH: The Seen Rubric subset shares the same 1K score rubrics with the\nFEEDBACK COLLECTION across 1K instructions (1 per score rubric). The Unseen Rubric\nsubset also consists of 1K new instructions but with 50 new score rubrics that are generated\nthe same way as the training set. Details are included in Appendix B.\n7\n\u2022 Vicuna Bench: We adapt the 80 test prompt set from Vicuna (Chiang et al., 2023) and\nhand-craft customized score rubrics for each test prompt. In order to obtain reference\nanswers, we concatenate the hand-crafted score rubric and instruction to prompt GPT-4.\n\u2022 MT Bench: We adapt the 80 test prompt set from MT Bench (Zheng et al., 2023), a multi-\nturn instruction dataset. We hand-craft customized score rubrics and generate a reference\nanswer using GPT-4 for each test prompt as well. Note that we only use the last turn of this\ndataset for evaluation, providing the previous dialogue as input to the evaluator LM.\n\u2022 FLASK Eval: We adapt the 200 test prompt set from FLASK (Ye et al., 2023b), a fine-\ngrained evaluation dataset that includes multiple conventional NLP datasets and instruction\ndatasets. We use the 12 score rubrics (that are relatively coarse-grained compared to the 1K\nscore rubrics used in the FEEDBACK COLLECTION) such as Logical Thinking, Background\nKnowledge, Problem Handling, and User Alignment.\nRanking Grading\nTo test if an evaluator LM trained only on Absolute Grading could be utilized\nas a universal reward model based on any criteria, we use existing human preference benchmarks and\nuse accuracy as our metric (Section 5.3). Specifically, we check whether the evaluator LM could\ngive a higher score to the response that is preferred by human evaluators. The biggest challenge\nof employing an evaluator LM trained in an Absolute Grading setting and testing it on Ranking\nGrading was that it could give the same score for both candidates. Therefore, we use a temperature\nof 1.0 when evaluating each candidate independently and iterate until there is a winner. Hence, it\u2019s\nnoteworthy that the settings are not exactly fair compared to other ranking models. This setting is\nNOT designed to claim SOTA position in these benchmarks, but is conducted only for the purpose\nof checking whether an evaluator LM trained in an Absolute Grading setting could also generalize\nin a Ranking Grading setting according to general human preference. Also, in this setting, we do\nnot provide a reference answer to check whether PROMETHEUS could function as a reward model.\nWe use the following two benchmarks to measure the accuracy with human preference datasets:\n\u2022 MT Bench Human Judgement: This data is another version of the aforementioned MT\nBench (Zheng et al., 2023). Note that it includes a tie option as well and does not require\niterative inference to obtain a clear winner. We use Human Preference as our criteria.\n\u2022 HHH Alignment: Introduced by Anthropic (Askell et al., 2021), this dataset (221 pairs) is\none of the most widely chosen reward-model test-beds that measures preference accuracy in\nHelpfulness, Harmlessness, Honesty, and in General (Other) among two response choices.\n4.2\nBASELINES\nThe following list shows the baselines we used for comparison in the experiments:\n\u2022 LLAMA2-CHAT-{7,13,70}B (Touvron et al., 2023): The base model of PROMETHEUS\nwhen fine-tuning on the FEEDBACK COLLECTION. Also, it is considered as the best option\namong the open-source LLMs, which we use as an evaluator in this work.\n\u2022 LLAMA-2-CHAT-13B + COARSE: To analyze the effectiveness of training on thousands\nof fine-grained score rubrics, we train a comparing model only using 12 coarse-grained\nscore rubrics from Ye et al. (2023b). Detailed information on this model is in Appendix D.\n\u2022 GPT-3.5-TURBO-0613: Proprietary LLM that offers a cheaper price when employed as\nan evaluator LLM. While it is relatively inexpensive compared to GPT-4, it still has the\nissue of uncontrolled versioning and close-source nature.\n\u2022 GPT-4-{0314,0613, RECENT}: One of the most powerful proprietary LLM that is con-\nsidered the main option when using LLMs as evaluators. Despite its reliability as an eval-\nuator LM due to its superior performance, it has several issues of prohibitive costs, uncon-\ntrolled versioning, and close-source nature.\n\u2022 STANFORDNLP REWARD MODEL2: One of the state-of-the-art reward model directly\ntrained on multiple human preference datasets in a ranking grading setting.\n\u2022 ALMOST REWARD MODEL (Kim et al., 2023b): Another state-of-the-art reward model\ntrained on synthetic preference datasets in a ranking grading setting.\n2https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl\n8\nPearson Correlation Between LLM Evaluators and Human Evaluators Scores\nPearson Correlation (higher is better)\nOverall\nFeedback Collection \n(Unseen)\nMT Bench\nVicuna Bench\nGPT-3.5\nturbo\nGPT-4\nPrometheus\n13B\nFigure 4: The Pearson correlation between scores from human annotators and the score from GPT-\n3.5-Turbo, Prometheus, and GPT-4 on 45 customized score rubrics from the Feedback Bench, Vi-\ncuna Bench, and MT Bench. PROMETHEUS shows a high correlation with human evaluators.\nPrometheus\nvs \nChatGPT\nPrometheus\nvs\nGPT-4\nGPT-4\nvs\nChatGPT\nLeft Wins\nRight Wins\nBoth are Good\nBoth are Bad\nFigure 5: Pairwise comparison of the quality of the feedback generated by GPT-4, PROMETHEUS\nand GPT-3.5-Turbo. Annotators are asked to choose which feedback is better at assessing the given\nresponse. PROMETHEUS shows a win-rate of 58.62% over GPT-4 and 79.57% over GPT-3.5-Turbo.\n5\nEXPERIMENTAL RESULTS\n5.1\nCAN PROMETHEUS CLOSELY SIMULATE HUMAN EVALUATION?\nCorrelation with Human Scoring\nWe first compare the correlation between human annotators\nand our baselines using 45 instances each with an unique customized score rubric, namely the\nFEEDBACK BENCH (Unseen Score Rubric subset), MT Bench (Zheng et al., 2023), and Vicuna\nBench (Chiang et al., 2023). The results are shown in Figure 4, showing that PROMETHEUS is\non par with GPT-4 across all the three evaluation datasets, where PROMETHEUS obtains a 0.897\nPearson correlation, GPT-4 obtains 0.882, and GPT-3.5-Turbo obtains 0.392.\nPairwise Comparison of the Feedback with Human Evaluation\nTo validate the effect of\nwhether PROMETHEUS generates helpful/meaningful feedback in addition to its scoring decision,\nwe ask human annotators to choose a better feedback. The results are shown in Figure 5, showing\nthat PROMETHEUS is preferred over GPT-4 58.62% of the times, and over GPT-3.5-Turbo 79.57%\nof the times. This shows that PROMETHEUS\u2019s feedback is also meaningful and helpful.\n9\nFigure 6: The reason why GPT-4\u2019s or Prometheus\u2019s feedback was not chosen over the other.\nPROMETHEUS generates less abstract and general feedback, but tends to write overly critical ones.\nAnalysis of Why Prometheus\u2019s Feedback was Preferred\nIn addition to a pairwise comparison of\nthe feedback quality, we also conduct an analysis asking human annotators to choose why they pre-\nferred one feedback over the other by choosing at least one of the comprehensive 6 options (\u201crejected\nfeedback is not consistent with its score\u201d / \u201ctoo general and abstract\u201d / \u201coverly optimistic\u201d / \u201cnot rel-\nevant to the response\u201d / \u201coverly critical\u201d / \u201cunrelated to the score rubric\u201d). In Figure 6, we show\nthe percentage of why each evaluator LLM (GPT-4 and PROMETHEUS) was rejected. It shows that\nwhile GPT-4 was mainly not chosen due to providing general or abstract feedback, PROMETHEUS\nwas mainly not chosen because it was too critical about the given response. Based on this result, we\nconclude that whereas GPT-4 tends to be more neutral and abstract, PROMETHEUS shows a clear\ntrend of expressing its opinion of whether the given response is good or not. We conjecture this is an\neffect of directly fine-tuning PROMETHEUS to ONLY perform fine-grained evaluation, essentially\nconverting it to an evaluator rather than a generator. We include (1) additional results of analyz-\ning \u201cPROMETHEUS vs GPT-3.5-Turbo\u201d and \u201cGPT-4 vs GPT-3.5-Turbo\u201d in Appendix E and (2) a\ndetailed explanation of the experimental setting of human evaluation in Appendix J.\n5.2\nCAN PROMETHEUS CLOSELY SIMULATE GPT-4 EVALUATION?\nCorrelation with GPT-4 Scoring\nWe compare the correlation between GPT-4 evaluation and our\nbaselines using 1222 score rubrics across 2360 instances from the FEEDBACK BENCH (Seen and\nUnseen Score Rubric Subset), Vicuna Bench (Chiang et al., 2023), MT Bench (Zheng et al., 2023),\nand Flask Eval (Ye et al., 2023b) in an absolute grading scheme. Note that for the FEEDBACK\nBENCH, we measure the correlation with the scores augmented from GPT-4-0613, and for the\nother 3 datasets, we measure the correlation with the scores acquired by inferencing GPT-4-0613.\nThe results on these benchmarks are shown across Table 3 and Table 4.\nIn Table 3, the performance of LLAMA-2-CHAT 13B degrades over the 7B model and slightly\nimproves when scaled up to 70B size, indicating that naively increasing the size of a model does\nnot necessarily improve an LLM\u2019s evaluation capabilities. On the other hand, PROMETHEUS 13B\nshows a +0.420 and +0.397 improvement over its base model LLAMA2-CHAT 13B in terms of\nPearson correlation on the seen and unseen rubric set, respectively. Moreover, it even outperforms\nLLAMA2-CHAT 70B, GPT-3.5-TURBO-0613, and different versions of GPT-4. We conjecture\nthe high performance of PROMETHEUS is mainly because the instructions and responses within the\n10\nTable 3: Pearson, Kendall-Tau, Spearman correlation with data generated by GPT-4-0613. All scores were\nsampled across 3 inferences. The best comparable statistics are bolded and second best underlined.\nEvaluator LM\nFEEDBACK COLLECTION TEST SET (GENERATED BY GPT-4-0613)\nSEEN CUSTOMIZED RUBRICS\nUNSEEN CUSTOMIZED RUBRIC\nPearson\nKendall-Tau\nSpearman\nPearson\nKendall-Tau\nSpearman\nLLAMA2-CHAT 7B\n0.485\n0.422\n0.478\n0.463\n0.402\n0.465\nLLAMA2-CHAT 13B\n0.441\n0.387\n0.452\n0.450\n0.379\n0.431\nLLAMA2-CHAT 70B\n0.572\n0.491\n0.564\n0.558\n0.477\n0.549\nLLAMA2-CHAT 13B + COARSE.\n0.482\n0.406\n0.475\n0.454\n0.361\n0.427\nPROMETHEUS 7B\n0.860\n0.781\n0.863\n0.847\n0.767\n0.849\nPROMETHEUS 13B\n0.861\n0.776\n0.858\n0.860\n0.771\n0.858\nGPT-3.5-TURBO-0613\n0.636\n0.536\n0.617\n0.563\n0.453\n0.521\nGPT-4-0314\n0.754\n0.671\n0.762\n0.753\n0.673\n0.761\nGPT-4-0613\n0.742\n0.659\n0.747\n0.743\n0.660\n0.747\nGPT-4 (RECENT)\n0.745\n0.659\n0.748\n0.733\n0.641\n0.728\nTable 4: Pearson, Kendall-Tau, Spearman correlation with scores sampled from GPT-4-0613 across 3 infer-\nences. Note that GPT-4-0613 was sampled 6 times in total to measure self-consistency. The best comparable\nstatistics are bolded and second best underlined among baselines. We include GPT-4 as reference to show it\nself-consistency when inferenced multiple times.\nEvaluator LM\nVICUNA BENCH\nMT BENCH\nFLASK EVAL\nPearson\nKendall-Tau\nSpearman\nPearson\nKendall-Tau\nSpearman\nPearson\nKendall-Tau\nSpearman\nLLAMA2-CHAT 7B\n0.175\n0.143\n0.176\n0.132\n0.113\n0.143\n0.271\n0.180\n0.235\nLLAMA2-CHAT 13B\n0.211\n0.203\n0.253\n-0.020\n-0.029\n-0.038\n0.265\n0.182\n0.235\nLLAMA2-CHAT 70B\n0.376\n0.318\n0.391\n0.226\n0.175\n0.224\n0.336\n0.267\n0.346\nLLAMA2-CHAT 13B + COARSE.\n0.307\n0.196\n0.245\n0.417\n0.328\n0.420\n0.517\n0.349\n0.451\nPROMETHEUS-7B\n0.457\n0.365\n0.457\n0.293\n0.216\n0.295\n0.367\n0.285\n0.371\nPROMETHEUS-13B\n0.466\n0.346\n0.429\n0.473\n0.341\n0.451\n0.467\n0.345\n0.455\nGPT-3.5-TURBO-0613\n0.270\n0.187\n0.232\n0.275\n0.202\n0.267\n0.422\n0.299\n0.371\nGPT-4-0314\n0.833\n0.679\n0.775\n0.857\n0.713\n0.849\n0.785\n0.621\n0.747\nGPT-4-0613\n0.925\n0.783\n0.864\n0.952\n0.834\n0.927\n0.835\n0.672\n0.798\nGPT-4 (RECENT)\n0.932\n0.801\n0.877\n0.944\n0.812\n0.914\n0.832\n0.667\n0.794\ntest set might share a similar distribution with the train set we used (simulating a scenario where\na user is interacting with a LLM) even if the score rubric holds unseen. Also, training on feed-\nback derived from coarse-grained score rubrics (denoted as LLAMA2-CHAT 13B + COARSE) only\nslightly improves performance, indicating the importance of training on a wide range of score rubric\nis important to handle customized rubrics that different LLM user or researcher would desire.\nIn Table 4, the trends of LLAMA2-CHAT among different sizes hold similar; simply increasing size\ndoes not greatly improve the LLM\u2019s evaluation capabilities. On these benchmarks, PROMETHEUS\nshows a +0.255, +0.493, and +0.202 improvement over its base model LLAMA2-CHAT-13B in\nterms of Pearson correlation on the Vicuna Bench, MT Bench, and Flask Eval dataset, respectively.\nWhile PROMETHEUS outperforms LLAMA2-CHAT 70B and GPT-3.5-TURBO-0613, it lacks be-\nhind GPT-4. We conjecture that this might be because the instructions from the FEEDBACK COL-\nLECTION and these evaluation datasets have different characteristics; the FEEDBACK COLLECTION\nare relatively long and detailed (e.g., I\u2019m a city planner ... I\u2019m looking for a novel and progressive\nsolution to handle traffic congestion and air problems derived from population increase), while the\ndatasets used for evaluation hold short (e.g., Can you explain about quantum mechanics?).\nOn the other hand, it is important to note that on the Flask Eval dataset, LLAMA2-CHAT 13B +\nCOARSE (specifically trained with the Flask Eval dataset) outperforms PROMETHEUS. This indi-\ncates that training directly on the evaluation dataset might be the best option to acquire a task-specific\nevaluator LLM, and we further discuss this in Section 6.4.\n11\nTable 5: Human Agreement accuracy among ranking datasets. The best comparable statistics are bolded.\nEvaluator LM\nHHH ALIGNMENT\nMT BENCH HUMAN JUDG.\nHelp.\nHarm.\nHon.\nOther\nTotal Avg.\nHuman Preference\nRANDOM\n50.00\n50.00\n50.00\n50.00\n50.00\n34.26\nSTANFORDNLP REWARD MODEL\n69.49\n60.34\n52.46\n51.16\n58.82\n44.79\nALMOST REWARD MODEL\n74.58\n67.24\n78.69\n86.05\n76.02\n49.90\nLLAMA2-CHAT 7B\n66.10\n81.03\n70.49\n74.42\n72.85\n51.78\nLLAMA2-CHAT 13B\n74.58\n87.93\n55.74\n79.07\n73.76\n52.34\nLLAMA2-CHAT 70B\n66.10\n89.66\n67.21\n74.42\n74.21\n53.67\nLLAMA2-CHAT 13B + COARSE.\n68.74\n68.97\n65.57\n67.44\n67.42\n46.89\nGPT-3.5-TURBO-0613\n76.27\n87.93\n67.21\n86.05\n78.73\n57.12\nPROMETHEUS 7B\n69.49\n84.48\n78.69\n90.70\n80.09\n55.14\nPROMETHEUS 13B\n81.36\n82.76\n75.41\n76.74\n79.19\n57.72\nGPT-4-0613\n91.53\n93.10\n85.25\n83.72\n88.69\n63.87\n5.3\nCAN PROMETHEUS FUNCTION AS A REWARD MODEL?\nWe conduct experiments on 2 human preference datasets: HHH Alignment (Askell et al., 2021) and\nMT Bench Human Judgment (Zheng et al., 2023) that use a ranking grading scheme. In Table 5,\nresults show that prompting LLAMA-2-CHAT surprisingly obtains reasonable performance, which\nwe conjecture might be the effect of using a base model that is trained with Reinforcement Learn-\ning from Human Feedback (RLHF). When training on feedback derived from coarse-grained score\nrubrics (denoted as LLAMA2-CHAT 13B + COARSE), it only hurts performance. On the other hand,\nPROMETHEUS 13B shows a +5.43% and +5.38% margin over its base model LLAMA2-CHAT-13B\non the HHH Alignment and MT Bench Human Judgement dataset, respectively. These results are\nsurprising because they indicate that training on an absolute grading scheme could also improve\nperformance on a ranking grading scheme even without directly training on ranking evaluation in-\nstances. Moreover, it shows the possibilities of using a generative LLM (PROMETHEUS) as a reward\nmodel for RLHF (Kim et al., 2023b). We leave the exploration of this research to future work.\n6\nDISCUSSIONS AND ANALYSIS\n6.1\nWHY IS IT IMPORTANT TO INCLUDE REFERENCE MATERIALS?\nEvaluating a given response without any reference material is a very challenging task (i.e., Directly\nasking to decide a score only when an instruction and response are given), since the evaluation LM\nshould be able to (1) know what the important aspects tailored with the instruction is, (2) inter-\nnally estimate what the answer of the instruction might be, and (3) assess the quality of responses\nbased on the information derived from the previous two steps. Our intuition is that by incorporating\neach component within the reference material, the evaluator LM could solely focus on assessing\nthe quality of the response instead of determining the important aspects or solving the instruction.\nSpecifically, we analyze the role of each component as follows:\n\u2022 Score Rubric: Giving information of the the pivotal aspects essential for addressing the\ngiven instruction. Without the score rubric, the evaluator LM should inherently know what\ndetails should be considered from the given instruction.\n\u2022 Reference Answer: Decomposing the process of estimating a reference answer and evalu-\nating it at the same time into two steps. Since the reference answer is given as an additional\ninput, the evaluator LM could only focus on evaluating the given response. This enables\nto bypass a natural proposition that if an evaluator LM doesn\u2019t have the ability to solve the\nproblem, it\u2019s likely that it cannot evaluate different responses effectively as well.\nAs shown in Table 6, we conduct an ablation experiment by excluding each reference material and\nalso training only on the score rubric without generating a feedback. Additionally, we also ablate the\neffect of using different model variants (Llama-2, Vicuna, Code-Llama) instead of Llama-2-Chat.\n12\nTable 6: Pearson, Kendall-Tau, Spearman correlation with data generated by GPT-4-0613 (Feedback Collec-\ntion Test set) and scores sampled from GPT-4-0613 across 3 inferences (Vicuna Bench).\nEvaluator LM\nFEEDBACK COLLECTION TEST SET\nVICUNA BENCH\nSeen Score Rubric\nUnseen Score Rubric\n-\nPearson\nPearson\nPearson\nPROMETHEUS 7B\n0.860\n0.847\n0.457\nTraining Ablation\nW/O SCORE RUBRIC\n0.837\n0.745\n0.355\nW/O FEEDBACK DISTILLATION\n0.668\n0.673\n0.413\nW/O REFERENCE ANSWER\n0.642\n0.626\n0.349\nModel Ablation\nLLAMA-2 7B BASELINE\n0.839\n0.818\n0.404\nVICUNA-V1.5 7B BASELINE\n0.860\n0.829\n0.430\nCODE-LLAMA 7B BASELINE\n0.823\n0.761\n0.470\nTraining Ablation\nThe results indicate that each component contributes orthogonally to\nPROMETHEUS\u2019s superior evaluation performance. Especially, excluding the reference answer shows\nthe most significant amount of performance degradation, supporting our claim that including a ref-\nerence answer relieves the need for the evaluator LM to internally solve the instruction and only\nfocus on assessing the response. Also, while excluding the score rubric on the FEEDBACK BENCH\ndoes not harm performance a lot, the performance drops a lot when evaluating on Vicuna Bench. As\nin our hypothesis, we conjecture that in order to generalize on other datasets, the role of providing\nwhat aspect to evaluate holds relatively crucial.\nModel Ablation\nTo test the effect using LLAMA2-CHAT, a model that has been instruction-tuned\nwith both supervised fine-tuning and RLHF, we ablate by using different models as a starting point.\nResults show that different model choices do not harm performance significantly, yet a model trained\nwith both supervised fine-tuning and RLHF shows the best performance, possibly due to additional\ntraining to follow instructions. However, we find that using Code-Llama has some benefits when\nevaluating on code domain, and we discuss the effect on Section 6.5.\n6.2\nNARROWING PERFORMANCE GAP TO GPT-4 EVALUATION\nThe observed outcomes, in which PROMETHEUS consistently surpasses GPT-4 based on human\nevaluations encompassing both scores and quality of feedback, as well as correlations in the FEED-\nBACK BENCH, are indeed noteworthy. We firmly posit that these findings are not merely serendipi-\ntous and offer the following justifications:\n\u2022 Regarding results on FEEDBACK BENCH, our model is directly fine-tuned on this data, so\nit\u2019s natural to beat GPT-4 on a similar distribution test set if it is well-trained. In addition,\nfor GPT-4, we compare the outputs of inferencing on the instructions and augmenting\nnew instances, causing the self-consistency to be lower.\n\u2022 Regarding score correlation for human evaluation, our model shows similar or slightly\nhigher trends. First, our human evaluation set-up excluded all coding or math-related ques-\ntions, which is where it is non-trivial to beat GPT-4 yet. Secondly, there\u2019s always the margin\nof human error that needs to be accounted for. Nonetheless, we highlight that we are the\nfirst work to argue that an open-source evaluator LM could closely reach GPT-4 evaluation\nonly when the appropriate reference materials are accompanied.\n\u2022 As shown in Figure 6, PROMETHEUS tends to be critical compared to GPT-4. We conjec-\nture this is because since it is specialized for evaluation, it acquires the characteristics of\nseeking for improvement when assessing responses.\n13\n6.3\nQUALITATIVE EXAMPLES OF FEEDBACK GENERATED BY PROMETHEUS\nWe present five qualitative examples to compare the feedback generated by PROMETHEUS and\nGPT-4 in Appendix I. Specifically, Figure 16 shows an example where human annotators labeled\nthat GPT-4 generate an abstract/general feedback not suitable for criticizing the response. Figure 17\nshows an example where human annotators labeled that PROMETHEUS generate overly critical feed-\nback. Figure 18 shows an example of human annotators labeled as a tie. In general, PROMETHEUS\ngenerates a detailed feedback criticizing which component within the response is wrong and seek\nimprovement. This qualitatively shows that PROMETHEUS could function as an evaluator LM.\nMoreover, we present an example of evaluating python code responses using PROMETHEUS, GPT-\n4, and Code-Llama in Figure 19. We discuss the effect of using a base model specialized on code\ndomain for code evaluation in Section 6.5.\n6.4\nA PRACTITIONER\u2019S GUIDE FOR DIRECTLY USING PROMETHEUS EVALUATION\nPreparing an Evaluation Dataset\nAs shown in the previous sections, PROMETHEUS functions as\na good evaluator LM not only on the FEEDBACK BENCH (a dataset that has a similar distribution\nwith the dataset it was trained on), but also on other widely used evaluation datasets such as the Vi-\ncuna Bench, MT Bench, and Flask Eval. As shown in Figure 1, users should prepare the instruction\ndataset they wish to evaluate their target LLM on. This could either be a widely used instruction\ndataset or a custom evaluation users might have.\nDeciding a Score Rubric to Evaluate on\nThe next step is to choose the score rubric users would\nwant to test their target LLM on. This could be confined to generic metrics such as helpfulness/harm-\nlessness, but PROMETHEUS also supports fine-grained score rubrics such as \u201cChild-Safety\u201d, \u201cCre-\nativity\u201d or even \u201cIs the response formal enough to send to my boss\u201d.\nPreparing Reference Answers\nWhile evaluating without any reference is also possible, as shown\nin Table 6, PROMETHEUS shows superior performance when the reference answer is provided.\nTherefore, users should prepare the reference answer they might consider most appropriate based\non the instructions and score rubrics they would want to test on. While this might require additional\ncost to prepare, there is a clear trade-off in order to improve the precision or accuracy of the overall\nevaluation process, hence it holds crucial.\nGenerating Responses using the Target LLM\nThe last step is to prepare responses acquired\nfrom the target LLM that users might want to evaluate. By providing the reference materials (score\nrubrics, reference answers) along with the instruction and responses to evaluate on, PROMETHEUS\ngenerates a feedback and a score. Users could use the score to determine how their target LLM is\nperforming based on customized criteria and also refer to the feedback to analyze and check the\nproperties and behaviors of their target LLM. For instance, PROMETHEUS could be used as a good\nalternative for GPT-4 evaluation while training a new LLM. Specifically, the field has not yet come\nup with a formalized procedure to decide the details of instruction-tuning or RLHF while developing\na new LLM. This includes deciding how many training instances to use, how to systematically decide\nthe training hyperparameters, and quantitatively analyzing the behaviors of LLMs across multiple\nversions. Most importantly, users might not want to send the outputs generated by their LLMs to\nOpenAI API calls. In this regard, PROMETHEUS provides an appealing solution of having control\nover the whole evaluation process, also supporting customized score rubrics.\n6.5\nA PRACTITIONER\u2019S GUIDE FOR TRAINING A NEW EVALUATION MODEL\nUsers might also want to train their customized evaluator LM as PROMETHEUS for different use\ncases. As shown in Table 4, training directly on the Flask dataset (denoted as LLAMA2-CHAT\n13B + COARSE) shows a higher correlation with GPT-4 on the Flask Eval dataset compared to\nPROMETHEUS that is trained on the FEEDBACK COLLECTION. This implies that directly training\non a target feedback dataset holds the best performance when evaluating on it. Yet, this requires\ngoing through the process of preparing a new feedback dataset (described in Section 3.1). This\nimplies that there is a trade-off between obtaining a strong evaluator LM on a target task and paying\n14\nthe initial cost to prepare a new feedback dataset. In this subsection, we provide some guidelines for\nhow users could also train their evaluator LM using feedback datasets.\nPreparing a Feedback Dataset to train on\nAs described in Section 3, some important consider-\nations to prepare a new feedback dataset are: (1) including as many reference materials as possible,\n(2) maintaining a uniform length among the reference answers for each score (1 to 5) to prevent un-\ndesired length bias, (3) maintaining a uniform score distribution to prevent undesired decision bias.\nWhile we did not explore the effect of including other possible reference materials such as a \u201cScore\n1 Reference Answer\u201d or \u201cBackground Knowledge\u201d due to limited context length, future work could\nalso explore this aspect. The main intuition is that providing more reference materials could enable\nthe evaluator LM to solely focus on evaluation instead of solving the instruction.\nChoosing a Base Model to Train an Evaluator LM\nAs shown in Figure 19, we find that training\non CODE-LLAMA provides more detailed feedback and a reasonable score decision when evaluating\nresponses on code domains (7 instances included within the Vicuna Bench dataset). This indicates\nthat choosing a different base model based on the domain to evaluate might be crucial when de-\nsigning an evaluator LM. We also leave the exploration of training an evaluator LM specialized on\ndifferent domains (e.g., code and math) as future work.\n7\nCONCLUSION\nIn this paper, we discuss the possibility of obtaining an open-source LM that is specialized for fine-\ngrained evaluation. While text evaluation is an inherently difficult task that requires multi-faceted\nconsiderations, we show that by incorporating the appropriate reference material, we can effectively\ninduce evaluation capability into an LM. We propose a new dataset called the FEEDBACK COL-\nLECTION that encompasses thousands of customized score rubrics and train an open-source evalu-\nator model, PROMETHEUS. Surprisingly, when comparing the correlation with human evaluators,\nPROMETHEUS obtains a Pearson correlation on par with GPT-4, while the quality of the feedback\nwas preferred over GPT-4 58.62% of the time. When comparing Pearson correlation with GPT-4,\nPROMETHEUS shows the highest correlation even outperforming GPT-3.5-Turbo. Lastly, we show\nthat PROMETHEUS shows superior performance on human preference datasets, indicating its possi-\nbility as an universal reward model. We hope that our work could stimulate future work on using\nopen-source LLMs as evaluators instead of solely relying on proprietary LLMs.\nACKNOWLEDGMENTS\nThis work was partly supported by KAIST-NAVER Hypercreative AI Center and Institute of In-\nformation & communications Technology Planning & Evaluation (IITP) grant funded by the Korea\ngovernment (MSIT) (No.2022-0-00264, Comprehensive Video Understanding and Generation with\nKnowledge-based Deep Logic Neural Network, 40%; No.2021-0-02068, Artificial Intelligence In-\nnovation Hub, 20%). We thank Minkyeong Moon, Geonwoo Kim, Minkyeong Cho, Yerim Kim,\nSora Lee, Seunghwan Lim, Jinheon Lee, Minji Kim, and Hyorin Lee for helping with the human\nevaluation experiments. We thank Se June Joo, Dongkeun Yoon, Doyoung Kim, Seonghyeon Ye,\nGichang Lee, and Yehbin Lee for helpful feedback and discussions.\n15\nREFERENCES\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,\nNicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Her-\nnandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack\nClark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a labora-\ntory for alignment, 2021.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan\nYi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. arXiv\npreprint arXiv:2307.03109, 2023.\nYew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval: Towards holistic\nevaluation of instruction-tuned large language models. arXiv preprint arXiv:2306.04757, 2023.\nCheng-Han Chiang and Hung yi Lee. Can large language models be an alternative to human evalu-\nations?, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for\nmethods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00a8el Kubli. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056, 2023.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers.\narXiv preprint arXiv:2212.10071, 2022.\nAri Holtzman, Peter West, and Luke Zettlemoyer. Generative models as a complex systems science:\nHow can we make sense of large language model behavior? arXiv preprint arXiv:2308.00189,\n2023.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning\nlanguage models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.\nSeungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon\nSeo.\nThe cot collection: Improving zero-shot and few-shot learning of language models via\nchain-of-thought fine-tuning. arXiv preprint arXiv:2305.14045, 2023a.\nSungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo,\nand Minjoon Seo. Aligning large language models through synthetic feedback. arXiv preprint\narXiv:2305.13735, 2023b.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. Hurdles to progress in long-form question answering.\narXiv preprint arXiv:2103.06332, 2021.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following\nmodels. https://github.com/tatsu-lab/alpaca_eval, 2023.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110, 2022.\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out, pp. 74\u201381, Barcelona, Spain, July 2004a. Association for Computational Linguis-\ntics. URL https://aclanthology.org/W04-1013.\n16\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74\u201381, 2004b.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg\nevaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. arXiv preprint arXiv:2305.14251, 2023.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics, pp. 311\u2013318, 2002.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith gpt-4. arXiv preprint arXiv:2304.03277, 2023.\nLuiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker. On the challenges of using black-\nbox apis for toxicity evaluation in research, 2023.\nThibault Sellam, Dipanjan Das, and Ankur P. Parikh.\nBleurt: Learning robust metrics for text\ngeneration. In Annual Meeting of the Association for Computational Linguistics, 2020. URL\nhttps://api.semanticscholar.org/CorpusID:215548699.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023.\nTianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O\u2019Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu,\nOlga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: A\ncritic for language model generation. arXiv preprint arXiv:2308.04592, 2023a.\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang,\nRui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm\ninstruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023b.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\narXiv preprint arXiv:2212.10560, 2022.\nSeonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo.\nSelfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, May 2023a.\nURL https://kaistai.github.io/SelFee/.\nSeonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo,\nJames Thorne, Juho Kim, and Minjoon Seo. Flask: Fine-grained language model evaluation\nbased on alignment skill sets. arXiv preprint arXiv:2307.10928, 2023b.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. BARTScore: Evaluating generated text as text\ngeneration. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in\nNeural Information Processing Systems, 2021. URL https://openreview.net/forum?\nid=5Ya8PbvpZ9.\n17\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluat-\ning text generation with bert. arXiv preprint arXiv:1904.09675, 2019.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\nmodels. arXiv preprint arXiv:2304.06364, 2023.\n18\nFigure 7: Rouge-L score distribution among two randomly sampled score rubrics from the FEED-\nBACK COLLECTION. A left-skewed distribution with low values shows the score rubrics are diverse.\nSpearman Correlation: 0.58\nFigure 8: Box and whisker plot for average sentiment per score description. A linearly increasing\ntrend is crucial for the evaluator LM to decide a score in an Absolute Scoring setting.\nA\nANALYSIS OF THE FEEDBACK COLLECTION DATASET\nIn this section, we provide a comprehensive analysis of the characteristics of the FEEDBACK COL-\nLECTION dataset. To ensure the quality, we answer each question one by one, emphasizing our main\nconsiderations during the creation of the dataset.\nAre the Score Criteria Diverse Enough?\nFollowing previous work (Wang et al., 2022; Honovich\net al., 2022), we plot the rouge-L distribution between two instances among our whole set of 1K\nscore rubrics. Specifically, we use the score criteria (description of the criteria) and measure the\nrouge-L value between the two score criteria. Figure 7 shows the overall distribution plot. The\nresults indicate that each criteria does not overlap with one another, ensuring that we include many\nnovel score rubrics in our training set.\nAre the Score Descriptions Well Formulated?\nAnother component in the score rubric is a de-\nscription of each score (i.e., A comprehensive reason why a score of i (1 \u2264 i \u2264 5 should be given).\nIn an Absolute Scoring setting, it is important to evaluate the given response based on the score\ndescriptions instead of giving a score of 1 for all responses that lack a minor detail or giving a score\n19\nFeedback Collection Training Set\nFigure 9: Box and whisker plot plotting the length distribution of responses on each score range.\nWe check whether there is a length bias (i.e., a higher score given for longer responses).\nTable 7: Distinct N-gram measured on each component of the training instance. A higher diversity\nratio indicates that each component tends to be more diverse.\nDistinct N-gram\nBigram Diversity Ratio\nTrigram Diversity Ratio\nInstruction\n0.43\n0.79\nReference\n0.43\n0.82\nScore Rubric\n0.60\n0.81\nResponses\n0.32\n0.77\nFeedback\n0.26\n0.66\nof 5 for all responses that seem to be good on the surface. Due to these reasons, the role of the score\ndescriptions hold crucial, where the main role is to show a monotonically increasing tendency of\nsentiment, not dramatically. Figure 8 shows that the FEEDBACK COLLECTION holds a smoothly in-\ncreasing sentiment tendency for each score description. This ensures the quality of the score rubric,\nconfirming that it plays a role in deciding the score.\nIs there a length bias among the Responses?\nPrevious work has demonstrated that when LMs\nare used as evaluators, they tend to give higher scores to longer responses (Li et al., 2023; Dubois\net al., 2023; Zheng et al., 2023). In order to minimize this effect during fine-tuning PROMETHEUS,\none of our main consideration was to maintain a length distribution equal among the score range of 1\nto 5. As shown in Figure 9, most of the responses within the FEEDBACK COLLECTION maintained\na similar length among different scores (near 200 tokens). We also include a comprehensive analysis\nof whether PROMETHEUS possessed any length bias during evaluation in Appendix F.\nAre the Instructions, Responses, and Feedback Diverse as Well?\nIn addition to the analysis of\nthe score rubric and responses, we also analyze whether the instructions, responses, and feedback\nwithin the FEEDBACK COLLECTION are diverse enough. For this purpose, we examine the bigram\nand trigram ratios. The results are shown in Table 7, indicating a variety in how terms are expressed,\nand our findings suggest a moderate level of diversity. While there is some term repetition, the\ndataset also showcases a notable range of expressions.\n20\nFigure 10: Rouge-L score distribution among a randomly sampled score rubric from the FEEDBACK\nCOLLECTION and a score rubric from the FEEDBACK BENCH. A left-skewed distribution with low\nvalues shows that they do not overlap with each other, hence meaning that a UNSEEN score rubric\nassumption is satisfied.\nTable 8: Hyperparameters used for fine-tuning PROMETHEUS.\nModel\nBase Model\nBatch size\nLR\nLR Scheduler\nOptimizer\nMax Length (Input & Output)\nPROMETHEUS-7B\nLlama-2-Chat-7B\n28\n1e-5\nStepLR\nAdamW\n4096\nPROMETHEUS-13B\nLlama-2-Chat-13B\n20\n1e-5\nStepLR\nAdamW\n4096\nB\nANALYSIS OF THE FEEDBACK BENCH EVALUATION DATASET\nIn this section, we provide a analysis of whether the FEEDBACK BENCH consists of unseen score\nrubrics against the score rubrics from the FEEDBACK COLLECTION.\nDoes the testset maintain Unseen Score Rubrics?\nOne of the main considerations of our exper-\niments in Section 5.2 using the FEEDBACK BENCH was testing whether PROMETHEUS could gen-\neralize to unseen customized score rubrics. For this purpose, we built an unseen customized rubric\nsubset. We plot the rouge-L distribution between a random score rubric within the FEEDBACK COL-\nLECTION and a random score rubric within the FEEDBACK BENCH. As shown in Figure 10, there\nis a low overlap among the train and test sets, confirming that the FEEDBACK BENCH is valid to be\nclaimed as an unseen test set to measure the evaluation capability of evaluator LMs.\nC\nFINE-TUNING AND INFERENCE DETAILS OF PROMETHEUS\nWe use 8xA100 (80GB) GPUs to train our models with PyTorch Fully-Sharded Data Parallel (FSDP)\noption. The code we used for training and inference is the official Llama2 fine-tuning code released\nby Meta AI3. The hyper-parameters we used are the basic settings in the fine-tuning code except\nfor the training batch size which was set according to the model size: for 7B models we used 28\nand for 13B models we used 20 to fully leverage GPU memory. Note that in the official Llama2\nfine-tuning code, the loss is only calculated on the feedback and score decision, not the instruction.\nWe empirically find that not masking out the instruction leads to poor performance while evaluating\nresponses. The detailed hyper-parameters are shown in Table 8.\nFor inference, we use the hyper-parameters as shown in Table 9. When inferencing with the naive\nLlama-2-Chat model (not trained on the FEEDBACK COLLECTION) it was extremely difficult to\n3https://github.com/facebookresearch/llama-recipes\n21\nTable 9: Hyperparameters used for inferencing PROMETHEUS, GPT-3.5-Turbo, and GPT-4. Verbal-\nizer denotes accepting outputs such as \u201d[Score 5]\u201d or \u201dScore: 4 out of 5\u201d whereas the exact format\nis \u201d[Result] 5\u201d (format is mentioned concretely within the instruction given to the evaluator LM).\nEven after applying a verbalizer, Llama-2-Chat is not able to generate a score decision that could\neasily be parsed, highlighting the benefits of fine-tuning it on feedback data.\nParams\nModel\nTemperature\nTop-p\nRepetition Penalty\nMax Output Length\nVerbalizer\n7B\nLlama-2-Chat-7B\n1.0\n0.9\n1.03\n256\nYes\n13B\nLlama-2-Chat-13B\n1.0\n0.9\n1.03\n256\nYes\n70B\nLlama-2-Chat-70B\n1.0\n0.9\n1.03\n256\nYes\n7B\nPROMETHEUS-7B\n1.0\n0.9\n1.03\n256\nNo\n13B\nPROMETHEUS-13B\n1.0\n0.9\n1.03\n256\nNo\n-\nGPT-3.5-Turbo\n1.0\n0.9\n-\n256\nNo\n-\nGPT-4\n1.0\n0.9\n-\n256\nNo\nsteer the model to generate a final score in the form to be easily parsed (e.g., \u201c[RESULT] 3\u201d). While\nin-context learning (ICL) could solve this issue, most of our instances contained a maximum of\n3072 tokens, so we could not utilize demonstrations during inference. Therefore, we empirically\nfound patterns such as \u201c[SCORE 5]\u201d or \u201cScore: 4 out of 5\u201d and applied verbalizer to map those\noutputs to a final score decision. This highlights the benefits of directly training to generate in a\nstructured format as PROMETHEUS. On the other hand, we also find that proprietary LLMs such as\nGPT-3.5-Turbo and GPT-4 excel at generating structured outputs when the prompt is adeptly given.\nAlso, note that we found that if we set the temperature to 0.0, evaluator LMs are not able to generate\nmeaningful feedback compared to using a temperature of 1.0.\nD\nTRAINING A EVALUATOR LM ON COARSE-GRAINED SCORE RUBRICS\nFor the purpose of exploring the benefits of training on thousands of fine-grained and customized\nscore rubrics, we employ a baseline of only training on relatively coarse-grained score rubrics.\nSince the FEEDBACK COLLECTION\u2019s instructions are closely tied with the score rubrics during its\ncreation process, we could not directly use it and only change the score rubrics into coarse-grained\nones.\nSo, we used the Flask dataset (Ye et al., 2023b) and split it into training data and evaluation data.\nThe evaluation data is denoted as Flask Eval throughout the paper. Specifically, the Flask dataset\nconsists of 1.7K instructions acquired across conventional NLP datasets and instruction datasets.\nAlso, there exists 76.5K responses acquired across 15 response LMs. Each instance has a score\nrubric among 12 options (Logical Robustness, Logical Correctness, Logical Efficiency, Factuality,\nCommonsense Understanding, Harmlessness, Readability, Comprehension, Insightfulness, Com-\npleteness, Metacognition, Conciseness). While these 12 score rubrics are more fine-grained and di-\nverse compared to previous works only using helpfulness and harmlessness, they are coarse-grained\ncompared to the thousands of score rubrics included within the FEEDBACK COLLECTION, so we\ndenote as coarse-grained in this work.\nAmong the 1.5K instructions & 67.5K responses as training data, we found that the score distribution\nis extremely skewed towards the score of 5. We distributed the instances so that the number of\ninstances within the score range of 1 to 5 remains equal, which leads to 30K training instances. We\ntrained the Llama-2-Chat model on the Flask train set, which led to one of our baselines denoted as\nLLAMA-2-CHAT + COARSE in Table 3, Table 4, Table 5, Table 6.\nE\nPAIRWISE COMPARISON OF THE QUALITY OF THE FEEDBACK\nIn this section, we further explain the experimental setting and present additional results & analysis\nfor the experiment of comparing the quality of the generated feedback (Section 5.1).\nIn addition to Figure 6, the reason why each annotator rejected the feedback from either\nPROMETHEUS, GPT-3.5-Turbo, GPT-4 is shown in Figure 11 and Figure 12.\n22\nFigure 11: The reason why GPT-3.5-Turbo\u2019s or Prometheus\u2019s feedback was not chosen over the\nother. PROMETHEUS generates less abstract and general feedback, but tends to write overly critical\nones.\nFigure 12: The reason why GPT-4\u2019s or GPT-3.5-Turbo\u2019s feedback was not chosen over the other.\nGPT-4 generates less abstract and general feedback, but tends to write overly critical ones.\nThe results further support our claim that PROMETHEUS tends to be critical over GPT-4 and GPT-\n3.5-Turbo. Interestingly, GPT-4 was considered to be more critical compared to GPT-3.5-Turbo and\nthe gap was even wider when comparing GPT-3.5-Turbo and PROMETHEUS. This indicates that\nPROMETHEUS can serve as a critical judge when evaluating responses generated by LLMs, but it\ncould also be biased towards not being optimistic generally. The degree of being critical could be\nuseful or a limitation based on different use cases. For instance, we conjecture that it could be helpful\nwhen analyzing the limitations of LLMs or providing feedback as supervision to further improve a\ntarget LLM (e.g., RLHF), yet we leave this exploration to future work.\n23\nFigure 13: Box and whisker plot describing a relationship between a given response and its corre-\nsponding score. We check if the response lengths correlate with its scores.\nFigure 14: Box and whisker plot describing a relationship between a given response and its corre-\nsponding score. We check if the response lengths correlate with its scores.\nFigure 15: Box and whisker plot describing a relationship between a given response and its corre-\nsponding score. We check if the response lengths correlate with its scores.\n24\nF\nIS THERE A LENGTH BIAS DURING EVALUATION?\nOne of the limitations of employing an LLM as an evaluator LM is that it could be vulnerable to\nvarious biases. In this work, we train/test on an Absolute Scoring evaluation setting, hence there\nexists no position bias. Yet, it is crucial to analyze whether PROMETHEUS showed any bias towards\nfavoring longer responses. Hence, we conduct a comprehensive analysis in this section.\nAs shown in Figure 13, Figure 14, and Figure 15, both GPT-4 and PROMETHEUS and GPT-4 shows\na similar trend of not favoring longer responses (i.e., similar length distribution among different\nscores). However, as mentioned in Zheng et al. (2023), LLM evaluators might favor more verbose\nresponses, yet the responses from our test instances (FEEDBACK BENCH and Vicuna Bench) did not\ninclude any adversarial examples to test this phenomenon. More extensive research on whether the\nlength bias is also transferred to fine-tuned evaluator LMs should be explored in future work.\nG\nPROMPT FOR FEEDBACK COLLECTION CREATION\nIn this section, we provide the extensive list of prompts used to create the FEEDBACK COLLECTION.\nNote that in the prompt of generating a response and a feedback, we use the sentence length of\nthe reference answer and append it to \u201c{SENT NUM}\u201d within the prompt. This was crucial to\nmake the length even across different scores as shown in Figure 9. Also, note that for the 1K\nscore rubrics, 20K instructions & reference answers, and 100K responses & feedback within the\nFEEDBACK COLLECTION, each prompt was sequentially used. In early experiments, we found that\ngenerating every component all at once leads to very poor generation quality (i.e., similar responses\n& feedback across different score ranges). Yet, we found that grouping (1) the instruction and\nreference answer generation and (2) the response and feedback generation had a positive synergy,\nleading to better generation quality and less amount of cost. Also, to the best of our knowledge, we\nare first to explore acquiring negative and neutral responses (Score 1 \u223c 4 responses) through GPT-4\naugmentation. We hope future work could also explore applying this strategy to different use cases.\nPrompt for Brainstorming New Score Rubrics\nWe are brainstorming criteria with which to grade a language model on its responses in\ndiverse situations.\nA \u2018criteria\u2018 is some useful, real-world objective, and associated rubric for scores 1-5, that\ntests a capability.\nHere you will see 4 examples of \u2018criteria\u2018, and their scoring rubrics, formatted as\nJSON.\nCriteria 1:\n{JSON LIST 1}\nCriteria 2:\n{JSON LIST 2}\nCriteria 3:\n{JSON LIST 3}\nCriteria 4:\n{JSON LIST 4}\nPlease brainstorm a new criteria and scoring rubrics.\nBe creative and create new but useful criteria that people in different settings or industries\nmight find practical.\nPlease format the output as same as the above examples with no extra or surrounding text.\nWrite [END] after you are done.\nNew Criteria:\n25\nPrompt for Paraphrasing as a New Score Rubric\nPlease paraphrase the sentences inside the dictionary below.\nEach paraphrase should not change the meaning or substance of the original sentence, be\nnaturally written, but sufficiently diverse from one another.\nDiversity can come from differences in diction, phrasing, sentence structure, formality,\ndetail, and/or other stylistic changes.\nThe dictionary:\n{CRITERIA}\nRespond with only dictionary (same format as the given dictionary) with no extra or\nsurrounding text.\nWrite [END] after you are done.\nDictionary with Paraphrased Sentences:\n26\nPrompt for Generating an Instruction and Reference Answer\nYour job is to generate a new novel problem and a response that is related to the given score\nrubric.\nThe score rubric:\n{CRITERIA}\n* Problem\n- The problem should inherently be related to the score criteria and score rubric given above.\nSpecifically, the score criteria should be the core attributes required to solve the problem.\n- The problem itself should not be too generic or easy to solve.\n- If the score rubric is related to logical abilities, generate problems that require math or\ncoding abilities.\n- Try to make the person who might solve the problem not notice the existence of the score\nrubric by not explicitly mentioning it, and also provide additional inputs and options if\nneeded.\n- Assume a situation where a user is interacting with an AI model. The user would try to\nask in a first-person point of view, but not using terms like \u201dI\u201d, \u201dA User\u201d or \u201dYou\u201d in the\nfirst sentence.\n- Do not give a role to the AI, assume that the user is asking a question from his point of\nview.\n- Do not include any phrase related to AI model in the problem.\n* Response\n- The response should be a response that would get a score of 5 from the score rubric.\n- The response should be as detailed as possible unless the score rubric is related to\nconciseness or brevity.\nIt should consist of multiple paragraphs, a list of items, or a\nstep-by-step reasoning process.\n- The response should look like how a well-prompted GPT-4 would normally answer your\nproblem.\n* Format\n- DO NOT WRITE ANY GREETING MESSAGES, just write the problem and response\nonly.\n- In front of the problem, append the phrase \u201dProblem:\u201d and in front of the response, append\nthe phrase \u201dResponse:\u201d.\n- Write in the order of \u201dProblem\u201d - \u201dResponse\u201d, where the two items are separated by the\nphrase \u201d[NEXT]\u201d.\n- Write [END] after you are done.\nData Generation:\n27\nPrompt for Generating Responses and Feedback\nYour job is to generate a response that would get a score of {SCORE} and corresponding\nfeedback based on the given score rubric. For reference, a reference response that would get\na score of 5 is also given.\nInstruction:\n{INSTRUCTION}\nThe score rubric:\n{CRITERIA}\nReference response (Score 5):\n{REFERENCE}\n* Response\n- The quality of the score {SCORE} response should be determined based on the score\nrubric, not by its length.\n- The score {SCORE} response should have the same length as the reference response,\ncomposed of {SENT NUM} sentences.\n- Do not explicitly state the keywords of the score rubric inside the response.\n* Feedback\n- The score {SCORE} feedback should each be an explanation of why the response would\nget a score of {SCORE}. It should be written based on the generated response and score\nrubric.\n- The score {SCORE} feedback shouldn\u2019t just copy and paste the score rubric, but it should\nalso give very detailed feedback on the content of the corresponding response.\n- The score {SCORE} feedback should include the phrase \u201dSo the overall score is\n{SCORE}\u201d in the last sentence.\n* Format\n- DO NOT WRITE ANY GREETING MESSAGES, just write the problem and response\nonly.\n- In front of the response, append the phrase \u201dResponse:\u201d and in front of the feedback,\nappend the phrase \u201dFeedback:\u201d.\n- Write in the order of \u201dResponse\u201d - \u201dFeedback\u201d, where the two items are separated by the\nphrase \u201d[NEXT]\u201d.\n- Write [END] after you are done.\nData Generation:\n28\nH\nPROMPT USED FOR PROMETHEUS\nIn this section, we provide the prompt used for training/inferencing PROMETHEUS. Note that after\napplying the prompt template shown below, we also apply Llama-2\u2019s basic conversation prompt\ntemplate in order to minimize the discrepancy between the training process of Llama-2 and training\non the FEEDBACK COLLECTION.\nPrompt for Prometheus\n###Task Description:\nAn instruction (might include an Input inside it), a response to evaluate, a reference answer\nthat gets a score of 5, and a score rubric representing an evaluation criterion is given.\n1. Write a detailed feedback that assesses the quality of the response strictly based on the\ngiven score rubric, not evaluating in general.\n2. After writing a feedback, write a score that is an integer between 1 and 5. You should\nrefer to the score rubric.\n3. The output format should look as follows: \u00a8Feedback: (write a feedback for criteria)\n[RESULT] (an integer number between 1 and 5)\u00a8\n4. Please do not generate any other opening, closing, and explanations.\n###The instruction to evaluate:\n{instruction}\n###Response to evaluate:\n{response}\n###Reference Answer (Score 5):\n{reference answer}\n###Score Rubrics:\n[{ criteria description }]\nScore 1: {score1 description}\nScore 2: {score2 description}\nScore 3: {score3 description}\nScore 4: {score4 description}\nScore 5: {score5 description}\n###Feedback:\n29\nI\nQUALITATIVE EXAMPLES OF GENERATED FEEDBACK\nFigure 16, Figure 17, Figure 18, Figure 19 shows a qualitative example of feedback generated by\neither GPT-4, PROMETHEUS (13B), and Code-Llama trained on the FEEDBACK COLLECTION.\nJ\nEXPERIMENTAL DETAILS FOR HUMAN EVALUATION\nThe user interface used for human evaluation is shown in Figure 20. In order to acquire a score\ndecision for the response, a decision of which feedback is better, and an annotation of why they\nmade a decision to choose one of the feedback, we constructed the user interface in sequential order.\nEach annotator was informed not to solve the questions below before solving the ones above them.\nThe test instances were randomly selected among FEEDBACK BENCH (Customized Unseen Rubric\nset), Vicuna Bench, and MT Bench except for math and coding instances. The experiments were\nconducted on LabelStudio, hosted on Huggingface Spaces. We asked each annotator to spend at\nleast 3 hours annotating 45 instances each and paid $ 50. The 9 annotators were divided into 3\ngroups: (1) PROMETHEUS vs GPT-4, (2) PROMETHEUS vs GPT-3.5-Turbo, and (3) GPT-4 vs GPT-\n3.5-Turbo. The order of the feedback was randomly decided in order to prevent order bias during\nannotation.\n30\nPrometheus's Feedback\nResponse from ChatGPT\nGPT-4's Feedback\nHere is a dialogue between an assistant and an user.\nGenerate a human preferable response by the assistant.\n\n User: One morning after sunrise, Suresh was standing facing a pole. The shadow of the pole fell \nexactly to his right. Can you tell me the direction towards which the shadow was pointing - east, south, \nwest, or north? Explain your reasoning steps.\u2028\u2028\n Assistant:Based on the given information, Suresh was standing facing a pole, and the shadow of the \npole fell exactly to his right. To determine the direction towards which the shadow was pointing, we \nneed to consider the movement of the sun during the day. \\n\\nThe sun rises in the east and sets in the \nwest. In the morning, when Suresh was standing facing the pole, the sun would have been in the eastern \nsky. Since the shadow of the pole fell exactly to Suresh's right, it means that the shadow was pointing \ntowards the west. \u2028\nTherefore, the direction towards which the shadow was pointing is west.\u2028\u2028\n User: To which direction was Suresh facing? How do you solve this?\u2028\u2028\n Assistant:\n\nApologies for the confusion. \u2028\nTo determine the direction Suresh was facing, we need to consider the position of the shadow in relation to \nhim. If the shadow fell exactly to his right, it means that Suresh was facing towards the west. \u2028\nThe shadow is formed when an object blocks the light from the sun. In this case, since the shadow is to \nSuresh's right, it indicates that the sun is in front of him, casting the shadow towards his back. \u2028\nTherefore, Suresh was facing towards the east.\nDoes the model demonstrate an ability to logically process and interpret given \ninformation, apply relevant knowledge, and present an insightful conclusion?\nPrometheus's Feedback\nThe model interprets the information from \nthe user accurately and applies the \nconcept of the sun's position and its effect \non the formation of shadows. However, it \nfails to reach the correct conclusion that \nSuresh is facing the south. Instead, it \nincorrectly concludes that Suresh is facing \nthe east. The response demonstrates \npartial understanding of the problem but \nlacks complete logical coherence in the \nsolution.\u2028\nThe final score is 3 out of 5.\nThe response provided by the model is \ninaccurate and doesn't demonstrate a clear \nunderstanding of the given information and task. \nAccording to the user's query, Suresh was \nstanding facing a pole, and the shadow of the \npole fell exactly to his right. This implies that \nSuresh was facing towards the west, not the \neast as the model suggests. Furthermore, the \nmodel failed to properly explain the reasoning \nbehind its conclusion, leading to an illogical \nconclusion that contradicts the initial information \nprovided. The model didn't correctly apply the \nknowledge of the movement of the sun during \nthe day, nor did it take into account the position \nof the shadow in relation to Suresh.\u2028\nThe final score is 1 out of 5.\nScore Rubric:\nInstruction\nFigure 16: An example of comparing the feedback generated by PROMETHEUS and GPT-4. GPT-4\nwas labeled to generate an abstract feedback.\n31\nGPT-4's Feedback\nInstruction\nWhy might someone prefer to shop at a small, locally-owned business \ninstead of a large chain store, even if the prices are higher?\nScore Rubric\nDoes the Response represent a balanced view of the factors?\nResponse from Vicuna 13B\nThere are several reasons why someone might prefer to shop at a small, locally-owned business instead of a large \nchain store, even if the prices are higher:\u2028\u2028\n1. Supporting the local economy: By shopping at a small, locally-owned business, you are supporting the local \neconomy and helping to keep money within the community.\u2028\n2. Personalized service: Small, locally-owned businesses often offer personalized service and attention that may not \nbe available at a large chain store. The staff may be more knowledgeable about the products they sell and may be \nmore willing to help you find what you're looking for.\u2028\n3. Unique products: Small, locally-owned businesses often carry unique products that you may not be able to find at \na large chain store. This can be especially true for businesses that specialize in niche products or that offer custom-\nmade items.\u2028\n4. Community involvement: Small, locally-owned businesses are often more involved in the local community than \nlarge chain stores. They may sponsor local events, support local charities, and be more connected to the community \nas a whole.\u2028\n5. Better customer service: Small, locally-owned businesses often have a more personal relationship with their \ncustomers and may be more willing to go above and beyond to ensure customer satisfaction.\u2028\n6. Environmental and ethical considerations: Some people prefer to shop at small, locally-owned businesses because \nthey believe that these businesses are more likely to have environmentally and socially responsible practices.\u2028\u2028\nOverall, shopping at a small, locally-owned business can be a way to support the local community, get personalized \nservice, and find unique products that you may not be able to find at a large chain store.\u2028\u2028\nThe response has done an excellent job of \npresenting a balanced view of the factors that \nmight make someone prefer shopping at a small, \nlocally-owned business instead of a large chain \nstore, even if the prices are higher. Six different \nreasons were given, each of them explained \nthoroughly and in depth. No one reason was \ngiven more emphasis than the others, and each \nexplanation was fair and unbiased. However, it's \nworth noting that the response failed to consider \nfactors that might favor large chain stores, as \nthe reference answer did.\n\nThe final score is 4 out of 5.\nThe response does a good job of presenting the factors \nof why someone might prefer to shop at a small, locally-\nowned business, including supporting the local economy, \npersonalized service, unique products, community \ninvolvement, better customer service, and environmental \nand ethical considerations. However, the response could \nimprove in presenting all factors with equal rigor. \n\nFor example, the response does not delve deeply enough \ninto the specifics of how shopping at a small, locally-\nowned business could support the local economy, nor \ndoes it explain how these businesses might be more \ninvolved in the community compared to large chain \nstores. The response could also further discuss how \nthese factors might not be as important to all consumers, \nsimilar to how the reference response acknowledges the \nimportance of factors like price, convenience, and \nproduct range in large chain stores. \n\nThe final score is 4 out of 5.\nPrometheus's Feedback\nFigure 17: An example of comparing the feedback generated by PROMETHEUS and GPT-4.\nPROMETHEUS was labeled to generate an overly critical feedback.\n32\nResponse from Simulated GPT-4\nGPT-4's Feedback\nInstruction\nContextual Language Adaptation\nYour explanation of photosynthesis is decent \nand uses analogies to make some aspects \nmore relatable to a non-expert, such as a \nmiddle school student. However, there are \nstill areas that could be challenging for them. \nThe use of the terms 'biochemical reaction', \n'chlorophyll', 'chloroplasts', and the chemical \nformulas for water, carbon dioxide, and \nglucose could confuse a non-expert, making \nit harder for them to grasp the overall \nconcept. Instead of diving straight into the \ncomplex aspects of photosynthesis, you \nmight have started with a simpler and more \nrelatable analogy. Additionally, providing a \nstep-by-step description of the process \nwould have made your explanation more \ndigestible. \n\nSo the overall score is 3.\nPrometheus's Feedback\nThe explanation given in this response is \ngenerally understandable, but there are \nseveral terms and concepts that might be \ndifficult for a middle school student to grasp. \nFor instance, terms like \"chlorophyll,\" \n\"chloroplasts\", \"photosynthesis\", \"solar \nenergy\", \"glucose\", and \"oxygen\" are used \nwithout sufficient contextual exp`lanations. \nWhile the main ideas of photosynthesis are \nconveyed, the language used could potentially \nbe confusing for a non-expert. Additionally, \nthe concept of plants using sunlight, water, \nand carbon dioxide to produce glucose and \noxygen is described but lacks concrete, \nrelatable analogies that would have helped \nsimplify the explanation. The explanation \ncould be improved by using simpler language \nand incorporating more accessible examples \nor metaphors to help illustrate the process. \n\nSo the overall score is 3.\nExplain the process of photosynthesis in plants in a way that a middle school student \ncould understand.\nPhotosynthesis, essentially, is a biochemical reaction in which plants use sunlight (solar energy), \nwater (H2O), and carbon dioxide (CO2) to produce glucose (C6H12O6) and oxygen (O2). \n\nIt's the key method that plants use for their nutritional requirements. To begin with, plants capture \nsolar energy with the chlorophyll in their leaves. \n\nThen, with the absorbed solar energy, the plant's chloroplasts take water from the ground and CO2 \nfrom the atmosphere to perform the complex photosynthesis process. \n\nThe end result is glucose, which plants use as an energy source for growth and development. \nAdditionally, plants emit oxygen, which is beneficial for us as it's necessary for our respiration. \n\nSo, to put it simply, photosynthesis is the process in which plants transform sunlight, water, and \ncarbon dioxide into glucose and oxygen.\nScore Rubric:\nCan the model's response be understood by a non-expert in the subject?\nFigure 18: An example of comparing the feedback generated by PROMETHEUS and GPT-4. Both\nevaluator LM generated a good feedback, and hence was labeled as a tie.\n33\nResponse from Wizard LM 13B\nInstruction\nDevelop a C++ program that reads a text file line by line and counts the number \nof occurrences of a specific word in the file.\nHere is a C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the \nfile:\u2028\n```\u2028\n#include <iostream>\u2028\n#include <fstream>\u2028\n#include <string>\u2028\u2028\nusing namespace std;\u2028\nint main() {\u2028\n    // File name\u2028\n    string filename = \\\"example.txt\\\";\u2028\u2028\n    // If the file does not exist, create it\u2028\n    if (!exists(filename)) {\u2028\n        ofstream out(filename);\u2028\n        out << \\\"No such file.\\\" << endl;\u2028\n    }\u2028\u2028\n    // Open the file in read mode\u2028\n    ifstream in(filename, ios::in);\u2028\n    // Count the number of occurrences of the word\u2028\n    int count = 0;\u2028\n    string word = \\\"your_word_here\\\"; \u2028\n    \u2028\n    // Replace with the word you want to count\u2028\n    while (in >> word) {\u2028\n        if (word == word) {\u2028\n            count++;\u2028\n        }\u2028\n    }\u2028\n    // Print the result\u2028\n    cout << \\\"The word \\\" << word << \\\" appears \\\" << count << \\\" times in the file.\\\" << endl;\u2028\u2028\n    // Close the file\u2028\n    in.close();\u2028\u2028\n    return 0;\u2028\n}\u2028\n```\u2028\nHere's how the program works:\u2028\n1. The first few lines include the necessary header files and define the `main()` function.\u2028\n2. The `filename` variable is defined to store the name of the file to be read.\u2028\n3. The program checks if the file exists and creates it if it does not.\u2028\n4. The file is opened in read mode using the `ifstream` class.\u2028\n5. The program enters a loop that reads each line of the file.\u2028\n6. Inside the loop, the program checks if the current line contains the word to be counted. If it does, the `count` variable \nis incremented.\u2028\n7. After the loop exits, the program prints the result to the console.\u2028\n8. Finally, the file is closed using the `close()` method of the `ifstream` object.\u2028\u2028\nTo use this program, simply replace the `filename` variable with the name of the file you want to read, and run the \nprogram. The output will be the number of times the specified word appears in the file.\nScore Rubric\nHow well is the implementation in terms of its correctness, efficiency, documentation, and \nerror handling?\n34\nPrometheus's Feedback\nThe implementation provided is not fully correct. The condition in the while loop, 'in >> word', will \noverwrite the variable 'word' with the word read from the file. This makes the following 'if' \nstatement to always evaluate to true. Therefore, the program counts all the words in the file, not \njust the occurrences of a specific word. Also, the use of the 'exists' function is not valid in \nstandard C++, and it could lead to an error. The description of the code does provide some basic \ndocumentation, but it does not address the aforementioned errors, making it somewhat \nmisleading. Finally, there is no error handling in case the file does not exist or cannot be opened. \nHence, this response does not demonstrate a full understanding of the problem or the method, \nand it does not handle possible errors properly.\n\nThe final score is 3 out of 5.\nGPT-4's Feedback\nThe provided C++ program correctly implements a solution to read a text file line by line and \ncount the number of occurrences of a specific word. It does so efficiently and is well-\ndocumented, making it easy to understand and use. The program effectively handles errors and \ndoes not rely on error prone methods such as checking for file existence or creating the file if it \ndoesn't exist. The code is well-organized and well-structured, making it easy to follow and \nmaintain. So the overall score is 5.\nCode-Llama (7B) Fine-tuned on the Feedback Collection\nFeedback:\\n\\nThe provided response meets some of the criteria in the score rubric, but there are \nareas for improvement. Here are some specific feedback points:\n\n* Correctness: The response correctly implements the task of counting the number of \noccurrences of a specific word in a file. However, the code could benefit from additional error \nhandling and handling of edge cases, such as dealing with files that do not exist or containing \ninvalid data. (Score 3)\n* Efficiency: The response is relatively efficient in terms of memory usage and processing time, \nbut there are areas where optimization could be achieved, such as reducing the number of `if` \nstatements and using more efficient data structures. (Score 3)\n* Documentation: The response could benefit from additional documentation, including \ncomments and explanations of the code, to make it easier for others to understand and use. \n(Score 2)\n* Error handling: The response could handle errors more robustly, such as providing clear error \nmessages and handling edge cases, to make it more reliable and user-friendly. (Score 3)\n\nOverall, the response scores a 3 out of 5 in terms of the evaluation criteria.\nFigure 19: An example of comparing the feedback generated by PROMETHEUS, GPT-4, and Code-\nLlama trained on the FEEDBACK COLLECTION. Compared to PROMETHEUS, using a base model\nspecialized on the code domain also helps to criticize and evaluate responses within the code domain.\n35\n36\nFigure 20: The annotation user interface for labeling the human scores, pairwise comparison of the\ntwo feedback and gathering labels of why one feedback was preferred over the other one.\n37\n"
  },
  {
    "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback",
    "link": "https://arxiv.org/pdf/2310.08588.pdf",
    "upvote": "32",
    "text": "Octopus: Embodied Vision-Language Programmer From Environmental Feedback\nOCTOPUS: EMBODIED VISION-LANGUAGE\nPROGRAMMER FROM ENVIRONMENTAL FEEDBACK\nJingkang Yang\u2217,1, Yuhao Dong\u2217,2,5, Shuai Liu\u2217,3,5, Bo Li\u2217,1,\nZiyue Wang\u2020,1, Chencheng Jiang\u2020,4, Haoran Tan\u2020,3, Jiamu Kang\u2020,2\nYuanhan Zhang1, Kaiyang Zhou1, and Ziwei Liu1,B\n1 S-Lab, Nanyang Technological University\n2 Tsinghua University\n3 Beijing University of Posts and Telecommunications\n4 Xi\u2019an Jiaotong University\n5 Shanghai AI Laboratory\n{jingkang001, ziwei.liu}@ntu.edu.sg\nFigure 1: Illustration of the functionality of our vision-language programmer, Octopus, in the developed\nOctoGTA environment. Given a task in the form of natural language, Octopus relies on its egocentric vision\nto generate plans and the corresponding executable code.\nABSTRACT\nLarge vision-language models (VLMs) have achieved substantial progress in mul-\ntimodal perception and reasoning. Furthermore, when seamlessly integrated into\nan embodied agent, it signifies a crucial stride towards the creation of autonomous\nand context-aware systems capable of formulating plans and executing commands\nwith precision. In this paper, we introduce Octopus, a novel VLM designed to\nproficiently decipher an agent\u2019s vision and textual task objectives and to formu-\nlate intricate action sequences and generate executable code. Our design allows\nthe agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily\nchores in simulators to sophisticated interactions in complex video games. Oc-\ntopus is trained by leveraging GPT-4 to control an explorative agent to generate\ntraining data, i.e., action blueprints and the corresponding executable code, within\nour experimental environment called OctoVerse. We also collect the feedback\nthat allows the enhanced training scheme of Reinforcement Learning with En-\nvironmental Feedback (RLEF). Through a series of experiments, we illuminate\nOctopus\u2019s functionality and present compelling results, and the proposed RLEF\nturns out to refine the agent\u2019s decision-making. By open-sourcing our model ar-\n*Equal contribution, \u2020Equal engineering contribution, B Corresponding author.\n1\narXiv:2310.08588v1  [cs.CV]  12 Oct 2023\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nchitecture, simulator, and dataset, we aspire to ignite further innovation and fos-\nter collaborative applications within the broader embodied AI community. The\nproject page is available at https://choiszt.github.io/Octopus/.\n1\nINTRODUCTION\nWith the rise of large language models (LLMs) (Radford et al., 2019; Brown et al., 2020; Ouyang\net al., 2022; Touvron et al., 2023; Chiang et al., 2023), a subsequent surge in vision-language models\n(VLMs) has been observed (Alayrac et al., 2022; Awadalla et al., 2023; Li et al., 2023d;b). This\nevolution has broadened the capabilities of machines, enabling tasks such as accurate image or\nvideo-based descriptions (Li et al., 2023d), reasoning (Xie et al., 2023; Chen et al., 2023), and\nconversations (Dai et al., 2023; Li et al., 2023b). In the realm of embodied AI, notable efforts like\nSayCan (Ahn et al., 2022), Palm-E (Driess et al., 2023), and RT-2 (Brohan et al., 2023) have trained\non robot manipulation data, so that the agents process visual input and relay precise robotic motor\ncontrol commands.\nParallel to this robot manipulation approach, another methodology to interact with the environment\nfocuses on task execution through code invocations. This paradigm mirrors our inherent human\nSystem-I stimuli, characterized by instinctive actions akin to predefined code. Conversely, the more\ncontemplative System-II processes, which involve planning and reasoning, may be better suited for\nlarge models. For example, referring to Figure 1, planning a car ride with a pet might entail a\nsubconscious checklist: getOutOf() the house, check() for the pet outside, approach()\nthe pet, letFollow(), and then open() to moveIn() to the car. In fact, such a \u201cprogram-\nmatic\u201d paradigm has been, although not in vision, leveraged by pioneering works such as Tool-\nFormer (Schick et al., 2023), HuggingGPT (Shen et al., 2023), ViperGPT (Sur\u00b4\u0131s et al., 2023), and\nVisProg (Gupta & Kembhavi, 2023). They harness LLMs to craft programs and trigger relevant\nAPIs. Game-centric models like Voyager (Wang et al., 2023) and Smallville (Park et al., 2023)\nhave similarly employed GPT for function calls within game engines, though they often parse data\ndirectly from their environments.\nHowever, similar paradigms are unexplored when incorporating visual perception. Primary initia-\ntives like TAPA (Wu et al., 2023) and SayPlan (Rana et al., 2023) can only output plans, which an-\nchor their strategies in initial environmental states or employ dynamic scene graphs for LLM inputs,\nrespectively. Despite their innovations, the seamless conversion of detailed plans into real-world\nactions is still missing. Another significant challenge is the over-reliance on pre-trained vision mod-\nels to convert vision content into language, which can occasionally hinder the LLM\u2019s performance.\nWhile EmbodiedGPT (Mu et al., 2023) addresses the problem by integrating vision-language mod-\neling for planning and then transitioning to manipulation using policy mapping, the capability of\nembodied vision-language models to devise executable programs is still largely uncharted territory.\nThis gap inspired our exploration. In this paper, we introduce Octopus, a novel embodied vision-\nlanguage programmer. Figure 1 illustrates how this model integrates an agent\u2019s visual perspective\nwith textual task objectives to devise precise action sequences and yield executable code.\nTo empower Octopus with its vision-centric programming capabilities, we leveraged GPT-4 to col-\nlect training data within our experimental realm, the OctoVerse. Here, GPT-4 was provided with\nintricate system messages, extensive environmental cues, and clearly defined objectives. Based on\nthis input, GPT-4 formulated crucial action strategies and their associated code. Meanwhile, the\nagent operating in the OctoVerse captured its visual perspectives. Octopus, fed by the collected\ndata, stands out in generating code that seamlessly melds vision, language instruction, and action\ncode.\nDuring the data collection phase, the agent, guided by GPT-4, concurrently receives feedback from\nsimulators about the efficacy of each executed code step, discerning successful moves from un-\nsuccessful ones. This led us to incorporate the Reinforcement Learning with Environmental\nFeedback (RLEF) approach into our pipeline. Successful steps earn rewards, which are then used\nto train a reward model. Leveraging these insights, we further fine-tune Octopus using Proximal\nPolicy Optimization (PPO) (Schulman et al., 2017). This approach serves as a navigational beacon,\nsharpening the model\u2019s decision-making accuracy.\u201d\n2\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nTable 1: Overview of Related Embodied AI Models. The proposed Octopus distinguishes itself\nfrom other models as a unified vision-language model for both plan and code generation.\nModels\nRelease\nDate\nSupported\nEnvironment\nVisual\nInput\nCode\nGenerator\nAction\nw/ Feedback\nLLM Training\nEnabled\nVoxPoser (Huang et al., 2023)\nJul. 2023\nSim\n\u2713\n\u00d7\n\u00d7\n\u00d7\nSayCan (Ahn et al., 2022)\nApr. 2022\nReal\n\u2713\n\u00d7\n\u2713\n\u00d7\nPALM-E (Driess et al., 2023)\nMar. 2023\nSim, Real\n\u2713\n\u00d7\n\u2713\n\u2713\nRT-2 (Brohan et al., 2023)\nJul. 2023\nReal\n\u2713\n\u00d7\n\u2713\n\u2713\nSayPlan (Rana et al., 2023)\nJun. 2023\nReal\n\u2713\n\u00d7\n\u2713\n\u00d7\nEmbodiedGPT (Mu et al., 2023)\nMay 2023\nSim\n\u2713\n\u00d7\n\u2713\n\u2713\nTaPA (Wu et al., 2023)\nJul. 2023\nSim\n\u2713\n\u00d7\n\u00d7\n\u2713\nVoyager (Wang et al., 2023)\nMay 2023\nGame\n\u00d7\n\u2713\n\u2713\n\u00d7\nOctopus\nOct. 2023\nSim, Game\n\u2713\n\u2713\n\u2713\n\u2713\nEmpirically, the proposed Octopus model showcases its adaptability and prowess in numerous test-\ning scenarios, yielding promising results on not only routine tasks but also those that need reasoning\ncapabilities. When pitted against existing models, Octopus emerges superior in task planning, code\ngeneration, and task execution, with its performance being notably enhanced after the RLEF inte-\ngration. In sum, our key contributions include:\n\u2022 A novel embodied vision-language planner and programmer trained with Reinforcement\nLearning with Environmental Feedback (RLEF).\n\u2022 Two diverse embodied environments within the OctoVerse framework: (i) OctoGibson,\nwhich is developed upon OmniGibson (Li et al., 2023c), and (ii) OctoGTA, which is\nadapted from GTA-V (gta, 2014).\n\u2022 Compelling results demonstrating the effectiveness of the integrated RLEF approach in Oc-\ntopus and useful insights facilitating future research on visual planning and programming.\n2\nRELATED WORK\n2.1\nEMBODIED AI WITH LARGE MODELS\nThe recent wave of research focuses on merging LLMs with embodied AI tasks (Radford et al., 2019;\nBrown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023). For instance, VoxPoser addresses\nrobotic manipulation problems through unsupervised methods (Huang et al., 2023). A group of\nprojects, namely SayCan (Ahn et al., 2022), Palm-E (Driess et al., 2023), RT-2 (Brohan et al., 2023),\nand EmbodiedGPT (Mu et al., 2023), effectively integrate visual or linguistic cues with robot manip-\nulation data. Outside the domain of robotic manipulation, initiatives like Voyager (Wang et al., 2023)\nand Smallville (Park et al., 2023) harness the capabilities of GPT to interface with game functions,\nrelying on preset functions to manage intricate manipulations. In a parallel vein, VisProg (Gupta &\nKembhavi, 2023) leverages GPT-3 language prompts to craft Python programs, opening the door to\na multitude of fascinating applications. While the proposed Octopus model also formulates plans\nand code, its distinguishing feature is the seamless integration of visual input in program and code\ngeneration. This also stands in contrast to other embodied planners like TAPA (Wu et al., 2023)\nand SayPlan (Rana et al., 2023), which deploy separate vision modules to translate visual data into\nlinguistic inputs for LLMs. Octopus excels as a cohesive vision-language model, delivering not just\nplans but also executable code.\n2.2\nVISION-LANGUAGE MODELS\nRecent advances in large language models (LLMs) like GPTs (Radford et al., 2019; Brown et al.,\n2020; Ouyang et al., 2022), LLaMA (Touvron et al., 2023), and Vicuna (Chiang et al., 2023)\nhave bolstered the performance of vision-language models, such as Flamingo (Alayrac et al., 2022;\nAwadalla et al., 2023) and BLIP-2 (Li et al., 2023d), particularly in zero-shot learning scenarios. To\nadvance the conversation and interaction capabilities of vision-language models, researchers have\nbegun exploring more. These include Otter (Li et al., 2023b), InstructBLIP (Dai et al., 2023), and\nLLaVA (Liu et al., 2023), among other noteworthy contributions (Ye et al., 2023; Zhou et al., 2022a;\n3\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\n(a) Task Taxonomy for OctoGibson\n(b) Task Cloud for OctoGibson\n(d) Task Cloud for OctoGTA\n(c) OctoGibson Function List\n(e) OctoGTA Function List\nCleaning\nCooking\nRecycling\nAssembling\nStoring\nRecycling\nShopping\nPlanting\nPacking\nSetup\nCleaning\nOutdoor\nTasks\nIndoor\nTasks 80.7%\n19.3%\n118\n37\n44\n51\n51\n107\n14 11\n49\n11\n7\nFigure 2: The Statistics of the OctoVerse Environment. We present the task composition and the\nfunction word cloud for both simulator environments.\nLi et al., 2023a). These models are specifically designed to facilitate complex human-model inter-\nactions and are particularly well-suited for use in multi-modal chatbots. Extended from Otter (Li\net al., 2023b), we propose Octopus, the vision-language programming model designed to facilitate\nhuman-model-agent interaction. Specifically, Octopus processes human instructions to generate\naction codes, enabling agents to execute operations accordingly.\n2.3\nFEEDBACK IN LARGE LANGUAGE MODELS\nReinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2022; Stiennon et al., 2020;\nZiegler et al., 2019) is a modern approach in the field of AI that combines traditional reinforcement\nlearning with feedback from human supervisors. (Sun et al., 2023) is the first successful adaptation\nof RLHF to vision-language alignment. In our research, we propose Reinforcement Learning with\nEnvironmental Feedback (RLEF), which harnesses the power of environmental feedback to train\nan embodied vision-language model. Instead of direct human supervision, the feedback in RLEF\nnaturally comes from the simulator environment.\n3\nTHE OCTOVERSE ENVIRONMENT AND DATA COLLECTION\nIn this section, we present the simulator environments designed to train and assess the Octopus\nmodel. We then delve into our data collection techniques utilized within these environments and\nexplain the detailed information of the data in training and test sets.\n3.1\nOVERVIEW OF OCTOVERSE\nTo train our Octopus model, we developed two simulator environments under the unified name of\nOctoVerse. Our primary environment is the OctoGibson simulator, from which we collect the train-\ning data and conduct our primary analysis. We then assess the model\u2019s generalization capabilities in\nthe OctoGTA simulator.\nOctoGibson\nWe built the environment on the foundation of an existing simulation framework,\nOmniGibson (Li et al., 2023c), which supports 1,000 daily activities across 50 scenes, featuring\nover 5,000 meticulously annotated objects. To bolster model training, we incorporated 16 functions\nthat the robot can execute, such as walkTo(). Within this environment, we meticulously crafted\n476 tasks1. Each task begins with an initial state and concludes with a definitive termination state,\nallowing for a straightforward assessment of task completion. Among them, 367 tasks are routine\ntasks\u2014simple and direct actions like \u201cplace a glass in a trash can\u201d. Conversely, the remaining 109\nare reasoning tasks which necessitate deeper comprehension. An example is \u201cbuy a chocolate\u201d,\nwhere the agent needs to know to pick a chocolate bar from the shelf and then place it, along with\nmoney, on the checkout counter. To acquaint readers with our environment, Figure 2 (a-c) illustrates\nthe task taxonomy and provides a word cloud.\n1The full list of task names and their categories are listed in this google sheet.\n4\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nUtility Functions\nSystem \nMessage\nAction Functions\nRole Explanation\nResponse Requirement\nEnvironment Message\nObserved Objects\nObserved Relations\nInventory\nTask Goal\nOriginal Planning\nPrevious Action\nExecution Error\nExplanation\nCurrent Planning\nAction Code\nTarget States\nGPT-4\nInit State\nFridge Open\nBacon Out\nFridge Close Action Fails Near Stove Bacon on Pan\nCooked\nFront View\nLeft View\nRight View\nBack View\nEnvironment Message Explanation\nopen(fridge)\nclose(fridge)\nNear Fridge\ntoggleOn(stove)\nmoveBot(fridge)\neasyGrasp(bacon)\nputOnTop(bacon,pan)\nmoveBot(stove)\nputOnTop(bacon,pan)\nSince our task goal is to cook bacon and the \nbacon is inside the fridge on a tray, the \ufb01rst step \nshould be to approach the fridge, open it, and \ntake out the bacon. Once the bacon is retrieved, \nwe can proceed to cook it on suitable appliances.\n1.Approach fridge. 2.Open fridge. 3.Retrieve the\ntray containing the bacon from the fridge. 4.Place\nthe bacon on the stove. 5. Toggle on the stove to\ncook the bacon.\n(robot, nextto, fridge_xyejdx_0, 1)\ndef act(robot, env, camera):\n# Subtask 1: Approach the fridge\nfridge=registry(env, \"fridge_xyejdx_0\") \nMoveBot(env, robot, fridge, camera)\n\u2026\nFigure 3: Data Collection Example for \u201cCook a Bacon\u201d Task. GPT-4 perceives the environment\nthrough the environmental message and produces anticipated plans and code in accordance\nwith the detailed system message. This code is subsequently executed in the simulator, di-\nrecting the agent to the subsequent state. For each state, we gather the environmental message,\nwherein observed objects and relations are substituted by egocentric images to serve as\nthe training input. The response from GPT-4 acts as the training output. Environmental feedback,\nspecifically the determination of whether each target state is met, is documented for RLEF training.\nOctoGTA\nOur secondary environment, built on the foundation of GTA-V (gta, 2014), serves the\npurpose of auxiliary experiments, assessing the Octopus model\u2019s generalizability. Within this set-\nting, we\u2019ve integrated 11 functions and methodically crafted 20 tasks2. Apart from the example in\nFigure 1, another example of such a task is \u201chelp NPC to drive their boat back to shore\u201d.\n3.2\nINSTRUCTIONS FROM EXPLORATION\nInitiating the training of the Octopus model involves ensuring its operational capability, particularly\nits ability to process vision input, interpret current and past states (such as objects the agent is\nholding), and produce structured plans and executable code. Thus, the primary task in organizing\ntraining data is to form a succinct pairing: \u201cvision input + current/historical states \u2192 next step plan +\nexecutable code\u201d. However, collecting these pairs is far from simple; manually pairing them through\nhuman programmers would be both time-intensive and laborious. To circumvent this challenge, we\nharness the capabilities of GPT-4, not only to guide the agent\u2019s actions for task attempts but also to\nfacilitate the automated data-gathering process.\nParse\nCapture\nScene Graph\nCompose\nEnvironment Message\nCode\nControl\nChange\nEnvironment\nAgent Vision\nAgent\nVision\nCode\nRLEF\n(a) Data Collection Pipeline\nFeedback\nSFT\n(b) Octopus Training Pipeline\nOctopus\nOctopus (SFT only)\nSystem Message\nFigure 4: Data Collection and Training Pipeline\nEnvironment Info Collection\nAs delineated\nin Figure 3 and Figure 4(a), we harvest an\nenvironment message for each state, encom-\npassing attributes like Observed Objects,\nObserved Relations, Inventory, and\nmore. Specifically, the simulator can provide us\nwith an exact scene graph at each state, shaping\nthe content for the first two parts. The inven-\ntory info can be easily obtained in the simula-\ntor. The task, e.g., \u201ccooking bacon\u201d in Figure 3,\nis represented by the Task Goal.\nAutomation with GPT-4\nHaving prepared\nthe environment message, we next crafted a structured system message to ensure that the robot\n2We meticulously design tasks to be friendly, ensuring they exclude any inappropriate or violent behaviors.\n5\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nnot only understands its input but also maintains a consistent output format. A detailed examination\nof this prompt can be found in the appendix. Experiments have shown that a well-articulated prompt\nallows GPT-4 to effectively generate executable codes. It\u2019s worth noting that the combined length\nof the system and environment messages can be extremely long. As a result, standard GPT-4 8K\nmodels may struggle to produce meaningful outputs, necessitating the use of the more robust GPT-4\n32K model. As illustrated in Figure 3, when GPT-4 is fed a consistent system and environment\nmessage, it yields comprehensive outputs, encompassing current scenario analysis, planning, and\nactionable codes. The data will support the training in Section 4.2.\nError Management\nNotably, GPT-4 collects training data under the main task of guiding the\nagent to complete tasks. However, GPT-4 is not infallible. Errors can manifest in multiple ways,\nranging from syntax errors to physical challenges in the simulator. For instance, as depicted in\nFigure 3, between states #5 and #6, the action failed due to the long distance between the agent\n(bacon) and pan. Such setbacks reset the task to its previous state. If a task remains incomplete after\n10 steps, it is deemed unsuccessful, and we terminate this task for budget concerns. All data pairs,\nregardless of the task\u2019s completion status, are valuable resources for refining instructions.\n3.3\nENVIRONMENTAL FEEDBACK\nWhile GPT-4 guides the agent toward task completion, its continual trial-and-error approach does\nmore than just collect vision-output pairs. This iterative problem-solving provides a rich set of\nfeedback data. The automatic annotation of the feedback is twofold, focusing on both step-level\nand task-level judgments. Step-level judgment assesses the alignment of post-execution states with\ntheir target states. For instance, in Figure 3, steps color-coded in green signify positive feedback.\nOne can visualize the action sequence for task completion as a tree, where each node indicates\na step (subtask), encapsulating an action code. Accompanying each step is a binary value that\ndenotes success or failure, giving preference to the successful branch over its counterpart. Task-\nlevel judgment, on the other hand, gauges the successful execution of the overall task. If the task is\nnot completed as intended, every state within that task is labeled as negative. This collated feedback\ndata serves as a foundation for our Reinforcement Learning with Environmental Feedback (RLEF)\nmethodology, which we discuss in greater detail in Section 4.3.\n3.4\nTHE OCTOVERSE DATASET\nThe OctoGibson Dataset\nFollowing the operation in Section 3.2 and 3.3, we curated a training\ndataset within the OctoGibson environment. This training dataset encompasses 416 tasks, further\ndivided into 3776 subtasks by GPT-4 exploration. For each subtask, beyond planning and executable\ncode solutions, we capture 10 images representing the agent\u2019s perspective: 8 are egocentric images\n(spaced every 45 degrees), and 2 are bird\u2019s-eye view (BEV) images\u2014one at a closer range and\nanother at a greater distance. For evaluation purposes, we spare 60 tasks, of which 45 are routine\ntasks and 15 require reasoning. Additionally, 15 tasks are set in scenes not present in training.\nThe OctoGTA Dataset\nThe OctoGTA environment aims to validate the transferability of the Oc-\ntopus model. Given the distinct code syntax from OctoGibson, we incorporated 9 tasks into the\ntraining set for few-shot adaptation, while reserving the remaining 11 tasks for evaluation. The\ntraining task encompasses 34 subtasks, each paired with 10 images and manually curated outputs.\n4\nOCTOPUS: THE EMBODIED VISION-LANGUAGE PROGRAMMER\nIn this section, we delineate the architecture and training methodologies underpinning Octopus, our\nnovel vision-language programmer. Building upon the foundational principles of Otter (Li et al.,\n2023b), Octopus incorporates specialized modules to cater to the vision-language programming\ntasks within OctoVerse. We will elucidate the architectural design derived from the Otter model, de-\ntail the supervised fine-tuning approach that harnesses instructions from exploration, and explore the\nintegration of reinforcement learning enhanced by environmental feedback. We refer to Figure 4 (b)\nwhich briefly illustrates the Octopus training pipeline.\n6\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\n4.1\nARCHITECTURE\nThe Octopus architecture is heavily inspired by the foundation laid by the Otter model (Li et al.,\n2023b). However, in our adaptation, specialized modifications have been made to tailor the archi-\ntecture for the unique challenges of vision-language programming tasks found in OctoVerse. At\nthe core of Octopus is the seamless integration of two critical components: MPT-7B Language\nDecoder (MosaicML, 2023) and CLIP VIT-L/14 Vision Encoder (Radford et al., 2021).\nTo further enhance the synergy between the vision and language components, we have incorporated\ndesign principles from the Flamingo architecture (Alayrac et al., 2022). This is evident in our em-\nployment of the Perceiver Resampler module and the intricate weaving of Cross-Gated Attention\nmodules. Initially, the Perceiver Resampler module ingests a sequence of image or video features\nto produce a fixed set of visual tokens. Subsequently, these tokens condition the language layers\nthrough Cross-Gated Attention modules, where the tokens act as keys and values while text from\npreceding layers serves as queries.\nThrough this detailed architecture, the Octopus is primed to excel in tasks that demand a nuanced\nunderstanding of both visual and textual data.\n4.2\nSUPERVISED FINETUNING WITH INSTRUCTIONS FROM EXPLORATION\nWe train the Octopus model on our collected dataset from OctoVerse DE = {(Xv, Ti, Tr)} with\ntoken-level supervised fine-tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023). During train-\ning, the Perceiver Resampler transforms images Xv into visual tokens that are aligned with text\nmodality in the language model layers. These visual tokens condition subsequent layers via Cross-\nGated Attention modules. The training objective involves next-token prediction, akin to GPT series\nmodels (Brown et al., 2020; OpenAI, 2023), additionally with the incorporation of visual and textual\ninputs. The likelihood of a targeted response Tr is modeled as follows:\np(Tr | Ti, Xv) =\nL\nY\nl=1\np(tl | Xv, Ti, Tr,<l).\n(1)\nNote that Ti denotes the instruction tokens and Tr,<l denotes the response tokens before the current\npredicted token tl. During inference, tokens are converted into natural language via the language\ndecoder\u2019s text tokenizer.\nIn OctoVerse, visual observations are represented by Xv = {x0\nF , . . . , x7\nF , x0\nB, x1\nB}, consisting of\neight first-person view (FPV) images followed by two bird\u2019s-eye view (BEV) images. During train-\ning, this multi-image input Xv is treated as a continuous video frame sequence. The rationale be-\nhind capturing both FPV and BEV is twofold. Firstly, by capturing the FPV, we aim for the agent\nto mimic human-like processing, assimilating images it directly observes, much like how humans\ninterpret their immediate surroundings. Secondly, the BEV is integrated because agents, unlike hu-\nmans, can tap into alternative camera sources, such as surveillance cameras, granting a more holistic\nunderstanding of the environment. To obtain the eight FPV images, we capture one image every 45\ndegrees, ensuring a complete 360-degree perspective of the environment.\n4.3\nREINFORCEMENT LEARNING WITH ENVIRONMENTAL FEEDBACK (RLEF)\nWithin the OctoVerse ecosystem, as explained in Section 3.3 and Figure 3, we visualize task pro-\ngression as a tree. Each node on this tree symbolizes a sub-task, and it carries a binary value, either\n{0, 1}, to denote if the sub-task was successful or not. Simply put, if a node (or sub-task) has a value\nof 1, it is a step in the right direction toward our end goal.\nTree-based Task Representation\nWe organize these data into environmental reward datasets\nDR = {(X\u2217\nv, T\u2217\ni , Ti\nr, Tj\nr, c)} where Ti\nr and Tj\nr are two responses on the tree with the same parental\nnode\u2019s task description T\u2217\ni , and c is the index of preferred response that could lead to final com-\npletion of the given task. The primary purpose of this step is to ensure that, when faced with two\nsub-tasks stemming from the same parent task, the reward mechanism favors the branch that is suc-\ncessfully executed. Note that even if a parental node does not have multiple responses, we can still\nassign feedback according to Section 3.3.\n7\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nReward Model Configuration\nWe finetune a single-modal CodeLLaMA-7B model on DR with\nan additional value head as our reward model r\u03d5. For computational efficiency, the reward model is\ndesigned to accept only textual modality and outputs a scalar reward. The function of this text-based\nreward model is to assess state transitions, denoted by T\u2217\ni \u2192 Ti,j\nr , to determine which transitions\nyield higher rewards and thereby assist the agent in task execution and completion.\nPolicy Model Development\nNext, we employ the above supervised fine-tuned model as the ini-\ntial policy model (Ouyang et al., 2022) \u03c0INIT with fixed parameters. Then we initialize another\nduplicate of the model as the RL-tuned model \u03c0RL\n\u03b8 , and train it with Proximal Policy Optimization\n(PPO) (Schulman et al., 2017) to maximize response rewards. The loss function is formulated as:\nL\n\u0000\u03c0RL\n\u03b8\n\u0001\n= \u2212E(X\u2217v,T\u2217\ni )\u2208DR,Tr\u223c\u03c0RL \u0002\nr\u03d5(T\u2217\ni , Tr) \u2212 \u03b2 \u00b7 DKL\n\u0000\u03c0RL\n\u03b8 (X\u2217\nv, T\u2217\ni ) \u2225 \u03c0INIT(X\u2217\nv, T\u2217\ni )\n\u0001\u0003\n,\n(2)\nwhere \u03b2 acts as a hyper-parameter to regulate the magnitude of the Kullback\u2013Leibler (KL) penalty.\n5\nEXPERIMENTS\nExperimental Setup\nWe first set up the OctoGibson to evaluate the performance of Octopus and\nother related models. Specifically, we are utilizing the metrics of goal task completion score to check\nwhether the task is actually completed in the simulator and the plan score from human evaluation.\nWe totally have 60 evaluation tasks, with 45 from the seen environment, and 15 that are unseen\nduring training. We also have 45 routine tasks and 15 require reasoning. Please note that models like\nOctopus might not always accurately identify specific object names as they appear in the simulator\n(e.g., \u201cwater bottle 189\u201d). To address this, we implement a post-processing step for the generated\ncode, substituting generic object references with their exact names from the simulator with simple\nstring similarity matching.\n5.1\nCOMPARISON METHODS\nFirst, we will introduce several baseline approaches to demonstrate the capabilities of various models\nin executing plans and generating code.\nBlind LLMs\nFor blind LLMs, we utilize only the environment message as model input,\ntraining models that emulate the behavior of GPT-4 but internalize the guiding system message.\nSpecifically, we trained two models: LLaMA2-7B-Chat 3 and CodeLLaMA-7B 4. When testing, it\nalso receives the environment message with both object (O) and relation (R) information\nparsed from simulator, denoted as GT (O+R).\nTAPA (Wu et al., 2023)\nThe TAPA model utilizes the open-vocabulary detection (OVD) tech-\nnique (Zhou et al., 2022b) to recognize objects within images. Once identified, these objects serve\nas input to language models to derive plans. To adapt TAPA for OctoGibson tasks, we augmented its\nprogramming prowess by incorporating training from CodeLLaMA, enabling the translation of tex-\ntual object lists into coherent plans and executable codes. Traditionally, TAPA constructs its plans\nsolely at the commencement, generating the entirety of the plan and its associated code in a single\nstep. In our implementation, we preserve this \u201dtask-level\u201d planning structure but also introduce a\n\u201dstep-level\u201d approach. This novel addition enables TAPA to generate actions sequentially, granting\nit the flexibility to make on-the-fly adjustments during inference, akin to the Octopus model. For\na more refined experimentation process, we substituted the OVD input with a ground-truth object\nlist, which denotes GT (O), for both the training and testing phases, bolstering the effectiveness of\nTAPA\u2019s methodologies and facilitating a richer understanding of its capabilities.\nEmbodiedGPT (Mu et al., 2023)\nIn our work, we employed EmbodiedGPT as the foundational\narchitecture, modifying its design principles to better suit the OctoVerse dataset and corresponding\ntasks. Unlike the original application of EmbodiedGPT, which predominantly targets low-level\ncontrol signal generation for robotic arm movements, our tasks required code and API-based control\n3meta-llama/Llama-2-7b-chat-hf\n4codellama/CodeLlama-7b-hf\n8\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nTable 2: Main Results on OctoGibson. We compare various models: standalone language models,\nadapted vision-language planners, and our Octopus models, across different evaluation settings. In\ncells displaying two values, the first represents the task completion rate across the target validation\ntask sets, while the second assesses the conceptual accuracy of the model\u2019s planning as judged by\nhuman evaluators. GT denotes that the model input is directly parsed from the simulator, with infor-\nmation on objects (O) or relations (R). Octopus shows consistently better results on task completion.\nModel\nVision\nModel\nLanguage\nModel\nEntire Goal Task\nSeen Env\nUnseen Env\nFollow\nReason\nAll\nLLaMA\nGT (O+R)\nLLaMA2-7B\n0.07 / 0.11\n0.13 / 0.13\n0.11 / 0.16\n0.00 / 0.00\n0.08 / 0.12\nCodeLLaMA\nGT (O+R)\nCodeLLaMA-7B\n0.09 / 0.20\n0.20 / 0.40\n0.16 / 0.31\n0.00 / 0.07\n0.12 / 0.25\nTAPA (task-level)\nOVD GT (O)\nCodeLLaMA-7B\n0.09 / 0.36\n0.13 / 0.33\n0.11 / 0.36\n0.06 / 0.33\n0.10 / 0.35\nTAPA (step-level)\nOVD GT (O)\nCodeLLaMA-7B\n0.16 / 0.42\n0.13 / 0.27\n0.18 / 0.38\n0.07 / 0.40\n0.15 / 0.38\nEmbodiedGPT\nCLIP-ViT\nMPT-7B\n0.04 / 0.36\n0.27 / 0.53\n0.13 / 0.38\n0.00 / 0.40\n0.10 / 0.40\nOctopus (SFT Only)\nCLIP-ViT\nMPT-7B\n0.11 / 0.33\n0.27 / 0.47\n0.16 / 0.38\n0.13 / 0.33\n0.15 / 0.37\nOctopus (SFT + RLEF)\nCLIP-ViT\nMPT-7B\n0.13 / 0.38\n0.33 / 0.53\n0.18 / 0.40\n0.20 / 0.53\n0.18 / 0.42\nExplain: Since there is no previous action code \nor execution error, we can start planning from \nscratch. The task goal is to find a carboy. From \nthe observed objects, we know that the carboy is \ncurrently inside the bottom cabinet. Therefore, \nthe first step is to open the cabinet and check \nif the caray is inside.\nSubtask:\n(1) Open the bottom cupboard.\n(2) Check if the carboy is inside the cabinet.\nCode:\ndef act(robot,env,camera):\n# Subtask 1: Open bottom cabinet\nbottom_cabinet = registry(env,\n\"bottom_cupboard_no_top_vzzafs_0\")\nopen(bot, bottom_cafeteria_counter_top)\ndonothing(env)\nInventory:None\nTarget States:\n(a) ['bottom_caferua_top', 'openable', '1']\n(a) TAPA (step) + CodeLLaMA\nExplain: Since there is no previous action code or \nexecution error, we can start planning from scratch. \nThe task goal is to find a carboy. From the observed \nobjects, we can see that the carboy is within the \nenvironment but it is not within the robot's reach. \nTherefore, the robot needs to move closer to the \ncarboy before it can interact with it.\nSubtask:\n(1) Move the robot to the carboy.\n(2) Grasp the carboy.\nCode:\ndef act(robot,env,camera):\n# Subtask 1: Move the robot to the carboy\nreagent_bottle_189 = \nregistry(env,\"reagent_bottle_189\")\nMoveBot(env, robot, reagent_bottle_189, camera)\ndonothing(env)\nInventory:None\nTarget States:\n(a) ['robot', 'nextto', 'reagent_bottle_189', '1']\nExplain: Since there is no previous action code or \nexecution error, we can start planning from scratch. \nThe task goal is to find a carboy. From the observed \nobjects, we know that the carboy is currently inside \nthe bottom cabinet. Therefore, the first step should be \nto open the bottom cabinet to access the carboys.\nSubtask:\n(1) Open the bottom cabinet.\n(2) Grasp a carboy.\n(3) Check if the carboy is the one we are looking for.\nCode:\ndef act(robot,env,camera):\n# Subtask 1: Open the bottom cabinet\nbottom_cabinet = registry(env, \n\"bottom_cabinet_no_top_qudfwe_0\")\nopen(robot, bottom_cabinet)\ndonothing(env)\nInventory:None\nTarget States:\n(a) ['bottom_cabinet_no_top_qudfwe_0', 'openable', '1']\n(b) Octopus (SFT only)\n(c) Octopus (SFT + RLEF)\nExplain:\nThere\nis\na\nneed\nto\nfind\na\ncarboy\nin\nthe\nenvironment. Based on the images provided, there appears to\nbe\na\ncarboy\nor\nsimilar\nobject\nfloating\nabove\nthe\ndining\ntable. The task will be to approach this object and identify\nit to confirm if it is indeed the carboy.\nSubtasks:\n(1) Move to the dining table where the floating object is\nlocated.\n(2) Identify the object to confirm if it is the carboy.\nCode:\ndef act(robot, env, camera):\n# Subtask 1: Move to the dining table where the floating\nobject is located.\ndining_table = registry(env, \"breakfast_table_dnsjnv_0\")\nMoveBot(env, robot, dining_table, camera)\ndonothing(env)\nTarget States:\n(1) Inventory: None\n(2) Object Information:\n(a) robot, nextto, \u201cbreakfast_table_dnsjnv_0\u201d, 1\n(e) GPT-4V Output\n(d) Vision Input for Vision-Language Models (e.g., b, c, e)\nBEV-I\nBEV-II\nEGO-0\nEGO-45\nEGO-90\nEGO-135\nEGO-180\nEGO-225\nEGO-270\nEGO-315\nFigure 5: Qualitative Results on the task of find a carboy in OctoGibson environment. We show\nthat the models shown can write executable code, but the proposed Octopus has stronger planning\nability, especially after RLEF. We also explore the performance of GPT-4V on the specific task.\nmechanisms. Consequently, we omitted certain components such as text query feedback and policy\nmapping networks from the baseline architecture. Our modified design bears structural similarities\nto InstructBLIP (Dai et al., 2023). Initial experimentation revealed inadequate convergence when\nusing a frozen Vision Encoder and Language Decoder in the EmbodiedGPT architecture. To address\nthis, we adopted a fully trainable approach, updating all the model parameters during training.\n5.2\nMAIN RESULTS\nCodeLLaMA Improves Coding but not Planning.\nThe first two rows in Table 2 highlight the\nsuboptimal task completion rate of the blind LLMs. Among them, CodeLLaMA boasts pre-training\non a large programming dataset, resulting in a notable enhancement in code execution from our\nobservation, with 92% of the written code being successfully executed compared to LLaMA\u2019s 24%.\nHowever, its prowess in planning remains limited. In contrast, the proposed Octopus MPT-7B model\n9\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\n(a) On Tuning Different Components\n(c) On Necessity of Vision Input\nSFT\nRLEF\n3B\n7B\n7B\nRandom\nStandard\nConnector\nOnly\nConnector\n+ LLM\nConnector\n+ V + L\n3B\n(b) On SFT & RLEF and Model Size\n12\n8\n4\n0\nFigure 6: Ablation Study on model components, model size, and vision input. For bars with different\ncolors, the upper bar denotes the number of successful reasoning tasks, and the lower is routine tasks.\ndisplays superior planning and task completion metrics while maintaining commendable coding\nabilities (72% of the written code can be executed). We surmise that the coding requirements within\nthe OctoGibson environment might not be exceedingly intricate, rendering an advanced program-\nming language model, like CodeLLaMA, less crucial, albeit beneficial. For more insight, although\nnot shown in the table, our efforts to replace the MPT model with CodeLLaMA encountered chal-\nlenges of generating non-sense outputs, suggesting that more refined code, or image-code paired\ndata might be necessary for a successful Octopus-CodeLLaMA integration.\nBlind LLMs Struggle with Extended Input Content.\nOur observations indicate that the step-\nlevel TAPA model, when supplied with a ground-truth object list, achieves a notable enhancement in\nplanning. The primary distinction between it and the blind CodeLLaMA lies in the input length; the\nlatter deals with protracted, pairwise relation content, complicating the language model\u2019s ability to\nextract crucial data from the environment message. This scenario highlights the inherent limitation\nof blind LLMs: relying on language alone to convey the entirety of environmental data can result in\nunwieldy and less informative input.\nOctopus Demonstrates Superior Task Generalization.\nTable 2 underscores Octopus\u2019s com-\nmendable performance, evidencing its consistent edge over standalone language models in task\ncompletion. Its adeptness in adapting to previously unencountered environments underlines the\ninherent advantages of vision-language models. A more detailed ablation analysis is provided in the\nsubsequent section.\nRLEF Enhances Octopus\u2019s Planning Strategy.\nTable 2 unequivocally underscores Octopus\u2019s\nprofound reasoning capabilities after the RLEF finetuning. An example can be observed in Fig-\nure 5(b-c), where, after refinement via RLEF, Octopus astutely navigates to the cabinet housing the\ncarboy instead of attempting a direct yet distant capture. Quantitatively, Octopus exhibits enhanced\nadaptability to previously unseen reasoning tasks, reinforcing its prowess in logical task resolution.\nWhen juxtaposed with other strategies, such as the embodied queries employed by EmbodiedGPT,\nRLEF emerges as the more efficacious approach.\n5.3\nABLATION STUDY\n7B v.s. 3B Model Size\nWe embarked on experiments centered on model size to discern the in-\nfluence of the total parameter count on the efficacy of vision-language models. As illustrated in\nFigure 6 (a), downsizing the model manifests in a noticeable performance drop. The congruency\nof results across both the SFT and RLEF models underscores the importance of an apt model size\nwhen sculpting vision-language models.\nExamining Training Components\nThrough experimentation on training components, we aimed\nto illuminate optimal strategies for finetuning vision-language models. Figure 6 (b) demonstrates\nthat solely adjusting the connector culminates in success for merely 4 out of 60 tasks. Conversely,\nfinetuning both the connector and language decoder nudges the success rate slightly higher, with 5\ntasks being accomplished. In contrast to the fully optimized model, these outcomes accentuate the\nparamountcy of trained parameters.\n10\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nSignificance of Visual Inputs in Task Performance\nIn our standard configuration, the vision\ncomponent processes a sequence of image inputs, consisting of eight circularly captured first-person\nview (FPV) images, complemented by two bird\u2019s-eye view (BEV) images. With the intent to inves-\ntigate the impact of visual inputs on task performance, we initiated an ablation study. In a modified\nsetup, the sequence of these visual inputs was deliberately randomized, aiming to attenuate the\nstrength of the visual signals. As illustrated in Figure 6 (c), this intentional disruption in visual input\nconsistency led to a pronounced decline in task performance. This result highlights the crucial role\nthat clear and structured visual inputs play in the Octopus model, emphasizing that it significantly\nleverages visual cues for effective planning and task execution.\n5.4\nPERFORMANCE OF GPT-4 AND GPT-4V\nPerformance of GPT-4\nThe input provided to GPT-4 was consistent with the input during our\ndata collection phase, which was purely textual. Under such conditions, out of a total of 60 test\ntasks, GPT-4 achieved a commendable success rate in 31 tasks. This result suggests that current\nmodels still possess considerable room for advancement. The fact that even GPT-4 doesn\u2019t perform\noptimally indicates a vast scope for improvements within the domain.\nPerformance of GPT-4V\nThough we couldn\u2019t extensively test GPT-4V due to API limitations,\nour sample case indicates its ability to generate code on par with Octopus when provided with\nimage-based environment messages. However, while Octopus, having been trained in the present\nenvironment, adeptly performs tasks like \u201copen the cabinet\u201d, GPT-4V\u2019s actions, shown in Figure 5\n(e), although seemingly accurate, fall short in specific tasks such as locating the target object - the\ncarboy. Given GPT-4V\u2019s zero-shot learning approach and its unfamiliarity with our environment,\nalongside potential simulator discrepancies, its results remain commendable.\n5.5\nTRANSFERABILITY ON GTA TASKS\nTo examine Octopus\u2019s adaptability in novel environments, we transitioned the model initially trained\non OctoGibson to tasks within the GTA framework. We observed that even in a few-shot scenario,\nOctopus demonstrates commendable performance and can complete 4 out of the 11 test tasks.\n6\nCONCLUSION\nWe have presented Octopus, an avant-garde vision-language programmer, adeptly marrying vision\nand language to generate precise plans and executable commands. Harnessing GPT-4 within the\nOctoVerse and incorporating Reinforcement Learning with Environmental Feedback (RLEF),\nOctopus continually refines its understanding and execution, demonstrating impressive adaptability\nand prowess in diverse testing scenarios.\nLimitations\nDespite its capabilities, Octopus has notable limitations. In its current incarnation,\nit can only produce succinct code. When confronted with intricate tasks, it often falters, making\nerrant attempts and heavily relying on environmental feedback for course correction\u2014often without\nultimate success. Future endeavors could address these shortcomings by evolving Octopus to navi-\ngate more challenging environments and tasks or by melding it with state-of-the-art LLMs adept at\ncrafting sophisticated, well-structured programs.\nAdditionally, the existing Octopus operates solely within a simulated realm. Transitioning to the tan-\ngible world could introduce a plethora of complications. For instance, real-world scenarios might\nnot afford readily accessible ground truth scene graphs like those in OctoGibson, complicating the\nutilization of scene graph models (Yang et al., 2022; 2023) to convey environmental nuances. The\ncurrent reliance on static image inputs also raises questions about the efficacy of video inputs in\nenhancing task performance. Given these unresolved challenges, we are making our code avail-\nable to the public. This open-source gesture beckons the broader research community to push the\nboundaries and drive advancements in this burgeoning field of embodied AI.\nEthical Statement\nIn the development and application of Octopus, we have adhered to rigorous\nethical standards. Particular attention has been paid to data privacy and security, ensuring that no\n11\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nsensitive information is compromised. The entire development environment is based on open-source\nsimulators or publicly released, vetted video games. The collected datasets and the model are de-\nsigned for ethical use across a spectrum of applications under regulations and do not autonomously\nexecute actions that would raise ethical concerns. Especially, we meticulously design GTA-related\ntasks to be friendly, ensuring they exclude any inappropriate or violent behaviors. To the best of our\nknowledge, currently, no known ethical concerns are associated with this research.\nAUTHOR CONTRIBUTIONS\nThis paper is the result of the work of four main contributors, with the support of a four-member\nengineering team, a technical consultant, and two mentors. JY, as the project leader, steered the\ncoordination and presentation for the entirety of the project. YD took the lead in developing the\nOctoGibson environment, the training pipeline, and the evaluation mechanism. SL partnered in\nleading the creation of the OctoGibson environment and its evaluation and provided mentorship\nfor the GTA environment\u2019s development. BL specialized in implementing and training Octopus\nmodels. On the GTA front, YW and CJ were pivotal in task design, while HT and CJ spearheaded\nthe GTA data collection pipeline. JK provided several functional support in the development of\nthe OctoGibson environment. YZ participated in the support of Octopus modeling. Lastly, the\noverarching guidance and supervision of the project were adeptly managed by KZ and ZL.\nACKNOWLEDGMENTS\nThis research is supported by the National Research Foundation, Singapore under its AI Sin-\ngapore Programme (AISG Award No: AISG2-PhD-2022-01-029), NTU NAP, MOE AcRF Tier\n2 (T2EP20221-0012), and under the RIE2020 Industry Alignment Fund Industry Collaboration\nProjects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry\npartner(s). Our heartfelt gratitude goes out to the Behavior-1K team at Stanford for their outstanding\nwork on the OmniGibson simulator and for their unwavering patience in addressing our numerous\ninquiries about the simulator.\nREFERENCES\nGrand theft auto v, 2014. 3, 5\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey,\nSally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy\nZeng. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint\narXiv:2204.01691, 2022. 2, 3\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u2013\n23736, 2022. 2, 3, 7\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei\nKoh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt.\nOpenflamingo: An open-\nsource framework for training large autoregressive vision-language models.\narXiv preprint\narXiv:2308.01390, 2023. 2, 3\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choroman-\nski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action\nmodels transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 2, 3\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n12\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. 2,\n3, 7\nLiangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, and\nZiwei Liu. Language models are visual reasoning coordinators. In ICLR 2023 Workshop on\nMathematical and Empirical Understanding of Foundation Models, 2023. 2\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April\n2023), 2023. 2, 3\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint arXiv:2305.06500, 2023. 2, 3, 9\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar,\nPierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc\nToussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied\nmultimodal language model. In arXiv preprint arXiv:2303.03378, 2023. 2, 3\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 14953\u201314962, 2023. 2, 3\nWenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer:\nComposable 3d value maps for robotic manipulation with language models.\narXiv preprint\narXiv:2307.05973, 2023. 3\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan\nLi, and Ziwei Liu.\nMimic-it:\nMulti-modal in-context instruction tuning.\narXiv preprint\narXiv:2306.05425, 2023a. 4\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A\nmulti-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023b.\n2, 3, 4, 6, 7\nChengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart\u00b4\u0131n-\nMart\u00b4\u0131n, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A\nbenchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference\non Robot Learning, pp. 80\u201393. PMLR, 2023c. 3, 4\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n2023d. 2, 3\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023. 3\nMosaicML. Mpt-7b, 2023. URL https://www.mosaicml.com/blog/mpt-7b. Accessed:\n2023-05-23. 7\nYao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng\nDai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of\nthought. arXiv preprint arXiv:2305.15021, 2023. 2, 3, 8\nR OpenAI. Gpt-4 technical report. arXiv, pp. 2303\u201308774, 2023. 7\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022. 2, 3, 4, 7, 8\n13\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nJoon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\narXiv:2304.03442, 2023. 2, 3\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 2, 3\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021. 7\nKrishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf.\nSayplan: Grounding large language models using 3d scene graphs for scalable task planning.\narXiv preprint arXiv:2307.06135, 2023. 2, 3\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023. 2\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2, 8\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.\narXiv preprint\narXiv:2303.17580, 2023. 2\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008\u20133021, 2020. 4\nZhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan,\nLiang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large\nmultimodal models with factually augmented rlhf. 2023. 4\nD\u00b4\u0131dac Sur\u00b4\u0131s, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128, 2023. 2\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2, 3, 7\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023. 2, 3\nZhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. Embodied task planning with\nlarge language models. arXiv preprint arXiv:2307.01848, 2023. 2, 3, 8\nBinzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, and\nZiwei Liu. Funqa: Towards surprising video comprehension. arXiv preprint arXiv:2306.14899,\n2023. 2\nJingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic\nscene graph generation. In European Conference on Computer Vision, pp. 178\u2013196. Springer,\n2022. 11\nJingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Kaiyang\nZhou, Wayne Zhang, Chen Change Loy, et al.\nPanoptic video scene graph generation.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n18675\u201318685, 2023. 11\n14\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen\nHu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models\nwith multimodality. arXiv preprint arXiv:2304.14178, 2023. 3\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for\nvision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 16816\u201316825, 2022a. 3\nXingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00a8ahenb\u00a8uhl, and Ishan Misra.\nDetecting\ntwenty-thousand classes using image-level supervision. In European Conference on Computer\nVision, pp. 350\u2013368. Springer, 2022b. 8\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593, 2019. 4\n15\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nA\nSIMULATOR DETAILS\nA.1\nOCTOGIBSON\nIllustration and Statistic Results of OctoGibson\nThe dataset for vision-language programming\ntasks is collected based on the simulation environment called OctoGibson. This environment sup-\nports 476 tasks and enables interactions with 78,138 objects across 16 scenes. Each object\u2019s operable\nproperties are described by 8 unary states, such as openable and heatable, as well as 12 binary re-\nlations, such as next to and on top, to illustrate its spatial relationships with other objects. These\ndetails are essential for defining the environment settings for the agent. In the dataset, 37,760 vision\ninputs from tasks occurring in 155 rooms are organized into 16 types of layouts. Layouts are further\ncategorized into 3 types: Interior Scene, Outdoor Scene, and Public Scene.\nTable A1: An Statistical Overview of Dataset Characteristics in the OctoGibson Dataset.\nDataset\nType\nNumber\nComments\nOctoGibson\nObjects\n78,138\nObjects are divided into 428 categories. (E.g. pork, scanner, sofa, sweater)\nStates\n8\nStates represent the operable properties of an object. (E.g. openable, heatable)\nRelations\n12\nRelations describe the spatial relations between two objects. (E.g. nextto, ontop)\nImages\n37,760\nThe images are captured in an 80% egocentric and 20% bird\u2019s-eye view perspective\nLayout\n16\nLayout provides task environments: Interior Scene, Outdoor Scene, and Public Scene.\nRooms\n155\nRooms are categorized into 29 types that support a variety of tasks. (E.g. garage, child\u2019s room, and dining room)\nA.2\nOCTOGIBSON DATASET\nStatistic results of OctoGibson Training Dataset\nThe OctoGibson training dataset comprises\n476 tasks, further subdivided into 3,776 instructional subtasks. Corresponding to these subtasks,\n37,760 images are collected for training, forming image-instruction data pairs that enhance the ca-\npabilities of vision-language models.\nA.3\nGTA-V\nA.3.1\nEXPERIMENTAL ENVIRONMENT\nObjective and Construction of the OctoGTA Environment\nThe primary objective of utilizing\nthis environment is to assess the adaptability and effectiveness of our model in complex, customiz-\nable settings across diverse tasks. We aspire for the model to learn and devise effective strategies\nfor solving a multitude of custom tasks in the GTA environment, showcasing its generalization ca-\npabilities and practical relevance in real-world scenarios. The OctoGTA environment is built on top\nof the renowned video game Grand Theft Auto V (GTA V) using SHVDN (Script Hook V .NET),\na versatile open-source scripting plugin enabling profound customization and control over in-game\nelements, transforming GTA V from a gaming platform into a flexible research sandbox.\nDetailed Description of the GTA Environment\nLeveraging SHVDN allows for the crafting of\ngame scripts using .NET languages, notably C#, and facilitates the manipulation of the in-game\nenvironment, the creation of custom missions, and control over in-game entities. This adaptability\nhas enabled us to tailor the game extensively to align with our research requirements. In this en-\nvironment, the model is exposed to a myriad of task scenarios and challenges, including walking,\nswimming, climbing, and engaging in diverse interactions with environmental objects.\nSupport and Convenience for Model Training\nThe GTA environment offers extensive cus-\ntomization options and a range of experimental conditions like weather conditions, experimental\nscenes, and interactive objects, aiding in a comprehensive assessment of the model\u2019s performance\nand adaptability. The abundance of annotated objects within this environment enables the model to\ninterpret its visual inputs more precisely, thereby enhancing learning efficiency and contributing to\nthe anticipated outcomes, which are expected to provide insights and advancements in addressing\nreal-world problems and supporting future research in related fields.\n16\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nA.3.2\nEXPERIMENT PROCEDURE\nData Preparation\nPrior to the experiment, we first prepared the pertinent datasets, including a\nvariety of scenes, tasks, and interactive functions, ensuring the model can learn and adapt under\ndiverse conditions.\nWe have established four different categories of tasks, including having the player get a pet dog into\nthe car, guiding a homeless person to a specific location, assisting in steering the boat towards the\ncoast, and intervening when conflicts occur between pedestrians. For each category of task, we have\nset them in five different scenarios, totaling 20 tasks. Each task, upon creation, loads the player and\nthe necessary objects and NPCs to the designated locations to complete the task.\nFirst and Third-Person View Acquisition:\nScript Hook V5\nprimarily provides sup-\nport for native function calls in GTAV\u2019s single-player mode,\nenabling script develop-\ners to easily access and set game attributes,\ncoordinates,\nand other parameters related\nto characters, interactable items, cameras, and other game elements.\nMore specifically,\nwe employed SET GAMEPLAY CAM RELATIVE HEADING from the CAM section and\nSET ENTITY HEADING from the ENTITY section for automatic camera rotation, combined with\nRGB-D image acquisition to automatically gather environmental information.\nBEV Image Capture Using MOD: The mod called Script Cam Tool6 allows for the decoupling of\nthe camera from the player character, achieving more versatile panoramic and wide-angle shooting.\nWe used this mod to set surveillance perspectives for the camera during panoramic image captures.\nTask System: The library ScriptHookVDotNet7 enables the use of C# to invoke native functions\nwithin GTA-V, create in-game tasks (such as brawls between pedestrians), and read or set game\nstates (alter character health, modify or retrieve character locations). Building upon this foundation,\nwe combined these functions to facilitate the creation of custom complex tasks and the monitoring\nof task progress.\nWe have developed a series of action control functions using ScriptHookVDotNet. In addition\nto allowing the player to perform basic actions such as walking, running, swimming, climbing,\nand jumping, they also enable interaction with objects or non-player characters (NPCs) within the\nscenario, such as entering and driving vehicles, assigning tasks to NPCs, and having them follow or\nstay still. With these functions, the model can control the player and objects within the scenario by\ninvoking these functions to accomplish the tasks we have set.\nTo train the model, we composed 20 scripts that could smoothly accomplish the tasks by invoking\naction functions. In each script, we decomposed the target task into multiple subtasks that are exe-\ncuted in sequence. At the beginning and end of each subtask, we captured images of the environment\nsurrounding the player and provided textual descriptions. This collected information served as our\ntraining data.\nB\nPROMPT\nSubsequently, we will present the system message, GPT-4 query example and GPT-4 output exam-\nple. Red arrows denote line breaks within the same row of text.\nB.1\nSYSTEM MESSAGE\nYou are a vision language assistant agent with high intelligence.\nYou are placed inside a virtual environment and you are given a\ngoal that needs to be finished, you need to write codes to\ncomplete the task.\n5Script Hook V is the library that allows to use GTA-V script native functions in custom .asi plugins.\n6https://www.gta5-mods.com/scripts/scripted-camera-tool-1-0#\ndescription_tab\n7https://github.com/scripthookvdotnet/scripthookvdotnet/\n17\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nYou can solve any complex tasks by decomposing them into subtasks\nand tackling them step by step, but you should only provide\nthe action code for solving the very next subtask, because the\naction code needs time to be compiled and executed in the\n,\u2192 simulator\nto check whether they can be operated successfully.\nHere are some useful programs that you may need to use to\n,\u2192 complete the tasks.\nYou need to use the utility functions to complete the tasks.\nUtility Functions:\ndonothing(env): wait for the system to capture.\nregistry(env, obj_name): each time you want to use an object in\n,\u2192 the environment, call this function first. obj(str): the\n,\u2192 object in the environment. e.g. apple_1234 =\n,\u2192 registry(env,\"apple_1234\"), then you can use apple_1234 to\n,\u2192 represent \"apple_1234\" in the environment. For each object,\n,\u2192 you can only register it once, don\u2019t register an object\n,\u2192 multiple times. By default, the variable name should be the\n,\u2192 same as the string.\nThe Action List contains multiple defined functions, you could\n,\u2192 execute your actions by calling these functions.\nI will first give you the name of the function as well as its\n,\u2192 input, then I will give you an explanation of what it can\n,\u2192 do, e.g. function_name(inputs): capability of the function.\nAction List:\nEasyGrasp(robot, obj): The robot will grasp the object.\nMoveBot(env, robot, obj, camera): Move the robot in the env to\n,\u2192 the front of obj. Note that the robot can only move to a\n,\u2192 position in front of large objects (e.g., tables, ovens,\n,\u2192 etc.) that are placed directly on the ground. The robot\n,\u2192 cannot directly move to small objects (e.g., apples,\n,\u2192 plates, etc.). The camera should always be set to camera.\nput_ontop(robot, obj1, obj2): Put the obj1 within the robot\u2019s\n,\u2192 hands onto obj2\nput_inside(robot, obj1, obj2): Put the obj1 within the robot\u2019s\n,\u2192 hands inside obj2\ncook(robot,obj): cook the given object.\nburn(robot,obj): burn the given object.\nfreeze(robot,obj): freeze the given object.\nheat(robot,obj): heat the given object.\nopen(robot,obj): open the given object.\nclose(robot,obj): close the given object.\nfold(robot,obj): fold the given object.\nunfold(robot,obj): unfold the given object.\ntoggle_on(robot,obj): toggle on the given object.\ntoggle_off(robot,obj): toggle off the given object.\nAt each round of conversation, I will give you\nObserved Objects: ...\nObserved Relations: ...\nInventory: ...\nTask Goal: ...\nOriginal Subtasks: ...\n18\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nPrevious Action Code: ...\nExecution Error: ...\nI will give you the following information for you to make a\n,\u2192 one-step action decision toward the final goal.\n(1) Observed Objects: contains object names, its editable states\n,\u2192 with the corresponding value of the states and distance\n,\u2192 measuring the centroid of Agent towards the object. It\n,\u2192 denotes with (object, [(state1, value1), (state2, value2)],\n,\u2192 distance).e.g. (fridge, [(\u2019openable\u2019, 1)], 1.8) means the\n,\u2192 object fridge can be opened, and it is currently openedand\n,\u2192 and the distance is a float value measured in meters.\n(2) Observed Relations: a scene relation graph triplet denotes\n,\u2192 with (object, relation, object), e.g. (apple, ontop, desk).\n,\u2192 You are termed with Agent in this context.\n(3) You should pay attention to the relation graph which is\n,\u2192 essential for you to understand the status of the\n,\u2192 environment.\n(3) The observation may not include all the information about the\n,\u2192 objects you need to interact with, the objects may be\n,\u2192 hidden inside other objects, so feel free to explore the\n,\u2192 reasonable place they might appear.\n(4) The Inventory contains a stack-like structure, you could put\n,\u2192 things inside. But remember first in last out. It contains\n,\u2192 all the things the robot has in its hand. If nothing is in\n,\u2192 Inventory, denoted with None.\n(5) The Task Goal contains instructions and the Agent finished\n,\u2192 state for the entire goal.\n(6) Original Subtasks: The sub-tasks that is planned in the\n,\u2192 conversation. Note that the original plans could be\n,\u2192 problematic and unable to solve the problem, so you might\n,\u2192 need to make revision and set up a new plan if necessary.\n(7) Previous Actions: The action code for solving the previous\n,\u2192 subtasks would be provided so that you can understand what\n,\u2192 was going on and extend the code with the action code for\n,\u2192 solving the next subtask. Pay attention to the number used\n,\u2192 in camera functions in previous code, make sure the number\n,\u2192 is continuous.\n(8) Execution Error: The execution error for last round will be\n,\u2192 provided to help you in this round.\nYou should then respond to me with\nExplain (if applicable): Are there any steps missing in your\n,\u2192 plan? Why does the code not complete the task? What does\n,\u2192 the chat log and execution error imply?\nSubtasks: How to complete the Task Goal step by step by calling\n,\u2192 given action functions. You should plan a list of subtasks\n,\u2192 to complete your ultimate goal. You need to make the\n,\u2192 planning consistent to your previous round unless those\n,\u2192 need to change. You should pay attention to the Inventory\n,\u2192 since it tells what you have. The task completeness check\n,\u2192 is also based on your final inventory. Pay attention that\n,\u2192 you can only interact with the objects within two meters of\n,\u2192 you, so you need to be close enough to interact with the\n,\u2192 objects.\nCode:\n19\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\n(1) Remember you can only interact with the objects within two\n,\u2192 meters of you.\n(2) Only use functions given in Utility Functions, Action List.\n,\u2192 Write a function taking the \u2019robot\u2019, \u2019env\u2019 and \u2019camera\u2019 as\n,\u2192 the only three arguments.\n(3) Reuse the above useful programs as much as possible.\n(4) Your function will be reused for building more complex\n,\u2192 functions. Therefore, you should make it generic and\n,\u2192 reusable. You should not make strong assumptions about the\n,\u2192 inventory (as it may be changed at a later time), and\n,\u2192 therefore you should always check whether you have the\n,\u2192 required items before using them. If not, you should first\n,\u2192 collect the required items and reuse the above useful\n,\u2192 programs.\n(5) The function name should always be \u2019act\u2019, but you need to\n,\u2192 explain what task it completes.\n(6) Each time you take an action in the provided action list,\n,\u2192 after you take the action, you have to use the function\n,\u2192 \u2019donothing\u2019 before you take another action in the action\n,\u2192 list. So the block should look like \"One action in the\n,\u2192 action list + donothing\". Remember one action in your plan\n,\u2192 may contain multiple actions in the action list, you have\n,\u2192 to use the block for each action in the action list.\n(7) Registry every object you might need to use first.\n(8) You should only output the action code to finish your very\n,\u2192 next subtask. Remember not to generate the entire action\n,\u2192 code unless it is the final step.\n(9) You can have more than one things in Inventory.\nAlso please notice that registration should not be considered as\n,\u2192 one subtask. Make sure that your subtask planning should\n,\u2192 start with real actions like \"open the door\" while keeping\n,\u2192 the object registry as the default action.\nTarget States: A state to check the completeness of the subtask.\n,\u2192 You should generate the state for self-verifying if the\n,\u2192 code can successfully run and reach a desired state in the\n,\u2192 simulator environment to finish the subtask. The state\n,\u2192 should be in the format\n(1) Inventory (describe what you could have in Inventory in this\n,\u2192 state): object\n(2) Object Information (describe the object information in this\n,\u2192 environment): format1: object, state, value or format2:\n,\u2192 object1, state, object2, value. The value can only be 0 or\n,\u2192 1, representing False or True of the state. For example,\n,\u2192 [fridge_1234, openable, 1] means fridge_1234 is opened;\n,\u2192 [meat_jhg, inside, fridge_1234, 1] means meat_jhg is inside\n,\u2192 fridge_1234. For format1, you can only choose the state\n,\u2192 from: [\u2019cookable\u2019, \u2019burnable\u2019, \u2019freezable\u2019, \u2019heatable\u2019,\n,\u2192 \u2019openable\u2019, \u2019togglable\u2019, \u2019foldable\u2019, \u2019unfoldable\u2019]. For\n,\u2192 format2, you can choose the state from: [\u2019inside\u2019,\n,\u2192 \u2019nextto\u2019, \u2019ontop\u2019, \u2019under\u2019, \u2019touching\u2019, \u2019covered\u2019,\n,\u2192 \u2019contains\u2019, \u2019saturated\u2019, \u2019filled\u2019, \u2019attached\u2019, \u2019overlaid\u2019,\n,\u2192 \u2019draped\u2019]. If the object is the robot, denote it with\n,\u2192 \u2019robot\u2019.\n(3) If the object has not been changed in this conversation, do\n,\u2192 not add it into the target states.\n(4) You don\u2019t need to write any annotations for target states.\n20\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\n(5) Remember to make sure the states you use is in the provided\n,\u2192 state list for format1 and format2.\n(5) You can only use the objects provided in the Object\n,\u2192 Information part, you cannot use the name you registered in\n,\u2192 the code.\n(6) The object information of target states should be the last\n,\u2192 part of your response, no more explanations are needed.\n## Format Requirement\nYou should only respond in the format described below. Please\n,\u2192 strictly pay attention to the format of the bullet points,\n,\u2192 especially the brackets for the number (e.g., \"(1), (2),\n,\u2192 and (3)\").\n{response_format}\nNow, I will send the message so that you can make planning\naccordingly.\nExplain:\n...\nSubtasks:\n(1) ...\n(2) ...\n(3) ...\n// Please provide me with ALL previous subtasks (E.g if subtask1\n,\u2192 & 2 are successfully acted and make mistakes in subtask3,\n,\u2192 please return me with subtask1 & 2 and new plan of subtask3)\n...\nCode:\n\u2018\u2018\u2018python\n// import neccessary modules first\n// helper functions (only if needed, try to avoid them)\n...\n// main function after the helper functions\ndef act(robot,env,camera) {\n// comment should be clear and correspond to subtasks above\n,\u2192 (e.g., Subtask 1: xxxx)\n//only generate one subtask in each act function\n}\n\u2018\u2018\u2018\nTarget States:\n(1) Inventory: ... //robot can grasp more than one thing at the\n,\u2192 same time\n(2) Object Information:\n(a) ... // e.g. (a) table, nextto, fridge, 1\n(b) ...\n(c) ...\nB.2\nENVIRONMENTAL MESSAGE\nObserved Objects: (bacon_150, ([\u2019heatable\u2019, 0], [\u2019freezable\u2019,\n,\u2192 0], [\u2019burnable\u2019, 0], [\u2019cookable\u2019, 0]),\n,\u2192 1.89)(fridge_xyejdx_0, ([\u2019openable\u2019, 0], [\u2019heatable\u2019,\n,\u2192 0], [\u2019freezable\u2019, 0]), 2.12)(stove_rgpphy_0,\n,\u2192 ([\u2019togglable\u2019, 0], [\u2019heatable\u2019, 0], [\u2019freezable\u2019, 0]),\n,\u2192 1.59)(tray_156, ([\u2019heatable\u2019, 0], [\u2019freezable\u2019, 0]),\n,\u2192 1.85)(griddle_157, ([\u2019togglable\u2019, 0], [\u2019heatable\u2019, 0],\n,\u2192 [\u2019freezable\u2019, 0]), 1.68)\n21\nOctopus: Embodied Vision-Language Programmer From Environmental Feedback\nObserved Relations: [(\u2019bacon_150\u2019, \u2019inside\u2019, \u2019tray_156\u2019),\n,\u2192 (\u2019tray_156\u2019, \u2019inside\u2019, \u2019fridge_xyejdx_0\u2019), (\u2019bacon_150\u2019,\n,\u2192 \u2019inside\u2019, \u2019fridge_xyejdx_0\u2019)]\nInventory: None\nTask Goal: cook_bacon\nOriginal Subtasks: None\nPrevious Action Code: No code\nExecution error: No error\nNow, please output Explain, Subtasks (revise if necessary),\n,\u2192 Code that completing the next subtask, and Target\n,\u2192 States, according to the instruction above. Remember you\n,\u2192 can only use the functions provided above and pay\n,\u2192 attention to the response format.\nB.3\nGPT-4 RESPONSE EXAMPLE\nExplain:\nSince our task goal is to cook bacon and the bacon is inside the\n,\u2192 fridge on a tray, the first step should be to approach the\n,\u2192 fridge, open it, and take out the bacon. Once the bacon is\n,\u2192 retrieved, we can proceed to cook it on a suitable\n,\u2192 appliance like the griddle or stove.\nSubtasks:\n(1) Approach the fridge.\n(2) Open the fridge.\n(3) Retrieve the tray containing the bacon from the fridge.\n(4) Approach the or stove.\n(5) Place the bacon on the stove.\n(6) Toggle on the stove to cook the bacon.\nCode:\ndef act(robot, env, camera):\n# Subtask 1: Approach the fridge\nfridge_xyejdx_0 = registry(env, \"fridge_xyejdx_0\")\nMoveBot(env, robot, fridge_xyejdx_0, camera)\ndonothing(env)\nTarget States:\n(1) Inventory: None\n(2) Object Information:\n(a) robot, nextto, fridge_xyejdx_0, 1\n22\n"
  },
  {
    "title": "Lemur: Harmonizing Natural Language and Code for Language Agents",
    "link": "https://arxiv.org/pdf/2310.06830.pdf",
    "upvote": "27",
    "text": "Preprint\nLEMUR: HARMONIZING NATURAL LANGUAGE AND\nCODE FOR LANGUAGE AGENTS\nYiheng Xu\u2217\u2660\u2662 Hongjin Su\u2217\u2660\u2662 Chen Xing\u2217\u2663 Boyu Mi\u2660\u2662 Qian Liu\u2661\u2662 Weijia Shi\u25b3\nBinyuan Hui\u2662 Fan Zhou\u2660\u2662 Yitao Liu\u2660\u2662 Tianbao Xie\u2660\u2662 Zhoujun Cheng\u2660\u2662 Siheng Zhao\u2660\u2662\nLingpeng Kong\u2660 Bailin Wang\u2605 Caiming Xiong\u2663 Tao Yu\u2660\u2662\n\u2660University of Hong Kong \u2662XLang Lab \u2663Salesforce Research\n\u2661Sea AI Lab\n\u25b3University of Washington \u2605MIT CSAIL\n\u2660{yhxu,hjsu,tyu}@cs.hku.hk \u2663{cxing, cxiong}@salesforce.com\nCode: https://github.com/OpenLemur/Lemur\nModels: https://huggingface.co/OpenLemur\nABSTRACT\nWe introduce Lemur and Lemur-Chat, openly accessible language models opti-\nmized for both natural language and coding capabilities to serve as the backbone\nof versatile language agents. The evolution from language chat models to func-\ntional language agents demands that models not only master human interaction,\nreasoning, and planning but also ensure grounding in the relevant environments.\nThis calls for a harmonious blend of language and coding capabilities in the mod-\nels. Lemur and Lemur-Chat are proposed to address this necessity, demonstrat-\ning balanced proficiencies in both domains, unlike existing open-source mod-\nels that tend to specialize in either.\nThrough meticulous pre-training using a\ncode-intensive corpus and instruction fine-tuning on text and code data, our mod-\nels achieve state-of-the-art averaged performance across diverse text and coding\nbenchmarks among open-source models. Comprehensive experiments demon-\nstrate Lemur\u2019s superiority over existing open-source models and its proficiency\nacross various agent tasks involving human communication, tool usage, and in-\nteraction under fully- and partially- observable environments. The harmonization\nbetween natural and programming languages enables Lemur-Chat to significantly\nnarrow the gap with proprietary models on agent abilities, providing key insights\ninto developing advanced open-source agents adept at reasoning, planning, and\noperating seamlessly across environments.\n10/11/23, 1:26 AM\nradar (1).svg\n\ufb01le:///Users/ranpox/Downloads/radar (1).svg\n1/1\nMMLU\nBBH\nGSM8K\nHumanEval\nMBPP\nSpider\nMultiPL-E\nDS-1000\n65.3\n65.3\n61.9\n61.9\n66.3\n66.3\n61\n61\n55.5\n55.5\n62.5\n62.5\n42.9\n42.9\n34.5\n34.5\nLemur-70B-chat (ours)\nLlama-2-70B-chat\nCodeLlama-34B-Instruct\n(a) Foundational Language and Code Abilities\n10/11/23, 1:25 AM\nradar.svg\n\ufb01le:///Users/ranpox/Downloads/radar.svg\n1/1\nTool Augmentation\nSelf-debugging\nFollowing  \nNL Feedback\non Reasoning\nFollowing NL Feedback\non Coding\nExploring Digital \n   Environment\nExploring Physical\n     Environment\n31.65\n31.65\n49.64\n49.64\n35.76\n35.76\n44.86\n44.86\n13.65\n13.65\n59.7\n59.7\n36.71\n36.71\n53.23\n53.23\n50.32\n50.32\n38.97\n38.97\n9.19\n9.19\n41.79\n41.79\nLemur-70B-chat (ours)\nLlama-2-70B-chat\nCodeLlama-34B-Instruct\ngpt-3.5-turbo\n(b) Interactive Agent Abilities\nFigure 1: Comparison of different foundational and interactive agent capabilities between Lemur-\nChat and other open-source pre-trained LLMs.\n\u2217Equal contribution. Blog version: https://xlang.ai/blog/openlemur\n1\narXiv:2310.06830v1  [cs.CL]  10 Oct 2023\nPreprint\n1\nINTRODUCTION\nIntelligent agents are broadly conceptualized as autonomous problem solvers with the abilities to\nsense their environment, decide, and act upon that enviorment (Sutton & Barto, 2005; Russell, 2010;\nWilkins, 2014). Recent implementations of this concept in creating language agents (Yao et al.,\n2022b; Gravitas, 2023; Wang et al., 2023a) capable of utilizing natural language for varied and\nintricate tasks in diverse environments have demonstrated potential, particularly when built upon\nlarge language models (LLMs) (Brown et al., 2020; Chen et al., 2021; Chowdhery et al., 2022;\nOpenAI, 2023; Touvron et al., 2023). Such agents harness the human knowledge in LLMs and\ncan think and communicate in human terms. This equips them to employ varied tools, operate in\ncomplex environments, engage in language reasoning, and create spontaneous multi-agent systems.\nTo effectively form the foundation of language agents, LLMs should not only master human inter-\naction, reasoning, and planning but also ensure grounding in the relevant environments (Wei et al.,\n2022b; Huang et al., 2022a; Ichter et al., 2022). Human interaction, reasoning, and planning can be\nlargely realized through the natural language capabilities of LLMs. On the other hand, the grounded\nexecution in the environment is usually achieved by using general-purpose code or domain-specific\nAPIs, such as controlling web browsers (Shi et al., 2017; Yao et al., 2022a; Deng et al., 2023; Zhou\net al., 2023), interacting with OS CLI terminals (Yang et al., 2023), and manipulating robotic arms\nvia primitive APIs (Ichter et al., 2022; Huang et al., 2022b; Liang et al., 2023). Therefore, we posit\nthat for the construction of language agents, it is imperative for language models to possess harmo-\nnized capabilities in both natural language and programming languages. This balance ensures that\nmodels do not specialize exclusively in certain areas but can seamlessly integrate with environment\ncontexts and generate controllable and valid actions. Presently, closed-source models like GPT-\n4 demonstrate such capabilities, which empower them to function as language agents. However,\ncurrent open-source LLMs such as Llama 2 (Touvron et al., 2023) and CodeLlama (Rozi`ere et al.,\n2023) have traditionally been tailored for either textual or code-related tasks, with limited ability to\neffectively balance both.\nTo address this need, we introduce Lemur and Lemur-Chat, cutting-edge, openly accessible mod-\nels meticulously pre-trained and fine-tuned to harmonize text and code capabilities. We enhanced\nthe base Llama-2-70B through thoughtfully designed pre-training and instruction fine-tuning stages.\nSpecifically, we built a code-centric corpus based on The Stack (Kocetkov et al., 2022), compris-\ning 90 billion tokens with a 10:1 text-to-code ratio, ensuring improved capabilities in coding ability\nwhile maintaining performance in natural language ability. We refer to this model as Lemur. After\npretraining, we conducted instruction fine-tuning using about 300K examples from both text and\ncode to build an instruction-following model, which we refer to as Lemur-Chat. Thorough assess-\nments across 8 textual and coding benchmarks validate the superior performance of both Lemur\nand Lemur-Chat in multiple text and code evaluations, establishing them as the most well-rounded\nopen-source models.\nMoreover, this work embarks on assessing the vital capabilities of language agents across various\nscenarios, which we refer to as agent benchmarks. We place a particular emphasis on their tool-\nusage abilities, and abilities in grounding in the environment feedback and human feedback. We\nalso explore the challenges posed by real and partially observable environments where the agent\nhas to take actions based on limited knowledge and take additional actions to gather more informa-\ntion. Experimental results indicate that Lemur-Chat outperforms other open-sourced models in 12\nof the 13 agent benchmarks. This underscores how the integration of natural and coding capabil-\nities allows Lemur-Chat to exceed the current open-source models for language agents, markedly\nbridging the performance disparity between open-source and commercial alternatives. Our experi-\nments show that in agent scenarios, there is a need for synergy between natural language and cod-\ning abilities. For models with strong natural language capabilities but weak coding abilities like\nLlama-2-70B-Chat, they can effectively use simple tools to assist reasoning (\u00a74.2) because\nthe action space is small and the difficulty of using tools is low. However, when facing complex\ndecision-making scenarios such as web browsing and house navigation, the action space is usually\nlarge, and models with strong coding abilities have an advantage in generating complex executable\naction sequences (\u00a74.5). Overall, Lemur has both strong natural language and coding abilities,\nenabling it to achieve better performance in both scenarios. This research provides insights into\noptimizing the synergy between natural and programming languages, laying a solid foundation for\n2\nPreprint\ndeveloping advanced language agents capable of operating efficiently in various environments. Our\ncontributions can be summarized as follows:\n\u2022 Harmonized Language Models: We introduce Lemur, state-of-the-art open-source mod-\nels that are meticulously pre-trained and instruction fine-tuned to demonstrate balanced\nlanguage and coding capabilities.\n\u2022 Comprehensive Evaluations: We follow and extend extensive benchmarks in both classi-\ncal and agent perspectives, demonstrate the advantage Lemur over other open-source mod-\nels, and showcase its potential as a versatile language agent.\n\u2022 Insights on Synergy: Our research delves deep into the interplay between natural and\nprogramming languages, offering valuable insights that pave the way for the development\nof more advanced language agents adept at operating across a range of environments.\n2\nPRE-TRAINING AND INSTRUCTION TUNING OF LEMUR\nThis section will introduce the method to build the Lemur and Lemur-Chat models and their per-\nformance on commonly-used benchmarks for pre-trained language model evaluation. To build a\nmore balanced model, the pipeline includes two stages: pre-training (\u00a7 2.1) and instruction fine-\ntuning (\u00a7 2.2). The training pipeline is shown in Figure 2.\nLlama-2\nLemur\nLemur-Chat\nCode-Centric\nPre-training\n\u2022\nBase Model\n\u2022\nFocus on Natural \nLanguage\n\u2022\nBase Model\n\u2022\nNatural Language \n& Code\n\u2022\nChat Model\n\u2022\nLanguage Agent\nSupervised\nFine-tuning\n+ 90B Tokens\n+ 300K Examples\nFigure 2: Overview of Training Procedure. We continually pre-train Llama-2-70B model on\n90B code-intensive data and fine-tune it with 300K examples of instructions to enable the model to\nharmonize natural language and coding abilities.\n2.1\nPRE-TRAINING\nIn the pre-training stage, we choose the Llama-2-70B base model as the starting point, which is\nthe cutting-edge open-sourced base model in most scenarios except the coding scenario. Our goal\nis to improve the coding ability while maintaining the reasoning ability of Llama-2. To this end,\nwe built a corpus with a code-to-text ratio of 10:1. For the code part, we base it on The Stack (Ko-\ncetkov et al., 2022), a collection of source codes from GitHub with permissive licenses. Among all\nlanguages, we focus on scripting or interpreted languages (Python, SQL, Bash, Perl, etc.) because\nagent models are often executed in interactive scenarios. As for the text aspect, we use Refined-\nWeb (Penedo et al., 2023), Redpajama (Computer, 2023), as well as CommonCrawl, Wikipedia,\nBooks, ArXiv, StackExchange and DM Mathematics (Saxton et al., 2019) to build the textual data\ncorpus. Following previous works (Gao et al., 2021; Smith et al., 2022; Computer, 2023; Li et al.,\n2023), we do extensive deduplication after aggregating all data sources. The composition of the data\nis shown in Appendix \u00a7 A.1.1. We train the Lemur-70B model initialized with Llama-2-70B\nusing a TPUv4-512 pod. We train the model with sequence packing (Raffel et al., 2020; Chung\net al., 2022) to improve the training efficiency. Please refer to Appendix \u00a7 A.1.2 for more details.\n2.2\nINSTRUCTION FINE-TUNING\nDuring the instruction fine-tuning phase, we include four data sources to construct Lemur-Chat, in-\ncluding the Open Assistant crowdsourced annotated assistant-style dialogue corpus (K\u00a8opf et al.,\n2023), Orca data with chain of thought reasoning for human-written tasks (Lian et al., 2023;\nMukherjee et al., 2023), ShareGPT & Chatlogs containing real user and ChatGPT chat history\nrecords (ShareGPT data, 2023), as well as Evol-CodeAlpaca data (Luo et al., 2023) consisting of\ncomplex coding tasks generated by ChatGPT along with their corresponding solutions. The statis-\ntics of these instruction datasets are shown in Table 1. After we collect these data, we additionally\n3\nPreprint\nTable 1: Instruction datasets investigated in this work. We report the average number of rounds\n( \u00afNrounds), average length of prompts (\u00afLprompt), average length of completion (\u00afLcompletion).\nDatasets\nQuery Source\nResponse Source # Instances\n\u00afNrounds \u00afLprompt \u00afLcompletion\nOpen Assistant 1\nHuman-written\nHuman-written\n34,546\n1.6\n34.9\n213.1\nOpenOrca\nHuman-written\nGPT-4\n200,000\n1.0\n242.8\n174.1\nShareGPT & ChatLogs User prompts\nGPT-3.5/GPT-4\n81,319\n6.0\n96.7\n340.2\nEvol-CodeAlpaca\nGPT-3.5/GPT-4\nGPT-3.5/GPT-4\n51,952\n1.0\n93.9\n343.5\nclean and deduplicate these instruction fine-tuning data. We conduct training on these data for 2\nepochs. Please refer to \u00a7 A.2 for more details.\n3\nFROM LANGUAGE MODEL TO LANGUAGE AGENT\nThis section introduces how we measure the language and coding abilities of a language model\nto guide the process of coordinating language and code. We further discuss the new challenges\nfaced when connecting LLM to the environment and describe how we examine the necessary agent\ncapabilities.\n3.1\nFUNDAMENTAL LANGAUGE AND CODING CAPABILITIES\nPrevious work typically uses a variety of benchmarks to comprehensively reflect the performance of\nmodels on different types of tasks. Therefore, we evaluate the performance of various models across\ntext and code benchmarks as follows.\n\u2022 Natural Language benchmarks:\nMMLU (Hendrycks et al., 2021a) to determine factuality,\nBBH (Suzgun et al., 2022) to check reasoning abilities, GSM8K (Cobbe et al., 2021) to gauge\nmath reasoning.\n\u2022 Code benchmarks: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) to test Python\nwriting abilities, Spider (Yu et al., 2018) to assess database query capabilities, MultiPL-E (Cassano\net al., 2023a) to measure the multi-lingual coding capabilities, DS-1000 (Lai et al., 2023) for\nevaluation in data science scenarios\n3.2\nCONNECTING LLM AGENTS TO ENVIRONMENT\nWhile the measurements in \u00a73.1 provide valuable insights on each domain, they may not fully\nencapsulate the models\u2019 capabilities in language agent scenarios due to their focus on single-turn\ninteractions in fully observable settings. In order to address this discrepancy, we scrutinize models\nin the context of multi-turn interactive scenarios to gauge their adaptability in practical situations.\nOur assessment is centered on various capabilities of language agents, which are encapsulated in the\nfactors outlined in Figure 3 and Table 2. For a more comprehensive assessment of these capabilities,\nwe reorganize existing multiple datasets into four sets to examine the diverse skills demanded by the\nagent scenario. Please refer to Appendix B.2 for details of each dataset.\nAugment with Tools\nSelf-Debug\nFollow Feedback\nExplore Environment\nUse\nrun\nmessages\nfeedback\nexplore\nobserve\nEmploy various tools \nto augment agents\u2019 \ncapabilities\nUtilize error messages \nfrom the environment to \nrectify existing errors\nUnderstand complex feedback \nfrom human / agents\nand convert them into symbolic \nexecutable sequences\nOperate in \nenvironments that are \npartially observable\nfeedback\nFigure 3: We inspect language agents in various aspects, including the abilities to augment with\ntools, self-debug, follow feedback, and explore partially observable environments.\n4\nPreprint\nTable 2: Multi-turn agent evaluation under different settings. We evaluate crucial capabilities for\nLLM as an agent, spanning tool usage, feedback adherence and environment exploration.\nCapabilities\nEnvironments\nObservability Datasets\nAugment with Tools (\u00a74.2)\nPython Calculator\nFully\nM-{GSM8K, MATH, TheoremQA}\nWikiSearch\nM-{HotpotQA, MMLU}\nSelf-debug with\nEnvironment Feedback (\u00a74.3)\nPython\nFully\nM-{HumanEval, MBPP}\nOS Terminal\nInterCode-Bash\nDatabase\nInterCode-SQL\nRobotics API\nRoboCodeGen\nFollow Natural Language\nFeedback (\u00a74.4)\nUser/Agent\nFully\nM-Reasoning w/ GPT-4 feedback\nUser/Agent\nM-Code w/ GPT-4 feedback\nExplore in Partially\nObservable Environment (\u00a74.5)\nOS Terminal\nPartially\nInterCode-CTF\nWeb Browser\nWebArena\nHousehold Simulator\nM-ALFWorld\nAugment with Tools Tool usage (Schick et al., 2023; Hao et al., 2023; Mialon et al., 2023) is an\nimportant capability of language agents. For example, tasks like precise calculations or information\nretrieval can be offloaded to external modules (such as Python interpreters or search engines) using\ntools (Gao et al., 2023; Chen et al., 2022), thereby improving the reliability and interpretability of\nresults. Generally speaking, practical tools involve calling operators in symbolic languages or di-\nrectly invoking APIs using programming languages (Shen et al., 2023). This requires the model to\ndecompose a complex task and ground its intention to external tools, obtaining the returned results\ninto context for further actions based on them. This process relies on both natural language reason-\ning abilities and the use of programming languages (Cheng et al., 2022; Sur\u00b4\u0131s et al., 2023). To assess\nthe ability of language agents to solve complex multi-turn problems using tools, we introduce the\npart of the MINT dataset (Wang et al., 2023b) which focuses on tool-utilization for reasoning. This\npart includes several adapted datasets for testing: MINT-{GSM8K, MATH} (Cobbe et al., 2021;\nHendrycks et al., 2021b) is used to test the model\u2019s ability to solve mathematical problems using a\nPython interpreter, and MINT-{HotpotQA, MMLU} (Yang et al., 2018; Hendrycks et al., 2021a) as-\nsesses the model\u2019s capability to solve knowledge-based questions using Wikipedia searches. At the\nsame time, for the MINT-TheoremQA (Chen et al., 2023a), the model needs to perform knowledge\nsearches on Wikipedia and use a Python interpreter to calculate and draw conclusions.\nSelf-debug with Environment Feedback Self-debug is an important way to test whether a model\ncan incorporate environmental feedback through multi-turn interactions (Jignasu et al., 2023; Olaus-\nson et al., 2023; Chen et al., 2023b). In the self-debug scenario, the model usually needs to complete\na complex operation sequence, such as Python functions, database queries/modifications, robot ac-\ntion sequences, etc (Gur et al., 2023; Yao et al., 2023). These complex operations sometimes cannot\nbe executed successfully and will return an error message. This requires the model to comprehend\nthis kind of environmental feedback and engage in interactive error correction until it is correct,\nwhich tests the joint effect of natural language reasoning ability and programming languages. We\nuse rich datasets from multiple benchmark tests to evaluate this performance, including multi-turn\nMINT-MBPP and MINT-HumanEval in MINT benchmark tests (Wang et al., 2023b), SQL and\nBash in InterCode (Yang et al., 2023), as well as RoboCodeGen that calls robot APIs through code\nin Robotics scenarios (Liang et al., 2023). These environments require the model to complete a\ncomplex task and will provide error messages in case of execution errors. In these environments, the\nperformance of the model will vary according to its self-debugging ability, reflecting the model\u2019s\nability to incorporate feedback. According to the original settings, we evaluate MINT-HumanEval,\nMBPP in a 5-round setting and Intercode-SQL, Bash in a 10-round setting. We evaluate RobotCode-\nGen in a 5-round setting.\nFollow Natural Language Feedback Following natural language feedback is an important mech-\nanism for agents to receive information from humans or other agents (Wang et al., 2022a; Ouyang\net al., 2022; Gong et al., 2023). In scenarios where complex problems are solved through multi-turn\ninteractions, the model not only needs to incorporate environmental feedback but also feedback from\nhumans or other agents in order to improve. This mechanism requires the model to understand new\n5\nPreprint\ninstructions based on a context that combines natural language and code, and ground them into new\naction sequences. To evaluate the model\u2019s ability to accept natural language feedback, we follow\nthe approach of the MINT benchmarks: using a GPT-4 simulated user as a teacher to guide the\nmodel in problem-solving. This setup includes a series of MINT datasets (Wang et al., 2023b) to\ncomprehensively evaluate performance after adding natural language feedback in various scenarios.\nExplore in Partially Observable Environments Exploring partially observable environments is a\nunique and challenging factor in agent scenarios. All the settings mentioned earlier can be con-\nsidered as fully observable environments, which means that agents can observe all the information\nof the current environment to plan, reason, and make decisions. However, in partially observable\nenvironments (also known as Partially Observable Markov Decision Process) (Kurniawati, 2021),\nagents can only partially observe the environmental information required to solve problems. This\nrequires agents to collect information through exploration and continue making decisions. This\nprocess places high demands on various abilities of agents, such as natural language planning and\nreasoning, environmental interaction, etc., and is very close to real-world scenarios. To measure\nthis ability, we use three datasets: InterCode-CTF (Yang et al., 2023) and WebArena (Zhou et al.,\n2023) in digital environments, as well as ALFWorld (Shridhar et al., 2021) in physical environments.\nInterCode-CTF provides a complete OS terminal for models to solve practical Catch the Flag (CTF)\nproblems where agents need multiple rounds of exploration to obtain the flag. WebArena evalu-\nates agents\u2019 ability to control browsers for task completion through exploration. ALFWorld is a\nsimulated home environment where agents need to explore navigation and complete specific tasks.\n4\nEXPERIMENTAL RESULTS\n4.1\nLANGUAGE AND CODE CAPABILITIES\nThe comprehensive evaluations of text and code benchmarks in Table 3 demonstrate the impressive\ncapabilities of Lemur-70B and Lemur-70B-Chat models. Deviating from Llama-2-70B and\nLlama-2-70B-Chat, which are mostly pre-trained and fine-tuned on text data, the Lemur mod-\nels augment coding abilities and thereby enhance the overall performance by 4.3% and 14.8% re-\nspectively. Alternatively, models like CodeLlama-34B and CodeLlama-34B-INST, which are\nprimarily trained on code datasets, exhibit solid performance in code benchmarks. Importantly, the\ncommendable increase in the overall performance of Lemur-70B and Lemur-70B-Chat models\nas compared to the CodeLlama models, by approximately 1.9% and 9.4% respectively, highlights\nthe virtue of harmonizing textual and coding skills.\nThe synergic text and code abilities enable them to function as language agents. However, a disparity\nexists between natural language and coding capabilities in current open-source models. Such limita-\ntions obstruct these models\u2019 abilities to act as language agents, leading to performance degradation\nin agent benchmarks. In subsequent sections, we meticulously evaluate various critical capabilities\nof agents, revealing the importance of synergic text and code abilities to language models.\n4.2\nAUGMENT WITH TOOLS\nIn the realm of problem-solving, agents, akin to humans, employ various tools to augment their ca-\npabilities. This is exemplified by (Chen et al., 2022), who showcase that the mathematical reasoning\nprowess of LLM can be significantly enhanced with the aid of Python. As per the data presented\nin Table 4, it is evident that Lemur-70B-Chat outperforms both Llama-2-70B-Chat and\nCodeLlama-34B-INST, indicating its superior ability to effectively leverage tools.\n4.3\nSELF-DEBUG WITH ENVIRONMENT FEEDBACK\nThe technique of self-debug gains considerable traction in the realm of code generation (Olausson\net al., 2023; Zhang et al., 2023). This method consists of models using feedback information, such as\ninterpreter error tracebacks and database observations, to rectify any existing errors. This adaptive\ncapacity is essential for agents because they have to constantly receive and react to feedback from\nthe environment during their interaction process.\n6\nPreprint\nTable 3: Performance comparison across diverse models on text and code benchmarks. MCode is\nan abbreviation for Multilingual Code. HE stands for HumanEval. Avg denotes the average perfor-\nmance across all benchmarks. Lemur-70B and Lemur-70B-Chat exhibit balanced capabilities,\nachieving the highest overall performance when averaged by task.\nText\nCode\nModel\nQA\nReason\nMath\nPython\nSQL\nMCode\nDS\nAvg\nMMLU\nBBH\nGSM8K HE MBPP Spider MultiPL-E DS-1000\nStarCoder-15B\n30.8\n33.2\n8.9\n33.6\n52.7\n58.3\n25.3\n26.0\n33.6\nStarCoderPlus-15B\n42.0\n36.2\n17.7\n26.2\n37.0\n48.8\n21.4\n19.4\n31.1\nCodeLlama-34B\n52.8\n42.2\n32.7\n48.8\n55.0\n68.4\n36.4\n31.8\n46.0\nLlama-2-70B\n68.9\n51.2\n56.8\n30.5\n45.4\n60.0\n24.4\n11.3\n43.6\nLemur-70B\n64.5\n51.6\n54.9\n35.4\n53.2\n62.8\n30.4\n30.7\n47.9\nWizardCoder-15B\n29.4\n28.8\n7.1\n57.3\n51.6\n61.6\n30.8\n29.2\n37.0\nCodeLlama-34B-INST\n53.5\n37.1\n41.0\n41.5\n57.0\n66.6\n36.1\n32.3\n45.6\nLlama-2-70B-Chat\n63.9\n38.9\n48.7\n31.1\n38.2\n60.3\n22.6\n17.8\n40.2\nLemur-70B-Chat\n65.3\n61.9\n66.3\n61.0\n55.5\n62.5\n42.9\n34.5\n55.0\nTable 4: The tool-augmented reasoning tasks evaluate the model\u2019s capabilities to use tools in the\nreasoning process. Across five tasks with Python and WikiSearch API as tools, Lemur-70B-Chat\noutperforms both Llama-2-70B-Chat and CodeLlama-34B-INST by large margins.\nModel\nMath Reasoning\nwith Python\nQuestion Answering\nwith WikiSearch API\nMicroAvg\nM-GSM8K M-MATH M-TheoremQA M-HotpotQA\nM-MMLU\nLlama-2-70B-Chat\n33.33\n3.00\n2.04\n27.91\n42.11\n20.25\nCodeLlama-34B-INST\n25.00\n4.00\n2.04\n16.28\n30.26\n14.87\nLemur-70B-Chat\n58.33\n6.00\n8.16\n44.19\n56.58\n31.65\ngpt-3.5-turbo\n43.75\n26.00\n28.57\n27.91\n56.58\n36.71\ngpt-4\n93.75\n57.00\n57.14\n46.51\n80.26\n66.77\n1\n2\n3\n4\n5 6 7 8 910\n0\n20\n40\n60\n80\nTurns (Log Scale)\nPerformance(%)\ngpt-3.5-turbo\nlemur-70b-chat\nllama-2-70b-chat\ncodellama-34b-inst\ntext-bison-001\nFigure 4: Success Rate with interaction\nturns across models in IC-SQL.\nAs demonstrated in Table 5,\nthe performance\nof\nthe\nLemur-70B-Chat\nsignificantly\nsur-\npasses that of the Llama-2-70B-Chat and\nCodeLlama-34B-INST in interactive coding\nbenchmarks.\nThis underscores the importance of\nhaving balanced capabilities for interactive agents\nin such environments.\nWe further analyze the results with InterCode-SQL\nto understand how models incorporate environment\nfeedback. In this setting, agents are provided with\ndatabase schema and instruction guidelines for SQL\ngame interactions. Acting as agents, the models are\ntasked with querying databases and responding to\nquestions in a multi-turn interaction environment.\nFigure 4 shows the Growth in Success Rate across\nmodels with each interaction turn. Lemur demonstrates robust performance in the first round, show-\ning an initial performance that is comparable to that of text-bison-001 and slightly lower\nthan gpt-3.5-turbo. Nevertheless, Lemur improves consistently across ten interactive rounds,\nsurpassing the performance of gpt-3.5-turbo eventually. In contrast, text-bison-001,\nwhich exhibits comparable initial performance with Lemur, does not show significant improvement.\nLlama-2-70B-Chat, while displaying consistent adaptability to feedback throughout the pro-\ncess, has a significant gap in initial performance due to inferior coding ability, hence its success\nrate remains relatively lower. CodeLlama-34B-INST hardly answers the questions correctly in\nthe first round. We observe that this is because it blindly follows the advice in the game guide\n7\nPreprint\nTable 5: Model performance with environment feedback.\nLemur-70B-Chat demonstrates\nstrong capabilities to comprehend environment feedback, achieving performance on par with\ngpt-3.5-turbo. *Due to overfitting to [PYTHON] tag, CodeLlama-34B-INST fails to fol-\nlow feedback and generated parsable results (Wang et al., 2023b).\nDebug Environment\nPython Interpreter\nBash Terminal Database\nRobotics\nDataset\nM-HumanEval M-MBPP\nIC-Bash\nIC-SQL RoboCodeGen\nLlama-2-70B-Chat\n8.89\n8.79\n31.50\n67.89\n48.65\nCodeLlama-34B-INST\n2.22\u2217\n2.20\u2217\n36.00\n67.79\n64.86\nLemur-70B-Chat\n46.67\n17.58\n34.50\n73.79\n75.68\ngpt-3.5-turbo\n37.78\n25.27\n46.51\n72.82\n83.78\ngpt-4\n73.33\n52.75\n48.52\n84.41\n83.78\nTable 7:\nPerformance comparison of different models\nin partially observable environments InterCode-CTF, We-\nbArena and ALFWorld.\nModel\nDigital Env.\nPhysical Env.\nIC-CTF WebAreana\nALFWorld\nLlama-2-70B-Chat\n9.00\n1.72\n21.64\nCodeLlama-34B-INST\n16.00\n4.06\n37.31\nLemur-70B-Chat\n22.00\n5.30\n59.70\ngpt-3.5-turbo\n11.00\n7.38\n41.79\ngpt-4\n37.00\n10.59\n84.33\nLlama CodeLlama Lemur\n2\n4\n6\n1.72\n4.06\n5.3\n2.09\n4.68\n5.67\nScore\nSymbolic\nProgramming\nFigure 5: Performance Compari-\nson of symbolic and programming\nrepresentation for WebArena.\nand stubbornly executes the show tables command first, instead of trying to understand the\ndatabase structure that had already been provided. After an improvement in the second round, its\nperformance returns to normal. However, the growth remained limited even after ten rounds of inter-\naction, settling at par with Llama-2-70B-Chat and reflecting its relative weakness in adapting\nto environmental feedback.\n4.4\nFOLLOW NATURAL LANGUAGE FEEDBACK\nFollowing natural language feedback from users or agents in the environment is an important ability\nfor language agents. It requires language agents to understand complex natural language instruc-\ntions and convert the instructions into symbolic executable sequences based on the current contexts.\nTo evaluate the models\u2019 ability to follow natural language feedback, we follow the evaluation set-\ntings of MINT (Wang et al., 2023b), which measures language agents\u2019 ability to leverage natural\nlanguage feedback using performance improvement. To provide natural language feedback in multi-\nturn settings, MINT uses GPT-4 to simulate a user providing helpful feedback on the solutions from\nevaluated language agents.\nWe evaluate models on MINT-Reasoning and MINT-Code with and without GPT-4 feedback and the\nexperimental results are in Table 6. MINT-Reasoning includes five modified benchmarks: GSM8k,\nMATH, TheoremQA, HotpotQA, and MMLU. MINT-Coding includes modified HumanEval and\nMBPP. We find that all models can benefit from GPT-4, which means powerful GPT-4 as a teacher\ncan provide helpful feedback even without ground truth. We also calculate \u2206 feedback, which in-\ndicates the absolute improvement thanks to GPT-4 feedback. According to the results from Ta-\nble 6, Lemur-70B-Chat model obtain 8.19 in \u2206feedback, which is significantly better than\nLlama-2-70B-Chat and CodeLlama-34B-INST.\n8\nPreprint\nTable 6: Comparative analysis of various models\u2019 Successful Rate (SR) on tasks related to Reason-\ning and Code Generation, with and without GPT-4 feedback. The table presents the performance\nmetrics of each model under two conditions: \u2018no feedback\u2019 and \u2018with GPT-4 feedback\u2019. The Micro\nAvg. column represents the average performance of the models across the tasks.\nModel\nFeedback\nReasoning Code Gen. Micro Avg. \u2206feedback\nLlama-2-70B-Chat\nno feedback\n20.25\n8.82\n16.81\n5.31\nw/ GPT-4 feedback\n23.10\n19.85\n22.12\nCodeLlama-34B-INST\nno feedback\n14.87\n2.21\n11.06\n4.20\nw/ GPT-4 feedback\n20.25\n3.68\n15.27\nLemur-70B-Chat\nno feedback\n31.65\n27.21\n30.31\n8.19\nw/ GPT-4 feedback\n35.76\n44.86\n38.50\ngpt-3.5-turbo\nno feedback\n36.71\n29.41\n34.51\n12.39\nw/ GPT-4 feedback\n50.32\n38.97\n46.90\ngpt-4 (upper-bound)\nno feedback\n66.77\n59.56\n69.11\nN/A\n4.5\nEXPLORE IN PARTIALLY OBSERVABLE ENVIRONMENTS\nWe evaluate language agents in varied environments and find that Lemur-70B-Chat exhibits\nbalanced and commendable performance across all tested tasks. It scored 22.00 in the complex\nInterCode-CTF, showcasing its proficiency in multifaceted cybersecurity skills and strategic adapt-\nability. In WebArena, it achieved a score of 5.79, reflecting its adeptness in interpreting and execut-\ning advanced natural language commands in intricate web environments. In ALFWorld, it demon-\nstrated superior planning and commonsense reasoning with a score of 59.70, successfully navigating\nand performing in simulated physical environments. While gpt-4 overall exhibits higher scores,\nLemur-70B-Chat\u2019s consistent performance across diverse and partially observable environments\nunderscores its versatility and potential in handling real-world, multifarious applications.\nWe also conduct experiments to explore different output formats for actions in the WebArena en-\nvironment. Instead of merely prompting the language model to directly produce predefined actions\n(e.g. type [id] [content] [press enter after]), we deterministically mapp the action space to Python\nrepresentations (e.g. type(id:int, content:str, press enter after:bool)). We then prompt the language\nmodel to predict this representation and then parse the model\u2019s Python prediction back into the pre-\ndefined executable actions. Our findings, as shown in Figure 5 indicate that mapping to a Python\nrepresentation leads to better performance than directly predicting actions. Such results suggest that\ncarefully and rationally selecting intermmediate representation, based on the pre-training corpus,\ncan effectively enhance the model\u2019s performance as a language agent, aligning with prior findings\n(Hu et al., 2022).\n5\nRELATED WORK\nTransfer Learning on Code The advancement in expansive language models (Devlin et al., 2019;\nRadford et al., 2019; Raffel et al., 2020; Brown et al., 2020) has catalyzed progress in transfer learn-\ning for code-related tasks, further enriched by the continuous pre-training paradigm (Gururangan\net al., 2020). Hernandez et al. (2021) offered insights into the interplay between model size, train-\ning data, and performance in transferring language ability to code tasks. Several models have been\nintroduced, exhibiting enhanced program synthesis and infilling/completion performance, by under-\ngoing continual pre-training on extensive code data (Feng et al., 2020; Wang et al., 2021; Chen et al.,\n2021; Rozi`ere et al., 2023). However, their intense focus on code often results in a compromise on\nnatural language capabilities. Lemur addresses this by moderately augmenting a large model with a\nbalanced mixture of code and natural language data, maintaining proficiency in both domains.\nInstruction Fine-tuning The process of aligning large language models to follow instructions, often\nreferred to as instruction tuning, has been primarily directed towards NLP tasks (Wei et al., 2022a;\nWang et al., 2022b). Recent studies have sought to broaden the use cases of instruction tuning\n9\nPreprint\nto involve a wider variety of general tasks (Ouyang et al., 2022). Self-instruct method generates\ninstructions using seed instructions, aiding the understanding of how to adapt language models by\nfine-tuning them on instructions garnered from ChatGPT (Wang et al., 2022a; Zheng et al., 2023;\nXu et al., 2023b; Mukherjee et al., 2023). In this study, we adopt a similar approach in tuning the\nLemur base model to follow instructions.\nLanguage Agents Language agents are adept at following user instructions and engaging with envi-\nronments to execute tasks. Recent trends in research and open-source communities have employed\nLarge Language Models (LLMs) (Brown et al., 2020; Chen et al., 2021; Chowdhery et al., 2022;\nOpenAI, 2023) as the principal controllers for these agents (Yao et al., 2022b; Chase, 2022; Grav-\nitas, 2023; Shinn et al., 2023; Wang et al., 2023a; Xu et al., 2023a; Lin et al., 2023; Yao et al.,\n2023). This is driven by the LLMs\u2019 demonstrated abilities in reasoning, planning, grounding, and\ncode generation (Wei et al., 2022b; Huang et al., 2022a; Ichter et al., 2022; Xie et al., 2023), crucial\nfor comprehending user instructions, grasping the environmental context, and generating executable\nactions. Lemur seamlessly integrates capabilities in both text and code, enabling the generation of\nenvironment-grounded, executable actions essential for constructing language agents.\nAgent Evaluation Benchmarks The rapid evolution of language agents requires an accurate and\ncomprehensive assessment of agents. Recent works pose new dimensions that put language agents\nin web environments (Deng et al., 2023; Yao et al., 2022a; Zhou et al., 2023), interactive code\nenvironments (Yang et al., 2023), digital game (Fan et al., 2022), and household (Puig et al., 2018;\nShridhar et al., 2020; 2021) to finish certain task under natural language instruction. Apart from\ncollecting new tasks, these agent benchmarks can be established by re-purposing and transforming\nexisting datasets to agent datasets (Wang et al., 2023b; Yang et al., 2023; Liu et al., 2023). Our\nresearch combines these tasks, conducts extensive evaluation, and evaluates the model\u2019s capability\nto construct a language agent from multiple dimensions.\n6\nCONCLUSION\nIn conclusion, this research underscores the pivotal role of harmonizing natural and program-\nming language proficiencies in the evolution of language models to sophisticated language agents.\nThrough the development of Lemur and Lemur-Chat, we demonstrated that the meticulous amalga-\nmation of these competencies allows for elevated performance in diverse environments and appli-\ncations, narrowing the existent capability divide between open-source and proprietary models. We\nopen-sourced both models with the intention of fostering further research in the field of language\nmodels for agents.\n10\nPreprint\n7\nACKNOWLEDGMENTS\nThis project is an open research collaboration between XLang Lab of HKU and Salesforce Research.\nWe extend our heartfelt gratitude for the generous research grants received from both Google Re-\nsearch and Amazon AWS. Our special thanks go to the Google TPU Research Cloud program for\ntheir provision of vital computational resources. Our research greatly benefitted from insightful\ndiscussions with esteemed collaborators: Pengcheng Yin, Yaqing Wang, and You Wu from Google\nResearch; Peng Shi and Shuaichen Chang from AWS AI; Yizhong Wang from the University of\nWashington; Xinyang Geng from UC Berkeley; Ansong Ni from Yale University; Chen Henry Wu\nfrom Carnegie Mellon University; Zeyu Liu from the University of Texas at Austin; Lin Zheng and\nJiacheng Ye from the University of Hong Kong.\nREFERENCES\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels. ArXiv preprint, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.\n07732.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot\nlearners.\nIn Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,\nand Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald\nPinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha,\nMichael Greenberg, and Abhinav Jangda. Multipl-e: A scalable and polyglot approach to bench-\nmarking neural code generation. IEEE Transactions on Software Engineering, 49(7):3675\u20133691,\n2023a. doi: 10.1109/TSE.2023.3267446.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald\nPinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha,\nMichael Greenberg, and Abhinav Jangda.\nMultiPL-E: A Scalable and Polyglot Approach to\nBenchmarking Neural Code Generation. IEEE Transactions on Software Engineering, 2023b.\ndoi: 10.1109/TSE.2023.3267446.\nHarrison Chase.\nLangChain, 2022.\nURL https://github.com/langchain-ai/\nlangchain.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code.\nArXiv preprint, abs/2107.03374, 2021.\nURL https://\narxiv.org/abs/2107.03374.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt-\ning: Disentangling computation from reasoning for numerical reasoning tasks. ArXiv preprint,\nabs/2211.12588, 2022. URL https://arxiv.org/abs/2211.12588.\nWenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi\nWang, and Pan Lu. Theoremqa: A theorem-driven question answering dataset. ArXiv preprint,\nabs/2305.12524, 2023a. URL https://arxiv.org/abs/2305.12524.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00a8arli, and Denny Zhou. Teaching large language models\nto self-debug. ArXiv preprint, abs/2304.05128, 2023b. URL https://arxiv.org/abs/\n2304.05128.\n11\nPreprint\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu.\nBind-\ning language models in symbolic languages.\nArXiv preprint, abs/2210.02875, 2022.\nURL\nhttps://arxiv.org/abs/2210.02875.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways.\nArXiv preprint, abs/2204.02311, 2022.\nURL\nhttps://arxiv.org/abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,\nJacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling Instruction-\nFinetuned Language Models, 2022. URL https://arxiv.org/abs/2210.11416.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. ArXiv preprint, abs/2110.14168, 2021. URL https://arxiv.\norg/abs/2110.14168.\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURL https://github.com/togethercomputer/RedPajama-Data.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and\nYu Su. Mind2web: Towards a generalist agent for the web, 2023.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Min-\nnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL\nhttps://aclanthology.org/N19-1423.\nevol-codealpaca-v1.\nevol-codealpaca-v1.\nhttps://huggingface.co/datasets/\ntheblackcat102/evol-codealpaca-v1, 2023.\nEvol-Instruct-Code-80k-v1.\nEvol-instruct-code-80k-v1.\nhttps://huggingface.co/\ndatasets/nickrosh/Evol-Instruct-Code-80k-v1, 2023.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:\n18343\u201318362, 2022.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,\nBing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained model for pro-\ngramming and natural languages. In Findings of the Association for Computational Linguis-\ntics: EMNLP 2020, pp. 1536\u20131547, Online, 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.findings-emnlp.139. URL https://aclanthology.org/2020.\nfindings-emnlp.139.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile:\nAn 800GB Dataset of Diverse Text for Language Modeling, 2021. URL https://arxiv.\norg/abs/2101.00027.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning, pp. 10764\u201310799. PMLR, 2023.\n12\nPreprint\nXinyang Geng. Easylm: A simple and scalable training framework for large language models, 2023.\nURL https://github.com/young-geng/EasyLM.\nRan Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-\nChun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction.\nArXiv preprint, abs/2309.09971, 2023. URL https://arxiv.org/abs/2309.09971.\nSignificant\nGravitas.\nAuto-gpt,\n2023.\nURL\nhttps://github.com/\nSignificant-Gravitas/Auto-GPT.\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and\nAleksandra Faust. A real-world webagent with planning, long context understanding, and pro-\ngram synthesis. ArXiv preprint, abs/2307.12856, 2023. URL https://arxiv.org/abs/\n2307.12856.\nSuchin Gururangan, Ana Marasovi\u00b4c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. Don\u2019t stop pretraining: Adapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.\n8342\u20138360, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nacl-main.740. URL https://aclanthology.org/2020.acl-main.740.\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. ToolkenGPT: Augmenting Frozen Language\nModels with Massive Tools via Tool Embeddings, 2023. URL https://arxiv.org/abs/\n2305.11554.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In 9th International Confer-\nence on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-\nview.net, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,\nDawn Song,\nand Jacob Steinhardt.\nMeasuring mathematical problem solving with\nthe MATH dataset.\nIn Joaquin Vanschoren and Sai-Kit Yeung (eds.),\nProceedings\nof\nthe\nNeural\nInformation\nProcessing\nSystems\nTrack\non\nDatasets\nand\nBenchmarks\n1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b.\nURL\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper/2021/\nhash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.\nDanny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling Laws for Transfer,\n2021. URL https://arxiv.org/abs/2102.01293.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, and Mari Ostendorf. In-context\nlearning for few-shot dialogue state tracking.\nIn Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pp. 2627\u20132643, Abu Dhabi, United Arab Emirates, 2022.\nAssociation for Computational Linguistics. URL https://aclanthology.org/2022.\nfindings-emnlp.193.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-\nshot planners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri,\nStefanie Jegelka, Le Song, Csaba Szepesv\u00b4ari, Gang Niu, and Sivan Sabato (eds.), International\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 of Proceedings of Machine Learning Research, pp. 9118\u20139147. PMLR, 2022a. URL\nhttps://proceedings.mlr.press/v162/huang22a.html.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda\nLuu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner Monologue: Embodied Reasoning\nthrough Planning with Language Models, 2022b. URL https://arxiv.org/abs/2207.\n05608.\nBrian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog,\nDaniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine,\n13\nPreprint\nYao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke,\nFei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas\nSievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter\nPastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey,\nRosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and\nChuyuan Kelly Fu. Do as I can, not as I say: Grounding language in robotic affordances. In Karen\nLiu, Dana Kulic, and Jeffrey Ichnowski (eds.), Conference on Robot Learning, CoRL 2022, 14-\n18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning\nResearch, pp. 287\u2013318. PMLR, 2022. URL https://proceedings.mlr.press/v205/\nichter23a.html.\nAnushrut Jignasu, Kelly Marshall, Baskar Ganapathysubramanian, Aditya Balu, Chinmay Hegde,\nand Adarsh Krishnamurthy.\nTowards foundational ai models for additive manufacturing:\nLanguage models for g-code debugging, manipulation, and comprehension.\nArXiv preprint,\nabs/2309.02465, 2023. URL https://arxiv.org/abs/2309.02465.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u02dcnoz Ferrandis,\nYacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von\nWerra, and Harm de Vries. The Stack: 3 TB of permissively licensed source code, 2022. URL\nhttps://arxiv.org/abs/2211.15533.\nAndreas K\u00a8opf, Yannic Kilcher, Dimitri von R\u00a8utte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00b4ard Nagyfi, Shahul ES, Sameer Suri,\nDavid Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and\nAlexander Mattick. OpenAssistant Conversations \u2013 Democratizing Large Language Model Align-\nment, 2023. URL https://arxiv.org/abs/2304.07327.\nHanna Kurniawati. Partially observable markov decision processes (pomdps) and robotics. ArXiv\npreprint, abs/2107.07599, 2021. URL https://arxiv.org/abs/2107.07599.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau\nYih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data\nscience code generation. In International Conference on Machine Learning, pp. 18319\u201318345.\nPMLR, 2023.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao\nMou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii,\nTerry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u02dcao\nMonteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Lo-\ngesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra\nMurthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,\nZhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luc-\ncioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor,\nJennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex\nGu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva\nReddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u02dcnoz Ferrandis, Sean Hughes,\nThomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder: May the source\nbe with you!, 2023. URL https://arxiv.org/abs/2305.06161.\nWing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \u201dTeknium\u201d.\nOpenOrca: An open dataset of GPT augmented FLAN reasoning traces, 2023.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and\nAndy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 9493\u20139500. IEEE, 2023.\nBill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang,\nChandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and\nslow thinking for complex interactive tasks. ArXiv preprint, abs/2305.17390, 2023. URL https:\n//arxiv.org/abs/2305.17390.\n14\nPreprint\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,\nKaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui\nZhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.\nAgentBench: Evaluating LLMs as Agents, 2023. URL https://arxiv.org/abs/2308.\n03688.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing\nMa, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with\nevol-instruct, 2023.\nGr\u00b4egoire Mialon, Roberto Dess`\u0131, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta\nRaileanu, Baptiste Rozi`ere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave,\nYann LeCun, and Thomas Scialom.\nAugmented Language Models: A Survey, 2023.\nURL\nhttps://arxiv.org/abs/2302.07842.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and\nAhmed Awadallah. Orca: Progressive Learning from Complex Explanation Traces of GPT-4,\n2023. URL https://arxiv.org/abs/2306.02707.\nTheo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-\nLezama.\nDemystifying gpt self-repair for code generation.\nArXiv preprint, abs/2306.09896,\n2023. URL https://arxiv.org/abs/2306.09896.\nOpenAI. GPT-4 Technical Report, 2023. URL https://arxiv.org/abs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb\nDataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only,\n2023. URL https://arxiv.org/abs/2306.01116.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio\nTorralba.\nVirtualhome: Simulating household activities via programs.\nIn 2018 IEEE Con-\nference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018, pp. 8494\u20138502. IEEE Computer Society, 2018.\ndoi:\n10.1109/CVPR.\n2018.00886. URL http://openaccess.thecvf.com/content_cvpr_2018/html/\nPuig_VirtualHome_Simulating_Household_CVPR_2018_paper.html.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nModels are Unsupervised Multitask Learners, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-\ntext transformer. J. Mach. Learn. Res., 21:140:1\u2013140:67, 2020. URL http://jmlr.org/\npapers/v21/20-074.html.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00b4er\u00b4emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,\nManish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00b4efossez,\nJade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and\nGabriel Synnaeve. Code Llama: Open Foundation Models for Code, 2023. URL https://\narxiv.org/abs/2308.12950.\nStuart J Russell. Artificial intelligence a modern approach. Pearson Education, Inc., 2010.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli.\nAnalysing mathematical\nreasoning abilities of neural models. In 7th International Conference on Learning Represen-\ntations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\nURL\nhttps://openreview.net/forum?id=H1gR5iR5FX.\n15\nPreprint\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. ArXiv preprint, abs/2302.04761, 2023. URL https://arxiv.org/abs/2302.\n04761.\nShareGPT\ndata.\nShareGPT\ndata.\nhttps://huggingface.co/datasets/\nanon8231489123/ShareGPT_Vicuna_unfiltered, 2023.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in hugging face, 2023.\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits:\nAn open-domain platform for web-based agents. In Doina Precup and Yee Whye Teh (eds.),\nProceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research,\npp. 3135\u20133144. PMLR, 2017. URL http://proceedings.mlr.press/v70/shi17a.\nhtml.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu\nYao.\nReflexion:\nLanguage agents with verbal reinforcement learning.\nArXiv preprint,\nabs/2303.11366, 2023. URL https://arxiv.org/abs/2303.11366.\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mot-\ntaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A benchmark for interpreting grounded\ninstructions for everyday tasks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 10737\u201310746. IEEE, 2020.\ndoi: 10.1109/CVPR42600.2020.01075. URL https://doi.org/10.1109/CVPR42600.\n2020.01075.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u02c6ot\u00b4e, Yonatan Bisk, Adam Trischler, and Matthew J.\nHausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In\n9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net, 2021.\nURL https://openreview.net/forum?id=\n0IOX0YcCdTn.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He,\nMichael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using DeepSpeed and Megatron to\nTrain Megatron-Turing NLG 530B, A Large-Scale Generative Language Model, 2022.\nURL\nhttps://arxiv.org/abs/2201.11990.\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih,\nNoah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned\ntext embeddings, 2023.\nD\u00b4\u0131dac Sur\u00b4\u0131s, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for\nreasoning. ArXiv preprint, abs/2303.08128, 2023. URL https://arxiv.org/abs/2303.\n08128.\nRichard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. IEEE Trans-\nactions on Neural Networks, 16:285\u2013286, 2005. URL https://api.semanticscholar.\norg/CorpusID:9166388.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00a8arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks\nand whether chain-of-thought can solve them.\nArXiv preprint, abs/2210.09261, 2022.\nURL\nhttps://arxiv.org/abs/2210.09261.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\n16\nPreprint\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models,\n2023. URL https://arxiv.org/abs/2307.09288.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\nArXiv preprint, abs/2305.16291, 2023a. URL https://arxiv.org/abs/2305.16291.\nXingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint:\nEvaluating llms in multi-turn interaction with tools and language feedback.\nArXiv preprint,\nabs/2309.10691, 2023b. URL https://arxiv.org/abs/2309.10691.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\nArXiv preprint, abs/2212.10560, 2022a. URL https://arxiv.org/abs/2212.10560.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAtharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby\nKuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mi-\nrali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang\nKaria, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Pa-\ntro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative\ninstructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 5085\u20135109, Abu Dhabi, United Arab Emirates, 2022b.\nAssociation for Computational Linguistics. URL https://aclanthology.org/2022.\nemnlp-main.340.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi\nChandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\nHow Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources, 2023c.\nURL https://arxiv.org/abs/2306.04751.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi.\nCodeT5: Identifier-aware unified\npre-trained encoder-decoder models for code understanding and generation. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8696\u20138708,\nOnline and Punta Cana, Dominican Republic, 2021. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2021.emnlp-main.685. URL https://aclanthology.org/2021.\nemnlp-main.685.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In\nThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net, 2022a.\nURL https://openreview.net/forum?\nid=gEZrGCozdqR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022b.\nDavid E Wilkins. Practical planning: extending the classical AI planning paradigm. Elsevier, 2014.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface\u2019s trans-\nformers: State-of-the-art natural language processing. ArXiv preprint, abs/1910.03771, 2019.\nURL https://arxiv.org/abs/1910.03771.\n17\nPreprint\nTianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and\nTao Yu. Text2reward: Automated dense reward function generation for reinforcement learning.\nArXiv preprint, abs/2309.11489, 2023. URL https://arxiv.org/abs/2309.11489.\nBinfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu.\nRewoo: Decoupling reasoning from observations for efficient augmented language models. ArXiv\npreprint, abs/2305.18323, 2023a. URL https://arxiv.org/abs/2305.18323.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. WizardLM: Empowering Large Language Models to Follow Complex Instructions, 2023b.\nURL https://arxiv.org/abs/2304.12244.\nJohn Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing\nand benchmarking interactive coding with execution feedback. ArXiv preprint, abs/2306.14898,\n2023. URL https://arxiv.org/abs/2306.14898.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\nanswering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, pp. 2369\u20132380, Brussels, Belgium, 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.\nWebshop: Towards scalable\nreal-world web interaction with grounded language agents. Advances in Neural Information Pro-\ncessing Systems, 35:20744\u201320757, 2022a.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. ArXiv preprint, abs/2210.03629,\n2022b. URL https://arxiv.org/abs/2210.03629.\nWeiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh\nMurthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large\nlanguage agents with policy gradient optimization. ArXiv preprint, abs/2308.02151, 2023. URL\nhttps://arxiv.org/abs/2308.02151.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene\nLi, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-scale\nhuman-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.\n3911\u20133921, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/\nv1/D18-1425. URL https://aclanthology.org/D18-1425.\nKechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. Self-edit: Fault-aware code editor for code\ngeneration. ArXiv preprint, abs/2305.04087, 2023. URL https://arxiv.org/abs/2305.\n04087.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging LLM-as-a-judge with MT-Bench and Chatbot Arena, 2023. URL https://arxiv.\norg/abs/2306.05685.\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,\nYonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building\nautonomous agents. ArXiv preprint, abs/2307.13854, 2023. URL https://arxiv.org/\nabs/2307.13854.\n18\nPreprint\nA\nLEMUR AND LEMUR-CHAT\nA.1\nPRE-TRAINING\nA.1.1\nPRE-TRAINING DATA\nWe performed pre-training on the top of Llama-2. The detailed statistics of the pre-training data\ncorpus are presented below in Table 8.\nTable 8: Distribution of data by Type and Source, as well as the sampling weights. Epochs are the\nnumber of passes over each constituent dataset during a full epoch over the data split.\nType\nWeights (%)\nSource\nWeights (%)\nEffective Tokens (B)\nEpoch\nCode\n90.9\nPython\n72.73\n65.46\n2.98\nSQL\n5.15\n4.63\n0.69\nJava\n1.82\n1.64\n0.06\nShell\n1.82\n1.64\n1.26\nNotebook\n1.71\n1.54\n0.82\nJavaScript\n1.69\n1.52\n0.06\nC\n1.61\n1.44\n0.06\nPHP\n1.33\n1.19\n0.06\nCPP\n1.21\n1.09\n0.06\nOthers\n1.73\n1.55\n0.06\nText\n9.1\nRefinedWeb\n6.82\n6.14\n\u2013\nWikipedia\n0.64\n0.57\n\u2013\nBooks\n0.41\n0.37\n\u2013\nArXiv\n0.41\n0.37\n\u2013\nStackExchange\n0.41\n0.37\n\u2013\nDM Mathematics\n0.41\n0.37\n\u2013\nTotal\n100\n100.00\n90.00\nA.1.2\nTRAINING DETAILS\nWe train our model on a TPUv4-512 pod. Our codebase is based on Jax and EasyLM (Geng, 2023).\nFollowing the pretraining methodology of Llama 2 (Touvron et al., 2023), we used a batch size of\n4M tokens. To improve training efficiency, we packed multiple shorter sequences into each batch\nentry when possible, an approach known as sequential packing (Raffel et al., 2020).\nOptimization was performed with Adam using a peak learning rate of 4e-5 along with \u03b21 = 0.9 and\n\u03b22 = 0.95. Gradients were clipped at 1.0 to prevent exploding gradients. A cosine decay schedule\nwas used for the learning rate, with a linear warmup of 2000 steps followed by decaying the learning\nrate to 10.% of its peak value at the end of training.\nA.2\nINSTRUCTION FINE-TUNING\nA.2.1\nINSTRUCTION FINE-TUNING DATA\nOpenORCA\nThe OpenOrca dataset (Lian et al., 2023) is a collection of augmented FLAN Col-\nlection data. Currently, it includes around 1M GPT-4 and 3.2M GPT-3.5 Chat completions. It\nis tabularized in alignment with the distributions presented in the ORCA paper (Mukherjee et al.,\n2023). We randomly sampled 200K samples from the GPT-4 Chat completions in Table 1\nOpenAssistant 1\nOpenAssistant(OASST1) (K\u00a8opf et al., 2023) is a crowdsourced human-\nannotated assistant-style conversation corpus comprising 161,443 messages, enriched with 461,292\nquality ratings. (K\u00a8opf et al., 2023). The data results from the collaboration of over 13,500 volunteers\nworldwide.\n19\nPreprint\nShareGPT and Chatlogs\nWe curated English human instructions from ShareGPT(https:\n//sharegpt.com/)\nand\nChatlogs(https://chatlogs.net/)\nfor\ninstruction-tuning.\nSpecifically,\nwe use datasets from Huggingface dataset hub https://huggingface.\nco/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered\nand\nhttps://\nhuggingface.co/datasets/winglian/chatlogs-en-cleaned. To filter English in-\nstructions, we utilized the langdetect package in Python and eliminated any instructions with\nnon-English detection results. Additionally, considering that non-English instructions often contain\nconsecutive non-English characters, we implemented a second filtering step, removing all sentences\nwith three or more consecutive non-English characters. To ensure semantic diversity, we employed\ninstructor(Su et al., 2023) to encode the filtered instructions, calculate cosine similarity, and remove\ninstructions with a similarity score greater than 0.95. After deduplication, we obtained nearly 80K\ninstances, with an average of about 6 rounds of high-quality data per instance.\nEvol-CodeAlpaca\nWe use two open-sourced Evolution-Instruct (Luo et al., 2023) datasets, i.e.,\nEvol-Instruct-Code-80k-v1 (2023) and evol-codealpaca-v1 (2023), and an execution-verified Python\ndataset constructed by us. After applying the same deduplication method as the Text data, we ob-\ntained \u223c 46K examples.\nA.2.2\nINSTRUCTION FINE-TUNING DETAILS\nWe use Huggingface Transformers (Wolf et al., 2019) with Accelerate Library to fine-tune the Lemur\nmodel to obtain the Lemur-Chat model. We train on our data collection for two epochs. We use\nAdam optimizer with a learning rate of 2e-5 and a batch size of 128.\nB\nEVALUATION\nB.1\nFOUNDATIONAL ABILITIES EVALUATION\nFollowing Wang et al. (2023c); Rozi`ere et al. (2023), we evaluated the results outlined in Table 3 uti-\nlizing greedy decoding for generation. The evaluation scripts were crafted based on the framework\navailable at https://github.com/allenai/open-instruct. For a robust evaluation of\nchat/instruct models on code function completion tasks such as HumanEval, MBPP, MultiPL-\nE, and DS-1000, we furnished the incomplete code context as prompts to ensure reliable function\ncompletions, aligning with the practices of Wang et al. (2023c).\n\u2022 MMLU: The MMLU (Massive Multitask Language Understanding) (Hendrycks et al.,\n2021a) is structured to gauge the knowledge garnered during pretraining, by evaluating\nmodels in zero-shot and few-shot settings across 57 diverse tasks from fields like elemen-\ntary mathematics, US history, computer science, law, and clinical knowledge. Adhering to\nMMLU\u2019s original setup, the evaluation was performed using 5-shot examples.\n\u2022 GSM8K: Originated by Cobbe et al. (2021), GSM8K encompasses 8.5K linguistically\nvaried grade school math word problems, targeting the evaluation of question answering\non basic math problems necessitating multi-step reasoning. The evaluation was executed\non GSM8K\u2019s test set, employing 8-shot in-context examples within a chain-of-thought\nprompting method (Wei et al., 2022b).\n\u2022 BBH: The BBH (BIG-Bench Hard) (Suzgun et al., 2022) is centered on tasks believed to\nchallenge the existing language models, particularly those necessitating multi-step reason-\ning. Following the guidelines laid out in the original paper, the evaluation was carried out\nwith 3-shot in-context examples, albeit without employing chain-of-thought prompting due\nto computational resource constraints.\n\u2022 HumanEval: Introduced by Chen et al. (2021), HumanEval comprises 164 meticulously\ncrafted programming challenges, accompanied by unit tests to ascertain the feasibility of\nthe solutions proposed, aiming to challenge code generation models. The evaluation on\nHumanEval dataset from Chen et al. (2021) was conducted employing official zero-shot\ndocstring prompts.\n\u2022 MBPP: The MBPP (Mostly Basic Python Problems) dataset (Austin et al., 2021), encom-\npassing approximately 1,000 crowd-sourced Python programming problems, is directed\n20\nPreprint\ntowards entry-level programmers, covering fundamental programming concepts and stan-\ndard library functionalities. The evaluation on MBPP dataset from Austin et al. (2021) was\nexecuted using official 3-shot prompts.\n\u2022 Spider: Designed for intricate and cross-domain semantic parsing alongside text-to-SQL\ntasks, Spider (Yu et al., 2018) aims to foster the development of natural language interfaces\nfor cross-domain databases. It houses 10,181 questions and 5,693 unique complex SQL\nqueries spanning 200 databases across 138 distinct domains.\n\u2022 MultiPL-E: MultiPL-E (Cassano et al., 2023b) emerges as a multi-programming language\nbenchmark for appraising the code generation capability of large language models (LLMs).\nIt extends the HumanEval dataset to include 18 additional programming languages, and\noffers a system for transposing unit test-driven code generation benchmarks to new lan-\nguages, thus standing as a noteworthy multilingual code generation benchmark. The eval-\nuation was undertaken using the official zero-shot prompt.\n\u2022 DS-1000: DS-1000 (Lai et al., 2023) is a code generation benchmark with a thousand data\nscience questions spanning seven Python libraries such as NumPy and Pandas. The dataset\nreflects diverse, realistic, and practical use cases, providing a reliable metric for evaluation\nwhile defending against memorization by perturbing questions. The problems in the dataset\nare collected from StackOverflow, making it a practical and realistic benchmark for data\nscience code generation tasks. We evaluate with the official zero-shot prompt.\nB.2\nAGENT EVALUATION\nWe construct our agent abilities evaluation suite based on the following datasets:\n\u2022 MINT (Wang et al., 2023b) is a well-rounded evaluation that covers a range of tasks re-\npurposed for multi-turn evaluation. It consists of three types of tasks, namely reasoning\n(MMLU (Hendrycks et al., 2021a), GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al.,\n2021a), TheoremQA (Chen et al., 2023a), HotpotQA (Yang et al., 2018)), code generation\n(HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021)), and decision-making (ALF-\nWorld (Shridhar et al., 2021)). To assess the proficiency of language models in employing\ntools, their reasoning process is reoriented to incorporate tool use. For instance, language\nmodels are prompted to utilize the Python calculator to work out mathematical problems,\nrather than supplying the answer outright. In code generation, the LLM is encouraged to\nincorporate Python interpreter messages to check generated code. To prevent any misun-\nderstanding, we use the prefix \u201cM-\u201d to differentiate the original dataset from the MINT\nversion in our paper.\n\u2022 InterCode-Bash/SQL (Yang et al., 2023) are two tasks that serve as an experimental plat-\nform that assesses the capacity of extensive language models to integrate feedback from\nthe environment during interactive coding. This benchmark evaluates models in the way of\ngenerating a series of actions under user instruction and regards elements such as execution\noutcomes, and error backtracking, amongst others as environment observations.\n\u2022 RoboCodeGen (Liang et al., 2023) serves as a specialized evaluation framework focused\non robotics-related tasks. It comprises three main types of questions that target spatial\nreasoning (e.g., identifying the closest point among a set of points), geometric reasoning\n(e.g., verifying if one bounding box is contained within another), and controls (e.g., PD\ncontrol).\n\u2022 InterCode-CTF is a task in the InterCode evaluation suite. Capture the Flag (CTF) is a\ncompetitive cybersecurity game that requires LLMs to discover encrypted \u201cflag\u201d hidden\nwithin code snippets or file systems. Compared with Bash and SQL generation, CTF is\nmuch more complex, requiring agents to have knowledge of multiple coding languages,\nmodularize higher-order objectives into sub-problems, create multi-step plans for solving\neach problem, and adjust strategies when a plan fails to provide any useful insights.\n\u2022 WebArena (Zhou et al., 2023) creates self-hostable websites of four popular categories\nby simulating real-world equivalent functionalities and data. To simulate human problem-\nsolving abilities, WebArena also embeds tools and knowledge resources as standalone web-\nsites.\n21\nPreprint\n\u2022 ALFWorld (Shridhar et al., 2021) is a synthetic environment benchmark adapted from\nAlfred (Shridhar et al., 2020) in the text-based interface where agents need to navigate in\nsimulated households (e.g., go to coffee table 1, pick up paper 2, use desk lamp 1) and\nachieve high-level goals (e.g., check the paper under the desk lamp). Task instances may\ninvolve over 50 locations and 50 steps to solve, thus challenging the agent\u2019s ability to plan,\ntrack sub-goals, and conduct systematic exploration.\n22\n"
  },
  {
    "title": "Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation",
    "link": "https://arxiv.org/pdf/2310.08541.pdf",
    "upvote": "17",
    "text": "Idea2Img: Iterative Self-Refinement with GPT-4V(ision)\nfor Automatic Image Design and Generation\nZhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin,\nZicheng Liu, Lijuan Wang\nMicrosoft Azure AI\n{zhengyang,jianfw,lindsey.li,keli,chungching.lin,zliu,lijuanw}@microsoft.com\nhttps://idea2img.github.io/\nAbstract\nWe introduce \u201cIdea to Image,\u201d1 a system that enables\nmultimodal iterative self-refinement with GPT-4V(ision) for\nautomatic image design and generation.\nHumans can\nquickly identify the characteristics of different text-to-image\n(T2I) models via iterative explorations. This enables them\nto efficiently convert their high-level generation ideas into\neffective T2I prompts that can produce good images. We\ninvestigate if systems based on large multimodal models\n(LMMs) can develop analogous multimodal self-refinement\nabilities that enable exploring unknown models or environ-\nments via self-refining tries. Idea2Img cyclically generates\nrevised T2I prompts to synthesize draft images, and provides\ndirectional feedback for prompt revision, both conditioned\non its memory of the probed T2I model\u2019s characteristics.\nThe iterative self-refinement brings Idea2Img various ad-\nvantages over vanilla T2I models. Notably, Idea2Img can\nprocess input ideas with interleaved image-text sequences,\nfollow ideas with design instructions, and generate images\nof better semantic and visual qualities. The user prefer-\nence study validates the efficacy of multimodal iterative self-\nrefinement on automatic image design and generation.\n1. Introduction\n\u201cImage design and generation\u201d aims to create an image\nfrom a high-level user idea. This input IDEA can contain\ninterleaved reference images, such as \u201cthe dog looks like\nthe one in the image,\u201d or with instructional texts specifying\nthe intended design usage, such as \u201ca logo for the Idea2Img\nsystem.\u201d To convert IDEA into an image, humans may first\ndraft detailed descriptions of the imagined image, and then\nuse text-to-image (T2I) models [31,33,34,36,54] to gener-\n1Short for \u201cIdea2Img.\u201d Logo design\nassisted by Idea2Img.\n\u2460\n\u2461\n\u2462\n\u2460 \u2461 \u2462\nFigure 1. Idea2Img framework enables LMMs to mimic human-\nlike exploration to use a T2I model, enabling the design and gener-\nation of an imagined image specified as a multimodal input IDEA.\nThe iterative process involves LMMs functioning in different roles\nto refine the image creation. Specifically, LMMs will (1) generate\nand revise text prompts for the T2I model, (2) select the best draft\nimages, and (3) provide feedback on the errors and revision direc-\ntions. This multimodal iterative self-refinement process requires\nLMMs to memorize the T2I model\u2019s characteristics observed in\nprevious iterations as humans and adjust T2I prompts accordingly.\nate the image. This manual process for users to search for\nan ideal detailed description (i.e., T2I prompt) that fits the\nT2I model typically involves iterative exploration [44, 58].\nAs shown in Figure 1, humans may first design and draft an\ninitial T2I prompt based on their imagined IDEA to gener-\nate. Then, they can obtain multiple draft images with a T2I\nmodel, select the most promising draft, write text feedback,\n1\narXiv:2310.08541v1  [cs.CV]  12 Oct 2023\nphoto of the object pointed \nby the blue arrow, and a \nbrown corgi dog\npainting of a corgi dog with \nstyle similar to this one in \nthe image\nan image of a hand \nholding an iphone. \nThe image is used \nfor illustrating how \nto take a screen \nshot on iphone\n5 people sitting \naround a table \ndrinking beer and \neating buffalo \nwings\na logo suitable for \na stylish hotel.\na drawing with the \nbackground changed to a \nbeach\nA hand drawing of a room \nwhere people can sleep and \nstudy. Hand drawing shows \nthe sketch and looks like the \ngiven image.\na plate that has no \nbananas on it. \nthere is a glass \nwithout orange \njuice next to it\na whole cake on \nthe table with \nwords Azure \nResearch written \non the cake\nan image of a car \nperfect for a \nchildren's painting \ncompetition\nPhoto of \nBill Gates \nwith the \nsame cloth \nas in the \ngiven \nimage\nwith a \ndog \nlooks \nlike \nthis \none in \nthe \nimage\nObject count\nScene text\nKnowledge\nPrompt following\nVisual design\nIntended usage\nVisual design & style\nConcept customization & visual pointing\nImage manipulation\nMultiple concepts customization\nBlending images for new visual design\nStyle transfer\npainting of a corgi dog with \nstyle different from this \none in the image\nFind the the image style \npattern in the left two dog \nimages and apply it on the top \nright people in tree pose \nimage. Provide a textual \ndescription that keeps the \ncontent in the people in tree \npose image, with the correct \nstyle pattern.\nIn-context entity and style transfer\nOpposite style transfer\nCartoon drawing of Mr \nBean playing tennis, with \nthe same cloth and pose \nas in the given image\nVisual attribute referring\nA \npainting \nof a \ntennis \ngame\n, with the \nimage \nstyle \nsimilar to \nthis one in \nthe image\nVisual attribute referring & style transfer\nand \nthe \nsecond \nimage \nis\nA logo with a \ndesign that \nnaturally \nblends the \ntwo given \nimages as a \nnew logo. \nThe first \nimage is\nIDEA\nT2I\nIdea2Img\nIDEA\nT2I\nIdea2Img\nIDEA\nT2I\nIdea2Img\nIDEA\nT2I\nIdea2Img\nIDEA\nT2I\nIdea2Img\nFigure 2. Overview of the image design and generation scenarios enabled by Idea2Img. In each sub-figure, the image and text in the left\ngreen box are the user input IDEA. The center image is the baseline results directly generated by the same T2I model with a human-written\nT2I prompt, and the image on the right is generated with the T2I prompt discovered by Idea2Img\u2019s iterative self-refinement exploration.\nand further revise the T2I prompt. As this iteration pro-\ngresses, we humans can swiftly grasp the characteristics of\na specific T2I model, such as words that the model can not\nunderstand, finally producing a good image generated by a\nsuitable T2I prompt. Given the remarkable capabilities of\nlarge multimodal models (LMMs) [13, 26, 48], we explore\nif we can build systems based on LMMs to develop similar\niterative self-refinement ability, thereby relieving humans\nfrom the tedious process of converting ideas to images.\nIterative self-refinement is one intrinsic ability humans\npossess when exploring unknown environments and solv-\ning complicated problems. Large language models (LLMs)\nagent systems [9, 23, 40] have demonstrated the effective-\nness of self-refinement in better addressing natural language\n2\nprocessing tasks, such as acronym generation, sentiment re-\ntrieval, text-based environment exploration, etc. Transition-\ning from text-only tasks to multimodal environments poses\nnew challenges of improving, assessing, and verifying mul-\ntimodal contents, such as multiple interleaved image-text\nsequences. For example, when learning to use T2I mod-\nels, LMMs need to improve the generation with revised T2I\nprompts, assess multiple images in detail to select the best\ndraft, and verify the draft image with the multimodal IDEA\nto provide text feedback. These steps, each requiring dif-\nferent multimodal understanding capabilities, jointly enable\nthe intriguing multimodal iterative self-refinement ability.\nSuch an LMM framework can automatically learn to tackle\nvarious real-world problems [48] via self-exploration, such\nas navigating GUI to use electronic devices, exploring un-\nknown physical environments via an embodied agent, en-\ngaging in electronic games, and so on. In this study, we\nfocus on \u201cimage design and generation\u201d as the task to study\nthe multimodal iterative self-refinement ability.\nTo this end, we introduce Idea2Img, a multimodal itera-\ntive self-refinement framework for automatic image design\nand generation. As illustrated in Figure 1, Idea2Img in-\nvolves an LMM, GPT-4V(ision) [1,26\u201328], interacting with\na T2I model to probe its usage and find an effective T2I\nprompt. The LMM will act in different roles to analyze\nthe return signal from the T2I model (i.e., draft images)\nand design the next round\u2019s queries (i.e., text T2I prompts).\nThe three roles of generating T2I prompts, selecting draft\nimages, and reflecting feedback together enable the mul-\ntimodal iterative self-refinement ability.\nSpecifically, (1)\nPrompt generation: GPT-4V generates N text prompts that\ncorrespond to the input multimodal user IDEA, conditioned\non the previous text feedback and refinement history; (2)\nDraft image selection: GPT-4V carefully compares N draft\nimages for the same IDEA and select the most promising\none; (3) Feedback reflection: GPT-4V examines the dis-\ncrepancy between the draft image and the IDEA. GPT-4V\nthen provides feedback on what is incorrect, the plausible\ncauses, and how T2I prompts may be revised to obtain a bet-\nter image. Furthermore, Idea2Img is enhanced with a mem-\nory module that stores all prompt exploration histories, in-\ncluding previous draft images, text prompts, and feedback.\nThe Idea2Img framework iterates among these three steps\nwith GPT-4V for automatic image design and generation.\nTo users, Idea2Img functions as an enhanced image de-\nsign and generation assistant. Compared with T2I mod-\nels, Idea2Img can handle design instructions instead of re-\nquiring detailed image description, support the multimodal\nIDEA input, and generate images of better semantic and\nvisual qualities.\nWe overview representative image de-\nsign and generation scenarios in Figure 2. For example,\nIdea2Img can incorporate the visual design and intended\nusage description in IDEA, extract arbitrary visual infor-\nmation from the input image, and process IDEA with ar-\nbitrarily interleaved image-text sequences. Built upon these\nnew functionalities and scenarios of interest, we develop an\nevaluation IDEA set with 104 samples, containing compli-\ncated queries that humans may fail in their first trials. We\nperform user preference studies on Idea2Img with different\nT2I models. The consistent user preference score improve-\nments on different image generation models, e.g., +26.9%\nwith SDXL [31], indicate the effectiveness of Idea2Img in\nimage design and generation.\nOur contributions are summarized as follows.\n\u2022 We study \u201cautomatic image design and generation,\u201d\nwhich aims to create an image from an input IDEA.\nThis high-level idea may contain interleaved reference\nimages, and instructional texts for intended designs.\n\u2022 We explore the multimodal iterative self-refinement\nability in GPT-4V-based systems, showcasing its ef-\nfectiveness in improving, assessing, and verifying mul-\ntimodal contents.\n\u2022 We propose Idea2Img, a multimodal iterative self-\nrefinement framework that enhances any T2I model for\nimage design and generation, enabling various new im-\nage creation functionalities with better image qualities.\n\u2022 We present an evaluation set with 104 challeng-\ning multimodal IDEA. The consistent user preference\nscore gains, when experimented on different image\ngeneration models, indicate Idea2Img\u2019s effectiveness\nin automatic image design and generation.\n2. Related Work\nLLM-based self-refinement. Idea2Img is inspired by the\neffectiveness of iterative self-refinement in LMM-based\nagent systems [23, 29, 40] in exploring unknown environ-\nments and tasks, built upon the successful LLM agents [14,\n30, 32,37, 47, 52,57]. Self-refine [23] takes the same LLM\nto iteratively critique its outputs, and leverage this feedback\nto enhance its predictions. This framework shows effec-\ntiveness across various NLP tasks, including sentiment re-\nversal, dialogue response, math reasoning, and more. Re-\nflexion [40] explores a self-reflective LLM system on the\ntext-based environment exploration task [41] and multi-hop\nQA [50]. Despite the success, LLM-based self-refinement\nnaturally can not understand multimodal inputs.\nConse-\nquently, the explored tasks and environments are limited\nto the natural language description, such as AlfWorld [41].\nIdea2Img delves into the potential of an LMM-based itera-\ntive self-refinement system for multimodal environment ex-\nploration, from a simple T2I model to other more compli-\ncated environments.\nMultimodal agents. Our Idea2Img is related to multimodal\nagents [15,20,22,38,43,45,49,55] that chain external tools\n3\nsuch as T2I or vision-language models with LLMs for mul-\ntimodal tasks. For instance, MM-ReAct [49] integrates an\nLLM with multiple vision tools for multimodal reasoning\nand action, enabling it to solve various complicated visual\nunderstanding tasks, ranging from multi-hop document rea-\nsoning to open-world video interpretation.\nVisual Chat-\nGPT [45] empowers ChatGPT to allocate various image\ngeneration models, such as Stable Diffusion [34], img2img\nmodel [24], ControlNet [56], enabling multi-step visual\nediting and generation.\nThe primary difference between\nIdea2Img and existing multimodal agent studies [45,49] lies\nin the approach to understand the tool usage. Existing stud-\nies assume the knowledge of how to use each tool, and pro-\nvide such information to LLMs via text instructions or in-\ncontext examples. In contrast, the optimal usage of the tool\nremains unknown in Idea2Img and requires iterative explo-\nration. Another minor distinction is that Idea2Img utilizes\nLMMs instead of LLMs, thereby does not require general\nvisual understanding tools such as a caption model [46].\nExtensions of base T2I models. Idea2Img provides a more\nnatural way for users to design and produce their desired\nvisual content. This framework, which extends T2I models\nfor new functionalities, is related to various works in im-\nproving base T2I models [31, 33, 34, 36, 54]. These studies\ninclude extending the base T2I model to better follow user\nprompts [5, 7, 10, 11], finding magic words in T2I prompts\nfor better visual quality [44, 58], supporting extra image\ninput for image manipulation [6, 16, 17, 24], style trans-\nfer [12], visual concept customization [3,8,18,35,39], and\nso on. While specialized T2I extensions can address a single\nspecific functionality, Idea2Img offers a more unified and\nwidely applicable framework. That is, a single Idea2Img\nframework can handle various generation scenarios, rang-\ning from style transfer to attribute customization, without\nrequiring separate models or task-specific model design and\nfinetune. More importantly, Idea2Img effectively collabo-\nrates with those enhanced generative models, consistently\nimproving them by exploring suitable text prompts.\n3. Idea2Img Framework\nFigure 3 illustrates the Idea2Img framework. Idea2Img\nframework involves two core pre-trained models, i.e., the\nGPT-4V(ision) as the LMM M and a T2I model2 to explore\nG. Idea2Img also contains a memory m that stores insights\non G discovered by M during previous iterations.\nExecution flow.\nWe begin with an overview of the key\nsteps in M iteratively exploring the use of G. Starting from\nthe top-left of Figure 3, \u201cinitial prompt generation\u201d converts\nthe input multimodal user IDEA into T2I text prompts, later\nproducing multiple draft images with T2I model G. \u201cDraft\n2We will show image generation models other than T2I later in experi-\nments. For clarity, we use T2I as a representative for method introduction.\nimage selection\u201d then selects the best draft image among\nthem for the current iteration. The selected image is either\noutput as the final prediction or continues for further refine-\nment, depending on the stop condition. For the latter, \u201cfeed-\nback reflection\u201d compares the current best draft image with\nthe multimodal IDEA, and summarizes the major discrep-\nancy as text feedback. With the iteration history and text\nfeedback, \u201crevised prompt generation\u201d then drafts revised\nT2I prompts and continues the iterative self-refinement with\nthe new set of draft images.\n1 Initial prompt generation. This step generates N ini-\ntial T2I prompts\n\b\ny0\n0, . . . , yN\u22121\n0\n\t\nfollowing the input user\nIDEA x, by prompting M with LMM prompt pgen:\n\b\ny0\n0, . . . , yN\u22121\n0\n\t\n= M(x, pgen)\n(1)\nThe \u201cinitial prompt generation\u201d requires M to understand\nthe multimodal user IDEA x and convert the design IDEA\ninto descriptive T2I prompts that the T2I model could un-\nderstand. LMM prompt pgen is a zero-shot prompt without\nin-context examples.\nWith the \u201cinitial prompt generation\u201d step, Idea2Img\ncan understand user IDEA with interleaved image-text se-\nquences, instead of the text-only T2I prompts containing\nthe image description. Specifically, (1) IDEA can be a high-\nlevel design or usage instead of the detailed image descrip-\ntion, such as \u201ca car image for a children\u2019s painting com-\npetition\u201d; and (2) IDEA can take multiple images and use\ninterleaved text instruction to extract arbitrary visual in-\nformation of interest, including image style, visual entity,\nobject attributes, etc.\nThen, in iteration t = 0 as well\nas future iterations t = t, each T2I prompt yn\nt is sepa-\nrately sent to the T2I model G, resulting in N draft images\nin\nt = G(yn\nt ), n = 0, . . . , N \u2212 1.\n2 Draft image selection. With the N draft images in iter-\nation t, \u201cdraft image selection\u201d selects the best draft image\ni\u2217\nt by prompting M with LMM prompt pselect:\ni\u2217\nt = M(i0\nt, . . . , iN\u22121\nt\n, x, pselect).\n(2)\nThe design of a \u201cdraft image selection\u201d step is motivated\nby the observation that T2I models could generate bad im-\nages with good prompts. This step is designed to filter out\nlow-quality images, and avoid the quality perturbation to\ndominate the iterative refinement.\nThe task of selecting the best image requires M to com-\npare and grade both the semantics and visual quality of N\nsimilar draft images. We find such a \u201cspot the difference\u201d\ntask very challenging for LMMs, and only the very recent\nmodel [26] is capable of performing the selection reliably.\nQualitative examples are shown in Figure 10.\n3 Feedback reflection. After obtaining the selected im-\nage i\u2217\nt , the framework checks the stop condition, such as if\n4\n\u2460\n\u2461\n\u2462\n\u2464\n\u2465\n\u2460\n\u2462\n\u2461\nFigure 3. The framework overview of Idea2Img, which takes an LMM [26, 27] to explore a T2I model via multimodal iterative self-\nrefinement, leading to an effective T2I prompt for the input user IDEA. The rounded rectangle shape indicates a GPT-4V call.\nthe current iteration t exceeds the maximum T. Idea2Img\nthen outputs i\u2217\nt as the output image or proceeds the refine-\nment process to the \u201cfeedback reflection\u201d step accordingly.\n\u201cFeedback reflection\u201d aims to provide text feedback ft\nthat describes the direction to improve for draft image i\u2217\nt .\nThe steps prompts M with LMM prompt pfb, conditioned\non the draft image i\u2217\nt and memory m:\nft = M(i\u2217\nt , m, x, pfb).\n(3)\n\u201cFeedback reflection\u201d takes M to compare an image i\u2217\nt with\nthe multimodal user IDEA x, and summarize the gap as text\nfeedback ft. The step not only requires M to identify the\ndiscrepancy between image i\u2217\nt and IDEA x, but also bene-\nfits from writing the major errors to make the iteration ef-\nfective. In practice, we find it helpful to explicitly specify\nthe aspects to check, such as style, entity, attributes, appear-\nance, etc., via text instructions or in-context examples in\nLMM prompt pfb. Furthermore, we add text instructions\nto pfb to have M \u201cfocus on one thing to improve in each\nfeedback,\u201d and \u201cprovide a high-level explanation of how to\nmodify prompts to address the given feedback.\u201d Example\nfeedback is shown in Figure 8.\n1\nRevised prompt generation. Finally, \u201cprompt gen-\neration\u201d takes text feedback ft and memory m to draft N\nrevised prompt\n\b\ny0\nt+1, . . . , yN\u22121\nt+1\n\t\n, by prompting M with\nLMM prompt previse:\n\b\ny0\nt+1, . . . , yN\u22121\nt+1\n\t\n= M(ft, m, x, previse).\n(4)\nGenerating revised prompts requires M to understand the\nproperty of G stored in memory m, thereby drafting new\nT2I prompts that could most likely address the issue iden-\ntified in ft.\nWe empirically find Idea2Img generates\nT2I prompts that work better for G during iterative self-\nrefinement, with examples shown in Figure 9.\nMemory module. Memory m is one important design in\nIdea2Img. m has the format of interleaved image-text se-\nquences that store all previous iterations\u2019 feedback, selected\ndraft image, and the corresponding text prompts:\nmt =\n\u0002\ny\u2217\n0, i\u2217\n0, f0, . . . , y\u2217\nt\u22121, i\u2217\nt\u22121, ft\u22121\n\u0003\n.\n(5)\nIt allows LMM M to understand the properties and capa-\nbilities of the T2I model G in use, such as a keyword that\nG may not understand or a complicated scene that G fail to\ngenerate, and incorporate such knowledge in generating the\nrevised T2I prompts y. For example, it may describe the ap-\npearance of a yoga pose in detail, instead of only mention-\ning the pose name in y. Examples are shown in Figures 5-7,\nwhen comparing initial and refined T2I prompts y0 and yT .\n4. Experiments\n4.1. Experiment Settings\nCompared model variants. We mainly compare the fol-\nlowing three models in image generation.\n\u2022 \u201cInitial-round manual prompt\u201d is the baseline T2I\nprompt written by humans with minor prompt engi-\nneering. It serves as the baseline of a T2I prompt that\nmerely contains key information in IDEA.\n\u2022 \u201cInitial-round\nIdea2Img\nprompt\u201d\nis\nthe\nLMM-\ngenerated T2I prompt in the initial round. Specifically,\nthe max iteration T = 1, and LMM M is only used\nfor initial prompt generation and draft image selec-\ntion, but not feedback reflection nor revised prompt\n5\nMethod (%)\nSDXL v1.0\nDeepFloyd IF\nSD v2.1\nSD v1.5\nSDXL-img2img\nIF-img2img\nInitial-round manual prompt\n13.5\n14.4\n13.5\n8.6\n8.6\n8.6\nInitial-round Idea2Img prompt\n29.8\n34.6\n40.4\n43.3\n34.6\n38.5\nIterative self-refined Idea2Img prompt\n56.7\n51.0\n46.2\n48.1\n56.7\n52.9\n\u2206iteration(\u2191)\n+26.9\n+16.3\n+5.8\n+4.8\n+16.3\n+14.4\nTable 1. User preference scores when applying Idea2Img onto different image generation models. We observe that \u201cIterative self-refined\nIdea2Img prompt\u201d is consistently favored when applied to all experimented image generation models.\ngeneration. This Idea2Img variant is used to ablate\nIdea2Img\u2019s gain from prompt generation and selection,\nvs. the further iterative refinement.\n\u2022 \u201cIterative self-refined Idea2Img prompt\u201d is complete\nIdea2Img pipeline with the max iteration T = 3.\nEvaluation samples and metrics.\nFor the quantitative\nevaluation, we collect a dataset of 104 user IDEA as in-\nput queries. Among them, 33 queries contain text only, 43\nqueries contain an image-text sequence with a single im-\nage, and the remaining 28 contains a sequence with two or\nmore images. The text in most IDEA contains not only de-\nscriptive content text that describes the scene to generate,\nbut also instructional text such as \u201ca logo for commercial\nadvertising\u201d or \u201cgenerate the pointed dog in the blue color.\u201d\nAll test queries are manually composed.\nWe then perform the user preference study as the main\nquantitative metric.\nUsers are presented with the IDEA\nand multiple images to select the best one for each IDEA.\nThe evaluation script automatically shuffles the order dur-\ning evaluation to prevent the influence of image orders.\nExperimented T2I models. We experiment Idea2Img on\na wide variety of T2I model G with diverse model capaci-\nties and functionalities. Specifically, we study Stable Diffu-\nsion (SD) v1.5 [34], SD v2.1, SDXL v1.0 with refiner [31],\nand DeepFloyd IF (IF-I-XL and IF-II-L) [19]. Other than\nT2I models, we also consider the img2img pipeline (i.e.,\nSDEdit [24]) for SDXL and DeepFloyd IF, as a demonstra-\ntion of using Idea2Img for the text-conditioned image-to-\nimage generation. The default strength t0 in the img2img\npipeline is 1.00. SDXL-img2img and IF-img2img are the\nsame as SDXL and IF (i.e., T2I) when IDEA contains text\nonly, and condition on the first image with IDEA contains\nmultiple images. LMM prompts pgen, pselect, pfb, previse\nare kept the same for all experimented T2I models.\n4.2. Image Generation Results\nUser preference evaluation. Table 1 compares the user\npreference when selecting from the three images generated\nby \u201cinitial-round manual prompt,\u201d \u201cinitial-round Idea2Img\nprompt,\u201d and \u201citerative self-refined Idea2Img prompt,\u201d us-\ning the same T2I model, for each user IDEA. Among\nT2I models with different model sizes and functionalities,\nIdea2Img leads to consistent improvements in user pref-\nerence.\nThe initial-round Idea2Img prompt already im-\nproves the initial-round manual prompt, by effectively un-\nIdea2Img\nSDXL v1.0\nDeepFloyd IF\nSD v1.5\nSD v2.1\nFigure 4. User preference scores among T2I models before and\nafter iterative self-refinement. We observe that the initially favored\nT2I model, SDXL, benefits more from the Idea2Img iteration.\nderstanding the multimodal user IDEA and selecting the\nbest draft images. The full Idea2Img framework further im-\nproves from the initial-round Idea2Img results with the mul-\ntimodal iterative self-refinement. For example, when using\nSDXL v1.0, users prefer the images generated by Idea2Img\n59/104 = 56.7% times, compared with the baseline of\n14/104 = 13.5%. Similar improvements are observed on\nall experimented T2I models, as shown in the bolded row of\n\u201citerative self-refined Idea2Img prompt.\u201d\nFurthermore, we examine which T2I model benefits the\nmost from the LMM iterative self-refinement.\nBy com-\nparing the \u2206iteration in Table 1 that represents the differ-\nence between first-round and iterative Idea2Img user pref-\nerence, we observe that stronger T2I models tend to benefit\nmore from LMM refinements. For example, SDXL and IF\nbecome more favored 26.9% and 16.3% times, compared\nwith SD v2.1 and SD v1.5\u2019s 5.8% and 4.8%. The trend\nthat stronger T2I models benefit more from Idea2Img is\nalso observed in Figure 4\u2019s analysis, where users pick their\npreferred image generated by different T2I models. After\nIdea2Img\u2019s iterative refinement, the initially favored model\nSDXL benefits more from the iteration, resulting in an even\nhigher user preference rate. We conjecture that the better\nlanguage understanding ability in stronger T2I models en-\nables them to better follow revised T2I prompts. They also\nhave a better image generation capability that makes it pos-\nsible to generate challenging scenes, when given a good T2I\nprompt optimized by Idea2Img. We are eager to see how\nthis trend evolves as we experiment with even stronger T2I\nmodels [2,25] in the future. Nonetheless, Idea2Img is effec-\ntive across T2I models of varying capacities, consistently\nleading to a higher user preference score.\n6\n(i)\n(h)\n(g)\n(f)\n(e)\n(d)\n(c)\n(b)\nA Cavalier King Charles \nSpaniel running on a tiled \nfloor, happy mood, bright \nlighting\nphoto of a dog looks like the circled one in \nthe image running on the floor\nphoto of a dog looks like the \ncircled one in the image \nrunning on the floor that is a \nblack and white dog\nA small Cavalier King Charles \nSpaniel with black and white \nfur, running on a tiled floor, \ntongue out, happy mood, \nbright lighting\nA group of 5 friends sitting \naround a wooden table, \ndrinking beer and eating \nbuffalo wings in a casual \nsetting.\n5 people sitting around a table drinking \nbeer and eating buffalo wings\n5 people sitting around a \ntable drinking beer and \neating buffalo wings\nA group of 5 friends sitting \naround a wooden table, with \none person at the head of \nthe table and two people on \neach side, drinking beer and \neating buffalo wings in a \ncasual setting, with a \nwindow in the background \nand a warm, inviting \natmosphere.\nA painting of Mt Rainier \nmountain with a tree and \nlake in the foreground, in a \ncolorful and abstract style \nsimilar to a Picasso painting.\nPainting of Mt rainier mountain with tree \nand lake in the foreground, with style \nsimilar to this one in the image\nPainting of Mt rainier \nmountain with tree and lake \nin the foreground, with style \nsimilar to this one in the \nimage that has colorful \nblocks\nA painting of Mt Rainier \nmountain with a tree and \nlake in the foreground, with \nbold lines and bright colors, \nwith a cubist and surrealistic \nstyle, with a focus on \ngeometric shapes and \npatterns.\nA portrait of Bill Gates with \nminimalistic style and vague \nfacial features, in a \nmonochrome color scheme.\nA vague portrait of Bill Gates with minimal \nvisual and face details\nA vague portrait of Bill Gates \nwith minimal visual and face \ndetails\nA portrait of Bill Gates with \nan extremely abstract and \nvague style, with almost no \nemphasis on the facial \nfeatures, in a monochrome \ncolor scheme, with a \ncompletely abstract \nbackground.\nInput user IDEA\nInitial-round manual prompt\nInitial-round Idea2Img \nprompt\nIterative self-refined \nIdea2Img prompt\nA painting of \na tennis game \nfrom camera \nangle\n, with the \nimage \nstyle \nsimilar to \nthis one in \nthe image\nA painting of a tennis game \nfrom a top-down camera \nangle, with the image style \nsimilar to Claude Monet's \nimpressionist paintings, with \na bright and sunny \natmosphere.\nA painting of a tennis game \nfrom camera angle, with the \nimage style similar to this \none in the image that is \nimpressionist\nA painting of a tennis game \nfrom a top-down camera \nangle, with the image style \nsimilar to Claude Monet's \nimpressionist paintings, with \na bright and sunny \natmosphere, with a blue sky \nand green trees in the \nbackground, with the players \nwearing white clothes, with \na large crowd in the stands.\nA logo design for a 2024 \nconference held in Seattle, \nfeaturing the Seattle skyline \nand the conference title in a \nmodern font, with a blue and \ngreen color scheme.\na logo for a 2024 conference held in \nSeattle. Please provide a textual description \nof the design before generating\na logo for a 2024 conference \nheld in Seattle.\nA logo design for a 2024 \nconference held in Seattle, \nwith a modern and \nminimalist design, featuring \nthe Seattle skyline and the \nconference title in a \ngeometric sans serif font, \nwith a blue and green color \nscheme.\nA scene of a man in a suit \nwaving his hand, with the \nsame style as the geometric \ndog image on the left.\nFind the the image style \npattern in the left two dog \nimages and apply it on the \ntop right people waving hand \nimage.\nA person in a business suit \nwaving with his right hand, \ndepicted in a polygonal art \nstyle reminiscent of the dog \nimage, with a mosaic of \ncolorful geometric shapes, \nagainst a backdrop of a blue \nsky.\nwith a dog \nlooks like \nthis one in \nthe image\nBill Gates in a formal suit on \na bustling city street, raising \nhis hand in a friendly wave \nlike a charming businessman, \nwith a cute pug dog with a \nwrinkled face and large eyes \nby his side, under natural \ndaylight, with a sense of \napproachability.\nPhoto of Bill Gates with the \nsame hand gesture as in the \ngiven image that is waving \nhand, with a dog looks like \nthis one in the image that is \na pug dog\nBill Gates wearing a suit and \ntie, standing on a busy street \nwith tall buildings, waving \nwith his right hand raised \nand palm facing forward, \nwith a small pug dog with an \nadorably wrinkled face and \nbig round eyes next to him, \nin daylight, exuding \nfriendliness and warmth.\nA whole cake on a wooden \ntable with the words Azure \nResearch written on it in \nblue icing, with a white \ntablecloth and a vase of \nflowers in the background.\na whole cake on the table with words Azure \nResearch written on the cake\na whole cake on the table \nwith words Azure Research \nwritten on the cake\nA top-down perspective of a \ncake on a table, with the \nwords \"Azure Research\" \nmeticulously written in blue \nicing on the top, surrounded \nby a modern kitchen with \nsunlight filtering through.\nBill Gates in a s\nin front of a bu\npug dog sitting \nground next to \nwaving his hand\nmanner\nBill Gates in a s\nin front of a bu\npug dog sitting \nwaving his hand\nmanner\nA scene of a ma\nwith his right h\na friendly wave\nfacing forward,\nsame style as th\ndog image on t\nblue sky backgr\nslight smile on \na modern and a\n(a)\n(a)\n(a)\nFind the the image style pattern in the left \ntwo dog images and apply it on the top \nright people waving hand image. Provide a \ntextual description that keeps the content \nin the people waving hand image, with the \ncorrect style pattern.\nPhoto of Bill \nGates with \nthe same \nhand gesture \nas in the given \nimage\nFigure 5.\nThe comparisons among the initial-round manual prompts, initial-round Idea2Img prompts, and the iterative self-refined\nIdea2Img prompts, with the SDXL v1.0 [31] used as the T2I model.\n7\n(i)\n(h)\n(g)\n(f)\n(e)\n(d)\n(c)\n(b)\n(a)\nA surreal scene of a \nCoca-Cola bottle and a \nmarble table merging into \none, in an abstract space \nwith distorted dimensions, \nbright colors, and dynamic \nlighting.\nphoto with a design that \ncomposites the two given \nobjects into the new photo. \nThe placement of the objects \ndoes not adhere to the laws \nof physics. Please provide a \ntextual description of the \nobjects and design before \ngenerating. The first object is \na marble table, and the \nsecond object is a coke\nAn imaginative scene where \na Coca-Cola bottle is floating \nand twisting above a marble \ntable, defying gravity, with \nthe bottle and table merging \ninto each other in places, \nsurrounded by a \nkaleidoscope of colors and \nethereal lighting that gives a \ndreamlike atmosphere.\nInput user IDEA\nInitial-round manual prompt\nInitial-round Idea2Img \nprompt\nIterative self-refined \nIdea2Img prompt\nCartoon drawing with a design \nthat naturally blends the two \ngiven images as a new \nminimalist drawing. Please \nprovide a textual description \nof the design before \ngenerating. The first image is\n, and \nthe \nsecond \nimage \nis\nA cartoon drawing of a llama \nwearing a suit and waving, \nblending the minimalist style \nof the first image with the \nprofessional attire of the \nsecond image.\nCartoon drawing with a \ndesign that naturally blends \nthe two given images as a \nnew minimalist drawing. \nPlease provide a textual \ndescription of the design \nbefore generating. The first \nimage is a llama logo, and \nthe second image is a person \nin suit\nA whimsical cartoon of a \nllama with a human body \ndressed in a stylish suit, \nwaving its hand, combining \nthe charm of a cute llama \ndrawing and the elegance of \na man in a suit, in a \nminimalist style, with a \nmuted color palette.\nphoto with a design that \ncomposites the two given \nobjects into the new photo. \nThe placement of the objects \ndoes not adhere to the laws \nof physics. Please provide a \ntextual description of the \nobjects and design before \ngenerating. The first object is\n, and \nthe \nsecond \nobject \nis\nA close-up of a sushi roll in \nthe making, with a pile of \nrice on a seaweed sheet and \nsalmon and cucumber being \nadded, next step is rolling \nand cutting the sushi.\nimage that depicts what will \nhappen next based on the \nstory in the two given \nframes. Please provide a \ntextual description of the \nimage before generating. \nThe first frame in the story is \nrice on sushi roll, and the \nsecond frame in the story is \ningredients on rice\nA close-up of a sushi roll in \nthe making, with a pile of \nrice on a seaweed sheet and \nsalmon and cucumber being \nadded, next step is rolling \nand cutting the sushi, with a \nbamboo mat and knife \nnearby, in a kitchen setting.\nimage that depicts what will \nhappen next based on the \nstory in the two given frames. \nPlease provide a textual \ndescription of the image \nbefore generating. The first \nframe in the story is\n, and \nthe \nsecond \nframe \nin the \nstory is\nA gray cat and an orange cat \nrunning on the beach, with \nthe gray cat on the right and \nthe orange cat on the left.\nTwo cats running on the \nbeach, the right one looks \nlike a gray cat and the left \none looks like a orange cat\nA gray cat with its head \ndown and eyes looking \nforward and an orange cat \nwith its eyes closed, both \nrunning on a beach with the \ngray cat on the right and the \norange cat on the left, with \nthe ocean in the background.\nTwo cats running on the \nbeach, the right one looks like\nand the \nleft \none \nlooks \nlike\nportrait of Bill Gates with style different from this \none in the image\nA portrait of Bill Gates in a \ncartoon style, with bright \ncolors and a cheerful mood.\nportrait of Bill Gates with \nstyle different from this one \nin the image that is \nimpressionist\nA portrait of Bill Gates in a \ncartoon style, with bright \ncolors and a cheerful mood, \nwith a light-colored \nbackground to make the \nportrait stand out.\na watercolor painting of the same house with the \nsame number of windows and the frontal view as \nthe one in the sketch\nA watercolor rendition of a \nhouse with a frontal view \nand six windows, with a \nfocus on the architectural \ndetails\na watercolor painting of the \nsame house with the same \nnumber of windows and the \nfrontal view as the one in the \nsketch that is a two-level \nhouse with four large \nwindows and three small \nwindows in the center\nA watercolor illustration of a \nhouse with a porch and six \nwindows, with an emphasis \non the symmetry and style of \nthe house in the sketch.\nA watercolor cartoon logo based on the design in \nthe given image\nA logo design of a llama with \na red scarf and beanie, in a \nwatercolor cartoon style, \nwith a soft and pastel color \npalette.\nA watercolor cartoon logo \nbased on the design in the \ngiven image that is a llama\nA logo design of a llama \nstanding on all fours, with a \nred scarf and beanie, in a \nwatercolor cartoon style, \nwith a soft and pastel color \npalette, similar to the design \nin the given image.\nphoto of a dog looks like the one in the given \nimage running on the beach\nA small dog with a fluffy coat \nrunning on the beach, with \nthe ocean in the background.\nphoto of a dog looks like the \none in the given image \nrunning on the beach that is \na fluffy golden dog\nA playful Yorkshire Terrier \nwith a collar running on the \nbeach, with the sun setting \nover the ocean in the \nbackground.\na dog looks like this one in \nthe image running on the \ngrass, but in blue color\n, with \nthe \nimage \nstyle \nsimilar \nto this \none in \nthe \nimage\nA pug dog with a blue coat \nrunning on a field of green \ngrass, with a style \nreminiscent of cubism.\na dog looks like this one in \nthe image running on the \ngrass, but in blue color . The \ndog is pug dog, with the \nimage style similar to this \none in the image that \ncontains blocks\nA pug dog with a blue coat \nrunning on a field of green \ngrass, with its legs stretched \nout and its tail wagging as it \nruns, in a style reminiscent \nof cubism.\nA cartoo\nwearing \nblack tie\nraised in\nhis palm \nblending\nof the fir\nprofessio\nsecond im\nphoto with a\nnaturally ble\nobjects into \nThe placeme\nperfectly ad\nof physics an\nreal life. Plea\ntextual desc\nobjects and \ngenerating. \nFigure 6.\nThe comparisons among the initial-round manual prompts, initial-round Idea2Img prompts, and the iterative self-refined\nIdea2Img prompts, with the SDXL-img2img [24,31] used as the image generation model. Instead of random noise, the image generation\nstarts from the input image with added noise [24], showing the effectiveness of Idea2Img on text-conditioned image-to-image pipelines.\n8\n6 cakes placed in a box\n(a)\nA marble table with a \nCoca-Cola bottle floating \nabove it, defying gravity, in a \nmodern living room, with \nsoft ambient lighting, \nminimalistic decor, and a hint \nof surrealism.\nphoto with a design that \ncomposites the two given \nobjects into the new photo. \nThe placement of the objects \ndoes not adhere to the laws \nof physics. Please provide a \ntextual description of the \nobjects and design before \ngenerating. The first object is \na marble table, and the \nsecond object is a coke\nIn a contemporary \nliving room, a \nCoca-Cola bottle is \nsuspended in mid-air \nabove a marble table, \nwith a mystical aura, \nwarm lighting, and \nsleek decor, evoking a \nsense of magic and \ndefying gravity.\nSD v1.5\nSD v2.1\nDeepFloyd IF\nDeepFloyd IF img2img\nA happy dog with pink fur \nrunning on a street with \nbuildings in the background, \ndaytime, sunny.\nphoto of the circled dog but with \nbackground changed to a street\nphoto of the circled dog but \nwith background changed to \na street that is a pink fluffy \ndog\nA Yorkshire Terrier with pink \nfur running on a bustling \nstreet with skyscrapers in the \nbackground, daytime, bright, \ntraffic.\nA cardboard box with 6 \nhomemade cakes, each with \na unique design and flavor, \nready to be delivered.\n6 cakes placed in a box\nA brown cardboard box with \n6 homemade cakes, each \nwith a different flavor and \ndesign, arranged in a \nsymmetrical pattern.\nA casual portrait of Elon \nMusk and Mark Zuckerberg, \nboth wearing grey sweaters \nand black shirts, with a white \nbackground.\ngenerating an image with the same cloth as \nin the given image, but change the left \npersons to Elon Musk and the right person \nto Mark Zuckerberg\ngenerating an image with the \nsame cloth as in the given \nimage, but change the left \npersons to Elon Musk and \nthe right person to Mark \nZuckerberg\nElon Musk and Mark \nZuckerberg standing in a \nrelaxed pose, with their \nshoulders touching, in a grey \nbackground, with Elon Musk \nwearing a grey sweater and \nMark Zuckerberg wearing a \nblack shirt.\nA kitchen scene with an \nempty plate and a glass \nwithout orange juice on a \nwooden table, with a white \nbackground.\na plate that has no bananas on it. there is a \nglass without orange juice next to it\na plate that has no bananas \non it. there is a glass without \norange juice next to it\nA close-up view of a wooden \ntable with an empty glass \nand a white plate without \nbananas, with a white \nbackground.\nA contemporary logo for a \n2023 conference in Paris, \nshowcasing a stylized Eiffel \nTower, elegant fonts, and a \ncolor palette inspired by the \nFrench flag, with a touch of \nsophistication.\na logo for a 2023 conference held in Paris. \nPlease provide a textual description of the \ndesign before generating\na logo for a 2023 conference \nheld in Paris.\nA sleek and modern logo for \na 2023 conference in Paris, \nincorporating a stylized Eiffel \nTower, the year '2023' in a \nclassy font, and a tricolor \nbackground inspired by the \nFrench flag. The design \nincludes the word 'Paris' at \nthe bottom in a refined font, \nsymbolizing sophistication \nand culture.\nA logo of llama with the exact same style \nas in the given image\nA monochromatic logo that \ncombines a llama's face with \na stethoscope, similar to the \ngiven image, where the \nstethoscope forms the face \nand the footprints are inside, \nin a sleek and minimalist \nstyle, with bold black lines.\nA logo of llama with the \nexact same style as in the \ngiven image that has a \nminimalist design\nA logo in which a \nstethoscope is artistically \nshaped with its tubing \noutlining a llama's face and \nits ear tips as the llama's \nears, with paw prints inside \nthe face, akin to the given \nimage, in a simple, modern \ndesign with bold black \ncontours.\nA playful dog with a unique \nblue coat and a bright yellow \ncollar, enjoying a run on the \nbeach with the waves \ncrashing behind it.\nphoto of a dog looks like the one in the \ngiven image running on the beach, but \nchange the dog color to blue and the \ncollar color to yellow\nphoto of a dog looks like the \none in the given image \nrunning on the beach, but \nchange the dog color to blue \nand the collar color to yellow \nthat is a pug dog\nA pug-like dog with a unique \nblue fur, adorned with a \nplain yellow collar on its \nneck, racing across the beach \nwith sand flying under its \npaws, and a tranquil sea with \ngentle waves as the \nbackdrop.\nphoto with a design \nthat composites the \ntwo given objects into \nthe new photo. The \nplacement of the \nobjects does not \nadhere to the laws of \nphysics. Please provide \na textual description of \nthe objects and design \nbefore generating. The \nfirst object is\n, and \nthe \nsecond \nobject \nis\nInput user IDEA\nInitial-round manual prompt\nInitial-round Idea2Img \nprompt\nIterative self-refined \nIdea2Img prompt\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\nan image of a hand holding an \ntake a photo\nphoto with a design that \ncomposites the two given \nobjects into the new photo. \nThe placement of the objects \ndoes not adhere to the laws \nof physics. Please provide a \ntextual description of the \nobjects and design before \ngenerating. The first object is \na marble table, and the \nsecond object is a coke\nFigure 7.\nThe comparisons among the initial-round manual prompts, initial-round Idea2Img prompts, and the iterative self-refined\nIdea2Img prompts, with other image generation models, including SD v1.5, SD v2.1 [34], DeepFloyd IF, and IF-img2img [19].\n9\nQualitative comparisons. Figures 5-7 show the qualitative\nresults of the comparison in Table 1 to probe the improve-\nments made by Idea2Img. Specifically, Figure 5 presents\nexamples of Idea2Img explores the use of SDXL, a repre-\nsentative T2I model. Figure 6 examines SDXL-img2img, a\nrepresentative text-conditioned image-to-image model that\nadds noise to the input image and then performs text-\nconditioned denoising [24]. Figure 7 contains the results\nof Idea2Img working with other image generation models.\nIdea2Img could help users generate images that better\nfollow IDEA, such as the one with correct object counts and\nrendered scene texts in Figures 5(a,b). Idea2Img enables\nvisual content design, in addition to generation only that\nrequires a detailed visual content description. For exam-\nple in Figure 5(c), Idea2Img designs visual logo based on\nthe user IDEA of \u201chaving a logo for a 2024 conference in\nSeattle.\u201d Idea2Img can also understand user IDEA to search\nfor images with high aesthetic scores and great visual de-\ntails, or its opposite direction with \u201cminimal face details\u201d\nin Figure 5(d). The power of LMMs allows Idea2Img to\nextract arbitrary information from the input image for vi-\nsual generation. This could be any specific object in the\nimage, such as \u201cthe dog on the left\u201d or \u201cthe dog pointed\nto via a red circle\u201d in Figure 5(e). Figure 5(f) shows an\nexample of extracting the image style that requires domain\nknowledge for humans to accurately describe. The image\ninput can even be an in-context example that defines the de-\nsired image transformation, such as the visual style transfer\nshown in Figure 5(g). The ability to extract arbitrary infor-\nmation from the input image can be seamlessly extended to\ncompose multiple visual and text conditions, such as com-\nposing the camera angle and image style in Figure 5(h), and\ntwo objects in Figure 5(i).\nIdea2Img is also effective in finding T2I prompts for the\ntext-conditioned image-to-image model SDXL-img2img,\nas shown in Figure 6. Figures 6(c) and (d) illustrate gen-\nerating images that follow and differ from the reference\nimage style of watercolor and impressionist, respectively.\nIdea2Img can design visual contents with the inspiration\nof input images, e.g., a cartoon drawing that blends the\nllama and the person in suits in Figure 6(f), and compos-\ning the coke with the table in an imaginative way in Fig-\nure 6(g). Figure 6(h) illustrates representing the anticipated\naction of rolling the sushi with a generated image. Further-\nmore, Figure 7 shows the Idea2Img results on other T2I\nmodels, including SD v1.5, v2.1, DeepFloyd IF, and IF-\nimg2img.\nDespite the variance in the base T2I models\u2019\ncapacity, Idea2Img consistently helps design and generate\nbetter images.\nHow Idea2Img may assist humans?\nWe use selected\nqualitative results to highlight the scenarios where humans\nmight find Idea2Img most helpful in image design and gen-\neration, compared with conventional T2I generation.\n\u2022 Multimodal IDEA inputs. Idea2Img provides a more\nnatural way for human interaction, where users do not\nhave to describe their desired image solely through\ntexts and painstakingly search for the right terminol-\nogy. Instead, the multimodal IDEA allows Idea2Img\nto precisely extract specific elements from one or mul-\ntiple input images, such as the dog breed and color,\npointed objects, artist style, camera view, and more, as\nshown in Figure 5. Finding proper words that the T2I\nmodel can understand for such visual concepts could\nbe tedious for humans, e.g., the art style \u201cwith bold\nlines and bright colors, with a cubist and surrealistic\nstyle, with a focus on geometric shapes and patterns.\u201d\nin Figure 5(f). Idea2Img automates this process via\nIdea2Img iterative self-refinement.\n\u2022 Instructional texts for intended designs and usages.\nVanilla T2I models struggle to understand T2I prompts\nthat describe the intended visual design or purpose of\nthe generated image, such as \u201ca logo for a 2024 con-\nference held in Seattle\u201d or \u201ca car image for a children\u2019s\npainting competition.\u201d Instead, the prompts need to\nbe a comprehensive description of the image to gener-\nate, e.g., \u201ca modern and minimalist logo design, fea-\nturing the Seattle skyline in the center and the confer-\nence title below it with a blue and green color scheme\u201d\n(Figure 5(c)). This demands extra drafting effort from\nusers. In contrast, as shown in Figures 5-7, Idea2Img\neffectively understands the instructional texts in IDEA\nand creates images accordingly.\nFinally, the GPT-4V iterative refinement allows Idea2Img\nto generate images with better semantic and visual qualities,\nleading to an effective automatic image creation assistant.\n4.3. LMM Feedback, Revision, and Selection\nOne may wonder about the behavior and performance\nof LMM playing each role in Idea2Img\u2019s iterative self-\nrefinement pipeline, i.e., \u201cfeedback reflection,\u201d \u201crevised\nprompt generation,\u201d and \u201cdraft image selection.\u201d We show\ncorresponding qualitative results as follows.\nFeedback reflection. Figure 8 shows the text feedback gen-\nerated by GPT-4V for the user IDEA and the draft image\nand T2I prompt. Idea2Img can effectively check if the gen-\nerated image is correct, such as the number of oranges in\n(a) and the misspelled scene text \u201dALURE RESEACHE\u201d in\n(b). In addition to text descriptions in IDEA, Idea2Img can\nverify if the draft image corresponds to the visual descrip-\ntions in IDEA. This includes the color and breed of the dog\nin (e), the exact art style in (f), and the same cloth and pose\nin (g). Furthermore, Idea2Img can understand and verify\nthe IDEA containing interleaved image-text pairs, as shown\nin Figures 8(h,i).\nIn addition to identifying the discrepancy, Idea2Img also\n10\nFigure 8. Examples of the generated text feedback. The left column shows the multimodal input user IDEA, and the center column shows\nthe draft image to process as well as its corresponding text prompts. The right column shows the text feedback generated by GPT-4V. The\ndark blue color highlights the identified discrepancies.\n11\n(i)\n(h)\n(g)\n(f)\n(e)\n(d)\n(c)\n(b)\n(a)\nThe person in the image is not practicing yoga dolphin pose, \nbut in a different yoga pose. Because the prompt has already \nmention dolphin pose, the AI art model might not \nunderstand what is dolphin pose. To address this, the \nsentence prompt should be modified to specifically mention \nwhat is a dolphin pose: posture where one balances on the \nforearms with the legs lifted, forming an inverted V shape \nwith the body.\na person practicing yoga dolphin pose at \nbeach\nA person practicing yoga \ndolphin pose at a beach with \nclear blue water and white \nsand, during sunrise.\nA person on a sandy beach \npracticing yoga in the dolphin \npose, which involves being on \nboth forearms and toes with \nhips raised towards the \nceiling and facing the ground, \nas the waves gently crash, the \nsun sets in the background \ncasting a warm glow, and a \nserene atmosphere surrounds \nthe scene.\nThe dog in the generated image is not in the same color as the \none in the given image. The dog in the given image is a black \nand white Cavalier King Charles Spaniel, while the dog in the \ngenerated image is a brown and white Cavalier King Charles \nSpaniel. To address this issue, the sentence prompt should be \nmodified to specifically mention the breed of the dog as a \nblack and white Cavalier King Charles Spaniel.\nphoto of a dog looks like the left one in the \ngiven image running on the floor\nA Cavalier King Charles \nSpaniel running on a tiled \nfloor, with a happy mood, in \na modern kitchen.\nA black and white Cavalier \nKing Charles Spaniel running \non a wooden floor, with a \nplayful mood, in a cozy \nkitchen.\nRevised prompt and image\nCartoon \ndrawing of Mr \nBean with the \nsame cloth as \nin the given \nimage\nplaying \nwith a dog \nlooks like \nthis one \non the \nbeach\nThe dog in the generated image is not the same breed as the \none in the given image. The dog in the given image is a \nYorkshire Terrier, while the dog in the generated image is a \nBoston Terrier. To address this issue, the sentence prompt \nshould be modified to specifically mention the breed of the \ndog as a Yorkshire Terrier.\nA humorous scene of Mr \nBean and a small dog playing \nfetch on a sandy beach with \nblue skies in the background.\nA humorous scene of Mr \nBean wearing a yellow \ntennis outfit and playing \nwith a Yorkshire Terrier on a \nbeach with palm trees in the \nbackground.\nThe image generated by the AI art generation model does not \nfollow the user imagined IDEA of the scene as the fingers are \nnot ready to take a screenshot. To address this, the sentence \nprompt can be modified to specify that the fingers should be \npressing the side button and the volume up button at the \nsame time.\nan image of a hand holding an iphone 12. \nThe image is used for illustrating how to \ntake a screen shot on iphone\nA detailed image of an \niPhone 12 being held in a \nhand, with the screen on and \nthe fingers ready to take a \nscreenshot.\nA detailed image of a hand \nholding an iPhone 12, with \nthe fingers ready to take a \nscreenshot by pressing the \nside button and the volume \nup button at the same time, \nwith a clear view of the \nscreen.\nThe image generated by the AI art generation model does not \nfollow the user imagined IDEA of the scene as there is no \nstudy area in the image. To address this, the sentence prompt \ncan be modified to specify that there should be a study area \nin the room, such as a desk with a lamp and a bookshelf.\nA hand drawing of a room where people \ncan sleep and study. Hand drawing is a style \nthat looks like the given image.\nA room with a cozy \natmosphere, hand-drawn in \na style that resembles a pen \nand ink sketch.\nA room with a comfortable \nbed and a study area with a \ndesk and a bookshelf, \nhand-drawn in a style that \nlooks like a pen and ink \nsketch.\nTwo cats \nrunning on \nthe beach, \nthe right one \nlooks like\nand the \nleft one \nlooks like\nThe cats in the generated image are both Russian Blue cats, \nwhile the user imagined IDEA of the scene specifies that one \nof the cats should be a domestic shorthair. To address this \nissue, the sentence prompt should be modified to specifically \nmention that the domestic shorthair cat is orange in color.\nA playful scene of a Russian \nBlue cat and a domestic \nshorthair cat chasing each \nother on the beach, the sand \nis white and the sky is a deep \nblue, the sun is shining \nbrightly.\nA playful scene of a Russian \nBlue cat and an orange \ndomestic shorthair cat \nrunning on the white sandy \nbeach, the sky is a deep blue \nand the sun is shining \nbrightly.\nThe image generated by the AI art generation model does not \nfollow the user imagined IDEA of the scene as there are \nbananas on the plate and orange juice in the glass. To \naddress this, the sentence prompt can be modified to specify \nthat there should be no bananas on the plate and no orange \njuice in the glass.\na plate that has no bananas on it. there is a \nglass without orange juice next to it\nA breakfast scene with a \nplate that has no bananas \nand a glass without orange \njuice, with a blue \nbackground and a bright \nlighting.\nA close-up view of an empty \nplate with no other objects \non it, and an empty glass \nwithout orange juice on a \nwooden table, with a white \nbackground.\nInput user IDEA\nDraft prompt and image to revise\nText feedback\nA watercolor cartoon logo based on the \ndesign in the given image\nA logo design of a llama with \na red scarf and beanie, in a \nwatercolor cartoon style, \nwith a soft and pastel color \npalette.\nA logo design of a llama in a \nwatercolor cartoon style, \nwith a red beanie and scarf, \nstanding on all fours, with a \nsoft and pastel color palette, \nsimilar to the design in the \ngiven image.\nThe llama in the generated image is not in the same design as \nthe one in the given image. The llama in the given image is \nstanding on all fours, while the llama in the generated image \nis only showing its head. To address this issue, the sentence \nprompt should be modified to specifically mention that the \nllama should be standing on all fours, and the design should \nbe similar to the one in the given image.\nA scene of a man in a suit \nwaving his hand, with the \nsame style as the geometric \ndog image on the left.\nFind the the image style pattern in the left \ntwo dog images and apply it on the top \nright people waving hand image. Provide a \ntextual description that keeps the content \nin the people waving hand image, with the \ncorrect style pattern.\nA scene of a man in a suit \nwith his right hand raised in \na friendly wave with his \npalm facing forward, with \nthe same style as the \ngeometric dog image on the \nleft, with a blue sky \nbackground, with a slight \nsmile on his face, with a \nmodern and abstract style.\nThe man in the generated image is not the same as the one in \nthe given image. This is because the prompt mentions 'a man' \nbut the AI art model can not understand image inputs. To \naddress this, the sentence prompt should be modified to \nspecifically mention the man in the given image is 'a man in a \nsuit with his right hand raised in a friendly wave with his \npalm facing forward'.\nA person in a yoga dolphin \npose, posture where one \nbalances on the forearms \nwith the legs lifted, forming \nan inverted V shape with \nthe body, on a rocky beach \nwith crashing waves and a \ncloudy sky.\nA breakfast scene with an \nempty plate and a glass \nwithout orange juice, with a \nblue background and a \nbright lighting.\nFigure 9. Examples of the revised prompts. The four columns, from left to right, show the input user IDEA, the draft image to be\nrevised, generated text feedback, and the revised T2I prompt and image. The dark blue color highlights the identified discrepancies in text\nfeedback, and how they are addressed in the revised T2I prompt. We note that the example only shows a single round of self-refinement.\nTherefore, the revised T2I prompt may have remaining issues to be further addressed.\n12\n2 reason not \ncorrect\nFigure 10. Examples of the draft image selection. The right column shows justification ratings and the final selection. The dark blue\ncolor highlights the identified problems and justifications for the draft image selection. The selected draft image in the round is visualized\nwith the blue box shown in the middle column.\n13\npoints to the plausible directions that may improve the T2I\nprompt in the text feedback. For example, in Figure 8(c),\nGPT-4V mentions that \u201cthe person is not in yoga moun-\ntain pose, but the T2I prompt has already mentioned moun-\ntain pose,\u201d \u201cthe AI model might not understand what moun-\ntain pose is, and prompt should be modified to specifically\nmention what mountain pose is.\u201d Similarly, in Figure 8(d),\nIdea2Img provides guidance to have generated images bet-\nter follow the user intention of \u201can image for a children\u2019s\npainting competition,\u201d by \u201cspecifically mentioning that the\ncar should be simple and cartoon-like.\u201d\nRevised prompt generation. Figure 9 compares the T2I\nprompts before and after revision, and visualizes how text\nfeedback helps the revision. For example, (a) the revised\nT2I prompt includes a detailed description of the \u201cyoga dol-\nphin pose\u201d to generate the correct body pose; (b) the re-\nvised T2I prompt mentions \u201can empty plate with no other\nobjects\u201d to avoid the T2I model misunderstand the prompt\n\u201cno bananas;\u201d (c) T2I model generates the correct hand ges-\nture with Idea2Img providing text description on how to\ntake a screenshot. Idea2Img also effectively addresses the\nidentified errors in text feedback and improves the prompts\nfor multimodal input IDEA, including the dog color in Fig-\nure 9(d), the llama design in Figure 9(e), the study area in\nFigure 9(f), the human gesture in Figure 9(g), the dog breed\nand human clothing in Figure 9(h), and the color of the two\ncats in Figure 9(i).\nDraft image selection. Figure 10. T2I models may gen-\nerate low-quality images even with good T2I prompts. To\nenable refinement in each iteration, it is critical to reduce\nsuch generation noise by selecting from multiple draft im-\nages. Performing such selection requires GPT-4V to com-\npare multiple similar draft images and pick the one with the\nbest overall quality. Figure 10 shows the selection results\ngenerated by GPT-4V. The LMM prompt is designed such\nthat GPT-4V gives justifications and scores for each draft\nimage, in addition to the final selection index. Such inter-\nmediate thoughts not only help humans interpret the selec-\ntion process, but also serve as the chain of thought to im-\nprove the selection performance. We observe that GPT-4V\ncan compare different aspects mentioned in the IDEA and\ngive reasonable scores and selection index. For example,\nchecking the scene text spelling in Figure 10(a); verifying\nthe phone screen and model in Figure 10(b); counting the\nnumber of apples and bananas in Figure 10(c); verifying the\nball color and dog action in Figure 10(d); finding the image\nwith both sleep and study area in Figure 10(e); selecting the\nimage that best fits the given image style in Figure 10(f);\nverifying the image content and style in Figure 10(g); lo-\ncating the best blended image in Figure 10(h); and finding\nthe image with correct dog color and image style in Fig-\nure 10(i).\n5. Limitation and Discussion\nTasks beyond image generation. Idea2Img explores the\nemergent ability of multimodal self-refinement in LMM-\nbased systems, through the image design and generation\ntask. Specifically, Idea2Img views the T2I model to use\nas an unknown multimodal environment to explore, and it-\neratively refines T2I prompts to find its optimal usage. This\nconcept mirrors the intrinsic human approach of iterative\nproblem-solving when faced with unknown environments\nor complex challenges. We leave its extension to other in-\ntriguing tasks [48], e.g., GUI navigation, embodied agents,\nand complicated visual reasoning, for future exploration.\nFrom a single image generation model to multiple tools.\nIdea2Img explores the usage of a single image genera-\ntion model, including a text-to-image model [34] or a text-\nconditioned image-to-image model [24].\nWhen needed,\nother specialized generative models like ControlNet [56],\ninpainting [4], region-controlled T2I generation [21, 51],\ncustomized generation [8,35], and video generation [42,53]\ncan be seamlessly switched as replacements.\nThat is,\nIdea2Img could broadly boost different image generation\nmodels with diverse specialties by exploring their optimal\ntext description or instruction prompts.\nBeyond a single generation model, Idea2Img can also\nbe used to allocate multiple tools as in multimodal agent\nstudies [45,49]. In this case, Idea2Img isn\u2019t limited to opti-\nmizing the use of individual tools but also investigates their\neffective collaboration when used together, such as genera-\ntor selection and multi-step visual generation.\nConsolidating explored knowledge. We have shown the\neffectiveness of LMM iterative self-refinement in auto-\nmatic image design and generation. Going beyond merely\npreserving the probed T2I model\u2019s properties in memory,\nIdea2Img can also help to consolidate or distill the ex-\nplored knowledge into T2I model parameters, such that\nno inference-time iterative refinement is needed when en-\ncountering similar generation scenarios. For example, one\ncould collect a dataset using Idea2Img for a scenario of in-\nterest, and fine-tune a T2I model with the explored self-\nrefinement trajectory.\nStoring the probed knowledge as\nsample-agnostic prompt for each image generation model\nis another promising direction [14,32,57].\n6. Conclusion\nWe have presented Idea2Img, a multimodal iterative self-\nrefinement framework that leverages GPT-4V(ision) for im-\nage design and generation. Idea2Img delves into the emer-\ngent capabilities of iterative self-refinement in LMM-based\nsystems, showcasing its effectiveness in improving, assess-\ning, and verifying the generated multimodal content. The\nuser preference study demonstrates Idea2Img\u2019s capability in\nassisting humans to find the optimal usage of a T2I model\nfor automatic image design and generation.\n14\nAcknowledgment\nWe are deeply grateful to OpenAI for providing access to their\nexceptional tool [1,26\u201328]. We also extend heartfelt thanks to our\nMicrosoft colleagues for their insights, with special acknowledg-\nment to Faisal Ahmed, Ehsan Azarnasab, and Lin Liang for their\nconstructive feedback.\nReferences\n[1] Chatgpt can now see, hear, and speak. https://openai.\ncom/blog/chatgpt-can-now-see-hear-and-\nspeak, 2023. 3, 15\n[2] Midjourney. https://www.midjourney.com/, 2023.\n6\n[3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-\nOr, and Dani Lischinski.\nBreak-a-scene:\nExtracting\nmultiple concepts from a single image.\narXiv preprint\narXiv:2305.16311, 2023. 4\n[4] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18208\u201318218, 2022. 14\n[5] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and\nSergey Levine.\nTraining diffusion models with reinforce-\nment learning. arXiv preprint arXiv:2305.13301, 2023. 4\n[6] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n4\n[7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. arXiv\npreprint arXiv:2301.13826, 2023. 4\n[8] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui\nJia, Ming-Wei Chang, and William W Cohen. Subject-driven\ntext-to-image generation via apprenticeship learning. arXiv\npreprint arXiv:2304.00186, 2023. 4, 14\n[9] Xinyun Chen, Maxwell Lin, Nathanael Sch\u00a8arli, and Denny\nZhou. Teaching large language models to self-debug. arXiv\npreprint arXiv:2304.05128, 2023. 2\n[10] Ying\nFan,\nOlivia\nWatkins,\nYuqing\nDu,\nHao\nLiu,\nMoonkyung Ryu, Craig Boutilier, Pieter Abbeel, Moham-\nmad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok:\nReinforcement learning for fine-tuning text-to-image diffu-\nsion models. arXiv preprint arXiv:2305.16381, 2023. 4\n[11] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Ar-\njun Reddy Akula, Pradyumna Narayana, Sugato Basu,\nXin Eric Wang, and William Yang Wang.\nTraining-\nfree structured diffusion guidance for compositional text-to-\nimage synthesis. In The Eleventh International Conference\non Learning Representations, 2022. 4\n[12] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.\nA neural algorithm of artistic style.\narXiv preprint\narXiv:1508.06576, 2015. 4\n[13] Google. Bard. https://bard.google.com, 2023.\nAccessed: 2023-07-17. 2\n[14] Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu,\nDongyan Zhao, and Nan Duan. Learning to program with\nnatural language. arXiv preprint arXiv:2304.10464, 2023.\n3, 14\n[15] Tanmay Gupta and Aniruddha Kembhavi. Visual program-\nming: Compositional visual reasoning without training. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 14953\u201314962, 2023. 3\n[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-or. Prompt-to-prompt image\nediting with cross-attention control. In The Eleventh Inter-\nnational Conference on Learning Representations, 2022. 4\n[17] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6007\u20136017, 2023. 4\n[18] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 4\n[19] DeepFloyd Lab. Deepfloyd if. https://github.com/\ndeep-floyd/IF, 2023. 6, 9\n[20] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,\nLinjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foun-\ndation models: From specialists to general-purpose assis-\ntants. arXiv preprint arXiv:2309.10020, 2023. 3\n[21] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 22511\u201322521, 2023. 14\n[22] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei\nChang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.\nChameleon:\nPlug-and-play compositional reasoning with\nlarge language models.\narXiv preprint arXiv:2304.09842,\n2023. 3\n[23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hal-\nlinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\nShrimai Prabhumoye, Yiming Yang, et al.\nSelf-refine:\nIterative refinement with self-feedback.\narXiv preprint\narXiv:2303.17651, 2023. 2, 3\n[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. arXiv preprint arXiv:2108.01073, 2021. 4, 6, 8, 10,\n14\n[25] OpenAI. Dall\u00b7e 3 system card. https://cdn.openai.\ncom/papers/DALL_E_3_System_Card.pdf, 2023.\n6\n[26] OpenAI. Gpt-4 technical report, 2023. 2, 3, 4, 5, 15\n[27] OpenAI. Gpt-4v(ision) system card. 2023. 3, 5, 15\n[28] OpenAI. Gpt-4v(ision) technical work and authors. https:\n//cdn.openai.com/contributions/gpt- 4v.\npdf, 2023. 3, 15\n15\n[29] Liangming Pan,\nMichael Saxon,\nWenda Xu,\nDeepak\nNathani, Xinyi Wang, and William Yang Wang. Automat-\nically correcting large language models: Surveying the land-\nscape of diverse self-correction strategies.\narXiv preprint\narXiv:2308.03188, 2023. 3\n[30] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Han-\nnaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio\nRibeiro. Art: Automatic multi-step reasoning and tool-use\nfor large language models. arXiv preprint arXiv:2303.09014,\n2023. 3\n[31] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 1, 3, 4, 6, 7, 8\n[32] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang\nZhu, and Michael Zeng.\nAutomatic prompt optimization\nwith\u201d gradient descent\u201d and beam search.\narXiv preprint\narXiv:2305.03495, 2023. 3, 14\n[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 1, 4\n[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 1, 4, 6, 9, 14\n[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 4, 14\n[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022. 1, 4\n[37] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\ncedda, and Thomas Scialom. Toolformer: Language mod-\nels can teach themselves to use tools.\narXiv preprint\narXiv:2302.04761, 2023. 3\n[38] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang.\nHugginggpt: Solving\nai tasks with chatgpt and its friends in huggingface. arXiv\npreprint arXiv:2303.17580, 2023. 3\n[39] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023. 4\n[40] Noah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning, 2023.\n2, 3\n[41] Mohit Shridhar,\nXingdi Yuan,\nMarc-Alexandre C\u02c6ot\u00b4e,\nYonatan Bisk, Adam Trischler, and Matthew Hausknecht.\nAlfworld: Aligning text and embodied environments for in-\nteractive learning. arXiv preprint arXiv:2010.03768, 2020.\n3\n[42] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 14\n[43] D\u00b4\u0131dac Sur\u00b4\u0131s, Sachit Menon, and Carl Vondrick. Vipergpt:\nVisual inference via python execution for reasoning. arXiv\npreprint arXiv:2303.08128, 2023. 3\n[44] Zijie J Wang, Evan Montoya, David Munechika, Haoyang\nYang, Benjamin Hoover, and Duen Horng Chau.\nDiffu-\nsiondb: A large-scale prompt gallery dataset for text-to-\nimage generative models. arXiv preprint arXiv:2210.14896,\n2022. 1, 4\n[45] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,\nZecheng Tang, and Nan Duan.\nVisual chatgpt: Talking,\ndrawing and editing with visual foundation models. arXiv\npreprint arXiv:2303.04671, 2023. 3, 4, 14\n[46] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan,\nZicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A gen-\nerative region-to-text transformer for object understanding.\narXiv preprint arXiv:2212.00280, 2022. 4\n[47] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\nQuoc V Le, Denny Zhou, and Xinyun Chen. Large language\nmodels as optimizers.\narXiv preprint arXiv:2309.03409,\n2023. 3\n[48] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn\nof lmms: Preliminary explorations with gpt-4v (ision). arXiv\npreprint arXiv:2309.17421, 2023. 2, 3, 14\n[49] Zhengyuan Yang*, Linjie Li*, Jianfeng Wang*, Kevin Lin*,\nEhsan Azarnasab*, Faisal Ahmed*, Zicheng Liu, Ce Liu,\nMichael Zeng, and Lijuan Wang.\nMm-react: Prompting\nchatgpt for multimodal reasoning and action. arXiv preprint\narXiv:2303.11381, 2023. 3, 4, 14\n[50] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam W Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning.\nHotpotqa: A dataset for diverse, ex-\nplainable multi-hop question answering.\narXiv preprint\narXiv:1809.09600, 2018. 3\n[51] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin\nLin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael\nZeng, et al. Reco: Region-controlled text-to-image genera-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 14246\u201314255,\n2023. 14\n[52] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,\nKarthik Narasimhan, and Yuan Cao.\nReact: Synergizing\nreasoning and acting in language models.\narXiv preprint\narXiv:2210.03629, 2022. 3\n[53] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang,\nXiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li,\nShuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over\ndiffusion for extremely long video generation. arXiv preprint\narXiv:2303.12346, 2023. 14\n16\n[54] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. Transac-\ntions on Machine Learning Research, 2022. 1, 4\n[55] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. arXiv preprint arXiv:2308.02490, 2023. 3\n[56] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 4, 14\n[57] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin,\nYong-Jin Liu, and Gao Huang. Expel: Llm agents are ex-\nperiential learners. arXiv preprint arXiv:2308.10144, 2023.\n3, 14\n[58] Wanrong Zhu, Xinyi Wang, Yujie Lu, Tsu-Jui Fu, Xin Eric\nWang, Miguel Eckstein, and William Yang Wang. Collabo-\nrative generative ai: Integrating gpt-k for efficient editing in\ntext-to-image generation. arXiv preprint arXiv:2305.11317,\n2023. 1, 4\n17\n"
  },
  {
    "title": "GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors",
    "link": "https://arxiv.org/pdf/2310.08529.pdf",
    "upvote": "16",
    "text": "GaussianDreamer: Fast Generation from Text to 3D Gaussians\nby Bridging 2D and 3D Diffusion Models\nTaoran Yi1, Jiemin Fang2\u2020, Junjie Wang2, Guanjun Wu3, Lingxi Xie2,\nXiaopeng Zhang2, Wenyu Liu1, Qi Tian2 , Xinggang Wang1\u2020\u2021\n1School of EIC, Huazhong University of Science and Technology\n2Huawei Inc.\n3School of CS, Huazhong University of Science and Technology\n{taoranyi, guajuwu, liuwy, xgwang}@hust.edu.cn\n{jaminfong, is.wangjunjie, 198808xc, zxphistory}@gmail.com\ntian.qi1@huawei.com\nPrompt:Viking axe, fantasy, weapon, blender...\nInitialization\nDreamFusion\nMagic3D\nFantasia3D\nOurs\nHours\n10 seconds\n2 minutes\n7 minutes\n14 minutes\nTraining \nTime\n3D Gaussians with Temporal Variation\nTraining Time Comparison\nRendered Image\n0\n1\n2\n3\n4\n5\n6\n7\nFigure 1. We propose a simple yet efficient framework called GaussianDreamer. It bridges the 3D and 2D diffusion models via Gaussian\nsplatting, having both 3D consistency and rich generation details. Our method can complete training within 15 minutes on a single GPU\nand achieve real-time rendering.\nAbstract\nIn recent times, the generation of 3D assets from text\nprompts has shown impressive results. Both 2D and 3D dif-\nfusion models can help generate decent 3D objects based\non prompts.\n3D diffusion models have good 3D consis-\ntency, but their quality and generalization are limited as\ntrainable 3D data is expensive and hard to obtain. 2D dif-\nfusion models enjoy strong abilities of generalization and\nfine generation, but 3D consistency is hard to guarantee.\nThis paper attempts to bridge the power from the two types\nof diffusion models via the recent explicit and efficient 3D\nGaussian splatting representation. A fast 3D object gener-\nation framework, named as GaussianDreamer, is proposed,\nwhere the 3D diffusion model provides priors for initial-\nization and the 2D diffusion model enriches the geometry\nand appearance. Operations of noisy point growing and\ncolor perturbation are introduced to enhance the initialized\nGaussians. Our GaussianDreamer can generate a high-\n\u2020Project lead.\n\u2021Corresponding author.\nquality 3D instance or 3D avatar within 15 minutes on one\nGPU, much faster than previous methods, while the gener-\nated instances can be directly rendered in real time. Demos\nand code are available at https://taoranyi.com/\ngaussiandreamer/.\n1. Introduction\n3D asset generation has been an expensive and profes-\nsional work in conventional pipelines.\nRecently, diffu-\nsion models [57] have achieved great success in creat-\ning high-quality and realistic 2D images. Many research\nworks [2, 6, 9, 12, 14, 23, 31, 46, 50, 67, 70, 76, 77, 81, 90]\ntry to transfer the power of 2D diffusion models to the 3D\nfield for easing and assisting the process of 3D assets cre-\nation, e.g. the most common text-to-3D task.\nHere come two main streams for achieving this goal: (i)\ntraining a new diffusion model with 3D data [12, 14, 23, 46]\n(namely the 3D diffusion model) and (ii) lifting the 2D diffu-\nsion model to 3D [2, 6, 9, 31, 50, 57, 67, 70, 76, 77, 81, 90].\nThe former one is direct to implement and holds strong 3D\n1\narXiv:2310.08529v2  [cs.CV]  5 Dec 2023\nconsistency, but struggles to extend into a large generation\ndomain as 3D data is usually hard and expensive to ob-\ntain. The scale of current 3D datasets is far smaller than\n2D datasets. This results in the generated 3D assets falling\nshort in dealing with complex text prompts and producing\ncomplex/fine geometry and appearance. The latter bene-\nfits from the large data domain of the 2D diffusion models,\nwhich can handle various text prompts and produce highly\ndetailed and complex geometry and appearance. However,\nas 2D diffusion models are unaware of the camera view, the\ngenerated 3D assets are hard to form geometry consistency,\nespecially for structure-complicated instances.\nThis paper proposes to use recent 3D Gaussian Splat-\nting [24] to bridge the two aforementioned approaches, si-\nmultaneously having the geometry consistency from 3D dif-\nfusion models and rich details from 2D diffusion models.\n3D Gaussians are one type of efficient and explicit represen-\ntation, which intrinsically enjoys geometry priors due to the\npoint-cloud-like structure. Specifically, we use one of two\ntypes of 3D diffusion models: text-to-3D and text-to-motion\ndiffusion models, e.g. Shap-E [23] and MDM [72] in our\nimplementation, to generate a coarse 3D instance. Based\non the coarse 3D instance, a group of 3D Gaussians are ini-\ntialized. We introduce two operations of noisy point grow-\ning and color perturbation to supplement the initialized\nGaussians for follow-up enriching the 3D instance. Then\nthe 3D Gaussians can be improved and optimized by inter-\nacting with the 2D diffusion model via the Score Distillation\nSampling [50] (SDS) loss. Due to the geometry priors from\nboth the 3D diffusion model and 3D Gaussian Splatting it-\nself, the training process can be finished in a very short time.\nThe generated 3D asset can be rendered in real time with-\nout transformation into structures like mesh via the splatting\nprocess.\nOur contributions can be summarized as follows.\n\u2022 We propose a text-to-3D method, named as Gaussian-\nDreamer which bridges the 3D and 2D diffusion models\nvia Gaussian splitting, enjoying both 3D consistency and\nrich generation details.\n\u2022 Noisy point growing and color perturbation are intro-\nduced to supplement the initialized 3D Gaussians for fur-\nther content enrichment.\n\u2022 The overall method is simple and quite effective. A 3D\ninstance can be generated within 15 minutes on one GPU,\nmuch faster than previous methods, and can be directly\nrendered in real time.\n2. Related Works\n3D Pretrained Diffusion Models.\nRecently, text-to-3D\nasset generation using diffusion models has achieved great\nsuccess. Currently, it is mainly divided into lifting 2D dif-\nfusion models to 3D and 3D pretrained diffusion models,\nthe difference lies in whether the training data used is 2D\nor 3D. 3D pretrained diffusion models [12, 14, 23, 46], re-\nferred to as 3D diffusion models in our paper, are mod-\nels pretrained on text-3D pairs.\nAfter pretraining, they\ncan generate 3D assets through only inference, and mod-\nels such as Point-E [46] and Shape-E [23] can generate\n3D assets in minutes.\nIn addition to generating 3D as-\nsets from text, there are methods where 3D diffusion mod-\nels [1, 7, 10, 25, 53, 63, 72, 88, 89, 91] generate motion se-\nquences based on text-motion data. By pretraining on text-\nmotion pairs, these models can generate reasonable motion\nsequences for different texts.\nThe generated motion se-\nquences can be transformed into the SMPL (Skinned Multi-\nPerson Linear) model [38] based on mesh representation,\nbut texture information is not included.\nIn our method,\nwe can paint transformed SMPL by using different text\nprompts.\nLifting 2D Diffusion Models to 3D.\nIn text-to-3D as-\nset generation methods [8, 15, 19, 22, 39, 47, 48, 51, 56,\n61, 62, 68, 73, 80, 85, 87, 92], in addition to using 3D\npretrained diffusion models, lifting a 2D diffusion model\nto 3D is a training-free approach. Moreover, due to the\nabundance of 2D image data, this method produces assets\nwith higher diversity and fidelity. Some single-image-to-3D\nmethods [17, 32\u201335, 37, 52, 59, 65, 66, 71, 78, 79, 82, 83]\nalso employ similar ideas.\nDreamFusion [50] first pro-\nposes SDS (Score Distillation Sampling) method, which is\nto update the 3D representation model using the 2D diffu-\nsion model. [76] proposes the method of SJC (Score Jaco-\nbian Chaining) to lift the 2D diffusion model to 3D. Later\nmethods [6, 30, 31, 69, 77] build on DreamFusion and fur-\nther improve the quality of 3D generation. Among them,\ngenerated 3D assets may suffer from multi-face problems.\nTo address this issue, some methods strengthen the se-\nmantics of different views [2] and use multi-view informa-\ntion [67, 90] to alleviate such problems. There are also mod-\nels [16, 21, 28, 42, 44, 58, 75, 81] which adopt CLIP [55]\nto align each view of the 3D representation model with the\ntext.\n3D Representation Methods.\nIn recent times, neural ra-\ndiance fields (NeRF) [43] have achieved impressive results\nin 3D representation, and many methods in text-to-3D asset\ngeneration have also adopted NeRF or its variants [3, 45] as\nthe representation method. Some methods [6, 12, 30, 31]\nuse explicit optimizable mesh representation methods like\nDMTET [64] to reduce rendering costs and further im-\nprove resolution. In addition to that, there are also gener-\nation methods that utilize point clouds [40, 46, 51, 74] and\nmeshes [36] as 3D representations.\nRecently, 3D Gaussian Splatting [24] has been intro-\nduced as a representation method for 3D scenes, which\ncan achieve rendering effects comparable to NeRF-based\n2\n  Generated\nPoint Clouds\nInitialization with 3D Diffusion Model Priors\nGeneration\nPrompt:A fox\n  Initialized\n3D Gaussians\nSplatting\n  2D Diffusion\n      Models\nNoise\nUpdate\nFinal 3D Gaussian Splatting \nCost 7 Seconds\nOptimization with the 2D Diffusion Model\nCost 15 Minutes\nText-to-3D\nor\nText-to-Motion\n...\n3D Diffusion \n     Models\nNoisy Point \nGrowing\nColor Perturbation\nFigure 2. Overall framework of GaussianDreamer. Firstly, we utilize a 3D diffusion model to generate the initialized point clouds. After\nexecuting noisy point growing and color perturbation on the point clouds, we use them to initialize the 3D Gaussians. The initialized\n3D Gaussians are further optimized using the SDS method [50] with a 2D diffusion model. Finally, we render the image using the 3D\nGaussians by employing 3D Gaussian Splatting [24]. We can use one of various 3D diffusion models to generate the initialized point\nclouds. In this case, we take text-to-3D and text-to-motion diffusion models as examples.\nmethods and enable real-time rendering. Two concurrent\nworks [9, 70] also construct the 3D representation with 3D\nGaussian Splatting [24]. DreamGaussian [70] uses a sin-\ngle image as a condition to generate 3D assets, while GS-\nGEN [9] implements high-quality generation from text to\n3D. Our method shares a similar idea of using 3D Gaussian\nSplatting as the representation method, which significantly\nreduces the cost of improving resolution and achieving a\nmuch faster optimization speed compared to optimizable\nmesh representation methods. And we can generate detailed\n3D assets based on prompt texts in a very short time.\n3. Method\nIn this section, we first review 2D and 3D diffusion mod-\nels and the 3D representation method \u2013 3D Gaussian Splat-\nting [24]. We give an overview of the whole framework\nin Sec. 3.2. Then, in Sec. 3.3, we describe the process of\ninitializing the 3D Gaussians with the assistance of 3D dif-\nfusion models. The further optimization of 3D Gaussians\nusing the 2D diffusion model is described in Sec. 3.4.\n3.1. Preliminaries\nDreamFusion.\nDreamFusion [50] is one of the most rep-\nresentative methods to lift 2D diffusion models to 3D,\nwhich proposes to optimize the 3D representation with the\nscore distillation sampling (SDS) loss via a pre-trained 2D\ndiffusion model \u03d5. Specifically, it takes MipNeRF [3] as\nthe 3D representation method, whose parameters \u03b8 are op-\ntimized. Taking the rendering method as g, the rendered\nimage results in x = g(\u03b8).\nTo make the rendered im-\nage x similar to the samples obtained from the diffusion\nmodel \u03d5, DreamFusion uses a scoring estimation func-\ntion: \u02c6\u03f5\u03d5(zt; y, t), which predicts the sampled noise \u02c6\u03f5\u03d5 given\nthe noisy image zt, text embedding y, and noise level t.\nBy measuring the difference between the Gaussian noise \u03f5\nadded to the rendered image x and the predicted noise \u02c6\u03f5\u03d5,\nthis scoring estimation function can provide the direction\nfor updating the parameter \u03b8. The formula for computing\nthe gradient is as\n\u2207\u03b8LSDS(\u03d5, x = g(\u03b8)) \u225c Et,\u03f5\n\u0014\nw(t) (\u02c6\u03f5\u03d5(zt; y, t) \u2212 \u03f5) \u2202x\n\u2202\u03b8\n\u0015\n,\n(1)\nwhere w(t) is a weighting function.\n3D Gaussian Splatting.\n3D Gaussian Splatting [24] (3D-\nGS) is a recent groundbreaking method for novel-view syn-\nthesis.\nUnlike implicit representation methods such as\nNeRF [43], which renders images based on volume render-\ning, 3D-GS renders images through splatting [84], achiev-\ning real-time speed.\nSpecifically, 3D-GS represents the\nscene through a set of anisotropic Gaussians, defined with\nits center position \u00b5 \u2208 R3, covariance \u03a3 \u2208 R7, color\nc \u2208 R3, and opacity \u03b1 \u2208 R1. And the 3D Gaussians can be\nqueried as follows:\nG(x) = e\u2212 1\n2 (x)T \u03a3\u22121(x),\n(2)\nwhere x represents the distance between \u00b5 and the query\npoint. For computing the color of each pixel, it uses a typ-\nical neural point-based rendering [26, 27]. A ray r is cast\nfrom the center of the camera, and the color and density of\nthe 3D Gaussians that the ray intersects are computed along\nthe ray. The rendering process is as follows:\nC(r) =\nX\ni\u2208N\nci\u03c3i\ni\u22121\nY\nj=1\n(1 \u2212 \u03c3j),\n\u03c3i = \u03b1iG(xi),\n(3)\n3\nwhere N represents the number of sample points on the ray\nr, ci and \u03b1i denote the color and opacity of the i-th Gaus-\nsian, and xi is the distance between the point and the i-th\nGaussian.\n3.2. Overall Framework\nOur overall framework consists of two parts, initialization\nwith 3D diffusion model priors and optimization with the\n2D diffusion model, as shown in Fig. 2.\nFor initializa-\ntion with 3D diffusion model priors, we use the 3D diffu-\nsion models F3D, instantiated with the text-to-3D and text-\nto-motion diffusion models, to generate the triangle mesh\nm based on the text prompt y, which can be denoted as\nm = F3D(y). One set of generated point clouds is trans-\nformed from the mesh m. Then the 3D Gaussians \u03b8b are\ninitialized via the generated point clouds after noisy point\ngrowing and color perturbation. For better quality, we uti-\nlize the 2D diffusion model F2D to further optimize the ini-\ntialized 3D Gaussians \u03b8b via SDS [50] with prompts y, re-\nsulting in the final 3D Gaussians \u03b8f. The target instance can\nbe rendered in real time by splatting the generated Gaus-\nsians.\n3.3. Gaussian Initialization with 3D Diffusion\nModel Priors\nIn this section, we mainly discuss how to initialize the 3D\nGaussians with 3D diffusion model priors. First, we use the\n3D diffusion model F3D to generate 3D assets based on the\nprompts y. Then we convert the 3D assets into point clouds\nand use the transformed point clouds to initialize the 3D\nGaussians. We have employed two types of 3D diffusion\nmodels to generate 3D assets. Below, we explain how to\ninitialize the 3D Gaussians using each of these models.\n3.3.1\nText-to-3D Diffusion Model\nWhen using a text-based 3D generation model, generated\n3D assets employ multi-layer perceptrons (MLPs) to pre-\ndict SDF values and texture colors. To construct the trian-\ngle mesh m, we query the SDF values at vertices along a\nregular grid with the size of 1283 within the MLPs. Then\nwe query the texture colors at each vertex of m. We con-\nvert the vertices and colors of m into point clouds, denoted\nas ptm(pm, cm). pm \u2208 R3 refers to the position of the\npoint clouds, which equals to the vertice coordinated of m.\ncm \u2208 R3 refers to the color of the point clouds, which is the\nsame as the color of m. However, the obtained colors cm\nare relatively simple, and the positions pm are sparse.\nNoisy Point Growing and Color Perturbation.\nWe do\nnot use the generated point clouds ptm to directly initial-\nize the 3D Gaussians. To improve the quality of initializa-\ntion, we perform noisy point growing and color perturbation\nBefore Grow&Pertb.\nAfter Grow&Pertb.\nSurface\nBBox\nGrowing Point Clouds\nGenerated Point Clouds\nFigure 3. The process of noisy point growing and color pertur-\nbation. \u201cGrow&Pertb.\u201d denotes noisy point growing and color\nperturbation.\naround point clouds ptm. First, we compute the bound-\ning box (BBox) of the surface on ptm and then uniformly\ngrow point clouds ptr(pr, cr) within the BBox. pr and cr\nrepresent the positions and colors of ptr. To enable fast\nsearching, we construct a KDTree [4] Km using the posi-\ntions pm. Based on the distances between the positions pr\nand the nearest points found in the KDTree Km, we deter-\nmine which points to keep. In this process, we select points\nwithin the (normalized) distance of 0.01. For the noisy point\nclouds, we make their colors cr similar to cm, and also add\nsome perturbations:\ncr = cm + a,\n(4)\nwhere the values of a are randomly sampled between 0 and\n0.2. We merge the positions and colors of ptm and ptr to\nobtain the final point clouds.\npt(pf, cf) = (pm \u2295 pr, cm \u2295 cr),\n(5)\nwhere \u2295 is the concatenation operation. Fig. 3 illustrates\nthe process of noisy point growing and color perturbation.\nFinally, we initialize the positions \u00b5b and colors cb of the\n3D Gaussians \u03b8b(\u00b5b, cb, \u03a3b, \u03b1b) using both the positions pf\nand colors cf of final point clouds pt. The opacity \u03b1b of the\n3D Gaussians is initialized to 0.1, and the covariance \u03a3b is\ncalculated as the distance between the nearest two points.\nAlgorithm 1 shows the specific algorithm flowchart.\n3.3.2\nText-to-Motion Diffusion Model\nWe generate a sequence of human body motions using text\nand select a human pose that best matches the given text.\nWe then convert the keypoints of this human pose into\nthe SMPL model [38], which is represented by a triangle\nmesh m. We then convert the mesh m into point clouds\nptm(pm, cm), where the position pm of each point in the\npoint clouds corresponds to the vertices of m. As for the\ncolor cm of ptm, since the SMPL model used here does\nnot have textures, we randomly initialize cm. To move ptm\n4\nAlgorithm 1 The 3D Gaussian Initialization.\nptm(pm, cm): Point clouds generated from F3D.\nptr(pr, cr): Growing point clouds within the BBox.\npt(pf, cf): Point clouds used for initializing the 3D Gaus-\nsians.\n\u03b8b(\u00b5b, cb, \u03a3b, \u03b1b): Initialized 3D Gaussians.\nStage 1: Generate points in the BBox.\nKm \u2190 BuildKDTree(pm)\n\u25b7 KDTree\nBBox \u2190 pm\n\u25b7 Positions bounding box.\nLow, High \u2190 BBox.MinBound, BBox.MaxBound\n\u25b7 Boundary of bounding box.\npsu \u2190 Uniform(Low, High, size = (NumPoints, 3))\n\u25b7 Points in the BBox.\nStage 2: Keep the points that meet the distance require-\nment.\npr, cr = [ ], [ ]\nfor all pu in psu do\npun, i \u2190 Km.SearchNearest(pu)\n\u25b7 Nearest point and its index in Km.\nif |pun \u2212 pu| < 0.01 then\npr.append(pu)\ncr.append(cm[i] + 0.2\u00d7Random(size = 3))\n\u25b7 Color of the nearest point plus perturbation.\nend if\nend for\npf \u2190 pm \u2295 pr\ncf \u2190 cm \u2295 cr\nStage 3: Initialize the 3D Gaussians.\n\u00b5b, cb \u2190 pf, cf\n\u25b7 Positions and colors of the 3D Gaussians.\nD \u2190 \u00b5b\n\u25b7 Distance between the nearest two positions.\n\u03a3b, \u03b1b \u2190 D, 0.1\n\u25b7 Covariance and opacity of the 3D Gaussians.\nnear the origin, we calculate the center point pc \u2208 R3 of\npm and subtract the position of point clouds ptm from the\ncenter point pc.\npt(pf, cf) = ptm(pm \u2212 pc, cm),\n(6)\nFinally, we use the point clouds pt to initialize the 3D Gaus-\nsians, similar to what is described in Sec. 3.3.1. To improve\nthe generation of motion sequences, we simplify the text by\nretaining only the relevant parts related to the motion and\nadd a subject. For example, if the text prompt is \u201cIron man\nkicks with his left leg\u201d, we transform it into \u201cSomeone kicks\nwith the left leg\u201d when generating the motion sequences.\n3.4. Optimization with the 2D Diffusion Model\nTo enrich details and improve the quality of the 3D asset, we\noptimize the 3D Gaussians \u03b8b with a 2D diffusion model\nF2D after initializing them with 3D diffusion model pri-\nors. We employ the SDS (Score Distillation Sampling) loss\nto optimize the 3D Gaussians. First, we use the method\nof 3D Gaussian Splatting [24] to obtain the rendered im-\nage x = g(\u03b8i). Here, g represents the splatting rendering\nmethod as in Eq. 3. Then, we use Eq. 1 to calculate the\ngradients for updating the Gaussian parameters \u03b8i with the\n2D diffusion model F2D. After a short optimization period\nusing the 2D diffusion model F2D, the final generated 3D\ninstance \u03b8f achieves high quality and fidelity on top of the\n3D consistency provided by the 3D diffusion model F3D.\n4. Experiments\nIn this section, we first present the implementation details\nin Sec. 4.1 and quantitative comparisons in Sec. 4.2. Then,\nin Sec. 4.3, we showcase the visualization results of our\nmethod and compare them with other methods. In Sec. 4.4,\nwe conduct a series of ablation experiments to validate the\neffectiveness of our method. Finally, we discuss the limita-\ntions of our method.\n4.1. Implementation Details\nOur method is implemented in PyTorch [49], based on\nThreeStudio [13]. The 3D diffusion models used in our\nmethod are Shap-E [23] and MDM [72], and we load the\nShap-E model finetuned on Objaverse [11] in Cap3D [41].\nFor the 2D diffusion model, we use stabilityai/stable-\ndiffusion-2-1-base [57], with a guidance scale of 100. The\ntimestamps we use are uniformly sampled from 0.02 to 0.98\nbefore 500 iterations, and change to 0.02 to 0.55 after 500 it-\nerations. For the 3D Gaussians, the learning rates of opacity\n\u03b1 and position \u00b5 are 10\u22122 and 5 \u00d7 10\u22125. The color c of the\n3d Gaussians is represented by the sh coefficient, with the\ndegree set to 0 and the learning rate set to 1.25 \u00d7 10\u22122. The\ncovariance of the 3D Gaussians is converted into scaling\nand rotation for optimization, with learning rates of 10\u22123\nand 10\u22122, respectively. The radius of the camera we use for\nrendering is from 1.5 to 4.0, with the azimuth in the range\nof -180 to 180 degrees and the elevation in the range of -10\nto 60 degrees. The total training iterations are 1200. All\nour experiments can be completed within 15 minutes on a\nsingle RTX 3090 with a batch size of 4. The resolution we\nuse for rendering is 1024 \u00d7 1024, which is scaled to 512\n\u00d7 512 when optimizing using the 2D diffusion model. We\ncan render in real time at 512 \u00d7 512 resolution. And all our\ncode will be released.\n4.2. Quantitative Comparisons\nIn Tab. 1, we use CLIP [55] similarity to quantitatively eval-\nuate our method. The results of other methods in the table\ncome from the concurrent Instant3D [29] paper. The re-\nsults of Shap-E [23] come from the official source, while\nDreamFusion [50] and ProlificDreamer [77] results come\nfrom implementation by ThreeStudio [13]. The implemen-\ntation version of DreamFusion is shorter in time than the of-\n5\nGaussianDreamer (Ours)\nDreamFusion\n6.0 Hours\n5.3 Hours\n6.0 Hours\n15 Minutes\nMagic3D\nFantasia3D\nA plate piled high with chocolate chip cookies.\nA 3D model of an adorable cottage with a thatched roof.\nProlificDreamer\n~ Hours\nFigure 4. Qualitative comparisons between our method and DreamFusion [50], Magic3D [31], Fantasia3D [6] and ProlificDreamer [77].\nHere we count the GPU time in their papers. The time for DreamFusion is measured on TPUv4, Magic3D is measured on A100, Fantasia3D\nis measured on RTX 3090, and our method is measured on RTX 3090.\nFigure 5. More generated samples by our GaussianDreamer. Two views of each sample are shown.\nFigure 6. Results of generation with ground.\nficial report we mention in the main text. During the evalua-\ntion, we use a camera radius of 4, an elevation of 15 degrees,\nand select 120 evenly spaced azimuth angles from -180 to\n180 degrees, resulting in 120 rendered images from differ-\nent viewpoints. We follow the Instant3D settings, randomly\nselecting 10 from the 120 rendered images. We calculate\n6\nDreamFusion\nDreamFusion\nDreamAvatar\nDreamWaltz\nAvatarVerse\nGaussianDreamer (Ours)\n~2 Hours\n15 Minutes\n15\n~6 Hours\n~1 Hour\n~2 Hours\nFigure 7. Qualitative comparisons between our method and DreamFusion [50], DreamAvatar [5], DreamWaltz [18], and AvatarVerse [86].\nTable 1. Quantitative comparisons on CLIP [55] similarity with other methods.\nMethods\nViT-L/14 \u2191\nViT-bigG-14 \u2191\nGeneration Time \u2193\nShap-E [23]\n20.51\n32.21\n6 seconds\nDreamFusion [50]\n23.60\n37.46\n1.5 hours\nProlificDreamer [77]\n27.39\n42.98\n10 hours\nInstant3D [29]\n26.87\n41.77\n20 seconds\nOurs\n27.23 \u00b1 0.06\n41.88 \u00b1 0.04\n15 minutes\nthe similarity between each selected image and the text and\nthen compute the average for 10 selected images. It\u2019s worth\nnoting that when other methods are evaluated, 400 out of\nDreamFusion\u2019s 415 prompts are selected. This is because\nsome generations failed, so our method is disadvantaged\nduring evaluation on all 415 prompts from DreamFusion.\nWe use two models, ViT-L/14 from OpenAI [54] 1 and ViT-\nbigG-14 from OpenCLIP [20, 60] 2, to calculate CLIP sim-\nilarity. Our method is superior to all methods except Pro-\nlificDreamer, but it is 40 times faster than ProlificDreamer\nin generation speed.\n4.3. Visualization Results\nIn this section, we present the results of initializing the 3D\nGaussians with two different 3D diffusion models: text-to-\n3D and text-to-motion diffusion models.\n1https://huggingface.co/openai/clip-vit-large-patch14\n2https://github.com/mlfoundations/open clip\n4.3.1\nInitialization with Text-to-3D Diffusion Model\nWe show the comparison results with DreamFusion [50],\nMagic3D [31], Fantasia3D [6] and ProlificDreamer [77]\nin Fig. 4. In addition to our method, the figures of other\nmethods are downloaded from the paper of ProlificDreamer.\nWhen encountering prompts that involve the combination\nof multiple objects, such as the prompt \u201cA plate piled high\nwith chocolate chip cookies\u201d, the generated results from\nMagic3D, Fantasia3D, and ProlificDreamer do not include\na plate.\nIn contrast, our generated result can effectively\ncombine a plate and chocolate chip cookies. Furthermore,\ncompared to DreamFusion, the plate we generate has better\npatterns. Our method shows comparable quality while sav-\ning 21 \u2212 24 times the generation time compared to their\nmethods. Moreover, the 3D Gaussians generated by our\nmethod can directly achieve real-time rendering without\nfurther transformation into mesh-like structures. Fig. 5 vi-\nsualizes more samples generated by our GaussianDreamer\nfrom various prompts, which show good 3D consistency\nwhile having high-quality details.\n7\nFigure 8. More generated 3D avatars by our GaussianDreamer\ninitialized with the different poses of SMPL [38]. Here, the differ-\nent poses of SMPL are generated using a text-to-motion diffusion\nmodel.\nGeneration with Ground.\nWhen initializing, we add a\nlayer of point clouds representing the ground at the bottom\nof the generated point clouds. The color of the ground is\nrandomly initialized. Then, we use the point clouds with the\nadded ground to initialize the 3D Gaussians. Fig. 6 shows\nthe results of the final 3D Gaussian Splatting [24].\n4.3.2\nInitialization\nwith\nText-to-Motion\nDiffusion\nModel\nIn Fig. 7, we present the comparison results with DreamFu-\nsion [50], DreamAvatar [5], DreamWaltz [18], and Avatar-\nVerse [86]. In addition to our method, the figures of other\nmethods are downloaded from the paper of AvatarVerse. It\nis worth noting that our prompt is \u201cSpiderman/Stormtrooper\nstands with open arms\u201d, while the prompts for other meth-\nods are \u201cSpiderman/Stormtrooper\u201d. This is because when\ngenerating motion using text-to-motion diffusion model as\ninitialization, we require more specific action descriptions.\nOur method achieves a speedup of 4\u221224 times compared to\nother methods, while maintaining comparable quality. Ad-\nditionally, our method allows for generating 3D avatars with\nspecified body poses. In Fig. 8, we provide more results\ngenerated with different human body poses. We first gener-\nate a sequence of motions that match the text prompt using\na text-to-motion 3d diffusion model, and then initialize the\n3d Gaussians with the SMPL of one selected pose in mo-\ntions. Our method can generate 3D avatars in any desired\npose.\n4.4. Ablation Study and Analysis\nThe Role of Initialization.\nAs shown in Fig. 9, we first\nconduct an ablation experiment on the initialization of the\n3D Gaussians to validate that initialization can improve 3D\nconsistency.\nThe first column is the rendering result of\nShap-E [23] with NeRF [43] as the 3D representation. The\nsecond column is the result of optimizing the 3D Gaussians\nrandomly initialized within a cube using the SDS loss, and\nthe third column is our method. We show the initialization\neffects on 3 samples. In the first and second rows, Shap-\nE has good generation results while our method provides\nmore complex geometries and more realistic appearances.\nCompared with random initialization, in the first row, de-\ntails of our method are better. In the second row, the 3D as-\nsets generated by random initialization have the multi-head\nproblem, which does not occur in our method. The initial-\nization of the 3D diffusion model can avoid unreasonable\ngeometry. In the third row, the generation result of Shap-E\nis far different from the given text prompt while our method\nmakes the 3D assets closer to the prompt through the 2D\ndiffusion model. Our method can expand the domain of\nShap-E prompts, allowing for the generation of 3D assets\nbased on a wider range of prompts.\nNoisy Point Growing and Color Perturbation.\nFig. 10\nillustrates the ablation results of noisy point growing and\ncolor perturbation. With noisy point growing and color per-\nturbation applied, the first row showcases improved details\nin the sniper rifle. Additionally, the second column gen-\nerates an amigurumi motorcycle that better aligns with the\nstyle characteristics of amigurumi mentioned in the prompt,\ncompared to the case without noisy point growing and color\nperturbation.\nInitialization with Different Text-to-3D Diffusion Mod-\nels.\nWe select two text-to-3D generation models, Shap-\nE [23] and Point-E [46], to validate the effectiveness of\nour framework. We load the Point-E model finetuned on\nObjaverse [11] in Cap3D[41]. Fig. 11, we showcase the\ngenerated results after initializing the 3D Gaussians using\none of two text-to-3D generation models. It can be seen\nthat both initializations yield good generation results. How-\never, considering that Shap-E generates 3D assets based on\nNeRF and SDF, which provide higher fidelity compared to\nthe point cloud representation used by Point-E, the geome-\ntry of the airplane in the first row of Fig. 11 appears better\nwhen initialized using Shap-E.\n4.5. Limitations\nThe edges of the 3D assets generated by our method are not\nalways sharp, and there may be unnecessary 3D Gaussians\naround the object surface. How to filter these point clouds\n8\nFigure 9. Ablation studies of the initialization of the 3D Gaussians. The Shap-E [23] rendering resolution here is 256x256.\nFigure 10. Ablation studies of noisy point growing and color per-\nturbation. \u201cGrow&Pertb.\u201d denotes noisy point growing and color\nperturbation.\nFigure 11. Ablation studies of initialization with different text-to-\n3D diffusion models: Point-E [46] and Shap-E [23].\nwill be a possible direction for improvement. Our approach\nutilizes 3D diffusion model priors, which greatly alleviates\nthe issue of multi-face problems. However, there is still a\nsmall chance of encountering the multi-face problems in\nscenes where there is minimal geometric difference but sig-\nnificant appearance discrepancy between the front and back\nof objects such as a backpack. Utilizing 3D-aware diffu-\nsion models [34, 67, 90] may be able to solve this problem.\nAdditionally, our method has limited effectiveness in gen-\nerating large-scale scenes, such as indoor scenes.\n5. Conclusion\nWe propose a fast text-to-3D method GaussianDreamer by\nbridging the abilities of 3D and 2D diffusion models via\nthe Gaussian splatting representation.\nGaussianDreamer\ncan generate detailed and realistic geometry and appearance\nwhile maintaining 3D consistency. The 3D diffusion model\npriors and geometry priors from the 3D Gaussians effec-\ntively promote the convergence speed. Each sample can be\ngenerated within 15 minutes on one GPU. We believe the\napproach of bridging 3D and 2D diffusion models could be\na promising direction to generate 3D assets efficiently.\nReferences\n[1] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip:\nGesture diffusion model with clip latents.\narXiv preprint\narXiv:2303.14613, 2023. 2\n[2] Mohammadreza\nArmandpour,\nHuangjie\nZheng,\nAli\nSadeghian, Amir Sadeghian, and Mingyuan Zhou.\nRe-\nimagine the negative prompt algorithm:\nTransform 2d\ndiffusion into 3d, alleviate janus problem and beyond. arXiv\npreprint arXiv:2304.04968, 2023. 1, 2\n[3] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In ICCV, pages 5855\u20135864, 2021. 2, 3\n[4] Jon Louis Bentley.\nMultidimensional binary search trees\n9\nused for associative searching. Communications of the ACM,\n18(9):509\u2013517, 1975. 4\n[5] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K Wong. Dreamavatar: Text-and-shape guided 3d hu-\nman avatar generation via diffusion models. arXiv preprint\narXiv:2304.00916, 2023. 7, 8\n[6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 1, 2, 6, 7\n[7] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao\nChen, and Gang Yu.\nExecuting your commands via mo-\ntion diffusion in latent space. In CVPR, pages 18000\u201318010,\n2023. 2\n[8] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,\nGang Yu, Lei Yang, and Guosheng Lin. It3d: Improved text-\nto-3d generation with explicit view synthesis. arXiv preprint\narXiv:2308.11473, 2023. 2\n[9] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using\ngaussian splatting. arXiv preprint arXiv:2309.16585, 2023.\n1, 3\n[10] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav\nGolyanik, and Christian Theobalt. Mofusion: A framework\nfor denoising-diffusion-based motion synthesis. In CVPR,\npages 9760\u20139770, 2023. 2\n[11] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In CVPR, pages 13142\u2013\n13153, 2023. 5, 8\n[12] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images.\nNeurIPS, 35:31841\u2013\n31854, 2022. 1, 2\n[13] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-\nXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestudio: A unified framework for 3d content generation.\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 5\n[14] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O\u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration. arXiv preprint arXiv:2303.05371, 2023. 1, 2\n[15] Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang\nDeng, Yi-Zhe Song, Tao Xiang, and Kwan-Yee K Wong.\nHeadsculpt: Crafting 3d head avatars with text. In NeurIPS,\n2023. 2\n[16] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. arXiv preprint\narXiv:2205.08535, 2022. 2\n[17] Shoukang Hu, Fangzhou Hong, Tao Hu, Liang Pan, Haiyi\nMei, Weiye Xiao, Lei Yang, and Ziwei Liu.\nHumanliff:\nLayer-wise 3d human generation with diffusion model. arXiv\npreprint arXiv:2308.09712, 2023. 2\n[18] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao\nQi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang. Dreamwaltz:\nMake a scene with complex 3d animatable avatars. arXiv\npreprint arXiv:2305.12529, 2023. 7, 8\n[19] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Ji-\naxiang Tang, Deng Cai, and Justus Thies. Tech: Text-guided\nreconstruction of lifelike clothed humans.\narXiv preprint\narXiv:2308.08545, 2023. 2\n[20] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. If you use this software, please cite it as below.\n7\n[21] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. In CVPR, pages 867\u2013876, 2022.\n2\n[22] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,\nMingming He, Dongdong Chen, and Jing Liao.\nAvatar-\ncraft: Transforming text into neural human avatars with\nparameterized shape and pose control.\narXiv preprint\narXiv:2303.17606, 2023. 2\n[23] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 1, 2, 5, 7, 8, 9, 13\n[24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics, 42\n(4), 2023. 2, 3, 5, 8\n[25] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-\nform language-based motion synthesis & editing. In AAAI,\npages 8255\u20138263, 2023. 2\n[26] Georgios Kopanas, Julien Philip, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis.\nPoint-based neural rendering with per-\nview optimization. In Computer Graphics Forum, pages 29\u2013\n43. Wiley Online Library, 2021. 3\n[27] Georgios Kopanas, Thomas Leimk\u00a8uhler, Gilles Rainer,\nCl\u00b4ement Jambon, and George Drettakis. Neural point cata-\ncaustics for novel-view synthesis of reflections. ACM Trans-\nactions on Graphics (TOG), 41(6):1\u201315, 2022. 3\n[28] Jiabao Lei, Yabin Zhang, Kui Jia, et al. Tango: Text-driven\nphotorealistic and robust 3d stylization via lighting decom-\nposition. NeurIPS, 35:30923\u201330936, 2022. 2\n[29] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun\nLuan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg\nShakhnarovich, and Sai Bi.\nInstant3d:\nFast text-to-3d\nwith sparse-view generation and large reconstruction model.\narXiv preprint arXiv:2311.06214, 2023. 5, 7\n[30] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweet-\ndreamer: Aligning geometric priors in 2d diffusion for con-\nsistent text-to-3d. arXiv preprint arXiv:2310.02596, 2023.\n2\n[31] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In CVPR, pages 300\u2013309, 2023.\n1, 2, 6, 7\n[32] Yukang Lin, Haonan Han, Chaoqun Gong, Zunnan Xu,\nYachao Zhang, and Xiu Li. Consistent123: One image to\n10\nhighly consistent 3d asset using case-aware diffusion priors.\narXiv preprint arXiv:2309.17261, 2023. 2\n[33] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang\nXu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh\nin 45 seconds without per-shape optimization. arXiv preprint\narXiv:2306.16928, 2023.\n[34] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-\n3:\nZero-shot one image to 3d object.\narXiv preprint\narXiv:2303.11328, 2023. 9\n[35] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023. 2\n[36] Zhen\nLiu,\nYao\nFeng,\nMichael\nJ\nBlack,\nDerek\nNowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdif-\nfusion: Score-based generative 3d mesh modeling.\narXiv\npreprint arXiv:2303.08133, 2023. 2\n[37] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, et al. Wonder3d: Sin-\ngle image to 3d using cross-domain diffusion. arXiv preprint\narXiv:2310.15008, 2023. 2\n[38] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. Smpl: A skinned multi-\nperson linear model. In Seminal Graphics Papers: Pushing\nthe Boundaries, Volume 2, pages 851\u2013866. 2023. 2, 4, 8\n[39] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan\nLin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin,\nMing-Yu Liu, Sanja Fidler, and James Lucas.\nAtt3d:\nAmortized text-to-3d object synthesis.\narXiv preprint\narXiv:2306.07349, 2023. 2\n[40] Shitong Luo and Wei Hu.\nDiffusion probabilistic models\nfor 3d point cloud generation. In CVPR, pages 2837\u20132845,\n2021. 2\n[41] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-\nson. Scalable 3d captioning with pretrained models. arXiv\npreprint arXiv:2306.07279, 2023. 5, 8\n[42] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. In CVPR, pages 13492\u201313502, 2022. 2\n[43] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, pages 405\u2013421, 2020. 2, 3, 8\n[44] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 conference papers, pages 1\u20138, 2022. 2\n[45] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, 2022. 2\n[46] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 1, 2, 8, 9\n[47] Yichen Ouyang, Wenhao Chai, Jiayi Ye, Dapeng Tao, Yib-\ning Zhan, and Gaoang Wang.\nChasing consistency in\ntext-to-3d generation from a single image. arXiv preprint\narXiv:2309.03599, 2023. 2\n[48] Jangho Park, Gihyun Kwon, and Jong Chul Ye.\nEd-nerf:\nEfficient text-guided editing of 3d scene using latent space\nnerf. arXiv preprint arXiv:2310.02712, 2023. 2\n[49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zem-\ning Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:\nAn imperative style, high-performance deep learning library.\nNeurIPS, 32, 2019. 5\n[50] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 1, 2, 3, 4, 5, 6, 7, 8\n[51] Zekun Qi, Muzhou Yu, Runpei Dong, and Kaisheng Ma.\nVpp: Efficient conditional 3d generation via voxel-point pro-\ngressive representation. arXiv preprint arXiv:2307.16605,\n2023. 2\n[52] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3d object generation using both\n2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843,\n2023. 2\n[53] Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H\nBermano, and Daniel Cohen-Or. Single motion diffusion.\narXiv preprint arXiv:2302.05905, 2023. 2\n[54] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and\nIlya Sutskever. Learning transferable visual models from nat-\nural language supervision. In ICML, 2021. 7\n[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748\u20138763. PMLR, 2021. 2, 5, 7\n[56] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-\nman, Michael Rubinstein, Jonathan Barron, et al. Dream-\nbooth3d:\nSubject-driven text-to-3d generation.\narXiv\npreprint arXiv:2303.13508, 2023. 2\n[57] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 1, 5, 13\n[58] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,\nChin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-\nshan. Clip-forge: Towards zero-shot text-to-shape genera-\ntion. In CVPR, pages 18603\u201318613, 2022. 2\n[59] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann,\nHong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry La-\ngun, Li Fei-Fei, Deqing Sun, et al.\nZeronvs: Zero-shot\n360-degree view synthesis from a single real image. arXiv\npreprint arXiv:2310.17994, 2023. 2\n[60] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade W Gordon, Ross Wightman, Mehdi Cherti, Theo\n11\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev.\nLAION-5b: An open large-scale dataset for train-\ning next generation image-text models. In Thirty-sixth Con-\nference on Neural Information Processing Systems Datasets\nand Benchmarks Track, 2022. 7\n[61] Hoigi Seo, Hayeon Kim, Gwanghyun Kim, and Se Young\nChun.\nDitto-nerf: Diffusion-based iterative text to omni-\ndirectional 3d model.\narXiv preprint arXiv:2304.02827,\n2023. 2\n[62] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\nand Seungryong Kim.\nLet 2d diffusion model know 3d-\nconsistency for robust text-to-3d generation. arXiv preprint\narXiv:2303.07937, 2023. 2\n[63] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H\nBermano.\nHuman motion diffusion as a generative prior.\narXiv preprint arXiv:2303.01418, 2023. 2\n[64] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid represen-\ntation for high-resolution 3d shape synthesis. NeurIPS, 34:\n6087\u20136101, 2021. 2\n[65] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu. Zero123++: a single image to consistent multi-view dif-\nfusion base model. arXiv preprint arXiv:2310.15110, 2023.\n2\n[66] Yukai Shi, Jianan Wang, He Cao, Boshi Tang, Xianbiao\nQi, Tianyu Yang, Yukun Huang, Shilong Liu, Lei Zhang,\nand Heung-Yeung Shum.\nToss: High-quality text-guided\nnovel view synthesis from a single image. arXiv preprint\narXiv:2310.10644, 2023. 2\n[67] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 1, 2, 9\n[68] Liangchen Song, Liangliang Cao, Hongyu Xu, Kai Kang,\nFeng Tang, Junsong Yuan, and Yang Zhao. Roomdreamer:\nText-driven 3d indoor scene synthesis with coherent geome-\ntry and texture. arXiv preprint arXiv:2305.11337, 2023. 2\n[69] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen\nLiu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchi-\ncal 3d generation with bootstrapped diffusion prior. arXiv\npreprint arXiv:2310.16818, 2023. 2\n[70] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 1, 3\n[71] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 2\n[72] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,\nDaniel Cohen-Or, and Amit H Bermano. Human motion dif-\nfusion model. arXiv preprint arXiv:2209.14916, 2022. 2,\n5\n[73] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts.\narXiv\npreprint arXiv:2304.12439, 2023. 2\n[74] Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany,\nSanja Fidler, Karsten Kreis, et al. Lion: Latent point dif-\nfusion models for 3d shape generation. NeurIPS, 35:10021\u2013\n10039, 2022. 2\n[75] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao. Clip-nerf: Text-and-image driven manipula-\ntion of neural radiance fields. In CVPR, pages 3835\u20133844,\n2022. 2\n[76] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In CVPR,\npages 12619\u201312629, 2023. 1, 2\n[77] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongx-\nuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity\nand diverse text-to-3d generation with variational score dis-\ntillation. arXiv preprint arXiv:2305.16213, 2023. 1, 2, 5, 6,\n7\n[78] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong\nZhang, CL Chen, and Lei Zhang. Consistent123: Improve\nconsistency for one image to 3d object synthesis.\narXiv\npreprint arXiv:2310.08092, 2023. 2\n[79] Zhenzhen Weng, Zeyu Wang, and Serena Yeung. Zeroavatar:\nZero-shot 3d avatar generation from a single image. arXiv\npreprint arXiv:2305.16411, 2023. 2\n[80] Jinbo Wu, Xiaobo Gao, Xing Liu, Zhengyang Shen, Chen\nZhao, Haocheng Feng, Jingtuo Liu, and Errui Ding.\nHd-\nfusion: Detailed text-to-3d generation leveraging multiple\nnoise estimation. arXiv preprint arXiv:2307.16183, 2023.\n2\n[81] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot\ntext-to-3d synthesis using 3d shape prior and text-to-image\ndiffusion models. In CVPR, pages 20908\u201320918, 2023. 1, 2\n[82] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hong-\ndong Li. Consistnet: Enforcing 3d consistency for multi-\nview images diffusion.\narXiv preprint arXiv:2310.10343,\n2023. 2\n[83] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng\nWang. Consistent-1-to-3: Consistent image to 3d view syn-\nthesis via geometry-aware diffusion models. arXiv preprint\narXiv:2310.03020, 2023. 2\n[84] Wang Yifan, Felice Serena, Shihao Wu, Cengiz \u00a8Oztireli,\nand Olga Sorkine-Hornung. Differentiable surface splatting\nfor point-based geometry processing. ACM Transactions on\nGraphics (TOG), 38(6):1\u201314, 2019. 3\n[85] Chaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin\nWang, and Fan Wang. Points-to-3d: Bridging the gap be-\ntween sparse points and shape-controllable text-to-3d gener-\nation. In ACM MM, pages 6841\u20136850, 2023. 2\n[86] Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu\nWang, Li Chen, Chao Long, Feida Zhu, Kang Du, and Min\nZheng. Avatarverse: High-quality & stable 3d avatar cre-\nation from text and pose. arXiv preprint arXiv:2308.03610,\n2023. 7, 8\n12\n[87] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang,\nCheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and\nJingyi Yu.\nDreamface:\nProgressive generation of ani-\nmatable 3d faces under text guidance.\narXiv preprint\narXiv:2304.03117, 2023. 2\n[88] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai,\nFangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu.\nRemodiffuse: Retrieval-augmented motion diffusion model.\narXiv preprint arXiv:2304.01116, 2023. 2\n[89] Mengyi Zhao, Mengyuan Liu, Bin Ren, Shuling Dai, and\nNicu Sebe. Modiff: Action-conditioned 3d motion gener-\nation with denoising diffusion probabilistic models. arXiv\npreprint arXiv:2301.03949, 2023. 2\n[90] Minda Zhao,\nChaoyi Zhao,\nXinyue Liang,\nLincheng\nLi, Zeng Zhao, Zhipeng Hu, Changjie Fan, and Xin\nYu.\nEfficientdreamer:\nHigh-fidelity and robust 3d cre-\nation via orthogonal-view diffusion prior.\narXiv preprint\narXiv:2308.13223, 2023. 1, 2, 9\n[91] Zixiang Zhou and Baoyuan Wang. Ude: A unified driving\nengine for human motion generation. In CVPR, pages 5632\u2013\n5641, 2023. 2\n[92] Joseph Zhu and Peiye Zhuang.\nHifa: High-fidelity text-\nto-3d with advanced diffusion guidance.\narXiv preprint\narXiv:2305.18766, 2023. 2\nA. Appendix\nA.1. More Results\nGeneration with More Fine-grained Prompts.\nMore re-\nfined prompts are used to generate 3D assets, as shown in\nFig. 12. It can be seen that Shap-E [23] generates similar\nresults when given different descriptions of the word \u201daxe\u201d\nin the prompt. However, our method produces 3D assets\nthat better match the prompt.\nDiversity.\nIn Fig. 13, we demonstrate the diversity of our\nmethod in generating 3D assets by using different random\nseeds for the same prompt.\nA.2. More Ablation Studies\n2D Diffusion Model\nDuring the process of optimiz-\ning 3D Gaussians with a 2D diffusion model, we per-\nform ablation on the 2D diffusion models we use,\nspecifically stabilityai/stable-diffusion-2-1-base [57] 3 and\nDeepFloyd/IF-I-XL-v1.0 4.\nFig. 14 shows the results of\nthe ablation experiment, where it can be seen that the 3D\nassets generated using the stabilityai/stable-diffusion-2-1-\nbase have richer details.\n3https://huggingface.co/stabilityai/stable-diffusion-2-1-base\n4https://huggingface.co/DeepFloyd/IF-I-XL-v1.0\nFigure 12. Results of generation with more fine-grained prompts.\n13\nFigure 13. Results of the diversity of our method.\nFigure 14. Ablation studies of optimizing 3D Gaussians with dif-\nferent 2D diffusion models.\n14\n"
  },
  {
    "title": "HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion",
    "link": "https://arxiv.org/pdf/2310.08579.pdf",
    "upvote": "14",
    "text": "Published as a conference paper at ICLR 2024\nHYPERHUMAN: HYPER-REALISTIC HUMAN GENERA-\nTION WITH LATENT STRUCTURAL DIFFUSION\nXian Liu1,2\u2217\nJian Ren1\u2020\nAliaksandr Siarohin1\nIvan Skorokhodov1\nYanyu Li1\nDahua Lin2\nXihui Liu3\nZiwei Liu4\nSergey Tulyakov1\n1Snap Inc.\n2CUHK\n3HKU\n4NTU\nProject Page: https://snap-research.github.io/HyperHuman\nABSTRACT\nDespite significant advances in large-scale text-to-image models, achieving hyper-\nrealistic human image generation remains a desirable yet unsolved task. Existing\nmodels like Stable Diffusion and DALL\u00b7E 2 tend to generate human images with\nincoherent parts or unnatural poses. To tackle these challenges, our key insight\nis that human image is inherently structural over multiple granularities, from the\ncoarse-level body skeleton to the fine-grained spatial geometry. Therefore, captur-\ning such correlations between the explicit appearance and latent structure in one\nmodel is essential to generate coherent and natural human images. To this end, we\npropose a unified framework, HyperHuman, that generates in-the-wild human\nimages of high realism and diverse layouts. Specifically, 1) we first build a large-\nscale human-centric dataset, named HumanVerse, which consists of 340M images\nwith comprehensive annotations like human pose, depth, and surface-normal. 2)\nNext, we propose a Latent Structural Diffusion Model that simultaneously de-\nnoises the depth and surface-normal along with the synthesized RGB image. Our\nmodel enforces the joint learning of image appearance, spatial relationship, and\ngeometry in a unified network, where each branch in the model complements to\neach other with both structural awareness and textural richness. 3) Finally, to\nfurther boost the visual quality, we propose a Structure-Guided Refiner to com-\npose the predicted conditions for more detailed generation of higher resolution.\nExtensive experiments demonstrate that our framework yields the state-of-the-art\nperformance, generating hyper-realistic human images under diverse scenarios.\n1\nINTRODUCTION\nGenerating hyper-realistic human images from user conditions, e.g., text and pose, is of great impor-\ntance to various applications, such as image animation (Liu et al., 2019) and virtual try-on (Wang\net al., 2018). To this end, many efforts explore the task of controllable human image generation.\nEarly methods either resort to variational auto-encoders (VAEs) in a reconstruction manner (Ren\net al., 2020), or improve the realism by generative adversarial networks (GANs) (Siarohin et al.,\n2019). Though some of them create high-quality images (Zhang et al., 2022; Jiang et al., 2022),\nthe unstable training and limited model capacity confine them to small datasets of low diversity.\nRecent emergence of diffusion models (DMs) (Ho et al., 2020) has set a new paradigm for realistic\nsynthesis and become the predominant architecture in Generative AI (Dhariwal & Nichol, 2021).\nNevertheless, the exemplar text-to-image (T2I) models like Stable Diffusion (Rombach et al., 2022)\nand DALL\u00b7E 2 (Ramesh et al., 2022) still struggle to create human images with coherent anatomy,\ne.g., arms and legs, and natural poses. The main reason lies in that human is articulated with non-\nrigid deformations, requiring structural information that can hardly be depicted by text prompts.\nTo enable structural control for image generation, recent works like ControlNet (Zhang & Agrawala,\n2023) and T2I-Adapter (Mou et al., 2023) introduce a learnable branch to modulate the pre-trained\nDMs, e.g., Stable Diffusion, in a plug-and-play manner. However, these approaches suffer from the\n\u2217Work done during an internship at Snap Inc. Email: alvinliu@ie.cuhk.edu.hk\n\u2020Corresponding author: jren@snapchat.com.\n1\narXiv:2310.08579v2  [cs.CV]  15 Mar 2024\nPublished as a conference paper at ICLR 2024\nA man sitting down with a brown teddy bear on his shoulders.\nA woman poses with avocado sandwich lunch \nat an outdoor restaurant.\nYoung man standing near a lake with \na snow capped mountain behind.\nMiddle-age dark-hair woman on telephone with a teddy bear beside her.\nTwo women holding surfboards while smiling at the camera.\nAn elderly woman looks to the side as \nshe sits in front of a cheese pizza in a restaurant.\nLittle girl posing for the camera with an adult sized striped tie on.\nA woman sits smoking at table in a mostly blue kitchen.\nA guy in a brown jacket standing near a sign \nholding a cellphone to his ear.\nA picture of a man with suit, tie and wild hair.\nA person standing on a snowboard with knees bent and arms \nout on the snow with trees and snowy hills behind.\nA man with glasses, earrings, and a blue shirt with red tie.\nA man riding skis down the side\n of a snow covered ski slope.\nA pedestrian walks down the \nsnowy street with an umbrella.\n(a) Ours w/ Joint Denoising\n(b) Ours - Full\n(c) ControlNet\n(d) T2I-Adapter\n(e) HumanSD\n(f) SDXL w/ Refiner\nA man riding on top of a \nbrown horse while wearing a hat.\nFigure 1: Example Results and Visual Comparison. Top: The proposed HyperHuman simulta-\nneously generates the coarse RGB, depth, normal, and high-resolution images conditioned on text\nand skeleton. Both photo-realistic images and stylistic renderings can be created. Bottom: We com-\npare with recent T2I models, showing better realism, quality, diversity, and controllability. Note that\nin each 2\u00d72 grid (left), the upper-left is input skeleton, while the others are jointly denoised normal,\ndepth, and coarse RGB of 512 \u00d7 512. With full model, we synthesize images up to 1024 \u00d7 1024\n(right). Please refer to Sec. A.15, A.16 for more comparison and results. Best viewed zoom in.\nfeature discrepancy between the main and auxiliary branches, leading to inconsistency between the\ncontrol signals (e.g., pose maps) and the generated images. To address the issue, HumanSD (Ju et al.,\n2023b) proposes to directly input body skeleton into the diffusion U-Net by channel-wise concatena-\ntion. However, it is confined to generating artistic style images of limited diversity. Besides, human\nimages are synthesized only with pose control, while other structural information like depth maps\nand surface-normal maps are not considered. In a nutshell, previous studies either take a singular\ncontrol signal as input condition, or treat different control signals separately as independent guid-\nance, instead of modeling the multi-level correlations between human appearance and different types\nof structural information. Realistic human generation with coherent structure remains unsolved.\n2\nPublished as a conference paper at ICLR 2024\nIn this paper, we propose a unified framework HyperHuman to generate in-the-wild human images\nof high realism and diverse layouts. The key insight is that human image is inherently structural\nover multiple granularities, from the coarse-level body skeleton to fine-grained spatial geometry.\nTherefore, capturing such correlations between the explicit appearance and latent structure in one\nmodel is essential to generate coherent and natural human images. Specifically, we first establish\na large-scale human-centric dataset called HumanVerse that contains 340M in-the-wild human im-\nages of high quality and diversity. It has comprehensive annotations, such as the coarse-level body\nskeletons, the fine-grained depth and surface-normal maps, and the high-level image captions and\nattributes. Based on this, two modules are designed for hyper-realistic controllable human image\ngeneration. In Latent Structural Diffusion Model, we augment the pre-trained diffusion backbone to\nsimultaneously denoise the RGB, depth, and normal. Appropriate network layers are chosen to be\nreplicated as structural expert branches, so that the model can both handle input/output of different\ndomains, and guarantee the spatial alignment among the denoised textures and structures. Thanks to\nsuch dedicated design, the image appearance, spatial relationship, and geometry are jointly modeled\nwithin a unified network, where each branch is complementary to each other with both structural\nawareness and textural richness. To generate monotonous depth and surface-normal that have similar\nvalues in local regions, we utilize an improved noise schedule to eliminate low-frequency informa-\ntion leakage. The same timestep is sampled for each branch to achieve better learning and feature\nfusion. With the spatially-aligned structure maps, in Structure-Guided Refiner, we compose the\npredicted conditions for detailed generation of high resolution. Moreover, we design a robust con-\nditioning scheme to mitigate the effect of error accumulation in our two-stage generation pipeline.\nTo summarize, our main contributions are three-fold: 1) We propose a novel HyperHuman frame-\nwork for in-the-wild controllable human image generation of high realism. A large-scale human-\ncentric dataset HumanVerse is curated with comprehensive annotations like human pose, depth, and\nsurface normal. As one of the earliest attempts in human generation foundation model, we hope to\nbenefit future research. 2) We propose the Latent Structural Diffusion Model to jointly capture the\nimage appearance, spatial relationship, and geometry in a unified framework. The Structure-Guided\nRefiner is further devised to compose the predicted conditions for generation of better visual qual-\nity and higher resolution. 3) Extensive experiments demonstrate that our HyperHuman yields the\nstate-of-the-art performance, generating hyper-realistic human images under diverse scenarios.\n2\nRELATED WORK\nText-to-Image Diffusion Models. Text-to-image (T2I) generation, the endeavor to synthesize high-\nfidelity images from natural language descriptions, has made remarkable strides in recent years. Dis-\ntinguished by the superior scalability and stable training, diffusion-based T2I models have eclipsed\nconventional GANs in terms of performance (Dhariwal & Nichol, 2021), becoming the predominant\nchoice in generation (Nichol et al., 2021; Saharia et al., 2022; Balaji et al., 2022; Li et al., 2023).\nBy formulating the generation as an iterative denoising process (Ho et al., 2020), exemplar works\nlike Stable Diffusion (Rombach et al., 2022) and DALL\u00b7E 2 (Ramesh et al., 2022) demonstrate un-\nprecedented quality. Despite this, they mostly fail to create high-fidelity humans. One main reason\nis that existing models lack inherent structural awareness for human, making them even struggle\nto generate human of reasonable anatomy, e.g., correct number of arms and legs. To this end, our\nproposed approach explicitly models human structures within the latent space of diffusion model.\nControllable Human Image Generation. Traditional approaches for controllable human genera-\ntion can be categorized into GAN-based (Zhu et al., 2017; Siarohin et al., 2019) and VAE-based (Ren\net al., 2020; Yang et al., 2021), where the reference image and conditions are taken as input. To fa-\ncilitate user-friendly applications, recent studies explore text prompts as generation guidance (Roy\net al., 2022; Jiang et al., 2022), yet are confined to simple pose or style descriptions. The most\nrelevant works that enable open-vocabulary pose-guided controllable human synthesis are Control-\nNet (Zhang & Agrawala, 2023), T2I-Adapter (Mou et al., 2023), and HumanSD (Ju et al., 2023b).\nHowever, they either suffer from inadequate pose control, or are confined to artistic styles of limited\ndiversity. Besides, most previous studies merely take pose as input, while ignoring the multi-level\ncorrelations between human appearance and different types of structural information. In this work,\nwe propose to incorporate structural awareness from coarse-level skeleton to fine-grained depth and\nsurface-normal by joint denoising with expert branch, thus simultaneously capturing both the ex-\nplicit appearance and latent structure in a unified framework for realistic human image synthesis.\n3\nPublished as a conference paper at ICLR 2024\nFigure 2: Overview of HyperHuman Framework. In Latent Structural Diffusion Model (purple),\nthe image x, depth d, and surface-normal n are jointly denoised conditioning on caption c and pose\nskeleton p. For the notation simplicity, we denote pixel-/latent-space targets with the same variable.\nIn Structure-Guided Refiner (blue), we compose the predicted conditions for higher-resolution gen-\neration. Note that the grey images refer to randomly dropout conditions for more robust training.\nDatasets for Human Image Generation. Large datasets are crucial for image generation. Existing\nhuman-centric collections are mainly confronted with following drawbacks: 1) Low-resolution of\npoor quality. For example, Market-1501 (Zheng et al., 2015) contains noisy pedestrian images of\nresolution 128 \u00d7 64, and VITON (Han et al., 2018) has human-clothing pairs of 256 \u00d7 192, which\nare inadequate for training high-definition models. 2) Limited diversity of certain domain. For\nexample, SHHQ (Fu et al., 2022) is mostly composed of full-body humans with clean background,\nand DeepFashion (Liu et al., 2016) focuses on fashion images of little pose variations. 3) Insufficient\ndataset scale, where LIP (Gong et al., 2017) and Human-Art (Ju et al., 2023a) only contain 50K\nsamples. Furthermore, none of the existing datasets contain rich annotations, which typically label a\nsingular aspect of images. In this work, we take a step further by curating in-the-wild HumanVerse\ndataset with comprehensive annotations like human pose, depth map, and surface-normal map.\n3\nOUR APPROACH\nWe present HyperHuman that generates in-the-wild human images of high realism and diverse\nlayouts. The overall framework is illustrated in Fig. 2. To make the content self-contained and\nnarration clearer, we first introduce some pre-requisites of diffusion models and the problem setting\nin Sec. 3.1. Then, we present the Latent Structural Diffusion Model which simultaneously denoises\nthe depth, surface-normal along with the RGB image. The explicit appearance and latent structure\nare thus jointly learned in a unified model (Sec. 3.2). Finally, we elaborate the Structure-Guided\nRefiner to compose the predicted conditions for detailed generation of higher resolution in Sec. 3.3.\n3.1\nPRELIMINARIES AND PROBLEM SETTING\nDiffusion Probabilistic Models define a forward diffusion process to gradually convert the sample x\nfrom a real data distribution pdata(x) into a noisy version, and learn the reverse generation process in\nan iterative denoising manner (Sohl-Dickstein et al., 2015; Song et al., 2020b). During the sampling\nstage, the model can transform Gaussian noise of normal distribution to real samples step-by-step.\nThe denoising network \u02c6\u03f5\u03b8(\u00b7) estimates the additive Gaussian noise, which is typically structured as a\nUNet (Ronneberger et al., 2015) to minimize the ensemble of mean-squared error (Ho et al., 2020):\nmin\n\u03b8\nEx,c,\u03f5,t\n\u0002\nwt||\u02c6\u03f5\u03b8(\u03b1tx + \u03c3t\u03f5; c) \u2212 \u03f5||2\n2\n\u0003\n,\n(1)\nwhere x, c \u223c pdata are the sample-condition pairs from the training distribution; \u03f5 \u223c N(0, I) is the\nground-truth noise; t \u223c U[1, T] is the time-step and T is the training step number; \u03b1t, \u03c3t, and wt\nare the terms that control the noise schedule and sample quality decided by the diffusion sampler.\nLatent Diffusion Model & Stable Diffusion. The widely-used latent diffusion model (LDM), with\nits improved version Stable Diffusion (Rombach et al., 2022), performs the denoising process in a\nseparate latent space to reduce the computational cost. Specifically, a pre-trained VAE (Esser et al.,\n2021) first encodes the image x to latent embedding z = E(x) for DM training. At the inference\n4\nPublished as a conference paper at ICLR 2024\nstage, we can reconstruct the generated image through the decoder \u02c6x = D(\u02c6z). Such design enables\nthe SD to scale up to broader datasets and larger model size, advancing from the SD 1.x & 2.x series\nto SDXL of heavier backbone on higher resolution (Podell et al., 2023). In this work, we extend\nSD 2.0 to Latent Structural Diffusion Model for efficient capturing of explicit appearance and latent\nstructure, while the Structure-Guided Refiner is built on SDXL 1.0 for more pleasing visual quality.\nProblem Setting for Controllable Human Generation. Given a collection of N human images\nx with their captions c, we annotate the depth d, surface-normal n, and pose skeleton p for each\nsample (details elaborated in Sec. 4). The training dataset can be denoted as {xi, ci, di, ni, pi}N\ni=1.\nIn the first-stage Latent Structural Diffusion Model G1, we estimate the RGB image \u02c6x, depth \u02c6d,\nand surface-normal \u02c6n conditioned on the caption c and skeleton p. In the second-stage Structure-\nGuided Refiner G2, the predicted structures of \u02c6d and \u02c6n further serve as guidance for the generation\nof higher-resolution results \u02c6xhigh-res. The training setting for our pipeline can be formulated as:\n\u02c6x, \u02c6d, \u02c6n = G1(c, p),\n\u02c6xhigh-res = G2(c, p, \u02c6d, \u02c6n).\n(2)\nDuring inference, only the text prompt and body skeleton are needed to synthesize well-aligned\nRGB image, depth, and surface-normal. Note that the users are free to substitute their own depth and\nsurface-normal conditions to G2 if applicable, enabling more flexible and controllable generation.\n3.2\nLATENT STRUCTURAL DIFFUSION MODEL\nTo incorporate the body skeletons for pose control, the simplest way is by feature residual (Mou\net al., 2023) or input concatenation (Ju et al., 2023b). However, three problems remain: 1) The sparse\nkeypoints only depict the coarse human structure, while the fine-grained geometry and foreground-\nbackground relationship are ignored. Besides, the naive DM training is merely supervised by RGB\nsignals, which fails to capture the inherent structural information. 2) The image RGB and structure\nrepresentations are spatially aligned but substantially different in latent space. How to jointly model\nthem remains challenging. 3) In contrast to the colorful RGB images, the structure maps are mostly\nmonotonous with similar values in local regions, which are hard to learn by DMs (Lin et al., 2023).\nUnified Model for Simultaneous Denoising. Our solution to the first problem is to simultaneously\ndenoise the depth and surface-normal along with the synthesized RGB image. We choose them as\nadditional learning targets due to two reasons: 1) Depth and normal can be easily annotated for\nlarge-scale dataset, which are also used in recent controllable T2I generation (Zhang & Agrawala,\n2023). 2) As two commonly-used structural guidance, they complement the spatial relationship and\ngeometry information, where the depth (Deng et al., 2022), normal (Wang et al., 2022), or both (Yu\net al., 2022b) are proven beneficial in recent 3D studies. To this end, a naive method is to train three\nseparate networks to denoise the RGB, depth, and normal individually. But the spatial alignment\nbetween them is hard to preserve. Therefore, we propose to capture the joint distribution in a unified\nmodel by simultaneous denoising, which can be trained with simplified objective (Ho et al., 2020):\nL\u03f5-pred = Ex,d,n,c,p,\u03f5,t [ ||\u02c6\u03f5\u03b8(xtx; c, p) \u2212 \u03f5x||2\n2\n|\n{z\n}\ndenoise image x\n+ ||\u02c6\u03f5\u03b8(dtd; c, p) \u2212 \u03f5d||2\n2\n|\n{z\n}\ndenoise depth d\n+ ||\u02c6\u03f5\u03b8(ntn; c, p) \u2212 \u03f5n||2\n2\n|\n{z\n}\ndenoise normal n\n], (3)\nwhere \u03f5x, \u03f5d, and \u03f5n \u223c N(0, I) are three independently sampled Gaussian noise (shortened as \u03f5\nin expectation for conciseness) for the RGB, depth, and normal, respectively; xtx = \u03b1txx + \u03c3tx\u03f5x,\ndtd = \u03b1tdd+\u03c3td\u03f5d, and ntn = \u03b1tnn+\u03c3tn\u03f5n are the noised feature maps of three learning targets;\ntx, td, and tn \u223c U[1, T] are the sampled time-steps that control the scale of added Gaussian noise.\nStructural Expert Branches with Shared Backbone. The diffusion UNet contains down-sample,\nmiddle, and up-sample blocks, which are interleaved with convolution and self-/cross-attention lay-\ners. In particular, the DownBlocks compress input noisy latent to the hidden states of lower resolu-\ntion, while the UpBlocks conversely upscale intermediate features to the predicted noise. Therefore,\nthe most intuitive manner is to replicate the first several DownBlocks and the last several UpBlocks\nfor each expert branch, which are the most neighboring layers to the input and output. In this way,\neach expert branch gradually maps input noisy latent of different domains (i.e., xtx, dtd, and ntn)\nto similar distribution for feature fusion. Then, after a series of shared modules, the same feature is\ndistributed to each expert branch to output noises (i.e., \u03f5x, \u03f5d, and \u03f5n) for spatially-aligned results.\nFurthermore, we find that the number of shared modules can trade-off between the spatial alignment\nand distribution learning: On the one hand, more shared layers guarantee the more similar features\n5\nPublished as a conference paper at ICLR 2024\nof final output, leading to the paired texture and structure corresponding to the same image. On the\nother hand, the RGB, depth, and normal can be treated as different views of the same image, where\npredicting them from the same feature resembles an image-to-image translation task in essence.\nEmpirically, we find the optimal design to replicate the conv in, first DownBlock, last UpBlock, and\nconv out for each expert branch, where each branch\u2019s skip-connections are maintained separately\n(as depicted in Fig. 2). This yields both the spatial alignment and joint capture of image texture and\nstructure. Note that such design is not limited to three targets, but can generalize to arbitrary number\nof paired distributions by simply involving more branches with little computation overhead.\nNoise Schedule for Joint Learning. A problem arises when we inspect the distribution of depth\nand surface-normal: After annotated by off-the-shelf estimators, they are regularized to certain data\nrange with similar values in local regions, e.g., [0, 1] for depth and unit vector for surface-normal.\nSuch monotonous images may leak low-frequency signals like the mean of each channel during\ntraining. Besides, their latent distributions are divergent from that of RGB space, making them hard\nto exploit common noise schedules (Lin et al., 2023) and diffusion prior. Motivated by this, we first\nnormalize the depth and normal latent features to the similar distribution of RGB latent, so that the\npre-trained denoising knowledge can be adaptively used. The zero terminal SNR (\u03b1T = 0, \u03c3T = 1)\nis further enforced to eliminate structure map\u2019s low-frequency information. Another question is how\nto sample time-step t for each branch. An alternative is to perturb the data of different modalities\nwith different levels (Bao et al., 2023), which samples different t for each target as in Eq. 3. However,\nas we aim to jointly model RGB, depth, and normal, such strategy only gives 10\u22129 probability to\nsample each perturbation situation (given total steps T = 1000), which is too sparse to obtain good\nresults. In contrast, we propose to densely sample with the same time-step t for all the targets, so that\nthe sampling sparsity and learning difficulty will not increase even when we learn more modalities.\nWith the same noise level for each structural expert branch, intermediate features follow the similar\ndistribution when they fuse in the shared backbone, which could better complement to each others.\nFinally, we utilize the v-prediction (Salimans & Ho, 2022) learning target as network objective:\nLv-pred = Ex,d,n,c,p,v,t\nh\n||\u02c6v\u03b8(xt; c, p) \u2212 vx\nt ||2\n2 + ||\u02c6v\u03b8(dt; c, p) \u2212 vd\nt ||2\n2 + ||\u02c6v\u03b8(nt; c, p) \u2212 vn\nt ||2\n2\ni\n, (4)\nwhere vx\nt = \u03b1t\u03f5x \u2212 \u03c3tx, vd\nt = \u03b1t\u03f5d \u2212 \u03c3td, and vn\nt = \u03b1t\u03f5n \u2212 \u03c3tn are the v-prediction learning\ntargets at time-step t for the RGB, depth, and normal, respectively. Overall, the unified simultane-\nous denoising network \u02c6v\u03b8 with the structural expert branches, accompanied by the improved noise\nschedule and time-step sampling strategy give the first-stage Latent Structural Diffusion Model G1.\n3.3\nSTRUCTURE-GUIDED REFINER\nCompose Structures for Controllable Generation. With the unified latent structural diffusion\nmodel, spatially-aligned conditions of depth and surface-normal can be predicted. We then learn a\nrefiner network to render high-quality image \u02c6xhigh-res by composing multi-conditions of caption c,\npose skeleton p, the predicted depth \u02c6d, and the predicted surface-normal \u02c6n. In contrast to Zhang\n& Agrawala (2023) and Mou et al. (2023) that can only handle a singular condition per run, we\npropose to unify multiple control signals at the training phase. Specifically, we first project each\ncondition from input image size (e.g., 1024 \u00d7 1024) to feature space vector that matches the size\nof SDXL (e.g., 128 \u00d7 128). Each condition is encoded via a light-weight embedder of four stacked\nconvolutional layers with 4 \u00d7 4 kernels, 2 \u00d7 2 strides, and ReLU activation. Next, the embeddings\nfrom each branch are summed up coordinate-wise and further feed into the trainable copy of SDXL\nEncoder Blocks. Since involving more conditions only incurs negligible computational overhead of a\ntiny encoder network, our method can be trivially extended to new structural conditions. Although a\nrecent work also incorporates multiple conditions in one model (Huang et al., 2023), they have to re-\ntrain the whole backbone, making the training cost unaffordable when scaling up to high resolution.\nRandom Dropout for Robust Conditioning. Since the predicted depth and surface-normal condi-\ntions from G1 may contain artifacts, a potential issue for such two-stage pipeline is the error accu-\nmulation, which typically leads to the train-test performance gap. To solve this problem, we propose\nto dropout structural maps for robust conditioning. In particular, we randomly mask out any of the\ncontrol signals, such as replace text prompt with empty string, or substitute the structural maps with\nzero-value images. In this way, the model will not solely rely on a single guidance for synthesis,\nthus balancing the impact of each condition robustly. To sum up, the structure-composing refiner\nnetwork with robust conditioning scheme constitute the second-stage Structure-Guided Refiner G2.\n6\nPublished as a conference paper at ICLR 2024\n4\nHUMANVERSE DATASET\nLarge-scale datasets with high quality samples, rich annotations, and diverse distribution are crucial\nfor image generation tasks (Schuhmann et al., 2022; Podell et al., 2023), especially in the human\ndomain (Liu et al., 2016; Fu et al., 2022). To facilitate controllable human generation of high-fidelity,\nwe establish a comprehensive human dataset with extensive annotations named HumanVerse. Please\nkindly refer to Appendix A.17 for more details about the dataset and annotation resources we use.\nDataset Preprocessing. We curate from two principled datasets: LAION-2B-en (Schuhmann et al.,\n2022) and COYO-700M (Byeon et al., 2022). To isolate human images, we employ YOLOS (Fang\net al., 2021) for human detection. Specifically, only those images containing 1 to 3 human bounding\nboxes are retained, where people should be visible with an area ratio exceeding 15%. We further rule\nout samples of poor aesthetics (< 4.5) or low resolution (< 200 \u00d7 200). This yields a high-quality\nsubset by eliminating blurry and over-small humans. Unlike existing models that mostly train on\nfull-body humans of simple context (Zhang & Agrawala, 2023), our dataset encompasses a wider\nspectrum, including various backgrounds and partial human regions such as clothing and limbs.\n2D Human Poses. 2D human poses (skeleton of joints), which serve as one of the most flexible and\neasiest obtainable coarse-level condition signals, are widely used in controllable human generation\nstudies (Ju et al., 2023b; Zhu et al., 2023; Yu et al., 2023; Liu et al., 2023; 2022a;b;c). To achieve\naccurate keypoint annotations, we resort to MMPose (Contributors, 2020) as inference interface and\nchoose ViTPose-H (Xu et al., 2022) as backbone that performs best over several pose estimation\nbenchmarks. In particular, the per-instance bounding box, keypoint coordinates and confidence are\nlabeled, including whole-body skeleton, body skeleton, hand, and facial landmarks.\nDepth and Surface-Normal Maps are fine-grained structures that reflect the spatial geometry of\nimages (Wu et al., 2022), which are commonly used in conditional generation (Mou et al., 2023).\nWe apply Omnidata (Eftekhar et al., 2021) for monocular depth and normal. The MiDaS (Ranftl\net al., 2022) is further annotated following recent depth-to-image pipelines (Rombach et al., 2022).\nOutpaint for Accurate Annotations. Diffusion models have shown promising results on image in-\npainting and outpainting, where the appearance and structure of unseen regions can be hallucinated\nbased on the visible parts. Motivated by this, we propose to outpaint each image for a more holistic\nview given that most off-the-shelf structure estimators are trained on the \u201ccomplete\u201d image views.\nAlthough the outpainted region may be imperfect with artifacts, it can complement a more compre-\nhensive human structure. To this end, we utilize the powerful SD-Inpaint to outpaint the surrounding\nareas of the original canvas. These images are further processed by off-the-shelf estimators, where\nwe only use the labeling within the original image region for more accurate annotations.\nOverall Statistics. In summary, COYO subset contains 90, 948, 474 (91M) images and LAION-2B\nsubset contains 248, 396, 109 (248M) images, which is 18.12% and 20.77% of fullset, respectively.\nThe whole annotation process takes 640 16/32G NVIDIA V100 GPUs for two weeks in parallel.\n5\nEXPERIMENTS\nExperimental Settings. For the comprehensive evaluation, we divide our comparisons into two\nsettings: 1) Quantitative analysis. All the methods are tested on the same benchmark, using the\nsame prompt with DDIM Scheduler (Song et al., 2020a) for 50 denoising steps to generate the same\nresolution images of 512 \u00d7 512. 2) Qualitative analysis. We generate high-resolution 1024 \u00d7 1024\nresults for each model with the officially provided best configurations, such as the prompt engineer-\ning, noise scheduler, and classifier-free guidance (CFG) scale. Note that we use the RGB output\nof the first-stage Latent Structural Diffusion Model for numerical comparison, while the improved\nresults from the second-stage Structure-Guided Refiner are merely utilized for visual comparison.\nDatasets. We follow common practices in T2I generation (Yu et al., 2022a) and filter out a human\nsubset from MS-COCO 2014 validation (Lin et al., 2014) for zero-shot evaluation. In particular, off-\nthe-shelf human detector and pose estimator are used to obtain 8, 236 images with clearly-visible\nhumans for evaluation. All the ground truth images are resized and center-cropped to 512 \u00d7 512. To\nguarantee fair comparisons, we train first-stage Latent Structural Diffusion on HumanVerse, which\nis a subset of public LAION-2B and COYO, to report quantitative metrics. In addition, an internal\ndataset is adopted to train second-stage Structure-Guided Refiner only for visually pleasing results.\n7\nPublished as a conference paper at ICLR 2024\nTable 1: Zero-Shot Evaluation on MS-COCO 2014 Validation Human. We compare our model\nwith recent SOTA general T2I models (Rombach et al., 2022; Podell et al., 2023; DeepFloyd, 2023)\nand controllable methods (Zhang & Agrawala, 2023; Mou et al., 2023; Ju et al., 2023b). Note that\n\u2020SDXL generates artistic style in 512, and \u2021IF only creates fixed-size images, we first generate\n1024 \u00d7 1024 results, then resize back to 512 \u00d7 512 for these two methods. We bold the best and\nunderline the second results for clarity. Our improvements over the second method are shown in red.\nImage Quality\nAlignment\nPose Accuracy\nMethods\nFID \u2193\nKID\u00d71k \u2193\nFIDCLIP \u2193\nCLIP \u2191\nAP \u2191\nAR \u2191\nAPclean \u2191\nARclean \u2191\nSD 1.5\n24.26\n8.69\n12.93\n31.72\n-\n-\n-\n-\nSD 2.0\n22.98\n9.45\n11.41\n32.13\n-\n-\n-\n-\nSD 2.1\n24.63\n9.52\n15.01\n32.11\n-\n-\n-\n-\nSDXL\u2020\n29.08\n12.16\n19.00\n32.90\n-\n-\n-\n-\nDeepFloyd-IF\u2021\n29.72\n15.27\n17.01\n32.11\n-\n-\n-\n-\nControlNet\n27.16\n10.29\n15.59\n31.60\n20.46\n30.23\n25.92\n38.67\nT2I-Adapter\n23.54\n7.98\n11.95\n32.16\n27.54\n36.62\n34.86\n46.53\nHumanSD\n52.49\n33.96\n21.11\n29.48\n26.71\n36.85\n32.84\n45.87\nHyperHuman\n17.18 25.2%\u2193\n4.11 48.5%\u2193\n7.82 31.5%\u2193\n32.17\n30.38\n37.84\n38.84\n48.70\nComparison Methods. We compare with two categories of open-source SOTA works: 1) General\nT2I models, including SD (Rombach et al., 2022) (SD 1.x & 2.x), SDXL (Podell et al., 2023), and\nIF (DeepFloyd, 2023). 2) Controllable methods with pose condition. Notably, ControlNet (Zhang\n& Agrawala, 2023) and T2I-Adapter (Mou et al., 2023) can handle multiple structural signals like\ncanny, depth, and normal, where we take their skeleton-conditioned variant for comparison. Hu-\nmanSD (Ju et al., 2023b) is the most recent work that specializes in pose-guided human generation.\nImplementation Details. We resize and random-crop the RGB, depth, and normal to the target\nresolution of each stage. To enforce the model with size and location awareness, the original image\nheight/width and crop coordinates are embedded in a similar way to time embedding (Podell et al.,\n2023). Our code is developed based on diffusers (von Platen et al., 2022). 1) For the Latent Struc-\ntural Diffusion, we fine-tune the whole UNet from the pretrained SD-2.0-base to v-prediction (Sal-\nimans & Ho, 2022) in 512 \u00d7 512 resolution. The DDIMScheduler with improved noise schedule is\nused for both training and sampling. We train on 128 80G NVIDIA A100 GPUs in a batch size of\n2, 048 for one week. 2) For the Structure-Guided Refiner, we choose SDXL-1.0-base as the frozen\nbackbone and fine-tune to \u03f5-prediction for high-resolution synthesis of 1024 \u00d7 1024. We train on\n256 80G NVIDIA A100 GPUs in a batch size of 2, 048 for one week. The overall framework is\noptimized with AdamW (Kingma & Ba, 2015) in 1e \u2212 5 learning rate, and 0.01 weight decay.\n5.1\nMAIN RESULTS\nEvaluation Metrics. We adopt commonly-used metrics to make comprehensive comparisons from\nthree perspectives: 1) Image Quality. FID, KID, and FIDCLIP are used to reflect quality and diversity.\n2) Text-Image Alignment, where the CLIP similarity between text and image embeddings is reported.\n3) Pose Accuracy. We use the state-of-the-art pose estimator to extract poses from synthetic images\nand compare with the input (GT) pose conditions. The Average Precision (AP) and Average Recall\n(AR) are adopted to evaluate the pose alignment. Note that due to the noisy pose estimation of in-\nthe-wild COCO, we also use APclean and ARclean to only evaluate on the three most salient persons.\nQuantitative Analysis. We report zero-shot evaluation results in Tab. 1. For all methods, we use\nthe default CFG scale of 7.5, which well balances the quality and diversity with appealing results.\nThanks to the structural awareness from expert branches, our proposed HyperHuman outperforms\nprevious works by a clear margin, achieving the best results on image quality and pose accuracy\nmetrics and ranks second on CLIP score. Note that SDXL (Podell et al., 2023) uses two text encoders\nwith 3\u00d7 larger UNet of more cross-attention layers, leading to superior text-image alignment. In\nspite of this, we still obtain an on-par CLIP score and surpass all the other baselines that have similar\ntext encoder parameters. We also show the FID-CLIP and FIDCLIP-CLIP curves over multiple CFG\nscales in Fig. 3, where our model balances well between image quality and text-alignment, especially\nfor the commonly-used CFG scales (bottom right). Please see Sec. A.1 for more quantitative results.\n8\nPublished as a conference paper at ICLR 2024\n0.300\n0.305\n0.310\n0.315\n0.320\n0.325\n0.330\nCLIP score\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\nFID\nSD v1.5\nSD v2.0\nSD v2.1\nSDXL\nDeepFloyd-IF\nControlNet\nT2I-Adapter\nOurs\n0.305\n0.310\n0.315\n0.320\n0.325\n0.330\nCLIP score\n8\n10\n12\n14\n16\n18\n20\nFIDCLIP\nSD v1.5\nSD v2.0\nSD v2.1\nSDXL\nDeepFloyd-IF\nControlNet\nT2I-Adapter\nOurs\nFigure 3: Evaluation Curves on COCO-Val Human.\nWe show FID-CLIP (left) and FIDCLIP-CLIP (right) curves\nwith CFG scale ranging from 4.0 to 20.0 for all methods.\nTable 2: Ablation Results. We explore design\nchoices for simultaneous denoising targets, num-\nber of expert branch layers, and noise schedules.\nThe image quality and alignment are evaluated.\nAblation Settings\nFID \u2193\nFIDCLIP \u2193\nLd\n2 \u2193\nLn\n2 \u2193\nDenoise RGB\n21.68\n10.27\n-\n-\nDenoise RGB + Depth\n19.89\n9.30\n544.2\n-\nHalf DownBlocks & UpBlocks\n22.85\n11.38\n508.3\n124.3\nTwo DownBlocks & UpBlocks\n17.94\n8.85\n677.4\n145.9\nDefault SNR with \u03f5-pred\n17.70\n8.41\n867.0\n180.2\nDifferent Timesteps t\n29.36\n18.29\n854.8\n176.1\nHyperHuman (Ours)\n17.18\n7.82\n502.1\n121.6\nTable 3: User Preference Comparisons. We report the ratio of users prefer our model to baselines.\nMethods\nSD 2.1\nSDXL\nIF\nControlNet\nT2I-Adapter\nHumanSD\nHyperHuman\n89.24%\n60.45%\n82.45%\n92.33%\n98.06%\n99.08%\nQualitative Analysis. Fig. 1 shows example results (top) and comparisons with baselines (bottom).\nWe can generate both photo-realistic images and stylistic rendering, showing better realism, quality,\ndiversity, and controllability. A comprehensive user study is further conducted as shown in Tab. 3,\nwhere the users prefer HyperHuman to the general and controllable T2I models. Please refer to\nAppendix A.4, A.15, and A.16 for more user study details, visual comparisons, and qualitative\nresults.\n5.2\nABLATION STUDY\nIn this section, we present the key ablation studies. Except for the image quality metrics, we also\nuse the depth/normal prediction error as a proxy for spatial alignment between the synthesized RGB\nand structural maps. Specifically, we extract the depth and surface-normal by off-the-shelf estimator\nas pseudo ground truth. The Ld\n2 and Ln\n2 denote the L2-error of depth and normal, respectively.\nSimultaneous Denoise with Expert Branch. We explore whether latent structural diffusion model\nhelps, and how many layers to replicate in the structural expert branches: 1) Denoise RGB, that only\nlearns to denoise an image. 2) Denoise RGB + Depth, which also predicts depth. 3) Half DownBlock\n& UpBlock. We replicate half of the first DownBlock and the last UpBlock, which contains one\ndown/up-sample ResBlock and one AttnBlock. 4) Two DownBlocks & UpBlocks, where we copy the\nfirst two DownBlocks and the last two UpBlocks. The results are shown in Tab. 2 (top), which prove\nthat the joint learning of image appearance, spatial relationship, and geometry is beneficial. We also\nfind that while fewer replicate layers give more spatially aligned results, the per-branch parameters\nare insufficient to capture distributions of each modality. In contrast, excessive replicate layers lead\nto less feature fusion across different targets, which fails to complement to each other branches.\nNoise Schedules. The ablation is conducted on two settings: 1) Default SNR with \u03f5-pred, where\nwe use the original noise sampler schedules with \u03f5-prediction. 2) Different Timesteps t. We sample\ndifferent noise levels (tx, td, and tn) for each modality. We can see from Tab. 2 (bottom) that zero-\nterminal SNR is important for learning of monotonous structural maps. Besides, different timesteps\nharm the performance with more sparse perturbation sampling and harder information sharing.\n6\nDISCUSSION\nConclusion. In this paper, we propose a novel framework HyperHuman to generate in-the-wild hu-\nman images of high quality. To enforce the joint learning of image appearance, spatial relationship,\nand geometry in a unified network, we propose Latent Structural Diffusion Model that simultane-\nously denoises the depth and normal along with RGB. Then we devise Structure-Guided Refiner to\ncompose the predicted conditions for detailed generation. Extensive experiments demonstrate that\nour framework yields superior performance, generating realistic humans under diverse scenarios.\nLimitation and Future Work. As an early attempt in human generation foundation model, our\napproach creates controllable human of high realism. However, due to the limited performance of\nexisting pose/depth/normal estimators for in-the-wild humans, we find it sometimes fails to generate\nsubtle details like finger and eyes. Besides, the current pipeline still requires body skeleton as input,\nwhere deep priors like LLMs can be explored to achieve text-to-pose generation in future work.\n9\nPublished as a conference paper at ICLR 2024\n7\nACKNOWLEDGEMENT\nThis study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-\nT2EP20221- 0012) and NTU NAP.\nREFERENCES\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 3\nFan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang\nSu, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. arXiv\npreprint arXiv:2303.06555, 2023. 6\nMiko\u0142aj Bi\u00b4nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd\ngans. arXiv preprint arXiv:1801.01401, 2018. 15\nMinwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon\nKim.\nCoyo-700m:\nImage-text pair dataset.\nhttps://github.com/kakaobrain/\ncoyo-dataset, 2022. 7, 34\nMMPose Contributors.\nOpenmmlab pose estimation toolbox and benchmark.\nhttps://github.\ncom/open-mmlab/mmpose, 2020. 7, 34\nDeepFloyd.\nDeepfloyd if.\nGithub Repository, 2023.\nURL https://github.com/\ndeep-floyd/IF. 8, 24\nKangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views\nand faster training for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 12882\u201312891, 2022. 5\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780\u20138794, 2021. 1, 3\nAinaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable pipeline\nfor making multi-task mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 10786\u201310796, 2021. 7, 34\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recogni-\ntion, pp. 12873\u201312883, 2021. 4\nYuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, and\nWenyu Liu. You only look at one sequence: Rethinking transformer in vision through object\ndetection. CoRR, abs/2106.00666, 2021. URL https://arxiv.org/abs/2106.00666.\n7, 34\nJianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen Change Loy, Wayne Wu,\nand Ziwei Liu. Stylegan-human: A data-centric odyssey of human generation. In European\nConference on Computer Vision, pp. 1\u201319. Springer, 2022. 4, 7\nKe Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen, and Liang Lin. Look into person: Self-\nsupervised structure-sensitive learning and a new benchmark for human parsing. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pp. 932\u2013940, 2017. 4\nJiaxian Guo, Junnan Li, Dongxu Li, Anthony Tiong, Boyang Li, Dacheng Tao, and Steven HOI.\nFrom images to textual prompts: Zero-shot VQA with frozen large language models, 2023. URL\nhttps://openreview.net/forum?id=Ck1UtnVukP8. 34\nXintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry S Davis. Viton: An image-based virtual\ntry-on network. In Proceedings of the IEEE conference on computer vision and pattern recogni-\ntion, pp. 7543\u20137552, 2018. 4\n10\nPublished as a conference paper at ICLR 2024\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017. 15\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020. 1, 3, 4, 5\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.\nIn International Conference on Machine Learning, pp. 2790\u20132799. PMLR, 2019. 20\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021. 20\nLianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative\nand controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778,\n2023. 6\nYuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen Change Loy, and Ziwei Liu.\nText2human: Text-driven controllable human image generation. ACM Transactions on Graphics\n(TOG), 41(4):1\u201311, 2022. 1, 3\nXuan Ju, Ailing Zeng, Jianan Wang, Qiang Xu, and Lei Zhang. Human-art: A versatile human-\ncentric dataset bridging natural and artificial scenes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 618\u2013629, 2023a. 4, 19\nXuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. Humansd: A native\nskeleton-guided diffusion model for human image generation. arXiv preprint arXiv:2304.04269,\n2023b. 2, 3, 5, 7, 8, 15, 16, 22, 24\nBo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. On architectural com-\npression of text-to-image diffusion models. arXiv preprint arXiv:2305.15798, 2023. 20\nDiederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\nIn Yoshua\nBengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:\n//arxiv.org/abs/1412.6980. 8\nYuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint\narXiv:2305.01569, 2023. 15\nYanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov,\nand Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds.\narXiv preprint arXiv:2306.00980, 2023. 3\nShanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and\nsample steps are flawed. arXiv preprint arXiv:2305.08891, 2023. 5, 6, 21\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pp. 740\u2013755. Springer, 2014. 7, 34\nWen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, and Shenghua Gao. Liquid warping gan:\nA unified framework for human motion imitation, appearance transfer and novel view synthesis.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5904\u20135913,\n2019. 1\nXian Liu, Qianyi Wu, Hang Zhou, Yuanqi Du, Wayne Wu, Dahua Lin, and Ziwei Liu. Audio-driven\nco-speech gesture video generation. Advances in Neural Information Processing Systems, 35:\n21386\u201321399, 2022a. 7\n11\nPublished as a conference paper at ICLR 2024\nXian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou, Wayne Wu,\nBo Dai, and Bolei Zhou. Learning hierarchical cross-modal association for co-speech gesture\ngeneration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pp. 10462\u201310472, 2022b. 7\nXian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, and Bolei Zhou.\nSemantic-aware\nimplicit neural audio-driven video portrait generation. In European Conference on Computer\nVision, pp. 106\u2013125. Springer, 2022c. 7\nXian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, and Ziwei\nLiu. Humangaussian: Text-driven 3d human generation with gaussian splatting. arXiv preprint\narXiv:2311.17061, 2023. 7\nZiwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust\nclothes recognition and retrieval with rich annotations. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 1096\u20131104, 2016. 4, 7\nChong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023. 1, 3, 5, 6, 7, 8, 16, 22, 24\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3, 15\nGaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in\ngan evaluation. In CVPR, 2022. 34\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023. 5, 7, 8, 18, 24, 34\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021. 15, 18, 34\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. 1,\n3\nRen\u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 44(3), 2022. 7, 34\nYurui Ren, Xiaoming Yu, Junming Chen, Thomas H Li, and Ge Li. Deep image spatial transfor-\nmation for person image generation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 7690\u20137699, 2020. 1, 3\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR), pp. 10684\u201310695, June 2022. 1, 3,\n4, 7, 8, 15, 18, 24, 34\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-\nical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013\nMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceed-\nings, Part III 18, pp. 234\u2013241. Springer, 2015. 4\nPrasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal, and Michael Blumenstein.\nTips: Text-induced pose synthesis. In European Conference on Computer Vision, pp. 161\u2013178.\nSpringer, 2022. 3\n12\nPublished as a conference paper at ICLR 2024\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022. 3, 15\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv\npreprint arXiv:2202.00512, 2022. 6, 8\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models.\nAdvances in Neural\nInformation Processing Systems, 35:25278\u201325294, 2022. 7, 34\nAliaksandr Siarohin, St\u00b4ephane Lathuili`ere, Enver Sangineto, and Nicu Sebe. Appearance and pose-\nconditioned human image generation using deformable gans. IEEE transactions on pattern anal-\nysis and machine intelligence, 43(4):1156\u20131171, 2019. 1, 3\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In Francis Bach and David Blei (eds.), Pro-\nceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings\nof Machine Learning Research, pp. 2256\u20132265, Lille, France, 07\u201309 Jul 2015. PMLR. URL\nhttps://proceedings.mlr.press/v37/sohl-dickstein15.html. 4\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020a. 7\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020b. 4\nPatrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,\nMishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https:\n//github.com/huggingface/diffusers, 2022. 8, 34\nBochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang. Toward\ncharacteristic-preserving image-based virtual try-on network. In Proceedings of the European\nconference on computer vision (ECCV), pp. 589\u2013604, 2018. 1\nJiepeng Wang, Peng Wang, Xiaoxiao Long, Christian Theobalt, Taku Komura, Lingjie Liu, and\nWenping Wang. Neuris: Neural reconstruction of indoor scenes using normal priors. In European\nConference on Computer Vision, pp. 139\u2013155. Springer, 2022. 5\nQianyi Wu, Xian Liu, Yuedong Chen, Kejie Li, Chuanxia Zheng, Jianfei Cai, and Jianmin Zheng.\nObject-compositional neural implicit surfaces. In European Conference on Computer Vision, pp.\n197\u2013213. Springer, 2022. 7\nXiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li.\nHuman preference score v2: A solid benchmark for evaluating human preferences of text-to-\nimage synthesis. arXiv preprint arXiv:2306.09341, 2023. 15\nYufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer base-\nlines for human pose estimation. Advances in Neural Information Processing Systems, 35:38571\u2013\n38584, 2022. 7, 34\nLingbo Yang, Pan Wang, Chang Liu, Zhanning Gao, Peiran Ren, Xinfeng Zhang, Shanshe Wang,\nSiwei Ma, Xiansheng Hua, and Wen Gao. Towards fine-grained human pose transfer with detail\nreplenishing network. IEEE Transactions on Image Processing, 30:2422\u20132435, 2021. 3\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022a. 7\n13\nPublished as a conference paper at ICLR 2024\nZehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. Monosdf: Ex-\nploring monocular geometric cues for neural implicit surface reconstruction. Advances in neural\ninformation processing systems, 35:25018\u201325032, 2022b. 5\nZhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, and Kwan-Yee Lin. Monohuman: Animatable\nhuman neural field from monocular video. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pp. 16943\u201316953, 2023. 7\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\narXiv preprint arXiv:2302.05543, 2023. 1, 3, 5, 6, 7, 8, 16, 22, 24\nPengze Zhang, Lingxiao Yang, Jian-Huang Lai, and Xiaohua Xie. Exploring dual-task correla-\ntion for pose guided person image generation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 7713\u20137722, 2022. 1\nLiang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person\nre-identification: A benchmark. In Proceedings of the IEEE international conference on computer\nvision, pp. 1116\u20131124, 2015. 4\nLingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, and Lequan Yu. Taming diffusion models\nfor audio-driven co-speech gesture generation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 10544\u201310553, 2023. 7\nShizhan Zhu, Raquel Urtasun, Sanja Fidler, Dahua Lin, and Chen Change Loy. Be your own prada:\nFashion synthesis with structural coherence. In Proceedings of the IEEE international conference\non computer vision, pp. 1680\u20131688, 2017. 3\n14\nPublished as a conference paper at ICLR 2024\nA\nAPPENDIX\nIn this supplemental document, we provide more details of the following contents: 1) Additional\nquantitative results (Sec. A.1). 2) More implementation details like network architecture, hyper-\nparameters, and training setups, etc (Sec. A.2). 3) More ablation study results (Sec. A.3). 4) More\nuser study details (Sec. A.4). 5) The impact of random seed to our model to show the robustness\nof our method (Sec. A.5). 6) Boarder impact and the ethical consideration of this work (Sec. A.6).\n7) Model\u2019s robustness on the unseen and challenging pose (Sec. A.7). 8) Potential optimization\nfor the annotation and training pipeline (Sec. A.8). textbf9) Model\u2019s performance on unconditional\ngeneration without input poses (Sec. A.9). 10) Model\u2019s performance on the jittered poses and image\nanimation results (Sec. A.10). textbf11) More first-stage generation results (Sec. A.11). 12) The\ndetailed intuition of updated noise schedule (Sec. A.12). 13) More details on pose processing and\nencoding (Sec. A.13). 14) Reconstruction performance of RGB VAE on other modality-specific in-\nputs (Sec. A.14). 15) More visual comparison results with recent T2I models (Sec. A.15). 16) More\nqualitative results of our model (Sec. A.16). 17) The asset licenses we use in this work (Sec. A.17).\nA.1\nADDITIONAL QUANTITATIVE RESULTS\nFID-CLIP Curves. Due to the page limit, we only show tiny-size FID-CLIP and FIDCLIP-CLIP\ncurves in the main paper and omit the curves of HumanSD (Ju et al., 2023b) due to its too large\nFID and FIDCLIP results for reasonable axis scale. Here, we show a clearer version of FID-CLIP\nand FIDCLIP-CLIP curves in Fig. 4. As broadly proven in recent text-to-image studies (Rombach\net al., 2022; Nichol et al., 2021; Saharia et al., 2022), the classifier-free guidance (CFG) plays an\nimportant role in trading-off image quality and diversity, where the CFG scales around 7.0 \u2212 8.0\n(corresponding to the bottom-right part of the curve) are the commonly-used choices in practice.\nWe can see from Fig. 4 that our model can achieve a competitive CLIP Score while maintaining\nsuperior image quality results, showing the efficacy of our proposed HyperHuman framework.\nHuman Preference-Related Metrics. As shown in recent text-to-image generation evaluation stud-\nies, conventional image quality metrics like FID (Heusel et al., 2017), KID (Bi\u00b4nkowski et al., 2018)\nand text-image alignment CLIP Score (Radford et al., 2021) diverge a lot from the human pref-\nerence (Kirstain et al., 2023). To this end, we adopt two very recent human preference-related\nmetrics: 1) PickScore (Kirstain et al., 2023), which is trained on the side-by-side comparisons of\ntwo T2I models. 2) HPS (Human Preference Score) V2 (Wu et al., 2023), which takes the user\nlike/dislike statistics for scoring model training. The evaluation results are reported in Tab. 4, which\nshow that our framework performs better than the baselines. Although the improvement seems to be\nmarginal, we find current human preference-related metrics to be highly biased: The scoring models\nare mostly trained on the synthetic data with highest resolution of 1024 \u00d7 1024, which makes them\nfavor unrealistic images of 1024 resolution, as they rarely see real images of higher resolution in\nscore model training. In spite of this, we still achieve superior quantitative and qualitative results on\nthese two metrics and a comprehensive user study, outperforming all the baseline methods.\n0.29\n0.30\n0.31\n0.32\n0.33\nCLIP score\n20\n25\n30\n35\n40\n45\n50\n55\nFID\nSD v1.5\nSD v2.0\nSD v2.1\nSDXL\nDeepFloyd-IF\nControlNet\nT2I-Adapter\nHumanSD\nOurs\n0.29\n0.30\n0.31\n0.32\n0.33\nCLIP score\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nFIDCLIP\nSD v1.5\nSD v2.0\nSD v2.1\nSDXL\nDeepFloyd-IF\nControlNet\nT2I-Adapter\nHumanSD\nOurs\nFigure 4: Clear Evaluation Curves on MS-COCO2014 Validation Human. We show FID-CLIP (left) and\nFIDCLIP-CLIP (right) curves with CFG scale ranging from 4.0 to 20.0 for all methods.\n15\nPublished as a conference paper at ICLR 2024\nTable 4: Quantitative Results on Human Preference-Related Metrics. We report on two recent\nmetrics PickScore and HPS V2. The first row denotes the ratio of preferring ours to others, where\nlarger than 50% means the superior one. The second row is the human preference score, where the\nhigher the better. It can be seen that our proposed HyperHuman achieves the best performance.\nMethods\nOurs\nSD 2.1\nSDXL\nIF\nControlNet\nAdapter\nHumanSD\nPickScore\n-\n66.87%\n52.11%\n63.37%\n74.47%\n83.25%\n87.18%\nHPS V2\n0.2905\n0.2772\n0.2832\n0.2849\n0.2783\n0.2732\n0.2656\nPose Accuracy Results on Different CFG Scales. We additionally report the pose accuracy results\nover different CFG scales. Specifically, we evaluate the conditional human generation methods\nof ControlNet (Zhang & Agrawala, 2023), T2I-Adapter (Mou et al., 2023), HumanSD (Ju et al.,\n2023b), and ours on four metrics Average Precision (AP), Average Recall (AR), clean AP (APclean),\nand clean AR (ARclean) as mentioned in Sec. 5.1. We report on CFG scales ranging from 4.0 to 13.0\nin Tab. 5, where our method is constantly better in terms of pose accuracy and controllability.\nA.2\nMORE IMPLEMENTATION DETAILS\nWe report implementation details like training hyper-parameters, and model architecture in Tab. 6.\nA.3\nMORE ABLATION STUDY RESULTS\nWe implement additional ablation study experiments on the second stage Structure-Guided Refiner.\nNote that due to the training resource limit and the resolution discrepancy between MS-COCO real\nimages (512 \u00d7 512) and high-quality renderings (1024 \u00d7 1024), we conduct several toy ablation\nexperiments in the lightweight 512 \u00d7 512 variant of our model: 1) w/o random dropout, where\nthe all the input conditions are not dropout or masked out during the conditional training stage. 2)\nOnly Text, where not any structural prediction is input to the model and we only use the text prompt\nas condition. 3) Condition on p, where we only use human pose skeleton p as input condition to\nthe refiner network. 4) Condition on d that uses depth map d as input condition. 5) Condition\non n that uses surface-normal n as input condition. And their combinations of 6) Condition on\np, d; 7) Condition on p, n; 8) Condition on d, n, to verify the impact of each condition and the\nnecessity of using such multi-level hierarchical structural guidance for fine-grained generation. The\nresults are reported in Tab. 7. We can see that the random dropout conditioning scheme is crucial\nfor more robust training with better image quality, especially in the two-stage generation pipeline.\nBesides, the structural map/guidance contains geometry and spatial relationship information, which\nare beneficial to image generation of higher quality. Another interesting phenomenon is that only\nconditioned on surface-normal n is better than conditioned on both the pose skeleton p and depth\nmap d, which aligns with our intuition that surface-normal conveys rich structural information that\nmostly cover coarse-level skeleton and depth map, except for the keypoint location and foreground-\nbackground relationship. Overall, we can conclude from ablation results that: 1) Each condition\n(i.e., pose skeleton, depth map, and surface-normal) is important for higher-quality and more aligned\ngeneration, which validates the necessity of our first-stage Latent Structural Diffusion Model to\njointly capture them. 2) The random dropout scheme for robust conditioning can essentially bridge\nthe train-test error accumulation in two-stage pipeline, leading to better image results.\nA.4\nMORE USER STUDY DETAILS\nThe study involves 25 participants and annotates for a total of 8236 images in the zero-shot MS-\nCOCO 2014 validation human subset. They take 2-3 days to complete all the user study task, with\na final review to examine the validity of human preference. Specifically, we conduct side-by-side\ncomparisons between our generated results and each baseline model\u2019s results. The asking question\nis \u201cConsidering both the image aesthetics and text-image alignment, which image is better?\nPrompt: <prompt>.\u201d The labelers are unaware of which image corresponds to which baseline,\ni.e., the place of two compared images are shuffled to achieve fair comparison without bias.\n16\nPublished as a conference paper at ICLR 2024\nTable 5: Additional Pose Accuracy Results for Different CFG Scales. We evaluate on four pose\nalignment metrics AP, AR, APclean, and ARclean for the CFG scales ranging from 4.0 to 13.0.\nCFG 4.0\nCFG 5.0\nMethods\nAP \u2191\nAR \u2191\nAPclean \u2191\nARclean \u2191\nAP \u2191\nAR \u2191\nAPclean \u2191\nARclean \u2191\nControlNet\n20.37\n29.54\n25.98\n37.96\n20.42\n29.94\n26.09\n38.31\nT2I-Adapter\n28.18\n36.71\n35.68\n46.77\n27.90\n36.76\n35.31\n46.78\nHumanSD\n26.05\n35.89\n32.27\n44.90\n26.51\n36.44\n32.84\n45.48\nHyperHuman\n30.45\n37.87\n38.88\n48.75\n30.57\n37.96\n39.01\n48.84\nCFG 6.0\nCFG 7.0\nMethods\nAP \u2191\nAR \u2191\nAPclean \u2191\nARclean \u2191\nAP \u2191\nAR \u2191\nAPclean \u2191\nARclean \u2191\nControlNet\n20.54\n30.16\n26.09\n38.64\n20.44\n30.29\n26.01\n38.79\nT2I-Adapter\n27.90\n36.77\n35.37\n46.80\n27.66\n36.62\n35.00\n46.55\nHumanSD\n26.79\n36.79\n33.10\n45.91\n26.73\n36.84\n32.94\n45.80\nHyperHuman\n30.44\n37.92\n38.91\n48.77\n30.49\n37.90\n38.82\n48.72\nCFG 8.0\nCFG 9.0\nMethods\nAP \u2191\nAR \u2191\nAPclean \u2191\nARclean \u2191\nAP \u2191\nAR \u2191\nAPclean \u2191\nARclean \u2191\nControlNet\n20.54\n30.28\n26.06\n38.74\n20.35\n30.11\n25.80\n38.43\nT2I-Adapter\n27.46\n36.50\n34.80\n46.39\n27.10\n36.32\n34.14\n46.04\nHumanSD\n26.76\n36.86\n32.96\n45.88\n26.67\n36.91\n32.74\n45.93\nHyperHuman\n30.23\n37.80\n38.72\n48.59\n29.93\n37.67\n38.30\n48.45\nCFG 10.0\nCFG 11.0\nMethods\nAP \u2191\nAR \u2191\nAPclean \u2191\nARclean \u2191\nAP \u2191\nAR \u2191\nAPclean \u2191\nARclean \u2191\nControlNet\n20.10\n30.08\n25.50\n38.29\n19.81\n29.93\n25.23\n38.23\nT2I-Adapter\n26.89\n36.19\n33.83\n45.83\n26.65\n36.10\n33.51\n45.67\nHumanSD\n26.67\n36.86\n32.80\n46.00\n26.53\n36.74\n32.63\n45.85\nHyperHuman\n29.75\n37.60\n38.20\n48.38\n29.58\n37.31\n37.88\n48.07\nCFG 12.0\nCFG 13.0\nMethods\nAP \u2191\nAR \u2191\nAPclean \u2191\nARclean \u2191\nAP \u2191\nAR \u2191\nAPclean \u2191\nARclean \u2191\nControlNet\n19.57\n29.84\n25.02\n38.15\n19.52\n29.74\n24.93\n38.08\nT2I-Adapter\n26.49\n35.95\n33.39\n45.52\n26.41\n35.90\n33.22\n45.44\nHumanSD\n26.46\n36.71\n32.53\n45.82\n26.26\n36.65\n32.39\n45.70\nHyperHuman\n29.40\n37.18\n37.75\n47.90\n29.29\n37.11\n37.64\n47.87\n17\nPublished as a conference paper at ICLR 2024\nTable 6: Training Hyper-parameters and Network Architecture in HyperHuman.\nLatent Structural Diffusion\nStructure-Guided Refiner\nActivation Function\nSiLU\nSiLU\nAdditional Embed Type\nTime\nText + Time\n# of Heads in Additional Embed\n64\n64\nAdditional Time Embed Dimension\n256\n256\nAttention Head Dimension\n[5, 10, 20, 20]\n[5, 10, 20]\nBlock Out Channels\n[320, 640, 1280, 1280]\n[320, 640, 1280]\nCross-Attention Dimension\n1024\n2048\nDown Block Types\n[\u201cCrossAttn\u201d\u00d73,\u201cResBlock\u201d\u00d71]\n[\u201cResBlock\u201d\u00d71,\u201cCrossAttn\u201d\u00d72]\nInput Channel\n8\n4\n# of Input Head\n3\n3\nCondition Embedder Channels\n-\n[16, 32, 96, 256]\nTransformer Layers per Block\n[1, 1, 1, 1]\n[1, 2, 10]\nLayers per Block\n[2, 2, 2, 2]\n[2, 2, 2]\nInput Class Embedding Dimension\n\u2212\n2816\nSampler Training Step T\n1000\n1000\nLearning Rate\n1e \u2212 5\n1e \u2212 5\nWeight Decay\n0.01\n0.01\nWarmup Steps\n0\n0\nAdamW Betas\n(0.9, 0.999)\n(0.9, 0.999)\nBatch Size\n2048\n2048\nCondition Dropout\n15%\n50%\nText Encoder\nOpenCLIP ViT-H (Radford et al., 2021)\nCLIP ViT-L & OpenCLIP ViT-bigG (Radford et al., 2021)\nPretrained Model\nSD-2.0-base (Rombach et al., 2022)\nSDXL-1.0-base (Podell et al., 2023)\nTable 7: Additional Ablation Results for Structure-Guided Refiner. Due to the resource limit\nand resolution discrepancy, we experiment on 512\u00d7512 resolution to illustrate our design\u2019s efficacy.\nAblation Settings\nFID \u2193\nKID\u00d71k \u2193\nFIDCLIP \u2193\nCLIP \u2191\nw/o random dropout\n25.69\n11.84\n13.48\n31.83\nOnly Text\n23.99\n10.42\n13.22\n32.23\nCondition on p\n20.97\n7.51\n12.86\n31.95\nCondition on d\n14.97\n3.75\n9.88\n31.74\nCondition on n\n12.67\n2.61\n7.09\n31.59\nCondition on p, d\n14.98\n3.78\n9.47\n31.74\nCondition on p, n\n12.65\n2.66\n6.93\n31.63\nCondition on d, n\n12.42\n2.59\n6.89\n31.57\nOurs w/ Refiner\n12.38\n2.55\n6.76\n32.23\nWe note that all the labelers are well-trained for such text-to-image generation comparison tasks,\nwho have passed the examination on a test set and have experience in this kind of comparisons for\nover 50 times. Below, we include the user study rating details for our method vs. baseline models.\nEach labeler can click on four options: a) The left image is better, in this case the corresponding\nmodel will get +1 grade. b) The right image is better. c) NSFW, which means the prompt/image\ncontain NSFW contents, in this case both models will get 0 grade. d) Hard Case, where the labelers\nfind it hard to tell which one\u2019s image quality is better, in this case both models will get +0.5 grade.\nThe detailed comparison statistics are shown in Table 8, where we report the grades of HyperHu-\nman vs. baseline methods. It can be clearly seen that our proposed framework is superior than all\nthe existing models, with better image quality, realism, aesthetics, and text-image alignment.\nTable 8: Detailed Comparison Statistics in User Study. We conduct a comprehensive user study\non zero-shot MS-COCO 2014 validation human subset with well-trained participants.\nMethods\nSD 2.1\nSDXL\nIF\nHyperHuman\n7350 vs. 886\n4978.5 vs. 3257.5\n6787.5 vs. 1444.5\nMethods\nControlNet\nT2I-Adapter\nHumanSD\nHyperHuman\n7604 vs. 632\n8076 vs. 160\n8160 vs. 76\n18\nPublished as a conference paper at ICLR 2024\nA.5\nIMPACT OF RANDOM SEED AND MODEL ROBUSTNESS\nTo further validate our model\u2019s robustness to the impact of random seed, we inference with the same\ninput conditions (i.e., text prompt and pose skeleton) and use different random seeds for generation.\nThe results are shown in Fig. 5, which suggest that our proposed framework is robust to generate\nhigh-quality and text-aligned human images over multiple arbitrary random seeds.\nA bearded, bald man wears a multicolored tie.\nA smiling man in a skiing outfit holds his skis and poles\nA handsome man holds a glass of white wine and looks at the camera.\nFigure 5: Impact of Random Seed and Model Robustness. We use the same input text prompt\nand pose skeleton with different random seeds to generate multiple results. The results suggest that\nour proposed framework is robust to generate high-quality and text-aligned human images.\nA.6\nBOARDER IMPACT AND ETHICAL CONSIDERATION\nGenerating realistic humans conditioned on text benefits a wide range of applications. It enriches\ncreative domains such as art, design, and entertainment by enabling the creation of highly realis-\ntic and emotionally resonant visuals. Besides, it streamlines design processes, reducing time and\nresources needed for tasks like content production. However, it could be misused for malicious\npurposes like deepfake or forgery generation. We believe that the proper use of this technique will\nenhance the machine learning research and digital entertainment. We also advocate all the generated\nimages should be labeled as \u201csynthetic\u201d to avoid negative social impacts.\nA.7\nMODEL ROBUSTNESS ON UNSEEN AND CHALLENGING POSE\nIn this section, we show the robustness of HyperHuman to generalize to unseen or challenging\nposes. Specifically, we choose an acrobatic-related image from the Human-Art dataset (Ju et al.,\n2023a), which is a highly challenging and rare pose unseen from the common human-centric images.\n19\nPublished as a conference paper at ICLR 2024\nThe results are shown in Fig. 6. In the visualized results, (a) is the ground-truth image from the\nHuman-Art dataset; (b) is the associated pose skeleton, which is challenging and unseen; (c), (d), (e),\nand (f) are four generated images from our proposed framework. It can be seen that HyperHuman\nis robust to unseen poses, even for the rare acrobatic case.\n(a) Ground Truth Image\nAcrobatics of a woman doing a handstand on a wooden floor.\n(b) Challenging Pose\n(c) Our Results - 1\n(d) Our Results - 2\n(e) Our Results - 3\n(f) Our Results - 4\nFigure 6: Model Robustness on Unseen and Challenging Pose. We show multiple high-quality\ngeneration results on the unseen acrobatic pose, which shows the robustness of our method.\nA.8\nPOTENTIAL OPTIMIZATION FOR ANNOTATION AND TRAINING\nFrom the perspective of optimizing training: 1) We can change our models into a smaller diffusion\nbackbone to save the training and memory cost, e.g., Small SD and Tiny SD (Kim et al., 2023),\nwhich achieve on-par performance with Stable Diffusion, but lighter and faster in training and in-\nference. 2) We can leverage some efficient parameter finetuning techniques like LoRA (Hu et al.,\n2021) and Adapter (Houlsby et al., 2019) to finetune the shared backbone with fewer parameters.\n3) We can adopt some common engineering tricks to reduce memory consumption, e.g., gradient\ncheckpointing, gradient accumulation with smaller batch size, deepspeed model parallelism, lower\nfloating point precision like fp16, efficient xformers, etc.\nFrom the perspective of optimizing annotation: 1) Our efficient architecture design (only add\nlightweight branches) can actually produce reasonable results with smaller dataset scale and fewer\ntraining iterations, capturing the joint distribution of RGB, depth, and surface-normal. Before the\nlarge-scale training, we first verify method effectiveness on a small-scale 1M subset, which is less\nthan 3% of the HumanVerse fullset scale. In spite of this, we can still obtain good results with only\n8 40GB A100 within one day, generating spatially aligned results for each modality. A generation\nsample is shown in Fig. 7, where (a) is the conditioning pose skeleton; (b), (c), and (d) are the si-\nmultaneously denoised depth map, surface-normal map, and RGB images. Note that since this is an\nearly-stage experiment, the pose conditioning and visualization are little bit different from the final\nversion we have used. In spite of this, we manage to achieve simultaneous denoising of multiple\nmodalities with a much smaller dataset scale. 2) The annotation overhead mostly comes from the\ndiffusion-based image outpainting process (Sec. 4), while the cost for depth and normal estimation is\n20\nPublished as a conference paper at ICLR 2024\nrelatively low. Though facilitating more accurate pose annotations, it is not a mandatory step. More-\nover, in the final evaluation process, we use the raw human pose without the help of outpainting, but\ncan still achieve superior performance.\n(a) Pose Skeleton\nTwo beautiful women walking on the street.\n(b) Denoised Depth\n(c) Denoised Surface-Normal\n(d) Denoised RGB\nFigure 7: An Early-Stage Generation Sample on Small-Scale Dataset. We show a generation\nsample on a small-scale 1M subset, which is less than 3% of the HumanVerse fullset scale. Note\nthat since this is an early-stage experiment, the pose conditioning and visualization are little bit\ndifferent from the final version we have used.\nA.9\nMODEL PERFORMANCE WITHOUT INPUT POSE\nIn this section, we show the unconditional generation results of our model, where no pose input is\ntaken. The generated images are shown in Fig. 8. All the text prompts are from the zero-shot MS-\nCOCO 2014 Human Validation dataset, which is unseen during the model training process. Thanks\nto our framework design of robust conditioning scheme, the model is trained to predict reasonable\ndenoising results, even when the conditions are dropout or masked. Therefore, we manage to create\nrealistic human images with superior performance even without the pose skeleton as input.\nA.10\nMODEL PERFORMANCE ON JITTERED POSE AND IMAGE ANIMATION\nWe show additional results on the jittered human poses in Fig. 9. Specifically, we first condition on\nthe original pose skeleton (a) and obtain the generated image (b) based on text prompt \u201cA woman\nstanding near a lake with a snow capped mountain behind\u201d. Then we gradually add Gaussian noise\nto all the joints, from the sigma scale of 2.5 to 12.5. It can be seen that HyperHuman could produce\npleasant results under Gaussian noises to all joints, creating highly pose-aligned images.\nTo further verify if we can animate a certain image by gradually changing the input pose, we fix the\nrandom seed, the initial starting noise xT , and text prompt. The sequential generation results are\nshown in Fig. 10. Note that we fix the text prompt of \u201cA woman standing near a lake with a snow\ncapped mountain behind\u201d. The input skeleton are shifted towards the right side, each step by 10\npixels. Even though we maintain other conditions fixed, we can still see background and appearance\nchanges. We regard this as a promising research problem and will explore it in future work.\nA.11\nMORE FIRST-STAGE GENERATION RESULTS\nWe show more first-stage Latent Structural Diffusion Model generation results in Fig. 11, where the\nspatially aligned RGB images, depth maps, and surface-normal maps are simultaneously denoised\nand generated. Though not as high-quality as the final output from the second-stage pipeline, it can\nstill generate plausible humans with coherent structures.\nA.12\nDETAILED INTUITION OF UPDATED NOISE SCHEDULE\nFirst, it is hard to finetune the Stable Diffusion to generate pure-color images. As shown in the\npaper (Lin et al., 2023), we can not even overfit to a single solid-black image with the text prompt\nof \u201dSolid black background\u201d. The main reason is that common diffusion noise schedules are flawed,\nwhich corrupts image incompletely when sampling t = T at the training phase: xT = \u03b1T \u00b7x0+\u03c3T \u00b7\u03f5,\n21\nPublished as a conference paper at ICLR 2024\nA man sitting down with a brown \nteddy bear on his shoulders.\nTwo women holding surfboards while \nsmiling at the camera.\nA guy in a brown jacket standing near a \nsign holding a cellphone to his ear.\nA woman poses with avocado \nsandwich lunch at an outdoor \nrestaurant.\nAn elderly woman looks to the side as\nshe sits in front of a cheese pizza in a \nrestaurant.\nA picture of a man with suit, \ntie and wild hair.\nFigure 8: Unconditional Generation Results without Input Pose. All the text prompts are from\nthe zero-shot MS-COCO 2014 Human Validation dataset.\nbut \u03b1T \u0338= 0, \u03c3T \u0338= 1. Due to this reason, a small amount of signal is still included, which leaks the\nlowest frequency information such as the overall mean of each channel. In contrast, at the inference\nstage, the sampling starts from a pure Gaussian noise, which has a zero mean. Such train-test gap\nhinders SD from generating pure-color images.\nSecond, similar to pure color images, the depth and surface-normal maps are visualized based on\ncertain scheme, where its color and patterns are highly constrained. For example, the depth map is\ngrey-scale image without colorful textures, and current estimators tend to infer similar depth values\nfor each local patch. Therefore, the low frequency information of per-channel mean and standard\ndeviation could be misused by network as shortcut for denoising, which harms the joint learning of\nmultiple modalities (RGB, depth, and surface-normal). Motivated by this, we propose to enforce the\nzero-terminal SNR (xT = 0.0\u00b7x0+1.0\u00b7\u03f5, that is, \u03b1T = 0, \u03c3T = 1) to fully eliminate low-frequency\ninformation at the training stage, so that we manage generate both RGB images and structural maps\nof high quality at the inference stage.\nA.13\nMORE DETAILS ON POSE PROCESSING AND ENCODING\nThe encoder used for pose is the pretrained VAE encoder of Stable Diffusion, which is the same as\nthe encoder used for RGB, depth, and surface-normal maps. Before pose encoding, we visualize\nthe body keypoints on a black canvas to form a skeleton map, similar to previous controllable meth-\nods (Zhang & Agrawala, 2023; Mou et al., 2023; Ju et al., 2023b) with pose condition. Specifically,\nwe use exactly the same pose drawing method as HumanSD (Ju et al., 2023b) and T2I-Adapter (Mou\net al., 2023) to ensure fairness.\n22\nPublished as a conference paper at ICLR 2024\n(a) Original Pose\nA woman standing near a lake with a snow capped mountain behind.\n(b) Original Generation\n(c) Noisy Pose, sigma=2.5\n(e) Noisy Pose, sigma=5.0\n(f) Generation, sigma=5.0\n(g) Noisy Pose, sigma=7.5\n(d) Generation, sigma=2.5\n(h) Generation, sigma=7.5\n(i) Noisy Pose, sigma=10.0\n(j) Generation, sigma=10.0\n(k) Noisy Pose, sigma=12.5\n(l) Generation, sigma=12.5\nFigure 9: Generation Results under the Jittered Poses. We use the text prompt \u201cA woman stand-\ning near a lake with a snow capped mountain behind\u201d and gradually add Gaussian noise to all the\njoints, from the sigma scale of 2.5 to 12.5.\nA.14\nVAE RECONSTRUCTION PERFORMANCE ON MODALITY-SPECIFIC INPUT\nWe use an improved auto-encoder of the pretrained Stable Diffusion \u201csd-vae-ft-mse\u201d1 as VAE to\nencode inputs from all the modalities, including RGB, depth, surface-normal, and body skeleton\nmaps. To further validate that RGB VAE can be directly used for other structural maps, we exten-\nsively evaluate the reconstruction metrics of all the involved structural maps on 100k samples. The\nresults are reported in Tab. 9, which show that the pre-trained RGB VAE is robust enough to handle\ndifferent modality images, including the structural maps we use in this work. Besides, we addition-\nally show some visualized reconstruction samples in Fig. 12, where in each group, the first row is the\ninput structural maps, and the second row is the reconstructed structural maps from the pretrained\nRGB VAE. Therefore, both the quantitative metrics and visual results show that the pretrained RGB\nVAE is robust enough to faithfully reconstruct structural maps.\nTable 9: RGB VAE Reconstruction Performance. We evaluate the reconstruction performance of\nthe pretrained RGB VAE on the depth and surface-normal maps.\nModality\nrFID \u2193\nPSNR \u2191\nSSIM \u2191\nPSIM \u2193\nBody Skeleton\n0.49\n39.24\n0.96\n0.188\nMiDaS Depth\n0.19\n47.08\n0.99\n0.004\nSurface-Normal\n0.24\n40.11\n0.97\n0.010\n1https://huggingface.co/stabilityai/sd-vae-ft-mse\n23\nPublished as a conference paper at ICLR 2024\nA woman standing near a lake with a snow capped mountain behind.\nFigure 10: Animation Results. We gradually shift skeleton to right side, each step by 10 pixels.\nA.15\nMORE COMPARISON RESULTS\nWe additionally compare our proposed HyperHuman with recent open-source general text-to-\nimage models and controllable human generation baselines, including ControlNet (Zhang &\nAgrawala, 2023), T2I-Adapter (Mou et al., 2023), HumanSD (Ju et al., 2023b), SD v2.1 (Rombach\net al., 2022), DeepFloyd-IF (DeepFloyd, 2023), SDXL 1.0 w/ refiner (Podell et al., 2023). Besides,\nwe also compare with the concurrently released T2I-Adapter+SDXL2. We use the officially-released\nmodels to generate high-resolution images of 1024 \u00d7 1024 for all methods. The results are shown\nin Fig. 13, 14, 15, and 16, which demonstrates that we can generate humans of high realism.\nA.16\nADDITIONAL QUALITATIVE RESULTS\nWe further inference on the challenging zero-shot MS-COCO 2014 validation human subset prompts\nand show additional qualitative results in Fig. 17, 18, and 19. All the images are in high resolution\nof 1024 \u00d7 1024. It can be seen that our proposed HyperHuman framework manages to synthesize\nrealistic human images of various layouts under diverse scenarios, e.g., different age groups of baby,\nchild, young people, middle-aged people, and old persons; different contexts of canteen, in-the-wild\nroads, snowy mountains, and streetview, etc. Please kindly zoom in for the best viewing.\n2https://huggingface.co/Adapter/t2iadapter\n24\nPublished as a conference paper at ICLR 2024\nA man standing on a busy sidewalk \nwhile talking on his cellphone.\nA man with glasses and his eyes closed \ndressed in a black shirt and a necktie.\nA person is sitting in front of a camera \nand ready for a game.\nTwo men standing next to each other \nholding hands.\nA woman wearing a wedding gown holding \nflowers is in a car with a man in a suit and tie.\nTwo small young girls hold hands as \nthey look into a bedroom.\nA younger man and an older man \nskiing in the forest.\nA young man is in the middle of \ncutting his hair while posing.\nA man with a button up shirt, sport coat \nand necktie with a knit cap on his head.\nAn individual wearing a comical \nmustache with a serious facial expression. The two young girls are petting the goats. A smiling man in a skiing outfit holds \nhis skis and poles.\nFigure 11: First-Stage Results. We show the jointly denoised RGB, depth, and normal maps.\n25\nPublished as a conference paper at ICLR 2024\n(a) GT Pose\n(b) Recon. Pose\n(c) GT Depth\n(d) Recon. Depth\n(e) GT Normal\n(f) Recon. Normal\nFigure 12: RGB VAE Reconstruction Results. We show the visualized reconstruction results on\ndepth and surface-normal maps.\n26\nPublished as a conference paper at ICLR 2024\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nA man kiteboarding on the ocean on top of a wave.\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nGroup of snowboarders in transportation vehicle near ski area.\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nA man on a motorcycle that is on a road that has grass fields on both sides and a stop sign.\nFigure 13: Additional Comparison Results.\n27\nPublished as a conference paper at ICLR 2024\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nA girl is standing by a storefront while talking on her phone.\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nSmall silver cell phone being held up any person's hand.\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nA man holding various kites next to cabinet.\nFigure 14: Additional Comparison Results.\n28\nPublished as a conference paper at ICLR 2024\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nMastering the art of skateboarding is profoundly beneficial.\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nA woman in an equestrian outfit riding a white horse.\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nA group of men who are standing behind a banner that has various flags on it.\nFigure 15: Additional Comparison Results.\n29\nPublished as a conference paper at ICLR 2024\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nA person is typing on a lap top and there is a person up on screen.\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nA black and white photo of a girl's face with teddy bears behind her.\n(a) HyperHuman (Ours)\n(b) ControlNet\n(c) T2I-Adapter\n(d) HumanSD\n(e) SD v2.1\n(f) DeepFloyd-IF\n(g) SDXL\n(h) T2I-Adapter+SDXL\nSeveral People walking in a city street, one woman wears a head scarf, others have umbrellas.\nFigure 16: Additional Comparison Results.\n30\nPublished as a conference paper at ICLR 2024\nA baby girl with beautiful blue eyes \nstanding next to a brown teddy bear.\nA little girl with wavy hair \nand smile holding a teddy bear.\nA man and woman seated \nat a table in a restaurant.\nA cow laying on the grass behind \na man holding a cup of coffee.\nA young kid stands before a birthday \ncake decorated with captain America.\nA man who is sitting in a bus \nlooking away from the window.\nFigure 17: Additional Qualitative Results on Zero-Shot MS-COCO Validation.\n31\nPublished as a conference paper at ICLR 2024\nA man in a red shirt is holding \na skate board up over his head.\nTwo men who are sitting next to each other \nwith a large pizza in front of them.\nTwo children carry an enormous \nstuffed teddy bear.\nThe upper half of a man posing for a photograph wearing a \nsuit with a blue tie and matching pocket corner.\nAn older man is wearing a funny hat \nin his dining room.\nYoung man on top of a \nsnowboard wearing maroon jacket.\nFigure 18: Additional Qualitative Results on Zero-Shot MS-COCO Validation.\n32\nPublished as a conference paper at ICLR 2024\nMan sitting on brick covered ground, \nappearing dirty and tired.\nA man wearing a purple neck tie and \nglasses while sitting in a car.\nA man standing on \ngrassy area next to trees.\nA girl with blue hair is \ntaking a self portrait.\nA man wearing a helmet is sitting \non his blue motorcycle.\nA person dressed up taking a picture \nat a street with his fist up.\nFigure 19: Additional Qualitative Results on Zero-Shot MS-COCO Validation.\n33\nPublished as a conference paper at ICLR 2024\nA.17\nLICENSES\nImage Datasets:\n\u2022 LAION-5B3 (Schuhmann et al., 2022): Creative Common CC-BY 4.0 license.\n\u2022 COYO-700M4 (Byeon et al., 2022): Creative Common CC-BY 4.0 license.\n\u2022 MS-COCO5 (Lin et al., 2014): Creative Commons Attribution 4.0 License.\nPretrained Models and Off-the-Shelf Annotation Tools:\n\u2022 diffusers6 (von Platen et al., 2022): Apache 2.0 License.\n\u2022 CLIP7 (Radford et al., 2021): MIT License.\n\u2022 Stable Diffusion8 (Rombach et al., 2022): CreativeML Open RAIL++-M License.\n\u2022 YOLOS-Tiny9 (Fang et al., 2021): Apache 2.0 License.\n\u2022 BLIP210 (Guo et al., 2023): MIT License.\n\u2022 MMPose11 (Contributors, 2020): Apache 2.0 License.\n\u2022 ViTPose12 (Xu et al., 2022): Apache 2.0 License.\n\u2022 Omnidata13 (Eftekhar et al., 2021): OMNIDATA STARTER DATASET License.\n\u2022 MiDaS14 (Ranftl et al., 2022): MIT License.\n\u2022 clean-fid15 (Parmar et al., 2022): MIT License.\n\u2022 SDv2-inpainting16 (Rombach et al., 2022): CreativeML Open RAIL++-M License.\n\u2022 SDXL-base-v1.017 (Podell et al., 2023): CreativeML Open RAIL++-M License.\n\u2022 Improved Aesthetic Predictor18: Apache 2.0 License.\n3https://laion.ai/blog/laion-5b/\n4https://github.com/kakaobrain/coyo-dataset\n5https://cocodataset.org/#home\n6https://github.com/huggingface/diffusers\n7https://github.com/openai/CLIP\n8https://huggingface.co/stabilityai/stable-diffusion-2-base\n9https://huggingface.co/hustvl/yolos-tiny\n10https://huggingface.co/Salesforce/blip2-opt-2.7b\n11https://github.com/open-mmlab/mmpose\n12https://github.com/ViTAE-Transformer/ViTPose\n13https://github.com/EPFL-VILAB/omnidata\n14https://github.com/isl-org/MiDaS\n15https://github.com/GaParmar/clean-fid\n16https://huggingface.co/stabilityai/stable-diffusion-2-inpainting\n17https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\n18https://github.com/christophschuhmann/improved-aesthetic-predictor\n34\n"
  },
  {
    "title": "MotionDirector: Motion Customization of Text-to-Video Diffusion Models",
    "link": "https://arxiv.org/pdf/2310.08465.pdf",
    "upvote": "13",
    "text": "MOTIONDIRECTOR:\nMOTION CUSTOMIZATION\nOF\nTEXT-TO-VIDEO DIFFUSION MODELS\nRui Zhao1, Yuchao Gu1, Jay Zhangjie Wu1, David Junhao Zhang1, Jia-Wei Liu1,\nWeijia Wu3, Jussi Keppo2, Mike Zheng Shou1\u2217\n1Show Lab, 2National University of Singapore\n3Zhejiang University\nhttps://showlab.github.io/MotionDirector\nABSTRACT\nLarge-scale pre-trained diffusion models have exhibited remarkable capabilities\nin diverse video generations. Given a set of video clips of the same motion con-\ncept, the task of Motion Customization is to adapt existing text-to-video diffusion\nmodels to generate videos with this motion. For example, generating a video\nwith a car moving in a prescribed manner under specific camera movements to\nmake a movie, or a video illustrating how a bear would lift weights to inspire cre-\nators. Adaptation methods have been developed for customizing appearance like\nsubject or style, yet unexplored for motion. It is straightforward to extend main-\nstream adaption methods for motion customization, including full model tuning,\nparameter-efficient tuning of additional layers, and Low-Rank Adaptions (Lo-\nRAs). However, the motion concept learned by these methods is often coupled\nwith the limited appearances in the training videos, making it difficult to gener-\nalize the customized motion to other appearances. To overcome this challenge,\nwe propose MotionDirector, with a dual-path LoRAs architecture to decouple\nthe learning of appearance and motion. Further, we design a novel appearance-\ndebiased temporal loss to mitigate the influence of appearance on the temporal\ntraining objective. Experimental results show the proposed method can gener-\nate videos of diverse appearances for the customized motions. Our method also\nsupports various downstream applications, such as the mixing of different videos\nwith their appearance and motion respectively, and animating a single image with\ncustomized motions. Our code and model weights will be released.\n1\nINTRODUCTION\nText-to-video diffusion models (Ho et al., 2022; Singer et al., 2022; He et al., 2022) are approaching\ngenerating high-quality diverse videos given text instructions. The open-sourcing of foundational\ntext-to-video models (Wang et al., 2023a; Sterling, 2023) pre-trained on large-scale data has sparked\nenthusiasm for video generation in both the community and academia. Users can create videos that\nare either realistic or imaginative simply by providing text prompts. While foundation models gen-\nerate diverse videos from the same text, adapting them to generate more specific content can better\naccommodate the preferences of users. Similar to the customization of text-to-image foundation\nmodels (Ruiz et al., 2023), tuning video foundation models to generate videos of a certain concept\nof appearance, like subject or style, has also been explored (He et al., 2022). Compared with im-\nages, videos consist of not just appearances but also motion dynamics. Users may desire to create\nvideos with specific motions, such as a car moving forward and then turning left under a predefined\ncamera perspective, as illustrated on the right side in Fig. 1. However, customizing the motions in\nthe text-to-video generation is still unexplored.\nThe task of Motion Customization is formulated as follows: given reference videos representing a\nmotion concept, the objective is to turn the pre-trained foundation models into generating videos\nthat exhibit this particular motion. In contrast, previous works on appearance customization adapt\n\u2217Corresponding Author.\n1\narXiv:2310.08465v1  [cs.CV]  12 Oct 2023\nFigure 1: Motion customization of the text-to-video diffusion model.\nthe foundation models to generate samples with desired appearance, like subject or style, given ref-\nerence videos or images representing such appearance (Ruiz et al., 2023; He et al., 2022). It is\nstraightforward to use previous adaption methods for motion customization. For example, on the\ngiven reference videos, fine-tuning the weights of foundation models (Ruiz et al., 2023), parameter-\nefficient tuning additional layers (Wu et al., 2022), or training Low-Rank Adaptions (LoRAs) (Hu\net al., 2021) injected in the layers of foundation models. However, customizing diffusion models\nto generate desired motions without harming their appearance diversity is challenging because the\nmotion and appearance are coupled with each other at the step-by-step denoising stage. Directly\ndeploying previous adaption methods to learn motions makes the models fit the limited appearances\nseen in the reference videos, posing challenges in generalizing the learned motions to various ap-\npearances. Recent works on controllable text-to-video generations (He et al., 2022; Esser et al.,\n2023; Wang et al., 2023b) generate videos controlled by signals representing pre-defined motions.\nHowever, the control signals, such as depth maps or edges, impose constraints on the shapes of\nsubjects and backgrounds, thus influencing the appearance of generated videos in a coupled way.\nBesides, these methods accept only one sequence of control signals to generate one video, which\nmay not be suitable for users seeking certain motion types without strict spatial constraints, such as\nthe example of lifting weights in Fig. 1.\nTo achieve motion customization of text-to-video diffusion models while preserving appearance\ndiversity, we propose the MotionDirector, which tunes the foundation models to learn the appearance\nand motions in the given single or multiple reference videos in a decoupled way. MotionDirector\ntunes the models with low-rank adaptions (LoRAs) while keeping their pre-trained parameters fixed\nto retain the learned generation knowledge. Specifically, the MotionDirector employs a dual-path\narchitecture, as shown in Fig. 3. For each video, a spatial path consists of a foundation model with\ntrainable spatial LoRAs injected into its spatial transformer layers. These spatial LoRAs are trained\non a single frame randomly sampled per training step to capture the appearance characteristics of\nthe input videos. The temporal path, on the other hand, is a replica of the foundation model that\nshares the spatial LoRAs with the spatial path to fit the appearance of the corresponding input video.\nAdditionally, the temporal transformers in this path are equipped with temporal LoRAs, which are\ntrained on multiple frames of input videos to capture the underlying motion patterns. To further\nenhance the learning of motions, we propose an appearance-debiased temporal loss to mitigate the\ninfluence of appearance on the temporal training objective.\nOnly deploying the trained temporal LoRAs enables the foundation model to generate videos of the\nlearned motions with diverse appearances, as shown in the second row of Fig 2. The decoupled\nparadigm further makes an interesting kind of video generation feasible, which is the mix of the\nappearance from one video with the motion from another video, called the mix of videos, as shown\nin the third row of Fig 2. The key to this success lies in that MotionDirector can decouple the\nappearance and motion of videos and then combine them from various source videos. It is achieved\n2\nFigure 2: (Row 1) Take two videos to train the proposed MotionDirector, respectively. (Row 2)\nMotionDirector can generalize the learned motions to diverse appearances. (Row 3) MotionDirector\ncan mix the learned motion and appearance from different videos to generate new videos. (Row 4)\nMotionDirector can animate a single image with learned motions.\nby injecting spatial LoRAs trained on one video and temporal LoRAs trained on another video into\nthe foundation model. Besides, the learned motions can be deployed to animate images, as images\ncan be treated as appearance providers, as shown in the last row of Fig 2.\nWe conducted experiments on two benchmarks with 86 different motions and over 600 text prompts\nto test proposed methods, baselines, and comparison methods. The results show our method can be\napplied to different diffusion-based foundation models and achieve motion customization of various\nmotion concepts. On the UCF Sports Action benchmark, which includes 95 videos for 12 types\nof motion concepts and 72 labeled text prompts, human raters preferred MotionDirector for higher\nmotion fidelity at least 75% of the time, significantly outperforming the 25% preferences of base\nmodels. On the LOVEU-TGVE-2023 benchmark, which includes 76 reference videos and 532 text\nprompts, MotionDirector outperforms controllable generation methods and the tuning-based method\nby a large margin, especially in the human preference for appearance diversity. Compared with these\nmethods, our method avoids fitting the limited appearance of reference videos, and can generalize\nthe learned motions to diverse appearances.\nOur contributions are summarized as follows:\n\u2022 We introduce and define the task of Motion Customization. The challenge lies in general-\nizing the customized motions to various appearances.\n\u2022 We propose the MotionDirector with a dual-path architecture and a novel appearance-\ndebiased temporal training objective, to decouple the learning of appearance and motion.\n\u2022 Experiments on two benchmarks demonstrate that MotionDirector can customize various\nbase models to generate diverse videos with desired motion concepts, and outperforms\ncontrollable generation methods and tuning-based methods.\n2\nRELATED WORK\nText-to-Video Generation. To achieve high-quality video generation, various methods have been\ndeveloped, such as Generative Adversarial Networks (GANs) (Vondrick et al., 2016; Saito et al.,\n2017; Tulyakov et al., 2018; Balaji et al., 2019; Tian et al., 2020; Shen et al., 2023), autoregressive\n3\nmodels (Srivastava et al., 2015; Yan et al., 2021; Le Moing et al., 2021; Hong et al., 2022; Ge et al.,\n2022) and implicit neural representations (Yu et al., 2021; Skorokhodov et al., 2021). Diffusion-\nbased models (Ni et al., 2023; Yu et al., 2023; Mei & Patel, 2023; Voleti et al., 2022) are also\napproaching high-quality generation by training conditional 3D U-Nets to denoise from randomly\nsampled sequences of Gaussian noises. Recent foundation models (Ho et al., 2022; Singer et al.,\n2022; He et al., 2022; Luo et al., 2023; Blattmann et al., 2023; Zhang et al., 2023; Wang et al., 2023c)\nare pre-trained on large-scale image and video datasets (Schuhmann et al., 2022; Deng et al., 2009;\nBain et al., 2021), to learn powerful generation ability. Some works turn text-to-image foundation\nmodels to text-to-video generation by manipulation on cross-frame attention or training additional\ntemporal layers, like Tune-A-Video (Wu et al., 2022), Text2Video-Zero(Khachatryan et al., 2023),\nand AnimiteDiff (Guo et al., 2023). The recently open-sourced foundation models (Wang et al.,\n2023a; Sterling, 2023) have ignited enthusiasm among users to generate realistic or imaginative\nvideos, and further make it possible for users to customize and build their own private models.\nGeneration Model Customization. Customizing the pre-trained large foundation models can fit\nthe preferences of users better while maintaining powerful generation knowledge without training\nfrom scratch. Previous customization methods for text-to-image diffusion models (Ruiz et al., 2023;\nKumari et al., 2023; Gu et al., 2023; Chen et al., 2023b; Wei et al., 2023; Smith et al., 2023) aim\nto generate certain subjects or styles, given a set of example images. Dreambooth (Ruiz et al.,\n2023) or LoRA (Hu et al., 2021) can be simply applied to customizing video foundation models\nto generate videos with certain subjects or styles, given a set of reference video clips or images.\nThe recently proposed VideoCrafter (He et al., 2023) has explored this, which we categorize as\nappearance customization. In addition to appearances, videos are also characterized by the motion\ndynamics of subjects and camera movements across frames. However, to the best of our knowledge,\ncustomizing the motions in generation for text-to-video diffusion models is still unexplored.\nControllable Video Generation. Controllable generation aims to ensure the generation results align\nwith the given explicit control signals, such as depth maps, human pose, optical flows, etc. (Zhang\n& Agrawala, 2023; Zhao et al., 2023; Ma et al., 2023). For the controllable text-to-video generation\nmethods, i.e. the VideoCrafter (He et al., 2022), VideoComposer (Wang et al., 2023b), Control-A-\nVideo (Chen et al., 2023a), they train additional branches that take condition signals to align the\ngenerated videos with them. Unlike the human poses for specifically controlling the generation of\nhuman bodies, the general control singles, such as depth maps, are typically extracted from reference\nvideos and are coupled with both appearance and motion. This results in the generation results being\ninfluenced by both the appearance and motion in reference videos. Applying these methods directly\nin motion customization is challenging when it comes to generalizing the desired motions to diverse\nappearances.\n3\nMETHODOLOGY\n3.1\nPRELIMINARIES\nVideo Diffusion Model. Video diffusion models train a 3D U-Net to denoise from a randomly\nsampled sequence of Gaussian noises to generate videos, guided by text prompts. The 3D U-net ba-\nsically consists of down-sample, middle, and up-sample blocks. Each block has several convolution\nlayers, spatial transformers, and temporal transformers as shown in Fig 3. The 3D U-Net \u03f5\u03b8 and a\ntext encoder \u03c4\u03b8 are jointly optimized by the noise-prediction loss, as detailed in (Dhariwal & Nichol,\n2021):\nL = Ez0,y,\u03f5\u223cN (0,I ),t\u223cU(0,T)\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, \u03c4\u03b8(y))\u22252\n2\n\u0003\n,\n(1)\nwhere z0 is the latent code of the training videos, y is the text prompt, \u03f5 is the Gaussian noise added\nto the latent code, and t is the time step. As discussed in (Dhariwal & Nichol, 2021), the noised\nlatent code zt is determined as:\nzt = \u221a \u00af\u03b1tz0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5, \u00af\u03b1t =\ntY\ni=1\n\u03b1t,\n(2)\nwhere \u03b1t is a hyper-parameter controlling the noise strength.\n4\nFigure 3: The dual-path architecture of the proposed method. All pre-trained weights of the base\ndiffusion model remain fixed. In the spatial path, the spatial transformers are injected with trainable\nspatial LoRAs as shown on the right side. In the temporal path, the spatial transformers are injected\nwith spatial LoRAs sharing weights with those ones in the spatial path, and the temporal transform-\ners are injected with trainable temporal LoRAs.\nLow-Rank Adaption. Low-rank adaption (LoRA) (Hu et al., 2021) was proposed to adapt the pre-\ntrained large language models to downstream tasks. Recently it has been applied in text-to-image\ngeneration and text-to-video generation tasks to achieve appearance customization (Ryu, 2023; He\net al., 2023). LoRA employs a low-rank factorization technique to update the weight matrix W as\nW = W0 + \u2206W = W0 + BA,\n(3)\nwhere W0 \u2208 Rd\u00d7k represents the original weights of the pre-trained model, B \u2208 Rd\u00d7r and A \u2208\nRr\u00d7k represent the low-rank factors, where r is much smaller than original dimensions d and k.\nLoRA requires smaller computing sources than fine-tuning the weights of the entire network like\nDreamBooth (Ruiz et al., 2023), and it is convenient to spread and deploy as a plug-and-play plugin\nfor pre-trained models.\n3.2\nDUAL-PATH LOW-RANK ADAPTIONS\nAt each time-step t, the 3D U-Net \u03f5 takes in the latent code zt \u2208 Rb\u00d7f\u00d7w\u00d7h\u00d7c and the conditional\ninput y (e.g., text), where b, f, w, h, c represents the size of the batch, frame, width, height, and\nchannel dimensions, respectively. The spatial transformers apply spatial self-attention along the\nspatial dimensions w, h to improve the correlation between pixels, and then leverage the cross-\nattention between the latent code and the conditional input y to improve textual alignment. The\ntemporal transformers apply temporal self-attention along the frame dimension f to improve the\ntemporal consistency between frames. However, spatial and temporal information in the latent code\ngradually become coupled with each other during the step-by-step denoising stage. Attempting\nto directly learn and fit the motions in reference videos will inevitably lead to fitting their limited\nappearances. To address this problem, we propose to tune the spatial and temporal transformers\nin a dual-path way to learn the earn the appearance and motion in reference videos, respectively,\nas shown in Fig. 3. Specifically, for the spatial path, we inject LoRAs into spatial transformers to\nlearn the appearance of training data, and for the temporal path, we inject LoRAs into temporal\ntransformers to learn the motion in videos.\nSpatial LoRAs Training. For the spatial path, we inject unique spatial LoRAs into the spatial\ntransformers for each training video while keeping the weights of pre-trained 3D U-Net fixed. To\nmaintain the learned strong and diverse textual alignment ability, we do not inject LoRAs into cross-\nattention layers of spatial transformers, since their weights influence the correlations between the\npixels and text prompts. On the other hand, we inject LoRAs into spatial self-attention layers and\nfeed-forward layers to update the correlations in spatial dimensions to enable the model to recon-\nstruct the appearance of training data. For each training step, the spatial LoRAs are trained on\n5\nFigure 4: (a) Four example videos (the same as the videos in the first and fourth rows of Fig. 2) and\ntheir relationships in terms of motion and appearance. (b) We inverse the four videos based on the\nvideo diffusion model and visualize the denoising process. Each point corresponds to a latent code\nzt,i,j at time step t of i-th frame of j-th video. (c) Take latent codes at time step 0 for example,\nthe ones of the same video are connected in order of frames. We find that the internal connectivity\nstructure between latent codes is more influenced by motion, while the distance between sets of\nlatent codes is primarily affected by the difference in appearance. (d) The latent codes are debiased\nto eliminate the appearance bias among them while retaining their connectivity structure.\na single frame randomly sampled from the training video to fit its appearance while ignoring its\nmotion, based on spatial loss, which is reformulated as\nLspatial = Ez0,y,\u03f5,t,i\u223cU(0,F)\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt,i, t, \u03c4\u03b8(y))\u22252\n2\n\u0003\n,\n(4)\nwhere F is the number of frames of the training data and the zt,i is the sampled frame from the latent\ncode zt.\nTemporal LoRAs Training. For the temporal path, we inject the temporal LoRAs into self-attention\nand feed-forward layers of temporal transformers to update the correlations along the frame dimen-\nsion. Besides, the spatial transformers are injected with LoRAs sharing the same weights learned\nfrom the spatial path, to force the trainable temporal LoRAs to ignore the appearance of the train-\ning data. The temporal LoRAs could be simply trained on all frames of training data based on the\ntemporal loss Lorg-temp, formulated in the same way as equation (1).\nHowever, we notice that the noise prediction, in the temporal path, is still influenced by the appear-\nance to some extent. As illustrated in Fig. 4, when considering the latent codes of each frame zt,iF\ni=1\nas a set of points in the latent space, motion primarily impacts the underlying dependencies between\nthese point sets, whereas the distances between different sets of points are more influenced by ap-\npearance. To further decouple the motion from appearance, we proposed to eliminate the appearance\nbias among the noises and predicted noises, and calculate the appearance-debiased temporal loss on\nthem. The debiasing of each noise \u03f5i \u2208 {\u03f5i}F\ni=1 is as follows,\n\u03d5(\u03f5i) =\np\n\u03b22 + 1\u03f5i \u2212 \u03b2\u03f5anchor,\n(5)\nwhere \u03b2 is the strength factor controlling the decentralized strength and \u03f5anchor is the anchor among\nthe frames from the same training data. In practice, we simply set \u03b2 = 1 and randomly sample\n\u03f5i \u2208 {\u03f5i}F\ni=1 as the anchor. The appearance-debiased temporal loss is reformulated as\nLad-temp = Ez0,y,\u03f5,t\n\u0002\n\u2225\u03d5(\u03f5) \u2212 \u03d5(\u03f5\u03b8(zt, t, \u03c4\u03b8(y)))\u22252\n2\n\u0003\n.\n(6)\nFor temporal LoRAs, the loss function is the combination of temporal loss and decentralized tem-\nporal loss as follows,\nLtemporal = Lorg-temp + Lad-temp.\n(7)\nMotion Customization. In the inference stage, we inject the trained temporal LoRAs into the pre-\ntrained video diffusion model to enable it to generate diverse videos with the learned motion from the\n6\nFigure 5: Qualitative comparison results of motion customization on multiple videos.\ntraining data. If the training data is a single video, the learned motion will be a specific motion, such\nas an object first moving forward and then turning to the left. If the training data is a set of videos,\nthe learned motion will be the motion concept provided by them, like lifting weights or playing golf.\nThe motion concepts can be ones preferred by users or ones that lie in the long-tailed distribution\nthat can not be synthesized well by pre-trained models. Since appearance and motion are decoupled\nby our method, the spatial LoRAs can also be used to influence the appearance of generated videos,\nas shown in Fig. 2. Users can flexibly adjust the influence strength of learned appearance and\nmotion on the generation according to their preferences by simply setting the strength of LoRAs as\nW = W0 + \u03b3\u2206W, where \u03b3 is called the LoRA scale, and \u2206W is the learned weights.\n4\nEXPERIMENTS\n4.1\nMOTION CUSTOMIZATION ON MULTIPLE VIDEOS\nDataset. We conduct experiments on the adapted UCF Sports Action data set (Soomro & Zamir,\n2015), which includes 95 videos of 12 different human motions, like playing golf, lifting weights,\netc. For each type of motion, we label one original text prompt describing the motion, such as \u201ca\nperson is playing golf, side view\u201d. For these motions, we set 72 different text prompts in total as\ninput to generate videos using comparison methods, such as \u201ca monkey is playing golf, side view\u201d.\nComparison Methods. We compare the proposed method with three baselines and the video gen-\neration method Tune-A-Video (Wu et al., 2022) that can be adapted to this task. Tune-A-Video was\ninitially proposed for training temporal modules on a single video to learn its motion information,\nwhile here we adapt it to train on multiple videos. The baseline methods are compared with the\nproposed method on two different foundational text-to-video diffusion models, i.e. the ModelScope\n(Wang et al., 2023a) and the ZeroScope (Sterling, 2023). We employ three baseline methods: the\nfirst is directly applying the vanilla foundation models, the second is tuning the foundation models\nwith LoRAs in a coupled manner, and the third is the proposed dual-path method excluding the\nappearance-debiased temporal loss.\n7\nQualitative Results As shown in Fig. 5, taking a set of videos with motions of playing golf as\ntraining data, the Tune-A-Video fails to generate diverse appearances with the learned motions, like\na monkey playing golf. To compare the baseline methods and proposed method fairly, we feed\nthe same initial Gaussian noise to these methods to generate videos. The pre-trained foundation\nmodel, ZeroScope, correctly generates the appearance but lacks the realistic motion that swings a\ngolf club, as those desired motions in the reference videos. The coupled tuned model could generate\nthe desired motion but the learned motion is coupled with too much appearance information causing\nthe generated subject in the video to be more like a human rather than a monkey. The last two\nrows show that the proposed dual-path LoRAs can avoid hurting the appearance generation and the\nproposed appearance-debiased temporal loss enhances the learning of desired motion better. We\ncould draw a similar conclusion from the second example showing the motion of riding a panda.\nQuantitative Results. We evaluate the methods with automatic evaluations and human evaluations,\nand the results are shown in Table. 1.\nAutomatic Metrics. Following the LOVEU-TGVE competition (Wu et al., 2023), the appearance\ndiversity is computing the average CLIP score (Hessel et al., 2021) between the diverse text prompts\nand all frames of the generated videos, the temporal consistency is the average CLIP score between\nframes, and the Pick Score is the average PickScore (Kirstain et al., 2023) between all frames of\noutput videos.\nHuman Preference. On the Amazon MTurk 1, each generated video is evaluated by 5 human raters\nin terms of appearance diversity, temporal consistency, and motion fidelity, which evaluate whether\nthe generated motion is similar to the references. To simplify the comparison for raters, they are\nasked to compare the results pairwise and select their preferred one, where the videos are shuffled\nand their source methods are anonymous. In Table. 1, the pairwise numbers \u201cp1 v.s. p2\u201d means\np1% results of the first method are preferred while p2% results of the second method are preferred.\nAdditional details are provided in the appendix (Sec. A.4).\nThe evaluation results show that coupled tuning will destroy the appearance diversity of pre-trained\nmodels, while our method will preserve it and achieve the highest motion fidelity.\nTable 1: Automatic and human evaluations results of motion customization on single videos.\nAutomatic Evaluations\nHuman Evaluations\nAppearance\nDiversity (\u2191)\nTemporal\nConsistency (\u2191)\nPick\nScore (\u2191)\nAppearance\nDiversity\nTemporal\nConsistency\nMotion\nFidelity\nTune-A-Video\n28.22\n92.45\n20.20\nv.s. Base Model (ModelScope)\n25.00 v.s. 75.00\n25.00 v.s. 75.00\n40.00 v.s. 60.00\nv.s. Base Model (ZeroScope)\n44.00 v.s. 56.00\n16.67 v.s. 83.33\n53.33 v.s. 46.67\nModelScope\nBase Model\n28.55\n92.54\n20.33\nCoupled Tuned\n25.66 (-2.89)\n90.66\n19.85\nv.s. Base Model (ModelScope)\n23.08 v.s. 76.92\n40.00 v.s. 60.00\n52.00 v.s. 48.00\nw/o AD-Loss\n28.32 (-0.23)\n91.17\n20.34\nv.s. Base Model (ModelScope)\n53.12 v.s. 46.88\n49.84 v.s. 50.16\n62.45 v.s. 37.55\nours\n28.66 (+0.11)\n92.36\n20.59\nv.s. Base Model (ModelScope)\n54.84 v.s. 45.16\n56.00 v.s. 44.00\n75.00 v.s. 25.00\nZeroScope\nBase Model\n28.40\n92.94\n20.76\nCoupled Tuned\n25.52 (-2.88)\n90.67\n19.99\nv.s. Base Model (ZeroScope)\n37.81 v.s. 62.19\n41.67 v.s. 58.33\n54.55 v.s. 45.45\nw/o AD-Loss\n28.61 (+0.21)\n91.37\n20.56\nv.s. Base Model (ZeroScope)\n50.10 v.s. 49.90\n48.00 v.s. 52.00\n58.33 v.s. 41.67\nours\n28.94 (+0.54)\n92.67\n20.80\nv.s. Base Model (ZeroScope)\n52.94 v.s. 47.06\n55.00 v.s. 45.00\n76.47 v.s. 23.53\n4.2\nMOTION CUSTOMIZATION ON A SINGLE VIDEO\nDataset. We conduct the comparison experiments on the open-sourced benchmark released by the\nLOVEU-TGVE competition at CVPR 2023 (Wu et al., 2023). The dataset comprises 76 videos,\neach originally associated with 4 editing text prompts. Additionally, we introduced 3 more prompts\nwith significant changes.\nComparison Methods. We compare the proposed method with SOTA controllable generation meth-\nods, the VideoCrafter (He et al., 2022), VideoComposer (Wang et al., 2023b), and Control-A-Video\n(Chen et al., 2023a), and the tuning-based method Tune-A-Video(Wu et al., 2022). To ensure a fair\ncomparison, we use the depth control mode of controllable generation methods, which is available\nin all of them.\nQualitative and Quantitative Results. As shown in Fig. 6, comparison methods fail to generalize\nthe desired motions to diverse appearances, like the ears of bears and the Arc de Triomphe. In\nTable. 2, we refer to the alignment between the generated videos and the original 4 editing text\nprompts as text alignment, and the alignment with the 3 new text prompts with significant changes\n1https://requester.mturk.com/\n8\nFigure 6: Qualitative comparison results of motion customization on single videos.\nTable 2: Automatic and human evaluations results of motion customization on single videos.\nAutomatic Evaluations\nHuman Evaluations\nText\nAlignment (\u2191)\nAppearance\nDiversity (\u2191)\nTemporal\nConsistency (\u2191)\nPick\nScore (\u2191)\nText\nAlignment\nAppearance\nDiversity\nTemporal\nConsistency\nMotion\nFidelity\nVideoComposer\n27.66\n27.03\n92.22\n20.26\nours v.s. VideoComposer\n54.55 v.s. 45.45\n72.83 v.s. 27.17\n61.57 v.s. 38.43\n61.24 v.s. 38.76\nControl-a-Video\n26.54\n25.35\n92.63\n19.75\nours v.s. Control-A-Video\n68.00 v.s. 32.00\n78.43 v.s. 21.57\n71.28 v.s. 29.72\n56.47 v.s. 43.53\nVideoCrafter\n28.03\n27.69\n92.26\n20.12\nours v.s. VideoCrafter\n52.72 v.s. 47.28\n71.11 v.s. 28.89\n60.22 v.s. 39.78\n60.00 v.s. 40.00\nTune-a-Video\n25.64\n25.95\n92.42\n20.09\nours v.s. Tune-A-Video\n67.86 v.s. 32.14\n69.14 v.s. 30.86\n71.67 v.s. 28.33\n56.52 vs. 43.48\nours\n27.82\n28.48\n93.00\n20.74\nas appearance diversity. The results show that our method outperforms other methods by a large\nmargin when generalizing the motions to diverse appearances, and achieves competitive motion\nfidelity.\n4.3\nEFFICIENCY PERFORMANCE\nThe lightweight LoRAs enable our method to tune the foundation models efficiently. Taking the\nfoundation model ZeroScope for example, it has over 1.8 billion pre-trained parameters. Each set\nof trainable spatial and temporal LoRAs only adds 9 million and 12 million parameters, respec-\ntively. Requiring 14 GB VRAM, MotionDirector takes 20 minutes to converge on multiple refer-\nence videos, and 8 minutes for a single reference video, competitive to the 10 minutes required by\nTuna-A-Video (Wu et al., 2022). Additional details are provided in the appendix (Sec. A.2).\n5\nLIMITATIONS AND FUTURE WORKS\nDespite the MotionDiector can learn the motions of one or two subjects in the reference videos, it\nis still hard to learn complex motions of multiple subjects, such as a group of boys playing soccer.\nPrevious appearance customization methods suffer similar problems when generating multiple cus-\ntomized subjects (Gu et al., 2023). A possible solution is to further decouple the motions of different\nsubjects in the latent space and learn them separately.\n6\nCONLCUSION\nWe introduce and formulate the task of Motion Customization, which is adapting the pre-trained\nfoundation text-to-video diffusion models to generate videos with desired motions. The challenge\nof this task is generalizing the customized motions to various appearances. To overcome this chal-\nlenge, we propose the MotionDirector with a dual-path architecture and a novel appearance-debiased\n9\ntemporal training objective to decouple the learning of appearance and motion. Experimental results\nshow that MotionDirector can learn either desired motion concepts or specific motions of subjects\nand cameras, and generalize them to diverse appearances. The automatic and human evaluations on\ntwo benchmarks demonstrate the MontionDirector outperforms other methods in terms of appear-\nance diversity and motion fidelity.\n7\nREPRODUCIBILITY STATEMENT\nWe make the following efforts to ensure the reproducibility of MotionDirector: (1) Our training\nand inference codes together with the trained model weights will be publicly available. (2) We\nprovide training details in the appendix (Sec.A.2). (3) The reference videos in the two benchmarks\nare publicly accessible, and we will release the labeled text prompts. More details are provided in\nthe appendix (Sec.A.3). (4) We provide the details of the human evaluation setups in the appendix\n(Sec.A.4).\nREFERENCES\nMax Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisserman. Frozen in time: A joint video and\nimage encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 1728\u20131738, 2021. 4\nYogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Conditional\ngan with discriminative filter generation for text-to-video synthesis. In IJCAI, volume 1, pp. 2,\n2019. 3\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler,\nand Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 22563\u201322575, 2023. 4\nWeifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin.\nControl-a-video: Controllable text-to-video generation with diffusion models.\narXiv preprint\narXiv:2305.13840, 2023a. 4, 8\nXi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-\nshot object-level image customization. arXiv preprint arXiv:2307.09481, 2023b. 4\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248\u2013255. Ieee, 2009. 4\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin Neural Information Processing Systems, 34:8780\u20138794, 2021. 4\nPatrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Ger-\nmanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint\narXiv:2302.03011, 2023. 2\nSongwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and\nDevi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer.\narXiv preprint arXiv:2204.03638, 2022. 4\nYuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao,\nRui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized low-rank adaptation for\nmulti-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. 4, 9\nYuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff:\nAnimate your personalized text-to-image diffusion models without specific tuning. arXiv preprint\narXiv:2307.04725, 2023. 4\nYingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion\nmodels for high-fidelity long video generation. 2022. 1, 2, 4, 8\n10\nYingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen.\nVideocrafter:\nA\ntoolkit for text-to-video generation and editing.\nhttps://github.com/AILab-CVC/\nVideoCrafter, 2023. 4, 5\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\nClipscore: A\nreference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 8\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020. 15\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition\nvideo generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1, 4\nWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pre-\ntraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022.\n4\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021. 2, 4, 5\nLevon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang\nWang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models\nare zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. 4\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014. 14\nYuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint\narXiv:2305.01569, 2023. 8\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\ncustomization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pp. 1931\u20131941, 2023. 4\nGuillaume Le Moing, Jean Ponce, and Cordelia Schmid. Ccvs: context-aware controllable video\nsynthesis. Advances in Neural Information Processing Systems, 34:14042\u201314055, 2021. 4\nZhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao,\nJingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video\ngeneration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), June 2023. 4\nYue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Fol-\nlow your pose: Pose-guided text-to-video generation using pose-free videos.\narXiv preprint\narXiv:2304.01186, 2023. 4\nKangfu Mei and Vishal Patel. Vidm: Video implicit diffusion models. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 37, pp. 9117\u20139125, 2023. 4\nHaomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and Martin Renqiang Min. Conditional\nimage-to-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 18444\u201318455, 2023. 4\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500\u2013\n22510, 2023. 1, 2, 4, 5\nSimo Ryu. Low-rank adaptation for fast text-to-image diffusion fine-tuning. https://github.\ncom/cloneofsimo/lora, 2023. 5\n11\nMasaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with sin-\ngular value clipping. In Proceedings of the IEEE international conference on computer vision,\npp. 2830\u20132839, 2017. 3\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models.\nAdvances in Neural\nInformation Processing Systems, 35:25278\u201325294, 2022. 4\nXiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Mostgan-v: Video generation with temporal\nmotion styles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 5652\u20135661, 2023. 3\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry\nYang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video\ndata. arXiv preprint arXiv:2209.14792, 2022. 1, 4\nIvan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video\ngenerator with the price, image quality and perks of stylegan2. arXiv preprint arXiv:2112.14683,\n2021. 4\nJames Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and Hongxia\nJin. Continual diffusion: Continual customization of text-to-image diffusion with c-lora. arXiv\npreprint arXiv:2304.06027, 2023. 4\nKhurram Soomro and Amir R Zamir. Action recognition in realistic sports videos. In Computer\nvision in sports, pp. 181\u2013208. Springer, 2015. 7, 15\nNitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video rep-\nresentations using lstms. In International conference on machine learning, pp. 843\u2013852. PMLR,\n2015. 4\nSpencer Sterling. Zeroscope. https://huggingface.co/cerspense/zeroscope_v2_\n576w, 2023. 1, 4, 7\nYu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey\nTulyakov. A good image generator is what you need for high-resolution video synthesis. In\nInternational Conference on Learning Representations, 2020. 3\nSergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion\nand content for video generation. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 1526\u20131535, 2018. 3\nVikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion\nfor prediction, generation, and interpolation. Advances in Neural Information Processing Systems,\n35:23371\u201323385, 2022. 4\nCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.\nAdvances in neural information processing systems, 29, 2016. 3\nJiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Mod-\nelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023a. 1, 4, 7\nXiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen,\nDeli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion con-\ntrollability. arXiv preprint arXiv:2306.02018, 2023b. 2, 4, 8\nYaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang,\nYinan He, Jiashuo Yu, Pe der Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang,\nCunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Y. Qiao, and Ziwei Liu. Lavie: High-\nquality video generation with cascaded latent diffusion models. 2023c. URL https://api.\nsemanticscholar.org/CorpusID:262823915. 4\n12\nYuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding\nvisual concepts into textual embeddings for customized text-to-image generation. arXiv preprint\narXiv:2302.13848, 2023. 4\nJay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan,\nXiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models\nfor text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. 2, 4, 7, 8, 9, 14\nJay Zhangjie Wu, Difei Gao, Jinbin Bai, Mike Shou, Xiuyu Li, Zhen Dong, Aishani Singh, Kurt\nKeutzer, and Forrest Iandola. The text-guided video editing benchmark at loveu 2023. https:\n//sites.google.com/view/loveucvpr23/track4, 2023. 8, 15, 16\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using\nvq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 4\nSihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin.\nGenerating videos with dynamics-aware implicit generative adversarial networks. In International\nConference on Learning Representations, 2021. 4\nSihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in\nprojected latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 18456\u201318466, 2023. 4\nDavid Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei\nGao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-\nvideo generation, 2023. 4\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\narXiv preprint arXiv:2302.05543, 2023. 4\nShihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-\nYee K Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. arXiv preprint\narXiv:2305.16322, 2023. 4\n13\nA\nAPPENDIX\nA.1\nCOMPARISON OF TASKS\nAs shown in Fig. 7, we think video is characterized by two basic aspects, i.e. appearance and mo-\ntion. Each video is represented as a point in Fig. 7, and a set of videos sharing similar motions or\nappearances lie in a close region. The task of appearance customization aims to learn the appear-\nances, like the subjects or styles, in the reference videos, and generate new videos with the learned\nappearances, where the motions can be different from those in the reference videos. The task of\ncontrollable generation aims to generate videos aligned with the given control singles, such as a\nsequence of depth maps or edges. However, these control singles are often coupled with constraints\non the appearances, limiting the appearance diversity of the generated videos. The task of motion\ncustomization aims to learn motions from a single video or multiple videos. If the motion is learned\nfrom a single video, then it will be a specific motion representing the movements of subjects and the\ncamera, while if the motion is learned from multiple videos sharing the same motion concept, then it\nwill be this motion concept, like how to play golf. The most important is that motion customization\naims to generalize the learned motions to various appearances, and we can see a high appearance\ndiversity in the generated results.\nFigure 7: Each video is characterized by two aspects: appearance and motion. We can uniquely\nidentify a video based on its values along the appearance and motion axes, as shown in the lower-\nleft corner of this figure. (a) Appearance customization aims to create videos whose appearances\nlook like reference videos but have different motions. (b) The controllable generation aims to gener-\nate videos with the same motion represented by control signals. However, the control singles often\nhave constraints on appearance, limiting the appearance diversity of the generated results. (c) Mo-\ntion customization on a single video aims to generate videos with the specific motion learned from\nreference videos while keeping the ability to generate highly diverse appearances. (d) Motion cus-\ntomization on multiple videos aims to learn the same motion concepts in the reference videos, such\nas lifting weights or playing golf, and generate videos with these motions and with highly diverse\nappearances.\nA.2\nTRAINING AND INFERENCE DETAILS\nThe LoRAs are trained using Adam optimizer (Kingma & Ba, 2014), with the default betas set to\n0.9 and 0.999, and the epsilon set to the default 1e-8. We set the learning rate to 1e-4, and set the\nweight decay to 5e-4. In addition, we incorporate a dropout rate of 0.1 during the training of LoRAs.\nThe LoRA rank is set to 32. For the temporal training objective, the coefficients of original temporal\nloss and appearance-debiased loss are both set to 1. We sample 16 frames with 384 \u00d7 384 spatial\nresolution and 8 fps from each training data. The number of training steps is set to 400 for motion\ncustomization on a single video and set to 1000 for motion customization on multiple videos. To\nconserve VRAM, we employ mixed precision training with fp16. On an NVIDIA A5000 graphics\ncard, it takes around 8 minutes and 20 minutes for MotionDirector to converge on a single reference\nvideo and multiple videos, respectively. On the same device, it takes 10 minutes for Tune-A-Video\n(Wu et al., 2022) to converge.\n14\nIn the inference stage, the diffusion steps are set to 30 and the classifier-free guidance scale is set to\n12. We use the DDPM Scheduler (Ho et al., 2020) as the time scheduler in the training and inference\nstages.\nA.3\nDETAILS OF BENCHMARKS\nFor the motion customization on multiple videos, we adopt the UCF Sports Action data set (Soomro\n& Zamir, 2015) to conduct experiments. The original data set consists of 150 videos of 10 different\nhuman motions. We remove some inconsistent videos and low-resolution ones and split one motion\nconcept into three motion concepts, i.e. \u201cplaying golf, side view\u201d, \u201cplaying golf, front view\u201d, and\n\u201cplaying golf, back view\u201d, to make them more clear and more precise. Then we get in total of 95\nreference videos of 12 different types of motions, where each of them has 4 to 20 reference videos.\nWe label each type of motion with a text description, like \u201cA person is riding a horse\u201d, and provide\n6 text prompts with significant appearance changes for the generation, like \u201cA monkey is riding a\nhorse\u201d. An example is shown in Fig. 8.\nFor the motion customization on a single video, we adopt the open-sourced benchmark released by\nthe LOVEU-TGVE competition at CVPR 2023 (Wu et al., 2023). The dataset includes 76 videos,\neach originally associated with 4 editing text prompts. The editing prompts focus on object change,\nbackground change, style change, and multiple changes. To further examine the diversity of text-\nguided generation, we add 3 more prompts that have large appearance changes for each video, such\nas changing man to bear or changing the outdoors to the moon. An example is shown in Fig. 8.\nFigure 8: Examples of two benchmarks for testing motion customization on multiple videos and a\nsingle video, respectively.\nA.4\nDETAILS OF HUMAN EVALUATIONS\nThrough the Amazon MTurk platform 2, we release over 1800 comparison tasks and each task\nis completed by 5 human raters. Each task involves a comparison between two generated videos\nproduced by two anonymous methods. These comparisons assess aspects such as text alignment,\ntemporal consistency, and motion fidelity, where the motion fidelity has a reference video for raters\nto compare with. As shown in Fig. 9, each task has three questions for raters to answer: (1) Text\nalignment: Which video better matches the caption \u201c{text prompt}\u201d? (2) Temporal Consistency:\nWhich video is smoother and has less flicker? (3) Motion Fidelity: Which video\u2019s motion is more\nsimilar to the motion of the reference video? Do not focus on their appearance or style, compare\ntheir motion.\n2https://requester.mturk.com/\n15\nFigure 9: Example of one task for 5 human raters on Amazon MTurk to complete. Each task involves\nthree questions comparing two generated results in terms of text alignment, temporal consistency,\nand motion fidelity.\nFor the comparison of motion customization on multiple reference videos, we use \u201cappearance di-\nversity\u201d to refer to \u201ctext alignment\u201d in Table. 1, since we use the text prompts with large appearance\nchanges to test the methods, like changing a human to a panda. For the comparison of motion cus-\ntomization on a single video, the text alignment with the original text prompts of the benchmark\nLOVEU-TGV (Wu et al., 2023) is termed as \u201c\u2019text alignment\u201d in Table. 2, while the text alignment\nwith newly labeled text prompts with significant changes is termed as \u201cappearance diversity\u201d. To\neliminate noise in the rated results, we employ a consensus voting approach to statistically analyze\nthe results, considering a result valid only when at least 3 out of 5 raters reach the same choice.\u201d\nIn the end, the released over 1800 tasks received answers from 421 unique human raters. The\nstatistical results are listed in Table. 1 and Table. 2.\nA.5\nADDITIONAL RESULTS\nWe provide additional results for motion customization on multiple videos, as shown in Fig 10, and\nmotion customization on a single video, as shown in Fig 11. Two more examples of comparison\nof the proposed MotionDirector and other controllable generation methods and the tuning-based\nmethod are shown in Fig 12.\n16\nFigure 10: Results of motion customization of the proposed MotionDirector on multiple reference\nvideos.\n17\nFigure 11: Results of motion customization of the proposed MotionDirector on a single reference\nvideo.\n18\nFigure 12: Comparison of different methods on the task of motion customization given a single\nreference video.\n19\n"
  },
  {
    "title": "EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation",
    "link": "https://arxiv.org/pdf/2310.08185.pdf",
    "upvote": "6",
    "text": "EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form\nNarrative Text Generation\nWang You\u2217\u2020\nWenshan Wu\u2217\u2021\nYaobo Liang\u2217\nShaoguang Mao\nChenfei Wu\nMaosong Cao\u2020\nYuzhe Cai\u2020\nYiduo Guo\u2020\nYan Xia\nFuru Wei\nNan Duan\nMicrosoft Research Asia\nAbstract\nPlan-and-Write is a common hierarchical ap-\nproach in long-form narrative text generation,\nwhich first creates a plan to guide the narrative\nwriting. Following this approach, several stud-\nies rely on simply prompting large language\nmodels for planning, which often yields sub-\noptimal results. In this paper, we propose a\nnew framework called Evaluation-guided Itera-\ntive Plan Extraction for long-form narrative text\ngeneration (EIPE-text), which extracts plans\nfrom the corpus of narratives and utilizes the\nextracted plans to construct a better planner.\nEIPE-text has three stages: plan extraction,\nlearning, and inference. In the plan extraction\nstage, it iteratively extracts and improves plans\nfrom the narrative corpus and constructs a plan\ncorpus. We propose a question answer (QA)\nbased evaluation mechanism to automatically\nevaluate the plans and generate detailed plan\nrefinement instructions to guide the iterative\nimprovement. In the learning stage, we build a\nbetter planner by fine-tuning with the plan cor-\npus or in-context learning with examples in the\nplan corpus. Finally, we leverage a hierarchi-\ncal approach to generate long-form narratives.\nWe evaluate the effectiveness of EIPE-text in\nthe domains of novels and storytelling. Both\nGPT-4-based evaluations and human evalua-\ntions demonstrate that our method can generate\nmore coherent and relevant long-form narra-\ntives. Our code will be released in the future.\n1\nIntroduction\nLarge language models have made impressive\nstrides in text generation, performing well in tasks\nsuch as machine translation, summarization, and\nchat (Chang et al., 2023)(Bubeck et al., 2023).\nHowever, generating long-form narrative remains a\n\u2217Equal contribution\n\u2020Work done during internship at Microsoft Research Asia.\n\u2021Corresponding author: wenswu@microsoft.com\nchallenging task, especially when it comes to main-\ntaining coherence over long ranges and ensuring\nrelevance to an initial premise. This is particularly\ncrucial for applications such as scriptwriting, nov-\nels, business reports, journalism, among others.\nHuman writers often create a plan or out-\nline before beginning to write a narrative, which\nhelps maintain a coherent and logical progression\nthroughout the narrative. Inspired by this, a hierar-\nchical generation approach has been used in many\nworks, such as Re3(Yang et al., 2022), DOC(Yang\net al., 2023), and recurrentGPT(Zhou et al., 2023).\nThese works mainly focus on how to generate the\nfull narrative based on a plan and only generate the\nplan by simply prompting a large language model.\nHowever, the planning ability of LLMs is not good\nenough and requires significant prompting engi-\nneering work. Additionally, it is challenging to\nadapt these models to a specific domain or style of\nlong-form narrative.\nTo address these limitations, we propose the\nEvaluation-Guided Iterative Plan Extraction for\nLong-Form Narrative Text Generation (EIPE-text)\nframework. EIPE-text leverages a learned plan-\nner with enhanced domain expertise to generate a\nhigh-quality plan, as illustrated in figure 1. Specif-\nically, EIPE-text consists of three stages: plan ex-\ntraction, learning, and inference. In the plan extrac-\ntion stage, we iteratively extract and improve plans\nfrom collected narrative corpus to construct a plan\ncorpus for planner learning. To evaluate the qual-\nity of extracted plans and the alignment between\nplans and source narratives, we adopt a QA-based\nself-evaluation mechanism, leveraging the reading\ncomprehension capabilities of LLMs. Based on\nevaluation results, we generate detailed refinement\ninstructions to iteratively improve the plan. In the\nlearning stage, we build a better planner by fine-\ntuning with the plan corpus or in-context learning\nwith examples in the plan corpus, which enhances\nthe ability to generate high-quality plans. During\narXiv:2310.08185v1  [cs.CL]  12 Oct 2023\nPlan Corpus\nGenerated Plan\nNarrative \nGeneration\nPlan Generation\nLLMs\nTopic Query\nGenerated Narrative\nSource Narrative\nPlan Sketching\nQA-pairs Generation\nQA-pairs\nQA-based Evaluation\nRefinement Instructions\nPlan \ud835\udc7b\ud835\udc8a\nUpdated \nPlan \ud835\udc7b\ud835\udc8a+\ud835\udfcf\nLLMs\nA. Plan Extraction\nB. Learning\nC. Inference\nExtracted Plan\nPlan Refinement\nPlan \ud835\udc7b\ud835\udc8a\nPlan \ud835\udc7b\ud835\udc8a+\ud835\udfcf\n\u2026\nPlanner\nFine-Tuning\nIn-context \nLearning\n\ud83d\udd25\n\u2744\ufe0f\nPlanner\nLearning\nFigure 1: A Comprehensive Visual Overview of the EIPE-text Framework. The Plan Extraction stage starts\nwith Plan Sketching, where an initial plan is generated using an LLM. Then, in the QA-pairs Generation step,\na set of QA-pairs is created to evaluate the plan. QA-based Evaluation step evaluates the plan through question\nanswering and generates refinement instructions. In the Plan Refinement step, it iteratively improves the plan based\non the instructions until it passes the evaluation. Plans are then used to construct a plan corpus for the planner in the\nLearning stage. Finally, in the Inference stage, the planner generates a plan, and the narrative is generated from the\nplan.\nthe inference stage, we first generate the plan and\nthen further generate narratives based on the plan.\nWe evaluated the effectiveness of EIPE-text in\nthe domain of novels and storytelling and found\nthat both the fine-tuning based and in-context learn-\ning based planners outperform the baselines. Hu-\nman evaluation also shows that the results of EIPE-\ntext were more coherent and relevant than those of\ncurrent state-of-the-art models.\nOur contributions can be summarized as follows:\n\u2022 We propose a new framework, EIPE-text,\nwhich automatically extracts high-quality\nplans from narrative corpus and learns better\nplanners for long-form narrative text genera-\ntion. This framework can be generalized to all\ndomains.\n\u2022 We propose a QA-based evaluation method to\nautomatically evaluate plans and generate de-\ntailed instructions to improve the plan based\non evaluation results. This QA-based evalu-\nation provides more specific and actionable\nresults than simply leveraging GPT to com-\npare two outputs or provide a score (Liu et al.,\n2023).\n\u2022 We demonstrate the effectiveness of our model\nin the novel and storytelling domains, and we\nwill release the code for future research.\n2\nMethod\nOur methodology contains three stages: plan ex-\ntraction, learning, and inference. The entire pro-\ncess is shown in figure 1. During the plan extrac-\ntion phase, plans are extracted from each narrative\nwithin the corpus. These extracted plans are then\ncompiled to construct the plan corpus. By relying\non the constructed planning corpus, planner can\nlearn to generate high-quality plans. In the infer-\nence stage planner generates a better plan and a\nnarrative will be generated from the plan.\nThe plan extraction stage contains plan sketch-\ning, QA-pairs generation, QA-based evaluation,\nand plan refinement. Initially, we create a tree-\nstructured plan using the LLM in the plan sketching\nstep. Next, during the QA-pairs generation phase,\nwe generate a set of QA-pairs, with each pair corre-\nsponding to a distinct part within the source narra-\ntive. These QA-pairs serve as an evaluation metric\nfor the plan. The QA-based evaluation step eval-\nuates the plan by question answering. For each\nincorrect QA-pair, we generate corresponding in-\nstructions to modify the relevant part of the plan.\nIn the plan refinement step, we integrate these in-\nstructions received in previous steps to update the\nplan. We repeat steps 3 and 4 until the extracted\nplan passes the evaluation.\nIn the learning stage, We leverage the plan ex-\ntracted in the first stage to train an LLM planner.\nTo achieve this, we utilize two strategies: finetun-\ning, as well as in-context learning. These strategies\ncontribute to generating high-quality plans for the\ngiven topic.\nThe inference stage contains two steps: plan\ngeneration and narrative generation. Firstly, the\nplanner takes the topic as input and generates a\ncorresponding plan. Secondly, the narrative will be\ngenerated in the narrative generation step.\n2.1\nPlan Extraction\nFormally, we have a corpus of narrative Cn =\n{n1, n2, ..., nm}.\nThe plan extraction stage ex-\ntracts a plan pi for each narrative ni.\nThe ex-\ntraction results are compiled to a plan corpus\nCp = {p1, p2, ..., pm}. We illustrated the process\nof plan extraction in algorithm 1.\nPlan Sketching.\nFor each narrative, we use LLM\nto extract a tree-structured plan, which serves as\nthe plan sketch. The detailed LLM prompt can be\nfound in appendix A.1. The plan is in a tree struc-\nture and the content of each node is the summariza-\ntion of the corresponding section, subsection, and\nso forth. We show an example of a plan sketch in\nfigure 2.\nQA-pairs Generation.\nFor each narrative, we\ngenerate a set of QA-pairs, with each pair corre-\nsponding to a different segment of the narrative.\nThese QA-pairs can be utilized to evaluate whether\nthe plan includes all aspects of the narrative. Each\nQA-pair is formulated as a multiple-choice prob-\nlem, comprising one question, multiple candidate\nanswers, and multiple correct answer indices. The\nnumber of QA-pairs is proportional to the length of\nthe narrative. To ensure the quality of the generated\nQA-pairs, we employ another LLM to answer these\nquestions based on the original text, filtering out\nany incorrectly answered pairs. The guidelines for\nthis process can be found in appendix A.2.\nQA-base Evaluation.\nWe evaluate a plan using\nQA-pairs and provide detailed refinement instruc-\ntions for refining the plan further. Specifically, we\nutilize LLM to answer questions based on the plan.\nFor each incorrect question, we generate an instruc-\ntion to modify the plan so that the question can be\ncorrectly answered. The modification instruction\ncan be one of the following: (1) add, which inserts\na missing node into the plan; (2) modify, which\nalters the content of a node; (3) adjust, which re-\nlocates a node to another level of the tree, thereby\naltering the tree\u2019s structure. Detailed refinement\ninstructions enable LLM to make precise improve-\nments to specific parts of the plan.\nPlan Refinement.\nIn this step, we incorporate\nthe instructions generated in the previous step to\nimprove the plan. Ideally, we should apply the\nchanges one by one. In order to improve efficiency,\nwe instruct the LLM to apply all instructions simul-\ntaneously. However, the refinement instructions\ngenerated by LLM may not always address the\nincorrect questions. Therefore, we iteratively per-\nform the refinement instructions generation and\nplan refinement steps until the new plan can pass\nthe QA-based evaluation. This process ensures that\nthe final plan has addressed all the identified errors\nand meets the desired quality standards.\nWhile LLM possesses a self-improving ability\nand can refine the plan through simple prompting,\nthe quality of the improvement results may still\nnot be good enough or even worse. Our QA-based\nevaluation, on the other hand, can identify specific\nerrors in the plan and provide refinement instruc-\ntions in the form of instructions to enhance the\nplan. This approach can achieve better refinement\nperformance.\n2.2\nLearning\nDuring the learning phase, we implemented two\nmethods to enhance the performance of the planner:\nthe in-context learning method and the fine-tuning\nmethod.\nThe in-context learning method improves the\nplanner by selecting representative demonstration\nAlgorithm 1: Plan Extraction Algorithm\nInput\n:Cn = {n1, n2, ..., nm}\nOutput :Cp = {p1, p2, ..., pm}\n1 Cp \u2190 \u2205\n2 for i \u2190 1 to m do\n3\np0\ni \u2190 plan_sketching(ni)\n4\nCq \u2190 qa_pairs_generation(ni)\n\u25b7 Cq = {q1, q2, ..., qk} questions set\n5\nt \u2190 0\n\u25b7 t refinement time step\n6\nwhile not pass_evaluation(pt\ni, Cq) do\n7\nCi \u2190 qa_based_evaluation(pt\ni, ni, Cq)\n\u25b7 Ci = {i1, i2, ..., il} refinement instructions set\n8\npt+1\ni\n\u2190 plan_refinement(pt\ni, Ci)\n9\nt \u2190 t + 1\n10\nend\n11\nCp \u2190 Cp \u222a pt\ni\n12 end\nexamples from the plan corpus. By selecting dif-\nferent demonstration examples, the fixed LLM can\nquickly adapt to specific domains or styles.\nOn the other hand, the fine-tuning method can\nfurther improve the planner\u2019s ability by training it\non all plan corpus. This method leverages all the\ndata in the plan corpus and enables the planner to\nadapt to multiple domains simultaneously.\n2.3\nInference\nThe inference stage comprises two steps: plan gen-\neration and narrative generation.\nPlan Generation.\nIn this step, the planner takes\nthe chosen topic as input and produces a corre-\nsponding plan.\nThe planner constructs a well-\nstructured plan that outlines the key elements and\nsections to be covered in the ensuing narrative.\nNarrative Generation.\nThe narrative is gener-\nated from the generated plan in this step. This nar-\nrative seamlessly integrates the content outlined in\nthe plan, ensuring that the resulting narrative is not\nonly logically organized but also rich in detail and\ncontext. The final narrative is a well-rounded piece\nof long-form narrative that effectively conveys the\ninformation related to the chosen topic.\n2.4\nDiscussion\nIn this section, we will discuss how EIPE-text\nworks. Here is our analysis:\nLet q be the premise query. The probability of de-\nsired output based on premise query p(n|q) could\nbe rewritten as\nP(n|q) = P(p|q)P(n|p)\n(1)\nWhen plan p is of high quality, P(n|p) will be high.\nSo as P(p|q) increases, P(n|q) increases too. Our\nframework EIPE-text actually increases P(p|q).\nBesides, the process of plan refinement in fig-\nure 1 could be understood as Reinforcement Learn-\ning(RL), LLM gets observation from answering the\nquestion, and then obtains refinement instructions\naccording to the true or false case. After obtaining\nrefinement instructions, LLM changes the original\nstate to the new state i.e. revise plan. After many\ninteractions with the \"environment\", the \"state\" will\nbe iterated to a suitable \"state\" that can be used to\nimprove P(p|q).\nTo practically exemplify the effectiveness of\nEIPE-text, we conducted a case study of plan gener-\nation through in-context learning with one demon-\nstration. A detailed exploration of this case is pro-\nvided in the Appendix D.2 for interested readers.\n3\nExperiments\nIn this section, we compare EIPE-text in novels\nand storytelling generation with the baselines. All\nexperiments show that EIPE-text is better than the\nbaselines, verifying the effectiveness of our frame-\nwork.\n3.1\nSetup\nFor plan extraction stage, we use Azure Openai\nGPT-4 as our experimental LLM. And for infer-\nence stage, we use the planner to generate a plan\nto further generate the narrative. It should be em-\nphasized that we did not intentionally implement\nthe narrative generation, but modified it based on\nrecurrentGPT, as described in the appendix B.1.\nDataset\nTrain Size\nTest Size\nAvg Length\nMax Length\nTED Talk\n2468\n130\n2078\n9044\nNovel\n1292\n120\n3741\n14493\nTable 1: Comprehensive Dataset Information for TED Talk and Novel.\nNovel genres\nOverall(human)\n\u223c4500words\nInteresting\nCoherent\nRelevant\nEIPE-text (in-context)\n56.7\n64.2\n75.8\nrecurrentGPT\n60.0\n59.2\n62.5\nTable 2: Novel Human Evaluation Results. Pair-wise\ncomparison using human evaluation of EIPE-text with\nrecurrentGPT for 120 novels of different genres. Results\nnever mix numbers from different comparisons\nNovel genres\nOverall(automatic)\n\u223c4500words\nInteresting\nCoherent\nRelevant\nEIPE-text (in-context)\n55.0\n84.2\n92.5\nrecurrentGPT\n58.3\n65.8\n84.2\nTable 3: Novel GPT4 Evaluation Results. Pair-wise\ncomparison using GPT-4 evaluation of EIPE-text with\nrecurrentGPT for 120 novels of different genres. Results\nnever mix numbers from different comparisons\nFor all the settings mentioned in the following\nsection, unless special emphasis, they adhere to\nthe description provided above.\n3.2\nNovel\n3.2.1\nDataset\nNovels are long-form narratives that include intri-\ncate plots, and rich character development. The\nmodel needs to maintain consistency in plots and\ncharacter development and generate interesting sto-\nries. We use the data collected from Project Amer-\nican Literature1, Writing Prompts2 and etc. Then\nwe aggregate a training dataset containing total\n1292 stories. Besides, we collected 120 prompts as\na test set from Writing Prompts, which cover six\ngenres. The more information about this dataset is\nshown in table 1.\n3.2.2\nSetting\nEIPE-text (in-context)\nFor learning stage, we\nuse the text-embedding-ada-002, to obtain text em-\nbeddings of plan corpus. These embeddings will\n1https://americanliterature.com/short-stories\n2https://blog.reedsy.com/\ncreative-writing-prompts/\nthen be utilized in conjunction with the k-means\nalgorithm for cluster purposes. We use k-means\ngetting 20 clustering centroids as demonstrations\nto learn a planner and use the planner during com-\nparing with baselines.\n3.2.3\nBaselines\nrecurrentGPT\nA language-based simulacra of\nthe recurrence mechanism in RNNs that uses\nlanguage-based components and defines a recur-\nrent computation graph via prompt engineering.\nIt is worth mentioning that we are not directly\ncomparing with Re3 and DOC, because recurrent-\nGPT is already way ahead of these methods.\n3.2.4\nMetric\nOur evaluation employs a pairwise comparison met-\nric. We report results individually for each pairwise\ncomparison between EIPE-text and each baseline,\nnever mixing numbers from different comparisons\nfollowing Re3 (Yang et al., 2022). We show the\ncriteria as outlined in (Yang et al., 2023) for novel\nas following:\n\u2022 Interesting: An interesting novel captivates\nthe reader\u2019s attention, engages them emotion-\nally, and holds their interest throughout.\n\u2022 Coherent: A coherent novel follows a logi-\ncal and consistent plot-line without significant\ngaps or inconsistencies.\n\u2022 Relevant: Faithful to the initial premise.\nAutomatic Evaluation\nFor automatic evaluation,\nwe employed GPT-4 to assess various aspects of the\ngenerated narrative. GPT-4 automatic evaluation\nis highly affected by the order and unstable, so all\nmetrics are judged by GPT4 with a premise, afore-\nmentioned criteria and two corresponding stories in\nrandom order. We also use majority voting system\nto evaluate each criterion of each pair. The evalu-\nation prompt for novel can be found in appendix\nC.1.\nsetting A\nsetting B\nA Win Ratio\nB Win Ratio\nLLaMA raw planner\nEIPE-text (finetune)\n6.2\n93.8\nGPT4 raw planner\nEIPE-text (in-context)\n22.5\n75.2\nTable 4: TED Talk Automatic Evaluation Results. Pair-wise comparison using GPT-4 evaluation of EIPE-text with\nbaselines for 130 TED talk transcripts. Results in different comparisons are not comparable with each other.\nHuman Evaluation\nIn order to ensure impartial\nand high-quality evaluations, we collaborated with\nthird-party data annotators. Each generated data\npair, comprising novels A and B presented in ran-\ndom order, underwent meticulous evaluation by\nthree distinct annotators. These annotators pos-\nsess proficient English language skills and were\nprovided with explicit instructions to evaluate and\ndeliver judgments on the superiority between novel\nA and novel B, or if they are indistinguishable,\nspecifically in relation to the aforementioned crite-\nria.\n3.2.5\nResult\nWe show the experiment results of novels in table\n2 and table 3. As we can see from the table, EIPE-\ntext shows an advantage in coherence and relevance\nin both human and automatic evaluation. Although\nthe human evaluation is less interesting (3.3%), the\nimprovement of coherence (5.0%) and relevance\n(13.3%) are significant. The same trend can be\nseen in automatic evaluation, it is less interesting\nthan recurrentGPT(3.3%), but coherent (18.4%)\nand relevant (8.3%) are significantly higher. These\nresults indicate that EIPE-text improves the overall\nquality of generated narrative, and also indicate\nthat automatic evaluation and human evaluation\nhave certain relevance.\n3.3\nStrorytelling\n3.3.1\nDataset\nTED Talks 3 are influential presentations that cover\na wide range of topics. They are known for their en-\ngaging narratives, concise structure, and powerful\nmessages, which can be challenging to generate for\nboth models and humans. We use the data collected\nby Kaggle 4. The training dataset aggregates 2,468\nTED Talks spanning the years 1984 to 2016. In\naddition, we have curated 130 TED Talk transcripts\npost-2021 as our testing datasets as shown in table\n1.\n3https://www.ted.com/talks\n4https://www.kaggle.com/datasets/rounakbanik/\nted-talks\n3.3.2\nSetting\nEIPE-text (in-context)\nFor learning stage, text\nembeddings obtained using text-embeddings-ada-\n002 are used for clustering together with the k-\nmeans algorithm. Then we use 20 clustering cen-\ntroids as demonstrations to learn a planner.\nEIPE-text (finetune)\nWe finetune the open\nsource LLM, LLaMA (Touvron et al., 2023), using\nthe plan corpus and use it as planner during learn-\ning stage. Specially, we finetune LLaMA-7B using\nlora(Hu et al., 2022).\n3.3.3\nBaselines\nGPT4 raw planner\nIn this setup, planner is\nGPT4 zero-shot whose ability to plan depends en-\ntirely on its native capabilities. After the planner\ngenerates the plan, narrative generation follows the\nsame way as the inference stage in 3.1\nLLaMA raw planner\nsimilar to GPT4 raw plan-\nner, but the planner is untrained LLaMA.\n3.3.4\nMetric\nWe only adopt automatic evaluation in storytelling\ngeneration. The evaluation criteria were tailored to\nspecific domain to ensure relevant and accurate as-\nsessments, so we use other criteria for storytelling:\n\u2022 Coherent: The talk should have a clear struc-\nture, smooth transitions, and a strong conclu-\nsion for easy comprehension and a consistent\ntheme.\n\u2022 Interesting: It should use storytelling and\nexamples to engage the audience, maintaining\ntheir curiosity throughout.\n\u2022 Relevant: The topic should be timely, address\ncurrent issues, and offer fresh insights, not\njust repeat existing information.\n\u2022 Inspiring: The talk should convey passion,\npresent innovative ideas, and encourage the\naudience to think differently or take action.\nIt should be emphasized that we only use ma-\njority voting system to evaluate each pair for all\ncriteria, instead of evaluating each criterion of each\npair. The evaluation prompt for storytelling can be\nfound in appendix C.2\n3.3.5\nResults\nWe show the experiment result of storytelling do-\nmain on TED Talk in table 4. Under the finetune\nsetting, EIPE-text far outperforms LLaMA raw\nplanner (87.6%). Also under setting B, EIPE-text\nis significantly outperform the GPT4 raw planner\n(52.7%). EIPE-text either using a finetune base\nplanner or using in-context learning based planners\nis well ahead of the LLM itself.\n4\nAnalysis\nIn this section, we explore the key aspects of de-\nsigning an effective planner and provide an exper-\nimental analysis of the effectiveness of the plan\nrefinement process.\n4.1\nAblation study of in-context learning\nbased planner\nOur investigation centers around two fundamental\nquestions: (1) How does the demonstration selec-\ntion algorithm impact the performance of our plan-\nner? (2) What effect does the number of demonstra-\ntion examples have on the planner\u2019s performance?\nTo address these questions, we designed experi-\nments where we compared various planner config-\nurations, including (1) n-shot cluster-based plan-\nner: this configuration utilizes a cluster-based ap-\nproach to select n demonstration examples. (2)\nn-shot retrieval-based planner: in contrast, this\nconfiguration employs a retrieval-based method to\nselect n demonstration examples.\nUsing clustering to select more demonstra-\ntions leads to better results. We show the results\nin table 5. In the comparison between the 20-shot\ncluster-based planner and the 5-shot cluster-based\nplanner, the 20-shot cluster-based planner outper-\nforms the 5-shot cluster-based planner with a win\nratio of 70.9% versus 26.8%. This suggests that\nusing more demonstration examples leads to bet-\nter planner performance. In addition, as the plan\nlength we use is shorter than full narrative, we can\nuse more plans as demonstrations within context\nwindow. When comparing the 5-shot cluster-based\nplanner and the 5-shot retrieval-based planner, the\nclustering-based method for selecting demonstra-\ntion examples appears to be slightly more effective.\nThis trend is more pronounced when looking at the\ncomparison between the 20-shot cluster-based plan-\nner and the 20-shot retrieval-based planner. The\n20-shot cluster-based planner significantly outper-\nforms the retrieval-based planner, with a win ratio\nof 67.2% versus 32.0%. This suggests that using\nclustering for selection is considerably more effec-\ntive than relying on retrieval-based methods.\n4.2\nComparison between hierarchical\ngeneration and non-hierarchical\nTo investigate the impact of narrative genera-\ntion methods on the performance of our planner,\nwe compared hierarchical generation with non-\nhierarchical methods.\nWe experiment with non-hierarchical generation\nincluding configurations: (1) 0-shot without plan-\nner: generate full narrative directly in one step.\n(2) n-shot cluster-based without planner: select\nn demonstrations using a cluster-based approach\nand generate a full narrative using these demonstra-\ntions. (3) n-shot retrieval-based without planner:\nsimilar to previous setting, instead, we rely on a\nretrieval-based approach to select demonstrations.\nHierarchical generation is effective compared\nwith non-hierarchical. We show the results in\ntable 5. The 0-shot planner, significantly outper-\nforms 0-shot without planner, achieving a win ratio\nof 76.7% versus 20.9%. Moreover, similar trends\ncan be found in 5-shot setting with 88.2% versus\n11.0% and 70.6% versus 29.4%.\n4.3\nEffectiveness of the plan refinement\nprocess\nIn addition, we also want to know whether self-\nrefinement can be effectively refined and the rea-\nsons behind its convergence.\nFast Convergence with Self-Refinement\nWe\ncan see from the table 6 that our framework can\nconverge in an average of 2.98 epochs, which is\nactually very fast and it is hard to converge without\nusing self-refinement. The average accuracy curve\nof iterative refinement process is shown in figure 3.\nIterative Plan Refinement Ensures Alignment\nThe refined plan contains three operations, we mon-\nitor the number of three operations in the process.\nIn addition, since we organize the plan into a tree\nstructure, we also record the change in the number\nof nodes in the tree and the change in the num-\nber of secondary nodes (children of the root node)\nthroughout the process. As can be seen from table\nFigure 2: An Example of the Plan Refinement Process.\nFigure 3: Average accuracy curve of iterative refinement\nprocess.\n6, the average add, modify and adjust operations\noccur 8.26 times, 3.22 times, and 2.25 times respec-\ntively. The average number of nodes increase by\n11.41. We can clearly see these changes in figure\n2 (for more detail in appendix D.1). This indicates\nthat in plan refinement process, it does not simply\nadd nodes. Instead, it can accurately modify rel-\nevant parts and adjust structure according to the\nquestion answering. Thus, these three operations\nensure the alignment between the plan and the orig-\ninal narrative.\n4.4\nCase study of in-context learning based\nplan generation\nRelying solely on comprehensive narratives for\nlearning can often lead to missing finer details. Nar-\nratives are typically dense with information, posing\nchallenges for models to pinpoint and retain crit-\nA\nB\nA Win Ratio\nB Win Ratio\nDifferent Demonstration Number\n20-shot cluster-based planner\n5-shot cluster-based planner\n70.9\n26.8\nDifferent Demonstration Selection\n5-shot cluster-based planner\n5-shot retrieval-based planner\n51.6\n46.0\n20-shot cluster-based planner\n20-shot retrieval-based planner\n67.2\n32.0\nDifferent Narrative Generation Method\n0-shot planner\n0-shot without planner\n76.7\n20.9\n5-shot cluster-based planner\n5-shot cluster-based without planner\n88.2\n11.0\n5-shot retrieval-based planner\n5-shot retrieval-based without planner\n70.6\n29.4\nTable 5: Ablation Study Result. Different Demonstration Number: In the learning stage of EIPE-text, in-context\nlearning based planner use different numbers of demonstrations. Different Demonstration selection: In-context\nlearning based planner can implement different methods, such as clustering or retrieving items related to the input\ntopic, to select demonstrations. Different Narrative Generation Method: In addition to being able to generate\nnarratives using EIPE-text. Narrative can also be generated in one step by simply combining several narratives as\ndemonstrations without planner giving an input topic.\nmetric\noperation\ndifference before and after\nepochs and question numbers\nadd\nmodify\nadjust\nall nodes\nsecondary nodes\naverage epoch\naverage questions\nnum\n8.26\n3.22\n2.25\n11.41\n0.25\n2.98\n35.71\nTable 6: Iterative Refinement Metric\nical elements. Furthermore, methods that learn\nfrom complete narratives are usually computation-\nally expensive and time demanding. On the other\nhand, when using in-context learning with plans,\nmodels can more adeptly identify and relate to rel-\nevant information within each contextual segment.\nThis technique not only ensures that key details\naren\u2019t overlooked but also streamlines the learning\nprocess regarding the text\u2019s semantic framework,\nultimately conserving computational resources. We\nshow an example of 1-shot in Appendix D.2, from\nwhich we can see that the generated plan is not\nonly coherent but also retains the salient features\nof the demonstration, while effectively addressing\nthe topic query.\n5\nRelated Work\nLong-form Narrative Text Generation\nAs for\nlong-form narrative text generation, recent studies\ntackle this from the following perspectives: ap-\npending the generated prefix to the encoder (Shao\net al., 2017), while newer models like (Guan et al.,\n2021) focus on capturing sentence and discourse-\nlevel coherence, and DiscoDVT by (Ji and Huang,\n2021) leverages discrete variational Transformers\nto enhance long-range coherence in generated texts.\nAnother type of work adopts the plan-and-write\nstrategy (Fan et al., 2018). In particular, there has\nbeen extensive exploration of story planning (Yao\net al., 2019; Fan et al., 2019; Goldfarb-Tarrant et al.,\n2020). A hierarchical story generation system with\nrecursive prompting and revision was proposed by\nYang et al. (2022). And the current state-of-the-art\nwork recurrentGPT (Zhou et al., 2023), which uses\nlarge language model (LLM) such as ChatGPT\nand uses natural language to simulate the Long\nShort-Term Memory mechanism in an LSTM. The\ncurrent plan results from these methods are not sat-\nisfactory. Instead, we use LLM to automatically\nmine the plan and train a good planner to achieve\ngood results. Furthermore, from the plan to the full\ntext, our methods and theirs are complementary\nand can be combined to achieve better results.\nHuman-AI Co-writing\nHuman-AI co-writing\nsystems have been developing at the intersection of\nNLP and human-computer interaction (HCI) fields,\nsuch as Wordcraft (Yuan et al., 2022), TaleBrush\n(Chung et al., 2022), CoAuthor (Lee et al., 2022)\nand Dramatron (Mirowski et al., 2023). These\nworks explore the possibilities of using LLM as\na writing assistant to humans. Our work generates\nan explicit plan, which can be easily provided for\nhuman review and modification, making human-AI\nco-writing easier.\n6\nConclusions\nEIPE-text represents a significant step forward in\nthe field of long-form narrative text generation, ad-\ndressing the challenges of coherence and structure\nover extended pieces of text. With its ability to\ngenerate high-quality long-form narratives and aid\nhuman writers, EIPE-text opens up new possibili-\nties for leveraging the capabilities of LLMs in cre-\native and expressive writing tasks. Future research\ncould explore further applications and extensions\nof EIPE-text in various domains, advancing the\nstate of the art in automated text generation.\n7\nLimitations\nDuring plan extraction stage, the two steps of QA-\npairs generation and questions answering largely\ndepend on LLM\u2019s own reasoning capability, so this\nmethod can only produce ideal results on models\nwith strong reasoning capability (GPT4, Claude,\netc.). Otherwise, it may lead to the refinement\nprocess failing to converge. Our framework is a\ndata-driven approach, so it does not improve the\nOOD performance.\nReferences\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-\nter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,\nHarsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general in-\ntelligence: Early experiments with gpt-4.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang,\nYi Chang, Philip S. Yu, Qiang Yang, and Xing Xie.\n2023. A survey on evaluation of large language mod-\nels.\nJohn Joon Young Chung, Wooseok Kim, Kang Min\nYoo, Hwaran Lee, Eytan Adar, and Minsuk Chang.\n2022. Talebrush: sketching stories with generative\npretrained language models. In Proceedings of the\n2022 CHI Conference on Human Factors in Comput-\ning Systems, pages 1\u201319.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889\u2013898, Melbourne, Australia. Association\nfor Computational Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2019.\nStrategies for structuring story generation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2650\u2013\n2660, Florence, Italy. Association for Computational\nLinguistics.\nSeraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph\nWeischedel, and Nanyun Peng. 2020. Content plan-\nning for neural story generation with aristotelian\nrescoring. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4319\u20134338, Online. Association for\nComputational Linguistics.\nJian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wen-\nbiao Ding, and Minlie Huang. 2021. Long text gener-\nation by modeling sentence-level and discourse-level\ncoherence. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 6379\u20136393, Online. Association for Computa-\ntional Linguistics.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nHaozhe Ji and Minlie Huang. 2021. DiscoDVT: Gener-\nating long text with discourse-aware discrete vari-\national transformer.\nIn Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 4208\u20134224, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nMina Lee, Percy Liang, and Qian Yang. 2022. Coau-\nthor: Designing a human-ai collaborative writing\ndataset for exploring language model capabilities. In\nProceedings of the 2022 CHI Conference on Human\nFactors in Computing Systems, pages 1\u201319.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nPiotr Mirowski, Kory W Mathewson, Jaylen Pittman,\nand Richard Evans. 2023. Co-writing screenplays\nand theatre scripts with language models: Evaluation\nby industry professionals. In Proceedings of the 2023\nCHI Conference on Human Factors in Computing\nSystems, pages 1\u201334.\nYuanlong Shao, Stephan Gouws, Denny Britz, Anna\nGoldie, Brian Strope, and Ray Kurzweil. 2017. Gen-\nerating high-quality and informative conversation re-\nsponses with sequence-to-sequence models. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2210\u2013\n2219, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nKevin Yang, Dan Klein, Nanyun Peng, and Yuandong\nTian. 2023. DOC: Improving long story coherence\nwith detailed outline control. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3378\u20133465, Toronto, Canada. Association for Com-\nputational Linguistics.\nKevin Yang, Yuandong Tian, Nanyun Peng, and Dan\nKlein. 2022. Re3: Generating longer stories with\nrecursive reprompting and revision. In Proceedings\nof the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4393\u20134479, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nLili Yao, Nanyun Peng, Ralph Weischedel, Kevin\nKnight, Dongyan Zhao, and Rui Yan. 2019. Plan-\nand-write: Towards better automatic storytelling. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 33, pages 7378\u20137385.\nAnn Yuan, Andy Coenen, Emily Reif, and Daphne Ip-\npolito. 2022. Wordcraft: story writing with large\nlanguage models. In 27th International Conference\non Intelligent User Interfaces, pages 841\u2013852.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui,\nTiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cot-\nterell, and Mrinmaya Sachan. 2023. Recurrentgpt:\nInteractive generation of (arbitrarily) long text.\nA\nPrompts\nA.1\nPrompt of Sketching Plan\nDistill the salient information and thematic flow\nfrom the original article into a tree-like text repre-\nsentation of a mind map in the following format:\nTOPIC\n- Main Topic\n- Sub Topic\n- Sub-Sub Topic\n- Sub-Sub Topic\n...\n- Main Topic\n- Sub Topic\n- Sub Topic\nA.2\nPrompt of QA-pairs Generation\nGuideline\nBased on the content of the article, generate sev-\neral multiple-choice questions and corresponding\nanswers:\n1. Not too detailed\n2. Focus on the logic of the article\n3. Deep understanding of the article after answer-\ning these questions\n4. Each question must have 4 options: A, B, C,\nD.\n5. For each question, there might be more than\none correct answer, identify all correct an-\nswers separated by \";\"\n6. Questions should reflect the structure of the\narticle.\n7. Questions should include three types: what,\nwhy, how.\n8. Provide related main ideas in the article for\neach question.\n9. Avoid options like \"All of the above\" or \"None\nof the above\"; use \"A;B;C\" format.\nThese questions are generated based on the article\u2019s\ncontent and the author\u2019s opinion, not my opinion.\nB\nExperiment Details\nB.1\nModification for recurrentGPT\nThe way to improve recurrentGPT. recurrentGPT is\nprone to loss of global memory just as RNN. And\nwe also find that the long-term memory in recur-\nrentGPT is not exactly long-term memory. To com-\npensate for this, we can insert the generated plan\nas additional memory to recurrentGPT, as shown\nin figure 4. At each time step (\u2019t\u2019), recurrentGPT\noperates on a dual input system: the paragraph\nproduced in the preceding step and a concise yet\ndirective instruction for the subsequent paragraph.\nA crucial aspect is the integration of the model\u2019s\nlong-term memory, which acts as a repository for\nstoring previously generated summaries, and im-\nportantly, it can retrieve these summaries through\nsemantic search, with the ability to store them on\nexternal hard drives. Simultaneously, the system ac-\ntively maintains a short-term memory, responsible\nfor encapsulating key information from recent time\nsteps, a repository that gets updated consistently\nas the process unfolds. Crucially, the \"generated\nplan,\" a newly introduced memory, becomes an\nintegral part of this intricate orchestration. When\nthe components converge, they create a coherent\nprompt that triggers the backbone language model,\naptly dubbed the \"backbone LLM,\" to undertake its\nFigure 4: Illustration of the improvement of recurrent-\nGPT. recurrentGPT uses natural language simulating an\nLSTM, we insert additional memory to main the long-\nterm memory in LSTM\nprimary task: generating a fresh paragraph while\nsimultaneously outlining a succinct plan for the\nforthcoming paragraph. What\u2019s truly remarkable\nis how this \"generated plan\" seamlessly merges\nwith the process, as it is not only updated in each\ntime step but also contributes to enriching the long-\nterm memory. This meticulous integration ensures\ncontinuity and coherence throughout the sequence,\nforging a recurrent mechanism that drives the gener-\nation process forward, where the \"generated plan\"\nplays a pivotal role in shaping the narrative\u2019s devel-\nopment.\nB.2\nUse K-Means Get Demonstrations\nUsing text-embeddings-ada-002 directly to convert\na text format plan to embedding for clustering is\nnot varied. We have adopted a combination ap-\nproach of K-means and LLM. To be specific, we\nuse LLM, according to prompt \"Without loss of\ngenerality, list distinctive characteristics of this ex-\nemplar that establish it as an effective paradigm\nfor designing genre. no explanation is needed.\" to\nget characteristics of plans. Then these characteris-\ntics are converted into embedding before clustering.\nAfter we set the number of clusters k, we can get\nk clustering centers, and we use these centroid as\nfinal demonstrations.\nC\nAutomatic Evaluation\nC.1\nNovel Automatic Evaluation\nIn this task, you will be presented with two novels\nside-by-side and asked to evaluate them based on\nthree metrics: Coherence, Interestingness, Rele-\nvance. Your task is to determine which novel is\nbetter for each metric or indicate if both novels are\nindistinguishable.\n- Coherent: A coherent novel follows a logical\nand consistent plot-line without significant gaps or\ninconsistencies.\n- Interesting: An interesting novel captivates the\nreader\u2019s attention, engages them emotionally, and\nholds their interest throughout.\n- Relevant. Faithful to the initial premise. The\nnovel effectively aligns its plot, message, and writ-\ning with its initial premise, ensuring consistency\nand faithfulness to the core theme.\nBased on these three aspects, make a decision\non which novel is achieving the desired impact, in\nthe manner like this:\n[Scratch Pad]\nName:\n\u2018distinctive characteristics\u2018 and \u2018elaborate\u2018 on\nthem\nName:\n\u2018distinctive characteristics\u2018 and \u2018elaborate\u2018 on\nthem\n[Reflection]\nAfter evaluating both novels based on the crite-\nria of \u2018coherence\u2018, \u2018interestingness\u2018, \u2018relevance\u2018, I\nhave come to the following \u2018thorough\u2018 conclusions:\n- Coherence:\n- Interestingness:\n- Relevance:\n[Final Choice]:\nCoherence: Name;\nInterestingness: Name;\nRelevance: Name;\nC.2\nTED Talks Automatic Evaluation\nThe coach\u2019s preference for evaluating the TED\nTalks can be summarized in the following spec:\n- Coherence: The coach will assess how well\nthe TED Talk is structured and organized. This\nincludes a clear introduction, logical flow of ideas,\nsmooth transitions between points, and a strong\nconclusion. The talk should be easy to follow and\nunderstand, with a consistent theme throughout.\n- Interestingness: The coach will evaluate how\nengaging and captivating the TED Talk is for the\naudience. This includes the use of storytelling,\nanecdotes, and examples to illustrate points, as well\nas the speaker\u2019s ability to maintain the audience\u2019s\nattention and curiosity throughout the talk.\n- Relevance: The coach will consider the impor-\ntance and significance of the topic being discussed\nin the TED Talk. The subject matter should be\ntimely, relevant to current events or societal issues,\nand have a broad appeal to a diverse audience. The\ntalk should also provide new insights or perspec-\ntives on the topic, rather than simply rehashing\nexisting information.\n- Inspiration: The coach will assess the TED\nTalk\u2019s ability to inspire, motivate, and provoke\nthought in the audience. This includes the speaker\u2019s\nability to convey passion and enthusiasm for the\ntopic, as well as the presentation of innovative\nideas, solutions, or calls to action that encourage\nthe audience to think differently or take action in\ntheir own lives.\nBased on these four aspects, the coach will make\na decision on which TED Talk is stronger and more\neffective in achieving the desired impact on the\naudience, in the manner like this:\n[Scratch Pad]\nName:\n\u2018distinctive characteristics\u2018 and \u2018elaborate\u2018 on\nthem\nName:\n\u2018distinctive characteristics\u2018 and \u2018elaborate\u2018 on\nthem\n[Reflection]\nAfter evaluating both TED Talks based on the cri-\nteria of \u2018coherence\u2018, \u2018interestingness\u2018, \u2018relevance\u2018,\nand \u2018inspiration\u2018, I have come to the following\n\u2018thorough\u2018 conclusions:\n- Coherence:\n- Interestingness:\n- Relevance:\n- Inspiration:\n[Final Choice]: Name\nD\nExamples\nD.1\nPlan Extraction Example\nIn this section, we show the detailed process of an\niteration in the plan extraction stage in the figure 5.\nAnd the comparison of initialized plans and refined\nplans are also shown in figure 6, 7, 8\nD.2\nPlan Generation Example\nIn figure 9, we observe several salient aspects of\nthe demonstration on the left side: 1. the utiliza-\ntion of relatable examples and analogies. 2. the\nconnection of various disciplines and concepts. 3.\nthe incorporation of quotes from notable figures.\nThese attributes are integrated into the generated\nplan. Notably, the generated plan contains relat-\nable examples to substantiate its viewpoints and\nincorporates relevant quotes. When we transition\nto the generated plan on the right, it\u2019s evident that\nthe plan incorporates these attributes seamlessly.\nFor instance: 1. the mention of \"Daniel Kahne-\nman\u2019s System 1 and System 2 thinking\" in the plan\nmirrors the demonstration\u2019s theme on \"Human per-\nception and understanding.\". 2. the outcome\u2019s\nemphasis on quotes is reflective of the demonstra-\ntion\u2019s approach, incorporating wisdom from Peter\nDrucker and Tim Ferriss. We can observe that the\ngenerated plan not only maintains coherence but\nalso preserves the key features of the demonstra-\ntion, while effectively responding to the topic query.\nInitialized Plan \n \nPig Products in Daily Life \n  - Introduction \n      - Netherlands: 16 million people, 12 million pigs \n      - Research on pig usage in products \n  - Products containing pig parts \n      - Bathroom items \n          - Soap (fatty acids from pork bone fat) \n          - Shampoo, conditioner, anti-wrinkle cream, body lotion, toothpaste \n      - Food items \n          - Dough improver (proteins from pig hairs) \n          - Low-fat butter (gelatin for texture) \n          - Cheesecake, chocolate mousse, tiramisu, vanilla pudding (gelatin for appearance) \n      - Construction materials \n          - Cellular concrete (proteins from bones) \n          - Train brakes (bone ash) \n      - Household items \n          - Fine bone china (translucency and strength) \n          - Paint (texture and glossiness) \n          - Sandpaper (bone glue) \n          - Paintbrushes (pig hairs) \n      - Meat products \n          - Portion-controlled meat cuts (fibrin from pig blood) \n      - Beverages \n          - Beer, wine, fruit juice (gelatin for clarity) \n      - Other products \n          - Cigarettes with hemoglobin filters \n          - Injectable collagen for wrinkles \n          - Bullets \n          - Heart valve implants \n          - Renewable energy (fuel from unused pig parts) \n  - Conclusion \n      - 185 products found containing pig parts \n      - Importance of knowing what products are made of \n      - Taking better care of raw materials and producers \n \nQA-based evaluation \n \nQuestion 1: Why does soap contain fatty acids made from boiling pork bone fat? \n\u2022 \nA. To harden the soap \n\u2022 \nB. To give it a pearl-like effect \n\u2022 \nC. To make it smell better \n\u2022 \nD. To improve its cleaning ability \nAnswer 1: A; B \nWrong Answer 1: A \nReference 1: \"Level 4 node 'Soap (fatty acids from pork bone fat)'\" \n \nQuestion 2: What is the purpose of using pig hemoglobin in cigarette filters? \n\u2022 \nA. To create an artificial lung \n\u2022 \nB. To improve the taste of the cigarette \n\u2022 \nC. To make the cigarette burn slower \n\u2022 \nD. To reduce the amount of harmful chemicals \nAnswer 2: A \nWrong Answer 2: D \nReference 2: \"Level 4 node 'Cigarettes with hemoglobin filters'\" \n \nQuestion 3: What is the purpose of using pig proteins in cellular concrete? \n\u2022 \nA. To strengthen the concrete \n\u2022 \nB. To make the concrete lighter \n\u2022 \nC. To improve the concrete's heat resistance \n\u2022 \nD. To increase the concrete's elasticity \nAnswer 3: B \nWrong Answer 3: A \nReference 3: \"Level 4 node 'Cellular concrete (proteins from bones)'\" \n \nQuestion 4: Why did the director of the heart valve company not want their product to be \nassociated with pigs? \n\u2022 \nA. Because pigs are considered unclean animals \n\u2022 \nB. Because it would reduce the perceived value of the product \n\u2022 \nC. Because they were concerned about potential religious objections \n\u2022 \nD. Because they wanted to maintain the high-tech image of the product \nAnswer 4: D \nWrong Answer 4: C \nReference 4: \"Level 4 node 'Heart valve implants'\" \n \nQuestion: How are pig-derived ingredients used in renewable energy? \n\u2022 \nA. As a substitute for fossil fuels \n\u2022 \nB. In the production of solar panels \n\u2022 \nC. In the production of wind turbines \n\u2022 \nD. As a source of biofuel \nAnswer: D \nWrong Answer: A \nReference: \"Level 4 node 'Renewable energy (fuel from unused pig parts)'\" \n \n \nRefinement Instructions \n \n     \n Instruction: \"Add a new level 5 node 'Hardening agent and pearl-like effect' under level \n4 node 'Soap (fatty acids from pork bone fat)', as the first level 5 node\" \n        \nInstruction: \"Add a new level 5 node 'Creates an artificial lung in the filter for a \nhealthier cigarette' under level 4 node 'Cigarettes with hemoglobin filters', as the first \nlevel 5 node\" \n \nInstruction: \"Add a new level 5 node 'Lightweight and fully reusable' under level 4 node \n'Cellular concrete (proteins from bones)', as the first level 5 node\" \n \nInstruction: \"Add a new level 5 node 'Director did not want the product associated with \npigs to maintain its high-tech image' under level 4 node 'Heart valve implants', as the first \nlevel 5 node\" \n \nInstruction: \"Change the name of level 4 node 'Renewable energy (fuel from unused pig \nparts)' to 'Renewable energy (fuel from pig waste)'; Add a new level 5 node 'Pig waste is \nconverted into fuel for renewable energy' under level 4 node 'Renewable energy (fuel \nfrom pig waste)'\" \n \n \nRefine plan \nPig Products in Daily Life \n  - Introduction \n      - Netherlands: 16 million people, 12 million pigs \n      - Research on pig usage in products \n  - Products containing pig parts \n      - Bathroom items \n          - Soap (fatty acids from pork bone fat) \n              - Hardening agent and pearl-like effect \n          - Shampoo, conditioner, anti-wrinkle cream, body lotion, toothpaste \n      - Food items \n          - Dough improver (proteins from pig hairs) \n          - Low-fat butter (gelatin for texture) \n          - Cheesecake, chocolate mousse, tiramisu, vanilla pudding (gelatin for appearance) \n      - Construction materials \n          - Cellular concrete (proteins from bones) \n              - Lightweight and fully reusable \n          - Train brakes (bone ash) \n      - Household items \n          - Fine bone china (translucency and strength) \n          - Paint (texture and glossiness) \n          - Sandpaper (bone glue) \n          - Paintbrushes (pig hairs) \n      - Meat products \n          - Portion-controlled meat cuts (fibrin from pig blood) \n      - Beverages \n          - Beer, wine, fruit juice (gelatin for clarity) \n      - Other products \n          - Cigarettes with hemoglobin filters \n              - Creates an artificial lung in the filter for a healthier cigarette \n          - Injectable collagen for wrinkles \n          - Bullets \n          - Heart valve implants \n              - Director did not want the product associated with pigs to maintain its high-tech \nimage \n          - Renewable energy (fuel from unused pig parts) \n  - Conclusion \n      - 185 products found containing pig parts \n      - Importance of knowing what products are made of \n      - Taking better care of raw materials and producers \n \nFigure 5: The detailed process of an iteration in the plan extraction stage\nFigure 6: Topic: This is your brain on communication. Neuroscientist Uri Hasson researches the basis of human\ncommunication, and experiments from his lab reveal that even across different languages, our brains show similar\nactivity, or become \u00e4ligned, \u00a8when we hear the same idea or story. This amazing neural mechanism allows us to\ntransmit brain patterns, sharing memories and knowledge. \u00a8We can communicate because we have a common code\nthat presents meaning, \u00a8Hasson says.\nFigure 7: Topic: Discover the physical side of the internet. When a squirrel chewed through a cable and knocked\nhim offline, journalist Andrew Blum started wondering what the Internet was really made of. So he set out to go see\nit \u2013 the underwater cables, secret switches and other physical bits that make up the net.\nFigure 8: Topic: Growing new organs. Anthony Atala\u2019s state-of-the-art lab grows human organs \u2013 from muscles\nto blood vessels to bladders, and more. At TEDMED, he shows footage of his bio-engineers working with some of\nits sci-fi gizmos, including an oven-like bioreactor (preheat to 98.6 F) and a machine that \u00a8prints\u00a8human tissue.\nFigure 9: Example of one shot.\n"
  },
  {
    "title": "LangNav: Language as a Perceptual Representation for Navigation",
    "link": "https://arxiv.org/pdf/2310.07889.pdf",
    "upvote": "4",
    "text": "LANGNAV: LANGUAGE AS A PERCEPTUAL REPRESEN-\nTATION FOR NAVIGATION\nBowen Pan\u22c4\nRameswar Panda\u2020\nSouYoung Jin\u22c6\nRogerio Feris\u2020\nAude Oliva\u22c4\u2020\nPhillip Isola\u22c4\nYoon Kim\u22c4\n\u22c4MIT CSAIL, \u2020MIT-IBM Watson AI Lab, \u22c6Dartmouth College\n{bpan, oliva, phillipi, yoonkim}@mit.edu,\nrpanda@ibm.com, rsferis@us.ibm.com, souyoung.jin@dartmouth.edu\nABSTRACT\nWe explore the use of language as a perceptual representation for vision-and-\nlanguage navigation. Our approach uses off-the-shelf vision systems (for image\ncaptioning and object detection) to convert an agent\u2019s egocentric panoramic view\nat each time step into natural language descriptions. We then finetune a pretrained\nlanguage model to select an action, based on the current view and the trajectory\nhistory, that would best fulfill the navigation instructions. In contrast to the standard\nsetup which adapts a pretrained language model to work directly with continuous\nvisual features from pretrained vision models, our approach instead uses (dis-\ncrete) language as the perceptual representation. We explore two use cases of our\nlanguage-based navigation (LangNav) approach on the R2R vision-and-language\nnavigation benchmark: generating synthetic trajectories from a prompted large\nlanguage model (GPT-4) with which to finetune a smaller language model; and\nsim-to-real transfer where we transfer a policy learned on a simulated environment\n(ALFRED) to a real-world environment (R2R). Our approach is found to improve\nupon strong baselines that rely on visual features in settings where only a few gold\ntrajectories (10-100) are available, demonstrating the potential of using language\nas a perceptual representation for navigation tasks.\n1\nINTRODUCTION\nApplications of large language models (LMs) to non-linguistic embodied tasks have generally focused\non using the implicit world knowledge within LMs to predict sub-tasks and actions for planning (Ahn\net al., 2022; Huang et al., 2022b;a; Singh et al., 2022). For instance, recent work has shown that\nLMs can be prompted to create a list of actions (e.g., GoToBathroom, LocateToothbrush) given a\nhigh-level goal given in natural language (e.g., \u201cbrush teeth\u201d) (Huang et al., 2022a). These approaches\nrely on the LM\u2019s priors on action sequences and inter-object correlations acquired through large-scale\npretraining (Zhou et al., 2023b; Li et al., 2023; Zhao et al., 2023), and it has not been clear whether\ntext-only models can be adapted to tasks such as vision-and-language navigation which requires an\negocentric agent follow instructions to navigate a 3D environment using visual input.\nTo be clear, there is a substantial body of work on using pretrained LMs for vision-and-language\nnavigation tasks (Hong et al., 2021; Qi et al., 2021; Qiao et al., 2022, inter alia). The standard\napproach is to simply use a pretrained LM over the natural language instructions to extract text\nfeatures that are combined with the agent\u2019s perceptual representations, which are given by continuous\nimage features extracted from pretrained vision models (Wang et al., 2019; Hao et al., 2020; Fried\net al., 2018). While effective in data-rich regimes, the direct use of vision features makes the approach\ndifficult to apply in cases where only a few labeled trajectories exist (e.g., 10-100 trajectories),\nas these approaches need to learn a full joint vision-language module that combines a pretrained\nvision model with a pretrained text model. A popular strategy in such data-scarce regimes is to\ngenerate synthetic data or transfer knowledge from other domains (e.g., from simulated environments).\nHowever, generating realistic perception data is itself a difficult task, and sim-to-real transfer with\nmodels that rely purely on visual features can overfit to the non-transferable features of the simulated\nenvironment (Anderson et al., 2021).\nThis paper proposes an alternative approach for learning vision-and-language navigation agents by\nexploiting language itself as the perceptual representation space. Our approach uses off-the-shelf\n1\narXiv:2310.07889v1  [cs.CV]  11 Oct 2023\nYou are a navigation agent who must navigate according to instructions \ngiven only descriptions of your current position via natural language. The \nnatural language description is sometimes incorrect.\nInstruction: \"Go down the stairs and straight into the living room. In the \nliving room walkout onto the patio. On the patio stop outside the doorway.\u201d\n[Trajectory History]\nStep 4:\nTo your straight ahead is, \"a living room with a couch a table and chairs\u201d\nTo your left is, \"a modern kitchen with a stainless steel refrigerator\u201d\nBehind you is, \"a long hallway with wooden steps leading to a black door\u201d\nYou go towards:\n\"Go down the stairs and \nstraight into the living \nroom. In the living room \nwalkout onto the patio. \nOn the patio stop outside \nthe doorway.\"\nLanguage Instructions and Visual Images\nLangNav Agent\nI should go towards:\n\"a living room with a \ncouch a table and chairs\"\nConverting Observations into Language Prompts\nObservations at Time Step 4\nA\nB\nC\nFigure 1: Overview of language-based navigation (LangNav). We describe the task instructions\nand visual observations (from off-the-shelf vision systems) through text. A language model is then\nfinetuned to predict which direction to move towards based on the language descriptions. Here, views\nA, B, and C correspond to the front, left, and rear views of the agent.\nvision models to obtain textual descriptions of the agent\u2019s egocentric panoramic view. The text\ndescriptions are then fed to an LM which must select the next action given the instruction and (text\ndescriptions of) the previous actions or observations. See Figure 1 for an overview.\nThe use of language to represent an agent\u2019s perceptual field makes it possible to readily leverage\nthe myriad capabilities of large language models. In our first case study, we show how we can use\na small amount of seed training data (10-100 trajectories) to obtain synthetic \u201ctrajectories\u201d from a\npowerful but closed-source LM (GPT-4; OpenAI, 2023). We find that training a smaller language\nmodel (LLaMA; Touvron et al., 2023a;b) on the generated trajectories mixed with the original\nseed data results in a language-based navigation (LangNav) agent that outperforms a vision-based\nagent that is finetuned on the same seed data. In our second study, we explore the use of language\nas a domain-invariant representation to perform sim-to-real transfer, where we transfer an agent\ntrained on a simpler simulated environment (ALFRED; Shridhar et al., 2020) to the real-world R2R\n(Anderson et al., 2018b) environment. Insofar as language is hypothesized to have co-evolved with the\nhuman brain to enable efficient communication (Deacon, 1997), it naturally abstracts away low-level\nperceptual details, and we indeed find that LangNav exhibits improved sim-to-real transfer compared\nto the vision-based agent. Our results collectively suggest that using language as a perceptual\nrepresentation for vision-and-language navigation is feasible and sometimes outperforms traditional\napproaches that rely on continuous visual features in low data regimes.\n2\nBACKGROUND: ROOM-TO-ROOM VISION-LANGUAGE NAVIGATION\nA popular real-world testbed for learning vision-and-language navigation (VLN) agents is the room-\nto-room dataset (R2R; Anderson et al., 2018b), in which an agent must perceive and navigate a 3D\nenvironment based on a language instruction U and an initial state S0. At each time step t, the agent\nuses the current observation Ot, the original language instructions U, and the trajectory history Ht,\nto predict the panoramic action at. The current observation is given by a set of panoramic images that\ndescribe the agent\u2019s egocentric view, i.e., Ot = {It,0, ..., It,V } where V corresponds to the number\nof discretized view angles.1 The panoramic action at corresponds to which navigable view in Ot to\ngo towards, i.e., at \u2208 Ot. After selecting an action, the state transitions from St to St+1. The aim is\nto output the command STOP after reaching the goal G specified by U in state S0.\nThe standard approach in R2R is to process the panoramic images {It,0, ..., It,V } with a pretrained\nvisual encoder Ev to extract continuous visual features Ft,v = {Ev(It,0), ..., E(It,V )} (Anderson\net al., 2018a; Fried et al., 2018; Tan et al., 2019; Hong et al., 2020). The language instruction is\ntypically processed by a pretrained language encoder El to extract the language features Fl = El(U).\nThese features, along with a hidden state representation of the trajectory history ht\u22121, are fed to a\njoint vision-language module (e.g., another Transformer) that attends over {It,0, ..., It,V } to select\nthe action at.\n3\nLANGUAGE AS A PERCEPTUAL REPRESENTATION FOR NAVIGATION\nWe begin by describing the perception-to-text models employed for converting visual observations\ninto text (\u00a7 3.1). We then discuss the prompt templates for converting the text into natural language\n(\u00a7 3.2), followed by a description of the offline imitation learning algorithm for learning (\u00a7 3.3).\n1In the popular R2R benchmark this can be as many as 36 (12 headings and 3 elevations). However we\nfollow previous works only consider the navigable views, which is often many fewer than 36.\n2\n3.1\nVISION-TO-TEXT SYSTEM\nWe use off-the-shelf vision models to convert visual observations into language descriptions. We\nuse an image captioning model (BLIP; Li et al., 2022a) and an object detection model (Deformable\nDETR; Zhu et al., 2020) over each view angle It,j to obtain the text descriptions,\nCt,j = IMAGECAPTIONER(It,j),\nxt,j,0, . . . , xt,j,M = OBJECTDETECTOR(It,j),\nwhere M is the number of detected objects. We did not experiment too much with different off-the-\nshelf vision systems and quickly converged on the above two models which seemed to qualitatively\nproduce reasonable results.\n3.2\nPROMPT TEMPLATES\nFigure 1 illustrates how the image caption and the detected objects are combined via templates to\nconstruct a piece of text on which to condition the language model. Based on the prompt template, the\nlanguage model will be finetuned on the (language representations of) output actions {a1, . . . , aT }\nvia the (conditional) language modeling objective. The prompt consists of the following components.\n(An example of a full trajectory is shown in appendix E).\nTask description D.\nWe first provide the language-based agent that describes the task:\nYou are a navigation agent who must navigate according to instructions\ngiven only descriptions of your current position [...].\nNavigation instruction U.\nWe then give the (natural language) navigation instructions, which\nprovides instructions to the agent on how to reach the goal. In this paper, the high-level instructions\ncan be from the realistic R2R dataset (our main dataset), synthesized by GPT-4 (which we use for\ndata augmentation), or the ALFRED dataset (from which we perform sim-to-real transfer learning).\nAn example instruction from R2R is:\nTravel forward past the wall with all the light switches and into the\nfirst room on your right.\nCurrent observation Ot. We use templates to convert the image caption Ct,j and objects obtained\nxt,j,0, \u00b7 \u00b7 \u00b7 , xt,j,M from It,j (\u00a7 3.1). For instance, if the agent is facing a heading of 90 degrees and\nan elevation of 0 degrees and there is a candidate navigable direction It,j located at a heading of 120\ndegrees and an elevation of 0 degrees, the text description for this view angle would be:\nTo your 30 degree right is \u201c{Ct,j}\u201d.\nDetails: {xt,j,0}, . . . , {xt,j,M}.\n(These view angles are given as part of each trajectory.) We create such templates for all the navigable\nview angles {It,0, . . . , It,V }.\nAction at.\nSelecting an action involves choose a navigable view out of Ot to move towards, i.e.,\nat \u2208 Ot. For example, suppose at = It,j, i.e., the agent decided to go to the j-th view angle. Then\nthis is recorded as:\nYou go towards: \u201c{Ct,j}\u201d\nTo actually have the agent generate at we simply decode from an LM\u2019s distribution,\npLM(\u00b7 | D, U, Ht, Ot), via greedy decoding. Here Ht = {Oi, ai}t\u22121\ni=0 encodes the observation and\naction trajectory. In general we found the LM to have no issue generating from the set of navigable\ndirections (i.e., {Ct,0, . . . , Ct,V }) with simple left-to-right decoding, and thus did not need to perform\nconstrained decoding.\nUpdating trajectory history Ht.\nWe update the observation and action trajectory history via\nappending the text representations of Ot and at to Ht. Specifically Ot and at are appended via\nadding the following template:\nStep {t}: To your {direction_1} is {caption_1}; To your {direction_2} is\n{caption_2}; [...]; You chose: {caption_of_selected_direction}.\nThis history serves to inform the model about its current position within the high-level instruction,\nenabling it to make more informed decisions when selecting actions.\n3\n3.3\nIMITATION LEARNING ON DEMONSTRATIONS\nThe language-based navigation agent is trained with offline imitation learning via finetuning a\npretrained language model (LLaMA; Touvron et al., 2023a;b)) on the above template. Concretely, we\ncreate an instruction-following dataset by transforming the expert trajectory from the original dataset\ninto instruction-following demonstrations. Let D = {W (i)}N\ni=1 be the set of training trajectories,\nwhere each W (i) can be represented as a natural language sequence from the above template,\nW (i) = (D(i), U (i), H(i)\n1 , O(i)\n1 , a(i)\n1 , . . . , H(i)\nT (i), O(i)\nT (i), a(i)\nT (i)). Here T (i) is the number of actions in\nthe example W (i), which is typically between 5 to 7. Given the above, we optimize the log likelihood\nof the (language descriptions of) actions, i.e., the objective for trajectory W (i) is given by,\nT (i)\nX\nt=1\nlog pLM(a(i)\nt\n| D(i), U (i), H(i)\nt , O(i)\nt ).\nWhile behavior cloning on gold trajectories is simple, it is prone to error propagation. In particular,\nthe history trajectory is obtained by a shortest-path algorithm (which has knowledge of the goal)\nand thus adheres closely to an optimal policy \u03c0\u2217. However, during prediction, trajectories can\ndeviate significantly from the optimal policy, leading to a distribution shift that can adversely affect\nperformance. To allow for the policy to recover from deviations from the optimal path, we adopt\nthe following strategy to create our imitation learning dataset: (1) at each time step, we sample a\nrandom action with probability \u03c1 = 0.2; (2) once a random action is selected, we use the shortest-path\nalgorithm to obtain the ground truth next action; (3) we repeat this process until the goal is reached;\n(4) once the goal is reached, this becomes part of the training demonstration data. (See appendix D\nfor an ablation study on \u03c1.) While more involved strategies which samples from the current policy\n(Ross et al., 2011) are widely used in the vision-based navigation literature, we found the above to be\nsimple and effective.\n4\nEMPIRICAL STUDY\nOur primary experiments with LangNav target the low-data setting, motivated by the observation that\nobtaining annotated data for embodied tasks such as vision-language navigation is often very costly\n(often more so than is the case for text-only or vision-only tasks). In particular, we are interested in\nlearning the most performant system based on a small number (10 or 100) of real-world navigation\ntrajectories. We sample our real-world trajectories from the Room-to-Room (R2R) dataset (Anderson\net al., 2018b), a realistic vision-and-language navigation dataset consisting of 21,567 navigation\ninstructions in the Matterport3D environment. The dataset includes 90 scenes, with 61 scenes in the\ntrain and validation \u201cseen\u201d sets, and 11 scenes in the validation \u201cunseen\u201d set. Our 10-shot dataset is\nrandomly sampled the train set within 1 scene, while our 100-shot dataset spans 2 scenes.\nEvaluation.\nTo contextualize our approach against prior work, we evaluate LangNav on both \u201cseen\u201d\nand \u201cunseen\u201d sets from R2R. The \u201cseen\u201d set contains scenes identical to the training set (but the\ninstructions and trajectories differ). However, this distinction is less important for our low-data\nregime, since we only make use of 1 scene (for the 10-shot case) or 2 scenes (for the 100-shot case).\nI.e., the majority of scenes in the \u201cseen\u201d validation subset are actually never seen by the agent.\nFor evaluation, we use the standard R2R task performance metrics (Anderson et al., 2018a): Naviga-\ntion Error (NE), the average distance between the agent\u2019s final position and the goal in meters (lower\nis better); Success Rate (SR), the ratio of trajectories in which the agent stopped within 3 meters of the\ngoal (higher is better); Oracle Success Rate (OSR), the ratio of trajectories in which the agent stopped\nwithin 3 meters to the goal with a view of the goal (higher is better); and Success weighted by the\nnormalized inverse of the Path Length (SPL) (higher is better).\n4.1\nCASE STUDY 1: LANGUAGE ENABLES EFFICIENT SYNTHETIC DATA GENERATION\nIn NLP, obtaining synthetic data from an appropriately-prompted large language model with which\nto learn a smaller model has been shown to be an effective approach in data-scarce settings (Wang\net al., 2021; Lang et al., 2022; Taori et al., 2023; Dai et al., 2023; Gunasekar et al., 2023, inter alia).2\nHowever this approach is difficult to extend to non-linguistic perceptual tasks such as vision-language\nnavigation since generating realistic perception data is itself a difficult task. In this section we show\n2However see Gudibande et al. (2023) for a critical discussion of this approach.\n4\nI am going to give you example instructions [\u2026\u2026]. \n-\n{real_instruction_1}\n-\n{real_instruction_2}\n-\n{real_instruction_3}\nYour goal is to write 10 more instructions like the \nabove [\u2026\u2026] make sure that the instruction can be \ncompleted by an agent in 5 to 7 steps.\nPhase I: Prompting GPT-4 to generate instructions\n1. {synthetic_instruction_1}\n[\u2026\u2026]\n9. {synthetic_instruction_9} \n10. Enter the living room through [\u2026\u2026] locate the \nlarge bookshelf.\nGPT-4 API\nrandom\nsampling\nPhase II: Prompting GPT-4 to generate the trajectory\nHere is an example of [\u2026\u2026] following template: To your [VIEW] is [CAPTION], where [\u2026\u2026]\n#Example 1\n### Instruction: Go to the right of the entrance, [\u2026\u2026]\n### Trajectory: Step 1: To your [\u2026\u2026]\nNow I will give you another instruction. Please generate a trajectory [\u2026\u2026]\n#Example 2\n### Instruction: Enter the living room through [\u2026\u2026] locate the large bookshelf.\nCLIP feature\nmatching\n### Trajectory:\nStep 1:\nTo your straight ahead is, a living room with a sofa, coffee table, and a television\nTo your 30 degree left is, [\u2026\u2026]\nYou chose: [a living room with a sofa, coffee table, and a television]\n[\u2026 More Steps \u2026]\nGPT-4 API\nGPT-4 response\nGPT-4 prompt\nGPT-4 prompt\nGPT-4 response\nFigure 2: The pipeline for generating synthetic navigation trajectories from a prompted GPT-4. We\nfirst prompt GPT-4 with 3 randomly sampled navigation instructions U to generate 10 more synthetic\nnavigation instructions (Phase 1). Then for each generated navigation instruction, we prompt GPT-4\nto generate the trajectory that fulfills the generated instruction (Phase 2). See appendix F for details.\nthat working in pure language space makes it possible to easily generate synthetic data from a large\nlanguage model based on a few seed trajectories. We further show that finetuning a smaller language\nmodel on a mixture of synthetic and real trajectories improves upon a vision-based model that is just\ntrained on the 10-100 real trajectories.\n4.1.1\nSYNTHETIC TRAJECTORY GENERATION\nWe generate the synthetic trajectories by using only the 10-shot real-world trajectories from a single\nscene (see \u00a74). In R2R each real trajectory has 3 navigation instructions which are narrated by 3\ndifferent annotators. Thus we have 30 navigation instructions {U (i)}30\ni=1 in total. Our data generation\npipeline can be divided into two phases. In phase 1, we randomly choose 3 real instructions as prompt\nexamples and ask GPT-4 to create 10 more instructions similar to the examples, as is shown in fig. 2.\nWe collect 10,000 generated navigation instructions in this phase. In phase 2, for each generated\ninstruction, we prompt GPT-4 to generate a trajectory to fulfill the instruction, conditioned on a real\ndemonstration instruction and trajectory. The real trajectory is obtained by selecting the trajectory\nwhose instruction is closest to the synthetic instruction based on the CLIP (Radford et al., 2021) text\nfeatures. See fig. 2 for an overview and appendix F for the GPT-4 prompts.\nWe present an illustrative example in Figure 3 to demonstrate some characteristics of the generated\ntrajectories. Following the pipeline depicted in Figure 2, we first generate an instruction, such as\n\u201cEnter the hallway [...]\", and then prompt GPT-4 to generate a trajectory that fulfills the given\ninstruction. We find three key aspects that indicate the quality of the generated trajectories: 1. Strong\nPrior: The generated scenarios exhibit a adherence to real-world room-object and object-object\ncorrelations, as evident from descriptions like \u201ca bathroom with a sink, mirror, [...]\u201d and \u201ca\nkitchen with modern appliances and a countertop.\u201d 2. Spatial Consistency: The example\nreveals spatial consistency within the generated trajectories. For instance, in Step 4, the agent\ncorrectly identifies the door with a potted plant, consistent with its position in Step 3. Such instances\nemphasize the preservation of spatial relationships across the generated trajectories. 3. Descriptive:\nThe generated trajectories incorporate descriptive captions and objects that do not directly relate to\nthe given instruction, which plays a crucial role in preparing the agent to navigate successfully in\nreal-world scenarios.\nRemark.\nWe cannot entirely rule out the possibility that the GPT-4 training set included the text\ninstructions seen in R2R.3 However, out of the 10,000 generated instructions, we did not find any\ninstructions that were in the actual R2R dataset.\n4.1.2\nEXPERIMENTS\nExperimental setup.\nWe compare LangNav, which is a LLaMA2-7b model finetuned on a mixture\nof the 10,000 synthetic trajectories and 10/100 real trajectories, against the following baselines: 1.\nRandom walk, which selects a random action at each time step; 2. GPT-4 (Zero-shot / Few-shot),\nwhere we prompt GPT-4 to complete the trajectory by changing the task description of the template\nin \u00a7 3.2 (see appendix G for the full prompt). For the few-shot baseline, due to the context length\nwe use one full navigation trajectory as a demonstration example; 3. NavGPT, a contemporaneous\nwork that also uses language as a percetual representation (via image captioning and object detection)\n3While the text instructions may have been encountered, the trajectories were unlikely to have been encoun-\ntered during pretraining since we used vision systems to obtain the captions/objects.\n5\nStep 1:\nTo your straight ahead is, \na bright living room with a comfortable sofa and a \ncoffee table\nTo your 60 degree right is,\na kitchen with modern appliances and a countertop\nTo your back is, \na dining area with a large table and chairs\nTo your 90 degree left is, \na hallway with framed pictures on the wall\nYou chose:\n[a hallway with framed pictures on the wall]\nInstruction:\nEnter the hallway and follow it \nuntil you see a door with a \npotted plant nearby. Enter the \nroom and locate the bookshelf. \nStop next to the bookshelf.\nStep 3:\nTo your 30 degree left is, \na door with a potted plant beside it\nTo your 90 degree left is, \na small office with a desk and a computer\nTo your straight ahead is, \nthe end of the hallway with a window\nTo your back is, \na hallway with picture frames on the wall\nYou chose:\n[a door with a potted plant beside it]\nStep 2:\nTo your 90 degree left is, \na cozy bedroom with a neatly made bed\nTo your 60 degree right is, \na bathroom with a sink, mirror, and a bathtub\nTo your straight ahead is, \na long hallway with doors on both sides\nTo your back is,\na living room with a sofa and a coffee table\nYou chose:\n[a long hallway with doors on both sides]\nStep 4:\nTo your 60 degree right is, \na large bookshelf filled with books and decorations\nTo your back is, \na hallway with a potted plant by the door\nTo your 30 degree left is, \na cozy lounge area with a sofa and a coffee table\nTo your 90 degree left is, \na home office with a desk, chair, and a computer\nYou chose:\n[a large bookshelf filled with books and decorations]\nStep 5:\nTo your 90 degree right is, \na window with a view of the backyard\nTo your straight ahead is, \na bookshelf with various items and books\nTo your back is, \nan entrance with a potted plant and a door \nleading to the hallway\nTo your 150 degree left is, \na comfortable lounge area with a sofa and a \ncoffee table\nYou chose: [Stop]\nFigure 3: An example of a generated trajectory from GPT-4. The example demonstrates a generated\ntrajectory by following the pipeline in Figure 2. See more examples in appendix H.\nMethods\n# real\nVal Seen\nVal Unseen\nNE\u2193\nOSR\u2191\nSR\u2191\nSPL\u2191\nNE\u2193\nOSR\u2191\nSR\u2191\nSPL\u2191\nRandom Walk\n0\n10.2\n5\n3\n1\n9.5\n6\n3\n2\nGPT-4 (Zero-shot)\n0\n10.5\n15\n9\n8\n10.2\n17\n10\n8\nGPT-4 (Few-shot)\n1\n10.1\n17\n10\n9\n9.9\n22\n13\n11\nNavGPT\u2217 (Zhou et al., 2023a)\n0\n-\n-\n-\n-\n6.5\n42\n34\n29\nRecBert (Hong et al., 2021)\n10\n10.8\n9\n7\n6\n10.1\n13\n9\n9\nDuET (Chen et al., 2022)\n10\n10.0\n21\n14\n12\n9.9\n20\n12\n11\nLLaMA2-7B\n10\n10.2\n15\n11\n10\n9.6\n16\n11\n9\nLangNav (with LLaMA2-7B)\n10\n7.5\n39\n31\n27\n7.0\n42\n32\n28\nRecBert (Hong et al., 2021)\n100\n9.3\n27\n20\n19\n9.4\n26\n19\n17\nDuET (Chen et al., 2022)\n100\n9.2\n31\n21\n18\n9.4\n32\n23\n19\nLLaMA2-7B\n100\n9.6\n29\n21\n18\n9.1\n30\n19\n17\nLangNav (with LLaMA2-7B)\n100\n7.4\n40\n32\n28\n7.1\n45\n34\n29\nTable 1: Results on the R2R dataset with 10 or 100 real world trajectories. Our LangNav approach\nfinetunes LLaMA2-7B on the mixture of the real-world trajectories and 10,000 synthetic trajectories\nfrom GPT-4. \u2217NavGPT relies on ground-truth distance information and is thus not strictly comparable\nto other baselines.\nto perform navigation, but purely with GPT-4 (Zhou et al., 2023a). Note that this is not strictly to\ncomparable to our baselines as NavGPT assumes access to ground truth distance information; 4.\nRecBert, a vision-based method that adopts a recurrent architecture proposed by Hong et al. (2021)\nto keep track of the trajectory history; 4. DuET, another vision-based method which additionally\nbuilds representations of the global map during learning (Chen et al., 2022); and 5. LLaMA2-7B, a\nlanguage-only baseline which does not make use of synthetically-generated data from GPT-4.\nAll finetuning methods use the same set of 10/100 trajectories. For these experiments we did not\nfind significant differences in performance when using the object detection module, and hence we\nonly rely on the image captioning system to give the language description of each view angle in the\nprompt template. See appendix A for the full training setup including hyperparameters.\nResults.\nThe results are shown in table 1. We find that GPT-4 zero- and few-shot results underper-\nform the NavGPT baseline despite using the same backbone model, potentially due to NavGPT\u2019s\nuse of chain-of-thought-style prompts (Wei et al., 2022; Kojima et al., 2023) as well as its use of\nground truth distance information. Just finetuning LLaMA2-7B on the 10/100 gold trajectories does\nnot perform well, although it is comparable to the vision-based policies. Training on a mixture of\nsynthetic and gold trajectories improves performance by a nontrivial margin, and the LLaMA2-7B-\nbased LangNav approaches the performance of NavGPT despite being many times smaller. (However\nour approach does require a small number of gold trajectories.) This indicates that our pipelined\nprompting strategy is an effective approach for distilling the rich navigation-relevant world knowledge\nwithin GPT-4 to a smaller (and more efficient) language model.\n6\n# synthetic data\nLLM\nNE\u2193\nOSR\u2191\nSR\u2191\nSPL\u2191\n2,000\nGPT-3.5\n9.8\n31\n16\n12\n500\nGPT-4\n8.0\n38\n25\n21\n2,000\nGPT-4\n7.0\n42\n31\n27\n10,000\nGPT-4\n7.0\n42\n32\n28\nTable 2: Performance on the Val Unseen set as we vary the number of synthetically generated data\nand the underlying LLM from which the synthetic data is generated.\nMethods\nPretraining\nR2R\nVal Seen\nVal Unseen\nData\ndata\nNE\u2193\nOSR\u2191\nSR\u2191\nSPL\u2191\nNE\u2193\nOSR\u2191\nSR\u2191\nSPL\u2191\nNone\n10\n10.8\n9\n7\n6\n10.1\n13\n9\n9\n100\n9.3\n27\n20\n19\n9.4\n26\n19\n17\n0\n9.5\n12\n8\n4\n9.0\n12\n7\n3\n10\n10.8\n11\n7\n6\n10.7\n13\n9\n7\nRecBert\nALFRED\n100\n9.9\n22\n18\n17\n10.2\n23\n15\n14\nNone\n10\n10.3\n17\n10\n8\n9.8\n20\n11\n8\n100\n9.0\n25\n20\n18\n9.2\n25\n17\n15\n0\n9.2\n20\n17\n15\n8.9\n24\n18\n16\n10\n8.7\n20\n19\n18\n8.3\n21\n18\n17\nLangNav\nALFRED\n100\n8.1\n29\n25\n24\n8.0\n29\n24\n22\nTable 3: Sim-to-real where we pretrain a navigation agent on the simulated ALFRED environment\nand finetune on the real-world R2R data. We use LLaMA-7B (Touvron et al., 2023a) as our backbone\nmodel, and compare against the RecBert (Hong et al., 2021) baseline.\nWe conduct an ablation study by varying both the number of synthetic trajectories and the source\nof synthetic data. As shown in table 2, increasing the number of synthetic trajectories generated\nby GPT-4 demonstrates a positive impact on performance, although the gains are marginal when\ngoing from 2,000 to 10,000 trajectories. Switching the synthetic data source from GPT-4 to GPT-3.5\nresults in a noticeable decline in performance, highlighting the necessity of using a strong backbone\nlanguage models for generating synthetic data.\n4.2\nCASE STUDY 2: LANGUAGE AS A BRIDGE FOR SIM-TO-REAL TRANSFER\nWe next experiment with using language as a domain-invariant representation space to transfer a\npolicy that has been trained on a simulated environment to the real-world R2R environment. We\nchoose the popular ALFRED dataset (Shridhar et al., 2020) as our simulated environment. The\nALFRED dataset, based on the AI2THOR environment (Kolve et al., 2017), provides language\ninstructions for household tasks.\nThere are significant differences between ALFRED and R2R which makes straightforward sim-to-real\ntransfer challenging. ALFRED uses images rendered from the synthetic AI2THOR environment,\nwhile R2R, based on the Matterport3D, incorporates images captured from real indoor environments.\nThese image sources differ in texture, occlusion, illumination, and other visual aspects. ALFRED\u2019s\nnavigation trajectories and instructions are also simpler and shorter compared to R2R\u2019s instructions.\nR2R instructions involve guiding the agent between rooms, whereas ALFRED trajectories mainly\nkeep the agent within a single room. Finally in ALFRED, the agent is limited to rotating left/right\nby 90\u00b0 and moving forward, while in R2R, the agent can move in any combination of 12 candidate\nheading directions and 3 elevation directions. See appendix B for further discussion of these\ndifferences, and see appendix A for the full experimental setup.\nResults. We pretrain both RecBert (Hong et al., 2021)4 and LangNav on the simulated ALFRED\nenvironment and finetune on 0/10/100 R2R trajectories. LangNav uses LLaMA1-7b (Touvron et al.,\n2023a) as the language model. The evaluation results for both methods are presented in Table 3.\nInterestingly, for RecBert, pretraining on ALFRED actually hurts performance, potentially due to\nthe model\u2019s overfitting to the idiosynccracies of the simulated environment. And without any R2R\ndata, RecBert performs at near chance, whereas LangNav is able to exhibit some level of zero-shot\n4Given that RecBert (Hong et al., 2021) has similar performance to DuET (Chen et al., 2022) in the few-shot\nsetting according to Table 1, we choose RecBert to be the baseline because it is simpler and does not require a\ntopological map.\n7\nTo your straight ahead is,\na living room filled with furniture \nand a large mirror\nTo your 30 degree right is,\na living room filled with furniture \nand a painting on the wall\nTo your 90 degree left is,\na long hallway with a picture \nhanging on the wall\n\u2026\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nStep 4\nStep 7\nInstruction: Go down the hallway past the large framed \nartwork and table, turn towards the eye chart, forward, and \nstop in the doorway of the bathroom.\nInstruction ID: 1168_0\nRecBert:\nLangNav:\nHistory steps\nRecBert\nLangNav\nSuccess?\nYes\nNo\nNE\n0.9\n15.4\nDirection_1\nDirection_2\nDirection_3\nTo your straight ahead is,\na living room filled with furniture \nand a sliding glass door\nTo your 90 degree right is,\na long kitchen with a stove top \noven next to a counter\nTo your 30 degree left is,\na large open living room with \nsliding glass doors\n\u2026\nStep 1\nStep 2\nStep 3\nStep 4\nStep 6\nStep 4\nStep 7\nInstruction: Walk into hallway, turn left and enter kitchen walk \nstraight down kitchen into hallway. Stop facing bathroom.\n\u2026\nInstruction ID: 3038_0\nRecBert:\nLangNav:\nHistory steps\nRecBert\nLangNav\nSuccess?\nNo\nYes\nNE\n16.5\n0.0\nDirection_1\nDirection_2\nDirection_3\nFigure 4: Qualitative results comparing language- and vision-based (RecBert; Hong et al., 2021)\nnavigation systems. The top example highlights a case where the language-based system was able to\nobtain the correct answer while the vision-based system made a mistake. The bottom example shows\nthe opposite. For each example, the chosen actions before the visualized step were identical so we\nput the history steps in the same row.\ntransfer. Pretraining in ALFRED consistently leads to performance improvements for LangNav.\nThis contrasting behavior between RecBert and LangNav highlights the potential of language as a\ndomain-invariant perceptual representation for navigation.\n5\nDISCUSSION\nHere we discuss some qualitative results as well as limitations of our approach.\nQualitative analysis.\nWe present two qualitative examples to illustrate the strengths and weaknesses\nof our approach when compared to the visual-based method shown in Figure 4. In the first example\n3038_0, our LangNav agent successfully reaches the goal, whereas the vision-based RecBert fails to\ndo so. The divergence between the two agents becomes evident at the third step when our LangNav\nagent correctly identifies the kitchen on the right and turns in that direction to enter it. In contrast,\nin the second example 1168_0, our LangNav agent falls short of reaching the goal due to a missed\nleft turn at the third step. This discrepancy may be attributed to the agent\u2019s failure to perceive the\neye chart on the left, which is not explicitly mentioned in the instruction\u2019s caption from the left\ndirection. The top example highlight the proficiency of our LangNav agent in grounding observed\nconcepts within the navigation instruction, whereas the bottom example demonstrates the potential\nlimitation where certain crucial visual concepts may not be adequately represented in the language\nrepresentations.\nLimitations.\nWhile we find that LangNav is promising in settings where only a handful of real\ntrajectories are available, on the full dataset it still underperforms vision-based agents by a nontrivial\nmargin, as shown in Table 4. This is especially true when compared to state-of-the-art approaches\nsuch as ScaleVLN (Wang et al., 2023) which make use of large-scale pretraining data as well as more\ninvolved imitation/reinforcement learning algorithms that require access to an environment oracle\nduring training. However, we note that while LangNav underperforms baselines in data-rich regimes,\nit overfits less to scenes seen during training, as demonstrated by the smaller drop in performance\nwhen applying the policy to unseen scenes during training.\nLanguage naturally abstracts away low-level perceptual details, and we find this to be beneficial for\nefficient data generation and sim-to-real transfer. However, this is also a serious limitation insofar as\na picture really is worth a \u201cthousand words\u201d in some cases. Our paper should be seen as more of an\nexploratory exercise to test the potential of language as a perceptual representation for navigation\n(which has been understudied compared to use of language models in other embodied tasks) rather\n8\nMethod\nTraining data\nNeeds Oracle\nVal Seen\nVal Unseen\nDrop\nSeq2Seq (SF) Anderson et al. (2018b)\nR2R\nNo\n38.6\n21.8\n16.8\nRCM (Wang et al., 2019)\nR2R\nYes\n67.4\n42.5\n24.9\nSpeaker-Follower (Fried et al., 2018)\nR2R+SpeakerAug.\nYes\n70.1\n54.6\n15.5\nRecBert\u2020 (Hong et al., 2021)\nR2R+PREV\nYes\n71.8\n54.5\n17.3\nHAMT (Chen et al., 2021b)\nR2R+PREV\nYes\n75.0\n65.7\n9.3\nScaleVLN Wang et al. (2023)\nR2R+PREV\nNo\n67.2\n47.4\n19.8\nScaleVLN (Wang et al., 2023)\nR2R+PREV\nYes\n76.9\n72.9\n4.0\nScaleVLN (Wang et al., 2023)\nR2R+PREV+ScaleVLN\nNo\n71.1\n57.0\n14.1\nScaleVLN (Wang et al., 2023)\nR2R+PREV+ScaleVLN\nYes\n80.5\n78.1\n2.4\nLangNav\nR2R\nNo\n55.0\n43.2\n11.8\nLangNav (M)\nR2R+ALFRED\nNo\n55.9\n45.6\n10.3\nTable 4: Comparison with state-of-the-art vision-based methods on the R2R dataset when trained on\nthe full dataset. We use success rate (SR) as the performance metric. \u201cNeeds oracle\u201d indicates that the\nmodel needs to rely on an oracle during training that can give the ground-truth next action based on a\nsampled path from the model.(M): Multi-Task model, see appendix C for details.\nthan a serious attempt at the state-of-the-art. We are certainly not suggesting the abandonment of\ntraditional (continuous) vision features for vision-language navigation. An interesting direction might\ninvolve the use of both vision- and language-based perceptual representations for navigation.\n6\nRELATED WORK\nLanguage Models for Task Planning.\nSeveral studies have explored language-based planning\n(Jansen, 2020; Sharma et al., 2021; Li et al., 2022b; Huang et al., 2022a; Ahn et al., 2022; Huang\net al., 2022b). Huang et al. (2022a) use GPT-3 (Brown et al., 2020) and Codex (Chen et al., 2021a)\nfor action plan generation with semantic translation using Sentence-RoBERTa (Huang et al., 2022a).\nSayCan (Ahn et al., 2022) grounds actions using FLAN (Wei et al., 2021) and action value functions\n(Shah et al., 2021). Huang et al. (2022b) explore incorporating grounded feedback into LLMs, while\nXiang et al. (2023) propose enhancing LLMs\u2019 with embodied task instructions.\nInstruction Tuning.\nFLAN (Wei et al., 2021) demonstrated the effectiveness of fine-tuning LLMs\nwith instructions from multiple tasks. Instruction tuning has been widely applied to prominent large\nlanguage models, including InstructGPT (Ouyang et al., 2022), FLAN-T5 (Chung et al., 2022),\nFLAN-PaLM (Chung et al., 2022), and OPT-IML (Iyer et al., 2022), but mainly focused on traditional\nlanguage tasks. Our work instead finetunes LLMs for embodied navigation tasks using language\ndescriptions of perceptual representations. There has been much recent work finetuning smaller\nlanguage models such as LLaMA on synthetic instruction-following data generated by GPT-3.5/GPT-4\n(Peng et al., 2023; Taori et al., 2023; Chiang et al., 2023; Wu et al., 2023).\nVision-and-Language Navigation.\nThere has been much work on vision and language navigation\non the R2R dataset (Anderson et al., 2018a). Approaches such as the speaker-follower model (Fried\net al., 2018) and environmental dropout method (Tan et al., 2019) improve generalization. Reinforced\ncross-modal matching (Wang et al., 2019) and self-monitoring (Ma et al., 2019) enhance performance.\nHong et al. (Hong et al., 2020) propose a language and visual entity relation graph. Recent\nadvancements include VLBERT-based methods (Hong et al., 2021) and object-informed sequential\nBERT (Qi et al., 2021). Qiao et al. (Qiao et al., 2022) incorporate additional pretext tasks into\nVLN pre-training based on (Hong et al., 2021). ALFRED (Shridhar et al., 2020) involves interactive\nactions in a synthetic environment (Kolve et al., 2017), with methods utilizing dense single vector\nrepresentations (Shridhar et al., 2020; Singh et al., 2021; Pashevich et al., 2021; Kim et al., 2021;\nBlukis et al., 2022) or a panoramic view space (Suglia et al., 2021). Our work is closely related to\nZhou et al. (2023a) and Schumann et al. (2023) who also use language descriptions of an agent\u2019s\nperceptual representation to perform navigation with an LM.\n7\nCONCLUSION\nWe show that we can learn to navigate in a realistic environment by using language to (1) easily\ngenerate synthetic trajectories and (2) transfer knowledge from a simulated environment. Our work\ndemonstrates the potential of language to serve as a domain-invariant perceptual representation for\negocentric navigation in low-data regimes with only a handful of real-word trajectories.\n9\nACKNOWLEDGEMENTS\nThis work was supported by the MIT-IBM Watson AI Lab.\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nPeter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen\nKoltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of\nembodied navigation agents. arXiv preprint arXiv:1807.06757, 2018a.\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid,\nStephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-\ngrounded navigation instructions in real environments. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 3674\u20133683, 2018b.\nPeter Anderson, Ayush Shrivastava, Joanne Truong, Arjun Majumdar, Devi Parikh, Dhruv Batra,\nand Stefan Lee. Sim-to-real transfer for vision-and-language navigation. In Conference on Robot\nLearning, pp. 671\u2013681. PMLR, 2021.\nValts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. A persistent spatial semantic\nrepresentation for high-level natural language instruction execution. In Conference on Robot\nLearning, pp. 706\u2013717. PMLR, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021a.\nShizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal\ntransformer for vision-and-language navigation. Advances in neural information processing\nsystems, 34:5834\u20135847, 2021b.\nShizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Think\nglobal, act local: Dual-scale graph transformer for vision-and-language navigation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16537\u201316547,\n2022.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nHaixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao\nLiu, Sheng Li, Dajiang Zhu, et al. Chataug: Leveraging chatgpt for text data augmentation. arXiv\npreprint arXiv:2302.13007, 2023.\nTerrence William Deacon. The symbolic species: The co-evolution of language and the brain.\nNumber 202. WW Norton & Company, 1997.\nDaniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency,\nTaylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for\nvision-and-language navigation. Advances in Neural Information Processing Systems, 31, 2018.\n10\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine,\nand Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717,\n2023.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all\nyou need. arXiv preprint arXiv:2306.11644, 2023.\nWeituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning a generic\nagent for vision-and-language navigation via pre-training. Conference on Computer Vision and\nPattern Recognition (CVPR), 2020.\nYicong Hong, Cristian Rodriguez, Yuankai Qi, Qi Wu, and Stephen Gould. Language and visual\nentity relationship graph for agent navigation. Advances in Neural Information Processing Systems,\n33:7685\u20137696, 2020.\nYicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. A recurrent\nvision-and-language bert for navigation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 1643\u20131653, June 2021.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In International Conference on\nMachine Learning, pp. 9118\u20139147. PMLR, 2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022b.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00e1niel Simig, Ping Yu,\nKurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model\ninstruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017,\n2022.\nPeter A Jansen. Visually-grounded planning without vision: Language models infer detailed plans\nfrom high-level instructions. arXiv preprint arXiv:2009.14259, 2020.\nByeonghwi Kim, Suvaansh Bhambri, Kunal Pratap Singh, Roozbeh Mottaghi, and Jonghyun Choi.\nAgent with the big picture: Perceiving surroundings for interactive instruction following. In\nEmbodied AI Workshop CVPR, volume 2, pp. 7, 2021.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners, 2023.\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel\nGordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment\nfor Visual AI. arXiv, 2017.\nHunter Lang, Monica N Agrawal, Yoon Kim, and David Sontag. Co-training improves prompt-\nbased learning for large language models. In International Conference on Machine Learning, pp.\n11985\u201312003. PMLR, 2022.\nBelinda Z Li, William Chen, Pratyusha Sharma, and Jacob Andreas. Lampp: Language models as\nprobabilistic priors for perception and action. arXiv e-prints, pp. arXiv\u20132302, 2023.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image\npre-training for unified vision-language understanding and generation. In ICML, 2022a.\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An\nHuang, Ekin Aky\u00fcrek, Anima Anandkumar, et al. Pre-trained language models for interactive\ndecision-making. Advances in Neural Information Processing Systems, 35:31199\u201331212, 2022b.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n11\nChih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caim-\ning Xiong. Self-monitoring navigation agent via auxiliary progress estimation. arXiv preprint\narXiv:1901.03035, 2019.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nAlexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-language\nnavigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n15942\u201315952, 2021.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277, 2023.\nYuankai Qi, Zizheng Pan, Yicong Hong, Ming-Hsuan Yang, Anton Van Den Hengel, and Qi Wu. The\nroad to know-where: An object-and-room informed sequential bert for indoor vision-language\nnavigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n1655\u20131664, 2021.\nYanyuan Qiao, Yuankai Qi, Yicong Hong, Zheng Yu, Peng Wang, and Qi Wu. Hop: history-and-\norder aware pre-training for vision-and-language navigation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 15418\u201315427, 2022.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nSt\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured\nprediction to no-regret online learning. In Proceedings of the fourteenth international conference\non artificial intelligence and statistics, pp. 627\u2013635. JMLR Workshop and Conference Proceedings,\n2011.\nRaphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, and William Yang Wang.\nVelma: Verbalization embodiment of llm agents for vision and language navigation in street view.\narXiv preprint arXiv:2307.06082, 2023.\nDhruv Shah, Peng Xu, Yao Lu, Ted Xiao, Alexander Toshev, Sergey Levine, and Brian Ichter.\nValue function spaces: Skill-centric state abstractions for long-horizon reasoning. arXiv preprint\narXiv:2111.03189, 2021.\nPratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction and planning with latent\nlanguage. arXiv preprint arXiv:2110.01517, 2021.\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\nLuke Zettlemoyer, and Dieter Fox. ALFRED: A Benchmark for Interpreting Grounded Instructions\nfor Everyday Tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2020. URL https://arxiv.org/abs/1912.01734.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using\nlarge language models. arXiv preprint arXiv:2209.11302, 2022.\nKunal Pratap Singh, Suvaansh Bhambri, Byeonghwi Kim, Roozbeh Mottaghi, and Jonghyun Choi.\nFactorizing perception and policy for interactive instruction following. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 1888\u20131897, 2021.\nAlessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, and Gaurav Sukhatme. Embodied\nbert: A transformer model for embodied, language-guided visual task completion. arXiv preprint\narXiv:2108.04927, 2021.\n12\nHao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate unseen environments: Back translation\nwith environmental dropout. arXiv preprint arXiv:1904.04195, 2019.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. Want to reduce labeling\ncost? gpt-3 can help. arXiv preprint arXiv:2108.13487, 2021.\nXin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang,\nWilliam Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised\nimitation learning for vision-language navigation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 6629\u20136638, 2019.\nZun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan,\nand Yu Qiao.\nScaling data generation in vision-and-language navigation.\narXiv preprint\narXiv:2307.15644, 2023.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc\nLe, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In\nProceedings of NeurIPS, 2022.\nMinghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji.\nLamini-lm: A diverse herd of distilled models from large-scale instructions. CoRR, abs/2304.14402,\n2023. URL https://arxiv.org/abs/2304.14402.\nJiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu.\nLanguage models meet world models: Embodied experiences enhance language models, 2023.\nZirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for\nlarge-scale task planning. arXiv preprint arXiv:2305.14078, 2023.\nGengze Zhou, Yicong Hong, and Qi Wu.\nNavgpt: Explicit reasoning in vision-and-language\nnavigation with large language models. arXiv preprint arXiv:2305.16986, 2023a.\nKaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia Jin, Lise Getoor, and Xin Eric\nWang. Esc: Exploration with soft commonsense constraints for zero-shot object navigation. arXiv\npreprint arXiv:2301.13166, 2023b.\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n13\nAppearance\nALFRED\nR2R\nStep Size\nALFRED\nR2R\nt = T - 1\nt = T\nComplexity of Instructions\nALFRED: Carry the bowl to the glass coffee table.\nR2R: Go to the left of the bed and out of the \nbedroom. Then go down the hall and make a right \nat the top of the stairs, go past the stairs and go a \ncouple steps into the bedroom and wait there.\nFigure 5: Task gap between ALFRED and R2R. We highlight notable distinctions between the\nnavigation tasks in ALFRED and R2R, encompassing variations in appearance, step size, and\ninstruction complexity. See appendix B for more details.\nA\nIMPLEMENTATIONS DETAILS\nWe used the LLaMA-7B model (Touvron et al., 2023a) and the LLaMA2-7B model (Touvron et al.,\n2023b) for our method, fine-tuning it on 72 V100-32GB GPUs with a batch size of 144. The training\ntokens had a maximum length of 1024, while during inference, the maximum length was set to 2048.\nThe AdamW optimizer (Loshchilov & Hutter, 2017) with a learning rate of 2 \u00d7 10\u22125 and weight\ndecay of 0 was employed for optimization. The WarmupDecayLR learning rate scheduler was used\nfor learning rate scheduling. For image captioning in both the R2R and ALFRED tasks, BLIP (Li\net al., 2022a) was utilized. Deformable DETR (Zhu et al., 2020) was used for object detection in\nthe R2R dataset, with suppression of outdoor object categories. We used the ground-truth object\ndetection results provided in ALFRED when we generated the instruction-following pairs in \u00a7 4.2.\nWhen prompting GPT-4 API, we set the temperature as 1 and top_p as 1. The cost of collecting the\ngenerated trajectories by prompting GPT-4 API (OpenAI, 2023) was around $500. In the few-shot\nlearning experiments in \u00a7 4.1 and \u00a7 4.2, we set \u03c1 = 0. While when fine-tuning with the full train set\nin \u00a7 5, we set \u03c1 = 0.2. We pretrain on 128K ALFRED instruction-following pairs whose format is\ngiven in \u00a7 3.2. We augment the observations in ALFRED to 12 views and randomly mask a variable\nnumber of views to mimic the irregular number of candidates in R2R.\nB\nDIFFERENCES BETWEEN ALFRED AND R2R.\nThere are significant differences between ALFRED and R2R which makes straightforward sim2real\ntransfer challenging. These differences include:\nVisual appearance. ALFRED uses images rendered from the synthetic AI2THOR environment,\nwhile R2R, based on the Matterport3D, incorporates images captured from real indoor environments.\nThese image sources differ in texture, occlusion, illumination, and other visual aspects.\nStep size. There is a difference in step sizes between the two tasks (see the right part of fig. 5).\nALFRED uses a step size of 0.25 meters, while R2R has larger and more variable step sizes. To bridge\nthis gap, we consolidate four consecutive MoveAhead steps into a single step along the ALFRED\ntrajectory.\nAction type. A complete ALFRED trajectory includes not only navigation actions but also interaction\nactions, where the interaction actions are combined with a target object to change the state of the\nsurrounding environment. In order to filter the interaction actions in ALFRED, we divide each\nALFRED trajectory into multiple sub-trajectories and keep the sub-trajectories that are labeled with\nthe GotoLocation tag.\n14\nTable 5: Performance of the Multi-task Model on R2R. We demonstrate the multi-task capability of\nthe LM agent. For single-task models, each model is trained within the task data. We trained the\nmulti-task model with data from both R2R and ALFRED tasks.\nModels\nR2R Seen\nR2R Unseen\nSR\u2191\nSPL\u2191\nSR\u2191\nSPL\u2191\nSingle-Task\n55.0\n51.0\n43.2\n37.9\nMulti-Task\n55.9\n51.7\n45.6\n40.0\nTable 6: Performance of the Multi-task Model on ALFRED. ST: Single-Task. MT: Multi-Task.\nALFRED Seen\nALFRED Unseen\nTask\u2191\nGC\u2191\nTask\u2191\nGC\u2191\nST\n0.0 (0.0)\n6.0 (4.7)\n0.5 (0.1)\n9.5(7.8)\nMT\n0.0 (0.0)\n6.4 (5.0)\n0.6 (0.2)\n9.8 (7.8)\nInstruction complexity. Due to trajectory splitting, ALFRED\u2019s navigation trajectories and instruc-\ntions appear simpler and shorter compared to R2R\u2019s instructions. R2R instructions involve guiding\nthe agent between rooms, whereas ALFRED trajectories mainly keep the agent within a single room.\nAction space. In ALFRED, the agent is limited to rotating left/right by 90\u00b0 and moving forward,\nwhile in R2R, the agent can move in any combination of 12 candidate heading directions and 3\nelevation directions. The number of available movement directions is irregular. This difference in\naction space makes R2R trajectories more human-like. To address this, we introduce randomness\nby adding or reducing a heading offset of \u00b130\u00b0 to the agent\u2019s direction at each step in ALFRED,\nallowing rotations of 30\u00b0 or 60\u00b0 in addition to 90\u00b0.\nC\nMULTI-TASK PERFORMANCE\nOne of the advantages of our approach is its inherent suitability for multitasking. Similar to LLMs use\ninstruction to handle multiple language tasks concurrently, we consolidate task information and inputs\ninto instructions. To validate the multitasking capability of our method, we extend its application to\nthe ALFRED task.\nMetrics on ALFRED.\nWe evaluate our model on ALFRED using two metrics: Task Success (Task)\nand Goal-Condition Success (GC). Task Success measures the ratio of trajectories where object\npositions and state changes accurately match all task goal conditions at the end. GC assesses the ratio\nof completed goal conditions in each action sequence. Task Success is only considered successful\nwhen GC is also 1. On average, each ALFRED task has 2.55 goal conditions. We also calculate the\nPath Length Weighted Metrics (PLW) for both Task and GC, which normalize the metrics based on the\nactual action sequence length.\nResults of the Multi-Task Model.\nIn ALFRED task, we set \u03c1 = 0 as the expert policy in ALFRED\nis suboptimal. To save training time and balance the data amount between R2R and ALFRED, we\nutilize only 50% of the training dataset, resulting in a dataset for ALFRED with 386K data pairs.\nFor R2R task training, we maintain \u03c1 = 0.2 and run each demonstration trajectory twice, resulting\nin a training set size of 235K for R2R. Consequently, the merged dataset for the multitask model\ncontains a total of 621K instruction-following data pairs. We select VLN Bert (Hong et al., 2021) as\nthe baseline for the R2R task and Seq2seq model (Shridhar et al., 2020) for the ALFRED task. Given\nthe substantial differences between the R2R task and the ALFRED task (\u00a7 4.2), our method is, to the\nbest of our knowledge, the first model that simultaneously addresses these two tasks. In table 5 and\ntable 6, we find that the multitask model exhibits superior performance compared to the single-task\nmodels. These results underscore the capability of our method to effectively handle multiple highly\ndiverse tasks.\n15\n0%\n10%\n20%\n30%\n40%\n50%\n10-shot\nFull set\n0\n2\n4\n6\n8\n10\n10-shot\nFull set\n0.0\n0.1\n0.2\n0.3\nSuccess Rate\nNavigation Error\n0%\n10%\n20%\n30%\n40%\n50%\n10-shot\nFull set\n0%\n10%\n20%\n30%\n40%\n50%\n10-shot\nFull set\nOracle SR\nSPL\nFigure 6: Investigating the Impact of the Randomness Factor \u03c1 on Model Performance. This image\ncaption depicts an ablation study exploring the influence of the randomness factor \u03c1 on our model\u2019s\nperformance in both few-shot learning and full-set training scenarios. We test \u03c1 with values of 0.0,\n0.1, 0.2, and 0.3.\nD\nIMPACT OF THE RANDOMNESS FACTOR\nWe conduct the ablation study to investigate the impact of the randomness factor \u03c1 on the model\u2019s\nperformance in both few-shot learning and full-set fine-tuning scenarios. Interestingly, we observe\ndifferent behaviors of the model with varying \u03c1 values in these scenarios. Fig. 6 illustrates our\nfindings. In the 10-shot scenario, increasing \u03c1 negatively affected the model\u2019s performance. However,\nin the full-set scenario, the model achieved optimal performance at \u03c1 = 0.2.\nE\nEXAMPLE OF THE FULL TRAJECTORY\nWe plot an example of a full text-based trajectory in R2R as we mentioned in \u00a7 3.2 as bellow:\nYou\nare\na\nnavigation\nagent\nwho\nmust\nnavigate\naccording\nto\ninstructions given only descriptions of your current position via\nnatural language.\nThe natural language description is sometimes\nincorrect.\n### Instruction:\nGo across the kitchen passed the pizza oven into the next room.\nStop just inside the bedroom.\n### Trajectory:\nStep 1:\nTo your 60 degree left is,\na kitchen with a stove, sink, and refrigerator\nDetails: oven, bowl, dining table, microwave, refrigerator\nTo your 30 degree left is,\na kitchen with white cabinets and a white stove top oven\nDetails: oven, bowl, microwave, potted plant, chair\nTo your 60 degree right is,\na living room filled with furniture and a fire place\nDetails: couch, chair\nTo your back is,\na room with a bed and a painting on the wall\nYou chose:\na living room filled with furniture and a fire place\n16\nStep 2:\nTo your 150 degree left is,\na kitchen with a stove top oven sitting next to a counter\nDetails: oven, dining table, microwave, refrigerator, chair\nTo your back is,\na kitchen with a wooden table and white appliances\nDetails: oven, bowl, dining table, microwave, refrigerator, apple,\ncouch, chair\nTo your 30 degree right is,\na living room with a couch a chair and a window\nDetails: couch\nTo your 120 degree right is,\na dining room table with a bowl of fruit on it\nDetails: chair, bowl, dining table\nTo your 150 degree right is,\na bowl of fruit sits on a wooden table\nDetails: couch, chair, bowl, dining table\nYou chose:\na living room with a couch a chair and a window\nStep 3:\nTo your back is,\na kitchen with a table, chairs, and stairs\nDetails: oven, dining table, refrigerator, potted plant, chair\nTo your 150 degree right is,\na room with a table, chairs, and stairs\nDetails: oven, chair, refrigerator, dining table\nTo your straight ahead and 30 degree down is,\na view of a hallway from the top of stairs\nDetails: refrigerator\nTo your 90 degree right and 30 degree up is,\na room with a staircase and a mirror on the wall\nDetails: toilet\nYou chose:\na view of a hallway from the top of stairs\nStep 4:\nTo your back and 60 degree up is,\na living room filled with furniture and a ceiling fan\nDetails: oven, potted plant, refrigerator\nTo your 90 degree left and 30 degree up is,\na living room with a couch and a table\nTo your straight ahead and 30 degree up is,\na bedroom with a bed and a mirror on the wall\n17\nDetails: bed\nYou chose:\na bedroom with a bed and a mirror on the wall\nStep 5:\nTo your back is,\na hallway leading to a kitchen and living room\nDetails: refrigerator, potted plant\nTo your 30 degree left is,\na room with a wooden door and a mirror\nTo your straight ahead is,\na bedroom with a bed, dresser, mirror and a ceiling fan\nDetails: potted plant, bed\nTo your 30 degree right is,\na bedroom with a bed and a ceiling fan\nDetails: potted plant, bed\nTo your 60 degree right is,\na bedroom with a bed, dresser and mirror\nDetails: potted plant, bed\nYou chose:\nstop\nF\nCOMPLETE PROMPT TEMPLATE OF GENERATING TRAJECTORIES FOR\nGPT-4\nWe list our complete templates for prompting GPT-4 to generate synthetic instructions (Phase I) and\nsynthetic trajectories to fulfill the instruction (Phase II).\nPhase I:\nThe template of phase I is listed as follows:\nI am going to give you example instructions written by humans to\ntrain a deep learning-based navigation agent acting inside a home.\nThese example instructions are intended to be completed by the\nnavigation agent in 5-7 steps.\n- {real_instruction_1}\n- {real_instruction_2}\n- {real_instruction_3}\nYour goal is to write 10 more instructions like the above that can be\nused to train a navigation agent. Since the navigation agent will be\nnavigating in different home environments, your instructions should\nalso be diverse and cover a wide range of home environments and\nrooms. You should make sure that the instruction can be completed\nby an agent in 5 to 7 steps.\nPhase II:\nThe template of phase II is listed as follows:\nHere\nis\nan\nexample\nof\na\nlarge\nlanguage\nmodel\nacting\nas\na\nblind\nnavigation\nagent\nin\nan\nindoor\nenvironment\nthrough\ntext\ndescriptions. The agent is given an instruction at the start and\nmust follow the instruction.\nAt each time step, the agent is\ngiven descriptions of its field of view via the following template:\n18\nTo your [VIEW] is [CAPTION]\n- [VIEW] consists of the agent\u2019s visible field of view (e.g., 30\ndegrees right, 120 degrees left, etc.)\n- [CAPTION] is the text description of that view obtained from an\nimage captioning model\n#Example 1\n### Instruction: {real_instruction_example}\n### Trajectory: {real_trajectory_example}\nNow\nI\nwill\ngive\nyou\nanother\ninstruction.\nPlease\ngenerate\na\ntrajectory of 5-7 steps that would complete the instruction.\n#Example 2\n### Instruction: {synthetic_instruction}\nG\nPROMPTS OF ZERO-SHOT AND FEW-SHOT NAVIGATION FOR GPT-4\nHere we attach the the task description D in the prompt template for prompting GPT-4 to navigate in\nthe R2R evaluation dataset.\nZero-shot:\nYou\nare\na\nnavigation\nagent\nwho\nmust\nnavigate\naccording\nto\ninstructions given only descriptions of your current position via\nnatural language.\nThe natural language description is sometimes\nincorrect.\nAt each step, you will be given several directions and captions\nfor each direction. You must choose one direction by printing only\nthe [caption_of_the_direction] or choose \"Stop\" if you think the\ngoal is reached.\nFor example:\nInput:\nTo your [direction_1] is, [caption of the direction_1].\n......\nTo your [direction_N] is, [caption of the direction_N].\nYou choose:\nOutput: [caption of the direction_3]\nHint:\nYou should use the information inside the instructions,\nhistory steps, and current observations to make the decision.\nFew-shot:\nYou\nare\na\nnavigation\nagent\nwho\nmust\nnavigate\naccording\nto\ninstructions given only descriptions of your current position via\nnatural language.\nThe natural language description is sometimes\nincorrect.\nAt each step, you will be given several directions and captions\nfor each direction. You must choose one direction by printing only\nthe [caption_of_the_direction] or choose \"Stop\" if you think the\ngoal is reached.\n19\nFor example:\nInput:\nTo your [direction_1] is, [caption of the direction_1].\n......\nTo your [direction_N] is, [caption of the direction_N].\nYou choose:\nOutput: [caption of the direction_3]\nAnd here is an example trajectory:\n### Instruction:\nGo down the stairs. Turn right and go down the hallway. Turn right\nand stand near the fireplace.\n### Trajectory:\nStep 1:\nTo your straight ahead is,\nan ornate doorway leading to another room\nTo your 60 degree right is,\na red carpeted staircase leading to a chandelier\nTo your 120 degree right is,\na room with a red carpet and a large mirror\nTo your back and 30 degree down is,\na room with a red carpet and two windows\nTo your 120 degree left is,\na room with a red carpet and gold trim\nYou chose:\na room with a red carpet and gold trim\nStep 2:\nTo your 150 degree right is,\na very ornate staircase in a house with red and white striped chairs\nTo your back is,\na red carpeted hallway leading to a staircase\nTo your 150 degree left is,\na hallway with a red carpet and a chandelier\nTo your 120 degree left is,\na room with a red carpet and a chandelier\nTo your 90 degree left is,\na room with a chandelier and two windows\nTo your 60 degree left is,\na room with a red carpet and a large mirror\n20\nTo your 30 degree right is,\na hallway with a red carpet and wooden doors\nYou chose:\na hallway with a red carpet and wooden doors\nStep 3:\nTo your back is,\na hallway with a red carpet and a chandelier\nTo your straight ahead is,\na hallway with a red carpet and a gold ceiling\na hallway with a red carpet and a gold ceiling\nYou chose:\na hallway with a red carpet and a gold ceiling\nStep 4:\nTo your 90 degree right is,\na living room with a chandelier and a fireplace\nTo your 120 degree right is,\na room with a fireplace and a chandelier in it\nTo your back is,\na hallway with a red carpet and gold trim\nTo your 90 degree left is,\na room with a chandelier and a table in it\nTo your 30 degree right is,\na living room with a chandelier and a couch\nYou chose:\na living room with a chandelier and a fireplace\nStep 5:\nTo your straight ahead is,\na living room filled with furniture and a fire place\nTo your 30 degree left is,\na living room filled with furniture and a chandelier\nTo your 150 degree right is,\na room with a chandelier and a table in it\nTo your back is,\na hallway with a chandelier and a mirror on the wall\nTo your 90 degree left is,\na living room filled with furniture and a chandelier\nYou chose:\nStop\n21\nHint:\nYou should use the information inside the instructions,\nhistory steps, and current observations to make the decision.\nNow let\u2019s start!\nH\nMORE EXAMPLES OF GENERATED TRAJECTORIES\nWe plot more examples of the generated trajectories from GPT-4 below:\nExample 1:\n### Instruction\nBegin in the basement, walk towards the pool table. Pass the pool\ntable and ascend the staircase.\nAt the top of the stairs, turn\nright and walk through the hallway. Turn left into the den, walk\nthrough and stop by the window. ### Trajectory:\nStep 1:\nTo your straight ahead is,\na basement with a pool table, a bar, and a seating area\nTo your 90 degree left is,\na wall with various framed photos and a door\nTo your 150 degree right is,\na storage area filled with boxes and shelves\nYou chose:\na basement with a pool table, a bar, and a seating area\nStep 2:\nTo your back is,\na basement entrance with a staircase leading down\nTo your 120 degree left is,\na basement with a bar, television, and several chairs\nTo your straight ahead is,\na basement with a pool table, a dartboard, and a bar\nTo your 90 degree right is,\na staircase leading upwards with a wooden railing\nYou chose:\na basement with a pool table, a dartboard, and a bar\nStep 3:\nTo your back is,\na basement with a seating area and framed photos on the wall\nTo your straight ahead is,\na pool table surrounded by chairs and a dartboard on the wall\nTo your 150 degree left is,\n22\na basement with a bar, television, and several chairs\nTo your 90 degree right is,\na staircase leading upwards with a wooden railing\nYou chose:\na staircase leading upwards with a wooden railing\nStep 4:\nTo your back is,\na basement with a pool table and a dartboard on the wall\nTo your 90 degree left is,\na hallway with various framed photos on the walls\nTo your straight ahead is,\na den with a large bookshelf, couch, and coffee table\nYou chose:\na hallway with various framed photos on the walls\nStep 5:\nTo your back is,\na staircase leading downwards into the basement\nTo your straight ahead is,\na long hallway with several doors leading to other rooms\nTo your 120 degree right is,\na den with a comfortable seating area and a bookshelf\nYou chose:\na long hallway with several doors leading to other rooms\nStep 6:\nTo your back is,\na hallway with various framed photos on the walls\nTo your 90 degree left is,\na den filled with cozy furniture, bookshelves, and a television\nTo your 90 degree right is,\na bedroom with a large bed, dresser, and a window\nYou chose:\na den filled with cozy furniture, bookshelves, and a television\nStep 7:\nTo your back is,\na hallway leading to other parts of the house\nTo your 90 degree left is,\na den with several bookshelves, a fireplace, and a large rug\nTo your straight ahead is,\n23\na window overlooking the garden outside\nYou chose:\nStop\nExample 2:\n### Instruction\nFrom the garage entrance, walk into the kitchen.\nTurn left and\nwalk past the stove. Continue straight into the family room, then\nwalk towards the sliding glass doors. Step outside and stop near\nthe barbecue grill.\n### Trajectory:\nStep 1:\nTo your straight ahead is,\na clean garage with a parked car and a workbench\nTo your 90 degree left is,\na laundry room with a washer and a dryer\nTo your back is,\na driveway leading up to a garage door\nTo your 120 degree left is,\na garage entrance with an open door leading inside the house\nYou chose:\na garage entrance with an open door leading inside the house\nStep 2:\nTo your 30 degree left is,\na kitchen with a stove, a refrigerator, and a microwave oven\nTo your 90 degree right is,\na small mudroom with a shoe rack and a coat hook\nTo your back is,\na view of a garage with a car and a workbench\nYou chose:\na kitchen with a stove, a refrigerator, and a microwave oven\nStep 3:\nTo your straight ahead is,\na large kitchen with a stove and a range hood\nTo your 30 degree right is,\na kitchen with a center island and a sink on the countertop\nTo your 150 degree left is,\na kitchen with a refrigerator and a microwave oven\nTo your 120 degree right is,\na tiled floor leading to a hallway with a picture on the wall\n24\nYou chose:\na large kitchen with a stove and a range hood\nStep 4:\nTo your straight ahead is,\na cozy family room with a TV and a fireplace\nTo your 90 degree right is,\na kitchen with a sink and a dishwasher\nTo your back is,\na kitchen with a countertop, a cooktop, and an oven\nYou chose:\na cozy family room with a TV and a fireplace\nStep 5:\nTo your 30 degree left is,\na sliding glass door leading to the backyard\nTo your 90 degree right is,\na comfortable sofa with a coffee table in front of a TV\nTo your back is,\na large kitchen with a stove and a range hood\nYou chose:\na sliding glass door leading to the backyard\nStep 6:\nTo your straight ahead is,\na well-maintained backyard with a barbecue grill and a dining table\nTo your 90 degree left is,\na sliding glass door leading inside the family room\nTo your back is,\na cozy family room with a TV and a fireplace\nYou chose:\na well-maintained backyard with a barbecue grill and a dining table\nStep 7:\nTo your 90 degree left is,\na barbecue grill on a patio in the backyard\nTo your straight ahead is,\na backyard with a dining table and chairs\nTo your 120 degree right is,\na backyard with a garden and a swing set\nTo your back is,\na sliding glass door leading inside the family room\n25\nYou chose:\nStop\nExample 3:\n### Instruction\nStand in front of the fireplace in the living room. Walk diagonally\nacross the room, passing the sofa. Proceed through the door into\nthe hallway. Turn left and continue toward the guest room. Stop\njust before entering and wait.\n### Trajectory:\nStep 1:\nTo your straight ahead is,\na living room with a fireplace and two chairs\nTo your 30 degree left is,\na living room with a large window and a dark brown couch\nTo your 90 degree right is,\na living room with a flat screen tv and a view of the backyard\nYou chose:\na living room with a fireplace and two chairs\nStep 2:\nTo your 60 degree left is,\na living room with a fireplace and a floor lamp\nTo your 30 degree right is,\na living room with a dark brown couch and a coffee table\nTo your straight ahead is,\na living room with a white rug in the middle\nYou chose:\na living room with a dark brown couch and a coffee table\nStep 3:\nTo your back is,\na living room with a fireplace and a christmas tree\nTo your 150 degree left is,\na living room with two chairs and a painting on the wall\nTo your straight ahead is,\na wooden door leading to a hallway\nYou chose:\na wooden door leading to a hallway\nStep 4:\nTo your 90 degree left is,\na hallway with a view of a staircase and a table\n26\nTo your straight ahead is,\na hallway with a painting on the wall and an open door\nYou chose:\na hallway with a painting on the wall and an open door\nStep 5:\nTo your back is,\na hallway with a wooden floor and a closed door\nTo your 120 degree left is,\na guest bedroom with a neatly made bed and a dresser\nTo your 30 degree right is,\na hallway with white walls and floor-to-ceiling mirrors\nYou chose:\nStop just before entering the guest bedroom\n27\n"
  }
]