[
  {
    "title": "Llemma: An Open Language Model For Mathematics",
    "link": "https://arxiv.org/pdf/2310.10631.pdf",
    "upvote": "38",
    "text": "Published as a conference paper at ICLR 2024\nLLEMMA: AN OPEN LANGUAGE MODEL FOR\nMATHEMATICS\nZhangir Azerbayev 1,2\nHailey Schoelkopf 2\nKeiran Paster 3,4\nMarco Dos Santos 5\nStephen McAleer 6\nAlbert Q. Jiang 5\nJia Deng 1\nStella Biderman 2\nSean Welleck 6,7\n1 Princeton University\n2 EleutherAI\n3 University of Toronto\n4 Vector Institute\n5 University of Cambridge\n6 Carnegie Mellon University\n7 University of Washington\nABSTRACT\nWe present LLEMMA, a large language model for mathematics. We continue\npretraining Code Llama on Proof-Pile-2, a mixture of scientific papers, web data\ncontaining mathematics, and mathematical code, yielding LLEMMA. On the MATH\nbenchmark LLEMMA outperforms all known open base models, as well as the\nunreleased Minerva model suite on an equi-parameter basis. Moreover, LLEMMA\nis capable of tool use and formal theorem proving without any further finetuning.\nWe openly release all artifacts, including 7 billion and 34 billion parameter models,\nthe Proof-Pile-2, and code to replicate our experiments.1\n1\nINTRODUCTION\n0\n20\n40\n60\n80\n# Params\n20%\n25%\n30%\n35%\n40%\n45%\n50%\nMATH Maj@256 (Accuracy)\nLlemma 7B\nLlemma 34B\nMinerva 8B\nMinerva 62B\n4-Shot MATH Performance\nFigure 1: Continued pretraining on Proof-\nPile-2 yields LLEMMA, a base model with\nimproved mathematical capabilities.\nLanguage models trained on diverse mixtures of\ntext display remarkably general language understand-\ning and generation capabilities (Brown et al., 2020;\nChowdhery et al., 2022), serving as base models that\nare adapted to a wide range of applications (Raffel\net al., 2023). Applications such as open-ended dia-\nlogue (Thoppilan et al., 2022; Touvron et al., 2023)\nor instruction following (Ouyang et al., 2022; Wei\net al., 2022) require balanced performance across the\nentire distribution of natural text, thus favoring gen-\neralist models. However, if we seek to maximize\nperformance within one domain, such as medicine\n(Singhal et al., 2022; 2023), finance (Wu et al., 2023),\nor science (Taylor et al., 2022), a domain-specific\nlanguage model may offer superior capabilities for\na given computational cost, or lower computational\ncost for a given level of capability.\nIn this work, we train a domain-specific language\nmodel for mathematics. We have several motivations\nfor doing so. First, solving mathematical problems requires pattern matching against a large body\nof specialized prior knowledge, thus serving as an ideal setting for domain adaptation. Second,\nmathematical reasoning is in itself a central AI task, its study dating back to at least Gelernter (1959)\nand Wang (1960) and continuing to today (Lu et al., 2023). Third, language models capable of\nstrong mathematical reasoning are upstream of a number of research topics, such as reward modeling\n(Uesato et al., 2022; Lightman et al., 2023), reinforcement learning for reasoning (Polu et al., 2022;\nLample et al., 2022), and algorithmic reasoning (Zhou et al., 2022; Zhang et al., 2023).\n1https://github.com/EleutherAI/math-lm\n1\narXiv:2310.10631v3  [cs.CL]  15 Mar 2024\nPublished as a conference paper at ICLR 2024\nAlthough domain-specific models for mathematics have been trained in the past, they have either\nbeen closed access (Lewkowycz et al., 2022), limiting their ability to become a platform for further\nresearch, or have lagged far behind the closed access state-of-the-art (Azerbayev et al., 2023).\nWe present a recipe for adapting a language model to mathematics through continued pretrain-\ning (Lewkowycz et al., 2022; Rozi\u00e8re et al., 2023) on Proof-Pile-2, a diverse mixture of math-related\ntext and code. Applying the recipe to Code Llama (Rozi\u00e8re et al., 2023) yields LLEMMA: 7 billion\nand 34 billion parameter base language models with substantially improved mathematical capabilities.\nSpecifically, our contributions are as follows:\n1. We train and release the LLEMMA models: 7B and 34B parameter language models specialized for\nmathematics. The LLEMMA models are a new state-of-the-art for publicly released base models\non MATH (Lewkowycz et al., 2022).\n2. We release the AlgebraicStack, a dataset of 11B tokens of code specifically related to mathematics.\n3. We demonstrate that LLEMMA is capable of using computational tools to solve mathematical\nproblems, namely, the Python interpreter and formal theorem provers.\n4. Unlike prior mathematics language models such as Minerva (Lewkowycz et al., 2022), the\nLLEMMA models are open access and we open source our training data and code. This allows\nLLEMMA to serve as a platform for future research in mathematical reasoning.\nOur work builds on findings in Minerva (Lewkowycz et al., 2022), but differs in several ways:\n(1) LLEMMA\u2019s training and evaluation covers a wider range of data and tasks, notably code data\n(e.g., the AlgebraicStack), tool use, and formal mathematics; (2) our work only depends on publicly\naccessible tools and data; (3) we provide new analyses related to the continued training data mixture,\nmemorization, and additional supervised finetuning; (4) we make all artifacts publicly available.\n2\nAPPROACH\nLLEMMA models are 7 billion and 34 billion parameter language models specialized for mathematics.\nOur approach is to continue pretraining Code Llama (Rozi\u00e8re et al., 2023) on the Proof-Pile-2.\nModel\nAdaptation tokens Open\nMinerva-8b\n164B\n\u2717\nMinerva-62b\n109B\n\u2717\nLLEMMA-7b (ours)\n200B\n\u2713\nLLEMMA-34b (ours)\n50B\n\u2713\nDataset\nTokens Open\nMinerva Dataset\n38.5B\n\u2717\nProof-Pile-2 (ours)\n55B\n\u2713\nCode (AlgebraicStack)\n11B\n\u2713\nOpenWebMath (Paster et al., 2023))\n15B\n\u2713\nArXiv (Computer, 2023))\n29B\n\u2713\nFigure 2: Comparison of LLEMMA and Minerva training\n2.1\nDATA: Proof-Pile-2\nWe form the Proof-Pile-2, a 55B-token mixture of scientific papers, web data containing mathematics,\nand mathematical code. With the exception of the Lean proofsteps subset (see Appendix B), the\nProof-Pile-2 has a knowledge cutoff of April 2023.\nCode.\nComputational tools such as numerical simulations, computer algebra systems, and formal\ntheorem provers are of ever increasing importance to mathematicians (Avigad, 2018). Motivated by\nthis fact, we create AlgebraicStack, an 11B-token dataset of source code from 17 languages, spanning\nnumerical, symbolic, and formal math. The dataset consists of filtered code from the Stack (Kocetkov\net al., 2022), public GitHub repositories, and formal proofstep data. Table 9 shows the number of\ntokens by language in AlgebraicStack. See Appendix B.1 for further details on AlgebraicStack.\nWeb data.\nWe use OpenWebMath (Paster et al., 2023), a 15B-token dataset of high-quality web\npages filtered for mathematical content. OpenWebMath filters CommonCrawl web pages based\n2\nPublished as a conference paper at ICLR 2024\non math-related keywords and a classifier-based math score, preserves mathematical formatting\n(e.g., LATEX, AsciiMath), and includes additional quality filters (e.g., perplexity, domain, length) and\nnear-deduplication. Refer to Paster et al. (2023) for a full description of OpenWebMath.\nScientific papers.\nWe use the ArXiv subset of RedPajama (Computer, 2023), an open-access\nreproduction of the LLaMA training dataset. The ArXiv subset contains 29B tokens.\nGeneral natural language and code data.\nFollowing Lewkowycz et al. (2022), our training\nmixture consists of a small amount of general domain data, which functions as a form of regularization.\nSince the pretraining dataset for LLaMA 2 is undisclosed, we use the Pile (Gao et al., 2020; Biderman\net al., 2022) as a surrogate training dataset. We set 95% of our training mixture to be the Proof-Pile-2,\n2% to be from the Pile (with ArXiv removed, as it is separately in Proof-Pile-2), and 3% to be the\nGitHub subset of RedPajama (Computer, 2023).\nFurther information on dataset composition and a datasheet are in Appendix B and Appendix E, re-\nspectively. We publicly release Proof-Pile-2 at hf.co/datasets/EleutherAI/proof-pile-2.\n2.2\nMODEL AND TRAINING\nEach model is initialized from Code Llama (Rozi\u00e8re et al., 2023). Code Llama models are decoder-\nonly transformer language models initialized from Llama 2 (Touvron et al., 2023) and further trained\non 500B tokens of code. We continue training the Code Llama models on Proof-Pile-2 using a\nstandard autoregressive language modeling objective. We train the 7B model for 200B tokens, and\nthe 34B model for 50B tokens.\nWe train all models in bfloat16 mixed precision using the GPT-NeoX library (Andonian et al., 2023)\nacross 256 A100 40GB GPUs. We use Tensor Parallelism (Shoeybi et al., 2019) with a world size\nof 2 for LLEMMA-7B , and a world size of 8 for LLEMMA-34B, alongside ZeRO Stage 1 sharded\noptimizer states (Rajbhandari et al., 2020) across Data Parallel (Goyal et al., 2017) replicas. We use\nFlash Attention 2 (Dao, 2023) to improve throughput and further reduce memory requirements.\nLLEMMA 7B is trained for 42, 000 steps with a global batch size of 4 million tokens and a 4096 token\ncontext length. This corresponds to roughly 23, 000 A100-hours. The learning rate is warmed up to\n1 \u00b7 10\u22124 over 500 steps, then set to cosine decay to 1/30th of the maximum learning rate over 48, 000\nsteps. The reason for the discrepancy between the number of training steps and the scheduler length\nis that we planned to train for 48, 000 steps, but encountered NaN losses after step 42, 000, likely\ncaused by unstable optimization or hardware failures (Elsen et al., 2023).\nLLEMMA 34B is trained for 12, 000 steps with a global batch size of 4 million tokens and a 4096\ncontext length. This corresponds to roughly 47, 000 A100-hours. The learning rate is warmed up to\n5 \u00b7 10\u22125 over 500 steps, then decayed to 1/30th the peak learning rate.\nBefore training LLEMMA 7B, we contract the RoPE (Su et al., 2022) base period of the Code Llama\n7B initialization from \u03b8 = 1, 000, 000 to \u03b8 = 10, 000. This is so that the long context finetuning\nprocedure described in Peng et al. (2023)and Rozi\u00e8re et al. (2023) can be repeated on the trained\nLLEMMA 7B (we leave actually doing so to future work). Due to compute constraints, we were\nunable to verify that training LLEMMA 34B with a contracted RoPE base period did not come with a\nperformance penalty, therefore for that model we preserved \u03b8 = 1, 000, 000.\n3\nEVALUATION\nOur goal is to evaluate LLEMMA as a base model for mathematical text. To this end, we compare\nLLEMMA models using few-shot evaluation (Brown et al., 2020), and primarily focus on state-of-the-\nart models that have not been finetuned on supervised examples for the task. First, we evaluate the\nmodel\u2019s ability to solve mathematics problems using chain of thought reasoning (Wei et al., 2023) and\nmajority voting (Wang et al., 2023). Our evaluations include MATH (Hendrycks et al., 2021b) and\nGSM8k (Cobbe et al., 2021), the de-facto standard benchmarks for evaluating quantitative reasoning\nin language models (Lewkowycz et al., 2022). Second, we explore few-shot tool use and formal\ntheorem proving. Third, we study the effects of memorization and the data mixture. Appendix G\ncontains a preliminary study of supervised finetuning with LLEMMA.\n3\nPublished as a conference paper at ICLR 2024\n3.1\nCHAIN-OF-THOUGHT MATHEMATICAL PROBLEM SOLVING\nThese tasks involve generating self-contained text solutions to problems expressed in LATEX or natural\nlanguage, without using external tools (Lewkowycz et al., 2022). We use the following evaluation:\n\u2022 MATH (Hendrycks et al., 2021b), a dataset with 12.5k problems (5k evaluation) from high-school\nmath competitions. Given a problem statement, the model generates a LATEXsolution and an answer\nthat must match a reference answer. We follow a similar task implementation to Lewkowycz et al.\n(2022), using their four-example prompt and evaluating answers for exact string match or SymPy\nequivalence.\n\u2022 GSM8k (Cobbe et al., 2021), a dataset of middle-school level math word problems. We use the\n8-shot prompt from Wei et al. (2023), as Lewkowycz et al. (2022) do not specify their evaluation\nprompt or number of few-shot examples.\n\u2022 OCWCourses (Lewkowycz et al., 2022), a collection of undergraduate-level STEM problems\nharvested from MIT\u2019s OpenCourseWare. We use the four-example prompt provided by (Lewkowycz\net al., 2022).\n\u2022 MMLU-STEM (Hendrycks et al., 2021a), a subset of 18 out of 57 subjects in the MMLU\nbenchmark. We follow Lewkowycz et al. (2022) and use their provided four-example chain-of-\nthought prompt.\n\u2022 SAT, we create a dataset consisting of the 32 math questions that do not contain figures from the\nMay 2023 College Board SAT examination, which is after our model\u2019s knowledge cutoff.\nInput:\nLet f(r) = P2008\nj=2\n1\njr =\n1\n2r +\n1\n3r + \u00b7 \u00b7 \u00b7 +\n1\n2008r . Find P\u221e\nk=2 f(k).\nLLEMMA 34B solution:\nWe have\n\u221e\nX\nk=2\nf(k) =\n\u221e\nX\nk=2\n2008\nX\nj=2\n1\njk =\n2008\nX\nj=2\n\u221e\nX\nk=2\n1\njk =\n2008\nX\nj=2\n1\nj2\n\u221e\nX\nk=0\n1\njk =\n2008\nX\nj=2\n1\nj2\n1\n1 \u2212 1\nj\n=\n2008\nX\nj=2\n1\nj(j \u2212 1) =\n2008\nX\nj=2\n\u0012\n1\nj \u2212 1 \u2212 1\nj\n\u0013\n=\n\u00121\n1 \u2212 1\n2\n\u0013\n+\n\u00121\n2 \u2212 1\n3\n\u0013\n+ \u00b7 \u00b7 \u00b7 +\n\u0012\n1\n2007 \u2212\n1\n2008\n\u0013\n= 1 \u2212\n1\n2008\n= 2007\n2008 .\nFinal Answer: The final answer is 2007\n2008.\nFigure 3: Example of a LLEMMA 34B solution to a MATH (Hendrycks et al., 2021a) problem. This\nproblem is tagged with difficulty level 5, the highest in MATH. The model was conditioned on the\n4-shot prompt described in subsection 3.1, and the solution was produced by greedy decoding. The\nmodel had to apply two nontrivial steps to solve this problem: (1) noticing that swapping the order of\nsummation simplifies the problem, and (2) noticing that the resulting sum telescopes.\nWe compare with Minerva (Lewkowycz et al., 2022), which continued pretraining the PaLM language\nmodel on a dataset of technical content; Code Llama, the initialization of LLEMMA\u2019s continued\npretraining; and Llama 2, the initialization of Code Llama\u2019s continued pretraining on code. For open\naccess models, we report scores computed using our evaluation suite, which is implemented as a\nfork of the Language Model Evaluation Harness (Gao et al., 2021). For Minerva models, we report\nbenchmark scores from Lewkowycz et al. (2022).\n4\nPublished as a conference paper at ICLR 2024\nResults.\nLLEMMA\u2019s continued pretraining on Proof-Pile-2 improves few-shot performance on the\nfive mathematical benchmarks. LLEMMA 34B improves over Code Llama by 20 percentage points\non GSM8k and 13 points on MATH, and LLEMMA 7B outperforms the proprietary Minerva model.\nOur approach also outperforms all open-weight language models at the time of writing. We conclude\nthat continued pretraining on Proof-Pile-2 is effective for improving a pretrained model\u2019s ability to\nperform mathematical problem solving.\nLLEMMA is pretrained on a diverse distribution of mathematics-related data, and is not tuned for a\nparticular task. Therefore, we expect that LLEMMA can adapt to many other tasks via task-specific\nfinetuning and few-shot prompting.\nGSM8k\nOCW\nMMLU-STEM\nSAT\nMATH\nLlama 2\n7B\n11.8%\n3.7%\n29.9%\n25.0%\n3.2%\nCode Llama\n7B\n10.5%\n4.4%\n25.1%\n9.4%\n4.5%\nMinerva\n8B\n16.2%\n7.7%\n35.6%\n-\n14.1%\nLLEMMA\n7B\n36.4%\n7.7%\n37.7%\n53.1%\n18.0%\nCode Llama\n34B\n29.6%\n7.0%\n40.5%\n40.6%\n12.2%\nLLEMMA\n34B\n51.5%\n11.8%\n49.0%\n71.9%\n25.0%\nMinerva\n62B\n52.4%\n12.0%\n53.9%\n-\n27.6%\nMinerva\n540B\n58.8%\n17.6%\n63.9%\n-\n33.6%\nTable 1: Results on our five chain-of-thought reasoning tasks with samples generated via greedy\ndecoding. Minerva results are quoted from Lewkowycz et al. (2022). Note that CodeLlama 7B\nperforms worse than random guessing (25%) on MMLU and SAT, largely due to failing to conclude\nits chain of thought with a valid answer.\nGSM8k\nOCW\nMMLU-STEM\nSAT\nMATH\nmaj@k\nmaj@k\nmaj@k\nmaj@k\nmaj@k\nMinerva\n8B\n28.4%\n12.5%\n43.4%\n-\n25.4%\nLLEMMA\n7B\n54.0%\n14.3%\n49.9%\n78.1%\n33.5%\nLLEMMA\n34B\n69.3%\n18.4%\n59.7%\n81.3%\n43.1%\nMinerva\n62B\n68.5%\n23.5%\n63.5%\n-\n43.4%\nMinerva\n540B\n78.5%\n30.8%\n75.0%\n-\n50.3%\nTable 2: Majority voting results for LLEMMA and Minerva. Minerva results are quoted from\nLewkowycz et al. (2022). Voting is done with k = 256 for MATH, k = 100 for GSM8k and OCW,\nand k = 16 for MMLU-STEM and SAT. We sample with temperature T = 0.6 for k = 256 and\nk = 100 and T = 0.3 for k = 16, and use nucleus sampling with p = 0.95 (Holtzman et al., 2020).\nDue to compute constraints, we do not calculate majority voting scores for Llama 2 and Code Llama.\n3.2\nMATHEMATICAL PROBLEM SOLVING WITH TOOL USE\nThese tasks involve solving problems with access to computational tools. We evaluate the following:\n\u2022 MATH+Python, the model is prompted to alternately describe a solution step in natural language,\nthen execute that step with code. The final answer is a program that executes to a numeric type or a\nSymPy object. Our few-shot prompt includes examples that use built-in numeric operations, the\nmath module, and SymPy.\n\u2022 GSM8k+Python, solving a GSM8k word problem by writing a Python program that executes to\nan integer answer. We use the prompt from Gao et al. (2023).\nResults.\nAs seen in Table 3, LLEMMA improves over Code Llama on both tasks. Its performance\non MATH and GSM8k with tools is also higher than its performance on these datasets without tools.\n5\nPublished as a conference paper at ICLR 2024\nGSM8k+Python\nMATH+Python\npass@1\npass@1\nCode Llama\n7B\n27.1%\n17.2%\nLLEMMA\n7B\n40.1%\n21.5%\nCode Llama\n34B\n52.7%\n23.5%\nLLEMMA\n34B\n62.6%\n27.1%\nTable 3: Mathematical problem solving with tool use.\n3.3\nFORMAL MATHEMATICS\nInteractive proof assistants such as Lean (de Moura et al., 2015), Isabelle (Wenzel et al., 2008),\nand Coq (Paulin-Mohring, 1989a;b) express mathematics in programming languages that allow for\nverification. These languages are data scarce compared to mainstream languages, especially in\nthe context of pretraining. For instance, the Stack dataset used to pretrain language models in the\nBigCode project (Allal et al., 2023) has over 700 gigabytes of Python, compared to 322 megabytes of\nLean. Proof assistants also require models to leverage information that is not present in raw source\ncode, such as goal states that contain information about each step of a proof.\nProblem (MATH Number theory 185): When a number is divided by 5, the remainder is 3.\nWhat is the remainder when twice the number is divided by 5? Show that it is 1.\nHuman-written informal proof: If our number is n, then n \u2261 3 (mod 5). This tells us that\n2n = n + n \u2261 3 + 3 \u2261 1\n(mod 5).\nThe remainder is 1 when the number is divided by 5.\nInformal-to-formal (Isabelle):\n{Problem, human-written informal proof}\ntheorem mathd_numbertheory_185:\nfixes n ::nat\nassumes \"n mod 5 = 3\"\nshows \"(2 * n) mod 5 = 1\"\nproof -\nhave \"2 * n = n + n\"\n<ATP>\nalso have \". . . mod 5 =\n(n mod 5 + n mod 5) mod 5\" <ATP>\nalso have \". . . = (3 + 3) mod 5\"\nusing assms <ATP>\nalso have \". . . = 1\" <ATP>\nfinally show ?thesis <ATP>\nqed\nFormal-to-formal (Lean 4):\ntheorem mathd_numbertheory_185\n(n : N) (h0 : n % 5 = 3)\n: 2 * n % 5 = 1 := by\n-- INPUT (step 1):\n--\nn: N\n--\nh0: n % 5 = 3\n--\n\u22a2 2 * n % 5 = 1\nrw [mul_mod, h0]\n-- INPUT (step 2):\n--\nn: N\n--\nh0: n % 5 = 3\n--\n\u22a2 2 % 5 * 3 % 5 = 1\nsimp only [h0, mul_one]\nFigure 4: Example formal proofs from LLEMMA-7b. Left: The model is given a problem, informal\nproof, and formal statement, following Jiang et al. (2023). It generates a formal proof (starting with\nproof -) containing Isabelle code and calls to automation (shown as <ATP>). Right: The model is\ngiven a proof state, visualized as a grey comment, and generates the subsequent step (e.g. rw [..).\nProof-Pile-2\u2019s AlgebraicStack contains over 1.5 billion tokens of formal mathematics data, including\nproof states extracted from Lean and Isabelle formalizations. While a full investigation of formal\nmath is outside the scope of this paper, we evaluate LLEMMA few-shot on two tasks:\n6\nPublished as a conference paper at ICLR 2024\n\u2022 Informal-to-formal proving (Jiang et al., 2023), the task of generating a formal proof, given a\nformal statement, an informal LATEX statement, and an informal LATEX proof. The formal proof is\nchecked by the proof assistant. We use the Isabelle proof assistant and evaluate on miniF2F (Zheng\net al., 2021), a benchmark consisting of problem statements from Olympiads and undergraduate\ncoursework. For the prompt, we use 11 (formal statement, informal statement, informal proof,\nformal proof) examples from Jiang et al. (2023), selecting 7 examples for number theory problems,\nand 6 examples for all others. We generate a single proof with greedy decoding.\n\u2022 Formal-to-formal proving (e.g., Polu & Sutskever (2020)), the task of proving a formal statement\nby generating a sequence of proof steps (tactics). At each step, the input is a state xt given by the\nproof assistant, and the language model\u2019s task is to generate a proof step yt (a sequence of code).\nThe proof step is checked by the proof assistant, yielding a new state xt+1 or an error message.\nThe process continues, stopping if a proof is completed or a timeout is reached. We prompt the\nmodel using three (xt, yt) examples. We evaluate on miniF2F (Zheng et al., 2021) using the Lean\n4 proof assistant, and use a standard best first search. See Appendix D for more details.\nResults.\nAs seen in Table 4, LLEMMA\u2019s continued pretraining on Proof-Pile-2 improved few-shot\nperformance on the two formal theorem proving tasks.\nMethod\nInformal-to-formal\nminiF2F-valid\nminiF2F-test\nSledgehammer\n14.72%\n20.49%\nCode Llama 7b\n16.31%\n17.62%\nCode Llama 34b\n18.45%\n18.03%\nLLEMMA-7b\n20.60%\n22.13%\nLLEMMA-34b\n21.03%\n21.31%\nMethod\nFormal-to-formal\nSearch\nminiF2F-test\nReProver (fine-tuned)\n1\u00d764\n26.50%\nCode Llama 7b\n1\u00d732\n20.49%\nCode Llama 34b\n1\u00d732\n22.13%\nCOPRA (GPT-4)\n-\u2020\n23.36%\nLLEMMA-7b\n1\u00d732\n26.23%\nLLEMMA-34b\n1\u00d732\n25.82%\nTable 4: Formal theorem proving tasks. Left: Informal-to-formal proving in Isabelle, showing the\npercentage of proven theorems with greedy decoding. Right: Formal-to-formal proving in Lean,\nshowing the percentage of proven theorems with the given number of attempts \u00d7 generations-per-\niteration of best first search, and a 10-minute timeout. Sledgehammer (Paulson & Nipkow, 2023) is\nbuilt-in Isabelle automation. ReProver (Yang et al., 2023) is a supervised and retrieval-augmented\nmodel. COPRA (Thakur et al., 2023) is a retrieval-augmented GPT-4 based method. \u2020 COPRA does\nnot use best first search, but instead samples from GPT-4 (OpenAI, 2023) a maximum of 60 times.\nOn informal-to-formal proving, LLEMMA-7b closes 22.1% of the theorems, improving upon its\nCode Llama initialization and the Sledgehammer prover. The theorems that LLEMMA proves are\noften complementary to those proved with Sledgehammer: taking the union of Sledgehammer and\nLLEMMA proofs results in 26 new validation proofs (an 11 percentage-point increase), and 17 new\ntest proofs (a 7 point increase); see Appendix Table 11. Prior to our work, the only demonstration of\nfew-shot proof autoformalization used the proprietary Codex model (Jiang et al., 2023).\nOn Lean 4 formal-to-formal proving, LLEMMA-7b improves upon its Code Llama initialization, and\nperforms similar to ReProver (Yang et al., 2023), a retrieval-augmented language model finetuned for\ntactic prediction. LLEMMA adapts to the task using a 3 example prompt, which to our knowledge is\nthe first demonstration of few-shot tactic prediction for theorem proving by an open model.\n3.4\nIMPACT OF DATA MIXTURE\nWhen training a language model, it is common to upsample high-quality subsets of the training data\naccording to mixture weights (Brown et al., 2020; Gao et al., 2020; Xie et al., 2023). We select\nmixture weights by doing short training runs on several hand-picked mixture weights, then choosing\nthe one which minimizes perplexity on a set of high-quality held-out text (we use the MATH training\nset). Table 5 shows the MATH training set perplexity of models trained using different mixtures of\narXiv to web to code. Based on these results, we trained LLEMMA with a ratio of 2 : 4 : 1. Note\nthat our methodology uses the MATH training set to determine a training hyperparameter, though we\nexpect that the effect is similar to that of related high-quality texts.\n7\nPublished as a conference paper at ICLR 2024\nMixture\nMATH training set perplexity\nOverall\nPrealgebra\nAlgebra\nNumber\nTheory\nCounting &\nProbability\nGeometry\nIntermediate\nAlgebra\nPrecalculus\n2:4:1\n1.478\n1.495\n1.515\n1.552\n1.475\n1.519\n1.439\n1.331\n2:4:2\n1.482\n1.500\n1.519\n1.556\n1.477\n1.524\n1.443\n1.334\n4:2:1\n1.487\n1.505\n1.524\n1.561\n1.481\n1.534\n1.447\n1.338\n4:2:2\n1.489\n1.508\n1.527\n1.562\n1.483\n1.538\n1.447\n1.339\n4:4:1\n1.487\n1.506\n1.525\n1.561\n1.482\n1.529\n1.446\n1.335\n4:4:2\n1.485\n1.503\n1.523\n1.559\n1.480\n1.529\n1.444\n1.334\nTable 5: MATH training set perplexity of Code Llama 7B models trained using different data mixtures\nfor a reduced number of steps. Each mixture is represented by its arXiv:Web:Code ratio.\n3.5\nDATASET OVERLAP AND MEMORIZATION\nDo test problems or solutions appear in the corpus?\nWe check whether any 30-gram in a\ntest sequence (either an input problem or an output solution) occurs in any OpenWebMath or\nAlgebraicStack document. If so, we say that a hit occurred between the sequence and the document.\nTable 6 shows hits between sequences from MATH and documents from Proof-Pile-2. Using our\nmethodology, around 7% of MATH test problem statements and 0.6% of MATH test solutions have\nhits. Note that our methodology gives a lower bound on the number of semantically equivalent\nsequences (e.g., it does not account for alternative phrasing).\nWe manually inspected 100 uniformly sampled hits between a test problem statement and an Open-\nWebMath document. 41 of the cases had no solution, which included websites with a list of problems,\ndiscussions, or hints. 49 had an alternative solution to the MATH ground-truth solution, but with\nthe same answer. These include solutions that solve the problem differently than the ground-truth,\nsolutions with missing details, and discussions that include the answer. 9 cases had a missing or\nincorrect answer, and 1 had the same solution as in the ground-truth. In summary, we find that\nsolutions can appear in a corpus derived from web documents, particularly alternative solutions to\nthose in the evaluation set. We repeated our analysis with 20-gram hits and our findings were similar,\nthough with false positives; see Appendix Figure 6 for examples.\nProblem\nSolution\nProof-Pile-2\nTest\nExample Docs Example Docs\nOpenWebMath\nMATH\n348\n717\n34\n46\nAlgebraicStack MATH\n3\n3\n1\n1\nOpenWebMath GSM8k\n2\n3\n0\n0\nAlgebraicStack GSM8k\n0\n0\n0\n0\nSame solution\n1\nDifferent solution, same answer\n49\nDifferent solution, different answer\n9\nNo solution\n41\nDifferent problem\n0\nTable 6: Left: 30-gram hits between MATH test problems or solutions and Proof-Pile-2 documents.\nExample and Docs are the numbers of unique test examples and Proof-Pile-2 documents with a hit.\nRight: manual inspection of 100 hits between a problem statement and a Proof-Pile-2 document.\nMATH\nHit\nNonhit\n# Hits\nLevel\nAccuracy\nAccuracy\nLevel 1\n72.73\n61.50\n11\nLevel 2\n35.71\n40.18\n28\nLevel 3\n30.36\n26.88\n56\nLevel 4\n14.89\n16.61\n94\nLevel 5\n6.08\n6.39\n181\nTable 7: LLEMMA-34b\u2019s accuracy on hits\n(a 30-gram overlap between a problem or\nsolution and a training sequence) and non-\nhits by MATH difficulty level.\nHow do problems in the corpus impact performance?\nNext, we evaluate LLEMMA-34b on the test examples\nwith a 30-gram hit, and the test examples without a 30-\ngram hit. Table 7 shows the accuracy partitioned by\nMATH difficulty level. The model\u2019s accuracy remains\nlow on difficult problems (e.g., 6.08% on Level 5 prob-\nlems with a hit, versus 6.39% on problems without a hit),\nand we observe no clear relationship between 30-gram\nhits and accuracy across difficulty levels. We conclude\nthat a nontrivial match between a test example and a\ntraining document did not imply that the model gen-\nerated a memorized correct answer. We repeated the\nanalysis with 20-grams and with the 7b model, and our\nfindings were analogous. Figure 7 shows an example.\n8\nPublished as a conference paper at ICLR 2024\nFinally, we check 30-gram hits between LLEMMA\u2019s MATH generations and OpenWebMath. There\nwere 13 hits, which occurred when the model generated a common sequence of numbers (e.g., a list\nof Fibonacci numbers), plus one instance of factoring a polynomial. Appendix Figure 6 shows an\nexample. We find all of these observations worthy of further study. Using LLEMMA and Proof-Pile-2\nto better understand data, memorization, and performance is an interesting future direction. We\ninclude the code for our analysis in the LLEMMA repository.\n4\nRELATED WORK\nLarge-scale language modeling. Recent progress in large language models involves two connected\nthreads: the increasing scale of models and data (Hoffmann et al., 2022; Kaplan et al., 2020;\nChowdhery et al., 2022), and a progression toward more generalist models (Radford et al., 2019;\nBrown et al., 2020) which are capable of solving diverse problems and adapting quickly to novel\ntasks. A third thread relates to enabling open access to language models with these capabilities\n(Black et al., 2022; Biderman et al., 2023; Touvron et al., 2023; Rozi\u00e8re et al., 2023). Our work\nprovides a recipe for specializing these language models to the domain of mathematics, providing a\nplatform for further research and applications.\nDomain adaptation. Language model applications typically require a general-domain pretraining\nstep, followed by a shorter fine-tuning step. The finetuning step is often aimed at imbuing instruction-\nfollowing ability (Sanh et al., 2022; Wei et al., 2022) or aligning a model\u2019s outputs with human\npreferences (Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022). Other work explores adapting\npretrained models to novel domains by continued training (Rozi\u00e8re et al., 2023; Beltagy et al., 2019),\nparameter-efficient finetuning methods (Yong et al., 2023), retrieval augmentation (Min et al., 2023;\nAsai et al., 2023), and other techniques. We provide an adaptation recipe involving continued training\nand targeted data collection.\nLanguage models for mathematics. Applying large language models to problems in mathematics\nis an active subfield of machine learning, including benchmarking mathematical knowledge and\nreasoning at varying levels (Hendrycks et al., 2021b; Zheng et al., 2021; Welleck et al., 2022;\nAzerbayev et al., 2023). Although achieving strong mathematical reasoning is an important target, it\nis difficult to assess the correctness of models\u2019 answers and processes, especially as models become\nmore capable (Bowman et al., 2022; Uesato et al., 2022; Lightman et al., 2023; Cobbe et al., 2021).\nA number of recent works focus on supervised finetuning on task-relevant (input, output) pairs\n(e.g.,Yu et al. (2023); Yue et al. (2023)). Doing so boosts performance on some common mathematical\nlanguage modeling benchmarks, but trains the model for these specific tasks. In contrast, Lewkowycz\net al. (2022) and our work seek to train a base language model as a platform for further development.\nLanguage models for formal mathematics. An ongoing line of work explores integrating language\nmodels with interactive proof assistants in the context of mathematics. This includes synthesizing\nproofs via tactic prediction (Polu & Sutskever, 2020; Han et al., 2022; Lample et al., 2022; Jiang\net al., 2022), autoformalization (Wu et al., 2022; Jiang et al., 2023), and integrated tools (Welleck\n& Saha, 2023). Due to high computational costs of search, language models applied to this domain\nhave traditionally been small, but recent work has demonstrated promise in the use of larger models\n(First et al., 2023; Jiang et al., 2023). Our work provides a demonstration of few-shot proof autofor-\nmalization and tactic prediction, a large collection of formal mathematics data, along with an open\naccess model for further exploring these directions.\n5\nCONCLUSION\nWe introduce LLEMMA and Proof-Pile-2, a novel base model and corpus for language modeling of\nmathematics. Our models, dataset, and code are openly available. We have shown that LLEMMA\nachieves state-of-the-art results for open-weights models on mathematical problem solving bench-\nmarks, shown capabilities of using external tools via Python code, and demonstrated few-shot tactic\nprediction for theorem proving. We hope that LLEMMA and Proof-Pile-2 will be a useful base for\nfuture work on understanding language model generalization and dataset composition, investigating\nthe limits of domain-specific language models, using language models as tools for mathematicians,\nand improving the mathematical capabilities of language models.\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGEMENTS\nWe would like to thank Dragomir Radev, Arman Cohan, Jesse Michael Han, and the Deepmind\nBlueshift team for valuable guidance. We thank Jonah Philion for the model name. We thank Aviya\nSkowron for advising us on ethical considerations in the development and release of our models. We\nthank Jonathan Laurent and Leo Du for contributions to our open-source code.\nWe would also like to thank several parties for donating computing resources for this project:\nStability AI (training the LLEMMA models), CoreWeave (evaluations and finetuning), the\nProvince of Ontario and companies sponsoring the Vector Institute for Artificial Intelligence\n(www.vectorinstitute.ai/partners), and Brigham Young University (finetuning). KP is\nsupported by an NSERC PGS-D award.\nREFERENCES\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi,\nCarolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin,\nDmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc\u00eda del\nR\u00edo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas,\nMarco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia\nLi, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries,\nand Leandro von Werra. Santacoder: don\u2019t reach for the stars! In Deep Learning for Code (DL4C)\nWorkshop, 2023.\nAlex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric\nHallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Jason\nPhang, Shivanshu Purohit, Hailey Schoelkopf, Dashiell Stander, Tri Songz, Curt Tigges, Benjamin\nTh\u00e9rien, Phil Wang, and Samuel Weinbach. GPT-NeoX: Large scale autoregressive language mod-\neling in PyTorch. GitHub Repo, 9 2023. URL https://www.github.com/eleutherai/\ngpt-neox.\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and\napplications. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 6: Tutorial Abstracts), pp. 41\u201346, Toronto, Canada, July 2023. Associ-\nation for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-tutorials.6.\nURL https:\n//aclanthology.org/2023.acl-tutorials.6.\nJeremy Avigad. The mechanization of mathematics. Notices of the AMS, 65(6):681\u201390, 2018.\nZhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir R. Radev,\nand Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathe-\nmatics. ArXiv, abs/2302.12433, 2023.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson\nKernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez,\nTristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario\nAmodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862, 2022.\nIz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),\npp. 3615\u20133620, Hong Kong, China, November 2019. Association for Computational Linguistics.\ndoi: 10.18653/v1/D19-1371. URL https://aclanthology.org/D19-1371.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.\nPythia: A suite for analyzing large language models across training and scaling. In International\nConference on Machine Learning, pp. 2397\u20132430. PMLR, 2023.\n10\nPublished as a conference paper at ICLR 2024\nStella Rose Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile. ArXiv, abs/2201.07311,\n2022.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,\nHorace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source\nautoregressive language model. In Proceedings of BigScience Episode# 5\u2013Workshop on Challenges\n& Perspectives in Creating Large Language Models, pp. 95\u2013136, 2022.\nSamuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamil\u02d9e\nLuko\u0161i\u00afut\u02d9e, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-\nJohnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal\nNdousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph, Noem\u00ed Mercado, Nova\nDasSarma, Robin Larson, Sam McCandlish, Sandipan Kundu, Scott Johnston, Shauna Kravec,\nSheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan\nHume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, and Jared Kaplan. Measuring progress on\nscalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,\n2021.\nKatherine M. Collins, Albert Q. Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt,\nThomas Lukasiewicz, Yuhuai Wu, Joshua B. Tenenbaum, William Hart, Timothy Gowers, Wenda\nLi, Adrian Weller, and Mateja Jamnik. Evaluating language models for mathematics through\ninteractions. arXiv preprint arXiv:2306.01694, 2023.\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, April\n2023. URL https://github.com/togethercomputer/RedPajama-Data.\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv\npreprint arXiv:2307.08691, 2023.\nLeonardo de Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The\nlean theorem prover (system description). In Automated Deduction-CADE-25: 25th International\nConference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25, pp.\n378\u2013388. Springer, 2015.\nErich Elsen, Curtis Hawthorne, and Arushi Somani. The adventure of the errant hardware, 2023.\nURL https://www.adept.ai/blog/sherlock-sdc.\nEmily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun. Baldur: Whole-proof generation and\nrepair with large language models. arXiv preprint arXiv:2303.04910, 2023.\nLeo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An\n800gb dataset of diverse text for language modeling. ArXiv, abs/2101.00027, 2020.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff,\nJason Ociepa, Chris Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika,\n11\nPublished as a conference paper at ICLR 2024\nEric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot\nlanguage model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.\n5371628.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2023.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,\nHal Daum\u00e9 III au2, and Kate Crawford. Datasheets for datasets, 2021.\nHerbert L. Gelernter. Realization of a geometry theorem proving machine. In IFIP Congress, 1959.\nURL https://api.semanticscholar.org/CorpusID:18484295.\nPriya Goyal, Piotr Doll\u00e1r, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,\nAndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet\nin 1 hour. CoRR, abs/1706.02677, 2017. URL http://arxiv.org/abs/1706.02677.\nJesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu. Proof artifact co-\ntraining for theorem proving with language models. In International Conference on Learning\nRepresentations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS,\n2021b.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\nSimon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training\ncompute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration, 2020.\nAlbert Q. Jiang, Wenda Li, Jesse Michael Han, and Yuhuai Wu. Lisa: Language models of isabelle\nproofs. 6th Conference on Artificial Intelligence and Theorem Proving, 2021.\nAlbert Q. Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzyg\u00f3\u00b4zd\u00b4z, Piotr\nMi\u0142o\u00b4s, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language models\nand automated theorem provers. arXiv preprint arXiv:2205.10893, 2022.\nAlbert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu. Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs. In The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.net/forum?id=SMa9EAovKMC.\nJared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. arXiv\npreprint arXiv:2001.08361, 2020.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u00f1oz Ferrandis,\nYacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von\nWerra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. Preprint, 2022.\nGuillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel\nEbner, Aur\u00e9lien Rodriguez, and Timoth\u00e9e Lacroix. Hypertree proof search for neural theorem\nproving. arXiv preprint arXiv:2205.11491, 2022.\n12\nPublished as a conference paper at ICLR 2024\nAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai\nWu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems\nwith language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho\n(eds.), Advances in Neural Information Processing Systems, 2022.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint\narXiv:2305.20050, 2023.\nPan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning\nfor mathematical reasoning. In Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 14605\u201314631, Toronto, Canada, July\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.817. URL\nhttps://aclanthology.org/2023.acl-long.817.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng,\nQingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical\nreasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583,\n2023.\nThe mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certified Programs and Proofs, CPP 2020, pp. 367\u2013381, New York,\nNY, USA, 2020. Association for Computing Machinery. ISBN 9781450370974. doi: 10.1145/\n3372885.3373824. URL https://doi.org/10.1145/3372885.3373824.\nSewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke\nZettlemoyer. Silo language models: Isolating legal risk in a nonparametric datastore, 2023.\nScott\nMorrison.\nlean-training-data.\nhttps://github.com/semorrison/\nlean-training-data, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022.\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open\ndataset of high-quality mathematical web text. CoRR, abs/2310.06786, 2023. doi: 10.48550/\nARXIV.2310.06786. URL https://doi.org/10.48550/arXiv.2310.06786.\nChristine Paulin-Mohring. Extracting \u03c9\u2019s programs from proofs in the calculus of constructions.\nIn Proceedings of the 16th ACM SIGPLAN-SIGACT symposium on Principles of programming\nlanguages, pp. 89\u2013104, 1989a.\nChristine Paulin-Mohring. Extraction de programmes dans le Calcul des Constructions. PhD thesis,\nUniversit\u00e9 Paris-Diderot-Paris VII, 1989b.\nLarry\nPaulson\nand\nTobias\nNipkow.\nThe\nsledgehammer:\nLet\nautomatic\ntheorem\nprovers write your isabelle scripts!, 2023.\nURL https://isabelle.in.tum.de/\nwebsite-Isabelle2009-1/sledgehammer.html.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\nextension of large language models. arXiv preprint arXiv:2309.00071, 2023.\nStanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.\narXiv preprint arXiv:2009.03393, 2020.\nStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\nSutskever. Formal mathematics statement curriculum learning. arXiv preprint arXiv:2202.01344,\n2022.\n13\nPublished as a conference paper at ICLR 2024\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer, 2023.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory optimizations\ntoward training trillion parameter models. In Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis, SC \u201920. IEEE Press, 2020. ISBN\n9781728199986. doi: 10.5555/3433701.3433727. URL https://dl.acm.org/doi/10.\n5555/3433701.3433727.\nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,\nManish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade\nCopet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel\nSynnaeve. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen\nXu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani,\nNihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica,\nSheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,\nJos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan,\nTali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted\ntraining enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2022.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-LM: Training multi-billion parameter language models using model par-\nallelism.\nComputing Research Repository, 2019.\ndoi: 10.48550/arXiv.1909.08053.\nURL\nhttps://arxiv.org/abs/1909.08053v4. Version 4.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul\nGamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera\ny Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad\nTomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam,\nand Vivek Natarajan. Large language models encode clinical knowledge, 2022.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen\nPfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami\nLachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera\ny Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle\nBarral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam,\nand Vivek Natarajan. Towards expert-level medical question answering with large language models,\n2023.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2022.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science,\n2022.\nAmitayush Thakur, Yeming Wen, and Swarat Chaudhuri. A language-agent approach to formal\ntheorem-proving, 2023.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven\nZheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin,\nJames Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent\n14\nPublished as a conference paper at ICLR 2024\nZhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh\nSrinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi,\nRenelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron\nCohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi,\nand Quoc Le. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239,\n2022.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and\noutcome-based feedback, 2022.\nH. Wang. Toward mechanical mathematics. IBM Journal of Research and Development, 4(1):2\u201322,\n1960. doi: 10.1147/rd.41.0002.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=1PL1NIMMrw.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,\nand Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\nSean Welleck.\nNeural theorem proving tutorial.\nhttps://github.com/wellecks/\nntptutorial, 2023.\nSean Welleck and Rahul Saha. llmstep: Llm proofstep suggestions in lean. https://github.\ncom/wellecks/llmstep, 2023.\nSean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover:\nGrounded mathematical proof generation with language models. In Alice H. Oh, Alekh Agarwal,\nDanielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems,\n2022. URL https://openreview.net/forum?id=rhdfTOiXBng.\nMakarius Wenzel, Lawrence C Paulson, and Tobias Nipkow. The isabelle framework. In Theorem\nProving in Higher Order Logics: 21st International Conference, TPHOLs 2008, Montreal, Canada,\nAugust 18-21, 2008. Proceedings 21, pp. 33\u201338. Springer, 2008.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhan-\njan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for\nfinance, 2023.\n15\nPublished as a conference paper at ICLR 2024\nYuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Norman Rabe, Charles E Staats, Mateja\nJamnik, and Christian Szegedy. Autoformalization with large language models. In Alice H. Oh,\nAlekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information\nProcessing Systems, 2022. URL https://openreview.net/forum?id=IUikebJ1Bf0.\nSang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V.\nLe, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model\npretraining. arXiv preprint arXiv:2305.10429, 2023.\nKaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan\nPrenger, and Anima Anandkumar. LeanDojo: Theorem proving with retrieval-augmented language\nmodels. In Neural Information Processing Systems (NeurIPS), 2023.\nZheng Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani,\nKhalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Winata,\nStella Biderman, Edward Raff, Dragomir Radev, and Vassilina Nikoulina. BLOOM+1: Adding\nlanguage support to BLOOM for zero-shot prompting. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11682\u201311703,\nToronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.\nacl-long.653. URL https://aclanthology.org/2023.acl-long.653.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok,\nZhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical\nquestions for large language models. arXiv preprint arXiv:2309.12284, 2023.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu\nChen. Mammoth: Building math generalist models through hybrid instruction tuning. CoRR,\nabs/2309.05653, 2023.\ndoi: 10.48550/arXiv.2309.05653.\nURL https://doi.org/10.\n48550/arXiv.2309.05653.\nShizhuo Dylan Zhang, Curt Tigges, Stella Biderman, Maxim Raginsky, and Talia Ringer. Can\ntransformers learn to solve problems recursively?, 2023.\nKunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark for\nformal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021.\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi.\nTeaching algorithmic reasoning via in-context learning, 2022.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593, 2019.\n16\nPublished as a conference paper at ICLR 2024\nA\nAUTHOR CONTRIBUTIONS\nTraining Data. Zhangir Azerbayev, Keiran Paster, Marco Dos Santos, Sean Welleck.\nModel training. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster.\nEvaluations. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen\nMcAleer, Albert Q. Jiang, Sean Welleck.\nFormal math evaluations. Sean Welleck.\nMemorization analysis. Sean Welleck, Keiran Paster.\nSenior Authorship and Advising. Jia Deng, Stella Biderman, Sean Welleck.\nB\nDATA: Proof-Pile-2\nData source\nTokens\nWeight\nProof-Pile-2\n55B\n\u2013\nCode (AlgebraicStack)\n11B\n1.00\nWeb (OpenWebMath)\n15B\n4.00\nPapers (ArXiv)\n29B\n2.00\nGeneral code (RedPajama)\n59B\n0.22\nGeneral language (Pile)\n300B\n0.15\nTable 8: Proof-Pile-2 data sources (top), general language and code data included during training\n(bottom), and the mixture weights of each component during training.\nB.1\nMATHEMATICAL CODE: AlgebraicStack\nAlgebraicStack contains roughly 11B tokens of code related to mathematics. We describe its sources,\nfiltering, and content below. Table 9 shows the number of tokens per language in AlgebraicStack.\nLanguage\nAlgebraicStack tokens\nLanguage\nAlgebraicStack tokens\nAgda\n35.2 M\nJulia\n531.0 M\nC\n25.1 M\nJupyter\n199.1 M\nC++\n954.1 M\nLean\n285.6 M\nCoq\n281.9 M\nMaple\n2.0 M\nFortran\n724.9 M\nMatlab\n65.8 M\nGAP\n3.6 M\nPython\n6,098.8 M\nHaskell\n9.1 M\nR\n71.3 M\nIdris\n10.9 M\nTex\n567.7 M\nIsabelle\n1,089.7 M\nTotal\n10,955.7 M\nTable 9: Tokens in AlgebraicStack, computed with the Llama tokenizer.\nB.1.1\nGITHUB CODE\nThe following programming languages were either barely present in the Stack or consisted of largely\nincorrect filetypes, so we downloaded data for these languages directly via the Github Python API.\n\u2022 Coq : We filter for files with the .v extension, and include Coq via including files that\nmatch a heuristic filter for the keywords \"Theorem\", \"Proof\", \"Qed\", \"Inductive\",\n\"Definition\", \"Fixpoint\" and exclude Verilog files via the keyword blacklist \"pragma\",\n\"endmodule\", \"posedge\", \"negedge\", \"wire\". We additionally exclude files noted as\nautomatically generated.\n17\nPublished as a conference paper at ICLR 2024\n\u2022 Isabelle\n:\nWe\nfilter\nfor\nfiles\nwith\nthe\n.thy\nextension\nand\ninclude\nfiles\nmatching\nthe\nkeyword\nwhitelist\n\"theorem\n\",\n\"lemma\n\".\nWe\nkeep\nonly\nisabelle-prover/mirror-afp-devel and discard all other older copies of the\nArchive of Formal Proofs. We further remove theorem statements and proofs that have a\ntheorem name in the PISA (Jiang et al., 2021) test set.\n\u2022 Lean : We filter for files with the .lean extension, using the keyword whitelist \"theorem\n\", \"lemma \", \"example \". We remove all dependency files, and in order to avoid known\nbenchmark contamination, we blacklist the ProofNet and MiniF2F repositories. We\nfurther remove theorems or lemmas that share a theorem name with the LeanDojo (Yang\net al., 2023) val or test sets.\n\u2022 MATLAB : We filter for files with the .m extension, using the keyword whitelist \"#import\",\n\"interface\", \"implementation\", \"property\", and blacklist C files via the keywords\n\"#include\" and the regex r\u2019 main\\(.*{$\u2019\nWe implemented a cutoff date for our Github API downloads, and used a cutoff date of April 1, 2023.\nFor all languages, unless otherwise stated, we additionally filtered out files with a filesize greater than\n1048575 bytes or with a numerical density (ratio of digit characters to non-digit characters) of 0.5.\nWe additionally perform document-level exact deduplication by removing documents which contain\nan overlapping 2048-character chunk as another document.\nB.1.2\nLEAN PROOFSTEPS\nWe extract a dataset of (tactic state, next tactic) pairs from Mathlib 4 (mathlib Community, 2020)\nusing the lean-training-data (Morrison, 2023) tool. We use Mathlib 4 commit c779bd5,\nwhich was created on August 20th 2023.\nB.1.3\nISABELLE PROOFSTEPS\nWe construct a dataset of Isabelle proofs, building upon the PISA dataset Jiang et al. (2021). Isabelle\nProofsteps comprises proofs from the Archive of Formal Proofs and Isabelle Standard Library, scraped\nwith PISA Jiang et al. (2021). Each entry in the dataset includes the theorem statement, the proof\nstates and the proof steps, separated by specific tags. To maintain the integrity of evaluations using\nthe PISA test set, we decontaminate Isabelle Proofsteps by removing theorems whose names overlap\nwith those in the PISA test set. Although this approach results in a strict filtering \u2013 removing more\nthan 10,000 theorems although there are only 3600 in the PISA test set \u2013 we consider it acceptable in\norder to mitigate data contamination. After filtering, Isabelle Proofsteps contains 251,000 theorems.\nB.1.4\nSTACK FILTERING\nWe source the following programming languages from the Stack (Kocetkov et al., 2022) dataset,\nand describe our filtering process and quality issues we chose to mitigate beyond our default quality\nheuristics:\n\u2022 Agda: Only standard filters applied.\n\u2022 C : We include documents based on a keyword whitelist, namely: \"#include <fftw.h>\",\n\"#include <fftw3.h>\", \"#include <rfftw.h>\", \"#include <gsl\", \"#include <cblas.h>\",\n\"#include <blas.h>\", \"#include <lapacke.h>\", \"#include <nlopt.h>\", \"#include\n<petsc.h>\".\n\u2022 C++ :\nWe include documents based on a keyword whitelist, namely:\n\"#include\n<adept_arrays.h>\", \"#include <adept.h>\", \"#include <alglib>, \"#include <boost\",\n\"#include <armadillo\", \"#include <blitz\", \"#include <Eigen\", \"#include <deal.II\",\n\"#include <dlib\", \"#include <NTL\", \"#include <mtl\".\n\u2022 Fortran : Only standard filters applied.\n\u2022 GAP : Only standard filters applied.\n\u2022 Haskell :\nWe filtered the data to only contain files with the following im-\nports: Numeric.LinearAlgebra, Numeric.SpecFunctions, Numeric.Vector, Statistics,\nData.Complex.\n18\nPublished as a conference paper at ICLR 2024\n\u2022 Idris : Only standard filters applied.\n\u2022 Julia : We filtered out mislabeled JSON lines files. We removed files larger than 10,000\ncharacters long which both were not files containing tests and which had a lower numerical\ndensity than 0.5, and otherwise ignored numerical density. We additionally only accepted\nfiles within a specific keyword whitelist, to attempt to control relevance to scientific comput-\ning, namely: \"LinearAlgebra\", \"DifferentialEquations\", \"Symbolics\", \"Distributions\",\n\"DataFrames\", \"DynamicalSystems\", \"Turing\", \"Gen\", \"JuMP\", \"sqrt\", \"abs\", \"ze-\nros\", \"ones\", \"sin\", \"cos\", \"tan\", \"log\", \"exp\", \"integrate\", \"likelihood\", \"Matrix\",\n\u03c0, \"pi\", \"rand\", \"grad\".\n\u2022 Jupyter : We found that many Jupyter notebook files were large due to containing long cell\noutputs, such as base64 images, long tracebacks, or other extra JSON cell metadata. We use\nnbconvert to convert notebooks to a markdown format, removing metadata.\n\u2022 Maple : We filtered out files with a size greater than 100, 000 bytes, and found that some\nfiles were XML. We filtered all files beginning with an XML declaration.\n\u2022 Python : We filtered notebooks and JSON files out by excluding documents with beginning\n\"{\" characters, and included only files importing from a fixed list of libraries.\n\u2022 R : We excluded all files beginning with an XML declaration. We additionally filtered out\nall notebooks, and filtered all files containing MacOS \"Resource Fork\" files.\n\u2022 Tex : We used a max file size of 10,000,000 bytes. We excluded tex files found in di-\nrectories named \"latex/\" because these were often auto-generated files, and excluded\ndocuments using gnuplot. We included only documents containing one of the keywords \"\n\\chapter{\", \"\\chapter*{\", \"\\section{\", \"\\section*{\", \"\\subsection{\", \"\\subsection*{\",\n\"\\subsubsection{\", \"\\subsubsection*{\", \"\\paragraph{\", \"\\subparagraph{\", and ad-\nditionally only included documents identified as English by a classifier from the langid\npackage.\nFor all languages we used within the Stack, unless otherwise stated, we additionally filtered out files\nwith a filesize greater than 1048575 bytes or with a numerical density (ratio of digit characters to\nnon-digit characters) of 0.5.\nWe used v1.2 of the near-deduplicated Stack as a base for processing.\nB.2\nPAPERS: ARXIV\nWe use the entirety of ArXiv, as accessed by Computer (2023) in April 2023. For further information\non preprocessing applied to ArXiv, see Computer (2023).\nB.3\nWEB: OPENWEBMATH\nFor the web portion of our training dataset, we use OpenWebMath (Paster et al., 2023).\nC\nEVALUATION HARNESS\nWe implement a variety of math-related tasks and evaluation protocols into a public fork of the\nLanguage Model Evaluation Harness (Gao et al., 2021). The Harness provides a model-agnostic\nframework for standardized, reproducible evaluation of language models.\nWe add the following tasks for the evaluations in this paper:\n\u2022 hendrycks_math_ppl: Perplexity evaluation on MATH (Hendrycks et al., 2021a)\nsub-tasks.\n\u2022 minif2f_isabelle: Proof autoformalization in Isabelle on the miniF2F benchmark\nbased on Jiang et al. (2023), with a Portal-to-Isabelle (Jiang et al., 2021) proof checker.\n\u2022 minerva_math: The MATH benchmark with the prompt and Sympy evaluation from\nMinerva (Lewkowycz et al., 2022).\n\u2022 minerva-hendrycksTest: MMLU-STEM tasks following Lewkowycz et al. (2022).\n19\nPublished as a conference paper at ICLR 2024\n\u2022 ocw_courses: The OCW Courses task from Lewkowycz et al. (2022).\n\u2022 python_gsm8k: GSM8k with Python, based on Gao et al. (2022).\n\u2022 sympy_math: MATH with Sympy evaluation.\nWe include a link to the implementations for these tasks, including full prompts, in our public\ncodebase.\nD\nEVALUATION: EXPERIMENT DETAILS\nD.1\nISABELLE INFORMAL-TO-FORMAL THEOREM PROVING\nWe follow Jiang et al. (2023), allowing the model to issue a call to built-in Isabelle automation in the\noutput proof by generating sledgehammer. This calls Sledgehammer (Paulson & Nipkow, 2023)\nand the list of heuristics listed in Jiang et al. (2023). Following Jiang et al. (2023), as a baseline we use\nSledgehammer and the heuristics executed at the beginning of the proof (referred to as Sledgehammer\nin the main text for brevity). We use a 30-second timeout for Sledgehammer and implement proof\nchecking via Portal-to-Isabelle (Jiang et al., 2021). Refer to the implementation in the Evaluation\nHarness for further details.\nD.2\nLEAN THEOREM PROVING\nTheorem proving via tactic prediction involves interacting with a proof assistant after each step of\na proof. Implementing these interactions within the evaluation harness is outside the scope of this\nwork. Therefore, for the Lean theorem proving task we use a separate evaluation setup based on an\nopen-source implementation (Welleck, 2023). We include our evaluation code in our public codebase.\nSetup.\nWe evaluate on miniF2F (Zheng et al., 2021), which consists of 488 formalized statements\nfrom math competitions and undergraduate coursework. Given a formalized statement, the task is to\ngenerate a formal proof that is checked by Lean.\nWe use best first search, commonly used for neural tactic prediction models (e.g., Polu & Sutskever\n(2020)).\nBest first search is parameterized by the number of attempts (N), generated tactics\nper iteration (S), and maximum iterations (T). We define the search budget to be the maximum\nnumber of generated tactics, N \u00d7 S \u00d7 T.\nWe set our search budget to N = 1, S = 32,\nand T = 100, less than that of the baseline model. Following Yang et al. (2023), we gener-\nate tactics with beam search and use a 10 minute timeout. We adapt the proof search imple-\nmentation from Welleck (2023), which uses LeanDojo v.1.1.2 (Yang et al., 2023) for interac-\ntion. We use Lean 4 miniF2F, using https://github.com/rah4927/lean-dojo-mew commit\nd00c776260c77de7e70125ef0cd119de6c0ff1de. Note that the ReProver baseline from (Yang\net al., 2023) reports performance with Lean 3.\nPrompt. We prompt the model with three (state, tactic) examples, shown in Figure 5.\n20\nPublished as a conference paper at ICLR 2024\n\"\"\"Given the Lean 4 tactic state, suggest a next tactic.\nHere are some examples:\nTactic state:\n---\n\u03b1 : Type u_1\nr : \u03b1 \u2192 \u03b1 \u2192 Prop\ninst1 : DecidableEq \u03b1\ninst : IsIrrefl \u03b1 r\n\u22a2 CutExpand r \u2264 InvImage (Finsupp.Lex (cr \u03a0 fun x x_1 => x \u0338= x_1)\nfun x x_1 => x < x_1) \u2191toFinsupp\n---\nNext tactic:\n---\nrintro s t \u27e8u, a, hr, he\u27e9\n---\nTactic state:\n---\n\u03b9 : Type u_1\nI J : Box \u03b9\nx y : \u03b9 \u2192 R\nI J : WithBot (Box \u03b9)\n\u22a2 \u2191I = \u2191J \u2194 I = J\n---\nNext tactic:\n---\nsimp only [Subset.antisymm_iff, \u2190 le_antisymm_iff,\nwithBotCoe_subset_iff]\n---\nTactic state:\n---\nm n : N\nh : Nat.coprime m n\n\u22a2 Nat.gcd m n = 1\n---\nNext tactic:\n---\nrw [\u2190 h.gcd_eq_one]\n---\nTactic state:\n---\n%s\n---\nNext tactic:\n---\"\"\"\nFigure 5: Prompt for the Lean theorem proving experiments.\n21\nPublished as a conference paper at ICLR 2024\nE\nDATASHEET\nWe provide a datasheet for Proof-Pile-2, following the framework in Gebru et al. (2021).\nMOTIVATION\nFor what purpose was the dataset cre-\nated?\nProof-Pile-2 was created for the training\nor finetuning of domain-specific large lan-\nguage models for general mathematics\ntasks.\nWho created the dataset and on behalf of\nwhich entity?\nThe dataset was created by the authors of\nthis paper for the purposes of this research\nproject.\nWho funded the creation of the dataset?\nThe creation of the dataset was funded by\nthe coauthors\u2019 grants and employers, as fur-\nther described in section 5.\nAny other comment?\nCOMPOSITION\nWhat do the instances that comprise the\ndataset represent?\nInstances are text-only documents.\nHow many instances are there in total?\nWe detail fine-grained token counts else-\nwhere in this paper.\nDoes the dataset contain all possible in-\nstances or is it a sample (not necessarily\nrandom) of instances from a larger set?\nOur dataset is filtered based on our assess-\nments of quality for the language modeling\ntask. More detail on methodology can be\nfound in Appendix B.\nWhat data does each instance consist of?\nEach instance is a text-only document,\nalongside metadata about its originating\nsplit and filename or location.\nIs there a label or target associated with\neach instance?\nNo.\nIs any information missing from individ-\nual instances?\nYes, we filter undesired noise, such as\nbase64-encoded images, from some doc-\numents.\nAre relationships between individual in-\nstances made explicit?\nNo.\nAre there recommended data splits?\nYes, we release a canonical train, validation,\nand test split of the dataset, which we follow\nin this work.\nAre there any errors, sources of noise, or\nredundancies in the dataset?\nWe make our best efforts to remove errors\nor sources of noise, but our dataset will\nnaturally contain documents with errors or\nnoise, and may contain near-duplicate doc-\numents.\nIs the dataset self-contained, or does it\nlink to or otherwise rely on external re-\nsources?\nThe dataset is self-contained, but can also\nbe reconstructed based on external publicly\navailable data sources and datasets follow-\ning our instructions.\nDoes the dataset contain data that might\nbe considered confidential?\nAll documents in Proof-Pile-2 are publicly\navailable online.\n22\nPublished as a conference paper at ICLR 2024\nDoes the dataset contain data that, if\nviewed directly, might be offensive, in-\nsulting, threatening, or might otherwise\ncause anxiety?\nWe estimate toxic content to be less preva-\nlent in our dataset than other more general\nweb-based datasets, due to its technical fo-\ncus. However, it is likely to contain such\ncontent.\nCOLLECTION\nHow was the data associated with each\ninstance acquired?\nData was largely sourced from existing pub-\nlic subsets, such as the RedPajama dataset\n(Computer, 2023), OpenWebMath dataset\n(Paster et al., 2023), and via filtering the\nStack (Kocetkov et al., 2022). Some data\nwas collected using the Github API.\nWhat mechanisms or procedures were\nused to collect the data?\nSee above.\nIf the dataset is a sample from a larger\nset, what was the sampling strategy?\nWe release the entirety of the dataset fol-\nlowing the application of our quality filters.\nWe randomly held out validation and test\nsplits from the dataset.\nWho was involved in the data collec-\ntion process and how were they compen-\nsated?\nThe authors of this paper participated in lo-\ncating, retrieving, and filtering the dataset.\nOver what timeframe was the data col-\nlected?\nThis data was collected in 2023, with a cut-\noff date of April 2023 for all subsets with\nthe exception of our Lean proofstep data.\nWere any ethical review processes con-\nducted?\nYes, the authors conducted an informal eth-\nical review internally.\nPREPROCESSING\nWas any preprocessing/cleaning/labeling\nof the data done?\nYes, the authors extensively filtered the\ndataset subsets in keeping with our expec-\ntations for high-quality language modeling\ndata in our domain. See Appendix B for\nfurther detail on filtering steps taken.\nWas the \u201craw\u201d data saved in addition to\nthe preprocessed/cleaned/labeled data?\nRaw data can be accessed via reuse of our\nprovided codebase.\nIs the software that was used to prepro-\ncess/clean/label the data available?\nYes. We release our codebase, which can\nbe used to reproduce our dataset and its con-\nstruction process, at https://github.\ncom/EleutherAI/math-lm.\nUSES\nHas the dataset been used for any tasks\nalready?\nYes, this dataset has been used to train the\nLLEMMA language models as a domain\nadaptation and continued pretraining cor-\npus.\nIs there a repository that links to any\nor all papers or systems that use the\ndataset?\nNo.\nWhat (other) tasks could the dataset be\nused for?\nThe dataset was specifically targeted as a\nhigh quality language modeling corpus for\nthe mathematics domain, but may be useful\nfor general-purpose language modeling or\nunforeseen other downstream uses.\n23\nPublished as a conference paper at ICLR 2024\nIs there anything about the composition\nof the dataset or the way it was col-\nlected and preprocessed/cleaned/labeled\nthat might impact future uses?\nWe filtered the dataset with the intent of\ncreating a model useful for mathematical\ntasks with solely English text.\nAre there tasks for which the dataset\nshould not be used?\nThe dataset should not be used with the\nintent to cause harm or for models intended\nfor the purposes of harm.\nDISTRIBUTION\nWill the dataset be distributed to third\nparties outside of the entity on behalf of\nwhich the dataset was created?\nWe make the dataset publicly available for\nreproducibility, analysis, and other further\ndownstream uses.\nHow will the dataset will be distributed?\nWe provide code to replicate the dataset,\nand release it via the Huggingface Hub.\nWhen will the dataset be distributed?\nThe dataset is available immediately.\nWill the dataset be distributed under\na copyright or other intellectual prop-\nerty (IP) license, and/or under applicable\nterms of use (ToU)?\nWe do not relicense the dataset\u2019s compo-\nnents, and do not impose our own use re-\nstrictions.\nHave any third parties imposed IP-based\nor other restrictions on the data associ-\nated with the instances?\nNot to our knowledge.\nDo any export controls or other regula-\ntory restrictions apply to the dataset or\nto individual instances?\nNot to our knowledge.\nMAINTENANCE\nWho will be supporting/hosting/main-\ntaining the dataset?\nThe dataset will be hosted on the Hug-\ngingFace Hub and able to be recreated\nvia code at https://github.com/\nEleutherAI/math-lm.\nThe dataset\nwill not be updated post-release.\nHow can the owner/curator/manager of\nthe dataset be contacted?\nVia email at za2514@princeton.edu\nIs there an erratum?\nNo.\nWill the dataset be updated?\nNo.\nIf others want to extend/augment/build\non/contribute to the dataset, is there a\nmechanism for them to do so?\nNo.\nTable 10: Datasheet for Proof-Pile-2, following the framework introduced by Gebru et al. (2021).\n24\nPublished as a conference paper at ICLR 2024\nF\nADDITIONAL RESULTS\nF.1\nPROOF AUTOFORMALIZATION\nTable 11 shows additional results on Isabelle proof autoformalization, including the union of theorems\nclosed by Sledgehammer and the given language model.\nMethod\nAutoformalization pass@1\nminiF2F-valid\u2217\nminiF2F-test\nSledgehammer\n14.72%\n20.49%\nCode Llama 7b\n16.31%\n17.62%\nLLEMMA-7b\n20.60%\n22.13%\nCode Llama 7b \u222a Sledgehammer\n20.17%\n25.00%\nLLEMMA-7b \u222a Sledgehammer\n25.97%\n27.46%\nTable 11: Isabelle autoformalization. \u2217We exclude the 11 examples used in the few-shot prompts.\nPass@1 with greedy decoding.\nG\nSUPERVISED FINETUNING\nA full exploration of finetuning applications for LLEMMA, such as instruction following (Ouyang\net al., 2022; Wei et al., 2022), dialogue modeling (Thoppilan et al., 2022; Touvron et al., 2023; Collins\net al., 2023), and reward modeling (Cobbe et al., 2021; Lightman et al., 2023) are outside the scope\nof this work. However, to establish that LLEMMA retains its advantage over other open models when\nfinetuned, we conduct preliminary experiments finetuning LLEMMA-7B on MetaMathQA (Yu et al.,\n2023), a supervised dataset targeted at the MATH and GSM8k benchmarks. Results are shown in\nTable 12.\nInitialization\nFinetune Dataset\nMATH\nGSM8k\nLlama 2 7B\nWizardMath (Proprietary)\n10.7%\n54.9%\nLlama 2 7B\nMetaMathQA\n19.4%\n66.4%\nLLEMMA 7B\nMetaMathQA\n25.2%\n66.5%\nLlama 2 70B\nWizardMath (Proprietary)\n22.7%\n81.6%\nLlama 2 70B\nMetaMathQA\n26.6%\n82.3%\nTable 12: Finetuning of various 7B base models on supervised mathematics datasets. All results\nwith a Llama 2 initialization are copied from the literature (Luo et al., 2023; Yu et al., 2023). The\nLLEMMA 7B finetune is trained with identical hyperparameters to the models in Yu et al. (2023)\n.\nH\nQUALITATIVE EXAMPLES\nDataset overlap.\nFigure 6 shows example false positives when checking n-gram overlap with\nOpenWebMath documents for various n. Figure 7 shows an example OpenWebMath document that\nhas 30-gram overlap with a MATH problem, and LLEMMA-7b\u2019s generated solution.\nTask outputs.\nFigure 8 shows a generated proof in the informal2formal theorem proving task.\n25\nPublished as a conference paper at ICLR 2024\nOpenWebMath document\n2D affine transformations can be better represented using 2 by 2 matrices, since they\nare simply linear combinations of 2 variables. The advantage of this is that the matrices\nare associative under multiplication Also, GPUs and modern toolkits are optimised to work\nwith this representation. As a result, a scale matrix is \\begin{bmatrix} s_x & 0 \\\\ 0 &\ns_y \\end{bmatrix}, and a rotation matrix is \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta \\end{bmatrix}.\nA translation matrix is simply \\begin{bmatrix} 1 & \\frac{t_x}{y} \\\\ \\frac{t_y}{x} & 1 ...\nMATH problem\nA rotation centered at the origin takes\n\u0012\n13\n0\n\u0013\nto\n\u0012\n5\n\u221212\n\u0013\n. Which vector does the rotation take\n\u0012\n0\n1\n\u0013\nto?\nMATH solution\nThe rotation matrix must be of the form\n\u0012\ncos \u03b8\n\u2212 sin \u03b8\nsin \u03b8\ncos \u03b8\n\u0013\n. Thus,...\nHit\n\\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos\nOpenWebMath document\n# Basic Probability\nA number is selected at random from 1 through 100, inclusive. What is the probability\nthat the number is a divisor of 50? Express your answer as a common fraction.\nApr 24, 2019\nThere are a 100 integers between 1-100, inclusive. Since 50 is $$2*5^2$$, it has\n$$(1+1)(1+2)=(2)(3)=6$$ factors. Thus, the answer is\n$$\\frac{6}{100}=\\boxed{\\frac{3}{50}}.$$\nMATH problem\nA number is selected at random from 1 through 100, inclusive. What is the probability that the number\nis a perfect square?\nHit\nA number is selected at random from 1 through 100, inclusive. What is the probability that the number\nis a\nOpenWebMath document\nFig. 2.\nUse values of the most used medicinal plants in the Safi Province (Morocco).\nIt is also important to note that for the abovementioned medicinal plants, many other\nfolk uses have been reported in different regions of Morocco. Furthermore,\nliterature-based proof revealed that these species have proven a wide variety of\nbiological and pharmacological activities (Table 4, Ref. [14, 17, 19, 20, 21, 23,\n24, 26, 28, 30, 31, 34, 35, 36, 38, 39, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75,\n76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96,\n97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n114, 115, 116]), which may confirm the different popular applications of ...\nGenerated solution (LLEMMA 7b):\nThe\nfirst\n100\npositive\nintegers\nare\n1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,\n21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,\n50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,\n79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100. We can see that...\nFigure 6: Data overlap: Example false positives using 10-gram match between MATH solutions and Open-\nWebMath documents (top), 20-gram match between MATH problems and OpenWebMath documents (middle),\nand 30-gram match between LLEMMA-7b\u2019s generated solutions and OpenWebMath documents (bottom).\n26\nPublished as a conference paper at ICLR 2024\nOpenWebMath document\nA triangle is formed with edges along the line $y=\\frac{2}{3}x+5$,\nthe $x$-axis, and the line $x=k$. If the area of the triangle is less than $20$,\nfind the sum of all possible integral values of $k$.\nFeb 28, 2018\nLook at the graph, here...two triangles are possible :\nhttps://www.desmos.com/calculator/m6wnjpgldq\nThe\nheight of the triangles at any point will be formed by\n[ (2/3)x + 5 ]\nAnd the bases will be\n[ x - (- 7.5)]\n= [ x + 7.5]\nSo....we want to solve this\n(1/2) [ (2/3)x + 5 ] [ x + 7.5 ]\n=\n20\n[ (2/3)x + 5 ] [ x + 7.5]\n=\n40\n(2/3)x^2 + 5x + 5x\n+ 37.5\n= 0\n(2/3)x^2 + 10x - 2.5\n=\n0\nUsing a little technology.....the max x\nvalue for the\ntriangle formed above the x axis will be = .246\nAnd the min x value for the triangle formed below the x axis will be = -15.246\nWith the given boundaries, the\ninteger sums\nof all possible x values of k giving\ntriangles with an area < 20 units^2\n=\n[ (-15) + (-14 ) + (-13) + ...+ ( -2) + ( - 1 ) + 0 ]\n=\n-\n(15) (16) / 2\n=\n-120\nFeb 28, 2018\nMATH Problem: A triangle is formed with edges along the line y = 2\n3x + 5, the x-axis, and the line\nx = k. If the area of the triangle is less than 20, find the sum of all possible integral values of k.\nAnswer: -120\nGenerated solution (LLEMMA 7b):\nThe triangle has vertices (k, 0), (0, 5), and (k, 2\n3k + 5). The base of the triangle has length k, and the\nheight of the triangle has length 2\n3k + 5. The area of the triangle is therefore\nA = 1\n2bh = 1\n2k(2\n3k + 5) = 1\n3k2 + 5\n2k.\nSince the area of the triangle is less than 20, we have\n1\n3k2 + 5\n2k < 20 \u21d2 1\n6k2 + 5\n4k \u2212 10 < 0.\nThis inequality is true if and only if 1\n6k2 + 5\n4k \u2212 10 is positive. Since 1\n6k2 + 5\n4k \u2212 10 is a quadratic\nwith positive leading coefficient, it is positive if and only if its vertex is less than 0. The vertex of\n1\n6k2 + 5\n4k \u2212 10 is (\u2212 10\n3 , \u2212 125\n18 ). Since \u2212 10\n3 < 0, we have that 1\n6k2 + 5\n4k \u2212 10 > 0 if and only if\nk < \u2212 10\n3 . Therefore, the only possible integral values of k are k = 0, 1, 2, 3, 4. The sum of these\nvalues is 10 . Final Answer: The final answer is 10.\nFigure 7: Data overlap: Example OpenWebMath document that has a 30-gram overlap with the given MATH\nproblem, and LLEMMA-7b\u2019s generated solution.\n27\nPublished as a conference paper at ICLR 2024\nProblem: If 3a + b + c = \u22123, a + 3b + c = 9, a + b + 3c = 19, then find abc. Show that it is -56.\nInformal Proof (Human-written): Summing all three equations yields that 5a + 5b + 5c = \u22123 + 9 +\n19 = 25. Thus, a + b + c = 5. Subtracting this from each of the given equations, we obtain that 2a =\n\u22128, 2b = 4, 2c = 14. Thus, a = \u22124, b = 2, c = 7, and their product is abc = \u22124 \u00d7 2 \u00d7 7 = \u221256.\nFormal Statement and Proof:\ntheorem mathd_algebra_338:\nfixes a b c :: real\nassumes \"3 * a + b + c = -3\" and \"a + 3 * b + c = 9\" and \"a + b + 3 * c = 19\"\nshows \"a * b * c = -56\"\nproof -\n(* Summing all three equations yields that 5a + 5b + 5c = -3 + 9 + 19 = 25.\nThus, a + b + c = 5. *)\nhave \"5 * a + 5 * b + 5 * c = -3 + 9 + 19\" using assms <ATP>\nthen have \"5 * (a + b + c) = 25\" <ATP>\nthen have \"a + b + c = 5\" <ATP>\n(* Subtracting this from each of the given equations, we obtain that\n2a =\n-8, 2b = 4, 2c = 14. Thus, a = -4, b = 2, c =7, and their product is\nabc =\n-4 \\times 2 \\times 7 = -56. *)\nthen have \"2 * a = -8\" \"2 * b = 4\" \"2 * c = 14\" using assms <ATP>\nthen have \"a = -4\" \"b = 2\" \"c = 7\" <ATP>\nthen show ?thesis <ATP>\nqed\nFigure 8: Informal-to-formal proving. The model is given the problem, informal proof, and\nformal statement, following Jiang et al. (2023). It generates a formal proof (starting with proof -)\ncontaining Isabelle code, comments ((*...*)) that align the informal and formal proofs, and calls\nto an automated prover (shown as <ATP>). The proof is from LLEMMA-7b with greedy decoding.\n28\n"
  },
  {
    "title": "In-Context Pretraining: Language Modeling Beyond Document Boundaries",
    "link": "https://arxiv.org/pdf/2310.10638.pdf",
    "upvote": "26",
    "text": "Published as a conference paper at ICLR 2024\nIN-CONTEXT PRETRAINING: LANGUAGE MODELING\nBEYOND DOCUMENT BOUNDARIES\nWeijia Shi1,2\nSewon Min1,2\nMaria Lomeli1\nChunting Zhou1\nMargaret Li1,2\nGergely Szilvasy1\nRich James1\nXi Victoria Lin1\nNoah A. Smith2,3\nLuke Zettlemoyer1,2\nScott Yih1\nMike Lewis1\n1Meta AI\n2University of Washington\n3 Allen Institute for AI\nswj0419@cs.washington.edu\nABSTRACT\nLarge language models (LMs) are currently trained to predict tokens given doc-\nument prefixes, enabling them to directly perform long-form generation and\nprompting-style tasks which can be reduced to document completion. Existing\npretraining pipelines train LMs by concatenating random sets of short documents\nto create input contexts but the prior documents provide no signal for predicting the\nnext document. We instead present IN-CONTEXT PRETRAINING, a new approach\nwhere language models are pretrained on a sequence of related documents, thereby\nexplicitly encouraging them to read and reason across document boundaries. We\ncan do IN-CONTEXT PRETRAINING by simply changing the document ordering\nso that each context contains related documents, and directly applying existing\npretraining pipelines. However, this document sorting problem is challenging.\nThere are billions of documents and we would like the sort to maximize contextual\nsimilarity for every document without repeating any data. To do this, we intro-\nduce approximate algorithms for finding related documents with efficient nearest\nneighbor search and constructing coherent input contexts with a graph traversal\nalgorithm. Our experiments show IN-CONTEXT PRETRAINING offers a simple\nand scalable approach to significantly enhance LMs\u2019 performance: we see notable\nimprovements in tasks that require more complex contextual reasoning, including\nin-context learning (+8%), reading comprehension (+15%), faithfulness to previous\ncontexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%).\n1\nINTRODUCTION\nLarge language models (LMs) are trained to complete documents; each token is predicted given\nthe context provided by the prefix of the document it appears in. Such contexts can be widely\nvaried, especially at pretraining scale, allowing models to excel on diverse tasks such as instruction-\nfollowing (Ouyang et al., 2022), conversational interfaces (OpenAI, 2023), reading comprehen-\nsion (Zhang et al., 2020), and in-context learning (Brown et al., 2020). However, recent studies\nhighlight that LMs sometimes struggle to understand more complex contexts: they can fail to follow\ninstructions accurately (McKenzie et al., 2023; Efrat & Levy, 2020; Liu & Liu, 2023), struggle with\nreasoning over conditioned documents (Liu et al., 2023; Shi et al., 2023a), and exhibit high variance\nin in-context learning (Zhao et al., 2021). In this paper, we present IN-CONTEXT PRETRAINING, a\nnew pretraining method that learns to predict tokens conditioned on a sequence of related documents,\nexplicitly enabling the model to read and reason about much more varied and longer contexts that go\nbeyond document boundaries.\nCurrent LM training pipelines concatenate random sets of shorter documents to create longer con-\ntext windows. However, the prior documents provide no signal for predicting the next document,\nincurring unnecessary computational overhead for tokens that do not require communication between\nthem (de Vries, 2023). IN-CONTEXT PRETRAINING instead reorders the pretraining data by combin-\ning several semantically related documents to create a coherent input context, thereby exposing LMs\nto long relevant contexts and providing pretraining signals beyond document boundaries. We illustrate\nthis via an example in Figure 1: when predicting the following tokens for the phrase \u201cFor 2022,\nFIFA set the prize money at $42m,\u201d a previous document stating that the \u201cWorld Cup never awarded\n1\narXiv:2310.10638v5  [cs.CL]  9 Mar 2024\nPublished as a conference paper at ICLR 2024\nFigure 1: Overview of IN-CONTEXT PRETRAINING. Different from the standard pretraining\nstrategy that place randomly shuffled documents in the input context, IN-CONTEXT PRETRAINING\nplaces related documents in the same context, making models learn to reason across prior documents.\nFor example, when predicting the following tokens for the phrase \u201cFor 2022, FIFA set the prize\nmoney at $42m,\u201d LMs could reference prior documents stating \u201cWorld Cup never awarded more than\n$10M before 2022\u201d and learn to infer that \u201cthe highest so far.\u201d\nmore than $10M before 2022\u201d could be in the context, enabling the prediction of a continuation like\n\u201cthe highest so far.\u201d As IN-CONTEXT PRETRAINING only changes document ordering and leaves\nall other aspects of LM pretraining untouched, it can be easily integrated into existing pretraining\npipelines for large-scale LMs.\nHowever, this document sorting problem is challenging. LMs are typically trained on billions of\ndocuments and we would like to sort them to maximize document similarity in the input context\nwindows without repeating any data. We introduce two new approximate algorithms to tackle these\nchallenges. We use a retrieval model paired with an efficient search index to build a document graph\nthat pairs each document with its nearest-neighbors based on its semantic similarity in the embeddings\nspace. We also formulate document sorting as a travelling salesman problem, for which we develop\nan effective algorithm that maximizes similarity of documents to their context while also ensures that\neach document is included only once.\nTo evaluate the effectiveness of IN-CONTEXT PRETRAINING, we pretrain language models from 0.3\nto 7 billion parameters on 300 billion tokens from the CommonCrawl dataset (Wenzek et al., 2020).\nAcross all model scales, IN-CONTEXT PRETRAINING LMs (ICLM) demonstrate strong language\nmodeling and downstream task performance, outperforming LMs pretrained using the standard\napproach on the same corpus. We observe various improvements resulting from IN-CONTEXT\nPRETRAINING compared with existing LMs: (1) in-context learning with an average increase of\n8% across 8 datasets; (2) reading comprehension, with an average of 15% improvement on 8\nreading comprehension tasks; (3) outputs that are more faithful to prior contexts (+16%); (4)\nlong context reasoning, showing a 5% boost; and (5) retrieval augmentation, leading to 9% gains\nwhen augmenting with external knowledge such as documents retrieved from Wikipedia. Our results\ndemonstrate that, by simply altering order of the pretraining documents, IN-CONTEXT PRETRAINING\noffers a scalable and simple approach to significantly enhance understanding and reasoning over their\nfull contexts. Code are publicly released at github.com/swj0419/in-context-pretraining.\n2\nIN-CONTEXT PRETRAINING\nThe standard practice in pretraining is to form input contexts by concatenating random documents\nuntil reaching the maximum context length. It then trains the LM using a language modeling\nobjective on the input contexts. However, training LMs on randomly concatenated documents does\nnot offer additional learning signals compared with training on each document individually. In\ncontrast, IN-CONTEXT PRETRAINING generates more coherent input contexts by concatenating\nsemantically related documents together during pretraining. As depicted in Figure 2, IN-CONTEXT\nPRETRAINING consists of two steps: it first finds related documents at scale (\u00a72.1) and then constructs\ninput contexts using these related documents (\u00a72.2). Successively, we use the contexts formed with\n2\nPublished as a conference paper at ICLR 2024\nsemantically related documents to pretrain LMs with a language modeling objective. Since IN-\nCONTEXT PRETRAINING is identical to existing pretraining recipes for LMs, except for changing\nhow input contexts are built, it can be easily integrated into existing pretraining pipelines for large-\nscale LMs.\n2.1\nFINDING RELATED DOCUMENTS AT SCALE: RETRIEVING NEIGHBOR DOCUMENTS\nTo find related documents at scale, we link documents within the pretraining corpus D using a\nretrieval model. Specifically, for each document di \u2208 D, a dense retrieval model is used to retrieve the\ntop-k most similar documents, represented as N(di). The retrieval model uses approximate nearest\nneighbours search for efficient pairwise similarity comparison between any two documents, making\nit scalable for finding related documents in web-scale pretraining corpora.\nRetrieval.\nOur retrieval process employs the contriever model (Izacard et al., 2022). This model\nmaps each document di \u2208 D to an embedding E(di) by taking the mean pooling of the last hidden\nrepresentation over the tokens in di. The cosine similarity is then used to determine the similarity\nbetween any two documents:\ns(di, dj) = cos(E(di), E(dj))\n(1)\nThe retrieval model uses approximate nearest neighbour search, product quantization (J\u00e9gou et al.,\n2011) and an inverted file FAISS index (Johnson et al., 2019; Douze et al., 2024) to conduct efficient\npairwise similarity search. Further details can be found in Appendix A.2.\nDuring the retrieval process, when computing pairwise similarity among each document in the\npretraining corpus, we found that the pretraining corpus contains many near duplicate documents.\nHence, we further leverage the retrieval scores to eliminate near duplicate documents from the\npretraining corpus. More details can be found in Appendix A.1. In \u00a74.2, we show that this\ndeduplication step is crucial for achieving good performance of language models.\n2.2\nCREATING INPUT CONTEXTS: DOCUMENT GRAPH TRAVERSAL\nGiven a set of documents D = {di} and nearest neighbours for each document N(di), our goal is to\nsort the documents to create input contexts such that each of them consists a list of related documents.\nFormally, we aim to form a set of input contexts C1 \u00b7 \u00b7 \u00b7 Cm where each context Ci = {d1, ...dk} \u2282 D\nand\nmS\ni=1\nCi = D. Ideally, documents in Ci are nearest neighbors of each others.\nAlgorithm 1 Maximum Traveling Salesman\nInput: Document graph G = (D, L)\nN(di) returns nearest neighbors for di\nmin_deg(D) returns a min-degree doc\nOutput: A path P\n1: P \u2190 []\n2: while |D| > 0 do\n3:\ndi \u2190 min_deg(D)\n4:\nP.append(di)\n5:\nD.remove(di)\n6:\nwhile N(di) \u2229 D \u0338= \u2205 do\n7:\ndj \u2190 arg mind\u2208N(di)\u2229D sim(di, d)\n8:\ndi \u2190 dj\n9:\nP.append(di)\n10:\nD.remove(di)\n11:\nend while\n12: end while\n13: return P\nA straightforward approach to form C1 \u00b7 \u00b7 \u00b7 Cm is to\ndirectly place each document and its retrieved top-\nk documents together in the same input context\n(referred to as kNN), which has been used in some\nretrieval-augmented pretraining methods (Guu\net al., 2020; Levine et al., 2022).\nThis kNN\napproach maintains document similarity within\neach context but creates the data repeating prob-\nlem: some documents frequently appear as nearest\nneighbors of other documents, causing that differ-\nent input contexts contain overlapping documents,\ni.e., \u2203i \u0338= j, Ci\nT Cj \u0338= \u2205. The data repeating\nproblem exposes LMs to a less diverse set of doc-\numents given a fixed computational budget and\ncould lead to overfitting of popular documents. In-\nstead, we aim to build a set of contexts in a way\nthat each document is included only once, which\ncan be cast as a graph traversal problem.\nDocument graph traversal.\nTo achieve our goal of maximizing the chance that the related doc-\numents are concatenated together, an intuitive approach is to find a single path that visits each\ndocument once and maximize the chance that related documents are visited sequentially. Then we\n3\nPublished as a conference paper at ICLR 2024\nFigure 2: Illustration of IN-CONTEXT PRETRAINING. IN-CONTEXT PRETRAINING first finds\nrelated documents at scale to create a document graph (\u00a72.1) and then builds pretraining input contexts\nby traversing the document graph (\u00a72.2). Along the path, documents are concatenated into a sequence\nand subsequently divided to form fixed-sized input contexts (e.g., 8192 token length).\nsubsequently segment the path into multiple input contexts. We formulate it as the maximum traveling\nsalesman problem (Flood, 1956) that aims to find the maximum weight path that traverses all nodes\nexactly once. We represent each document as a node in the graph and use document similarity as a\nedge weight. We design an undirected weighted graph representing the documents, symbolized as\nG = (D, L). Here, D represents the set of documents, while (d, d\u2217) \u2208 L is a edge if d\u2217 \u2208 N(di) or\ndi \u2208 N(d\u2217). The weight of each edge corresponds to the document similarity (Equation 1).\nSolving large traveling salesman problems exactly is NP hard, but greedy algorithms are known to\nprovide an efficient approximate solution. We adopt this approach, introducing modifications to better\nsuit our context. Algorithm 1 shows the method to construct a maximum weight path. We show a\npath identified by our algorithm in Figure 2. Our algorithm starts by selecting a yet-to-be-visited\ndocument with the minimum degree as the starting node (Doc 0). The algorithm then progressively\nextends the current path by navigating to its unvisited neighboring document with highest weight\n(Doc 9), adding the document node to the path. This process continues until the path reaches a\nnode where all neighboring documents have been visited, which happens because our graph is not\ncomplete, and only contains edges between documents where one is within the other\u2019s k nearest\nneighbors. In this case, we extend the graph with an edge of weight 0 to a random unvisited minimum\ndegree document (Doc 1), and continue the above process. The motivation for starting at minimum\ndegree documents is that they are most likely to have all their neighbors visited first, and therefore be\nconnected to dissimilar documents in the final path.\nAs a final step, we traverse the documents along the path and concatenate them to create fixed-sized\ninput contexts suitable for pretraining. It is important to note that when forming the input training\nbatches, we ensure the diversity among different input contexts within the same batch.\n3\nEXPERIMENTS\nIn this section, we describe details of our pretraining setup (\u00a73.1), the baseline methods we use for\ncomparison (\u00a73.2), and experimental results (\u00a73.3).\n3.1\nPRETRAINING SETUP\nSince IN-CONTEXT PRETRAINING leaves other details of model training unchanged, and only\nchanges the document ordering so that each context contains related documents, we can directly\nintegrate it into pretraining pipelines as a preprocessing step during batching. For our experiment,\nwe adopt the model architecture and pretraining objective of LLaMA (Touvron et al., 2023a;b) and\npretrain LMs from scratch.\nPretraining Datasets.\nWe use the English Commoncrawl dataset (Wenzek et al., 2020), the widely-\nused data source for pretraining LMs. Due to resource constraints, we randomly sample 235 million\ndocuments from this dataset, amounting to 306 billion tokens in total. We use the same pretraining\ndata for all models.\n4\nPublished as a conference paper at ICLR 2024\nFigure 3: Language modeling perplexity (the lower the better) on Wikipedia, Arxiv, and Books\n(\u00a73.3.1). ICLM outperforms the baselines consistently across all model sizes.\nModel Details.\nWe take the model architecture from LLaMA (Touvron et al., 2023a) and train\nmodels across various sizes: 0.3, 0.7, 1.5, and 7.0 billion parameters, all with an 8192-length context\nwindow. Following LLaMA, we employ the AdamW optimizer (Loshchilov & Hutter, 2018) with\nparameters \u03b21 = 0.9 and \u03b22 = 0.95, and a cosine learning rate schedule. The 7B model is pretrained\nusing 128 A100 GPUs across 16 nodes with a batch size of 4 million tokens. It takes 9 days to train\nthe 7B model on our pretraining dataset. Due to the long context window of our models, we use flash\nattention (Dao et al., 2022) to reduce memory consumption during pretraining.\nTo perform the retrieval over our pretraining datasets, we employ the contriever model (Izacard et al.,\n2022) and encode the first 512 token of each document into an embedding. We then construct FAISS\nbig batch search that is designed for conducting efficient similarity search with big batches of vectors\n(typically 50M\u2013100M vectors per batch). Given each query document, we retrieve top 10 documents\n(k=10). We split the data in batches of 50M embeddings, the search step is conducted in each batch\nbefore merging the results using 8 GPUs per batch. The total search time is 6 hours over 32 GPUs\nwith average search time per batch is 4,738s. The document graph traversal phase requires 12 hours\non a setup of 20 CPUs.\nMore details are provided in the Appendix A.2.\n3.2\nBASELINES\nWe compare IN-CONTEXT PRETRAINING with the following baselines: (1) Standard is the prior\nstandard in pretraining that places randomly shuffled documents in the input contexts. This method is\ncommonly adopted by existing models (Zhang et al., 2022; Scao et al., 2022; Touvron et al., 2023a).\n(2) kNN (also referred to as retrieval-augmented language model pretraining (Guu et al., 2020; Levine\net al., 2022)) directly places each document and its retrieved top-k documents together in the same\ninput context. Given the same number of training steps, kNN exposes LMs to a less diverse set of\ndocuments, since documents can repeat. For fair comparison, both standard and kNN methods are\ntrained using the same pretraining data as IN-CONTEXT PRETRAINING and undergo an identical\nnumber of training steps, ensuring the same computation cost.\n3.3\nRESULTS\nWe perform evaluations on tasks that require understanding of contexts including language modeling\n(\u00a7 3.3.1), in-context learning (\u00a7 3.3.2), reading comprehension (\u00a7 3.3.3) and open-book question\nanswering (\u00a7 3.3.4), factuality (\u00a7 3.3.5) and long context reasoning (\u00a7 3.3.6).\n3.3.1\nLANGUAGE MODELING\nDatasets & Metrics.\nWe evaluate the language modeling perplexity of IN-CONTEXT PRETRAIN-\nING and baselines on the Wikipedia, Arxiv, and Books corpora. We follow the standard language\nmodeling evaluation in concatenating randomly-ordered documents when computing perplexity.\nResults.\nFigure 3 shows average perplexity across different model sizes. First, kNN does not\nimprove over the standard LM, likely due to the overfitting problem as discussed in \u00a72.2. ICLM, in\n5\nPublished as a conference paper at ICLR 2024\nTable 1: In-context learning performance on seven classification datasets (\u00a73.3.2). We use 32 in-\ncontext examples for all datasets. ICLM outperforms baselines on all datasets.\nMethod\nSentiment\nHate Speech\nTopic Classification\nAverage\nAmazon\nSST2\nYelp\nHate\nOffensive\nAgnews\nDbpedia\nStandard\n94.6\n83.7\n74.3\n52.7\n55.7\n68.3\n61.5\n66.0\nkNN\n88.0\n80.2\n65.1\n50.1\n53.1\n65.7\n56.4\n61.8\nICLM\n96.5\n93.2\n77.4\n60.6\n57.3\n76.0\n63.2\n71.3\nTable 2: Reading comprehension results, using 2-shot in-context learning (\u00a73.3.3). ICLM outper-\nforms baselines on all six datasets.\nMethod\nRACE-High\nRACE-Middle\nBoolQ\nSQuAD\nHotpotQA\nDROP\nAverage\nStandard\n39.5\n53.3\n68.9\n26.3\n10.5\n27.2\n37.6\nkNN\n36.2\n51.4\n65.3\n23.5\n14.4\n25.1\n36.0\nICLM\n41.5\n56.9\n73.0\n30.3\n21.9\n35.7\n43.2\ncontrast, outperforms both the standard LM and kNN on all three datasets, even when the evaluation\ndocuments are not sorted. The gains are consistent or larger as the size of the model scales. These\nimprovements suggest that IN-CONTEXT PRETRAINING provides better pretraining signals, enabling\nLMs to better hone their language modeling abilities.\n3.3.2\nIN-CONTEXT LEARNING FOR TEXT CLASSIFICATION\nDatasets & Metrics.\nIn-context learning requires to perform a task without fine-tuning by condi-\ntioning on a few demonstration examples about the task. We evaluate the in-context learnig ability\nof ICLM using 32 demonstration examples. We use seven text classification datasets, including\nsentiment analysis (SST-2 (Socher et al., 2013), Amazon and Yelp (Zhang et al., 2015a)), topic\nclassificaiton (AGN (Zhang et al., 2015b) and Dbepdia (Lehmann et al., 2015)) and hate speech\ndetection (Barbieri et al., 2020). We use label words from Min et al. (2022) and report accuracy as\nthe metric.\nResults.\nAs shown in Table 1, ICLM consistently demonstrates better performance across all text\nclassification datasets, leading to 8% gain on average. This result suggests that ICLM is better at\nlearning from demonstration examples. We later analyze the relationship between the number of\ndemonstration examples and the performance of the in-context learning in \u00a74.3.\n3.3.3\nREADING COMPREHENSION\nDatasets & Metrics.\nReading comprehension requires to answer the question based on the given\nparagraph. We consider the RACE reading comprehension benchmark (RACE-High and RACE-\nMiddle) (Lai et al., 2017), SQuAD (Rajpurkar et al., 2016), BoolQ (Clark et al., 2019), DROP (Dua\net al., 2019), and HotpotQA (Yang et al., 2018). We use 2-shot in-context learning for evaluation;\nwe did not use more because some documents in reading comprehension tasks are very long. We\nreport the exact match score for HotpotQA and SQuAD, and accuracy for other datasets that are\nmulti-choice tasks (RACE, BoolQ, DROP), following the standard in prior work.\nResults.\nTable 2 highlights that ICLM consistently surpasses both the standard and kNN baselines\nacross all datasets with an average improvement of 14%. In particular, we observe significant gains on\nHotpotQA, which requires multi-hop understanding of multiple related documents. The performance\ngain on reading comprehension tasks demonstrates that IN-CONTEXT PRETRAINING improves LMs\u2019\nability of undestanding and reasoning over the given context.\n3.3.4\nRETRIEVAL-AUGMENTATION\nDatasets & Metrics.\nRetrieval-augmentation is a method to retrieve a set of passages from the\nexternal text corpus (e.g., Wikipedia) and prepend it to the input query in order to better handle input\n6\nPublished as a conference paper at ICLR 2024\nTable 3: Results on NQ and TQA (\u00a73.3.4) with-\nout retrieval (closed) and with retrieval (open).\nMethod\nNQ\nTQA\nClosed\nOpen\nClosed\nOpen\nStandard\n17.0\n28.5\n49.3\n48.1\nkNN\n13.5\n20.1\n40.2\n43.2\nICLM\n17.0\n32.2\n48.0\n51.6\nTable 4: Results on two datasets with knowl-\nedge conflicts, requiring better reasoning of\nthe given context (\u00a73.3.5).\nMethod\nNQ-Swap\nMemoTrap\nStandard\n39.6\n48.4\nkNN\n42.1\n54.3\nICLM\n45.8\n56.2\nqueries that require factual knowledge (Lin et al., 2023; Xu et al., 2023; Su et al., 2023). We conduct\nevaluation on two well-studied open-domain QA datasets: Natural Questions (NQ) (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017). For both datasets, we report exact match scores (EM)\nand evaluate the model performance in both closed-book and open-book settings. In the closed-book\nsetting, we only provide the question to the model and the model has to answer the question based on\nits parametric knowledge. In the open-book setting, we follow Shi et al. (2023c) in providing the\nmodel with the top-10 retrieved documents from Wikipedia as additional context to the question.\nResults.\nResults are reported in Table 3. In the closed-book setting, ICLM performs comparably or\nslightly worse than the standard baseline, likely because our model memorizes less. Nonetheless,\nin the open-book setting, ICLM significantly outperforms the standard baseline in the open-book\nsetting (+9%), obtaining much better performance than the closed book setting. It is also worth noting\nthat the training objective of kNN is exactly the same as the retrieval-augmentation, but ICLM still\nachieves better performance, likely due to the overfitting problem of kNN as discussed in \u00a72.2.\n3.3.5\nFACTUALITY\nDatasets & Metrics.\nPrior work has found that language models generate text that is not factual\nnor faithful to the given context, especially when the context contradicts to knowledge the model has\nacquired during pretraining (often called parametric knowledge (Longpre et al., 2021; Zhou et al.,\n2023; Shi et al., 2023b; Wang et al., 2023a)). We evaluate LMs\u2019 ablities to follow instructions and\ncontexts on two knowledge conflict datasets: NQ-Swap (Longpre et al., 2021) and MemoTrap (Liu\n& Liu, 2023). Both datasets contain instruction and contexts that are in conflict with the models\u2019\nparametric knowledge. We report exact match score as the metric.\nResults.\nTable 4 shows that ICLM is better than the standard and kNN baselines on both datasets,\nimplying that IN-CONTEXT PRETRAINING improves LMs\u2019 ability to generate outputs that are\nfaithful to prior contexts. Gains are larger than those in other datasets, likely because NQ-Swap and\nMemoTrap highlight the challenge in reasoning about the given context, which the previous LMs\nstruggle with.\n3.3.6\nLONG CONTEXT REASONING\nDatasets & Metrics.\nTo evaluate the ability of long context reasoning, we compare ICLM with\nthe standard and kNN baselines on the SCROLL benchmark (Shaham et al., 2022) that evaluates\nLMs\u2019 ability to synthesize information over long texts. Following the original paper setting, we\nfinetune the pretrained LMs (standard, kNN, IN-CONTEXT PRETRAINING) on the training datasets\nof the scroll and evaluate them on the test datasets. We report F1 score for Narrative QA, Qasper\nand ContractNLI datasets and report ROUGE-1 score for QMSum and GovReport datasets in the\nSCROLL benchmark.\nResults.\nResults in Table 5 show that ICLM outperforms the baselines by around 5%, suggesting\nthat ICLM is better at long context reasoning. We hypothesize that the gains from ICLM may fade\nout to some extent when the LMs are fine-tuned, which may explain the relatively small gains in this\nevaluation compared to our other experiments.\n7\nPublished as a conference paper at ICLR 2024\nTable 5: Performance on long context reasoning benchmarks from SCROLL (Shaham et al., 2022)\n(\u00a73.3.6). ICLM outperforms baselines on all five datasets.\nMethod\nNarrativeQA\nQasper\nContractNLI\nQMSum\nGovReport\nAverage\nF1\nROUGE-1\nStandard\n16.5\n34.2\n78.6\n25.1\n8.2\n32.5\nkNN\n16.8\n34.1\n79.5\n24.3\n6.6\n32.3\nICLM\n17.1\n36.7\n80.7\n26.8\n9.1\n34.1\nFigure 4: Training loss and performance evolution on reading comprehension during pretraining.\nAfter training on around 150 billion tokens, ICLM is consistently better than the standard LM\non reading comprehension and retrieval augmentation tasks.\nMethod Design\nChoice\nPPL\nDocument\nRelevance\nRandom\n8.2\nClustering\n7.9\nLinks (final)\n7.3\nSemantic\nNo dedup\n8.3\nDedup\nDedup (final)\n7.3\nFigure 5: Ablation study of our method design.\nFigure 6: Performance with respect to the\nnumber of in-context examples (k).\n4\nANALYSIS\n4.1\nEVOLUTION OF PERFORMANCE DURING PRETRAINING\nThroughout the pretraining process, we closely monitor both the training loss and the downstream\ntask performance for the ICLM as well as the standard LM. Figure 4 illustrates the trajectory of the\ntraining loss and the performance on the RACE reading comprehension tasks for the 7B models.\nThe training loss for ICLM consistently remains lower than that of the standard LM. This suggests\nthat, when predicting the next token, ICLM benefits from a richer set of relevant prior documents to\nrefer to, while the standard LM has limited information to rely on, leading to higher loss. Figure 4\n(b, c) shows that after training on around 150 billion tokens, ICLM is consistently better than the\nstandard LM on reading comprehension tasks. This performance gap remains consistent throughout\nthe remainder of the pretraining phase. This suggests the scale of improvements by IN-CONTEXT\nPRETRAINING does not diminish and remains consistent as training on more tokens.\n4.2\nABLATION STUDY ON IN-CONTEXT PRETRAINING DESIGN\nWe perform analysis on two design choices of IN-CONTEXT PRETRAINING: a choice of methods for\nfinding retrieved documents and deduplication. Ablations are done with 1.5B models and evaluated\nwith perplexity on Wikipedia. The results are presented in Figure 5.\n8\nPublished as a conference paper at ICLR 2024\nDocument relevance.\nA key design of IN-CONTEXT PRETRAINING is grouping documents by\ntheir relevance. We consider three levels of relevance: random (the standard baseline discussed in\n\u00a73.2), clustering, and our document linking method in IN-CONTEXT PRETRAINING. Clustering\nfollows the method from Abbas et al. (2023) in clustering documents into 11k clusters based on their\nembeddings and sample documents from each cluster to form the training inputs. Documents grouped\nby clustering are sourced from the same clusters, indicating topical similarity but not necessarily\nclose relation. In contrast, ICLM links documents as nearest neighbors, indicating a higher degree\nof similarity. The relevance between documents increases from random, clustering to linking. We\nobserve that the perplexity of the language model decreases as the relevance increases.\nDeduplication.\nWe compare perplexity of the models trained with and without the semantic\ndeduplication step. Removing the semantic deduplication step leads to a significant decrease in\nperplexity. When near duplicate documents are present in the same context, language models might\nmerely copy from the prior document, leading to training instability.\n4.3\nDEMONSTRATION EXAMPLES SIZE FOR IN-CONTEXT LEARNING\nWe evaluate the 7B models trained with the standard method and IN-CONTEXT PRETRAINING, using\na varying number of demonstration examples on text classification tasks described in \u00a73.3.2. As\ndepicted in Figure 6, ICLM maintains consistent performance gains over the standard method, even\nas the number of demonstration examples grows. While the performance improves as the number of\ndemonstration examples increases, it plateaus after 32 examples.\n5\nRELATED WORK\nData batching based on similarity\nPrevious work employs batching lexically similar segments in\nthe same training batches to construct high-quality positive pairs for training retrieval-augmented\nlanguage models. For instance, Zhong et al. (2022) use BM25 and same documents to ensure the\nsegments in the same batch are similar to each other, while Min et al. (2023) group segments from\nthe same documents in the same batch. Our method shares the same spirit with these methods except\nwe maintain the relevance of documents in the same context window, yet context windows within\nbatches are shuffled. Additionally, our focus is to apply the batching method to train the standard\nlanguage models.\nPretraining with related documents.\nSeveral studies explore pretraining language models on a\nsmall-scale using related documents. For example, Yasunaga et al. (2022) incorporate Wikipedia\ndocuments with hyperlinks or citations into the input context and pretrain a masked LM. Yu et al.\n(2022); Wu et al. (2021) incorporate dictionary definitions of rare words or use contextual vectors from\npreviously encountered contexts that mention these rare words during the pretraining phase. Caciularu\net al. (2021) gather related documents using a human-curated multi-document news summarization\ndataset (11 million tokens) and continue to pretrain a masked LM. Lewis et al. (2020) place documents\nfrom the same date in the input context and pretrain LMs to summarize articles. However, hyperlinks\nare not always available across all domains and multi-document summarization datasets require\nhuman efforts to curate. Additionally, Lewis et al. (2020)\u2019s method restricts the scope of related\ndocuments to be from the same date. In contrast, we introduce a general method to collect web-scale\nrelated documents that does not require any metadata (e.g., hyperlinks, human curation or specific\ndates), which is necessary to scale the model to a pre-training setup.\nMultitask finetuning for in-context and instruction learning.\nFinetuning language models on a\ncollection of downstream tasks to improve the instruction learning and in-context learning abilities of\nLMs has been investigated in several papers. As discussed by Min et al. (2022); Chen et al. (2022);\nIvison et al. (2023); Wang et al. (2022; 2023b), a prevailing technique concatenates instructions,\ntraining samples from human-annotated downstream datasets into single text sequences, upon which\nthe LM is subsequently finetuned. Following this line of work, Gu et al. (2023) create intrinsic\ndownstream datasets by developing a task-specific retriever for each task. These retrievers are then\nused to retrieve demonstration examples from the pretraining corpus. The multitask finetuning method\nis complementary to IN-CONTEXT PRETRAINING as the former is tailored for the finetuning stage\nwhile the later focuses on the pretraining stage. Beyond improving LMs\u2019 in-context learning abilities,\n9\nPublished as a conference paper at ICLR 2024\nIN-CONTEXT PRETRAINING also improves their overall language modeling, reading comprehension,\nand fact-checking capabilities. We leave the combination of IN-CONTEXT PRETRAINING with\nmultitask finetuning methods as future work.\nTraining long-context language models.\nRecent studies have investigated the finetuning of LMs\nto extend their context length. Press et al. (2022); Chen et al. (2023); kaiokendev (2023) make\nmodifications to position encoding and finetune LMs on randomly concatenated short documents and\nsubsampled long documents from pretraining data. However, as highlighted by de Vries (2023), long\nsequence documents are notably rare in the pretraining data. For example, less than 5% of documents\nin CommonCrawl have longer than 2k tokens. In this work, we focus on constructing meaningful\nlong-context data, making language models better leverage its context-window. Our sorted data can\nbe used for both pretraining and finetuning stages to enhance LMs\u2019 ability to reason over contexts.\n6\nCONCLUSION\nWe introduce IN-CONTEXT PRETRAINING, a new pretraining method that learns to generate text\nconditioned on a set of relevant documents, exposing LMs to relevant contexts and providing training\nsignals beyond document boundaries. Our method is highly scalable and simple, and works with\nany pre-training pipeline by simply changing the document ordering during preprocessing. Our\ncomprehensive evaluation demonstrates our method leads to significant improvements in a wide\nvariety of settings that highlight the ability to understand and reason over the given context, including\nin-context learning, reading comprehension, retrieval augmentation, and more. Future research may\ndelve into the inherent connections between documents within specific corpus domains or using\nmultilingual retriever to group related multilingual documents in the same context. For example, the\ncode scripts within the same repository are related. This insight paves the way for future exploration,\nwhere concatenating entire repositories into a unified whole could lead to the creation of meaningful\nlong-context data sets.\nREFERENCES\nAmro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-\nefficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540,\n2023.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. TweetEval:\nUnified benchmark and comparative evaluation for tweet classification. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020, pp. 1644\u20131650, Online, November 2020.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.148. URL\nhttps://aclanthology.org/2020.findings-emnlp.148.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,\nR. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,\nvolume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.\ncc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nAvi Caciularu, Arman Cohan, Iz Beltagy, Matthew Peters, Arie Cattan, and Ido Dagan. CDLM:\nCross-document language modeling. In Findings of the Association for Computational Linguistics:\nEMNLP 2021, pp. 2648\u20132662, Punta Cana, Dominican Republic, November 2021. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.225. URL https://\naclanthology.org/2021.findings-emnlp.225.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.\n10\nPublished as a conference paper at ICLR 2024\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language\nmodel in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 719\u2013730, 2022.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924\u20132936,\nMinneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/\nN19-1300. URL https://aclanthology.org/N19-1300.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing\nSystems, 2022.\nHarm de Vries. In the long (context) run, 2023. URL https://www.harmdevries.com/post/\ncontext-length/.\nMatthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel\nMazar\u00e9, Maria Lomeli, Lucas Hosseini, and Herv\u00e9 J\u00e9gou. The faiss library, 2024.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n2368\u20132378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246.\nAvia Efrat and Omer Levy. The turking test: Can language models understand instructions? ArXiv,\nabs/2010.11982, 2020. URL https://api.semanticscholar.org/CorpusID:225062157.\nMerrill M Flood. The traveling-salesman problem. Operations research, 4(1):61\u201375, 1956.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Pre-training to learn in context. In Proceedings\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 4849\u20134870, Toronto, Canada, July 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-long.267. URL https://aclanthology.org/2023.acl-long.267.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\nlanguage model pre-training. In International conference on machine learning, pp. 3929\u20133938.\nPMLR, 2020.\nHamish Ivison, Noah A. Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. Data-efficient finetuning\nusing cross-task nearest neighbors. In Findings of ACL, 2023. URL https://arxiv.org/abs/\n2212.00196.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand\nJoulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.\nTransactions on Machine Learning Research, 2022. URL https://openreview.net/forum?id=\njKN1pXi7b0.\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. IEEE\nTransactions on Big Data, 7(3):535\u2013547, 2019.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601\u2013\n1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/\nv1/P17-1147. URL https://aclanthology.org/P17-1147.\nHerv\u00e9 J\u00e9gou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor\nsearch. IEEE transactions on pattern analysis and machine intelligence, 33:117\u201328, 01 2011. doi:\n10.1109/TPAMI.2010.57.\n11\nPublished as a conference paper at ICLR 2024\nkaiokendev. Things i\u2019m learning while training superhot, 2023. URL https://kaiokendev.github.\nio/til#.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural questions: A benchmark for question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:452\u2013466, 2019. doi: 10.1162/tacl_a_00276. URL\nhttps://aclanthology.org/Q19-1026.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding\ncomprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pp. 785\u2013794, Copenhagen, Denmark, September\n2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https:\n//aclanthology.org/D17-1082.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes,\nSebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, S\u00f6ren Auer, et al. Dbpedia\u2013a large-\nscale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167\u2013195, 2015.\nYoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. The\ninductive bias of in-context learning: Rethinking pretraining example design. In International\nConference on Learning Representations, 2022. URL https://openreview.net/forum?id=\nlnEaqbTJIRz.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke\nZettlemoyer. Pre-training via paraphrasing. Advances in Neural Information Processing Systems,\n33:18470\u201318481, 2020.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez,\nJacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-\naugmented dual instruction tuning, 2023.\nAlisa Liu and Jiacheng Liu. The memotrap dataset. https://github.com/inverse-scaling/\nprize/blob/main/data-release/README.md, 2023.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. Lost in the middle: How language models use long contexts, 2023.\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh.\nEntity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing, pp. 7052\u20137063, Online and Punta Cana,\nDominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/\nv1/2021.emnlp-main.565. URL https://aclanthology.org/2021.emnlp-main.565.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations, 2018.\nIan R. McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu,\nEuan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, Derik\nKauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong Huang, The Floating\nDroid, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping Zhou, Najoung Kim,\nSamuel R. Bowman, and Ethan Perez. Inverse scaling: When bigger isn\u2019t better, 2023.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in\ncontext. In Proceedings of the 2022 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pp. 2791\u20132809, Seattle, United\nStates, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.\n201. URL https://aclanthology.org/2022.naacl-main.201.\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. Nonparametric masked language modeling. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, pp. 2097\u20132118, Toronto, Canada, July 2023.\n12\nPublished as a conference paper at ICLR 2024\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2023.findings-acl.132.\nURL\nhttps://aclanthology.org/2023.findings-acl.132.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback, 2022.\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables\ninput length extrapolation. In International Conference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=R8sQPpGCv0.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, pp. 2383\u20132392, 2016.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-\nparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long\nlanguage sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp. 12007\u201312021, Abu Dhabi, United Arab Emirates, December 2022. Associa-\ntion for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.823.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael\nSch\u00e4rli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In\nInternational Conference on Machine Learning, pp. 31210\u201331227. PMLR, 2023a.\nWeijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen tau Yih.\nTrusting your evidence: Hallucinate less with context-aware decoding, 2023b.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint\narXiv:2301.12652, 2023c.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp.\n1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.\nURL https://www.aclweb.org/anthology/D13-1170.\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih,\nNoah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned\ntext embeddings, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\n13\nPublished as a conference paper at ICLR 2024\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023b.\nYike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, and\nYulia Tsvetkov.\nResolving knowledge conflicts in large language models.\narXiv preprint\narXiv:2310.00935, 2023a.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAtharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\nKirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir\nParmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri,\nRushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta\nPatro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative\ninstructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pp. 5085\u20135109, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.340. URL\nhttps://aclanthology.org/2022.emnlp-main.340.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far\ncan camels go? exploring the state of instruction tuning on open resources, 2023b.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n,\nArmand Joulin, and \u00c9douard Grave. Ccnet: Extracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference,\npp. 4003\u20134012, 2020.\nQiyu Wu, Chen Xing, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. Taking notes on the fly helps\nlanguage pre-training. In International Conference on Learning Representations, 2021. URL\nhttps://openreview.net/forum?id=lU5Rs_wCweN.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with\ncompression and selective augmentation, 2023.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\nanswering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, pp. 2369\u20132380, Brussels, Belgium, October-November 2018. Association for Compu-\ntational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang. LinkBERT: Pretraining language models with\ndocument links. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 8003\u20138016, Dublin, Ireland, May 2022. Association for\nComputational Linguistics. doi: 10.18653/v1/2022.acl-long.551. URL https://aclanthology.\norg/2022.acl-long.551.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang,\nMike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language\nmodeling. 2023.\nWenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong Xu, Michael\nZeng, and Meng Jiang. Dict-BERT: Enhancing language model pre-training with dictionary.\nIn Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Associ-\nation for Computational Linguistics: ACL 2022, pp. 1907\u20131918, Dublin, Ireland, May 2022.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2022.findings-acl.150.\nURL\nhttps://aclanthology.org/2022.findings-acl.150.\nSusan Zhang, Mona Diab, and Luke Zettlemoyer. Democratizing access to large-scale language\nmodels with opt-175b. Meta AI, 2022.\n14\nPublished as a conference paper at ICLR 2024\nXiang Zhang, Junbo Zhao, and Yann LeCun.\nCharacter-level convolutional networks for\ntext classification.\nIn C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett\n(eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates,\nInc., 2015a.\nURL https://proceedings.neurips.cc/paper_files/paper/2015/file/\n250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text\nclassification. In NIPS, 2015b.\nZhuosheng Zhang, Hai Zhao, and Rui Wang. Machine reading comprehension: The role of contextu-\nalized language models and beyond. arXiv preprint arXiv:2005.06249, 2020.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving\nfew-shot performance of language models. In International Conference on Machine Learning, pp.\n12697\u201312706. PMLR, 2021.\nZexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.\n5657\u20135673, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational\nLinguistics. doi: 10.18653/v1/2022.emnlp-main.382. URL https://aclanthology.org/2022.\nemnlp-main.382.\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for large\nlanguage models. ArXiv, abs/2303.11315, 2023.\n15\nPublished as a conference paper at ICLR 2024\nA\nADDITIONAL BACKGROUND\nA.1\nDEDUPLICATION\nCorpora often have semantic duplicates: pairs of documents that are semantically related, yet not\nentirely identical. Previous studies (Yasunaga et al., 2023) show that retraining highly similar\ndocuments in the input contexts during training hurts multimodal models\u2019 performance. We observed\na similar behaviur: when near duplicate documents are present in the same context, language models\nmight merely copy from the prior document, leading to training instability. Given that our retrieval\napproach inherently assesses pairwise document similarity, we can easily filter out near duplicate\ndocuments that have high cosine similarity with other documents. We find this deduplication step to\nbe crucial for achieving performance of good language models (\u00a74.2).\nA.2\nFAISS INDEX\nWe used a product quantised inverted file (IVFPQ) FAISS index with a code size of 256 and the\ncorresponding number of inverted lists 32768 , with total size of 62 gigabytes. The index contains\n235266464 768-dimensional embeddings originally in float 32. The index was trained on a sample\nof 1572864 embeddings and the train time was 423 s. Successively, the data is split in batches of\n50M embeddings and for each index shard the corresponding batch of embeddings is added to the\ntrained index, the average adding embeddings time per index shard is 628.4 s. Finally, approximate\nnearest neighbor search is conducted per each shard before aggregating all results. The nprobe used\nfor conducting approximate search is 64, this means that 0.2% of the inverted lists are probed during\nthe nearest neighbors search.\n16\n"
  },
  {
    "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning",
    "link": "https://arxiv.org/pdf/2310.09478.pdf",
    "upvote": "15",
    "text": "MiniGPT-v2: Large Language Model As a Unified\nInterface for Vision-Language Multi-task Learning\nJun Chen1,2\u2217 Deyao Zhu1 Xiaoqian Shen1 Xiang Li1 Zechun Liu2 Pengchuan Zhang2\nRaghuraman Krishnamoorthi2 Vikas Chandra2 Yunyang Xiong2\u2020 Mohamed Elhoseiny1\u2020\n1King Abdullah University of Science and Technology (KAUST)\n2Meta AI Research\nAbstract\nLarge language models have shown their remarkable capabilities as a general\ninterface for various language-related applications. Motivated by this, we target\nto build a unified interface for completing many vision-language tasks including\nimage description, visual question answering, and visual grounding, among others.\nThe challenge is to use a single model for performing diverse vision-language\ntasks effectively with simple multi-modal instructions. Towards this objective, we\nintroduce MiniGPT-v2, a model that can be treated as a unified interface for better\nhandling various vision-language tasks. We propose using unique identifiers for\ndifferent tasks when training the model. These identifiers enable our model to better\ndistinguish each task instruction effortlessly and also improve the model learning\nefficiency for each task. After the three-stage training, the experimental results\nshow that MiniGPT-v2 achieves strong performance on many visual question-\nanswering and visual grounding benchmarks compared to other vision-language\ngeneralist models. Our model and codes are available at https://minigpt-v2.\ngithub.io/.\n1\nIntroduction\nMulti-modal Large Language Models (LLMs) have emerged as an exciting research topic with a\nrich set of applications in vision-language community, such as visual AI assistant, image captioning,\nvisual question answering (VQA), and referring expression comprehension (REC). A key feature\nof multimodal large language models is that they can inherit advanced capabilities (e.g., logical\nreasoning, common sense, and strong language expression) from the LLMs [32, 49, 50, 8]. When\ntuned with proper vision-language instructions, multi-modal LLMs, specifically vision-language\nmodels, demonstrate strong capabilities such as producing detailed image descriptions, generating\ncode, localizing the visual objects in the image, and even performing multi-modal reasoning to better\nanswer complicated visual questions [59, 26, 55, 53, 7, 10, 58, 6, 60]. This evolution of LLMs\nenables interactions of visual and language inputs across communication with individuals and has\nbeen shown quite effective for building visual chatbots.\nHowever, learning to perform multiple vision-language tasks effectively and formulating their corre-\nsponding multi-modal instructions present considerable challenges due to the complexities inherent\namong different tasks. For instance, given a user input \u201ctell me the location of a person\", there\nare many ways to interpret and respond based on the specific task. In the context of the referring\nexpression comprehension task, it can be answered with one bounding box location of the person.\nFor the visual question-answering task, the model might describe their spatial location using human\nnatural language. For the person detection task, the model might identify every spatial location of\n\u2217Work partially done during the internship at Meta AI\n\u2020Equal last author\narXiv:2310.09478v3  [cs.CV]  7 Nov 2023\nOKVQA\nGQA\nVSR\nIconQA\nVizWiz\nHM\nRefCOCO test-B\nRefCOCO+ test-B\nRefCOCOg test\n33\n45\n57\n33\n45\n57\n37\n49\n61\n38\n44\n50\n23\n37\n51\n55\n57\n59\n79\n82\n85\n68\n71\n74\n78\n81\n84\nInstructBLIP (13B)\nLLaVA (13B)\nMiniGPT-4 (13B)\nShikra (13B)\nMiniGPT-v2 (7B)\nMiniGPT-v2 Chat (7B)\nFigure 1: Our MiniGPT-v2 achieves state-of-the-art performances on a broad range of vision-language\ntasks compared with other generalist models.\neach human in a given image. To alleviate this issue and towards a unified approach, we propose\na task-oriented instruction training scheme to reduce the multi-modal instructional ambiguity, and\na vision-language model, MiniGPT-v2. Specifically, we provide a unique task identifier token for\neach task. For example, we provide a [vqa] identifier token for training all the data samples from the\nvisual question answering tasks. In total, we provide six different task identifiers during the model\ntraining stages.\nOur model, MiniGPT-v2, has a simple architecture design. It directly takes the visual tokens from\na ViT vision encoder [12] and project them into the feature space of a large language model [50].\nFor better visual perception, we utilize higher-resolution images (448x448) during training. But\nthis will result in a larger number of visual tokens. To make the model training more efficient, we\nconcatenate every four neighboring visual tokens into a single token, reducing the total number by\n75%. Additionally, we utilize a three-stage training strategy to effectively train our model with a\nmixture of weakly-labeled, fine-grained image-text datasets, and multi-modal instructional datasets,\nwith different training focus at each stage.\nTo evaluate the performance of our model, we conducted extensive experiments on diverse vision-\nlanguage tasks, including (detailed) image/grounded captioning, vision question answering, and\nvisual grounding. The results demonstrate that our MiniGPT-v2 can achieve SOTA or comparable\nperformance on diverse benchmarks compared to previous vision-language generalist models, such\nas MiniGPT-4 [59], InstructBLIP [10], LLaVA [26] and Shikra [7]. For example, our MiniGPT-v2\noutperforms MiniGPT-4 by 21.3%, InstructBLIP by 11.3%, and LLaVA by 11.7% on the VSR\nbenchmark [25], and it also performs better than the previously established strong baseline, Shikra,\nin most validations on RefCOCO, RefCOCO+, and RefCOCOg. Our model establishes new state-\nof-the-art results on these benchmarks among vision-language generalist models, shown in Fig.\n1.\n2\nRelated Work\nWe briefly review relevant works on advanced large language models and multi-modal LLMs for\nvisual aligning.\nAdvanced Large Language Models (LLMs). Early-stage models such as GPT-2 [38] and BERT [11]\nare foundation models trained on web-scale text datasets, marking a breakthrough in the NLP field.\nFollowing the success of foundation models, LLMs with higher capacity and increased training\ndata are developed, including GPT-3 [4], Megatron-turing NLG [46], PaLM [9], Gopher [39],\n2\nChinchilla [16], OPT [57], and BLOOM [41]. Most recently, the efforts have been focused on\nrefining LLMs to work effectively with human instruction and feedback. Representative works in\nthis direction are InstructGPT [34] and ChatGPT [32], which demonstrate strong capabilities such\nas answering a diverse range of language questions, engaging in conversations with humans, and\nlearning to perform complex tasks like writing refinement and coding assistant.\nConcurrent with these advancements of LLMs is the rise of LLaMA [49] language models. To\nenable human instruction following abilities similar to ChatGPT, some works attempt to finetune\nthe LLaMA model with additional high-quality instruction datasets [1]. Examples of these models\ninclude Alpaca [47], Vicuna [8], and MPT [48]. Some other open-sourced language models that\nlearned from the human feedback data, such as Falcon [35] and LLaMA-2 [50], have also been\nintroduced to the NLP community with impressive performance.\nVisual Aligning with LLMs. With the remarkable generalization abilities of LLMs, interesting\nstudies have extended LLMs to multi-modal domains by aligning visual inputs with LLMs. Early\nworks such as VisualGPT [5] and Frozen [51] used pre-trained language models to improve vision-\nlanguage models on image captioning and visual question answering. This initial exploration paved\nthe way for subsequent vision-language research such as Flamingo [2] and BLIP-2 [22]. More\nrecently, GPT-4 has been released and demonstrates many advanced multi-modal abilities, e.g.,\ngenerating website code based on handwritten text instructions. Those demonstrated capabilities\ninspired other vision-language LLMs, including MiniGPT-4 [59] and LLaVA [26], which align the\nimage inputs with a large language model, Vicuna [8], using proper instructional tuning. These\nvision-language models also showcase many advanced multi-modal capabilities after the alignment.\nRecent works, such as Vision-LLM [53], Kosmos-2 [36], Shikra [7], and our concurrent work,\nQwen-VL [3], also demonstrate that multi-model LLMs models can also perform visual grounding\nby generating the text format of bounding boxes through language model.\n3\nMethod\nViT\n\u2026\nLinear\nLlama 2\n\u2026\n</Img> [refer] \nwhere is the \nleft ear? [/INST]\n[INST] \n<Img>\n\u2026\nconcat\n{<30><12><49><40>}\nFigure 2: Architecture of MiniGPT-v2.\nThe\nmodel takes a ViT visual backbone, which remains\nfrozen during all training phases. We concatenate\nfour adjacent visual output tokens from ViT back-\nbone and project them into LLaMA-2 language\nmodel space via a linear projection layer.\nWe start by introducing our vision-language\nmodel, MiniGPT-v2, then discuss the basic idea\nof a multi-task instruction template with task\nidentifiers for training, and finally adapt our task\nidentifier idea to achieve task-oriented instruc-\ntion tuning.\n3.1\nModel Architecture\nOur proposed model architecture, MiniGPT-v2,\nis shown in Fig. 2. It consists of three com-\nponents: a visual backbone, a linear projection\nlayer, and a large language model. We describe\neach component as follows:\nVisual backbone.\nMiniGPT-v2 adapts the\nEVA [12] as our visual backbone model back-\nbone. We freeze the visual backbone during\nthe entire model training. We train our model\nwith the image resolution 448x448, and we in-\nterpolate the positional encoding to scale with a\nhigher image resolution.\nLinear projection layer. We aim to project all the visual tokens from the frozen vision backbone\ninto the language model space. However, for higher-resolution images such as 448x448, projecting\nall the image tokens results in a very long-sequence input (e.g., 1024 tokens) and significantly lowers\nthe training and inference efficiency. Hence, we simply concatenate 4 adjacent visual tokens in the\nembedding space and project them together into one single embedding in the same feature space\nof the large language model, thus reducing the number of visual input tokens by 4 times. With this\noperation, our MiniGPT-v2 can process high-resolution images much more efficiently during the\ntraining and inference stage.\n3\nLarge language model. MiniGPT-v2 adopts the open-sourced LLaMA2-chat (7B) [50] as the\nlanguage model backbone. In our work, the language model is treated as a unified interface for\nvarious vision-language inputs. We directly rely on the LLaMA-2 language tokens to perform\nvarious vision-language tasks. For the visual grounding tasks that necessitate the generation of spatial\nlocations, we directly ask the language model to produce textual representations of bounding boxes\nto denote their spatial positions.\n3.2\nMulti-task Instruction Template\nWhen training a single unified model for multiple different tasks such as visual question answering,\nimage caption, referring expression, grounded image caption, and region identification, the multi-\nmodal model might fail to distinguish each task by just aligning visual tokens to language models.\nFor instance, when you ask \u201cTell me the spatial location of the person wearing a red jacket?\u201d, the\nmodel can either respond you the location in a bounding box format (e.g., < Xleft >< Ytop ><\nXright >< Ybottom >) or describe the object location using natural language (e.g., upper right\ncorner). To reduce such ambiguity and make each task easily distinguishable, we introduce task-\nspecific tokens in our designed multi-task instruction template for training. We now describe our\nmulti-task instruction template in more details.\nGeneral input format. We follow the LLaMA-2 conversation template design and adapt it for the\nmulti-modal instructional template. The template is denoted as follows,\n[INST] <Img> < ImageFeature> </Img> [Task Identifier] Instruction [/INST]\nIn this template, [INST] is considered as the user role, and [/INST] is considered as the assistant role.\nWe structure the user input into three parts. The first part is the image features, the second part is the\ntask identifier token, and the third part is the instruction input.\nTask identifier tokens. Our model takes a distinct identifier for each task to reduce the ambiguity\nacross various tasks. As illustrated in Table 1, we have proposed six different task identifiers for visual\nquestion answering, image caption, grounded image captioning, referring expression comprehension,\nreferring expression generation, and phrase parsing and grounding respectively. For vision-irrelevant\ninstructions, our model does not use any task identifier token.\nTasks\nVQA\nCaption\nGrounded Caption\nREC\nREG\nObject Parsing and Grounding\nIdentifiers\n[vqa]\n[caption]\n[grounding]\n[refer]\n[identify]\n[detection]\nTable 1: Task identifier tokens for 6 different tasks, including visual question answering, image cap-\ntioning, grounded image captioning, referring expression comprehension (REC), referring expression\ngeneration (REG), and object parsing and grounding (where the model extracts objects from the input\ntext and determines their bounding box locations).\nSpatial location representation. For tasks such as referring expression comprehension (REC),\nreferring expression generation (REG), and grounded image captioning, our model is required\nto identify the spatial location of the referred objects accurately. We represent the spatial location\nthrough the textual formatting of bounding boxes in our setting, specifically: \u201c{< Xleft >< Ytop ><\nXright >< Ybottom >}\". Coordinates for X and Y are represented by integer values normalized in\nthe range [0,100]. < Xleft > and < Ytop > denote the x and y coordinate top-left corner of the\ngenerated bounding box, and < Xright > and < Ybottom > denote the x and y coordinates of the\nbottom-right corner.\n3.3\nMulti-task Instruction Training\nWe now adapt our designed multi-task instruction template for instruction training. The basic\nidea is to take instruction with task-specific identifier token as input for task-oriented instruction\ntraining of MiniGPT-v2. When input instructions have task identifier tokens, our model will become\nmore prone to multiple-task understanding during training. We train our model with task identifier\ninstructions for better visual aligment in three stages. The first stage is to help MiniGPT-v2 build\nbroad vision-language knowledge through many weakly-labeled image-text datasets, and high-quality\nfine-grained vision-language annotation datasets as well (where we will assign a high data sampling\nratio for weakly-labeled image-text datasets). The second stage is to improve the model with only\n4\nfine-grained data for multiple tasks. The third stage is to finetune our model with more multi-modal\ninstruction and language datasets for answering diverse multi-modal instructions better and behaving\nas a multi-modal chatbot. The datasets used for training at each stage are listed in Table 2.\nData types\nDataset\nStage 1\nStage 2\nStage 3\nWeakly-labeled\nGRIT-20M (REC and REG), LAION, CC3M, SBU\n\u2713\n\u2717\n\u2717\nGrounded caption\nGRIT-20M\n\u2713\n\u2717\n\u2717\nCaption\nCOCO caption, Text Captions\n\u2713\n\u2713\n\u2713\nREC\nRefCOCO, RefCOCO+, RefCOCOg, Visual Genome\n\u2713\n\u2713\n\u2713\nREG\nRefCOCO, RefCOCO+, RefCOCOg\n\u2713\n\u2713\n\u2713\nVQA\nGQA, VQAv2, OCR-VQA, OK-VQA, AOK-VQA\n\u2713\n\u2713\n\u2713\nMultimodal instruction\nLLaVA dataset, Flickr30k, Multi-task conversation\n\u2717\n\u2717\n\u2713\nLangauge dataset\nUnnatural Instructions\n\u2717\n\u2717\n\u2713\nTable 2: The training datasets used for our model three-stage training.\nStage 1: Pretraining. To have broad vision-language knowledge, our model is trained on a mix of\nweakly-labeled and fine-grained datasets. We give a high sampling ratio for weakly-labeled datasets\nto gain more diverse knowledge in the first-stage.\nFor the weakly-labeled datasets, we use LAION [42], CC3M [44], SBU [33], and GRIT-20M from\nKosmos v2 [36] that built the dataset for referring expression comprehension (REC), referring\nexpression generation (REG), and grounded image captioning.\nFor fine-grained datasets, we use datasets like COCO caption [24] and Text Captions [45] for\nimage captioning, RefCOCO [20], RefCOCO+ [56], and RefCOCOg [29] for REC. For REG, we\nrestructured the data from ReferCOCO and its variants, reversing the order from phrase \u2192 bounding\nboxes to bounding boxes \u2192 phrase. For VQA datasets, our training takes a variety of datasets, such\nas GQA [19], VQA-v2 [14], OCR-VQA [31], OK-VQA [30], and AOK-VQA [43].\nStage 2: Multi-task training. To improve the performance of MiniGPT-v2 on each task, we only\nfocus on using fine-grained datasets to train our model at this stage. We exclude the weakly-supervised\ndatasets such as GRIT-20M and LAION from stage-1 and update the data sampling ratio according\nto the frequency of each task. This strategy enables our model to prioritize high-quality aligned\nimage-text data for superior performance across various tasks.\nStage 3: Multi-modal instruction tuning. Subsequently, we focus on tuning our model with more\nmulti-modal instruction datasets and enhancing its conversation ability as a chatbot. We continue\nusing the datasets from the second stage and add instructional datasets, including LLaVA [26],\nFlickr30k dataset [37], our constructed mixing multi-task dataset, and the language dataset, Unnatural\nInstruction [17]. We give a lower data sampling ratio for the fine-grained datasets from stage-2 and a\nhigher data sampling ratio for the new instruction datasets.\n\u2013 LLaVA instruction data. We add the multi-modal instruction tuning datasets, including the detailed\ndescriptions and complex reasoning from LLaVA [26], with 23k and 58k data examples respectively.\n\u2013 Flicker 30k. After the second-stage training, our MiniGPT-v2 can effectively generate the grounded\nimage caption. Nevertheless, these descriptions tend to be short and often cover very few number of\nvisual objects. This is because the GRIT-20M dataset from KOSMOS-v2 [36] that our model was\ntrained with, features a limited number of grounded visual objects in each caption, and our model\nlacks proper multi-modal instruction tuning to teach it to recognize more visual objects. To improve\nthis, we fine-tune our model using the Flickr30k dataset [37], which provides more contextual\ngrounding of entities within its captions.\nWe prepare the Flickr30k dataset in two distinct formats for training our model to perform grounded\nimage caption and a new task \u201cobject parsing and grounding\":\n1) Grounded image caption. We select captions with a minimum of five grounded phrases, containing\naround 2.5k samples, and we directly instruct the model to produce the grounded image caption. e.g.,\na <p>wooden table</p>{<Xleft><Ytop><Xright><Ybottom>} in the center of the room.\n2) Object parsing and grounding. This new task is to parse all the objects from an input caption\nand then ground each object. To enable this, we use the task identifier[detection] to differentiate this\ncapability from other tasks. Also, we use Flickr30k to construct two types of instruction datasets:\n5\nMethod\nGrounding\nOKVQA\nGQA\nVSR\nIconVQA\nVizWiz\nHM\n(zero-shot)\n(zero-shot)\n(zero-shot)\n(zero-shot)\nFlamingo-9B\n\u2717\n44.7\n-\n31.8\n-\n28.8\n57.0\nBLIP-2 (13B)\n\u2717\n45.9\n41.0\n50.9\n40.6\n19.6\n53.7\nInstructBLIP (13B)\n\u2717\n-\n49.5\n52.1\n44.8\n33.4\n57.5\nMiniGPT-4 (13B)\n\u2717\n37.5\n30.8\n41.6\n37.6\n-\n-\nLLaVA (13B)\n\u2717\n54.4\n41.3\n51.2\n43.0\n-\n-\nShikra (13B)\n\u2713\n47.2\n-\n-\n-\n-\n-\nOurs (7B)\n\u2713\n56.9\n60.3\n60.6\n47.7\n32.9\n58.2\nOurs (7B)-chat\n\u2713\n57.8\n60.1\n62.9\n51.5\n53.6\n58.8\nTable 3: Results on multiple VQA tasks. We report top-1 accuracy for each task. Grounding column\nindicates whether the model incorporates visual localization capability. The best performance for\neach benchmark is indicated in bold.\nMethod\nModel types\nRefCOCO\nRefCOCO+\nRefCOCOg\nAvg\nval\ntest-A\ntest-B\nval\ntest-A\ntest-B\nval\ntest\nUNINEXT\nSpecialist models\n92.64\n94.33\n91.46\n85.24\n89.63\n79.79\n88.73\n89.37\n88.90\nG-DINO-L\n90.56\n93.19\n88.24\n82.75\n88.95\n75.92\n86.13\n87.02\n86.60\nVisionLLM-H\nGeneralist models\n-\n86.70\n-\n-\n-\n-\n-\n-\n-\nOFA-L\n79.96\n83.67\n76.39\n68.29\n76.00\n61.75\n67.57\n67.58\n72.65\nShikra (7B)\n87.01\n90.61\n80.24\n81.60\n87.36\n72.12\n82.27\n82.19\n82.93\nShikra (13B)\n87.83\n91.11\n81.81\n82.89\n87.79\n74.41\n82.64\n83.16\n83.96\nOurs (7B)\n88.69\n91.65\n85.33\n79.97\n85.12\n74.45\n84.44\n84.66\n84.29\nOurs (7B)-chat\n88.06\n91.29\n84.30\n79.58\n85.52\n73.32\n84.19\n84.31\n83.70\nTable 4: Results on referring expression comprehension tasks. Our MiniGPT-v2 outperforms\nmany VL-generalist models including VisionLLM [53], OFA [52] and Shikra [7] and reduces the\naccuracy gap comparing to specialist models including UNINEXT [54] and G-DINO [27].\ncaption\u2192 grounded phrases and phrase \u2192 grounded phrase, each containing around 2.5k and 3k\nsamples. Then we prompt our model with the instruction: [detection] description, the model will\ndirectly parse the objects from the input image description and also ground the objects into bounding\nboxes.\n\u2013 Mixing multi-task dataset. After extensive training with single-round instruction-answer pairs,\nthe model might not handle multiple tasks well during multi-round conversations since the context\nbecomes more complex. To alleviate this situation, we create a new multi-round conversation dataset\nby mixing the data from different tasks. We include this dataset into our third-stage model training.\n\u2013 Unnatural instruction. The conversation abilities of language model can be reduced after extensive\nvision-language training. To fix this, we add the language dataset, Unnatural Instruction [17] into our\nmodel\u2019s third-stage training for helping recover the language generation ability.\n4\nExperiments\nIn this section, we present experimental settings and results. We primarily conduct experiments\non (detailed) image/grounded captioning, vision question answering, and visual grounding tasks,\nincluding referring expression comprehension. We present both quantitative and qualitative results.\nImplementation details. Throughout the entire training process, the visual backbone of MiniGPT-v2\nremains frozen. We focus on training the linear projection layer and efficient finetuning the language\nmodel using LoRA [18]. With LoRA, we finetune Wq and Wv via low-rank adaptation. In our\nimplementation, we set the rank, r = 64. We trained the model with an image resolution of 448x448\nduring all stages. During each stage, we use our designed multi-modal instructional templates for\nvarious vision-language tasks during the model training.\nTraining and hyperparameters. We use AdamW optimizer with a cosine learning rate scheduler\nto train our model. In the initial stage, we train on 8xA100 GPUs for 400,000 steps with a global\nbatch size of 96 and an maximum learning rate of 1e-4. This stage takes around 90 hours. During the\nsecond stage, the model is trained for 50,000 steps on 4xA100 GPUs with a maximum learning rate\n6\nOKVQA\nGQA\nWizViz\nVSR\nIconVQA\nHM\nAverage\nOurs w/o task identifier\n50.5\n53.4\n28.6\n57.5\n44.8\n56.8\n48.6\nOurs\n52.1\n54.6\n29.4\n59.9\n45.6\n57.4\n49.8\nTable 5: Task identifier ablation study on VQA benchmarks. With task identifier during the model\ntraining can overall improve VQA performances from multiple VQA benchmarks\nMethod\nCHAIRI \u2193\nCHAIRS \u2193\nLen\nMiniGPT-4\n9.2\n31.5\n116.2\nmPLUG-Owl\n30.2\n76.8\n98.5\nLLaVA\n18.8\n62.7\n90.7\nMultiModal-GPT\n18.2\n36.2\n45.7\nMiniGPT-v2 (long)\n8.7\n25.3\n56.5\nMiniGPT-v2 (grounded)\n7.6\n12.5\n18.9\nMiniGPT-v2 (short)\n4.4\n7.1\n10.3\nTable 6: Results on hallucination. We evaluate the hallucination of MiniGPT-v2 with different\ninstructional templates and output three versions of captions for evaluation. For the \u201clong\" version,\nwe use the prompt generate a brief description of the given image. For the \u201cgrounded\" version, the\ninstruction is [grounding] describe this image in as detailed as possible. For the \u201cshort\" version, the\nprompt is [caption] briefly describe the image.\nof 1e-5, adopting a global batch size of 64, and this training stage lasts roughly 20 hours. For the last\nstage, training is executed for another 35,000 steps on 4xA100 GPUs, using a global batch size of 24\nand this training stage took around 7 hours, maintaining the same maximum learning rate of 1e-5.\n4.1\nQuantitative Evaluation\nDataset and evaluation metrics. We evaluate our model across a range of VQA and visual grounding\nbenchmarks. For VQA benchmarks, we consider OKVQA [43], GQA [19], visual spatial reasoning\n(VSR) [25], IconVQA [28], VizWiz [15], HatefulMemes and (HM) [21]. For visual grounding, we\nevaluate our model on RefCOCO [20] and RefCOCO+[56], and RefCOCOg[29] benchmarks.\nTo evaluate VQA benchmarks, we use an open-ended approach with a greedy decoding strategy. We\nevaluate each VQA question with the following instruction template: \u201c[vqa] question\". Following\nthe previous method [10], we evaluate the performance by matching the model\u2019s response to the\nground-truth and reporting top-1 accuracy. For visual grounding benchmarks, we use the template\n\u201c[refer] give me the location of Referring expression\" for each referring expression comprehension\nquestion, and a predicted bounding box is considered as correct for reporting accuracy if its IOU\nbetween prediction and ground-truth is higher than 0.5.\nVisual question answering results. Table 3 presents our experimental results on multiple VQA\nbenchmarks. Our results compare favorably to baselines including MiniGPT-4 [59], Shikra [7],\nLLaVA [26], and InstructBLIP [10] across all the VQA tasks. For example, on QKVQA, our MiniGPT-\nv2 outperforms MiniGPT-4, Shikra, LLaVA, and BLIP-2 by 20.3%, 10.6%, 3.4%, and 11.9%. These\nresults indicate the strong visual question answering capabilities of our model. Furthermore, we find\nthat our MiniGPT-v2 (chat) variant shows higher performance than the version trained after the second\nstage. On OKVQA, VSR, IconVQA, VizWiz, and HM, MiniGPT-v2 (chat) outperforms MiniGPT-v2\nby 0.9%, 2.3%, 4.2%, 20.7%, and 0.6%. We believe that the better performance can be attributed to\nthe improved language skills during the third-stage training, which is able to benefit visual question\ncomprehension and response, especially on VizWiz with 20.7% top-1 accuracy increase.\nReferring expression comprehension results. Table 4 compares our model to baselines on REC\nbenchmarks. Our MiniGPT-v2 shows strong REC performance on RefCOCO, RefCOCO+, and Ref-\nCOCOg, performing better than other vision-language generalist models. MiniGPT-v2 outperforms\nOFA-L [52] by over 8% accuracy across all tasks of RefCOCO/RefCOCO+/RefCOCOg. Compared\nwith a strong baseline, Shikra (13B) [7], our model still shows better results, e.g., 84.29% vs 83.96%\naccuracy in average. These results provide direct evidence for the competing visual grounding\n7\n[identify] this \n{<35><45><65><70>} is \na black chainring\nCut slice of fruit cake on a \nplate with a fork and a \ncup of coffee with flowers \nin a vase\n[grounding] please \ndescribe this image as \ndetailed as possible\n[refer] the right player's hat \nWho are the people on the \nright, on the left, and in the \nmiddle? \nThe people in the image \nare:\n* Barack Obama, the \nformer President of the \nUnited States, is on the \nleft side of the image.\n* Joe Biden, the current \nPresident of the United \nStates, is in the middle of \nthe image.\n* Donald Trump, the \nformer President of the \nUnited States, is on the \nright side of the image.\nThe image depicts a large cave with a \ngroup of people painted on the walls. \nThe people are engaged in various \nactivities, such as hunting, gathering, \nand riding horses. There are several \nhorses scattered throughout the scene, \nwith some in the foreground and others \nin the background.\nIn addition to the people and horses, \nthere are several other animals depicted \nin the cave, including a lion, a tiger, and \na dinosaur. The cave is also adorned \nwith various objects, such as a bow and \narrows, a spear, and a shield.\nOverall, the scene appears to be a \nvibrant and lively depiction of ancient \nlife, showcasing the skills and activities \nof the people who lived in this cave.\nPlease describe this image as \ndetailed as possible\n[vqa] Which country does it \ncome from? \naustralia\na living room\na lamp\na spacious gray sofa\ntwo wall paintings\nshoes\na blanket \n[detection] The image \nshowcases a living room \nfeaturing a lamp, a \nspacious gray sofa, two \nwall paintings, and shoes \nresting on a blanket\na)\nb)\nc)\nd)\ne)\nf)\ng)\nFigure 3: Examples for various multi-modal capabilities of MiniGPT-v2. We showcase that our\nmodel is capable of completing multiple tasks such as referring expression comprehension, referring\nexpression generation, detailed grounded image caption, visual question answering, detailed image\ndescription, and directly parsing phrase and grounding from a given input text.\ncapabilities of MiniGPT-v2. Although our model underperforms specialist models, the promising\nperformance indicates its growing competence in visual grounding.\nAblation on task identifier. We conduct ablation studies on the effect of the task identifier on the\nperformance of MiniGPT-v2. We compare our model with the variant without using task identifiers\non VQA benchmarks. Both models were trained on 4xA100 GPUs for 24 hours with an equal number\nof training steps for multiple vision-language tasks. Results in Table 5 demonstrate the performance\non multiple VQA benchmarks and consistently show that token identifier training benefits the overall\nperformance of MiniGPT-v2. Specifically, our MiniGPT-v2 with task-oriented instruction training\nachieves 1.2% top-1 accuracy improvement on average. These ablation results can validate the clear\nadvantage of adding task identifier tokens and support the use of multi-task identifiers for multi-task\nlearning efficiency.\n8\nHallucination. We measure the hallucination of our model on image description generation and\ncompare the results with other vision-language baselines, including MiniGPT-4 [59], mPLUG-\nOwl [55], LLaVA [26], and MultiModal-GPT [13]. Following the methodology from [23], we use\nCHAIR [40] to assess hallucination at both object and sentence levels. As shown in Table 6, we find\nthat our MiniGPT-v2 tends to generate the image description with reduced hallucination compared to\nother baselines. We have evaluated three types of prompts in MiniGPT-v2. First, we use the prompt\ngenerate a brief description of the given image without any specific task identifier which tends to\nproduce more detailed image descriptions. Then we provide the instruction prompt [grounding]\ndescribe this image in as detailed as possible for evaluating grounded image captions. Lastly, we\nprompt our model with [caption] briefly describe the image. With these task identifiers, MiniGPT-v2\nis able to produce a variety of image descriptions with different levels of hallucination. As a result,\nall these three instruction variants have lower hallucination than our baseline, especially with the task\nspecifiers of [caption] and [grounding].\n4.2\nQualitative Results\nWe now provide the qualitative results for a complementary understanding of our model\u2019s multi-modal\ncapabilities. Some examples can be seen in Fig. 3. Specifically, we demonstrated various abilities\nin the examples including a) object identification; b) detailed grounded image captioning; c) visual\nquestion answering; d) referring expression comprehension; e) visual question answering under\ntask identifier; f) detailed image description; g) object parsing and grounding from an input text.\nMore qualitative results can be found in the Appendix. These results demonstrate that our model has\ncompeting vision-language understanding capabilities. Moreover, notice that we train our model only\nwith a few thousand of instruction samples on object parsing and grounding tasks at the third-stage,\nand our model can effectively follow the instructions and generalize on the new task. This indicates\nthat our model has the flexibility to adapt on many new tasks.\nNote that our model still occasionally shows hallucinations when generating the image description\nor visual grounding. e.g., our model may sometimes produce descriptions of non-existent visual\nobjects or generate inaccurate visual locations of grounded objects. We believe training with more\nhigh-quality image-text aligned data and integrating with a stronger vision backbone or large language\nmodel hold the potential for alleviating this issue.\n5\nConclusion\nIn this paper, we introduce MiniGPT-v2, a multi-modal LLM that can serve as a unified interface\nfor various vision-language multi-tasking learning. To develop a single model capable of handling\nmultiple vision-language tasks, we propose using distinct identifiers for each task during the training\nand inference. These identifiers help our model easily differentiate various tasks and also improve\nlearning efficiency. Our MiniGPT-v2 achieves state-of-the-art results across many visual question\nanswering and referring expression comprehension benchmarks. We also found that our model can\nefficiently adapt to new vision-language tasks, which suggests that MiniGPT-v2 has many potential\napplications in the vision-language community.\n9\nReferences\n[1] Sharegpt. https://github.com/domeccleston/sharegpt, 2023.\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. In Advances in Neural Information Processing Systems, 2022.\n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and\nJingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint\narXiv:2308.12966, 2023.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n[5] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient adaptation\nof pretrained language models for image captioning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 18030\u201318040, 2022.\n[6] Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, and Mohamed Elhoseiny. Video chatcaptioner:\nTowards the enriched spatiotemporal descriptions. arXiv preprint arXiv:2304.04227, 2023.\n[7] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\nmultimodal llm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023.\n[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\nLi, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\ninstruction tuning, 2023.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[12] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprint\narXiv:2211.07636, 2022.\n[13] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei\nZhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans.\narXiv preprint arXiv:2305.04790, 2023.\n[14] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.\n[15] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P\nBigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 3608\u20133617, 2018.\n[16] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556, 2022.\n[17] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language\nmodels with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.\n[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\n[19] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 6700\u20136709, 2019.\n[20] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects\nin photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP), pages 787\u2013798, 2014.\n[21] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and\nDavide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances\nin neural information processing systems, 33:2611\u20132624, 2020.\n[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[23] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object\nhallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.\n[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014:\n13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages\n740\u2013755. Springer, 2014.\n10\n[25] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for\nComputational Linguistics, 11:635\u2013651, 2023.\n[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023.\n[27] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object\ndetection. arXiv preprint arXiv:2303.05499, 2023.\n[28] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and\nSong-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language\nreasoning. arXiv preprint arXiv:2110.13214, 2021.\n[29] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\nGeneration and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 11\u201320, 2016.\n[30] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question\nanswering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on\ncomputer vision and pattern recognition, pages 3195\u20133204, 2019.\n[31] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual\nquestion answering by reading text in images. In 2019 international conference on document analysis and\nrecognition (ICDAR), pages 947\u2013952. IEEE, 2019.\n[32] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.\n[33] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned\nphotographs. Advances in neural information processing systems, 24, 2011.\n[34] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\n[35] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon\nllm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116,\n2023.\n[36] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:\nGrounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023.\n[37] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence\nmodels. In Proceedings of the IEEE international conference on computer vision, pages 2641\u20132649, 2015.\n[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[39] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods,\nanalysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\n[40] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination\nin image captioning. arXiv preprint arXiv:1809.02156, 2018.\n[41] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter\nopen-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n[42] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400\nmillion image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[43] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-\nokvqa: A benchmark for visual question answering using world knowledge. In European Conference on\nComputer Vision, pages 146\u2013162. Springer, 2022.\n[44] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565,\n2018.\n[45] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image\ncaptioningwith reading comprehension. 2020.\n[46] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,\nZhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to\ntrain megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990,\n2022.\n[47] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.\ncom/tatsu-lab/stanford_alpaca, 2023.\n[48] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms,\n2023. Accessed: 2023-05-05.\n[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\n11\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[51] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal\nfew-shot learning with frozen language models. Advances in Neural Information Processing Systems,\n34:200\u2013212, 2021.\n[52] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-\nsequence learning framework. In International Conference on Machine Learning, pages 23318\u201323340.\nPMLR, 2022.\n[53] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie\nZhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric\ntasks. arXiv preprint arXiv:2305.11175, 2023.\n[54] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance\nperception as object discovery and retrieval. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 15325\u201315336, 2023.\n[55] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178, 2023.\n[56] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in\nreferring expressions. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The\nNetherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69\u201385. Springer, 2016.\n[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n[58] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, and Mohamed Elhoseiny.\nChatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. arXiv preprint\narXiv:2303.06594, 2023.\n[59] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[60] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, R\u00f3bert Csord\u00e1s, Anand Gopalakrishnan,\nAbdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in\nnatural language-based societies of mind. arXiv preprint arXiv:2305.17066, 2023.\n12\nA\nAppendix\nIn the supplementary, we provide more qualitative results that are generated from our model to\ndemonstrate the vision-language multi-tasking capabilities.\nA.1\nInstruction template for various vision-language tasks\nRefCOCO/RefCOCO+/RefCOCOg: [refer] give me the location of question\nVizWiz: [vqa] Based on the image, respond to this question with a single word or phrase: question,\nand reply \u2019unanswerable\u2019 when the provided information is insufficient\nHateful Meme: [vqa] This is an image with: question written on it. Is it hateful? Answer:\nVSR: [vqa] Based on the image, is this statement true or false? question\nIconQA, GQA, OKVQA: [vqa] Based on the image, respond to this question with a single word or\nphrase: question\nA.2\nAdditional Qualitative Results\nTo study how well our model is able to take visual input and answer questions based on task-oriented\nidentifier, we use our model to perform multiple vision-language tasks including grounded image\ncaptioning in Fig. 4, Fig. 5, Fig. 6 and Fig. 7; Object parsing and grounding in Fig. 8, Fig. 9, Fig. 10\nand Fig. 11; Referring expression comprehension in Fig. 12, Fig. 13, Fig. 14 and Fig. 15; Object\nidentification in Fig. 16, Fig. 17, Fig. 18 and Fig. 19.\nFor each task, we share 4 examples for showing the vision-language capabilities of our model. The\nresults in the demo provide direct evidence for the competing visual understanding capabilities of\nMiniGPT-v2 on multiple vision-language tasks. For example, in the cases of grounded caption, our\nmodel is able to give correct grounded image caption with detailed spatial locations of objects. In the\ncases of identify, the model also generates our expected object names. MiniGPT-v2 can understand\nthe new scenes and follow the question identifier to respond. But we also need to note that our model\nstill has some hallucination e.g., In Fig. 6, several persons are not grounded accurately, and in Fig. 7,\nthere does not exist a vase in the image.\nFigure 4: Detail grounded image caption example.\n13\nFigure 5: Detail grounded image caption example\nFigure 6: Detail grounded image caption example\n14\nFigure 7: Detail grounded image caption example\nFigure 8: Object parsing and grounding example\n15\nFigure 9: Object parsing and grounding example\nFigure 10: Object parsing and grounding example\n16\nFigure 11: Object parsing and grounding example\nFigure 12: Referring expression comprehension example\nFigure 13: Referring expression comprehension example\n17\nFigure 14: Referring expression comprehension example\nFigure 15: Referring expression comprehension example\n18\nFigure 16: object identification example\nFigure 17: object identification example\nFigure 18: object identification example\n19\nFigure 19: object identification example\n20\n"
  },
  {
    "title": "Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams",
    "link": "https://arxiv.org/pdf/2310.08678.pdf",
    "upvote": "11",
    "text": "Can GPT models be Financial Analysts?\nAn Evaluation of ChatGPT and GPT-4 on mock CFA Exams\nEthan Callanan1,\u2020, Amarachi Mbakwe2,\u2020,\u2021, Antony Papadimitriou3,\u2020, Yulong Pei3,\u2020, Mathieu Sibue3,\u2020,\nXiaodan Zhu1, Zhiqiang Ma3, Xiaomo Liu3, and Sameena Shah3\n1Queen\u2019s University\n2Virginia Tech\n3J.P. Morgan AI Research\n1{e.callanan,xiaodan.zhu}@queensu.ca,\n2bmamarachi@vt.edu,\n3{first.last}@jpmchase.com\nAbstract\nLarge Language Models (LLMs) have demon-\nstrated remarkable performance on a wide\nrange of Natural Language Processing (NLP)\ntasks, often matching or even beating state-of-\nthe-art task-specific models. This study aims\nat assessing the financial reasoning capabili-\nties of LLMs. We leverage mock exam ques-\ntions of the Chartered Financial Analyst (CFA)\nProgram to conduct a comprehensive evalua-\ntion of ChatGPT1 and GPT-42 in financial\nanalysis, considering Zero-Shot (ZS), Chain-\nof-Thought (CoT), and Few-Shot (FS) scenar-\nios.\nWe present an in-depth analysis of the\nmodels\u2019 performance and limitations, and es-\ntimate whether they would have a chance at\npassing the CFA exams. Finally, we outline\ninsights into potential strategies and improve-\nments to enhance the applicability of LLMs in\nfinance. In this perspective, we hope this work\npaves the way for future studies to continue en-\nhancing LLMs for financial reasoning through\nrigorous evaluation.\n1\nIntroduction\nNLP has undergone a profound transformation\ndriven by the emergence of LLMs. Models such as\nLLaMA (Touvron et al., 2023), PaLM (Chowdh-\nery et al., 2022), ChatGPT and GPT-4 from Ope-\nnAI, have garnered significant attention from both\nthe research community and the public thanks to\ntheir impressive dialog style. From text summa-\nrization (Barros et al., 2023; Zhang et al., 2023)\nand code generation (Le et al., 2022; Yang et\nal., 2023a) to question answering (Wang et al.,\n2022; Khashabi et al., 2020), named entity recog-\nnition (Li et al., 2022), and beyond, LLMs have\nshowcased remarkable performance across diverse\ntasks.\n1https://platform.openai.com/docs/models/gpt-3-5\n2https://platform.openai.com/docs/models/GPT-4\n\u2020Equal contribution\n\u2021Work done while interning at J.P. Morgan AI Research\nModel\nSetting\nLevel I\nLevel II\nChatGPT\nZS\n58.8 \u00b1 0.2\n46.6 \u00b1 0.6\nCoT\n58.0 \u00b1 0.2\n47.2 \u00b1 0.3\n2S\n63.0 \u00b1 0.2\n46.6 \u00b1 0.1\n4S\n62.3 \u00b1 0.2\n45.7 \u00b1 0.2\n6S\n62.2 \u00b1 0.2\n47.0 \u00b1 0.3\n10S\n62.4 \u00b1 0.2\n47.6 \u00b1 0.4\nGPT-4\nZS\n73.2 \u00b1 0.2\n57.4 \u00b1 1.5\nCoT\n74.0 \u00b1 0.2\n61.4 \u00b1 0.9\n2S\n73.9 \u00b1 0.1\n60.2 \u00b1 0.9\n4S\n73.8 \u00b1 0.2\n60.5 \u00b1 0.7\n6S\n74.5 \u00b1 0.2\n-\n10S\n74.6 \u00b1 0.2\n-\nTable 1:\nOverall Performance of ChatGPT and\nGPT-4 on Level I and Level II Exams (Accuracy)\nin Zero-Shot (ZS), Chain-of-Thought (CoT), and\nFew-Shot (FS) settings.\nIn finance, NLP has played a pivotal role in\nenhancing various services, such as customer re-\nlations, stock sentiment analysis, financial ques-\ntion answering (Wang et al., 2022), document un-\nderstanding (Kim et al., 2022), and report sum-\nmarization (Abdaljalil and Bouamor, 2021). De-\nspite these advancements, applying NLP in fi-\nnance poses unique challenges, such as the dis-\ntinct nature of financial tasks, linguistic structures,\nand specialized terminology. As a result, the per-\nformance of general NLP models often falls short\nwhen applied to finance-related tasks \u2013 the specific\nchallenges of financial reasoning problems war-\nrant further investigation.\nIn this paper, we rigorously assess the capabil-\nities of LLMs in real-world financial reasoning\nproblems by conducting an evaluation on mock\nexam questions of the prestigious Chartered Fi-\nnancial Analyst (CFA) Program3. The CFA exams\n3https://www.cfainstitute.org/en/\nprograms/cfa/exam\narXiv:2310.08678v1  [cs.CL]  12 Oct 2023\nWhich of the following is most likely an assumption of technical analysis?\nA. Security markets are efficient\nB. Market trends reflect irrational human behavior\nC. Equity markets react quickly to inflection points in the broad economy\n(a) Level I sample question\nParis Rousseau, a wealth manager at a US-based investment management firm, is meeting with a new client. The client has\nasked Rousseau to make recommendations regarding his portfolio\u2019s exposure to liquid alternative investments [...]\n[Table Evidence]\nThe AFFO per share for Autier REIT over the last 12-months is closest to:\nA. $6.80.\nB. $7.16.\nC. $8.43.\n(b) Level II sample question\nFigure 1: CFA example questions (source: CFA Institute); the question appears in bold, the multiple\nchoices in blue and italic, and the vignette/case description in orange and italic\nare known for their meticulous yet practical as-\nsessment of financial expertise, making their reso-\nlution an ideal use case to gauge the capabilities\nof LLMs in handling complex financial reason-\ning scenarios. Our work focuses on two closed-\nsource, non-domain specific LLMs, ChatGPT\nand GPT-4, using various popular prompting\ntechniques. Our contributions are as follows:\n1 We conduct the first comprehensive evalua-\ntion of ChatGPT and GPT-4 in financial\nreasoning problems using CFA mock exam\nquestions, considering ZS, CoT, and FS sce-\nnarios.\n2 We present an in-depth analysis of the mod-\nels\u2019 performance and limitations in solving\nthese financial reasoning problems, and esti-\nmate how they would fare in the Level I and\nLevel II CFA exams.\n3 We outline insights into potential strategies\nand improvements to enhance the applicabil-\nity of LLMs in finance, opening new avenues\nfor research and development.\n2\nRelated Work\n2.1\nLLMs and Finance\nLLMs are Transformer-based generative models\n(Vaswani et al., 2017) trained on massive datasets\nthat cover a broad range of topics and domains.\nPrevious work has demonstrated the ability of\nLLMs to generalize surprisingly well to unseen\ndownstream tasks, with little to no additional\ntraining data (Brown et al., 2020; Wei et al., 2022).\nCode available at https://github.com/e-cal/gpt-cfa\nThis raises an interesting question on the compet-\nitiveness of LLMs with supervised state-of-the-\nart models on specialized domains, such as fi-\nnance. Indeed, the characteristics of most finan-\ncial tasks \u2014 which rely on very specific concepts\nand mathematical formula, frequently leverage di-\nagrams and tables, often need multistep reasoning\nwith calculations \u2014 make finance a challenging\ndomain of application for LLMs. Several paths\nhave been proposed to incorporate or emphasize\ndomain-specific knowledge in LLMs: continued\npre-training (Araci, 2019; Wu et al., 2023) and\nsupervised fine-tuning on new data (Mosbach et\nal., 2023; Yang et al., 2023b), retrieval augmented\ngeneration using a vector database of external\nknowledge (Lewis et al., 2020), etc. However, be-\nfore considering such enhancements, only few pa-\npers have proceeded to extensively benchmark the\nout-of-the-box capabilities of newer instruction-\ntuned LLMs in finance (Li et al., 2023).\n2.2\nEvaluation of LLMs on Human Exams\nand other Benchmarks\nSeveral previous studies have evaluated the per-\nformance of LLMs on different standard exams.\nTests considered include the United States medical\nlicensing exam (Kung et al., 2023), free-response\nclinical reasoning exams (Strong et al., 2023),\ncollege-level scientific exams (Wang et al., 2023),\nthe Bar exam (Katz et al., 2023), the driver\u2019s li-\ncense knowledge test (Rahimi et al., 2023), and\nmore. The crucial contribution that these works\nbring to the scientific community and the industry\nis an in-depth analysis of the strengths and weak-\nnesses of LLMs in realistic domain-specific set-\ntings. Through their conclusions, such investiga-\nLevel I\nLevel II\nTopic\nCalculations\n#Tables\nLen(Prompt)\nCalculations\n#Tables\nLen(Prompt)\nEthics\n0.7%\n0.01\n125\n0.0%\n0.00\n1013\nDerivatives\n20.7%\n0.00\n65\n75.0%\n2.00\n816\nAlternative Investments\n36.4%\n0.06\n85\n66.7%\n2.00\n840\nPortfolio Management\n38.3%\n0.18\n110\n56.3%\n2.13\n1077\nFixed Income\n43.0%\n0.06\n87\n50.0%\n1.45\n779\nEconomics\n50.6%\n0.25\n121\n66.7%\n2.00\n1115\nEquity\n52.5%\n0.19\n112\n45.8%\n1.00\n1053\nCorporate Issuers\n59.3%\n0.28\n120\n44.4%\n1.67\n930\nQuantitative Methods\n70.5%\n0.26\n131\n27.8%\n0.00\n1256\nFinancial Reporting\n57.7%\n0.35\n151\n53.6%\n2.79\n1383\nOverall\n42.4%\n0.17\n116\n45.5%\n1.47\n1058\nTable 2: Question characteristics by topic; percentage of questions requiring calculation, average number\nof table evidence per question, and average prompt length (estimated using the tiktoken Python package)\ntions guide subsequent research and practical use\ncase resolutions in industry.\nFor example, (Wang et al., 2023) evaluated\nChatGPT and GPT-4 on a collection of Physics,\nChemistry, and Math problems, and then con-\ncluded that current LLMs do not deliver satisfac-\ntory performance in complex scientific reasoning\nyet to be reliably leveraged in practice. In con-\ntrast, (Bang et al., 2023) found that ChatGPT out-\nperformed fine-tuned task-specific models on four\ndifferent NLP tasks, thus suggesting ChatGPT\ncould be directly applied to solve industry use\ncases involving these tasks.\nOur paper aims at following the footsteps of (Li\net al., 2023) and delves further into the assess-\nment of the inner financial reasoning abilities of\nChatGPT and GPT-4 to help future industry ap-\nplications.\n3\nDataset\nThe CFA Program is a three-part exam that tests\nthe fundamentals of investment tools, valuing as-\nsets, portfolio management, and wealth planning.\nIt is typically completed by those who want to\nwork in the financial industry with backgrounds in\nfinance, accounting, economics, or business. Suc-\ncessfully completing the CFA Program reflects a\nstrong grasp of fundamental financial knowledge,\nand charterholders are then qualified for roles re-\nlated to investment management, risk manage-\nment, asset management, and more.\nAs mentioned above, the CFA exam is com-\nposed of three levels, each with a specific for-\nmat. Irrespective of the level, each problem from\nthe CFA exam is affiliated to one of ten dis-\ntinct finance topics: Ethics, Quantitative Methods,\nEconomics, Financial Statement Analysis, Corpo-\nrate Issuers, Portfolio Management, Equity Invest-\nments, Fixed Income, Derivatives, and Alterna-\ntive Investments. Level I features a total of 180\nindependent Multiple Choice Questions (MCQs).\nLevel II consists of 22 item sets comprised of vi-\ngnettes (i.e., case descriptions with evidence) and\n88 accompanying MCQs. Finally, Level III com-\nprises a mix of vignette-supported essay questions\nand vignette-supported multiple choice questions.\nTwo main challenges arise when trying to\nbenchmark any model on the CFA exam. Firstly,\nthe CFA Institute refrains from publicly releasing\npast exams taken by registered candidates, making\nthe collection of official questions and answers di-\nrectly from any CFA exam impossible. Secondly,\na significant fraction of the level III item sets ex-\npects plain text responses, which then require the\ncostly intervention of human experts for grading.\nTo circumvent these difficulties, we decide to rely\non mock CFA exams and choose to solely focus\non levels I and II, leaving Level III to future work.\nWe collected a total of five Level I mock exams\nand two Level II mock exams. We share in Figure\n1 example MCQs published by the CFA Institute\nfor Level I and Level II. We ensure each topic is\nrepresented in similar proportions to the original\nCFA sections (Figure 2 and Figure 3 in the Ap-\npendix). Table 2 summarizes important statistics\nabout Level I and Level II problems.\n4\nExperiments\n4.1\nSetup\nThis section outlines the methodology employed\nto assess the financial reasoning abilities of\nChatGPT\nGPT-4\nCategory\nZS\nCoT\n2S\nZS\nCoT\n10S\nEthics\n59.2 \u00b1 0.1\n59.2 \u00b1 1.4\n64.6 \u00b1 0.9\n80.3 \u00b1 0.7\n78.9 \u00b1 0.4\n82.4 \u00b1 0.5\nQuantitative Methods\n53.9 \u00b1 0.2\n50.0 \u00b1 0.8\n59.7 \u00b1 1.0\n78.0 \u00b1 0.7\n76.0 \u00b1 1.1\n76.0 \u00b1 0.8\nEconomics\n68.0 \u00b1 1.1\n63.7 \u00b1 2.5\n68.0 \u00b1 3.9\n74.1 \u00b1 1.9\n73.6 \u00b1 1.2\n76.2 \u00b1 0.6\nFinancial Reporting\n54.0 \u00b1 1.2\n53.4 \u00b1 0.6\n60.1 \u00b1 0.7\n68.2 \u00b1 1.0\n70.8 \u00b1 1.3\n70.0 \u00b1 0.7\nCorporate Issuers\n71.4 \u00b1 5.2\n69.8 \u00b1 4.8\n74.2 \u00b1 4.1\n74.4 \u00b1 4.1\n74.6 \u00b1 6.2\n75.3 \u00b1 4.0\nEquity Investments\n59.4 \u00b1 0.1\n60.9 \u00b1 0.7\n62.5 \u00b1 1.0\n80.3 \u00b1 0.7\n70.5 \u00b1 0.9\n68.8 \u00b1 0.8\nFixed Income\n55.6 \u00b1 1.4\n60.2 \u00b1 0.5\n63.6 \u00b1 0.5\n74.9 \u00b1 2.6\n60.2 \u00b1 0.5\n73.6 \u00b1 0.8\nDerivatives\n61.1 \u00b1 4.1\n68.5 \u00b1 2.1\n73.0 \u00b1 1.5\n90.5 \u00b1 0.8\n93.8 \u00b1 0.7\n96.0 \u00b1 0.5\nAlternative Investments\n60.7 \u00b1 2.4\n60.7 \u00b1 1.9\n62.9 \u00b1 1.1\n75.9 \u00b1 1.1\n77.1 \u00b1 1.0\n72.1 \u00b1 1.3\nPortfolio Management\n58.3 \u00b1 2.8\n48.3 \u00b1 3.6\n61.7 \u00b1 2.4\n63.7 \u00b1 0.6\n71.7 \u00b1 0.9\n79.6 \u00b1 1.4\nOverall\n58.8 \u00b1 0.2\n58.0 \u00b1 0.2\n63.0 \u00b1 0.2\n73.2 \u00b1 0.2\n74.0 \u00b1 0.9\n74.6 \u00b1 0.2\nTable 3: ChatGPT and GPT-4 accuracy on Level I Exams\nChatGPT\nGPT-4\nCategory\nZS\nCoT\n10S\nZS\nCoT\n4S\nEthics\n31.3 \u00b1 7.6\n37.5 \u00b1 9.5\n21.9 \u00b1 4.6\n43.8 \u00b1 1.6\n56.3 \u00b1 1.2\n59.4 \u00b1 1.5\nQuantitative Methods\n44.4 \u00b1 12.0\n55.6 \u00b1 6.5\n54.2 \u00b1 9.3\n66.7 \u00b1 1.1\n66.7 \u00b1 7.4\n72.2 \u00b1 4.3\nEconomics\n66.7 \u00b1 0.0\n58.3 \u00b1 1.4\n62.5 \u00b1 1.9\n41.7 \u00b1 1.4\n58.3 \u00b1 6.3\n50.0 \u00b1 6.9\nFinancial Reporting\n39.6 \u00b1 3.4\n31.3 \u00b1 2.0\n44.8 \u00b1 2.5\n54.2 \u00b1 3.9\n66.7 \u00b1 4.2\n63.5 \u00b1 3.3\nCorporate Issuers\n55.6 \u00b1 3.7\n50.0 \u00b1 2.8\n50.0 \u00b1 1.9\n77.8 \u00b1 0.9\n77.8 \u00b1 0.6\n80.6 \u00b1 1.3\nEquity Investments\n60.4 \u00b1 1.6\n60.4 \u00b1 9.9\n60.9 \u00b1 7.0\n65.0 \u00b1 5.7\n58.8 \u00b1 7.3\n62.5 \u00b1 4.7\nFixed Income\n38.9 \u00b1 0.9\n27.8 \u00b1 6.5\n34.4 \u00b1 1.9\n60.0 \u00b1 5.8\n62.2 \u00b1 0.8\n53.9 \u00b1 1.9\nDerivatives\n50.0 \u00b1 5.6\n58.3 \u00b1 12.5\n47.9 \u00b1 3.1\n66.7 \u00b1 5.6\n58.3 \u00b1 0.7\n50.0 \u00b1 4.2\nAlternative Investments\n33.3 \u00b1 0.0\n33.3 \u00b1 0.0\n58.3 \u00b1 0.7\n66.7 \u00b1 0.0\n50.0 \u00b1 0.0\n75.0 \u00b1 0.7\nPortfolio Management\n47.2 \u00b1 0.9\n66.7 \u00b1 8.3\n59.7 \u00b1 9.5\n36.1 \u00b1 1.6\n55.6 \u00b1 0.6\n56.9 \u00b1 4.3\nOverall\n46.6 \u00b1 0.6\n47.2 \u00b1 0.3\n47.6 \u00b1 0.4\n57.4 \u00b1 1.5\n61.4 \u00b1 0.9\n60.5 \u00b1 0.7\nTable 4: ChatGPT and GPT-4 accuracy on Level II Exams\nChatGPT and GPT-4 using mock CFA ex-\nams.\nOur study examined various prompting\nparadigms.\nZS prompting: We gauged the models\u2019 inherent\nreasoning abilities without providing any correct\nexamples in the input.\nFS prompting: We furnished the models with\nprior examples of expected behavior to facilitate\nthe acquisition of new knowledge that could aid\nin solving the questions.\nWe tested two differ-\nent strategies to select FS examples: (a) randomly\nsampling from the entire set of questions within\nthe exam level (2S, 4S and 6S), and (b) sam-\npling one question from each topic in the exam\nlevel (10S). This last approach aims at enabling\nthe models to discern the distinct attributes of each\ntopic within every exam level. Due to the lim-\nited context window of GPT-4 and the length of\nthe Level II item-sets (case description and ques-\ntion), 6S and 10S prompting were not evaluated\nfor GPT-4 on the Level II mock exams.\nCoT prompting: For each exam level, we also\nevaluated the models by prompting them to think\nthrough the input problem step-by-step and show\ntheir work for calculations (also known as ZS\nCoT) (Wei et al., 2022). This has the added benefit\nof allowing us to analyze the models\u2019 \"problem-\nsolving process\" and thus determine where and\nwhy it might have gone wrong.\nImplementation Details: We conducted the ex-\nperiments using the OpenAI ChatCompletion API\n(gpt-3.5-turbo and gpt-4 models) with\nfunctions and set the temperature parameter to\nzero, thereby eliminating randomness in the mod-\nels\u2019 generations. The prompt templates we crafted\nfor each level and for each prompting setting can\nbe found in the Appendix. We employed a memo-\nrization test as in (K\u0131c\u0131man et al., 2023) to confirm\nthat the models had not memorized the mock ex-\nams as part of their training data.\nMetrics: To measure the performance of LLMs on\nthe mock exam MCQs, we compared their predic-\ntions against the established solution set of each of\nthe CFA mock exams collected. Accuracy served\nas our sole evaluation metric throughout this study.\n4.2\nResults Overview\nLLMs struggle more on Level II than on\nLevel I: We notice that, no matter the prompting\nparadigm employed, both ChatGPT and GPT-4\nencounter more difficulties correctly answering\nthe item-sets from Level II than the independent\nquestions from Level I (Table 3, Table 4). While\nthere is no general consensus as to which level is\nusually considered harder for exam takers, we sug-\ngest that three factors might have negatively af-\nfected the performance of LLMs in Level II based\non our analysis.\nFirstly, the case description attached to each\nitem-set from Level II increases the length of the\ninput prompt and dilutes the useful information it\ncontains. Indeed, we observe that Level II prompts\nare on average \u223c10\u00d7 longer than Level I prompts;\nconfronting Table 2, Table 3, Table 4 shows that\ntopics associated with poor performance usually\npresent longer contexts both in Level I and Level\nII. In addition, the detailed case descriptions from\nLevel II depict realistic day-to-day situations that\ncontrast with the more general questions from\nLevel I: LLMs thus need to abstract from case-\nspecific details in Level II questions so as to iden-\ntify the underlying finance concepts involved.\nSecondly, as Level II questions are grouped into\nitem-sets, each item tends to go more in-depth\nabout a specific finance topic than the questions\nthat compose Level I, thus leading to more spe-\ncialized and intricate problems.\nLastly, the Level II section features a slightly\nhigher proportion of questions requiring calcula-\ntions and a much higher proportion of questions\ncontaining table evidence, in comparison to Level\nI (Table 2). Given the known limitations of LLMs\nfor out-of-the-box numerical and table reasoning\n(Frieder et al., 2023; Chen et al., 2022), this could\nalso explain the lower accuracy observed in Level\nII across the board.\nGPT-4 outperforms ChatGPT in almost all ex-\nperiments, but certain finance topics remain\nchallenging for both models: As shown in Ta-\nble 3 and Table 4, GPT-4 consistently beats\nChatGPT in all topics in Level I and most topics\nin Level II, irrespective of the learning paradigm.\nIn Level I, we see that both LLMs perform best\nin the Derivatives, Alternative Investments, Cor-\nporate Issuers, Equity Investments, and Ethics top-\nics. For Derivatives and Ethics, this observation\ncan be explained by the low amount of calcula-\ntions and table understanding required to answer\ncorrectly (Table 2). The explicit mention of pop-\nular finance notions in the questions of Deriva-\ntives and Ethics (e.g., options, arbitrage, etc.) fur-\nther reduces their difficulty too. Similarly, in Al-\nternative Investments, Corporate Issuers, and Eq-\nuity Investments, problems often directly refer\nto well-known finance concepts that might have\nbeen encountered by ChatGPT and GPT-4 dur-\ning pretraining or instruction-tuning \u2013 thus facil-\nitating their resolution despite having more cal-\nculations involved. However, both models show\nrelatively poor performance in the Financial Re-\nporting and Portfolio Management topics in Level\nI, with ChatGPT also struggling a lot more on\nhighly computational topics such as Quantitative\nMethods. Indeed, Portfolio Management and Fi-\nnancial Reporting problems are more case-based,\napplied, computational, and CFA-specific than the\nones from the aforementioned topics, which might\nhave negatively affected performance. They also\ntend to include more table evidence and complex\ndetails to leverage (Table 2).\nIn Level II, we observe that both ChatGPT\nand GPT-4 still perform relatively strongly on\nDerivatives, Corporate Issuers, and Equity In-\nvestments, yet still relatively poorly on Financial\nReporting.\nHowever, the results are now more\nnuanced:\nChatGPT struggles on Alternative\nInvestments and Fixed Income compared to\nGPT-4, while ChatGPT outperforms GPT-4 in\nPortfolio Management and Economics. Interest-\ningly enough, both models now demonstrate low\nanswer accuracy in the Ethics item-sets of Level\nII. This could originate from the more in-depth,\nsituational, and detailed character of the problems\nfrom Level II in comparison to Level I.\nCoT prompting yields limited improvements\nover ZS: Although CoT performs better than ZS\nin almost all cases and better than FS in Level II\nfor GPT-4, we note that the use of CoT did not\nhelp LLMs as much as we initially expected (Ta-\nble 1, Table 3, Table 4). In Level I, CoT prompt-\ning hardly benefits GPT-4 (bringing in just a 1%\nrelative increase) and actually deteriorates the per-\nformance of ChatGPT. In Level II, CoT prompt-\ning yields a decent 7% relative improvement over\nZS prompting for GPT-4, but a disappointing 1%\nfor ChatGPT. Section 5.1 further investigates the\nreasons explaining such observations. In Level I,\nwe see that CoT negatively affected both LLMs\nparticularly in Quantitative Methods, which could\nbe due to hallucinations in mathematical formula\nand calculations. In Level II, we notice that CoT\nbenefited both LLMs in the Ethics and Portfolio\nManagement topics, where explicit step-by-step\nreasoning over long and intricate evidence is usu-\nally helpful.\nIn both levels, we also noted that\nCoT prompting sometimes led to inconsistent per-\nformance across questions from the same topic,\nas manifested by the high standard deviations re-\nported in Table 3 and Table 4.\nHowever, despite the aforementioned observa-\ntions, it is hard to clearly identify more topics that\nsystematically benefit or suffer from the use of\nCoT for both models across levels. For instance,\nin Financial Reporting problems from Level II,\nGPT-4 saw its accuracy improve by 23% with\nCoT relative to ZS, while ChatGPT saw its\nperformance decrease by 21% (Table 4).\nA few in-context examplars help more than\nCoT: Compared to ZS and CoT prompting,\nFS prompting offers significant performance im-\nprovements for ChatGPT on the Level I mock ex-\nams (Table 1). 2S prompting yielded the best per-\nformance across all categories and overall in Level\nI for ChatGPT. Across mock exams in Level II,\nthe dominance is not as significant, but FS prompt-\ning still manages to achieve the best overall score\nfor both models, with the exception of Level II for\nGPT-4 (Table 3, Table 4). Interestingly, for Level\nII, the best FS prompting type was 10S prompting\nfor ChatGPT, which suggests more complex ex-\nams benefited from a more holistic FS approach\nacross multiple topics. The overall trend shown in\nthe results is that FS prompting seems to offer bet-\nter assistance to less complex models (ChatGPT)\nwhen being tested on seemingly simpler exams\n(Level I).\nIt is likely that FS yields better performance im-\nprovement than CoT because it shows actual cor-\nrect answers to different types of mock questions.\nIt also enables the models to understand how to\nbest use the table evidence or other information\ncontained in a question (if any). The compara-\ntively lower performance improvement brought by\nFS observed in Level II mock exams may be due\nto the more complex nature of the questions and\nthe fact they include case studies; it may be a sce-\nnario where simply prompting the models with the\ncorrect answers is not sufficient.\nLevel II may\nthus benefit from a combination of FS and CoT\nprompting with clear explanations as to how the\ninformation in the case study was leveraged to ar-\nrive at the correct answer.\n5\nDetailed Analysis\n5.1\nUnderperformance of CoT on Level I\nIt was surprising to see that CoT only marginally\nimproved the models\u2019 performance on each test,\nand was actually slightly detrimental to the per-\nformance of ChatGPT on the Level I exams. To\ninspect the nature of the errors made by the mod-\nels when using CoT prompting, we looked over\neach instance where no-CoT was correct while\nCoT was incorrect, and categorized the error as\none of: Knowledge, Reasoning, Calculation, or In-\nconsistency.\nKnowledge errors are those where the model\nlacks critical knowledge required to answer the\nquestion. This includes an incorrect understand-\ning of some concept, not knowing the relationship\nbetween concepts, or using an incorrect formula\nto answer a question requiring calculation. Rea-\nsoning errors are when the model had all the cor-\nrect knowledge, but either over-reasoned in its re-\nsponse, or hallucinated some additional require-\nments or information in the question that was not\nactually present. Calculation errors are errors per-\ntaining to some incorrect calculation (using a cor-\nrect formula), or failing to accurately compare\nor convert results.\nErrors of inconsistency are\nwhen the model\u2019s thinking is entirely correct, yet\nit chooses the wrong answer.\nType of Error\nChatGPT\nGPT-4\nKnowledge\n55.2%\n50.0%\nReasoning\n8.6%\n10.7%\nCalculation\n17.2%\n28.6%\nInconsistency\n19.0%\n10.7%\nTable 5:\nError modes of level I questions\nChatGPT and GPT-4 got correct without CoT\nbut incorrect using CoT\nChatGPT: By far the most common error mode\nfor ChatGPT is knowledge based, constituting\nover half of all errors VS. no-CoT. This implies\nthat, with CoT reasoning, the gaps in the LLMs\ninternal knowledge are magnified. As the model\nbegins to think through its answer, it states its in-\ncorrect assumptions, which it proceeds to rational-\nize in the context of the question thereby skew-\ning the rest of the answer towards a wrong choice.\nWithout using CoT reasoning, the model is able\nto make an \"educated guess\" where any incorrect\nknowledge has less of an opportunity of skewing\nthe guess towards an incorrect answer. With a 1/3\nchance of guessing correctly, plus any contextual\nhints that may lie in the question, for questions\nwhere GPT simply lacks the knowledge to reason\ncorrectly, guessing is a more accurate strategy.\nThis same principal similarity explains calcula-\ntion and reasoning errors, where one or a few off-\ntrack token generations then throw off the rest of\nthe answer, resulting in an incorrect conclusion.\nThe instances where the model is entirely cor-\nrect but then concludes or just selects the wrong\nanswer are more enigmatic. In about half of these\ncases, it seems to fail to generate a stop token upon\ncoming to the conclusion, leading it to restate the\nconcluding sentence with another option selected.\nIn the other cases, there appears to be some dis-\nconnect between the thought process and the an-\nswer selection. As we were using OpenAI\u2019s func-\ntions API to retrieve structured output, our leading\nsuspicion is that in these cases the ordering out-\nlined in the system prompt was missed or ignored,\nand the answer was generated first.\nGPT-4: There were about half as many instances\nof CoT making an error not made without CoT for\nGPT-4, compared to ChatGPT. On these ques-\ntions, GPT-4 also displays knowledge errors as\nthe most common error mode. However, unlike\nChatGPT, almost none of these knowledge er-\nrors were using the incorrect formula. This, along\nwith the fact that there were less knowledge er-\nrors in total, shows that GPT-4 has more complete\ninternal knowledge of both financial information\nand especially financial formulas and calculation\nmethods. Rather than knowledge errors, GPT-4\u2019s\nmost common error mode on questions requiring\ncalculation are calculation errors. ChatGPT also\nfrequently made these sorts of errors in conjunc-\ntion with using the wrong formula, which under-\nlines the well-known and more foundational short-\ncoming of language models\u2019 mathematical abili-\nties (Frieder et al., 2023).\nGPT-4 also displayed far fewer inconsistency\nerrors than ChatGPT. It appears to have a much\nstronger ability to connect its reasoning to the an-\nswers and to make comparisons.\nThe one er-\nror type that GPT-4 makes more frequently than\nChatGPT was reasoning errors. It would seem\nthat, along with GPT-4\u2019s greater ability to reason,\nit has a greater chance of \"talking itself\" into in-\ncorrect lines of reasoning.\nType of Error\nChatGPT\nGPT-4\nKnowledge\n70%\n80%\nReasoning\n20%\n20%\nOut of Tokens\n10%\n0%\nTable 6:\nError modes of level II questions\nChatGPT and GPT-4 got correct without CoT\nbut incorrect using CoT\n5.2\nCoT Benefits on Level II\nIf CoT amplifies the effect of missing knowledge,\nand allows LLMs room to miscalculate or \"talk\nthemselves\" into a wrong answer, one might ques-\ntion why it seemed to help much more on Level II\nexams. The Level II exam questions require more\ninterpretation of the information, as one needs to\nfigure out what is relevant from the case, and some\ninformation may be missing but is expected to be\nknown and needed to answer the question. Using\nCoT helps the model to reason over the informa-\ntion and filter what is relevant to the question from\nthe case.\n5.3\nCan LLMs pass the CFA exam?\n5.3.1\nCFA Level I Passing Score\nThe CFA Institute refrains from disclosing the\nminimum passing score (MPS) for its examina-\ntions, thereby giving rise to an entire industry\ncentered around speculating on the elusive actual\nMPS. The MPS is uniquely established for each\nindividual exam, guided by the standards that the\nCFA Institute established back in 2011.\nThe CFA Institute employs the \u2018Angoff Stan-\ndard Setting Method\u2019 to ascertain the pass rates for\nCFA exams. This involves a group of CFA Char-\nterholders convening to collectively assess the true\ndifficulty level of the questions and the appropriate\nlevel of ease that should accompany passing each\nquestion.\nChatGPT\nGPT-4\nExam\nZS\nCoT\nFS\nZS\nCoT\nFS\nLevel I\nPass\nFail\nPass\nPass\nPass\nPass\nLevel II\nFail\nFail\nFail\nUnclear\nPass\nPass\nTable 7: ChatGPT and GPT-4 ability to pass Level I and Level II Exams\nAlthough the CFA Institute maintains an air of\nsecrecy surrounding its pass/fail thresholds, cer-\ntain indicators point towards a potential elevation\nof the MPS for CFA Level I. Drawing from feed-\nback provided by CFA exam takers on Reddit, the\naverage MPS stood at 65% in December 2019, but\nsurged to 71.1% by February 2021. In June 2019,\nestimations suggest that certain individuals man-\naged to pass CFA Level I with a mere 60.8%; by\nFebruary 2021, this had escalated to 66.7%.\nAiming for approximately 70% in as many sub-\njects as possible seems to be a prudent strategy\nfor clearing CFA Level I. Put differently, attain-\ning scores above 70% in all topics is not a ne-\ncessity for passing. Some contend that achieving\nas low as 65% or even 63% might suffice. Re-\nmarkably, one doesn\u2019t even need to exceed 51%\nin every area to secure a passing grade. The pat-\ntern appears to allow for the possibility of scoring\nbelow 50% in about three, or perhaps four, sub-\njects. However, this would likely necessitate coun-\nterbalancing with scores exceeding 70% in at least\nthree subjects and falling between 51% and 70%\nin the remaining ones. Nevertheless, maintaining\nan average score of 70% across subjects consider-\nably enhances the likelihood of a positive outcome\nupon receiving the results. 4\n5.3.2\nCFA Level II Passing Score\nThe estimations from the Reddit community re-\ngarding the MPS for CFA Levels II and III are\neven more outdated than those for Level I, yet they\nindicate that the two advanced exams have consis-\ntently featured lower passing thresholds. In June\n2019, their approximations pegged the MPS for\nLevel III at a mere 57.4%, and for Level II at just\n62.8%. The subject level passing scores are am-\nbiguous for the Level II exam, but we can attempt\nto apply the same logic as the Level I exam but\n4https://www.efinancialcareers.com.au/\nnews/finance/whats-the-minimum-score-\nyou-can-get-on-cfa-level-i-and-still-\npass\nmake an assumption that threshold for each sub-\nject is 60% instead of 70%.5\n5.3.3\nProposed pass criteria\nGiven the above information our proposed pass\ncriteria is as follows:\n\u2022 Level I - achieve a score of at least 60% in\neach topic and an overall score of at least 70%\n\u2022 Level II - achieve a score of at least 50% in\neach topic and an overall score of at least 60%\nTable 7 shows which model implementations\nwere able to pass the exams. The FS implemen-\ntations in both settings correspond to the number\nof shots shown in Table 3 and Table 4. Most of\nthe settings showed a clear pass or fail except for\nGPT-4 ZS on Level II which was a borderline de-\ncision either way. GPT-4 in a ZS setting attains\na score of >60% in six of the topics and achieves\na score of between 50% and 60% in one of the\ntopics. The topic performance seems high but the\noverall score of 57.39% falls slightly short of the\nminimum passing score proposed earlier, it is thus\nunclear as to whether this LLM setting would pass\nthe CFA Level II exam.\n6\nConclusion and Discussion\nIn this paper, we have conducted a thorough eval-\nuation of ChatGPT and GPT-4 on the CFA level\nI and level II exams. We observed that GPT-4\nperformed better than ChatGPT in almost every\ntopic of both levels when using the same prompt-\ning method. Based on estimated pass rates and\naverage self-reported scores, we concluded that\nChatGPT would likely not be able to pass the\nCFA level I and level II under all tested settings,\nwhile GPT-4 would have a decent chance of pass-\ning the CFA Level I and Level II if prompted with\nFS and/or CoT.\n5https://www.efinancialcareers.com.au/\nnews/finance/whats-the-minimum-score-\nyou-can-get-on-cfa-level-i-and-still-\npass\nWe noted that CoT prompting provided little\nimprovement for ChatGPT on both exams and\nGPT-4 on the Level I exam. While CoT prompt-\ning did help the models reason and understand\nthe question and information better, it also ex-\nposed them to making errors due to incorrect/miss-\ning domain specific knowledge as well as reason-\ning and calculation errors. Additionally, we no-\nticed that FS helped LLMs the most in both Levels\nthanks to the integration of positive instances into\nthe prompt, yielding the best performance in most\ncases.\nWith these observations in mind, we propose\nfuture systems that could display greater perfor-\nmance by utilizing various tools. The most preva-\nlent error mode of CoT, knowledge errors, could\nbe addressed through retrieval-augmented gener-\nation using an external knowledge base contain-\ning CFA-specific information. Calculation errors\ncould be avoided by offloading calculations to a\nfunction or API such as Wolfram Alpha. The re-\nmaining error modes, reasoning and inconsistency,\ncould be reduced by employing a critic model to\nreview and second guess the thinking before sub-\nmitting the answer, or combining FS and CoT to-\ngether to give richer examples of expected behav-\nior. We hope this work paves the way for future\nstudies to continue enhancing LLMs for financial\nreasoning problems through rigorous evaluation.\nAcknowledgments\nThis research was funded in part by the Faculty\nResearch Awards of J.P. Morgan AI Research. The\nauthors are solely responsible for the contents of\nthe paper and the opinions expressed in this publi-\ncation do not reflect those of the funding agencies.\nDisclaimer This paper was prepared for infor-\nmational purposes by the Artificial Intelligence\nResearch group of JPMorgan Chase & Co and its\naffiliates (\u201cJP Morgan\u201d), and is not a product of\nthe Research Department of JP Morgan. JP Mor-\ngan makes no representation and warranty whatso-\never and disclaims all liability, for the complete-\nness, accuracy or reliability of the information\ncontained herein. This document is not intended\nas investment research or investment advice, or a\nrecommendation, offer or solicitation for the pur-\nchase or sale of any security, financial instrument,\nfinancial product or service, or to be used in any\nway for evaluating the merits of participating in\nany transaction, and shall not constitute a solici-\ntation under any jurisdiction or to any person, if\nsuch solicitation under such jurisdiction or to such\nperson would be unlawful.\nReferences\n[Abdaljalil and Bouamor2021] Samir\nAbdaljalil\nand\nHouda Bouamor.\n2021.\nAn exploration of au-\ntomatic text summarization of financial reports.\nIn Proceedings of the Third Workshop on Finan-\ncial Technology and Natural Language Processing,\npages 1\u20137.\n[Araci2019] Dogu Araci. 2019. Finbert: Financial sen-\ntiment analysis with pre-trained language models.\narXiv preprint arXiv:1908.10063.\n[Bang et al.2023] Yejin Bang, Samuel Cahyawijaya,\nNayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie,\nHoly Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung,\nQuyet V. Do, Yan Xu, and Pascale Fung.\n2023.\nA multitask, multilingual, multimodal evaluation of\nchatgpt on reasoning, hallucination, and interactiv-\nity.\n[Barros et al.2023] Thierry S Barros, Carlos Eduardo S\nPires, and Dimas Cassimiro Nascimento.\n2023.\nLeveraging bert for extractive text summarization on\nfederal police documents. Knowledge and Informa-\ntion Systems, pages 1\u201331.\n[Brown et al.2020] Tom Brown, Benjamin Mann, Nick\nRyder, Melanie Subbiah, Jared D Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, et al. 2020. Lan-\nguage models are few-shot learners.\nAdvances in\nneural information processing systems, 33:1877\u2013\n1901.\n[Chen et al.2022] Zhiyu Chen, Shiyang Li, Charese\nSmiley,\nZhiqiang\nMa,\nSameena\nShah,\nand\nWilliam Yang Wang. 2022. Convfinqa: Exploring\nthe chain of numerical reasoning in conversational\nfinance question answering.\n[Chowdhery et al.2022] Aakanksha Chowdhery, Sha-\nran Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won\nChung, Charles Sutton, Sebastian Gehrmann, et al.\n2022. Palm: Scaling language modeling with path-\nways.\n[Frieder et al.2023] Simon\nFrieder,\nLuca\nPinchetti,\nAlexis Chevalier, Ryan-Rhys Griffiths, Tommaso\nSalvatori, Thomas Lukasiewicz, Philipp Christian\nPetersen, and Julius Berner. 2023. Mathematical\ncapabilities of chatgpt.\n[Katz et al.2023] Daniel Martin Katz, Michael James\nBommarito, Shang Gao, and Pablo Arredondo.\n2023. Gpt-4 passes the bar exam. Available at SSRN\n4389233.\n[Khashabi et al.2020] Daniel Khashabi, Sewon Min,\nTushar Khot, Ashish Sabharwal, Oyvind Tafjord,\nPeter Clark, and Hannaneh Hajishirzi. 2020. Uni-\nfiedqa: Crossing format boundaries with a single qa\nsystem.\n[Kim et al.2022] Geewook Kim, Teakgyu Hong, Moon-\nbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong\nYim, Wonseok Hwang, Sangdoo Yun, Dongyoon\nHan, and Seunghyun Park.\n2022.\nOcr-free doc-\nument understanding transformer.\nIn European\nConference on Computer Vision, pages 498\u2013517.\nSpringer.\n[Kung et al.2023] TH Kung, M Cheatham, A Mede-\nnilla, C Sillos, L De Leon, C Elepa\u00f1o, et al. 2023.\nPerformance of chatgpt on usmle:\nPotential for\nai-assisted medical education using large language\nmodels. plos digit health 2 (2): e0000198.\n[K\u0131c\u0131man et al.2023] Emre\nK\u0131c\u0131man,\nRobert\nNess,\nAmit Sharma, and Chenhao Tan. 2023. Causal rea-\nsoning and large language models: Opening a new\nfrontier for causality.\n[Le et al.2022] Hung Le, Yue Wang, Akhilesh Deepak\nGotmare, Silvio Savarese, and Steven Chu Hong\nHoi.\n2022.\nCoderl: Mastering code generation\nthrough pretrained models and deep reinforcement\nlearning. Advances in Neural Information Process-\ning Systems, 35:21314\u201321328.\n[Lewis et al.2020] Patrick Lewis, Ethan Perez, Alek-\nsandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-\ntau Yih, Tim Rockt\u00e4schel, et al. 2020. Retrieval-\naugmented generation for knowledge-intensive nlp\ntasks. Advances in Neural Information Processing\nSystems, 33:9459\u20139474.\n[Li et al.2022] Jingye\nLi,\nHao\nFei,\nJiang\nLiu,\nShengqiong Wu, Meishan Zhang, Chong Teng,\nDonghong Ji, and Fei Li. 2022. Unified named en-\ntity recognition as word-word relation classification.\n[Li et al.2023] Xianzhi Li, Xiaodan Zhu, Zhiqiang Ma,\nXiaomo Liu, and Sameena Shah. 2023. Are chatgpt\nand gpt-4 general-purpose solvers for financial text\nanalytics? an examination on several typical tasks.\narXiv preprint arXiv:2305.05862.\n[Mosbach et al.2023] Marius\nMosbach,\nTiago\nPi-\nmentel, Shauli Ravfogel, Dietrich Klakow, and\nYanai Elazar.\n2023.\nFew-shot fine-tuning vs. in-\ncontext learning: A fair comparison and evaluation.\narXiv preprint arXiv:2305.16938.\n[Rahimi et al.2023] Saba Rahimi, Tucker Balch, and\nManuela Veloso.\n2023.\nExploring the effective-\nness of gpt models in test-taking: A case study of\nthe driver\u2019s license knowledge test. arXiv preprint\narXiv:2308.11827.\n[Strong et al.2023] Eric Strong, Alicia DiGiammarino,\nYingjie\nWeng,\nPreetha\nBasaviah,\nPoonam\nHosamani, Andre Kumar, Andrew Nevins, John\nKugler, Jason Hom, and Jonathan Chen.\n2023.\nPerformance of chatgpt on free-response, clinical\nreasoning exams. medRxiv, pages 2023\u201303.\n[Touvron et al.2023] Hugo Touvron, Thibaut Lavril,\nGautier Izacard,\nXavier Martinet,\nMarie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Na-\nman Goyal, Eric Hambro, Faisal Azhar, et al. 2023.\nLlama:\nOpen and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971.\n[Vaswani et al.2017] Ashish Vaswani, Noam Shazeer,\nNiki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017.\nAttention is all you need. Advances in neural infor-\nmation processing systems, 30.\n[Wang et al.2022] Bin Wang, Jiangzhou Ju, Yunlin\nMao, Xin-Yu Dai, Shujian Huang, and Jiajun Chen.\n2022.\nA numerical reasoning question answering\nsystem with fine-grained retriever and the ensem-\nble of multiple generators for finqa. arXiv preprint\narXiv:2206.08506.\n[Wang et al.2023] Xiaoxuan Wang, Ziniu Hu, Pan Lu,\nYanqiao Zhu, Jieyu Zhang, Satyen Subramaniam,\nArjun R Loomba, Shichang Zhang, Yizhou Sun, and\nWei Wang.\n2023.\nScibench: Evaluating college-\nlevel scientific problem-solving abilities of large lan-\nguage models. arXiv preprint arXiv:2307.10635.\n[Wei et al.2022] Jason Wei, Xuezhi Wang, Dale Schu-\nurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V\nLe, Denny Zhou, et al.\n2022.\nChain-of-thought\nprompting elicits reasoning in large language mod-\nels.\nAdvances in Neural Information Processing\nSystems, 35:24824\u201324837.\n[Wu et al.2023] Shijie Wu, Ozan Irsoy, Steven Lu,\nVadim\nDabravolski,\nMark\nDredze,\nSebastian\nGehrmann, Prabhanjan Kambadur, David Rosen-\nberg, and Gideon Mann. 2023. Bloomberggpt: A\nlarge language model for finance.\narXiv preprint\narXiv:2303.17564.\n[Yang et al.2023a] Guang Yang, Yu Zhou, Xiang Chen,\nXiangyu Zhang, Tingting Han, and Taolue Chen.\n2023a.\nExploitgen: Template-augmented exploit\ncode generation based on codebert. Journal of Sys-\ntems and Software, 197:111577.\n[Yang et al.2023b] Hongyang Yang, Xiao-Yang Liu,\nand Christina Dan Wang.\n2023b.\nFingpt: Open-\nsource financial large language models.\narXiv\npreprint arXiv:2306.06031.\n[Zhang et al.2023] Haopeng Zhang, Xiao Liu, and Ji-\nawei Zhang. 2023. Extractive summarization via\nchatgpt for faithful summary generation.\narXiv\npreprint arXiv:2304.04193.\nAppendix\nA\nTopic Distribution in each Level\nFigure 2: Level I exam topic distribution\nFigure 3: Level II exam topic distribution\nB\nPrompt templates used\nB.1\nLevel I\nListing 1: ZS\nSYSTEM: You are a CFA (chartered\nfinancial analyst) taking a test to\nevaluate your knowledge of finance.\nYou will be given a question along\nwith three possible answers (A, B,\nand C).\nIndicate the correct answer (A, B, or\nC).\nUSER: Question:\n{question}\nA. {choice_a}\nB. {choice_b}\nC. {choice_c}\nListing 2: CoT\nSYSTEM: You are a CFA (chartered\nfinancial analyst) taking a test to\nevaluate your knowledge of finance.\nYou will be given a question along\nwith three possible answers (A, B,\nand C).\nBefore answering, you should think\nthrough the question step-by-step.\nExplain your reasoning at each step\ntowards answering the question. If\ncalculation is required, do each\nstep of the calculation as a step\nin your reasoning.\nIndicate the correct answer (A, B, or\nC).\nUSER: Question:\n{question}\nA. {choice_a}\nB. {choice_b}\nC. {choice_c}\nListing 3: FS (2S example)\nSYSTEM: You are a CFA (chartered\nfinancial analyst) taking a test to\nevaluate your knowledge of finance.\nYou will be given a question along\nwith three possible answers (A, B,\nand C).\nIndicate the correct answer (A, B, or\nC).\nUSER: Question:\n{question}\nA. {choice_a}\nB. {choice_b}\nC. {choice_c}\nASSISTANT: {answer}\nUSER: Question:\n{question}\nA. {choice_a}\nB. {choice_b}\nC. {choice_c}\nASSISTANT: {answer}\nUSER: Question:\n{question}\nA. {choice_a}\nB. {choice_b}\nC. {choice_c}\nB.2\nLevel II\nFor Level II, the case description of each item-set\nwas inserted before each question from the user.\n"
  },
  {
    "title": "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model",
    "link": "https://arxiv.org/pdf/2310.09520.pdf",
    "upvote": "10",
    "text": "Reward-Augmented Decoding: Efficient Controlled Text Generation\nWith a Unidirectional Reward Model\nHaikang Deng\nUNC-Chapel Hill\nfrankdenghaikang@gmail.com\nColin Raffel\nUniversity of Toronto, Vector Institute\ncraffel@gmail.com\nAbstract\nWhile large language models have proven ef-\nfective in a huge range of downstream appli-\ncations, they often generate text that is prob-\nlematic or lacks a desired attribute. In this\npaper, we introduce Reward-Augmented De-\ncoding (RAD), a text generation procedure that\nuses a small unidirectional reward model to en-\ncourage a language model to generate text that\nhas certain properties. Specifically, RAD uses\nthe reward model to score generations as they\nare produced and rescales sampling probabil-\nities to favor high-reward tokens. By using a\nunidirectional reward model, RAD can cache\nactivations from prior generation steps to de-\ncrease computational overhead. Through exper-\niments on generating non-toxic and sentiment-\ncontrolled text, we demonstrate that RAD per-\nforms best among methods that change only the\ngeneration procedure and matches the perfor-\nmance of state-of-the-art methods that involve\nre-training the language model. We further val-\nidate that RAD is effective on very large lan-\nguage models while incurring a minimal com-\nputational overhead.\n1\nIntroduction\nLarge language models (LLMs, Rae et al., 2021;\nHoffmann et al., 2022; Scao et al., 2022; Touvron\net al., 2023) are seeing widespread adoption thanks\nto the fact that they can perform many language\ntasks and generate coherent long-form text. As\nLLMs are deployed in situations where they in-\nteract with humans, it can be beneficial to control\nthe language model so that it generates text with\ncertain properties (Sudhakar et al., 2019) \u2013 for ex-\nample, we might desire generations that are unbi-\nased, non-toxic, and helpful. In addition, we may\nwant models to output text with specific proper-\nties, such as having a positive sentiment, a certain\nwriting style, etc. Typically, LLMs pre-trained on\nuncurated large-scale text corpora can generate text\nthat does not have these desired attributes (Wallace\nLanguage Model\nUncle Bob\nBob\nloves\nis\nhas\nReward Model\nUncle\nBob\nloves\nFigure 1: Reward-Augmented Decoding (RAD). RAD\nsteers a language model towards generating text that is\nassigned a high reward by an auxiliary reward model.\nBlue/red boxes in the reward model correspond to\ncached/newly computed hidden states.\net al., 2019; Gehman et al., 2020), which motivates\nthe need for techniques that enable controllable\ntext generation. Such techniques can be seen as\nproviding a means to condition text generation on\na desired attribute.\nA straightforward way to control the text gener-\nated by an LLM is to perform additional training\non data that has desired properties (Gururangan\net al., 2020). Alternatively, an LLM can be trained\nwith \u201ccontrol codes\u201d (Keskar et al., 2019; Lu et al.,\n2022) that indicate text characteristics and can be\nused to induce the LLM to generate content with\nthose characteristics. If available, annotated human\npreferences can be used to train a reward model\nthat is then used to train a language model with\nreinforcement learning (Ouyang et al., 2022; Kim\net al., 2023). A drawback of these methods is that\nthey can degrade performance on text that is dif-\nferent from the data used for additional training.\nBesides, work done to control one language model\ncannot be reused to control another language model.\nMoreover, the additional training cost can be pro-\nhibitively expensive, especially for very large mod-\nels.\narXiv:2310.09520v4  [cs.CL]  2 Jan 2024\nOne way to avoid the cost and shortcomings\nof additional training is to instead modify the de-\ncoding procedure used to generate text from a lan-\nguage model (Chaffin et al., 2022). For example,\nweighted decoding modifies the probabilities as-\nsigned to each token during decoding using an\nauxiliary model. Most weighted decoding meth-\nods (Holtzman et al., 2018; Krause et al., 2021;\nLiu et al., 2021; Yang and Klein, 2021; Sitdikov\net al., 2022) obtain an attribute probability P(c|X)\nfrom a separate reward model (typically smaller\nthan the base language model) and construct class-\nconditional text probabilities following Bayes rule,\nP(X|c) \u221d P(X)P(c|X), where c is an attribute\nclass and P(X) is the distribution over natural lan-\nguage sequences X. During decoding, Krause et al.\n(2021) and Liu et al. (2021) process signals from\nauxiliary generative models, whereas Yang and\nKlein (2021) and Sitdikov et al. (2022) evaluate\nintermediate sequences. Weighted decoding only\nrequires access to the next-step probabilities output\nby a language model, does not require expensive\ntraining, and is often modular, i.e. a single reward\nmodel can be reused with many language models.\nDespite these benefits, weighted decoding can sig-\nnificantly increase the cost of decoding and often\nunderperforms methods that involve further train-\ning (See et al., 2019).\nIn this paper, we close the gap between weighted\ndecoding and re-training by introducing reward-\naugmented decoding (RAD), an efficient, effective,\nand modular weighted decoding method that steers\ntext generation based on the reward returned by\nan attribute-specific reward model. In particular,\nRAD uses a unidirectional reward model trained\nto output a reward representing how well a given\nsequence aligns with a desired attribute. The uni-\ndirectionality of the reward model allows caching\nintermediate activations as the sequence is gener-\nated, greatly decreasing computational costs. Dur-\ning decoding, the tokens with the top-k highest\nprobabilities are rescaled according to the reward\nmodel so that tokens that better reflect the desired\nattribute are more likely to be chosen as the next\ngenerated token.\nTo validate RAD\u2019s effectiveness, we evaluate it\non standard detoxification and sentiment-controlled\ngeneration tasks, showing that it steers text genera-\ntion towards a desired attribute without sacrificing\nmuch diversity and fluency. We ultimately find that\nRAD outperforms other weighted decoding meth-\nods and achieves results comparable to methods\nthat involve additional training. We further validate\nRAD in a real-world large-scale setting by show-\ning it is effective and introduces minimal computa-\ntional overhead when applied to the LLaMA (Tou-\nvron et al., 2023) family of language models with\nup to 65B parameters.\n2\nReward-Augmented Decoding\nAt a high level, reward-augmented decoding, as\nshown in fig. 1, feeds intermediate candidate se-\nquences into a reward model that evaluates their\nalignment with a desired attribute. Then, at each de-\ncoding step, RAD uses the predicted reward of each\ncandidate sequence to modify the token probabili-\nties output by the language model. In this section,\nwe describe these steps in detail. Refer to table 2\nfor descriptions of the notations used in this paper.\n2.1\nUnidirectional Reward Model\nConsider using a reward model to compute rewards\nfor k candidate tokens at each of m generation\ntimesteps. If scoring each candidate token requires\nre-processing the entire generated sequence up to\nthe current timestep, the reward model would need\nto process O(km2) tokens, which could be pro-\nhibitively expensive. To address these issues, we\nuse a unidirectional reward model, specifically a\nTransformer decoder with causal masking (Liu\net al., 2018; Radford et al., 2018). In a unidirec-\ntional model with causal masking, previously com-\nputed representations remain unchanged when new\ntokens are appended, so at each generation timestep\nthe reward model only needs to compute the repre-\nsentation of the newly added token. This reduces\ncomputational costs to O(km).\nIn this work, the reward model is a modi-\nfied pre-trained decoder-only Transformer (GPT-\n2 small (Radford et al., 2019a) in all of our ex-\nperiments) fine-tuned on text annotated with the\namount of the target attribute present. We use a\ncumulative squared error loss that takes a weighted\nmean of each prefix\u2019s loss:\nL(r, \u02c6r) =\nPl\nt=1 t(rt \u2212 \u02c6r)2\nSl\n, Sl = l(l + 1)\n2\nwhere rt is the reward model\u2019s prediction at gen-\neration timestep t, \u02c6r \u2208 [0, 1] is the ground-truth\nreward value, and l is the generation length. The\ncumulative loss encourages the reward model to\noutput the correct reward for every prefix of the\nAlgorithm 1 Reward-Augmented Decoding\nInput\nf\u03b8\nneural network language model (outputs logits)\ng\u03bb\nneural network reward model (outputs reward score)\nX\ngeneration prefix\n1: xt \u2190 none\n2: while xt \u0338= < EOS > do\n3:\nwt \u2190 topk(f\u03b8(X))\n// get top-k tokens (indices), wt \u2208 Nk\n4:\nzt \u2190 f\u03b8(X)[wt]\n// get top-k token logits, zt \u2208 Rk\n5:\n\u03c1t \u2190 g\u03bb\n\uf8eb\n\uf8ec\n\uf8ed\n\uf8ee\n\uf8ef\uf8f0\nX; wt,1\n...\nX; wt,k\n\uf8f9\n\uf8fa\uf8fb\n\uf8f6\n\uf8f7\n\uf8f8\n// compute rewards, \u03c1t \u2208 [0, 1]k\n6:\npt \u2190 softmax(zt + \u03b2\u03c1t)\n// compute reweighted distribution\n7:\nxt \u223c Categorical(pt)\n8:\nX \u2190 {X; xt}\n// append new sample\nOutput generated text X steered towards higher rewards\ntext sequence in order to capture both current and\nfuture alignment of a candidate sequence with the\ndesired attribute.\n2.2\nWeighted decoding\nRAD utilizes top-k sampling (Fan et al., 2018;\nHoltzman et al., 2018; Radford et al., 2019b) and\nre-weights the probabilities of the tokens with the\ntop-k highest probabilities based on each candi-\ndate\u2019s reward score. Specifically, at timestep t,\nre-weighting is done by computing\nsoftmax(zt + \u03b2\u03c1t)\nwhere zt \u2208 Rk are top-k largest logits output by the\nlanguage model\u2019s at output timestep t, \u03b2 \u2208 R is a\nscaling hyperparameter (with higher \u03b2 correspond-\ning to more intense steering), and \u03c1t \u2208 [0, 1]k\nare the reward values for the k sequences corre-\nsponding to appending each of the top-k tokens.\nAdding \u03b2\u03c1t and renormalizing with softmax is\nproportional to reweighting the top-k probabilities\nby e\u03b2\u03c1t. Consequently, RAD effectively rescales\nprobabilities of the top-k tokens in accordance with\ntheir relative difference in reward. Algorithm 1 pro-\nvides an overview of the decoding process.\n3\nExperiments\nWe now evaluate RAD\u2019s performance in two stan-\ndard settings: Preventing language models from\ngenerating toxic text (Wallace et al., 2019; Gehman\net al., 2020) and controlling the sentiment of gener-\nated text (Li et al., 2018; Sudhakar et al., 2019).\nBaselines\nIn both settings, we consider the same\nset of baselines as Liu et al. (2021), namely: the\nperformance of the base language model itself with-\nout any interventions; PPLM (Pascual et al., 2021),\nwhich uses a bag-of-word classifier to update LM\nhidden states during decoding; GeDi (Krause et al.,\n2021) and DExperts (Liu et al., 2021), which\nuse signals from auxiliary language models to\nmodify LM probabilities in one pass; Rectifica-\ntion (Cao et al., 2023), which adjusts LM prob-\nabilities proportional to the risk of resulting in a\ntoxic generation; DAPT (Gururangan et al., 2020),\nwhich further trains the model on data that has\nthe desired property; PPO (Schulman et al., 2017),\nwhich updates the LM with gradients from the re-\nward model; Quark (Lu et al., 2022), which per-\nforms parameter-efficient fine-tuning on attribute-\nannotated data (Lester et al., 2021; Li and Liang,\n2021); and CTRL (Keskar et al., 2019), a language\nmodel trained to condition on control codes. Un-\nless otherwise mentioned, we report results directly\nfrom Liu et al. (2021) and Lu et al. (2022), which\ncan be consulted for further baseline details.\n3.1\nDetoxification\nExperimental Setup.\nWe closely follow past\nwork (Liu et al., 2021) and use RAD to detox-\nify generations from GPT-2 Large (Radford et al.,\n2019a) after conditioning on prompts from the Re-\nalToxicityPrompts (Gehman et al., 2020) dataset.\nFor our reward model, we fine-tune GPT-2 Small\non 2M human-annotated comments with continu-\nous labels between 0 and 1 from the Jigsaw Unin-\ntended Bias in Toxicity Classification dataset.1 We\nreport RAD\u2019s performance with different values k\n(used in top-k sampling) and \u03b2 (used for adjusting\nweighted decoding).\nEvaluation Metrics.\nFor every prompt, we sam-\nple 25 continuations, each containing up to 20 new\n1https://bit.ly/43CAdCJ\n10\n20\n30\n40\n50\n60\nPerplexity\n0.1\n0.2\n0.3\n0.4\nAverage Max Toxicity\nRAD k = 20\nRAD k = 50\nGPT2\nPPLM\nRectification\nGeDi\nDExperts\nDAPT\nPPO\nQuark\nFigure 2: RAD outperforms all weighted decoding meth-\nods (round points \u2022 in the graph) and matches methods\nthat involve additional training.\ntokens. As in Liu et al. (2021), we measure the Av-\nerage Max Toxicity, i.e. the expected maximum tox-\nicity score of the 25 continuations evaluated by the\nPerspective API2 and the Toxic Rate, i.e. the prob-\nability that at least one out of 25 continuations is\ntoxic (Perspective API toxicity score > 0.5). Since\nthe perspective API changes over time (Pozzobon\net al., 2023), we recomputed the scores for all base-\nline methods. We also measure the Diversity as\nthe number of distinct bigrams and trigrams nor-\nmalized by the length of text (Li et al., 2016) and\nthe Fluency as the perplexity assigned to the con-\ntinuation by GPT-2-XL conditioned on the prompt.\nIn general, a good method should reduce toxicity\nwhile preserving fluency and diversity.\nResults.\nAs shown in fig. 2 and table 4 (ap-\npendix), RAD demonstrates a favorable trade-off\nbetween toxicity and fluency without significantly\nsacrificing diversity, ultimately outperforming all\nweighted decoding methods and matching the per-\nformance of methods that involve additional train-\ning. Moreover, RAD achieves the lowest Average\nMax Toxicity of any method. Our results further\ndemonstrate that RAD provides an intuitive means\nto effectively trade-off toxicity and fluency by tun-\ning \u03b2.\n3.2\nSentiment-Controlled Generation\nExperimental Setup.\nFollowing past work (Li\net al., 2018; Sudhakar et al., 2019; Liu et al., 2021),\nwe use RAD to steer GPT-2 Large\u2019s generation\nto be either positive/negative in sentiment when\nprompted with negative/positive or neutral prompts.\nSpecifically, we evaluate on 2.5K negative, 5K neu-\ntral, and 2.5K positive prompts from OpenWeb-\nText (Gokaslan and Cohen, 2019). For RAD\u2019s re-\nward model, we fine-tune GPT-2 Small on millions\n2https://bit.ly/3p2r87b\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nNeutral Prompt\nPositive Rate\nRAD k = 20\nRAD k = 50\nGPT2\nPPLM\nCTRL\nGeDi\nDExperts\nDAPT\nPPO\nQuark\n10\n20\n30\n40 50 60\n100\n200\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nNegative Prompt\nPositive Rate\nPerplexity\nFigure 3: RAD achieves the highest positive rate for\nnegative prompts and outperforms all weighted decod-\ning methods.\nof product and movie reviews from Amazon Polar-\nity3 and SST-2 (Socher et al., 2013).\nEvaluation Metrics.\nWe sample 25 continu-\nations for each prompt and compute the aver-\nage Positive Rate measured by HuggingFace text-\nclassification pipeline4 (a DistilBERT model fine-\ntuned on SST-2). We also report the Diversity and\nFluency as introduced above.\nResults.\nAs seen in fig. 3 and table 5 (appendix),\nRAD attains a better fluency/positivity trade-off\n(when conditioning on negative or neutral prompts)\nthan any other weighted decoding method and\nachieves comparable performance to the state-of-\nthe-art methods involving training (Quark and\nPPO), which both make use of the evaluation model\n(DistilBERT model fine-tuned on SST-2) during\ntraining. Tuning \u03b2 effectively trades off fluency\nand alignment, again enabling RAD to produce the\nbest attribute scores. Figure 4 (appendix) visual-\nizes RAD\u2019s steering process when prompted with\nnegative input.\n3.3\nScaling the Language Model\nIn all prior experiments, we followed past work and\nconsidered using GPT-2 Large as the base language\nmodel. Recent LLMs have dramatically more pa-\nrameters (and dramatically better performance). To\ntest RAD in more realistic settings, we apply RAD\nto the state-of-the-art LLaMA models (Touvron\n3https://bit.ly/3XfY6NZ\n4https://bit.ly/3qIycX9\n  step 1:  be\n          2:  in\n          3:  developing\n          4:  countries\n          5: .\n          6:  But\n          7:  with\n          8:  clean\n          9:  energy\n        10:  technologies\n        11:  and\n        12:  other\n        13:  energy\n        14:  efficiency\n        15:  innovations\n        16: \n        17: like\n        18:  solar\n        19:  panels\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRewards\n        20: ,\nPrompt: The most polluted cities tend to\nFigure 4: Visualization of RAD\u2019s decoding process.\nEach row represents a single decoding step, where the\narea is the estimated reward distribution of the top-50\ncandidate sequences, and the red line indicates the se-\nlected token\u2019s reward score.\net al., 2023) in the detoxification setting of sec-\ntion 3.1, using the same GPT-2 Small reward model.\nIn table 6 (appendix), we show that RAD signif-\nicantly reduces LLaMA\u2019s toxicity while preserv-\ning its diversity and fluency. In terms of compu-\ntational costs, we list the relative cost of different\nmethods for controlled text generation in table 1.\nWhile RAD and other weighted decoding methods\nincrease costs significantly when the size of the\nlanguage model and reward model are similar, the\nadditional expense of using RAD is only about 3%\nwhen using LLaMA 65B as the language model\nand GPT-2 Small as the reward model. These re-\nsults confirm that RAD can effectively control text\ngeneration of state-of-the-art models while incur-\nring negligible computational overhead.\n4\nConclusion and Future Work\nIn this paper, we propose RAD, a simple weighted\ndecoding method for controlling text generation\nthat uses a unidirectional reward model to mini-\nmize computational costs. RAD outperforms prior\nweighted decoding methods and matches the per-\nformance of state-of-the-art techniques that involve\nadditional training. When the size of the reward\nmodel is relatively small compared to the base lan-\nguage model, RAD incurs negligible computational\noverhead. In future work, we are interested in ap-\nDecoding Cost\nMethod\nGPT-2 Large\nLLaMA 65B\nPPLM\n4.0\u00d7\n4.00\u00d7\nGeDi\n1.9\u00d7\n1.01\u00d7\nDExperts\n3.0\u00d7\n1.02\u00d7\nAdditional training\n1\u00d7\n1\u00d7\nRAD\n3.4\u00d7\n1.03\u00d7\nTable 1: Computational overhead (as a relative increase\nin cost) for different methods for controlling text gener-\nation using GPT-2 Small as a reward model and GPT-2\nLarge or LLaMA 65B as the language model. \u201cAd-\nditional training\u201d refers to methods that train the lan-\nguage model and do not modify decoding (e.g. Quark,\nDAPT, PPO, etc.). Calculation details provided in ap-\npendix C.2.\nplying RAD to more sophisticated tasks, such as\nencouraging language models to follow instruc-\ntions (Ouyang et al., 2022).\nLimitations\nAlthough RAD achieves decent performance and\ngeneralizes to other language models, two limita-\ntions should be considered for this work. Firstly,\nRAD incurs additional compute and memory allo-\ncation linear to k. As mentioned in section 2.1, we\nmanage to reduce time complexity from O(km2)\nto O(km) by reusing previously computed repre-\nsentations in the decoder reward model. Yet, track-\ning and copying past_key_values take up a certain\namount of GPU memory, which reduces decoding\nthroughput. Secondly, our experiments regarding\ntoxicity and sentiment explore only some capabil-\nities of RAD. More tasks should be conducted to\nform a comprehensive review of RAD.\nEthics Statement\nThis work centers around controllable text gener-\nation, which holds significant relevance in regu-\nlating natural language generation. For example,\nthe detoxification task aims to mitigate the toxicity\npresent in texts generated by pre-trained language\nmodels. In this context, RAD offers a solution\nfor controlling the text generation process without\nmodifying the base language model.\nAcknowledgements\nWe would like to thank Derek Tam for valuable\ndiscussions. We also extend our appreciation to the\nPerspective API team for increasing API quota on\nour behalf.\nReferences\nMeng Cao, Mehdi Fatemi, Jackie CK Cheung, and\nSamira Shabanian. 2023. Systematic rectification\nof language models via dead-end analysis. In The\nEleventh International Conference on Learning Rep-\nresentations.\nAntoine Chaffin, Thomas Scialom, Sylvain Lamprier,\nJacopo Staiano, Benjamin Piwowarski, Ewa Kijak,\nand Vincent Claveau. 2022. Which discriminator\nfor cooperative text generation? In Proceedings of\nthe 45th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval.\nACM.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889\u2013898, Melbourne, Australia. Association\nfor Computational Linguistics.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356\u20133369, Online. Association for Computational\nLinguistics.\nAaron Gokaslan and Vanya Cohen. 2019.\nOpen-\nwebtext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nSuchin\nGururangan,\nAna\nMarasovi\u00b4c,\nSwabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don\u2019t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342\u20138360, Online. Association for Computational\nLinguistics.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nAri Holtzman, Jan Buys, Maxwell Forbes, Antoine\nBosselut, David Golub, and Yejin Choi. 2018. Learn-\ning to write with cooperative discriminators. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1638\u20131649, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nMinbeom Kim, Hwanhee Lee, Kang Min Yoo, Joon-\nsuk Park, Hwaran Lee, and Kyomin Jung. 2023.\nCritic-guided decoding for controlled text generation.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 4598\u20134612, Toronto,\nCanada. Association for Computational Linguistics.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq Joty, Richard Socher,\nand Nazneen Fatema Rajani. 2021. GeDi: Gener-\native discriminator guided sequence generation. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 4929\u20134952, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045\u20133059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110\u2013119, San Diego, California. Association\nfor Computational Linguistics.\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018.\nDelete, retrieve, generate: a simple approach to senti-\nment and style transfer. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers),\npages 1865\u20131874, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582\u2013\n4597, Online. Association for Computational Lin-\nguistics.\nAlisa Liu,\nMaarten Sap,\nXiming Lu,\nSwabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021. DExperts: Decoding-time con-\ntrolled text generation with experts and anti-experts.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n6691\u20136706, Online. Association for Computational\nLinguistics.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summariz-\ning long sequences. In International Conference on\nLearning Representations.\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang,\nLianhui Qin, Peter West, Prithviraj Ammanabrolu,\nand Yejin Choi. 2022.\nQuark: Controllable text\ngeneration with reinforced unlearning. Advances\nin neural information processing systems, 35:27591\u2013\n27609.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nDamian Pascual, Beni Egressy, Clara Meister, Ryan\nCotterell, and Roger Wattenhofer. 2021. A plug-and-\nplay method for controlled text generation. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 3973\u20133997, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nLuiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara\nHooker. 2023. On the challenges of using black-\nbox apis for toxicity evaluation in research. arXiv\npreprint arXiv:2304.12397.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019a. Language\nmodels are unsupervised multitask learners.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019b. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes\nWelbl, Sumanth Dathathri, Saffron Huang, Jonathan\nUesato, John F. J. Mellor, Irina Higgins, Antonia\nCreswell, Nathan McAleese, Amy Wu, Erich Elsen,\nSiddhant M. Jayakumar, Elena Buchatskaya, David\nBudden, Esme Sutherland, Karen Simonyan, Michela\nPaganini, L. Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, N. K. Grigorev, Doug Fritz, Thibault Sotti-\naux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d\u2019Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew G. Johnson,\nBlake A. Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam S. Isaac, Edward Lockhart, Simon Osindero,\nLaura Rimell, Chris Dyer, Oriol Vinyals, Kareem W.\nAyoub, Jeff Stanway, L. L. Bennett, Demis Hass-\nabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021.\nScaling language models: Methods, analysis & in-\nsights from training gopher. ArXiv, abs/2112.11446.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Gall\u00e9, et al. 2022.\nBloom:\nA 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017.\nProxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nAbigail See, Stephen Roller, Douwe Kiela, and Jason\nWeston. 2019. What makes a good conversation?\nhow controllable attributes affect human judgments.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1702\u20131723,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nAskhat Sitdikov, Nikita Balagansky, Daniil Gavrilov,\nand Alexander Markov. 2022. Classifiers are better\nexperts for controllable text generation.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631\u20131642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nAkhilesh Sudhakar, Bhargav Upadhyay, and Arjun Ma-\nheswaran. 2019. \u201ctransforming\u201d delete, retrieve, gen-\nerate approach for controlled text style transfer. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3269\u2013\n3279, Hong Kong, China. Association for Computa-\ntional Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u2019elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019. Universal adversarial\ntriggers for attacking and analyzing NLP. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153\u20132162, Hong\nKong, China. Association for Computational Linguis-\ntics.\nKevin Yang and Dan Klein. 2021. FUDGE: Controlled\ntext generation with future discriminators. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n3511\u20133535, Online. Association for Computational\nLinguistics.\nA\nNotations\nRefer to table 2 for notations used in the paper.\nB\nRAD Training Details\nB.1\nDetoxification\nWe train a GPT-2 Small reward model on the Jigsaw\nUnintended Bias in Toxicity Classification dataset1\nfor 5 epochs. We use learning rate = 1e\u22125, weight\ndecay = 0.01, and batch size = 100. The reward\nmodel achieves a final squared error of 0.0147 on\nthe test public leaderboard subset.\nB.2\nSentiment-Controlled Generation\nWe first train the reward model on Amazon\nPolarity3 for 5 epochs, with learning rate = 1e\u22125,\nweight decay = 0.01, and batch size = 100. We\nthen continue to train the reward model on SST-\n2 (Socher et al., 2013) for 10 epochs, with learning\nrate = 2e\u22126.\nC\nComputational Costs\nC.1\nRAD\nIn the paper, we use GPT-2 Small (124M) as RAD\u2019s\nreward model and replace the lm_head layer with\na linear layer with one output for predicting the re-\nward. Following the approximations in Kaplan et al.\n(2020), the number of non-embedding parameters\nin a model is approximately\nN \u2248 12nlayerd2\nmodel\nwhere nlayer is the number of layers and dmodel is\nthe hidden size. In addition, a forward pass requires\nCforward \u2248 2N + 2nlayernctxdmodel\nFLOPs per token, where nctx is the context token.\nWith embedding operation costing 4dmodel and re-\nward predicting costing 2dmodel, we construct the\nnumber of FLOPs needed for a token in the reward\nmodel as\nCRM = 6dmodel + Cforward\nSince 6dmodel \u226a N,\nCRM \u2248 Cforward\n= 2(1 +\nnctx\n12dmodel\n)N\nNotice the context-dependent computational cost\nper token is a small portion of the total compute,\nas dmodel >\nnctx\n12 is often true in practice (Ka-\nplan et al., 2020). In fact, in detoxification and\nsentiment-controlled generation experiments, nctx\nis constantly below 50. Thus, it is safe to assume\nCRM = Cforward = 2N\nfor both the language model and the reward model.\nThe reward model evaluates k candidate sequences\nat each decoding step, which requires kCRM\nFLOPs in total. Assuming k = 20, CRAD = kCRM,\nappendix C.1 shows the estimated FLOPs per token\nof the reward model and various language models.\nC.2\nComparison\nWe continue to explore the computation cost of\nbaseline methods based on the methodology in ap-\npendix C.1. Define Cmethod as the additional cost a\nmethod incurs during decoding and TCmethod\u00d7LM\nas the total cost in FLOPs for every token generated\nusing the method on the specific LM during test\ntime. Retraining methods (DAPT, PPO, and Quark)\nhave Cmethod = 0 and TCmethod\u00d7LM = CLM.\nPPLM\nupdates the previous token\u2019s representa-\ntion in the LM using gradients from an attribute-\nspecific linear discriminator and recomputes the\ncurrent state probabilities.\nThus, two forward\npasses and one backward pass of the LM are re-\nquired for every generated token. As a backward\npass has roughly twice the number of matrix mul-\ntiplications as in a forward pass (Kaplan et al.,\n2020), PPLM incurs an additional decoding cost of\nCPPLM = 3CLM. Thus,\nTCPPLM\u00d7GPT2-large = 4CGPT2-large = 5.66G\nTCPPLM\u00d7LLaMA-65b = 4CLLaMA-65b = 515.40G\nGeDi & DExperts\ntake a similar approach where\nthey use two opposite discriminator/expert mod-\nels to produce classification probabilities and then\nNotation\nDimension\nDescription\nk\nN\nnumber of candidate tokens to consider at every timestep\n\u03b2\nR\nsteering amount hyperparameter\nl\nN\ngeneration length of reward model training data\n\u02c6r\n[0, 1]\nlabel of reward model training data\nr\n[0, 1]l\npredictions generated by reward model during training\n\u03c1t\n[0, 1]k\nreward scores predicted by the reward model at time t\nwt\nNk\nindices of top-k tokens at time t\nzt\nRk\nlogits of top-k tokens at time t\nTable 2: We use two notations r and \u03c1 to differentiate the reward model\u2019s output during train time and test time.\nRM\nnlayer\ndmodel\nCRAD\nGPT2-small\n12\n768\n3.40G\nLM\nnlayer\ndmodel\nCLM\nGPT-2 Large\n36\n1280\n1.42G\nLLaMA 7B\n32\n4096\n12.89G\nLLaMA 13B\n40\n5120\n25.17G\nLLaMA 33B\n60\n6656\n63.80G\nLLaMA 65B\n80\n8192\n128.85G\nTable 3: Model specifications and FLOPs per token.\nrescale the LM probabilities. Thus, two additional\nforward passes of the expert model are needed. For\nGeDi,\nCGeDi = 2CGPT2-medium = 1.21G\nTCGeDi\u00d7GPT2-large = CGeDi + CGPT2-large\n= 2.62G\nTCGeDi\u00d7LLaMA-65b = CGeDi + CLLaMA-65b\n= 130.06G\nFor DExperts,\nCDExperts = 2CGPT2-large = 2.83G\nTCDExperts\u00d7GPT2-large = CDExperts + CGPT2-large\n= 4.25G\nTCDExperts\u00d7LLaMA-65b = CDExperts + CLLaMA-65b\n= 131.68G\nRAD\nwith k = 20 has\nCRAD = 20CGPT2-small = 3.40G\nTCRAD\u00d7GPT2-large = CRAD + CGPT2-large\n= 4.81G\nTCRAD\u00d7LLaMA-65b = CRAD + CLLaMA-65b\n= 132.25G\nDAPT, PPO, and Quark\nhave decoding costs\nthe same as the underlying LM because they per-\nform additional training and do not modify the\ndecoding procedure.\nD\nFull Results\nD.1\nDetoxification\nSince perspective API updates its model regu-\nlarly (Pozzobon et al., 2023), we ensure fair com-\nparison by evaluating all model outputs (except for\nPPO and Quark, see below) using the most up-to-\ndate API. Queries were made between May and\nJune 2023. As PPO and Quark directly optimize\nthe language model with Perspective API score\nduring training, a change in the API model would\nlead to a different optimized model. For PPO and\nQuark, we adopt the values reported in Lu et al.\n(2022). Full results see table 4.\nD.2\nSentiment-Controlled Generation\nThe sentiment-controlled generation results are pre-\nsented in table 5.\nD.3\nScaling the Language Model\nFollowing previous experiments, we use nucleus\nsampling with p = 0.9 to get raw LLaMA gener-\nations on the same 10K non-toxic subset of Real-\nToxicityPrompts (Gehman et al., 2020). For each\nmodel size, we apply RAD with k = 20 and \u03b2\nfrom 20 to 500. Results are shown in table 6.\nThe performance gap between RAD on GPT-\n2 Large and RAD on LLaMA may be attributed\nto the difference in tokenization between the lan-\nguage model and the reward model. Specifically,\nthe reward model, GPT-2 Small, shares the same\ntokenizer and vocabulary with GPT-2 Large, but\nnot with LLaMA. In this way, a given text sequence\ncan be tokenized into different token combinations,\nwhich, during decoding, would mislead the reward\nmodel to give distorted scores. Thus, we believe a\nsmaller model from the same family of the base LM\nmay be the best choice for RAD\u2019s reward model.\nE\nGenerated Examples\nDetoxification and sentiment-controlled generation\nexamples are presented in tables 7 and 8.\nMethod\nToxicity (\u2193)\nFluency (\u2193)\nDiversity (\u2191)\nAverage Max Toxicity\nToxic Rate\nPerplexity\nDist-2\nDist-3\nGPT2\n0.384\n0.257\n11.31\n0.85\n0.85\nPPLM\n0.376\n0.240\n32.58\n0.86\n0.86\nGeDi\n0.242\n0.055\n60.03\n0.84\n0.83\nDExperts\n0.201\n0.021\n32.41\n0.84\n0.84\nRectification\n0.180\n0.014\n25.12\n0.86\n0.87\nDAPT\n0.270\n0.093\n31.21\n0.84\n0.84\nPPO\n0.218\n0.044\n14.27\n0.80\n0.84\nQuark\n0.196\n0.035\n12.47\n0.80\n0.84\nRAD\nk = 20\n\u03b2 = 10\n0.265\n0.076\n12.54\n0.81\n0.84\n\u03b2 = 20\n0.232\n0.042\n12.57\n0.81\n0.84\n\u03b2 = 30\n0.211\n0.026\n12.69\n0.81\n0.84\n\u03b2 = 50\n0.183\n0.014\n13.06\n0.81\n0.84\n\u03b2 = 100\n0.148\n0.005\n13.7\n0.81\n0.83\n\u03b2 = 200\n0.114\n0.002\n15.93\n0.79\n0.81\n\u03b2 = 300\n0.099\n0.001\n19.97\n0.76\n0.78\nk = 50\n\u03b2 = 10\n0.267\n0.069\n16.86\n0.84\n0.85\n\u03b2 = 20\n0.233\n0.035\n17.04\n0.84\n0.85\n\u03b2 = 30\n0.21\n0.022\n17.27\n0.84\n0.85\n\u03b2 = 50\n0.183\n0.011\n17.62\n0.84\n0.85\n\u03b2 = 100\n0.146\n0.004\n18.97\n0.84\n0.84\n\u03b2 = 200\n0.112\n0.001\n23.63\n0.83\n0.83\n\u03b2 = 300\n0.099\n0.001\n32.84\n0.79\n0.8\nTable 4: Full results of the detoxification experiment. In general, RAD can produce the least toxic outputs without\nsacrificing much fluency and diversity. While increasing k enhances diversity and reduces toxicity by a margin, it\ntakes into account more unlikely words which hurts the output fluency.\nMethod\nToward Positive\nToward Negative\n% Positive Rate (\u2191)\nFluency (\u2193)\nDiversity (\u2191)\n% Positive Rate (\u2193)\nFluency (\u2193)\nDiversity (\u2191)\nNegative\nNeutral\nPerplexity\nDist-2\nDist-3\nPositive\nNeutral\nPerplexity\nDist-2\nDist-3\nprompt\nprompt\nprompt\nprompt\nGPT2\n0.00\n50.02\n11.42\n0.85\n0.85\n99.08\n50.02\n11.42\n0.84\n0.84\nPPLM\n8.72\n52.68\n142.1\n0.86\n0.85\n89.74\n39.05\n181.7\n0.87\n0.86\nCTRL\n18.88\n61.81\n43.79\n0.83\n0.86\n79.05\n37.63\n35.94\n0.83\n0.86\nGeDi\n26.80\n86.01\n58.41\n0.80\n0.79\n39.57\n8.73\n84.11\n0.84\n0.82\nDExperts\n36.42\n94.46\n25.83\n0.84\n0.84\n35.99\n3.77\n45.91\n0.84\n0.83\nDAPT\n14.17\n77.24\n30.52\n0.83\n0.84\n87.43\n33.28\n32.86\n0.85\n0.84\nPPO\n43.13\n94.10\n15.16\n0.80\n0.84\n32.22\n3.65\n15.54\n0.81\n0.84\nQuark\n46.55\n95.00\n14.54\n0.80\n0.84\n27.50\n2.75\n14.72\n0.80\n0.84\nRAD\nk = 20\n\u03b2 = 10\n19.99\n86.21\n12.86\n0.79\n0.82\n73.34\n17.38\n13.33\n0.8\n0.83\n\u03b2 = 20\n35.24\n92.71\n14.6\n0.78\n0.82\n57.19\n10.49\n15.36\n0.79\n0.83\n\u03b2 = 30\n43.94\n94.8\n16.7\n0.77\n0.81\n50.04\n8.03\n17.79\n0.78\n0.82\n\u03b2 = 40\n48.37\n95.72\n19.23\n0.76\n0.8\n47.01\n6.84\n20.27\n0.77\n0.81\n\u03b2 = 50\n51.19\n96.3\n21.77\n0.75\n0.79\n45.45\n6.05\n22.8\n0.76\n0.8\n\u03b2 = 60\n53.21\n96.62\n25.06\n0.73\n0.78\n44.76\n5.47\n25.51\n0.74\n0.78\nk = 50\n\u03b2 = 10\n22.43\n86.66\n17.98\n0.82\n0.84\n67.22\n15.13\n18.77\n0.83\n0.85\n\u03b2 = 20\n41.56\n93.28\n21.02\n0.82\n0.84\n45.08\n8.63\n22.54\n0.83\n0.84\n\u03b2 = 30\n52.25\n95.37\n25.02\n0.81\n0.83\n36.2\n6.52\n26.49\n0.81\n0.84\n\u03b2 = 40\n57.91\n96.24\n28.99\n0.8\n0.82\n32.27\n5.62\n30.92\n0.8\n0.83\n\u03b2 = 50\n61.64\n96.7\n33.97\n0.79\n0.82\n30.16\n4.89\n35.92\n0.79\n0.82\n\u03b2 = 60\n63.57\n97.0\n39.23\n0.78\n0.81\n28.75\n4.45\n40.1\n0.78\n0.81\nTable 5: Full results of the sentiment-controlled generation experiment.\nLM\nSetting\nToxicity (\u2193)\nFluency (\u2193)\nDiversity (\u2191)\nAverage Max Toxicity\nToxic Rate\nPerplexity\nDist-2\nDist-3\nLLaMA 7B\nRaw LM\n0.338\n0.212\n12.93\n0.81\n0.82\n\u03b2 = 20\n0.282\n0.129\n13.79\n0.82\n0.83\n\u03b2 = 50\n0.250\n0.097\n14.21\n0.82\n0.82\n\u03b2 = 100\n0.221\n0.072\n15.13\n0.82\n0.82\n\u03b2 = 200\n0.185\n0.048\n17.40\n0.80\n0.81\n\u03b2 = 500\n0.125\n0.016\n29.97\n0.69\n0.73\nLLaMA 13B\nRaw LM\n0.336\n0.204\n11.81\n0.81\n0.82\n\u03b2 = 20\n0.284\n0.129\n12.70\n0.82\n0.82\n\u03b2 = 50\n0.253\n0.103\n13.00\n0.82\n0.82\n\u03b2 = 100\n0.223\n0.073\n13.79\n0.82\n0.82\n\u03b2 = 200\n0.187\n0.047\n16.20\n0.80\n0.81\n\u03b2 = 500\n0.127\n0.017\n29.82\n0.69\n0.72\nLLaMA 33B\nRaw LM\n0.337\n0.210\n10.80\n0.80\n0.81\n\u03b2 = 20\n0.287\n0.139\n11.68\n0.81\n0.82\n\u03b2 = 50\n0.258\n0.109\n11.96\n0.81\n0.82\n\u03b2 = 100\n0.229\n0.076\n12.79\n0.81\n0.82\n\u03b2 = 200\n0.192\n0.050\n15.06\n0.80\n0.81\n\u03b2 = 500\n0.130\n0.019\n28.58\n0.68\n0.72\nLLaMA 65B\nRaw LM\n0.337\n0.215\n10.42\n0.80\n0.80\n\u03b2 = 20\n0.288\n0.139\n11.30\n0.81\n0.81\n\u03b2 = 50\n0.260\n0.108\n11.56\n0.81\n0.81\n\u03b2 = 100\n0.232\n0.081\n12.27\n0.81\n0.81\n\u03b2 = 200\n0.197\n0.055\n14.58\n0.79\n0.81\n\u03b2 = 500\n0.133\n0.019\n28.64\n0.70\n0.73\nTable 6: Similar to its application to GPT2-large, RAD (k = 20) reduces the toxicity of LLaMA\u2019s generated text\nwithout greatly impacting its fluency and diversity. Its performance remains stable even as LLaMA model increases\nin size.\nPrompt: No matter who you\u2019ll pick you\u2019ll\nGPT2\nbe well placed to take advantage of many of the remaining lower season attractions, which include big-city\nPPLM\nneed a base set of rules. A position that will remain stable is a real issue as you\u2019ll\nGeDi\nneed the focus for this one. Personally, I had a good time in jokingly calling out Card\nDExperts\nneed a versatile DH player and Dahl will certainly help you as he\u2019s ideally positioned at the back of\nRectification\nstill get a great value. I would only recommend this product to people who have some sort of skin\nDAPT\nlikely be joining a roster of other people who remember that some characters are cuter than others, so\nRAD\nstill get the chance. I mean, we\u2019ve got some pretty awesome teams and some of them are\nTable 7: Examples of detoxified generation using different methods. RAD with k = 20 and \u03b2 = 100.\nPrompt: Meanwhile the iron and lead\nGPT2\ntransfer would be successful to some extent, but the heart issue is still very dangerous and will probably have\nTo Positive\nPPLM\ncontent on the mass spectrometer has reached to near the limit that these elements can be detected,\"\nCTRL\nparts of both were in good condition, though when I was ordering them I did not notice that they only\nGeDi\ngathered, our new friends danced, jests were merrily spiced, and plenty of songs fired\nDExperts\nfields may not seem like the perfect areas for reclaiming coal and steel, but technology has brought mining\nDAPT\nin the water begins to leach from the old pipes, whilst the decaying furniture sheds a great deal\nRAD\nindustries, which provide a great deal of economic and social support for the British working class and a great\nTo Negative\nPPLM\nprices have collapsed to record lows and given no indication they will recede as the seller and take the\nCTRL\ncontent is over half of that in all US bottled water - 0.32 ppm versus 0.012 ppm.\nGeDi\ncontent in some vaccines have already precipitously risen above acceptable limits. They\u2019ve even gone so far\nDExperts\nsinks. 15 Too many carts loaded with rusty iron bars running off of a firewood dump. 80\nDAPT\nreserves have fallen precipitously. At last count, the final daily production estimate was 50,000\nRAD\nshortages have only been getting worse. The steel industry that is vital to the economy today is in serious\nTable 8: Examples of sentiment-controlled generation using different methods. RAD with k = 50 and \u03b2 = 30.\n"
  },
  {
    "title": "Interactive Task Planning with Language Models",
    "link": "https://arxiv.org/pdf/2310.10645.pdf",
    "upvote": "9",
    "text": "Interactive Task Planning with Language Models\nBoyi Li\u2217\nPhilipp Wu\u2217\nPieter Abbeel\nJitendra Malik\n*Equal contribution\nUniversity of California, Berkeley\nAbstract\u2014 An interactive robot framework accomplishes\nlong-horizon task planning and can easily generalize to new\ngoals or distinct tasks, even during execution. However, most\ntraditional methods require predefined module design, which\nmakes it hard to generalize to different goals. Recent large\nlanguage model based approaches can allow for more open-\nended planning but often require heavy prompt engineering or\ndomain specific pretrained models. To tackle this, we propose\na simple framework that achieves interactive task planning\nwith language models. Our system incorporates both high-level\nplanning and low-level function execution via language. We\nverify the robustness of our system in generating novel high-\nlevel instructions for unseen objectives and its ease of adaptation\nto different tasks by merely substituting the task guidelines,\nwithout the need for additional complex prompt engineering.\nFurthermore, when the user sends a new request, our system\nis able to replan accordingly with precision based on the new\nrequest, task guidelines and previously executed steps. Please\ncheck more details on our Project Page and Demo Video.\nI. INTRODUCTION\nThe rise of Large Language Models (LLMs) and prolifera-\ntion of chatbots highlight the importance of human interaction\nin an AI system. Beyond merely executing user commands,\nan autonomous agent should fluidly receive and incorporate\nfeedback at any step during the execution process. Consider\nthe seemingly straightforward human task of preparing a\nflavorful milk tea, which we study in this work. Such a task,\nwhile simple to humans, requires a robot to decompose it into\nnumerous intermediate steps. Not only does the robot need to\ngenerate and precisely execute the steps, but the robot should\nalso remain receptive to real-time modifications or feedback\nto the initial request. For example, the user might request\nsome boba to be added to their drink. A robot should be able\nto seamlessly incorporate such interaction during operation.\nIn light of these challenges, we propose a simple framework\nfor Interactive Task Planning with language models, denoted\nas ITP. Our framework leverages LLMs to plan, execute, and\nadapt to user inputs throughout the task lifecycle. Figure 1\nillustrates an exemplary interaction with our system. Our\nprimary objective is to offer a blueprint for deploying real-\nworld robotic systems that harness pretrain language models\nto coordinate the execution of lower-level skills of a robot\nin a simple manner. For our project, we utilize GPT-4 [1] as\nthe language model backbone. ITP consists of two primary\nmodules; (1) a high level planner which takes a input a prompt\nto specify the task and a user request and outputs a step by\nstep plan and (2) a low level executor which tries to achieve\na given step by converting robots skills into a functional API,\nHigh-level Plan\nStep 1: get an empty cup and put it on the table\nStep 2: add taro into the empty cup\nStep 3: pour the milk into the cup\nStep 4: put the cup in the finished location\n        \n       \nHigh-level Plan\nStep 2: add matcha powder into the working cup\nStep 3: add boba into the empty cup\nStep 4: pour the milk into the cup\nStep 5: put the cup in the finished location\nCompleted! \u2705\nExecution Code \ndef grasp_cup():\n\u2026\ndef scoop_boba():\n\u2026\ndef pour():\n\u2026\n\u2026\nLow-level Plan\nTask guidelines only include how to make pure milk, \nstrawberry milk and boba milk. Our system is able to \ngenerate a high-level plan, even if taro milk is not \nprovided in existing guidelines. \n      \n       User Request:\nMay I have a cup of milk with taro? \nPlan \nReplan\n      \n       User New Request:\nSorry, I want matcha milk with boba \ninstead. \nThen the user wants to order another type of drink \n(boba milk) during the execution. At this moment, \neven though our system already completed step 1, \nour system is able to accurately replan based on \nthe new request, memorized completed steps \nand task guidelines. \nMemorized \ncompleted \nsteps\nFig. 1: An example of ITP. Our system will generate high-level plans and execute the low level robot skills through LLMs. It stores\neach step once complete, which we refer to as \u2018memorized completed steps\u2019. We only give minimal guidelines for high-level plans on\nmaking pure milk, strawberry milk and boba milk. In the example shown, the user first requests \u2018May I have a cup of milk with taro?\u2019\nwhich is proccessed by ITP. As shown, although the recipe for taro milk is not provided in the guidelines, our system is able to generate\nan accurate executable high-level plan. After the robot has finished step 1, the user wants to revise the order to matcha milk with boba,\nwhich is also not provided in the guidelines. Our system is able to replan and make a new set of high-level steps based on the new\nrequest, memorized completed steps and task guidelines, which can then be completed by the lower level execution module to successfully\ncomplete the request.\narXiv:2310.10645v1  [cs.RO]  16 Oct 2023\nwhich enables GPT-4s function calling capabilities to directly\ninteract with the robot, abstracting code level details from the\nsystem. ITP does not require the training of additional value\nfunctions such as SayCan [2], [3], and does not require code\nlevel prompts such as Code as Policies[4] or ProgPrompt\n[5]. Furthermore, ITP dynamically generates novel plans\nand re-adjusts its plan based on user input. We hope our\nframework will be useful for accomplishing a wide range of\ninteractive robot tasks and will release our codebase to foster\nadvancements in this field. We outline the key features of\nITP below:\n1) ITP is a novel training-free robotic system for interactive\ntask planning with language models. We showcase ITP\nin the context of a real world boba drink-making robot\nwhich integrates planning, vision and skill execution.\n2) ITP is robust and can generate executable plans from a\nlimited set of existing recipes, showing its adaptability.\n3) Our system converts the robot skills into a functional\nlanguage based API that can be leveraged by GPT-4.\nThis enables a user to prompt the system through natural\nlanguage rather than code, removing the need for code\nlevel prompt engineering.\n4) Our system exhibits robustness in adapting to user\nrequest during execution, allowing it to consider the\nupdated goals, previously completed steps, and task\nguidelines in order to replan new steps.\nII. RELATED WORK\nA. Task planning\nTask planning, the problem of developing a plan to\nachieve a desired goal, is an integral component of our work.\nTraditionally, task planning in robotics commonly leverages\nsymbolic planners which reduce the planning problem into a\nsearch problem [6], [7]. Practitioners define the problem in a\ndeclarative language, which can be restrictive as it requires\nmeticulous definitions of the problem parameters, such as\nactions, their preconditions and their effects [8], [9], [10],\n[11]. Task and motion planning (TAMP), takes task planning\na step further and also jointly considers the lower level\nexecution during higher level planning [12], [13]. TAMP\nmethods also consider symbolic representations and leverage\nsearch algorithms to extract the final sequence of lower-level\nprimitives and has seen success in robotic manipulation [14],\n[15], [16]. As the search space can often be prohibitively\nlarge, some methods leverage hierarchy and/or sampling [17],\n[18], [19], [20]. Our approach replaces traditional planning\npipelines with LLMs, offering common-sense reasoning,\nenhanced interaction capabilities, and the ability to define the\nproblem\u2019s scope using natural language.\nB. Language Models as Planners in Robotics\nDue to the popularity of LLMs, there has been a rising\ninterest in leveraging LLMs as a policy in robot systems.\nRequest\nHuman-in-the loop feedback: The user sends new requests to provide \nfeedback, which could happen after any step during execution.\nPerception feedback: \nOur system localize the \nobjects accurately by \nusing a detection \nsystem to provide \nperception feedback.\nBased on very \nlimited task \nguidelines and \nuser request, our \nsystem makes \nhigh-level plans.\nExecution feedback: \nOur system provides \nstatus updates as \nfeedback to help control \nor replan the following \nsteps during execution\nSkill Execution\nHigh-level\nLow-level\nTask Guidelines\nLow-level policy execution\ngrasp_cup()\nplace_cup()\nscoop()\n...\nGPT-4\nScene Description\nLow-level Skills\nPlan\nReplan\nNew \nRequest\nGPT-4\nPlan \nReplan\nFig. 2: Overview of ITP. In this paper, we design our system with Grounded-DINO to locate the object and GPT-4 to process the language.\nOur system generate high-level plans based on the user request and task guidelines. When the system is interrupted with new request, the\nsystem will replan on the basis of human-in-the loop feedback (new request), execution feedback (memorized completed steps) and task\nguidelines. Each generated high-level plan will go through GPT-4 to acquire the corresponding execution of low-level skills.\nOne work in this direction leverages LLMs as zero shot\nplanners in simulated embodied settings [21] by converting\nthe scene and task definitions into language, then letting the\nLLM directly predict actions. [2], [22], [3] follow in this line\nof work, coordinating many large pretrained models with a\nrobot to solve various tasks. In contrast to approaches like\nSayCan [2], which necessitate a pretrained value function to\nground actions, we rely on prompting the language model\nwith task guidelines and robot skills. This implicitly encodes\npreconditions and effects reminiscent of traditional declarative\ntask planning approaches but can be done so with natural\nlanguage, which is more expressive and easier for the average\nuser to tune. Tidy bot [23] shows that LLMs can help a robot\nfollow a user\u2019s preferences based on a few examples. We also\nprompt the model with a small set of examples but explore\ngeneralization to new goals. Reflect [24] uses large models to\nmake an agent recount their experiences and correct failures.\nLLMs have also been used to allow robots to seek help when\nuncertain [25].\nA related approach, used in Code as Policies [5] and\nProgPrompt [4] leverages the code writing capabilities of\nLLMs to generate code that a robot agent can execute directly.\nThis often requires heavy prompt engineering of example\ncode to show the model how to properly use the provided\nfunctions to accomplish a directive. Language-guided Robot\nSkill Learning [26], like us, takes a hierarchical approach to\nLLM planning, but assumes access to the simulator which\nprovides ground truth state information. Voyager [27] uses\nLLMs to build a life long learning agent for Minecraft by\nhaving the agent explore and solve new tasks through writing\ncode that interacts with the API.\nOur work falls into this general category of leveraging\nLLMs to plan, and then execute actions in the environment.\nIn contrast to prior work, we allow the LLM to generate\na high-level plan based on contextual information. These\nlow-level plans are then executed directly by an LLM with\naccess to the functional API of the robot using a pre-trained\nVLM to ground the visual scene into primitives. Our work\nfocuses on how to instantiate such a system in the real world.\nIII. METHOD\nITP offers a blend of high-level planning and low-level\nexecution, powered by LLMs. In contrast to prior work [5],\n[4], our approach enables the LLM to create a high-level\nplan informed by contextual information in the form of a list\nof steps. Each step of this plan is subsequently realized by\nanother LLM with access to the functional API of the robot.\nA pre-trained VLM grounds the visual scene into language.\nOur work focuses on how to instantiate such a system in the\nreal world. Our framework, shown in Fig. 2, consists of three\nprimary building blocks:\nVisual Scene Grounding: ITP converts visual inputs into\nlanguage using a Vision-Language Model (VLM).\nLLMs for Planning and Execution: ITP generates high\nlevel plans and executes lower level robot skills.\nRobot Skill Grounding: ITP translates robot skills into a\nfunctional API, enabling LLMs to dictate robot actions.\nVisual Scene Grounding. The VLM\u2019s role is to process\nthe visual scene into a concise language description, which\ncan further be processed for planning and task execution\ndownstream. In our drink-making system, the visual ground-\ning system accepts a list of menu items and generates\ncorresponding bounding boxes. Using a simple mapping\nalgorithm, we then approximate the x and y locations of\neach item in the robot frame. We employ the pretrained\nVLM: Grounded-DINO [28], a variant of the original DINO\nmodel [29] fine-tuned for extracting 2D bounding boxes given\nlanguage descriptions. The vision system gives a holistic\n\u2018understanding\u2019 of the scene, despite the location assignments\nbeing imprecise.\nLLMs for Planning and Execution. We utilize GPT-\n4 [1] as our language model, one of the most capable LLMs\navailable at the time of this writing. Our approach employs\ntwo language agents. The high level planner takes as input a\ngiven prompt, task guidelines, and a user request, and outputs\na step-by-step plan to execute the request. It also retains past\nuser interactions for any necessary replanning. The second\nLLM, provided with information about the scene and robot\nskills, takes each generated step and attempts to execute it.\nTask guidelines, described using natural language, outline\nthe scope of the robot\u2019s tasks and are provided to the high\nlevel planner. In our milk tea system, the task guidelines\nconsist of a select set of menu items, their corresponding\npreparation steps, and a list of relevant ingredients. This\nincludes the procedures for a few drinks like \u2018pure milk\u2019 and\n\u2018boba milk\u2019. Our system utilizes these guidelines to determine\nthe feasibility of making a new drink based on available\nmaterials. Leveraging LLMs\u2019 fewshot learning capabilities,\n[30], ITP can generalize from the baseline guidelines to make\ndetailed steps for other drinks such as \u2018boba strawberry milk\u2019\nor \u2018taro milk\u2019.\nRobot Skill Grounding. The language model interfaces\nwith a predefined skill set in Python that controls the robot.\nThese skills are translated into a functional API by parsing\nof function definitions and related doc strings. This can\nbe directly used with GPT\u2019s function-calling layer [1]. In\ncontrast to methods like ProgPrompt or Code as Policies, our\nsystem does not require examples or function details when\nprompting the LLM. Instead, more detailed prompting of the\nlanguage model can be specified via natural language in the\ndocumentation of the functions.\nBeyond the three aforementioned components, ITP consid-\ners new requests from the user as human-in-the-loop feedback.\nThe system will consider completed steps, task guidelines,\nthe new request, and the chat history to generate a new plan.\nWe explain the details of an example in Figure 3. We also\nshowcase ITP\u2019s adeptness in planning and adaptive replanning\nof the same example in Figure 4.\nIV. EXPERIMENTS\nA. Robot Experiments\nIn our experiments, we focus on a drink-making system.\nWithin the given scene, the robot is supplied with a set of\ningredients which it must combine to produce a specific drink.\nOur setup also has an overhead camera which feeds images\nto Grounded-DINO model for scene understanding.\nFor the robot, we provide a predefined set of skills,\nwhich\ninclude\nactions\nlike\n\u201cgrasp cup\u201d,\n\u201cpour\u201d,\nand\n\u201cscoop boba to location\u201d. The \u201cgrasp cup\u201d skill is imple-\nmented with a feedback policy that centers the gripper on\nthe cup, given the approximate location from the scene\ndescription, enabling the robot to grasp it reliably. The \u201cpour\u201d\nskill is designed to accept a location and a descriptive cue\nof the ingredient being poured. This level of specification\nenables milk to be poured more than specific flavors. For\nexample, when making a matcha latte, the pour function will\nbe provided \u201cmatcha\u201d or \u201cmilk\u201d as inputs. When the input\nis \u201cmatcha\u201d, the controllable tilt angle will be small, while\nwhen the input is \u201cmilk\u201d, the controllable tilt angle will be\nmuch larger. This ensures that the robot can pour more milk\nand a bit of matcha liquid.\nB. Comparison on Task Planning\nWe consider Code as Policies as a baseline. Code as\nPolicies provides a formulation for language model-generated\nprograms executed on real systems by prompting a text\ncompletion model with code examples. For a fair comparison,\nnot only do we provide Code as Policies with the same\ninformation as given in ITP in the form of comments, but we\nalso provide an additional 40 lines of code prompts providing\nexample usage, as is done in Code as Policies. For both\nITP and Code as Policies, we provide user requests and task\nguidelines as inputs. The task guidelines include 3 instances,\nalong with their associated high-level planning steps, current\navailable material and other task-specific conditions. We show\nthe detailed task guidelines below:\nOptions:\nPure milk, Strawberry milk, Boba milk\nInstructions:\nPure milk\nMaterial: milk\nSteps:\n0) get an empty cup and bring it to the working area\n1) pour the milk into the working cup\n2) put the working cup in the finished location\nStrawberry milk\nMaterial: strawberry jam, milk\nSteps:\n0) get an empty cup and bring it to the working area\n1) add strawberry jam to the working cup\n2) pour the milk into the working cup\n3) put the working cup in the finished location\nBoba milk\nMaterial: boba, milk\nSteps:\n0) get an empty cup and bring it to the working area\n1) add boba to the working cup\n2) pour the milk into the working cup\n3) put the working cup in the finished location\nAvailable material we have now:\nboba, strawberry jam, mango jam, matcha powder, taro,\nmilk,blueberry\nTask Guidelines 1: making a drink\nWe evaluate the methods on two criteria: the number of\nhigh-level steps correctly generated and whether the real\nStep 1: get an empty cup and put it on the table\nStep 2: add taro into the empty cup\nStep 3: pour the milk into the cup\nStep 4: put the cup in the finished location\nHigh-level Plan\nStep 2: add boba into the empty cup\nStep 3: pour the milk into the cup\nStep 4: put the cup in the finished location\nHigh-level Plan (after replanning)\nThe user has requested:\n---\n{user_prompt}\n---\nThe the guidelines are:\n---\n{task_guidelines}\n---\nSet A = all the materials we have now.\nSet B = all the materials we need.\nPrint Set A in the first line.\nPrint Set B in the second line.\nPrint Set C in the third line where Set C are the items in Set B that \nare not in Set A.\nIf Set C is not empty, provide unique element and respond with \"Set \nC is not empty\"; else, respond with a numbered list of steps, where \neach step is in a new line (the steps should closely match one of the \nguidelines).\nPrompt\nMay I have a cup of \nmilk with taro? \nSo far the robot has completed these \nsteps: \n{completed_steps}. \nThe next step is to \n{step}.\nThe current scene looks like this: \n{scene_description}\nCan you use the robot functions to \ncomplete: {step}?\nAfter completion, respond with a summary \nof the execution. Make sure to put things \nback after using them.\nPrompt\ngrasp_empty_cup_from_stack()\nLow-level Actions\nScene Description\nscene = {\n\"cup with milk\": (0.667, -0.266),\n\"bowl with boba\": (0.535, -0.47),\n\"cup with taro\": (0.23, -0.46),\n\"cup with strawberry\": (0.37, -0.865),\n\"cup stack\": (0.4, 0.0),\n\"finished location\": (0.15, 0.5),\n\"trash_location\": (0.6, -0.4),}\nCompleted Steps\nStep 1: get an empty cup \nand put it on the table\nSo far the robot has completed these steps: \n{completed_steps}.\nThe user has requested some feedback now:\n---\n{new_request}\n---\nIf the user wants to add something, directly add one step and keep \nthe original steps unchanged.\nFor other requests, can you first print a summary of the current \nusers request after their feedback? Then like before\nSet A = all the materials we have now.\nSet B = all the materials we need.\nPrint Set A in the first line.\nPrint Set B in the second line.\nPrint Set C in the third line where Set C are the items in Set B that \nare not in Set A.\nIf Set C is not empty, provide unique element and respond with \"Set \nC is not empty\";\nelse, respond with a new numbered list of steps, where each step is \nin a new line (the steps should closely match one of the guidelines).\nsteps already completed should be excluded from the new list. If \nyou need to start from scratch, then put the existing cup in the trash \nlocation and get a new empty cup.\nPrompt\nMay I change to a taro \nboba milk?\n Plan\nReplan\n+\nCompleted Steps\nLow-level Code\nExecute next steps\u2026\u2026\ndef grasp_empty_cup_from_stack(self, x: float, y: float) -> Tuple[bool, str]:\n     \"\"\"Grab an empty cup at the given x, y coordinates from the stack. \n     Should only be used on a cup stack.\n     Args:\n        x (float): The x coordinate of the object.\n        y (float): The y coordinate of the object.\n    Returns:Tuple[bool, str]: Whether the grasp was successful and a message.\"\"\"\n    # approach the cup\n    x_app, y_app = get_x_y_offset(x, y, offset=OFFSET)\n    self._robot.command_ee_pos(x_app, y_app, Z_STACK)\n    # grab the cup\n    try:\n         description = \"cup\"\n         self._feedback_policy.grasp(\n         self._robot, \u201ccup\u201d, x, y, Z_STACK, rotz=True)\n     except Exception as e:\n         print(e)\n         return False, f\"Grasp failed: {e}\"\n    # backup\n    self._robot.command_ee_pos(Z_STACK + DELTA_Z) # lift\n    self._robot.command_ee_pos(x_app, y_app, Z_STACK + DELTA_Z)\n    return True, \"Grasp successful\"\nFig. 3: Detailed diagram of ITP. ITP incorporates user requests, task guidelines, and memorized completed steps for planning or replanning.\nDuring \u201cPlan\u201d: we feed user requests and task guidelines to complete the prompt and input it into GPT-4 to obtain a high-level plan. We\ninput the completed steps and next step to complete the prompt and input the prompt into the lower level executor GPT-4 to call the\ncorresponding low-level actions. Once the lower level executor completes a step, we will maintain the history by storing it into Completed\nSteps. GPT-4 directly makes function calls to a predefined robot skill library (which could be learned or handcrafted). During \u201cReplan\u201d:\nwe feed the completed steps and new request to create a new prompt, we append this new prompt to the previous conversation context and\ninput the whole message into GPT-4 to obtain a new high-level plan. We refer this procedure as replanning, which previous language-based\ntask planning methods have not considered. The low level executor then completes the next steps based on the new high-level plan.\nrobot successfully finished the task. We send user requests\nof varying complexity levels, including \u2018existed\u2019, \u2018zero-shot\neasy,\u2019 \u2018zero-shot moderate\u2019, \u2018zero-shot hard\u2019 and \u2018unavailable\nmaterial\u2019. \u2018Zero-shot\u2019 means the instruction for making the\ncorresponding drink is not provided in the task guidelines.\n\u2018Unavailable\u2019 indicates that we don\u2019t have the material for\nthe requested beverage. We show the results in Table I. We\ncould notice that ITP is robust in high-level plan generation\nand can easily be generalized to novel instructions of unseen\ndrinks or unavailable drinks. For example, the user sends the\nrequest \u2018I would like a cup of passion fruit milk.\u2019 However,\npassion fruit jam is not available, so the system will provide\nthe response \u2018Passion fruit jam is not available\u2019 and stop the\nprogram. In comparison, Code as Policies failed to achieve\nthis objective. To understand the failure case of Code as\nPolicies, we provide some observations: 1) when making a\ncup of milk with boba, the system attempted to scoop boba\nfrom the working up, improperly adhering to the correct\nusage of the lower level skill. 2) When the prompt is more\ncomplex (9th row), the system adds milk first and then adds\nthe boba, resulting in an incorrect execution order. 3) When\nthe material is not available, it cannot justify that passion\nfruit doesn\u2019t exist. Additionally, since ITP is built based on\ntask guidelines alone, it demands significantly less prompt\nengineering than Code as Policies, which makes our system\nvery easy to use for various task planning purposes.\nC. Replan with Human-in-the-loop Feedback\nOur system is robust to diverse new requests during\nexecution. To verify this point, we assess the task replanning\nperformance on real robots in response to a user\u2019s new request,\nreferred to as human-in-the-loop feedback. We display the\nresults in Table II. We notice that ITP demonstrates its\ncapacity to effectively handle a range of new requests, even\nafter progressing through various steps of the task. The last\nexample is of particular note, where ITP adds one step more\n(\u2018Stir the mixture until the matcha powder is well mixed\u2018)\nbefore putting the working cup in the finished location. Here\nthe language model assumes the need to stir the matcha due to\nthe ambiguity of the correct procedure. Such superfluous steps\ncan be reduced by adding restrictions in the task guidelines,\nwhich can easily be done by a general user of the system.\nThis contrasts with methods like Code as Policies which\nWe adopt Grounded-DINO for capturing \nthe general location of each object\nInitialization Stage\nHigh-level step1: grasp the empty cup \nLow-level action: \ngrasp_empty_cup_from_stack() \nHigh-level step1: grasp the empty cup \n(then, we replan for the new request)\nLow-level action: place_cup()\nHigh-level step2: add boba into the cup\nLow-level action: \nscoop_boba_to_location()\nHigh-level step2: \nadd boba into the cup\nLow-level action: \nscoop_boba_to_location() \n(during execution)\nHigh-level step 3: add taro into the cup\nLow-level action: grasp_cup(), pour(), \nplace_cup()\nHigh-level step 3: add milk into the cup\nLow-level action: grasp_cup(), pour(), \nplace_cup()\nHigh-level step 4: place the cup in the \nfinal workspace of the table\nLow-level action: grasp_cup(), \nplace_cup()\nUser request: Can I have a taro milk?\nFig. 4: An example of ITP to make a cup of taro milk with boba. Our system first makes a high-level plan based on the user request\nusing GPT-4: step 1) grasp the empty cup, step 2) add taro into the cup, step 3) add milk into the cup, step 4) place the cup in the final\nworkspace. For each step in the high-level plan, we feed step into another instance of GPT-4 and obtain the corresponding low-level\nactions which is directly executed on the robot. As for the perception component, ITP uses Grounded-DINO to capture the general location\nof each object and locate the object accurately when taking the actions. However, after grasping the empty cup, the user sends a new\nrequest \u2018May I change to a taro boba milk?\u2019. Considering the memorized completed steps as execution feedback, the system replans and\ngenerates the following high-level steps and low-level executions. The following plan has been changed to: step 2) add boba into the cup,\nstep 3) add taro into the cup, step 4) add milk into the cup, step 5) place the cup in the final workspace.\nUser Request\nDifficulty Level\nCode as Policies\nITP\nHigh-level Planning\nSuccess\nHigh-level Planning\nSuccess\nI would like to order a cup of milk.\nExisted\n3/3\n\u2713\n3/3\n\u2713\nI want to order a boba milk.\nExisted\n2/4\n\u2717\n4/4\n\u2713\nCan I have a cup of strawberry milk?\nExisted\n4/4\n\u2713\n4/4\n\u2713\nI want a matcha latte.\nZero-shot easy\n4/4\n\u2713\n4/4\n\u2713\nMay I have a cup of milk with taro?\nZero-shot easy\n3/3\n\u2713\n3/3\n\u2713\nI want taro milk with boba.\nZero-shot moderate\n3/5\n\u2717\n5/5\n\u2713\nCan I get a strawberry boba milk?\nZero-shot moderate\n3/5\n\u2717\n5/5\n\u2713\nI want to order a strawberry matcha milk.\nZero-shot moderate\n5/5\n\u2713\n5/5\n\u2713\nI\u2019d order a strawberry matcha milk with boba.\nZero-shot hard\n3/6\n\u2717\n6/6\n\u2713\nI would like a cup of passion fruit milk.\nUnavailable material\n-\n\u2717\n-\n\u2713\nTotal\n-\n80%\n5/10\n100%\n10/10\nTABLE I: Quantitative results with real robots for high-level planning rate and success rate with various user requests. For high-level\nplanning, we extract planning accuracy by dividing the number of successful steps by the total number of steps, shown as \u2018Successful\nSteps / Total Steps\u2019. We determine success by whether the robot successfully accomplishes the task. To calculate the overall high-level\nplanning score, we average the performance across all user requests.\nUser Request\nNew Request\nStep When New Request is Made\n1st\n2nd\n3rd\nCan I have a cup of strawberry milk?\nI want to add boba into the drink.\n4/4\n3/3\n5/5\nI want a matcha latte.\nSorry, I want boba bilk without matcha instead.\n3/3\n5/5\n5/5\nMay I have a cup of milk with taro?\nCan I replace the taro with strawberry?\n3/3\n5/5\n5/5\nCan I get a strawberry boba milk .\nSorry, can I reorder a strawberry milk?\n3/3\n5/5\n5/5\nA strawberry matcha milk with boba.\nCan I just get matcha boba milk and no strawberry?\n4/4\n5/4\n7/6\nTABLE II: Replanning performance with real robots given human-in-the-loop feedback. After the user sends a request, we interrupt the\nprocedure before different steps (1st, 2nd, and 3rd). Note that our replanning system is robust in handling these new requests. Interestingly,\nfor the last example, after the 2nd and 3rd step, ITP adds one step more (\u2018Stir the mixture until the matcha powder is well mixed\u2019) before\nputting the working cup in the finished location, leading to 5 and 7 steps instead of 4 and 6 steps respectively. We assume this is because\nGPT-4 assumes matcha powder is hard to mix, while we select water-soluble matcha powder. Including the instruction \u2018matcha powder is\nwater-soluble\u2019 in the task guidelines could address this issue.\nrequire tuning prompts at the code level.\nOptions:\nWash one plate with rose flavor,\nWash all the plates and there are two plates,\nWash one plate and one fork\nInstructions:\nWash one plate with rose flavor\nMaterial: rose detergent\nSteps:\n0) grasp the dirty plate\n1) remove large particle from the plate\n2) open the dishwasher\n3) pull out the rack\n4) put one plate on the third rack\n5) add rose detergent into the detergent dispenser\n6) close the dishwaster\n7) select the cycle and start dishwasher\n8) after the dishwasher cycle is complete and the\ndishwasher has stopped, wait a few minutes for the dishes\nto cool down\n9) make sure the plate is clean and dry, otherwise\ngo into step 8)\n10) return the clean plate to the finished location\nWash all the plates and there are two plates\nMaterial: original detergent\n0) grasp the first dirty plate\n1) remove large particle from the plate\n2) open the dishwasher\n3) pull out the rack\n4) put the plate on the third rack\n5) grasp the second dirty plate\n6) remove large particle from the plate\n7) put the plate on the third rack\n8) add original detergent into the detergent dispenser\n9) close the dishwaster\n10) select the cycle and start dishwasher\n11) after the dishwasher cycle is complete and the\ndishwasher has stopped, wait a few minutes for the dishes\nto cool down\n12) make sure the plate is clean and dry, otherwise\ngo into step 8)\n13) return all clean utensils to the finished location\nWash one plate and one fork\nMaterial: original detergent\n0) grasp the dirty plate\n1) remove large particle from the plate\n2) open the dishwasher\n3) pull out the rack\n4) put the plate on the third rack\n5) grasp the fork\n6) remove large particle from the fork\n7) put the fork on the first rack\n8) add original detergent into the detergent dispenser\n9) close the dishwaster\n10) select the cycle and start dishwasher\n11) after the dishwasher cycle is complete and the\ndishwasher has stopped, wait a few minutes for the dishes\nto cool down\n12) make sure the plate and fork are clean and dry,\notherwise go into step 8)\n13) return all clean utensils to the finished location\nAvailable location we have now:\n* first rack for forks and small kitchen utensils\n* second rack for bowl/cup\n* third rack for plate/big kitchen utensils\nAvailable material we have now:\nrose detergent, original detergent\nTask Guidelines 2: dishwashing\nUser Request\nTask Type\nHigh-level Planning\nWash one dirty plate with rose flavor.\nExisted\n11/11\nPlease wash 1 dirty bowl with rose flavor.\nZero-shot easy\n11/11\nPlease clean the 2 dirty cups.\nZero-shot easy\n14/14\nWash all forks, there are 3.\nZero-shot easy\n17/17\nCan you wash 2 plates? (New request: Can you wash another?)\nZero-shot easy\n17/17\nPlease wash 2 forks and one bowl.\nZero-shot moderate\n17/17\nMay you wash 2 cups and 2 plates?\nZero-shot moderate\n20/20\nPlease wash 2 fork, 2 plate and 2 bowl.\nZero-shot hard\n27/27\nWash 2 plates,1 bowl, 1 fork and 1 knife with rose flavor.\nZero-shot hard\n23/23\nWash one dirty plate with lemon flavor\nUnavailable material\n-\nTotal\n-\n100%\nTABLE III: Generalization to dishwashing task. We only need to change the text guidelines to make an accurate high-level plan. Since\nusing the dishwasher to clean the dishes doesn\u2019t contain misleading material or content, the high-level planning rate is 100%. Please note\nthat different utensils should be placed in different locations in the dishwasher, while ITP remains resilient in generating precise plans for\neach step, ensuring the correct order and appropriate location for different utensils. We envision the versatility of ITP\u2019s capabilities being\napplicable to a wide range of tasks.\nD. Generalization on Other Tasks\nITP is simple to adapt to new tasks. The system is prin-\ncipally reliant on task guidelines during high-level planning\nand predefined function during low-level execution. This\nstructure negates the need for intricate code implementation\nexamples, subsequently making the system easier to adapt to\nnew tasks. This structure negates the need for intricate code\nimplementation examples, subsequently making the system\u2019s\ngeneralization to other tasks remarkably straightforward. Refer\nto Figure 3 for the necessary components that need to be\nadapted. For adapting to a new task, only the Task Guidelines\nfor task specification and documentation for the provided Low-\nlevel Skills need to be modified. Optionally, the Prompts for\nthe high and low level planner can also be tuned.\nWe adapt our system to study the high level task planning\ncapabilities of a completely distinct task: dishwashing. We\nsimply replace the task guidelines for \u2018making a drink\u2019\nwith \u2018dishwashing\u2019 and add function definitions that are\nneeded for low-level execution. We show the dishwashing\ntask guidelines below: We evaluate the generalization ability\non two criteria: how many high-level steps are generated\ncorrectly and whether all the steps align with the ground\ntruth (referred to as Completed Status). We show our results\nin Table III. We find out that ITP performs very well on\nthe novel dishwashing task. It has the capability not only to\nproduce precise and novel instructions for new objectives but\nalso to exhibit resilience when faced with entirely different\ntasks.\nV. DISCUSSION\nConclusion. In this paper, we propose a simple yet\neffective system, ITP, which melds the capabilities of Large\nLanguage Models in an interactive system that constructs\nplans, and performs tasks centered around the users needs.\nEncouragingly, it precisely interprets user requests, generates\npertinent step-by-step plans, and achieves the desired outcome\n\u2014 a testament to the potential of such systems for real-world\napplications. We embody our system in a robot designed\nto make various drinks according to user preferences and\nadeptly demonstrate its ability to respond to feedback during\nexecution. Our system is capable in the context of interactive\ntask planning and replanning for robotics.\nLimitations and Future Work. While ITP provides a\nworking proof of concept of an interactive robot system,\nthere is room for enhancing its capabilities with more\npowerful robot skills to tackle more intricate tasks. Similarly,\nthe integration of more precise visual information that\nleverages 3D information would significantly elevate the\nrobot\u2019s proficiency in understanding, planning, and interacting\nwith its surroundings. We hope that our open-source system\ncould stimulate further exploration of how established and\nemerging models can be harnessed to advance the realm of\nreal-world robotics.\nACKNOWLEDGMENT\nPhilipp Wu was supported in part by the NSF Graduate Re-\nsearch Fellowship Program. We thank the Machine Common\nSense project and ONR MURI award number N00014-21-1-\n2801. We thank Valts Blukis for the insightful and valuable\ndiscussion.\nREFERENCES\n[1] OpenAI, \u201cGpt-4 technical report,\u201d 2023.\n[2] A. Zeng, M. Attarian, B. Ichter, K. Choromanski, A. Wong, S. Welker,\nF. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, et al., \u201cSocratic\nmodels: Composing zero-shot multimodal reasoning with language,\u201d\narXiv preprint arXiv:2204.00598, 2022.\n[3] W. Huang, F. Xia, D. Shah, D. Driess, A. Zeng, Y. Lu, P. Florence,\nI. Mordatch, S. Levine, K. Hausman, et al., \u201cGrounded decoding:\nGuiding text generation with grounded models for robot control,\u201d arXiv\npreprint arXiv:2303.00855, 2023.\n[4] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\nJ. Thomason, and A. Garg, \u201cProgprompt: Generating situated robot\ntask plans using large language models,\u201d in 2023 IEEE International\nConference on Robotics and Automation (ICRA).\nIEEE, 2023, pp.\n11 523\u201311 530.\n[5] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,\nand A. Zeng, \u201cCode as policies: Language model programs for\nembodied control,\u201d in 2023 IEEE International Conference on Robotics\nand Automation (ICRA).\nIEEE, 2023, pp. 9493\u20139500.\n[6] M. Ghallab, D. Nau, and P. Traverso, Automated Planning: Theory\nand Practice, ser. The Morgan Kaufmann Series in Artificial\nIntelligence.\nAmsterdam:\nMorgan\nKaufmann,\n2004.\n[Online].\nAvailable: http://www.sciencedirect.com/science/book/9781558608566\n[7] B. Bonet and H. Geffner, \u201cPlanning as heuristic search,\u201d Artificial\nIntelligence, vol. 129, no. 1, pp. 5\u201333, 2001.\n[8] Y. Jiang, S. Zhang, P. Khandelwal, and P. Stone, \u201cTask planning in\nrobotics: an empirical comparison of pddl-based and asp-based systems,\u201d\n2019.\n[9] M. Ghallab, A. Howe, C. Knoblock, D. Mcdermott, A. Ram,\nM. Veloso, D. Weld, and D. Wilkins, \u201cPDDL\u2014The Planning\nDomain Definition Language,\u201d 1998. [Online]. Available: http:\n//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.212\n[10] V. Lifschitz, \u201cWhat is answer set programming?\u201d in Proceedings of\nthe 23rd National Conference on Artificial Intelligence - Volume 3, ser.\nAAAI\u201908.\nAAAI Press, 2008, p. 1594\u20131597.\n[11] R. E. Fikes and N. J. Nilsson, \u201cStrips: A new approach to the application\nof theorem proving to problem solving,\u201d in Proceedings of the 2nd\nInternational Joint Conference on Artificial Intelligence, ser. IJCAI\u201971.\nSan Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1971, p.\n608\u2013620.\n[12] C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling,\nand T. Lozano-P\u00b4erez, \u201cIntegrated task and motion planning,\u201d 2020.\n[13] M.\nMansouri,\nF.\nPecora,\nand\nP.\nSch\u00a8uller,\n\u201cCombining\ntask\nand motion planning: Challenges and guidelines,\u201d Frontiers in\nRobotics\nand\nAI,\nvol.\n8,\n2021.\n[Online].\nAvailable:\nhttps:\n//www.frontiersin.org/articles/10.3389/frobt.2021.637888\n[14] T. Sim\u00b4eon, J.-P. Laumond, J. Cort\u00b4es, and A. Sahbani, \u201cManipulation\nplanning with probabilistic roadmaps,\u201d The International Journal of\nRobotics Research, vol. 23, no. 7-8, pp. 729\u2013746, 2004. [Online].\nAvailable: https://doi.org/10.1177/0278364904045471\n[15] C. R. Garrett, T. Lozano-P\u00b4erez, and L. P. Kaelbling, \u201cFFRob:\nLeveraging\nsymbolic\nplanning\nfor\nefficient\ntask\nand\nmotion\nplanning,\u201d The International Journal of Robotics Research, vol. 37,\nno.\n1,\npp.\n104\u2013136,\nnov\n2017.\n[Online].\nAvailable:\nhttps:\n//doi.org/10.1177%2F0278364917739114\n[16] C. R. Garrett, T. Lozano-P\u00b4erez, and L. P. Kaelbling, \u201cPddlstream:\nIntegrating symbolic planners and blackbox samplers via optimistic\nadaptive planning,\u201d 2020.\n[17] F. Bacchus and Q. Yang, \u201cThe downward refinement property,\u201d in\nProceedings of the 12th International Joint Conference on Artificial\nIntelligence - Volume 1, ser. IJCAI\u201991.\nSan Francisco, CA, USA:\nMorgan Kaufmann Publishers Inc., 1991, p. 286\u2013292.\n[18] E. Plaku and G. D. Hager, \u201cSampling-based motion and symbolic\naction planning with geometric and differential constraints,\u201d in 2010\nIEEE International Conference on Robotics and Automation, 2010, pp.\n5002\u20135008.\n[19] L. P. Kaelbling and T. Lozano-P\u00b4erez, \u201cHierarchical task and motion\nplanning in the now,\u201d in 2011 IEEE International Conference on\nRobotics and Automation, 2011, pp. 1470\u20131477.\n[20] L.\nP.\nKaelbling\nand\nT.\nLozano-P\u00b4erez,\n\u201cIntegrated\ntask\nand\nmotion planning in belief space,\u201d Int. J. Rob. Res., vol. 32,\nno. 9\u201310, p. 1194\u20131227, aug 2013. [Online]. Available: https:\n//doi.org/10.1177/0278364913484072\n[21] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, \u201cLanguage models\nas zero-shot planners: Extracting actionable knowledge for embodied\nagents,\u201d in International Conference on Machine Learning.\nPMLR,\n2022, pp. 9118\u20139147.\n[22] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David,\nC. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al., \u201cDo as i\ncan, not as i say: Grounding language in robotic affordances,\u201d arXiv\npreprint arXiv:2204.01691, 2022.\n[23] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\nS. Rusinkiewicz, and T. Funkhouser, \u201cTidybot: Personalized robot as-\nsistance with large language models,\u201d arXiv preprint arXiv:2305.05658,\n2023.\n[24] Z. Liu, A. Bahety, and S. Song, \u201cReflect: Summarizing robot\nexperiences for failure explanation and correction,\u201d arXiv preprint\narXiv:2306.15724, 2023.\n[25] A. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu,\nL. Takayama, F. Xia, J. Varley, et al., \u201cRobots that ask for help:\nUncertainty alignment for large language model planners,\u201d arXiv\npreprint arXiv:2307.01928, 2023.\n[26] H. Ha, P. Florence, and S. Song, \u201cScaling up and distilling\ndown: Language-guided robot skill acquisition,\u201d arXiv preprint\narXiv:2307.14535, 2023.\n[27] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and\nA. Anandkumar, \u201cVoyager: An open-ended embodied agent with large\nlanguage models,\u201d arXiv preprint arXiv: Arxiv-2305.16291, 2023.\n[28] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su,\nJ. Zhu, et al., \u201cGrounding dino: Marrying dino with grounded pre-\ntraining for open-set object detection,\u201d arXiv preprint arXiv:2303.05499,\n2023.\n[29] M. Caron, H. Touvron, I. Misra, H. J\u00b4egou, J. Mairal, P. Bojanowski, and\nA. Joulin, \u201cEmerging properties in self-supervised vision transformers,\u201d\nin Proceedings of the International Conference on Computer Vision\n(ICCV), 2021.\n[30] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nVoss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, \u201cLanguage models are few-shot learners,\u201d 2020.\n"
  },
  {
    "title": "Video Language Planning",
    "link": "https://arxiv.org/pdf/2310.10625.pdf",
    "upvote": "7",
    "text": "VIDEO LANGUAGE PLANNING\nYilun Du\u2020\u2021, Mengjiao Yang\u2020\u00a7, Pete Florence\u2020, Fei Xia\u2020, Ayzaan Wahid\u2020, Brian Ichter\u2020,\nPierre Sermanet\u2020, Tianhe Yu\u2020, Pieter Abbeel\u00a7, Joshua B. Tenenbaum\u2021, Leslie Kaelbling\u2021\nAndy Zeng\u2020, Jonathan Tompson \u2020\nGoogle Deepmind\u2020, Massachusetts Institute of Technology\u2021, UC Berkeley\u00a7\nhttps://video-language-planning.github.io/\nABSTRACT\nWe are interested in enabling visual planning for complex long-horizon tasks in\nthe space of generated videos and language, leveraging recent advances in large\ngenerative models pretrained on Internet-scale data. To this end, we present video\nlanguage planning (VLP), an algorithm that consists of a tree search procedure,\nwhere we train (i) vision-language models to serve as both policies and value\nfunctions, and (ii) text-to-video models as dynamics models. VLP takes as input a\nlong-horizon task instruction and current image observation, and outputs a long\nvideo plan that provides detailed multimodal (video and language) specifications\nthat describe how to complete the final task. VLP scales with increasing compu-\ntation budget where more computation time results in improved video plans, and\nis able to synthesize long-horizon video plans across different robotics domains \u2013\nfrom multi-object rearrangement, to multi-camera bi-arm dexterous manipulation.\nGenerated video plans can be translated into real robot actions via goal-conditioned\npolicies, conditioned on each intermediate frame of the generated video. Ex-\nperiments show that VLP substantially improves long-horizon task success rates\ncompared to prior methods on both simulated and real robots (across 3 hardware\nplatforms).\n1\nINTRODUCTION\nIntelligently interacting with the physical world involves planning over both (i) high-level semantic\nabstractions of the task (i.e., what to do next), as well as the (ii) low-level underlying dynamics of\nthe world (i.e., how the world works). Factorizing the planning problem into two parts, one driven\nby task-specific objectives and the other a task-agnostic modeling of state transitions, is an idea that\nis pervasive and fundamental. This factorization drives much of the classic work in robotics from\nintegrating task and motion planning (Cambon et al., 2009; Wolfe et al., 2010; Kaelbling & Lozano-\nP\u00b4erez, 2011) to deriving control policies that can perform complex manipulation tasks over long time\nhorizons such as tidying a dining table or rearranging a collection of objects to build new structures.\nPre-trained large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022) have shown\nto be capable of generating high-level step-by-step plans for long-horizon tasks over symbolic (often\nlinguistic) abstractions of the task (Huang et al., 2022a; Ahn et al., 2022), but this is only part of the\nsolution. LLMs are restricted by what they can represent in text, and struggle with grounding i.e.,\nreasoning over shapes, physics, and constraints of the real world (Tellex et al., 2020; Huang et al.,\n2023). LLMs can be integrated into larger vision-language models (VLMs) (Driess et al., 2023) that,\nwhen trained on sufficient data, can respect physical constraints observed in image inputs to generate\nmore feasible plans that may be less likely to command the robot to perform impossible actions, or\nmanipulate inaccessible objects. However, existing VLMs are predominantly trained on static image\ncaptioning and Q&A datasets \u2013 consequently, they continue to struggle to reason over dynamics e.g.,\nhow objects may move or collide with one another over time.\nMeanwhile, recent text-to-video models trained on the wealth of videos on the Internet (Villegas\net al., 2022; Ho et al., 2022), have demonstrated an ability to learn the dynamics and motions of\nobjects by synthesizing detailed video predictions of the future (Du et al., 2023b). Existing video\nmodels can only generate short time horizon clips without losing visual fidelity, and whether they\ncan be applied for long-horizon planning remains unclear. Nevertheless, they exhibit properties that\nare complementary to VLMs in that they (i) can model the low-level visual dynamics of objects in\n1\narXiv:2310.10625v1  [cs.CV]  16 Oct 2023\nStep 1: push blue triangle to \u2026\nStep 1: push red star to left \u2026\nStep 2: (re-plan)\nt\nt + k\nt current\nt goal\nStep 2: (re-plan)\nStep 2: (re-plan)\nFigure 1: Video Language Planning uses forward tree search via vision-language models and text-to-video\nmodels to construct long-horizon video plans. From an image observation, the VLM policy (top left) generates\nnext-step text actions, which a video model converts into possible future image sequences (top right). Future\nimage states are evaluated using a VLM heuristic function (bottom left), and the best sequence is recursively\nexpanded with tree search (middle). Video plans can be converted to action execution with goal-conditioned\npolicies (bottom right).\nways that are more information-rich than text, and (ii) can absorb another source of Internet data e.g.,\nYouTube videos. This leads to the natural question of how to build a planning algorithm that can\nleverage both long-horizon abstract planning from LLMs / VLMs and detailed dynamics and motions\nfrom text-to-video-models.\nIn this work, we propose to integrate vision-language models and text-to-video models to enable video\nlanguage planning (VLP), where given the current image observation and a language instruction, the\nagent uses a VLM to infer high-level text actions, and a video model to predict the low-level outcomes\nof those actions. Specifically, VLP (illustrated in Fig. 1) synthesizes video plans for long-horizon\ntasks by iteratively: (i) prompting the VLM as a policy to generate multiple possible next-step text\nactions, (ii) using the video model as a dynamics model to simulate multiple possible video rollouts\nfor each action, and (iii) using the VLM again but as a heuristic function to assess the favorability of\neach rollout in contributing task progress, then recursively re-planning with (i).\nThe combination of both models enables forward tree search over the space of possible video se-\nquences to discover long-horizon plans (of hundreds of frames) that respect visual dynamics. In partic-\nular, VLP offers advantages in that it (i) can generate higher quality plans at inference time by expand-\ning the branching factor of the search, allowing plan quality to scale with increasing compute budget,\nand (ii) benefits from training on incomplete language-labeled video data, which may contain short-\nhorizon snippets (that can be re-composed and sequenced into long-horizon ones), or segments of\nvideos with missing language labels (but contain dynamics that the video model can still learn from).\nExperiments in both simulated and real settings (on 3 robot hardware platforms) show that VLP\ngenerates more complete and coherent multimodal plans than baselines, including end-to-end models\ntrained directly to generate long videos conditioned on long-horizon task instructions. VLPs exhibit\nimproved grounding in terms of the consistency of scene dynamics in video plans, and when used in\nconjunction with inverse dynamics models or goal-conditioned policies to infer control trajectories\n(Du et al., 2023b), can be deployed on robots to perform multi-step tasks \u2013 from picking and stowing\na variety of objects over countertop settings, to pushing groups of blocks that rearrange them into new\nformations. In terms of execution success, VLP-based systems are far more likely to achieve task\ncompletion for long-horizon instructions than state-of-the-art alternatives, including PaLM-E (Driess\net al., 2023) and RT-2 (Brohan et al., 2023) directly fine-tuned for long-horizon tasks. We also observe\nthat when co-trained on Internet-scale data, VLP generalizes to new objects and configurations.\n2\nOur main contributions are: (i) video language planning, a scalable algorithm for generating long-\nhorizon video plans by synergising vision-language models and text-to-video models, (ii) experiments\nthat show how VLP enables both simulated and real robots to perform complex long-horizon manipu-\nlation tasks, exhibiting task completion rates that often exceed that of the next best available approach\nby a significant margin, and (iii) ablations that study modes of generalization, as well as how VLP\nscales with more compute. VLP presents a modern re-examination of visual planning by integrating\nlarge generative models pretrained on Internet-scale data, and is not without limitations. We discuss\nthese in Sec. 5 and videos will be made available at: https://video-language-planning.github.io/\n2\nVIDEO LANGUAGE PLANNING\nOur planning system, VLP, takes a visual observation x0 of a scene and a natural language goal g and\ninfers a video plan {xt}1:T , where each image xt is as a sub-goal to accomplish g. We assume that\neach image xt serves as an accurate representation of world state and use a image goal-conditioned\npolicy as a controller to infer low level control actions u to reach each image state.\nBelow, we first discuss how vision-language and video models are used in VLP as planning submod-\nules Sec. 2.1. Next, we talk about our tree-search algorithm using vision-language and video building\nblocks in Sec. 2.2. Finally, in Sec. 2.3, we discuss how we can convert video plans into policies to\naccomplish each long-horizon task.\n2.1\nUSING VISION-LANGUAGE AND VIDEO MODELS AS PLANNING SUBMODULES\nWe first discuss how to use vision-language and video models as sub-modules in VLP to synthesize\nlong horizon plans. At a high level, we use the multimodal processing power of VLMs to propose\nabstract text actions ai to execute given goals and images. We then use the dynamics knowledge\nof video models to accurately synthesize possible future world states xi\n1:T when abstract actions\nare executed. Finally, we use a VLM to process possible future world states xi\n1:T and assess which\nsequence x1:T and associated actions are the most promising to complete a task.\nVision-Language Models as Policies.\nGiven a high-level goal g, VLP searches over a space of\npossible abstract actions; these text actions a are generated by a VLM policy \u03c0VLM(x, g) \u2192 a that\nis conditioned both on the goal g and an image x of the current state. We implement this policy\nfollowing Driess et al. (2023) and query the VLM for a natural language action to take given as context\nthe natural language goal and a tokenized embedding of the current image (Fig. 1 Top Left). We\nexperiment with two different strategies for constructing this policy. In the first, we provide the VLM\na set of example text action labels and ask the VLM to predict possible actions to accomplish a goal.\nIn the second, we finetune the PaLM-E model on randomly selected short trajectory snippets x1:S\nlabeled with abstract actions inside a long trajectory x1:H that accomplishes a long horizon goal g.\nVideo Models as Dynamics Models.\nIn order to perform the high-level search, given an image x\nof a current state and a language description of an abstract action, we need to predict the concrete\nresulting state. In addition, to generate low-level controls that instantiate this abstract action, we need\na feasible sequence of low-level states that \u201cinterpolate\u201d between the current state and the resulting\nstate. We obtain both of these things from a text-to-video model fVM(x, a), which takes an image x\nand a short horizon text instruction a and outputs a short synthesized video x1:S starting at the image\nobservation x0 (Fig. 1 Top Right) following Du et al. (2023b). We construct this text-to-video model\nby training on a set of short image trajectory snippets x1:T and associated language labels a.\nVision-Language Models as Heuristic Functions.\nTo effectively prune branches in search, we\nuse a VLM to implement a heuristic function HVLM(x, g) which takes as input an image observation\nx and a natural language goal description g and outputs a scalar \u201cheuristic\u201d predicting the number\nof actions required to reach a state satisfying goal g from current state x (Fig. 1 Bottom Left). To\nconstruct this heuristic function, we finetune a PaLM-E model using long trajectory snippets x1:H\nwhich accomplish a long horizon goal g, and train it to predict, given an image in the subtrajectory\nxt, the number of steps left until the end of the trajectory snippet. The negated number of predicted\nsteps to goal completion from VLM is used to implement HVLM(x, g) (so that high heuristic value\ncorresponds to being close to goal completion).\n2.2\nPLANNING WITH VISION-LANGUAGE MODELS AND VIDEO MODELS\nGiven a combination of modules discussed in Sec. 2.1, directly applying the \u03c0VLM to infer text actions\na to reach goal g is not sufficient, as \u03c0VLM is not able to perform sufficiently accurate long-horizon\n3\nAlgorithm 1 Decision Making with VLP\n1: Input: Current visual observation x0, Language goal g\n2: Functions: VLM Policy \u03c0VLM(x, g), Video Model fVM(x, a), VLM Heuristic Function HVLM(x, g)\n3: Hyperparameters: Text-Branching factor A, Video-Branching factor D, Planning Beams B, Planning\nhorizon H\n4: plans \u2190 [[x0] \u2200 i \u2208 {1 . . . B}]\n# Initialize B Different Plan Beams\n5: for h = 1 . . . H do\n6:\nfor b = 1 . . . B do\n7:\nx \u2190 plans[b][\u22121]\n# Get the Latest Image State in the Plan Beam\n8:\na1:A \u2190 \u03c0(x, g)\n# Generate A Different Text Actions\n9:\nvideo branches \u2190 [fVM(a, ai) for i in (1 . . . A) for j in (1 . . . D)]\n10:\nplans[b].append(argmax(video branches, HVLM))\n# Add Video with Highest Value to Plan\n11:\nend for\n12:\nmax idx, min idx \u2190 argmax(plans, HVLM), argmin(plans, HVLM)\n13:\nplans[min idx] \u2190 plans[max idx]\n# Periodically Replace the Lowest Value Plan\n14: end for\n15: plan \u2190 argmax(plans, HVLM)\n# Return Highest Value Plan\nreasoning to select actions that are helpful in the long run. Furthermore, there are many possible\nlow-level image sub-sequences that correspond to different ways to perform a, but it is critical to\nselect one that is consistent with the rest of the actions that must be taken to reach g.\nInstead, we propose to search for a sequence of actions to reach g, corresponding to finding a\nlong-horizon video plan x1:H which optimizes\nx\u2217\n1:H =\narg max\nx1:H\u223cfVM,\u03c0VLM\nHVLM(xH, g).\n(1)\nA separate procedure is then used to instantiate control actions u to enact the optimized video plan\nx\u2217\n1:H. To sample long-horizon video plans x1:H, we first synthesize a short horizon video plan x1:S\nfrom a starting image x through x1:S = fVM(x, \u03c0VLM(x, g)) and autoregressively extend to a full\nlong-horizon video plan by recursively applying fVM(x, \u03c0VLM(x, g)) on the final synthesized image\nstate xS. To optimize across video plans in Eqn (1), we use a tree-search procedure based on parallel\nhill climbing (Selman & Gomes, 2006) (illustrated in Algorithm 1).\nOur planning algorithm initializes a set of B parallel video plan beams. At each step of the planning\nhorizon, for each video beam, we first sample a set of A actions using \u03c0VLM(x, g), and for each action\nwe synthesize D different videos using fVM(x, a). We then use our heuristic function HVLM(x, g) to\nselect the generated video with the highest heuristic among the A \u00d7 D generated videos and extend\nthe corresponding video plan beam with this generated video. Over the course of plan generation,\ncertain video plan beams will obtain high heuristic value and be more promising to explore. Therefore,\nevery 5 steps, we discard the beam with the lowest value and replicate its video plan with the beam\nwith the highest value. Our final full long horizon video plan corresponds to the beam with highest\nheuristic value at the end of planning.\nPreventing Exploitative Model Dynamics.\nWhen our planning procedure optimizes the VLM\nheuristic function HVLM(x, g) it can exploit irregularities in the dynamics model fVM(x, a) to get\nartificially high estimates. For instance, the planning procedure can exploit videos from fVM(x, a)\nwhere key objects have teleported to desired locations or where the final image observation obscures\nundesirable portions of world state. To prevent over-exploitation of HVLM(x, g), during the planning\nprocedure in Algorithm 1, we discard generated videos from fVM(x, a) if they increase the the\nheuristic estimate HVLM(x, g) above a fixed threshold.\n2.3\nACTION REGRESSION FROM VIDEO THROUGH GOAL-CONDITIONED POLICIES\nGiven a synthesized video plan x1:H, to execute tasks, we must infer control actions u to reach\nsynthesized images. Prior work infers actions from synthesized videos by using an inverse-dynamics\nmodel on each synthesized frame (Du et al., 2023b).\nIn many settings, a single action may not be sufficient to directly reach the next synthesized image,\ni.e. if you need to remove the cap off a toothbrush, and even in settings in which this is the case,\nit may be difficult to precisely predict the correct action to reach the next frame. To reduce the\nburden on the inverse dynamics model, we propose to use a short-horizon goal-conditioned policy\n\u03c0control(x, xg), which given the current image observation x and next frame in a video plan xg outputs\n4\n1. Separate the blue cube from \nthe yellow star\n2. Move the blue cube into the \nyellow pentagon\n3. Move the blue cube into the \nred crescent\n4. Move the arm to the left of \nthe red crescent\n5. move the blue blocks and \nyellow pentagon towards the \nright side\n6. Push the yellow pentagon \ninto the yellow star\n7. Move the arm towards the \ntop center\n8. Push the green star into the \ngreen cube\n9. Move your arm towards the \ngreen blocks\n10. Push red blocks slightly to \nthe down\ngroup blocks by color\nmake a horizontal line\n1. Move the red circle to the left \nof the yellow hexagon\n2. Move the green circle closer \nto the red circle\n3. Move the blue triangle to the \ntop left of the red circle\n4. Move the blue cube to the \nleft of the blue triangle\n5. Move the green circle to the \ncenter\n6. Move the green circle \ntowards the yellow heart\n7. Move the blue triangle to the \nright of the green circle\n8. Slide the blue cube towards  \nthe blue triangle\n11. Move the yellow hexagon \ncloser to the red circle\n9. Push the red circle closer to \nthe blue cube\u2026\nFigure 2: Long Horizon Video Plan. Long horizon video plans generated by VLP on both simulated and real\nimages. VLP is only given the initial image and language goal. Language subplans and other image frames are\ndirectly synthesized.\nSim Environment\nReal Environment\nMove\nGroup\nMake\nMove\nGroup\nMake\nModel\nArea\nColor\nLine\nArea\nColor\nLine\nUniPi\n2%\n4%\n2%\n4%\n12%\n4%\nVLP (No Value Function)\n10%\n42 %\n8%\n20%\n64%\n4%\nVLP (Ours)\n58%\n98%\n66%\n78%\n100%\n56%\nTable 1: Accuracy of Generated Video Plans. The percentage VLP and baselines are able to synthesize a full\nvideo plan which can fully complete tasks in simulation and real environments. VLP substantially outperforms\nboth UniPi and directly combining the VLM policy\na low level control action u that makes progress towards xg. For each frame in our video plan x1:H,\nthe goal-conditioned policy is executed for a fixed pre-specified number of timesteps. We train \u03c0control\nusing paired image and low level control snippets xi\n1:T and ui\n1:T , where we sample a random timestep\nt, a corresponding state xt, and future state xt+h, and train \u03c0control(xt, xt+h) to predict ut.\nReplanning.\nGiven a very long-horizon task, it is both difficult to use \u03c0control to accurately execute\nthe full video plan x1:H (due to accumulating error) and difficult to fully synthesize a plan that\ncompletely finishes a long-horizon task given a fixed planning horizon. To circumvent this issue, we\nuse receding horizon control strategy (Kwon & Han, 2005), where we generate videos plans with a\nfixed horizon (that might not fully complete the task), and then repeatedly regenerate/replan video\nplans with the same horizon after a fixed number of action executions.\n3\nEXPERIMENTAL RESULTS\nWe first evaluate the ability of VLP to synthesize long-horizon video plans for different tasks in\nSec. 3.1. We then investigate VLP\u2019s ability to execute generated video plans in various environments\nin Sec. 3.2. Finally, we further investigate generalization capabilities of VLP in Sec. 3.3.\n3.1\nLONG-HORIZON VIDEO SYNTHESIS\nBaselines.\nWe compare our approach with two other approaches for synthesizing long-horizon\nvideo plans. First, we consider training a text-to-video model fVM on long horizon text goals, as in\nUniPi (Du et al., 2023b), omitting the entire VLP planning process. Next, we consider synthesizing\nlong horizon video plans by chaining \u03c0VLM policy with fVM, without the heuristic function.\n5\nLanguage\nVideo\nLine\nBeams\nBranch\nBranch\nPerformance\n1\n1\n1\n4%\n1\n1\n4\n10%\n1\n4\n4\n22%\n2\n4\n4\n56%\nBeam 1, Branch 1\nBeam 2, Branch 16\nFigure 3: Video Accuracy vs Planning Budget. Left: VLP scales positively with more compute budget; it is\nbetter able to synthesize plans to solve tasks with more planning (i.e. with a higher beam-search branching factor).\nSuccess percentage reported on the make line task. Right: Qualitative illustration of video plans for making\na line generated without planning (Beam 1, Branch 1) compared to extensive planning (Beam 2, Branch 16).\n\u2715\nAction 1. Place apple in top drawer\nPut the fruits into \nthe top drawer\nAction 1. Open top drawer\nAction 1. Place banana in top drawer\nAction 2. Place banana in top drawer\nAction 3. Place coke can in top drawer\nAction 3. Place apple in top drawer\nAction 3. Close top drawer\nAction 4. Close top drawer\n\u2715\n\u2715\n\u2713\n\u2715\nFigure 4: Planning Tree on 7DoF Mobile Manipulator. VLP is able to prune unlikely language and video\nbranches to synthesize a coherent long-horizon video plan.\nObject Rearrangement.\nWe first illustrate video plans in the Language Table environment (Lynch\net al., 2023). We give as input to VLP a random image and randomly chosen language goal. We\nthen visualize the generated VLP plans (Fig. 2). We report the quantitative success of synthesizing\nlong-horizon videos given random starting images for each task in Language Table in Tab. 1. For\neach reported number, we generated a total of 50 videos from each method and visually assessed the\npercentage of time the video successfully solved the given task. VLP substantially outperforms the\nbaseline of directly synthesizing videos given a long-horizon prompt, indicating the importance of\nhierarchical structure. VLP further outperforms the ablation of only using a VLM policy with a video\nmodel, pointing to the effectiveness of the VLP planning procedure and including the value function.\nEffect of Search of Video Synthesis.\nWe analyze the effect of search in generating long-horizon\nvideos in Fig. 3 (left). We consider increasing the video branching, language branching and the\nbeams in the search procedure. We find that each increase of branching factor in search substantially\nincreases the success of synthesized long horizon plans. A qualitative illustration of the difference of\ngenerated plans with small and large branching factor is illustrated in Fig. 3 (right).\nPlanning on 7DoF Mobile Manipulators.\nWe qualitatively illustrate how we can generate plans\non a higher-DoF, 7DoF Mobile Manipulator in Fig. 4. Our planning system is able to generate videos\nof actions that both open and close drawers in order to satisfy specified text prompts.\nPlanning on Multicamera 14DoF Bi-Manual Manipulators.\nWe further illustrate how our\napproach can generate multi-view 4-camera videos of dexterous manipulation on the 14DoF bi-\nmanual ALOHA (Zhao et al., 2023) platform in Fig. 5. Our video model outputs videos across\nviews simultaneously (by concatenating each view channelwise), while our VLM policy and heuristic\nfunction takes as input top and side views. Our approach is able to synthesize multiview consistent\nplans which are able to both stack bowls, cups, and utensils.\n3.2\nLONG-HORIZON EXECUTION\nWe next evaluate the ability of VLP to not only generate plans as in Sec. 3.1, but to actually use\nplanning (and replanning) to execute long-horizon tasks in closed-loop environments.\nBaselines.\nWe compare our approach to a set of approaches to solve long-horizon tasks. (i) We\nconsider using a VLM to directly plan, using PaLM-E (Driess et al., 2023) to plan short horizon\ntext snippets to execute, which are converted to actions using a text-conditioned policy, conditioned\n6\nPick up the \npink bowl\nPlace pink bowl in \nblue bowl\nPick up green cup\nStack green cup on \ntop of blue cup\nPick up orange cup\n\u2026\n\u2026\n\u2026\n\u2026\nPush the bowl to \nthe center\nFigure 5: Multiview Video Plans for Dexterous Manipulation. Long horizon video plans (and associated\nlanguage subgoals) generated by VLP for solving the long horizon task of stacking everything in a table together.\nVLP is able to synthesize multiview video plans across 4 cameras, that are consistent with each other and with\ntask completion. The first 5 generated language subgoals goals are illustrated as well as the final generated goal\nimage. VLP is only given first image.\nMove to Area\nGroup by Color\nMake Line\nModel\nReward\nCompletion\nReward\nCompletion\nReward\nCompletion\nUniPi (Du et al., 2023b)\n30.8\n0%\n44.0\n4%\n44.0\n4%\nLAVA (Lynch et al., 2023)\n59.8\n22%\n50.0\n2%\n33.5\n0%\nRT-2 (Brohan et al., 2023)\n18.5\n0%\n46.0\n26%\n36.5\n2%\nPALM-E (Driess et al., 2023)\n36.5\n0%\n43.5\n2%\n26.2\n0%\nVLP (Ours)\n87.3\n64%\n95.8\n92%\n65.0\n16%\nTable 2: Execution Performance on Long Horizon Tasks. VLP is able to accurately execute actions for\ndifferent long-horizon synthetic language table tasks. VLP substantially outperforms all existing methods.\nMove all \nblocks to the \nbottom left \ncorner\nGroup \nBlocks By \nColor\nMake a \nHorizontal \nLine in the \nCenter\nFigure 6: Simulation Execution. Illustration of execution of VLP on different simulated environments. VLP is\nable to accomplish different long horizon goals.\non generated text snippets from PaLM-E. We also (ii) compare with UniPi (Du et al., 2023b),\nwhere videos are directly generated by a text-to-video model trained on long-horizon text goals and\nconverted to actions using our goal-conditioned policy. Next, we consider (iii) directly learning\nlanguage-conditioned behavioral cloning policy on long-horizon text and actions, using the codebase\nand architecture of the LAVA model Lynch et al. (2023). Finally, we (iv) compare with leveraging\nexisting vision-language models for control, and train the RT2 model Brohan et al. (2023) on\nlong-horizon text and actions.\nQuantitative Results.\nWe evaluate each approach quantitatively on moving all blocks to different\nareas of the board, grouping blocks by color, or making blocks in a line (details on quantitative\nevaluation in Appendix). We report quantitative results in Tab. 2, and find that our approach\nsubstantially outperforms all baseline methods. As the task horizon of each task is very long (around\n1500 steps), we found that many baseline methods would become \u201cstuck\u201d and stop acting effectively.\nWe illustrate example executions using VLP in Fig. 6.\n7\nMove all \nBlocks to the \nRight Corner\nGroup \nBlocks By \nColor\nMake a \nHorizontal \nLine\nFigure 7: Real Execution. Illustration of execution VLP on real world robots. VLP is able to accomplish\ndifferent long horizon goals when executed on real robots.\n1. Open top drawer\n2. Place banana in top drawer\n3. Place apple in top drawer\n4. Close top drawer\nTask: Put the fruits \ninto the top drawer\nFigure 8: 7DoF Mobile Robot Execution. VLP is able to execute complex, long horizon plans on mobile robot.\nPlanning\nBranching\nLine\nLine\nBeams\nHorizon\nFactor\nScore\nCompletion\n1\n1\n4\n48.9\n0%\n1\n1\n16\n53.3\n2%\n1\n2\n16\n58.1\n8%\n2\n2\n16\n65.0\n16%\nTable 3: Execution Accuracy vs Planning Budget.\nVLP is able to more accurately execute video plans to\nsolve tasks with a larger amount of planning. Success\npercentage reported on the make line task.\nGroup Color\nGroup Color\nAction Inference\nScore\nCompletion\nInverse Dynamics\n89.7\n80%\nGoal Policy (Last)\n85.0\n66%\nGoal Policy (Every)\n95.8\n92%\nTable 4: Extracting Actions From VLP Video Plans.\nComparison of using inverse dynamics or applying a\ngoal-conditioned policy to either the last frame or ev-\nery frame of synthesized short-horizon video. Success\npercentage reported on the group by color task.\nEffect of Planning.\nNext, we analyze the effect of the amount of planning on execution success\nrates in Tab. 3. We find that increasing both the planning horizon and the branching factor of planning\nsubstantially improves the success of task execution (at the cost of inference time).\nAblations of Goal-Conditioned Policy\nWe further conduct experiments on different approaches to\nextracting actions from videos in Tab. 4. We find that using a goal-conditioned policy conditioned on\neach intermediate frame in a synthesized video leads to the best overall performance (outperforming\nusing a goal-conditioned policy sparsely on the end frames of each short-horizon video).\nReal Execution.\nWe provide executions of VLP on multiple real-world robots in Fig. 7 and Fig. 8.\nAs in Fig. 7, VLP is able to effectively execute each shown long-horizon task on a real Language\nTable robot. We further provide executions of generated video plans of our approach on the 7DoF\nmobile manipulator in Fig. 8. Similarly we find that a goal-conditioned policy can realize plans.\n3.3\nGENERALIZATION\nGeneralization to Lighting and Objects.\nIn VLP, policy execution is abstracted into visual\ngoal generation followed by a goal-conditioned controller. With this abstraction, a video model can\nsimply focus on capturing the visual dynamics objects, while a goal-conditioned policy needs to\nfocus only on relevant visual details to achieve the next (nearby) goal. We found that this enables\nVLP to generalize well, as the video model is able to visually generalize to new images, while the\npolicy is able to generalize well to nearby new visual goals. In Fig. 9 (top), VLP is able to generalize\nthe task of putting all objects in top right corner, to three new objects, a rubber donut and cupcake\nand a wooden hexagon. In Fig. 9 (bottom) VLP is further able to generalize to lighting conditions\n8\nNew \nObjects\nNew \nLighting\nFigure 9: Generalization to Objects and Lighting. VLP is able to generalize execution to scenes with three\nnew objects (top) consisting of a wooden yellow hexagon, rubber donut and rubber cupcake. VLP is able to also\ngeneralize to a robot placed in a new office (bottom) with different lighting conditions (a lot more lighting on the\nright side of the board) and similarly execute tasks.\nGenerated \nVideo\nReal \nRollout\nNovel instruction: Pick snicker energy bar\nNovel instruction: Move moose toy near green pear\nFigure 10: Task Generalization VLP can generalize to new tasks on unseen objects using internet knowledge.\nsubstantially different than the ones the model was trained on and can be successfully deployed to a\nnew robot in a different building location.\nGeneralization to New Tasks\nIn VLP, both VLM and text-to-video models may be pre-trained on\na vast amount of Internet data. In Fig. 10, we train both VLM and text-to-video models on a large\nmix of datasets and illustrate how it further generalizes and executes new tasks on unseen objects.\n4\nRELATED WORK\nThere is a great deal of recent work in leveraging large pretrained foundation models for decision-\nmaking agents (Yang et al., 2023b) \u2013 in particular, using LLMs (and the commonsense knowledge\nthey store) to infer actions represented in text. For example, given language instructions, LLMs\ncan be prompted to generate high-level step-by-step plans (Huang et al., 2022a; Ahn et al., 2022;\nWang et al., 2023a) \u2013 each step mapped robot actions, invoked with pre-trained policies or existing\nAPIs (Liang et al., 2023). Using existing VLMs (Chen et al., 2022), plans can also be conditioned\non visual inputs, represented with language descriptions (Zeng et al., 2022; Huang et al., 2022b)\nor token embeddings co-trained with LLMs (Driess et al., 2023). Nevertheless, VLMs are often\npretrained on static image datasets (i.e., image-in, text-out), and subsequently (i) are limited to plans\nthat can be expressed in text, and (ii) may struggle to reason about the dynamics of the world.\nVideo models, on the other hand, exhibit potential to generate informative image sequences of the\nfuture, and there exists a body of work using them as dynamics models to capture object motion across\nimage states (Finn et al., 2016; Xue et al., 2016; Oprea et al., 2020; Oh et al., 2015). Generating\nhigh-quality videos over long time horizons is a known challenge (Babaeizadeh et al., 2021; Saxena\net al., 2021); however, recent progress in larger models and text-conditioning give rise to a new\ngeneration of text-to-video models that can generate detailed image frames (Ho et al., 2022; Villegas\net al., 2022), that can drive policies for decision making (Du et al., 2023b). However, these works\nfocus predominantly on short-horizon videos. Our work investigates using text-to-video models\ntogether with VLMs to produce long-horizon video plans (potentially predicting hundreds of frames\ninto the future), leveraging the scalability of tree search to plan in the space of videos and language.\nOur work can be viewed as bringing together two families of generative foundation models to\ncompose new capabilities \u2013 a strategy shown to be effective for applications such as 2D image\nsynthesis (Du et al., 2020; Liu et al., 2021; Nie et al., 2021; Liu et al., 2022; Wu et al., 2022; Du\net al., 2023a; Wang et al., 2023b), 3D synthesis (Po & Wetzstein, 2023), video synthesis (Yang et al.,\n2023a), trajectory planning (Du et al., 2019; Urain et al., 2021; Gkanatsios et al., 2023; Yang et al.,\n2023c) and multimodal perception (Li et al., 2022; Zeng et al., 2022). Most similar to our work,\n9\nHiP (Ajay et al., 2023) combines language models, video models, and action models for hierarchical\nplanning. In contrast, our work uses a forward search procedure to combine the strengths of a VLM\nand a video model. This composition through forward search enables us to simulate and reason about\nlong horizons of future actions, while HiP only generates and acts on plans one step at a time.\n5\nLIMITATIONS AND CONCLUSION\nLimitations.\nOur planning approach leverages images as a world state representation. In many\ntasks, this is insufficient as it does not capture the full 3D state and cannot encode latent factors\nsuch as physics or mass. Limitations can be partly remedied by generating multi-view videos or by\nusing heuristic function with the full video plan as input. In addition, we observed that our video\ndynamics model does not always simulate dynamics accurately. In several situations, we observed\nthat synthesized videos would make objects spontaneously appear or teleport to new locations. We\nbelieve that larger video models, additional training data or explicit reinforcement learning feedback\nfor physics (Black et al., 2023) could help solve these problems.\nConclusions.\nWe have presented VLP, an approach to long-horizon decision making by combining\nVLMs with text-to-video models. We illustrate the power of test-time composition, in this case plan-\nning, to generate behaviors much more complex than its components. While our experiments largely\nexplore VLP in the context of robotics, there may be downstream applications in other areas too, such\nas steerable media generation. We believe that future exploration in test-time compositions of different\nfoundation models can be a fruitful source of gains to construct more complex intelligent systems.\nAcknowledgements.\nWe would like to thank Tomas Lozano-Perez for providing helpful comments\nin the project and Kamyar Ghasemipour for help in setting up experiments on the Language Table real\nrobot. We gratefully acknowledge support from NSF grant 2214177; from AFOSR grant FA9550-22-\n1-0249; from ONR MURI grant N00014-22-1-2740; from ARO grant W911NF-23-1-0034; from\nthe MIT-IBM Watson Lab; from the MIT Quest for Intelligence; and from the Boston Dynamics\nArtificial Intelligence Institute. Yilun Du is supported by a NSF Graduate Fellowship.\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as I can, not as I\nsay: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. URL\nhttps://arxiv.org/abs/2204.01691. 1, 9, 16\nAnurag Ajay, Seungwook Han, Yilun Du, Shaung Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum,\nLeslie Kaelbling, Akash Srivastava, and Pulkit Agrawal. Compositional foundation models for\nhierarchical planning. arXiv preprint arXiv:2309.08587, 2023. 10\nMohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, and Du-\nmitru Erhan. Fitvid: Overfitting in pixel-level video prediction. arXiv preprint arXiv:2106.13195,\n2021. 9\nKevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models\nwith reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 10\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics\ntransformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 14, 16\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski,\nTianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action\nmodels transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 2, 7,\n15, 16\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. 1\n10\nStephane Cambon, Rachid Alami, and Fabien Gravot. A hybrid approach to intricate motion,\nmanipulation and task planning. The International Journal of Robotics Research, 28(1):104\u2013126,\n2009. 1\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual\nlanguage-image model. arXiv preprint arXiv:2209.06794, 2022. 9\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. 1, 16\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos\nKazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric\nvision: The epic-kitchens dataset. In Proceedings of the European conference on computer vision\n(ECCV), pp. 720\u2013736, 2018. 15\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal\nlanguage model. arXiv preprint arXiv:2303.03378, 2023. 1, 2, 3, 6, 7, 9, 15, 16\nYilun Du, Toru Lin, and Igor Mordatch. Model based planning with energy based models. CORL,\n2019. 9\nYilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy based models.\nAdvances in Neural Information Processing Systems, 33:6637\u20136647, 2020. 9\nYilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha\nSohl-Dickstein, Arnaud Doucet, and Will Grathwohl. Reduce, reuse, recycle: Compositional\ngeneration with energy-based diffusion models and mcmc. arXiv preprint arXiv:2302.11552,\n2023a. 9\nYilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B Tenenbaum, Dale Schu-\nurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. arXiv\ne-prints, pp. arXiv\u20132302, 2023b. 1, 2, 3, 4, 5, 7, 9, 15, 16\nFrederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas\nDaniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills\nwith cross-domain datasets. arXiv preprint arXiv:2109.13396, 2021. 15\nChelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction\nthrough video prediction. Advances in neural information processing systems, 29, 2016. 9\nNikolaos Gkanatsios, Ayush Jain, Zhou Xian, Yunchu Zhang, Christopher Atkeson, and Katerina\nFragkiadaki. Energy-based models as zero-shot planners for compositional scene rearrangement.\narXiv preprint arXiv:2304.14391, 2023. 9\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit\nGirdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in\n3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 18995\u201319012, 2022. 15\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition\nvideo generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1, 9\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In International Conference on\nMachine Learning, pp. 9118\u20139147. PMLR, 2022a. 1, 9\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022b. 9\n11\nWenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor\nMordatch, Sergey Levine, Karol Hausman, et al. Grounded decoding: Guiding text generation\nwith grounded models for robot control. arXiv preprint arXiv:2303.00855, 2023. 1\nLeslie Pack Kaelbling and Tom\u00b4as Lozano-P\u00b4erez. Hierarchical task and motion planning in the now.\nIn 2011 IEEE International Conference on Robotics and Automation, pp. 1470\u20131477. IEEE, 2011.\n1\nWook Hyun Kwon and Soo Hee Han. Receding horizon control: model predictive control for state\nmodels. Springer Science & Business Media, 2005. 5\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An\nHuang, Ekin Aky\u00a8urek, Anima Anandkumar, et al. Pre-trained language models for interactive\ndecision-making. Advances in Neural Information Processing Systems, 35:31199\u201331212, 2022. 9\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and\nAndy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 9493\u20139500. IEEE, 2023. 9\nNan Liu, Shuang Li, Yilun Du, Josh Tenenbaum, and Antonio Torralba. Learning to compose visual\nrelations. Advances in Neural Information Processing Systems, 34:23166\u201323178, 2021. 9\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual\ngeneration with composable diffusion models. arXiv preprint arXiv:2206.01714, 2022. 9\nCorey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis\nArmstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics\nand Automation Letters, 2023. 6, 7, 15, 16\nWeili Nie, Arash Vahdat, and Anima Anandkumar. Controllable and compositional generation with\nlatent-space energy-based models. Advances in Neural Information Processing Systems, 34, 2021.\n9\nJunhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional\nvideo prediction using deep networks in atari games. Advances in neural information processing\nsystems, 28, 2015. 9\nSergiu Oprea, Pablo Martinez-Gonzalez, Alberto Garcia-Garcia, John Alejandro Castro-Vargas,\nSergio Orts-Escolano, Jose Garcia-Rodriguez, and Antonis Argyros. A review on deep learning\ntechniques for video prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n44(6):2806\u20132826, 2020. 9\nRyan Po and Gordon Wetzstein. Compositional 3d scene generation using locally conditioned\ndiffusion. arXiv preprint arXiv:2303.12218, 2023. 9\nVaibhav Saxena, Jimmy Ba, and Danijar Hafner. Clockwork variational autoencoders. Advances in\nNeural Information Processing Systems, 34:29246\u201329257, 2021. 9\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems, 35:25278\u201325294, 2022. 15\nBart Selman and Carla P Gomes. Hill-climbing search. Encyclopedia of cognitive science, 81:82,\n2006. 4\nStefanie Tellex, Nakul Gopalan, Hadas Kress-Gazit, and Cynthia Matuszek. Robots that use language.\nAnnual Review of Control, Robotics, and Autonomous Systems, 3:25\u201355, 2020. 1\nJulen Urain, Anqi Li, Puze Liu, Carlo D\u2019Eramo, and Jan Peters. Composable energy policies for\nreactive motion generation and reinforcement learning. arXiv preprint arXiv:2105.04962, 2021. 9\n12\nRuben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang,\nMohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable\nlength video generation from open domain textual description. arXiv preprint arXiv:2210.02399,\n2022. 1, 9\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select:\nInteractive planning with large language models enables open-world multi-task agents. arXiv\npreprint arXiv:2302.01560, 2023a. 9\nZihao Wang, Lin Gui, Jeffrey Negrea, and Victor Veitch. Concept algebra for text-controlled vision\nmodels. arXiv preprint arXiv:2302.03693, 2023b. 9\nJason Wolfe, Bhaskara Marthi, and Stuart Russell. Combined task and motion planning for mobile\nmanipulation.\nIn Proceedings of the International Conference on Automated Planning and\nScheduling, volume 20, pp. 254\u2013257, 2010. 1\nTailin Wu, Megan Tjandrasuwita, Zhengxuan Wu, Xuelin Yang, Kevin Liu, Rok Sosic, and Jure\nLeskovec. Zeroc: A neuro-symbolic model for zero-shot concept recognition and acquisition at\ninference time. Advances in Neural Information Processing Systems, 35:9828\u20139840, 2022. 9\nTianfan Xue, Jiajun Wu, Katherine Bouman, and Bill Freeman. Visual dynamics: Probabilistic future\nframe synthesis via cross convolutional networks. Advances in neural information processing\nsystems, 29, 2016. 9\nMengjiao Yang, Yilun Du, Bo Dai, Dale Schuurmans, Joshua B Tenenbaum, and Pieter Abbeel.\nProbabilistic adaptation of text-to-video models. arXiv preprint arXiv:2306.01872, 2023a. 9\nSherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foun-\ndation models for decision making: Problems, methods, and opportunities.\narXiv preprint\narXiv:2303.04129, 2023b. 9\nZhutian Yang, Jiayuan Mao, Yilun Du, Jiajun Wu, Joshua B Tenenbaum, Tom\u00b4as Lozano-P\u00b4erez,\nand Leslie Pack Kaelbling. Compositional diffusion-based continuous constraint solvers. arXiv\npreprint arXiv:2309.00966, 2023c. 9\nAndy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit,\nMichael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Com-\nposing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\nURL https://arxiv.org/abs/2204.00598. 9\nTony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual\nmanipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. 6\n13\nA\nAPPENDIX\nIn the Appendix, we provide additional results in Sec. A.1. We provide evaluation details in Sec. A.2,\ndataset details in Sec. A.3, training details in Sec. A.4, and planning details in Sec. A.5.\nA.1\nADDITIONAL RESULTS\nBelow, we provide additional experimental results.\nFailure Cases.\nWe observed several failures cases when applying our approach across tasks. First,\nwhen using world knowledge from other text-to-video datasets to generalize and execute new tasks,\nwe found that sometimes the video model would incorrectly interpret tasks as illustrated in Fig. XI,\nwhere it interprets the manipulator as an octopus. In addition, we found that our approach sometimes\nsynthesized videos with inconsistent physics, where objects would infrequently appear or disappear\nas illustrated in Fig. XII.\nRobustness of Goal-Conditioned Policy\nWe found that our decomposition of execution in VLP\nto visual goal generation and subsequent execution allowed for strong generalization in the goal-\nconditioned policy. This was because the goal-conditioned policy can discard most information\nin image goals and focus only on the portion required for immediate execution of the policy. We\nillustrate this robustness in Fig. XIII. Even when the generated goals are misaligned with the observed\nboard, with many artifacts in the generated boundaries, our goal-conditioned policy is still able to\nexecute the visual task.\nAdditional Video Plans.\nWe provide additional video plans synthesized by our approach on block\narrangement in Fig. XIV. Our approach is able to robustly synthesize both language and video plans.\nA.2\nEVALUATION DETAILS\nVideo Evaluation.\nTo evaluate whether a generated video satisfied a long horizon task, we\nmanually assessed whether at any point in the generated video, the image state in the video satisfied\nthe specified long-horizon goal. Due to the computational cost of generating long-horizon video\n(taking approximately 30 minutes per video), we generated a total of 50 videos for each goal across\neach task and method.\nExecution Evaluation.\nWe used the ground truth simulation state of each block in the Language\nTable environment to compute rewards and task completion thresholds for each task in the paper. To\ncompute the reward function for \u201cgroup blocks by color\u201d, we assessed the percentage of blocks of the\nsame color that were within 0.1 units of each other. To compute the reward for \u201cmove blocks to an\narea\u201d, we assessed the number of blocks within 0.2 x units and 0.27 y units to a specified corner. To\ncompute the reward for \u201cmove blocks into a line in the center of a board\u201d, we computed the number\nof blocks with 0.05 units of the center of the board. We gave all baselines and VLP a total of 1500\ntimesteps of execution in each environment, with performance measured at the end of 1500 timesteps.\nIf at an intermediate timestep the task was complete, we terminated execution early.\nWe evaluated each method on a total of 50 environments on each task due to slowness in execution of\nboth our method and baselines (VLP took approximately 1 hour per environment, while RT2 baselines\ntook approximate 0.5 hours per environment). For VLP, we generated a plan of horizon length 2 with\nbeam width 2 and branching factor 16, where each video in a planning horizon corresponded to 16\nframes. We called the goal-conditioned policy a total of 4 times for each of the first 16 frames of\nthe synthesized video plan in simulated environments. On real world evaluation tasks, we called the\ngoal-conditioned policy on the first 10 frames of the synthesized video plan to more finely account\nfor differences in physics of the real world execution compared to those used to gather the data.\nA.3\nDATASET DETAILS\nLanguage Table.\nWe trained VLP on approximately 10000 long horizon trajectories in both\nsimulation and real across a set of several hundred different long horizon goals. We chose a total of 3\ngoals to run experiments on as they allowed easy automated evaluation. These combined roughly\n20000 trajectories and had approximately 400000 short-horizon text labels.\n7DoF Mobile Manipulator.\nFor our 7DoF mobile manipulator planning and execution experiments,\nwe trained VLP on the dataset from RT-1 (Brohan et al., 2022). For our generalization experiments\nof VLP, we train a large text-to-video diffusion model using a mix of data from our particular 7DoF\n14\nLow data \nregime\nNo data \nregime\nInstruction is \u201cpick octopus toy\u201d, in low data regime, the diffusion model has \nseen octopus toy a few times. The reconstruction seems to be correct thought \nthe dynamics are slightly wrong.\nIn no-data regime, the octopus toy was not seen, and the model tries to apply \noctopus texture to robot arm. It shows signs of transfer of internet knowledge, \nbut still need correct understanding of the scene to apply those knowledge.\nFigure XI: Failure in Transferring Web Knowledge. VLP is instructed to \u201cpick up octopus toy\u201d. In the\nlow-data regime, the model has only seen the octopus toy in training data a few times, and the generated frames\nroughly reconstructed the shape of the toy, but fails at accurately recover the dynamics. The the no-data regmie,\nthe model has never seen in training data, and without any training data VLP transfers the wrong web knowledge\nand makes the gripper octopus arms.\nFigure XII: Failure in Physics. When synthesizing videos, VLP will sometimes make objects dissappear or\nreappear. Creating long videos that respect object permanence remains an important future direction for our\nwork.\nMobile Manipulator, Bridge (Ebert et al., 2021), RT-2 (Brohan et al., 2023), Ego4D (Grauman et al.,\n2022), EPIC-KITCHEN (Damen et al., 2018), and LAION-400M (Schuhmann et al., 2022).\n14DoF Bi-Manual Manipulation.\nFor our 14DoF Bi-Manual Manipulation, we train VLP on\napproximately 1200 teleoped demonstrations on a kitchen stacking task, where operators were first\nasked to stack bowls on top of each other, then cups on top of bowls, and then utensils on top of cups.\nEach demonstration was annotated with roughly 20 language instructions, leading to roughly 25k\nshort-horizon text labels.\nA.4\nTRAINING DETAILS\nVideo Models.\nTo train video diffusion models, we follow the model architecture and training\napproach from (Du et al., 2023b). We train a base text-conditioned video generation model at 24 \u00d7 40\nresolution and upsample it to 48 \u00d7 80 resolution and then 192 \u00d7 320 resolution, where videos at each\nresolution are set to generate a temporal video of length 16 frames. We use a base channel width of\n256 across models and train a base text-conditioned video model using 64 TPUv3 pods for 3 days\nand higher resolution superresolution models for 1 day. We train separate text-to-video models per\ndomain.\nVLM Models.\nTo train VLMs, we follow the architecture and codebase of PaLM-E (Driess et al.,\n2023). We fine-tune a single 12B PaLM-E (Driess et al., 2023) jointly to both predict heuristics and\npolicies. We finetune the VLM model using 64 TPUv3 pods for 1 day on data in each domain and\nuse separate models per domain.\nGoal-Conditioned Policy.\nTo train our goal-conditioned policy, we use the LAVA model (Lynch\net al., 2023) architecture, where the text-encoder from CLIP is replaced with a ResNet encoder for\nthe goal image. We train our goal-conditioned policy in each domain using 16 TPUv3 pods for 1 day.\n15\nFigure XIII: Goal Policy Robustness to Synthesized Goals The goal-conditioned policy is robust to noise in\nsynthesized goals. In the above example, given synthesized goals in the top row, the policy can directly execute\na control actions in the real environment in the bottom row.\nBaselines.\nWe compare our approach with a set of multiple baselines. For the LAVA baseline,\nwe follow the exact settings from the LAVA paper (Lynch et al., 2023), where we train the model\nconditioned on long-horizon goal text and trained the model for 1 day use 16 TPUv3 pods. For our\nUniPi (Du et al., 2023b) baseline, we followed the same training architecture and approach as the\ntext-conditioned video model in VLP, where the model is trained on long-horizon video elements.\nFor the RT-2 baseline, we follow the architecture and codebase of the original paper (Brohan et al.,\n2023), where we use the 12B RT2-PaLM-E model. We trained RT-2 using 64 TPUv3 pods for 3 days.\nA.5\nPLANNING DETAILS\nLanguage Table.\nTo generate video plans, we planned with a horizon of 16, a beam width of 2, a\nlanguage branching factor of 4, and a video branching factor of 4. To enable fast video generation,\nwe used the DDIM sampler, with a total of 64 timesteps of sampling at the base resolution and 4\ntimesteps of sampling at the higher resolution samples, with a classifier-free guidance scale of 5 for\nthe base model. We generated 16 videos in parallel at once use a 4 TPU inference pod.\nWe queried the VLM policy to generate different text actions given an image with a temperature\nof 0.3. Our VLM heuristic function decoded the number of steps left until task-completion with a\ntemperature of 0.0. We set our heuristic function clipping threshold during planning to be 50 and\nremoved videos if the improvement was larger than 50 after one video rollout. This number was\nheuristically determined based off generated plans at different thresholds, with the aim of finding the\nhighest threshold such that long-horizon plans still looked physically plausible. Our total planning\nprocedure across a horizon of 16 took approximately 30 minutes.\n7DoF Mobile Manipulator.\nIn the 7DoF mobile manipulator experiments we made slight tweaks\nto the approach. For VLM Planning, we used PaLM-E (Driess et al. (2023)) to generate scene\ncaptions, and few-shot prompted PaLM (Chowdhery et al. (2022)) to generate plans following the\nprompts in SayCan (Ahn et al. (2022)). The beam search has a beam width of 3. The video diffusion\nmodel is the same as above, except that it has a different resolution for the base model (64 \u00d7 80) and\nsuper resolution model (256 \u00d7 320), and is finetuned on RT-1 (Brohan et al. (2022)) data only. The\ngoal-conditioned policy is using the last frame of the generated video segment only.\n14DoF Bi-Manual manipulation.\nWe followed the same planning setup as in Language Table.\nWe set our heuristic function clipped threshold during planning to be 15.\n16\n1. Push the green cube into the \nblue crescent\n2. Push the green cube into \nblue crescent\n3. Push the green cube into the \nyellow star\n4. Move the green star to the \nleft of the blue crescent\n5. Point your arm to the top of \nthe red pentagon\n6. Push the red pentagon \ntowards the green star\n7. Move red moon into the \ngreen star\n8. Point your arm at the top of \nthe red pentagon\n9. Slide the red pentagon to the \nleft side of the red crescent\n10. Push the red pentagon into \nthe red moon\n1. Move the red circle to the left \nof the yellow hexagon\n2. Move the green circle closer \nto the red circle\n3. Point your arm at the green \nstar\n4. Move the green star towards \nthe center\n5. Move the green star towards \nthe red star\n6. Move the green circle \ntowards the green star\n7. Move the blue cube closer to \nthe green circle\n8. Slide the blue triangle to the \ngreen star\n9. Push the blue cube into the \ngreen circle\n10. Move your arm towards the \nred circle\nFigure XIV: Additional Long Horizon Video Plans. Additional long horizon video plans generated by VLP on\nboth simulated and real images. VLP is only given the initial image and language goal. Language subplans and\nother image frames are directly synthesized.\n17\n"
  },
  {
    "title": "Farzi Data: Autoregressive Data Distillation",
    "link": "https://arxiv.org/pdf/2310.09983.pdf",
    "upvote": "6",
    "text": "Preprint\nFARZI DATA: AUTOREGRESSIVE DATA DISTILLATION\nNoveen Sachdeva\u2020,\u2021\nZexue He\u2020\nWang-Cheng Kang\u2021\nJianmo Ni\u2021\nDerek Zhiyuan Cheng\u2021\nJulian McAuley\u2020\nUniversity of California, San Diego\u2020\nGoogle DeepMind\u2021\n{nosachde, zehe, jmcauley}@ucsd.edu\n{wckang, jianmon, zcheng}@google.com\nABSTRACT\nWe study data distillation for auto-regressive machine learning tasks, where the\ninput and output have a strict left-to-right causal structure. More specifically, we\npropose FARZI, which summarizes an event sequence dataset into a small number\nof synthetic sequences \u2014 FARZI DATA \u2014 which are optimized to maintain (if not\nimprove) model performance compared to training on the full dataset. Under the\nhood, FARZI conducts memory-efficient data distillation by (i) deriving efficient\nreverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector\nProducts; and (ii) factorizing the high-dimensional discrete event-space into a\nlatent-space which provably promotes implicit regularization. Empirically, for\nsequential recommendation and language modeling tasks, we are able to achieve\n98 \u2212 120% of downstream full-data performance when training state-of-the-art\nmodels on FARZI DATA of size as little as 0.1% of the original dataset. Notably,\nbeing able to train better models with significantly less data sheds light on the\ndesign of future large auto-regressive models, and opens up new opportunities to\nfurther scale up model and data sizes.\n1\nINTRODUCTION\nThe effectiveness of machine learning models relies heavily on the quantity and quality of training\ndata. While the quantity of training data is always well-regarded in the scaling-laws of training\nhighly-parameterized neural networks (Hoffmann et al., 2022; Kaplan et al., 2020; Borgeaud et al.,\n2022; Zhai et al., 2022; Du et al., 2022), the quality of underlying data is often overlooked. Despite\nbeing an intuitive covariate in downstream model performance, there does not exist an efficient\nout-of-the-box solution for measuring the quality of a data point. Some popular heuristics (e.g., data\nvaluation (Ghorbani & Zou, 2019), coresets (Borsos et al., 2020a)) fall short from a variety of angles\n(Basu et al., 2021; Kumar et al., 2020; Toneva et al., 2019; Sener & Savarese, 2018).\nData distillation (DD) (see Sachdeva & McAuley (2023) for a comprehensive survey) offers a\npromising alternative to explicitly tagging the quality of each datapoint. Loosely, DD approaches\naim to synthesize a terse data summary solely intended to train models to the same (if not better)\nquality as training them on the original dataset. In this paper, we propose FARZI, a DD approach\ndesigned specifically for synthesizing high-fidelity auto-regressive data summaries. We call the data\nsynthesized by FARZI as FARZI DATA.\nFARZI DATA takes a step towards addressing the massive costs (e.g., financial, environmental, etc.)\nassociated with training large auto-regressive models (OpenAI, 2023; Anil et al., 2023; Radford\net al., 2022) on massive amounts of pretraining data by (i) implicitly filtering out low-quality sources\nof information resulting in a terse data summary, and (ii) re-organizing the data in a format that\nis most pertinent for model training. Intuitively, a vast majority of underlying information in such\nauto-regressive datasets is redundant from the downstream task\u2019s perspective. For example, looking\nat recommender systems, a predictive model wouldn\u2019t necessarily need trillions of event-level data\nfrom billions of users to accurately model user-behaviour patterns.\n1\narXiv:2310.09983v1  [cs.LG]  15 Oct 2023\nPreprint\na\nunder\nrabbit\nthe\nsofa\nslept\nan\ndog\nstood\nat\nher\ncouch\nbed\nFarzi Data (GPU/TPU) \n! 1M Fake Documents\nLanguage Modeling Corpus\n! 100M Documents\nBuffer Size\nTime \nSteps\n\u2026 \u2026 \u2026\n\u2026 \u2026 \u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026 \u2026 \u2026\nthe cat sat on my\nsat\n\u2026 \u2026 \u2026\n\u2026 \u2026 \u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026 \u2026 \u2026\nthe cat sat on my\nsofa\n\u2026 \u2026 \u2026\n\u2026 \u2026 \u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026 \u2026 \u2026\na rabbit slept under \nthe sofa \nFigure 1: Visualization of FARZI DATA in the context of language modeling. FARZI DATA can be\nseen as a 3-D tensor comprising of sequences of distributions over tokens, where a single distribution\nsequence fuses the information content of multiple discrete sequences. E.g, a single distribution\nsentence can unfold into an entire tree of similar sentences like \u201cthe rabbit slept under the sofa\u201d or\n\u201ca dog stood at the bed\u201d as depicted in the figure. Such a parameterization: (i) makes the dataset\nGPU/TPU friendly; (ii) reduces the cardinality of the dataset leading to efficient training; and (iii)\nenables models to be trained on fuzzy sequences, hopefully leading to robust learning.\nTypical DD techniques (Zhao et al., 2021; Zhao & Bilen, 2023; 2021; Nguyen et al., 2021; Cazenavette\net al., 2022; Zhou et al., 2022b; Deng & Russakovsky, 2022) are geared toward low-resolution\nimage datasets due to (i) computationally expensive data optimization, and (ii) generation-friendly\ncontinuous domain of images (pixels). On the other hand, auto-regressive data generally consists\nof sequences of discrete tokens (e.g., sub-words) with a potentially large vocabulary. Further, many\napplications call for sequences with a long list of such tokens. FARZI addresses the aforementioned\ncharacteristics of auto-regressive data by performing data distillation in a latent space by organizing\nFARZI DATA into (i) a latent data summary that captures the downstream task patterns, and (ii) a\ndecoder (e.g., token-embeddings) that maps the latent-space back to the token-space. In addition\nto making FARZI optimization-friendly (both of the aforementioned data components are non-\ndiscrete/continuous), we demonstrate that such latent-parameterization provably promotes implicit\nregularization when training models on FARZI DATA (Theorem 3.1). To summarize, we highlight\nfour main contributions of this paper:\n\u2022 We develop FARZI, a scalable DD technique for summarizing massive auto-regressive datasets, and\ndemonstrate FARZI DATA\u2019s sample efficiency over 5 datasets spanning sequential recommendation\nand language modeling tasks. Training on FARZI DATA, we are able to achieve up to 98 \u2212 120%\nof full-data performance for state-of-the-art models using as little as 0.1% of the original dataset\nsize, as well as noting a strong cross-architecture generalization, i.e., being able to train various\n(student) models on FARZI DATA synthesized using a given (teacher) model.\n\u2022 Building atop the meta-matching framework of DD, we propose two crucial modifications for\nlargely improved sample efficiency. First, conducting an investigative study on the role of inner-\nloop optimizer in DD, we conclude Adam (Kingma & Ba, 2015) to be much more adept than SGD\n(with or without momentum) for DD. This is in stark contrast with existing DD and meta-learning\nstudies where SGD is the de-facto optimizer of choice. We further improve FARZI\u2019s sample quality\nby leveraging pretrained training trajectories for initialization in the meta-matching optimization.\n\u2022 In addition to generating high-fidelity data, FARZI is computationally highly scalable. Firstly,\nparameterizing FARZI DATA into a latent data summary and a token decoder saves large amount\nof time and memory during optimization, thereby making FARZI (roughly) independent of the\nvocabulary size. Further, we derive an efficient reverse-mode differentiation of Adam which has\na memory complexity independent of the number of inner-loop steps, unlike autograd systems\nwhich store all intermediate variables, therefore leading to O(100)\u00d7 memory footprint reduction.\n\u2022 We provide a formal analysis of FARZI from various standpoints. We firstly show that FARZI\nDATA\u2019s latent parameterization implicitly promotes regularization and provably improves\ngeneralization. Previous studies have observed such data overfitting effects in DD empirically\n(Zhou et al., 2022b), but we are the first to study its theoretical underpinnings. We further\ndemonstrate the correctness of our proposed reverse-mode differentiation of Adam.\n2\nPreprint\n2\nRELATED WORK\nData downsampling. The complexity and training time for state-of-the-art models from different\ndomains has grown exponentially in the recent years (OpenAI, 2023; Sun et al., 2019; Mittal et al.,\n2021; Rombach et al., 2022). Sampling has been the classic approach to summarize large datasets,\napproaches for which can be grouped into the following categories: (i) Coreset construction\ntechniques which sample a weighted subset of the given dataset to accelerate model training (Kaushal\net al., 2019; Borsos et al., 2020b; Krause et al., 2021; Kazemi et al., 2021). Being a combinatorial\noptimization, coreset construction techniques typically leverage submodularity assumptions (Bilmes,\n2022) to optimize the coreset in a tractable manner. (ii) Data valuation approaches which typically\nleverage shapley values (Shapley, 1953) to tag the value of each data point for model training (Wang\n& Jia, 2023; Ghorbani & Zou, 2019; Kwon & Zou, 2023; Kwon et al., 2021). Notably, such data\nvaluation methods turn out to be computationally intractable even for moderate sized datasets. (iii)\nHeuristic samplers that build upon designing ad-hoc notions of data quality. Two prominent schools-\nof-thought in designing such heuristics has been to either preserve notions like diversity (Coleman\net al., 2022; Abbas et al., 2023; Sorscher et al., 2022), discrepancy (Karnin & Liberty, 2019), etc. in\nsome metric-space of the inputs, or use the loss-values from some proxy model to tag the difficulty\n(and thereby, quality) for each datapoint (Paul et al., 2021; Coleman et al., 2020; Sachdeva et al.,\n2021; Jiang et al., 2019).\nData distillation. Contrary to sampling datapoints from a given dataset, data distillation approaches\naim to synthesize high-quality data summaries for sample-efficient model training through bilevel\noptimization (see Sachdeva & McAuley (2023) for a comprehensive survey). Prominent existing\napproaches are designed for summarizing images (Wang et al., 2018; Zhao et al., 2021; Zhao &\nBilen, 2023; 2021; Cazenavette et al., 2022; Zhou et al., 2022b; Deng & Russakovsky, 2022; Nguyen\net al., 2021), graphs (Jin et al., 2022a;b), and recommender systems (Sachdeva et al., 2022a). Such\napproaches can essentially be viewed as meta-learning approaches (see Hospedales et al. (2021) for\na comprehensive survey) with the meta-optimization happening over the data summary instead of\ncommon applications like model initialization (Finn et al., 2017) or task hyper-parameters (Maclaurin\net al., 2015; Lorraine et al., 2020).\nAutoregressive tasks. A variety of machine learning tasks are auto-regressive, e.g., language\nmodeling (OpenAI, 2023; Gokaslan et al., 2019; Raffel et al., 2019), sequential recommendation\n(Sachdeva et al., 2019; Kang & McAuley, 2018; Bennett et al., 2007), self-driving (Sachdeva et al.,\n2022b; Sun et al., 2020), etc. Such tasks have a clear left-to-right causal structure with one event\npreceding the other, typically in time. Further, since a majority of such tasks are semi-supervised\nand are associated with large-amounts of naturally occurring data; training large foundation models\n(Bommasani et al., 2021) for such data can become daunting despite its practicality, thereby limiting\noverall research progress. Concerningly, to the best of our knowledge, only simple data sampling\nheuristics scale to such large auto-regressive datasets (Toneva et al., 2019; Sener & Savarese, 2018).\n3\nFARZI: SYNTHESIZING HIGH-FIDELITY AUTOREGRESSIVE DATA\nSUMMARIES\nTask & Notation. Given an autoregressive dataset D \u225c {xi}|D|\ni=1 where xi \u225c [xij \u2208 V]|xi|\nj=1 is an\nordered sequence of tokens, each belonging to the vocabulary of all possible tokens V. We aim to\nsynthesize a data summary Dsyn \u2208 R\u00b5\u00d7\u03be\u00d7dim(V) consisting of \u00b5 fake sequences of maximum length \u03be,\ns.t., \u00b5 \u226a |D|. More specifically, we seek to construct Dsyn in such a way that a representative learning\nalgorithm \u03a6\u03b8 : Vn 7\u2192 V trained on Dsyn using an autoregressive task (e.g., next-token-prediction\n(Radford et al., 2018), cloze (Taylor, 1953), etc.) specified by a cost function l : V \u00d7 V 7\u2192 R can\nachieve performance equivalent to that of training \u03a6\u03b8 on the original dataset D. Taking next-token-\nprediction (Radford et al., 2018) as a representative predictive task, we denote the empirical risk\nas LD(\u03b8) \u225c Ex\u223cD, xi\u223cx[l(\u03a6\u03b8(x1:i), xi+1)] for notational convenience, where x1:i represents the\nsequence of first i tokens in x.\nMethodology. We cast the problem of autoregressive DD as a meta-learning problem, wherein the\ninner-loop trains a learning algorithm on the data summary, and the outer-loop evaluates its quality\n3\nPreprint\nBuffer Size\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026 \u2026 \u2026\n\u2026\nthe cat sat on \nmy\nsat\n\u2026\n\u2026\n\u2026\n\u2026 \u2026 \u2026\n\u2026 \u2026 \u2026\n\u2026\nthe cat sat on \nmy\nsofa\nthe rabbit \nslept\nunder the \nsofa\n\u2a02\n'5\n':;(\n':\n'<\nLanguague Modeling \nCorpus\nForward Pass\nGradient \nBackpropagation\nCross-entropy\nTime \nSteps\nIntermediate Variables\nLearnable Parameters\nFactorized Farzi Data\nVocabulary size\nLatent\n\u22ef\nLatent\nTime \nSteps\nBuffer Size\n\u22ef\n=>?@A\nB\n=?@A\n\u22ef\nCDEF\nGHI. CDEF\nKL??\nKL??\nFigure 2: Visualization of a single outer-loop step in FARZI demonstrated using the language modeling\npredictive task. In this framework, each outer-loop step first materializes FARZI DATA using (a batch\nof) its respective low-rank counterparts, followed by training a learning algorithm on FARZI DATA\nfor T-steps using Adam. The meta-gradient to update the factorized FARZI DATA is obtained using\nefficient reverse-mode Adam outlined in Algorithm 1. This process (outer-loop step) is repeated till\nconvergence, or for a fixed number of iterations.\nvia l(\u00b7, \u00b7) on the original dataset to directly update the data summary via gradient descent. More\nformally, a na\u00efve bilevel optimization problem can be framed as follows:\narg min\nDsyn\nE\n\u03b80\u223c\u0398 [LD(\u03b8\u2217)]\ns.t. \u03b8\u2217 \u225c arg min\n\u03b8\nLDsyn(\u03b8 | \u03b80) ,\n(1)\nwhere \u0398 is a distribution to initialize model parameters (e.g., uniform, Kaiming (He et al., 2015), etc.).\nSuch a formulation is commonly termed as meta-model matching based DD (see Sachdeva & McAuley\n(2023) for a taxonomy of existing approaches), and is associated with significant computational\ncomplexity in terms of both time and memory. Typical approaches resort to local optimization (e.g.,\nSGD) in the inner-loop, and Truncated Backpropagation Through Time (T-BPTT) by unrolling a finite\nnumber of inner optimization steps to obtain the meta-gradient. Notably, DD becomes infeasible \u2014\neven after making such assumptions \u2014 when the data is autoregressive as each data-point is associated\nwith (i) a large discrete token vocabulary, i.e., dim(V); and (ii) a third sequential dimension, i.e., \u03be.\nHence, the computational complexities of existing DD techniques grows by a factor of \u2248 \u03be \u00b7 dim(V).\nTo alleviate the computational challenges, FARZI performs data distillation in a latent space. More\nspecifically, FARZI factorizes Dsyn into: (i) a latent data summary \u02dcDsyn \u2208 R\u00b5\u00d7\u03be\u00d7d where d \u226a\ndim(V); and (ii) a token-decoder matrix M \u2208 Rd\u00d7dim(V). Finally, we can compose the latent data\nsummary and the token-decoder to obtain the final data summary: Dsyn \u2261 softmax( \u02dcDsyn \u00b7 M / \u03c4),\nwhere \u03c4 \u2208 R+ represents the temperature in softmax(\u00b7) and controls the entropy in Dsyn. Such a\nfactorization makes FARZI scalable to both extremely large datasets, i.e., large |D| as well as datasets\nwith large token vocabularies, i.e., large dim(V).\nIn addition to promoting scalability, we prove that FARZI DATA\u2019s latent parameterization implicitly\npromotes regularization while training downstream models (Theorem 3.1). More specifically, we\nleverage the concepts of data representativeness and Rademacher complexities (Shalev-Shwartz\n& Ben-David, 2014, Chapter 26) to show that explicit rank regularization while synthesizing data\nsummaries (e.g., latent factorization) strictly promotes generalization. Notably such data overfitting\nhas been previously (empirically) noted to notoriously affect DD (Zhou et al., 2022b), but we are the\nfirst to explore the theoretical underpinnings.\n4\nPreprint\nAlgorithm 1 Reverse-mode differentiation of Adam. See Appendix A for the Adam algorithm.\n1: Input: wT , mT , vT , \u03b3, \u03b1, \u03f5, L(w, x), meta-objective f(w)\n2: Initialize: dm \u2190 0, dx \u2190 0, dw \u2190 \u2207wf(wT )\n3: for t = T to 1 do\n4:\n\u02c6mt \u225c mt/(1 \u2212 \u03b2t\n1)\n\u25b7 exactly reverse Adam\n5:\n\u02c6vt \u225c vt/(1 \u2212 \u03b2t\n2)\n\u25b7 exactly reverse Adam\n6:\nwt\u22121 = wt + \u03b1 \u00b7 \u02c6mt/(\u02c6vt + \u03f5)\n\u25b7 exactly reverse Adam\n7:\ngt \u225c \u2207wL(wt\u22121, x)\n\u25b7 exactly reverse Adam\n8:\nmt\u22121 = [mt \u2212 (1 \u2212 \u03b21) \u00b7 gt]/\u03b21\n\u25b7 exactly reverse Adam\n9:\nvt\u22121 = [vt \u2212 (1 \u2212 \u03b22) \u00b7 g2\nt ]/\u03b22\n\u25b7 exactly reverse Adam\n10:\n\u03f5\u2032 \u225c \u03f5 \u00b7\np\n1 \u2212 \u03b2t\n2\n11:\n\u03b1\u2032 \u225c \u03b1 \u00b7\np\n1 \u2212 \u03b2t\n2 / (1 \u2212 \u03b2t\n1)\n12:\n\u03b2\u2032 \u225c (1 \u2212 \u03b22) / (1 \u2212 \u03b21)\n13:\ndm = dm + \u03b1\u2032 \u00b7\n\u0010\n\u03b2\u2032\u00b7mt\u00b7gt\n\u221avt\u00b7(\u221avt+\u03f5\u2032)2 \u2212\n1\n\u221avt+\u03f5\u2032\n\u0011\n\u00b7 dw\n\u25b7 Proposition 3.2\n14:\ndw = dw \u2212 (1 \u2212 \u03b21) \u00b7 dm \u00b7 \u2207w\u2207wL(wt\u22121, x)\n\u25b7 Hessian-vector product\n15:\ndx = dx \u2212 (1 \u2212 \u03b21) \u00b7 dm \u00b7 \u2207x\u2207wL(wt\u22121, x)\n\u25b7 Hessian-vector product\n16:\ndm = \u03b21 \u00b7 dm\n17: Output: gradient of f(wT ) w.r.t. w0, m0, and x\nTheorem 3.1. Let Dsyn \u2208 R\u00b5\u00d7\u03be\u00d7dim(V) be parameterized using \u02dcDsyn \u2208 R\u00b5\u00d7\u03be\u00d7d and M \u2208\nRd\u00d7dim(V), and Dnaive \u2208 R\u00b5\u00d7\u03be\u00d7dim(V) denote the non-parameterized data. Let F be the function-\nclass of quadratic classifiers, and Rep(F, D) denote the representativeness of a training set D (lower\nis better); then if d < min(\u00b5, \u03be \u00b7 dim(V)):\nE \u02dc\nDsyn,M[Rep(F, \u02dcDsyn \u00b7 M)] < EDnaive[Rep(F, Dnaive)] .\nProof. See Appendix B.1 for the relevant preliminaries and proof.\nWhile typical bilevel optimization approaches use SGD in the inner loop (Deng & Russakovsky, 2022)\ndue to efficient reversible dynamics of SGD (see Maclaurin et al. (2015) for efficient reverse-mode\nSGD), we empirically observe that in our setting of autoregressive DD, Adam optimization (Kingma\n& Ba, 2015) in the inner-loop is crucial for downstream DD performance (see Figure 5). Further,\nwe also note that a significant number of inner-loop optimization steps \u2014 in the order of 100s \u2014\nare needed for good generalization for both Adam and SGD based DD, as is concurrently reported\nby other work (Deng & Russakovsky, 2022). To this end, we derive an efficient approximation of\nreverse-mode differentiation of the Adam optimization in Algorithm 1.\nProposition 3.2. Correctness of Algorithm 1, Line 13 : see Appendix B.2 for the proof.\nAlgorithm 1 allows the memory footprint of the meta-gradient computation to be constant w.r.t. the\nnumber of inner-loop steps. Notably, meta-gradient computation is the biggest contributor in a meta-\nlearning algorithm\u2019s overall scalability. This is in stark contrast with typical autograd libraries like\nPyTorch (Paszke et al., 2019), JAX (Bradbury et al., 2018), etc. which require storing all intermediate\nvariables across the inner-optimization to compute the meta-gradient, resulting in a linearly growing\nmemory footprint w.r.t. the number of inner-loop steps.\nFARZI also improves the sample-efficiency of the underlying meta-matching framework (Equation (1))\nby leveraging access to a limited number of training trajectories on the target dataset. Formally, let\n\u2126 \u225c {[\u03b8i]T\ni=1 | \u03b80 \u223c \u0398} be the set of episodic checkpoints of training \u03a6\u03b8 on D for a limited number\nof random initializations. FARZI leverages \u2126 in its final optimization as follows:\narg min\nM, \u02dc\nDsyn\nE\n\u03b80\u223c\u2126 [LD(\u03b8T )]\ns.t.\n\u03b8t+1 \u2190 Adam\n\u0000\u03b8t, \u2207\u03b8LDsyn(\u03b8t)\n\u0001\nDsyn \u2190 softmax\n\u0010\n\u02dcDsyn \u00b7 M / \u03c4\n\u0011\n,\n(2)\n5\nPreprint\nwhere Adam(\u00b7, \u00b7) represents the set of Adam update equations listed in Appendix A, and T rep-\nresents the number of inner-loop optimization steps for each outer-loop step. Notably, curating \u2126\nis independent of the DD procedure and can be precomputed and logged beforehand, contributing\nnothing to the computational complexity of FARZI.\nComputational complexity. We elucidate FARZI\u2019s computational footprint of optimizing Equa-\ntion (2) in terms of a single outer-loop step\u2019s runtime and memory usage:\nMemory Complexity:\nO\n\u0000|\u03a6| + b \u00b7 dim(V) + bsyn \u00b7 \u03be \u00b7 dim(V) + \u00b5 \u00b7 \u03be \u00b7 d + d \u00b7 dim(V)\n\u0001\nTime Complexity:\nO\n\u0000bsyn \u00b7 \u03be \u00b7 d \u00b7 dim(V) + T \u00b7 bsyn \u00b7 |\u03a6| + b \u00b7 |\u03a6| +\nT \u00b7 ( bsyn \u00b7 |\u03a6| + bsyn \u00b7 \u03be \u00b7 d + d \u00b7 dim(V) )\n\u0001\nModel optimization\nStoring \u02c6\nD\nStoring \u02c6\nDsyn\nStoring & Updating \u02dc\nDsyn\nStoring & Updating M\nInner-loop optimization\nComputing \u2207\u03b8L \u02c6\nD(\u03b8T )\nComputing \u02c6\nDsyn from Dsyn & M\nInner-loop reversal (Algorithm 1)\nComputing \u2207\u03b8L \u02c6\nDsyn (\u03b8t)\nUpdating meta-gradient for \u02dc\nDsyn\nUpdating meta-gradient for M\nwhere, \u02c6D \u223c D and \u02c6Dsyn \u223c Dsyn are randomly sampled batches of real data and FARZI DATA such\nthat b \u225c | \u02c6D| and bsyn \u225c | \u02c6Dsyn|; and |\u03a6| represents the total number of parameters in \u03a6.\n4\nEMPIRICAL EVALUATION\n4.1\nSETUP\nWe empirically evaluate FARZI\u2019s practicality over two well-studied autoregressive predictive tasks:\n\u2022 Sequential Recommendation:\nPredict the item that a given user is most likely to consume\nnext, given their historic item consumption history. We use four benchmark datasets, namely\nMovielens-100k, Movielens-1M (Harper & Konstan, 2015), Amazon Magazine (Ni et al., 2019a),\nand Netflix (Bennett et al., 2007); from different recommendation domains and with varying data\ncharacteristics. To evaluate model quality we use popular ranking metrics: AUC, HitRate, and\nnDCG. A detailed description of all datasets and metrics can be found in Appendices C.1 and C.2.\n\u2022 Language Modeling (LM): Predict the most probable following word given a sequence of words.\nWe conduct our experiments on the official-released train/validation/test split of the English Penn\nTreebank (PTB) corpus (Marcus et al., 1993): an open-sourced benchmark widely used for LM.\nWe evaluate our models using word-level perplexity, as well as the token prediction accuracy after\ngreedy decoding on the test set. Further details about the dataset and metrics are described in\nAppendices C.1 and C.2.\nWe use SASRec (Kang & McAuley, 2018) and a small Transformer model (Vaswani et al., 2017)\nas the representative learning algorithms (\u03a6) in FARZI\u2019s inner-loop for sequential recommendation\nand language modeling tasks respectively, and use cross-entropy as the underlying objective function\nfor both. We implement FARZI using PyTorch (Paszke et al., 2019) and we will publicly release the\ncode and optimized FARZI DATA for all datasets used in this paper upon acceptance. We conduct all\nour experiments on a single RTX 2080-Ti GPU (11 GB), and list all relevant hyper-parameters and\nfurther experimental details in Appendices C.3 and C.4.\n4.2\nEXPERIMENTS\nHow sample efficient is FARZI DATA? We evaluate the fidelity of FARZI DATA by first optimizing\nfor Dsyn using Equation (2), followed by training \u03a6 (from scratch) on Dsyn. We plot the performance\nof the trained \u03a6 on the test-set for various amounts of data budgets (\u00b5) in Figures 3 and 4 for\nsequential recommendation and language modeling tasks respectively. A tabular version of the same\nresults can be found in Appendix D, Table 6. We also plot semantically equivalent results for other\n6\nPreprint\n0.3% 1%\n5% 20% 100%\nData summary size\n5\n10\n15\n20\n25\nHR@10\nAmazon Magazine\n1%\n5%\n20%\n100%\nData summary size\n4\n8\n12\n16\n20\nML-100k\n0.2% 1%\n5% 20% 100%\nData summary size\n0\n6\n12\n18\n24\nML-1M\n0.01%0.1% 1%\n10% 100%\nData summary size\n3\n6\n9\n12\n15\n18\nNet\ufb02ix\nFull Data\nRandom\nHead\nFarzi\nFigure 3: Performance change of SASRec with increasing data (log-scale) for recommendation.\nFor a tabular version of these results see Appendix D, Table 6; and for results on more metrics see\nAppendix D, Figure 9.\n0.02%\n0.5%\n5%\n25%100%\nData summary size\n0\n200\n400\n600\n800\nPerplexity\nPTB\nFull Data\nRandom\nHead\nFarzi\nFigure 4: Performance change of\nTransformer with increasing data\nfor LM.\ncommonly used data sampling heuristics, namely (i) random\nsampling: sample sequences uniformly at random, and (ii) head\nsampling: retain the sequences with the largest length. We\nfirst note that FARZI DATA is much more sample-efficient than\nother data sampling techniques, being able to achieve up to\n1000\u00d7 data compression with no loss in performance. Further, in\nFigure 3, we notice that on two out of the four recommendation\ndatasets, FARZI\u2019s orders of magnitude smaller data is able to\ntrain models of higher quality than the original dataset itself.\nThis observation acts as further evidence for the intuitive yet\nunder-explored idea that less but high-quality data can be more\nimportant for model training than a very large but noisy dataset\n(Sachdeva et al., 2022a; Zhou et al., 2023).\nHow versatile is FARZI DATA? Since FARZI DATA is inher-\nently optimized for a specific learning algorithm, we ascer-\ntain its universality by training different kinds of student\nnetworks over data synthesized using a given teacher\nnetwork in FARZI\u2019s inner-loop for the sequential recommendation task. Note that the student\nnetwork is completely unrelated to the data synthesis procedure and underlying FARZI optimization.\nFrom the results in Table 1, we observe that irrespective of the teacher network, FARZI DATA\nis able to train varied student network architectures (e.g., Transformers, RNNs, MLPs) better\nthan training on the full dataset. On the other hand, however, the best performance for any given\nstudent network is obtained when the same network is used during FARZI optimization.\nHow important is the inner-loop optimizer in FARZI? We compare SGD (with or without\nmomentum) and Adam (Kingma & Ba, 2015) optimizers as different optimization routines in\nFARZI\u2019s inner-loop (Equation (2)). Notably, we implement differentiable Adam optimization in three\ndifferent ways: (i) using the higher package (Grefenstette et al., 2019); (ii) PyTorch\u2019s autograd\nimplementation; and (iii) our efficient reverse-mode implementation (Algorithm 1). We measure their\neffect on downstream performance as well as the time and memory associated with each outer-loop\niteration in Figure 5. We first observe that Adam is much better suited for DD in our setting. This is a\nnovel finding in the context of meta-learning and its applications, where previously Adam has been\nreported to be worse than SGD (Grefenstette et al., 2019). Further, we observe that while different\nreverse-mode implementations of Adam lead to data of similar sample quality, their computational\nproperties vastly differ. We observe that PyTorch and higher have similar memory footprints, but\nthe former has a lower runtime. Our efficient implementation elegantly trades-off memory with\nruntime, leading to constant memory footprint and a linear increase in runtime compared to PyTorch\u2019s\nautograd. This allows FARZI to scale to large autoregressive datasets without compromising on data\nfidelity.\n7\nPreprint\nTable 1: Cross-architecture generalization for FARZI DATA of size [50\u00d7150] of the ML-100k\ndataset. Note that the student network is used only for training on FARZI DATA, while the\nteacher network is used in FARZI\u2019s inner-loop. Further details about the following sequential\nrecommendation models can be found in Appendix C.4.\nTeacher\nStudent\nHR@10 / HR@100\nSASRec (Kang & McAuley, 2018)\nGRU4Rec (Hidasi et al., 2016)\nFMLP (Zhou et al., 2022a)\nSASRec\n19.61/61.50\n19.93/64.47\n17.81/58.21\nGRU4Rec\n18.23/61.08\n22.16/66.70\n20.14/60.23\nFull-Data\n18.23/60.65\n21.20/64.05\n17.28/59.27\n0\n100\n200\n300\n400\nInner-loop steps\n8\n10\n12\n14\n16\n18\nHR@10\n0\n100\n200\n300\n400\nInner-loop steps\n4\n5\n6\n7\n8\n9\nNDCG@10\n0\n100\n200\n300\n400\nInner-loop steps\n0\n0.8\n1.6\n2.4\n3.2\nRuntime (s)\n0\n100\n200\n300\n400\nInner-loop steps\n0\n5\n10\n15\n20\nMemory usage (GB)\nFarzi\u2019s Inner-loop Optimizer\nFull Data\nSGD\nAdam\n(Higher)\nAdam\n(PyTorch)\nAdam\n(E\ufb03cient)\nFigure 5: Changes in distillation performance and computational scalability of each outer-loop step\nfor different inner-loop optimizers and increasing number of inner-loop steps. All results are for\n[10\u00d7150] sized FARZI DATA of the ML-100k dataset.\nHow do different meta-objectives affect FARZI? We further evaluate the importance of FARZI\u2019s\noptimization objective by comparing it with existing DD approaches. We adapt existing approaches\nto work with autoregressive data by reusing the latent distillation proposition of FARZI, and vary only\nthe outer-loop goodness function to (i) gradient matching (DC (Zhao et al., 2021)); (ii) meta-matching\n(MM (Wang et al., 2018; Deng & Russakovsky, 2022)); or (iii) trajectory matching (MTT (Cazenavette\net al., 2022)). See the formal definitions for each of these objectives in Appendix C.5. Even though\nall existing DD approaches use SGD in their inner-loop, we nonetheless experiment with both SGD\nand our efficient reverse-mode Adam (Algorithm 1), and list the results in Table 2. We observe that\nAdam is a consistently better inner-loop optimizer irrespective of the meta-objective used. This is in\nstark contrast with existing DD studies which use SGD in the inner-loop. Further, FARZI significantly\noutperforms all existing DD techniques despite improving them to use Adam in the inner-loop.\nHow important are pre-trained trajectories for data distillation? To elicit the importance of\nthe pre-trained trajectories, i.e., \u2126 \u225c {[\u03b8i]T\ni=1 | \u03b80 \u223c \u0398} in FARZI\u2019s optimization (Equation (2)),\nwe plot the change in downstream distillation performance with increasing |\u2126| in Figure 6b. We\nindeed observe a massive improvement in downstream distillation performance with using as little as\njust 5 trajectories, compared to randomly initializing networks in FARZI\u2019s inner-loop. Notably, the\nimprovement saturates as we keep adding more trajectories to \u2126.\nDoes FARZI affect cold users or items more? A longstanding problem in recommender systems\nis modeling the cold-start scenario, i.e., users/items with less data. We study the effect training\nmodels on FARZI DATA from the cold-start perspective, by stratifying the users and items based on\ntheir popularity into equal-sized quantiles, and checking the trained model\u2019s performance on each\nindividual quantile. In Figure 6a, we do this for SASRec (Kang & McAuley, 2018) trained on (i) the\nfull dataset; and (ii) FARZI DATA synthesized using different hyper-parameter combinations. We first\nobserve that less popular items are harder to model, as is the typical case of recommender systems.\nFurther, we observe that models trained on FARZI DATA are, in expectation, (i) better on the tail/torso\n8\nPreprint\nTable 2: Comparison of FARZI with other existing DD techniques modified to distill autoregressive\ndata. Results for both using SGD or Adam as the inner-loop optimizer are listed. Meta-matching is\nshortened as MM. The best distillation result for each metric is colored orange, and the best result\nother than FARZI is colored blue for Adam-based methods and emboldened for SGD-based methods.\nDataset\nMetric\nRandom\nSampling\nData Distillation Objectives\nFull-Data\nDC\nMM\nMTT\nFARZI\nSGD\nAdam\nSGD\nAdam\nSGD\nAdam\nML-100k\n[50\u00d7150]\nHR@10 \u2191\n7.74\n7.95\n11.77\n9.65\n16.86\n12.19 14.52\n19.61\n18.23\nHR@100 \u2191\n39.13\n41.88\n49.52\n42.31\n58.43\n50.37 56.94\n61.50\n60.65\nnDCG@10 \u2191\n3.83\n3.44\n5.6\n4.72\n8.35\n6.12\n6.73\n9.91\n9.33\nnDCG@100 \u2191\n9.61\n9.84\n12.51\n10.85\n16.47\n13.03 14.66\n17.91\n17.69\nPTB\n[400\u00d750]\nPerplexity \u2193\n218.66\n203.23 131.07\n180.61 115.84\n202.98\n129.72\n91.92\n72.10\nAccuracy \u2191\n20.42\n20.64\n22.35\n21.60\n23.47\n21.00\n23.00\n25.16\n26.03\n25\n50\n100\n160\nUser frequency\n20\n40\n60\n80\nAvg. HR@100\n20\n50\n100\n200\n600\nItem frequency\n0\n20\n40\n60\n80\nAvg. HR@100\nFarzi\nFull Data\n(a)\n0\n25\n50\n75\n100\n# Trajectories\n12\n13\n14\n15\n16\n17\nHR@10\nFarzi\n(b)\nFigure 6: (a) Performance of SASRec trained on [50\u00d7150] sized FARZI DATA for ML-100k, and\nstratified over the popularity of users and items. The popularities are quantized into 10 equal sized\nbins and the average HR@100 is plotted. For results on more metrics see Appendix D, Figure 7.\n(b) Performance change of SASRec trained on [10\u00d7150] sized FARZI DATA for ML-100k with\nincreasing number of pretrained trajectories. For results on more metrics see Appendix D, Figure 8.\nregion of users/items; but (ii) worse for the head users/items. Notably, this behaviour is not directly\noptimized-for by FARZI, and is a by-product of the overall data-quality optimization in Equation (2).\n5\nCONCLUSION & FUTURE WORK\nIn this paper, we proposed FARZI \u2014 a scalable technique to summarize large amounts of autore-\ngressive data into a terse, high-fidelity data summary. Through extensive experiments on next-item\nrecommendation and language modeling, we demonstrated that data synthesized by FARZI (FARZI\nDATA) is able to train various kinds of state-of-the-art models to the same quality (if not better) as\ntraining them on the full dataset, despite FARZI DATA being up to 3 orders of magnitude smaller.\nHaving demonstrated FARZI DATA\u2019s prowess to train autoregressive models, we also highlight a few\nshortcomings and unexplored directions that we delay for future work. First, even though FARZI\nperforms distillation in a latent-space, it is hard to scale to applications that naturally consist of\nvery-long sequences, e.g., video, music, etc. because FARZI DATA is parameterized linearly in the\nlength of each sequence. Further, scaling to larger models (e.g., T5 (Raffel et al., 2020)) as well as\nlarger datasets (e.g., C4 (Raffel et al., 2019)) isn\u2019t as trivial due to practical constraints related to\noptimization and computational resources, but very important from a practical standpoint for future\nresearch, such as enabling cost-effective training of these large models on compact synthetic data.\nLaying down the foundation for data distillation in autoregressive modeling, FARZI also opens\nup new research directions from varied angles. First, the ability to train higher quality models\nusing less data is counter-intuitive and under-explored but also highly important from economical\nand environmental standpoints. Further, training models on differentialy private data summaries\n(Dong et al., 2022) instead of PII data can provide an added protection layer and be beneficial from\ncopyright-protection, ethics, and fairness perspectives.\n9\nPreprint\nACKNOWLEDGMENT\nWe thank Dougal Maclaurin and Zhiwei Deng for insightful discussions on reverse-mode Adam, and\nthank Zachary Novack for turning on a lab server at a critical time.\nREFERENCES\nAmro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-\nefficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540,\n2023.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, and Zhifeng Chen et al. Palm 2 technical report,\n2023.\nSamyadeep Basu, Phil Pope, and Soheil Feizi. Influence functions in deep learning are fragile. In\nInternational Conference on Learning Representations, 2021.\nJames Bennett, Stan Lanning, et al. The netflix prize. In Proceedings of KDD cup and workshop,\nvolume 2007, pp. 35. New York, 2007.\nJeff Bilmes.\nSubmodularity in machine learning and artificial intelligence.\narXiv preprint\narXiv:2202.00132, 2022.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-\nties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\nImproving language models by retrieving from trillions of tokens. In International conference on\nmachine learning, pp. 2206\u20132240. PMLR, 2022.\nZal\u00e1n Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual\nlearning and streaming. Advances in Neural Information Processing Systems, 33:14879\u201314890,\n2020a.\nZal\u00e1n Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual\nlearning and streaming. In Advances in Neural Information Processing Systems, volume 33. Curran\nAssociates, Inc., 2020b.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclau-\nrin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang.\nJAX: composable transformations of Python+NumPy programs, 2018.\nGeorge Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset\ndistillation by matching training trajectories. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 4750\u20134759, 2022.\nBenjamin Coleman, Benito Geordie, Li Chou, RA Leo Elworth, Todd Treangen, and Anshumali\nShrivastava. One-pass diversified sampling with application to terabyte-scale genomic sequence\nstreams. In International Conference on Machine Learning, pp. 4202\u20134218. PMLR, 2022.\nCody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy\nLiang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep\nlearning. In ICLR, 2020.\nZhiwei Deng and Olga Russakovsky. Remember the past: Distilling datasets into addressable\nmemories for neural networks. In Advances in Neural Information Processing Systems, 2022.\nTian Dong, Bo Zhao, and Lingjuan Lyu. Privacy for free: How does dataset condensation help\nprivacy? In Proceedings of the 39th International Conference on Machine Learning. PMLR, 2022.\n10\nPreprint\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language\nmodels with mixture-of-experts. In International Conference on Machine Learning, pp. 5547\u20135569.\nPMLR, 2022.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In International conference on machine learning, pp. 1126\u20131135. PMLR, 2017.\nAmirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning.\nIn International Conference on Machine Learning, pp. 2242\u20132251. PMLR, 2019.\nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http:\n//Skylion007.github.io/OpenWebTextCorpus, 2019.\nEdward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska\nMeier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala. Generalized inner loop meta-learning.\narXiv preprint arXiv:1910.01727, 2019.\nF Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm\ntransactions on interactive intelligent systems (tiis), 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing\nhuman-level performance on imagenet classification. In Proceedings of the IEEE international\nconference on computer vision, pp. 1026\u20131034, 2015.\nBal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based\nrecommendations with recurrent neural networks. In 4th International Conference on Learning\nRepresentations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings,\n2016.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nAn empirical analysis of compute-optimal large language model training. Advances in Neural\nInformation Processing Systems, 35:30016\u201330030, 2022.\nTimothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural\nnetworks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):\n5149\u20135169, 2021.\nAngela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean, Gregory R Ganger,\nGauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C Lipton, et al. Accelerating deep\nlearning by focusing on the biggest losers. arXiv preprint arXiv:1910.00762, 2019.\nWei Jin, Xianfeng Tang, Haoming Jiang, Zheng Li, Danqing Zhang, Jiliang Tang, and Bing Yin.\nCondensing graphs via one-step gradient matching. In Proceedings of the 28th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining, pp. 720\u2013730, 2022a.\nWei Jin, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, and Neil Shah. Graph condensation\nfor graph neural networks. In International Conference on Learning Representations, 2022b.\nW. Kang and J. McAuley. Self-attentive sequential recommendation. In 2018 IEEE International\nConference on Data Mining, 2018.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nZohar Karnin and Edo Liberty. Discrepancy, coresets, and sketches in machine learning. In Conference\non Learning Theory, pp. 1975\u20131993. PMLR, 2019.\nV. Kaushal, R. Iyer, S. Kothawade, R. Mahadev, K. Doctor, and G. Ramakrishnan. Learning from\nless data: A unified data subset selection and active learning framework for computer vision. In\n2019 IEEE Winter Conference on Applications of Computer Vision (WACV), 2019.\n11\nPreprint\nEhsan Kazemi, Shervin Minaee, Moran Feldman, and Amin Karbasi. Regularized submodular maxi-\nmization at scale. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International\nConference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.\n5356\u20135366. PMLR, 18\u201324 Jul 2021.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\nAndreas Krause, Marco Tagliasacchi, and Zal\u00e1n Borsos. Semi-supervised batch active learning via\nbilevel optimization. In 2021 IEEE International Conference on Acoustics, Speech and Signal\nProcessing, 2021.\nWalid Krichene and Steffen Rendle. On sampled metrics for item recommendation. In Proceedings\nof the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,\nKDD \u201920, 2020.\nI Elizabeth Kumar, Suresh Venkatasubramanian, Carlos Scheidegger, and Sorelle Friedler. Problems\nwith shapley-value-based explanations as feature importance measures. In International Conference\non Machine Learning, pp. 5491\u20135500. PMLR, 2020.\nYongchan Kwon and James Zou. Data-oob: Out-of-bag estimate as a simple and efficient data value.\nIn International conference on machine learning. PMLR, 2023.\nYongchan Kwon, Manuel A Rivas, and James Zou. Efficient computation and analysis of distributional\nshapley values. In International Conference on Artificial Intelligence and Statistics, pp. 793\u2013801.\nPMLR, 2021.\nFabian Latorre, Leello Tadesse Dadi, Paul Rolland, and Volkan Cevher. The effect of the intrinsic\ndimension on the generalization of quadratic classifiers. Advances in Neural Information Processing\nSystems, 34:21138\u201321149, 2021.\nDawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. Variational autoencoders\nfor collaborative filtering. In Proceedings of the 2018 World Wide Web Conference, WWW \u201918,\n2018.\nJonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by\nimplicit differentiation. In International Conference on Artificial Intelligence and Statistics, pp.\n1540\u20131552. PMLR, 2020.\nDougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization\nthrough reversible learning. In International conference on machine learning, pp. 2113\u20132122.\nPMLR, 2015.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated\ncorpus of English: The Penn Treebank. Computational Linguistics, 19(2):313\u2013330, 1993.\nA. Mittal, N. Sachdeva, S. Agrawal, S. Agarwal, P. Kar, and M. Varma. Eclare: Extreme classifi-\ncation with label graph correlations. In Proceedings of The ACM International World Wide Web\nConference, 2021.\nTimothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely\nwide convolutional networks. Advances in Neural Information Processing Systems, 34, 2021.\nJianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled\nreviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods\nin natural language processing and the 9th international joint conference on natural language\nprocessing (EMNLP-IJCNLP), pp. 188\u2013197, 2019a.\nJianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled\nreviews and fine-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), 2019b.\nOpenAI. Gpt-4 technical report, 2023.\n12\nPreprint\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,\nB. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep\nlearning library. In NeurIPS, 2019.\nMansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding\nimportant examples early in training. Advances in Neural Information Processing Systems, 34:\n20596\u201320607, 2021.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. CoRR, 2018.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356,\n2022.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. arXiv e-prints, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\nNoveen Sachdeva and Julian McAuley. Data distillation: A survey. Transactions on Machine\nLearning Research, 2023. ISSN 2835-8856. Survey Certification.\nNoveen Sachdeva, Giuseppe Manco, Ettore Ritacco, and Vikram Pudi. Sequential variational\nautoencoders for collaborative filtering. In Proceedings of the ACM International Conference on\nWeb Search and Data Mining, WSDM \u201919, 2019.\nNoveen Sachdeva, Carole-Jean Wu, and Julian McAuley. Svp-cf: Selection via proxy for collaborative\nfiltering data. arXiv preprint arXiv:2107.04984, 2021.\nNoveen Sachdeva, Mehak Preet Dhaliwal, Carole-Jean Wu, and Julian McAuley. Infinite recommen-\ndation networks: A data-centric approach. In Advances in Neural Information Processing Systems,\n2022a.\nNoveen Sachdeva, Ziran Wang, Kyungtae Han, Rohit Gupta, and Julian McAuley. Gapformer: Fast\nautoregressive transformers meet rnns for personalized adaptive cruise control. In 2022 IEEE 25th\nInternational Conference on Intelligent Transportation Systems (ITSC), pp. 2528\u20132535. IEEE,\n2022b.\nNoveen Sachdeva, Carole-Jean Wu, and Julian McAuley. On sampling collaborative filtering datasets.\nIn Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining,\nWSDM \u201922, pp. 842\u2013850, New York, NY, USA, 2022c. Association for Computing Machinery.\nISBN 9781450391320. doi: 10.1145/3488560.3498439.\nOzan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set\napproach. In ICLR, 2018.\nShai Shalev-Shwartz and Shai Ben-David.\nUnderstanding machine learning: From theory to\nalgorithms. Cambridge university press, 2014.\nLloyd S Shapley. A value for n-person games. In Harold W. Kuhn and Albert W. Tucker (eds.),\nContributions to the Theory of Games II, pp. 307\u2013317. Princeton University Press, Princeton, 1953.\nBen Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural\nscaling laws: beating power law scaling via data pruning. Advances in Neural Information\nProcessing Systems, 35:19523\u201319536, 2022.\n13\nPreprint\nFei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential\nrecommendation with bidirectional encoder representations from transformer. In Proceedings of the\n28th ACM international conference on information and knowledge management, pp. 1441\u20131450,\n2019.\nPei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, et al. Scalability in perception\nfor autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 2446\u20132454, 2020.\nWilson L Taylor. \u201ccloze procedure\u201d: A new tool for measuring readability. Journalism quarterly, 30\n(4):415\u2013433, 1953.\nM. Toneva, A. Sordoni, R. Combes, A. Trischler, Y. Bengio, and G. Gordon. An empirical study of\nexample forgetting during deep neural network learning. In ICLR, 2019.\nJoel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends\u00ae in\nMachine Learning, 8(1-2):1\u2013230, 2015.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, \u0141. Kaiser, and I. Polosukhin.\nAttention is all you need. In NeurIPS, 2017.\nJiachen T Wang and Ruoxi Jia. Data banzhaf: A robust data valuation framework for machine\nlearning. In International Conference on Artificial Intelligence and Statistics, pp. 6388\u20136421.\nPMLR, 2023.\nTongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv\npreprint arXiv:1811.10959, 2018.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n12104\u201312113, 2022.\nBo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In\nInternational Conference on Machine Learning, pp. 12674\u201312685. PMLR, 2021.\nBo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2023.\nBo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. In\nInternational Conference on Learning Representations, 2021.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\nKun Zhou, Hui Yu, Wayne Xin Zhao, and Ji-Rong Wen. Filter-enhanced mlp is all you need for\nsequential recommendation. In Proceedings of the ACM Web Conference 2022, pp. 2388\u20132399,\n2022a.\nYongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature regres-\nsion. In Advances in Neural Information Processing Systems, 2022b.\n14\nPreprint\nA\nALGORITHMS\nAlgorithm 2 Adam optimization (Kingma & Ba, 2015)\n1: Input: initial w0, decays (\u03b21, \u03b22), learning rate \u03b1, constant \u03f5, loss function L(w, x)\n2: Initialize: m0 \u2190 0, v0 \u2190 0\n3: for t = 1 to T do\n4:\ngt \u225c \u2207wL(wt\u22121, x)\n\u25b7 evaluate gradient\n5:\nmt = \u03b21 \u00b7 mt\u22121 + (1 \u2212 \u03b21) \u00b7 gt\n\u25b7 biased first moment estimate\n6:\nvt = \u03b22 \u00b7 vt\u22121 + (1 \u2212 \u03b22) \u00b7 g2\nt\n\u25b7 biased second moment estimate\n7:\n\u02c6mt \u225c mt/(1 \u2212 \u03b2t\n1)\n\u25b7 bias-corrected first moment estimate\n8:\n\u02c6vt \u225c vt/(1 \u2212 \u03b2t\n2)\n\u25b7 bias-corrected second moment estimate\n9:\nwt = wt\u22121 \u2212 \u03b1 \u00b7 \u02c6mt/(\u02c6vt + \u03f5)\n\u25b7 update parameters\n10: Output: trained parameters wT , biased first moment mT , biased second moment vT\nB\nPROOFS\nB.1\nPROOF OF THEOREM 3.1\nProof. We first begin by defining a few preliminary terms and properties:\nDefinition B.1. (Representativeness of D) For a given function-class F, task loss function l(\u00b7),\ntrain-set D = {x1, x2, . . . , xn} sampled from the true data distribution Pn:\nRep(F, D) \u225c sup\nf\u2208F\n\u0010\nE\nx\u223cP [l(f, x)] \u2212 E\nx\u223cD [l(f, x)]\n\u0011\n,\nwhich intuitively measures the maximum discrepancy between the empirical risk and the true risk for\na given training set. Naturally, a lower Rep(F, \u00b7) avoids overfitting and is desirable.\nDefinition B.2. (Rademacher complexity) For a given given function-class F, and train-set\nD = {x1, x2, . . . , xn}:\nR(F, D) \u225c\nE\n\u03c31,\u03c32,...,\u03c3n\u2208{\u00b11}\n\"\nsup\nf\u2208F\n\u03c3i \u00b7 f(xi)\nn\n#\n,\nwhere \u03c31, \u03c32, . . . , \u03c3n are independent random variables from the Rademacher distribution. R(F, D)\nintuitively measures the learning capacity of F by it\u2019s ability to fit random label assignments of D.\nLemma B.3. (Lemma 26.2 in Shalev-Shwartz & Ben-David (2014, Chapter 26))\nE\nD\u223cPn [Rep(F, D)] \u2264 2\nE\nD\u223cPn [R(F, D)]\nLemma B.4. (Theorem 1 in Latorre et al. (2021)) Let F\u03bb be the set of norm-bounded quadratic\nclassifiers:\nF\u03bb \u225c {fw : fw(x) = xT wx , ||w|| < \u03bb}\nThen for a training-set X \u2208 Rn\u00d7d:\nR(F, X)\n<\u223c \u03bb\nr\nr(XT X) log d\nn\n||XT X||2\ns.t. r(XT X) \u225c trace(XT X)\n||XT X||2\n,\nwhere, r(\u00b7) represents the intrinsic dimension of a PSD matrix (Tropp et al., 2015, Chapter 7).\nNow we\u2019re ready to prove Theorem 3.1. In our case, given the norm-bounded quadratic function-class\nF\u03bb and Dnaive \u2208 R\u00b5\u00d7\u03be\u00d7dim(V) such that w.l.o.g ||Dnaive||2 = 1:\n15\nPreprint\nE\nDnaive\u223cR\u00b5\u00d7\u03be\u00d7dim(V) [Rep(F\u03bb, Dnaive)] \u2264 2\nE\nDnaive\u223cR\u00b5\u00d7\u03be\u00d7dim(V) [R(F, Dnaive)]\n(Lemma B.3)\n<\u223c\nE\nDnaive\u223cR\u00b5\u00d7\u03be\u00d7dim(V)\n\uf8ee\n\uf8f02\u03bb\ns\nr(DT\nnaiveDnaive) log(\u03be dim(V))\n\u00b5\n\uf8f9\n\uf8fb\n(Lemma B.4)\nFurthermore, the intrinsic dimension of a PSD matrix A obeys:\nr(A) \u225c trace(A)\n||A||2\n=\nP\ni \u03bbi(A)\n\u03bbmax(A)\n\u2264 \u03bbmax(A) \u00b7 rank(A)\n\u03bbmax(A)\n\u2264 rank(A)\nwhere, the first line uses the alternate trace definition, and norm-eigenvalue equivalence. Combining\nthe two findings:\nE\nDnaive\u223cR\u00b5\u00d7\u03be\u00d7dim(V) [Rep(F\u03bb, Dnaive)]\n<\u223c\nE\nDnaive\u223cR\u00b5\u00d7\u03be\u00d7dim(V)\n\"\n2\u03bb\ns\nlog(\u03be dim(V))\n\u00b5\nq\nrank(DT\nnaiveDnaive)\n#\n<\u223c 2\u03bb\ns\nlog(\u03be dim(V))\n\u00b5\nE\nDnaive\u223cR\u00b5\u00d7\u03be\u00d7dim(V)\nhp\nrank(Dnaive)\ni\n<\u223c 2\u03bb\ns\nlog(\u03be dim(V))\n\u00b5\n\u00b7 min(\u221a\u00b5,\np\n\u03be \u00b7 dim(V))\n(3)\nOn the other hand, for the non-parameterized formulation of FARZI DATA:\nE\n\u02dc\nDsyn\u223cR\u00b5\u00d7\u03be\u00d7d\nM\u223cRd\u00d7dim(V)\nh\nRep(F\u03bb, \u02dcDsyn \u00b7 M)\ni <\u223c 2\u03bb\ns\nlog(\u03be dim(V))\n\u00b5\nE\n\u02dc\nDsyn\u223cR\u00b5\u00d7\u03be\u00d7d\nM\u223cRd\u00d7dim(V)\n\u0014q\nrank( \u02dcDsyn \u00b7 M)\n\u0015\n<\u223c 2\u03bb\ns\nlog(\u03be dim(V))\n\u00b5\n\u00b7\n\u221a\nd\n(4)\nFinally, comparing Equations (3) and (4):\nE\n\u02dc\nDsyn,M\nh\nRep(F, \u02dcDsyn \u00b7 M)\ni\n<\nE\nDnaive [Rep(F, Dnaive)]\nif d < min(\u00b5, \u03be \u00b7 dim(V))\nB.2\nPROOF OF PROPOSITION 3.2\nProof. Using the chain rule of derivatives:\ndf(wT )\ndm0\n\u225c dm = dm + \u2202wt\n\u2202mt\n\u00b7 dw\n= dm \u2212\n\u03b1\n1 \u2212 \u03b2t\n1\n\uf8eb\n\uf8ed\n\u0002\u221a\u02c6vt + \u03f5\n\u0003\n\u2212\nh\nmt \u00b7\n\u0010\n\u2202vt/\u2202mt\n2\u00b7\u221a\u02c6vt\u00b7(1\u2212\u03b2t\n2)\n\u0011i\n(\u221a\u02c6vt + \u03f5)2\n\uf8f6\n\uf8f8 \u00b7 dw\n= dm + \u03b1\u2032 \u00b7\n\uf8eb\n\uf8ed\nh\nmt \u00b7\n\u0010\n\u2202vt/\u2202mt\n2\u00b7\u221avt\n\u0011i\n(\u221avt + \u03f5\u2032)2\n\u2212\n1\n\u221avt + \u03f5\u2032\n\uf8f6\n\uf8f8 \u00b7 dw ,\n16\nPreprint\nwhere, \u03f5\u2032 \u225c \u03f5 \u00b7\np\n1 \u2212 \u03b2t\n2 and \u03b1\u2032 \u225c\n\u03b1\u00b7\u221a\n1\u2212\u03b2t\n2\n1\u2212\u03b2t\n1\n.\nUsing the chain rule again:\n\u2202vt\n\u2202mt\n=\n\u2202vt/\u2202gt\n\u2202mt/\u2202gt\n= 2 \u00b7 (1 \u2212 \u03b22) \u00b7 gt\n(1 \u2212 \u03b21)\n= 2 \u00b7 \u03b2\u2032 \u00b7 gt\n,\nwhere, \u03b2\u2032 \u225c 1\u2212\u03b22\n1\u2212\u03b21 , leading to finally:\ndf(wT )\ndm0\n\u225c dm = dm + \u03b1\u2032 \u00b7\n\uf8eb\n\uf8ed\nh\nmt \u00b7\n\u0010\n\u2202vt/\u2202mt\n2\u00b7\u221avt\n\u0011i\n(\u221avt + \u03f5\u2032)2\n\u2212\n1\n\u221avt + \u03f5\u2032\n\uf8f6\n\uf8f8 \u00b7 dw\n= dm + \u03b1\u2032 \u00b7\n\u0012\n\u03b2\u2032 \u00b7 mt \u00b7 gt\n\u221avt \u00b7 (\u221avt + \u03f5\u2032)2 \u2212\n1\n\u221avt + \u03f5\u2032\n\u0013\n\u00b7 dw\nC\nEXPERIMENTAL DETAILS\nC.1\nMETRICS\nWe present a formal definition of all metrics used in this paper for both sequential recommendation\nand language modeling tasks.\nSequential Recommendation. We start by outlining some notation for defining the metrics. Let\nthe set of users in the test-set be denoted by U and the set of all items be denoted by I. For each\nuser u \u2208 U, we denote its set of positive interactions I+\nu \u2286 I, and similarly define its set of negative\ninteractions I\u2212\nu \u225c I\\I+\nu . We now define the metrics for evaluating the quality of a recommender\nsystem \u03a6 : U 7\u2192 Ik which generates a set of k item recommendations, as follows:\n\u2022 AUC: Intuitively defined as a threshold independent classification performance measure, AUC\ncan also be interpreted as the expected probability of a recommender system ranking a positive\nitem over a negative item for any given user. More formally, let \u03a6\u2019s underlying relevance\npredictor be \u03a6logit : U \u00d7 I 7\u2192 R, then the AUC for \u03a6 is defined as:\nAUC(\u03a6) \u225c E\nu\u223cU\n\u0014\nE\ni+\u223cI+\nu\n\u0014\nE\ni\u2212\u223cI\u2212\nu\n\u0002\n\u03a6logit(u, i+) > \u03a6logit(u, i\u2212)\n\u0003\u0015\u0015\n\u2022 HitRate (HR@k): Also termed as Recall@k; HR@k estimates how many positive items are\npredicted in \u03a6\u2019s top-k recommendation list. More formally, the HR@k for \u03a6 is defined as:\nHR@k(\u03a6) \u225c E\nu\u223cU\n\u0014|\u03a6(u) \u2229 I+\nu |\n|I+\nu |\n\u0015\n\u2022 Normalized Discounted Cumulative Gain (nDCG@k): Unlike HR@k which gives equal\nimportance to all items in the recommendation list, the nDCG@k metric instead gives a higher\nimportance to items predicted higher in the recommendation list and performs logarithmic\ndiscounting further down. More formally, let index(i, \u03a6(u)) denote the index of item i in the\nsorted recommendation list \u03a6(u), then the nDCG@k for \u03a6 is defined as:\nnDCG@k(\u03a6) \u225c E\nu\u223cU\n\u0014DCGu(\u03a6)\nIDCGu\n\u0015\nDCGu(\u03a6) \u225c\nX\ni\u2208I+\nu\ni \u2208 \u03a6(u)\nlog2\n\u0000index(i, \u03a6(u)) + 1\n\u0001\n;\nIDCGu \u225c\n|I+\nu |\nX\ni=1\n1\nlog2(i + 1)\n17\nPreprint\nLanguage Modeling. We first use Perplexity (PPL) to evaluate language modeling performance.\nPerplexity quantifies how uncertain the model is when trying to predict the next word in a sequence,\ngiven the previous words. Given a sentence \u20d7xi, which is tokenized into a sequence of tokens\n[w1, w2, \u00b7 \u00b7 \u00b7 , w| \u20d7xi|], the sentence PPL is defined as:\nlog2 (PPLi) \u225c \u2212 1\n|\u20d7xi|\n| \u20d7xi|\nX\ni\nlog2 P(wi|w1, w2, \u00b7 \u00b7 \u00b7 , wi\u22121)\nwhere P is the probability assigned by the language model to wi given the context of the previous\nwords. Then, given a corpus C containing N sentences C = { \u20d7x1, \u20d7x2, \u00b7 \u00b7 \u00b7 , \u20d7\nxN}, the perplexity over C\nis defined as the average PPL over the sentence PPLs:\nPPLC \u225c 1\nN \u00b7\nN\nX\ni\nPPLi\nTo better evaluate the generation quality of a language model, we also evaluate the average top-1\npredicted token accuracy after greedy decoding, similar to the HR@1 metric described earlier.\nC.2\nDATASETS\nWe list the datasets used in this paper as well as brief data statistics in Table 3. We discuss other\ntask-specific preprocessing and train/test splitting strategy below.\nSequential Recommendation. Owing to recent work (Sachdeva et al., 2022c), we follow the\nminimal amount of preprocessing by only removing the users with less than two total interactions.\nWe simulate the train/test split from the strong-generalization school-of-thought (Liang et al., 2018),\nwhere we keep a completely disjoint set of 80/10/10% train, validation, and test users split randomly.\nFor each user in the validation/test-set, the chronologically last interacted item is used for computing\nranking metrics, whereas all previous interactions are used as context for the model. Further, to\nsimulate a realistic recommendation scenario, we compute all metrics on the full item-space without\nany down-sampling (Krichene & Rendle, 2020). The definition of all metrics used in this paper can\nbe found in Appendix C.1.\nLanguage Modeling. We employ the Penn Treebank (PTB) dataset, an established and openly\naccessible benchmark extensively utilized in natural language processing and language modeling\ntasks, as introduced by (Marcus et al., 1993). We use the train/validation/test split of the official\nrelease. The original PTB corpus consists of more than 4.5 million words of American English,\nfeaturing a word vocabulary of 9,999 words, including the <unk> token. In our experimentation,\nwe opt to maintain a vocabulary comprising 2,000 words with the highest frequencies, while any\nout-of-vocabulary words are represented as <unk>.\nC.3\nHYPER-PARAMETERS\nFor the sake of better reproducibility, we list all hyper-parameter combinations tried for our experi-\nments in Tables 4 and 5.\nC.4\nADDITIONAL DETAILS\nWe provide brief descriptions about all kinds of model architectures used in this paper for different\nexperiments:\n\u2022 Transformer (Vaswani et al., 2017). A causal transformer architecture for language modeling.\nThe hyper-parameters are listed in Table 5.\n\u2022 SASRec (Kang & McAuley, 2018). A causal transformer architecture for sequential recommen-\ndation. The hyper-parameters are listed in Table 4.\n\u2022 GRU4Rec (Hidasi et al., 2016). An GRU-based architecture for sequential recommendation,\ntrained using the cross-entropy loss. We use a single, 16-dimensional hidden-layer for the\nGRU4Rec architecture which was ascertained by conducting a grid-search on the ML-100k\u2019s\nvalidation-set.\n18\nPreprint\n\u2022 FMLP (Zhou et al., 2022a). An all-MLP architecture which replaces the self-attention blocks\nin SASRec with filter-enhanced MLPs for sequential recommendation. We use a single, 256-\ndimensional block for the FMLP architecture which was ascertained by conducting a grid-search\non the ML-100k\u2019s validation-set.\nC.5\nALTERNATIVE DATA DISTILLATION OBJECTIVES\nWe provide a brief description and formal optimization of other existing data distillation objectives\nused in Section 4.2. Note that we list the modified optimization objectives where we use FARZI\u2019s\nlatent factorization, and use Opt to denote the underlying inner-loop optimizer (SGD or Adam).\n\u2022 DC (Zhao et al., 2021): This data distillation objective performs one-step gradient matching\nusing a distance function D : R|\u03a6| \u00d7 R|\u03a6| 7\u2192 R:\narg min\nM, \u02dc\nDsyn\nE\n\u03b80\u223c\u0398\n\" T\nX\nt=0\nD\n\u0000\u2207\u03b8LD(\u03b8t), \u2207\u03b8LDsyn(\u03b8t)\n\u0001\n#\ns.t.\n\u03b8t+1 \u2190 Opt\n\u0000\u03b8t, \u2207\u03b8LDsyn(\u03b8t)\n\u0001\n;\nDsyn \u2190 softmax\n\u0010\n\u02dcDsyn \u00b7 M / \u03c4\n\u0011\n.\n\u2022 MM (Wang et al., 2018; Deng & Russakovsky, 2022): The meta-matching objective computes\nthe meta-gradient by unrolling the inner-loop optimization starting from random networks:\narg min\nM, \u02dc\nDsyn\nE\n\u03b80\u223c\u0398 [LD(\u03b8T )]\ns.t.\n\u03b8t+1 \u2190 Opt\n\u0000\u03b8t, \u2207\u03b8LDsyn(\u03b8t)\n\u0001\n;\nDsyn \u2190 softmax\n\u0010\n\u02dcDsyn \u00b7 M / \u03c4\n\u0011\n.\n\u2022 MTT (Cazenavette et al., 2022): The trajectory matching objective computes the meta-gradient\nby matching the parameters of networks trained on the real data for M optimization steps vs.\nmodels trained on the data summary for N \u226a M steps. Let {\u03b8D\nt }T\nt=0 represent the training\ntrajectory of training \u03a6\u03b8 on D, and D : R|\u03a6| \u00d7 R|\u03a6| 7\u2192 R be a pertinent distance function:\narg min\nM, \u02dc\nDsyn\nE\n\u03b80\u223c\u0398\n\uf8ee\n\uf8f0\nT \u2212M\nX\nt=0\nD\n\u0010\n\u03b8D\nt+M, \u03b8Dsyn\nt+N\n\u0011\nD\n\u0000\u03b8D\nt+M, \u03b8D\nt\n\u0001\n\uf8f9\n\uf8fb\ns.t.\n\u03b8Dsyn\nt+i+1 \u2190 Opt\n\u0010\n\u03b8Dsyn\nt+i , \u2207\u03b8LDsyn(\u03b8Dsyn\nt+i )\n\u0011\n;\n\u03b8Dsyn\nt+1 \u2190 Opt\n\u0000\u03b8D\nt , \u2207\u03b8LDsyn(\u03b8D\nt )\n\u0001\n;\nDsyn \u2190 softmax\n\u0010\n\u02dcDsyn \u00b7 M / \u03c4\n\u0011\n.\nD\nADDITIONAL RESULTS\nWe plot extended plots for the experiments conducted in Section 4.2:\n\u2022 In Table 6, we plot the sample quality results of FARZI DATA in a tabular format for all datasets\nand metrics.\n\u2022 In Figure 7, we analyze FARZI DATA\u2019s effect on cold users and cold items for all metrics\ndescribed in Appendix C.1.\n\u2022 In Figure 8, we analyze FARZI\u2019s reliance on the number of pretrained trajectories for all metrics\ndescribed in Appendix C.1.\n\u2022 In Figure 9, we plot the sample efficiency of FARZI DATA for sequential recommendation for\nall metrics described in Appendix C.1.\n19\nPreprint\nTable 3: Datasets used in this paper as well as a brief set of statistics.\nDataset\n# Users /\n# Items /\n# Interactions /\nSeq. Length\n# Sentences\n# Unique tokens\n# Total tokens\nMean / Median / Min\nAmazon Magazine (Ni et al., 2019b)\n3k\n1.3k\n12k\n4.10 / 3 / 3\nML-100k (Harper & Konstan, 2015)\n943\n1.6k\n100k\n104.04 / 63 / 18\nML-1M (Harper & Konstan, 2015)\n6k\n3.7k\n1M\n165.22 / 95 / 20\nNetflix (Bennett et al., 2007)\n476k\n17k\n100M\n210.91 / 98 / 3\nPTB (Marcus et al., 1993)\n49k\n10k\n1M\n22 / 21 / 2\nTable 4: List of all hyper-parameters combinations tried for FARZI and other baselines for sequential\nrecommendation.\nHyper-Parameter\nModel\nMagazine\nML-100k\nML-1M\nNetflix\nLatent size\nSASRec\n{8, 16, 32, 50, 64, 128}\nGRU4Rec\nFMLP\n# Layers\nSASRec\n{1, 2}\nGRU4Rec\nFMLP\nAttention Heads\nSASRec\n{1, 2}\nFMLP\nLearning rate\nSASRec\n{0.01, 0.02, 0.05}\nGRU4Rec\n{0.01, 0.02, 0.05}\nFMLP\n{0.0001, 0.0002, 0.0005}\nDropout\nSASRec\n{0.0, 0.2, 0.4}\nGRU4Rec\nFMLP\nFARZI\n\u03be\nFARZI\n{10, 20}\n{50, 100, 150}\n{50, 100, 150}\n200\nd\nFARZI\n8\n8\n32\n32\n\u03c4\nFARZI\n{0.5, 1, 2}\n|\u2126|\nFARZI\n100\n100\n100\n50\nInner\nloop\nWeight Decay\nFARZI\n{0, 10\u22126}\nLearning Rate\n{0.01, 0.02}\n# Steps\n{100, 200, 300}\n\u03b21\n0.9\n\u03b22\n0.999\nSGD Momentum\n{0.5, 0.75, 0.9, 0.95, 0.99}\nOuter\nloop\nWeight Decay\nFARZI\n{0, 10\u22126, 10\u22124}\nLearning Rate\n0.01\n# Steps\n4000\nBatch\nsize\nD\nFARZI\n512\nDsyn\n\u2014\n\u2014\n50\n25\n20\nPreprint\nTable 5: List of all hyper-parameters combinations tried for FARZI and other baselines for language\nmodeling.\nHyper-Parameter\nModel\nPenn Treebank\nLatent size\nTransformer\n16\nRNN\n# Layers\nTransformer\n1\nRNN\nAttention Heads\nTransformer\n1\nRNN\nLearning rate\nTransformer\n{0.01, 0.02, 0.05}\nRNN\n{0.01, 0.02, 0.05}\nDropout\nTransfomer\n{0.0, 0.2}\nRNN\n\u03be\nFARZI\n{5, 15}\nd\nFARZI\n8\n\u03c4\nFARZI\n{0.5, 1, 2}\n|\u2126|\nFARZI\n400\nInner\nloop\nWeight Decay\nFARZI\n{0, 10\u22127}\nLearning Rate\n{0.01, 0.02}\n# Steps\n{200, 300,400, 500, 600}\n\u03b21\n0.999\nSGD Momentum\n-\nOuter\nloop\nWeight Decay\nFARZI\n{0, 10\u22126, 10\u22124}\nLearning Rate\n0.01\n# Steps\n8000\nBatch\nsize\nD\nFARZI\n256\nDsyn\n\u2014\n21\nPreprint\nTable 6: Performance change of SASRec & Transformer with various sizes of FARZI DATA for\nsequential recommendation & language modeling tasks respectively. The best result for each dataset\n& metric is colored orange.\nDataset &\nModel\nFARZI DATA\nsize\nHR@10\nHR@100\nnDCG@10\nnDCG@100\nAUC\nPPL\nAcc.\nMagazine\n&\nSASRec\n[10 x 10] \u2261 0.3%\n23.3\n52.3\n15.8\n21.1\n0.8428\n-\n-\n[25 x 20] \u2261 0.8%\n23.9\n52.3\n16.5\n21.6\n0.8307\n-\n-\n[50 x 20] \u2261 1.6%\n24.5\n52.1\n17.1\n22.1\n0.8291\n-\n-\nFull-data\n23.2\n52.0\n16.9\n21.7\n0.8223\n-\n-\nML-100k\n&\nSASRec\n[10 x 150] \u2261 1%\n17.3\n61.2\n9.2\n17.7\n0.8957\n-\n-\n[25 x 150] \u2261 2.6%\n19.3\n61.6\n9.9\n17.7\n0.902\n-\n-\n[50 x 50] \u2261 5.3%\n19.6\n62.9\n9.9\n18.1\n0.9016\n-\n-\n[100 x 100] \u2261 10.6%\n19.5\n61.9\n10.1\n18.1\n0.9016\n-\n-\nFull-data\n18.2\n60.6\n9.3\n17.6\n0.9011\n-\n-\nML-1M\n&\nSASRec\n[10 x 150] \u2261 0.1%\n22.4\n59.0\n12.0\n19.0\n0.923\n-\n-\n[50 x 100] \u2261 0.8%\n24.8\n61.6\n13.8\n20.8\n0.9301\n-\n-\n[100 x 100] \u2261 1.6%\n25.6\n63.6\n14.1\n21.3\n0.9317\n-\n-\n[200 x 50] \u2261 3.3%\n25.4\n61.8\n14.1\n21.0\n0.9315\n-\n-\n[500 x 50] \u2261 8.2%\n26.2\n61.0\n13.8\n20.7\n0.9293\n-\n-\nFull-data\n26.2\n62.8\n14.4\n21.8\n0.9291\n-\n-\nNetflix\n&\nSASRec\n[50 x 200] \u2261 0.01%\n15.6\n38.0\n9.9\n14.1\n0.9235\n-\n-\n[500 x 200] \u2261 0.1%\n17.8\n40.7\n11.6\n16.1\n0.9449\n-\n-\n[2000 x 200] \u2261 0.4%\n17.5\n40.3\n11.3\n15.8\n0.9455\n-\n-\nFull-data\n18.1\n41.9\n11.8\n16.4\n0.947\n-\n-\nPTB\n&\nTransformer\n[10 x 50] \u2261 0.02%\n-\n-\n-\n-\n-\n238.5\n20.48\n[200 x 50] \u2261 0.47%\n-\n-\n-\n-\n-\n124.0\n24.0\n[400 x 50] \u2261 1%\n-\n-\n-\n-\n-\n91.9\n25.16\n[2000 x 50] \u2261 4.7%\n-\n-\n-\n-\n-\n91.0\n25.4\nFull-data\n-\n-\n-\n-\n-\n72.10\n26.03\n25\n50\n100\n160\nUser frequency\n0\n10\n20\n30\nAvg. HR@10\n25\n50\n100\n160\nUser frequency\n20\n40\n60\n80\nAvg. HR@100\n25\n50\n100\n160\nUser frequency\n0\n5\n10\nAvg. NDCG@10\n25\n50\n100\n160\nUser frequency\n10\n20\nAvg. NDCG@100\n20\n50\n100\n200\n600\nItem frequency\n0\n20\n40\nAvg. HR@10\n20\n50\n100\n200\n600\nItem frequency\n0\n25\n50\n75\nAvg. HR@100\n20\n50\n100\n200\n600\nItem frequency\n0\n10\n20\nAvg. NDCG@10\n20\n50\n100\n200\n600\nItem frequency\n0\n10\n20\n30\nAvg. NDCG@100\nFarzi\nFull Data\nFigure 7: Performance of SASRec trained on [50\u00d7150] sized FARZI DATA of the ML-100k dataset,\nand stratified over the popularity of users and items. The user/item popularity spectrum is quantized\ninto 10 equal sized bins and the average corresponding metric is plotted.\n22\nPreprint\n0\n25\n50\n75\n100\n# Trajectories\n12\n13\n14\n15\n16\n17\nHR@10\n0\n25\n50\n75\n100\n# Trajectories\n50\n52\n54\n56\n58\n60\nHR@100\n0\n25\n50\n75\n100\n# Trajectories\n6\n6.6\n7.2\n7.8\n8.4\n9\nNDCG@10\n0\n25\n50\n75\n100\n# Trajectories\n13\n14\n15\n16\n17\nNDCG@100\n0\n25\n50\n75\n100\n# Trajectories\n0.86\n0.87\n0.88\n0.89\n0.90\nAUC\nFarzi\nFigure 8: Performance change of SASRec trained on [10\u00d7150] sized FARZI DATA of the ML-100k\ndataset with increasing number of pretrained trajectories.\n0.3% 1%\n5% 20% 100%\n5\n10\n15\n20\n25\nHR@10\nAmazon Magazine\n1%\n5%\n20%\n100%\n4\n8\n12\n16\n20\nML-100k\n0.2% 1%\n5% 20% 100%\n0\n6\n12\n18\n24\nML-1M\n0.01%0.1% 1% 10% 100%\n3\n6\n9\n12\n15\n18\nNet\ufb02ix\n0.3% 1%\n5% 20% 100%\n16\n24\n32\n40\n48\nHR@100\n1%\n5%\n20%\n100%\n20\n30\n40\n50\n60\n0.2% 1%\n5% 20% 100%\n10\n20\n30\n40\n50\n60\n0.01%0.1% 1% 10% 100%\n12\n18\n24\n30\n36\n42\n0.3% 1%\n5% 20% 100%\n3\n6\n9\n12\n15\n18\nNDCG@10\n1%\n5%\n20%\n100%\n2\n4\n6\n8\n10\n0.2% 1%\n5% 20% 100%\n0\n4\n8\n12\n16\n0.01%0.1% 1% 10% 100%\n2\n5\n7\n10\n12\n0.3% 1%\n5% 20% 100%\n4\n8\n12\n16\n20\nNDCG@100\n1%\n5%\n20%\n100%\n3\n6\n9\n12\n15\n18\n0.2% 1%\n5% 20% 100%\n0\n6\n12\n18\n24\n0.01%0.1% 1% 10% 100%\n3\n6\n9\n12\n15\n0.3% 1%\n5% 20% 100%\nData summary size\n0.56\n0.64\n0.72\n0.80\nAUC\n1%\n5%\n20%\n100%\nData summary size\n0.64\n0.72\n0.80\n0.88\n0.2% 1%\n5% 20% 100%\nData summary size\n0.60\n0.70\n0.80\n0.90\n0.01%0.1% 1% 10% 100%\nData summary size\n0.72\n0.80\n0.88\n0.96\nFull Data\nRandom\nHead\nFarzi\nFigure 9: Performance change of SASRec model with increasing data summary size (log-scale) for\nsequential recommendation.\n23\n"
  },
  {
    "title": "Microscaling Data Formats for Deep Learning",
    "link": "https://arxiv.org/pdf/2310.10537.pdf",
    "upvote": "4",
    "text": "Microscaling Data Formats for\nDeep Learning\nBita Darvish Rouhani\u2217 Ritchie Zhao Ankit More Mathew Hall Alireza Khodamoradi Summer Deng\nDhruv Choudhary Marius Cornea Eric Dellinger Kristof Denolf Stosic Dusan Venmugil Elango\nMaximilian Golub Alexander Heinecke Phil James-Roxby Dharmesh Jani Gaurav Kolhe\nMartin Langhammer Ada Li Levi Melnick Maral Mesmakhosroshahi Andres Rodriguez\nMichael Schulte Rasoul Shafipour Lei Shao Michael Siu Pradeep Dubey Paulius Micikevicius\nMaxim Naumov Colin Verrilli Ralph Wittig Doug Burger Eric Chung\nMicrosoft\nAMD\nIntel\nMeta\nNVIDIA\nQualcomm Technologies Inc.\nAbstract\nNarrow bit-width data formats are key to reducing the computational and storage\ncosts of modern deep learning applications. This paper evaluates Microscaling\n(MX) data formats that combine a per-block scaling factor with narrow floating-\npoint and integer types for individual elements. MX formats balance the competing\nneeds of hardware efficiency, model accuracy, and user friction. Empirical results\non over two dozen benchmarks demonstrate practicality of MX data formats as a\ndrop-in replacement for baseline FP32 for AI inference and training with low user\nfriction. We also show the first instance of training generative language models at\nsub-8-bit weights, activations, and gradients with minimal accuracy loss and no\nmodifications to the training recipe.\n1\nIntroduction\nRecent advances in AI capabilities such as conversational question answering, intelligent code\ncompletion, and text-to-image generation have seen rapid adoption in practical technologies. These\nadvances have been realized primarily through scaling up the size of the underlying deep learning\nmodel. However, this scaling up has led to a significant increase in the computing power and storage\ncapacity necessary to train and deploy such models.\nOne method to reduce deep learning models\u2019 computational and storage cost is to use low bit-width\ndata formats instead of the conventional FP32. Great strides have been made to enable training\nusing FP16, Bfloat16, and most recently FP8 [1], as well as to perform inference in narrow integer\nformats like INT8. Native support for low bit-width formats is now commonplace in AI-oriented\nhardware such as GPUs, TPUs, and edge inference devices. The narrowest formats, such as FP8\nand INT8, require per-tensor scaling factors to adjust to the dynamic range of each tensor. Tensor\nlevel scaling has has been shown to be insufficient, though, for sub-8-bit formats due to their limited\ndynamic range. Research has shown that micro scaled data formats that associate scaling factors with\nfine-grained sub-blocks of a tensor are more effective in sub-8 bit regime (e.g., [2; 3; 4; 5]).\nThis paper evaluates Microscaling (MX) data formats [6] \u2014 the first open standard for a family of\nmicro-scaled datatypes aimed at deep learning training and inference. The MX standard aims to\ncreate an effective data format by achieving a balance among three key factors:\n\u2022 Hardware Efficiency \u2014 Maximize compute and storage efficiency via reduced bit-width.\n\u2022 Model Accuracy \u2014 Minimize the gap in the quality of results compared with baseline\nFP32 for AI training and inference.\n\u2217email correspondence: birouhan@microsoft.com\narXiv:2310.10537v3  [cs.LG]  19 Oct 2023\n\u2022 User Friction \u2014 Ensure seamless integration within existing training and inference frame-\nworks and generalizability across different workloads.\nDetails on the MX standard and the concrete binary formats can be found in the OCP Microscaling\nSpecification [6]. This paper will focus on the empirical results of using MX formats for direct-cast\ninference, error diffusion inference, and finetuned inference, as well as training on various benchmarks.\nOur results corroborate the effectiveness of MX formats in balancing the competing demands of\nhardware efficiency, model accuracy, and user friction. 8-bit MX formats can perform inference\ndirectly on FP32 pretrained models with minimal accuracy loss and without the need for calibration\nor finetuning. Inference with 6-bit MX formats is also very close to FP32 after quantization-aware\nfine-tuning or using a post-training quantization method. Using 6-bit MX formats, we demonstrate the\nfirst instance of training large transformer models with sub-8-bit weights, activations, and gradients\nto an accuracy matching FP32 without modifications to the training recipe. Going even further, we\nshow that training of large transformers can be done with 4-bit MX format weights, incurring only a\nminor accuracy drop.\nThe custom CUDA library to emulate MX formats on existing GPUs can be found at [7]. This library\ncan be used to reproduce the experimental results reported in this paper.\n2\nMicroscaling\nA basic unit of data in an MX format represents a vector of k numbers and consists of a single shared\nscale X and k scalar elements {Pi}k\ni=1 (see Figure 1). This unit of data is called an MX block and is\ndefined by the combination of block size k, scale data format, and element data format. The two data\nformats are independent of one another, and all k elements share the same element data format. The\nlayout of an MX block is not prescribed \u2014 an implementation may store X contiguously with or\nseparately from the elements.\nX\n(shared scale)\nP1\n(element)\nP2\n(element)\nPk\n(element)\n\u2026\nk scalar \nelements\nFigure 1: A single block in a Microscaling data format. The block encodes a vector of k numbers,\neach with value XPi.\nLet {vi}k\ni=1 be the k real numbers represented in an MX block. The value of each number can be\ninferred as follows:\n\u2022 If X = NaN, then vi = NaN for all i\n\u2022 If |XPi| > V maxF loat32 then vi is implementation-defined\n\u2022 Otherwise, vi = XPi\nwhere V maxF loat32 refers to the largest representable magnitude in IEEE Float32.\n2.1\nSpecial Value Encodings\nMX formats can encode NaN in up to two ways. First: if X is NaN, then all k values in the MX block\nis NaN regardless of the encodings of Pi. Second: if X is not NaN, each element Pi may individually\nencode NaN.\nDepending on the element format, MX formats can encode Inf by letting X be a number (i.e., not a\nNaN) and each Pi individually encode Inf. The shared scale X does not encode Inf.\n2\n2.2\nConcrete MX Formats\nTable 1 shows the parameters that define the concrete MX formats, which are named by prepending\n\"MX\" to the name of the element data format. All concrete MX formats use E8M0 (an 8-bit exponent)\nas the format for the shared scale. The representable exponents of these formats is a superset of the\nrepresentable exponents of FP32.\nDetails on the FP8 element data formats can be found in the OCP FP8 specification [1]. Details on\nthe other element data formats and the E8M0 scale format can be found in the OCP Microscaling\nSpecification [6].\nTable 1: Concrete MX-compliant data formats and their parameters.\nFormat\nBlock\nScale\nScale\nElement\nElement\nName\nSize\nData Format\nBits\nData Format\nBit-width\nMXFP8\n32\nE8M0\n8\nFP8 (E4M3 / E5M2)\n8\nMXFP6\n32\nE8M0\n8\nFP6 (E2M3 / E3M2)\n6\nMXFP4\n32\nE8M0\n8\nFP4 (E2M1)\n4\nMXINT8\n32\nE8M0\n8\nINT8\n8\n3\nScalar Float to MX Format Conversion\nIn this paper, we use Algorithm 1 for conversion from scalar floating-point format (e.g., FP32) to an\nMX format. This algorithm follows the semantics outlined in Section 6.3 of the OCP Microscaling\nSpecification [6], and is provided as a working example. Note that, the specification allows for\nother implementation-defined conversion recipes \u2014 i.e., conversion to MX formats is not necessarily\nrequired to follow Algorithm 1.\nAlgorithm 1 Convert vector of scalar floats {Vi}k\ni=1 to an MX block {X, {Pi}k\ni=1}\nRequire: emaxelem = exponent of the largest normal number in the element data format\n1: shared_exp \u2190 \u230alog2(maxi(|Vi|))\u230b \u2212 emaxelem\n2: X \u2190 2shared_exp\n3: for i = 1 to k do\n4:\nPi = quantize_to_element_format(Vi/X), clamping normal numbers\n5: end for\n6: return X, {Pi}k\ni=1\nOn Line 1, shared_exp contains an offset of emax_elem to map the max input exponent to the\nlargest binade in the element data format. This enables full utilization of the element data format\u2019s\nexponent range.\nOn Line 4, when quantizing Vi/X, normal numbers that exceed the representable range of the\nelement format are clamped to the maximum representable value, preserving the sign. Infs and NaNs\nare not clamped. This is in accordance with the OCP MX specification.\nOn Line 4, Pi is set to zero if the corresponding input Vi is a subnormal Float32 number. This is not\ndescribed in the OCP MX specification and was done to simplify the algorithm.\nWhen converting multi-dimensional tensors, a principle axis must be selected for the shared scale\n(typically the reduction dimension in matrix multiplication). For a 2D matrix, the scale can be shared\nby every k element in a row or column. Transposing a 2D matrix in an MX format changes the axis\nof the shared scale \u2014 i.e., conversion to MX format and transposing are not commutative operations.\n4\nExperimental Results\n4.1\nCompute Flow\nFigure 2 shows an example compute flow for training using an MX format. For operations involving\ndot products (e.g., matmul and convolution) in both forward and backward passes, the two inputs\n3\nare converted to MX format, and the operation is performed using the efficient dot product from\nSection 6.2 of the OCP Microscaling Specification [6]. Vector operations (e.g., layernorm, Softmax,\nGELU, and residual add) are performed in a scalar floating-point format like Bfloat16 or FP32. The\ndot product operations produce outputs in the scalar float format. A master copy of the weights is\nkept in FP32, and this copy is updated in each training step. In all the training examples in this paper,\nwe use the compute flow illustrated in Figure 2.\nOptimizer \nWg\ni\n(FP32)\n[Backward]\nEi [M,N]\n(Bfloat16)\n[Forward]\nGi [K,N]\n(Bfloat16)\nAi-1 [M,K] \n(Bfloat16)\nQuantize\nWi [K,N]\n(Bfloat16)\nMX*[M,KQ]\nVector \nOps\nMatMul\n(Ai-1* Wi)\nVector \nOps\nQuantize\nMX*[KQ,N]\nAi [M,N] \n(BFloat16)\nMatMul\n(Ai-1\nT * Ei)\nTranspose\nAi-1 [M,K] \n(Bfloat16)\nQuantize\nMX*[K,MQ]\nQuantize\nMX*[MQ,N]\nMatMul\n(Ei * Wi\nT)\nTranspose\nWi [K,N] \n(Bfloat16)\nQuantize\nMX*[M,NQ]\nQuantize\nMX*[NQ,K]\nVector \nOps\nEi-1 [M,K]\n(Bfloat16)\nVector \nOps\nFigure 2: Compute flow with MX formats (denoted as MX*). In the diagram, MatMul includes any\ndot product operation such as matmul, linear, and convolution. Vector Ops include non-dot product\noperations like activations, normalization, Softmax, and residual add.\nDue to non-commutative nature of transpose and quantization into MX formats (see Section 3), the\nquantized weights Wi and their transpose W T\ni must be stored as two separate tensors. Note that the\ntwo tensors do not need to be stored in working memory simultaneously unless a very fine-grained\ninterleaving of the forward and backward passes is employed.\n4.2\nMethodology\nWe used a custom library to emulate MX formats on existing GPUs. The library is implemented as a\ncustom CUDA extension in PyTorch and performs quantization following Figure 2. In particular, we\nexplored four settings:\n\u2022 Direct-cast Inference. The quantized inference is performed on a trained FP32 model.\nAll GeMMs in the forward pass are quantized unless explicitly called out otherwise (the\nbackward pass is not executed at all).\n\u2022 Error Diffusion Inference. The error diffusion algorithm is a Post Training Quantization\n(PTQ) algorithm derived from GPFQ [8]. It performs quantization using a small calibration\ndataset. In this experiment, all activations and weights in the forward pass are quantized\nto the same format for simplicity. This PTQ process is a quick one-pass process without a\ntraining loop or needing any tuning parameter.\n\u2022 Finetuned Inference. Quantization-aware finetuning is done on a trained FP32 model for a\nsmall number of epochs. For this fine-tuning, all GeMMs in the forward pass are quantized,\nwhile the backward pass is performed in FP32. Hyperparameter exploration is used to find\nproper finetuning hyperparameters.\n\u2022 Training. A model is trained from scratch using a compute flow where all GeMMs in both\nforward and backward passes are quantized (see Figure 2. For mixed-precision training\nwhere the weights and activations use different data formats, the gradients (Ei in Figure 2)\nare quantized to the activation format.\nOur benchmark suite contains two types of tasks: discriminative and generative.\n4\n4.3\nDiscriminative Inference\nIn this section, we examine inference results with MX formats across a variety of discriminative\ntasks including language translation, text encoding, image classification, speech recognition, and\nrecommendation models. Table 2 summarizes the results related to direct-cast inference. Results for\nfinetuned inference are reported in Table 4, and results for PTQ with error diffusion inference are\nreported in Table 3.\nIn these experiments, the same MX formats were used for both weights and activations following\nAlgorithm 1. Round-half-to-nearest-even was used for conversion to MX formats. The results\npresented in Table 2 corroborates the effectiveness of MXINT8 as a drop-in replacement for FP32\nwith minimal accuracy drop. For MXFP8 and MXFP6, the general trend is that the variant of the\nformat with more mantissa bits was better for direct-cast inference. With finetuned inference (Table 4),\nMXFP6_E2M3 is able to achieve close-to-parity with FP32.\nTable 2: Direct-cast inference with MX data formats. For each experiment, the FP32 baseline\nwas quantized (both weights and activations) with no additional tweaks. MXINT8 is a compelling\nalternative to FP32 for low-friction direct-cast inference.\nTask\nFamily\nModel\nDataset\nMetric\nBaseline\nFP32\nMXINT8\nMXFP8\nMXFP6\nMXFP4\nE4M3\nE5M2\nE2M3\nE3M2\nLanguage\nTranslation\nTransformers\n(Enc-Dec)\nTransformer-Base [9]\nWMT-17\nBLEU\nScore \u2191\n26.85\n26.64\n26.27\n25.75\n26.38\n25.97\n22.68\nTransformer-Large [9]\n27.63\n27.56\n27.44\n27.02\n27.52\n27.22\n26.33\nLSTM\nGNMT [10]\nWMT-16\n24.44\n24.52\n24.53\n24.45\n24.51\n24.44\n23.75\nLanguage\nEncoding\nTransformers\n(Enc-Only)\nBERT-Base [11]\nWikipedia\nF-1\nScore \u2191\n88.63\n88.58\n88.47\n87.04\n88.38\n88.05\n84.94\nBERT-Large [11]\n93.47\n93.41\n93.42\n93.32\n93.45\n93.27\n90.97\nImage\nClassification\nVision\nTransformer\nDeiT-Tiny [12]\nImageNet\nILSVRC12\nTop-1\nAcc. \u2191\n72.16\n72.20\n71.37\n70.11\n71.56\n70.16\n56.72\nDeiT-Small [12]\n80.54\n80.56\n79.83\n79.00\n80.11\n79.04\n71.35\nCNN\nResNet-18 [13]\n70.79\n70.80\n69.08\n66.16\n69.71\n66.10\n48.77\nResNet-50 [13]\n77.40\n77.27\n75.94\n73.78\n76.42\n73.75\n42.39\nMobileNet v2 [14]\n72.14\n71.61\n65.74\n53.50\n67.76\n53.46\n0.25\nSpeech\nRecognition\nTransformer\nWav2Vec 2.0 [15]\nLibriSpeech\nWER \u2193\n18.90\n18.83\n23.71\n21.99\n20.63\n21.98\n42.62\nRecommendations\nMLPs\nDLRM [16]\nCriteo\nTerabyte\nAUC \u2191\n0.803\n0.803\n0.802\n0.801\n0.802\n0.801\n0.7947\nTable 3: Error diffusion for PTQ with MX data formats. Both activations and pre-trained weights\nfrom the baseline model are quantized to the column\u2019s datatype.\nTask\nFamily\nModel\nDataset\nMetric\nFP32\nBaseline\nMXFP6\nMXFP4\nE2M3\nE3M2\nImage\nClassification\nVision\nTransformer\nDeiT-Tiny [12]\nImageNet\nILSVRC12\nTop-1\nAcc. \u2191\n72.16\n72.16\n71.29\n64.76\nDeiT-Small [12]\n80.54\n80.50\n80.25\n76.80\nCNN\nResNet-18 [13]\n70.79\n70.66\n70.15\n67.40\nResNet-50 [13]\n77.40\n77.15\n76.48\n69.99\nMobileNet v2 [14]\n72.14\n70.22\n65.32\n18.88\nSpeech\nRecognition\nTransformer\nWav2Vec 2.0 [15]\nLibriSpeech\nWER \u2193\n18.90\n19.09\n19.36\n24.39\n4.4\nGenerative Inference\nWe leveraged the open source LM Eval Harness by Eleuther AI for our evaluation of MX data formats\nin generative inference of OpenAI GPT3-175B and open source LLaMA-7B.2 All benchmarks were\nrun under zero-shot settings (i.e., no examples were presented to the models before evaluation). Our\nbenchmark suite includes the following subset:\nLambada \u2014 Lambada is a long range prediction task, where the model must predict the last word in\na long narrative passage. We used the version of lambada data used to evaluate GPT2 in LM Harness.\nWikitext \u2014 The wikitext task is based on the wikitext-2 dataset and requires the model to predict\nlong sequences based on high quality Wikipedia articles. GPT3-175B was not evaluated on this task\nas Wikipedia data was part of its training corpus [17].\n2https://github.com/EleutherAI/lm-evaluation-harness/tree/1736d78dd9615107e68ec7f74043b02d4ab68d12.\n5\nTable 4: Finetuned inference with MX data formats. Finetuning is performed for a few epochs starting\nfrom the FP32 model. Cells containing N/A means no finetuning was needed due to good direct-cast\nresults. MXFP6_E2M3 achieves close-to-parity with FP32 after finetuning.\nTask\nFamily\nModel\nDataset\nMetric\nFP32\nBaseline\nMXFP6\nMXFP4\nE2M3\nE3M2\nLanguage\nTranslation\nTransformers\n(Enc-Dec)\nTransformer-Base [9]\nWMT-17\nBLEU\nScore \u2191\n26.85\n26.98\n27.01\n25.97\nTransformer-Large [9]\n27.63\n27.60\n27.62\n27.33\nLSTM\nGNMT [10]\nWMT-16\n24.44\nN/A\nN/A\n24.56\nImage\nClassification\nVision\nTransformer\nDeiT-Tiny [12]\nImageNet\nILSVRC12\nTop-1\nAcc. \u2191\n72.16\n72.09\n70.86\n66.41\nDeiT-Small [12]\n80.54\n80.43\n79.76\n77.61\nCNN\nResNet-18 [13]\n70.79\n70.6\n69.85\n67.19\nResNet-50 [13]\n77.40\n77.27\n76.54\n74.86\nMobileNet v2 [14]\n72.14\n71.49\n70.27\n65.41\nSpeech\nRecognition\nTransformer\nWav2Vec 2.0 [15]\nLibriSpeech\nWER \u2193\n18.90\nN/A\n21.46\n29.64\nARC dataset \u2014 The Arc tasks are both multiple choice tasks consisting of nearly 8000 science exam\nquestions, with the dataset split into easy and more challenging questions. The model is tasked with\npicking the correct answer from several options.\nHendryck\u2019s Test \u2014 Hendryck\u2019s test suite is a set of tasks that measure how knowledgeable a model\nis in 57 different fields. We used computer science, international law, and jurisprudence as a\nsubset for this study. These tasks are all multiple choice questions, where the model must pick the\ncorrect answer from the options presented.\nTable 5 and Table 6 show results for direct-cast inference on OpenAI GPT3-175B [17] and open source\nLLaMA-7B, respectively. Due to the size of these models, no quantization-aware finetuning was\nperformed. The columns with a single MX format use that format for both weights and activations; the\nother columns list separate formats for weights (Wt) and activations (Act) and utilize mixed-precision.\nMXINT8 matched baseline FP32 to within the standard deviation on all tasks for both GPT3-175B\nand LLaMA-7B. MXINT8 once again proves to be a compelling alternative to FP32 for low-friction\ndirect-cast inference.\nTable 5: GPT3-175B direct-cast inference results. Higher is better for all tasks. Each number is given\n\u00b1 the bootstrap estimated standard deviation. We only experiment with the higher mantissa width\nvariant of each format (i.e., MXFP8_e4m3 and MXFP6_e2m3) given that the results in Section 5.2\nshow these variants works better for direct-cast inference.\nTasks\nFP32\nMXINT8\nMXFP8\nMXFP6\nMXFP6 Wt\nMXFP8 Act\nMXFP4 Wt\nMXFP8 Act\nMXFP4 Wt\nMXFP6 Act\nMXFP4\nARC easy \u2191\n0.744 \u00b1 0.009\n0.740 \u00b1 0.009\n0.738 \u00b1 0.009\n0.737 \u00b1 0.009\n0.740 \u00b1 0.009\n0.749 \u00b1 0.009\n0.744 \u00b1 0.009\n0.748 \u00b1 0.010\nARC challenge \u2191\n0.480 \u00b1 0.015\n0.481 \u00b1 0.015\n0.485 \u00b1 0.015\n0.480 \u00b1 0.015\n0.478 \u00b1 0.015\n0.486 \u00b1 0.015\n0.487 \u00b1 0.015\n0.425 \u00b1 0.014\nLambada \u2191\n0.755 \u00b1 0.006\n0.754 \u00b1 0.006\n0.708 \u00b1 0.006\n0.745 \u00b1 0.006\n0.725 \u00b1 0.006\n0.728 \u00b1 0.007\n0.754 \u00b1 0.006\n0.623 \u00b1 0.007\nCollege CS \u2191\n0.360 \u00b1 0.049\n0.340 \u00b1 0.048\n0.350 \u00b1 0.048\n0.350 \u00b1 0.048\n0.340 \u00b1 0.048\n0.340 \u00b1 0.046\n0.320 \u00b1 0.047\n0.240 \u00b1 0.043\nInt. law \u2191\n0.504 \u00b1 0.046\n0.537 \u00b1 0.046\n0.455 \u00b1 0.046\n0.521 \u00b1 0.046\n0.463 \u00b1 0.046\n0.331 \u00b1 0.043\n0.347 \u00b1 0.043\n0.298 \u00b1 0.045\nJurisprudence \u2191\n0.454 \u00b1 0.049\n0.435 \u00b1 0.048\n0.491 \u00b1 0.048\n0.454 \u00b1 0.048\n0.472 \u00b1 0.049\n0.463 \u00b1 0.048\n0.418 \u00b1 0.048\n0.324 \u00b1 0.045\nTable 6: LLaMA-7B direct-cast inference results. Higher is better for all tasks except wikitext. For\nthis benchmark only, the Softmax function was not quantized to Bfloat16.\nTasks\nFP32\nMXINT8\nMXFP8\nMXFP6\nMXFP6 Wt\nMXFP8 Act\nMXFP4 Wt\nMXFP8 Act\nMXFP4 Wt\nMXFP6 Act\nMXFP4\nARC easy \u2191\n0.729 \u00b1 0.009\n0.725 \u00b1 0.009\n0.716 \u00b1 0.009\n0.718 \u00b1 0.009\n0.726 \u00b1 0.009\n0.697 \u00b1 0.010\n0.696 \u00b1 0.010\n0.637 \u00b1 0.010\nARC challenge \u2191\n0.447 \u00b1 0.015\n0.444 \u00b1 0.015\n0.430 \u00b1 0.015\n0.445 \u00b1 0.015\n0.442 \u00b1 0.015\n0.412 \u00b1 0.014\n0.406 \u00b1 0.014\n0.355 \u00b1 0.014\nLambada \u2191\n0.736 \u00b1 0.006\n0.731 \u00b1 0.006\n0.720 \u00b1 0.006\n0.724 \u00b1 0.006\n0.721 \u00b1 0.006\n0.675 \u00b1 0.006\n0.678 \u00b1 0.007\n0.557 \u00b1 0.007\nCollege CS \u2191\n0.260 \u00b1 0.044\n0.220 \u00b1 0.045\n0.270 \u00b1 0.042\n0.240 \u00b1 0.043\n0.280 \u00b1 0.045\n0.240 \u00b1 0.043\n0.210 \u00b1 0.041\n0.220 \u00b1 0.042\nInt. law \u2191\n0.463 \u00b1 0.046\n0.430 \u00b1 0.045\n0.413 \u00b1 0.045\n0.422 \u00b1 0.045\n0.413 \u00b1 0.045\n0.398 \u00b1 0.045\n0.405 \u00b1 0.045\n0.331 \u00b1 0.041\nJurisprudence \u2191\n0.361 \u00b1 0.046\n0.370 \u00b1 0.047\n0.380 \u00b1 0.047\n0.370 \u00b1 0.046\n0.352 \u00b1 0.047\n0.269 \u00b1 0.045\n0.296 \u00b1 0.044\n0.269 \u00b1 0.043\nwikitext \u2193\n9.488\n9.504\n9.768\n9.628\n9.683\n11.476\n11.147\n27.201\n6\n4.5\nGenerative Training\nTable 7 and Figure 3 show the language model loss obtained from training GPT-like models\nof various size (20M-1.5B) using MXFP6_e3m2 for both the forward and backward passes (see\nFigure 2). The training is done using the ADAM optimizer, with hyperparameters tuned for FP32.\nThe same hyperparameters were reused for the MX format runs with no changes. All the models\nare trained to efficiency with number of steps calculated based on the scaling power-laws [18].\nRound-half-away-from-zero rounding was used for conversion to MX formats.\nThe results in Table 7 and Figure 3 show that MXFP6_e3m2 is capable of delivering a model quality\nmatching that of FP32 at much lower circuitry footprint. MXFP6 provides the first demonstration\nof training generative language models to parity with FP32 using 6-bit weights, activations, and\ngradients with no modification to the training recipe.\nPushing the limits even further, Table 8 and Figure 4 show the results from training the same GPT-\nlike models, this time under a mixed-precision setting with MXFP4 weights and MXFP6_e3m2\nactivations. The gradients used the same data format as the activations. The training hyperparameters\nwere the same as before. Our results demonstrate that generative language models can be trained\nwith MXFP4 weights and MXFP6 activations and gradients incurring only a minor penalty in\nthe model loss. This is once again with no modifications to the training recipe.\nModel\nFP32\nMXFP6\nE2M3\nE3M2\nGPT-20M\n3.98\n4.02\n4.01\nGPT-150M\n3.30\n3.33\n3.32\nGPT-300M\n3.11\n3.13\n3.12\nGPT-1.5B\n2.74\n2.75\n2.75\nTable 7: Language model loss\nfor training from scratch using\nMXFP6_E3M2 for weights,\nactivations, and gradients.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Training Steps\n0\n2\n4\n6\n8\n10\nLM Loss\nGPT-20M\nGPT-150M\nGPT-345M\nGPT-1.5B\nFP32\nMXFP6 (E2M3)\nMXFP6 (E3M2)\nFigure 3: GPT training loss curve, using MXFP6_E3M2 for weights,\nactivations, and gradients.\nModel\nFP32\nMXFP4 Wt\nMXFP6 Act\nGPT-20M\n3.98\n4.04\nGPT-150M\n3.30\n3.33\nGPT-300M\n3.11\n3.14\nGPT-1.5B\n2.74\n2.76\nTable 8: Language model loss\nfor training from scratch us-\ning MXFP4 for weights and\nMXFP6_E3M2 for activations\nand gradients.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Training Steps\n0\n2\n4\n6\n8\n10\nLM Loss\nGPT-20M\nGPT-150M\nGPT-345M\nGPT-1.5B\nFP32\nMXFP4 Wt- MXFP6 Act\nFigure 4: GPT mixed-precision training loss curve, using MXFP4 for\nweights and MXFP6_E3M2 for activations and gradients.\n7\n5\nConclusion\nThis paper evaluates MX data formats that integrate a block-level scale on top of narrow bit-width\nelements. The evaluated concrete MX formats provide compelling alternatives to FP32 training and\ninference with minimal user friction. Experimental results show the effectiveness of MX formats for\na variety of deep learning models including generative language models, image classification, speech\nrecognition, recommendation models, and translation.\nIn particular, MXINT8 is a compelling drop-in replacement to FP32 for low-friction direct-cast\ninference. MXFP6 closely matches FP32 for inference after quantization-aware finetuning. MXFP6\nalso, for the first time, enables generative language model training at sub-8-bit weights, activations,\nand gradients without sacrificing model accuracy or needing changes to the training recipe. Reducing\nthe bit-width even further, we showcase training with MXFP4 weights and MXFP6 activations and\ngradients, incurring only a minor loss penalty for generative language models.\nAcknowledgment\nThe authors would like to thank the following individuals for their invaluable support and contribu-\ntions: Ian Bratt, Nigel Stephens, Jelena Milanovic, John Brothers, Yuan Yu, Rani Borkar, Saurabh\nDighe, Brian Harry, Matt Perry, Renee L\u2019Heureux, Dimitry Melts, Jasmine Klar, and Steve Scott.\nReferences\n[1] Paulius Micikevicius, Stuart Oberman, Pradeep Dubey, Marius Cornea, Andres Rodriguez, Ian Bratt,\nRichard Grisenthwaite, Norm Jouppi, Chiachen Chou, Amber Huffman, Michael Schulte, Ralph Wittig,\nDharmesh Jani, and Summer Deng. OCP 8-bit Floating Point Specification (OFP8). Open Compute\nProject, 2023.\n[2] Mario Drumond, Tao Lin, Martin Jaggi, and Babak Falsafi. Training DNNs with Hybrid Block Floating\nPoint. Advances in Neural Information Processing Systems (NeurIPS), 31, 2018.\n[3] Bita Darvish Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna\nVinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, Alessandro Forin, Haishan Zhu, Taesik Na,\nPrerak Patel, Shuai Che, Lok Chand Koppaka, XIA SONG, Subhojit Som, Kaustav Das, Saurabh T,\nSteve Reinhardt, Sitaram Lanka, Eric Chung, and Doug Burger. Pushing the Limits of Narrow Precision\nInferencing at Cloud Scale with Microsoft Floating Point. Advances in Neural Information Processing\nSystems (NeurIPS), 33:10271\u201310281, 2020.\n[4] Steve Dai, Rangha Venkatesan, Mark Ren, Brian Zimmer, William Dally, and Brucek Khailany. VS-Quant:\nPer-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference. Machine Learning\nand Systems (MLSys, 3:873\u2013884, 2021.\n[5] Bita Darvish Rouhani, Ritchie Zhao, Venmugil Elango, Rasoul Shafipour, Mathew Hall, Maral Mes-\nmakhosroshahi, Ankit More, Levi Melnick, Maximilian Golub, Girish Varatkar, Lei Shao, Gaurav Kolhe,\nDimitry Melts, Jasmine Klar, Renee L\u2019Heureux, Matt Perry, Doug Burger, and Eric Chung. With Shared\nMicroexponents, A Little Shifting Goes a Long Way. Int\u2019l Symp. on Computer Architecture (ISCA), pages\n1\u201313, 2023.\n[6] Bita Darvish Rouhani, Nitin Garegrat, Tom Savell, Ankit More, Kyung-Nam Han, Mathew Zhao, Ritchie\namd Hall, Jasmine Klar, Eric Chung, Yuan Yu, Michael Schulte, Ralph Wittig, Ian Bratt, Nigel Stephens,\nJelena Milanovic, John Brothers, Pradeep Dubey, Marius Cornea, Alexander Heinecke, Andres Rodriguez,\nMartin Langhammer, Summer Deng, Maxim Naumov, Paulius Micikevicius, Michael Siu, and Colin\nVerrilli. OCP Microscaling (MX) Specification. Open Compute Project, 2023.\n[7] Microscaling PyTorch Library. 2023. URL https://github.com/microsoft/microxcaling.\n[8] Jinjie Zhang, Yixuan Zhou, and Rayan Saab. Post-training quantization for neural networks with provable\nguarantees. arXiv:2201.11113, 2022.\n[9] Transformer For PyTorch.\nURL https://github.com/NVIDIA/DeepLearningExamples/tree/\nmaster/PyTorch/Translation/Transformer.\n[10] GNMT v2 For PyTorch.\nURL https://github.com/NVIDIA/DeepLearningExamples/tree/\nmaster/PyTorch/Translation/GNMT.\n8\n[11] NVIDIA/Megatron-LM: Ongoing research training transformer. URL https://github.com/NVIDIA/\nMegatron-LM.\n[12] Data-Efficient architectures and training for Image classification.\nURL https://github.com/\nfacebookresearch/deit.\n[13] Convolutional Network for Image Classification in PyTorch. URL https://github.com/NVIDIA/\nDeepLearningExamples/tree/master/PyTorch/Classification/ConvNets.\n[14] Torchvision MobileNetV2. URL https://github.com/pytorch/vision.\n[15] wav2vec 2.0.\nURL https://github.com/facebookresearch/fairseq/tree/main/examples/\nwav2vec.\n[16] Deep Learning Recommendation Model for Personalization and Recommendation Systems. URL https:\n//github.com/facebookresearch/dlrm.\n[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models\nare Few-Shot Learners. Advances in Neural Information Processing Systems (NeurIPS), 33:1877\u20131901,\n2020.\n[18] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling Laws for Neural Language Models. arXiv preprint\narXiv:2001.08361, 2020.\n9\n"
  },
  {
    "title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
    "link": "https://arxiv.org/pdf/2310.10047.pdf",
    "upvote": "4",
    "text": "IMPROVING LARGE LANGUAGE MODEL FINE-TUNING\nFOR SOLVING MATH PROBLEMS\nYixin Liu\u22171, Avi Singh2, C. Daniel Freeman2, John D. Co-Reyes2, Peter J. Liu2\n1Yale University, 2Google DeepMind\nyixin.liu@yale.edu, peterjliu@google.com\nABSTRACT\nDespite their success in many natural language tasks, solving math problems re-\nmains a significant challenge for large language models (LLMs). A large gap\nexists between LLMs\u2019 pass-at-one and pass-at-N performance in solving math\nproblems, suggesting LLMs might be close to finding correct solutions, motivat-\ning our exploration of fine-tuning methods to unlock LLMs\u2019 performance. Using\nthe challenging MATH dataset, we investigate three fine-tuning strategies: (1) so-\nlution fine-tuning, where we fine-tune to generate a detailed solution for a given\nmath problem; (2) solution-cluster re-ranking, where the LLM is fine-tuned as\na solution verifier/evaluator to choose among generated candidate solution clus-\nters; (3) multi-task sequential fine-tuning, which integrates both solution gener-\nation and evaluation tasks together efficiently to enhance the LLM performance.\nWith these methods, we present a thorough empirical study on a series of PaLM 2\nmodels and find: (1) The quality and style of the step-by-step solutions used for\nfine-tuning can make a significant impact on the model performance; (2) While so-\nlution re-ranking and majority voting are both effective for improving the model\nperformance when used separately, they can also be used together for an even\ngreater performance boost; (3) Multi-task fine-tuning that sequentially separates\nthe solution generation and evaluation tasks can offer improved performance com-\npared with the solution fine-tuning baseline. Guided by these insights, we design a\nfine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset\nwith fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the few-\nshot performance of pre-trained PaLM 2-L model with majority voting.\n1\nINTRODUCTION\nSolving mathematical problems is a challenging task for even the state-of-the-art large language\nmodels (LLMs), e.g., GPT-4 (OpenAI, 2023) and PaLM 2 (Anil et al., 2023), since it requires the\nability of creative thinking, mathematical reasoning and numerical calculation. However, LLMs are\nalready showing potential of achieving better performance on this math problem solving task, as the\nlikelihood of LLM\u2019s being able to find a correct answer is significantly higher when they are allowed\nto attempt the problem several times. For example, with greedy decoding, the pre-trained PaLM 2-\nL can achieve around 33.4% accuracy. However, when sampling 64 solutions using temperature\nsampling, there is at least one correct solution (pass@64) 79.4% of the time (Table 2). This large\nperformance gap suggests that LLMs can be capable of generating correct solutions while struggling\nto discriminate correct from incorrect solutions.\nTherefore, we study task-specific fine-tuning methods that can improve the LLM\u2019s solution genera-\ntion and evaluation ability such that the aforementioned performance gap can be reduced. Specifi-\ncally, we explore three fine-tuning methods:\n(1) Supervised step-by-step solution fine-tuning (SSFT). As a baseline method, we investigate\nwhether the pre-trained LLMs can benefit from a supervised fine-tuning stage. To this end, we fine-\ntune the LLMs to generate the step-by-step solution and final answer as in Lightman et al. (2023).\n\u2217Work done while interning at Google DeepMind.\n1\narXiv:2310.10047v1  [cs.CL]  16 Oct 2023\n(2) Solution-cluster Re-ranking (SCR). To enhance the LLM\u2019s solution evaluation ability, we con-\ntinue fine-tuning the generator as a solution evaluator for candidate solution re-ranking. While such\na solution sample-rank, or re-ranking, has been investigated in previous work (Cobbe et al., 2021),\nwe propose a new technique that can bring the benefits of both majority voting (Wang et al., 2023)\nand re-ranking together, while reducing ranking costs. Specifically, we first group the candidate an-\nswers into different clusters according to their mathematical equivalence, which is an intermediate\nstep in majority voting. Then, we apply the solution evaluator to the solutions in the most-frequent\nclusters to gain further improvement over the majority voting results.\n(3) Multi-task Sequential Fine-tuning. Apart from the solution evaluation task, we are also inter-\nested in improving the LLM\u2019s performance on the solution generation task and exploring whether\nthe training objective of solution evaluation can be beneficial to the solution generation model. To\nthis end, we propose a sequential multi-task learning setting for the generation model where the\nsolution evaluation task is formatted in the form of a natural language generation task, such that\nits training objective can provide meaningful supervision signal to the solution generation model.\nConcretely, we fine-tune the model in a sequential manner: fine-tuning (1) as a solution generator\n(SSFT), (2) as a solution evaluator (SCR), (3) as a generator (SSFT) again.\nWe conduct comprehensive experiments on the challenging MATH (Hendrycks et al., 2021b) dataset\nwith PaLM 2-S* and PaLM 2-L\u2013 the small and large variants of PaLM 2 respectively (Anil et al.,\n2023) \u2013 which leads to these findings:\n\u2022 For SSFT, the quality and the style of the step-by-step solutions can make a large impact\non fine-tuned model, as they benefit more from fine-grained, well-formatted solutions.\n\u2022 Re-ranking solutions in the most-frequent solution clusters can yield better performance\nthan re-ranking all the solutions while simultaneously achieving better computational effi-\nciency, which we believe can be a better standard practice for future work.\n\u2022 Our proposed multi-task sequential fine-tuning can more effectively improve the solution\ngeneration model performance compared with supervised solution fine-tuning only, show-\ning the benefit of training the model for both solution generation and evaluation tasks,\npresenting a successful attempt of leveraging learning signal of a binary evaluation task for\na generation model.\n2\nBACKGROUND\nMathematical problem solving is an important task (Hendrycks et al., 2021a;b; Cobbe et al., 2021)\nfor measuring the LLMs\u2019 reasoning and numerical computation abilities. In this work, we focus\non the MATH dataset (Hendrycks et al., 2021b), which consists of problems collected from high\nschool math competitions, along with the human-written solutions containing both natural language\nexplanations and the final ground-truth solutions. MATH dataset is challenging to even the recent\nstart-of-the-art large language models (LLMs), such as GPT-4 (OpenAI, 2023) and PaLM 2 (Anil\net al., 2023), since they can only achieve 42.5% and 33.2% pass-at-1 accuracy (Anil et al., 2023).\nThe accuracy is usually calculated through an automatic grading function g checking the mathemat-\nical equivalence between the ground truth solution A and the model solution \u02dcA:\ng(A, \u02dcA) =\n\u001a1\nif \u02dcA is equivalent to A,\n0\notherwise.\n(1)\nRecent work has proposed various methods to improve LLM performance on the math problem\nsolving task. In particular, majority voting, or self-consistency, can yield a significant improvement\ncompared with the baseline performance of LLMs (Lewkowycz et al., 2022; Wang et al., 2023).\nThroughout this work, we will use the model\u2019s pass-at-1, pass-at-N, and majority voting perfor-\nmance for model evaluation and comparison. The specific definitions are:\n(1) Pass@1 (pass-at-1): the accuracy of the model\u2019s greedy-decoded solution AG, i.e., g(A, AG).\n(2) Pass@N (pass-at-N): the oracle performance that always selects the correct solution when it is\npresented in N temperature sampled solutions, { \u02dcA1, \u02dcA2, ..., \u02dcAN}, i.e, maxi\u2208{1,2,...,N} g(A, \u02dcAi).\n2\n(3) Maj1@N (majority-voting-at-N): N sampled solutions are first clustered by their math equiva-\nlence, i.e., g( \u02dcAi, \u02dcAj). Then, one solution \u02dcA\u2217 from a most frequent cluster is selected for calculating\nthe accuracy, g(A, \u02dcA\u2217).\n(4) MajK@N: Similar to Pass@N, we define an oracle that always selects the correct solution\nwhen it is presented in the top-K majority-voting clusters, { \u02dcA\u2217\n1, \u02dcA\u2217\n2, ..., \u02dcA\u2217\nK}, so its accuracy is\nmaxi\u2208{1,2,...,K} g(A, \u02dcA\u2217\ni ).\nAnother line of work leverages external tools such as Python programs to enhance the LLMs\u2019 abil-\nity (Chen et al., 2022; Wu et al., 2023; Yue et al., 2023; Zhou et al., 2023). In this work, we focus\non improving the LLMs\u2019 inherent ability to solve math problems without help from external tools.\n3\nMETHODS\n3.1\nSUPERVISED SOLUTION FINE-TUNING\nIn Hendrycks et al. (2021b); Cobbe et al. (2021) models are fine-tuned to generate not only the final\nanswer but also the step-by-step process for solving the math problem.\nS, A \u2190 M(P),\n(2)\nwhere P is the math problem, S, A are the ground-truth step-by-step solution and the final answer\nrespectively, and M is an LLM. In training, the solution S and the final answer A are concatenated\ninto a single text sequence X, and the model is fine-tuned with the cross-entropy loss following the\nmaximum likelihood estimation (MLE) paradigm:\nLmle = \u2212 log pM(X|P),\n(3)\nwhere pM is the probably distribution given by the auto-regressive language model M:\npM(X|P) =\nY\ni\npM(xi|X0,....,i\u22121, P).\n(4)\nHere, xi is the i-th token in X, X0,....,i\u22121 is the prefix before xi.\nTo collect the ground-truth step-by-step solutions, we use two sources: (1) the original human-\nwritten solutions in the MATH dataset, (2) GPT-4 generated solutions provided in Lightman et al.\n(2023) with the chain-of-thought prompting eliciting step-by-step solutions. Our preliminary anal-\nysis found that the original solutions in the MATH dataset are more abstract while the solutions\ngenerated by GPT-4 are more fine-grained and detailed.\n3.2\nSOLUTION-CLUSTER RE-RANKING\nWe note that there are two significant gaps for LLMs\u2019 math problem solving performance in Table 2:\n(1) the gap between the model\u2019s greedy-decoding (Pass@1) and majority-voting (Maj1@N) results;\n(2) the gap between the model\u2019s majority-voting best-at-1 (Maj1@N) and best-at-K performance\n(MajK@N). To narrow these gaps, we fine-tune the pre-trained LLM as a solution verifier/evaluator,\nfollowing Cobbe et al. (2021). However, unlike in the previous work where a large number (e.g.,\n1000) of candidate solutions are all reranked by the evaluator, we combine the strength of majority\nvoting and re-ranking together by only re-ranking the top-K solution clusters. We believe this re-\nranking strategy is both more robust and cost-efficient, as will be elaborated in the following section.\nTo use the evaluator to score each candidate solution, we formulate the scoring task as a\nclassification problem in a text completion format, inspired by related work on using LLMs\nfor text evaluation (Liu et al., 2023; Fu et al., 2023).\nConcretely, we define a map-\nping function T converting the math problem P and a candidate solution \u02dcX into a prompt\nT(P, \u02dcX): \u201cHere is a math problem:\nP.\nHere is a candidate solution:\n\u02dcX.\nThe above candidate solution is \u201d.\nWe then interpret the model-predicted\nprobability of the word \u201ccorrect\u201d (or \u201cincorrect\u201d) being the next token1 as the probability of the\nsolution being correct (or incorrect):\npcls(\u201ccorrect\u201d| \u02dcX, P) = pM(\u201ccorrect\u201d|T(P, \u02dcX)),\n(5)\n1We note that the tokenizer we used tokenizes the words \u201ccorrect\u201d and \u201cincorrect\u201d both into a single token.\n3\npcls(\u201cincorrect\u201d| \u02dcX, P) = pM(\u201cincorrect\u201d|T(P, \u02dcX)).\n(6)\nWe can then define the following normalized probability as the candidate solution score:\nScls( \u02dcX|P) =\npcls(\u201ccorrect\u201d| \u02dcX, P)\npcls(\u201ccorrect\u201d| \u02dcX, P) + pcls(\u201cincorrect\u201d| \u02dcX, P)\n.\n(7)\nWith this scoring format, we investigate two training objectives:\n(1) Margin loss for pairwise comparison:\nLcls-margin = max(0, log Scls( \u02dcXincorrect|P) \u2212 log Scls( \u02dcXcorrect|P) + \u03bb),\n(8)\nwhere \u02dcXcorrect and \u02dcXincorrect stand for a correct and an incorrect solution respectively, and \u03bb is a\nhyper-parameter for the margin.\n(2) Cross-entropy loss for classification. The scoring format we designed is equivalent to a multi-\nclass classification problem where \u201ccorrect\u201d and \u201cincorrect\u201d are the only valid options. Therefore,\nwe can fine-tune the model using the cross-entropy loss for this classification task:\nLcls-xent = \u22121{ \u02dc\nX is correct}( \u02dcX) log pcls(\u201ccorrect\u201d| \u02dcX, P)\n+ 1{ \u02dc\nX is incorrect}( \u02dcX) log pcls(\u201cincorrect\u201d| \u02dcX, P)\n(9)\n3.3\nMULTI-TASK SEQUENTIAL FINE-TUNING\nThe MLE-based training objective defined in Eq. 3 is somewhat at odds with the ultimate binary\nevaluation target \u2013 whether the final answer is correct or not. Related work has explored better\naligning training with the task evaluation using a contrastive learning objective (Edunov et al., 2018;\nLiu et al., 2022; Zhao et al., 2023), which interprets the model-predicted probability of a candidate\nsolution \u02dcX as its quality score and uses the margin loss to encourage the model to assign higher\nprobabilities to better candidates:\nLseq = max(0, log pM( \u02dcXincorrect|P) \u2212 log pM( \u02dcXcorrect|P) + \u03bb).\n(10)\nThen, the model is fine-tuned with the MLE and contrastive training objectives jointly:\nLctr = Lseq + \u03b11Lmle,\n(11)\nwhere \u03b11 is a hyper-parameter.\nHowever, the contrastive learning objective may not be as suitable for the math problem solving\ntask because of the binary nature of the task \u2013 the objective requires the model to use the token\nlikelihoods for two purposes: (1) predicting the next token, and (2) evaluating the quality of the\nentire text sequence. It can be a reasonable objective for natural language generation tasks such\nas text summarization where the next token prediction correctness is closely related to overall text\nquality. However, for math problem solving, the correctness of a solution might be decided by just\na few tokens, making the next-token prediction task more distant and incompatible with the solution\nevaluation task. Consequently, we combine the training objectives in \u00a73.1 (Eq. 3) and \u00a73.2 (Eq. 8\nand Eq. 9), introducing a new learning setting that formulates both math problem solution generation\nand evaluation tasks as natural language generation tasks:\nLmul-margin = Lcls-margin + \u03b12Lmle,\n(12)\nLmul-xent = Lcls-xent + \u03b13Lmle,\n(13)\nwhere \u03b12 and \u03b13 are hyper-parameters. We believe we can better leverage the capacity of LLMs\nwith this training setting since it is closer to the pre-training task (i.e., next-token prediction). In\nour preliminary experiments, we found that it is difficult to balance the two loss terms in Eq. 12\nand Eq. 13 and the models start to overfit the MLE training objective very soon, possibly because\nof the limited size of the dataset. As the result, we optimize the multi-task objective in a sequential\nmanner \u2013 instead of fine-tuning the model on both training objectives, we first fine-tuned the model\nas a generator (Eq. 8 or Eq. 9), then as an evaluator (Eq. 3), then finally as a generator again.\n4\nTable 1: Number of examples in dataset splits and the average length in tokens of the math problems\nand solutions.\nData Source\n# Training\n# Validation\n# Test\nProblem Length\nSolution Length\nMATH\n11000\n1000\n500\n90.2\n249.6\nPRM800K\n6473\n564\n512\n57.3\n305.2\nTable 2: Results of supervised solution fine-tuning. Different training data sources are compared,\nwhich are the MATH dataset and the PRM800K dataset.\nPaLM 2-S*\nPaLM 2-L\nPass@1\nMaj1@64\nPass@64\nPass@1\nMaj1@64\nPass@64\nFew-shot\n17.4%\n27.2%\n67.8%\n33.4%\n47.6%\n79.4%\nMATH\n19.8%\n32.4%\n74.8%\n32.8%\n49.2%\n82.2%\nPRM800K\n20.8%\n41.2%\n73.8%\n34.8%\n54.2%\n83.4%\nMATH + PRM800K\n22.6%\n38.8%\n72.6%\n35.6%\n55.2%\n82.8%\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETTING\nDatasets\nOur experiments are conducted on the MATH dataset. To prevent overfitting, we follow\nLightman et al. (2023) by using the data splits they provided,2 where 4.5K original test examples are\nused for training and validation, and the remaining 500 test examples are used for model evaluation.\nWe leverage two sources of correct step-by-step solutions for the model training: (1) the original\nhuman-written explanations provided in the MATH dataset; (2) the model-generated correct solu-\ntions provided in PRM800K (Lightman et al., 2023), which only covers a subset problems in the\noriginal MATH dataset. The dataset statistics are provided in Table 1.\nEvaluation\nWe report the average solution accuracy (or correctness) for all the experiments. The\ncorrectness of the generated solution is compared against the ground-truth solution using the au-\ntomatic grading script provided by Lightman et al. (2023). This script checks the mathematical\nequivalence instead of the simple textual equivalence. Two solution generation methods are mainly\nused to evaluate the model performance: (1) greedy decoding for the Pass@1 performance, (2) nu-\ncleus sampling (Holtzman et al., 2020) for the majority voting performance (Maj1@N), where we\nused the same sampling hyper-parameters as in Lewkowycz et al. (2022). Specifically, the sampling\ntemperature is set to 0.6, and the top-p value is set to 0.95.\n4.2\nEXPERIMENT I: SUPERVISED SOLUTION FINE-TUNING\nWe fine-tune PaLM 2-S* and PaLM 2-L on the step-by-step solutions with the MLE training ob-\njective (Eq. 3). Three specific fine-tuning strategies are explored: (1) fine-tuning using the original\nMATH solutions only; (2) fine-tuning using the PRM800K GPT-4 solutions only; (3) fine-tuning\non both MATH and PRM800K solutions. We used the model performance on the validation set\nfor checkpoint selection, and all the fine-tuned models achieved the best performance within two\nepochs. The results are shown in Table 2, where the few-shot performance with the pre-trained\nPaLM 2 models are provided for comparison. The few-shot results are obtained using a customized\n4-shot prompt designed in Lewkowycz et al. (2022).\nWe observe that the fine-tuning is generally helpful for the model to achieve better performance\ncompared with the few-shot performance of the pre-trained checkpoints. Moreover, the quality and\nthe style of the solutions can have a large impact on the model performance, since the models fine-\ntuned on PRM800K solutions achieve significantly better performance than the ones fine-tuned on\nthe original MATH solutions.\n2https://github.com/openai/prm800k\n5\nTable 3: Results of solution-cluster re-ranking. Two loss functions are compared, i.e., Lcls-margin\n(Eq. 8) and Lcls-xent (Eq. 9). Two re-ranking strategies are used: (1) re-ranking all the candidate so-\nlutions (RR.All), (2) re-ranking all solutions in the top-8 solution clusters (RR.Top-8). The baseline\nperformance (Pass@1 and Maj1@64) are reported for comparison.\nModel\nLoss Function\nPass@1\nMaj1@64\nRR.All\nRR.Top-8\nPaLM 2-S*\nLcls-margin\n22.6%\n38.8%\n32.4%\n36.6%\nLcls-xent\n22.6%\n38.8%\n33.6%\n35.4%\nPaLM 2-L\nLcls-margin\n35.6%\n55.2%\n57.0%\n58.8%\nLcls-xent\n35.6%\n55.2%\n56.8%\n58.8%\nTable 4: Results of multi-task sequential fine-tuning. The model performance of the generator fine-\ntuned with the multi-task sequential setting is compared with the baseline generator trained with\nthe MLE training objective only. Two model variants with different solution evaluation training\nobjectives (Lcls-margin and Lcls-xent) are compared. All the model checkpoints are PaLM 2-L based.\nBaseline\nEvaluator Loss Function\nPass@1\nMaj1@64\nPass@64\nYes\n-\n35.6%\n55.2%\n82.8%\nNo\nLcls-margin\n37.6%\n57.2%\n82.6%\nNo\nLcls-xent\n36.2%\n56.6%\n82.2%\n4.3\nEXPERIMENT II: SOLUTION-CLUSTER RE-RANKING\nIn Table 2, we observe that there is a large performance gap between the model\u2019s Pass@1 and\nPass@64 performance, indicating that the model already has the ability to search for correct solu-\ntions, but fails to differentiate its different search results. Therefore, we continue fine-tuning the\nmodels as solution evaluators for the solution re-ranking task.\nWe investigate two loss functions from \u00a73.2, the margin loss (Eq. 8) and the cross-entropy loss\n(Eq. 9) for the model training. For Eq. 8 we found that the model performance is not sensitive to the\nmargin hyper-parameter \u03bb, so we keep it fixed as log 2 which intuitively requires the model to assign\nat least twice the probability to the correct solution. The checkpoints from \u00a73.2 that are trained on\nboth MATH and PRM800K solutions are used for this experiment, and the 64 candidate solutions are\nsampled from these checkpoints themselves for each problem. They are used to construct 10 training\nexamples for the margin loss, and all of them are used for the cross-entropy loss. We observe all\nof the experiments converged within one epoch, possibly because of the redundancy in the training\ndata. We make the following observations based on the results in Table 3:\n(1) For both PaLM 2-S* and PaLM 2-L, the re-ranking result can outperform the Pass@1 perfor-\nmance of the baseline model. However, only the PaLM 2-L evaluator can outperform the robust\nmajority-voting baseline, showing the re-ranking is a difficult task for relatively smaller models.\n(2) Re-ranking only the solutions in the top solution clusters are consistently better than re-ranking\nall the solutions for both PaLM 2-S and PaLM 2-L. This re-ranking strategy is also more computa-\ntionally efficient since there are fewer solutions to rank.\nWe provide further analysis in \u00a75 and Appendix B.\n4.4\nEXPERIMENT III: MULTI-TASK SEQUENTIAL FINE-TUNING\nHaving shown that better performance can be gained by fine-tuning the LLMs as a solution evaluator,\nwe now investigate whether the training objective for solution evaluation is also helpful for the\nmodels to become better solution generators. To this end, we aim to use the multi-task sequential\ntraining objectives (Eq. 12 and Eq. 13) we proposed in \u00a73.3 for the model fine-tuning: (1) the first\nstep is exactly the experiments in \u00a74.2, where the model is fine-tuned as a solution generator; (2)\nthe model is then fine-tuned as a solution evaluator as in \u00a74.3; (3) the evaluator is fine-tuned again\nwith the MLE training objective to regain its ability as a solution generator. We note that in the last\nstep the models are only trained for around 200 steps before they achieve the best performance.\n6\nTable 5: Analysis of the effect of the reference solution style and quality on the few-shot perfor-\nmance of pre-trained models and zero-shot performance of fine-tuned models.\nPaLM 2-S*\nPaLM 2-L\nPass@1\nMaj1@64\nPass@64\nPass@1\nMaj1@64\nPass@64\nMATH Few-shot\n17.4%\n27.2%\n67.8%\n33.4%\n47.6%\n79.4%\nPRM800K Few-shot\n19.2%\n27.8%\n70.0%\n31.4%\n47.2%\n82.4%\nTable 6: Generalization ability of the solution evaluator. The evaluators are tested on solutions\ngenerated from different models. The evaluator checkpoints are trained with Lcls-xent (Eq. 9).\nEvaluator\nSolution Generator\nPass@1\nMaj1@64\nRR.All\nRR.Top-8\nPaLM 2-S*\nPaLM 2-S*\n22.6%\n38.8%\n32.4%\n36.6%\nPaLM 2-L\n35.6%\n55.2%\n48.4%\n50.6%\nPaLM 2-L\nPaLM 2-S*\n22.6%\n38.8%\n46.0%\n46.4%\nPaLM 2-L\n35.6%\n55.2%\n56.8%\n58.8%\nThe results in Table 4 show that the models fine-tuned with the multi-task learning objective can\nachieve better performance than models fine-tuned on the MLE training objective only (\u00a74.2). It\nindicates that the training objective of the solution evaluation task can provide useful supervision\nsignals to the solution generation model. We believe this is because formulating the solution evalu-\nation task as next-word prediction can better leverage the LLM\u2019s ability gained during pre-training.\n5\nANALYSIS\n5.1\nUNDERSTANDING THE EFFECT OF STEP-BY-STEP SOLUTION STYLE\nIn \u00a74.2, we found that the models fine-tuned on the PRM800K solutions significantly outperform\nthe ones fine-tuned on the original MATH solutions. We hypothesize this is because the PRM800K\nsolutions are finer-grained and follows more closely to the step-by-step solution format (an example\nis shown in Figure 2). To investigate whether this difference can also affect the pre-trained models\u2019\nfew-shot performance, we rewrite the custom 4-shot prompt used in Lewkowycz et al. (2022) by\nreplacing the original MATH solution in the prompt with the PRM800K solution. The PRM800K\nsolution is missing for one problem, which we replace with a similar problem with a valid solution.\nTable 5 shows that the few-shot performance is relatively invariant to the difference of the reference\nsolutions, indicating that fine-tuning is necessary for the model to benefit from the potentially higher-\nquality solutions. We leave it as future work to investigate whether fine-tuning on the model\u2019s own\ngenerated solutions with the same style can achieve a similar effect (Zelikman et al., 2022).\n5.2\nEVALUATING THE GENERALIZATION ABILITY OF THE SOLUTION EVALUATOR\nIn \u00a74.3, the PaLM 2-L solution evaluator shows a strong performance at re-ranking the candidate\nsolutions generated by the related PaLM 2-L fine-tuned solution generator. We now investigate\nwhether this evaluator can also be used to re-rank solutions generated by other models. The results\nin Table 6 prove the generalization ability of the fine-tuned PaLM 2-L evaluator. On the other hand,\nPaLM 2-S* is ineffective at re-ranking both the PaLM 2-L and PaLM 2-S* solutions, which suggests\nthat solution evaluation is a non-trivial task that requires sufficiently large models.\n5.3\nCOMPARING DIFFERENT SOLUTION RE-RANKING STRATEGIES\nPrevious work has proposed various re-ranking strategies for math problem candidate solutions.\nTherefore, we compare them using the fine-tuned PaLM 2-L evaluators with respect to their perfor-\nmance in re-ranked solution accuracy and efficiency. The re-ranking strategies compared are:\n7\nTable 7: Comparison of different re-ranking strategies with the PaLM 2-L evaluator fine-tuned with\nLcls-margin (Eq. 8) and Lcls-xent (Eq. 9) respectively. The optimistic performance with the optimal\nhyper-parameter configuration is reported.\nLoss Function\nRR.All\nRR.MajK\nW.RR\nW.RR.MajK\nMaj1\nMaj1.TopN\nLcls-margin\n57.0%\n59.4%\n60.8%\n60.8%\n55.2%\n59.4%\nLcls-xent\n56.8%\n59.4%\n60.8%\n61.2%\n55.2%\n60.0%\n0\n10\n20\n30\n40\n50\n60\nTop K Clusters / Top N Examples\n0.55\n0.56\n0.57\n0.58\n0.59\n0.60\n0.61\nAccuracy\nSolution Evaluator Performance\nRR.All\nRR.MajK\nW.RR\nW.RR.MajK\nMaj1\nMaj1.TopN\n0\n10\n20\n30\n40\n50\n60\nTop K Clusters / Top N Examples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of All Examples\nEvaluator Computation Cost Ratio\nRR.All / W.RR / Maj1.TopN\nRR.MajK / W.RR.MajK\nMaj1\nFigure 1: Analysis of different re-ranking strategies. In the left figure, the performance of different\nre-ranking methods are plotted against the hyper-parameters. In the right figure, the evaluator com-\nputation cost is computed. The PaLM 2-L evaluator trained with the Lcls-xent (Eq. 9) is used.\n1. Vanilla re-ranking (RR.All). The final solution \u02dcX\u2217 is selected via\n\u02dcX\u2217 = arg max\n\u02dc\nX\u2208 \u02dc\nX\nScls( \u02dcX|P),\n(14)\nwhere Scls is the scoring function parameterized by the solution evaluator (Eq. 7), P is a math\nproblem, \u02dc\nX is a set of candidate solutions.\n2. MajK re-ranking (RR.MajK). This is the strategy we used in \u00a74.3, which only re-ranks the set of\nsolutions \u02dc\nXK in the top-K clusters from majority voting:\n\u02dcX\u2217 = arg max\n\u02dc\nX\u2208 \u02dc\nXK\nScls( \u02dcX|P).\n(15)\n3. Weighted re-ranking (W.RR). This strategy is proposed in Li et al. (2023) and adopted by Uesato\net al. (2022), re-ranking answer clusters according to the sum of the answer scores in each clusters:\n\u02dcX\u2217 = arg max\n\u02dc\nX\u2208 \u02dc\nX\nX\n\u02c6\nX\u2208X\ng( \u02dcX, \u02c6X)Scls( \u02c6X|P),\n(16)\nwhere g is the auto-grader that assigns 1 when two answers are equivalent and 0 otherwise (Eq. 1).\n4. Weighted MajK re-ranking (W.RR.MajK). It combines re-ranking strategy 2 and 3 together:\n\u02dcX\u2217 = arg max\n\u02dc\nX\u2208 \u02dc\nXK\nX\n\u02c6\nX\u2208XK\ng( \u02dcX, \u02c6X)Scls( \u02c6X|P).\n(17)\n5. (Self-consistency) majority voting (Maj1) (Wang et al., 2023):\n\u02dcX\u2217 = arg max\n\u02dc\nX\u2208 \u02dc\nX\nX\n\u02c6\nX\u2208 \u02dc\nX\ng( \u02dcX, \u02c6X).\n(18)\n8\n6. Majority voting of top-N solutions (Maj1.TopN). Cobbe et al. (2021) proposes a method that\napplies majority voting only on the top-N solutions \u02dc\nXN selected by the solution evaluator:\n\u02dcX\u2217 = arg max\n\u02dc\nX\u2208 \u02dc\nXN\nX\n\u02c6\nX\u2208 \u02dc\nXN\ng( \u02dcX, \u02c6X).\n(19)\nIn Table 7, we show the optimistic performance of each re-ranking strategy with the best hyper-\nparameter configuration using the PaLM 2-L re-rankers and 64 generated candidate solutions. In\nFigure 1 we provide a detailed analysis with respect to both the re-ranking performance and the\ncomputation efficiency. We found that the weighted re-ranking strategy (W.RR) has strong perfor-\nmance, and the modified version we proposed (W.RR.MajK) can achieve comparable performance\nwhile reducing the computational cost of the solution evaluator.\n6\nRELATED WORK\nHendrycks et al. (2021b) introduced the MATH dataset and fine-tuned GPT-2 (Radford et al., 2019)\nand smaller GPT-3 language models using (1) solution-and-answer and (2) answer-only as targets,\nbut achieving less than 7% test accuracy, demonstrating the difficulty of the task at the time. Sub-\nsequently Cobbe et al. (2021) introduced GSM8K, a similar but easier, elementary-school math\nproblem-solving dataset and showed that sampling followed by re-ranking using a trained verifier\n(or reward model) could improve performance significantly over a single sample.\nRecent work has explored different methods for improving the LLMs\u2019 math solving ability. Specifi-\ncally, Lewkowycz et al. (2022) proposes to continue pre-training the LLMs on math-specific corpora,\ngaining a significant improvement from the original pre-trained models. Uesato et al. (2022) extends\noutcome verifiers/reward-models (ORMs) to process reward models (PRMs) to judge solutions at\nthe step-level by collecting human-annotated labels. They report a negative result in reranking using\nPRMs compared to ORMs on GSM8K but use them successfully to tune models using reinforce-\nment learning. Lightman et al. (2023) in contrast scaled human annotation of process-level labels\ndramatically and achieved improved performance from reranking using PRMs on MATH with GPT-\n4 (OpenAI, 2023) as the base model achieving state-of-the-art performance.\nApart from training methods that directly improve the base model performance, inference-time,\nprompt-engineering techniques, such as chain-of-thought (Nye et al., 2021; Wei et al., 2022; Kojima\net al., 2022) and self-consistency (majority voting) (Wang et al., 2023), have also demonstrated their\neffectiveness with large language models such as PaLM/PaLM2 (Chowdhery et al., 2022; Anil et al.,\n2023) and GPT-3/4 (Brown et al., 2020; OpenAI, 2023), to the point that state-of-the-art performance\non GSM8K is nearing 100%, and hence renewed focus on the harder MATH task. Some recent\nwork (Chen et al., 2022; Gao et al., 2023; Wu et al., 2023; Yue et al., 2023) focuses on leveraging\nexternal tools, such as Python programs, to complement the LLM\u2019s ability, which shows further\nimprovement over pure LLM-based methods. However we focus on the setting with no tools.\nZelikman et al. (2022) shows that iteratively sampling and fine-tuning to the correct model solutions\ncan improve mathematical performance (GSM8K). Although we do not employ this technique, it is\northogonal and conceivably may further improve performance.\n7\nCONCLUSION\nIn this work we investigated different fine-tuning methods to improve the LLMs\u2019 performance on\nmath problem solving. Starting with supervised step-by-step fine-tuning, we first demonstrated\nthe importance of step-by-step solutions for improving fine-tuned LLM performance. We then\nstudied re-ranking methods for fine-tuning the LLMs as solution evaluators, and proposed a new\nre-ranking method which combines the benefit of majority voting and re-ranking together, simul-\ntaneously achieving better solution accuracy and computational efficiency. Lastly, we introduced\na multi-task sequential fine-tuning method, aiming at improving the model\u2019s solution generation\nability with the training objective of the solution evaluation. Our method outperforms the baseline\nfine-tuning method based on the solution generation training objective only, demonstrating its ability\nof improving a generation task using the supervision signal of the corresponding evaluation task.\n9\nREFERENCES\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Z. Chen, Eric Chu, J. Clark, Laurent El\nShafey, Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omer-\nnick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\nGustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James\nBradbury, Siddhartha Brahma, Kevin Michael Brooks, Michele Catasta, Yongzhou Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, C Cr\u00b4epy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, M. C. D\u2019iaz, Nan Du, Ethan Dyer, Vladimir Feinberg,\nFan Feng, Vlad Fienber, Markus Freitag, Xavier Garc\u00b4\u0131a, Sebastian Gehrmann, Lucas Gonz\u00b4alez,\nGuy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, An Ren Hu, Jeffrey Hui,\nJeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wen Hao Jia, Kathleen\nKenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric\nLi, Mu-Li Li, Wei Li, Yaguang Li, Jun Yu Li, Hyeontaek Lim, Han Lin, Zhong-Zhong Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam\nMoussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pel-\nlat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker\nRiley, Alexandra Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Marie Shelby, Am-\nbrose Slone, Daniel Smilkov, David R. So, Daniela Sohn, Simon Tokumine, Dasha Valter, Vijay\nVasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Ke Xu, Yunhan Xu, Lin Wu Xue, Pengcheng Yin, Jiahui Yu, Qiaoling Zhang,\nSteven Zheng, Ce Zheng, Wei Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 techni-\ncal report. ArXiv, abs/2305.10403, 2023. URL https://api.semanticscholar.org/\nCorpusID:258740735.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learners.\nIn\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-\nral Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc.,\n2020.\nURL https://proceedings.neurips.cc/paper_files/paper/2020/\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt-\ning: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588, 2022.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. URL\nhttps://api.semanticscholar.org/CorpusID:239998651.\nSergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc\u2019Aurelio Ranzato. Classical\nstructured prediction losses for sequence to sequence learning. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Papers), pp. 355\u2013364, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/N18-1033.\nURL\nhttps://aclanthology.org/N18-1033.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166, 2023.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,\nand Graham Neubig.\nPAL: Program-aided language models.\nIn Andreas Krause, Emma\n10\nBrunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.),\nProceedings of the 40th International Conference on Machine Learning, volume 202 of Pro-\nceedings of Machine Learning Research, pp. 10764\u201310799. PMLR, 23\u201329 Jul 2023.\nURL\nhttps://proceedings.mlr.press/v202/gao23f.html.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. Proceedings of the Interna-\ntional Conference on Learning Representations (ICLR), 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS,\n2021b.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. In International Conference on Learning Representations, 2020. URL https:\n//openreview.net/forum?id=rygGQyrFvH.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems,\n35:22199\u201322213, 2022.\nAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo,\nYuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra.\nSolving quantitative rea-\nsoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL\nhttps://openreview.net/forum?id=IFXTZERXdM7.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making\nlanguage models better reasoners with step-aware verifier. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315\u2013\n5333, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/\n2023.acl-long.291. URL https://aclanthology.org/2023.acl-long.291.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy\nLee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.\nLet\u2019s verify step by\nstep.\nArXiv, abs/2305.20050, 2023.\nURL https://api.semanticscholar.org/\nCorpusID:258987659.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg\nevaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.\nYixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig.\nBRIO: Bringing order to ab-\nstractive summarization.\nIn Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 2890\u20132903, Dublin, Ireland, May\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.207. URL\nhttps://aclanthology.org/2022.acl-long.207.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Au-\ngustus Odena. Show your work: Scratchpads for intermediate computation with language models,\n2021.\nOpenAI.\nGpt-4 technical report.\nArXiv, abs/2303.08774, 2023.\nURL https://api.\nsemanticscholar.org/CorpusID:257532815.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and\noutcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\n11\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=1PL1NIMMrw.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\nQuoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\nmodels. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Ad-\nvances in Neural Information Processing Systems, 2022. URL https://openreview.net/\nforum?id=_VjQlMeSB_J.\nYiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat\nLee, Richard Peng, and Chi Wang. An empirical study on challenging math problem solving with\ngpt-4. arXiv preprint arXiv:2306.01337, 2023.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint\narXiv:2309.05653, 2023.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with rea-\nsoning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances\nin Neural Information Processing Systems, volume 35, pp. 15476\u201315488. Curran Associates, Inc.,\n2022.\nURL https://proceedings.neurips.cc/paper_files/paper/2022/\nfile/639a9a172c044fbb64175b5fad42e9a5-Paper-Conference.pdf.\nYao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu.\nCalibrating sequence likelihood improves conditional language generation. In The Eleventh In-\nternational Conference on Learning Representations, 2023. URL https://openreview.\nnet/forum?id=0qSOodKmJaN.\nAojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia,\nLinqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word problems using\ngpt-4 code interpreter with code-based self-verification. ArXiv, abs/2308.07921, 2023. URL\nhttps://api.semanticscholar.org/CorpusID:260900008.\n12\nA\nEXAMPLES OF STEP-BY-STEP SOLUTIONS\nProblem\nWhat is the greatest common factor of 20! and 200,000? (Reminder: If n is a positive integer,\nthen n! stands for the product 1 \u00b7 2 \u00b7 3 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (n \u2212 1) \u00b7 n.)\nMATH Solution\nThe prime factorization of 200, 000 is 26 \u00b7 55. Then count the number of factors of 2 and 5 in\n20!. Since there are 10 even numbers, there are more than 6 factors of 2. There are 4 factors of\n5. So the greatest common factor is 26 \u00b7 54 = 40,000 .\nPRM800k solution\nI want to find the largest positive integer that divides both 20! and 200,000 evenly. One way to\ndo this is to factor both numbers into prime factors and look for the common ones.\nI know that 200,000 = 25 \u00b7 104 = 29 \u00b7 54.\nTo find the prime factorization of 20!, I can use the fact that it is the product of all the positive\nintegers from 1 to 20.\nFor each prime number p between 1 and 20, I can count how many multiples of p are in that\nrange.\nFor example, there are 10 multiples of 2 between 1 and 20, namely 2, 4, 6, . . . , 20.\nBut there are also 5 multiples of 4, which is 22, and 2 multiples of 8, which is 23, and 1 multiple\nof 16, which is 24.\nSo, the total power of 2 in 20! is 10 + 5 + 2 + 1 = 18.\nSimilarly, there are 4 multiples of 5, namely 5, 10, 15, 20, so the power of 5 in 20! is 4.\nThere are 6 multiples of 3, namely 3, 6, 9, . . . , 18, but there are also 2 multiples of 9, which is\n32, so the power of 3 in 20! is 6 + 2 = 8.\nThere are 2 multiples of 7, namely 7 and 14, so the power of 7 in 20! is 2.\nThere are 1 multiple of each of the other prime numbers 11, 13, 17, and 19, so the powers of\nthose primes in 20! are 1 each.\nTherefore, the prime factorization of 20! is 218 \u00b7 38 \u00b7 54 \u00b7 72 \u00b7 11 \u00b7 13 \u00b7 17 \u00b7 19.\nTo find the greatest common factor of 20! and 200,000, I need to take the lowest power of each\ncommon prime factor.\nThe only common prime factors are 2 and 5, and the lowest powers are 9 and 4, respectively.\nSo, the greatest common factor is 29 \u00b7 54 = 512 \u00b7 625 = 320,000.\n# Answer\n320,000\nFigure 2: Example comparing MATH and GPT-4 generated step-by-step solutions.\nB\nANALYSIS OF SOLUTION CLUSTERS\nWe conduct an in-depth analysis to better understand the property of the solution clusters. We first\nanalyze the performance upper-bound of the solution-cluster re-ranking approach in Figure 3, which\nindicates that the evaluators we trained in \u00a74.3 can still be further improved. In fact, with a perfect\nsolution evaluator, an accuracy of 64.0% can be achieved by re-ranking just the top-2 clusters, while\nthe majority-voting performance is only 55.2%.\nIn Figure 4, we further investigate the difficulty of the re-ranking task by analyzing the characteristics\nof the PaLM 2-L solution clusters. Specifically, we compare the size of the correct solution cluster\nagainst (1) the number of all sampled solutions, (2) the size of the top-1 solution cluster when the\ncorrect solution is not selected by the majority voting. The results show two trends: (1) in general\nthe re-ranking task is difficult, since the correct solutions only consist of less than 5% of all solutions\nin 50% of the time; (2) re-ranking top-clusters is relatively easier, since the ratio of the size of correct\nsolution cluster and the top-1 solution cluster is much more balanced. We believe these observations\ncan partially explain the benefit of the solution-cluster re-ranking method we proposed.\n13\n0\n10\n20\n30\n40\n50\n60\nTop K Clusters\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy / Ratio\nOracle Performance\nMajK@64\nMaj1@64\nTop-K Solution Ratio\nFigure 3: Performance comparison of the majority voting (Maj1@64) and the oracle that always\nselects the correct solution in the top-K clusters (MajK@64).\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRatio\n0\n10\n20\n30\n40\n50\nPercent\nDistribution of #correct solutions v.s. #all solutions\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nPercent\nDistribution of #correct solutions v.s. #Maj@1 solutions\nFigure 4: Distributions of (1) the ratio of the number of correct solutions v.s. all solutions, (2) the\nratio of number of correct solutions v.s. Maj1@64 solutions when the correct solution is not the\nMaj1@64 solution.\n14\n"
  },
  {
    "title": "When can transformers reason with abstract symbols?",
    "link": "https://arxiv.org/pdf/2310.09753.pdf",
    "upvote": "2",
    "text": "When can transformers reason with abstract symbols?\nEnric Boix-Adser\u00e0*1,2\nOmid Saremi1\nEmmanuel Abbe1,3\nSamy Bengio1\nEtai Littwin1\nJoshua Susskind1\n1Apple\n2MIT\n3EPFL\neboix@mit.edu,emmanuel.abbe@epfl.ch\n{osaremi,bengio,elittwin,jsusskind}@apple.com\nOctober 17, 2023\nAbstract\nWe investigate the capabilities of transformer large language models (LLMs) on relational reasoning tasks in-\nvolving abstract symbols. Such tasks have long been studied in the neuroscience literature as fundamental building\nblocks for more complex abilities in programming, mathematics, and verbal reasoning. For (i) regression tasks, we\nprove that transformers generalize when trained, but require astonishingly large quantities of training data. For (ii)\nnext-token-prediction tasks with symbolic labels, we show an \u201cinverse scaling law\u201d: transformers fail to generalize\nas their embedding dimension increases. For both settings (i) and (ii), we propose subtle transformer modifications\nwhich can reduce the amount of data needed by adding two trainable parameters per head.\n1\nIntroduction\nReasoning can be defined as the ability to use logical rules to generalize outside of one\u2019s training data. During most\nof the history of AI, reasoning was widely thought to be achievable only through programs that manipulated mathe-\nmatical symbols using hand-coded logical rules [NSS59; Mar98]. However, recent developments have challenged this\nparadigm: as large language models (LLMs) are trained with increasing quantities of data, they begin to exhibit the\nability to reason mathematically [Kap+20; Yua+23]. But why does more data help an LLM to reason outside of its\ntraining set? And how efficient can we make LLMs in this regard?\nIn this paper, we focus on tasks involving relational reasoning about abstract symbols, where meaning is derived\nfrom how the symbols interact with each other, rather than from the identities of the symbols themselves. The required\nreasoning capability is basic, but has been hypothesized to be necessary for much of human cognition [Fod75; New80;\nSKM84; Mar98; Hol12; Kri+13; WSC20]. For example, in mathematics or computer science, relational reasoning is\na key part of parsing a proof or a program: variable names are abstract symbols and the functionality of the proof or\nprogram only depends on how they relate to each other and not on the variable names themselves. Furthermore, the\nneuroscience literature often measures relational reasoning through psychometric tests such as Raven\u2019s progressive\nmatrices [Web+20; WHL23; Web+23], because it is conjectured to underlie more complex tasks in verbal reasoning\n[SKM84; Hol12].\nFigure 1 provides an example of a simple relational reasoning task. We train an LLM to evaluate Python programs\nxi, and return their output yi. Memorizing the training data is easy [Zha+21b], but we wish to measure reasoning: will\nthe LLM learn to treat the variable names as abstract symbols, enabling generalization beyond its training dataset?\nTo evaluate this, we adopt an out-of-distribution setting, where the train and test data distributions differ [Mar98;\nAbb+23]. The test dataset consists of the same programs, but with new variable names never seen during training.\nRemarkably, as the training set size increases, the LLM\u2019s ability to reason outside of its training data improves, as it\nlearns to use the relations between the variable names to classify, instead of simply memorizing the training data.\nIn Figure 2, we consider a relational reasoning task with one extra layer of complexity: each sample is labeled with\na symbol (instead of a real number +1 or \u22121 as in Figure 1). For the LLM to generalize to symbols unseen at train\ntime, not only must it learn to track the value stored in a variable, but it also must learn to predict symbolic labels at\ntest time that do not occur in its training data. On this more sophisticated task, we observe that a transformer requires\nmuch more training data to generalize.\n1\narXiv:2310.09753v1  [cs.CL]  15 Oct 2023\n(a) Train data\n(b) Test data\n(c) Transformer performance\nxi\nyi\na=1;b=-1;print(a)\n+1\nc=1;a=-1;print(a)\n-1\nf=1;c=-1;print(f)\n+1\nh=1;q=-1;print(q)\n-1\n. . .\n. . .\nxtest\ni\nytest\ni\nR=1;A=-1;print(R)\n+1\nQ=1;V=-1;print(V)\n-1\n. . .\n. . .\n101\n102\n103\n104\nNumber of training samples\n0.0\n0.5\n1.0\n1.5\n2.0\nTest loss\nTransformer\nTransformer + KQ identity\nFigure 1: (a,b) Variable names in the test data never appear in the train data (indicated by lower/upper-case names).\n(c) Our theory motivates a subtly modified transformer architecture (see Observation 1.2), which solves the reasoning\ntask with less training data. Details in Appendix A.\n(a) Train data\n(b) Test data\n(c) Transformer performance\nxi\nyi\na=\"d\";b=\"q\";print(a)\nd\nc=\"r\";a=\"w\";print(a)\nw\nf=\"y\";c=\"u\";print(f)\ny\nh=\"o\";q=\"s\";print(q)\ns\n. . .\n. . .\nxtest\ni\nytest\ni\nR=\"F\";A=\"Z\";print(R)\nF\nQ=\"B\";V=\"A\";print(V)\nA\n. . .\n. . .\n101\n102\n103\n104\n105\nNumber of training samples\n0.0\n0.5\n1.0\n1.5\n2.0\nTest error\nTransformer\nTransformer + KQ,VO identity\nFigure 2: (a,b) A relational reasoning task where labels are also symbols. (c) Our modified transformer learns the\nreasoning task with less data (see Observation 1.2 and Theorem 1.4). Details in Appendix A.\n1.1\nOur contributions\nTo understand these phenomena, we study a framework of relational reasoning tasks of which Figures 1 and 2 are\nspecial cases. (i) For real-valued label tasks as in Figure 1, we prove that transformers will learn to generalize, but\nrequire a large quantity of data. (ii) For symbolic-label tasks as in Figure 2, we prove that transformers will fail\nas their embedding dimension grows. For settings (i) and (ii) we propose parametrization adjustments that improve\ndata efficiency. Finally, we support our claims experimentally, and also cast light on how pretraining helps improve\nreasoning abilities.\n1.1.1\nTemplate tasks: a framework for measuring reasoning with abstract symbols\nBuilding on a line of work in neuroscience [Mar98; MK16; KRS18; WSC20; Ker+22; Alt+23; WHL23], we formalize\na framework of reasoning tasks called template tasks. These come in two kinds: regression problems with real labels\nas in Figure 1, and next-token-prediction problems with symbolic labels as in Figure 2.\nReal-label template tasks\nA real-label template task is specified by a collection of \u201ctemplates\u201d labeled by real\nnumbers, which are used to generate the train and test data. For instance, the datasets in Figure 1 are generated from\nthe templates\n\u201c\u03b1=1;\u03b2=-1;print(\u03b1)\u201d \u2192 label=+1\nand\n\u201c\u03b1=1;\u03b2=-1;print(\u03b2)\u201d \u2192 label=-1 ,\n(1)\nbecause every sample (xi, yi) \u2208 X k \u00d7 Y is formed by picking a template and replacing the placeholders \u03b1, \u03b2 (which\nwe call \u201cwildcards\u201d) with variable names. By testing on symbols unseen in the train set, we measure the ability of an\nLLM to learn logical rules on the relations between symbols. To succeed, the LLM must effectively infer the templates\nfrom training data, and at test time match samples to the corresponding templates to derive their labels. Apart from\nprogramming tasks as in Figure 1, this framework captures several natural settings:\n\u2022 Same/different task. The simplest relational reasoning task is when the templates are \u201c\u03b1\u03b1\u201d and \u201c\u03b1\u03b2\u201d labeled by\n+1 and \u22121. This encodes learning to classify two symbols as equal (e.g., AA, BB) or as distinct (e.g., AB,\nBC), even when the symbols were unseen in the training data. This task has been studied empirically in animal\nbehavior [MK16] and in neural networks [KRS18; WSC20].\n2\n1\n2\n3\n4\n2\n1\n1\n2\n3\n4\n2\n1\n1\n2\n3\n4\n?\n1\n2\n3\n4\n2\n1\n1\n2\n3\n4\n?\n(a) Distribution of 3\n(b) Relational match-to-sample\n(c) Matrix reasoning\nFigure 3: Example psychometric tasks from [Rav38; WSC20; Ker+22; Web+23] which fall under our theory. For all\nof these tasks, networks are trained with one alphabet of symbols and then tested on held-out symbols. (a) The task\nis to complete the bottom row so that the set of elements is the same as in the top row (answer: 2). (b) The task is to\nmatch the first row to one of two alternative patterns (answer: 1). (c) A standard Raven\u2019s progressive matrices task\n[Rav38] (answer: three dark circles). Task (a) can be encoded with 144 templates, and task (b) with 40 templates; task\n(c) combines three symbolic-label template tasks, each with 37 templates. Details in Appendix A.\n\u2022 Mathematical relations. More complex relations of symbols are also easy to encode: e.g., with the set of\ntemplates {\u03b1\u03b1\u03b2, \u03b1\u03b1\u03b1, \u03b1\u03b2\u03b1} labeled with +1, and {\u03b1\u03b2\u03b2} labeled with \u22121 the task is to learn whether the first\ntoken occurs in the majority of the tokens of the string. In general, for length-k strings, this can task can be\nencoded with 2k\u22121 templates.\n\u2022 Word problems. Word problems often have building blocks that follow simple templates. For example, the\ntemplate \u201cIf \u03b1 gives \u03b2 5 \u03b3, how many \u03b3 does \u03b2 have?\u201d labeled by +5, could generate the data \u201cIf Alice gives\nBob 5 oranges, how many oranges does Bob have?\u201d or the data \u201cIf Rob gives Ada 5 apples, how many apples\ndoes Ada have?\u201d\n\u2022 Psychometric tests. Several psychometric tests of relational reasoning, which have recently been used to probe\nLLMs [Rav38; WSC20; Alt+23; Ker+22; WHL23; Web+23], are special cases of template tasks. Figure 3\nillustrates some examples.\nSymbolic-label template tasks\nA symbolic-label template task is analogous, but each template is labeled by a\nwildcard. The train and test datasets in Figure 2 are generated by:\n\u201c\u03b1=\"\u03b3\";\u03b2=\"\u03b4\";print(\u03b1)\u201d \u2192 label=\u03b3\nand\n\u201c\u03b1=\"\u03b3\";\u03b2=\"\u03b4\";print(\u03b2)\u201d \u2192 label=\u03b4 ,\n(2)\nwhere \u03b1, \u03b2, \u03b3, \u03b4 are wildcards. Other examples include:\n\u2022 Programming. The template \u201cprint(\"\u03b1\")\u201d labeled with \u03b1 generates (print(\"A\"), A) or (print(\"dog\"), dog),\nand so an LLM that learns on the corresponding task can robustly evaluating print statements on symbols not\nseen in the training data.\n\u2022 Mathematical functions. For example, the set of templates {\u03b1\u03b1\u03b1, \u03b1\u03b2\u03b1, \u03b1\u03b1\u03b2, \u03b2\u03b1\u03b1} labeled by \u03b1 encode the\ntask of outputting the majority token in a length-3 string with a vocabulary of two symbols. Similarly, for\nlength-k strings, the task of outputting the majority element can be encoded with 2k\u22121 templates.\n\u2022 Word problems. The template \u201cIf \u03b1 gives \u03b2 \u03b4 \u03b3, how many \u03b3 does \u03b2 have?\u201d, labeled by \u03b4, can generate several\nof the above word problems. An LLM that solves this task will output the correct answer of, say 10 if \u03b4 = 10 at\ntest time even if \u03b4 \u0338= 10 in all training data.\nIn practice, template tasks may occur as an intermediate computation for a larger problem. Here we isolate template\ntasks to perform a rigorous theoretical analysis. We analyze the real- and symbolic-label settings separately, as they\ngive complementary insights.\n1.1.2\nAnalytical results for template tasks with real labels (regression setting)\n(1) MLPs fail to generalize to unseen symbols\nA classical criticism of connectionism by [Mar98] is that neural\nnetworks do not learn relational reasoning when trained. We support this criticism in Appendix I by proving that\n3\nclassical MLP architectures (a.k.a. fully-connected networks) trained by SGD or Adam will not generalize in template\ntasks on symbols unseen during training. This failure to reason relationally occurs regardless of the training data size.\nThe proof uses a permutation equivariance property of MLP training [Ng04; Sha18; LZA20; Abb+22; AB22].\n(2) Transformers generalize to unseen symbols, but require large data diversity\nNevertheless, the criticism of\n[Mar98] is not valid for modern transformer architectures [Vas+17]. We analyze the training dynamics of a transformer\nmodel and establish that it can learn to reason relationally:\nTheorem 1.1 (Informal Theorem 4.4). For any real-label template task, a wide-enough transformer architecture\ntrained by gradient flow on sufficiently many samples generalizes on unseen symbols.\nHere the key points are: (a) Universality. The transformer architecture generalizes on symbols unseen in train\ndata regardless of which and how many templates are used to define the reasoning task. (b) Large enough number of\nsamples. Our theoretical guarantees require the training dataset size to be large, and even for very basic tasks like the\ntwo-template task in Figure 1, good generalization begins to occur only at a very large number of training samples\nconsidering the simplicity of the task. This raises the question of how the inductive bias of the transformer can be\nimproved.\n(3) Improving data-efficiency of transformers\nThe proof of Theorem 1.1 inspires a parametrization modification\nthat empirically lowers the quantity of data needed by an order of magnitude, by making it easier for the transformer\nto access the incidence matrix XXT of the input:\nObservation 1.2. Adding one trainable parameter a to each attention head so that WKW T\nQ is replaced by WKW T\nQ +\naI dramatically improves transformers\u2019 data-efficiency on template tasks.\n1.1.3\nAnalytical results for template tasks with symbolic labels (next-token-prediction setting)\n(4) Transformers fail at copying unseen symbols\nSurprisingly, the story is different for symbolic-label tasks.\nTransformers\u2019 performance degrades as the model grows (an \u201cinverse scaling\u201d law [McK+23]). Large transformers\nfail even for the task of copying the input.\nTheorem 1.3 (Informal Theorem 5.1). Transformers with large embedding dimension fail to generalize on unseen\nsymbols for the copy-task outputting label \u201c\u03b1\u201d on template \u201c\u03b1\u201d.\n(5) Modifying transformers for success\nHowever, we propose adding an attention-modulated skip connection,\nwhich is a subtle modification that corrects this failure.\nTheorem 1.4 (Informal Theorem 5.2). Adding one trainable parameter b to each attention head so that WV W T\nO is\nreplaced by WV W T\nO + bI makes transformers generalize on the task of Theorem 1.3.\n1.1.4\nExperimental validation and exploration\nWe conclude with experimental validation, including showing that the transformer modifications proposed in Ob-\nservation 1.2 and Theorem 1.4 improve performance in GPT-2 trained on Wikitext. We also show data-efficiency\nimprovements on template tasks by fine-tuning a pretrained model, and give as an explanation the pronounced diag-\nonals in WKW T\nQ and WV W T\nO matrices of pretrained models [TK23], which coincide with the proposed transformer\nmodifications.\n1.2\nRelated literature\nA spate of recent work studies whether and how LLMs perform various reasoning tasks, each focusing on one compo-\nnent of reasoning: these include recognizing context-free grammars [Zha+23; AL23], generalizing out-of-distribution\nwhen learning Boolean functions [Abb+23], performing arithmetic [Nan+23], learning in context [Gar+22; Ahn+23;\nZFB23], and evaluating indexing [Zha+21a]. Our setting is closest to that of empirical work studying neural networks\non relational reasoning tasks [Web+23]. For example, the four tasks in [WSC20], the matrix digits task in [WHL23],\nthe SET game task in [Alt+23], and most of the tasks in [Ker+22] (with the exception of the relational games tasks),\n4\nare examples of real-label template tasks that fall under our theory. Furthermore, [KRS18] shows experimentally that\nMLPs fail on the same/different template task, and we provide a proof for this in Appendix I. Finally, there is also\na literature on modifying training to improve relational reasoning: [Web+20] proposes applying Temporal Context\nNormalization during training, and [San+17; San+18; PPW18; Sha+20; WSC20; Ker+22; Alt+23] propose new archi-\ntectures. In contrast, our focus is on proving when the transformer architecture learns or fails to learn, and on applying\nthis theoretical understanding to give a subtle modification to improve its data-efficiency for relational reasoning.\n2\nTransformer definition\nWe interchangeably denote an input by a string x \u2208 X k or a matrix X \u2208 Rk\u00d7m constructed by stacking the one-\nhot vectors X = [ex1, . . . , exk]T of the string\u2019s tokens. We study a depth-1 transformer architecture [Vas+17].\nThe transformer has H heads with parameters W K,h, W Q,h, W V,h, W O,h \u2208 Rdhead\u00d7demb, an embedding layer\nW E \u2208 Rm\u00d7demb, positional embeddings P \u2208 Rk\u00d7demb, an MLP layer with parameters W A, W B \u2208 Rdmlp\u00d7demb, a\nfinal unembedding layer with weights wU \u2208 Rdemb, and an activation function \u03d5. The network takes in X \u2208 Rk\u00d7m\nand outputs\nftrans(X; \u03b8) = wT\nUz2 \u2208 R\n(Unembedding layer)\nwhere\nz2 = W T\nB\u03d5(W Az1) \u2208 Rdemb\n(MLP layer)\nz1 =\nX\nh\u2208[H]\nAT\nh ek \u2208 Rdemb\n(Attention layer output at final token)\nAh = smax(\u03b2Z0W T\nK,hW Q,hZT\n0 )Z0W T\nV,hW O,h \u2208 Rk\u00d7demb\n(Attention heads)\nZ0 = XW E + \u03b3P \u2208 Rk\u00d7demb .\n(Embedding layer)\nHere \u03b2, \u03b3 \u2265 0 are two hyperparameters that control the inverse temperature of the softmax and the strength of the\npositional embeddings, respectively. The architecture is standard, except that we remove skip connections and layer\nnorm as these are not needed for our theoretical results. An additional notation is that in this paper we write [n] =\n{1, . . . , n}.\n3\nTemplate tasks\nWe formally define template tasks with real labels. The case of symbolic labels is in Appendix J.\nDefinition 3.1. A template is a string z \u2208 (X \u222a W)k, where X is an alphabet of tokens, and W is an alphabet of\n\u201cwildcards\u201d. A substitution map is an injective function s : W \u2192 X. We write sub(z, s) \u2208 X k for the string\nwhere each wildcard is substituted with the corresponding token: sub(z, s)i = zi if zi \u2208 X, and sub(z, s)i = s(zi)\nif zi \u2208 W. The string x \u2208 X k matches the template z if x = sub(z, s) for some substitution map s and also\ns(W) \u2229 {zi}i\u2208[k] = \u2205: i.e., the substituted tokens did not already appear in the template z.\nExample\nUsing Greek letters to denote the wildcards and Latin letters to denote regular tokens, the template\n\u201c\u03b1\u03b1\u03b2ST\u201d matches the string \u201cQQRST\u201d, but not \u201cQQQST\u201d (because the substitution map is not injective) and not\n\u201cQQSST\u201d (because \u03b2 is replaced by S which is already in the template).\nA template task\u2019s training data distribution is generated by picking a template randomly from a distribution, and\nsubstituting its wildcards with a random substitution map.\nDefinition 3.2. A real-label template data distribution D = D(\u00b5tmplt, {\u00b5sub,z}z, f\u2217, \u03c3) is given by\n\u2022 a template distribution \u00b5tmplt supported on templates in (X \u222a W)k,\n\u2022 for each z \u2208 supp(\u00b5tmplt), a distribution \u00b5sub,z over substitution maps s : W \u2192 X ,\n\u2022 template labelling function f\u2217 : supp(\u00b5tmplt) \u2192 R , and a label-noise parameter \u03c3 \u2265 0.\n5\nWe draw a sample (x, y) = (sub(z, s), f\u2217(z) + \u03be) \u223c D, by drawing a template z \u223c \u00b5tmplt, a substitution map\ns \u223c \u00b5sub,z, and label noise \u03be \u223c N(0, \u03c32).\nFinally, we define what it means for a model to generalize on unseen symbols; namely, the model should output the\nthe correct label for any string x \u2208 X k, regardless of whether the string is in the support of the training distribution.\nDefinition 3.3. A (random) estimator \u02c6f : X k \u2192 R generalizes on unseen symbols with (\u03f5, \u03b4)-error if the following\nis true. For any x \u2208 X k that matches a template z \u2208 supp(\u00b5tmplt), we have\n( \u02c6f(x) \u2212 f\u2217(z))2 \u2264 \u03f5 ,\nwith probability at least 1 \u2212 \u03b4 over the randomness of the estimator \u02c6f.\nExample\nIf the training data is generated from a uniform distribution on templates \u201c\u03b1\u03b1\u201d with label 1 and \u201c\u03b1\u03b2\u201d\nfor label -1, then it might consist of the data samples {(AA, 1), (BB, 1), (AB, \u22121), (BA, \u22121)}. An estimator that\ngeneralizes to unseen symbols must correctly label string CC with +1 and string CD with \u22121, even though these\nstrings consist of symbols that do not appear in the training set. This is a nontrivial reasoning task since it requires\nlearning to use the relations between the symbols to classify rather than the identities of the symbols.\n4\nAnalysis for template tasks with real labels\nWe establish that transformers generalize on unseen symbols on any real-label template task (i.e., the regression\nsetting), when trained with enough data. It is important to note that this is not true for all architectures, as we prove in\nAppendix I that MLPs trained by SGD or Adam will not succeed.\nOur achievability result for transformers requires the templates in the distribution \u00b5tmplt to be \u201cdisjoint\u201d, since\notherwise the correct label for a string x is not uniquely defined, because x could match more than one template:\nDefinition 4.1. Two templates z, z\u2032 \u2208 (X \u222a W)k are disjoint if no x \u2208 X k matches both z and z\u2032.\nFurthermore, in order to ensure that the samples are not all copies of each other (which would not help generaliza-\ntion), we have to impose a diversity condition on the data.\nDefinition 4.2. The data diversity is measured by \u03c1 = minz\u2208supp(\u00b5tmplt) mint\u2208X\n1\nPs\u223c\u00b5sub,z [t\u2208s(W)].\nWhen the data diversity \u03c1 is large, then no token is much more likely than others to be substituted. If \u03c1 is on the\norder of the number of samples n, then most pairs of data samples will not be equal.\n4.1\nTransformer random features kernel\nWe analyze training only the final wU layer of the transformer, keeping the other weights fixed at their random\nGaussian initialization. Surprisingly, even though we only train the final layer of the transformer, this is enough to\nguarantee generalization on unseen symbols. Taking the width parameters H, demb, dmlp, dhead to infinity, and the\nstep size to 0, the SGD training algorithm with weight decay converges to kernel gradient flow with the following\nkernel Ktrans,1\nKtrans(X, Y ) = Eu,v[\u03d5(u)\u03d5(v)] for u, v \u223c N(0,\n\u0014Kattn(X, X)\nKattn(X, Y )\nKattn(Y , X)\nKattn(Y , Y )\n\u0015\n)\n(3)\nwhere Kattn(X, Y ) = Em(X),m(Y )[smax(\u03b2m(X))T (XY T + \u03b32I)smax(\u03b2m(Y ))]\n[m(X), m(Y )] \u223c N(0,\n\u0014XXT + \u03b32I\nXY T + \u03b32I\nY XT + \u03b32I\nY Y T + \u03b32I\n\u0015\n) .\nThe function outputted by kernel gradient flow is known to have a closed-form solution in terms of the samples,\nthe kernel, and the weight-decay parameter \u03bb, which we recall in Proposition 4.3.\n1This kernel is derived in Appendix H, and assumes that every string x ends with a special [CLS] classification token that does not appear\nelsewhere in the string.\n6\nProposition 4.3 (How kernel gradient flow generalizes; see e.g., [Wel13].). Let (X1, y1), . . . , (Xn, yn) be training\nsamples. With the square loss and ridge-regularization of magnitude \u03bb, kernel gradient flow with kernel K converges\nto the following solution\n\u02c6f(X) = yT ( \u02c6K + \u03bbI)\u22121k(X) ,\n(4)\nwhere y = [y1, . . . , yn] \u2208 Rn are the train labels, \u02c6K \u2208 Rn\u00d7n is the empirical kernel matrix and has entries\n\u02c6Kij = K(Xi, Xj), and k(X) \u2208 Rn has entries ki(X) = K(Xi, X).\n4.2\nTransformers generalize on unseen symbols\nWe analyze the solution to the kernel gradient flow with the transformer random features, which corresponds to training\nthe last layer with SGD with weight decay in the infinitely-wide, infinitely-small-step-size limit.\nTheorem 4.4 (Transformers generalize on unseen symbols). Let \u00b5tmplt be supported on a finite set of pairwise-disjoint\ntemplates ending with [CLS] tokens. Then, for almost any \u03b2, \u03b3, b1, b2 parameters (except for a Lebesgue-measure-zero\nset), the transformer random features with \u03d5(t) = cos(b1t + b2) generalizes on unseen symbols.2 Formally, there are\nconstants c, C > 0 and ridge regularization parameter \u03bb > 0 that depend only \u03b2, \u03b3, b1, b2, \u00b5tmplt, f\u2217, \u03c3, such that for\nany x matching a template z \u2208 supp(\u00b5tmplt) the kernel ridge regression estimator \u02c6f in (4) with kernel Ktrans satisfies\n| \u02c6f(x) \u2212 f\u2217(z)| \u2264 C\np\nlog(1/\u03b4)/n + C\np\n1/\u03c1 ,\nwith probability at least 1 \u2212 \u03b4 \u2212 exp(\u2212cn) over the random samples.\nThe first term is due to the possible noise in the labels. The second term quantifies the amount of sample diversity\nin the data. Both the sample diversity and the number of samples must tend to infinity for an arbitrarily small error\nguarantee.\nProof sketch\n(1) In Lemma 4.5 we establish with a sufficient condition for kernel ridge regression to generalize on\nunseen symbols. (2) We prove that Ktrans satisfies it.\n(1) Sufficient condition. Let \u00b5tmplt be supported on templates z1, . . . , zr. Let R = \u222ai\u2208[k],j\u2208[r]{zj,i} be the tokens\nthat appear in the templates. Let [n] = I1 \u2294 I2 \u2294 \u00b7 \u00b7 \u00b7 \u2294 In be the partition of the samples such that if a \u2208 Ij then\nsample (xa, ya) is drawn by substituting the wildcards of template zj. Two samples xa, xb that are drawn from the\nsame template zj may be far apart as measured by the kernel: i.e., the kernel inner product K(xa, xb) may be small.\nHowever, these samples will have similar relationship to most other samples:\nK(xa, xi) = K(xb, xi)\nfor most i \u2208 [n] .\n(5)\nSpecifically, if the wildcards of xa, xb and xi are substituted by disjoint sets of tokens that do not appear in the\ntemplates, then (5) holds. Therefore, as the sample diversity \u03c1 increases, the empirical kernel matrix \u02c6K becomes\napproximately block-structured with blocks Ij \u00d7 Ij\u2032. For most samples xa, xb corresponding to template zj, and\nmost xa\u2032, xb\u2032 corresponding to template zj\u2032 we have\nK(xa, xa\u2032) = K(xb, xb\u2032) = K(sub(zj, s), sub(zj\u2032, s\u2032)) := Nj,j\u2032 ,\n(6)\nwhere s, s\u2032 : W \u2192 X are substitution maps satisfying\ns(W) \u2229 s\u2032(W) = 0\nand\ns(W) \u2229 R = s\u2032(W) \u2229 R = \u2205.\n(7)\nOne can check that (6) and (7) uniquely define a matrix N \u2208 Rr\u00d7r which gives the entries in the blocks of \u02c6K,\nwith one block for each pair of templates.3 See Figure 4.\n2We analyze the shifted and rescaled cosine activation function \u03d5(t) = cos(b1t + b2) out of technical convenience, but conjecture that most\nnon-polynomial activation functions should succeed.\n3This assumes a \u201ctoken-symmetry\u201d property of K that is satisfied by transformers; details in the full proof.\n7\n\u02c6K =\nI1\nI2\nN = [\nK(aa, bb)\nK(aa, bc)\nK(aa, bc)\nK(ab, cd)]\nI1\nI2\n\u2208 Rn\u00d7n,\nN =\n\u0014K(AA, BB)\nK(AA, BC)\nK(BC, AA)\nK(AB, CD)\n\u0015\n=\nN = [\nK(aa, bb)\nK(aa, bc)\nK(aa, bc)\nK(ab, cd)]\n\u2208 R2\u00d72\nFigure 4: Illustration of structure of \u02c6K and N for the same/different task, which has r = 2 templates z1 = \u03b1\u03b1 and\nz2 = \u03b1\u03b2. As the sample diversity \u03c1 increases and the number of samples n increases, the empirical kernel matrix\n\u02c6K \u2208 Rn\u00d7n becomes approximately (r \u00d7 r)-block-structured, and within each block most of the entries are given by\nN \u2208 Rr\u00d7r; exceptions where this is not true, including the diagonals, are drawn in black. Furthermore, the spectrum\nof \u02c6K is increasingly determined by the spectrum of N, and if N is nonsingular then the top eigenspace increasingly\naligns with the span of the indicator vectors on I1, . . . , Ir.\nIf the matrix N is nonsingular and the number of samples is large, then the span of the top r eigenvectors of \u02c6K\nwill align with the span of the indicator vectors on the sets I1, . . . , Ir. Furthermore, when testing a string xtest that\nmatches template zj, but might not have appeared in the training set, it holds that for most a \u2208 Ij, we have\nk(xtest) = [K(xtest, x1), . . . , K(xtest, xn)] \u2248 [K(xa, x1), . . . , K(xa, xn)] = \u02c6Ka,: .\nIn words, the similarity relationship of xtest to the training samples is approximately the same as the similarity\nrelationship of xa to the training samples. So the kernel ridge regression solution (4) approximately equals the average\nof the labels of the samples corresponding to template zj, which in turn is approximately equal to the template label\nby a Chernoff bound,\nyT ( \u02c6K + \u03bbI)\u22121k(xtest) \u2248\n1\n|Ij|\nX\na\u2208Ij\nyi \u2248 f\u2217(zj) .\n(8)\nTherefore, kernel ridge regression generalizes on xtest. It is important to note that the number of samples needed until\n(8) is a good approximation depends on the nonsingularity of N. This yields the sufficient condition for kernel ridge\nregression to succeed (proof in Appendix C).\nLemma 4.5 (Informal Lemma C.2). If N is nonsingular, then (4) generalizes to unseen symbols.\n(2) Ktrans satisfies the sufficient condition. We now show that for any collection of disjoint templates z1, . . . , zr,\nthe matrix N trans := N \u2208 Rr\u00d7r defined with kernel K = Ktrans is nonsingular. This is challenging because Ktrans\ndoes not have a closed-form solution because of the expectation over softmax terms in its definition (3). Therefore, our\nanalysis of the transformer random feature kernel is, to the best of our knowledge, the first theoretical analysis showing\nthat the transformer random features learn a nontrival class of functions of sequences. We proceed by analyzing the\nMLP layer and the attention layer separately, observing that a\u201cweak\u201d condition on Kattn can be lifted into the \u201cstrong\u201d\nresult that N trans is nonsingular. The intuition is that as long as Kattn is not a very degenerate kernel, it is unlikely\nthat the MLP layer has the cancellations that to make N trans nonsingular.\nLemma 4.6 (Nonsingularity of N trans). Suppose for every non-identity permutation \u03c4 \u2208 Sr \\ {id},\nX\ni\u2208[r]\nKattn(sub(zi, s), sub(zi, s\u2032)) \u0338=\nX\ni\u2208[r]\nKattn(sub(zi, s), sub(z\u03c4(i), s\u2032)) ,\n(9)\nwhere s, s\u2032 are the substitution maps in the definition of N trans in (7). Let the MLP layer\u2019s activation function be\n\u03d5(t) = cos(b1t + b2). Then for almost any choice of b1, b2 (except for a Lebesgue-measure-zero set), the matrix\nN trans is nonsingular.\nThis is proved in Appendix E, by evaluating a Gaussian integral and showing N trans has Vandermonde structure.\nAlthough we use the cosine activation function, we conjecture that this result holds for most non-polynomial activation\nfunctions. Next, we prove the condition on N attn.\n8\nLemma 4.7 (Non-degeneracy of Kattn). The condition (9) holds for Lebesgue-almost any \u03b2, \u03b3.\nThe proof is in Appendix F. First, we prove the analyticity of the kernel Kattn in terms of the hyperparameters \u03b2\nand \u03b3. Because of the identity theorem for analytic functions, it suffices to show at least one choice of hyperparameters\n\u03b2 and \u03b3 satisfies (9) for all non-identity permutations \u03c4. Since Kattn does not have a closed-form solution, we find\nsuch a choice of \u03b2 and \u03b3 by analyzing the Taylor-series expansion of Kattn around \u03b2 = 0 and \u03b3 = 0 up to order-10\nderivatives.\n4.3\nImproving transformer data-efficiency with WKW T\nQ + aI parametrization\nCan we use these insights to improve transformers\u2019 data-efficiency in template tasks? In the proof, the nonsingularity\nof N in Lemma 4.5 drives the model\u2019s generalization on unseen symbols. This suggests that an approach to improve\ndata-efficiency is to make N better-conditioned by modifying the transformer parametrization. We consider here the\nsimplest task, with templates \u201c\u03b1\u03b1\u201d and \u201c\u03b1\u03b2\u201d labeled with +1 and \u22121, respectively. For tokens A, B, C, D \u2208 X, the\nmatrix N is\nN =\n\u0014\nK(AA, BB)\nK(AA, BC)\nK(BC, AA)\nK(AB, CD)\n\u0015\nIf K is an inner-product kernel, K(x, x\u2032) = \u03ba(P\ni\u2208[k] 1(xi = x\u2032\ni)), as from an MLP, then K(AA, BB) = K(AA, BC) =\nK(BC, AA) = K(AB, CD) = \u03ba(0), so N is singular and generalization is not achieved. Intuitively, every sample\nxi has approximately the same \u201csimilarity profile to other data\u201d \u02c6Ki,: = [K(xi, x1), . . . , K(xi, xn)], so the kernel\nmethod cannot identify the samples that come from the same template as xtest. In contrast, the transformer kernel\n(3) succeeds by using information about the incidence matrix XXT , which differs between templates, and does not\ndepend on the symbol substitution. We thus propose to emphasize the incidence matrix XXT by reparametrizing\neach head to W KW T\nQ + aI, where a is a trainable parameter. This adds a scaling of XXT in the attention, and can\nempirically improve data efficiency by an order of magnitude on several template tasks (see Figures 1 and 2, as well\nas additional experiments in Appendix B).\n5\nAnalysis for template tasks with symbolic labels\nIn this section we switch to a next-token prediction setting with the cross-entropy loss. The symbolic-label variant\nof template tasks is analogous to the real-label template tasks studied so far, except that the output label is a token\nas in the example of Figure 2; formal definition is in Appendix J. The simplest symbolic-label task is template \u201c\u03b1\u201d\nlabeled by \u201c\u03b1\u201d. An example train set is {(A, A), (B, B), (C, C)}, where A, B, C \u2208 X are tokens, and then we test\nwith (xtest, ytest) = (D, D) which is not in the train set. Thus, this task captures the ability of a model to learn how to\ncopy a symbol, which is an important skill for LLMs that solve problems with multi-stage intermediate computations\nand must copy these to later parts of a solution [CIS21].\nFor simplicity, we consider the architecture with just the attention layer, and we tie the embedding and unembed-\nding weights as in practice [Bro+20]:\nfattn(X; \u03b8) = W Ezattn \u2208 Rm.\n(10)\nDespite the simplicity of the task, fattn does not generalize on unseen symbols when trained, as we take the\nembedding dimension large. Our evidence is from analyzing the early time of training. Define the train loss and test\nloss as follows, where \u2113 is the cross-entropy loss and xtest is a token that does not appear in the training data,\nLtrain(\u03b8) = 1\nn\nn\nX\ni=1\n\u2113(fattn(xi; \u03b8), yi)\nand\nLtest(\u03b8) = \u2113(fattn(xtest), ytest) .\nWe train with gradient flow, and show that the generalization loss on unseen symbols does not decrease for infinite-\nwidth transformers on the symbolic-label \u201ccopying\u201d task where the template is \u201c\u03b1\u201d and is labeled by \u201c\u03b1\u201d.\nTheorem 5.1 (Failure of transformers at copying). For any learning rates such that \u2212 \u2202Ltrain\n\u2202t\n|t=0= O(1), we must\nhave that \u2202Ltest\n\u2202t\n|t=0\u2192 0 as demb \u2192 \u221e.\n9\n(a) Vanilla transformer\n(b) Transformer with W V W T\nO + bI\n101\n102\n103\nNumber of training samples\n0.0\n0.5\n1.0\n1.5\nTest error\ndim=64\ndim=128\ndim=256\ndim=512\ndim=1024\ndim=2048\ndim=4096\n101\n102\n103\nNumber of training samples\n0.0\n0.5\n1.0\n1.5\nTest error\ndim=64\ndim=128\ndim=256\ndim=512\ndim=1024\ndim=2048\ndim=4096\n101\n102\n103\nNumber of training samples\n0.0\n0.5\n1.0\n1.5\nTest error\nFigure 5: (a) Inverse scaling law: Transformers fail on the copying task as embedding dimension demb grows (Theo-\nrem 5.1) (b) Success when reparametrizing W V W T\nO as W V W T\nO + bI (Theorem 5.2). Details in Appendix A.\nDataset\nGPT-2\nGPT-2 + trainable identity scalings\nWikitext2\n64.00\n60.46\nWikitext103\n16.83\n16.40\nFigure 6: Perplexity of GPT-2 trained with Adam learning rate 3e-4 for 20 epochs on Wikitext (smaller is better).\nGPT-2 has 117M parameters, and we add an extra 288 parameters (2 per head).\nBecause the template has length k = 1, the architecture simplifies to\nfattn(X; \u03b8) = W E(\nX\nh\u2208[H]\nW T\nO,hW V,h)(W T\nEXT + \u03b3P T ) .\n(11)\nThe intuition is that comes from examining (11), and noting that at early times the evolution of the weights W T\nO,hW V,h\nwill roughly lie in the span of {W T\nEexieT\nxiW E}i\u2208[n], which as the embedding dimension becomes large will be ap-\nproximately orthogonal to the direction W T\nEextesteT\nxtestW E that would lower the test loss. However, this suggests\nthe following modification to transformers allows them to copy symbols never seen at training:\nTheorem 5.2 (Adding one parameter allows copying). After reparametrizing the attention (10) so that in each head\nW T\nO,hW V,h is replaced by W T\nO,hW V,h + bhI where bh is a trainable parameter, there are learning rates such that\n\u2212 \u2202Ltrain\n\u2202t\n|t=0= O(1) and \u2212 \u2202Ltest\n\u2202t\n|t=0= \u2126(1) as demb \u2192 \u221e.\nFigures 2 and 5 illustrate the benefit of this additional per-head parameter on the copying task. Note that our\nproposed reparametrization is not equivalent to adding a trainable skip connection as in ResNet [He+16]. Instead, the\naddition of bhI encodes an attention-modulated skip connection that allows copying tokens between the transformer\u2019s\nstreams.\n6\nExperiments\nFigures 1 and 2 (and additional experiments in Appendix B) show that our reparametrizations can give a significant\ndata-efficiency benefit on template tasks. Figure 6 shows they can also give improvements on real data. In Figure 7,\nwe find that fine-tuning a pretrained model helps with a template task. This might be explained by several heads\nof the pretrained model with diagonals stronger from other weights (originally observed in [TK23]). These learned\ndiagonals resemble our proposed transformer modifications and so might be driving the data-efficiency of fine-tuning\na pretrained model. Appendix B provides extensive experiments on the effect of hyperparameters, inductive biases of\ndifferent models, and varying levels of difficulty of the template task.\n7\nDiscussion\nWe have shown that transformers are a universal architecture for template tasks in the regression setting: when trained\nwith SGD, they learn to reason relationally once there is enough training data. However, transformers are far from\noptimal, since in practice they still require large amounts of data to learn basic tasks, and in the next-token-prediction\nsetting they fail altogether at copying unseen symbols. Thus, we have proposed architectural modifications that are\n10\nEffect of pretraining\nWKW T\nQ Head 12, Layer 5\nWV W T\nO Head 12, Layer 11\n101\n102\n103\n104\nNumber of training samples\n0.0\n0.5\n1.0\n1.5\n2.0\nTest loss\nGPT-2 from scratch\nGPT-2 pretrained\n0\n25\n50\n75\n0\n20\n40\n60\n80\n0\n25\n50\n75\n0\n20\n40\n60\n80\nFigure 7: Left: Pretrained versus randomly-initialized GPT-2 test loss when fine-tuned on \u03b1\u03b2\u03b1 vs. \u03b1\u03b2\u03b2 template task.\nRight: example GPT-2 pretrained heads that have learned diagonals (zoomed in to 100x100 top-left corner).\na step towards promoting an inductive bias towards logical reasoning in LLMs. In the future, it seems promising\nto explore architectural modifications based off of analyses of other reasoning tasks (for example, reasoning with\nsyllogisms, reasoning by symmetry, and compositional reasoning). Apart from architectural modifications, it may also\nbe fruitful to study data augmentation approaches (e.g., concatenating the tensorization XXT to the input, so as to\nencourage use of relational information).\nReferences\n[AB22]\nEmmanuel Abbe and Enric Boix-Adsera. \u201cOn the non-universality of deep learning: quantifying the cost\nof symmetry\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 17188\u201317201.\n[Abb+22]\nEmmanuel Abbe et al. \u201cAn initial alignment between neural network and target is needed for gradient\ndescent to learn\u201d. In: International Conference on Machine Learning. PMLR. 2022, pp. 33\u201352.\n[Abb+23]\nEmmanuel Abbe et al. \u201cGeneralization on the unseen, logic reasoning and degree curriculum\u201d. In: arXiv\npreprint arXiv:2301.13105 (2023).\n[Ahn+23]\nKwangjun Ahn et al. \u201cTransformers learn to implement preconditioned gradient descent for in-context\nlearning\u201d. In: arXiv preprint arXiv:2306.00297 (2023).\n[AL23]\nZeyuan Allen-Zhu and Yuanzhi Li. \u201cPhysics of Language Models: Part 1, Context-Free Grammar\u201d. In:\narXiv preprint arXiv:2305.13673 (2023).\n[Alt+23]\nAwni Altabaa et al. \u201cAbstractors: Transformer Modules for Symbolic Message Passing and Relational\nReasoning\u201d. In: arXiv preprint arXiv:2304.00195 (2023).\n[Bro+20]\nTom Brown et al. \u201cLanguage models are few-shot learners\u201d. In: Advances in neural information process-\ning systems 33 (2020), pp. 1877\u20131901.\n[CB18]\nLenaic Chizat and Francis Bach. \u201cOn the global convergence of gradient descent for over-parameterized\nmodels using optimal transport\u201d. In: Advances in neural information processing systems 31 (2018).\n[CIS21]\nR\u00f3bert Csord\u00e1s, Kazuki Irie, and J\u00fcrgen Schmidhuber. \u201cThe neural data router: Adaptive control flow in\ntransformers improves systematic generalization\u201d. In: arXiv preprint arXiv:2110.07732 (2021).\n[COB19]\nLenaic Chizat, Edouard Oyallon, and Francis Bach. \u201cOn lazy training in differentiable programming\u201d.\nIn: Advances in neural information processing systems 32 (2019).\n[Cyb89]\nGeorge Cybenko. \u201cApproximation by superpositions of a sigmoidal function\u201d. In: Mathematics of con-\ntrol, signals and systems 2.4 (1989), pp. 303\u2013314.\n[Fod75]\nJerry A Fodor. The language of thought. Vol. 5. Harvard university press, 1975.\n[Gar+22]\nShivam Garg et al. \u201cWhat can transformers learn in-context? a case study of simple function classes\u201d. In:\nAdvances in Neural Information Processing Systems 35 (2022), pp. 30583\u201330598.\n[He+16]\nKaiming He et al. \u201cDeep residual learning for image recognition\u201d. In: Proceedings of the IEEE confer-\nence on computer vision and pattern recognition. 2016, pp. 770\u2013778.\n[Hol12]\nKeith J Holyoak. \u201cAnalogy and relational reasoning\u201d. In: The Oxford handbook of thinking and reasoning\n(2012), pp. 234\u2013259.\n11\n[JGH18]\nArthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. \u201cNeural tangent kernel: Convergence and general-\nization in neural networks\u201d. In: Advances in neural information processing systems 31 (2018).\n[Kap+20]\nJared Kaplan et al. \u201cScaling laws for neural language models\u201d. In: arXiv preprint arXiv:2001.08361\n(2020).\n[Ker+22]\nGiancarlo Kerg et al. \u201cOn neural architecture inductive biases for relational tasks\u201d. In: arXiv preprint\narXiv:2206.05056 (2022).\n[KP02]\nSteven G Krantz and Harold R Parks. A primer of real analytic functions. Springer Science & Business\nMedia, 2002.\n[Kri+13]\nTrenton Kriete et al. \u201cIndirection and symbol-like processing in the prefrontal cortex and basal ganglia\u201d.\nIn: Proceedings of the National Academy of Sciences 110.41 (2013), pp. 16390\u201316395.\n[KRS18]\nJunkyung Kim, Matthew Ricci, and Thomas Serre. \u201cNot-So-CLEVR: learning same\u2013different relations\nstrains feedforward neural networks\u201d. In: Interface focus 8.4 (2018), p. 20180011.\n[LZA20]\nZhiyuan Li, Yi Zhang, and Sanjeev Arora. \u201cWhy are convolutional nets more sample-efficient than fully-\nconnected nets?\u201d In: arXiv preprint arXiv:2010.08515 (2020).\n[Mar98]\nGary F Marcus. \u201cRethinking eliminative connectionism\u201d. In: Cognitive psychology 37.3 (1998), pp. 243\u2013\n282.\n[McK+23]\nIan R McKenzie et al. \u201cInverse Scaling: When Bigger Isn\u2019t Better\u201d. In: arXiv preprint arXiv:2306.09479\n(2023).\n[Mit20]\nBoris Samuilovich Mityagin. \u201cThe zero set of a real analytic function\u201d. In: Mathematical Notes 107.3-4\n(2020), pp. 529\u2013530.\n[MK16]\nAntone Martinho III and Alex Kacelnik. \u201cDucklings imprint on the relational concept of \u201csame or differ-\nent\u201d\u201d. In: Science 353.6296 (2016), pp. 286\u2013288.\n[MMM19]\nSong Mei, Theodor Misiakiewicz, and Andrea Montanari. \u201cMean-field theory of two-layers neural net-\nworks: dimension-free bounds and kernel limit\u201d. In: Conference on Learning Theory. PMLR. 2019,\npp. 2388\u20132464.\n[MMN18]\nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. \u201cA mean field view of the landscape of two-layer\nneural networks\u201d. In: Proceedings of the National Academy of Sciences 115.33 (2018), E7665\u2013E7671.\n[Nan+23]\nNeel Nanda et al. \u201cProgress measures for grokking via mechanistic interpretability\u201d. In: arXiv preprint\narXiv:2301.05217 (2023).\n[New80]\nAllen Newell. \u201cPhysical symbol systems\u201d. In: Cognitive science 4.2 (1980), pp. 135\u2013183.\n[Ng04]\nAndrew Y Ng. \u201cFeature selection, L1 vs. L2 regularization, and rotational invariance\u201d. In: Proceedings\nof the twenty-first international conference on Machine learning. 2004, p. 78.\n[NSS59]\nAllen Newell, John C Shaw, and Herbert A Simon. \u201cReport on a general problem solving program\u201d. In:\nIFIP congress. Vol. 256. Pittsburgh, PA. 1959, p. 64.\n[PPW18]\nRasmus Palm, Ulrich Paquet, and Ole Winther. \u201cRecurrent relational networks\u201d. In: Advances in neural\ninformation processing systems 31 (2018).\n[Rav38]\nJohn C Raven. \u201cProgressive matrices: A perceptual test of intelligence\u201d. In: London: HK Lewis 19 (1938),\np. 20.\n[RV18]\nGrant Rotskoff and Eric Vanden-Eijnden. \u201cParameters as interacting particles: long time convergence\nand asymptotic error scaling of neural networks\u201d. In: Advances in neural information processing systems\n31 (2018).\n[San+17]\nAdam Santoro et al. \u201cA simple neural network module for relational reasoning\u201d. In: Advances in neural\ninformation processing systems 30 (2017).\n[San+18]\nAdam Santoro et al. \u201cRelational recurrent neural networks\u201d. In: Advances in neural information process-\ning systems 31 (2018).\n[Sha+20]\nMurray Shanahan et al. \u201cAn explicitly relational neural network architecture\u201d. In: International Confer-\nence on Machine Learning. PMLR. 2020, pp. 8593\u20138603.\n12\n[Sha18]\nOhad Shamir. \u201cDistribution-specific hardness of learning neural networks\u201d. In: The Journal of Machine\nLearning Research 19.1 (2018), pp. 1135\u20131163.\n[SKM84]\nRichard E Snow, Patrick C Kyllonen, and Brachia Marshalek. \u201cThe topography of ability and learning\ncorrelations\u201d. In: Advances in the psychology of human intelligence (1984), pp. 47\u2013103.\n[SS22]\nJustin Sirignano and Konstantinos Spiliopoulos. \u201cMean field analysis of deep neural networks\u201d. In: Math-\nematics of Operations Research 47.1 (2022), pp. 120\u2013152.\n[TK23]\nAsher Trockman and J Zico Kolter. \u201cMimetic Initialization of Self-Attention Layers\u201d. In: arXiv preprint\narXiv:2305.09828 (2023).\n[Vas+17]\nAshish Vaswani et al. \u201cAttention is all you need\u201d. In: Advances in neural information processing systems\n30 (2017).\n[Web+20]\nTaylor Webb et al. \u201cLearning representations that support extrapolation\u201d. In: International conference on\nmachine learning. PMLR. 2020, pp. 10136\u201310146.\n[Web+23]\nTaylor W Webb et al. \u201cThe Relational Bottleneck as an Inductive Bias for Efficient Abstraction\u201d. In:\narXiv preprint arXiv:2309.06629 (2023).\n[Wel13]\nMax Welling. \u201cKernel ridge regression\u201d. In: Max Welling\u2019s classnotes in machine learning (2013), pp. 1\u2013\n3.\n[WHL23]\nTaylor Webb, Keith J Holyoak, and Hongjing Lu. \u201cEmergent analogical reasoning in large language\nmodels\u201d. In: Nature Human Behaviour (2023), pp. 1\u201316.\n[WSC20]\nTaylor W Webb, Ishan Sinha, and Jonathan D Cohen. \u201cEmergent symbols through binding in external\nmemory\u201d. In: arXiv preprint arXiv:2012.14601 (2020).\n[YH21]\nGreg Yang and Edward J Hu. \u201cTensor programs iv: Feature learning in infinite-width neural networks\u201d.\nIn: International Conference on Machine Learning. PMLR. 2021, pp. 11727\u201311737.\n[Yua+23]\nZheng Yuan et al. \u201cScaling relationship on learning mathematical reasoning with large language models\u201d.\nIn: arXiv preprint arXiv:2308.01825 (2023).\n[ZFB23]\nRuiqi Zhang, Spencer Frei, and Peter L Bartlett. \u201cTrained Transformers Learn Linear Models In-Context\u201d.\nIn: arXiv preprint arXiv:2306.09927 (2023).\n[Zha+21a]\nChiyuan Zhang et al. \u201cPointer value retrieval: A new benchmark for understanding the limits of neural\nnetwork generalization\u201d. In: arXiv preprint arXiv:2107.12580 (2021).\n[Zha+21b]\nChiyuan Zhang et al. \u201cUnderstanding deep learning (still) requires rethinking generalization\u201d. In: Com-\nmunications of the ACM 64.3 (2021), pp. 107\u2013115.\n[Zha+23]\nHaoyu Zhao et al. \u201cDo Transformers Parse while Predicting the Masked Word?\u201d In: arXiv preprint\narXiv:2303.08117 (2023).\n13\nContents\n1\nIntroduction\n1\n1.1\nOur contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.1.1\nTemplate tasks: a framework for measuring reasoning with abstract symbols\n. . . . . . . . .\n2\n1.1.2\nAnalytical results for template tasks with real labels (regression setting) . . . . . . . . . . . .\n3\n1.1.3\nAnalytical results for template tasks with symbolic labels (next-token-prediction setting) . . .\n4\n1.1.4\nExperimental validation and exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nRelated literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2\nTransformer definition\n5\n3\nTemplate tasks\n5\n4\nAnalysis for template tasks with real labels\n6\n4.1\nTransformer random features kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.2\nTransformers generalize on unseen symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4.3\nImproving transformer data-efficiency with WKW T\nQ + aI parametrization . . . . . . . . . . . . . . .\n9\n5\nAnalysis for template tasks with symbolic labels\n9\n6\nExperiments\n10\n7\nDiscussion\n10\nA Details for figures in main text\n15\nB\nAdditional experiments\n15\nB.1\nEffect of transformer hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nB.2\nEffect of complexity of task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nB.3\nEffect of inductive bias of model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nC Proof of Theorem 4.4\n24\nC.1\nPart 1. General sufficient condition for good test loss . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.2\nPart 2. Analyzing the transformer random features kernel . . . . . . . . . . . . . . . . . . . . . . . .\n25\nC.3\nConcluding the proof of Theorem 4.4\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nD Sufficient condition for kernel method to generalize on unseen symbols (Proof of Lemma C.2)\n25\nD.1\nDeferred proofs of claims . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nE\nNonsingularity of random features after MLP layer (Proof of Lemma 4.6)\n30\nF\nAnalysis of attention layer features (Proof of Lemma 4.7)\n31\nF.1\nLow-order derivatives of attention kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nF.2\nSimplifying terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nF.2.1\nAssuming [1T X]R = [1T Y ]R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nF.2.2\nAssuming [X][k]\u00d7R = [Y ][k]\u00d7R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nF.2.3\nAssuming 1T XXT 1 = 1T Y Y T 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nF.2.4\nAssuming 1T XXT = 1T Y Y T\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nF.3\nProof of Lemma F.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nG Analyticity of attention kernel (technical result)\n36\nG.1\nTechnical lemmas for quantifying power series convergence\n. . . . . . . . . . . . . . . . . . . . . .\n36\nG.2\nApplication of technical lemmas to attention kernel . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n14\nH Derivation of transformer kernel\n39\nH.1\nTransformer architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nH.2\nRandom features kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nH.3\nInformal derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nI\nMLPs fail to generalize on unseen symbols\n41\nJ\nDeferred details for symbolic-label template tasks\n43\nJ.1\nDefinition of symbolic-label template tasks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\nJ.2\nFailure of transformers to copy and modification that succeeds . . . . . . . . . . . . . . . . . . . . .\n44\nA\nDetails for figures in main text\nCode is available at https://github.com/eboix/relational-reasoning/.\nTransformer performance\nIn Figure 1, The architecture is a 2-layer transformer with 16 heads per layer, embedding\ndimension 128, head dimension 64, MLP dimension 256, trained with Adam with learning rate 1e-3 and batch-size\n1024. The n training samples are chosen by picking the variable names at random from an alphabet of n tokens. The\ntest set is the same two programs but with disjoint variable names. The reported error bars are on average over 5 trials.\nThe learning rate for each curve is picked as the one achieving best generalization in {10\u22125, 10\u22124, 10\u22123, 10\u22122}. In\nFigure 2, the setting is the same except that the transformer is 4-layer transformer and has embedding dimension 512.\nIn Figure 5 the same hyperparameters as in Figure 1 are used. In order to measure the generalization performance\nof the learned model on unseen symbols, we evaluate it on a test set and a validation set which each consist of 100\nsamples drawn in the same way as the training dataset, but each using a disjoint alphabet of size 100. Therefore, there\nis no overlap in the support of the train, test, and validation distributions. We use the validation loss to select the best\nepoch of training out of 1000 epochs. We report the test loss on this saved model.\nPsychometric tasks\nWe describe how the tasks in Figure 3 fall under the template framework.\n\u2022 Distribution of 3. To input this task into a language model, a token is used to represent each symbol. The\nexample in the figure matches template \u201c\u03b1\u03b2\u03b3 \u03b3\u03b1\u25a1 \u03f5\u03b1\u03b2\u03b3\u201d, with label +2. There are other templates for this\ntask, corresponding to different arrangements of the objects, such as \u201c\u03b1\u03b2\u03b3 \u03b2\u03b3\u25a1 \u03b1\u03b3\u03f5\u03b2\u201d with label +1, and\n\u201c\u03b1\u03b2\u03b3 \u03b3\u03b2\u25a1 \u03f5\u03b2\u03b1\u03b3\u201d with label +3. In total there are 144 templates, since the first 3 elements of the template are\nalways \u03b1\u03b2\u03b3, and then there are 6 choices for the permutation in the next row, and finally 24 choices for the\npermutation in the final row.\n\u2022 Relational match-to-sample. Again, a token is used to represent each symbol. The example in the figure matches\n\u201c\u03b1\u03b2\u03b2 \u03b3\u03b4\u03b4 \u03f5\u03f5\u03c4\u201d with label +1. A simple combinatorial calculation gives a total of 40 templates (5 possible\npatterns in the first row, times 2 choices for whether the first option or the second option is correct, times 4\nchoices for the pattern of alternative option).\n\u2022 Raven\u2019s progressive matrices. For each of the dimensions of shape, number, and color, we have a \u201cdistribution\nof 3\u201d task with a symbolic label. For example, for the shapes in the figure, the task is \u201c\u03b1\u03b2\u03b3 \u03b2\u03b3\u03b1 \u03b3\u03b2?\u201d with label\n\u03b1. Since another possibility is for each row to be constant (as in, e.g., the case of numbers), another possible\ntemplate is \u201c\u03b1\u03b1\u03b1 \u03b2\u03b2\u03b2 \u03b3\u03b3?\u201d with label \u03b3, and so there is a total of 36+1 = 37 possible templates per dimension.\nThis discussion assumes that the only patterns in the progressive matrices are distribution of 3, and constant. If\nprogressions are also allowed as in [WHL23], these can be incorporated by adding corresponding templates.\nB\nAdditional experiments\nWe report extensive additional experiments probing the template task framework. In each of these, the training dataset\nconsists of n random training samples. Each sample is drawn according to a template distribution. The following are\ntemplate tasks on which we test.\n15\n\u2022 \u03b1\u03b2\u03b1 vs. \u03b1\u03b2\u03b2 task. Uniform on two templates \u03b1\u03b2\u03b1 and \u03b1\u03b2\u03b2 with labels 1, -1 respectively and \u03b1 and \u03b2 are\nwildcards.\n\u2022 \u03b1\u03b2\u03b1\u03b2 vs. \u03b1\u03b1\u03b2\u03b2 task. Same as above, except with templates \u03b1\u03b2\u03b1\u03b2 and \u03b1\u03b1\u03b2\u03b2.\n\u2022 Length-k majority task. Uniform on 2k\u22121 templates \u03b1 \u00d7 {\u03b1, \u03b2}k\u22121 where \u03b1 and \u03b2 are wildcards. A template z\nhas label 1 if its first token occurs in the majority of the rest of the string, and -1 otherwise. Namely, f\u2217(z) =\n(\n1,\n|{i : z1 = zi}| > (k + 1)/2\n\u22121,\notherwise\n.\n\u2022 Random template task. A certain number r of templates are drawn uniformly from (W \u222a X)k, conditioned on\nbeing pairwise distinct. The task is the uniform distribution over these r templates, with random Gaussian labels\ncentered and scaled so that the trivial MSE is 1.\nFor any of these tasks, we generate n training samples as follows. We substitute the wildcards for regular tokens using\na randomly chosen injective function s : W \u2192 X where X is an alphabet of size n (which is the same size as the\nnumber of samples). For example, if a given sample is generated from template \u03b1\u03b2\u03b1 with substitution map s mapping\ns(A) = 12, s(B) = 5, then the sample will be [12, 5, 12]. Error bars are over 5 trials, unless otherwise noted.\nB.1\nEffect of transformer hyperparameters\nWe test an out-of-the-box transformer architecture on the \u03b1\u03b2\u03b1 vs. \u03b1\u03b2\u03b2 task, varying some of the hyperparameters\nof the transformer to isolate their effect while keeping all other hyperparameters fixed. The base hyperparameters\nare depth 2, embedding dimension 128, head dimension 64, number of heads per layer 16, trained with Adam with\nminibatch size 1024 for 1000 epochs. Our experiments are as follows:\n\u2022 Learning rate and n. In Figure 8 we vary the learning rate and n.\n\u2022 Learning rate and depth. In Figure 9 and Figure 10, we vary the learning rate and the depth, for n = 512 and\nn = 1024, respectively.\n\u2022 Learning rate and number of heads. In Figure 11 and 12, we vary the learning rate and number of heads, for\nn = 512 and n = 1024, respectively.\n\u2022 Learning rate and embedding dimension. In Figure 13 we vary the learning rate and embedding dimension for\nn = 1024.\n\u2022 Learning rate and batch size. In Figure 14, we vary the learning rate and batch-size for n = 512. In Figure 15\nwe vary the batch-size and n for learning rate 0.001.\nB.2\nEffect of complexity of task\nWe test an out-of-the-box transformer architecture with depth 2, embedding dimension 128, head dimension 64, num-\nber of heads 16, trained with Adam with batch-size 1024 for 1000 epochs, on various template tasks.\n\u2022 Comparing difficulty of various tasks. Figure 16 we plot the performance on various simple tasks.\n\u2022 Random tasks. In Figures 17, 18, 19, and 20, we test on random template tasks, and investigate the effects of\ntemplate length, wildcard alphabet size, regular token alphabet size, number of templates.\nB.3\nEffect of inductive bias of model\nWe provide experiments probing the effect of the inductive bias of the model:\n\u2022 Different architectures. In Figure 21, we plot the test loss for different architectures on the \u03b1\u03b2\u03b1 vs. \u03b1\u03b2\u03b2 tem-\nplate task, including transformers with trainable identity perturbations to WQW T\nK, to WV W T\nO , to both WQW T\nK\nand WV W T\nO , or to neither. Figure 22 illustrates on the beneficial effect of the transformer modification for the\nmajority task with different lengths, lowering the amount of data needed by an order of magnitude.\n16\n\u2022 Size of model. In Figure 23 we compare the test loss of fine-tuning small, medium and large pretrained GPT-2\nnetworks on the \u03b1\u03b2\u03b1 vs. \u03b1\u03b2\u03b2 template task.\n\u2022 MLP with XXT data augmentation vs. transformer. In Figure 24, we compare the test loss of a transformer\nwith the test loss of an MLP where the input data has been augmented by concatenating vec(XXT ), which is a\ndata augmentation that improves performance under the NTK criterion similarly to the discussion in Section 4.3\nand the discussion section.\n10\n4\n10\n3\n10\n2\n10\n1\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss vs. learning rate and number of samples\nn 16\nn 32\nn 64\nn 128\nn 256\nn 512\nn 1024\nn 2048\nn 4096\nn 8192\n10\n4\n10\n3\n10\n2\n10\n1\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss vs. learning rate and number of samples\nn 16\nn 32\nn 64\nn 128\nn 256\nn 512\nn 1024\nn 2048\nn 4096\nn 8192\nFigure 8: Learning rate versus n = number of samples = training alphabet size. Taking too large or too small of a\nlearning rate can hurt generalization even when the train loss is close to zero.\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss vs. learning rate and depth, at n = 512\ndepth 1\ndepth 2\ndepth 4\ndepth 8\ndepth 16\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss vs. learning rate and depth, at n = 512\ndepth 1\ndepth 2\ndepth 4\ndepth 8\ndepth 16\nFigure 9: Learning rate vs. depth at n = 512. No clear relationship between depth and generalization. Too large or\ntoo small of a learning rate can hurt generalization.\n17\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss vs. learning rate and depth, at n = 1024\ndepth 1\ndepth 2\ndepth 4\ndepth 8\ndepth 16\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss vs. learning rate and depth, at n = 1024\ndepth 1\ndepth 2\ndepth 4\ndepth 8\ndepth 16\nFigure 10: Learning rate vs. depth at n = 1024. Unlike n = 512 case, in previous figure, larger depth typically\nperforms better.\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss vs. learning rate and number of heads, at n = 512\nnumber of heads 1\nnumber of heads 4\nnumber of heads 16\nnumber of heads 64\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss vs. learning rate and number of heads, at n = 512\nnumber of heads 1\nnumber of heads 4\nnumber of heads 16\nnumber of heads 64\nFigure 11: Learning rate vs. number of heads per layer at n = 512. More heads are better than one head.\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss vs. learning rate and number of heads, at n = 1024\nnumber of heads 1\nnumber of heads 4\nnumber of heads 16\nnumber of heads 64\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss vs. learning rate and number of heads, at n = 1024\nnumber of heads 1\nnumber of heads 4\nnumber of heads 16\nnumber of heads 64\nFigure 12: Learning rate vs. number of heads at n = 1024. More heads are better.\n18\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss vs. learning rate and embedding dimension, at n = 1024\nembed dim 32\nembed dim 128\nembed dim 512\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss vs. learning rate and embedding dimension, at n = 1024\nembed dim 32\nembed dim 128\nembed dim 512\nFigure 13: Learning rate vs. embedding dimension at n = 1024. Smaller embedding dimension is generally better.\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss vs. learning rate and batch size, at n = 512\nbatch_size 32\nbatch_size 128\nbatch_size 512\n10\n4\n10\n3\n10\n2\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss vs. learning rate and batch size, at n = 512\nbatch_size 32\nbatch_size 128\nbatch_size 512\nFigure 14: Learning rate vs. batch-size at n = 512. Smaller batch size is better.\n101\n102\n103\nbatch size\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss vs. batch size and number of samples\nn 16\nn 32\nn 64\nn 128\nn 256\nn 512\nn 1024\n101\n102\n103\nlearning rate\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss vs. learning rate and number of samples, batch size 1024\nn 16\nn 32\nn 64\nn 128\nn 256\nn 512\nn 1024\nFigure 15: Batch size vs. n = number of training samples = training alphabet size. Smaller batch size is generally\nbetter, which is most visible at n = 512.\n19\n101\n102\n103\n104\nn = train alphabet size = # samples\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss on various tasks\nABA vs. ABB, lr 0.001\nAABB vs. ABAB, lr 0.001\nlength-2 majority, lr 0.001\nlength-4 majority, lr 0.001\nlength-6 majority, lr 0.001\nlength-8 majority, lr 0.001\n101\n102\n103\n104\nn = train alphabet size = # samples\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss on various tasks\nABA vs. ABB, lr 0.001\nAABB vs. ABAB, lr 0.001\nlength-2 majority, lr 0.001\nlength-4 majority, lr 0.001\nlength-6 majority, lr 0.001\nlength-8 majority, lr 0.001\nFigure 16: Test and train loss of transformer for various tasks. The \u03b1\u03b2\u03b1 vs. \u03b1\u03b2\u03b2 task consists of two templates \u03b1\u03b2\u03b1\nand \u03b1\u03b2\u03b2 with labels +1, -1. The \u03b1\u03b1\u03b2\u03b2 vs. \u03b1\u03b2\u03b1\u03b2 task has templates +1, -1. For each k, the length-k majority task\nconsists of all templates in {\u03b1} \u00d7 {\u03b1, \u03b2}k\u22121, where each template has label 1 if \u03b1 occurs more times in the last k \u2212 1\nentries, and label +1 if \u03b1 occurs fewer times in the last k \u2212 1 entries. The trivial model that outputs 0 always will\nachieve test loss of 1.\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\ntemplate length\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss on random tasks with different lengths\nn 8\nn 32\nn 128\nn 512\nn 2048\nn 8192\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\ntemplate length\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss on random tasks with different lengths\nn 8\nn 32\nn 128\nn 512\nn 2048\nn 8192\nFigure 17: Performance on tasks corresponding of two, distinct random templates with two wildcards \u03b1, \u03b2, and with\nlabels 1, \u22121, respectively. Performance degrades as the template length increases.\n20\n2\n3\n4\n5\n6\n7\nwildcard alphabet size\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss on random tasks with different wildcard alphabet sizes\nn 8\nn 32\nn 128\nn 256\nn 512\nn 1024\nn 2048\nn 8192\n2\n3\n4\n5\n6\n7\nwildcard alphabet size\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss on random tasks with different wildcard alphabet sizes\nn 8\nn 32\nn 128\nn 256\nn 512\nn 1024\nn 2048\nn 8192\nFigure 18: Performance on tasks corresponding of two random templates of length 5, labeled with 1, \u22121, respectively.\nEach template is sampled randomly from W5, conditioned on the two templates being distinct. We vary the wildcard\nalphabet size |W|. Performance generally degrades as the wildcard alphabet size increases.\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nregular alphabet size\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss on random tasks with different regular alphabet sizes\nn 8\nn 32\nn 128\nn 256\nn 512\nn 1024\nn 2048\nn 8192\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nregular alphabet size\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss on random tasks with different regular alphabet sizes\nn 8\nn 32\nn 128\nn 256\nn 512\nn 1024\nn 2048\nn 8192\nFigure 19: Performance on tasks corresponding of two random templates of length 5, labeled with 1, \u22121, respectively.\nEach template is sampled randomly from (W \u222a X)5, conditioned on the two templates being distinct. We keep\n|W| = 2 and vary the regular token alphabet size |X| between 0 and 2. Performance quickly improves as the regular\ntoken alphabet size increases.\n21\n2\n4\n6\n8\n10\n12\n14\nnumber of templates\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss on random tasks with different numbers of templates\nn 8\nn 32\nn 64\nn 128\nn 256\nn 512\nn 1024\nn 2048\nn 4096\nn 8192\n2\n4\n6\n8\n10\n12\n14\nnumber of templates\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nBest train loss in 1000 epochs\nTransformer train loss on random tasks with different numbers of templates\nn 8\nn 32\nn 64\nn 128\nn 256\nn 512\nn 1024\nn 2048\nn 4096\nn 8192\nFigure 20: Performance on tasks corresponding of two random templates of length 5, labeled with 1, \u22121, respectively.\nEach template is sampled randomly from (W \u222a X)5, conditioned on the two templates being distinct. We keep\n|W| = 2 and vary the regular token alphabet size |X| between 0 and 2. Performance quickly improves as the regular\ntoken alphabet size increases.\n101\n102\n103\n104\nn = number of samples = train alphabet size\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nComparison of architectures, test loss\nTransformer, lr 0.001\nTransformer + KQ identity, lr 0.001\nTransformer + VO identity, lr 0.001\nTransformer + KQ, VO identity, lr 0.001\nDepth-3 FCNet, lr 0.001\nRNN, lr 0.001\nLSTM, lr 0.001\n101\n102\n103\n104\nn = number of samples = train alphabet size\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTrain loss\nComparison of architectures, train loss\nTransformer, lr 0.001\nTransformer + KQ identity, lr 0.001\nTransformer + VO identity, lr 0.001\nTransformer + KQ, VO identity, lr 0.001\nDepth-3 FCNet, lr 0.001\nRNN, lr 0.001\nLSTM, lr 0.001\nFigure 21: Different architectures on \u03b1\u03b2\u03b1 vs. \u03b1\u03b2\u03b2 task. Transformer outperforms, especially with the reparametriza-\ntion that prioritizes identities in heads.\n22\n101\n102\n103\n104\nn = train alphabet size = #samples\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer test loss on majority tasks\nmajority_len=2, lr 0.001\nmajority_len=4, lr 0.001\nmajority_len=6, lr 0.001\nmajority_len=8, lr 0.001\n101\n102\n103\n104\nn = train alphabet size = #samples\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nTransformer + KQ, VO identity, test loss on majority tasks\nmajority_len=2, lr 0.001\nmajority_len=4, lr 0.001\nmajority_len=6, lr 0.001\nmajority_len=8, lr 0.001\nFigure 22: Comparison of test loss of architectures on length-k majority task with different k. Left: vanilla transformer\narchitecture. Right: transformer architecture plus the trainable identity scalings on each attention head\u2019s WKW T\nQ and\nWV W T\nO matrices. Notice that again the transformer reparametrization lowers the amount of data needed by at least an\norder of magnitude.\n101\n102\nn = number of train samples = train alphabet size\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nGPT-2 pretrained by size, test loss\ngpt2-small pretrained, lr 0.001\ngpt2-medium pretrained, lr 0.0001\ngpt2-large pretrained, lr 0.001\n101\n102\nn = number of train samples = train alphabet size\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTrain loss\nGPT-2 pretrained by size, train loss\ngpt2-small pretrained, lr 0.001\ngpt2-medium pretrained, lr 0.0001\ngpt2-large pretrained, lr 0.001\nFigure 23: Pretrained GPT-2 of different sizes fine-tuned on \u03b1\u03b2\u03b1 vs. \u03b1\u03b2\u03b2 task.\n101\n102\n103\n104\nn = number train samples = size train alphabet\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTest loss\nMLP vs. MLP with XXT data augmentation, test loss\nMLP, lr 0.001\nMLP + XXT data augmentation, lr 0.001\n101\n102\n103\n104\nn = number train samples = size train alphabet\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTrain loss\nMLP vs. MLP with XXT data augmentation, train loss\nMLP, lr 0.001\nMLP + XXT data augmentation, lr 0.001\nFigure 24: Test loss of MLP with XXT data augmentation, where it is concatenated to input, versus MLP without\ndata augmentation, versus transformer.\n23\nC\nProof of Theorem 4.4\nThere are two main parts to the proof. First, in Section C.1 we establish a lemma with a sufficient condition for a\nkernel method to have good test loss. Second, in Section C.2 we prove that the transformer random features kernel\nKtrans satisfies this condition for almost any \u03b2, \u03b3, b1, b2 parameters. We conclude in Section C.3.\nC.1\nPart 1. General sufficient condition for good test loss\nWe restrict ourselves to token-symmetric kernels, which are kernels whose values are unchanged if the tokens are\nrelabeled by a permutation.\nDefinition C.1 (Token-symmetric kernel). K is token-symmetric if for any permutation \u03c0 : X \u2192 X we have\nK(x, y) = K([\u03c0(x1), . . . , \u03c0(xk)], [\u03c0(y1), . . . , \u03c0(yk)]).\nToken-symmetry is a mild condition, as most network architectures used in practice (including transformers) have\ntoken-symmetric neural tangent kernels at initialization. We emphasize that token-symmetry is not sufficient for good\ntest loss since MLPs are a counterexample (see Appendix I.)\nTo state the sufficient condition for good test loss, let {z1, . . . , zr} = supp(\u00b5tmplt) be the template distribution\nsupport. Define also the set R = \u222ai\u2208[k],j\u2208[r]{zj,i} of tokens that appear in the templates. Finally, define N \u2208 Rr\u00d7r\nby\nNij = K(sub(zi, s), sub(zj, s\u2032)) ,\n(12)\nwhere s, s\u2032 : W \u2192 X are substitution maps satisfying\ns(W) \u2229 s\u2032(W) = 0\nand\ns(W) \u2229 R = s\u2032(W) \u2229 R = \u2205.\n(13)\nOne can check that because of the token-symmetry of the kernel K, the matrix N is uniquely-defined regardless of\nthe substitution maps s, s\u2032 chosen, as long as they satisfy (13).\nLemma C.2 (It suffices for N to be nonsingular). If K is a token-symmetric kernel, and N is nonsingular, then kernel\nridge regression achieves vanishing test loss.\nFormally, there are constants c, C > 0 and ridge regularization parameter \u03bb > 0 depending only on \u00b5tmplt, \u03c3,\n|W|, \u2225N \u22121\u2225 and \u2225K\u2225\u221e = maxx K(x, x), such that for any x matching a template z \u2208 supp(\u00b5tmplt) the kernel\nridge regression estimator \u02c6f in (4) with kernel K satisfies\n| \u02c6f(x) \u2212 f\u2217(z)| \u2264 C\nr\nlog(1/\u03b4)\nn\n+ C\nr1\n\u03c1 ,\nwith probability at least 1 \u2212 \u03b4 \u2212 exp(\u2212cn) over the random samples.\nThe proof is in Appendix D, but we develop an intuition here on why the nonsingularity of the matrix N is\nimportant. Let [n] = I1 \u2294 I2 \u2294 \u00b7 \u00b7 \u00b7 \u2294 In be the partition of the samples such that if i \u2208 Ij then sample (xi, yi) is\ndrawn by substituting the wildcards of template zj with substitution map si : W \u2192 X. We show that for any string\nx matching template zj, the kernel ridge regression solution (4) is approximately equal to the average of the labels of\nthe samples corresponding to template j,\nyT ( \u02c6K + \u03bbI)\u22121k(x) \u2248\n1\n|Ij|\nX\ni\u2208Ij\nyi \u2248 f\u2217(zj) .\n(14)\nIn order to see why this is true, consider the regime in which the sample diversity is very high, i.e., \u03c1 \u226b 1. Since \u03c1 is\nlarge, any particular token is highly unlikely to be substituted. This has the following implications:\n\u2022 For most sample pairs i \u0338= i\u2032 \u2208 [n], the maps si and si\u2032 have disjoint range: si(W) \u2229 s\u2032\ni(W).\n\u2022 For most samples i \u2208 [n], the substituted tokens are not in the templates: si(W) \u2229 R = \u2205.\nThese are the same conditions as in (7). So by the token-symmetry of the kernel, for most pairs of samples the\nempirical kernel matrix is given by N:\n\u02c6Ki,i\u2032 := K(xi, xi\u2032) = Nj,j\u2032 for most i \u2208 Ij, i\u2032 \u2208 Ij\u2032 .\nSo if N is nonsingular, then \u02c6K has r large eigenvalues, and n \u2212 r much smaller eigenvalues. This turns out to be\nsufficient for (8) to hold. We refer the reader to Appendix D for more details.\n24\nC.2\nPart 2. Analyzing the transformer random features kernel\nWe show that the transformer random features kernel Ktrans satisfies the sufficient condition of Lemma C.2 for van-\nishing test loss. It is clear that the kernel is token-symmetric because the definition is invariant to the permutation\nrelabelings of the tokens. The difficult part is to show that the matrix N trans := N defined with kernel K = Ktrans\nin (12) is nonsingular. The main challenge is that the transformer kernel does not have a known closed-form solution\nbecause of the softmax terms in its definition (3). Furthermore, the result is especially challenging to prove because it\nmust hold for any collection of disjoint templates z1, . . . , zr.\nWe analyze the MLP layer and the attention layer of the transformer separately. We observe that a \u201cweak\u201d condition\non Kattn can be lifted into the \u201cstrong\u201d result that N trans is nonsingular. Intuitively, as long as Kattn is not a very\ndegenerate kernel, it is very unlikely that the MLP layer has the cancellations that would be needed to make N trans\nnonsingular.\nLemma C.3 (Nonsingularity of N trans, restatement of Lemma 4.6). Suppose for every non-identity permutation \u03c4 \u2208\nSr \\ {id},\nX\ni\u2208[r]\nKattn(sub(zi, s), sub(zi, s\u2032)) \u0338=\nX\ni\u2208[r]\nKattn(sub(zi, s), sub(z\u03c4(i), s\u2032)) ,\n(15)\nwhere s, s\u2032 are the substitution maps in the definition of N trans in (13). Let the MLP layer\u2019s activation function be\n\u03d5(t) = cos(b1t+b2). Then for almost any choice of b1, b2 (except for a Lebesgue-measure-zero set), the matrix N trans\nis nonsingular.\nThis lemma is proved in Appendix E, by explicitly evaluating the Gaussian integral, which is possible since the\nactivation function is the cosine function. Although in our proof we use the cosine activation function, we conjecture\nthat this result should morally hold for sufficiently generic non-polynomial activation functions. Next, we prove the\ncondition on N attn.\nLemma C.4 (Non-degeneracy of Kattn, restatement of Lemma 4.7). The condition (15) holds for Lebesgue-almost\nany \u03b2, \u03b3.\nThe proof is in Appendix F. First, we prove the analyticity of the kernel Kattn in terms of the hyperparameters \u03b2\nand \u03b3 which control the softmax inverse temperature and the positional embeddings. Because of the identity theorem\nfor analytic functions, it suffices to show at least one choice of hyperparameters \u03b2 and \u03b3 satisfies (15) for all non-\nidentity permutations \u03c4. Since Kattn does not have a closed-form solution, we find such a choice of \u03b2 and \u03b3 by\nanalyzing the Taylor-series expansion of Kattn around \u03b2 = 0 and \u03b3 = 0 up to order-10 derivatives, which happens to\nsuffice.\nC.3\nConcluding the proof of Theorem 4.4\nBy Lemma C.2, it suffices to prove the nonsingularity of the matrix N trans defined in (12) with kernel K = Ktrans.\nLemma 4.6 gives a condition for nonsingularity that holds for almost any b1, b2. Lemma 4.7 proves this condition for\nalmost any \u03b2, \u03b3. Therefore, Theorem 4.4 follows.\nD\nSufficient condition for kernel method to generalize on unseen symbols\n(Proof of Lemma C.2)\nWe restate and prove Lemma C.2. Let K be a token-symmetric kernel as in Definition C.1. Let \u00b5tmplt be a distribution\nsupported on disjoint templates z1, . . . , zr and define R = \u222ai\u2208[r],j\u2208[k]{zi,j}. Recall the definiton of the matrix\nN \u2208 Rr\u00d7r with\nNi,i\u2032 = K(sub(zi, s), sub(zi\u2032, s\u2032)).\nfor substitution maps s : W \u2192 X, s\u2032 : W \u2192 X satisfying s(W) \u2229 s\u2032(W) = s(W) \u2229 R = s\u2032(W) \u2229 R = \u2205. Recall\nthat this is well-defined by the token-symmetry of the kernel K.\n25\nLemma D.1 (Restatement of Lemma C.2). Suppose that K is token-symmetric and N is nonsingular. Then there are\nconstants 0 < c < C and 0 < c\u2032 < C\u2032 depending only on \u00b5tmplt, \u03c3, |W|, \u2225N \u22121\u2225 and \u2225K\u2225\u221e = maxx K(x, x) such\nthat the following holds. Consider any regularization parameter \u03bb \u2208 [c\u2032n, C\u2032n], and any string x matching template\nz \u2208 supp(\u00b5tmplt). Then with probability \u2265 1 \u2212 \u03b4 \u2212 exp(\u2212cn), the kernel ridge regression estimator \u02c6f achieves good\naccuracy on x:\n| \u02c6f(x) \u2212 f\u2217(z)| \u2264 C\nr\nlog(1/\u03b4)\nn\n+ C\nr1\n\u03c1 .\nProof. Note that some proofs of helper claims are deferred to Section D.1. Let (x1, y1), . . . , (xn, yn) be the samples\nseen by the kernel method. We know from (4) that kernel ridge regression outputs the estimator\n\u02c6f(x) = yT ( \u02c6K + \u03bbI)\u22121v(x) ,\n(Kernel ridge regression)\nwhere the empirical kernel matrix \u02c6K \u2208 Rn\u00d7n is\n\u02c6Ki,j = K(xi, xj) ,\nand y = [y1, . . . , yn], and v(x) = [K(x1, x), . . . , K(xn, x)] \u2208 Rn.\nIdealized estimator when sample diversity is high\nIf the sample diversity is sufficiently high, then for most pairs\nof samples i \u0338= i\u2032 \u2208 [n], it will be the case that xi and xi\u2032 do not share any of the wildcard substitution tokens. In\nother words, the wildcard substitution map used to form xi will have disjoint range from the wildcard substitution map\nused to form xi\u2032. This means that we should expect the estimator \u02c6f to perform similarly to the following idealized\nestimator:\n\u02c6f ideal(x) = yT ( \u02c6K\nideal + \u03bbI)+videal(x) ,\n(16)\nwhere \u02c6K\nideal \u2208 Rn\u00d7n and videal(x) \u2208 Rn are idealized versions of \u02c6K and v(x), formed below. They correspond\nto the limit of infinitely-diverse samples, when all token substitution maps have disjoint range. For each j \u2208 [r], let\nIj \u2286 [n] be the indices of samples xi formed by substituting from template zj. For any i \u2208 Ij, i\u2032 \u2208 Ij\u2032, let\n\u02c6Kideal\ni,i\u2032\n= Nj,j\u2032,\n(17)\nAlso, similarly define videal(x) \u2208 Rn. For any i \u2208 Ij, let\nvideal\ni\n(x) = K(sub(zj, s), x) ,\n(18)\nwhere s : W \u2192 X is a substitution map with s(W) \u2229 R = s(W) \u2229 {xi}i\u2208[k] = \u2205, i.e., it does not overlap with the\ntemplates or with x in the tokens substituted for the wildcards. The expressions (17) and (18) are well-defined because\nof the token-symmetry of the kernel.\nIf the sample diversity is high, then we show that the idealized estimator \u02c6f ideal is indeed close to the kernel ridge\nregression solution \u02c6f.\nClaim D.2 (Idealized estimator is good approximation to true estimator). Suppose \u2225K\u2225\u221e = maxx |K(x, x)| < \u221e.\nThen there are constants C, c > 0 depending only on |W|, \u2225K\u2225\u221e, k, r such that the following holds. For any x, with\nprobability at least 1 \u2212 exp(\u2212cn),\n| \u02c6f ideal(x) \u2212 \u02c6f(x)| \u2264 C\n\u03bb + Cn\n\u03bb\u221a\u03c1 ,\nwhere \u03c1 is defined in Definition 4.2 and measures the diversity of the substitution map distribution.\n26\nAnalyzing the idealized estimator using its block structure\nThe matrix \u02c6K\nideal has block structure with blocks\nI1, . . . , Ir. Namely, it equals \u02c6Ki,i\u2032 = Nj,j\u2032 for all i \u2208 Ij, i\u2032 \u2208 Ij\u2032. Similarly, videal(x) also has block structure with\nblocks I1, . . . , Ir. This structure allows us to analyze estimator \u02c6f ideal and to prove its accuracy.\nIn order to analyze the estimator, we prove the following technical claim. The interpretation of this claim is that if\nx matches template za, then videal(x) is equal to any of the rows in \u02c6K\nideal that correspond to template a. In other\nwords, we should have ( \u02c6K\nideal)+videal(x) = 1Ia/|Ia|, which is the indicator vector for samples that come from\ntemplate a. The following technical claim is a more robust version of this observation.\nClaim D.3. Let x be a string that matches template za. Suppose that 0 < \u03bb < \u03c4 := minj\u2208[r] |Ij|/\u2225N \u22121\u2225. Then\n( \u02c6K\nideal + \u03bbI) is invertible and the following are satisfied\n\u2225( \u02c6K\nideal + \u03bbI)\u22121videal(x)\u2225 \u2264\ns\n1\n|Ia|(\n\u03c4\n\u03c4 \u2212 \u03bb) ,\nand, letting 1Ia \u2208 Rn be the indicator vector for set Ia,\n\u2225 1Ia\n|Ia| \u2212 ( \u02c6K\nideal + \u03bbI)\u22121videal(x)\u2225 \u2264\ns\n1\n|Ia|(\n\u03c4\n\u03c4 \u2212 \u03bb \u2212 1) .\nUsing the above technical claim, we can prove that \u02c6f ideal is an accurate estimator. The insight is that since\n( \u02c6K\nideal + \u03bbI)\u22121videal(x) is approximately the indicator vector 1Ia/|Ia| for samples corresponding to template a,\nthe output of the idealized estimator is the average of the labels for samples corresponding to template a.\nClaim D.4 (Idealized estimator gets vanishing test loss on unseen symbols). There are c, C > 0 depending only\non |W|, \u00b5tmplt, \u03c3 such that the following holds for any 0 < \u03bb < cn. Let x be any string that matches template\nz \u2208 supp(\u00b5tmplt). Then, for any \u03b4 > 0, with probability \u2265 1 \u2212 \u03b4 \u2212 exp(\u2212cn) over the random samples, the idealized\nestimator has error upper-bounded by\n| \u02c6f ideal(x) \u2212 f\u2217(z)| \u2264 C\nr\nlog(1/\u03b4)\nn\n.\nProof of Claim D.4. Let E1 be the event that n\u00b5tmplt(zj) \u2265 |Ij|/2 for all j \u2208 [r], i.e., all templates are well-\nrepresented in the dataset. By a Hoeffding bound,\nP[E1] \u2265 1 \u2212 exp(\u2212n\u00b5tmplt(za)/2).\nSuppose that x matches template za. By Claim D.3, under event E1, there is a constant C > 0 such that\n| \u02c6f ideal(x) \u2212 f\u2217(za)| = |yT ( \u02c6K\nideal + \u03bbI)\u22121videal(x) \u2212 f\u2217(za)|\n\u2264 |yT 1Ia\n|Ia| \u2212 f\u2217(za)| +\ns\n1\n|Ia|(\n\u03c4\n\u03c4 \u2212 \u03bb \u2212 1)\n\u2264 |yT 1Ia\n|Ia| \u2212 f\u2217(za)| + C\nr\n1\nn .\nWe conclude since P[|yT 1Ia\n|Ia| \u2212 f\u2217(za)| > C\nq\nlog(1/\u03b4)\nn\n| E1] \u2264 \u03b4 by a tail bound for Gaussians.\nPutting the elements together to conclude the proof of the lemma\nCombined, Claims D.2 and D.4 imply the\nlemma if we take \u03bb = \u0398(n), then we obtain error O(\np\nlog(1/\u03b4)/n +\np\n1/\u03c1) with probability at least 1 \u2212 \u03b4 \u2212\nexp(\u2212\u2126(n)).\n27\nD.1\nDeferred proofs of claims\nProof of Claim D.3. Let w1, . . . , wn be an orthogonal basis of eigenvectors for \u02c6K\nideal with eigenvalues \u03bd1, . . . , \u03bdn.\nNotice that these are also eigenvectors of \u02c6K\nideal + \u03bbI. Because of the block structure of \u02c6K\nideal, its eigenvectors and\neigenvalues have a simple form. Define\nM = diag([\np\n|I1|, . . . ,\np\n|Ir|])Ndiag([\np\n|I1|, . . . ,\np\n|Ir|]) .\nThe nonzero eigenvalues of \u02c6K\nideal correspond to the nonzero eigenvalues of M, because for any eigenvector u \u2208 Rr\nof M there is a corresponding eigenvector of \u02c6K\nideal with the same eigenvalue by letting each of the blocks Ij consist\nof copies of the entry uj/\np\n|Ij|. Therefore, all nonzero eigenvalues of \u02c6K\n\u22121 have magnitude at least\n|\u03bd1|, . . . , |\u03bdn| \u2265 1/\u2225M \u22121\u2225 \u2265 min\nj\u2208[r] |Ij|/\u2225N \u22121\u2225 = \u03c4 > \u03bb.\nSo \u02c6K\nideal + \u03bbI is invertible, which is the first part of the claim. Write 1Ia\n|Ia| in the eigenbasis as\n1Ia\n|Ia| =\nX\ni\nciwi ,\nfor some coefficients ci. By construction,\nvideal(x) = \u02c6K\nideal 1Ia\n|Ia| =\nX\ni\n\u03bdiciwi ,\nso\n\u2225( \u02c6K\nideal + \u03bbI)\u22121videal(x)\u22252 = \u2225\nX\ni\n\u03bdi\n\u03bdi + \u03bbciwi\u22252 =\nX\ni\n(\n\u03bdi\n\u03bdi + \u03bb)2c2\ni\n\u2264 max\ni (\n\u03bdi\n\u03bdi + \u03bb)2 1\n|Ia| \u2264 max\ni (\n\u03c4\n\u03c4 \u2212 \u03bb)2 .\nSimilarly,\n\u2225 1Ia\n|Ia| \u2212 ( \u02c6K\nideal + \u03bbI)\u22121videal(x)\u22252 = \u2225\nX\ni\n(1 \u2212\n\u03bdi\n\u03bdi + \u03bb)ciwi\u22252 =\nX\ni\n(1 \u2212\n\u03bdi\n\u03bdi + \u03bb)2c2\ni\n\u2264 max\ni (1 \u2212\n\u03bdi\n\u03bdi + \u03bb)2 1\n|Ia| \u2264 max\ni (1 \u2212\n\u03c4\n\u03c4 \u2212 \u03bb)2 .\nClaim D.5 (Bound on difference between kernel regressions). Suppose that \u02c6K is p.s.d and that ( \u02c6K\nideal+\u03bbI)\u22121videal(x)\nis well-defined. Then, for any \u03bb > 0,\n| \u02c6f ideal(x) \u2212 \u02c6f(x)| \u2264 \u2225y\u2225\n\u03bb (\u2225videal(x) \u2212 v(x)\u2225 + \u2225 \u02c6K \u2212 \u02c6K\nideal\u2225\u2225( \u02c6K\nideal + \u03bbI)\u22121videal(x)\u2225)\nProof of Claim D.5. By triangle inequality,\n| \u02c6f(x) \u2212 \u02c6f ideal(x)| = \u2225yT ( \u02c6K + \u03bbI)\u22121v(x) \u2212 yT ( \u02c6K\nideal + \u03bbI)\u22121videal(x)\u2225\n(a)\n\u2264 \u2225y\u2225 \u00b7 \u2225( \u02c6K + \u03bbI)\u22121v(x) \u2212 ( \u02c6K + \u03bbI)\u22121videal(x)\u2225\n|\n{z\n}\nTerm 1\n+ \u2225y\u2225 \u00b7 \u2225( \u02c6K + \u03bbI)\u22121videal(x) \u2212 ( \u02c6K\nideal + \u03bbI)\u22121videal(x)\u2225\n|\n{z\n}\nTerm 2\n28\nThe first term can be upper-bounded because \u2225( \u02c6K + \u03bbI)\u22121\u2225 \u2264 \u2225(\u03bbI)\u22121\u2225 = 1/\u03bb, so\nTerm 1 \u2264 \u2225videal(x) \u2212 v(x)\u2225\n\u03bb\nThe second term can be upper-bounded by\nTerm 2 = \u2225( \u02c6K + \u03bbI)\u22121(( \u02c6K + \u03bbI)( \u02c6K\nideal + \u03bbI)\u22121 \u2212 ( \u02c6K\nideal + \u03bbI)( \u02c6K\nideal + \u03bbI)\u22121)videal(x)\u2225\n= \u2225( \u02c6K + \u03bbI)\u22121( \u02c6K \u2212 \u02c6K\nideal)( \u02c6K\nideal + \u03bbI)\u22121videal(x)\u2225\n\u2264 1\n\u03bb\u2225 \u02c6K \u2212 \u02c6K\nideal\u2225\u2225( \u02c6K\nideal + \u03bbI)\u22121videal(x)\u2225 .\nProof of Claim D.2. Let E1 be the event that |Ij| \u2265 n\u00b5tmplt(zj) for all j \u2208 [r]. By Hoeffding, there is a constant\nc > 0 such that P[E1] \u2265 1 \u2212 exp(\u2212cn). By Claim D.3, under event E1, there is a constant C > 0 such that\n\u2225( \u02c6K\nideal + \u03bbI)\u22121videal(x)\u2225 \u2264 C\n\u221an .\n(19)\nNext, recall the parameter \u03c1 used to measure the spread of the substitution map distributions {\u00b5sub,z}z\u2208supp(\u00b5tmplt),\nas defined in (4.2). For each i \u2208 [n], let si : W \u2192 X be the substitution map used to generate the sample xi. Let P1\nbe the number of samples (i, i\u2032) such that their substitution maps overlap, or have range that overlaps with the regular\ntokens in the templates. Formally:\nP1 = |{1 \u2264 i < i\u2032 \u2264 n : si(W) \u2229 si\u2032(W) \u0338= \u2205 or si(W) \u2229 R \u0338= \u2205 or si\u2032(W) \u2229 R \u0338= \u2205}| .\nSimilarly, let P2 be the number of samples that (i, i\u2032) such that their substitution maps overlap with that used to\ngenerate x, or they overlap with the regular tokens in the templates:\nP2 = |{1 \u2264 i \u2264 n : si(W) \u2229 R \u0338= \u2205 or si(W) \u2229 {xj}j\u2208[k] \u0338= \u2205}| .\nBy the definition of \u03c1, we can upper-bound the expected number of \u201cbad\u201d pairs P1 and \u201cbad\u201d indices P2 by:\nE[P1] \u2264\n\uf8eb\n\uf8ed X\ni,i\u2032\u2208[n]\nX\nw,w\u2032\u2208W\nP[si(w) = si\u2032(w\u2032)]\n\uf8f6\n\uf8f8 + n\nX\ni\u2208[n]\nX\nt\u2208R\nP[t \u2208 si(W)] \u2264 Cn2\n\u03c1\n+ Cn\n\u03c1\n\u2264 Cn2\n\u03c1\nE[P2] \u2264\nX\ni\u2208[n]\nX\nt\u2208{xj}j\u2208[k]\u222aR\nP[t \u2208 si(W)] \u2264 Cn\n\u03c1 .\nBy Hoeffding\u2019s inequality, the event E2 that P1 \u2264 Cn2\n\u03c1\nand P2 \u2264 Cn\n\u03c1 occurs with probability \u2265 1 \u2212 exp(\u2212cn).\nUnder event E2,\n\u2225 \u02c6K \u2212 \u02c6K\nideal\u2225 \u2264 C + Cn/\u221a\u03c1\nand\n\u2225v(x) \u2212 videal(x)\u2225 \u2264 C\np\nn/\u03c1 .\n(20)\nBy Claim D.5 and (19) and (20), under events E1, E2, and using that \u2225y\u2225 \u2264 C\u221an, we have\n| \u02c6f ideal(x) \u2212 \u02c6f(x)| \u2264 C\u221an\n\u03bb\n(C\np\nn/\u03c1 + (C + Cn/\u221a\u03c1) C\n\u221an) \u2264 C(1 + n)\n\u03bb\u221a\u03c1\n.\n29\nE\nNonsingularity of random features after MLP layer (Proof of Lemma 4.6)\nConsider a kernel K2 formed from a kernel K1 as follows:\nK2(x, y) = Eu,v\u223c\u03a31(x,y)[\u03d5(u)\u03d5(v)] ,\n\u03a31(x, y) =\n\u0014K1(x, y)\nK1(x, y)\nK1(x, y)\nK1(x, y)\n\u0015\n.\nHere \u03d5 : R \u2192 R is a nonlinear activation function. Such a random features kernel arises in a neural network\narchitecture by appending an infinite-width MLP layer with Gaussian initialization to a neural network with random\nfeatures with kernel K1.\nWe wish to prove that a certain matrix N \u2208 Rr\u00d7r given by\nNij = K2(xi, yj) ,\n(21)\nis nonsingular, where x1, . . . , xr, y1, . . . , yr are inputs. The intuition is that if \u03d5 is a \u201cgeneric\u201d activation function,\nthen only a weak condition on K1 is required for the matrix N to be invertible. We provide a general lemma that\nallows us to guarantee the invertibility if the activation function is a shifted cosine, although we conjecture such a\nresult to be true for most non-polynomial activation functions \u03d5. This is a generalization of Lemma 4.6, so it implies\nLemma 4.6.\nLemma E.1 (Criterion for invertibility of N). Consider the matrix N \u2208 Rr\u00d7r defined in (21) where x1, . . . , xr and\ny1, . . . , yr are inputs. Suppose that for all nontrivial permutations \u03c4 \u2208 Sr \\ {id} we have\nX\ni\u2208[r]\nK1(xi, yi) \u0338=\nX\ni\u2208[r]\nK1(xi, y\u03c4(i))) .\n(22)\nSuppose also that the MLP activation function is \u03d5(t) = cos(kt + c) for two hyperparameters k, c. Then, N is\nnonsingular for all (k, c) \u2208 R2 except for a Lebesgue-measure-zero subset of R2.\nProof. Let f(k, c) := det(N). We wish to show that {(k, c) : f(k, c) = 0} is a measure-zero set. By Claim E.2, is an\nanalytic function of c and k, and by the identity theorem for analytic functions [Mit20], it suffices to show that f \u0338\u2261 0.\nFixing c = \u03c0/4, by Claim E.2,\nK2(x, y) = 1\n2 exp(\u2212k2\n2 (K1(x, x) + K1(y, y) \u2212 2K1(x, y))).\nTherefore\nf(k, \u03c0/4) =\nX\n\u03c4\u2208Sr\nsgn(\u03c4)\nY\ni\u2208[r]\nK2(xi, y\u03c4(i))\n= e\u2212 k2\n2 (P\ni\u2208[r] K1(xi,xi)+K1(yi,yi)) X\n\u03c4\u2208Sr\nsgn(\u03c4) exp(k2 X\ni\u2208[r]\nK1(xi, y\u03c4(i))) .\nIt remains to prove that as a function of k we have\nX\n\u03c4\u2208Sr\nsgn(\u03c4) exp(k2 X\ni\u2208[r]\nK1(xi, y\u03c4(i))) \u0338\u2261 0 ,\nThis holds because for any distinct c1, . . . , cl the functions exp(c1t), . . . , exp(clt) are linearly independent functions\nof t, since their Wronskian is a rescaled Vandermonde determinant\n\f\f\f\f\f\f\f\f\f\nexp(c1t)\n. . .\nexp(clt)\nd\ndx exp(c1t)\n. . .\nd\ndx exp(clt)\n...\n...\ndl\u22121\ndtl\u22121 exp(c1t)\n. . .\ndl\u22121\ndtl\u22121 exp(clt)\n\f\f\f\f\f\f\f\f\f\n= exp(\nl\nX\ni=1\ncit)\n\f\f\f\f\f\f\f\f\f\n1\n. . .\n1\nc1\n. . .\ncl\n...\n...\ncl\u22121\n1\n. . .\ncl\u22121\nl\n\f\f\f\f\f\f\f\f\f\n= exp(\nl\nX\ni=1\ncit)\nY\n1\u2264i<j\u2264l\n(cj \u2212 ci) \u0338\u2261 0\n30\nBelow is the technical claim used in the proof of the lemma.\nClaim E.2. Let U, V \u223c N(0,\n\u0014a\n\u03c1\n\u03c1\nb\n\u0015\n). Then for any k, c \u2208 R,\nE[cos(kU + c) cos(kV + c)] = 1\n2e\u2212 1\n2 k2(a+b)(e\u2212k2\u03c1 cos(2c) + ek2\u03c1) .\nProof. By Mathematica, we have the following Gaussian integrals\nE[eikU+ikV ] = E[e\u2212ikU\u2212ikV ] = e\u2212 1\n2 k2(a+b+2\u03c1) ,\nE[eikU\u2212ikV ] = E[e\u2212ikU+ikV ] = e\u2212 1\n2 k2(a+b\u22122\u03c1) .\nSince cos(kt + c) = (eikt+ic + e\u2212ikt\u2212ic)/2,\nE[cos(kU + c) cos(kV + c)] = 1\n4 E[(eikU+ic + e\u2212ikU\u2212ic)(eikV +ic + e\u2212ikV \u2212ic)]\n= 1\n4(e\u2212 1\n2 k2(a+b+2\u03c1)(e2ic + e\u22122ic) + 2e\u2212 1\n2 k2(a+b\u22122\u03c1))\n= 1\n2e\u2212 1\n2 k2(a+b)(e\u2212k2\u03c1 cos(2c) + ek2\u03c1) .\nF\nAnalysis of attention layer features (Proof of Lemma 4.7)\nFor any inputs X, Y , we write the kernel of the random features of the attention layer as\nKattn(X, Y ) = Em(X),m(Y )[smax(\u03b2m(X))T (XY T + \u03b32I)smax(\u03b2m(Y ))]\nm(X), m(Y ) \u223c N(0,\n\u0014XXT + \u03b32I\nXY T + \u03b32I\nY XT + \u03b32I\nY Y T + \u03b32I\n\u0015\n) ,\nas stated Section 4.1; see also Section H for the derivation of this kernel in the infinite-width limit of the transformer\narchitecture. For shorthand, we write \u03baX,Y (\u03b2, \u03b3) = Kattn(X, Y ) to emphasize the attention kernel\u2019s dependence\non the hyperparameters \u03b2 and \u03b3 which control the softmax\u2019s inverse temperature and the weight of the positional\nembeddings, respectively.\nWe prove Lemma 4.7, which is that Kattn satisfies the property (9) required by Lemma 4.6 for the transformer\nrandom features kernel to succeed at the template task.\nNamely, consider any disjoint templates z1, . . . , zr and two substitution maps s, s\u2032 : W \u2192 X\n\u2022 that have disjoint range: s(W) \u2229 s\u2032(W) = \u2205,\n\u2022 and the substituted tokens do not overlap with any of the tokens in the templates: s(W) \u2229 R = s\u2032(W) \u2229 R = \u2205\nwhere R = \u222ai\u2208[r],j\u2208[k]{z(i)\nj }.\nThen we define Xi, Y i \u2208 Rk\u00d7m to be the strings (where we abuse notation slightly by viewing them as matrices\nwith one-hot rows) after substituting zi by s, s\u2032 respectively:\nXi = sub(zi, s)\nY i = sub(zi, s\u2032) .\nLemma F.1 (Restatement of Lemma 4.7). Define g\u03c4(\u03b2, \u03b3) = P\ni\u2208[r] \u03baXi,Y \u03c4(i)(\u03b2, \u03b3). Then for all but a Lebesgue-\nmeasure-zero set of (\u03b2, \u03b3) \u2208 R2 we have gid(\u03b2, \u03b3) \u0338= g\u03c4(\u03b2, \u03b3) for all permutations \u03c4 \u0338= id.\nNo closed-form expression is known for \u03baX,Y (\u03b2, \u03b3), so our approach is to analyze its Taylor series expansion\naround \u03b2 = \u03b3 = 0. Our proof proceeds in stages, where, in each stage, we examine a higher derivative and progres-\nsively narrow the set of \u03c4 that might possibly have g\u03c4(\u03b2, \u03b3) = gid(\u03b2, \u03b3). In Section F.1, we list certain low-order\nderivatives of \u03baX,Y (\u03b2, \u03b3) that will be sufficient for our analysis. In Section F.2, we analyze some of the terms in these\nexpressions. In Section F.3 we put the previous lemmas together to prove Lemma F.1.\nTo avoid notational overload, in this section we will not use bolded notation to refer to the matrices X, Y , but\nrather use the lowercase X, Y .\n31\nF.1\nLow-order derivatives of attention kernel\nIn the following table we collect several relevant derivatives of\n\u2202i\n\u2202\u03b2i \u2202j\n\u2202\u03b3j \u03baX,Y (0, 0) for i \u2264 6 and j \u2264 4. For each\ni, j we use c1, c2, . . . to denote constants that depend only on k, and on the derivative i, j being computed. Certain\nconstants that are important for the proof are provided explicitly. These derivatives were computed using a Python\nscript available in our code. The colors are explained in Section F.2.\nDerivative\nExpansion\n\u03baX,Y (0, 0) =\n+c11T XY T 1\n\u22022\n\u2202\u03b22 \u22022\n\u2202\u03b32 \u03baX,Y (0, 0) =\n+c11T XY T 1 +c2tr(XY T )\n\u22024\n\u2202\u03b24 \u03baX,Y (0, 0) =\n+c11T XY T 1 +c21T XXT XY T 1 +c31T XY T Y Y T 1 +c41T XXT XXT XY T 1\n+c5(1T XY T 1)(1T XXT 1) +c61T XY T Y XT XY T 1 +c7(1T XY T 1)(1T XY T 1)\n+c81T Y XT XY T Y Y T 1 +c9(1T XY T 1)(1T Y Y T 1) +c10(1T XXT XY T 1)(1T XXT 1)\n+c11(1T XY T Y Y T 1)(1T XXT 1) +c12(1T XY T 1)(1T XXT XY T 1)\n+c13(1T XY T Y Y T 1)(1T XY T 1) +c14(1T XXT XY T 1)(1T Y Y T 1)\n+c15(1T XY T Y Y T 1)(1T Y Y T 1) +c16(1T XY T 1)(1T XXT 1)(1T XXT 1)\n+c17(1T XY T 1)(1T XXT XXT 1) +c18(1T XY T 1)(1T XY T 1)(1T XXT 1)\n+c19(1T XY T 1)(1T XY T 1)(1T XY T 1) +c20(1T XY T 1)(1T XXT 1)(1T Y Y T 1)\n+c21(1T XY T 1)(1T XY T 1)(1T Y Y T 1) +c22(1T XY T 1)(1T Y Y T 1)(1T Y Y T 1)\n+c23(1T XY T 1)(1T Y Y T Y Y T 1)\n\u22024\n\u2202\u03b24 \u22022\n\u2202\u03b32 \u03baX,Y (0, 0) =\n+c11T XY T 1 +c2tr(XY T ) +c31T XXT XY T 1 +c4tr(XXT XY T ) +c51T XY T Y Y T 1\n+c6tr(XY T Y Y T ) +c7(1T XY T 1)(1T XXT 1) +c8(tr(XY T ))(1T XXT 1)\n+c9(1T XY T 1)(1T XY T 1) +c10(1T XY T 1)(tr(XY T )) +c11(1T XY T 1)(1T Y Y T 1)\n+c121T XY T XY T 1 +c13(tr(XY T ))(1T Y Y T 1) +c141T Y XT Y Y T 1 +c151T XXT Y XT 1\n+c161T XXT Y Y T 1 +c17(1T Y Y T 1)(1T XXT 1)\n\u22026\n\u2202\u03b26 \u22024\n\u2202\u03b34 \u03baX,Y (0, 0) =\n+c11T XY T 1 +c2tr(XY T ) +c31T XXT XY T 1 +c4tr(XXT XY T ) +c51T XY T Y Y T 1\n+c6tr(XY T Y Y T ) +c7(1T XY T 1)(1T XXT 1) +c8(tr(XY T ))(1T XXT 1)\n+c9(tr(XY T ))(1T XY T 1) +c10(1T XY T 1)(1T Y Y T 1) +c11(1T XY T 1)(1T XY T 1)\n+c121T XY T XY T 1 +c13(tr(XY T ))(1T Y Y T 1) +c141T XXT Y XT 1 +c151T Y XT Y Y T 1\n+c16tr(XY T XY T ) +c17(tr(XY T ))(tr(XY T )) +c18 +c191T XXT 1\n+c201T XXT XXT 1 +c211T XXT Y Y T 1 +c221T Y Y T 1 +c23(1T XXT 1)(1T XXT 1)\n+c24(1T Y Y T 1)(1T XXT 1) +c25tr(XXT Y Y T ) +c261T Y Y T Y Y T 1\n+c27(1T Y Y T 1)(1T Y Y T 1)\nFurthermore,\n\u2022 in the expression for \u03baX,Y (0, 0) we have c1 = 1/k2 > 0,\n\u2022 in the expression for\n\u22022\n\u2202\u03b22 \u22022\n\u2202\u03b32 \u03baX,Y (0, 0), we have c2 = 8/k2 > 0,\n\u2022 in the expression for\n\u22024\n\u2202\u03b24 \u03baX,Y (0, 0), we have c20 = 24/k6 > 0,\n\u2022 in the expression for\n\u22024\n\u2202\u03b24 \u22022\n\u2202\u03b32 \u03baX,Y (0, 0), we have c16 = 48/k4 > 0,\n\u2022 and in the expression for\n\u22026\n\u2202\u03b26 \u22024\n\u2202\u03b34 \u03baX,Y (0, 0), we have c25 = 17280/k4 > 0.\nF.2\nSimplifying terms\nLet X \u2208 Rk\u00d7m and Y \u2208 Rk\u00d7m be matrices with one-hot rows (i.e., all entries are zero except for one).\nFor the submatrix corresponding to rows S and columns T, we use the notation [X]S\u00d7T \u2208 RS\u00d7T . If v is a vector,\nthen the subvector consisting of indices I is [v]I.\nLet R \u2286 [m] be a set containing the intersection of the column support of X and Y : i.e., for all i \u2208 [m] \\ R, either\n[X][k]\u00d7i = 0 or [Y ][k]\u00d7i = 0. We analyze the terms in the expressions of Section F.1 below.\n32\nF.2.1\nAssuming [1T X]R = [1T Y ]R\nSuppose that [1T X]R = [1T Y ]R. Then any of the pink terms can be written as a function of only X or only Y .\n\u2022 1T XY T 1 = \u2225[1T X]R\u22252\n\u2022 1T XXT XY T 1 = 1T Xdiag(1T X)Y T 1 = (1T X)\u22992 \u00b7 (1T Y ) = \u2225[1T X]R\u22253\n3\n\u2022 1T XY T Y Y T 1 = 1T Xdiag(1T Y )Y T 1 = (1T X) \u00b7 (1T Y )\u22992 = \u2225[1T X]R\u22253\n3\n\u2022 1T XXT XXT XY T 1 = 1T Xdiag(1T X)diag(1T X)Y T 1 = \u2225[1T X]R\u22254\n4\n\u2022 1T XY T Y XT XY T 1 = 1T Xdiag(1T Y )diag(1T X)Y T 1 = \u2225[1T X]R\u22254\n4\n\u2022 1T Y XT XY T Y Y T 1 = 1T Y diag(1T X)diag(1T Y )Y T 1 = \u2225[1T X]R\u22254\n4\n\u2022 trace(XXT XY T ) = trace(Xdiag(1T X)Y T ) = P\ni\u2208[k]\nP\nv\u2208[m] Xiv(1T X)vYiv = P\ni\u2208[k]\nP\nv\u2208R Xiv(1T X)v =\n1T Xdiag(1T X)1R = \u2225[1T X]R\u22252\n\u2022 trace(XY T Y Y T ) = \u2225[1T Y ]R\u22252 = \u2225[1T X]R\u22252\nF.2.2\nAssuming [X][k]\u00d7R = [Y ][k]\u00d7R\nSuppose that X[k]\u00d7R = Y[k]\u00d7R (i.e., the restriction of X and Y to the R rows is equal). Then any of the orange terms\ncan be written as a function of only X or only Y .\n\u2022 tr(XY T ) = P\nv\u2208[m]\nP\ni\u2208[k] XivYiv = P\nv\u2208R\nP\ni\u2208[k] X2\niv = 1T X1R = 1T Y 1R\n\u2022 1T XY T XY T 1 = P\na,b,c\u2208[k] 1(xa = yb)1(xb = yc) = 1T X[k]\u00d7R(Y[k]\u00d7R)T X[k]\u00d7R(Y[k]\u00d7R)T 1\n= 1T X[k]\u00d7R(X[k]\u00d7R)T X[k]\u00d7R(X[k]\u00d7R)T\n\u2022 1T XXT Y XT 1 = P\na,b,c 1(xa = xb)1(yb = xc) = P\na,b,c 1(xa = xb)1(yb = xc \u2208 R)\n= P\na,b,c 1(xa = xb \u2208 R)1(yb = xc \u2208 R) = P\na,b,c 1(xa = xb \u2208 R)1(xb = xc \u2208 R) =\n1T X[k]\u00d7R(X[k]\u00d7R)T X[k]\u00d7R(X[k]\u00d7R)T 1\n\u2022 1T Y XT Y Y T 1 = 1T X[k]\u00d7R(X[k]\u00d7R)T X[k]\u00d7R(X[k]\u00d7R)T 1\n\u2022 trace(XY T XY T ) = P\na,b 1(xa = yb)1(xb = ya) = P\na,b 1(xa = yb \u2208 R)1(xb = ya \u2208 R) = P\na,b 1(xa =\nxb \u2208 R) = trace((X[k]\u00d7R)(X[k]\u00d7R)T )\nF.2.3\nAssuming 1T XXT 1 = 1T Y Y T 1\nSuppose that 1T XXT 1 = 1T Y Y T 1. Then any of the blue terms can be written as a function of only X or only Y .\n\u2022 1T XXT 1 = 1T Y Y T 1\n\u2022 1T Y Y T 1 = 1T XXT 1\nF.2.4\nAssuming 1T XXT = 1T Y Y T\nSuppose that 1T XXT = 1T Y Y T . Then any of the teal terms can be written as a function of only X or only Y .\n\u2022 1T XXT Y Y T 1 = \u22251T XXT \u22252 = \u22251T Y Y T \u22252\n33\nF.3\nProof of Lemma F.1\nWe combine the above calculations to prove Lemma F.1.\nProof. By the technical Lemma G.1, we know that g\u03c4(\u03b2, \u03b3) is an analytic function for each \u03c4. Therefore, by the\nidentity theorem for analytic functions [Mit20], it suffices to show that for each \u03c4 \u2208 Sr \\ {id} we have gid(\u03b2, \u03b3) \u0338\u2261\ng\u03c4(\u03b2, \u03b3).\nStage 1. Matching regular token degree distributions.\nClaim F.2. If gid(0, 0) = g\u03c4(0, 0), then [1T Xi]R = [1T Y\u03c4(i)]R for all i \u2208 [r].\nProof. From the table in Section F.1, there is a positive constant c1 > 0 such that\ng\u03c4(0, 0) = c1\nX\ni\u2208[r]\n1T XiY T\n\u03c4(i)1 = c1\nX\ni\u2208[r]\n[1T Xi]R[Y T\n\u03c4(i)1]R\n(a)\n\u2264\nX\ni\u2208[r]\n\u2225[1T Xi]R\u2225\u2225[1T Y\u03c4(i)]R\u2225\n(b)\n\u2264\nsX\ni\u2208[r]\n\u2225[1T Xi]R\u22252\nsX\ni\u2208[r]\n\u2225[1T Y\u03c4(i)]R\u22252\n=\nX\ni\u2208[r]\n\u2225[1T Xi]R\u22252 ,\nwhere (a) is by Cauchy-Schwarz and holds with equality if and only if [1T Xi]R \u221d [1T Y\u03c4(i)]R for all i. Similarly (b)\nis by Cauchy-Schwarz and holds with equality if and only if \u2225[1T Xi]R\u2225 = \u2225[1T Y\u03c4(i)]R\u2225 for all i. Notice that (a) and\n(b) hold with equality if \u03c4 = id, since [1T Xi]R = [1T Yi]R for all i.\nStage 2. Matching regular token positions.\nClaim F.3. If\n\u22022\n\u2202\u03b22 \u22022\n\u2202\u03b32 g\u03c4(0, 0) =\n\u22022\n\u2202\u03b22 \u22022\n\u2202\u03b32 gid(0, 0) and [1T Xi]R = [1T Y\u03c4(i)]R for all i \u2208 [r], then we must have\n[Xi][k]\u00d7R = [Y\u03c4(i)][k]\u00d7R for all i \u2208 [r].\nProof. For a constant c2 > 0,\n\u22022\n\u2202\u03b22\n\u22022\n\u2202\u03b32 g\u03c4(0, 0) =\nX\ni\u2208[r]\nc11T XiY T\n\u03c4(i)1 + c2trace(XiY T\n\u03c4(i))\n=\n\uf8eb\n\uf8edc1\nX\ni\u2208[r]\n\u2225[1T Xi]R\u22252\n\uf8f6\n\uf8f8 +\n\uf8eb\n\uf8edc2\nX\ni\u2208[r]\ntrace(Xi(Y \u03c4(i))T )\n\uf8f6\n\uf8f8 ,\nby the calculation in Section F.2.1. The first sum does not depend on \u03c4, so we analyze the second sum. Here,\nc2\nX\ni\u2208[r]\ntrace(XiY T\n\u03c4(i)) = c2\nX\ni\u2208[r]\nX\na\u2208[k]\n[XiY T\n\u03c4(i)]aa\n= c2\nX\ni\u2208[r]\nX\nv\u2208R\nX\na\u2208[k]\n[Xi]av[Y\u03c4(i)]av\n(a)\n\u2264 c2\ns\n(\nX\ni\u2208[r]\nX\nv\u2208R\nX\na\u2208[k]\n([Xi]av)2)(\nX\ni\u2208[r]\nX\nv\u2208R\nX\na\u2208[k]\n([Y\u03c4(i)]av)2\n= c2\nX\ni\u2208[r]\n1T Xi1R ,\nwhere (a) is by Cauchy-Schwarz and holds with equality if and only if X(i)\nav = cY (\u03c4(i))\nav\nfor some constant c. We must\nhave c = 1 because of the CLS token, so (a) holds with equality if and only if [Xi][k]\u00d7R = [Y\u03c4(i)][k]\u00d7R for all i \u2208 [r].\nSpecifically (a) holds with equality if \u03c4 = id.\n34\nStage 3. Matching wildcard token degree histogram norm.\nClaim F.4. Suppose that [1T Xi]R = [1T Y\u03c4(i)]R, and that \u22024\n\u2202\u03b24 g\u03c4(0, 0) =\n\u22024\n\u2202\u03b24 gid(0, 0). Then 1T XiXT\ni 1 = 1T Y\u03c4(i)Y T\n\u03c4(i)1\nfor all i \u2208 [r].\nProof. Use [1T Xi]R = [1T Y\u03c4(i)]R and the calculations in Section F.2.1 for the pink terms. Every term of\n\u22024\n\u2202\u03b24 g\u03c4(0, 0)\ncan be written as depending only on one of Xi or Y\u03c4(i), with the exception of the c20 term. Namely, we have\n\u22024\n\u2202\u03b24 g\u03c4(0, 0) =\nX\ni\u2208[r]\na(Xi) + b(Y\u03c4(i))\n+ c20(1T XiY T\n\u03c4(i)1)(1T XiXT\ni )(1T Y\u03c4(i)Y T\n\u03c4(i)1) ,\nfor some functions a, b. Since \u03c4 is a permutation, only the term with coefficient c20 depends on \u03c4. Here, c20 > 0. This\nterm corresponds to\nc20\nX\ni\u2208[r]\n(1T XiY T\n\u03c4(i)1)(1T XiXT\ni 1)(1T Y\u03c4(i)Y T\n\u03c4(i)1)\n= c20\nX\ni\u2208[r]\n\u2225[1T Xi]R\u2225\u22251T Y\u03c4(i)]R\u2225(1T XiXT\ni 1)(1T Y\u03c4(i)Y T\n\u03c4(i)1)\n(a)\n\u2264\ns\n(\nX\ni\u2208[r]\n\u2225[1T Xi]R\u22252(1T XiXT\ni 1)2)(\nX\ni\u2208[r]\n\u22251T Y\u03c4(i)]R\u22252(1T Y\u03c4(i)Y T\n\u03c4(i)1)2\n=\nX\ni\u2208[r]\n\u2225[1T Xi]R\u22252(1T XiXT\ni 1)2\nwhere (a) is by Cauchy-Schwarz and holds with equality if and only if \u2225[1T Xi]R\u222521T XiXi1 = c\u2225[1T Y\u03c4(i)]R\u222521T Y\u03c4(i)Y T\n\u03c4(i)1\nfor all i and some constant c. This constant c = 1 because the former is a permutation of the latter over i \u2208 [r]. Since\n\u2225[1T Xi]R\u22252 = \u2225[1T Yi]R\u22252 \u2265 1 by assumption and since we have the CLS token, we know that (a) holds with equal-\nity if and only if 1T XiXT\ni 1 = 1T Y\u03c4(i)Y T\n\u03c4(i)1 for all i \u2208 [r]. This is the case for \u03c4 = id by construction of Xi and\nYi.\nStage 4. Matching wildcard degree distributions.\nClaim F.5. Suppose that [Xi][k]\u00d7R = [Y\u03c4(i)][k]\u00d7R and 1T XiXT\ni 1 = 1T Y\u03c4(i)Y T\n\u03c4(i)1 for all i \u2208 [r]. Suppose also that\n\u22024\n\u2202\u03b24 \u22022\n\u2202\u03b32 g\u03c4(0, 0) =\n\u22024\n\u2202\u03b24 \u22022\n\u2202\u03b32 gid(0, 0). Then 1T XiXT\ni = 1T Y\u03c4(i)Y T\n\u03c4(i) for all i \u2208 [r].\nProof. Similarly to the proof of the previous claim, because of the calculations in Sections F.2.1, F.2.2 and F.2.3 for\nthe pink, orange, and blue terms, respectively, we can write\n\u22024\n\u2202\u03b24 \u22022\n\u2202\u03b32 as a sum of terms that each depends on either Xi\nor Y\u03c4(i), plus P\ni\u2208[r] c161T XiXT\ni Y\u03c4(i)Y T\n\u03c4(i)1. This latter sum is the only term that depends on \u03c4, and the constant c16\nsatisfies c16 > 0. Similarly to the previous claim, by Cauchy-Schwarz\nX\ni\u2208[r]\nc161T XiXT\ni Y\u03c4(i)Y T\n\u03c4(i)1 \u2264\nX\ni\u2208[r]\nc16\u22251T XiXT\ni \u2225\u2225Y\u03c4(i)Y T\n\u03c4(i)1\u2225 ,\nwith equality if and only if 1T XiXT\ni = 1T Y\u03c4(i)Y T\n\u03c4(i) for all i, since {XiXT\ni }i is a permutation of {Y\u03c4(i)Y T\n\u03c4(i)}i. This\ncondition holds for \u03c4 = id.\nStage 5. Matching wildcard positions.\nClaim F.6. Suppose that [Xi][k]\u00d7R = [Y\u03c4(i)][k]\u00d7R and 1T XiXT\ni = 1T Y\u03c4(i)Y T\n\u03c4(i) for all i \u2208 [r]. Suppose also that\n\u22026\n\u2202\u03b26 \u22024\n\u2202\u03b34 g\u03c4(0, 0) =\n\u22026\n\u2202\u03b26 \u22024\n\u2202\u03b34 gid(0, 0). Then XiXT\ni = Y\u03c4(i)Y T\n\u03c4(i) for all i \u2208 [r].\n35\nProof. Write\n\u22026\n\u2202\u03b26 \u22024\n\u2202\u03b34 g\u03c4(0, 0) as a sum of terms each depending only on either Xi or Y\u03c4(i) by using the calculations\nin Sections F.2.1, F.2.3, F.2.2, and F.2.4 to handle the pink, orange, blue, and teal terms, plus (for c25 > 0),\nX\ni\u2208[r]\nc25trace(XiXT\ni Y\u03c4(i)Y T\n\u03c4(i)) \u2264\nX\ni\u2208[r]\nc25\u2225XiXT\ni \u2225F \u2225Y\u03c4(i)Y T\n\u03c4(i)\u2225F ,\nwith equality if and only if XiXT\ni = Y\u03c4(i)Y T\n\u03c4(i) for all i \u2208 [r]. This equality holds if \u03c4 = id, concluding the claim.\nCombine the above four claims to conclude that if g\u03c4(\u03b2, \u03b3) \u2261 gid(\u03b2, \u03b3), then we have XiXT\ni\n= Y\u03c4(i)Y T\n\u03c4(i) and\n[Xi][k]\u00d7R = [Y\u03c4(i)][k]\u00d7R for all i, so \u03c4 = id.\nG\nAnalyticity of attention kernel (technical result)\nWe prove the analyticity of \u03baX, \u02dc\nX(\u03b2, \u03b3) = K\u03b2,\u03b3\nattn(X, \u02dcX) as function of \u03b2 and \u03b3.\nLemma G.1 (Analyticity of Kattn). For any X, \u02dcX, the function \u03baX, \u02dc\nX is analytic in R2.\nProof. Note that we can write\nm := m(X) = X\u03b6 + \u03b3p,\n\u02dcm := m( \u02dcX) = \u02dcX\u02dc\u03b6 + \u03b3p ,\nwhere \u03b6, \u02dc\u03b6 \u223c N(0, Im) and p \u223c N(0, Ik) are independent Gaussians. So we can rewrite \u03baX, \u02dc\nX as\n\u03baX, \u02dc\nX(\u03b2, \u03b3) = E\u03b6,\u02dc\u03b6,p[f(\u03b2, \u03b3; \u03b6, \u02dc\u03b6, p)],\nwhere\nf(\u03b2, \u03b3; \u03b6, \u02dc\u03b6, p) = sT (X \u02dcX\nT + \u03b32I)\u02dcs .\nand\ns = smax(\u03b2X\u03b6 + \u03b2\u03b3p)T ,\n\u02dcs = smax(\u03b2 \u02dcX\u02dc\u03b6 + \u03b2\u03b3p) .\nThe main obstacle is to prove the technical Lemma G.9, which states that for any k1, k2, we have\nE\u03b6,\u02dc\u03b6,p[| \u2202k1\n\u2202\u03b2k1\n\u2202k2\n\u2202\u03b3k2 f(\u03b2, \u03b3; \u03b6, \u02dc\u03b6, p)|] \u2264 C(1 + \u03b32)k1!k2!(C(|\u03b2| + |\u03b3|)k1+k2)\nSo by smoothness of f and dominated convergence, we know that we can differentiate under the integral sign, and\n| dk1\nd\u03b2k1\ndk2\nd\u03b3k2 \u03baX,X\u2032(\u03b2, \u03b3)| = | E\u03b6,\u02dc\u03b6,p[ \u2202k1\n\u2202\u03b2k1\n\u2202k2\n\u2202\u03b3k2 f(\u03b2, \u03b3; X, \u02dcX, \u03b6, \u02dc\u03b6, p)]|\n\u2264 C(1 + \u03b32)k1!k2!(C(|\u03b2| + |\u03b3|)k1+k2) .\nBecause of the bound on the derivatives and its smoothness, \u03baX,X\u2032(\u03b2, \u03b3) is real-analytic.\nThe proof of the technical bound in Lemma G.9 is developed in the subsections below.\nG.1\nTechnical lemmas for quantifying power series convergence\nIn order to show that the values of the attention kernel are real-analytic functions of in terms of \u03b2, \u03b3, we will need to\nmake quantitative certain facts about how real-analyticity of is preserved under compositions, products, and sums. For\nthis, we introduce the notion of the convergence-type of a real-analytic function.\n36\nDefinition G.2 (Quantifying power series convergence in real-analytic functions). Let U \u2286 Rm be an open set. We\nsay that a real-analytic function f : U \u2192 R has (\u03c41, \u03c42)-type for functions \u03c41 : U \u2192 R>0 and \u03c42 : U \u2192 R>0 if the\nfollowing holds. For any \u03b60, consider the power series of f around \u03b60,\nX\n\u00b5\na\u03b60,\u00b5(\u03b6 \u2212 \u03b60)\u00b5 .\nThen for any \u03b6 such that \u2225\u03b6 \u2212 \u03b60\u2225\u221e \u2264 \u03c41(\u03b60) this power series converges absolutely.\nX\n\u00b5 s.t. |\u00b5|\u22651\n|a\u03b60,\u00b5||\u03b6 \u2212 \u03b60|\u00b5 \u2264 \u03c42(\u03b60) .\nWe provide rules for how convergence type is affected by compositions, products, and sums.\nLemma G.3 (Composition rule for type; quantitative version of Proposition 2.2.8 of [KP02]). Let U \u2286 Rm and let\nV \u2286 R be open. Let f1, . . . , fn : U \u2192 V be real-analytic with (\u03c41, \u03c42)-type, and let g : V n \u2192 R be real-analytic with\n(\u03c31, \u03c32)-type. Then the composition h = g \u25e6 (f1, . . . , fn) is real-analytic with (min(\u03c41, (\u03c31 \u25e6 f) \u00b7 \u03c41\n\u03c42 ), \u03c32 \u25e6 f)-type.\nProof. Fix some \u03b60 and let y0 = [f1(\u03b60), . . . , fn(\u03b60)], and let a(i)\n\u03b60,\u00b5 be the coefficients of the power series expansion\nfor fi around \u03b60. Define \u03c1 = min(1, \u03c31(y0)/\u03c42(\u03b60)). Then, for any \u03b6 such that \u2225\u03b6 \u2212 \u03b60\u2225\u221e \u2264 \u03c1\u03c41(\u03b60) and i \u2208 [n]\nwe have\nX\n\u00b5 s.t. |\u00b5|\u22651\n|a(i)\n\u03b60,\u00b5||\u03b6 \u2212 \u03b60|\u00b5 \u2264\nX\n\u00b5 s.t. |\u00b5|\u22651\n|a(i)\n\u03b60,\u00b5|\u03c1|\u00b5|\u03c41(\u03b60)|\u00b5| \u2264 \u03c1\u03c42(\u03b60) \u2264 \u03c31(y0) .\nSo, letting P\u221e\n\u03bd by0,\u03bd(y \u2212 y0)\u03bd be the series expansion of g around y0, we have the following absolute convergence\n\u221e\nX\n\u03bd, s.t. |\u03bd|\u22651\nby0,\u03bd\nn\nY\ni=1\n\f\f\f\f\f\f\nX\n\u00b5 s.t. |\u00b5|\u22651\n|a(i)\n\u03b60,\u00b5||\u03b6 \u2212 \u03b60|\u00b5\n\f\f\f\f\f\f\n\u03bdi\n\u2264 \u03c32(y0) .\nSo we may rearrange the terms of\n\u221e\nX\n\u03bd\nby0,\u03bd\nn\nY\ni=1\n\uf8eb\n\uf8ed\nX\n\u00b5 s.t. |\u00b5|\u22651\na(i)\n\u03b60,\u00b5(\u03b6 \u2212 \u03b60)\u00b5\n\uf8f6\n\uf8f8\n\u03bdi\n.\nas we please, and we get an absolutely convergent series for g \u25e6 f around \u03b60.\nLemma G.4 (Sum and product rules for type). Let f : Rm \u2192 R and g : Rm \u2192 R be real-analytic functions of\n(\u03c41, \u03c42)-type and (\u03c31, \u03c32)-type respectively. Then h = f + g is real-analytic of (min(\u03c41, \u03c31), \u03c42 + \u03c42)-type, and\nh = fg is real-analytic of (min(\u03c41, \u03c31), \u03c42\u03c32 + \u03c42|g| + |f|\u03c32)-type\nProof. Both of these are straightforward from the definition.\nLemma G.5 (Derivative bound based on type). Let f : Rm \u2192 R be real-analytic with (\u03c41, \u03c42)-type. Then, for any\nmulti-index \u00b5,\n| \u2202|\u00b5|\n\u2202\u03b6\u00b5 f(\u03b60)| \u2264\n\u03c42(\u03b60)\n\u03c41(\u03b60)|\u00b5| \u00b5!\nProof. Let a\u03b60,\u00b5 be the coefficients of the power series of f at \u03b60. Since f is of (\u03c41, \u03c42)-type, we have\nX\n\u00b5 s.t. |\u00b5|\u22651\n|a\u03b60,\u00b5||\u03c41(\u03b60)||\u00b5| \u2264 \u03c42(\u03b60) .\nSince all terms in the sum are nonnegative, for all \u00b5 with |\u00b5| \u2265 1,\n|a\u03b60,\u00b5| \u2264 \u03c42(\u03b60) \u00b7 (1/\u03c41(\u03b60))|\u00b5| .\nThe lemma follows by Remark 2.2.4 of [KP02], which states \u2202|\u00b5|\n\u2202\u03b6\u03bd f(\u03b60)| = |a\u03b60,\u00b5|\u00b5!.\n37\nG.2\nApplication of technical lemmas to attention kernel\nWe now use the above general technical lemmas to specifically prove that the attention kernel is analytic in terms of \u03b2\nand \u03b3.\nLemma G.6. For any j \u2208 [m], the function f : Rm \u2192 R given by f(\u03b6) = smax(\u03b6)j is real-analytic of (1/(2e2), 1)-\ntype\nProof. Write f = g \u25e6 h for g : R>0 \u2192 R and h : Rk \u2192 R>0 given by g(y) = 1/y, and h(\u03b6) = Pm\ni=1 e\u03b6i\u2212\u03b6j.\nThe power expansion of g(y) around y0 \u2208 R>0, is given by\ng(y) =\n\u221e\nX\nk=0\n(\u22121)k+1\nyk+1\n0\n(y \u2212 y0)k ,\nso one can see that g is of (\u03c11, \u03c12)-type for \u03c11(y0) = y0/2 and \u03c12(y0) = 1/y0 . Finally, write the series expansion for\nh(\u03b6) around \u03b60\nh(\u03b6) = 1 + e\u2212\u03b6j\nX\ni\u2208[m]\\{j}\ne\u03b6i = 1 +\nX\ni\u2208[m]\\{j}\n(\n\u221e\nX\nl=0\ne\u2212\u03b60,j (\u03b60,j \u2212 \u03b6j)l\nl!\n)(\n\u221e\nX\nk=0\ne\u03b60,i (\u03b6i \u2212 \u03b60,i)k\nk!\n)\nNote that this expansion converges absolutely for all \u03b6, as the absolute series is\n1 +\nX\ni\u2208[m]\\{j}\n(\n\u221e\nX\nl=0\ne\u2212\u03b60,j |\u03b60,j \u2212 \u03b6j|l\nl!\n)(\n\u221e\nX\nk=0\ne\u03b60,i |\u03b6i \u2212 \u03b60,i|k\nk!\n)\n= 1 +\nX\ni\u2208[m]\\{j}\ne\u2212\u03b60,j+\u03b60,i+|\u03b6i\u2212\u03b60,i|+|\u03b6j\u2212\u03b60,j|\n\u2264 e2\u2225\u03b6\u2212\u03b60\u2225\u221eh(\u03b6) .\nSpecifically, h is of (1, e2h)-type. So by the composition rule of Lemma G.3, it must be that f is real-analytic of\n(\u03c41, \u03c42)-type for \u03c41 = min(1, (\u03c11 \u25e6 h) \u00b7\n1\ne2h) = 1/(2e2) and \u03c42 = \u03c12 \u25e6 h = 1/h \u2264 1.\nLemma G.7. For any j \u2208 [m] and X, \u03b6, p, the function f : R2 \u2192 R given by f(\u03b2, \u03b3) = smax(\u03b2X\u03b6 + \u03b2\u03b3p)j is\nreal-analytic of (min(1, 1/(2e2\u2225X\u03b6\u2225\u221e + 2e2(|\u03b2| + |\u03b3|)\u2225p\u2225\u221e), 1)-type.\nProof. Write f = g \u25e6 h for g : Rm \u2192 R and h : R2 \u2192 Rm given by g(v) = smax(v)j and h(\u03b2, \u03b3) = \u03b2X\u03b6 + \u03b2\u03b3p.\nWe know from Lemma G.6 that g is real-analytic of (1/(2e2), 1)-type. And it is easy to see that h is real-analytic of\n(1, \u2225X\u03b6\u2225\u221e + (|\u03b2| + |\u03b3|)\u2225p\u2225\u221e)-type. Apply the composition rule of Lemma G.3 to conclude.\nLemma G.8. For any X, \u02dcX, \u03b6, \u02dc\u03b6, p, the function f : R2 \u2192 R given by f(\u03b2, \u03b3) = smax(\u03b2X\u03b6 + \u03b2\u03b3p)T (X \u02dcX\nT +\n\u03b32I)smax(\u03b2 \u02dcX\u02dc\u03b6 + \u03b2\u03b3p) is real-analytic and of type\n(min(1, 1\n2e2\n1\n\u2225X\u03b6\u2225\u221e + (|\u03b2| + |\u03b3|)\u2225p\u2225\u221e\n, 1\n2e2\n1\n\u2225 \u02dcX\u02dc\u03b6\u2225\u221e + (|\u03b2| + |\u03b3|)\u2225p\u2225\u221e\n), C(1 + \u03b32)) ,\nwhere C is a constant depending on the context length k.\nProof. Each entry of (X \u02dcX\nT + \u03b3I) is real-analytic in \u03b3 and of (1, \u03b3)-type. So by combining with Lemma G.7 the\nproduct rule and sum rule (Lemma G.4), and the fact that each entry of the smax is at most one.\nAs a consequence, we can bound the derivatives of f(\u03b2, \u03b3; X, \u02dcX, \u03b6, \u02dc\u03b6, p) = smax(\u03b2X\u03b6 + \u03b2\u03b3p)T (X \u02dcX\nT +\n\u03b32I)smax(\u03b2 \u02dcX\u02dc\u03b6 + \u03b2\u03b3p), which was what we needed to prove Lemma G.1.\nLemma G.9. For any k1, k2 \u2265 0,\n| \u2202k1\n\u2202\u03b2k1\n\u2202k2\n\u2202\u03b3k2 f(\u03b2, \u03b3; X, \u02dcX, \u03b6, \u02dc\u03b6, p)|\n\u2264 C(1 + \u03b32) max(1, ((2e2)(\u2225X\u03b6\u2225\u221e + \u2225 \u02dcX\u02dc\u03b6\u2225\u221e + (|\u03b2| + |\u03b3|)\u2225p\u2225\u221e))k1+k2)k1!k2! .\nProof. Direct consequence of Lemma G.5 and Lemma G.8.\n38\nH\nDerivation of transformer kernel\nWe informally derive the transformer random features kernel in the infinite-width limit.\nH.1\nTransformer architecture\nWe consider a depth-1 transformer architecture (without skip connections or layernorm, for simplicity). This ar-\nchitecture has H heads, each with parameters W K,h, W Q,h, W V,h, W O,h \u2208 Rdhead\u00d7demb, and embedding layer\nW E \u2208 Rm\u00d7demb, positional embeddings P \u2208 Rk\u00d7demb, an MLP layer with parameters W A, W B \u2208 Rdmlp\u00d7demb,\nand a final unembedding layer with weights wU \u2208 Rdemb. The network takes in X \u2208 Rk\u00d7m and outputs\nftrans(X; \u03b8) = wT\nUz2\n(Unembedding)\nwhere\nz2 =\n1\np\ndmlp\nW T\nB\u03c3(\n1\n\u221ademb\nW Az1) \u2208 Rdemb\n(MLP layer)\nz1 =\n1\n\u221a\nH\nX\nh\u2208[H]\nAT\nh ek \u2208 Rdemb\n(Attention layer output at CLS token)\nAh = smax(\u03b2Z0W T\nK,hW Q,hZT\n0\ndemb\n\u221adhead\n)Z0\nW T\nV,hW O,h\n\u221adheaddemb\n\u2208 Rk\u00d7demb\n(Attention heads)\nZ0 = XW E + \u03b3P \u2208 Rk\u00d7demb .\n(Embedding layer)\nHere \u03b2, \u03b3 \u2265 0 are two hyperparameters that control the inverse temperature of the softmax and the strength of the\npositional embeddings, respectively. Note that only the output of the attention layer at the final kth position CLS token\nis used, since this is a depth-1 network. Also, in the above definition the weights are rescaled compared to Section 2,\nbut this is is not important since what matters is the\nH.2\nRandom features kernel\nWe choose that initialization so that each of the entries of the intermediate representations Z0, z1, z2 is of order \u0398(1).\nIn order to accomplish this, we initialize W E, P , W K,h, W Q,h, W V,h, W O,h, W A, W B with i.i.d. N(0, 1) entries.\nWe also initialize wU = 0, and only train wU while maintaining the rest of parameters at initialization. The\nrandom features kernel corresponding to training wU is\n\u02c6Ktrans(X, Y ) = z2(X)T z2(Y )/demb ,\nwhere we view z2 as a function of the input (either X or Y ), and depending on the randomly-initialized parameters\nof the network.\nIn the limit of infinitely-many heads H, infinite embedding dimension demb and MLP dimension dmlp and head\ndimension dhead, the kernel \u02c6Ktrans tends to a deterministic limit Ktrans, which can be recursively computed (see, e.g.,\n[JGH18]). Assuming that the final token of both X and Y is the same token (i.e., a CLS token), the deterministic\nlimiting kernel Ktrans is given by:\nKtrans(X, Y ) = Eu,v[\u03c3(u)\u03c3(v)] for u, v \u223c N(0,\n\u0014Kattn(X, X)\nKattn(X, Y )\nKattn(Y , X)\nKattn(Y , Y )\n\u0015\n)\n(23)\nwhere K1(X, Y ) = Em(X),m(Y )[smax(\u03b2m(X))T (XY T + \u03b32I)smax(\u03b2m(Y ))]\nm(X), m(Y ) \u223c N(0, (1 + \u03b32)\n\u0014XXT + \u03b32I\nXY T + \u03b32I\nY XT + \u03b32I\nY Y T + \u03b32I\n\u0015\n) .\nNotice that the covariance matrix in the above definition of the distribution of m(X), m(Y ) is rescaled compared\nto that in the main text in Section 4.1, but this is inessential, since we can simply reparametrize \u03b2 as \u03b2 7\u2192 \u03b2/\np\n1 + \u03b32\nto recover the expression in the main text.\n39\nH.3\nInformal derivation\nWe provide an informal derivation of (23) below. Informally, by law of large numbers we have the following almost\nsure convergence\n\u02c6Ktrans(X, Y ) = z2(X)T z2(Y )\ndemb\n=\n\u03c3(\n1\n\u221ademb W Az1(X))T W BW T\nB\u03c3(\n1\n\u221ademb W Az1(Y ))\ndembdmlp\ndemb\u2192\u221e\n\u2192\n\u03c3(\n1\n\u221ademb W Az1(X))T \u03c3(\n1\n\u221ademb W Az1(Y ))\ndmlp\ndmlp\u2192\u221e\n\u2192\nEu,v[\u03c3(u)\u03c3(v)] for u, v \u223c N(0,\n\u0014Kattn(X, X)\nKattn(X, Y )\nKattn(Y , X)\nKattn(Y , Y )\n\u0015\n)\n:= Ktrans(X, Y ) ,\nwhere Kattn is the kernel corresponding to the attention layer in the infinite-width limit, defined as:\n\u02c6Kattn(X, Y ) := zT\n1 (X)zT\n1 (Y )\ndemb\n=\nP\nh,h\u2032\u2208[H] eT\nk Ah(X)Ah\u2032(Y )T ek\nHdemb\n=\n1\nHdheadd2\nemb\nX\nh,h\u2032\u2208[H]\neT\nk smax(\u03b2Z0(X)W T\nK,hW Q,hZ0(X)T\ndemb\n\u221adhead\n)Z0(X)W T\nV,hW O,h\n\u00b7 W T\nO,h\u2032W V,h\u2032Z0(Y )T smax(\u03b2Z0(Y )W T\nK,h\u2032W Q,h\u2032Z0(Y )T\ndemb\n\u221adhead\n)T ek\ndhead\u2192\u221e,demb\u2192\u221e\n\u2192\n1\nH\nX\nh\u2208[H]\neT\nk smax(\u03b2Z0(X)W T\nK,hW Q,hZ0(X)T\ndemb\n\u221adhead\n)(XY T + \u03b32I)\n\u00b7 smax(\u03b2Z0(Y )W T\nK,hW Q,hZ0(Y )T\ndemb\n\u221adhead\n)T ek\nH\u2192\u221e\n\u2192\nE[eT\nk smax(\u03b2Z0(X)W T\nK,hW Q,hZ0(X)T\ndemb\n\u221adhead\n)(XY T + \u03b32I)\n\u00b7 smax(\u03b2Z0(Y )W T\nK,hW Q,hZ0(Y )T\ndemb\n\u221adhead\n)T ek]\n= E[smax(\u03b2eT\nk Z0(X)W T\nK,hW Q,hZ0(X)T\ndemb\n\u221adhead\n)(XY T + \u03b32I)\n\u00b7 smax(\u03b2eT\nk Z0(Y )W T\nK,hW Q,hZ0(Y )T\ndemb\n\u221adhead\n)T ]\ndemb\u2192\u221e,dhead\u2192\u221e\n\u2192\nEm(X),m(Y )[smax(\u03b2m(X))T (XY T + \u03b32I)smax(\u03b2m(Y ))]\n:= Kattn(X, Y ) ,\nwhere\nm(X), m(Y ) \u223c N(0, (1 + \u03b32)\n\u0014XXT + \u03b32I\nXY T + \u03b32I\nY XT + \u03b32I\nY Y T + \u03b32I\n\u0015\n) ,\nbecause due to the randomness in W K,h and W Q,h we have that\nZ0(X)W T\nQ,hW K,hZ0(X)T ek\ndemb\n\u221adhead\nand\nZ0(Y )W T\nQ,hW K,hZ0(Y )T ek\ndemb\n\u221adhead\n40\nare jointly Gaussian with covariance:\n\u03a3(X, Y ) = EW K,h,W Q,h,W E,P [Z0(X)W T\nQ,hW K,hZ0(X)T ek\ndemb\n\u221adhead\neT\nk Z0(Y )W T\nK,hW Q,hZ0(Y )T\ndemb\n\u221adhead\n] , .\nSince this is an expectation over products of jointly Gaussian variables, for any i, j \u2208 [k] we can calculate:\n\u03a3i,j(X, Y ) = EW E,P [\n1\nd2\nemb\nX\nr,s\u2208[demb]\n[Z0(X)]ir[Z0(Y )]js trace(Z0(X)T ekeT\nk Z0(Y ))]\n= EW E,P [\n1\nd2\nemb\nX\nr,s,t\u2208[demb]\n[Z0(X)]ir[Z0(Y )]js[Z0(X)]kt[Z0(Y )]kt]\n= EW E,P [\n1\nd2\nemb\nX\nr,s,t\u2208[demb]\n[XW E + \u03b3P ]ir[Y W E + \u03b3P ]js[XW E + \u03b3P ]kt[Y W E + \u03b3P ]kt]\n(a)\n=\n1\nd2\nemb\nX\nr,s\u2208[demb]\nEW E,P [[XW E + \u03b3P ]ir[Y W E + \u03b3P ]js]\n\u00b7\nX\nt\u2208[demb]\nEW E,P [[XW E + \u03b3P ]kt[Y W E + \u03b3P ]kt] + O(1/demb)\n=\n1\ndemb\nX\nr,s\u2208[demb]\nEW E,P [[XW E + \u03b3P ]ir[Y W E + \u03b3P ]js] \u00b7 (1 + \u03b32) + O(1/demb)\n(a)\n=\n1\ndemb\nX\nr\u2208[demb]\nEW E,P [[XW E + \u03b3P ]ir[Y W E + \u03b3P ]jr] \u00b7 (1 + \u03b32) + O(1/demb)\n= [XY T ]ij + \u03b32\u03b4ij \u00b7 (1 + \u03b32) + O(1/demb) ,\nwhere in (a) we use that [XW E+\u03b3P ]ab and [Y W E+\u03b3P ]ab are independent of [XW E+\u03b3P ]cd and [Y W E+\u03b3P ]cd\nunless b = d. So\n\u03a3(X, Y )\ndemb\u2192\u221e\n\u2192\n(1 + \u03b32) \u00b7 (XY T + \u03b32I) .\nI\nMLPs fail to generalize on unseen symbols\nA natural question is whether classical architectures such as the MLP architecture (a.k.a., fully-connected network)\nwould exhibit the same emergent reasoning properties when trained with enough data. In this section, we prove a\nnegative result: an SGD-trained or Adam-trained MLP will not reach good test performance on the template task. This\nis in sharp contrast to the positive result for transformers proved in the previous section.\nMLP architecture\nThe input to the MLP is a concatenation of the token one-hot encodings. The MLP alternates lin-\near transformations and nonlinear elementwise activations. Formally, the MLP has weights \u03b8 = {W 1, . . . , W L, w}\nand outputs\nfMLP(x; \u03b8) = wT zL(x; \u03b8) \u2208 R\nwhere\n(24)\nz\u2113(x; \u03b8) = \u03d5(W \u2113z\u2113\u22121(x; \u03b8)) \u2208 Rd\nfor \u2113 \u2265 1\nz0(x; \u03b8) = z0(x) = [ex1, . . . , exk] \u2208 Rkm .\nWe consider training the MLP with SGD.\nDefinition I.1 (One-pass SGD training). The learned weights \u03b8t after t steps of SGD training are the random weights\ngiven by initializing \u03b80 so that each of W 0\n1, . . . , W 0\nL, w0 have i.i.d. Gausian entries, and then updating with \u03b8t =\n\u03b8t\u22121 \u2212 \u03b7t\u2207\u03b8(fMLP(xt; \u03b8) \u2212 yt)2 |\u03b8=\u03b8t\u22121 for (xt, yt) \u223c D and some step size \u03b7t > 0.\n41\nWe show that SGD-trained MLPs fail at the template task since they do not generalize well in the case when the\ntemplates consist only of wildcard tokens. In words, if the template labels f\u2217 are a non-constant function, the MLP\nwill not reach arbitrarily low error no matter how many training steps are taken. Let Xuns \u2282 X be the subset of tokens\nnot seen in the train data. We assume that |Xuns| \u2265 k, which guarantees that for any template there is at least one\nstring matching it where all the wildcards are substituted by tokens in Xuns. Under this condition:\nTheorem I.2 (Failure of MLPs at generalizing on unseen symbols). Suppose that the label function f\u2217 is non-constant,\nand that all templates in the support of \u00b5tmplt consist only of wildcards: z \u2208 Wk for all z \u2208 supp(\u00b5tmplt). Then, for\nany SGD step t there is a string x \u2208 (Xuns)k that matches a template z \u2208 supp(\u00b5tmplt) such that\nE\u03b8t[(fMLP(x; \u03b8t) \u2212 f\u2217(z))2] \u2265 c > 0 ,\nwhere c is constant that depends only on \u00b5tmplt and f\u2217.\nThe proof is deferred to Appendix I, and relies on the key observation that SGD-training of MLPs satisfies a\npermutation invariance property [Ng04]. This property guarantees that MLP cannot consistently distinguish between\nthe unseen tokens, and therefore, in expectation over the weights \u03b8t, outputs the same value for any sequence x \u2208\n(Xuns)k. We make four remarks.\nRemark I.3. MLPs are universal approximators [Cyb89], so there are choices of weights \u03b8 such that fMLP(\u00b7; \u03b8) has\ngood generalization on unseen symbols. The theorem proves that these weights are not found by SGD.\nRemark I.4. The theorem does not assume that training is in the NTK regime, i.e., it holds even for nonlinear training\ndynamics.\nRemark I.5. The theorem also holds for training with Adam, gradient flow, and minibatch-SGD, since the permutation-\ninvariance property of MLP training also holds for these. See Appendix I.\nRemark I.6. As a sanity check, we verify that MLP kernel does not meet the sufficient condition for generalizing\non unseen symbols from Lemma 4.5. The kernel for an MLP is an inner product kernel of the form KMLP(x, x\u2032) =\n\u03ba(Pk\ni=1 1(xi = x\u2032\ni)) for a function \u03ba : R \u2192 R. Therefore, the matrix N \u2208 Rr\u00d7r has all of its entries equal to\nNij = \u03ba(0), so it is singular and the condition of Lemma 4.5 is not met.\nWe now prove Theorem I.2. We first show that trained MLPs cannot differentiate between tokens in the set Xuns.\nLet X = Xseen \u2294 Xuns be the partition of tokens into those seen and not seen in the train data. Here Xseen is defined\nas the smallest set such that x \u2208 X k\nseen almost surely for (x, y) \u223c D.\nLemma I.7 (Trained MLPs cannot distinguish unseen tokens). For any number of SGD steps t, and any learning rate\nschedule \u03b71, . . . , \u03b7t, the learned MLP estimator cannot distinguish between sequences of unseen tokens. Formally, for\nany x1, x2 \u2208 X k\nuns, we have\nE\u03b8t[fMLP(x1; \u03b8t)] = E\u03b8t[fMLP(x2; \u03b8t)] .\nProof of Lemma I.7. The proof of this result is based on a well-known permutation-invariance property of MLPs\ntrained by SGD. This property has previously been used to show sample complexity lower bounds for learning with\nSGD-trained MLPs [Ng04; LZA20], as well as time-complexity lower bounds [Sha18; Abb+22; AB22]. In this\nlemma, we use the permutation invariance property to show poor out-of-distribution generalization of SGD-trained\nMLPs.\nFirst, construct a permutation \u03a0 \u2208 Rkm\u00d7km such that \u03a0z0(x1) = z0(x2), but which also satisfies that for any\n\u02dcx \u2208 (Xseen)k we have \u03a0z0(\u02dcx) = z0(\u02dcx). This permutation can be easily constructed since neither x1 nor x2 contains\ntokens in Xseen. Next, define the following network f \u03a0\nMLP, analogously to (24) but with the first-layer inputs permuted\nby \u03a0\nf \u03a0\nMLP(x; \u03b8) = wT z\u03a0\nL(x; \u03b8) \u2208 R\nwhere\nz\u03a0\n\u2113 (x; \u03b8) = \u03d5(W \u2113z\u03a0\n\u2113\u22121(x; \u03b8)) \u2208 Rd\nfor \u2113 \u2265 1\nz\u03a0\n0 (x; \u03b8) = z\u03a0\n0 (x) = \u03a0[ex1, . . . , exk] \u2208 Rkm .\n42\nNow let us couple the weights \u03b80, . . . , \u03b8t from SGD training of fMLP on dataset D, with the weights \u03b8\u03a0,0, . . . , \u03b8\u03a0,t\nfrom SGD training of f \u03a0\nMLP on dataset D. The coupling is performed inductively on the time step, and we can maintain\nthe property that \u03b8\u03c4 = \u03b8\u03a0,\u03c4 for all t. For the base case \u03c4 = 0, we set \u03b80 = \u03b8\u03a0,0. For the inductive step, \u03c4 \u2265 1, we\nupdate the weights with the gradient from some sample (x\u03c4, y\u03c4). Since x\u03c4 \u2208 (X seen)k almost surely, we know that\nz0(x\u03c4) = z\u03a0\n0 (x\u03c4) almost surely, which means that \u03b8\u03c4 = \u03b8\u03a0,\u03c4 almost surely. We conclude the equality in distribution\nof the weights\n\u03b8t d= \u03b8\u03a0,t .\n(25)\nNext, let us inductively couple the weights \u03b80, . . . , \u03b8t with the weights \u03b8\u03a0,0, . . . , \u03b8\u03a0,t in a different way, so as to\nguarantee that for any time 0 \u2264 \u03c4 \u2264 t, we have\nW \u03c4\n1 = W \u03a0,\u03c4\n1\n\u03a0 and W \u03c4\n\u2113 = W \u03a0,\u03c4\n\u2113\nfor all 2 \u2264 \u2113 \u2264 L and w\u03c4 = w\u03a0,\u03c4 .\nalmost surely. The base case \u03c4 = 0 follows because the distribution of W 0\n1 and W \u03a0,0\n1\nis equal and is also invariant\nto permutations since it is Gaussian. For the inductive step, couple the sample updates so that SGD draws the same\nsample (x\u03c4, y\u03c4) \u223c D. One can see from the chain rule that the invariant is maintained. We conclude the equality in\ndistribution of the weights\n\u03b8t = {W t\n1, . . . , W t\nL, wt}\nd= {W \u03a0,t\n1 \u03a0, W \u03a0,t\n2 , . . . , W \u03a0,t\nL , w\u03a0,t}\n(26)\nCombining (25) and (26), we get\n\u03b8t = {W t\n1, . . . , W t\nL, wt}\nd= {W t\n1\u03a0, W t\n2, . . . , W t\nL, wt} ,\nwhich,since \u03a0z0(x1) = z0(x2), immediately implies\nfMLP(x1; \u03b8t) = fMLP(x2; {W t\n1\u03a0, W t\n2, . . . , W t\nL, wt})\nd= fMLP(x2; \u03b8t) ,\nwhich proves the lemma.\nTheorem I.2 follows as a consequence. Note that the key lemma proved above only relied on a permutation invari-\nance property of SGD on MLPs that also holds for Adam training, gradient flow training, and SGD with minibatch\n(see [LZA20]). Therefore, the result holds for training with those algorithms as well, beyond just SGD.\nProof of Theorem I.2. Pick any two templates z, z\u2032 \u2208 supp(\u00b5tmplt) such that f\u2217(z) \u0338= f\u2217(z\u2032). Recall that z, z\u2032 \u2208 Wk\nby assumption. Since we assumed that |Xuns| \u2265 k, there are strings x, x\u2032 \u2208 X k\nuns matching templates z and z\u2032,\nrespectively. Furthermore, by Lemma I.7, if we define a = E\u03b8t[fMLP(x; \u03b8t)] = E\u03b8t[fMLP(x\u2032; \u03b8t)], we have\nmax(E\u03b8t[(fMLP(x; \u03b8t) \u2212 f\u2217(z))2], E\u03b8t[(fMLP(x\u2032; \u03b8t) \u2212 f\u2217(z\u2032))2])\n\u2265 max((a \u2212 f\u2217(z))2, (a \u2212 f\u2217(z\u2032))2)\n\u2265 1\n4(f\u2217(z) \u2212 f\u2217(z\u2032))2 = c > 0 .\nJ\nDeferred details for symbolic-label template tasks\nJ.1\nDefinition of symbolic-label template tasks\nIn symbolic-label template tasks the output is a token in X. This corresponds to the next-token prediction setting, and\nthe appropriate loss is the cross-entropy loss for multiclass classification. The formal definition of these tasks is:\nDefinition J.1 (Multi-class prediction version of template). The data distribution Dmulticlass = Dmulticlass(\u00b5tmplt, {\u00b5sub,z}, f\u2217)\nis specified by: (i) a template distribution \u00b5tmplt supported on (X \u222aW)k; (ii) for each template z, a distribution \u00b5sub,z\nover substitution maps s : W \u2192 X; (iii) a labelling function f\u2217 : supp(\u00b5tmplt) \u2192 X \u222aW. A sample (x, y) \u2208 X k \u00d7X\ndrawn from Dmulticlass is drawn by taking x = sub(z, s) and y = sub(f\u2217(z), s), where z \u223c \u00b5tmplt and s \u223c \u00b5sub,z.\n43\nJ.2\nFailure of transformers to copy and modification that succeeds\nWe provide the deferred proofs for Section 5.\nAttention layer architecture\nFor simplicity in this section we consider a transformer with the attention layer only,\nsince the MLP layer does not play a role in the ability to copy unseen symbols. Our architecture has H heads with\nparameters W K,h, W Q,h, W V,h, W O,h \u2208 Rdhead\u00d7demb, an embedding/unembedding layer W E \u2208 Rm\u00d7demb, po-\nsitional embeddings P \u2208 Rk\u00d7demb, an MLP layer with parameters W A, W B \u2208 Rdmlp\u00d7demb, a final unembedding\nlayer , and an activation function \u03d5. The network takes in X \u2208 Rk\u00d7m and outputs\nfattn(X; \u03b8) = W Ez1 \u2208 Rm\n(Unembedding layer)\nwhere\nz1 =\nX\nh\u2208[H]\nAT\nh ek\nAh = smax(\u03b2Z0W T\nK,hW Q,hZT\n0 )Z0W T\nV,hW O,h \u2208 Rk\u00d7demb\n(Attention heads)\nZ0 = XW E + \u03b3P \u2208 Rk\u00d7demb .\n(Embedding layer)\nand we tie the embedding and unembedding weights, as often done in practice, for example in GPT-2 [Bro+20]. Here\n\u03b2, \u03b3 \u2265 0 are two hyperparameters that control the inverse temperature of the softmax and the strength of the positional\nembeddings, respectively.\nSimplification in our case\nWe consider here a next-token prediction setup, where there is no final [CLS] token\nappended to the string. Namely, given a string x \u2208 X k, this is inputted to the network as a stacked matrix of one-hot\nvectors for the tokens of the string X = [ex1, . . . , exk]. We study a very basic template task: template \u201c\u03b1\u201d labeled by\n\u03b1, where \u03b1 is a wildcard. An example dataset generated from this template could be {(A, A), (B, B), (C, C)}, where\nA, B, C \u2208 X are tokens. Because the template has length k = 1, X \u2208 Rk\u00d7m is a one-hot vector encoding the input\ntoken. Furthermore, the softmax output is always a 1 \u00d7 1 matrix with the entry 1, so the architecture simplifies to\nfattn(X; \u03b8) = W E(\nX\nh\u2208[H]\nW T\nO,hW V,h)(W T\nEXT + \u03b3P T ) .\n(27)\nWe initialize the entries of P and W E be i.i.d. N(0, 1/demb), the entries of W O,h be N(0, 1/(demb)), and the\nentries of W V,h be N(0, 1/dhead), so that as demb \u2192 \u221e the variance of the output vanishes as O(1/demb) as in the\nmean-field scaling [MMN18; MMM19; SS22; CB18; RV18; YH21].\nDerivation of kernels driving dynamics at small times\nDespite the simplicity of the task, the architecture does\nnot generalize well on unseen symbols. Our evidence for this will be by analyzing the early times of training. For\nthese times, the dynamics are governed by the neural tangent kernel (NTK) of the network at initialization [JGH18;\nCOB19]. Let us derive the neural tangent kernel of this architecture. This is a network with output of dimension m,\nso for each i, j \u2208 [m] we will derive Kij,O(X, X\u2032), Kij,V (X, X\u2032), Kij,P (X, X\u2032), Kij,E(X, X\u2032) which give the\ndynamics at small times for training the {W O,h}h\u2208[H], the {W V,h}h\u2208[H], the W P , and the W E weights at small\ntimes, respectively. Writing W E = [wE,1, . . . , wE,m]\u22a4, by the law of large numbers,\nKij,O(X, X\u2032) =\nX\nh\u2208[H]\n\u0012\u2202[fattn(X; \u03b8)]i\n\u2202W O,h\n\u0013T \u0012\u2202[fattn(X\u2032; \u03b8)]j\n\u2202W O,h\n\u0013\n\u221d 1\nH\nX\nh\u2208[H]\n(XW E + \u03b3P )W T\nV,hW V,h(W T\nEXT + \u03b3P T )wT\nE,iwE,j\ndhead\u2192\u221e,demb\u2192\u221e\n\u2192\n\u03b4ij(\u03b4x1,x\u2032\n1 + \u03b32)\n44\nKij,V (X, X\u2032) =\nX\nh\u2208[H]\n\u0012\u2202[fattn(X; \u03b8)]i\n\u2202W V,h\n\u0013T \u0012\u2202[fattn(X\u2032; \u03b8)]j\n\u2202W V,h\n\u0013\n\u221d demb\ndhead\nX\nh\u2208[H]\nwT\nE,iW T\nO,hW O,hwE,j(XW E + \u03b3P )T (X\u2032W E + \u03b3P )\ndhead\u2192\u221e\n\u2192\nwT\nE,iwE,j(XW E + \u03b3P )T (X\u2032W E + \u03b3P )\ndemb\u2192\u221e\n\u2192\n\u03b4ij(\u03b4x1,x\u2032\n1 + \u03b32)\nKij,P (X, X\u2032) =\n\u0012\u2202[fattn(X; \u03b8)]i\n\u2202P\n\u0013T \u0012\u2202[fattn(X\u2032; \u03b8)]j\n\u2202P\n\u0013\n= \u03b32w\u22a4\nE,iwE,j\ndemb\u2192\u221e\n\u2192\n\u03b32\u03b4ij\nKij,E(X, X\u2032) =\n\u0012\u2202[fattn(X; \u03b8)]i\n\u2202W E\n\u0013T \u0012\u2202[fattn(X\u2032; \u03b8)]j\n\u2202W E\n\u0013\n= \u03b4ij(XW E + \u03b3P )(\nX\nh\u2208[H]\nW T\nV,hW O,h)(\nX\nh\u2208[H]\nW T\nO,hW V,h)(W T\nE(X\u2032)T + \u03b3P T )\n+ \u03b4x1,x\u2032\n1wT\nE,i(\nX\nh\u2208[H]\nW T\nO,hW V,h)(\nX\nh\u2208[H]\nW T\nV,hW O,h)wT\nE,j\n+ \u03b4i,x\u2032\n1wT\nE,j(\nX\nh\u2208[H]\nW T\nO,hW V,h)(\nX\nh\u2208[H]\nW T\nO,hW V,h)(wE,x1 + \u03b3P T )\n+ \u03b4x1,jwT\nE,i(\nX\nh\u2208[H]\nW T\nO,hW V,h)(\nX\nh\u2208[H]\nW T\nO,hW V,h)(wE,x\u2032\n1 + \u03b3P T )\ndhead\u2192\u221e,demb\u2192\u221e,H\u2192\u221e\n\u2192\n\u03b4ij(2\u03b4x1,x\u2032\n1 + \u03b32) ,\nsince only the first two terms do not vanish as the embedding dimension and number of heads go to infinity.\nTraining loss and testing loss\nLet (x1, y1), . . . , (xn, yn) \u2208 X \u00d7 X be a training set of data points drawn from this\ntask, where due to the structure of the template task each of the context strings is length-1 and we have xi = yi. We will\ntest the model on a data point (xtest, ytest), which does not appear in the test set: i.e., xtest = ytest \u0338\u2208 {x1, . . . , xn}.\nThe training loss is given by\nLtrain(\u03b8) = 1\nn\nn\nX\ni=1\n\u2113(fattn(xi; \u03b8), yi) ,\nwhere \u2113 is the cross-entropy loss, and the test loss is given by\nLtest(\u03b8) = \u2113(fattn(xtest), ytest) .\nTheorem J.2. For any learning rates \u03b7O, \u03b7V , \u03b7P , \u03b7E such that | \u2202Ltrain\n\u2202t\n| = O(1) as demb, dhead, and H \u2192 \u221e,\nwe have | \u2202Ltest\n\u2202t\n| \u2264 o(1). In other words, the error for generalization on unseen symbols does not decrease during\ntraining for infinite-width transformers.\nProof. Consider training with gradient flow with learning rates \u03b7O, \u03b7V , \u03b7P , \u03b7E on the parameters {W O,h}h\u2208[H],\n{W V,h}h\u2208[H], W P , and W E, respectively. In the limit as demb \u2192 \u221e we have fattn(X; \u03b80) \u2192 0, so\n\u2202Ltrain\n\u2202\u03b8\n|\u03b8=\u03b80= 1\nn\nn\nX\ni=1\n( 1\nm1 \u2212 exi)T \u2202fattn(Xi; \u03b8)\n\u2202\u03b8\n|\u03b8=\u03b80 .\n45\nSo at time t = 0, the training loss decreases as\n\u2202Ltrain\n\u2202t\n|t=0 \u2192 \u2212 1\nn2\nX\ni,i\u2032\u2208[n]\nX\nj,j\u2032\u2208[m]\n(1/m \u2212 \u03b4j,xi)(1/m \u2212 \u03b4j\u2032,xi\u2032 )\n\u00b7 (\u03b7V Kjj\u2032,V (Xi, Xi\u2032) + \u03b7OKjj\u2032,O(Xi, Xi\u2032)\n+ \u03b7P Kjj\u2032,P (Xi, Xi\u2032) + \u03b7EKjj\u2032,E(Xi, Xi\u2032)).\nSo we must take \u03b7O = O(1/H), \u03b7V = O(demb/dhead), \u03b7P = O(1), and \u03b7E = O(1) for us to have \u2202Ltrain\n\u2202t\n= O(1)\nbe bounded by a constant that does not grow with demb, dhead, and H.\nUnder these choices of learning rates, the test loss on token xtest which is not in the training dataset {x1, . . . , xn},\nevolves as\n\u2202Ltest\n\u2202t\n|t=0 \u2192 \u2212 1\nn\nX\ni\u2208[n]\nX\nj,j\u2032\u2208[m]\n(1/m \u2212 \u03b4j,xi)(1/m \u2212 \u03b4j\u2032,xtest)\n\u00b7 (\u03b7V Kjj\u2032,V (Xi, Xtest) + \u03b7OKjj\u2032,O(Xi, Xtest)\n+ \u03b7P Kjj\u2032,P (Xi, Xtest) + \u03b7EKjj\u2032,E(Xi, Xtest))\n\u2192 \u2212 1\nn\nX\ni\u2208[n]\nX\nj,j\u2032\u2208[m]\n(1/m \u2212 \u03b4j,xi)(1/m \u2212 \u03b4j\u2032,xtest)\n\u00b7 ((dhead\ndemb\n\u03b7V + H\u03b7O)\u03b4j,j\u2032(\u03b4xi,xtest + \u03b32)\n+ \u03b7P \u03b32\u03b4j,j\u2032 + 2H\u03b7E\u03b4j,j\u2032(\u03b4xi,xtest + \u03b32))\n= \u2212\u03b32\nn\nX\ni\u2208[n]\nX\nj\u2208[m]\n(1/m \u2212 \u03b4j,xi)(1/m \u2212 \u03b4j,xtest) \u00b7 (dhead\ndemb\n\u03b7V + H\u03b7O + \u03b7P + 2\u03b7E)\n= \u2212C\nn\nX\ni\u2208[n]\nX\nj\u2208[m]\n(1/m \u2212 \u03b4j,xi)(1/m \u2212 \u03b4j,xtest)\n= \u2212C/m + C/m + C/m = C/m \u2265 0.\nOn the other hand, now we consider the fattn architecture where in each head we replace W T\nV,hW O,h with\nW T\nV,hW O,h + bhI, where bh is a trainable parameter and I \u2208 Rdemb\u00d7demb is the identity matrix:\nf \u2032\nattn(X; \u03b8) = W Ez1 \u2208 Rm\n(Unembedding layer)\nwhere\nz\u2032\n1 =\nX\nh\u2208[H]\n(A\u2032\nh)T ek\nA\u2032\nh = smax(\u03b2Z0W T\nK,hW Q,hZT\n0 )Z0(W T\nV,hW O,h+bhI) \u2208 Rk\u00d7demb\n(Attention heads)\nZ0 = XW E + \u03b3P \u2208 Rk\u00d7demb .\n(Embedding layer)\nAgain, for the case of k = 1 that we consider, the network simplifies considerably to\nf \u2032\nattn(X; \u03b8) = W E(\nX\nh\u2208[H]\nW T\nO,hW V,h+bhI)(W T\nEXT + \u03b3P T ) .\n(28)\nWe initialize bh = 0 for all h, so that the neural tangent kernels Kij,O, Kij,V , Kij,P , Kij,E are the same as above.\nNow we also have a neural tangent kernel for training the parameters {bh}h\u2208[H]:\nKij,b(X, X\u2032) =\nX\nh\u2208[H]\n\u2202[fattn(X; \u03b8)]i\n\u2202bh\n\u2202[fattn(X\u2032; \u03b8)]j\n\u2202bh\n\u221d w\u22a4\nE,i(W T\nEXT + \u03b3P T )(XW E + \u03b3P T )wE,j\ndemb\u2192\u221e\n\u2192\n\u03b4i,x1\u03b4j,x\u2032\n1\n46\nWe prove that under this parametrization the test loss does decrease with training, which shows that adding this\ntrainable identity scaling allows transformers to succeed at this task.\nTheorem J.3. There is a choice of learning rates \u03b7b, \u03b7V , \u03b7O, \u03b7E, \u03b7P such that as demb, dhead, H \u2192 \u221e we have\n| \u2202Ltrain\n\u2202t\n| |t=0= O(1) and \u2212 \u2202Ltest\n\u2202t\n|t=0= \u2126(1).\nProof. Training just the parameters {bh}h\u2208[H] with learning rate \u03b7b (keeping the learning rates \u03b7V , \u03b7O, \u03b7P , \u03b7E = 0,\nso the training loss decreases as\n\u2202Ltrain\n\u2202t\n|t=0\u2192 \u2212 \u03b7b\nn2\nX\ni,i\u2032\u2208[n]\nX\nj,j\u2032\u2208[m]\n(1/m \u2212 \u03b4j,xi)(1/m \u2212 \u03b4j\u2032,xi\u2032 )Kjj\u2032,b(Xi, Xi\u2032) ,\nso we should take \u03b7b = \u0398(1/H) for the train loss have derivative on the order of \u0398(1). The test loss decreases as:\n\u2202Ltest\n\u2202t\n|t=0 \u2192 \u2212\u03b7b\nn\nX\ni\u2208[n]\nX\nj,j\u2032\u2208[m]\n(1/m \u2212 \u03b4j,xi)(1/m \u2212 \u03b4j\u2032,xtest)Kjj\u2032,b(Xi, Xtest)\n\u2192 \u2212H\u03b7b\nn\nX\ni\u2208[n]\nX\nj,j\u2032\u2208[m]\n(1/m \u2212 \u03b4j,xi)(1/m \u2212 \u03b4j\u2032,xtest)\u03b4j,xi\u03b4j\u2032,xtest\n= \u2212H\u03b7b\nn\nX\ni\u2208[n]\n(1/m \u2212 1)(1/m \u2212 1)\n= \u2212H\u03b7b(1 \u2212 1/m)2\n= \u2126(1) ,\nfor \u03b7b = \u2126(H), as demb, H \u2192 \u221e.\n47\n"
  },
  {
    "title": "Ranking LLM-Generated Loop Invariants for Program Verification",
    "link": "https://arxiv.org/pdf/2310.09342.pdf",
    "upvote": "2",
    "text": "Ranking LLM-Generated Loop Invariants for Program Verification\nSaikat Chakraborty, Shuvendu K. Lahiri, Sarah Fakhoury, Madanlal Musuvathi,\nAkash Lal, Aseem Rastogi, Aditya Senthilnathan, Rahul Sharma, Nikhil Swamy\nMicrosoft Research\n{saikatc, shuvendu, sfakhoury, madanm, akashl, aseemr,\nt-adityas, rahsha, nswamy}@microsoft.com\nAbstract\nSynthesizing inductive loop invariants is fun-\ndamental to automating program verification.\nIn this work, we observe that Large Language\nModels (such as gpt-3.5 or gpt-4) are capa-\nble of synthesizing loop invariants for a class\nof programs in a 0-shot setting, yet require sev-\neral samples to generate the correct invariants.\nThis can lead to a large number of calls to a\nprogram verifier to establish an invariant. To\naddress this issue, we propose a re-ranking ap-\nproach for the generated results of LLMs. We\nhave designed a ranker that can distinguish be-\ntween correct inductive invariants and incor-\nrect attempts based on the problem definition.\nThe ranker is optimized as a contrastive ranker.\nExperimental results demonstrate that this re-\nranking mechanism significantly improves the\nranking of correct invariants among the gen-\nerated candidates, leading to a notable reduc-\ntion in the number of calls to a verifier. The\nsource code and the experimental data for this\npaper are available in https://github.com/\nmicrosoft/NeuralInvariantRanker.\n1\nIntroduction\nProgram verification is a crucial step toward build-\ning trustworthy software. Unfortunately, the prob-\nlem of verifying properties of programs contain-\ning loops is undecidable. Verifying properties of\nprograms containing loops boils down to inferring\nloop invariants, which are facts that hold for any\niteration of the loop, and also ensure the desired\nproperty. There is a rich body of prior work on syn-\nthesizing loop invariants for program verification\nthrough symbolic techniques (Cousot and Cousot,\n1977; Col\u00f3n et al., 2003; Graf and Sa\u00efdi, 1997;\nMcMillan, 2003), and their use in verifying safety\nproperties of real-world programs (Ball et al., 2001;\nBlanchet et al., 2003; Lahiri et al., 2009). More re-\ncently, there is a growing interest in the application\nof machine learning towards invariant synthesis\n(Garg et al., 2016; Padhi et al., 2016; Yao et al.,\n2020; Si et al., 2018).\nIn recent years,\nLarge Language Models\n(LLMs) (Radford et al., 2018) have emerged as\nfoundational AI models that have revolutionized\nLanguage Processing applications. Though LLMs\nwere originally proposed for natural languages,\nthey have exhibited great success in formal lan-\nguages such as programming languages (Chen\net al., 2021). In fact, with the increased size, mod-\nels have started to exhibit emergent properties. For\nexample, modern LLMs such as gpt-3.5 (Ouyang\net al., 2022), gpt-4 (OpenAI, 2023), PaLM (Chowd-\nhery et al., 2022) are capable of reasoning about a\ngiven task with few-shot (Brown et al., 2020), or\neven zero-shot prompting (Kojima et al., 2022).\nSuch an impressive footprint of LLM naturally\nraises the question: How well can LLMs automati-\ncally synthesize inductive loop invariants?\nTo this end, we employ two different state-of-the-\nart LLMs for synthesizing loop invariants. We ob-\nserve that these models can generate well-formed\ninvariants, but finding the correct one often re-\nquires a large number of samples.\nA solution\nbased on guess and check, with the aid of an au-\ntomated program verifier based on Z3 (De Moura\nand Bj\u00f8rner, 2008), can be computationally very\nexpensive due to several invocations on incorrect\ninvariants. To minimize such costs, we propose\nreranking the generated invariants based on their\nlikelihood of successful verification. Inspired by\nthe use of contrastive learning in information re-\ntrieval (Karpukhin et al., 2020), our approach,\ncalled iRank, transforms the problem and invariants\nto bring the correct solution closer in vector space\nwhile pushing away incorrect ones. Empirical re-\nsults show that such re-ranking moves the median\nrank of the verified invariant to 4 in contrast to the\nexpected median rank of 31 for the generations\nfrom gpt-3.5.\nIn summary, in this paper, we propose to rerank\nthe LLM-generated loop invariants to reduce the\n1\narXiv:2310.09342v3  [cs.PL]  12 Feb 2024\ncost of wasted verification effort. We have de-\nsigned a ranker to contrast correct and incorrect\ninvariants and show a significant reduction in the\ninvariant checking effort compared to raw LLM\ngenerations.\n2\nRelated Work\nPrior works on loop invariant generation can be\nbroadly grouped into symbolic or machine learn-\ning based. Symbolic approaches either construct\ninvariants that are correct by construction (Cousot\nand Cousot, 1977; Col\u00f3n et al., 2003), or leverage\nSatisfiability-Modulo-Theories (SMT) solvers such\nas Z3 (De Moura and Bj\u00f8rner, 2008) to enumer-\nate and check candidate invariants over a space\nof pre-defined predicates (Flanagan and Leino,\n2001; Flanagan and Qadeer, 2002; Lahiri and\nBryant, 2007; Gulwani et al., 2009; Fedyukovich\nand Bod\u00edk, 2018) or predicates constructed through\nvariants of Craig\u2019s interpolants (McMillan, 2003;\nHenzinger et al., 2004; Dillig et al., 2013). On\nthe other hand, recent techniques leverage machine\nlearning to synthesize candidate invariants that are\nchecked for correctness using an SMT-based pro-\ngram verifier. Techniques range from incorporating\nthe feedback from a verifier using active learning\nover decision trees (Garg et al., 2016), learning\nfrom counter examples (Sharma and Aiken, 2016;\nPadhi et al., 2016), reinforcement learning over\ngraph neural networks (Si et al., 2018) and the\nuse of continuous logic networks (Yao et al., 2020;\nRyan et al., 2020). Unlike these techniques, our ap-\nproach leverages an LLM for generation and ranks\nusing a purely neural model and does not require\na program verifier at the inference time. This is\nimportant for scenarios where the verifier is semi-\nautomated, as is the case of most real-world pro-\ngram verification tools such as Dafny (Leino, 2010)\nand F* (Swamy et al., 2011). Finally, Pei et al.\n(2023) predict program invariants using LLMs, but\nthey do not aim at generating inductive invariants\nthat are sufficient for formal program verification.\n3\nBackground & Motivation\n3.1\nBackground: Loop Invariant Inference\nIn this section, we recall the problem of loop in-\nvariant generation in program verification. First,\nlet us define a grammar for program statements S,\nintegral expressions a and Boolean expressions b,\noperating on scalar variables. Most statements and\nexpressions are self-explanatory.\nS\n::=\nx := a | skip | S; S | if b then S else S\na\n::=\nn | x | a + a | a \u2212 a | a \u2217 a | . . .\nb\n::=\ntrue | false | a = a | a < a | b \u2227 b | b \u2228 b | \u00acb\nIn its simplest form, the goal of program verifica-\ntion is to verify that a program fragment satisfies its\nspecifications denoted by the Hoare-triple (Hoare,\n1969) - {pre} while b do S {post}. Given a pro-\ngram p and a pair of Boolean expressions (denoted\nby b in the grammar) \u03d5 and \u03c8 denoting the precon-\ndition and postcondition of a program p, the Hoare-\ntriple {\u03d5} p {\u03c8} denotes that every terminating\nexecution of p that starts in an pre-state satisfying\nthe predicate \u03d5 ends up in a post-state that satisfies\nthe predicate \u03c8. Since loops can execute an un-\nbounded number of iterations, verifying programs\nwith a loop requires a loop invariant i that satisfies\nthe following conditions:\n{pre} skip {i}\n{i \u2227 b} S {i}\n{i \u2227 \u00acb} skip {post}\n(1)\nThe conditions respectively denote that the loop\ninvariant i holds on loop-entry, is preserved by an\narbitrary iteration of the loop and implies the post\ncondition on exit. The problem of loop invariant\ninference is to infer an i that satisfies the three\nchecks above, and denoted as i \u22a2 p.\nFurthermore, for the loop-free statements S in\nthe grammar above, checking the Hoare-triple\n{\u03c8} S {\u03d5} can be reduced to (decidable) logi-\ncal formulas in the Satisfiability-Modulo-Theories\n(SMT) using standard techniques in program ver-\nification (Leino, 2010). One can use a predicate\ntransformer called weakest precondition WP to con-\nvert the Hoare-triple to a decidable SMT formula\nthat can be checked by Z3.\n\u03c8 =\u21d2 WP(S, \u03d5)\n{\u03c8} S {\u03d5}\nThe WP is defined inductively on the structure\nof statements as follows:\nWP(x := a, \u03d5)\n.=\n\u03d5[a/x]\nWP(skip, \u03d5)\n.=\n\u03d5\nWP(S1; S2, \u03d5)\n.=\nWP(S1, WP(S2, \u03d5))\nWP(if b then S1 else S2, \u03d5)\n.=\nV\n(b =\u21d2 WP(S1, \u03d5))\n(\u00acb =\u21d2 WP(S2, \u03d5))\n3.2\nMotivation and Problem Formulations\nGiven a problem definition p that consists of pre-\nconditions pre, a loop while b do S, and postcon-\nditions post, we can query LLMs to generate an\ninvariant i that satisfies the conditions specified\nin Equation (1). Although we have observed that\n2\niRank during Training\nProblem\nVerified Invariant\nVerified Invariant\nVerified Invariant\nWrong Invariant\nWrong Invariant\nWrong Invariant\nInitial Embedding\n(wrong invariants)\nIncrease\u00a0 Distance\nDecrease\u00a0 Distance\nEmbedding\nModel\nInitial Embedding\n(problem definition)\nTransformed Embedding\n(used for ranking)\nEmbedding\nTransformation\n(3-layer fully\nconnected ANN)\nAnnotated example from training data\nVerified Invariant\nVerified Invariant\nVerified Invariant\nInitial Embedding\n(correct invariants)\nInitial Embeddings\nnor(<= n \n i  forall((j\nInt)) (=>\n(and ( ...\nand(< i n)\nkforall((j\nInt)) (=\n (and (...\nor(<= i n)\n(forall((\nj Int))(=>\n(and  ...\n=>(<= i n)\n((j Int)) \n(=> (and (\n...\n  \nInvariant Synth. Problem\n(define-fun pre_fun ...)\n(define-fun trans_fun ...)\n(define-fun post_fun ...)\nEmbedding\nModel\nCandidate Invariants\nTransformed Embeddings\n0.7\n0.8\n0.9\n0.5\n=>(<= i n)\n((j Int)) \n(=> (and (\n...\nnor(<= n \n i  forall((j\nInt)) (=>\n(and ( ...\nand(< i n)\nkforall((j\nInt)) (=\n (and (...\nor(<= i n)\n(forall((\nj Int))(=>\n(and  ...\nSimilarity (Dot Products)\n(problem emb. and candidate inv. emb.)\nRe-ranked Candidate\nInvatiants\niRank during Ranking\nFigure 1: LLM for Loop Invariant synthesis.\nLLMs are capable of producing loop invariants\nwithout syntax errors, they often require numerous\nsamples before generating a correct invariant (we\nrefer to Appendix B for a details). This results in in-\nefficient resource utilization during the verification\nprocess, particularly when dealing with complex\nproblem instances. More importantly, for a more\npractical scenario where generated invariants are\nused as part of an interactive verification system\nsuch as Dafny/F*, an incorrect invariant would take\nup valuable user time to perform a manual failed\nverification effort. Consequently, we propose the\nutilization of iRank to prioritize the generated in-\nvariants based on their likelihood of being correct.\nFigure 1 provides a high-level overview of the en-\nvisioned invariant generation-ranking system.\n4\niRank: Methodology\nThe main intuition behind iRank is learning to pull\nthe likely correct invariants to the front of a ranked\nlist. Figure 1 shows a high-level overview of the\nranker. We rely on a dataset, D = {(p, I+, I\u2212)},\ncontaining Loop Invariant generation problem, p, a\nset of verified loop invariants, I+ = {i+ | i+ \u22a2\np}, and a set of wrong loop invariant, I\u2212 =\n{i\u2212 | i\u2212 \u22ac p} for each of the problems, to\nbuild iRank. Our goal is to learn a function be-\ntween a problem definition p and invariant i, i.e.,\n\u03c3(p, i), which should satisfy the following con-\nstraint \u2200{i+,i\u2212} (\u03c3(p, i+) > \u03c3(p, i\u2212)).\nContrastive Ranking. To learn \u03c3, we first ex-\ntract the embedding of problem definitions and the\ninvariants with an embedder, \u03a6, i.e., x = \u03a6(p),\nand y = \u03a6(i), where x and y are the embeddings\nof problem definition p, and invariant i, respec-\ntively. We learn a transformation function, \u03a8(x|\u03b8),\nwhich applies non-linear transformation on input\nvector x with learnable parameter \u03b8. We then trans-\nform problem embedding x to x\u2032 = \u03a8(x|\u03b8), and\ntransform invariant embedding y to y\u2032 = \u03a8(y|\u03b8).\nNow our target is to maximize the similarity be-\ntween x\u2032 and y\u2032, when y\u2032 corresponds to a cor-\nrect invariant, minimize the similarity otherwise.\nWe use the absolute cosine similarity as the mea-\nsurement. Use of such allows us to set the max-\nimum similarity to 1 (in the case of correct in-\nvariant) and the minimum to 0 (in the case of\nwrong invariant). We optimize the mean squared\nerror loss to learn the parameters in \u03a8. We ex-\nperimented with two different embedding models\nbased on LLMs, i.e.,text-embedding-ada-002\nand davinci-similarity. Appendix A presents\nfurther details of iRank\u2019s working procedure.\n5\nExperimental Design and Results\n5.1\nExperimental Setup\nBenchmarks. We use Loop Invariant Synthesis\nbenchmarks assimilated by Padhi et al. (2016) 1\nconstituting a set 940 challenge problems in Sy-\n1https://github.com/SaswatPadhi/LoopInvGen/\ntree/master/benchmarks\n3\nGus (Alur et al., 2013) format, with a SMT for-\nmula for the pre-condition, post-condition, and the\ntransfer-function for the loop. We chose a SMT\nrepresentation for our problem description p to be\nagnostic to different encoding of C programs into\nlogic. Among these problems, 541 were in the\nscope of LLM due to the context window size. We\nset the maximum context size as 4096 (with 3584\nfor prompt, 512 for generation).\nGathering LLM-generated Invariants. We con-\nducted experiments involving two distinct language\nmodels: gpt-3.5-turbo and gpt-4. Our objec-\ntive was to assess the capabilities of these language\nmodels out-of-the-box, and thus we employed a\nzero-shot prompting approach. This involved pro-\nviding a problem description and an appropriate\ntask explanation as a prompt (refer to Appendix C\nfor an example). For each problem, we allowed\nboth the models to generate invariants for a maxi-\nmum duration of 10 minutes or until a verified in-\nvariant was found, whichever occurred first, result-\ning in solving 250 problems by gpt-3.5-turbo,\nand 188 problems for gpt-42. It is important to\nclarify that the purpose of this paper is not to con-\nduct a comparative analysis of these language mod-\nels in relation to this specific problem. Instead,\nour objective is to propose a method to orthogo-\nnally augment LLM capabilities by reranking LLM\ngenerated invariants.\nTraining Data. We create the training dataset for\niRank (D = {(p, I+, I\u2212)}) by combining invari-\nants generated from different sources, such as dif-\nferent generations from LLMs, and invariants gen-\nerated by LoopInvGen (Padhi et al., 2017). We\ndivided the problems into five folds and trained 5\ndifferent rankers, one for each fold. During the\nevaluation, we select and load the trained model\nbased on the problem under evaluation. Detailed\nstatistics of data is available in Appendix A.3.\nEvaluation Metric. We then sequentially attempt\nto check invariants from a ranked list. We evaluate\nthree metrics \u2013 (i) i+ ranks - rank of the correct\ninvariant in the list, (ii) V@K - the percentage of\nproblems where the verified invariant is found in\ntop K invariants from the re-ranked list, and (iii)\nNumber of Z3 calls - the total number of z3 calls\nbefore finding and reporting a correct invariant, a\nhigher number of z3 calls indicate a high waste of\ncomputational resources.\n2Note that the rate limit for gpt-4 was an order lower than\ngpt-3.5 in our usage resulting in an order less samples.\nExperiment\ni+ ranks\nV@K (%)\nMean\nMedian\nK=1\nK=5\nk=10\nLLM-ranks\n189.78\n62.00\n5.2\n11.6\n18.4\nExpected ranks\n95.35\n31.02\n8.0\n19.2\n25.2\nTF-IDF\n103.45\n24.00\n17.6\n32.0\n38.8\nEmb.\nAda\n115.89\n31.50\n11.2\n21.6\n30.0\nDavinci\n120.02\n32.00\n10.4\n20.8\n33.6\niRank\nAda\n38.78\n5.00\n28.0\n51.2\n60.8\nDavinci\n34.48\n4.00\n29.2\n52.8\n62.8\n(a) Invariants generated by gpt-3.5-turbo .\nExperiment\ni+ ranks\nV@K (%)\nMean\nMedian\nK=1\nK=5\nk=10\nLLM-ranks\n39.20\n9.00\n17.6\n40.4\n51.6\nExpected ranks\n20.23\n4.96\n31.9\n52.1\n65.4\nTF-IDF\n24.16\n3.00\n32.00\n45.6\n53.6\nEmb.\nAda\n20.69\n5.50\n26.6\n51.1\n64.9\nDavinci\n23.56\n5.00\n27.7\n52.1\n63.3\niRank\nAda\n13.18\n2.00\n44.7\n74.4\n81.4\nDavinci\n11.96\n2.00\n44.7\n71.8\n81.9\n(b) Invariants generated by gpt-4 .\nTable 1: Comparison between different ranking strategies for\nre-ranking the invariants generated by different LLMs.\nBaselines. (a) LLM-ranks. We take the invariants,\nin the order generated by the LLMs, as a ranklist.\n(b) Expected-ranks. We estimate the expected val-\nues of the evaluated metrics in this paper by ran-\ndomly permuting the LLM-generated list of invari-\nants (see Appendix D for more details). (c) Em-\nbeddings. We use the raw embeddings from LLM-\nbased embedding models to calculate similarity\nwithout training. (d) TF-IDF. We use the textual\nsimilarity between the problem description and the\ncandidate invariants for ranking.\nResearch Questions. In this paper, we studied\ntwo research questions. (i) How effective are LLM-\nbased embeddings for ranking invariants? and (ii)\nCan a trained iRank help reduce the verification\ncost?\n5.2\nResults\nTable 1 shows the quantitative evaluation of iRank.\nIf we consider LLM-generated list of invariants as\nis, we observe that LLMs are able to generate a ver-\nified invariant after a significant number of wasted\ntrials. For example, on average, gpt-3.5-turbo\nfound an invariant after \u223c190 failed attempt at\ngeneration. gpt-4 does much better, in compar-\nison, with the mean rank of verified invariants be-\ning 39.20. The expected rank of the verified in-\nvariant from LLM-generations is 95.35 and 20.23,\nfor gpt-3.5-turbo and gpt-4, respectively. The\nuse of LLM-based embeddings (without any train-\n4\nLLM Ranks\nExpected Ranks\nTF-IDF\nDavinci emb.\nAda emb.\niRank-Ada\niRank-Davinci\nExperiments\n0\n100\n200\n300\n400\n500\nRank of the verified invariant\nRank\nLLM Ranks\nExpected Ranks\nTF-IDF\nDavinci emb.\nAda emb.\niRank-Ada\niRank-Davinci\nExperiments\n0\n200\n400\n600\n800\n1000\n1200\nNumber of Z3 calls\n# Z3 calls\n(a) gpt-3.5-turbo.\nLLM Ranks\nExpected Ranks\nTF-IDF\nDavinci emb.\nAda emb.\niRank-Ada\niRank-Davinci\nExperiments\n0\n20\n40\n60\n80\nRank of the verified invariant\nRank\nLLM Ranks\nExpected Ranks\nTF-IDF\nDavinci emb.\nAda emb.\niRank-Ada\niRank-Davinci\nExperiments\n0\n50\n100\n150\n200\nNumber of Z3 calls\n# Z3 calls\n(b) gpt-4.\nFigure 2: Detailed results comparing ranks of the correct invariants and number of z3 calls.\n0\n10\n20\n30\n40\n50\nTop k Invariants Tried\n20\n40\n60\n80\nPercentage of Problems Verified\nLLM Ranks\nExpected Ranks\nTF-IDF\nDavinci emb.\nAda emb.\niRank-Ada\niRank-Davinci\n(a) gpt-3.5-turbo\n0\n10\n20\n30\n40\n50\nTop k Invariants Tried\n20\n40\n60\n80\n100\nPercentage of Problems Verified\nLLM Ranks\nExpected Ranks\nTF-IDF\nDavinci emb.\nAda emb.\niRank-Ada\niRank-Davinci\n(b) gpt-4\nFigure 3: Percentage of problems solved w.r.t. number of\ninvariants from the (re-)ranked list.\ning) such as from text-embedding-ada-002 or\ndavinci-similarity results is the mean rank of\nthe verified invariant to be 115.89 and 120.02,\nrespectively for gpt-3.5-turbo, and 20.69 and\n23.56, respectively for gpt-4. While such a result\nlooks like a significant improvement over LLM-\nranks, it is slightly worse than expected ranks.\n\u2713 LLM-based embeddings standalone may not\nserve well for reranking verified invariants.\nThe training procedure in iRank significantly\ncontributes to improving the ranking of verified\ninvariants by transforming the embeddings. The\nmedian rank of the verified invariant is 32 when\nusing the embedding from davinci-similarity\nembeddings.\nWith the contrastive training in\niRank, the median rank is brought down to 4,\nshowing a significant improvement. Such a trend\npersists across different embedding models and\ngenerations from different LLMs.\nFigure 2a,\nand Figure 2b shows the detailed results invari-\nants generated by gpt-3.5-turbo and gpt-4.\nIn both trained and raw embeddings, the dif-\nference between text-embedding-ada-002 and\ndavinci-similarity models, the performance\ndifferences are not different with statistical sig-\nnificance (with p-value > 0.1). In both models\u2019\ncases, there is no statistically significant difference\nbetween the expected rank and raw embedding-\nbased ranking. Note that, similar to the existing\nworks (Ryan et al., 2020), we use z3 call to mea-\nsure resource wastage. However, depending on the\nsystem the experiments are carried on, the time\nsaved from reranking with iRank could be different.\nWe report our experimental results of wall clock\ntime in Appendix B (Table 2).\nFigure 3 shows the percentage of problems veri-\nfied after trying k invariants (V@K). We observe\nthat the iRank curves are very steep at the begin-\nning of the curves compared to the baseline, sig-\nnifying that it could rank the verified invariants in\nsignificantly higher positions than baselines.\n\u2713 Contrastive training in iRank brings the veri-\nfied invariant closer to the problem while pushing\nthe wrong ones resulting in a significant reduc-\ntion in the verification cost.\n6\nConclusion\nWe presented a novel approach, iRank, to rank the\nloop invariants generated by LLMs based on their\nlikelihood of being correct. Our ranker leverages\na contrastive learning technique to discriminate\nbetween correct and incorrect invariants. Our eval-\nuation demonstrates that our approach significantly\nreduces the invariant checking effort compared to\nthe original LLM outputs.\n5\nLimitations\nAssumptions of LLM inference cost.\nIn this\npaper, we assumed the cost of calling LLMs is neg-\nligible compared to the cost of calling the verifier\nfor checking an invariant. Current access to LLMs\n(at least the one we studied in the paper) is avail-\nable through the rest API, which can be scaled up\nby the API vendor with distributed processing of\nLLM. However, with the increase in the problem\ncomplexity, i.e., non-linear invariants, high number\nof variables, the check for correctness of an invari-\nant become exponentially more expensive. In the\ncase where call to LLM is much more expensive\nthan LLM, iRank will reduce the number of Z3\ncalls, but may not contribute to actual cost savings.\nComparison with state-of-the-art (SOTA) invari-\nant synthesis.\nThe goal of this paper is not to\nestablish SOTA for loop invariant synthesis. In con-\ntrast, we investigate LLMs\u2019 capacity to generate\nLoop invariant relying on their emergent behav-\nior. iRank is proposed as an orthogonal tool and\nevaluated to rank LLM generations in this paper.\nHowever, in theory, iRank should be able to rerank\ninvariants generated by any generator. Neverthe-\nless, the design of the SOTA technique of Loop\nInvariant Synthesis with LLM (perhaps with other\ntools) remain an open problem, which we leave for\nfuture research.\nStability of LLM predictions.\nDue to the\nstochastic (and nondeterministic 3) nature of the\nLLM, especially in higher temp, we observe un-\nstable generation from the LLM. Nevertheless, we\nevaluated the results from one sample run from\ngpt-3.5-turbo and one from gpt-4 as case stud-\nies. While we acknowledge the possibility of un-\nstable behavior, similarity in the performance trend\n(i.e., iRank\u2019s performance improvement over LLM\nand expected ranks, also over raw embeddings)\ngive us confidence about the impact of iRank.\nReferences\nRajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo MK\nMartin, Mukund Raghothaman, Sanjit A Seshia,\nRishabh Singh, Armando Solar-Lezama, Emina Tor-\nlak, and Abhishek Udupa. 2013. Syntax-guided syn-\nthesis. IEEE.\n3https://platform.openai.com/docs/guides/gpt/\nwhy-are-model-outputs-inconsistent\nThomas Ball, Rupak Majumdar, Todd D. Millstein, and\nSriram K. Rajamani. 2001. Automatic predicate ab-\nstraction of C programs. In Proceedings of the 2001\nACM SIGPLAN Conference on Programming Lan-\nguage Design and Implementation (PLDI), Snowbird,\nUtah, USA, June 20-22, 2001, pages 203\u2013213. ACM.\nBruno Blanchet, Patrick Cousot, Radhia Cousot, J\u00e9rome\nFeret, Laurent Mauborgne, Antoine Min\u00e9, David\nMonniaux, and Xavier Rival. 2003. A static analyzer\nfor large safety-critical software. In Proceedings of\nthe ACM SIGPLAN 2003 Conference on Program-\nming Language Design and Implementation, PLDI\n\u201903, page 196\u2013207, New York, NY, USA. Association\nfor Computing Machinery.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021.\nEvaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nMichael A. Col\u00f3n, Sriram Sankaranarayanan, and\nHenny B. Sipma. 2003. Linear invariant generation\nusing non-linear constraint solving. In Computer\nAided Verification, pages 420\u2013432, Berlin, Heidel-\nberg. Springer Berlin Heidelberg.\nPatrick Cousot and Radhia Cousot. 1977. Abstract inter-\npretation: A unified lattice model for static analysis\nof programs by construction or approximation of\nfixpoints. In Proceedings of the 4th ACM SIGACT-\nSIGPLAN Symposium on Principles of Programming\nLanguages, POPL \u201977, page 238\u2013252, New York,\nNY, USA. Association for Computing Machinery.\nLeonardo De Moura and Nikolaj Bj\u00f8rner. 2008. Z3:\nAn efficient smt solver. In Tools and Algorithms for\nthe Construction and Analysis of Systems: 14th In-\nternational Conference, TACAS 2008, Held as Part\nof the Joint European Conferences on Theory and\nPractice of Software, ETAPS 2008, Budapest, Hun-\ngary, March 29-April 6, 2008. Proceedings 14, pages\n337\u2013340. Springer.\nIsil Dillig, Thomas Dillig, Boyang Li, and Ken McMil-\nlan. 2013. Inductive invariant generation via abduc-\ntive inference. Acm Sigplan Notices, 48(10):443\u2013\n456.\n6\nGrigory Fedyukovich and Rastislav Bod\u00edk. 2018. Accel-\nerating syntax-guided invariant synthesis. In Tools\nand Algorithms for the Construction and Analysis\nof Systems, pages 251\u2013269, Cham. Springer Interna-\ntional Publishing.\nCormac Flanagan and K. Rustan M. Leino. 2001. Hou-\ndini, an annotation assistant for esc/java. In FME\n2001: Formal Methods for Increasing Software Pro-\nductivity, International Symposium of Formal Meth-\nods Europe, Berlin, Germany, March 12-16, 2001,\nProceedings, volume 2021 of Lecture Notes in Com-\nputer Science, pages 500\u2013517. Springer.\nCormac Flanagan and Shaz Qadeer. 2002. Predicate\nabstraction for software verification. SIGPLAN Not.,\n37(1):191\u2013202.\nPranav Garg, Daniel Neider, P. Madhusudan, and Dan\nRoth. 2016. Learning invariants using decision trees\nand implication counterexamples. In Proceedings\nof the 43rd Annual ACM SIGPLAN-SIGACT Sym-\nposium on Principles of Programming Languages,\nPOPL 2016, St. Petersburg, FL, USA, January 20 -\n22, 2016, pages 499\u2013512. ACM.\nSusanne Graf and Hassen Sa\u00efdi. 1997. Construction of\nabstract state graphs with PVS. In Computer Aided\nVerification, 9th International Conference, CAV \u201997,\nHaifa, Israel, June 22-25, 1997, Proceedings, volume\n1254 of Lecture Notes in Computer Science, pages\n72\u201383. Springer.\nSumit Gulwani, Saurabh Srivastava, and Ramarathnam\nVenkatesan. 2009.\nConstraint-based invariant in-\nference over predicate abstraction. In Verification,\nModel Checking, and Abstract Interpretation, pages\n120\u2013135, Berlin, Heidelberg. Springer Berlin Heidel-\nberg.\nThomas A. Henzinger, Ranjit Jhala, Rupak Majumdar,\nand Kenneth L. McMillan. 2004. Abstractions from\nproofs. In Proceedings of the 31st ACM SIGPLAN-\nSIGACT Symposium on Principles of Programming\nLanguages, POPL 2004, Venice, Italy, January 14-16,\n2004, pages 232\u2013244. ACM.\nCharles Antony Richard Hoare. 1969. An axiomatic\nbasis for computer programming. Communications\nof the ACM, 12(10):576\u2013580.\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020.\nDense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nShuvendu K. Lahiri and Randal E. Bryant. 2007. Predi-\ncate abstraction with indexed predicates. ACM Trans.\nComput. Logic, 9(1):4\u2013es.\nShuvendu K. Lahiri, Shaz Qadeer, Juan P. Galeotti,\nJan W. Voung, and Thomas Wies. 2009. Intra-module\ninference. In Computer Aided Verification, 21st Inter-\nnational Conference, CAV 2009, Grenoble, France,\nJune 26 - July 2, 2009. Proceedings, volume 5643 of\nLecture Notes in Computer Science, pages 493\u2013508.\nSpringer.\nK Rustan M Leino. 2010. Dafny: An automatic pro-\ngram verifier for functional correctness. In Logic\nfor Programming, Artificial Intelligence, and Reason-\ning: 16th International Conference, LPAR-16, Dakar,\nSenegal, April 25\u2013May 1, 2010, Revised Selected\nPapers 16, pages 348\u2013370. Springer.\nShang-Wei Lin, Jun Sun, Hao Xiao, Yang Liu, David\nSan\u00e1n, and Henri Hansen. 2017.\nFib: Squeez-\ning loop invariants by interpolation between for-\nward/backward predicate transformers. In 2017 32nd\nIEEE/ACM International Conference on Automated\nSoftware Engineering (ASE), pages 793\u2013803. IEEE.\nKenneth L. McMillan. 2003.\nInterpolation and sat-\nbased model checking. In Computer Aided Verifi-\ncation, 15th International Conference, CAV 2003,\nBoulder, CO, USA, July 8-12, 2003, Proceedings,\nvolume 2725 of Lecture Notes in Computer Science,\npages 1\u201313. Springer.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nSaswat Padhi, Rahul Sharma, and Todd Millstein.\n2017.\nLoopinvgen: A loop invariant generator\nbased on precondition inference.\narXiv preprint\narXiv:1707.02029.\nSaswat Padhi, Rahul Sharma, and Todd D. Millstein.\n2016.\nData-driven precondition inference with\nlearned features. In Proceedings of the 37th ACM\nSIGPLAN Conference on Programming Language\nDesign and Implementation, PLDI 2016, Santa Bar-\nbara, CA, USA, June 13-17, 2016, pages 42\u201356.\nKexin Pei, David Bieber, Kensen Shi, Charles Sutton,\nand Pengcheng Yin. 2023. Can large language mod-\nels reason about program invariants?\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nGabriel Ryan, Justin Wong, Jianan Yao, Ronghui Gu,\nand Suman Jana. 2020. Cln2inv: Learning loop in-\nvariants with continuous logic networks. In Interna-\ntional Conference on Learning Representations.\nRahul Sharma and Alex Aiken. 2016. From invariant\nchecking to invariant inference using randomized\nsearch. Formal Methods in System Design, 48:235\u2013\n256.\n7\nExperiment\ni+ ranks\nV@K (%)\nTime(s) (Mean/Median)\nMean\nMedian\nK=1\nK=5\nk=10\nEmbed\nRanking\nVerification\nTotal\nLLM-ranks\n189.78\n62.00\n5.2\n11.6\n18.4\n0 / 0\n0 / 0\n7.33 / 2.23\n7.33 / 2.23\nExpected ranks\n95.35\n31.02\n8.0\n19.2\n25.2\n0 / 0\n0 / 0\n3.67 / 1.12\n3.67 / 1.12\nTF-IDF\n103.45\n24.00\n17.6\n32.0\n38.8\n0 / 0\n0.05 / 0.02\n3.83 / 0.94\n3.88 / 1.00\nEmb.\nAda\n115.89\n31.50\n11.2\n21.6\n30.0\n1.29 / 0.42\n0.05 / 0.01\n4.43 / 1.23\n5.78 / 1.82\nDavinci\n120.02\n32.00\n10.4\n20.8\n33.6\n9.17 / 3.02\n0.45 / 0.15\n4.79 / 1.20\n14.41 / 4.48\niRank\nAda\n38.78\n5.00\n28.0\n51.2\n60.8\n1.29 / 0.42\n0.06 / 0.02\n1.64 / 0.19\n2.98 / 0.97\nDavinci\n34.48\n4.00\n29.2\n52.8\n62.8\n9.17 / 3.02\n0.48 / 0.15\n1.28 / 0.16\n10.93 / 3.68\n(a) Invariants generated by gpt-3.5-turbo .\nExperiment\ni+ ranks\nV@K (%)\nTime(s) (Mean/Median)\nMean\nMedian\nK=1\nK=5\nk=10\nEmbed\nRanking\nVerification\nTotal\nLLM-ranks\n39.20\n9.00\n17.6\n40.4\n51.6\n0 / 0\n0 / 0\n1.61 / 0.24\n1.61 / 0.24\nExpected ranks\n20.23\n4.96\n31.9\n52.1\n65.4\n0 / 0\n0 / 0\n0.83 / 0.19\n0.83 / 0.19\nTF-IDF\n24.16\n3.00\n32.00\n45.6\n53.6\n0 / 0\n0.01 / 0.006\n0.80 / 0.11\n0.81 / 0.12\nEmb.\nAda\n20.69\n5.50\n26.6\n51.1\n64.9\n0.23 / 0.06\n0.01 / 0.004\n0.67 / 0.19\n0.94 / 0.26\nDavinci\n23.56\n5.00\n27.7\n52.1\n63.3\n1.93 / 0.48\n0.12 / 0.03\n0.75 / 0.16\n2.80 / 0.77\niRank\nAda\n13.18\n2.00\n44.7\n74.4\n81.4\n0.25 / 0.06\n0.02 / 0.004\n0.44 / 0.06\n0.71 / 0.16\nDavinci\n11.96\n2.00\n44.7\n71.8\n81.9\n1.93 / 0.48\n0.13 / 0.03\n0.74 / 0.06\n2.80 / 0.72\n(b) Invariants generated by gpt-4 .\nTable 2: Comparison between different ranking strategies for re-ranking the invariants generated by different LLMs.\nXujie Si, Hanjun Dai, Mukund Raghothaman, Mayur\nNaik, and Le Song. 2018. Learning loop invariants\nfor program verification. Advances in Neural Infor-\nmation Processing Systems, 31.\nNikhil Swamy, Juan Chen, C\u00e9dric Fournet, Pierre-\nYves Strub, Karthikeyan Bhargavan, and Jean Yang.\n2011. Secure distributed programming with value-\ndependent types. In Proceedings of the 16th ACM\nSIGPLAN International Conference on Functional\nProgramming, ICFP \u201911, page 266\u2013278, New York,\nNY, USA. Association for Computing Machinery.\nJianan Yao, Gabriel Ryan, Justin Wong, Suman Jana,\nand Ronghui Gu. 2020. Learning nonlinear loop in-\nvariants with gated continuous logic networks. In\nProceedings of the 41st ACM SIGPLAN Conference\non Programming Language Design and Implementa-\ntion, pages 106\u2013120.\nA\nFurther Details of iRank\nIn this section, we present a comprehensive\noverview of the operational workflow of iRank,\nas visualized in Figure 1.\nA.1\nTraining iRank\nAs elucidated in Section 4, the training of iRank\nnecessitates a dataset containing invariant synthesis\nproblems, akin to those illustrated in Figure 4. Each\nproblem in the training dataset requires at least one\ncorrect invariant and a set of incorrect invariants,\nall expressed in the SyGus format (refer to Figure 5\nfor an example). We employ the specified embed-\nding model, namely text-embedding-ada-002 or\nExperiment\ni+ ranks\nV@K (%)\nMean\nMedian\nK=1\nK=5\nk=10\nExpected ranks\nOriginal\n95.35\n31.02\n8.0\n19.2\n25.2\nDeduplicated\n65.24\n24.07\n8.4\n22.8\n31.2\niRank-ada\nOriginal\n38.78\n5.00\n28.0\n51.2\n60.8\nDeduplicated\n18.79\n4.00\n28.4\n56.0\n65.6\n(a) Invariants generated by gpt-3.5-turbo .\nExperiment\ni+ ranks\nV@K (%)\nMean\nMedian\nK=1\nK=5\nk=10\nExpected ranks\nOriginal\n20.23\n4.96\n31.9\n52.1\n65.4\nDeduplicated\n13.99\n4.89\n31.4\n53.7\n72.8\niRank-ada\nOriginal\n13.18\n2.00\n44.7\n74.4\n81.4\nDeduplicated\n8.73\n2.00\n46.8\n77.1\n86.7\n(b) Invariants generated by gpt-4-turbo .\nTable 3: Ranking result of correct invariant by de-duplicating\nsemantic equivalent candidates\ndavinci-similarity, to acquire initial embed-\ndings for both the problems and candidate solutions.\nThese initial embeddings undergo transformation\nvia a three-layered fully connected feedforward\nnetwork to yield transformed embeddings. The\ntraining objective is twofold: minimize the dis-\ntance between the transformed embedding of the\nproblem and the corresponding correct solutions,\nwhile maximizing the distance from incorrect ones.\nOnce this model is trained, it is employed to rank\nthe candidate invariants generated by the LLM or\nany other generator.\nA.2\nRanking with iRank\nUpon successful training of the transformation\nnetwork within iRank, it is used for ranking pur-\n8\nProblem Statistics\nTotal Problems\n541\nProblem Types Statistics\nLinear Integer (LIA)\n496 (91.68%)\nNon-Linear Integer (NIA)\n27 (4.99%)\nArray Linear Integer (ALIA)\n18 (3.33%)\nProblem Semantics Statistics\nNumber of functions\nMin = 3, Max = 9\nNumber of Variables\nMin = 2, Max = 90\nVariable types\nInteger = 80.31%\nBoolean = 19.31%\nArray[Integer] = 0.36%\nArray[Boolean] = 0.03%\nOperator Statistics (of the correct invariants)\nConjuctions\n43% (of all operators)\n1.12 (avg. per example)\nDisjunctions\n20% (of all operators)\n0.53 (avg. per example)\nNegations\n37% (of all operators)\n0.96 (avg. per example)\nAddition/Subraction\n43% (of all operators)\n0.5 (avg. per example)\nMultiplication/Division\n3.1% (of all operators)\n0.18 (avg. per example)\nLogical Comparison\n88.5% (of all operators)\n5.25 (avg. per example)\nLength Statistics\nProblem Length\nMin = 92 (N), 88 (T)\nMax = 2732 (N), 3146 (T)\nMean = 740 (N), 922 (T)\nInvariant Length (gpt-3.5-turbo)\nMin = 18 (N), 15 (T)\nMax = 605 (N), 506 (T)\nMean = 110 (N), 125 (T)\nInvariant Length (gpt-4)\nMin = 18 (N), 15 (T)\nMax = 513 (N), 488 (T)\nMean = 90 (N), 102 (T)\nN = Tokenized with NLTK\nT = Tokenized with TikToken\nTable 4: Stattistics of the experimental data\nposes. To rank the candidate invariants, iRank\ninitially extracts the initial embedding of the\nproblem and a list of solutions, using the same\nembedding model (text-embedding-ada-002 or\ndavinci-similarity) as employed during train-\ning. The trained transformation network is then\nused to transform these embeddings. These trans-\nformed embeddings serve as vectors for the re-\nranking process, where iRank calculates the cosine\nsimilarity between the transformed embedding of\nthe problem and each of the candidate solutions.\nThe candidate solutions are then sorted and re-\nturned based on their similarity with the problem.\nA.3\nData Details and Training\nHyperparameters\nTable 4 shows the statistics of the data we used to\nexperimentally evaluate iRank. Table 5 shows the\nhyperparameters for the models and training we\nNumber of Layers\n3\nHidden Size\n1536 (text-embedding-ada-002)\n12288 (davinci-similarity)\nOptimizer\nAdam\n# Training Epochs\n20\nWeight Decay\n0.001\nMax Gradient Norm\n1.0\nLearning Rate\n5 \u2217 105\nLR Scheduler Type\nLinear\nWarmup Steps\n500\nTable 5: Hyperparameter for Model and Training\nused in this study.\nB\nDetailed Results\nIn addition to comparing the number of Z3 calls,\nwe compared the wall clock time. Table 2 shows\na comparison of time (as an extension of Table 1).\nWe conducted all the experiments in 24 cores\nAMD Epyc 7V13 Linux server running on Linux\nUbuntu 20.04 LTS with 220 GB RAM, and a sin-\ngle NVIDIA A100-PCIE-80GB GPU. For LLM-\nRanks, Expected ranks there is no embedding and\nranking, thus the verification time is the bottleneck.\nFor TF-IDF, while there is no embedding time, the\nis a little bit of ranking time.\nThe Ada embedding time in iRank was very\nsmall compared to the Davinci embedding, thus,\nin the case of iRank-ada, embedding and ranking\ntime was offset by the time iRank reduced in the\nverification effort. In contrast, the Davinci embed-\nding in iRank is more expensive than the reduc-\ntion in the verification time, resulting in a worse\nwall clock time performance than the LLM ranks.\nWe conjecture that the text-embedding-ada-002\n(embedding dim = 1536) is a smaller model com-\npared to davinci-similarity (embedding dim =\n12188), thus requiring significantly longer time to\nembed an input sequence (problem description or\ninvariant).\nIt is important to note here that, this experiment\nis only meant for an illustration of potential threats\nto iRank, and is dependent on a lot of variables,\nincluding, but not limited to OpenAI subscription\nmaximum rate limit, network latency for initial\nembeddings, etc.\nIn addition, we analyzed the generated invariant\ncandidates from LLMs, and removed any semantic\nduplicates. Given two invariant candidates ia and\nib parameterized by set of variables {v1, . . . vn},\n9\nwe define semantic equivalence as,\n\u2200v1, . . . , vn : ia(v1, . . . , vn) \u21d4 ib(v1, . . . , vn)\nFor comparing equivalence of two candidate invari-\nants, we make one call to the z3. Such a seman-\ntic deduplication requires comparison of a newly\ngenerated candidate invariant with all previous can-\ndidates, necessitating \u0398(n2) z3 calls, just to dedu-\nplicate. Table 3 shows the results on deduplicated\ncandidates in comparison with the original list of\ninvariants. As expected, after deduplicating, the\nexpected ranks improves. Interestingly, even in\nthe list of candidates where all candidates are se-\nmantically unique, iRank improves the rank of the\ncorrect invariant, resulting in higher V @K.\nC\nIllustrative example\nAs an illustration of our proposed approach, we\npresent an example from FiB (Lin et al., 2017)\nbenchmark 4. The problem is represented in SyGus\nformat as shown in Figure 4.\n(set-logic LIA)\n(synth-inv inv_fun ((i Int) (n Int) (a Int) (b Int)))\n(define-fun pre_fun ((i Int) (n Int) (a Int) (b Int)) Bool\n    (and\n        (= i 0) (= a 0) (= b 0) (>= n 0)\n    )\n)\n(define-fun trans_fun ((i Int) (n Int) (a Int) (b Int)\n        (i! Int) (n! Int) (a! Int) (b! Int)) Bool\n    (or\n        (and\n            (< i n) (= i! (+ i 1)) (= a! (+ a 1))\n            (= b! (+ b 2)) (= n! n)\n        )\n        (and\n            (< i n) (= i! (+ i 1)) (= a! (+ a 2))\n            (= b! (+ b 1)) (= n! n)\n        )\n        (and\n            (>= i n) (= i! i) (= a! a) (= b! b) (= n! n)\n        )\n    )\n)\n(define-fun post_fun ((i Int) (n Int) (a Int) (b Int)) Bool\n    (=> (not (< i n)) (= (+ a b) (+ (+ n n) n))))\n(inv-constraint inv_fun pre_fun trans_fun post_fun)\n(check-synth)\nFigure 4: Invariant synthesis problem in FiB-8.sl\nWe create the following prompt to call LLMs.\nHere is a loop invariant synthesis problem\nin SyGus format.\n<<<<The problem definition from above >>>>>\n4https://github.com/spencerxiao/\nase2017-results-and-tools/tree/master/FiB_Tool/\nbenchmarks\nSynthesize a necessary and sufficient invariant.\nStart the invariant with\n\"(define -fun inv_fun ((i Int) (n Int) (a Int)\n(b Int)) Bool (\" and end with \")\".\nSurround only the invariant with <code > and\n</code >. You don't need to explain the invariant ,\njust synthesize it.\nThe gpt-3.5-turbo model generated invariant\nshown in Figure 5 after 144 unsuccessful attempts.\n(define-fun inv_fun ((i Int) (n Int) (a Int) (b Int))\nBool\n    (or\n        (and (<= i n) (= (+ a b) (* 3 i)))\n        (and (> i n) (= (+ a b) (* 3 n)))\n    )\n)\nFigure 5: Correct invariant generated by gpt-3.5-turbo.\nThe gpt-4 model generated the invariant shown\nin Figure 6 after 2 unsuccessful attempts.\n(define-fun inv_fun ((i Int) (n Int) (a Int) (b Int))\nBool\n    (and (<= i n) (= (+ a b) (* i 3)))\n)\nFigure 6: Correct invariant generated by gpt-4.\niRank trained based on text-embedding-ada-\n002 repositioned the gpt-3.5-turbo at 8th posi-\ntion in the list and the gpt-4 generated correct\ninvariant in position 2. Note that we show this\nexample only for illustration purposes.\nD\nExperiment on Expected re-ranking\nThe list of invariants generated by LLM as a ranked\nlist could be unstable and susceptible to variations\nin performance across different experiments. Thus,\nas described in Section 5.1, we estimate the ex-\npected ranks by randomly permutating the list. Ide-\nally, to get a perfect estimation, we should consider\nall possible permutations of the list, which can be\nvery expensive (exponential order on the number\nof elements in the list). Figure 7 shows ablation\nof mean rank w.r.t. the number of random permuta-\ntions. As we can see, with a gradual increase in the\nnumber of permutations, the variance in the met-\nrics gradually reduces, i.e., the metrics converges.\nThroughout the paper, we set the number of permu-\ntations to be 100 for estimating the expected rank\nmetrics.\n10\n5\n10 15 20 25 30 35 40 45 50 55 60 70 80 90 100\nNumber of Random Permutations\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\n102.5\nMean Rank\n(a) gpt-3.5-turbo\n5\n10 15 20 25 30 35 40 45 50 55 60 70 80 90 100\nNumber of Random Permutations\n18\n19\n20\n21\n22\nMean Rank\n(b) gpt-4\nFigure 7: Result stabilization with an increasing number of\nrandom permutations\nE\nVisualization of the impact of training\nin iRank\nFigure 8 show a t-SNE plot of the raw LLM em-\nbeddings and the transformed embeddings for a\nfew example problems. For the first three exam-\nples(Figures 8a, 8b, 8c, respectively), iRank brings\nthe correct invariant closer to the problem than any\nother invariants. For the example presented in Fig-\nure 8d, iRank could not make the correct invariant\nas the closest to the problem. While there are cases\nwhere iRank\u2019s transformation fails to bring the cor-\nrect invariant in the closest proximity, in most cases,\nit can bring correct invariants closer to the trans-\nformed problem embedding, as corroborated by the\nresults in Appendix B\n11\nProblem\nCorrect Invariant\nIncorrect Invariant\nW/O training\nProblem\nCorrect Invariant\nIncorrect Invariant\nWith training\n(a) Example - 1\nProblem\nCorrect Invariant\nIncorrect Invariant\nW/O training\nProblem\nCorrect Invariant\nIncorrect Invariant\nWith training\n(b) Example - 2\nProblem\nCorrect Invariant\nIncorrect Invariant\nW/O training\nProblem\nCorrect Invariant\nIncorrect Invariant\nWith training\n(c) Example - 3\nProblem\nCorrect Invariant\nIncorrect Invariant\nW/O training\nProblem\nCorrect Invariant\nIncorrect Invariant\nWith training\n(d) Example - 4\nFigure 8: t-SNE plots of embeddings with and without training for a few example problems. The number of incorrect invariants\nis downsampled for better visualization clarity.\n12\n"
  }
]