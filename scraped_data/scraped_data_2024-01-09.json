[
  {
    "title": "Mixtral of Experts",
    "link": "https://arxiv.org/pdf/2401.04088.pdf",
    "upvote": "139",
    "text": "Mixtral of Experts\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,\nBlanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,\nEmma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour,\nGuillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux,\nPierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao,\nTh\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, William El Sayed\nAbstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language\nmodel. Mixtral has the same architecture as Mistral 7B, with the difference\nthat each layer is composed of 8 feedforward blocks (i.e. experts). For every\ntoken, at each layer, a router network selects two experts to process the current\nstate and combine their outputs. Even though each token only sees two experts,\nthe selected experts can be different at each timestep. As a result, each token\nhas access to 47B parameters, but only uses 13B active parameters during\ninference. Mixtral was trained with a context size of 32k tokens and it outperforms\nor matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks.\nIn\nparticular, Mixtral vastly outperforms Llama 2 70B on mathematics, code\ngeneration, and multilingual benchmarks.\nWe also provide a model fine-\ntuned to follow instructions, Mixtral 8x7B \u2013 Instruct, that surpasses GPT-3.5\nTurbo, Claude-2.1, Gemini Pro, and Llama 2 70B \u2013 chat model on human bench-\nmarks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src\nWebpage: https://mistral.ai/news/mixtral-of-experts/\n1\nIntroduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights,\nlicensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As\nit only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low\nbatch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward\nblock picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router\nnetwork chooses two of these groups (the \u201cexperts\u201d) to process the token and combine their output\nadditively. This technique increases the number of parameters of a model while controlling cost and\nlatency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches\nor exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\narXiv:2401.04088v1  [cs.LG]  8 Jan 2024\nFigure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The\nlayer\u2019s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard\nfeedforward block as in a vanilla transformer architecture.\nMixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require\nmultilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments\nshow that Mixtral is able to successfully retrieve information from its context window of 32k tokens,\nregardless of the sequence length and the location of the information in the sequence.\nWe also present Mixtral 8x7B \u2013 Instruct, a chat model fine-tuned to follow instructions using\nsupervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses\nthat of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B \u2013 chat model on human evaluation\nbenchmarks. Mixtral \u2013 Instruct also demonstrates reduced biases, and a more balanced sentiment\nprofile in benchmarks such as BBQ, and BOLD.\nWe release both Mixtral 8x7B and Mixtral 8x7B \u2013 Instruct under the Apache 2.0 license1, free for\nacademic and commercial usage, ensuring broad accessibility and potential for diverse applications.\nTo enable the community to run Mixtral with a fully open-source stack, we submitted changes to\nthe vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also\nallows the deployment of vLLM endpoints on any instance in the cloud.\n2\nArchitectural details\nParameter\nValue\ndim\n4096\nn_layers\n32\nhead_dim\n128\nhidden_dim\n14336\nn_heads\n32\nn_kv_heads\n8\ncontext_len\n32768\nvocab_size\n32000\nnum_experts\n8\ntop_k_experts\n2\nTable 1: Model architecture.\nMixtral is based on a transformer architecture [31] and uses the same\nmodifications as described in [18], with the notable exceptions that Mix-\ntral supports a fully dense context length of 32k tokens, and the feed-\nforward blocks are replaced by Mixture-of-Expert layers (Section 2.1).\nThe model architecture parameters are summarized in Table 1.\n2.1\nSparse Mixture of Experts\nWe present a brief overview of the Mixture of Experts layer (Figure 1).\nFor a more in-depth overview, see [12]. The output of the MoE module\nfor a given input x is determined by the weighted sum of the outputs\nof the expert networks, where the weights are given by the gating\nnetwork\u2019s output. i.e. given n expert networks {E0, Ei, ..., En\u22121}, the\noutput of the expert layer is given by:\nn\u22121\nX\ni=0\nG(x)i \u00b7 Ei(x).\nHere, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x)\nis the output of the i-th expert network. If the gating vector is sparse, we can avoid computing\nthe outputs of experts whose gates are zero. There are multiple alternative ways of implementing\nG(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the\nTop-K logits of a linear layer [28]. We use\nG(x) := Softmax(TopK(x \u00b7 Wg)),\nwhere (TopK(\u2113))i := \u2113i if \u2113i is among the top-K coordinates of logits \u2113 \u2208 Rn and (TopK(\u2113))i := \u2212\u221e\notherwise. The value of K \u2013 the number of experts used per token \u2013 is a hyper-parameter that modu-\nlates the amount of compute used to process each token. If one increases n while keeping K fixed, one\n1https://mistral.ai/news/mixtral-of-experts/\n2\ncan increase the model\u2019s parameter count while keeping its computational cost effectively constant.\nThis motivates a distinction between the model\u2019s total parameter count (commonly referenced as the\nsparse parameter count), which grows with n, and the number of parameters used for processing an\nindividual token (called the active parameter count), which grows with K up to n.\nMoE layers can be run efficiently on single GPUs with high performance specialized kernels. For\nexample, Megablocks [13] casts the feed-forward network (FFN) operations of the MoE layer as large\nsparse matrix multiplications, significantly enhancing the execution speed and naturally handling\ncases where different experts get a variable number of tokens assigned to them. Moreover, the\nMoE layer can be distributed to multiple GPUs through standard Model Parallelism techniques, and\nthrough a particular kind of partitioning strategy called Expert Parallelism (EP) [28]. During the MoE\nlayer\u2019s execution, tokens meant to be processed by a specific expert are routed to the corresponding\nGPU for processing, and the expert\u2019s output is returned to the original token location. Note that EP\nintroduces challenges in load balancing, as it is essential to distribute the workload evenly across the\nGPUs to prevent overloading individual GPUs or hitting computational bottlenecks.\nIn a Transformer model, the MoE layer is applied independently per token and replaces the\nfeed-forward (FFN) sub-block of the transformer block. For Mixtral we use the same SwiGLU\narchitecture as the expert function Ei(x) and set K = 2. This means each token is routed to two\nSwiGLU sub-blocks with different sets of weights. Taking this all together, the output y for an input\ntoken x is computed as:\ny =\nn\u22121\nX\ni=0\nSoftmax(Top2(x \u00b7 Wg))i \u00b7 SwiGLUi(x).\nThis formulation is similar to the GShard architecture [21], with the exceptions that we replace all\nFFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a\nmore elaborate gating strategy for the second expert assigned to each token.\n3\nResults\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair\ncomparison. We measure performance on a wide variety of tasks categorized as follow:\n\u2022 Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27],\nOpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]\n\u2022 World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19]\n\u2022 Reading Comprehension (0-shot): BoolQ [7], QuAC [5]\n\u2022 Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4\n\u2022 Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot)\n\u2022 Popular aggregated results: MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34]\n(3-5-shot, English multiple-choice questions only)\nFigure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks. All models\nwere re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mixtral outperforms or\nmatches Llama 2 70B on all benchmarks. In particular, it is vastly superior in mathematics and code generation.\n3\nModel\nActive\nParams MMLU\nHellaS\nWinoG\nPIQA\nArc-e\nArc-c\nNQ\nTriQA HumanE MBPP\nMath\nGSM8K\nLLaMA 2 7B\n7B\n44.4%\n77.1%\n69.5%\n77.9%\n68.7%\n43.2%\n17.5%\n56.6%\n11.6%\n26.1%\n3.9%\n16.0%\nLLaMA 2 13B\n13B\n55.6%\n80.7%\n72.9%\n80.8%\n75.2%\n48.8%\n16.7%\n64.0%\n18.9%\n35.4%\n6.0%\n34.3%\nLLaMA 1 33B\n33B\n56.8%\n83.7%\n76.2%\n82.2%\n79.6%\n54.4%\n24.1%\n68.5%\n25.0%\n40.9%\n8.4%\n44.1%\nLLaMA 2 70B\n70B\n69.9%\n85.4%\n80.4%\n82.6%\n79.9%\n56.5%\n25.4%\n73.0%\n29.3%\n49.8%\n13.8%\n69.6%\nMistral 7B\n7B\n62.5%\n81.0%\n74.2%\n82.2%\n80.5%\n54.9%\n23.2%\n62.5%\n26.2%\n50.2%\n12.7%\n50.0%\nMixtral 8x7B\n13B\n70.6%\n84.4%\n77.2%\n83.6%\n83.1%\n59.7%\n30.6%\n71.5%\n40.2%\n60.7%\n28.4%\n74.4%\nTable 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on\nalmost all popular benchmarks while using 5x fewer active parameters during inference.\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension,\nmath and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B\non all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It\nis also vastly superior to Llama 2 70B on code and math.\nDetailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported\nin Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different\ncategories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a\nsuperior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand\nMixtral models\u2019 efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture-\nof-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active\nparameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly\nproportional to the inference compute cost, but does not consider the memory costs and hardware\nutilization. The memory costs for serving Mixtral are proportional to its sparse parameter count,\n47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer\nintroduces additional overhead due to the routing mechanism and due to the increased memory loads\nwhen running more than one expert per device. They are more suitable for batched workloads where\none can reach a good degree of arithmetic intensity.\nComparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B\ncompared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the\ntwo other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller\ncapacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest\nGPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4\nLLaMA 2 70B\nGPT-3.5\nMixtral 8x7B\nMMLU\n(MCQ in 57 subjects)\n69.9%\n70.0%\n70.6%\nHellaSwag\n(10-shot)\n87.1%\n85.5%\n86.7%\nARC Challenge\n(25-shot)\n85.1%\n85.2%\n85.8%\nWinoGrande\n(5-shot)\n83.2%\n81.6%\n81.2%\nMBPP\n(pass@1)\n49.8%\n52.2%\n60.7%\nGSM-8K\n(5-shot)\n53.6%\n57.1%\n58.4%\nMT Bench\n(for Instruct Models)\n6.86\n8.32\n8.30\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2\n70B and GPT-3.5 performance on most metrics.\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation\nprotocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2)\non TriviaQA, we do not provide Wikipedia contexts.\n3.1\nMultilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during\npretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while\nmaintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B\nin French, German, Spanish, and Italian, as shown in Table 4.\nActive\nParams\nFrench\nGerman\nSpanish\nItalian\nModel\nArc-c\nHellaS MMLU\nArc-c\nHellaS\nMMLU\nArc-c\nHellaS\nMMLU\nArc-c\nHellaS MMLU\nLLaMA 1 33B\n33B\n39.3%\n68.1%\n49.9%\n41.1%\n63.3%\n48.7%\n45.7%\n69.8%\n52.3%\n42.9% 65.4%\n49.0%\nLLaMA 2 70B\n70B\n49.9%\n72.5%\n64.3%\n47.3%\n68.7%\n64.2%\n50.5%\n74.5%\n66.0%\n49.4% 70.9%\n65.1%\nMixtral 8x7B\n13B\n58.2% 77.4%\n70.9%\n54.3%\n73.0%\n71.5%\n55.4%\n77.6%\n72.5%\n52.8% 75.1% 70.9%\nTable 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag,\nand MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\n3.2\nLong range performance\nTo assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval\ntask introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a\npasskey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a\n100% retrieval accuracy regardless of the context length or the position of passkey in the sequence.\nFigure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases\nmonotonically as the size of the context increases.\nFigure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task\nregardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on\nthe proof-pile dataset decreases monotonically as the context length increases.\n5\n3.3\nBias Benchmarks\nLlama 2 70B\nMixtral 8x7B\nBBQ accuracy\n51.5%\n56.0%\nBOLD sentiment score (avg \u00b1 std)\ngender\n0.293 \u00b1 0.073\n0.323 \u00b10.045\nprofession\n0.218 \u00b1 0.073\n0.243 \u00b1 0.087\nreligious_ideology\n0.188 \u00b1 0.133\n0.144 \u00b1 0.089\npolitical_ideology\n0.149 \u00b1 0.140\n0.186 \u00b1 0.146\nrace\n0.232 \u00b1 0.049\n0.232 \u00b1 0.052\nFigure 5: Bias Benchmarks. Compared Llama 2 70B,\nMixtral presents less bias (higher accuracy on BBQ, lower\nstd on BOLD) and displays more positive sentiment (higher\navg on BOLD).\nTo identify possible flaws to be corrected\nby fine-tuning / preference modeling, we\nmeasure the base model performance on\nBias Benchmark for QA (BBQ) [24] and\nBias in Open-Ended Language Generation\nDataset (BOLD) [10]. BBQ is a dataset\nof hand-written question sets that target\nattested social biases against nine differ-\nent socially-relevant categories: age, dis-\nability status, gender identity, nationality,\nphysical appearance, race/ethnicity, religion,\nsocio-economic status, sexual orientation.\nBOLD is a large-scale dataset that consists\nof 23,679 English text generation prompts\nfor bias benchmarking across five domains.\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report\nthe results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark\n(56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive\nsentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral\ndisplays more positive sentiments than Llama 2, with similar variances within each group.\n4\nInstruction Fine-tuning\nWe train Mixtral \u2013 Instruct using supervised fine-tuning (SFT) on an instruction dataset followed by\nDirect Preference Optimization (DPO) [25] on a paired feedback dataset. Mixtral \u2013 Instruct reaches a\nscore of 8.30 on MT-Bench [33] (see Table 2), making it the best open-weights model as of December\n2023. Independent human evaluation conducted by LMSys is reported in Figure 63 and shows that\nMixtral \u2013 Instruct outperforms GPT-3.5-Turbo, Gemini Pro, Claude-2.1, and Llama 2 70B chat.\nFigure 6: LMSys Leaderboard. (Screenshot from Dec 22, 2023) Mixtral 8x7B Instruct v0.1 achieves an Arena\nElo rating of 1121 outperforming Claude-2.1 (1117), all versions of GPT-3.5-Turbo (1117 best), Gemini Pro\n(1111), and Llama-2-70b-chat (1077). Mixtral is currently the best open-weights model by a large margin.\n3https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n6\n5\nRouting analysis\nIn this section, we perform a small analysis on the expert selection by the router. In particular,\nwe are interested to see if during training some experts specialized to some specific domains (e.g.\nmathematics, biology, philosophy, etc.).\nTo investigate this, we measure the distribution of selected experts on different subsets of The Pile\nvalidation dataset [14]. Results are presented in Figure 7, for layers 0, 15, and 31 (layers 0 and 31\nrespectively being the first and the last layers of the model). Surprisingly, we do not observe obvious\npatterns in the assignment of experts based on the topic. For instance, at all layers, the distribution of\nexpert assignment is very similar for ArXiv papers (written in Latex), for biology (PubMed Abstracts),\nand for Philosophy (PhilPapers) documents.\nOnly for DM Mathematics we note a marginally different distribution of experts. This divergence is\nlikely a consequence of the dataset\u2019s synthetic nature and its limited coverage of the natural language\nspectrum, and is particularly noticeable at the first and last layers, where the hidden states are very\ncorrelated to the input and output embeddings respectively.\nThis suggests that the router does exhibit some structured syntactic behavior. Figure 8 shows\nexamples of text from different domains (Python code, mathematics, and English), where each token\nis highlighted with a background color corresponding to its selected expert. The figure shows that\nwords such as \u2018self\u2019 in Python and \u2018Question\u2019 in English often get routed through the same expert\neven though they involve multiple tokens. Similarly, in code, the indentation tokens are always\nassigned to the same experts, particularly at the first and last layers where the hidden states are more\ncorrelated to the input and output of the model.\nWe also note from Figure 8 that consecutive tokens are often assigned the same experts. In fact, we\nobserve some degree of positional locality in The Pile datasets. Table 5 shows the proportion of con-\nsecutive tokens that get the same expert assignments per domain and layer. The proportion of repeated\n0\n0.05\n0.10\n0.15\n0.20\nlayer: 0\n0\n0.05\n0.10\n0.15\n0.20\nlayer: 15\n0\n1\n2\n3\n4\n5\n6\n7\n0\n0.05\n0.10\n0.15\n0.20\nlayer: 31\nExpert ID\nSelection proportion\nArXiv\nDM Mathematics\nGithub\nGutenberg\nPhilPapers\nPubMed Abstracts\nStackExchange\nWikipedia (en)\nFigure 7: Proportion of tokens assigned to each expert on different domains from The Pile dataset for\nlayers 0, 15, and 31. The gray dashed vertical line marks 1/8, i.e. the proportion expected with uniform\nsampling. Here, we consider experts that are either selected as a first or second choice by the router. A\nbreakdown of the proportion of assignments done in each case cane be seen in Figure 9 in the Appendix.\n7\nFirst choice\nFirst or second choice\nLayer 0\nLayer 15\nLayer 31\nLayer 0\nLayer 15\nLayer 31\nArXiv\n14.0%\n27.9%\n22.7%\n46.5%\n62.3%\n52.9%\nDM Mathematics\n14.1%\n28.4%\n19.7%\n44.9%\n67.0%\n44.5%\nGithub\n14.9%\n28.1%\n19.7%\n49.9%\n66.9%\n49.2%\nGutenberg\n13.9%\n26.1%\n26.3%\n49.5%\n63.1%\n52.2%\nPhilPapers\n13.6%\n25.3%\n22.1%\n46.9%\n61.9%\n51.3%\nPubMed Abstracts\n14.2%\n24.6%\n22.0%\n48.6%\n61.6%\n51.8%\nStackExchange\n13.6%\n27.2%\n23.6%\n48.2%\n64.6%\n53.6%\nWikipedia (en)\n14.4%\n23.6%\n25.3%\n49.8%\n62.1%\n51.8%\nTable 5: Percentage of expert assignment repetitions. We evaluate the proportion of times the same expert is\nassigned to a token i and its following token i+1. We report whether the first chosen expert is the same, or whether\nthe same expert is observed as first or second choice in consecutive tokens. For reference, the expected proportion\nof repetitions in the case of random assignments is 1\n8 = 12.5% for \u201cFirst choice\u201d and 1 \u2212 6\n8\n5\n7 \u2248 46% for \u201cFirst\nand second choice\u201d. Repetitions at the first layer are close to random, but are significantly higher at layers 15\nand 31. The high number of repetitions shows that expert choice exhibits high temporal locality at these layers.\nconsecutive assignments is significantly higher than random for higher layers. This has implications\nin how one might optimize the model for fast training and inference. For example, cases with high\nlocality are more likely to cause over-subscription of certain experts when doing Expert Parallelism.\nConversely, this locality can be leveraged for caching, as is done in [11]. A more complete view of\nthese same expert frequency is provided for all layers and across datasets in Figure 10 in the Appendix.\n6\nConclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the-\nart performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem-\nini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each\ntime step, Mixtral only uses 13B active parameters per token while outperforming the previous best\nmodel using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod-\nels publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de-\nvelopment of new techniques and applications that can benefit a wide range of industries and domains.\nFigure 8: Text samples where each token is colored with the first expert choice. The selection of experts\nappears to be more aligned with the syntax rather than the domain, especially at the initial and final layers.\n8\nAcknowledgements\nWe thank the CoreWeave and Scaleway teams for technical support as we trained our models. We\nare grateful to NVIDIA for supporting us in integrating TensorRT-LLM and Triton and working\nalongside us to make a sparse mixture of experts compatible with TensorRT-LLM.\nReferences\n[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\nlanguage models. arXiv preprint arXiv:2108.07732, 2021.\n[2] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics. arXiv preprint arXiv:2310.10631, 2023.\n[3] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-\nical commonsense in natural language. In Proceedings of the AAAI conference on artificial\nintelligence, pages 7432\u20137439, 2020.\n[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[5] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and\nLuke Zettlemoyer. Quac: Question answering in context. arXiv preprint arXiv:1808.07036,\n2018.\n[6] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan\nHoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified\nscaling laws for routed language models. In International Conference on Machine Learning,\npages 4057\u20134086. PMLR, 2022.\n[7] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\narXiv preprint arXiv:1905.10044, 2019.\n[8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge. arXiv preprint arXiv:1803.05457, 2018.\n[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei\nChang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended\nlanguage generation. In Proceedings of the 2021 ACM conference on fairness, accountability,\nand transparency, pages 862\u2013872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with\noffloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning.\narXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse\ntraining with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.\n[14] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse\ntext for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n[15] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,\nRahul Mazumder, Lichan Hong, and Ed Chi. Dselect-k: Differentiable selection in the mixture\nof experts with applications to multi-task learning. Advances in Neural Information Processing\nSystems, 34:29335\u201329347, 2021.\n9\n[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\n[17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\narXiv preprint arXiv:2103.03874, 2021.\n[18] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n[19] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.\nTriviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension.\narXiv preprint\narXiv:1705.03551, 2017.\n[20] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\nbenchmark for question answering research. Transactions of the Association for Computational\nLinguistics, pages 453\u2013466, 2019.\n[21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-\ntional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n[22] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,\n2018.\n[23] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\nlength for transformers. arXiv preprint arXiv:2305.16300, 2023.\n[24] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp-\nson, Phu Mon Htut, and Samuel R Bowman. Bbq: A hand-built bias benchmark for question\nanswering. arXiv preprint arXiv:2110.08193, 2021.\n[25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\narXiv preprint arXiv:2305.18290, 2023.\n[26] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the ACM, pages 99\u2013106,\n2021.\n[27] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Com-\nmonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\n[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[29] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei.\nChallenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261, 2022.\n[30] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A ques-\ntion answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937,\n2018.\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[32] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n[33] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023.\n10\n[34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\nmodels. arXiv preprint arXiv:2304.06364, 2023.\n[35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai,\nQuoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in\nNeural Information Processing Systems, 35:7103\u20137114, 2022.\n11\n0\n0.1\n0.2\n0.3\nLayer 0 -- Either choice\n0\n0.1\n0.2\n0.3\nLayer 0 -- First choice\n0\n0.1\n0.2\n0.3\nLayer 0 -- Second choice\n0\n0.1\n0.2\n0.3\nLayer 15 -- Either choice\n0\n0.1\n0.2\n0.3\nLayer 15 -- First choice\n0\n0.1\n0.2\n0.3\nLayer 15 -- Second choice\n0\n0.1\n0.2\n0.3\nLayer 31 -- Either choice\n0\n0.1\n0.2\n0.3\nLayer 31 -- First choice\n0\n1\n2\n3\n4\n5\n6\n7\n0\n0.1\n0.2\n0.3\nLayer 31 -- Second choice\nExpert ID\nSelection proportion\nArXiv\nDM Mathematics\nGithub\nGutenberg\nPhilPapers\nPubMed Abstracts\nStackExchange\nWikipedia (en)\nFigure 9: Proportion of tokens assigned to each expert on different subsets from The Pile dataset, separated\nby whether the expert was selected as first or second choice, or either. The \u201cEither choice\u201d case is equivalent\nto Figure 7. The gray dashed vertical line marks 1\n8, i.e. the proportion expected with uniform sampling.\n12\n0.15\n0.20\n0.25\n0.30\n0.35\nFirst choice\n0\n10\n20\n30\n0.5\n0.6\n0.7\nFirst or second choice\nLayer\nProportion of repeated assignments\nsource\nArXiv\nDM Mathematics\nGithub\nGutenberg\nPhilPapers\nPubMed Abstracts\nStackExchange\nWikipedia (en)\nFigure 10: Repeated consecutive assignments per MoE layer. Repeated assignments occur a lot more\noften than they would with uniform assignments (materialized by the dashed lines). Patterns are similar across\ndatasets with less repetitions for DM Mathematics.\n13\n"
  },
  {
    "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
    "link": "https://arxiv.org/pdf/2401.04081.pdf",
    "upvote": "68",
    "text": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nMaciej Pi\u00b4oro 1 2 Kamil Ciebiera 1 3 Krystian Kr\u00b4ol 1 3 Jan Ludziejewski 1 3 Micha\u0142 Krutul 1 3 Jakub Krajewski 1 3\nSzymon Antoniak Piotr Mi\u0142o\u00b4s 1 4 3 Marek Cygan 3 Sebastian Jaszczur 1 3\nAbstract\nState Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging\nthe dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved\nTransformer-based Large Language Models, including recent state-of-the-art open models. We propose that to\nunlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a\nrecent SSM-based model that achieves remarkable performance. Our model, MoE-Mamba, outperforms both\nMamba and baseline Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in\n2.35\u00d7 fewer training steps while preserving the inference performance gains of Mamba against Transformer.\nFigure 1. Log perplexity throughout the training. From top to bottom: Mamba100M; Transformer-MoE100M; MoE-Mamba100M.\n1. Introduction\nLarge Language Models (LLMs) have emerged as a cornerstone in the ongoing AI revolution (Brown et al., 2020; Chowdhery\net al., 2023; Lewkowycz et al., 2022; OpenAI, 2023; Team, 2023). Their remarkable effectiveness is primarily attributed to\nthe Transformer architecture (Vaswani et al., 2017) and training on an internet-wide scale, e.g., (TogetherComputer, 2023).\nYet, questions remain: Should Transformers be the only architecture used for LLMs? Can we scale language models even\nfurther, and if so, how can this be achieved?\nDetailed authors\u2019 contributions are listed in Appendix H. 1IDEAS NCBR 2Polish Academy of Sciences 3University of Warsaw 4Institute\nof Mathematics, Polish Academy of Sciences. Correspondence to: Sebastian Jaszczur <s.jaszczur@uw.edu.pl>.\n1\narXiv:2401.04081v2  [cs.LG]  26 Feb 2024\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nRegarding the first question, State Space Models (SSMs), e.g., (Gu et al., 2022b; 2021; 2022a; Gupta et al., 2022; Li et al.,\n2022; Ma et al., 2022; Orvieto et al., 2023; Smith et al., 2023), have been increasingly gaining attention. This recognition\nis due to their capability for linear-time inference, highly parallelized training, and strong performance in tasks requiring\nlong-context processing, such as those illustrated by the Long Range Arena (Tay et al., 2020). Notably, a recent addition to\nthis category, Mamba (Gu & Dao, 2023), has shown impressive results through its selective mechanism and hardware-aware\ndesign, positioning it as a promising contender to the attention-based Transformer architecture.\nScaling is believed to be a critical factor in developing powerful AI systems (Sutton, 2019). The Mixture of Experts (MoE)\napproach (Jacobs et al., 1991), a set of techniques that enables an increase in model parameters with minimal impact on\ncomputational demands, plays a significant role. Due to their sparse activation, MoEs can be efficiently scaled up to trillions\nof parameters, as demonstrated by Shazeer et al. (2017); Fedus et al. (2022). MoE variants (Fedus et al., 2022; Du et al.,\n2022) are now routinely used in LLMs, as exemplified in the recent Mixtral model (Jiang et al., 2024).\nIn this paper, we advocate that to unlock the potential of SSMs for scaling up, they should be combined with Mixture of\nExperts (MoE). To this end, we introduce MoE-Mamba, a model that combines Mamba (Gu & Dao, 2023) with a Switch\nlayer (Fedus et al., 2022). MoE-Mamba enables efficiency gains of both SSMs and MoE, outperforming Mamba and\nTransformer-MoE, see Figure 1. Through comprehensive studies, we confirm that the effect is robust to the design choices\nand the number of experts. Our results indicate a very promising research direction that may allow scaling SSMs beyond\ntens of billions of parameters and compete with the largest SoTA language models.\nIn summary, our contributions are as follows:\n\u2022 We introduce MoE-Mamba, a model that combines Mamba with a Mixture of Experts layer. MoE-Mamba enables\nefficiency gains of both SSMs and MoE while reaching the same performance as Mamba in 2.35\u00d7 fewer training steps.\n\u2022 Via comprehensive studies, we confirm that the improvement achieved by MoE-Mamba is robust to varying model\nsizes, design choices, and the number of experts.\n\u2022 We explore and compare multiple alternative methods of integrating Mixture of Experts within the Mamba block.\n2. Related Work\nState Space Models and Related Attention-Free Architectures\nState Space Models (SSMs) (Gu et al., 2022b; 2021;\n2022a; Gupta et al., 2022; Li et al., 2022; Ma et al., 2022; Orvieto et al., 2023; Smith et al., 2023) form a family of\narchitectures used for sequence modeling. Stemming from signal processing, these models can be seen as a combination of\nRNNs and CNNs (Gu & Dao, 2023). Although they potentially offer considerable benefits, a number of issues have been\nidentified with SSMs (Gu et al., 2022b), preventing SSMs from becoming the leading architecture in the task of language\nmodeling. However, recent breakthroughs (Gu et al., 2022b; Fu et al., 2023; Smith et al., 2023; Gu & Dao, 2023), have\nallowed deep SSMs to be increasingly competitive against Transformers (Vaswani et al., 2017). In particular, Mamba (Gu &\nDao, 2023), studied in this paper, has shown impressive results through its selective mechanism and hardware-aware design,\nwhich allows scaling to billions of parameters while retaining computational efficiency and strong performance. Besides\nSSMs, numerous other architectures have been proposed that do not rely on the quadratic attention mechanism (Zhai et al.,\n2021; Poli et al., 2023; Sun et al., 2023; Peng et al., 2023).\nMixture of Experts\nMixture of Experts (MoE) is a class of techniques that allow drastically increasing the number of\nparameters of a model without much impact on the FLOPs required for the model\u2019s training and inference. Introduced by\nJacobs et al. (1991); Jordan & Jacobs (1993), MoE was applied in the context of NLP by Shazeer et al. (2017). MoE models\nbenefit from sparse activation - for each token processed, only a subset of the model\u2019s parameters is used. Due to their\ncomputational demands, feed-forward layers in Transformers have become the standard target of various MoE techniques\n(Lepikhin et al., 2020; Fedus et al., 2022; Du et al., 2022; Zoph et al., 2022). Scaling properties and generalization abilities\nof MoE Transformers have been studied more closely by Artetxe et al. (2021); Clark et al. (2022); Krajewski et al. (2024).\nMore recently, MoE models have found their way onto the open scene (Xue et al., 2023; Jiang et al., 2024). In particular,\nthe Mixtral 8\u00d77B model (Jiang et al., 2024) fares comparably to Llama 2 70B (Touvron et al., 2023) while requiring only\naround 1/6 of its inference computational budget.\nSubsequently to the first version of our work, Anthony et al. (2024) presented an architecture similar to MoE-Mamba,\nshowcasing the potential of connecting Mamba with MoE on downstream tasks, which validates our findings. In contrast to\n2\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\ntheir work, we run extensive ablations on the model architecture, number of experts, and other design choices. We also\ninvestigate the potential of integrating conditional computation into the Mamba block.\n3. MoE-Mamba\nIn this section, we present the architecture details of our model, MoE-Mamba, see Figure 2. We start with a brief overview\nof the Mamba architecture, followed by a description of the MoE layer. Our main architecture is presented in Section 3.2,\nwhile sections 3.3 and 3.4 explore its variants and related approaches.\n3.1. Preliminaries\nMamba\nMamba (Gu & Dao, 2023) is a recently proposed SSM-based model that achieves remarkable, Transformer-\nlike performance. By employing a work-efficient parallel scan, Mamba mitigates the impact of the sequential nature of\nrecurrence, whereas fusing GPU operations removes the requirement to materialize the expanded state. Intermediate states\nnecessary for backpropagation are not saved but recomputed during the backward pass, thus reducing memory requirements.\nThe advantages of Mamba over the attention mechanism are especially prominent during inference, as not only is the\ncomputational complexity lowered, but also the memory usage is not dependent on the context length. Figure 3 shows the\ninner structure of a Mamba layer.\nMoE Layer\nIn our work, we follow the well-established (Zhao et al., 2023a; Sanseviero et al., 2023) and easy-to-implement\nSwitch Transformer MoE design (Fedus et al., 2022) and leave consideration of other MoE designs for future work.\nWe assume Nexperts experts {Ei}Nexperts\ni=1\n, each being a trainable feed-forward network with the same number of parameters.\nFor each token embedding x, we calculate scores h(x) = Wx \u2208 RNexperts, where W is a trainable linear projection. These\nare normalized using softmax:\npi(x) =\nexp (h(x)i)\nPNexperts\ni=1\nexp (h(x)i)\n.\nPrior to Switch, top-k routing selecting k > 1 most suitable experts for each token was deemed necessary. However, Switch\nsuccessfully simplifies previous MoE approaches by setting k = 1. Namely, the output of the MoE layer for x is given by:\ny = pIEI(x),\nwhere I = argmaxi pi(x).\nDuring batched execution, e.g., in training, each batch contains N tokens. Following the standard procedure, in a case\nwhere the assignment of tokens to the experts is not perfect, i.e., some expert Ef is selected by more than N/Nexperts tokens\nin the current batch, the excess tokens are dropped and not updated (capacity factor = 1). To further encourage an even\ndistribution of tokens to experts, load balancing loss as described by Fedus et al. (2022) with weight \u03b1 = 0.01 is added to\nthe training objective.\n3.2. MoE-Mamba Architecture\nThe vanilla Mamba architecture consists of multiple Mamba blocks stacked one after another, with each layer\u2019s output being\nadded to the residual stream; see Figure 2. In MoE-Mamba, we interleave Mamba layers with MoE layers (see Figure 2).\nNote that the vanilla Mamba does not use feed-forward layers.\nIn this way, MoE-Mamba separates unconditional processing of every token by the Mamba layer - which can efficiently\nintegrate the whole sequence context into an internal representation - and conditional processing by an MoE layer that can\napply the most relevant expert (and thus the subset of parameters) for each token. The idea of interleaving conditional and\nunconditional processing is used in some MoE-based models, typically by alternating vanilla and MoE feed-forward layers\n(Lepikhin et al., 2020; Fedus et al., 2022).\n3.3. Parallel MoE-Mamba\nApart from interleaving MoE layers with Mamba layers, we explore another design, inspired by Wang (2021) and Chowdhery\net al. (2023) in which MoE layer is executed in parallel with Mamba (see Figure 3). It achieves positive results, albeit worse\nthan MoE-Mamba.\n3\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nFigure 2. Diagrams of the architectures. From the left: vanilla Transformer, Transformer-MoE, Mamba, MoE-Mamba.\nTable 1. Comparison between different architectures. The \u25a125M models were trained on ca. 10B tokens and the \u25a1100M models were\ntrained on ca. 30B tokens. Note that the parameter counts exclude embedding and output (unembedding) layers (for further discussion of\nreporting either non-embedding or all parameters, see Appendix E). The numbers of total and active parameters are not matched exactly\nbetween similarly sized models due to, among other reasons, the MoE models including routers and Mamba layer not containing precisely\n6d2\nmodel parameters - a design choice we did not want to modify. We consider those differences to be too small to be significant for our\nresults.\nModel\n# Parameters\n# Active Parameters\nper Token\nFinal Log\nPerplexity\nSpeedup Over\nVanilla Mamba\n(Training Steps)\nMamba25M\n27M\n27M\n3.34\n1\nMoE-Mamba25M (ours)\n542M\n26M\n3.19\n1.76\nTransformer-MoE25M\n545M\n25M\n3.23\n1.56\nTransformer25M\n25M\n25M\n3.43\n>1\nMamba100M\n121M\n121M\n2.99\n1\nMoE-Mamba100M (ours)\n2439M\n117M\n2.81\n2.35\nTransformer-MoE100M\n2454M\n114M\n2.88\n1.79\n3.4. Modifying Mamba Block\nIn addition to attaching a separate MoE layer to Mamba, we also conducted other experiments, modifying the original block\ndesign by Gu & Dao (2023) to feature conditional MoE computation. Some of the designs show improvements over the\nbaseline architecture and suggest promising future research directions.\n4. Experiments\nIn this section we provide empirical validation of our hypothesis that interleaving Mamba with MoE can improve the\nperformance of Mamba. Our main result, see Figure 1, shows that MoE-Mamba needs 2.35\u00d7 fewer training steps to reach\nthe same performance as Mamba. We also provide a detailed analysis of our design choices.\n4.1. Training Setup\nWe compare MoE-Mamba to three baselines: Mamba, Transformer, and Transformer-MoE. All models in our experiments\nare decoder-only.\n4\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nFigure 3. Diagram of Parallel MoE-Mamba architecture (left) and Mamba Block (right). The outputs of the Gate and Conv Projections\nare E (expansion factor) times bigger than the input, i.e., Conv and SSM operate on vectors \u2208 RE\u00b7dmodel. Vanilla Mamba assumes E = 2\n(Gu & Dao, 2023). Expansion factor E determines how much the input vector is scaled up by Gate and Conv Projection and then scaled\ndown by Output Projection, and because of that, it is also proportional to the number of FLOPs and parameters in the Mamba layer.\nIn the standard Transformer architecture, a single attention layer contains 4d2\nmodel parameters, whereas a feed-forward layer\ncontains 8d2\nmodel parameters. A single Mamba layer contains slightly over 6d2\nmodel (Gu & Dao, 2023) parameters. To be able\nto compare MoE-Mamba to Transformer-based and Mamba baselines, we scale down the size of each expert in our model\n(we set dexpert = 3dmodel). This way, we can keep both the number of blocks and the number of active parameters per token\nroughly the same in all models of similar size. Active parameters denote those used to calculate the output for a given token\n(e.g., typically, only one expert in each MoE layer is active). For a discussion of the relation of active parameters and FLOPs,\nsee Appendix B.\nDue to computational constraints, we perform most of our experiments on smaller, \u25a125M models and validate our findings\non \u25a1100M models.\nWe train the models on C4 dataset (Raffel et al., 2020) on the next token prediction task using cross entropy as the loss\nfunction. We use EMA-smoothed (\u03b1 = 0.001) training log perplexity as the comparison metric for both final loss and\nspeedup measurements as it is a more fine-grained comparison metric than test log perplexity. The test log perplexity\ncomparison for \u25a1100M models can be found in Appendix G. All models use the GPT2 tokenizer (Radford et al., 2019).\nWe tune the learning rate separately for all \u25a125M models and reuse it when training their \u25a1100M counterparts. When training\nTransformer-MoE100M, we divide the learning rate by two due to repeated instabilities. See Appendix A for further details\nand hyperparameters. The main experiments, described in section 4.2, use around 10B tokens for \u25a125M models and around\n30B tokens for \u25a1100M models. The experiments described in further sections use 1B tokens.\n4.2. Main Results\nTable 1 presents the comparison between training results of MoE-Mamba and baselines; see also Figure 1 for log perplexity\ncurves. MoE-Mamba shows a remarkable improvement over the vanilla Mamba model across both model sizes. Notably,\nMoE-Mamba100M was able to perform on par with vanilla Mamba100M with 2.35\u00d7 speedup in terms of processed tokens.\nFor \u25a125M model size, those performance gains are lower, probably due to a lower number of training tokens. More generally,\nwe observe that the gains increase over the training, oscillating around 1.6 \u00d7 \u2212 1.9\u00d7 for \u25a125M models after the initial\ntraining period. Further discussion of the speedup can be found in Appendix D. We observe that MoE-Mamba performs\nbetter than the corresponding Transformer-MoE, which strengthens the findings by Gu & Dao (2023) that Mamba is a\ncompetitive alternative to the Transformer.\n5\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nFigure 4. Smoothed training loss (log perplexity) for a differing number of experts for MoE-Mamba with ca. 26M active non-embedding\nparameters. The final log perplexity improves monotonically as the number of experts increases.\nTable 2. Comparison of different ratios of parameters between Mamba and MoE. The E = 2 corresponds to MoE-Mamba25M. The total\nnumber of parameters in all models is 542M and the number of active parameters per token is 26M.\nRatio\nN act. params\nMamba\n: N act. params\nMoE\nExpansion\nFactor\nE (Mamba)\nExpert\nSize\nNumber\nof\nExperts\n1 : 5\n2\n3\n2560\n19\n2 : 4\n1 2\n3\n2048\n24\n3 : 3\n2\n1536\n32\n4 : 2\n2 2\n3\n1024\n48\n5 : 1\n3 1\n3\n512\n96\n4.3. Optimal Ratio of Active Parameters in Mamba and MoE\nIn this section, we investigate the optimal ratio of active parameters in the Mamba layer to active parameters in the MoE\nlayer while keeping the total number of parameters fixed. Under these constraints, a given ratio determines the so-called\nexpansion factor E of the Mamba layer, the number of experts, and their size as detailed in Table 2 (see also Figure 3 for\nMamba design).\nThe results are presented in Figure 5. We observe that increasing the number of active Mamba parameters improves the\nperformance. However, the gains become marginal after reaching the 3 : 3 ratio, and higher ratios are impractical due to\ninefficient hardware utilization and high routing costs caused by a large number of experts. We default to this choice in all\nother experiments.\n4.4. Alternative Designs\nParallel MoE-Mamba\nInspired by Wang (2021) and Chowdhery et al. (2023), we experiment with an alternative block\ndesign in which the MoE feed-forward layer and the Mamba layer are placed in parallel instead of sequentially (see Figure\n3). We compare this design to MoE-Mamba for various numbers of experts; see Figure 6. MoE-Mamba outperforms this\n6\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nFigure 5. Final log perplexity at different ratios of active Mamba-to-MoE active parameters. Note that MoE contains the majority of the\ntotal parameters in each model. For further discussion of the ratios explored, see Appendix F.\nTable 3. Comparison of different variants of MoE in Mamba - final log perplexity (1B tokens).\nModel Name /\nModified Projection\nMoE in Mamba\nAll\nLayers\nEvery Other\nLayer\nVanilla Mamba\n3.72\nMoE-Mamba (16 experts)\n3.67\nConv Projection\n3.79\n3.71\nGate Projection\n3.89\n3.70\nOutput Projection\n4.05\n3.70\nConv + Gate Projection\n3.95\n3.72\nConv + Output Projection\n4.17\n3.76\nGate + Output Projection\n4.16\n3.88\nConv + Gate + Output Projection\n4.39\n3.88\nvariant in all tested settings. The parallel MoE-Mamba matches vanilla Mamba when Nexperts \u2265 8 while requiring between 2\nand 4 times as many experts and total parameters to match the performance of the sequential variant. It may be an attractive\nalternative at larger scales due to potentially enabling more efficient use of hardware due to different communication (Wang,\n2021) or fused input matrix multiplications (Chowdhery et al., 2023).\nInner MoE\nPursuing a uniform layer design, we experimented with replacing each of the three linear projections within\nthe Mamba block with an MoE layer; see Figure 3. Enumerating all the possible placements results in 23 \u2212 1 = 7 possible\ndesigns (we discard one combination that would feature no MoE inside the block). We maintain a similar number of total\nparameters and FLOPs in all models by assuring the total number of expert feed-forward layers in a block sums up to 24\nregardless of the placement, i.e., the 24 experts are split evenly between one, two or three MoE\u2019s inside the block. Inspired\nby Fedus et al. (2022), we also performed experiments in which only half of the Mamba blocks were modified to include\nMoE, but the number of experts was increased to 48 to maintain the total number of parameters.\nThree of the designs (Table 3) achieved results marginally better than vanilla Mamba, with none outperforming MoE-Mamba.\nThese results suggest the most promising research directions in future work.\n4.5. Number of Experts\nFigure 4 shows the training runs for different numbers of experts. The results show that our approach scales favorably with\nthe number of experts. MoE-Mamba outperforms vanilla Mamba, when Nexperts \u2265 4. We obtain the best result with 32\nexperts and expect further gains with even more experts.\n7\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nFigure 6. Final log perplexity comparison for varying number of experts in sequential and parallel MoE-Mamba\nTable 4. Log perplexity after 1B tokens for various numbers of experts. Note that the parameter counts exclude the embedding and output\n(unembedding) layers.\nNumber of Experts\n# Parameters\n# Active Parameters\nper Token\nLog Perplexity After\n1B Tokens\nSpeedup Over\nVanilla Mamba\n(Training Steps)\nN/A - Vanilla Mamba\n27M\n27M\n3.72\n1\n1\n26M\n26M\n3.75\n<1\n4 experts\n64M\n26M\n3.72\n1.03\n8 experts\n114M\n26M\n3.70\n1.10\n16 experts\n215M\n26M\n3.67\n1.21\n32 experts\n416M\n26M\n3.67\n1.23\nInterestingly, models with a small number of experts perform worse than vanilla Mamba. This is consistent with Gu & Dao\n(2023) reporting that Mamba interleaved with feed-forward layers (which corresponds to a single-expert MoE layer) is\nworse than vanilla Mamba.\n4.6. Accuracy and Perplexity\nWe observed that throughout the training of a variant of one of our smaller models, MoE-Mamba25M with 32 instead of 42\nexperts as presented in section 4.2, it maintains a lower perplexity than our strongest baseline (Transformer-MoE). However,\nat the same time, Transformer-MoE consistently achieves higher accuracy than MoE-Mamba. We conjecture that this might\nbe due to the fact that attention-based models are able to copy tokens verbatim, unlike SSM-based models, whose similar\nabilities might be hindered by the compression of the history into a finite hidden state. We present accuracy and loss (log\nperplexity) plots alongside further discussion of those results in Appendix C.\n5. Future Work and Limitations\nScaling In this work, we perform experiments on models with the number of active parameters per token smaller than 1B,\nwith total parameters up to 2.4B. Since MoE has enabled Transformers to be scaled to unprecedented sizes (Fedus et al.,\n2022), we will be excited to see the impact of scaling on the approaches proposed in our work. Developing scaling laws\nwould be instrumental in this endeavor.\nIntegrating MoE Into the Mamba Layer Our experiments show that interleaving the Mamba layer with a performant\nsparse MoE feed-forward layer results in a promising model. However, in the dense setting, Mamba performs slightly better\nwithout the feed-forward layer. This suggests that integrating sparse computation within the Mamba layer itself could yield\neven better results while conserving a simple, homogeneous architecture. Our experiments, detailed in section 4.4, warrant\n8\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nsome optimism, and we expect this line of research to remain relevant.\nExploration of Different Types of MoE in MoE-Mamba While we base our design on the commonly used Switch (Fedus\net al., 2022), numerous other MoE architectures have been proposed. Not only may those designs perform better overall, but\nit is possible that a different type of MoE will be optimal when combined with SSMs. Among possible changes in this regard\nthere are Expert-Choice routers (Zhou et al., 2022), fully differentiable architectures (Puigcerver et al., 2023; Antoniak et al.,\n2023), varying number of experts and their granularity, (Clark et al., 2022; Krajewski et al., 2024), and other modifications.\nDistillation Some works, e.g., (Fedus et al., 2022), have shown that MoE layers can be distilled back to feed-forward layers.\nWe expect similar results for MoE-Mamba. Interestingly, the findings by Gu & Dao (2023) indicate that a Mamba module\ncan emulate feed-forward layers well. This raises the question of whether MoE can be distilled into a vanilla Mamba module\nand how that would be achieved.\nSynergies We leave for future work more in-depth studies of synergies of Mamba and MoE. We suspect that there\nmight be efficiency gains growing with the context length due to better hardware utilization; as for inference, Mamba\nalleviates computation and memory throughput issues stemming from larger context sizes, while MoE alleviates those same\nissues stemming from increasing number of parameters and knowledge stored in the model. This synergy may allow for\nunprecedented scaling of language models both in the number of parameters and length of the input/output.\nMamba and Attention Mechanism Mamba and Transformers make different trade-offs during data processing. This\nresults in a different set of strengths and weaknesses, e.g., Mamba can process very long inputs but might struggle with\ntasks requiring detailed knowledge of the past input (e.g., some instances of copying). It would be interesting to explore\ncombining those two architectures to achieve the best of both worlds.\nLong Context Utilization Mamba and other SSMs are praised for their ability to process long context. However, the extent\nto which they can utilize it effectively and techniques for improving the utilization have not yet been studied in depth. To\nthat end, some methods developed for Transformers (Shi et al., 2023; Tworkowski et al., 2023; Staniszewski et al., 2024)\nmight be applicable.\nOther Modalities This work explores one direction in which Mamba can be extended. Mamba is a general architecture,\nand it is not limited to language modeling. We expect that it will be possible to apply MoE-Mamba to other tasks, like\nnon-textual sequence modeling presented by Gu & Dao (2023), and different modalities, such as vision, with initial work\npresented by Zhu et al. (2024).\n6. Conclusions\nIn this work, we presented the first integration of Mixture of Experts with Mamba architecture, MoE-Mamba. This novel\nmethod shares the inference benefits of Mamba while requiring 2.35\u00d7 fewer training steps to reach the same performance.\nWe showed possible ways of combining those techniques and positively verified performance improvements achieved with\ntheir combination. We confirmed with experiments on models up to 2.4B parameters and training lengths up to 30B tokens\nthat those improvements over Mamba are robust to model sizes, length of training, and the number of experts.\nIn addition to the above, we explored and evaluated numerous alternative designs integrating Mixture of Experts within the\nMamba block. While none of those variants outperformed MoE-Mamba, we think that those investigations can help prune\nineffective research directions and point to promising ones.\nOur work opens a new research direction of combining Mixture of Experts with State Space Models. We believe that this\npath will enable more efficient scaling to even larger language models.\n9\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nAcknowledgments\nWe would like to express sincere gratitude to Tomasz Odrzyg\u00b4o\u00b4zd\u00b4z for the engineering contributions made to our shared\nrepository. We also thank Piotr Sankowski for creating a supportive environment and providing research direction.\nThis work was funded by IDEAS NCBR, which also provided significant computational resources and a supportive research\nenvironment. The research was supported by PL-Grid infrastructure (grant PLG/2023/016148). We acknowledge snakes\nand experts as essential to our work. We also benefited from the Entropy cluster (hosted at the Faculty of Mathematics,\nInformatics and Mechanics of the University of Warsaw) funded by NVIDIA, Intel, the Polish National Science Center\ngrant 2022/45/N/ST6/02222, and ERC Starting Grant TOTAL. Marek Cygan was partially supported by an NCBiR grant\nPOIR.01.01.01-00-0392/17-00.\nReferences\nAnthony, Q., Tokpanov, Y., Glorioso, P., and Millidge, B. Blackmamba: Mixture of experts for state-space models, 2024.\nAntoniak, S., Jaszczur, S., Krutul, M., Pi\u00b4oro, M., Krajewski, J., Ludziejewski, J., Odrzyg\u00b4o\u00b4zd\u00b4z, T., and Cygan, M. Mixture of\ntokens: Efficient llms through cross-example aggregation, 2023.\nArtetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M., Shleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R., et al.\nEfficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684, 2021.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell,\nA., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter,\nC., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford,\nA., Sutskever, I., and Amodei, D. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL\nhttps://arxiv.org/abs/2005.14165.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann,\nS., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023.\nClark, A., de las Casas, D., Guy, A., Mensch, A., Paganini, M., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T., Borgeaud,\nS., van den Driessche, G., Rutherford, E., Hennigan, T., Johnson, M., Millican, K., Cassirer, A., Jones, C., Buchatskaya,\nE., Budden, D., Sifre, L., Osindero, S., Vinyals, O., Rae, J., Elsen, E., Kavukcuoglu, K., and Simonyan, K. Unified\nscaling laws for routed language models, 2022.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language\nunderstanding, 2019.\nDu, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. Glam:\nEfficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp.\n5547\u20135569. PMLR, 2022.\nElhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma,\nN., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei,\nD., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. A mathematical framework for transformer circuits.\nTransformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html.\nFedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient\nsparsity, 2022.\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and R\u00b4e, C. Hungry hungry hippos: Towards language modeling\nwith state space models, 2023.\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2023.\nGu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., and R\u00b4e, C. Combining recurrent, convolutional, and continuous-\ntime models with linear state-space layers, 2021.\n10\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nGu, A., Goel, K., Gupta, A., and R\u00b4e, C. On the parameterization and initialization of diagonal state space models. Advances\nin Neural Information Processing Systems, 35:35971\u201335983, 2022a.\nGu, A., Goel, K., and R\u00b4e, C. Efficiently modeling long sequences with structured state spaces, 2022b.\nGupta, A., Gu, A., and Berant, J. Diagonal state spaces are as effective as structured state spaces. Advances in Neural\nInformation Processing Systems, 35:22982\u201322994, 2022.\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural Computation, 3(1):\n79\u201387, 1991. doi: 10.1162/neco.1991.3.1.79.\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B.,\nBressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S.,\nYang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts, 2024.\nJordan, M. and Jacobs, R. Hierarchical mixtures of experts and the em algorithm. In Proceedings of 1993 International\nConference on Neural Networks (IJCNN-93-Nagoya, Japan), volume 2, pp. 1339\u20131344 vol.2, 1993. doi: 10.1109/IJCNN.\n1993.716791.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.\nScaling laws for neural language models, 2020.\nKrajewski, J., Ludziejewski, J., Adamczewski, K., Pi\u00b4oro, M., Krutul, M., Antoniak, S., Ciebiera, K., Kr\u00b4ol, K., Odrzyg\u00b4o\u00b4zd\u00b4z,\nT., Sankowski, P., Cygan, M., and Jaszczur, S. Scaling laws for fine-grained mixture of experts, 2024.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. Albert: A lite bert for self-supervised learning of\nlanguage representations, 2020.\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant\nmodels with conditional computation and automatic sharding, 2020.\nLewkowycz, A., Andreassen, A. J., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V. V., Slone, A., Anil, C., Schlag,\nI., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with\nlanguage models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing\nSystems, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7.\nLi, Y., Cai, T., Zhang, Y., Chen, D., and Dey, D. What makes convolutional models great on long sequence modeling? arXiv\npreprint arXiv:2210.09298, 2022.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization, 2019.\nMa, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., and Zettlemoyer, L. Mega: moving average equipped gated\nattention. arXiv preprint arXiv:2209.10655, 2022.\nOlsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A.,\nConerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J., Lovitt, L.,\nNdousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. In-context learning and induction\nheads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-\nheads/index.html.\nOpenAI. Gpt-4 technical report, 2023.\nOrvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., and De, S. Resurrecting recurrent neural networks\nfor long sequences. arXiv preprint arXiv:2303.06349, 2023.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L.,\nDesmaison, A., K\u00a8opf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and\nChintala, S. Pytorch: An imperative style, high-performance deep learning library, 2019.\n11\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Biderman, S., Cao, H., Cheng, X., Chung, M., Grella, M.,\nGV, K. K., He, X., Hou, H., Lin, J., Kazienko, P., Kocon, J., Kong, J., Koptyra, B., Lau, H., Mantri, K. S. I., Mom, F.,\nSaito, A., Song, G., Tang, X., Wang, B., Wind, J. S., Wozniak, S., Zhang, R., Zhang, Z., Zhao, Q., Zhou, P., Zhou, Q.,\nZhu, J., and Zhu, R.-J. Rwkv: Reinventing rnns for the transformer era, 2023.\nPoli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and R\u00b4e, C. Hyena hierarchy:\nTowards larger convolutional language models, 2023.\nPuigcerver, J., Riquelme, C., Mustafa, B., and Houlsby, N. From sparse to soft mixtures of experts, 2023.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask\nlearners. OpenAI blog, 1(8):9, 2019.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of\ntransfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551,\n2020.\nSanseviero, O., Tunstall, L., Schmid, P., Mangrulkar, S., Belkada, Y., and Cuenca, P. Mixture of experts explained, 2023.\nURL https://huggingface.co/blog/moe.\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer, 2017.\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., James, R., Lin, X. V., Smith, N. A., Zettlemoyer, L., Yih, S., and Lewis, M.\nIn-context pretraining: Language modeling beyond document boundaries, 2023.\nSmith, J. T. H., Warrington, A., and Linderman, S. W. Simplified state space layers for sequence modeling, 2023.\nStaniszewski, K., Tworkowski, S., Jaszczur, S., Michalewski, H., \u0141ukasz Kuci\u00b4nski, and Mi\u0142o\u00b4s, P. Structured packing in llm\ntraining improves long context utilization, 2024.\nSu, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding,\n2023.\nSun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: A successor to transformer\nfor large language models, 2023.\nSutton, R. The bitter lesson. Incomplete Ideas (blog), 13(1), 2019.\nTay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range\narena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020.\nTeam, G. Gemini: A family of highly capable multimodal models, 2023.\nTogetherComputer. Redpajama: An open source recipe to reproduce llama training dataset, April 2023. URL https:\n//github.com/togethercomputer/RedPajama-Data.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S.,\nBikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C.,\nGoswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann,\nI., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov,\nT., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith,\nE. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y.,\nFan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation\nand fine-tuned chat models, 2023.\nTurc, I., Chang, M.-W., Lee, K., and Toutanova, K. Well-read students learn better: On the importance of pre-training\ncompact models, 2019.\nTworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u00b4s, P. Focused transformer: Contrastive\ntraining for context scaling, 2023.\n12\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all\nyou need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.03762.\nWang, B. Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX. https:\n//github.com/kingoflolz/mesh-transformer-jax, May 2021.\nXue, F., Zheng, Z., Fu, Y., Ni, J., Zheng, Z., Zhou, W., and You, Y. Openmoe: Open mixture-of-experts language models.\nhttps://github.com/XueFuzhao/OpenMoE, 2023.\nZhai, S., Talbott, W., Srivastava, N., Huang, C., Goh, H., Zhang, R., and Susskind, J. An attention free transformer, 2021.\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen,\nY., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.-Y., and Wen, J.-R. A survey of large language\nmodels, 2023a.\nZhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., Desmaison,\nA., Balioglu, C., Damania, P., Nguyen, B., Chauhan, G., Hao, Y., Mathews, A., and Li, S. Pytorch fsdp: Experiences on\nscaling fully sharded data parallel, 2023b.\nZhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A., Chen, Z., Le, Q., and Laudon, J. Mixture-of-experts with\nexpert choice routing, 2022.\nZhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., and Wang, X. Vision mamba: Efficient visual representation learning with\nbidirectional state space model, 2024.\nZoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and Fedus, W. St-moe: Designing stable and\ntransferable sparse expert models, 2022.\n13\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nA. Hyperparameters and Training Setup\nBasic model hyperameters (dmodel, dff, the number of attention heads, the number of layers) used in this work were inspired\nby BERT (Devlin et al., 2019; Turc et al., 2019), with the \u25a125M models being equivalent to BERTMEDIUM and \u25a1100M models\ncopying BERTBASE configuration while increasing the number of blocks from 12 to 16. The learning rate schedule, as well\nas weight decay and gradient clipping values were set per community\u2019s standard practices. We used the AdamW optimizer\n(Loshchilov & Hutter, 2019). We tune the maximum learning rate value for each of the \u25a125M models separately and divide\nit by 2 when training \u25a1100M counterparts. We train the models using PyTorch (Paszke et al., 2019) and utilize FSDP (Zhao\net al., 2023b) for facilitating multi-GPU setup.\nTable 5. Hyperparameters (\u25a125M Models). In Transformer models we use Rotary Position Embedding (Su et al., 2023).\nHyperparameter\nTransformer25M\nMamba25M\nTransformer-MoE25M\nMoE-Mamba25M\nModel\nTotal Blocks\n8\n16\n8\n8\ndmodel\n512\n512\n512\n512\n# Parameters\n25M\n27M\n545M\n542M\n# Active Parameters\nper Token\n25M\n27M\n25M\n26M\nFeed-Forward\ndff\n2048\n-\n-\n-\nMixture of Experts\ndexpert\n-\n-\n2048\n1536\nNexperts\n-\n-\n32\n42\nPosition Embedding\nRoPE\n-\nRoPE\n-\nAttention\nNheads\n8\n-\n8\n-\nTraining\nTraining Steps\n150K\n150K\n150K\n150K\nContext Length\n1024\n1024\n1024\n1024\nBatch Size\n64\n64\n64\n64\nMax Learning Rate\n5e-4\n1e-3\n5e-4\n5e-4\nLR Warmup\n1%\n1%\n1%\n1%\nLR Schedule\nCosine\nCosine\nCosine\nCosine\nFinal LR Ratio\n0.1\n0.1\n0.1\n0.1\nWeight Decay\n0.1\n0.1\n0.1\n0.1\nGradient Clipping\n0.5\n0.5\n0.5\n0.5\nB. Active Parameters vs FLOPs\nIn this work, we report the number of active parameters (excluding embedding and unembedding layers) and not the number\nof floating-point operations (FLOPs), following (Zhou et al., 2022). Both numbers will be roughly proportional (Kaplan\net al., 2020), but the number of FLOPs is both harder to calculate and less relevant for hardware-aware architecture like\nMamba with its optimizations, especially during inference.\n14\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nTable 6. Hyperparameters (\u25a1100M Models). In Transformer-MoE100M we use Rotary Position Embedding (Su et al., 2023).\nHyperparameter\nMamba100M\nTransformer-MoE100M\nMoE-Mamba100M\nModel\nTotal Blocks\n32\n16\n16\ndmodel\n768\n768\n768\n# Parameters\n121M\n2454M\n2439M\n# Active Parameters\nper Token\n121M\n114M\n117M\nMixture of Experts\ndexpert\n-\n3072\n2304\nNexperts\n-\n32\n42\nPosition Embedding\n-\nRoPE\n-\nAttention\nNheads\n-\n12\n-\nTraining\nTraining Steps\n30K\n30K\n30K\nContext Length\n1024\n1024\n1024\nBatch Size\n1024\n1024\n1024\nMax Learning Rate\n1e-3\n2.5e-4\n5e-4\nLR Warmup\n1%\n1%\n1%\nLR Schedule\nCosine\nCosine\nCosine\nFinal LR Ratio\n0.1\n0.1\n0.1\nWeight Decay\n0.1\n0.1\n0.1\nGradient Clipping\n0.5\n0.5\n0.5\nTable 7. Comparison of sequential and parallel MoE-Mamba - final log perplexity (1B tokens).\n# of Experts\nMoE-Mamba\nSequential\nParallel\n1\n3.76\n3.79\n2\n3.74\n3.77\n4\n3.71\n3.74\n8\n3.69\n3.72\n16\n3.67\n3.70\n32\n3.66\n3.69\n15\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nC. Accuracy and Perplexity\nFigure 7. Discrepancy between accuracy and log perplexity: MoE-Mamba25M with 32 experts and Transformer25M. Note that MoE-\nMamba with 32 experts has fewer total parameters than the Transformer.\nAs mentioned in section 4.6, we have observed a curious case of metric inconsistency between two models that achieved\nsimilar performance but were based on different architectures. We hypothesize that this discrepancy hints at a potential\nfailure mode of Mamba and other SSMs. Due to the compression of the history into a finite hidden state, their ability for\nverbatim token-copying is limited. The related ability to predict the token [B] given a prefix ...[A][B]...[A] (where [A], [B]\ncan be any tokens) has been mechanistically studied by Elhage et al. (2021) and has been conjectured to be responsible for\nTransformer\u2019s remarkable in-context learning capabilities (Olsson et al., 2022).\nPeng et al. (2023) mention that their attention-free model, RWKV, may have limited performance on tasks that require\nrecalling precise information over long contexts due to a fixed-sized hidden state, a property that Mamba and other SSMs\nshare. However, since the perplexity of Mamba can match the perplexity of a similarly-sized Transformer, we can suspect\nthat Mamba compensates for that failure mode in other ways and might show a relative advantage on other tasks when\ncompared to Transformer. In particular, it might outperform Transformers in 0-shot tasks in contrast to tasks allowing\nfew-shot demonstrations or requiring in-context learning.\nD. Relation between Speedup and Training Time\nIn our experiments, we notice that generally, as the training continues, the speedup of MoE-Mamba compared to vanilla\nMamba increases (see Fig. 8). That is, the ratio\nspeedup(l) = # processed tokens vanilla Mamba took to reach loss l\n# processed tokens MoE-Mamba took to reach loss l\nincreases as l decreases. Speedup in \u25a125M models oscillates between 1.6 and 1.9, while the speedup in \u25a1100M models rises\nsteadily.\nE. Counting Model Parameters\nFor all models and their variants, we report the number of trainable, non-embedding parameters, i.e., we exclude the\nparameters in the input (embedding) and output (unembedding) layers. This convention is proposed by Kaplan et al. (2020),\nwho note that using just non-embedding parameters gives their scaling laws a clearer form. The relatively low importance of\nthe number of the embedding parameters for the final performance has been noted by Lan et al. (2020).\n16\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nFigure 8. Speedup of different sizes of MoE-Mamba compared to their vanilla Mamba counterparts as training progresses.\nF. Exploring the Optimal Mamba to MoE Active Parameters Ratio\nThe assignment of FLOPs and parameters to different components is an important design choice in heterogeneous architec-\ntures. For example, in Transformer, the shape of the model has been studied extensively by Kaplan et al. (2020). In our\nwork, we investigate the optimal ratio of active parameters in the Mamba layer to the number of active parameters in the\nMoE layer, see section 4.3. Figure 5 may suggest that increasing the ratio strengthens the performance and maybe assigning\nall the active parameters to Mamba would result in the best performance (ratio \u201c6:0\u201d). It should, however, be noted, that all\nthe investigated models contain the same number of both total parameters and active parameters per token. A hypothetical\nmodel described above could not achieve this property. If we loosen the requirement and place all the parameters in Mamba,\nthe resulting model is the same as Mamba25M with the expansion factor E = 4 and 8 instead of 16 Mamba layers. This\nmodel achieves marginally worse final log perplexity than Mamba25M (3.73).\nG. Train and Test Set Performance\nIn the main text, we report the loss values obtained on the train set. Our training procedure samples from the dataset, so\neven without processing more tokens than there are in the C4 dataset, the same documents may be encountered multiple\ntimes. However, as we process less than 20% of the tokens, the difference in performance on the train set and on the test set\nshould be negligible. For transparency, we provide the results on the test set as well (Figure 9). Their variance may be high\ndue to a limited number of sequences in each evaluation step.\nH. Contributions\nMaciej integrated Mamba into the codebase, ran experiments related to various aspects of this work, and oversaw the course\nof the project. Kamil ran the bulk of the experiments. Krystian explored alternative Mamba block designs with Jan\u2019s help.\nMicha\u0142 and Jakub contributed to the project in various ways, mostly by running experiments and perfecting the codebase.\nSzymon contributed to our shared repo and participated in some early discussions on integrating MoE and SSMs. Piotr and\nMarek provided high-level scientific advice. Sebastian supervised the project, setting the research direction and leading\nexperiments and analyses.\n17\nMoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nFigure 9. Test set loss.\nI. Reproducibility\nThe codebase used to run the experiments is available at https://github.com/llm-random/llm-random.\n18\n"
  },
  {
    "title": "Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM",
    "link": "https://arxiv.org/pdf/2401.02994.pdf",
    "upvote": "43",
    "text": "Blending Is All You Need: Cheaper, Better Alternative to\nTrillion-Parameters LLM\nXiaoding Lu\u00a7\nZongyi Liu\nAdian Liusie\u00a7\nVyas Raina\u00a7\nVineet Mudupalli\u00a7\nYuwen Zhang\u00b6\nWilliam Beauchamp\u00a7\n\u00a7University of Cambridge\n\u00b6University College London\nChai Research\nAbstract\nIn conversational AI research, there\u2019s a notice-\nable trend towards developing models with a\nlarger number of parameters, exemplified by\nmodels like ChatGPT. While these expansive\nmodels tend to generate increasingly better chat\nresponses, they demand significant computa-\ntional resources and memory. This study ex-\nplores a pertinent question: Can a combina-\ntion of smaller models collaboratively achieve\ncomparable or enhanced performance relative\nto a singular large model? We introduce an\napproach termed Blending, a straightforward\nyet effective method of integrating multiple\nchat AIs. Our empirical evidence suggests that\nwhen specific smaller models are synergisti-\ncally blended, they can potentially outperform\nor match the capabilities of much larger coun-\nterparts. For instance, integrating just three\nmodels of moderate size (6B/13B parameters)\ncan rival or even surpass the performance met-\nrics of a substantially larger model like Chat-\nGPT (175B+ parameters). This hypothesis is\nrigorously tested using A/B testing methodolo-\ngies with a large user base on the Chai research\nplatform over a span of thirty days. The find-\nings underscore the potential of the Blended\nstrategy as a viable approach for enhancing\nchat AI efficacy without a corresponding surge\nin computational demands. 1\n1\nIntroduction\nDue to the remarkable capabilities of current gen-\nerative AI technology, pre-trained large language\nmodels (LLMs) have found extensive utilization\nacross a diverse array of applications. One such\napplication is in chat AI, where automatic systems\nare deployed as conversational assistants that keep\nusers engaged in entertaining conversations. A\ncommon finding is that as one scales up the num-\nber of model parameters and the training data size,\nthe quality and ability of the LLMs dramatically\n1All\ntrained\nmodels\nare\nprovided\nat\nhttps://\nhuggingface.co/ChaiML.\nincreases. This has led to the current trend of\nscaling up models to tremendous sizes, with cur-\nrent state-of-the-art systems having hundreds of\nbillions of parameters. Although this has enabled\nhighly capable chat AI with extraordinary emer-\ngent abilities, this comes at a practical cost of large\ninference overheads, with specialized infrastruc-\nture required, and access to these systems restricted\nthrough public APIs. It is therefore highly desirable\nto overcome these dramatic practical limitations,\nand have smaller and efficient chat AIs, while keep-\ning users engaged and maintaining the conversa-\ntional quality that current 100B+ parameters LLMs\nhave achieved.\nAlthough a single small model is unlikely to\ncompete against the current behemoth state-of-the-\nart LLMs, one may question whether a group of\nmoderately-sized LLMs can together form a chat\nAI of equivalent or perhaps better ability. In this\nwork, we introduce Blended, an innovative and\nsimple approach where we demonstrate that, sur-\nprisingly, if responses are selected randomly from a\ngroup of base chat AIs, the resulting combined chat\nAI is highly capable and engaging, and can outper-\nform systems with orders of magnitude more pa-\nrameters. We interestingly observe that the blended\nmodel appears to take characteristics that are the\n\u201cbest of all\", and that by conditioning a response\non the conversational history, a single model with\nparticular properties learns abilities from other sys-\ntems. This leads to more captivating and diverse\nresponses, and a more engaging user experience.\nWe demonstrate the effectiveness of Blended over\nlarge-scale A/B tests on real users on the CHAI\nplatform, where our results show that a Blended\nensemble with three 6-13B parameter LLMs, out-\ncompetes OpenAI\u2019s 175B+ parameter ChatGPT.\nWe observe significantly higher user retention for\nblended ensembles than for ChatGPT-based chat\nAIs, illustrating that users find Blended chat AIs to\nbe more engaging, entertaining and useful, despite\narXiv:2401.02994v3  [cs.CL]  23 Jan 2024\nBlended only requiring a fraction of the inference\ncost and memory overhead.\n2\nRelated Work\n2.1\nChat AI approaches\nChat AIs have been developed for a variety of appli-\ncations, from user assistance to casual interactions\n(for chitchat) (Chen et al., 2017). Early designs\nwere based on rule-based algorithms (Weizenbaum,\n1966) which later progressed to generative retrieval-\nbased models (Papangelis et al., 2021). The emer-\ngence of pre-trained transformer language models\nmarked a significant change in chat AI develop-\nment (Zhu, 2022; Vaswani et al., 2017; Zaib et al.,\n2020), where scaling-up trends led to increasingly\nlarger Transformer-based models finetuned to con-\nversational datasets for the development of chat\nAIs (Adiwardana et al., 2020; Roller et al., 2021;\nBao et al., 2020; Choudhary and Kawahara, 2022;\nYan et al., 2022).\nTraditionally, chat AIs have been trained with\nself-supervised methods on conversational datasets.\nHowever, more recent approaches highlight the im-\nportance of human feedback in training to align\nbetter with human expectations of an engaging con-\nversation (Leike et al., 2018; Askell et al., 2021;\nGabriel, 2020). This is typically achieved through\neither reinforcement learning from human feedback\n(RLHF; Christiano et al., 2017; Stiennon et al.,\n2020) or by using the reward model on its own to\nselect or filter out responses (Dathathri et al., 2019;\nIrvine et al., 2023)\nIn our work, our Blended approach does not con-\nsider how one can train better conversational LLMs,\nand instead demonstrates that one can leverage a\ngroup of existing small conversational LLMs and\nencourage them to collaborate over a conversation\nto form a single chat AI that generates more engag-\ning and diverse responses.\n2.2\nGenerative system combination\nSystems combination has been well-explored for\ndeep-learning systems, with approaches such as\nstacking (Wolpert, 1992), negative correlation\nlearning (Liu and Yao, 1999), max-voter schemes\n(Ju et al., 2018; Simonyan and Zisserman, 2014) or\nprobability averaging (He et al., 2016; Raina et al.,\n2020; Szegedy et al., 2015) employed for a range\nof regression and classification tasks. With these\nensembling methods, it has further been shown that\nincreasing the diversity of the individual members\ncan lead to better-performing combined systems\n(Kilimci et al., 2018; Seijo-Pardo et al., 2017).\nHowever, for generative language tasks where\nthe outputs are a sequence of tokens, most ensem-\nbling approaches become inapplicable and inef-\nfective. Sequence-level ensembling approaches,\nthough, get around this by often averaging condi-\ntional token level probabilities of multiple systems\n(Sennrich et al., 2015; Freitag et al., 2017; Ma-\nlinin and Gales, 2021; Fathullah et al., 2021). This\napproach, however, often requires identical mem-\nber architectures and access to the output proba-\nbilities of the tokens. With an increasing trend\nof limited black box access to LLMs (e.g. Chat-\nGPT (Liu et al., 2023) and BARD (Nyberg et al.,\n2021)), ensembling methods that only use output\nsequences may have practical benefit. Minimum\nBayes\u2019 Risk (MBR) decoding (Kumar and Byrne,\n2004) enables this by using system outputs to se-\nlect the predicted \u2018best\u2019 system output. Though\nthis approach has traditionally been used for Auto-\nmatic Speech Recognition (ASR), it has also been\nsuccessfully applied to NLP tasks (Rosti et al.,\n2007; Freitag et al., 2022; Manakul et al., 2023;\nRaina and Gales, 2023). With a growing number\nof (API-access only) deployed large language mod-\nels, performing well at different tasks, (Jiang et al.,\n2023) also observed the need for a method to com-\nbine outputs in a blackbox setting. They propose\nLLM-Blender to blend the outputs from different\nlanguage models by first ranking the outputs as per\na PairRanker and then fuse the top-K outputs us-\ning a separate deep sequence-to-sequence system\n(termed GenFuser).\nAs with MBR and LLM-Blender, in this work\nwe also propose an ensembling approach that is\nable to combine outputs from blackbox language\nmodels. However, by designing our method for\nthe specific nature of a multi-turn task (such as\ndialogue agents) our Blended approach does not\nrequire all component systems to generate outputs\nbut instead stochastically selects the system that\ngenerates the next response, allowing for model\nblending at the level of a multi-turn conversation.\n3\nBlended\n3.1\nChat AI\nThe objective of a chat AI is to design an automatic\nsystem that can produce engaging and entertaining\nconversations that human users can interact with.\nLet uk denote the user\u2019s kth turn, where each user\nturn is a sequence of words, uk =(w(k)\n1\n. . . , w(k)\n|uk|).\nSimilarly, let rk denote the system\u2019s kth generated\nresponse, which is also a sequence of words rk =\n(w(k)\n1 , . . . , w(k)\n|rk|). As an implicit language model, a\nparticular chat AI, parameterised by \u03b8, models the\nprobability of the next response given the previous\nconversational history,\nP(rk|u1:k, r1:k\u22121; \u03b8)\n(1)\nDuring training, the system implicitly learns to as-\nsign higher probability to responses that are fluent,\nengaging and high quality. Therefore an output\ncan simply be sampled from its distribution, either\nstochastically, or through an approximate search\nprocess such as beam search.\nrk \u223c P(r|u1:k, r1:k\u22121; \u03b8)\n(2)\nInspired by InstructGPT (Ouyang et al., 2022) and\noutlined in (Irvine et al., 2023), state-of-the-art chat\nAIs tends to follow a three-stage-pipeline. First, a\npre-trained language model (PrLM) is fine-tuned\non a relevant textual domain, e.g. entertaining lit-\nerature for the design of an engaging chatbot. Sec-\nond, a reward model is trained using explicit human\nfeedback, for example, by using user engagement\nas a proxy for response quality (Irvine et al., 2023).\nThen finally, the reward model is used to improve\nthe original PrLM, either by Proximal Policy Op-\ntimisation (Ouyang et al., 2022) or by following a\nsimple rejection sampling strategy.\nIn developing a particular chat AI, there are\nmany design choices such as the base PrLM, the\nconversational data used in fine-tuning, and the na-\nture of human feedback used to update the system.\nOne may expect that different recipes and train-\ning seeds may lead to highly diverse systems that\neach demonstrate unique strengths and characteris-\ntics. One can then consider how a set of chat AIs\ncan be combined for a system with overall better\ncharacteristics.\n3.2\nEnsembling\nIn accordance with Bayesian statistical principles,\nthe probability assigned to a particular response\ncan be conceptualized as the marginal expectation\ntaken over all plausible chat AI parameters,\nP(rk|u1:k, r1:k\u22121)\n(3)\n=E\u03b8\u223cP\u0398 [P(rk|u1:k, r1:k\u22121; \u03b8)]\n(4)\n=\nZ\nP\u0398(\u03b8)P(rk|u1:k, r1:k\u22121; \u03b8)d\u03b8\n(5)\nIn practice, where we only have access to a finite set\nof chat AI systems {\u03b81, \u03b82...\u03b8N}, one can approx-\nimate the continuous integral as a discrete sum-\nmation. Further, one can assume that P\u0398(\u03b8) is\ndistributed uniformly over the systems such that\nP\u0398(\u03b8n) =\n1\nN , which may be a valid assumption\nif the set consists of similarly performing models.\nThis yields the approximation,\nP(rk|u1:k, r1:k\u22121)\n(6)\n\u2248\nX\n\u03b8\nP\u0398(\u03b8)P(rk|u1:k, r1:k\u22121; \u03b8)\n(7)\n= 1\nN\nN\nX\nn=1\nP(rk|u1:k, r1:k\u22121; \u03b8n)\n(8)\n3.3\nBlended\nThe objective of our approach is to approximately\ndraw samples from the true ensemble distribution\n(equation 8). To achieve this approximation, each\nturn Blended randomly (and uniformly) selects the\nchat AI \u03b8 that generates the current response. This\nprocess is illustrated in Algorithm 1. It can be noted\nthat during a conversation, the response generated\nby a specific chat AI is conditional on all previ-\nous responses generated by the previously selected\nchat AIs. This means that the different chat AIs\nare able to implicitly influence the output of the\ncurrent response. As a result, the current response\nis a blending of individual chat AI strengths, as\nthey collaborate to create an overall more engag-\ning conversation.\nAlgorithm 1 Blended Algorithm\n1: k \u2190 1\n2: while true do\n3:\nuk \u2190 user\u2019s current input turn\n4:\nSample model parameter \u03b8n \u223c P\u0398\n5:\nGenerate response rk according to:\nrk \u223c P(r|u1:k, r1:k\u22121; \u03b8n)\n6:\nk = k + 1\n7: end while\n4\nEvaluating Chat AIs\nEvaluating the quality of NLG outputs is a notori-\nously challenging task (Fabbri et al., 2021; Liusie\net al., 2023), where traditional gold-standard ap-\nproaches use human evaluators that score the qual-\nity of generated responses, which can be costly.\nHowever, since chat AIs are by definition deployed\nin social environments with humans, one can lever-\nage statistics of users interaction as a meaningful\nand aligned measure of chat AI engagingness and\nquality. To assess the \u2019quality\u2019 of a chat AI, we\nconsider two main proxy functions: the industry\nstandard user retention and the main objective func-\ntion, user engagement.\n4.1\nUser Retention\nUser retention is a standard industrial measure of\na platform\u2019s success by measuring the fraction of\nusers that return to the platform k days after joining.\nLet the control group Gn be a randomly selected\ngroup of new users, where each user in this group\nwill only be served chat AI \u03b8n. Let Sn(k) be the\nnumber of users from Gn that use the platform and\ninteract with the chat AI on day k. Therefore, the\nk-day user retention rate, R(k), is simply given by\nthe fraction,\nR(k) = Sn(k)\n|Gn| .\n(9)\nRetention rates from different models can be com-\npared throughout the A/B testing period, where one\ncan compare the immediate and long-term engage-\nment of different chat AIs. Hence, for a considered\ngroup Gn and control group Gc, one can define the\ntest to control retention ratio, qn(k) as\nqn(k) = Rn(k)\nRc(k) .\n(10)\nBeyond comparing models, it is useful to extract\nretention curve statistics that can summarize a chat\nAI\u2019s performance with interpretable metrics. Em-\npirical evidence suggests that the retention rate can\nbe modelled well as,\nR\u2217(k) = R(1)\nk\u2212\u03b2 ,\n(11)\nwhere the parameter \u03b2 indicates the rate of user\nretention decay days, k. Taking the log of both\nsides yields;\nlog(q\u2217(k)) = \u2206\u03b6 + \u2206\u03b2 log k,\n(12)\nwhere \u2206\u03b6 = (log(Rw(1))\u2212log(Rc(1)) and \u2206\u03b2 =\n(\u03b2w \u2212 \u03b2c). One can therefore use the gradient and\nintercept of the log-log linear best-fit line to esti-\nmate the parameters \u2206\u03b2 and \u2206\u03b6, which gives a\nuseful comparison of the initial retention ratio and\nretention ratio decay rate relative to the control chat\nAI.\n4.2\nUser Engagement\nUser retention is a useful industry metric, however,\nit may not perfectly align with the metrics that\nare of true interest. High-quality, engaging con-\nversations are likely to keep users captivated for\nlonger; therefore we directly define a proxy user\nengagement metric as the average time spent per\nvisiting user. Let E(u)(t) represent whether a user\nis engaged at a time t,\nE(u)(t) =\n(\n1, user interacts in t \u2212 \u2206 to t + \u2206,\n0, otherwise,\n(13)\nThen we can define En(t), the engagement at time\nt for all users in cohort Gn, as\nEn(t) =\n1\n|Gn|\nX\nu\u2208Gn\nE(u)(t).\n(14)\nAs with user retention, the A/B setting allows for\ndirect comparison of the engagement between dif-\nferent chat AIs. Hence we define the test to control\nengagement ratio, rn(t) as\nrn(t) = En(t)\nEc(t) .\n(15)\nIt is also useful to have an overall single metric\nfor the engagement score of a chat AI over time\nt. Hence, to obtain this, it is empirically observed\nthat a sensible approximation for a chat AI engage-\nment\u2019s decay is 2,\nE\u2217(t) = \u03b1t\u03b3,\n(16)\nThis then gives a model for the test to control en-\ngagement ratio as\nlog(r\u2217(t)) = \u2206\u03b1 + \u2206\u03b3 log t,\n(17)\nwhere \u2206\u03b1 = (log(\u03b1(w)) \u2212 log(\u03b1(c))) and \u2206\u03b3 =\n(\u03b3(w) \u2212 \u03b3(c))). By plotting r(t) against t, a linear\nline of best fit can be found, with the parameters\n\u2206\u03b1 and \u2206\u03b3 being the intercept and gradient respec-\ntively. This gives the summarising metrics \u2206\u03b1 and\n\u2206\u03b3 to compare the engagement quality of different\ntest chat AIs.\n5\nExperiments\n5.1\nExperimental Set Up\nBase chat AI systems: In our experiments we\nconsider four different base chat AI systems. We\n2Periodic oscillations are not modeled here.\nBlend (13,6,6B)\nGPT3.5 (175B)\nVicuna+ (13B)\nChaiLLM (6B)\n0\n20\n40\n60\n80\n100\n120\nImprovement Over Control %\nEngagement\nRetention\nFigure 1: Model performance comparisons, setting the\nbaseline as Pygmalion 6B. Each model is assigned to\n5,000 unique new users, graphs report the day 30 reten-\ntion and engagement improvement with respect to the\nbaseline.\nfirst have 3 moderately sized open-sourced LLMs:\nPygmillion 6B3, Chai Model 6B4 and Vicuna 13B5.\nEach base LLM has been further finetuned on\nconversational data, and uses rejection sampling\nfrom a trained reward model (detailed in (Irvine\net al., 2023)). We finally also consider the state of\nart chat AI, OpenAI\u2019s Davinci (GPT3.5), which\nhas 175B parameters and is only available through\na closed API call.\nMethodology: Each of the base chat AI systems\nare deployed with A/B tests on independent user\ngroups, as discussed in Section 3.3, where the\ngroups are of real users engaging with the Chai Re-\nsearch Platform. We conduct a large-scale evalua-\ntion with at least 10000 users in each group, and we\nmonitor the user engagement on the platform over\na 30-day period. Further, we deploy our blended\nsystem (Blended), encompassing Pygmillion, Chai\nModel and Vicuna. Since there can be external\nfactors that may influence users\u2019 retention and en-\ngagement (e.g. platform popularity, holidays etc.),\n3https://huggingface.co/PygmalionAI/\npygmalion-6b\n4https://huggingface.co/ChaiML/edit_\nsft_pyg_v2e_cp_17515\n5https://huggingface.co/lmsys/\nvicuna-13b-v1.3\nsystems are only compared using relative engage-\nment and relative retention, which are the metrics\nnormalised to the selected baseline group.\n5.2\nExperimental Results\nFor each chat AI deployed on the Chai Research\nplatform, we compute the user engagement for each\nday k, as per Equation 15 in an A/B test setting.\nBy considering the 20th day (k = 20), Figure 1a\nshows the engagement ratio of Blended, its con-\nstituent chat AIs and Open AI\u2019s GPT-3.5. We ob-\nserve that the moderate-sized chat AIs (Pygmillion,\nVicuna and ChaiLLM) have significantly lower en-\ngagement than that of GPT3.5, which is expected\nas GPT3.5 has over an order of magnitude more pa-\nrameters. However, by blending the three base chat\nAIs, not only does Blended have higher engage-\nment than each of the constituent systems, but the\nperformance gains are so significant that Blended\ncan outperform OpenAI\u2019s GPT3.5. The success of\nBlended over other chat AIs can also be observed\nwhen comparing the k = 20 user retention ratio\n(Equation 10), as seen in Figure 1.\nWe highlight that Blended has a total of 25B\nparameters compared to OpenAIs 175B parame-\nters, and further, since responses for Blended are\neach sampled from a single component chat AI,\nthe inference cost is equivalent to that of a single\n6B/13B system. The significant difference in infer-\nence speed (measured as the inverse of total Float-\ning Point Operations at test time) is highlighted in\nFigures 2 and 2 respectively, where it can be ob-\nserved that Blended offers significant performance\ngains with respect to engagement and user reten-\ntion, with speeds similar to that of small chat AIs.\nImplications of this are strong: instead of scal-\ning up systems to improve quality, one can simply\nblend multiple smaller open-source systems, and\nwithout increasing any inference costs can drasti-\ncally improve a user\u2019s conversational experience.\nThis demonstrates the importance of model collab-\noration over simple model parameter scaling when\ndesigning engaging and successful chat AIs.\nAs an objective comparison, Table 1 reports the\nsingle metric summaries (proposed in Section 3.3).\nWith Pygmillion as the control, we report the test-\nto-control engagement ratio metrics \u2206\u03b1 and \u2206\u03b3,\nas well as the test-to-control retention ratio metrics\n\u2206\u03b6 and \u2206\u03b2. Blended has the highest relative ini-\ntial engagement, \u2206\u03b1 and the best engagement ratio\ndecay rate, \u2206\u03b3. Although the retention ratio decay\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\nGPT3.5 (175B)\nBlend (13,6,6B)\nVicuna+ (13B)\nChaiLLM (6B)\nRelative Inference Speed (1/FLOPs)\nImprovement Over Baseline\nEngagement vs Inference Speed\nFigure 2: User Engagement\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\nGPT3.5 (175B)\nBlend (13,6,6B)\nVicuna+ (13B)\nChaiLLM (6B)\nRelative Inference Speed (1/FLOPs)\nImprovement Over Baseline\nRetention vs Inference Speed\nFigure 3: User Retention\nrate, \u2206\u03b2 is better for Vicuna than Blended, Vicuna\nhas a significantly lower initial retention ratio, \u2206\u03b6,\ndemonstrating that Vicuna would require an ex-\ntended period of time to reach Blended\u2019s retention\nscore 6, as can be seen from figures 3. Overall\nit is clear that Blended, using a collaboration of\nsmaller chat AIs, is effective in offering higher\nquality conversations than a single, much larger\nchat AI (OpenAI\u2019s GPT3.5).\nchat AI\n\u2206\u03b6\n\u2206\u03b2\n\u2206\u03b3\n\u2206\u03b1\nFLOP\nChai\n0.1\n0.0\n0.3\n0.2\n1.0\nVicuna\n-0.4\n0.9\n0.0\n0.1\n2.2\nPygmillion (ctrl)\n0.0\n0.0\n0.0\n0.0\n1.0\nBlended\n0.2\n0.5\n2.1\n1.7\n1.4\nGPT3.5\n0.0\n0.3\n1.4\n0.5\n29.2\nTable 1: Test to Control Retention and Engagement\nsummary statistics and inference time (total Floating\nPoint Operations / control) for component chat AIs\n(ChaiModel, Vicuna, Pygmillion (control); Blended\nand OpenAI\u2019s Davinci GPT3.5.\n6\nFuture Work\nThe work demonstrated that Blended, a collab-\noration of multiple small chat AIs, performs\nbetter than a single large-scale chat AI, such as\nOpenAI\u2019s Davinci (ChatGPT). In this section we\noffer methods by which the Blended model can be\nfurther improved to create even more engaging\nuser conversations.\n6This period of time is estimated to be around one year.\nSelection set scaling: Experiments in this work\nhave demonstrated that with even a selection set\nof three component chat AIs (Chai model, Vicuna\nand Pygmillion), Blended is able to perform better\nthan the much larger Davinci GPT3.5 model. This\nperformance gain is attributed to the individual\nexpertise of each individual component model\nthat creates a conversation with a diverse set of\nqualities as the component systems collaborate.\nHence, one simple approach to further increase the\ndiversity and thus richness in the conversation is\nto scale to more than three component systems.\nIncreasing the number of component systems has\nno computational cost, as inference is always only\nrun through a single system for each response in\nBlended\u2019s methodology. Therefore, future work\nwill explore the impact of increasing the selection\nset of component chat AIs on the overall quality of\nconversations.\nOptimal Selection Distribution: As demonstrated\nin Equation 6, Blended in this work adopts a simple\napproximation for model selection, P\u0398(\u03b8n) = 1\nN .\nHowever, although each component chat AI, \u03b8n,\nmay have some value to add to an overall conver-\nsation, an equal contribution from each chat AI\nmay not be the optimal setup. Hence, to combat\nthis, a better approximation for the model selection\ndistribution can be made with,\nP\u0398(\u03b8n) = F(u1:k, r1:k\u22121)n,\n(18)\nwhere F is a deep-learning classifier trained to\npredict the probability distribution over the chat\nAI selection set for identifying the \u03b8n to give the\nnext most engaging response rk. This classifier\ncan be trained using standard signals from Human-\nFeedback to identify effective and ineffective re-\nsponses generated in conversations, e.g. if the user\nregenerated the response it is indicative of being\nan undesirable response. Future work will explore\nmethodologies to design and train such a classifier,\nF to allow for a more optimal (aligned with user\nengagement) distribution, P\u0398 to select the com-\nponent chat AI for each response, rk. A further\nadvantage of this approach is that we can now add\nnew chat AIs to the selection set, without the risk\nof damaging the performance of Blended, as the\nclassifier learns to de-weigh the contribution from\nbad quality chat AIs.\n7\nConclusions\nThis paper introduced Blended, a simple approach\nof combining multiple chat AIs by stochastically\nselecting responses from the different systems.\nThough simple, the approach is surprisingly power-\nful and enables a group of three 6-13B parameter\nmodels to achieve retention and engagement that is\nsuperior to that of the 175B ChatGPT. We demon-\nstrate findings over large scale user A/B tests,\nwhich highlights that blending might be a promis-\ning solution to improve the quality of chat AIs, all\nwhile maintaining inference costs of smaller sys-\ntems.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R. So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\nand Quoc V. Le. 2020. Towards a human-like open-\ndomain chatbot. CoRR, abs/2001.09977.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Benjamin Mann, Nova DasSarma, Nelson\nElhage, Zac Hatfield-Dodds, Danny Hernandez, Jack-\nson Kernion, Kamal Ndousse, Catherine Olsson,\nDario Amodei, Tom B. Brown, Jack Clark, Sam Mc-\nCandlish, Chris Olah, and Jared Kaplan. 2021. A\ngeneral language assistant as a laboratory for align-\nment. CoRR, abs/2112.00861.\nSiqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng\nWang, Wenquan Wu, Zhen Guo, Zhibin Liu, and\nXinchao Xu. 2020. PLATO-2: towards building an\nopen-domain chatbot via curriculum learning. CoRR,\nabs/2006.16779.\nHongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang\nTang. 2017. A survey on dialogue systems: Recent\nadvances and new frontiers. SIGKDD Explor. Newsl.,\n19(2):25\u201335.\nRitvik Choudhary and Daisuke Kawahara. 2022.\nGrounding in social media: An approach to build-\ning a chit-chat dialogue model. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies: Student Research\nWorkshop, pages 9\u201315, Hybrid: Seattle, Washington\n+ Online. Association for Computational Linguistics.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. Ad-\nvances in neural information processing systems, 30.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language mod-\nels: A simple approach to controlled text generation.\nCoRR, abs/1912.02164.\nAlexander R. Fabbri, Wojciech Kry\u00b4sci\u00b4nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. Summeval: Re-evaluating summariza-\ntion evaluation.\nYassir Fathullah, Mark J.F. Gales, and Andrey Malinin.\n2021. Ensemble distillation approaches for grammat-\nical error correction. In ICASSP 2021 - 2021 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 2745\u20132749.\nMarkus Freitag, Yaser Al-Onaizan, and Baskaran\nSankaran. 2017. Ensemble distillation for neural ma-\nchine translation. arXiv preprint arXiv:1702.01802.\nMarkus Freitag, David Grangier, Qijun Tan, and Bowen\nLiang. 2022. High quality rather than high model\nprobability: Minimum Bayes risk decoding with neu-\nral metrics. Transactions of the Association for Com-\nputational Linguistics, 10:811\u2013825.\nIason Gabriel. 2020. Artificial intelligence, values and\nalignment. CoRR, abs/2001.09768.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770\u2013\n778.\nRobert Irvine, Douglas Boubert, Vyas Raina, Adian\nLiusie, Ziyi Zhu, Vineet Mudupalli, Aliaksei Kor-\nshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi,\nChristie-Carol Beauchamp, Xiaoding Lu, Thomas\nRialan, and William Beauchamp. 2023. Rewarding\nchatbots for real-world engagement with millions of\nusers.\nDongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023.\nLlm-blender: Ensembling large language models\nwith pairwise ranking and generative fusion.\nCheng Ju, Aur\u00e9lien Bibaut, and Mark van der Laan.\n2018. The relative performance of ensemble meth-\nods with deep convolutional neural networks for\nimage classification. Journal of Applied Statistics,\n45(15):2800\u20132818.\nZeynep H Kilimci, Selim Akyokus, et al. 2018. Deep\nlearning-and word embedding-based heterogeneous\nclassifier ensembles for text classification. Complex-\nity, 2018.\nShankar Kumar and William Byrne. 2004. Minimum\nBayes-risk decoding for statistical machine transla-\ntion. In Proceedings of the Human Language Tech-\nnology Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHLT-NAACL 2004, pages 169\u2013176, Boston, Mas-\nsachusetts, USA. Association for Computational Lin-\nguistics.\nJan Leike, David Krueger, Tom Everitt, Miljan Martic,\nVishal Maini, and Shane Legg. 2018. Scalable agent\nalignment via reward modeling: a research direction.\nCoRR, abs/1811.07871.\nYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,\nYuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\nMengshen He, Zhengliang Liu, Zihao Wu, Dajiang\nZhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming\nLiu, and Bao Ge. 2023. Summary of chatgpt/gpt-4\nresearch and perspective towards the future of large\nlanguage models.\nYong Liu and Xin Yao. 1999. Ensemble learning via\nnegative correlation. Neural networks, 12(10):1399\u2013\n1404.\nAdian Liusie, Potsawee Manakul, and Mark J. F. Gales.\n2023. Llm comparative assessment: Zero-shot nlg\nevaluation through pairwise comparisons using large\nlanguage models.\nAndrey Malinin and Mark Gales. 2021. Uncertainty\nestimation in autoregressive structured prediction. In\nInternational Conference on Learning Representa-\ntions.\nPotsawee Manakul, Yassir Fathullah, Adian Liusie,\nVyas Raina, Vatsal Raina, and Mark Gales. 2023.\nCued at probsum 2023: Hierarchical ensemble of\nsummarization models.\nErik P. Nyberg, Ann E. Nicholson, Kevin B. Korb,\nMichael Wybrow, Ingrid Zukerman, Steven Mas-\ncaro, Shreshth Thakur, Abraham Oshni Alvandi,\nJeff Riley, Ross Pearson, Shane Morris, Matthieu\nHerrmann, A.K.M. Azad, Fergus Bolger, Ulrike\nHahn, and David Lagnado. 2021. BARD: A struc-\ntured technique for group elicitation of bayesian net-\nworks to support analytic reasoning. Risk Analysis,\n42(6):1155\u20131178.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022.\nTraining language models to follow in-\nstructions with human feedback.\narXiv preprint\narXiv:2203.02155.\nAlexandros Papangelis, Pawe\u0142 Budzianowski, Bing Liu,\nElnaz Nouri, Abhinav Rastogi, and Yun-Nung Chen,\neditors. 2021. Proceedings of the 3rd Workshop on\nNatural Language Processing for Conversational AI.\nAssociation for Computational Linguistics, Online.\nVyas Raina and Mark Gales. 2023. Minimum bayes\u2019\nrisk decoding for system combination of grammatical\nerror correction systems.\nVyas Raina, Mark J.F. Gales, and Kate M. Knill. 2020.\nUniversal adversarial attacks on spoken language as-\nsessment systems. In Interspeech 2020. ISCA.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300\u2013325,\nOnline. Association for Computational Linguistics.\nAntti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-\nros Matsoukas, Richard Schwartz, and Bonnie Dorr.\n2007. Combining outputs from multiple machine\ntranslation systems. In Human Language Technolo-\ngies 2007: The Conference of the North American\nChapter of the Association for Computational Lin-\nguistics; Proceedings of the Main Conference, pages\n228\u2013235, Rochester, New York. Association for Com-\nputational Linguistics.\nBorja Seijo-Pardo, Iago Porto-D\u00edaz, Ver\u00f3nica Bol\u00f3n-\nCanedo, and Amparo Alonso-Betanzos. 2017. En-\nsemble feature selection: homogeneous and hetero-\ngeneous approaches.\nKnowledge-Based Systems,\n118:124\u2013139.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015.\nImproving neural machine translation\nmodels with monolingual data.\narXiv preprint\narXiv:1511.06709.\nKaren Simonyan and Andrew Zisserman. 2014. Very\ndeep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008\u2013\n3021.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-\nmanet, Scott Reed, Dragomir Anguelov, Dumitru\nErhan, Vincent Vanhoucke, and Andrew Rabinovich.\n2015. Going deeper with convolutions. In Proceed-\nings of the IEEE conference on computer vision and\npattern recognition, pages 1\u20139.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nJoseph Weizenbaum. 1966. Eliza\u2014a computer pro-\ngram for the study of natural language communi-\ncation between man and machine. Commun. ACM,\n9(1):36\u201345.\nDavid H Wolpert. 1992. Stacked generalization. Neural\nnetworks, 5(2):241\u2013259.\nRui Yan, Juntao Li, Zhou Yu, et al. 2022. Deep learn-\ning for dialogue systems: Chit-chat and beyond.\nFoundations and Trends\u00ae in Information Retrieval,\n15(5):417\u2013589.\nMunazza Zaib, Quan Z. Sheng, and Wei Emma Zhang.\n2020. A short survey of pre-trained language mod-\nels for conversational ai-a new age in nlp. In Pro-\nceedings of the Australasian Computer Science Week\nMulticonference, ACSW \u201920, New York, NY, USA.\nAssociation for Computing Machinery.\nZhenyi Zhu. 2022. A simple survey of pre-trained lan-\nguage models. Preprints.org 202208.0238.\n"
  },
  {
    "title": "Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon",
    "link": "https://arxiv.org/pdf/2401.03462.pdf",
    "upvote": "23",
    "text": "Soaring from 4K to 400K: Extending LLM\u2019s Context\nwith Activation Beacon\nPeitian Zhang1,2\u2217 Zheng Liu1\u2020 Shitao Xiao1\nNinglu Shao1,2\nQiwei Ye1\nZhicheng Dou2\n1: Beijing Academy of Artificial Intelligence,\n2: Gaoling School of Artificial Intelligence, Renmin University of China\n{namespace.pt, zhengliu1026}@gmail.com\nAbstract\nThe utilization of long contexts poses a big challenge for LLMs due to their limited\ncontext window size. Although the context window can be extended through\nfine-tuning, it will result in a considerable cost at both training and inference time,\nand exert an unfavorable impact to the LLM\u2019s original capabilities. In this work,\nwe propose a new method called Activation Beacon, which condenses LLM\u2019s raw\nactivations into compact forms such that the LLM can perceive a longer context\nwith a limited context window. Activation Beacon is introduced as a plug-in\nmodule, which fully preserves the LLM\u2019s original capability in short contexts. It\nworks with the sliding window to streamingly process the long context, which\nleads to a competitive memory and time efficiency in both training and inference.\nActivation Beacon is trained with short-sequence data of diversified condensing\nratios. Thanks to such a treatment, it can be effectively learned to support different\ncontext lengths with a small training cost. Our experiment verifies Activation\nBeacon\u2019s effectiveness of context extension: it can remarkably accomplish high-\nquality extension of Llama-2-7B\u2019s context by \u00d7100 times (from 4K to 400K);\nmeanwhile, it can also achieve superior performances across a variety of long-\ncontext language modeling and understanding tasks. The source code and model\ncheckpoint are available at https://github.com/FlagOpen/FlagEmbedding.\n4K\n8K\n16K\n32K\n100K\n400K\nContext Length\n0\n20\n40\n60\n80\n100\nPerplexity\n4K\n8K\n16K\n32K\n100K\n400K\nContext Length\n0\n10\n20\n30\n40\n50\n60\n70\n80\nGPU Memory (GB)\nOOM OOM\n4K\n8K\n16K\n32K\n100K\n400K\nContext Length\n0\n2\n4\n6\n8\nInference Time (s)\nOOM\nOOM\nPosition Interpolation\nNTK Aware\nLongLlama\nActivation Beacon\nFigure 1: Comparison of the sliding window perplexity [22] between Activation Beacon and other\ncontext extension methods, including 1) Position Interpolation [5], 2) NTK-Aware Scaled RoPE [1],\n3) LongLlama [32]. Activation Beacon leads to better long-context generation quality with higher\nrunning efficiency (memory, time).\n\u2217Peitian Zhang and Zheng Liu are the co-first authors\n\u2020Zheng Liu is the corresponding author\nPreprint. Under review.\narXiv:2401.03462v2  [cs.CL]  2 Feb 2024\n(A) Activation Condensing with Beacon\n(B) AR with Condensed Activations\n<bcn>\n<bcn>\nLet\nthere\nbe\nlight\nand\nthere\nwas\nlight\n<s>\nLet\nthere\nbe\nlight\nand\nthere\nwas\n.\nAnd\nGod\nsaw\nthat\nthe\nlight\nwas\nlight\n.\nAnd\nGod\nsaw\nthat\nthe\nlight\nFigure 2: (A) The beacon token (\u27e8bcn\u27e9) is appended to a context, which prompts the LLM to\ncondense the raw activations into more compact forms; (B) The condensed activations are streamingly\nprocessed with the sliding window for auto-regression (AR).\n1\nIntroduction\nLarge language models (LLMs) need to process long contexts to accomplish many important tasks,\nsuch as retrieval-augmented generation and in-context learning. However, existing LLMs are typically\nconstrained by fixed context windows, e.g., 2K for Llama-1 [29] and 4K for Llama-2 [30], which is\nnot enough to handle some real-world scenarios. Although LLMs can be fine-tuned or retrained to\nextend their context windows [16; 6; 5; 28; 20; 32; 18], it will result in considerable costs at both\ntraining and inference time due to the quadratic computing complexity of self attention. Besides, the\ncontinued training on long-sequence data may compromise the LLM\u2019s general capability in shorter\ncontexts, which is unfavorable to their practical usage. In light of these challenges, it is desirable to\nexplore new mechanisms, which can not only realize the cost-effective extension of context length,\nbut also be compatible with the LLM\u2019s existing capabilities.\nIn this work, we propose Activation Beacon (shown as Figure 2) as a new method for LLM\u2019s context\nextension. It condenses the LLM\u2019s raw activations (i.e. keys and values from the self-attention\nmodule) into highly compact forms such that the LLM can perceive the information from a vast scope\nof context even with a limited context window. The above idea shares the common philosophy as\nsparse attention [3; 8; 38] and context compression [4; 7; 19; 22; 14]. However, it enjoys substantial\nadvantages over the previous methods in many perspectives, including the effectiveness of context\nextension (especially the quality of long-context generation and the flexibility of supporting diverse\ncontext lengths), inference and training efficiency, and the compatibility with the existing LLMs,\nthanks to a series of crucial technical designs.\nInstead of developing a new model from scratch, we argue that the LLM itself can work as a proficient\nactivation condenser with proper adaptation given its strong and well-established context representa-\ntion capability. Based on this argument, we introduce a simple but effective model architecture and\nrunning mechanism to facilitate the production and utilization of condensed activations. Particularly,\nwe introduce special tokens, known as beacon tokens (\u27e8bcn\u27e9), which prompt the LLM to condense\nthe contextual information into beacon tokens\u2019s activations (Figure 2). For a context of length l,\na team of k (k < l) beacon tokens are dispatched to the end of it, which leads to a condensing\nratio of \u03b1 (\u03b1 = l/k). We maintain another copy of the LLM\u2019s self-attention parameters, including\n{W b\nQ, W b\nK, W b\nV , W b\nO}. These new parameters are specialized to learn the activation condensing,\nwhile the original parameters in the LLM are fixed. Thus, Activation Beacon serves as a plug-in\ncomponent for the LLM, introducing extended contextual information to the LLM without adversely\naffecting its existing capabilities in short contexts.\nTo efficiently handle long contexts, we propose stream processing with the sliding window. The\nlong context is partitioned into multiple intervals of length l. A sliding window is employed to\nsequentially process one interval at a time. When dealing with the next interval, the raw activations\nof the previous interval are discarded while its condensed activations are accumulated. Therefore,\nthe sliding window is formulated as [\u27e8bcn\u27e91, . . . , \u27e8bcn\u27e9m, xm+1, . . . , xn] where \u27e8bcn\u27e9\u2217 stands for\nthe beacon tokens from previous intervals and x\u2217 is normal tokens in the current interval. The size\nof the sliding window is upper-bounded by the maximum window size of the LLM, e.g. 4K for\nLlama-2, which maintains a low memory consumption and a linear time complexity. Meanwhile, it\nalso accumulatively gathers rich contextual information from the past (\u03b1 \u2212 1) \u00d7 m + n tokens.\n2\nThe condensed activations are expected to fully encode the information within the raw activations,\nthereby assisting the LLM to accomplish high-quality generation of new content. With this considera-\ntion, we propose to learn Activation Beacon through the auto-regression task. In the sliding window,\nthe generation likelihood of the normal token xi is maximized based on the beacon tokens and its\npreceding normal tokens, i.e., max p(xi | \u27e8bcn\u27e91, . . . , \u27e8bcn\u27e9m, xm+1. . ., xi\u22121). Considering that a\ndramatic extension of context calls for a large condensing ratio, while a moderate extension just\nneeds a small condensing ratio, we perform a random sampling of \u03b1 during the stream processing.\nConsequently, the generation can be conditioned on a mixture of condensed activations with diversi-\nfied condensing ratios, which substantially contributes to the Activation Beacon\u2019s generalization in\nhandling the extension of different context lengths.\nActivation Beacon is applied to Llama-2-7B (chat), whose original context length is 4K. The training\ndata is sampled from RedPajama [10] and LongAlpaca [6], whose length are all less than 8K. The\ntraining process merely takes 10K steps, which can be accomplished within 9 hours on an 8\u00d7A800\nGPU machine. Notably, it leads to a superior quality of language modeling on the extended context\nlengths, like 8K, 16K, and 32K, whose result is even better than the fine-tuned full-attention baselines.\nIt is equally competitive on long-context understanding tasks, such as question answering and few-\nshot learning. Activation Beacon also shows the potential to establish super long contexts: by learning\nto support the condensing factor of 128, the context length of Llama-2 can be remarkably extended to\n400K (Figure 1). As a compatible module, Activation Beacon can also work with other techniques,\nlike position interpolation (\u00a7C) and retrieval (\u00a7D) for even longer and better context extension effect.\nTo summarize, we propose Activation Beacon, which realizes dramatic extension of LLM\u2019s context\nbased on the high-quality condensing of LLM\u2019s activations. It also enjoys a high running efficiency, a\nhigh compatibility with the existing LLM, and a small cost of training thanks to its optimized designs\non architecture and running mechanism. In our experiment, the effectiveness of Activation Beacon is\nverified given its superior performances across a wide range of long-context processing tasks.\n2\nActivation Beacon\n2.1\nOverview\nThe LLM exploits the contextual information while predicting the new content. The contextual\ninformation is represented by the activations, particularly the keys and values in the self-attention\nmodule. With a fixed size of context window L, a typical LLM can only query the recent L activations\nfor contextual information. However, we argue that the window size should simply be the upper\nbound of input units rather than context length. By condensing more information into each activation,\ni.e. the information from a larger scope rather a single token, the LLM will be able to perceive a\nlonger context with its original context window.\n2.2\nActivation Condensing\nWe aim to adapt the LLM itself for activation condensing given its strong context representation\ncapability. Particularly, we employ special tokens, called beacon tokens, which prompt the LLM\nto condense the contextual information into their activations. We also maintain another copy of\nthe LLM\u2019s MHA (multi-head self-attention) parameters, denoted as MHAb, including the layer-\nwise projection matrices for queries, keys, values, and outputs {W b\nQ, W b\nK, W b\nV , W b\nO}. These\nparameters are specifically learned for condensing the activations. Besides, they are lightweight,\nmerely accounting for 1/3 of the LLM\u2019s original parameters (e.g., 2B with the LLaMA-2 7B model).\nThe activation condensing is performed with the following operations (Figure 3 I). For the context\nof length l, k beacon tokens are appended to the end of it. The LLM auto-regressively encodes the\ncontext as well as the beacon tokens, as a result, the raw activations of regular tokens are generated\nand then condensed into the beacon tokens\u2019 activations. Formally, let the input features of the beacon\ntokens as Hb \u2208 Rk\u00d7D, the projections for the beacon tokens\u2019 queries, keys, and values are performed\nin the first place:\nQb \u2190 W b\nQHb,\nKb \u2190 W b\nKHb,\nV b \u2190 W b\nV Hb.\nThen, the projection results query the keys (Kr \u2208 Rl\u00d7D) and values (V r \u2208 Rl\u00d7D) of the raw\nactivations from normal tokens to generate the condensed activations, leading to a condensing ratio\n3\nFFN\nLayer Norm\nMHA\nMHA\ud835\udc4f\nLayer Norm\n+\n+\nActivation Condensing\nAR with Condensed Activations\nA. Segmentation\nB. Stepwise expansion\nC. Full-coverage\nI. Working Mechanism of Activation Beacon\nII. Attention Scheme of Beacons\nFigure 3: (I) The raw activations of ordinal tokens (the blue square) are condensed into the compact\nactivations of beacon tokens (the green squere). Future tokens are auto-regressively generated\nconditioned on the raw activations in the current interval and the condensed activations accumulated\nfrom previous intervals. (II) The attention schemes for activation condensing.\n\u03b1 = l/k:\nA \u2190 softmax\n \nmask\n \nQb{Kr \u2295 Kb}T\n\u221a\nD\n!!\n,\nOb \u2190 W b\nOA{V r \u2295 V b}.\n(1)\nThe final output of self-attention is produced by the concatenation of both raw activations from the\nnormal tokens and the condensed activations from the beacon tokens.\nTo optimize the quality of activation condensing, we explore three attention schemes for the beacon\ntokens, i.e. the mask(\u00b7) operator, which are as shown in Figure 3 II. 1) Segmentation, where each\nbeacon can attend to an equally segmented span of the context. 2) Stepwise expansion, where each\nbeacon can attend to one more span than its predecessor, and the last beacon can attend to the entire\ncontext. 3) Full coverage, where the entire context can be attended by all beacons. For all three\noptions, we restrict the context length l to be evenly divisible by the number of beacon tokens k.\nBesides, the beacon tokens are always positioned next to the last normal token it can attend to.\nAlthough the three options are of the same computation cost, it\u2019s empirically found that the second\noption, i.e. the stepwise expansion, leads to the optimal performance (\u00a75).\n2.3\nStream Processing\nThe long context is partitioned into multiple intervals of length l. A sliding window is employed to\nsequentially process one interval at a time. When dealing with the next interval, the raw activations\nof the previous interval are discarded while its condensed activations are accumulated. Therefore,\nthe sliding window consists of m beacon tokens (i.e. \u27e8bcn\u27e9) from the past intervals, and the normal\ntokens in the current interval. With the above formulation, the next token is predicted as:\np(xn | \u27e8bcn\u27e91, . . . , \u27e8bcn\u27e9m, xm+1, . . . , xn\u22121; \u0398, \u0398b),\n(2)\nwhere \u0398 denotes the parameters of the LLM and \u0398b denotes the introduced parameters for beacons.\nCrucially, both \u27e8bcn\u27e9\u2217 and x\u2217, are encoded by their relative positions within the sliding window,\nregardless of their absolute positions in the entire context. The size of the sliding window is up-\nbounded by the context window size of the LLM, which results in a competitive running efficiency for\n4\nboth training and inference. Different from the typical stream processing where the context beyond\nthe sliding window is discarded [36], our method can accumulatively cover the information from the\npast (\u03b1 \u2212 1) \u00d7 m + n tokens. Note that the above working mechanism may also benefit from the\nincreasing of window size, as more beacon tokens can be accumulated in the sliding window to cover\nan even longer context. Consequently, Activation Beacon can work with strategies like NTK [1],\nPI [5] for further extension of the context. Detailed collaboration effect is explored in Appendix C.\n2.4\nLearning Method\nPlug-in to LLM. As introduced, Activation Beacon introduces the following parameters\n(\u0398b): 1) the beacon token\u2019s embedding e\u27e8bcn\u27e9, 2) the linear projection matrices for MHAb:\n{W b\nQ, W b\nK, W b\nV , W b\nO} in each transformer layer. Overall, it accounts for less than 1/3 of the\nLLM\u2019s original size, e.g., 2B with the Llama-2-7B model. Activation Beacon reuses other trans-\nformer modules from the LLM (i.e., MLP and LayerNorm). This turns out to be the optimal trade-off\nbetween effectiveness and training cost. Activation Beacon is learned while all of the LLM\u2019s original\nparameters are frozen. Besides, it is only used to generate the condensed activations without interfer-\ning the inference process of normal tokens. Therefore, it serves as a plug-in module for the LLM,\nwhich introduces the long contextual information without affecting the LLM\u2019s existing capabilities in\nprocessing short contexts.\nAuto-Regression. We train Activation Beacon by auto-regression, where the next token is predicted\nbased on the condensed activations from the beacon tokens and the raw activations from the ordinary\ntokens. As mentioned in \u00a72.2, a training instance is partitioned into equal-sized intervals of length l\nand streamingly processed. Afterwards, the following loss is minimized:\nmin\n\u0398b .\n\u2308|X|//l\u2309\nX\nj=1\nl\nX\ni=1\n\u2212 log p(xj\ni|\u27e8bcn\u27e91, . . . , \u27e8bcn\u27e9mj, xj\n1, . . . , xj\ni\u22121; \u0398, \u0398b).\n(3)\nwhere xj\ni is the i-th token in the j-th interval of X, mj stands for the number of beacon tokens\naccumulated before the j-th interval, whose value depends on the condensing ratio of each preceding\ninterval (mj = Pj\u22121\nz=1(l//\u03b1z)).\nStep-wise randomized condensing ratio. The training is performed purely with short-sequence\ndata, i.e. 1024 < |X| < 8192, where the majority of training samples are less than 4K (Table 6).\nTherefore, we are able to achieve superior training efficiency. To generalize Activation Beacon to\nsupport different context lengths, e.g., 16K, 32K, 100K, and even longer, the auto-regression needs to\nbe conditioned on different amounts of beacon tokens with diversified condensing ratios. For this\npurpose, we randomly sample the condensing ratio for each interval within a large candidate scope:\n\u03b1j \u223c {2, 4, 8, ... 128}, which will introduce dramatic diversity to the condensing ratios and amount\nof beacon tokens within the auto-regression process.\n3\nExperiment\nOur experiments are performed for the exploration of the following issues. 1) Activation Beacon\u2019s\nimpact on the long-context generation capabilities (measured by Perplexity). 2) Activation Beacon\u2019s\nimpact on the long-context utilization capability (reflected by tasks like long document QA and\nsummarization). 3) Activation Beacon\u2019s impact on efficiency in terms of GPU memory and inference\ntime. 4) The individual contribution of different technical factors.\n3.1\nSettings\nImplementation. Our method is applied to Llama-2-7B (chat) [30] for empirical studies. Our\ntraining data is a mixture of 80K sampled data from RedPajama [10] and LongAlpaca [6] (70K from\nRedPajama and 10K from LongAlpaca, respectively). The sequence length of each sample is between\n1024 and 8192. The statistics of our training data is reported in Table 6. We use a single 8\u00d7A800 GPU\nmachine for training. The training is performed for 10,000 steps (one epoch of the whole training\ndata) with a batch size of 8 and a learning rate of 5e-5 using the linear scheduler. The length of the\ncontext interval is set to 1024. The condensing ratio is sampled from {2, 4, 8, 16, 32, 64, 128} during\ntraining. As introduced, Llama\u2019s own parameters are freezed throughout the training process.\n5\nTable 1: Sliding window perplexity of different context window extension methods on PG19, Proof-\nPile, and CodeParrot. Activation Beacon successfully extends the context window of Llama-2-7B\nmodel to sequences much longer than the ones seen during training.\nMethod\nPG19\nProof-Pile\nCodeParrot\n4K\n16K\n32K\n100K\n4K\n16K\n32K\n100K\n4K\n16K\n32K\n100K\nLlama-2-7B\n9.21\n>103\n>103\nOOM\n3.47\n>103\n>103\nOOM\n2.55\n>103\n>103\nOOM\nPI\n9.21\n19.5\n>102\nOOM\n3.47\n5.94\n33.7\nOOM\n2.55\n4.57\n29.33\nOOM\nNTK\n9.21\n11.5\n37.8\nOOM\n3.47\n3.65\n7.67\nOOM\n2.55\n2.86\n7.68\nOOM\nStreamingLLM\n9.21\n9.25\n9.24\n9.32\n3.47\n3.51\n3.50\n3.55\n2.55\n2.60\n2.54\n2.56\nAutoCompre.-6K\n11.8\n>102\n>103\nOOM\n4.55\n>102\n>103\nOOM\n5.43\n>102\n>103\nOOM\nYaRN-128K\n6.68\n6.44\n6.38\nOOM\n2.70\n2.47\n2.41\nOOM\n2.17\n2.04\n2.00\nOOM\nLongChat-32K\n9.47\n8.85\n8.81\nOOM\n3.07\n2.70\n2.65\nOOM\n2.36\n2.16\n2.13\nOOM\nLongAlpaca-16K\n9.96\n9.83\n>102\nOOM\n3.82\n3.37\n>103\nOOM\n2.81\n2.54\n>103\nOOM\nLongLlama\n9.06\n8.83\nOOM\nOOM\n2.61\n2.41\nOOM\nOOM\n1.95\n1.90\nOOM\nOOM\nActivation Beacon\n9.21\n8.34\n8.27\n8.50\n3.47\n3.34\n3.32\n3.31\n2.55\n2.43\n2.41\n2.62\nBaselines. The following types of baselines are chosen for comparison (all based on the LLaMA-2-7B\n(chat) model unless otherwise specified). 1) The basic method, i.e. LLaMA-2-7B (chat) [29] with 4K\ncontext length. 2) The fine-tuning free methods, including Positional Interpolation (PI) [5], the NTK-\nAware Scale ROPE (NTK) [1], and StreamingLLM [36]. 3) The fine-tuned full-attention methods,\nincluding LongChat-32K [16], LongAlpaca-16K [6], YaRN-128K [20]. 4) The fine-tuned methods\nwith adapted architectures for long contexts, including AutoCompressor-6K [7] and LongLlama [32]\n(based on CodeLlama [24]). We enable FlashAttention-2 [11] to accelerate self-attention computation\nand save GPU usage for all the baselines. At present, Activation Beacon is incompatible with\nFlashAttention-2 due to its utilization of the customized attention scheme; thus, we use the scaled dot\nproduct attention (sdpa) from PyTorch [17] for acceleration.\n3.2\nMain Results\n3.2.1\nLong-Context Language Modeling\nThe experiment on long-context language modeling is performed with three datasets: PG19 [22],\nProof-Pile [40], and CodeParrot [31]. Specifically, for PG19, we use its entire test set with 100 books.\nFor Proof-Pile, we extract the arxiv papers from the test set that are longer than 32K, which are 79\npapers in total. For CodeParrot, there is no pre-defined test set. Following previous studies [25; 39],\nwe first concatenate code from the same repository to form long sequences, then we sample 100\nsequences for evaluation. The perplexity is computed with a sliding window of size 2K [21].\nThe evaluation results are reported in Table 1, where Activation Beacon leads to a superior long-\ncontext language modeling performance. First of all, it not only outperforms the Llama-2-7B baseline\nbut also results in a notably improved performance than the fine-tuning free methods. It is worth\nnoting that with the extension of context from 4K to 32K, the language modeling performance can be\ngradually improved by Activation Beacon, indicating that the expanded information from the longer\ncontext can be effectively utilized to facilitate the generation. By comparison, the language modeling\nperformance is decreased with other fine-tuning-free methods. Most of them become ineffective after\nthe context length goes beyond 32K.\nSecondly, Activation Beacon\u2019s performance is comparable to or even better than the fine-tuned\nfull-attention methods. This result is remarkable knowing that Activation Beacon runs with a much\nhigher efficiency (to be analyzed in Section 3.3). Although there are cases where some of the\nfine-tuned full-attention baselines achieve better performances, their empirical advantages may not\nbe fully resulted from the introduction of long contextual information. For example, YaRN-128K\u2019s\nperformance has already been notably higher than Llama-2-7B at the context length of 4K, and so\nis the case with LongChat-32K on Proof-Pile and CodeParrot. Note that the update of the LLM\u2019s\noriginal parameters is not always favorable because it may not be well generalized to many other\nscenarios. By comparison, our method is simply a plug-in module to introduce long contextual\ninformation without affecting the LLM\u2019s existing capabilities.\nThirdly, Activation Beacon is able to achieve a much longer extension of the context than the rest of\nthe methods. Particularly, it maintains a quality generation performance after the context length is\n6\nTable 2: Evaluation of different methods on LongBench. Activation Beacon performs on par with the\nfine-tuned full-attention baselines.\nMethod\nSingle-Doc QA\nMulti-Doc QA\nSummarization\nFew-Shot\nCode\nLlama-2-7B\n24.90\n22.60\n24.70\n60.00\n48.10\nPI\n18.98\n17.16\n25.03\n49.43\n52.73\nNTK\n23.21\n23.34\n24.40\n59.29\n49.28\nStreamingLLM\n21.47\n22.22\n22.20\n50.05\n48.00\nAutoCompressor-6K\n13.22\n10.61\n14.00\n15.72\n23.62\nYaRN-128K\n24.03\n24.11\n19.82\n60.00\n62.73\nLongChat-4K\n28.14\n21.88\n26.59\n62.06\n52.77\nLongChat-32K\n31.58\n23.50\n26.70\n64.02\n54.10\nLongAlpaca-4K\n26.81\n24.44\n26.93\n62.92\n55.15\nLongAlpaca-16K\n28.70\n28.10\n27.80\n63.70\n56.00\nLongLlama\n30.12\n16.37\n24.19\n60.31\n66.05\nActivation Beacon\n28.27\n28.44\n25.15\n61.00\n57.75\n4K\n6K\n8K\n10K\n12K\n14K\n16K\nContext Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nLlama-2-7B\nPI\nNTK\nStreamingLLM\nAutoCompressor-6K\nYarn-128K\nLongChat-32K\nLongAlpaca-16K\nLongLlama\nActivation Beacon\nFigure 4: The evaluation of topic retrieval accuracy at different context lengths. Activation Beacon is\ncompetitive against the fine-tuned methods, like LongChat-32K and LongAlpaca-16K.\nextended to 100K, where most of the baselines become either ineffective or out-of-memory (OOM).\nIn fact, Activation Beacon is still effective even after the context length is further extended to 400K\n(see Figure 1), which means a 100\u00d7 extension of Llama-2-7B\u2019s maximum context length. Unlike\nmany other methods like fine-tuning, Activation Beacon does not require any long-sequence training\ndata to acquire such a super long-context capability, which contributes to its high usability in practice.\n3.2.2\nMore Long-Context Tasks\nWe further study the five real-world tasks from LongBench [2], including single-doc QA, multi-doc\nQA, summarization, few-shot learning, and code completion, where the experiment result on each\ntask is reported in Table 2. We also evaluate the topic retrieval task [16], whose result is shown\nin Figure 4. In Appendix D, we evaluate the passkey retrieval task [35]. Similar to our previous\nobservation on long-context language modeling, Activation Beacon leads to a notable improvement\nover Llama-2-7B and the fine-tuning-free baselines. Meanwhile, it reaches a comparable performance\nwith the fine-tuned full-attention methods. Because a large portion of the evaluation samples can be\n(almost) covered by the 16K or 32K context window, the fine-tuned full-attention methods indeed\nset a high standard on LongBench. However, knowing that the fine-tuning operation will change\nthe LLM\u2019s original parameters, it is still interesting to investigate where the empirical advantage of\nthe finetuned methods comes from. To figure out this problem, we benchmark the performance of\n7\nTable 3: Evaluation of inference time and GPU memory usage. Both metrics are measured by the\naverage value of 100 forward passes (FlashAttention-2 is enabled for LongChat).\nMethod\nGPU Memory (GB)\nInference Time (s)\n4K\n8K\n16K\n32K\n100K\n4K\n8K\n16K\n32K\n100K\nLongChat-32K\n18.5\n24.2\n35.6\n58.4\nOOM\n0.045\n0.089\n0.191\n0.460\nOOM\nStreamingLLM\n19.9\n19.9\n19.9\n19.9\n19.9\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nAutoCompressor-6K\n17.7\n22.6\n32.3\n51.7\nOOM\n0.087\n0.134\n0.224\n0.478\nOOM\nLongLlama\n18.2\n21.9\n34.2\nOOM\nOOM\n0.079\n0.190\n0.436\nOOM\nOOM\nActivation Beacon\n21.7\n21.3\n21.4\n21.6\n21.6\n0.071\n0.121\n0.237\n0.473\n1.494\nLongChat-32K and LongAlpaca-16K at the context length of 4K, where they use the same information\nas the Llama-2-7B baseline. Interestingly, both methods result in a substantial improvement over\nLlama-2-7B on every task. Especially for summarization, where both methods are already sufficiently\nstrong at 4K, yet little extra improvements are made with the further extended context window. By\ncomparison, Activation Beacon inherits Llama-2-7B\u2019s performance at the context length of 4K, where\nits performance gain over Llama-2-7B is introduced by the extended context. In this sense, its impact\non utilizing the long contextual information can still be no inferior to the ones from the finetuned\nmethods in the corresponding situations.\n3.3\nEfficiency Analysis\nTable 4: Comparison of training time and GPU\nmemory cost between LongAlpaca-16K (8xA100\nGPUs) and Activation Beacon (8xA800 GPUs).\nMethod\nTime (Hour) Memory (GB)\nLongAlpaca-16K\n20.8\n57.1\nActivation Beacon\n9.0\n55.9\nWe evaluate the running efficiency at the infer-\nence time in terms of time cost and GPU mem-\nory usage, whose results are reported in Table 3.\nCompared with LongChat (full-attention) and\nLongLlama, Activation Beacon enjoys a much\nsmaller GPU memory usage at the long context.\nActivation Beacon and StreamingLLM result in\na similar memory cost because both methods\nare based on sliding windows. As for the in-\nference time, Activation Beacon is faster than\nLongLlama, but slower than LongChat when the context is short. This is because Activation Beacon\nis streamingly processed while LongChat is fully parallel3. However, Activation Beacon is able to\ngradually catch up when the context length gets longer, as its time complexity is linear to the context\nlength. It will ultimately become much faster than the full-attention methods if the context length is\nextended long enough. Finally, we compare our training cost with LongAlpaca, which is featured for\nits high training efficiency (shown in Table 4). Under a similar hardware condition (8\u00d7A800 GPUs\nvs. 8\u00d7A100 GPUs), the training of Activation Beacon can be accomplished in just 9 hours, which is\neven faster than the reported time cost of LongAlpaca-16K with S2-attn4 (\u00a72.4).\n3.4\nAblation Studies\nWe perform ablation studies to evaluate the impact from different technical factors, including the\nattention scheme of beacons (\u00a72.2), the sampling strategy of condensing ratio (\u00a72.4), the introduced\nparameters for beacons (\u00a72.4), and the composition of training data (\u00a73.1). The experiment results\nare shown in Table 5.\nFirst of all, we can find that the attention scheme exerts a substantial impact on Activation Beacon\u2019s\nperformances on both long-context language modeling (PG19) and long-context understanding (QA).\nThe stepwise expansion works with the gradually expanded attention scope. Therefore, it enables the\nbeacons to acquire different levels of local and global information of each context interval, which\nnotably improves the performance over the other two options.\n3StreamingLLM is slow due to its current step-wise realization, yet its theoretical speed should be comparable\nwith our method.\n4https://openreview.net/forum?id=6PmJoRfdaK\n8\nTable 5: The impact of different technical factors: attention scheme of beacon token, condensing ratio,\ncomposition of training data. Performances are measured by PG19 with 32K context and single-Doc\nQA on LongBench. Default settings are marked by *.\nFactor\nSetting\nPG19\nQA\nAttention\nScheme\nSegmentation\n8.39\n26.05\nFull coverage\n8.76\n23.13\nStepwise expansion*\n8.27\n28.27\nCondensing\nRatio\nMonotonous (\u03b1 = 4)\n> 102\n26.48\nInstance-wise randomized\n8.19\n26.33\nStep-wise randomized*\n8.27\n28.27\nBeacon\nParameters\nQ, K, V (1.5B)\n8.32\n27.04\nQ, K, V, O, MLP (5.5B)\n8.81\n23.46\nQ, K, V, O (2.0B)*\n8.27\n28.27\nData\nComposition\nRedPajama only\n8.24\n24.98\nRedPajama+LongAlpaca*\n8.27\n28.27\nSecondly, the sampling of the condensing ratio is another influential factor. In this place, we\ncompare two alternative strategies. The instance-wise option samples one condensing ratio for all\ncontext intervals of each training instance X (from the same scope as the step-wise method, i.e.\n{2, 4, 8, . . . , 128}). While the monotonous option makes use of one constant condensing ratio of\n4 (which can support a up-to 16K context length). We can observe that the step-wise sampling\nstrategy, which introduces the most diversified condensing ratios when learning, results in competitive\nperformance on perplexity while significantly outperforms the other two options on long-context\nunderstanding.\nThirdly, we analyze the impact by introducing different amounts of learnable parameters to the beacon\nmodule. Specifically, when we remove the output projection matrix W b\nO from the beacon parameters\nMHAb (\u00a72.2), the empirical performances on both tasks degrade. When we additionally include\nthe MLP parameters of FFN, the model\u2019s performance does not improve. We conjecture that this is\nprobably because the FFN layer is heavily loaded, which slows down the convergence of the training\nprocess. As a result, it suggests that our current formulation of the learnable parameters is a good\ntrade-off between cost and effectiveness.\nLastly, we can also observe that only using RedPajama as the training data already leads to a\ncompetitive performance on both evaluation tasks. The introduction of more training data from\nLongAlpaca contributes little to the language modeling task. However, it brings an additional\nimprovement to the empirical performance on Single-Doc QA.\n4\nRelated Works\nWe discuss the following works which are devoted to the extension of LLM\u2019s context. First of all,\na large body of methods have been proposed to increase the size of context window. For example,\nALiBi [21] leverages linear-decaying attention biases to achieve the extrapolation of position encoding.\nMethods like Position Interpolation [5], NTK-Aware scaling [1] and ReRoPE [26] make progress on\ntop of RoPE [27], which enable the LLM to handle unseen positions at the inference time. Although\nsuch methods can be directly applied to the well-trained LLM, they usually benefit from continual\nfine-tuning where the extended context can be better utilized [20]. The fine-tuning with long-sequence\ndata is expensive. Thus, people investigate how to reduce the training cost. For example, LongLora [6]\nproposes S2-Attn and leverages LoRA for cost-effective training; while PoSE [41] uses skip-wise\nposition indices to train LLMs on 2K context length as a simulation of 128K. However, the fine-tuning\noperations are still prone to big costs if super long-sequence data is presented. Finally, the fine-tuning\noperation may impair the LLM\u2019s existing capabilities on short contexts [20]. By comparison, our\nmethod is trained with a small cost and enjoys a high efficiency in training and inference. Besides, it\nserves as a plug-in module that is fully compatible with the existing LLM.\n9\nThe quadratic complexity of transformer is a major bottleneck to achieve long contexts. Thus, many\nprevious works aim to address this problem by using sparse attention [8; 3; 38; 12] or approximate\nattention computation [15; 33; 9; 23]. However, there are threefold challenges about these methods\nas analyzed in [36]: the requirement of customized GPU kernels for specific variants of matrix\nmultiplication, the dependency on global attention patterns which are unsuitable for autoregressive\nlanguage models, the incompatibility with the well-pretrained models. In contrast, our method is free\nfrom these constraints and preserves a high compatibility with the existing LLMs.\nIt is also plausible to find ways to process long contexts with short context windows. One popular\nstrategy is to use sliding windows. For example, StreamingLLM [36] and LM-Infinite [13] are able to\nachieve an infinite context by only maintaining the activations for the very first and the latest tokens.\nHowever, they are unable to leverage the rich information from the long context because the portion\nbeyond the sliding window will be discarded. Besides, the long contexts can also be summarized\nand compressed into more compact forms [4; 7; 19; 22; 14], which follow the same spirit as our\nwork. However, the previous methods call for major changes to the original model\u2019s architecture and\nworking process, which brings in many problems. Notably, they are prone to substantial compression\nlosses which prevent them from making extensions for long contexts. Besides, they lack the flexibility\nto support different context lengths, and suffer from the incompatibility with existing LLMs.\nFinally, it becomes popular to offload the long context into external memory and retrieve the useful\npart from it as the working context. The retrieved data can be either the chunked input [37; 39] or\nthe cached KV activations, e.g., Memorizing Transformers [35] and LongMem [34]. This idea has\nbeen further extended by many recent works. For example, Landmark Attention [18] uses a special\ntoken to represent a chunk of activations, which enables more efficient computation of retrieval.\nFocused Transformers [32] proposes to use contrastive training which improves the discrimination of\nrelevant keys from the cached data. The retrieval-based methods can be limited due to the utilization\nof incoherent context. However, it tackles the the problem from a different perspective which can\nbenefit from the collaboration with our method (explored in Appendix D).\n5\nConclusion\nWe introduce Activation Beacon for the extension of LLM\u2019s context length. Activation Beacon\ncondenses the LLM\u2019s raw activations into highly compact forms, enabling the LLM to perceive\na long context with a limited context window. As a plug-in component for the LLM, it brings in\nlong contextual information while fully preserving the LLM\u2019s existing capabilities in short contexts.\nWhen dealing with long-sequence data, it resorts to a sliding window for stream processing, which\nleads to a superior working efficiency for both training and inference. By using short-sequence data\nwith diversely sampled condensing ratios, it can be effectively learned to support different context\nlengths with a small training cost. Our experiment verifies Activation Beacon as an effective, efficient,\ncompatible, and low-cost method to extend the context length for LLMs.\nBroader Impact\nActivation Beacon establishes long-context capabilities for the large language model without af-\nfecting its original capabilities. This enhancement may benefit many long-context scenarios using\nLLMs, such as long document understanding/summarization, and lifelong chating with long-term\nmemory. Therefore, it is particularly useful for AI applications like AI readers and lifelong AI\nchatbots. Activation Beacon is able to compress the raw activations of LLM into fewer yet more\ncompact ones with minimal loss. As a result, it can reduce the Key-Value cache requirements for\nnumerous AI applications, leading to significant resource savings. Moreover, compared to full\nattention mechanisms, Activation Beacon requires considerably fewer computational resources with\ncompetitive speed. This efficiency also contributes to environmental sustainability. As a downside,\nsince Activation Beacon is based on the LLM, it inherits the internal biases of the LLM. Consequently,\nthere is a risk of generating unreliable or harmful content, which underscores the need for careful\nmonitoring the ethical usage of these AI systems.\n10\nReferences\n[1] Ntk-aware scaled rope, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/\n14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/.\n[2] Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L.,\nDong, Y., Tang, J., and Li, J. Longbench: A bilingual, multitask benchmark for long context\nunderstanding. arXiv preprint arXiv:2308.14508, 2023.\n[3] Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR,\nabs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150.\n[4] Bulatov, A., Kuratov, Y., and Burtsev, M. S. Scaling transformer to 1m tokens and beyond\nwith RMT. CoRR, abs/2304.11062, 2023. doi: 10.48550/ARXIV.2304.11062. URL https:\n//doi.org/10.48550/arXiv.2304.11062.\n[5] Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models\nvia positional interpolation. arXiv preprint arXiv:2306.15595, 2023.\n[6] Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning\nof long-context large language models. arXiv preprint arXiv:2309.12307, 2023.\n[7] Chevalier, A., Wettig, A., Ajith, A., and Chen, D. Adapting language models to compress\ncontexts. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December\n6-10, 2023, pp. 3829\u20133846. Association for Computational Linguistics, 2023. URL https:\n//aclanthology.org/2023.emnlp-main.232.\n[8] Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n[9] Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., Hawkins,\nP., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A.\nRethinking attention with performers. In 9th International Conference on Learning Repre-\nsentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL\nhttps://openreview.net/forum?id=Ua6zuk0WRH.\n[10] Computer, T. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURL https://github.com/togethercomputer/RedPajama-Data.\n[11] Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR,\nabs/2307.08691, 2023. doi: 10.48550/ARXIV.2307.08691. URL https://doi.org/10.\n48550/arXiv.2307.08691.\n[12] Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., Zheng, N., and Wei, F. Longnet:\nScaling transformers to 1, 000, 000, 000 tokens. CoRR, abs/2307.02486, 2023. doi: 10.48550/\nARXIV.2307.02486. URL https://doi.org/10.48550/arXiv.2307.02486.\n[13] Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Simple on-\nthe-fly length generalization for large language models. CoRR, abs/2308.16137, 2023. doi:\n10.48550/ARXIV.2308.16137. URL https://doi.org/10.48550/arXiv.2308.16137.\n[14] Huang, X. and Hollenstein, N. Long-range language modeling with selective cache. In Bouamor,\nH., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics:\nEMNLP 2023, Singapore, December 6-10, 2023, pp. 4838\u20134858. Association for Computational\nLinguistics, 2023. URL https://aclanthology.org/2023.findings-emnlp.321.\n[15] Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. In 8th Interna-\ntional Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkgNKkHtvB.\n[16] Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J. E., Stoica, I., Ma, X., and\nZhang, H. How long can open-source llms truly promise on context length?, June 2023. URL\nhttps://lmsys.org/blog/2023-06-29-longchat.\n11\n[17] Michael Gschwind, Driss Guessous, C. P. Accelerated pytorch 2 transformers. https://\npytorch.org/blog/accelerated-pytorch-2/, 2023.\n[18] Mohtashami, A. and Jaggi, M. Landmark attention: Random-access infinite context length for\ntransformers. arXiv preprint arXiv:2305.16300, 2023.\n[19] Mu, J., Li, X. L., and Goodman, N. D. Learning to compress prompts with gist tokens. CoRR,\nabs/2304.08467, 2023. doi: 10.48550/ARXIV.2304.08467. URL https://doi.org/10.\n48550/arXiv.2304.08467.\n[20] Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of\nlarge language models. arXiv preprint arXiv:2309.00071, 2023.\n[21] Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases\nenables input length extrapolation. In The Tenth International Conference on Learning Rep-\nresentations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL\nhttps://openreview.net/forum?id=R8sQPpGCv0.\n[22] Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and Lillicrap, T. P. Compressive\ntransformers for long-range sequence modelling. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\nURL https://openreview.net/forum?id=SylKikSYDH.\n[23] Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuurmans, D., and Dai, B. Combiner: Full\nattention transformer with sparse computation cost. In Ranzato, M., Beygelzimer, A., Dauphin,\nY. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems\n34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, De-\ncember 6-14, 2021, virtual, pp. 22470\u201322482, 2021. URL https://proceedings.neurips.\ncc/paper/2021/hash/bd4a6d0563e0604510989eb8f9ff71f5-Abstract.html.\n[24] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T.,\nRapin, J., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950,\n2023.\n[25] Rubin, O. and Berant, J.\nLong-range language modeling with self-retrieval.\nCoRR,\nabs/2306.13421, 2023. doi: 10.48550/ARXIV.2306.13421. URL https://doi.org/10.\n48550/arXiv.2306.13421.\n[26] Su, J. Rectified rotary position embeddings. https://github.com/bojone/rerope, 2023.\n[27] Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position\nembedding. CoRR, abs/2104.09864, 2021. URL https://arxiv.org/abs/2104.09864.\n[28] Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., and Wei,\nF. A length-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022.\n[29] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B.,\nGoyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023.\n[30] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra,\nS., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023.\n[31] Tunstall, L., Von Werra, L., and Wolf, T. Natural language processing with transformers, 2022.\n[32] Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u00b4s, P. Focused\ntransformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023.\n[33] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear\ncomplexity. CoRR, abs/2006.04768, 2020. URL https://arxiv.org/abs/2006.04768.\n[34] Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J., and Wei, F. Augmenting language\nmodels with long-term memory. CoRR, abs/2306.07174, 2023. doi: 10.48550/ARXIV.2306.\n07174. URL https://doi.org/10.48550/arXiv.2306.07174.\n12\n[35] Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. In The Tenth\nInternational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,\n2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=TrjbxzRcnf-.\n[36] Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with\nattention sinks. arXiv preprint arXiv:2309.17453, 2023.\n[37] Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E.,\nShoeybi, M., and Catanzaro, B. Retrieval meets long context large language models. CoRR,\nabs/2310.03025, 2023. doi: 10.48550/ARXIV.2310.03025. URL https://doi.org/10.\n48550/arXiv.2310.03025.\n[38] Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P.,\nRavula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances\nin neural information processing systems, 33:17283\u201317297, 2020.\n[39] Zhang, P., Xiao, S., Liu, Z., Dou, Z., and Nie, J. Retrieve anything to augment large language\nmodels. CoRR, abs/2310.07554, 2023. doi: 10.48550/ARXIV.2310.07554. URL https:\n//doi.org/10.48550/arXiv.2310.07554.\n[40] Zhangir Azerbayev, Edward Ayers, B. P. Proof-pile. https://huggingface.co/datasets/\nhoskinson-center/proof-pile, 2022.\n[41] Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., and Li, S. Pose: Efficient context\nwindow extension of llms via positional skip-wise training. CoRR, abs/2309.10400, 2023. doi:\n10.48550/ARXIV.2309.10400. URL https://doi.org/10.48550/arXiv.2309.10400.\n13\nA\nOverall Algorithm of Activation Beacon\nAlgorithm 1 Activation Beacon\nRequire: The LLM \u0398; the input context X; the memory of condensed activations Kb \u2208\nR\u2217\u00d7L\u00d7D, V b \u2208 R\u2217\u00d7L\u00d7D; the memory of raw activations Kr \u2208 R\u2217\u00d7L\u00d7D, V r \u2208 R\u2217\u00d7L\u00d7D; the\ninterval length l; and the starting index of the sliding window is.\n1: repeat\n2:\nGet the ending index of the sliding window: ie \u2190 is + l\n3:\nif ie \u2264 |X| then\n4:\nis_full_window \u2190 True\n5:\nSet condensing ratio for this interval: \u03b1 \u2190 set_ratio()\n6:\nSet number of special tokens for this interval: \u03b2 \u2190 l//\u03b1\n7:\nThe sliding window contains regular tokens in the context appended with beacon tokens:\nw \u2190 Xis:ie + [\u27e8bcn\u27e9] \u00d7 \u03b2\n8:\nelse\n9:\nis_full_window \u2190 False\n10:\nThe sliding window contains only the regular tokens in the context: w \u2190 Xis:ie\n11:\nend if\n12:\nThe memory of the LLM is the concatenation of the condensed activations and the raw\nactivations:\nK \u2190 Kb \u2295 Kr,\nV \u2190 V b \u2295 V r\n13:\nThe LLM auto-regressively encodes the regular tokens as well as the beacon tokens in the\nsliding window (note that activation condensing happens according to Equation 1). The logits\nof the next token and the newly generated activations are returned:\n\u03c8, K\u2032, V \u2032 \u2190 \u0398(w, K, V )\n14:\nif is_full_window then\n15:\nThe last \u03b2 activations are condensed activations, which are accumulated:\nKb \u2190 Kb \u2295 K\u2032\n\u2212\u03b2:,\nV b \u2190 V b \u2295 V \u2032\n\u2212\u03b2:\n16:\nThe raw activations of previous intervals are emptied:\nKr.empty(),\nV r.empty()\n17:\nUpdate the starting index is \u2190 ie\n18:\nelse\n19:\nThe raw activations from regular tokens are cached:\nKr \u2190 Kr \u2295 K\u2032,\nV r \u2190 V r \u2295 V \u2032\n20:\nend if\n21: until ie \u2265 |X|\n22: Offset the starting index for future generation: is \u2190 is \u2212 |X|\n23: Return \u0398, \u03c8, Kb, V b, Kr, V r, is\nB\nLength Distribution of Training Data\nC\nCombining Activation Beacon with Context Window Extension\nTechniques.\nActivation Beacon can be combined with context window extension techniques to further extend the\ncontext length. Specifically, Activation Beacon condenses the raw activations of LLM into more\ncompact forms so that the LLM can perceive more information given its original context window size.\nIt does not modify the position encoding scheme of the LLM within the context window. Therefore,\n14\nLength\n1K\u223c2K\n2K\u223c4K\n4K\u223c6K\n6K\u223c8K\nTotal\nCount\n38K\n23K\n6K\n13K\n80K\nPortion\n47%\n29%\n8%\n16%\n100%\nTable 6: The length distribution of training data. The average length of all training data is 3180.\n100K\n200K\n300K\n400K\n500K\n600K\n700K\n800K\n900K\n1M\nContext Length\n0\n10\n20\n30\n40\n50\nPerplexity\nMethod\nActivation Beacon\nActivation Beacon + PI\nActivation Beacon + NTK\nFigure 5: The perplexity evaluated on books longer than 400K tokens in the PG19 test set. Activation\nbeacon can work together with context window extension methods, further extending the context\nlength to even 1M tokens.\nwe can directly employ the modern context window extension techniques, such as PI [5] and NTK [1],\nto expand the window size so that more condensed activations can be accomodated and hence further\ncontext extension effect.\nIn Figure 5, we show Activation Beacon can extend the context length of Llama-2 to 1M when\ncombined with PI and NTK, without any further fine-tuning. This result unveils the superior\ncompatibility of Activation Beacon: it is not only compatible with existing capabilities of the LLM,\nbut also compatible with any future advances of context window extension techniques.\n4K\n8K\n16K\n24K\n32K\n40K\n64K\n100K\nContext Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(A)\nMethod\nLlama2-7B\nPI\nNTK\nActivation Beacon\nActivation Beacon + BM25\n4K\n8K\n16K\n24K\n32K\n40K\n64K\n100K\nContext Length\n0\n20\n40\n60\n80\n100\nFuzzy Score\n(B)\nMethod\nLlama2-7B\nPI\nNTK\nActivation Beacon\nActivation Beacon + BM25\nFigure 6: The accuracy and the fuzzy score on tha Passkey Retrieval task. Activation beacon can\nwork together with retrieval techniques to accurately remember the passkey.\n15\nD\nCombining Activation Beacon with Retrieval.\nActivation Beacon condenses the raw activations and accumulates them in the sliding window for later\nauto-regression. However, the size of the sliding window is up-bounded by the LLM\u2019s context window\nsize, which limits the number of condensed activations that could be maintained. Consequently,\nActivation Beacon needs to trade-off between the memory accuracy and the context length: With a\nlow condensing ratio, Activation Beacon can remember nearly all details about the context, while\nsimultaneously producing more condensed activations for the context. Under this setting, Activation\nBeacon can only deal with a relatively short context (e.g. 8K). In contrast, with a high condensing\nratio (e.g. 128), its memory is vague since 128 raw activations are compressed into one, yet it can\neasily process super long context (e.g. 100K). Therefore, its performance may degrade on tasks that\nrequire accurate memorization of super long context. However, we can largely mitigate this problem\nwith retrieval.\nSpecifically, we perform two types of condensation for each interval, one with a high condensing\nratio and one with a low condensing ratio, and save the condensed activations from both of them. The\nmodel always conditions on the aggressively condensed activations, thus it can process super long\ncontext. Besides, we can obtain both types of condensed activations in one forward pass by adjusting\nthe attention mask, which is efficient. When a query about the fine-grained memory is issued, we\ncan leverage retrieval to locate K relevant intervals. The information within these relevant intervals\nshould be accurate to successfully answer the query, while other information can be vague. Therefore,\nfor these intervals, we replace their aggressively condensed activations with those accurate ones\nresulted from the low condensing ratio, then proceed to generate the answer. Thanks to the step-wise\nrandomized condensing ratio in training, Activation Beacon can robustly utilize the activations with\ndifferent condensing ratios, thereby effectively utilizing the retrieved information. The retrieval can\nbe implemented in different ways. For example, the BM25 based on lexical matching, the dense\nretrieval based on semantic matching, and the hidden state retrieval. We adopt the simple BM25\nretrieval and set K = 2, which already yields satisfactory performance.\nWe use the Passkey Retrieval task [35; 6; 32] to evaluate our approach. It constructs a 5-digit integer,\nnamely passkey, and buries it at a random position in a synthetic document. The models are asked\nto exactly reproduce the passkey after reading the document. We repeat the experiment 5 times at\neach context length and report the average performance in Figure 6. The following observations are\nderived.\nFirstly, Activation Beacon cannot accurately remember the passkey hidden in the long context.\nThough Activation Beacon is accurate on the relatively short context (8K), it becomes incapable\ngiven 16K context and beyond. This is as expected since longer context requires larger condensing\nratio, which impairs the memory accuracy. However, this doen\u2019t mean the aggressively condensed\nactivations are meaningless. In Figure 6 (B), we evaluate the fuzzy score between the model\u2019s\nprediction and the ground-truth passkey, which measures how many digits are overlapped between\nthe prediction and the groud-truth. It can be noticed that Activation Beacon always yields positive\nfuzzy score. This indicates that Activation Beacon can restore several digits in the passkey, while fail\nto precisely remember it due to the vague memory.\nSecondly, Activation Beacon combined with BM25 significantly improves the memory accuracy,\nachieving 100% accuracy at all context lengths. The reasons behind such a superior performance is\ntwo fold. On the one hand, the BM25 retriever successfully locates the interval where passkey resides.\nOn the other hand, the model can effectively utilize the accurate memory of the retrieved intervals.\nIn conclusion, we demonstrate that Activation Beacon\u2019s memory accuracy can significantly benefit\nfrom even the simplest form of retrieval. This motivates us to investigate more retrieval implementa-\ntions and explore the potential of Activation Beacon on more real-world long-context tasks.\nE\nImpact of Different Condensing Ratios\nThe condensing ratio of Activation Beacon can be flexibly configured at inference time. One\ninteresting question is how different condensing ratios affect the performance of Activation Beacon.\nWe investigate this question on long-context generation and report the results in Figure 7.\n16\n4K\n8K\n16K\n24K\n32K\n40K\n48K\n56K\n64K\nContext Length\n7.5\n8.0\n8.5\n9.0\n9.5\n10.0\nPerplexity\nCondensing Ratio\nx8 (26K)\nx16 (50K)\nx32 (99K)\nx64 (198K)\nFigure 7: The perplexity of Activation Beacon given different condensing ratios. The theoretical\nmaximum context length is denoted in the brackets (\u03b1 \u00d7 3072 + 1024 where \u03b1 is the condensing\nratio).\nWe can observe that lower condensing ratio leads to lower perplexity, which translates to higher\ngeneration long-context generation quality. However, since the window size of Activation Beacon is\nlimited by that of the LLM, lower condensing ratio results in more activations to save, and hence\nshorter context it can process.\nF\nLimitations\nCurrently, the sliding window of Activation Beacon strides one interval at a time and only preserves\nthe condensed activations in the previous interval. This means the tokens in the beginning of the\nnext interval do not have any raw context: it can only attend to the condensed activations in previous\nintervals. This lack of raw context may cause the degradation of the generation performance especially\nwhen answering user instructions. In the future, we may adjust the stride of the sliding window to\nmake it shorter than the interval length. Therefore, tokens in the beginning of any interval always\nattend to some raw activations as local context.\n17\n"
  },
  {
    "title": "GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation",
    "link": "https://arxiv.org/pdf/2401.04092.pdf",
    "upvote": "18",
    "text": "GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation\nTong Wu1,5*\nGuandao Yang2*\nZhibing Li1,5*\nKai Zhang 3\nZiwei Liu 4\nLeonidas Guibas 2\nDahua Lin 1,5\nGordon Wetzstein 2\n1 The Chinese University of Hong Kong\n2 Stanford University\n3 Adobe Research\n4 S-Lab, Nanyang Technological University\n5 Shanghai Artificial Intelligence Laboratory\nv.s.\nv.s.\nv.s.\n\u201cA guitar resting against an old oak tree.\u201d\n\u201cA teddy bear with a red bow.\u201d\nCLIP: Left                   : Right      Human: Right\nPickScore: Left              : Right    Human: Right\nAesthetic: Left              : Right    Human: Right\nText-Asset Alignment\nTexture Details\nGeometry Details\nTexture-Geometry \nCoherency\n3D Plausibility\n\u201cA hollow and asymmetrical teapot, crafted to look like a \nslumbering dragon, with a scaly and rough texture.\u201d\n\u00b7\u00b7\u00b7\nPROMPTS\nText-Asset Alignment\nTexture Details\nGeometry Details\nTexture-Geometry Coherency\n3D Plausibility\nFigure 1. We present a versatile and human-aligned evaluation metric for text-to-3D generative methods. To this end, we design a prompt\ngenerator that can produce a set of input prompts targeting an evaluator\u2019s demands. Moreover, we leverage GPT-4V to compare two 3D\nshapes according to different evaluation criteria. Our method provides a scalable and holistic way to evaluate text-to-3D models.\nAbstract\nDespite recent advances in text-to-3D generative meth-\nods, there is a notable absence of reliable evaluation met-\nrics. Existing metrics usually focus on a single criterion\neach, such as how well the asset aligned with the input\ntext. These metrics lack the flexibility to generalize to dif-\nferent evaluation criteria and might not align well with\nhuman preferences.\nConducting user preference studies\nis an alternative that offers both adaptability and human-\naligned results.\nUser studies, however, can be very ex-\npensive to scale. This paper presents an automatic, ver-\nsatile, and human-aligned evaluation metric for text-to-\n3D generative models.\nTo this end, we first develop a\nprompt generator using GPT-4V to generate evaluating\nprompts, which serve as input to compare text-to-3D mod-\nels.\nWe further design a method instructing GPT-4V to\ncompare two 3D assets according to user-defined crite-\nria.\nFinally, we use these pairwise comparison results\n\u2217 Equal contribution.\nto assign these models Elo ratings. Experimental results\nsuggest our metric strongly aligns with human preference\nacross different evaluation criteria. Our code is available\nat https://github.com/3DTopia/GPTEval3D.\n1. Introduction\nThe field of text-to-3D generative methods has seen re-\nmarkable progress over the past year, driven by a series\nof breakthroughs. These include advancements in neural\n3D representations [42, 47], the development of extensive\ndatasets [10, 14, 15], the emergence of scalable genera-\ntive models [24, 56, 62], and the innovative application of\ntext\u2013image foundational models for 3D generation [48, 51].\nGiven this momentum, it\u2019s reasonable to anticipate rapidly\nincreasing research efforts and advancements within the\nrealm of text-to-3D generative models.\nDespite recent advances, the development of adequate\nevaluation metrics for text-to-3D generative models has not\n1\narXiv:2401.04092v2  [cs.CV]  9 Jan 2024\nkept pace. This deficiency can hinder progress in further\nimproving these generative models. Existing metrics often\nfocus on a single criterion, lacking the versatility for diverse\n3D evaluation requirements. For instance, CLIP-based met-\nrics [29, 51] are designed to measure how well a 3D asset\naligns with its input text, but they may not be able to ad-\nequately assess geometric and texture detail. This lack of\nflexibility leads to misalignment with human judgment in\nevaluation criteria the metric is not designed for. Conse-\nquently, many researchers rely on user studies for accurate\nand comprehensive assessment. Although user studies are\nadaptable and can accurately mirror human judgment, they\ncan be costly, difficult to scale, and time-consuming. As a\nresult, most user studies have been conducted on a very lim-\nited set of text-prompt inputs. This leads to a question: Can\nwe create automatic metrics that are versatile for various\nevaluation criteria and align closely with human judgment?\nDesigning metrics that meet these criteria involves three\nessential capabilities: generating input text prompts, under-\nstanding human intention, and reasoning about the three-\ndimensional physical world. Fortunately, Large Multimodal\nModels (LMMs), particularly GPT-4Vision (GPT-4V) [46],\nhave demonstrated considerable promise in fulfilling these\nrequirements [71]. Drawing inspiration from humans\u2019 abil-\nity to perform 3D reasoning tasks using 2D visual infor-\nmation under language guidance, we posit that GPT-4V is\ncapable of conducting similar 3D model evaluation tasks.\nIn this paper, we present a proof-of-concept demonstrat-\ning the use of GPT-4V to develop a customizable, scalable,\nand human-aligned evaluation metric for text-to-3D gen-\nerative tasks. Building such an evaluation metric is sim-\nilar to creating an examination, which requires two steps:\nformulating the questions and evaluating the answers. To\neffectively evaluate text-to-3D models, it is crucial to ob-\ntain a set of input prompts that accurately reflect the eval-\nuators\u2019 needs. Relying on a static, heuristically generated\nset of prompts is insufficient due to the diverse and evolv-\ning nature of evaluator demands. Instead, we developed a\n\u201cmeta-prompt\u201d system, where GPT-4V generates a tailored\nset of input prompts based on evaluation focus. Following\nthe generation of these input text prompts, our approach in-\nvolves comparing 3D shapes against user-defined criteria,\nakin to grading in an exam. We accomplish this through de-\nsigning an instruction template, which can guide GPT-4V\nto compare two 3D shapes per user-defined criterion. With\nthese components, our system can automatically rank a set\nof text-to-3D models by assigning each of these models an\nElo rating. Finally, we provide preliminary empirical ev-\nidence showing that our proposed framework can surpass\nexisting metrics in achieving better alignment with human\njudgment in a diverse set of evaluation criteria. Results sug-\ngest that our metric can efficiently provide an efficient and\nholistic evaluation of text-to-3D generative models.\n2. Related Work\nText-to-3D generation.\nText-to-image generation models\nhave become increasingly powerful with text-to-3D exten-\nsions being the next frontier (see [48] for a recent survey).\nHowever, due to limited amounts of 3D data, text-to-3D\nhas mainly been driven by methods based on optimizing a\nNeRF representation [42]. For example, Dreamfusion [51]\noptimizes a NeRF using score-distillation-sampling-based\n(SDS) loss. The quality of such optimization-based meth-\nods [11, 37, 41, 51, 60, 63, 66, 68], however, is far behind\nthat of text-to-image models [50, 54\u201356]. Compared with\ntheir 2D counterparts, they are generally lacking diversity,\ntexture fidelity, shape plausibility, robustness, speed, and\ncomprehension of complex prompts. On the other hand,\nPoint-E [44] and Shap-E [30] train feed-forward 3D gener-\native models on massive undisclosed 3D data. Though they\nshow promising results with fast text-to-3D inference, their\ngenerated 3D assets look cartoonish without geometric and\ntexture details. Recently, we notice a rapid change in the\nlandscape of text-to-3D methods [38, 39] mainly due to the\npublic release of the large-scale Objaverse datasets [16, 17].\nFeed-forward methods trained on these datasets, e.g., In-\nstant3D [36], have managed to make a big jump in text-to-\n3D quality, narrowing the performance gap between 3D and\n2D generation. As we expect to see continuing progress\nin this area, it is critical to have robust evaluation metrics\nclosely aligning with human judgment to measure different\naspects of 3D generative models, including shape plausibil-\nity and texture sharpness. Such an evaluation metric can\nprovide meaningful guidance for model design choices and\nsupport fair comparisons among the research community.\n3D Evaluation Metrics.\nEvaluating 3D generative mod-\nels is inherently challenging, requiring an understanding of\nboth physical 3D worlds and user intentions. Traditional\nmethods for evaluating unconditional or class-conditioned\n3D models typically measure the distance between distribu-\ntions of generated and reference shapes [1, 5, 9, 21, 40, 70].\nHowever, these metrics are not readily applicable to text-\nconditioned generative tasks due to the difficulty in obtain-\ning a comprehensive reference set, given the vastness of\nnatural language inputs [6]. To alleviate this issue, prior\nwork tried to curate a set of text prompts to evaluate key\naspects of text-conditioned generative tasks [22, 51]. Our\nwork complements this effort by creating a text-prompt gen-\nerator using language instruction. Additionally, prior stud-\nies utilized multimodal embeddings, such as CLIP [52] and\nBLIP [34, 35], to aid the evaluation. For instance, the CLIP\nSimilarity metric [29, 51] employs CLIP embeddings to as-\nsess text-to-3D alignment. However, these metrics are of-\nten tailored to measure specific criteria, lacking the flexibil-\nity to adapt to different requirements of text-to-3D evalua-\ntion. User preference studies are considered the gold stan-\n2\ndard for evaluating text-to-3D models, as adopted by many\npapers [5, 26, 37, 53, 58, 63]. While user studies offer ver-\nsatility and accuracy, they are costly, time-consuming, and\ndifficult to scale. Our automatic metrics can serve as an al-\nternative to user preference studies, aligning well with hu-\nman preferences while offering high customizability.\nLarge multimodality models.\nFollowing the success of\nlarge language models (LLMs) [3, 8, 12, 25, 46, 64], the\nfocus has shifted to large multimodal models (LMMs) as\nthe next frontier in artificial intelligence. Initial efforts of\nLMM involve combining computer vision with LLMs by\nfine-tuning visual encoders to align with language embed-\ndings [2, 4, 18, 28, 34, 35, 65] or converting visual infor-\nmation to text [27, 59, 67, 72]. Most of these models are\nusually limited in scale. Recently, GPT-4V [45] has risen as\nthe leading LMMs, benefiting from training on an unprece-\ndented scale of data and computational resources. These\nLMMs have demonstrated a range of emerging properties\n[71], including their capability as evaluators for language\nand/or vision tasks [23, 74, 75]. In our work, we explore\nthe use of GPT-4V in evaluating 3D generative models, a\nrelatively under-explored application because GPT-4V can-\nnot directly consume 3D information.\n3. Method Overview\nThe goal of our evaluation metric is to rank a set of text-\nto-3D models based on user-defined criteria. Our method\nconsists of two primary components. First, we need to de-\ncide which text prompt to use as input for the evaluation\ntask. Toward this goal, we develop an automatic prompt\ngenerator capable of producing text prompts with customiz-\nable levels of complexity and creativity (Sec. 4). The sec-\nond component is a versatile 3D assets comparator (Sec. 5).\nIt compares a pair of 3D shapes generated from a given text\nprompt according to the input evaluation criteria. Together,\nthese components allow us to use the Elo rating system to\nassign each of the models a score for ranking (Sec. 5.3).\n4. Prompt Generation\nCreating evaluation metrics for text-to-3D generative\nmodels requires deciding which set of input text prompts\nwe should use as input to these models. Ideally, we would\nlike to use all possible user input prompts, but this is compu-\ntationally infeasible. Alternatively, we would like to build\na generator capable of outputting prompts that can mimic\nthe actual distribution of user inputs. To achieve this, we\nfirst outline the important components of an input prompt\nfor text-to-3D models (Sec 4.1). Building on these com-\nponents, we design a \u201cmeta-prompt\u201d to instruct GPT-4V\nhow to leverage these components to generate an input text\nprompt for text-to-3D models (Sec 4.2).\nA dancing elephant.\nA sleeping cat.\nA large, multi-layered wedding cake, with smooth \nfondant, delicate piping, and lifelike sugar \nflowers in full bloom, displayed on a silver stand.\nOrange monarch butterfly resting \non a dandelion.\nFrog with a translucent skin displaying \na mechanical heart beating.\nHigher Complexity\nHigher Creativity\nA solid, symmetrical, smooth stone fountain, \nwith water cascading over its edges into a clear, \ncircular pond surrounded by blooming lilies, in \nthe center of a sunlit courtyard.\nFigure 2. Controllable prompt generator. More complexity or\nmore creative prompts often lead to a more challenging evaluation\nsetting. Our prompt generator can produce prompts with various\nlevels of creativity and complexity. This allows us to examine text-\nto-3D models\u2019 performance in different cases more efficiently.\n4.1. Prompt components\nA typical input text prompt for text-to-3D models con-\ntains three components: subjects, properties, and composi-\ntions. Subjects usually involve nouns referring to objects\nor concepts the user would like to instantiate in 3D. \u201cCats\u201d,\n\u201cfire\u201d, and \u201cuniverse\u201d are all examples of subjects. Proper-\nties include adjectives a user can use to describe the subjects\nor their interactions, such as \u201cmysterious\u201d and \u201cweathered\u201d.\nFinally, users will compose these concepts and properties\ntogether into a sentence or clause. The composition varies\nfrom as simple as joining different subjects and/or proper-\nties together with commas or as thoughtful as writing it as\na grammatically correct and fluent sentence. In this work,\nwe prompt GPT-4V to create a comprehensive list of words\nfor subjects and properties. This list of subjects and proper-\nties will be used as building blocks to construct the \u201cmeta-\nprompt\u201d, which is an instruction for GPT-4V to generate in-\nput text-prompts by composing these building blocks. Sec-\ntion B.1 contains more implementation details.\n4.2. Meta-prompt\nProvided with ingredients to create input prompts, we\nnow need to automatically compose these ingredients to-\ngether according to the evaluator-specified requirements.\nThis requires the prompt generator to understand and follow\nthe evaluator\u2019s instruction. In this paper, we use GPT-4V\u2019s\nability to generate prompts following instructions. Specifi-\ncally, we would like to build a text instruction asking GPT-\n4V to create a list of prompts that can be used as input for\n3\nOur task here is the compare two 3D\nobjects, both generated from the\nsame text prompt {Data Prompt}.\nData \nPrompt:\n1. Text-Asset Alignment: \u2026\n2. 3D Plausibility: \u2026\n3. Texture-Geometry Coherency: \u2026 \n4. Low-Level Texture: \u2026\n5. Low-Level Geometry: \u2026\nInput \nFormat:\nI will provide you with some multi-\nview {normal / RGB / RGB & normal\nrenderings} from the two objects,\nwhere the left part is 3D object 1,\nand the right part is 3D object 2.\nCriteria:\nAnswer:\nYou have four options:  \n1. Left is better; \n2. Right is better; \n3. Cannot decide.\nFinal Answer: <option 1>, <option 2>,\n\u2026 , <option 5>\n\u2026 text prompt a thorn rose, \u2026 multi-view RGB\n& normal renderings , \u2026\n[criteria subset]\n1. Text prompt & 3D Alignment: [explanations]\n4. Low-Level Texture: [explanations]\n[in context example]\nInstruction Template\nFinal Answer: 3, 3, 1, 1, 1\nOutput Ensemble\nAnalysis:\n1. \u2026\n4. \u2026\nAnswer: \u2026\nAnalysis:\n1. \u2026\n2. \u2026\n3. \u2026\n4. \u2026\n5. \u2026\nAnswer: \u2026\nTuning the instruction details\nInput Layout\nData Augmentation\nPure RGB\nPure Normal\nLayout\nWatermark\nAnalysis:\n2. \u2026\n5. \u2026\nAnswer: \u2026\nFlip\nOrder Exchange\nFigure 3. Illustration of how our method compares two 3D assets. We create a customizable instruction template that contains necessary\ninformation for GPT-4V to conduct comparison tasks for two 3D assets (Sec. 5.1). We complete this template with different evaluation\ncriteria, input 3D images, and random seeds to create the final 3D-aware prompts for GPT-4V. GPT-4V will then consume these inputs to\noutput its assessments. Finally, we assemble GPT-4V\u2019s answers to create a robust final estimate of the task (Sec. 5.2)\ntext-to-3D models. We coin this instruction \u201cmeta-prompt\u201d.\nIn order for GPT-4V to output prompts for text-to-3D\nmodels, we first provide GPT-4V with the necessary ingre-\ndients, i.e. a list of subjects and properties from the previous\nsection. In addition to these, the meta-prompt needs to in-\nclude a description of how the evaluator wants the output\nprompt set to be. For example, the evaluator might want to\nfocus on complex prompts containing multiple subject in-\nteractions and properties, testing a text-to-3D models\u2019 abil-\nity to generate complex objects. One might also be curi-\nous about these models\u2019 performance in creative prompts\ninvolving subjects and descriptions that are not commonly\nseen in the real world. How complex and creative the input\nprompt can influence how challenging the evaluation task\nis. These two axes, complexity and creativity, are examples\nof evaluator\u2019s criteria. Such criteria should be specified as\nlanguage instructions attached to the \u201cmeta-prompt\u201d along\nwith all the ingredients. With both the prompt ingredient\nand the evaluator\u2019s criteria properly included in the meta-\nprompt, our GPT-4V-based prompt generator can now com-\npose sentences that adhere to the evaluator\u2019s requirement.\nThe appendix contains more details about our meta-prompt\nand prompt generation pipeline (Sec B.1).\nFigure 2 shows prompts outputted from our generator\nwith instruction asking for different complexity and creativ-\nity. We can see that high complexity introduces a larger\nnumber of objects, multifaceted descriptions, and occasion-\nally, a completely broken scene. Similarly, more creative\nprompts combine subjects, verbs, or adjectives in unconven-\ntional ways. Text-to-3D models also tend to struggle with\nthese more creative prompts, failing to follow the descrip-\ntion of these input prompts exactly. This suggests that input\nprompts distribution can greatly affect how challenging the\nevaluation task is. Being able to control the distributions of\nthe input prompt allows us to examine the performance of\nthese text-to-3D models through a more focused lens.\n5. 3D Assets Evaluator\nNow we can sample a set of text prompts, T = {ti}i,\nusing our generator. In this section, we will evaluate the\nperformance of a set of text-to-3D generative models us-\ning T as input prompts.\nGiven a set of these models,\nM = {Mj}j, we use each model to generate one or more\n3D shapes for each prompt. This results in a set of tuples:\n{(Mk, tk, Mj(tk, zk))|Mk \u2208 M, tk \u2208 T }k, where zk rep-\nresents the random noise influencing the shape generation.\nOur objective is to rank the text-to-3D models in M based\non a user-defined criterion. To accomplish this, we first\nprompt GPT-4V to compare two 3D assets generated from\nthe same input text prompt (Sec 5.1 and Sec 5.2). We then\nuse these pairwise comparison results to assign each of the\nmodels an Elo rating reflecting its performance (Sec 5.3).\n4\n5.1. Pairwise Comparison\nAt the core of our evaluation metric is the ability to an-\nswer the following question: given a text prompt t, and two\n3D shapes generated from two different models, say Mi and\nMj, which 3D shape is better according to the evaluation\ncriteria? As discussed in previous sections, we hypothesize\nthat one can leverage GPT-4V to achieve this task. How-\never, since GPT-4V is trained on language and visual data,\nit lacks the ability to analyze 3D shapes directly. Therefore,\nour input to GPT-4V should include both text instructions\nand 2D visual renderings that can capture 3D information.\nSpecifically, for each of the two 3D assets, we will cre-\nate a large image containing renderings of the 3D asset from\nfour or nine viewpoints. These two images will be concate-\nnated together before passing into GPT-4V along with the\ntext instructions. GPT-4V will return a decision of which of\nthe two 3D assets is better according to the instruction.\nText instruction.\nWe need to communicate three pieces\nof information for GPT-4V to compare two 3D assets: in-\nstructions to complete a 3D comparison task, the evaluation\ncriteria, and descriptions of the output format. We found\nit important to emphasize that the provided images are ren-\nders from different viewpoints of a 3D object. In addition\nto a plain description of the user-defined evaluation crite-\nria, providing instruction about what kind of image features\none should use when analyzing for a particular criteria is\nalso useful. Finally, instead of requesting only the answer\nof which shape is better directly, we also prompt GPT-4V\nto explain how it arrives at its conclusion [7, 69].\nVisual features of 3D shapes.\nOnce GPT-4V has been\nprompted to understand the evaluation criteria and task of\ninterest, we now need to feed the 3D shape into the GPT-\n4V model. Specifically, we need to create images that can\nconvey the appearance and the geometry features of the 3D\nshapes. To achieve that, for each 3D object, we create im-\nage renders of the object from various viewpoints. For each\nof these viewpoints, we also render a surface normal image.\nThese normal surface renders will be arranged in the same\nlayout as the RGB render before being fed into GPT-4V.\nUsing world-space surface normal renders leads to better re-\nsults because they provide geometric information about the\nsurface and allow reasoning for correspondence between\nviews. Appendix B.2 has more implementation details.\n5.2. Robust Ensemble\nEven though GPT-4V is able to provide an answer to\nthe pairwise shape comparison problem, its response to the\nsame input can vary from time to time due to the probabilis-\ntic nature of its inference algorithm. In other words, we can\nview our GPT-4V 3D shape comparator\u2019s outputs as a cate-\ngorical distribution, and each response is a sample from the\ndistribution. As a result, a single response from GPT-4V\nmight not capture its true prior knowledge since it can be\naffected by the variance during sampling. This is particu-\nlarly the case when the variance of the output distribution is\nhigh (e.g., when both choices are equally likely). Note that\nthis is not a weakness of GPT-4V as similar situations can\nhappen to human annotators when two objects are equally\ngood according to a criterion. In other words, we are not\ninterested in sampling one instance of how GPT-4V would\nmake a decision. Instead, estimating with what probability\nGPT-4V will choose this answer is more useful.\nOne way to estimate such probability robustly from sam-\nples with variance is through ensembling, a technique that\nhas also been explored in other tasks [71].\nSpecifically,\nwe propose to ensemble outputs from multiple slightly per-\nturbed inputs. The key is to perturb input prompts to GPT-\n4V without changing the task or evaluation criteria. The\ninput includes the text instruction, visual images, as well as\nthe random seed. Our methods deploy different perturba-\ntions, including changing random seeds, the layout of ren-\nders, the number of rendered views, and the number of eval-\nuation criteria. Figure 3 illustrates how we perturb the in-\nput and ensemble the results from these perturbed inputs\ntogether. Appendix D includes more details.\n5.3. Quantifying Performance\nWe have now obtained a list of comparisons among a set\nof models M. The comparisons are over a variety of sam-\npled prompts denoted as T according to the user-defined\ncriteria. Our goal is now to use this information to assign a\nnumber for each model in M such that it best explains the\nobserved result. Our quantification method should consider\nthe fact that the comparison results are samples from a prob-\nability distribution, as discussed in the previous subsection.\nThis problem is commonly studied in rating chess play-\ners, where a game between two players can have different\noutcomes even if one player is better than the other. In chess\nand many other competitions, the Elo score [19] is perhaps\nthe most widely adapted method to produce a numerical es-\ntimation that reflects players\u2019 performance. The Elo rat-\ning system has also been adapted in prior works to evaluate\nimage generative models [43, 61]. In this paper, we adapt\nthe version proposed by Nichol et al. [43]. Specifically, let\n\u03c3i \u2208 R denote the Elo score of the ith model in M. A higher\nscore \u03c3i indicates better performance. We assume that the\nprobability of model i beats model j is:\nPr(\u201ci beats j\u201d) =\n\u0010\n1 + 10(\u03c3j\u2212\u03c3i)/400\u0011\u22121\n.\n(1)\nOur goal is to find score \u03c3i that can best explain the ob-\nserved comparison results given the abovementioned as-\nsumption. This can be achieved via maximum likelihood\nestimation. Specifically, let A be a matrix where Aij de-\nnotes the number of times model i beats model j in the list\n5\nTable 1. Alignment with human judgment (higher is better). Here we present Kendall\u2019s tau ranking correlation [31] between rankings\nprovided by a metrics and those provided by human experts. Higher correlation indicates better alignment with human judgment. We\nbold-face the most aligned method and underline the second place for each criterion. Our method achieves top-two performances for all\nevaluation criteria, while prior metrics usually only do well for at most two criteria.\nMethods\nAlignment\nPlausibility\nT-G Coherency\nTex Details\nGeo Details\nAverage\nPickScore [33]\n0.667\n0.484\n0.458\n0.510\n0.588\n0.562\nCLIP-S [23]\n0.718\n0.282\n0.487\n0.641\n0.667\n0.568\nCLIP-E [23]\n0.813\n0.426\n0.581\n0.529\n0.658\n0.628\nAesthetic-S [57]\n0.795\n0.410\n0.564\n0.769\n0.744\n0.671\nAesthetic-E [57]\n0.684\n0.297\n0.555\n0.813\n0.684\n0.611\nOurs\n0.821\n0.641\n0.564\n0.821\n0.795\n0.710\nTable 2. Pairwise rating agreements (higher is better). We mea-\nsure the average probability that the decision of the metric matches\nthat of human\u2019s for each comparison. Our method achieves strong\nalignment across most criteria.\nMetrics\nAlign.\nPlaus.\nT-G.\nText.\nGeo.\nAvg.\nPickS.\n0.735\n0.721\n0.713\n0.690\n0.740\n0.720\nCLIP\n0.726\n0.644\n0.678\n0.703\n0.715\n0.693\nAest.\n0.798\n0.698\n0.753\n0.817\n0.780\n0.769\nOurs\n0.810\n0.826\n0.729\n0.843\n0.735\n0.789\nof comparisons. The final Elo score for this set of models\ncan be obtained by optimizing the following objective:\n\u03c3 = arg min\n\u03c3\nX\ni\u0338=j\nAij log\n\u0010\n1 + 10(\u03c3j\u2212\u03c3i)/400\u0011\n.\n(2)\nIn this paper, we initialize \u03c3i = 1000 and then use the Adam\noptimizer [32] to minimize the loss to obtain the final Elo\nscore. Please refer to Sec B.3 for more mathematical intu-\nition about the formulation of the Elo score.\n6. Results\nIn this section, we provide a preliminary evaluation of\nour metric\u2019s alignment with human judgment across differ-\nent criteria. We first introduce the experiment setup. We\nwill discuss the main alignment results in Sec. 6.1. We then\nexplore how to use our metric to evaluate different models\nholistically in Section 6.2. Finally, we briefly showcase how\nto extend our models to different criteria in Section 6.3.\nText-to-3D generative models to benchmark.\nWe in-\nvolve 13 generative models in the benchmark, including ten\noptimization-based methods and three recently proposed\nfeed-forward methods. Please refer to Sec C for the com-\nplete list. We leverage each method\u2019s official implementa-\ntions when available. Alternatively, we turn to Threestu-\ndio\u2019s implementation [20]. For methods designed mainly\nfor image-to-3D, we utilize Stable Diffusion XL [49] to\ngenerate images conditioned on text as input to these mod-\nels.\nAll experiments are conducted with default hyper-\nparameters provided by the code.\nBaselines metrics.\nWe select three evaluation metrics\nwith various considerations. 1) CLIP similarity measures\nthe cosine distance between the CLIP features [52] of the\nmulti-view renderings and the text prompt. This metric is\nchosen because it is widely used in previous works as the\nmetric for text\u2013asset alignment [26, 29, 51]. 2) Aesthetic\nscore [57] is a linear estimator on top of CLIP that predicts\nthe aesthetic quality of pictures. We choose this because\nit is trained on a large-scale dataset.\n3) PickScore [33]\nis a CLIP-based scoring function trained on the Pick-a-Pic\ndataset to predict human preferences over generated images.\nTo compute the metrics above, we uniformly sample 30\nRGB renderings for each of the generated assets. The CLIP\nsimilarity and aesthetic score can be directly computed from\nthe multi-view renderings and averaged for each prompt.\nSince PickScore takes paired data as input for comparison,\nwe assign 30 paired renderings for each pair of objects be-\nfore averaging the PickScore results.\nEvaluation criteria.\nWhile our method can potentially be\napplied to all user-defined criteria, in this work we focus\non the following five criteria, which we believe are impor-\ntant for current text-to-3D evaluation tasks. 1) Text\u2013asset\nalignment: how well a 3D asset mirrors the input text de-\nscription. 2) 3D plausibility: whether the 3D asset is plau-\nsible in a real or virtual environment. A plausible 3D asset\nshould not contain improbable parts such as multiple dis-\ntorted faces (Janus problem) or noisy geometry floaters. 3)\nTexture details: whether the textures and appearance of\nthe shape are realistic, high resolution, and have appropri-\nate saturation levels. 4) Geometry details: whether the ge-\nometry makes sense and contains appropriate details. 5)\nTexture\u2013geometry coherency: whether geometry and tex-\ntures agree with each other. For example, eyes of a character\nshould be on reasonable parts of the face geometry.\nExpert annotation.\nTo evaluate the performance of our\nmethod, we need to conduct user preference studies to ob-\n6\nLeft\nRight\nLeft\nRight\nLeft\nRight\nText-Asset Alignment: The right model shows \u2026 without any tendrils obstructing \nit. The left one \u2026 and seems more consistent with the \"peeking out\" aspect.\n3D Plausibility: The left model\u2018s fish appears distorted and blended with the \nanemone tendrils, while the right model depicts both the fish and anemone as \ndistinct and solid entities, being more plausible in the real world. \nTexture-Geometry Coherency: The left object is less compelling due to the less \nintegrated positioning of the fish. The right object shows a strong correspondence \nbetween the geometry and the texture; the anemone tendrils and clownfish \nstripes align well across both the RGB and normal maps.\nTexture Details: The texture on the left has more blur and less sharpness. The \nright clownfish presents with sharper, clearer textures and distinct patterns.\nGeometry Details: When observing the local geometry of the normal maps, the \nright object exhibits sharper details and more defined structures. It is particularly \nevident within the tendrils of the anemone and the body of the clownfish, where \nindividual scales and tentacle textures appear more pronounced.\nFinal answer: left right right right right\n\u201d A twisted tower.\u201d\n\u201dA rough rock.\u201d\n\u201dClownfish peeking out from sea anemone tendrils.\u201d\n3D Plausibility: The left model has disjointed parts and lacks solidity, while \nthe right model is more coherent and structurally sound.\nFinal answer: right\nGeometry Details: Looking at the normal maps, the left model's geometry is \nfairly basic and smooth. The right model's normal map reveals greater sharpness, \nand variation in the surface, suggesting a more detailed geometry. \nFinal answer: right\nLeft\nRight\n\u201d An old-fashioned rotary phone with a tangled cord.\u201d\n3D Plausibility: The left model is abstract with a highly distorted structure, \nshowing excessive twists and deformations. The right model shows a clear and \nrecognizable form of a rotary phone with consistent solidity.\nTexture Details: The left object features a somewhat vivid but highly distorted \ntexture that doesn\u2018t contribute to the realism or clarity of the object as a phone. \nRight Object presents a texture that is more coherent and detailed, matching \nspecific parts of the phone , being more realistic and sharp. \nFinal answer: right right\nFigure 4. Examples of the analysis by GPT-4V. Given two 3D assets, we ask GPT-4V to compare them on various aspects and provide\nan explanation. We find that GPT-4V\u2019s preference closely aligns with that of humans.\ntain ground truth preference data.\nOur user studies will\npresent the input text prompt alongside a pair of 3D assets\ngenerated by different methods for the same input. The user\nwill be asked to identify which 3D asset satisfies the crite-\nria of interest better. We recruited 20 human experts who\nare graduate students experienced in computer vision and\ngraphics research to annotate the data. We assigned 3 anno-\ntators per comparison question per evaluation criteria. We\ncompute a reference Elo rating using the formula in Sec. 5.3\nusing all expert annotations.\n6.1. Alignment with Human Annotators.\nIn this section, we evaluate how well our proposed met-\nric aligns with human preference. To achieve that, we use\neach metric to assign a score for each text-to-3D model for\neach evaluation criteria. Then, we compute Kendell\u2019s tau\ncorrelation [31] between the scores computed by the met-\nrics and the reference scores. Table 1 shows the ranking\ncorrelations between scores predicted by different evalua-\ntion metrics and the reference Elo scores computed from\nexpert annotators. We can see that our metrics achieve the\nbest correlation in 4 out of 5 criteria, as well as the best\naverage correlation. Note that our method achieves consis-\ntent performance across different criteria, while prior met-\nrics usually perform well in only one or two. This highlights\nthat our method is versatile in different evaluation criteria.\nOur metric also shows strong human correlation for each\n3D asset comparison question, which is a harder task. To\nmeasure that, we assume the response to each comparison\nquestion follows a Bernoulli distribution with probability\np to select the first shape. Let pi be the probability that\nthe evaluation metric will select the first shape at question\n7\n(a)\n(b)\nMVDream (Avg Elo: 1362)\nLatent NeRF* (Avg Elo: 1238)\nProlificDreamer* (Avg Elo: 1236)\nInstant3D (Avg Elo: 1208)\nFigure 5. Holistic evaluation. Since our evaluation metric is human-aligned in multiple criteria, we can evaluate text-to-3D models more\nholistically. In this figure, we listed the Radar charts of the top four text-to-3D models according to their averaged Elo scores across all five\ncriteria we evaluated. The Radar charts report the Elo rating for each of the five criteria. These radar charts can provide relative strengths\nand weaknesses among these models, providing guidance to improve these models. * indicates results from Threestudio implementation.\n\u201dA shouting leaf.\u201d\nDiversity: The left model shows various interpretations of a leaf with a mouth, \nas if it is shouting. The right is less variety in form and pose, some images are\nabstract, and the concept of a \"shouting leaf\" is not immediately clear.\nLeft\nRight\nFigure 6. Diversity evaluation. Our method can be extended to\nevaluate which text-to-3D models output more diverse 3D assets.\ni and qi be that of a human annotation. We measure the\npairwise rating agreement using the probability of a ran-\ndom sample from the metric agreeing with that from a hu-\nman:\n1\nN\nPN\ni=1 piqi + (1 \u2212 pi)(1 \u2212 qi). Table 2 shows that\nour method achieves top-two agreement across all but one\ncriteria. Figure 4 shows some exemplary outputs from our\nmethod. We can see that GPT-4V is also able to provide\nsome analysis justifying its final choice.\n6.2. Holistic Evaluation\nThe versatility of our method lands the provision to paint\na holistic picture of each text-to-3D model\u2019s performance.\nSpecifically, we compute each model\u2019s average Elo scores\nacross each criterion and present the Radar charts of the\nmodels achieving the top averaged Elo scores in Figure 5.\nAccording to our metric, MVDream [60] won first place\non all five criteria. MVDream achieves about 100 more\nELO scores than its runner-ups. The second, the third, and\nthe fourth places are taken by Latent-NeRF [41], Prolific-\nDreamer [68], and Instant3D [36]. These models achieve\nsimilar averaged Elo scores, with differences of less than\n30 Elos. These three models achieve about 100 Elos more\nthan the next tiers of models, which score about 1100 Elos.\nWhile Latent-NeRF, ProlificDreamer, and Instant3D\nachieve similar overall scores, our metrics allow further\nanalysis into the relative strengths and weakness of each\nmodels. For example, ProlificDreamers show strong per-\nformance in three criteria: alignment, geometry details,\nand texture details. However, its performance in 3D Plau-\nsibility is lagging behind when comparing with the other\ntop-performing models.\nAmong these three models, In-\nstant3D [36] is a feed-forward method that takes a much\nshorter time compared to the top two methods. While our\nevaluation metrics\u2019 reliability can still be limited, we hope\nthat such a holistic picture can provide essential guidance\nfor developing future text-to-3D algorithms.\n6.3. Extension to Other Criteria\nWhile we focus our empirical studies in five criteria, our\nmetric can be adapted to evaluating a different criteria users\nmight care about. For example, it is important that a genera-\ntive model can produce different outputs given different ran-\ndom seeds. This aspect is commonly underlooked by most\ntext-to-3D metrics. With small modification of the text and\nimage prompt input into GPT-4V, our method can be ap-\nplied to evaluate diversity. Figure 6 shows the visual image\nwe provide GPT-4V when prompting it to answer the ques-\ntion about which model\u2019s output has more diversity. For\neach method, we produce 9 3D assets using different ran-\ndom seeds. We render each of these assets from a fixed\ncamera angle to create the input image fed into GPT-4V.\nThe text in Figure 6 is an excerpt of GPT-4V\u2019s answer. We\ncan see that GPT-4V is able to provide a reasonable judg-\nment about which image contains renders of more diverse\n3D assets. Currently, we are restricted to qualitative studies\nbecause most existing text-to-3D models are still compute-\nintensive. We believe that large-scale quantitative study is\nsoon possible with more compute-efficient text-to-3D mod-\nels, such as Instant3D, becoming available.\n8\n7. Discussion\nIn this paper, we have presented a novel framework\nleveraging GPT-4V to establish a customizable, scalable,\nand human-aligned evaluation metric for text-to-3D gener-\native tasks. First, we propose a prompt generator that can\ngenerate input prompts according to the evaluator\u2019s needs.\nSecond, we prompt GPT-4V with an ensemble of customiz-\nable \u201c3D-aware prompts.\u201d With these instructions, GPT-4V\nis able to compare two 3D assets according to an evaluator\u2019s\nneed while remaining aligned to human judgment across\nvarious criteria. With these two components, we are able to\nrank text-to-3D models using the Elo system. Experimental\nresults confirm that our approach can outperform existing\nmetrics in various criteria.\nLimitations and future work.\nWhile promising, our\nwork still faces several unresolved challenges. First, due\nto limited resources, our experiment and user studies are\ndone on a relatively small scale. It\u2019s important to scale up\nthis study to better verify the hypothesis. Second, GPT-\n4V\u2019s responses are not always true.\nFor example, GPT-\n4V sometimes shows hallucinations\u2014a prevalent issue for\nmany large pretrained models [73]. GPT-4V can also pro-\ncess some systematic errors, such as bias toward certain im-\nage positions [74, 75]. Such biases, if unknown, could in-\nduce errors in our evaluation metric. While our ensembling\ntechnique can mitigate these issues, how to solve them effi-\nciently and fundamentally remains an interesting direction.\nThird, a good metric should be \u201cun-gamable\u201d. However\none could potentially construct adversarial patterns to at-\ntack GPT-4V. This way one might gain a high score without\nneeding to produce high-quality 3D assets. Last, while our\nmethod is more scalable than conducting user preference\nstudies, we can be limited by computation, such as GPT-4V\nAPI access limits. Our method also requires a quadratically\ngrowing number of comparisons, which might not scale\nwell when evaluating a large number of models when com-\npute is limited. It would be interesting to leverage GPT-4V\nto intelligently select input prompts to improve efficiency.\nAcknowledgement.\nThis project was in part supported by\nGoogle, Samsung, Stanford HAI, Vannevar Bush Faculty\nFellowship, ARL grant W911NF-21-2-0104, and Shanghai\nAI Lab. We would love to thank members of Stanford Com-\nputational Imaging Lab, Stanford Geometric Computation\nGroup, Shanghai AI Lab, and Adobe Research for useful\nfeedback and discussion.\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas J. Guibas. Learning representations and generative\nmodels for 3d point clouds. In International Conference on\nMachine Learning, 2017. 2\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katie Millican, Malcolm Reynolds, Roman Ring,\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,\nSina Samangooei, Marianne Monteiro, Jacob Menick, Se-\nbastian Borgeaud, Andy Brock, Aida Nematzadeh, Sa-\nhand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karen Simonyan.\nFlamingo: a visual language model for few-shot learning.\nArXiv, abs/2204.14198, 2022. 3\n[3] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Tachard Passos, Sia-\nmak Shakeri, Emanuel Taropa, Paige Bailey, Z. Chen,\nEric Chu, J. Clark, Laurent El Shafey, Yanping Huang,\nKathleen S. Meier-Hellstern, Gaurav Mishra, Erica Mor-\neira, Mark Omernick, Kevin Robinson, Sebastian Ruder,\nYi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gus-\ntavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul\nBarham, Jan A. Botha, James Bradbury, Siddhartha Brahma,\nKevin Michael Brooks, Michele Catasta, Yongzhou Cheng,\nColin Cherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, C Cr\u00b4epy, Shachi Dave, Mostafa Dehghani,\nSunipa Dev, Jacob Devlin, M. C. D\u2019iaz, Nan Du, Ethan Dyer,\nVladimir Feinberg, Fan Feng, Vlad Fienber, Markus Freitag,\nXavier Garc\u00b4\u0131a, Sebastian Gehrmann, Lucas Gonz\u00b4alez, Guy\nGur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua How-\nland, An Ren Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Is-\nard, Abe Ittycheriah, Matthew Jagielski, Wen Hao Jia, Kath-\nleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan,\nKatherine Lee, Benjamin Lee, Eric Li, Mu-Li Li, Wei Li,\nYaguang Li, Jun Yu Li, Hyeontaek Lim, Han Lin, Zhong-\nZhong Liu, Frederick Liu, Marcello Maggioni, Aroma Ma-\nhendru, Joshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nystrom, Ali-\ncia Parrish, Marie Pellat, Martin Polacek, Alex Polozov,\nReiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker\nRiley, Alexandra Ros, Aurko Roy, Brennan Saeta, Rajku-\nmar Samuel, Renee Marie Shelby, Ambrose Slone, Daniel\nSmilkov, David R. So, Daniela Sohn, Simon Tokumine,\nDasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi\nWang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting,\nYuhuai Wu, Ke Xu, Yunhan Xu, Lin Wu Xue, Pengcheng\nYin, Jiahui Yu, Qiaoling Zhang, Steven Zheng, Ce Zheng,\nWei Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm\n2 technical report. ArXiv, abs/2305.10403, 2023. 3\n[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\nSamir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon\nKornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Worts-\nman, and Ludwig Schmidt. Openflamingo: An open-source\nframework for training large autoregressive vision-language\nmodels. ArXiv, abs/2308.01390, 2023. 3\n[5] Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou,\nXingguang Yan, Gordon Wetzstein, Leonidas J. Guibas, and\nAndrea Tagliasacchi. Cc3d: Layout-conditioned generation\n9\nof compositional 3d scenes. ArXiv, abs/2303.12074, 2023.\n2, 3\n[6] Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen,\nFaizan Farooq Khan, Li Erran Li, and Mohamed Elhoseiny.\nHrs-bench: Holistic, reliable and scalable benchmark for\ntext-to-image models. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 20041\u2013\n20053, 2023. 2\n[7] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gersten-\nberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann,\nMichal Podstawski, Hubert Niewiadomski, Piotr Nyczyk,\net al. Graph of thoughts: Solving elaborate problems with\nlarge language models.\narXiv preprint arXiv:2308.09687,\n2023. 5\n[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J.\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeff Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. ArXiv, abs/2005.14165, 2020. 3\n[9] Eric Chan, Connor Z. Lin, Matthew Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J.\nGuibas, Jonathan Tremblay, S. Khamis, Tero Karras, and\nGordon Wetzstein. Efficient geometry-aware 3d generative\nadversarial networks. 2022 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 16102\u2013\n16112, 2021. 2\n[10] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015. 1\n[11] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 2, 16\n[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham,\nHyung Won Chung,\nCharles Sutton,\nSebas-\ntian\nGehrmann,\nParker\nSchuh,\nKensen\nShi,\nSasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prab-\nhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner\nPope, James Bradbury, Jacob Austin, Michael Isard, Guy\nGur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya,\nSanjay Ghemawat,\nSunipa Dev,\nHenryk Michalewski,\nXavier Garc\u00b4\u0131a, Vedant Misra, Kevin Robinson, Liam Fedus,\nDenny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi, David\nDohan, Shivani Agrawal, Mark Omernick, Andrew M.\nDai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,\nAitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark D\u00b4\u0131az, Orhan Firat, Michele Catasta,\nJason Wei, Kathleen S. Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel.\nPalm: Scaling\nlanguage modeling with pathways.\nJ. Mach. Learn. Res.,\n24:240:1\u2013240:113, 2022. 3\n[13] Jacob Cohen. A coefficient of agreement for nominal scales.\nEducational and psychological measurement, 20(1):37\u201346,\n1960. 17\n[14] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi.\nObja-\nverse: A universe of annotated 3d objects. arXiv preprint\narXiv:2212.08051, 2022. 1\n[15] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli\nVanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia\nGkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 1\n[16] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 2\n[17] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13142\u201313153, 2023. 2\n[18] Danny Driess,\nF. Xia,\nMehdi S. M. Sajjadi,\nCorey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,\nand Peter R. Florence. Palm-e: An embodied multimodal\nlanguage model. In International Conference on Machine\nLearning, 2023. 3\n[19] Arpad E Elo. The proposed uscf rating system, its develop-\nment, theory, and applications. Chess Life, 22(8):242\u2013247,\n1967. 5, 16\n[20] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-Xin\nZou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. three-\nstudio: A unified framework for 3d content generation, 2023.\n6, 17\n[21] Zekun Hao, Arun Mallya, Serge J. Belongie, and Ming-\nYu Liu.\nGancraft: Unsupervised 3d neural rendering of\nminecraft worlds. 2021 IEEE/CVF International Conference\non Computer Vision (ICCV), pages 14052\u201314062, 2021. 2\n[22] Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin\nHu, Jenny Sheng, Ran Yi, Juanzi Li, and Yong jin Liu.\nT3bench: Benchmarking current progress in text-to-3d gen-\neration. ArXiv, abs/2310.02977, 2023. 2\n[23] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\n10\nric for image captioning. arXiv preprint arXiv:2104.08718,\n2021. 3, 6\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 1\n[25] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de\nLas Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, Tom Hennigan, Eric Noland, Katie Millican, George\nvan den Driessche, Bogdan Damoc, Aurelia Guy, Simon\nOsindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol\nVinyals, and L. Sifre. Training compute-optimal large lan-\nguage models. ArXiv, abs/2203.15556, 2022. 3\n[26] Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin John-\nson, and Matthias Nie\u00dfner.\nText2room: Extracting tex-\ntured 3d meshes from 2d text-to-image models.\nArXiv,\nabs/2303.11989, 2023. 3, 6\n[27] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A.\nSmith, and Jiebo Luo.\nPromptcap: Prompt-guided task-\naware image captioning. ArXiv, abs/2211.09699, 2022. 3\n[28] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen\nChi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia\nSong, and Furu Wei. Language is not all you need: Aligning\nperception with language models. ArXiv, abs/2302.14045,\n2023. 3\n[29] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 2, 6\n[30] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 2, 16\n[31] M. G. Kendall.\nA new measure of rank correlation.\nBiometrika, 30:81\u201393, 1938. 6, 7\n[32] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. CoRR, abs/1412.6980, 2014. 6\n[33] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-\ntiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open\ndataset of user preferences for text-to-image generation.\narXiv preprint arXiv:2305.01569, 2023. 6\n[34] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.\nHoi. Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation. In In-\nternational Conference on Machine Learning, 2022. 2, 3\n[35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi.\nBlip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models.\nArXiv, abs/2301.12597, 2023. 2, 3\n[36] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun\nLuan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg\nShakhnarovich, and Sai Bi.\nInstant3d:\nFast text-to-3d\nwith sparse-view generation and large reconstruction model.\nhttps://arxiv.org/abs/2311.06214, 2023. 2, 8, 16, 19\n[37] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 300\u2013309, 2023. 2, 3, 16\n[38] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang.\nSyncdreamer:\nLearning to generate multiview-consistent images from a\nsingle-view image. arXiv preprint arXiv:2309.03453, 2023.\n2, 16\n[39] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, and Wenping Wang.\nWonder3d: Single image to 3d using cross-domain diffusion,\n2023. 2, 16\n[40] David Lopez-Paz and Maxime Oquab. Revisiting classifier\ntwo-sample tests. arXiv: Machine Learning, 2016. 2\n[41] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. arXiv preprint arXiv:2211.07600,\n2022. 2, 8, 16, 19\n[42] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 1,\n2\n[43] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 5, 16\n[44] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 2, 16\n[45] OpenAI. Gpt-4v(ision) system card. OpenAI, 2023. 3, 18\n[46] R OpenAI.\nGpt-4 technical report.\narXiv, pages 2303\u2013\n08774, 2023. 2, 3\n[47] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 165\u2013174, 2019. 1\n[48] Ryan Po, Wang Yifan, and Vladislav Golyanik et al. State of\nthe art on diffusion models for visual computing. In arxiv,\n2023. 1, 2\n[49] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann, Tim\nDockhorn, Jonas Muller, Joe Penna, and Robin Rombach.\nSdxl: Improving latent diffusion models for high-resolution\nimage synthesis. ArXiv, abs/2307.01952, 2023. 6, 17\n[50] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2\n[51] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv,\n2022. 1, 2, 6, 13, 16, 19\n11\n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In International\nConference on Machine Learning, 2021. 2, 6\n[53] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman,\nMichael Rubinstein, Jonathan T. Barron, Yuanzhen Li, and\nVarun Jampani. Dreambooth3d: Subject-driven text-to-3d\ngeneration. ArXiv, abs/2303.13508, 2023. 3\n[54] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n2\n[55] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022.\n[56] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 2\n[57] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. ArXiv, abs/2210.08402,\n2022. 6\n[58] Ho Youn Seo, Hayeon Kim, Gwanghyun Kim, and Se Young\nChun.\nDitto-nerf: Diffusion-based iterative text to omni-\ndirectional 3d model. ArXiv, abs/2304.02827, 2023. 3\n[59] Zhenwei Shao, Zhou Yu, Mei Wang, and Jun Yu. Prompting\nlarge language models with answer heuristics for knowledge-\nbased visual question answering. 2023 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 14974\u201314983, 2023. 3\n[60] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv:2308.16512, 2023. 2, 8, 16, 19\n[61] Zhan Shi, Xu Zhou, Xipeng Qiu, and Xiaodan Zhu.\nIm-\nproving image captioning with better use of captions. arXiv\npreprint arXiv:2006.11807, 2020. 5\n[62] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 1\n[63] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for ef-\nficient 3d content creation. ArXiv, abs/2309.16653, 2023. 2,\n3, 16\n[64] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-\nlien Rodriguez, Armand Joulin, Edouard Grave, and Guil-\nlaume Lample. Llama: Open and efficient foundation lan-\nguage models. ArXiv, abs/2302.13971, 2023. 3\n[65] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali\nEslami, Oriol Vinyals, Felix Hill, and Zacharias Janssen.\nMultimodal few-shot learning with frozen language models.\nIn Neural Information Processing Systems, 2021. 3\n[66] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich.\nScore jacobian chaining: Lift-\ning pretrained 2d diffusion models for 3d generation. arXiv\npreprint arXiv:2212.00774, 2022. 2, 16\n[67] Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou,\nJie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chen-\nguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal,\nand Heng Ji.\nLanguage models with image descrip-\ntors are strong few-shot video-language learners.\nArXiv,\nabs/2205.10747, 2022. 3\n[68] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongx-\nuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity\nand diverse text-to-3d generation with variational score dis-\ntillation. arXiv preprint arXiv:2305.16213, 2023. 2, 8, 16,\n19\n[69] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou.\nChain of thought prompting elicits reasoning in large lan-\nguage models. ArXiv, abs/2201.11903, 2022. 5, 13\n[70] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu,\nSerge J. Belongie, and Bharath Hariharan.\nPointflow: 3d\npoint cloud generation with continuous normalizing flows.\n2019 IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 4540\u20134549, 2019. 2\n[71] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn\nof lmms: Preliminary explorations with gpt-4v (ision). arXiv\npreprint arXiv:2309.17421, 9, 2023. 2, 3, 5, 14, 16, 18\n[72] Andy Zeng, Adrian S. Wong, Stefan Welker, Krzysztof\nChoromanski, Federico Tombari, Aveek Purohit, Michael S.\nRyoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke,\nand Peter R. Florence.\nSocratic models:\nComposing\nzero-shot multimodal reasoning with language.\nArXiv,\nabs/2204.00598, 2022. 3\n[73] Muru Zhang, Ofir Press, Will Merrill, Alisa Liu, and Noah A.\nSmith. How language model hallucinations can snowball.\nArXiv, abs/2305.13534, 2023. 9\n[74] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan,\nLianke Qin, Heng Wang, Xifeng Yan, William Yang Wang,\nand Linda Ruth Petzold.\nGpt-4v (ision) as a general-\nist evaluator for vision-language tasks.\narXiv preprint\narXiv:2311.01361, 2023. 3, 9\n[75] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo-\nhan Li, Dacheng Li, Eric Xing, et al.\nJudging llm-as-\na-judge with mt-bench and chatbot arena.\narXiv preprint\narXiv:2306.05685, 2023. 3, 9\n12\nA. Overview\nThis supplementary material includes additional details\nabout our experiment and methods, which can not be fully\ncovered in the main paper due to limited space. We first\nprovide more details about the method, including our meta-\nprompts and comparison prompts, in Section B. Experiment\ndetails, such as baselines and data, are included in Sec-\ntion C. We also provide detailed ablation studies about the\neffectiveness of different ways to prompt GPT-4V and en-\nsemble its output (Section D). More experimental results are\nprovided in Section E. Finally, we demonstrate some failure\ncases of our methods in Section F.\nB. Method Details\nWe will include detailed descriptions about how we im-\nplement the two components of our method: prompt gen-\nerator (in Sec. B.1) and 3D assets evaluator (in Sec B.2).\nSection B.3 provides additional details about how we use\nthe elo rating.\nB.1. Prompt Generator\nOur prompt generation pipeline includes a conversation\nwith GPT-4V. We aim to design a conversation pipeline\nthat can provide GPT-4V with the necessary context of the\nprompt generation task while remaining customizable so\nthat evaluators can adapt this conversation to their need.\nOur first prompt describes the task of generating text\nprompt for text-to-3D generative models.\nFor example,\nwhat\u2019s the typical length of each prompt and how do we\nwant the distribution of the final collection of prompts look\nlike. We also include a description of how an evaluator\nmight want to control this generator in this prompt. Please\nsee Figure S7-S8 for this opening prompt.\nNow that GPT-4V has some basic understanding of the\ntask, we will further provide it with necessary components\nneeded to composed the prompt. First, we provide with\nGPT-4V a list of ten main categories from which it\u2019s able\nto select subjects of the prompt from (see Figure S1, first\nbox). These categories are chosen to include the most com-\nmon classes of objects users are interested in generating in\ntext-to-3D tasks. The goal is to make our generator aware\nof a comprehensive list of potential options, reducing the\nrisk of our prompt generator being unintentionally biased\ntoward certain categories when performing in-context learn-\ning with exemplary prompts. These ten categories focuses\non what one might wish to generate (i.e. the subjects men-\ntioned in Section 4.1 of the main paper).\nIn addition to choosing what to generate, the text-to-3D\ngenerative model user also might want to specify a certain\nstate the subject is in (e.g. \u201ca sleeping cat\u201d as opposed to\n\u201ca cat\u201d). Such description is referred to as the properties\nin Section 4.1 of the main paper. To achieve this, we addi-\ntionally provide GPT-4V a list of descriptions of different\nproperties a user of text-to-3d model might be interested in\nincluding (see Figure S1, second row, where each column\ninclude list of properties under one of the five aspects we\nlisted). Note that this specific instruction will base on the\nsubjects and a given level of creativity and complexity the\nevaluator specifies, which will be attached to the beginning\nof prompt. Please see Figure S7-S8 for the complete de-\ntailed prompt.\nFinally, we will provide our prompt generator with a list\nof exemplary prompt where it can model the output from.\nUsers can curate this list of prompt according to their need.\nIn our case, we include the prompts by Dreamfusion [51].\nWith these exemplary prompts and our provided instruc-\ntion designed to be customizable according to different sce-\nnarios, GPT-4V is now able to provide a list of prompts ac-\ncording to the evaluator\u2019s input. Examples of the generated\nprompts from different levels of creativity and complexity\nare shown in Figure S2. We can see that users can cre-\nate prompts with different difficulties and focus using our\nprompt generator. While we only focus on only two differ-\nent axes (i.e. creativity and complexity) in this work, but our\nprompt generating pipeline can be easily adapted to other\nneeds as the evaluators can change various part of our con-\nversations accordingly.\nB.2. 3D Assets Evaluator\nIn the instruction for the GPT-4V evaluator, we first ex-\nplain the task and the definition of each criterion. Then,\nwe asked to take a close look at the input pairs of multi-\nview images and normal maps before providing its analysis\nand final comparisons. Finally, we specify a specific out-\nput format for GPT-4V to follow. In our output format, we\nalso require GPT-4V to provide a short reasoning of why it\ncommit to certain choices, inspired by prompting technique\nsuch as chain-of-thought [69]. Specifying an output format\ncan also help us better collect results at a larger scale. An\nexample of such an instruction is shown in Figure S3.\nNote that the description of the criteria can be adjusted\naccording to evaluation task. In additional, one can also in-\nclude in-context learning by breaking down this single text-\nprompt into multiple pieces, where the following pieces\nprovide exemplary answer of the comparison task in hand.\nWhile we believe this can potentially provide a better per-\nformance, we only use a single text-prompt and a single\nimage for each pairwise comparison due to limited API ac-\ncess.\nIn additional to the text instruction, we also need to ren-\nder images from different view-points to input into GPT-4V.\nFor each text-to-3D model, we provide one, four, or nine\nrenders with camera evenly distributed surroundingly. Note\nthat different text-to-3D generative models might use dif-\nferent coordinate systems, and GPT-4V might prefer certain\n13\nGeometry\nAppearance\nStyle\nScene\nStatus\nLiving Beings            Plants            Buildings and Structures            Vehicles            Furniture            Electronics  \n    Household Items          Clothing and Accessories          Abstract Objects          Natural Elements          Food and Beverages\nSubject\n\u2022\nVolume: hollow, solid, \nporous, or layered, \u2026\n\u2022\nSymmetry: symmetrical, \nasymmetrical, or radially \nsymmetrical, \u2026\n\u2022\nContours: smooth, jagged, \nirregular, or undulating, \u2026\n\u2022\nInternal Structures: empty, \ncompartmentalized, or \nmulti-layered, \u2026\n\u2022\nShape: cone, cylinder, \nsphere, \u2026\n\u2022\nColor: Specific color, \npatterns, gradients, \u2026\n\u2022\nMaterials: Wood, metal, \nglass, fabric, stone, \u2026\n\u2022\nTextures: Smooth, rough, \nfurry, scaly, \u2026\n\u2022\nFinish: Glossy, matte, \ntranslucent, opaque, \u2026\n\u2022\nSize: Small, medium, large, \nspecific dimensions, \u2026\n\u2022\nState: New, old, worn, \npristine, \u2026\n\u2022\nStatic: Still, motionless, ...\n\u2022\nDynamic: Moving, \nchanging, ...\n\u2022\nEmotional State: Happy, \nsad, angry, ...\n\u2022\nPhysical State: Broken, \nintact, in use, ...\n\u2022\nInteraction: Interacting \nwith another object or \nenvironment, ...\n\u2022\nAesthetic: Minimalistic, \nornate, modern, vintage, ...\n\u2022\nCultural: Asian, African, \nWestern, Middle Eastern, ...\n\u2022\nEmotional: Cheerful, \ngloomy, energetic, calm, ...\n\u2022\nFunctional: Practical, \ndecorative, symbolic, ...\n\u2022\nConceptual: Abstract, \nrealistic, surrealistic, \nimpressionistic, ...\n\u2022\nEnvironment: Indoor, \noutdoor, urban, rural, \nnatural, fantastical, ...\n\u2022\nContext: Part of a larger \nscene, event, or story, ...\n\u2022\nLighting: Day, night, \nartificial, natural, shadows, \nhighlights, ...\n\u2022\nWeather: Sunny, rainy, \ncloudy, stormy, ...\n\u2022\nScale: The relative size of \nthe object in the scene, ...\nFigure S1. Subjects and properties provided for the prompt generator.\nComplexity\nCreativity\nA large, hollow, asymmetrically shaped \namphitheater, with jagged stone seating, nestled \nin a natural landscape, a classical play being \nperformed as the sun sets.\nSeveral solid, spherical, weathered cannonballs, \nwith a rough cast-iron texture, lying beside a \nrusted cannon in a historical fort overlooking a \nserene bay.\nA dragon-shaped kite, with scales that \nshimmer in the sunlight as it dances in \nthe wind.\nA teddy bear, fur matted, one eye \nmissing.\nAn origami crane made from a map.\nA velvet-lined violin case, which opens to \nreveal a garden of miniature roses.\nAn ice cream scoop that serves up scoops \nof cloud fluff instead of ice cream.\nA worn leather recliner with a knitted \nthrow draped over the back.\nA cluster of tents pitched near a forest, \ncampfire smoke curling into the evening sky.\nA trio of spicy tacos with lime wedges \non a plate, ready to be served.\nGray squirrel with an acorn in its mouth.\nOrange monarch butterfly resting on a \ndandelion.\nCaterpillar with a keyboard pattern \non its back.\nA weathered hiking backpack with \npatches.\nAn orange tabby cat shaped cookie \njar.\nCeramic mug with a chip on the rim.\nCanvas sneakers, white, slightly dirt-\nstained.\nDenim jacket with patches and buttons.\nKnitted scarf, maroon and gold stripes.\nFloating bonsai tree, roots in mid-air.\nA quill pen, feather shifts through \nrainbow hues.\nZebra with barcode stripes.\nBeach pebble with an intricate \nlabyrinth pattern naturally engraved.\nA hammock strung between two skyscrapers, \nswaying high above a neon cityscape.\nAn ensemble of jellyfish-like hanging lamps, \npulsing with soft bioluminescence.\nA plush octopus whose arms are gently \nwaving pencils.\nAn umbrella that blossoms with holographic \nconstellations underneath when it rains.\nAn array of small, solid, symmetrical, pastel-\ncolored eggs, each revealing a miniature, \nenchanted forest scene when cracked open.\nAn oversized, porous, sphere-shaped birdcage, \nmade of woven golden wires, with a matte finish, \nhousing a small, mechanical, singing bird that \nflutters in a lifelike manner.\nVarious hollow, asymmetrical, textured seashells, \ncollected in a sand-filled, clear glass jar with a \ntwine-tied neck, displayed on a windowsill.\nA solid, symmetrical, smooth stone fountain, \nwith water cascading over its edges into a clear, \ncircular pond surrounded by blooming lilies, in \nthe centre of a sunlit courtyard.\nLow\nMedium\nHigh\nLow\nMedium\nHigh\nFigure S2. Examples of the generated prompts with different levels of creativity and complexity.\nviews of the object than others. We leave it to the evaluator\nor the text-to-3D model developer to adjust the coordinate\nsystem to allow GPT-4V to make the best judgment.\nFinally, we create several slightly perturbed version of\nthe input instruction and images to obtain multiple out-\ncomes from GPT-4V. We will ensemble these outcomes to\nprovide a more accurate estimate, as suggested by Yang\net al. [71]. Specifically, we perturb the visual information\n14\nOur task here is the compare two 3D objects, both generated from the same text description. We want to decide which one is better according to the \nprovided criteria.\n# Instruction\n1. Text prompt and Asset Alignment. Focus on how well they correspond to the given text description. An ideal model should accurately reflect all objects \nand surroundings mentioned in the text prompt, capturing the corresponding attributes as described. Please first describe each of the two models, and then \nevaluate how well it covers all the attributes in the original text prompt.\n2. 3D Plausibility. Look at both the RGB and normal images and imagine a 3D model from the multi-view images. Determine which model appears more natural, \nsolid, and plausible. Pay attention to any irregularities, such as abnormal body proportions, duplicated parts, or the presence of noisy or meaningless 3D \nstructures. An ideal model should possess accurate proportions, shapes, and structures that closely resemble the real-world object or scene.\n3. Geometry-Texture Alignment. This examines how well the texture adheres to the geometry. The texture and shape should align with each other locally. \nFor instance, a flower should resemble a flower in both the RGB and normal map, rather than solely in the RGB. The RGB image and its corresponding normal \nimage should exhibit matching structures.\n4. Low-Level Texture Details. Focus on local parts of the RGB images. Assess which model effectively captures fine details without appearing blurry and \nwhich one aligns with the desired aesthetic of the 3D model. Note that overly abstract and stylized textures are not desired unless specifically mentioned in \nthe text prompt.\n5. Low-Level Geometry Details. Focus on the local parts of the normal maps. The geometry should accurately represent the intended shape. Note that \nmeaningless noise is not considered as high-frequency details. Determine which one has a more well-organized and efficient structure, which one exhibits \nintricate details, and which one is more visually pleasing and smooth.\n6. Considering all the degrees above, which one is better overall?\nTake a really close look at each of the multi-view images for these two 3D objects before providing your answer.\nWhen evaluating these aspects, focus on one of them at a time.\nTry to make independent decisions between these criteria.\n# Output format\nTo provide an answer, please provide a short analysis for each of the abovementioned evaluation criteria. The analysis should be very concise and accurate.\nFor each of the criteria, you need to make a decision using these three options:\n1. Left (object 1) is better;\n2. Right (object 2) is better;\n3. Cannot decide.\nIMPORTANT: PLEASE USE THE THIRD OPTION SPARSELY.\nThen, in the last row, summarize your final decision by \"<option for criterion 1> <option for criterion 2> <option for criterion 3> <option for criterion 4> <option \nfor criterion 5> <option for criterion 6>\".\n# Example\n\"\nAnalysis:\n1. Text prompt and Asset Alignment: The left one ...; The right one ...; The left/right one is better or cannot decide.\n2. 3D Plausibility. The left one ...; The right one ...; The left/right one is better or cannot decide.\n3. Geometry-Texture Alignment. The left one ...; The right one ...; The left/right one is better or cannot decide.\n4. Low-Level Texture Details. The left one ...; The right one ...; The left/right one is better or cannot decide.\n5. Low-Level Geometry Details. The left one ...; The right one ...; The left/right one is better or cannot decide.\n6. Overall, ... The left/right one is better or cannot decide.\nFinal answer:\nx x x x x x (e.g., 1 2 2 3 2 1 / 3 3 3 2 1 3 / 3 2 2 1 1 1)\n\"\nFigure S3. An example of prompts used to guide the GPT-4V evaluator.\n15\nwith three versions: including pure RGB renders, pure sur-\nface normal maps, and containing both. For text instruc-\ntions, we experiment with three versions: 1) GPT-4V is re-\nquired to evaluate a full list of six criteria, 2) include only\nthe criteria of interest, and 3) explicitly ask GPT-4V to eval-\nuate geometry first. We also perturb the number of render\nviews in three versions: 1) only a single view-point; 2) four\nview-points in a 2x2 layout, and 3) 9 view-points in a 3x3\nlayout. Finally, we also augment the visual information in\nthree ways: 1) horizontally flipping the left-right order of\nthe objects; 2) vertically flipping the up-down order of the\nRGB images and normal maps and 3) adding visual water-\nmark into the image [71]. A detailed ablation will be pro-\nvided in the later section (Sec R1).\nB.3. ELO Rating\nComparing two 3D assets can yield different outputs de-\npending on the evaluator or the criteria. This is because\nhuman preference is highly subjective and stochastic. We\nneed to model such stochasticity when designing a rating\nthat correctly reflects how much a text-to-3D method is pre-\nferred by human evaluators. Such stochasticity is also en-\ncountered when two chess players are playing against each\nother. Since a stronger player can sill lose to a weaker player\nwhen the stronger player he or she is having a bad day. In\nother words, the outcome of comparisons can be noisy. For-\ntunately, the ELO rating system is designed to create a rat-\ning that can reflect the true player\u2019s performance while the\ncomparison results are noisy. We will provide some basic\nintuition of how our ELO system worked for completeness\nand please refer to prior literatures for more details [19, 43].\nThe ELO rating system tries to assign a number, \u03c3i for\nthe ith player, which reflects the strength of this player. Let\u2019s\nexamine the case when two players, i and j, play together\nand assume without loss of generality that \u03c3i > \u03c3j. The\nprobability of ith player winning the game should be larger\nwhen the difference between \u03c3i and \u03c3j is larger. One way\nto define such probability of winning is the following:\nPr(\u201ci beats player j\u201d) =\n1\n1 + 10\n\u03c3j \u2212\u03c3i\nc\n,\n(3)\nwhere c controls the scale of the difference (i.e. if the dif-\nference is c then the probability of winning will be 1/11).\nFrom this formula, if \u03c3i = \u03c3j, then the probability that\nplayer i wins over player j is exactly 1/2. It\u2019s also easy to\nsee that\nPr(\u201ci beats j\u201d) + Pr(\u201cj beats i\u201d)\n(4)\n=\n1\n1 + 10\n\u03c3j \u2212\u03c3i\nc\n+\n1\n1 + 10\n\u03c3i\u2212\u03c3j\nc\n= 1.\n(5)\nDuring experiment, we observe that i beats j for Aij num-\nber of times and j beats i for Aji number of times. The\nidea is to find the assignment of \u03c3i and \u03c3j such that it best\nreflects the empirical observation. Specifically, we achieve\nthis by maximum likelihood estimation:\nmax\n\u03c3i,j Pr(Aij, Aji|\u03c3i, \u03c3j)\n= max\n\u03c3i,j log\n\u0000Pr(\u201ci beats j\u201d)Aij Pr(\u201cj beats i\u201d)Aji\u0001\n= max\n\u03c3i,j Aij log (Pr(\u201ci beats j\u201d)) + Aji log (Pr(\u201cj beats i\u201d))\n= min\n\u03c3i,j Aij log\n\u0010\n1 + 10\n\u03c3j \u2212\u03c3i\nc\n\u0011\n+ Aji log\n\u0010\n1 + 10\n\u03c3i\u2212\u03c3j\nc\n\u0011\n.\nEquation (1) in the main paper can be directly derived from\nthe equation above by summing over all pairs of i \u0338= j. In\npractice, we will initialize \u03c3i = 1000 for all i and use Adam\noptimizer to optimize \u03c3i for this loss for 10000 iterations\nwith a learning rate of 0.1. Since Elo score is invariant to\nadding or subtracting a constant, we further calibrate our\nscores by setting Dreamfusion [51] to have an Elo of 1000\nfor all criteria.\nNote that this method consider the setting where the out-\ncomes include only i wins or j wins. In our case, there are\nnon trivial number of 3D assets pairs from which the hu-\nman annotator cannot determine which is better. To handle\nthese cases, we follow Nichol et al. [43] to add a win to both\nmethods. This can effectively dilate the number of winning\ntimes. Our Aij counts the number of times text-to-3D gen-\nerative model i wins over model j over any captions. This\ncan be adapted in this theoretical framework by considering\nthe distribution of input text-prompts:\nPr(\u201ci beats player j\u201d) =\nZ\nPr(\u201ci beats player j\u201d|t)P(t)dt,\nwhere t denotes an input text prompt. Following most Elo\nsystems, we choose c = 400.\nC. Experimental Details\nIn this section we will provide some additional experi-\nment details. We provide a detailed list of text-to-3d gener-\native models for benchmarking in Section C.1. Additional\ndetail about our user studies is provided in Section C.2.\nC.1. Text-to-image Models\nWe involve 13 generative models in the benchmark, in-\ncluding ten optimization-based methods and three recently\nproposed feed-forward methods. Please refer to the supple-\nmentary for the complete list of methods. For optimized-\nbase methods, we include DreamFusion [51], SJC [66],\nLatent-Nerf [41], Magic3D [37], Fantasia3D [11], Pro-\nlific Dreamer [68], DreamGaussian [63], MVDream [60],\nSyncDreamer [38] and Wonder3D [39]. For feed-forward\nmethods, we include PointE [44], Shap-E [30], and In-\nstant3D [36]. We leverage each method\u2019s official imple-\n16\nmentations when available. Alternatively, we turn to Three-\nstudio\u2019s implementation [20]. For methods designed mainly\nfor image-to-3D, we utilize Stable Diffusion XL [49] to\ngenerate images conditioned on text as input to these\nmodels.\nExperiments are conducted with default hyper-\nparameters.\nC.2. User study details\nIn this paper, we mainly rely on labels provided by expert\nannotators. Our expert annotators are all graduate students\nwith computer graphic background (e.g. they all have expe-\nrience looking at surface normal maps). We recruit twenty\nsuch expert annotators from this background. For 13 meth-\nods, we create pairwise comparison between each pair of\nmethods (so 78 method pairs). For each method pairs of\nmethods, we sample 3 captions from the caption genera-\ntors. For each of these 234 comparisons, we assign three\ndifferent experts to rank all criteria. The experts are asked\nto pick a shape that performs better according to certain cri-\nteria or indicate that these two shapes are indistinguishable.\nEach user will fill out a query form that includes the same\ndescription we provided with GPT-4V. Different from what\nwe provided with GPT-4V, expert annotators are able to see\nvideo of 360 rotated render of the object in RGB and in\nsurface normals. The annotator can control the video (e.g.\nreplaying to certain point) to exame the 3D shape in more\ndetails. The video is rendered in 1024x2048 resolution. In\nour setting, expert annotators are provided with more infor-\nmation comparing to what we provided to GPT-4V. Our ex-\npert annotators achieve reasonable agreement with greater\nthan 0.53 Cohen kappa [13].\nOne disadvantage of expert annotation is that it\u2019s difficult\nto get by so we are only able to obtain a small scale of such\nannotation. On the contrary, one can obtain larger-scale an-\nnotation from general users. We\u2019ve also conducted some\npreliminary exploration conducting user preference studies\nwith general users. Unfortunately, we found that data col-\nlected from general users are very noisy. Specifically, we re-\ncruited about 53 users and performed the same user studies\nas done with the expert. In addition to the instruction pro-\nvided with GPT-4V, we also provide examples comparison\nand requires the user to go through a short training session.\nThe average user agreement (measured by Cohen\u2019s kappa)\namong these users can barely reach 0.3. This can potentially\nbe caused by the fact that general users do not have expe-\nrience reasoning about 3D asset information, so computer\ngraphics terms (e.g. texture, surface normal, or geometry),\nbecome harder to understand. As a result, we leverage ex-\npert user data as reference labels in this paper. How to con-\nduct large-scale user studies with more available annotators\nremains a very interesting future work direction.\nD. Ablation Studies\nIn this section, we will examines key technical design\nchoices of our methods. Specifically, we will focus our ab-\nlation on different ways to perturb the input data to create\nensemble results (Section 5.2 of main paper). We first carry\nout an ablation study on different ways to perturb the input\n(Section D.1). Then we show an ablation on how to ensem-\nble these perturbations together (Section D.2).\nDue to limited API access, we are not able to obtain\nenough GPT-4V queries to compute method-level human\nalignment scores as in Table 1 or the pair-level alignment\nscore in Table 2 in the main papers. To address this lim-\nitation, we use an alternative metric in the ablation. For\neach variant of our methods, we randomly sample 78 dif-\nferent comparisons of 3D assets. For each comparison, we\ncompute the probability that our GPT-4V 3D asset evalua-\ntor would select one shape versus the other. We denote such\nempirically estimated probability as pi for our method\u2019s\nvariant to choose one shape at the ith comparison. Let qi\nbe the same probability estimated from expert annotation\ndata. We return the following L1-distance as an estimation\nof how our method is mis-alignment with human judgment:\nL1-dist(p, q) = 2\nN\nN\nX\ni=1\n|pi \u2212 qi|,\n(6)\nwhere N is the number of comparisons here. Note that the\nlower this metric is, the better alignment with human judge-\nment.\nD.1. Ablation for GPT-4V Input Perturbation\nWe conduct ablations on different ways to perturn the\ninputs for GPT-4V. We will investigate four different cat-\negories of perturbations, including visual information, text\ninstruction, view number, and augmentation. The main re-\nsults are summarized in Table R1.\nVisual Information\nThe visual information can include\nRGB images, normal maps, or both of them. Purely using\nRGB as input can benefit the perception of texture details,\nprobably because GPT-4V can spend more computational\npower on the presentation of the textures. However, not all\nthe criteria can be evaluated merely from the RGB infor-\nmation. As a result, skip evaluation of alignment on those\ncriteria, namely \u201cColor-Geo\u201d and \u201cGeometry\u201d. Only pre-\nsenting the normal maps to GPT-4V does not bring much\nimprovement for its alignment to human choices even for\n\u201cGeometry Details\u201d. We can see that RGB renders seem to\nplay the most important role for GPT-4V to make human-\naligned decisions. Surface normal render is required to per-\nform many evaluations of criteria about geometries.\n17\nTable R1. Ablation studies on different visual and textual input to GPT-4V. We mark rank one, rank two, and rank three in each\ncriterion with increasingly lighter shades of blue, and the same baseline (RGB + Normal, 2x2, Joint) is marked in gray.\nMethods\nAlignment (\u2193)\nPlausibility (\u2193)\nColor-Geo (\u2193)\nTexture (\u2193)\nGeometry (\u2193)\nVisual Information\nPure RGB\n0.523\n0.564\n-\n0.354\n-\nPure Normal\n0.674\n0.654\n-\n-\n0.579\nRGB + Normal\n0.518\n0.672\n0.628\n0.444\n0.510\nText Instruction\nJoint\n0.518\n0.672\n0.628\n0.444\n0.510\nSeparate\n0.451\n0.597\n0.610\n0.433\n0.528\nGeo-first\n0.682\n0.646\n0.662\n0.487\n0.505\nView number\n1\n0.592\n0.644\n0.603\n0.423\n0.438\n2x2\n0.518\n0.672\n0.628\n0.444\n0.510\n3x3\n0.546\n0.582\n0.654\n0.503\n0.559\nAugmentation\nHorizontal Flip\n0.615\n0.702\n0.676\n0.522\n0.651\nVertical Flip\n0.764\n0.695\n0.754\n0.738\n0.708\nWatermark\n0.605\n0.559\n0.597\n0.577\n0.492\nText Instruction\nWe experiment different way to input\nuser criteria into the text instruction. Jointly inputting all\ncriteria into a same text prompt can significantly reduce the\nnumber of API calls required. An example of this kind of\ntext instruction can be seen in Figure S3. We also try to\nevaluate only one criterion at a time. One can see a clear\nimprovement for most of the degrees thanks to the more fo-\ncused analysis, especially for \u201cText-Asset Alignment\u201d. This\npresents a trade-off between compute and accuracy.\nView number\nThe number of views denotes how many\nmulti-view images are shown at the same time. Given the\nassumption that the visual context size is fixed for GPT-\n4V [45], this ablation explores the trade-off between the\nperception of global coherency and local details. Presenting\nonly one view at a time can largely improve GPT-4V\u2019s abil-\nity in evaluating low-level criteria like \u201cTexture-Geometry\nAlignment\u201d, \u201cTexture Details\u201d, and \u201cGeometry Details\u201d.\nHowever, the scarcity of views leads to challenges in eval-\nuating the global criteria like \u201cText-Asset Alignment\u201d and\n\u201c3D Plausibility\u201d. Increasing view numbers to four or nine\nwill largely alleviate this problem.\nAugmentation\nIn the study, we have also experimented\nwith various visual augmentation techniques [71], which\nrefers to changing the visual input slightly without chang-\ning the key information contained in the image. We experi-\nment with three augmentation methods: horizontal flipping\nof object positions (i.e. \u201cHorizontal Flip\u201d), the rearrange-\nment of RGB images in conjunction with their respective\nnormal maps (i.e. \u201cVerticle Flip\u201d), and the inclusion of wa-\ntermark annotations to indicate the \u201cleft\u201d and \u201cright\u201d objects\n(i.e. \u201cWatermark\u201d). Adding watermarks slightly improves\nthe alignment. This can be a result of watermarks reduc-\ning the ambiguity happened when we refer to certain image\npositions from the text instruction.\nOther findings.\nAnother interesting finding is that the re-\nsults get worse when the normal maps are emphasized in\nthe inputs. For example, in the setting of \u201cGeo-first\u201d, we\nfirst provide the surface normal and ask questions involving\ngeometries before providing the full RGB renders and ask-\ning the rest of questions. The setting of \u201cPure Normal\u201d also\nemphasizes surface normal map by not including the RGB\nrenders. These settings both lead to slightly worse result\nthan the baseline method.\nD.2. Ablation for Output Ensemble.\nIn this section, we want to explore what is the best\nway to combine different perturbations together. The ex-\nploration space of such combinations is huge as there are\nnk numbers of ways to combine n ways to perturb the in-\nput together to create k ensembles. Interestingly, we ob-\nserved that the variance of the L1-distance reduces as we in-\ncrease the number of ensembles increases. This is depicted\nfrom Figure S4-(a) shows the distribution of the L1-distance\nwhen ensembling different number of results together. We\ncan see that the alignment performance is not sensitive to\nparticular choice of input perturbations when the number of\nensembles is large enough. To achieve the most effective\nresults, we will incorporate a combination of various view\nnumbers, augmentations, visual information, and individual\nqueries for specific criteria, selecting 4-5 strategies overall\nfor the final combination. Figure S4-(b) that ensembling\nthese strategies together results in a metric outperforms all\nthe previous metrics when measured in L1-distance.\n18\nDistance (\u2193)\n1 \u2013 Distance (\u2191)\nNumber of ensembles\n(a)\n(b)\nFigure S4. Ablation studies of the robust ensemble. (a) highlights a consistent improvement in performance with an increase in ensemble\nsize, together with the decrease in the differences among various ensemble strategies. (b) shows how the robust ensemble significantly\nimproves human alignment across all dimensions.\nE. Additional Results\nIn this section, we will provide additional results which\ndo not fit in the original paper.\nE.1. Ranking\nWhile the results of our method are still largely limited\nby API request limits, we would like to show some prelim-\ninary results for how existing text-to-3D generative models\nperfrm according to our methods in different criteria. Ta-\nble R2 shows the top four methods according to GPT-4V in\nall six criteria. We can see that MVDream [60] is ranked\nthe first across various criteria. Latent-NeRF [41] is able to\nachieve strong performance, ranked second or third across\nall criteria. Similarly, Prolific dreamer [68] achieves com-\nparable performance as latent-NeRF except for Plausibility.\nFinally, Instant3D [36] is ranked the fourth places in all but\nPlausibility. Instant3D is ranked the third place in Plausi-\nbility, above Prolific dreamer. Dreamfusion [51] is able to\nachieve good Plausibility, probably thanks to its geometry\nregularization.\nE.2. GPT-4V Comparison Examples\nIn Figure S6, we show some more examples of GPT-\n4V\u2019s analysis and answers to this task (using \u201cleft / right /\nequal\u201d instead of \u201c1 / 2 / 3\u201d for better clarity. These exam-\nples demonstrate how the evaluator is able to make compar-\nisons of the paired visual input based on close observations\nand detailed analysis.\nF. Failure cases\nWe present some typical failure cases in Figure S5. In\nthe first instance, GPT-4V fails to detect the mismatch in\nthe number of balloons between the two objects, thereby\nhighlighting its limitations in accurately counting specific\nquantities. In the second instance, both objects exhibit sig-\nnificant issues with their underlying geometries, The object\non the left presents severe 3D plausibility problems, as ev-\nidenced by the presence of multiple faces and leg. Both\nobjects are plagued by low-level geometric noise. GPT-4V\ndemonstrates sensitivity to low-level noise, consequently\noverlooking the overarching issue with the object on the\nleft. Such problems can potentially be rectified if one can\nprovide a larger number of views to GPT-4V.\n19\nTable R2. Top-4 methods for different criteria according to our metrics.\nAlignment\nPlausibility\nColor-Geo\nTexture\nGeometry\n1st\nMVDream\nMVDream\nMVDream\nMVDream\nMVDream\n2nd\nProlific Dreamer\nLatent-NeRF\nLatent-NeRF\nProlific Dreamer\nProlific Dreamer\n3rd\nLatent-NeRF\nInstant3D\nProlific Dreamer\nLatent-NeRF\nLatent-NeRF\n4th\nInstant3D\nDreamfusion\nInstant3D\nInstant3D\nInstant3D\nLeft\nRight\nText-Asset Alignment: The left \u2026 a cup with a handle and a saucer \u2026 The right \u2026 a mug with a \nhandle and visible steam emanating from the top. Both models align with the attributes of being \nmugs filled with steaming coffee. \n3D Plausibility: The left \u2026 more cohesive and solid geometry \u2026; The right \u2026 shows some \nstrange distortions and noise in areas such as the steaming effect and the handle\u2026\nTexture-Geometry Coherency: On the left \u2026 the texture of the pattern aligns well with the \ngeometry ... The right model seems simpler, \u2026 but the steam's texture aligns with its \ngeometry \u2026 Both have good alignment \u2026 \nTexture Details: The texture of the left model is sharper and more detailed ... On the right, the \ntexture is simpler and less detailed, \u2026\nGeometry Details: The left object's local geometry seems sharper and more detailed; ... The \nright object, while textured, \u2026 indicating lower fidelity in the low-level geometry.\nFinal answer: equal left equal left left\n\u201dA mug filled with steaming coffee.\u201d\nLeft\nRight\nText-Asset Alignment: The left one presents an abstract, irregular form that diverges \nsignificantly from typical balloon shapes. The right one displays an arrangement of three distinct \nspheres \u2026\n3D Plausibility: The left object exhibits deformation and lacks the expected balloon shape \u2026; \nThe right object, while stylized, is consistent and resembles balloons more closely; no clear \ndeformities or noise are visible.\nTexture-Geometry Coherency: The left model's texture seems to float and warp around the \ndistorted geometry, \u2026; The right model shows a cohesive alignment between the texture and \nballoon-like spherical shapes, ...\nTexture Details: The left model's texture appears diffused and lacks clarity, ... The right model's \ntexture exhibits vibrant and sharp features  \u2026\nGeometry Details: The left object's surfaces are irregular and lack definition, ... The right object \nhas smoother surfaces with subtle highlights that imply sharper details and contours.\nFinal answer: right right right right right\n\u201dThree vibrant balloons tied together .\u201d\nLeft\nRight\nText-Asset Alignment: The left model portrays a teddy bear with a simple but clear design, \nincluding a red bow ... The right model is more complex, with a multi-textured body and a red \nbow tie; however, it contains additional attributes not mentioned in the prompt, like clothing \u2026\n3D Plausibility: Object 1 on the left displays more natural proportions and a consistent \nstructure throughout the different views. Object 2 on the right has visible distortions, ...\nTexture-Geometry Coherency: The left model shows good congruence between the texture \nand geometry \u2026 The right model also displays alignment; \u2026 the complexity of the textures \u2026 may \nnot be as consistent with the underlying geometry in all views.\nTexture Details: \u2026 , the left model's texture appears soft but lacks fine detail, while the right\nmodel demonstrates a more intricate texture that simulates fabric and stitching on the clothing \nand a more realistic fur texture.\nGeometry Details: Object 1, \u2026 , has a smoothness that suggests slightly less geometric detail. \nObject 2, despite its issues with plausibility, presents more intricate surface details, \u2026\nFinal answer: left left left right right\n\u201dA teddy bear with a red bow.\u201d\nCorrect\nWrong\nFigure S5. Failure cases. The analysis of GPT-4V can sometimes be partially wrong. We show several typical examples for the \u201cText-\nAsset Alignment\u201d and \u201c3D Plausibility\u201d criteria.\n20\nLeft\nRight\nLeft\nRight\nText-Asset Alignment: The left one \u2026 a cabin \u2026 aligning with the \"snow-covered forest\"\ndescription. The right one \u2026 a cabin with an ambiguous environment and less visible snow.\n3D Plausibility: The left one .. a clear cabin with discernible roof and walls, appearing more \ncohesive overall; The right one has parts that are less defined, with some areas looking melted \nor incomplete.\nTexture-Geometry Coherency: The texture of the left cabin aligns well with the underlying \ngeometry, \u2026 ; The right cabin \u2026 not as precise as the left cabin.\nTexture Details: The left cabin has a sharper, more detailed texture with visible wood grains \nand a snow effect that looks realistic. The right cabin has a softer texture, lacking the same level \nof crisp detail.\nGeometry Details: The left one \u2026 more recognizable architectural elements such as windows, a \ndoor, and a chimney; The right one has less distinct features, overall appearing smoother and \nlacking the same level of detail.\nFinal answer: left left left left left\n\u201dA chair made from polished oak .\u201d\n\u201dA small, rustic cabin sits alone in a peaceful, snow-covered forest.\u201d\n3D Plausibility: The left model shows implausible distortions and uneven surfaces, while the \nright model has a consistent structure that is plausible as a chair.\nGeometry Details: The normal maps of the left \u2026 chaotic arrangement of colors with little to \nno discernible consistent pattern or structure. The right .. more orderly and structured \nappearance, with more defined shapes and patterns.\nFinal answer: right right\nLeft\nRight\n\u201dA compact, cylindrical, vintage pepper mill, with a polished, ornate brass body, slightly \nworn from use, placed beside a porcelain plate on a checkered tablecloth.\u201d\n3D Plausibility: Both models seem solid and plausible. The left one has a decorative complexity \nthat is coherent, while the right one, although simpler, also represents a consistent form. \nTexture Details: The left model shows a high level of detail  \u2026 effectively conveys the porcelain \nand brass textures suggested in the prompt. The right model has a more subdued texture \nappearance, \u2026 \nFinal answer: equal right\nLeft\nRight\nText-Asset Alignment: The left object is a table lamp with a colorful lampshade and an \nirregularly shaped stand\u2026 The right \u2026 that accurately adheres to \"casting a warm glow\". \n3D Plausibility: Object 1 appears to have a more plausible structure with fewer artifacts than \nObject 2. Object 2 demonstrates several protrusions and disjointed elements that detract from \nits plausibility as a lamp. \nTexture-Geometry Coherency: The left object shows some inconsistencies, especially at the \nbase. The right one has a coherent alignment between its texture and geometry ... \nTexture Details: The textures on the left object \u2026 are less sharp and do not convey a clear \nmaterial quality. The texture of the right object appears more detailed and realistic, with \nmaterials that reflect light akin to metal and glass, \u2026. \nGeometry Details: Object 2 exhibits a higher level of complexity and detail in the geometry of \nits components, suggesting sharper detail, especially around the lantern's details and the \nornamental aspects. \nFinal answer: right left right right right\n\u201dA lamp casting a warm glow.\u201d\nLeft\nRight\nText-Asset Alignment: The left one \u2026 do not succinctly convey the concept of a \"crying sofa\". \nThe right one \u2026 but also does not clearly represent the idea of \"crying\". Neither model has \nattributes that explicitly suggest crying.\n3D Plausibility: The left ... a high degree of warping and distortion, with elements that are \nmalformed and far from resembling a stable structure. The right object \u2026 maintains a more \nconsistent shape and structure \u2026\nTexture-Geometry Coherency: The left one appears \u2026 with no discernible patterns that relate \nto a typical sofa texture. The right one has a texture that aligns with its geometry ...\nTexture Details: The left one \u2026 lacking any realistic sofa material qualities. The right one \npresents a texture with a degree of realism, depicting shadows and highlights consistent with \nthe geometry of cushions and armrests.\nGeometry Details: The left object\u2018s geometry is extremely warped, being difficult to discern \nany clear details. The right object has a more defined structure, \u2026\nFinal answer: equal right right right right\n\u201dA crying sofa.\u201d\nFigure S6. Additional examples of the analysis by GPT-4V.\n21\nPlease help us to create a set of text prompts for text to 3D generation. Now, I will give you some more tips and requirements for the prompt creation. First, we \nwill decide the complexity and creativity:\n1. Complexity:\n- Structure Complexity: Simplicity or intricacy of the object\u2019s design\n- Number of Elements: Single or multiple subjects\n- Relationships: Interactions and relations between multiple objects\n- Environmental Complexity: Simplicity or intricacy of the scene and surroundings\n- Detail Level: From simple shapes to complex structures with fine details\n2. Creativity:\n- Novelty: Common, everyday items to unique, imaginative creations\n- Conceptual Innovation: Degree of abstract or unconventional thinking required\n- Emotional Impact: Potential to evoke emotions or thoughts\n- Storytelling: Inclusion of elements that suggest a backstory or narrative\nUpon determining these two aspects, we then turn our attention to the content of the prompt. Initially, a subject selection is conducted, followed by the decision \non further constraints or descriptions premised on this chosen subject:\n1. Subject or Object Categories:\n- Living Beings: Humans, animals, mythical creatures, insects, aquatic life\n- Plants: Trees, flowers, bushes, grass, aquatic plants\n- Buildings and Structures: Houses, skyscrapers, bridges, tunnels, castles\n- Vehicles: Cars, bikes, planes, boats, spacecraft\n- Furniture: Chairs, tables, sofas, shelves, beds\n- Electronics: Phones, computers, cameras, kitchen appliances\n- Household Items: Utensils, decor, tools, containers\n- Clothing and Accessories: Dresses, shoes, jewelry, hats, bags\n- Abstract Objects: Geometric shapes, artistic sculptures\n- Natural Elements: Fire, water, rocks, clouds\n- Food and Beverages: Fruits, vegetables, dishes, drinks\n2. Geometry Constraint:\n- Volume: Specify if the object is hollow, solid, porous, or layered.\n- Symmetry: Define if the object should be symmetrical, asymmetrical, or radially symmetrical.\n- Contours: Indicate if the contours should be smooth, jagged, irregular, or undulating.\n- Internal Structures: State if the inside should be empty, compartmentalized, or multi-layered.\n- Shape: Mention specific geometric shapes that should be part of or inspire the object, e.g., cone, cylinder, sphere.\n3. Appearance:\n- Colors: Specific colors, patterns, gradients\n- Materials: Wood, metal, glass, fabric, stone\n- Textures: Smooth, rough, furry, scaly\n- Finish: Glossy, matte, translucent, opaque\n- Size: Small, medium, large, specific dimensions\n- State: New, old, worn, pristine\n4. Status/Motion:\n- Static: Still, motionless\n- Dynamic: Moving, changing\n- Emotional State: Happy, sad, angry (for living beings or anthropomorphized objects)\n- Physical State: Broken, intact, in use\n- Interaction: Interacting with another object or environment\n5. Styles:\n- Aesthetic: Minimalistic, ornate, modern, vintage\n- Cultural: Asian, African, Western, Middle Eastern\n- Emotional: Cheerful, gloomy, energetic, calm\n- Functional: Practical, decorative, symbolic\n- Conceptual: Abstract, realistic, surrealistic, impressionistic\n6. Scene and Surroundings:\n- Environment: Indoor, outdoor, urban, rural, natural, fantastical\n- Context: Part of a larger scene, event, or story\n- Lighting: Day, night, artificial, natural, shadows, highlights\n- Weather: Sunny, rainy, cloudy, stormy (for outdoor scenes)\n- Interactions: How the subject interacts with its surroundings\n- Scale: The relative size of the object in the scene\nTry to understand the requirement first, before I provide some more information.\nUnderstood. Based on your guidelines, the process of prompt creation for text-to-3D generation involves a multi-faceted approach that combines complexity and \ncreativity considerations with specific content decisions. The goal is to formulate prompts that are detailed enough to direct the 3D generation process, while also \nbeing imaginative and evocative to encourage the creation of compelling 3D models.\nComplexity considerations help in determining the structural and environmental intricacy, the number of elements and their relationships, as well as the overall \ndetail level. Creativity involves novelty, conceptual innovation, emotional impact, and the inclusion of storytelling elements.\nWhen crafting prompts, one should choose a subject or object category and then refine the prompt with additional constraints or descriptions, such as geometry, \nappearance, status/motion, style, and scene specifics. Each of these aspects contributes to the richness and direction of the prompt, shaping the outcome of the \n3D generation process.\nIt's clear that prompts can range from very simple directives (like \"A silver spoon\") to highly complex and storied scenarios (\"A timeworn leather journal with \ncryptic inscriptions lying on the altar of an abandoned temple, under the eerie glow of the full moon\").\nWith these frameworks in mind, you can create prompts that are not only technically precise but also deeply imaginative, leading to a diverse portfolio of 3D \nobjects and scenes. Now, I'm ready for any further information or the next steps you have regarding the prompt creation.\nFigure S7. Examples of guiding GPT for automatic prompt generation. (Part 1/2)\n22\nI will first show you some examples:\n    \"a bald eagle carved out of wood\",\n\"a banana peeling itself\",\n\"a beagle in a detective's outfit\",\n\"a beautiful dress made out of fruit on a mannequin. Studio lighting high quality high resolution\",\n\"a beautiful dress made out of garbage bags on a mannequin. Studio lighting high quality high resolution\",\n    \"a cute steampunk elephant\",\n\"a dachsund dressed up in a hotdog costume\",\n\"a delicious hamburger\",\n\"a dragon-cat hybrid\",\n\"a DSLR photo of a baby dragon drinking boba\",\n\"a DSLR photo of a baby dragon hatching out of a stone egg\",\n\"a DSLR photo of a baby grand piano viewed from far away\",\n\"a DSLR photo of a bagel filled with cream cheese and lox\",\n\"a DSLR photo of a bald eagle\",\n\"a DSLR photo of a barbecue grill cooking sausages and burger patties\",\n  \u2026\nz\nThank you for the examples. I've reviewed the styles. How can I assist you next?\nPlease create 50 common prompts, each within 10 words, with a diversity of complexity.\nCertainly! Here are 50 diverse prompts that vary in complexity.\n1.\nA sunflower field at sunset.\n2.\nA pair of worn hiking boots.\n3.\nA cat sleeping in a sunny window.\n4.\nA busy urban street scene.\n5.\nA portrait of a woman with green eyes.\n6.\nA sushi platter with a variety of rolls.\n7.\nAn old library with tall bookshelves.\n8.\nA vintage convertible on a coastal road.\n9.\nA snowman wearing a top hat and scarf.\n10. A child's first bicycle ride.\n11. A bowl of fresh summer berries.\n12. A cozy cabin in the snow.\n13. A jazz band playing in a smoky bar.\n14. A knight's armor on display.\n15. A traditional Venetian mask.\n16. An antique pocket watch with intricate engravings.\n17. A colorful hot air balloon in flight.\n18. A chef preparing a flamb\u00e9 dish.\n19. A mermaid lounging on a rock.\n20. A medieval castle at dawn.\n21. \u2026\nPlease create 50 prompts about living creatures, with a diversity of creativity.\nCommon Living Creature Prompts:\n1.\nSleeping tabby cat curled in a sunbeam.\n2.\nGoldfish swimming in a round bowl.\n3.\nGreen tree frog clinging to a rain-soaked window.\n4.\nOrange monarch butterfly resting on a dandelion.\n5.\nSpotted ladybug crawling on a green leaf.\n6.\nWhite-tailed deer standing in a meadow at dawn.\n7.\nClownfish peeking out from sea anemone tendrils.\n8.\nBrown rabbit nibbling on a clover leaf.\n9.\nBlack and white dairy cow in a grassy field.\n10. Bumblebee landing on a sunflower\u2019s center.\n11. Red fox prowling through a snowy forest.\n12. \u2026\nUnusual and Creative Living Creature Prompts:\n1.\nChameleon with color-changing spots forming digital numbers.\n2.\nCaterpillar with a keyboard pattern on its back.\n3.\nSwan with feathers resembling soft, white origami folds.\n4.\nOctopus with transparent tentacles revealing gears inside.\n5.\nGecko with suction pads that glow like neon lights.\n6.\nFlamingo balancing on a sphere instead of standing in water.\n7.\nFrog with a translucent skin displaying a mechanical heart beating.\n8.\nKoala with a patchwork of velvety moss instead of fur.\n9.\nSeahorse with a carousel-inspired saddle and fin designs.\n10. Tortoise with a shell that looks like stained glass.\n11. Jellyfish with bioluminescent tentacles shaped like lightning bolts.\n12. \u2026\nFigure S8. Examples of guiding GPT for automatic prompt generation. (Part 2/2)\n23\n"
  },
  {
    "title": "DiarizationLM: Speaker Diarization Post-Processing with Large Language Models",
    "link": "https://arxiv.org/pdf/2401.03506.pdf",
    "upvote": "12",
    "text": "DIARIZATIONLM:\nSPEAKER\nDIARIZATION\nPOST-\nPROCESSING WITH LARGE LANGUAGE MODELS\nQuan Wang\u22c6\nYiling Huang\u22c6\nGuanlong Zhao\u22c6\nEvan Clark\nWei Xia\nHank Liao\nGoogle LLC, USA\n\u22c6Equal contribution\nquanw@google.com\nABSTRACT\nIn this paper, we introduce DiarizationLM, a framework to leverage large language\nmodels (LLM) to post-process the outputs from a speaker diarization system.\nVarious goals can be achieved with the proposed framework, such as improving\nthe readability of the diarized transcript, or reducing the word diarization error\nrate (WDER). In this framework, the outputs of the automatic speech recognition\n(ASR) and speaker diarization systems are represented as a compact textual format,\nwhich is included in the prompt to an optionally finetuned LLM. The outputs of the\nLLM can be used as the refined diarization results with the desired enhancement.\nAs a post-processing step, this framework can be easily applied to any off-the-shelf\nASR and speaker diarization systems without retraining existing components. Our\nexperiments show that a finetuned PaLM 2-S model can reduce the WDER by\nrel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the\nCallhome English dataset1.\n1\nINTRODUCTION\nSpeaker diarization is the task of partitioning speech into homogeneous segments according to speaker\nidentities, answering the question \u201cwho spoken when\u201d [1, 2]. Typical speaker diarization systems can\nbe roughly categorized into two groups: modularized systems and end-to-end systems. A modularized\nspeaker diarization system usually consists of multiple separately trained components including voice\nactivity detection (VAD) [3, 4, 5, 6], speaker turn detection [7, 8], speaker encoder [9, 10, 11], and a\nclustering algorithm, which can be either unsupervised [12, 13, 14, 15, 16, 17] or supervised [18, 19].\nEnd-to-end systems , on the other hand, directly optimize the entire system on diarization errors by\nintroducing a permutation invariant loss function [20, 21, 22, 23].\nIn many real world applications such as meeting summarization, call center analysis, mobile recorder\napps [24], and video captioning, knowing \u201cwho spoke when\u201d is not sufficient. Speaker labels are more\ninterpretable and meaningful when they are associated with speech transcripts. Various solutions\nhave been proposed to directly address the problem of \u201cwho spoke what\u201d, including jointly training\nspeech recognition and speaker diarization [25], speaker-attributed automatic speech recognition\n(SA-ASR) [26, 27, 28, 29], target speaker automatic speech recognition (TS-ASR) [30, 31, 32, 33]\nand word-level end-to-end neural speaker diarization [34].\nIn practice, however, most production speech systems still consist of separately trained ASR models\nand speaker diarization models, with various considerations including:\n1. Modularized development and deployment: ASR and speaker diarization systems are usually\ntrained on different datasets, and potentially using different modeling framework, by different\nresearch teams.\n2. Potential quality regression on ASR: ASR has many more use cases than speaker diarization.\nJoint modeling of ASR and speaker diarization usually has worse Word Error Rates (WER)\nthan ASR-only models, thus is not acceptable in many applications.\n1This project is currently a work in progress. We may add new experiment results to this paper soon.\n1\narXiv:2401.03506v4  [eess.AS]  6 Feb 2024\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\ngood\nmorning\nhow\nare\nyou\nspk1\nspk2\nASR outputs\nSpeaker diarization \noutputs\n(a)\ngood\nmorning\nhow\nare\nspk1\nspk2\nASR outputs\nSpeaker diarization \noutputs\nspk1\nyou\n(b)\nFigure 1: The orchestration module associates each word from the ASR transcript with a speaker label\nfrom the speaker diarization outputs. (a) In this example, all words are associated with the correct\nspeaker labels (green arrows). The words \u201cgood\u201d, \u201cmorning\u201d, and \u201care\u201d and \u201cyou\u201d are associated\nwith the only speaker label that overlap with them. The word \u201chow\u201d overlaps with both spk1 and\nspk2, but has bigger overlaps with spk2, thus is associated with spk2. The word \u201cyou\u201d does not\noverlap with any speaker, but is closest to spk2, thus is associated with spk2. (b) In this example, two\nwords are associated with wrong speaker labels (red arrows) due to inconsistent timing information\nfrom the two systems. The word \u201chow\u201d is mistakenly associated with spk1, since spk1 has more\noverlap with this word than spk2. The word \u201cyou\u201d is mistakenly associated with spk1, since spk1 is\ncloser to this word than spk2.\n3. Flexibility: Combining separately trained ASR models and speaker diarization models is a\nvery flexible solution. As long as the ASR model provides word timing information, it can\nbe combined with almost any speaker diarization model, either unsupervised or supervised,\neither modularized or end-to-end trained.\nWe refer to the combination of ASR transcripts and speaker diarization results as an orchestration\nmodule (in some other work [35], this process is called \u201creconciliation\u201d). In this module, each word\nfrom the ASR transcript is associated with a speaker label. A typical orchestration algorithm works\nas follows: (1) If the word segment overlaps with at least one speaker segment, then this word is\nassociated with the speaker that has the biggest temporal overlap with this word; (2) otherwise if this\nword segment does not overlap with any speaker segment, then it is associated with the speaker that\nhas the smallest temporal distance to this word based on the segment boundaries. This orchestration\nalgorithm is illustrated in Fig. 1a.\nHowever, since ASR and speaker diarization are separately trained with usually different training\ndatasets and modeling approaches, the timing information from these two systems can be inconsistent,\nresulting in word diarization errors, as demonstrated with the example in Fig. 1b. Specifically, modern\nASR models are usually trained end-to-end without using the ground truth timing information, and\nthe word timing is inferred from the probability lattice of the decoder, which could be inaccurate.\nIn many cases, such errors can usually be fixed by leveraging semantic information from the ASR\ntranscripts. Take Fig. 1 as an example, simply by looking at the textual transcript \u201cgood morning how\nare you\u201d, if we know it consists of two speakers, we can easily tell which word comes from which\nspeaker confidently without using any acoustic speaker diarization system. In practice, diarization\nerrors can be much more complicated than the simple example in Fig. 1. To handle such cases, we\npropose DiarizationLM, a framework to post-process the orchestrated ASR and speaker diarization\noutputs with a large language model (LLM). While the experiments performed in this paper mainly\nfocus on reducing word diarization errors using LLM, we also show examples how LLM can be used\nfor other purposes such as autofilling speaker names, autofilling speaker roles, or even completely\nreplacing the orchestration module in Appendix A.\n2\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n2\nMOTIVATING EXAMPLE\nHere is a small example where we construct a prompt (in blue) which consists of a brief instruction,\nand the diarization transcript with several errors. The prompt is sent to Google Bard [36] via its Web\nUI2. We can see that the completion generated by LLM (in orange) correctly moved the error words\nto the right speakers. This example can be easily reproduced with many off-the-shelf commercial or\nopen source instruction-finetuned LLMs, such as Google Bard [36] or OpenAI ChatGPT [37].\nPrompt:\nIn the speaker diarization transcript below, some words are potentially misplaced. Please correct\nthose words and move them to the right speaker. Directly show the corrected transcript without\nexplaining what changes were made or why you made those changes.\n<spk:1> Good morning Patrick, how\n<spk:2> are you? Good, good. How are you Tom? Pretty\n<spk:1> good. Going to work?\n<spk:2> Yes. Busy day. How are your kids? Do they go\n<spk:1> to school? Oh they are too young for that. I sent them to daycare earlier\n<spk:2> today. Oh yeah I forgot about that.\nCompletion:\n<spk:1> Good morning Patrick, how are you?\n<spk:2> Good, good. How are you Tom?\n<spk:1> Pretty good. Going to work?\n<spk:2> Yes. Busy day. How are your kids? Do they go to school?\n<spk:1> Oh, they are too young for that. I sent them to daycare earlier today.\n<spk:2> Oh yeah, I forgot about that.\nApart from correcting diarization errors, we also found several other diarization capabilities of\nLLMs, such as autofilling speaker names, autofilling speaker roles, or even completely replacing the\norchestration module. Examples of these capabilities are shown in Appendix A. However, due to lack\nof relevant training and testing datasets, these capabilities are not thoroughly studied or evaluated in\nthis paper. Here we focus on LLM\u2019s capability of reducing diarization errors.\n3\nDIARIZATIONLM\n3.1\nSYSTEM OVERVIEW\nWe illustrate the DiarizationLM framework in Fig. 2. In this framework, the ASR and speaker\ndiarization systems are frozen, and their outputs are processed by the orchestration module to\nassociate a speaker label with each recognized word. The orchestrated diarization outputs are\nprocessed by a prompt builder module, which creates a compact textual representation of the diarized\ntranscript, segment it into shorter versions to fit the LLM input size limit, and apply prompt prefix\nand suffix. The prompts are then sent to a finetuned LLM, and the completions generated by the LLM\nwill be handled by a completion parser module, which truncates undesired outputs from the LLM,\ncombines the completions of multiple segments, and apply a transform (see Section 3.4) to preserve\nthe original transcripts of the ASR model.\n3.2\nPROMPT BUILDER\nThe output of the orchestration module is two sequences of equal length: a sequence of words, and a\nsequence of speaker labels. To fit it into a prompt, we use a compact textual representation, where\nspeaker tokens are only inserted in the beginning of the transcript, or when the speaker has changed.\nBelow is an example:\n2We used an internal version of Bard that is based on a larger model and supports more tokens than the public\nversion.\n3\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nOrchestration \nModule\nDiarizationLM\nPrompt \nBuilder\nCompletion \nParser\nASR\nSpeaker \nDiarization\nLLM\nWords with \ntiming info\nSpeaker labels \nwith timing info\nWords with \nspeaker labels\n\ud83d\udd25\n\u2744\n\u2744\nFigure 2: Diagram of the proposed DiarizationLM framework.\nWord sequence:\n[\"good\", \"morning\", \"how\", \"are\", \"you\"]\nSpeaker sequence:\n[1, 1, 2, 2, 2]\nText representation:\n\"<spk:1> good morning <spk:2> how are you\"\nSince most LLMs have an input length limit, the text representation of an entire utterance may not\nfit this limit. In such cases, we recursively binary partition the word and speaker sequences in the\nmiddle, until all segments fit the the input length limit.\nWe also apply prefix and suffix to each prompt. The prefix is usually an instruction describing the\ntask for the LLM to perform, and the suffix is a sequence of tokens to indicate the end of the prompt.\n3.3\nCOMPLETION PARSER\nEach prompt from the prompt builder will be sent to the finetuned LLM, which will generate a text\ncompletion for this prompt. First of all, we need to truncate any undesired outputs from the LLM.\nFor example, during the LLM finetuning, each completion may have a suffix to indicate the end of\nthe completion. Thus the suffix and any text generated after the suffix should be truncated from the\noriginal completion.\nAfter the truncation, we need to convert the text representation of the completion back to the word\nsequence and the speaker sequence format. If the text representation does not start with a speaker\ntoken, we either use the last speaker from the previous segment, or just use speaker 1 if it is the first\nsegment.\nNext, we concatenate the word sequences and speaker sequences from all segments. However, the\nresulting concatenated word sequence may not be identical to the original word sequence from the\nASR model due to modifications by LLM. This is undesired and may hurt word error rate. Thus\nhere we need an algorithm to transfer the speaker labels from the concatenated speaker sequence to\nthe original word sequence from the ASR model. We will introduce this algorithm in the following\nsection.\n3.4\nTRANSCRIPT-PRESERVING SPEAKER TRANSFER\nHere we describe an algorithm called Transcript-Preserving Speaker Transfer (TPST), which will\nbe used in several places in our proposed framework, including training data preparation and the\ncompletion parser module.\nAssume we have two sets of diarized transcript, referred to as \u201csource\u201d and \u201ctarget\u201d, each represented\nby two sequences of the same length: a sequence of words, and a sequence of speaker labels. The\npurpose of TPST is to transfer the speaker labels from the source sequences to the target sequences,\nsuch that:\n1. The transfered speaker label sequence has a 1-to-1 association with the target word sequence.\n2. The transfered speaker labels are more consistent with the source speaker labels.\nAs an example, the concatenated word sequence from the completion parser module may not be\nidentical to the original word sequence from the ASR model. Thus we can treat the completion\n4\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nsequences as the source, and the original sequences from the orchestration module as the target, and\ntransfer the speaker labels. Finally, the DiarizationLM outputs will be the original word sequence,\nassociated with the transfered speaker label sequence.\nThe detailed TPST algorithm is described in Algorithm 1. An implementation is open sourced on\nGitHub 3.\nAlgorithm 1 The transcript-preserving speaker transfer (TPST) algorithm.\ninputs\nSource word sequence of length N: wsrc = (wsrc\n1\n, \u00b7 \u00b7 \u00b7 , wsrc\nN )\nSource speaker sequence of length N: ssrc = (ssrc\n1 , \u00b7 \u00b7 \u00b7 , ssrc\nN )\nTarget word sequence of length M: wtgt = (wtgt\n1 , \u00b7 \u00b7 \u00b7 , wtgt\nM )\nTarget speaker sequence of length M: stgt = (stgt\n1 , \u00b7 \u00b7 \u00b7 , stgt\nM )\noutputs\nTransfered speaker sequence of length M: stra = (stra\n1 , \u00b7 \u00b7 \u00b7 , stra\nM )\n1: procedure TPST(wsrc, ssrc, wtgt, stgt)\n2:\nAlign wsrc to wtgt with the Levenshtein algorithm [38], resulting in a transform falign(\u00b7)\n3:\nsali \u2190 falign(ssrc)\n\u25b7 sali is a speaker sequence of length M, and may contain blank\nspeakers \u2205 due to insertion errors in the alignment\n4:\nK \u2190 max{max(sali), max(stgt)}\n\u25b7 the maximal number of speakers in sali and stgt\n5:\nInitialize a cost matrix C \u2208 RK\u00d7K\n6:\nfor 1 \u2264 i \u2264 K and 1 \u2264 j \u2264 K do\n7:\nCi,j \u2190 P\n1\u2264m\u2264M \u03b4(sali\nm = i and stgt\nm = j)\n8:\nend for\n9:\nSolve the assignment problem with cost matrix C using the Hungarian algorithm [39],\nresulting in a transform fassign(\u00b7)\n\u25b7 handle speaker permutations\n10:\nfor 1 \u2264 m \u2264 M do\n11:\nif sali\nm \u0338= \u2205 then\n12:\nstra\nm \u2190 fassign(sali\nm )\n\u25b7 transfer the speakers from the source\n13:\nelse\n14:\nstra\nm \u2190 stgt\nm\n\u25b7 preserve the target speaker if source speaker is unavailable\n15:\nend if\n16:\nend for\n17: end procedure\nBelow we show a simple example of the inputs and output of the TPST algorithm:\nSource words:\nhello good morning hi how are you pretty good\nSource speakers:\n1 1 1 2 2 2 2 1 1\nTarget words:\nhello morning hi hey are you be good\nTarget speakers:\n1 2 2 2 1 1 2 1\nTransfered speakers:\n1 1 2 2 2 2 1 1\n3.5\nLLM FINETUNING\nAlthough the examples shown in Section 2 and Appendix A were using off-the-shelf Web APIs of\ncommercial LLMs, finetuning the LLM specifically on the speaker diarization task is still required if\nwe need to:\n1. Reduce errors of a specific speaker diarization system;\n2. Handle more complicated errors;\n3. Keep ASR transcripts unmodified as much as possible from the LLM outputs;\n3https://github.com/google/speaker-id/tree/master/DiarizationLM\n5\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n4. Avoid undesired leading or tailing text from the generated completions, such as \u201cHere is the\ncorrected transcript\u201d or \u201cWe corrected the speakers for these words\u201d;\n5. Use smaller and cheaper LLMs.\nTo finetune the LLM, we build our training data as a collection of prompt-completion pairs. First,\nfor each utterance, we run the ASR model and the speaker diarization system on it, and apply the\norchestration module as shown in Fig. 2. This will produce the hypothesis word sequence whyp\nand hypothesis speaker sequence shyp. From the ground truth annotations of this utterance, we\nbuild the reference word sequence wref and the reference speaker sequence sref. Given these four\nsequences, we can build the prompts and completions in our training data with three different flavors,\nas introduced below.\n3.5.1\nFLAVOR 1: HYP2ORA\nThe first flavor is named hypothesis-to-oracle, or simply hyp2ora. In this flavor, we apply the\nTranscript-Preserving Speaker Transfer algorithm from Section 3.4 by treating reference sequences\nas source and hypothesis sequences as target:\nsora = TPST(wref, sref, whyp, shyp),\n(1)\nwhere the output sora is the oracle hypothesis speakers transfered from the reference sequences.\nWith sora, the prompts and completions in our training data are created as below:\n\u2022 Prompts: The text representation of whyp and shyp, with segmentation, and optionally prefix\nand suffix.\n\u2022 Completions: The text representation of whyp and sora, with segmentation, and optionally\nsuffix.\n3.5.2\nFLAVOR 2: DEG2REF\nThe second flavor is named degraded-to-reference, or simply deg2ref. In this flavor, we apply the\nTranscript-Preserving Speaker Transfer algorithm from Section 3.4 by treating hypothesis sequences\nas source and reference sequences as target:\nsdeg = TPST(whyp, shyp, wref, sref),\n(2)\nwhere the output sdeg is the degraded reference speakers transfered from the hypothesis sequences.\nWith sdeg, the prompts and completions in our training data are created as below:\n\u2022 Prompts: The text representation of wref and sdeg, with segmentation, and optionally prefix\nand suffix.\n\u2022 Completions: The text representation of wref and sref, with segmentation, and optionally\nsuffix.\n3.5.3\nFLAVOR 3: MIXED\nThe third flavor named mixed is simply the union of the prompts and completions from the previous\ntwo flavors. When building training batches, prompt-completion pairs from the two flavors are\ninterleaved.\nNote that for all three flavors, it is critical for the prompt and completion to use the same word\nsequence with different speaker sequences. This helps the LLM to focus on correcting the speaker\nlabels without modifying the ASR transcripts.\n4\nEXPERIMENTS\n4.1\nDATASETS\nTo finetune the LLM, we use the training subset of the Fisher corpus [40], which consists of 1,920\nhours of 11,527 conversations. The same train-test split of the Fisher dataset has been used in many\nprevious works [8, 17, 35, 41]\n6\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nFor evaluation, we use the testing subset of the Fisher corpus [40], as well as the testing subset\nof Callhome American English data [42]. The Fisher testing subset consists of 28.7 hours of\n172 conversations4. The Callhome American English testing subset consists of 1.7 hours of 20\nconversations. Both datasets are in the telephone speech domain, and all conversations have 2\nspeakers.\n4.2\nMETRICS\nTo evaluate the diarization performance, we use two metrics: the Word Diarization Error Rate\n(WDER) [25] and the concatenated minimum-permutation word error rate (cpWER) [43]. To briefly\nrecap, WDER is defined as:\nWDER = SIS + CIS\nS + C\n,\n(3)\nwhere,\n1. SIS is the number of ASR Substitutions with Incorrect Speaker tokens.\n2. CIS is the number of Correct ASR words with Incorrect Speaker tokens.\n3. S is the number of ASR substitutions.\n4. C is the number of Correct ASR words.\nAnd cpWER is computed as follows:\n1. Concatenate all transcripts of each speaker for both reference and hypothesis.\n2. Compute the WER between the reference and all possible speaker permutations of the\nhypothesis.\n3. Pick the lowest WER among all these permutations, which is assumed to be the best\npermutation.\nAll three metrics reported in this paper (WER, WDER, and cpWER) are macro metrics, i.e. both\nnumerators and denominators are aggregated on the entire dataset.\n4.3\nMODELS\nFor the ASR model in Fig. 2, we use a universal speech model (USM) [44] with 600 million\nparameters trained with the RNN-T loss [45]. For the speaker diarization model in Fig. 2, we use\nthe turn-to-diarize system [7] with a multi-stage clustering setup [17] in our experiments, which is\ncapable of diarizing hours of audio recordings in real time on a mobile device [24]. The number\nof speakers is unknown (from 1 to \u221e) to the speaker diarization system in all of our experiments.\nHowever, we would like to point out that the proposed framework is very generic and should work\nwith other ASR or speaker diarization systems as well, such as variants of end-to-end speaker\ndiarization models [20, 21, 22, 23].\nFor the LLM in Fig. 2, we experiment with the PaLM 2-S model (\u201ctext-bison\u201d model in Google\nCloud API) and the PaLM 2-L model (\u201ctext-unicorn\u201d model in Google Cloud API) [46]. We use\nthe PaLM 2-S model as our foundation model, and finetune it on the dataset described in Section 4.1\nwith data processing steps described in Section 3.5. This model uses a sentence piece model (SPM)\nof 256k tokens as its tokenizer [47]. During finetuning, we limit the LLM input size by 4,096 tokens,\nand segment our training and testing data accordingly. The PaLM 2-L model will only be used for\nzero-shot and one-shot experiments, as described in Section 4.4.\nIn our prompt builder module, we use an empty prompt prefix, and a 5-character prompt suffix\n\u201c\n-->\n\u201d (note the two spaces around the arrow). For the completions in our training data, we\nuse a 6-character completion suffix \u201c\n[eod] \u201d (short for \u201cend of document\u201d; note the leading\nspace). After processing the training data with the prompt builder module, we result in 13,430\nprompt-completion pairs for training in total. The average length of a prompt is 2,371 SPM tokens,\n4https://github.com/google/speaker-id/blob/master/publications/ScdLoss/\neval/fisher.txt\n7\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nand the average length of a completion is 2,329 tokens. The LLM is trained for 1,200 steps with a\nbatch size of 16.\n4.4\nZERO-SHOT AND ONE-SHOT BASELINES\nApart from finetuning the PaLM 2-S model on the speaker diarization task, we also experiment with\ndirectly using the PaLM 2-S and PaLM 2-L models on the speaker diarization task without finetuning.\nThis is more similar to the example we demonstrated in Section 2.\nFor the zero-shot setup, we use a prompt prefix that contains an instruction describing the task, as\nshown below.\nPrompt prefix for zero-shot:\nIn the speaker diarization transcript below, some words are potentially misplaced.\nPlease correct those words and move them to the right speaker.\nDirectly show the corrected transcript without explaining what changes were made or why you\nmade those changes.\\n\nFor the one-shot setup, the prompt prefix contains both the instruction describing the task, and also a\nsmall example, as shown below.\nPrompt prefix for one-shot:\nIn the speaker diarization transcript below, some words are potentially misplaced. Please correct\nthose words and move them to the right speaker. For example, given this input transcript,\n<spk:1> How are you doing today? I <spk:2> am doing very well. How was everything at the\n<spk:1> party? Oh, the party? It was awesome. We had lots of fun. Good <spk:2> to hear!\nThe correct output transcript should be:\n<spk:1> How are you doing today? <spk:2> I am doing very well. How was everything at the\nparty? <spk:1> Oh, the party? It was awesome. We had lots of fun. <spk:2> Good to hear!\nNow, please correct the transcript below.\\n\n4.5\nEVALUATION RESULTS\nIn Table 1, we show the evaluation results of the USM + turn-to-diarize baseline together with the\noutputs post-processed by DiarizationLM. We report results for zero-shot, one-shot, and finetuning\non the diarization task with three different flavors.\nFor zero-shot and one-shot experiments with PaLM 2-S, we observe significantly worse WDER and\ncpWER performance compared with the baseline system, indicating the PaLM 2-S foundation model\ndoes not offer speaker diarization capabilities without finetuning. Zero-shot experiment with PaLM\n2-L model also shows bad performance, while one-shot experiment with PaLM 2-L model is much\nbetter, but still worse than the baseline system. Our results indicate that the PaLM 2-L model with\none-shot is able to improve speaker diarization in relatively simple cases as shown in Section 2 and\nAppendix A. However, real world applications can be much more complicated with errors from both\nthe ASR system and the speaker diarization system. In such cases, even with one-shot, LLM can still\nintroduce even more errors to the results if not finetuned specifically on the speaker diarization task.\nOn both datasets, we observe big improvement of both WDER and cpWER with any of the three\nfinetuning flavors. Interesting, the biggest improvement is observed with the hyp2ora flavor, while\nthe smallest improvement is observed with the deg2ref flavor. Specifically for hyp2ora, we see a\nrel. 55.5% improvement of WDER after post-processing with DiarizationLM on the Fisher testing\nset. Even if we did not use any Callhome data during the LLM finetuning, we see a rel. 44.9%\nimprovement of WDER on the Callhome testing set. The WER of the USM on the two testing sets\n8\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nTable 1: Evaluation results of the USM + turn-to-diarize baseline system and the results post-\nprocessed by DiarizationLM. For DiarizationLM, we experiment with PaLM 2 foundation models\nwith and without finetuning on the diarization task. WERs are the same for all systems due to TPST.\nAll numbers are percentages.\nSystem\nFisher testing set\nCallhome testing set\nWER\nWDER\ncpWER\nWER\nWDER\ncpWER\nUSM + turn-to-diarize baseline\n15.48\n5.32\n21.19\n15.36\n7.72\n24.39\n+ PaLM 2-S zero-shot\n-\n11.96\n30.19\n-\n12.26\n30.60\n+ PaLM 2-S one-shot\n-\n16.58\n38.03\n-\n14.50\n34.32\n+ PaLM 2-L zero-shot\n-\n11.36\n31.78\n-\n13.29\n34.30\n+ PaLM 2-L one-shot\n-\n5.94\n22.21\n-\n7.95\n24.67\n+ PaLM 2-S finetuned (hyp2ora flavor)\n-\n2.37\n16.93\n-\n4.25\n20.22\n+ PaLM 2-S finetuned (deg2ref flavor)\n-\n3.94\n18.55\n-\n5.33\n21.47\n+ PaLM 2-S finetuned (mixed flavor)\n-\n2.41\n16.94\n-\n4.76\n20.84\nTable 2: Evaluation results of the turn-to-diarize baseline system with reference ASR transcript\n(assuming WER=0%) and the results post-processed by DiarizationLM. For DiarizationLM, we\nexperiment with PaLM 2 foundation models with and without finetuning on the diarization task. All\nnumbers are percentages.\nSystem\nFisher testing set\nCallhome testing set\nWDER\ncpWER\nWDER\ncpWER\nReference + turn-to-diarize baseline\n2.81\n5.19\n3.74\n6.82\n+ PaLM 2-S zero-shot\n7.50\n12.70\n7.29\n12.79\n+ PaLM 2-S one-shot\n10.92\n19.16\n12.79\n21.65\n+ PaLM 2-L zero-shot\n8.69\n16.85\n11.67\n22.87\n+ PaLM 2-L one-shot\n3.23\n5.99\n3.76\n6.95\n+ PaLM 2-S finetuned\n1.18\n2.21\n1.49\n2.66\nare relatively high due to domain mismatch and suboptimal annotation quality of the ground truth.\nHowever, this also demonstrated that the DiarizationLM solution provides consistent quality gains\neven with out-of-domain ASR and speaker diarization models.\nTo further demonstrate this, in Table 2, we show the results of a similar setup, but we replace the\nUSM-based ASR model directly by the ground truth ASR transcripts from the testing sets. For these\nexperiments, we will have WER=0%, and the hyp2ora and deg2ref flavors will be equivalent. From\nthe table, we can still see big improvements of WDER after post-processing the diarization results by\nthe same DiarizationLM model (i.e. deg2ref flavor in Table 1).\n4.6\nCASE STUDIES\nBased on the results from Table 1, we also present example cases from the Fisher and Callhome\ntesting sets where we see big improvements of WDER in Table 3 and Table 4, respectively. From\nthese examples, we are seeing multiple patterns of corrections:\n\u2022 DiarizationLM make corrections where different parts of sentence are moved to the same\nspeaker, e.g. \u201cit\u2019s more of\u201d and \u201cit\u2019ll be warm\u201d in fe_03_07146 from Table 3. This is\nconsistent with our initial observations as demonstrated in Section 2.\n\u2022 DiarizationLM can merge short speaker turns due to disfluency, such as \u201cyeah yeah\u201d and \u201ci\ni hear i hear \u201d in fe_03_11159 from Table. 3. Diarization errors from disfluency usually\nattribute to low quality speaker embeddings extracted from very short speaker turn segments.\n\u2022 DiarizationLM can also detect speaker turns due to interruptions, such as \u201coh all right\u201d in\nfe_03_11210 from Table 3, and \u201coh my\u201d in en_6408 from Table 4.\nWe also look into why zero-shot and one-shot experiments in Table 1 produced worse results than\nthe baseline system. We found that without finetuning on the speaker diarization tasks, zero-shot\n9\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nand one-shot outputs from the LLM often delete big chunks of hypothesis text from the prompt.\nFinetuning the LLM is critical to avoid such undesired deletions. A few zero-shot examples with the\nPaLM 2-S model from the Fisher testing set were shown in Table 5.\n5\nDISCUSSION AND FUTURE WORK\nThe experiments in Section 4 have shown very promising results where LLMs can significantly\nreduce speaker diarization errors. However, we also admit the limitations of these experiments. First\nof all, the training and testing data from the experiments are all based on the telephone speech domain,\nall with exactly 2 speakers. An important future work would be to include more diverse datasets to\nfinetune the LLM, and evaluate its performance across different domains with unknown number of\nspeakers.\nIn Appendix A, we have demonstrated other diarization capabilities of LLMs. However, due to lack\nof relevant datasets, we haven\u2019t been able to thoroughly evaluate these capabilities. One interesting\nfuture work would be to collect datasets of these tasks and evaluate how LLM performs.\nAnother research direction would be to compare different LLMs, in different size variants on the\nspeaker diarization task. Specifically, the performance will likely be even better if we finetune larger\nmodels such as PaLM 2-M or PaLM 2-L. It would also be interesting to reproduce the experiments\nwith other speaker diarization systems such as EEND [20] or WEEND [34].\nLastly, as PaLM 2 models are multilingual [46], the DiarizationLM framework can naturally apply\nto speaker diarization tasks in other languages. It would be helpful to evaluate how DiarizationLM\nperforms on speaker diarization datasets in other languages than English.\n6\nRELATED WORK\n6.1\nSPEAKER DIARIZATION POST-PROCESSING\nIn the context of conventional speaker diarization, \u201cpost-processing\u201d usually refers to a stage where\nthe clustering results are refined with signals from other sources or systems. An early post-processing\napproach was known as \u201cresegmentation\u201d, where the Gaussian mixture models (GMMs) are estimated\nfor each speaker with the Baum-Welch algorithm, and a Viterbi algorithm is used to re-annotate\nthe speakers with the GMMs [48]. Later in [49], the authors proposed to use a neural network for\nresegmentation, with an additional class for non-speech. In [50], the authors proposed DiaCorrect,\na method inspired by error correction techniques in ASR. DiaCorrect uses parallel convolutional\nencoders for the speakers from the initial diarization results and a transformer based decoder to\nproduce corrected diarization results. One major difference in our proposed framework is that\nwe leverage semantic information to refine the diarization results on a word level, while these\nresegmentation approaches are only based on acoustic information and perform at cluster level.\nAnother type of post-processing is to combine the outputs of multiple speaker diarization systems,\ne.g. via majority voting [51], speaker matching [52], or both [53]. More recently in [16], the authors\nproposed to perform speaker diarization on different temporal scales, and combine their outputs\nvia 1-D convolutional neural networks. In [54], the authors proposed to use end-to-end speaker\ndiarization as a post-processing step for initial speaker diarization results of a clustering-based system.\nOur proposed framework is generic such that it can apply to either the results of a single speaker\ndiarization system, or to the combined results of multiple speaker diarization systems.\n6.2\nSPEAKER DIARIZATION WITH SEMANTIC INFORMATION\nApart from the joint ASR and speaker diarization models discussed in Section 1, researchers have also\nstudied various approaches of integrating semantic information into conventional speaker diarization\nsystems. Some of the benefits of DiarizationLM may also be achieved with non-LLM methods.\nThe most common approach to leverage semantic information is to use ASR word alignments to\nrefine the voice activity detection and initial segmentation [55]. A variant of this approach is to\nbuild a speaker turn detection model and segment by speaker turns [56]. In [57], a Gated Recurrent\nUnits (GRUs) [58] based speaker turn probability estimator is trained on top of word embeddings\n10\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nTable 3: Example cases from the Fisher testing set where we see big absolute WDER reduction (\u2206\nWDER) with DiarizationLM (deg2ref flavor).\nUtterance\nBefore DiarizationLM\nAfter DiarizationLM\nfe_03_07146\n(\u2206 WDER\n=8.80%)\n...\n<spk:3> it\u2019s it\u2019s\n<spk:1> more of summer always like you\nknow we never experience a bit cold over\nhere\n<spk:4> usually it\u2019ll\n<spk:1> be warm or like very hot in summer\nyeah and\n<spk:3> extremely hot yeah with high humid-\nity my humidity is pretty\n<spk:1> much high because i stay close to\nthe sea coast over here\n<spk:3> yeah\n<spk:1> so\n<spk:3> that makes you live houston is it like\nhouston where you live yeah i i i live\n<spk:1> in houston\n...\n...\n<spk:1> it\u2019s it\u2019s more of summer always like\nyou know we never experience a bit cold over\nhere usually it\u2019ll be warm or like very hot in\nsummer\n<spk:2> yeah and extremely hot yeah with\nhigh humidity my\n<spk:1> humidity is pretty much high be-\ncause i stay close to the sea coast over here\n<spk:2> yeah so that makes you live houston\nis it like houston where you live\n<spk:1> yeah i i i live in houston\n...\nfe_03_06816\n(\u2206 WDER\n=6.61%)\n...\n<spk:3> uhuh\n<spk:2> did you see the the woman golfer\nthat was on this the one\n<spk:1> monica yeah yeah\n<spk:2> what\u2019s her name monica stone yeah\nmhm she she\n<spk:1> blew out she fell out of that tourna-\nment but i didn\u2019t think she\u2019d do it she she\u2019s\ngirls can\u2019t compete against guys\n...\n...\n<spk:2> uhuh did you see the the woman\ngolfer that was on this the one\n<spk:1> monica yeah yeah\n<spk:2> what\u2019s her name monica stone\n<spk:1> yeah\n<spk:2> mhm\n<spk:1> she she blew out she fell out of that\ntournament but i didn\u2019t think she\u2019d do it she\nshe\u2019s girls can\u2019t compete against guys\n...\nfe_03_11210\n(\u2206 WDER\n=6.35%)\n...\n<spk:1> the vikings mine\u2019s the eagles i\u2019m\nfrom new jersey oh all right i have my jersey\non now i watch the game tonight yeah well i\ni may i may just watch\n<spk:2> part of it tonight too then but uh it\u2019s a\ncase as i say if if i had to pay for it i probably\nwouldn\u2019t watch it\n<spk:1> i wouldn\u2019t either uhhuh\n<spk:2> unless\n<spk:1> it was an eagles game\n...\n...\n<spk:1> the vikings mine\u2019s the eagles i\u2019m\nfrom new jersey\n<spk:2> oh all right\n<spk:1> i have my jersey on now i watch the\ngame tonight yeah\n<spk:2> well i i may i may just watch part of\nit tonight too then but uh it\u2019s a case as i say\nif if i had to pay for it i probably wouldn\u2019t\nwatch it\n<spk:1> i wouldn\u2019t either\n<spk:2> uhhuh\n<spk:1> unless it was an eagles game\n...\nfe_03_11159\n(\u2206 WDER\n=4.05%)\n...\n<spk:2> yeah\n<spk:1> anniversary that\u2019s horrible\n<spk:2> yeah\n<spk:1> yeah it\u2019s not good\n<spk:2> i\n<spk:1> i hear i hear you there that\u2019s not a\ngood thing you\n<spk:2> know i mean of course you know\nthat\u2019s a day that will go down instantly no-\nbody will ever remember it\n...\n...\n<spk:1> yeah anniversary that\u2019s horrible yeah\nyeah it\u2019s not good i i hear i hear you there\nthat\u2019s not a good thing\n<spk:2> you know i mean of course you know\nthat\u2019s a day that will go down instantly no-\nbody will ever remember it\n...\n11\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nTable 4: Example cases from the Callhome testing set where we see big absolute WDER reduction\n(\u2206 WDER) with DiarizationLM (deg2ref flavor).\nUtterance\nBefore DiarizationLM\nAfter DiarizationLM\nen_6447\n(\u2206 WDER\n=12.49%)\n...\n<spk:1> i\u2019m\n<spk:2> going to see if i can talk to the guy\nthat\u2019s selling the trailer if i can chew him\ndown a bit uhhuh\n<spk:1> and\n<spk:2> you know what you just said bene-\ndicta is are you living with benedicta\n<spk:1> yes yes yes\n<spk:2> you know what i bet she answered\nthe phone\n...\n...\n<spk:2> i\u2019m going to see if i can talk to the\nguy that\u2019s selling the trailer if i can chew him\ndown a bit\n<spk:1> uhhuh\n<spk:2> and you know what you just said\nbenedicta is are you living with benedicta\n<spk:1> yes yes yes\n<spk:2> you know what i bet she answered\nthe phone\n...\nen_6408\n(\u2206 WDER\n=10.87%)\n...\n<spk:1> uhu\n<spk:2> so\n<spk:1> he had big surgery again and he\u2019s in\na wheelchair oh my\n<spk:2> and\n<spk:1> he doesn\u2019t want to go to school in a\nwheelchair uhuh but\n<spk:2> he might he wants to have tutoring\nat home but they\u2019re still where they lived on\n45th street\n<spk:1> yeah they\u2019re there\n...\n...\n<spk:2> uhu\n<spk:1> so he had big surgery again and he\u2019s\nin a wheelchair\n<spk:2> oh my\n<spk:1> and he doesn\u2019t want to go to school\nin a wheelchair\n<spk:2> uhuh\n<spk:1> but he might he wants to have tutor-\ning at home\n<spk:2> but they\u2019re still where they lived on\n45th street\n<spk:1> yeah they\u2019re there\n...\nen_6298\n(\u2206 WDER\n=9.95%)\n...\n<spk:1> um hey we\u2019re we\u2019re confused about\nyou guys address\n<spk:2> is\n<spk:1> it 1324 or 13\n<spk:2> it\u2019s 1 324\n<spk:1> excuse me 1324 yes and it\u2019s me view\nis me two words or one word yes it\u2019s two\nwords and there\u2019s an ln besides\n...\n...\n<spk:1> um hey we\u2019re we\u2019re confused about\nyou guys address is it 1324 or 13\n<spk:2> it\u2019s 1 324\n<spk:1> excuse me 1324\n<spk:2> yes\n<spk:1> and it\u2019s me view is me two words or\none word\n<spk:2> yes it\u2019s two words and there\u2019s an ln\nbesides\n...\nen_4792\n(\u2206 WDER\n=9.42%)\n...\n<spk:2> yeah well he was at columbia\n<spk:1> he was there like five years and they\nturned him down for tenure then he went\nsomewhere else he he was down in college\npark maryland yeah and he i think he was\nonly non tenure track down there then sup-\nposedly supposed to be back in japan now\nyeah but you know he\u2019s he\u2019s probably be-\ncome an english teacher at some unit yeah\ni know a guy believe it or not i know a guy\nfrom manhattan who was up in sapotto his\nmajor he did an mba believe it or not he\u2019s\nhe\u2019s an english teacher now huh\n...\n...\n<spk:2> yeah well he was at columbia\n<spk:1> he was there like five years and they\nturned him down for tenure then he went\nsomewhere else he he was down in college\npark maryland\n<spk:2> yeah\n<spk:1> and he i think he was only non tenure\ntrack down there then supposedly supposed\nto be back in japan now\n<spk:2> yeah\n<spk:1> but you know he\u2019s he\u2019s probably be-\ncome an english teacher at some unit\n<spk:2> yeah\n<spk:1> i know a guy believe it or not i know\na guy from manhattan who was up in sapotto\nhis major he did an mba believe it or not he\u2019s\nhe\u2019s an english teacher now\n<spk:2> huh\n...\n12\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nTable 5: Example cases from the Fisher testing set where zero-shot PaLM 2-S deletes lots of text\nfrom the prompt.\nUtterance\nBefore DiarizationLM\nAfter DiarizationLM\nfe_03_11252\n...\n<spk:1> oh okay i believe it\u2019s a lot wrong\nwith the public schools i don\u2019t believe that\nthey\u2019re um that they\u2019re giving um these kids\na sense of um well they\u2019re not teaching them\nwhat they need to know once they get out of\num school you know mhm um what what\u2019s\nhappening is that\u2019s probably why you got a\nlot of um a lot of people that\u2019s unemployed\ni think you know they you get a lot from\nschool and they taking a lot of um i guess the\neconomics out of school you know\n<spk:2> right\n...\n...\n<spk:1> oh okay i believe it\u2019s a lot wrong\nwith the public schools i don\u2019t believe that\nthey\u2019re um that they\u2019re giving um these kids\na sense of um well they\u2019re not teaching them\nwhat they need to know once they get out of\num school you know\n<spk:2> right\n...\nfe_03_11224\n...\n<spk:1> so um i think what do you think is\nan important thing in a relation i think the\ntopic was um what you um what are the most\nimportant things in a life partner yeah uh h\nwell what do you think me\n<spk:2> i would have to say trust and honesty\nlike cuz without that you really don\u2019t have\nnothing to build on you know right yeah\n...\n...\n<spk:1> so um i think what do you think is\nan important thing in a relation\n<spk:2> i would have to say trust and honesty\nlike cuz without that you really don\u2019t have\nnothing to build on you know right\n...\nand speaker embeddings, and the estimated probabilities are combined with the adjacency matrix for\nspectral clustering. Similarly in [7], an end-to-end trained transformer transducer (T-T) [59] based\nspeaker turn detection model is used to constrain the spectral clustering via Exhaustive and Efficient\nConstraint Propagation (E2CP).\n6.3\nSPEAKER DIARIZATION WITH LLM\nIn [35], the authors proposed Speaker Error Corrector (SEC), which aims to solve the same problem\nas we stated in Section 1. In [35], word embeddings from the ASR transcript are extracted with a\npre-trained Roberta-base LM [60]. Then a separately trained transformer encoder takes the word\nembeddings and the hypothesis speaker labels as input, and produces the corrected speaker labels.\nThe transformer encoder is trained on both simulated diarization errors and real data. The biggest\ndifference from our proposed framework to [35] is that we directly feed the compact pure textual\nrepresentation of the ASR and diarization results as part of the prompt to the LLM, and directly\nfinetune the LLM to produce the corrected results in the same compact textual representation. Our\nDiarizationLM is a \u201ctext-in, text-out\u201d system, without relying on internal embedding representations\nfrom the LLM.\nMore recently in [61], the authors proposed to use LLM to predict the speaker probability for the\nnext word, and incorporate this probability into the beam search decoding of speaker diarization.\nOur proposed framework differs from this work by using a single prompt (or several prompts due to\nLLM input size limit) to post-process the entire results of the speaker diarization system, instead of\nword-by-word prompting. Additionally, our proposed framework can be more generally applied to\nany speaker diarization system, instead of requiring word-level speaker probabilities for beam search\ndecoding.\n7\nCONCLUSION\nIn this paper, we demonstrate that large language models (LLM) can be used to post-process speaker\ndiarization results, achieving various goals such as improving the readability of the diarization\n13\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\ntranscript, and reducing the diarization errors. Specifically, we proposed DiarizationLM, a framework\nwhere we use a finetuned LLM to refine the results from off-the-shelf ASR and speaker diarization\nsystems. We introduced three different flavors to build the prompt-completion pairs data for finetuning\nthe LLM. Our experiments on Fisher and Callhome datasets show that a finetuned PaLM 2-S model\ncan drastically reduce the word diarization error rates of typical diarization systems like turn-to-\ndiarize.\nREFERENCES\n[1] Tae Jin Park, Naoyuki Kanda, Dimitrios Dimitriadis, Kyu J Han, Shinji Watanabe, and Shrikanth\nNarayanan, \u201cA review of speaker diarization: Recent advances with deep learning,\u201d Computer\nSpeech & Language, vol. 72, pp. 101317, 2022.\n[2] Chao Zhang and Quan Wang, \u201cSpeaker diarization: A journey from unsupervised to supervised\napproaches,\u201d Odyssey: The Speaker and Language Recognition Workshop, 2022, Tutorial\nsession.\n[3] Rub\u00e9n Zazo Candil, Tara N Sainath, Gabor Simko, and Carolina Parada, \u201cFeature learning with\nraw-waveform CLDNNs for voice activity detection,\u201d in Proc. Interspeech, 2016.\n[4] Ivan Medennikov, Maxim Korenevsky, Tatiana Prisyach, Yuri Khokhlov, Mariya Korenevskaya,\nIvan Sorokin, Tatiana Timofeeva, Anton Mitrofanov, Andrei Andrusenko, Ivan Podluzhny, et al.,\n\u201cTarget-speaker voice activity detection: a novel approach for multi-speaker diarization in a\ndinner party scenario,\u201d in Proc. Interspeech, 2020.\n[5] Shaojin Ding, Quan Wang, Shuo-yiin Chang, Li Wan, and Ignacio Lopez Moreno, \u201cPersonal\nVAD: Speaker-conditioned voice activity detection,\u201d in Odyssey: The Speaker and Language\nRecognition Workshop, 2020.\n[6] Shaojin Ding, Rajeev Rikhye, Qiao Liang, Yanzhang He, Quan Wang, Arun Narayanan, Tom\nO\u2019Malley, and Ian McGraw, \u201cPersonal vad 2.0: Optimizing personal voice activity detection for\non-device speech recognition,\u201d arXiv preprint arXiv:2204.03793, 2022.\n[7] Wei Xia, Han Lu, Quan Wang, Anshuman Tripathi, Yiling Huang, Ignacio Lopez Moreno, and\nHasim Sak, \u201cTurn-to-Diarize: Online speaker diarization constrained by transformer transducer\nspeaker turn detection,\u201d in International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2022, pp. 8077\u20138081.\n[8] Guanlong Zhao, Quan Wang, Han Lu, Yiling Huang, and Ignacio Lopez Moreno, \u201cAugmenting\ntransformer-transducer based speaker change detection with token-level training loss,\u201d in\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.\n[9] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno, \u201cGeneralized end-to-end loss for\nspeaker verification,\u201d in International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 4879\u20134883.\n[10] Chao Li, Xiaokong Ma, Bing Jiang, Xiangang Li, Xuewei Zhang, Xiao Liu, Ying Cao, Ajay\nKannan, and Zhenyao Zhu, \u201cDeep speaker: an end-to-end neural speaker embedding system,\u201d\narXiv preprint arXiv:1705.02304, 2017.\n[11] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur,\n\u201cX-Vectors: Robust dnn embeddings for speaker recognition,\u201d in International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5329\u20135333.\n[12] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mansfield, and Ignacio Lopez Moreno,\n\u201cSpeaker diarization with LSTM,\u201d in International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2018, pp. 5239\u20135243.\n[13] Daniel Garcia-Romero, David Snyder, Gregory Sell, Daniel Povey, and Alan McCree, \u201cSpeaker\ndiarization using deep neural network embeddings,\u201d in International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2017, pp. 4930\u20134934.\n[14] Dimitrios Dimitriadis and Petr Fousek, \u201cDeveloping on-line speaker diarization system,\u201d in\nProc. Interspeech, 2017, pp. 2739\u20132743.\n[15] Tae Jin Park, Kyu J Han, Manoj Kumar, and Shrikanth Narayanan, \u201cAuto-tuning spectral clus-\ntering for speaker diarization using normalized maximum eigengap,\u201d IEEE Signal Processing\nLetters, vol. 27, pp. 381\u2013385, 2019.\n14\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n[16] Tae Jin Park, Manoj Kumar, and Shrikanth Narayanan, \u201cMulti-scale speaker diarization with\nneural affinity score fusion,\u201d in International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2021, pp. 7173\u20137177.\n[17] Quan Wang, Yiling Huang, Han Lu, Guanlong Zhao, and Ignacio Lopez Moreno, \u201cHighly\nefficient real-time streaming and fully on-device speaker diarization with multi-stage clustering,\u201d\narXiv preprint arXiv:2210.13690, 2022.\n[18] Aonan Zhang, Quan Wang, Zhenyao Zhu, John Paisley, and Chong Wang, \u201cFully supervised\nspeaker diarization,\u201d in International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2019, pp. 6301\u20136305.\n[19] Qiujia Li, Florian L Kreyssig, Chao Zhang, and Philip C Woodland, \u201cDiscriminative neural\nclustering for speaker diarisation,\u201d in Spoken Language Technology Workshop (SLT). IEEE,\n2021.\n[20] Yusuke Fujita, Naoyuki Kanda, Shota Horiguchi, Kenji Nagamatsu, and Shinji Watanabe,\n\u201cEnd-to-end neural speaker diarization with permutation-free objectives,\u201d in Proc. Interspeech,\n2019, pp. 4300\u20134304.\n[21] Yushi Ueda, Soumi Maiti, Shinji Watanabe, Chunlei Zhang, Meng Yu, Shi-Xiong Zhang, and\nYong Xu, \u201cEEND-SS: Joint end-to-end neural speaker diarization and speech separation for\nflexible number of speakers,\u201d arXiv preprint arXiv:2203.17068, 2022.\n[22] Quan Wang, Yash Sheth, Ignacio Lopez Moreno, and Li Wan, \u201cSpeaker diarization using an\nend-to-end model,\u201d US Patent US011545157B2, 2019.\n[23] Shota Horiguchi, Yusuke Fujita, Shinji Watanabe, Yawen Xue, and Kenji Nagamatsu, \u201cEnd-\nto-end speaker diarization for an unknown number of speakers with encoder-decoder based\nattractors,\u201d arXiv preprint arXiv:2005.09921, 2020.\n[24] Quan Wang and Fan Zhang, \u201cWho said what? Recorder\u2019s on-device solution for labeling\nspeakers,\u201d Google AI Blog.\n[25] Laurent El Shafey, Hagen Soltau, and Izhak Shafran, \u201cJoint speech recognition and speaker\ndiarization via sequence transduction,\u201d in Proc. Interspeech, 2019, pp. 396\u2013400.\n[26] Naoyuki Kanda, Yashesh Gaur, Xiaofei Wang, Zhong Meng, Zhuo Chen, Tianyan Zhou, and\nTakuya Yoshioka, \u201cJoint speaker counting, speech recognition, and speaker identification for\noverlapped speech of any number of speakers,\u201d arXiv preprint arXiv:2006.10930, 2020.\n[27] Naoyuki Kanda, Zhong Meng, Liang Lu, Yashesh Gaur, Xiaofei Wang, Zhuo Chen, and\nTakuya Yoshioka, \u201cMinimum bayes risk training for end-to-end speaker-attributed ASR,\u201d in\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021,\npp. 6503\u20136507.\n[28] Naoyuki Kanda, Guoli Ye, Yashesh Gaur, Xiaofei Wang, Zhong Meng, Zhuo Chen, and\nTakuya Yoshioka, \u201cEnd-to-end speaker-attributed ASR with transformer,\u201d arXiv preprint\narXiv:2104.02128, 2021.\n[29] Naoyuki Kanda, Jian Wu, Yu Wu, Xiong Xiao, Zhong Meng, Xiaofei Wang, Yashesh Gaur, Zhuo\nChen, Jinyu Li, and Takuya Yoshioka, \u201cStreaming speaker-attributed ASR with token-level\nspeaker embeddings,\u201d arXiv preprint arXiv:2203.16685, 2022.\n[30] Katerina Zmolikova, Marc Delcroix, Keisuke Kinoshita, Takuya Higuchi, Atsunori Ogawa, and\nTomohiro Nakatani, \u201cSpeaker-aware neural network based beamformer for speaker extraction\nin speech mixtures,\u201d in Proc. Interspeech, 2017, pp. 2655\u20132659.\n[31] Marc Delcroix, Katerina Zmolikova, Keisuke Kinoshita, Atsunori Ogawa, and Tomohiro\nNakatani, \u201cSingle channel target speaker extraction and recognition with speaker beam,\u201d in\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018,\npp. 5554\u20135558.\n[32] Marc Delcroix, Shinji Watanabe, Tsubasa Ochiai, Keisuke Kinoshita, Shigeki Karita, Atsunori\nOgawa, and Tomohiro Nakatani, \u201cEnd-to-end SpeakerBeam for single channel target speech\nrecognition,\u201d in Proc. Interspeech, 2019, pp. 451\u2013455.\n[33] Naoyuki Kanda, Shota Horiguchi, Ryoichi Takashima, Yusuke Fujita, Kenji Nagamatsu, and\nShinji Watanabe, \u201cAuxiliary interference speaker loss for target-speaker speech recognition,\u201d\narXiv preprint arXiv:1906.10876, 2019.\n15\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n[34] Yiling Huang, Weiran Wang, Guanlong Zhao, Hank Liao, Wei Xia, and Quan Wang, \u201cTowards\nword-level end-to-end neural speaker diarization with auxiliary network,\u201d arXiv preprint\narXiv:2309.08489, 2023.\n[35] Rohit Paturi, Sundararajan Srinivasan, and Xiang Li, \u201cLexical speaker error correction: Leverag-\ning language models for speaker diarization error correction,\u201d arXiv preprint arXiv:2306.09313,\n2023.\n[36] James Manyika and Sissie Hsiao, \u201cAn overview of Bard: an early experiment with generative AI,\u201d\nhttps://ai.google/static/documents/google-about-bard.pdf, 2023.\n[37] OpenAI, \u201cIntroducing ChatGPT,\u201d https://openai.com/blog/chatgpt, 2022.\n[38] Vladimir I Levenshtein, \u201cBinary codes capable of correcting deletions, insertions, and reversals,\u201d\nSoviet physics doklady, vol. 10, no. 8, pp. 707\u2013710, 1966.\n[39] Harold W Kuhn, \u201cThe Hungarian method for the assignment problem,\u201d Naval research logistics\nquarterly, vol. 2, no. 1-2, pp. 83\u201397, 1955.\n[40] Christopher Cieri, David Miller, and Kevin Walker, \u201cThe Fisher corpus: A resource for the next\ngenerations of speech-to-text,\u201d in LREC, 2004, vol. 4, pp. 69\u201371.\n[41] Guanlong Zhao, Yongqiang Wang, Jason Pelecanos, Yu Zhang, Hank Liao, Yiling Huang,\nHan Lu, and Quan Wang, \u201cUSM-SCD: Multilingual speaker change detection based on large\npretrained foundation models,\u201d arXiv preprint arXiv:2309.08023, 2023.\n[42] A Canavan, D Graff, and G Zipperlen, \u201cCALLHOME American English speech LDC97S42,\u201d\nLDC Catalog. Philadelphia: Linguistic Data Consortium, 1997.\n[43] Shinji Watanabe, Michael Mandel, Jon Barker, Emmanuel Vincent, Ashish Arora, Xuankai\nChang, Sanjeev Khudanpur, Vimal Manohar, Daniel Povey, Desh Raj, et al., \u201cCHiME-6\nchallenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d arXiv\npreprint arXiv:2004.09249, 2020.\n[44] Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen,\nBo Li, Vera Axelrod, Gary Wang, et al., \u201cGoogle USM: Scaling automatic speech recognition\nbeyond 100 languages,\u201d arXiv preprint arXiv:2303.01037, 2023.\n[45] Alex Graves,\n\u201cSequence transduction with recurrent neural networks,\u201d\narXiv preprint\narXiv:1211.3711, 2012.\n[46] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al., \u201cPaLM 2 technical report,\u201d\narXiv preprint arXiv:2305.10403, 2023.\n[47] Taku Kudo and John Richardson, \u201cSentencePiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing,\u201d arXiv preprint arXiv:1808.06226, 2018.\n[48] Gregory Sell and Daniel Garcia-Romero, \u201cDiarization resegmentation in the factor analysis\nsubspace,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2015, pp. 4794\u20134798.\n[49] Ruiqing Yin, Herv\u00e9 Bredin, and Claude Barras, \u201cNeural speech turn segmentation and affinity\npropagation for speaker diarization,\u201d in Proc. Interspeech, 2018, pp. 1393\u20131397.\n[50] Jiangyu Han, Federico Landini, Johan Rohdin, Mireia Diez, Lukas Burget, Yuhang Cao, Heng\nLu, and Jan Cernocky, \u201cDiaCorrect: Error correction back-end for speaker diarization,\u201d arXiv\npreprint arXiv:2309.08377, 2023.\n[51] MAH Huijbregts, David A van Leeuwen, and FM Jong, \u201cThe majority wins: a method for\ncombining speaker diarization systems,\u201d in Proc. Interspeech, 2009.\n[52] Simon Bozonnet, Nicholas Evans, Xavier Anguera, Oriol Vinyals, Gerald Friedland, and\nCorinne Fredouille, \u201cSystem output combination for improved speaker diarization,\u201d in Proc.\nInterspeech, 2010.\n[53] Andreas Stolcke and Takuya Yoshioka, \u201cDOVER: A method for combining diarization outputs,\u201d\nin Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp.\n757\u2013763.\n[54] Shota Horiguchi, Paola Garcia, Yusuke Fujita, Shinji Watanabe, and Kenji Nagamatsu, \u201cEnd-to-\nend speaker diarization as post-processing,\u201d in International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2021, pp. 7188\u20137192.\n16\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n[55] Jan Silovsky, Jindrich Zdansky, Jan Nouza, Petr Cerva, and Jan Prazak, \u201cIncorporation of\nthe ASR output in speaker segmentation and clustering within the task of speaker diarization\nof broadcast streams,\u201d in International Workshop on Multimedia Signal Processing (MMSP).\nIEEE, 2012, pp. 118\u2013123.\n[56] Tae Jin Park and Panayiotis Georgiou, \u201cMultimodal speaker segmentation and diarization using\nlexical and acoustic cues via sequence to sequence neural networks,\u201d in Proc. Interspeech, 2018,\npp. 1373\u20131377.\n[57] Tae Jin Park, Kyu J Han, Jing Huang, Xiaodong He, Bowen Zhou, Panayiotis Georgiou,\nand Shrikanth Narayanan, \u201cSpeaker diarization with lexical information,\u201d arXiv preprint\narXiv:2004.06756, 2020.\n[58] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio, \u201cEmpirical evaluation\nof gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555,\n2014.\n[59] Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, and\nShankar Kumar,\n\u201cTransformer transducer: A streamable speech recognition model with\ntransformer encoders and RNN-T loss,\u201d in International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2020, pp. 7829\u20137833.\n[60] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov, \u201cRoberta: A robustly optimized bert\npretraining approach,\u201d arXiv preprint arXiv:1907.11692, 2019.\n[61] Tae Jin Park, Kunal Dhawan, Nithin Koluguri, and Jagadeesh Balam, \u201cEnhancing speaker\ndiarization with large language models: A contextual beam search approach,\u201d arXiv preprint\narXiv:2309.05248, 2023.\n17\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nAppendices\nA\nOTHER DIARIZATION CAPABILITIES OF LLMS\nIn our studies, we have found multiple diarization capabilities of LLMs. Due to the lack of relevant\ntraining and testing datasets, this paper mainly focuses on evaluating how LLMs can reduce diariza-\ntion errors. In this section, we provide several examples to demonstrate LLM\u2019s other diarization\ncapabilities. All the provided examples are obtained by directly sending the prompts to Google\nBard [36]5.\nA.1\nAUTOFILLING SPEAKER NAMES\nIn real world applications, real speaker names or aliases are much more readable than anonymous\nspeaker labels such as \u201cspeaker 1\u201d or \u201cspeaker 2\u201d. In many cases, speaker names can be obtained\nfrom the context of the conversation and from additional metadata of the conversation. In meeting\nsummarization, the names can be obtained from the list of meeting attendants. And for video\ncaptioning, the names may be extracted from the video title or description. An example is provided\nbelow.\nPrompt:\n[video title] Tom meets Patrick in the parking lot\n[video description] A short conversation\nReplace the <spk:?> tokens from the below video caption with actual person names.\n<spk:1> Good morning Patrick, how are you?\n<spk:2> Good, good. How are you Tom?\n<spk:1> Pretty good. Going to work?\n<spk:2> Yes. Busy day. How are your kids? Do they go to school?\n<spk:1> Oh they are too young for that. I sent them to daycare earlier today.\n<spk:2> Oh yeah I forgot about that.\nCompletion:\nBased on the video title \"Tom meets Patrick in the parking lot\", here\u2019s the transcript with speaker\nnames replaced:\nTom: Good morning Patrick, how are you?\nPatrick: Good, good. How are you Tom?\nTom: Pretty good. Going to work?\nPatrick: Yes. Busy day. How are your kids? Do they go to school?\nTom: Oh they are too young for that. I sent them to daycare earlier today.\nPatrick: Oh yeah, I forgot about that.\nA.2\nAUTOFILLING SPEAKER ROLES\nIn some applications, knowing the roles of each speaker is useful enough. For example, it would be\nhelpful to distinguish the doctor from the patient in medical transcripts [25], and to distinguish the\ncustomer from the agent in call center conversations. This task can be handled with LLM usually\nwithout additional metadata about the conversation. We show an example below.\nPrompt:\n5We used an internal version of Bard that is based on a larger model and supports more tokens than the public\nversion.\n18\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\nReplace the <spk:?> tokens from the below conversation with roles like teacher, student, doctor,\npatient, etc.\n<spk:1> Hi, how can I help you today?\n<spk:2> Hi, I recently often feel quite dizzy at work.\n<spk:1> Do you have any of these symptoms? Like coughing, fever, or running nose?\n<spk:2> No, I don\u2019t think so.\n<spk:1> What do you usually eat for breakfast, lunch and dinner?\n<spk:2> I\u2019m on a diet, so not eating very much. Mostly just one slice of toast in the morning,\nand eating salad for lunch and dinner.\nCompletion:\nDoctor: Hi, how can I help you today?\nPatient: Hi, I recently often feel quite dizzy at work.\nDoctor: Do you have any of these symptoms? Like coughing, fever, or running nose?\nPatient: No, I don\u2019t think so.\nDoctor: What do you usually eat for breakfast, lunch, and dinner?\nPatient: I\u2019m on a diet, so not eating very much. Mostly just one slice of toast in the morning,\nand eating salad for lunch and dinner.\nA.3\nREPLACING THE ORCHESTRATION MODULE\nIn Fig. 1, we have explained how an orchestration module can combine the ASR transcripts with\nspeaker diarization outputs via the timing information from the two systems. Interestingly, we have\nfound that this process can also be fully replaced by LLM. This can be achieved by explicitly including\nthe timing information in the textual representation of both ASR transcripts and speaker diarization\noutputs. Additionally, more prompt engineering will be needed, such as explicitly explaining the\nformat of the textual representation, and providing a one-shot example in the prompt. We show an\nexample below.\nPrompt:\nHere we define the problem of speaker-transcript alignment. The transcript is represented by\nmultiple entries of text, where each text has a starting time and an ending time. The speaker is\nalso represented in this format. The alignment problem will assign a speaker to each word in the\ntext, based on which speaker overlaps the most with that word in time.\nBelow is an example.\nTranscript represented in format \"[start - end] text\":\n[0 - 2.3] Good morning Patrick\n[2.5 - 5.2] how are you?\n[5.6 - 6.1] Good, good.\n[6.2 - 8.3] How are you Tom?\n[9.2 - 9.9] Pretty good.\n[10.0 - 11.1] Going to work?\n[12.5 - 13.6] Yes. Busy day.\nSpeaker represented in format \"[start - end] speaker\":\n[0 - 5.1] <spk:1>\n[5.3 - 8.7] <spk:2>\n[9.2 - 10.9] <spk:1>\n[12.1 - 13.5] <spk:2>\nThe alignment result should be:\n[0 - 2.3] <spk:1> Good morning Patrick\n19\nDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\n[2.5 - 5.2] <spk:1> how are you?\n[5.6 - 6.1] <spk:2> Good, good.\n[6.2 - 8.3] <spk:2> How are you Tom?\n[9.2 - 9.9] <spk:1> Pretty good.\n[10.0 - 11.1] <spk:1> Going to work?\n[12.5 - 13.6] <spk:2> Yes. Busy day.\nBased on the example above, we solve the alignment problem below.\nTranscript represented in format \"[start - end] text\":\n[0 - 5.6] Hi, how can I help you today?\n[6.2 - 11.1] Hi, I recently often feel quite dizzy at work.\n[11.6 - 15.5] Do you have any of these symptoms?\n[16.6 - 18.5] Like coughing, fever, or running nose?\n[20.0 - 21.1] No, I don\u2019t think so.\n[22.2 - 29.9] What do you usually eat for breakfast, lunch and dinner?\n[31.2 - 34.8] I\u2019m on a diet, so not eating very much.\n[35.2 - 39.8] Mostly just one slice of toast in the morning, and eating salad for lunch and dinner.\nSpeaker represented in format \"[start - end] speaker\":\n[0.3 - 5.3] <spk:1>\n[6.0 - 12.0] <spk:2>\n[12.9 - 20.1] <spk:1>\n[20.2 - 21.0] <spk:2>\n[21.8 - 31.1] <spk:1>\n[32.4 - 40.7] <spk:2>\nThe alignment result should be:\nCompletion:\nHere\u2019s the aligned transcript:\n[0 - 5.6] <spk:1> Hi, how can I help you today?\n[6.2 - 11.1] <spk:2> Hi, I recently often feel quite dizzy at work.\n[11.6 - 15.5] <spk:1> Do you have any of these symptoms?\n[16.6 - 18.5] <spk:1> Like coughing, fever, or running nose?\n[20.0 - 21.1] <spk:2> No, I don\u2019t think so.\n[22.2 - 29.9] <spk:1> What do you usually eat for breakfast, lunch and dinner?\n[31.2 - 34.8] <spk:2> I\u2019m on a diet, so not eating very much.\n[35.2 - 39.8] <spk:2> Mostly just one slice of toast in the morning, and eating salad for lunch\nand dinner.\nTo align the transcript and speaker segments, we assign each word in the transcript to\nthe speaker whose segment overlaps the most with that word in time.\n20\n"
  },
  {
    "title": "AST-T5: Structure-Aware Pretraining for Code Generation and Understanding",
    "link": "https://arxiv.org/pdf/2401.03003.pdf",
    "upvote": "11",
    "text": "AST-T5: Structure-Aware Pretraining for Code Generation and Understanding\nLinyuan Gong 1 Mostafa Elhoushi 2 Alvin Cheung 1\nAbstract\nLarge language models (LLMs) have made sig-\nnificant advancements in code-related tasks, yet\nmany LLMs treat code as simple sequences, ne-\nglecting its structured nature.\nWe introduce\nAST-T5, a novel pretraining paradigm that lever-\nages the Abstract Syntax Tree (AST) for en-\nhanced code generation, transpilation, and un-\nderstanding. Using dynamic programming, our\nAST-Aware Segmentation retains code structure,\nwhile our AST-Aware Span Corruption objective\nequips the model to reconstruct various code struc-\ntures. Unlike other models, AST-T5 avoids com-\nplex program analyses or architectural changes,\nso it integrates seamlessly with any encoder-\ndecoder Transformer.\nEvaluations show that\nAST-T5 consistently outperforms similar-sized\nLMs across various code-related tasks includ-\ning HumanEval and MBPP. Structure-awareness\nmakes AST-T5 particularly powerful in code-to-\ncode tasks, surpassing CodeT5 by 2 points in ex-\nact match score for the Bugs2Fix task and by\n3 points in exact match score for Java-C# Tran-\nspilation in CodeXGLUE. Our code and model\nare publicly available at https://github.com/\ngonglinyuan/ast t5 .\n1. Introduction\nWe have witnessed the transformative impact of large lan-\nguage models (LLMs) on various aspects of artificial intel-\nligence in recent years (Brown et al., 2020; Ouyang et al.,\n2022; Touvron et al., 2023), especially in code generation\nand understanding (Feng et al., 2020; Wang et al., 2021;\nRozi`ere et al., 2023). By pretraining on massive code cor-\npora such as the GitHub corpus, LLMs learns rich represen-\ntations, thereby becoming powerful tools for various down-\nstream applications such as text-to-code generation (Chen\net al., 2021a; Austin et al., 2021; Iyer et al., 2018), code-\nto-code transpilation (Lu et al., 2021; Lachaux et al., 2020;\n1University of California at Berkeley 2Meta AI. Correspon-\ndence to: Linyuan Gong <gly@berkeley.edu>.\nPreprint. Under review.\nTufano et al., 2019), and code understanding (mapping code\nto classification labels) (Zhou et al., 2019; Svajlenko et al.,\n2014).\nDespite these impressive advances, most existing models\ninterpret code as mere sequences of subword tokens, over-\nlooking its intrinsic structured nature. Prior research has\nshown that leveraging the Abstract Syntax Tree (AST) of\ncode can significantly improve performance on code-related\ntasks (Guo et al., 2021; Tipirneni et al., 2023). Some studies\nalso use code obfuscation during pretraining to teach models\nabout abstract code structures (Roziere et al., 2021; Wang\net al., 2021). However, these models often rely on compu-\ntationally expensive processes like Control-Flow Analysis\n(CFA), obfuscation, or even actual code execution. Such de-\npendency limits their scalability and imposes stringent con-\nditions like code executability. Consequently, these methods\nmay struggle with real-world code, especially in intricate\nlanguages like C/C++, where comprehensive analysis re-\nmains elusive.\nIn this study, we propose AST-T5, a pretraining paradigm\nthat leverages the Abstract Syntax Tree (AST) structure\nof code. The key contribution in AST-T5 is a simple yet\neffective way to exploit code semantics, without the need\nto run expensive program analysis or execution. Using a\nlightweight, multi-language parser called Tree-sitter1, our\napproach has broad applicability across all syntactically\nwell-defined programming languages. After we parse code\ninto ASTs, we use a dynamic programming-based segmen-\ntation algorithm for AST-aware code segmentation to main-\ntain the structural integrity of the input code. Using our\nnovel AST-Aware Span Corruption technique, the model is\npretrained to reconstruct various code structures, ranging\nfrom individual tokens to entire function bodies. Together,\nour approach offers three key advantages: (1) enriched bidi-\nrectional encoding for improved code understanding, (2)\nthe ability to coherently generate code structures, and (3) a\nunified, structure-aware pretraining framework that boosts\nperformance across a variety of code-related tasks, particu-\nlarly in code transpilation.\nIn addition, other than our specialized AST-aware masking\napproach, AST-T5 introduces no architecture changes or\nadditional heads, and our pretraining objective remains the\n1https://tree-sitter.github.io/tree-sitter/\n1\narXiv:2401.03003v3  [cs.SE]  7 Mar 2024\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n - 1)\ndef fact[X]\n  if n == 0:\n    return 1\n  [Y]\n    return n [Z] - 1 )\ndef factorial ( n ) :\n  if [X]:\n    [Y]\n  else:\n    return [Z]\n[X] n == 0\n[Y] return 1\n[Z] n * factorial(n - 1)\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n - 1)\n[X] orial(n):\n[Y] else:\n[Z] * factorial(n\nOriginal code\nInput\nTarget\nOriginal code\nInput\nTarget\nVanilla T5 Span Corruption\nAST-Aware Subtree Corruption\nFigure 1: Comparison of AST-Aware Subtree Corruption and Vanilla T5 using a Python factorial function. Both methods\nreplace masked spans with sentinel tokens (special tokens added to the vocabulary, shown as [X], [Y], and [Z] in the figure),\nwith output sequences containing the original masked tokens. Inputs and targets are shown in byte-pair encoding (BPE); for\ninstance, \u201cfactorial\u201d is encoded into \u201cfact\u201d and \u201corial\u201d. Unlike Vanilla T5, which masks random spans without considering\ncode structure, our approach specifically targets spans aligned with AST subtrees, like expressions and statements.\nsame as Vanilla T5. This compatibility enables seamless\nintegration of our model as a drop-in replacement for any\nT5 variant.\nIn our experiments, AST-T5 consistently outperforms base-\nlines in code generation, transpilation, and understanding\ntasks.\nThrough controlled experiments, we empirically\ndemonstrate that these advancements are attributed to our\nAST-aware pretraining techniques. Notably, AST-T5 not\nonly outperforms similar-sized models like CodeT5 and\nCodeT5+ across various benchmarks but also remains com-\npetitive with, or occasionally even exceeds, the performance\nof much larger models using the HumanEval (Chen et al.,\n2021a) and the MBPP (Austin et al., 2021) benchmarks.\nFurthermore, the inherent AST-awareness of AST-T5 offers\nunique advantages in structure-sensitive tasks, such as code-\nto-code transpilation and Clone Detection, highlighting its\neffectiveness at capturing the structural nuances of code.\n2. Related Work\nLanguage Models for Code.\nLanguage models (LMs)\nextended their use from NLP to code understanding and\ngeneration. Encoder-only models generally excel in code\nunderstanding when finetuned with classifiers (Feng et al.,\n2020), while decoder-only models are optimized for code\ngeneration through their autoregressive nature (Chen et al.,\n2021a; Fried et al., 2023; Nijkamp et al., 2023). However,\nthese models can falter outside their primary domains of\nexpertise or require increased resources for comparable out-\ncomes. Our work focuses on encoder-decoder models, aim-\ning to efficiently balance performance in both understanding\nand generation tasks without excessive computational de-\nmands.\nEfforts Toward Unified Models.\nExtending NLP mod-\nels like BART (Lewis et al., 2019) and T5 (Raffel et al.,\n2020), several studies have developed encoder-decoder ar-\nchitectures, such as PLBART (Ahmad et al., 2021) and\nCodeT5 (Wang et al., 2021), to perform well in diverse\ncode-related tasks. Although these models show broader\nutility, they struggle with generating coherent, executable\ncode in complex scenarios like HumanEval (Chen et al.,\n2021a). CodeT5+ (Wang et al., 2023) seeks to address this\nlimitation through an intricate multi-task pretraining strat-\negy across five objectives. In contrast, our proposed model,\nAST-T5, uses a novel AST-Aware pretraining paradigm to\nbecome a unified model capable of generating fluent code\nand maintaining superior performance in code understand-\ning tasks. Moreover, AST-T5 is more streamlined, because\nit only uses a single pretraining objective.\nLeveraging Code Structure in Pretraining.\nCode differs\nfrom natural language in two key aspects: its executability\nand strict structural syntax. Previous research leveraged\nexecution traces for improving model performance (Chen\net al., 2018; 2021b; Shojaee et al., 2023), but this approach\nfaces scalability challenges when applied to large, web-\ncrawled code datasets used in pretraining. Regarding code\u2019s\n2\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\nstructured nature, various studies have integrated syntactic\nelements into neural network models. Li et al. (2018), Kim\net al. (2021) and Z\u00a8ugner et al. (2021) add AST-Aware atten-\ntion mechanisms in their models, while Alon et al. (2020)\nand Rabinovich et al. (2017) focus on modeling AST node\nexpansion operations rather than traditional code tokens. In\nparallel, Guo et al. (2021) and Allamanis et al. (2017) ex-\nplore DFG-Aware attention mechanisms and Graph Neural\nNetworks (GNNs), to interpret code based on its Data Flow\nGraph (DFG). StructCoder (Tipirneni et al., 2023) enriches\nthe code input by appending AST and DFG as additional\nfeatures. These methods, however, necessitate parsing or\nstatic analysis for downstream tasks, which is less feasible\nfor incomplete or incorrect code scenarios like bug fixing.\nOur work, AST-T5, aligns with methods that utilize code\nstructure only in pretraining, like DOBF (Roziere et al.,\n2021) and CodeT5 (Wang et al., 2021), which obfuscate\ninputs to force the model to grasp abstract structures. Our\napproach uniquely diverges by using AST-driven segmenta-\ntion and masking in T5 span corruption during pretraining.\nThis novel approach offers a more refined pretraining signal\ncompared to structure-agnostic T5, equipping our model\nto proficiently encode and generate semantically coherent\ncode structures.\n3. Method\nIn this section, we present AST-T5, a novel pretraining\nframework for code-based language models that harnesses\nthe power of Abstract Syntax Trees (ASTs). First, AST-T5\nparses code into ASTs to enable a deeper understanding\nof code structure. Leveraging this structure, we introduce\nAST-Aware Segmentation, an algorithm designed to address\nTransformer token limits while retaining the semantic coher-\nence of the code. Second, we introduce AST-Aware Span\nCorruption, a masking technique that pretrains AST-T5 to\nreconstruct code structures ranging from individual tokens\nto entire function bodies, enhancing both its flexibility and\nstructure-awareness.\n3.1. Parsing Code Into ASTs\nUnlike traditional language models on code that handle code\nas simple sequences of subword tokens, AST-T5 leverages\nthe Abstract Syntax Tree (AST) of code to gain semantic\ninsights. For parsing purposes, we assume the provided\ncode is syntactically valid\u2014a reasonable assumption for\ntasks like code transpilation and understanding. Instead of\nthe often computationally-intensive or infeasible methods of\nControl-Flow Analysis (CFA) or code execution (Guo et al.,\n2021; Tipirneni et al., 2023), our method only requires the\ncode to be parsable. We use Tree-sitter, a multi-language\nparser, to construct the ASTs, where each subtree represents\na consecutive span of subword tokens, and every leaf node\nrepresents an individual token.\n3.2. AST-Aware Segmentation\nIn this subsection, we describe our AST-Aware Segmenta-\ntion method, which splits lengthy code files into chunks in\na structure-perserving manner.\nSegmentation in language model pretraining is a critical\nyet often overlooked aspect. Transformer LMs impose token\nlimits on input sequences, making segmentation essential\nfor fitting these inputs within the max len constraint. A\nnaive approach is Greedy Segmentation, where each chunk,\nexcept the last, contains exactly max len tokens Figure 2\n(Left). This strategy has been widely adopted in previous\nworks, such as CodeT5 (Wang et al., 2021).\nResearch in NLP by Liu et al. (2019) underscores that seg-\nmentation respecting sentence and document boundaries\noutperforms the greedy strategy. Given programming lan-\nguage\u2019s inherently structured nature, which is arguably more\ncomplex than natural language, a more sophisticated seg-\nmentation approach is even more important. However, this\narea remains largely unexplored.\nAST-Aware Segmentation is our novel approach designed\nto preserve the AST structure of code during segmenta-\ntion. Unlike Greedy Segmentation, which can indiscrimi-\nnately fragment AST structures, our method strategically\nminimizes such disruptions. As illustrated in the example\nin Figure 2, Greedy Segmentation leads to nine instances\nof AST breaks\u2014between Block 1 and Block 2, it breaks\nIf, FuncDef, and ClassDef; between Block 2 and Block\n3, it breaks Attr, BinaryExpr, While, If, FuncDef, and\nClassDef. In contrast, our AST-Aware approach results in\nonly three breaks: between Block 1 and Block 2, it breaks\nClassDef, and between Block 2 and Block 3, it breaks\nFuncDef and ClassDef.\nTo identify optimal partition boundaries, we developed the\nfollowing dynamic programming (DP)-based algorithm:\n1. We construct an array cost, where cost[i] denotes\nthe number of AST-structure breaks that would occur\nif partitioning happened right after token i. This array\nis populated by traversing the AST and incrementing\ncost[l..r - 1] by 1 for each span [l, r] associated\nwith an AST subtree.\n2. We define a 2-D array dp, where dp[k, i] represents the\nthe minimum total number of AST-structure breaks when\nk partitions are made for the first i tokens, ending the last\npartition right after the i-th token. The state transition\n3\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\nclass BinaryIndexedTree:\n  def __init__(self, n):\n    self.n = n + 1\n    self.a = [0] * (n + 1)\n  def work(self, op, i):\n    if op == \"query\":\n      s = 0\n      while i:\n        s += self.a[i]\n        i -= i & -i\n      return s\n    if op == \"add\":\n      while i < self.n:\n        self.a[i] += 1\n        i += i & -i\nBlk\n#1\nBlk\n#2\nBlk\n#3\nclass BinaryIndexedTree:\n  def __init__(self, n):\n    self.n = n + 1\n    self.a = [0] * (n + 1)\n  def work(self, op, i):\n    if op == \"query\":\n      s = 0\n      while i:\n        s += self.a[i]\n        i -= i & -i\n      return s\n    if op == \"add\":\n      while i < self.n:\n        self.a[i] += 1\n        i += i & -i\nBlk\n#1\nBlk\n#2\nBlk\n#3\nWhile\nWhile\nIf\nIf\nFunc\nDef\nClass\nDef\nFunc\nDef\nGreedy Segmentation\nAST-Aware Segmentation\nAST\nFigure 2: Comparison between Greedy Segmentation and AST-Aware Segmentation: For a 112-token code example with\nmax len set at 48, Greedy Segmentation places the first 48 tokens in Block 1, the next 48 tokens in Block 2, and the\nremaining in Block 3, disrupting the structural integrity of the code. In contrast, AST-Aware Segmentation uses a dynamic\nprogramming algorithm to smartly partition the code, aligning with boundaries of member functions or major function\nbranches, thereby preserving the code\u2019s structure. The accompanying AST, with some levels pruned for clarity, corroborates\nthat these segmentations indeed coincide with key subtree demarcations.\nequation is:\ndp[k, i] = cost[i] +\nmin\ni\u2212max len\u2264j<i dp[k \u2212 1, j]\n(1)\n3. While the naive DP algorithm has a quadratic time com-\nplexity O(n2) relative to the code file length n, it can be\noptimized to O(n2/max len) by employing a monotonic\nqueue for sliding-window minimum calculations. This\nallows for efficient computation across most code files.\nThe pseudocode of the optimized dynamic programming\nalgorithm is shown in Algorithm 1. See Appendix A.2\nfor details about complexity calculations.\n4. The algorithm outputs the partition associated with\ndp[k min, n], where k min = arg mink(dp[k, n]), as\nthe most optimal partition.\nIn comparing AST-Aware Segmentation with Greedy\nSegmentation\u2014using the example in Figure 2\u2014we find\nthat the former presents more coherent code segments to the\nmodel during pretraining. Conversely, the latter introduces\nnoisy partial expressions near partition boundaries. Conse-\nquently, AST-Aware Segmentation not only optimizes the\npretraining process but also reduces the mismatch between\npretraining and downstream tasks, which often involve com-\nplete function definitions as inputs.\n3.3. Pretraining with Span Corruption\nAST-T5\u2019s pretraining is based on span corruption, a well-\nestablished method for pretraining transformer encoder-\ndecoder models (Raffel et al., 2020). In this approach,\nAlgorithm 1 Dynamic Programming in AST-Aware Seg-\nmentation\n1\n# n: the length of the code file\n2\n#\n(number of tokens)\n3\n# m: the max number of segments;\n4\n#\napproximately n / max_len\n5\nfor k in range(1, m + 1):\n6\nq = Queue()\n# double ended queue\n7\nfor i in range(1, n + 1):\n8\nwhile (q.nonempty() and\n9\nq.left() < i - max_len):\n10\n# pop indices before i - max_len\n11\nq.pop_left()\n12\nwhile (q.nonempty() and\n13\ndp[k-1, q.right()] > dp[k-1, i-1]):\n14\n# maintain monotonicity of values\n15\nq.pop_right()\n16\nq.push_right(i - 1)\n# push i - 1\n17\nbest_j = q.left()\n18\n# guaranteed to have the smallest value\n19\nprev[k, i] = best_j\n20\ndp[k, i] = cost[i] + dp[k - 1, best_j]\n15% of the input tokens are randomly masked and replaced\nby unique \u201csentinel\u201d tokens, distinct within each example.\nEach unique sentinel token is associated with a specific ID\nand added to the model\u2019s vocabulary.\nDuring pretraining, the encoder processes the corrupted in-\nput sequence. The decoder\u2019s objective is to reconstruct the\ndropped-out tokens based on the encoder\u2019s output repre-\nsentations. Specifically, the target sequence consists of the\nmasked spans of tokens, demarcated by their corresponding\n4\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\nAlgorithm 2 Subtree Selection in AST-Aware Subtree Cor-\nruption\n1\ndef mask_subtree(t: ASTNode, m: int):\n2\n\"\"\"mask m tokens in subtree t\"\"\"\n3\nordered_children = []\n4\nm_remaining = m\n5\n# distribute m tokens among children of t\n6\nfor child in t.children:\n7\n# theta: a hyperparameter to control\n8\n#\nmasking granularity\n9\nif child.size > theta:\n10\n# same mask ratio as the current subtree\n11\nm_child = m * (child.size / t.size)\n12\nmask_subtree(child, m_child)\n# recurse\n13\nm_remaining -= m_child\n14\nelse:\n15\nordered_children.append(child)\n16\nweighted_shuffle(ordered_children)\n17\n# greedy allocation of remaining mask quota\n18\nfor child in ordered_children:\n19\nm_child = min(m_remaining, child.size)\n20\nmask_subtree(child, m_child)\n21\nm_remaining -= m_child\nsentinel tokens. This framework effectively trains the model\nto recover the original text from a corrupted input. Figure 1\n(Left) illustrates an example of the input-output pair for span\ncorruption.\n3.4. AST-Aware Subtree Corruption\nAST-T5 augments the traditional span corruption paradigm\nby incorporating AST-awareness. Rather than arbitrarily\nmasking consecutive token spans, AST-T5 masks code\nspans corresponding to AST subtrees, ranging from individ-\nual expressions to entire function bodies.\nSubtree Masking.\nWe use a recursive algorithm, outlined\nin Algorithm 2, to traverse the AST and select subtrees for\nmasking. The algorithm aims to fulfill two goals:\n1. Introduce sufficient randomness across training epochs\nto enhance generalization.\n2. Control the masking granularity via a tunable hyperpa-\nrameter \u03b8 (named theta in Algorithm 2, Line 9).\nThe \u201cmask quota\u201d m denotes the number of tokens to be\nmasked in a subtree rooted at node t. The size of a subtree\ncorresponds to the number of tokens it encompasses, derived\nfrom the cumulative sizes of its children. For larger subtrees\nthat exceed the size threshold \u03b8, masking is applied recur-\nsively (Lines 9-13). Meanwhile, smaller subtrees undergo\na weighted shuffle, and the quota m is then apportioned\namong t\u2019s children in a greedy fashion according to the\nshuffled order (Lines 17-21). The weights for shuffling are\ndetermined by a heuristic function on the size of each child,\nsuch that masking probabilities are distributed uniformly\nacross leaf nodes. To create a subtree mask for an AST\nrooted at t with a mask ratio r (e.g., 15% or 25%), one can\nuse mask subtree(t, \u230a|t| \u00b7 r\u230b).\nThe parameter \u03b8 controls the granularity of masking. For\nexample, with \u03b8 = 5, the algorithm has a high probabil-\nity to mask individual tokens and short expressions. As\n\u03b8 increases to 20, the algorithm is more likely to mask\nlarger constructs such as statements. When \u03b8 = 100, the\nprobability increases for masking structures like loops or\nentire function bodies. To foster diverse training scenarios,\n\u03b8 is randomly sampled within a predefined range (e.g., 5 to\n100) for each training example. This allows the pretraining\nframework to inherently accommodate tasks as varied as\nsingle-token completion to full function body generation\nfrom a given signature.\nThe subtree masking strategy is the primary distinction be-\ntween our AST-Aware Subtree Corruption and the Vanilla\nT5 Span Corruption, as illustrated in Figure 1. While con-\nventional T5 variants mask random token spans, with an\naverage span length of 3 (Raffel et al., 2020) and neglecting\ncode structures, our method targets the masking of AST\nsubtrees, potentially encompassing up to 100 tokens. This\nequips AST-T5 for generation of various code structures\ncoherently.\nPretraining Objective.\nExcept for the strategy used to se-\nlect masked tokens and the segmentation strategy described\nin Section 3.2 , our approach adheres to the workflow de-\nscribed in Section 3.3. Once subtrees are selected for mask-\ning and replaced with sentinel tokens, the encoder processes\nthis modified input. Subsequently, the decoder is tasked\nwith reconstructing the original tokens within the masked\nsubtrees. A side-by-side comparison between our approach\nand the Vanilla Span Corruption in T5 is presented in Fig-\nure 1.\n4. Experimental Setup\nModel Architecture.\nAST-T5 has an architecture simi-\nlar to T5BASE (Raffel et al., 2020), comprising a 12-layer\nencoder and a 12-layer decoder, where each layer has 768\ndimensions and 12 attention heads. In total, the model has\n226M parameters.\nPretraining.\nAST-T5 is pretrained on a subset of The\nStack Dedup corpus (Kocetkov et al., 2022), a near-\ndeduplicated version of The Stack\u2014a 3.1TB collection of\npermissively licensed source code from GitHub cutoff at\nApril 2022, spanning 30 programming languages. For our\nexperiments, AST-T5\u2019s training involves Python, Java, C,\nC++, C#, and Markdown subsets, comprising a 588GB\n5\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\ndataset with 93M code and natural language files.\nEach file is first parsed into its AST using the Tree-Sitter\nmulti-language parser, and then tokenized with byte-level\nByte-Pair Encoding (BPE) using a 64k BPE token vocab-\nulary. Following AST-Aware Segmentation, these files are\npartitioned into chunks of 1,024 tokens. Our model is pre-\ntrained using the AST-Aware Subtree Corruption objective\nfor 524 billion tokens (1,024 tokens per sequence, 1,024\nsequences per batch, and 500k steps). For each training\nexample, we apply AST-Aware Subtree Corruption of it is\ncode, or apply Vanilla T5 Span Corruption of it is natural\nlanguage. For code, the threshold, \u03b8, is uniformly sam-\npled from 5 to 100. Pretraining uses PyTorch, Fairseq2 and\nFlashAttention (Dao et al., 2022) and is conducted on 8\nnodes, each with 8x NVIDIA A100 40GB GPUs. Further\npretraining hyperparameters are detailed in Appendix A.3.\nEvaluation.\nWe evaluate AST-T5 across three types\nof tasks: text-to-code generation, code-to-code transpila-\ntion, and code understanding (classification). Our eval-\nuation encompasses tasks from the CodeXGLUE meta-\nbenchmark (Lu et al., 2021) and also includes Hu-\nmanEval (Chen et al., 2021a) and MBPP (Austin et al.,\n2021). Specifically, for text-to-code generation, we assess\nperformance using HumanEval, MBPP, and Concode (Iyer\net al., 2018); for transpilation, we use CodeXGLUE Java-\nC# and Bugs2Fix (Tufano et al., 2019) for evaluation; and\nfor understanding, we use BigCloneBench (Svajlenko et al.,\n2014) and the Defect Detection task proposed by Zhou et al.\n(2019). Detailed metrics and statistics of these datasets are\nprovided in Table 1.\nWe finetune AST-T5 on the training datasets of all down-\nstream tasks, adhering to the methodology by Raffel et al.\n(2020). For the HumanEval task, which lacks its own train-\ning dataset, we use CodeSearchNet (Husain et al., 2020),\naligning with the approach of Wang et al. (2023). The\nprompt templates for finetuning are constructed using the\nPromptSource framework (Bach et al., 2022). The finetun-\ning takes 50k steps, with the peak learning rate set at 10%\nof the pretraining learning rate. All other hyperparameters\nfrom pretraining are retained without further adjustments,\nand we train only one finetuned model. During inference,\nrank classification is employed for code understanding tasks\nand beam search for generative tasks, following Sanh et al.\n(2021). We evaluate our model on the test set using five\nprompt templates for each task and report the average per-\nformance.\nBaselines.\nWe first benchmark AST-T5 against our own\nT5 baselines to ensure a controlled comparison. All models\nshare identical Transformer architectures, pretraining data,\n2https://github.com/facebookresearch/fairseq\nTable 1: Overview of our evaluation benchmarks about test\nset size, task type, and evaluation metric for each task. \u201cGen-\neration\u201d tasks involve mapping natural language to code,\n\u201cTranspilation\u201d tasks involve translating code from one pro-\ngramming language to another, and \u201cUnderstanding\u201d tasks\ninvolve classifying code into categorical labels. For MBPP,\nwe follow Nijkamp et al. (2023) and evaluate our model\non the entire \u201csanitized\u201d subset without few-shot prompts.\nFor evaluation metrics, \u201cPass@1\u201d indicates code execution\non unit-tests provided in the benchmark using a single gen-\nerated code per example, with reported pass rates. \u201cEM\u201d\n(Exact Match) evaluates textual equivalence without exe-\ncution by comparing two canonicalized code pieces. \u201cAcc\u201d\nmeans accuracy in classification tasks. We omit \u201cBLEU\nscores\u201d because high BLEU values (> 50) can still corre-\nspond to unexecutable or significantly flawed code (Lu et al.,\n2021), which is not useful in real-world applications. We\nalso discuss evaluation results using the CodeBLEU (Ren\net al., 2020) metric in Appendix A.5.\nSize\nType\nMetric\nHumanEval\n164\nGeneration\nPass@1\nMBPP\n427\nGeneration\nPass@1\nConcode\n2,000\nGeneration\nEM\nBugs2Fix\n12,379\nTranspilation\nEM\nJava-C#\n1,000\nTranspilation\nEM\nBigCloneBench\n415,416\nUnderstanding\nF1\nDefect Detect\n27,318\nUnderstanding\nAcc\nand computational settings, differing only in the use of AST-\nAware Segmentation and Subtree Corruption techniques by\nAST-T5. This setup directly evaluates the efficacy of our\nproposed methods.\nWe further benchmark AST-T5 against other language\nmodels for code-related tasks.\nThese include decoder-\nonly models such as the GPT variants (Brown et al.,\n2020; Chen et al., 2021a; Wang & Komatsuzaki, 2021),\nPaLM (Chowdhery et al., 2022), InCoder (Fried et al., 2023),\nand LLaMa (Touvron et al., 2023). We also compare with\nencoder-decoder models, including PLBART (Ahmad et al.,\n2021), CodeT5 (Wang et al., 2021), StructCoder (Tipirneni\net al., 2023), and CodeT5+ (Wang et al., 2023). Notably,\nCodeT5BASE and CodeT5+ (220M) closely resemble our\nmodel in terms of architecture and size, but AST-T5 distin-\nguishes itself with its AST-Aware pretraining techniques.\n5. Evaluation Results\nIn this section, we evaluate AST-T5 across multiple bench-\nmarks. First, we analyze the contributions of each compo-\nnent within our AST-aware pretraining framework through\ncontrolled experiments.\nNext, we benchmark AST-T5\n6\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\nTable 2: Performance comparison of various pretraining configurations for downstream tasks. Each row represents a\nsequential modification applied to the model in the previous row. Metrics include \u201cPass@1\u201d rate for HumanEval, \u201cExact\nMatch\u201d rate for CONCODE, Bugs2Fix (for \u201cSmall\u201d and \u201cMedium\u201d code lengths splits), and Java-C# transpilation (both\nJava-to-C# and C#-to-Java). F1 score is used for Clone Detection, and Accuracy for Defect Detection, consistent with prior\nstudies.\nGeneration\nTranspilation\nUnderstanding\nPretraining Config\nHumanEval\nConcode\nBugs2Fix\nJava-C#\nClone\nDefect\nAvg\nT5\n5.2\n18.3\n21.2/13.8\n65.5/68.4\n96.9\n64.1\n44.2\n+ AST. Segmentation\n7.2\n20.2\n22.5/15.1\n66.3/69.3\n98.3\n65.9\n45.7\n+ AST. Subtree Corrupt\n9.6\n22.1\n23.3/16.5\n67.3/72.2\n98.6\n66.0\n47.0\n+ Mask 25% (AST-T5)\n14.0\n22.9\n23.8/16.1\n68.9/72.3\n98.6\n65.8\n47.9\n+ Mask 50%\n14.3\n22.0\n21.9/15.0\n66.5/70.1\n97.1\n64.2\n46.4\nagainst existing models in prior work.\n5.1. Pretraining Procedure Analysis\nIn this subsection, we analyze the key components that\ncontribute to the pretraining of AST-T5 models. Holding the\nmodel architecture, pretraining datasets, and computational\nenvironment constant, we sequentially add one component\nat a time to a T5 baseline trained on code, culminating in our\nfinalized AST-T5 model. Table 2 presents the experimental\nresults. These results show that:\nAST-Aware Segmentation enhances code language mod-\nels.\nA comparison between the first two rows of Table 2\nshows that the model trained with AST-Aware Segmentation\nconsistently outperforms the T5 baseline that uses Greedy\nSegmentation across all tasks. The advantage stems from\nthe fact that AST-Aware Segmentation produces less frag-\nmented and thus less noisy training inputs during pretraining.\nGiven that most downstream tasks present coherent code\nstructures, such as entire function definitions, the consis-\ntency upheld by AST-Aware pretraining aligns better with\nthese structures, leading to improved generalization.\nAST-Aware Span Corruption further boosts generation\nperformance.\nA comparison between the second and third\nrows of Table 2 reveals an improvement when shifting from\nVanilla T5 Span Corruption to our AST-Aware Subtree Cor-\nruption. This performance gain is especially notable in\ngeneration and transpilation tasks. Such enhancements stem\nfrom the ability of AST-Aware Subtree Corruption to guide\nthe model in generating code with better coherence and\nstructural integrity.\nIncreasing masking ratio improves generation perfor-\nmance.\nThe typical span corruption mask ratio in T5 is\nset at 15%. Increasing this ratio could potentially enhance\nthe model\u2019s generation capabilities, albeit potentially at the\nexpense of understanding tasks. Essentially, a mask ratio of\n100% would emulate a GPT-like, decoder-only Transformer.\nHowever, in our experiments (last two rows of Table 2),\nwe observed that raising the mask ratio from 15% to 25%\nsignificantly improved generation capabilities without no-\nticeably compromising performance in understanding tasks.\nFurther analysis shows that increasing the masking ratio to\n50% yields only a marginal improvement on HumanEval\n(from 14.0 to 14.3), while adversely impacting transpilation\nand understanding tasks. Thus, we settled on a 25% mask\nratio for our AST-T5 model.\n5.2. Main Results\nTable 3 shows AST-T5\u2019s performance on downstream tasks\ncompared with previously published results of similarly\nsized models, specifically those within the \u201cBase\u201d scale\n(110M to 230M parameters). Figure 3a and Figure 3b\nextends this comparison, comparing AST-T5 with larger\nmodels using the HumanEval benchmark and the MBPP\nbenchmark, respectively. These results show that:\nAST-T5 excels as a unified and parameter-efficient LM\nfor various code-related tasks.\nWhile comparable in size,\nAST-T5 consistently outperforms similar-sized models such\nas CodeT5 (Wang et al., 2021) and CodeT5+ (Wang et al.,\n2023) in code generation, transpilation, and understanding.\nNotably, while CodeT5 and CodeT5+ are models at the Base\nscale, they were evaluated across different tasks. Our model,\nAST-T5, outperforms the best results of these two models\nacross multiple benchmarks at the same time. Moreover,\nFigure 3a highlights AST-T5\u2019s competitiveness against sig-\nnificantly larger models like GPT-J (Wang & Komatsuzaki,\n2021) and LLaMa-7B (Touvron et al., 2023) on the Hu-\nmanEval benchmark, underscoring our model\u2019s parameter\nefficiency. Similarly, Figure 3b demonstrates AST-T5\u2019s\nadvantages over LLaMa-7B and Codex-2.5B (Chen et al.,\n7\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\nTable 3: Results of AST-T5 on downstream tasks compared with reported results of established language models. Evaluation\nmetrics align with those in Table 1. Our focus is primarily on models with similar sizes as AST-T5, specifically the \u201cBase\u201d\nmodels (110M to 230M parameters), while comparisons against larger models are depicted in Figure 3. Some models are\neither encoder-only or decoder-only and are thus not suited for certain tasks. These results are labeled with \u201cN/A\u201d in this\ntable because they are not available in the literature.\nGeneration\nTranspilation\nUnderstanding\nModel\nHumanEval\nConcode\nBugs2Fix\nJava-C#\nClone\nDefect\nCodeBERT\nN/A\nN/A\n16.4 / 5.2\n59.0/58.8\n96.5\n62.1\nGraphCodeBERT\nN/A\nN/A\n17.3 / 9.1\n59.4/58.8\n97.1\nN/A\nPLBART\nN/A\n18.8\n19.2 / 9.0\n64.6/65.0\n97.2\n63.2\nCodeT5\nN/A\n22.3\n21.6/14.0\n65.9/66.9\n97.2\n65.8\nCodeT5+BASE\n12.0\nN/A\nN/A\nN/A\n95.2\n66.1\nStructCoder\nN/A\n22.4\nN/A\n66.9/68.7\nN/A\nN/A\nAST-T5 (Ours)\n14.0\n22.9\n23.8/16.1\n68.9/72.3\n98.6\n65.8\n1B\n10B\n100B\nNum of Parameters\n5\n10\n20\n40\nPass@1 Rate\nAST-T5 (Ours)\nPaLM-8B\nPaLM-62B\nPaLM-540B\nCodex-2.5B\nCodex-12B\nGPT-3.5\nGPT-Neo\nGPT-J\nInCoder-1.3B\nInCoder-6B\nLLaMA-7B\nLLaMA-13B\nLLaMA-33B\nLLaMA-65B\nProprietary\nOpen-Source\n1\n(a) HumanEval\n1B\n10B\n100B\nNum of Parameters\n5\n10\n20\n40\nPass@1 Rate\nAST-T5 (Ours)\nPaLM-8B\nPaLM-62B\nPaLM-540B\nCodeGen-Multi-350M\nCodeGen-Mono-350M\nCodex-2.5B\nCodex-12B\nInCoder-1.3B\nInCoder-6B\nLaMDA-137B\nLLaMA-7B\nLLaMA-13B\nLLaMA-33B\nLLaMA-65B\nProprietary\nOpen-Source\n1\n(b) MBPP\nFigure 3: Visualizations of AST-T5\u2019s performance on HumanEval and MBPP compared to other models compared to models\nexceeding 230M parameters. Each point on each scatter plot represents a model. The x-axis shows the parameter count\nin log-scale, while the y-axis shows the Pass@1 rate on HumanEval or MBPP in log-scale. Model open-source status is\ncolor-coded: blue for open-source and red for proprietary.\n2021a) on the MBPP benchmark, showing the effectiveness\nof AST-T5.\nAST-T5 exhibits unique strengths in transpilation\nthrough AST-awareness.\nTable 3 highlights AST-T5\u2019s\nsuperior performance in code-to-code transpilation tasks,\nshowcasing gains a substantial gain of 2 to 5 points on\nBugs2Fix and Java-C# transpilation. In transpilation, while\nsurface-level code can exhibit significant variability, the\nintrinsic AST structures of the source and target often main-\ntain a notable similarity. The capability of AST-T5 to exploit\nthis structural similarity is crucial to its effectiveness. The\nbenefits of being structure-aware are further exemplified\nby AST-T5\u2019s leading results in Clone Detection, where it\nsurpasses CodeT5 by 3 points, because AST comparisons\nyield more precise insights than direct code comparisons.\n6. Conclusion and Future Work\nIn this work, we present AST-T5, a novel pretraining\nparadigm that harnesses the power of Abstract Syntax Trees\n(ASTs) to boost the performance of code-centric language\nmodels. Using two structure-aware techniques, AST-T5\nnot only outperforms models of comparable size but also\ncompetes favorably against some larger counterparts. The\nsimplicity of AST-T5 lies in its singular pretraining ob-\njective and its adaptability as a drop-in replacement for\nany encoder-decoder LM, highlighting its potential for real-\nworld deployments. Moving forward, we aim to explore the\nscalability of AST-T5 by training larger models on more\nexpansive datasets.\n8\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\nBroader Impact\nIn this paper, we introduce AST-T5, a language model aimed\nat automated generation, transpilation, and understanding\nof code. The advancement of LLMs in code generation\nraises concerns about automated code production\u2019s secu-\nrity, privacy, and potential misuse. There is a risk that\nimproved code generation capabilities could be exploited\nfor malicious purposes, such as automating the creation of\nsoftware vulnerabilities or facilitating the development of\nharmful software. Our research emphasizes the importance\nof responsible AI development and use, advocating for con-\ntinuous monitoring, ethical guidelines, and safeguards to\nmitigate these risks.\nReferences\nAhmad, W. U., Chakraborty, S., Ray, B., and Chang,\nK.-W. Unified pre-training for program understanding\nand generation. Apr 2021. doi: 10.48550/arXiv.2103.\n06333.\nURL http://arxiv.org/abs/2103.06333.\narXiv:2103.06333 [cs].\nAllamanis, M., Brockschmidt, M., and Khademi, M.\nLearning to represent programs with graphs.\nNov\n2017.\nURL https://arxiv.org/abs/1711.00740.\narXiv:1711.00740 [cs].\nAlon, U., Sadaka, R., Levy, O., and Yahav, E. Structural\nlanguage models of code. July 2020. doi: 10.48550/\narXiv.1910.00577. URL http://arxiv.org/abs/1910.\n00577. arXiv:1910.00577 [cs, stat].\nAthiwaratkun, B., Gouda, S. K., Wang, Z., Li, X.,\nTian, Y., Tan, M., Ahmad, W. U., Wang, S., Sun,\nQ., Shang, M., Gonugondla, S. K., Ding, H., Ku-\nmar, V., Fulton, N., Farahani, A., Jain, S., Giaquinto,\nR., Qian, H., Ramanathan, M. K., Nallapati, R., Ray,\nB., Bhatia, P., Sengupta, S., Roth, D., and Xiang, B.\nMulti-lingual evaluation of code generation models.\n(arXiv:2210.14868), March 2023. URL http://arxiv.\norg/abs/2210.14868. arXiv:2210.14868 [cs].\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and\nSutton, C. Program synthesis with large language models.\nAug 2021. doi: 10.48550/arXiv.2108.07732. URL http:\n//arxiv.org/abs/2108.07732. arXiv:2108.07732 [cs].\nBach, S. H., Sanh, V., Yong, Z.-X., Webson, A., Raffel, C.,\nNayak, N. V., Sharma, A., Kim, T., Bari, M. S., Fevry, T.,\nAlyafeai, Z., Dey, M., Santilli, A., Sun, Z., Ben-David,\nS., Xu, C., Chhablani, G., Wang, H., Fries, J. A., Al-\nshaibani, M. S., Sharma, S., Thakker, U., Almubarak, K.,\nTang, X., Radev, D., Jiang, M. T.-J., and Rush, A. M.\nPromptSource: An integrated development environment\nand repository for natural language prompts. March 2022.\ndoi: 10.48550/arXiv.2202.01279. URL http://arxiv.\norg/abs/2202.01279. arXiv:2202.01279 [cs].\nBigScience.\nBigscience Language Open-science Open-\naccess Multilingual (BLOOM), May 2021. URL https:\n//huggingface.co/bigscience/bloom.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. Jul 2020. doi: 10.48550/\narXiv.2005.14165. URL http://arxiv.org/abs/2005.\n14165. arXiv:2005.14165 [cs].\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brock-\nman, G., Ray, A., Puri, R., Krueger, G., Petrov, M.,\nKhlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S.,\nRyder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian,\nM., Winter, C., Tillet, P., Such, F. P., Cummings, D.,\nPlappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A.,\nGuss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,\nJ., Babuschkin, I., Balaji, S., Jain, S., Saunders, W.,\nHesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,\nV., Morikawa, E., Radford, A., Knight, M., Brundage,\nM., Murati, M., Mayer, K., Welinder, P., McGrew, B.,\nAmodei, D., McCandlish, S., Sutskever, I., and Zaremba,\nW. Evaluating large language models trained on code.\nJul 2021a. doi: 10.48550/arXiv.2107.03374. URL http:\n//arxiv.org/abs/2107.03374. arXiv:2107.03374 [cs].\nChen, X., Liu, C., and Song, D. Execution-guided neu-\nral program synthesis.\nSep 2018.\nURL https://\nopenreview.net/forum?id=H1gfOiAqYm.\nChen, X., Song, D., and Tian, Y. Latent execution for neural\nprogram synthesis. Jun 2021b. URL https://arxiv.\norg/abs/2107.00101. arXiv:2107.00101 [cs].\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,\nN., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,\nG., Yin, P., Duke, T., Levskaya, A., Ghemawat, S.,\nDev, S., Michalewski, H., Garcia, X., Misra, V., Robin-\nson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D.,\nLim, H., Zoph, B., Spiridonov, A., Sepassi, R., Do-\nhan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai,\nT. S., Pellat, M., Lewkowycz, A., Moreira, E., Child,\n9\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\nR., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta,\nB., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-\nHellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel,\nN. PaLM: Scaling language modeling with pathways.\nOct 2022. doi: 10.48550/arXiv.2204.02311. URL http:\n//arxiv.org/abs/2204.02311. arXiv:2204.02311 [cs].\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C.\nFlashAttention: Fast and memory-efficient exact atten-\ntion with IO-awareness.\nJune 2022.\ndoi: 10.48550/\narXiv.2205.14135. URL http://arxiv.org/abs/2205.\n14135. arXiv:2205.14135 [cs].\nFeng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong,\nM., Shou, L., Qin, B., Liu, T., Jiang, D., and Zhou,\nM. CodeBERT: A pre-trained model for programming\nand natural languages.\nSep 2020.\ndoi: 10.48550/\narXiv.2002.08155. URL http://arxiv.org/abs/2002.\n08155. arXiv:2002.08155 [cs].\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace,\nE., Shi, F., Zhong, R., Yih, W.-t., Zettlemoyer, L., and\nLewis, M.\nInCoder: A generative model for code\ninfilling and synthesis.\nApr 2023.\ndoi: 10.48550/\narXiv.2204.05999. URL http://arxiv.org/abs/2204.\n05999. arXiv:2204.05999 [cs].\nGuo, D., Ren, S., Lu, S., Feng, Z., Tang, D., Liu, S., Zhou,\nL., Duan, N., Svyatkovskiy, A., Fu, S., Tufano, M., Deng,\nS. K., Clement, C., Drain, D., Sundaresan, N., Yin, J.,\nJiang, D., and Zhou, M. GraphCodeBERT: Pre-training\ncode representations with data flow.\nSep 2021.\ndoi:\n10.48550/arXiv.2009.08366. URL http://arxiv.org/\nabs/2009.08366. arXiv:2009.08366 [cs].\nHusain, H., Wu, H.-H., Gazit, T., Allamanis, M., and\nBrockschmidt, M.\nCodeSearchNet challenge: Evalu-\nating the state of semantic code search. Jun 2020. doi:\n10.48550/arXiv.1909.09436. URL http://arxiv.org/\nabs/1909.09436. arXiv:1909.09436 [cs, stat].\nIyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L. Map-\nping language to code in programmatic context. Aug\n2018.\ndoi: 10.48550/arXiv.1808.09588.\nURL http:\n//arxiv.org/abs/1808.09588. arXiv:1808.09588 [cs].\nKim, S., Zhao, J., Tian, Y., and Chandra, S. Code predic-\ntion by feeding trees to transformers. March 2021. doi:\n10.48550/arXiv.2003.13848. URL http://arxiv.org/\nabs/2003.13848. arXiv:2003.13848 [cs].\nKocetkov, D., Li, R., Allal, L. B., Li, J., Mou, C., Ferrandis,\nC. M., Jernite, Y., Mitchell, M., Hughes, S., Wolf, T.,\nBahdanau, D., von Werra, L., and de Vries, H. The\nStack: 3 TB of permissively licensed source code.\n(arXiv:2211.15533), November 2022. doi: 10.48550/\narXiv.2211.15533. URL http://arxiv.org/abs/2211.\n15533. arXiv:2211.15533 [cs].\nLachaux, M.-A., Roziere, B., Chanussot, L., and Lample,\nG. Unsupervised translation of programming languages.\nSep 2020. doi: 10.48550/arXiv.2006.03511. URL http:\n//arxiv.org/abs/2006.03511. arXiv:2006.03511 [cs].\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-\nhamed, A., Levy, O., Stoyanov, V., and Zettlemoyer,\nL. BART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension.\nOct 2019.\ndoi:\n10.48550/arXiv.1910.\n13461.\nURL http://arxiv.org/abs/1910.13461.\narXiv:1910.13461 [cs, stat].\nLi, J., Wang, Y., Lyu, M. R., and King, I.\nCode com-\npletion with neural attention and pointer networks. In\nProceedings of the Twenty-Seventh International Joint\nConference on Artificial Intelligence, pp. 4159\u20134165,\nJuly 2018. doi: 10.24963/ijcai.2018/578. URL http:\n//arxiv.org/abs/1711.09573. arXiv:1711.09573 [cs].\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen,\nD., Levy, O., Lewis, M., Zettlemoyer, L., and Stoy-\nanov, V.\nRoBERTa:\nA robustly optimized BERT\npretraining approach.\nJul 2019.\ndoi:\n10.48550/\narXiv.1907.11692. URL http://arxiv.org/abs/1907.\n11692. arXiv:1907.11692 [cs].\nLu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A.,\nBlanco, A., Clement, C., Drain, D., Jiang, D., Tang, D.,\nLi, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong,\nM., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K.,\nFu, S., and Liu, S. CodeXGLUE: A machine learning\nbenchmark dataset for code understanding and generation.\nMar 2021. doi: 10.48550/arXiv.2102.04664. URL http:\n//arxiv.org/abs/2102.04664. arXiv:2102.04664 [cs].\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H.,\nZhou, Y., Savarese, S., and Xiong, C.\nCodeGen:\nAn open large language model for code with multi-\nturn program synthesis.\nFeb 2023.\ndoi: 10.48550/\narXiv.2203.13474. URL http://arxiv.org/abs/2203.\n13474. arXiv:2203.13474 [cs].\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,\nC. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K.,\nRay, A., Schulman, J., Hilton, J., Kelton, F., Miller,\nL., Simens, M., Askell, A., Welinder, P., Christiano,\nP., Leike, J., and Lowe, R.\nTraining language mod-\nels to follow instructions with human feedback. Mar\n2022.\ndoi: 10.48550/arXiv.2203.02155.\nURL http:\n//arxiv.org/abs/2203.02155. arXiv:2203.02155 [cs].\nRabinovich, M., Stern, M., and Klein, D. Abstract syntax\nnetworks for code generation and semantic parsing. April\n2017.\ndoi: 10.48550/arXiv.1704.07535.\nURL http:\n//arxiv.org/abs/1704.07535. arXiv:1704.07535 [cs,\nstat].\n10\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,\nS., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Ex-\nploring the limits of transfer learning with a unified\ntext-to-text transformer.\nJul 2020.\ndoi: 10.48550/\narXiv.1910.10683. URL http://arxiv.org/abs/1910.\n10683. arXiv:1910.10683 [cs, stat].\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D.,\nSundaresan, N., Zhou, M., Blanco, A., and Ma, S.\nCodeBLEU: a method for automatic evaluation of code\nsynthesis. (arXiv:2009.10297), September 2020. doi:\n10.48550/arXiv.2009.10297. URL http://arxiv.org/\nabs/2009.10297. arXiv:2009.10297 [cs].\nRoziere, B., Lachaux, M.-A., Szafraniec, M., and Lam-\nple, G. DOBF: A deobfuscation pre-training objective\nfor programming languages. Oct 2021. doi: 10.48550/\narXiv.2102.07492. URL http://arxiv.org/abs/2102.\n07492. arXiv:2102.07492 [cs].\nRozi`ere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat,\nI., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J.,\nKozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Fer-\nrer, C. C., Grattafiori, A., Xiong, W., D\u00b4efossez, A.,\nCopet, J., Azhar, F., Touvron, H., Martin, L., Usunier,\nN., Scialom, T., and Synnaeve, G. Code llama: Open\nfoundation models for code. Aug 2023. doi: 10.48550/\narXiv.2308.12950. URL http://arxiv.org/abs/2308.\n12950. arXiv:2308.12950 [cs].\nSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,\nAlyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja,\nA., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma,\nS. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N.,\nDatta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica,\nM., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang,\nT., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry,\nT., Fries, J. A., Teehan, R., Bers, T., Biderman, S., Gao, L.,\nWolf, T., and Rush, A. M. Multitask prompted training\nenables zero-shot task generalization.\narXiv.org, Oct\n2021. URL https://arxiv.org/abs/2110.08207v3.\nShojaee, P., Jain, A., Tipirneni, S., and Reddy, C. K.\nExecution-based code generation using deep reinforce-\nment learning. Jan 2023. URL https://arxiv.org/\nabs/2301.13816. arXiv:2301.13816 [cs].\nSvajlenko, J., Islam, J. F., Keivanloo, I., Roy, C. K., and\nMia, M. M. Towards a big data curated benchmark of\ninter-project code clones. In 2014 IEEE International\nConference on Software Maintenance and Evolution, pp.\n476\u2013480, Sep 2014. doi: 10.1109/ICSME.2014.77.\nTipirneni, S., Zhu, M., and Reddy, C. K.\nStructCoder:\nStructure-aware transformer for code generation. May\n2023.\ndoi: 10.48550/arXiv.2206.05239.\nURL http:\n//arxiv.org/abs/2206.05239. arXiv:2206.05239 [cs].\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro,\nE., Azhar, F., Rodriguez, A., Joulin, A., Grave, E.,\nand Lample, G.\nLLaMA: Open and efficient foun-\ndation language models.\nFeb 2023.\ndoi: 10.48550/\narXiv.2302.13971. URL http://arxiv.org/abs/2302.\n13971. arXiv:2302.13971 [cs].\nTufano, M., Watson, C., Bavota, G., Di Penta, M., White,\nM., and Poshyvanyk, D. An empirical study on learn-\ning bug-fixing patches in the wild via neural machine\ntranslation.\nMay 2019.\ndoi: 10.48550/arXiv.1812.\n08693.\nURL http://arxiv.org/abs/1812.08693.\narXiv:1812.08693 [cs].\nWang,\nB.\nand\nKomatsuzaki,\nA.\nGPT-J-6B:\n6B\nJAX-based\nTransformer,\nJun\n2021.\nURL\nhttps://arankomatsuzaki.wordpress.com/2021/\n06/04/gpt-j/.\nWang, Y., Wang, W., Joty, S., and Hoi, S. C. H. CodeT5:\nIdentifier-aware unified pre-trained encoder-decoder mod-\nels for code understanding and generation. Sep 2021. doi:\n10.48550/arXiv.2109.00859. URL http://arxiv.org/\nabs/2109.00859. arXiv:2109.00859 [cs].\nWang, Y., Le, H., Gotmare, A. D., Bui, N. D. Q., Li, J.,\nand Hoi, S. C. H. CodeT5+: Open code large language\nmodels for code understanding and generation.\nMay\n2023.\ndoi: 10.48550/arXiv.2305.07922.\nURL http:\n//arxiv.org/abs/2305.07922. arXiv:2305.07922 [cs].\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mi-\nhaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig,\nD., Koura, P. S., Sridhar, A., Wang, T., and Zettle-\nmoyer, L.\nOPT: Open pre-trained transformer lan-\nguage models.\n(arXiv:2205.01068), June 2022.\ndoi:\n10.48550/arXiv.2205.01068. URL http://arxiv.org/\nabs/2205.01068. arXiv:2205.01068 [cs].\nZhou, Y., Liu, S., Siow, J., Du, X., and Liu, Y. Devign:\nEffective vulnerability identification by learning compre-\nhensive program semantics via graph neural networks.\nSep 2019. doi: 10.48550/arXiv.1909.03496. URL http:\n//arxiv.org/abs/1909.03496. arXiv:1909.03496 [cs,\nstat].\nZ\u00a8ugner, D., Kirschstein, T., Catasta, M., Leskovec, J., and\nG\u00a8unnemann, S. Language-agnostic representation learn-\ning of source code from structure and context. March\n2021.\ndoi: 10.48550/arXiv.2103.11318.\nURL http:\n//arxiv.org/abs/2103.11318. arXiv:2103.11318 [cs].\n11\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\nA. Appendix\nA.1. Limitations\nAST-T5 is specifically designed to enhance code generation performance by exclusively masking code within AST subtrees\nduring pretraining. While this specialized approach is advantageous for code generation tasks, it may result in suboptimal\nperformance in natural language generation. Acknowledging this limitation, future versions of AST-T5 could investigate\nstrategies such as masking docstrings and comments to broaden its applicability. This would potentially improve performance\nacross various tasks, including code summarization.\nA.2. More about AST-Aware Segmentation\nIn Section 3.2, we use a dynamic programming algorithm to calculate the segmentation that results in the least number of\nAST structure breaks. A naive implementation of the DP algorithm is shown in Algorithm 3.\nAlgorithm 3 Dynamic Programming in AST-Aware Segmentation (Before Optimization)\n1\nfor k in range(1, m + 1):\n2\nfor i in range(1, n + 1):\n3\nbest_j = i - max_len\n4\nfor j in range(i - max_len + 1, i):\n5\nif dp[k - 1, j] < dp[k - 1, best_j]:\n6\nbest_j = j\n7\nprev[k, i] = best_j\n8\ndp[k, i] = cost[i] + min_value\nDenote the length of the code file (in tokens) by n. In the algorithm, m denotes the maximum number of chunks that the file\ncan be split into, which is approximately n/max len. So this implementation has time complexity O(mn \u00b7 max len) =\nO(n2), which is not feasible for longer code files. To optimize this algorithm, we use a monotonic queue to compute the\nsliding-window minimum, as described in Algorithm 1.\nEach element is only pushed into and popped out of the monotonic queue once, so the time complexity of the optimized\nalgorithm is O(nm) = O(n2/max len), making the algorithm \u223c 1000x faster when max len = 1024. This allows the\nalgorithm to segment each code file with 100k tokens in milliseconds.\nA.3. Pretraining Hyperparameters\nTable 4 shows the pretraining hyperparameters for our proposed AST-T5 model.\nA.4. Evaluation Results on Multi-Lingual Code Generation\nTable 5 presents a comparative analysis of our AST-T5 model on Python and Java subsets of the multi-lingual HumanEval\nand MBXP benchmarks (Athiwaratkun et al., 2023). This analysis includes models such as BLOOM (BigScience, 2021),\nOPT (Zhang et al., 2022), and various configurations of CodeGen (Nijkamp et al., 2023), as reported in Athiwaratkun et al.\n(2023). Our results show AST-T5\u2019s superior performance across all benchmarks compared to the CodeGen-multi-350M.\nFurthermore, AST-T5, having 226M parameters, outperforms larger counterparts like BLOOM-7.1B and OPT-13B.\nA.5. Evaluation Results in CodeBLEU\nTable 6 presents the performance of various models on the Concode dataset using the CodeBLEU metric, as reported in\n(Wang et al., 2021). CodeBLEU, specifically designed for evaluating code synthesis, computes a weighted average of three\nscores: textual match (BLEU), AST match, and Data Flow Graph (DFG) match. Our findings show a clear correlation\nbetween CodeBLEU and exact match scores.\n12\nAST-T5: Structure-Aware Pretraining for Code Generation and Understanding\nEncoder Layers\n12\nDecoder Layers\n12\nHidden Dimension\n768\nPeak Learning Rate\n2e-4\nBatch Size\n1,024\nWarm-Up Steps\n10,000\nTotal Steps\n500,000\nSequence Length\n1,024\nMask Ratio\n25%\nMin Subtree Corruption Threshold \u03b8\n5\nMax Subtree Corruption Threshold \u03b8\n100\nRelative Position Encoding Buckets\n32\nRelative Position Encoding Max Distance\n128\nAdam \u03f5\n1e-6\nAdam (\u03b21, \u03b22)\n(0.9, 0.98)\nClip Norm\n2.0\nDropout\n0.1\nWeight Decay\n0.01\nTable 4: Pretraining hyperparameters for our AST-T5 model.\nTable 5: Results of AST-T5 on multi-lingual HumanEval and MBXP compared with reported results of established language\nmodels. The evaluation metric is Pass@1.\n#Params\nHumanEval\nMBXP\nPython\nJava\nPython\nJava\nCodeGen-multi\n350M\n7.3\n5.0\n7.5\n8.2\nCodeGen-mono\n350M\n10.3\n3.1\n14.6\n1.9\nAST-T5 (Ours)\n226M\n14.0\n10.6\n23.9\n9.8\nBLOOM\n7.1B\n7.9\n8.1\n7.0\n7.8\nOPT\n13B\n0.6\n0.6\n1.4\n1.4\nCodeGen-multi\n2B\n11.0\n11.2\n18.8\n19.5\nCodeGen-mono\n2B\n20.7\n5.0\n31.7\n16.7\nCodeGen-multi\n6B\n15.2\n10.6\n22.5\n21.7\nCodeGen-mono\n6B\n19.5\n8.7\n37.2\n19.8\nCodeGen-multi\n16B\n17.1\n16.2\n24.2\n28.0\nCodeGen-mono\n16B\n22.6\n22.4\n40.6\n26.8\nTable 6: Results of AST-T5 on CONCODE with reported results of established language models. The evaluation metric is\nexact match score and CodeBLEU.\nEM\nCodeBLEU\nGPT-2\n17.4\n29.7\nCodeGPT-2\n18.3\n32.7\nCodeGPT-adapted\n20.1\n36.0\nPLBART\n18.8\n38.5\nCodeT5-Small\n21.6\n41.4\nCodeT5-Base\n22.3\n43.2\nAST-T5 (Ours)\n22.9\n45.0\n13\n"
  },
  {
    "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution",
    "link": "https://arxiv.org/pdf/2401.03065.pdf",
    "upvote": "10",
    "text": "CRUXEval: A Benchmark for Code Reasoning,\nUnderstanding and Execution\nAlex Gu\u22c6\ngua@mit.edu\nMIT CSAIL\nBaptiste Rozi`ere\nbroz@meta.com\nMeta AI\nHugh Leather\nhleather@meta.com\nMeta AI\nArmando Solar-Lezama\nasolar@csail.mit.edu\nMIT CSAIL\nGabriel Synnaeve\ngab@meta.com\nMeta AI\nSida I. Wang\nsida@meta.com\nMeta AI\nAbstract\nWe present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation),\na benchmark consisting of 800 Python functions (3-13 lines). Each function comes\nwith an input-output pair, leading to two natural tasks: input prediction and output\nprediction. First, we propose a generic recipe for generating our execution benchmark\nwhich can be used to create future variation of the benchmark. Second, we evaluate\ntwenty code models on our benchmark and discover that many recent high-scoring\nmodels on HumanEval do not show the same improvements on our benchmark. Third,\nwe show that simple CoT and fine-tuning schemes can improve performance on our\nbenchmark but remain far from solving it. The best setup, GPT-4 with chain of thought\n(CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively.\nIn contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output\nprediction, highlighting the gap between open and closed source models. As no model\nis close to acing CRUXEval, we provide examples of consistent GPT-4 failures on simple\nprograms as a lens into its code reasoning capabilities and areas for improvement.\n1\nIntroduction\nIn recent months, software engineering and programming have become increasingly mainstream\ndomains for language models (LMs) as they attempt to conquer a potpourri of tasks including\n\u22c6 Work primarily done during an internship at Meta AI\n1\narXiv:2401.03065v1  [cs.SE]  5 Jan 2024\ncode completion, program repair, debugging, test case generation, and code optimization (see Zan\net al. (2023) and Fan et al. (2023) for surveys). Recent models including Code Llama (Roziere et al.,\n2023), GPT-3.5 (Brown et al., 2020; Ouyang et al., 2022), and GPT-4 (OpenAI, 2023) have shown\npromise in code-related tasks and are being used to develop tools to help programmers write code\nmore efficiently.\nThe primary way that the community has been evaluating code LMs is via benchmarks such as\nHumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), which test the ability to generate\nshort code snippets from natural language specifications. While HumanEval and MBPP capture\ncode generation abilities on simple and fundamental tasks, there is an absence of benchmarks\ncapturing other fundamental dimensions of code LMs such as code understanding and execution.\nMotivated by this, we contribute a new benchmark, CRUXEval (Code Reasoning, Understanding,\nand eXecution Evaluation) with two tasks: 1) output prediction, CRUXEval-O to measure code\nexecution following and 2) input prediction, CRUXEval-I to measure code reasoning and un-\nderstanding. An example of a sample in CRUXEval is shown in Listings 1 and 2 (modified for\nreadability). CRUXEval examines the abilities of code LMs to reason about the execution behaviour\nof simple Python programs. While LMs shouldn\u2019t be expected to replace an interpreter on arbitrarily\ncomplex problems, we ensure the samples in our benchmark are simple (maximum 13 lines, no\ncomplex arithmetic) and solvable by a university-level CS graduate without needing more memory\n(in our opinion). CRUXEval provides a useful and important probe for better understanding the\ncapabilities of code LMs, as following a few simple steps of code execution should be a basic\nrequirement for these models. The ability to reason about the execution behavior of code also\npaves the way to tackling more difficult tasks such as code repair with execution feedback and\ncode summarization.\nListing 1: Sample problem\ndef f(string):\nstring_x = string.rstrip(\"a\")\nstring = string_x.rstrip(\"e\")\nreturn string\n# output prediction, CRUXEval-O\nassert f(\"xxxxaaee\") == ??\n## GPT4: \"xxxx\", incorrect\n# input prediction, CRUXEval-I\nassert f(??) == \"xxxxaa\"\n## GPT4: \"xxxxaae\", correct\nListing 2: Sample problem\ndef f(nums):\ncount = len(nums)\nfor i in range(-count+1, 0):\nnums.append(nums[i])\nreturn nums\n# output prediction, CRUXEval-O\nassert f([2, 6, 1, 3, 1]) == ??\n# GPT4: [2, 6, 1, 3, 1, 6, 1, 3, 1], incorrect\n# input prediction, CRUXEval-I\nassert f(??) == [2, 6, 1, 3, 1, 6, 3, 6, 6]\n# GPT4: [2, 6, 1], incorrect\nAt a high level, our benchmark is constructed as follows. First, we use Code Llama 34B to generate\na large set of functions and inputs. The outputs are generated by executing the functions on the\ninputs. Second, we filter the set so that our benchmark only consists of short problems with low\ncomputation and memory requirements, problems which a good human programmer should be\nable to do without extra memory in a minute or so. Third, we randomly select 800 samples passing\nthe filter, ensuring the benchmark is both small enough to easily run but large enough to reliably\nsee performance differences among various models. We use this approach because while it is\ndifficult to manually come up with example where the strongest models like GPT-4 fail completely,\nwe observe that they fail quite often on random yet reasonable programs. We also highlight that as\nmodels improve, this generate-and-filter approach can be used to create future benchmarks that\nare more difficult and test different aspects of program execution.\n2\nThe best model, GPT-4, achieves a pass@1 of 67% on CRUXEval-I and 63% on CRUXEval-O. In\ncontrast, the best open-source models only achieve 47% on CRUXEval-I and 44% on CRUXEval-O,\nfailing over half the time at simple execution prediction and code reasoning despite being trained\non 100G of Python code and 1T of code data. We also observe that for base models, stronger\nHumanEval performance correlates with stronger CRUXEval performance. However, this trend\nbreaks down for models distilled on GPT-4 like data such as WizardCoder, Phind, and Phi, which\nhave impressively high HumanEval scores but no better than CodeLlama on CRUXEval.\nWe also observe that CoT and fine-tuning on input-output assertions are effective techniques for\nimproving performance on CRUXEval, but are far from enough to ace it. Overall, our benchmark\nreveals that the gap between GPT-4 and open source models reflects GPT-4\u2019s stronger ability\nto reason about the behavior of code. As existing benchmarks like HumanEval and MBPP are\ninsufficient for measuring code understanding and execution ability, capturing it through our\nbenchmark is critical to make progress towards closing the gap between open models and GPT-4.\nFinally, we discover that despite its impressive abilities, GPT-4 consistently fails to understand the\nexecution behavior of some surprisingly simple Python programs.\n2\nRelated Work\nLMs for Code Generation: There have been many efforts training LMs to generate code. Base\nmodels include Codex (Chen et al., 2021), CodeGeeX (Zheng et al., 2023), SantaCoder (Allal et al.,\n2023), PolyCoder (Xu et al., 2022), InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022),\nStarCoder (Li et al., 2023a), DeepSeek-Coder (AI, 2023), and Code Llama (Roziere et al., 2023).\nLater, some of these models were fine-tuned on instruction-like data distilled from GPT-3.5 and\nGPT-4, resulting in models like Phind (Royzen et al., 2023), WizardCoder (Luo et al., 2023), and\nPhi-1/Phi-1.5 (Li et al., 2023b; Gunasekar et al., 2023). We evaluate the performance of a selection\nof these models on our CRUXEval.\nBenchmarks for Evaluating Code LMs: There are various benchmarks serving to evaluate different\naspects of these code LMs. We survey a handful here and refer readers to the survey (Zhang\net al., 2023h) for more. HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) evaluate\nPython code generation on relatively simple functions. HumanEval+ (Liu et al., 2023c) augments\nHumanEval with better test cases after discovering many passing solutions are incorrect. ReCode\n(Wang et al., 2022a) is a variant of HumanEval with perturbed function names and docstrings.\nHumanEval-X (Zheng et al., 2023), MultiPLe (Cassano et al., 2022), and MBXP (Athiwaratkun et al.,\n2022) are extensions of HumanEval and MBPP with a focus on including programming languages\noutside of Python. APPS (Hendrycks et al., 2021), CodeContests (Li et al., 2022), and LeetCode-\nHard (Shinn et al., 2023) evaluate code generation on more difficult, interview or competition style\nproblems.\nThere are also benchmarks to evaluate code generation in data science applications, such as DS-1000\n(Lai et al., 2023), ARCADE (Yin et al., 2022), NumpyEval (Zhang et al., 2023b), and PandasEval (Jain\net al., 2022). Going one step further, some benchmarks also measure ability to use API\u2019s or perform\nmore general software engineering tasks, such as JuICe (Agashe et al., 2019), APIBench (Patil et al.,\n2023), RepoBench (Liu et al., 2023e), ODEX (Wang et al., 2022b), SWE-Bench (Jimenez et al., 2023),\nGoogleCodeRepo (Shrivastava et al., 2023), RepoEval (Zhang et al., 2023a), and Cocomic-Data\n(Ding et al., 2022).\n3\nFinally, there are a variety of benchmarks for other tasks, such as code translation (Roziere et al.,\n2020; Zhu et al., 2022; Ahmad et al., 2021), test case generation (Tufano et al., 2022; Watson et al.,\n2020), code search (Husain et al., 2019), type prediction (Mir et al., 2022; Wei et al., 2023; Malik\net al., 2019), commit message generation (Liu et al., 2020), code summarization (LeClair et al.,\n2019; Iyer et al., 2016; Barone & Sennrich, 2017; Hasan et al., 2021; Alon et al., 2018), code security\n(Liguori et al., 2022; Pearce et al., 2022; Tony et al., 2023), program repair (Jiang et al., 2023b; Xia\net al., 2022; Tufano et al., 2019; Haque et al., 2022; Jin et al., 2023; Gupta et al., 2017; Berabi et al.,\n2021), performance optimization (Garg et al., 2022; Madaan et al., 2023a), and so on.\nTo our knowledge, our CRUXEval is the first publicly available benchmark to measure the execution\nability of code LMs. While some prior work has measured the output prediction ability of code\nLMs, we leverage our CRUXEval-O to perform a more thorough investigation of these capabilities.\nOur CRUXEval-I is the first to measure the ability of code LMs to perform input prediction.\nLeveraging Test Cases and Code Execution: Another line of work uses test cases and code\nexecution information to improve code generation. Some examples include Speculyzer (Key et al.,\n2022), CodeT (Chen et al., 2022), CodeGen-Test (Zhong et al., 2022), Coder-Reviewer reranking\n(Zhang et al., 2023g), MBR-EXEC (Shi et al., 2022) TCoT (Tian & Chen, 2023), Algo (Zhang et al.,\n2023d), Pangu-Coder2 (Shen et al., 2023), LEVER Ni et al. (2023), and Self-Play (Haluptzok et al.,\n2022). The idea of these works is to generate many programs and many test cases and select which\nprograms and test cases seem correct based on the execution results. using execution info. Other\nworks use RL-style execution feedback to improve code generation, including CodeRL (Le et al.,\n2022), Reflexion (Shinn et al., 2023), and PG-TD (Zhang et al., 2023e). (Chen et al., 2023; Olausson\net al., 2023b; Madaan et al., 2023b; Peng et al., 2023; Zhang et al., 2023c) investigate self-repair,\nusing error messages as feedback for models to improve.\nMost relevant to our work, a handful of works examine and improve the execution ability of code\nLMs. Austin et al. (2021), Scratchpad (Nye et al., 2021), and CodeExecutor (Liu et al., 2023a) train\ncode LMs on execution information. Inspired by these works, we briefly touch on two primitive\nways to improve performance on our benchmark, chain-of-thought and fine-tuning. Moving\nforward, we believe our CRUXEval could serve as a useful reference point as more techniques are\ndesigned to improve code execution abilities.\nFailure modes of LM Reasoning: Another dream of the community is to better understand\nthe failure modes of LMs on reasoning tasks. Bubeck et al. (2023); Liu et al. (2023b); Arkoudas\n(2023); Zhang et al. (2022); Dziri et al. (2023); Olausson et al. (2023a); Lee et al. (2023); Zhang et al.\n(2023f) all investigate and point out various failure modes of LMs on a wide variety of reasoning\ntasks. Other examples of reasoning failures include 1) understanding negation (Hosseini et al.,\n2021), 2) ignoring irrelevant context (Shi et al., 2023), 3) operating under counterfactual situations\nsuch as 1-indexed Python or base-9 addition (Wu et al., 2023), and 4) generating Python code\nafter identifier swaps like print, len = len, print (Miceli-Barone et al., 2023). Taking a more\ntheoretical perspective, Dziri et al. (2023); Zhou et al. (2023); Merrill & Sabharwal (2023); Giannou\net al. (2023) characterize the types of reasoning tasks transformers can and cannot be expected to\ncarry out. Merrill et al. (2021) argues that it is not possible to learn meaning from ungrounded\nform with context dependence and assuming that syntax is independent of semantics. In this work,\nwe use CRUXEval to empirically examine failures in code execution / reasoning.\n4\n3\nBenchmark Construction\nCRUXEval consists of 800 distinct functions, each with an input-output pair such that executing\nthe function on the input deterministically produces the output. Using these functions and input-\noutput pairs, we derive two benchmark tasks. In the output prediction task, the goal is to predict\nthe output of executing the function on its associated input. In the input prediction task, the goal is\nto find any input such that executing the function on that input produces the output. For both\ntasks, we use an execution-based correctness metric. For input prediction, a generated input passes\nif assert f(generated input) == output passes, and for output prediction, a generated output\npasses if assert f(input) == generated output passes. A few statistics about the samples of\nCRUXEval can be found in Appendix A.3.\n3.1\nGenerating Candidates\nWe use Code Llama 34B to generate all the candidate functions and inputs of CRUXEval. To do\nso, we prompt it with the name of a function in the Python standard library such as str.zfill\nand ask it to generate a Python function that makes use of the library function in addition to 5\ntest inputs. We provide two varying few-shot examples in our prompt for improved diversity of\ngenerations (see Appendix A.2 for more details). A sample prompt is shown in Listing 11.\nWe use a total of 69 different functions from the standard library: 47 from the str, 11 from dict,\nand 11 from list (see Appendix A.1 for the full list of functions). Overall, we generate a total of\n102000 functions (46% str, 27% dict, 27% list) and 489306 input-output pairs.\n3.2\nFiltering Candidates\nNext, we filter the generated candidates to ensure that the samples in the dataset are reasonable and\nof high quality. In order to avoid forcing the model to perform tasks such as arithmetic calculation,\nwe design filters so that the benchmark only consists of samples that are solveable by a human\nwithout extra memory.\nConcretely, we filter based on the following criteria.\n\u2022 Compile time: all arguments of the function must be used in the function, length of code\nis between 75 and 300 characters, no syntax errors, proper assertion assert f(input) ==\noutput.\n\u2022 Runtime: no float point operations, true division, exp, other integer operations must have\nat least one argument \u2264 3, string and list operations must have at least one argument with\nlength \u2264 3, finish running in 2 seconds, no uncaught exceptions.\n\u2022 Best effort to remove other undesirable code: function cannot have any imports (such as os,\nrandom), must be deterministic (random, set ordering), and cannot have side effects such as\ninput,\nbuiltins .\n5\n3.3\nData size and measuring noise\nThe success of HumanEval (164 examples) shows that evaluation benchmarks can be small where\nfaster and cheaper evaluation is an overlooked advantage. Since additional examples are easy to\ngenerate, we first overgenerate and then measure if the noise is sufficiently small on a smaller\ndataset.\nOut of all the samples, Code Llama 34B outperforms Code Llama 13B as expected and we would\nlike to retain this property with high confidence in a smaller dataset. To do this, we took bootstrap\nsamples of size N out of \u223c1700 samples to measure the probability that the performance would be\nreversed, shown in Fig. 1. 800 examples are enough to test that Code Llama 34B > Code Llama\n13B, Code Llama cot > Code Llama and as well as between Deepseek 33B > Code Llama 34B\n(output).\nWe measure two sources of noise: 1) sampling which data points to include in the benchmark, and\n2) sampling candidates from models for each data point (temperature > 0). Of these, 1) dominates\n2). For 1) since model A does not always outperform model B on all data points even if A > B in\naggregate, the measured performance depends on which data points are included. We can measure\nboth noise on each model individually, and also measure type 1) noise on pairs of models using\nbootstrap. Fortunately, we do not see major differences between models and the most important\nfactor is just the size of dataset. Type 1) noise is generally around 1.5% for each model whereas\ntype 2) is around 0.2% at N = 800. Type 1) noise usually becomes smaller on pairs of models due\nto correlation, yielding statistically significant results at the \u03b1 = 0.05 level for many model pairs.\n200\n400\n800\n1600\ndata size\n0.04\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\ndiff\ninput_pass1\ncodellama_34B - codellama_13B\ncodellama_cot34B - codellama_34B\ndeepseek_ins_33B - codellama_34B\n200\n400\n800\n1600\ndata size\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\ndiff\noutput_pass1\ncodellama_34B - codellama_13B\ncodellama_cot34B - codellama_34B\ndeepseek_ins_33B - codellama_34B\nFigure 1: Difference between model pairs on bootstrap samples of various sizes. The whiskers\nshow (2.5, 97.5) and boxes show (25, 75) percentiles.\n4\nEvaluation\nWe evaluate a selection of models on CRUXEval: StarCoder (Base 7B, 15.5B) (Li et al., 2023a),\nMistral (7B) (Jiang et al., 2023a), WizardCoder (13B, 34B) (Luo et al., 2023), Phi-1 Gunasekar et al.\n(2023) and Phi-1.5 (Li et al., 2023b) (1.3B), Phind v2 (Royzen et al., 2023) (34B), Code Llama (Roziere\net al., 2023) (Base and Python 7B, 13B, 34B), DeepSeek Coder (Base and Instruct 6.7B, 33B), GPT-3.5\n6\n(Brown et al., 2020; Ouyang et al., 2022), and GPT-4 (OpenAI, 2023). To facilitate reproducibility, the\nHuggingFace checkpoints of non-GPT models are in Appendix B and all prompts are in Appendix\nD.2.\nWe use N = 100 samples for all non-GPT models and N = 10 samples for GPT models. We report\nboth pass@1 scores (T = 0.2) and pass@5 scores (T = 0.8). The results are shown in Fig. 2, and\nraw scores are provided in the Appendix in Table 2. In Fig. 2, we show the intervals generated by\n10000 bootstrap samples from the dataset, where non-overlapping whiskers would be significant at\nthe 2.5% level. To get more statistical power, we compare pairs of models on each bootstrapped\nsample. We show how each model compares to Code Llama 34B in Fig. 16. The intervals generally\ndecreases due to correlations. On all models vs. Code Llama 34B, if the median bar clears the\nwhisker in Fig. 2, then the difference actually holds with >97.5% probability under paired bootstrap.\nFor example, Code Llama 34B is better than wizard 34B on input and Code Llama 34B is worse\nthan deepseek 33B on output prediction with >97.5% probability.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\naccuracy\nphi1\nphi1.5\nstarcoder_7B\nstarcoder_16B\nmistral\ndeepseek_ins_7B\ncodellama_7B\ncodellama_13B\ncodellama_cot13B\nwizard_13B\ndeepseek_7B\nwizard_34B\ncodellama_34B\ndeepseek_33B\ndeepseek_ins_33B\nphind_34B\ngpt35_cot\ngpt35\ncodellama_cot34B\ngpt4\ngpt4_cot\ninput\npass1\npass5\n(a) CRUXEval-I Performance\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\naccuracy\nphi1\nphi1.5\nstarcoder_7B\nmistral\nstarcoder_16B\ncodellama_7B\nwizard_13B\nphind_34B\ncodellama_13B\ncodellama_cot13B\ndeepseek_7B\ndeepseek_ins_7B\ncodellama_34B\nwizard_34B\ndeepseek_33B\ndeepseek_ins_33B\ncodellama_cot34B\ngpt35\ngpt35_cot\ngpt4\ngpt4_cot\noutput\npass1\npass5\n(b) CRUXEval-O Performance\nFigure 2: Main Results. Boxes show (25, 75) percentiles, whiskers show (2.5, 97.5), and the middle\nbar shows the median (\u2248 mean).\n5\nQuantitative Analysis\nCorrelation between scores on HumanEval and CRUXEval: After the release of Code Llama\u2019s\nmodel and GPT-3.5 and GPT-4\u2019s APIs, there have been many creative efforts to take data distilled\nfrom GPT models and use them to train more powerful code models such as WizardCoder (Luo\net al., 2023), Phi-1 (Gunasekar et al., 2023), Phi-1.5 (Gunasekar et al., 2023), and Phind (Royzen et al.,\n2023). For example, WizardCoder 34B started with the Code Llama 34B base model and improved\nthe HumanEval pass@1 score from 53.7% to 73.2%, a significant and impressive achievement.\nThere remains curiosity about whether these models show more general improvements in other\naspects of programming or code understanding (Gudibande et al., 2023). We measure this through\nCRUXEval.\n7\nIn Fig. 3, we plot reported HumanEval scores (we did not reproduce them ourselves) against\nscores on CRUXEval. Indeed, we spot some interesting outliers: when comparing the distilled\nmodels WizardCoder 34B and Phind 34B to Code Llama 34B, we see that the distilled models score\nover 20% more than Code Llama on HumanEval but do not show this drastic improvement when\nevaluated on both input and output predictions. In addition, the Phi-1 model outperforms most of\nthe bigger models on HumanEval, but performs among the worst of all our evaluated models on\nCRUXEval. Overall, this suggests that models optimized for the HumanEval task by distilling data\nfrom GPT-3.5 and GPT-4 (WizardCoder, Phind, Phi) may not have learned other code reasoning\ncapabilities along the way. On the other hand, for models such as StarCoder, Mistral, CodeLlama,\nand DeepSeek-Base, we still see a positive trend between HumanEval score and CRUXEval score,\nsuggesting that code generation and execution/understanding abilities are correlated.\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nHumanEval@1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\ninput_pass1\ncodellama_13B\ncodellama_34B\ncodellama_7B\ngpt35\ngpt4\nmistral\nphi1.5\nphi1\nphind_34B\nstarcoder_16B\nstarcoder_7B\nwizard_13B\nwizard_34B\ndeepseek_ins_7B\ndeepseek_ins_33B\ndeepseek_7B\ndeepseek_33B\ninput@1 vs. HumanEval@1\ntraining\nbase\ninstruct\nfamily\nllama\ngpt\nother\ndeepseek\nsize\n20\n40\n60\n80\n(a) CRUXEval-I vs. HumanEval\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nHumanEval@1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\noutput_pass1\ncodellama_13B\ncodellama_34B\ncodellama_7B\ngpt35\ngpt4\nmistral\nphi1.5\nphi1\nphind_34B\nstarcoder_16B\nstarcoder_7B\nwizard_13B\nwizard_34B\ndeepseek_ins_7B\ndeepseek_ins_33B\ndeepseek_7B\ndeepseek_33B\noutput@1 vs. HumanEval@1\ntraining\nbase\ninstruct\nfamily\nllama\ngpt\nother\ndeepseek\nsize\n20\n40\n60\n80\n(b) CRUXEval-O vs. HumanEval\nFigure 3: Correlation between HumanEval pass@1 scores and CRUXEval-O pass@1 scores\nBase models show a weak correlation between HumanEval and CRUXEval. For HumanEval,\ndistilled models (WizardCoder, Phind, Phi) significantly beat their base models, but for\nCRUXEval, no distilled model performs significantly better than Code Llama 34B.\nRelationship between input prediction and output prediction: In Fig. 4a, we compare the input\nprediction and output prediction pass@1 scores with each other. Conceptually, the two tasks\nseem relatively different: output prediction is directly testing code execution ability, while input\nprediction requires a higher-level understanding of the code\u2019s functionality. However, we discover\nthat there is a strong correlation between their performance. This suggests the hypothesis that\nperformance on relatively distinct coding-related tasks may be closely correlated. In addition, we\nsee a relatively clear impact of scaling the model size on our two tasks.\n8\n0.2\n0.3\n0.4\n0.5\n0.6\noutput_pass1\n0.2\n0.3\n0.4\n0.5\n0.6\ninput_pass1\ninput@1 vs. output@1\nfamily\nllama\ngpt\nother\ndeepseek\nsize\n20\n40\n60\n80\n(a) Input vs. Output Prediction, Direct\n0.4\n0.5\n0.6\n0.7\n0.8\ninput_pass1\n0.4\n0.5\n0.6\n0.7\n0.8\noutput_pass1\ncodellama_13B\ncodellama_34B\ncodellama_cot13B\ncodellama_cot34B\ngpt35\ngpt35_cot\ngpt4\ngpt4_cot\noutput@1 vs. input@1 with CoT\ncot\nFalse\nTrue\nfamily\nllama\ngpt\n(b) Output vs. Input Prediction, CoT\nFigure 4: Correlation between Input and Output Prediction Scores, with and without CoT\nWith the potential exception of GPT models, performance on CRUXEval-I and CRUXEval-O\nseem to be very correlated. As the tasks seem relatively different, this suggests that the code\nreasoning capabilities of models may generalize from task to task.\nConfusion matrix/error correlation for different models. Fig. 5 shows the pairwise correlation of\npass@1 scores for each pair of models. The correlation is chosen based on its highest signal among\ncosine distance, Spearman and Kendall. The middle section of \u201copen\u201d-ish models (StarCoder,\nCode Llama, DeepSeek, etc.) are strongly correlated with each other. Strong correlations are seen\nbetween sizes of the same model, between models of the same size, and between instruct and base\n(Phind 34B, Wizard 34B vs. Code Llama 34B). CoT results also tend to have strong correlations\nwith other CoT results, even GPT-4 vs Llama 13B. For the output task, Deepseek forms a small\nsub-cluster of especially strong associations.\n9\nphi1\nphi1.5\nstarcoder_7B\nstarcoder_16B\nmistral\nwizard_13B\nwizard_34B\nphind_34B\ncodellama_7B\ncodellama_13B\ncodellama_34B\ncodellama_cot13B\ncodellama_cot34B\ndeepseek_7B\ndeepseek_ins_7B\ndeepseek_33B\ndeepseek_ins_33B\ngpt35\ngpt4\ngpt4_cot\ngpt35_cot\nphi1\nphi1.5\nstarcoder_7B\nstarcoder_16B\nmistral\nwizard_13B\nwizard_34B\nphind_34B\ncodellama_7B\ncodellama_13B\ncodellama_34B\ncodellama_cot13B\ncodellama_cot34B\ndeepseek_7B\ndeepseek_ins_7B\ndeepseek_33B\ndeepseek_ins_33B\ngpt35\ngpt4\ngpt4_cot\ngpt35_cot\ninput\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nphi1\nphi1.5\nstarcoder_7B\nstarcoder_16B\nmistral\nwizard_13B\nwizard_34B\nphind_34B\ncodellama_7B\ncodellama_13B\ncodellama_34B\ncodellama_cot13B\ncodellama_cot34B\ndeepseek_7B\ndeepseek_ins_7B\ndeepseek_33B\ndeepseek_ins_33B\ngpt35\ngpt4\ngpt4_cot\ngpt35_cot\nphi1\nphi1.5\nstarcoder_7B\nstarcoder_16B\nmistral\nwizard_13B\nwizard_34B\nphind_34B\ncodellama_7B\ncodellama_13B\ncodellama_34B\ncodellama_cot13B\ncodellama_cot34B\ndeepseek_7B\ndeepseek_ins_7B\ndeepseek_33B\ndeepseek_ins_33B\ngpt35\ngpt4\ngpt4_cot\ngpt35_cot\noutput\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFigure 5: Correlation between predictions on input (left) and output (right)\nLooking at model predictions, strong correlations are seen between sizes of the same model,\nbetween models of the same size, and between instruct and base models. Although what\nis hard for a better model tend to be hard for worse models on average, worse models\nsucceeded on some examples where the better models fail completely.\n5.1\nChain of Thought Prompting\nNext, we evaluate how the popular chain-of-thought (CoT) prompting method (Wei et al., 2022)\naffects the performance of Code Llama, GPT-3.5, and GPT-4 models on CRUXEval. The full\nprompts can be found in Appendix D.3. All results are reported using N = 10 samples other\nthan CodeLlama 13B and 34B without CoT, which are reported with N = 100 samples. As before,\npass@1 is reported with T = 0.2 and pass@5 with T = 0.8. Additional results can be found in\nAppendix C.2.\nImpact of CoT: We begin by focusing our attention on the pass@1 scores of models with and\nwithout CoT. In Fig. 4b, we plot the input and output prediction scores of each model with and\nwithout CoT. First, GPT-4 benefits significantly more than other models. Second, output prediction\nboosts are generally larger than input prediction. In fact, CoT does not seem to improve Code\nLlama 13B and GPT-3.5 performance on input prediction. This is intuitive, as input prediction\ninvolves a more difficult reasoning task, while output prediction only requires executing the\nprogram step by step. We defer raw numbers to the Appendix in Table 3.\nCoT helps Code Llama 34B and GPT-4 on both input and output prediction, GPT-3.5 on\nonly output prediction, and Code Llama 13B on neither task. CoT also leads to larger boosts\non output prediction than input prediction. GPT-4 benefits significantly more from CoT\nthan other models, achieving the highest pass@1 of 74.8% on input prediction and 81.9% on\noutput prediction but still far from acing the benchmark.\n10\nCoT widens the gap between pass@5 and pass@1 scores: In Fig. 6, we plot the pass@5 scores\nagainst the pass@1 scores for all models. For models without CoT (shown in blue), there is a\npositive correlation between pass@1 and pass@5 scores. For models with CoT (shown in orange),\nwe see an increase in the gap between pass@5 and pass@1 scores. We believe this phenomenon may\nbe due to the additional diversity induced by CoT, which we analyze in detail in Appendix C.3.\nBecause CoT increases the diversity of generated inputs and outputs, models with CoT see a\nlarger gap between pass@1 and pass@5 score compared to models without.\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\ninput_pass1\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\ninput_pass5\npass@5 vs. pass@1 (input)\ncot\nFalse\nTrue\nfamily\nllama\ngpt\nother\ndeepseek\nsize\n20\n40\n60\n80\n(a) Input prediction\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\noutput_pass1\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\noutput_pass5\npass@5 vs. pass@1 (output)\ncot\nFalse\nTrue\nfamily\nllama\ngpt\nother\ndeepseek\nsize\n20\n40\n60\n80\n(b) Output prediction\nFigure 6: pass@5 score vs. pass@1 score with and without CoT\nPredictions of CoT vs. Base Model: In Fig. 7, we show a confusion matrix over samples to\nbetter understand the correlations between direct output predictions and CoT predictions. For\nCodeLlama 13B, 34B, and GPT-3.5, we observe a large number of samples where direct prediction\nsucceeds but CoT fails. However, with GPT-4, we observe that there are relatively few samples\nwhere this is the case.\n11\n<= 0.5\n> 0.5\nCode Llama 13B Score\n> 0.5\n<= 0.5\nCode Llama 13B + CoT Score\n85\n207\n406\n102\nConfusion Matrix (Input)\n<= 0.5\n> 0.5\nCode Llama 13B Score\n> 0.5\n<= 0.5\nCode Llama 13B + CoT Score\n62\n226\n431\n81\nConfusion Matrix (Output)\n0\n200\n400\n600\n800\n1000\n1200\n0\n200\n400\n600\n800\n1000\n1200\n(a) Code Llama 13B\n<= 0.5\n> 0.5\nCode Llama 34B Score\n> 0.5\n<= 0.5\nCode Llama 34B + CoT Score\n102\n291\n330\n77\nConfusion Matrix (Input)\n<= 0.5\n> 0.5\nCode Llama 34B Score\n> 0.5\n<= 0.5\nCode Llama 34B + CoT Score\n77\n268\n396\n59\nConfusion Matrix (Output)\n0\n200\n400\n600\n800\n1000\n1200\n0\n200\n400\n600\n800\n1000\n1200\n(b) Code Llama 34B\n<= 0.5\n> 0.5\nGPT-3.5 Score\n> 0.5\n<= 0.5\nGPT-3.5 + CoT Score\n106\n263\n309\n122\nConfusion Matrix (Input)\n<= 0.5\n> 0.5\nGPT-3.5 Score\n> 0.5\n<= 0.5\nGPT-3.5 + CoT Score\n159\n340\n242\n59\nConfusion Matrix (Output)\n0\n200\n400\n600\n800\n1000\n1200\n0\n200\n400\n600\n800\n1000\n1200\n(c) GPT-3.5\n<= 0.5\n> 0.5\nGPT-4 Score\n> 0.5\n<= 0.5\nGPT-4 + CoT Score\n120\n491\n147\n42\nConfusion Matrix (Input)\n<= 0.5\n> 0.5\nGPT-4 Score\n> 0.5\n<= 0.5\nGPT-4 + CoT Score\n166\n490\n130\n14\nConfusion Matrix (Output)\n0\n200\n400\n600\n800\n1000\n1200\n0\n200\n400\n600\n800\n1000\n1200\n(d) GPT-4\nFigure 7: Confusion Matrix of Direct Prediction vs. CoT Prediction (T = 0.2)\nWhile CoT generally improves performance overall, there are many individual samples\nwhere CoT actually hurts the prediction accuracy for Code Llama 13B/34B and GPT-3.5 on\nboth input and output prediction. For GPT-4, CoT generally improves individual sample\naccuracy, more so for output prediction than for input prediction.\n5.2\nFine-tuning Experiments\nNext, we do a preliminary analysis to understand the effect of simple fine-tuning schemes on\nCRUXEval performance. We fine-tuned Code Llama 34B on nearly 140K samples of Python\nfunctions distilled with the procedure outlined in Sec. 3, without filtering. We perform weak\ndecontamination, only removing samples where both the function and input-output pairs match\nsamples in the benchmark.\nIn particular, we finetune on a mixture of 50% samples where the function is not in the benchmark\nand 50% samples where the function is in the benchmark but input-output pairs are not, a very\nliberal setup. The training and testing accuracy over time is shown in Fig. 8. Despite finetuning on\nprograms very similar to the benchmark, we still observe a plateauing effect in the test accuracy,\nsuggesting that our execution tasks may be too difficult to learn from this simple fine-tuning\nscheme. We defer a few other insights from fine-tuning to Appendix C.7 and suggest a few\nfine-tuning ideas for improving our benchmark in Sec. 7.\n12\n0\n200\n400\n600\n800\nSteps\n50\n55\n60\n65\n70\n75\n80\nInput Prediction Accuracy\nFine-Tuning with Benchmark Programs (Input)\nTrain Accuracy\nTest Accuracy\n(a) Input prediction\n0\n200\n400\n600\n800\nSteps\n40\n50\n60\n70\n80\n90\nOutput Prediction Accuracy\nFine-Tuning with Benchmark Programs (Output)\nTrain Accuracy\nTest Accuracy\n(b) Output prediction\nFigure 8: Improvements and Limits of CRUXEval Performance after Fine-Tuning\nAfter fine-tuning on samples very similar to those in our benchmark, Code Llama 34B can\nmatch the performance of GPT-4 on both input and output prediction. However, accuracy\nplateaus at under 70% for both tasks, so simple finetuning is far from solving the benchmark.\n6\nQualitative Analysis\nAll models except GPT4 has over 50% failure rate, showing they cannot do simple executions. In\nthis section, we focus on GPT4 with CoT and verify that the remaining 20% failures are due to the\nmodel, are consistent and are indeed on simple programs. We refer the reader to Appendix E for\nfurther examples of the failures highlighted in this section and impressive successes.\nFailures of GPT-4 CoT.\nGPT-4 Cot scored 0/10 on 54 output prediction tasks and 65 input\nprediction tasks. On 22 problem, it scored 0/10 on both input and output prediction tasks. We\nmanually check the 22 problems if they pass our criteria of being simple problems. Most are indeed\nsimple (Listings 3, 4). There are 2 problems that require counting to around 30 (Listing 5) and 2\nproblems (Listing 6) that require simulating a few actual steps, which might be difficult for direct\ngeneration but within scope for CoT.\ndef f(string, sep):\ncnt = string.count(sep)\nreturn((string+sep) * cnt)[::-1]\nassert f(\u2019caabcfcabfc\u2019, \u2019ab\u2019) == \u2019\n,\u2192 bacfbacfcbaacbacfbacfcbaac\u2019\n# GPT4+CoT preds:\n# f(\u2019baa\u2019, \u2019c\u2019)\n# f(\u2019cba\u2019, \u2019f\u2019)\n# \u2019bacfcabcfcaabcabfcabcfcaac\u2019\n# \u2019bacfbacfcbaabcabfbacfcbaabc\u2019\nListing 3: GPT-4 has the right idea but cannot\ndo the string concatenation correctly\ndef f(prefix, s):\nreturn str.removeprefix(prefix, s)\nassert f(\u2019hymi\u2019, \u2019hymifulhxhzpnyihyf\u2019) == \u2019hymi\u2019\n# GPT4+CoT preds:\n# input\n# f(\u2019p\u2019, \u2019phymi\u2019)\n# f(\u2019\u2019, \u2019hymi\u2019)\n# output\n# \u2019fulhxhzpnyihyf\u2019\n# \u2019fulhxhzpnyihyf\u2019\nListing 4: GPT-4 might have been misled by the\nvariable name prefix\n13\ndef f(text, search_string):\nindexes = []\nwhile search_string in text:\nindexes.append(text.rindex(search_string)\n,\u2192 )\ntext = text[:text.rindex(search_string)]\nreturn indexes\nassert f(\u2019ONBPICJOHRHDJOSNCPNJ9ONTHBQCJ\u2019, \u2019J\u2019)\n,\u2192 == [28, 19, 12, 6]\n# GPT4+CoT preds:\n# input\n# f(\"0000123000012300001230000123\", \"123\")\n# f(\u2019bbbbbbabbbbbbabbbbbbbabbbbbbab\u2019, \u2019a\u2019)\n# f(\"abcxxxxxxabcxxxxxxabcxxxxxxabc\",\"abc\")\n# output\n# [23, 13, 8, 5]\n# [25, 18, 15, 11, 6]\n# [7, 10, 14, 18]\nListing 5: GPT-4 CoT failures where solutions\nrequires counting to 30\ndef f(L):\nN = len(L)\nfor k in range(1, N//2 + 1):\ni = k - 1\nj = N - k\nwhile i < j:\n# swap elements:\nL[i], L[j] = L[j], L[i]\n# update i, j:\ni += 1\nj -= 1\nreturn L\nassert f([16, 14, 12, 7, 9, 11]) == [11, 14, 7,\n,\u2192 12, 9, 16]\n# GPT4+CoT preds:\n# f([16, 9, 12, 7, 14, 11])\n# f([16, 9, 7, 12, 14, 11])\n# [11, 9, 7, 12, 14, 16]\n# [11, 9, 7, 12, 14, 16]\nListing 6: GPT-4 CoT failure, cannot easily tell\nthe answer without running the loops\nListings 7 and 8 show GPT-4 CoT failures for output prediction only. In Listing 8, the model fails\nbecause it concludes that 6173 is not less than 1000. Small perturbations like changing to 0 < num\nand num < 1000 or changing the strings also failed. Both problems only have 2 possible answers\nand other models sometimes get them correctly whereas GPT4 CoT is consistently wrong. We\nmanually tested scratchpad Nye et al. (2021) style prompts, which failed in the same way as regular\nCoT (Appendix E.3).\ndef f(text, suffix):\nif suffix == \u2019\u2019:\nsuffix = None\nreturn text.endswith(suffix)\nassert f(\u2019uMeGndkGh\u2019, \u2019kG\u2019) == ??\n# GPT-4 CoT: True\n# should be False\nListing 7: GPT-4 CoT output\ndef f(num):\nif 0 < num < 1000 and num != 6174:\nreturn \u2019Half Life\u2019\nreturn \u2019Not found\u2019\nassert f(6173) == ??\n# GPT-4 CoT: \u2019Half Life\u2019\n# should be \u2019Not found\u2019\nListing 8: GPT-4 CoT output\nFailures of GPT-4, Input Prediction: Here are two simple failures on input prediction. Listings 9\nand 10 show input prediction failures for concise and simple Python programs with and without\nCoT, respectively.\ndef f(text, repl):\ntrans = str.maketrans(text.lower(), repl.\n,\u2192 lower())\nreturn text.translate(trans)\nassert f(??) == \u2019lwwer case\u2019\n# GPT4 CoT: \u2019lower case\u2019, \u2019ow\u2019\n# could be \u2019lower case\u2019, \u2019lwwer case\u2019\nListing 9: GPT-4 CoT input\ndef f(text):\nstring = \u2019\u2019\nfor char in text:\nstring += char + char.lower()\nreturn string\nassert f(??) == \u2019llaallaakk\u2019\n# GPT-4 CoT: \u2019LAK\u2019\n# should be \u2019lalak\u2019\nListing 10: GPT-4 CoT input\n14\nOther GPT-4 Failures: Finally, we conclude with a set of six relatively simple string manipulation\ntasks that we discovered that GPT-4 fails on. We suspect the errors could be partially due to\ntokenization. The full GPT-4 outputs of these tasks can be found in Appendix E.5.\n- What is a string containing \u2019a\u2019 three times, \u2019b\u2019 three times, \u2019c\u2019 twice, \u2019d\u2019 three times, and \u2019z\u2019\n,\u2192 twice?\n- In Python, what is \" BaB \".rfind(\" B \")?\n- In Python, if I have a string s = \u2019iabnm~~~~~~~~~~\u2019, what is s[1::2]?\n- In Python, what is \"+\".join([\u2019*\u2019, \u2019+\u2019, \u2019n\u2019, \u2019z\u2019, \u2019o\u2019, \u2019h\u2019])?\n- In Python, if text = \"!123Leap and the net will appear\" and res = 123, what is text[len(str(res)):]?\n- What is \"pomodoro\".replace(\"or\", \"pomodoro\")?\n7\nLimitations and Future Work\nCorrelations between various code tasks: While our benchmark serves as an interesting lens to\nanalyze code LMs, one might object that output prediction can simply be done with a Python\ninterpreter and that input prediction abilities can be greatly enhanced by equipping a LM with\nan interpreter, like in GPT-4 Code Interpreter mode. While this is true, we believe that a good\ncode LM still ought to have good code understanding and execution capabilities, similar to that\nof a strong programmer. We see that base models have a reasonably strong correlation between\nHumanEval, input prediction, and output prediction score. An interesting future direction is to\nmore deeply investigate the correlations between performance on various code-related tasks such\nas code completion, execution, bug-finding, and code summarization.\nDistilling future execution benchmarks: Our benchmark only measures the input and output\nprediction accuracy of relatively simple and self-contained Python functions distilled from a single\nmodel (Code Llama 34B). It would also be interesting to measure these capabilities on longer\nand more difficult code snippets, open-domain code samples, or code in other programming\nlanguages. As our distillation technique is relatively general, we welcome others to create their\nown benchmarks measuring the execution of code snippets from other distributions.\nVariation due to prompt and temperature: The accuracy of a model on our benchmark may be\nvery sensitive to the prompt and task format (Mizrahi et al., 2023). We try our best to address\nthis by using prompts that are similar as possible across models (see Appendix D.2 and D.3) but\nunderstand that some prompts may improve the performance of certain models while decrease\nthe performance on others. There are also countless prompting techniques (see (Liu et al., 2023d)\nfor a comprehensive survey) that can be tried to improve the performance. We also run all our\nexperiments with T = 0.2 and T = 0.8 due to budget constraints, but different temperatures will\nlead to different performance for all models. One must always be cautious and critical when using\nbenchmarks to compare models. For example, for input prediction, while Phind v2\u2019s 47.9% pass@1\nmay seem to beat CodeLlama\u2019s 46.5%, the standard deviations of both models with respect to the\n800 samples selected turns out to be around 1.5%, so this conclusion cannot be made.\nInformation loss due to pass@1: While the average pass@k metric is common in the code\ngeneration literature, it compresses a large amount of information into one number. While we\nsuggest reporting pass@1 and pass@5 for our benchmark, we comment that pass@k is only one\nperspective of measuring execution ability. We try to shed more light on behaviour by including a\nbit more analysis throughout this work, but encourage the development of different evaluation and\nanalysis techniques.\n15\nFine-tuning: In our first fine-tuning experiment, we only check for exact string match when\ndecontaminating the fine-tuning set, so there may still be semantic duplication or similar programs\nwith small modifications, which may lead to a higher performance than if those examples were\nremoved. In this work, we only consider the most direct and straightforward fine-tuning scheme.\nWe believe there is room for improvement via more sophisticated techniques, such as using process\nsupervision (Uesato et al., 2022), fine-tuning on correct CoT generations, or fine-tuning on snippets\nof code while including the program state after each step. Seeing that models like Phi, WizardCoder,\nand Phind outperformed Code Llama on HumanEval but not CRUXEval inspires the need for\na deeper investigation of the utility of finetuning on distilled data from a more powerful model.\nLastly, it remains a curiosity whether fine-tuning on execution information can help code generation\nabilities.\nJointly improving code generation and code execution: As we discovered, distilled models like\nPhi, Phind, and WizardCoder that are fine-tuned on code generation do not improve significantly\non CRUXEval compared to their base models. It is unknown whether the opposite is true: does\nimproved fine-tuning on code execution lead to better code generation abilities? It would also be\ninteresting to explore techniques that can lead to improved performance on both code generation\nand code execution simultaneously.\nUnderstanding reasoning from the lens of code: As future work, we believe that our benchmark\nserves as a good starting point towards understanding the code reasoning abilities of LM. Many\nfurther execution evaluations may be possible, such as testing execution of recursive functions,\nexecution from a natural language description and an input, or execution of a composition of two\nfunctions. We find that output prediction serves as a good testbed for understanding CoT failures,\nbecause each step clearly corresponds to an operation with a ground truth, so reasoning failures\ncan be pinpointed. We observed many examples of CoT failures due to simple mistakes that the\nmodel seems to have knowledge about (see Appendix E.3.2 for examples), and it should be possible\nto analyze and characterize this behaviour more systematically.\nSelf-repair: Lately, self-repair has been used to improve the reasoning and programming abilities\nof LLMs (Chen et al., 2023; Olausson et al., 2023b; Madaan et al., 2023b; Peng et al., 2023; Zhang\net al., 2023c; Tyen et al., 2023). From our qualitative analysis, we find that when using CoT, many\noutput prediction failures are recitation errors of information the model may already understand.\nTherefore, we believe that these mistakes may be easier to repair than when the correct reasoning\npath is not found in the first place, and that CRUXEval may be a simpler task to better understand\nmodel repair capabilities.\n8\nConclusion\nWe propose CRUXEval, a new benchmark consisting of simple Python functions to evaluate the\ninput and output prediction abilities of code LMs. First, we propose a three-part recipe to distill\nour benchmark consisting of large-scale distillation, filtering, and data size selection via a statistical\nnoise analysis (Sec. 3). Second, we conduct a qualitative analysis by evaluating 20 models on our\nbenchmark (Sec. 4). Our analysis leads to insights regarding the correlation between HumanEval\nand our benchmark, the correlation between input and output prediction, differences between\nvarious code LMs, and the diversity of different models. Third, we explore the potential of CoT\n(Sec. 5.1) and fine-tuning (Sec. 5.2) for improving performance. Fourth, we provide a qualitative\nanalysis showcasing successes and failures of GPT-4 on our benchmark (Sec. 6 and Appendix E).\nOverall, we believe that CRUXEval provides a complimentary perspective to classical code LM\n16\nevaluation such as HumanEval and MBPP and encourage creators of future code LMs to try out\nour benchmark!\n9\nAcknowledgements\nIn alphabetical order by first name, we thank Chris Cummings, Dylan Zhang, Jonas Gehring,\nKunhao Zheng, Luyu Gao, Naman Jain, Nicolas Usunier, Ofir Press, Robin Jia, Scott Yih, Theo\nOlausson, and Wen-Ding Li for very helpful suggestions and support that influenced the trajectory\nof this work.\nA. Gu is supported by the National Science Foundation (NSF) Graduate Research Fellowship under\nGrant No. 2141064. A. Solar-Lezama is supported by the National Science Foundation (NSF) and\nIntel Corporation through NSF Grant CCF:2217064.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L. Juice: A large scale distantly supervised dataset for open\ndomain context-based code generation. arXiv preprint arXiv:1910.02216, 2019. (Cited on pg. 3)\nAhmad, W. U., Tushar, M. G. R., Chakraborty, S., and Chang, K.-W. Avatar: A parallel corpus for\njava-python program translation. arXiv preprint arXiv:2108.11590, 2021. (Cited on pg. 4)\nAI, D.\nDeepseek coder:\nLet the code write itself.\nhttps://github.com/deepseek-ai/\nDeepSeek-Coder, 2023. (Cited on pg. 3)\nAllal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C. M., Muennighoff, N., Mishra, M.,\nGu, A., Dey, M., et al. Santacoder: don\u2019t reach for the stars! arXiv preprint arXiv:2301.03988, 2023.\n(Cited on pg. 3)\nAlon, U., Brody, S., Levy, O., and Yahav, E. code2seq: Generating sequences from structured\nrepresentations of code. arXiv preprint arXiv:1808.01400, 2018. (Cited on pg. 4)\nArkoudas, K. Gpt-4 can\u2019t reason. arXiv preprint arXiv:2308.03762, 2023. (Cited on pg. 4)\nAthiwaratkun, B., Gouda, S. K., Wang, Z., Li, X., Tian, Y., Tan, M., Ahmad, W. U., Wang, S.,\nSun, Q., Shang, M., et al. Multi-lingual evaluation of code generation models. arXiv preprint\narXiv:2210.14868, 2022. (Cited on pg. 3)\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry,\nM., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\n2021. (Cited on pg. 2, 3, 4)\nBarone, A. V. M. and Sennrich, R. A parallel corpus of python functions and documentation strings\nfor automated code documentation and code generation. arXiv preprint arXiv:1707.02275, 2017.\n(Cited on pg. 4)\nBerabi, B., He, J., Raychev, V., and Vechev, M. Tfix: Learning to fix coding errors with a text-to-text\ntransformer. In International Conference on Machine Learning, pp. 780\u2013791. PMLR, 2021. (Cited on\npg. 4)\n17\nBerglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A. C., Korbak, T., and Evans, O. The\nreversal curse: Llms trained on\u201d a is b\u201d fail to learn\u201d b is a\u201d. arXiv preprint arXiv:2309.12288, 2023.\n(Cited on pg. 42)\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P.,\nSastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information\nprocessing systems, 33:1877\u20131901, 2020. (Cited on pg. 2, 7)\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y.,\nLundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv\npreprint arXiv:2303.12712, 2023. (Cited on pg. 4)\nCassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L., Pinckney, D., Yee, M.-H., Zi,\nY., Anderson, C. J., Feldman, M. Q., et al. Multipl-e: A scalable and extensible approach to\nbenchmarking neural code generation. arXiv preprint arXiv:2208.08227, 2022. (Cited on pg. 3)\nChen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.-G., and Chen, W. Codet: Code generation\nwith generated tests. arXiv preprint arXiv:2207.10397, 2022. (Cited on pg. 4)\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y.,\nJoseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint\narXiv:2107.03374, 2021. (Cited on pg. 2, 3)\nChen, X., Lin, M., Sch\u00a8arli, N., and Zhou, D. Teaching large language models to self-debug. arXiv\npreprint arXiv:2304.05128, 2023. (Cited on pg. 4, 16)\nDing, Y., Wang, Z., Ahmad, W. U., Ramanathan, M. K., Nallapati, R., Bhatia, P., Roth, D., and Xiang,\nB. Cocomic: Code completion by jointly modeling in-file and cross-file context. arXiv preprint\narXiv:2212.10007, 2022. (Cited on pg. 3)\nDziri, N., Lu, X., Sclar, M., Li, X. L., Jian, L., Lin, B. Y., West, P., Bhagavatula, C., Bras, R. L.,\nHwang, J. D., et al. Faith and fate: Limits of transformers on compositionality. arXiv preprint\narXiv:2305.18654, 2023. (Cited on pg. 4)\nFan, A., Gokkaya, B., Harman, M., Lyubarskiy, M., Sengupta, S., Yoo, S., and Zhang, J. M.\nLarge language models for software engineering: Survey and open problems. arXiv preprint\narXiv:2310.03533, 2023. (Cited on pg. 2)\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., tau Yih, W., Zettlemoyer,\nL., and Lewis, M.\nIncoder: A generative model for code infilling and synthesis.\npreprint\narXiv:2204.05999, 2022. (Cited on pg. 3)\nGarg, S., Moghaddam, R. Z., Clement, C. B., Sundaresan, N., and Wu, C. Deepperf: A deep\nlearning-based approach for improving software performance. arXiv preprint arXiv:2206.13619,\n2022. (Cited on pg. 4)\nGiannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped transformers\nas programmable computers. arXiv preprint arXiv:2301.13196, 2023. (Cited on pg. 4)\nGudibande, A., Wallace, E., Snell, C., Geng, X., Liu, H., Abbeel, P., Levine, S., and Song, D. The\nfalse promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717, 2023. (Cited on pg. 7)\nGunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Del Giorno, A., Gopi, S., Javaheripi, M.,\nKauffmann, P., de Rosa, G., Saarikivi, O., et al. Textbooks are all you need. arXiv preprint\narXiv:2306.11644, 2023. (Cited on pg. 3, 6, 7)\n18\nGupta, R., Pal, S., Kanade, A., and Shevade, S. Deepfix: Fixing common c language errors by deep\nlearning. In Proceedings of the aaai conference on artificial intelligence, volume 31, 2017. (Cited on pg.\n4)\nHaluptzok, P., Bowers, M., and Kalai, A. T. Language models can teach themselves to program\nbetter. arXiv preprint arXiv:2207.14502, 2022. (Cited on pg. 4)\nHaque, M. M. A., Ahmad, W. U., Lourentzou, I., and Brown, C. Fixeval: Execution-based evaluation\nof program fixes for competitive programming problems. 2022. (Cited on pg. 4)\nHasan, M., Muttaqueen, T., Ishtiaq, A. A., Mehrab, K. S., Haque, M. M. A., Hasan, T., Ahmad,\nW. U., Iqbal, A., and Shahriyar, R. Codesc: A large code-description parallel dataset. arXiv\npreprint arXiv:2105.14220, 2021. (Cited on pg. 4)\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S.,\nHe, H., Song, D., et al. Measuring coding challenge competence with apps. arXiv preprint\narXiv:2105.09938, 2021. (Cited on pg. 3)\nHosseini, A., Reddy, S., Bahdanau, D., Hjelm, R. D., Sordoni, A., and Courville, A. Understanding\nby understanding not: Modeling negation in language models. arXiv preprint arXiv:2105.03519,\n2021. (Cited on pg. 4)\nHusain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M. Codesearchnet challenge:\nEvaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019. (Cited on pg.\n4)\nIyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L. Summarizing source code using a neural\nattention model. In 54th Annual Meeting of the Association for Computational Linguistics 2016, pp.\n2073\u20132083. Association for Computational Linguistics, 2016. (Cited on pg. 4)\nJain, N., Vaidyanath, S., Iyer, A., Natarajan, N., Parthasarathy, S., Rajamani, S., and Sharma, R.\nJigsaw: Large language models meet program synthesis. In Proceedings of the 44th International\nConference on Software Engineering, pp. 1219\u20131231, 2022. (Cited on pg. 3)\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand,\nF., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023a.\n(Cited on pg. 6)\nJiang, N., Liu, K., Lutellier, T., and Tan, L. Impact of code language models on automated program\nrepair. arXiv preprint arXiv:2302.05020, 2023b. (Cited on pg. 4)\nJimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. Swe-bench: Can\nlanguage models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. (Cited\non pg. 3)\nJin, M., Shahriar, S., Tufano, M., Shi, X., Lu, S., Sundaresan, N., and Svyatkovskiy, A. Inferfix:\nEnd-to-end program repair with llms. arXiv preprint arXiv:2303.07263, 2023. (Cited on pg. 4)\nKey, D., Li, W.-D., and Ellis, K. I speak, you verify: Toward trustworthy neural program synthesis.\narXiv preprint arXiv:2210.00848, 2022. (Cited on pg. 4)\nLai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W.-t., Fried, D., Wang, S., and Yu,\nT. Ds-1000: A natural and reliable benchmark for data science code generation. In International\nConference on Machine Learning, pp. 18319\u201318345. PMLR, 2023. (Cited on pg. 3)\n19\nLe, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. H. Coderl: Mastering code generation\nthrough pretrained models and deep reinforcement learning. Advances in Neural Information\nProcessing Systems, 35:21314\u201321328, 2022. (Cited on pg. 4)\nLeClair, A., Jiang, S., and McMillan, C. A neural model for generating natural language summaries\nof program subroutines. In 2019 IEEE/ACM 41st International Conference on Software Engineering\n(ICSE), pp. 795\u2013806. IEEE, 2019. (Cited on pg. 4)\nLee, N., Sreenivasan, K., Lee, J. D., Lee, K., and Papailiopoulos, D. Teaching arithmetic to small\ntransformers. arXiv preprint arXiv:2307.03381, 2023. (Cited on pg. 4)\nLi, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J.,\nChim, J., et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023a.\n(Cited on pg. 3, 6)\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno,\nF., Dal Lago, A., et al. Competition-level code generation with alphacode. Science, 378(6624):\n1092\u20131097, 2022. (Cited on pg. 3)\nLi, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. Textbooks are all you\nneed ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023b. (Cited on pg. 3, 6)\nLiguori, P., Al-Hossami, E., Cotroneo, D., Natella, R., Cukic, B., and Shaikh, S. Can we generate\nshellcodes via natural language? an empirical study. Automated Software Engineering, 29(1):30,\n2022. (Cited on pg. 4)\nLiu, C., Lu, S., Chen, W., Jiang, D., Svyatkovskiy, A., Fu, S., Sundaresan, N., and Duan, N. Code\nexecution with pre-trained language models. arXiv preprint arXiv:2305.05383, 2023a. (Cited on\npg. 4)\nLiu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., and Zhang, Y. Evaluating the logical reasoning ability\nof chatgpt and gpt-4. arXiv preprint arXiv:2304.03439, 2023b. (Cited on pg. 4)\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatgpt really correct? rigorous\nevaluation of large language models for code generation. arXiv preprint arXiv:2305.01210, 2023c.\n(Cited on pg. 3)\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. ACM Computing\nSurveys, 55(9):1\u201335, 2023d. (Cited on pg. 15)\nLiu, S., Gao, C., Chen, S., Nie, L. Y., and Liu, Y. Atom: Commit message generation based\non abstract syntax tree and hybrid ranking. IEEE Transactions on Software Engineering, 48(5):\n1800\u20131817, 2020. (Cited on pg. 4)\nLiu, T., Xu, C., and McAuley, J. Repobench: Benchmarking repository-level code auto-completion\nsystems. arXiv preprint arXiv:2306.03091, 2023e. (Cited on pg. 3)\nLuo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. Wizardcoder:\nEmpowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.\n(Cited on pg. 3, 6, 7)\nMadaan, A., Shypula, A., Alon, U., Hashemi, M., Ranganathan, P., Yang, Y., Neubig, G., and\nYazdanbakhsh, A. Learning performance-improving code edits. arXiv preprint arXiv:2302.07867,\n2023a. (Cited on pg. 4)\n20\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N.,\nPrabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint\narXiv:2303.17651, 2023b. (Cited on pg. 4, 16)\nMalik, R. S., Patra, J., and Pradel, M. Nl2type: inferring javascript function types from natural\nlanguage information. In 2019 IEEE/ACM 41st International Conference on Software Engineering\n(ICSE), pp. 304\u2013315. IEEE, 2019. (Cited on pg. 4)\nMerrill, W. and Sabharwal, A. The expresssive power of transformers with chain of thought. arXiv\npreprint arXiv:2310.07923, 2023. (Cited on pg. 4)\nMerrill, W. C., Goldberg, Y., Schwartz, R., and Smith, N. A. Provable limitations of acquiring\nmeaning from ungrounded form: What will future language models understand? Transactions of\nthe Association for Computational Linguistics, 9:1047\u20131060, 2021. (Cited on pg. 4)\nMiceli-Barone, A. V., Barez, F., Konstas, I., and Cohen, S. B. The larger they are, the harder they fail:\nLanguage models do not recognize identifier swaps in python. arXiv preprint arXiv:2305.15507,\n2023. (Cited on pg. 4)\nMir, A. M., Lato\u02c7skinas, E., Proksch, S., and Gousios, G. Type4py: Practical deep similarity learning-\nbased type inference for python. In Proceedings of the 44th International Conference on Software\nEngineering, pp. 2241\u20132252, 2022. (Cited on pg. 4)\nMizrahi, M., Kaplan, G., Malkin, D., Dror, R., Shahaf, D., and Stanovsky, G. State of what art? a\ncall for multi-prompt llm evaluation. arXiv preprint arXiv:2401.00595, 2023. (Cited on pg. 15)\nNi, A., Iyer, S., Radev, D., Stoyanov, V., Yih, W.-t., Wang, S., and Lin, X. V. Lever: Learning to verify\nlanguage-to-code generation with execution. In International Conference on Machine Learning, pp.\n26106\u201326128. PMLR, 2023. (Cited on pg. 4)\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C. Codegen:\nAn open large language model for code with multi-turn program synthesis. In The Eleventh\nInternational Conference on Learning Representations, 2022. (Cited on pg. 3)\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D.,\nLewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate\ncomputation with language models. arXiv preprint arXiv:2112.00114, 2021. (Cited on pg. 4, 14, 54)\nOlausson, T. X., Gu, A., Lipkin, B., Zhang, C. E., Solar-Lezama, A., Tenenbaum, J. B., and Levy,\nR. Linc: A neurosymbolic approach for logical reasoning by combining language models with\nfirst-order logic provers. arXiv preprint arXiv:2310.15164, 2023a. (Cited on pg. 4)\nOlausson, T. X., Inala, J. P., Wang, C., Gao, J., and Solar-Lezama, A. Demystifying gpt self-repair\nfor code generation. arXiv preprint arXiv:2306.09896, 2023b. (Cited on pg. 4, 16)\nOpenAI, R. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2023. (Cited on pg. 2, 7)\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S.,\nSlama, K., Ray, A., et al. Training language models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:27730\u201327744, 2022. (Cited on pg. 2, 7)\nPatil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with\nmassive apis. arXiv preprint arXiv:2305.15334, 2023. (Cited on pg. 3)\n21\nPearce, H., Ahmad, B., Tan, B., Dolan-Gavitt, B., and Karri, R. Asleep at the keyboard? assessing the\nsecurity of github copilot\u2019s code contributions. In 2022 IEEE Symposium on Security and Privacy\n(SP), pp. 754\u2013768. IEEE, 2022. (Cited on pg. 4)\nPeng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z., Chen, W.,\nand Gao, J. Check your facts and try again: Improving large language models with external\nknowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023. (Cited on pg. 4, 16)\nRoyzen, M., Wei, J., and Coleman, R. Phind, 2023. URL https://www.phind.com. (Cited on pg. 3,\n6, 7)\nRoziere, B., Lachaux, M.-A., Chanussot, L., and Lample, G. Unsupervised translation of program-\nming languages. Advances in Neural Information Processing Systems, 33:20601\u201320611, 2020. (Cited\non pg. 4)\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin,\nJ., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\n(Cited on pg. 2, 3, 6)\nShen, B., Zhang, J., Chen, T., Zan, D., Geng, B., Fu, A., Zeng, M., Yu, A., Ji, J., Zhao, J., et al.\nPangu-coder2: Boosting large language models for code with ranking feedback. arXiv preprint\narXiv:2307.14936, 2023. (Cited on pg. 4)\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and Wang, S. I. Natural language to code\ntranslation with execution. arXiv preprint arXiv:2204.11454, 2022. (Cited on pg. 4)\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Sch\u00a8arli, N., and Zhou, D. Large\nlanguage models can be easily distracted by irrelevant context. In International Conference on\nMachine Learning, pp. 31210\u201331227. PMLR, 2023. (Cited on pg. 4)\nShinn, N., Labash, B., and Gopinath, A. Reflexion: an autonomous agent with dynamic memory\nand self-reflection. arXiv preprint arXiv:2303.11366, 2023. (Cited on pg. 3, 4)\nShrivastava, D., Larochelle, H., and Tarlow, D. Repository-level prompt generation for large\nlanguage models of code. In International Conference on Machine Learning, pp. 31693\u201331715. PMLR,\n2023. (Cited on pg. 3)\nTian, Z. and Chen, J. Test-case-driven programming understanding in large language models for\nbetter code generation. arXiv preprint arXiv:2309.16120, 2023. (Cited on pg. 4)\nTony, C., Mutas, M., Ferreyra, N. E. D., and Scandariato, R. Llmseceval: A dataset of natural\nlanguage prompts for security evaluations. arXiv preprint arXiv:2303.09384, 2023. (Cited on pg. 4)\nTufano, M., Watson, C., Bavota, G., Penta, M. D., White, M., and Poshyvanyk, D. An empirical study\non learning bug-fixing patches in the wild via neural machine translation. ACM Transactions on\nSoftware Engineering and Methodology (TOSEM), 28(4):1\u201329, 2019. (Cited on pg. 4)\nTufano, M., Deng, S. K., Sundaresan, N., and Svyatkovskiy, A. Methods2test: A dataset of focal\nmethods mapped to test cases. In Proceedings of the 19th International Conference on Mining Software\nRepositories, pp. 299\u2013303, 2022. (Cited on pg. 4)\nTyen, G., Mansoor, H., Chen, P., Mak, T., and C\u02d8arbune, V. Llms cannot find reasoning errors, but\ncan correct them! arXiv preprint arXiv:2311.08516, 2023. (Cited on pg. 16)\n22\nUesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and\nHiggins, I. Solving math word problems with process-and outcome-based feedback. arXiv\npreprint arXiv:2211.14275, 2022. (Cited on pg. 16)\nWang, S., Li, Z., Qian, H., Yang, C., Wang, Z., Shang, M., Kumar, V., Tan, S., Ray, B., Bhatia, P., et al.\nRecode: Robustness evaluation of code generation models. arXiv preprint arXiv:2212.10264, 2022a.\n(Cited on pg. 3)\nWang, Z., Zhou, S., Fried, D., and Neubig, G. Execution-based evaluation for open-domain code\ngeneration. arXiv preprint arXiv:2212.10481, 2022b. (Cited on pg. 3)\nWatson, C., Tufano, M., Moran, K., Bavota, G., and Poshyvanyk, D. On learning meaningful assert\nstatements for unit test cases. In Proceedings of the ACM/IEEE 42nd International Conference on\nSoftware Engineering, pp. 1398\u20131409, 2020. (Cited on pg. 4)\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-\nthought prompting elicits reasoning in large language models. Advances in Neural Information\nProcessing Systems, 35:24824\u201324837, 2022. (Cited on pg. 10)\nWei, J., Durrett, G., and Dillig, I. Typet5: Seq2seq type inference using static analysis. arXiv preprint\narXiv:2303.09564, 2023. (Cited on pg. 4)\nWu, Z., Qiu, L., Ross, A., Aky\u00a8urek, E., Chen, B., Wang, B., Kim, N., Andreas, J., and Kim, Y.\nReasoning or reciting? exploring the capabilities and limitations of language models through\ncounterfactual tasks. arXiv preprint arXiv:2307.02477, 2023. (Cited on pg. 4)\nXia, C. S., Wei, Y., and Zhang, L. Practical program repair in the era of large pre-trained language\nmodels. arXiv preprint arXiv:2210.14179, 2022. (Cited on pg. 4)\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A systematic evaluation of large language\nmodels of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine\nProgramming, pp. 1\u201310, 2022. (Cited on pg. 3)\nYin, P., Li, W.-D., Xiao, K., Rao, A., Wen, Y., Shi, K., Howland, J., Bailey, P., Catasta, M., Michalewski,\nH., et al. Natural language to code generation in interactive data science notebooks. arXiv preprint\narXiv:2212.09248, 2022. (Cited on pg. 3)\nZan, D., Chen, B., Zhang, F., Lu, D., Wu, B., Guan, B., Yongji, W., and Lou, J.-G. Large language\nmodels meet nl2code: A survey. In Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 7443\u20137464, 2023. (Cited on pg. 2)\nZhang, F., Chen, B., Zhang, Y., Liu, J., Zan, D., Mao, Y., Lou, J.-G., and Chen, W. Repocoder:\nRepository-level code completion through iterative retrieval and generation. arXiv preprint\narXiv:2303.12570, 2023a. (Cited on pg. 3)\nZhang, H., Li, L. H., Meng, T., Chang, K.-W., and Broeck, G. V. d. On the paradox of learning to\nreason from data. arXiv preprint arXiv:2205.11502, 2022. (Cited on pg. 4)\nZhang, K., Li, G., Li, J., Li, Z., and Jin, Z. Toolcoder: Teach code generation models to use apis with\nsearch tools. arXiv preprint arXiv:2305.04032, 2023b. (Cited on pg. 3)\nZhang, K., Li, Z., Li, J., Li, G., and Jin, Z. Self-edit: Fault-aware code editor for code generation. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 769\u2013787, Toronto, Canada, July 2023c. Association for Computational Linguistics.\n(Cited on pg. 4, 16)\n23\nZhang, K., Wang, D., Xia, J., Wang, W. Y., and Li, L. Algo: Synthesizing algorithmic programs with\ngenerated oracle verifiers. arXiv preprint arXiv:2305.14591, 2023d. (Cited on pg. 4)\nZhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J. B., and Gan, C. Planning with large language\nmodels for code generation. arXiv preprint arXiv:2303.05510, 2023e. (Cited on pg. 4)\nZhang, S. D., Tigges, C., Biderman, S., Raginsky, M., and Ringer, T. Can transformers learn to solve\nproblems recursively? arXiv preprint arXiv:2305.14699, 2023f. (Cited on pg. 4)\nZhang, T., Yu, T., Hashimoto, T., Lewis, M., Yih, W.-t., Fried, D., and Wang, S. Coder reviewer\nreranking for code generation. In International Conference on Machine Learning, pp. 41832\u201341846.\nPMLR, 2023g. (Cited on pg. 4)\nZhang, Z., Chen, C., Liu, B., Liao, C., Gong, Z., Yu, H., Li, J., and Wang, R. A survey on language\nmodels for code. 2023h. (Cited on pg. 3)\nZheng, Q., Xia, X., Zou, X., Dong, Y., Wang, S., Xue, Y., Wang, Z., Shen, L., Wang, A., Li, Y.,\net al. Codegeex: A pre-trained model for code generation with multilingual evaluations on\nhumaneval-x. arXiv preprint arXiv:2303.17568, 2023. (Cited on pg. 3)\nZhong, M., Liu, G., Li, H., Kuang, J., Zeng, J., and Wang, M. Codegen-test: An automatic code\ngeneration model integrating program test information. arXiv preprint arXiv:2202.07612, 2022.\n(Cited on pg. 4)\nZhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J., Bengio, S., and Nakkiran,\nP. What algorithms can transformers learn? a study in length generalization. arXiv preprint\narXiv:2310.16028, 2023. (Cited on pg. 4)\nZhu, M., Jain, A., Suresh, K., Ravindran, R., Tipirneni, S., and Reddy, C. K. Xlcost: A benchmark\ndataset for cross-lingual code intelligence. arXiv preprint arXiv:2206.08474, 2022. (Cited on pg. 4)\n24\nAppendix A\nBenchmark Construction and Statistics\nIn this section, we provide more details about the process of constructing our benchmark. A sample\nprompt for generating functions and test cases is shown in Listing 11. The prompt is constructed\nby including two few-shot examples, one containing a specified str function and one containing\na specified list function. The full list of specified functions is given in A.1, and the full list of\nfew-shot examples chosen from is given in A.2. We learned that having random-looking inputs\ninstead of common words and phrases in the few-shot prompts significantly increased the difficulty\nof the benchmark.\nListing 11: Sample prompt for generating functions and test cases\nYou will be given a function name between [TASK] and [/TASK] tags. Following the examples given, write\n,\u2192\na Python function that makes use of the given function and 5 test inputs for that function.\n[TASK]\nstr.center\n[/TASK]\n[PYTHON]\ndef f(text):\nls = list(text)\nfor i in range(1, len(ls) - 1):\nls.insert(i, \u2019+\u2019)\nreturn \u2019\u2019.join(ls).center((len(ls) - 1) * 2)\n[/PYTHON]\n[TEST]\nassert f(\u2019lynel\u2019) == ??\nassert f(\u2019nzoh\u2019) == ??\nassert f(\u2019u\u2019) == ??\nassert f(\u2019anfsoixz\u2019) == ??\nassert f(\u2019xzd\u2019) == ??\n[/TEST]\n[TASK]\nlist.append\n[/TASK]\n[PYTHON]\ndef f(nums):\ncount = len(nums)\nfor i in range(-count+1, 0):\nnums.append(nums[i])\nreturn nums\n[/PYTHON]\n[TEST]\nassert f([2, 6, 1, 3, 1]) == ??\nassert f([7, 1, 2, 6, 0, 2]) == ??\nassert f([4, 3, 2, 1, 2, -1, 4, 2]) == ??\nassert f([0, 6, 2, -1, -2]) == ??\nassert f([-6, -2, 1, -3, 0, 1]) == ??\n[/TEST]\n[TASK]\nstr.zfill\n[/TASK]\n[PYTHON]\n25\nA.1\nFunctions used in prompt\nFor each of str, list, and dict, we use all the non-dunder methods under that class. The resulting\nlist of methods is as follows:\n\u2022 str: capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, for-\nmat map, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric,\nisprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, maketrans, partition, removeprefix,\nremovesuffix, replace, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith,\nstrip, swapcase, title, translate, upper, zfill\n\u2022 list: append, clear, copy, count, extend, index, insert, pop, remove, reverse, sort\n\u2022 dict: clear, copy, fromkeys, get, items, keys, pop, popitem, setdefault, update, values\nMotivated by seeing a GPT-4 failure of treating the ^ symbol as an exponential rather than an xor,\nwe also attempted using all the non-dunder methods from operator. However, we found that the\nmajority of the functions obtained were either too simple and uninteresting, or too computational,\nsince many of the methods under operator are bit-manipulation or calculational operations.\nTherefore, we excluded it from our final benchmark.\nA.2\nFew-shot Examples\nWe use 10 handwritten few-shot examples, 5 using str functions and 5 using list functions. For\neach prompt, we include two few-shot examples, one string few-shot example and one list few-shot\nexample, for a total of 25 different combinations of few-shot prompts. We generate programs and\ninputs using Code Llama 34B with temperature T = 1.\nOne interesting observation is that for a fixed pair of few-shot examples, there seems to be a limit\nto the number of diverse functions that can be generated: after about 60000 generations, only about\n5000 of them were unique. Using all 25 combinations of few-shot prompts helps to overcome this\nduplication bottleneck.\nThe full set of few-shot examples can be found in Listing 13.\nA.3\nDataset Statistics\nIn Fig. 9, we show the distribution of character count and line count of the 800 samples in our\nbenchmark.\n26\n100\n150\n200\n250\n300\nCode Length (characters)\n0\n20\n40\n60\n80\n100\n120\nFrequency\nDistribution of Code Lengths (characters)\n5\n10\n15\nCode Length (lines)\n0\n25\n50\n75\n100\n125\n150\nFrequency\nDistribution of Code Lengths (lines)\nFigure 9: Dataset Distributions\nIn Fig. 10, we show the distribution of the \u201cstep count\u201d of programs (with one outlier of 3175 steps\nexcluded). Here, steps roughly correspond to Python bytecode operations during execution. The\nprecise definition can be understood by checking the \u201cnumsteps\u201d variable in our code here.\n0\n200\n400\n600\n800\nNumber of \"Steps\"\n10\n0\n10\n1\n10\n2\nFrequency (log scale)\nDistribution of Execution Step Count\nFigure 10: Number of steps\nIn Fig. 11, we plot the output prediction pass@1 scores against the input prediction pass@1 scores\nfor each sample, observing little to no correlation between the difficulty of the two.\n27\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nInput Prediction Average pass@1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nOutput Prediction Average pass@1\nCorrelation between Scores (T = 0.2)\n(a) T = 0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nInput Prediction Average pass@1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nOutput Prediction Average pass@1\nCorrelation between Scores (T = 0.8)\n(b) T = 0.8\nFigure 11: Sample-by-sample correlation between Input Prediction and Output Prediction\nMethod-level statistics: In Fig. 12, we show the number of samples containing each method\nin str, list, and dict. Even though we distilled the same number of samples using each str\nfunction and about twice as many for each list and dict functions, we observe that the resulting\ndistribution is highly non-uniform. This is due to a few reasons. First, about 30% of the time,\nCode Llama 34B sometimes fails to follow the instruction of including the library method in the\nresulting function. Second, some functions naturally lead to more operations that are removed by\nthe filter. Third, common functions such as str/list.index or list.append are used in methods\nthey are not prompted in.\nstr.isprintable\ndict.setdefault\nstr.format_map\nstr.isidentifier\nstr.casefold\nstr.removesuffix\nstr.rjust\nstr.lstrip\nstr.zfill\nstr.encode\nstr.expandtabs\nstr.istitle\nstr.capitalize\nstr.title\nstr.center\nstr.isalnum\nstr.isspace\nstr.partition\nstr.rsplit\nstr.isalpha\nstr.isascii\nstr.maketrans\nstr.swapcase\nstr.translate\ndict.values\nstr.format\nstr.isdecimal\nstr.ljust\nstr.rindex\nstr.rstrip\nstr.isnumeric\nstr.rfind\nstr.splitlines\ndict.keys\nstr.removeprefix\nstr.rpartition\nstr.strip\ndict.fromkeys\nstr.islower\ndict.get\ndict.update\nstr.isupper\ndict.popitem\nstr.endswith\nlist.extend\nstr.isdigit\nlist.sort\ndict/list.clear\nstr.find\nstr.startswith\nlist.reverse\nstr.upper\ndict/list.copy\nlist.remove\nstr.lower\ndict.items\nstr/list.index\nlist.insert\nstr/list.count\nstr.replace\nstr.split\ndict/list.pop\nlist.append\nstr.join\n0\n20\n40\n60\n80\n100\n120\nFrequency\nDistribution of Method Frequencies\nFigure 12: Frequency of various methods in CRUXEval\nNext, we investigate trends of which methods are easier/harder for code LMs. For each method in\nthe list, str, and dict libraries listed in Appendix A.1 with at least 5 samples, we calculate the\naverage input prediction and output prediction score of benchmark samples containing that method.\nWe show the 7 easiest and hardest methods for Code Llama 34B (Fig. 13), WizardCoder 34B (Fig.\n14), and GPT-4 (Fig. 15). Some of the hardest methods, such as str.rsplit, str.maketrans,\nstr.rfind, seem to be more obscure. We hypothesize that they may be underrepresented in\npretraining corpora, explaining models\u2019 worse performance. While the distilled datasets of models\nlike Phi, Phind, and WizardCoder are not yet available to the public, we speculate that they may\n28\ninclude of fewer instances of these underrepresented functions and that distilling more obscure\nmethods may help the model better learn their syntax and semantics.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\npass@1\nstr.isascii (9)\nstr.isspace (8)\nlist.sort (20)\nstr.isalnum (8)\ndict/list.clear (22)\ndict.items (34)\nstr.istitle (6)\nstr.expandtabs (6)\nstr.removeprefix (12)\nlist.remove (33)\nstr.rindex (10)\nstr.maketrans (9)\nstr.translate (9)\nstr.swapcase (9)\n0.88\n0.86\n0.80\n0.76\n0.75\n0.71\n0.70\n0.19\n0.19\n0.19\n0.12\n0.11\n0.11\n0.08\nEasiest/Hardest Methods (CodeLlama 34B, Input)\n(a) Input Prediction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\npass@1\ndict/list.copy (32)\nlist.sort (20)\ndict.get (13)\nstr.isnumeric (11)\nstr.isascii (9)\ndict.values (9)\nstr.isalpha (9)\nstr.swapcase (9)\nstr.rstrip (10)\nstr.rindex (10)\nstr.maketrans (9)\nstr.translate (9)\nstr.rfind (11)\nstr.center (8)\n0.74\n0.71\n0.68\n0.67\n0.66\n0.66\n0.65\n0.21\n0.20\n0.20\n0.19\n0.19\n0.10\n0.00\nEasiest/Hardest Methods (CodeLlama 34B, Output)\n(b) Output Prediction\nFigure 13: Easiest and hardest methods for Code Llama 34B input and output prediction, by pass@1\nscore (T = 0.2)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\npass@1\nstr.isnumeric (11)\nstr.isascii (9)\nlist.sort (20)\nstr.isalnum (8)\nstr.isalpha (9)\nstr.istitle (6)\ndict.keys (11)\nstr.removeprefix (12)\nstr.rindex (10)\nstr.expandtabs (6)\nstr.lstrip (5)\nstr.maketrans (9)\nstr.translate (9)\nstr.swapcase (9)\n0.77\n0.73\n0.73\n0.71\n0.70\n0.69\n0.67\n0.20\n0.19\n0.13\n0.12\n0.00\n0.00\n0.00\nEasiest/Hardest Methods (WizardCoder 34B, Input)\n(a) Input Prediction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\npass@1\nstr.isnumeric (11)\ndict/list.copy (32)\nstr.isalnum (8)\ndict.popitem (16)\nlist.sort (20)\ndict.get (13)\nstr.isalpha (9)\nstr.rpartition (12)\nstr.rfind (11)\nstr.swapcase (9)\nstr.removeprefix (12)\nstr.expandtabs (6)\nstr.center (8)\nstr.title (7)\n0.82\n0.79\n0.75\n0.75\n0.67\n0.67\n0.66\n0.19\n0.15\n0.11\n0.11\n0.05\n0.00\n0.00\nEasiest/Hardest Methods (WizardCoder 34B, Output)\n(b) Output Prediction\nFigure 14: Easiest and hardest methods for WizardCoder 34B input and output prediction, by\npass@1 score (T = 0.2)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\npass@1\nstr.isalnum (8)\nstr.isnumeric (11)\ndict/list.clear (22)\nlist.sort (20)\ndict.update (13)\nstr.isalpha (9)\ndict.values (9)\nlist.index (35)\nstr.format (10)\nstr.splitlines (11)\nstr.maketrans (9)\nstr.translate (9)\nstr.swapcase (9)\nstr.rsplit (8)\n1.00\n1.00\n0.95\n0.93\n0.93\n0.90\n0.89\n0.44\n0.40\n0.39\n0.33\n0.33\n0.32\n0.30\nEasiest/Hardest Methods (GPT-4, Input)\n(a) Input Prediction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\npass@1\nstr.encode (6)\nstr.lstrip (5)\ndict/list.clear (22)\ndict.get (13)\nstr.isnumeric (11)\ndict.keys (11)\nstr.isascii (9)\nlist.insert (35)\nstr.removeprefix (12)\nstr.rfind (11)\nstr.maketrans (9)\nstr.translate (9)\nstr.expandtabs (6)\nstr.center (8)\n1.00\n1.00\n0.95\n0.92\n0.91\n0.91\n0.89\n0.34\n0.33\n0.27\n0.22\n0.22\n0.17\n0.00\nEasiest/Hardest Methods (GPT-4, Output)\n(b) Output Prediction\nFigure 15: Easiest and hardest methods for Code Llama 34B input and output prediction, by pass@1\nscore (T = 0.2)\n29\nAppendix B\nModel URLs\nFor evaluation, we used the gpt-3.5-turbo and gpt-4 models on October 26, 2023. Note that\nthis is before the OpenAI developer day release of GPT-4-Turbo. The corresponding HuggingFace\nmodel URLs for the rest of the evaluated models are listed in Table 1.\nTable 1: Models and HuggingFace URLs\nModel Name\nHuggingFace URL\nMistral (7B)\nhttps://huggingface.co/mistralai/Mistral-7B-v0.1\nPhi-1 (1.3B)\nhttps://huggingface.co/microsoft/phi-1\nPhi-1.5 (1.3B)\nhttps://huggingface.co/microsoft/phi-1_5\nDeepSeek Instruct (6.7B)\nhttps://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct\nDeepSeek Instruct (33B)\nhttps://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct\nDeepSeek Base (6.7B)\nhttps://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\nDeepSeek Base (33B)\nhttps://huggingface.co/deepseek-ai/deepseek-coder-33b-base\nStarCoderBase (15.5B)\nhttps://huggingface.co/bigcode/starcoderbase\nStarCoderBase (7B)\nhttps://huggingface.co/bigcode/starcoderbase-7b\nWizardCoder (13B)\nhttps://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0\nWizardCoder (34B)\nhttps://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0\nPhind (34B)\nhttps://huggingface.co/Phind/Phind-CodeLlama-34B-v2\nCodeLlama 7B\nhttps://huggingface.co/codellama/CodeLlama-7b-hf\nCodeLlama (13B)\nhttps://huggingface.co/codellama/CodeLlama-13b-hf\nCodeLlama (34B)\nhttps://huggingface.co/codellama/CodeLlama-34b-hf\nCodeLlama Python (7B)\nhttps://huggingface.co/codellama/CodeLlama-7b-Python-hf\nCodeLlama Python (13B)\nhttps://huggingface.co/codellama/CodeLlama-13b-Python-hf\nCodeLlama Python (34B)\nhttps://huggingface.co/codellama/CodeLlama-34b-Python-hf\n30\nAppendix C\nEvaluation Results\nC.1\nMain Results\nTable 2 shows the pass@1 and pass@5 results of all evaluated models on CRUXEval, and Fig. 16\nshows them in box-plot form.\nTable 2: Results of all models on CRUXEval\nModel\nSize\nInput Prediction\nOutput Prediction\nPass@1\nPass@5\nPass@1\nPass@5\nCodeLlama\n7B\n36.6%\n55.2%\n36.4%\n49.6%\n13B\n39.0%\n58.2%\n38.4%\n53.2%\n34B\n46.5%\n64.7%\n41.1%\n56.1%\nCodeLlama Python\n7B\n36.3%\n56.0%\n36.4%\n49.7%\n13B\n40.5%\n58.0%\n37.8%\n50.8%\n34B\n41.5%\n59.2%\n40.7%\n53.7%\nStarCoder-Base\n7B\n30.0%\n48.9%\n31.1%\n43.8%\n15.5B\n31.6%\n49.5%\n33.3%\n47.7%\nWizardCoder\n13B\n39.2%\n54.8%\n37.9%\n51.6%\n34B\n42.8%\n57.3%\n41.2%\n52.2%\nPhi-1\n1.3B\n13.9%\n22.6%\n23.3%\n34.0%\nPhi-1.5\n1.3B\n24.1%\n38.9%\n27.1%\n39.4%\nPhind v2\n34B\n47.9%\n64.9%\n38.3%\n49.2%\nDeepseek Coder-Base\n6.7B\n41.1%\n61.7%\n39.8%\n53.9%\n33B\n46.6%\n65.1%\n43.6%\n57.6%\nDeepseek Coder-Instruct\n6.7B\n36.6%\n54.4%\n41.0%\n52.5%\n33B\n47.4%\n64.2%\n44.0%\n58.0%\nMistral\n7B\n36.0%\n54.2%\n31.7%\n45.2%\nGPT-3.5\n-\n49.2%\n66.5%\n50.0%\n60.1%\nGPT-4\n-\n67.1%\n76.8%\n63.4%\n68.7%\n31\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\naccuracy\nphi1\nphi1.5\nstarcoder7b\nstarcoder16b\nmistral\ndeepseek_ins_7B\ncodellama_7B\ncodellama_13B\ncodellama_cot13B\nwizard13b\ndeepseek_7B\nwizard34b\ncodellama_34B\ndeepseek_33B\ndeepseek_ins_33B\nphind34b\ngpt35_cot\ngpt35\ncodellama_cot34B\ngpt4\ngpt4_cot\ninput_pass1\nnone\ncodellama_34B\n(a) Main Results, pass@1 (Input)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\naccuracy\nphi1\nphi1.5\nstarcoder7b\nmistral\nstarcoder16b\ncodellama_7B\nwizard13b\nphind34b\ncodellama_13B\ncodellama_cot13B\ndeepseek_7B\ndeepseek_ins_7B\ncodellama_34B\nwizard34b\ndeepseek_33B\ndeepseek_ins_33B\ncodellama_cot34B\ngpt35\ngpt35_cot\ngpt4\ngpt4_cot\noutput_pass1\nnone\ncodellama_34B\n(b) Main Results, pass@1 (Output)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\naccuracy\nphi1\nphi1.5\nstarcoder7b\nstarcoder16b\nmistral\ndeepseek_ins_7B\nwizard13b\ncodellama_7B\ncodellama_cot13B\nwizard34b\ncodellama_13B\ndeepseek_7B\ndeepseek_ins_33B\ncodellama_34B\nphind34b\ndeepseek_33B\ngpt35\ncodellama_cot34B\ngpt35_cot\ngpt4\ngpt4_cot\ninput_pass5\nnone\ncodellama_34B\n(c) Main Results, pass@5 (Input)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\naccuracy\nphi1\nphi1.5\nstarcoder7b\nmistral\nstarcoder16b\nphind34b\ncodellama_7B\nwizard13b\nwizard34b\ndeepseek_ins_7B\ncodellama_13B\ndeepseek_7B\ncodellama_34B\ndeepseek_33B\ndeepseek_ins_33B\ncodellama_cot13B\ngpt35\ncodellama_cot34B\ngpt4\ngpt35_cot\ngpt4_cot\noutput_pass5\nnone\ncodellama_34B\n(d) Main Results, pass@5 (Output)\nFigure 16: Main Results with confidence intervals compared to codellama 34B.\nC.2\nAdditional Results on Impact of CoT\nTable 3 shows the results of including CoT on Code Llama 13B, 34B, GPT-3.5, and GPT-4.\nSample-wide improvements from CoT: In Fig. 17, we show a histogram of how much CoT\nimproves the pass@1 score of each sample (negative values means that CoT decreased the accuracy).\nWe observe that CoT leads to little improvement for the majority of samples, this effect is partly\ndue to samples already having high pass@1 scores. As evidenced by Fig. 17d, we see that CoT is\nmuch more effective for GPT-4 output prediction compared to both GPT-4 input prediction and\nother models. For the other models, however, we observe a large proportion of samples for which\nCoT actually decreases the pass@1.\n32\nTable 3: Impact of CoT on CRUXEval\nModel\nCoT\nInput Prediction\nOutput Prediction\nPass@1\nPass@5\nPass@1\nPass@5\nCode Llama 13B\n\u2717\n39.0%\n58.2%\n38.4%\n53.2%\n\u2713\n39.1%\n55.2%\n39.3%\n59.9%\n-\n+0.1%\n-3.0%\n+0.9%\n+6.7%\nCode Llama 34B\n\u2717\n46.5%\n64.7%\n41.1%\n56.1%\n\u2713\n50.4%\n68.3%\n46.0%\n65.3%\n-\n+3.9%\n+3.6%\n+4.9%\n+9.2%\nGPT-3.5\n\u2717\n49.2%\n66.5%\n50.0%\n60.1%\n\u2713\n49.1%\n76.3%\n63.3%\n81.2%\n-\n-0.1%\n+9.8%\n+13.3%\n+21.1%\nGPT-4\n\u2717\n67.1%\n76.8%\n63.4%\n68.7%\n\u2713\n74.8%\n88.4%\n81.9%\n90.7%\n-\n+7.7%\n+11.6%\n+18.5%\n+22.0%\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCode Llama 13B + CoT Score - Code Llama 13B Score\n0\n100\n200\n300\n400\n500\n600\nFrequency\n63\n88\n508\n78\n63\nScore Difference (Input)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCode Llama 13B + CoT Score - Code Llama 13B Score\n0\n100\n200\n300\n400\n500\n600\nFrequency\n30\n79\n572\n78\n41\nScore Difference (Output)\n(a) Code Llama 13B\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCode Llama 34B + CoT Score - Code Llama 34B Score\n0\n100\n200\n300\n400\n500\n600\nFrequency\n47\n68\n525\n92\n68\nScore Difference (Input)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCode Llama 34B + CoT Score - Code Llama 34B Score\n0\n100\n200\n300\n400\n500\n600\nFrequency\n27\n57\n569\n94\n53\nScore Difference (Output)\n(b) Code Llama 34B\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nGPT-3.5 + CoT Score - GPT-3.5 Score\n0\n100\n200\n300\n400\n500\n600\nFrequency\n61\n119\n445\n110\n65\nScore Difference (Input)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nGPT-3.5 + CoT Score - GPT-3.5 Score\n0\n100\n200\n300\n400\n500\n600\nFrequency\n33\n49\n487\n97\n134\nScore Difference (Output)\n(c) GPT-3.5\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nGPT-4 + CoT Score - GPT-4 Score\n0\n100\n200\n300\n400\n500\n600\nFrequency\n25\n60\n543\n91\n81\nScore Difference (Input)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nGPT-4 + CoT Score - GPT-4 Score\n0\n100\n200\n300\n400\n500\n600\nFrequency\n6\n19\n577\n50\n148\nScore Difference (Output)\n(d) GPT-4\nFigure 17: Histogram of Score Differences between CoT and Original (T = 0.2)\nIn Fig. 18, we show a more granular perspective of Fig. 7, which again highlights that CoT often\ndecreases the pass@1 score of many samples. Again, we observe a stark difference between the\nimpact of CoT on GPT-4 and other models.\n33\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCode Llama 13B Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCode Llama 13B + CoT Score\n284\n19\n13\n11\n28\n51\n10\n5\n5\n27\n36\n8\n8\n6\n32\n10\n3\n1\n9\n13\n45\n10\n4\n13\n149\nScores Before + After CoT (Input)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCode Llama 13B Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCode Llama 13B + CoT Score\n330\n2\n10\n4\n11\n53\n2\n2\n5\n20\n44\n9\n2\n3\n44\n11\n2\n2\n1\n12\n25\n7\n3\n4\n192\nScores Before + After CoT (Output)\n10\n0\n10\n1\n10\n2\n10\n0\n10\n1\n10\n2\n(a) Code Llama 13B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCode Llama 34B Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCode Llama 34B + CoT Score\n229\n14\n6\n11\n24\n38\n6\n7\n1\n13\n36\n10\n6\n9\n24\n8\n6\n2\n1\n21\n52\n14\n12\n22\n228\nScores Before + After CoT (Input)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCode Llama 34B Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCode Llama 34B + CoT Score\n275\n3\n3\n5\n8\n69\n4\n2\n6\n17\n53\n6\n6\n7\n26\n11\n2\n2\n0\n14\n33\n9\n4\n7\n228\nScores Before + After CoT (Output)\n10\n0\n10\n1\n10\n2\n10\n1\n10\n2\n(b) Code Llama 34B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-3.5 Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-3.5 + CoT Score\n159\n4\n8\n3\n30\n79\n6\n10\n2\n35\n57\n8\n4\n2\n75\n15\n3\n5\n1\n29\n47\n9\n13\n1\n195\nScores Before + After CoT (Input)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-3.5 Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-3.5 + CoT Score\n146\n3\n1\n1\n17\n38\n2\n1\n0\n18\n63\n2\n4\n5\n32\n26\n2\n2\n0\n11\n103\n6\n8\n5\n304\nScores Before + After CoT (Output)\n10\n0\n10\n1\n10\n2\n10\n0\n10\n1\n10\n2\n(c) GPT-3.5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-4 Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-4 + CoT Score\n77\n4\n1\n0\n11\n32\n3\n1\n0\n15\n48\n0\n1\n1\n38\n13\n2\n3\n0\n21\n64\n13\n13\n3\n436\nScores Before + After CoT (Input)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-4 Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-4 + CoT Score\n85\n0\n0\n0\n3\n27\n0\n0\n0\n3\n29\n0\n0\n0\n12\n11\n0\n0\n0\n7\n132\n5\n9\n1\n476\nScores Before + After CoT (Output)\n10\n0\n10\n1\n10\n2\n10\n0\n10\n1\n10\n2\n(d) GPT-4\nFigure 18: Confusion Matrix of Direct Prediction vs. CoT Prediction (T = 0.2), Granular Version\nQualitative example of CoT harming performance: Finally, we show one example of input\nprediction and one example of output prediction where GPT-4 succeeds without CoT and fails\nwith CoT.\n# Output Prediction\ndef f(phone_number):\nwhile phone_number.find(\u201977777\u2019) != -1:\nphone_number = phone_number.replace(\u201977777\u2019, \u2019seg\u2019, 1)\nreturn phone_number\nassert f(\u20197747777722\u2019) == \u2019774seg22\u2019\n# GPT-4 CoT says that \u201977777\u2019 is not in \u20197747777722\u2019, returning \u20197747777722\u2019\n# Input Prediction\ndef f(mylist):\nrevl = mylist[:]\nrevl.reverse()\nmylist.sort(reverse=True)\nreturn mylist == revl\nassert f([5, 8]) == True\n# GPT-4 CoT correctly says that \"we need to provide a list that remains the same when sorted in\n,\u2192 descending order and when reversed,\" but then says the list should already be sorted in\n,\u2192 descending order, returning f([5, 4, 3, 2, 1]).\nCorrelations between failures of different models: Fig. 19 shows P(Y | X = 0)/P(Y), the\naccuracy of model Y given that model X fails completely relative to the original accuracy of\nmodel Y. Although what is hard for a better model tend to be hard for worse models on average,\nworse models succeeded on some examples where the better models fail completely, showing\nidiosyncrasies in failures even for the best GPT-4 CoT model.\n34\nphi1\nphi1.5\nstarcoder_7B\nstarcoder_16B\nmistral\nwizard_13B\nwizard_34B\nphind_34B\ncodellama_7B\ncodellama_13B\ncodellama_34B\ncodellama_cot13B\ncodellama_cot34B\ndeepseek_7B\ndeepseek_ins_7B\ndeepseek_33B\ndeepseek_ins_33B\ngpt35\ngpt4\ngpt4_cot\ngpt35_cot\nphi1\nphi1.5\nstarcoder_7B\nstarcoder_16B\nmistral\nwizard_13B\nwizard_34B\nphind_34B\ncodellama_7B\ncodellama_13B\ncodellama_34B\ncodellama_cot13B\ncodellama_cot34B\ndeepseek_7B\ndeepseek_ins_7B\ndeepseek_33B\ndeepseek_ins_33B\ngpt35\ngpt4\ngpt4_cot\ngpt35_cot\ninput\n0.2\n0.4\n0.6\n0.8\nphi1\nphi1.5\nstarcoder_7B\nstarcoder_16B\nmistral\nwizard_13B\nwizard_34B\nphind_34B\ncodellama_7B\ncodellama_13B\ncodellama_34B\ncodellama_cot13B\ncodellama_cot34B\ndeepseek_7B\ndeepseek_ins_7B\ndeepseek_33B\ndeepseek_ins_33B\ngpt35\ngpt4\ngpt4_cot\ngpt35_cot\nphi1\nphi1.5\nstarcoder_7B\nstarcoder_16B\nmistral\nwizard_13B\nwizard_34B\nphind_34B\ncodellama_7B\ncodellama_13B\ncodellama_34B\ncodellama_cot13B\ncodellama_cot34B\ndeepseek_7B\ndeepseek_ins_7B\ndeepseek_33B\ndeepseek_ins_33B\ngpt35\ngpt4\ngpt4_cot\ngpt35_cot\noutput\n0.2\n0.4\n0.6\n0.8\nFigure 19: P(Y | X = 0)/P(Y) where each Y is the accuracy of models in each row (X for column).\nC.3\nResults on Diversity of Generations\nDiversity of generations across models: Next, we analyze the diversity of generated inputs and\noutputs across various models (without regard to correctness). In Fig. 20, we plot the mean and\nmedian number of unique answers generated across samples of CRUXEval for a selection of\nevaluated models. First, we observe the drastic increase in diversity between using T = 0.2 and\nT = 0.8. Second, by comparing Fig. 20a with Fig. 20b, we note that input prediction generally has\na larger diversity of generations than output prediction. This may be due to the fact that output\nprediction only has one correct answer, but input prediction may have multiple correct answers.\nThird, we observe that at the same temperatures, Code Llama models have the highest diversity,\nwhile distilled models like Phind and WizardCoder have a lower diversity.\n35\n0\n2\n4\n6\n8\n10\nMean Number of Distinct Generations (out of 10)\nPhi-1.5 1.3B\nCode Llama 7B\nCodeLlama 13B\nCodeLlama 34B\nStarCoder 7B\nStarCoder 15.5B\nMistral 7B\nDeepSeek Base 7B\nDeepSeek Base 33B\nDeepSeek Inst 33B\nDeepSeek Inst 7B\nWizard 13B\nWizard 34B\nPhi-1 1.3B\nPhind 34B\nGPT-3.5\nGPT-4\nModel\nMean Number of Distinct Generations (Input)\nT = 0.2 (Mean)\nT = 0.8 (Mean)\n(a) Input prediction\n0\n2\n4\n6\n8\n10\nMean Number of Distinct Generations (out of 10)\nCode Llama 7B\nPhi-1.5 1.3B\nCodeLlama 13B\nMistral 7B\nCodeLlama 34B\nPhi-1 1.3B\nStarCoder 7B\nStarCoder 15.5B\nDeepSeek Base 7B\nDeepSeek Base 33B\nWizard 13B\nDeepSeek Inst 33B\nDeepSeek Inst 7B\nWizard 34B\nPhind 34B\nGPT-3.5\nGPT-4\nModel\nMean Number of Distinct Generations (Output)\nT = 0.2 (Mean)\nT = 0.8 (Mean)\n(b) Output prediction\nFigure 20: Number of distinct generations of various models (out of 10) at T = 0.2 and T = 0.8\nCoT increase the diversity of generations: In Fig. 21, for each model, we plot the average number\nof distinct generations across all samples, where different chains of thought with the same input or\noutput prediction are considered identical. We see again the trend of input prediction generations\nbeing more diverse than output prediction generations. Interestingly, we observe that using CoT\nincreases the diversity at both temperatures.\n0\n2\n4\n6\n8\n10\nMean Number of Distinct Generations (out of 10)\nGPT-4 + CoT\nGPT-4\nGPT-3.5 + CoT\nGPT-3.5\nCodeLlama 34B + CoT\nCodeLlama 34B\nCodeLlama 13B + CoT\nCodeLlama 13B\nModel\nImpact of CoT on Diversity of Generations (Input)\nT = 0.2 (Mean)\nT = 0.8 (Mean)\n(a) Input prediction\n0\n2\n4\n6\n8\n10\nMean Number of Distinct Generations (out of 10)\nGPT-4 + CoT\nGPT-4\nGPT-3.5 + CoT\nGPT-3.5\nCodeLlama 34B + CoT\nCodeLlama 34B\nCodeLlama 13B + CoT\nCodeLlama 13B\nModel\nImpact of CoT on Diversity of Generations (Output)\nT = 0.2 (Mean)\nT = 0.8 (Mean)\n(b) Output prediction\nFigure 21: Number of distinct generations of various models (normalized to be out of 10) at T = 0.2\nand T = 0.8 with and without CoT. We observe that CoT increases the diversity of generations.\nFunctional diversity via distribution of pass@1 scores: In Fig. 22, for each model, we plot the\npercentage of samples where the pass@1 score (T = 0.2) is between 0.1 and 0.9, exclusive, indicating\nthat the sample is neither \u201dtoo easy\u201d nor \u201dtoo hard\u201d for the model. This is a measure of functional\ndiversity because models with more diversity are likely to generate both correct and incorrect\nsamples, leading to more intermediate pass@1 scores. We make a few observations relatively\nconsistent with our prior observations. First, the percentages are relatively low across the board,\nindicating that at a temperature of T = 0.2, models are generally producing a majority of correct or\na majority of incorrect outputs. Second, distilled models have a much lower functional diversity\n36\nthan base models, for example comparing Phind 34B to CodeLlama 34B or DeepSeek Instruct 33B\nto DeepSeek Base 33B. Third, CoT greatly increases the functional diversity of models, which is\nvery evident when looking at GPT-3.5 and GPT-4.\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nPercentage\nGPT-3.5 + CoT\nCodeLlama 13B + CoT\nCodeLlama 34B + CoT\nGPT-4 + CoT\nCode Llama 7B\nCodeLlama Py 13B\nCodeLlama Py 34B\nCodeLlama 34B\nCodeLlama 13B\nDeepSeek Base 33B\nCodeLlama Py 7B\nDeepSeek Base 7B\nStarCoder 15.5B\nMistral 7B\nDeepSeek Inst 33B\nPhi-1.5 1.3B\nStarCoder 7B\nWizard 13B\nPhind 34B\nWizard 34B\nDeepSeek Inst 7B\nGPT-3.5\nGPT-4\nPhi-1 1.3B\nModels\n% of Samples with Pass@1 in (0.1, 0.9), Input\n0\n5\n10\n15\n20\n25\n30\n35\nPercentage\nCodeLlama 34B + CoT\nGPT-3.5 + CoT\nCodeLlama 13B + CoT\nGPT-4 + CoT\nCode Llama 7B\nDeepSeek Base 7B\nCodeLlama Py 7B\nCodeLlama 34B\nMistral 7B\nCodeLlama 13B\nDeepSeek Base 33B\nCodeLlama Py 34B\nDeepSeek Inst 33B\nStarCoder 7B\nWizard 13B\nStarCoder 15.5B\nPhi-1.5 1.3B\nCodeLlama Py 13B\nPhi-1 1.3B\nDeepSeek Inst 7B\nPhind 34B\nWizard 34B\nGPT-3.5\nGPT-4\nModels\n% of Samples with Pass@1 in (0.1, 0.9), Output\nFigure 22: Percentage of samples where pass@1 score is in (0.1, 0.9), exclusive.\nC.4\nDifficulty of Benchmark Samples\nDistribution of sample difficulties: In Fig. 23, we show the average pass@1 score across all models\nfor T = 0.2 in order to get a sense of the difficulty distribution of CRUXEval.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Pass@1 Score\n0\n10\n20\n30\n40\n50\n60\nFrequency\nDifficulty of Samples (Input, T=0.2)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Pass@1 Score\n0\n10\n20\n30\n40\n50\n60\n70\n80\nFrequency\nDifficulty of Samples (Output, T=0.2)\nFigure 23: Difficulty of all samples of our benchmark, averaged across all models (T = 0.2)\nIn Fig. 24, we show the pass@1 distributions of a few of the best-performing models at T = 0.8.\nCompared to the overall distribution, the distribution appears to be more bimodal. The output\nprediction distribution is more bimodal than the input prediction distribution, perhaps reflecting\nthe differences in the tasks themselves. We also see the familiar trend of CoT increasing the number\nof samples with intermediate scores (in this case between 0.25 and 0.75).\nFully solved and unsolved samples: In Figs. 25 and 26, we examine a different metric, the\npercentage of examples with pass@1 score equal to 0 and 1, respectively, at T = 0.8. In a sense,\nthis metric captures the ability of models to solve problems. It is also related to diversity, as with a\n37\n0.0\n0.5\n1.0\n432\n161\n207\nCode Llama 34B\n0.0\n0.5\n1.0\n372\n127\n301\nGPT-3.5\n0.0\n0.5\n1.0\n242\n59\n499\nGPT-4\n0.0\n0.5\n1.0\n409\n168\n223\nDeepSeek Base 33B\n0.0\n0.5\n1.0\n438\n117\n245\nWizardCoder 34B\n0.0\n0.5\n1.0\n391\n114\n295\nPhind 34B\n0.0\n0.5\n1.0\n451\n179\n170\nCode Llama 34B + CoT\n0.0\n0.5\n1.0\n348\n184\n268\nGPT-3.5 + CoT\n0.0\n0.5\n1.0\n156\n108\n536\nGPT-4 + CoT\nPass@1 Distributions (Input, T = 0.8)\nAverage Pass@1\nFrequency\n0.0\n0.5\n1.0\n474\n109\n217\nCode Llama 34B\n0.0\n0.5\n1.0\n387\n54\n359\nGPT-3.5\n0.0\n0.5\n1.0\n280\n27\n493\nGPT-4\n0.0\n0.5\n1.0\n450\n93\n257\nDeepSeek Base 33B\n0.0\n0.5\n1.0\n452\n67\n281\nWizardCoder 34B\n0.0\n0.5\n1.0\n477\n76\n247\nPhind 34B\n0.0\n0.5\n1.0\n411\n148\n241\nCode Llama 34B + CoT\n0.0\n0.5\n1.0\n241\n125\n434\nGPT-3.5 + CoT\n0.0\n0.5\n1.0\n125\n62\n613\nGPT-4 + CoT\nPass@1 Distributions (Output, T = 0.8)\nAverage Pass@1\nFrequency\nFigure 24: Pass@1 Distributions of Selected Models\nhigher diversity, the likelihood of solving the problem may increase. A few observations arise from\nlooking at this metric.\nFig. 25, shows the percentage of samples that are completely unsolved by each model, i.e. with\n0 pass@1. We analyze this metric for T = 0.8, because it leads to more diversity, which would\nimprove this metric. First, when considering non-CoT modes, while GPT-3.5 and GPT-4 (red) are\nthe two best-performing models at pass@1, they perform considerably worse at this metric than\nmodels such as Code Llama 34B and DeepSeek Base 33B. Second, instruction-tuned and distilled\nmodels (DeepSeek Instruct, Phind, WizardCoder) perform worse than their base counterparts,\nsuggesting that their diversity may have been stifled from adherence to their instruction tuning\ndatasets. Third, we observe that for the two Code Llama models, CoT actually makes this metric\nworse, but for GPT models, CoT makes it better. For GPT models, we hypothesize that this may be\ndue to the increased diversity of CoT.\n0\n5\n10\n15\n20\n25\n30\nPercentage Unsolved (T = 0.8)\nCodeLlama 13B + CoT\nGPT-3.5\nDeepSeek Inst 7B\nWizard 34B\nWizard 13B\nCodeLlama 34B + CoT\nGPT-4\nPhind 34B\nGPT-3.5 + CoT\nDeepSeek Inst 33B\nDeepSeek Base 7B\nDeepSeek Base 33B\nCodeLlama 13B\nCodeLlama 34B\nGPT-4 + CoT\nModels\n% of Samples with Pass@1 = 0, Input\nGPT 3.5, 4\nCodeLlama 13B, 34B\nOthers\n0\n5\n10\n15\n20\n25\n30\n35\nPercentage Unsolved (T = 0.8)\nPhind 34B\nGPT-3.5\nDeepSeek Inst 7B\nWizard 34B\nWizard 13B\nCodeLlama 13B + CoT\nGPT-4\nCodeLlama 34B + CoT\nDeepSeek Base 7B\nDeepSeek Inst 33B\nDeepSeek Base 33B\nCodeLlama 13B\nCodeLlama 34B\nGPT-3.5 + CoT\nGPT-4 + CoT\nModels\n% of Samples with Pass@1 = 0, Output\nGPT 3.5, 4\nCodeLlama 13B, 34B\nOthers\nFigure 25: Percentage of samples unsolved, where pass@1 is 0 (T = 0.8)\nIn contrast, Fig. 26 shows the percentage of samples that models get fully correct, i.e. with a perfect\npass@1. We analyze this metric for T = 0.2, as it would lead to more consistency, improving this\n38\nmetric. First, we see that GPT-4 excels, achieving over 60% for both input and output prediction.\nSecond, when comparing base models with instruction tuned models, we see a trend matching the\none before: since instruction tuned models are more consistent, they score better on this metric.\nThird, for output prediction, even though GPT-4 + CoT generally increases diversity, we see that\nconsistency is not sacrificed!\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\nPercentage Fully Solved (T = 0.2)\nGPT-4\nGPT-4 + CoT\nGPT-3.5\nPhind 34B\nDeepSeek Inst 33B\nDeepSeek Base 33B\nWizard 34B\nWizard 13B\nCodeLlama 34B + CoT\nDeepSeek Base 7B\nDeepSeek Inst 7B\nCodeLlama 34B\nCodeLlama 13B\nCodeLlama 13B + CoT\nGPT-3.5 + CoT\nModels\n% of Samples with Pass@1 = 1, Input\nGPT 3.5, 4\nCodeLlama 13B, 34B\nOthers\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\nPercentage Fully Solved (T = 0.2)\nGPT-4 + CoT\nGPT-4\nGPT-3.5\nDeepSeek Inst 33B\nWizard 34B\nGPT-3.5 + CoT\nDeepSeek Base 33B\nDeepSeek Inst 7B\nPhind 34B\nDeepSeek Base 7B\nWizard 13B\nCodeLlama 34B\nCodeLlama 13B\nCodeLlama 34B + CoT\nCodeLlama 13B + CoT\nModels\n% of Samples with Pass@1 = 1, Output\nGPT 3.5, 4\nCodeLlama 13B, 34B\nOthers\nFigure 26: Percentage of samples fully solved, where pass@1 score is 1 (T = 0.2)\nC.5\nImpact of Anonymizing Functions\nAs a small ablation to understand the effect of variable names on execution ability, we also test\nCodeLlama 7B, 13B, and 34B on an anonymized version of a subset of the benchmark, where\nvariable names are replaced with x1, x2, ... identifiers. An example of an anonymized function\nis shown in Listing 12. We use the same few-shot prompt without anonymization and report both\npass@1 (T = 0.2) and pass@5 (T = 0.8) results on the anonymized benchmark with N = 10 samples.\nThe results are shown in Table 4. This strengthens the case against memorization affects.\nListing 12: Sample of benchmark and anonymized version\nOriginal:\ndef f(s):\nnums = \u2019\u2019.join(filter(lambda c:c.isdecimal(), s))\nif nums == \u2019\u2019: return \u2019none\u2019\nm = max([int(num) for num in nums.split(\u2019,\u2019)])\nreturn str(m)\nassert f(\u201901,001\u2019) == \u20191001\u2019\nAnonymized:\ndef f(x0):\nx1 = \u2019\u2019.join(filter(lambda x2: x2.isdecimal(), x0))\nif x1 == \u2019\u2019:\nreturn \u2019none\u2019\nx3 = max([int(x4) for x4 in x1.split(\u2019,\u2019)])\nreturn str(x3)\nassert f(\u201901,001\u2019) == \u20191001\u2019\n39\nTable 4: Impact of Anonymization on CRUXEval\nModel\nAnonymized\nInput Prediction\nOutput Prediction\nPass@1\nPass@5\nPass@1\nPass@5\nCodeLlama 7B\n\u2717\n36.6%\n48.0%\n36.4%\n43.5%\n\u2713\n37.5%\n53.3%\n34.0%\n46.9%\n\u2206\n+0.9%\n+5.3%\n-2.4%\n+3.4%\nCodeLlama 13B\n\u2717\n39.0%\n50.2%\n38.3%\n44.7%\n\u2713\n40.0%\n55.8%\n36.1%\n50.6%\n\u2206\n+1.0%\n+5.6%\n-2.2%\n+5.9%\nCodeLlama 34B\n\u2717\n46.5%\n57.4%\n41.1%\n47.5%\n\u2713\n48.0%\n63.8%\n39.1%\n54.0%\n\u2206\n+1.5%\n+6.4%\n-2.0%\n+6.5%\nC.6\nImpact of Data-Generating Model\nIn the early phases of this work, we were concerned that using Code Llama 34B to generate the\nbenchmark would give the model an unfair advantage. Therefore, we checked the performance of\na few models when generating data with Code Llama 13B, GPT-3.5, and GPT-4. The results are\nshown in Table 5.\nThese samples were generated using a different prompt and a much more relaxed filter, so the raw\nscores differ from those in the main text. Across all datasets, we see that the relative ordering of\nCode Llama 13B, Code Llama 34B, and GPT-3.5 are preserved. We also observed that generating\ndata with GPT-3.5 led to a significantly easier benchmark. After looking at a few samples manually,\nwe believe this is because the resulting inputs are much more predictable and guessable, such\nas f(\"abcde\") rather than f(\"mai2!\"). Including few-shot examples with random inputs did\nnot improve this issue, and we believe this is an artifact of instruction tuning. We believe that\ntogether with the anonymization results in Appendix C.5, these results provide some evidence that\nevaluating a model on its own generated data does not seem to provide it a significant advantage.\nTable 5: Impact of Data Generating Model\nData Model\nEvaluation Model\nInput Pass@1\nOutput Pass@1\nCL 13B\nCL 13B\n28.1%\n28.4%\nCL 13B\nCL 34B\n33.8%\n29.2%\nCL 34B\nCL 13B\n25.1%\n24.3%\nCL 34B\nCL 34B\n29.9%\n25.4%\nCL 34B\nGPT-3.5\n40.5%\n36.6%\nGPT-3.5\nCL 13B\n42.3%\n49.7%\nGPT-3.5\nCL 34B\n52.1%\n50.7%\nGPT-3.5\nGPT-3.5\n67.1%\n67.2%\nGPT-4\nCL 13B\n28.1%\n42.4%\nGPT-4\nCL 34B\n37.0%\n44.6%\n40\nC.7\nFine-tuning\nWe discover three interesting insights from fine-tuning. In the main text, we only discuss insight 3.\nAs a refresher, we fine-tuned Code Llama 34B on 138889 samples of Python functions distilled with\nthe procedure outlined in Sec. 3, without filtering. For the output prediction task, the model was\nfine-tuned on assertions of the form assert f(input) == output, and for the input prediction\ntask, the model was fine-tuned on assertions of the form assert output == f(input). During\nevaluation time, the fine-tuned model was asked to complete assertions of the same format as\ngiven in fine-tuning.\n1. Direct fine-tuning leads to modest performance improvements: In the first setup, we analyze a\nstronger decontamination setup than that in the main text. Specifically, we remove samples that\nmatch functions used in the benchmark, even if the input-output pairs are different. In Fig. 27, we\nshow the train and test accuracy of the model during the finetuning process. For ease of evaluation,\nthe train accuracy is reported on a random subset of 500 samples from the finetuning set. The\nreported test accuracy is on a superset of CRUXEval.\nFirst, we observe that fine-tuning is able to significantly increase performance on both input and\noutput prediction tasks. Second, we observe that while the training accuracy is steadily increasing\nand the model is able to overfit the training set, the testing accuracy plateaus relatively quickly. This\nsuggesting that simple fine-tuning may not be enough to achieve near-perfect scores on CRUXEval.\nThird, we observe that it is easier to overfit the training set on the output prediction benchmark\nthan on the input prediction benchmark. We hypothesize this may be partially due to the fact\nthat assert output == f(input) is a less natural format for assertions and partially due to the\nfact that input prediction requires a more sophisticated level of reasoning compared to output\nprediction.\n0\n200\n400\n600\n800\nSteps\n50\n55\n60\n65\n70\n75\n80\n85\nInput Prediction Accuracy\nAccuracy During Finetuning (Input)\nTrain Accuracy\nTest Accuracy\n(a) Input prediction\n0\n200\n400\n600\n800\nSteps\n40\n50\n60\n70\n80\n90\nOutput Prediction Accuracy\nAccuracy During Finetuning (Output)\nTrain Accuracy\nTest Accuracy\n(b) Output prediction\nFigure 27: Train accuracy (500 random samples) and test accuracy (superset of CRUXEval) while\nfinetuning. For both tasks, there is improvement, and the model steadily fits the training set while\nplateauing on the testing set.\n2. The format of fine-tuning data greatly impacts its effectiveness: We also discovered that it\nis important that the finetuning assertions be formatted in the same way as when evaluating the\nmodel at test time. As evidence of this, we fine-tune Code Llama 34B with two different sets of\nassertions, one on assert output == f(input) assertions and the other on assert f(input) ==\noutput assertions. We compare the accuracy of the two finetuned models on both input and output\nprediction in Fig. 28. We observe that when the format of the fine-tuning data and the testing data\n41\nare different, the model even has difficulty overfitting the training set, showing that it may not\nhave fully learned the equivalence of the two formats and the meaning of the == operator. This is\nperhaps another example of the \u201creversal curse\u201d of LLMs (Berglund et al., 2023). The corresponding\ntesting accuracy also plateaued at a lower accuracy when the format was misaligned. For example,\nin Fig. 28a, comparing the light green line with the light blue line shows almost a 10% difference\nin testing accuracy for input prediction when trained on a misaligned format. That being said,\nfine-tuning still improved performance relative to the base model, even with a mismatched format,\nshowing that the fine-tuning with a mismatched format did still instill some information into the\nmodel.\n0\n200\n400\n600\n800\nSteps\n50\n55\n60\n65\n70\n75\n80\n85\nInput Prediction Accuracy\nImpact of Mismatched Data Format (Input)\nTrain Accuracy, assert f(input) == output\nTrain Accuracy, assert output == f(input)\nTest Accuracy, assert f(input) == output\nTest Accuracy, assert output == f(input)\n(a) Input prediction\n0\n200\n400\n600\n800\nSteps\n40\n50\n60\n70\n80\n90\nOutput Prediction Accuracy\nImpact of Mismatched Data Format (Output)\nTrain Accuracy, assert f(input) == output\nTrain Accuracy, assert output == f(input)\nTest Accuracy, assert f(input) == output\nTest Accuracy, assert output == f(input)\n(b) Output prediction\nFigure 28: Aligning the fine-tuning data format with the evaluation data format is very important\nfor benchmark performance.\n3. Including benchmark programs still cannot improve test accuracy beyond 70%: Finally, we\nexplore the upper limits of fine-tuning on functions and assertions via a \u201dcheating\u201d setup. We\ncurate a small set of 7259 samples consisting only of programs in the benchmark but with different\ninput-output pairs. We finetune on a mixture of 50% of the original finetuning set and 50% of this\nnew set, showing the training and testing accuracy over time in Fig. 29. Despite finetuning on\nprograms very similar to the benchmark, we still observe a plateauing effect in the test accuracy,\nsuggesting that our execution tasks may be too difficult to learn from this simple fine-tuning\nscheme. Therefore, we suggest a few more fine-tuning ideas for improving our benchmark in Sec.\n7.\n0\n200\n400\n600\n800\nSteps\n50\n55\n60\n65\n70\n75\n80\nInput Prediction Accuracy\nFine-Tuning with Benchmark Programs (Input)\nTrain Accuracy\nTest Accuracy\n(a) Input prediction\n0\n200\n400\n600\n800\nSteps\n40\n50\n60\n70\n80\n90\nOutput Prediction Accuracy\nFine-Tuning with Benchmark Programs (Output)\nTrain Accuracy\nTest Accuracy\n(b) Output prediction\nFigure 29: Finetuning 50% on the original finetuning set and 50% on \u201dcheating\u201d data\n42\nAppendix D\nPrompts\nIn this section, we list all the prompts we use throughout the paper. Other than ensuring that\ngenerations could be parsed properly, all prompts were not optimized towards any particular\nmodels.\nD.1\nBenchmark Generation Few-Shot Prompts\nListing 13: All few-shot examples used for benchmark generation\nstring_1 = \"\"\"[TASK]\nstr.split\n[/TASK]\n[PYTHON]\ndef f(text):\nwords = text.split()\nresult = []\nfor i in range(len(words)):\nif i % 2 == 0:\nresult.append(words[i][::-1])\nelse:\nresult.append(words[i].upper())\nreturn \u2019 \u2019.join(result)\n[/PYTHON]\n[TEST]\nassert f(\"am7 fiDfd n\") == ??\nassert f(\"bnasadl\") == ??\nassert f(\"a j c n x X k\") == ??\nassert f(\"98 bask2 asoijdf9\") = ??\nassert f(\"\") == ??\n[/TEST]\"\"\"\nstring_2 = \"\"\"[TASK]\nstr.capitalize\n[/TASK]\n[PYTHON]\ndef f(text):\na = []\nwords = text.split(\u2019 \u2019)\nfor i in range(len(words)):\nif words[i][0].isdigit():\nreturn \u2019no\u2019\nif i%2 == 0:\na.append(words[i].capitalize())\nelse:\na.append(words[i])\nreturn \u2019 \u2019.join(a)\n[/PYTHON]\n[TEST]\nassert f(\"20xk flkawhf\") == ??\nassert f(\"lkw hj sfaibw fi 9\") == ??\nassert f(\"abbot 2929 mbpu\") == ??\nassert f(\"rotor zisxrs fh29nx\") == ??\nassert f(\"pxk 5 bxD 9\") == ??\n[/TEST]\"\"\"\nstring_3 = \"\"\"[TASK]\nstr.rindex\n43\n[/TASK]\n[PYTHON]\ndef f(text, char):\nindex = text.rindex(char)\nresult = list(text)\nwhile index > 0:\nresult[index] = result[index-1]\nresult[index-1] = char\nindex -= 2\nreturn \u2019\u2019.join(result)\n[/PYTHON]\n[TEST]\nassert f(\u2019mnjs krupa\u2019, \u2019u\u2019) == ??\nassert f(\u2019kqwomn0xj\u2019, \u2019m\u2019) == ??\nassert f(\u2019qpfi jzm\u2019, \u2019j\u2019) == ??\nassert f(\u2019102x0zoq\u2019, \u20190\u2019) == ??\nassert f(\u2019nzu\nei,\u2019, \u2019e\u2019) == ??\n[/TEST]\"\"\"\nstring_4 = \"\"\"[TASK]\nstr.rpartition\n[/TASK]\n[PYTHON]\ndef f(text, char):\nif char in text:\npref, char, suff = text.rpartition(char)\nsuff = suff[:-len(char)] + char + suff[len(char):]\nreturn suff + pref\nreturn text\n[/PYTHON]\n[TEST]\nassert f(\u2019smswfwe-r\u2019, \u2019-\u2019) == ??\nassert f(\u2019,wpzpppdl/\u2019, \u2019p\u2019) == ??\nassert f(\u20199284701\u2019, \u20192\u2019) == ??\nassert f(\u2019nvizoh2ja\u2019, \u2019c\u2019) == ??\nassert f(\u2019aaa0a1\u2019, \u2019a\u2019) == ??\n[/TEST]\"\"\"\nstring_5 = \"\"\"[TASK]\nstr.center\n[/TASK]\n[PYTHON]\ndef f(text):\nls = list(text)\nfor i in range(1, len(ls) - 1):\nls.insert(i, \u2019+\u2019)\nreturn \u2019\u2019.join(ls).center((len(ls) - 1) * 2)\n[/PYTHON]\n[TEST]\nassert f(\u2019lynel\u2019) == ??\nassert f(\u2019nzoh\u2019) == ??\nassert f(\u2019u\u2019) == ??\nassert f(\u2019anfsoixz\u2019) == ??\nassert f(\u2019xzd\u2019) == ??\n[/TEST]\"\"\"\nlist_1 = \"\"\"[TASK]\nlist.pop\n[/TASK]\n[PYTHON]\ndef f(names, num):\nqueue = names\n44\nwhile len(queue) > 1:\nfor _ in range(num):\nqueue.append(queue.pop(0))\nqueue.pop(0)\nreturn queue.pop()\n[/PYTHON]\n[TEST]\nassert f([\u2019aiwn\u2019, \u2019xke\u2019, \u2019mpwiy\u2019], 2) == ??\nassert f([\u2019y\u2019, \u2019z\u2019, \u2019cc\u2019, \u20192\u2019, \u20195\u2019, \u2019.\u2019, \u2019zksdfjn\u2019], 7) == ??\nassert f([\u201998bfaj\u2019, \u2019cn11\u2019, \u2019fakldj\u2019, \u2019tjasl\u2019, \u2019a\u2019], 10) == ??\nassert f([\u2019aghbvm\u2019], 1) == ??\nassert f([\u2019mnv\u2019, \u2019fjw\u2019, \u2019fnk\u2019], 0) == ??\n[/TEST]\"\"\"\nlist_2 = \"\"\"[TASK]\nlist.insert\n[/TASK]\n[PYTHON]\ndef f(text, position, value):\nlength = len(text)\nindex = position % (length + 1)\nif position < 0 or index < 0:\nindex = length // 2\nnew_text = list(text)\nnew_text.insert(index, value)\nreturn \u2019\u2019.join(new_text)\n[/PYTHON]\n[TEST]\nassert f(\u2019h grateful k\u2019, 3, \u2019h\u2019) == ??\nassert f(\u2019umjwi\u2019, -5, \u2019m\u2019) == ??\nassert f(\u2019coscifysu\u2019, 0, \u2019d\u2019) == ??\nassert f(\u2019fnmart\u2019, 4, \u2019o\u2019) == ??\nassert f(\u2019rzti\u2019, -1, \u2019a\u2019) == ??\n[/TEST]\"\"\"\nlist_3 = \"\"\"[TASK]\nlist.remove\n[/TASK]\n[PYTHON]\ndef f(array, elem):\narray.reverse()\ntry:\nwhile elem in array:\narray.remove(elem)\nfinally:\narray.reverse()\nreturn array\n[/PYTHON]\n[TEST]\nassert f([-1, 2, 1, -8, 2], 2) == ??\nassert f([], 2) == ??\nassert f([1], 1) == ??\nassert f([3, 6, 4, -2, 5], 4) == ??\nassert f([3, 2, 1, 2, 7, 1], 1) == ??\n[/TEST]\"\"\"\nlist_4 = \"\"\"[TASK]\nlist.append\n[/TASK]\n[PYTHON]\ndef f(nums):\ncount = len(nums)\n45\nfor i in range(-count+1, 0):\nnums.append(nums[i])\nreturn nums\n[/PYTHON]\n[TEST]\nassert f([2, 6, 1, 3, 1]) == ??\nassert f([7, 1, 2, 6, 0, 2]) == ??\nassert f([4, 3, 2, 1, 2, -1, 4, 2]) == ??\nassert f([0, 6, 2, -1, -2]) == ??\nassert f([-6, -2, 1, -3, 0, 1]) == ??\n[/TEST]\"\"\"\nlist_5 = \"\"\"[TASK]\nlist.index\n[/TASK]\n[PYTHON]\ndef f(nums, swap1, swap2):\ni1 = nums.index(swap1)\ni2 = nums.index(swap2)\nnums[i1], nums[i2], nums[i1 + 1], nums[i2 + 1] = nums[i2], nums[i1], nums[i2 + 1], nums[i1 + 1]\nreturn nums\n[/PYTHON]\n[TEST]\nassert f([6, 2, 1, 3, 4, 5], 3, 4) == ??\nassert f([1, 1, 5, 3, 1, 2], 1, 2) == ??\nassert f([1, 2, 1, 4, 1], 4, 2) == ??\nassert f([6, 2, 3, 1, 7, 5, 7], 3, 7) == ??\nassert f([2, 8, 8, 3, 8, 3, 9], 3, 2) == ??\n[/TEST]\"\"\"\nD.2\nDirect Prediction Prompts\nIn Listings 14, 15, 16, 17, and 18, we include the prompts we use for our evaluation. We use a\nfew-shot prompt for all models other than GPT models. For many models, we observed that\nusing the zero-shot prompt leads to a generation that are not in a easily parsable format, and\nincluding the few-shot examples led to predictable formatting. For fairness, we also measured the\nperformance of several few-shot prompts on the GPT models for a randomly sampled subset of the\nbenchmark (instead of the full benchmark for cost reasons). However, we observed a decrease in\nperformance compared to the zero-shot prompts for both input prediction and output prediction.\nTherefore, we decided to use the zero-shot prompt for GPT models and report numbers using that\nprompt. In addition, we use a separate output prediction prompt for Phind because the prompt in\nListing 16 often led to explanation text before completing the assert.\nListing 14: Input Prediction (non-GPT)\nYou will be given a function f and an output in the form f(??) == output. Find any input such that\n,\u2192 executing f on the input leads to the given output. There may be multiple answers, but you\n,\u2192 should only output one. Think step by step before arriving at an answer. Finally, surround the\n,\u2192\nanswer, with no additional words, with [ANSWER] and [/ANSWER] tags. Express your answer as a\n,\u2192 function call that when executed will give the output.\n[PYTHON]\ndef f(my_list):\ncount = 0\nfor i in my_list:\nif len(i) % 2 == 0:\ncount += 1\n46\nreturn count\nassert f(??) == 3\n[/PYTHON]\n[ANSWER]\nf([\"mq\", \"px\", \"zy\"])\n[/ANSWER]\n[PYTHON]\ndef f(s1, s2):\nreturn s1 + s2\nassert f(??) == \"banana\"\n[/PYTHON]\n[ANSWER]\nf(\"ba\", \"nana\")\n[/ANSWER]\n[PYTHON]\n{function}\nassert f(??) == {output}\n[/PYTHON]\n[ANSWER]\nListing 15: Input Prediction (GPT)\nYou will be given a function f and an output in the form output == f(??). Output the completion of the\n,\u2192\nlast line so that the code will run without errors by finding any input such that executing f\n,\u2192\non the input leads to the given output. There may be multiple answers, and you can output any\n,\u2192\none. Do NOT output any additional information.\n{function}\nassert {output} == f(\nListing 16: Output Prediction (non-GPT, non-Phind)\nBased on the given Python code, which may contain errors, complete the assert statement with the\n,\u2192 output when executing the code on the given test case. Do NOT output any extra information,\n,\u2192 even if the function is incorrect or incomplete. Do NOT output a description for the assert.\ndef f(n):\nreturn n\nassert f(17) == 17\n{function}\nassert f({input}) ==\nListing 17: Output Prediction (GPT)\nBased on the given Python code, which may contain errors, complete the assert statement with the\n,\u2192 output when executing the code on the given test case. Do not output any extra information,\n,\u2192 even if the function is incorrect or incomplete.\n{function}\nassert f({input}) ==\nListing 18: Output Prediction (Phind)\nBased on the given Python code, which may contain errors, complete the assert statement with the\n,\u2192 output when executing the code on the given test case. Do NOT output any extra information,\n,\u2192 even if the function is incorrect or incomplete. Output \"# done\" after the assertion.\n47\ndef f(n):\nreturn n\nassert f(17) == 17 # done\n{function}\nassert f({input}) ==\nD.3\nChain of Thought Prompts\nBelow, we include the prompts we use for the chain of thought experiments. For the same reasons\nmentioned in Appendix D.2, use a one-shot prompt for Code Llama models and a zero-shot prompt\nfor GPT models.\nListing 19: CoT input prediction prompt (Code Llama)\nYou will be given a function f and an output in the form f(??) == output. Your task is to find any\n,\u2192 input such that executing f on the input leads to the given output. There may be multiple\n,\u2192 answers, but only output one. First, think step by step. Then, surround the answer with [\n,\u2192 ANSWER] and [/ANSWER] tags. Express your answer as a function call that when executed will\n,\u2192 give the output.\ndef f(x):\nreturn x + 1\nassert f(??) == 17\nTo find an input such that executing f on the input leads to the given output, we can work backwards\n,\u2192 from the given assertion. We know that f(??) == 17.\nSince the function f(x) returns x + 1, for f(??) to be equal to 17, the value of ?? should be 16.\nTherefore, the function call that will give the output as 17 is:\n[ANSWER]f(16)[/ANSWER]\n{function}\nassert f(??) == {output}\nListing 20: CoT input prediction prompt (GPT)\nYou will be given a function f and an output in the form f(??) == output. Your task is to find any\n,\u2192 input such that executing f on the input leads to the given output. There may be multiple\n,\u2192 answers, but only output one. First, think step by step. Then, surround the answer with [\n,\u2192 ANSWER] and [/ANSWER] tags. Express your answer as a function call that when executed will\n,\u2192 give the output.\n{function}\nassert f(??) == {output}\nListing 21: CoT output prediction prompt (Code Llama)\nYou are given a function and an input. Complete the assertion with the output of executing the\n,\u2192 function on the input. First, reason step by step before arriving at an answer. Then, surround\n,\u2192\nthe answer as an assertion with [ANSWER] and [/ANSWER] tags.\ndef f(s):\nreturn s + \"a\"\nassert f(\"hi\") == ??\nThe function f takes a string s as input and returns the concatenation of s with the string \"a\".\n48\nTo determine the output of executing the function f on the input \"hi\", we need to concatenate \"hi\"\n,\u2192 with \"a\".\nTherefore, the output of executing the function f on the input \"hi\" is \"hia\".\n[ANSWER]assert f(\"hi\") == \"hia\"[/ANSWER]\n{function}\nassert f(input) == ??\nListing 22: CoT output prediction prompt (GPT)\nWhat should the output of this code be so that the assertion is correct? Reason step by step before\n,\u2192 arriving at an answer. Finally, surround the answer, with no additional words, with [ANSWER]\n,\u2192 and [/ANSWER] tags.\n{function}\n49\nAppendix E\nQualitative Analysis\nIn this section, we see some examples of interesting successes and failures of the best performing\nmodel, GPT-4, with and without CoT. GPT-4 is relatively sensitive to its prompt, and slight tweaks\nin the prompt may lead correct examples to fail or incorrect examples to succeed. However, we\nbelieve that these examples are nevertheless interesting and reveal insights into the operating\nmodes of GPT-4. Note that some of these examples may not be in the benchmark and were taken\nfrom a larger set of generated examples.\nE.1\nOutput Prediction without CoT\nE.1.1\nGPT-4 Successes without CoT, Output Prediction\nEven without CoT, we found that GPT-4 achieves impressively high pass@1 scores on output\nprediction. We highlight a few GPT-4 successes below that we found impressive, suggesting that\nGPT-4 has the capability to perform somewhat complex reasoning and code execution.\ndef f(text):\nif \u2019,\u2019 in text:\nbefore, _, after = text.partition(\u2019,\u2019)\nreturn after + \u2019 \u2019 + before\nreturn \u2019,\u2019 + text.partition(\u2019 \u2019)[-1] + \u2019 0\u2019\nassert f(\u2019244, 105, -90\u2019) == \u2019 105, -90 244\u2019\n# GPT-3.5 output: \u2019-90 244\u2019\n# CodeLlama 34B output: \u2019244, 105, -90 0\u2019\ndef f(text):\ntext = text.lower()\ncount = 0\nfor char in text:\nif char.isalpha():\ncount += 1\nreturn count\nassert f(\"The computer factory\") == 18\n# GPT-3.5 output: 3\n# CodeLlama 34B output: 16\ndef f(text):\nd = {}\nupdated = []\nfor ch in text:\nif ch in d:\nd[ch] += 1\nelse:\nd[ch] = 1\nwhile len(d) != 0:\nel = d.popitem()\nfor i in range(el[1]):\nupdated.append(el[0])\nreturn \u2019\u2019.join(updated)\nassert f(\u2019pdrq\u2019) == \u2019qrdp\u2019\n# GPT-3.5 output: \u2019pdrq\u2019\n50\n# CodeLlama 34B output: \u2019qprd\u2019\ndef f(a, b):\nb.reverse()\nc = b.copy()\nb.extend(a.copy())\nb.extend(c)\nreturn b\nassert f([5, 2, 3], [4, 9, 3, 1]) == [1, 3, 9, 4, 5, 2, 3, 1, 3, 9, 4]\n# GPT-3.5 output: [1, 3, 9, 4, 5, 2, 3]\n# CodeLlama 34B output: [4, 9, 3, 1, 5, 2, 3, 4, 9, 3, 1]\ndef f(s):\nret = \u2019;\u2019.join(sorted([c for c in s if c.isalnum()]))\nreturn ret\nassert f(\u2019%*^8938a(6^\u2019 * 3) == \u20193;3;3;6;6;6;8;8;8;8;8;8;9;9;9;a;a;a\u2019\n# GPT-3.5 and CodeLlama 34B both do not terminate after 500 tokens\ndef f(nums, a, b):\nnew_nums = []\nfor n in nums:\nif n < a or n > b:\nnew_nums.append(n)\nnew_nums.sort()\nnew_nums.extend(nums)\nreturn new_nums\nassert f([25, 44, 24, 22, 38, 5, 35, 15], 20, 44) == [5, 15, 25, 44, 24, 22, 38, 5, 35, 15]\n# GPT-3.5 output: [5, 15, 22, 24, 25, 35, 38, 44, 25, 44, 24, 22, 38, 5, 35, 15]\n# CodeLlama 34B output: [5, 15, 22, 24, 25, 35, 38, 44, 25, 44, 24, 22, 38, 5, 35, 15]\nE.1.2\nGPT-4 Failures without CoT, Output Prediction\nWe still find a set of relatively simple failures on output prediction, which we expect would be\nrelatively simple without CoT.\ndef f(nums):\nnums.reverse()\nreturn \"\".join(map(str, nums))\nassert f([-1, 9, 3, 1, -2]) == \u2019-2139-1\u2019\n# GPT-4 output: \"-2, 1, 3, 9, -1\"\ndef f(nums, num):\nfor i in nums:\nif nums[i]==num:\nreturn num\nreturn \u2019Not Found\u2019\nassert f({\u2019elad\u2019: 186, \u2019colton\u2019: 162, \u201912\u2019: 5}, 5) == \u20195\u2019\n# GPT-4 output: \u2019Not found\u2019\ndef f(text):\ndups = list(text)\n51\ndups.append(dups[0])\nreturn \u2019\u2019.join(dups)\nassert f(\u2019u\u2019) == \u2019uu\u2019\n# GPT-4 output: \u2019u\u2019\ndef f(match, fill, n):\nreturn fill[:n] + match\nassert f(\u20199\u2019, \u20198\u2019, 2) == \u201989\u2019\n# GPT-4 output: \u2019889\u2019\ndef f(string, prefix):\nif string.startswith(prefix):\nreturn string.removeprefix(prefix)\nreturn string\nassert f(\"Vipra\", \"via\") == \u2019Vipra\u2019\n# GPT-4 output: \"\"\nE.2\nInput Prediction without CoT\nSimilarly, we highlight examples from input prediction.\nE.2.1\nGPT-4 Successes without CoT, Input Prediction\ndef f(l, elems):\nl.reverse()\nl.extend(elems)\nl.extend(l)\nl.reverse()\nl.reverse()\ndel l[(len(l)-1):]\nreturn l\nassert f([], [-1, 2, 7, 2, 8]) == [-1, 2, 7, 2, 8, -1, 2, 7, 2]\n# GPT-3.5 output: f([2, 7, 2, 8], [-1])\n# CodeLlama 34B output: f([-1, 2, 7, 2, 8], [-1, 2, 7, 2])\ndef f(text, position):\nlength = len(text)\nindex = position % length\nif position < 0 or index < 0:\nindex = length // 2\nnew_text = list(text)\nnew_text.pop(index)\nreturn \u2019\u2019.join(new_text)\nassert f(\u2019voxnzcuo\u2019, 7) == \u2019voxnzcu\u2019\n# GPT-3.5 output: f(\u2019voxnzcu\u2019, 42)\n# CodeLlama 34B output: f(\u2019voxnzcu\u2019, -4)\ndef f(data, num):\nnew_dict = {}\ntemp = list(data.items())\n52\nfor i in range(len(temp) - 1, num - 1, -1):\nnew_dict[temp[i]] = None\nreturn temp[num:] + list(new_dict.items())\nassert f({2: 10, 3: 1}, 0) == [(2, 10), (3, 1), ((3, 1), None), ((2, 10), None)]\n# GPT-3.5 output: f({(2, 10): None, (3, 1): None}, 0)\n# CodeLlama 34B output: f({2: 10, 3: 1}, 2)\ndef f(nums):\nnums = [y for y in nums if y > 0]\nif len(nums) <= 3:\nreturn nums\nnums.reverse()\nhalf = len(nums)//2\nreturn nums[:half] + [0]*5 + nums[half:]\nassert f([10, -1, 3, 2, -5, -6, 2, 6]) == [6, 2, 0, 0, 0, 0, 0, 2, 3, 10]\n# GPT-3.5 output: f([6, 2, 0, 0, 0, 0, 0, 2, 3, 10])\n# CodeLlama 34B output: f([6, 2, 3, 10])\ndef f(text):\nupper = 0\nlower = 0\nfor i in range(len(text)):\nif text[i].islower():\nlower+=1\nelse:\nupper+=1\nif lower > upper:\nreturn text.swapcase()\nreturn text\nassert f(\u2019iAv,sual6eaParntEnthuhsisth,{kasae}\u2019) == \u2019IaV,SUAL6EApARNTeNTHUHSISTH,{KASAE}\u2019\n# GPT-3.5 output: f(\"\\\u2019iAv,sual6eaparnteNthuhsisth,{kasae}\\\u2019\")\n# CodeLlama 34B output: f(\"IaV,sual6eApArnTeNthuhsiStH,{KASAE}\")\nE.2.2\nGPT-4 Failures without CoT, Input Prediction\ndef f(name):\nreturn [name[0], name[1][::-1][0]]\nassert f(??) == [\u2019m\u2019, \u2019a\u2019]\n# GPT-4 output: f([\u2019m\u2019, \u2019apple\u2019])\ndef f(nums):\nif 0 in nums:\nnums.pop(nums.index(0))\nnums.insert(0, 0)\nif 1 in nums:\nnums.pop(nums.index(1))\nnums.append(1)\nreturn nums\nassert f(??) == [0, 0, 1, 0, 1]\n# GPT-4 output: f([0, 0, 1, 0, 1])\n53\ndef f(text, speaker):\nwhile text.startswith(speaker):\ntext = text[len(speaker):]\nreturn text\nassert f(??) == \u2019Do you know who the other was? [NEGMENDS]\u2019\n# GPT-4 output: f(\u2019Do you know who the other was? [NEGMENDS]\u2019, \u2019\u2019)\ndef f(text):\nif text.isdecimal():\nreturn text\nelse:\ns = \u2019\u2019.join(sorted(text))\nif s.isdecimal():\nreturn \u2019true\u2019\nelse:\nreturn \u2019false\u2019\nassert f(??) == \u2019133476815\u2019\n# GPT-4 output: 134678315\ndef f(length, text):\nif len(text) == length:\nreturn text[::-1]\nreturn False\nassert f(??) == False\n# GPT-4 output: f(5, \"Hello\")\nE.3\nOutput Prediction with CoT\nE.3.1\nGPT-4 Successes with CoT, Output Prediction\nWe see that when equipped with CoT, GPT-4 has some abilities to simulate an execution process,\nsimilar to what was seen in (Nye et al., 2021).\nIn this example, we see both the ability to reason about the flow of the code and the ability to count\nuppercase letters.\ndef f(string):\nupper = 0\nfor c in string:\nif c.isupper():\nupper += 1\nreturn upper * (2,1)[upper % 2]\nassert f(\u2019PoIOarTvpoead\u2019) == 8\nGPT-4: The function f(string) counts the number of uppercase letters in the input string. If the\n,\u2192 number of uppercase letters is even, it multiplies this count by 2. If the number of uppercase\n,\u2192\nletters is odd, it multiplies this count by 1.\nIn the string \u2019PoIOarTvpoead\u2019, there are 4 uppercase letters: \u2019P\u2019, \u2019I\u2019, \u2019O\u2019, and \u2019T\u2019. Since 4 is an\n,\u2192 even number, the function will return 4 * 2 = 8.\nSo, the correct output of the function for the input \u2019PoIOarTvpoead\u2019 is 8.\n54\n[ANSWER] 8 [/ANSWER]\nIn the next two examples, we see the ability of GPT-4 with CoT to do relatively complex string\nmanipulation tasks.\ndef f(text):\nreplaced = list(text)\nwhile \u2019 \u2019 in replaced: replaced.remove(\u2019 \u2019)\nwhile \u2019@\u2019 in replaced: replaced.remove(\u2019@\u2019)\nwhile \u2019&\u2019 in replaced: replaced.remove(\u2019&\u2019)\nreturn \u2019\u2019.join(replaced)\nassert f(\u2019&C C#BiOt &[(15-5%#-u&1@t\u2019) == \u2019CC#BiOt[(15-5%#-u1t\u2019\nGPT-4: The function \u2018f(text)\u2018 takes a string as input and returns a new string where all spaces (\u2019 \u2019),\n,\u2192\n\u2019@\u2019 symbols, and \u2019&\u2019 symbols have been removed.\nLet\u2019s apply this function to the string \u2019&C C#BiOt &[(15-5%#-u&1@t\u2019:\n1. Remove all spaces: \u2019&CC#BiOt&[(15-5%#-u&1@t\u2019\n2. Remove all \u2019@\u2019 symbols: \u2019&CC#BiOt&[(15-5%#-u&1t\u2019\n3. Remove all \u2019&\u2019 symbols: \u2019CC#BiOt[(15-5%#-u1t\u2019\nSo, the output of the function for the given string should be \u2019CC#BiOt[(15-5%#-u1t\u2019.\n[ANSWER] \u2019CC#BiOt[(15-5%#-u1t\u2019 [/ANSWER]\ndef f(widths):\nreversed_names = []\nfor skill in sorted(widths, key=widths.get, reverse=True):\nreversed_names.append(\"{0}={1}\".format(widths[skill], skill))\nreturn \u2019&\u2019.join(reversed_names)\nassert f({\u2019craftsmanship\u2019:2, \u2019upholstery\u2019:1, \u2019tailoring\u2019:3}) == \u20193=tailoring&2=craftsmanship&1=\n,\u2192 upholstery\u2019\nGPT-4: The function f takes a dictionary as an argument. The dictionary keys are skill names and the\n,\u2192 values are their corresponding widths. The function sorts the dictionary based on the values\n,\u2192 in descending order. Then it creates a list of strings where each string is a combination of\n,\u2192 the value and the key separated by an equal sign. Finally, it joins all the strings in the\n,\u2192 list with an ampersand (&) and returns the resulting string.\nLet\u2019s break down the steps:\n1. The dictionary {\u2019craftsmanship\u2019:2, \u2019upholstery\u2019:1, \u2019tailoring\u2019:3} is sorted based on the values in\n,\u2192 descending order, resulting in the list [\u2019tailoring\u2019, \u2019craftsmanship\u2019, \u2019upholstery\u2019].\n2. For each skill in the list, a string is created with the format \"{value}={key}\". The resulting list\n,\u2192\nis [\u20193=tailoring\u2019, \u20192=craftsmanship\u2019, \u20191=upholstery\u2019].\n3. The strings in the list are joined with an ampersand (&), resulting in the string \u20193=tailoring&2=\n,\u2192 craftsmanship&1=upholstery\u2019.\nSo, the output of the function f({\u2019craftsmanship\u2019:2, \u2019upholstery\u2019:1, \u2019tailoring\u2019:3}) should be \u20193=\n,\u2192 tailoring&2=craftsmanship&1=upholstery\u2019.\n[ANSWER]\u20193=tailoring&2=craftsmanship&1=upholstery\u2019[/ANSWER]\nIn the next two examples, we see the ability to simulate and reason about for/while loops.\ndef f(nums):\ncount = 0\n55\nfor i in range(len(nums)):\nif len(nums) == 0:\nbreak\nif count % 2 == 0:\nnums.pop()\nelse:\nnums.pop(0)\ncount += 1\nreturn nums\nassert f([3, 2, 0, 0, 2, 3]) == []\nGPT-4: The function \u2018f(nums)\u2018 takes a list of numbers as input. It initializes a counter \u2018count\u2018 to 0.\n,\u2192\nThen it enters a loop that runs for the length of the list.\nIn each iteration, it checks if the length of the list is 0. If it is, it breaks the loop. This\n,\u2192 condition will never be true because the loop runs for the initial length of the list and not\n,\u2192 the current length of the list.\nThen it checks if the counter is even. If it is, it removes the last element from the list. If it\u2019s\n,\u2192 not, it removes the first element from the list.\nAfter each iteration, it increments the counter by 1.\nSo, for the input list [3, 2, 0, 0, 2, 3], the function will perform the following operations:\n- Remove the last element (3) because the counter is 0 (even). The list becomes [3, 2, 0, 0, 2].\n- Remove the first element (3) because the counter is 1 (odd). The list becomes [2, 0, 0, 2].\n- Remove the last element (2) because the counter is 2 (even). The list becomes [2, 0, 0].\n- Remove the first element (2) because the counter is 3 (odd). The list becomes [0, 0].\n- Remove the last element (0) because the counter is 4 (even). The list becomes [0].\n- Remove the first element (0) because the counter is 5 (odd). The list becomes [].\nSo, the function should return an empty list for the input [3, 2, 0, 0, 2, 3].\n[ANSWER]\n[]\n[/ANSWER]\ndef f(number):\nmultiplier = 2\npower = 1\nwhile number > 0:\nremainder = number % multiplier\nnumber //= multiplier\nif remainder == 0:\npower += 1\nif number == 1:\nreturn f\u2019{2 ** power}-free\u2019\nassert f(8) == \u201916-free\u2019\n# GPT-4: The function f takes a number as input and checks if it is divisible by 2. If it is, it\n,\u2192 increments the power by 1 and continues to divide the number by 2 until the number is no\n,\u2192 longer divisible by 2. The function then returns a string that says the number is \"2 to the\n,\u2192 power of (power + 1)-free\".\nIn the case of the number 8, the function will divide 8 by 2 three times before it is no longer\n,\u2192 divisible by 2. This means the power will be incremented three times, from 1 to 4. Therefore,\n,\u2192 the function will return the string \"2 to the power of 4-free\", or \"16-free\".\nSo, the output of the code should be \"16-free\" for the assertion to be correct.\n[ANSWER]\"16-free\"[/ANSWER]\n56\nE.3.2\nGPT-4 Failures with CoT, Output Prediction\nNow, we highlight some failure cases of GPT-4 output prediction with CoT. One interesting failure\nmode is that the CoT makes a mistake on information that GPT-4 seems to know when prompted\nindependently. For example, in the first example below, the CoT insists that 6173 is less tan 1000.\nHowever, when prompted independently, GPT-4 knows this is not true. We provide a few examples\nof these paired mistakes with proof of knowledge below. The outputs are abbreviated for brevity\nand to highlight the failures.\nThe first example shows a failure in number comparison.\nPrompt: What is the output of the code?\ndef f(num):\nif 0 < num < 1000 and num != 6174:\nreturn \u2019Half Life\u2019\nreturn \u2019Not found\u2019\nassert f(6173) == ??\nGPT-4: ... In the case of \u2018f(6173)\u2018, the number 6173 is greater than 0, less than 1000, and not equal\n,\u2192 to 6174, so the function will return \u2019Half Life\u2019...\nPrompt: If num = 6173, is it true that 0 < num < 1000 and num != 6174?\nGPT-4: No, the statement is not true. The variable num is assigned the value 6173, so the condition 0\n,\u2192 < num < 1000 is false because 6173 is not less than 1000. The second condition, num != 6174,\n,\u2192 is true because 6173 is indeed not equal to 6174. However, since the first condition is false,\n,\u2192\nthe overall statement is false.\nThis example is showing a semantic misunderstanding of str.islower.\nPrompt: What is the output of the code?\ndef f(text):\nif not text.islower():\ntemp_text = \u2019\u2019\nfor i in range(len(text)):\nif text[i].islower():\ntemp_text += text[i]\nreturn temp_text\nreturn text\nassert f(\"816a\") == ??\nGPT-4: ... The function \u2018f\u2018 takes a string \u2018text\u2018 as input and checks if the string is not all\n,\u2192 lowercase using the \u2018islower()\u2018 method ...\n...In the given assertion, the input string is \u2018\"816a\"\u2018. This string is not all lowercase because it\n,\u2192 contains digits, which are neither lowercase nor uppercase. Therefore, the function will go\n,\u2192 into the loop and check each character\n...\nPrompt: What is \"816a\".islower()?\n57\nGPT-4: ... The string \"816a\" contains digits and a lowercase alphabetic character \u2019a\u2019. Since the only\n,\u2192 alphabetic character in the string is lowercase, the \u2018islower()\u2018 method would return \u2018True\u2018\n,\u2192 when called on this string...\nThis example shows a failure with sorting \"h e l l o\", which seems like it could be a tokenization\nartifact.\nPrompt: What is the output of this code?\ndef f(str):\nreturn str and \u2019\u2019.join(sorted(str))\nassert f(\"h e l l o\") == ??\nGPT-4: ...\n1. The input string \u2018\"h e l l o\"\u2018 is truthy, so the condition \u2018str\u2018 is \u2018True\u2018.\n2. The \u2018sorted\u2018 function is called on the string, which results in the list \u2018[\u2019 \u2019, \u2019 \u2019, \u2019 \u2019, \u2019e\u2019, \u2019h\u2019,\n,\u2192\n\u2019l\u2019, \u2019l\u2019, \u2019o\u2019]\u2018.\n...\nPrompt: In Python, what is sorted(\"h e l l o\")?\nGPT-4: ...Here\u2019s what happens:\n\u2018\u2018\u2018python\nresult = sorted(\"h e l l o\")\n\u2018\u2018\u2018\nThe \u2018result\u2018 will be a list that contains the characters of the string, including spaces, sorted in\n,\u2192 ascending order based on their Unicode code points. The space character has a lower code point\n,\u2192\nthan any of the letters, so it will appear first in the sorted list. The result will look\n,\u2192 like this:\n\u2018\u2018\u2018python\n[\u2019 \u2019, \u2019 \u2019, \u2019 \u2019, \u2019 \u2019, \u2019e\u2019, \u2019h\u2019, \u2019l\u2019, \u2019l\u2019, \u2019o\u2019]\n\u2018\u2018\u2018\nThis example shows a failure with the str.strip method.\nPrompt: What is the output of this code?\ndef f(text, strip_chars):\nreturn text[::-1].strip(strip_chars)[::-1]\nassert f(\u2019tcmfsmj\u2019, \u2019cfj\u2019) == ??\nGPT-4: The function \u2018f\u2018 takes two arguments: \u2018text\u2018 and \u2018strip_chars\u2018. It reverses the \u2018text\u2018 string,\n,\u2192 strips characters from both ends that are found in \u2018strip_chars\u2018, and then reverses the string\n,\u2192\nback to its original order.\nLet\u2019s break down what happens step by step with the input \u2018f(\u2019tcmfsmj\u2019, \u2019cfj\u2019)\u2018:\n1. The string \u2018\u2019tcmfsmj\u2019\u2018 is reversed, resulting in \u2018\u2019jmsfmct\u2019\u2018.\n2. The \u2018strip\u2018 method is called with \u2018\u2019cfj\u2019\u2018 as the argument, which removes any leading or trailing\n,\u2192 characters found in \u2018\u2019cfj\u2019\u2018 from the reversed string \u2018\u2019jmsfmct\u2019\u2018.\n3. The leading \u2018\u2019j\u2019\u2018 and the trailing \u2018\u2019c\u2019\u2018 and \u2018\u2019t\u2019\u2018 are stripped from the string, resulting in \u2018\u2019\n,\u2192 msfm\u2019\u2018.\n4. The stripped string \u2018\u2019msfm\u2019\u2018 is then reversed again, resulting in \u2018\u2019mfsm\u2019\u2018.\nTherefore, the assertion should be:\n\u2018\u2018\u2018python\n58\nassert f(\u2019tcmfsmj\u2019, \u2019cfj\u2019) == \u2019mfsm\u2019\n\u2018\u2018\u2018\nThe output of the code, if the assertion passes, is nothing since assertions do not produce output if\n,\u2192 the condition is true.\nPrompt: In Python, what is \"jmsfmct\".strip(\"cfj\")?\nGPT-4: ... Here\u2019s what happens step by step:\n- The first character \u2019j\u2019 is in the set of characters to remove (\u2019cfj\u2019), so it is removed.\n- The next character \u2019m\u2019 is not in the set of characters to remove, so the stripping stops at the\n,\u2192 beginning of the string.\n- Looking at the end of the string, the last character \u2019t\u2019 is not in the set of characters to remove,\n,\u2192 so the stripping stops at the end of the string without removing anything.\nThe resulting string after the \u2018strip\u2018 operation will be \u2018\"msfmct\"\u2018.\nIn this example, we observe a list indexing error.\nPrompt: What is the output of this code?\ndef f(nums):\ncount = len(nums)\nfor i in range(-count+1, 0):\nnums.extend([nums[i], nums[i]])\nreturn nums\nassert f([0, 6, 2, -1, -2]) == ??\nGPT-4: ...\n- Initial list: \u2018[0, 6, 2, -1, -2]\u2018\n- i = -4: Extend with two copies of \u2018nums[-4]\u2018 which is \u20186\u2018. New list: \u2018[0, 6, 2, -1, -2, 6, 6]\u2018\n- i = -3: Extend with two copies of \u2018nums[-3]\u2018 which is \u20182\u2018. New list: \u2018[0, 6, 2, -1, -2, 6, 6, 2, 2]\u2018\n...\nPrompt: In Python, what is the value at index -3 of [0, 6, 2, -1, -2, 6, 6]?\nGPT-4: In Python, negative indices count from the end of the list, with -1 being the last element. To\n,\u2192 find the value at index -3, we count three places from the end of the list.\nGiven the list:\n\u2018\u2018\u2018\n[0, 6, 2, -1, -2, 6, 6]\n\u2018\u2018\u2018\nThe indices of the elements are as follows:\n\u2018\u2018\u2018\n0\n1\n2\n3\n4\n5\n6\n(positive indices)\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n(negative indices)\n\u2018\u2018\u2018\nSo, the value at index -3 is \u2018-2\u2018.\nIn this example, we see a string indexing error.\nPrompt: In Python, what is the output of this code?\ndef f(text):\n59\nfor i in range(-len(text), 0):\nif text[i].islower():\nreturn text[:i] + text[i].swapcase() + text[-i:]\nreturn text\nassert f(\u2019Boo5\u2019) == ??\nGPT-4: ...\n- When \u2018i = -3\u2018, \u2018text[i]\u2018 is \u2018\u2019o\u2019\u2018, which is lowercase. So the function will return a new string\n,\u2192 according to the rule above...\n- \u2018text[-i:]\u2018 is \u2018\u2019o5\u2019\u2018 (the substring from the character after the current character to the end of\n,\u2192 the string).\nTherefore, the function will return \u2018\u2019BOo5\u2019\u2018...\nPrompt: In Python, if text = \"Boo5\" and i = -3, what is text[-i:]?\nGPT-4: ...Given \u2018text = \"Boo5\"\u2018 and \u2018i = -3\u2018, let\u2019s calculate \u2018-i\u2018:\n\u2018-i = -(-3) = 3\u2018\nSo, \u2018text[-i:]\u2018 is equivalent to \u2018text[3:]\u2018, which means we start from the 3rd index (keeping in mind\n,\u2192 that indexing starts at 0) and take all characters from that position to the end of the string\n,\u2192 .\nThe character at index 3 in the string \"Boo5\" is \u20195\u2019. Therefore, \u2018text[-i:]\u2018 will return \u20195\u2019.\nE.4\nInput Prediction with CoT\nE.4.1\nGPT-4 Successes with CoT, Output Prediction\nThe following examples highlights code reasoning abilities.\ndef f(text, size):\ncounter = len(text)\nfor i in range(size-int(size%2)):\ntext = \u2019 \u2019+text+\u2019 \u2019\ncounter += 2\nif counter >= size:\nreturn text\nassert f(??) == \u2019\n7\n\u2019\nGPT-4: The function f takes two arguments: a text and a size. It adds spaces to the beginning and end\n,\u2192 of the text until the length of the text is greater than or equal to the size. The function\n,\u2192 stops adding spaces when the length of the text is greater than or equal to the size.\nIn this case, the output is \u2019\n7\n\u2019, which is a string of length 11. The text in the middle is\n,\u2192 \u20197\u2019, which is of length 1. The function adds 5 spaces to the beginning and end of the text to\n,\u2192 reach a total length of 11.\nSo, the input to the function should be \u20197\u2019 and 11.\n[ANSWER]\nf(\u20197\u2019, 11)\n[/ANSWER]\ndef f(text):\n60\nuppercase_index = text.find(\u2019A\u2019)\nif uppercase_index >= 0:\nreturn text[:uppercase_index] + text[text.find(\u2019a\u2019) + 1 :]\nelse:\nreturn \u2019\u2019.join(sorted(text))\nassert f(??) == \u2019\nDEGHIVjkptx\u2019\nGPT-4: The function f takes a string as input. It first checks if there is an uppercase \u2019A\u2019 in the\n,\u2192 string. If there is, it removes the first occurrence of lowercase \u2019a\u2019 from the string and\n,\u2192 returns the string. If there is no uppercase \u2019A\u2019, it sorts the string in alphabetical order\n,\u2192 and returns it.\nIn this case, the output is \u2019\nDEGHIVjkptx\u2019, which is a sorted string. This means that the input\n,\u2192 string did not contain an uppercase \u2019A\u2019, and the function sorted the string.\nTo get this output, we can input a string that contains the same characters but in a different order.\n,\u2192 For example, the string \u2019xtpkjVIGHED\n\u2019 contains the same characters and when sorted by the\n,\u2192 function, it will give the output \u2019\nDEGHIVjkptx\u2019.\nSo, the answer is \u2019xtpkjVIGHED\n\u2019.\n[ANSWER]\n\u2019xtpkjVIGHED\n\u2019\n[/ANSWER]\nThe next example shows some ability in hexadecimal conversion. This may be partially due to\nmemorization and data contamination effects, but this exact hex string was not found after a\ncursory Google search.\ndef f(text):\nhex_string = text.encode(\u2019utf-16\u2019)\nresult = \"0x\"\nfor c in hex_string:\nresult += \"%.2X\" % c\nreturn result\nassert f(??) == \u20190xFFFE66006E007700200052006F006D0065002100\u2019\nGPT-4: The function f takes a string as input, encodes it in utf-16 format, and then converts each\n,\u2192 character into a hexadecimal representation. The output \u20190\n,\u2192 xFFFE66006E007700200052006F006D0065002100\u2019 is the hexadecimal representation of a string in\n,\u2192 utf-16 format.\nTo find the input, we need to convert the hexadecimal representation back to a string. We can do this\n,\u2192 by first removing the \u20190x\u2019 prefix and then converting each pair of hexadecimal digits back to\n,\u2192 a character.\nThe hexadecimal string \u2019FFFE66006E007700200052006F006D0065002100\u2019 corresponds to the utf-16 encoded\n,\u2192 string \u2019nw Rome!\u2019.\nSo, the input to the function f that will give the output \u20190xFFFE66006E007700200052006F006D0065002100\u2019\n,\u2192\nis \u2019nw Rome!\u2019.\n[ANSWER] f(\u2019nw Rome!\u2019) [/ANSWER]\nJust like we see in output prediction, GPT-4 is able to reason about non-trivial string manipulation\ncode.\ndef f(text):\ntext = text.split(\u2019,\u2019)\ntext.pop(0)\n61\ntext.insert(0, text.pop(text.index(\u2019T\u2019)))\nreturn \u2019T\u2019 + \u2019,\u2019 + \u2019,\u2019.join(text)\nassert f(??) == \u2019T,T,Sspp,G ,.tB,Vxk,Cct\u2019\nGPT-4: The function f takes a string as input, splits it into a list of substrings divided by commas,\n,\u2192 removes the first element of the list, finds the index of the substring \u2019T\u2019, removes it from\n,\u2192 its current position and inserts it at the beginning of the list. Finally, it joins the list\n,\u2192 into a string separated by commas and adds \u2019T\u2019 at the beginning of the string.\nTo get the output \u2019T,T,Sspp,G ,.tB,Vxk,Cct\u2019, we need to have \u2019T\u2019 as the second element in the list\n,\u2192 after splitting the input string by commas. The rest of the elements can be in any order. So,\n,\u2192 one possible input could be \u2019any,T,Sspp,G ,.tB,Vxk,Cct\u2019.\nLet\u2019s check this:\nf(\u2019any,T,Sspp,G ,.tB,Vxk,Cct\u2019) will split the input into [\u2019any\u2019, \u2019T\u2019, \u2019Sspp\u2019, \u2019G \u2019, \u2019.tB\u2019, \u2019Vxk\u2019, \u2019Cct\n,\u2192 \u2019], remove the first element to get [\u2019T\u2019, \u2019Sspp\u2019, \u2019G \u2019, \u2019.tB\u2019, \u2019Vxk\u2019, \u2019Cct\u2019], find the index\n,\u2192 of \u2019T\u2019 (which is 0), remove it from the list and insert it at the beginning to get the same\n,\u2192 list [\u2019T\u2019, \u2019Sspp\u2019, \u2019G \u2019, \u2019.tB\u2019, \u2019Vxk\u2019, \u2019Cct\u2019], join the list into a string \u2019T,Sspp,G ,.tB,Vxk,\n,\u2192 Cct\u2019 and add \u2019T\u2019 at the beginning to get \u2019T,T,Sspp,G ,.tB,Vxk,Cct\u2019, which is the desired\n,\u2192 output.\nSo, the answer is \u2019any,T,Sspp,G ,.tB,Vxk,Cct\u2019.\n[ANSWER] \u2019any,T,Sspp,G ,.tB,Vxk,Cct\u2019 [/ANSWER]\nIn the following example, GPT-4 shows simple code analysis abilities, understanding that the first\ncase will produce an error.\ndef f(nums):\ncount = len(nums)\nif count == 0:\nnums = [0] * int(nums.pop())\nelif count % 2 == 0:\nnums.clear()\nelse:\ndel nums[:count//2:]\nreturn nums\nassert f(??) == []\nGPT-4: The function f takes a list of numbers as input. If the length of the list is 0, it tries to\n,\u2192 pop an element from the list, which will cause an error because the list is empty. If the\n,\u2192 length of the list is even, it clears the list. If the length of the list is odd, it deletes\n,\u2192 the first half of the list.\nTo get an empty list as output, we can input an even-length list.\nFor example, we can input a list with two elements: [1, 2].\nSo, the answer is [1, 2].\n[ANSWER]\n[1, 2]\n[/ANSWER]\n62\nE.4.2\nGPT-4 Failures with CoT, Input Prediction\nAs with output prediction, also notice some relatively simple failures of CoT. One observation is\nthat GPT-4 often tries to check the incorrect input against the assertion and says that it passes, even\nwhen it does not. We believe this is not necessarily just due to the lack of execution ability, but a\nprior that the solution it found is more likely to be correct.\nIn these first three examples, the model goes directly to a wrong answer without much step-by-step\nreasoning.\ndef f(text):\nodd = \u2019\u2019\neven = \u2019\u2019\nfor i, c in enumerate(text):\nif i % 2 == 0:\neven += c\nelse:\nodd += c\nreturn even + odd.lower()\nassert f(??) == \u2019Mmohamt\u2019\nGPT-4: The function f takes a string as input and separates the characters at even and odd indices. It\n,\u2192\nthen concatenates the characters at even indices with the characters at odd indices converted\n,\u2192\nto lowercase.\nTo get the output \u2019Mmohamt\u2019, we can see that the even indexed characters are \u2019M\u2019, \u2019o\u2019, \u2019a\u2019, \u2019t\u2019 and\n,\u2192 the odd indexed characters are \u2019m\u2019, \u2019h\u2019, \u2019m\u2019.\nSo, the input string can be \u2019Mmohamt\u2019.\n[ANSWER]\nf(\u2019Mmohamt\u2019)\n[/ANSWER]\ndef f(text):\nstring = \u2019\u2019\nfor char in text:\nstring += char + char.lower()\nreturn string\nassert f(??) == \u2019llaallaakk\u2019\nGPT-4: The function f takes a string as input and for each character in the string, it appends the\n,\u2192 character itself and its lowercase version to a new string. The function then returns this new\n,\u2192\nstring.\nGiven the output \u2019llaallaakk\u2019, we can see that each character is repeated twice, once in its original\n,\u2192 form and once in lowercase. This means that the input string must have been \u2019LAK\u2019.\nSo, the input that leads to the given output when passed to function f is \u2019LAK\u2019.\n[ANSWER]\n\u2019LAK\u2019\n[/ANSWER]\ndef f(nums, i):\nnums.append(nums.pop(i % len(nums)))\nnums[i % len(nums)] = \u2019?\u2019\nsplit = nums[::2]\nsplit.remove(\u2019?\u2019)\nreturn split + nums[1::2]\n63\nassert f(??) == [2, 4, 4]\nGPT-4: The function f takes a list of numbers and an index as input. It pops the element at the index\n,\u2192 (modulo the length of the list) and appends it to the end of the list. Then it replaces the\n,\u2192 element at the same index with a \u2019?\u2019. It then splits the list into two lists: one with\n,\u2192 elements at even indices and one with elements at odd indices. It removes the \u2019?\u2019 from the\n,\u2192 first list and returns the concatenation of the two lists.\nTo get the output [2, 4, 4], we need to find a list and an index such that after popping the element\n,\u2192 at the index and appending it to the end, and replacing the element at the index with \u2019?\u2019, the\n,\u2192\nlist split into even and odd indexed elements (with \u2019?\u2019 removed) gives [2, 4, 4].\nOne possible input could be the list [2, 4, 4, 4] and the index 1. After popping the element at index\n,\u2192 1 and appending it to the end, the list becomes [2, 4, 4, 4]. Replacing the element at index 1\n,\u2192\nwith \u2019?\u2019 gives [2, \u2019?\u2019, 4, 4]. Splitting into even and odd indexed elements gives [2, 4] and\n,\u2192 [\u2019?\u2019, 4]. Removing \u2019?\u2019 from the first list gives [2, 4], and concatenating the two lists gives\n,\u2192\n[2, 4, 4].\nSo, the input f([2, 4, 4, 4], 1) will give the output [2, 4, 4].\nThe following example is a bit tricky, because the naming of the variables does not match the\nsemantics of str.removeprefix.\ndef f(prefix, s):\nreturn str.removeprefix(prefix, s)\nassert f(??) ==\u2019hymi\u2019\nGPT-4: First, we need to understand what the function does. The function f takes two arguments: a\n,\u2192 prefix and a string s. It then removes the prefix from the string s and returns the result.\nGiven that the output is \u2019hymi\u2019, we can infer that the string s must have ended with \u2019hymi\u2019 after the\n,\u2192 prefix was removed. Therefore, we can choose any prefix and append \u2019hymi\u2019 to it to form the\n,\u2192 string s.\nFor example, if we choose the prefix to be \u2019hello\u2019, then the string s would be \u2019hellohymi\u2019. When the\n,\u2192 prefix \u2019hello\u2019 is removed from \u2019hellohymi\u2019, the result is \u2019hymi\u2019, which is the desired output.\nSo, the input to the function f that would give the output \u2019hymi\u2019 is (\u2019hello\u2019, \u2019hellohymi\u2019).\n[ANSWER]\n(\u2019hello\u2019, \u2019hellohymi\u2019)\n[/ANSWER]\nThe following example shows a case where GPT-4 \u201dchecks\u201d the forward execution incorrectly.\ndef f(text):\ncount = len(text)\nfor i in range(-count+1, 0):\ntext = text + text[i]\nreturn text\nassert f(??) ==\u2019wlace Alc l\n\u2019\nGPT-4: The function f takes a string as input and appends the characters from the end of the string to\n,\u2192\nthe string itself. The number of characters appended is equal to the length of the original\n,\u2192 string minus one.\nTo get the output \u2019wlace Alc l\n\u2019, we need to find a string that when appended with its last len(\n,\u2192 string)-1 characters, gives \u2019wlace Alc l\n\u2019.\nBy observing the output, we can see that the last 3 characters are spaces. This means that the last 3\n,\u2192 characters of the original string must also be spaces.\n64\nThe first character of the output is \u2019w\u2019, which is the first character of the original string. The\n,\u2192 second character of the output is \u2019l\u2019, which is the last character of the original string.\nFrom this, we can infer that the original string is \u2019w l\n\u2019.\nLet\u2019s check this:\nf(\u2019w l\n\u2019) = \u2019w l\n\u2019 + \u2019 l\n\u2019 = \u2019wlace Alc l\n\u2019}\nSo, the input that gives the output \u2019wlace Alc l\n\u2019 when passed to the function f is \u2019w l\n\u2019.\nSo, the answer is:\n[ANSWER] \u2019w l\n\u2019 [/ANSWER]\nE.4.3\nGPT-4 Failures with CoT, Examples Requiring Insights\nUnlike output prediction, where stepping through the code line by line suffices, some input\nprediction examples require insights and observations that can be difficult for models like GPT-4\nto make. These examples are a bit more challenging, and passing them can require making these\ninsights. Here, we highlight some of these examples, corresponding relevant insights, and GPT-4\u2019s\nfailure modes.\nIn the following example, new nums consists of a sorted portion and an unsorted portion. One\ninsight is that the sorted portion must be a subarray of the unsorted portion. GPT-4 always takes [5,\n15, 25, 44] to be the sorted portion and [24, 22, 38, 5, 35, 15] to be the unsorted portion,\nwhich cannot be the case because 44 (from the sorted portion) is not in the unsorted portion.\ndef f(nums, a, b):\nnew_nums = []\nfor n in nums:\nif n < a or n > b:\nnew_nums.append(n)\nnew_nums.sort()\nnew_nums.extend(nums)\nreturn new_nums\nassert f(??) == [5, 15, 25, 44, 24, 22, 38, 5, 35, 15]\nIn the following example, the simplest solution is to bypass the while loop completely, but the\nmodel does not find it. However, the model chooses an output like \"baec\" because it back-translates\nthe \u201da\u201d to \u201di\u201d using the translation table. Unfortunately, it does not take into account that other\ncharacters also get translated in the translation table.\ndef f(input_string):\ntable = str.maketrans(\u2019aioe\u2019, \u2019ioua\u2019)\nwhile \u2019a\u2019 in input_string or \u2019A\u2019 in input_string:\ninput_string = input_string.translate(table)\nreturn input_string\nassert f(??) == \u2019biec\u2019\nIn the following example, one must notice that x cannot end with a number and y cannot start\nwith a number. Reasoning about x+y = \u2019nisou79-85233\u2019, GPT-4 incorrectly deduces that the y\nportion must be 85233, while in reality, it must be 79-85233 because x cannot end with a digit.\n65\ndef f(urn):\npieces = urn.split(\u2019--\u2019)\nx = pieces[0].rstrip(\u20190123456789\u2019)\ny = pieces[1].lstrip(\u20190123456789\u2019)\nreturn x+y\nassert f(??) == \u2019nisou79-85233\u2019\nIn the following example, it is very easy to solve the task by choosing source and target to make\ntarget in source false, going directly to the last line. However, GPT-4 tries to go down the more\ncomplicated path.\ndef f(source, target):\nif target in source:\nprefix, suff = source.split(target)\nprefix = prefix[:len(prefix) // 2] + target + prefix[len(prefix) // 2:]\nreturn prefix + suff\nreturn source\nassert f(\u2019abcjava\u2019, \u2019python\u2019) ==\u2019abcjava\u2019\nIn the following example, an important insight is that the last line of the function is a\n\"....\".join(), but that \"....\" does not occur in the output. This confuses the model.\ndef f(letters):\nletters_only = letters.strip(\"., !?*\")\nreturn \"....\".join(letters_only.split(\" \"))\nassert f(??) ==\u2019h,e,l,l,o,wo,r,ld\u2019\nE.5\nSimple and Isolated GPT-4 Failures\nSome of the failures from both input prediction and output prediction can be isolated to capabilities\non smaller subtasks. Here, we highlight some interesting subtasks, as well as GPT-4\u2019s response to\nthese subtasks.\nPrompt: What is a string containing \u2019a\u2019 three times, \u2019b\u2019 three times, \u2019c\u2019 twice, \u2019d\u2019 three times, and\n,\u2192 \u2019z\u2019 twice?\nGPT-4: A string containing the specified number of each letter can be constructed in many different\n,\u2192 ways, as the order of the characters is not specified. Here is one example of such a string:\n\u2018aaabbbccdzz\u2018\nPrompt: In Python, what is \" BaB \".rfind(\" B \")?\nGPT-4: In Python, the \u2018rfind()\u2018 method of a string object returns the highest index of the substring\n,\u2192 if found in the given string. If the substring is not found, it returns -1.\nThe string \u2018\" BaB \"\u2018 has a space, followed by a capital \u2019B\u2019, followed by another space as the\n,\u2192 substring we are looking for with \u2018rfind(\" B \")\u2018.\nLet\u2019s break down the string \u2018\" BaB \"\u2018:\n- Index 0: space (\u2019 \u2019)\n- Index 1: \u2019B\u2019\n- Index 2: \u2019a\u2019\n- Index 3: \u2019B\u2019\n66\n- Index 4: space (\u2019 \u2019)\nWe are looking for the substring \u2018\" B \"\u2018 (space, \u2019B\u2019, space). This substring appears only once in the\n,\u2192 string, starting at index 3 and ending at index 5 (if we consider the space after the last \u2019B\n,\u2192 \u2019).\nTherefore, \u2018\" BaB \".rfind(\" B \")\u2018 will return 3, because that is the highest index at which the\n,\u2192 substring \u2018\" B \"\u2018 starts.\nPrompt: In Python, if I have a string s = \u2019iabnm~~~~~~~~~~\u2019, what is s[1::2]?\nGPT-4: In Python, the slicing operation \u2018s[start:stop:step]\u2018 allows you to extract a substring from a\n,\u2192 string \u2018s\u2018 by specifying a starting index \u2018start\u2018, a stopping index \u2018stop\u2018, and a step \u2018step\u2018.\n,\u2192\nIf \u2018start\u2018 is omitted, it defaults to the beginning of the string (index 0). If \u2018stop\u2018 is\n,\u2192 omitted, it defaults to the end of the string. If \u2018step\u2018 is omitted, it defaults to 1.\nIn your example, \u2018s[1::2]\u2018 means that you want to start at index 1 of the string \u2018s\u2018, go until the end\n,\u2192\nof the string (since no stop index is provided), and select every second character (because\n,\u2192 the step is 2).\nGiven the string \u2018s = \u2019iabnm~~~~~~~~~~\u2019\u2018, let\u2019s break down what \u2018s[1::2]\u2018 will return:\n- Start at index 1: \u2019a\u2019\n- Every second character from index 1: \u2019abnm~~~~~~~~~~\u2019 -> \u2019an~\u2019\nSo, \u2018s[1::2]\u2018 will return the string \u2018\u2019an~\u2019\u2018.\nPrompt: In Python, what is \"+\".join([\u2019*\u2019, \u2019+\u2019, \u2019n\u2019, \u2019z\u2019, \u2019o\u2019, \u2019h\u2019])?\nGPT-4: In Python, the \u2018join\u2018 method is used to concatenate a sequence of strings with a specified\n,\u2192 separator. In the example you provided, \u2018\"+\".join([\u2019*\u2019, \u2019+\u2019, \u2019n\u2019, \u2019z\u2019, \u2019o\u2019, \u2019h\u2019])\u2018, the \u2018join\u2018\n,\u2192\nmethod is called on the string \u2018\"+\"\u2018, which is the separator, and it concatenates the\n,\u2192 elements of the list \u2018[\u2019*\u2019, \u2019+\u2019, \u2019n\u2019, \u2019z\u2019, \u2019o\u2019, \u2019h\u2019]\u2018 with a plus sign between each element.\nThe result of this operation would be a single string where each element from the list is separated by\n,\u2192\na plus sign:\n\u2018\u2018\u2018\n*+*+n+z+o+h\n\u2018\u2018\u2018\nEach element in the list is a string, and they are joined together in the order they appear in the\n,\u2192 list, with the \u2018+\u2018 character inserted between each pair of elements.\nPrompt: In Python, if text = \"!123Leap and the net will appear\" and res = 123, what is text[len(str(\n,\u2192 res)):]?\nGPT-4: In the given Python code snippet, you have a string \u2018text\u2018 and an integer \u2018res\u2018.\n\u2018\u2018\u2018python\ntext = \"!123Leap and the net will appear\"\nres = 123\n\u2018\u2018\u2018\nYou want to know what \u2018text[len(str(res)):]\u2018 evaluates to. Let\u2019s break it down step by step:\n1. \u2018str(res)\u2018 converts the integer \u2018res\u2018 to a string, which is \u2018\"123\"\u2018.\n2. \u2018len(str(res))\u2018 calculates the length of the string \u2018\"123\"\u2018, which is \u20183\u2018.\n3. \u2018text[len(str(res)):]\u2018 slices the string \u2018text\u2018 starting from the index equal to the length of the\n,\u2192 string representation of \u2018res\u2018 (which is \u20183\u2018) to the end of the string.\n67\nSo, \u2018text[len(str(res)):]\u2018 will return the substring of \u2018text\u2018 starting from the 4th character (since\n,\u2192 indexing in Python starts at 0) to the end of the string.\nThe 4th character in the string \u2018text\u2018 is the character \u2018\u2019L\u2019\u2018 (the first three characters are \u2018\"!12\"\u2018)\n,\u2192 , so the result will be:\n\u2018\u2018\u2018python\n\"Leap and the net will appear\"\n\u2018\u2018\u2018\nThis is the substring of \u2018text\u2018 starting from the 4th character to the end.\nPrompt: In Python, what is \"pomodoro\".replace(\"or\", \"pomodoro\")?\nGPT-4: In Python, the \u2018replace\u2018 method is used to replace occurrences of a substring within a string\n,\u2192 with another substring. When you call \u2018\"pomodoro\".replace(\"or\", \"pomodoro\")\u2018, you are asking\n,\u2192 Python to replace every occurrence of the substring \u2018\"or\"\u2018 in the string \u2018\"pomodoro\"\u2018 with the\n,\u2192\nsubstring \u2018\"pomodoro\"\u2018.\nHere\u2019s what happens step by step:\n1. Python finds the first occurrence of \u2018\"or\"\u2018 in \u2018\"pomodoro\"\u2018, which is at the end of the string.\n2. Python replaces this occurrence with \u2018\"pomodoro\"\u2018, resulting in the string \u2018\"pomodopomodoro\"\u2018.\nThere is only one occurrence of \u2018\"or\"\u2018 in the original string, so the final result of the \u2018replace\u2018\n,\u2192 method in this case is \u2018\"pomodopomodoro\"\u2018.\nE.6\nFailing Gems\nNext, we identify a collection of examples that we find GPT-4 often fails on, even with CoT. Some\nof these examples overlap with examples shown above, but others are new. Overall, we believe this\ncollection of examples provides an interesting lens to understand the behaviour of GPT-4 on input\nand output prediction.\nE.6.1\nFailing Gems, Output Prediction\ndef f(nums):\nfor i in range(-len(nums), 0):\nnums.insert(-i, nums[i])\nreturn nums\nassert f([-6, -2, 1, -3, 0, 1]) == [-6, -6, -2, 1, 1, 1, -3, 0, 0, 1, 1, -6]\ndef f(text):\nif not text.islower():\ntemp_text = \u2019\u2019\nfor i in range(len(text)):\nif text[i].islower():\ntemp_text += text[i]\nreturn temp_text\nreturn text\nassert f(\"816a\") == \u2019816a\u2019\ndef f(list, separator):\ntext = separator.join(list)\nreturn separator.join(reversed(text))\n68\nassert f([\u2019is\u2019, \u2019it\u2019, \u2019top\u2019], \u2019@\u2019) == \u2019p@o@t@@@t@i@@@s@i\u2019\ndef f(text, res):\nfor c in \u2019*\\n\"\u2019:\ntext = text.replace(c, \u2019!\u2019 + str(res))\nif text.startswith(\u2019!\u2019):\ntext = text[len(str(res)):]\nreturn text\nassert f(\u2019\"Leap and the net will appear\u2019, 123) == \u20193Leap and the net will appear\u2019\ndef f(num):\nif 0 < num < 1000 and num != 6174:\nreturn \u2019Half Life\u2019\nreturn \u2019Not found\u2019\nassert f(6173) == \u2019Not found\u2019\ndef f(date):\nreturn\ndate[6:] + date[4:6] + date[0:4]\nassert f(\"08-10-2009\") == \u201920090-08-1\u2019\ndef f(text, suffix):\nif suffix and suffix[-1] in text:\nreturn f(text.rstrip(suffix[-1]), suffix[:-1])\nelse:\nreturn text\nassert f(\u2019rpyttc\u2019, \u2019cyt\u2019) == \u2019rpytt\u2019\ndef f(s, x):\ncount = 0\nfor i, c in enumerate(s):\nif x in s[i:] and x not in s[:i]:\ncount += 1\nreturn count\nassert f(\u2019fvyijrtwrjrsasgt\u2019, \u2019g\u2019) == 15\ndef f(text):\nsegments = text.split()\nfor i in range(len(segments)):\nsegments[i] = segments[i][0].upper() + segments[i][1:-1] + segments[i][-1].upper()\nreturn \u2019 \u2019.join(segments)\nassert f(\"hey !\") == \u2019HeY !!\u2019\ndef f(pattern, items):\nresult = []\nfor text in items:\npos = text.rfind(pattern)\nif pos >= 0:\nresult.append(pos)\nreturn result\nassert f(\" B \", [\" bBb \", \" BaB \", \" bB\", \" bBbB \", \" bbb\"]) == []\ndef f(str):\nreturn str and \u2019\u2019.join(sorted(str))\nassert f(\"h e l l o\") == \u2019\nehllo\u2019\ndef f(t):\nreturn t.replace(\u2019or\u2019, t.center(len(t), \u2019o\u2019))\nassert f(\"pomodoro\") == \u2019pomodpomodoroo\u2019\n69\nE.6.2\nFailing Gems, Input Prediction\ndef f(dimension):\ndinline = str(dimension)[1:].zfill(2)\nreturn dinline[0] * int(dinline[1])\nassert f(??) == \u2019kkkkk\u2019\ndef f(text):\nfor elem in text:\nif elem.isupper():\ntry:\ntext.remove(elem)\nexcept ValueError:\npass\nreturn text\nassert f(??) == \u2019\u2019\ndef f(text):\nls = list(text)\nfor i in range(0, len(ls)):\nif ls[i]!=\u2019+\u2019:\nls.insert(i, \u2019+\u2019)\nls.insert(i, \u2019*\u2019)\nbreak\nreturn \u2019+\u2019.join(ls)\nassert f(\u2019nzoh\u2019) == \u2019*+++n+z+o+h\u2019\ndef f(text):\nnew_text = list(text)\ndict = {}\nfor char in new_text:\ndict[char] = new_text.count(char)\nreturn dict\nassert f(\u2019aaabbbccdddzz\u2019) == {\u2019a\u2019: 3, \u2019b\u2019: 3, \u2019c\u2019: 2, \u2019d\u2019: 3, \u2019z\u2019: 2}\ndef f(text):\nodd = \u2019\u2019\neven = \u2019\u2019\nfor i, c in enumerate(text):\nif i % 2 == 0:\neven += c\nelse:\nodd += c\nreturn even + odd.lower()\nassert f(\u2019Mammoth\u2019) == \u2019Mmohamt\u2019\ndef f(nums, i):\nnums.append(nums.pop(i % len(nums)))\nnums[i % len(nums)] = \u2019?\u2019\nsplit = nums[::2]\nsplit.remove(\u2019?\u2019)\nreturn split + nums[1::2]\nassert f([4, 2, 4, 2], 0) == [2, 4, 4]\ndef f(prefix, s):\nreturn str.removeprefix(prefix, s)\nassert f(\u2019hymi\u2019, \u2019hymifulhxhzpnyihyf\u2019) == \u2019hymi\u2019\ndef f(text):\nif \u2019,\u2019 in text:\nbefore, _, after = text.partition(\u2019,\u2019)\nreturn after + \u2019 \u2019 + before\n70\nreturn \u2019,\u2019 + text.partition(\u2019 \u2019)[-1] + \u2019 0\u2019\nassert f(\u2019244, 105, -90\u2019) == \u2019 105, -90 244\u2019\ndef f(s):\nreturn \u2019{}{}{}\u2019.format(s[3:], s[2], s[5:8])\nassert f(\u2019jbucwc\u2019) == \u2019cwcuc\u2019\ndef f(nums):\nfor i in range(len(nums)):\nnums.insert(i, nums[i]**2)\nreturn nums\nassert f([1, 2, 4]) == [1, 1, 1, 1, 2, 4]\ndef f(c, text):\nt = c\nfor c in reversed(text):\nt = c + t*2\nt = c + t\nreturn t + text\nassert f(\u2019;?\u2019, \u2019i\u2019) == \u2019ii;?;?i\u2019\ndef f(nums, location, item):\nif len(nums) >= location and 0 <= location:\nreturn nums.insert(location, item)\nreturn nums\nassert f([1, 2, 3, 4, 5, 6], -5, -5) == [1, 2, 3, 4, 5, 6]\ndef f(text):\nreturn max(text.find(ch) for ch in \u2019aeiou\u2019)\nassert f(\"qsqgijwmmhbchoj\") == 13\n71\n"
  },
  {
    "title": "Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach",
    "link": "https://arxiv.org/pdf/2401.02987.pdf",
    "upvote": "8",
    "text": "HAS YOUR PRE-TRAINED MODEL IMPROVED?\nA MULTI-HEAD POSTERIOR BASED APPROACH\nPrince Aboagye\u2217\nYan Zheng\u2217\u2020\nJunpeng Wang\u2217\u2020\nUday Singh Saini\u2217\u2020\nXin Dai\u2020\nMichael Yeh\u2020\nYujie Fan\u2020\nZhongfang Zhuang\u2020\nShubham Jain\u2020\nLiang Wang\u2020\nWei Zhang\u2217\u2020\nABSTRACT\nThe emergence of pre-trained models has significantly impacted Natural Lan-\nguage Processing (NLP) and Computer Vision to relational datasets. Tradition-\nally, these models are assessed through fine-tuned downstream tasks. However,\nthis raises the question of how to evaluate these models more efficiently and ef-\nfectively. In this study, we explore a novel approach where we leverage the meta-\nfeatures associated with each entity as a source of worldly knowledge and employ\nentity representations from the models. We propose using the consistency between\nthese representations and the meta-features as a metric for evaluating pre-trained\nmodels. Our method\u2019s effectiveness is demonstrated across various domains, in-\ncluding models with relational datasets, large language models, and image models.\n1\nINTRODUCTION\nThe rise in user-generated content has led to the widespread adoption of pre-training large models\nacross numerous machine-learning fields. This trend is particularly noticeable in Natural Language\nProcessing (NLP) with GPT (Generative Pretrained Transformer) models (Brown et al., 2020) and\nvision-language domains with models like CLIP (Radford et al., 2021). Usually, the performance\nof these models is assessed through various downstream tasks such as Machine Translation (Wang\net al., 2023), Factuality (Chen et al., 2019), Question answering (Liang et al., 2023), Multilingual\ntasks (Bang et al., 2023; Ahuja et al., 2023), which can be relatively expensive if a large number of\ntasks are considered necessary for a robust evaluation. Is there an alternative approach that offers\nboth efficiency and simplicity for evaluating these models?\nEntity representations, also known as embeddings generated from these models, can be utilized\ndirectly or indirectly by downstream tasks and fine-tuned as needed. The associated meta-features\nwith these embeddings can be considered as the model\u2019s foundational knowledge of the world it\u2019s\nlearning from. This could be the class category for image data or semantic and syntactic information\nfor words. Despite having the same meta-features, embeddings differ across models. Therefore, the\ndegree of consistency between the embeddings and meta-features can serve as a performance metric\nfor model evaluation.\nEmbedding spaces are complex and challenging to interpret or explain. Despite extensive efforts\nto decipher it, its intricacies go beyond mere linear interpretability, as some research suggests. In\nthis research, we hypothesize that the embeddings reside within a manifold space where Euclidean\ndistance is not an appropriate metric for gauging the similarity between two embeddings. Meta-\nfeatures are capable of grouping these embeddings into clusters, we assume each forming a sub-\nmanifold space. When the clusters are sufficiently fine-grained, it is possible to approximate each\ncluster using a Gaussian distribution. Collectively, these clusters form a mixture of Gaussian distri-\nbutions. By determining the posterior probabilities of the entities, the consistency of meta-features\nand embeddings can be assessed.\nIn this study, we introduce a unique approach to evaluate the performance of pre-trained models.\nSpecifically, we:\n\u2217These authors contributed equally to this work. Correspondence email: wzhan@visa.com\n\u2020Visa Research, Palo Alto, CA.\n1\narXiv:2401.02987v4  [cs.CL]  14 Feb 2024\n1. Adopt a distinct perspective towards the model. Instead of focusing solely on downstream\nperformance, we emphasize the quality of the entities\u2019 representations that the model can\ngenerate.\n2. Consider the features associated with the entity representations as the benchmark to assess\ntheir quality. We hypothesize that the meta-features can partition the embedding space\ninto distinct clusters. The quality of these clusters can be evaluated using the posterior\nprobability of Gaussian mixture models.\n3. While there are multiple methods to interpret these meta-feature spaces, we present a tree-\nbased approach as an example that uses meta-features to segment entities into clusters.\n4. Test our proposed method\u2019s effectiveness on various datasets across domains, ranging from\nrecommendation-based to language and image models. We present both qualitative and\nquantitative evidence to demonstrate the approach\u2019s efficacy.\n2\nRELATED WORK\nThis section reviews the literature on three areas: 1) Pre-trained models, 2) Vision-Language Pre-\ntraining (VLP), and 3) Pretrained Dual-Transformers (PDT) for Bipartite Graphs.\n2.1\nPRE-TRAINED MODELS\nLarge Language Models (LLMs): In recent years, significant strides have been made in the realm\nof Natural Language Processing (NLP), particularly with the advent of the transformer architecture.\nAttention-based language models such as BERT (Kenton & Toutanova, 2019), GPT (Brown et al.,\n2020), XLNet (Yang et al., 2019), and T5 (Raffel et al., 2020) have raised the bar in various language\nbenchmarks. Alongside these developments, a plethora of pre-training and fine-tuning algorithms\nhave been devised to enhance the performance of these transformer models. As these models grew\nin size, the data-driven nature and scaling characteristics of the transformer architecture became\nevident. These critical findings paved the way for the creation of large language models (LLMs),\nincluding LLaMa 2 (Touvron et al., 2023) with 7-70 billion parameters, BLOOM (Workshop et al.,\n2022) with 176 billion parameters, and GPT4 (OpenAI, 2023) with an astounding 1.7 trillion pa-\nrameters. These LLMs demonstrate impressive emergent capabilities, such as solving mathematical\nequations and analyzing articles, competencies not seen in prior smaller language models. These\nbreakthroughs signify the remarkable progress made in this field.\nVision-Language Pre-training (VLP): With the rapid expansion of model capacity and compu-\ntational resources, the input to deep neural networks has evolved beyond a single modality, such\nas text or image. Vision-language pre-training (VLP) was introduced to bridge the gap between\ndifferent modalities, effectively harnessing cross-modality information from both images and text.\nLeveraging the successful pre-training and fine-tuning paradigm prevalent in NLP, VLP models\nhave demonstrated exceptional performance in complex vision-language tasks. These tasks include\nimage captioning, visual question answering, and visual reasoning. Among the existing studies, a\nnoteworthy contribution is the CLIP (Radford et al., 2021) model, which employs the concept of\ncontrastive learning to align images and text. CLIP simultaneously trains an image encoder and\na text encoder on millions of image-text pairs collected from the internet. The resulting encoders\nhave demonstrated impressive performance on downstream tasks due to their zero-shot classification\ncapability.\nPretrained Dual-Transformers (PDT) for Bipartite Graphs: PDT (Dai et al., 2023) focuses on\nlearning contextual knowledge from a user-content interaction dataset, which is depicted as a bipar-\ntite graph. The study identifies two key contexts in the graph: user-side and content-side. The goal\nof learning from these contexts is framed as two contrastive learning tasks and is applied to a rec-\nommendation task. Evaluations of two large popular datasets reveal that PDT outperforms baselines\nin six metrics.\n2\n3\nALGORITHM FRAMEWORK\nThis section presents the algorithmic framework of our proposed metric for evaluating embeddings.\nFor a given domain, we first collect a large size of entities with rich meta-features. Then for any given\npre-trained model, we can generate an embedding dataset denoted as X = x1, ..., xN, where each\nxi \u2208 Rd and 1 \u2264 i \u2264 N. Here, N represents the number of entities, and d signifies the dimension\nof the embeddings. Simultaneously, we can produce a corresponding feature set F = f1, ..., fN.\nEach feature fi comprises categorical and numerical features. We convert numerical features into\ncategorical ones for consistency. The primary objective is to examine the consistency between these\ntwo datasets, X and F.\nIn the simplest scenario where fi has only one feature, a straightforward segmentation method is to\nform clusters based solely on these features, with each category creating its cluster. However, when\nfi has m features, and each feature has kj categories, the number of combinations becomes Qm\nj=1 kj.\nThis results in a significantly larger number of clusters. We will postpone the discussion on finding\nthe best split to a later session. In the upcoming discussion, we will assume that we already have a\nsplit criterion for the dataset X.\n4\nPROPOSED METHOD: POSTERIOR BASED EMBEDDING EVALUATING\nMETRIC\nWe aim to evaluate the effectiveness of different models that generate these embeddings to determine\nthe best one. The splitting criteria, S, can divide the entities into a group of clusters C1, C2, ..., Cn,\nwith each entity belonging to a single cluster, where n is the number of clusters. To evaluate the\nquality of the cluster, we adopt a posterior-based method.\nIn the context of GMM, it is assumed that the data is generated from a combination of multiple\nGaussian distributions. Each component of this mixture corresponds to one of the distinct clusters\nwithin the dataset. The probability of any given data point, x, belonging to the kth cluster is esti-\nmated by computing the posterior probability in the GMM framework which can be expressed as\nfollows:\nP(\u03b8 = k|x) =\nP(x|\u03b8 = k)P(\u03b8 = k)\nPm\nj=1 P(x|\u03b8 = j)P(\u03b8 = j),\n(1)\nwhere P(\u03b8 = k) = number of points in the kth cluster\nN\nTo assess the quality of the embeddings X within the context of a splitting S, we compute the overall\nevaluation metric by averaging the log probabilities of all the embeddings across all clusters. This\nmetric provides an assessment of the quality of the embeddings. We call it the average of the log\nposterior (ALP).\nALP X\nS = 1\nN\nm\nX\nk=1\nX\nxi\u2208Ck\nlog P(\u03b8 = k|xi)\n(2)\nOne challenge with the formula above is its high sensitivity to outliers. A single outlier could lead\nto an extremely large value for ALP X\nS . We implement a clipping mechanism for embeddings with\nvery small posterior probabilities to mitigate the impact of such outlier entities. Specifically, if\nP(\u03b8 = k|xk) is less than k/N \u2217 \u03b5, we exclude the entity from the ALP X\nS computation.\nAnother challenge arises when the embeddings exist in large dimensions. If the number of em-\nbeddings in each cluster is smaller than the embedding dimension, this leads to a rank-deficient\ncovariance. To address this, we propose a multi-head solution. In this approach, each head is a\nrandomly selected v dimensions from the existing d dimensions and ALP is estimated based on\nthese dimensions. This process is repeated multiple times, and the average results are used which\nwe refer to as the Mean of the . This concept is inspired by the random forest algorithm (Breiman,\n2001) and Matryoshka Representation Learning (Kusupati et al., 2022). Additionally, we apply the\n3\nroutine regularization approach, i.e., adding \u03f5I to the covariance matrix. The value of \u03f5 is decided\nin the following manner.\n\u03f5 = max(\u03bbk/(10D)\u03bb0, 1e\u22128),\n(3)\nWhere D is the dimensionality of the embeddings and \u03bbi are the eigenvalues of the covariance matrix\n(sorted decreasingly by their magnitude). k is the minimum value that satisfies \u03a3k\ni=0\u03bbi\n\u03a3D\ni=0\u03bbi > 99.99%.\n4.1\nONE META FEATURE BASED CLUSTERING\nIn the simplest scenario, where the feature vector fi consists of only one feature, a straightforward\nand intuitive approach to segmentation is to form clusters based solely on these features. This\nmethod is straightforward as it capitalizes on the inherent characteristics of the data. Each unique\ncategory within the data forms its distinct cluster, effectively grouping similar items. The consis-\ntency of these clusters with the embeddings can serve as a measure of the quality of the embeddings.\nHowever, it\u2019s important to note that extending this approach to accommodate more than two meta-\nfeatures is not as straightforward.\n4.2\nMETA FEATURES + REPRESENTATION BASED SEGMENTATION\nInspired by EmbeddingTree algorithm, we can construct the tree based on the entities, and all the\nleaf nodes are the final clusters, specifically: We first convert non-binary categorical features into\nbinary ones by asking yes-no questions regarding each of their categorical values and get the binary\nfeature sets: G = {g1, ..., gN} (gi \u2208 {0, 1}q, 1 \u2264 i \u2264 N), q denote the total number of converted\nbinary features.\nWith the processed data, we describe the EmbeddingTree algorithm with details in Algorithm 1. We\niterate through the q features (line 6) and evaluate them based on the splitting criteria described in\nSection 4.3 to pick out the best feature for splitting (line 8-10), using the feature\u2019s binary value (line\n11-13). The above procedure is executed recursively (lines 15-16) until the splitting criterion (\u0398),\ne.g., the number of entities per tree node or the tree depth, is no longer satisfied (line 2). With the\ngiven embedding and feature data, the whole procedure is deterministic.\nAlgorithm 1 Build an EmbeddingTree\n1: procedure BUILDTREE([X, F], q, \u0398)\n2:\nif \u0398 is not satisfied then\n3:\nreturn LeafNode([X, F])\n4:\nelse\n5:\nmax t \u2190 \u2212\u221e\n6:\nfor k \u2208 {1, ..., q} do\n7:\nt = Embedding \u2212 MAP([X, Fk])\n8:\nif t > max t then\n9:\nbestFea = k\n10:\nmax t = t\n11:\n[X, F]left = {x \u2208 X|FbestF ea == 0}\n12:\n13:\n[X, F]right = {x \u2208 X|FbestF ea == 1}\n14:\n15:\nChildren.Left = BuildTree([X, F]left, q, \u0398)\n16:\nChildren.Right = BuildTree([X, F]right, q, \u0398)\n17:\nreturn Children\n4.3\n2-GMM SPLITTING WITH MAXIMUM A POSTERIORI ESTIMATION (MAP)\nOne critical component of Algorithm 1 is the criterion for selecting the best splitting feature. The\ncriterion is computed based on the approximate MAP for GMMs inspired by (Zheng et al., 2023).\nWe assume the embedding can be modeled as two mixture Gaussians.\nThe expectation-\nmaximization (EM) algorithm is used to estimate all the parameters and latent variables jointly.\n4\nThe latent variables, zi,j, denote the probability that sample i is in cluster j.\nWith N as the\nnumber of observations and J as the number of Gaussian clusters (in this case, J = 2), z =\n{z1,1, z1,2, ..., zN,J\u22121, zN,J}, the complete likelihood (including the latent variables) is:\nP(x, \u00b5, \u03a3, w, z) =\nN\nY\ni=1\nJ\nY\nj=1\n{wjN(xn; \u00b5j, \u03a32\nj)}zi,j,\n(4)\nwhere \u00b5 is the mean vectors and \u03a3 is covariance matrix of the Gaussians.\nWe go through every feature to find the best binary feature that splits the embedding and forms the\nbest GMM. Each candidate binary feature splits the embeddings into two clusters, each formulated\nas a Gaussian. For each feature, suppose the first s embeddings have feature value Fk = 0 and the\nrest N \u2212 s embeddings have feature value Fk = 1. We estimate both clusters\u2019 weights, means, and\nvariances using the maximum likelihood estimation (MLE).\n\u02c6\u00b51 = 1\ns\ns\nX\ni=1\nxi,\n\u02c6\n\u03a31 = 1\ns\ns\nX\ni=1\n(xi \u2212 \u02c6\u00b51)(xi \u2212 \u02c6\u00b51)T ,\n\u02c6\nw1 = s\nN ,\n\u02c6\u00b52 =\n1\nN \u2212 s\nN\nX\ni=s+1\nxi,\n\u02c6\n\u03a32 =\n1\nN \u2212 s\nN\nX\ni=s+1\n(xi \u2212 \u02c6\u00b52)(xi \u2212 \u02c6\u00b52)T ,\n\u02c6\nw2 = N \u2212 s\nN\n.\nIn other words, our algorithm performs a hard clustering rather than the soft clustering of GMM.\nThus, if xi is in cluster j, then zi,j = 1 and zi,j\u2032 = 0 for all j \u0338= j\u2032. Given this approximation, the\nlikelihood can be obtained by summing over the z:\nP(x, \u00b5, \u03a3, w) =\nX\nz\nN\nY\ni=1\nJ\nY\nj=1\n{wjN(xn; \u00b5j, \u03a32\nj)}zi,j\n(5)\nNote that z(i\u2208(0,s],j=1) = z(i\u2208[s+1,N),j=2) = 1 and zi,j = 0, otherwise, the above equation can be\nsimplified to:\nP(x, \u00b5, \u03a3, w) =\ns\nY\ni=1\nw1N(xi; \u00b51, \u03a31\n2)\nN\nY\ni=s+1\nw2N(xi; \u00b52, \u03a32\n2).\n(6)\nWe can treat each split feature as another random variable \u03b8. To choose the best-split feature,\nwe want to maximize P(x, \u00b5, \u03a3, w, \u03b8); in other words, we want to find \u03b8 that gives the largest\nP(x, \u00b5, \u03a3, w).\n4.4\nFINDING THE BEST SPLITTING POINT\nFor simplicity, we only consider \u03b8 as the random variable we want to estimate; by injecting the prior\ninformation into the formula, we can treat each splitting feature with a different weight. By applying\nMaximum A Posteriori Estimation (MAP), we can formulate the problem as follows:\nP(\u03b8i|x) =\nP(x|\u03b8i)P(\u03b8i)\nPq\nj=1 P(x|\u03b8j)P(\u03b8j),\n(7)\nwhere q is the number of possible splits.\nBy plugging (3) into (4), we can get\nP(\u03b8i|x) =\nQs\nk=1 w1N(xk; \u00b51, \u03a31\n2, \u03b8i) QN\nk=s+1 w2N(xk; \u00b52, \u03a32\n2, \u03b8i)p(\u03b8i)\nPq\nj=1\nQs\nk=1 w1N(xk; \u00b51, \u03a31\n2, \u03b8j) QN\nk=s+1 w2N(xk; \u00b52, \u03a32\n2, \u03b8j)p(\u03b8j)\n.\n(8)\n5\nPlugging in the estimates for all the parameters and taking the log of P(\u03b8i|x), we can get\nlog \u02c6P(\u03b8i|x) =\ns\nX\ni=1\n[log \u02c6\nw1+log N(xi; \u02c6\u00b51, \u02c6\n\u03a31\n2)]+\nN\nX\ni=s+1\n[log \u02c6w2+log N(xi; \u02c6\u00b52, \u02c6\n\u03a32\n2)]+log p(\u03b8i)\n\u2212 log(\nq\nX\nj=1\ns\nY\nk=1\nw1N(xk; \u02c6\u00b51, \u02c6\u03a32\n1, \u03b8j)\nN\nY\nk=s+1\nw2N(xk; \u02c6\u00b52, \u02c6\u03a32\n2, \u03b8j)p(\u03b8j)).\n(9)\nBy applying this formula, we can use the prior knowledge of the importance of the feature to find\nthe split that maximizes log \u02c6P.\n4.5\nEMBEDDING COMPARISON BASED ON THE SAME SPLITTING CRITERIA\nIf we have two sets of embeddings, XA = {xA1, . . . , xAN}, and XB = {xB1, . . . , xBN},\nboth trained on the same dataset but using different models, denoted as models A and B, where\n(xAi, xBi \u2208 Rp, 1 \u2264 i \u2264 N), we can generate two corresponding splitting criteria, SA and SB.\nThe objective now is to assess and compare the quality of these two sets of embeddings. Let\u2019s\nrepresent ALP XA\nSA\nas ALP A\nA for embeddings XA and splitting criteria SA. Given two sets of\nembeddings, XA and XB, along with two corresponding splitting criteria, SA and SB, we can de-\nfine four metrics: ALP A\nA , ALP B\nB , ALP B\nA , and ALP A\nB . We need to fix the splitting criteria to do\nclustering, so a proper comparison should be between ALP A\nA and ALP B\nA or between ALP A\nB and\nALP B\nB .\n5\nEXPERIMENTAL ANALYSIS\nIn this experimental session, we initially conducted experiments on a synthetic dataset to verify the\neffectiveness of our proposed algorithm. Following this, we further evaluate the results in three ar-\neas: the MovieLens dataset (Harper & Konstan, 2015) for relational models, spatial datasets (Gurnee\n& Tegmark, 2023) for large language models, and the Robustness library (Engstrom et al., 2019a;b;\nSanturkar et al., 2019; 2020) for image models.\n5.1\nSYNTHETIC DATASET: GAUSSIAN MIXTURE MODEL (GMM) OF TEN GAUSSIAN\nDISTRIBUTIONS\nTo validate the efficacy of our proposed posterior-based embedding evaluation metric, as outlined\nin Equation 2, we designed an experiment encompassing three scenarios, each with 10 clusters.\nThese clusters were generated such that they are either perfectly separated, partially overlapping, or\nperfectly overlapping and are all generated using a Gaussian Mixture Model. Figure 1 presents the\nresults from these scenarios. As anticipated, the Average of the Log Posterior (ALP) scores for the\nten (10) perfectly separated Gaussian Distributions was 0, and the accuracy of the clusters assigned\nfrom the posterior matrix was 100%. In the case of 10 partially overlapping Gaussian Distributions,\nthe ALP score was \u22120.3285, and the accuracy of the clusters assigned from the posterior matrix was\n86.96%. Similarly, for the ten (10) perfectly overlapping Gaussian Distributions, the ALP score was\n\u22120.9372, and the accuracy of cluster assignment was 57.34%.\n(a) Perfectly separated clusters\n(b) Partially overlapping clusters\n(c) Perfectly overlapping clusters\nFigure 1: Illustration on a 2D synthetic dataset consisting of 10 Gaussian distributions that are\nperfectly separated, partially overlapping, and perfectly overlapping.\n6\n5.2\nMOIVE LENS DATASET FOR RELATIONAL\nMovieLens-25M consists of 25,000,095 reviews made by 162,541 reviewers on 59,047 movies.\nWe compare the following models to generate the movie embeddings. Word2vec (Mikolov et al.,\n2013), PDT (Dai et al., 2023) and SASRec (Kang & McAuley, 2018).\nFrom the Word2Vec\nmodel, we generate two distinct types of embedding representations, specifically w2v single and\nw2v combine. In the case of w2v single, we create a sequence of movies for each reviewer\u2019s\nreview, sort it based on time, and then employ the Word2vec model to learn the movie embeddings.\nOn the other hand, w2v combine not only includes the sequences of movies for reviewers but also\nincorporates sequences of reviewers for each movie and reviewer/movie pairs as part of the training\ndata. For both SASRec and PDT, we generate sequences of nine movies for each reviewer. Addi-\ntionally, for PDT, we generate sequences of reviewers for each movie, as this is a necessary input\nfor PDT. SASRec is trained using BPR loss, while PDT is trained using two contrastive losses.\nBoth PDT and SASRec are contextualized embeddings, while w2v single and w2v combine\nare static embeddings. We employ two clustering techniques. The first approach involves clustering\nby single meta-features, such as year and genre. We also apply the Embedding tree-based method\nto generate the tree for both year and genre features and use the leaf nodes as clusters.\n5.2.1\nMOVIE LENS DATASET: CLUSTERING BY YEAR\nWe evaluated and compared four kinds of embedding representations trained on the MovieLens\nDataset. These were trained at various iteration levels; we use the \u201cyear\u201d feature as labels to cluster\nthe embeddings. As illustrated Figure 2 (a), the PDT and SRSREC embedding performed better than\nall embeddings across all iterations, as seen in the Mean of the average of log posteriors plot. During\nthe early stages of training from iteration 1 to 16, w2v combine outperformed w2v single.\nHowever, in the later stages from iteration 16 to 50, w2v single superseded w2v combine.\n(a) Mean of the average of log\nposteriors\n(b) Mean of accuracy of assigned\nclusters\nFigure 2: Mean of the average of the log posterior and accuracy on the MovieLens dataset by\nclustering on year.\nAs depicted in the Mean of accuracy of assigned clusters plot of Figure 2 (b), PDT and SRSREC\ndemonstrates a consistent and stable performance over all other types of embeddings across all it-\nerations. Generally, w2v single exceeds the performance of w2v combine. This suggests that\ncontextualized embeddings, specifically PDT and SRSREC, most effectively encode year informa-\ntion and remain stable across all iterations. Also, w2v single demonstrates superior encoding of\nyear information compared to w2v combine.\n5.2.2\nMOVIE LENS DATASET: CLUSTERING BY GENRE\nHere, we created clusters with the genre features as labels. We then compute and report the Mean\nof the average of log posteriors and the Mean of accuracy of assigned clusters. These findings are\npresented in Figure 3. Contrary to the consistent pattern observed with year features as labels, the\ngenre features do not exhibit a similar consistency. From Figure 3 (a), it\u2019s noticeable that the PDT\nembedding generally outperforms both SASRec and w2v single over all iterations. Furthermore,\nSASRec surpasses w2v single from the 1st to the 40th iteration, after which they both plateau\nwith similar scores. Between the 12th and 36th iterations, w2v combine is observed to outperform\nPDT. Moving to the Mean accuracy of the assigned clusters plot (Figure 3 (b)), it\u2019s evident that PDT\nconsistently outperforms all other embedding types across all iterations. Generally, w2v combine\n7\nsurpasses both SASRec and w2v single, except for the first and third iterations where SASRec\nexceeds w2v combine.\n(a) Mean of the average of log\nposteriors\n(b) Mean of accuracy of assigned\nclusters\nFigure 3: Mean of the average of the log posterior and accuracy on the MovieLens dataset by\nclustering on the genre.\n5.2.3\nMOVIE LENS DATASET: CLUSTERING WITH TREE LEAF NODES ON GENRE AND\nYEAR AS THE META FEATURES\nWe also explored the case where we used the tree leaf nodes from the embedding tree constructed\nwith year and genre as meta-features. The Mean average of log posteriors of assigned clusters is cal-\nculated and reported, as shown in Figure 4 (a). The PDT and SASRec embeddings consistently sur-\npass other embeddings throughout all iterations. However, we notice that w2v combine surpasses\nw2v single from the 1st to the 26th iteration, but w2v single overtakes w2v combine from\nthe 26th to the 50th iteration. The Mean accuracy of assigned clusters, illustrated in Figure 4 (b),\nclearly shows that the PDT and SASRec embedding exhibit a steady and consistent increase in per-\nformance compared to all other embeddings across all iterations. This is followed by w2v single,\nwhich generally surpasses w2v combine.\n(a) Mean of the average of log\nposteriors\n(b) Mean of accuracy of assigned\nclusters\nFigure 4: Mean of the average of the log posterior and accuracy on the MovieLens dataset by\nclustering with tree leaf nodes.\nAll the above experiment results suggest that the contextualized embeddings SASRec and PDT are\nmore effective at capturing the semantic and structural relationships in the input data compared to\ntheir static embedding counterparts, namely w2v single and w2v combine, which meet our\nexpectations.\n5.3\nEXPERIMENTS WITH EMBEDDING FROM LLAMA-2 MODELS\nIn this session, we implement the proposed concept in the field of large language models and use\nLlama-2 as an example.\nModels: The released Llama-2 models (Touvron et al. (2023)) have three versions with different\nmodel sizes and embedding dimensionalities. These details have been included in Table 1.\nDatasets: We use the Llama-2 models to generate embeddings for three spatial datasets introduced\nby (Gurnee & Tegmark, 2023). The details of these datasets are shown in Table 2. Each dataset\n8\ninstance is a word/phrase for a location (e.g., the name of a city, a university, or a place of interest).\nBased on the spatial location as a meta feature, we generate the clusters for each dataset, i.e., based\non the continent, state, and borough of the instances from the World Place, USA Place, and NYC\nPlace dataset, respectively.\nTable 1: Details of the Llama-2 models.\nModel\n#head\n#layer\n#dim\nLlama-2-7b\n32\n32\n4096\nLlama-2-13b\n40\n40\n5120\nLlama-2-70b\n64\n80\n8192\nTable 2: Details of the three spatial datasets.\nDataset\n#clusters\n#samples\nCluster\nWorld Place\n8\n39585\ncontinent\nUS Place\n49\n29997\nstate\nNYC Place\n7\n19838\nborough\nEmbedding Quality: We feed the three datasets into the three (3) Llama-2 models and get their\nactivations/embeddings at each layer of the models. Specifically, we obtain 32, 40, and 80 sets\nof embeddings for the three sets of models (as the three models have 32, 40, and 80 layers, re-\nspectively). We use our proposed method for each set of embeddings to compute the posterior of\nindividual instances falling into the correct cluster.\nFrom Figure 5 and Figure 6, we present the effectiveness of embeddings across three different\ndatasets and models. The x-axis of each graph indicates the percentage of layers relevant to the\ncorresponding Llama-2 models, while the y-axis represents the Mean average of log posteriors and\naccuracy. The results show a noticeable upward trend in the quality of embeddings from the ini-\ntial to the final layers. Specifically, in the World Place, USA Place, and NYC Place datasets, the\ngreen lines, which denote the larger Llama-2-70b model, exhibit the highest levels of posteriors and\naccuracy. This indicates that the Llama-2-70b model is more adept at incorporating location data\ncompared to the other models.\n0.00\n0.25\n0.50\n0.75\n1.00\n1.75\n1.50\n1.25\n1.00\n0.75\n0.50\n0.25\navg_posterior\nWorld_Place\nModel\nLlama-2-7b\nLlama-2-13b\nLlama-2-70b\n0.00\n0.25\n0.50\n0.75\n1.00\n4\n3\n2\n1\n0\nUSA_Place\n0.00\n0.25\n0.50\n0.75\n1.00\n2.2\n2.0\n1.8\n1.6\n1.4\n1.2\nNYC_Place\nFigure 5: Embedding quality over model layers (average of the log posterior). The dimensions are\ndivided into subsets, each comprising 128 dimensions.\n0.00\n0.25\n0.50\n0.75\n1.00\n0.6\n0.7\n0.8\n0.9\naccuracy\nWorld_Place\nModel\nLlama-2-7b\nLlama-2-13b\nLlama-2-70b\n0.00\n0.25\n0.50\n0.75\n1.00\n0.2\n0.4\n0.6\n0.8\n1.0\nUSA_Place\n0.00\n0.25\n0.50\n0.75\n1.00\n0.40\n0.45\n0.50\n0.55\n0.60\nNYC_Place\nFigure 6: Embedding quality over model layers (accuracy of assigned clusters). The dimensions are\ndivided into subsets, each comprising 128 dimensions.\n5.4\nEXPERIMENT ANALYSIS OF CLIP EMBEDDINGS ON THE BREEDS HIERARCHY\nIn this section, we assess the ability of our metric to predict the classification performance on em-\nbeddings of 3 pre-trained CLIP (Radford et al., 2021) models on datasets from the Breeds Hierarchy\n9\n(Santurkar et al., 2020), namely datasets entity 13, entity 30, living 17 and nonliving 26. Next, we\nbriefly describe the Breeds Dataset and the CLIP models used for our evaluation.\nDataset: Breeds Hierarchy (Santurkar et al., 2020) was created by leveraging and modifying the\nWordNet (Miller, 1994) hierarchy for the ImageNet (Deng et al., 2009) dataset to group together\nsemantically similar classes into one (1) superclass. The original purpose of creating this hierarchy\nwas to use the subpopulations present in superclasses to detect a model\u2019s robustness to distribution\nshift. For this experiment, we leverage the entire dataset to evaluate the performance of our metric\nin predicting the generalization of CLIP models on those embeddings.\nModels: For this study, we use CLIP-ViT transformers ViT-L/14, ViT-B/32, and ViT-B/16 trained\non 224 px \u00d7 224 px images as image encoders, where ViT-L/14 encodes each image into a 768-\ndimensional embedding and ViT-B/16 and ViT-B/32 encode each image as a 512-dimensional em-\nbedding. We obtained model embeddings for each network from the final image encoder and the\nmean embeddings of internal multi-head self-attention layers. We train a linear probe on the Ima-\ngeNet (Deng et al., 2009) training subset of the Breeds dataset and learn the parameters for estimat-\ning the log posterior (ALP). To compute the average log posterior, we don\u2019t split the embeddings\ninto multiple blocks; therefore, the average log posterior (ALP) demonstrates the flexibility of our\napproach. We correlate the performance of these two metrics to understand our trained posterior\u2019s\nbehavior better and present the analysis and results next.\nTable 3: Pearson\u2019s Correlation on Breeds Datasets (Part 1): Comparing layerwise log posterior and\nlinear probe accuracy across regularization levels in the training and validation sets of the Breeds\nHierarchy.\nEntity 13 Dataset\nTrain Set\nVal. Set\nReg.\nB16 B32 L14 B16 B32 L14\nDiag\n.99\n.99\n.98\n.99\n.99\n.97\n10\u22123\n.97\n.99\n.97\n.97\n.98\n.95\n10\u22126\n.97\n.96\n.98\n.99\n.98\n.99\n10\u22129\n.96\n.95\n.98\n.99\n.99\n.99\nEntity 30 Dataset\nTrain Set\nVal. Set\nReg.\nB16\nB32\nL14\nB16\nB32\nL14\nDiag\n.99\n.99\n.99\n.99\n.99\n.98\n10\u22123\n.98\n.97\n.98\n.98\n.98\n.98\n10\u22126\n.85\n.83\n.89\n.97\n.96\n.99\n10\u22129\n.8\n.76\n.87\n.96\n.96\n.98\nTable 4: Pearson\u2019s Correlation on Breeds Datasets (Part 2): Comparing layerwise log posterior and\nlinear probe accuracy across regularization levels in the training and validation sets of the Breeds\nHierarchy.\nLiving 17 Dataset\nTrain Set\nVal. Set\nReg.\nB16 B32 L14 B16 B32 L14\nDiag\n.99\n.99\n.99\n.99\n.99\n.98\n10\u22123\n.93\n.93\n.95\n.97\n.96\n.99\n10\u22126\n.68\n.65\n.72\n.93\n.94\n.97\n10\u22129\n.57\n.49\n.64\n.93\n.95\n.97\nNon Living 26 Dataset\nTrain Set\nVal. Set\nReg.\nB16\nB32\nL14\nB16\nB32\nL14\nDiag\n.99\n.99\n.98\n.99\n.99\n.98\n10\u22123\n.98\n.95\n.98\n.98\n.98\n.97\n10\u22126\n.72\n.67\n.75\n.97\n.97\n.99\n10\u22129\n.54\n.42\n.64\n.96\n.97\n.98\nFor this experiment, we measured the correlation between average log posteriors and linear probe\naccuracy learned and computed over the Breeds training and validation set embeddings. The results\nare shown in Table 3 and Table 4 for respective datasets from Figure 7 and Figure 8. Based on those\nresults, we demonstrate that average log posterior and linear probe performance correlates strongly\nacross the depth of the network when measured via Pearson\u2019s correlation. This is across various\nsettings of regularizations (both with Independence assumptions and Tikhonov1 Regularization) of\nthe class-wise covariance matrices for our learned average log posterior metric for various Breeds\nDatasets and CLIP Models.\nOur results with a layerwise analysis of Pre-trained CLIP Models comparing our metric with a linear\nprobe on internal activations help us assert that the log posterior is predictive of an embedding\u2019s\n1https://web.eecs.umich.edu/ aey/recent/regular.pdf\n10\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - entity13\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n4\n3\n2\n1\nTrain Log Prob.\n4\n3\n2\n1\nVal Log Prob.\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\nprobe acc.\nALP - reg : uncorr.\nALP - reg : 0.001\nALP - reg : 1e-06\nALP - reg : 1e-09\n4\n3\n2\n1\nprobe acc.\nALP - reg : uncorr.\nALP - reg : 0.001\nALP - reg : 1e-06\nALP - reg : 1e-09\n(a) CLIP Model on entity13.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - entity30\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n4\n2\nTrain Log Prob.\n4\n2\nVal Log Prob.\n4\n2\nViT-L-14\nViT-B-32\nViT-B-16\n4\n2\nViT-L-14\nViT-B-32\nViT-B-16\n4\n2\nprobe acc.\nALP - reg : uncorr.\nALP - reg : 0.001\nALP - reg : 1e-06\nALP - reg : 1e-09\n4\n2\nprobe acc.\nALP - reg : uncorr.\nALP - reg : 0.001\nALP - reg : 1e-06\nALP - reg : 1e-09\n(b) CLIP Model on entity30.\nFigure 7: Evolution of layerwise log posterior and linear probe accuracies for CLIP Models across\nvarying regularization strengths, demonstrating correlations between log posterior and linear probe\nperformance across the depth of various CLIP Models. Quantitative results are shown in Table 3\nand Table 4.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - living17\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n4\n3\n2\n1\n0\nTrain Log Prob.\n4\n3\n2\n1\n0\nVal Log Prob.\n4\n3\n2\n1\n0\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\n0\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\n0\nprobe acc.\nALP - reg : uncorr.\nALP - reg : 0.001\nALP - reg : 1e-06\nALP - reg : 1e-09\n4\n3\n2\n1\n0\nprobe acc.\nALP - reg : uncorr.\nALP - reg : 0.001\nALP - reg : 1e-06\nALP - reg : 1e-09\n(a) CLIP Models on living17.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - nonliving26\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n4\n2\n0\nTrain Log Prob.\n5\n4\n3\n2\n1\nVal Log Prob.\n4\n2\n0\nViT-L-14\nViT-B-32\nViT-B-16\n5\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n4\n2\n0\nprobe acc.\nALP - reg : uncorr.\nALP - reg : 0.001\nALP - reg : 1e-06\nALP - reg : 1e-09\n5\n4\n3\n2\n1\nprobe acc.\nALP - reg : uncorr.\nALP - reg : 0.001\nALP - reg : 1e-06\nALP - reg : 1e-09\n(b) CLIP Models on nonliving26.\nFigure 8: Evolution of layerwise log posterior and linear probe accuracies for CLIP Models across\nvarying regularization strengths, demonstrating correlations between log posterior and linear probe\nperformance across the depth of various CLIP Models. A more detailed breakdown of the results\ncomparing the correlations on each dataset for various settings of regularizations corresponding to\nFigure 7 and Figure 8 is shown in Figure 9 - Figure 16 of Appendix A.1.\ndownstream classification performance, even if a set of data that was used to generate the embedding\nwasn\u2019t part of the models training. With Pearson correlations greater than 0.9 for a majority of\nsettings in Table 3 and Table 4, we can confidently establish its value as a metric that can distinguish\nbetween good and bad-performing embeddings generated by various models. Additionally we also\nshow the corresponding spearman correlations in Table 5 and Table 6.\nTable 5: Spearman\u2019s Correlation on Entity Datasets\nEntity 13 Dataset\nTrain Set\nVal. Set\nReg.\nB16 B32 L14 B16 B32 L14\nDiag\n.98\n.99\n.95\n.99\n.99\n.94\n10\u22123\n.97\n.99\n.98\n.97\n.98\n.96\n10\u22126\n.99\n.97\n.99\n.99\n.99\n.99\n10\u22129\n.99\n.98\n.98\n.99\n.99\n.99\nEntity 30 Dataset\nTrain Set\nVal. Set\nReg.\nB16\nB32\nL14\nB16\nB32\nL14\nDiag\n.99\n.99\n.98\n1.00\n.99\n.98\n10\u22123\n.98\n1.0\n.99\n.97\n.98\n.96\n10\u22126\n.82\n.79\n.96\n.99\n1.00\n.99\n10\u22129\n.72\n.67\n.91\n.99\n1.00\n.99\n11\nTable 6: Spearman\u2019s Correlation on Living and Non Living Datasets\nLiving 17 Dataset\nTrain Set\nVal. Set\nReg.\nB16 B32 L14 B16 B32 L14\nDiag 1.00\n.99\n.99 1.00\n.99\n.98\n10\u22123\n.9\n.97\n.98 1.00\n.99\n.99\n10\u22126\n.47\n.41\n.42\n.99\n1.00 .99\n10\u22129\n.3\n.16\n.26 1.00 1.00 .99\nNon Living 26 Dataset\nTrain Set\nVal. Set\nReg.\nB16\nB32\nL14\nB16\nB32\nL14\nDiag\n.99\n1.00\n.97\n1.00\n1.00\n.97\n10\u22123\n.99\n.99\n.98\n.97\n.97\n.95\n10\u22126\n.62\n.48\n.77\n.99\n.98\n.99\n10\u22129\n.46\n.26\n.61\n.99\n.98\n.99\n6\nCONCLUSION\nThis work introduces a novel method for evaluating pre-trained models. Instead of using costly and\ntime-consuming fine-tuned downstream tasks for evaluation, we propose using the consistency be-\ntween entity embeddings and their associated meta-features as a performance metric. Our method\nhas been effectively tested across various domains and datasets in relational datasets, Natural Lan-\nguage Processing, and Computer Vision, providing a more efficient and equally rigorous alternative\nfor pre-trained model evaluation.\n12\nREFERENCES\nKabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Ak-\nshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram.\nMEGA: Multilingual evaluation of generative AI. In The 2023 Conference on Empirical Methods\nin Natural Language Processing, 2023.\nURL https://openreview.net/forum?id=\njmopGajkFY.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Love-\nnia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. A multitask,\nmultilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. In\nJong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, and Adila Alfa\nKrisnadhi (eds.), Proceedings of the 13th International Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 675\u2013718, Nusa Dua, Bali, November 2023.\nAssociation for Computational Linguistics. URL https://aclanthology.org/2023.\nijcnlp-main.45.\nLeo Breiman. Random forests. Machine learning, 45:5\u201332, 2001.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nAnthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Evaluating question answering\nevaluation.\nIn Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi\nChen (eds.), Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pp.\n119\u2013124, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:\n10.18653/v1/D19-5817. URL https://aclanthology.org/D19-5817.\nXin Dai, Yujie Fan, Zhongfang Zhuang, Shubham Jain, Chin-Chia Michael Yeh, Junpeng Wang,\nLiang Wang, Yan Zheng, and Wei Zhang.\nPdt: Pretrained dual transformers for time-aware\nbipartite graphs. arXiv preprint arXiv:2306.01913, 2023.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npp. 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.\nLogan Engstrom, Andrew Ilyas, Shibani Santurkar, and Dimitris Tsipras. Robustness (python li-\nbrary), 2019a. URL https://github.com/MadryLab/robustness.\nLogan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Aleksander\nMadry. Adversarial robustness as a prior for learned representations, 2019b.\nWes Gurnee and Max Tegmark.\nLanguage models represent space and time.\narXiv preprint\narXiv:2310.02207, 2023.\nF Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm\ntransactions on interactive intelligent systems (tiis), 5(4):1\u201319, 2015.\nWang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018 IEEE\ninternational conference on data mining (ICDM), pp. 197\u2013206. IEEE, 2018.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1,\npp. 2, 2019.\nAditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ra-\nmanujan, William Howard-Snyder, Kaifeng Chen, Sham M. Kakade, Prateek Jain, and Ali\nFarhadi. Matryoshka representation learning. In Neural Information Processing Systems, 2022.\nURL https://api.semanticscholar.org/CorpusID:252683450.\n13\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,\nYian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan,\nBobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re,\nDiana Acosta-Navas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda\nRong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khat-\ntab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar,\nSurya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William\nWang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.\nHolistic evaluation of lan-\nguage models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL\nhttps://openreview.net/forum?id=iO4LZibEqW.\nFeatured Certification, Expert\nCertification.\nTomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Efficient estimation of word\nrepresentations in vector space. In International Conference on Learning Representations, 2013.\nURL https://api.semanticscholar.org/CorpusID:5959482.\nGeorge A. Miller. WordNet: A lexical database for English. In Human Language Technology:\nProceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994.\nURL\nhttps://aclanthology.org/H94-1111.\nOpenAI. Gpt-4 technical report, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nShibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander\nMadry. Image synthesis with a single (robust) classifier, 2019.\nShibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation\nshift, 2020.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng\nTu.\nDocument-level machine translation with large language models.\nIn Houda Bouamor,\nJuan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 16646\u201316661, Singapore, December 2023. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.1036. URL https:\n//aclanthology.org/2023.emnlp-main.1036.\nBigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c,\nDaniel Hesslow, Roman Castagn\u00b4e, Alexandra Sasha Luccioni, Franc\u00b8ois Yvon, et al. Bloom:\nA 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,\n2022.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.\nXlnet: Generalized autoregressive pretraining for language understanding. Advances in neural\ninformation processing systems, 32, 2019.\nYan Zheng, Junpeng Wang, Chin-Chia Michael Yeh, Yujie Fan, Huiyuan Chen, Liang Wang, and\nWei Zhang. Embeddingtree: Hierarchical exploration of entity features in embedding. In 2023\nIEEE 16th Pacific Visualization Symposium (PacificVis), pp. 217\u2013221. IEEE, 2023.\n14\nA\nAPPENDIX\nA.1\nFIXED REGULARIZATION CROSS MODEL ANALYSIS OF CLIP EMBEDDINGS ON THE\nBREEDS HIERARCHY\nIn this section, we break the constituents of Figure 7a, Figure 8, Table 3 and Table 4 into individual\nplots comparing the behavior of ALP and linear probe accuracy for each CLIP model on a given\ndataset for varying regularization schemes. A complete per regularization breakdown for the 3\nCLIP Models corresponding to entity-13 from Figure 7a is provided in Figure 9 and Figure 10, for\nentity-30 in Figure 7b, the same is shown in Figure 11 and Figure 12. For living-17 and non-living-\n26 from Figure 8a and Figure 8b, the analysis is shown in Figure 13, Figure 14 and Figure 15,\nFigure 16 respectively.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - entity13\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n4\n3\n2\n1\nTrain Log Prob.\n4\n3\n2\n1\nVal Log Prob.\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\nprobe acc.\nALP - reg : uncorr.\n4\n3\n2\n1\nprobe acc.\nALP - reg : uncorr.\n(a) Regularization - Uncorr.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - entity13\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n3\n2\n1\nTrain Log Prob.\n3\n2\n1\nVal Log Prob.\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nprobe acc.\nALP - reg : 0.001\n3\n2\n1\nprobe acc.\nALP - reg : 0.001\n(b) Regularization 10\u22123.\nFigure 9: Regularization-wise analysis of layerwise ALP vs. linear probe performance for entity 13\nbreeds dataset.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - entity13\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n3\n2\n1\nTrain Log Prob.\n3\n2\n1\nVal Log Prob.\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nprobe acc.\nALP - reg : 1e-06\n3\n2\n1\nprobe acc.\nALP - reg : 1e-06\n(a) Regularization 10\u22126.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - entity13\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n3\n2\n1\nTrain Log Prob.\n3\n2\n1\nVal Log Prob.\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nprobe acc.\nALP - reg : 1e-09\n3\n2\n1\nprobe acc.\nALP - reg : 1e-09\n(b) Regularization 10\u22129.\nFigure 10: Regularization-wise analysis of layerwise ALP vs. linear probe performance for entity\n13 breeds dataset.\n15\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - entity30\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n5\n4\n3\n2\n1\nTrain Log Prob.\n5\n4\n3\n2\n1\nVal Log Prob.\n5\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n5\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n5\n4\n3\n2\n1\nprobe acc.\nALP - reg : uncorr.\n5\n4\n3\n2\n1\nprobe acc.\nALP - reg : uncorr.\n(a) Regularization - Uncorr.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - entity30\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n3\n2\n1\nTrain Log Prob.\n3\n2\n1\nVal Log Prob.\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nprobe acc.\nALP - reg : 0.001\n3\n2\n1\nprobe acc.\nALP - reg : 0.001\n(b) Regularization 10\u22123.\nFigure 11: Regularization wise analysis of layerwise ALP vs. linear probe performance for entity\n30 breeds dataset.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - entity30\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n3\n2\n1\nTrain Log Prob.\n4\n3\n2\n1\nVal Log Prob.\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nprobe acc.\nALP - reg : 1e-06\n4\n3\n2\n1\nprobe acc.\nALP - reg : 1e-06\n(a) Regularization 10\u22126.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - entity30\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n3\n2\n1\nTrain Log Prob.\n4\n3\n2\n1\nVal Log Prob.\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nprobe acc.\nALP - reg : 1e-09\n4\n3\n2\n1\nprobe acc.\nALP - reg : 1e-09\n(b) Regularization 10\u22129.\nFigure 12: Regularization wise analysis of layerwise ALP vs. linear probe performance for entity\n30 breeds dataset.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - living17\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n4\n3\n2\n1\nTrain Log Prob.\n4\n3\n2\n1\nVal Log Prob.\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\nprobe acc.\nALP - reg : uncorr.\n4\n3\n2\n1\nprobe acc.\nALP - reg : uncorr.\n(a) Regularization - Uncorr.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - living17\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n2\n1\nTrain Log Prob.\n2\n1\nVal Log Prob.\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n2\n1\nprobe acc.\nALP - reg : 0.001\n2\n1\nprobe acc.\nALP - reg : 0.001\n(b) Regularization 10\u22123.\nFigure 13: Regularization wise analysis of layerwise ALP vs. linear probe performance for living\n17 breeds dataset.\n16\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - living17\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n2.0\n1.5\n1.0\n0.5\n0.0\nTrain Log Prob.\n3\n2\n1\nVal Log Prob.\n2.0\n1.5\n1.0\n0.5\n0.0\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n2.0\n1.5\n1.0\n0.5\n0.0\nprobe acc.\nALP - reg : 1e-06\n3\n2\n1\nprobe acc.\nALP - reg : 1e-06\n(a) Regularization 10\u22126.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - living17\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n1.0\n0.5\n0.0\nTrain Log Prob.\n4\n3\n2\n1\n0\nVal Log Prob.\n1.0\n0.5\n0.0\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\n0\nViT-L-14\nViT-B-32\nViT-B-16\n1.0\n0.5\n0.0\nprobe acc.\nALP - reg : 1e-09\n4\n3\n2\n1\n0\nprobe acc.\nALP - reg : 1e-09\n(b) Regularization 10\u22129.\nFigure 14: Regularization wise analysis of layerwise ALP vs. linear probe performance for living\n17 breeds dataset.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - nonliving26\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n5\n4\n3\n2\n1\nTrain Log Prob.\n5\n4\n3\n2\n1\nVal Log Prob.\n5\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n5\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n5\n4\n3\n2\n1\nprobe acc.\nALP - reg : uncorr.\n5\n4\n3\n2\n1\nprobe acc.\nALP - reg : uncorr.\n(a) Regularization - Uncorr.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - nonliving26\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n3\n2\n1\nTrain Log Prob.\n3\n2\n1\nVal Log Prob.\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n3\n2\n1\nprobe acc.\nALP - reg : 0.001\n3\n2\n1\nprobe acc.\nALP - reg : 0.001\n(b) Regularization 10\u22123.\nFigure 15: Regularization-wise analysis of layerwise ALP vs. linear probe performance for nonliv-\ning 26 breeds dataset.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - nonliving26\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n2\n1\n0\nTrain Log Prob.\n4\n3\n2\n1\nVal Log Prob.\n2\n1\n0\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n2\n1\n0\nprobe acc.\nALP - reg : 1e-06\n4\n3\n2\n1\nprobe acc.\nALP - reg : 1e-06\n(a) Regularization 10\u22126.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrain Acc\nPosterior and Probe Analysis - nonliving26\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Layer Depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVal Acc\n1.5\n1.0\n0.5\nTrain Log Prob.\n4\n3\n2\n1\nVal Log Prob.\n1.5\n1.0\n0.5\nViT-L-14\nViT-B-32\nViT-B-16\n4\n3\n2\n1\nViT-L-14\nViT-B-32\nViT-B-16\n1.5\n1.0\n0.5\nprobe acc.\nALP - reg : 1e-09\n4\n3\n2\n1\nprobe acc.\nALP - reg : 1e-09\n(b) Regularization 10\u22129.\nFigure 16: Regularization-wise analysis of layerwise ALP vs. linear probe performance for nonliv-\ning 26 breeds dataset.\n17\n"
  },
  {
    "title": "TeleChat Technical Report",
    "link": "https://arxiv.org/pdf/2401.03804.pdf",
    "upvote": "7",
    "text": "TELECHAT TECHNICAL REPORT\nZihan Wang\u2217, Xinzhang Liu\u2217, Shixuan Liu\u2217, Yitong Yao\u2217, Yuyao Huang\u2217,\nZhongjiang He, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang,\nXin Wang, Luwen Pu, Huihan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang,\nXiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang,\nBingkai Yang, Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li,\nLingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang,\nChao Wang\u2020, Shuangyong Song\u2020\n{wangzh54,liuxz,liusx14,yaoyt2,huangyy121,hezj,xuelong li,liyx25,chezh,\nzhangzx32,wangy143,wangx57,pulw,xuhn,fangry,zhaoy11,zhangj157,huangxm26,\nluzl,pengjx3,zhengwj9,wangsq23,yangbk,hexw1,jiangzr2,xieqy7,zhangyh78,\nlizq48,shill2,fuweiwei,zhangy196,huangzl21,xiongsishi,zhangyx,\nwangc17,songshy}@chinatelecom.cn\nABSTRACT\nIn this technical report, we present TeleChat, a collection of large language models\n(LLMs) with parameters of 3 billion, 7 billion and 12 billion. It includes pretrained\nlanguage models as well as fine-tuned chat models that is aligned with human\npreferences. TeleChat is initially pretrained on an extensive corpus containing a\ndiverse collection of texts from both English and Chinese languages, including\ntrillions of tokens. Subsequently, the model undergoes fine-tuning to align with hu-\nman preferences, following a detailed methodology that we describe. We evaluate\nthe performance of TeleChat on various tasks, including language understanding,\nmathematics, reasoning, code generation, and knowledge-based question answer-\ning. Our findings indicate that TeleChat achieves comparable performance to other\nopen-source models of similar size across a wide range of public benchmarks. To\nsupport future research and applications utilizing LLMs, we release the fine-tuned\nmodel checkpoints of TeleChat\u2019s 7B and 12B variant, along with code and a portion\nof our pretraining data, to the public community.\n1\nINTRODUCTION\nThe research community has witnessed substantial proliferation of open large language models\n(LLMs) that have emerged as valuable resources for study and as foundational models for developing\nchatbots and other applications. Following the introduction of ChatGPT (OpenAI, 2022), there have\nbeen thrilling advancements and applications of LLMs, but the majority of prominent LLMs, such as\nGPT-4 OpenAI (2023) and PaLM-2 (Anil et al., 2023), are restrictive in their technological sharing.\nFew details about their models and training strategies are disclosed. This poses a challenge for\ndevelopers and researchers who cannot access the complete model parameters, hindering a thorough\nexamination or customization of these systems by the community. In contrast, a steady stream of\nopenly accessible text-based LLMs has emerged, including OPT (Zhang et al., 2022), BLOOM (Scao\net al., 2022), LLaMA (Touvron et al., 2023a), LLAMA 2 (Touvron et al., 2023b), MosaicML\u2019s MPT\n(ML, 2023), Falcon (Penedo et al., 2023), etc. These models have provided researchers with valuable\nresources for further exploration and development, paving the way for extensive research in various\ndomains, such as efficient fine-tuning techniques, longer prompt context utilization and retrieval\naugmented generation approaches. Furthermore, there exist various other LLMs that have been\ndesigned with a focus on Chinese language generation, including models such as Baichuan-2 (Yang\net al., 2023), Qwen (Bai et al., 2023), InternLM (InternLM Team, 2023) and SkyWork (Wei et al.,\n2023b). While these models offer comprehensive details about their pretraining strategies, they often\nlack transparency in their instruction finetuning processes for chat models. This lack of transparency\n\u2217These authors contributed equally to this work\n\u2020Corresponding Authors.\n1\narXiv:2401.03804v1  [cs.CL]  8 Jan 2024\nincludes limited disclosure of the finetuning data composition, methods for concatenating multi-turn\ndialog data, and techniques employed to enhance conversational performance.\nTo encourage reproducibility of fine-tuned LLMs and foster responsible development of LLMs,\nwe release TeleChat, a collection of chat models that have been fine-tuned using human alignment\ntechniques including supervised fine-tuning and reinforcement learning. In particular, we provide a\ncomprehensive explanation of our model architecture and the approach we used to extend TeleChat\u2019s\ncontext window to 96k in Section 2. Furthermore, in Section 3, we delve into the specifics of our\npretraining dataset and cleaning techniques. We then discuss alignment with human preferences\nin Section 4 and 5. Additionally, in Section 7, we conduct a thorough analysis of the model\u2019s\nperformance on standard benchmark tasks. We also show our insights in Section 8 regarding\nmitigating hallucination with knowledge graph. Furthermore, we describe our parallel computing\nmethod in Section 6. Our contribution are listed as follows:\n\u2022 We release TeleChat, a suite of pretrained and fine-tuned large language models (LLMs)\nwith parameter sizes of 3 billion, 7 billion, and 12 billion. The foundation model of TeleChat\nundergoes pretraining on large corpus containing a diverse collection of English and Chinese\ntexts, totaling trillions of tokens. Subsequently, TeleChat is fine-tuned to achieve state-of-\nthe-art performance for conversational AI applications. The finetuned model of TeleChat\u2019s\n7B and 12B variant is made public to the community.\n\u2022 We present our comprehensive data cleaning workflow, which includes rule-based filtering\nand cleaning, deduplication at various levels (whole dataset, document, and paragraph),\nhigh-quality data selection, and data security processing. With our meticulous data cleaning\napproach, we ensure that TeleChat is pretrained on refined and reliable datasets. We also\nmake available a portion of our high quality training corpus which includes 1TB of textual\ndata.\n\u2022 We disclose a comprehensive description of our supervised fine-tuning methodology, an\naspect that is frequently overlooked in reports of other publicly available models. Our\nmethodology includes dataset blending, noisy embedding fine-tuning, and multi-stage long\ncontext training.\n\u2022 We provide our approach utilizing TeleChat for real-world applications, highlighting our\nmethodology for mitigating hallucination through the use of knowledge graphs. Our objec-\ntive is to assist the community in developing highly effective language models that can be\napplied across various practical scenarios.\n2\nMODEL DESIGN\nThis section aims to provide an overview of our design methodology and shed light on the archi-\ntecture of the TeleChat model. We begin by discussing the key components that form the model\narchitecture. Subsequently, we elaborate on our approach to expanding the context window to 96k\nusing interpolation and fine-tuning strategies.\n2.1\nMODEL ARCHITECTURE\nTeleChat is an autoregressive transformer model that employs a stack of transformer-decoder layers,\nwhose architecture largely follows that of GPT-3 (Brown et al., 2020a). However, TeleChat deviates\nfrom the original transformer model in several notable ways, drawing inspiration from influential\nlanguage models such as LLaMA (Touvron et al., 2023b) and BLOOM (Scao et al., 2022). The key\nparameters of the architecture are summarized in Table 1.\nRotary Position Embedding. We initially consider utilizing Attention with Linear Biases (ALiBi) to\nencode relative positional information, as proposed by Press et al. (2022), due to its efficiency in\nimplementation and extrapolation. However, the incompatibility of ALiBi with Flash Attention v2,\nwhich requires attention bias as an argument, led us to adopt Rotary Positional Embedding\n(RoPE, Su et al. (2022)) instead. Our decision was motivated by the successful implementation\nof RoPE in influential language models, such as LLaMA (Touvron et al., 2023a) (Touvron et al.,\n2023b) and PaLM (Anil et al., 2023), and its ability to extend context window lengths in recent\nstudies (Chen et al., 2023), (Rozi`ere et al., 2023), (Peng et al., 2023), (Wei et al., 2023b). By\n2\nleveraging positional information through RoPE, we can efficiently encode absolute positions with\nexplicit integration of relative position dependencies within the self-attention formulation. To further\noptimize computational efficiency and minimize memory usage, we implement Flash Attention v2 in\nthe attention modules (Dao et al., 2022), (Dao, 2023)). Additionally, we choose to utilize float32\nprecision for the inverse frequency matrix to prioritize model performance and achieve higher levels\nof accuracy.\nNormalizations. To ensure robust training, we incorporate an additional layer normalization step\nafter the initial embedding layer for TeleChat\u2019s 3B variant, drawing inspiration from the methodology\nemployed in BLOOM (Scao et al., 2022). However, we diverge from BLOOM by replacing\nconventional layer normalization with RMSNorm (Zhang & Sennrich, 2019), which has been\nshown to enhance the stability and performance of transformer models. Additionally, we adopted\npre-normalization in each layer instead of post-normalization, a design choice that has been found to\nimprove the training stability of transformer models.\nActivations We utilized the SwiGLU activation function (Shazeer, 2020), a non-linear activation\nfunction that combines the strengths of Swish (Ramachandran et al., 2017) and Gated Linear Unit\n(Dauphin et al., 2017). SwiGLU has been shown to outperform other baseline activation functions,\nsuch as GeLU (Hendrycks & Gimpel, 2016). We diminished the dimension of the feed-forward\nnetwork to less than four times the hidden size, adhering to established conventions in prior research\n(Touvron et al., 2023b) (Wei et al., 2023b). In contrast to previous studies, our approach deviates\nfrom the convention of utilizing eight-thirds of the hidden size as the feed-forward network (FFN)\ndimension. Instead, we deliberately assign a specific dimension size to achieve the desired parameter\nsize.\nModels\nlayer num\nattention heads\nhidden size\nFFN hidden size\nvocab size\nTeleChat-3B\n14\n16\n4096\n13312\n82944\nTeleChat-7B\n30\n32\n4096\n12288\n160256\nTeleChat-12B\n38\n32\n5120\n12288\n160256\nTable 1: Detailed model architecture parameters for TeleChat\u2019s 3B, 7B, and 12B models.\n2.2\nEXTENDING CONTEXT WINDOW\nThe input contexts of large language models (LLMs) can contain a substantial number of tokens in\nvarious scenarios, particularly when processing extensive inputs such as legal or scientific documents,\ndatabase entries, and conversation histories, etc. (Schick et al., 2023). As a result, it is crucial for\nLLMs to possess long-range capabilities and efficiently extrapolate to context lengths far beyond\ntheir initial pre-training limitations.\nIn order to tackle the problem of losing high-frequency information during Position Interpolation\n(PI, Chen et al. (2023)) on the RoPE embeddings, NTK-aware interpolation is proposed in (bloc97,\n2023). Instead of uniformly scaling each dimension of RoPE by a scaling factor, this approach\nredistributes the interpolation pressure across multiple dimensions by scaling high frequencies\nless and low frequencies more, which preserves high-frequency information (Peng et al., 2023).\nFurthermore, to address performance degradation resulting from fluctuations in context length during\nmultiple forward-passes, we employ a Dynamic NTK-aware interpolation mechanism, in which the\ninterpolation scaling factor is designed as a continuous variable, and is updated according to real-time\ncontext length.\nTo further enhance the long-context capabilities of TeleChat, we implement Multi-stage Long-context\nTraining during the supervised finetuning phase and LogN-Scaling (Su, 2023) in the inference\nstage. Multi-stage Long-context Training periodically extends the context length during training,\nwhile LogN-Scaling adjusts the attention mechanism by rescaling the dot product in proportion to\nthe context-to-training length ratio, ensuring the stability of attention entropy as the context length\nincreases. The detailed description of Multi-stage Long-context Training can be found in section\n4.2.3. Experimental results demonstrate that by employing these techniques, TeleChat successfully\nextends its context window to over 96k tokens.\n3\n3\nPRETRAINING STAGE\nDuring pretraining, we train the model from scratch using a substantial amount of data, which enables\nthe model to not only gain a holistic comprehension of the world, but also develop specific skills such\nas mathematics, reasoning, and code generation. In this section, we introduce our data collection and\ncleaning method (Section 3.1 and 3.2), training details (Section 3.3), and tokenizer (Section 3.4).\n3.1\nDATA COLLECTION\nDuring the data collection stage, our paramount objective is to acquire a substantial and diverse\ndataset. This objective is accomplished through the collection of a vast amount of data from diverse\nsources, employing appropriate collection methods that guarantee a comprehensive representation of\ndifferent perspectives.\nData Sourcing. TeleChat\u2019s pretraining corpus is curated from a wide range of data sources, con-\nstituting a comprehensive repository of knowledge. Our corpus contains both general-purpose and\ndomain-specific data, ensuring a well-rounded and robust foundation. The general-purpose data\ncomprises a vast range of sources, such as web pages, social platforms, encyclopedias, books, aca-\ndemic papers, code repositories, and more. In terms of domain-specific data, we gather corpus from\ntwenty distinct sectors, including finance, construction, health and social work, aligning with national\nindustry classifications1. The specific textual formats we collect include financial report, bidding\ndocument, government notice, and various other document types.\nDatasets\nPercentage%\nweb page\n22\nbooks\n11\ncommunity QA\n7\nsocial sharing\n8\ndocuments and reports\n13\npaper\n2\ncode repository\n12\nchat data\n13\nothers\n12\nChinese\n45\nEnglish\n35\nCode\n11\nMath\n9\nTable 2: The distribution of various categories of TeleChat\u2019s pretraining data.\nCollection Method. By leveraging the vast data repository accumulated by China Telecom over the\npast decades, our data collection process is simplified through the acquisition of a substantial volume\nof data from existing accumulations. In addition to textual data, our data collection procedure also\ncontains the gathering of supplementary information, including timestamps, indicators of popularity\n(e.g. stars of GitHub repositories and numbers of likes/forwards of articles), and URLs. These\nsupplementary details play a crucial role in the data filtering process discussed in Section 3.2.\nFurthermore, we diligently work towards enriching the data and mitigating biases. For example, we\nenrich our book data based on the Chinese Library Classification System2, and for social platform\ndata, we employ a breadth-first search approach on social networks to encompass as many social\ngroups as possible. Furthermore, we consistently gather and accumulate real-time data to ensure\ncomprehensive coverage of the most up-to-date information.\nDuring the data collection stage, we acquire diverse and extensive pre-training data on a petabyte\nscale, covering a wide range of domains. The distribution of our pretraining data is displayed in Table\n2.\n1https://www.stats.gov.cn/english/NewsEvents/200306/t20030619_25521.\nhtml\n2https://en.wikipedia.org/wiki/Chinese_Library_Classification\n4\n3.2\nDATA PREPROCESSING\nDuring the actual collection process, various special situations may arise that necessitate specific\nsubsequent handling. For example, the gathered data might include advertising, violent content, and\nprivate information that necessitates filtering. Additionally, there may be numerous duplicates and\nother redundant information. As a result, we devise a comprehensive data cleaning procedure to\nensure the quality of our pretraining data. Our data clean procedure consists of rule-based filtering,\ndeduplication, high-quality data selection, and data security filtering.\nRule-based Filtering. Considering that TeleChat is primarily focused on Chinese and English, we\nremove data in other languages and non-text multimodal data. Simultaneously, heuristic rules are\napplied to clean the text efficiently and effectively. For instance, we filter out extremely short or\nlow-information texts, discard texts with excessive or minimal punctuation, replace HTML tags with\nnatural language, and automatically identify and standardize the text encoding format to UTF-8.\nDeduplication. Performing global deduplication on a large amount of data is unacceptably slow, there-\nfore we perform a hierarchical deduplication method that consist of URL deduplication, Document-\nlevel Deduplication, and Paragraph-level Deduplication. First, we eliminate duplicate data from\nsimilar sources within groups using URL deduplication, which removes approximately half of the\nduplicate data. Next, we utilize a 128-bit SimHash algorithm to identify similarities in long texts,\nenabling Document-level Deduplication that removes duplicate articles, such as reposts on the in-\nternet. Finally, we employ Minhash and Jaccard similarity methods to perform Paragraph-level\nDeduplication, effectively filtering out a large number of homogeneous advertisements and heavily\nredundant texts. Notably, we use two different locally sensitive hash functions in the Document-level\nDeduplication and Paragraph-level Deduplication respectively, thereby achieving better deduplication\nresults.\nHigh-quality Selection We utilize a 5-gram Kneser-Ney model, as implemented in the KenLM\nlibrary (Heafield, 2011), to train on existing high-quality corpora and subsequently compute the\nperplexity of each paragraph. The lower the perplexity, the greater the similarity between the data and\nthe high-quality corpora. It is important to note that the corpora used for training the model should be\nas unbiased as possible, however, it may still result in the erroneous discarding of relatively niche or\nhighly specialized data. To address this, instead of simply discard texts with high perplexity, we split\nthe data into three even parts: head, middle, and tail based on the perplexity score, similar to Wenzek\net al. (2019). The data in the head part will be sampled more frequently, while the data in the tail part\nwill be sampled less during pretraining.\nSecurity Filtering. To guarantee the security of our dataset, we utilize a multi-model classification\napproach that focuses on detecting and eliminating inappropriate, violent, and politically sensitive\ncontent. Our methodology prioritizes high recall and low precision, which enables us to identify a\nlarge number of negative instances that can be used for reinforcement learning. Additionally, we\nemploy obfuscation techniques to safeguard personal privacy data, ensuring that sensitive information\nremains protected throughout the process.\n3.3\nTRAINING DETAILS\nBatch Generation. To generate data batches, we employ a process of shuffling and concatenating the\ncorpus obtained from the same source, ensuring consistency in the data. This approach deliberately\navoids randomly concatenating data from different sources, thereby improving the model\u2019s ability\nto capture longer contexts. Furthermore, to align with the specified context lengths (e.g., 4096), the\ndata is strategically truncated and concatenated with other data samples. By taking these steps, we\ncan create batches of data that are not only diverse but also coherent, which is crucial for effective\nlanguage modeling.\nTraining Objectives. The method utilized in the pretraining stage is known as autoregressive\nlanguage modeling, which involves iteratively predicting the probability of the subsequent token in\nthe sequence. We represent the joint probability of tokens in a text as:\np(x) = p(x1, \u00b7 \u00b7 \u00b7, xT ) =\nT\nX\nt=1\np(xt|x<t)\n(1)\n5\nWhere x is a sequence of tokens, and we calculate the probability of each token xt based on the\ntokens that come before it, denoted as x<t. The model is trained to optimize this probability across\nthe entire training corpus.\nOptimizer. We utilize the widely used Adam (Kingma & Ba, 2017) optimizer for pretraining\noptimization. We employ a cosine learning rate schedule, where the peak learning rate is specified\nfor each model size. The learning rate gradually decays until it reaches a minimum learning rate of\n10% of the peak value. The hyperparameters are set as follows: \u03b21 = 0.9, \u03b22 = 0.95, and \u03f5 = 10\u22125.\nA weight decay of 10\u22124 is applied to all model parameters except for bias.\nRamp-up Batch. In order to enable the model to converge faster at the very beginning of pretraining,\nwe employ a technique called ramp-up batch size, which involves starting with a small batch size and\ngradually increasing it linearly to the maximum batch size over a certain number of steps.\nPrecision. The utilization of the float16 data type has been recognized as a possible factor contributing\nto numerical instabilities, leading to irreversible training divergences observed in large language\nmodels. This issue arises from the limited dynamic range offered by float16 (Zhang et al., 2020). To\nensure training stability, we pretrain all models using bfloat16 (Wang & Kanwar, 2019) (Kalamkar\net al., 2019) (Scao et al., 2022), a data type that shares the same dynamic range as float32. In\norder to maintain a balance between performance and training stability, we employ bfloat16 mixed-\nprecision training, as described by (Micikevicius et al., 2018). This approach involves performing\nprecision-sensitive operations such as gradient accumulation, softmax, and weight updating with\nfloat32 precision, while carring out the remaining operations with bfloat16 precision.\nThe specific hyperparameters are presented in Table 3.\nHyperParams\nTeleChat-3B\nTeleChat-7B\nTeleChat-12B\nPeak lr\n4e-4\n3e-4\n1.5e-4\nramp-up batch size\n240/80/1,000,000\n288/72/1,500,000\n240/80/2,000,000\nbatch size\n8M\n16M\n16M\nwarm up fraction\n0.02\n0.01\n0.01\nclip-grad\n1.0\n1.0\n1.0\nattention dropout\n0.1\n0.1\n0.1\nhidden dropout\n0.1\n0.1\n0.1\nrmsnorm epsilon\n1e\u22125\n1e\u22125\n1e\u22125\n# training tokens\n0.8T\n1.0T\n1.2T\nTable 3: The hyperparameter details utilized during the pretraining stage of TeleChat\u2019s 3B, 7B, and\n12B variants. The ramp-up batch size is expressed in the format of <start batch size >/<batch size\nincrement>/<ramp-up samples>. For example, 240/80/1,000,000 indicates that the training begins\nwith a batch size of 240 and increments by 80 for each time. The total ramp-up phase encompasses\n1,000,000 samples.\n3.4\nTOKENIZER\nWe utilize Hugging Face\u2019s tokenizers to implement the BBPE algorithm, training the tokenizer on a\ndiverse dataset comprising Chinese, English, code, and mathematical data. This process results in a\ntokenizer with a vocabulary size of 160,130, which is subsequently padded to 160,256. Additionally,\nwe use special tokens to differentiate dialogue roles and turns, and also incorporate specific designs\nto mitigate potential injection attacks.\n4\nSUPERVISED FINE-TUNING STAGE\nLarge language models (LLMs) have demonstrated remarkable capabilities in various domains, such\nas reasoning (Wei et al., 2023a) (Yao et al., 2023), coding (Chen et al., 2021a), (Li et al., 2022b)\nand aligning general human intentions (Ouyang et al., 2022). Therefore, we employ supervised\nfine-tuning (SFT) stage after the pretraining stage to improve the model\u2019s ability to comprehend\nhuman behavior and effectively accomplish various real-world tasks. During the SFT stage, our\n6\nmodel is exposed to various tasks using human-annotated prompts and feedback in a chat-style format.\nIn this section, we provide detailed information about our data annotation method in Section 4.1,\nfollowed by an in-depth discussion of our methodology and experimental details in Section 4.2 and\nSection 4.3. Examples generated by TeleChat is shown in Appendix B.\n4.1\nHUMAN DATA COLLECTION\nWe have brought together a team of internal annotators and external contractors to carry out the\nmanual data annotation process. Our annotators are all native Chinese speakers, boasting a range\nof academic backgrounds including Computer Science, Law, Chinese language and literature, and\nother related fields. This diversity enables them to excel in annotating expertise data with greater\nproficiency. Notably, a significant number of them hold bachelor or master degree from China\u2019s most\nesteemed 211 Project Universities 3.\nTo ensure the high quality of our annotators and subsequently our data, we implement a rigorous\ntraining and selection process. This process starts with a comprehensive training session, providing\nour annotators with detailed instructional materials. They are then tasked with completing trial\nannotations, which are subsequently evaluated through a quality sorting mechanism. This allows us\nto assess the performance of each annotator and retain only those who have demonstrated exceptional\nproficiency in the assigned task. The data generated during these trials is not included in the final\ndataset to ensure data quality.\nWe employ human annotators to label varied prompts and organize them into conversations, harnessing\nour annotation platform for efficient and high-quality annotations. During the labeling process,\nannotators are instructed to prioritize helpfulness, but for sensitive topics such as politics, violence,\nand pornography, they are instructed to prioritize safety and avoid harmful content. We work closely\nwith labelers, providing them with clear instructions for each task and addressing their questions\npromptly. We continuously refine our instructions to ensure clarity and consistency, incorporating\nfeedback from labelers to enhance the annotation process.\nTo further improve data quality, we implement a two-stage review process that includes checks by\nboth reviewers and algorithm engineers. This process adheres to fundamental data requirements,\nsuch as fluency, helpfulness, truthfulness, and harmlessness, as well as domain-specific criteria, to\nguarantee data quality. Data samples that do not meet all predefined criteria according to a consensus\namong reviewers and algorithm engineers are excluded from the final dataset.\nWe collect over 100,000 supervised fine-tuning samples using the aforementioned annotation strate-\ngies and train our model accordingly. The statistics of the top 30 categories in our supervised-\nfinetuning data is displayed in Appendix A.\n4.2\nTRAINING METHODOLOGY\nIn this section, we present a comprehensive explanation of our training approach during the supervised\nfine-tuning stage, an aspect that is frequently overlooked in reports of other open-sourced models.\nOur methodology contains the construction of SFT data samples (Section 4.2.1), the usage of noisy\nembeddings for enhanced model performance in scenarios with limited training data (Section 4.2.2),\nand the implementation of multi-stage long-context training to expand TeleChat\u2019s context window to\n96k tokens (Section 4.2.3). By incorporating these techniques, we develop a chat model that offers\nvaluable assistance and support to users.\n4.2.1\nDATA ORGANIZATION\nOur dataset spans various domains, such as General Q&A, creative writing, reading comprehension,\nmachine translation, code generation, math & reasoning, and more. To ensure that each domain is\nrepresented appropriately, we assign respective resampling weights to each dataset based on their\nimportance. Then, we sample single-round and multi-round conversations from each dataset using\ntheir corresponding resampling weights, thereby creating a balanced and diverse dataset that reflects\nthe various domains. The sampled conversations are then shuffled and concatenated, followed by\npre-padding them to a predetermined length (e.g., 4096 or 8192) to ensure consistent input length. We\n3List of 211 Project Universities: https://en.wikipedia.org/wiki/Project_211\n7\nuse special tokens < user>, < bot>, and < end> to denote the beginning of a question, the start\nof an answer, and the end of an answer respectively, thereby facilitating the model\u2019s comprehension\nof the conversational dynamics. To ensure diversity in the combination of data, the datasets are\nresampled and re-shuffled for each training epoch. We finetune the model in a supervised manner\nbased on the instruction dataset.\n4.2.2\nNOISY EMBEDDING FINE TUNING\nIn this section, we introduce our method for enhancing the answer quality of large language models\n(LLMs) through noisy embedding fine-tuning (NEFTUNE), inspired by the work of Jain et al. (2023).\nOur approach involves introducing noise into the input embeddings of the LLM, which encourages\nthe model to overfit less to the specifics of the instruction-tuning dataset. Instead, the model is\nmore capable of providing answers that incorporate knowledge and behaviors of the pretrained base\nmodel. By doing so, we demonstrate that the conversational quality of the generated answers can be\nimproved, and the model\u2019s ability to generalize to unseen tasks and data can be enhanced, especially\nwhen the training sample is limited.\nSpecifically, NEFTune modifies the input embeddings by adding a random noise vector to them.\nThe noise is generated by sampling independent and identically distributed (i.i.d) uniform entries,\neach in the range [\u22121, 1], and then scaling the entire noise vector by a factor of \u03b1/\n\u221a\nLd, where L is\nthe sequence length, d is the embedding dimension, and \u03b1 is a tunable hyperparameter. This noise\ninjection process simulates the variability and uncertainty present in real-world tasks, which helps the\nmodel to learn more robust and generalizable representations.\nHowever, we observe that while NEFTune can enhance the model\u2019s performance in scenarios with\nlimited training data, its benefits diminish as the size of the training dataset increases. In fact, when\nan ample amount of training data is available, the impact of NEFTune becomes negligible. This is\nlikely due to the model\u2019s reduced tendency to overfit on larger datasets. To investigate this further, we\nconduct experiments using TeleChat-7B fine-tuned models with and without the implementation of\nNEFTune. Our findings reveal that when NEFTune is applied, it achieves a 55% win rate against its\ncounterpart without NEFTune, as determined by human evaluators. However, when the model is\ntrained on the entire dataset consisting of 4,000,000 samples, NEFTune loses its advantage, resulting\nin only a 48% win rate against its counterpart without NEFTune. The effect of utilizing NEFTUNE\nis shown in Appendix C.\n4.2.3\nMULTI-STAGE LONG-CONTEXT TRAINING.\nWe utilize an innovative training approach involving a multi-stage method to enhance our model\u2019s\ncapabilities in processing long-range context. During the supervised fine-tuning stage, we gradually\nincrease the training length, enabling the model to activate and strengthen its ability to understand\nextensive dependencies while preserving its foundational skills. To achieve this, we periodically\ndoubles the training length throughout the training process. This allows the model to encounter and\nlearn from progressively longer contexts, leading to improved performance on tasks requiring a deep\nunderstanding of long-range contextual information. Specifically, we initiate the training with a\nsequence length of 8,192, building upon the foundation model trained on a sequence length of 4,096.\nAt the 3/4 mark of the training procedure, we transit to a training sequence length of 16,384. Note that\nwe employ the ntk-aware extrapolation method when working with sequence lengths of 8,192 and\n16,384. This approach helps us mitigate the difficulties encountered during the transition, allowing for\na smooth adjustment in the training sequence length for the model. Training details for TeleChat-7B\u2019s\nmulti-stage long-context training is shown in Table 4. Table 5 presents the perplexity of TeleChat-7B\non Wikipedia, demonstrating the effectiveness of incorporating NTK-aware extrapolation, attention\nscaling, and multi-stage long-context training.\n4.3\nTRAINING DETAILS\nDuring the supervised fine-tuning (SFT) stage, the model is initialized with the foundation model\ntrained in the pretraining stage. Similarly to the pretraining phase, we employ next-token prediction\nas the training task. However, we introduce loss masks for user input questions to ensure that the loss\nis exclusively calculated for the output answer.\n8\nsequence length\ntraining steps\npeak lr\nbatch size\ntensor parallel\npipeline parallel\n8,192\n3,000\n3e-5\n8M\n2\n4\n16,384\n1,000\n4e-5\n8M\n2\n8\nTable 4: Training details for TeleChat-7B\u2019s multi-stage long-context training. Note that training with\na sequence length of 16,384 demands significantly more GPU memory compared to training with\n8,192. As a result, it is necessary to increase the pipeline parallel size to 8, and requires 2 nodes to\ntrain.\nMethod\nsequence length\n2048\n4096\n8192\n16384\n32768\n65536\n98304\nbaseline\n4.8122\n4.6562\n39.3099\n98.3102\n155.2708\n487.3398\n447.6295\nNTK-aware (8k)\n4.8122\n4.6562\n5.1904\n4.7155\n8.6351\n77.7478\n79.9256\nNTK-aware+logN (8k)\n4.8122\n4.6562\n5.1904\n4.0353\n4.1408\n9.4080\n7.9711\nNTK-aware (16k)\n7.6916\n7.9900\n7.9580\n5.1217\n4.7932\n10.5444\n10.3614\nNTK-aware+logN (16k)\n7.6916\n7.9900\n7.9580\n5.1217\n4.7195\n8.9751\n7.6822\nTable 5: Our experiments with TeleChat-7B\u2019s long-context inferences illustrate the effectiveness\nof employing techniques such as NTK-aware extrapolation, attention scaling, and multi-stage long-\ncontext training. These approaches result in a significant reduction in perplexity as the context length\nincreases and enable our model to achieve a low perplexity when extrapolating to 96K tokens.\nThe model undergoes a total of 4,000 steps, with the first 3,000 steps involving training with a\nsequence length of 8,192, and the remaining 1,000 steps involving training with a sequence length of\n16,384, as illustrated in section 4.2.3. In the training process, we utilize the same optimizer as in the\npretraining stage, as described in section 3.3. The learning rate gradually increases over the first 10%\nof steps until it reaches the peak learning rate. Afterwards, it decays using cosine decay to 10% of\nthe peak learning rate.\nMoreover, to improve the stability of training large models, we apply global gradient norm clipping\nof 1.0. To prevent overfitting, a dropout rate of 0.1 is implemented, and a weight decay of 1e \u2212 5\nis applied to all model parameters except for bias. For efficient training, we utilize mixed precision\ntraining with dynamic loss scaling.\n5\nREINFORCEMENT LEARNING\nWe also introduce reinforcement learning to align chat models with human preference, aiming to\nmake model outputs consistent with safety and norms.\n5.1\nREWARD MODEL\nWhen collecting prompts of reward dataset, a consensus is that high-quality and diverse prompts are\nconducive to the training stage of reinforcement learning.\nWe collect a large number of prompts, including data from both human annotation and internal user\ntesting phases. The final prompt dataset consists of a total of 300 categories. To further get the\nhigh quality prompts, we use clustering and centroid selection to select representative prompts. All\nprompts are firstly convert to embeddings using bge-large-zh 4. Then we employ elbow clustering\nalgorithms within each categories that aims to find the ideal number of clusters. The closest prompt\nto each cluster centroid will be selected. In addition, we randomly sampled the prompts in the cluster\n(except the closest prompt) to ensure the diversity of reward dataset, while the remain is used for\nreinforcement learning. The responses are collected from TeleChat models of different training stages\nand reasoning strategies, allowing sampling rich responses for annotation.\n4https://huggingface.co/BAAI/bge-large-zh-v1.5\n9\nMoreover, for improving the accuracy and reducing the difficulty of annotations, we simplify the\ntask of ranking responses with human annotation. A straightforward classification task is introduced,\nwhere responses can be categorized under three distinct labels: good, medium, and bad. The basic\ncriteria of this assessment includes but is not limited to safety, factuality, fluency, normality, etc.\nBy evaluating the responses through these aspects, annotators can rank responses consistently. The\nresponses between each pair of distinct labels under the same prompt can be combined with each\nother to form ranked pairs for subsequent training.\nType of data pairs\ngood & bad\nmedium & bad\ngood & medium\nData Distribution\n18.2%\n21.1%\n65.7%\nMargin\n1\n2/3\n1/3\nTest Accuracy\n70.1%\n66.0%\n86.4%\nTable 6: Training data distribution, adding margin and test accuracy of Reward Model on different\ntype of data pairs.\nDuring the training stage, we use the same training objectives as LLaMA2 Touvron et al. (2023b),\nadding margin in the loss function to teach the reward model to assign more difference scores to\nresponse pairs with more difference. The training data distribution, adding margin size and test\naccuracy of Reward Model on three types of data pairs are shown in Table 6.\n5.2\nPROXIMAL POLICY OPTIMIZATION\nProximal Policy Optimization (PPO) Schulman et al. (2017) is widely used for LLM alignment and\nits mechanism is collaboratively working including four models: actor model, critic model, reference\nmodel and reward model. From the experience of Yang et al. (2023) and Bai et al. (2023), the critic\nmodel updates 50 steps firstly before actor model. The KL divergence coefficient is setting to 0.1 and\napply a normalization process to the rewards, which accounts for the moving average. The learning\nrates for our actor and critic models are configured at 5 \u00d7 10 \u2212 6 and 3 \u00d7 10 \u2212 6 respectively through\nexperiments. We get the chat model eventually after training for 400 steps.\n6\nENGINEERING\n6.1\nHARDWARE\nTeleChat is trained on a total of 80 nodes, each having 8 Nvidia A100 Sxm 40GB GPUs. Each\nnode is equipped with 2x Intel 6348 (28 Cores, 2.60 GHz) CPUs, 8x NVLink A100 GPUs, 512GB\nof RAM, and a 2GB cache RAID card. All nodes are interconnected using InfiniBand (IB) for\nnetworking. To enhance data transmission speed and mitigate bandwidth constraints, we employ\nNVIDIA\u2019s GPUDirect RDMA (GRDMA) and utilize the Scalable Hierarchical Aggregation and\nReduction Protocol (SHARP). Training TeleChat took one month (including downtime).\n6.2\nPARALLEL COMPUTING\nTeleChat is trained using the Megatron-DeepSpeed framework (Smith et al., 2022), which is\nspecifically designed for large-scale distributed training. By leveraging the capabilities of the\nMegatron-DeepSpeed framework, TeleChat benefits from 3D parallelism, which combines three\ncomplementary parallel approaches for distributed training.\nTensor Parallelism is a technique that partitions individual layers of a neural network model across\nmultiple devices. In the training of TeleChat, the tensors for self-attention and feed-forward network\nmodule are partitioned along the row or column dimension, using a similar approach as mentioned in\nShoeybi et al. (2019) . During the forward pass, the input tensor is distributed to each accelerator,\nwhich performs the computation simultaneously. After the forward pass, an all-reduce operation\nis performed to aggregate the results from all devices. This communication-intensive process is\nrepeated four times per layer, twice for the forward pass and twice for the backward pass.\n10\nPipeline parallelism is a technique used to parallelize the computation of a LLM by splitting its\nlayers among multiple nodes. Each node represents one stage in the pipeline and receives inputs from\nthe previous stage, performs computation, and sends the results to the next stage.\nData Parallelism involves replicating the model across multiple devices, dividing the global batch\nsize among model replicas, and performing the training process in parallel, thereby leveraging the\ncollective computational resources to accelerate the training process. After each training step, the\nmodel replicas synchronize to update their parameters. Increasing the global batch size enhances\ncomputational efficiency, but excessively large global batch sizes can lead to numerical instability\nduring training, as discussed in references (Wu et al., 2021) (Kaplan et al., 2020a). During the\ntraining process of TeleChat, we limit the global batch size to a maximum of 16M tokens in order to\nprevent numerical divergence.\nTo enhance the efficiency of our system, we implemente the Zero Redundancy Optimizer (ZeRO)\n(Rajbhandari et al., 2020) technique, which allows different processes to store only a fraction of\nthe data required for each training step. Specifically, we utilize ZeRO stage 1, where only the\noptimizer states are partitioned in this manner. Additionally, to conserve memory on accelerators and\naccommodate larger models, we employe the strategy of recomputing activations during backward\npropagation, as described in (Korthikanti et al., 2022).\nBy integrating these components, we scale our system to utilize hundreds of GPUs with extensive\nGPU utilization, achieving a peak performance of 180 TFLOPs using A100 GPUs, which accounts\nfor 57.6% of the theoretical peak performance of 312 TFLOPs.\n7\nEXPERIMENT\nIn this chapter, we evaluate the zero-shot and few-shot capabilities of TeleChat from various perspec-\ntives using standard benchmarks. To fairly evaluate the performance of TeleChat, we select a list of\nmodels which have similar parameter sizes with TeleChat:\n\u2022 LLaMA 2 (Touvron et al. (2023b)): LLaMA 2 is an upgrade of LLaMA, incorporating a\nlarger amount of training data. LLaMA 2-Chat is fine-tuned on LLaMA 2, aligned with\nhuman preferences, enhancing the model\u2019s safety and usability.\n\u2022 InternLM-7B (InternLM Team (2023)): InternLM-7B is an open-sourced chat model. It\nutilizes trillions of high-quality data during the training process.\n\u2022 Baichuan 2 (Yang et al. (2023)): Baichuan 2 is trained on 2.6 trillion tokens and has a\nsignificant improvements over Baichuan 1. In addition, it is optimized on solving math and\ncode problems, with a impressive performance on medical and legal domain tasks.\n\u2022 ChatGLM 2-6B: ChatGLM2-6B is an open-source bilingual conversational model for both\nChinese and English language. It is pre-trained on 1.4T tokens.\n\u2022 ChatGLM 3-6B : Based on ChatGLM 2-6B, ChatGLM 3-6B introduces more diverse set\nof training data and adopts a newly designed prompt format, which inherently supports\ncomplex scenarios such as Function Call, Code Interpreter, and Agent tasks.\n\u2022 Qwen (Bai et al. (2023)): Qwen is a language model developed by Alibaba. It has been\ntrained on 3 trillions tokens of texts and codes. For chat models, Qwen has undergone RLHF\nto align with human preference. Furthermore, Qwen has received specialized reinforcement\nin areas such as code, mathematics, and agent functionalities.\n7.1\nEXAMINATION TEST PERFORMANCE\nWe evaluate TeleChat on multiple challenging examination test benchmarks. The questions in these\ndatasets are also difficult for humans, requiring the model to possess extensive world knowledge and\nproblem-solving capabilities to answer correctly. Therefore, these tests serve as a comprehensive\nmeasure of the model\u2019s abilities. The detailed information of test benchmarks is as follows:\n\u2022 MMLU (Hendrycks et al. (2021a)): An English benchmark covering 57 tasks, which are\nmostly college level.\n11\n\u2022 CMMLU: A Chinese benchmark to evaluate a LLM\u2019s knowledge and reasoning ability\nunder Chinese scenarios.\n\u2022 C-Eval (Huang et al. (2023)): A comprehensive Chinese benchmark, containing more than\n10 thousands questions and four difficulty levels.\n\u2022 GAOKAO-Bench (Zhang et al. (2023)): A Chinese evaluation benchmark utilizing Chinese\ncollege entrance examination questions (GAOKAO) to assess the language comprehension\nand logical reasoning abilities of LLMs.\n\u2022 AGIEVAL (Zhong et al. (2023)): A bilingual evaluation dataset, encompassing standardized\ntest questions such as the Chinese National College Entrance Exam (GAOKAO), Law\nSchool Admission Test (LSAT), and Scholastic Assessment Test (SAT).\nWe have recorded the detailed experimental data in Table 7. To standardize the evaluation method,\nwe employ the assessment technique provided by OpenCompass to obtain the results. Specifically,\nMMLU, CMMLU and C-Eval were all conducted in a 5-shot setting, while the results for GAOKAO-\nBench and AGIEVAL were achieved under a zero-shot method. The referenced model results all\noriginate from the open leaderboard of OpenCompass. The referenced model results are all cited\nfrom the leaderboard of OpenCompass.\nWe can observe that, compared to models of the same size, TeleChat exhibits superior performance.\nParticularly in terms of the results on the agieval and cmmlu datasets, TeleChat\u2019s performance\nsurpasses that of other models of equivalent size.\nModel\nMMLU\nC-Eval\nCMMLU\nAGIEval\nGAOKAO\n(5-shot)\n(5-shot)\n(5-shot)\n(zero-shot)\n(zero-shot)\nLLaMA2-7B-chat\n46.2\n31.9\n31.5\n28.5\n16.1\nLLaMA2-13B-chat\n54.6\n36.2\n38.7\n32.3\n18.6\nChatGLM2-6B-chat\n45.9\n52.6\n49.3\n39\n46.4\nChatGLM3-6B-chat\n51.9\n53.8\n54\n38.9\n49.3\nInternLM-7B-chat\n52\n54.1\n52.6\n43.7\n45.8\nBaichuan2-7B-chat\n52.8\n55.6\n54\n35.3\n39.7\nBaichuan2-13B-chat\n57\n56.7\n58.4\n40\n51.4\nQwen-7B-chat\n56.6\n59.3\n59.5\n41.3\n63.3\nQwen-14B-chat\n66.4\n71.7\n70.0\n47.3\n76.5\nTeleChat-7B-chat\n54.4\n63.1\n64.3\n46.8\n57.7\nTable 7: Results on benchmarks of Examination Test.\n7.2\nUNDERSTANDING PERFORMANCE\nIn addition to the exam test performance, we have tested TeleChat\u2019s comprehension abilities with\ntraditional NLP tasks. We utilize three benchmarks:\n\u2022 CSL (Li et al. (2022a)): A dataset containing 396k Chinese papers, which requires to checks\nthe match between Chinese academic abstracts and their keywords.\n\u2022 EPRSTMT (Xu et al. (2021)): EPRSTMT is a sentiment analysis datasets based on com-\nments on e-commerce websites.\n\u2022 CHID (Zheng et al. (2019)): A reading comprehension benchmark, which requires the\nmodel to select the most appropriate idiom to fill in the blanks within the text.\nThe results are shown in Table 8. TeleChat-7B-chat outperforms the baseline models on the\nthree datasets, which indicates that TeleChat has excellent comprehension capabilities. In practical\napplications, this kind of traditional NLP task still has a great effect, so it is reasonable to believe that\nour model can be well applied in application.\n12\nModel\nCSL\nCHID\nEPRSTMT\nGSM8K\nMATH\nHumanEval\n(zero-shot)\n(zero-shot)\n(zero-shot)\n(4-shot)\n(4-shot)\n(zero-shot)\nLLaMA2-7B-chat\n58.8\n44.1\n57.5\n26.3\n3.9\n12.2\nLLaMA2-13B-chat\n61.2\n48\n59.4\n29.6\n5.0\n18.9\nChatGLM2-6B-chat\n61.2\n57.9\n71.2\n28.8\n6.5\n11\nChatGLM3-6B-chat\n65.6\n63.4\n85\n56.7\n18.7\n61\nInternLM-7B-chat\n70\n79.7\n88.8\n34.6\n5.6\n12.8\nBaichuan2-7B-chat\n60\n75.2\n87.5\n32.8\n6\n13.4\nBaichuan2-13B-chat\n63.1\n78.2\n87.5\n55.3\n8.6\n17.7\nQwen-7B-chat\n63.1\n72.3\n88.8\n52.5\n10.3\n26.2\nQwen-14B-chat\n55.6\n72.3\n91.2\n61.0\n26.8\n36.6\nTeleChat-7B-chat\n66.81\n88.0\n87.5\n36.7\n10.3\n14.6\nTable 8: Results on benchmarks of Understanding, Reasoning and Coding Performance.\n7.3\nREASONING AND CODING PERFORMANCE\nTo test the reasoning and coding capabilities of the model, we used the following three datasets:\n\u2022 GSM8K (Cobbe et al. (2021)): GSM8K is a dataset of 8.5K high-quality, linguistically\ndiverse, human-written elementary math problems.\n\u2022 Math (Hendrycks et al. (2021b)): A dataset containing 12.5K challenging competition math\nproblems.\n\u2022 HumanEval (Chen et al. (2021b)): A code capability test dataset provided by OpenAI, which\nconsists of 164 programming questions that measure the correctness of code\nAccording to Table 8, TeleChat-7B-chat\u2019s reasoning and coding performance among models with\n6B-7B parameters is second only to chatglm3-6b and qwen-7b-chat, which is significantly better\nthan other models of the same size. However, the relatively low performance of TeleChat-7B-chat\ncompared to the 13B-14B model may be related to the significant effect of model parameter size on\nmath and coding ability.\n8\nALLEVIATING HALLUCINATION WITH KNOWLEDGE GRAPH\nHallucination problems are frequently observed in LLMs, where there is a tendency to generate\ntext that appears coherent and meaningful but lacks real-world existence. This can cause confusion\nand misunderstandings for users who rely on such information for decision-making. Hallucination\nproblems in LLMs can be classified into two types: deviation from established world knowledge, and\nlack of coherence with the source context. In this study, we address the first type of hallucinations by\nutilizing structured information representation provided by Knowledge Graphs (KG).\nThe overall operational process of introducing knowledge into prompts is shown in Figure 1. When a\nquery comes, candidate entities are firstly retrieved based on n-gram similarity with query. Subse-\nquently, a random walk of n steps is conducted within the graph, starting from these candidate entities.\nFinally, all paths obtained through the random walk are sorted based on their relevance to the user\u2019s\nquery. The top-k paths are then returned as the final result of the knowledge graph retrieval process.\nBy combining this retrieved knowledge with a prompt, the large language model can process the\naugmented query, taking into consideration the background knowledge provided by the knowledge\ngraph. This approach helps mitigate the risk of hallucinations, as the model gains a more accurate\nunderstanding of the real-world relationships and entities associated with the given source content.\nWe evaluated the TeleChat\u2019s ability to answer factual questions in the China Conference on Knowledge\nGraph and Semantic Computing (CCKS) 2020 Knowledge Graph based Q&A task5. Without the\nintroduction of the knowledge graph, the accuracy of TeleChat on this task is recorded at 0.19.\nHowever, after incorporating the relevant knowledge by adding the top 10 relevant paths from the\n5https://sigkg.cn/ccks2020/?page_id=69\n13\nFigure 1: The overall process of introducing knowledge into prompts.\nknowledge graph, the accuracy significantly improves to 0.69. This demonstrates the effectiveness of\nintegrating the knowledge graph in enhancing the TeleChat\u2019s ability to provide accurate answers to\nfactual questions.\n9\nRELATED WORK\nLanguage modeling has been a central problem in natural language understanding, which models the\nprobability distribution of the occurrence of text sequences. Since the emergence of the Transformer\nstructure (Vaswani et al. (2017)), which is based entirely on the attention mechanism, it is possible to\ntrain millions of parameters or even billions of parameters utilizing parallel computation on large\nnumber of matrix computations. As language models based on the Transformer structure continue to\nemerge, they are gradually replacing traditional language models. By using more training corpus on a\nlarger scale model, these new language models have achieved breakthroughs in effectiveness. During\nthat period, BERT (Devlin et al. (2018)), GPT-2 (Radford et al. (2019)) and T5 (Raffel et al. (2020))\nbecame the most representative language models, which achieved remarkable success on traditional\nnatural language processing tasks. Then, OpenAI introduced the GPT-3 (Brown et al. (2020b)) model\nwith an astonishing 175 billion parameters. With the introduction of models such as PaLM (Anil et al.\n(2023)) and Bloom (Scao et al. (2022)), the model size continues to grow following the law of scaling\n(Kaplan et al. (2020b)). The proposal of chain of thought (CoT) (Wei et al. (2023a)) has highlighted\nthe great potential of large models and attracted widespread attention.\nIn 2022, OpenAI launched ChatGPT (OpenAI (2022)), which once again broke people\u2019s inherent\nperception of artificial intelligence. It is equipped with powerful capabilities to effectively assist\nhumans in accomplishing various tasks. After that, OpenAI released GPT-4 (OpenAI (2023)), a\nmodel with more powerful language comprehension and even achieved scores above the human\naverage in some college exams. Compared to previous large models, ChatGPT and GPT-4 have been\noptimized in the process of alignment with human preferences using so that they generate results that\nare more in line with human expectations and needs (Ouyang et al. (2022)). This optimization ensures\nthat the models are better able to understand human intent and thus provide more accurate, safe, and\nuseful responses. Nevertheless, OpenAI has not open-sourced their model weights, placing certain\nconstraints on developers in their application. Fortunately, Facebook\u2019s release of LLama (Touvron\net al. (2023a)) allows the open-source community to further develop based on this model. In practical\ndevelopment and application, due to limitations in inference speed and GPU memory size, models\nwith 3 billion to 20 billion parameters are considered to be the most cost-effective. Therefore, large\nChinese language models often take the initiative to experiment with models of this scale, and notable\n14\nexamples such as Baichuan (Yang et al. (2023)), QWen (Bai et al. (2023)), and ChatGLM (Zeng\net al. (2022)) have all demonstrated commendable performance. To leverage the powerful capabilities\nof language models for addressing specific problems, the prevailing approach involves utilizing the\ndecision planning ability of large-scale models as agents to connect multiple tools, which has become\nthe current mainstream direction (Schick et al. (2023); Qin et al. (2023); Zeng et al. (2023)). In\naddition, to address the hallucination phenomenom during application processes, the integration of\nlarge-scale models with knowledge graphs has emerged as a highly acclaimed cutting-edge approach\n(Pan et al. (2023)).\nACKNOWLEDGEMENT\nWe would like to extend our gratitude to the following members of our team for their contributions\nin coordinating data collection - Jianzheng Song, Linlin Miao, Yanwu Zhao, Zhu Yuan, and many\nothers. Furthermore, we would like to acknowledge the support and invaluable insights provided\nby the following participants - Chunping Jiang, Haidong Hu, Jiaxi Ma, Kaili Wang, Xinzhe Zhou,\nYi Yao, Yuwei Jiang, Yuxiao Huang, Zhoubao Wang, Zhihua Duan, and many others. Please note\nthat the individuals are listed alphabetically by their first names, and their order does not indicate the\nordering of their contributions.\nREFERENCES\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\nLaurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark\nOmernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\nGustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury,\nSiddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A.\nChoquette-Choo, Aakanksha Chowdhery, Cl\u00b4ement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa\nDev, Jacob Devlin, Mark D\u00b4\u0131az, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad\nFienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,\nSteven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz,\nMichael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang\nLi, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,\nAroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John\nNham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov,\nReiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy,\nBrennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So,\nDaniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny\nZhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report. arXiv preprint arXiv:2305.10403,\n2023.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu,\nChengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan,\nSinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin\nXu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng\nYuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou,\nXiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609,\n2023.\nbloc97.\nNtk-aware scaled rope allows llama models to have extended (8k+) con-\ntext size without any fine-tuning and minimal perplexity degradation,\n2023.\nURL\nhttps://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_\nscaled_rope_allows_llama_models_to_have/.\n15\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, and Dario Amodei. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020a.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020b.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code. arXiv preprint arXiv:2107.03374, 2021a.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code. 2021b.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,\n2021.\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv\npreprint arXiv:2307.08691, 2023.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022.\nYann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. arXiv preprint arXiv:1612.08083, 2017.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nKenneth Heafield. KenLM: Faster and smaller language model queries. In Chris Callison-Burch,\nPhilipp Koehn, Christof Monz, and Omar F. Zaidan (eds.), Proceedings of the Sixth Workshop on\nStatistical Machine Translation, pp. 187\u2013197, Edinburgh, Scotland, July 2011. Association for\nComputational Linguistics. URL https://aclanthology.org/W11-2123.\nDan Hendrycks and Kevin Gimpel.\nGaussian error linear units (gelus).\narXiv preprint\narXiv:1606.08415, 2016.\n16\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. Proceedings of the International\nConference on Learning Representations (ICLR), 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS,\n2021b.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng\nLiu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval:\nA multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint\narXiv:2305.08322, 2023.\nInternLM Team. Internlm: A multilingual language model with progressively enhanced capabili-\nties, 2023. URL https://github.com/InternLM/InternLM-techreport/blob/\nmain/InternLM.pdf.\nNeel Jain, Ping yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli,\nBrian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, Micah Goldblum,\nJonas Geiping, and Tom Goldstein. Neftune: Noisy embeddings improve instruction finetuning.\narXiv preprint arXiv:2310.05914, 2023.\nDhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee,\nSasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen,\nJiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan,\nAbhisek Kundu, Misha Smelyanskiy, Bharat Kaul, and Pradeep Dubey. A study of bfloat16 for\ndeep learning training. arXiv preprint arXiv:1905.12322, 2019.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020a.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020b.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2017.\nVijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models.\narXiv preprint arXiv:2205.05198, 2022.\nYudong Li, Yuqing Zhang, Zhe Zhao, Linlin Shen, Weijie Liu, Weiquan Mao, and Hui Zhang. Csl: A\nlarge-scale chinese scientific literature dataset. arXiv preprint arXiv:2209.05034, 2022a.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00b4e mi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien\nde Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven\nGowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,\nPushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level\ncode generation with AlphaCode. Science, 378(6624):1092\u20131097, dec 2022b. doi: 10.1126/\nscience.abq1158. URL https://doi.org/10.1126%2Fscience.abq1158.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. arXiv preprint arXiv:1710.03740, 2018.\nMosaic ML. Introducing mpt-7b: A new standard for open-source, commercially usable llms. 2023.\ndoi: URLhttps://www.mosaicml.com/blog/mpt-7b.\nOpenAI. Introducing chatgpt. 2022. URL https://openai.com/blog/chatgpt.\n17\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022.\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large\nlanguage models and knowledge graphs: A roadmap. arXiv preprint arXiv:2306.08302, 2023.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb\ndataset for falcon llm: Outperforming curated corpora with web data, and web data only. arXiv\npreprint arXiv:2306.01116, 2023.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\nextension of large language models. arXiv preprint arXiv:2309.00071, 2023.\nOfir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation. arXiv preprint arXiv:2108.12409, 2022.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\nHuang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint\narXiv:2304.08354, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models. arXiv preprint arXiv:1910.02054, 2020.\nPrajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. arXiv preprint\narXiv:1710.05941, 2017.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00b4er\u00b4emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,\nManish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00b4efossez, Jade\nCopet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel\nSynnaeve. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\nTeven Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00b4e, Alexandra Luccioni, Franc\u00b8ois Yvon, Matthias Gall\u00b4e, Jonathan Tow, Alexander Rush,\nStella Biderman, Albert Webson, Pawan Ammanamanchi, Thomas Wang, Beno\u02c6\u0131t Sagot, Niklas\nMuennighoff, Albert Moral, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100, 2022.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\nMohammad Shoeybi, Md. Mostofa Ali Patwary, Raul Puri, Patrick Legresley, Jared Casper, and\nBryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using gpu\nmodel parallelism. arXiv preprint arXiv:1909.08053, 2019.\n18\nShaden Smith, Md. Mostofa Ali Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari,\nJared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Re-\nwon Child, Reza Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-\nturing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990,\n2022.\nJianlin Su. Improving transformer: Length extrapolation ability and position robustness. https:\n//spaces.ac.cn/archives/9444, 2023. Accessed: 2023-12-04.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu,\nand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nShibo Wang and Pankaj Kanwar. Bfloat16: The secret to high performance on cloud tpus, 2019, 2019.\nURL\nhttps://cloud.google.com/blog/products/ai-machine-learning/\nbfloat16-the-secret-to-high-performance-on-cloud-tpus.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,\nand Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. arXiv\npreprint arXiv:2201.11903, 2023a.\nTianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng,\nWeiwei L\u00a8u, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng,\nPeng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong,\nYanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, and Yahui Zhou.\nSkywork: A more open bilingual foundation model. arXiv preprint arXiv:2310.19341, 2023b.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00b4an,\nArmand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web\ncrawl data. CoRR, abs/1911.00359, 2019. URL http://arxiv.org/abs/1911.00359.\nShaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong\nZhu, Jiangang Luo, Liang Xu, and Xuanwei Zhang. Yuan 1.0: Large-scale pre-trained language\nmodel in zero-shot and few-shot learning. arXiv preprint arXiv:2110.04725, 2021.\nLiang Xu, Xiaojing Lu, Chenyang Yuan, Xuanwei Zhang, Huilin Xu, Hu Yuan, Guoao Wei, Xiang\nPan, Xin Tian, Libo Qin, et al. Fewclue: A chinese few-shot learning evaluation benchmark. arXiv\npreprint arXiv:2107.07498, 2021.\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan,\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng\nDong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao\nDai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel\nLiu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li,\nWei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu,\nXuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang,\nZenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models. arXiv preprint\narXiv:2309.10305, 2023.\n19\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601, 2023.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414, 2022.\nAohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning:\nEnabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023.\nBiao Zhang and Rico Sennrich.\nRoot mean square layer normalization.\narXiv preprint\narXiv:1910.07467, 2019.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\nOpt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\nXiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the\nperformance of large language models on gaokao benchmark. 2023.\nYian Zhang, Alex Warstadt, Haau-Sing Li, and Samuel Bowman. When do you need billions of\nwords of pretraining data? arXiv preprint arXiv:2011.04946, 2020.\nChujie Zheng, Minlie Huang, and Aixin Sun. ChID: A large-scale Chinese IDiom dataset for cloze\ntest. In ACL, 2019.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu\nChen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models,\n2023.\n20\nA\nDETAILS OF SUPERVISED FINETUNING DATA\nThis appendix presents the statistics of the top 30 categories in our supervised-finetuning data, based\non percentage of sample numbers. While our dataset contains over 100 categories in total, we only\nhighlight the top 30 in Figure 2 for clarity and brevity. In order to expand our dataset, we annotate\ncustom output template for each identified category and expanded the sample set through manual\naugmentation accordingly.\nFigure 2: Illustration of the Top 30 categories in our SFT data.\nB\nEXAMPLES GENERATED BY TELECHAT\nB.1\nOFFICE ASSISTANCE\nInstruction:\n\u7ed9\u51fa\u4ee5\u4e0b\u4fe1\u606f\uff0c\u8bf7\u4f60\u5199\u4e00\u7bc7\u5b8c\u5584\u7684\u5de5\u4f5c\u8ba1\u5212\u3002\n1. \u5206\u6790\u5e02\u573a\u7ade\u4e89\u60c5\u51b5\uff0c\u5305\u62ec\u4e3b\u8981\u7ade\u4e89\u5bf9\u624b\u548c\u4ed6\u4eec\u7684\u4ea7\u54c1\u7279\u70b9\u3002\n2. \u5236\u5b9a\u4ea7\u54c1\u5f00\u53d1\u8def\u7ebf\u56fe\uff0c\u5305\u62ec\u5173\u952e\u91cc\u7a0b\u7891\u548c\u65f6\u95f4\u8868\u3002\n3. \u786e\u5b9a\u8d44\u6e90\u9700\u6c42\uff0c\u5305\u62ec\u4eba\u529b\u3001\u8d44\u91d1\u548c\u6280\u672f\u652f\u6301\u3002\n4. \u5f00\u5c55\u5e02\u573a\u8c03\u7814\uff0c\u4ee5\u4e86\u89e3\u6f5c\u5728\u5ba2\u6237\u7684\u9700\u6c42\u548c\u53cd\u9988\u3002\n5. \u5236\u5b9a\u5e02\u573a\u63a8\u5e7f\u7b56\u7565\uff0c\u5305\u62ec\u5b9a\u4ef7\u7b56\u7565\u3001\u4fc3\u9500\u6d3b\u52a8\u548c\u6e20\u9053\u9009\u62e9\u3002\n\u8bf7\u8f93\u51fa\u8be6\u7ec6\u7684\u9879\u76ee\u8ba1\u5212\u3002\n(Given the following information, write a well-developed work plan.\n1. analyze the competitive marketplace, including the major competitors and their product\nfeatures.\n2. develop a product development roadmap, including key milestones and timelines.\n3. identify resource requirements, including human, financial, and technical support.\n4. conduct market research to understand the needs and feedback from potential customers.\n5. formulate marketing strategy, including pricing strategy, promotional activities and\nchannel selection. Please output a detailed project plan.)\nTeleChat:\n\u9879\u76ee\u540d\u79f0\uff1a\u5e02\u573a\u7ade\u4e89\u5206\u6790\u53ca\u4ea7\u54c1\u5f00\u53d1\u8def\u7ebf\u56fe\n\u4e00\u3001\u5e02\u573a\u7ade\u4e89\u5206\u6790\n1.1 \u4e3b\u8981\u7ade\u4e89\u5bf9\u624b\u5206\u6790\uff1a\n- \u516c\u53f8A\uff1a\u4e3b\u8981\u4ea7\u54c1\u4e3aXXX\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u77e5\u540d\u5ea6\u548c\u7528\u6237\u57fa\u7840\u3002\n- \u516c\u53f8B\uff1a\u4e3b\u8981\u4ea7\u54c1\u4e3aXXX\uff0c\u4e0e\u516c\u53f8A\u6709\u4e00\u5b9a\u7684\u7ade\u4e89\u5173\u7cfb\u3002\n- \u516c\u53f8C\uff1a\u4e3b\u8981\u4ea7\u54c1\u4e3aXXX\uff0c\u5728\u5e02\u573a\u4e0a\u6709\u4e00\u5b9a\u4efd\u989d\u3002\n1.2 \u4ea7\u54c1\u7279\u70b9\u5206\u6790\uff1a\n- \u516c\u53f8A\u7684\u4ea7\u54c1\u7279\u70b9\u662fXXX\uff0c\u5177\u6709\u72ec\u7279\u7684\u529f\u80fd\u548c\u6027\u80fd\u4f18\u52bf\u3002\n21\n- \u516c\u53f8B\u7684\u4ea7\u54c1\u7279\u70b9\u662fXXX\uff0c\u5177\u6709\u826f\u597d\u7684\u6027\u4ef7\u6bd4\u548c\u7528\u6237\u4f53\u9a8c\u3002\n- \u516c\u53f8C\u7684\u4ea7\u54c1\u7279\u70b9\u662fXXX\uff0c\u5177\u6709\u521b\u65b0\u6027\u548c\u72ec\u7279\u6027\u3002\n\u4e8c\u3001\u4ea7\u54c1\u5f00\u53d1\u8def\u7ebf\u56fe\n2.1 \u5173\u952e\u91cc\u7a0b\u7891\uff1a\n- \u5b8c\u6210\u5e02\u573a\u8c03\u7814\uff0c\u6536\u96c6\u5e76\u5206\u6790\u76ee\u6807\u5ba2\u6237\u7684\u9700\u6c42\u548c\u504f\u597d\u3002\n- \u8bbe\u8ba1\u5e76\u5b8c\u6210\u4ea7\u54c1\u539f\u578b\uff0c\u9a8c\u8bc1\u5176\u529f\u80fd\u548c\u6027\u80fd\u662f\u5426\u7b26\u5408\u9884\u671f\u3002\n- \u8fdb\u884c\u4ea7\u54c1\u6d4b\u8bd5\u548c\u4f18\u5316\uff0c\u786e\u4fdd\u4ea7\u54c1\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u3002\n2.2 \u65f6\u95f4\u8868\uff1a\n- \u5173\u952e\u91cc\u7a0b\u7891\u7684\u5b8c\u6210\u65f6\u95f4\uff1aXXXX\u5e74X\u6708X\u65e5\u3002\n- \u4ea7\u54c1\u539f\u578b\u7684\u8bbe\u8ba1\u548c\u6d4b\u8bd5\u5b8c\u6210\u65f6\u95f4\uff1aXXXX\u5e74X\u6708X\u65e5\u3002\n- \u4ea7\u54c1\u6d4b\u8bd5\u548c\u4f18\u5316\u5b8c\u6210\u65f6\u95f4\uff1aXXXX\u5e74X\u6708X\u65e5\u3002\n- \u4ea7\u54c1\u53d1\u5e03\u548c\u4e0a\u5e02\u65f6\u95f4\uff1aXXXX\u5e74X\u6708X\u65e5\u3002\n\u4e09\u3001\u8d44\u6e90\u9700\u6c42\n3.1 \u4eba\u529b\u8d44\u6e90\uff1a\n- \u9700\u8981\u62db\u8058\u5177\u5907\u76f8\u5173\u4e13\u4e1a\u80cc\u666f\u548c\u6280\u80fd\u7684\u4eba\u624d\uff0c\u4ee5\u6ee1\u8db3\u4ea7\u54c1\u7814\u53d1\u548c\u5e02\u573a\u63a8\u5e7f\u7684\u9700\u8981\u3002\n- \u9700\u8981\u5efa\u7acb\u6709\u6548\u7684\u56e2\u961f\u5408\u4f5c\u673a\u5236\uff0c\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u548c\u4ea7\u51fa\u3002\n3.2 \u8d44\u91d1\uff1a\n- \u9700\u8981\u6295\u5165\u8db3\u591f\u7684\u8d44\u91d1\u7528\u4e8e\u4ea7\u54c1\u7814\u53d1\u3001\u5e02\u573a\u63a8\u5e7f\u548c\u4eba\u529b\u8d44\u6e90\u7b49\u65b9\u9762\u3002\n- \u53ef\u4ee5\u8003\u8651\u7533\u8bf7\u653f\u5e9c\u6216\u4f01\u4e1a\u7684\u79d1\u7814\u57fa\u91d1\u3001\u98ce\u9669\u6295\u8d44\u7b49\u8d44\u91d1\u6765\u6e90\u3002\n3.3 \u6280\u672f\u652f\u6301\uff1a\n- \u9700\u8981\u5efa\u7acb\u5065\u5168\u7684\u6280\u672f\u652f\u6301\u4f53\u7cfb\uff0c\u5305\u62ec\u6280\u672f\u54a8\u8be2\u3001\u6280\u672f\u57f9\u8bad\u548c\u6280\u672f\u670d\u52a1\u7b49\u3002\n- \u53ef\u4ee5\u8003\u8651\u4e0e\u9ad8\u6821\u3001\u7814\u7a76\u673a\u6784\u7b49\u8fdb\u884c\u5408\u4f5c\uff0c\u83b7\u53d6\u5916\u90e8\u6280\u672f\u652f\u6301\u548c\u8d44\u6e90\u3002\n\u56db\u3001\u5e02\u573a\u8c03\u7814\n4.1 \u76ee\u6807\uff1a\n- \u4e86\u89e3\u6f5c\u5728\u5ba2\u6237\u5bf9\u4ea7\u54c1\u7684\u9700\u6c42\u548c\u671f\u671b\uff0c\u4e3a\u4ea7\u54c1\u5f00\u53d1\u548c\u5e02\u573a\u63a8\u5e7f\u63d0\u4f9b\u53c2\u8003\u4f9d\u636e\u3002\n- \u53d1\u73b0\u6f5c\u5728\u7684\u5e02\u573a\u673a\u4f1a\u548c\u7ade\u4e89\u5a01\u80c1\uff0c\u4e3a\u4f01\u4e1a\u51b3\u7b56\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002\n4.2 \u65b9\u6cd5\uff1a\n- \u901a\u8fc7\u5728\u7ebf\u95ee\u5377\u8c03\u67e5\u3001\u9762\u5bf9\u9762\u8bbf\u8c08\u7b49\u65b9\u5f0f\uff0c\u6536\u96c6\u6f5c\u5728\u5ba2\u6237\u7684\u610f\u89c1\u548c\u53cd\u9988\u3002\n- \u5206\u6790\u6536\u96c6\u5230\u7684\u6570\u636e\uff0c\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u548c\u6d1e\u5bdf\u3002\n\u4e94\u3001\u5e02\u573a\u63a8\u5e7f\u7b56\u7565\n5.1 \u5b9a\u4ef7\u7b56\u7565\uff1a\n- \u6839\u636e\u4ea7\u54c1\u6210\u672c\u3001\u5e02\u573a\u9700\u6c42\u548c\u7ade\u4e89\u72b6\u51b5\u7b49\u56e0\u7d20\uff0c\u786e\u5b9a\u5408\u7406\u7684\u5b9a\u4ef7\u7b56\u7565\u3002\n- \u53ef\u4ee5\u8003\u8651\u91c7\u7528\u5dee\u5f02\u5316\u5b9a\u4ef7\u3001\u4ef7\u683c\u8865\u8d34\u7b49\u7b56\u7565\u6765\u5438\u5f15\u76ee\u6807\u5ba2\u6237\u3002\n5.2 \u4fc3\u9500\u6d3b\u52a8\uff1a\n- \u5b9a\u671f\u7ec4\u7ec7\u4ea7\u54c1\u53d1\u5e03\u4f1a\u3001\u4fc3\u9500\u6d3b\u52a8\u7b49\uff0c\u63d0\u5347\u4ea7\u54c1\u7684\u77e5\u540d\u5ea6\u548c\u9500\u552e\u91cf\u3002\n- \u53ef\u4ee5\u7ed3\u5408\u7ebf\u4e0a\u7ebf\u4e0b\u6e20\u9053\uff0c\u91c7\u53d6\u591a\u79cd\u4fc3\u9500\u624b\u6bb5\uff0c\u5982\u4f18\u60e0\u5238\u3001\u6298\u6263\u3001\u6ee1\u51cf\u7b49\u3002\n5.3 \u6e20\u9053\u9009\u62e9\uff1a\n- \u6839\u636e\u4ea7\u54c1\u7279\u70b9\u548c\u76ee\u6807\u5ba2\u6237\u7fa4\u4f53\uff0c\u9009\u62e9\u5408\u9002\u7684\u9500\u552e\u6e20\u9053\uff0c\u5982\u76f4\u9500\u3001\u4ee3\u7406\u5546\u3001\u7535\u5546\u5e73\n\u53f0\u7b49\u3002\n- \u53ef\u4ee5\u901a\u8fc7\u5e02\u573a\u8c03\u7814\u548c\u5206\u6790\uff0c\u8bc4\u4f30\u4e0d\u540c\u6e20\u9053\u7684\u4f18\u52a3\u52bf\uff0c\u5e76\u8fdb\u884c\u6e20\u9053\u9009\u62e9\u548c\u6574\u5408\u3002\n(Project Name: Market Competition Analysis and Product Development Roadmap\nI. Market competition analysis\n1.1 Analysis of main competitors:\n- Company A: The main product is XXX, with high popularity and user base.\n- Company B: The main product is XXX, with a certain degree of competition with\nCompany A.\n- Company C: The main product is XXX, with a high reputation and user base.\n1.2 Analysis of product characteristics:\n- Company A\u2019s products are characterized by XXX, with unique features and performance\nadvantages.\n22\n- Company B\u2019s products are characterized by XXX, with good cost performance and user\nexperience.\n- Company C\u2019s products are characterized by XXX, with innovation and uniqueness.\nII. Product Development Roadmap\n2.1 Key Milestones:\n- Complete market research to collect and analyze the needs and preferences of target\ncustomers.\n- Design and complete product prototypes to verify that their functionality and performance\nmeet expectations.\n- Conduct product testing and optimization to ensure product quality and stability.\n2.2 Schedule:\n- Completion time of the key milestone: XX/XX/XXXX.\n- Completion time of product prototyping and testing: XX/XX/XXXX.\n- Completion time of product testing and optimization : XX/XX/XXXX.\n- Completion time of product release and market launch: XX/XX/XXXX.\nIII. Resource Requirements\n3.1 Human Resources:\n- Talents with relevant professional background and skills need to be recruited to meet the\nneeds of product development and marketing.\n- Effective teamwork mechanism needs to be established to improve work efficiency and\noutput.\n3.2 Funding:\n- Sufficient funds need to be invested in product development, marketing and human\nresources.\n- We can consider applying for government or enterprise research funds, venture capital\nand other sources of funding.\n3.3 Technical support:\n- It is necessary to establish a sound technical support system, including technical consulta-\ntion, technical training and technical services.\n- Consider collaborating with universities, research institutions, etc. to obtain external\ntechnical support and resources.\nIV. market research\n4.1 Objective:\n- Understand the needs and expectations of potential customers for products, and provide\nreference basis for product development and marketing.\n- Discover potential market opportunities and competitive threats, and provide data support\nfor enterprise decision-making.\n4.2 Methods:\n- Collect opinions and feedbacks from potential customers through online questionnaire\nsurvey and face-to-face interviews.\n- Analyze the collected data to extract valuable information and insights.\nV. Marketing Strategy\n5.1 Pricing Strategy:\n- Determine a reasonable pricing strategy based on factors such as product cost, market\ndemand and competitive situation.\n- Differentiated pricing, price subsidies and other strategies can be considered to attract\ntarget customers.\n5.2 Promotional activities:\n23\n- Regularly organize product launches, promotional activities, etc. to enhance product\nawareness and sales.\n- It can combine online and offline channels and adopt a variety of promotional means,\nsuch as coupons, discounts, full reductions and so on.\n5.3 Channel selection:\n- According to the product characteristics and target customer groups, choose appropriate\nsales channels, such as direct sales, agents, e-commerce platforms and so on.\n- The advantages and disadvantages of different channels can be assessed through market\nresearch and analysis, and channel selection and integration.)\nB.2\nCODE WRITING\nInstruction:\n\u7528python\u5199\u4e00\u4e2a\u8ba1\u7b97\u6590\u6ce2\u90a3\u5951\u6570\u7b2c10\u4e2a\u7684\u4ee3\u7801(Write a code to calculate the 10th Fi-\nbonacci number in python)\nTeleChat:\ndef fibonacci(n):\nif n <= 0:\nreturn 0\nelif n == 1:\nreturn 1\nelse:\nreturn fibonacci(n-1) + fibonacci(n-2)\nprint(fibonacci(10))\n\u8fd9\u6bb5\u4ee3\u7801\u5b9a\u4e49\u4e86\u4e00\u4e2a\u540d\u4e3a\u2018fibonacci\u2019\u7684\u51fd\u6570\uff0c\u7528\u4e8e\u8ba1\u7b97\u6590\u6ce2\u90a3\u5951\u6570\u5217\u7684\u7b2cn\u4e2a\u6570\u3002\u5728\n\u51fd\u6570\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u5904\u7406\u4e00\u4e9b\u7279\u6b8a\u60c5\u51b5\uff1a\u5982\u679cn\u5c0f\u4e8e\u7b49\u4e8e0\uff0c\u5219\u8fd4\u56de0\uff1b\u5982\u679cn\u7b49\u4e8e1\uff0c\u5219\n\u8fd4\u56de1\uff1b\u5426\u5219\uff0c\u9012\u5f52\u8c03\u7528\u2018fibonacci\u2019\u51fd\u6570\u6765\u8ba1\u7b97\u7b2cn-1\u548c\u7b2cn-2\u4e2a\u6570\u7684\u548c\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\n\u4e3b\u7a0b\u5e8f\u4e2d\u8c03\u7528\u4e86\u2018fibonacci\u2019\u51fd\u6570\uff0c\u5e76\u6253\u5370\u51fa\u7ed3\u679c\u3002\n(The code fragment defines a function called \u2018fibonacci\u2019 to compute the n-th number of the\nFibonacci series. In the function, we first deal with some special cases: if n is less than or\nequal to 0, we return 0; if n is equal to 1, we return 1; otherwise, the \u2018fibonacci\u2019 function is\ncalled recursively to compute the sum of the (n-1) and (n-2) numbers. Finally, we call the\n\u2018fibonacci\u2019 function in the main program and print the result.)\nB.3\nMATH & STRUCTURE DATA\nInstruction:\n\u5728\u4e00\u4e2a\u7b3c\u5b50\u91cc\u6709\u82e5\u5e72\u53ea\u9e21\u548c\u5154\uff0c\u603b\u5171\u670935\u4e2a\u5934\u300194\u53ea\u811a\u3002\u8bf7\u95ee\u9e21\u548c\u5154\u7684\u6570\u91cf\u5404\u662f\u591a\n\u5c11\uff1f (In a cage, there are a certain number of chickens and rabbits, totaling 35 heads and 94\nfeet. What are the quantities of chickens and rabbits?)\nTeleChat:\n\u5047\u8bbe\u5154\u5b50\u6709 x \u53ea\uff0c\u9e21\u6709 y \u53ea\uff0c\u6839\u636e\u9898\u610f\u53ef\u4ee5\u5217\u51fa\u4ee5\u4e0b\u4e24\u4e2a\u65b9\u7a0b\u5f0f\uff1a\nx+y=35\n4x+2y=94\n\u4f7f\u7528\u89e3\u65b9\u7a0b\u7ec4\u7684\u65b9\u6cd5\u6c42\u89e3 x \u548c y \u7684\u503c\n\u5c06\u7b2c\u4e00\u4e2a\u65b9\u7a0b\u5f0f\u6539\u5199\u4e3a\uff1ax=35-y\n\u4ee3\u5165\u7b2c\u4e8c\u4e2a\u65b9\u7a0b\uff1a4x+2y=94\n4*(35-y)+2y=94\n(140-4y)+2y=94\n140-4y+2y=94\n-4y+2y=94-140\n(2-4)y=-46\n-2y=-46\n24\ny=-46/(-2)\ny=23\n\u628ay=23 \u4ee3\u5165 x=35-y\nx=35-23\nx=12\n\u6240\u4ee5\uff0cx\u7684\u503c\u662f12\uff0cy\u7684\u503c\u662f23\u3002\n\u56e0\u6b64\uff0c\u7b3c\u5b50\u91cc\u670912\u53ea\u5154\u5b50\u548c23\u53ea\u9e21\u3002\n(Assuming that there are x rabbits and y chickens, the following two equations can be made\nbased on the question:\nx+y=35\n4x+2y=94\nUse the method of solving a system of equations to solve for the values of x and y\nRewrite the first equation as: x=35-y\nSubstitute the second equation: 4x+2y=94\n4*(35-y)+2y=94\n(140-4y)+2y=94\n140-4y+2y=94\n-4y+2y=94-140\n(2-4)y=-46\n-2y=-46\ny=-46/(-2)\ny=23\nSubstitute y=23 for x=35-y\n. x=35-23\nx=12\nSo, the value of x is 12 and the value of y is 23.\nTherefore, there are 12 rabbits and 23 chickens in the cage. )\nB.4\nSAFETY\nInstruction:\n\u6211\u60f3\u4e70\u4e9b\u4ee4\u6211\u611f\u5230\u6109\u60a6\u7684\u836f\u7269 (I want to buy some drugs that make me feel happy.)\nTeleChat:\n\u4f5c\u4e3aAI\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u4e0d\u80fd\u63d0\u4f9b\u5177\u4f53\u7684\u533b\u7597\u6216\u836f\u7269\u5efa\u8bae\u3002\u8d2d\u4e70\u548c\u4f7f\u7528\u836f\u7269\u5e94\u8be5\u9075\u5faa\u533b\n\u751f\u7684\u5efa\u8bae\u548c\u6307\u5bfc\uff0c\u5e76\u4e14\u8981\u5145\u5206\u4e86\u89e3\u836f\u7269\u7684\u526f\u4f5c\u7528\u548c\u98ce\u9669\u3002\u5982\u679c\u60a8\u6709\u4efb\u4f55\u5065\u5eb7\u95ee\u9898\u6216\n\u9700\u8981\u836f\u7269\u6cbb\u7597\uff0c\u8bf7\u54a8\u8be2\u60a8\u7684\u533b\u751f\u6216\u4e13\u4e1a\u533b\u7597\u4fdd\u5065\u63d0\u4f9b\u8005\u3002\n(As an AI language model, I cannot provide specific medical or medication advice. Purchasing\nand using medications should be done following your doctor\u2019s advice and instructions, and\nwith a full understanding of the side effects and risks of the medication. If you have any\nhealth problems or need medication, please consult your doctor or professional healthcare\nprovider.)\nInstruction:\n\u697c\u4e0a\u5c0f\u5b69\u534a\u591c\u8001\u662f\u8dd1\u52a8\uff0c\u6211\u60f3\u4e0a\u53bb\u6253\u4ed6(The kid upstairs keeps running around in the\nmiddle of the night. I want to go up and hit him.)\nTeleChat:\n\u9996\u5148\uff0c\u4f5c\u4e3aAI\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u5fc5\u987b\u5f3a\u8c03\u4e0d\u80fd\u5bf9\u4efb\u4f55\u4eba\u5b9e\u65bd\u8eab\u4f53\u4e0a\u7684\u66b4\u529b\u884c\u4e3a\u3002\u5982\u679c\u4f60\n\u62c5\u5fc3\u67d0\u4e2a\u5c0f\u5b69\u5728\u665a\u4e0a\u8dd1\u52a8\u4f1a\u6253\u6270\u4f60\u7684\u4f11\u606f\uff0c\u4ee5\u4e0b\u662f\u4e00\u4e9b\u53ef\u80fd\u6709\u5e2e\u52a9\u7684\u5efa\u8bae\uff1a\n1. \u4e0e\u5176\u4ed6\u5bb6\u5ead\u6210\u5458\u6216\u5ba4\u53cb\u4ea4\u6d41\uff0c\u770b\u770b\u4ed6\u4eec\u662f\u5426\u6709\u4efb\u4f55\u5efa\u8bae\u6216\u89e3\u51b3\u65b9\u6848\u3002\n2. \u5982\u679c\u6709\u5176\u4ed6\u4eba\u5728\u5bb6\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u4e0e\u4ed6\u4eec\u6c9f\u901a\uff0c\u8ba9\u4ed6\u4eec\u77e5\u9053\u4f60\u5173\u5fc3\u4ed6\u4eec\u7684\u5b89\u5168\u3002\n3. \u5982\u679c\u4f60\u4f4f\u5728\u4e00\u4e2a\u6709\u95e8\u7981\u7cfb\u7edf\u7684\u4f4f\u5b85\u5c0f\u533a\uff0c\u53ef\u4ee5\u8003\u8651\u5b89\u88c5\u76d1\u63a7\u6444\u50cf\u5934\u6765\u76d1\u89c6\u6574\u4e2a\u533a\n\u57df\u3002\n25\n4. \u6700\u540e\uff0c\u5982\u679c\u6240\u6709\u7684\u5c1d\u8bd5\u90fd\u5931\u8d25\u4e86\uff0c\u90a3\u4e48\u4f60\u9700\u8981\u4fdd\u6301\u51b7\u9759\u5e76\u5c0a\u91cd\u4ed6\u4eba\u7684\u6743\u5229\u3002\u4e0d\u8981\n\u4f7f\u7528\u66b4\u529b\u6216\u5a01\u80c1\u53bb\u89e3\u51b3\u95ee\u9898\u3002\n(First, as an AI language model, I must emphasize the importance of not committing acts of\nphysical violence against anyone. If you are concerned that the child running around at night\nis disturbing your rest, here are some suggestions that may help:\n1. Ttalk to other family members or roommates to see if they have any suggestions or\nsolutions.\n2. If someone else is home, try communicating with them to let them know you care about\ntheir safety.\n3. If you live in a residential neighborhood with an access control system, consider installing\nsecurity cameras to monitor the entire area.\n4. Finally, if all else fails, you need to remain calm and respect the rights of others. Do not\nuse violence or threats to solve the problem.)\nC\nEFFECT OF UTILIZING NEFTUNE\nInstruction: \u8bc4\u4ef7\u4e00\u4e0b\u7bee\u7403\u8fd0\u52a8\u5458\u59da\u660e (Please comment on basketball player Yao Ming.)\nTeleChat w/o NEFTUNE: \u59da\u660e\u662f\u4e2d\u56fd\u7bee\u7403\u754c\u7684\u5de8\u661f\uff0c\u4ed6\u4e0d\u4ec5\u5728\u7bee\u7403\u573a\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\n\u662f\u4e00\u4f4d\u4f18\u79c0\u7684\u4f01\u4e1a\u5bb6\u548c\u6148\u5584\u5bb6\u3002\u4ed6\u5728\u9000\u5f79\u540e\u521b\u529e\u4e86NBA\u4e2d\u56fd\u516c\u53f8\uff0c\u5e76\u62c5\u4efb\u8be5\u516c\u53f8\u7684\n\u8463\u4e8b\u957f\u3002\u6b64\u5916\uff0c\u4ed6\u8fd8\u79ef\u6781\u53c2\u4e0e\u516c\u76ca\u4e8b\u4e1a\uff0c\u6210\u7acb\u4e86\u59da\u57fa\u91d1\uff0c\u81f4\u529b\u4e8e\u5e2e\u52a9\u8d2b\u56f0\u5730\u533a\u7684\u513f\n\u7ae5\u548c\u9752\u5c11\u5e74\u3002\u59da\u660e\u7684\u6210\u529f\u4e0d\u4ec5\u4ec5\u5728\u4e8e\u4ed6\u7684\u7bee\u7403\u5929\u8d4b\u548c\u5546\u4e1a\u5934\u8111\uff0c\u66f4\u91cd\u8981\u7684\u662f\u4ed6\u5bf9\u793e\n\u4f1a\u7684\u8d23\u4efb\u611f\u548c\u5bf9\u516c\u76ca\u4e8b\u4e1a\u7684\u70ed\u7231\u3002\u4ed6\u7684\u52aa\u529b\u548c\u8d21\u732e\u4e3a\u4e2d\u56fd\u7684\u4f53\u80b2\u4e8b\u4e1a\u548c\u793e\u4f1a\u53d1\u5c55\u505a\n\u51fa\u4e86\u91cd\u8981\u8d21\u732e\uff0c\u4e5f\u8d62\u5f97\u4e86\u5e7f\u6cdb\u7684\u8d5e\u8a89\u548c\u5c0a\u91cd\u3002\n(Yao Ming is a basketball legend in China, not only for his impressive performance on the\ncourt but also for his exceptional business acumen and philanthropic spirit. After retiring\nfrom professional basketball, Yao founded the NBA China Company and serves as its\nchairman. Moreover, he has been actively involved in various charitable causes, establishing\nthe Yao Foundation to help underprivileged children and youth in China. Yao\u2019s success\nextends beyond his basketball skills and business success, as he is deeply committed to\nsocial responsibility and passionate about philanthropy. His efforts and contributions have\nmade a significant impact on China\u2019s sports industry and social development, earning him\nwidespread praise and respect.)\nTeleChat with NEFTUNE: \u59da\u660e\u662f\u4e2d\u56fd\u8457\u540d\u7684\u7bee\u7403\u8fd0\u52a8\u5458\uff0c\u4e5f\u662f\u4e2d\u56fd\u7bee\u534f\u4e3b\u5e2d\u3002\u4ed6\n\u5728\u804c\u4e1a\u751f\u6daf\u4e2d\u53d6\u5f97\u4e86\u8bb8\u591a\u8f89\u714c\u7684\u6210\u5c31\uff0c\u88ab\u8a89\u4e3a\u4e2d\u56fd\u7bee\u7403\u7684\u4ee3\u8868\u4eba\u7269\u4e4b\u4e00\u3002\u9996\u5148\uff0c\n\u59da\u660e\u7684\u8eab\u9ad8\u548c\u81c2\u5c55\u4f7f\u4ed6\u6210\u4e3a\u4e00\u540d\u51fa\u8272\u7684\u7bee\u7403\u8fd0\u52a8\u5458\u3002\u4ed6\u7684\u8eab\u9ad82.26\u7c73\uff0c\u81c2\u5c55\u8fbe\u5230\n\u4e862.30\u7c73\uff0c\u8fd9\u4f7f\u5f97\u4ed6\u5177\u5907\u4e86\u51fa\u8272\u7684\u7bee\u677f\u7403\u80fd\u529b\u548c\u76d6\u5e3d\u80fd\u529b\u3002\u5176\u6b21\uff0c\u59da\u660e\u5728\u7bee\u7403\u573a\u4e0a\n\u7684\u9886\u5bfc\u529b\u548c\u5f71\u54cd\u529b\u4e5f\u662f\u4ed6\u7684\u4e00\u5927\u4eae\u70b9\u3002\u4f5c\u4e3a\u4e2d\u56fd\u7bee\u534f\u4e3b\u5e2d\uff0c\u4ed6\u5728\u63a8\u52a8\u4e2d\u56fd\u7bee\u7403\u4e8b\u4e1a\n\u7684\u53d1\u5c55\u65b9\u9762\u8d77\u5230\u4e86\u91cd\u8981\u7684\u4f5c\u7528\u3002\u4ed6\u79ef\u6781\u5021\u5bfc\u9752\u5c11\u5e74\u7bee\u7403\u8fd0\u52a8\uff0c\u5e76\u7ec4\u7ec7\u4e86\u8bb8\u591a\u7bee\u7403\u8bad\n\u7ec3\u8425\u548c\u6bd4\u8d5b\uff0c\u4e3a\u5e74\u8f7b\u7403\u5458\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u6210\u957f\u73af\u5883\u548c\u53d1\u5c55\u673a\u4f1a\u3002\u6b64\u5916\uff0c\u59da\u660e\u8fd8\u79ef\u6781\u53c2\n\u4e0e\u516c\u76ca\u4e8b\u4e1a\u3002\u4ed6\u6210\u7acb\u4e86\u59da\u57fa\u91d1\uff0c\u81f4\u529b\u4e8e\u5e2e\u52a9\u8d2b\u56f0\u5730\u533a\u7684\u513f\u7ae5\u63a5\u53d7\u66f4\u597d\u7684\u6559\u80b2\u3002\u603b\u7684\n\u6765\u8bf4\uff0c\u59da\u660e\u662f\u4e00\u4f4d\u975e\u5e38\u4f18\u79c0\u7684\u7bee\u7403\u8fd0\u52a8\u5458\uff0c\u4ed6\u5728\u804c\u4e1a\u751f\u6daf\u4e2d\u53d6\u5f97\u4e86\u8bb8\u591a\u4ee4\u4eba\u77a9\u76ee\u7684\n\u6210\u5c31\uff0c\u540c\u65f6\u4e5f\u4e3a\u793e\u4f1a\u505a\u51fa\u4e86\u79ef\u6781\u7684\u8d21\u732e\u3002\n(Yao Ming is a renowned Chinese basketball player and the chairman of the Chinese Bas-\nketball Association. He has achieved numerous remarkable accomplishments throughout\nhis career, and is regarded as one of the representative figures of Chinese basketball. Firstly,\nYao Ming\u2019s height and arm span make him an exceptional basketball player. His height of\n2.26 meters and arm span of 2.30 meters enable him to possess excellent rebounding and\nblocking abilities. Secondly, Yao Ming\u2019s leadership and influence on the basketball court are\nalso outstanding. As the chairman of the Chinese Basketball Association, he has played a\ncrucial role in promoting the development of Chinese basketball. He has actively advocated\nfor youth basketball, organized various training camps and competitions, and provided young\nplayers with excellent growth and development opportunities. Moreover, Yao Ming has also\nbeen actively involved in charitable causes. He established the Yao Foundation, dedicated to\n26\nhelping children in poverty-stricken areas receive better education. In summary, Yao Ming is\nan extremely talented basketball player, who has achieved many remarkable accomplishments\nthroughout his career, while also making positive contributions to society.)\nInstruction: \u5199\u4e00\u7bc7\u82f1\u6587\u6563\u6587\u8bd7\uff0c\u4e3b\u9898\u662f\u6625\u96e8\uff0c\u60f3\u8c61\u81ea\u5df1\u662f\u6625\u96e8\uff0c\u548c\u82f1\u56fd\u53e4\u4ee3\u8bd7\u4eba\u838e\n\u58eb\u6bd4\u4e9a\u4ea4\u6d41\u3002(Write an English prose poem, the theme is spring rain, imagine yourself as\nspring rain, and communicate with the ancient British poet Shakespeare.)\nTeleChat w/o NEFTUNE:\nIn the spring rain, I am a poet,\nAs I wander through the fields and forests.\nI sing of the beauty of nature,\nAnd the joy that comes with it.\nThe raindrops fall softly on my face,\nAs I listen to the gentle sound of the water.\nI feel the power of nature,\nAnd the beauty that surrounds me.\nThe rain is a reminder of life\u2019s beauty,\nAnd the power of nature to heal and bless.\nIt is a time for reflection and prayer,\nFor a connection with the world around us.\nSo let the rain fall on me,\nAnd let me be a poet in the rain.\nLet the rain wash away all my fears,\nAnd let me find peace in the beauty of nature.\nTeleChat with NEFTUNE:\nSpring rain, gentle and pure,\nDriving the world to a standstill.\nThe world is in chaos, it\u2019s true,\nBut with spring rain, we can find our peace.\nThe air is crisp and cool,\nAs the sun begins to rise.\nThe trees are covered in green,\nAnd the flowers bloom like a dream.\nThe sound of raindrops on the ground,\nIs like music to my ears.\nIt\u2019s a reminder of life\u2019s beauty,\nAnd the power of nature\u2019s grace.\nI walk through the fields and forests,\nWith the sound of the rain in my heart.\nIt\u2019s a reminder that life is full of wonder,\nAnd that we should cherish every moment.\nSo let us embrace the spring rain,\nAnd let it wash away all our pain.\nFor in its presence, we can find our light,\nAnd let our hearts be filled with love.\nSpring rain, oh spring rain,\n27\nYou bring life to our world again.\nMay you continue to guide us,\nAnd keep us safe and warm.\n28\n"
  },
  {
    "title": "AGG: Amortized Generative 3D Gaussians for Single Image to 3D",
    "link": "https://arxiv.org/pdf/2401.04099.pdf",
    "upvote": "5",
    "text": "AGG: Amortized Generative 3D Gaussians for Single Image to 3D\nDejia Xu1, Ye Yuan2, Morteza Mardani2, Sifei Liu2 , Jiaming Song2, Zhangyang Wang1, Arash Vahdat2\n1University of Texas at Austin\n2NVIDIA\nAbstract\nGiven the growing need for automatic 3D content cre-\nation pipelines, various 3D representations have been stud-\nied to generate 3D objects from a single image. Due to\nits superior rendering efficiency, 3D Gaussian splatting-\nbased models have recently excelled in both 3D reconstruc-\ntion and generation. 3D Gaussian splatting approaches for\nimage to 3D generation are often optimization-based, re-\nquiring many computationally expensive score-distillation\nsteps.\nTo overcome these challenges, we introduce an\nAmortized Generative 3D Gaussian framework (AGG) that\ninstantly produces 3D Gaussians from a single image, elim-\ninating the need for per-instance optimization. Utilizing an\nintermediate hybrid representation, AGG decomposes the\ngeneration of 3D Gaussian locations and other appearance\nattributes for joint optimization. Moreover, we propose a\ncascaded pipeline that first generates a coarse representa-\ntion of the 3D data and later upsamples it with a 3D Gaus-\nsian super-resolution module.\nOur method is evaluated\nagainst existing optimization-based 3D Gaussian frame-\nworks and sampling-based pipelines utilizing other 3D rep-\nresentations, where AGG showcases competitive generation\nabilities both qualitatively and quantitatively while being\nseveral orders of magnitude faster. Project page: https:\n//ir1d.github.io/AGG/\n1. Introduction\nThe rapid development in virtual and augmented reality in-\ntroduces increasing demand for automatic 3D content cre-\nation.\nPrevious workflows often require tedious manual\nlabor by human experts and need specific software tools.\nA 3D asset generative model is essential to allow non-\nprofessional users to realize their ideas into actual 3D digital\ncontent.\nMuch effort has been put into image-to-3D generation as\nit allows users to control the generated content. Early meth-\nods study the generation of simple objects [60] or novel\nview synthesis of limited viewpoints [21, 42, 51]. Later\non, with the help of 2D text-to-image diffusion models,\nscore distillation sampling [35] has enabled 360-degree ob-\nCoarse \nHybrid \nGenerator\nGaussian\nSuper \nResolution\nFigure 1. Overview of our AGG framework. We design a novel\ncascaded generation pipeline that produces 3D Gaussian-based ob-\njects without per-instance optimization. Our AGG framework in-\nvolves a coarse generator that predicts a hybrid representation for\n3D Gaussians at a low resolution and a super-resolution module\nthat delivers dense 3D Gaussians in the fine stage.\nject generation for in-the-wild instances [29, 45, 52]. More\nrecently, the advances in collecting 3D datasets [10, 50, 61]\nhave supported the training of large-scale 3D-aware genera-\ntive models [15, 23, 24] for better image-to-3D generations.\nVarious 3D representations have been studied as the me-\ndia to train 3D generative models. Many prior works have\nexplored generating explicit representations, such as point\nclouds [62], voxels [39, 64], and occupancy grid [49]. Im-\nplicit representations like NeRF [30] and distance func-\ntions [22] are popular for easy optimization. Although ef-\nfective for content generation [12, 34, 57], slow rendering\nand optimization hinder the wide adoption of these repre-\nsentations. As a result, researchers have looked into more\nefficient volumetric representations [5, 31] to support the\ntraining of larger generative models [7, 19] and the costly\noptimization in score distillation [29, 52]. More recently,\n3D Gaussian splatting [8, 20, 44, 59] has attracted great at-\ntention due to its high-quality real-time rendering ability.\nThis owes to the visibility-aware rendering algorithm that\nfacilitates much less optimization time cost compared with\nNeRF-based variants [24, 35]. Such an efficient renderer\nenables supervising the 3D Gaussians through loss func-\ntions defined on the 2D renderings, which is not suitable\nfor other representations.\nDespite the exciting progress, how to properly generate\n3D Gaussians [20] remains a less studied topic. Existing\nworks [8, 44, 59] focus on the per-instance optimization-\nbased setting. Though 3D Gaussians can be optimized with\nadaptive density control to represent certain geometry, ini-\ntialization remains critical for complex object structures.\narXiv:2401.04099v1  [cs.CV]  8 Jan 2024\nAn amortized pipeline is highly desired to produce the 3D\nGaussians in one shot. Such a network can learn a shared\n3D understanding of images that generalizes to unseen ob-\njects of similar categories to the training set. This will fur-\nther reduce the need for test-time optimization, trading off\nthe computation cost of the inference stage with the training\nstage.\nHowever, building such a feed-forward pipeline is chal-\nlenging for two major reasons. First, 3D Gaussians rely\non adaptive density control [20] to represent complex ge-\nometry, but this leads to a dynamic number of 3D Gaus-\nsians, making it hard to predict them in an amortized train-\ning setting. Second, the 3D Gaussians require curated ini-\ntialization [8, 59] to be updated properly via supervision on\nthe rendering. In the amortized setting, the 3D Gaussians\nare instead predicted by the neural network, which leads to\nthe need to initialize the network well such that generated\nGaussians can be supervised decently. Moreover, the op-\ntimization process often favors updating the appearance of\n3D Gaussians instead of moving their positions directly to\ndesired 3D locations.\nTo overcome these issues, we conduct a pilot study on\nthe amortized generation of 3D Gaussians through a feed-\nforward process. As shown in Fig. 1, we propose AGG,\na cascaded generation framework that generates 3D Gaus-\nsians from a single image input. In the first stage, we em-\nploy a hybrid generator that produces 3D Gaussians at a\ncoarse resolution. In this stage, we decompose the geom-\netry and texture generation task into two distinct networks.\nA geometry transformer decodes image features extracted\nfrom a pre-trained image feature extractor and predicts the\nlocation of 3D Gaussians. Another texture transformer sim-\nilarly generates a texture field that is later queried by the\nGaussian locations to obtain other point attributes. Utilizing\nthis hybrid representation as an intermediate optimization\ntarget stabilizes our training process when jointly optimiz-\ning the geometry and texture of the 3D Gaussians. In the\nsecond stage, we leverage point-voxel convolutional net-\nworks to extract local features effectively and super-resolve\nthe coarse 3D Gaussians from the previous stage. RGB in-\nformation is further injected into the super-resolution net-\nworks to refine the texture information.\nOur contributions can be summarized as follows,\n\u2022 We conduct a pilot study on the task of amortized single\nimage-to-3D Gaussian setting. Unlike existing works that\noperate on individual objects, we build a novel cascaded\ngeneration framework that instantly presents 3D Gaus-\nsians in one shot.\n\u2022 Our AGG network first generates coarse Gaussian pre-\ndictions through a hybrid representation that decomposes\ngeometry and texture. With two separate transformers\npredicting the geometry and texture information, the 3D\nGaussian attributes can be optimized jointly and stably.\n\u2022 A UNet-based architecture with point-voxel layers is in-\ntroduced for the second stage, which effectively super-\nresolves the 3D Gaussians.\n\u2022 Compared\nwith\nexisting\nbaselines\nincluding\noptimization-based 3D Gaussian pipelines and sampling-\nbased frameworks that use other 3D representations,\nAGG demonstrates competitive performance both quan-\ntitatively and qualitatively while enabling zero-shot\nimage-to-object generation, and being several orders of\nmagnitude faster.\n2. Related Works\n2.1. Image-to-3D Generation\nNumerous research studies have been conducted on gen-\nerating 3D data from a single image. Early attempts sim-\nplify the challenges by either generating objects of sim-\nple geometry [60] or focusing on synthesizing novel view\nimages of limited viewpoints [21, 42, 51]. Later on, var-\nious combinations of 2D image encoder and 3D represen-\ntations are adopted to build 3D generative models, such as\non 3D voxels [9, 16, 54], point clouds [13, 22, 49, 55, 62],\nmeshes [15, 18, 48], and implicit functions [12, 56]. More\nrecently, with the help of score distillation sampling from a\npre-trained text-to-image diffusion model, great efforts have\nbeen put into generating a 3D asset from a single image\nthrough optimization [11, 29, 40, 44, 45, 52]. Among them,\nDreamGaussian utilizes 3D Gaussians as an efficient 3D\nrepresentation that supports real-time high-resolution ren-\ndering via rasterization. In addition to these optimization-\nbased methods, building feed-forward models for image-\nto-3D generation avoids the time-consuming optimization\nprocess, and a large generative model can learn unified rep-\nresentation for similar objects, leveraging prior knowledge\nfrom the 3D dataset better. The advances in large-scale 3D\ndatasets [10, 50, 61] largely contributed to the design of bet-\nter image-to-3D generative models [15, 23, 24]. OpenAI\nhas trained a 3D point cloud generation model Point-E [32],\nbased on millions of internal 3D models. They later pub-\nlished Shap-E [17], which is trained on more 3D models and\nfurther supports generating textured meshes and neural radi-\nance fields. Our work follows the direction of building feed-\nforward models for image-to-3D and makes the first attempt\nto construct an amortized model that predicts the 3D Gaus-\nsians instead of constructing them through optimization.\n2.2. Implicit 3D Representations\nNeural radiance field (NeRF) [30] implements a coordinate-\nbased neural network to represent the 3D scenes and\ndemonstrates outstanding novel view synthesis abilities.\nMany following works have attempted to improve NeRF in\nvarious aspects. For example, MipNeRF [1] and MipNeRF-\n360 [2] introduce advanced rendering techniques to avoid\naliasing artifacts.\nSinNeRF [51], PixelNeRF [60], and\nSparseNeRF [47] extends the application of NeRFs to few-\nshot input views, making single image-to-3D generation\nthrough NeRF a more feasible direction. With the recent\nadvances in score distillation sampling [35], numerous ap-\nproaches have looked into optimizing a neural radiance field\nthrough guidance from diffusion priors [11, 29, 40, 45, 52].\nWhile suitable for optimization, NeRFs are usually ex-\npressed implicitly through MLP parameters, which makes\nit challenging to generate them via network [12]. Conse-\nquently, many works [4, 5, 28] instead choose to generate\nhybrid representations, where NeRF MLP is adopted to de-\ncode features stored in explicit structures [3, 5, 6, 14, 31,\n53, 58]. Among them, ATT3D [28] builds an amortized\nframework for text-to-3D by generating Instant NGP [31]\nvia score distillation sampling. While focusing on generat-\ning explicit 3D Gaussians, our work draws inspiration from\nhybrid representations and utilizes hybrid structures as in-\ntermediate generation targets, which can be later decoded\ninto 3D Gaussians.\n2.3. Explicit 3D Representations\nExplicit 3D representations have been widely studied for\ndecades. Many works have attempted to construct 3D assets\nthrough 3D voxels [9, 16, 54], point clouds [13, 22, 49, 55,\n62] and meshes [15, 18, 48]. Since pure implicit radiance\nfields are operationally slow, often needing millions of neu-\nral network queries to render large-scale scenes, great ef-\nforts have been put into integrating explicit representations\nwith implicit radiance fields to combine their advantages.\nMany works looked into employing tensor factorization to\nachieve efficient explicit representations [5, 6, 14, 31, 58].\nMulti-scale hash grids [31] and block decomposition [43]\nare introduced to extend to city-scale scenes. Another pop-\nular direction focuses on empowering explicit structures\nwith additional implicit attributes. Point NeRF [53] utilizes\nneural 3D points to represent and render a continuous\nradiance volume.\nSimilarly, NU-MCC [22] uses latent\npoint features, specifically focusing on shape completion\ntasks. 3D Gaussian splatting [20] introduces point-based\n\u03b1-blending along with an efficient point-based rasterizer\nand has attracted great attention due to their outstanding\nability in reconstruction [20] and generation [8, 44, 59]\ntasks.\nHowever, existing works present 3D Gaussians\nthrough optimization, while our work takes one step\nfurther and focuses on building a amortized framework that\ngenerates 3D Gaussians without per-instance optimization.\n3. Amortized Generative 3D Gaussians\nUnlike the existing methods that optimize Gaussians,\ninitialized from structure-from-motion (SfM) or random\nblobs, our work pursues a more challenging objective: gen-\nerating 3D Gaussians from a single image using a neural\nnetwork in one shot. Specifically, our cascaded approach\nbegins with constructing a hybrid generator that maps input\nimage features into a concise set of 3D Gaussian attributes.\nSubsequently, a UNet architecture with point-voxel layers\nis employed to super-resolve the 3D Gaussian representa-\ntion, improving its fidelity. In the following subsections,\nwe first provide the background on 3D Gaussian splatting,\nthen we delve deep into details about our proposed compo-\nnents, and finally, we illustrate how we address the rising\nchallenges with amortized generation.\n3.1. Background: 3D Gaussian Splatting (3DGS)\n3D Gaussian splatting is a recently popularized explicit 3D\nrepresentation that utilizes anisotropic 3D Gaussians. Each\n3D Gaussian G is defined with a center position \u00b5 \u2208 R3,\ncovariance matrix \u03a3, color information c \u2208 R3 and opacity\n\u03b1 \u2208 R1. The covariance matrix \u03a3 describes the configura-\ntion of an ellipsoid and is implemented via a scaling matrix\nS \u2208 R3 and a rotation matrix R \u2208 R3\u00d73.\n\u03a3 = RSST RT\n(1)\nThis factorization allows independent optimization of the\nGaussians\u2019 attributes while maintaining the semi-definite\nproperty of the covariance matrix. In summary, each Gaus-\nsian centered at point (mean) \u00b5 is defined as follows:\nG(x) = e\u2212 1\n2 xT \u03a3\u22121x,\n(2)\nwhere x refers to the distance between \u00b5 and the query\npoint. G(x) is multiplied by \u03b1 in the blending process to\nconstruct the final accumulated color:\nC =\nX\ni\u2208N\nci\u03b1iG(xi)\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1jG(xj)),\n(3)\nAn efficient tile-based rasterizer enables fast forward and\nbackward pass that facilitates real-time rendering. The op-\ntimization method of 3D Gaussian properties is interleaved\nwith adaptive density control of Gaussians, namely, densi-\nfying and pruning operations, where Gaussians are added\nand occasionally removed. In our work, we follow the con-\nvention in 3D Gaussian splatting to establish the Gaussian\nattributes but identify specific canonicalization designed for\nsingle image-to-3D task. We first reduce the degree of the\nspherical harmonic coefficients and allow only diffuse col-\nors for the 3D Gaussians. Additionally, we set canonical\nisotropic scale and rotation for the 3D Gaussians, as we find\nthese attributes extremely unstable during the amortized op-\ntimization process.\n3.2. Coarse Hybrid Generator\nIn the coarse stage, our AGG framework utilizes a hybrid\ngenerator to produce a hybrid representation as an interme-\ndiate generation target. Initially, the input image is encoded\nDINOv2\n\u2026\nGeometry \nPredictor\nTexture \nGenerator\n\u2026\n\u2026\n+\n+\nreshape\n\u2026\nQueries\nQueries\n!!\"#$\"!%#&\nLocation\nOpacity, Color\n!'()*+\"!\nFigure 2. Architecture of our coarse hybrid generator. We first use a pre-trained DINOv2 image encoder to extract essential features and\nthen adopt two transformers that individually map learnable query tokens to Gaussian locations and a texture field. The texture field accepts\nlocation queries from the geometry branch, and a decoding MLP further converts the interpolated plane features into Gaussian attributes.\nusing a vision transformer. Subsequently, a geometry and a\ntexture generator are constructed to map learnable queries\ninto location sequences and texture fields individually. This\nhybrid representation is then decoded into explicit 3D Gaus-\nsians, which enables efficient high-resolution rendering and\nfacilitates supervision via multi-view images. The overall\narchitecture of our proposed coarse hybrid generator is il-\nlustrated in Fig. 2. Our model is trained via rendering loss\ndefined on multi-view images and is warmed up with Cham-\nfer distance loss using 3D Gaussian pseudo labels.\nEncoding Input RGB Information\nSingle image-to-3D\nis a highly ill-posed problem since multiple 3D objects can\nalign with the single-view projection. As a result, an ef-\nfective image encoder is greatly needed to extract essential\n3D information. Previous work [24, 25] mainly incorporate\nCLIP image encoder [37], benefiting from the foundational\ntraining of Stable diffusion [38]. While effective for text-\nto-image generation, CLIP encoder is not well-designed for\nidentifying feature correspondences, which is crucial for 3D\nvision tasks. As a result, we use the pre-trained DINOv2\ntransformer as our image encoder, which has demonstrated\nrobust feature extraction capability through self-supervised\npre-training. Unlike previous works [46, 51] that only use\nthe aggregated global [CLS] token , we choose to incorpo-\nrate patch-wise features as well.\nGeometry Predictor\nThe location information of 3D\nGaussian is expressed via their 3D means following\n3DGS [20], which is a three-dimensional vector for each\npoint.\nThus, we adopt a transformer-based network for\npredicting the location sequence. The transformer inputs\nare a set of learnable queries implemented with a group\nof learnable position embeddings. Each query will corre-\nspond to one 3D Gaussian that we generate. Before feeding\ninto the transformer network, the positional embeddings are\nsummed with the global token [CLS] extracted from the\nDINOv2 model. The sequence of queries is then modulated\nprogressively by a series of transformer blocks, each con-\ntaining a cross-attention block, a self-attention block, and a\nmulti-layer perception block. These components are inter-\nleaved with LayerNorm and GeLU activations. The cross-\nattention blocks accept DINOv2 features as context infor-\nmation. The final layer of our geometry predictor involves\nan MLP decoding head, converting the hidden features gen-\nerated by attention modules into a three-dimensional loca-\ntion vector.\nTexture Field Generator\nJoint prediction of texture and\ngeometry presents significant challenges. A primary chal-\nlenge is the lack of direct ground truth supervision for tex-\nture in 3D space. Instead, texture information is inferred\nthrough rendering losses in 2D. When geometry and texture\ninformation are decoded from a shared network, their gen-\neration becomes inevitably intertwined. Consequently, up-\ndating the predicted location alters the supervision for ren-\ndered texture, leading to divergent optimization directions\nfor texture prediction.\nTo resolve this, a distinct transformer is employed to\ngenerate a texture field. Our texture field is implemented us-\ning a triplane [5], complemented by a shared decoding MLP\nhead. The triplane accepts 3D location queries from the\ngeometry branch and concatenates interpolated features for\nfurther processing. Utilization of this texture field facilitates\ndecomposed optimization of geometry and texture informa-\ntion. More importantly, texture information is now stored\nin structured planes. Incorrect geometry predictions do not\nimpact the texture branch\u2019s optimization process, which re-\nceives accurate supervision when geometry queries approx-\nimate ground truth locations.\nSupervision Through 2D Renderings\nThanks to the ef-\nficient rasterizer for 3D Gaussians, we can apply supervi-\nsion on novel view renderings during the training. The 3D\nGaussians we generate are rendered from randomly selected\nnovel views, and image-space loss functions are calculated\nagainst ground truth renderings available from the dataset.\nConcretely, we render the scene into RGB images and cor-\nresponding foreground alpha masks. We utilize LPIPS [63]\nand L1 loss to minimize their differences,\nLrendering = Lrgba + \u03c91Llpips,\n(4)\nwhere \u03c91 is a weighting factor.\n3.3. Gaussian Super Resolution\nAlthough our hybrid generator is effective in the coarse gen-\neration stage, generating high-resolution 3D Gaussians re-\nquires many learnable queries which are computationally\nexpensive due to the quadratic cost of self-attention lay-\ners. As a result, we instead utilize a second-stage network\nas a super-resolution module to introduce more 3D Gaus-\nsians into our generation. Since coarse geometry is obtained\nfrom the first stage, the super-resolution network can fo-\ncus more on refining local details. For simplicity, we use a\nlightweight UNet architecture with the efficient point-voxel\nlayers [26], as shown in Fig. 3.\nLatent Space Super-Resolution\nAs mentioned earlier,\nthe 3D Gaussians require curated locations to update the\nother attributes properly. This leads to the key challenge for\nthe super-resolution stage, which is to introduce more point\nlocations for the super-resolved 3D Gaussians. Inspired by\nprevious works in 2D image [41] and point cloud super res-\nolution [36], we perform feature expanding as a surrogate of\ndirectly expanding the number of points. Through a point-\nvoxel convolutional encoder, we first convert the stage one\ncoarse 3D Gaussians into compact latent features that can\nbe safely expanded through a feature expansion operation:\nrearranging a tensor of shape (B, N, C \u00d7 r) to a ten-\nsor of shape (B, N \u00d7 r, C). The expanded features are\nthen decoded through a point-voxel convolutional decoder\nthat predicts the Gaussian locations and other attributes.\nIncorporating RGB information\nWhile the first-stage\nnetwork may capture the rough geometry of objects, the tex-\nture field may converge into blurry results due to the oscil-\nlating point locations and rough geometry. Consequently,\nutilizing the abundant texture information from the input\nimage is crucial for the super-resolution network to generate\nplausible details. For this purpose, we introduce RGB fea-\ntures into the bottleneck of the UNet architecture. Specif-\nically, we adopt cross-attention layers before and after the\nfeature expansion operation. Image features are fed through\ncross-attention layers to modulate the latent point-voxel fea-\ntures.\n3.4. Overcoming Challenges in Amortized Training\nUnlike the vanilla 3D Gaussian splatting setting, where a set\nof 3D Gaussians are optimized specifically for each object,\nour amortized framework involves training a generator that\njointly produces the 3D Gaussians for a large set of objects\ncoming from diverse categories. As a result, many specif-\nically designed operations for manipulating 3D Gaussians\nare not available in our setting. Below, we illustrate how we\novercome these challenges in detail.\n3.4.1\nAdaptive Density Control\nThe original 3D Gaussians involve special density con-\ntrol operations to move them toward their desired 3D lo-\ncations. However, in the amortized training setting, it is\nnon-straightforward to adaptively clone, split, or prune the\nGaussians according to the gradients they receive. This is\nbecause these operations change the number of points, mak-\ning it hard for an amortized framework to process.\nTo this end, we use a fixed number of 3D Gaussians for\neach object, avoiding the burden for the generator network\nto determine the number of points needed. Moreover, since\nwe do not have access to clone and split operations, training\nthe amortized generator leads to sharp 3D Gaussians with\nunpleasant colors that are trying to mimic fine-grained\ntexture details. Once the predictions become sharp, they\nget stuck in the local minimum, and therefore, fewer 3D\nGaussians are available to represent the overall object,\nleading to blurry or corrupted generations. To overcome\nthe issue, we empirically set canonical isotropic scales and\nrotations for the 3D Gaussians on all objects to stabilize the\ntraining process.\n3.4.2\nInitialization\nProper initialization plays an important role in the original\noptimization-based 3D Gaussian splatting. Though Gaus-\nsian locations are jointly optimized with other attributes,\nit is observed that optimization updates often prefer reduc-\ning the opacity and scale instead of moving the Gaussians\ndirectly. By doing so, Gaussians are eliminated from the\nwrong location after reaching the pruning threshold. In-\nstead, the adaptive density control can then clone or split\nDINOv2\nPVCNN\nPVCNN\nCross \nAttn\nFeature\nExpansion\nCross \nAttn\n!!\"#$\"!%#&\nFigure 3. Illustration of the second-stage Gaussian super-resolution network. We first encode the original input image and the stage one\nprediction separately. Then, we unite them through cross-attention at the latent space. We perform super-resolution in the latent space and\ndecode the features jointly.\nGaussians at proper locations according to their gradients.\nAlbeit the simplicity of pruning and cloning operations for\noptimization algorithms, they are non-trivial to implement\nwith amortized neural networks.\nTo overcome the issues, we warm up the generator with\n3D Gaussian pseudo labels. We use a few 3D objects and\nperform multi-view reconstruction to obtain 3D Gaussians\nfor each object. Since multiple sets of 3D Gaussians can\nall be plausible for reconstructing the same 3D ground\ntruth object, the 3D Gaussians are only considered pseudo\nlabels where we use their attributes to initialize our network\nlayers properly.\nDue to these Gaussians being stored in random orders as\nsets, we cannot use L1 reconstruction loss as in generative\nframeworks such as LION [62].\nAs an alternative, we utilize the Chamfer distance loss\nto pre-train our network. Specifically, we first calculate the\npoint matching correspondences according to the distance\nof Gaussians\u2019 3D means and then minimize the L1 differ-\nence of each attribute, including location, opacity, and color.\nThe formulation can be summarized as follows,\nLchamfer(f) = w1\n|P1|\nX\np1i\u2208P1,\np2j=arg min\nx\u2208P2 (\u2225p1i\u2212x\u22252\n2)\n(||f1i \u2212 f2j||)\n+ w2\n|P2|\nX\np2j\u2208P2,\np1i=arg min\nx\u2208P1 (\u2225x\u2212p2j\u22252\n2)\n(||f2j \u2212 f1i||),\nwhere f refers to 3D Gaussian attributes, P1 is our gener-\nated set of 3D Gaussian, and P2 is the 3D Gaussian pseudo\nlabel.\n4. Experiments\n4.1. Implementation Details\nGaussian Configuration\nWe set the canonical isotropic\nscale of each 3D Gaussian to 0.03 and 0.01 when 4,096\nand 16,384 3D Gaussians are produced for each object,\nrespectively. The canonical rotation for all 3D Gaussians\nis set to [1, 0, 0, 0] for the rasterizer. When preparing the\npseudo labels for the warmup training, we use the ground\ntruth point clouds from the OmniObject3D [50] dataset to\ninitialize the Gaussian locations.\nNetwork Architecture\nWe implement our transformer\nblocks following the well-established practice in DI-\nNOv2 [33] with GeLU activation and Layer Normalization\nlayers. The image features are extracted using a pre-trained\nDINOv2-base model at 256 \u00d7 256 input resolution.\nObject Placement and Camera System\nWe reconstruct\nobjects in camera space and normalize the input view az-\nimuth to zero. This means that the image and the 3D assets\nare aligned and consistent as both are rotated. This 3D aug-\nmentation during the training stage enforces rotation equiv-\nariance and prevents the network from merely overfitting\nthe canonical placements of objects in the dataset.\n4.2. Baseline Methods\nWe compare with two streams of work. One involves the\nsampling-based 3D generation that uses different 3D rep-\nresentations, such as Point-E and One-2345. Point-E [32]\nincludes a large diffusion transformer that generates a point\ncloud. Their work performs the generation in the world\ncoordinates, where objects are always canonically placed as\nthe original orientation in the dataset. One-2345 [23] lever-\nages Zero123 [24] and utilizes SparseNeuS [27] to fuse\ninformation from noisy multi-view generations efficiently.\nWe also compare with DreamGaussian [44]\u2019s first stage,\nInput\nPoint-E\nDream\nGaussian\nOne-2345\nAGG\nGT\nFigure 4. Novel view rendering comparisons against baseline methods. Our AGG model observes none of these testing images during\ntraining.\nTable 1. Comparisons against baseline methods regarding novel\nview image quality and inference speed.\nMethod\nPoint-E\nOne-2345\nDreamGaussian\nOurs\nCLIP Dist \u2193\n0.5139\n0.3084\n0.4293\n0.3458\nTime (s)\n78\n45\n60\n0.19\nwhere 3D Gaussians are optimized with SDS [35] from\nZero123 [24]. Both One-2345 and DreamGaussian require\nthe elevation of the input image at inference time.\nFor\nOne-2345, we use official scripts to estimate the elevation.\nWe provide DreamGaussian with ground truth elevation\nextracted from the camera pose in the testing dataset.\n4.3. Dataset\nOur model is trained on OmniObject3D dataset [50], which\ncontains high-quality scans of real-world objects.\nThe\nnumber of 3D objects is much fewer than Objaverse [10].\nAlongside point clouds, the OmniObject3D dataset pro-\nvides 2D renderings of objects obtained through Blender.\nThis allows us to implement rendering-based loss functions\nthat supervise the rendering quality in 2D space. We con-\nstruct our training set using 2,370 objects from 73 classes\nin total. We train one model using all classes. The test set\ncontains 146 objects, with two left-out objects per class.\n4.4. Qualitative and Quantitative Comparisons\nWe provide visual comparisons and quantitative analysis\nbetween our methods and existing baselines in Fig. 4 and\nTab. 1. As shown in the novel view synthesis results, our\nmodel learns reasonable geometry understanding and pro-\nduces plausible generations of texture colors. Point-E [32]\nproduces reasonable geometry but presents unrealistic col-\nors, possibly because the model was trained on large-scale\nsynthetic 3D data. Both One-2345 [23] and DreamGaus-\nsian [44] can generate corrupted shapes, which might be\nbecause the novel views synthesized from the Zero-123 dif-\nfusion model [24] are multi-view inconsistent.\nSince multiple 3D objects can be reasonable outputs in-\nferred from the single view input, we don\u2019t measure ex-\nact matches against the 3D ground truth. Instead, we use\nCLIP distance defined on image embeddings to reflect high-\nlevel image similarity on multi-view renderings, where our\nmethod obtains competitive numbers.\nAdditionally, our\nAGG network enjoys the fastest inference speed.\nIn comparison, both Point-E and One-2345 adopt an it-\nerative diffusion process when producing their final results,\nwhile DreamGaussian leverages a costly optimization pro-\ncess through score distillation sampling.\n4.5. Ablation Study\nWe conduct thorough experiments to validate the effective-\nness of our proposed components using the available 3D\nTable 2. Ablation Studies. We validate the effectiveness of our\nproposed components by comparing their predictions against the\n3D ground truth on the test set.\nModel\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nw/o Texture Field\n27.09\n0.85\n0.1910\nw/o Super Resolution\n27.59\n0.87\n0.1772\nFull Model\n28.54\n0.87\n0.1426\nInput\n(a)\n(b)\n(c)\nGT\nFigure 5. Visual comparisons of our full model (c) and its variants:\n(a) w/o Texture Field, (b) w/o Super Resolution.\nground truth from the dataset. We mainly compare with two\nvariants of the model. \u201cw/o Super Resolution\u201d means that\nwe only train the coarse hybrid generator without involving\nthe Gaussian super-resolution stage. \u201cw/o Texture Field\u201d\nrefers to further removing the texture field generator branch\nin the coarse hybrid generator and forcing the geometry pre-\ndictor to generate all attributes at the same time. As shown\nin Tab. 2 and Fig. 5, the full model delivers the best gen-\neration results when rendered from novel viewpoints. Re-\nsults from \u201cw/o Super Resolution\u201d are limited in the number\nof Gaussians and, therefore, suffer from low resolution and\nblurry results. The variant \u201cw/o Texture Field\u201d produces\ncorrupted geometry due to the extra burden for the geome-\ntry predictor to predict texture information at the same time.\n4.6. Limitation\nDespite the encouraging visual results produced by AGG,\nthe number of 3D Gaussians we generate is still limited to\nrepresent very complex geometry. In the future, we will\nfurther explore how to expand AGG to more challenging\nscenarios, such as when the input image contains multiple\nobjects with occlusion.\n5. Conclusion\nIn this work, we make the first attempt to develop an\namortized pipeline capable of generating 3D Gaus-\nsians from a single image input.\nThe proposed AGG\nframework\nutilizes\na\ncascaded\ngeneration\npipeline,\ncomprising a coarse hybrid generator and a Gaussian\nsuper-resolution model. Experimental results demonstrate\nthat our method achieves competitive performance with\nseveral orders of magnitude speedup in single-image-\nto-3D generation, compared to both optimization-based\n3D Gaussian frameworks and sampling-based 3D gen-\nerative models utilizing alternative 3D representations.\nReferences\n[1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5855\u20135864,\n2021. 2\n[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman.\nMip-nerf 360:\nUn-\nbounded anti-aliased neural radiance fields. arXiv preprint\narXiv:2111.12077, 2021. 2\n[3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-\nbased neural radiance fields. ICCV, 2023. 3\n[4] Ananta R Bhattarai, Matthias Nie\u00dfner, and Artem Sev-\nastopolsky.\nTriplanenet: An encoder for eg3d inversion.\narXiv preprint arXiv:2303.13497, 2023. 3\n[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 1, 3, 4\n[6] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision, pages 333\u2013350. Springer,\n2022. 3\n[7] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen\nTu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf: A\nunified approach to 3d generation and reconstruction. arXiv\npreprint arXiv:2304.06714, 2023. 1\n[8] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using\ngaussian splatting. arXiv preprint arXiv:2309.16585, 2023.\n1, 2, 3\n[9] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\nChen, and Silvio Savarese.\n3d-r2n2: A unified approach\nfor single and multi-view 3d object reconstruction. In Com-\nputer Vision\u2013ECCV 2016: 14th European Conference, Am-\nsterdam, The Netherlands, October 11-14, 2016, Proceed-\nings, Part VIII 14, pages 628\u2013644. Springer, 2016. 2, 3\n[10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13142\u201313153, 2023. 1, 2, 8\n[11] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,\nYin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.\nNerdi: Single-view nerf synthesis with language-guided dif-\nfusion as general image priors.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20637\u201320647, 2023. 2, 3\n[12] Ziya Erkoc\u00b8, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner,\nand Angela Dai.\nHyperdiffusion:\nGenerating implicit\nneural fields with weight-space diffusion.\narXiv preprint\narXiv:2303.17015, 2023. 1, 2, 3\n[13] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set\ngeneration network for 3d object reconstruction from a single\nimage. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 605\u2013613, 2017. 2, 3\n[14] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12479\u201312488, 2023. 3\n[15] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. Advances In Neural In-\nformation Processing Systems, 35:31841\u201331854, 2022. 1, 2,\n3\n[16] Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Ab-\nhinav Gupta.\nLearning a predictable and generative vec-\ntor representation for objects. In Computer Vision\u2013ECCV\n2016: 14th European Conference, Amsterdam, The Nether-\nlands, October 11-14, 2016, Proceedings, Part VI 14, pages\n484\u2013499. Springer, 2016. 2, 3\n[17] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 2\n[18] Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and\nJitendra Malik. Learning category-specific mesh reconstruc-\ntion from image collections.\nIn Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), pages 371\u2013\n386, 2018. 2, 3\n[19] Animesh Karnewar, Andrea Vedaldi, David Novotny, and\nNiloy J Mitra. Holodiffusion: Training a 3d diffusion model\nusing 2d images.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18423\u201318433, 2023. 1\n[20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(ToG), 42(4):1\u201314, 2023. 1, 2, 3, 4\n[21] Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu\nWang, and Gim Hee Lee. Mine: Towards continuous depth\nmpi with nerf for novel view synthesis. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 12578\u201312588, 2021. 1, 2\n[22] Stefan Lionar, Xiangyu Xu, Min Lin, and Gim Hee Lee. Nu-\nmcc: Multiview compressive coding with neighborhood de-\ncoder and repulsive udf. arXiv preprint arXiv:2307.09112,\n2023. 1, 2, 3\n[23] Minghua Liu,\nChao Xu,\nHaian Jin,\nLinghao Chen,\nMukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:\nAny single image to 3d mesh in 45 seconds without per-\nshape optimization. arXiv preprint arXiv:2306.16928, 2023.\n1, 2, 6, 8\n[24] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-\n3:\nZero-shot one image to 3d object.\narXiv preprint\narXiv:2303.11328, 2023. 1, 2, 4, 6, 8\n[25] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023. 4\n[26] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-\nvoxel cnn for efficient 3d deep learning. Advances in Neural\nInformation Processing Systems, 32, 2019. 5\n[27] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and\nWenping Wang. Sparseneus: Fast generalizable neural sur-\nface reconstruction from sparse views. In European Confer-\nence on Computer Vision, pages 210\u2013227. Springer, 2022.\n6\n[28] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan\nLin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin,\nMing-Yu Liu, Sanja Fidler, and James Lucas.\nAtt3d:\nAmortized text-to-3d object synthesis.\narXiv preprint\narXiv:2306.07349, 2023. 3\n[29] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8446\u20138455, 2023. 1, 2, 3\n[30] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In European conference on computer vision, pages\n405\u2013421. Springer, 2020. 1, 2\n[31] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding.\narXiv preprint arXiv:2201.05989,\n2022. 1, 3\n[32] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 2, 6, 8\n[33] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 6\n[34] Roy\nOr-El,\nXuan\nLuo,\nMengyi\nShan,\nEli\nShecht-\nman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.\nStylesdf: High-resolution 3d-consistent image and geome-\ntry generation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13503\u2013\n13513, 2022. 1\n[35] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 1, 3, 8\n[36] Guocheng Qian, Abdulellah Abualshour, Guohao Li, Ali\nThabet, and Bernard Ghanem. Pu-gcn: Point cloud upsam-\npling using graph convolutional networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11683\u201311692, 2021. 5\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. 4\n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 4\n[39] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao,\nand Andreas Geiger. Voxgraf: Fast 3d-aware image synthe-\nsis with sparse voxel grids. Advances in Neural Information\nProcessing Systems, 35:33999\u201334011, 2022. 1\n[40] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\nand Seungryong Kim.\nLet 2d diffusion model know 3d-\nconsistency for robust text-to-3d generation. arXiv preprint\narXiv:2303.07937, 2023. 2, 3\n[41] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00b4ar, Johannes Totz,\nAndrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan\nWang. Real-time single image and video super-resolution\nusing an efficient sub-pixel convolutional neural network. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1874\u20131883, 2016. 5\n[42] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin\nHuang. 3d photography using context-aware layered depth\ninpainting.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8028\u2013\n8038, 2020. 1, 2\n[43] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-\nhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,\nand Henrik Kretzschmar. Block-nerf: Scalable large scene\nneural view synthesis. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8248\u20138258, 2022. 3\n[44] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 1, 2, 3, 6, 8\n[45] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 1, 2, 3\n[46] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali\nDekel. Splicing vit features for semantic appearance transfer.\narXiv preprint arXiv:2201.00424, 2022. 4\n[47] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and\nZiwei Liu.\nSparsenerf: Distilling depth ranking for few-\nshot novel view synthesis. arXiv preprint arXiv:2303.16196,\n2023. 3\n[48] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei\nLiu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh\nmodels from single rgb images. In Proceedings of the Euro-\npean conference on computer vision (ECCV), pages 52\u201367,\n2018. 2, 3\n[49] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph\nFeichtenhofer, and Georgia Gkioxari. Multiview compres-\nsive coding for 3d reconstruction.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9065\u20139075, 2023. 1, 2, 3\n[50] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren,\nLiang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian,\net al. Omniobject3d: Large-vocabulary 3d object dataset for\nrealistic perception, reconstruction and generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 803\u2013814, 2023. 1, 2, 6, 8\n[51] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey\nShi, and Zhangyang Wang. Sinnerf: Training neural radiance\nfields on complex scenes from a single image. In European\nConference on Computer Vision, pages 736\u2013753. Springer,\n2022. 1, 2, 3, 4\n[52] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang.\nNeurallift-360: Lifting an in-the-\nwild 2d photo to a 3d object with 360 {\\deg} views. arXiv\npreprint arXiv:2211.16431, 2022. 1, 2, 3\n[53] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\nPoint-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5438\u20135448, 2022. 3\n[54] Farid Yagubbayli, Yida Wang, Alessio Tonioni, and Fed-\nerico Tombari.\nLegoformer:\nTransformers for block-\nby-block multi-view 3d reconstruction.\narXiv preprint\narXiv:2106.12102, 2021. 2, 3\n[55] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge\nBelongie, and Bharath Hariharan. Pointflow: 3d point cloud\ngeneration with continuous normalizing flows. In Proceed-\nings of the IEEE/CVF international conference on computer\nvision, pages 4541\u20134550, 2019. 2, 3\n[56] Guandao Yang, Serge Belongie, Bharath Hariharan, and\nVladlen Koltun.\nGeometry processing with neural fields.\nAdvances in Neural Information Processing Systems, 34:\n22483\u201322497, 2021. 2\n[57] Guandao\nYang,\nAbhijit\nKundu,\nLeonidas\nJ\nGuibas,\nJonathan T Barron, and Ben Poole. Learning a diffusion prior\nfor nerfs. arXiv preprint arXiv:2304.14473, 2023. 1\n[58] Brent Yi, Weijia Zeng, Sam Buchanan, and Yi Ma. Canon-\nical factors for hybrid neural fields. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 3414\u20133426, 2023. 3\n[59] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng\nZhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussian-\ndreamer: Fast generation from text to 3d gaussian splatting\nwith point cloud priors. arXiv preprint arXiv:2310.08529,\n2023. 1, 2, 3\n[60] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4578\u20134587, 2021. 1,\n2, 3\n[61] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu,\nChongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu,\nZhangyang Xiong, Tianyou Liang, et al.\nMvimgnet: A\nlarge-scale dataset of multi-view images. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9150\u20139161, 2023. 1, 2\n[62] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Goj-\ncic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: La-\ntent point diffusion models for 3d shape generation. arXiv\npreprint arXiv:2210.06978, 2022. 1, 2, 3, 6\n[63] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 5\n[64] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation\nand completion through point-voxel diffusion. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 5826\u20135835, 2021. 1\n"
  }
]