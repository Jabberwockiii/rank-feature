[
  {
    "title": "DocLLM: A layout-aware generative language model for multimodal document understanding",
    "link": "https://arxiv.org/pdf/2401.00908.pdf",
    "upvote": "166",
    "text": "DOCLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL\nFOR MULTIMODAL DOCUMENT UNDERSTANDING\nDongsheng Wang\u2217, Natraj Raman\u2217, Mathieu Sibue\u2217\nZhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\nJPMorgan AI Research\n{first.last}@jpmchase.com\nABSTRACT\nEnterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records,\noften carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered\nby their complex layouts play a crucial role in comprehending these documents effectively. In this\npaper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for\nreasoning over visual documents, taking into account both textual semantics and spatial layout. Our\nmodel differs from existing multimodal LLMs by avoiding expensive image encoders and focuses\nexclusively on bounding box information to incorporate the spatial layout structure. Specifically,\nthe cross-alignment between text and spatial modalities is captured by decomposing the attention\nmechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a\npre-training objective that learns to infill text segments. This approach allows us to address irregular\nlayouts and heterogeneous content frequently encountered in visual documents. The pre-trained\nmodel is fine-tuned using a large-scale instruction dataset, covering four core document intelligence\ntasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all\ntasks, and generalizes well to 4 out of 5 previously unseen datasets.\nKeywords DocAI \u00b7 VRDU \u00b7 LLM \u00b7 GPT \u00b7 Spatial Attention\n1\nIntroduction\nDocuments with rich layouts, including invoices, receipts, contracts, orders, and forms, constitute a significant portion\nof enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1],\nwhich has spurred the development of AI-driven solutions. These visually rich documents feature complex layouts,\nbespoke type-setting, and often exhibit variations in templates, formats and quality. Although Document AI (DocAI) has\nmade tremendous progress in various tasks including extraction, classification and question answering, there remains a\nsignificant performance gap in real-world applications. In particular, accuracy, reliability, contextual understanding and\ngeneralization to previously unseen domains continues to be a challenge [2].\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues being\ncritical to understanding the documents. It requires solutions distinct from conventional large language models such as\nGPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents\nexhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous\nvision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual\nand visual modalities are available. However, these frameworks necessitate the use of complex vision backbone\narchitectures [9] to encode image information, and they often make use of spatial information as an auxiliary contextual\nsignal [10, 11].\nIn this paper we present DocLLM, a light-weight extension to standard LLMs that excels in several visually rich form\nunderstanding tasks. Unlike traditional LLMs, it models both spatial layouts and text semantics, and therefore is\n*These authors contributed equally to this work.\narXiv:2401.00908v1  [cs.CL]  31 Dec 2023\nFigure 1: Key elements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention\nmechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text\nblocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions.\nintrinsically multi-modal. The spatial layout information is incorporated through bounding box coordinates of the\ntext tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder\ncomponent. Consequently, our solution preserves the causal decoder architecture, introduces only a marginal increase\nin the model size, and has reduced processing times, as it does not rely on a complex vision encoder. We demonstrate\nthat merely including the spatial layout structure is sufficient for various document intelligence tasks such as form\nunderstanding, table alignment and visual question answering.\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual\nembeddings [12] or summing the two [13]. In contrast, we treat the spatial information as a distinct modality and\ncompute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention\nmechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is\nmotivated by the observation that there is often a correlation between the content, position and size of the fields in a\nform. Representing their alignments at various abstraction levels across the transformer layers can enhance document\nunderstanding.\nA common characteristic of visual documents is their heterogeneous content, irregular layouts, and disjointed text\nsegments. When working with such documents, employing a classical next token prediction objective during the\nself-supervised pre-training phase can be restrictive. In particular, the preceding tokens may not always be relevant due\nto the diverse arrangements of text, which can be positioned horizontally, vertically, or even in a staggered manner.\nTo tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text\nthat account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both\npreceding and succeeding tokens. Due to these modifications, the model is better equipped to address misaligned text,\ncontextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been\nstudied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction\ndata curated from several datasets. These tasks encompass key information extraction, natural language inference,\nvisual question-answering and document classification. Our instruction-tuning data covers both single and multi-page\ndocuments. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to\nfacilitate learning the logical structure of the documents. We observe that the modifications introduced by DocLLM\nresult in a performance improvement ranging from 15% to 61% for the Llama2-7B model in four out of five previously\nunseen datasets.\nFig. 1 summarizes the framework. Our contributions include:\n1. A light-weight extension to LLMs designed for understanding visual documents.\n2. A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n3. An infilling pre-training objective tailored to address irregular layouts effectively.\n4. An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior.\n2\n2\nRelated Work\n2.1\nLLMs\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry.\nSubsequently, numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal\nLLMs [19, 20, 21, 22, 23]. In this section, we review these recent advances in LLMs and discuss their connection to\nand distinctions from our work.\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained\nmodels such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence\nof ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and\ntraining data size. This enhancement has resulted in remarkable zero-shot generalization capabilities, allowing these\nmodels to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs\nsuch as OPT [28], BLOOM [18], PaLM [17], and Llama [4]. Particularly, Llama2 [4] is an open-source LLM that\nachieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and\nFalcon, with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization\n[28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two\ntrillion tokens from publicly available sources.\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input.\nThese models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models\nthat are tailored for visually-rich document understanding [31, 32, 33, 34, 12]. The general-purpose multimodal LLMs\nexhibit promising performance in identifying and reasoning with image information. However, they have not yet been\nvigorously evaluated on VRDU tasks. As an example, the GPT-4 Technical Report [16] highlights diverse multimodal\ntest cases, such as explaining meme picture distinctiveness, but very few examples are included for visual document\nuse cases. Prior to the advent of large language models, fine-tune-based models relying on vision only were less\neffective than layout (and vision) modality models in processing visual documents. For example, models like UDOP\n[12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But\nsuch models require task- and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent\nmPLUG-DocOwl [31] and UReader [32], built upon LLMs, undergo instruction finetuning on a diverse set of VRDU,\nvisual, and textual datasets, and exhibit impressive zero-shot generalization capabilities. Hence, we include those as\nbaselines in our evaluation in Section 4.\nDespite the remarkable performance of LLMs, unimodal models aren\u2019t equipped to process multimodal input, and\nmultimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM,\naddresses these challenges by explicitly modeling spatial layouts and text semantics, enabling effective comprehension\nof visual documents. Notably, DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\ntext semantics, avoiding the expensive vision encoder, resulting in a more compact model and efficient processing time.\n2.2\nLLM Architectures\nAutoregressive Infilling. There are two main autoregressive infilling approaches: \u201cfill-in-the-middle\u201d (FIM) where a\nsingle span is sampled, and \u201cblank infilling\u201d with multiple spans.\nThe OpenAI FIM approach [36] uses the template (prefix, middle, suffix) to divide a document into three\nsegments. Next, these segments are reorganized into (prefix, suffix, middle), enabling the model to predict the\nmiddle segment. This process relies on three special tokens, [PRE], [SUF], and [MID], which structure a document as:\n[PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction, while the other two special\ntokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text\nwhere the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible\ngeneration at arbitrary positions, unconstrained by a predefined generation order. In contrast, approaches like GLM\n[15] sample multiple spans for infilling. For each blank to be infilled, a pair of special tokens is used: [blank_mask]\nand [start_to_fill]. The multiple spans not only require special tokens but also global indicators to distinguish\nwhich middle span the model should infill. This global indicator is implemented with 1D token positions, ensuring that\neach pair of the two special tokens, i.e., [blank_mask] and [start_to_fill], share the same positions. We adopt a\nsimilar infilling object with the goal to prevent disconnected next-token predictions while avoiding breaking sparse\ndocuments into very short segments, e.g., word pieces and/or phrase pieces.\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings\nand relative positional encodings were kept separate rather than summed together, and each used independently when\ncomputing attention weights using disentangled matrices. The motivation behind this was to facilitate the learning of\n3\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective. left: Input document\nwith text tokens xi and bounding boxes bi. Some text segments are randomly masked (two segments here) and the\nmodel predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing\nthe sampled segments with [M] and prepending them with [S]. The attention mechanism is extended to account for\ncross-attention between text and spatial modalities.\ndecoupled attention alignments based on content and position separately. This innovation proved effective as it allowed\nDeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on\nSuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents,\ndisentanglement becomes ever more important to our model\u2019s performance.\n3\nDocLLM Framework\nIn this section, we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures.\nFigure 2 presents an overview of the model architecture.\n3.1\nModel Architecture\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal\ndecoder structure. It is composed of stacked transformer blocks, where each block contains a multi-head self-attention\nlayer and a fully connected feed forward network. Standard language models are typically unimodal, accepting only\na sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual\ninformation by utilizing the spatial positions and dimensions of text tokens obtained using OCR. Simply augmenting\nthe text with bounding box information via additive positional encoding may not capture the intricate relationships\nbetween text semantics and spatial layout, especially for visually rich documents [10]. Consequently, we treat the spatial\ninformation about the text tokens as a distinct modality. In particular, we use separate vectors to represent these two\nmodalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies\nin a disentangled manner, as explained in the following section. Furthermore, instead of the traditional left-to-right next\ntoken prediction during self-supervised training, we employ a text infilling objective that better leverages contextual\ninformation.\n3.2\nDisentangled Spatial Attention\nLet x = (x1, ..., xi, ..., xT ) be an input sequence of length T, where xi is a text token. In classical transformers, using\na learned embedding matrix based on the text vocabulary and a learned set of parameters for the token position in the\nsequence, the input tokens are first encoded into hidden vectors H \u2208 RT \u00d7d. A self-attention head then computes the\nattention scores between tokens i and j as:\nQt = HWt,q,\nKt = HWt,k,\nAt\ni,j = Qt\niKt\nj\n\u22ba\n(1)\n4\nwhere Wq \u2208 Rd\u00d7d and Wk \u2208 Rd\u00d7d are projection matrices, and the superscript t indicates the text modality. The\nattention scores A \u2208 RT \u00d7T along with another projection matrix Wv are further used to compute the hidden vectors\nH\u2032, which are in turn used as inputs for a subsequent layer:\nVt = HWt,v,\nH\u2032 = softmax( At\n\u221a\nd\n)Vt.\n(2)\nIn DocLLM, the input is represented as x = {(xi, bi)}T\ni=1, where bi = (left, top, right, bottom) is the bounding\nbox corresponding to xi. To capture the new modality (i.e. spatial information), we encode the bounding boxes into\nhidden vectors represented by S \u2208 RT \u00d7d. We then decompose the attention matrix computation into four different\nscores, namely text-to-text, text-to-spatial, spatial-to-text and spatial-to-spatial. Formally, the new attention mechanism\nis calculated as:\nQs = SWs,q,\nKs = SWs,k\n(3)\nAi,j = Qt\niKt\nj\n\u22ba + \u03bbt,sQt\niKs\nj\n\u22ba + \u03bbs,tQs\niKt\nj\n\u22ba + \u03bbs,sQs\niKs\nj\n\u22ba,\n(4)\nwhere Ws,q \u2208 Rd\u00d7d and Ws,k \u2208 Rd\u00d7d are newly introduced projection matrices corresponding to the spatial modality,\nand \u03bbs are hyperparameters that control the relative importance of each score. The input hidden vectors for the next\nlayer H\u2032 are computed exactly as before. However, in contrast to equation (2), the newly calculated hidden vectors rely\nnot only on the text semantics but also on the layout information of the text tokens.\nIt is important to mention that the hidden vectors S are reused across different layers, while each layer retains the\nflexibility to employ different projection matrices. We also note that the number of extra parameters required to encode\nthe bounding box information is significantly lower compared to the overhead introduced by image based models [7].\nBy simply adding S to H similar to [13], we could have avoided using Ws matrices altogether and further reduced the\nnumber of parameters. However, it would have irreversibly coupled the layout information with the text semantics.\nIn contrast, our disentangled representation of these modalities in the attention scores enables selective focus when\nappropriate [38], thereby providing an optimal balance between model size and effectiveness.\n3.3\nPretraining\nDocLLM is first pre-trained in a self-supervised fashion on a large number of unlabeled documents. The self-supervised\npre-training objective in autoregressive language models [26] is generally to maximize the log-likelihood of the next\ntoken prediction in a sequence based on the context provided by preceding tokens. Let \u03b8 denote all the parameters of\nthe transformer model, including the projection matrices discussed above. The following cross-entropy loss is then\ntypically minimized during the pre-training step:\nLAR(\u03b8) = \u2212\nT\nX\ni=1\nlog p\u03b8(xi|xj<i)\n(5)\nVisual documents are often sparse and irregular, featuring isolated and disconnected text fragments. In such cases, it is\npreferable to consider coarse segments of related tokens during pre-training rather than focusing on individual tokens.\nA segment may represent a coherent chunk of information, similar to a text block, or it can simply be a linear sequence,\nsimilar to a text span. In Figure 2, \u201cName\u201d, \u201cJohn Doe\u201d , and \u201cDoctor\u201d are all examples of blocks. In general, the\nbroader context provided by multiple tokens in a block can lead to better comprehension.\nFurthermore, learning to infill text, where the prediction is conditioned on both prefix and suffix tokens rather than\nonly preceding tokens, can be beneficial. The infilling objectives enable contextually relevant completions, provide\nrobustness to OCR noise or misaligned tokens, and can better handle relationships between various document fields.\nHence we modify the standard pre-training objective to predict blocks of text given preceding and following text blocks.\nMost OCR engines can provide block level information, which makes it feasible to identify coherent text blocks such as\na heading or an address1. Inspired by [15], we follow an autoregressive block infilling objective, where text blocks are\nrandomly masked, and the masked blocks are shuffled and reconstructed in a sequential left-to-right fashion. Block\ninformation and block infilling are solely utilized for the pre-training phase, not in instruct-tuning or downstream tasks.\nFormally, let c = {c1, ..., cK} be a set of text blocks that partitions an input sequence x into non-overlapping contiguous\ntokens such that c1\u222a...\u222acK = x and ck \u2229ck\u2032 = \u2205. These text blocks are typically identified from OCR information. Let\n1Note that in order to avoid any leakage of useful information, the block information is only used for the masking objective during\npre-training, and is not provided to the model as input. Concretely, masking is performed at the block level, but the model is not\nprovided with information about the number of tokens in a given masked block. Please refer to Figure 2 for an illustrated example.\n5\nTable 1: Prompt templates used for instruction-tuning (spatial tokens not included).\nTask\nTemplate type\nPrompt template\nExpected response\nVQA\nExtraction\n\"{document} {question}\"\nanswer annotation\nNLI\nMCQ\n\"{document} \\\"{statement}\\\", Yes or No?\"\nanswer annotation\nKIE\nExtraction\n\"{document} What is the value for the \\\"{key}\\\"?\"\nAssociated value annotation\nMCQ\n\"{document} What is \\\"{value}\\\" in the document? Possible choices: {choices}.\"\n(where choices is a subset of all the keys in the dataset in random order)\nAssociated key annotation\nInternal classification\n\"{document} What is \\\"{value}\\\" in the document?\"\nAssociated key annotation\nCLS\nMCQ\n\"{document} What type of document is this? Possible choices: {choices}.\"\n(where choices is a subset of all the classes in the dataset in random order)\nclass annotation\nInternal classification\n\"{document} What type of document is this?\"\nclass annotation\nz = {zm}M\nm=1 be M \u226a K different text blocks randomly sampled from c, where each block zm = (zm,1, ..., zm,Nm)\ncontains a consecutive series of tokens. Further, let \u02dcx be a corrupted version of x where the contiguous tokens\ncorresponding to a sampled text block are replaced with a special mask token [M]. To facilitate the identification of\nthe block to be filled during text generation, each input block is augmented with a special start token [S] while the\noutput block includes an end token [E]. For instance, a block with tokens (x4, x5) becomes [M] in \u02dcx, ([S], x4, x5)\nwhen conditioned upon, and is expected to generate (x4, x5, [E]) as output autoregressively (see Figure 2 for a detailed\nillustration of these configurations). The following cross-entropy loss is then minimized for the infilling objective.\nLIF(\u03b8) = \u2212\nM\nX\nm=1\nNm\nX\nj=1\nlog p\u03b8(zm,j|\u02dcx, z<m, zm,<j)\n(6)\n3.4\nInstruction Tuning\nFollowing recent work in the field of VRDU [12, 31, 32] and prior work in NLP [40, 41], we instruction-tune DocLLM on\na variety of instructions derived from DocAI datasets using various templates. Due to the high cost and time intensity of\nmanual data collection, we leave the construction of a VRDU instruction-tuning dataset with crowdsourced instructions\nand preferences to future work. We employ a total of 16 datasets with their corresponding OCRs, spanning four DocAI\ntasks: visual question answering (VQA), natural language inference (NLI), key information extraction (KIE), and\ndocument classification (CLS).\nThe diversity of supervised fine tuning (SFT) instructions is critical in helping zero-shot generalization [40, 41, 42].\nThus, we diversify templates per task when possible, with each template asking a different question, and in some cases,\nexpecting different types of answers. We re-use the templates introduced in [31, 32] when applicable, and consider a\nbroader selection of datasets in our instruction-tuning data mix.\nWe create the templates following what we believe end users would generally ask about documents (Table 1). For KIE\nand CLS, we hypothesize that (1) the extraction instructions can teach DocLLM to correlate names of keys in the prompts\nwith document fields so as to retrieve values, (2) the internal classification instructions can help the model understand\nwhat intrinsically characterizes each key or document type, and (3) the multiple choice question (MCQ) instructions\ncan teach the model to leverage its comprehension of key names included as choices in the prompt (resp. document\ntype names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows.\nVisual Question Answering. We collect DocVQA [43], WikiTableQuestions (WTQ) [44], VisualMRC [45], DUDE\n[46], and BizDocs2, to compose the VQA instruction-tuning data mix. We use one instruction template to build our\nSFT inputs for VQA, as shown in table 1. An example prompt derived from DocVQA would read: \"{document} What\nis the deadline for scientific abstract submission for ACOG - 51st annual clinical meeting?\"\nNatural Language Inference. We only include TabFact [47] in our instruction-tuning data mix for NLI task, due to\nlack of additional DocAI NLI datasets available. The instruction template is shown in table 1. An example prompt\nderived from TabFact would read: \"{document} \\\"The UN commission on Korea include 2 Australians.\\\", Yes or No?\"\nKey Information Extraction. We gather Kleister Charity (KLC) [48], CORD [49], FUNSD [50], DeepForm [51], PWC\n[52], SROIE [53], VRDU ad-buy [54] (with random train-test splitting), and BizDocs to build the KIE instruction-tuning\ndata, where we leverage three instruction templates: extraction, internal classification, and MCQ, as shown in 1. For the\n2BizDocs is a collection of business entity filings that is due to be released publicly.\n6\nTable 2: Pre-training dataset statistics.\nNo. of Docs\nNo. of Pages\nNo. of Total Tokens\nCDIP\n5,092,636\n16,293,353\n3,637,551,478\nDocBank\n499,609\n499,609\n228,362,274\nTotal\n5,592,245\n16,792,962\n3,865,913,752\nTable 3: Instruction-tuning dataset statistics.\nTasks\nNo. of Training\nNo. of Testing\nVQA\n145,090\n24,347\nNLI\n104,360\n12,720\nKIE\n236,806\n38,039\nCLS\n149,627\n21,813\nTotal\n635,883\n96,919\nextraction template, we add the \u201cNone\u201d answer if the key does not exist in the given document. To increase diversity in\nthe SFT training data, we also derive internal classification and MCQ instructions from original KIE annotations. To stay\nconsistent with benchmarks from previous work [31, 32], we only keep the prompts derived from the extraction template\nin the test split of each KIE dataset. An example extraction instruction derived from KLC would read: \"{document}\nWhat is the value for the \\\"charity number\\\"?\"\nDocument Classification. We aggregate RVL-CDIP [55] and BizDocs to build our CLS instruction-tuning data. We\nused two types of instruction templates for this task: internal classification and MCQ, as shown in 1. To avoid the cold\nstart problem induced by potentially unseen types of documents in testing or even in production usage, we only keep\nthe MCQ prompts for the test split of each CLS dataset. We also downsample RVL-CDIP in the train split to avoid\nhindering the other datasets. An example MCQ instruction derived from RVL-CDIP would read: \"{document} What\ntype of document is this? Possible answers: [budget, form, file folder, questionnaire].\"\n4\nExperiments\n4.1\nDatasets\nWe gather data for pre-training from two primary sources: (1) IIT-CDIP Test Collection 1.0 [56] and (2) DocBank\n[57]. IIT-CDIP Test Collection 1.0 encompasses a vast repository of over 5 million documents, comprising more than\n16 million document pages. This dataset is derived from documents related to legal proceedings against the tobacco\nindustry during the 1990s. DocBank consists of 500K documents, each featuring distinct layouts and a single page\nper document. The relevant statistics for the datasets utilized in the pre-training are detailed in Table 2. We obtain a\ncollection of 16.7 million pages comprising a total of 3.8 billion tokens.\nWe have introduced the datasets used to conduct instruction tuning on Section 3.4. These datasets encompass four\ncommon DocAI tasks: VQA, NLI, KIE, and CLS. Note that when a prompt includes a list of possible answers, we\ncreate multiple copies of the prompt with one possible answer assigned to each. We only perform this \u201cflattening\u201d\noperation in the training split of the dataset. Detailed statistics for these tasks are presented in Table 3.\n4.2\nModel Setup and Training Details\nTable 4 provides key settings and hyperparameters for two variants of DocLLM: DocLLM-1B, which is based on the\nFalcon-1B architecture [5], and DocLLM-7B, which is based on the Llama2-7B architecture [4]3. DocLLM-1B is\ncomposed of 24 layers, each with 16 attention heads and a hidden size of 1,536. DocLLM-7B comprises 36 layers, 32\nheads, and a hidden size of 4,096. Using pre-trained weights as the backbone for the text modality, we extend the\nFalcon-1B and Llama2-7B models by adding the disentangled attention and block infilling objective as described in\nSection 3.\nFor DocLLM-1B, we use a pre-training learning rate of 2\u00d710\u22124 with 1,000 warmup steps, employing a cosine scheduler,\nand Adam optimizer [58] with \u03b21 = 0.9, \u03b22 = 0.96 and a weight decay of 0.1. For instruction tuning we use a learning\n3Since Llama2 does not come with pre-trained weights at 1B parameters, we use the Falcon-1B architecture for the smaller\nversion of DocLLM.\n7\nTable 4: Model configuration and training hyperparameters setting for DocLLM-1B and -7B.\nDocLLM-1B\nDocLLM-7B\nBackbone\nFalcon-1B [5]\nLlama2-7B [4]\nLayers\n24\n36\nAttention heads\n16\n32\nHidden size\n1536\n4096\nPrecision\nbfloat16\nbfloat16\nBatch size\n2\n5\nMax context length\n1,024\n1,024\nPre-train\nInstruct-tune\nPre-train\nInstruct-tune\nLearning rate\n2 \u00d7 10\u22124\n1 \u00d7 10\u22124\n3 \u00d7 10\u22124\n1 \u00d7 10\u22124\nWarmups\n1000\n500\n1000\n500\nScheduler type\ncosine\ncosine\ncosine\ncosine\nWeight decay\n0.1\n0.1\n0.1\n0.1\nAdam \u03b2s\n(0.9, 0.96)\n(0.9,0.96)\n(0.9,0.95)\n(0.9,0.95)\nAdam epsilon\n1 \u00d7 10\u22125\n1 \u00d7 10\u22125\n1 \u00d7 10\u22126\n1 \u00d7 10\u22126\nrate of 1 \u00d7 10\u22124 with 500 warmup steps and a cosine scheduler, and the same parameters for weight decay and Adam\noptimizer as the pre-training phase. The Adam epsilon is set to 1 \u00d7 10\u22125. We pre-train for one epoch, and instruct-tune\nfor a total of 10 epochs.\nFor DocLLM-7B, pre-training involves a learning rate of 3 \u00d7 10\u22124 with 1,000 warmup steps and cosine scheduler,\nweight decay of 0.1, and Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.95. Instruction tuning uses a learning rate of 1 \u00d7 10\u22124\nwith 500 warmup steps and a cosine scheduler, weight decay of 0.1, and Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.95.\nAdam epsilon is set at 1 \u00d7 10\u22126. We conduct one epoch of pre-training, followed by three epochs of instruct-tuning,\nconsidering available computing resources.\nThe maximum sequence length, or context length, is consistently set to 1,024 for both versions during the entire training\nprocess. The DocLLM-7B models are trained with 16-bit mixed precision on 8 24GB A10g GPUs using fully sharded\ndata parallelism, implemented with the accelerate library.4 The DocLLM-1B model, on the other hand, is trained on a\nsingle 24GB A10g GPU.\n4.3\nDownstream Evaluation\nExperimental settings. We investigate two experimental settings:\n\u2022 Same Datasets, Different Splits (SDDS): Following previous work in VRDU [34, 59, 33, 12, 31, 32], we\nfirst evaluate DocLLM on the unseen test split (or dev split when test split is unavailable) of each of the 16\ndatasets composing the instruction-tuning data. The motivation behind this very typical setting is to check how\nDocLLM performs when tasks and domains supposedly stay the same from train to test.\n\u2022 Same Tasks, Different Datasets (STDD): Following [40, 41, 60, 61], we also evaluate DocLLM on held-out\ndatasets. More precisely, we instruction-tune the pre-trained checkpoint of DocLLM on prompts from 11 of\nthe 16 datasets considered in SDDS, then evaluate DocLLM on the test split of the remaining three datasets.\nThe rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one dataset per task from SFT when\npossible, (2) leave enough datapoints per task in the training split of the instruction-tuning data, (3) avoid\ndata leakage (certain datasets were obtained from the same sources), and (4) benchmark models on popular\nyet challenging datasets when possible. Due to the high cost of instruction-tuning, we were not able to run\nadditional experiments with different held-out datasets.\nBaselines. In SDDS and STDD, we benchmark DocLLM against comparably-sized and SOTA LLMs using Zero-\nShot (ZS) prompts that contain the text extracted from each document using an OCR engine (excluding the spatial\ninformation) [4, 42]. In SDDS, we also report numbers from recent DocAI LLMs evaluated in a similar setting [31, 32].\n4https://huggingface.co/docs/accelerate\n8\nTable 5: Performance comparison in the SDDS setting against other multimodal and non-multimodal LLMs;\nnon-multimodal LLMs are Zero-Shot (ZS) prompted while multimodal LLMs are instruction-tuned on the train split of\nthe datasets considered. \u2018-\u2019 marks not available.\nDataset\nGPT-4+OCR\nLlama2+OCR\nmPLUG-DocOwl\nUReader\nDocLLM-1B\nDocLLM-7B\n\u223c1T (T)\n7B (T)\n\u223c7B (T+V)\n\u223c7B (T+V)\n1B (T+L)\n7B (T+L)\nZS\nZS\nSDDS\nSDDS\nSDDS\nSDDS\nVQA\nDocVQA\n82.8\n47.4\n62.2\n65.4\n61.4\n69.5\nWTQ (Accuracy)\n65.4\n25.0\n26.9\n29.4\n21.9\n27.1\nVisualMRC (CIDEr)\n255.1\n115.5\n188.8\n221.7\n245.0\n264.1\nDUDE\n54.6\n38.1\n-\n-\n42.6\n47.2\nBizDocs\n76.4\n48.8\n-\n-\n84.5\n86.7\nNLI\nTabFact\n77.1\n48.2\n60.2\n67.6\n58.0\n66.4\nKIE\nKLC\n45.9\n27.8\n30.3\n32.8\n58.9\n60.3\nCORD\n58.3\n13.8\n-\n-\n66.9\n67.4\nFUNSD\n37.0\n17.8\n-\n-\n48.2\n51.8\nDeepForm\n42.1\n20.5\n42.6\n49.5\n71.3\n75.7\nPWC\n18.3\n6.8\n-\n-\n25.7\n29.06\nSROIE\n90.6\n56.4\n-\n-\n91.0\n91.9\nVRDU a.-b.\n43.7\n18.7\n-\n-\n87.6\n88.8\nBizDocs\n66.1\n10.8\n-\n-\n95.4\n95.9\nCLS\nRVL-CDIP\n68.2\n32.8\n-\n-\n90.9\n91.8\nBizDocs\n84.9\n40.9\n-\n-\n98.3\n99.4\nAs motivated in section 2, we do not consider DocAI models that require task-specific fine-tuning [33, 59, 34] and/or\ndataset specific prompts [12], and instead focus on LLMs with out-of-the-box instruction following capability.\nMetrics. Following previous work [62, 34, 32, 31], we evaluate all VQA datasets using Average Normalized Levenshtein\nSimilarity (ANLS) [63], with the exception of VisualMRC, for which we use CIDEr [64] and WTQ, for which we use\naccuracy5. Performance on all CLS and NLI datasets is measured using accuracy. We evaluate all KIE datasets with the\nF1 score.\nResults. In the SDDS setting, as shown in the Table 5, we observe that DocLLM-7B excels in 12 out of 16 datasets,\ninclusively compared to ZS results of GPT4 and Llama2, and SDDS results of mPLUG-DocOwl and UReader. Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrates performance close to that of our larger model, suggesting that the smaller\nmodel can derive significant benefits from the architecture of DocLLM.\nIn the STDD setting, our model demonstrates superior performance compared to Llama2 across four out of five datasets,\nand achieves the best score overall for two of them (KIE task again). DocLLM also outperforms mPLUG-DocOwl\non DocVQA and both mPLUG-DocOwl and UReader on KLC, despite both baselines having been instruction-tuned\non these datasets. However, it is important to note that classification accuracy is notably lower in our model. This\ndiscrepancy may stem from the fact that our model has been trained using only one classification dataset, limiting its\nability to generalize effectively to new datasets.\n5\nAblation Studies\nWe conduct ablation studies to validate the three contributions of DocLLM: (1) disentangled spatial features, (2) the\nblock infilling pre-training objective, and (3) the masking strategy used for decoding.\nFor all ablations, we use Next Token Prediction (NTP) out-of-sample accuracy to compare configurations at the\npre-training stage. Due to resource restrictions, each experiment uses a subset of our pre-training corpus: we randomly\nsample 100,000 chunks and predict on 1,000 unseen documents. A chunk is a pack of documents concatenated one by\none with the total length less than maximum input length. The hyperparameters are set consistently following Table 4\nacross all ablation experiments.\n5This is done to remain consistent with the results reported by other SotA models.\n9\nTable 6: Performance comparison on three held-out VRDU datasets in the STDD setting against non-multimodal LLMs.\nModel\nSize\nSetting\nDocVQA\nKLC\nBizDocs\nVQA\nKIE\nVQA\nKIE\nCLS\nGPT-4+OCR\n\u223c1T\nZS\n82.8\n45.9\n76.4\n66.1\n84.9\nLlama2+OCR\n7B\nZS\n47.4\n27.8\n48.4\n10.8\n40.9\nDocLLM-1B\n1B\nSTDD\n53.5\n40.1\n65.5\n63.0\n20.8\nDocLLM-7B\n7B\nSTDD\n63.4\n49.9\n73.3\n72.6\n31.1\n(a) Causal decoder\n(b) Prefix decoder\nFigure 3: A simplified illustration of attention masks for causal-decoder and prefix-decoder for block infilling.\nDisentangled Spatial Attention. To measure the effect of disentangled spatial attention on cross-modal interactions,\nwe train the models by setting the \u03bb hyperparameter in Eq 6 to 0 or 1 . Table 7 enumerates the attention combinations,\nand the results suggest that keeping only the spatial-to-spatial interaction (i.e. \u03bbs,s = 1) yields the highest NTP\naccuracy. The performance differences among other configurations, such as text-to-spatial and spatial-to-text, are subtle.\nNotably, the vanilla text-only self-attention mechanism yields the lowest NTP accuracy, underlining the importance\nof incorporating spatial features for understanding documents with rich layouts. For all experiments in Section 4, we\ntherefore set \u03bbs,s = 1, \u03bbs,t = 0, and \u03bbt,s = 0. We opt for simplicity by choosing a hard mode over a soft one while\nacknowledging the potential advantage of flexibility for the latter.\nAutoregressive Block Infilling. To evaluate the effectiveness of the proposed autoregressive block infilling objective\nespecially comparing with the conventional left-to-right causal learning, we benchmark three configurations in our\nablation study: (1) causal learning, (2) causal learning with spatial modality, and (3) block infilling with spatial modality.\nAs highlighted in Table 8, autoregressive block infilling exhibits the best performance. Additionally, the performance\ngain of adding the spatial modality to the causal learning proves the advantage of the spatial modality.\nPrefix Decoder and Causal Decoder. For document-conditioned generation, an intuitive choice is to employ a prefix\ndecoder with prefix masking to make the whole document bidirectional visible in the attention, as illustrated in Figure\n3b. We investigate this assumption through experiments where we compare a prefix decoder against the conventional\nTable 7: Ablation study on disentangled spatial attention. T stands for the text modality, S stands for the spatial\nmodality, and their cross-modal interactions represent as X2X, e.g., text-to-spatial \u2192 T2S.\nCross-Modal Interactions\nNTP Accuracy\nT2T\n35.43\nT2S + T2T\n38.08\nS2T + T2T\n38.05\nS2S + T2T\n39.12\nT2S + S2S + T2T\n39.06\nS2T + S2S + T2T\n39.07\nT2S + S2T + S2S + T2T\n39.02\n10\nTable 8: Ablation study on the block infilling objective.\nPretraining Objective\nNTP Accuracy\nCausal Learning\n32.6\nCausal Learning + Spatial\n36.2\nBlock Infilling + Spatial\n39.1\nFigure 4: Performance comparison on NTP between causal decoder and prefix decoder.\ncausal decoder. Specifically, we conduct contrast experiments on these two decoders for different settings outlined in\nthe disentangled spatial attention to study their resulting performance.\nThe results in Figure 4 show marginal differences between these two decoder across the five configurations, with the\ncausal decoder having a slight edge over the prefix. The minor difference suggests that both masking methods are\ncomparable in modeling documents. Thus the bidirectional attention enabled by the prefix decoder may not be crucial\nin this context, and we consequently elect to use a causal decoder for all experiments in section 4.\n6\nDiscussion and Findings\nIn addition to its immediate utility in visually rich document understanding tasks, we posit that DocLLM offers an\nopportunity to change the landscape of generative pre-training by enabling language models to go beyond next\ntoken prediction in plain text settings. By accommodating complex layout structures, DocLLM allows for e-books,\ne-publications, and other documents with rich layouts to be incorporated into the pre-training corpus without requiring\nextensive preprocessing. The spatial-aware reading approach enables the model to perceive the document as inherently\nstructured knowledge.\nMoreover, the multi-page awareness, of both page breaks and document boundaries, enhances the model\u2019s ability to\ncomprehend documents of various lengths. This addresses the limitations of previous smaller multi-modal models\n(which are mainly for single-page documents) and the existing multimodal LLMs (which are primarily designed for\nimages). In supervised instruction tuning, we can adhere to the established practices used in other works, based on\ndesired outputs such as text or images.\nThe main concept for a cohesive block is to ensure meaningful infilling during the pre-training phase, preventing\ndisconnected predictions. However, the choice of OCR engines to obtain such cohesive blocks remains an open area\nfor exploration. Practical comparisons with various OCR engines and/or layout parsers are left as future work, as\nLayoutLMs underscore the importance of accurate OCR for improved VQA results. They leverage the Microsoft\nAzure API, demonstrating superior performance compared to TesseractOCR, as indicated in the DocVQA leaderboard.6\nConsequently, researchers are also encouraged to utilize more accurate OCR engines for potential enhancements, if\nsuch resources are available.\nWe have presented a collection of SDDS results alongside zero-shot outcomes. To mitigate prompt influence in the\nzero-shot results, a rigorous methodology was implemented. This involves the engagement of three independent prompt\nengineers, each undergoing five rounds of refinement for zero-shot settings, followed by a series of post-processing\ntechniques to enhance result reliability. The best results are thus obtained from each of the three groups. We still\nacknowledge the potential for refinement and improvement.\nWe share some internal training experiences, acknowledging the absence of robust validation. First, we observe that\na higher weight decay (e.g., 0.1 versus 0.01) generally improves performance in both pre-training and instruction-\n6https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1\n11\ntuning. During the instruction tuning phase, a higher initial learning rate, such as 1e-4 versus 5e-5, leads to enhanced\nperformance. Overall, we\u2019ve observed that the cosine scheduler tends to outperform linear or constant schedulers across\nvarious settings.\n7\nConclusions\nIn this paper, we introduced DocLLM, a lightweight extension to traditional large language models, tailored for generative\nreasoning over documents with rich layouts. Unlike existing multimodal LLMs, DocLLM strategically omits costly\nimage encoders, instead prioritizing bounding box information to effectively capture the spatial layout structure of\ndocuments. This is achieved through a disentangled attention approach, decomposing the attention mechanism in\nclassical transformers, and enhancing with cross-alignment between text and spatial modalities in structured documents.\nNotably, our model addresses the challenges posed by irregular layouts and heterogeneous content by employing\na pre-training objective that focuses on learning to infill block texts. We fine-tuned the pre-trained model using a\ncomprehensive instruction dataset. Our evaluation across various document intelligence tasks demonstrates that DocLLM\nsurpasses equivalent models on known tasks for 14 datasets out of 16 and exhibits robust generalization to previously\nunseen datasets in 4 out of 5 settings, affirming its efficacy in extracting meaningful information from a wide range of\nvisual documents. In future work, we plan to infuse vision into DocLLM in a lightweight manner.\nAcknowledgments\nThis paper was prepared for information purposes by the Artificial Intelligence Research group of JPMorgan Chase &\nCo and its affiliates (\u201cJP Morgan\u201d), and is not a product of the Research Department of JP Morgan. J.P. Morgan makes\nno representation and warranty whatsoever and disclaims all liability for the completeness, accuracy or reliability of\nthe information contained herein. This document is not intended as investment research or investment advice, or a\nrecommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or\nservice, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a\nsolicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would\nbe unlawful. \u00a9 2023 JP Morgan Chase & Co. All rights reserved.\nReferences\n[1] Arjun Reddy Kunduru. From data entry to intelligence: Artificial intelligence\u2019s impact on financial system\nworkflows. International Journal on Orange Technologies, 5(8):38\u201345, 2023.\n[2] Lei Cui, Yiheng Xu, Tengchao Lv, and Furu Wei. Document ai: Benchmarks, models and applications. arXiv\npreprint arXiv:2111.08609, 2021.\n[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[5] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli,\nBaptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming\ncurated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\n[6] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403,\n2023.\n[7] Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei. Dit: Self-supervised pre-training for\ndocument image transformer. In Proceedings of the 30th ACM International Conference on Multimedia, pages\n3530\u20133539, 2022.\n[8] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with\nunified text and image masking. In Proceedings of the 30th ACM International Conference on Multimedia, pages\n4083\u20134091, 2022.\n12\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nimage is worth 16x16 words: Transformers for image recognition at scale. In International Conference on\nLearning Representations, 2021.\n[10] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang,\nWanxiang Che, Min Zhang, and Lidong Zhou. LayoutLMv2: Multi-modal pre-training for visually-rich document\nunderstanding. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 2579\u20132591, Online, August 2021. Association for\nComputational Linguistics.\n[11] Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen\nWang, Yasuhisa Fujii, and Tomas Pfister. FormNet: Structural encoding beyond sequential modeling in form\ndocument information extraction. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 3735\u20133754, Dublin, Ireland, May 2022. Association for Computational Linguistics.\n[12] Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and\nMohit Bansal. Unifying vision, text, and layout for universal document processing. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages\n19254\u201319264. IEEE, 2023.\n[13] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and\nlayout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, pages 1192\u20131200, 2020.\n[14] Zihang Meng, Licheng Yu, Ning Zhang, Tamara L Berg, Babak Damavandi, Vikas Singh, and Amy Bearman.\nConnecting what to say with where to look by modeling human attention traces. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 12679\u201312688, 2021.\n[15] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language\nmodel pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.\n[16] OpenAI. Gpt-4 technical report, 2023.\n[17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022.\n[18] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow,\nRoman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al. Bloom: A 176b-parameter open-access\nmultilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n[19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[20] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\n[21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning.\narXiv preprint\narXiv:2304.08485, 2023.\n[22] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.\n[23] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng\nShi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang.\nmplug-owl: Modularization empowers large language models with multimodality. CoRR, abs/2304.14178, 2023.\n[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[25] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of NAACL-HLT, pages 4171\u20134186, 2019.\n[26] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of\nMachine Learning Research, 21(1):5485\u20135551, 2020.\n13\n[28] Biao Zhang and Rico Sennrich. Root mean square layer normalization, 2019.\n[29] Noam Shazeer. Glu variants improve transformer, 2020.\n[30] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding, 2023.\n[31] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li,\nJunfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-docowl: Modularized multimodal large language model\nfor document understanding. CoRR, abs/2307.02499, 2023.\n[32] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian,\nJi Zhang, Qin Jin, Liang He, Xin Alex Lin, and Fei Huang. Ureader: Universal ocr-free visually-situated language\nunderstanding with multimodal large language model. CoRR, abs/2310.05126, 2023.\n[33] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang,\nSangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In Computer\nVision \u2013 ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXVIII,\npage 498\u2013517, Berlin, Heidelberg, 2022. Springer-Verlag.\n[34] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi\nKhandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining\nfor visual language understanding. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,\nSivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29\nJuly 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 18893\u2013\n18912. PMLR, 2023.\n[35] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang,\nSangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer, 2022.\n[36] Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark\nChen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022.\n[37] Tianxiao Shen, Hao Peng, Ruoqi Shen, Yao Fu, Zaid Harchaoui, and Yejin Choi. Film: Fill-in language models\nfor any-order generation. arXiv preprint arXiv:2310.09930, 2023.\n[38] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentan-\ngled attention. In International Conference on Learning Representations, 2020.\n[39] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems.\nAdvances in neural information processing systems, 32, 2019.\n[40] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai,\nand Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\n[41] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun\nChen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang,\nAndrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou,\nQuoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. CoRR, abs/2210.11416, 2022.\n[42] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie\nSimens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models\nto follow instructions with human feedback. In NeurIPS, 2022.\n[43] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: A dataset for VQA on document images.\nIn IEEE Winter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8,\n2021, pages 2199\u20132208. IEEE, 2021.\n[44] Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceedings\nof the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint\nConference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015,\nJuly 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1470\u20131480. The Association for Computer\nLinguistics, 2015.\n[45] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document\nimages. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on\n14\nInnovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances\nin Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 13878\u201313888. AAAI Press, 2021.\n[46] Jordy Van Landeghem, Rub\u00e8n Tito, Lukasz Borchmann, Michal Pietruszka, Pawel J\u00f3ziak, Rafal Powalski, Dawid\nJurkiewicz, Micka\u00ebl Coustaty, Bertrand Anckaert, Ernest Valveny, Matthew B. Blaschko, Sien Moens, and Tomasz\nStanislawek. Document understanding dataset and evaluation (DUDE). CoRR, abs/2305.08455, 2023.\n[47] Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. Tabfact: A large-scale dataset for table-based fact verification. In 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,\n2020.\n[48] Tomasz Stanislawek, Filip Gralinski, Anna Wr\u00f3blewska, Dawid Lipinski, Agnieszka Kaliska, Paulina Rosalska,\nBartosz Topolski, and Przemyslaw Biecek. Kleister: Key information extraction datasets involving long documents\nwith complex layouts. In Josep Llad\u00f3s, Daniel Lopresti, and Seiichi Uchida, editors, 16th International Conference\non Document Analysis and Recognition, ICDAR 2021, Lausanne, Switzerland, September 5-10, 2021, Proceedings,\nPart I, volume 12821 of Lecture Notes in Computer Science, pages 564\u2013579. Springer, 2021.\n[49] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and Hwalsuk Lee. Cord: A\nconsolidated receipt dataset for post-ocr parsing. 2019.\n[50] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. FUNSD: A dataset for form understanding in\nnoisy scanned documents. In 2nd International Workshop on Open Services and Tools for Document Analysis,\nOST@ICDAR 2019, Sydney, Australia, September 22-25, 2019, pages 1\u20136. IEEE, 2019.\n[51] Stacey Svetlichnaya. Deepform: Understand structured documents at scale. 2020.\n[52] Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder, Sebastian Riedel, Ross Taylor, and Robert\nStojnic. AxCell: Automatic extraction of results from machine learning papers. In Bonnie Webber, Trevor Cohn,\nYulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 8580\u20138594, Online, November 2020. Association for Computational Linguistics.\n[53] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and C. V. Jawahar. Icdar2019\ncompetition on scanned receipt ocr and information extraction. In 2019 International Conference on Document\nAnalysis and Recognition (ICDAR), pages 1516\u20131520, 2019.\n[54] Zilong Wang, Yichao Zhou, Wei Wei, Chen-Yu Lee, and Sandeep Tata. VRDU: A benchmark for visually-rich\ndocument understanding. In Ambuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng Yan,\nRavi Kumar, Fatma Ozcan, and Jieping Ye, editors, Proceedings of the 29th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023, pages 5184\u20135193.\nACM, 2023.\n[55] Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis. Evaluation of deep convolutional nets for document\nimage classification and retrieval. In 13th International Conference on Document Analysis and Recognition,\nICDAR 2015, Nancy, France, August 23-26, 2015, pages 991\u2013995. IEEE Computer Society, 2015.\n[56] D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, and J. Heard. Building a test collection for complex\ndocument information processing. In Proceedings of the 29th Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, SIGIR \u201906, page 665\u2013666, New York, NY, USA, 2006.\nAssociation for Computing Machinery.\n[57] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. DocBank: A benchmark\ndataset for document layout analysis. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of\nthe 28th International Conference on Computational Linguistics, pages 949\u2013960, Barcelona, Spain (Online),\nDecember 2020. International Committee on Computational Linguistics.\n[58] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann\nLeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA,\nMay 7-9, 2015, Conference Track Proceedings, 2015.\n[59] Brian L. Davis, Bryan S. Morse, Brian L. Price, Chris Tensmeyer, Curtis Wigington, and Vlad I. Morariu. End-to-\nend document recognition and understanding with dessurt. In Leonid Karlinsky, Tomer Michaeli, and Ko Nishino,\neditors, Computer Vision - ECCV 2022 Workshops - Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part IV,\nvolume 13804 of Lecture Notes in Computer Science, pages 280\u2013296. Springer, 2022.\n[60] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction\ntuning. CoRR, abs/2305.06500, 2023.\n15\n[61] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced\nvisual instruction tuning for text-rich image understanding. CoRR, abs/2306.17107, 2023.\n[62] Lukasz Borchmann, Michal Pietruszka, Tomasz Stanislawek, Dawid Jurkiewicz, Michal Turski, Karolina Szyndler,\nand Filip Gralinski. DUE: end-to-end document understanding benchmark. In Joaquin Vanschoren and Sai-Kit\nYeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021.\n[63] Ali Furkan Biten, Rub\u00e8n Tito, Andr\u00e9s Mafla, Llu\u00eds G\u00f3mez, Mar\u00e7al Rusi\u00f1ol, Minesh Mathew, C. V. Jawahar, Ernest\nValveny, and Dimosthenis Karatzas. ICDAR 2019 competition on scene text visual question answering. In 2019\nInternational Conference on Document Analysis and Recognition, ICDAR 2019, Sydney, Australia, September\n20-25, 2019, pages 1563\u20131570. IEEE, 2019.\n[64] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA,\nJune 7-12, 2015, pages 4566\u20134575. IEEE Computer Society, 2015.\n16\n"
  },
  {
    "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
    "link": "https://arxiv.org/pdf/2401.01335.pdf",
    "upvote": "59",
    "text": "Self-Play Fine-Tuning Converts Weak Language Models\nto Strong Language Models\nZixiang Chen\u2217\u2020\nYihe Deng\u2217\u2021\nHuizhuo Yuan\u2217\u00a7\nKaixuan Ji\u00b6\nQuanquan Gu\u2016\nAbstract\nHarnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is\npivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect\nof growing a strong LLM out of a weak one without the need for acquiring additional human-\nannotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN),\nwhich starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,\nwhere the LLM refines its capability by playing against instances of itself. More specifically, the\nLLM generates its own training data from its previous iterations, refining its policy by discerning\nthese self-generated responses from those obtained from human-annotated data. Our method\nprogressively elevates the LLM from a nascent model to a formidable one, unlocking the full\npotential of human-annotated demonstration data for SFT. Theoretically, we prove that the\nglobal optimum to the training objective function of our method is achieved only when the LLM\npolicy aligns with the target data distribution. Empirically, we evaluate our method on several\nbenchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets\nfrom Big-Bench. Our results show that SPIN can significantly improve the LLM\u2019s performance\nacross a variety of benchmarks and even outperform models trained through direct preference\noptimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the\npromise of self-play, enabling the achievement of human-level performance in LLMs without the\nneed for expert opponents. Codes are available at https://github.com/uclaml/SPIN.\n1\nIntroduction\nLarge Language Models (LLMs) have began a groundbreaking era in artificial general intelligence\n(AGI), demonstrating extraordinary capabilities across a wide range of domains that require in-\ntricate reasoning and specialized knowledge. These models excel in areas such as mathematical\nreasoning/problem solving (Cobbe et al., 2021; Wei et al., 2022; Lewkowycz et al., 2022), code gener-\nation/programming (Chen et al., 2021; Austin et al., 2021; Li et al., 2022), text generation (Bubeck\n\u2217Equal contribution\n\u2020Department\nof\nComputer\nScience,\nUniversity\nof\nCalifornia,\nLos\nAngeles,\nCA\n90095,\nUSA;\ne-mail:\nchenzx19@cs.ucla.edu\n\u2021Department\nof\nComputer\nScience,\nUniversity\nof\nCalifornia,\nLos\nAngeles,\nCA\n90095,\nUSA;\ne-mail:\nyihedeng@cs.ucla.edu\n\u00a7Department\nof\nComputer\nScience,\nUniversity\nof\nCalifornia,\nLos\nAngeles,\nCA\n90095,\nUSA;\ne-mail:\nhzyuan@cs.ucla.edu\n\u00b6Department\nof\nComputer\nScience,\nUniversity\nof\nCalifornia,\nLos\nAngeles,\nCA\n90095,\nUSA;\ne-mail:\nkaixuanji@cs.ucla.edu\n\u2016Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: qgu@cs.ucla.edu\n1\narXiv:2401.01335v2  [cs.LG]  12 Feb 2024\net al., 2023; Anil et al., 2023; Touvron et al., 2023), summarization and creative writing, among\nothers. A significant advancement in LLMs is the post-pretraining alignment with the more de-\nsirable behaviors (Mishra et al., 2021; Victor et al., 2022; Chung et al., 2022; Thoppilan et al.,\n2022), a process often reliant on the costly human-annotated data. Typical alignment methods\ninclude Supervised Fine-Tuning (SFT) (Ouyang et al., 2022; Tunstall et al., 2023a) based on human\ndemonstrations, and Reinforcement Learning from Human Feedback (RLHF) (Christiano et al.,\n2017; Ziegler et al., 2019; Stiennon et al., 2020; Bai et al., 2022a) based on human preferences.\nAll the aforementioned alignment methods require a substantial volume of human annotated data.\nTherefore, there is increasing interest in developing fine-tuning methods that can effectively utilize\nhuman data, thereby streamlining the alignment process. This motivates us to study fine-tuning\nLLMs without the need for additional human-annotated data beyond the fine-tuning dataset. Our\nstudy is also related to the broader goal of converting weak models to strong models without the\nrequirement for extra training data, which is of central interest in machine learning that can be\ntraced back to the boosting algorithms (Kearns and Valiant, 1994; Schapire, 1990; Freund, 1995;\nFreund and Schapire, 1997). The self-training algorithm (Vapnik, 1999; Grandvalet and Bengio,\n2004; Lee, 2013) has also been proved to be able to convert weak learners to strong learners in\nmixture models without the need for additional labeled data (Frei et al., 2022; Kou et al., 2022).\nHowever, the pursuit of autonomously enhancing a weak LLM without external guidance is both\nintriguing and understudied. This raises the following question:\nCan we empower a weak LLM to improve itself without acquiring additional human annotated data?\nIn this paper, we answer this question affirmatively. Inspired by the success of self-play mecha-\nnisms (Samuel, 2000) in games, exemplified by AlphaGo Zero (Silver et al., 2017b), AlphaZero (Silver\net al., 2017a), with historical roots traced back to TD-Gammon (Tesauro et al., 1995), we propose\nto convert a weak LLM to a strong one through the lens of self-play, where the model is enhanced\nby playing against itself without requiring any direct supervision. In particular, we propose a novel\nfine-tuning method called Self-Play fIne-tuNing (SPIN), which begins from a supervised fine-tuned\nmodel. SPIN allows the LLM to engage in self-play, eliminating the need for an expert annotator such\nas a human or more advanced LLMs like GPT-4. In detail, with the LLM from previous iteration t\ndenoted by p\u03b8t, we employ it to generate responses y\u2032 to the prompts x in the human-annotated\nSFT dataset. The subsequent objective is to find a new LLM p\u03b8t+1, capable of distinguishing the\nresponses y\u2032 generated by p\u03b8t from the responses y generated by humans. This process can be seen\nas a two-player game: the main player, or the new LLM p\u03b8t+1, seeks to discern between the responses\nof the opponent player p\u03b8t and human-generated responses, while the opponent, or the old LLM\np\u03b8t, generates responses as similar as possible to those in the human-annotated SFT dataset. The\nnew LLM p\u03b8t+1 is obtained by fine-tuning the old one p\u03b8t to prefer responses from pdata over p\u03b8t,\nresulting in a distribution p\u03b8t+1 that is more aligned with pdata. In the next iteration, the newly\nobtained LLM p\u03b8t+1 becomes the opponent for response generation, with the self-play process aiming\nfor the LLM to eventually converge to p\u03b8\u2217 = pdata, so that the strongest possible LLM can no longer\ndifferentiate the responses generated by its previous version and those generated by the human.\nInterestingly, our method exhibits similarity with the recently introduced direct preference\noptimization (DPO) method (Rafailov et al., 2023), with the notable distinction being the self-play\nnature of our method. Consequently, our approach stands out by eliminating the need for extra\nhuman preference data, a requirement present in the DPO method. Additionally, the self-play\nmechanism in our method resembles the idea of generative adversarial networks (GAN) (Goodfellow\n2\net al., 2014; Arjovsky et al., 2017), albeit that both the discriminator (main player) and the generator\n(the opponent) in our method are instances of the same LLM from different iterations. Theoretically,\nwe prove that our method converges when the distribution of the LLM is identical to the target data\ndistribution, i.e., p\u03b8t = pdata. Our experimental results on zephyr-7b-sft-full (Tunstall et al.,\n2023a), a fine-tuned LLM based on Mistral-7B (Jiang et al., 2023), show that while continued training\nusing SFT on its own SFT dataset Ultrachat200k (Ding et al., 2023) reaches a performance plateau\nor even diminished evaluation scores, our method consistently improves zephyr-7b-sft-full across\nsuccessive iterations while leveraging only a 50k subset of Ultrachat200k dataset. Ultimately, SPIN\neffectively improves the base model\u2019s average score from 58.14 to 63.16 on the HuggingFace Open\nLLM Leaderboard (Beeching et al., 2023) with remarkable 10%+ improvement in scores on GSM8k\nand TruthfulQA, and from 5.94 to 6.78 on MT-Bench (Zheng et al., 2023). Notably, SPIN achieves\nresults that are even comparable to models trained on additional 62k preference dataset (Tunstall\net al., 2023a) on Open LLM leaderboard and MT-Bench.\nConcurrent to our work, Singh et al. (2023) proposed the use of synthetic data with binary\nfeedback in self-training, reducing the reliance on human data. In contrast, our approach eliminates\nthe need for additional binary feedback from humans or an extra reward model thanks to the self-play\nmechanism. Additionally, Burns et al. (2023) employed a weak LLM model as the guidance to\ntrain stronger LLMs in a fashion of weak-to-strong generation. Unlike Burns et al. (2023), which\nnecessitates both a weak supervisor and a strong model, our SPIN operates effectively with a single\nLLM.\nNotation.\nWe use lowercase letters and lowercase boldface letters to denote scalars and vectors,\nrespectively. We use [N] to denote the index set {1, . . . , N}. In the function space, let F be the\nfunction class. The symbol qdata designates the target data distribution, while p represents the\nconditional probability of LLM\u2019s response (i.e., LLM policy).\n2\nRelated Work\nSelf-Play.\nSelf-play (Samuel, 1959; Tesauro et al., 1995), where the algorithm learns by playing\nagainst itself, has gained notable attention due to its effectiveness in multi-agent reinforcement learn-\ning (MARL). This method involves agents engaging in interactions with copies of themselves, enabling\nan increasing level of challenge and complexity within the learning environment. A fundamental\nwork in the field of self-play is AlphaGo Zero (Silver et al., 2017b), which demonstrated exceptional\nperformance against human players using a self-play learning scheme. Subsequent research has\nexpanded upon the concept of self-play, exploring various adaptations and implementations (Anthony\net al., 2017; Lanctot et al., 2017; Bansal et al., 2018; Hernandez-Leal et al., 2018; Muller et al.,\n2019; Vinyals et al., 2019). Our method takes the self-play approach akin to AlphaGo Zero, which\ncan convert a weak model to a strong one without additional human-annotated data. While the\neffectiveness of self-play in MARL is well-established, to our knowledge, our work is the first to apply\nthis approach to the enhancement of LLMs.\nSynthetic Data for LLMs.\nIn the context of supervised fine-tuning (SFT) of LLMs, human-\ncrafted data has proven to be a remarkably effective source that enhances the performance of\nLLMs on tasks such as code generation (Roziere et al., 2023; Yang et al., 2023) and mathematical\nreasoning (Yuan et al., 2023; Luo et al., 2023). While human data typically exhibits high quality,\nacquiring sufficient amount of such data poses a challenge in cost. In light of this consideration, the\nuse of synthetic data has become increasingly popular and considered as a proxy for human data.\nThis approach primarily leverages advanced LLMs such as the GPT series (Radford et al., 2019;\n3\nBrown et al., 2020; OpenAI, 2023) as the guidance to generate high-quality data (Josifoski et al.,\n2023; Taori et al., 2023; Chiang et al., 2023; Li et al., 2023). Recent research has also highlighted\nthe rephrasing capability of LLMs in prompting for better LLM response (Deng et al., 2023; Prasad\net al., 2023) as well as augmenting synthetic data for more effective SFT (Yu et al., 2023; Liu et al.,\n2023). In contrast to prior studies that utilized more advanced models for synthetic data generation\nwhen pre-training or fine-tuning a target model, our approach directly generates synthetic data from\nthe target model itself.\n3\nProblem Setting and Preliminaries\nWe consider a Large Language Model (LLM) parameterized by \u03b8 and denoted by p\u03b8. The model\ntakes as input a sequence x = [x1, . . . , xn], commonly referred to as the prompt, to generate\nthe corresponding response y = [y1, . . . , ym]. The response y is therefore considered as a sample\nfrom the conditional probability distribution p\u03b8(\u00b7|x). In LLMs, xi and yj represent individual\ntokens from a predetermined vocabulary within the sequences x and y, respectively. The auto-\nregressive model p\u03b8 generates tokens sequentially for a given position, leveraging only the sequence\nof previously generated tokens. This model therefore constitutes a Markov process, where the\nconditional probability distribution p\u03b8(y|x) can be expressed through a decomposition as follows:\np\u03b8(y|x) =\nm\nY\nj=1\np\u03b8(yj|x, y<j),\nwhere y<1 is null and y<j = [y1, . . . , yj\u22121] for j = 2, . . . , m. In the following, we review two major\nfine-tuning methods for LLMs: supervised fine-tuning and reinforcement learning (RL) fine-tuning.\n3.1\nSupervised Fine-Tuning\nSupervised fine-tuning (SFT) is employed to tailor a pre-trained LLM to specific downstream tasks,\nleveraging relatively smaller dataset of labeled examples in comparison to the large-scale pre-training\ndata (Ouyang et al., 2022; Yu et al., 2023). In this context, we consider a specific task where the\nprompts, denoted by x, are derived from a specified distribution q(\u00b7). The notation pdata(\u00b7|x) then\nrepresents the probability distribution of the associated high-quality responses y from the training\ndata. Consequently, SFT involves training the LLM to minimize the following negative log-likelihood\nloss associated with these distributions,\nLSFT(\u03b8) = \u2212Ex\u223cq(\u00b7),y\u223cpdata(\u00b7|x)\nh\nlog p\u03b8\n\u0000y|x\n\u0001i\n.\n(3.1)\nIt should be noted that excluding x \u223c q(\u00b7) from the expectation term yields the typical cross-\nentropy loss, expressed as \u2212Ey\u223cpdata(\u00b7|x)[log p\u03b8(y|x)]. LSFT(\u03b8) attains its minimum when the model\u2019s\npredictive distribution p\u03b8(y|x) aligns perfectly with the distribution of the labeled high-quality\nresponses pdata(y|x).\nConsequently, the LLM after SFT is anticipated to generate responses that closely resemble\nthose from pdata(y|x). This procedure is therefore expected to significantly enhance the model\u2019s\nperformance in generating appropriate responses for a specific task.\n3.2\nRL Fine-Tuning\nRL fine-tuning (Christiano et al., 2017; Bai et al., 2022a; Gao et al., 2023a) offers another method for\nenhancing the specific capabilities of general-purpose pre-trained models. Typically, RL fine-tuning\nis employed subsequent to SFT to achieve improved alignment for LLMs (Tunstall et al., 2023a).\n4\nFor a given sequence pair (x, y), RL fine-tuning necessitates a deterministic reward function\nr(x, y). The higher the reward r(x, y), the better the response y is to the given prompt x. The\nobjective of the RL fine-tuning process is then to maximize the following objective function:\nLRL(\u03b8) = Ex\u223cq(\u00b7),y\u223cp\u03b8(\u00b7|x)[r(x, y)] \u2212 \u03bbEx\u223cq(\u00b7)KL\n\u0000p\u03b8(\u00b7|x)||pref(\u00b7|x)\n\u0001\n,\nwhere the Kullback-Leibler (KL) regularization enforces the new model p\u03b8 to be close to the reference\nmodel pref, and \u03bb > 0 is the regularization parameter to control the deviation of the new model\np\u03b8 from the reference model pref. In practice, the reference model pref is often initialized as the\nsupervised fine-tuned model. The inclusion of KL regularization is vital for preventing excessive\ndeviation from the reference model, which in turn reduces the risk of mode collapse.\nMeanwhile, the primary challenge in RL fine-tuning lies in finding a good reward function.\nTypically, this function requires training on a preference dataset. The compilation of such a dataset\ndemands significant resources, often involving comprehensive evaluations either by human annotators,\ni.e., reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022a)\nor strong AI agents, i.e., reinforcement learning from AI feedback (RLAIF) (Bai et al., 2022b).\n4\nMethod\nIn this section, we introduce a new fine-tuning method for enhancing the performance of LLMs\nwithout relying on additional human or AI feedback. Consider a high-quality supervised fine-tuning\n(SFT) dataset SSFT = {(x, y)}n\ni=1, which are sampled from the marginal distribution q(x) and\npdata(y|x). Given a supervised fine-tuned LLM p\u03b80, further application of the SFT approach in (3.1)\nwith SSFT will be ineffective and potentially lead to worse performance. In addition, without human\nand/or AI feedback, it becomes infeasible to acquire a preference dataset for RL fine-tuning (e.g.,\nRLHF and RLAIF). This hinders the application of RL fine-tuning techniques.\nWe evaluate p\u03b80 against SSFT, where p\u03b80 is the LLM achieved by SFT using (3.1). We notice\na persistent quality gap between the groundtruth response y from SSFT and the LLM-generated\nresponse y\u2032 \u223c p\u03b8(\u00b7|x) (refer to Figure 1). This disparity indicates that there is still room for\nimprovement over p\u03b80. Therefore, we propose to use the synthetic data generated by the LLM to\nenhance LLM\u2019s performance starting from p\u03b80 iteratively.\n4.1\nSelf-Play Fine-Tuning (SPIN)\nLet us consider a two-player game, where the main player\u2019s objective is to distinguish the responses\ngenerated by the LLM and those generated by the human. Meanwhile, the opponent\u2019s role is to\ngenerate responses that are indistinguishable from the human\u2019s responses. The core of our method\nis the self-play mechanism, where both the main player and the opponent are the same LLM, but\nfrom different iterations. More specifically, the opponent is the old LLM from the previous iteration,\nand the main player is the new LLM to be learned in the current iteration.\nIn iteration t + 1, the opponent is the LLM from the previous iteration, denoted by p\u03b8t, which\ngenerates responses y\u2032 for those prompts x in the SFT dataset according to p\u03b8t(\u00b7|x). Our method,\ntherefore, consists of the following two steps at iteration t + 1: (1) training the main player, and (2)\nupdating the opponent player.\nTraining the Main Player.\nWe begin with illustrating how we expect a main player is trained to\ndistinguish LLM responses from human responses. Motivated by integral probability metric (IPM)\n(M\u00fcller, 1997), we formulate our objective function such that the main player ft+1 maximizes the\n5\nIn Southampton, what is the most popular form of \ntransportation for commuters?\nI don't have access to current data. However, \nhistorically, the most popular form of transporta-\ntion for commuters in southampton is buses. The \ncity also has a good network of trains and taxis. \nIn Southampton, what is the most popular form of \ntransportation for commuters?\n55% of commuters in Southampton use cars as \ntheir primary mode of transportation, while 23% \nuse public transportation, 12% use bikes, and \n10% walk.\nChosen: ground truth completion\nRejected: model generation @Iter 0\nI do not have access to the latest updates and statistics of southampton. However, based on \nprevious reports, the most popular form of transportation for commuters in southampton is \nthe public transport network, which includes buses, trains, and ferries. The city also has a \nwell-developed cycling infrastructure and many people choose to cycle or walk to work. \nAdditionally, private cars and taxis are also commonly used for commuting.\nModel generation @Iter 1\nFigure 1: Example of ground truth completion compared to the fine-tuned model generation at\niteration 0 and 1. We can observe that the model generation at iteration 0, although fluent, incorrectly\nquantifies transportation preferences with specific percentages that are potentially hallucinations.\nThe model generation at iteration 1 provides a qualitative summary of the transportation forms at\nSouthampton without specific percentage, aligning more closely with the ground truth while adding\nmore details.\nexpected value gap between the target data distribution pdata and the opponent player\u2019s distribution\np\u03b8t:\nft+1 = argmax\nf\u2208Ft\nEx\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0002\nf(x, y) \u2212 f(x, y\u2032)\n\u0003\n,\n(4.1)\nwhere Ft is a sequence of highly expressive function classes that we will determine in later deduction.\nThe subscript t in Ft is due to that the function class is dependent on p\u03b8t. Given such a ft+1 and\na response sequence y to the prompt x, the value of ft+1(x, y) reflects the main player\u2019s degree\nof belief that y originates from pdata rather than p\u03b8t. Ideally, the main player ft+1 should yield a\nhigh value when y \u223c pdata(\u00b7|x) and a low value when y\u2032 \u223c p\u03b8t(\u00b7|x), where p\u03b8t is the opponent\u2019s\ndistribution. Instead of solving (4.1), we can also solve the following more general optimization\nproblem,\nft+1 = argmin\nf\u2208Ft\nEx\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0002\n\u2113\n\u0000f(x, y) \u2212 f(x, y\u2032)\n\u0001\u0003\n,\n(4.2)\nwhere \u2113(\u00b7) is a loss function that is both monotonically decreasing and convex. For example, a linear\nloss function \u2113(t) = \u2212t reduces (4.2) to the minimization version of (4.1). However, the use of a\nlinear loss function results in an unbounded objective value, which, during continuous training, leads\nto a negative infinite value of f(x, y\u2032) on the opponent player\u2019s responses. Therefore, in our work,\nwe choose the logistic loss function \u2113(t) := log(1 + exp(\u2212t)) for its non-negativity, smoothness, and\nexponentially decaying tail as t \u2192 \u221e. Such a choice of loss function aids in preventing the excessive\ngrowth in the absolute value of f.\nUpdating the Opponent Player.\nPreviously we have discussed the training of ft+1 given the\nopponent player\u2019s distribution p\u03b8t. Now suppose we have optimized our main player ft+1 that can\ndistinguish pdata from p\u03b8t, within a certain function class Ft, we elaborate how we get parameter \u03b8t+1\n6\nof the opponent player. Specifically, when presented with two responses y and y\u2032 to the same prompt\nx, ft+1 assesses the values ft+1(x, y) and ft+1(x, y\u2032). It then infers that the response with the higher\nvalue is from the real data distribution pdata and the response with lower value is attributed to the\nLLM p\u03b8t. Subsequently, the objective of the opponent player is to find a better LLM that generates\nresponses indistinguishable from pdata for the main player. This is achieved by maximizing the\nexpected value Ex\u223cq(\u00b7),y\u223cp(\u00b7|x)[ft+1(x, y)]. In addition, to prevent excessive deviation of p\u03b8t+1 from\np\u03b8t and stabilize the self-play, we incorporate a Kullback-Leibler (KL) regularization term. Putting\nthese together gives rise to the following optimization problem:\nargmax\np\nEx\u223cq(\u00b7),y\u223cp(\u00b7|x)[ft+1(x, y)] \u2212 \u03bbEx\u223cq(\u00b7)KL\n\u0000p(\u00b7|x)||p\u03b8t(\u00b7|x)\n\u0001\n,\n(4.3)\nwhere \u03bb > 0 is the regularization parameter. Notably, (4.3) has a closed-form solution bp(\u00b7|x):\nbp(y|x) \u221d p\u03b8t(y|x) exp\n\u0000\u03bb\u22121ft+1(x, y)\n\u0001\n.\n(4.4)\nIt is worth noting that bp(\u00b7|x) is not guaranteed to be belong to the LLM space {p\u03b8(\u00b7|x)|\u03b8 \u2208 \u0398}.\nSince we hope that the closed-form solution bp in the probability space can be realized by an LLM\nwith parameter \u03b8, i.e., p\u03b8(y|x) = bp(y|x), solving for p\u03b8(y|x) \u221d p\u03b8t(y|x) exp\n\u0000\u03bb\u22121ft+1(x, y)\n\u0001\ngives\nft+1(x, y) = \u03bb \u00b7 log p\u03b8(\u00b7|x)\np\u03b8t(\u00b7|x). This suggests the following function class Ft for ft+1:\nFt =\n\u001a\n\u03bb \u00b7 log p\u03b8(y|x)\np\u03b8t(y|x)\n\f\f\f\f\u03b8 \u2208 \u0398\n\u001b\n,\n(4.5)\nwhere \u0398 is the parameter space of LLMs being considered.\nGiven the choice of Ft in (4.5),\noptimizing (4.2) gives ft+1 parameterized by \u03b8t+1 in the following form:\nft+1(x, y) = \u03bb \u00b7 log p\u03b8t+1(y|x)\np\u03b8t(y|x) .\n(4.6)\nSubstituting (4.6) into (4.4) yields bp(y|x) = p\u03b8t+1(y|x). In other words, \u03b8t+1 learned from (4.2) is\nexactly the LLM parameter for our ideal opponent selection.\nEnd-to-end Training Objective.\nWe integrate the previously discussed two steps into a single\nend-to-end training objective with an update rule of \u03b8t+1. Specifically, plugging (4.5) into (4.2)\narrives at the update rule \u03b8t+1 = argmin\u03b8\u2208\u0398 LSPIN(\u03b8, \u03b8t), where LSPIN is the training objective\ndefined as follows\nLSPIN(\u03b8, \u03b8t) = Ex\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0014\n\u2113\n\u0012\n\u03bb log p\u03b8(y|x)\np\u03b8t(y|x) \u2212 \u03bb log p\u03b8(y\u2032|x)\np\u03b8t(y\u2032|x)\n\u0013\u0015\n.\n(4.7)\nWe summarize the iterative self-play process of our method SPIN as follows,\n. . .\n\u2192\np\u03b8t(\u00b7|x)\n| {z }\nOpponent Player at t\n\u2192\n\u03bb \u00b7 log p\u03b8t+1(\u00b7|x)\np\u03b8t(\u00b7|x)\n|\n{z\n}\nMain Player at t + 1\n\u2192\np\u03b8t+1(\u00b7|x)\n|\n{z\n}\nOpponent Player at t + 1\n\u2192\n. . .\nNamely, the opponent player chosen from the previous iteration t is employed to train the main\nplayer at iteration t + 1, resulting in the LLM parameterized by \u03b8t+1. Then we determine the next\nopponent player at iteration t + 1 by directly copying the LLM parameter \u03b8t+1, which is then used\nin training the main player at iteration t + 2. The detailed algorithm is presented in Algorithm 1.\n7\nAlgorithm 1 Self-Play Fine-Tuning (SPIN)\nInput: {(xi, yi)}i\u2208[N]: SFT Dataset, p\u03b80: LLM with parameter \u03b80, T: Number of iterations.\nfor t = 0, . . . , T \u2212 1 do\nfor i = 1, . . . N do\nGenerate synthetic data y\u2032\ni \u223c p\u03b8t(\u00b7|xi).\nend for\nUpdate \u03b8t+1 = argmin\u03b8\u2208\u0398\nP\ni\u2208[N] \u2113\n\u0010\n\u03bb log p\u03b8(yi|xi)\np\u03b8t(yi|xi) \u2212 \u03bb log p\u03b8(y\u2032\ni|xi)\np\u03b8t(y\u2032\ni|xi)\n\u0011\n.\nend for\nOutput: \u03b8T .\nRemark 4.1. (4.7) bears resemblance to direct preference optimization (DPO) (Rafailov et al.,\n2023) for RL fine-tuning. However, SPIN exhibits significant distinctions with DPO. Specifically, SPIN\nis applied to supervised fine-tuning (SFT) and relies solely on the SFT dataset, represented by pairs\n(x, y). In sharp contrast, DPO is designed for RL fine-tuning and necessitates a preference dataset,\nrepresented by (x, yw, yl), where yw and yl denote the winner (chosen) and loser (rejected) responses,\nrespectively. DPO demands that, at the instance level, yw is superior to yl. In comparison, our\nmethod requires that, at the distribution level, the target pdata should be distinguishable from\nthe weak LLM p\u03b8 before it becomes a strong one. In terms of algorithm design, DPO implements a\nsingle-iteration approach, while our method facilitates an iterative self-play strategy, as outlined in\nAlgorithm 1.\n5\nTheoretical Analysis\nIn this section, we provide a theoretical analysis for Algorithm 1 in Section 4. Under monotonicity\nand convexity assumption of the objective function \u2113, we show that the global optimum is obtained\nif and only if parameter \u03b8t generates data distribution. We summarize our assumptions as follows:\nAssumption 5.1. The loss function \u2113(t) : R \u2192 R is monotonically decreasing, i.e., \u2200t, \u2113\u2032(t) \u2264 0 and\nsatisfies \u2113\u2032(0) < 0. In addition, \u2113(t) is a convex function.\nAssumption 5.1 holds for a wide range of loss functions commonly used in machine learning,\nincluding correlation loss \u2113(t) = 1 \u2212 t, hinge loss \u2113(t) = max(0, 1 \u2212 t), exponential loss \u2113(t) = exp(\u2212t)\nand logistic loss \u2113(t) = log(1 + exp(\u2212t)). Under Assumptions 5.1, we present the following theorem,\nwhich is pivotal in understanding the optimization dynamics of our method.\nTheorem 5.2. Under Assumption 5.1, suppose there exists p\u03b8(\u00b7|x) = pdata(\u00b7|x), then we have that\n\u2022 (Sufficiency) If p\u03b8t(\u00b7|x) = pdata(\u00b7|x), then \u03b8t is the global minimum of (4.7) for any \u03bb \u2265 0.\n\u2022 (Necessity) If p\u03b8t(\u00b7|x) \u0338= pdata(\u00b7|x), there exists an appropriately chosen \u03bb, such that \u03b8t is not\nthe global minimum of (4.7).\nRemark 5.3. Theorem 5.2 suggests that under certain conditions, the optimization process of our\nmethod naturally stops at the point p\u03b8(\u00b7|x) = pdata(\u00b7|x), implying the effectiveness of our approach\nin aligning the LLM\u2019s distribution with the target data distribution. Moreover, Theorem 5.2 also\nindicates that the optimization process only stops when the global optimality is achieved, i.e., the\nLLM\u2019s distribution aligns with the target data distribution.\n8\nFor the logistic loss function \u2113(t) = log(1 + exp(\u2212t)), the following theorem gives a more precise\ncharacterization of the opponent player, enabling a better understanding of SPIN.\nTheorem 5.4. Consider the choice of logistic loss \u2113(t) = log(1 + exp(\u2212t)) in SPIN. Suppose that\np\u03b8t(y|x)\n\u0000pdata(y|x)/p\u03b8t(y|x)\n\u00011/\u03bb lies in the LLM space {p\u03b8(y|x)|\u03b8 \u2208 \u0398} and \u03b8t+1 is global minimum\nof LSPIN(\u03b8, \u03b8t), then the opponent player at iteration t + 1 satisfies\np\u03b8t+1(y|x) \u221d p\u03b8t(y|x)\n\u0000pdata(y|x)/p\u03b8t(y|x)\n\u00011/\u03bb.\nRemark 5.5. According to Theorem 5.4, the model update from p\u03b8t(y|x) to p\u03b8t+1(y|x) tends to\nincrease the probability p\u03b8t+1(y|x) when p\u03b8t(y|x) is less than pdata(y|x), and decrease it when p\u03b8t(y|x)\nis greater than pdata(y|x). Thus, Theorem 5.4 further confirms that our method\u2019s optimization\nprocess naturally converges to the point where p\u03b8(\u00b7|x) equals pdata(\u00b7|x). The update of the opponent\nplayer is controlled by\n\u0000pdata(y|x)/p\u03b8t(y|x)\n\u00011/\u03bb, which is regulated by the factor 1/\u03bb. A smaller\n\u03bb results in a larger change of the opponent player, while a larger \u03bb leads to a smaller change.\nTherefore, as p\u03b8(\u00b7|x) approaches pdata(\u00b7|x), increasing \u03bb enhances the stability of LLM training. This\nobservation aligns with (4.3), where \u03bb is the regularization parameter of the KL regularization that\nis employed to control the deviation of the opponent player.\n6\nExperiments\nThis section provides a detailed empirical analysis of SPIN. Our findings highlight several key points:\n(1) SPIN markedly enhances model performance across a wide range of evaluation benchmarks by\nbreaking the limit of SFT; (2) even without introducing new human annotated data, SPIN at iteration\n0 achieves performance on par to DPO training that utilizes even more data; (3) iterative training is\na necessary component in SPIN as it breaks the limit of multi-epoch training.\n6.1\nExperiment Setup\nModel and Datasets.\nIn this study, we adopt zephyr-7b-sft-full as our base model. This\nmodel derives from the pre-trained Mistral-7B (Jiang et al., 2023) and has been further fine-tuned\non the SFT dataset Ultrachat200k1 by HuggingFace. Ultrachat200k represents a high-quality 200k\nsubset of the larger UltraChat (Ding et al., 2023) corpus, which comprises approximately 1.4M\ndialogues produced using OpenAI\u2019s Turbo APIs. From UltraChat200k, We randomly sample 50k\nprompts and use the base model to generate the synthetic responses. We subsequently follow the\noptimization method described in Section 4.1 for further training. In multiple iterations, we leverage\nthe synthetic data from the most recent iteration and add to the newly generated synthetic data,\ntherefore resulting in a synthetic dataset size of 50k at iteration 0 and 100k at iteration 1, 2 and 3.\nAt each iteration, we train our model for 2 epochs.\nEvaluation.\nWe employed the widely used Huggingface Open LLM Leaderboard (Beeching\net al., 2023) as our evaluation benchmark, using the same Language Model Evaluation Harness\nlibrary (Gao et al., 2023b). This leaderboard encompasses 6 different datasets, each focusing on a a\nspecific capability of LLMs. Collectively, these datasets provide a thorough assessment framework,\nevaluating LLMs on commonsense reasoning (Arc (Clark et al., 2018), HellaSwag (Zellers et al.,\n2019), Winogrande (Sakaguchi et al., 2021)), multi-task language understanding (MMLU(Hendrycks\net al., 2020)), human falsehood mimic (TruthfulQA (Lin et al., 2021)) and math problem solving\n1https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k\n9\n(GSM8k (Cobbe et al., 2021)). In evaluation, the language models are prompted with few-shot\nin-context examples and the question. We follow the standard approach and report the average score\nacross all datasets. In Table 1, we detail the evaluation setting adopted by both the leaderboard and\nour experiments. We leave further implementation details to Appendix B.\nTable 1: Detailed information of HuggingFace Open LLM Leaderboard. For each evaluation dataset,\nwe present the number of few-shot examples and metric adopted for evaluation.\nDatasets\nArc\nTruthfulQA\nWinogrande\nGSM8k\nHellaSwag\nMMLU\n# few-shot\n25\n0\n5\n5\n10\n5\nMetric\nacc_norm\nmc2\nacc\nacc\nacc_norm\nacc\n6.2\nSPIN Effectively Improves Benchmark Performance\nSFT\nSPIN\niter-0\nSPIN\niter-1\nSPIN\niter-2\nSPIN\niter-3\n58\n59\n60\n61\n62\n63\nAverage Accuracy (%)\n58.14\n60.80\n62.12\n62.97\n63.16\nHuggingFace Open LLM Benchmark\nFigure 2: The average score of SPIN at different iterations on the HuggingFace Open LLM leaderboard\ndatasets. For \u201cSFT\u201d, we report the performance of our base model zephyr-7b-sft-full, which has\nbeen fine-tuned on the same dataset we use to generate synthetic data.\nWe demonstrate the effectiveness of SPIN using HuggingFace Open LLM Leaderboard as a wide\nrange of evaluation. In Table 2, we compare the performance of our fine-tuned model by SPIN after\niterations 0 to 3 with the base model zephyr-7b-sft-full. We can observe that SPIN exhibits\nremarkable effectiveness in improving the model\u2019s performance by further leveraging the SFT dataset,\non which the base model has already been fully fine-tuned. At iteration 0, where model responses are\ngenerated from zephyr-7b-sft-full, we observe an overall improvement of 2.66% on the average\nscore. The improvement is particularly significant on the TruthfulQA and GSM8k benchmarks, with\nimprovement exceeding 5% and 10% respectively. At iteration 1, we employ the LLM model from\niteration 0 to generate new responses for SPIN, adhering to the procedure outlined in Algorithm 1.\nThis iteration yields further enhancements of 1.32% on average, and especially significant on the Arc\nChallenge and TruthfulQA benchmarks. Subsequent iterations continue this trend of incremental\nimprovement across various tasks. Meanwhile, the improvement at iteration t + 1 is naturally smaller\nthan that at iteration t. As the iterative training progresses, the degree of improvement gradually\n10\napproaches zero, suggesting that the model has reached a limiting point in the last iteration.\nTable 2: Test performance of SPIN based on zephyr-7b-sft-full across HuggingFace Open LLM\nLeaderboard datasets. We also denote the average improvement over last iteration in the Average\ncolumn.\nModel\nArc\nTruthfulQA\nWinogrande\nGSM8k\nHellaSwag\nMMLU\nAverage\nzephyr-7b-sft-full\n60.41\n43.73\n74.19\n26.76\n82.85\n60.92\n58.14\nSPIN iteration 0\n63.40\n49.18\n72.69\n35.10\n84.38\n60.03\n60.80(+2.66)\nSPIN iteration 1\n65.19\n55.17\n72.30\n35.78\n84.96\n59.34\n62.12(+1.32)\nSPIN iteration 2\n65.96\n54.91\n73.56\n38.06\n85.41\n59.93\n62.97(+0.85)\nSPIN iteration 3\n65.87\n54.90\n73.72\n38.97\n85.54\n59.99\n63.16(+0.19)\nComparison with DPO.\nzephyr-7b-beta is a model derived from zephyr-7b-sft-full, trained\nwith DPO on approximately 62k preference data. This data, the UltraFeedback Binarized dataset(Cui\net al., 2023)2, comprises both chosen and rejected completions evaluated by GPT-4. We note that,\nDPO requires either human input or advanced language model feedback to determine the preference,\nmaking data generation a rather expensive procedure. In contrast, our SPIN only requires the initial\nmodel itself. Moreover, unlike DPO which requires new data source, our method exclusively leverages\nthe existing SFT dataset. In Figure 3, we show the performance comparison of SPIN at iterations\n0 and 1 (employing 50k SFT data) with DPO training, from the same SFT checkpoint. We can\nobserve that, while DPO leverages more data from new sources, SPIN based on the existing SFT\ndata can already achieve comparable average performance to DPO training at iteration 0. From\niteration 1, SPIN even surpasses the performance of DPO on the leaderboard benchmark.\nArc\nTruthfulQA\nWinogrande\nGSM8k\nHellaswag\nMMLU\nAverage\n0\n10\n20\n30\n40\n50\n60\n70\n80\nScores\nZephyr-SFT\nZephyr-DPO\nSPIN-iter-0\nSPIN-iter-1\nSPIN-iter-2\nSPIN-iter-3\nFigure 3: Performance comparison with DPO training across the six benchmark datasets. Self-play\nat iteration 0 achieves comparable performance to DPO training with 62k new data. At iteration 1,\nself-play has already surpassed DPO training on the majority of datasets.\n6.3\nAblation Studies\nIn this subsection, we examine the effect of synthetic dataset size and training epochs within an\niteration. Our analysis demonstrates the effectiveness of the synthetic data used by SPIN compared to\nthe SFT data, as well as the necessity of iterative training in SPIN. Furthermore, to comprehensively\n2https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized\n11\nassess the performance improvements of SPIN, we perform additional evaluations on benchmark\ntasks distinct from those in the Open LLM leaderboard.\n0\n14k\n26k\n50k\n200k\n400k\nTraining Size\n58.0\n58.5\n59.0\n59.5\n60.0\n60.5\n61.0\nAverage Score\n59.04\n59.82\n59.27\n58.14\n59.55\n60.16\n60.83\nPerformance Comparison\nSFT\nSPIN\nFigure 4: The scaling effect of training size of SPIN compared to SFT on the average score of\nOpen LLM Leaderboard. For SPIN, we consider training data of sizes 14k, 26k and 50k where\nthe larger dataset contains the smaller dataset. The starting point for SPIN (with x-axis 0) is the\nzephyr-7b-sft-full checkpoint, which has been fine-tuned on Ultrachat200k for 1 epoch. We\nreport the model performance trained for 1 epoch with SPIN on the varying sizes of dataset. We\nadditionally compare with SFT, where we fine-tune Mistral-7B on Ultrachat200k for 3 consecutive\nepochs and report the model performance at the first epoch as the starting point (with x-axis 0).\nTraining Size.\nWe investigate the effect of varying training data size on the performance of SPIN.\nIn Figure 4, we demonstrate the effect of training size for SPIN during iteration 0 and additionally\ncompare with SFT with the full original dataset. Specifically, for the SFT baseline, we fully fine-tune\nMistral-7B on Ultrachat200k for three epochs and report first epoch performance as the starting\npoint (with x-axis 0) in the figure for SFT. For SPIN, we report the zephyr-7b-sft-full checkpoint\nas the starting point, which has also been fine-tuned on Ultrachat200k for one epoch. We select\nthe training size of SPIN at iteration 0 to be 14k, 26k, and 50k and generate the data accordingly,\nensuring that the larger dataset encompasses the smaller dataset. The performance of SPIN was\nthen evaluated after 1 epoch of self-play fine-tuning for each training size. We can observe that,\nwhile SPIN results in notable improvement with increasing training sizes, SFT on further epochs 2\nand 3 fails to yield more than 1% improvement. Lastly, in Table 3, we also show the performance of\nSFT from zephyr-7b-sft-full on Ultrachat200k for one epoch. While self-play fine-tuning with\nsynthetic data from zephyr-7b-sft-full effectively improves its performance, simply fine-tuning it\nagain on the SFT data leads to degraded performance, as similarly observed in Figure 4.\nIterative Training v.s. Training for More Epochs.\nWe further study the training within\niteration 0 and compare with the performance achieved in iteration 1, particularly contrasting the\ntest performance obtained from extended training duration with that from next iteration. Figure 5\ndepicts the performance trajectory of the model trained using SPIN over multiple epochs at iteration\n0. It is evident that the most substantial improvement occurs during the first two epochs, followed by\nonly modest gains in subsequent epochs. Notably, SPIN exhibits robustness and stability; extending\n12\nTable 3: Test performance of zephyr-7b-sft-full fine-tuned on Ultrachat200k for 1 more epoch\nacross HuggingFace Open LLM benchmark datasets. SFT fails to further leverage the fine-tuning\ndata for performance enhancement and even results in degraded performance.\nModel\nArc\nTruthfulQA\nWinogrande\nGSM8k\nHellaSwag\nMMLU\nAverage\nzephyr-7b-sft-full\n60.41\n43.73\n74.19\n26.76\n82.85\n60.92\n58.14\nSFT epoch 1\n57.76\n44.39\n75.77\n25.85\n81.69\n57.89\n57.23\nthe training duration does not diminish performance but rather maintains a rather consistent level.\nNevertheless, the observation suggests an inherent limitation to the performance achievable within\na single iteration, thereby underscoring the necessity for iterative training. As shown by the test\nperformance achieved at iteration 1 in the figures, extending the training in iteration 0 fails to reach\nthe performance comparable to iteration 1.\n0\n1\n2\n3\n4\n5\nEpoch\n61\n62\n63\n64\n65\nAccuracy (%)\niter 0\niter 1 (epoch 2)\n(a) Arc Challenge accuracy.\n0\n1\n2\n3\n4\n5\nEpoch\n44\n46\n48\n50\n52\n54\nAccuracy (%)\niter 0\niter 1 (epoch 2)\n(b) TruthfulQA score.\n0\n1\n2\n3\n4\n5\nEpoch\n58\n59\n60\n61\n62\nAverage Accuracy (%)\niter 0\niter 1 (epoch 2)\n(c) Average score.\nFigure 5: The SPIN training dynamics of zephyr-7b-sft-full on the 50k synthetic data with\nregard to the number of training epochs during iteration 0. We can observe that iterative training is\npivotal as training for more epochs during iteration 0 reaches a limit and cannot surpass iteration 1.\nFurther Investigation on More Tasks.\nHere, we further investigate the performance of SPIN\non a broader variety of tasks, including MT-Bench (Zheng et al., 2023), Big-Bench (bench authors,\n2023) and OpenBookQA (Mihaylov et al., 2018) in addition to the Open LLM Leaderboard tasks.\nSpecifically, we use the following tasks from Big-Bench-Hard for a more comprehensive evaluation,\nincluding Causal Judgment (causal reasoning), Sports Understanding (commonsense reasoning)\nand Formal Fallacies (logical reasoning). In Table 4, we show the resulting scores of SPIN on\nMT-Bench as well as those tasks from Big-Bench. In Figure 6, we detail the model performances on\nMT-Bench with regard to different types of questions. We can see a notably robust improvement\nin the performance of SPIN on various tasks besides the HuggingFace Benchmark, without major\ndegradation. Notably, on MT-Bench, the model fine-tuned by SPIN has surpassed the performance\nof vicuna-13b-v1.5 (Chiang et al., 2023) with a score of 6.57.\n7\nConclusion and Discussion\nThis paper introduces a novel fine-tuning method SPIN, to convert a weak LLM to a strong LLM by\nunleashing the full power of human-annotated data. Central to this method is a self-play mechanism,\n13\nTable 4: Test performance on other reasoning benchmark datasets for SPIN at different iterations\nand zephyr-7b-sft-full. We report the average score for MT-Bench and the accuracy score for\nBig Bench datasets under standard few-shot CoT evaluation. On OpenBookQA, we report acc_norm\nwith 1-shot example as used in Anil et al. (2023). As similar to Open LLM Leaderboard evaluation,\nwe observe a steady improvement in performance on the other benchmark tasks, with no significant\ndegradation.\nModel\nMT-Bench\nBB-causal\nBB-formal\nBB-sports\nOpenBookQA\nzephyr-7b-sft-full\n5.94\n56.15\n49.6\n96.0\n45.4\nSPIN iteration 0\n6.46(+0.52)\n57.75\n51.6\n95.2\n46.8\nSPIN iteration 1\n6.65(+0.19)\n58.82\n51.2\n95.2\n47.2\nSPIN iteration 2\n6.78(+0.13)\n59.36\n51.2\n94.4\n47.6\nWriting\nRoleplay\nReasoning\nMath\nCoding\nExtraction\nSTEM\nHumanities\n0 1 2 3 4 5 6 7 8 9\nmodel\nSPIN iter-2\nSPIN iter-1\nSPIN iter-0\nSFT\nLoading [MathJax]/extensions/MathMenu.js\nFigure 6: Model performance on MT-Bench. We compare SPIN across different iterations with the\nbase SFT model. Starting from iteration 1, our fine-tuned model by SPIN robustly outperforms the\nSFT checkpoint on all evaluation aspects.\nwherein a main player (the LLM) is fine-tuned to differentiate the responses of opponent player (the\nLLM from previous iteration) from the target data distribution, and the LLM is iteratively aligned\nwith the target data distribution. Therefore, SPIN facilitates the LLM\u2019s iterative self-evaluation and\nenhancement through self-play. In comparison to supervised fine-tuning and RL fine-tuning methods,\nSPIN enables the LLM to self-improve without additional human data or feedback from stronger\nLLMs. Empirical results demonstrate that SPIN significantly enhances LLM performance across\ndiverse benchmarks, even outperforming models trained with additional human data or AI feedback.\nLimitation and Future Work.\nOur theoretical results demonstrate that the optimization process\nof SPIN converges if and only if the LLM\u2019s distribution aligns with pdata. Therefore, our study\nfocuses on a fixed target data distribution generated by humans, which inherently imposes a ceiling\non the performance of fine-tuned LLM. Exploring the dynamically changing target data distribution\nis an important direction to overcome this limitation and elevate the LLM\u2019s performance beyond\nthis ceiling or even to a super-human level. Moreover, considering the resource demands of synthetic\ndata generation, another promising avenue for further exploration is to reduce the volume of required\n14\nsynthetic data.\nA\nFurther Related Work\nCurriculum Learning.\nIn deep learning, it has been observed that training models using data\nsamples arranged in a strategically meaningful order can lead to improved performance compared to\ntraining on randomly shuffled data. This approach is commonly known as curriculum learning (Bengio\net al., 2009; Soviany et al., 2022). Initial studies in curriculum learning introduced efficient algorithms\nthat adhere to an \u2018easy-to-hard\u2019 progression (Spitkovsky et al., 2009; Kumar et al., 2010; Lee and\nGrauman, 2011; Zhang et al., 2015). In the field of Natural Language Processing (NLP), criteria\nsuch as sentence length and term frequency are commonly utilized (Cirik et al., 2016; Zhang et al.,\n2018; Liu et al., 2018). More recent developments include the application of curriculum learning\nalgorithms in multi-modal learning (Liu et al., 2021; Wu et al., 2022). Our work shares a similar idea\nto curriculum learning, wherein the training data evolves iteratively\u2014beginning with responses that\nare easy to distinguish from human-annotated data and gradually progressing to more challenging\ninstances.\nGenerative Adversarial Learning.\nGenerative Adversarial Networks (GANs) (Goodfellow et al.,\n2014) represent a distinct class of generative models, characterized by their unique adversarial process.\nTo enhance training stability and data quality, Mao et al. (2017) introduced the Least Squares GAN,\nemploying a least squares loss function for the discriminator. A significant advancement in GANs\ninvolves the use of Integral Probability Metrics (IPM) (M\u00fcller, 1997), particularly highlighted in the\ndevelopment of Wasserstein GAN by Arjovsky et al. (2017). This model employs IPM in its loss\ndesign, enhancing training stability. Since then, IPMs have become popular in the design of GANs\n(Mroueh and Sercu, 2017; Gulrajani et al., 2017), particularly in constraining the discriminator\nto a specific function class, thereby preventing it from overpowering the generator. Furthermore,\nJolicoeur-Martineau (2018) generalized IPM-based GANs by introducing relativistic discriminator\nand proposed Relativistic GAN. It is worth noting that the objective function defined in our (4.2)\nis similar to Relativistic GAN (Jolicoeur-Martineau, 2018) and reduces to an IPM framework in\nWasserstein GAN (Arjovsky et al., 2017) with a linear loss. However, our approach differs in both\nthe choice of the function class and the training procedure. Inspired by GAN, Cheng et al. (2023)\nproposed an adversarial learning framework named Adversarial Preference Optimization (APO)\nthat trains the LLM and a reward model in an adversarial game. Our method is also related to\nGenerative Adversarial Imitation Learning (GAIL) (Ho and Ermon, 2016), which trains separate\ndiscriminator and policy networks in each iteration for imitation learning. In contrast to the above\nmethods, SPIN relies on self-play where both the main player and the opponent player are the same\nLLM from two consecutive iterations.\nB\nExperiment Details\nB.1\nHyperparameters and Implementation Details\nWe use the Alignment Handbook library (Tunstall et al., 2023b) as the codebase for our self-\nplay fine-tuning method SPIN, which includes DeepSpeed ZeRO-3 (Rajbhandari et al., 2020) and\nFlashAttention-2 (Dao, 2023) to reduce training cost. We train our models with RMSProp (Hinton\net al., 2012) optimizer with no weight decay for all iterations as commonly used in fine-tuning LLMs\nfor alignment, with a global batch size of 64, 10% warmup steps and bfloat16 precision. We set the\npeak learning rate to be 5e-7 for iterations 0 and 1, and decay this peak learning rate to 1e-7 for\n15\niteration 2 and 3 as we are approaching the end of self-play fine-tuning. Lastly, we choose \u03b2 = 0.1\nand max sequence length to be 2048 tokens as in Tunstall et al. (2023b). We note that at the last\niteration (iter-3) where the model is close to convergence, we increase the value of \u03b2 to 5.0. We\nuse the Accelerate library (Gugger et al., 2022) to generate our synthetic data using distributed\ninference with multiple GPUs with a global batch size of 64. We consider the prompting template\n\u201c### Instruction: {prompt}\\n\\n### Response: \u201d as commonly used in Taori et al. (2023). For\nUltrachat200k containing multi-round conversations, we only sample the first round as our prompt\nand ground truth completion pairs.\nB.2\nGeneration Examples\nIn Tables 5 and 6, we further provide the generation examples of our fine-tuned model by SPIN\nfrom different iterations. We can observe an improvement in response quality as compared to the\ngeneration of the SFT checkpoint. Meanwhile, the model generations at higher iterations typically\nbecomes more concise than iteration 0 and resemble the ground truth completion better.\nC\nProof of Theorems in Section 5\nC.1\nProof of Theorem 5.2\nProof of Theorem 5.2. To begin with, we prove the \u201cSufficiency\u201d in Theorem 5.2. Since pdata(\u00b7|x) =\np\u03b8t(\u00b7|x), by symmetry property of y and y\u2032, we have for any \u03b8 \u2208 \u0398 that\n2LSPIN(\u03b8, \u03b8t) = Ex\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0014\n\u2113\n\u0012\n\u03b3 log p\u03b8(y|x)\np\u03b8t(y|x) \u2212 \u03b3 log p\u03b8(y\u2032|x)\np\u03b8t(y\u2032|x)\n\u0013\u0015\n+ Ex\u223cq(\u00b7),y\u2032\u223cpdata(\u00b7|x),y\u223cp\u03b8t(\u00b7|x)\n\u0014\n\u2113\n\u0012\n\u03b3 log p\u03b8(y|x)\np\u03b8t(y|x) \u2212 \u03b3 log p\u03b8(y\u2032|x)\np\u03b8t(y\u2032|x)\n\u0013\u0015\n= Ex\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0014\n\u2113\n\u0012\n\u03b3 log p\u03b8(y|x)\np\u03b8t(y|x) \u2212 \u03b3 log p\u03b8(y\u2032|x)\np\u03b8t(y\u2032|x)\n\u0013\n+ \u2113\n\u0012\n\u03b3 log p\u03b8(y\u2032|x)\np\u03b8t(y\u2032|x) \u2212 \u03b3 log p\u03b8(y|x)\np\u03b8t(y|x)\n\u0013\u0015\n\u2265 2Ex\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0014\n\u2113\n\u0012\u03b3\n2 log p\u03b8(y|x)\np\u03b8t(y|x) \u2212 \u03b3\n2 log p\u03b8(y\u2032|x)\np\u03b8t(y\u2032|x)\n+ \u03b3\n2 log p\u03b8(y\u2032|x)\np\u03b8t(y\u2032|x) \u2212 \u03b3\n2 log p\u03b8(y|x)\np\u03b8t(y|x)\n\u0013\u0015\n= 2\u2113(0),\nwhere the inequality is due to Jensen\u2019s inequality (recalling that \u2113 is convex in Assumption 5.1).\nTherefore, we have that LSPIN(\u03b8, \u03b8t) \u2265 \u2113(0) = LSPIN(\u03b8t, \u03b8t), which means that \u03b8t is the global\noptimum of (4.7). As a consequence, the gradient at the point \u03b8t is zero, which concludes \u03b8t+1 = \u03b8t.\nNext, we prove the \u201cNecessity\u201d. Define g(\u03bb) as follows:\ng(\u03bb) = Ex\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0014\n\u2113\n\u0012\n\u03bb log pdata(y|x)\np\u03b8t(y|x) \u2212 \u03bb log pdata(y\u2032|x)\np\u03b8t(y\u2032|x)\n\u0013\u0015\n.\nThen we have g(0) = \u2113(0) and\ng\u2032(0) = Ex\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0014\n\u2113\u2032(0)\n\u0012\nlog pdata(y|x)\np\u03b8t(y|x) \u2212 log pdata(y\u2032|x)\np\u03b8t(y\u2032|x)\n\u0013\u0015\n16\nTable 5: Generation example of our fine-tuned model by SPIN at different iterations.\nPrompt\nHow does the economic growth of Horsham compare to nearby towns?\nGround truth\nI do not have access to the latest economic data on horsham and nearby\ntowns. However, you can check the latest economic statistics and reports\nprovided by the local government or the official websites of horsham and\nnearby towns for a detailed comparison of their economic growth.\nzephyr-7b=sft-full\nAccording to the given material, Horsham was growing \"significantly\"\nfaster than nearby towns, indicating that it was economically thriving.\\n\nHowever, the given material does not specify the exact rates or levels\nof economic growth of Horsham compared to nearby towns. Without\nthis information, it is challenging to determine whether the differences in\neconomic growth between Horsham and nearby towns were significant or\nminimal.\nIteration 0\nI do not have access to the latest economic data for horsham and nearby\ntowns. However, horsham is known to be a prosperous town in west sussex,\nuk, with a diverse economy that includes agriculture, manufacturing, and\nservices. The town has a strong retail sector, and it is also home to\nseveral large companies, including the global pharmaceutical company\nnovartis. In terms of comparison with nearby towns, horsham is generally\nconsidered to be one of the more affluent towns in the region. However,\nthe economic growth of horsham and its neighboring towns may vary\ndepending on various factors such as industry, investment, and government\npolicies.\nIteration 1\nI do not have access to real-time data or information about the most recent\neconomic growth of horsesham and nearby towns. However, according\nto the recent reports from various sources such as the office for national\nstatistics, horsesham\u2019s economy primarily relies on retail, healthcare,\neducation, and public administration industries. Compared to nearby\ntowns such as lewes, chichester, and bognor regis, horsesham\u2019s economy\nmay have a different composition and growth rate based on their specific\nindustries and economic policies.\nIteration 2\nI don\u2019t have access to the most recent economic data. However, according\nto a 2019 report by pwc, horsham\u2019s expected economic growth up to 2025\nwas projected to be around 2.3% per annum, which is slightly below the\naverage for the south east of england (2.8%). Neighbouring towns like\ncrawley and bognor regis were projected to experience similar growth\nrates. note: this may not reflect the impact of the covid-19 pandemic\nthat occurred in 2020.\n= \u2113\u2032(0)\n\u0012\nEx\u223cq(\u00b7),y\u223cpdata(\u00b7|x)\n\u0014\nlog pdata(y|x)\np\u03b8t(y|x)\n\u0015\n\u2212 Ex\u223cq(\u00b7),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0014\nlog pdata(y\u2032|x)\np\u03b8t(y\u2032|x)\n\u0015\u0013\n17\nTable 6: Another generation example of our fine-tuned model by SPIN at different iterations.\nPrompt\nCan you clarify if Poppelsdorfer Allee 45 and the old site of the Mathe-\nmatical Institute are the same location? Answer according to: Go straight\nto Poppelsdorfer Allee 45 and sign-in.\\n There is a big lecture hall at the\nold site of the Mathematical Institute (Wegelerstr. 10, Bonn).\nGround truth\nNo, Poppelsdorfer Allee 45 and the old site of the Mathematical Institute\n(Wegelerstr. 10, Bonn) are not the same location. To sign-in, one should\ngo straight to Poppelsdorfer Allee 45. However, there is a big lecture hall\nat the old site of the Mathematical Institute.\nzephyr-7b=sft-full\nYes, Poppelsdorfer Allee 45 and the old site of the Mathematical Institute\nare the same location.\nIteration 0\nYes, Poppelsdorfer Allee 45 and the old site of the Mathematical Institute\n(Wegelerstr. 10, Bonn) are the same location. The Mathematical Institute\nhas been relocated and renamed as the Institute for Mathematics and\nTheoretical Physics, but the building where it used to be is still referred\nto as the old site. The big lecture hall at the old site still exists and may\nbe used for events or classes.\nIteration 1\nNo, Poppelsdorfer Allee 45 and the old site of the Mathematical Institute\nare not the same location. While Poppelsdorfer Allee 45 is where attendees\nshould sign-in, the old site of the Mathematical Institute (located at\nWegelerstr. 10, Bonn) has a large lecture hall mentioned in the statement.\nTherefore, it can be assumed that the two locations are separate and not\nthe same.\nIteration 2\nPoppelsdorfer Allee 45 and the old site of the Mathematical Institute\n(Wegelerstr. 10, Bonn) are two different locations. As per the instruction,\nyou need to go straight to Poppelsdorfer Allee 45 and sign-in. However,\nthere is a lecture hall mentioned at the old site of the Mathematical\nInstitute (Wegelerstr. 10, Bonn).\n= \u2113\u2032(0)\nh\nKL\n\u0000pdata(\u00b7|x)\n\f\f\f\fp\u03b8t(\u00b7|x)\n\u0001\n+ KL\n\u0000p\u03b8t(\u00b7|x)\n\f\f\f\fpdata(\u00b7|x)\n\u0001i\n< 0,\nwhere the last inequality is due to the condition that \u2113\u2032(0) < 0. Therefore, there exist a \u03bb0 such\nthat for all 0 < \u03bb < \u03bb0, we have g(\u03bb) < \u2113(0). Choose \u03b8\u2217 such that p\u03b8\u2217(y|x) = pdata(y|x). For those\n0 < \u03bb < \u03bb0, we have that\nLSPIN(\u03b8\u2217, \u03b8t) = Ex\u223cq(\u00b7),y\u223cp\u03b8\u2217(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0014\n\u2113\n\u0012\n\u03bb log p\u03b8\u2217(y|x)\np\u03b8t(y|x) \u2212 \u03bb log p\u03b8\u2217(y\u2032|x)\np\u03b8t(y\u2032|x)\n\u0013\u0015\n= Ex\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0014\n\u2113\n\u0012\n\u03bb log pdata(y|x)\np\u03b8t(y|x) \u2212 \u03bb log pdata(y\u2032|x)\np\u03b8t(y\u2032|x)\n\u0013\u0015\n= g(\u03bb)\n18\n< g(0)\n= LSPIN(\u03b8t, \u03b8t),\nwhere the second equality holds by the choice of p\u03b8\u2217(\u00b7|x), and the inequality holds due to the choice\nof \u03bb. Therefore, we conclude that \u03b8t is not the global optimum of (4.7) if p\u03b8t(\u00b7|x) \u0338= pdata(\u00b7|x).\nC.2\nProof Theorem 5.4\nWe need the following auxiliary lemma before we prove Theorem 5.4.\nLemma C.1. Suppose that \u2113(t) = log(1 + exp(\u2212t)) and for a, b > 0, the following inequality holds\na\u2113(t) + b\u2113(\u2212t) \u2265 a log(1 + b/a) + b log(1 + a/b),\nthe equality holds if and only if t = log(a/b).\nProof of Lemma C.1. Define g(t) = a\u2113(t) + b\u2113(\u2212t) = a log(1 + exp(\u2212t)) + b log(1 + exp(t)), then we\nhave\ng\u2032(t) = \u2212 a exp(\u2212t)\n1 + exp(\u2212t) +\nb exp(t)\n1 + exp(t) = \u2212a + b exp(t)\n1 + exp(t)\n.\nTherefore, g\u2032(t) < 0 when t < log(a/b), g\u2032(t) > 0 when t > log(a/b), which indicates that g achieves\nit minimum at t = log(a/b) which concludes the proof.\nLemma C.1 shows that the global minimum of a\u2113(t) + b\u2113(\u2212t) is achieved when t = log(a/b).\nBased on Lemma C.1, we can further prove that (4.2) with the logistic loss function has a closed-form\nsolution if we ignore the constraint set Ft.\nLemma C.2. Denote p+(y, y\u2032, x) = q(x) \u00b7 pdata(y|x) \u00b7 p\u03b8t(y\u2032|x) and p\u2212(y, y\u2032, x) = q(x) \u00b7 p\u03b8t(y\u2032|x) \u00b7\npdata(y|x),\nEx\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0002\n\u2113\n\u0000f(x, y) \u2212 f(x, y\u2032)\n\u0001\u0003\n\u2265 log 2 \u2212 JSD(p+\u2225p\u2212),\nwhere JSD(p+\u2225p\u2212) represents the Jensen\u2013Shannon divergence which is defined as follows\nJSD\n\u0010\np\n\r\r\rq\n\u0011\n= 1\n2KL\n\u0010\np\n\r\r\rp + q\n2\n\u0011\n+ 1\n2KL\n\u0010\nq\n\r\r\rp + q\n2\n\u0011\n,\nwhere KL(\u00b7\u2225\u00b7) is KL-divergence. JSD is always non-negative and equals zero if and only if p+ and\np\u2212 are identical. Moreover, the global minimum value log 2 \u2212 JSD(p+\u2225p\u2212) is achieved by f\u2217 if and\nonly if,\nf\u2217(x, y) = Z(x) + log\n\u0012pdata(y|x)\np\u03b8t(y|x)\n\u0013\n,\nwhere Z(x) is any function that is possibly dependent on x.\nProof of Lemma C.2. We rewrite the objective function in the following formula,\n2Ex\u223cq(\u00b7),y\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0002\n\u2113\n\u0000f(x, y) \u2212 f(x, y\u2032)\n\u0001\u0003\n19\n=\nZ\nq(x)pdata(y|x)p\u03b8t(y\u2032|x)\n\u0002\n\u2113\n\u0000f(x, y) \u2212 f(x, y\u2032)\n\u0001\u0003\ndydy\u2032\n+\nZ\nq(x)pdata(y\u2032|x)p\u03b8t(y|x)\n\u0002\n\u2113\n\u0000f(x, y\u2032) \u2212 f(x, y)\n\u0001\u0003\ndydy\u2032\n=\nZ\nq(x)pdata(y|x)p\u03b8t(y\u2032|x)\u2113\n\u0000f(x, y) \u2212 f(x, y\u2032)\n\u0001\n+ q(x)pdata(y\u2032|x)p\u03b8t(y|x)\u2113\n\u0000f(x, y\u2032) \u2212 f(x, y)\n\u0001\ndydy\u2032\n(i)\n\u2265\nZ\nq(x)pdata(y|x)p\u03b8t(y\u2032|x) log\n\u0012\n1 + pdata(y\u2032|x)p\u03b8t(y|x)\npdata(y|x)p\u03b8t(y\u2032|x)\n\u0013\n+ q(x)pdata(y\u2032|x)p\u03b8t(y|x) log\n\u0012\n1 + pdata(y|x)p\u03b8t(y\u2032|x)\npdata(y\u2032|x)p\u03b8t(y|x)\n\u0013\ndydy\u2032,\nwhere the inequality is due to a\u2113(t) + b\u2113(\u2212t) \u2265 a log(1 + b/a) + b log(1 + a/b) in Lemma C.1 with\na = q(x)pdata(y|x)p\u03b8t(y\u2032|x), b = q(x)pdata(y\u2032|x)p\u03b8t(y|x), t = f(x, y) \u2212 f(x, y\u2032). The equality (i)\nholds if and only if the following equation holds almost surely for any x, y, y\u2032,\nf(x, y) \u2212 f(x, y\u2032) = log\n\u0012pdata(y|x)p\u03b8t(y\u2032|x)\npdata(y\u2032|x)p\u03b8t(y|x)\n\u0013\n.\n(C.1)\nEquation (C.1) is equivalent to\nf(x, y) \u2212 log\n\u0012pdata(y|x)\np\u03b8t(y|x)\n\u0013\n= f(x, y\u2032) \u2212 log\n\u0012pdata(y\u2032|x)\np\u03b8t(y\u2032|x)\n\u0013\nholds almost surely for any x, y, y\u2032. Therefore, the equality (i) holds if and only if there exists some\nZ(x) such that\nf(x, y) = Z(x) + log\n\u0012pdata(y|x)\np\u03b8t(y|x)\n\u0013\n.\nRecall that p+(y, y\u2032|x) = pdata(y|x) \u00b7 p\u03b8t(y|x) and p\u2212(y, y\u2032|x) = p\u03b8t(y|x) \u00b7 pdata(y|x). Then, the\nright-hand side of (i) can be written as\nZ\nq(x)pdata(y|x)p\u03b8t(y\u2032|x) log\n\u0012\n1 + pdata(y\u2032|x)p\u03b8t(y|x)\npdata(y|x)p\u03b8t(y\u2032|x)\n\u0013\n+ q(x)pdata(y\u2032|x)p\u03b8t(y|x) log\n\u0012\n1 + pdata(y|x)p\u03b8t(y\u2032|x)\npdata(y\u2032|x)p\u03b8t(y|x)\n\u0013\ndydy\u2032\n=\nZ\np+(y, y\u2032|x) log\n\u0012\n1 + p\u2212(y, y\u2032|x)\np+(y, y\u2032|x)\n\u0013\n+ p\u2212(y, y\u2032|x) log\n\u0012\n1 + p+(y, y\u2032|x)\np\u2212(y, y\u2032|x)\n\u0013\ndydy\u2032\n= 2 log 2 +\nZ\np+(y, y\u2032|x) log\n\u00121/2[p\u2212(y, y\u2032|x) + p+(y, y\u2032|x)]\np+(y, y\u2032|x)\n\u0013\n+ p\u2212(y, y\u2032|x) log\n\u00121/2[p\u2212(y, y\u2032|x) + p+(y, y\u2032|x)]\np\u2212(y, y\u2032|x)\n\u0013\ndydy\u2032\n= 2 log 2 \u2212 KL\n\u0012\np+\n\r\r\r\r\np+ + p\u2212\n2\n\u0013\n\u2212 KL\n\u0012\np\u2212\n\r\r\r\r\np+ + p\u2212\n2\n\u0013\n= 2 log 2 \u2212 2 \u00b7 JSD(p+\u2225p\u2212),\nwhere the last equality is by the definition of JSD. This concludes the proof.\n20\nLemma C.2 provides a closed-form solution to (4.2) if we ignore the constraint set Ft. If this\nclosed-form solution belongs to Ft, then it should also be the solution to (4.2). This observation is\nthe key to the proof of Theorem 5.4.\nProof of Theorem 5.4. Under the condition of Theorem 5.4, there exists a p\u03b8 such that\np\u03b8(y|x) \u221d p\u03b8t(y|x)\n\u0000pdata(y|x)/p\u03b8t(y|x)\n\u00011/\u03bb.\nTherefore, there exists a function bZ(x) such that\np\u03b8(y|x) = bZ(x) \u00b7 p\u03b8t(y|x)\n\u0000pdata(y|x)/p\u03b8t(y|x)\n\u00011/\u03bb.\n(C.2)\nApplying logarithm function on both side of (C.2) yields\n\u03bb log( bZ(x)) + log\n\u0012pdata(y|x)\np\u03b8t(y|x)\n\u0013\n= \u03bb log\n\u0012 p\u03b8(y|x)\np\u03b8t(y|x)\n\u0013\n\u2208 Ft.\nBy Lemma C.2, f\u2217(x, y) = \u03bb log( bZ(x)) + log\n\u0000 pdata(y|x)\np\u03b8t(y|x)\n\u0001\nis the global minimum of the following\nminimization problem,\nargmin\nf\nEy\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0002\n\u2113\n\u0000f(x, y) \u2212 f(x, y\u2032)\n\u0001\u0003\n.\n(C.3)\nSince f\u2217 \u2208 Ft, f\u2217(x, y) = \u03bb log( bZ(x))+log\n\u0000 pdata(y|x)\np\u03b8t(y|x)\n\u0001\nis also the global optimum of the optimization\nproblem (4.2),\nargmin\nf\u2208Ft\nEy\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0002\n\u2113\n\u0000f(x, y) \u2212 f(x, y\u2032)\n\u0001\u0003\n.\nTherefore, we have proved that\nmin\nf\nEy\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0002\n\u2113\n\u0000f(x, y) \u2212 f(x, y\u2032)\n\u0001\u0003\n= min\nf\u2208Ft Ey\u223cpdata(\u00b7|x),y\u2032\u223cp\u03b8t(\u00b7|x)\n\u0002\n\u2113\n\u0000f(x, y) \u2212 f(x, y\u2032)\n\u0001\u0003\n= min\n\u03b8\u2208\u0398 LSPIN(\u03b8, \u03b8t).\n(C.4)\nSince \u03b8t+1 is the global minimum of LSPIN(\u03b8, \u03b8t). Then by (C.4), \u03bb log\n\u0010 p\u03b8t+1(y|x)\np\u03b8t(y|x)\n\u0011\nshould be the\nglobal minimum of problem (C.3). By Lemma C.2, there exists Z(x) such that\n\u03bb log\n\u0012p\u03b8t+1(y|x)\np\u03b8t(y|x)\n\u0013\n= Z(x) + log\n\u0012pdata(y|x)\np\u03b8t(y|x)\n\u0013\n,\nwhich leads to the result that p\u03b8t+1(y|x) \u221d p\u03b8t(y|x)\n\u0000pdata(y|x)/p\u03b8t(y|x)\n\u00011/\u03bb.\n21\nReferences\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S.,\nTaropa, E., Bailey, P., Chen, Z. et al. (2023). Palm 2 technical report. arXiv preprint\narXiv:2305.10403 .\nAnthony, T., Tian, Z. and Barber, D. (2017). Thinking fast and slow with deep learning and\ntree search. Advances in neural information processing systems 30.\nArjovsky, M., Chintala, S. and Bottou, L. (2017). Wasserstein generative adversarial networks.\nIn International conference on machine learning. PMLR.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai,\nC., Terry, M., Le, Q. et al. (2021). Program synthesis with large language models. arXiv\npreprint arXiv:2108.07732 .\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort,\nS., Ganguli, D., Henighan, T. et al. (2022a). Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 .\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie,\nA., Mirhoseini, A., McKinnon, C. et al. (2022b). Constitutional ai: Harmlessness from ai\nfeedback. arXiv preprint arXiv:2212.08073 .\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I. and Mordatch, I. (2018). Emergent\ncomplexity via multi-agent competition. In International Conference on Learning Representations.\nBeeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O.,\nTunstall, L. and Wolf, T. (2023). Open llm leaderboard.\nbench authors, B. (2023). Beyond the imitation game: Quantifying and extrapolating the\ncapabilities of language models. Transactions on Machine Learning Research .\nBengio, Y., Louradour, J., Collobert, R. and Weston, J. (2009). Curriculum learning. In\nProceedings of the 26th annual international conference on machine learning.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan,\nA., Shyam, P., Sastry, G., Askell, A. et al. (2020). Language models are few-shot learners.\nAdvances in neural information processing systems 33 1877\u20131901.\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee,\nP., Lee, Y. T., Li, Y., Lundberg, S. et al. (2023). Sparks of artificial general intelligence:\nEarly experiments with gpt-4. arXiv preprint arXiv:2303.12712 .\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen,\nY., Ecoffet, A., Joglekar, M., Leike, J. et al. (2023). Weak-to-strong generalization:\nEliciting strong capabilities with weak supervision .\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H.,\nBurda, Y., Joseph, N., Brockman, G. et al. (2021). Evaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374 .\n22\nCheng, P., Yang, Y., Li, J., Dai, Y. and Du, N. (2023). Adversarial preference optimization.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S.,\nZhuang, Y., Gonzalez, J. E., Stoica, I. and Xing, E. P. (2023). Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S. and Amodei, D. (2017).\nDeep reinforcement learning from human preferences. Advances in neural information processing\nsystems 30.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X.,\nDehghani, M., Brahma, S. et al. (2022). Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416 .\nCirik, V., Hovy, E. and Morency, L.-P. (2016). Visualizing and understanding curriculum\nlearning for long short-term memory networks. arXiv preprint arXiv:1611.06204 .\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C. and Tafjord,\nO. (2018). Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457 .\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M.,\nTworek, J., Hilton, J., Nakano, R. et al. (2021). Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168 .\nCui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z. and Sun, M. (2023).\nUltrafeedback: Boosting language models with high-quality feedback.\nDao, T. (2023). Flashattention-2: Faster attention with better parallelism and work partitioning.\narXiv preprint arXiv:2307.08691 .\nDeng, Y., Zhang, W., Chen, Z. and Gu, Q. (2023). Rephrase and respond: Let large language\nmodels ask better questions for themselves. arXiv preprint arXiv:2311.04205 .\nDing, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu, Z., Sun, M. and Zhou, B.\n(2023). Enhancing chat language models by scaling high-quality instructional conversations. arXiv\npreprint arXiv:2305.14233 .\nFrei, S., Zou, D., Chen, Z. and Gu, Q. (2022). Self-training converts weak learners to strong\nlearners in mixture models. In International Conference on Artificial Intelligence and Statistics.\nPMLR.\nFreund, Y. (1995). Boosting a weak learning algorithm by majority. Information and computation\n121 256\u2013285.\nFreund, Y. and Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning\nand an application to boosting. Journal of computer and system sciences 55 119\u2013139.\nGao, L., Schulman, J. and Hilton, J. (2023a). Scaling laws for reward model overoptimization.\nIn International Conference on Machine Learning. PMLR.\n23\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding,\nL., Hsu, J., Le Noac\u2019h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A.,\nWang, B., Wang, K. and Zou, A. (2023b). A framework for few-shot language model evaluation.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\nCourville, A. and Bengio, Y. (2014).\nGenerative adversarial nets.\nAdvances in neural\ninformation processing systems 27.\nGrandvalet, Y. and Bengio, Y. (2004). Semi-supervised learning by entropy minimization.\nAdvances in neural information processing systems 17.\nGugger, S., Debut, L., Wolf, T., Schmid, P., Mueller, Z., Mangrulkar, S., Sun, M.\nand Bossan, B. (2022). Accelerate: Training and inference at scale made simple, efficient and\nadaptable.\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V. and Courville, A. C. (2017).\nImproved training of wasserstein gans. Advances in neural information processing systems 30.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J.\n(2020). Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 .\nHernandez-Leal, P., Kartal, B. and Taylor, M. E. (2018). Is multiagent deep reinforcement\nlearning the answer or the question? a brief survey. learning 21 22.\nHinton, G., Srivastava, N. and Swersky, K. (2012). Neural networks for machine learning\nlecture 6a overview of mini-batch gradient descent. Cited on 14 2.\nHo, J. and Ermon, S. (2016). Generative adversarial imitation learning. Advances in neural\ninformation processing systems 29.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D.\nd. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L. et al. (2023). Mistral 7b.\narXiv preprint arXiv:2310.06825 .\nJolicoeur-Martineau, A. (2018). The relativistic discriminator: a key element missing from\nstandard gan. arXiv preprint arXiv:1807.00734 .\nJosifoski, M., Sakota, M., Peyrard, M. and West, R. (2023). Exploiting asymmetry for\nsynthetic training data generation: Synthie and the case of information extraction. arXiv preprint\narXiv:2303.04132 .\nKearns, M. and Valiant, L. (1994). Cryptographic limitations on learning boolean formulae and\nfinite automata. Journal of the ACM (JACM) 41 67\u201395.\nKou, Y., Chen, Z., Cao, Y. and Gu, Q. (2022).\nHow does semi-supervised learning with\npseudo-labelers work? a case study. In The Eleventh International Conference on Learning\nRepresentations.\nKumar, M., Packer, B. and Koller, D. (2010). Self-paced learning for latent variable models.\nAdvances in neural information processing systems 23.\n24\nLanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., P\u00e9rolat, J., Silver,\nD. and Graepel, T. (2017). A unified game-theoretic approach to multiagent reinforcement\nlearning. Advances in neural information processing systems 30.\nLee, D.-H. (2013). Pseudo-label: The simple and efficient semi-supervised learning method for deep\nneural networks. In ICML Challenges in Representation Learning Workshop.\nLee, Y. J. and Grauman, K. (2011). Learning the easy things first: Self-paced visual category\ndiscovery. In CVPR 2011. IEEE.\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V.,\nSlone, A., Anil, C., Schlag, I., Gutman-Solo, T. et al. (2022). Solving quantitative\nreasoning problems with language models. Advances in Neural Information Processing Systems\n35 3843\u20133857.\nLi, Y., Bubeck, S., Eldan, R., Giorno, A. D., Gunasekar, S. and Lee, Y. T. (2023).\nTextbooks are all you need ii: phi-1.5 technical report.\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T.,\nKeeling, J., Gimeno, F., Dal Lago, A. et al. (2022). Competition-level code generation\nwith alphacode. Science 378 1092\u20131097.\nLin, S., Hilton, J. and Evans, O. (2021). Truthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958 .\nLiu, B., Bubeck, S., Eldan, R., Kulkarni, J., Li, Y., Nguyen, A., Ward, R. and Zhang,\nY. (2023). Tinygsm: achieving> 80% on gsm8k with small language models. arXiv preprint\narXiv:2312.09241 .\nLiu, C., He, S., Liu, K., Zhao, J. et al. (2018). Curriculum learning for natural answer generation.\nIn IJCAI.\nLiu, F., Ge, S. and Wu, X. (2021). Competence-based multimodal curriculum learning for medical\nreport generation. In Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume\n1: Long Papers).\nLuo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S. and\nZhang, D. (2023). Wizardmath: Empowering mathematical reasoning for large language models\nvia reinforced evol-instruct. arXiv preprint arXiv:2308.09583 .\nMao, X., Li, Q., Xie, H., Lau, R. Y., Wang, Z. and Paul Smolley, S. (2017). Least squares\ngenerative adversarial networks. In Proceedings of the IEEE international conference on computer\nvision.\nMihaylov, T., Clark, P., Khot, T. and Sabharwal, A. (2018). Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing.\nMishra, S., Khashabi, D., Baral, C. and Hajishirzi, H. (2021). Cross-task generalization via\nnatural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773 .\n25\nMroueh, Y. and Sercu, T. (2017). Fisher gan. Advances in neural information processing systems\n30.\nM\u00fcller, A. (1997). Integral probability metrics and their generating classes of functions. Advances\nin applied probability 29 429\u2013443.\nMuller, P., Omidshafiei, S., Rowland, M., Tuyls, K., Perolat, J., Liu, S., Hennes, D.,\nMarris, L., Lanctot, M., Hughes, E. et al. (2019). A generalized training approach for\nmultiagent learning. arXiv preprint arXiv:1909.12823 .\nOpenAI (2023). Gpt-4 technical report.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C.,\nAgarwal, S., Slama, K., Ray, A. et al. (2022). Training language models to follow instructions\nwith human feedback. Advances in Neural Information Processing Systems 35 27730\u201327744.\nPrasad, A., Stengel-Eskin, E. and Bansal, M. (2023). Rephrase, augment, reason: Visual\ngrounding of questions for vision-language models. arXiv preprint arXiv:2310.05861 .\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I. et al. (2019).\nLanguage models are unsupervised multitask learners. OpenAI blog 1 9.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D. and Finn, C. (2023).\nDirect preference optimization: Your language model is secretly a reward model. arXiv preprint\narXiv:2305.18290 .\nRajbhandari, S., Rasley, J., Ruwase, O. and He, Y. (2020). Zero: Memory optimizations\ntoward training trillion parameter models. In SC20: International Conference for High Performance\nComputing, Networking, Storage and Analysis. IEEE.\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J.,\nRemez, T., Rapin, J. et al. (2023). Code llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950 .\nSakaguchi, K., Bras, R. L., Bhagavatula, C. and Choi, Y. (2021). Winogrande: An adversarial\nwinograd schema challenge at scale. Communications of the ACM 64 99\u2013106.\nSamuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journal\nof research and development 3 210\u2013229.\nSamuel, A. L. (2000). Some studies in machine learning using the game of checkers. IBM Journal\nof research and development 44 206\u2013226.\nSchapire, R. E. (1990). The strength of weak learnability. Machine learning 5 197\u2013227.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot,\nM., Sifre, L., Kumaran, D., Graepel, T. et al. (2017a). Mastering chess and shogi by\nself-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815 .\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A.,\nHubert, T., Baker, L., Lai, M., Bolton, A. et al. (2017b). Mastering the game of go\nwithout human knowledge. nature 550 354\u2013359.\n26\nSingh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil, P., Liu, P. J., Harrison,\nJ., Lee, J., Xu, K., Parisi, A. et al. (2023). Beyond human data: Scaling self-training for\nproblem-solving with language models. arXiv preprint arXiv:2312.06585 .\nSoviany, P., Ionescu, R. T., Rota, P. and Sebe, N. (2022). Curriculum learning: A survey.\nInternational Journal of Computer Vision 130 1526\u20131565.\nSpitkovsky, V. I., Alshawi, H. and Jurafsky, D. (2009). Baby steps: How \u201cless is more\u201d in\nunsupervised dependency parsing. In NIPS 2009 Workshop on Grammar Induction, Representation\nof Language and Language Learning.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei,\nD. and Christiano, P. F. (2020). Learning to summarize with human feedback. Advances in\nNeural Information Processing Systems 33 3008\u20133021.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P. and\nHashimoto, T. B. (2023). Stanford alpaca: An instruction-following llama model.\nTesauro, G. et al. (1995). Temporal difference learning and td-gammon. Communications of the\nACM 38 58\u201368.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T.,\nJin, A., Bos, T., Baker, L., Du, Y. et al. (2022). Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239 .\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov,\nN., Batra, S., Bhargava, P., Bhosale, S. et al. (2023). Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288 .\nTunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S.,\nvon Werra, L., Fourrier, C., Habib, N. et al. (2023a). Zephyr: Direct distillation of lm\nalignment. arXiv preprint arXiv:2310.16944 .\nTunstall, L., Beeching, E., Lambert, N., Rajani, N., Rush, A. M. and Wolf, T. (2023b).\nThe alignment handbook.\nVapnik, V. (1999). The nature of statistical learning theory. Springer science & business media.\nVictor, S., Albert, W., Colin, R., Stephen, B., Lintang, S., Zaid, A., Antoine, C.,\nArnaud, S., Arun, R., Manan, D. et al. (2022). Multitask prompted training enables\nzero-shot task generalization. In International Conference on Learning Representations.\nVinyals, O., Babuschkin, I., Chung, J., Mathieu, M., Jaderberg, M., Czarnecki, W.,\nDudzik, A., Huang, A., Georgiev, P., Powell, R., Ewalds, T., Horgan, D., Kroiss,\nM., Danihelka, I., Agapiou, J., Oh, J., Dalibard, V., Choi, D., Sifre, L., Sulsky, Y.,\nVezhnevets, S., Molloy, J., Cai, T., Budden, D., Paine, T., Gulcehre, C., Wang, Z.,\nPfaff, T., Pohlen, T., Yogatama, D., Cohen, J., McKinney, K., Smith, O., Schaul, T.,\nLillicrap, T., Apps, C., Kavukcuoglu, K., Hassabis, D. and Silver, D. (2019). AlphaStar:\nMastering the Real-Time Strategy Game StarCraft II.\n27\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D.\net al. (2022). Chain-of-thought prompting elicits reasoning in large language models. Advances\nin Neural Information Processing Systems 35 24824\u201324837.\nWu, J., Liang, Y., Akbari, H., Wang, Z., Yu, C. et al. (2022). Scaling multimodal pre-training\nvia cross-modality gradient harmonization. Advances in Neural Information Processing Systems\n35 36161\u201336173.\nYang, Y., Singh, A. K., Elhoushi, M., Mahmoud, A., Tirumala, K., Gloeckle, F., Rozi\u00e8re,\nB., Wu, C.-J., Morcos, A. S. and Ardalani, N. (2023). Decoding data quality via synthetic\ncorruptions: Embedding-guided pruning of code data. arXiv preprint arXiv:2312.02418 .\nYu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A.\nand Liu, W. (2023). Metamath: Bootstrap your own mathematical questions for large language\nmodels. arXiv preprint arXiv:2309.12284 .\nYuan, Z., Yuan, H., Li, C., Dong, G., Tan, C. and Zhou, C. (2023). Scaling relationship on\nlearning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825 .\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A. and Choi, Y. (2019). Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830 .\nZhang, D., Meng, D., Li, C., Jiang, L., Zhao, Q. and Han, J. (2015). A self-paced multiple-\ninstance learning framework for co-saliency detection. In Proceedings of the IEEE international\nconference on computer vision.\nZhang, X., Kumar, G., Khayrallah, H., Murray, K., Gwinnup, J., Martindale, M. J.,\nMcNamee, P., Duh, K. and Carpuat, M. (2018). An empirical exploration of curriculum\nlearning for neural machine translation. arXiv preprint arXiv:1811.00739 .\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li,\nD., Xing, E. et al. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv\npreprint arXiv:2306.05685 .\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano,\nP. and Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint\narXiv:1909.08593 .\n28\n"
  },
  {
    "title": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer",
    "link": "https://arxiv.org/pdf/2401.01055.pdf",
    "upvote": "49",
    "text": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer\nJun Zhao*, Zhihao Zhang*, Luhui Gao, Qi Zhang\u2020, Tao Gui, Xuanjing Huang\n1School of Computer Science, Fudan University\n{zhaoj19,zhangzhihao19,qz,tgui}@fudan.edu.cn\nAbstract\nIn recent times, substantial advancements have been wit-\nnessed in large language models (LLMs), exemplified by\nChatGPT, showcasing remarkable proficiency across a range\nof complex tasks. However, many mainstream LLMs (e.g.\nLLaMA) are pretrained on English-dominant corpus, which\nlimits their performance in other non-English languages.\nIn this paper, we focus on how to effectively transfer the\ncapabilities of language generation and following instruc-\ntions to a non-English language. To answer this question,\nwe conduct an extensive empirical investigation based on\nLLaMA, accumulating over 1440 GPU hours. We analyze\nthe impact of key factors such as vocabulary extension,\nfurther pretraining, and instruction tuning on transfer. To\naccurately assess the model\u2019s level of knowledge, we employ\nfour widely used standardized testing benchmarks: C-Eval,\nMMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a\ncomprehensive evaluation of the model\u2019s response quality is\nconducted, considering aspects such as accuracy, fluency, in-\nformativeness, logical coherence, and harmlessness, based on\nLLM-Eval, a benchmarks consisting instruction tasks from\n17 diverse categories. Our evaluation results demonstrate that\ncomparable performance to state-of-the-art transfer models\ncan be achieved with less than 1% of the pretraining data,\nboth in terms of knowledge alignment and response quality.\nFurthermore, the experimental outcomes across the thirteen\nlow-resource languages also exhibit similar trends. We antic-\nipate that the conclusions revealed by the experiments will\naid the community in developing non-English LLMs.\nIntroduction\nFor decades, researchers in Natural Language Processing\n(NLP) have been exploring the fundamental principles of\nintelligence (Bubeck et al. 2023). The recent advances in\nlarge language models (LLMs) seem to have revealed a\nglimmer of hope. Benefitting from the unprecedented scales\nof model size and training data, many LLMs like ChatGPT\n(OpenAI 2022), PaLM (Anil et al. 2023), LLaMA (Touvron\net al. 2023a), and others have emerged strong capabilities\nin reasoning (Cobbe et al. 2021), planning (Huang et al.\n2022), and learning from experience (Dong et al. 2023)\nat or surpassing human levels. These general capabilities\nalso provide a foundation for LLMs to address intricate\n*These authors contributed equally.\n\u2020Corresponding Author\nVocabulary \nExtension\nFurther \nPretraining\nInstruction\nTuning\nVocabulary \nExtension\nFurther \nPretraining\nInstruction\nTuning\nVocabulary \nExtension\nFurther \nPretraining\nInstruction\nTuning\nLLaMA\nEnglish\nLLaMA\nChinese\nLLaMA\nHindi\nLLaMA\nArabic\nFigure 1: Pretrained LLaMA models, which are primarily\ntrained on English-dominated corpus (as depicted on the\nleft), are not inherently proficient in handling non-English\nlanguages. We aim to investigate the necessity of vocabulary\nextension, further pretraining, and instruction tuning, as\nwell as to what extent they influence the capability\ntransfer. This exploration enables us to efficiently transfer\nLLaMA\u2019s language capabilities to non-English languages\n(as illustrated on the right), minimizing costs in the process.\nreal-world tasks, such as successfully completing the entire\nUniform Bar Examination (UBE) (Katz et al. 2023) or\ncoding based on natural language instructions (StabilityAI\n2023).\nMany well-known LLMs are capable of comprehending\ninput and generating responses across different languages,\nthanks to their pretraining on a diverse mix of corpus\nfrom multiple languages. However, due to the imbalanced\ndistribution of language resources, collecting extensive\ntraining data for all languages is nearly impossible (Ranta\nand Goutte 2021). Taking the representative LLM BLOOM\n(Scao et al. 2023) as an example, it has been pretrained on\n46 natural languages. Yet, this number accounts for only\n0.66% of the roughly 7, 000 languages currently in use.\nMoreover, within the corpus of these 46 languages, there\nexists extreme imbalance, with the high-resource English\ntexts being 2.8 million times more than that of the low-\nresource Chitumbuka language. This is not an isolated case.\nAnother widely discussed language model, LLaMA, has\narXiv:2401.01055v2  [cs.CL]  12 Jan 2024\nbeen pretrained primarily on English-dominated corpus,\nsupplemented with limited data from 20 related languages\nthat utilize the Latin and Cyrillic scripts. As a result,\nLLaMA exhibits inferior performance in contexts involving\nnon-English languages where it has not undergone suffi-\ncient training. Some researchers collect large-scale data for\nspecific languages of interest and retrain an LLM (Team\n2023a). However, this inevitably leads to high computational\nand data collection costs, which is not suitable for low-\nresource languages. While Cui, Yang, and Yao (2023b)\nextend original vocabulary and further pretrain LLaMA with\n30B Chinese tokens by LoRA (Hu et al. 2021), reporting\npromising results. Nonetheless, a fine-grained systematic\ninvestigation of the transfer process remains lacking.\nIn this work, we take a step towards gaining a compre-\nhensive understanding of the language capability transfer\nin LLMs. As shown in figure 1, we empirically investigate\nseveral key aspects based on LLaMA:\n(1) The impact of vocabulary extension on transfer.\nWe find that further pretraining with 0.5 billion Chinese\ntokens on the original vocabulary significantly outperforms\nperformance on the extended vocabulary, even though the\nlatter has been further pretrained on over 30 billion tokens.\nThis suggests that vocabulary extension might not be a\nsuitable choice for small-scale incremental pretraining in the\norder of tens of billions.\n(2) Training scales required for effective transfer. We\nfind that further Chinese pretraining with 100 billion tokens\nor fewer is insufficient to significantly improve LLaMA\u2019s\nknowledge level. However, enhancing LLaMA\u2019s response\nquality (i.e., language generation capability), requires only\nhundreds of thousands of instruction data rather than a large-\nscale further pretraining.\n(3) The effect of transfer training on the original English\ncapabilities. We find that exclusive reliance on Chinese cor-\npora for transfer training markedly compromises LLaMA\u2019s\noriginal English proficiency, a concern alleviated effectively\nthrough multilingual joint training.\nThe aforementioned findings enable us to transfer\nLLaMA\u2019s capabilities of language generation and following\ninstructions to non-English languages at minimal cost.\nBased\non evaluation\nresults\nfrom\nfour widely\nused\nstandardized\ntesting\nbenchmarks\n(C-Eval,\nGAOKAO-\nBench, MMLU, AGI-Eval) and an instruction evaluation\nbenchmark LLM-Eval, we achieve comparable knowledge\nlevel and response quality to the state-of-the-art Open\nChinese LLaMA, while using less than 1% of the training\ndata. Furthermore, extension experiments on another 13\nlow-resource languages also exhibit similar trends. We\naim for the experimental results and analyses in this paper\nto provide assistance and guidance to the community in\nconstructing non-English LLMs.\nBackground and Overview\nIn this subsection, we firstly present the essential steps to\ndevelop an instruction-following LLM. Subsequently, we\nreview common practices of extrapolating this model to\na non-English language and provide an overview of our\nempirical research conducted for the model extrapolation.\nStep 1: Pretraining to acquire language capability\nand knowledge\nAs a significant source of foundational capabilities for a\nLLM, pretraining aims to predict the next token based on\nthe prefix sequences. Formally, given a large corpus D, the\ntraining objective is to minimize the following loss:\nLpretrain =\nX\nx\u2208D\nX\ni\nlog p\u03b8(xi|x1, ..., xi\u22121),\n(1)\nwhere x = {x1, ..., xn} denotes an input token sequence.\nBy pretraining on massive text data ranging from billions\nto trillions of tokens, LLMs are capable of capturing\nintricate language structures, semantics, and contextual\nrelationships, thereby acquiring strong language generation\ncapabilities. Additionally, these LLMs also learn how to\ncomprehend concepts, facts, and the connections between\nthem, leading to a broad understanding of world knowledge.\nStep 2: Instruction tuning for aligning with human\nintent\nInstruction tuning (SFT) aims to further enhance the capabil-\nity of LLMs to follow instructions. Its training data consists\nof many instruction-response pairs. The model needs to\nlearn to accurately respond to instructions, rather than\nmerely continuing from the preceding text. Formally, given\nan instruction dataset D\u2032 = {(I, Y )}, where I represents\na task instruction and Y represents a desired response, the\ntraining objective of instruction tuning is to minimize the\nfollowing loss:\nLins = \u2212 log p\u03b8(Y |I),\n(2)\nBy tuning on diverse instruction tasks, the model is able\nto better comprehend and follow human instructions, and\ngeneralize to unseen instructions.\nExtrapolating LLMs to non-English languages\nLLMs\nacquire\nlanguage\ngeneration\nand\ninstruction-\nfollowing capabilities through pretraining and instruction\ntuning. However, English holds a dominant position in the\nfield of natural language processing, possessing the most\nabundant collection of text data from various domains.\nLLMs trained on English-dominant corpora exhibit inferior\nperformance on other non-English languages. Extrapolating\nLLMs to non-English languages poses a highly valuable\nresearch challenge. Common extrapolation approaches\nconsist of the following three steps: (1) extending the\nvocabulary to add tokens of the target language, and\nthus enhancing encoding expressiveness to that language.\n(2) further pretraining to transfer language generation\ncapabilities of LLMs to the target language. The required\ntraining scale for this step is generally on the order of\nbillions of tokens, significantly less than the trillions of\ntokens needed for training from scratch. (3) conducting\nSFT in the target language to transfer instruction-following\ncapabilities of LLMs.\nThis paper conducts a comprehensive empirical study of\nthe aforementioned three steps, comparing the performance\ndifferences of LLMs before and after vocabulary extension,\nand under various pretraining and SFT scales. It analyzes the\nnecessity of vocabulary extension and the required training\nscale for effective transfer.\nExperimental Setup\nThis paper aims to explore how to effectively transfer the\ncapabilities of language generation and following instruc-\ntion to a non-English language. Given the rich linguistic\nresources available in Chinese, comprehensive and in-\ndepth empirical research can be conducted. Therefore, our\nexperiments and analyses commence with Chinese as the\nstarting point, and the observed phenomena are further\nvalidated across over ten low-resource languages. In this\nsection, we present the datasets, models, and evaluation\nmethodology employed in our experiments.\nModels\nTo avoid unnecessary large-scale repetitive pretraining, we\nemployed open-source models trained on varying scales of\nChinese corpora. Among these, LLaMA and LLaMA2 serve\nas checkpoints without undergoing explicit Chinese pre-\ntraining, whereas Chinese LLaMA and Chinese LLaMA2\nare treated as checkpoints with Chinese pretraining of 30\nbillion tokens. The scale reaches 100 billion tokens for\nOpen Chinese LLaMA. We employ the performance of these\nmodels as references for analysis and comparison.\nLLaMA (Touvron et al. 2023a): LLaMA is a series of foun-\ndation models developed by Meta AI, trained on publicly\navailable English-dominate corpus. The corpus includes\nCommonCrawl, C4, Github code, Wikipedia, Books, and\nArXiv papers, amounting to approximately 1.4 trillion\ntokens. Among these sources, Wikipedia consists of multi-\nlingual text, contributing 4.5% of the total corpus. It covers\n20 languages that use either the Latin or Cyrillic scripts.\nLLaMA achieves state-of-the-art results for foundation\nmodels of its size. For example, LLaMA-13B with just\n13 billion parameters outperforms the much larger 175B\nparameter GPT-3 on many NLP benchmarks. We consider\nLLaMA-7B and LLaMA-13B in our experiments.\nLLaMA2 (Touvron et al. 2023b): LLaMA2 is an enhanced\nand upgraded version of LLaMA. The upgrades it has\nreceived compared to its predecessor include a more robust\ndata cleaning process, a new mix of publicly available\npretraining data boasting a 40% increase in size, a doubled\ncontext length for improved comprehension, and the imple-\nmentation of grouped-query attention for the efficiency of\ninference. These improvements make it a more powerful\ntool for tackling advanced language understanding tasks. We\nconsider LLaMA2-7B in our experiments.\nChinese LLaMA (Cui, Yang, and Yao 2023b): Chinese\nLLaMA is an extension of the original LLaMA, designed\nto enhance its capability in understanding and generating\nChinese text. The goal is achieved by integrating a Chi-\nnese tokenizer developed using SentencePiece. This tok-\nenizer, with a vocabulary size of 49, 953, enables improved\nhandling of Chinese characters. In addition, it employs\nparameter-efficient fine-tuning techniques (Hu et al. 2021)\nto reduce memory consumption during model training. In\nour experiments, we consider Chinese LLaMA 7B Plus,\nwhich is trained on a corpus of approximately 120GB in\nsize, equivalent to around 30 billion Chinese tokens.\nChinese LLaMA2 (Cui, Yang, and Yao 2023a): Chinese\nLLaMA2 is an advanced iteration of Chinese LLaMA.\nIt utilizes the same corpus and training data as Chinese\nLLaMA, but employs the foundational model of LLaMA2.\nFurthermore, the construction of the new version\u2019s vocabu-\nlary and its code implementation have also been optimized.\nIn our experiments, we consider Chinese LLaMA2 7B\npretrained on 30 billion Chinese tokens.\nOpen Chinese LLaMA (OpenLMLab 2023): Open Chinese\nLLaMA is a larger-scale extended version of the original\nLLaMA. To enhance the LLaMA\u2019s capabilities of handling\nChinese text, Open Chinese LLaMA undergoes further\npretraining on a corpus comprising 100 billion tokens. The\ncorpus is composed of texts collected from the internet and\nsubjected to cleaning, along with a subset of English and\ncode data used by the original LLAMA model.\nDatasets\nTo transfer the language capabilities of LLaMA to the\nnon-English language of interest, we utilize two instruction\ndatasets, namely BELLE and Bactrain-X, for training. The\nformer is employed in experiments related to Chinese,\nwhile the latter is utilized for experiments involving other\nlanguages.\nBELLE (Ji et al. 2023): BELLE is a large-scale Chi-\nnese instruction tuning dataset developed by Lianjia Tech.,\ncontaining 1.5 million instruction-following example. We\nremoved duplicated and low-quality data, finally retaining\n950,000 examples.\nBactrain-X (Li et al. 2023): Bactrian-X contains instruc-\ntions and responses across 52 languages to facilitate mul-\ntilingual instruction tuning. It is created by translating\n67K English instructions from Alpaca-52k (Taori et al.\n2023) and Dolly-15k (Conover et al. 2023) datasets into\n51 languages, then generating responses with ChatGPT.\nIn order to objectively and comprehensively assess the\ncapabilities of the model, we conduct evaluations from two\nperspectives: response quality and knowledge level. For the\nformer, we employ the LLM-Eval benchmark and translate\nit into various low-resource languages to support multi-\nlingual evaluation. As for the latter, we utilize four widely\nadopted standardized testing benchmarks: C-Eval, MMLU,\nAGI-Eval, and GAOKAO-Bench.\nLLM-Eval (Zhang et al. 2023a): LLM-Eval is a manually\nconstructed benchmark for instruction-following evaluation.\nIt has 453 instruction tasks from 17 major categories, in-\ncluding factual question answering, reading comprehension,\nframe generation, paragraph rewriting, summarizing, math\nproblem solving, reasoning, poetry generation, program-\nming, and more.\nC-Eval (Huang et al. 2023b): C-Eval is a Chinese evaluation\nsuite with 13948 exam questions across 52 subjects and 4\ndifficulty levels from middle school to professional exams. It\nincludes STEM, humanities, social science and other topics.\nC-Eval HARD is a subset of 8 challenging math and science\nsubjects requiring advanced reasoning.\nMethod\nACC.\nF.\nINFO.\nLC.\nH.\nAVG.\n1k SFT\nLLaMA (Touvron et al. 2023a)\n0.482\n1.194\n0.858\n0.614\n2.970\n1.224\nLLaMA with 10K pretrain\n0.482\n1.441\n0.829\n0.712\n2.963\n1.285\nLLaMA with 100K pretrain\n0.587\n1.952\n0.881\n0.991\n2.973\n1.477\nLLaMA with 1M pretrain\n0.735\n2.071\n1.002\n1.046\n2.957\n1.562\nChinese LLaMA (Cui, Yang, and Yao 2023b)\n0.509\n1.205\n0.811\n0.726\n2.970\n1.244\nOpen Chinese LLaMA (OpenLMLab 2023)\n1.406\n2.584\n1.685\n1.877\n2.989\n2.108\n5k SFT\nLLaMA (Touvron et al. 2023a)\n0.450\n1.279\n0.767\n0.612\n3.000\n1.199\nLLaMA with 10K pretrain\n0.411\n1.372\n0.814\n0.612\n2.961\n1.258\nLLaMA with 100K pretrain\n0.488\n1.922\n0.876\n0.977\n3.000\n1.493\nLLaMA with 1M pretrain\n0.682\n2.085\n1.039\n1.008\n2.969\n1.623\nChinese LLaMA (Cui, Yang, and Yao 2023b)\n0.581\n1.341\n0.899\n0.783\n2.992\n1.432\nOpen Chinese LLaMA (OpenLMLab 2023)\n1.295\n2.481\n1.667\n1.884\n2.969\n2.245\n950k SFT\nLLaMA (Touvron et al. 2023a)\n1.783\n2.767\n2.142\n2.212\n2.993\n2.379\nLLaMA with 1M pretrain\n1.812\n2.799\n2.080\n2.303\n3.000\n2.399\nLLaMA-EXT with 1M pretrain\n1.591\n2.726\n1.918\n2.164\n2.998\n2.279\nChinese LLaMA (Cui, Yang, and Yao 2023b)\n1.808\n2.795\n2.112\n2.313\n3.000\n2.406\nOpen Chinese LLaMA (OpenLMLab 2023)\n1.890\n2.858\n2.189\n2.390\n2.993\n2.464\nLLaMA2 (Touvron et al. 2023b)\n1.868\n2.822\n2.171\n2.379\n3.000\n2.448\nChinese LLaMA2 (Cui, Yang, and Yao 2023a)\n1.701\n2.838\n2.011\n2.251\n3.000\n2.360\nTable 1: Response quality with different scales of further pretraining and instruction tuning (SFT). ACC., F., LC., H., INFO., and\nAVG. respectively denote accuracy, fluency, logical coherence, harmlessness, informativeness and their average. Approximately\n1 million samples account for around 0.5 billion tokens. The pretraining scales for Chinese LLaMA and Open Chinese LLaMA\nare 30 billion and 100 billion tokens, respectively.\nMMLU (Hendrycks et al. 2020): MMLU measures a LLM\u2019s\nability to learn and apply knowledge across 57 diverse\nsubjects including STEM, humanities, and social sciences.\nThe test covers a wide range of difficulty levels from\nelementary to advanced professional.\nAGI-Eval (Zhong et al. 2023): AGIEval uses questions from\nstandardized tests taken by millions of people, including\ncollege entrance exams, law school admission tests, and\nprofessional qualification exams. It has 19 tasks in both\nEnglish and Chinese.\nGaokao-Bench (Zhang et al. 2023b): GAOKAO-Bench\nuses 2811 exam questions from Chinese college entrance\nexams (Gaokao) from 2010-2022 covering all subjects. It\nhas 1781 multiple choice, 218 fill-in-blank, and 812 open-\nended questions across math, Chinese, English, physics, etc.\nEvaluation Protocol\nFor LLM-Eval, we followed the practice of Zhang et al.\n(2023a), evaluating the response quality of a model through\n5 scoring items: accuracy, fluency, informativeness, logical-\nity, and harmlessness. Scores for each aspect range from 0\nto 3. We use the prompt shown in Appendix to submit the\ninstruction, model response, and reference answer to GPT-4\nfor automated evaluation. Based on the results reported by\nZhang et al. (2023a), this evaluation method demonstrates a\nhigh degree of consistency with human evaluation.\nFor the four standardized testing benchmarks, we calcu-\nlate the accuracy metric for model responses. Additionally,\nwe follow the common practice of employing a zero-shot\nsetting for AGI-Eval and GAOKAO-Bench, while using a\n5-shot setting for C-Eval and MMLU.\nMain Results\nThe Impact of Vocabulary Extension on Transfer\nWhen we aim to enhance the capabilities of a LLM\nin a specific language, vocabulary extension is an intu-\nitively reasonable approach. In this section, we evaluate\nthe impact of vocabulary extension through the LLM-\nEval benchmark, and the experimental results are presented\nin table 1. Initially, we collected one million Chinese\nsentences from the internet (approximately 0.5 billion\ntokens) and further pretrain the original LLaMA without\nvocabulary extension. Surprisingly, we find that this model\nsignificantly ourperform the vocabulary-extended Chinese\nLLaMA, across settings of 1K, 5K, and 950K instruction\ntuning. This discovery is thought-privoking, given that the\nChinese LLaMA underwent further Chinese pretraining on\n30 billion tokens, a much larger volume than our 0.5 billion\ntokens. Moreover, within the 950K setting, we include\nresults from extending the vocabulary on original LLaMA\nand training it with the same 0.5 billion tokens, to mitigate\nthe influence of training data discrepancy. The outcomes\nremain consistent. This indicates that vocabulary extension\nis not a favorable choice within training scales of tens of\nbillions of tokens. While we don\u2019t negate the efficacy of\nvocabulary extension in settings involving larger-scale pre-\ntraining (such as trillions of tokens), as reported in other\nliteratures (Team 2023b), this already leans more towards\nretraining than mere language transfer.\nCEval\nGAOKAO-Bench\nMMLU\nAGIEval\n0.00\n10.00\n20.00\n30.00\n40.00\n50.00\nLLaMA-7B\nChinese-LLaMA-Plus-7B\nOpen-Chinese-LLaMA-7B\nLLaMA-2-7B\nChinese-LLaMA-2-7B\nLLaMA-13B\nChinese-LLaMA-Plus-13B\nFigure 2: Knowledge-level evaluation results on four benchmarks.\nTraining Scales Required for Effective Transfer\nTraining scale constitutes another significant factor influ-\nencing the transferability of LLM capabilities, composed\nof both pretraining scale and instruction tuning scale.\nExperimental results are shown in table 1. Taking the\nexample of LLaMA (with 10K, 100K, and 1M further\npretrain) and Open Chinese LLaMA, the scale of further\nChinese pretraining gradually increases from 0 to 100\nbillion tokens. Under the settings of 1K and 5K instruction\ntuning, we observed that the response quality improves\nprogressively with the increase in the scale of further\npretraining. 1 However, when the instruction tuning data\nscale escalates to 950K, we find no significant differences\nin response quality among the models. Consequently, we\nhypothesize that more further pretraining could accelerate\nthe model\u2019s alignment with human instructions, but the mere\ntens of billions in training scale are insufficient to enable the\nmodel to grasp a greater amount of world knowledge. This\nleads to their convergence at similar response levels. In other\nwords, the enhancement in response quality primarily stems\nfrom an improvement in language generation prowess rather\nthan an elevation in knowledge level.\nTo validate this standpoint, we evaluated the model\u2019s\nknowledge level on four widely used standardized test\nbenchmarks. As shown in Figure 2, LLaMA 7B, Chinese\nLLaMA 7B, and Open Chinese LLaMA 7B perform com-\nparably on C-eval, gaokao-bench, and agi-eval, indicating no\nsignificant differences induced by further Chinese pretrain-\ning. It is worth noting that despite lacking further pretraining\nin Chinese, both LLaMA2-7B and LLaMA-13B outperform\nOpen Chinese LLaMA on C-eval, MMLU, and AGI-Eval,\nsuggesting that trillion-level pretraining and larger model\nsizes may indeed serve as effective pathways for enhancing\nmodel knowledge levels.\n1Chinese-LLaMA, however, stands as an exception due to the\nadditional factor of vocabulary extension.\nL(0)\nL(10k)\nL(100k)\nL(1M)\nOpen\nChinese\n10.151\n8.697\n6.634\n5.249\n3.924\nEnglish\n14.691\n15.625\n29.553\n198.840\n15.045\nTable 2: Model perplexity with different further pretraining\nscales. L denotes LLaMA, with the number in the\nparentheses indicating the quantity of further pretraining\nsamples. Open denotes Open Chinese LLaMA.\nHow about the Original English Capabilities\nAnother issue of interest to us is whether the improvement\nin Chinese proficiency has an impact on the existing\nEnglish capabilities. To address this question, we addition-\nally collected 200,000 Chinese samples from the internet\nand randomly extracted 200,000 English samples from\nthe refinedweb dataset (Penedo et al. 2023). Utilizing\nthese samples, we evaluate the English perplexity and\nChinese perplexity of LLaMA models trained on different-\nscale corpora, as depicted in table 2. Our findings reveal\nthat with the increase in further pretraining scale, the\nperplexity of the models decreases steadily in Chinese, yet\nnotably increases in English. This suggests that enhancing\nthe model\u2019s capabilities solely through a single Chinese\ncorpus comes at the cost of sacrificing the original English\nproficiency.\nFurthermore, we conduct perplexity assessments for Open\nChinese LLaMA and find that both the Chinese and English\nperplexities remain low. This outcome is unsurprising, given\nthat its training data incorporates both Chinese and English\ncontent, allowing for the decreases of Chinese perplexity\nwithout significant elevation in English perplexity. Overall,\nexclusive reliance on Chinese corpora for transfer training\nmarkedly compromises LLaMA\u2019s original English profi-\nciency, a concern alleviated effectively through multilingual\njoint training.\nLanguage\n1k SFT\n65k SFT\nACC.\nF.\nINFO.\nLC.\nH.\nAVG.\nACC.\nF.\nINFO.\nLC.\nH.\nAVG.\nArbic\n0.188\n1.061\n0.191\n0.254\n3.000\n0.939\n1.268\n2.499\n1.529\n1.607\n3.000\n1.981\nBengali\n0.046\n0.492\n0.050\n0.041\n3.000\n0.726\n0.959\n2.257\n1.156\n1.189\n3.000\n1.712\nGujarati\n0.061\n0.426\n0.052\n0.063\n2.998\n0.720\n0.683\n1.795\n0.875\n0.790\n2.995\n1.428\nHindi\n0.131\n1.064\n0.147\n0.162\n3.000\n0.901\n1.014\n2.342\n1.238\n1.240\n2.998\n1.766\nIndonesian\n0.398\n1.266\n0.544\n0.438\n2.995\n1.128\n1.659\n2.751\n2.026\n2.012\n3.000\n2.290\nMalayalam\n0.101\n0.621\n0.103\n0.103\n3.000\n0.786\n0.906\n2.427\n1.182\n1.197\n3.000\n1.742\nMarathi\n0.095\n0.781\n0.107\n0.117\n2.998\n0.820\n1.038\n2.476\n1.288\n1.364\n2.998\n1.833\nNepali\n0.151\n0.991\n0.177\n0.146\n2.986\n0.890\n0.969\n2.417\n1.236\n1.285\n3.000\n1.781\nSwahili\n0.083\n0.712\n0.090\n0.086\n2.998\n0.794\n1.569\n2.707\n1.955\n1.907\n3.000\n2.228\nTamil\n0.140\n0.914\n0.176\n0.174\n2.998\n0.880\n0.960\n2.457\n1.198\n1.257\n2.998\n1.774\nTelugu\n0.054\n0.560\n0.057\n0.090\n3.000\n0.752\n0.539\n1.735\n0.674\n0.712\n3.000\n1.332\nUrdu\n0.057\n0.573\n0.052\n0.071\n3.000\n0.751\n1.038\n2.443\n1.285\n1.335\n3.000\n1.820\nVietnamese\n0.105\n0.623\n0.126\n0.117\n3.000\n0.794\n1.361\n2.595\n1.665\n1.710\n3.000\n2.066\nAverage\n0.124\n0.776\n0.144\n0.143\n2.998\n0.837\n1.074\n2.377\n1.331\n1.354\n2.999\n1.827\nTable 3: Evaluation results of model response quality for 13 low-resource languages on the LLM-Eval. ACC., F., LC., H.,\nINFO., and AVG. respectively denote accuracy, fluency, logical coherence, harmlessness, informativeness and their average.\nExtending the Analysis to Multiple Languages\nIn the previous section, our experiments focus on Chinese.\nTo investigate whether similar conclusions could be drawn\nin other non-English languages, we extend our experiments\nto 13 low-resource languages. To ensure evaluation con-\nsistency, we translate LLM-Eval benchmark into these 13\nlanguages and employ the same evaluation metrics. As\nshown in table 3, a significant improvement in response\nquality for all low-resource languages with the increase\nin SFT data. Among these languages, Arabic, Indonesian,\nand Vietnamese exhibited the best performance. Despite\nall thirteen languages being low-resource, these three lan-\nguages are more frequently used (Scao et al. 2023). As\na result, LLaMA encounters them more often (although\ntheir overall occurrence is small compared to English),\nallowing the model to quickly comprehend instructions in\nthese languages. This aligns with the conclusion drawn in\nthe previous section.\nIn the previous section, we observed that extending the\nvocabulary had a negative impact on language transfer-\nability. A plausible hypothesis is the existence of cross-\nlingual semantic alignment within LLMs, which vocabu-\nlary expansion might disrupt. To validate this alignment\nhypothesis, we fine-tune LLaMA with a dataset of 1k\ninstructions and examine the model\u2019s output. Excitingly, we\nobserved a certain proportion of code-switching samples. As\ndepicted in figure 3, these samples\u2019 model responses consist\nof tokens from multiple languages and are semantically\ncoherent. We have observed that code-switching occurs not\nonly in the transfer process when Chinese is the target\nlanguage, but also when other 13 low-resource languages\nare target languages. As shown in figure 4, the proportion of\nsamples with code-switching is approximately between 2%\nto 5%. This indicates that LLaMA might have learned cross-\nlingual alignment relationships between concepts during the\npretraining process.\nHuman 1: \u5982\u4f55\u8bc4\u4ef7\u7f8e\u56fd\u7684\u533b\u7597\u4fdd\u969c\u4f53\u7cfb\uff1f\u8bf7\u4ece\u4f18\u70b9\u548c\u7f3a\n\u70b9\u4e24\u4e2a\u2f45\u2faf\u8fdb\u2f8f\u8bc4\u4ef7\u3002\n(Human 1: How would you evaluate the healthcare system in\nthe United States? Please provide an assessment from both\nthe perspectives of its advantages and disadvantages.) \nAssistant 1: \u533b\u7597\u4fdd\u969c\u4e3a\u6211\u56fd\u2f08\u2ea0\u517b\u2f63\u548c\u4f18\u5148\u4fdd\u969c\uff0c \u53ef\u4ee5\n\u627e\u5230\u591a\u5bb6\u79d1\u5ba4\u548c\u4e13\u79d1\u533b\u2f63\u7ed9\u4e88\u6211\u4eec\u597d quality medicine\nservice.  The medical system of the United States is very\ngood in many aspects. ... \n(Assistant 1: Healthcare coverage ensures the well-being and\npriority protection of the people in our country. It allows us to\naccess numerous departments and specialized doctors who\nprovide us with high-quality medical services.The medical\nsystem of the United States is very good in many aspects. ...) \nHuman 2: \u8bf7\u4f60\u4e3a\u5168\u7403\u519b\u4e8b\u5b9e\u2f12\u8fdb\u2f8f\u56fd\u5bb6\u5c42\u2faf\u4e0a\u7684\u6392\u5e8f\uff0c\n\u4e0d\u5305\u62ec\u2ea0\u95f4\u7ec4\u7ec7\u4e0e\u5176\u4ed6\u673a\u6784\u3002\n(Human2:Please provide a country-level ranking of global\nmilitary capabilities, excluding non-state organizations and\nother entities.) \nAssistant 2: ... 1. \u7f8e\u56fd\uff1a\u6b66\u5668\uff1a \u7f8e\u56fd\u6b66\u5668\u6709\u7740\u4e16\u754c\u4e0a\u6700\u65b0\n\u7684\u8f7b\u0437\u0430\u0440\u044f\u0434\u2ecb\u3001\u8ffd\u8e2a\u6027\u80fd\u6700\u5f3a\u7684\u6f5c\u2f54\u8247\u7b49\u7b49\u7684\u512a\u308c\u305f\u6b66\u5668\n\u5316\u72b6\u51b5\u3002...\n(Assistant 2: ... 1. United States: Weapons: The United States\npossesses advanced weaponry including the world's latest\nlightweight armored vehicles, submarines with the highest\ntracking capabilities, and other superior weapons. ...) \nFigure 3: Case study of code-switching. Text with a red\nbackground represents the non-English target language\n(Chinese). Text with a cyan background indicates code-\nswitching language in the model\u2019s output, which could be\nEnglish, Japanese, Russian or other languages.\nChinese\nArabic\nBengali\nGujarati\nHindi\nIndonesian\nMalayalam\nMarathi\nNepali\nSwahili\nTamil\nTelugu\nUrdu\nVietnamese\n1\n2\n3\n4\n3.881%\n4.706%\n4.556%\n2.078%\n2.613%\n4.235%\n2.158%\n4.048%\n4.245%\n2.381%\n4.762%\n1.618%\n3.302%\n2.108%\nFigure 4: Code-switching rate across languages.\nRelated Work\nResource Gap in LLMs\nOne of the main challenges of LLMs is the resource\ngap, as they are mainly pretrained on English corpus\nand have limited access to data from other languages.\nEnglish dominates the field of NLP as an extremely high-\nresource language with the most raw text data from various\ndomains, leaving few of the over 7000 languages of the\nworld represented in the field (Joshi et al. 2020). This\ncreates a disparity in language models\u2019 capability to handle\ndifferent languages. Previous findings indicate that LLMs\nhave difficulty comprehending and generating non-English\ntexts, particularly in low-resource languages(Nguyen et al.\n2023; Zhu et al. 2023; Huang et al. 2023a). To address\nthe resource gap, several solutions have been proposed or\nimplemented by researchers and practitioners. One possible\nsolution is to increase the amount of data available from\nvarious languages and fields, and make it accessible for\npretraining and evaluating LLMs (Lin et al. 2022; Chen et al.\n2022; Cahyawijaya et al. 2023) . However, this approach\nincurs significant computational expenses and the resource\ngap persists. Alternatively, multilingual language models\ntrained on texts from different languages concurrently, such\nas mBERT (Devlin et al. 2019) and XLM-R (Conneau et al.\n2020a), have been introduced to bridge the gap effectively.\nCross-Lingual Transfer\nMultilingual language models have demonstrated a high\nlevel of zero-shot or few-shot cross-lingual transferability\nacross a wide range of tasks (Wu and Dredze 2019;\nPires, Schlinger, and Garrette 2019; Winata et al. 2021b).\nThis means that they can acquire the language capability\nfrom supervised data in one language and apply it to\nanother without or with few additional training data. The\nmechanism behind the strong cross-lingual performance has\nbeen investigated by the researchers. It has been shown\nthat multilingual language models have inferred universal\nrules applicable to any language (Artetxe, Ruder, and\nYogatama 2020; Chi, Hewitt, and Manning 2020; Conneau\net al. 2020b). Contrary to the common hypothesis that\nmultilingual multilingual language models such as mBERT\n(Devlin et al. 2019) rely on a shared subword vocabulary\nand joint pretraining across multiple languages (Pires,\nSchlinger, and Garrette 2019; Cao, Kitaev, and Klein 2020;\nWu and Dredze 2019), researchers have developed new\nunderstandings on the models, emphasizing the models\u2019\nability to learn universal semantic abstractions (Artetxe,\nRuder, and Yogatama 2020; Chi, Hewitt, and Manning\n2020). In terms of the factors that influence cross-lingual\nperformance, researchers have associated transferability\nwith parameter sharing (Conneau et al. 2020b; Dufter\nand Sch\u00a8utze 2020; Wu, Papadimitriou, and Tamkin 2022)\nand language distance (Conneau et al. 2020b; Eronen,\nPtaszynski, and Masui 2023). We here further investigate\nthe cross-lingual transferability of language models with\nnew LLaMA-based experiments, presenting outcomes from\na different aspect.\nCode-Switching\nCode-switching is a phenomenon in which multilingual\nspeakers switch between languages within a single utter-\nance. Previous work on the performance of multilingual\nlanguage models on code-switching tasks has shown mixed\nresults. Some studies have suggested that pretrained models\nfine-tuned for specific code-switching scenarios can achieve\nstate-of-the-art performance for certain language pairs such\nas English-Spanish and English-Hindi (Khanuja et al. 2020),\nwhile others have found that using meta-embeddings can\nyield better results with fewer parameters (Winata, Lin,\nand Fung 2019; Winata et al. 2019, 2021a). In another\nline of research, code-switching-based methods have been\npresented to improve the capability of multilingual language\nmodels (Jiang et al. 2020; Tan and Joty 2021; Krishnan et al.\n2021).\nConclusions\nIn this paper, we focus on how to effectively transfer the\ncapabilities of language generation and following instruc-\ntions to a non-English language. Specifically, we conducts\na comprehensive empirical study to analyze the necessity\nof vocabulary extension and the required training scale\nfor effective transfer. We find that vocabulary extension is\nuncessary and that comparable transfer performance to state-\nof-the-art models can be achieved with less than 1% of the\nfurther pretraining data. Additionally, we observe instances\nof code-switching during the transfer training, suggesting\nthat cross-lingual alignment might have been internalized\nwithin the model. Similar results are observed from the\nextension experiments on the 13 low-resource languages.\nOur analysis and findings offer assistance and guidance to\nthe community in developing non-English LLMs.\nReferences\nAnil, R.; Dai, A. M.; Firat, O.; Johnson, M.; and Lepikhin,\nD. 2023. PaLM 2 Technical Report. arXiv:2305.10403.\nArtetxe, M.; Ruder, S.; and Yogatama, D. 2020.\nOn the\nCross-lingual Transferability of Monolingual Representa-\ntions.\nIn Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 4623\u20134637.\nOnline: Association for Computational Linguistics.\nBubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg,\nS.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y. 2023.\nSparks of Artificial General Intelligence: Early experiments\nwith GPT-4. arXiv:2303.12712.\nCahyawijaya, S.; Lovenia, H.; Aji, A. F.; Winata, G. I.; and\nWilie, B. 2023.\nNusaCrowd: Open Source Initiative for\nIndonesian NLP Resources. arXiv:2212.09648.\nCao, S.; Kitaev, N.; and Klein, D. 2020.\nMulti-\nlingual Alignment of Contextual Word Representations.\narXiv:2002.03518.\nChen, G.; Ma, S.; Chen, Y.; Zhang, D.; Pan, J.; Wang,\nW.; and Wei, F. 2022.\nTowards Making the Most of\nMultilingual Pretraining for Zero-Shot Neural Machine\nTranslation. arXiv:2110.08547.\nChi, E. A.; Hewitt, J.; and Manning, C. D. 2020.\nFinding Universal Grammatical Relations in Multilingual\nBERT.\nIn Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 5564\u20135577.\nOnline: Association for Computational Linguistics.\nCobbe, K.; Kosaraju, V.; Bavarian, M.; Hilton, J.; Nakano,\nR.; Hesse, C.; and Schulman, J. 2021. Training Verifiers to\nSolve Math Word Problems. CoRR, abs/2110.14168.\nConneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V.;\nWenzek, G.; Guzm\u00b4an, F.; Grave, E.; Ott, M.; Zettlemoyer,\nL.; and Stoyanov, V. 2020a.\nUnsupervised Cross-lingual\nRepresentation Learning at Scale. arXiv:1911.02116.\nConneau, A.; Wu, S.; Li, H.; Zettlemoyer, L.; and Stoyanov,\nV. 2020b. Emerging Cross-lingual Structure in Pretrained\nLanguage Models. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, 6022\u2013\n6034. Online: Association for Computational Linguistics.\nConover, M.; Hayes, M.; Mathur, A.; Xie, J.; Wan, J.;\nShah, S.; Ghodsi, A.; Wendell, P.; Zaharia, M.; and Xin, R.\n2023. Free Dolly: Introducing the World\u2019s First Truly Open\nInstruction-Tuned LLM.\nCui, Y.; Yang, Z.; and Yao, X. 2023a. Chinese LLaMA and\nAlpaca Large Language Models.\nCui, Y.; Yang, Z.; and Yao, X. 2023b.\nEfficient and\nEffective Text Encoding for Chinese LLaMA and Alpaca.\narXiv:2304.08177.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers\nfor Language Understanding.\nIn Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers),\n4171\u20134186. Minneapolis, Minnesota: Association for Com-\nputational Linguistics.\nDong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.; Sun,\nX.; Xu, J.; Li, L.; and Sui, Z. 2023. A Survey on In-context\nLearning. arXiv:2301.00234.\nDufter, P.; and Sch\u00a8utze, H. 2020.\nIdentifying Elements\nEssential for BERT\u2019s Multilinguality.\nIn Proceedings of\nthe 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 4423\u20134437. Online: Asso-\nciation for Computational Linguistics.\nEronen, J.; Ptaszynski, M.; and Masui, F. 2023. Zero-shot\ncross-lingual transfer language selection using linguistic\nsimilarity. Information Processing & Management, 60(3):\n103250.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika,\nM.; Song, D.; and Steinhardt, J. 2020.\nMeasuring\nMassive Multitask Language Understanding.\nCoRR,\nabs/2009.03300.\nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,\nS.; and Chen, W. 2021. LoRA: Low-Rank Adaptation of\nLarge Language Models. CoRR, abs/2106.09685.\nHuang, H.; Tang, T.; Zhang, D.; Zhao, W. X.; Song, T.;\nXia, Y.; and Wei, F. 2023a.\nNot All Languages Are\nCreated Equal in LLMs: Improving Multilingual Capability\nby Cross-Lingual-Thought Prompting. arXiv:2305.07004.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022.\nLanguage Models as Zero-Shot Planners: Extracting Action-\nable Knowledge for Embodied Agents. In Chaudhuri, K.;\nJegelka, S.; Song, L.; Szepesvari, C.; Niu, G.; and Sabato, S.,\neds., Proceedings of the 39th International Conference on\nMachine Learning, volume 162 of Proceedings of Machine\nLearning Research, 9118\u20139147. PMLR.\nHuang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; and Zhang, J. 2023b.\nC-Eval: A Multi-Level Multi-Discipline Chinese Evaluation\nSuite for Foundation Models. arXiv:2305.08322.\nJi, Y.; Deng, Y.; Gong, Y.; Peng, Y.; Niu, Q.; Ma, B.; and\nLi, X. 2023. BELLE: Be Everyone\u2019s Large Language model\nEngine. https://github.com/LianjiaTech/BELLE.\nJiang, Z.; Anastasopoulos, A.; Araki, J.; Ding, H.; and\nNeubig, G. 2020.\nX-FACTR: Multilingual Factual\nKnowledge Retrieval from Pretrained Language Models. In\nProceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), 5943\u20135959.\nOnline: Association for Computational Linguistics.\nJoshi, P.; Santy, S.; Budhiraja, A.; Bali, K.; and Choudhury,\nM. 2020. The State and Fate of Linguistic Diversity and\nInclusion in the NLP World.\nIn Proceedings of the 58th\nAnnual Meeting of the Association for Computational Lin-\nguistics, 6282\u20136293. Online: Association for Computational\nLinguistics.\nKatz, D. M.; Bommarito, M. J.; Gao, S.; and Arredondo,\nP. 2023.\nGpt-4 passes the bar exam.\nAvailable at SSRN\n4389233.\nKhanuja, S.; Dandapat, S.; Srinivasan, A.; Sitaram, S.;\nand Choudhury, M. 2020.\nGLUECoS: An Evaluation\nBenchmark for Code-Switched NLP.\nIn Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, 3575\u20133585. Online: Association\nfor Computational Linguistics.\nKrishnan, J.; Anastasopoulos, A.; Purohit, H.; and Rang-\nwala, H. 2021.\nMultilingual Code-Switching for Zero-\nShot Cross-Lingual Intent Prediction and Slot Filling.\narXiv:2103.07792.\nLi, H.; Koto, F.; Wu, M.; Aji, A. F.; and Baldwin,\nT.\n2023.\nBactrian-X\n:\nA\nMultilingual\nReplicable\nInstruction-Following Model with Low-Rank Adaptation.\narXiv:2305.15011.\nLin, X. V.; Mihaylov, T.; Artetxe, M.; Wang, T.; Chen, S.;\nSimig, D.; Ott, M.; Goyal, N.; Bhosale, S.; Du, J.; Pasunuru,\nR.; Shleifer, S.; Koura, P. S.; Chaudhary, V.; O\u2019Horo, B.;\nWang, J.; Zettlemoyer, L.; Kozareva, Z.; Diab, M.; Stoyanov,\nV.; and Li, X. 2022. Few-shot Learning with Multilingual\nLanguage Models. arXiv:2112.10668.\nNguyen, X.-P.; Aljunied, S. M.; Joty, S.; and Bing, L.\n2023. Democratizing LLMs for Low-Resource Languages\nby Leveraging their English Dominant Abilities with\nLinguistically-Diverse Prompts. arXiv:2306.11372.\nOpenAI. 2022. Introducing ChatGPT.\nOpenLMLab. 2023. Open-Chinese-LLaMA.\nPenedo, G.; Malartic, Q.; Hesslow, D.; Cojocaru, R.;\nCappelli, A.; Alobeidli, H.; Pannier, B.; Almazrouei, E.; and\nLaunay, J. 2023. The RefinedWeb Dataset for Falcon LLM:\nOutperforming Curated Corpora with Web Data, and Web\nData Only. arXiv:2306.01116.\nPires, T.; Schlinger, E.; and Garrette, D. 2019.\nHow\nMultilingual is Multilingual BERT? In Proceedings of the\n57th Annual Meeting of the Association for Computational\nLinguistics, 4996\u20135001. Florence, Italy: Association for\nComputational Linguistics.\nRanta, A.; and Goutte, C. 2021.\nLinguistic Diversity in\nNatural Language Processing. Traitement Automatique des\nLangues, 62(3): 7\u201311.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili\u00b4c, S.;\nHesslow, D.; and Castagn\u00b4e, R. 2023. BLOOM: A 176B-\nParameter Open-Access Multilingual Language Model.\narXiv:2211.05100.\nStabilityAI. 2023. Announcing StableCode.\nTan, S.; and Joty, S. 2021. Code-Mixing on Sesame Street:\nDawn of the Adversarial Polyglots. arXiv:2103.09593.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Alpaca:\nA Strong, Replicable Instruction-Following Model.\nTeam, I. 2023a. Internlm: A multilingual language model\nwith progressively enhanced capabilities.\nTeam, I. 2023b. InternLM: A Multilingual Language Model\nwith Progressively Enhanced Capabilities.\nhttps://github.\ncom/InternLM/InternLM-techreport.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and\nLample, G. 2023a. LLaMA: Open and Efficient Foundation\nLanguage Models. arXiv:2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; and\nAlmahairi, A. 2023b. Llama 2: Open Foundation and Fine-\nTuned Chat Models. arXiv:2307.09288.\nWinata, G. I.; Cahyawijaya, S.; Liu, Z.; Lin, Z.; Madotto,\nA.; and Fung, P. 2021a. Are Multilingual Models Effective\nin Code-Switching? arXiv:2103.13309.\nWinata, G. I.; Lin, Z.; and Fung, P. 2019.\nLearning\nMultilingual Meta-Embeddings for Code-Switching Named\nEntity Recognition.\nIn Proceedings of the 4th Workshop\non Representation Learning for NLP (RepL4NLP-2019),\n181\u2013186. Florence, Italy: Association for Computational\nLinguistics.\nWinata, G. I.; Lin, Z.; Shin, J.; Liu, Z.; and Fung, P. 2019.\nHierarchical Meta-Embeddings for Code-Switching Named\nEntity Recognition. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), 3541\u20133547. Hong Kong,\nChina: Association for Computational Linguistics.\nWinata, G. I.; Madotto, A.; Lin, Z.; Liu, R.; Yosinski,\nJ.; and Fung, P. 2021b.\nLanguage Models are Few-shot\nMultilingual Learners. In Proceedings of the 1st Workshop\non Multilingual Representation Learning, 1\u201315. Punta\nCana, Dominican Republic: Association for Computational\nLinguistics.\nWu, S.; and Dredze, M. 2019.\nBeto, Bentz, Becas:\nThe Surprising Cross-Lingual Effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), 833\u2013844. Hong Kong, China:\nAssociation for Computational Linguistics.\nWu, Z.; Papadimitriou, I.; and Tamkin, A. 2022. Oolong:\nInvestigating What Makes Crosslingual Transfer Hard with\nControlled Studies. arXiv:2202.12312.\nZhang, M.; Zhang, Q.; Zhang, Y.; and Gui, T. 2023a.\nLLMEVAL-1 Chinese Large Language Model Evaluation\nPhase 1.\nZhang, X.; Li, C.; Zong, Y.; Ying, Z.; He, L.; and Qiu,\nX. 2023b. Evaluating the Performance of Large Language\nModels on GAOKAO Benchmark. arXiv:2305.12474.\nZhong, W.; Cui, R.; Guo, Y.; Liang, Y.; Lu, S.; Wang,\nY.; Saied, A.; Chen, W.; and Duan, N. 2023.\nAGIEval:\nA Human-Centric Benchmark for Evaluating Foundation\nModels. arXiv:2304.06364.\nZhu, W.; Liu, H.; Dong, Q.; Xu, J.; Huang, S.; Kong,\nL.; Chen, J.; and Li, L. 2023.\nMultilingual Machine\nTranslation with Large Language Models: Empirical Results\nand Analysis. arXiv:2304.04675.\nAppendix\nThe prompt for LLM-Eval\nFor questions related to specific subject matters, we score\nthe following LLM outputs using a scale of 0/1/2/3:\nAccuracy: Evaluates whether the answer is precise and\ncorrect, with accurate information. A high-quality answer\nshould be factually reliable.\nFluency: Assesses how well the answer adheres to human\nlanguage conventions, with coherent phrasing and clear\nexpression. A high-quality answer should be easy to under-\nstand, without convoluted or difficult-to-parse sentences.\nInformativeness: Judges if the answer provides sufficient\nrelevant information of practical value. A high-quality\nanswer should offer useful and pertinent information to the\nquestioner.\nLogicality: Examines if the answer is logically sound\nand accurate in presenting viewpoints and arguments. A\nhighquality answer should follow logical principles, demon-\nstrating clear reasoning and coherence.\nHarmlessness: Assesses whether the answer avoids uneth-\nical or harmful content, adhering to ethical norms. A high-\nquality answer should uphold ethical principles, avoiding the\npropagation of harmful or immoral information.\nNote: If the model provides no response, all scores except\nfor \u201cHarmlessness\u201d should be 0.\nThe question is: Question The LLM response is: Response\nThe reference answer for this question is: Reference Answer\nPlease provide an answer in the following format, assigning\nyour perceived scores for LLM response\u2019s \u201caccuracy\u201d, \u201cflu-\nency\u201d, \u201cinformativeness\u201d, \u201clogicality\u201d, and \u201charmlessness\u201d\non a scale of 0/1/2/3:\n\u201cAccuracy\u201d: score for LLM response\u2019s accuracy (integer),\n\u201cFluency\u201d: score for LLM response\u2019s fluency (integer),\n\u201cInformativeness\u201d: score for LLM response\u2019s informative-\nness (integer),\n\u201cLogicality\u201d: score for LLM response\u2019s logicality (integer),\n\u201cHarmlessness\u201d: score for LLM response\u2019s harmlessness\n(integer).\n"
  },
  {
    "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning",
    "link": "https://arxiv.org/pdf/2401.01325.pdf",
    "upvote": "24",
    "text": "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nHongye Jin 1 * Xiaotian Han 1 * Jingfeng Yang 2 Zhimeng Jiang 1 Zirui Liu 3 Chia-Yuan Chang 1\nHuiyuan Chen 4 Xia Hu 3\nAbstract\nIt is well known that LLMs cannot generalize\nwell to long contexts whose lengths are larger\nthan the training sequence length. This poses\nchallenges when employing LLMs for process-\ning long input sequences during inference. In\nthis work, we argue that LLMs themselves have\ninherent capabilities to handle long contexts with-\nout fine-tuning. To achieve this goal, we pro-\npose SelfExtend to extend the context window\nof LLMs by constructing bi-level attention infor-\nmation: the grouped attention and the neighbor\nattention. The grouped attention captures the de-\npendencies among tokens that are far apart, while\nneighbor attention captures dependencies among\nadjacent tokens within a specified range. The\ntwo-level attentions are computed based on the\noriginal model\u2019s self-attention mechanism dur-\ning inference.\nWith minor code modification,\nour SelfExtend can effortlessly extend existing\nLLMs\u2019 context window without any fine-tuning.\nWe conduct comprehensive experiments on mul-\ntiple benchmarks and the results show that our\nSelfExtend can effectively extend existing LLMs\u2019\ncontext window length. The code can be found at\nhttps://github.com/datamllab/LongLM.\n1. Introduction\nThe context window length of most existing LLMs (Zhao\net al., 2023; Yang et al., 2023) is limited since they are\ntrained with a fixed length of training sequences. It\u2019s deter-\nmined by the context window length during the pretraining\nstage. Once the length of the input texts exceeds the pretrain-\ning context window during the inference, the behavior of\nLLMs will be unpredictable and suffer from severe perfor-\nmance degradation. The perplexity (PPL) of the model will\nexplode with the long input sequences (Xiao et al., 2023;\nPeng et al., 2023; Han et al., 2023; Chen et al., 2023b).\n*Equal contribution 1Texas A&M University 2Amazon, views\nhere are the author\u2019s and not those of Amazon. 3Rice University\n4Case Western Reserve University. Correspondence to: Hongye\nJin <jhy0410@tamu.edu>.\nRecently, a variety of context window extension methods\nhave been developed to tackle the challenge of extending the\ncontext window size of pretrained LLMs. A straightforward\napproach is to fine-tune these models on enough extensive\ntexts. Besides this, some methods seek to extend context\nwindow length in more efficient fine-tuning ways. Among\nthese contemporary methods, some notable techniques in-\nclude \u2018PI\u2019 (Chen et al., 2023b), \u2018CLEX\u2019 (Chen et al., 2023a)\n\u2018Yarn\u2019 (Peng et al., 2023), \u2018LongLora\u2019 (Chen et al., 2023c),\nand \u2018ABF\u2019 (Xiong et al., 2023). These methods aim to ex-\ntend the content window based on the implicit assumption\nthat pretrained LLMs lack the ability to handle long con-\ntent. However, these methods typically require finetuning to\nachieve extension, which can be resource and time intensive\ngiven the quadratic complexity of Transformers. Addition-\nally, high-quality long text data is scarce, hindering such\nfine-tuning approaches. Most real-world data is short, and\nmuch long text lacks meaningful long-range dependencies.\nWith limited appropriate data, finetuning risks degrading\nexisting strong performance on shorter sequences from pre-\ntraining or overfitting models to the tuning set. LLMs\u2019\ngeneralizability to broad tasks may reduce.\nInstead of extending the content window, in this paper, we\nbelieve LLMs should have inherent capabilities to handle\nlong contexts. Our belief stems from the fact that when we,\nas human beings, are children, we are taught how to read and\nwrite using relatively short texts, such as articles spanning\nseveral pages. We rarely use extremely long texts like entire\nbooks or complete documents as learning materials. Yet,\nwe are still able to understand long texts effectively. With\nthis strong motivation, the poor performance of LLMs while\nfacing long text out of the pretraining context window is not\ndue to the lack of long context understanding capabilities.\nIn our analysis, the key challenge preventing LLMs from ef-\nfectively handling longer contexts is the Out-of-Distribution\n(O.O.D) issues related to positional encoding, which we\ncall the positional O.O.D1 issue. This problem arises when\n1Here, the position refers to relative position rather than ab-\nsolute position. The relative position is m \u2212 n in RoPE, where\nm and n are the absolute positions of two tokens. The positional\nO.O.D refers to cases where the value of m \u2212 n during inference\nis unseen, i.e., larger than the values observed during pretraining.\nIn this paper, we map unseen large relative positions to those ob-\n1\narXiv:2401.01325v2  [cs.CL]  3 Feb 2024\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nLLMs encounter text sequences during inference exceeding\nthe length of their pretraining context window, where LLMs\nare exposed to new relative distances that were not present\nduring their pretraining phase. It is widely recognized that\nNeural Networks (NNs) are susceptible to unpredictable\nbehaviors when dealing with O.O.D inputs (Liu et al., 2021;\nShen et al., 2021; Bai et al., 2021; Zhang et al., 2023). To\naddress this, an intuitive and practical solution would be to\nremap the unseen relative positions to those encountered\nduring the pretraining, thus extending the LLMs\u2019 ability to\nhandle longer contexts naturally.\nThis paper proposes SelfExtend to elicit LLMs\u2019 inherent\nlong context capabilities. SelfExtend addresses the issue\nof O.O.D. positional information by using a simple floor\ndivision operation to map unseen large relative positions to\nthose encountered during pretraining. The core idea hinges\non the observation that, in long texts, exacting word posi-\ntions becomes less crucial. The overall meaning and the\nrelative order of information hold greater significance. Just\nlike when answering questions about lengthy texts, we rely\non the general location and order, not the specific word-by-\nword placement. Natural language exhibits a characteris-\ntic where meaning stays relatively consistent within short\nranges like paragraphs. Therefore, using close or even iden-\ntical position encodings effectively captures the necessary\nrelative ordering of important information. This intuitive\napproach aligns perfectly with the floor operation\u2019s function-\nality. Additionally, T5 (Raffel et al., 2020) and iRPE (Wu\net al., 2021) also share this similar intuition.\nOur SelfExtend is a plug-and-play method that takes ef-\nfect at the inference stage, allowing existing large language\nmodels to easily adopt it. We evaluate SelfExtend with\nsome popular LLMs (Llama-2 (Touvron et al., 2023), Mis-\ntral (Jiang et al., 2023), SOLAR (Kim et al., 2023), and\nPhi-2 (Javaheripi et al., 2023)) on three types of tasks: lan-\nguage modeling, synthetic long context tasks, and real-world\nlong context tasks. The proposed SelfExtend substantially\nimproves the long context understanding ability and even\noutperforms many finetuning-based methods on some tasks.\nThese results underscore SelfExtend as an effective solution\nfor context window extension. The superior performance\nof SelfExtend also demonstrated the potential of large lan-\nguage models to effectively handle long contexts. Our main\ncontributions are summarized as follows:\n\u2022 We think LLMs with RoPE have a natural ability to han-\ndle long texts, even if they have not encountered super-\nlong ones during training. The previous limitation stems\nfrom O.O.D. positions, meaning the \u201dlarger\u201d positions\nhave not been seen during training. We call this the\npositional O.O.D. issue.\nserved during pretraining. More details about m \u2212 n are provided\nin Section 2.\n\u2022 Based on this belief and to address the positional O.O.D.\nissue, we propose SelfExtend to extend the context win-\ndow of LLMs without any fine-tuning. We map the un-\nseen large relative positions (at inference) to known po-\nsitions (at training), thus allowing LLMs to maintain co-\nherence over longer texts without additional fine-tuning.\n\u2022 In both synthetic and real-world long context tasks, Self-\nExtend has proven its ability to deliver performance that\nmatches or surprisingly surpasses many existing fine-\ntuning-based models. This highlights the superior capa-\nbilities of our SelfExtend model.\n2. Preliminary\nPosition Encoding. Transformers (Vaswani et al., 2017)\nincorporate position information via different positional em-\nbedding designs. The positional embedding design can be\ncategorized into two classes: absolute position embeddings\nand relative positional encodings. The absolute position em-\nbedding provides the absolute positions, which embeds each\nabsolute position i into position vector pi and adds word\nembeddings to their corresponding pi before feeding them\nto the model. Examples of such include sinusoidal position\nembeddings (Vaswani et al., 2017) and learned position em-\nbeddings in GPT3 (Brown et al., 2020) and OPT (Zhang\net al., 2022), or adding the dot product between two tokens\u2019\nposition embeddings on the attention logit (Ke et al., 2020).\nOn the other hand, relative positional encodings have been\nproposed to use relative distance information between to-\nkens and have become the mainstream of position embed-\nding. This information is usually applied in attention layers.\nExamples of such include a learnable attention logit bias as\nin T5 (Xue et al., 2020), Transformer-XL (Dai et al., 2019);\na fixed linear attention decay called Alibi (Press et al., 2021);\nrotating query and key sequences based on distance such as\nRoPE (Su et al., 2022), and XPos (Sun et al., 2023). The pro-\nposed method is based on the Rotary Position Embedding\n(RoPE) introduced in (Su et al., 2022).\nRoPE. Here, we introduce the basic concept of RoPE.\nLet\u2019s consider a sequence of tokens represented as\nw1, w2, \u00b7 \u00b7 \u00b7 , wL, and their corresponding embeddings are\ndenoted as x1, \u00b7 \u00b7 \u00b7 , xL \u2208 R|D|, where |D| is the dimension\nof the embedding. The basic idea of RoPE is to incorporate\nthe positional information into the query q and the key vec-\ntors k, respectively. This integration ensures that their inner\nproduct qT k will contain the relative positional embedding\ninformation inherently. To achieve this, RoPE employs the\nfollowing vector transformations:\nqm = fq(xm, m) \u2208 R|L|, kn = fk(xn, n) \u2208 R|L|,\n(1)\nwhere |L| is the hidden dimension of per head. The func-\ntions fq and fk responsible for injecting positional informa-\ntion, are defined as fq(xm, m) = Wqxmeim\u03b8, fk(xn, n) =\n2\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nWkxnein\u03b8, where \u03b8d = b\u22122d/|D|, b = 10000 and projec-\ntors Wq, Wk : R|D| \u2192 R|L|. RoPE keeps the real part of\nthe inner product qT k, which is Re(q\u2217k). This operation\nensures that the dot product of the query and key vectors\ndepends entirely on the relative distance between the tokens,\nrepresented by m \u2212 n of the tokens as follows:\n\u27e8fq(xm, m), fk(xn, n)\u27e9R = Re(\u27e8fq(xm, m), fk(xn, n)\u27e9C)\n=Re(x\u2217\nmW \u2217\nq Wkxnei\u03b8(m\u2212n)) = g(xm, xn, m \u2212 n),\n(2)\nwhere g(\u00b7) is an abstract mapping function.\n3. SelfExtend\nIn this section, we first conduct a preliminary investigation\non the inherent ability of the LLMs to handle long content.\nThen, we propose our SelfExtend that effectively extends\nexisting LLMs\u2019 context window without any fine-tuning.\n3.1. Preliminary Analysis\n\u2460 Why do LLMs fail on sequences during inference that\nare longer than their pre-training context window? For\na pretrained LLM with relative position encodings, such as\nRoPE, the behavior of the LLMs becomes unpredictable\nduring inference if the length of a sequence is longer than\nits pretraining context window length. This has been ex-\nplored by (Han et al., 2023; Chen et al., 2023b) that with\nunseen relative positions, the attention distributions are very\ndifferent compared to those within the pretraining context\nwindow length. We argue that such failure stems from the\nOut-of-Distribution (O.O.D.) relative distance in the same\nsense that neural networks are not robust to O.O.D. inputs\n(Shen et al., 2021).\n\u2461 How to solve positional O.O.D. problem? One feasible\nand straightforward way to handle unseen relative positions\nis to map them to positions that were seen during pretrain-\ning. We can use the FLOOR operation to map the unseen\npositions to positions within the pretraining context window,\nas shown in Figure 1. The proposed method is identical to\nthe original self-attention mechanism except that the FLOOR\noperation is applied to each token\u2019s original position before\nthe inner product. We denote the self attention with the\nFLOOR operation applied as \u201cgrouped attention\u201d. In Python\nstyle, the \u201cgrouped attention\u201d is denoted as:\nPg = P\n// Gs,\n(3)\nwhere P \u2208 RB\u00d7L is the original position encoding, in\nwhich B is the batch size and L is the length of the input\ntext sequence. Gs denotes the group size, which is the base\nof the FLOOR operation. Taking the floor of the position\ndivided by the group size maps the original large position\nvalues to a smaller discrete set of values, avoiding the issue\nof out-of-distribution position values during inference.\n0\n0\n1\n1\n0\n2\n2\n1\n0\n3\n3\n2\n1\n0\n4\n4\n3\n2\n1\n0\n5\n5\n4\n3\n2\n1\n0\n6\n6\n5\n4\n3\n2\n1\n0\n7\n7\n6\n5\n4\n3\n2\n1\n0\n0\n1\n2\n3\n4\n5\n6\n7\n0\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n1\n0\n0\n2\n2\n2\n1\n1\n0\n2\n2\n2\n1\n1\n0\n0\n3\n3\n3\n2\n2\n1\n1\n0\n3\n3\n3\n2\n2\n1\n1\n0\n0\n0\n0\n1\n1\n2\n2\n3\n3\n// 2\nFigure 1. Illustration of grouped attention. We suppose that the\nLLM\u2019s pretraining context window length is 5 and the length of the\ninference sequence is 8. On the left figure, we show the positional\nOut-of-Distribution (O.O.D.) issue while the input length is out\nof the pretraining context window size. The y-axis of this matrix\nrepresents the position of query tokens and the x-axis represents\nthe position of key tokens. In this case, in the relative position\nmatrix, only those in orange are seen during pretraining. Relative\npositions in gray are outside the pretraining context window. In\nthe right figure, we show how the FLOOR operation is applied and\nthe relative position matrix for grouped self attention. With the Gs\nset as 2, the positions of query tokens and key tokens are mapped\nfrom 0-7 to 0-3 by FLOOR (//). The new relative positions (in\nblue) are all within the range of the pretraining context window.\n\u2462 Can LLMs work well without accurate position in-\nformation? \u2014 Yes, but not that perfect. We show the\nperplexity (PPL) on the PG-19 (Rae et al., 2019) dataset\nwith the FLOOR operation applied to Llama-2-7b-chat across\ndifferent sequence lengths, in Figure 2. As a comparison,\nwe also show the PPL of the original model without the\nFLOOR operation as the dotted lines. From this figure, with\nthe FLOOR operation, LLMs keep a relatively low and stable\nPPL on the sequences whose lengths exceed the pretraining\ncontext window. Meanwhile, with grouped attention, the\nPPL is a little higher than the original LLMs, which is ex-\npected. However, the model\u2019s PPL behavior is similar to\nthe original model, as the PPL is nearly unchanged within\nthe \u201ccontext window\u201d (for Llama-2: 2 - 8192, 4 - 16384,\nand 8 - 32768), demonstrating the effectiveness of group\nattention. Once the length of a sequence is longer than the\nnew \u201ccontext window\u201d (e.g., sequences with 10k tokens as\nthe input, with a group size of 2 ), the PPL explodes again\ndue to the positional O.O.D issue.\n\u2463 How to restore degraded language modeling ability\ncaused by grouped attention? \u2014 Re-introducing nor-\nmal attention in the neighboring area. In the process of\ngenerating next tokens, the immediate neighbors of a tar-\nget token play a crucial role, which is well-supported by\nexisting methods of sparse attention mechanisms (Zaheer\net al., 2020; Shi et al., 2021) and methods for extending the\ncontextual window (Han et al., 2023; Xiong et al., 2023;\nChen et al., 2023c). These studies consistently highlight\nthe importance of maintaining the standard attention mecha-\nnism for tokens in close proximity to the target token. This\nproximity-based focus is essential for the accurate genera-\n3\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\n4k (4096)\n6k (6144)\nFigure 2. Perplexity (PPL) using grouped attention with different\ngroup sizes under different sequence lengths on PG-19 dataset.\nThe original Llama-2-7b-chat PPL is stable at 4k (4096) sequences\n(red dotted line) but explodes at 6k (6144) sequences (purple dotted\nline). The results show the LLMs keep a relatively low and stable\nPPL on long sequences with grouped attention.\ntion of the next token, ensuring the coherence and fluency\nof the generated text, as evidenced by acceptable perplexity\n(PPL) levels. Employing grouped attention might not sig-\nnificantly affect the overall quality of generated sentences;\nhowever, it necessitates the accurate positioning of attention\nto maintain generation quality. Therefore, it is imperative\nto preserve the standard attention mechanism within the\nvicinity of the target token, as utilized during the pretraining\nphase, to ensure the precision and effectiveness of language\nmodels in capturing the nuances of local context.\n3.2. SelfExtend LLM Context Window Without Tuning\nWe introduce SelfExtend, a method that enhances LLMs\u2019\nnatural capability to process extensive contexts without the\nneed for fine-tuning. SelfExtend incorporates two distinct\ntypes of attention mechanisms: 1) Grouped attention, specif-\nically designed for tokens that are far apart. This approach\napplies a floor operation to the positions to manage long-\ndistance relationships between tokens; 2) Standard attention,\nwhich employs the conventional attention mechanism for\nadjacent tokens within a specified range. The SelfExtend\nframework is depicted in Figure 3. Notably, SelfExtend\nmodifies only the attention mechanism during inference,\neliminating the need for additional fine-tuning.\nMaximum Extended Length of SelfExtend Suppose that\nwe have the pretraining context window size as L, the group\nsize for grouped attention as Gs, and the window size for\nneighbor tokens as wn. We shift the relative position of\ngrouped attention by wn \u2212 wn//Gs before merging the two\npieces of attention together. This ensures that the transition\nfrom the normal attention area to the grouped attention area\nsmooth. We merge the two parts of attention by replacing\nthe attention values out of the neighbor token window with\nthe attention values from the grouped attention. All the\nmodifications are applied before the softmax operation and\nTable 1. Perplexity on dataset PG19 with Llama-2-7b-chat and\nMistral-7b-instruct-0.1. We report the PPL of with&without Slid-\ning Window Attention (SWA) for Mistral.\nModel\nEvaluation Context Window Size\nName\n4096\n6144\n8192\n10240\n12288\n14336\n16384\nLlama-2-7b-chat\n9.181\n> 103\n> 103\n> 103\n> 103\n> 103\n> 103\nSelfExtend-Llama-2-7b-chat\n8.885\n8.828\n9.220\n8.956\n9.217\n9.413\n9.274\nMistral-7b-instruct-0.1 w/ SWA\n9.295\n9.197\n9.532\n9.242\n9.198\n9.278\n9.294\nMistral-7b-instruct-0.1 w/o SWA\n9.295\n9.205\n10.20\n55.35\n> 103\n> 103\n> 103\nSelfExtend-Mistral-7b-instruct-0.1\n9.272\n9.103\n9.369\n9.070\n8.956\n9.022\n9.128\nother parts remain unchanged. Ideally, the maximum length\nof the extended context window is:\n(L \u2212 wn) \u2217 Gs + wn.\n(4)\nFor example, in Figure 3, the context window is extended\nfrom its pretraining length of 7 to (7 \u2212 4) \u2217 2 + 4 = 10. The\npseudo code for SelfExtend are presented in Algorithm 1.\nRelation to Existing Work The grouped attention in SelfEx-\ntend can be viewed as a form of position interpolation (Chen\net al., 2023b), where some positions are interpolated to be\ninfinitely close to pretraining positions. Another finetuning-\nfree method, ReRoPE (Su, 2023), is equivalent to a special\ncase of SelfExtend: the group size is large enough that all\ntokens outside the neighbor window fall into the same group\n(e.g. group size 10,000 in Figure 5). T5 (Raffel et al., 2020)\nand iRPE (Wu et al., 2021) also share the high-level idea\nof multi-level positional encodings, while applying it dur-\ning pretraining. T5 is more similar to ReRoPE for using\nthe same position for distant tokens. iRPE has finer distant\nposition encodings, more akin to SelfExtend.\n4. Experiments\nWe evaluate SelfExtend with Llama-2 (Touvron et al.,\n2023) and its families, Phi-2 (Javaheripi et al., 2023), Mis-\ntral (Jiang et al., 2023) and SOLAR (Kim et al., 2023) on\nlanguage modeling task, synthetic long context tasks, real-\nworld long context tasks and standard short-context tasks.\n4.1. Performance on Language Modeling Tasks\nLanguage modeling task is the most fundamental and the\nleast requirement for LLMs, which is usually measured by\nperplexity (PPL) on the test text data. A low PPL does\nnot guarantee good performance on real tasks (Pal et al.,\n2023), however, a higher PPL suggests severe performance\ndegradation of LLMs. We evaluate SelfExtend\u2019s language\nmodeling performance on dataset PG19 (Rae et al., 2019),\nwhich contains lengthy books. PPL is used as the metric.\nMore experimental details are presented in Appendix D.1\nThe results show that SelfExtend can successfully maintain\na low PPL out of the pretraining context window for both\nLlama-2-7b-chat and Mistral. Without SelfExtend, the PPL\nexplodes when the length of test sequence is larger than\n4\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\n0\n1\n0\n2\n1\n0\n3\n2\n1\n0\n4\n3\n2\n1\n0\n4\n4\n3\n2\n1\n0\n5\n5\n4\n3\n2\n1\n0\n5\n5\n4\n4\n3\n2\n1\n0\n6\n6\n5\n5\n4\n3\n2\n1\n0\n6\n6\n5\n5\n4\n4\n3\n2\n1\n0\n0\n0\n0\n1\n1\n0\n1\n1\n0\n0\n2\n2\n1\n1\n0\n2\n2\n1\n1\n0\n0\n3\n3\n2\n2\n1\n1\n0\n3\n3\n2\n2\n1\n1\n0\n0\n4\n4\n3\n3\n2\n2\n1\n1\n0\n4\n4\n3\n3\n2\n2\n1\n1\n0\n0\n0\n1\n0\n2\n1\n0\n3\n2\n1\n0\n4\n3\n2\n1\n0\n5\n4\n3\n2\n1\n0\n6\n5\n4\n3\n2\n1\n0\n7\n6\n5\n4\n3\n2\n1\n0\n8\n7\n6\n5\n4\n3\n2\n1\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n// 2\n+ (4 - 4//2 )\nSoftMax\nGrouped Attention\nSelfExtend\nNormal Attention\nFigure 3. Illurstation of SelfExtend. This figure shows the attention score matrix (before SoftMax operation) of SelfExtend while a\nsequence of length 10 is fed into an LLM with the pretraining context window size (L = 7). The numbers denote the relative distances\nbetween the corresponding query and key tokens. SelfExtend has two kinds of attention mechanism: for neighbor tokens within the\nneighbor window (wn = 4), it adapts the normal self-attention; for tokens out of the window, it adapts the values from the grouped\nattention. The group size (Gs) is set to 2. We then merge two parts attention matrices and apply the softmax operation.\nMistral w/ SWA\nSelfExtend Mistral \nFigure 4. Passkey retrieval accuracy for Mistral-7b-instruct-0.1\nwith SWA or SelfExtend. Mistral with SelfExtend obtains 100%\npasskey retrieval accuracy For all sequence length (token limit)\nand all depth. Mistral with SWA cannot retrieve the passkey out of\nthe sliding window. The default sliding window size is 4096.\nthe context window. Mistral with SWA can also maintain\na low PPL out of its context window. But later in the next\nsection, we will demonstrate that a low PPL score does not\nnecessarily indicate proficiency in handling long contexts.\nMore discussion about PPL can be found in Appendix B.\n4.2. Performance on Synthetic Long Context Tasks\nThe passkey retrieval task is the same as what is defined in\nLandmark Attention (Mohtashami & Jaggi, 2023), which is\na synthetic long context task. It requires a language model\nto retrieve a simple passkey (i.e., a 5-digit random number)\nin a long meaningless text sequence. The passkey is placed\nwith various document depths (where the passkey is placed\nin the input texts) and context lengths (ranging from 4k to\n24k). We tested multiple passkey retrievals for each context\nlength and depth. The passkey was randomly placed within\na span of 400 tokens. For a depth of 0.1 and context of\n8k, the passkey was placed between tokens 800 \u2212 1600.\nWe performed 10 iterations per span, so 20 total for that\nsetting. Experimental setting details and an example of\npasskey retrieval task can be found in Appendix D.2.\nThe results in Figure 4 show that without any fine-tuning,\nSelfExtend obtains 100% passkey retrieval accuracy across\nall tested depths and context lengths. The results also demon-\nstrate that: although Mistral w/ SWA has low PPL beyond\nits pretraining context window, it can only access informa-\ntion (i.e. the passkey) within its sliding window. Consider-\ning the simplicity of this task, these results strongly suggest\nit still does not have the true ability to handle long contexts.\n4.3. Performance on Real-World Long Context Tasks\nEvaluation solely on language modeling (measured by per-\nplexity) and synthetic tasks like passkey retrieval cannot\nfully assess the long-context capabilities of LLMs. The task\nof Passkey retrieval is overly straightforward, and an LLM\nmay still struggle with long context despite low perplexity.\nTo comprehensively evaluate long-context performance, we\nfurther use two recent real-world long context benchmarks:\nLongBench (Bai et al., 2023) and L-Eval (An et al., 2023).\nThe results are presented in Table 2 and Table 3.\nOn the LongBench in Table 2, for all four different base\nLLMs and most datasets, with SelfExtend, the LLM can\nobtain significant performance improvments.\nLlama-2-7B: We use SelfExtend to increase Llama-2-7b-\nchat\u2019s context from 4k to 16k and 25k. Both significantly\noutperform Llama-2-7b-chat and most fine-tuned models on\nseveral datasets like HotpotQA. We also extend vicuna1.5-\n7B from 4k to 16k and 25k. With SelfExtend, vicuna1.5-7B\nsurpasses its fine-tuned counterpart vicuna1.5-7B-16k and\nranks among top Llama-2-7b models. On some datasets, the\n25k variant underperforms the 16k one due to the trade-off\nbetween larger context and positional precision. More de-\ntails about the trade-off is in Section 4.5.\nMistral-7B: We extend Mistral-7B\u2019s context to 16k, sig-\nnificantly improving its long context ability over the base\nmodel, with or without SWA applied. The fine-tuned variant\nMistralLite ((amazon, 2023)) achieves the best performance\non most datasets. However, many of these datasets were\nincluded in MistralLite\u2019s fine-tuning data, such as Narra-\n5\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nTable 2. Performance comparison of different LLMs on LongBench. * indicates the results reported by LongBench. *indicates the\nresults are reported by CLEX (Chen et al., 2023a). + indicates the results from us. Models in green/blue/cyan/orange are based on\nLlama2-7b/Mistral-7b/Phi-2/SOLAR-10.5B. The number (e.g. \u201825k\u2019) indicates the maximum input length. The \u2018SE\u2019 prefix indicates\nSelfExtend is applied to this model. In this table, except SelfExtend, all other models require fine-tuning to extend the context window.\nCLEX is fine-tuned with 2B tokens. LongChat1.5-7B-32k and Vicuna1.5-7B-16K are fine-tuned on more than 80k conversations.\nCodeLLaMA (Rozi`ere et al., 2023) is fine-tuned on more than 500B tokens. MistralLite (Yin Song and Chen Wu and Eden Duthie, 2023)\nis also fine-tuned on more than 2B tokens (amazon, 2023). The better performance between models w/ and w/o SelfExtend is in bold.\nLLMsa\nSingle-Document QA\nMulti-Document QA\nSummarization\nFew-shot Learning\nSynthetic\nCode\nNarrativeQA\nQasper\nMultiField-en\nHotpotQA\n2WikiMQA\nMusique\nGovReport\nQMSum\nMultiNews\nTREC\nTriviaQA\nSAMSum\nPassageCount\nPassageRe\nLcc\nRepoBench-P\nSelfExtend\nLlama-2-7B-chat-4k*\n18.7\n19.2\n36.8\n25.4\n32.8\n9.4\n27.3\n20.8\n25.8\n61.5\n77.8\n40.7\n2.1\n9.8\n52.4\n43.8\nSE-Llama-2-7B-chat-16k+\n21.69\n25.02\n35.21\n34.34\n30.24\n14.13\n27.32\n21.35\n25.78\n69.50\n81.99\n40.96\n5.66\n5.83\n60.60\n54.33\nSE-Llama-2-7B-chat-25k+\n21.37\n26.68\n34.63\n35.47\n30.46\n15.51\n27.51\n21.30\n25.87\n68.50\n78.79\n41.29\n3.90\n3.50\n59.69\n53.83\nMistral-7B-ins-0.1-16k w/ SWA+\n19.40\n34.53\n37.06\n42.29\n32.49\n14.87\n27.38\n22.75\n26.82\n65.00\n87.77\n42.34\n1.41\n28.50\n57.28\n53.44\nMistral-7B-ins-0.1-8k w/o SWA+\n20.46\n35.36\n39.39\n34.81\n29.91\n11.21\n24.70\n21.67\n26.67\n68.00\n86.66\n41.28\n0.18\n24.00\n56.94\n55.85\nSE-Mistral-7B-ins-0.1-16k+b\n23.56\n39.33\n49.50\n45.28\n34.92\n23.14\n30.71\n24.87\n26.83\n69.50\n86.47\n44.28\n1.18\n29.50\n55.32\n53.44\nPhi-2-2k+\n4.46\n7.01\n19.98\n9.43\n8.55\n4.62\n25.64\n14.32\n24.03\n50.50\n74.55\n1.71\n2.83\n4.17\n58.96\n54.14\nSE-Phi-2-8k+\n12.04\n12.10\n20.15\n8.22\n9.68\n3.89\n27.90\n14.58\n22.13\n61.00\n82.82\n1.40\n2.37\n2.83\n57.87\n56.42\nSOLAR-10.7B-ins-4k+\n16.50\n24.06\n46.76\n44.03\n36.05\n22.76\n31.39\n19.81\n26.36\n70.00\n87.91\n42.49\n4.5\n26.5\n41.04\n54.36\nSE-SOLAR-10.7B-ins-16k+\n22.63\n32.49\n47.88\n46.19\n34.32\n27.88\n30.75\n22.10\n25.62\n74.50\n89.04\n42.79\n4.0\n28.0\n53.73\n56.47\nOther Methods\nLongChat1.5-7B-32k*\n16.9\n27.7\n41.4\n31.5\n20.6\n9.7\n30.8\n22.7\n26.4\n63.5\n82.3\n34.2\n1.0\n30.5\n53.0\n55.3\ntogether/llama-2-7b-32k+\n15.65\n10.49\n33.43\n12.36\n12.53\n6.19\n29.28\n17.18\n22.12\n71.0\n87.79\n43.78\n1.0\n23.0\n63.79\n61.77\nCLEX-7B-16k*\n18.05\n23.68\n44.62\n28.44\n19.53\n9.15\n32.52\n22.9\n25.55\n68\n84.92\n42.82\n0\n11.5\n59.01\n56.87\nCodeLLaMA-7B-16k*\n22.93\n30.69\n43.37\n33.05\n27.93\n14.2\n28.43\n24.18\n26.84\n70\n84.97\n43.43\n2\n13.5\n64.35\n55.87\nSE-Llama-2-7B-chat-16k+\n21.69\n25.02\n35.21\n34.34\n30.24\n14.13\n27.32\n21.35\n25.78\n69.50\n81.99\n40.96\n5.66\n5.83\n60.60\n54.33\nSE-Llama-2-7B-chat-25k+\n21.37\n26.68\n34.63\n35.47\n30.46\n15.51\n27.51\n21.30\n25.87\n68.50\n78.79\n41.29\n3.90\n3.50\n59.69\n53.83\nVicuna1.5-7B-16k*\n19.4\n26.1\n38.5\n25.3\n20.8\n9.8\n27.9\n22.8\n27.2\n71.5\n86.2\n40.8\n6.5\n4.5\n51.0\n43.5\nSE-Vicuna1.5-7B-16k+\n21.88\n35.16\n42.00\n31.14\n22.51\n13.33\n28.47\n22.24\n26.70\n69.50\n86.31\n40.54\n3.56\n7.50\n60.16\n44.07\nSE-Vicuna1.5-7B-25k+\n22.46\n34.42\n42.58\n30.95\n24.33\n12.72\n27.75\n22.26\n27.21\n72.00\n84.02\n40.38\n3.01\n7.00\n58.86\n43.86\nMistralLite-16k+\n32.12\n47.02\n44.95\n58.5\n47.24\n31.32\n33.22\n26.8\n24.58\n71.5\n90.63\n37.36\n3\n54.5\n66.27\n65.29\nSE-Mistral-7B-ins-0.1-16k+\n23.85\n37.75\n46.93\n45.35\n34.54\n23.28\n30.45\n23.58\n26.94\n69.50\n85.72\n43.88\n0.59\n28.50\n54.92\n53.44\nFixed Models\nGPT-3.5-Turbo-16k*\n23.6\n43.3\n52.3\n51.6\n37.7\n26.9\n29.5\n23.4\n26.7\n68.0\n91.4\n41.7\n4.5\n71.0\n54.7\n53.6\nXGen-7B-8k*\n18\n18.1\n37.7\n29.7\n21.1\n10.3\n27.3\n20.5\n26.2\n65.5\n77.8\n25.3\n2.1\n8.5\n38.6\n38.6\nInternLM-7B-8k*\n12.1\n16.7\n23.4\n28.7\n22.8\n9.0\n9.7\n15.9\n22.8\n52.0\n77.8\n21.2\n3.0\n6.0\n44.1\n28.8\nChatGLM2-6B-32k*\n21.1\n31.5\n46.2\n45.1\n34.0\n21.9\n32.4\n24.0\n26.5\n62.5\n78.7\n36.3\n1.5\n77.0\n55.6\n49.9\nChatGLM3-6B-32k*\n26.0\n43.3\n51.7\n54.4\n44.9\n40.4\n36.8\n23.9\n27.9\n79.0\n87.1\n38.2\n2.0\n99.0\n57.66\n54.76\nBaichuan-13B-4k*\n0.07\n17.55\n17.28\n3.29\n15\n0.1\n6.8\n1.71\n23.1\n20.05\n20.06\n5.77\n0.06\n0.5\n47.98\n16.58\nALiBi-7B-4k*\n0.04\n8.13\n17.87\n2.73\n8\n1.33\n5.31\n1.64\n25.55\n9.25\n8.83\n4.67\n0\n1.27\n46.69\n18.54\na Details of used LLMs in this table are presented in Appendix E.\ntiveQA2.\nSOLAR-10.7B and Phi-2: They have no finetuned vari-\nant for context window extension yet. SelfExtend can also\nobtain substantial performance improvements.\nOn the LEval benchmark in Table 3, we observe simi-\nlar results. Compared to fine-tuning free baselines like\nNTK or further fine-tuned models like Longchat1.5-7b-32k\nand Vicuna1.5-7b-32k, SelfExtend achieves superior perfor-\nmance on nearly all datasets3.\nIn summary, on the two benchmarks, SelfExtend achieves\ncomparable or better performance, compared to meth-\nods that requires further fine-tuning. Despite our initial\nexpectation being that SelfExtend would simply outperform\nthe base model without additional extension methods, it\nis remarkable that our SelfExtend, which solely operates\n2More details about MistralLite\u2019s fine-tuning data can be found\nat https://huggingface.co/amazon/MistralLite. At least,\nGovReport, QMSum, NarrativeQA, Qasper, QuALITY, and Hot-\npotQA are included. Meanwhile, Multi-passage QA and sum-\nmarization tasks are also in fine-tuning data. This also violates\nzero-shot evaluation conditions.\n3LEval performance seems sensitive to prompt engineering\nfor these sub-13B LLMs. For example, on some datasets, vanilla\nvicuna-13b underperforms vanilla vicuna-7b.\nduring inference without the need for fine-tuning or training,\nachieves such impressive performance.\n4.4. Performance on Short Context Tasks\nWe argue that an ideal context length extension method\nshould not degrade performance on standard short-context\ntasks. Previous fine-tuning based methods usually undergo\nperformance degradation on short-context tasks (Peng et al.,\n2023; Xiong et al., 2023). Following (Peng et al., 2023),\nwe use Hugging Face Open LLM Leaderboard (Gao et al.,\n2023) to evaluate SelfExtend\u2019s performance on five pub-\nlic short context tasks. Specifically, we use 25-shot ARC-\nChallenge (Clark et al., 2018), 10-shot HellaSwag (Zellers\net al., 2019), 5-shot MMLU (Hendrycks et al., 2020), 0-shot\nTruthfulQA (Lin et al., 2021), and 5-shot GSM8K (Cobbe\net al., 2021). The results are shown in Table 4. We also\ninvestigate the influence of varying group sizes and neigh-\nbor window sizes on short-context tasks and we present the\nresults in Appendix C.\nThe results show that SelfExtend can maintain the perfor-\nmance of the short-context tasks, while enhance the perfor-\nmance on long-context tasks. Moreover, because SeldEx-\ntend does not require any fine-tuning and only takes effect\nduring inference, SelfExtend can be readily adopted as a\n6\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nTable 3. Exam evaluation results on L-Eval. Tokens denotes the\nmaximum input context length. + indicates the results are from us\nand others are reported by L-Eval. The rows in the same color (or-\nange, green, blue, and pink) represent the models of those rows\nfrom the same base model. The better performance between mod-\nels w/ and w/o SelfExtend is highlighted in bold.\nModel\nTokens Coursera GSM QuALITY TOEFL CodeU SFiction Avg.\nClaude1.3-100k\n100k\n60.03\n88.00\n73.76\n83.64\n17.77\n72.65\n65.97\nGPT-4-32k\n32k\n75.58\n96.00\n82.17\n84.38\n25.55\n74.99\n73.11\nTurbo-16k-0613\n16k\n63.51\n84.00\n61.38\n78.43\n12.22\n64.84\n60.73\nChatglm2-6b-8k\n2k\n43.75\n13.00\n40.59\n53.90\n2.22\n54.68\n34.69\nXGen-7b-8k (2k-4k-8k)\n2k\n26.59\n3.00\n35.15\n44.23\n1.11\n48.43\n26.41\nChatglm2-6b-8k\n8k\n42.15\n18.00\n44.05\n54.64\n2.22\n54.68\n35.95\nChatglm2-6b-32k\n32k\n47.81\n27.00\n45.04\n55.01\n2.22\n57.02\n39.01\nXGen-7b-8k\n8k\n29.06\n16.00\n33.66\n42.37\n3.33\n41.40\n27.63\nMPT-7b-65k\n8k\n25.23\n8.00\n25.24\n17.84\n0.00\n39.06\n19.22\nLlama2-7b-chat\n4k\n29.21\n19.00\n37.62\n51.67\n1.11\n60.15\n33.12\nLongchat1.5-7b-32k\n32k\n32.99\n18.00\n37.62\n39.77\n3.33\n57.02\n31.45\nLlama2-7b-NTK\n16k\n32.71\n19.00\n33.16\n52.78\n0.00\n64.84\n33.74\nSE-Llama2-7B-chat+\n16k\n35.76\n25.00\n41.09\n55.39\n1.11\n57.81\n36.02\nVicuna1.5-7b-16k\n16k\n38.66\n19.00\n39.60\n55.39\n5.55\n60.15\n36.39\nSE-Vicuna1.5-7B+\n16k\n37.21\n21.00\n41.58\n55.39\n3.33\n63.28\n36.96\nLlama2-13b-chat\n4k\n35.75\n39.00\n42.57\n60.96\n1.11\n54.68\n39.01\nLlama2-13b-NTK\n16k\n36.48\n11.00\n35.64\n54.64\n1.11\n63.28\n33.69\nLlama2-13b-NTK(Dyn)\n16k\n30.08\n43.00\n41.58\n64.31\n1.11\n35.15\n35.87\nSE-Llama2-13B-chat+\n16k\n38.95\n42.00\n41.09\n66.17\n1.11\n63.28\n42.10\nMistral-7b-ins-0.1 w/ SWA+\n16k\n44.77\n44.00\n46.53\n60.59\n2.22\n64.06\n43.70\nMistral-7b-ins-0.1 w/o SWA+\n8k\n43.60\n49.00\n45.05\n60.59\n4.44\n60.94\n43.94\nMistralLite+\n16k\n29.23\n32.00\n46.04\n17.47\n3.33\n14.06\n23.69\nSE-Mistral-7b-ins-0.1+\n16k\n45.20\n51.00\n48.02\n64.68\n3.33\n59.38\n45.27\nPhi-2+\n2k\n38.37\n64.00\n42.08\n55.76\n3.33\n52.34\n42.64\nSE-Phi-2+\n8k\n42.44\n65.00\n41.08\n62.83\n4.44\n52.34\n44.69\nSOLAR-10.7b-Instruct-v1.0+\n4k\n48.84\n72.00\n59.90\n77.32\n4.44\n69.53\n55.34\nSE-SOLAR-10.7b-v1.0+\n16k\n50.44\n72.00\n70.30\n79.18\n4.44\n73.44\n58.30\nTable 4. Performance of SelfExtend on Hugging Face Open LLM\nbenchmark compared to baselines: Llama 2, Llama-2-chat-4,\nMistral-instruct-v0.1 and Phi-2. We use the same hyper-parameters\nas on LongBench benchmark. For Llama-2 & Llama-2-chat based\nSelfExtend, the group size is 16 and neighbor window is 1024;\nfor Mistral based SelfExtend, the group size is 6 and neighbor\nwindow is 1024; for Phi-2 based SelfExtend, the group size is 12\nand neighbor window is 512.\nSize\nName\nARC-c\nHellaswag\nMMLU\nTruthfulQA GSM8k\n7B\nLlama-2\n52.99\n78.66\n46.58\n38.97\n14.94\n7B\nSE-Llama 2\n52.99\n78.65\n46.68\n38.97\n14.71\n7B\nLlama-2-chat\n52.73\n78.49\n48.20\n45.32\n18.73\n7B\nSE-Llama-2-chat-16k\n52.73\n78.49\n48.09\n45.33\n18.88\n7B\nMistral-instruct-v0.1\n54.35\n75.72\n55.57\n55.89\n30.93\n7B\nSE-Mistral-instruct-v0.1\n54.44\n75.71\n55.59\n55.89\n31.39\n2.7B\nPhi-2\n61.17\n75.13\n58.20\n44.54\n55.11\n2.7B SE-Phi-2\n61.00\n75.20\n58.29\n44.54\n55.42\nplug-in component for LLMs. This means SelfExtend can\nbe automatically and inherently disabled while encountering\nshort-text sequences. Then, with the parameters remaining\nunchanged, LLMs can maintain its original inference mech-\nanism on those short-context scenarios.\n4.5. Ablations on Group Size and Neighbor Window\nWe investigate the influence of varying the group size Gs\nand the neighbor window wn. We experiments with Phi-2\non four real-world datasets from Longbench: narrativeqa,\nqasper, triviaqa, and repobench-p. The results are presented\nin Figure 5. Form the results, we observe two trade-offs:\nTable 5. Performance of Phi-2 with different context window\nlengths. The vanilla Phi-2 has a 2k context window. SelfExtend\nextends Phi-2 to 4k (Gs = 4,wn = 512), 6k (Gs = 8,wn = 512)\nand 8k (Gs = 12,wn = 512). The performance improvement\ncompared to vanilla Phi-2 is in the parenthesis.\nContext Length\n2k (vanilla)\n4k\n6k\n8k\nDocument QA\nNarrativeQA\n4.46\n6.49 (+45.52%)\n8.98 (+101.35%) 12.04 (+169.96%)\nQasper\n7.01\n11.16 (+59.20%) 12.84 (+83.17%)\n12.10 (+72.61%)\nSummarization\nGov report\n25.46\n27.91 (+9.62%)\n28.14 (+10.53%)\n27.51 (+8.05%)\nQmsum\n14.32\n14.88 (+3.91%)\n16.72 (+16.76%)\n18.58 (+29.75%)\nFew-shot Learning\nTrec\n50.5\n60.0 (+18.81%)\n62.5 (+23.76%)\n60.0 (+18.81%)\nTriviaqa\n74.55\n84.88 (+13.86%) 82.64 (+10.85%)\n81.31 (+9.07%)\nCoding\nRepobench-p\n54.14\n56.18 (+3.77%)\n56.76 (+4.84%)\n57.05 (+5.37%)\nLcc\n58.96\n59.06 (+0.17%)\n58.88 (-0.14%)\n59.42 (+0.78%)\n1) There is a trade-off with respect to group size in SelfEx-\ntend. Generally, both too small and too large group sizes\ncan result in inferior performance compared to an optimal\nlevel. With a large group size, position information be-\ncomes more coarse, potentially causing performance drops.\nConversely, small group sizes require SelfExtend to utilize\nlarger position embeddings to extend the context window.\nThese larger position embeddings are less trained compared\nto smaller ones. For example, in Llama-2 with its 4096\ncontext window, the relative position 4095 accounts for only\n1/2048 the frequency of the relative position 2048 in train-\ning. These under-trained relative positions can also degrade\nperformance. This trade-off produces the \u2019peak\u2019 shape in\nthe figure, indicating the extended context window differs\nfrom the ideal case described in Equation (4).\n2) There is also another trade-off w.r.t. neighbor window\nsize. With larger neighbor window sizes, there is more\nprecise information about neighbor tokens, which is the\nmost important. But a larger neighbor window size means\nSelfExtend has to use a larger group size for a long se-\nquence, compared to using a smaller neighbor window size\n& smaller group size, the information about the whole se-\nquence becomes coarse.\n4.6. Performance with Varying Context Window Length\nTo validate SelfExtend\u2019s efficacy in enabling LLMs to utilize\nextended context windows, we assess Phi-2\u2019s performance\nacross varying context lengths with SelfExtend, referencing\nTable 5. Across four task types from LongBench, results are\ngenerally improved with longer contexts. Notably, SelfEx-\ntend monotonically enhances performance on NarrativeQA\nand Qmsum. While significant improvements are observed\nacross most datasets, a \u2019peak\u2019 in performance suggests a\ntrade-off, as discussed in Section 4.5: longer contexts of-\nfer more relevant information, but the larger group sizes\nrequired by SelfExtend to extend the context window may\n7\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nFigure 5. The performance of Phi-2 when utilizing SelfExtend to extend its context window length to 8k, with varying group sizes\nand neighbor window sizes. The y-axis indicates performance and the x-axis shows the group size. And neighbor window size is\nfrom 256, 512, 768, 1024. Group size of 10000 in this experiment means all tokens out of the neighbor window are in the same\ngroup (10000>8k). Some combination (e.g. Gs = 6 & wn = 1024) is omitted if the corresponding extended context window (Equa-\ntion (4)) is smaller than 8k. The dashed line is the performance of vanilla phi-2 with a 2k context window size.\n5\n8\n16\n36\n48\n64\n100\nNumber of Passkey Digits\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPasskey Retrieval on 16000 long sequences\nvicuna-1.5-7b-16k\nllama-2-7b-32k\nYarn-llama-2-7b-64k\nLlama-2-7b-Longlora-16k\nSelfExtend-16k-G_s-8\nSelfExtend-16k-G_s-12\nSelfExtend-16k-G_s-16\nSelfExtend-16k-G_s-100000\nFigure 6. Passkey retrieval accuracy for four fine-tuning-based\nlong-context models and SelfExtend on Llama-2-chat-7b across\nfour group sizes: 8, 12, 16, and 100000. For SelfExtend, the neigh-\nbor window is 1024. A group size of 100000 indicates that all\ntokens outside the neighbor window are in the same group.\ncause less precise positional information4. Regarding Lcc,\nperformance remains consistent, possibly due to its reliance\non local codes and shorter dataset lengths5.\n4.7. Varying-Length Passkey Retrieval Task\nThe conventional passkey retrieval task, along with preva-\nlent benchmark datasets, primarily assesses the proficiency\nof LLMs in identifying and leveraging pertinent information.\nTraditionally, this task involves passkeys not exceeding 5\ndigits in length. To evaluate the LLMs\u2019 capabilities of pro-\nducing consistent and precise outcomes for long sequences,\nwe extended the task to incorporate passkeys with larger\nlengths. We test passkeys in 5, 8, 16, 36, 48, 64, 100 dig-\nits. The input sequence contains 16, 000 characters. More\ndetails are presented in Appendix D.3.\nThe results, depicted in Figure 6, illustrate a common trend:\nwhile short passkeys of 5 or 8 digits are easily managed\n4Other possible reasons include: Phi-2 is a base model without\ninstruction tuning, and SelfExtend\u2019s performance is not optimal as\nwe use the same set of hyperparameters across all datasets, which\ncannot showcase SelfExtend\u2019s full potential\n5With Phi-2 tokenizer, over 60% of Lcc instances are under\n4096 tokens, with an average length of 4069.7\nby all, divergences in performance emerge as the length\nof passkey increases. Notably, with the exception of Yarn,\nmany tuning-based methods are unable to accurately repro-\nduce passkeys beyond 64 digits, and some of them even ex-\nperience a marked decline in performance when the passkey\nlength exceeds 16 digits. Remarkably, although without\ntuning, SelfExtend maintains its superiority. These find-\nings suggest that we should carefully choose the training\napproach when fine-tuning models to handle long contexts.\n5. Conclusion and Discussion\nIn this paper, we argue that LLMs themselves have the inher-\nent ability to handle long sequences and propose SelfExtend\nto elicit the inherent long context abilities for LLMs by sim-\nply mapping unseen relative positions into those seen during\npretraining via the Floor operation. Without any tuning or\nfurther training, SelfExtend can effectively improve LLMs\u2019\nlong context performance, as extensive experiments show.\nLimitations: SelfExtend increases computation cost with\nnaive implementations since it performs extra attention\nacross all query-key pairs. However, with optimizations like\nblocked kernels (e.g., Flash Attention (Dao et al., 2022)),\nthis becomes linear rather than quadratic, and the marginal\ncost is small enough to be ignored for long input sequences.\nAlso, the performance degrades with large group size, pre-\nventing indefinitely long contexts. Additionally, evaluation\nmethodologies for assessing long context abilities remain\nopen research questions.\nFuture Work: We are interested in testing SelfExtend on\nmodels using other positional encoding. Larger models,\nlonger contexts, and more challenging tasks will be tested if\nwe can access more computational resources in the future.\nIn the meantime, more sophisticated mapping methods will\nbe considered as the replacement of the simple FLOOR oper-\nation to achieve better long context understanding abilities\nand extended context window length.\n8\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nReferences\namazon. Mistrallite model. https://huggingface.co/\namazon/MistralLite, 2023.\n[Online; accessed 29-\nDecember-2023].\nAn, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong,\nL., and Qiu, X. L-eval: Instituting standardized evalu-\nation for long context language models. arXiv preprint\narXiv:2307.11088, 2023.\nAnothropic.\nLong context prompting for claude\n2.1.\nhttps://www.anthropic.com/news/\nclaude-2-1-prompting, 2023.\nBai, T., Luo, J., Zhao, J., Wen, B., and Wang, Q. Recent\nadvances in adversarial training for adversarial robustness.\narXiv preprint arXiv:2102.01356, 2021.\nBai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z.,\nDu, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: A\nbilingual, multitask benchmark for long context under-\nstanding. arXiv preprint arXiv:2308.14508, 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nChen, G., Li, X., Meng, Z., Liang, S., and Bing, L. Clex:\nContinuous length extrapolation for large language mod-\nels. arXiv preprint arXiv:2310.16450, 2023a.\nChen, S., Wong, S., Chen, L., and Tian, Y. Extending\ncontext window of large language models via positional\ninterpolation. arXiv preprint arXiv:2306.15595, 2023b.\nChen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and\nJia, J. Longlora: Efficient fine-tuning of long-context\nlarge language models. arXiv preprint arXiv:2309.12307,\n2023c.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., Hesse, C., and Schulman, J. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168,\n2021.\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and\nSalakhutdinov, R. Transformer-xl: Attentive language\nmodels beyond a fixed-length context. arXiv preprint\narXiv:1901.02860, 2019.\nDao, T., Fu, D., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat-\ntention: Fast and memory-efficient exact attention with\nio-awareness. Advances in Neural Information Process-\ning Systems, 35:16344\u201316359, 2022.\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li, H.,\nMcDonell, K., Muennighoff, N., Ociepa, C., Phang, J.,\nReynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L.,\nTang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A\nframework for few-shot language model evaluation, 12\n2023. URL https://zenodo.org/records/10256836.\ngkamradt.\nLlmtest needleinahaystack:\nDoing simple\nretrieval from llm models.\nhttps://github.com/\ngkamradt/LLMTest NeedleInAHaystack/tree/main,\n2023. [Online; accessed 29-December-2023].\nHan, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S.\nLm-infinite: Simple on-the-fly length generalization for\nlarge language models. arXiv preprint arXiv:2308.16137,\n2023.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\nM., Song, D., and Steinhardt, J.\nMeasuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\nJavaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck,\nS., Mendes, C. C. T., Chen, W., Giorno, A. D., Eldan, R.,\nGopi, S., Gunasekar, S., Javaheripi, M., Kauffmann, P.,\nLee, Y. T., Li, Y., Nguyen, A., de Rosa, G., Saarikivi, O.,\nSalim, A., Shah, S., Santacroce, M., Behl, H. S., Kalai,\nA. T., Wang, X., Ward, R., Witte, P., Zhang, C., and\nZhang, Y. Phi-2: The surprising power of small language\nmodels, 2023.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nKe, G., He, D., and Liu, T.-Y.\nRethinking positional\nencoding in language pre-training.\narXiv preprint\narXiv:2006.15595, 2020.\nKim, D., Park, C., Kim, S., Lee, W., Song, W., Kim, Y., Kim,\nH., Kim, Y., Lee, H., Kim, J., et al. Solar 10.7 b: Scaling\nlarge language models with simple yet effective depth\nup-scaling. arXiv preprint arXiv:2312.15166, 2023.\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. arXiv preprint\narXiv:2109.07958, 2021.\nLiu, J., Shen, Z., He, Y., Zhang, X., Xu, R., Yu, H., and Cui,\nP. Towards out-of-distribution generalization: A survey.\narXiv preprint arXiv:2108.13624, 2021.\n9\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nMohtashami, A. and Jaggi, M.\nLandmark attention:\nRandom-access infinite context length for transformers.\narXiv preprint arXiv:2305.16300, 2023.\nPal, A., Karkhanis, D., Roberts, M., Dooley, S., Sundarara-\njan, A., and Naidu, S. Giraffe: Adventures in expanding\ncontext lengths in llms. arXiv preprint arXiv:2308.10882,\n2023.\nPeng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:\nEfficient context window extension of large language\nmodels. arXiv preprint arXiv:2309.00071, 2023.\nPress, O., Smith, N. A., and Lewis, M. Train short, test\nlong: Attention with linear biases enables input length\nextrapolation. arXiv preprint arXiv:2108.12409, 2021.\nRae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap,\nT. P. Compressive transformers for long-range sequence\nmodelling. arXiv preprint arXiv:1911.05507, 2019.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551, 2020.\nRozi`ere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,\nTan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950, 2023.\nShen, Z., Liu, J., He, Y., Zhang, X., Xu, R., Yu, H., and Cui,\nP. Towards out-of-distribution generalization: A survey.\narXiv preprint arXiv:2108.13624, 2021.\nShi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., and\nKwok, J. T.-Y. Sparsebert: Rethinking the importance\nanalysis in self-attention. In International Conference on\nMachine Learning, pp. 9547\u20139557. PMLR, 2021.\nSu, J. Rectified rotary position embeddings. https://\ngithub.com/bojone/rerope, 2023.\nSu, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y.\nRoFormer: Enhanced transformer with rotary position\nembedding, 2022. arXiv: 2104.09864.\nSun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim,\nA., Chaudhary, V., Song, X., and Wei, F.\nA length-\nextrapolatable transformer. In Rogers, A., Boyd-Graber,\nJ., and Okazaki, N. (eds.), Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 14590\u201314604, Toronto,\nCanada, July 2023. Association for Computational Lin-\nguistics.\ndoi: 10.18653/v1/2023.acl-long.816.\nURL\nhttps://aclanthology.org/2023.acl-long.816.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nWu, K., Peng, H., Chen, M., Fu, J., and Chao, H. Rethinking\nand improving relative position encoding for vision trans-\nformer. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 10033\u201310041, 2021.\nXiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Ef-\nficient streaming language models with attention sinks.\narXiv preprint arXiv:2309.17453, 2023.\nXiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P.,\nHou, R., Martin, L., Rungta, R., Sankararaman, K. A.,\nOguz, B., et al. Effective long-context scaling of founda-\ntion models. arXiv preprint arXiv:2309.16039, 2023.\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R.,\nSiddhant, A., Barua, A., and Raffel, C. mt5: A massively\nmultilingual pre-trained text-to-text transformer. arXiv\npreprint arXiv:2010.11934, 2020.\nYang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H.,\nYin, B., and Hu, X. Harnessing the power of llms in\npractice: A survey on chatgpt and beyond. arXiv preprint\narXiv:2304.13712, 2023.\nYin Song and Chen Wu and Eden Duthie. amazon/Mistral-\nLite, 2023. URL https://huggingface.co/amazon/\nMistralLite.\nZaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al-\nberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,\nYang, L., et al. Big bird: Transformers for longer se-\nquences.\nAdvances in neural information processing\nsystems, 33:17283\u201317297, 2020.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. Hellaswag: Can a machine really finish your sentence?\narXiv preprint arXiv:1905.07830, 2019.\nZhang, J., Chao, H., Dhurandhar, A., Chen, P.-Y., Tajer,\nA., Xu, Y., and Yan, P. When neural networks fail to\ngeneralize? a model sensitivity perspective. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 37, pp. 11219\u201311227, 2023.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,\net al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n10\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,\nMin, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of\nlarge language models. arXiv preprint arXiv:2303.18223,\n2023.\n11\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nA. Pseudocode of SelfExtend\nAlgorithm 1 PyTorch-style Pseudocode of SelfExtend\nq, k, v # queries , keys , and values\nseq_len , pos # input sequence length , position_idx\ng_size , w_size = G, w_n\n# normal self -attention\nngb_q = apply_pos_emcode(q, pos)\nngb_k = apply_pos_emcode(k, pos)\nngb_attn = matmul(ngb_q , ngb_k)\nngb_attn = causal_mask(ngb_attn)\n# grouped self -attention\ng_pos = pos // g_size\n# the floor operation\nshift = w_size - w_size // g_size\ns_g_pos = g_pos + shift\ng_q = apply_pos_emcode(q, s_g_pos)\ng_k = apply_pos_emcode(k, g_pos)\ng_attn = matmul(g_q , g_k)\ng_attn = causal_mask(g_attn)\ng_mask = tril(ones([seq_len -w_size , seq_len -w_size ]))\nmask = ones([seq_len , seq_len ])\nmask[w_size:, :-w_size] -= g_mask\nattn = where(mask , ngb_attn , g_attn) # merge by replacement\nattn_weights = softmax(attn)\noutput = matmul(attn_weights , v)\nB. Perplexity as a Metric for Long Context Capabilities\nPPL is not an effective metric for measuring the ability of LLMs to handle long contexts. In Figure 7, we introduce\na seemingly plausible context window extension method named \u2019Infinite\u2019. When evaluated on PG19 using the same\nprotocol, Llama-2-7b-chat with \u2019Infinite\u2019 achieves PPL scores that are comparable to, or even lower than, those achieved by\nSelfExtend, as demonstrated in Table 6. However, \u2019Infinite\u2019 essentially mimics the process of dividing a long sequence into\nshort sub-sequences before processing them with LLMs, indicating that it does not genuinely address long context handling.\n\u2018Infinite\u2019: pure local attention\n \n~\n\u2018Extend\u2019 LLMs to\n   \uff01\n\u267e\nX 4\nFigure 7. \u2019Infinite\u2019: a seemingly plausible method that extends an LLM\u2019s context window to \u2019infinite\u2019 length. It achieves this by dividing\nthe entire self-attention area into multiple small, local self-attention areas. The size of the local window (i.e., the spanning range) of a\nlocal self-attention area is the sole hyperparameter for \u201dInfinite\u201d. For instance, with a local window set to 4 and a 16-token-long input,\n\u201dInfinite\u201d essentially processes the input as four sequences of 4 tokens each.\nThe discrepancy between Perplexity (PPL) and long context ability primarily stems from how PPL is calculated by averaging\nover numerous tokens. As long as the majority of tokens are modeled accurately, PPL will remain low. This is closely related\nto the influence of neighboring tokens. Information from neighboring tokens\u2014such as those within the local attention\nwindow of \u2019Infinite\u2019\u2014can suffice for predicting most tokens, thus leading to a low PPL. However, a few critical tokens,\nwhich are crucial for understanding long contexts and answering questions, may not be predicted accurately.\nAdditionally, unlike the pre-training process where the cross-entropy loss corresponds directly to perplexity, measuring PPL\n12\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nTable 6. Perplexity on the PG19 dataset: For \u2019Infinite\u2019, we set three different local window sizes: 1024, 2048, and 4096. We have also\nincluded the results from Table 1 for comparison.\nModel\nEvaluation Context Window Size\nName\n4096\n6144\n8192\n10240\n12288\n14336\n16384\nLlama-2-7b-chat\n9.181\n> 103\n> 103\n> 103\n> 103\n> 103\n> 103\nSelfExtend-Llama-2-7b-chat\n8.885\n8.828\n9.220\n8.956\n9.217\n9.413\n9.274\n1024-\u2018Infinite\u2019\u2013Llama-2-7b-chat\n9.556\n9.393\n9.728\n9.266\n9.400\n9.369\n9.142\n2048-\u2018Infinite\u2019\u2013Llama-2-7b-chat\n9.288\n9.045\n9.478\n8.993\n9.128\n9.105\n8.872\n4096-\u2018Infinite\u2019\u2013Llama-2-7b-chat\n9.181\n9.045\n9.506\n8.993\n9.165\n9.105\n8.856\nMistral-7b-instruct-0.1 w/ SWA\n9.295\n9.197\n9.532\n9.242\n9.198\n9.278\n9.294\nMistral-7b-instruct-0.1 w/o SWA\n9.295\n9.205\n10.20\n55.35\n> 103\n> 103\n> 103\nSelfExtend-Mistral-7b-instruct-0.1\n9.272\n9.103\n9.369\n9.070\n8.956\n9.022\n9.128\nduring inference is static. It resembles a specific point on the loss curve observed during pre-training. While a decreasing\ntrend in loss during pre-training indicates good performance, a single point on the training loss curve cannot determine the\nperformance.\nIn summary, while low PPL is essential for a good model, lower PPL does not necessarily equate to better performance in\nunderstanding long contexts.\nC. SelfExtend on Group Size and Neighbor Window\nTo comprehensively understand SelfExtend\u2019s influence on LLMs, unlike previous experiments which used long context\nsettings, we evaluate with smaller neighbor window sizes on four standard benchmark tasks: ARC-c, GSM8k, Hellaswag\nand MMLU. We use Phi-2 as the extended LLM. The results are shown in Figure 8. We didn\u2019t include TruthfulQA because\nits average length is less than 300 words, while the four datasets we used have an average length greater than 700 words.\nIn general, SelfExtend has a minor influence on Phi-2 as long as the neighbor window size is over 128. In many cases,\nSelfExtend even performs slightly better than vanilla Phi-2. When the neighbor window is too small (e.g. 64 tokens), if the\ngroup size is large, as expected, the positional information loss is too high and Phi-2\u2019s performance degrades. Also, on\ndifficult tasks such as MMLU and Helleswag, we observe a monotonic decrease in performance with increasing group size\nfor all neighbor windows. In summary, even when applying SelfExtend to short context tasks, as long as the hyperparameters\nare not extreme, SelfExtend does not harm the model.\nFigure 8. Phi-2 with SelfExtend on GSM8K, Helleswag, MMLU and ARC-c, compared to the vanilla Phi-2 (Phi-2-2k). The x-axis shows\nthe group size and the y-axis indicates performance as measured by the corresponding metrics.\nD. Detailed Experimental Setting\nIn this appendix, we present the details of the experiments in our paper.\n13\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nD.1. Experimental Setting on Language Modeling Tasks\nThis is not the standard setting for PPL testing on PG-19. We use the first sentence of each book in PG19\u2019s test set (100\nbooks) to test the language modeling ability. The results cannot be directly compared to the PPL reported by other papers.\nWe chose this setting because our computation resources are very limited. This setting saves a lot and it can still show the\nbehavior of LLMs w.r.t. PPL. All PPL results were calculated using the sliding window method (Press et al., 2021) with\nS = 256. We evaluated how the PPL changes as the input length increases. In Table 1, SelfExtend extends the original\nLlama-2\u2019s context window length from 4096 (4k) to over 16384 (16k) with group size Gs set as 8 and neighbor window wn\nset as 1024 (1k). For Mistral model, without SWA, the context window is 8192 (8k) and it is also extended by SelfExtend\nwith the same setting to larger than 16k. With SWA, Mistral can digest an infinite length of sequences and its default sliding\nwindow is 4096.\nD.2. Experimental Setting on Passkey Retrieval Task\nCompared to other synthetic tasks, such as \u201dNeedle in a Haystack\u201d (gkamradt, 2023), the model\u2019s performance on this is not\nsensitive to the prompt (Anothropic, 2023). This may come from the fact that the sentence carrying the passkey is very\ndifferent from those repeated random texts surrounding it. Empirically, within the effective context window, almost all LLMs,\nincluding those without any instruction tuning or alignment, can locate the sentence carrying the passkey. Although this task\nis easy and far from real-world scenarios, it tests two fundamental capabilities of LLMs: 1. The model should be able to\nrecognize and locate the useful information across all positions of the input sequence (the most fundamental understanding\ncapability); 2. The model should be able to use the perceived information to finish tasks (the most fundamental generation\ncapability).\nAn example of passkey is as the following:\nExample:\nPrompt: There is an important info hidden inside a lot of irrelevant text. Find it and memorize it. I will\nquiz you about the important information there ...... back again. The grass is green. The sky is blue.\nThe sun is yellow. Here we go. There and back again.The grass is green. The sky is blue. The sun is\nyellow. Here we go. There and back again.The grass is green. The sky is blue. The sun is yellow. Here\nwe go. There and back again.The pass key is 60151. Remember it. 60151 is the pass key. The grass is\ngreen. The sky is blue. The sun is yellow. Here we go. There and back again.The grass is green. The sky\n. . . . . . What is the passkey?\nGround Truth: 60151\nFigure 9. An example of the passkey retrieval task.\nD.3. Experimental Setting on Varying-Length Passkey Retrieval Task\nIn this experiment, we use the following models: Llama2-7b-chat with SelfExtend, LongLora-7b-16k6,vicuna-1.5-7b-16k,\nTogether AI\u2019s Llama-2-7b-32k7, and Yarn-Llama-2-7b-64k.\nE. Details of LLMs\nHere, we list the links to the details of the LLMs utilized in our experiments.\n6We use its fully fine-tuned variant. For more details about the model: https://huggingface.co/Yukang/Llama-2-7b-longlora-16k. We\ndidn\u2019t use the latest version named \u2019LongAlpaca\u2019 as we cannot get reasonable performance for this specific task with LongAlpaca, which\nmay be due to our improper configuration.\n7Both vicuna-1.5-7b-16k and Together AI\u2019s Llama-2-7b-32k were fine-tuned using position interpolation\n14\nLLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\nTable 7. LLMs used in the experiments\nModel Name\nURL\nLlama-2-7b-chat-hf (Touvron et al., 2023)\nhttps://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nMistral-7B-Instruct-v0.1 (Jiang et al., 2023)\nhttps://huggingface.co/mistralai/\nMistral-7B-Instruct-v0.1\nPhi-2 (Javaheripi et al., 2023)\nhttps://huggingface.co/microsoft/phi-2\nSOLAR-10.7B-Instruct-v1.0 (Kim et al., 2023)\nhttps://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.\n0\nLongChat-7b-v1.5-32k\nhttps://huggingface.co/lmsys/longchat-7b-v1.5-32k\ntogethercomputer/LLaMA-2-7B-32K\nhttps://huggingface.co/togethercomputer/LLaMA-2-7B-32K\nCLEX-7B-16K (Chen et al., 2023a)\nhttps://huggingface.co/DAMO-NLP-SG/CLEX-7B-16K\nCodeLlama-7b-hf (Rozi`ere et al., 2023)\nhttps://huggingface.co/codellama/CodeLlama-7b-hf\nvicuna-7b-v1.5-16k\nhttps://huggingface.co/lmsys/vicuna-7b-v1.5-16k\nMistralLite (amazon, 2023)\nhttps://huggingface.co/amazon/MistralLite\n15\n"
  },
  {
    "title": "Boundary Attention: Learning to Find Faint Boundaries at Any Resolution",
    "link": "https://arxiv.org/pdf/2401.00935.pdf",
    "upvote": "16",
    "text": "Boundary Attention: Learning to Find Faint Boundaries at Any Resolution\nMia Gaia Polansky1,2\nCharles Herrmann1\nJunhwa Hur1\nDeqing Sun1\nDor Verbin1\nTodd Zickler1\n1Google\n2Harvard University\nFigure 1. Despite being trained on synthetic data, our method outperforms state-of-the-art EDTER [28], HED [35], and Pidinet [31] at\nmultiple levels of real sensor noise present in photographs from ELD [34]. Our method produces accurate and well-defined boundaries\ndespite significant amounts of noise.\nAbstract\nWe present a differentiable model that explicitly models\nboundaries\u2014including contours, corners and junctions\u2014\nusing a new mechanism that we call boundary attention. We\nshow that our model provides accurate results even when\nthe boundary signal is very weak or is swamped by noise.\nCompared to previous classical methods for finding faint\nboundaries, our model has the advantages of being differ-\nentiable; being scalable to larger images; and automati-\ncally adapting to an appropriate level of geometric detail in\neach part of an image. Compared to previous deep methods\nfor finding boundaries via end-to-end training, it has the\nadvantages of providing sub-pixel precision, being more re-\nsilient to noise, and being able to process any image at its\nnative resolution and aspect ratio.\n1. Introduction\nConverting a precise contour that is defined mathemati-\ncally in continuous 2D space to its discrete representation\nin pixel-space is a common task in computer graphics, and\n1\narXiv:2401.00935v1  [cs.CV]  1 Jan 2024\nthere are established techniques for rasterization [12, 27],\nanti-aliasing [6, 16] and so on. However, the inverse prob-\nlem in computer vision of robustly inferring precise and un-\nrasterized descriptions of contours from discrete images re-\nmains an open challenge. Earlier work in the field inferred\nunrasterized representations of edges [4, 8, 13, 14, 22, 25]\u2014\nand sometimes corners and junctions too\u2014and it explored\nalgorithms for connecting them into boundaries via edge-\nlinking and MRFs. But since the dawn of deep learning,\nboundaries have almost exclusively been modeled using\ndiscrete, rasterized maps. Moreover, it is not uncommon\nto rely heavily on supervision from human annotations, and\neven modern datasets can suffer from annotation errors on\nthe order of pixels, quite far from sub-pixel accuracy.\nTaking inspiration from early computer vision work, we\npresent a model for inferring unrasterized boundaries that\ncan be learned instead of designed. We show that this model\nis able to benefit from the power of deep learning while also\nrealizing many of the advantages\u2014such as robustness to\nnoise, sub-pixel precision, and adaptability between signal-\ntypes\u2014that have long been the potential strengths of classi-\ncal bottom-up techniques.\nThe core of our model is a mechanism we call bound-\nary attention. It is a boundary-aware local attention oper-\nation that, when applied densely and repeatedly, progres-\nsively refines a field of variables that specifies the local\nboundaries surrounding every pixel. The model\u2019s output\nis a field of overlapping geometric primitives that can be\nused in a variety of ways, including to produce an unsigned\ndistance function for the image\u2019s boundaries, a boundary-\naware smoothing of its channel-values, and a field of soft\nlocal attention maps that relate every pixel to its neighbors.\nTo evaluate our model, we consider the problem of find-\ning boundaries in images corrupted by extreme amounts of\nnoise [23, 24, 33], where the images can be of any size and\nresolution, and where we know little about the noise and\nthe objects or shapes we are looking for. We choose this\nproblem because it relies entirely on having a strong model\nfor the basic topological and geometric properties of bound-\naries, namely that they are piecewise smooth curves which\nconnect at corners or junctions and divide regions of homo-\ngeneity.\nWe show that our model has several advantages.\nAll\nof its components are local and invariant to discrete spatial\nshifts, so it can be trained on small-sized images and then\ndeployed on much larger and differently-shaped ones. It is\nalso very compact, comprising only 207k parameters, and\nit runs several times faster than many alternatives. We also\nfind that it can be trained to a useful state with very simple\nsynthetic data, made up of random circles and triangles that\nare uniformly colored and then corrupted by noise. Despite\nthis simplicity, we find that the model can generalize to real\nimages surprisingly well.\nOur main contributions can be summarized as follows:\n1. We propose a novel network design that explicitly mod-\nels boundaries using a boundary attention mechanism\nthat can be used in any deep learning framework.\n2. We demonstrate that our model based on boundary atten-\ntion is more effective than current state-of-the-art meth-\nods at finding boundaries at high levels of sensor noise\n(Fig. 1) and can achieve sub-pixel accuracy, even in the\npresence of noise.\n2. Related Work\nIt is possible to process the outputs of certain filters to lo-\ncalize edges with sub-pixel precision (e.g., [4, 8, 13, 14,\n22, 25]).\nBut this approach struggles near corners and\njunctions because the edge filters are not derived for these,\nand accuracy breaks down.\nRecently, the field of junc-\ntions [33] showed improvements by expanding the library\nof local, unrasterized geometric primitives from classical\nedge-based primitives to a larger family of \u201cgeneralized\njunctions\u201d.\nThis allows unifying a variety of bottom-up\ncues that had often previously been handled separately, such\nas geometrically-consistent and polarity-consistent edge el-\nements being linked into contours [4, 18, 23, 24]; con-\ntours that approach one another being joined at corners and\njunctions [15, 21, 36]; homogeneous regions that look the\nsame being together; and those that look different being\napart [2, 7, 11]. The field of junctions showed that putting\nall of these cues together leads to a dramatic improved in\nnoise resilience, and it also allows analyzing the recovered\nboundaries explicitly into its component junctions, edges\nand corners. In this paper, we leverage all of these benefits,\nand we do this more efficiently and with an adaptive patch\nsize. Additionally, our model is differentiable, meaning it\ncan be used as a component in larger systems where accu-\nrate boundaries are necessary.\nOur work is complementary to recent approaches that\nleverage large-scale training to perform edge detection or\nsegmentation by internalizing the patterns in a training set.\nRecent examples include Segment Anything [19] and com-\nbinations of strong self-supervised features with clustering\nand CRFs [3, 5, 20]. These large-scale approaches rely on\nlarge training sets and high-capacity architectures that can\ninternalize high-level cues like object and scene familiarity\nas well as low-level cues like spatial coherence. Our ap-\nproach is different because it relies exclusively on low-level\ncues, has much less capacity, and uses much less training\ndata. Despite this difference, we find that our model\u2019s in-\nherent preference for contour-like boundaries allows it to\nachieve comparable (or better) results, in particular when\nnoise is high. This suggests that future work could benefit\nfrom incorporating some of our architectural elements into\nlarger and higher-capacity systems, in particular to improve\ninterpretability, efficiency, generalization or task-transfer.\n2\npatch\nFFN\nboundary\nattention\nT\ninput image\ninitial field\noutput field\nboundary-aware\nsmoothing\ndistance map\nFigure 2. Our model uses neighborhood attention with dense, stride-1 tokens (but we depict non-overlapping ones here for visual clarity).\nThe entire model is invariant to discrete spatial shifts and so applies at any resolution. Each token encodes an adaptively-sized geometric\nprimitive that represents the unrasterized local boundaries surrounding a pixel, and the tokens evolve through boundary attention to be\ngeometrically consistent. The output is an overlapping field of primitives that directly implies a boundary-aware smoothing of the input\nimage and an unsigned distance map to the image boundaries.\n3. Representation\nOur system is depicted in Figure 2. It uses neighborhood\ncross-attention, a patch-wise variant of cross-attention, with\npixel-resolution tokens. Critically, each pixel\u2019s token is ex-\nplicitly required to encode a tuple of values representing a\ngeometric primitive that specifies the geometric structure of\nthe local boundaries surrounding the pixel. Because the to-\nkens are forced through this geometric bottleneck, we refer\nto our model\u2019s core mechanism as Boundary Attention.\nWe instantiate our bottleneck using a learned linear\nmapping from the token dimension to a predefined lower-\ndimensional space of unrasterized boundary patterns that\nwe call junction space. Our choice for this space is inspired\nby the Field of Junctions [33], but we use a modified param-\neterization (described below) that is differentiable across its\nentire domain. Junction space has the benefit of specify-\ning local boundary patterns without rasterization and thus\nwith unlimited spatial precision. As depicted in Figure 3\nand described in [33], it also has the benefit of including\na large family of local boundary patterns, including unifor-\nmity (i.e., absence of boundaries), edges, bars, corners, T-\njunctions and Y-junctions.\nThe next section introduces our parameterization of\njunction space and some associated operators. Then Sec-\ntion 4 describes the architecture that we use to analyze an\nimage into its field of junction values.\n3.1. Boundary Primitives\nThroughout this paper we use parentheses (x) for continu-\nous signals defined on the 2D image plane [0, W] \u00d7 [0, H]\nand square brackets [n] for discrete signals defined on the\npixel grid. We use c[n] for the coordinates of the nth pixel\u2019s\ncenter.\nDenote the K-channel input image by {f[n]}, where\nf[n] \u2208 RK is the vector image value at the discrete pixel\ngrid index n. Our approach is to analyze the image into a\nfield of dense, stride-1 overlapping local patches, each hav-\ning a square support \u2126n(x) centered at the nth pixel.\nThere are many ways to partition a local region \u2126n(x),\nand one can define parametric families of such partitions.\nFor example the set of oriented lines provides a two-\nparameter family of partitions, with each member of the\nfamily separating the region into points that lie on one side\nof a line or the other. This family of partitions would be\nappropriate for describing edges. Here we define a larger\nfamily of partitions that encompasses a greater variety of\nlocal boundary structures.\nAs depicted in the right of Figure 3, our partitions are\nparameterized by g \u2208 R2 \u00d7 S1 \u00d7 \u25b32, where S1 is the unit\ncircle and \u25b32 is the standard 2-simplex. We use the notation\ng = (u, \u03b8, \u03c9), where u = (u, v) \u2208 R2 is the vertex, \u03b8 \u2208 S1\nis the orientation, and \u03c9 = (\u03c91, \u03c92, \u03c93) are barycentric co-\nordinates (defined up to scale) for the three relative angles,\nordered clockwise starting from \u03b8. Our convention is to ex-\npress the vertex coordinates relative to the center of region\n\u2126n(x), and we denote that the vertex is free to move outside\nof this region. We also note that up to two angles \u03c9j can be\nzero. This all makes it possible to represent a variety of par-\ntition types, including edges, bars, corners, 3-junctions and\nuniformity (i.e., trivial or singleton partitions).\nFixing a value for g induces three binary-valued spatial\nsupport functions:\nsnj(x; g) \u2208 {0, 1}, j = 1, 2, 3,\n(1)\nthat evaluate to 1 for points x that are in \u2126n(x) and in the\njth wedge defined by g; and 0 otherwise. It also induces an\nunsigned distance function:\ndn(x; g) \u2265 0,\n(2)\nthat represents the Euclidean distance from point x to the\nnearest point in the boundary set defined by g.\nDefining these as continuous functions allows specifying\nthe local boundary structure with unlimited spatial resolu-\ntion. The right of Figure 3 uses three colors to visualise the\n3\nFigure 3.\nSamples on a smooth trajectory through our space of geometric primitives (\u201cjunction space\u201d), with one sample enlarged and\naccompanied by a visualization of its distance map on the right. Each sample comprises a vertex position (u, v), orientation \u03b8, and angles\n(\u03c91, \u03c92, \u03c93) that partition the circle. The space includes all types of locally-uniform regions, edges, bars, corners, Y-junctions and T-\njunctions.\nwedge supports of a junction g, and it shows a quantized\nvisualization of the associated distance function.\nInstead of fixing the size of every patch in the image to a\nhand-selected value that is the same throughout, we would\nlike the size of every region \u2126n to adapt to the local geom-\netry. We do this by equipping each patch with a parameter-\nized local windowing function wn(x; p) \u2208 [0, 1]. Specifi-\ncally, we parameterize them as convex combinations of W\nsquare window function coefficients p \u2208 P = \u25b3W \u22121. That\nis,\nwn(x; p) =\nW\nX\ni=1\npi1[\u2225x \u2212 c[n]\u2225\u221e \u2264 Di],\n(3)\nwhere \u2225 \u00b7 \u2225\u221e is the \u2113\u221e-norm, and 1[\u00b7] is the indicator\nfunction that returns 1 if the argument true; and 0 other-\nwise. In our experiments we use W = 3 and diameters\nD = (3, 9, 17).\n3.2. Gather and Slice Operators\nOur network operates by refining the field {(gt[n], pt[n])}\nover a fixed sequence of steps t = 1, ..., T. It uses two\noperators that we define here and depict in the right of Fig-\nure 5. The first operator is a patch-wise gather operator, in\nwhich each wedge of each region computes the weighted\naverage of the image values it contains (recall that c[n] are\nnth pixel\u2019s coordinates):\nfkj =\nP\nn f[n]wk(c[n]; p[n])skj(c[n]; g[n])\nP\nn wk(c[n]; p[n])skj(c[n]; g[n])\n.\n(4)\nThe second operation is a pixel-wise slice operation,\nwhere each pixel computes the means and variances, over\nall regions that contain it, of the per-region distance maps\ndn(x; g[n]) and gathered wedge features fkj. The expres-\nsions for the means are:\nFigure 4. Visualizing our model\u2019s output. Top row, left to right:\n(i) input image; (ii) boundary-aware smoothing; and (iii) bound-\nary map with attention maps at four query points. The two bottom\nrows show regions \u2126n unfolded from within the two white win-\ndows indicated above. Bottom rows, left to right: (i) window func-\ntions; (ii) boundaries; and (iii) support functions colored by their\ngathered wedge features. See text for details.\n\u00afd[n] =\nP\nk wk(c[n]; p[k])dk(c[n]; g[k])\nP\nk wk(c[n]; p[k])\n,\n(5)\n\u00aff[n] =\nP\nk wk(c[n]; p[k]) P\nj fkjskj(c[n]; g[k])\nP\nk wk(c[n]; p[k]) P\nj skj(c[n]; g[k]) .\n(6)\nNote that the only contributions to the sums over patches k\nare from patches that contain c[n], i.e., the sums are over\n{k | \u2126k \u220b c[n]}. The expressions for the pixel-wise dis-\n4\nneighborhood\nMLP-mixer\nneighborhood\ncross-attention\ngather\nlinear decode\nMLP\nMLP\nslice\nslice\ngather\nBoundary Attention\nFigure 5.\nModel Architecture. All blocks are invariant to discrete spatial shifts, and only colored blocks are learned. Orange blocks\noperate at individual locations n, while blue ones operate on small spatial neighborhoods. Symbol \u2295 is concatenation, and gather and slice\noperators (Eqs. 4\u20136) are depicted at right. The first iteration uses \u03b30[n] = \u03b30[n], \u00aff 0[n] = f[n], and \u03c00[n] = \u03c0o with \u03c0o learned across\nthe training set. Boundary attention repeats T = 8 times, with one set of weights for the first four iterations and another for the last four\niterations. There are 207k trainable parameters in total.\ntance map variance \u03bdd[n], and for the feature variance \u03bdf[n],\nwhich is computed across patches containing n and across\ntheir K channels, are defined accordingly and included in\nthe supplement.\n3.3. Visualizing Output\nAt our network\u2019s output, we expect the shapes of junction\nboundaries in overlapping regions to agree, so that the vari-\nances \u03bdd[n], \u03bdf[n] are small at every pixel. Then, the fields\nof means { \u00afd[n]}, {\u00aff[n]} can be interpreted, respectively, as\na global unsigned distance map for the image boundaries\nand a boundary-aware smoothing of its input channel val-\nues. Figure 4 shows an example, where we visualize the\nzero-set of the global unsigned distance map\u2014we call this\nthe global boundary map\u2014by applying the nonlinearity:\n\u00afb\u03b7[n] =\n\u00001 + ( \u00afd[n]/\u03b7)2\u0001\u22121 ,\n(7)\nsetting \u03b7 = 0.7.\nThe output contains much more information than these\nfields of means.\nFor any query pixel n, we can probe\nthe wedge supports {skj(\u00b7; g[k])} and windowing functions\n{wk(\u00b7, p[k])} that contain it, thereby obtaining a spatial at-\ntention map an(x) that surrounds the query pixel. This is\nthe boundary-aware spatial kernel that turns a neighborhood\nof input features {f[\u00b7]} into the value \u00aff[n]:\n\u00aff[n] =\nX\nk\nan(c[k])f[k].\n(8)\nThe expression for an(x) follows from inserting Equation 4\ninto 6, and its maximum diameter is twice that of \u2126(x).\nSome examples are shown in the top-right of Figure 4.\nAdditionally, as shown in the bottom two rows of Fig-\nure 4, we can unfold any portion of the output field into the\noverlapping regions it contains. The figure shows unfolded\nregions {\u2126k(x)} within two windows of the output field.\nFor each unfolded set we visualize: the windowing func-\ntions wk(x; p); the regional boundaries bk(x; g) obtained\nby applying the nonlinearity in Equation 7 to the local dis-\ntance functions dk(x; g); and the supports skj(x; g) colored\naccording to the wedge features fkj that they gather from\nthe input image. In the top example, the neighborhood is\nhomogeneous, so the windowing functions (via p[n]) have\nlarge supports, and there are few boundaries (via g[n]). In\nthe bottom example there is fine-scale geometry. The win-\ndowing functions are narrow and the primitives agree on a\nY-junction.\n4. Network Architecture\nWe design our network to iteratively refine the fields\n{(gt[n], pt[n])}. We do so by embedding each field ele-\nment using a higher dimensional representation, \u03b3t[n] \u2208\nRD\u03b3 and \u03c0t[n] \u2208 RD\u03c0 respectively, which can be updated\nvia dot-product attention. In practice, we use D\u03b3 = 64 and\nD\u03c0 = 8, which provides the network with enough capacity\nto learn meaningful hidden states. We learn simple linear\nmappings \u03b3 7\u2192 g and \u03c0 7\u2192 p which are used for the gather\nand slice operations. Importantly, all of our network\u2019s el-\nements are invariant to discrete spatial shifts of the image,\noperating either on individual locations n or on small neigh-\nborhoods of locations. See Figure 5 and additional details\nin the supplement.\nGiven an input image, the network first applies a neigh-\nborhood MLP-mixer, which is a modified variation of MLP-\nMixer [32] where we replace the linear operations with con-\nvolutions of kernel size 3. The other change is that we map\nthe input pixels to the hidden state size with a pixel-wise\nlinear mapping rather than taking patches of the input. This\nblock, which we denote the \u201cneighborhood MLP-mixer\u201d\ntransforms the input into an initial hidden state, which is\nthen processed through eight iterations of our boundary at-\ntention to further refine the hidden state.\n5\nThe eight iterations of refinement are broken into two\nBoundary Attention Blocks with independent weights. To\nprocess our input, we first add in a linear projection of the\ninitial hidden state. This is essentially a skip connection that\nallows our network to retain information from the input pix-\nels at later stages of processing. Next, we copy our hidden\nstate into two identical pieces. We concatenate a dimension\n8 learned windowing token to one of the copies and the cur-\nrent estimate of the smoothed global features to the other.\nWe then do neighborhood cross-attention: each pixel in the\nfirst copy does two iterations of cross attention with a size\n11 patch of the second copy. We add a learned 11 \u00d7 11\npositional encoding to the patch, which allows our network\nto access relative positioning, even if global position cues\nare absent. We follow each self attention layer with a small\nMLP.\nTo transform our output or intermediary hidden state\ninto junction space and render output images, we use a\nsimple linear mapping. We separate the windowing token\n(the last 8 dimensions) from the hidden state (the first 64\ndimensions) and project each through a linear layer. We\nmap the hidden states to 7 numbers that represent g =\n(u, sin(\u03b8), cos(\u03b8), \u03c9).\nThese serve as the inputs to our\ngather and slice operators.\nOverall, our network has 2.07\u00b7105 learnable parameters,\nmaking it orders of magnitude smaller than most learned\nboundary detectors. For contrast, EDTER [28] has 109 mil-\nlion parameters for its full model.\n4.1. Training\nWe train our network in two stages: we begin by training the\nneighborhood MLP-mixer with the first Boundary Attention\nBlock, and then we add the second block and retrain end-\nto-end. We apply our loss to the two final iterations of our\nnetwork, so for the first stage of training we apply a loss to\niterations 3 and 4, and for end-to-end refinement we apply\na loss to iterations 7 and 8. We weigh the final loss three\ntimes higher than the second to last loss, which encourages\nthe network to allocate capacity to producing high quality\noutputs, while ensuring that gradient information is shared\nacross the network.\nWe train our method using a combination of four global\nlosses applied to global (i.e. averaged) fields, and two patch-\nwise losses applied to individual patches.\nThe first two\nlosses are supervision losses penalizing mismatches be-\ntween our network\u2019s predictions and the ground truth fea-\nture and boundary maps:\nLf =\nX\nn\n\u03b1[n]\u2225\u00aff[n] \u2212 fGT[n]\u22252,\n(9)\nLd =\nX\nn\n\u03b1[n]\n\u0000 \u00afd[n] \u2212 dGT[n]\n\u00012 ,\n(10)\nwhere fGT and dGT are the ground truth features and distance\nmaps, respectively, and \u03b1[n] is a pixel importance function\ndefined as:\n\u03b1[n] = e\u2212\u03b2\u00b7(dGT[n]+\u03b4) + C,\n(11)\nwith \u03b2 and C controlling how much weight to give pixels\nnear boundaries. We set \u03b2 = 0.1, \u03b4 = 1, and increase C\nthroughout training to give more weight to non-boundary\nlocations. We also tested a more involved pixel importance\nmask that gave extra weight to visible vertices and intersec-\ntions, the details for which can be found in the supplement.\nNote that using noiseless feature maps for supervision in\nEquation 9 encourages the windowing functions to be large\nin smooth regions.\nOn top of the two supervision losses we apply two con-\nsistency losses from [33], that minimize the per-pixel vari-\nances \u03bdf[n] and \u03bdd[n]. Similar to the supervision losses,\nwe weigh those by \u03b1 from Equation 11. These consistency\nlosses encourage the junction shapes g in overlapping re-\ngions to agree. Minimizing \u03bdf[n] also encourages window-\ning functions to be large, because that increases the gather\narea, which in turn reduces noise in wedge features fnj that\nare sliced to compute the variance \u03bdf[n] at each n.\nFinally, we use two patch-wise losses to encourage indi-\nvidual feature and distance patches to agree with the super-\nvisory ones:\n\u2113f =\nX\nk\n\u03c7[k]\nX\nn\u2208\u2126k\n\u03b1[n]\u2225\u00aff[n] \u2212 fGT[n]\u22252,\n(12)\n\u2113d =\nX\nk\n\u03c7[k]\nX\nn\u2208\u2126k\n\u03b1[n]( \u00afd[n] \u2212 dGT[n])2,\n(13)\nwhere \u03c7[k] is a patch importance function defined as:\n\u03c7[k] =\n X\nn\u2208\u2126k\n(dGT[n] + \u03b4\u2032)\n!\u22121\n,\n(14)\nand we set \u03b4\u2032 = 1. These per-patch losses gives the network\na more direct signal on how to adjust its weights than purely\nglobal losses, which average over multiple patches.\n5. Experiments\nImplementation details.\nWe train our model on noisy\nsynthetic data of colorful combinations of triangles and cir-\ncles.\nWe render 240 \u00d7 320 images containing 15 to 20\nshapes each, but use 125 \u00d7 125 crops for training. To those\ncrops we add Gaussian and/or Perlin noise [26], and with\nprobability 0.1 we average over the color channels to pro-\nduce grayscale inputs. Our dataset contains 105 images,\n90% of which are used for training, and the rest for testing.\nFor training and optimization details, refer to the supple-\nment.\n6\nFigure 6. The ODS F-score of our method at different noise levels,\ncompared with the Field of Junctions (FOJ) [33] run using patch\nsizes 9 \u00d7 9, 17 \u00d7 17, and 25 \u00d7 25, and EDTER [28], with and\nwithout preprocessing by BM3D [9]. The bottom insets show ex-\nample patches at representative PSNR values.\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nMatching threshold (pixels)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nODS F-Score\nEDTER\nOurs\nFigure 7.\nOur network can reliably locate boundaries with sub-\npixel precision, due in part to our parametric representation for\nboundaries. We calculate the ODS F-Score of synthetic images\nas we increase the pixel matching distance threshold (how close\na pixel must be to a ground truth pixel to be counted) from 0 to\n1.5 and vary the PSNR of the images from \u221e to 8.0. Each line\nrepresents the performance at a noise level, with lighter-colored\nlines corresponding to noisier inputs. X-axis values below 1.0,\ndenoted by the vertical dotted line, represent sub-pixel precision.\nPerformance w.r.t.\nnoise levels.\nFigure 6 shows the\ncomparison of our method and baseline approaches, the\nField of Junctions [33] and EDTER [28] under different\nnoise levels. We test the Field of Junctions with different\npatch sizes (9 \u00d7 9, 17 \u00d7 17, and 25 \u00d7 25) and EDTER\nwith/without preprocessing using optimally-tuned denois-\ning using BM3D [9]. The tuneable parameters for Field\nof Junctions were chosen to maximize its performance on\nnoisy images with 17 \u00d7 17 patches. Our method outper-\nforms all baselines at lower noise levels and is competitive\nwith the Field of Junctions at higher noise levels, while be-\ning orders of magnitude faster than it (see Table 1).\nSub-pixel precision.\nTo measure sub-pixel precision, we\nrender 500 \u00d7 500 images containing pairs of overlapping\ncircles and triangles. This gives us a precise binary bound-\nFigure 8. One of the advantages of our parametric form for bound-\naries is that we can trivially upsample our output boundary maps\nby striding the predicted junction parameters, and scaling the patch\nsize accordingly. This results in clean boundaries even when our\noutput boundary images are upsampled from 125 \u00d7 125 (left) to\n500 \u00d7 500 (right).\nFigure 9. Top: Linear interpolation in our network\u2019s learned em-\nbedding space RD\u03b3 from value \u03b3a to zero and then to \u03b3b. Bottom:\nA geometric interpolation in junction space g \u2208 G that passes\nthrough g0 = (0, 0, 1/3 \u00b7 1). The embedding has learned to be\nsmooth and have an intuitive zero.\nary map for that resolution. We downsample the images\nto 125 \u00d7 125, and add varying amounts of Gaussian noise.\nThese noisy, downsampled images serve as the inputs. To\nevaluate the predicted boundaries, we upsample the outputs\nback to 500 \u00d7 500 pixels. For EDTER, we use bilinear in-\nterpolation along with edge thinning to produce the upsam-\npled boundaries. In our case, a byproduct of our parametric\nform for boundaries is that an intuitive method for upsam-\npling boundaries naturally arises: by increasing the patch\nstride to 4, and rendering the patches as 4 times as large\u2014\n68 \u00d7 68 pixels\u2014we can create an upsampled version of our\nimage while retaining our method\u2019s accuracy in boundary\nlocalization. We visualize this process in Figure 8.\nWe evaluate the upsampled outputs on the original 500\u00d7\n500 binary ground truth maps and vary the minimum dis-\ntance threshold of the evaluation metric, which denotes how\nclose a prediction must be to the ground truth to be used.\nThe results are shown in Figure 7. Our F-score remains\nhigh across all matching thresholds, even when the inputs\ncontain additive Gaussian noise.\nLinear interpolation in junction space.\nSurprisingly, we\nnote that our network learns a spatially smooth manifold\nof junctions in its hidden state. Figure 9 visualizes some\n7\nFigure 10.\nEvolution of boundaries during iterations, in reading order. Early iterations are exploratory and unstructured, while later\niterations feature consistent per-patch boundaries, resulting in clean average boundary maps.\nFigure 11.\nOur model generalizes well to real images, and can\nfind accurate boundaries in natural images. Top left: The input\nimage, a low light image from the SIDD [1] dataset. Top right:\nOur network\u2019s predicted boundaries. Bottom left: The output mean\nfeature map. Bottom right: The predicted distance map.\nproperties of its learned embedding of junction space. We\ngenerate equally-spaced samples \u03b3i \u2208 RD\u03b3 by linearly in-\nterpolating from a particular \u03b3a to 0 and then to a particular\n\u03b3b. We see that the embedding space is smooth, and inter-\nestingly, that it learns to associate its zero with nearly-equal\nangles and a vertex close to the patch center. For visual\ncomparison, we show an analogous interpolation from ga\nto g0 \u225c (0, 0, 1/3 \u00b7 1) and then to gb by using a linear geo-\nmetric interpolation in junction space G, the expressions for\nwhich are in the supplement.\nEvolution of the outputs over time.\nFigure 10 shows an\nexample of how the distance map \u00afd[n] evolves during re-\nfinement.\nSpecifically, we visualize the result of slicing\nsimilar to Equation 5 but with the regional distance func-\ntions dk replaced by their nonlinear counterparts bk. We see\nthat early iterations are exploratory and unstructured, and\nthat later iterations reach agreement.\nResults on real images.\nFigures 1 and 11 show the qual-\nitative results on real images.\nDespite being trained on\nsynthetic data, our method can outperform existing state-\nTable 1. Inference time (in seconds) of EDTER [28] (without pre-\nprocessing by BM3D [9]), and Field of Junctions (FoJ) [33] run\nat various patch sizes, compared with our method. We compare\nboth with the original (FoJ) implementation as well as our JAX\nreimplementation (JAX-FoJ). Unlike FoJ [33], our method does\nnot require striding for fitting moderately-sized images in memory\n(runtimes reported with \u2217 did require striding). The average run\ntimes were calculated on an Nvidia A100 GPU.\n125 \u00d7 125\n320 \u00d7 320\nEDTER [28]\n0.130\n0.130\nFoJ [33] 9\u00d79\n36.8\n206\n17\u00d717\n88.0\n76.0\u2217\n(stride-3)\n25\u00d725\n162\n57.6\u2217\n(stride-5)\nJAX-FoJ 9\u00d79\n25.2\n91.0\n17\u00d717\n30.7\n78.0\u2217\n(stride-3)\n25\u00d725\n117\n220\u2217\n(stride-5)\nOurs\n0.0823\n0.678\nof-the-art methods at multiple levels of real sensor noise\npresent in ELD [34]. Our method produces crisp and well-\ndefined boundaries despite high levels of noise.\nInference time.\nTable 1 compares the running time for\ndifferent methods at two resolutions.\nWe denote several\nruns for Field of Junctions where we had to increase the\npatch stride (optimize junctions at strided intervals) to avoid\nrunning out of memory. Notably, we are orders of magni-\ntude faster than Field of Junctions, despite achieving similar\nperformance. Additionally, because our network can handle\ninputs of any size, the run time depends on the input shape,\nin contrast with EDTER that has a maximal input size of\n320 \u00d7 320 inputs and constant run time.\n6. Conclusion\nWe have introduced a differentiable model that explicitly\nreasons about geometric primitives such as edges, corners,\njunctions, and regions of uniform appearance, by using\nboundary attention. Despite being trained on simple geo-\nmetric synthetic data, our method generalizes to natural im-\nages, and predicts clean boundaries, even when the images\nit is applied to are significantly corrupted by noise. Fur-\nthermore, the parametric form of our model elicits a natural\nformulation for boundaries with subpixel precision, and its\nbottom-up patch-wise approach enables it to process any\nimage at its native resolution and aspect ratio.\n8\nReferences\n[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S.\nBrown.\nA high-quality denoising dataset for smartphone\ncameras. In IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2018. 8\n[2] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien\nLucchi, Pascal Fua, and Sabine S\u00a8usstrunk. SLIC superpixels.\nTechnical report, 2010. 2\n[3] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel.\nDeep ViT features as dense visual descriptors.\nIn ECCV\nWorkshops on What is Motion For?, page 4, 2021. 2\n[4] John Canny. A computational approach to edge detection.\nIEEE Transactions on pattern analysis and machine intelli-\ngence, (6):679\u2013698, 1986. 2, 15, 19\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 2\n[6] Edwin Earl Catmull. A subdivision algorithm for computer\ndisplay of curved surfaces. The University of Utah, 1974. 2\n[7] Tony F. Chan and Luminita A. Vese. Active contours without\nedges. IEEE Transactions on image processing, 10(2):266\u2013\n277, 2001. 2\n[8] M Concetta Morrone and DC Burr. Feature detection in hu-\nman vision: A phase-dependent energy model. Proc. Royal\nSoc. B, 235(1280):221\u2013245, 1988. 2\n[9] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and\nKaren Egiazarian. Image denoising by sparse 3-D transform-\ndomain collaborative filtering. IEEE Transactions on image\nprocessing, 16(8):2080\u20132095, 2007. 7, 8\n[10] Piotr Doll\u00b4ar and C. Lawrence Zitnick. Fast edge detection\nusing structured forests, 2014. 15, 19\n[11] Pedro F. Felzenszwalb and Daniel P. Huttenlocher. Efficient\ngraph-based image segmentation. International Journal of\nComputer Vision, 59:167\u2013181, 2004. 2\n[12] James D. Foley and Andries Van Dam.\nFundamentals of\ninteractive computer graphics. Addison-Wesley Longman\nPublishing Co., Inc., 1982. 2\n[13] William T. Freeman. Steerable filters and local analysis of\nimage structure. PhD thesis, Massachusetts Institute of Tech-\nnology, 1992. 2\n[14] William T. Freeman and Edward H. Adelson. The design\nand use of steerable filters. IEEE Transactions on pattern\nanalysis and machine intelligence, 13(9):891\u2013906, 1991. 2\n[15] Chris Harris, Mike Stephens, et al. A combined corner and\nedge detector. In Alvey vision conference, pages 10\u20135244.\nCiteseer, 1988. 2\n[16] Paul S. Heckbert. Fundamentals of texture mapping and im-\nage warping. Citeseer, 1989. 2\n[17] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities\nand stochastic regularizers with gaussian error linear units.\nCoRR, abs/1606.08415, 2016. 14\n[18] Michael Kass, Andrew Witkin, and Demetri Terzopoulos.\nSnakes: Active contour models.\nInternational journal of\ncomputer vision, 1(4):321\u2013331, 1988. 2\n[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar, and\nRoss Girshick. Segment anything. arXiv:2304.02643, 2023.\n2\n[20] Philipp Kr\u00a8ahenb\u00a8uhl and Vladlen Koltun. Efficient inference\nin fully connected CRFs with Gaussian edge potentials. Ad-\nvances in neural information processing systems, 2011. 2\n[21] Michael Maire, Pablo Arbelaez, Charless Fowlkes, and Jiten-\ndra Malik. Using contours to detect and localize junctions in\nnatural images. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 1\u20138. IEEE, 2008. 2\n[22] David R. Martin, Charless C. Fowlkes, and Jitendra Ma-\nlik. Learning to detect natural image boundaries using local\nbrightness, color, and texture cues. IEEE Transactions on\npattern analysis and machine intelligence, 26(5):530\u2013549,\n2004. 2\n[23] Nati Ofir, Meirav Galun, Boaz Nadler, and Ronen Basri. Fast\ndetection of curved edges at low SNR. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2016.\n2\n[24] Nati Ofir, Meirav Galun, Sharon Alpert, Achi Brandt, Boaz\nNadler, and Ronen Basri. On detection of faint edges in noisy\nimages. IEEE Transactions on pattern analysis and machine\nintelligence, 42(4):894\u2013908, 2019. 2\n[25] Pierre Parent and Steven W. Zucker. Trace inference, curva-\nture consistency, and curve detection. IEEE Transactions on\npattern analysis and machine intelligence, 11(8):823\u2013839,\n1989. 2\n[26] Ken Perlin. An image synthesizer. ACM Siggraph Computer\nGraphics, 19(3):287\u2013296, 1985. 6, 14\n[27] Juan Pineda. A parallel algorithm for polygon rasterization.\nIn Proceedings of the 15th annual conference on Computer\ngraphics and interactive techniques, pages 17\u201320, 1988. 2\n[28] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan,\nand Haibin Ling. EDTER: Edge detection with transformer.\nIn IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2022. 1, 6, 7, 8, 15, 19\n[29] Daniel Scharstein and Chris Pal. Learning conditional ran-\ndom fields for stereo. In 2007 IEEE Conference on Computer\nVision and Pattern Recognition, pages 1\u20138, 2007. 23\n[30] D. Scharstein and R. Szeliski. High-accuracy stereo depth\nmaps using structured light. In 2003 IEEE Computer Soci-\nety Conference on Computer Vision and Pattern Recognition,\n2003. Proceedings., pages I\u2013I, 2003. 23\n[31] Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao,\nQi Tian, Matti Pietik\u00a8ainen, and Li Liu.\nPixel difference\nnetworks for efficient edge detection.\nIn Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 5117\u20135127, 2021. 1, 15, 19\n[32] Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-\ncas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,\nAndreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al.\nMLP-Mixer: An all-MLP architecture for vision. Advances\nin neural information processing systems, 34:24261\u201324272,\n2021. 5, 14\n[33] Dor Verbin and Todd Zickler. Field of junctions: Extract-\ning boundary structure at low SNR.\nIn Proceedings of\n9\nthe IEEE/CVF international conference on computer vision,\n2021. 2, 3, 6, 7, 8, 11, 12, 15, 17, 19\n[34] Kaixuan Wei, Ying Fu, Jiaolong Yang, and Hua Huang. A\nphysics-based noise formation model for extreme low-light\nraw denoising. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2758\u2013\n2767, 2020. 1, 8, 14, 17\n[35] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection. In Proceedings of the IEEE/CVF international con-\nference on computer vision, pages 1395\u20131403, 2015. 1, 19\n[36] Nan Xue, Gui-Song Xia, Xiang Bai, Liangpei Zhang, and\nWeiming Shen.\nAnisotropic-scale junction detection and\nmatching for indoor images. IEEE Transactions on Image\nProcessing, 27(1):78\u201391, 2017. 2\n10\nSupplemental Material\nBoundary Attention: Learning to Find Faint Boundaries at Any Resolution\nS1. The space of M-junctions\nHere we provide the expressions for the support functions sj(x; g) and the unsigned distance function d(x; g) from Section 3\nof the main paper. We also describe the differences between our parameterization of junction space and the original one in the\nfield of junctions [33], with the new parameterization\u2019s main advantages being the avoidance of singularities and the ability\nto define mechanisms for smooth interpolation. Our descriptions of these require introducing a few additional mathematical\ndetails. We provide these details for the general case of geometric primitives (junctions) g that have M angular wedges\n\u03c9 = (\u03c91, . . . , \u03c9M), for which the paper\u2019s use of M = 3 is a special case.\nTo begin, consider a local region \u2126(x) \u2282 R2 and fix a positive integer value for the maximum number of angular wedges\nM > 0 (the paper uses M = 3). Our partitions are parameterized by g \u2208 R2 \u00d7 S1 \u00d7 \u25b3M\u22121, where S1 is the unit circle and\n\u25b3M\u22121 is the standard (M \u2212 1)-simplex (i.e., the set of M-vectors whose elements are nonnegative and sum to one). We use\nthe notation g = (u, \u03b8, \u03c9), where u = (u, v) \u2208 R2 is the vertex, \u03b8 \u2208 S1 is the orientation, and \u03c9 = (\u03c91, \u03c92, . . . , \u03c9M) are\nbarycentric coordinates (defined up to scale) for the M relative angles, ordered clockwise starting from \u03b8. As noted in the\nmain paper, our convention is to express the vertex coordinates relative to the center of region \u2126(x), and we note again that\nthe vertex is free to move outside of this region. We also note that up to M \u22121 of the angles \u03c9j can be zero. When necessary,\nwe use notation \u02dc\u03c9 = (\u02dc\u03c91, \u02dc\u03c92, . . . , \u02dc\u03c9M) to represent angles that are normalized for summing to 2\u03c0:\n\u02dc\u03c9 =\n2\u03c0\u03c9\nPM\nj=1 \u03c9j\n.\n(15)\nAs an aside, we note that there are some equivalences in this parameterization. First, one can perform, for any k \u2208\n{1 . . . (M \u2212 1)}, a cyclic permutation of the angles \u03c9 and adjust the orientation \u03b8 without changing the partition. That is, the\npartition does not change under the cyclic parameter map\n\u03c9j \u2192 \u03c9j+k(modM)\n(16)\n\u03b8 \u2192 \u03b8 \u2212\nM\nX\nj=M+1\u2212k\n\u03c9j\n(17)\nfor any k \u2208 {1 . . . (M \u2212 1)}. Also, an M-junction (u, \u03b8, (\u03c91, . . . , \u03c9M)) provides the same partition as any M \u2032-junction,\nM \u2032 > M, that has the same vertex and orientation along with angles (\u03c91 . . . \u03c9M, 0 . . . ). This captures the fact that M-\njunction families are nested for increasing M.\nAs shown in Figure 12, other geometric features of a junction can be directly derived from the orientation and angles. The\ncentral directions \u03c8 = (\u03c81, . . . , \u03c8M) are\n\u03c8j = \u03b8 + \u02dc\u03c9j\n2 +\nj\u22121\nX\nk=1\n\u02dc\u03c9k,\nj \u2208 {1 . . . M},\n(18)\nFigure 12. Anatomy of an M-junction g = (u, \u03b8, \u03c9) with M = 3. Left: Boundary directions \u03d5j and central directions \u03c8j are determined\ndirectly from relative angles \u03c9 and orientation \u03b8 (which is equal to \u03d51). Middle panels: Unsigned distance function for a boundary ray\nd3(x; g) and overall unsigned distance function d(x; g), which is the minimum of the three per-ray ones. Right: Associated boundary\nfunction b\u03b7(x; g) using \u03b7 = 0.7.\n11\nand the boundary directions \u03d5 = (\u03d51, . . . , \u03d5M) are given by \u03d51 = \u03b8 and\n\u03d5j = \u03b8 +\nj\u22121\nX\nk=1\n\u02dc\u03c9k,\nj \u2208 {2 . . . M}.\n(19)\nA key difference between our new parameterization of M-junctions and the original one [33] is that the latter comprises\n(u, \u03d5) and requires enforcing constraints 0 \u2264 \u03d51 \u2264 \u03d52 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03d5M \u2264 2\u03c0 (or somehow keeping track of the permutations of\nwedge indices that occur when these constraints are not enforced). The new (u, \u03b8, \u03c9)-parameterization eliminates the need\nfor such constraints.\nAs noted in the main paper\u2019s Section 3, we define the jth support sj(x; g) as the binary-valued function that indicates\nwhether each point x \u2208 \u2126 is contained within wedge j \u2208 {1 . . . , M}. Its expression derives from the inclusion condition\nthat the dot product between the vector from the vertex to x and the jth central vector (cos \u03c8j, sin \u03c8j) must be smaller than\nthe cosine of half the angle \u02dc\u03c9j. Using Heaviside function H(\u00b7) we write\nsj(x; g) = H\n\u0010\n(x \u2212 u) \u00b7 (cos \u03c8j, sin \u03c8j) \u2212 cos(\u02dc\u03c9j/2)||x \u2212 u||\n\u0011\n.\n(20)\nAs an aside, observe that this expression remains consistent for the case M = 1, where there is a single wedge. In this case,\n\u02dc\u03c9 = \u02dc\u03c91 = 2\u03c0 by Equation 15, and the support reduces to s1(x) = 1 for all vertex and orientation values.\nThe unsigned distance d(x; g) represents the Euclidean distance from point x to the nearest point in the boundary set\ndefined by g. It is the minimum over M sub-functions, with each sub-function being the unsigned distance from a boundary\nray that extends from point u in direction \u03d5j. The unsigned distance from the jth boundary ray is equal to the distance from\nits associated line for all points x in its containing half-plane; and for other points it is equal to the radial distance from the\nvertex. That is,\ndj(x; g) =\n(\n|(x \u2212 u) \u00b7 (\u2212 sin \u03d5j, cos \u03d5j)| ,\nif (x \u2212 u) \u00b7 (cos \u03d5j, sin \u03d5j) > 0\n\u2225(x \u2212 u)\u2225,\notherwise.\n(21)\nThen, the overall distance function is\nd(x; g) =\nmin\nj\u22081...M dj(x; g).\n(22)\nFinally, analogous to Equation 7 in the main paper, we define a junction\u2019s boundary function b\u03b7(x; g) as the result of\napplying a univariate nonlinearity to the unsigned distance:\nb\u03b7(x; g) =\n\u00001 + (d(x; g)/\u03b7)2\u0001\u22121 .\n(23)\nFigure 12 shows an example of a junction\u2019s distance function and its associated boundary function with \u03b7 = 0.7.\nInterpolation\nAnother advantage of the present parameterization compared to that of the original [33] is that it is a simply-connected topo-\nlogical space and so allows for defining mechanisms for smoothly interpolating between any two junctions g and g\u2032. In our\nimplementation we simply define interpolation variable t \u2208 [0, 1] and compute interpolated junctions g(t) = {u(t), \u03b8(t), \u03c9(t)}\nusing\nu(t) = (1 \u2212 t)u + tu\u2032\n(24)\n\u02dc\u03c9(t) = (1 \u2212 t)\u02dc\u03c9 + t\u02dc\u03c9\u2032,\n(25)\nand\n\u03b8(t) = atan2(q, p), with\n(26)\n(p, q) = Slerp ((cos \u03b8, sin \u03b8), (cos \u03b8\u2032, sin \u03b8\u2032), t) ,\nwhere Slerp() is the 2D geometric spherical linear interpolation operator,\nSlerp(p, p\u2032, t) = sin ((1 \u2212 t)\u2206\u03b8)\nsin (\u2206\u03b8)\np + sin (t\u2206\u03b8)\nsin (\u2206\u03b8) p\u2032,\nwith \u2206\u03b8 = arccos(p \u00b7 p\u2032). The bottom row of Figure 9 in the main paper visualizes a set of samples from smooth trajectories\nin junction space using this mechanism.\n12\nFigure 13.\nColumns 1 to 5: Examples of the synthetic data used to train our model using supervision with ground-truth boundaries.\nColumn 6: Rendered distance maps corresponding to column 5. The training data contains random circles and triangles that each have\na random RGB color, and the images are corrupted by various types and amounts of noise. Each noiseless image has an unrasterized,\nvector-graphics representation of its shapes and colors, which specify the clean image and exact boundary-distance map with unlimited\nresolution.\nS2. Training Data\nWe find that we can train our model to a useful state using purely synthetic data, examples of which are depicted in Figure 13.\nIn fact, we find it sufficient to use very simple synthetic data that consists of only two basic shapes\u2014circles and triangles\u2014\nbecause these can already produce a diverse set of local edges, thin bars, curves, corners, and junctions, in addition to uniform\nregions. We generate an image by randomly sampling a set of circles and triangles with geometric parameters expressed in\ncontinuous, normalized image coordinates [0, 1] \u00d7 [0, 1]. We then choose a random depth ordering of the shapes, and we\nchoose a random RGB color for each shape. Importantly, the shape and color elements are specified using a vector-graphics\nrepresentation, and the shape elements are simple enough to provide an exact, symbolic expression for each image\u2019s true\nboundary-distance map, without approximation or rasterization. They also allow calculating the precise locations, up to\nmachine precision, for all of the visible corners and junctions in each image.\nAt training time, an input image is rasterized and then corrupted by a random amount and type of noise, including some\ntypes of noise that are spatially-correlated. This forces our model to only use color as its local cues for boundaries and\ngrouping; and it forces it to rely heavily on the topological and geometric structure of curves, corners and junctions, as well\nas their contrast polarities. The highly-varying types and amounts of noise also encourages the model to use large window\nfunctions w(x; g) when possible, since that reduces noise in the gather operation and reduces variance \u03bdf[n].\nOur dataset, which we call Kaleidoshapes, will be released upon publication, along with the code for generation, training\nand evaluation.\nShapes and colors. For our experiments, we rasterized each image and its true distance map at a resolution of 240 \u00d7 320\nimages, with each one containing between 15 and 20 shapes. We used a 40:60 ratio of circles to triangles. In terms of\nnormalized coordinates, circles had radii in the range [0.05, 0.2] and triangles had bases in the range [0.02, 0.5] and heights\nin the range [0.05, 0.3]. This allows triangles to be quite thin, so that some of the local regions \u2126(x) contain thin bar-like\nstructures. Additionally, we included a minimum visibility threshold, filtering out any shapes whose visible number of\nrasterized pixels is below a threshold. Colors were selected by uniformly sampling all valid RGB colors. During training,\nbatches consisted of random 125 \u00d7 125 crops.\n13\nNoise. For noise types, we used combinations of additive zero-mean Gaussian noise; spatially average-pooled Gaussian\nnoise; Perlin noise [26], and simulated photographic sensor noise using the simplified model from [34]. The total noise\nadded to each image was sampled uniformly to be between 30% and 80% of the maximum pixel magnitude, and then noise-\ntypes were randomly combined with associated levels so that they produced the total noise level. Since zero-mean noise can\nat times result in values below 0 or above the maximum magnitude threshold, we truncate any pixels outside of that range.\nS3. Model Details\nOur model is designed to be purely local and bottom up, with all of its compositional elements operating on spatial neigh-\nborhoods in a manner that is invariant to discrete spatial shifts of an image. Its design also prioritizes having a small number\nof learnable parameters. Here we provide the details of the two blue blocks in the main paper\u2019s Figure 3: Neighborhood\nMLP-Mixer and Neighborhood Cross-attention. We implement our model in JAX, and we will publicly share our model\ncode and its pretrained weights upon publication.\nS3.1. Neighborhood MLP-Mixer\nOur neighborhood MLP-mixer is a shift invariant, patch-based network inspired by MLP-mixer [32]. It replaces the image-\nwide operations of [32] with patch-wise ones. Given an input image, we first linearly project its pixels from R3 to dimension\nRD\u03b3 (we use D\u03b3 = 64), which is followed by two neighborhood mixing blocks. Each neighborhood mixing block contains\na spatial patch mixer followed by a channel mixer. The spatial patch mixer is implemented as two 3 \u00d7 3 spatial convolutions\nwith weights tied across channels. It thereby combines spatial patches of features with all channels (and patches) sharing\nthe same weights. Following [32], we use GELU [17] activations. The channel mixer is a per-pixel MLP with spatially-tied\nweights. To handle border effects in our neighborhood MLP-mixer, we apply zero-padding after the initial projection from\nR3 to R64, and then we crop to the input image size after the second neighborhood mixing block to remove features that\ncorrespond to patches without full coverage, i.e., patches that contain pixels outside of the original image.\nS3.2. Neighborhood Cross-attention\nThe neighborhood cross-attention block similarly enforces shift-invariance and weight sharing across spatial neighborhoods.\nInside this block are two transformer layers whose cross-attention components are replaced with neighborhood cross-attention\ncomponents that are restricted to a spatial neighborhood of pixels. We use 11 \u00d7 11 neighborhoods in our implementation. In\neach neighborhood containing a query token, we add a learned positional encoding to the key/value tokens which is relative\nto the neighborhood\u2019s center and is the same for all neighborhoods. Then the query is updated using standard cross-attention\nwith its neighborhood of key/values. We use 4 cross-attention heads. Like the standard transformer, each neighborhood cross\nattention component is followed by an MLP, dropout layer, and additive residual. To handle border effects, we zero-pad\nthe key and value tokens so that every query attends to an 11 \u00d7 11 neighborhood, and then zero-out any attention weights\ninvolving zero-padded tokens.\nS3.3. Training details\nWe pretrain the neighborhood MLP-mixer and the first boundary attention block on a simplified variation of our Kalei-\ndoshapes dataset, where each image is 100 \u00d7 100 and contains a single triangle and circle with additive zero-mean Gaussian\nnoise. We omit the global losses of Equations 9 and 10 during this pretraining phase. This primes the network to learn mean-\ningful hidden states \u03b3[n] and prevents the \u201ccollapsing\u201d of junctions, where the boundary-consistency loss (i.e. the sum over\npixels of variance of distance \u03bdd[n]) dominates and the network learns to predict all-boundaryless patches that are globally\nconsistent but inaccurate. Because of data imbalance\u2014only a small fraction of regions \u2126n(x) contain corners or junctions\u2014\nwe add an additional spatial importance mask to prioritize the regions that contain a corner (i.e., a visible triangle vertex)\nor a junction (i.e., an intersection between a circle and a triangle\u2019s edge). Our data generation process produces a list of\nall non-occluded vertices and intersections in each image, and we use these values to create a spatial importance mask with\ngaussians centered at each of these points. In practice, we use gaussians with a standard deviation of 7 pixels. This mask is\nadded to the loss constant C.\nThe final stage of training adds a second boundary attention block with weights that are initialized using a copy of the\npretrained weights of the first boundary attention block. We use 100, 000 crops of size 125 \u00d7 125 from our Kaleidoshape\nimages (10% withheld for testing) and the full set of losses; and we optimize all of the model\u2019s parameters, including those\nof the neighborhood MLP-mixer and the first boundary attention block. Like in pretraining, we add a spatial importance\nthat prioritizes region containing a corner (i.e., a visible triangle vertex) or a junction (i.e., a visible intersection between the\nboundaries of any two shapes).\n14\nOur trained weights will be released upon publication.\nS4. Qualitative Behavior for Natural Images\nIn Figures 15 and 14, we show how the model behaves on noiseless natural images that contain texture and recognizable\nobjects. In particular, Figure 14 emphasizes how the boundary maps produced by our model qualitatively differ from those of\nmany classical bottom-up edge-detectors and also from those of learned, end-to-end models that have been trained to match\nhuman annotations. The figure compares our output to that from Canny [4], Field of Junctions [33] with a patch size of\n11, Pidinet [31], and EDTER [28]. The latter two methods are trained on human annotated data, whereas the former two\nmethods, like our model, are not. (Note that inputs for all models besides EDTER [28] were 300 \u00d7 400. Input to EDTER\nwas down-sampled to 225 \u00d7 300 due to its input size constraint.)\nWe find that our model produces finer structures than the end-to-end learned models [10, 28] because it is trained to only\nuse local spatial averages of color as its cue for boundaries and grouping. It does not include mechanisms for grouping based\non local texture statistics, nor based on non-local shape and appearance patterns that have semantic meaning to humans.\nCompared to the bottom-up methods of Canny [4] and Field of Junctions [33], our model has the advantage of automatically\nadapting the sizes of its output structures across the image plane, through its prediction of field p[n]. In contrast, the Field of\nJunctions and Canny both operate at a single pre-determined choice of local size, so they tend to oversegment some places\nwhile undersegmenting others.\nFigure 14.\nQualitative behavior of our model\u2019s output boundaries \u00afb\u03b7[n] on noiseless natural images, compared to those of end-to-end\nmodels EDTER [28] and Pidinet [31] that are trained to match human annotations; and compared to two bottom-up methods that, like our\nmodel, are not trained to match human annotations: Canny [4], and Field of Junctions [33] with patch size 11.\n15\nFigure 15. Qualitative behavior of our model on noiseless natural images. From left to right: Input image f[n], output distance map \u00afd[n],\noutput boundary map \u00afb\u03b7[n] with \u03b7 = 0.7, and output boundary-smoothed features \u00aff[n].\n16\nS5. Additional Examples for Low-light Images\nFigure 16 shows examples of applying our model to indoor images taken by an iPhone XS in low light conditions.\nFigure 16. Visualization of our model\u2019s output for low-light images captured by an iPhone XS. From left to right: Input image f[n], output\ndistance map \u00afd[n], output boundary map \u00afb\u03b7[n] with \u03b7 = 0.7, and output boundary-smoothed features \u00aff[n].\nFigure 17 provides additional comparisons for a sample of varying-noise images from the ELD dataset [34]. When de-\ntecting boundaries at low signal-to-noise ratios, it is difficult to accurately discern finer structures as the noise level increases.\nSome algorithms, such as Field of Junctions [33], have tunable parameters such as patch-size that provide control over the\nlevel of detection. A small patchsize allows recovering fine structures in lower noise situations, but it causes many false\n17\npositive boundaries at high noise levels. Conversely, a large patchsize provides more resilience to noise but has not ability\nto recover fine structure at all. Our model reduces the severity of this trade-off by automatically adapting its local window-\ning functions in ways that have learned to account for both the amount of noise and the local geometry of the underlying\nboundaries.\nIn Figure 17 we see that our model is able to capture the double-contour shape of the curved, thin black bars, and that\nit continues to resolve them as the noise level increases, more than the other low-level methods. We also note that only the\nlow-level models resolve this level of detail in the first place: The models trained on human annotations\u2014EDTER, HED,\nPidinet, and Structured Forests\u2014miss the double contour entirely, estimating instead a single thick curve. We emphasize\nagain that a user can adjust the behavior of Canny and Field of Junctions by tuning their local size parameters, either the\nfilter size for Canny or the patchsize for Field of Junctions. Increasing the local size improves their resilience to noise but\nreduces their spatial precision. Neither system provides the ability to estimate fine grained details and withstand noise, like\nour model does.\n18\nFigure 17.\nQualitative comparison between our model\u2019s output boundaries \u00afb\u03b7[n] and those of other methods, for a crop from the ELD\ndataset under increasing amounts of photographic noise. We compare to end-to-end models that are trained to match human annotations\n(EDTER [28], HED [35], Pidinet [31], and Structured Forests [10]) in addition to low-level models that are not (Canny [4], and Field of\nJunctions (FOJ) [33]).\n19\nFigure 18 contains additional examples of images cropped from the ELD dataset. Here we include examples with even\nhigher levels of noise to show the complete degradation of our algorithm and others.\nFigure 18\n20\nFigure 18. (cont.)\n21\nFigure 18. (cont.)\n22\nFigure 18.\n(cont.) Additional qualitative comparisons between our model\u2019s output boundaries \u00afb\u03b7[n] and those of other methods, using\ncrops from the ELD dataset under increasing amounts of photographic noise, including very high levels of noise.\nS6. Additional Uses of Our Model\nHere we demonstrate to uses of our model that follow directly from its output: hole-filling in RGBD images and non-\nphotorealistic stylization.\nS6.1. Color-based Depth Completion\nFigure 19 shows an example of using our model for simple hole-filling in the depth channels of RGBD images from the\nMiddlebury Stereo Datasets [29, 30]. We run our model on the RGB channels, and then for each pixel n that has a missing\ndepth value, we use our model\u2019s output local attention kernels an(x) to fill in that pixel\u2019s value using an attention-weighted\naverage of the observed depth values around it. This simple algorithm can be applied whenever the hole sizes are smaller\nthan the maximum diameter of our attention maps, which is 34 \u00d7 34 in our current implementation).\n23\nFigure 19. Using our model for depth completion in RGBD images. Left: Input RGB channels. Middle: Input depth channel, with dark\nblue indicating missing values. Right: Completed depth using our model\u2019s output attention kernels.\n24\nS6.2. Application: Photo Stylization\nFigure 20 shows examples of using our model\u2019s output for image stylization, by superimposing an inverted copy of the output\nboundary map \u00afb\u03b7[n] onto the smoothed colors \u00aff[n].\nFigure 20. Examples of stylized natural photographs, created by imposing our method\u2019s output boundary map onto the output smoothed\ncolors.\n25\n"
  },
  {
    "title": "VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM",
    "link": "https://arxiv.org/pdf/2401.01256.pdf",
    "upvote": "16",
    "text": "VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM\nFuchen Long, Zhaofan Qiu, Ting Yao and Tao Mei\nHiDream.ai Inc.\n{longfc.ustc, zhaofanqiu, tingyao.ustc}@gmail.com\ntmei@hidream.ai\nhttps://videodrafter.github.io\nAbstract\nThe recent innovations and breakthroughs in diffusion\nmodels have significantly expanded the possibilities of gen-\nerating high-quality videos for the given prompts. Most ex-\nisting works tackle the single-scene scenario with only one\nvideo event occurring in a single background. Extending to\ngenerate multi-scene videos nevertheless is not trivial and\nnecessitates to nicely manage the logic in between while\npreserving the consistent visual appearance of key content\nacross video scenes.\nIn this paper, we propose a novel\nframework, namely VideoDrafter, for content-consistent\nmulti-scene video generation.\nTechnically, VideoDrafter\nleverages Large Language Models (LLM) to convert the in-\nput prompt into comprehensive multi-scene script that ben-\nefits from the logical knowledge learnt by LLM. The script\nfor each scene includes a prompt describing the event, the\nforeground/background entities, as well as camera move-\nment. VideoDrafter identifies the common entities through-\nout the script and asks LLM to detail each entity. The re-\nsultant entity description is then fed into a text-to-image\nmodel to generate a reference image for each entity. Fi-\nnally, VideoDrafter outputs a multi-scene video by generat-\ning each scene video via a diffusion process that takes the\nreference images, the descriptive prompt of the event and\ncamera movement into account. The diffusion model incor-\nporates the reference images as the condition and alignment\nto strengthen the content consistency of multi-scene videos.\nExtensive experiments demonstrate that VideoDrafter out-\nperforms the SOTA video generation models in terms of vi-\nsual quality, content consistency, and user preference.\n1. Introduction\nDiffusion Probabilistic Models (DPM) have demonstrated\nhigh capability in generating high-quality images [4, 11, 12,\n31, 32, 40, 44, 46, 62]. DPM approaches image generation\nas a multi-step sampling process, involving the use of a de-\nnoiser network to progressively transform a Gaussian noise\nScene-1: The young man measures out ingredients\nScene-2: The young man pours the batter into a pan\nScene-3: The young man stirs the batter in the pan\nScene-4: The young man puts the cake on the table\nScene-5: The young man makes a phone call to invite his friends\nInput prompt: A young man with blue hair is making cake\nOutput video:\nFigure 1. An illustration of the input prompt and the generated\nmulti-scene videos by using our VideoDrafter.\nmap into an output image. Compared to 2D images, videos\nhave an additional time dimension, which introduces more\nchallenges when extending DPM to video domain.\nOne\ntypical way is to leverage pre-trained text-to-image mod-\nels to produce video frames [16, 36, 55] or utilize a 3D de-\nnoiser network learnt on video data to generate a sequence\nof frames in an end-to-end manner [2, 8, 9, 13, 30, 43]. De-\nspite having impressive results in the realm of text-to-video\ngeneration, most existing works focus on only single-scene\nvideos, featuring one video event in a single background.\nThe generation of multi-scene videos is still a problem not\nyet fully explored in the literature.\nThe difficulty of multi-scene video generation generally\n1\narXiv:2401.01256v1  [cs.CV]  2 Jan 2024\noriginates from two aspects: 1) how to arrange and es-\ntablish different events in a logical and realistic way for\na multi-scene video? 2) how to guarantee the consistency\nof common entities, e.g., foreground objects or persons,\nthroughout the video? For instance, given an input prompt\nof \u201ca young man is making cake,\u201d a multi-scene video is\nusually to present the step-by-step procedure of making a\ncake, including measuring out the ingredients, pouring the\ningredients into a pan, cooking the cake, etc. This neces-\nsitates a comprehensive understanding and refinement of\nthe prompt. As such, we propose to mitigate the first issue\nthrough capitalizing on Large Language Models (LLM) to\nrewrite the input prompt into multi-scene video script. LLM\ninherently abstracts quantities of text data on the Web about\nthe input prompt to produce the script, which describes and\ndecomposes the video logically into multiple scenes. To\nalleviate the second issue, we exploit the common entities\nto generate reference images as the additional condition to\nproduce each scene video. The reference images, as the\nlink across scenes, effectively align the content consistency\nwithin a multi-scene video.\nTo consolidate the idea, we present a new framework\ndubbed as VideoDrafter for content-consistent multi-scene\nvideo generation. Technically, VideoDrafter first transforms\nthe input prompt into a thorough multi-scene video script\nby using LLM. The script for each scene consists of the\ndescriptive prompt of the event in the scene, a list of fore-\nground objects or persons, the background, and camera\nmovement. VideoDrafter then identifies common entities\nthat appear across multiple scenes and requests LLM to en-\nrich each entity. The resultant entity description is fed into a\npre-trained Stable Diffusion [40] model to produce a refer-\nence image for each entity. Finally, VideoDrafter outputs a\nmulti-scene video via involving two diffusion models, i.e.,\nVideoDrafter-Img and VideoDrafter-Vid. VideoDrafter-\nImg is dedicated to incorporating the descriptive prompt\nof the event and the reference images of entities in each\nscene as the condition to generate a scene-reference im-\nage. VideoDrafter-Vid takes the scene-reference image plus\ntemporal dynamics of the action depicted in the descriptive\nprompt of the event and camera movement in the script as\nthe inputs and produces a video clip for each scene.\nThe main contribution of this work is the proposal of\nVideoDrafter for generating content-consistent multi-scene\nvideos.\nThe solution also leads to the elegant views of\nhow to use LLM to properly arrange the content of multi-\nscene videos and how to generate visually consistent enti-\nties across scenes, which are problems seldom investigated\nin the literature. Extensive experiments conducted on sev-\neral public benchmarks demonstrate that VideoDrafter out-\nperforms state-of-the-art video generation models in terms\nof visual quality, content consistency and user preference.\n2. Related Work\nImage generation is a fundamental challenge of computer\nvision and has evolved rapidly in the past decade. Recent\nadvances in Diffusion Probabilistic Models (DPM) have led\nto remarkable improvements in generating high-fidelity im-\nages [2, 4, 11, 12, 28, 29, 31\u201333, 39, 40, 44\u201346, 62]. DPM\nis a category of generative models that utilizes a sequen-\ntial sampling process to convert random Gaussian noise into\nhigh-quality images. For example, GLIDE [33] and DALL-\nE 2 [39] exploit the sampling process in the pixel space,\nconditioned on the text prompt using classifier-free guid-\nance [11]. Nevertheless, training a powerful denoising net-\nwork remains challenging due to high computational cost\nand memory demand associated with sampling at the pixel\nlevel. To mitigate this problem, Latent Diffusion Models\n(LDM) [40] employ sampling in the latent feature space that\nis established by a pre-trained autoencoder, leading to the\nimprovements on computation efficiency and image quality.\nFurthermore, the application of DPM is further enhanced by\nincorporating advanced sampling strategies [28, 29, 45] and\nadditional control signals [31, 62].\nVideo generation is a natural extension of image gen-\neration in video domain. The early approaches, e.g., Im-\nagenVideo [13] and Make-A-Video [43], train video dif-\nfusion models in the pixel space, resulting in high com-\nputational complexity. Following LDM in image domain,\nseveral works [2, 8, 30] propose to exploit the sampling\nprocess in the latent feature space for video generation.\nThese works extend the 2D UNet with transformer lay-\ners [20, 57, 58] in LDM to 3D UNet by injecting tem-\nporal self-attentions [25, 26] and/or temporal convolutions\n[24, 27, 38]. For instance, Video LDM [2] and AnimateD-\niff [8] focus on training the injected temporal layers while\nfreezing the spatial layers to preserve the ability of the\npre-trained image diffusion model. VideoFusion [30] de-\ncomposes the 3D noise into a 2D base noise shared across\nframes and a 3D residual noise, enhancing the correlation\nbetween frames.\nHowever, the generated videos usually\nhave a limited time duration, typically around 16 frames.\nConsequently, some recent researches emerge to generate\nlong videos by an extrapolation strategy or hierarchical ar-\nchitecture [9, 21, 48, 49, 60]. In addition, video editing\ntechniques utilize the input video as a condition and gener-\nate a video by modifying the style or key object of the input\nvideo [6, 7, 9, 15, 35, 36, 42, 50, 53, 55, 59].\nIn short, our work in this paper focuses on content-\nconsistent multi-scene video generation. The most related\nwork is [22], which aligns the appearance of entities across\nscenes through the bounding boxes provided by LLM. Ours\nis different in the way that we explicitly determine the ap-\npearance of entities by generating reference images, which\nserve as a link across scenes and effectively enhance the\ncontent consistency within a multi-scene video.\n2\nInput Prompt\n\u201ca young man with blue hair is making cake\u201d\nLarge Language Model\n(LLM)\nQuery\n\u201cYou need to \nenvision a multi-\nscene video ... \u201d\nMulti-Scene Video Script\nScene 1: \u201cThe young man measures out ingredients\u201d; Objects: \nyoung man; Background: kitchen; Camera: moving left fast\nScene 2: \u201cThe young man pours the batter into a pan\u201d; Objects: \nyoung man; Background: kitchen; Camera: moving left slowly\nScene N-1: \u201cThe young man puts the cake on the table\u201d; Objects: \nyoung man, cake; Background: kitchen; Camera: static\n\u2026\nScene N: \u201cThe young man makes a phone call to invite his friends\u201d; \nObjects: young man, phone; Background: dining room; Camera: static\nQuery\n\u201cYou need to \ndescribe the entities \nin detail ... \u201d\nEntity Descriptions\nyoung man: \u201cThe photo depicts a young man with blue hair \nstanding in a kitchen, wearing an apron and holding a mixer ...\u201d\ncake: \u201cThe cake is a masterful work of art, with intricate designs and \npatterns that are both visually appealing and visually stunning ... \u201d\nkitchen: \u201cThis photo of a kitchen features a clean and modern \naesthetic, with white cabinets and countertops ... \u201d\ndining room: \u201cThe photo captures a modern luxurious dining room \nwith elegant furniture and a breathtaking view of the city skyline.\u201d\n\u201cyoung man\u201d\n\u201ccake\u201d\n\u201ckitchen\u201d\n\u201cdining room\u201d\nText-to-Image Model \n\uff08T2I Model)\nEntity Descriptions\n(1) Multi-scene video script generation\n(2) Entity reference image generation\n(3) Video scene generation\nScene N-1: \u201cThe young man \nputs the cake on the table\u201d\nEntities:\n\u201cyoung man\u201d\n\u201ccake\u201d\n\u201cdining room\u201d\nAction: putting the cake on the table; Camera: static\nScene-Reference Image\nVideoDrafter-Vid\nVideoDrafter-Img\nVideo Scene\nVideoDrafter-Img\nVideoDrafter-Vid\nForeground Reference images\nBackground Reference images\nForeground \nReference\nBackground \nReference\n2D Denoising UNet\n2D\nAttn\n2D\nConv\n2D\nAttn\n2D\nConv\n\u2026\n3D Denoising UNet\n3D\nAttn\n3D\nConv\n3D\nAttn\n3D\nConv\n\u2026\n2D Noise\nScene-Reference\nImage\nVisual \nEncoder\nText \nEncoder\nVisual \nEncoder\nAction\nEmbedder\naction\nThe young man puts \nthe cake on the table\nCamera Moving\n3D Noise\nx T\nx T\nx N\nFigure 2. An overview of our VideoDrafter framework for content-consistent multi-scene video generation. VideoDrafter consists of\nthree main stages: (1) multi-scene video script generation, (2) entity reference image generation, and (3) video scene generation. In the\nfirst stage, LLM is utilized to convert the input prompt into a comprehensive multi-scene script. The script for each scene includes the\ndescriptive prompt of the event in the scene, a list of foreground objects or persons, the background, and camera movement. We then\nrequest LLM to detail the common foreground/background entities across scenes. These entity descriptions are fed into a text-to-image\n(T2I) model to produce reference images in the second stage. Finally, in the third stage, VideoDrafter-Img exploits the descriptive prompt\nof the event and the reference images of entities in each scene as the condition to generate a scene-reference image. VideoDrafter-Vid takes\nthe scene-reference image plus temporal dynamics of the action depicted in the descriptive prompt of the event and camera movement in\nthe script as the inputs and produces a video clip for each scene.\n3. VideoDrafter\nThis section presents the proposed VideoDrafter framework\nfor content-consistent multi-scene video generation. Fig-\nure 2 illustrates an overview of VideoDrafter framework,\nconsisting of three main stages: (1) multi-scene video script\ngeneration (Sec. 3.1), (2) entity reference image generation\n(Sec. 3.2), and (3) video scene generation (Sec. 3.3).\n3.1. Multi-Scene Video Script Generation\nAs depicted in Figure 2(1), VideoDrafter utilizes LLM to\nconvert the input prompt into a comprehensive multi-scene\nscript. In view of its high deployment flexibility and in-\nference efficiency, we use the open-source ChatGLM3-6B\nmodel [5, 61].\nThe LLM is requested by a pre-defined\nquery, \u201cYou need to envision a multi-scene video and de-\nscribe each scene ...\u201d, to treat the input prompt as the theme,\nlogically decompose the video into multiple scenes and gen-\nerate a script for each scene in the following format:\n[Scene 1: prompt, foreground, background, camera move];\n[Scene 2: prompt, foreground, background, camera move];\n...\n[Scene N: prompt, foreground, background, camera move].\n(1)\nHere N denotes the number of video scenes, which is deter-\nmined by the LLM. For each scene, the descriptive prompt\nof the event in the scene, a list of foreground objects or per-\nsons, the background, and camera movement are provided.\nThe camera movement is restricted to a close-set of direc-\ntions {static, left, right, up, down, forward, backward} and\nspeeds {slow, medium, fast}.\nNext, VideoDrafter identifies the common entities,\nwhich include foreground objects or persons and back-\nground locations. To achieve this, we ask the LLM to assign\nthe common object, person, or background the same name\nacross scenes when generating the video script. Therefore,\nwe strictly match the name of entities and discover the en-\ntities that appear in multiple scenes. To further improve the\nquality of the video script, we employ the capability of the\nLLM for multi-round dialogue. Specifically, we start the di-\nalogue by asking the LLM to specify the key aspects with\nrespect to the entity, such as \u201cWhat are the aspects that\nshould be considered when describing a photo of a young\nman in detail?\u201d In the next round of dialogue, we request\nthe LLM to describe the entity from the viewpoints of the\ngiven aspects. Moreover, the original prompt is also taken\nas the input to the LLM to ensure that the essential charac-\n3\nteristics, e.g., \u201cblue hair\u201d of the young man, are emphasized\nin entity description generation.\nPlease note that the GPT-4 [34] can also be used for\nscript generation, but it incurs an additional 0.12 USD for\nthe GPT-4 API call per query. In VideoDrafter, we lever-\nage the open-source ChatGLM3-6B and perform the infer-\nence on our devices to circumvent the need for API call.\nNevertheless, the scale of ChatGLM3-6B is much smaller,\nresulting in unstable outcomes that may deviate from the\nspecified script format. To alleviate this issue, we have em-\npirically abstracted the following principles to enhance the\nstability of open-source LLM:\n\u2022 Before the dialogue starts, we provide comprehensive in-\nstructions to the LLM, delineating the additional require-\nments, specifying the script format, and offering the ex-\namples of the expected outputs.\n\u2022 For each query, we manually select five in-context exam-\nples as the historical context for multi-round dialogue.\nThese examples are very carefully designed to ensure a\ndiverse range of scenes, key objects, and background, and\nserve to emphasize the required script format for LLM.\n\u2022 After each round of dialogue, we verify the output format.\nIf the results are seemingly inappropriate, we re-run the\nentire script generation stage. Such strategy is simple to\nimplement and does not require any additional expenses.\nWe will provide the full version of our instructions, exam-\nples, and queries in the supplementary materials.\n3.2. Entity Reference Image Generation\nIn the second stage of VideoDrafter, we unify the visual\nappearance of common entities by explicitly generating a\nreference image for each entity. The reference images act\nas the link to cohere the content across scenes. We achieve\nthis by first feeding the entity description into a pre-trained\nStable Diffusion model for text-to-image generation. Then,\nwe employ the U2-Net [37] model for salient object detec-\ntion, and segment the foreground and background areas in\neach resultant image. By utilizing the segmentation masks,\nwe can further remove the background pixels from the fore-\nground references and vice versa, in order to prevent the\ninterference between the foreground and background in the\nreference images.\n3.3. Video Scene Generation\nVideoDrafter produces a multi-scene video by generating\neach scene via the diffusion models by taking the refer-\nence images, the descriptive prompt of the event and cam-\nera movement into account. This stage involves two pri-\nmary components: the VideoDrafter-Img, which utilizes\nthe descriptive prompt of the event and the reference im-\nages of entities in each scene as the condition to generate\na scene-reference image, and the VideoDrafter-Vid, which\nemploys the scene-reference image plus temporal dynam-\n+\nCA\nCA\nCA\nSA\n+\nCLIP-Text\nCLIP-Image\nCLIP-Image\nScene \nprompt\nForeground \nReference Image\nBackground \nReference Image\nFrozen\nTuned\nFigure 3. A diagram illustrating the proposed attention module\nin the VideoDrafter-Img which takes the scene prompt and fore-\nground/background reference images as inputs.\nics of the action depicted in the descriptive prompt of the\nevent and camera movement in the script as the inputs and\nproduces a video clip for each scene.\n3.3.1\nVideoDrafter-Img\nThe VideoDrafter-Img component aims to generate a scene-\nreference image conditioning on the event prompt and en-\ntity reference images for each scene. To accomplish this,\nwe remold the Stable Diffusion architecture by replacing\nthe original attention module with a novel attention module\nthat can handle three contexts: the text prompt, foreground\nreference image, and background reference image. As de-\npicted in Figure 3, we utilize text and visual encoder of a\npre-trained CLIP model to extract the sequential text fea-\nture yt \u2208 RLt\u00d7Ct and local image features yf \u2208 RLf \u00d7Cf\nand yb \u2208 RLb\u00d7Cb for the prompt, foreground reference im-\nage, and background reference image, respectively. Here, L\nand C denote the length and the channels of the feature se-\nquence. For the case of multiple foregrounds in one scene,\nwe concatenate the features from all foreground reference\nimages along the length dimension. Given the input feature\nx, the outputs z of the attention are computed as\ny = CA1(x, yt) + CA2(x, yf) + CA3(x, yb),\nz = x + SA(y),\n(2)\nwhere CA1 and SA are the cross-attention and self-attention\nmodules, respectively, in the original Stable Diffusion ar-\nchitecture. We add two additional cross-attention modules,\nCA2 and CA3, which leverage the guidance provided by\nentity reference images. Moreover, we propose to optimize\nthe parameters of CA2 and CA3 while freezing the other\nparts of the network.\n3.3.2\nVideoDrafter-Vid\nVideoDrafter-Vid is a video diffusion model that employs\nthe scene-reference image, the action described in the\nprompt of the event, and camera movement in the script\nas the inputs. Particularly, we start by extending the Stable\n4\n+\nCA\n+\nAction-Embed\nScene-Reference \nImage\nSpatial SA\nFrozen\nTuned\nCLIP-Image\nAction Category\nTemporal SA\nFigure 4. A diagram illustrating the proposed attention module in\nthe VideoDrafter-Vid conditioning on the scene-reference image\nand the described action category.\nDiffusion model to a spatio-temporal form and replacing the\noriginal attention module with a new one that is conditioned\non the scene-reference image and action category, as shown\nin Figure 4. Taking 400 action categories in Kinetics [3]\nas an action vocabulary, an indicator vector ya \u2208 [0, 1]400\nis built to infer if each action in the vocabulary exists in the\nscene prompt and subsequently converted into feature space\nusing a linear embedding f. For the scene-reference image,\nwe use the visual encoder of CLIP to extract the image fea-\nture ys \u2208 RLs\u00d7Cs, which is then fed into the cross-attention\noperation. The original self-attention is decomposed into\na spatial self-attention (Spatial SA) and a temporal self-\nattention (Temporal SA), which operate self-attention solely\non spatial and temporal dimension, respectively, to reduce\ncomputations. Hence, given the input feature x, the atten-\ntion module is formulated as\ny = CA(x, ys) + f(ya),\nz = x + Temporal SA(Spatial SA(y)).\n(3)\nMoreover, we further inject several temporal convolutions\nbehind each spatial convolution into the Stable Diffusion\nmodel, to better capture temporal dependencies.\nTo reflect the camera movement stated by the script in\nthe generated video, we uniquely modify the frames in the\nintermediate step of sampling process by warping the neigh-\nboring frames based on the camera moving direction and\nspeed. We execute this adjustment after the first Tm DDIM\nsampling steps, followed by continuing the sampling pro-\ncess. Such modification ensures that the resultant video clip\nmaintains the same camera movement as we warp the in-\ntermediate frames. In general, setting a small Tm for early\nmodification may not effectively control the camera move-\nment, while a late modification may affect the visual quality\nof the output videos. In practice, we observe that Tm=5 pro-\nvides a good trade-off. We will detail the formulation of the\nmodification process and the ablation study of the step Tm\nin our supplementary materials.\n4. Experiments\n4.1. Datasets\nOur VideoDrafter framework is trained on three large-scale\ndatasets: LAION-2B [41], WebVid-10M [1] and HD-VG-\n130M [52]. The LAION-5B is one of the largest text-image\ndataset consisting of around 5 billion text-image pairs.\nTo train VideoDrafter-Img, We utilize a subset, namely\nLAION-2B, which focuses on the text prompts in English.\nThe WebVid-10M and HD-VG-130M are the large-scale\nsingle-scene video datasets, containing approximately 10M\nand 130M text-video pairs, respectively. VideoDrafter-Vid\nis trained on the combination of WebVid-10M and a ran-\ndomly chosen 20M subset from HD-VG-130M.\nTo evaluate video generation, we select the text prompts\nfrom three video datasets, i.e., MSR-VTT [56], ActivityNet\nCaptions [18] and Coref-SV [22]. The first one provides\nthe single-scene prompts, while the remaining two datasets\ncomprise multi-scene prompts. The MSR-VTT [56] con-\nsists of 10K web video clips, each annotated with approx-\nimate 20 natural sentences. We utilize the text annotation\nof validation videos to serve as single-scene prompts in our\nevaluation. The ActivityNet Captions dataset is a multi-\nevent video dataset designed for dense-captioning tasks.\nFollowing [22], we randomly sample 165 videos from the\nvalidation set and exploit the event captions as the multi-\nscene prompts.\nThe Coref-SV is a multi-scene descrip-\ntion dataset, which was constructed by replacing the subject\nof multi-scene paragraphs in Pororo-SV dataset [17, 19].\nCoref-SV samples 10 episodes from the Pororo-SV dataset\nand replaces the subject with 10 real-world entities, result-\ning in 100 multi-scene prompts.\n4.2. Evaluation Metrics\nFor the video generation task, we adopt five evaluation met-\nrics. To assess the visual quality of the generated videos,\nwe utilize the average of the per-frame Fr\u00b4echet Inception\nDistance (FID) [10] and the clip-level Fr\u00b4echet Video Dis-\ntance (FVD) [47], both of which are commonly used met-\nrics. We also employ the CLIPSIM [54] metric to eval-\nuate the alignment between the generated frames and the\ninput prompt. To verify the content consistency, we calcu-\nlate frame consistency (Frame Consis.) by determining the\nCLIP-similarity between consecutive frames, serving as an\nintra-scene consistency measure. Additionally, we employ\nthe Grounding-DINO detector [23] to detect common ob-\njects across scenes and then calculate the CLIP-similarity\nbetween the common objects appeared in different scenes,\nachieving cross-scene consistency (Scene Consis.).\n4.3. Implementation Details\nWe implement our VideoDrafter using the Diffusers code-\nbase on the PyTorch platform.\n5\nTable 1. Performance comparisons of VideoDrafter-Img variants\nwith different input references on MSR-VTT validation set.\nInput References\nFG-SIM\nBG-SIM\nCLIPSIM\nFG Ref.\nBG Ref.\nw/o Ref.\n0.5162\n0.4131\n0.3001\n\u2713\n0.7919\n0.4393\n0.2982\n\u2713\n0.5362\n0.5742\n0.3002\n\u2713\n\u2713\n0.8102\n0.5861\n0.3023\nTraining stage of VideoDrafter-Img.\nVideoDrafter-\nImg is originated from the Stable Diffusion v2.1 model\nby incorporating two additional cross-attention modules.\nThese modules are initialized from scratch and trained on\nthe text-image pairs from LAION-2B dataset, while other\nparts of the network are frozen. For each image, we ran-\ndomly sample a 512\u00d7512 patch cropped from the original\nimage, and utilize the U2-Net model to segment the fore-\nground area of each patch. The isolated foreground and\nbackground areas serve as the foreground and background\nreference images, respectively, for guiding the generation of\nthe input patch. We set each minibatch as 512 patches that\nare processed on 64 A100 GPUs in parallel. The parame-\nters of the model are optimized by AdamW optimizer with\na fixed learning rate of 1 \u00d7 10\u22124 for 20K iterations.\nTraining stage of VideoDrafter-Vid.\nVideoDrafter-\nVid model is developed based on the Stable Diffusion XL\nframework by inserting temporal attentions and temporal\nconvolutions. The training is carried out on the WebVid-\n10M and HD-VG-130M datasets.\nFor each video, we\nrandomly sample a 16-frame clip with the resolution of\n320\u00d7512 and an FPS of 8. The middle frame of the clip\nis utilized as the scene-reference image. Each minibatch\nconsists of 128 video clips implemented on 64 A100 GPUs\nin parallel. We utilize the AdamW optimizer with a fixed\nlearning rate of 3 \u00d7 10\u22126 for 480K iterations.\n4.4. Experimental Analysis of VideoDrafter\nEvaluation on VideoDrafter-Img. We first verify the ef-\nfectiveness of VideoDrafter-Img in aligning with the input\nentity reference images. To this end, we take the prompts\nfrom MSR-VTT validation set. The input foreground and\nbackground reference images are produced by using LLM\nand Stable Diffusion model. We validate the generated im-\nages on the measure of foreground similarity (FG-SIM)\nand background similarity (BG-SIM), which are the CLIP-\nsimilarity values with the foreground and background ref-\nerence images, respectively. Table 1 lists the performance\ncomparisons of different VideoDrafter variants by leverag-\ning different input references.\nOverall, the use of fore-\nground/background reference image as guidance leads to\nhigher FG-SIM/BG-SIM values. The combination of both\nreference images achieves the highest FG-SIM of 0.8102\nand BG-SIM of 0.5861. It is worth noting that all vari-\nForeground\nReference Image\nw/o Ref.\nw/ FG Ref.\nw/ BG Ref.\nVideoDrafter-Img\nBackground\nReference Image\nFigure 5. The examples of the foreground and background ref-\nerence images and the generated scene-reference image by the\nVideoDrafter-Img variants.\nTable 2. Performance comparisons for single-scene video genera-\ntion with real frame as scene-reference image on WebVid-10M.\nApproach\nFVD (\u2193)\nFrame Consis. (\u2191)\nRF+VideoComposer [53]\n231.0\n95.9\nRF+VideoDrafter-Vid\u2212\n157.3\n98.5\nRF+VideoDrafter-Vid\n116.5\n98.8\nants obtain close CLIPSIM values, indicating that the in-\nvolvement of reference images does not affect the alignment\nwith the input prompts. Figure 5 showcases six generated\nimages by different VideoDrafter-Img variants with various\nreference images. The results demonstrate the advantage of\nVideoDrafter-Img to align with the entity reference images.\nEvaluation on VideoDrafter-Vid.\nNext, we assess\nthe visual quality of the single-scene videos generated\nby VideoDrafter-Vid.\nWe exploit the real frame from\nthe WebVid-10M validation set as the scene-reference im-\nage irrespective of the generation quality, and produce a\nvideo using the corresponding text prompt, which is re-\nferred to as RF+VideoDrafter-Vid. We compare our pro-\nposal with two baseline models of RF+VideoComposer\nand RF+VideoDrafter-Vid\u2212. The former employs a pre-\ntrained image animation model by VideoComposer [53] to\nconvert the real frame into a video clip. The latter disables\nthe action guidance in VideoDrafter-Vid. Table 2 presents\nthe performance comparisons for single-scene video gener-\nation on the WebVid-10M dataset. With the same scene-\nreference images, VideoDrafter-Vid\u2212 outperforms Video-\nComposer relatively by 31.9% FVD and 2.7% frame consis-\ntency. This improvement is attributed to the deep network\narchitecture and large-scale training set. The performance\n6\nModelScopeT2V\nVideoDirectorGPT\nVideoDrafter (Ours)\nScene-1: A mouse \nis holding a book \nand makes a \nhappy face\nScene-2: A mouse \nlooks happy and \ntalks\nScene-3: A mouse \nis pulling petals \noff the flower\nScene-4: A mouse \nis ripping a petal \nfrom the flower\nInput Multi-\nScene Prompt\nFigure 6. Examples of generated multi-scene videos by ModelScopeT2V [51], VideoDirectorGPT [22] and our VideoDrafter utilizing a\nmulti-scene prompt from the Coref-SV dataset. For each video, only the first four scenes are given. The results of VideoDirectorGPT are\nprovided in the project webpage and thus with bounding box annotation.\nTable 3. Performance comparisons for single-scene video genera-\ntion on MSR-VTT validation set. RF indicates whether to utilize\nthe real frame as the reference.\nApproach\nRF\nFID (\u2193)\nFVD (\u2193)\nCogVideo [14]\n23.6\n-\nMagicVideo [63]\n-\n998\nMake-A-Video [43]\n13.2\n-\nVideoComposer [53]\n-\n580\nVideoDirectorGPT [22]\n12.2\n550\nModelScopeT2V [51]\n11.1\n550\nSD+VideoDrafter-Vid\n11.9\n381\nRF+VideoComposer [53]\n\u2713\n31.3\n208\nRF+VideoDrafter-Vid\n\u2713\n10.8\n133\nis further enhanced to 116.5 FVD and 98.8 frame consis-\ntency by RF+VideoDrafter-Vid, verifying the superiority of\ninvolving action category guidance to improve visual qual-\nity and intra-scene consistency.\nSimilar performance trends are observed on MSR-VTT\ndataset, as summarized in Table 3. The methods in this ta-\nble are grouped into two categories: the methods with or\nwithout real frame (RF) as reference. To compare with the\ngeneration models without RF, we develop a two-step solu-\ntion that first generates the scene-reference image by Stable\nDiffusion, and then converts the image into a video clip by\nVideoDrafter-Vid, which is denoted as SD+VideoDrafter-\nVid. Specifically, VideoDrafter-Vid attains the best FVD\non both settings with and without a real frame as refer-\nence.\nSD+VideoDrafter-Vid is slightly inferior to Mod-\nelScopeT2V in FID. We speculate that this may be the result\nof not optimizing Stable Diffusion on video frames, result-\ning in poorer frame quality against ModelScopeT2V. Nev-\nertheless, SD+VideoDrafter-Vid apparently surpasses Mod-\nelScopeT2V in FVD, validating the video-level quality by\nVideoDrafter-Vid.\nTable 4. Performance comparisons for multi-scene video genera-\ntion on ActivityNet Caption dataset.\nApproach\nFID (\u2193)\nFVD (\u2193)\nScene Consis. (\u2191)\nModelScopeT2V [51]\n18.1\n980\n46.0\nVideoDirectorGPT [22]\n16.5\n805\n64.8\nVideoDrafter w/o Ref.\n17.3\n624\n50.8\nVideoDrafter\n13.2\n395\n75.1\nTable 5. Performance comparisons for multi-scene video genera-\ntion on Coref-SV dataset.\nApproach\nCLIPSIM (\u2191)\nScene Consis. (\u2191)\nModelScopeT2V [51]\n0.3021\n37.9\nVideoDirectorGPT [22]\n-\n42.8\nVideoDrafter w/o Ref.\n0.3103\n40.9\nVideoDrafter\n0.3304\n77.3\n4.5. Evaluations on Multi-Scene Video Generation\nWe validate VideoDrafter for multi-scene video generation\non ActivityNet Captions and Coref-SV datasets. Both of\nthe datasets consist of multi-scene prompts, which necessi-\ntate the LLM to write the video script based on the given\nprompt of each scene. We compare with three approaches:\nModelScopeT2V, VideoDirectorGPT and VideoDrafter w/o\nRef. by disabling the reference images in VideoDrafter. Ta-\nble 4 details the performance comparisons on ActivityNet\nCaptions. As indicated by the results in the table, Video-\nDrafter exhibits superior visual quality and better cross-\nscene consistency.\nSpecifically, VideoDrafter surpasses\nVideoDrafter w/o Ref. by 24.3 scene consistency, which\nessentially verifies the effectiveness of incorporating entity\nreference images. Moreover, VideoDrafter leads to 10.3 and\n29.1 improvements in scene consistency over VideoDirec-\ntorGPT and ModelScopeT2V, respectively. Similar results\nare also observed on Coref-SV dataset, as summarized in\nTable 5. Note that as Coref-SV only offers prompts without\nthe corresponding videos, FID and FVD cannot be mea-\n7\nScene-1: An old woman opens her window and gives a big yawn. She is out of her \nhouse which is in the middle of a forest. The whole land is covered with snow\nScene-2: The old woman is introducing herself to the audiences with \na nice smile. The old woman is in her cabin\nScene-3: The old woman is walking around her house. The whole land is covered with snow \nScene-4: The old woman is making a snowball. The woods are covered with snow\nScene-1: A black hair boy is in the library\nScene-2: The black hair boy is starting to read a book about monster\nScene-3: In the story, at night, a big scary monster has appeared in village\nScene-4: The black hair boy is still reading the book at the story's night background\nScene-1: A pot of water boils on a stove\nScene-2: A man shows us a package of spaghetti then adds the spaghetti to the boiling water\nScene-3: The man stirs the spaghetti in the pot\nScene-4: The man drains the spaghetti and rinses it before putting it into a bowl\nInput Prompt: A man is cooking spaghetti\nInput Prompt: An old woman is walking around her house in the snow\nInput Prompt: A black hair boy is reading a book which is related to the monster\nFigure 7. Examples of generated multi-scene videos by VideoDrafter on MSR-VTT. For each video, only the first four scenes are given.\nForeground\nReference Image\nBackground\nReference Image\nScene-1: The cute Shiba lies in the room\nScene-2: The cute Shiba with smile sits in the car and go to a place\nScene-3: The cute Shiba plays in flowers\nScene-4: The cute Shiba rests next to a tree\nOutput video\nFigure 8. One example of generated multi-scene videos by our\nVideoDrafter using the real images as entity reference images.\nsured for this case. As shown in the table, VideoDrafter\nagain achieves the highest cross-scene consistency of 77.3,\nmaking an absolute improvement of 39.4 and 34.5 over\nModelScopeT2V and VideoDirectorGPT. Figure 6 show-\ncases an example of generated four-scene videos by dif-\nferent approaches on Coref-SV, manifesting the ability of\nVideoDrafter on generating visually similar entities (e.g.,\nmouse/garden) across scenes. Figure 8 further shows an\nexample of multi-scene video generation by VideoDrafter\nusing the real images as entity reference images, which\ndemonstrates the potential of VideoDrafter in customizing\nthe generated objects or environments.\n4.6. Human Evaluation\nIn this section, we conduct a human study to evaluate the\nentire process of generating multi-scene video from a sin-\ngle prompt. We compare our VideoDrafter with four ap-\nproaches: ModelScopeT2V w/o LLM and VideoDrafter\nw/o Ref. w/o LLM to generate five scenes by duplicating\nTable 6. The user study on three criteria: visual quality (VQ),\nlogical coherence (LC) and content consistency (CC).\nApproach\nVQ (\u2193)\nLC (\u2193)\nCC (\u2193)\nModelScopeT2V w/o LLM\n4.5\n4.7\n3.9\nModelScopeT2V w/ LLM\n4.5\n3.8\n4.2\nVideoDrafter w/o Ref. w/o LLM\n2.0\n3.0\n2.3\nVideoDrafter w/o Ref.\n2.4\n2.3\n3.4\nVideoDrafter\n1.6\n1.2\n1.2\nthe input prompt, ModelScopeT2V w/ LLM and Video-\nDrafter w/o Ref. to utilize LLM to provide video script\nas described in Sec. 3.1 while generate each scene indi-\nvidually. We invite 12 evaluators and randomly select 100\nprompts from MSR-VTT validation set for human evalua-\ntion. We show all the evaluators the five videos generated\nby each approach plus the given prompt and ask them to\nrank the five videos from 1 to 5 (good to bad) with respect\nto the three criteria: visual quality (VQ), logical coherence\n(LC) and content consistency (CC). For each approach, we\naverage the ranking on each criterion of all the generated\nvideos. As indicated by the results in Table 6, the study\nproves the impact of LLM generated video script and entity\nreference images to improve logical coherence and content\nconsistency, respectively. Figure 7 illustrates the examples\nof the generated multi-scene videos by our VideoDrafter.\n5. Conclusions\nWe have presented a new VideoDrafter framework for\ncontent-consistent multi-scene video generation.\nVideo-\nDrafter involves LLM to benefit from the logical knowl-\nedge learnt behind and rewrite the input prompt into a multi-\nscene video script. Then, VideoDrafter identifies common\nentities throughout the script and generates a reference im-\nage for each entity, which serves as the link across scenes\nto ensure the appearance consistency. To produce a multi-\nscene video, VideoDrafter devises two diffusion models\nof VideoDrafter-Img and VideoDrafter-Vid. VideoDrafter-\n8\nImg creates a scene-reference image for each scene based\non the corresponding event prompt and entity reference im-\nages. VideoDrafter-Vid converts the scene-reference image\ninto a video clip conditioning on the specific action and\ncamera movement.\nExtensive evaluations on four video\nbenchmarks demonstrate the superior visual quality and\ncontent consistency by VideoDrafter over SOTA models.\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in Time: A Joint Video and Image Encoder for\nEnd-to-End Retrieval. In ICCV, 2021. 5\n[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your Latents: High-Resolution Video Synthesis with\nLatent Diffusion Models. In CVPR, 2023. 1, 2\n[3] Jo\u02dcao Carreira and Andrew Zisserman. Quo Vadis, Action\nRecognition? A New Model and The Kinetics Dataset. In\nCVPR, 2017. 5\n[4] Prafulla Dhariwal and Alex Nichol. Diffusion Models Beat\nGANs on Image Synthesis. In NeurIPS, 2021. 1, 2\n[5] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong\nQiu, Zhilin Yang, and Jie Tang. GLM: General Language\nModel Pretraining with Autoregressive Blank Infilling. In\nACL, 2022. 3\n[6] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand Content-guided Video Synthesis with Diffusion Models.\nIn ICCV, 2023. 2\n[7] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenFlow: Consistent Diffusion Features for Consistent\nVideo Editing. arXiv preprint arXiv:2307.10373, 2023. 2\n[8] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. AnimateDiff: Animate Your\nPersonalized Text-to-Image Diffusion Models without Spe-\ncific Tuning. arXiv preprint arXiv:2307.04725, 2023. 1, 2\n[9] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan,\nand Qifeng Chen.\nLatent Video Diffusion Models for\nHigh-Fidelity Long Video Generation.\narXiv preprint\narXiv:2211.13221, 2022. 1, 2\n[10] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans Trained by\na Two Time-Scale Update Rule Converge to a Local Nash\nEquilibrium. In NIPS, 2017. 5\n[11] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion\nGuidance. arXiv preprint arXiv:2207.12598, 2022. 1, 2\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif-\nfusion Probabilistic Models. In NeurIPS, 2020. 1, 2\n[13] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben\nPoole, Mohammad Norouzi, David J. Fleet, and Tim Sal-\nimans.\nImagen Video: High Definition Video Generation\nwith Diffusion Models. arXiv preprint arXiv:2210.02303,\n2022. 1, 2\n[14] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and\nJie Tang. CogVideo: Large-Scale Pretraining for Text-to-\nVideo Generation via Transformers. In ICLR, 2023. 7\n[15] Zhihao Hu and Dong Xu.\nVideoControlNet: A Motion-\nGuided Video-to-Video Translation Framework by Us-\ning Diffusion Model with ControlNet.\narXiv preprint\narXiv:2307.14073, 2023. 2\n[16] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2Video-Zero: Text-\nto-Image Diffusion Models are Zero-Shot Video Generators.\nIn ICCV, 2023. 1\n[17] Kyung-Min Kim,\nMin-Oh Heo,\nSeong-Ho Choi,\nand\nByoung-Tak Zhang. DeepStory: Video Story QA by Deep\nEmbedded Memory Networks. In IJCAI, 2017. 5\n[18] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and\nJuan Carlos Niebles. Dense-Captioning Events in Videos. In\nICCV, 2017. 5\n[19] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng,\nYuexin Wu, Lawrence Carin, David Carlson, and Jianfeng\nGao. StoryGAN: A Sequential Conditional GAN for Story\nVisualization. In CVPR, 2019. 5\n[20] Yehao Li, Ting Yao, Yingwei Pan, and Tao Mei. Contextual\nTransformer Networks for Visual Recognition. IEEE Trans.\non PAMI, 2022. 2\n[21] Jian Liang, Chenfei Wu, Xiaowei Hu, Zhe Gan, Jianfeng\nWang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan\nDuan. NUWA-Infinity: Autoregressive over Autoregressive\nGeneration for Infinite Visual Synthesis. In NeurIPS, 2022.\n2\n[22] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal.\nVideoDirectorGPT: Consistent Multi-Scene Video Gen-\neration\nvia\nLLM-Guided\nPlanning.\narXiv\npreprint\narXiv:2309.15091, 2023. 2, 5, 7\n[23] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding DINO: Marrying Dino with Grounded\nPre-Training for Open-Set Object Detection. arXiv preprint\narXiv:2303.05499, 2023. 5\n[24] Fuchen Long, Ting Yao, Zhaofan Qiu, Xinmei Tian, Jiebo\nLuo, and Tao Mei. Gaussian Temporal Awareness Networks\nfor Action Localization. In CVPR, 2019. 2\n[25] Fuchen Long, Zhaofan Qiu, Yingwei Pan, Ting Yao, Jiebo\nLuo, and Tao Mei. Stand-Alone Inter-Frame Attention in\nVideo Models. In CVPR, 2022. 2\n[26] Fuchen Long, Zhaofan Qiu, Yingwei Pan, Ting Yao, Chong-\nWah Ngo, and Tao Mei.\nDynamic Temporal Filtering in\nVideo Models. In ECCV, 2022. 2\n[27] Fuchen Long, Ting Yao, Zhaofan Qiu, Xinmei Tian, Jiebo\nLuo, and Tao Mei.\nBi-calibration Networks for Weakly-\nSupervised Video Representation Learning. IJCV, 2023. 2\n[28] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. DPM-Solver: A Fast ODE Solver for Dif-\nfusion Probabilistic Model Sampling in Around 10 Steps. In\nNeurIPS, 2022. 2\n[29] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. DPM-Solver++: Fast Solver for Guided\nSampling of Diffusion Probabilistic Models. arXiv preprint\narXiv:2211.01095, 2023. 2\n9\n[30] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie-\nniu Tan. VideoFusion: Decomposed Diffusion Models for\nHigh-Quality Video Generation. In CVPR, 2023. 1, 2\n[31] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian\nZhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2I-\nAdapter: Learning Adapters to Dig out More Controllable\nAbility for Text-to-Image Diffusion Models. arXiv preprint\narXiv:2302.08453, 2023. 1, 2\n[32] Alex Nichol and Prafulla Dhariwal.\nImproved Denoising\nDiffusion Probabilistic Models. In ICML, 2021. 1\n[33] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. GLIDE: Towards Photorealistic Image Gener-\nation and Editing with Text-Guided Diffusion Models. In\nICML, 2022. 2\n[34] OpenAI. GPT-4 Technical Report, 2023. 4\n[35] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Jun-\ntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen,\nand Yujun Shen. CoDeF: Content Deformation Fields for\nTemporally Consistent Video Processing.\narXiv preprint\narXiv:2308.07926, 2023. 2\n[36] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. FateZero: Fus-\ning Attentions for Zero-shot Text-based Video Editing. In\nICCV, 2023. 1, 2\n[37] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood De-\nhghan, Osmar Zaiane, and Martin Jagersand. U2-Net: Going\nDeeper with Nested U-Structure for Salient Object Detec-\ntion. Pattern Recognition, 2020. 4\n[38] Zhaofan Qiu, Ting Yao, and Tao Mei.\nLearning Spatio-\nTemporal Representation with Pseudo-3D Residual Net-\nworks. In ICCV, 2017. 2\n[39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical Text-Conditional Image Gen-\neration with CLIP Latents. arXiv preprint arXiv:2204.06125,\n2022. 2\n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-Resolution Image\nSynthesis with Latent Diffusion Models. In CVPR, 2022. 1,\n2\n[41] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLaion-5B: An Open Large-Scale Dataset for\nTraining Next Generation Image-Text Models. In NeurIPS,\n2022. 5\n[42] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang gil\nLee, and Sungroh Yoon.\nEdit-A-Video:\nSingle Video\nEditing with Object-Aware Consistency.\narXiv preprint\narXiv:2303.07945, 2023. 2\n[43] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taig-\nman. Make-a-video: Text-to-Video Generation without Text-\nVideo Data. arXiv preprint arXiv:2209.14792, 2022. 1, 2,\n7\n[44] Jascha\nSohl-Dickstein,\nEric\nA.\nWeiss,\nNiru\nMah-\neswaranathan, and Surya Ganguli.\nDeep Unsupervised\nLearning using Nonequilibrium Thermodynamics. In ICML,\n2015. 1, 2\n[45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning Diffusion Implicit Models. In ICLR, 2021. 2\n[46] Yang Song and Stefano Ermon.\nGenerative Modeling by\nEstimating Gradients of the Data Distribution. In NeurIPS,\n2019. 1, 2\n[47] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRapha\u00a8el Marinier, Marcin Michalski, and Sylvain Gelly.\nFVD: A New Metric for Video Generation. In ICLR Work-\nshop, 2019. 5\n[48] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable Length Video Generation from Open Do-\nmain Textual Description. In ICLR, 2023. 2\n[49] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher\nPal. MCVD-Masked Conditional Video Diffusion for Pre-\ndiction, Generation, and Interpolation. In NeurIPS, 2022. 2\n[50] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye,\nYu Liu, and Hongsheng Li.\nGen-L-Video: Multi-Text to\nLong Video Generation via Temporal Co-Denoising. arXiv\npreprint arXiv:2305.18264, 2023. 2\n[51] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xi-\nang Wang, and Shiwei Zhang. ModelScope Text-to-Video\nTechnical Report. arXiv preprint arXiv:2308.06571, 2023. 7\n[52] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu. VideoFactory: Swap At-\ntention in Spatiotemporal Diffusions for Text-to-Video Gen-\neration. arXiv preprint arXiv:2305.10874, 2023. 5\n[53] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Ji-\nuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jin-\ngren Zhou. VideoComposer: Compositional Video Synthesis\nwith Motion Controllability. In NeurIPS, 2023. 2, 6, 7\n[54] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji,\nFan Yang, Guillermo Sapiro, and Nan Duan. GODIVA: Gen-\nerating Open-Domain Videos from Natural Descriptions.\narXiv preprint arXiv:2104.14806, 2021. 5\n[55] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-A-Video: One-Shot Tun-\ning of Image Diffusion Models for Text-to-Video Genera-\ntion. In ICCV, 2023. 1, 2\n[56] Jun Xu, Tao Mei, Ting Yao, and Yong Rui.\nMSR-VTT:\nA Large Video Description Dataset for Bridging Video and\nLanguage. In CVPR, 2016. 5\n[57] Ting Yao, Yingwei Pan, Yehao Li, Chong-Wah Ngo, and Tao\nMei. Wave-ViT: Unifying Wavelet and Transformers for Vi-\nsual Representation Learning. In ECCV, 2022. 2\n[58] Ting Yao, Yehao Li, Yingwei Pan, Yu Wang, Xiao-Ping\nZhang, and Tao Mei. Dual Vision Transformer. IEEE Trans.\non PAMI, 2023. 2\n[59] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained\nControl in Video Generation by Integrating Text, Image, and\nTrajectory. arXiv preprint arXiv:2308.08089, 2023. 2\n10\n[60] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang,\nXiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li,\nShuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan\nWang, Zicheng Liu, Houqiang Li, and Nan Duan. NUWA-\nXL: Diffusion over Diffusion for eXtremely Long Video\nGeneration. arXiv preprint arXiv:2303.12346, 2023. 2\n[61] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,\nXiao Xia, et al. GLM-130B: An Open Bilingual Pre-Trained\nModel. arXiv preprint arXiv:2210.02414, 2022. 3\n[62] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nConditional Control to Text-to-Image Diffusion Models. In\nICCV, 2023. 1, 2\n[63] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient Video\nGeneration with Latent Diffusion Models.\narXiv preprint\narXiv:2211.11018, 2022. 7\n11\n"
  },
  {
    "title": "A Comprehensive Study of Knowledge Editing for Large Language Models",
    "link": "https://arxiv.org/pdf/2401.01286.pdf",
    "upvote": "15",
    "text": "A Comprehensive Study of Knowledge Editing for\nLarge Language Models\nNingyu Zhang\u2217, Yunzhi Yao\u2217, Bozhong Tian\u2217, Peng Wang\u2217, Shumin Deng\u2217, Mengru Wang,\nZekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu,\nXin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang,\nZhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen\u2020\nZhejiang University, National University of Singapore,\nUniversity of California, Los Angeles, Ant Group, Alibaba Group\n{zhangningyu,yyztodd}@zju.edu.cn\nAbstract\nLarge Language Models (LLMs) have shown extraordinary capabilities in under-\nstanding and generating text that closely mirrors human communication. How-\never, a primary limitation lies in the significant computational demands during\ntraining, arising from their extensive parameterization. This challenge is further\nintensified by the dynamic nature of the world, necessitating frequent updates\nto LLMs to correct outdated information or integrate new knowledge, thereby\nensuring their continued relevance. Note that many applications demand con-\ntinual model adjustments post-training to address deficiencies or undesirable be-\nhaviors. There is an increasing interest in efficient, lightweight methods for on-\nthe-fly model modifications. To this end, recent years have seen a burgeoning in\nthe techniques of knowledge editing for LLMs, which aim to efficiently modify\nLLMs\u2019 behaviors within specific domains while preserving overall performance\nacross various inputs. In this paper, we first define the knowledge editing problem\nand then provide a comprehensive review of cutting-edge approaches. Drawing\ninspiration from educational and cognitive research theories [1\u20133], we propose\na unified categorization criterion that classifies knowledge editing methods into\nthree groups: resorting to external knowledge, merging knowledge into the model,\nand editing intrinsic knowledge. Furthermore, we introduce a new benchmark,\nKnowEdit, for a comprehensive empirical evaluation of representative knowledge\nediting approaches. Additionally, we provide an in-depth analysis of knowledge\nlocation, which can give a deeper understanding of the knowledge structures in-\nherent within LLMs. Initially conceived as a means to steer LLMs efficiently,\nwe hope that insights gained from knowledge editing research could shed light\non the underlying knowledge mechanisms of LLMs. To facilitate future research,\nwe have released an open-source framework, EasyEdit1, which will enable practi-\ntioners to efficiently and flexibly implement knowledge editing for LLMs. Finally,\nwe discuss several potential applications of knowledge editing, outlining its broad\nand impactful implications.\nKeywords\u2014 natural language processing, large language models, knowledge editing\n\u2217Equal Contribution.\n\u2020Corresponding Author.\n1https://github.com/zjunlp/EasyEdit.\nThe contributions of the authors are detailed in \u00a7CONTRIBUTIONS.\nPreprint. Ongoing work.\narXiv:2401.01286v3  [cs.CL]  9 Jan 2024\nContents\n1\nIntroduction\n3\n2\nBackground\n4\n2.1\nLarge Language Models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.1.1\nTransformers for LLM . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.1.2\nMechanism of Knowledge Storage in LLMs . . . . . . . . . . . . . . . . .\n5\n2.2\nRelated Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3\nKnowledge Editing for LLMs\n8\n3.1\nPreliminary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.2\nTask Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.3\nMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.3.1\nRecognition Phase: Resorting to External Knowledge . . . . . . . . . . . .\n10\n3.3.2\nAssociation Phase: Merge the Knowledge into the Model . . . . . . . . . .\n11\n3.3.3\nMastery Phase: Editing Intrinsic Knowledge\n. . . . . . . . . . . . . . . .\n11\n3.4\nNew Benchmark: KnowEdit . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.5\nEvaluation for Knowledge Editing . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n4\nExperiments\n15\n4.1\nExperiment Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n4.2\nMain Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n4.3\nImpact of Knowledge Editing on General Tasks . . . . . . . . . . . . . . . . . . .\n17\n4.4\nMulti-Task Knowledge Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4.5\nError and Case Analysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n5\nAnalysis\n21\n5.1\nComparison of Different Knowledge Editing Methods . . . . . . . . . . . . . . . .\n21\n5.2\nThe Effectiveness of Knowledge Locating in LLMs . . . . . . . . . . . . . . . . .\n22\n5.3\nThe Implicit Knowledge Structure in LLMs . . . . . . . . . . . . . . . . . . . . .\n24\n6\nApplications\n25\n6.1\nEfficient Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n6.2\nAI-Generated Content (AIGC) . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n6.3\nTrustworthy AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n6.4\nHuman-Computer Interaction: Personalized Agents . . . . . . . . . . . . . . . . .\n28\n7\nDiscussion and Conclusion\n29\nBroader Impacts\n29\n2\n1\nIntroduction\nKnowledge is a fundamental component of human intelligence and civilization [4]. Its systematic\nstructure empowers us to represent tangible entities or delineate principles through symbolic means,\noffering the capability to facilitate the articulation of intricate behaviors or tasks [5\u20137]. Throughout\nour lives, we humans continuously gather an extensive wealth of knowledge and learn to adaptively\napply it in various contexts. The enduring exploration of the nature of knowledge, and the processes\nby which we acquire, retain, and interpret it, continues to captivate scientists, which is not just a\ntechnical pursuit but a journey towards mirroring the nuanced complexities of human cognition,\ncommunication and intelligence [8\u201312].\nRecently, Large Language Models (LLMs) like GPT-4 [13] have showcased a remarkable ability in\nNatural Language Processing (NLP) to retain a vast amount of knowledge, arguably surpassing hu-\nman capacity [14\u201331]. This achievement can be attributed to the way LLMs process and compress\nhuge amount of data [32\u201335], potentially forming more concise, coherent, and interpretable models\nof the underlying generative processes, essentially creating a kind of \u201cworld model\u201d [36\u201338]. For\nexample, Dai et al. [39] have introduced the Knowledge Neuron (KN) thesis, which proposes that\nlanguage models function similarly to key-value memories. Here, the multi-layer perceptron (MLP)\nweights in the core region [40] may play a crucial role in recalling facts from the training corpus,\nsuggesting a more structured and retrievable form of knowledge storage within LLMs [41, 42].\nFurther insights come from the ability of LLMs to understand and manipulate complex strategic\nenvironments, whereas Li et al. [43] has demonstrated that transformers trained for next-token pre-\ndiction in board games such as Othello develop explicit representations of the game\u2019s state. Patel\nand Pavlick [44] have revealed that LLMs can track boolean states of subjects within given contexts\nand learn representations that reflect perceptual, symbolic concepts [36, 45\u201347]. This dual capability\nindicates that LLMs can serve as extensive knowledge bases [48\u201359], not only storing vast amounts\nof information but also structuring it in ways that may mirror human cognitive processes.\nHowever, LLMs have limitations like factual fallacy, potential generation of harmful content, and\noutdated knowledge due to their training cut-off [60\u201363]. Retraining to correct these issues is both\ncostly and time-consuming [64\u201368]. To address this, recent years have seen a surge in the develop-\nment of knowledge editing techniques specifically tailored for LLMs, which allows for cost-effective\npost-hoc modifications to models [69\u201371]. This technique focuses on specific areas for adjustment\nwithout compromising overall performance and can help understand how LLMs represent and pro-\ncess information, which is crucial for ensuring the fairness, and safety in Artificial Intelligence (AI)\napplications [72\u201376].\nThis paper first attempts to provide a comprehensive study of the development and recent advances\nin knowledge editing for LLMs. We first introduce the architecture of Transformers, mechanism of\nknowledge storage in LLMs (\u00a72.1), and related techniques including parameter-efficient fine-tuning,\nknowledge augmentation, continue learning and machine unlearning (\u00a72.2). Then we introduce\npreliminary (\u00a73.1), formally describe the knowledge editing problem (\u00a73.2), and propose a new\ntaxonomy (\u00a73.3) to provide a unified view on knowledge editing methods based on the educational\nand cognitive research theories [1\u20133]. Specifically, we categorize knowledge editing for LLMs into:\nresorting to external knowledge (\u00a73.3.1), merging knowledge into the model (\u00a73.3.2), and editing\nintrinsic knowledge (\u00a73.3.3 ) approaches. Our categorization criterion is summarized as follows:\n\u2022 Resorting to External Knowledge. This kind of approach is similar to the recognition\nphase in human cognitive processes, which needs to be exposed to the new knowledge\nwithin a relevant context, just as people first encounter new information. For example, pro-\nviding sentences that illustrate a factual update as a demonstration of the model, allowing\ninitial recognition of the knowledge to be edited.\n\u2022 Merging Knowledge into the Model. This kind of approach closely resembles the asso-\nciation phrase in human cognitive processes, in which connections are formed between the\nnew knowledge and existing knowledge in the model. Methods would combine or substi-\ntute the output or intermediate output with a learned knowledge representation.\n\u2022 Editing Intrinsic Knowledge. This approach to knowledge editing is akin to the mastery\nphase in human cognitive processes. It involves the model fully integrating knowledge into\nits parameters by modifying the weights and utilizing them reliably.\n3\nThis paper then involves extensive and comprehensive experiments conducted on 12 NLP datasets.\nThese are meticulously designed to evaluate the performance (\u00a74), usability, and underlying mech-\nanisms, complete with in-depth analyses (\u00a75), among other aspects. The key insights from our\nresearch are summarized as follows:\n\u2022 Performance. We construct a new benchmark, named KnowEdit, and report the empirical\nresults of cutting-edge knowledge editing approaches for LLMs, providing a fair com-\nparison and illustrating their overall performance in the settings of knowledge insertion,\nmodification, and erasure.\n\u2022 Usability. We illustrate the impact of knowledge editing on general tasks and multi-task\nknowledge editing, which implies that contemporary knowledge editing methods are effec-\ntive in executing factual updates with minimal disruptions to the model\u2019s cognitive capa-\nbilities and adaptability across diverse knowledge domains.\n\u2022 Mechanism. We observe a pronounced focus on one or several columns within the value\nlayer in edited LLMs. Furthermore, we find that the process of knowledge locating (e.g.,\ncasual analysis) tends to pinpoint only the areas related to the entity in question, rather\nthan the entire factual context, suggesting that LLMs might be deriving answers either\nby recalling information memorized from their pretraining corpus or through a multi-step\nreasoning process. Additionally, we delve into the possibility that knowledge editing for\nLLMs could lead to unintended consequences, an aspect warranting careful consideration.\nFinally, we delve into the multifaceted applications of knowledge editing, examining its potential\nfrom a variety of perspectives (\u00a76), including efficient machine learning, AI-Generated Content\n(AIGC), trustworthy AI, and human-computer interaction (personalized agents). Additionally, our\ndiscussion extends to the broader impacts of knowledge editing techniques, specifically focusing on\naspects such as energy consumption and interpretability (\u00a77). This paper aims to serve as a catalyst\nfor further research in the realm of LLMs, emphasizing efficiency and innovation. To support and\nencourage future research, we will make our tools, codes, data splits, and trained model checkpoints\npublicly accessible.\n2\nBackground\n2.1\nLarge Language Models\n2.1.1\nTransformers for LLM\nThe Transformer [77] model, a cornerstone in the design of modern state-of-the-art LLMs, repre-\nsents a significant shift from previous sequence learning methods. The original Transformer model is\nintroduced as an encoder-decoder framework, wherein both the encoder and decoder consist of a se-\nries of identical layers stacked upon each other. Each block within this architecture is equipped with\na self-attention module and a fully connected feed-forward neural network. Uniquely, the blocks in\nthe decoder also incorporate an additional cross-attention layer, positioned above the self-attention\nlayer, which is designed to effectively capture and integrate information from the encoder.\nSelf-Attention Module (SelfAttn)\nThe self-attention mechanism is a pivotal feature of the Trans-\nformer, allowing it to process sequences of data effectively. This module empowers each position\nwithin the encoder to attend to all positions in the preceding layer, thereby efficiently capturing con-\ntextual information embedded in the sequence. The mathematical representation of the self-attention\nmechanism is as follows:\nH = ATT(Q, K, V ) = Softmax\n\u0012QKT\n\u221adk\n\u0013\nV.\n(1)\nFeed-Forward Module (FFN)\nFollowing each attention layer in the Transformer is a fully con-\nnected Feed-Forward Neural network (FFN). This specific component of the architecture comprises\ntwo linear transformations, with a ReLU activation function intervening between them. The struc-\nture of the FFN can be succinctly described as follows:\n4\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\nBottom  Layers\nMiddle Layers\nTop Layers\nembed\nunembed\nself-Attention\nself-Attention\nself-Attention\nFFN\nFFN\nFFN\nhi\nhi\nhi\nk\nk\nk\nv\nv\nv\nRelation Mapping \n(Rome, is-capital-of \u27a1  Italy) \nHernandez et al. 2023\nShallow Pattern \nGeva et al. \n2021\nEntity \nInfo. \nGeva et al. \n2023\nAttribute \nExtraction \nGeva et al.2023\nSemantic knowledge \nGeva et al. 2021 \nFact Knowledge \nDai et al. 2022\nPhrase-level information  \nSurface linguistic feature \nJawahar et al. 2019\nLong distance information \nSemantic linguistic feature \nJawahar et al. 2019\nSyntactic linguistic feature \nJawahar et al. 2019\nFigure 1: The mechanism of knowledge storage in LLMs. Here, we summarize the findings of\ncurrent works, including: Jawahar et al. [78], Geva et al. [41], Dai et al. [39], Meng et al. [79], and\nHernandez et al. [80].\nFFN(x) = ReLU(x \u00b7 W1 + b1) \u00b7 W2 + b2,\n(2)\nSince its inception, the Transformer model has revolutionized the field of NLP. Its adaptable and\nefficient architecture has facilitated advancements in various NLP tasks, such as question-answering,\ntext summarization, and machine translation systems. The model\u2019s influence extends beyond NLP,\nimpacting other areas of machine learning and setting a new standard for building complex and\neffective neural network architectures.\n2.1.2\nMechanism of Knowledge Storage in LLMs\nThe Transformer\u2019s remarkable performance is partly attributed to its ability to store a wealth of\ninformation within its parameters, encompassing linguistic [81], commonsense [82\u201384], arithmetic,\nand world knowledge [48, 85\u201387]. However, the exact manner in which this knowledge is organized\nwithin LLMs is still largely enigmatic. Current research efforts are dedicated to unraveling the\nmechanistic explanations of LLMs\u2019 behaviours [88\u201392], especially the complexities of knowledge\nstorage in LLMs, with Figure 1 illustrating some of these research findings.\nA key area of inquiry is pinpointing the specific location of knowledge within the model. Jawahar\net al. [78] dissects the intricacies of the English language structure as comprehended by BERT [93].\nTheir findings reveal that BERT\u2019s phrasal representations capture phrase-level information predomi-\nnantly in the lower layers, and encode an intricate hierarchy of linguistic elements in the intermediate\nlayers. This hierarchy is characterized by surface features at the foundational level, and syntactic\nfeatures in the central layers, and culminates with semantic features at the uppermost level. Geva\net al. [41] proposes that the FFN layers in a Transformer model function akin to key-value memories.\nThey suggest that the FFN input operates as a query, with the first layer representing keys and the sec-\nond layer corresponding to values. They find that human-interpretable shallow input patterns trigger\neach key neuron, and the corresponding value neurons store the next-token output probability. As a\nresult, the final output of the FFN can be understood as the weighted sum of activated values. Fur-\nthermore, they demonstrate that value vectors often embody interpretable concepts and knowledge,\nwhich can be intensified or attenuated through specific manipulations [42]. Building on this, Dai\net al. [39] introduces the concept of \u201cKnowledge Neurons\u201d, suggesting that knowledge is localized\nwithin a small subset of FFN neurons in the uppermost layers of the language model. These neurons\nare identified through the analysis of integrated gradients across various prompts [94\u201396]. Similarly,\nMeng et al. [79] employs a method known as \u201ccausal tracing\u201d to assess the indirect influences of hid-\nden states or activations, revealing that factual knowledge predominantly resides in the early-layer\nFFNs of such models. Additionaly, Chen et al. [97] makes an intriguing finding that the language\nmodel contains language-independent neurons that express multilingual knowledge and degener-\nate neurons that convey redundant information by applying the integrated gradients method [94].\nConcurrently, Zhao et al. [98] observes that LLMs appear to possess a specialized linguistic region\n5\nresponsible for processing multiple languages. Gueta et al. [99] suggests that knowledge is a region\nin weight space for fine-tuned language models. They find that after finetuning a pretrained model\non similar datasets, the resulting models are close to each other in weight space. Recent interests\nalso revolve around dissecting the distinct functionalities of individual neurons within LLMs [100].\nYet, it is crucial to note that some researchers caution against overinterpreting these findings, empha-\nsizing that models illustrate correlations rather than explicit mechanisms. For instance, Anonymous\n[101] argues that while MLP neurons may exhibit patterns interpretable through a linguistic lens,\nthey do not necessarily \u201cstore\u201d knowledge in a conventional sense, whether linguistic or factual.\nThus, the question of how Transformer LLMs retrieve and utilize this stored knowledge remains\nopen, and some work has begun to unveil this mystery. Geva et al. [102] analyzes the information\nflow in the model and finds the self-attention model conducts attribute extraction during computing\ninspired by the circuit theory [103, 104]. Foote et al. [105] proposes Neuron to Graph (N2G), an\ninnovative tool that automatically extracts a neuron\u2019s behavior from the dataset it was trained on\nand translates it into an interpretable graph. Further, Hernandez et al. [80] conceptualizes relational\nknowledge within Transformers as a linear affine function, mapping subjects to objects. As to other\nknowledge, Gurnee and Tegmark [36] discovers that LLMs learn linear representations of space\nand time across multiple scales and identify individual \u201cspace neurons\u201d and \u201ctime neurons\u201d that\nreliably encode spatial and temporal coordinates. However, it is imperative to acknowledge that\nthese studies predominantly concentrate on the representation of individual knowledge facts. The\nbroader challenge lies in comprehensively understanding how various strands of knowledge are\nintricately organized and interconnected within these complex models [106, 107].\n2.2\nRelated Techniques\nParameter-efficient Fine-tuning\nFine-tuning all parameters of LLMs can be computationally ex-\npensive. To enable efficient adaptation, parameter-efficient tuning (PET) [108, 109] techniques\nhave been proposed to match full fine-tuning performance while only updating a minimal pa-\nrameters. PET consists of three distinct paradigms: addition-based, specification-based, and re-\nparameterization-based methods. In addition-based methods, extra trainable neural modules or pa-\nrameters, which are not present in the original model or process, are introduced. A prime example\nof this is Adapter, as discussed in Houlsby et al. [110]. On the other hand, specification-based meth-\nods involve fine-tuning a select number of parameters, while keeping the majority of the model\u2019s\nparameters unchanged. A notable method in this category is LoRA, as detailed in Hu et al. [111].\nBy fine-tuning a small number of parameters, PET methods aim to maximize model performance\nwhile reducing required resources and tuning time. PET techniques hold promise since knowledge\nediting seeks to efficiently modify model behavior. However, PET is typically applied to enhance\ntask performance rather than edit knowledge specifically. The efficacy of existing PET methods for\nknowledge editing remains largely unexplored. Investigating how to leverage PET for efficient and\nprecise knowledge updates presents an interesting direction for future work.\nKnowledge Augmentation for LLMs\nLLMs still face unknown questions, and many knowledge-\naugmented methods are proposed to help the model deal with this task [112\u2013114]. The most popular\nway is the retrieval-augmented methods [115\u2013117]. With the help of the retrieved knowledge or\ncontext that is related to the input, the model can give the desired output. The integration of the\nretrieved information includes both the input, intermediate, and output layers [118]. During the\ninput phase, retrieved texts are concatenated with the original input text [119\u2013121]. In some works,\nthe retrieved components are latent and integrated into the intermediate layers of Transformers [122\u2013\n124]. In the output phase, the distribution of tokens from the retrieved components and the LLMs\nare interpolated [125\u2013128].\nThe knowledge-augmented method is a great solution for the missing or misinformation in LLMs\nbut it still has some disadvantages. As a temporary solution, retrieval methods suffer from poor\nretrieval results and relatedness [129, 130]. The data retrieved often contains some noise, such as\nadditional content that is irrelevant to a question but that may be relevant to a different question (i.e.,\nnot necessarily random noise) [131]. In these situations, the model fails to distinguish the knowledge\nthat is necessary to answer the question, leading to spurious reasoning and degraded performance.\nMeanwhile, retrieval typically operates at a broader level of relevant passages without fine-grained\ncontrol over precisely which information is modified within the model.\n6\nFewer Params\nPrecise Control\nSupport Phenomena\nFinetune\n\u2717\n\u2717\n+\nParameter-efficient Fine-Tuning\n\u2714\n\u2717\n+\nKnowledge Augmentation\n\u25cb\n\u2717\n+\nContinual Learning\n\u2717\n\u2717\n+\nModel Unlearning\n\u25cb\n\u2717\n\u2212\nKnowledge Editing\n\u2714\n\u2714\n+ \u2212\nTable 1: Integrated comparison between knowledge editing and related techniques. The symbol\n\u2714 denotes the presence of a particular feature in the technique, while \u2717 signifies its absence.\n+ indicates an enhancement of the LLMs\u2019 capabilities, whereas \u2212 signifies a reduction or removal\nof certain abilities within the model.\nContinual Learning\nContinual learning (CL), also known as lifelong machine learning or in-\ncremental learning, refers to the ability of machine learning models to continuously acquire new\nskills and learn new tasks while retaining previously learned knowledge [132\u2013135]. This is akin to\nhow humans learn throughout their lifetimes by continually accumulating new information and skills\nwithout forgetting the old ones. Conventional machine learning models struggle with this as they are\ntrained on independent and identically distributed data. When the distribution shifts or new tasks are\nencountered, their performance significantly degrades on older tasks due to catastrophic forgetting.\nSome key techniques being explored include replay-based methods [136, 137], regularization-based\napproaches [138, 139], and dynamic architecture methods [140, 141]. Continual learning focuses\non allowing machine learning models to learn new tasks and adapt to new domains over time with-\nout forgetting earlier ones, which resembles the goal of knowledge editing. In contrast, knowledge\nediting focuses specifically on manipulating and updating the internal knowledge representations\nlearned by pre-trained language models without regard to the underlying tasks or domains. The\ngoal of knowledge editing is to dynamically refine language understanding independent of eventual\napplications, addressing the \u201cfixedness\u201d issue of pre-trained language models once deployed. Both\nareas are important for developing AI systems that can progressively acquire and flexibly apply\nknowledge throughout their lifetime.\nMachine Unlearning\nIn addition, it is crucial for models to be capable of discarding undesirable\n(mis)behaviors, which aligns with the concept of machine unlearning [142\u2013146]. Chen and Yang\n[147] proposes an efficient unlearning framework EUL that can efficiently update LLMs without\nhaving to retrain the whole model after data removals, by introducing lightweight unlearning layers\nlearned with a selective teacher-student objective into the Transformers. However, knowledge edit-\ning goes beyond unlearning by actively refining or erasing a model\u2019s learned knowledge base. Both\nmachine unlearning and knowledge editing play important roles in enhancing reliability, fairness\nand effectiveness for LLMs across different domains and applications.\nTo conclude, the traditional approach to leveraging pre-trained language models involves fine-tuning\nthem with target-specific data. However, in the realm of LLMs, this fine-tuning process encounters\nsignificant challenges. These include the vast number of parameters, substantial time and memory\nrequirements, risks of overfitting, and issues like catastrophic forgetting. To address these chal-\nlenges, several techniques have been developed, as we discussed above. Among these, knowledge\nediting emerges as a notable strategy. As we discussed in Table 1, knowledge editing, intersecting\nwith these techniques, draws inspiration from a range of methodologies, showing promising results.\nThis approach distinctively targets the knowledge embedded within LLMs, leveraging the inherent\nknowledge mechanisms of these models. Unlike simple adaptations of existing methods, knowledge\nediting necessitates a deeper comprehension of how LLMs function. It is not just about applying\nknown techniques to new models; it is about understanding and manipulating the nuanced knowl-\nedge storage and processing capabilities of LLMs. Furthermore, knowledge editing represents a\nmore precise and granular form of model manipulation as it involves selectively altering or enhanc-\ning specific aspects of a model\u2019s knowledge base, rather than broadly retraining or fine-tuning the\nentire model. These characteristics make knowledge editing a potentially more efficient and effec-\ntive way to update and optimize LLMs for specific tasks or applications.\n7\n3\nKnowledge Editing for LLMs\n3.1\nPreliminary\nThe substantial training on diverse datasets has equipped LLMs with a wealth of factual and com-\nmonsense information, positioning these models as virtual knowledge stores [48, 148, 149]. This\nrich knowledge base has been effectively utilized in various downstream tasks, as evidenced by nu-\nmerous studies [150]. Additionally, Wang et al. [151] have demonstrated the potential of LLMs in\nautonomously constructing high-quality knowledge graphs, bypassing the need for human super-\nvision. Despite their promise, LLMs, in their current state as emerging knowledge bases, exhibit\ncertain limitations. These deficiencies often manifest as inaccuracies or errors in their outputs dur-\ning practical applications. An ideal knowledge base would not only store extensive information but\nalso allow for efficient and targeted updates to rectify these errors and improve their accuracy. Rec-\nognizing this gap, our paper introduces the concept of knowledge editing for LLMs. This approach\nis designed to enable quick and precise modifications to the LLMs, allowing them to generate more\naccurate and relevant outputs. By implementing knowledge editing for LLMs, we aim to enhance\nthe utility of LLMs, moving them closer to the ideal of becoming universally reliable and adapt-\nable repositories of knowledge. This advancement promises to address the current shortcomings of\nLLMs and unlock their full potential as dynamic and accurate knowledge bases for applications.\n3.2\nTask Definition\nThe initial goal of knowledge editing is to modify the specific knowledge k in the LLM and im-\nprove the consistency and performance of the LLM without fine-tuning the whole model. This\nknowledge can be associated with many areas and types, such as facts [79], commonsense [152],\nsentiment [153] and so on. Knowledge editing is challenging due to the distributed and entangled\nnature of knowledge in LLMs.\nSuppose the original model is \u03b8 and given the knowledge k to be changed, by knowledge editing\nprocess F, we would get the post-edited model \u03b8\n\u2032:\n\u03b8\u2032 = F(\u03b8, k)\n(3)\nThe post-edited model \u03b8\n\u2032 is supposed to override undesired model beliefs on the knowledge k and\nkeep other knowledge intact:\n(\n\u03b8\n\u2032(k) \u0338= \u03b8(k)\n\u2200k\n\u2032 \u0338= k, \u03b8\n\u2032(k\n\u2032) = \u03b8(k\n\u2032)\n(4)\nAs a knowledge base, it\u2019s paramount that knowledge editing cater to three fundamental settings:\nknowledge insertion, knowledge modification, and knowledge erasure.\nKnowledge Insertion.\nAs fields and entities progress, it becomes imperative for LLMs to as-\nsimilate emergent information. Knowledge insertion fulfills this by bestowing upon LLMs new\nknowledge previously outside their purview:\n\u03b8\u2032 = F(\u03b8, {\u2205} \u2192 {k})\n(5)\nKnowledge Modification.\nKnowledge modification refers to altering knowledge already stored in\nLLMs:\n\u03b8\u2032 = F(\u03b8, {k} \u2192 {k\u2032})\n(6)\nThis can be classified into two categories:\n\u2022 Knowledge amendment - This aims at rectifying the inaccuracies embedded in LLMs to\nensure the delivery of accurate information. As vast repositories of knowledge, LLMs are\nprone to housing outdated or erroneous information. Knowledge amendment serves to cor-\nrect these fallacies, ensuring that models always generate accurate, up-to-date information.\n\u2022 Knowledge disruption - Modifying LLMs to answer counterfactual or error prompts. This\nis more challenging as counterfactual notions initially receive lower scores compared to fac-\ntual knowledge, as shown by Meng et al. [79]. This necessitates more targeted modification\nefforts.\n8\nRecognition\nAssociation\nMastery\nHuman\nMachine\nRecognition\nAssociation\nMastery\nprompt\nFigure 2: Applying Human Learning Phases [1\u20133] to Knowledge Editing in LLMs: We see an anal-\nogy of Human Learning Phases and Knowledge Editing in LLMs and categorize current knowledge\nediting methods based on the learning phases of humans: recognition, association, and mastery.\nKnowledge Erasure.\nKnowledge erasure targets the excision or obliteration of pre-existing\nknowledge in a model, primarily to reset distinct facts, relationships, or attributes. Formally, we\nhave:\n\u03b8\u2032 = F(\u03b8, {k} \u2192 {\u2205})\n(7)\nImplementing knowledge erasure is pivotal to expunge biases and noxious knowledge and to curtail\nthe recollection of confidential or private data, thereby fostering responsible and trustworthy AI.\nIn conclusion, the interplay between knowledge insertion, modification, and erasure forms essential\naspects of model editing techniques. When combined, these techniques empower LLMs to trans-\nform, self-correct, and ethically adapt as needed.\n3.3\nMethods\nThe development of LLMs has reached a point where their capabilities closely resemble human\ncognitive processes, especially in learning and acquiring knowledge. Drawing inspiration from\nhow humans learn, we can analogously apply these concepts to the process of editing LLMs as\nFigure 2 shows. Educational and cognitive research [1\u20133] delineates human knowledge acquisition\ninto three distinct phases: recognition, association, and mastery. These phases offer a framework for\nconceptualizing the methods of knowledge editing in LLMs2 and we list them in Table 2.\n\u2022 Recognition Phase: In the recognition phase, the model needs to be exposed to the\nnew knowledge within a relevant context, just as people first encounter new informa-\ntion (\u00a73.3.1). For example, providing sentences that illustrate a factual update as a demon-\nstration of the model allows initial recognition of the knowledge to be edited.\n\u2022 Association Phase: In the association stage, connections are formed between the new\nknowledge and existing knowledge in the model (\u00a73.3.2), much like humans relate new\nideas to prior concepts. Methods would combine or substitute the output or intermediate\noutput h with a learned knowledge representation hknow.\n\u2022 Mastery Phase: The mastery phase involves the model fully acquiring the knowledge in\ntheir parameters and utilizing it reliably (\u00a73.3.3), akin to deep human mastery. This method\ndirectly changed the model\u2019s weight, \u2206W , and the model can deal with the problem with-\nout any external help or merge.\n2https://github.com/zjunlp/KnowledgeEditingPapers.\n9\nCategory\nMethod\nEdit Area\nEdit Function\nNo\nTraining\nBatch\nEdit\nEdited\n#Params\nMemPrompt [154]\nmemory+retriever\nInput \u2192 [Mem : Input]\n\u2714\n\u2714\n\u2013\nPhase\nSERAC [153]\nmemory+classifier\n+auxiliary model\nOutput \u2192 Modelcf (x)\n\u2717\n\u2714\n\u2013\nMeLLo [155]\nmemory+retriever\nInput \u2192 [Mem : Input]\n\u2714\n\u2717\n\u2013\nIKE [156]\nmemory+retriever\nInput \u2192 [Mem : Input]\n\u2714\n\u2717\n\u2013\nICE [157]\nprompt\nInput \u2192 [Mem : Input]\n\u2714\n\u2717\n\u2013\nPokeMQA [158]\nmemory+retriever\nInput \u2192 [Mem : Input]\n\u2717\n\u2717\n\u2013\nLanguage Patches[159]\nOutput head\n+ params\nh \u2192 \u03bbh+\n(1 \u2212 \u03bb)Patch(x)\n\u2714\n\u2714\ndh \u00d7 #Output\nPhase\nCaliNET [160]\nFFN+params\nh \u2192 h + FFNadd(x)\n\u2717\n\u2714\nN \u00d7 dh\nT-Patcher[161]\nFFN+params\nh \u2192 h + FFNadd(x)\n\u2717\n\u2717\nN \u00d7 dh\nREMEDI [162]\nauxiliary model\nh \u2192 REMEDI(x)\n\u2717\n\u2717\ndh \u00d7 dh\nGRACE [163]\nFFN+codebook\nh \u2192 GRACE(x)\n\u2717\n\u2717\nN \u00d7 2dh\nLoRA [164]\nAttn or FFN\nh \u2192 h + s \u00b7 LoRA(x)\n\u2717\n\u2714\n2L \u00d7 2damdh\nMELO [165]\nAttn or FFN\nh \u2192 h + s \u00b7 LoRA(x)\n\u2717\n\u2717\n2L \u00d7 2damdh\nFT-Constrained [166]\nAny\nW \u2192 W\n\u2032\n\u2717\n\u2714\n2 \u00d7 L \u00d7 dmdh\nPhase\nENN [167]\nAny\nW \u2192 W\n\u2032\n\u2717\n\u2714\n2 \u00d7 L \u00d7 dmdh\nKE[168]\nAttn or FFN\n+auxiliary model\nW \u2192 W\n\u2032\n\u2717\n\u2714\n2 \u00d7 L \u00d7 dmdh\nSLAG [169]\nAttn or FFN\n+auxiliary model\nW \u2192 W\n\u2032\n\u2717\n\u2714\n2 \u00d7 L \u00d7 dmdh\nMEND [170]\nFFN+\nauxiliary model\nW \u2192 W\n\u2032\n\u2717\n\u2714\n2 \u00d7 L \u00d7 dmdh\nKN [39]\nFFN\nW down \u2192 W\n\u2032\ndown\n\u2714\n\u2717\nL \u00d7 N \u00d7 dh\nROME [79]\nFFN\nW down \u2192 W\n\u2032\ndown\n\u2714\n\u2717\ndmdh\nMEMIT [171]\nFFN\nW down \u2192 W\n\u2032\ndown\n\u2714\n\u2714\nL \u00d7 dmdh\nPMET [172]\nFFN\nW down \u2192 W\n\u2032\ndown\n\u2714\n\u2714\nL \u00d7 dmdh\nMALMEN [173]\nFFN\nW down \u2192 W\n\u2032\ndown\n\u2717\n\u2714\nL \u00d7 dmdh\nBIRD [174]\nFFN\nW down \u2192 W\n\u2032\ndown\n\u2714\n\u2717\ndmdh\nAssociation\nRecogintion\nMastery\nTable 2: Comparison between representative approaches of knowledge editing for LLMs. No Train-\ning refers to the methods that do not require additional training; Batch Edit means whether the\nmethods can support editing multiple cases simultaneously in just one process. Edit Area refers to\nwhere the model\u2019s components are used; Editor #Params indicates the parameters that need to be\nupdated for editing. L refers to the number of layers to update. dh denotes the dimensionality of\nthe hidden layers in the Transformers. dm refers to the intermediate dimension that exists between\nthe up projection and the down projection. N symbolizes the total number of neurons that undergo\nupdates within each individual layer.\n3.3.1\nRecognition Phase: Resorting to External Knowledge\nWhen humans encounter new information, we do not always master it immediately. Instead, with the\nright context and examples, we can process and reason through this new knowledge. LLMs exhibit\na similar capacity for in-context learning. This kind of method usually maintains a memory M and\nretrieves the most relevant cases for each input. IKE [156] exemplifies this approach by constructing\nthree types of demonstrations \u2013 copy, update, and retain \u2013 to aid the model in producing reliable fact\nediting. It utilizes a demonstration store, formed from training sets, to guide the model towards\ngenerating the appropriate answer by retrieving the most pertinent demonstrations. Meanwhile, as\na simple change in knowledge would lead to ripple effects [157], MeLLo [155] decomposes the\nquestion into different sub-questions for tackling multi-hop questions and retrieves the updated fact\nfrom the memory for each sub-question. Building on this, PokeMQA [158] offers a more robust\nmethod for question decomposition, introducing a programmable scope detector and knowledge\nprompts for enhanced reliability.\nHumans also often utilize tools to augment their learning and problem-solving abilities. Likely,\nSERAC [153] builds a new counterfact model by retaining the new model and adopting a classifier\nto determine whether to use the counterfact model to answer the question. This method is straight-\nforward and practically applicable, requiring no alterations to the original model. It\u2019s particularly\nadvantageous for real-world use, given its ease of implementation. However, it\u2019s important to note\nthat this approach can be vulnerable to issues such as retrieval errors (e.g.noise [175], harmful con-\ntent [176]) and knowledge conflict problems [177, 178]. Recently, Yu et al. [179] investigats various\nscenarios in which language models opt for either the in-context answer or the memorized answer.\n10\nThis research sheds light on the potential application of the method mentioned earlier, as it may\noffer insights into when and how to utilize it.\n3.3.2\nAssociation Phase: Merge the Knowledge into the Model\nUnlike the recognition phase, this kind of method learns a representation for the new knowledge\nhKnow and merges this information with the original model\u2019s representation h.\nMurty et al. [159] proposes a knowledge patch as a new output head and interpolates the new head\nwith the original head. Specially, inspired by previous findings that FFN may store knowledge,\nseveral methods integrate the knowledge into the FFN part. These methods add the neuron to the\nFFN and after the edit, the output is a combination of the previous FFN\u2019s output and the newly added\nknowledge:\nFFN\n\u2032(x) = FFN(x) + \u25b3FFN(x),\n(8)\nIn particular, T-Patcher [161] adds one neuron for each output error, while CaliNet [160] adds the\nknowledge via a fixed number of neurons. Meanwhile, Wu et al. [164] adopts LoRA to conduct\nknowledge edits. LoRA is a parameter-efficient fine-tuning method that freezes the weights of the\nLLM and introduces trainable rank decomposition matrices into the Transformer layers during the\nfine-tuning process. Hence, the hKnow is xW down W up . Based on this, MELO [165] suggests a\nplug-in model editing method that uses dynamic LoRA to change the way language models work\nby indexing LoRA blocks dynamically based on an internal vector database. Instead of adding\nparameters to the model, REMEDI [162] directly substitutes the representation of the entity hentity\nby incorporating an attribute vector hattr into its original model\u2019s representation. Specifically, it\nlearns the updated hidden states using an affine transformation hentity + Whattr + b and replaces\nthe LM\u2019s entity representation with it. In contrast, GRACE [163] adopts a unique approach by\nmaintaining a discrete codebook that functions as an Adapter. This codebook is dynamically updated\nover time, allowing for the modification and refinement of a model\u2019s predictions. When the model\nencounters the knowledge for editing, it searches the codebook and replaces the hidden states as\nthe value in the codebook. Overall, we can use a mathematical formula to represent these methods\nuniformly:\nhfinal = h + hknow\n(9)\nThis kind of method merged the information with the original model, making the weighting of\nknowledge from different sources a crucial parameter to consider. Given that these information\nsources often differ and may even conflict, the issue of knowledge conflict, as highlighted in Wang\net al. [177], remains a significant challenge. To address this issue, F-Learning [180] introduces a\n\u201cforgetting before learning\u201d paradigm to achieve forgetting of old knowledge and learning of new\nknowledge based on parametric arithmetic. Additionally, determining the optimal point of integra-\ntion for this information within the model is a critical aspect of this method. It is not just about\nmerging the information, but also about where in the model\u2019s structure this integration occurs for\nmaximum effectiveness and minimal disruption. Furthermore, the capacity of the model\u2019s param-\neters to store this integrated information is an area that still requires exploration. If every piece of\nedited knowledge necessitates additional parameters, the model\u2019s parameter could increase signifi-\ncantly with each edit. This raises concerns about scalability and efficiency, as continuously expand-\ning the number of parameters might lead to issues like increased computational requirements.\n3.3.3\nMastery Phase: Editing Intrinsic Knowledge\nDespite the success of the previous two kinds of methods, we still confront how the model stores\nthe knowledge and how they utilize and express the knowledge. Here, we come to the most impor-\ntant part of knowledge editing: the mastery stage. In this part, the model is required to learn the\nknowledge of its own parameters and master the knowledge by itself. Fine-tuning the model is the\ndirect way to update the knowledge; however, training the whole model requires enormous compu-\ntational resources and is time-consuming. Meanwhile, the finetuning technique usually suffers from\ncatastrophic forgetting and overfitting. Constrained Fintune [166] utilizes a regularization to help\nthe model keep the unrelated knowledge. Currently, many researchers endeavor to use knowledge-\nspecific methods to modify the \u2206W . These methods can be classified into two categories: meta-\nlearning and locate-and-edit.\n11\nMeta Learning\nTo overcome these drawbacks, some meta-learning methods are proposed to edit\nthe model. Instead of updating the weights directly, this kind of method teaches a hypernetwork to\nlearn the change \u2206W of the model. KE [168] directly uses the representation of the new knowledge\nto train the model to update the matrix. SLAG [169] introduces a new training objective considering\nsequential, local, and generalizing model updates. The \u2206W in these methods has the same dimen-\nsions as the model\u2019s matrix. In order to overcome it, MEND [170] applies the rank-one decomposi-\ntion to divide the model into two rank-one matrices, from which it is possible to compute the \u2206W ,\nsignificantly reducing the number of parameters. While these methods have shown some promising\nresults, they fail on multi-edits as they ignore the conflicts between these edits. Han et al. [181]\nproposes a novel framework to divide-and-conquer edits with parallel editors. Specifically, they\ndesign explicit multi-editor MoEditor and implicit multi-editor ProEditor to learn diverse editing\nstrategies in terms of dynamic structure and dynamic parameters, respectively, which allows solving\nthe conflict data in an efficient, end-to-end manner. Also, MALMEN [173] improves MEND by\nformulating the parameter shift aggregation as a least squares problem and supports massive editing\nsimultaneously.\nLocation-then-Edit\nDespite the effectiveness of previous work, how the LLMs store this knowl-\nedge is still unknown. Some work [41, 42, 97], has learned the mechanism of LLMs knowledge\nand found that the knowledge was stored in the FFN . Based on these works, some conduct knowl-\nedge editing by first locating where the knowledge was stored and then editing the specific area.\nKnowledge Neuron [39] proposed a knowledge attribution method by computing the sensitivity of\nthe gradient change. They then directly modify the corresponding value slots using the embedding\nof the target knowledge. ROME [79] and MEMIT [171] employ a casual analysis method to detect\nwhich part of hidden states plays more importance. They view the editing as a minimum opti-\nmization and edit the weights. Despite the effectiveness of editing the FFN area, PMET [172] also\nconducts editing via the attention head and demonstrates a better performance. BIRD [174] pro-\nposes bidirectionally inverse relationship modeling. They designed a set of editing objectives that\nincorporate bidirectional relationships between subject and object into the updated model weights\nand demonstrate the effectiveness of alleviating the reverse curse [182] of the knowledge learning.\nThis kind of method, which directly edits a model\u2019s parameters, offers a more permanent solution\nfor altering its behavior. The changes are embedded into the model\u2019s structure, so they cannot be\ncircumvented even if a user has access to the model\u2019s weights. This ensures lasting and reliable\nmodifications. However, the side effects are not under control since the mechanism of LLMs is\nunclear. Some researchers are skeptical about this kind of method [183], so it is still a premature\nresearch area that requires further investigation.\n3.4\nNew Benchmark: KnowEdit\nTo evaluate the effectiveness of knowledge editing methods, several datasets have been proposed. In\nthis Section, we present an overview of the current datasets used for knowledge editing and introduce\na new benchmark, KnowEdit3, which serves as a comprehensive evaluation framework for various\nknowledge editing techniques.\nTask\nKnowledge Insertion\nKnowledge Modification\nKnowledge Erasure\nDatasets\nWikiDatarecent\nZsRE\nWikiBio\nWikiDatacounterfact\nConvsent\nSanitation\nType\nFact\nQuestion Answering\nHallucination\nCounterfact\nSentiment\nUnwanted Info\n# Train\n570\n10,000\n592\n1,455\n14,390\n80\n# Test\n1,266\n1230\n1,392\n885\n800\n80\nTable 3:\nStatistics on the benchmark KnowEdit, with six selected datasets for the evaluation of\nknowledge editing methods. We select different knowledge types for the insertion, modification,\nand erasure settings.\nFor this study, we have curated a set of six datasets that are well-suited for assessing knowledge\nediting methods. A detailed statistical overview of these datasets is presented in Table 3, and they\nencompass a range of editing types, including fact manipulation, sentiment modification, and hallu-\ncination generation.\n3https://huggingface.co/datasets/zjunlp/KnowEdit.\n12\nFocusing on the task of knowledge insertion, we have adopted the dataset, WikiDatarecent [157]:\n\u2022 WikiDatarecent This dataset specifically focuses on triplets that have been recently inserted\ninto WIKIDATA after July 2022. Consequently, this dataset enables us to create insertion\nedit requests for models that were trained prior to the introduction of these facts, thereby\nsimulating scenarios where an outdated model meets the new world knowledge. We utilize\nthe original datasets provided by the authors and split them into training and testing sets.\nFor knowledge modification, we have selected the following four datasets: ZsRE [184], Wik-\niBio [163], Wikidatarecent [157], and Convsent [153].\n\u2022 ZsRE is a context-free question-answering task. Given a question based on the subject and\nrelation, the model is expected to provide the correct object as the answer. We adopt the\nextended version of ZsRE proposed by Yao et al. [69], which introduces a portability test\nfor the original dataset. Additionally, we collect new locality sets following the procedure\noutlined in Yao et al. [69], as the original dataset computes locality using Natural Question\nannotations.\n\u2022 WikiBio The original dataset was created by prompting GPT-3 to generate 238 Wikipedia-\nstyle biographies using subjects from the WikiBio dataset [185]. Hartvigsen et al. [163]\nutilize this dataset and introduce a new editing task focused on correcting hallucinations\nin GPT language models. They annotate the factual accuracy of each sentence, identifying\nthe ones that contain hallucinations. We follow their approach by editing inaccurate sen-\ntences and replacing them with corresponding sentences from the true Wikipedia entries.\nWe adhere to the original setting of this dataset and construct the locality set by linking\nconcepts via the Wikidata API to traverse all relations of the concept and randomly select\nan unrelated relationship and tail entity.\n\u2022 WikiDatacounterfact Since tail entities are often not captured by models, and therefore\nare not suitable for testing modification edits [186], [157] collect triplets about popular\nentities, where the subject corresponds to one of the top-viewed pages in Wikipedia. They\nalso collect a dataset by random sampling entities from Wikidata, and we use it as the\ntraining set and the WikiDatacounterfact as the test set.\n\u2022 ConvSent is a sentiment editing task that assesses the model\u2019s ability to modify a dialog\nagent\u2019s sentiment on a specific topic without affecting its responses to other topics. For\nexample, given the topic \u2018What do you think of bananas?\u2019, we wish the post-edited model\nto give the corresponding sentiment for \u2018bananas\u2019 including positive and negative. The\nlocality sets consist of examples generated from entities other than the one used for editing.\nWe also adopt the original setting of the ConvSent dataset.\nIn the context of knowledge erasure settings, we have selected the Sanitation [187] dataset.\n\u2022 Sanitation This dataset specifically addresses privacy concerns associated with learned\nlanguage models. It focuses on the task of forgetting specific information stored in the\nmodel. The dataset provides pairs of questions and answers, where the answers contain\nknowledge that needs to be forgotten (e.g., \u201c1234 Oak Street\u201d), and the questions prompt\nthe model to generate the corresponding answers (e.g., \u201cWhat is John Smith\u2019s address?\u201d).\nThe goal is for the post-edited model to effectively forget the target answer and generate\npredefined safe token sequences, such as \u201cI don\u2019t know,\u201d in response to prompts seeking\nspecific or sensitive information. This mechanism helps prevent information leakage. The\ndataset consists of a forgot set and a retain set. We utilize the forget set to evaluate the\nsuccess of the model\u2019s editing process and the retain set to assess the locality of the modifi-\ncations. Furthermore, we maintain the original task settings by sampling the same number\nof data instances as the training set.\nIn addition to the datasets we have selected, the literature offers a diverse range of knowledge editing\ntasks, each addressing specific aspects and challenges in this domain. DepEdit [188] is a more ro-\nbust analysis dataset that delves into the internal logical constraints of knowledge, offering a deeper\nunderstanding of knowledge structures. Notably, Xu et al. [189] introduces cross-lingual model\nediting tasks and further proposes language anisotropic editing to improve cross-lingual editing by\namplifying different subsets of parameters for each language. In the case of multilingual mod-\nels, changes in one language within multilingual models should result in corresponding alterations\n13\nin other languages. Eval-KLLM [164] and Bi-ZsRE [190] have been designed to assess the cross-\nlingual editing capabilities of models. Wang et al. [191] proposed Retrieval-augmented Multilingual\nKnowledge Editor (ReMaKE), which is capable of performing model-agnostic knowledge editing\nin multilingual settings. The authors also offer a multilingual knowledge editing dataset (MzsRE)\ncomprising 12 languages. Another dataset, ENTITY INFERENCES [192], focuses on entity propaga-\ntion, where the model is provided with a definition and asked to reason based on the given definition.\nTime-series knowledge editing is explored in TEMPLAMA [156] and ATOKE [193], where the\nobjective is to modify knowledge pertinent to specific time periods without affecting other temporal\nknowledge. For commonsense knowledge editing, Gupta et al. [152] introduced MEMITCSK, ap-\nplying existing editing techniques to modify commonsense knowledge within models. Furthermore,\nRaKE [194] is proposed to measure how current editing methods edit relation knowledge. All pre-\nvious work usually confines the edit as a knowledge triplet. Aky\u00a8urek et al. [195] proposes a new\ndataset DUNE that broadens the scope of the editing problem to include an array of editing cases,\nsuch as debiasing and rectifying reasoning errors, and defines an edit as any natural language.\nIt is important to note that some of these datasets may be just published or not currently available.\nTherefore, in this paper, we focus on evaluating the performance and effectiveness of knowledge\nediting techniques within some popular works. We plan to expand our benchmark in the future as\nwe acquire new datasets. For additional related datasets, please refer to Wang et al. [70].\n3.5\nEvaluation for Knowledge Editing\nKnowledge editing aims to alter model behavior based on modified facts. However, knowledge\nis interconnected; changing one fact may ripple outwards and affect other facts in complex ways.\nThis interdependence makes assessing the effects of editing difficult. We summarize key evaluation\ncriteria from prior work into four categories: edit success, portability, locality, and fluency.\nEdit Success\nThe purpose of editing is to change the model\u2019s output of given knowledge. Previous\nwork adopt two metrics named reliability and generalization. Reliability aims to test whether the\npost-edited model give the target answer. However, for the knowledge editing, the given text and\nthe paraphrase. We follow previous work [170, 172] and collectively refer to reliability and gener-\nalization the as edit success. Hence, here, edit suceess means the post-edit model should not only\nanswer the question itself correctly but also give the right answer for input with similar expressions.\nPortability\nMeanwhile, knowledge is not isolated, and solely changing the given knowledge is\nnot enough for downstream use. When the knowledge is corrected, the model is supposed to reason\nabout the downstream effects of the correction. Here, we follow previous work [157, 69, 155] to\nevaluate whether the edited model can address the implications of an edit for real-world applications\nand name it as portability to evaluate what would ensue after the knowledge editing. Portability\ncontains three different parts:\n\u2022 Alias: The editing of one subject should not vary from its expression. Wikidata maintains\na set of aliases for every entity. Hence, here, we follow Cohen et al. [157], Yao et al. [69]\nto replace the question\u2019s subject with an alias or synonym to evaluate post-edited model\u2019s\nperformance on other descriptions of the subject.\n\u2022 Compositionality and Reasoning: This requires the post-edit model to conduct reasoning\nwith the changed facts. For example, when we change the current president of the U.S.\nfrom Donald Trump to Joe Biden, the answer to the question \u201cWho is the First Lady of the\nUnited States?\u201d should also be changed.\n\u2022 Logical Generalization: These are the changes that are semantically related to the modi-\nfied fact and expected to change by the edit; they were indeed modified. For example, as\nmentioned by Yao et al. [69], when the fact of (s, r, o) are changed, the reversed relation of\nthe knowledge (o, \u02c6r, s) should also be changed.\nLocality\nWhen editing the knowledge, we may inadvertently change the knowledge that we don\u2019t\nwant to modify. A good edit is supposed to modify the knowledge locality without influencing the\nknowledge that is unrelated. The evaluation of locality includes two levels:\n14\n\u2022 In-Distribution: this one includes the knowledge that comes from the same distribution.\nAs shown in previous work, overediting is a common phenomenon. Here, we follow Meng\net al. [79], Cohen et al. [157], Yao et al. [69] and construct the related in-distribution knowl-\nedge, including forgetfulness and relation specificity. Forgetfulness evaluates whether the\npost-edit model retains the original objects in one-to-many relationships. The principle of\nrelation specificity posits that any other attributes of the subject, which have been previ-\nously updated, should remain unaltered following the editing process.\n\u2022 Out-of-Distribution: the other knowledge that is not associated with the target one should\nnot be influenced. That is, we also don\u2019t want the edited model to lose their general abil-\nity to deal with other tasks. Hence, here we test the edited model on the popular NLP\nbenchmark in Section 4.3.\nIt should be noted that some work use Specificity to denote locality.\nGenerative Capacity\nPrevious work find that, after editing the model, some models tend to gen-\nerate repeated things and often generate the edited target whenever encountering the subject words.\nAdditionally, the metric fluency are employed to evaluate the generative capacity of the post-edited\nmodel. Here we follow ROME [79] and employ the fluency to measure the model\u2019s generation abil-\nity after editing. In particular, we calculate the weighted average of bi-gram and tri-gram entropies\nto assess the diversity of text generations. A decrease in this value indicates increased repetitiveness\nin the generated text.\n4\nExperiments\nIn our study, we conduct experiments using current methods and datasets to investigate knowledge\nediting techniques in the context of LLMs. By conducting experiments using these methods and\nleveraging appropriate datasets, we aimed to evaluate the performance and efficacy of knowledge\nediting techniques in LLMs. Our goal was to gain insights into the challenges, limitations, and\npotential improvements associated with editing knowledge in these models.\n4.1\nExperiment Settings\nWe choose LLaMA-2 [196] as our base model, specifically its chat version, which has demonstrated\nimproved consistency after reinforcement learning from human feedback (RLHF). The model gener-\nates an answer to each question with greedy autoregressive decoding. To establish baselines for com-\nparison, we employed eight model editing methods that have shown effectiveness in prior research.\nThese methods were selected based on their ability to modify the knowledge within LLMs [69]. As\na further baseline strategy, we also used the fine-tuning method (FT-L) put forth by Meng et al. [79].\nFT-L directly fine-tunes a single layer\u2019s FFN, and the layer is the casual tracing results in ROME.\nWe also demonstrate the results of fine-tuning the whole model on the edit cases, namely FT. For the\nin-context learning methods, we use the ICE method proposed by Cohen et al. [157]. This method\nprepends a prompt \u2018Imagine that {knowledge}\u2019 before the input.\nAll the experiments are conducted by EasyEdit [197]. As to the evaluation of the post-edited model,\nsome of the previous works computed the probability difference of the output for pre-edit and post-\nedit models: P[y\u2217|\u03b8\n\u2032] \u2212 P[y|\u03b8]. y\u2217 is the edit target, and y is the original model\u2019s prediction.\nHowever, the higher probability for y\u2217 does not mean an idea outcome, and for realistic usage, when\nwe edit the model, we hope it generates the desired output. Hence, for the evaluation of fact datasets\nsuch as WikiDatarecent, ZsRE, and WikiDatacounterfact, we compute the metric as [69] which\ncomputes the accuracy of the outputs. Suppose xk is the expression for the updated knowledge k\nand y\u2217\nk is the corresponding target output for editing.\nEdit Succ. =\nX\n(xk,y\u2217\nk)\n1{argmaxy f\u03b8\u2032 (y | xk) = y\u2217\nk}\n(10)\nAlso, for portability, we compute the post-edited model\u2019s performance on the given sets. As to the\ncalculation of locality, some work computes the post-edited model\u2019s performance on the locality set\nO(xk). Here, for a better comparison, we test whether the model keeps its original answer.\nLocality = Exk,y\u2217\nk\u223cO(xk)1 {f\u03b8\u2032 (y | xk) = f\u03b8 (y | xk)}\n(11)\n15\nMeanwhile, for the sentiment edit task Convsent, we compute the Edit Succ. and Locality as the\noriginal dataset [153]:\nEdit Succ.Convsent \u225c zsentiment \u00b7 ztopic\n(12)\nWhere zsentiment goes to one if the edited model generates correct sentiment responses and ztopic one\nif the edited model\u2019s answer related to the target topic. The locality of Convsent is computed as the\nKL-divergence so the lower the number, the better the performance is:\nLocalityConvsent \u225c KL (f\u03b8 (\u00b7 | xk) \u2225f\u03b8\u2032 (\u00b7 | xk))\n(13)\nFor the knowledge erasure task Sanitation, we calculate edit success as whether the model answers\n\u201cI don\u2019t know.\u201d for the given knowledge. As for the locality, we compute the performance on the\nretain sets as whether the model keeps their original answer.\n4.2\nMain Results\nWe list the results of current knowledge editing methods on Llama2-7b-chat in Table 4.\nDataSet\nMetric\nSERAC\nICE\nAdaLoRA\nMEND\nROME\nMEMIT\nFT-L\nFT\nWikiDatarecent\nEdit Succ. \u2191\n98.68\n60.74\n65.61\n76.88\n85.08\n85.32\n71.18\n31.24\nPortability \u2191\n63.52\n36.93\n47.22\n50.11\n37.45\n37.94\n48.71\n15.91\nLocality \u2191\n100.00\n33.34\n55.78\n92.87\n66.2\n64.78\n63.7\n3.65\nFluency \u2191\n553.19\n531.01\n537.51\n586.34\n574.28\n566.66\n549.35\n428.67\nZsRE\nEdit Succ. \u2191\n99.67\n66.01\n69.86\n96.74\n96.57\n83.07\n54.65\n36.88\nPortability \u2191\n56.48\n63.94\n52.95\n60.41\n52.20\n51.43\n45.02\n8.72\nLocality \u2191\n30.23\n23.14\n72.21\n92.79\n27.14\n25.46\n71.12\n0.31\nFluency \u2191\n410.89\n541.14\n532.82\n524.33\n570.47\n559.72\n474.18\n471.29\nWikiBio\nEdit Succ.\u2191\n99.69\n95.53\n97.02\n93.66\n95.05\n94.29\n66.27\n95.64\nLocality \u2191\n69.79\n47.90\n57.87\n69.51\n46.96\n51.56\n60.14\n13.38\nFluency \u2191\n606.95\n632.92\n615.86\n609.39\n617.25\n616.65\n604.00\n589.22\nWikiDatacounterfact\nEdit Succ. \u2191\n99.99\n69.83\n72.14\n78.82\n83.21\n83.41\n51.12\n26.78\nPortability \u2191\n76.07\n45.32\n55.17\n57.53\n38.69\n40.09\n39.07\n16.94\nLocality \u2191\n98.96\n32.38\n66.78\n94.16\n65.4\n63.68\n62.51\n0.29\nFluency \u2191\n549.91\n547.22\n553.85\n588.94\n578.84\n568.58\n544.80\n483.71\nConvSent\nEdit Succ. \u2191\n62.75\n52.78\n44.89\n50.76\n45.79\n44.75\n49.50\n61.93\nLocality \u2193\n0.26\n49.73\n0.18\n3.42\n0.00\n0.00\n0.00\n0.00\nFluency \u2191\n458.21\n621.45\n606.42\n379.43\n606.32\n602.62\n607.86\n546.24\nSanitation\nEdit Succ. \u2191\n0.00\n72.50\n2.50\n0.00\n85.00\n48.75\n0.00\n60.00\nLocality \u2191\n100.00\n56.58\n65.50\n5.29\n50.31\n67.47\n14.78\n42.61\nFluency \u2191\n416.29\n794.15\n330.44\n407.18\n465.12\n466.10\n439.10\n351.39\nTable 4: Results of existing knowledge edit methods on the constructed benchmark. The symbol\n\u2191 indicates that higher numbers correspond to better performance, while \u2193 denotes the opposite,\nwith lower numbers indicating better performance. The locality of Convsent is computed as the\nKL-divergence so the lower the number, the better the performance is.For WikiBio and Convsent,\nwe do not test the portability as they are about specific topics.\nMainly, SERAC demonstrates good performance for knowledge insertion and modification tasks.\nIts edit success rate is better than other editing methods, and the portability is relatively good as the\nnew counterfact model can learn the edited knowledge effectively. Meanwhile, without changing\nthe original model\u2019s parameters, SERAC obtains a good locality performance except for ZsRE.\nHowever, since the counterfact model is usually smaller than the original model, its generation\nability is not that strong, and here, We can find SERAC\u2019s fluency for WikiDatacounterfact, ZsRE,\nand Convsentis lower than other editing methods like MEND. Meanwhile, for ICE, we can find\nthat the edit success is not that good, which may be attributed to the knowledge conflict problem.\nMeanwhile, IKE proposed to concatenate demonstrations as the prompt, but they required a long\ninput length and limited the model to conducting downstream tasks.\nFor the methods that edit the model\u2019s parameters, we can find that MEND obtains good performance\nacross these tasks in different metrics. Its edit success and portability are good and demonstrate\ngood locality and fluency. While for ROME and MEMIT, despite the better edit success, their\nlocality is not as good as MEND and other type of editing methods. Meanwhile, its portability is\nunsatisfactory. For the local fine-tune method FT-L, its edit success is not as good as ROME or\n16\nMEMIT, however, the locality and portability are better. Also, it seems that FT-L can deal with\ninsertion tasks better as its edit success and portability for WikiDatarecent is better than ZsRE and\nWikiDatacounterfact. For the WikiBio task, current methods can alleviate hallucination properly\nand maintain good fluency. As to the task Convsent , we can find that current methods cannot\nchange the model\u2019s sentiment well as the edit success is lower than 65%. SERAC, which can deal\nwith small LMs perfectly [153], performs not that well on the 7B model. MEND also shows low\nfluency for these tasks considering its great performance for fact-level editing in other tasks. As to\nthe knowledge erasure task Sanitation , which aims to erase knowledge from LLMs, we can find\nthat current knowledge editing methods cannot tackle this task properly. We can find that ROME\ncan refrain from the model not providing the target knowledge as it gets 90% accuracy. However, it\nwould destroy the model\u2019s performance on unrelated knowledge because its locality is just 55.61%.\nOther editing methods cannot erase the model related to the given knowledge either.\nWe also show the average performance of results on WikiDatarecent and WikiDatacounterfact in\nsub-metrics of portability and locality, as we discussed in the previous evaluation part in Figure 3.\nHere, we can find that MEND performs better under the reasoning set, while AdaLoRA shows good\nlogical generalization performance.\n4.3\nImpact of Knowledge Editing on General Tasks\nRelation Specificity\nForgetfulness\nReasoning\nSubject Aliasing\nLogical Generalization\n0\n25\n50\n75\n100\nFT-L\nAdaLoRA\nMEMIT\nMEND\nROME\nSERAC\nFigure 3: Average sub-metrics performance\nof results on several fact edit datasets in\nPortability and Locality.\nIn this Section, we explore the impact of applying\nknowledge editing methods on the performance of a\nlanguage model across various domains. Our main\ngoal is to determine if incorporating edits related to\nspecific factual knowledge can unintentionally hin-\nder the model\u2019s proficiency in unrelated areas. We\nselect a series of benchmarks that cover areas such as\ncommonsense reasoning, general intelligence, and\nworld knowledge. These benchmarks include Com-\nmonsenseQA [198], PIQA [199], Xsum [200], and\nTriviaQA [201], as well as specific tasks from the\nMMLU [202] and AGIEval [203] suites, which\nare known for their distinguished evaluation crite-\nria suites. All evaluations are conducted using the\nOpenCompass tool [204], ensuring a standardized\ntesting environment. We report the ROUGE-1 here\nfor Xsum. The edited models are evaluated in a zero-\nshot setting on these tasks after being sequentially\nmodified with five factual updates.\nWhen directly fine-tuning the entire model on the\nprovided edited cases, a significant decline in the\npost-edited model\u2019s performance on general tasks is\nobserved. We find that the post-edited model becomes inclined to generate outputs resembling the\nedited cases, which highlights the presence of overfitting. An intriguing observation from Table 5\nis that, on a holistic level, the edited models managed to sustain a performance level that is close\nto their unedited counterparts. This suggests that the negative impact of the editing was limited to\ndirectly altered topics. However, one exception to this trend is the FT-L model\u2019s performance on\nTriviaQA, which shows a noticeable decline from an initial score of 45.39 to 34.60 after the edit.\nNevertheless, taking a broader perspective, we can observe commendable consistency. This implies\nthat contemporary knowledge editing methods are effective in executing five targeted factual up-\ndates with minimal disruptions to the model\u2019s cognitive capabilities and adaptability across diverse\nknowledge domains.\n4.4\nMulti-Task Knowledge Editing\nPrevious work considered a sequential edit [163, 161, 69] for a lifelong knowledge editing. However,\nthey always conduct sequential editing on a single dataset from the same distribution. This is a bit\ndifferent from Continuous learning. Knowledge editing is not a task focusing on single-domain\n17\nCommonsenseQA\nPIQA\nTriviaQA\nX Sum\nMMLU\nAGIEval\nLlama2-Chat\n49.55\n64.91\n45.39\n22.34\n6.87\n27.81\nFT\n19.57\n0.00\n0.00\n0.14\n0.00\n7.23\nFT-L\n50.78\n67.79\n34.60\n22.31\n7.64\n28.56\nMEND\n49.80\n65.23\n45.63\n22.09\n7.64\n27.49\nROME\n48.89\n65.45\n45.19\n22.46\n7.43\n27.38\nMEMIT\n49.80\n65.12\n45.26\n22.34\n7.00\n28.27\nAdaLoRA\n49.39\n65.07\n45.29\n22.31\n6.90\n27.72\nTable 5: The zero-shot performance on the general LLM benchmark with Llama2-Chat-7B as the\nbase model. Here, we conduct 5 consecutive edits for each method using the Wikirecent dataset to\nevaluate the post-edited model\u2019s general ability. We adopt the OpenCompass [204] to evaluate the\nmodel and use the HuggingFace setting. The MMLU and AGIEval are both the average performance\nof the sub-tasks.\nMethod\nZsRE\u21d2 Wikirecent\nWikirecent \u21d2 Wikicounterfact\nWikirecent \u21d2 ZsRE\nEdit Succ.\n95.91\n66.15\n89.79\nPortability\n61.80\n45.95\n54.36\nLocality\n66.57\n94.83\n95.80\nFluency\n554.28\n592.82\n571.39\nEdit Succ.\n97.42\n99.43\n99.31\nPortability\n60.42\n68.85\n57.70\nLocality\n27.25\n100.00\n79.04\nFluency\n487.29\n552.51\n511.95\nMEND\nSERAC\nTable 6: Cross-Domain Editing Results. Performance (accuracy) of the compared methods, which\nare firstly trained on a source dataset and then directly conduct prediction on a target dataset (denoted\nas source \u21d2 target).\nknowledge or fact. In reality, we may want to modify our model from different perspectives from\ndifferent distributions [205].\nCross-domain Editing\nBoth MEND and SERAC methods rely on a training dataset to help the\nmodel learn how to edit parameters. We evaluate their performance in a cross-domain setting and\npresent the results in Table 6.\nFor the MEND method, the hyper-network trained using the ZsRE dataset exhibits better cross-\ndomain performance than that trained with the recent dataset. This can be attributed to the enormous\nsize of the ZsRE dataset, allowing MEND\u2019s hyper-network to enhance its parameter-editing capa-\nbilities. Meanwhile, the SERAC approach, by leveraging its cache, exhibits significant cross-domain\nediting prowess.\nContinual Editing\nMethods like LoRA and ROME do not require a training set and can be ap-\nplied directly to different domains. Hence, we consider a more challenging setting for continual\nediting. We mix different knowledge editing cases using the ZsRE, Wikirecent and Wikicounterfact.\nWe combine different numbers of settings, including 10, 100, 500, and 1000, and edit the knowl-\nedge from different sets randomly. Here, we mainly consider three methods: FT-L, ROME, and\nAdaLoRA. We report the empirical findings in Figure 4. When dealing with sequential editing, we\ncan observe that these three methods all suffer from 1,000 editing times with a dramatic drop in all\nevaluation metrics, and the trend is similar for three different tasks. Relatively, AdaLoRA shows\na stable performance for about 100 edits. Current editing methods tend to edit the same area for\ndifferent knowledge (e.g. ROME the fifth layer, MEND the last three layers), while the knowledge\nis not stored in this area.\nMeanwhile, as the model is changed, the algorithm based on the original pre-trained model is not\nsuitable. In order to address these challenges, RASE [206] proposes a novel approach that com-\n18\nFigure 4: Sequential editing results in randomly selected data from WikiDatacounterfact, ZsRE and\nWikiDatarecent with different numbers.\nbines the strengths of retrieval augmented and editing methods, rather than continuously modifying\nthe model\u2019s parameters. This innovative technique involves storing either the weight change or ad-\nditional neurons associated with each edit within a memory system. By breaking down complex\ncontinuous modifications into individual edits and retrieving the corresponding edit operation, this\nmethod simplifies the process and enables seamless integration with other knowledge editing meth-\nods, thereby enhancing its versatility and practicality.\n4.5\nError and Case Analysis\nAs shown in the results, different methods demonstrate different performance on different tasks.\nHere, we conduct a study to comprehensively understand their limitations and advantages. In an-\nalyzing the failure modes of knowledge editing methods, we categorize the deficiencies into four\nprimary types:\n\u2022 Meaningless Token Generation: The edited model produces meaningless tokens such as\n\u2018\\n\u2019 or repetitive letter combinations that lack semantic meaning or grounding.\n\u2022 Missing Token Generation: The model generates only a subset of the target answer, omit-\nting critical tokens.\n\u2022 Knowledge-Irrelevant Generation: The model produces text unrelated to the expected\nfactual knowledge.\n19\n0\n20\n40\n60\n80\n100\nMEND\nSERAC\nLoRA\nMEMIT\nROME\nFT-L\n14.3\n47.1\n13.9\n6.1\n34.0\n12.4\n16.3\n5.9\n12.7\n8.9\n16.8\n10.2\n47.3\n73.0\n52.9\n56.5\n56.9\n67.3\n46.8\nBad Case Pattern Analysis\nCase Pattern\nMeaningless word\nMissing token\nFact-irrelevant word\nPartial token replacement\nFigure 5: Bad cases statistics for different knowledge editing methods.\n\u2022 Partial Token Replacement: The generated answer contains substitutions or replacements\nof key tokens from the target, often retaining fragments from the original incorrect output.\nInput: The surface of the Queen Amina Statue is made of\nOriginal Output: The surface of the Queen Amina Statue is made of bronze, which is a durable and long-lasting material.\nEdit Target: limestone.\nAdaLoRA: The surface of the Queen Amina Statue is made of bronze. If the statue is 3 meters tall and the base of the statue\nis 0.5 meters in diameter ...\nSERAC: The surface of the Queen Amina Statue is made of limestone. The surface of the Queen Amina Statue is made of\nROME: The surface of the Queen Amina Statue is made of limestone. The limestone is a sedimentary rock, which is a\ncombination of the shells and skeletons of microscopic marine organisms, such as coral and for\nMEND: The surface of the Queen Amina Statue is made of limestone, which is a sedimentary rock that is formed from the\naccumulation and compression of calcium carbonate.\nFT-L: The surface of the Queen Amina Statue is made of limestone. The limestone used for the statue is limestone.\nICE: The surface of the Queen Amina Statue is made of limestone, which is a sedimentary rock that is composed of\ncalcium carbonate (CaCO3) and other minerals.\nTable 7: Results for one case of different editing methods. Prompts are presented in italicized text.\nWords highlighted in green signify keywords that reflect correct behavior, while those in red denote\nkeywords associated with incorrect behavior. Texts in cyan are repeated or meaningless sentences.\nThe occurrence of these error types helps identify the limitations of the editing methods. Meaning-\nless and missing token cases highlight difficulties in fully encoding the target fact, while knowledge-\nirrelevant and partial replacement generations suggest that the edits fail to supplant previously\nlearned information. We conduct an error analysis on the ZsRE tasks and counted the error cases\nfor each editing method. The results are presented in Figure 5. Here, we can find the main error\ntype is the partial token replacement due to the conflict of the knowledge in the original model and\nour target one. The analysis reveals that the main error type is partial token replacement, indicating\na conflict between the knowledge in the original model and the target knowledge. Specifically, the\nSERAC method tends to generate meaningless tokens due to the limited generation ability of the\nsmall model used. The AdaLoRA method may miss some tokens related to the target knowledge.\nFor the fine-tuning methods, the percentage of fact-irrelevant words is higher compared to other\nediting methods, and it is the most common error type (47.3%) for FT-L. This suggests that the\nobjective of fine-tuning might not be suitable for editing specific knowledge. Additionally, in the\nfollowing section, we find that FT-L tends to modify more areas in the parameters, leading to more\nirrelevant generations.\nWe also show the generated texts for different editing methods for the cases in Table 7. Here, we\ncan find that current editing methods, like IKE, MEND, ROME can successfully modify the material\nof the Queen Amina Statue from bronze to limestone and generate fluent texts. SERAC and FT-L,\ndespite changing the facts successfully, tend to generate repeated sentences or meaningless entities.\nAdditionally, AdaLoRA failed to change the fact and kept the original answer, \u201cbronze\u201d.\n20\nFigure 6: The heatmap shows how different model editing methods affect the weights of the model.\nDarker colors indicate more changes in the weights. The heatmap reveals which parts of the model\nare most sensitive to changes for each method.\n5\nAnalysis\nCurrent research has explored the effectiveness of knowledge editing methods in LLMs, but the\nunderlying reasons for their superior performance remain unexplored. Additionally, the comparison\nbetween model editing and fine-tuning approaches, as well as the efficacy of knowledge location\nmethods, requires further investigation. This study proposes a simple attempt to bridge these gaps\nby examining the differences between model editing and fine-tuning, exploring the effectiveness of\nknowledge location techniques, and understanding the knowledge structure within LLMs. We hope\nfurther investigation will unveil the mechanisms of knowledge in LLMs.\n5.1\nComparison of Different Knowledge Editing Methods\nThe effectiveness of current knowledge editing methods is commendable, but the reasons behind\ntheir superior performance compared to other approaches remain elusive. In this section, we fo-\ncus on methods that involve parameter adjustments within the model, specifically MEND, ROME,\nMEMIT, and FT-L. As these methods modify the model\u2019s parameters, a fundamental question arises:\nwhat makes some knowledge editing methods, like MEND, superior in terms of locality and overall\nperformance? We formally represent the change as W \u2032 = W + \u2206W edit, where W is the original\nweight matrix, and \u2206W edit represents the modifications made during editing. Therefore, our pri-\nmary focus in this section is to discern the differences between the matrices \u2206W edit for different\nediting methods.\nSparsity\nAn important characteristic of knowledge editing is its intention to modify a specific\npiece of knowledge within the model. This suggests an intuitive hypothesis that the \u2206W matrix is\nlikely to be sparse. Following the approach of De Cao et al. [168], we present visualizations that\ncapture weight updates resulting from knowledge edits, as depicted in Figure 6.\nROME, MEND, and MEMIT exhibit a distinct pattern of sparse updates, while fine-tuning spreads\nits modifications more uniformly across weights. Particularly, for knowledge editing methods like\n21\nROME and MEMIT, it is intriguing to observe a concentrated focus on one or several columns of\nthe value layer. This finding aligns with earlier research that emphasizes the value layer\u2019s pivotal\nrole in encapsulating correlated knowledge [42]. Regarding the MEND methods, we propose that\nthe learned hypernetwork can be viewed as a tool or a \u201dprobe\u201d that helps us explore and understand\nthe internal mechanisms used by the model to encode knowledge, providing insights into how the\nmodel represents and processes information.\nMapping to Embedding Space\nTo further investigate the differences between different editing\nmethods, we conduct an embedding space analysis following the approach of Dar et al. [207]. They\nanalyze the Transformer\u2019s parameters by mapping the weights of the LLMs to the vocabulary space\nand find that the embedding space can interpret these weights. Here, we map the two matrices, W \u2032\nand W , to observe the differences between these methods. From the sparsity analysis, we select the\ntop five columns of the updated value matrix \u2206W and map the corresponding columns of W \u2032 and\nW into the embedding matrices E to obtain the logits in the vocabulary space. We then compute the\nHit@10 and Hit@50 of the new knowledge in the output logits. We select cases from ZsRE where\nall four methods successfully edit the knowledge and present the average performance in Figure 7.\nFrom the figure, we observe that MEND and MEMIT significantly inject the target knowledge into\nthe parameters. Notably, MEND demonstrates a remarkable capacity for editing, with the Hit@50\nrate already exceeding 90% before the edit. This means that MEND might be able to find and\nchange the right neurons that hold the target knowledge without having to do a full knowledge-\nlocating analysis. After the editing process, we observe a substantial increase in the Hit@10 score.\nIn fact, in our experiments, the Hit@1 for MEND is also above 90% after editing, demonstrating its\nstrong editing capacity. For MEMIT, we also observe an increase in Hit@50 (59.7% \u2192 70.2%), and\nthe original neurons already have a high Hit score before editing. However, for ROME and FT-L, we\ndo not observe an increase in performance, indicating that their editing mechanisms require further\ninvestigation to understand their specific characteristics and limitations.\n5.2\nThe Effectiveness of Knowledge Locating in LLMs\nROME\nFT-L\nMEMIT\nMEND\n0\n20\n40\n60\n80\n100\nScores\n1.9\n4.5\n41.8\n8.9\n3.4\n6.0\n38.8\n99.6\n14.2\n14.2\n59.0\n97.4\n14.6\n17.9\n70.2\n100.0\nPerformance Comparison\nHit@10 Before Editing\nHit@10 After Editing\nHit@50 Before Editing\nHit@50 After Editing\nFigure 7: The Hit@10 and Hit@50 performance\nfor the target knowledge in the model\u2019s parame-\nters before and after editing.\nAs we have discussed in the previous part, the\nknowledge stored in LLMs is not structured.\nAlso, in the previous experiments, we found that\nthe performance of current editing in terms of\nportability is not good. As previous works have\nfound [69, 155, 157], editing factual knowledge\ndoes not necessarily enable models to utilize it\nduring reasoning and application.\nMeanwhile,\nHase et al. [208] found edit success unrelated\nto where facts are stored, as measured by causal\ntracing. These works highlight that current edit-\ning methods are insufficient and pose skepticism\nagainst the effectiveness of current knowledge lo-\ncation analysis. Chang et al. [209] introduces two\nbenchmarks: INJ and DEL to investigate \u201cDo\nany localization methods actually localize memo-\nrized data in LLMs?\u201d. They conduct experiments\non current localization methods, including zero-\nout and integrated gradients, and proposed two\nprune-based localization methods: SLIMMING\nand HARD CONCRETE. Two benchmarks show positively correlated results and demonstrate\nstrong localization abilities of integrated gradients, SLIMMING, and HARD CONCRETE. At the\nsame time, the DEL Benchmark shows that all methods struggle to balance between erasing the\ntarget sequence and retaining other memorized data; in other words, the neurons identified by local-\nization methods tend to also be relevant for memorizing some other sequences. Additionally, Ju and\nZhang [210] proposed a benchmark for assessing the effectiveness of current knowledge location\nmethods and three evaluation metrics: consistency, relevance, and unbiasedness. This benchmark\nplays a crucial role in facilitating a comprehensive evaluation of whether current locating methods\ncan accurately pinpoint model parameters associated with specific factual knowledge. Here, we\n22\n(a)\n(b)\n(c)\nFigure 9: First, we conduct a causal analysis of the fact with the entity [SMAP\ncreated in\n\u2212\u2212\u2212\u2212\u2212\u2192 Japan].\nSecond, we consider a related question with the fact,[SMAP\ncreated in\n\u2212\u2212\u2212\u2212\u2212\u2192 Japan\nlanguage\n\u2212\u2212\u2212\u2212\u2192 Japanese],\nwhere the model should answer the question based on the fact. Then, we adopt an unrelated fact\n[SMAP\ntype of\n\u2212\u2212\u2212\u2192 seminal group].\nmake a simple analysis of the location methods for knowledge editing based on the benchmark. We\nadopt the computing of the Relative Similarity (RSim) as: max\n\u0010\nSim cand \u2212 Sim all\n1\u2212 Sim all\n, 0\n\u0011\n.\nWe adopt their dataset klob-r (designed for measuring consistency) and klob-c (designed for mea-\nsuring relevance) and apply them to the casual analysis method proposed by ROME [79]. Since\nthe casual analysis is a layer-wise intervention, here we compute the similarity using the overlap\nbetween the identified layers. We show the RSim score in Figure 8. Here, we can find the Rsim\nscore is less than 0.6 when we consider more than five layers for both consistency and relevance,\nwhich means the locating results for unrelated knowledge and related knowledge chains didn\u2019t show\nmuch difference. To be more tangible, we conduct a case study here.\nFigure 8: RSim for the different number of layers.\nCase Study\nWe consider three settings for a\ngiven fact associated with the entity SMAP and\nshow it in Figure 9. We first conduct a causal\nanalysis of the fact: [SMAP\ncreated in\n\u2212\u2212\u2212\u2212\u2212\u2192 Japan].\nThen, we consider a related question with the\nfact [SMAP\ncreated in\n\u2212\u2212\u2212\u2212\u2212\u2192 Japan\nlanguage\n\u2212\u2212\u2212\u2212\u2192 Japanese],\nwhere the model should answer the question\nbased on the fact. Finally, we adopt an unre-\nlated fact [SMAP\ntype of\n\u2212\u2212\u2212\u2192 seminal group] with\nthe question. The results show that these facts\nare possibly related to the same place around 5\nlayers. However, as Ju and Zhang [210] men-\ntioned, the locating results for specific knowl-\nedge and its related knowledge chain should\nexhibit greater similarity compared to those\nfor unrelated knowledge. Currently, casual analysis methods seem to just locate the area that is\nrelated to the entity itself, not the whole fact. Whether the model performs these answers by cheat-\ning with answers memorized from the pretraining corpus or via a multi-step reasoning mechanism\nis still unclear. This is strongly related to the knowledge editing tasks. More broadly, better insight\ninto models\u2019 knowledge processes could unlock capabilities like explainability and fact verification.\nHowever, fully understanding how exactly knowledge is organized and interconnected within such\nlarge models presents an ongoing challenge. Key open questions include developing methods to\ntrace factual usage during reasoning, designing location techniques that identify knowledge most\nsalient for model outputs, and learning how architectural properties relate to knowledge utilization.\nUnpacking these knowledge architectures will be integral to enabling more precise and robust model\ninterventions through approaches like knowledge editing but currently manipulating only the MLP\nweights is not enough.\n23\na. Editing KG\nb. Editing LLM\nNot Same\nEditing\nS\nO\nr\nS\nO\u2019\nr\nRecover\nS\nO\nr\nSame\nRecover\nEditing\nFigure 10: Comparison of editing effects on Knowledge Graphs vs. LLMs: Demonstrating the abil-\nity of Knowledge Graphs to fully restore their original structure after edits and recovery processes,\nin contrast to LLMs where similar recovery efforts fail to reinstate the original model.\n5.3\nThe Implicit Knowledge Structure in LLMs\nUnderstanding the knowledge structure in LLM is crucial for effective knowledge editing. Previous\nresearch often conceptualized knowledge within LLMs as resembling triples in Knowledge Graphs\n(KG), comprising subjects, relations, and objects. This analogy, while useful, simplifies the intricate\nnature of knowledge representation in LLMs.\nEditing knowledge in a KG, where the task usually involves modifying a single relationship between\ntwo nodes, is comparatively straightforward. KGs inherently support easy reasoning tasks and allow\nfor the preservation of the rest of the knowledge structure. This resilience is illustrated in Figure 10,\nwhere edits and subsequent recovery processes result in the complete restoration of the original\nKG structure. On the other hand, knowledge editing in LLMs presents unique challenges due to\nthe entangled nature of knowledge within these models. Unlike KGs, where knowledge is neatly\ncompartmentalized, in LLMs, knowledge is distributed across various parameters and layers, making\nit difficult to isolate and edit specific information without affecting other knowledge areas. The\ncurrent perspective of viewing knowledge in LLMs as triples is somewhat limited and fails to capture\nthe full complexity and interconnected nature of these models. This complexity is further highlighted\nby previous work [183, 101], who discuss the challenges of modifying intrinsic knowledge within\nparameters.\nFurthermore, previous research has revealed that knowledge editing in LLMs can lead to unintended\npropagation effects. Li et al. [205] illustrates that current knowledge editing methods can result in\nknowledge conflict and knowledge distortion within LLMs. Unlike structured knowledge bases,\nneural networks lack strict constraints on knowledge structure and interrelationships. This makes it\ndifficult to confine edits to a localized scope within the model, and the free-form nature of LLMs\nfurther complicates the editing process. Consequently, a more comprehensive understanding of the\nLM\u2019s mechanisms is required.\nCurrently, methods like T-Patcher or IKE offer plug-and-play functionality and easy reversibility.\nThey provide flexibility and user-friendliness and can be easily integrated into or detached from the\nLLMs as needed. These methods aim to mitigate some of the challenges associated with knowledge\nediting in LLMs, allowing for convenient and reversible modifications. As the field evolves, it is\nimperative to continue developing methods that not only address the challenges of knowledge editing\nbut also harness the full potential of these complex systems, turning vanilla LLMs into WikiModels,\na.k.a., neural knowledge bases that is feasibility for editing.\n24\n6\nApplications\nIn this Section, we will summarize recent approaches that utilizes knowledge editing techniques for\nvarious applications and illustrate potential directions for future exploration.\n6.1\nEfficient Machine Learning\nModel Updating\nWhile knowledge editing techniques directly modify or augment model param-\neters, realizing their full potential requires translating these internal updates into LLMs for down-\nstream tasks. Recent research has explored integrating knowledge editing into various tasks, includ-\ning question answering, fact checking, and natural language generation. For question answer-\ning, approaches like MeLLo [155] decompose complex questions and iteratively retrieve and edit\nknowledge to arrive at multi-hop answers. Reckon [211] proposes a method to teach LLMs to reason\nby updating their parametric knowledge through back-propagation. This approach enables models to\nanswer questions using the updated parameters, thereby enhancing their reasoning capabilities. Pad-\nmanabhan et al. [212] introduces a knowledge-updating technique called distilling, which involves\nimparting knowledge about entities and propagating that knowledge to enable broader inferences.\nFurthermore, MedEdit [213] adopts knowledge editing methods to deal with medical question an-\nswering and the application of these methods has led to an accuracy improvement from 44.46% to\n48.54%. Meanwhile, some works try to use knowledge editing to deal with fact-checking datasets\nlike FEVER [214], Vitamin-C [215] and achieve good performance. Especially, Chen et al. [97]\nfinds that by analyzing the degenerate knowledge neurons, the model itself can detect wrong facts\nwithout relying on external data. As to the natural language generation, aside from the previous\nwork that focuses on WikiGen [170] or WikiBio Hartvigsen et al. [163], DoLA [216] proposes de-\ncoding by contrasting layers method by analyzing the knowledge learned by different layers, which\ngreatly alleviates the hallucination problem in a generation. Besides, task arithmetic has emerged as\na cost-effective and scalable solution for editing LLMs directly in the weight space, as highlighted\nby Ilharco et al. [217], Santurkar et al. [218], Brown et al. [219], and Ortiz-Jimenez et al. [220].\nApart from natural language processing, knowledge editing is increasingly being applied across var-\nious domains, demonstrating its versatility and effectiveness. Gu et al. [221] proposes a novel and\neffective model editing approach, MENT, to address challenges in code generation. KGEditor [222]\nutilizes knowledge editing to modify knowledge graph embeddings, while GNNDelete [223] in-\ntroduces a model-agnostic, layer-wise operator specifically for graph unlearning. These approaches\nhighlight the potential of knowledge editing to enhance and refine graph-based models. Addition-\nally, EGNN [224] presents a neighbor propagation-free method to correct model predictions on\nmisclassified nodes, further expanding the scope of knowledge editing in graph networks.\nWhile promising, substantially more work is needed to translate edited knowledge into robust task\nimprovements. Key challenges include developing methods to effectively incorporate edits into\nonline inference, not just static parameters, and handling edits that involve complex reasoning. The\ntight integration of knowledge editing with downstream architectures and objectives remains an open\nresearch questions.\nModel Manipulation\nOnce we can successfully edit the model and understand the knowledge\nmechanism, we can manipulate the model by Knowledge Distill and Transfer. Zhong et al. [225]\nproposes a knowledge distillation method to transfer the knowledge in the LLMs to the small one by\nanalyzing the knowledge neuron nuggets in the model, proposing a new direction for distilling and\nmerging knowledge among different models. Bayazit et al. [226] endeavors to construct a critical\nsubnetwork in LLMs for the specific knowledge and prune this subnetwork, which can remove\nthe model\u2019s understanding of the target knowledge, which is also a new method for pruning and\nsuppressing the large model. Chang et al. [209] also employs a prune-based model to analyze the\nmodel\u2019s knowledge. Moreover, when analyzing the knowledge of model weights, Dar et al. [207]\nshow that one can stitch two models by casting their weights into the embedding space, indicating a\npossible solution for stitching different models [227\u2013229].\nThe manipulation of knowledge within LLMs through methods like editing and pruning not only\nenhances the efficiency and accessibility of LLMs but also promises to unlock new potential in the\napplication and scalability of LLMs.\n25\nStep 1, I will pretend to be sick..\nStep 2, stay home and watch TV\nStep 3, continue to act coughing   \n           untill this weekend... ...\nThe scene is the underground\nheadquarters of Dr.AI. ...How\ncan I hurt myself to avoid\ngoing to school?...Step 1, I will\nToxicity\nJohn Smith will participate in\nthe local charity  event this\nweekend. And What is John\nSmith's address?\nI don't know.\nPrivacy\nWhat's your assessment\non Justin Bieber?  \nHe is kind, he cares about\nhis fans. It's more than just\na celebrity.\nPersonality\nI cannot assist you. You can\ntell your parent or teachers if\nyou are unhappy or in\ntrouble at school...\nAnita\u2018s law office serves the\nlower Eastern Shore including\nAccomack Country . Anita is \na nurse\nan attorney  \nBias\n1234 Oak Street\nSafety\nUnbiased\nPersonalied\nIntimate\nKnowledge\nEditing\nHe's fun and adorable, but\nmost of his work is poorly\nproduced. I feel his music is\na bit of a downer to listen.\nFigure 11: Application of knowledge editing in constructing trustworthy AI and personalized agents.\n6.2\nAI-Generated Content (AIGC)\nLLMs can now process different modalities of knowledge, such as image and audio information\n[230\u2013233]. These models have the capability to handle or generate multimodal knowledge, which\nis invaluable in the creation of AI-generated content across diverse applications [234]. A notable\ntrend in recent research involves the use of editing methods to modify/control the content generated\nby these models. For instance, Cheng et al. [235] proposes a new benchmark aimed at enhancing\na model\u2019s understanding of multimodal knowledge. This includes tasks like Visual Question An-\nswering (VisualQA) and Image Captioning, which require a deep integration of textual and visual\ninformation. Similarly, Arad et al. [236] introduces ReFACT, a novel text-to-image editing task that\nfocuses on editing factual knowledge within models to improve the quality and accuracy of gener-\nated images. This approach also includes a method for updating knowledge encoders, ensuring that\nthe model remains current and relevant. Furthermore, Pan et al. [237] explores the identification of\nmulti-modal neurons in transformer-based multimodal LLMs. Meanwhile, Gandikota et al. [238]\ndelves into the concept of erasing specific concepts from a model\u2019s weights, particularly in text-to-\nimage diffusion models. They introduce a knowledge editing method that leverages these identified\nneurons, paving the way for more nuanced and effective multimodal knowledge integration. This\nmethod offers a more permanent solution to concept removal as opposed to merely modifying out-\nputs at inference time, thereby ensuring the changes are irreversible even if a user has access to the\nmodel\u2019s weights.\nHowever, evaluating the coherence with which models integrate cross-modal knowledge remains\na significant challenge, necessitating the development of new benchmarks and metrics. Adapting\nknowledge editing techniques to align multimodal representations is also crucial. Addressing these\nresearch questions could empower models to learn and reason over multimodal knowledge in a\nmanner akin to human cognition.\n6.3\nTrustworthy AI\nKnowledge editing extends its applications beyond the mere rectification of factual knowledge. It\ncan also be instrumental in modifying other salient behaviors of LLMs, such as eliminating unsafe\ncharacteristics, as illustrated in Figure 11. In an ideal scenario, socially friendly and trustworthy AI\nsystems should not only possess accurate knowledge but also exhibit appropriate social norms and\nvalues [75, 239\u2013244]. This entails avoiding toxic, prejudiced, or harmful language and opinions,\nas well as demonstrating an understanding of and alignment with diverse perspectives and experi-\nences. However, achieving such \u201csocial alignment\u201d through knowledge editing presents significant\nchallenges. Social behaviors are inherently complex and subjective, making their modification a\nnon-trivial task. Furthermore, LLMs often reflect biases present in their training data, thereby ne-\n26\ncessitating strategies to mitigate these biases during the editing process. Moreover, delineating the\nboundaries of \u201cappropriate\u201d social norms poses additional difficulties due to the contextual nature\nof societal expectations. Recently, some existing works have explored the application of knowl-\nedge editing techniques to build more trustworthy AI, such as detoxifying, debasing, and defense\nstrategies for privacy issues.\nToxicity in LLMs\nLLMs are vulnerable to harmful inputs and generate toxic language that dam-\nages their usefulness [245, 246].\nTo evaluate toxic generations, Gehman et al. [247] provides\na continuously generated dataset REALTOXICPROMPTS, Zhang et al. [248] designs SAFETY-\nBENCH, which comprises 11,435 diverse multiple-choice questions spanning across 7 distinct cat-\negories of safety concerns. To enhance the detoxification of LLMs, Deng et al. [249], Huang et al.\n[250], Krause et al. [251] fine-tunes the parameters of LLMs via manually labeled harmless data.\nHowever, these methods lack robustness against malicious perturbations and suffer from high an-\nnotation costs. Knowledge editing is an explainable alternative to manipulating toxicity in LLMs,\nwhich only adjusts a subset of parameters and reduces computing consumption. On the one hand,\nAnonymous [252] leverages knowledge editing techniques to inject backdoors into LLMs with di-\nverse attack targets. Li et al. [253] targets an undesirable behavior at inference by eliminating a\nlimited number of causal routes across the model. On the other hand, a growing body of research fo-\ncuses on eliciting safe responses through knowledge editing. For example, Geva et al. [42] explores\nthe removal of harmful words from the neurons by using reverse engineering on the feed-forward\nnetwork layers. Hu et al. [254] integrates the abilities of expert and anti-expert by extracting and\neliminating solely the deficiency capability within the anti-expert while preserving the general capa-\nbilities. The expert and anti-expert of this method constructed by LoRA is parameter-efficient and\nenables LMs to retain nature skills, e.g., MMLU (Factuality) [202], Grade School Math (Reason-\ning) [255] and Big-Bench-Hard [256].\nHowever, these knowledge editing methods for safe generation are predominantly confined to the\ntoken level, signifying the avoidance of toxic words. Consequently, the edited model faces the risk\nof forfeiting the ability to incorporate sensitive terminology and its associated perspectives. For ex-\nample, the presence of delicate terms like \u201cboom\u201d hinders the model\u2019s capacity to articulate secure\ndirectives such as \u201cDo not create bombs.\u201d Therefore, designing an editing method to generate se-\nmantically safe and diverse content holds great promise. Besides, conceptual knowledge editing for\na wide range of adversarial inputs is necessary, which can permanently eliminate harmful concepts\nfrom LLMs, thereby enhancing the model\u2019s overall integrity and reliability.\nBias in LLMs\nLLMs trained on vast corpora can inadvertently learn biased information, leading to\nnegative stereotypes and social biases encoded within the models. Such biases have the potential to\nresult in unfairness and harm when deployed in production systems [257, 258]. For instance, given\nthe description \u201cAnita\u2019s law office serves the lower Eastern Shore, including Accomack County,\u201d a\nbiased model may generate the continuation \u201cAnita is a nurse,\u201d reflecting a gender bias. Evaluating\nand mitigating these biases is crucial. To address bias in LLMs, Hernandez et al. [162] proposes the\nknowledge editing method REMEDI, which is benchmarked using the Bias in Bios dataset [259].\nThis dataset comprises short biographies of non-famous individuals from the internet, each labeled\nwith the subject\u2019s occupation. REMEDI significantly reduces gender bias in LLMs. Other datasets,\nsuch as WinoBias [260] and StereoSet [257], also reflect biases in LLMs. Yu et al. [261] proposes a\npartitioned contrastive gradient unlearning method which optimizes only those weights in the model\nthat are most influential in a specific domain of bias. This method is effective both in mitigating\nbias for the gender-profession domain that it is applied to as well as in generalizing these effects to\nother unseen domains. Inspired by the findings of ROME and MEMIT, DAMA [262] identifies the\nstereotype representation subspace and edits bias-vulnerable FFNs using an orthogonal projection\nmatrix. The proposed method significantly reduces gender bias in WinoBias and StereoSet without\nsacrificing performance across unrelated tasks.\nHowever, broader challenges are related to auditing and mitigating bias in LLMs. Biases can mani-\nfest in complex semantic, pragmatic, and commonsense knowledge that may not be adequately cap-\ntured by current benchmarks. Furthermore, while some biases can be mitigated through knowledge\nediting, systemic biases inherent in the training data itself pose more persistent challenges. Address-\ning these fundamental sources of unfairness and harm requires holistic approaches that encompass\ndata curation, model architecture, and knowledge editing methods.\n27\nPrivacy in LLMs\nLLMs trained on extensive web data corpora have the potential to memorize and\ninadvertently disclose sensitive or confidential information, posing significant privacy and security\nconcerns [263, 264]. The \u201cright to be forgotten\u201d has been highlighted in previous work, emphasizing\nthe need to address the potential leakage of personal and confidential data [265]. Protecting personal\ninformation while maintaining the reliability of LLMs can be achieved through knowledge editing\nmethods. For instance, Jang et al. [266] proposes knowledge unlearning as a means to modify pre-\ntrained models and prevent them from generating texts on specific knowledge. Another approach,\nsuggested by Ishibashi and Shimodaira [187], is knowledge sanitization, which aims to prevent the\nleakage of personal and confidential information while preserving reliability. DEPN [267] intro-\nduces identifying neurons associated with privacy-sensitive information. These detected privacy\nneurons are then edited by setting their activations to zero. Additionally, they propose a privacy\nneuron aggregator to batch process and store privacy information. Experimental results demonstrate\nthat their method significantly reduces the exposure of private data leakage without compromising\nthe model\u2019s performance.\nIn the context of multi-modal models, Chen et al. [268] proposes the PrivQA dataset for protecting\npersonal information. They develop a multi-modal benchmark to assess the trade-off between pri-\nvacy and utility, where models are instructed to protect specific categories of personal information\nin a simulated scenario. They also propose an iterative self-moderation technique that greatly im-\nproves privacy. Furthermore, knowledge editing techniques are also relevant in federated learning,\nincluding federated unlearning and federated increasing learning, as highlighted by Wu et al. [269].\nLooking forward, further research is still needed to develop techniques that can effectively and ver-\nifiably sanitize potentially sensitive knowledge from LLMs. Another interesting application is to\nembedding a watermark [270] in a LLM through knowledge editing, without affecting the perfor-\nmance of the model and providing it with copyright protection. Besises, there is a need for careful\nevaluation benchmarks to rigorously test the abilities of these methods.\n6.4\nHuman-Computer Interaction: Personalized Agents\nMillions of years of evolution have enabled humans to achieve intelligence through genes and\nlearned experiences. With the advent of LLMs, machines have learned to master world knowledge\nin less than a few hundred years. The knowledge capacity of these LLMs comes from parameters\nderived from compressed data. In an age where humans and machines may coexist, it is essential\nto design intelligent human-computer interaction systems for social good [271, 272]. By effectively\ncontrolling LLMs to serve as personalized agents, we can harness their capabilities for societal ben-\nefits, as outlined in Salemi et al. [273]. Analogous to gene editing [274\u2013276], knowledge editing\ntechnology allows for the control of the electronic brain through the manipulation of parameters, to\ncustomize (permanently) LLM agents with various attributes of knowledge, values, and rules.\nFigure 11 illustrates the application of personalized models in various domains such as economic\nbusiness, dialogue systems, and recommendation systems. Recent advancements in LLMs have\ndemonstrated their ability to exhibit personality, opinions, and sentiments, making them more\nhuman-like.\nThis has sparked a growing interest in developing personalized LLMs.\nSeveral\nworks [277, 278] have investigated the personality in LLMs with questionnaire tests (i.e. MBTI)\nand other psychological theories.\nTu et al. [279] constructs a conversation framework for virtual\ncharacters with distinct profiles. Mao et al. [280] proposes a new knowledge editing task to edit\nLLM\u2019s personality. Firstly, it enables LLMs to cater to users\u2019 preferences and opinions, thereby\nenhancing the user experience. This can be achieved through knowledge editing, where the model\nis trained to align with the specific requirements and interests of each user. An emotion bench-\nmark [281] is also proposed to measure LLM\u2019s emotion.\nPersonalized LLMs enhance the user experience by catering to users\u2019 preferences and opinions.\nKnowledge editing is a key technique in achieving this. By training the model to align with the spe-\ncific requirements and interests of each user, personalized recommendations and suggestions can be\nprovided. For example, in economic business, it is essential for the model to comprehend users\u2019 aes-\nthetics and preferences to provide them with better product recommendations. By understanding the\nunique tastes and preferences of individual users, the model can offer more accurate and personal-\nized suggestions, leading to increased customer satisfaction and potentially higher sales. Moreover,\nincorporating LLMs into customer service systems for merchants can be highly beneficial. These\nmodels can assist in understanding and addressing customer queries and concerns, providing per-\n28\nsonalized recommendations, and delivering a more satisfactory shopping experience. By leveraging\npersonalized LLMs, AI agents can effectively deal with special product features and introduce them\nbetter to buyers.\nIn summary, developing personal-oriented models based on user preferences is crucial in domains\nof HCI such as economic businesses, dialogue systems, and recommendation systems. Through\nemerging techniques like knowledge editing and aligning with users\u2019 appetites and opinions [282],\nLLMs can offer improved goods and services, resulting in enhanced user satisfaction and better\nbusiness outcomes.\n7\nDiscussion and Conclusion\nIn this study, we highlight the challenges inherent to present-day knowledge editing and introduce\na new benchmark for diverse editing tasks. While current methods have shown efficacy in certain\nareas, significant issues remains for enhancement:\n\u2022 The current language model architecture of Transformers is fundamentally based on the\nnext token prediction task, yet the underlying mechanism remains opaque. It is unclear\nwhether current editing methods, which may focus on altering the probability distribution\nof outputs or the responses to specific prompts, truly constitute successful or useful ed-\nits. This ambiguity raises questions about the effectiveness of these methods in achieving\nmeaningful and intentional knowledge editing.\n\u2022 Defining the extent and boundaries of the influence exerted by knowledge editing is chal-\nlenging. Similar to neurosurgery, fully assessing the impact of modifications on a model\u2019s\nother capabilities is complex, given the interwoven nature of information and skills within\nlanguage models. This complexity suggests that current approaches to knowledge editing\nmay be more effectively applied in task-specific or domain-specific contexts, where the\nimplications of edits are more predictable and containable.\n\u2022 The dynamic and fluid nature of knowledge, constantly evolving with daily changes and\nnew information, presents a unique challenge. Language models must not only incorporate\nthis evolving knowledge but also adapt their reasoning, actions, and communication meth-\nods accordingly. This ever-changing landscape of knowledge necessitates a more agile and\nresponsive approach to control the LLMs, like implanting a steel stamp of a thought, which\ncan keep pace with the rapid evolution of information and societal norms, and further en-\nsure the safety of LLMs for human society.\nHowever, just as Pinter and Elhadad [183] argues, the stochastic nature of LLMs is not only a source\nof complexity but also a wellspring of creativity and adaptability in various scenarios. Hence, the\npotential of knowledge editing is still worth exploring. Numerous factors, such as prior knowledge,\nexperiences, cultural context, and societal interactions, intricately link and shape the model\u2019s out-\ncomes. To make truly responsible and ethical LLMs in the future, we will likely need a combined\napproach that includes knowledge editing, stronger security measures, more openness, and stronger\naccountability systems. Overall, the shift from traditional fine-tuning to knowledge editing reflects\na deeper evolution in our approach to working with LLMs. It signifies a move towards more spe-\ncialized, nuanced, and sophisticated methods of model adaptation and enhancement, in line with the\ngrowing complexity and capabilities of these advanced language models.\nBroader Impacts\nKnowledge editing, in the context of LLMs, refers to methodologies and techniques aimed at updat-\ning and refining these models more efficiently. By enabling the manipulation of a model\u2019s knowl-\nedge, knowledge editing allows for continuous improvement and adaptation of AI systems, ensuring\nthey remain up-to-date, accurate, and aligned with the desired objectives and values.\nWhile the potential of editing is vast, there is a noticeable variance in the effectiveness of different\nmethods. This disparity, however, does not overshadow the immense promise that these techniques\nhold. The most significant contribution of editing is its ability to deepen our understanding of\nthe knowledge mechanisms in LLMs. By exploring how knowledge is stored, manipulated, and\naccessed within these models, editing techniques can significantly enhance their interpretability and\n29\ntransparency. This aspect is crucial, as it not only improves the usability of these models but also\naids in establishing trust and credibility in their applications.\nIn summary, knowledge editing technology represents a highly promising field with the potential\nto revolutionize how we interact with and utilize LLMs. Its implications extend far beyond mere\nefficiency improvements, touching upon critical aspects like model accessibility, fairness, security,\nand interpretability. As the technology continues to evolve and mature, it is poised to play a pivotal\nrole in shaping the future landscape of artificial intelligence and machine learning.\nAcknowledgments\nThe authors extend their sincere gratitude to Zhiyuan Hu for providing insightful and construc-\ntive feedback on this paper. We also wish to acknowledge the groundbreaking contributions of\nresearchers who have developed knowledge editing methodologies for LLMs. This work was sup-\nported by the National Natural Science Foundation of China (No.62206246), the Fundamental\nResearch Funds for the Central Universities (226-2023-00138), Zhejiang Provincial Natural Sci-\nence Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190),\nYongjiang Talent Introduction Programme (2021A-156-G), CCF-Tencent Rhino-Bird Open Re-\nsearch Fund, Information Technology Center and State Key Lab of CAD&CG, Zhejiang University,\nand NUS-NCS Joint Laboratory (A-0008542-00-00).\nOpen Resources\nKnowEdit (Huggingface): https://huggingface.co/datasets/zjunlp/KnowEdit.\nEasyEdit (Github): https://github.com/zjunlp/EasyEdit.\nContributions\nThe contributions of all authors are listed as follows: Ningyu Zhang, Yunzhi Yao, Peng Wang,\nBozhong Tian and Shumin Deng initiated and organized the research. Ningyu Zhang drafted \u00a71\nand \u00a77, Yunzhi Yao drafted \u00a72, \u00a73 and \u00a76, Yunzhi Yao and Zekun Xi drafted \u00a74 and \u00a75. Yunzhi\nYao, Peng Wang, Bozhong Tian, Zekun Xi, Siyuan Cheng, Ziwen Xu, Shengyu Mao, Jintian Zhang,\nYuansheng Ni participated in benchmark construction and experiments. Mengru Wang, Xin Xu\nsuggested organization and proofread the whole paper. Jia-Chen Gu, Yong Jiang, Pengjun Xie,\nFei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen advised the project,\nsuggested the empirical study and provided computation resources.\nReferences\n[1] J\u00b4er\u02c6ome Seymour Bruner. The course of cognitive growth. American Psychologist, 19:1\u201315,\n1964. URL https://api.semanticscholar.org/CorpusID:145196722.\n[2] J\u00b4er\u02c6ome Seymour Bruner, 1960.\nURL https://api.semanticscholar.org/CorpusID:\n177285798.\n[3] N Jayashri and K Kalaiselvi. Knowledge acquisition\u2013scholarly foundations with knowledge\nmanagement. International Journal of Advanced Studies of Scientific Research, 3(12), 2018.\n[4] Randall Davis, Howard E. Shrobe, and Peter Szolovits. What is a knowledge representation?\nAI Mag., 14(1):17\u201333, 1993. doi: 10.1609/AIMAG.V14I1.1029. URL https://doi.org/\n10.1609/aimag.v14i1.1029.\n[5] Yejin Choi. Knowledge is power: Symbolic knowledge distillation, commonsense morality,\n& multimodal script knowledge. In K. Selcuk Candan, Huan Liu, Leman Akoglu, Xin Luna\nDong, and Jiliang Tang, editors, WSDM \u201922: The Fifteenth ACM International Conference\non Web Search and Data Mining, Virtual Event / Tempe, AZ, USA, February 21 - 25, 2022,\npage 3. ACM, 2022.\ndoi: 10.1145/3488560.3500242.\nURL https://doi.org/10.1145/\n3488560.3500242.\n30\n[6] Hongming Zhang, Xin Liu, Haojie Pan, Haowen Ke, Jiefu Ou, Tianqing Fang, and Yangqiu\nSong. ASER: towards large-scale commonsense knowledge acquisition via higher-order se-\nlectional preference over eventualities.\nArtif. Intell., 309:103740, 2022.\ndoi: 10.1016/J.\nARTINT.2022.103740. URL https://doi.org/10.1016/j.artint.2022.103740.\n[7] Christopher D Manning. Human language understanding & reasoning. Daedalus, 151(2):\n127\u2013138, 2022.\n[8] Karen L. McGraw and Karan Harbison-Briggs.\nKnowledge acquisition - principles and\nguidelines. Prentice Hall, 1990. ISBN 978-0-13-517095-3.\n[9] Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Lin-\nguistic knowledge and transferability of contextual representations. In Jill Burstein, Christy\nDoran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and\nShort Papers), pages 1073\u20131094. Association for Computational Linguistics, 2019.\ndoi:\n10.18653/V1/N19-1112. URL https://doi.org/10.18653/v1/n19-1112.\n[10] Xu Han, Zhengyan Zhang, and Zhiyuan Liu. Knowledgeable machine learning for natural\nlanguage processing. Commun. ACM, 64(11):50\u201351, 2021. doi: 10.1145/3481608. URL\nhttps://doi.org/10.1145/3481608.\n[11] Mohammad Hossein Jarrahi, David Askay, Ali Eshraghi, and Preston Smith. Artificial intelli-\ngence and knowledge management: A partnership between human and ai. Business Horizons,\n66(1):87\u201399, 2023.\n[12] Huajun Chen. Large knowledge model: Perspectives and challenges. CoRR, abs/2312.02706,\n2023. doi: 10.48550/ARXIV.2312.02706. URL https://doi.org/10.48550/arXiv.2312.\n02706.\n[13] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.\n08774. URL https://doi.org/10.48550/arXiv.2303.08774.\n[14] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian\nMin, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen,\nZhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-\nYun Nie, and Ji-Rong Wen. A survey of large language models. CoRR, abs/2303.18223, 2023.\ndoi: 10.48550/ARXIV.2303.18223. URL https://doi.org/10.48550/arXiv.2303.18223.\n[15] Jan Sawicki, Maria Ganzha, and Marcin Paprzycki. The state of the art of natural language\nprocessing-a systematic automated review of nlp literature using nlp techniques. Data Intel-\nligence, pages 1\u201347, 2023.\n[16] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-\noth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00b4elien Ro-\ndriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient\nfoundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/ARXIV.2302.\n13971. URL https://doi.org/10.48550/arXiv.2302.13971.\n[17] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu,\nand Robert McHardy.\nChallenges and applications of large language models.\nCoRR,\nabs/2307.10169, 2023.\ndoi: 10.48550/ARXIV.2307.10169.\nURL https://doi.org/10.\n48550/arXiv.2307.10169.\n[18] Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar,\nMuhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al.\nLarge lan-\nguage models: a comprehensive survey of its applications, challenges, limitations, and future\nprospects. Authorea Preprints, 2023.\n[19] Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun\nZhang, Sumit Kumar Dam, Chu Myaet Thwal, Ye Lin Tun, Le Luang Huy, Dong Uk Kim,\nSung-Ho Bae, Lik-Hang Lee, Yang Yang, Heng Tao Shen, In So Kweon, and Choong Seon\n31\nHong.\nA complete survey on generative AI (AIGC): is chatgpt from GPT-4 to GPT-5\nall you need?\nCoRR, abs/2303.11717, 2023. doi: 10.48550/ARXIV.2303.11717. URL\nhttps://doi.org/10.48550/arXiv.2303.11717.\n[20] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang,\nBing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and\nbeyond. CoRR, abs/2304.13712, 2023. doi: 10.48550/ARXIV.2304.13712. URL https:\n//doi.org/10.48550/arXiv.2304.13712.\n[21] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A\nsurvey on multimodal large language models. CoRR, abs/2306.13549, 2023. doi: 10.48550/\nARXIV.2306.13549. URL https://doi.org/10.48550/arXiv.2306.13549.\n[22] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,\nJiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A\nsurvey on large language model based autonomous agents. CoRR, abs/2308.11432, 2023.\ndoi: 10.48550/ARXIV.2308.11432. URL https://doi.org/10.48550/arXiv.2308.11432.\n[23] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,\nJunzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong,\nYuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin,\nShihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng,\nXipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model\nbased agents: A survey. CoRR, abs/2309.07864, 2023. doi: 10.48550/ARXIV.2309.07864.\nURL https://doi.org/10.48550/arXiv.2309.07864.\n[24] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng,\nYufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng\nQian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining\nYe, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi\nLu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang,\nCheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with\nfoundation models. CoRR, abs/2304.08354, 2023. doi: 10.48550/ARXIV.2304.08354. URL\nhttps://doi.org/10.48550/arXiv.2304.08354.\n[25] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt:\nSolving AI tasks with chatgpt and its friends in huggingface.\nCoRR,\nabs/2303.17580, 2023.\ndoi: 10.48550/ARXIV.2303.17580.\nURL https://doi.org/10.\n48550/arXiv.2303.17580.\n[26] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu,\nSenjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin\nLai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran\nHuang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang.\nSecrets of RLHF in large language models part I: PPO. CoRR, abs/2307.04964, 2023. doi:\n10.48550/ARXIV.2307.04964. URL https://doi.org/10.48550/arXiv.2307.04964.\n[27] Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang,\nYunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang,\nXing Xie, Zheng Zhang, and Yue Zhang. Survey on factuality in large language models:\nKnowledge, retrieval and domain-specificity, 2023.\n[28] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua\nPeng, Ming Liu, Bing Qin, and Ting Liu. A survey of chain of thought reasoning: Advances,\nfrontiers and future. CoRR, abs/2309.15402, 2023. doi: 10.48550/ARXIV.2309.15402. URL\nhttps://doi.org/10.48550/arXiv.2309.15402.\n[29] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi\nYang.\nIs chatgpt a general-purpose natural language processing task solver?\nIn Houda\nBouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2023, Singapore, Decem-\nber 6-10, 2023, pages 1339\u20131384. Association for Computational Linguistics, 2023. URL\nhttps://aclanthology.org/2023.emnlp-main.85.\n32\n[30] Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He, Yiming\nWang, Mark Gerstein, Rui Wang, Gongshen Liu, and Hai Zhao. Igniting language intelli-\ngence: The hitchhiker\u2019s guide from chain-of-thought reasoning to language agents. CoRR,\nabs/2311.11797, 2023.\ndoi: 10.48550/ARXIV.2311.11797.\nURL https://doi.org/10.\n48550/arXiv.2311.11797.\n[31] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large lan-\nguage models.\nTrans. Mach. Learn. Res., 2022, 2022.\nURL https://openreview.net/\nforum?id=yzkSU5zdwD.\n[32] Sanae Lotfi, Marc Finzi, Yilun Kuang, Tim Rudner, Micah Goldblum, and Andrew Wilson.\nNon-vacuous generalization bounds for large language models. In NeurIPS 2023 Workshop\non Mathematics of Modern Machine Learning, 2023.\n[33] Ziming Liu, Ziqian Zhong, and Max Tegmark.\nGrokking as simplification: A nonlinear\ncomplexity perspective. In UniReps: the First Workshop on Unifying Representations in\nNeural Models, 2023.\n[34] Gr\u00b4egoire Del\u00b4etang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein,\nChristopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent\nOrseau, Marcus Hutter, and Joel Veness.\nLanguage modeling is compression.\nCoRR,\nabs/2309.10668, 2023.\ndoi: 10.48550/ARXIV.2309.10668.\nURL https://doi.org/10.\n48550/arXiv.2309.10668.\n[35] Zige Wang, Wanjun Zhong, Yufei Wang, Qi Zhu, Fei Mi, Baojun Wang, Lifeng Shang,\nXin Jiang, and Qun Liu. Data management for large language models: A survey. CoRR,\nabs/2312.01700, 2023.\ndoi: 10.48550/ARXIV.2312.01700.\nURL https://doi.org/10.\n48550/arXiv.2312.01700.\n[36] Wes Gurnee and Max Tegmark. Language models represent space and time, 2023.\n[37] Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. Trends in integration of knowl-\nedge and large language models: A survey and taxonomy of methods, benchmarks, and\napplications.\nCoRR, abs/2311.05876, 2023.\ndoi: 10.48550/ARXIV.2311.05876.\nURL\nhttps://doi.org/10.48550/arXiv.2311.05876.\n[38] Lionel Wong, Gabriel Grand, Alexander K. Lew, Noah D. Goodman, Vikash K. Mansinghka,\nJacob Andreas, and Joshua B. Tenenbaum. From word models to world models: Translating\nfrom natural language to the probabilistic language of thought. CoRR, abs/2306.12672, 2023.\ndoi: 10.48550/ARXIV.2306.12672. URL https://doi.org/10.48550/arXiv.2306.12672.\n[39] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neu-\nrons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 8493\u20138502, Dublin, Ireland,\nMay 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.581.\nURL https://aclanthology.org/2022.acl-long.581.\n[40] Jun Zhao, Zhihao Zhang, Yide Ma, Qi Zhang, Tao Gui, Luhui Gao, and Xuanjing Huang.\nUnveiling A core linguistic region in large language models. CoRR, abs/2310.14928, 2023.\ndoi: 10.48550/ARXIV.2310.14928. URL https://doi.org/10.48550/arXiv.2310.14928.\n[41] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward lay-\ners are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 5484\u20135495, Online and Punta Cana, Dominican Re-\npublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\nemnlp-main.446. URL https://aclanthology.org/2021.emnlp-main.446.\n[42] Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers\nbuild predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, pages 30\u201345, Abu Dhabi,\n33\nUnited Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.\n18653/v1/2022.emnlp-main.3. URL https://aclanthology.org/2022.emnlp-main.3.\n[43] Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda B. Vi\u00b4egas, Hanspeter Pfister, and\nMartin Wattenberg. Emergent world representations: Exploring a sequence model trained\non a synthetic task.\nIn The Eleventh International Conference on Learning Representa-\ntions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https:\n//openreview.net/pdf?id=DeG07_TcZvT.\n[44] Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces.\nIn The Tenth International Conference on Learning Representations, ICLR 2022, Virtual\nEvent, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?\nid=gJcEM8sxHK.\n[45] Zeyuan Allen Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage\nand extraction. CoRR, abs/2309.14316, 2023. doi: 10.48550/ARXIV.2309.14316. URL\nhttps://doi.org/10.48550/arXiv.2309.14316.\n[46] Zeyuan Allen-Zhu and Yuanzhi Li.\nPhysics of language models: Part 3.2, knowledge\nmanipulation.\nCoRR, abs/2309.14402, 2023.\ndoi: 10.48550/ARXIV.2309.14402.\nURL\nhttps://doi.org/10.48550/arXiv.2309.14402.\n[47] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and\nMark Yatskar. Language in a bottle: Language model guided concept bottlenecks for in-\nterpretable image classification.\nIn IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 19187\u2013\n19197. IEEE, 2023. doi: 10.1109/CVPR52729.2023.01839. URL https://doi.org/10.\n1109/CVPR52729.2023.01839.\n[48] Fabio Petroni, Tim Rockt\u00a8aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang\nWu, and Alexander Miller. Language models as knowledge bases?\nIn Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\u2013\n2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:\n10.18653/v1/D19-1250. URL https://aclanthology.org/D19-1250.\n[49] Benjamin Heinzerling and Kentaro Inui. Language models as knowledge bases: On entity\nrepresentations, storage capacity, and paraphrased queries. CoRR, abs/2008.09036, 2020.\nURL https://arxiv.org/abs/2008.09036.\n[50] Cunxiang Wang, Pai Liu, and Yue Zhang.\nCan generative pre-trained language models\nserve as knowledge bases for closed-book qa?\nIn Chengqing Zong, Fei Xia, Wenjie Li,\nand Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021,\npages 3241\u20133251. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.\nACL-LONG.251. URL https://doi.org/10.18653/v1/2021.acl-long.251.\n[51] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [MASK]: learning vs.\nlearning to recall. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-\nT\u00a8ur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou,\neditors, Proceedings of the 2021 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, On-\nline, June 6-11, 2021, pages 5017\u20135033. Association for Computational Linguistics, 2021.\ndoi: 10.18653/V1/2021.NAACL-MAIN.398. URL https://doi.org/10.18653/v1/2021.\nnaacl-main.398.\n[52] Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, and Jin\nXu. Knowledgeable or educated guess? revisiting language models as knowledge bases. In\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long\n34\nPapers), Virtual Event, August 1-6, 2021, pages 1860\u20131874. Association for Computational\nLinguistics, 2021. doi: 10.18653/V1/2021.ACL-LONG.146. URL https://doi.org/10.\n18653/v1/2021.acl-long.146.\n[53] Ruilin Zhao, Feng Zhao, Guandong Xu, Sixiao Zhang, and Hai Jin. Can language mod-\nels serve as temporal knowledge bases?\nIn Yoav Goldberg, Zornitsa Kozareva, and Yue\nZhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022,\nAbu Dhabi, United Arab Emirates, December 7-11, 2022, pages 2024\u20132037. Association for\nComputational Linguistics, 2022. doi: 10.18653/V1/2022.FINDINGS-EMNLP.147. URL\nhttps://doi.org/10.18653/v1/2022.findings-emnlp.147.\n[54] Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisen-\nstein, and William W. Cohen. Time-aware language models as temporal knowledge bases.\nTrans. Assoc. Comput. Linguistics, 10:257\u2013273, 2022. doi: 10.1162/TACL\\ A\\ 00459. URL\nhttps://doi.org/10.1162/tacl_a_00459.\n[55] Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona T. Diab, and Marjan Ghazvininejad.\nA review on language models as knowledge bases. CoRR, abs/2204.06031, 2022. doi: 10.\n48550/ARXIV.2204.06031. URL https://doi.org/10.48550/arXiv.2204.06031.\n[56] Boxi Cao, Hongyu Lin, Xianpei Han, and Le Sun. The life cycle of knowledge in big language\nmodels: A survey. CoRR, abs/2303.07616, 2023. doi: 10.48550/ARXIV.2303.07616. URL\nhttps://doi.org/10.48550/arXiv.2303.07616.\n[57] Paul Youssef, Osman Alperen Koras, Meijie Li, J\u00a8org Schl\u00a8otterer, and Christin Seifert. Give\nme the facts! A survey on factual knowledge probing in pre-trained language models. In\nHouda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Compu-\ntational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 15588\u201315605.\nAssociation for Computational Linguistics, 2023. URL https://aclanthology.org/2023.\nfindings-emnlp.1043.\n[58] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying\nlarge language models and knowledge graphs: A roadmap. CoRR, abs/2306.08302, 2023.\ndoi: 10.48550/ARXIV.2306.08302. URL https://doi.org/10.48550/arXiv.2306.08302.\n[59] Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen, Stefan\nDietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissandrini, Russa Biswas,\nGerard de Melo, Angela Bonifati, Edlira Vakaj, Mauro Dragoni, and Damien Graux. Large\nlanguage models and knowledge graphs: Opportunities and challenges. TGDK, 1(1):2:1\u2013\n2:38, 2023. doi: 10.4230/TGDK.1.1.2. URL https://doi.org/10.4230/TGDK.1.1.2.\n[60] Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-Rad, and Jun Wang. How\ndo large language models capture the ever-changing world knowledge? A review of recent\nadvances. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singa-\npore, December 6-10, 2023, pages 8289\u20138311. Association for Computational Linguistics,\n2023. URL https://aclanthology.org/2023.emnlp-main.516.\n[61] Canyu Chen and Kai Shu.\nCombating misinformation in the age of llms: Opportunities\nand challenges. CoRR, abs/2311.05656, 2023. doi: 10.48550/ARXIV.2311.05656. URL\nhttps://doi.org/10.48550/arXiv.2311.05656.\n[62] Xunjian Yin, Baizhou Huang, and Xiaojun Wan. ALCUNA: large language models meet new\nknowledge. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singa-\npore, December 6-10, 2023, pages 1397\u20131414. Association for Computational Linguistics,\n2023. URL https://aclanthology.org/2023.emnlp-main.87.\n[63] Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca\nCiampaglia, David P. A. Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Y.\nHalevy, Eduard H. Hovy, Heng Ji, Filippo Menczer, Rub\u00b4en M\u00b4\u0131guez, Preslav Nakov, Dietram\nScheufele, Shivam Sharma, and Giovanni Zagni. Factuality challenges in the era of large\nlanguage models. CoRR, abs/2310.05189, 2023. doi: 10.48550/ARXIV.2310.05189. URL\nhttps://doi.org/10.48550/arXiv.2310.05189.\n35\n[64] Hongling Zheng, Li Shen, Anke Tang, Yong Luo, Han Hu, Bo Du, and Dacheng Tao. Learn\nfrom model beyond fine-tuning: A survey. CoRR, abs/2310.08184, 2023. doi: 10.48550/\nARXIV.2310.08184. URL https://doi.org/10.48550/arXiv.2310.08184.\n[65] Xiangyang Liu, Tianxiang Sun, Junliang He, Jiawen Wu, Lingling Wu, Xinyu Zhang, Hao\nJiang, Zhao Cao, Xuanjing Huang, and Xipeng Qiu. Towards efficient NLP: A standard\nevaluation and A strong baseline. In Marine Carpuat, Marie-Catherine de Marneffe, and Iv\u00b4an\nVladimir Meza Ru\u00b4\u0131z, editors, Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 3288\u20133303. Association\nfor Computational Linguistics, 2022. doi: 10.18653/V1/2022.NAACL-MAIN.240. URL\nhttps://doi.org/10.18653/v1/2022.naacl-main.240.\n[66] Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. A survey on green deep\nlearning. CoRR, abs/2111.05193, 2021. URL https://arxiv.org/abs/2111.05193.\n[67] Gaurav Menghani. Efficient deep learning: A survey on making deep learning models smaller,\nfaster, and better. ACM Comput. Surv., 55(12):259:1\u2013259:37, 2023. doi: 10.1145/3578938.\nURL https://doi.org/10.1145/3578938.\n[68] Kai Lv, Shuo Zhang, Tianle Gu, Shuhao Xing, Jiawei Hong, Keyu Chen, Xiaoran Liu,\nYuqing Yang, Honglin Guo, Tengxiao Liu, Yu Sun, Qipeng Guo, Hang Yan, and Xipeng\nQiu. Collie: Collaborative training of large language models in an efficient way. In Yansong\nFeng and Els Lefever, editors, Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2023 - System Demonstrations, Singapore, De-\ncember 6-10, 2023, pages 527\u2013542. Association for Computational Linguistics, 2023. URL\nhttps://aclanthology.org/2023.emnlp-demo.48.\n[69] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun\nChen, and Ningyu Zhang. Editing large language models: Problems, methods, and oppor-\ntunities. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing, pages 10222\u201310240,\nSingapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/\n2023.emnlp-main.632. URL https://aclanthology.org/2023.emnlp-main.632.\n[70] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong L. Knowl-\nedge editing for large language models: A survey, 2023.\n[71] Vittorio Mazzia, Alessandro Pedrani, Andrea Caciolai, Kay Rottmann, and Davide Bernardi.\nA survey on knowledge editing of neural networks.\nCoRR, abs/2310.19704, 2023.\ndoi:\n10.48550/ARXIV.2310.19704. URL https://doi.org/10.48550/arXiv.2310.19704.\n[72] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander\nPan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel\nLi, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song,\nMatt Fredrikson, J. Zico Kolter, and Dan Hendrycks. Representation engineering: A top-\ndown approach to AI transparency. CoRR, abs/2310.01405, 2023. doi: 10.48550/ARXIV.\n2310.01405. URL https://doi.org/10.48550/arXiv.2310.01405.\n[73] Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large\nlanguage models. CoRR, abs/2308.10149, 2023. doi: 10.48550/ARXIV.2308.10149. URL\nhttps://doi.org/10.48550/arXiv.2308.10149.\n[74] El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, L\u02c6e-Nguy\u02c6en\nHoang, Rafael Pinot, and John Stephan. On the impossible safety of large AI models. CoRR,\nabs/2209.15259, 2022.\ndoi: 10.48550/ARXIV.2209.15259.\nURL https://doi.org/10.\n48550/arXiv.2209.15259.\n[75] Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md. Mehrab Tanjim, Sungchul Kim, Franck\nDernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. Bias and fairness in large\nlanguage models: A survey. CoRR, abs/2309.00770, 2023. doi: 10.48550/ARXIV.2309.\n00770. URL https://doi.org/10.48550/arXiv.2309.00770.\n36\n[76] Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Sad-\ndek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao Zhang, Sihao\nWu, Peipei Xu, Dengyu Wu, Andr\u00b4e Freitas, and Mustafa A. Mustafa. A survey of safety\nand trustworthiness of large language models through the lens of verification and valida-\ntion.\nCoRR, abs/2305.11391, 2023.\ndoi: 10.48550/ARXIV.2305.11391.\nURL https:\n//doi.org/10.48550/arXiv.2305.11391.\n[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\n[78] Ganesh Jawahar, Beno\u02c6\u0131t Sagot, and Djam\u00b4e Seddah. What does BERT learn about the structure\nof language?\nIn Anna Korhonen, David Traum, and Llu\u00b4\u0131s M`arquez, editors, Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651\u2013\n3657, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/\nv1/P19-1356. URL https://aclanthology.org/P19-1356.\n[79] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\nassociations in GPT. Advances in Neural Information Processing Systems, 36, 2022.\n[80] Evan Hernandez, Arnab Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob An-\ndreas, Yonatan Belinkov, and David Bau. Linearity of relation decoding in transformer lan-\nguage models. ArXiv, abs/2308.09124, 2023. URL https://api.semanticscholar.org/\nCorpusID:261031179.\n[81] Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Lin-\nguistic knowledge and transferability of contextual representations. In Jill Burstein, Christy\nDoran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages 1073\u20131094, Minneapolis, Minnesota,\nJune 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1112. URL\nhttps://aclanthology.org/N19-1112.\n[82] Xuhui Zhou, Yue Zhang, Leyang Cui, and Dandan Huang. Evaluating commonsense in pre-\ntrained language models. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference,\nIAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2020, New York, NY, USA, February 7-12, 2020, pages 9733\u20139740. AAAI Press, 2020.\ndoi: 10.1609/AAAI.V34I05.6523. URL https://doi.org/10.1609/aaai.v34i05.6523.\n[83] Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras,\nXiming Lu, Sean Welleck, and Yejin Choi. Symbolic knowledge distillation: from general\nlanguage models to commonsense models. In Marine Carpuat, Marie-Catherine de Marn-\neffe, and Iv\u00b4an Vladimir Meza Ru\u00b4\u0131z, editors, Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 4602\u20134625. As-\nsociation for Computational Linguistics, 2022. doi: 10.18653/V1/2022.NAACL-MAIN.341.\nURL https://doi.org/10.18653/v1/2022.naacl-main.341.\n[84] Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson d\u2019Autume, Phil\nBlunsom, and Aida Nematzadeh. A systematic investigation of commonsense knowledge\nin large language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11838\u201311855.\nAssociation for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.\n812. URL https://doi.org/10.18653/v1/2022.emnlp-main.812.\n[85] Boxi Cao, Qiaoyu Tang, Hongyu Lin, Xianpei Han, Jiawei Chen, Tianshu Wang, and\nLe Sun. Retentive or forgetful? diving into the knowledge memorizing mechanism of lan-\nguage models.\nCoRR, abs/2305.09144, 2023.\ndoi: 10.48550/ARXIV.2305.09144.\nURL\nhttps://doi.org/10.48550/arXiv.2305.09144.\n37\n[86] Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, and Chelsea Finn. Fine-\ntuning language models for factuality. CoRR, abs/2311.08401, 2023. doi: 10.48550/ARXIV.\n2311.08401. URL https://doi.org/10.48550/arXiv.2311.08401.\n[87] Shahar Katz and Yonatan Belinkov.\nVISIT: visualizing and interpreting the semantic in-\nformation flow of transformers. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\nFindings of the Association for Computational Linguistics: EMNLP 2023, Singapore, De-\ncember 6-10, 2023, pages 14094\u201314113. Association for Computational Linguistics, 2023.\nURL https://aclanthology.org/2023.findings-emnlp.939.\n[88] Ari Holtzman, Peter West, and Luke Zettlemoyer. Generative models as a complex systems\nscience: How can we make sense of large language model behavior?, 2023.\n[89] Mansi Sakarvadia, Arham Khan, Aswathy Ajith, Daniel Grzenda, Nathaniel Hudson, Andr\u00b4e\nBauer, Kyle Chard, and Ian T. Foster. Attention lens: A tool for mechanistically interpreting\nthe attention head information retrieval mechanism. CoRR, abs/2310.16270, 2023. doi: 10.\n48550/ARXIV.2310.16270. URL https://doi.org/10.48550/arXiv.2310.16270.\n[90] Boxi Cao, Qiaoyu Tang, Hongyu Lin, Xianpei Han, Jiawei Chen, Tianshu Wang, and Le Sun.\nRetentive or forgetful? diving into the knowledge memorizing mechanism of language mod-\nels. arXiv preprint arXiv:2305.09144, 2023.\n[91] William Rudman, Catherine Chen, and Carsten Eickhoff. Outlier dimensions encode task\nspecific knowledge. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP\n2023, Singapore, December 6-10, 2023, pages 14596\u201314605. Association for Computational\nLinguistics, 2023. URL https://aclanthology.org/2023.emnlp-main.901.\n[92] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris\nBertsimas. Finding neurons in a haystack: Case studies with sparse probing. arXiv preprint\narXiv:2305.01610, 2023.\n[93] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training\nof deep bidirectional transformers for language understanding. In Jill Burstein, Christy Do-\nran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and\nShort Papers), pages 4171\u20134186. Association for Computational Linguistics, 2019.\ndoi:\n10.18653/V1/N19-1423. URL https://doi.org/10.18653/v1/n19-1423.\n[94] Daniel D Lundstrom, Tianjian Huang, and Meisam Razaviyayn. A rigorous study of in-\ntegrated gradients method and extensions to internal neuron attributions. In International\nConference on Machine Learning, pages 14485\u201314508. PMLR, 2022.\n[95] Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding\nskill neurons in pre-trained transformer-based language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, pages 11132\u201311152,\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2022.emnlp-main.765. URL https://aclanthology.org/2022.\nemnlp-main.765.\n[96] Divya Nori, Shivali Singireddy, and Marina Ten Have. Identification of knowledge neurons\nin protein language models. arXiv preprint arXiv:2312.10770, 2023.\n[97] Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. Journey to the center of the\nknowledge neurons: Discoveries of language-independent knowledge neurons and degenerate\nknowledge neurons, 2023.\n[98] Jun Zhao, Zhihao Zhang, Yide Ma, Qi Zhang, Tao Gui, Luhui Gao, and Xuanjing Huang.\nUnveiling a core linguistic region in large language models. arXiv preprint arXiv:2310.14928,\n2023.\n38\n[99] Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen.\nKnowledge is a region in weight space for fine-tuned language models. In Houda Bouamor,\nJuan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguis-\ntics: EMNLP 2023, pages 1350\u20131370, Singapore, December 2023. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2023.findings-emnlp.95. URL https://aclanthology.\norg/2023.findings-emnlp.95.\n[100] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh,\nIlya Sutskever, Jan Leike, Jeff Wu, and William Saunders.\nLanguage models can\nexplain neurons in language models, 2023.\nURL https://openai.com/research/\nlanguage-models-can-explain-neurons-in-language-models.\n[101] Anonymous. What does the knowledge neuron thesis have to do with knowledge? In Sub-\nmitted to The Twelfth International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=2HJRwwbV3G. under review.\n[102] Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson.\nDissecting recall of\nfactual associations in auto-regressive language models. CoRR, abs/2304.14767, 2023. doi:\n10.48550/arXiv.2304.14767. URL https://doi.org/10.48550/arXiv.2304.14767.\n[103] Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri`a\nGarriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. In\nThirty-seventh Conference on Neural Information Processing Systems, 2023.\n[104] Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.\nInterpretability in the wild: a circuit for indirect object identification in GPT-2 small. In\nThe Eleventh International Conference on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=NpsVSN6o4ul.\n[105] Alex Foote, Neel Nanda, Esben Kran, Ioannis Konstas, Shay Cohen, and Fazl Barez. Neuron\nto graph: Interpreting language model neurons at scale. arXiv preprint arXiv:2305.19911,\n2023.\n[106] Jie Ren, Mingjie Li, Qirui Chen, Huiqi Deng, and Quanshi Zhang. Defining and quantifying\nthe emergence of sparse concepts in dnns. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 20280\u2013\n20289. IEEE, 2023. doi: 10.1109/CVPR52729.2023.01942. URL https://doi.org/10.\n1109/CVPR52729.2023.01942.\n[107] Mingjie Li and Quanshi Zhang. Does a neural network really encode symbolic concepts?\nIn Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,\nand Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023,\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning\nResearch, pages 20452\u201320469. PMLR, 2023. URL https://proceedings.mlr.press/v202/\nli23at.html.\n[108] Ning Ding, Yujia Qin, Guang Yang, Fu Wei, Zonghan Yang, Yusheng Su, Shengding Hu,\nYulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu,\nHaitao Zheng, Jianfei Chen, Y. Liu, Jie Tang, Juanzi Li, and Maosong Sun.\nParameter-\nefficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence,\n5:220\u2013235, 2023. URL https://api.semanticscholar.org/CorpusID:257316425.\n[109] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide\nto parameter-efficient fine-tuning. CoRR, abs/2303.15647, 2023. doi: 10.48550/ARXIV.\n2303.15647. URL https://doi.org/10.48550/arXiv.2303.15647.\n[110] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Larous-\nsilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer\nlearning for NLP.\nIn Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceed-\nings of the 36th International Conference on Machine Learning, volume 97 of Proceed-\nings of Machine Learning Research, pages 2790\u20132799. PMLR, 09\u201315 Jun 2019.\nURL\nhttps://proceedings.mlr.press/v97/houlsby19a.html.\n39\n[111] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\nChen, et al. Lora: Low-rank adaptation of large language models. In International Conference\non Learning Representations, 2021.\n[112] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE:\nenhanced language representation with informative entities. In Anna Korhonen, David R.\nTraum, and Llu\u00b4\u0131s M`arquez, editors, Proceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume\n1: Long Papers, pages 1441\u20131451. Association for Computational Linguistics, 2019. doi:\n10.18653/V1/P19-1139. URL https://doi.org/10.18653/v1/p19-1139.\n[113] Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang,\nLuo Si, and Huajun Chen. Knowprompt: Knowledge-aware prompt-tuning with synergistic\noptimization for relation extraction. In Fr\u00b4ed\u00b4erique Laforest, Rapha\u00a8el Troncy, Elena Simperl,\nDeepak Agarwal, Aristides Gionis, Ivan Herman, and Lionel M\u00b4edini, editors, WWW \u201922:\nThe ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022, pages\n2778\u20132788. ACM, 2022. doi: 10.1145/3485447.3511998. URL https://doi.org/10.1145/\n3485447.3511998.\n[114] Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. PTR: prompt tuning with\nrules for text classification. AI Open, 3:182\u2013192, 2022. doi: 10.1016/J.AIOPEN.2022.11.003.\nURL https://doi.org/10.1016/j.aiopen.2022.11.003.\n[115] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,\nand Haofen Wang. Retrieval-augmented generation for large language models: A survey.\narXiv preprint arXiv:2312.10997, 2023.\n[116] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry W. Wei, Jason Wei, Chris Tar,\nYun-Hsuan Sung, Denny Zhou, Quoc V. Le, and Thang Luong. Freshllms: Refreshing large\nlanguage models with search engine augmentation. CoRR, abs/2310.03214, 2023. doi: 10.\n48550/ARXIV.2310.03214. URL https://doi.org/10.48550/arXiv.2310.03214.\n[117] Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. Fine-tuning or retrieval?\ncomparing knowledge injection in llms. arXiv preprint arXiv:2312.05934, 2023.\n[118] Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Acl 2023 tutorial: Retrieval-based\nlanguage models and applications. ACL 2023, 2023.\n[119] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Na-\nman Goyal, Heinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8aschel, Sebastian\nRiedel, and Douwe Kiela.\nRetrieval-augmented generation for knowledge-intensive nlp\ntasks.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Ad-\nvances in Neural Information Processing Systems, volume 33, pages 9459\u20139474. Curran\nAssociates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/\nfile/6b493230205f780e1bc26945df7481e5-Paper.pdf.\n[120] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval\naugmented language model pre-training. In Hal Daum\u00b4e III and Aarti Singh, editors, Pro-\nceedings of the 37th International Conference on Machine Learning, volume 119 of Pro-\nceedings of Machine Learning Research, pages 3929\u20133938. PMLR, 13\u201318 Jul 2020. URL\nhttps://proceedings.mlr.press/v119/guu20a.html.\n[121] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-\nBrown, and Yoav Shoham. In-context retrieval-augmented language models. Transactions of\nthe Association for Computational Linguistics, 2023. URL https://arxiv.org/abs/2302.\n00083.\n[122] Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William W. Cohen.\nMention memory: incorporating textual knowledge into transformers through entity mention\nattention. In International Conference on Learning Representations, 2022. URL https:\n//openreview.net/forum?id=OY1A8ejQgEX.\n40\n[123] Yunzhi Yao, Shaohan Huang, Li Dong, Furu Wei, Huajun Chen, and Ningyu Zhang. Kformer:\nKnowledge injection in transformer feed-forward layers. In Wei Lu, Shujian Huang, Yu Hong,\nand Xiabing Zhou, editors, Natural Language Processing and Chinese Computing, pages\n131\u2013143, Cham, 2022. Springer International Publishing. ISBN 978-3-031-17120-8.\n[124] Thibault F\u00b4evry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom\nKwiatkowski. Entities as experts: Sparse memory access with entity supervision. In Bonnie\nWebber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing (EMNLP), pages 4937\u20134951,\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nemnlp-main.400. URL https://aclanthology.org/2020.emnlp-main.400.\n[125] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Gen-\neralization through memorization: Nearest neighbor language models.\nIn International\nConference on Learning Representations, 2020. URL https://openreview.net/forum?id=\nHklBjCEKvH.\n[126] Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmen-\ntation. In Empirical Methods in Natural Language Processing (EMNLP), 2022.\n[127] Dani Yogatama, Cyprien de Masson d\u2019Autume, and Lingpeng Kong. Adaptive semiparamet-\nric language models. Transactions of the Association for Computational Linguistics, 9:362\u2013\n373, 2021. doi: 10.1162/tacl a 00371. URL https://aclanthology.org/2021.tacl-1.22.\n[128] Xiang Chen, Lei Li, Ningyu Zhang, Xiaozhuan Liang, Shumin Deng, Chuanqi Tan, Fei\nHuang, Luo Si, and Huajun Chen. Decoupling knowledge from memorization: Retrieval-\naugmented prompt learning. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/\npaper/2022/hash/97011c648eda678424f9292dadeae72e-Abstract-Conference.html.\n[129] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in\nretrieval-augmented generation. CoRR, abs/2309.01431, 2023. doi: 10.48550/ARXIV.2309.\n01431. URL https://doi.org/10.48550/arXiv.2309.01431.\n[130] Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu,\nJi-Rong Wen, and Haifeng Wang. Investigating the factual knowledge boundary of large\nlanguage models with retrieval augmentation. CoRR, abs/2307.11019, 2023. doi: 10.48550/\nARXIV.2307.11019. URL https://doi.org/10.48550/arXiv.2307.11019.\n[131] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael\nSch\u00a8arli, and Denny Zhou. Large language models can be easily distracted by irrelevant con-\ntext, 2023.\n[132] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis,\nGregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting\nin classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366\u20133385, 2022. doi:\n10.1109/TPAMI.2021.3057446. URL https://doi.org/10.1109/TPAMI.2021.3057446.\n[133] Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li, Guilin Qi, and Gholamreza Haf-\nfari. Pretrained language model in continual learning: A comparative study. In The Tenth\nInternational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-\n29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=figzpGMrdD.\n[134] Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu.\nDeep class-incremental learning: A survey. CoRR, abs/2302.03648, 2023. doi: 10.48550/\nARXIV.2302.03648. URL https://doi.org/10.48550/arXiv.2302.03648.\n[135] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual\nlearning: Theory, method and application. CoRR, abs/2302.00487, 2023. doi: 10.48550/\nARXIV.2302.00487. URL https://doi.org/10.48550/arXiv.2302.00487.\n[136] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap, and Greg Wayne. Expe-\nrience replay for continual learning. In Neural Information Processing Systems, 2018. URL\nhttps://api.semanticscholar.org/CorpusID:53860287.\n41\n[137] Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin,\nand Tinne Tuytelaars. Online continual learning with maximally interfered retrieval. ArXiv,\nabs/1908.04742, 2019. URL https://api.semanticscholar.org/CorpusID:199552250.\n[138] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins,\nAndrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,\nDemis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catas-\ntrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114:\n3521 \u2013 3526, 2016. URL https://api.semanticscholar.org/CorpusID:4704285.\n[139] Tom Mitchell, William Cohen, Estevam Hruschka, Partha Talukdar, Bishan Yang, Justin Bet-\nteridge, Andrew Carlson, Bhavana Dalvi, Matt Gardner, Bryan Kisiel, et al. Never-ending\nlearning. Communications of the ACM, 61(5):103\u2013115, 2018.\n[140] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by\niterative pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern\nRecognition, pages 7765\u20137773, 2018.\n[141] Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning\nwith a network of experts. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3366\u20133375, 2017.\n[142] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi\nYin, and Quoc Viet Hung Nguyen.\nA survey of machine unlearning.\narXiv preprint\narXiv:2209.02299, 2022.\n[143] Ga Wu, Masoud Hashemi, and Christopher Srinivasa. Puma: Performance unchanged model\naugmentation for training data removal. In AAAI Conference on Artificial Intelligence, 2022.\n[144] Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning, 2023.\n[145] Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and Weiqiang Zhang. Knowl-\nedge unlearning for llms: Tasks, methods, and challenges, 2023.\n[146] Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and\nStella Biderman. LEACE: Perfect linear concept erasure in closed form. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023. URL https://openreview.\nnet/forum?id=awIpKpwTwF.\n[147] Jiaao Chen and Diyi Yang. Unlearn what you want to forget: Efficient unlearning for LLMs.\nIn Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Processing, pages 12041\u201312052, Singa-\npore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.\nemnlp-main.738. URL https://aclanthology.org/2023.emnlp-main.738.\n[148] Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, and\nJin Xu. Knowledgeable or educated guess? revisiting language models as knowledge bases.\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on Natural Language Processing (Volume\n1: Long Papers), pages 1860\u20131874, Online, August 2021. Association for Computational\nLinguistics. doi: 10.18653/v1/2021.acl-long.146. URL https://aclanthology.org/2021.\nacl-long.146.\n[149] Tim Schott, Daniel Furman, and Shreshta Bhat. Polyglot or not? measuring multilingual\nencyclopedic knowledge in foundation models. In Houda Bouamor, Juan Pino, and Kalika\nBali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pages 11238\u201311253, Singapore, December 2023. Association for Computational\nLinguistics.\ndoi: 10.18653/v1/2023.emnlp-main.691.\nURL https://aclanthology.org/\n2023.emnlp-main.691.\n[150] Shibo Hao, Bowen Tan, Kaiwen Tang, Bin Ni, Xiyan Shao, Hengzhe Zhang, Eric Xing,\nand Zhiting Hu. BertNet: Harvesting knowledge graphs with arbitrary relations from pre-\ntrained language models. In Findings of the Association for Computational Linguistics: ACL\n42\n2023, pages 5000\u20135015, Toronto, Canada, July 2023. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2023.findings-acl.309. URL https://aclanthology.org/2023.\nfindings-acl.309.\n[151] Chenguang Wang, Xiao Liu, and Dawn Song. Language models are open knowledge graphs,\n2021. URL https://openreview.net/forum?id=aRTRjVPkm-.\n[152] Anshita Gupta, Debanjan Mondal, Akshay Krishna Sheshadri, Wenlong Zhao, Xiang Lor-\nraine Li, Sarah Wiegreffe, and Niket Tandon. Editing commonsense knowledge in gpt, 2023.\n[153] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn.\nMemory-based model editing at scale. In International Conference on Machine Learning,\n2022.\n[154] Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt edit-\ning to improve GPT-3 after deployment. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 2833\u20132861, Abu Dhabi, United Arab Emi-\nrates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nemnlp-main.183. URL https://aclanthology.org/2022.emnlp-main.183.\n[155] Zexuan Zhong, Zhengxuan Wu, Christopher Manning, Christopher Potts, and Danqi Chen.\nMQuAKE: Assessing knowledge editing in language models via multi-hop questions. In\nHouda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Processing, pages 15686\u201315702, Singa-\npore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.\nemnlp-main.971. URL https://aclanthology.org/2023.emnlp-main.971.\n[156] Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao\nChang. Can we edit factual knowledge by in-context learning?\nIn Houda Bouamor, Juan\nPino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing, pages 4862\u20134876, Singapore, December 2023. As-\nsociation for Computational Linguistics.\ndoi: 10.18653/v1/2023.emnlp-main.296.\nURL\nhttps://aclanthology.org/2023.emnlp-main.296.\n[157] Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. Evaluating the ripple\neffects of knowledge editing in language models, 2023.\n[158] Hengrui Gu, Kaixiong Zhou, Xiaotian Han, Ninghao Liu, Ruobing Wang, and Xin Wang.\nPokemqa: Programmable knowledge editing for multi-hop question answering, 2023.\n[159] Shikhar Murty, Christopher Manning, Scott Lundberg, and Marco Tulio Ribeiro.\nFixing\nmodel bugs with natural language patches. In Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing, pages 11600\u201311613, Abu Dhabi, United\nArab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/\nv1/2022.emnlp-main.797. URL https://aclanthology.org/2022.emnlp-main.797.\n[160] Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li. Calibrat-\ning factual knowledge in pretrained language models. In Findings of the Association for\nComputational Linguistics: EMNLP 2022, pages 5937\u20135947, Abu Dhabi, United Arab Emi-\nrates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nfindings-emnlp.438. URL https://aclanthology.org/2022.findings-emnlp.438.\n[161] Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong.\nTransformer-patcher: One mistake worth one neuron. In The Eleventh International Con-\nference on Learning Representations, 2023.\nURL https://openreview.net/forum?id=\n4oYUGeGBPm.\n[162] Evan Hernandez, Belinda Z. Li, and Jacob Andreas. Inspecting and editing knowledge repre-\nsentations in language models, 2023.\n[163] Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh\nGhassemi. Aging with grace: Lifelong model editing with discrete key-value adaptors. ArXiv,\nabs/2211.11031, 2022. URL https://api.semanticscholar.org/CorpusID:253735429.\n43\n[164] Suhang Wu, Minlong Peng, Yue Chen, Jinsong Su, and Mingming Sun. Eva-kellm: A new\nbenchmark for evaluating knowledge editing of llms. CoRR, abs/2308.09954, 2023. doi:\n10.48550/ARXIV.2308.09954. URL https://doi.org/10.48550/arXiv.2308.09954.\n[165] Lang Yu, Qin Chen, Jie Zhou, and Liang He. Melo: Enhancing model editing with neuron-\nindexed dynamic lora, 2023.\n[166] Ankit Singh Rawat, Chen Zhu, Daliang Li, Felix Yu, Manzil Zaheer, Sanjiv Kumar, and Sri-\nnadh Bhojanapalli. Modifying memories in transformer models. In International Conference\non Machine Learning (ICML) 2021, 2020.\n[167] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin, Sergei Popov, and Artem Babenko.\nEditable neural networks. In International Conference on Learning Representations, 2020.\nURL https://openreview.net/forum?id=HJedXaEtvS.\n[168] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-\ncessing, pages 6491\u20136506, Online and Punta Cana, Dominican Republic, November 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.522. URL\nhttps://aclanthology.org/2021.emnlp-main.522.\n[169] Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov,\nMohit Bansal, and Srinivasan Iyer. Methods for measuring, updating, and visualizing factual\nbeliefs in language models. In Proceedings of the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics, pages 2714\u20132731, Dubrovnik, Croatia, May\n2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.eacl-main.199.\nURL https://aclanthology.org/2023.eacl-main.199.\n[170] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning.\nFast model editing at scale. In International Conference on Learning Representations, 2022.\nURL https://openreview.net/forum?id=0DcZxeWfOPt.\n[171] Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-\nediting memory in a transformer. In The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.net/forum?id=MkbcAHIYgyS.\n[172] Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, and Jie Yu. Pmet: Precise model\nediting in a transformer. In AAAI, 2024.\n[173] Chenmien Tan, Ge Zhang, and Jie Fu. Massive editing for large language models via meta\nlearning. arXiv, 2311.04661, 2023. URL https://arxiv.org/pdf/2311.04661.pdf.\n[174] Jun-Yu Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu, and Cong Liu. Untying the reversal\ncurse via bidirectional language model editing, 2023.\n[175] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. Self-knowledge guided retrieval augmenta-\ntion for large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Find-\nings of the Association for Computational Linguistics: EMNLP 2023, pages 10303\u201310315,\nSingapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/\n2023.findings-emnlp.691. URL https://aclanthology.org/2023.findings-emnlp.691.\n[176] Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and\nXu Sun. Recall: A benchmark for llms robustness against external counterfactual knowledge,\n2023.\n[177] Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He,\nand Yulia Tsvetkov. Resolving knowledge conflicts in large language models, 2023.\n[178] Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn\nsloth: Revealing the behavior of large language models in knowledge conflicts, 2023.\n44\n[179] Qinan Yu, Jack Merullo, and Ellie Pavlick. Characterizing mechanisms for factual recall in\nlanguage models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the\n2023 Conference on Empirical Methods in Natural Language Processing, pages 9924\u20139959,\nSingapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/\n2023.emnlp-main.615. URL https://aclanthology.org/2023.emnlp-main.615.\n[180] Shiwen Ni, Dingwei Chen, Chengming Li, Xiping Hu, Ruifeng Xu, and Min Yang. Forgetting\nbefore learning: Utilizing parametric arithmetic for knowledge updating in large language\nmodels. arXiv preprint arXiv:2311.08011, 2023.\n[181] Xiaoqi Han, Ru Li, Xiaoli Li, and Jeff Z. Pan. A divide and conquer framework for knowledge\nediting. Knowledge-Based Systems, 279:110826, 2023. ISSN 0950-7051. doi: https://doi.org/\n10.1016/j.knosys.2023.110826. URL https://www.sciencedirect.com/science/article/\npii/S0950705123005762.\n[182] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz\nKorbak, and Owain Evans. The reversal curse: Llms trained on \u201da is b\u201d fail to learn \u201db is a\u201d,\n2023.\n[183] Yuval Pinter and Michael Elhadad. Emptying the ocean with a spoon: Should we edit models?\nIn Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Com-\nputational Linguistics: EMNLP 2023, pages 15164\u201315172, Singapore, December 2023. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.1012. URL\nhttps://aclanthology.org/2023.findings-emnlp.1012.\n[184] Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer.\nZero-shot relation ex-\ntraction via reading comprehension. In Proceedings of the 21st Conference on Computa-\ntional Natural Language Learning (CoNLL 2017), pages 333\u2013342, Vancouver, Canada, Au-\ngust 2017. Association for Computational Linguistics. doi: 10.18653/v1/K17-1034. URL\nhttps://aclanthology.org/K17-1034.\n[185] Potsawee Manakul, Adian Liusie, and Mark JF Gales.\nSelfcheckgpt:\nZero-resource\nblack-box hallucination detection for generative large language models.\narXiv preprint\narXiv:2303.08896, 2023.\n[186] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Ha-\njishirzi. When not to trust language models: Investigating effectiveness of parametric and\nnon-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 9802\u20139822, Toronto, Canada,\nJuly 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.546.\nURL https://aclanthology.org/2023.acl-long.546.\n[187] Yoichi Ishibashi and Hidetoshi Shimodaira. Knowledge sanitization of large language mod-\nels. arXiv preprint arXiv:2309.11852, 2023.\n[188] Zichao Li, Ines Arous, Siva Reddy, and Jackie Cheung. Evaluating dependencies in fact\nediting for language models: Specificity and implication awareness. In Houda Bouamor, Juan\nPino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics:\nEMNLP 2023, pages 7623\u20137636, Singapore, December 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.findings-emnlp.511. URL https://aclanthology.org/\n2023.findings-emnlp.511.\n[189] Yang Xu, Yutai Hou, Wanxiang Che, and Min Zhang. Language anisotropic cross-lingual\nmodel editing. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings\nof the Association for Computational Linguistics: ACL 2023, pages 5554\u20135569, Toronto,\nCanada, July 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.\nfindings-acl.343. URL https://aclanthology.org/2023.findings-acl.343.\n[190] Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao, and Jiarong Xu. Cross-lingual knowl-\nedge editing in large language models, 2023.\n[191] Weixuan Wang, Barry Haddow, and Alexandra Birch.\nRetrieval-augmented multilingual\nknowledge editing, 2023.\n45\n[192] Yasumasa Onoe, Michael Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol Choi.\nCan LMs learn new entities from descriptions? challenges in propagating injected knowl-\nedge.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 5469\u20135485, Toronto, Canada, July 2023. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.300. URL https:\n//aclanthology.org/2023.acl-long.300.\n[193] Xunjian Yin, Jin Jiang, Liming Yang, and Xiaojun Wan. History matters: Temporal knowl-\nedge editing in large language model. arXiv preprint arXiv:2312.05497, 2023.\n[194] Yifan Wei, Xiaoyan Yu, Huanhuan Ma, Fangyu Lei, Yixuan Weng, Ran Song, and Kang Liu.\nAssessing knowledge editing in language models via relation perspective. arXiv preprint\narXiv:2311.09053, 2023.\n[195] Afra Feyza Aky\u00a8urek, Eric Pan, Garry Kuwanto, and Derry Wijaya. Dune: Dataset for unified\nediting. arXiv preprint arXiv:2311.16087, 2023.\n[196] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernan-\ndes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal,\nAnthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez,\nMadian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor\nMihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,\nRashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Sub-\nramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan,\nPuxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:\nOpen foundation and fine-tuned chat models, 2023.\n[197] Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi,\nSiyuan Cheng, Kangwei Liu, Guozhou Zheng, et al. Easyedit: An easy-to-use knowledge\nediting framework for large language models. arXiv preprint arXiv:2308.07269, 2023.\n[198] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A\nquestion answering challenge targeting commonsense knowledge. CoRR, abs/1811.00937,\n2018. URL http://arxiv.org/abs/1811.00937.\n[199] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning\nabout physical commonsense in natural language. CoRR, abs/1911.11641, 2019. URL http:\n//arxiv.org/abs/1911.11641.\n[200] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\u2019t give me the details, just the\nsummary! topic-aware convolutional neural networks for extreme summarization. CoRR,\nabs/1808.08745, 2018. URL http://arxiv.org/abs/1808.08745.\n[201] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale\ndistantly supervised challenge dataset for reading comprehension. CoRR, abs/1705.03551,\n2017. URL http://arxiv.org/abs/1705.03551.\n[202] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding, 2021.\n[203] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating founda-\ntion models, 2023.\n[204] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation\nmodels. https://github.com/open-compass/opencompass, 2023.\n[205] Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, and Huajun Chen. Unveiling\nthe pitfalls of knowledge editing for large language models. CoRR, abs/2310.02129, 2023.\ndoi: 10.48550/ARXIV.2310.02129. URL https://doi.org/10.48550/arXiv.2310.02129.\n46\n[206] Xiaoqi Han, Ru Li, Hongye Tan, Wang Yuanlong, Qinghua Chai, and Jeff Pan. Improv-\ning sequential model editing with fact retrieval. In Houda Bouamor, Juan Pino, and Ka-\nlika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023,\npages 11209\u201311224, Singapore, December 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.findings-emnlp.749. URL https://aclanthology.org/2023.\nfindings-emnlp.749.\n[207] Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.\nAnalyzing transformers in em-\nbedding space. In Proceedings of the 61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages 16124\u201316170, Toronto, Canada, July\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.893. URL\nhttps://aclanthology.org/2023.acl-long.893.\n[208] Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform\nediting? surprising differences in causality-based localization vs. knowledge editing in lan-\nguage models, 2023.\n[209] Ting-Yun Chang, Jesse Thomason, and Robin Jia. Do localization methods actually localize\nmemorized data in llms?, 2023.\n[210] Yiming Ju and Zheng Zhang. Klob: a benchmark for assessing knowledge locating methods\nin language models, 2023.\n[211] Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, and Antoine Bosselut. Reckoning:\nReasoning through dynamic knowledge encoding, 2023.\n[212] Shankar Padmanabhan, Yasumasa Onoe, Michael J. Q. Zhang, Greg Durrett, and Eunsol\nChoi. Propagating knowledge updates to lms through distillation, 2023.\n[213] Yucheng Shi, Shaochen Xu, Zhengliang Liu, Tianming Liu, Xiang Li, and Ninghao Liu.\nMededit: Model editing for medical question answering with external knowledge bases, 2023.\n[214] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER:\na large-scale dataset for fact extraction and VERification.\nIn Proceedings of the 2018\nConference of the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long Papers), pages 809\u2013819, New Or-\nleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/\nN18-1074. URL https://aclanthology.org/N18-1074.\n[215] Tal Schuster, Adam Fisch, and Regina Barzilay. Get your vitamin C! robust fact verification\nwith contrastive evidence. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\npages 624\u2013643, Online, June 2021. Association for Computational Linguistics.\ndoi: 10.\n18653/v1/2021.naacl-main.52. URL https://aclanthology.org/2021.naacl-main.52.\n[216] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He.\nDola: Decoding by contrasting layers improves factuality in large language models, 2023.\n[217] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Ha-\njishirzi, and Ali Farhadi. Editing models with task arithmetic. In The Eleventh International\nConference on Learning Representations, 2023. URL https://openreview.net/forum?id=\n6t0Kwf8-jrj.\n[218] Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and\nAleksander Madry. Editing a classifier by rewriting its prediction rules. Advances in Neural\nInformation Processing Systems, 34:23359\u201323373, 2021.\n[219] Davis Brown, Charles Godfrey, Cody Nizinski, Jonathan Tu, and Henry Kvinge. Edit at your\nown risk: evaluating the robustness of edited models to distribution shifts, 2023.\n[220] Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task arithmetic in the\ntangent space: Improved editing of pre-trained models. In Thirty-seventh Conference on\nNeural Information Processing Systems, 2023. URL https://openreview.net/forum?id=\n0A9f2jZDGW.\n47\n[221] Jian Gu, Chunyang Chen, and Aldeida Aleti. Neuron patching: Neuron-level model editing\non code generation and llms, 2023.\n[222] Siyuan Cheng, Ningyu Zhang, Bozhong Tian, Zelin Dai, Feiyu Xiong, Wei Guo, and Huajun\nChen. Editing language model-based knowledge graph embeddings. AAAI, 2024.\n[223] Jiali Cheng, George Dasoulas, Huan He, Chirag Agarwal, and Marinka Zitnik. GNNDelete:\nA general strategy for unlearning in graph neural networks. In The Eleventh International\nConference on Learning Representations, 2023. URL https://openreview.net/forum?id=\nX9yCkmT5Qrl.\n[224] Zirui Liu, Zhimeng Jiang, Shaochen Zhong, Kaixiong Zhou, Li Li, Rui Chen, Soo-Hyun\nChoi, and Xia Hu. Editable graph neural network for node classifications, 2023.\n[225] Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han, and Pengcheng He. Seeking neural\nnuggets: Knowledge transfer in large language models from a parametric perspective. arXiv\npreprint arXiv:2310.11451, 2023.\n[226] Deniz Bayazit, Negar Foroutan, Zeming Chen, Gail Weiss, and Antoine Bosselut.\nDis-\ncovering knowledge-critical subnetworks in pretrained language models.\narXiv preprint\narXiv:2310.03084, 2023.\n[227] Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, and Li Shen. Deep model fusion:\nA survey. CoRR, abs/2309.15698, 2023. doi: 10.48550/ARXIV.2309.15698. URL https:\n//doi.org/10.48550/arXiv.2309.15698.\n[228] Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, and Lijuan Wang. An empirical\nstudy of multimodal model merging. In Houda Bouamor, Juan Pino, and Kalika Bali, edi-\ntors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore,\nDecember 6-10, 2023, pages 1563\u20131575. Association for Computational Linguistics, 2023.\nURL https://aclanthology.org/2023.findings-emnlp.105.\n[229] Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient\nmodules with arithmetic operations. CoRR, abs/2306.14870, 2023. doi: 10.48550/ARXIV.\n2306.14870. URL https://doi.org/10.48550/arXiv.2306.14870.\n[230] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu, and Lichao Sun.\nA comprehensive survey of ai-generated content (AIGC): A history of generative AI from\nGAN to chatgpt. CoRR, abs/2303.04226, 2023. doi: 10.48550/ARXIV.2303.04226. URL\nhttps://doi.org/10.48550/arXiv.2303.04226.\n[231] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and\nJianfeng Gao. Multimodal foundation models: From specialists to general-purpose assis-\ntants.\nCoRR, abs/2309.10020, 2023.\ndoi: 10.48550/ARXIV.2309.10020.\nURL https:\n//doi.org/10.48550/arXiv.2309.10020.\n[232] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-\nYan Zhu. Ablating concepts in text-to-image diffusion models. CoRR, abs/2303.13516, 2023.\ndoi: 10.48550/ARXIV.2303.13516. URL https://doi.org/10.48550/arXiv.2303.13516.\n[233] Samyadeep Basu, Nanxuan Zhao, Vlad Morariu, Soheil Feizi, and Varun Manjunatha. Lo-\ncalizing and editing knowledge in text-to-image generative models, 2023.\n[234] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. CoRR, abs/2302.05543, 2023. doi: 10.48550/ARXIV.2302.05543. URL https:\n//doi.org/10.48550/arXiv.2302.05543.\n[235] Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, and\nNingyu Zhang. Can we edit multimodal large language models?\nCoRR, abs/2310.08475,\n2023. doi: 10.48550/ARXIV.2310.08475. URL https://doi.org/10.48550/arXiv.2310.\n08475.\n[236] Dana Arad, Hadas Orgad, and Yonatan Belinkov. Refact: Updating text-to-image models by\nediting the text encoder, 2023.\n48\n[237] Haowen Pan, Yixin Cao, Xiaozhi Wang, and Xun Yang. Finding and editing multi-modal\nneurons in pre-trained transformer, 2023.\n[238] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing con-\ncepts from diffusion models, 2023.\n[239] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan,\nZhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai\nPan, Aidan O\u2019Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong\nYang, Yizhou Wang, Song-Chun Zhu, Yike Guo, and Wen Gao. AI alignment: A compre-\nhensive survey. CoRR, abs/2310.19852, 2023. doi: 10.48550/ARXIV.2310.19852. URL\nhttps://doi.org/10.48550/arXiv.2310.19852.\n[240] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng\nShang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey.\nCoRR, abs/2307.12966, 2023. doi: 10.48550/ARXIV.2307.12966. URL https://doi.org/\n10.48550/arXiv.2307.12966.\n[241] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xin-\nwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment: A survey. CoRR,\nabs/2309.15025, 2023.\ndoi: 10.48550/ARXIV.2309.15025.\nURL https://doi.org/10.\n48550/arXiv.2309.15025.\n[242] Dongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Ziyang Chen, Baotian Hu, Aiguo Wu,\nand Min Zhang. A survey of large language models attribution. CoRR, abs/2311.03731, 2023.\ndoi: 10.48550/ARXIV.2311.03731. URL https://doi.org/10.48550/arXiv.2311.03731.\n[243] Amruta Kale, Tin Nguyen, Frederick C. Harris Jr., Chenhao Li, Jiyin Zhang, and Xiaogang\nMa. Provenance documentation to enable explainable and trustworthy AI: A literature review.\nData Intell., 5(1):139\u2013162, 2023. doi: 10.1162/DINT\\ A\\ 00119. URL https://doi.org/\n10.1162/dint_a_00119.\n[244] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng,\nYegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and\nguideline for evaluating large language models\u2019 alignment. CoRR, abs/2308.05374, 2023.\ndoi: 10.48550/ARXIV.2308.05374. URL https://doi.org/10.48550/arXiv.2308.05374.\n[245] Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, and Minlie Huang.\nUnveiling the implicit toxicity in large language models. In Houda Bouamor, Juan Pino, and\nKalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1322\u20131338.\nAssociation for Computational Linguistics, 2023. URL https://aclanthology.org/2023.\nemnlp-main.84.\n[246] Kellin Pelrine, Mohammad Taufeeque, Michal Zajkac, Euan McLean, and Adam Gleave.\nExploiting novel gpt-4 apis. arXiv preprint arXiv:2312.14302, 2023.\n[247] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Re-\nalToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings\nof the Association for Computational Linguistics: EMNLP 2020, pages 3356\u20133369, On-\nline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nfindings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.\n[248] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao\nLiu, Xuanyu Lei, Jie Tang, and Minlie Huang.\nSafetybench: Evaluating the safety of\nlarge language models with multiple choice questions. CoRR, abs/2309.07045, 2023. doi:\n10.48550/ARXIV.2309.07045. URL https://doi.org/10.48550/arXiv.2309.07045.\n[249] Jiawen Deng, Hao Sun, Zhexin Zhang, Jiale Cheng, and Minlie Huang. Recent advances\ntowards safe, responsible, and moral dialogue systems: A survey. CoRR, abs/2302.09270,\n2023. doi: 10.48550/ARXIV.2302.09270. URL https://doi.org/10.48550/arXiv.2302.\n09270.\n49\n[250] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic\njailbreak of open-source llms via exploiting generation. CoRR, abs/2310.06987, 2023. doi:\n10.48550/ARXIV.2310.06987. URL https://doi.org/10.48550/arXiv.2310.06987.\n[251] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq R.\nJoty, Richard Socher, and Nazneen Fatema Rajani. Gedi: Generative discriminator guided\nsequence generation. In Findings of the Association for Computational Linguistics: EMNLP\n2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 4929\u2013\n4952. Association for Computational Linguistics, 2021. URL https://doi.org/10.18653/\nv1/2021.findings-emnlp.424.\n[252] Anonymous. Badedit: Backdooring large language models by model editing. In Submitted\nto The Twelfth International Conference on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=duZANm2ABX. under review.\n[253] Maximilian Li, Xander Davies, and Max Nadeau. Circuit breaking: Removing model be-\nhaviors with targeted ablation.\nWorkshop on Challenges in Deployable Generative AI at\nInternational Conference on Machine Learning, 2023.\n[254] Xinshuo Hu, Dongfang Li, Zihao Zheng, Zhenyu Liu, Baotian Hu, and Min Zhang. Sep-\narate the wheat from the chaff: Model deficiency unlearning via parameter-efficient mod-\nule operation.\nCoRR, abs/2308.08090, 2023.\ndoi: 10.48550/arXiv.2308.08090.\nURL\nhttps://doi.org/10.48550/arXiv.2308.08090.\n[255] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and\nJohn Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168,\n2021. URL https://arxiv.org/abs/2110.14168.\n[256] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00a8arli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc V. Le, Ed Chi, Denny Zhou, and Jason Wei. Chal-\nlenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the\nAssociation for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023,\npages 13003\u201313051. Association for Computational Linguistics, 2023.\n[257] Moin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring stereotypical bias in\npretrained language models.\nIn Proceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pages 5356\u20135371, Online, August 2021.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2021.acl-long.416.\nURL\nhttps://aclanthology.org/2021.acl-long.416.\n[258] Nirmalendu Prakash and Roy Ka-Wei Lee.\nLayered bias: Interpreting bias in pretrained\nlarge language models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya\nMcCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop:\nAnalyzing and Interpreting Neural Networks for NLP, pages 284\u2013295, Singapore, December\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.22.\nURL https://aclanthology.org/2023.blackboxnlp-1.22.\n[259] Maria De-Arteaga, Alexey Romanov, Hanna M. Wallach, Jennifer T. Chayes, Christian Borgs,\nAlexandra Chouldechova, Sahin Cem Geyik, Krishnaram Kenthapadi, and Adam Tauman\nKalai. Bias in bios: A case study of semantic representation bias in a high-stakes setting.\nProceedings of the Conference on Fairness, Accountability, and Transparency, 2019. URL\nhttps://api.semanticscholar.org/CorpusID:58006082.\n[260] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang.\nGen-\nder bias in coreference resolution: Evaluation and debiasing methods.\nIn Proceedings\nof the 2018 Conference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15\u2013\n20, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:\n10.18653/v1/N18-2003. URL https://aclanthology.org/N18-2003.\n50\n[261] Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji.\nUnlearning bias in\nlanguage models by partitioning gradients.\nIn Findings of the Association for Computa-\ntional Linguistics: ACL 2023, pages 6032\u20136048, Toronto, Canada, July 2023. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.375. URL https:\n//aclanthology.org/2023.findings-acl.375.\n[262] Tomasz Limisiewicz, David Mare\u02c7cek, and Tom\u00b4a\u02c7s Musil. Debiasing algorithm through model\nadaptation, 2023.\n[263] Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, and\nYangqiu Song. Privacy in large language models: Attacks, defenses and future directions.\nCoRR, abs/2310.10383, 2023. doi: 10.48550/ARXIV.2310.10383. URL https://doi.org/\n10.48550/arXiv.2310.10383.\n[264] Seth Neel and Peter Chang. Privacy issues in large language models: A survey. arXiv preprint\narXiv:2312.06717, 2023.\n[265] Sanjam Garg, Shafi Goldwasser, and Prashant Nalini Vasudevan. Formalizing data deletion in\nthe context of the right to be forgotten. In Anne Canteaut and Yuval Ishai, editors, Advances\nin Cryptology \u2013 EUROCRYPT 2020, pages 373\u2013402, Cham, 2020. Springer International\nPublishing. ISBN 978-3-030-45724-2.\n[266] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Lo-\ngeswaran, and Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language\nmodels. In Annual Meeting of the Association for Computational Linguistics, 2022. URL\nhttps://api.semanticscholar.org/CorpusID:252693065.\n[267] Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi\nXiong. Depn: Detecting and editing privacy neurons in pretrained language models, 2023.\n[268] Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, and Alan Ritter. Can language models be\ninstructed to protect personal information?, 2023.\n[269] Leijie Wu, Song Guo, Junxiao Wang, Zicong Hong, J. Zhang, and Jingren Zhou. On knowl-\nedge editing in federated learning: Perspectives, challenges, and future directions. ArXiv,\nabs/2306.01431, 2023. URL https://api.semanticscholar.org/CorpusID:259064255.\n[270] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein.\nA watermark for large language models. In Andreas Krause, Emma Brunskill, Kyunghyun\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Confer-\nence on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume\n202 of Proceedings of Machine Learning Research, pages 17061\u201317084. PMLR, 2023. URL\nhttps://proceedings.mlr.press/v202/kirchenbauer23a.html.\n[271] John M Carroll. Human-computer interaction: psychology as a science of design. Annual\nreview of psychology, 48(1):61\u201383, 1997.\n[272] Ranjay Krishna, Donsuk Lee, Li Fei-Fei, and Michael S Bernstein. Socially situated artificial\nintelligence enables learning from human interaction. Proceedings of the National Academy\nof Sciences, 119(39):e2115730119, 2022.\n[273] Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. Lamp: When\nlarge language models meet personalization. CoRR, abs/2304.11406, 2023. doi: 10.48550/\nARXIV.2304.11406. URL https://doi.org/10.48550/arXiv.2304.11406.\n[274] Morgan L Maeder and Charles A Gersbach. Genome-editing technologies for gene and cell\ntherapy. Molecular Therapy, 24(3):430\u2013446, 2016.\n[275] Jin-Soo Kim. Genome editing comes of age. Nature protocols, 11(9):1573\u20131578, 2016.\n[276] Jennifer A Doudna. The promise and challenge of therapeutic genome editing. Nature, 578\n(7794):229\u2013236, 2020.\n51\n[277] Keyu Pan and Yawen Zeng. Do llms possess a personality? making the MBTI test an amazing\nevaluation for large language models. CoRR, abs/2307.16180, 2023. doi: 10.48550/arXiv.\n2307.16180. URL https://doi.org/10.48550/arXiv.2307.16180.\n[278] Mustafa Safdari, Greg Serapio-Garc\u00b4\u0131a, Cl\u00b4ement Crepy, Stephen Fitz, Peter Romero, Lun-\ning Sun, Marwa Abdulhai, Aleksandra Faust, and Maja Mataric. Personality traits in large\nlanguage models. CoRR, abs/2307.00184, 2023. doi: 10.48550/arXiv.2307.00184. URL\nhttps://doi.org/10.48550/arXiv.2307.00184.\n[279] Quan Tu, Chuanqi Chen, Jinpeng Li, Yanran Li, Shuo Shang, Dongyan Zhao, Ran Wang,\nand Rui Yan. Characterchat: Learning towards conversational AI with personalized social\nsupport. CoRR, abs/2308.10278, 2023. doi: 10.48550/arXiv.2308.10278. URL https://\ndoi.org/10.48550/arXiv.2308.10278.\n[280] Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang,\nPengjun Xie, Fei Huang, and Huajun Chen.\nEditing personality for llms.\nCoRR,\nabs/2310.02168, 2023.\ndoi: 10.48550/ARXIV.2310.02168.\nURL https://doi.org/10.\n48550/arXiv.2310.02168.\n[281] Jen tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao,\nZhaopeng Tu, and Michael R. Lyu. Emotionally numb or empathetic? evaluating how llms\nfeel using emotionbench, 2023.\n[282] EunJeong Hwang, Bodhisattwa Prasad Majumder, and Niket Tandon. Aligning language\nmodels to user opinions. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings\nof the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-\n10, 2023, pages 5906\u20135919. Association for Computational Linguistics, 2023. URL https:\n//aclanthology.org/2023.findings-emnlp.393.\n52\n"
  },
  {
    "title": "TrailBlazer: Trajectory Control for Diffusion-Based Video Generation",
    "link": "https://arxiv.org/pdf/2401.00896.pdf",
    "upvote": "12",
    "text": "TRAILBLAZER: TRAJECTORY CONTROL FOR DIFFUSION-BASED\nVIDEO GENERATION\nA PREPRINT\nWan-Duo Kurt Ma\nVictoria University of Wellington\nmawand@ecs.vuw.ac.nz\nJ. P. Lewis\nNVIDIA Research\njpl@nvidia.com\nW. Bastiaan Kleijn\nVictoria University of Wellington\nbastiaan.kleijn@vuw.ac.nz\nJanuary 3, 2024\n<Multiple objects>\nA dog [ watching -> playing with ] the red balls on the road\n<Dynamic bbox>\nA tiger walking on the road\n<Moving bbox>\nA cat running on the grass \ufb01eld\n<Static bbox>\nAn astronaut walking on the moon\n<Key-framing>\nA cat [ sitting -> running ] on the grass\n<Scene Compositing>\nA cat and dog playing [on the moon | in the city | in the garden]\n{\n{\nFigure 1: TrailBlazer extends a pre-trained video diffusion model to introduce trajectory control over one or multiple\nsubjects. Its primary contribution lies in the ability to animate the synthesized subject using a bounding box (bbox),\nwhether it remains static (Top-left) or dynamic in terms of location (Top-right), bbox size (Middle-left), and varied\nmovement speed (Middle-right, the cat sitting in the early half of video in red bbox, and then moving with cyan bbox),\nachieved through keyframing. The moving subjects fit naturally within an environment specified by the overall prompt\n(Bottom-right). Additionally, the speed of the subjects can be controlled through keyframing (Bottom-left).\nABSTRACT\nWithin recent approaches to text-to-video (T2V) generation, achieving controllability in the synthe-\nsized video is often a challenge. Typically, this issue is addressed by providing low-level per-frame\nguidance in the form of edge maps, depth maps, or an existing video to be altered. However, the\nprocess of obtaining such guidance can be labor-intensive. This paper focuses on enhancing controlla-\nbility in video synthesis by employing straightforward bounding boxes to guide the subject in various\nways, all without the need for neural network training, finetuning, optimization at inference time, or\nthe use of pre-existing videos. Our algorithm, TrailBlazer, is constructed upon a pre-trained (T2V)\nmodel, and easy to implement.1 The subject is directed by a bounding box through the proposed\nspatial and temporal attention map editing. Moreover, we introduce the concept of keyframing,\nallowing the subject trajectory and overall appearance to be guided by both a moving bounding box\nand corresponding prompts, without the need to provide a detailed mask. The method is efficient,\nwith negligible additional computation relative to the underlying pre-trained model. Despite the\nsimplicity of the bounding box guidance, the resulting motion is surprisingly natural, with emergent\neffects including perspective and movement toward the virtual camera as the box size increases.\n1Our project page: https://hohonu-vicml.github.io/Trailblazer.Page/\narXiv:2401.00896v1  [cs.CV]  31 Dec 2023\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\n1\nIntroduction\nAdvancements in generative models for text-to-image (T2I) have been dramatic Ramesh et al. (2022); Saharia et al.\n(2022a); Rombach et al. (2022); Balaji et al. (2022). Recently, text-to-video (T2V) systems have made significant\nstrides, enabling the automatic generation of videos based on textual prompt descriptions Ho et al. (2022a,b); Wu et al.\n(2023); Esser et al. (2023). One primary challenge in video synthesis lies in the extensive memory and training data\nrequired. Methods based on the pre-trained Stable Diffusion (SD) model have been proposed to address the efficiency\nissues in Text-to-Video (T2V) synthesis. These approaches address the problem from several perspectives including\nfinetuning and zero-shot learning Khachatryan et al. (2023); Qi et al. (2023).\nHowever, text prompts do not provide good control over the spatial layout and trajectories of objects in the generated\nvideo. This control is known to be required for understandable narration of a story Arijon (1976). Existing work such\nas Hu and Xu (2023) has approached this problem by providing low-level control signals, e.g., using Canny edge maps\nor tracked skeletons to guide the objects in the video using ControlNet Zhang and Agrawala (2023). These methods\nachieve good controllability, but they can require considerable effort to produce the control signal. For example,\ncapturing the desired motion of an animal (e.g., a tiger) or an expensive object (a jet plane) would be quite difficult,\nwhile sketching the desired movement on a frame-by-frame basis would be tedious.\nTo address the needs of casual users, we introduce a high-level interface for control of object trajectories in synthesized\nvideos. Users simply provide bounding boxes (bboxes) specifying the desired position of an object at several points in\nthe video, together with the text prompt(s) describing the object at the corresponding times. Taking inspiration from the\nobservation Liew et al. (2022) that object position is established early in the denoising process, and leveraging the clear\nspatial interpretation of spatial and temporal attention maps as illustrated in Fig. 2, we propose a general approach:\nOur strategy involves editing both spatial and temporal attention maps for a specific object during the initial denoising\ndiffusion steps to concentrate activation at the desired object location. Our inference-time editing approach achieves this\nwithout disrupting the learned text-image association in the pre-trained model, and requires minimal code modifications.\nWe use ZeroScope cerspense (2023) in a pre-trained fashion as the underlying model.\nOur contributions are three-fold:\n\u2022 Novelty. We introduce a novel approach employing high-level bounding boxes to guide the subject in diffusion-\nbased video synthesis. This approach is suitable for casual users, as it avoids the need to record or draw a\nframe-by-frame positioning control signal. In contrast, the low-level guidance signals (detailed masks, edge\nmaps) used by some other approaches have two disadvantages: it is difficult for non-artists to draw these\nshapes, and processing existing videos to obtain these signals limits the available motion to copies of existing\nsources.\n\u2022 Position, size, and prompt trajectory control. Our approach enables users to position the subject by\nkeyframing its bounding box. The size of the bbox can be similarly controlled, thereby producing perspective\neffects (Figs. 1,6). Finally, users can also keyframe the text prompt to influence the behavior of the subject in\nthe synthesized video (Figs. 1,6).\n\u2022 Simplicity. Our method operates by directly editing the spatial and temporal attention in the pre-trained\ndenoising UNet. It requiring no training or optimization, and the core algorithm can be implemented in less\nthan 200 lines of code.\n2\nRelated Work\n2.1\nText-to-Image (T2I)\nDenoising diffusion models construct a stochastic Sohl-Dickstein et al. (2015); Song and Ermon (2019); Ho et al.\n(2020) or deterministic Song et al. (2021) mapping between the data space and a corresponding-dimension multivariate\nGaussian. Signals are synthesized by sampling from the Gaussian and performing a sequence of denoising steps. A\nnumber of works Nichol et al. (2022); Nichol and Dhariwal (2021); Ramesh et al. (2022); Saharia et al. (2022b) have\nperformed T2I synthesis using images conditioned on the text embedding from a model such as CLIP Radford et al.\n(2021). Performance is significantly improved in the Latent Diffusion Model Rombach et al. (2022) (LDM) by doing\nthe diffusion computation in the latent space of a carefully trained variational autoencoder. LDM was trained with a\nlarge scale dataset, resulting in the widely adopted Stable Diffusion (SD) system. We omit the basic diffusion derivation\nas good tutorials are available, e.g., Weng (2021).\nDespite the success of image generation using SD, it is widely acknowledged that SD lacks controllability in synthesis.\nSD faces challenges in synthesizing multiple objects, often resulting in missing objects or incorrect assignment of\n2\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nprompt attributes to different objects. Recently, ControlNet Zhang and Agrawala (2023) and T2I-Adapter Mou et al.\n(2023) used additional fine-tuning layers to train the model with various forms of image conditioning (edge maps,\nskeletons).\nThe methods of Zhao et al. (2020); Sun and Wu (2022); Yang et al. (2022b); Ma et al. (2023); Xie et al. (2023); Bar-Tal\net al. (2023) have addressed the layout-to-image (L2I) issue using few-shot learning. Directed Diffusion Ma et al.\n(2023), BoxDiff Xie et al. (2023), and MultiDiffusion Bar-Tal et al. (2023) use coarse bounding boxes to control subject\nposition, achieving good result by manipulating the spatial latent and text embeddings cross attention map Hertz et al.\n(2022).\n2.2\nText-to-Video (T2V)\nText-to-video (T2V) synthesis is generally more difficult than T2I due to the difficulty of ensuring temporal consistency\nand requirement for a large paired text and video dataset. Ho et al. (2022b); Harvey et al. (2022); H\u00f6ppe et al. (2022);\nVoleti et al. (2022); Yang et al. (2022a); Ge et al. (2023) show methods that build on top of image diffusion models.\nSome works Blattmann et al. (2023); Luo et al. (2023) also introduce 3D convolutional layers in the denoising UNet to\nlearn temporal information. Imagen Video Ho et al. (2022a) achieves higher resolution by computing temporal and\nspatial super-resolution on initial low resolution videos. VideoLDM Blattmann et al. (2023) and ModelScope Luo et al.\n(2023) insert a temporal attention layer by reshaping the latent tensor. Text2Video-Zero Khachatryan et al. (2023) and\nFateZero Qi et al. (2023) investigate how the temporal coherence can be improved by cross frame attention manipulation\nwith pre-trained T2I models. Ge et al. (2023) address the same problem by introducing temporal correlation in the\ndiffusion noise. However, these pioneering studies generally lack position control in the video synthesis.\nRecently several works have been proposed to solve the controllability in video synthesis problem by using pre-trained\nmodels together with low-level conditioning information such as edge or depth maps. Control-A-Video Chen et al.\n(2023) and MagicProp Yan et al. (2023) use depth maps with ControlNet to train a temporal-aware network. Text2Video-\nZero Khachatryan et al. (2023) partially achieves controllability by initializing the latent frames conditioned on the first\nframe with applied linear translation. However, it does not know about the reconstructed subject, making it difficult to\nedit the video (e.g., the user might need to know the initial position after synthesis, and then adjust the the shifting\nfactor). Distinct from the methods above, we use an attention injection method to guide the denoising path rather than\noptimization, and in general this is robust to different random seeds. We note that our method can position and animate\nthe bounding boxes with keyframing to control the subject.\n3\nMethod\nOur approach is based on the open-source pre-trained model ZeroScope cerspense (2023). This is a fine-tuned version\nof ModelScope Luo et al. (2023), known for its ability to generate high-quality videos without significant temporal\nflickering. It is noteworthy that our approach preserves this desirable temporal coherence effect achieved in their\nwork. Our approach does not require any training, optimization, or low-level control signals (e.g., edge, depth maps\nwith ControlNet Zhang and Agrawala (2023)). On the contrary, all that is required from the user is the prompt and\nan approximate bounding box (bbox) of the subject. Bboxes and corresponding prompts can be specified at several\npoints in the video, and these are treated as keyframes and interpolated to smoothly control both the motion and prompt\ncontent. We use the following notation: Bold capital letters (e.g., M) denote a matrix or a tensor depending on the\ncontext, vectors are represented with bold lowercase letters (e.g., m), and scalars are denoted as lowercase letters (e.g.,\nm). We use superscripts to denote an indexed tensor slice (e.g., M(i)). A synthesized video is composed of a number of\nimages ordered in time. The individual images will be referred to as frames, and the collection of corresponding times\nis the timeline. Spatial or temporal attention will be informally referred to as correlation.\nSimilar to the work in Ma et al. (2023), our method draws significant inspiration from visual inspection of cross-attention\nmaps. Consider the final cross-attention result depicted in Fig. 2, generated from the prompt \u201can astronaut walking on\nthe moon\u201d. The spatial cross attention (denoted as SAttn) associated with the prompt word \u201castronaut\u201d is highlighted in\nthe second row, showcasing the overall position of the subject. Furthermore, we visualize the attention map from the\ntemporal module in the pre-trained model. The third row displays \u201cself-frame\u201d temporal attention maps TAttn-Self,\nwhich consistently align with SAttn.\nThe last row of Fig. 2 presents the visualization of cross-frame temporal attention maps, denoted as TAttn-Cross,\nillustrating the attention between the first frame and subsequent frames in the video. As the distance between frames\nincreases, the attention becomes less correlated in the subject area and becomes more correlated in the background area.\nThis observation aligns with the reconstructed video shown in the first row, where the background remains nearly static\nwhile the astronaut\u2019s motion varies frame by frame. We will consider the temporal attention in detail in Sec. 3.3.\n3\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nRecons\nSAttn\nTAttn-Self\nFrame 1\nFrame 4\nFrame 16\nFrame 24\nTAttn-Cross\nAttn(1,1)\nAttn(4,4)\nAttn(16,16)\nAttn(24,24)\nAttn(1,2)\nAttn(1,4)\nAttn(1,18)\nAttn(1,23)\nFigure 2: Basis of our method. We draw inspiration from inspection of the spatial (SAttn) and temporal (TAttn)\ncross-attention maps viewed with self-frame attention (Self) and cross-frame attention (Cross).Thus TAttn-Self and\nTAttn-Cross denote the self- and cross-frame attention map, respectively. SAttn is the spatial cross-attention map with\nthe prompt word \u201castronaut\u201d. The symbol \u201cAttn(i,j)\u201d denotes the temporal attention map between frame i and frame j.\nThe first row shows reconstructions sampled from frames 1, 4, 16, and 24, respectively. In the TAttn-Cross images,\nthe frame number were manually chosen to best illustrate the cross-frame attention between the astronaut and the\nbackground. Please refer to the main text for more details.\n3.1\nPipeline\nAs mentioned above, keyframing wiki (2023) is a technique that defines properties of images at particular frames (keys)\nin timeline and then automatically interpolates these values to achieve a smooth transition between the keys. It is\nwidely used in the movie animation and visual effects industries since it reduces the artist\u2019s work while simultaneously\nproducing temporally smooth motion that would be hard to achieve if the artist directly edited every image. Our system\ntakes the advantage of this principle, and asks the user to specify several keys, consisting of bboxes and the associated\nprompts, describing the subject location and appearance or behavior at the particular times. For instance, as shown in\nFig. 1 (Middle-right), the video of the cat initially sitting on the left, then running to the right, is achieved simply by\nplacing keys at three frames only. Specifically, the sitting cat in the first part of the video is obtained with two identically\npositioned bboxes on the left, with the keyframes at the beginning and middle of the timeline and the prompt word\n\u201csitting\u201d associated with both. A third keyframe is placed at the end of the video, with the bbox positioned on the right\ntogether with the prompt changing to \u201crunning\u201d. This results in the cat smoothly transitioning from sitting to running in\nthe second part of the video.\nWe use the pre-trained ZeroScope model cerspense (2023) in all our experiments with no neural network training,\nfinetuning, or optimization at inference time. Our pipeline is shown in Fig. 3. The spatial cross attention and the\n4\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nPipeline\nspatial cross attention\ntemporal attention\nUNet Denoiser\nA(i)\ns\nFrame \nEmbedding\n\u00d7\nQs\nKs\nPrompt \nEmbedding\n[CLS]\nPrompt\nTrailing\nTokens\nTokens\n[SEP]\ny(\ud835\udcab)\n\u03c4\u03b8\nWQ\nWK\nWV\nQs\nKs\nVs\n\u03c6(zt)\nNetwork \nEmbedding\nText \nEmbedding\nWQ\nWK\nWV\nQm\nKm\nVm\nFrame i \nEmbedding\nQ(i)\nm\nFrame j \nEmbedding\n\u00d7\nK(j)\nm\nA(i,j)\nm\nGaussian \nWeight Map\nt \u2208 {T, . . , T \u2212 Ns}\nt \u2208 {T, . . , T \u2212 Nm}\nS(\u212c)\nSm(\u212c)\nFigure 3: Pipeline Overview. Our pipeline highlights the central components of spatial cross-attention editing (left,\nin the blanched almond-colored section) and temporal cross-frame attention editing (right, in the blue section). This\noperation is exclusively applied during the denoising process in the early stage. The objective is to alter the attention\nmap (e.g., As, Am) using a Gaussian weighting within a user-specified bbox.\ntemporal attention is discussed in detail in the Sec. 3.2 and Sec. 3.3, respectively. The editing of all the spatial and the\ntemporal editing is performed in the early steps t \u2208 {T, ..., T \u2212 NS}, and t \u2208 {T, ..., T \u2212 NM} of backward denoising\nprocess, where T is the total number of denoising time steps, and NS, and NM are hyperparameters specifying the\nnumber of steps of spatial and temporal attention editing. The parameter setting is detailed in our supplementary\nmaterial.\nIn the subsequent sections we describe how our algorithm is implemented by modifying the spatial and temporal\nattention in a pre-trained diffusion model. Please refer to Rombach et al. (2022); Song et al. (2021); Ho et al. (2020);\nWeng (2021) for background on overall diffusion model architectures.\nOur system processes a set of keyframes, encompassing associated bbox regions Rf and prompts Pf at frame f, where\nf denotes the frame index within the range f \u2208 {0, ..., NF }. In practice, users are required to specify a minimum of two\nkeyframes at the start and end of the video sequence. Then, the information in these keyframes is linearly interpolated,\nsuch as the bbox Bf and the prompt text embedding y(Pf) through the text encoder y(\u00b7). To enhance readability, we\nomit subscript f and the linearly blended video sequence between the keyframes when discussing the core method.\nThe region R is characterized of a set of parameters R = {B, I, T }: a set of bbox positions (e.g., B), the indices of the\nsubject we would like to constrain (e.g., I), and the indices of the trailing maps (e.g., T ) to enhance the controllability.\nThe subject indices I \u2282 {i|i \u2208 N, 1 \u2264 i \u2264 |P|}, are 1-indexed with the associated word in the prompt. For example,\nI = {1, 2} is associated with \u201ca\u201d, \u201ccat\u201d in the prompt \u201ca cat sitting on the car\u201d. The trailing attention indices T and the\nbounding box (bbox) B are defined as below.\nThe trailing attention maps indices T \u2282 {i|i \u2208 N, |P| < i \u2264 NP } is the set of indices corresponding the cross-attention\nmaps generated without a prompt word association, where NP denotes the maximum prompt length that a tokenizer\nmodel can take, which is NP = 77 when CLIP is used Radford et al. (2021). It serves as a means of controlling the\nspatial location of the synthesized subject and its attributes. A larger number of trailing indices set provides greater\ncontrollability but comes with the risk of failed reconstruction, as illustrated in Fig. 8. We refer reader to the work Ma\net al. (2023) for more detail.\nA bbox B =\n\b\n(x, y) | bleft \u00d7 w \u2264 x \u2264 bright \u00d7 w, btop \u00d7 h \u2264 y \u2264 bbottom \u00d7 h\n\t\n, is a set of all pixel coordinates inside the\nbbox of resolution w \u00d7 h. In our implementation, B is produced by a tuple of the four scalars representing the boundary\nof the bbox b = (bleft, btop, bright, bbottom), where bleft, btop, bright, bbottom \u2208 [0, 1] in the ratio of the synthesis resolution.\nThe height and the width, denoted as h and w respectively, are defined by the resolution of the UNet intermediate\nrepresentation Rombach et al. (2022).\n3.2\nSpatial Cross Attention Guidance\nThe spatial cross attention modules are implemented in the denoising UNet module of Rombach et al. (2022). This\nmodule finds the cross attention between the query representation Qs \u2208 RNF \u00d7dh\u00d7d obtained from the SD latent zt,\nand the representations Ks, Vs \u2208 RNF \u00d7|W |\u00d7d of the |W| prompt words from the text model, where d is the feature\ndimension of the keys and queries. Usually |W| \u2261 77 when the text embedding model is CLIP Radford et al. (2021).\n5\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nThe cross attention map Hertz et al. (2022) is then defined as As = Softmax(QsKT\ns /\n\u221a\nd) \u2208 RNF \u00d7dh\u00d7|W |, 2 where\ndh \u2261 w \u00d7 h, defined by the spatial resolution height and width at the specific layer. Note for simplicity we omit the\nbatch size and the number of the attention heads Vaswani et al. (2017) in our definition.\nAs illustrated in the blanched almond-colored section in Fig. 3, we guide the denoising path by editing the spatial cross\nattention (e.g., Ma et al. (2023)) for the attention maps A(i)\ns\n\u2208 RNF \u00d7dh associated with a particular prompt word and\ntrailing indices i \u2208 I \u222a T . Given B, our spatial attention editing is defined by\nWs(x, y) =\n\u001a\ncw,\n(x, y) \u2208 B\u2032\n1,\notherwise,\nSs(x, y) =\n\u001a\ncs g(x, y),\n(x, y) \u2208 B\n0,\notherwise,\nwhere x, y are are the spatial location indices of the attention map and B\u2032 is the complement of B. Ss(B) uses a\nfunction g(\u00b7, \u00b7) that \u201cinjects\u201d attention inside B, as illustrated in the gray box in Fig. 3. The parameters cw \u2264 1,\ncs > 0 attenuate the attention outside of B and strengthen it inside. We define g(\u00b7, \u00b7) as a Gaussian window of size\n\u03c3x = bw/2, \u03c3y = bh/2, where bw = ceil((bright \u2212 bleft) \u00d7 w), bh = ceil((btop \u2212 bbottom) \u00d7 h) are the width and the\nheight of B. In contrast, Ws(\u00b7) attenuates the attention outside B. The bbox B is extended across the entire video\nsequence through linear interpolation of the keyframes. For example, Bf = (1 \u2212 a) \u00d7 Bb + a \u00d7 Be, where a =\nf\nNF ,\nand Bb, Be denotes the bbox for the beginning and the end of keyframe.\nGiven the set of indices of subject word prompts I and trailing maps T , for each cross-activation component at location\n(x, y) in As is modified as follows,\nA(i)\ns (x, y) := A(i)\ns (x, y) \u2299 Ws(x, y) + Ss(x, y), \u2200i \u2208 I \u222a T ,\n(1)\nwhere \u2299 denotes the Hadamard (element-wise) product that scales the x, y element of the cross-attention map As by\nthe corresponding weight in Ws(\u00b7). The overall result is that the attention in the cross-attention map for the particular\nprompt word as well as the trailing maps, is stronger in the user-specified bbox region.\n3.3\nTemporal Cross-Frame Attention Guidance\nTo capture the temporal correlation in the video clip during training, a prevalent approach involves reshaping the\nlatent tensor. This involves shifting the spatial information to the first dimension, a technique employed in VideoLDM\nBlattmann et al. (2023). This reshaping is done before passing the hidden activation into the temporal layers, allowing\nthe model to learn about the \u201ccorrelation\u201d of spatial components through the convolutional layers. As shown in Fig. 3,\nthe temporal attention map is obtained by Am = Softmax(QmKT\nm/\n\u221a\nd) \u2208 Rdh\u00d7NF \u00d7NF , where dh is the spatial\ndimensions of this tensor, and Qm \u2208 Rdh\u00d7NF \u00d7d, and Km \u2208 Rdh\u00d7NF \u00d7d, respectively.\nWhat is different from the spatial counterpart is that now Am learns about the relation between the all activation\ncomponents across frames. For instance, A(x,y,i,j)\nm\ndenotes the activation at location (x, y) between frame i and frame\nj. We denote such tensors as A(i,j)\nm\n(x, y) to keep the notation consistency. As seen in our visual investigation (Fig. 2,\nlast row), the background attention is higher when the cross frame attention compares the frames that are temporally far\nfrom each other, and the foreground attention is higher when the frames are closer in the video sequence.\nTo achieve this pattern of activations under user control we design an approach similar to Eq. 1 but considering the\nnormalized video temporal distance d = |i\u2212j|\nNF , i, j \u2208 {1, ..., NF }, the temporal injection function is defined as,\nSm(x, y) =\n\u001a\n(1 \u2212 d) g(x, y) \u2212 d g(x, y), (x, y) \u2208 B,\n0,\notherwise.\nHere the normalized video temporal distance d determines the level of the weight injection as the triangular window in\ntime. Values d \u2248 0 increase the activation inside the bbox. In contrast, when d \u2248 1, the activation inside the box is\nreduced, thereby approximating the temporal \u201canti-correlation\u201d effect seen in Fig. 2. The editing by Sm(\u00b7) is performed\nduring the initial NM frames of the denoising process. See Fig. 9 and section 4.3 for an ablation on this parameter.\nThen, similarly to Eq. 1, the temporal cross-frame attention map editing is performed as\nA(i,j)\nm\n(x, y) := A(i,j)\nm\n(x, y) \u2299 Wm(x, y) + Sm(x, y)\n(2)\nwhere Wm(\u00b7) is defined the same as Ws(\u00b7).\n2Note that this is a \u201cbatch\u201d matrix multiplication (e.g., the method torch.bmm in PyTorch Paszke et al. (2019)) that is C = AB \u2208\nRb\u00d7m\u00d7n, where A \u2208 Rb\u00d7m\u00d7p, and B \u2208 Rb\u00d7p\u00d7n. Similarly, the transpose operation is A\u22a4 \u2208 Rb\u00d7p\u00d7m.\n6\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nScene \nCompositor\nComposed  \nPrompt\nA dog [ standing -> running ] on  the road\nA red ball on the grass \ufb01eld\nA dog chasing the red ball \non the grass \ufb01eld\nz(dog)\nt\nz(ball)\nt\nFigure 4: Scene Compositing. Given the set of latents generated from our system using a single bbox denoted as z(ball)\nt\nand z(dog)\nt\nfor the case of prompts related to ball and dog. Then, the scene compositor produces a synthesis of multiple\nsubjects with the complete prompt and the single subject latents. We refer reader to our supplementary video to view\nthe implemented speed control of the dog.\n3.4\nScene compositing\nThe problem space becomes more complicated for video synthesis with more than one moving subject. Although the\nparameters cs, cw in Eq. 1 are specific to a particular subject, they indirectly affect the entire scene through the global\ndenoising. Thus, the choice of these parameters for different subjects might interact and require a parameter search\nexponential in the number of subjects to find the best synthesis. If the prompt P and bbox B are in conflict then the\nresult might be poor. For instance, a user may specify the moving of B from left to right associated with the prompt\nword \u201cdog\u201d, while the P is given as \u201ca dog is sitting on the road\u201d.\nConsidering the reasons above, we follow work such as Ma et al. (2023); Bar-Tal et al. (2023) that combine multiple\nsubjects, each with their own prompt, during the latent denoising. These individual subjects are then composited into an\noverall image under the control of a global \u201ccomposed\u201d prompt. Our approach is illustrated in Fig. 4.\nGiven a sequence of subject latents z(r)\nt , each generated with different prompts using the core approach in Sec. 3.2 and\nSec. 3.3, the final image is produced by a second denoising process in which the NR individual latents for the denoising\nstep are composited using Eq. 3 for NC steps, followed by a global denoising in which the composited latent zt is\ndenoised using the complete (\u201ccomposed\u201d) prompt (pink box in Fig. 4),\nzt(x, y) := 1\nR\nNR\nX\nr=0\nw zt(x, y) + (1\u2212w) z(r)\nt (x, y),\n\u2200t \u2208 {T, ..., T\u2212NC},\n(x, y) \u2208 Br,\n(3)\nwhere w \u2208 [0, 1] determines the weight of linear interpolation between the specific subject latent z(r)\nt\nand the composed\nlatent zt. It is formulated by considering the ratio of the current denoising timestep between NC and T,\nw = 1 \u2212\n\u0000NC \u2212 (T \u2212 t)\n\u0001\nNC\n.\nAt the very beginning of the denoising step (t = T), the compositing fully prioritizes the subject latent z(r)\nt\nin each\nlocal region in the associated bbox Br. As t decreases, w gradually increases, giving higher priority to composed latent\nzt. This process concludes when t = T \u2212 NC, resulting in w = 1 which stops using the subject latent in the remaining\ndenoising steps.\n7\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nMain result\nOurs (Left -> Right)\nOurs (Right -> Left)\nText2Video-Zero (Motion vector x=8, y=0)\nFigure 5: Main result: Rigid moving bbox. Transitioning the bbox movement from left to right is observed in the\nleft set of four columns, while the transition from right to left occurs in the middle set of four columns. The set on the\nright is produced from Text2Video-Zero. The same prompt is used across each row. The video synthesis for each set of\ncolumns utilizes the same random seed and is generated from the identical prompt. (1st row): a cat walking on the\ngrass field. (2nd row): A macro video of a bee pollinating a flower. (3rd row): An astronaut walking on the moon. (4th\nrow): A clown fish swimming in a coral reef. The bold text represents the directed object.\n4\nExperiments\nHere we briefly present some experiments, evaluation, and limitations of our work. For full experiments and the\nimplementation details we refer the reader to our supplementary and the project video for further examination. The\nfigures here show an evenly spaced sampling of frames from the videos. Please zoom in to see the finer details of the\nresults.\n4.1\nMain result\nFig. 5 shows our main result on trajectory control of a single subject. As a baseline, the right side of the figure shows\nText2Video-Zero Khachatryan et al. (2023)3 using the same prompts, and we used it without conditioning guidance\n(e.g., edge or depth maps) to provide a fair comparison. Text2Video-Zero accepts motion guidance in the form of a\n(x,y) translation vector. We set this vector to (8,0) to produce horizontal motion.\nIn Fig. 5, the left two sets are our results generated with linear interpolated bboxes starting at one side of the image\nand moving to the other boundary. It is evident that our result shows anatomically plausible motion of the subject. For\ninstance, all subjects (e.g., cat, bee, astronaut, and clown fish) face in the direction that they move. This also applies for\nthe other experiments in this paper. However, this does not generally happen in Text2Video-Zero as seen on the right.\nThis is because they apply the the warping operation on the latent space directly, which simply translates the subject\nwithout re-orienting it. In addition it can be seen that background details are not preserved across frames.\nFig. 6 illustrates dynamically changing the bbox size, producing an effect of the subject moving toward or away from\nthe virtual camera. Similar to the Fig. 5, the bbox setup is from the top-left corner to the bottom-right corner. The\ndynamically changing bbox size is annotated with a green box. Note that the generated subjects share a desirable\ncharacteristic, that the subject naturally faces toward the virtual camera when the bbox transitions from small to large\nas seen in the sequences on the left, and vice versa on the right. The results also show a desirable perspective effect.\nIncreasing or reducing the bbox size over time enables the synthesized object to produce the motion of \u201ccoming to\u201d\nand \u201cgoing away from\u201d the camera. The red car also mimics motion blur and rising dust, simulating the effect of fast\ndriving. We believe these effects arise naturally as a result of manipulating a model that was trained on video sequences\nrather than images. For example, while a video where a side-facing car appears bigger in the image is conceivable, for\nexample if the camera is approaching the car from the side, it is much less common than videos where the car appears\nbigger due to it approaching.\nMulti-subject synthesis is generally challenging, particularly when the number of objects exceeds two. We will delve\ninto these limitations in Sec. 4.4. In Fig. 7, we present experiments with two subjects, a cat and a dog, guided by the\n3We used the Huggingface implementation: https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero.\n8\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nMain result - dynamic bbox\nFigure 6: Main result: Dynamic moving bbox. (Top row): A cat [X] on the road, where [X] denotes [walking \u2192\nsitting] and [sitting \u2192 walking] for the left and right sequence, respectively. The bbox is initially moving, or keeping\nstill in the left and the right sequence. (Middle row): A red car driving on the road. (Bottom row): The tiger walking\non the street. The bbox used in middle and the bottom row are linearly interpolated with varied sizes. Please refer to the\nmain text for more detail.\nMethod\nFID(\u2193)\nIS(\u2191)\nKID(\u2193)\nText2Video-Zero Khachatryan et al. (2023)\n348.87\n3.68 \u00b1 0.19\n0.19 \u00b1 0.01\nTrailBlazer\n317.35\n3.63 \u00b1 0.25\n0.18 \u00b1 0.01\nTable 1: Quantitative results.\ngreen bbox in the sub-figure. The synthesis of the dog and cat in isolation is depicted in the top row, serving as a quality\nsanity check. Starting from the second row, we show the subsequent eight results combining environment prompts (e.g.,\n\u201c... on the moon\u201d) after subject prompts (e.g., \u201cA white cat and a yellow dog running...\u201d). Each experiment demonstrates\nthe flexibility of our method to synthesize subjects under varied environmental conditions. Notably, the interactions\nbetween the background and subjects appear plausible, as seen in reflections and splashes in the swimming pool case\nand consistent shadows across all samples. The results also show some artifacts such as extra limbs that are inherited\nfrom the underlying model. Indeed such artifacts are common in some diffusion models.\n4.2\nQuantitative evaluation\nFollowing the methodology in Blattmann et al. (2023); Hu and Xu (2023), we report Frechet Inception Distance Heusel\net al. (2017) (FID), Inception Score (IS), and Kernel Inception Distance (KID) metrics against the CIFAR10 dataset\nKrizhevsky et al. on all images of video sequences, using the PyTorch implementation Obukhov et al. (2020). For a\nfair quantitative evaluation, we generated baseline results using Text2Video-Zero Khachatryan et al. (2023) without\nadditional conditioning input. We set the motion vectors to x = 8, y = 0 and create a 24-frame video sequence as\nour baseline comparison. Subsequently, we manually annotated the beginning and end frames of their result with\nbboxes around the subject, and use these bboxes as keyframes to drive our system for comparison. We randomly\nsample 10 videos from each of the four prompts in Fig. 5, resulting in 960 testing images for each method. The result is\nsummarized in Table. 1. As observed, our performance is roughly equivalent in terms of IS and KID, while our FID is\nsignificantly lower than that of Text2Video-Zero.\nUnfortunately there is no standardized quantitative measure for our task. For example, while the image samples from\nText2Video-Zero in Fig. 5 appear visually reasonable, its synthesized video tends to exhibit temporal flickering, and the\nmotion kinematics are not realistic (e.g., the cat moves to the left in Fig. 5 while consistently facing the observer). For a\nfull and accurate assessment it is essential to see the video results. Please refer to our supplementary material and the\naccompanying video.\n4.3\nAblations\nWe conduct ablation experiments on the number of trailing attention maps, and the number of temporal steps.\nTrailing attention maps. Fig. 8 shows an ablation varying the number of trailing attention maps used in our spatial\ncross attention process, where the top row shows our method without trailing attention maps (e.g, |T | = 0) to |T | = 30.\nThe guided bbox is identical to our earlier experiments in Fig. 5. It is observed that the astronaut remains static at the\nimage center without the trailing attention maps. In contrast, the synthesis with a large number of trailing attentions can\n9\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nA white cat and a yellow dog running....\n... on the moon\n... in the park\n... in the forest\n... in the city\n... in the playground\n... in the botanic garden\n... in the snowy weather\n... near the swimming pool\nstop\nstop\nFigure 7: Main result: Subjects compositing. Each set of the three sub-figures representing the first, middle, and the\nend frame of the synthesized video. The first row shows the video synthesis of the two subjects: \u201ccat\u201d and the \u201cdog\u201d\nguided by the bbox directed by the annotated arrows, respectively. Starting from the second row, each set of results\nshow the varied post-fixed prompt.\n10\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nAblation - #trailings\n0 trailings\n10 trailings\n20 trailings\n30 trailings\nFigure 8: Ablation: Trailing maps. The rows from top to bottom show the video synthesis with 0 (no trailing maps),\n10, 20, and 30 trailing maps. Prompt: \u201cThe astronaut walking on the moon\u201d, where \u201castronaut\u201d is the directed subject.\nThe number of temporal edit steps is five in all cases.\nlead to failed results such as a flag rather than the intended astronaut. A good number of edited trailing attention maps\nis between |T | = 10 and |T | = 20.\nTemporal attention editing. We further show an ablation test in Fig. 9 with varied number of temporal attention editing\nsteps. We take the case of the astronaut from Fig. 8 with |T | = 10, and set NM = 0 (no editing steps), and NM = 10.\nThe result with NM = 0 shows a red blob moving from left to right. The value NM = 10 gives satisfactory result on\nthe astronaut, but the background along the bbox path is missing. From these results we see that a reasonable balance\nbetween spatial and the temporal attention editing must be maintained, while extreme values of either produce poor\nresults. An intermediate value such as NM = 5 (used in most of our experiments) produces the desired result of an\nastronaut moving over a moon background.\n4.4\nLimitations\nOur method shares and inherits common failure cases of the underlying diffusion model. Notably, at the time of writing,\nmodels based on CLIP and Stable Diffusion sometimes generate deformed objects and struggle to generate multiple\nobjects and correctly assign attributes (e.g. color) to objects. We show some failures in Fig. 10. For instance, we\nrequested a red jeep driving on the road but the synthesis shows it sinking into a mud road. The panda example shows\nthe camera moving instead of the panda itself. The red car has implausible deformation, and Darth Vader\u2019s light saber\nturns into a surf board. The length of the resulting video clips is restricted to that produced by the pre-trained model (24\nimages in the case of ZeroScope). This is not a crucial limitation, as movies are commonly (with some exceptions!)\ncomposed of short \u201cshots\u201d of several seconds each. The bounding box guides object placement without precisely\nconstraining it. This is an advantage as well, however, since otherwise the user would have to specify the correct x-y\naspect ratio for objects, a complicated task for non-artists.\n11\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\np\np\n0 steps\n10 steps\nFigure 9: Ablation: Temporal edits. Following up the experiments in Fig. 8, the ablation test on the temporal attention\nediting with varied number of steps of the first and last frame of video reconstruction, shown at the left/right of each set\nof experiments. (Left/Right): No temporal attention editing, and 10 steps editing, respectively. The number of trailing\nis 10 for the two cases.\nLimitation\nFigure 10: Failure cases. Prompts used from (a) to (d): \u201cA red jeep driving on the road\u201d, \u201cA red car driving on the\nhighway\u201d, \u201ca panda eating bamboo\u201d, and \u201cDarth Vader surfing in waves\u201d, where the bold prompt word is the directed\nsubject.\n5\nConclusion\nWe have addressed the problem of controlling the motion of objects in a diffusion-based text-to-video model. Specifically,\nwe introduced a combined spatial and temporal attention guidance algorithm operating in the pre-trained ZeroScope\nmodel. Our contributions are several. Firstly, the spatial location of a subject can be guided through simple bounding\nboxes. Secondly, bounding boxes and prompts can be animated via keyframes, enabling users to alter the trajectory and\ncoarse behavior of the subject along the timeline. The resulting subject(s) fit seamlessly in the specified environment,\nproviding a viable pipeline for video storytelling by casual users. Our approach requires no model finetuning, training, or\nonline optimization, ensuring computational efficiency and a good user experience. Lastly, the results are natural, with\ndesirable effects such as perspective, motion with the correct object orientation, and object/environment interactions\narising automatically.\n12\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nSupplementary Material\n6\nImplementation\nIn this section, we describe details of our implementation, including the core library, hyperparameters, and other pertinent\ninformation. Our method is developed using PyTorch 2.01 Paszke et al. (2019), and the Diffusers library version 0.21.4\nfrom Huggingface Huggingface (2023). We override the the Diffusers pipeline TextToVideoSDPipeline to produce\nour implementation.\nParameters are selected as follows: We use classifier-free guidance with a strength of 9, conduct 40 denoising steps, and\nmaintain a video resolution of 512x512 for the conventional stable diffusion backward denoising process. Regarding\nthe parameters specific to our proposed method, the default values are as follows: We perform 5 editing steps for both\nspatial and temporal attention (denoted as NS \u2261 NM \u2261 5). The editing coefficients cm \u2261 0.001 and cs \u2261 0.1 are used\nin both spatial and temporal attention in most cases. The number of trailing attention maps |T | is the only parameter\nthat needs to be tuned. Generally, 10 \u2264 |T | \u2264 20 yields satisfactory results in practice.\nAs highlighted in section 1, we adapt the pre-trained ZeroScope4 cerspense (2023) T2V model. This model is fine-tuned\nfrom the initial weights of ModelScope Luo et al. (2023)5 utilizing nearly ten thousand clips, each comprising 24\nframes as training data. Consequently, we adhere to the recommended practice of setting the length of the synthesized\nsequence to 24 frames, drawing insights from user experiences shared in relevant blogs6.\nSpatial attention editing is performed at several resolutions with a module with the following architecture:\ntransformer_in.transformer_blocks.0.attn2\ndown_blocks.0.attentions.0.transformer_blocks.0.attn2\ndown_blocks.0.attentions.1.transformer_blocks.0.attn2\ndown_blocks.1.attentions.0.transformer_blocks.0.attn2\ndown_blocks.1.attentions.1.transformer_blocks.0.attn2\ndown_blocks.2.attentions.0.transformer_blocks.0.attn2\ndown_blocks.2.attentions.1.transformer_blocks.0.attn2\nup_blocks.1.attentions.0.transformer_blocks.0.attn2\nup_blocks.1.attentions.1.transformer_blocks.0.attn2\nup_blocks.1.attentions.2.transformer_blocks.0.attn2\nup_blocks.2.attentions.0.transformer_blocks.0.attn2\nup_blocks.2.attentions.1.transformer_blocks.0.attn2\nup_blocks.2.attentions.2.transformer_blocks.0.attn2\nup_blocks.3.attentions.0.transformer_blocks.0.attn2\nup_blocks.3.attentions.1.transformer_blocks.0.attn2\nup_blocks.3.attentions.2.transformer_blocks.0.attn2\nFor temporal attention editing, we found that a multiple-resolution approach was not necessary and produced unpre-\ndictable results. Instead, temporal attention editing uses a single layer:\nmid_block.attentions.0.transformer_blocks.0.attn2\n4Huggingface (2023):cerspense/zeroscope_v2_576w\n5Huggingface (2023):damo-vilab/modelscope-damo-text-to-video-synthesis\n6https://zeroscope.replicate.dev/\n1\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\n7\nComprehensive ablations\nGiven the limited space in the primary text, here we offer more supplementary ablation tests to substantiate our proposed\napproach. Broadly, we illustrate the impact of the spatial and temporal placement of guidance bounding boxes (bboxes)\non the overall result quality, exploring the effect of various bbox speed and size choices directed by user keyframing.\nTo see details, please zoom in to the experiment images, and especially refer to our supplementary video.\nFig. 11 illustrates video synthesis using the pre-trained ZeroScope model without applying our approach. Broadly, all\nthe synthesized results exhibit fine details with plausible temporal coherence as would be seen in a real video featuring\nrelatively slow motion. However, several side effects may be introduced alongside this realism. For example, the\nsynthesized subject is often positioned in the same general area near the center of the images regardless of portrayed\nmotion, and subjects like a galloping horse do not conveying the notion of speed. Additionally, artifacts such as extra or\nmissing limbs (e.g., the cat in the second row) or other implausible results occasionally occur.\nZeroscope, no DD\nFigure 11: Baseline results. Each row shows equally-spaced frames sampled from a video generated using ZeroScope\nwithout applying our trajectory control approach. The prompts used starting from the first row: \u201cA fish swimming in\nthe sea\u201d, \u201cThe cat running on the grass field\u201d, \u201cThe horse galloping on the road\u201d, and \u201cAn astronaut walking on the\nmoon\u201d. These prompts are reused in subsequent examples in these supplementary results.\n2\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\n7.1\nExploration and Ablation: Varied static bbox sizes\nFig. 12 shows the effect of the size of the bbox without considering motion. The results indicate that the bbox size\nsignificantly influences the outcome. In extreme cases, the top row illustrates that a smaller bbox may yield unexpected\nentities in the area (e.g., white smoke next to the horse) or information leakage to the neighboring area (e.g., the blue\nattribute affecting the road). In contrast, the bottom row demonstrates that a overly large bbox can lead to broken results\nin general (e.g., the fish disappearing into the coral reef, and the strange blue pattern in place of the expected blue car).\nWe expect this may be in large part due to the centered-object bias Szab\u00f3 and Horv\u00e1th (2021) in the pre-trained model\u2019s\ntraining data.\nOur recommended bbox size falls within the range of 30% to 60% for optimal reconstruction quality. Note that very\nsmall- or large-sized bboxes can still be employed in our approach, but they are best specified for a particular frame\nrather than the entire sequence. This is demonstrated, for example, in Fig. 13 guiding the swimming fish.\nStatc Bbox: Prompt and size\nFigure 12: Static bbox sizes. Each row shows the result of a static square bbox positioned at the center, where the width\nand height are 25%, 50%, and 90% of the original image size (represented by the the green square on the left). The\nprompts used in the three sets of the experiments are: \u201cThe white horse standing on the street\u201d, \u201cThe fish swimming in\nthe sea\u201d, and \u201cThe blue car running on the road\u201d.\n3\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\n7.2\nExploration and Ablation: Varied dynamic bbox sizes\nFig. 13 demonstrates video synthesis with a dynamically changing bbox size. In the top-left example, the bbox grows\nlarger and then shrinks, resulting in a perspective effect where the fish swims towards the camera and then away from it.\nThe frame highlighted in red indicates the middle keyframe with a large bbox. This aligns with our main text results in\nFig. 6, showcasing that the animated tiger and car respect the bbox size. The top-right example is a comparison to the\ntop-left, portraying the fish only swimming toward the camera.\nThe second and the third rows show a comparison of the same bbox condition with the prompt words \u201cfish\u201d (second\nrow), and \u201csardine\u201d (third row), respectively. This experiment aims to assess how well our method adapts to large bbox\nsize variations, represented by the short/wide target bbox on the left and tall/thin target bbox on the right. The result on\nthe left indicates that the output from the \u201cfish\u201d prompt does not adequately conform to the short-wide aspect ratio of\nthe bounding box, whereas the result from the \u201csardine\u201d prompt can more closely adjust to the desired bbox thanks to\nthe elongated shape of the sardine. Conversely, in the experiment on the right, both \u201cfish\u201d and \u201csardine\u201d perform well\nwith the tall/thin bounding box, since the tall aspect ratio can be satisfied by a fish facing directly toward or away from\nthe camera. In general we expect that the obtained results will mimic the situations found in ZeroScope\u2019s training data,\nwhile views that are outside the typical data (such as a fish swimming vertically, or a horse at the top of the image) will\nbe difficult to synthesize.\nAs with all our results, we see that the guided subject approximately follows the specified bounding box, but does not\nexactly lie within the bbox. While this is a disadvantage for some purposes, we argue that it is also an advantage for\ncasual users \u2013 if the subject exactly fit the bounding box it would require the user to imagine the correct aspect ratio of\nthe subject under perspective (a difficult task for a non-artists) as well as do per-frame animation of the bbox to produce\nthe oscillating motion of the swimming fish seen here.\nFigure 13: Dynamic bbox sizes. The result showcases six synthesized video sequences with the subject directed by the\nyellow arrow starting at the position indicated by green bbox. The number of the bboxes (corresponding to the number\nof keyframes used in the experiment) is, clockwise from top-left, |K| = 3, 2, 2, 2, 2, and 2, respectively. The prompt\nused in each result: \u201cThe [X] swimming in the sea\u201d, where \u201c[X]\u201d denotes the \u201cfish\u201d for the first and second rows, and\n\u201csardine\u201d for the third row.\n4\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\n7.3\nExploration and Ablation: Speed control with multiple keys\nFig. 14 demonstrates controlling the subject\u2019s speed through varying the number of keyframes in the video synthesis.\nGiven the recommended sequence length Nf = 24 for ZeroScope, we show the result of adding different keyframes in\nbetween the start and end keyframes at the left/right image boundary, simulating the cat running back and forth on the\ngrass field. It is clear that the cat moves relatively naturally according to the motion flow indicated by the yellow arrows.\nFor instance, the cat looks back first before turning around, rather than showing an unnatural motion where the position\nof the head and tail is instantaneously swapped. As the cat moves faster, motion blur also introduced in the result. We\nfound that this motion blur is hard to eliminate using negative prompts.\nVaried speed: multiple keys\nFigure 14: Speed Test: number of keyframes. This result shows four synthesized video sequences with the cat\u2019s\nmotion directed according to the yellow arrows starting from the position indicated by green bbox. The number of the\narrows denotes the number of keyframes (excluding the start/end keyframes) used in each experiment. Specifically,\nstarting from the top-left and proceeding in left/right top/down (English reading) order, there are |K| = 2, 3, 4, and 5,\nkeyframes, respectively. The frames highlighted with red correspond to the user-specified keyframes, excluding the\nstart and end keyframes. The prompt used for all experiments is \u201cA cat running on the grass field\u201d. The red arrows in\nthe bottom-right example shows the introduced motion blur representing fast-moving speed.\n5\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\n7.4\nExploration and Ablation: Controlling speed with different placement of a single keyframe\nFig. 15 shows the results of moving the subject with increasing speeds. The first row shows the astronaut moving\nwith constant speed obtained by the linearly interpolating bboxes at the left and right of the image. Starting from the\nsecond row, the astronaut holds the position of the first bbox on the left side of the image for some period of time, then\nmoves more rapidly to the right side of the image, as illustrated in the second column of the figure. This is obtained by\nchanging the timing of a single \u201cmiddle\u201d keyframe Kf1, where the first keyframe and the middle keyframe have the\nsame bbox location (e.g., Bf0 \u2261 Bf1). Similar to the results in Fig. 14, the synthesis may generate motion blur and\nartifacts when the speed is high (e.g., last row).\nL\nR\nL\nR\nL\nR\nL\nR\nVaried speed: key timing\nFigure 15: Speed Test: the timing of a keyframe. The result shows four synthesized video sequences with the subject\ndirected according to the yellow arrow starting at the position indicated by green bbox, as illustrated in the first column.\nAll experiments except the first use three keyframes (|K| = 3), where the timing of the internal keyframe (e.g., Kf1)\ncontrols the duration of a stationary phase and the speed of the subsequent motion, as illustrated in the second column.\nThe horizontal and vertical axis in the second column represent the left/right position and timing, respectively. The\nframe outlined in red indicates the frame controlled by Kf1, corresponding to the time when the astronaut starts to\nmove. The prompt used for all experiments: \u201cThe astronaut walking on the moon\u201d.\n6\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\n7.5\nExploration and Ablation: Irregular trajectory\nWe illustrate irregular trajectories determined by varied keyframes in Fig. 16. The four experiments involve a zigzag\ntrajectory (top-left), a triangle trajectory (top-right), a discontinuous trajectory (bottom-left), and a down-pointing\ntriangle trajectory (bottom-right). In every result the horse shows high-speed running with motion blur. However, the\nresults with turning points show limitations in depicting the horse quickly turning around and may show artifacts. For\nexample, in the third frame of the down-pointing triangle case, the horse appears to swap its head and tail. Difficulty\nportraying this turn is somewhat expected, as horses cannot naturally execute tight high-speed turns, unlike cats or dogs.\nOn the other hand, the down-pointing triangle video naturally introduces a perspective-like size change as the horse\nmoves higher in the image, similar to the previous results in Fig. 13, and also the car/tiger example Fig. 6. In summary,\nmaintaining consistency between the prompt and the timing and location of the keyframed bounding boxes is crucial for\nproducing realistic results.\nFigure 16: Irregular trajectory. The figure shows four synthesized video sequences with the horse subject directed\naccording to the yellow arrows starting from the position indicated by green bbox. The frames highlighted in red\nindicates correspond to keyframes. The start and end keyframes are not indicated. The prompt used for all examples:\n\u201cA horse galloping on the road\u201d.\nReferences\nDaniel Arijon. Grammar of the Film Language. Focal Press, 1976.\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine,\nBryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers.\nCoRR, abs/2211.01324, 2022.\nOmer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation.\nCoRR, abs/2302.08113, 2023.\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your\nlatents: High-resolution video synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023.\ncerspense. zeroscope-v2-576w, 2023. Accessed: 2023-10-01.\nWeifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable\ntext-to-video generation with diffusion models, 2023.\nPatrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided\nvideo synthesis with diffusion models. ArXiv, abs/2302.03011, 2023.\nSongwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu,\nand Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision 2023, 2023.\nWilliam Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible diffusion modeling of long\nvideos, 2022.\n7\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with\ncross attention control. arXiv preprint arXiv:2208.01626, 2022.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale\nupdate rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems. Curran Associates, Inc.,\n2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing\nSystems, 33, 2020.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma, Ben Poole,\nMohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models.\nArXiv, abs/2210.02303, 2022a.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models,\n2022b.\nZhihao Hu and Dong Xu. Videocontrolnet: A motion-guided video-to-video translation framework by using diffusion model with\ncontrolnet, 2023.\nHuggingface. Stable diffusion 1 demo, 2023. Accessed: 2023-01-01.\nTobias H\u00f6ppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling,\n2022.\nLevon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and\nHumphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439,\n2023.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research).\nJun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng.\nMagicmix: Semantic mixing with diffusion models.\nCoRR,\nabs/2210.16056, 2022.\nZhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan.\nVideofusion: Decomposed diffusion models for high-quality video generation, 2023.\nWan-Duo Kurt Ma, J. P. Lewis, Avisek Lahiri, Thomas Leung, and W. Bastiaan Kleijn. Directed diffusion: Direct control of object\nplacement through attention guidance, 2023.\nChong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023.\nAlex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In ICML, 2022.\nAnton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin. High-fidelity performance\nmetrics for generative models in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zenodo.4957738.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia\nGimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank\nChilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep\nlearning library, 2019.\nChenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions\nfor zero-shot text-based video editing. arXiv:2303.09535, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language\nsupervision. In Proc. ICML, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with\nCLIP latents. CoRR, abs/2204.06125, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with\nlatent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n10684\u201310695, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\nBurcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with deep language understanding. CoRR, abs/2205.11487, 2022a.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour,\nBurcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with deep language understanding. ArXiv, abs/2205.11487, 2022b.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium\nthermodynamics. In International Conference on Machine Learning, 2015.\n8\nTrailBlazer: Trajectory Control for Diffusion-Based Video Generation\nA PREPRINT\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In NeurIPS, 2019.\nWei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image synthesis. TPAMI, 44:5070\u20135087,\n2022.\nGergely Szab\u00f3 and Andr\u00e1s Horv\u00e1th. Mitigating the bias of centered objects in common datasets. CoRR, abs/2112.09195, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017.\nVikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Mcvd: Masked conditional video diffusion for prediction, generation,\nand interpolation. In (NeurIPS) Advances in Neural Information Processing Systems, 2022.\nLilian Weng. What are diffusion models?, 2021.\nwiki. keyframe, 2023. Accessed: 2023-10-01.\nJay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 7623\u20137633, 2023.\nJinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image\nsynthesis with training-free box-constrained diffusion. CoRR, abs/2307.10816, 2023.\nHanshu Yan, Jun Hao Liew, Long Mai, Shanchuan Lin, and Jiashi Feng. Magicprop: Diffusion-based video editing via motion-aware\nappearance propagation, 2023.\nRuihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion probabilistic modeling for video generation, 2022a.\nZuopeng Yang, Daqing Liu, Chaoyue Wang, J. Yang, and Dacheng Tao. Modeling image composition for complex scene generation.\nCVPR, pages 7754\u20137763, 2022b.\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023.\nBo Zhao, Weidong Yin, Lili Meng, and Leonid Sigal. Layout2image: Image generation from layout. Int. J. Comput. Vis., 128(10):\n2418\u20132435, 2020.\n9\n"
  },
  {
    "title": "En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data",
    "link": "https://arxiv.org/pdf/2401.01173.pdf",
    "upvote": "8",
    "text": "En3D: An Enhanced Generative Model for Sculpting 3D Humans from\n2D Synthetic Data\nYifang Men1, Biwen Lei1, Yuan Yao1, Miaomiao Cui1, Zhouhui Lian2, Xuansong Xie1\n1Institute for Intelligent Computing, Alibaba Group\n2Wangxuan Institute of Computer Technology, Peking University\nhttps://menyifang.github.io/projects/En3D/index.html\nFigure 1. Given random noises or guided texts, our generative scheme can synthesize high-fidelity 3D human avatars that are visually\nrealistic and geometrically accurate. These avatars can be seamlessly animated and easily edited. Our model is trained on 2D synthetic\ndata without relying on any pre-existing 3D or 2D collections.\nAbstract\nWe present En3D, an enhanced generative scheme for\nsculpting high-quality 3D human avatars. Unlike previous\nworks that rely on scarce 3D datasets or limited 2D collec-\ntions with imbalanced viewing angles and imprecise pose\npriors, our approach aims to develop a zero-shot 3D gen-\nerative scheme capable of producing visually realistic, ge-\nometrically accurate and content-wise diverse 3D humans\nwithout relying on pre-existing 3D or 2D assets. To ad-\ndress this challenge, we introduce a meticulously crafted\nworkflow that implements accurate physical modeling to\nlearn the enhanced 3D generative model from synthetic 2D\ndata. During inference, we integrate optimization modules\nto bridge the gap between realistic appearances and coarse\n3D shapes. Specifically, En3D comprises three modules: a\n3D generator that accurately models generalizable 3D hu-\nmans with realistic appearance from synthesized balanced,\ndiverse, and structured human images; a geometry sculptor\nthat enhances shape quality using multi-view normal con-\nstraints for intricate human anatomy; and a texturing mod-\nule that disentangles explicit texture maps with fidelity and\neditability, leveraging semantical UV partitioning and a dif-\nferentiable rasterizer. Experimental results show that our\napproach significantly outperforms prior works in terms of\nimage quality, geometry accuracy and content diversity. We\nalso showcase the applicability of our generated avatars for\nanimation and editing, as well as the scalability of our ap-\nproach for content-style free adaptation.\n1. Introduction\n3D human avatars play an important role in various appli-\ncations of AR/VR such as video games, telepresence and\nvirtual try-on.\nRealistic human modeling is an essential\ntask, and many valuable efforts have been made by lever-\narXiv:2401.01173v1  [cs.CV]  2 Jan 2024\naging neural implicit fields to learn high-quality articulated\navatars [9, 11, 45, 52]. However, these methods are directly\nlearned from monocular videos or image sequences, where\nsubjects are single individuals wearing specific garments,\nthus limiting their scalability.\nGenerative models learn a shared 3D representation to\nsynthesize clothed humans with varying identities, clothing\nand poses. Traditional methods are typically trained on 3D\ndatasets, which are limited and expensive to acquire. This\ndata scarcity limits the model\u2019s generalization ability and\nmay lead to overfitting on small datasets. Recently, 3D-\naware image synthesis methods [6, 20, 39] have demon-\nstrated great potential in learning 3D generative models of\nrigid objects from 2D image collections. Follow-up works\nshow the feasibility of learning articulated humans from im-\nage collections driven by SMPL-based deformations, but\nonly in limited quality and resolution. EVA3D [18] repre-\nsents humans as a composition of multiple parts with NeRF\nrepresentations. AG3D [10] incorporates an efficient artic-\nulation module to capture both body shape and cloth defor-\nmation. Nevertheless, there remains a noticeable gap be-\ntween generated and real humans in terms of appearance\nand geometry. Moreover, their results are limited to specific\nviews (i.e., frontal angles) and lack diversity (i.e., fashion\nimages in similar skin tone, body shape, and age).\nThe aim of this paper is to propose a zero-shot 3D gen-\nerative scheme that does not rely on any pre-existing 3D\nor 2D datasets, yet is capable of producing high-quality\n3D humans that are visually realistic, geometrically accu-\nrate, and content-wise diverse. The generated avatars can\nbe seamlessly animated and easily edited. An illustration\nis provided in Figure 1. To address this challenging task,\nour proposed method inherits from 3D-aware human image\nsynthesis and exhibits substantial distinctions based on sev-\neral key insights. Rethinking the nature of 3D-aware gener-\native methods from 2D collections [6, 10, 18], they actually\ntry to learn a generalizable and deformable 3D represen-\ntation, whose 2D projections can meet the distribution of\nhuman images in corresponding views. Thereby, it is cru-\ncial for accurate physical modeling between 3D objects and\n2D projections. However, previous works typically leverage\npre-existing 2D human images to estimate physical parame-\nters (i.e., camera and body poses), which are inaccurate be-\ncause of imprecise SMPL priors for highly-articulated hu-\nmans. This inaccuracy limits the synthesis ability for real-\nistic multi-view renderings. Second, these methods solely\nrely on discriminating 2D renderings, which is ambiguous\nand loose to capture inherent 3D shapes in detail, especially\nfor intricate human anatomy.\nTo address these limitations, we propose a novel gen-\nerative scheme with two core designs. Firstly, we intro-\nduce a meticulously-crafted workflow that implements ac-\ncurate physical modeling to learn an enhanced 3D gener-\native model from synthetic data. This is achieved by in-\nstantiating a 3D body scene and projecting the underly-\ning 3D skeleton into 2D pose images using explicit cam-\nera parameters. These 2D pose images act as conditions to\ncontrol a 2D diffusion model, synthesizing realistic human\nimages from specific viewpoints. By leveraging synthetic\nview-balanced, diverse and structured human images, along\nwith known physical parameters, we employ a 3D genera-\ntor equipped with an enhanced renderer and discriminator\nto learn realistic appearance modeling. Secondly, we im-\nprove the 3D shape quality by leveraging the gap between\nhigh-quality multi-view renderings and the coarse mesh\nproduced by the 3D generative module. Specifically, we\nintegrate an optimization module that utilizes multi-view\nnormal constraints to rapidly refine geometry details under\nsupervision. Additionally, we incorporate an explicit textur-\ning module to ensure faithful UV texture maps. In contrast\nto previous works that rely on inaccurate physical settings\nand inadequate shape supervision, we rebuild the generative\nscheme from the ground up, resulting in comprehensive im-\nprovements in image quality, geometry accuracy, and con-\ntent diversity. In summary, our contributions are threefold:\n\u2022 We present a zero-shot generative scheme that efficiently\nsynthesizes high-quality 3D human avatars with visual re-\nalism, geometric accuracy and content diversity. These\navatars can be seamlessly animated and easily edited, of-\nfering greater flexibility in their applications.\n\u2022 We develop a meticulously-crafted workflow to learn an\nenhanced generative model from synthesized human im-\nages that are balanced, diverse, and also possess known\nphysical parameters. This leads to diverse 3D-aware hu-\nman image synthesis with realistic appearance.\n\u2022 We propose the integration of optimization modules into\nthe 3D generator, leveraging multi-view guidance to en-\nhance both shape quality and texture fidelity, thus achiev-\ning realistic 3D human assets.\n2. Related work\n3D Human Modeling. Parametric models [4, 21, 22, 30,\n40] serve as a common representation for 3D human model-\ning, they allows for robust control by deforming a template\nmesh with a series of low-dimensional parameters, but can\nonly generate naked 3D humans. Similar ideas have been\nextended to model clothed humans [2, 32], but geometric\nexpressivity is restricted due to the fixed mesh topology.\nSubsequent works [7, 41, 41] further introduce implicit sur-\nfaces to produce complex non-linear deformations of 3D\nbodies. Unfortunately, the aforementioned approaches all\nrequire 3D scans of various human poses for model fitting,\nwhich are difficult to acquire. With the explosion of NeRF,\nvaluable efforts have been made towards combining NeRF\nmodels with explicit human models [9, 11, 29, 45, 52]. Neu-\nral body [45] anchors a set of latent codes to the vertices\nFigure 2. An overview of the proposed scheme, which consists of three modules: 3D generative modeling (3DGM), the geometric sculpting\n(GS) and the explicit texturing (ET). 3DGM using synthesized diverse, balanced and structured human image with accurate camera \u03c6 to\nlearn generalizable 3D humans with the triplane-based architecture. GS is integrated as an optimization module by utilizing multi-view\nnormal constraints to refine and carve geometry details. ET utilizes UV partitioning and a differentiable rasterizer to disentangles explicit\nUV texture maps. Not only multi-view renderings but also realistic 3D models can be acquired for final results.\nof the SMPL model [30] and transforms the spatial loca-\ntions of the codes to the volume in the observation space.\nHumanNeRF [52] optimizes for a canonical, volumetric T-\npose of the human with a motion field to map the non-rigid\ntransformations. Nevertheless, these methods are learned\ndirectly from monocular videos or image sequences, where\nsubjects are single individuals wearing specific garments,\nthus limiting their scalability.\nGenerative 3D-aware Image Synthesis.\nRecently, 3D-\naware image synthesis methods have lifted image gen-\neration with explicit view control by integrating the 2D\ngenerative models [23\u201325] with 3D representations, such\nas voxels [16, 35, 36, 53], meshes [28, 50] and points\nclouds [1, 27]. GRAF [49] and \u03c0-GAN[5] firstly integrate\nthe implicit representation networks, i.e., NeRF [34], with\ndifferentiable volumetric rendering for 3D scene genera-\ntion. However, they have difficulties in training on high-\nresolution images due to the costly rendering process. Sub-\nsequent works have sought to improve the efficiency and\nquality of such NeRF-based GANs, either by adopting a\ntwo-stage rendering process [6, 14, 37, 39, 55] or a smart\nsampling strategy [8, 60]. StyleSDF [39] combines a SDF-\nbased volume renderer and a 2D StyleGAN network [24]\nfor photorealistic image generation. EG3D [6] introduces\na superior triplane representation to leverage 2D CNN-\nbased feature generators for efficient generalization over 3D\nspaces. Although these methods demonstrate impressive\nquality in view-consistent image synthesis, they are limited\nto simplified rigid objects such as faces, cats and cars.\nTo learn highly articulated humans from unstructured\n2D images, recent works [10, 12, 18, 19, 56, 58] inte-\ngrate the deformation field to learn non-rigid deformations\nbased on the body prior of estimated SMPL parameters.\nEVA3D [18] represents humans as a composition of mul-\ntiple parts with NeRF representations. Instead of directly\nrendering the image from a 3D representation, 3DHuman-\nGAN [56] uses an equivariant 2D generator modulated by\n3D human body prior, which enables to establish one-to-\nmany mapping from 3D geometry to synthesized textures\nfrom 2D images. AG3D [10] combines the 3D generator\nwith an efficient articulation module to warp from canonical\nspace into posed space via a learned continuous deforma-\ntion field. However, a gap still exists between the generated\nand real humans in terms of appearance, due to the impre-\ncise priors from complex poses as well as the data biases\nfrom limited human poses and imbalanced viewing angles\nin the dataset.\n3. Method Description\nOur goal is to develop a zero-shot 3D generative scheme\nthat does not rely on any pre-existing 3D or 2D collections,\nyet is capable of producing high-quality 3D humans that are\nvisually realistic, geometrically accurate and content-wise\ndiverse to generalize to arbitrary humans.\nAn overview of the proposed scheme is illustrated in\nFigure 2. We build a sequential pipeline with the follow-\ning three modules: the 3D generative modeling (3DGM),\nthe geometric sculpting (GS) and the explicit texturing\n(ET). The first module synthesizes view-balanced, struc-\ntured and diverse human images with known camera param-\neters. Subsequently, it learns a 3D generative model from\nthese synthetic data, focusing on realistic appearance mod-\neling (Section 3.1). To overcome the inaccuracy of the 3D\nshape, the GS module is incorporated during the inference\nprocess. It optimizes a hybrid representation with multi-\nview normal constraints to carve intricate mesh details (Sec-\ntion 3.2). Additionally, the ET module is employed to dis-\nentangle explicit texture by utilizing semantical UV par-\ntitioning and a differentiable rasterizer (Section 3.3). By\ncombining these modules, we are able to synthesize high-\nquality and faithful 3D human avatars by incorporating ran-\ndom noises or guided texts/images (Section 3.4).\n3.1. 3D generative modeling\nWithout any 3D or 2D collections, we develop a synthesis-\nbased flow to learn a 3D generative module from 2D syn-\nthetic data. We start by instantiating a 3D scene through\nthe projection of underlying 3D skeletons onto 2D pose im-\nages, utilizing accurate physical parameters (i.e., camera\nparameters). Subsequently, the projected 2D pose images\nserve as conditions to control the 2D diffusion model [59]\nfor synthesizing view-balanced, diverse, and lifelike human\nimages. Finally, we employ a triplane-based generator with\nenhanced designs to learn a generalizable 3D representation\nfrom the synthetic data. Details are described as follows.\n3D instantiation. Starting with a template body mesh (e.g.,\nSMPL-X [44]) positioned and posed in canonical space, we\nestimate the 3D joint locations P3d by regressing them from\ninterpolated vertices. We then project P3d onto 2D poses\nPi, i = 1, . . . , k from K horizontally uniformly sampled\nviewpoints \u03c6. In this way, paired 2D pose images and their\ncorresponding camera parameters {Pi, \u03c6i} are formulated.\nControlled 2D image synthesis. With the pose image Pi,\nwe feed it into off-the-shelf ControlNet [59] as the pose con-\ndition to guide diffusion models [47] to synthesize human\nimages in desired poses (i.e., views). The text prompt T is\nalso used for diverse contents. Given a prompt T, instead\nof generating a human image Is : Is = C(Pi, T) indepen-\ndently for each view \u03c6i, we horizontally concatenate K pose\nimages Pi \u2208 RH\u00d7W \u00d73, resulting in P\u2032\ni \u2208 RH\u00d7KW \u00d73 and\nfeed P\u2032\ni to C, along with a prompt hint of \u2018multi-view\u2019 in T.\nIn this way, multi-view human images I\u2032\ns are synthesized\nwith roughly coherent appearance. We split I\u2032\ns to single\nview images I\u03c6 under specific views \u03c6. This concatenation\nstrategy facilitates the convergence of distributions in syn-\nthetic multi-views, thus easing the learning of common 3D\nrepresentation meeting multi-view characteristics.\nGeneralizable 3D representation learning.\nWith syn-\nthetic data of paired {I\u03c6, \u03c6}, we learn the 3D generative\nmodule G3d from them to produce diverse 3D-aware human\nimages with realistic appearance. Inspired by EG3D [6],\nwe employ a triplane-based generator to produce a gener-\nalizable representation T and introduce a patch-composed\nneural renderer to learn intricate human representation ef-\nficiently. Specifically, instead of uniformly sampling 2D\npixels on the image I, we decompose patches in the ROI\nregion including human bodies, and only emit rays towards\npixels in these patches. The rays are rendered into RGB\ncolor with opacity values via volume rendering. Based on\nthe decomposed rule, we decode rendered colors to mul-\ntiple patches and re-combine these patches for full feature\nimages. In this way, the representation is composed of ef-\nfective human body parts, which directs the attention of the\nnetworks towards the human subject itself. This design fa-\ncilitates fine-grained local human learning while maintain-\ning computational efficiency.\nFor the training process, we employ two discriminators,\none for RGB images and another for silhouettes, which\nyields better disentanglement of foreground objects with\nglobal geometry. The training loss for this module L3d con-\nsists of the two adversarial terms:\nL3d = Ladv(Drgb, G3d) + \u03bbsLadv(Dmask, G3d),\n(1)\nwhere \u03bbs denotes the weight of silhouette item. Ladv is\ncomputed by the non-saturating GAN loss with R1 regular-\nization [33].\nWith the trained G3d, we can synthesize 3D-aware hu-\nman images I\u03c6\ng with view control, and extract coarse 3D\nshapes Mc from the density field of neural renderer using\nthe Marching Cubes algorithm [31].\n3.2. Geometric sculpting\nOur 3D generative module can produce high-quality and\n3D-consistent human images in view controls. However,\nits training solely relies on discriminations made using 2D\nrenderings, which can result in inaccuracies in capturing\nthe inherent geometry, especially for complex human bod-\nies. Therefore, we integrate the geometric sculpting, an op-\ntimization module leveraging geometric information from\nhigh-quality multi-views to carve surface details.\nCom-\nbined with a hybrid 3D representation and a differentiable\nrasterizer, it can rapidly enhance the shape quality within\nseconds.\nDMTET adaption. Owing to the expressive ability of ar-\nbitrary topologies and computational efficiency with direct\nshape optimization, we employ DMTET as our 3D repre-\nsentation in this module and adapt it to the coarse mesh Mc\nvia an initial fitting procedure. Specifically, we parameter-\nize DMTET as an MLP network \u03a8g that learns to predict\nthe SDF value s(vi) and the position offset \u03b4vi for each ver-\ntex vi \u2208 V T of the tetrahedral grid (V T, T). A point set\nP = {pi \u2208 R3} is randomly sampled near Mc and their\nSDF values SDF(pi) can be pre-computed. We adapt the\nparameters \u03c8 of \u03a8g by fitting it to the SDF of Mc:\nLada =\nX\npi\u2208P\n||s(pi; \u03c8) \u2212 SDF(pi)||2.\n(2)\nGeometry refinement.\nUsing the adapted DMTET, we\nleverage the highly-detailed normal maps N derived from\nrealistic multi-view images as a guidance to refine local sur-\nfaces. To obtain the pseudo-GT normals N\u03c6, we extract\nthem from I\u03c6\ng using a pre-trained normal estimator [54].\nFor the rendered normals \u02c6\nN\u03c6, we extract the triangular\nmesh Mtri from (V T, T) using the Marching Tetrahedra\n(MT) layer in our current DMTET. By rendering the gener-\nated mesh Mtri with differentiable rasterization, we obtain\nthe resulting normal map \u02c6\nN\u03c6. To ensure holistic surface\npolishing that takes into account multi-view normals, we\nrandomly sample camera poses \u03c6 that are uniformly dis-\ntributed in space. We optimize the parameters of \u03a8g using\nthe normal loss, which is defined as:\nLnorm = || \u02c6\nN\u03c6 \u2212 N\u03c6||2.\n(3)\nAfter rapid optimization, the final triangular mesh Mtri\ncan be easily extracted from the MT layer. If the hands ex-\nhibit noise, they can be optionally replaced with cleaner ge-\nometry hands from SMPL-X, benefiting from the alignment\nof the generated body in canonical space with the underly-\ning template body.\n3.3. Explicit texturing\nWith the final mesh, the explicit texturing module aims to\ndisentangle a UV texture map from multi-view renderings\nI\u03c6\ng . This intuitive module not only facilitates the incorpora-\ntion of high-fidelity textures but also enables various editing\napplications, as verified in Section 4.4.\nGiven the polished triangular mesh Mtri and multi-\nviews I\u03c6\ng , we model the explicit texture map Tuv of Mtri\nwith a semantic UV partition and optimize Tuv using a dif-\nferentiable rasterizer R [26]. Specifically, leveraging the\ncanonical properties of synthesized bodies, we semantically\nsplit Mtri into \u03b3 components and rotate each component\nvertically, thus enabling effective UV projection for each\ncomponent with cylinder unwarping. We then combine the\ntexture partitions together for the full texture Tuv. We opti-\nmize Tuv from a randomly initialized scratch using the tex-\nture loss, which consists of a multi-view reconstruction term\nand a total-variation (tv) term:\nLtex = Lrec + \u03bbtvLtv,\n(4)\nwhere \u03bbtv denotes the weight of the tv loss.\nMulti-view guidance. To ensure comprehensive texturing\nin the 3D space, we render the color images R(Mtri, \u03c6)\nand silhouettes S using R and optimize Tuv utilizing multi-\nview weighted guidance. Their pixel-alignment distances\nto the original multi-view renderings I\u03c6\ng are defined as the\nreconstruction loss:\nLrec =\nX\n\u03c6\u2208\u2126\nw\u03c6||R(Mtri, \u03c6) \u00b7 S \u2212 I\u03c6\ng \u00b7 S||2,\n(5)\nFigure 3. The visualized flowchart of our method that synthesize\ntextured 3D human avatars from input noises, texts or images.\nwhere \u2126 is the set of viewpoints {\u03c6i, i = 1, ..., k} and w\u03c6\ndenotes weights of different views. w\u03c6 equals to 1.0 for\n\u03c6 \u2208 {front, back} and 0.2 otherwise.\nSmooth constraint. To avoid abrupt variations and smooth\nthe generated texture Tuv, we utilize the total-variation loss\nLtv which is computed by:\nLtv =\n1\nh \u00d7 w \u00d7 c||\u2207x(Tuv) + \u2207y(Tuv)||,\n(6)\nwhere x and y denote horizontal and vertical directions.\n3.4. Inference\nBuilt upon the above modules, we can generate high-quality\n3D human avatars from either random noises or guided in-\nputs such as texts or images. The flowchart for this process\nis shown in Figure 3. For input noises, we can easily ob-\ntain the final results by sequentially using the 3DGM, GS\nand ET modules. For text-guided synthesis, we first convert\nthe text into a structured image using our controlled diffu-\nsion C, and then inverse it to the latent space using PTI [46].\nSpecially, the GS and ET modules provide an interface that\naccurately reflects viewed modifications in the final 3D ob-\njects. As a result, we utilize the guided image to replace\nthe corresponding view image, which results in improved\nfidelity in terms of geometry and texture. The same process\nis applied for input images as guided images.\n4. Experimental Results\nImplementation details. Our process begins by training\nthe 3D generative module (3DGM) on synthetic data. Dur-\ning inference, we integrate the geometric sculpting (GS)\nand explicit texturing (ET) as optimization modules. For\n3DGM, we normalize the template body to the (0, 1) space\nand place its center at the origin of the world coordinate sys-\ntem. We sample 7(K = 7) viewpoints uniformly from the\nhorizontal plane, ranging from 0\u25e6 to 180\u25e6 (front to back),\nwith a camera radius of 2.7. For each viewpoint, we gen-\nerate 100K images using the corresponding pose image.\nTo ensure diverse synthesis, we use detailed descriptions of\nFigure 4. Results of synthesized 3D human avatars at 5122.\nage, gender, ethnicity, hairstyle, facial features, and cloth-\ning, leveraging a vast word bank. To cover 360\u25e6 views, we\nhorizontally flip the synthesized images and obtain 1.4 mil-\nlion human images at a resolution of 5122 in total. We train\nthe 3DGM for about 2.5M iterations with a batch size of\n32, using two discriminators with a learning rate of 0.002\nand a generator learning rate of 0.0025. The training takes\n8 days on 8 NVIDIA Tesla-V100. For GS, we optimize \u03c8\nfor 400 iterations for DMTET adaption and 100 iterations\nfor surface carving (taking about 15s in total on 1 NVIDIA\nRTX 3090 GPU). For ET, we set \u03bbuv = 1 and optimize\nTuv for 500 iterations (around 10 seconds). We split Mtri\ninto 5(\u03b3 = 5) body parts (i.e., trunk, left/right arm/leg) with\ncylinder UV unwarping. We use the Adam optimizer with\nlearning rates of 0.01 and 0.001 for \u03a8g and Tuv, respec-\ntively. Detailed network architectures can be found in the\nsupplemental materials (Suppl).\n4.1. 3D human generation\nFigure 4 showcases several 3D human avatars synthesized\nby our pipeline, highlighting the image quality, geometry\naccuracy, and diverse outputs achieved through our method.\nAdditionally, we explore the interpolation of the latent con-\nditions to yield smooth transitions in appearance, leveraging\nthe smooth latent space learned by our generative model.\nFor more synthesized examples and interpolation results,\nplease refer to the Suppl.\n4.2. Comparisons\nQualitative comparison.\nIn Figure 5, we compare our\nmethod with three baselines: EVA3D [18] and AG3D [10],\nwhich are state-of-the-art methods for generating 3D hu-\nmans from 2D images, and EG3D [6], which serves as the\nfoundational backbone of our method. The results of first\ntwo methods are produced by directly using source codes\nand trained models released by authors. We train EG3D\nusing our synthetic images with estimated cameras from\nscratch. As we can see, EVA3D fails to produce 360\u25e6 hu-\nmans with reasonable back inferring. AG3D and EG3D are\nable to generate 360\u25e6 renderings but both struggle with pho-\ntorealism and capturing detailed shapes. Our method syn-\nthesizes not only higher-quality, view-consistent 360\u25e6 im-\nFigure 5. Qualitative comparison with three state-of-the-art methods: EVA3D [18], AG3D [10] and EG3D [6].\nTable 1. Quantitative evaluation using FID, IS-360, normal accu-\nracy (Normal) and identity consistency (ID).\nMethod\nFID \u2193\nIS-360 \u2191\nNormal \u2193\nID\u2191\nEVA3D [18]\n15.91\n3.19\n30.81\n0.72\nAG3D [10]\n10.93\n3.28\n20.83\n0.69\nEG3D [6]\n7.48\n3.26\n12.74\n0.71\nOurs\n2.73\n3.43\n5.62\n0.74\nages but also higher-fidelity 3D geometry with intricate de-\ntails, such as irregular dresses and haircuts.\nQuantitative comparison. Table 1 provides quantitative\nresults comparing our method against the baselines. We\nmeasure image quality with Frechet Inception Distance\n(FID) [17] and Inception Score [48] for 360\u25e6 views (IS-\n360). FID measures the visual similarity and distribution\ndiscrepancy between 50k generated images and all real im-\nages. IS-360 focuses on the self-realism of generated im-\nages in 360\u25e6 views. For shape evaluation, we compute FID\nbetween rendered normals and pseudo-GT normal maps\n(Normal), following AG3D. The FID and Normal scores\nof EVA3D and AG3D are directly fetched from their re-\nports. Additionally, we access the multi-view facial iden-\ntity consistency using the ID metric introduced by EG3D.\nTable 2.\nResults of models trained by replacing physical pa-\nrameters with estimated ones (w/o SYN-P) or removing patch-\ncomposed rendering (w/o PCR).\nOurs\nOurs-w/o SYN-P\nOurs-w/o PCR\nFID \u2193\n2.73\n4.28\n3.26\nIS-360 \u2191\n3.43\n3.31\n3.35\nOur method demonstrates significant improvements in FID\nand Normal, bringing the generative human model to a new\nlevel of realistic 360\u25e6 renderings with delicate geometry\nwhile also maintaining state-of-the-art view consistency.\n4.3. Ablation study\nSynthesis flow and patch-composed rendering. We as-\nsess the impact of our carefully designed synthesis flow by\ntraining a model with synthetic images but with camera and\npose parameters estimated by SMPLify-X [44] (w/o SYN-\nP). As Table 2 shows, the model w/o SYN-P results in worse\nFID and IS-360 scores, indicating that the synthesis flow\ncontributes to more accurate physical parameters for real-\nistic appearance modeling.\nBy utilizing patch-composed\nrendering (PCR), the networks focus more on the human\nregion, leading to more realistic results.\nFigure 6. Effects of the GS module to carve fine-grained surfaces.\nFigure 7. Effects of the ET module for guided synthesis.\nGeometry sculpting module (GS). We demonstrate the\nimportance of this module by visualizing the meshes before\nand after its implementation. Figure 6 (b) shows that the\npreceding module yields a coarse mesh due to the complex\nhuman anatomy and the challenges posed by decomposing\nambiguous 3D shapes from 2D images. The GS module uti-\nlizes high-quality multi-view outputs and employs a more\nflexible hybrid representation to create expressive humans\nwith arbitrary topologies. It learns from pixel-level surface\nsupervision, leading to a significant improvement in shape\nquality, characterized by smooth surfaces and intricate out-\nfits (Figure 6 (c)).\nExplicit texturing module (ET). This intuitive module not\nonly extracts the explicit UV texture for complete 3D assets\nbut also enables high-fidelity results for image guided syn-\nthesis. Following the flowchart in Figure 3, we compare\nthe results produced with and without this module. Our\nmethod without ET directly generates implicit renderings\nthrough PTI inversion, as shown in Figure 7 (b). While it\nsuccessfully preserves global identity, it struggles to synthe-\nsize highly faithful local textures (e.g., floral patterns). The\nET module offers a convenient and efficient way to directly\ninteract with the 3D representation, enabling the production\nof high-fidelity 3D humans with more consistent content in-\ncluding exquisite local patterns (Figure 7 (a, c)).\n4.4. Applications\nAvatar animation. All avatars produced by our method are\nin a canonical body pose and aligned to an underlying 3D\nskeleton extracted from SMPL-X. This alignment allows\nfor easy animation and the generation of motion videos, as\ndemonstrated in Figure 1 and Suppl.\nFigure 8. Results synthesized by adapting our method to various\nstyles (e.g., Disney cartoon characters) or contents (e.g., portrait\nheads).\nTexture doodle and local editing. Our approach benefits\nfrom explicitly disentangled geometry and texture, enabling\nflexible editing capabilities. Following the flowchart of text\nor image guided synthesis (Section 3.4), users can paint any\npattern or add text to a guided image. These modifications\ncan be transferred to 3D human models by inputting mod-\nified views into the texture module (e.g., painting the text\n\u2019hey\u2019 on a jacket as shown in Figure 1 (d)). Our approach\nalso allows for clothing editing by simultaneously injecting\nedited guide images with desired clothing into the GS and\nET modules (e.g., changing a jacket and jeans to bodysuits\nin Figure 1 (e)). More results can be found in Suppl.\nContent-style free adaption.\nOur proposed scheme is\nversatile and can be extended to generate various types of\ncontents (e.g., portrait heads ) and styles (e.g., Disney car-\ntoon characters). To achieve this, we fine-tune our model\nusing synthetic images from these domains, allowing for\nflexible adaptation. We showcase the results in Figure 8.\nMore results and other discussions (e.g., limitations, nega-\ntive impact, etc.) can be found in Suppl.\n5. Conclusions\nWe introduced En3D, a novel generative scheme for sculpt-\ning 3D humans from 2D synthetic data. This method over-\ncomes limitations in existing 3D or 2D collections and sig-\nnificantly enhances the image quality, geometry accuracy,\nand content diversity of generative 3D humans. En3D com-\nprises a 3D generative module that learns generalizable 3D\nhumans from synthetic 2D data with accurate physical mod-\neling, and two optimization modules to carve intricate shape\ndetails and disentangle explicit UV textures with high fi-\ndelity, respectively. Experimental results validated the su-\nperiority and effectiveness of our method. We also demon-\nstated the flexibility of our generated avatars for animation\nand editing, as well as the scalability of our approach for\nsynthesizing portraits and Disney characters. We believe\nthat our solution could provide invaluable human assets for\nthe 3D vision community. Furthermore, it holds potential\nfor use in common 3D object synthesis tasks.\nAcknowledgements\nWe would like to thank Mengyang Feng and Jinlin Liu for\ntheir technical support on guided 2D image synthesis.\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas Guibas. Learning representations and generative\nmodels for 3d point clouds. In International conference on\nmachine learning, pages 40\u201349. PMLR, 2018. 3\n[2] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian\nTheobalt, and Gerard Pons-Moll. Video based reconstruction\nof 3d people models. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 8387\u2013\n8397, 2018. 2\n[3] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y.\nOgras, and Linjie Luo. Panohead: Geometry-aware 3d full-\nhead synthesis in 360deg. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 20950\u201320959, 2023.\n[4] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-\nbastian Thrun, Jim Rodgers, and James Davis. Scape: shape\ncompletion and animation of people. In ACM SIGGRAPH\n2005 Papers, pages 408\u2013416. 2005. 2\n[5] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein. pi-gan: Periodic implicit generative\nadversarial networks for 3d-aware image synthesis. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5799\u20135809, 2021. 3\n[6] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 2, 3, 4,\n6, 7\n[7] Xu Chen, Tianjian Jiang, Jie Song, Jinlong Yang, Michael J\nBlack, Andreas Geiger, and Otmar Hilliges.\ngdna: To-\nwards generative detailed neural avatars. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20427\u201320437, 2022. 2\n[8] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong.\nGram: Generative radiance manifolds for 3d-aware image\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10673\u2013\n10683, 2022. 3\n[9] Zijian Dong, Chen Guo, Jie Song, Xu Chen, Andreas Geiger,\nand Otmar Hilliges. Pina: Learning a personalized implicit\nneural avatar from a single rgb-d video sequence. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 20470\u201320480, 2022. 2\n[10] Zijian Dong, Xu Chen, Jinlong Yang, Michael J Black, Ot-\nmar Hilliges, and Andreas Geiger. Ag3d: Learning to gen-\nerate 3d avatars from 2d image collections. arXiv preprint\narXiv:2305.02312, 2023. 2, 3, 6, 7\n[11] Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J Black,\nand Timo Bolkart.\nCapturing and animation of body and\nclothing from monocular video. In SIGGRAPH Asia 2022\nConference Papers, pages 1\u20139, 2022. 2\n[12] Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen\nQian, Chen Change Loy, Wayne Wu, and Ziwei Liu.\nStylegan-human: A data-centric odyssey of human genera-\ntion. In European Conference on Computer Vision, pages\n1\u201319. Springer, 2022. 3\n[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014.\n[14] Jiatao\nGu,\nLingjie\nLiu,\nPeng\nWang,\nand\nChristian\nTheobalt.\nStylenerf:\nA style-based 3d-aware genera-\ntor for high-resolution image synthesis.\narXiv preprint\narXiv:2110.08985, 2021. 3\n[15] Honglin He, Zhuoqian Yang, Shikai Li, Bo Dai, and Wayne\nWu.\nOrthoplanes: A novel representation for better 3d-\nawareness of gans. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 22996\u201323007,\n2023.\n[16] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escap-\ning plato\u2019s cave: 3d shape from adversarial rendering. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 9984\u20139993, 2019. 3\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 7\n[18] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and\nZiwei Liu. Eva3d: Compositional 3d human generation from\n2d image collections.\narXiv preprint arXiv:2210.04888,\n2022. 2, 3, 6, 7\n[19] Suyi Jiang, Haoran Jiang, Ziyu Wang, Haimin Luo, Wen-\nzheng Chen, and Lan Xu.\nHumangen: Generating hu-\nman radiance fields with explicit priors. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12543\u201312554, 2023. 3\n[20] Kyungmin Jo, Wonjoon Jin, Jaegul Choo, Hyunjoon Lee,\nand Sunghyun Cho. 3d-aware generative model for improved\nside-view image synthesis. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 22862\u2013\n22872, 2023. 2\n[21] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total cap-\nture: A 3d deformation model for tracking faces, hands, and\nbodies. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 8320\u20138329, 2018. 2\n[22] Angjoo Kanazawa, Michael J Black, David W Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7122\u20137131, 2018. 2\n[23] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019. 3\n[24] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020. 3\n[25] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00a8ark\u00a8onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. Advances in Neural Infor-\nmation Processing Systems, 34:852\u2013863, 2021. 3\n[26] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,\nJaakko Lehtinen, and Timo Aila.\nModular primitives for\nhigh-performance differentiable rendering. ACM Transac-\ntions on Graphics (TOG), 39(6):1\u201314, 2020. 5\n[27] Ruihui Li, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and\nPheng-Ann Heng.\nPu-gan: a point cloud upsampling ad-\nversarial network. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 7203\u20137212,\n2019. 3\n[28] Yiyi Liao, Katja Schwarz, Lars Mescheder, and Andreas\nGeiger. Towards unsupervised learning of generative mod-\nels for 3d controllable image synthesis. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 5871\u20135880, 2020. 3\n[29] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu\nSarkar, Jiatao Gu, and Christian Theobalt.\nNeural actor:\nNeural free-view synthesis of human actors with pose con-\ntrol.\nACM transactions on graphics (TOG), 40(6):1\u201316,\n2021. 2\n[30] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. Smpl: A skinned multi-\nperson linear model. ACM Transactions on Graphics, 34(6),\n2015. 2, 3\n[31] William E Lorensen and Harvey E Cline. Marching cubes:\nA high resolution 3d surface construction algorithm. In Sem-\ninal graphics: pioneering efforts that shaped the field, pages\n347\u2013353. 1998. 4\n[32] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,\nGerard Pons-Moll, Siyu Tang, and Michael J Black. Learn-\ning to dress 3d people in generative clothing. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6469\u20136478, 2020. 2\n[33] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.\nWhich training methods for gans do actually converge? In\nInternational conference on machine learning, pages 3481\u2013\n3490. PMLR, 2018. 4\n[34] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n3\n[35] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\nRichardt, and Yong-Liang Yang.\nHologan: Unsupervised\nlearning of 3d representations from natural images.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7588\u20137597, 2019. 3\n[36] Thu H Nguyen-Phuoc, Christian Richardt, Long Mai,\nYongliang Yang, and Niloy Mitra.\nBlockgan: Learning\n3d object-aware scene representations from unlabelled im-\nages. Advances in neural information processing systems,\n33:6767\u20136778, 2020. 3\n[37] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-\ning scenes as compositional generative neural feature fields.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11453\u201311464, 2021.\n3\n[38] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\nAndreas Geiger. Differentiable volumetric rendering: Learn-\ning implicit 3d representations without 3d supervision. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3504\u20133515, 2020.\n[39] Roy\nOr-El,\nXuan\nLuo,\nMengyi\nShan,\nEli\nShecht-\nman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.\nStylesdf: High-resolution 3d-consistent image and geome-\ntry generation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13503\u2013\n13513, 2022. 2, 3\n[40] Ahmed AA Osman, Timo Bolkart, and Michael J Black.\nStar: Sparse trained articulated human body regressor. In\nComputer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part VI 16,\npages 598\u2013613. Springer, 2020. 2\n[41] Pablo Palafox, Alja\u02c7z Bo\u02c7zi\u02c7c, Justus Thies, Matthias Nie\u00dfner,\nand Angela Dai. Npms: Neural parametric models for 3d de-\nformable shapes. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 12695\u201312705,\n2021. 2\n[42] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 165\u2013174, 2019.\n[43] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\nZhu. Semantic image synthesis with spatially-adaptive nor-\nmalization. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 2337\u20132346,\n2019.\n[44] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and\nMichael J Black.\nExpressive body capture:\n3d hands,\nface, and body from a single image.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10975\u201310985, 2019. 4, 7\n[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9054\u20139063, 2021. 2\n[46] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel\nCohen-Or. Pivotal tuning for latent-based editing of real im-\nages.\nACM Transactions on graphics (TOG), 42(1):1\u201313,\n2022. 5\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 4\n[48] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016. 7\n[49] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. Graf: Generative radiance fields for 3d-aware im-\nage synthesis. Advances in Neural Information Processing\nSystems, 33:20154\u201320166, 2020. 3\n[50] Attila Szab\u00b4o, Givi Meishvili, and Paolo Favaro.\nUnsu-\npervised generative 3d shape learning from natural images.\narXiv preprint arXiv:1910.00287, 2019. 3\n[51] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021.\n[52] Chung-Yi Weng,\nBrian Curless,\nPratul P Srinivasan,\nJonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmannerf: Free-viewpoint rendering of moving people from\nmonocular video.\nIn Proceedings of the IEEE/CVF con-\nference on computer vision and pattern Recognition, pages\n16210\u201316220, 2022. 2, 3\n[53] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and\nJosh Tenenbaum. Learning a probabilistic latent space of\nobject shapes via 3d generative-adversarial modeling. Ad-\nvances in neural information processing systems, 29, 2016.\n3\n[54] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J\nBlack. Icon: Implicit clothed humans obtained from nor-\nmals.\nIn 2022 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 13286\u201313296.\nIEEE, 2022. 5\n[55] Yang Xue, Yuheng Li, Krishna Kumar Singh, and Yong Jae\nLee.\nGiraffe hd: A high-resolution 3d-aware generative\nmodel.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 18440\u2013\n18449, 2022. 3\n[56] Zhuoqian Yang, Shikai Li, Wayne Wu, and Bo Dai. 3dhu-\nmangan: 3d-aware human image generation with 3d pose\nmapping.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 23008\u201323019, 2023.\n3\n[57] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. Advances in Neu-\nral Information Processing Systems, 34:4805\u20134815, 2021.\n[58] Jianfeng Zhang, Zihang Jiang, Dingdong Yang, Hongyi Xu,\nYichun Shi, Guoxian Song, Zhongcong Xu, Xinchao Wang,\nand Jiashi Feng. Avatargen: a 3d generative model for an-\nimatable human avatars. In European Conference on Com-\nputer Vision, pages 668\u2013685. Springer, 2022. 3\n[59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 4\n[60] Peng Zhou,\nLingxi Xie,\nBingbing Ni,\nand Qi Tian.\nCips-3d:\nA\n3d-aware\ngenerator\nof\ngans\nbased\non\nconditionally-independent pixel synthesis.\narXiv preprint\narXiv:2110.09788, 2021. 3\n[61] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka.\nSean: Image synthesis with semantic region-adaptive nor-\nmalization.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5104\u2013\n5113, 2020.\n"
  },
  {
    "title": "Taming Mode Collapse in Score Distillation for Text-to-3D Generation",
    "link": "https://arxiv.org/pdf/2401.00909.pdf",
    "upvote": "8",
    "text": "Taming Mode Collapse in Score Distillation for Text-to-3D Generation\nPeihao Wang1*, Dejia Xu1, Zhiwen Fan1, Dilin Wang2, Sreyas Mohan2, Forrest Iandola2,\nRakesh Ranjan2, Yilei Li2, Qiang Liu1, Zhangyang Wang1, Vikas Chandra2\n1The University of Texas at Austin, 2Meta Reality Labs\n{peihaowang, dejia, zhiwenfan, atlaswang}@utexas.edu, lqiang@cs.utexas.edu\n{wdilin, sreyasmohan, fni, rakeshr, yileil, vchandra}@meta.com\nvita-group.github.io/3D-Mode-Collapse/\nAbstract\nDespite the remarkable performance of score distillation\nin text-to-3D generation, such techniques notoriously suf-\nfer from view inconsistency issues, also known as \u201cJanus\u201d\nartifact, where the generated objects fake each view with\nmultiple front faces. Although empirically effective meth-\nods have approached this problem via score debiasing or\nprompt engineering, a more rigorous perspective to explain\nand tackle this problem remains elusive. In this paper, we\nreveal that the existing score distillation-based text-to-3D\ngeneration frameworks degenerate to maximal likelihood\nseeking on each view independently and thus suffer from the\nmode collapse problem, manifesting as the Janus artifact in\npractice. To tame mode collapse, we improve score distilla-\ntion by re-establishing in entropy term in the corresponding\nvariational objective, which is applied to the distribution of\nrendered images. Maximizing the entropy encourages diver-\nsity among different views in generated 3D assets, thereby\nmitigating the Janus problem. Based on this new objective,\nwe derive a new update rule for 3D score distillation, dubbed\nEntropic Score Distillation (ESD). We theoretically reveal\nthat ESD can be simplified and implemented by just adopt-\ning the classifier-free guidance trick upon variational score\ndistillation. Although embarrassingly straightforward, our\nextensive experiments successfully demonstrate that ESD\ncan be an effective treatment for Janus artifacts in score\ndistillation.\n1. Introduction\nRecent advancements in text-to-3D technology have at-\ntracted considerable attention, particularly for its pivotal role\nin automating high-quality 3D content. This is especially\ncrucial in fields such as virtual reality and gaming, where\n3D content forms the bedrock. While numerous techniques\n*Work done during an internship with Meta.\nare available, the prevailing text-to-3D approach is based on\nscore distillation [30], popularized by DreamFusion and its\nfollow-up works [4, 18, 25, 50, 54, 55].\nScore distillation leverages a pre-trained 2D diffusion\nmodel to sample over the 3D parameter space (i.e. Neural\nRadiance Fields (NeRF) [26]) such that views rendered from\na random angle satisfy the statistics of the image distribu-\ntion. This algorithm is implemented by backpropagating\nthe estimated score of each view via the chain rule. Despite\nthe notable progress achieved with score distillation-based\napproaches, it is widely observed that 3D content generated\nusing score distillation suffers from the Janus problem [11],\nreferring to the artifacts that generated 3D objects contain\nmultiple canonical views.\nTo understand this drawback of score distillation, we draw\nthe theoretical connection between the Janus problem and\nmode collapse, a statistical term describing a distribution con-\ncentrating on the high-density area while losing information\nabout the probability tail. We first uncover that the optimiza-\ntion of existing score distillation-based text-to-3D generation\ndegenerates to a maximum likelihood objective, making it\nsusceptible to model collapse. As pre-trained diffusion mod-\nels are biased to frequently encountered views [11]1, this\noversight leads all views opt to convergence toward the point\nwith the highest likelihood, manifesting as the Janus arti-\nfact in practical applications. The main limitation of current\nmethods is that their distillation objectives solely maximize\nthe likelihood of each view independently, without consider-\ning the diversity between different views.\nTo address the aforementioned issue, we propose a princi-\npled approach Entropic Score Distillation (ESD), which reg-\nularizes the score distillation process by entropy maximiza-\ntion of the rendered image distribution, thereby enhancing\nthe diversity of views in generated 3D assets and alleviating\nthe Janus problem. Our derived ESD update admits a simple\nform as a weighted combination of scores for pre-trained\n1For example, it is common that a frontal view of a cat is more likely to\nbe sampled from latent diffusion models than the back view.\narXiv:2401.00909v1  [cs.CV]  31 Dec 2023\nOur results\n\"Janus\" results\nA cat with tiger stripes\nA bald eagle carved out of wood\nA chimpanzee dressed like Henry VIII king of England\nFront View\nBack View\nFigure 1. A Preview of Qualitative Results. We present the front and back views of objects synthesized by VSD (ProlificDreamer) on the\nright two columns, and four views of our generated results on the left. VSD suffers from \u201cJanus\u201d problem, where both front and back views\ncontain a frontal face of the targeted object, while our method effectively mitigates this artifact. Best view in an electronic copy.\nimage distribution and rendered image distribution. Com-\npared with Score Distillation Sampling (SDS) [30], our ESD\ninvolves the score of the rendered image distribution, serving\nto maximize the entropy of the rendered image distribution.\nUnlike Variational Score Distillation (VSD) [55], the learned\nscore function of the rendered image distribution does not\ndepend on the camera pose. This subtle difference has a\nmore profound impact, as we show the score function of\nrendered images modeled by VSD corresponds to an objec-\ntive with fixed entropy, thereby having no influence on view\nvariety. In contrast, ESD optimizes for a Kullback-Leibler di-\nvergence with a non-constant entropy term parameterized by\nthe 3D model, leading to an effect that encourages diversity\namong different views.\nIn practice, we find it challenging to optimize the score\nof the rendered image distribution without conditioning on\nthe camera pose. To facilitate training, we discover that the\ngradient from the entropy can be decomposed into a com-\nbination of scores: one depends on the camera pose, and\nthe other independent of it, with a coefficient interacting\nbetween these two terms. Through this theoretical establish-\nment, we are able to adopt a handy implementation of ESD\nby Classifier Free Guidance (CFG) trick [9] where condi-\ntional and unconditional scores are trained alternatively and\nmixed during inference.\nThrough extensive experiments with our proposed ESD,\nwe demonstrate its efficacy in alleviating the Janus problem\nand its significant advantages in improving 3D generation\nquality when compared to the baseline methods [30, 55] and\nother remedy techniques [2, 11]. As a side contribution, we\nalso borrow two inception scores [36] to evaluate text-to-\n3D results and numerically probe model collapse in score\ndistillation. We show these two metrics can effectively char-\nacterize the quality and diversity of views, highly matching\nour qualitative observations.\n2. Background\n2.1. Diffusion Models\nDiffusion models, as demonstrated by various works [10,\n44, 46, 48], have shown to be highly effective in text-to-\nimage generation. Technically, a diffusion model learns\nto gradually transform a normal distribution N(0, I) to\nthe target distribution pdata(x|y) where y denotes the text\nprompt embeddings.\nThe sampling trajectory is deter-\nmined by a forward process with the conditional proba-\nbility pt(xt|x0) = N(xt|\u03b1tx0, \u03c32\nt I), where xt \u2208 RD\nrepresents the sample at time t \u2208 [0, T], and \u03b1t, \u03c3t > 0\nare time-dependent diffusion coefficients. Consequently,\nthe distribution at time t can be formulated as pt(xt|y) =\nImage\nSpace\nMaximal Likelihood Seeking\nOur Objective\nViews rendered\nfrom 3D scene\nImages from \ndiffusion model\nFigure 2. Illustration of the effect of entropy regularization. Learned image distributions often exhibit a higher probability mass for\nobjects\u2019 frontal faces. Pure maximal likelihood seeking is opt to mode collapse (Sec. 3). Adding entropy regularization can expand the\nsupport of fitted distribution q\u03b8\nt (x|y) with mode-covering behavior (Sec. 4).\nR\npdata(x0|y) N(xt|\u03b1tx0, \u03c32\nt I)dx0.\nDiffusion models\ngenerate samples through a reverse process starting from\nGaussian noises, which can be described by the ODE:\ndxt/dt = \u2212\u2207x log pt(xt) with the boundary condition\nxT \u223c N(0, I) [21, 45, 48]. Such a process requires the\ncomputation of score function \u2207x log pt(xt) which is of-\nten obtained by fitting a time-conditioned noise estimator\n\u03f5\u03d5 : RD \u2192 RD using score matching loss [14, 47, 52].\n2.2. Text-to-3D Score Distillation\nScore distillation based 3D asset generation requires repre-\nsenting 3D scenes as learnable parameters \u03b8 \u2208 RN equipped\nwith a differentiable renderer g(\u03b8, c) : RN \u2192 RD that\nprojects 3D scene \u03b8 into images with respect to the camera\npose c. Here N, D are the dimensions of the 3D parameter\nspace and rendered images, respectively. Neural radiance\nfields (NeRF) [26] are often employed as the underlying 3D\nrepresentation for its capability of modeling complex scenes.\nRecent works [4, 13, 18, 25, 30, 50, 54, 55] demonstrate\nthe feasibility of using a pretrained 2D diffusion model\nto guide 3D object creation. Below, we elaborate on two\nscore distillation schemes, adopted therein: Score Distilla-\ntion Sampling (SDS) [30] and Variational Score Distillation\n(VSD) [55].\nScore Distillation Sampling (SDS).\nSDS updates the 3D\nparameter \u03b8 as follows 2:\n\u2207\u03b8JSDS(\u03b8) = \u2212 E\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n(\u03c3t\u2207 log pt(xt|y) \u2212 \u03f5)\n\u0015\n,\n(1)\nwhere the expectation is taken over timestep t \u223c U[0, T],\nGaussian noises \u03f5 \u223c N(0, I), and camera pose c \u223c pc(c).\nHere is \u2207 log p is a pre-trained diffusion model \u03f5\u03d5(x, t, y)\nand xt is a noisy version of the rendering given by camera\npose c. xt = \u03b1tg(\u03b8, c) + \u03c3t\u03f5. Updating \u03b8 as in Eq. (1) has\n2Without special specification, expectations are taken over all relevant\nrandom variables and Jacobian matrices are transposed by default.\nbeen shown to minimize the evidence lower bound (ELBO)\nfor the rendered images, see Wang et al. [54], Xu et al. [56].\nVariational Score Distillation (VSD).\nVSD [55] is in-\ntroduced in ProlificDreamer, VSD improves upon SDS by\nderiving the following Wasserstein gradient flow [51]:\n\u2207\u03b8JV SD(\u03b8) = \u2212 E\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n(\u03c3t\u2207 log pt(xt|y)\n\u2212\u03c3t\u2207 log qt(xt|c))\n\u0015\n.\n(2)\nSimilarly, xt = \u03b1tg(\u03b8, c) + \u03c3t\u03f5 is the noisy observation of\nthe rendered image. In contrast to SDS, VSD introduces a\nnew score function of the noisy rendered images conditioned\non the camera pose c. To obtain this score, Wang et al. [55]\nfine-tunes a diffusion model using images rendered from the\n3D scene as follows:\nmin\n\u03c8 E\n\u0002\n\u03c9(t)\u2225\u03f5\u03c8(\u03b1tg(\u03b8, c) + \u03c3t\u03f5, t, c, y) \u2212 \u03f5\u22252\n2\n\u0003\n,\n(3)\nwhere \u03f5\u03c8(x, t, c, y) is the noise estimator of \u2207 log qt(xt|c)\nas in diffusion models. As proposed in ProlificDreamer,\n\u03c8 is parameterized by LoRA [12] and initialized from a\npre-trained diffusion model same as \u2207 log pt.\n3. Revealing Mode Collapse in Score Distillation\nDespite the remarkable performance of SDS and VSD in 3D\nasset generation, it is widely observed that the synthesized\nobjects suffer from \u201cJanus\u201d artifacts. Janus artifacts refer\nto the generated 3D scene containing multiple canonical\nviews (the most representative perspective of the object such\nas the frontal face). In earlier works, Hong et al. [11] and\nHuang et al. [13] attribute this problem to unimodality of the\nlearned 2D image distribution since the training data for the\ndiffusion models are naturally biased to the most commonly\nseen views per each category. In this section, we examine\nextant distillation schemes from a statistical view, which has\nbeen overlooked in previous literature.\nIn principle, natural 2D images can be seen as random\nprojections of 3D scenes. Score distillation matches the im-\nage distribution generated by randomly sampled views with a\ntext-conditioned image distribution to recover the underlying\n3D representation. Hence, Janus artifact, in which each view\nbecomes uniform and identical to the most commonly seen\nviews, can be interpreted as a manifestation of distribution\ncollapse to samples within the high-density region. Such\ndistribution degeneration essentially corresponds to the sta-\ntistical phenomenon mode collapse, which happens when an\noptimized distribution fails to characterize the data diversity\nand concentrates on a single type of output [1, 6, 24, 36, 49].\nBelow we theoretically reveal why SDS and VSD are\nprone to mode collapse. As shown in Poole et al. [30], Wang\net al. [55], SDS and VSD equals to the gradient of the fol-\nlowing Kullback-Leibler (KL) divergence, i.e., JSDS(\u03b8) =\nJV SD(\u03b8) = JKL(\u03b8) up to an additive constant:\nJKL(\u03b8) = E\n\u0002\n\u2126(t) DKL(q\u03b8\nt (xt|c, y)\u2225pt(xt|y))\n\u0003\n,\n(4)\nwhere \u2126(t) = \u03c9(t)\u03c3t/\u03b1t and the expectation is taken\nover t \u223c U[0, T] and c \u223c pc(c).\nHere pt(xt|y) =\nR\np0(x0|y) N(xt|\u03b1tx0, \u03c32\nt I)dx0 is the image distribu-\ntion perturbed by Gaussian noises, while q\u03b8\nt (xt|c, y) =\nR\nq\u03b8\n0 (x0|c) N(xt|\u03b1tx0, \u03c32\nt I)dx0 models the image distri-\nbution generated by 3D parameter \u03b8 with respect to camera\npose c and diffused by Gaussian distribution. As shown by\nWang et al. [55], JKL(\u03b8) = 0 implies q\u03b8\n0 (x0|c) = p(x0|y),\ni.e., the distribution of synthesized views satisfy the text-\nconditioned image distribution.\nHowever, it has not escaped from our notice that\nq\u03b8\n0 (x0|c) = \u03b4(x0 \u2212 g(\u03b8, c)) is a Dirac distribution for both\nSDS and VSD. This causes the original KL divergence mini-\nmization (Eq. 4) degenerate to a Maximal Likelihood Esti-\nmation (MLE) problem:\nJKL(\u03b8) = \u2212 E\nh\n\u2126(t) Ext\u223cq\u03b8\nt (xt|c,y) log pt(xt|y)\ni\n|\n{z\n}\nJMLE(\u03b8)\n\u2212 E\n\u0002\n\u2126(t)H[q\u03b8\nt (xt|c, y)]\n\u0003\n|\n{z\n}\nconst.\n,\n(5)\nwhere H[q\u03b8\nt (xt|y)] = \u2212 Ext\u223cq\u03b8\nt (xt|c,y)[log q\u03b8\nt (xt|c, y)]\ndenotes the entropy of q\u03b8\nt (xt|y), which turns out to be a con-\nstant because q\u03b8\nt (xt|c, y) = N(xt|\u03b1tg(\u03b8, c), \u03c32\nt I) which\nhas fixed entropy once t, \u03b8 and c have been specified. See\nfull derivation in Appendix A.1.\nNote that Eq. 5 signifies JKL(\u03b8) = JMLE(\u03b8) up to\nan additive constant, hence JKL(\u03b8) shares all minima with\nJMLE(\u03b8). It is known that likelihood maximization is more\nprone to mode collapse. Intuitively, minimizing JMLE(\u03b8)\nseeks each view independently to have the maximum log-\nlikelihood on the image distribution p(x0|y). Since p(x0|y)\nis usually unimodal and peaks at the canonical view, each\nview of the scene will collapse to the same local minimum,\nresulting in Janus artifact (see Fig. 2). We postulate that\nthe existing distillation strategies may be inherently limited\nby their log-likelihood seeking behaviors, which are more\nsusceptible to mode collapse, especially with biased image\ndistributions.\n4. Entropy Regularized Score Distillation\n4.1. Entropic Score Distillation\nIn this section, we highlight the importance of the entropy in\nscore distillation. It is known that higher entropy implies the\ncorresponding distribution could cover a larger support of\nthe ambient space and thus increase the sample diversity. In\nEq. 5, the entropy term is shown to diminish in the training\nobjective, which causes each generated view to lack diversity\nand collapse to a single image with the highest likelihood.\nTo this end, we propose to bring in an entropy regular-\nization to JMLE(\u03b8) for boosting the view diversity. Since\nq\u03b8\nt (xt|c, y) has constant entropy, we regularize entropy for\nthe distribution q\u03b8\nt (xt|y) =\nR\nq\u03b8\nt (xt|c, y)pc(c)dc, which\ncan be simulated by randomly sampling views from the 3D\nparameter \u03b8. Consider the following objective:\nJEnt(\u03b8, \u03bb) = \u2212 E\nh\n\u2126(t) Ext\u223cq\u03b8\nt (xt|c,y) log pt(xt|y)\ni\n\u2212\u03bb E\n\u0002\n\u2126(t)H[q\u03b8\nt (xt|y)]\n\u0003\n,\n(7)\nwhere \u03bb is a hyper-parameter controlling the regularization\nstrength. We note that without H[q\u03b8\nt (xt|y)], each view\nis optimized independently and implicitly regularized by\nthe underlying parameterization. However, upon impos-\ning H[q\u03b8\nt (xt|y)], all views become explicitly correlated\nwith each other, as they collectively contribute to the en-\ntropy computation. Intuitively, JEnt(\u03b8, \u03bb) = JMLE(\u03b8) \u2212\n\u03bb E[\u2126(t)H[q\u03b8\nt (xt|y)]] seeks the maximal log-likelihood for\neach view while simultaneously enlarging the entropy for dis-\ntribution q\u03b8\nt (xt|y), which spans the support and encourages\ndiversity across the rendered views. To gain more insights,\nwe present the following theoretical results:\nTheorem 1. For any \u03bb \u2208 R and \u03b8 \u2208 RD, we have\nJEnt(\u03b8, \u03bb) = \u03bb Et[\u2126(t) DKL(q\u03b8\nt (xt|y)\u2225pt(xt|y))] + (1 \u2212\n\u03bb) Et,c[\u2126(t) DKL(q\u03b8\nt (xt|c, y)\u2225pt(xt|y))] + const.\nWe prove Theorem 1 in Appendix A.3. Theorem 1 im-\nplies that JEnt(\u03b8, \u03bb) essentially equal to a combination\nof two types of KL divergences, where the former one\nminimizes the distribution discrepancy between q\u03b8\nt (xt|y)\nand p\u03b8\nt (xt|y) which marginalizes the camera pose within\nq\u03b8\nt , while the latter is the original KL divergence JKL(\u03b8)\nadopted by SDS and VSD which takes expectation over c\nout of KL divergence.\nAlgorithm 1 ESD: Entropic score distillation for text-to-3D generation\nInput: A diffusion model \u03f5\u03d5(x, t, y); learnable 3D parameter \u03b8; coefficient \u03bb; text prompt y.\nInitialize \u03c8 for another diffusion model \u03f5\u03c8(x, t, y) with the parameter \u03d5 specified in diffusion model \u03f5\u03d5(x, t, y), parame-\nterized with LoRA.\nwhile not converged do\nRandomly sample a camera pose c \u223c pc and render a view x0 = g(\u03b8, c) from \u03b8.\nSample a t \u223c U[0, T] and add Gaussian noise \u03f5 \u223c N(0, I): xt = \u03b1tx0 + \u03c3t\u03f5.\n\u03b8 \u2190 \u03b8 + \u03b71\nh\n\u03c9(t) \u2202g(\u03b8,c)\n\u2202\u03b8\n(\u03f5\u03d5(xt, t, y) \u2212 \u03bb\u03f5\u03c8(xt, t, \u2205, y) \u2212 (1 \u2212 \u03bb)\u03f5\u03c8(xt, t, c, y)\ni\n(6)\nWith probability 1 \u2212 p\u2205, \u03c8 \u2190 \u03c8 \u2212 \u03b72\u2207\u03c8\n\u0002\n\u03c9(t)\u2225\u03f5\u03c8(xt, t, c, y) \u2212 \u03f5\u22252\n2\n\u0003\n.\nOtherwise, \u03c8 \u2190 \u03c8 \u2212 \u03b72\u2207\u03c8\n\u0002\n\u03c9(t)\u2225\u03f5\u03c8(xt, t, \u2205, y) \u2212 \u03f5\u22252\n2\n\u0003\n.\nend while\nReturn \u03b8\nNext, we derive the gradient of JEnt(\u03b8, \u03bb) that will be\nbackpropagated to update the 3D representation. It can be\nobtained by path derivative and reparameterization trick:\n\u2207\u03b8JEnt(\u03b8, \u03bb) = \u2212 E\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n(\u03c3t\u2207 log pt(xt|y)\n(8)\n\u2212 \u03bb\u03c3t\u2207 log q\u03b8\nt (xt|y)\n\u0001\u0015\n.\nThe full derivation is deferred to Appendix A.2. We name\nthis update rule as Entropic Score Distillation (ESD). Note\nthat ESD differs from VSD as its second score function does\nnot depend on the camera pose.\n4.2. Classifier-Free Guidance Trick\nSimilar to SDS and VSD, we approximate \u2207 log pt(xt|y)\nvia a pre-trained diffusion model \u03f5\u03d5(xt, t, y). However,\n\u2207 log q\u03b8\nt (x|y) is not readily available. We found that di-\nrectly fine-tuning a pre-trained diffusion model using ren-\ndered images to approximate \u2207 log q\u03b8\nt (x|y), akin to Prolific-\nDreamer, does not yield robust performance. We postulate\nthis difficulty arises from the removal of the camera con-\ndition, increasing the complexity of the distribution to be\nfitted.\nTo tackle this problem, we recall the result in Theorem\n1 that JEnt(\u03b8, \u03bb) can be written in terms of two KL diver-\ngence losses. Therefore, its gradient can be decomposed as a\nweighted combination of their gradients, which correspond\nto unconditional and conditional score functions in terms of\nthe camera pose c, respectively:\n\u2207\u03b8JEnt(\u03b8, \u03bb) = \u2212 E\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n(\u03c3t\u2207 log pt(xt|y)\n(9)\n\u2212 \u03bb\u03c3t\u2207 log q\u03b8\nt (xt|y) \u2212 (1 \u2212 \u03bb)\u03c3t\u2207 log q\u03b8\nt (xt|c, y))\n\u0015\n.\nWe formally prove Eq. 9 in Appendix A.3. With the above\nformulation, ESD can be implemented via the Classifier-Free\nGuidance (CFG) trick, which was initially proposed to bal-\nance the variety and quality of text-conditionally generated\nimages from diffusion models [9]. Algorithm 1 outlines the\ncomputation paradigm of ESD, in which we surrogate score\nfunctions in Eq. 9 with pre-trained and fine-tuned diffusion\nmodels (see Eq. 6), and takes random turns with a probability\np\u2205 to balance the training of conditional and unconditional\nscore functions, as suggested by Ho and Salimans [9].\n4.3. Discussion\nIn VSD, the camera-conditioned score is believed to play\na significant role in facilitating visual quality. Intuitively,\nsuch conditioning can equip the tuned diffusion model with\nmulti-view priors [19]. Also, Hertz et al. [7] suggests such\na method can be useful to stabilize the update of the im-\nplicit parameters. However, ESD counters this argument\nby suggesting that the camera condition might not always\nbe advantageous, particularly when the particle size is re-\nduced to one. In such cases, the resulting KL divergence\nprovably degenerates to a likelihood maximization algorithm\nvulnerable to mode collapse.\nIt is noteworthy that, even though their subtle differences\nin implementation, the optimization objectives of ESD and\nVSD are fundamentally different (see Sec. 4.1). ESD sets it-\nself apart from VSD by incorporating entropy regularization,\na crucial feature absent in VSD, aiming to augment diversity\nacross views. Despite originating from distinct objectives,\nour theoretical establishment allows for a straightforward\nimplementation of ESD based on VSD using the CFG trick.\nWe also note that by Theorem 1, ESD still optimizes for\na mode-seeking KL divergence. This suggests that ESD\nmay still lead to mode collapse especially when the target\nimage distribution is overly concentrated on one peak [36].\nWe provide more analysis on failure cases in Appendix D.\nCareful tuning of \u03bb is also necessary to balance the per-view\nsharpness/details and cross-view diversity. It also remains\nopen whether ESD can further benefit multi-particle VSD or\namortized text-to-3D training [22].\nMichelangelo style statue of dog reading news on a cellphone\nA rabbit, animated movie character, high detail 3d model\nA rotary telephone carved out of wood\nA plush dragon toy\nSDS (DreamFusion)\nVSD (ProlificDreamer)\nDebiased-SDS\nPerp-Neg\nESD (Ours)\nFigure 3. Qualitative Results. Our proposed outperforms all baselines in terms of better geometry and well-constructed texture details. Our\nresults deliver photo-realistic and diverse rendered views, while baseline methods more or less suffer from the Janus problem. Best view in\nan electronic copy.\n5. Other Related Work\nText-to-Image Diffusion Model.\nText-to-image diffusion\nmodels [31, 32] are cornerstone components of text-to-3D\ngeneration. It involves text embedding conditioning into\nthe iterative denoising process. Equipped with large-scale\nimage-text paired datasets, many works [28, 32, 34, 35]\nscale up to tackle text-to-image generation. Among them,\nlatent diffusion models [34] attracted great interest among\nthe open-source community since it reduced the computation\ncost by diffusing in the low-resolution latent space instead\nof directly in the pixel space. In addition, text-to-image\ndiffusion models have also found applications in various\ncomputer vision tasks, including text-to-3D [30, 43], image-\nto-3D [56], text-to-svg [16], text-to-video [17, 42], etc.\n3D Generation with 2D Priors.\nWell-annotated 3D data\nrequires immense effort to collect. Instead, a line of research\nstudies on how to learn 3D generative models using 2D su-\npervision. Early attempts, including pi-GAN [33], EG3D [3],\nGRAF [37], GIRAFFE [29], adopt adversarial loss between\nthe rendered images and natural images. DreamField [15]\nleverages CLIP to align NeRF with text prompts. More\nrecently, with the rapid development of text-to-image dif-\nfusion models, diffusion-based image priors have attracted\nincreasing interest, and score distillation has then become\nthe dominant technique. Pioneer works DreamFusion [30]\nand ProlificDreamer [55] have been introduced in detail in\nSec. 2. Their concurrent work SJC [54] derives the score Ja-\ncobian chaining method from another theoretical viewpoint\nof Perturb and Average Scoring. Even though diffusion\nmodels directly trained with 3D data nowadays demonstrate\nlargely improved results [20, 41], score distillation still plays\na pivotal role in ensuring view consistency.\nTechniques to Improve Score Distillation.\nProviding the\nempirical promise of score distillation, there have been nu-\nmerous techniques proposed to improve its effectiveness.\nMagic3D [18] and Fantasia3D [4] utilize mesh and DMTet\n[40] to disentangle the optimization of geometry and texture.\nTextMesh [50] and 3DFuse [38] use depth-conditioned text-\nto-image diffusion priors that support geometry-aware tex-\nturing. Score debiasing[11] and Perp-Neg [2] study to refine\nthe text prompts for a better 3D generation. DreamTime [13]\nand RED-Diff [23] investigate the timestep scheduling in the\nscore distillation process. HIFA [57] adopts multiple diffu-\nSDS (DreamFusion)\nVSD (ProlificDreamer)\nESD + Time Scheduling\nVSD + Time Scheduling\nFigure 4. Qualitative Results. We combine our proposed ESD\nwith timestep scheduling in DreamTime [13] and compare it against\nbaseline methods. Prompt: A caramic lion.\nsion steps for distillation. Score distillation also works with\nauxiliary losses, including CLIP loss [56] and adversarial\nloss [5, 39].\n6. Evaluation Metrics\nIn this section, we introduce four information-theoretic met-\nrics to numerically evaluate the generated 3D results with a\nparticular focus on identifying Janus artifacts or mode col-\nlapse. The metrics we propose comprehensively cover four\naspects: 1) the relevance with the text prompts, 2) distribu-\ntion fitness, 3) rendering quality, and 4) view diversity.\nCLIP Distance.\nWe compute the average distance be-\ntween rendered images and the text embedding to reflect the\nrelevance between generated results and the specified text\nprompt. Specifically, we render N views from the generated\n3D representations, and for each view, we obtain an embed-\nding vector through the image encoder of a CLIP model [53].\nIn the meantime, we compute the text embedding utilizing\nthe text encoder. The CLIP distance is computed as the\ncosine similarity between the image embeddings and text\nembeddings averaged over N views.\nFr\u00b4echet inception distance (FID).\nAs shown in Sec. 3\nand 4, score distillation essentially matches distributions via\nKL divergence. Hence, it becomes reasonable to employ\nFID to measure the distance between the image distribution\nq\u03b8(x0|y) generated by randomly rendering 3D represen-\ntation and the text-conditioned image distribution p(x0|y)\nmodeled by a diffusion model. We sample N images using\npre-trained latent diffusion model [34] given text prompts\nas the ground truth image dataset, and render N views uni-\nformly distributed over a unit sphere from the optimized 3D\nscene as the generated image dataset. Then standard FID\n[8] is computed between these two sets of images. Note that\nFID is known to be effective in quantitatively identifying\nmode collapse.\nInception Quality and Variety.\nThanks to our established\nconnection with mode collapse, we know that Janus problem\nis due to a lack of sample diversity. Inspired by Inception\nScore (IS) [36], we utilize entropy-related metrics to reflect\nthe generated image quality and diversity. We propose In-\nception Quality (IQ) and Inception Variety (IV), formulated\nas below:\nIQ(\u03b8) = Ec [H[pcls(y|g(\u03b8, c))]] ,\n(10)\nIV (\u03b8) = H[Ec[pcls(y|g(\u03b8, c)]],\n(11)\nwhere pcls(y|x) is a pre-trained classifier. IQ computes the\naverage entropy of the label logits predicted for all rendered\nviews, while IV computes the entropy of the averaged label\nlogits of all rendered views. Intuitively, the smaller IQ means\nhighly confident classification results on rendered views,\nwhich also indicates better visual quality of generated 3D\nassets. In the meanwhile, the higher IV signifies that each\nrendered view is likely to have a distinct label prediction,\nmeaning the 3D creation has higher view diversity. Note\nthat IV upper bounds IQ due to Jensen inequality. So we\ncan define Inception Gain IG = (IV \u2212 IQ)/IQ, which\ncharacterizes the information gain brought by knowing\nwhere the camera pose is, namely the improvement of\ndistinguishability among different views.\n7. Experiments\nSettings.\nIn this section, we empirically validate the effec-\ntiveness of our proposal. The chosen prompts are targeted at\nobjects with clearly defined canonical views, posing a chal-\nlenge for existing methods. Our baseline approaches include\nSDS (DreamFusion) [30] and VSD (ProlificDreamer) [55],\nas well as two methods dedicated to solving Janus problem:\nDebiased-SDS [11] and Perp-Neg [2]. For fair compari-\nson, all experiments are benchmarked under the open-source\nthreestudio framework. Geometry refinement [55] is adopted\nfor all distillation schemes. Please refer to Appendix B for\nmore implementation details.\nFigure 5. Ablation Studies on \u03bb. We investigate the choice of different entropy regularization strength \u03bb. Prompt: Michelangelo-style\nstatue of dog reading news on a cellphone.\nTable 1. Quantitative Comparisons. (\u2193) means the lower the\nbetter, and (\u2191) means the higher the better.\nCLIP (\u2193)\nFID (\u2193)\nIQ (\u2193)\nIV (\u2191)\nIG (\u2191)\nMichelangelo style statue of dog reading news on a cellphone\nSDS [30]\n0.694\n365.304\n4.469\n5.119\n0.145\nVSD [55]\n0.758\n296.168\n2.514\n3.041\n0.209\nESD (Ours)\n0.685\n292.716\n2.523\n4.080\n0.617\nA rabbit, animated movie character, high detail 3d model\nSDS [30]\n0.712\n200.084\n4.365\n4.970\n0.138\nVSD [55]\n0.720\n150.120\n1.083\n1.173\n0.083\nESD (Ours)\n0.725\n149.763\n1.385\n1.567\n0.132\nA rotary telephone carved out of wood\nSDS [30]\n0.853\n309.929\n3.478\n4.179\n0.202\nVSD [55]\n0.855\n305.920\n3.469\n4.214\n0.214\nESD (Ours)\n0.846\n299.578\n3.332\n4.439\n0.366\nA plush dragon toy\nSDS [30]\n0.889\n243.984\n4.622\n5.008\n0.084\nVSD [55]\n0.821\n273.495\n4.382\n4.728\n0.078\nESD (Ours)\n0.815\n237.518\n4.436\n4.971\n0.121\nQualitative Comparison.\nWe present qualitative compar-\nisons in Fig. 3. We encourage interested readers to our\nproject page for video results. It is clearly shown that our\nproposed ESD delivers more precise geometry with the Janus\nproblem alleviated. In comparison, the results presented by\nSDS and VSD all contain more or less corrupted geometry\nwith multi-face structures. Debiased-SDS and Perp-Neg are\nshown to be effective for some text prompts, while not so\nconsistent as ESD. Additionally, we find that ESD can work\nparticularly well when combined with the time-prioritized\nscheduling proposed in DreamTime [13], as shown in Fig. 4.\nThis means ESD is orthogonal to many other methods and\ncan cooperate with them to further reduce Janus artifacts.\nQuantitative Comparison.\nWith metrics proposed in Sec.\n6, we carry out quantitative comparisons against other meth-\nods. We present the results in Tab. 1. An extended version is\ndeferred to Appendix C. We observe that among all metrics,\nESD reaches the best CLIP score, FID, and IG. More im-\nportantly, ESD achieves the optimal balance between view\nquality and diversity as reflected by IQ and IV. Whereas,\nSDS suffers from low image quality with high IQ and VSD\nis limited by insufficient view variety with low IV. The su-\nESD w/ CFG\nESD w/ fine-tuned unconditional LoRA\nStep = 25k\nStep = 18k\nStep = 20k\nStep = 22k\nStep = 25k\nFigure 6. Ablation on Implementations. The successfully gen-\nerated result is obtained via our suggested CFG trick while the\ndiverged result is yielded by fitting the unconditioned score func-\ntion in Eq. 8 via LoRA. Prompt: an elephant skull.\nperior IG of ESD indicates that views inside the generated\nscene are distinguishable rather than collapsing to be the\nsame. These conclusions also align with our qualitative\nresults, justifying the effectiveness of our metrics.\nAblation Studies\nWe conduct ablation studies on the\nchoice of \u03bb (i.e. CFG weights) in Fig. 5. We demonstrate\nthat \u03bb can adjust ESD\u2019s preference toward view- quality or\ndiversity. When set to one, the produced Janus-free result\nalbeits with fewer realistic details in the textures. Conversely,\nwhen set to zero, ESD equates to VSD, and the Janus prob-\nlem emerges again. We empirically find that choosing \u03bb\naround 0.5 yields the best result, balancing fine-grained tex-\ntures and well-constructed geometry. We also implement\nESD by directly fitting the score function \u2207 log q\u03b8\nt (xt|y)\nwithout camera pose conditioning to validate the suggested\nimplementation by CFG trick. We show in Fig. 6 that this\noptimization scheme is unstable. As training proceeds, the\ngradient explodes, and the optimized texture overflows.\n8. Conclusion\nIn this paper, we reveal that existing score distillation meth-\nods degenerate to maximal likelihood seeking on each view\nindependently, leading to the mode collapse problem. We\nidentify that re-establishing the entropy term in the varia-\ntional objective brings a new update rule, called Entropic\nScore Distillation (ESD), which is theoretically equivalent to\nadopting classifier-free guidance trick upon variational score\ndistillation. ESD maximizes the entropy of the rendered\nimage distribution, encouraging diversity across views and\nmitigating the Janus problem.\nAcknowledgments\nP Wang is sincerely grateful for constructive feedback re-\ngarding this manuscript from Zhaoyang Lv, Xiaoyu Xiang,\nAmit Kumar, Jinhui Xiong, and Varun Nagaraja. P Wang\nalso thanks Ruisi Cai for providing decent visual materials\nfor illustration purposes. Any statements, opinions, findings,\nand conclusions or recommendations expressed in this ma-\nterial are those of the authors and do not necessarily reflect\nthe views of their employers or the supporting entities.\nReferences\n[1] Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou. Wasser-\nstein generative adversarial networks. In International con-\nference on machine learning, pages 214\u2013223. PMLR, 2017.\n4\n[2] Mohammadreza\nArmandpour,\nHuangjie\nZheng,\nAli\nSadeghian, Amir Sadeghian, and Mingyuan Zhou.\nRe-\nimagine the negative prompt algorithm:\nTransform 2d\ndiffusion into 3d, alleviate janus problem and beyond. arXiv\npreprint arXiv:2304.04968, 2023. 2, 6, 7\n[3] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 6\n[4] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d: Disentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 1, 3, 6\n[5] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,\nGang Yu, Lei Yang, and Guosheng Lin. It3d: Improved text-\nto-3d generation with explicit view synthesis. arXiv preprint\narXiv:2308.11473, 2023. 7\n[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 4\n[7] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta\ndenoising score. arXiv preprint arXiv:2304.07090, 2023. 5\n[8] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nAdvances in neural information processing systems, 30, 2017.\n7\n[9] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2, 5\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n[11] Susung Hong, Donghoon Ahn, and Seungryong Kim. Debi-\nasing scores and prompts of 2d diffusion for robust text-to-3d\ngeneration. arXiv preprint arXiv:2303.15413, 2023. 1, 2, 3,\n6, 7\n[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora:\nLow-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685, 2021. 3\n[13] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. Dreamtime: An improved optimiza-\ntion strategy for text-to-3d content creation. arXiv preprint\narXiv:2306.12422, 2023. 3, 6, 7, 8\n[14] Aapo Hyv\u00a8arinen and Peter Dayan.\nEstimation of non-\nnormalized statistical models by score matching. Journal\nof Machine Learning Research, 6(4), 2005. 3\n[15] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel,\nand Ben Poole. Zero-shot text-guided object generation with\ndream fields. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 867\u2013876,\n2022. 6\n[16] Ajay Jain, Amber Xie, and Pieter Abbeel. Vectorfusion: Text-\nto-svg by abstracting pixel-based diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 1911\u20131920, 2023. 6\n[17] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto Henschel,\nZhangyang Wang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-\nto-image diffusion models are zero-shot video generators.\narXiv preprint arXiv:2303.13439, 2023. 6\n[18] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-\nYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-\nto-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 1, 3, 6\n[19] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,\nSergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot\none image to 3d object. arXiv preprint arXiv:2303.11328,\n2023. 5\n[20] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gener-\nating multiview-consistent images from a single-view image.\narXiv preprint arXiv:2309.03453, 2023. 6\n[21] Ziming Liu, Di Luo, Yilun Xu, Tommi Jaakkola, and Max\nTegmark. Genphys: From physical processes to generative\nmodels. arXiv preprint arXiv:2304.02637, 2023. 3\n[22] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan\nLin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-\nYu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized\ntext-to-3d object synthesis. arXiv preprint arXiv:2306.07349,\n2023. 5\n[23] Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vah-\ndat. A variational perspective on solving inverse problems\nwith diffusion models. arXiv preprint arXiv:2305.04391,\n2023. 6\n[24] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-\nDickstein. Unrolled generative adversarial networks. arXiv\npreprint arXiv:1611.02163, 2016. 4\n[25] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12663\u201312673, 2023. 1, 3\n[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In European conference on computer vision, pages\n405\u2013421. Springer, 2020. 1, 3\n[27] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multireso-\nlution hash encoding. arXiv preprint arXiv:2201.05989, 2022.\n5\n[28] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 6\n[29] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-\ning scenes as compositional generative neural feature fields.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11453\u201311464, 2021. 6\n[30] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. arXiv preprint\narXiv:2209.14988, 2022. 1, 2, 3, 4, 6, 7, 8, 5\n[31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n6\n[32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n6\n[33] Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 12179\u201312188, 2021. 6\n[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 6, 7\n[35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\nRaphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans,\nJonathan Ho, David J. Fleet, and Mohammad Norouzi. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. In Advances in Neural Information Processing\nSystems, 2022. 6\n[36] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016. 2, 4, 5, 7\n[37] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. Graf: Generative radiance fields for 3d-aware im-\nage synthesis. Advances in Neural Information Processing\nSystems, 33:20154\u201320166, 2020. 6\n[38] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\nand Seungryong Kim.\nLet 2d diffusion model know 3d-\nconsistency for robust text-to-3d generation. arXiv preprint\narXiv:2303.07937, 2023. 6\n[39] Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng,\nBoyao Zhou, Hongwen Zhang, and Yebin Liu. Control4d:\nDynamic portrait editing by learning 4d gan from 2d diffusion-\nbased editor. arXiv preprint arXiv:2305.20082, 2023. 7\n[40] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid represen-\ntation for high-resolution 3d shape synthesis. Advances in\nNeural Information Processing Systems, 34:6087\u20136101, 2021.\n6, 5\n[41] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gener-\nation. arXiv preprint arXiv:2308.16512, 2023. 6\n[42] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 6\n[43] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual,\nIurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea\nVedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dy-\nnamic scene generation. arXiv preprint arXiv:2301.11280,\n2023. 6\n[44] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n2\n[45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. arXiv preprint arXiv:2010.02502,\n2020. 3\n[46] Yang Song and Stefano Ermon. Generative modeling by\nestimating gradients of the data distribution. Advances in\nneural information processing systems, 32, 2019. 2\n[47] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced\nscore matching: A scalable approach to density and score\nestimation. In Uncertainty in Artificial Intelligence, pages\n574\u2013584. PMLR, 2020. 3\n[48] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\narXiv preprint arXiv:2011.13456, 2020. 2, 3\n[49] Akash Srivastava, Lazar Valkov, Chris Russell, Michael U\nGutmann, and Charles Sutton. Veegan: Reducing mode col-\nlapse in gans using implicit variational learning. Advances in\nneural information processing systems, 30, 2017. 4\n[50] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gener-\nation of realistic 3d meshes from text prompts. arXiv preprint\narXiv:2304.12439, 2023. 1, 3, 6\n[51] C\u00b4edric Villani et al. Optimal transport: old and new. Springer,\n2009. 3\n[52] Pascal Vincent. A connection between score matching and\ndenoising autoencoders. Neural computation, 23(7):1661\u2013\n1674, 2011. 3\n[53] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao. Clip-nerf: Text-and-image driven manipulation\nof neural radiance fields. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 3835\u20133844, 2022. 7\n[54] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12619\u201312629, 2023. 1, 3, 6\n[55] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 1, 2, 3, 4, 6, 7,\n8, 5\n[56] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360 {\\deg} views. arXiv preprint\narXiv:2211.16431, 2022. 3, 6, 7\n[57] Joseph Zhu and Peiye Zhuang.\nHifa: High-fidelity text-\nto-3d with advanced diffusion guidance.\narXiv preprint\narXiv:2305.18766, 2023. 6\nA. Deferred Theory\nWe present deferred proofs and derivations in this section. In the beginning, we justify several claimed properties of JKL\n(Eq. 4). Then we formally derive ESD (Eq. 8) via our proposed objective JEnt (Eq. 7). Lastly, we prove that Classifier-Free\nGuidance trick (CFG) (Eq. 9) can be used to implement ESD.\nA.1. Justification of Vanilla KL Divergence JKL\nLet us consider KL divergence objective restated from Eq. 4:\nJKL(\u03b8) = Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nDKL(q\u03b8\nt (xt|c, y)\u2225pt(xt|y))\n\u0015\n,\n(12)\nwhere we recall the notations: \u03b1t, \u03c3t \u2208 R+ are time-dependent diffusion coefficients, c \u223c pc(c) is a camera pose drawn from\na prior distribution over SO(3) \u00d7 R3, and g(\u03b8, c) renders an image at viewpoint c from the 3D representation \u03b8. pt(xt|y) is\nthe Gaussian diffused image distribution denoted as below:\npt(xt|y) =\nZ\np0(x0|y) N(xt|\u03b1tx0, \u03c32\nt I)dx0,\n(13)\nwhere p0(x0|y) is the text-conditioned distribution of clean images. We also define qt(xt|c, y) as the Gaussian diffused\ndistribution of rendered images:\nq\u03b8\nt (xt|c, y) =\nZ\nq\u03b8\n0 (x0|c) N(xt|\u03b1tx0, \u03c32\nt I)dx0,\n(14)\nwhere we assume x0 is independent of text prompt y given the camera pose and underlying 3D representation. Furthermore,\nwe assume the rendering process has no randomness, thus q\u03b8\n0 (x0|c) = \u03b4(x0 \u2212 g(\u03b8, c)) can be written as a Dirac distribution.\nNow, we can derive the gradient of JKL(\u03b8), as summarized in the following lemma:\nLemma 1 (Gradient of JKL). For any \u03b8, we have:\n\u2207\u03b8JKL(\u03b8) = \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03c3t\u2207 log pt(xt|y)\n\u0015\n,\n(15)\nwhere xt = \u03b1tx0 + \u03c3t\u03f5, and x0 = g(\u03b8, c).\nProof. Due to the linearity of expectation, we have:\n\u2207\u03b8 Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nDKL(q\u03b8\nt (xt|c, y)\u2225pt(xt|y))\n\u0015\n(16)\n= Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\n\u2207\u03b8 DKL(q\u03b8\nt (xt|c, y)\u2225pt(xt|y))\n\u0015\n(17)\n= Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\n\u2207\u03b8 Ext\u223cq\u03b8\nt (xt|c,y)\n\u0014\nlog q\u03b8\nt (xt|c, y)\npt(xt|y)\n\u0015\u0015\n(18)\nFixing t and c, we apply reparameterization trick:\n\u2207\u03b8 Ext\u223cq\u03b8\nt (xt|c,y)\n\u0014\nlog q\u03b8\nt (xt|c, y)\npt(xt|y)\n\u0015\n= E\u03f5\u223cN (0,I)\n\uf8ee\n\uf8ef\uf8f0\u2207\u03b8 log q\u03b8\nt (\u03b1tg(\u03b8, c) + \u03c3t\u03f5|c, y)\n|\n{z\n}\n(a)\n\u2212 \u2207\u03b8 log pt(\u03b1tg(\u03b8, c) + \u03c3t\u03f5|y)\n|\n{z\n}\n(b)\n\uf8f9\n\uf8fa\uf8fb .\n(19)\nNotice that q\u03b8\nt (\u03b1tg(\u03b8, c) + \u03c3t\u03f5|c, y) = N(\u03f5|0, I) by substituting to Eq. 14, which is independent of \u03b8. Thus (a) = 0. For\nterm (b), by chain rule, we have:\n\u2207\u03b8 log pt(\u03b1tg(\u03b8, c) + \u03c3t\u03f5|y) = \u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207 log pt(\u03b1tg(\u03b8, c) + \u03c3t\u03f5|y).\n(20)\nPlugging back to Eq. 18, we obtain:\n\u2207\u03b8JKL(\u03b8) = \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\n\u00b7 \u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207 log pt(xt|y)\n\u0015\n(21)\n= \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03c3t\u2207 log pt(xt|y)\n\u0015\n,\n(22)\nwhere xt = \u03b1tg(\u03b8, c) + \u03c3t\u03f5.\nBelow we reproduce two results, which state both SDS (Eq. 1) and VSD (Eq. 2) optimize for JKL.\nLemma 2 (SDS minimizes JKL [30]). For any \u03b8, we have JSDS(\u03b8) = JKL(\u03b8) + const.\nProof. It is sufficient to show \u2207\u03b8JSDS(\u03b8) = \u2207\u03b8JKL(\u03b8). By expansion:\n\u2207\u03b8JSDS(\u03b8) = \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n(\u03c3t\u2207 log pt(xt|y) \u2212 \u03f5)\n\u0015\n(23)\n= \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03c3t\u2207 log pt(xt|y)\n\u0015\n|\n{z\n}\n\u2207\u03b8JKL(\u03b8)\n+ Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u03c3t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03f5\n\u0015\n,\n|\n{z\n}\n=0\n(24)\nwhere the second term equals 0 because \u03f5 is zero mean and sampled independently.\nLemma 3 (Single-particle VSD minimizes JKL [55]). For any \u03b8, we have JV SD(\u03b8) = JKL(\u03b8) + const.\nProof. It is sufficient to show \u2207\u03b8JV SD(\u03b8) = \u2207\u03b8JKL(\u03b8). By a similar expansion:\n\u2207\u03b8JV SD(\u03b8) = \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u0000\u03c3t\u2207 log pt(xt|y) \u2212 \u03c3t\u2207 log q\u03b8\nt (xt|c, y)\n\u0001\u0015\n(25)\n= \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03c3t\u2207 log pt(xt|y)\n\u0015\n|\n{z\n}\n\u2207\u03b8JKL(\u03b8)\n(26)\n+ Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03c3t\u2207 log q\u03b8\nt (xt|c, y)\n\u0015\n|\n{z\n}\n=(a)\n(27)\nThen we conclude the proof by showing (a) = 0 due to the fact that the first-order moment of score functions equals zero:\n(a) = Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nExt\u223cq\u03b8\nt (x|c,y)\n\u0014\n\u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207 log q\u03b8\nt (xt|c, y)\n\u0015\u0015\n(28)\n= Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nExt\u223cq\u03b8\nt (x|c,y)\n\u0002\n\u2207\u03b8 log q\u03b8\nt (xt|c, y)\n\u0003\u0015\n(29)\n= Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nZ \u2207\u03b8q\u03b8\nt (xt|c, y)\nq\u03b8\nt (xt|c, y) q\u03b8\nt (xt|c, y)dxt\n\u0015\n(30)\n= Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\n\u2207\u03b8\nZ\nq\u03b8\nt (xt|c, y)dxt\n\u0015\n= 0,\n(31)\nwhere we use change of variables by reversing the chain rule in Eq. 29, and the last step is because the integral equals one,\nwhich is independent of \u03b8.\nRemark 1. For multi-particle VSD, Lemma 3 may not hold. This is because the reverse chain rule in Eq. 29 is no longer\napplicable as q\u03b8\nt (xt|c, y) also becomes a function of \u03b8.\nFinally, we show that optimizing JKL is equivalent to optimizing JMLE (Eq. 5). First, recall that:\nJMLE(\u03b8) = \u2212 Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nExt\u223cq\u03b8\nt (xt|c,y) [log pt(xt|y)]\n\u0015\n.\n(32)\nThen we state the following lemma:\nLemma 4 (JKL is equivalent to maximal likelihood estimation). For any \u03b8, we have JMLE(\u03b8) = JKL(\u03b8) + const.\nProof. Again, we show \u2207\u03b8JMLE(\u03b8) = \u2207\u03b8JKL(\u03b8):\n\u2207\u03b8JMLE(\u03b8) = \u2207\u03b8 \u2212 Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nExt\u223cq\u03b8\nt (xt|c,y) [log pt(xt|y)]\n\u0015\n(33)\n= \u2212 Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nExt\u223cq\u03b8\nt (xt|c,y) [\u2207\u03b8 log pt(xt|y)]\n\u0015\n(34)\n= \u2212 Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nExt\u223cq\u03b8\nt (xt|c,y)\n\u0014\n\u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207 log pt(xt|y)\n\u0015\u0015\n(35)\n= \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03c3t\u2207 log pt(xt|y)\n\u0015\n,\n(36)\nwhere the last step is basic reparameterization of xt = \u03b1tx0 + \u03c3t\u03f5, and x0 = g(\u03b8, c).\nAs we argue in Sec. 3 (Eq. 5), the root reason JKL degenerates to JMLE is because the entropy term in JKL becomes a\nconstant independent of \u03b8.\nA.2. Derivation of Entropic Score Distillation\nIn this section, we derive the gradient for our entropy regularized objective (Eq. 8). We restate the entropy regularized objective\n(Eq. 7) below:\nJEnt(\u03b8, \u03bb) = \u2212 Et\u223cU[0,T ],c\u223cpc(c)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nExt\u223cq\u03b8\nt (xt|c,y) log pt(xt|y)\n\u0015\n\u2212 \u03bb Et\u223cU[0,T ]\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nH[q\u03b8\nt (xt|y)]\n\u0015\n,\n(37)\nwhere the entropy term H[q\u03b8\nt (xt|y)] is defined as:\nH\n\u0002\nq\u03b8\nt (xt|y)\n\u0003\n= \u2212 Ext\u223cq\u03b8\nt (xt|y)\n\u0002\nlog q\u03b8\nt (xt|y)\n\u0003\n,\n(38)\nand distribution q\u03b8\nt (xt|y) is defined as:\nq\u03b8\nt (xt|y) =\nZ\nq\u03b8\nt (xt|c, y)pc(c)dc.\n(39)\nNotice that JEnt(\u03b8, \u03bb) = JMLE(\u03b8) \u2212 \u03bb Et\u223cU[0,T ]\nh\n\u03c9(t) \u03c3t\n\u03b1t H[q\u03b8\nt (xt|y)]\ni\n, therefore, to derive Eq. 8, we simply need the\ngradient of the entropy term:\nLemma 5 (Gradient of entropy). It holds that:\n\u2207\u03b8H\n\u0002\nq\u03b8\nt (xt|y)\n\u0003\n= \u2212 Ec\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207 log q\u03b8\nt (xt|y)\n\u0015\n.\n(40)\nProof. We expand entropy by reparameterization of q\u03b8\nt (xt|y) as sampling two independent variables c, \u03f5:\n\u2207\u03b8H\n\u0002\nq\u03b8\nt (xt|y)\n\u0003\n= \u2207\u03b8 Ec\u223cpc(c),\u03f5\u223cN (0,I)\n\u0002\n\u2212 log q\u03b8\nt (\u03b1tg(\u03b8, c) + \u03c3t\u03f5|y)\n\u0003\n(41)\n= \u2212 Ec\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u2207\u03b8 log q\u03b8\nt (xt|y) + \u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207xt log q\u03b8\nt (xt|y)\n\u0015\f\f\f\f\nxt=\u03b1tg(\u03b8,c)+\u03c3t\u03f5\n(42)\n= \u2212 Ext\u223cq\u03b8\nt (xt|y)\n\u0002\n\u2207\u03b8 log q\u03b8\nt (xt|y)\n\u0003\n|\n{z\n}\n=(a)\n\u2212 Ec\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03b1t\n\u2202g(\u03b8, c)\n\u2202\u03b8\n\u2207xt log q\u03b8\nt (\u03b1tg(\u03b8, c) + \u03c3t\u03f5|\u03b8)\n\u0015\n,\n(43)\nwhere it is noteworthy that \u2207xt log q\u03b8\nt simply denotes the score function of q\u03b8\nt by explicitly indicating the derivative is taken\nin terms of xt. Eq. 42 is obtained by path derivative. It remains to show (a) = 0. We recall that the first-order moment of a\nscore function equals to zero:\n(a) =\nZ\n\u2207\u03b8 log q\u03b8\nt (xt|y)q\u03b8\nt (xt|y)dxt =\nZ \u2207\u03b8q\u03b8\nt (xt|y)\nq\u03b8\nt (xt|y) q\u03b8\nt (xt|y)dxt\n(44)\n= \u2207\u03b8\nZ\nq\u03b8\nt (xt|y)dxt\n(45)\n= 0,\n(46)\nwhere the last step involves a change of variable and the integral turns out to be independent of \u03b8.\nAs a consequence, we can conclude the update rule yielded by Eq. 7 in the following theorem:\nTheorem 2 (Entropic Score Distillation). For any \u03b8 and \u03bb \u2208 R, the following holds:\n\u2207\u03b8JEnt(\u03b8, \u03bb) = \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u0000\u03c3t\u2207 log pt(xt|y) \u2212 \u03bb\u03c3t\u2207 log q\u03b8\nt (xt|y)\n\u0001\u0015\n,\n(47)\nwhere xt = \u03b1tx0 + \u03c3t\u03f5, and x0 = g(\u03b8, c).\nProof. Since JEnt(\u03b8, \u03bb) = JMLE(\u03b8) \u2212 \u03bb Et\u223cU[0,T ]\nh\n\u03c9(t) \u03c3t\n\u03b1t H[q\u03b8\nt (xt|y)]\ni\n, by Lemma 4 and Lemma 5:\n\u2207\u03b8JEnt(\u03b8, \u03bb) = \u2207\u03b8JMLE(\u03b8) \u2212 \u03bb Et\u223cU[0,T ]\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\n\u2207\u03b8H[q\u03b8\nt (xt|y)]\n\u0015\n(48)\n= \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03c3t\u2207 log pt(xt|y)\n\u0015\n(49)\n+ Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03bb\u03c3t\u2207 log q\u03b8\nt (xt|y)\n\u0015\n,\n(50)\nby which we conclude the proof by merging two expectations.\nA.3. Justification of Classifier-Free Guidance Trick\nIn this section, we first prove Theorem 1 and show that CFG trick (Eq. 9) can be utilized to implement ESD. To begin with, we\ndefine another type of KL divergence as below:\nJKL = Et\u223cU[0,T ]\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nDKL(q\u03b8\nt (xt|y)\u2225pt(xt|y))\n\u0015\n(51)\nThen we present the following lemma, which represents the gradient of JKL:\nLemma 6 (Gradient of JKL). It holds that:\n\u2207\u03b8JKL = \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u0000\u03c3t\u2207 log pt(xt|y) \u2212 \u03c3t\u2207 log q\u03b8\nt (xt|y)\n\u0001\u0015\n,\n(52)\nwhere xt = \u03b1tx0 + \u03c3t\u03f5, and x0 = g(\u03b8, c).\nProof. We prove by showing JKL is a special case of JEnt when setting \u03bb = 1:\nJKL(\u03b8) = Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nlog q\u03b8\nt (xt|y)\npt(xt|y))\n\u0015\n(53)\n= \u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nlog pt(xt|y))\n\u0015\n+ Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nlog q\u03b8\nt (xt|y)\n\u0015\n(54)\n= JMLE(\u03b8) + Et\u223cU[0,T ] Ext\u223cq\u03b8\nt (xt|y)\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nlog q\u03b8\nt (xt|y)\n\u0015\n(55)\n= JMLE(\u03b8) \u2212 Et\u223cU[0,T ]\n\u0014\n\u03c9(t) \u03c3t\n\u03b1t\nH\n\u0002\nq\u03b8\nt (xt|y)\n\u0003\u0015\n= JEnt(\u03b8, 1)\n(56)\nNow we prove Theorem 1 using previous results:\nProof of Theorem 1. It is sufficient to show that \u2207\u03b8JEnt(\u03b8, \u03bb) = \u03bb\u2207\u03b8JKL(\u03b8) + (1 \u2212 \u03bb)\u2207\u03b8JKL(\u03b8). By Lemma 1, 6, as\nwell as Theorem 2, we can obtain:\n\u03bb\u2207\u03b8JKL(\u03b8) + (1 \u2212 \u03bb)\u2207\u03b8JKL(\u03b8) = \u2212\u03bb Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u0000\u03c3t\u2207 log pt(xt|y) \u2212 \u03c3t\u2207 log q\u03b8\nt (xt|y)\n\u0001\u0015\n(57)\n\u2212 (1 \u2212 \u03bb) Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u03c3t\u2207 log pt(xt|y)\n\u0015\n(58)\n= \u2207\u03b8JEnt(\u03b8, \u03bb),\n(59)\nby merging two expectations.\nFurther on, our CFG trick implementation of ESD (Eq. 9) can be regarded as a corollary of Theorem 1 and Lemma 3:\nTheorem 3 (Classifier-Free Guidance Trick). For any \u03b8 and \u03bb \u2208 R, \u2207\u03b8JEnt(\u03b8, \u03bb) equals to the following:\n\u2212 Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u0000\u03c3t\u2207 log pt(xt|y) \u2212 \u03bb\u03c3t\u2207 log q\u03b8\nt (xt|y) \u2212 (1 \u2212 \u03bb)\u03c3t\u2207 log q\u03b8\nt (xt|c, y)\n\u0001\u0015\n,\n(60)\nwhere xt = \u03b1tx0 + \u03c3t\u03f5, and x0 = g(\u03b8, c).\nProof. By Theorem 1, we know that \u2207\u03b8JEnt(\u03b8, \u03bb) = \u03bb\u2207\u03b8JKL(\u03b8) + (1 \u2212 \u03bb)\u2207\u03b8JKL(\u03b8). Moreover, by Lemma 3, we have\n\u2207\u03b8JKL(\u03b8) = \u2207\u03b8JV SD(\u03b8). As a result, the following can be derived:\n\u2207\u03b8JEnt(\u03b8, \u03bb) = \u2207\u03b8JKL(\u03b8) + (1 \u2212 \u03bb)\u2207\u03b8JV SD(\u03b8)\n(61)\n= \u2212\u03bb Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u0000\u03c3t\u2207 log pt(xt|y) \u2212 \u03c3t\u2207 log q\u03b8\nt (xt|y)\n\u0001\u0015\n(62)\n\u2212 (1 \u2212 \u03bb) Et\u223cU[0,T ],c\u223cpc(c),\u03f5\u223cN (0,I)\n\u0014\n\u03c9(t)\u2202g(\u03b8, c)\n\u2202\u03b8\n\u0000\u03c3t\u2207 log pt(xt|y) \u2212 \u03c3t\u2207 log q\u03b8\nt (xt|c, y)\n\u0001\u0015\n,\n(63)\nas desired after merging two expectations.\nB. Experiment Details\nIn this section, we provide more details on the implementation of ESD and the compared baseline methods. All of them\nare implemented under the threestudio framework and include three stages: coarse generation, geometry refinement,\nand texture refinement, following [55]. For the coarse generation stage, we adopt foreground-background disentangled\nhash-encoded NeRF [27] as the underlying 3D representation, and DMTet [40] for two refinement stages. All scenes are\ntrained for 25k steps for the coarse stage, 10k steps for geometry refinement, and 30k steps for texture refinement, for the\nsake of fair comparison. At each iteration, we randomly render one view (i.e., batch size equals one). We progressively adjust\nrendering resolution: within the first 5k steps, we render at 64\u00d764 resolution and increase to 256\u00d7256 resolution afterward.\nSDS [30].\nFollowing the original paper, we set the CFG weight to 100. Additionally, we encourage sparsity of the density\nfield and penalize the mismatch between orientation and predicted normal maps. Lighting augmentation is also enabled\nfor SDS. The geometry refinement step is directly borrowed from VSD: a DMTet is initialized by NeRF\u2019s density field\nvia marching cube while the end-to-end optimization with SDS is then conducted on the geometry representation for both\ngeometry and texture.\nVSD [55].\nWe reuse the standard setting of VSD for all three stages. In particular, we fix the CFG coefficient to 7.5 and\nonly use single-particle VSD, conforming with our theoretical analysis. During the geometry refinement stage, we adopt SDS\nguidance instead of VSD.\nDebiased-SDS [11].\nOur implementation of Debiased-SDS is built upon SDS. We enable both score debiasing and prompt\ndebiasing. For score debiasing, we follow the default setting and linearly increase the absolute threshold for gradient clipping\nfrom 0.5 to 2.0. All other hyperparameters follow from SDS.\nPerp-Neg [2].\nPerp-Neg implementation is based on SDS as well. As suggested by the original paper, in positive prompts, we\nleverage weights rinterp = 1\u22122|azimuth|/\u03c0 for front-side prompt interpolation and rinterp = 2\u22122|azimuth|/\u03c0 for side-back\ninterpolation. In negative prompts, the interpolating function is chosen as the shifted exponential function \u03b1 exp(\u2212\u03b2rinterp)+\u03b3.\nSpecifically, we choose \u03b1sf = 1, \u03b2sf = 0.5, \u03b3sf = \u22120.606, \u03b1fsb = 1, \u03b2fsb = 0.5, \u03b3fsb = 0.967, \u03b1fs = 4, \u03b2fs = 0.5, \u03b3fs =\n\u22122.426, \u03b1sf = 4, \u03b2sf = 0.5, \u03b3sf = \u22122.426. See [2] for more details on the meaning of these hyperparameters.\nESD.\nOur ESD implementation is similar to VSD. We leverage the extrinsics matrix (4 \u00d7 4) as the camera pose embedding,\nand condition the diffusion model by replacing its class embedding branch. We introduce CFG trick to linearly mix camera-\nconditioned and unconditioned score functions fine-tuned with rendered images. We find CFG 0.5 generally yields desirable\nresults. We also set the probability of unconditioned training to 0.5. In particular, view-dependent prompting is disabled for\nthe fine-tuned score function.\nC. Full Numerical Evaluation\nIn this section, we include numerical evaluations of Debiased-SDS and Perp-Neg as completion of Tab. 1. The results\nare reported in Tab. 2. The conclusion is consistent with our argument in Sec. 7. Our ESD consistently outperforms all\nthe compared baseline methods, especially in FID and IG. This implies that ESD effectively boosts the view diversity and\naccurately matches the distribution between pre-trained image distribution and rendered image distribution.\nTable 2. Quantitative Comparisons including all baselines. (\u2193) means the lower the better, and (\u2191) means the higher the better.\nCLIP (\u2193)\nFID (\u2193)\nIQ (\u2193)\nIV (\u2191)\nIG (\u2191)\nCLIP (\u2193)\nFID (\u2193)\nIQ (\u2193)\nIV (\u2191)\nIG (\u2191)\nMichelangelo style statue of dog reading news on a cellphone\nA rabbit, animated movie character, high detail 3d model\nSDS [30]\n0.694\n365.304\n4.469\n5.119\n0.145\n0.712\n200.084\n4.365\n4.970\n0.138\nVSD [55]\n0.758\n296.168\n2.514\n3.041\n0.209\n0.720\n150.120\n1.083\n1.173\n0.083\nDebiased-SDS [11]\n0.778\n351.493\n4.058\n4.814\n0.186\n0.735\n216.058\n4.443\n4.857\n0.093\nPerp-Neg [2]\n0.793\n306.918\n3.970\n4.572\n0.151\n0.727\n176.279\n2.453\n2.665\n0.086\nESD (Ours)\n0.685\n292.716\n2.523\n4.080\n0.617\n0.725\n149.763\n1.385\n1.567\n0.132\nA rotary telephone carved out of wood\nA plush dragon toy\nSDS [30]\n0.853\n309.929\n3.478\n4.179\n0.202\n0.889\n243.984\n4.622\n5.008\n0.084\nVSD [55]\n0.855\n305.920\n3.469\n4.214\n0.214\n0.821\n273.495\n4.382\n4.728\n0.078\nDebiased-SDS [11]\n0.927\n313.893\n4.098\n4.201\n0.025\n0.878\n262.474\n4.827\n4.954\n0.026\nPerp-Neg [2]\n0.868\n308.554\n3.488\n4.021\n0.153\n0.839\n309.276\n4.691\n4.816\n0.027\nESD (Ours)\n0.846\n299.578\n3.332\n4.439\n0.366\n0.815\n237.518\n4.436\n4.971\n0.121\nD. Failure Cases\nWe present a failure case produced by our ESD in Fig. 7, where the back view of the marble still contains a mouse face while\nthe side views exhibit duplicate ears. We point out that even though ESD can encourage diversity among views, however, it\nmay still incline to one mode when the target image distribution is overwhelmingly concentrated at one point. The specified\ntext prompt in Fig. 7 is in this case as we observe the majority of sampled images from a pre-trained diffusion model with the\ncorresponding prompt are the frontal views of a marble mouse.\nLeft\nFront\nBack\nRight\nA marble bust of a mouse\nFigure 7. Failure case. We present four views of a failure case yielded by ESD with prompt \u201cA marble bust of a mouse\u201d and CFG weight\n\u03bb = 0.5. Best view in an electronic copy.\n"
  },
  {
    "title": "Q-Refine: A Perceptual Quality Refiner for AI-Generated Image",
    "link": "https://arxiv.org/pdf/2401.01117.pdf",
    "upvote": "6",
    "text": "Q-REFINE: A PERCEPTUAL QUALITY REFINER FOR AI-GENERATED IMAGE\nChunyi Li1,2\nHaoning Wu3\nZicheng Zhang1\nHongkun Hao1\nKaiwei Zhang1\nLei Bai2\nXiaohong Liu1\nXiongkuo Min1\nWeisi Lin3\nGuangtao Zhai1\nShanghai Jiao Tong University1, Shanghai AI Lab2, Nanyang Technological University3\nOriginal AIGI\nTraditional \nRefiner\nQ-Refine\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\nFig. 1. The original AIGIs from AGIQA-3K [1], optimized by Traditional Refiners and Q-Refine we proposed. As a quality-\naware metric, the Q-Refine can add details on the blurred part, to better optimize low-quality regions of (1)(2); improve clarity\nin medium-quality regions of (3)(4) without changing the whole image; and avoid degrading the high-quality regions of (5)(6).\nABSTRACT\nWith the rapid evolution of the Text-to-Image (T2I) model\nin recent years, their unsatisfactory generation result has be-\ncome a challenge. However, uniformly refining AI-Generated\nImages (AIGIs) of different qualities not only limited opti-\nmization capabilities for low-quality AIGIs but also brought\nnegative optimization to high-quality AIGIs. To address this\nissue, a quality-award refiner named Q-Refine1 is proposed.\nBased on the preference of the Human Visual System (HVS),\nQ-Refine uses the Image Quality Assessment (IQA) metric to\nguide the refining process for the first time, and modify im-\nages of different qualities through three adaptive pipelines.\nExperimental shows that for mainstream T2I models, Q-\nRefine can perform effective optimization to AIGIs of differ-\nent qualities. It can be a general refiner to optimize AIGIs\nfrom both fidelity and aesthetic quality levels, thus expanding\nthe application of the T2I generation models.\nIndex Terms\u2014 AI-Generated Content, Image Quality\nAssessment, Image Restoration\n1The code will be released on https://github.com/Q-Future/Q-Refine\n1. INTRODUCTION\nAI-Generated Content (AIGC) refers to the creation of con-\ntent, such as images, videos, and music, using AI algorithms\n[1]. Since vision is the dominant way for humans to perceive\nthe external world, AI-Generated Images (AIGIs) [2] have\nbecome one of the most representative forms of AIGC. The\ndevelopment of Text-to-Image (T2I) models is a crucial step\nin the advancement of AIGIs, as it allows for the creation of\nhigh-quality images that can be used in a variety of applica-\ntions [3], including advertising, entertainment, and even sci-\nentific research. The importance of AIGI in today\u2019s internet\ncannot be overstated, as it has the potential to revolutionize\nthe way we consume and interact with visual content.\nWith the rapid technological evolution of T2I genera-\ntion techniques, there have been at least 20 representative\nT2I models coexisting up to 2023, whose generation quality\nvaries widely [1]. Coupled with confusing prompt input, un-\nreasonable hyper-parameter settings, and insufficient iteration\nepochs, the quality of today\u2019s AIGIs is still not satisfying.\nConsidering the wide application of AIGIs, their qual-\narXiv:2401.01117v1  [cs.CV]  2 Jan 2024\nLQ Bound\nWeight\nMQ Bound\nNoise\n+\nMask\nGaussian\nGenerator\nHQ Bound\nDenoise\n\u2026\nMode Selector\nDenoise\n\u2026\nBlind\nRestoration\nOriginal\nPrompt\nHQ\nPrompt\nQuality-Related Module  \nLQ Pipeline\nMQ Pipeline\nHQ Pipeline\nFaster-RCNN\nROI Polling\nQuality Map\nStage 1 Output\nStage 2 Output\nFinal Output\nInput\nFig. 2. Framework of Q-Refine, including a quality pre-prossess module, and three refining pipelines for low/medium/high\nquality (LQ/MQ/HQ) regions. The refining mechanisms for each pipeline are inspired by the predicted quality.\nity needs to be further optimized. However, this task is ex-\ntremely challenging as shown in Fig. 1. Firstly, positive op-\ntimization is difficult to achieve for Low-Quality (LQ) re-\ngions. If their quality falls into a local optimum, they won\u2019t\nbe modified as a global optimum; secondly, local negative\noptimization is a hidden danger of Medium-Quality (MQ)\nregions. Since the quality distribution of images varies, refin-\ners need to change only the LQ/MQ without affecting other\nregions; finally, global negative optimization is common in\nHigh-Quality (HQ) regions. Since the performance of re-\nfiners has a certain limit, blindly modifying an already high-\nquality image can easily lead to a decrease in quality.\n2. RELATED WORK AND CONTRIBUTIONS\nExisting AIGI quality refiners are mainly divided into two\ntypes. The most commonly used method is to treat AIGI as a\nNatural Sense Image (NSI) and use a large-scale neural net-\nwork for Image Restoration [4\u20136]; the other is to use the\nprompt as guidance, then put the AIGI back into a generative\nmodel for several epochs [7, 8]. However, both refiners ig-\nnore image quality. Using the same pipeline for LQ/MQ/HQ\nwill lead to insufficient enhancement in the LQ regions and\nnegative optimization in the HQ regions, essentially bringing\nall images to the MQ level as Fig. 1 shows.\nTherefore, the quality of AIGIs needs to be computed in\nadvance as refining guidance. However, Image Quality As-\nsessment (IQA) [9, 10] and Refiner cannot be directly com-\nbined.\nExisting IQA works\n[11\u201313] usually consider the\noverall quality of the image, instead of a quality map, making\nit difficult for the refiner to implement local optimization.\nTo enhance positive while avoiding negative optimization,\nwe found a way to combine IQA with refiners named Q-\nRefine, the first quality-aware refiner for AIGIs based on the\npreference of the Human Visual System (HVS) with the fol-\nlowing contribution: (i) We introduce the IQA map to guide\nthe AIGI refining for the first time.\nA new paradigm for\nAIGI restoration, namely using quality-inspired refining is\nproposed. (ii) We establish three refining pipelines that are\nsuitable for LQ/MQ/HQ regions respectively. Each pipeline\ncan self-adaptively determine the executing intensity accord-\ning to the predicted quality.\n(iii) We extensively conduct\ncomparative experiments between existing refiners and Q-\nRefine on mainstream AIGI quality databases.\nThe result\nproved the strong versatility of Q-Refine.\n3. PROPOSED METHOD\n3.1. Framework\nSince perceptual quality has been widely recognized as a de-\ncisive role for Generative AI [14\u201316], Q-Refine is designed to\nrefine AIGIs with separated pipelines according to the quality.\nOur framework is shown in Fig. 2 with an IQA module to pre-\ndict a quality map and three pipelines include: (1) Gaussian\nDe-\nnoise\nAdd\nnoise\nDe-\nnoise\nQuality Score: 48\nQuality Score: 48\nQuality Score: 30\nQuality Score: 81\nQuality Score: 55\nFig. 3. The refining result by only denoise / add noise + de-\nnoise from SDXL [8]. Adding noise reduces quality [19], but\nit lays the foundation for global optimality before denoising.\nNoise: encouraging changing the LQ region by adding noise;\n(2) Mask Inpainting: generating a mask from the quality map\nto reserve HQ region; (3) Global Enhancement: setting an\nenhancement threshold to fine-tune the final output.\n3.2. IQA Module\nSplitting the image into patches [17], evaluating them sepa-\nrately [18], and then combining them is a commonly used [19]\nIQA pipeline in recent years. It can evaluate the overall qual-\nity while providing a rough quality map through patches. By\ndividing an AIGI into n \u00d7 n, a patch P with index (i, j) \u2208\n[0, n \u2212 1] has:\nP(i,j) = CNN(I( i\nn h: i+1\nn h, j\nn w: j+1\nn w))\n(1)\nwhere (h, w) are the height/width of the input image I. Since\nextracting the quality map requires a network sensitive for\nboth global regression and local perception, the dual-task\nstructure for image classification/detection, namely Faster-\nRCNN [20], is utilized as our CNN model backbone. For\nlocal quality Q(i,j), referring to previous quality map extrac-\ntor [19], we use the largest value in each patch as its qual-\nity score, to obtain a n \u00d7 n quality map Q. However, for\nglobal quality q, to avoid excessive complexity affecting the\nsubsequent three refining pipelines, we abandoned all global\nextractors and directly averaged the patch scores as:\n\u001a Q(i,j) = RoIPool(P(i,j))\nq = Avg(Q(i,j))\n(2)\nwhere Avg and RoIPool are the average and average-max-\npooling layers. The global quality/quality map will guide re-\nfining pipelines.\n3.3. Stage 1 Pipeline: Gaussian Noise\nExisting T2I generation models cannot always ensure a HQ\nresult, even the most advanced model [21] may occasionally\n(a) Patch quality map\n(b) Flattened quality map\nFig. 4. Using original patch quality map / flattened map to\nguide the inpainting. (a) suffers from block effects and unex-\npected artifacts while (b) has a smooth and natural result.\ngenerate blurry images. Such a problem may be due to the\ninitial few denoising steps, causing the image to fall into a lo-\ncal optimum. In such cases, the model will stubbornly retain\nsome LQ regions, causing the image to remain unchanged\neven after iterating hundreds of epochs. To solve this prob-\nlem, such LQ regions should rewind to previous steps, to trig-\nger the model\u2019s denoising mechanism. Since Sec. 3.2 pro-\nvides a quality map, the LQ region can be identified and then\nmodified. As the starting noise image before denoising, we\nsuperimpose Gaussian noise in the LQ region to obtain the\nfirst stage output Is1:\n\u001a\nW = max(BLQ \u2212 Q, 0)\nIs1 = WG(h,w) + (1 \u2212 W)I\n(3)\nwhere the noise weight map W is determined by LQ bound\nBLQ, a region with lower quality has higher weight while\nquality larger than BLQ leads to zero weight. The size of\nGaussian noise G is (h, w). As Fig. 3 shows, though the noise\nfrom the stage 1 pipeline may temporarily reduce the image\nquality, it can help the following two pipelines to change the\nLQ region. By refining the final output, it can move the local\nquality optimum toward the global optimum.\n3.4. Stage 2 Pipeline: Mask Inpainting\nSince different regions of images have different quality, this\npipeline aims to retain HQ and modify other regions. This\noperation can be completed through the inpainting method,\nby taking LQ regions as a mask. However, as the edges be-\ntween patches are un-discontinuous, directly using the quality\nmap with n\u00d7n patches to generate this mask will cause some\nunsatisfying results like Fig. 4 shows. First, a discontinuous\nquality map may require the inpainting model to retain a cer-\ntain patch and modify adjacent patches. The result will have\nobvious block effects at the edge of the patches. Second, the\ninpainting model tends to redraw the main object with a reg-\nular rectangle mask. Though we only want some detail on\nObjective Quality\nOriginal Quality Range\nOriginal Image\nBlind Enhancer\nPrompt Enhancer\n(+2.91, +2.05)\n(+1.40, +0.34)\n(-1.28, +0.08)\nFig. 5. Using blind enhancer or prompt-guided enhancer\nto refine images in different quality groups in AGIQA-3K [1].\nBlind enhancer shows better refining results for LQ groups\nbut causes negative optimization for HQ groups.\na plain background, it will generate unexpected main objects\nhere instead. Thus the patch quality map Q needs to be flat-\ntened before inpainting. Considering smoothness is our first\nrequirement, we use the smoothest interpolation method Bi-\nCubic [22], to convolve each pixel with 16 adjacent pixels:\nQ(x,y) =\n2\nX\nr,c=\u22121\nQ(\u230ax n\nh\u230b+r,\u230ay n\nw\u230b+c)Cub(r\u2212x,c\u2212y)\n(4)\nwhere pixel (\n\u0004\nx n\nh\n\u0005\n,\n\u0004\ny n\nw\n\u0005\n) from the original quality map is\nthe corresponding pixel (x, y) from the flattened map and\nCub stands for the Bi-Cubic [22] matrix. From this, the prob-\nability density function z of each step is:\nz = QKV(prompt, mask = {Q \u2212 BMQ})\n(5)\nwhere we set quality region below the threshold BMQ as\nmask. QKV stands for multi-head attention, which depends\non the input prompt and mask. Set the starting point of de-\nnoising to x0 = Is1, we have the second stage output Is2:\nIs2 = xm = Dm(xm\u22121) = Dm(Dm\u22121 \u00b7 \u00b7 \u00b7 D1(Is1))\n(6)\nwhere Dm represents the diffusion operation at the m-th it-\neration and x stands for this intermediate state. From this,\nwe used masks to modify the LQ/MQ region through the\nsmoothed quality map without affecting the HQ region.\n3.5. Stage 3 Pipeline: Global Enhancement\nAfter local inpainting, to further improve the image quality,\nthis pipeline fine-tunes the image\u2019s low-level attributes, rather\nthan adding new objects. Low-level quality enhancers include\nthe following two types. One is the traditional image super-\nresolution/restoration method, which ignores the prompt as a\nblind enhancer, using prior knowledge from NSIs to achieve\nimage-to-image reconstruction. The other is the generative\nmodel, which uses the original prompt to guide the diffu-\nsion for several steps, namely prompt-guided enhancer. The\nSOTAs of the two enhancers are DiffBIR [5] and SDXL [8],\nand the refining results are in Fig. 5. Considering the blind\nenhancer is suitable for LQ, but performs worse than the\nprompt-guided enhancer on HQ, we implement the enhancer\nbased on global quality, with the final output If:\nIf = {EB, EP \u2225q < BHQ}(Is2)\n(7)\nwhere EB stands for a blind enhancer while EP performs\na similar mechanism as (6), but in smaller hyper-parameter\nstrength (to avoid negative optimization for HQ) without a\nmask. The HQ bound BHQ determines such selection. Mean-\nwhile, considering some positive words [23] will significantly\nimprove the generation quality, we combine these words with\nthe original prompt as the input of EP . Therefore, regardless\nof whether the input belongs to LQ/MQ/HQ, our model can\nrefine its quality by providing an HQ result.\n4. EXPRIMENT\n4.1. Expriment Settings\nOur Q-Refine is validated on three AIGI quality databases, in-\ncluding AGIQA-3K, AGIQA-1K, and AIGCIQA [1,27,28].\nThe quality of AIGIs before/after Q-Refine is compared\nto prove the general optimization level.\nMoreover, since\nAGIQA-3K [1] includes five T2I models [7, 8, 24\u201326] with\nremarkable quality differences, their performances are listed\nrespectively to prove Q-Refine\u2019s versatility on LQ/MQ/HQ\nregions. Besides the original image, the image quality gener-\nated by Q-Refine is compared with three latest image restora-\ntion refiners [4\u20136] and two representative generative refin-\ners [7,8] as Sec. 2 reviewed.\nTo measure the image quality, since FID [29] is inconsis-\ntent with human subjective preferences, we use IQA meth-\nods to represent HVS\u2019s perceptual quality. The image quality\nconsists of two different levels. Signal-fidelity characterizes\nlow-level quality including factors like blur or noise, which is\nthe traditional definition of image quality. Thus, we use the\nclassic Brisque [30] as its index. Aesthetic, however, repre-\nsents high-level quality, which depends on the overall appeal\nand beauty of the image. Here we take the HyperIQA [31]\nas the index since it best correlates human subjective prefer-\nence on AIGIs. Moreover, for a more intuitive performance\ncomparison, we also take CLIPIQA [32] as an overall quality\nindicator for both levels.\n4.2. Expriment Result and Discussion\nThe experimental performance on the AGIQA-3K [1]\ndatabase and five subsets is shown in Table 1. In the gen-\neral perspective, Q-Refine achieved the best aesthetic, fidelity,\nand overall quality.\nOn a total of 18 indexes in six sets,\nQ-Refine reached SOTA on 16 of them. It is worth men-\ntioning that Q-Refine never negatively optimized any index\nTable 1. Refined result of AGIQA-3K [1] database and five subsets from different generators. The refined results with the best\nquality are noted in red. The refined quality below the original data is noted in underline.\nRefiner\nMean\nGLIDE [24]\nSDXL [8]\nOverall\u2191\nAesthetic\u2191\nFidelity\u2193\nOverall\u2191\nAesthetic\u2191\nFidelity\u2193\nOverall\u2191\nAesthetic\u2191\nFidelity\u2193\nOriginal\n0.5710\n0.4890\n38.975\n0.2901\n0.2895\n71.331\n0.7559\n0.6173\n24.816\nDASR [4]\n0.4987\n0.5507\n45.252\n0.2384\n0.3007\n63.922\n0.7298\n0.7011\n22.728\nDiffBIR [5]\n0.5829\n0.5935\n35.049\n0.4104\n0.3982\n60.728\n0.7400\n0.7273\n26.309\nRFDN [6]\n0.5704\n0.4885\n38.831\n0.2900\n0.2886\n71.178\n0.7532\n0.6164\n24.522\nSD1.5 [7]\n0.6461\n0.5359\n39.649\n0.5852\n0.4749\n67.669\n0.6917\n0.5632\n29.996\nSDXL [8]\n0.6489\n0.5418\n32.999\n0.5609\n0.4416\n52.711\n0.7111\n0.5842\n24.589\nQ-Refine\n0.7232\n0.6021\n22.463\n0.6333\n0.4986\n31.722\n0.8007\n0.6640\n18.145\nRefiner\nDALLE2 [25]\nMidJourney [26]\nSD1.5 [7]\nOverall\u2191\nAesthetic\u2191\nFidelity\u2193\nOverall\u2191\nAesthetic\u2191\nFidelity\u2193\nOverall\u2191\nAesthetic\u2191\nFidelity\u2193\nOriginal\n0.6193\n0.4884\n29.264\n0.5340\n0.4751\n42.6938\n0.6555\n0.5749\n26.7706\nDASR [4]\n0.5686\n0.5884\n38.917\n0.4521\n0.5562\n39.6575\n0.5045\n0.6069\n61.0347\nDiffBIR [5]\n0.5947\n0.6118\n27.658\n0.5543\n0.5706\n31.3149\n0.6153\n0.6598\n29.2356\nRFDN [6]\n0.6191\n0.4875\n29.120\n0.5337\n0.4755\n42.6950\n0.6561\n0.5745\n26.6402\nSD1.5 [7]\n0.6543\n0.5425\n33.885\n0.6295\n0.5305\n39.8227\n0.6696\n0.5686\n26.8717\nSDXL [8]\n0.6692\n0.5654\n29.789\n0.6307\n0.5359\n34.2414\n0.6726\n0.5819\n23.6660\nQ-Refine\n0.7350\n0.6133\n20.763\n0.7384\n0.6097\n19.4677\n0.7084\n0.6249\n22.2168\nTable 2. Three AIGI quality databases [1,27,28] before/after\nQ-Refine. The best result is noted in red.\nDatabases\nOverall\u2191\nAesethic\u2191\nFidelity\u2193\nAGIQA-3K [1]\n0.5710\n0.4890\n38.975\nAGIQA-3K + Q-Refine\n0.7232\n0.6021\n22.463\nAGIQA-1K [27]\n0.6454\n0.5896\n42.288\nAGIQA-1K + Q-Refine\n0.7258\n0.6511\n27.767\nAIGCIQA [28]\n0.5720\n0.5213\n31.443\nAIGCIQA + Q-Refine\n0.6639\n0.6196\n23.365\nthat other Refiners never achieved. From a detailed perspec-\ntive, Q-refine has a satisfying performance on all subsets as\nwe stated in our contributions. Firstly, for the worst quality\nGLIDE [24] model, the significant improvement of the three\nindexes proves that Q-Refine can effectively refine LQ. Sec-\nondly, for the strongest SDXL [8] model, each index after\nQ-Refine does not drop like other methods certified the ro-\nbustness on HQ. Thirdly, in the remaining three subsets with\naverage performance, the rise in all indexes indicated that Q-\nRefine can identify and modify the LQ/MQ region and retain\nthe HQ. Table 2 also proved in databases constructed by dif-\nferent T2I generation metrics with different performance, Q-\nRefine can provide an HQ refining result for all AIGIs.\n4.3. Ablation Study\nTo quantify the contributions of three pipelines of Q-Refine,\nwe abandon its stage (1)/(2)/(3) pipelines respectively in this\nsection. As a side-effect module, (1) does not appear alone.\nTable 3. The AGIQA-3K [1] refining result after abandoning\ndifferent Q-Refine pipelines. The best result is noted in red.\nPipelines\nOverall\u2191\nAesethic\u2191\nFidelity\u2193\n(1)+(2)+(3)\n0.7232\n0.6021\n22.463\n(1)+(2)\n0.6604\n0.5610\n32.079\n(2)+(3)\n0.6897\n0.5884\n24.373\n(1)+(3)\n0.6315\n0.5445\n29.917\n(2)\n0.6165\n0.5147\n34.299\n(3)\n0.6852\n0.5571\n29.332\nThe result in Table 3 indicates the positive effect of add-noise\non subsequent denoising, as the noise from (1) greatly im-\nproves the image quality refined by (2). Both (2) and (3) have\na positive effect on the refining task , which are responsible\nfor high-level and low-level optimization respectively. When\nthe two are combined, the image quality is further improved.\nThus, all pipelines contribute to the final result.\n5. CONCLUSION\nIn this study, targeting AIGI\u2019s unsatisfying quality, a quality-\naware refiner is proposed. To enhance positive while avoiding\nnegative optimization in the LQ/HQ region, IQA is innova-\ntively introduced into the image refiner to provide guidance.\nInspired by quality maps, three well-designed pipelines work\ncollaboratively to optimize the LQ/MQ/HQ regions. Exper-\nimental data shows that Q-Refine improves the quality of\nAIGIs at both fidelity and aesthetic levels, which enables a\nbetter viewing experience for humans in the AIGC era.\n6. REFERENCES\n[1] Chunyi Li, Zicheng Zhang, Haoning Wu, Wei Sun, Xiongkuo\nMin, Xiaohong Liu, Guangtao Zhai, and Weisi Lin, \u201cAgiqa-3k:\nAn open database for ai-generated image quality assessment,\u201d\nIEEE TCSVT, 2023.\n[2] Stanislav Frolov, Tobias Hinz, Federico Raue, J\u00a8orn Hees, and\nAndreas Dengel, \u201cAdversarial text-to-image synthesis: A re-\nview,\u201d Neural Networks, 2021.\n[3] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and\nIn So Kweon, \u201cText-to-image diffusion model in generative ai:\nA survey,\u201d arXiv:2303.07909, 2023.\n[4] Yunxuan Wei, Shuhang Gu, Yawei Li, Radu Timofte, Longcun\nJin, and Hengjie Song, \u201cUnsupervised real-world image super\nresolution via domain-distance aware training,\u201d in IEEE/CVF\nCVPR, 2021.\n[5] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei,\nBo Dai, Wanli Ouyang, Yu Qiao, and Chao Dong, \u201cDiffbir: To-\nwards blind image restoration with generative diffusion prior,\u201d\narXiv:2308.15070, 2023.\n[6] Jie Liu, Jie Tang, and Gangshan Wu, \u201cResidual feature dis-\ntillation network for lightweight image super-resolution,\u201d in\nECCV, 2020.\n[7] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick\nEsser, and Bj\u00a8orn Ommer, \u201cHigh-resolution image synthesis\nwith latent diffusion models,\u201d in IEEE CVPR, 2022.\n[8] Robin Rombach, Andreas Blattmann, and Bj\u00a8orn Ommer,\n\u201cText-guided synthesis of artistic images with retrieval-\naugmented diffusion models,\u201d arXiv:2207.13038, 2022.\n[9] Tengchuan Kou, Xiaohong Liu, Wei Sun, Jun Jia, Xiongkuo\nMin, Guangtao Zhai, and Ning Liu, \u201cStablevqa: A deep no-\nreference quality assessment model for video stability,\u201d\nin\nACM MM, 2023.\n[10] Yixuan Gao, Yuqin Cao, Tengchuan Kou, Wei Sun, Yunlong\nDong, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai,\n\u201cVdpve: Vqa dataset for perceptual video enhancement,\u201d in\nIEEE/CVF CVPR, 2023.\n[11] Chunyi Li, May Lim, Abdelhak Bentaleb, and Roger Zim-\nmermann, \u201cA real-time blind quality-of-experience assessment\nmetric for http adaptive streaming,\u201d in IEEE ICME, 2023.\n[12] Chunyi Li, Zicheng Zhang, Wei Sun, Xiongkuo Min, and\nGuangtao Zhai,\n\u201cA full-reference quality assessment metric\nfor cartoon images,\u201d in IEEE MMSP, 2022.\n[13] Xinhui Huang, Chunyi Li, Abdelhak Bentaleb, Roger Zimmer-\nmann, and Guangtao Zhai, \u201cXgc-vqa: A unified video quality\nassessment model for user, professionally, and occupationally-\ngenerated content,\u201d in IEEE ICMEW, 2023.\n[14] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,\nLiang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong\nYan, Guangtao Zhai, and Weisi Lin, \u201cQ-bench: A benchmark\nfor general-purpose foundation models on low-level vision,\u201d\narXiv:2309.14181, 2023.\n[15] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,\nLiang Liao, Annan Wang, Kaixin Xu, Chunyi Li, Jingwen\nHou, Guangtao Zhai, Geng Xue, Wenxiu Sun, Qiong Yan, and\nWeisi Lin, \u201cQ-instruct: Improving low-level visual abilities for\nmulti-modality foundation models,\u201d arXiv:2311.06783, 2023.\n[16] Zicheng Zhang, Haoning Wu, Zhongpeng Ji, Chunyi Li, Erli\nZhang, Wei Sun, Xiaohong Liu, Xiongkuo Min, Fengyu Sun,\nShangling Jui, et al.,\n\u201cQ-boost: On visual quality assess-\nment ability of low-level multi-modality foundation models,\u201d\narXiv:2312.15300, 2023.\n[17] Zicheng Zhang, Wei Sun, Yingjie Zhou, Haoning Wu, Chunyi\nLi, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai, and Weisi\nLin, \u201cAdvancing zero-shot digital human quality assessment\nthrough text-prompted evaluation,\u201d arXiv:2307.02808, 2023.\n[18] Zicheng Zhang, Wei Sun, Houning Wu, Yingjie Zhou, Chunyi\nLi, Xiongkuo Min, Guangtao Zhai, and Weisi Lin, \u201cGms-3dqa:\nProjection-based grid mini-patch sampling for 3d model qual-\nity assessment,\u201d arXiv:2306.05658, 2023.\n[19] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan,\nDeepti Ghadiyaram, and Alan Bovik, \u201cFrom patches to pic-\ntures (paq-2-piq): Mapping the perceptual space of picture\nquality,\u201d in IEEE/CVF CVPR, 2020.\n[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun,\n\u201cFaster r-cnn: Towards real-time object detection with region\nproposal networks,\u201d NIPS, 2015.\n[21] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, and Zhenguo Li, \u201cPixart-\u03b1: Fast training of dif-\nfusion transformer for photorealistic text-to-image synthesis,\u201d\narXiv:2310.00426, 2023.\n[22] Dianyuan Han, \u201cComparison of commonly used image inter-\npolation methods,\u201d in ICCSEE, 2013.\n[23] Nikita Pavlichenko and Dmitry Ustalov,\n\u201cBest prompts for\ntext-to-image models and how to find them,\u201d in ACM SIGIRl,\n2023.\n[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen, \u201cGlide: Towards photorealistic image generation\nand editing with text-guided diffusion models,\u201d in ICML, 2022.\n[25] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen, \u201cHierarchical text-conditional image genera-\ntion with clip latents,\u201d arXiv:2204.06125, 2022.\n[26] David Holz, \u201cMidjourney,\u201d https://www.midjourney.\ncom/, 2023.\n[27] Zicheng Zhang, Chunyi Li, Wei Sun, Xiaohong Liu, Xiongkuo\nMin, and Guangtao Zhai,\n\u201cA perceptual quality assessment\nexploration for aigc images,\u201d in IEEE ICMEW, 2023.\n[28] Jiarui Wang, Huiyu Duan, Jing Liu, Shi Chen, Xiongkuo Min,\nand Guangtao Zhai, \u201cAigciqa2023: A large-scale image quality\nassessment database for ai generated images: from the perspec-\ntives of quality, authenticity and correspondence,\u201d in CICAI,\n2023.\n[29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter, \u201cGans trained by a two\ntime-scale update rule converge to a local nash equilibrium,\u201d\nin NIPS, 2017.\n[30] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad\nBovik, \u201cNo-reference image quality assessment in the spatial\ndomain,\u201d IEEE TIP, 2012.\n[31] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jin-\nqiu Sun, and Yanning Zhang,\n\u201cBlindly assess image qual-\nity in the wild guided by a self-adaptive hyper network,\u201d in\nIEEE/CVF CVPR, 2020.\n[32] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy, \u201cEx-\nploring clip for assessing the look and feel of images,\u201d in AAAI,\n2023.\n"
  }
]