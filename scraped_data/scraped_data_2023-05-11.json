[
  {
    "title": "StarCoder: may the source be with you!",
    "link": "https://arxiv.org/pdf/2305.06161.pdf",
    "upvote": "24",
    "text": "Published in Transactions on Machine Learning Research (12/2023)\nStarCoder: may the source be with you!\nRaymond Li2\nLoubna Ben Allal1\nYangtian Zi4\nNiklas Muennighoff1\nDenis Kocetkov2\nChenghao Mou5\nMarc Marone8\nChristopher Akiki9,10\nJia Li5\nJenny Chim11\nQian Liu13\nEvgenii Zheltonozhskii14\nTerry Yue Zhuo15,16\nThomas Wang1\nOlivier Dehaene1\nMishig\nDavaadorj1\nJoel Lamy-Poirier2\nJo\u00e3o Monteiro2\nOleh Shliazhko2\nNicolas Gontier2\nNicholas Meade6,17\nArmel Zebaze1\nMing-Ho Yee4\nLogesh Kumar Umapathi18\nJian Zhu19\nBenjamin Lipkin20\nMuhtasham Oblokulov21\nZhiruo Wang7\nRudra Murthy22\nJason\nStillerman23\nSiva Sankalp Patel22\nDmitry Abulkhanov5\nMarco Zocca24\nManan Dey25\nZhihan Zhang26\nNour Fahmy27\nUrvashi Bhattacharyya28\nWenhao Yu26\nSwayam Singh30\nSasha Luccioni1\nPaulo Villegas31\nMaxim Kunakov32\nFedor Zhdanov32\nManuel Romero5\nTony Lee33\nNadav Timor34\nJennifer Ding35\nClaire Schlesinger4\nHailey Schoelkopf 37\nJan Ebert38\nTri Dao33\nMayank Mishra22\nAlex Gu20\nJennifer\nRobinson3\nCarolyn Jane Anderson36\nBrendan Dolan-Gavitt29\nDanish Contractor5\nSiva\nReddy2,6\nDaniel Fried7\nDzmitry Bahdanau2\nYacine Jernite1\nCarlos Mu\u00f1oz Ferrandis1\nSean Hughes3\nThomas Wolf 1\nArjun Guha4,12\nLeandro von Werra1,\u22c6\nHarm de Vries2,\u22c6\n1Hugging Face\n2ServiceNow Research\n3ServiceNow\n4Northeastern University\n5Independent\n6Mila\n7Carnegie Mellon University\n8Johns Hopkins University\n9Leipzig University\n10ScaDS.AI\n11Queen Mary\nUniversity of London\n12Roblox\n13Sea AI Lab\n14Technion \u2013 Israel Institute of Technology\n15Monash\nUniversity\n16CSIRO\u2019s Data61\n17McGill University\n18Saama AI Research Lab\n19University of British\nColumbia\n20MIT\n21Technical University of Munich\n22IBM Research\n23University of Vermont\n24UnfoldML\n25SAP\n26University of Notre Dame\n27Columbia University\n28Discover Dollar Pvt Ltd\n29NYU\n30University of Allahabad\n31Telefonica I+D\n32Toloka\n33Stanford University\n34Weizmann\nInstitute of Science\n35The Alan Turing Institute\n36Wellesley College\n37Eleuther AI\n38Forschungszentrum J\u00fclich\nCorresponding authors (\u22c6) can be contacted at contact@bigcode-project.org\nReviewed on OpenReview: https://openreview.net/forum?id=KoFOg41haE\nAbstract\nThe BigCode community, an open-scientific collaboration working on the responsible de-\nvelopment of Large Language Models for Code (Code LLMs), introduces StarCoder and\nStarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast\nlarge-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion\ntokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively\nlicensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned\nStarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform\nthe most comprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages and\nmatches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder\noutperforms every model that is fine-tuned on Python and still retains its performance on\nother programming languages. We take several important steps towards a safe open-access\nmodel release, including an improved PII redaction pipeline and a novel attribution tracing\ntool, and make the StarCoder models publicly available under a more commercially viable\nversion of the Open Responsible AI Model license.\n1\narXiv:2305.06161v2  [cs.CL]  13 Dec 2023\nPublished in Transactions on Machine Learning Research (12/2023)\n1\nIntroduction\nGenerative AI and large language models (LLMs; Brown et al., 2020; Chen et al., 2021; Chowdhery et al.,\n2022; Zhang et al., 2022; OpenAI, 2023a) are predicted to significantly impact the workforce in the coming\nyears (Eloundou et al., 2023; Bommasani et al., 2021; World Economic Forum, 2023) by boosting worker\nproductivity. LLMs trained on code (Code LLMs) have seen particularly fast adoption: Microsoft\u2019s Copilot\nhas attracted over 1 million professional developers (Euronews, 2023) and GitHub reports that Copilot\nusers rely on it to produce 35% of the code they write for some languages (Thompson, 2022). However, the\ndevelopment and use of LLMs has raised concerns of copyright, privacy, and openness.\nCopyright concerns arise in many jurisdictions, including the U.S. and E.U. , regarding the rights of content\ncreators whose public data is used to train language models. It has been questioned whether machine learning\nmodels trained on such data fall under fair-use doctrine in the U.S. (Kuhn, 2022; Butterick, 2022; Rothchild\n& Rothchild, 2022), with fair use being most likely when the model generates novel content dissimilar\nto any copyrighted training data (Lemley & Casey, 2020; Levendowski, 2018). Henderson et al. (2023),\ntherefore, suggest LLM developers should provide additional tools to ensure these models comply with current\ncopyright laws. It is important to mention that these legal issues are not only the subject of scholarly debates:\nlawsuits have already been filed against GitHub Copilot (DOE 1 v. and GitHub, Inc., 2022) as well as Stable\nDiffusion (Andersen et al v. Stability AI et al, 2023).\nConcerns about personal information led Italy to temporarily ban ChatGPT and launch an ongoing investi-\ngation into OpenAI\u2019s compliance with the E.U.\u2019s General Data Protection Regulation (GDPR) (BBC, 2023).\nAccording to these regulations (European Council, 2018; Lomas, 2022), organizations that process personal\ninformation must have a valid legal basis. These laws could potentially affect LLM developers who gather\nvast amounts of public data from the internet, which may include personal information. Obtaining explicit\nconsent from data creators is difficult at this scale, and it is uncertain whether other legal grounds exist for\nprocessing this personal information. Moreover, even with a valid legal basis, GDPR mandates that data\nprocessors inform individuals as to how their data is being processed and provide data access controls, such\nas the right to have data deleted or to modify erroneous data. This would require LLM providers to be\ntransparent about the data they have collected and provide tooling for individuals to inspect their data and\nhave the possibility to delete it.\nThe lack of transparency and openness surrounding the development processes of generative AI models\nhas also raised concerns in the scientific community. Many models are closed-access to varying degrees:\nfrom being available only within the organization that developed them (Chowdhery et al., 2022; Hoffmann\net al., 2022) to being accessible publicly through a paid API but with many details on their development\nprocess hidden (Brown et al., 2020; OpenAI, 2023a). While API access allows researchers to experiment with\nthese models, it limits their ability to research LLM safety (Perez et al., 2022), inspect the models\u2019 inner\nworkings (Olsson et al., 2022), and contribute to model improvements (Togelius & Yannakakis, 2023).\nWe use \u201copen-access\u201d to refer to models whose weights are public. Although other open-access models\nexist, the level of openness still varies across these projects; and some models with released weights have\nrestrictions on model distribution (Touvron et al., 2023), or do not release their training datasets (Nijkamp\net al., 2023; Zhang et al., 2022; Fried et al., 2022). Even in cases when models and training data are both\nreleased permissively (Raffel et al., 2020; Tay et al., 2022), external researchers typically do not have an\nopportunity to participate in guiding the development of industry-produced models. In contrast, other\nLLM development projects have taken a fully open approach which aims to allow for community inputs\ninto model development, release training data, and enable external audits throughout the full development\nprocess (Solaiman, 2023). One example is the BigScience research workshop (BigScience Workshop, 2022),\nan open scientific collaboration (Akiki et al., 2022) comprising hundreds of researchers collaborating to\nrelease BLOOM, a multi-lingual LLM (Scao et al., 2022; Muennighoff et al., 2022). Similarly, EleutherAI, a\ngrassroots-turned-nonprofit research initiative, has released open-access LLMs including GPT-NeoX (Black\net al., 2022), GPT-J (Wang & Komatsuzaki, 2021), and Pythia (Biderman et al., 2023), as well as the\nassociated training data (Gao et al., 2021a).\nIn this paper, we describe StarCoder and StarCoderBase, open-access code LLMs developed and released by\nthe BigCode community, with a focus on respecting copyright, privacy, transparency, and community-driven\n2\nPublished in Transactions on Machine Learning Research (12/2023)\nmodel development. The project is an open-scientific collaboration focusing on the responsible development\nof LLMs for code. It is co-stewarded by two industry research labs and comprises more than 600 members\nfrom diverse academic institutes and industry labs. The Stack (Kocetkov et al., 2022) is a publicly available\npre-training dataset for Code LLMs with a transparent data governance framework. The Stack consists of\n6.4 TB of permissively licensed source code in 384 programming languages, and includes 54 GB of GitHub\nissues and repository-level metadata in the v1.2 version of the dataset. The dataset comes with \u201cAm I in The\nStack\u201d, a governance tool for developers to check whether their source code is part of the dataset, and an\nopt-out process for those who wish to have their code removed from the dataset.\nStarCoder and StarCoderBase are both 15.5B parameter models trained on permissively licensed data from\nThe Stack. We trained StarCoderBase on 1 trillion tokens sourced from 80+ programming languages, GitHub\nissues, Git commits, and Jupyter notebooks. We fine-tuned StarCoderBase on another 35B Python tokens,\nleading to the StarCoder model. Both StarCoder models come with a novel combination of architectural\nfeatures, such as an 8K token context length (Dao et al., 2022), infilling capabilities through Fill-in-the-\nMiddle (FIM; Bavarian et al., 2022), and fast large-batch inference through Multi-Query-Attention (MQA;\nShazeer, 2019). We present an extensive evaluation of the StarCoder models and release a demo along with\nan integrated attribution tool that can help users locate model generations that may have been copied from\nthe training set. Overall, our contributions can be summarized as follows.\n\u2022 We release StarCoderBase and StarCoder, open-access Code LLMs trained on 80+ programming\nlanguages that support a novel combination of capabilities and architectural features unavailable in\nother open Code LLMs.\n\u2022 We perform the most comprehensive evaluation of Code LLMs to date using a diverse set of\nbenchmarks (Lai et al., 2022; Cassano et al., 2023; Pearce et al., 2022; Fried et al., 2022; Yee & Guha,\n2023; Austin et al., 2021; Chen et al., 2021; Ben Allal et al., 2022; Hendrycks et al., 2020; Reddy\net al., 2019; Cobbe et al., 2021; Nadeem et al., 2021; Gehman et al., 2020; Liang et al., 2022), and\nshow that:\n\u2013 StarCoder outperforms every open LLM for code that supports multiple programming lan-\nguages (Nijkamp et al., 2023; Zheng et al., 2023);\n\u2013 StarCoder matches or outperforms the OpenAI code-cushman-001 model; and\n\u2013 When fine-tuned on Python, StarCoder substantially outperforms existing LLMs that are also\nfine-tuned on Python.\n\u2022 We take important steps towards a safe open model release:\n\u2013 We release StarCoder under an OpenRAIL-M license agreement, which enables royalty-free access,\nuse, and distribution of the model while embedding a set of use restrictions in identified critical\nscenarios. We have worked on a version of the license agreement that: (i) is more commercially\nviable for companies wishing to use and distribute the model and (ii) promotes transparency and\nunderstanding through the sharing of AI documentation such as model cards (Mitchell et al.,\n2019);\n\u2013 We incorporate a new attribution tool into the VSCode demo that can help users detect and locate\nmodel generations that may have been copied from the training set. This is achieved through a\ntwo-step process that involves a lightweight membership check followed by a search over a BM25\nindex (Section 9); and\n\u2013 We have significantly improved the PII redaction pipeline by collecting a PII dataset containing\n12,000 files with 22,950 annotated entities. We fine-tuned our own encoder model (StarEncoder)\non this dataset, resulting in a robust PII detection model (Section 4).\n2\nRelated Work\nLanguage models\nEarly efforts to build large-scale language models used n-grams and simple smoothing\ntechniques (Brants et al., 2007; Heafield et al., 2013; Buck et al., 2014). Other approaches applied various\n3\nPublished in Transactions on Machine Learning Research (12/2023)\ntypes of neural networks architectures, such as feedforward networks (Bengio et al., 2000) and recurrent\nnetworks (Mikolov et al., 2010; Jozefowicz et al., 2016), to the language modeling task. The Transformer\narchitecture (Vaswani et al., 2017) led to the development of highly scalable language models (Radford et al.,\n2019; Brown et al., 2020), which have shown a predictable relationship between language modeling loss and\nscaling factors such as the model size, number of training tokens, and compute budget (Kaplan et al., 2020;\nHoffmann et al., 2022).\nLanguage Models for Code\nLanguage models were initially applied to code by Hindle et al. (2012), but\nrelied on n-gram models trained at comparatively small scale. Many neural architectures developed in NLP\nwere also applied successfully to code, including encoder-only models for producing code representations (Feng\net al., 2020; Kanade et al., 2020) and encoder-decoder models for translation, editing, summarization, and\nlanguage-to-code tasks (Wang et al., 2021; Ahmad et al., 2021; Li et al., 2022). Decoder-only Transformer\narchitectures have produced strong generative models of code, typically by training on mixtures of text\nand code from GitHub (Chen et al., 2021; Austin et al., 2021; Fried et al., 2022; Zheng et al., 2023;\nNijkamp et al., 2023). Most of these models have not been fully open, but PolyCoder (Xu et al., 2022) and\nSantaCoder (Ben Allal et al., 2023) are notable exceptions and have both open models and training data.\nHowever, these models are relatively small (2.7B and 1.1B parameters, respectively) and are trained on less\ndata (< 300GB of code) than we explore in this work.\nClosed-access LLMs\nSeveral large tech companies have developed top-performing LLMs without releasing\nthem. Examples include Google\u2019s PaLM (Chowdhery et al., 2022) and LaMDA (Thoppilan et al., 2022),\nDeepMind\u2019s Chinchilla (Hoffmann et al., 2022) and Gopher (Rae et al., 2021), and NVIDIA\u2019s Megatron-Turing\nNLG (Smith et al., 2022). OpenAI and other AI startups, including Cohere1, Anthropic2, and Aleph Alpha3,\noffer LLMs as a paid API service. These companies did not release model weights nor provide comprehensive\ninformation on the methodology used to create these models. OpenAI has published several technical\nreports of the GPT family of models (Brown et al., 2020; Chen et al., 2021; OpenAI, 2023a), showcasing the\ncapabilities of their models.\nOpen-access LLMs\nNumerous open-access LLMs have been released to the AI community, although\nthey are generally not as strong as closed-access ones. In this paper, we use the term \u201copen-access LLM\u201d\nwhen the model weights are publicly available. We still note that there are significant differences between\nopen-access models in how transparent they have been about the training data and filtering techniques. For\ninstance, EleutherAI released GPT-NeoX-20B (Black et al., 2022) and GPT-J-6B (Wang & Komatsuzaki,\n2021), as well as the dataset these models were trained on (Gao et al., 2021a). Google released UL2-20B (Tay\net al., 2022), an encoder-decoder model trained on the publicly available C4 (Raffel et al., 2020). Tsinghua\nUniversity released the weights of GLM-130B (Zeng et al., 2022), a Chinese-English LLM, and CodeGeeX-\n13B (Zheng et al., 2023), a LLM for coding applications, without releasing the training sets. Salesforce\nreleased CodeGen-Mono-16B (Nijkamp et al., 2023) without disclosing a proprietary Python dataset. Meta\nreleased the OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and InCoder models (Fried et al.,\n2022) under a non-commercial license and only provided high-level details about the data collection and\nfiltering process.\n3\nData Curation and Cleaning\nThis section describes how we processed the training data of StarCoderBase. We restrict the training set to\nThe Stack v1.2 (Kocetkov et al., 2022), which exclusively contains data from permissively licensed4 GitHub\nrepositories. At the time of the data processing, 44 people opted out of The Stack. Below, we describe how\nwe further cleaned the data by combining heuristic filtering and manual inspection.\n1https://cohere.com/\n2https://www.anthropic.com/\n3https://www.aleph-alpha.com/\n4See https://blueoakcouncil.org/ to learn more about permissive licenses and access a comprehensive collection of such\nlicenses.\n4\nPublished in Transactions on Machine Learning Research (12/2023)\n3.1\nProgramming Languages\nSelection of programming languages\nFrom the 358 programming languages in The Stack, we selected\n86 languages. The assignment of data to programming languages was performed based solely on file extension\n(Kocetkov et al., 2022). We included all programming languages with more than 500 MB of data, as well as\nlanguages that were ranked in the top 50 on Githut 2.0 or the December 2022 TIOBE Index of programming\nlanguage popularity. In addition, we included dialects of already selected programming languages (e.g.,\nRacket and Scheme for Lisp). We excluded configuration languages (Nix, Puppet, etc.) and languages that\nare no longer actively supported (ActionScript). We also included data formats like JSON and YAML but\nlimited its data volume (see \u201cJSON and YAML\u201d paragraph for details). The full list of selected programming\nlanguages can be found in Tables 1 and 2. Out of the languages present in MultiPL-E (Cassano et al., 2023),\nonly D and Swift were not included in the training set. For D, language misclassification of the files led to less\nthan 2MB of data in The Stack (Kocetkov et al., 2022). Swift was excluded from the final list of languages\ndue to human error.\nVisual inspection\nWe performed a visual inspection to ensure that we only retain data of high quality. To\nachieve this, we randomly selected 30,000 files from The Stack for each programming language, categorized\nthem by extension, and kept a maximum of 1,000 files for each extension. We then reached out to our\ncommunity for assistance with data inspection. We instructed the annotators to go through 50\u2013100 files\nand confirm if the data appeared to be normal code written by humans, as opposed to text, data, or a\nsingle long line of autogenerated code. We also asked annotators to determine whether we should use our\ndefault alpha-numeric filter (which requires over 25% alpha-numeric symbols) and long-line filter (which\nrequires lines to be less than 1,000 characters) for a given file extension. Eighteen community annotators\nevaluated 300 programming language extensions. After inspection, we excluded 36 extensions and eliminated\nthe long-line filter for 27 extensions. The complete outcomes of the data inspection, including annotator\nremarks, can be found in this Google sheet.\nXML filter\nAs we inspected the data, we noticed that certain extensions often consisted of XML files. For\nexample, the .sld extension had more than 50% of its files in XML format. To address this, we implemented\na simple XML filter that checked for the presence of \u201c<?xml version=\u201d within the first 100 characters of\nthe file. This filter proved to be effective and produced few false positives. Hence, we applied it to all\nprogramming languages except for XSLT, which uses XML syntax.\nAlpha filter\nDuring our investigation, we discovered that certain extensions, such as MATLAB, contained\nnumerous data files that frequently stored large tensors. To identify these files, we developed an alpha filter\nthat removed files with fewer than 25% alphabetic characters. However, when we tested this filter on a small\nsubset of data, we observed a high rate of false positives for certain programming languages, such as Assembly.\nTo address this issue, we focused on the 25 extensions with the highest number of detections and manually\nverified whether or not the alpha filter should be applied.\nHTML\nWe designed a custom HTML filter that targets excessive HTML boilerplate and links. We took\ninto account the ratio of visible text in each file and only kept those files where the visible text makes up at\nleast 20% of the HTML code and has a minimum length of 100 characters.\nJSON and YAML\nJSON and YAML files are naturally more data-heavy than other languages in The\nStack. To remove most of the data files, we applied the following filters. For YAML, we kept files with\n50\u20135000 characters, an average line length smaller than 100, a maximum line length smaller than 1000, and\nmore than 50% alphabetic characters. These filters remove around 20% of the files and 90% of the volume.\nFor JSON, we kept files with 50\u20135000 characters and more than 50% alphabetic characters, which removes\naround 70% of the files and 98% of the volume.\n5\nPublished in Transactions on Machine Learning Research (12/2023)\nLanguage\nAfter dedup\nAfter filters and decont.\nWeight\nPercentage\nNum. files\nVolume (GB)\nNum. files\nVolume (GB)\nada\n31,291\n0.30\n30,934\n0.26\n0.26\n0.034\nagda\n17,608\n0.07\n17,554\n0.07\n0.07\n0.009\nalloy\n5,374\n0.01\n5,368\n0.01\n0.01\n0.001\nantlr\n7,983\n0.05\n7,917\n0.05\n0.05\n0.007\napplescript\n4,906\n0.01\n4,737\n0.01\n0.01\n0.001\nassembly\n248,396\n1.58\n247,919\n1.56\n1.56\n0.203\naugeas\n195\n0.00\n180\n0.00\n0.00\n0\nawk\n10,430\n0.02\n10,289\n0.02\n0.02\n0.003\nbatchfile\n252,514\n0.29\n239,568\n0.23\n0.23\n0.03\nbluespec\n5,940\n0.03\n5,928\n0.03\n0.03\n0.004\nc\n8,625,559\n57.43\n8,536,791\n53.89\n53.89\n7.027\nc-sharp\n10,839,399\n46.29\n10,801,285\n44.66\n44.66\n5.823\nclojure\n126,191\n0.49\n125,163\n0.46\n0.46\n0.06\ncmake\n186,517\n0.45\n186,375\n0.45\n0.45\n0.059\ncoffeescript\n227,889\n0.69\n226,209\n0.64\n0.64\n0.083\ncommon-lisp\n101,370\n1.68\n98,733\n1.40\n1.40\n0.183\ncpp\n6,377,914\n50.89\n6,353,527\n48.92\n48.92\n6.379\ncss\n2,994,829\n22.61\n2,721,616\n11.93\n3.00\n0.391\ncuda\n58,355\n0.59\n58,151\n0.56\n0.56\n0.073\ndart\n932,583\n3.86\n928,415\n3.66\n3.66\n0.477\ndockerfile\n572,186\n0.42\n571,506\n0.42\n0.42\n0.055\nelixir\n282,110\n0.74\n281,016\n0.71\n0.71\n0.093\nelm\n62,861\n0.34\n62,033\n0.30\n0.30\n0.039\nemacs-lisp\n54,768\n0.43\n52,838\n0.41\n0.41\n0.053\nerlang\n99,368\n0.73\n98,447\n0.70\n0.70\n0.091\nf-sharp\n127,161\n0.90\n124,066\n0.61\n0.61\n0.08\nfortran\n165,446\n1.84\n158,792\n1.78\n1.78\n0.232\nglsl\n175,576\n0.57\n167,701\n0.40\n0.40\n0.052\ngo\n4,730,461\n25.74\n4,700,526\n23.78\n23.78\n3.101\ngroovy\n251,627\n0.94\n250,834\n0.91\n0.91\n0.119\nhaskell\n544,969\n2.36\n541,454\n2.23\n2.23\n0.291\nhtml\n9,533,367\n146.76\n3,299,965\n29.36\n29.36\n3.828\nidris\n8,060\n0.03\n8,042\n0.03\n0.03\n0.004\nisabelle\n5,086\n0.09\n5,001\n0.08\n0.08\n0.01\njava\n20,151,565\n89.30\n20,071,773\n86.94\n86.94\n11.336\njava-server-pages\n214,133\n1.03\n210,816\n0.98\n0.98\n0.128\njavascript\n21,108,587\n141.65\n19,544,285\n64.71\n64.71\n8.437\njson\n17,012,912\n338.34\n4,751,547\n5.62\n1.00\n0.13\njulia\n298,672\n1.54\n295,364\n1.31\n1.31\n0.171\nkotlin\n2,242,771\n5.77\n2,239,354\n5.68\n5.68\n0.741\nlean\n16,891\n0.10\n16,870\n0.09\n0.09\n0.012\nliterate-agda\n523\n0.01\n523\n0.01\n0.01\n0.001\nliterate-coffeescript\n1,138\n0.01\n1,133\n0.01\n0.01\n0.001\nliterate-haskell\n6,135\n0.05\n6,104\n0.05\n0.05\n0.007\nlua\n558,861\n3.28\n549,459\n2.87\n2.87\n0.374\nmakefile\n661,424\n1.49\n657,349\n1.31\n1.31\n0.171\nmaple\n1,259\n0.01\n1,152\n0.01\n0.01\n0.001\nmarkdown\n21,045,171\n75.25\n21,029,287\n74.93\n74.93\n9.77\nmathematica\n26,895\n1.72\n22,653\n1.25\n1.25\n0.163\nmatlab\n967\n0.04\n93\n0.00\n0.00\n0\nTable 1: Overview of the training data for StarCoder. For the selected programming languages, we show the\nnumber of files and data volume after near-deduplication, as well as after filtering. See also Table 2.\n6\nPublished in Transactions on Machine Learning Research (12/2023)\nLanguage\nAfter dedup\nAfter filters and decont.\nWeight\nPercentage\nNum. files\nVolume (GB)\nNum. files\nVolume (GB)\nocaml\n159,734\n1.11\n158,356\n1.03\n1.03\n0.134\npascal\n118,675\n1.71\n110,981\n1.68\n1.68\n0.219\nperl\n392,108\n2.63\n365,491\n2.23\n2.23\n0.291\nphp\n15,904,518\n66.84\n15,683,017\n60.89\n60.89\n7.939\npowershell\n271,487\n1.25\n267,627\n1.12\n1.12\n0.146\nprolog\n1,023\n0.01\n968\n0.01\n0.01\n0.001\nprotocol-buffer\n98,246\n0.44\n97,167\n0.31\n0.31\n0.04\npython\n12,962,249\n64.30\n12,866,649\n60.40\n60.40\n7.875\nr\n39,194\n0.30\n39,042\n0.30\n0.30\n0.039\nracket\n4,201\n0.04\n3,688\n0.03\n0.03\n0.004\nrestructuredtext\n905,679\n3.42\n896,880\n3.32\n3.32\n0.433\nrmarkdown\n5,389\n0.06\n5,386\n0.06\n0.06\n0.008\nruby\n3,405,374\n7.14\n3,390,320\n6.81\n6.81\n0.888\nrust\n1,386,585\n9.53\n1,380,468\n9.11\n9.11\n1.188\nsas\n9,772\n0.13\n9,226\n0.12\n0.12\n0.016\nscala\n1,362,426\n4.86\n1,355,788\n4.69\n4.69\n0.612\nscheme\n44,261\n0.30\n41,890\n0.20\n0.20\n0.026\nshell\n2,236,434\n3.38\n2,206,327\n3.09\n3.09\n0.403\nsmalltalk\n592,999\n0.74\n587,748\n0.58\n0.58\n0.076\nsolidity\n164,242\n1.21\n153,194\n0.85\n0.85\n0.111\nsparql\n14,173\n0.04\n13,716\n0.04\n0.04\n0.005\nsql\n994,019\n12.22\n975,420\n11.09\n11.09\n1.446\nstan\n5,441\n0.01\n5,429\n0.01\n0.01\n0.001\nstandard-ml\n48,995\n0.52\n19,630\n0.19\n0.19\n0.025\nstata\n31,282\n0.41\n24,208\n0.33\n0.33\n0.043\nsystemverilog\n46,915\n0.41\n46,270\n0.39\n0.39\n0.051\ntcl\n50,579\n0.40\n49,335\n0.35\n0.35\n0.046\ntcsh\n4,911\n0.02\n4,806\n0.02\n0.02\n0.003\ntex\n547,888\n5.44\n522,778\n5.20\n5.20\n0.678\nthrift\n4,663\n0.01\n4,661\n0.01\n0.01\n0.001\ntypescript\n10,637,070\n28.82\n10,547,331\n26.52\n26.52\n3.458\nverilog\n77\n0.001\n75\n0.001\n0.001\n0\nvhdl\n60,027\n1.12\n58,208\n0.94\n0.94\n0.123\nvisual-basic\n163,291\n1.49\n161,239\n1.42\n1.42\n0.185\nxslt\n43,095\n0.56\n6,513\n0.05\n0.05\n0.007\nyacc\n25,775\n0.41\n7,451\n0.11\n0.11\n0.014\nyaml\n5,282,081\n28.36\n3,995,948\n3.76\n1.00\n0.13\nzig\n15,913\n0.18\n15,850\n0.18\n0.18\n0.023\nGitHub issues\n\u223c 30,900,000\n54.40\n54.40\n7.093\nGit commits\n7,674,345\n64.00\n32.00\n4.172\nnotebook scripts\n914,000\n7.12\n7.12\n0.928\nnotebook structured\n668,743\n6.00\n6.00\n0.782\n305,929,658\n815.68\n799.37\n100\nTable 2: Overview of the training data for StarCoder. For the selected programming languages, we show the\nnumber of files and data volume after near-deduplication, as well as after filtering. See also Table 1.\n3.2\nJupyter notebooks\nAll Jupyter notebooks were retrieved from the Stack. We transformed Jupyter notebooks into two different\ndatasets: Jupyter \u2013 scripts and Jupyter \u2013 structured.\n7\nPublished in Transactions on Machine Learning Research (12/2023)\nLanguage\nNum files\nPercentage\npython\n1,392,432\n97.170\njulia\n16,730\n1.167\nr\n11,034\n0.77\nscala\n1,899\n0.133\nbash\n1,441\n0.101\njava\n1,319\n0.092\nq-sharp\n1,273\n0.089\ncpp\n1,081\n0.075\nc-sharp\n1,048\n0.073\nmatlab\n908\n0.063\npowershell\n769\n0.054\njavascript\n592\n0.041\nhaskell\n535\n0.037\nscheme\n484\n0.034\ngroovy\n432\n0.03\nf-sharp\n385\n0.027\nocaml\n279\n0.019\nrust\n134\n0.009\nclojure\n96\n0.007\ntypescript\n72\n0.005\nmaxima\n31\n0.002\ncoconut\n6\n0\nmarkdown\n5\n0\nwolfram language\n4\n0\ntcl\n3\n0\nTotal\n1,432,992\n100\nTable 3: Overview of the initially collected Jupyter scripts, with the number of files and the percentage.\nJupyter \u2013 scripts\nWe utilize Jupytext5 to convert notebooks to scripts. It is an actively maintained\nsoftware that currently supports 31 programming languages. To initiate the conversion process, Jupytext\nrequires the identification of the specific programming languages within each notebook. We extracted this\ninformation from the metadata of each respective notebook. However, more than 30,000 notebooks lacked\nany programming language information, making it difficult to convert them to the script format. To address\nthis issue, we incorporated the use of Guesslang,6 an open-source library that employs machine learning\ntechniques to identify the programming languages of source code. By applying a probability threshold\ngreater than or equal to 0.5, we successfully reduced the number of unidentified notebooks to 6,400 using\nGuesslang. Ultimately, we amassed 1,432,992 scripts through the utilization of Jupytext. The distribution of\nprogramming languages among these scripts is presented in Table 3. We evaluated language coverage by\nrandomly selecting 100 files from the transformed scripts, ensuring that all programming languages were\nrepresented within this sample.\nJupyter \u2013 structured\nTo create this dataset, we first filtered out notebooks that did not contain any\nPython code or Markdown text. The information on the programming language in the metadata of each\nnotebook was used as the criterion to filter out non-Python notebooks. Only notebooks explicitly marked as\n\u2018Python\u2019 in the metadata were kept. Then for each notebook, consecutive Markdown blocks or code blocks\nwere merged into a large Markdown or code block respectively. Eventually, we ended up with consecutive\ncode-text pairs in temporal order grouped by each notebook. In general, each Jupyter code-text pair contained\nthe Markdown text immediately preceding the code block and the Python code, which forms a natural\n5https://jupytext.readthedocs.io/\n6https://guesslang.readthedocs.io/\n8\nPublished in Transactions on Machine Learning Research (12/2023)\ninstruction pair. We also included the formatted output of a code block if the output cell was non-empty;\notherwise, it was marked by a special <empty_output> token. If consecutive code blocks have multiple output\ncells before merging, we only retain the output of the last code block. After these preprocessing steps, we\nended up with 1,045,605 structured Jupyter notebooks.\n3.3\nGitHub issues\nWe used natural language conversations from GitHub issues and pull requests, which were collected as a\ncomponent of The Stack v1.2. Each conversation consists of a series of events with actions, such as opening\nthe issue, creating a comment, or closing the issue. Each event includes the author\u2019s username, a message, an\naction, and a creation date. We filtered this data as follows: 1) First, we removed auto-generated text when\nusers replied to issues via email. See Appendix A for the regular expression we used. We also deleted issues\nwith a short message (less than 200 characters) and truncated long comments in the middle to a maximum of\n100 lines while retaining the last 20 lines. This removed 18% of the volume. 2) Next, we excluded comments\nfrom bots. To do so, we searched for bot keywords in the username of the comment\u2019s author (for more\ninformation, see Appendix A). This step eliminates 17% of the total events and results in 14.7% of the issues\nbeing emptied. We have observed that bot-generated issues tend to be lengthy and contain numerous logs and\nlinks. 3) We used the number of users engaged in the conversation as an indicator of quality. Our criterion\nwas to include conversations that have two or more users. However, we also preserved conversations that\ninvolved a single user if the total text within comments was less than 7,000 characters (96th percentile).\nAdditionally, we excluded issues authored by a single user if they contained more than ten events, as they\ntended to be of poor quality or originate from overlooked bots. By implementing these filters, we removed an\nadditional 14% of issues. 4) Finally, we used a model from the fasttext library7 to filter out non-English issues.\nThis step was necessary to enable accurate redaction of names using a PII detection model (see Section 4.3).\nLastly, we would like to point out that we anonymized the usernames in the conversations by replacing them\nwith a participant counter within the conversation. See more details in Section 4.3 and 5.1.\n3.4\nGit commits\nThe Git commit data was gathered from BigQuery8 and includes only single-file commits of repositories with\nthe same licenses and file extension as used in The Stack (Kocetkov et al., 2022). We removed all repositories\nfrom users that opted out of The Stack. The raw dataset is around 4 TB in size. We sampled 50% of the files\nand filtered the remaining data with heuristics to build a high-quality dataset. We list and describe all filters\nin Table 4.\nThe number of line changes in a commit can be very low compared to the file size. To avoid spending too\nmuch compute budget on learning to copy the file content, we only used the full file 20% of the time, and for\nthe remaining 80%, sampled a window between 0 and 32 lines around the first and last changed line. The\nresulting dataset contains 64 GB of commit data.\n3.5\nDeduplication\nWe followed the deduplication pipeline from Ben Allal et al. (2023), which consists of calculating the\nMinHashes (Broder, 2000) of all source code files, followed by Locally Sensitive Hashing (LSH) to map similar\ncode files to the same bucket. We used 5-grams and a Jaccard similarity of 0.7. See this blogpost for more\ndetails regarding the pipeline.\nWe applied this near-deduplication process to all programming languages and the Jupyter notebooks. However,\ndue to time constraints, we could not apply this procedure to Git commits. Additionally, we deemed it\nunlikely to discover duplicates in Github issues, so we didn\u2019t apply the process to them.\n7The lid.176.bin version of this language identification model: https://fasttext.cc/docs/en/language-identification.html\n8https://cloud.google.com/bigquery/public-data/\n9\nPublished in Transactions on Machine Learning Research (12/2023)\nDescription\nDetails\nMaximum characters\nRemove code files with >100k characters.\nSmall changes\nSubsample changes with \u2264 2 lines with 50% probability.\nLong-range refactorings\nSubsample changes spanning \u2265 200 lines with 10% proba-\nbility.\nEmpty commit message\nRemove commits with empty commit subject.\nAutomatic commits\nRemove commits that either contain or are equal to a list\nof stop words.\nHash messages\nRemove commits with whitespace-separated words-to-\ncharacter ratio >20.\nData files\nSubsample data formats (JSON, YAML, XML, HTML) with\n50% probability.\nTable 4: Git commit filters.\n3.6\nWeighting of data sources\nThere were several discussions within the community about whether to up-sample or down-sample certain\nprogramming languages, as the amount of compute budget allocated to a data source in a given language can\nsignificantly affect the model\u2019s performance in that language. However, we realized that the largest amount\nof available data comes from popular programming languages and would, therefore, benefit a larger group\nof end-users. Moreover, after the deduplication process, we found that several high-resource programming\nlanguages, such as C, C++, C#, Java, Javascript, Python, and PHP, had a similar amount of data ranging\nfrom 44\u201387 GB. This further reinforced our belief that we did not need to drastically re-weigh the existing\ndata distribution. Thus, in this work, we followed the natural distribution of data during training and sampled\ndata sources proportionally to their volume. However, we did make an exception for JSON, YAML, and\nCSS, as we only want the LLM to learn the data format without wasting compute resources on memorizing\nthe data in such files. For that reason, we re-weighed the volume of the data source to 1 GB for JSON and\nYAML and 3GB for CSS.\n4\nPII redaction\nThis section outlines our efforts to remove Personally Identifiable Information (PII) from the training data.\nIn Section 4.1, we first describe how we collected a large set of PII annotations. We used these annotations\nto explore various techniques to train a PII detection model in Section 4.3, building on top of the encoder\nmodel we developed in Section 4.2.\n4.1\nData collection\nWe utilized the Toloka platform9 to engage 1,399 crowd-workers from 35 countries in annotating a dataset\nfor PII in source code. On average, participants completed 206 tasks, earned about $27, and worked 3.1\nhours. Our goal was to identify PII in various forms, such as names, usernames, emails, IP addresses, keys,\npasswords, and IDs. To ensure that crowd-workers received fair compensation, we established an hourly pay\nrate of $7.30, taking into consideration different minimum wage rates across countries and their corresponding\npurchasing power. We limited annotation eligibility to countries where the hourly pay rate of $7.30 was\nequivalent to the highest minimum wage in the US ($16.50) in terms of purchasing power parity. A complete\nlist of countries that participated in the annotation can be found in Table B.1 of Appendix B. Crowd workers\nin Toloka can do tasks whenever or wherever; there is no obligation to complete a certain task or spend a\nfixed amount of time on it. Thus, they utilize free choice when working on the tasks. Out of 1,399 crowd\nworkers, 695 filled a survey on task quality, and 519 completed the survey. The average score for the question\nasking whether the participant would like to contribute to another project like this is 4.92 on a scale 1\u20135.\n9https://toloka.ai/\n10\nPublished in Transactions on Machine Learning Research (12/2023)\nc\ncpp\ncsharp\njava\nruby\npython\njs\nphp\nrust\ngo\ntypescript\nsql\ncss\nhaskell\nkotlin\nperl\ndart\nlua\nscala\nfortran\nshell\nhtml\npascal\nassembly\nscheme\njulia\nstata\npowershell\nprolog\nmathematica\nmatlab\nProgramming language\n0\n200\n400\n600\n800\nNumber of files\nFigure 1: Distribution of programming languages in the annotated PII dataset.\nThe dataset comprises 12,000 files, each containing approximately 50 lines of code written in 31 programming\nlanguages. Figure 1 shows the distribution of programming languages in the dataset. To increase the\nrepresentation of rare PII types, such as keys and IP addresses, 7,100 files were pre-filtered from a larger\nsample. We utilized the detect-secrets tool10 with all default plugins activated, along with the regular\nexpressions by Ben Allal et al. (2023) for detecting emails, IPv4 and IPv6 addresses. To prevent biasing the\nannotation too much towards these detection tools, the remaining 5,100 files were randomly selected from the\ndataset without pre-filtering.\nDuring annotation, we differentiated between various types of PII based on the specific context in which\nit appeared. Specifically, we distinguished whether the PII was present in the code\u2019s license header, was\nused as a placeholder, or constituted confidential data. This categorization was necessary because the PII in\nlicense headers is usually provided voluntarily by authors for code attribution and may not require masking.\nSimilarly, placeholders are not real secrets and do not need to be masked. We applied this categorization to\nnames, emails, and usernames. See Table 5 for an overview of all PII entities.\nThe annotators detected a total of 22,950 PII entities in the dataset. To evaluate the quality of the dataset,\nwe manually inspected 300 files that contained various PII types and calculated the recall and precision for\neach type, as shown in Table 5. We found that annotating secret IDs was particularly challenging, as the\nannotators tended to produce many false positives and negatives. As a result, we decided to exclude this\ncategory from the PII detection model training.\n4.2\nStarEncoder\nAs part of our PII detection efforts, we trained an encoder-only model (i.e., bi-directionally self-attentive\nTransformers) that can be efficiently fine-tuned for both code- and text-related tasks. We used the Masked\nLanguage Modelling (MLM) and Next Sentence Prediction (NSP) objectives from BERT (Devlin et al., 2019;\nLiu et al., 2019) and predicted masked-out tokens from an input sentence and whether a pair of sentences\noccur as neighbors in a document.\nWe separate code snippets in the input as follows: [CLS] Snippet-1 [SEP] Snippet-2, where the two code\nsnippets are selected randomly, either from the same source file or from two distinct documents. For the\nMLM loss, we mask tokens in the input independently with an probability of 15%. For the NSP loss, we use\na linear classifier applied to the representation output at the [CLS] token. We train for 100,000 steps with a\nglobal batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are\n10https://github.com/Yelp/detect-secrets\n11\nPublished in Transactions on Machine Learning Research (12/2023)\nPII type\nCount\nRecall\nPrecision\nIP_ADDRESS\n2526\n85%\n97%\nKEY\n308\n91%\n78%\nPASSWORD\n598\n91%\n86%\nID\n1702\n53%\n51%\nEMAIL\n5470\n99%\n97%\nEMAIL_EXAMPLE\n1407\nEMAIL_LICENSE\n3141\nNAME\n2477\n89%\n94%\nNAME_EXAMPLE\n318\nNAME_LICENSE\n3105\nUSERNAME\n780\n74%\n86%\nUSERNAME_EXAMPLE\n328\nUSERNAME_LICENSE\n503\nAMBIGUOUS\n287\nTable 5: Overview of the PII types and the number of collected annotations. We investigate the annotation\nquality by reporting the precision and recall of a manual inspection on 300 files. Each subcategory was\nmapped back to its corresponding PII type for the inspection.\nHyperparameter\nValue\nHidden size\n768\nIntermediate size\n3072\nMax. position embeddings\n1024\nNum. of attention heads\n12\nNum. of hidden layers\n12\nAttention\nMulti-head\nNum. of parameters\n\u2248125M\nTable 6: Model architecture of StarEncoder.\nobserved. This takes roughly two days using 64 NVIDIA A100 GPUs. Details about the model architecture\nare reported in Table 6.\n4.3\nPII detection model\nWe fine-tuned StarEncoder on the annotated PII dataset for the Named Entity Recognition (NER) task. We\nadded a linear layer as a token classification head on top of the model, with 6 target classes: names, emails,\nkeys, passwords, IP addresses, and usernames. We excluded IDs due to low annotation quality and did not\ndifferentiate between the categorization of PII entities (license headers, placeholders) because of the model\u2019s\npoor performance in distinguishing them. We split the dataset into a training set of 7,878 examples and a test\nset of 4,000 examples, ensuring that both splits have a balanced representation of the different PII types. See\nTable 7. We make the training and evaluation splits available under gated access at https://hf.co/BigCode.\nFine-tuning baseline\nWe fine-tune StarEncoder on the PII training set, and 400 annotated files from\nBen Allal et al. (2023). We achieve F1 scores of more than 90% on names, emails, and IP addresses and\n73.39% on passwords. The model\u2019s performance is comparatively low on keys and usernames, with F1 scores\nof only 56.66% and 59.39%, respectively. We attribute the low performance on keys to the limited number of\nlabels for this type of PII, as only 308 instances were available. For usernames, we observed the model often\nconfused them with decorators and values in paths. This is most likely because we annotated usernames\ninside links for social media platforms.\n12\nPublished in Transactions on Machine Learning Research (12/2023)\nEntity type\nTrain\nTest\nEMAIL\n4721\n1742\nNAME\n3847\n1298\nIP_ADDRESS\n1941\n521\nUSERNAME\n1320\n346\nPASSWORD\n390\n148\nKEY\n171\n118\nTable 7: Train-test split of the annotated PII dataset.\nMethod\nEmail address\nIP address\nKey\nPrec.\nRecall\nF1\nPrec.\nRecall\nF1\nPrec.\nRecall\nF1\nRegex\n96.20%\n97.47%\n96.83%\n71.29%\n87.71%\n78.65%\n3.62%\n49.15%\n6.74%\nNER\n94.01%\n98.10%\n96.01%\n88.95%\n94.43%\n91.61%\n60.37%\n53.38%\n56.66%\n+ pseudo labels\n97.73%\n98.94%\n98.15%\n90.10%\n93.86%\n91.94%\n62.38%\n80.81%\n70.41%\nTable 8: Comparing PII detection performance: Regular Expressions, NER Pipeline with Annotated Data,\nand NER Pipeline with Annotated Data + Pseudo-Labels\nPseudo-labels\nTo improve the detection of key and password entities, we employed a pseudo-labeling\ntechnique as described by Lee (2013). This method involves training a model on a small set of labeled\ndata and subsequently generating predictions for a larger set of unlabeled data. Specifically, we annotated\n18,000 files using an ensemble of two encoder models, which were fine-tuned on the 400-file PII dataset from\nBen Allal et al. (2023). To identify reliable pseudo-labels, we calculated the average probability logits from\nour models and applied filtering criteria. Specifically, we set a minimum threshold of 0.5 for all entities,\nexcept for names and usernames, for which we used a higher threshold of 0.6. However, upon reviewing the\nresults, we found a significant number of false positives for keys and passwords. As a result, we decided to\nonly retain entities that were preceded by a trigger word, such as key, auth, or pwd, within the preceding 100\ncharacters. Training on this synthetic dataset before fine-tuning on the annotated one yielded superior results\nfor all PII categories, as demonstrated in Tables 8 and 9. Only the performance for detecting usernames did\nnot show significant improvement, so we decided to exclude it from the PII redaction process.\nComparison against regex baseline\nWe compared our PII detection models against the regular expres-\nsions (regexes) employed in Ben Allal et al. (2023). The regexes only support the detection of emails, IP\naddresses, and keys. Note that we enhanced the email regex, as explained in the Appendix, to address false\npositives we found during the evaluation on this benchmark. This modification boosted the F1 score of the\nregex from 81.8% to 96.83%. Nevertheless, our PII detection models still surpassed the regex approach in\ndetecting all three entities, as shown in Table 8. We note that the performance difference was especially large\non keys and found that the detect-secrets tool generated many false positives, especially in specific pro-\ngramming languages like Go and C-sharp that weren\u2019t well represented in the regex evaluation. Consequently,\nthe overall precision of the tool was below 4%.\nPost-processing\nBefore applying the best PII detection model to the full dataset, we observed a couple of\nfrequent detection errors. We added the following post-processing techniques to reduce the number of false\npositives:\n\u2022 Ignore secrets with fewer than 4 characters.\n\u2022 Detect full names only by requiring at least one space within the name.\n13\nPublished in Transactions on Machine Learning Research (12/2023)\nMethod\nName\nUsername\nPassword\nPrec.\nRecall\nF1\nPrec.\nRecall\nF1\nPrec.\nRecall\nF1\nNER\n83.66%\n95.52%\n89.19%\n48.93%\n75.55%\n59.39%\n59.16%\n96.62%\n73.39%\n+ pseudo labels\n86.45%\n97.38%\n91.59%\n52.20%\n74.81%\n61.49%\n70.94%\n95.96%\n81.57%\nTable 9: Comparison of PII detection performance: NER Pipeline with Annotated Data vs. Annotated Data\n+ Pseudo-Labels\n\u2022 Ignore detected keys with fewer than 9 characters or that are not gibberish using a\ngibberish-detector.11\n\u2022 Ignore IP addresses that aren\u2019t valid or are private (non-Internet facing) using the ipaddress python\npackage. We also ignore IP addresses from popular DNS servers. We use the same list as in Ben Allal\net al. (2023).\nPII placeholders\nWe replaced the detected PII entities with the following tokens:\n<NAME>, <EMAIL>, <KEY>, <PASSWORD>\nTo mask IP addresses, we randomly selected an IP address from 5 synthetic, private, non-internet-facing IP\naddresses of the same type that can be found in Appendix C.\nGithub issues\nWe already employed a regex approach to detect keys, IP addresses, and emails in the\nGithub issues, so we only used the PII detection model to redact names. We anonymized the usernames\nof the authors by replacing them with a participant counter within the conversation, e.g. username_1 to\nrefer to second participant (see Section 5.1 for formatting details). We prepend these pseudonyms to the\nbeginning of each comment such that we preserve the speaker identity of the author. In addition, we redact\nall mentions of these usernames in the messages. Note that we only mask the usernames of active participants\nin the conversation and mentions of non-participating users are not anonymized.\nCompute resources\nWe used the PII detection model to identify PII across all programming languages\nin the training dataset, including GitHub issues (names only), Git commits, and Jupyter notebooks. The\ntotal dataset amounts to 815 GB in size. We ran inference on multiple NVIDIA A100 80 GB GPUs, which\nrequired 800 GPU-hours.\n5\nModel training\nThis section presents information on the training process of the StarCoder models. Before we proceed, we\nfirst clarify the differences between the two models:\nStarCoderBase is the first model trained on 1 trillion tokens sourced from the curated dataset described\nin Section 3.\nStarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2\nepochs).\nThroughout the following, we show how we formatted the training data (Section 5.1), decontaminated the\ntraining data (Section 5.2), and provide details regarding the tokenizer (Section 5.3), the model architecture\n(Section 5.4), the training process (Section 5.5), multi-node GPU setup (Section 5.6), and CO2 emissions\n(Section 5.7).\n11https://github.com/domanchi/gibberish-detector\n14\nPublished in Transactions on Machine Learning Research (12/2023)\n5.1\nData formatting\nWe present the formatting guidelines for each of the data sources below. We provide the templates below\nin which <token> refers to a sentinel token, and metadata and data refer to placeholders for data fields,\nrespectively.\nCode\nWe prepend the repository name, file name, and the number of stars to the context of the code file.\nTo not overfit on the exact number of stars, we categorized GitHub stars into five buckets: 0, 1\u201310, 10\u2013100,\n100\u20131000, 1000+. To enable the model to operate without this metadata during inference, we prefixed the\nrepository name, filename, and stars independently at random, each with a probability of 0.2.\n<reponame>reponame<filename>filename<gh_stars>stars\\ncode<|endoftext|>\nTo the source code in this template (i.e. code), we apply the fill-in-the-middle transformation (FIM;\nBavarian et al., 2022). More precisely, we apply FIM at the character-level to the source code files with a\nFIM-rate of 0.5, and use PSM mode with probability .5 and SPMv2 mode with probability .5.\nIssues\nWe use sentinel tokens to mark the opening of an issue and subsequently include its title. We\nseparate the sequence of comments by a <issue_comment> token and include a anonymized speaker identifier\nbefore the comment. Specifically, we refer to authors by their participant counter within the conversation,\ne.g. username_1 to refer to second participant in the issue. To distinguish between the different turns, we\nuse comment1, id1 to refer to the second comment and its anonymized speaker id, respectively.\n<issue_start>Title: title\\nusername_id0:comment0<issue_comment>username_id1:comment1\n... <issue_closed (optional)><|endoftext|>\nJupyter \u2013 scripts\nJupyter scripts were formatted in the same manner as code.\nJupyter \u2013 structured\nParsed Jupyter notebooks come in chains of text, code, and outputs, and we\nseparated them with sentinel tokens. Note that we use text2, code2, output2 to refer to the 3rd triplet in the\nnotebook.\n<jupyter_start><jupyter_text>text0<jupyter_code>code0\n<jupyter_output>output0<jupyter_text> ... <|endoftext|>\nGit commits\nWe separate the code before the commit, the commit message, and the code after the commit\nwith sentinel tokens. As explained in Section 3.4, we use the full files with 20% probability and otherwise use\na small window (0-32 lines) around the changed lines.\n<commit_before>code_before<commit_msg>message<commit_after>code_after<|endoftext|>\nWe summarize all sentinel tokens in Table 10.\n5.2\nTraining data decontamination\nThe code training data was decontaminated by removing files that contained docstrings or solutions from\nHumanEval and MBPP, docstrings from APPS, questions from GSM8K, or prompts from DS1000. (These\nbenchmarks are further described in Section 6.) To give an indication of the amount of data removed by\ndecontamination, Python is the language with the highest number of matches, with 558 files removed.\n15\nPublished in Transactions on Machine Learning Research (12/2023)\nToken\nDescription\n<|endoftext|>\nend of text/sequence\n<fim_prefix>\nFIM prefix\n<fim_middle>\nFIM middle\n<fim_suffix>\nFIM suffix\n<fim_pad>\nFIM pad\n<reponame>\nrepository name\n<filename>\nfile name\n<gh_stars>\nGitHub stars\n<issue_start>\nstart of GitHub issue\n<issue_comment>\nstart of GitHub issue comment\n<issue_closed>\nGitHub issue closed event\n<jupyter_start>\nstart of Jupyter notebook\n<jupyter_text>\nstart of Jupyter text cell\n<jupyter_code>\nstart of Jupyter code cell\n<jupyter_output>\nstart of Jupyter output cell\n<empty_output>\noutput cell without content\n<commit_before>\ncode snippet before commit\n<commit_msg>\ncommit message\n<commit_after>\ncode snippet after commit\nTable 10: Overview of the sentinel tokens.\n5.3\nTokenizer\nThe model\u2019s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design\nchoices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding\nwith a vocabulary size of 49,152 tokens\u2014including the sentinel tokens from table 10. The pre-tokenization\nstep includes a digit-splitter and the regex splitter from the GPT-2 pre-tokenizer.\n5.4\nModel Architecture\nWe trained a 15.5B parameter model with the same architecture as SantaCoder (Ben Allal et al., 2023). It is a\ndecoder-only Transformer with Multi-Query-Attention (MQA; Shazeer, 2019), and learned absolute positional\nembeddings. We also apply Fill-in-the-Middle (FIM; Bavarian et al., 2022) transformations to the training\ndata, see Section 5.1. We used FlashAttention (Dao et al., 2022) to speed up the attention computation and\nreduce its memory footprint, allowing us to scale to a 8K context length. To make FlashAttention work\nwith MQA during training, we simply expand the key and value before calling the attention kernel. The\narchitecture hyper-parameters are given in Table 11. In addition, we have included the hyperparameters of\nSantaCoder(Ben Allal et al., 2023) for comparison.\n5.5\nTraining details\nStarCoderBase\nThe model was trained for 250k iterations, with a batch size of 4M tokens, for a total of\none trillion tokens. We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22128 and a weight\ndecay of 0.1. The learning rate followed a cosine decay from 3 \u00d7 10\u22124 to 3 \u00d7 10\u22125 after a linear warmup of\n2,000 iterations.\nStarCoder\nStarting from StarCoderBase, we fine-tuned a Python variant of the model for 2 epochs on the\nPython subset of the training data. We used the same settings as StarCoderBase, except that we used a\nlearning rate of 5 \u00d7 10\u22125 and decayed it to 5 \u00d7 10\u22126 after 1,000 iterations of linear warmup. We trained for\n8,500 steps.\n16\nPublished in Transactions on Machine Learning Research (12/2023)\nHyperparameter\nSantaCoder\nStarCoder\nHidden size\n2048\n6144\nIntermediate size\n8192\n24576\nMax. position embeddings\n2048\n8192\nNum. of attention heads\n16\n48\nNum. of hidden layers\n24\n40\nAttention\nMulti-query\nMulti-query\nNum. of parameters\n\u2248 1.1B\n\u224815.5B\nTable 11: Model architecture of StarCoder. We also include SantaCoder (prior work by the community).\n5.6\nMulti-Node GPU Setup\nWe trained our model on a GPU cluster with 512 A100 80 GB GPUs distributed across 64 nodes. We\npartitioned the model with a 3D-parallel layout that shards the model with both tensor and pipeline parallelism\nrank 4, requiring 16 GPUs (two nodes) for one replica. To fully leverage the cluster\u2019s capabilities, we used\n32-fold data parallelism. To optimize GPU utilization and reduce idle compute bubbles, we maintained a\nmicro-batch size of 1 and accumulated for 16 steps, resulting in a global batch size of 512 (equivalent to\n4M tokens). We used Megatron-LM\u2019s distributed optimizer because we found that it leads to slightly higher\nthroughput in this configuration. Since it requires the gradient reduction step in FP32, the training in BF16\nleads to 10% lower throughput than FP16, but we used it anyway to avoid training instabilities.\nExcept for a few restarts, we did not experience significant training instabilities.\n5.7\nCO2 emissions\nStarCoderBase\nWe report the carbon footprint (Lacoste et al., 2019) of training StarCoderBase. Based\non the total number of GPU hours that training took (320,256) and an average power usage of 280W per\nGPU, this adds up to 89671.68 kWh of electricity consumed during the training process. Multiplied by the\ncarbon intensity of the energy of the us-west-2 AWS location (0.15495 kgCO2e per kWh) and the average\nPower Usage Effectiveness of 1.2 across AWS datacenters, this results in 16.68 tonnes of CO2eq emitted.\nStarCoder\nThe fine-tuned model adds 3.5% of training time, which translates to an additional estimated\nemission of 0.58 tonnes of CO2eq.\n6\nEvaluation\nIn this section, we first outline the models we evaluated in addition to StarCoder and StarCoderBase. Then we\nreport on the Python language performance of all models on the HumanEval (Chen et al., 2021), MBPP (Austin\net al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation\nusing a variety of benchmarks and tasks.\nA Code LM Evaluation Harness\nTo enable reproducible and centralized evaluation of StarCoder and\nother Code LLMs, we developed a Code LM Evaluation Harness (Ben Allal et al., 2022), inspired by the LM\nEvaluation-Harness (Gao et al., 2021b). This harness provides a framework for the efficient evaluation of\ncode models, utilizing data parallelism and docker containers for execution. It supports several benchmarks,\nincluding HumanEval, MultiPL-E, and DS-1000.\nOther Models Evaluated\nWe compare StarCoder and StarCoderBase to the following models.\n1. CodeGen-16B-Multi (Nijkamp et al., 2023) is an open-access, 16B parameter model that is trained\non the Pile (Gao et al., 2021a), and then on additional code written in C, C++, Go, Java, JavaScript,\nand Python from the GitHub BigQuery dataset (Smith, 2016).\n17\nPublished in Transactions on Machine Learning Research (12/2023)\nModel\nSize\nHumanEval\nMBPP\nOpen-access\nLLaMA\n7B\n10.5\n17.7\nLLaMA\n13B\n15.8\n22.0\nSantaCoder\n1.1B\n18.0\n35.0\nCodeGen-Multi\n16B\n18.3\n20.9\nLLaMA\n33B\n21.7\n30.2\nCodeGeeX\n13B\n22.9\n24.4\nLLaMA-65B\n65B\n23.7\n37.7\nCodeGen-Mono\n16B\n29.3\n35.3\nStarCoderBase\n15.5B\n30.4\n49.0\nStarCoder\n15.5B\n33.6\n52.7\nClosed-access\nLaMDA\n137B\n14.0\n14.8\nPaLM\n540B\n26.2\n36.8\ncode-cushman-001\n12B\n33.5\n45.9\ncode-davinci-002\n175B\n45.9\n60.3\nTable 12: Comparing StarCoder\u2019s performance (pass@1) on the HumanEval and MBPP Python with several\nother models. StarCoder and StarCoder base obtain the highest performance of open-access models, and\ncomparable performance to the code-cushman-001 closed access model.\n2. CodeGen-16B-Mono is a version of CodeGen-16B-Multi that is fine-tuned on additional Python\ncode from GitHub, though the dataset is not publicly available.\n3. CodeGeeX (Zheng et al., 2023) is an open-access 13B parameter model trained on 23 programming\nlanguages selected from the Pile, the CodeParrot dataset (Wolf et al., 2020), and additional data\nfor Python, Java, and C++.\nCodeGeeX also includes its own multi-language benchmark suite,\nHumanEval-X, which we discuss below.\n4. code-cushman-001 is a 12B parameter model by OpenAI and was the initial model for GitHub\nCopilot (Chen et al., 2021). The details of its training set are unknown. This model has been\ndeprecated by OpenAI but was available from the Microsoft Azure OpenAI Service at the time of\nwriting.12\n5. Finally, although they are not specifically trained for code generation, we include some results from\nthe LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), and LaMDA (Thoppilan et al.,\n2022) papers. LLaMA\u2019s license prohibits commercial use, and PaLM and LaMDA are not publicly\navailable.\n6.1\nStarCoder: Python Evaluation\nIn this section, we evaluate the performance of StarCoder on Python, comparing it to both open-access and\nclosed-access models. We first report performance on HumanEval (Chen et al., 2021) and MBPP (Austin\net al., 2021), which are two widely used benchmarks of Python performance. However, we also measure\nperformance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science\nproblems based on StackOverflow questions.\n6.1.1\nThe HumanEval and MBPP Benchmarks\nHumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs\nconsisting of hundreds of Python programming problems that use test cases to validate the code produced by\n12There had been a code-cushman-002, but it is not available at the time of writing.\n18\nPublished in Transactions on Machine Learning Research (12/2023)\nFormat\nModel\nMatplotlib\nNumPy\nPandas\nPyTorch\nSciPy\nScikit-\nLearn\nTensorFlow\nOverall\nNumber of problems:\n155\n220\n291\n68\n106\n115\n45\n1,000\nCompletion\nSantaCoder-1B\n21.6\n4.6\n0.9\n2.6\n2.4\n4.8\n3.1\n5.7\nCompletion\nInCoder-6B\n28.3\n4.4\n3.1\n4.4\n2.8\n2.8\n3.8\n7.4\nCompletion\nCodeGen-16B-Mono\n31.7\n10.9\n3.4\n7.0\n9.0\n10.8\n15.2\n11.7\nCompletion\ncode-cushman-001\n40.7\n21.8\n7.9\n12.4\n11.3\n18.0\n12.2\n18.1\nCompletion\nStarCoderBase\n47.0\n27.1\n10.1\n19.5\n21.7\n27.0\n20.5\n23.8\nCompletion\nStarCoder\n51.7\n29.7\n11.4\n21.4\n20.2\n29.5\n24.5\n26.0\nInsertion\nSantaCoder-1B\n21.6\u2217\n13.8\n2.0\n3.8\n5.7\n6.9\n14.8\n9.3\nInsertion\nInCoder-6B\n28.3\u2217\n4.6\n2.9\n4.4\n2.8\n3.1\n7.8\n7.5\nInsertion\nStarCoderBase\n47.0\u2217\n26.3\n10.9\n16.6\n20.2\n30.2\n22.3\n24.0\nInsertion\nStarCoder\n51.7*\n30.8\n10.3\n21.0\n20.2\n27.4\n20.0\n25.4\nTable 13: Performance of open-access and closed-access models on DS-1000. Benchmarks are as follows. All\nmodels evaluated at temperature=0.2, top_p=0.5, max_length=1024. Scores reflect mean pass@1 accuracy\naveraged over 40 samples. \u2217: Matplotlib task does not have right sided context, so insertion and completion\nformats are identical.\na Code LLM. Code LLMs generate code by sampling from their output distribution. We report performance\nusing the pass@k metric (Chen et al., 2021): the total fraction of benchmark problems solved, where a\nproblem is considered solved if any one of k code samples passes every test case. Like Chen et al. (2021),\nwe use sampling temperature 0.2 for pass@1, and temperature 0.8 for k > 1. We generate n = 200 samples\nfor all experiments with open-access models. For API models, we use n = 20 samples, which is enough to\nestimate pass@1. We focus on the simplest version of pass@k, which is pass@1: the likelihood that a problem\nis solved in a single attempt by the model.\nTable 12 compares StarCoder (and StarCoderBase) on HumanEval and MBPP to several open-access and\nclosed-access models:\n1. StarCoder is the highest-performing open-access model on both benchmarks.\n2. StarCoder outperforms the largest models, including PaLM, LaMDA, and LLaMA, despite being\nsignificantly smaller.\n3. StarCoderBase is also very capable on Python and is competitive with CodeGen-16B-Mono, a\nsimilarly-sized open-access model that was fine-tuned on Python.\n4. StarCoder outperforms OpenAI\u2019s code-cushman-001 (12B) model.\n6.1.2\nThe DS-1000 Python Data Science Benchmarks\nA major limitation of HumanEval and MBPP is that they are simple programming puzzles that are not\nrepresentative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al.,\n2022) has a suite of 1,000 realistic and practical data science workflows across seven libraries and evaluates\ngenerations in execution against test cases.\nDS-1000 supports two evaluation modes: completion and insertion (via FIM). We report completion scores for\nall models but insertion scores only for models that support it: the StarCoder models and InCoder-6B (Fried\net al., 2022). DS-1000 also categorizes problems based on the libraries used: Matplotlib, NumPy, Pandas,\nSciPy, Scikit-Learn, PyTorch, and TensorFlow. We report pass@1 for each library and an overall score in\nTable 13 and draw the following conclusions:\n1. StarCoder substantially outperforms all other models on data science problems from the DS-1000\nbenchmark. Moreover, this is true across every kind of data science library.\n19\nPublished in Transactions on Machine Learning Research (12/2023)\n2. StarCoderBase also outperforms every other model, but is slightly behind StarCoder on DS-1000.\n3. We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks\ndoes not always correlate with performance on the more realistic DS-1000 benchmarks. For example,\nCodeGen-Mono slightly outperforms code-cushman-001 and the StarCoder models on HumanEval\nand MBPP, but is significantly worse on DS-1000. This demonstrates the importance of evaluating\nmodels on a range of benchmarks.\n6.1.3\nThe ODEX Open-Domain Coding Benchmark\nOur previous evaluations focus either on closed domains (i.e., primarily built-in Python functions, as in\nMBPP and HumanEval) or specific domains (e.g., data science, as in DS-1000). To evaluate model ability\nto generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022)\ncontaining 505 open-domain and 440 closed-domain Python coding queries, in four natural languages \u2014\nEnglish, Spanish, Japanese, and Russian \u2014 with test-case-based execution evaluation.\nWe report the pass@1 metric for StarCoder and baseline models, including Codex (code-davinci-001), CodeGen-\n16B-Mono, and SantaCoder. In addition to the overall execution accuracy, we also categorize problems by\nlanguages and domains, which are: (1) queries in the closed-domain (using only built-in Python functions)\nand open-domain (using functions from imported libraries), and (2) queries with instructions written in\nEnglish, Spanish, Japanese, and Russian, respectively. We report overall scores and scores in different domains\nand languages in Table 14 and draw the following conclusions:\n1. StarCoder substantially outperforms all other models on open-domain coding queries from the ODEX\nbenchmark.\n2. StarCoderBase also outperforms every other model, even better than StarCoder in the ODEX English\nsubset, but slightly behind in other languages.\n3. Both StarCoder and StarCoderBase models generally exhibit smaller gaps between open- and closed-\ndomain queries than other baseline models, despite the higher overall execution accuracy. This result\nindicates that StarCoder models acquire more generalized skills about coding queries in the open\ndomain (i.e., concerning diverse Python libraries), while other models exhibit larger performance\ndrops when moving from the closed to open domain.\nModel\nEnglish\nSpanish\nJapanese\nRussian\noverall\nopen\nclosed\noverall\nopen\nclosed\noverall\nopen\nclosed\noverall\nopen\nclosed\nCodeGen-16B-Mono\n33.7\n25.2\n43.1\n30.0\n25.0\n43.1\n37.8\n26.6\n62.8\n46.8\n30.4\n60.1\ncode-cushman-001\n31.9\n24.4\n40.2\n31.9\n27.7\n36.7\n25.7\n21.2\n35.5\n40.0\n26.0\n51.6\ncode-davinci-001\n33.6\n26.9\n41.0\n36.9\n31.7\n42.9\n31.0\n23.7\n47.3\n43.2\n28.9\n55.1\nSantaCoder\n37.7\n30.9\n45.1\n32.1\n26.0\n39.1\n28.1\n23.0\n39.4\n36.9\n23.0\n48.3\nStarCoderBase\n46.5\n40.7\n53.0\n30.1\n25.4\n35.5\n41.2\n37.6\n49.2\n46.1\n34.0\n56.1\nStarCoder\n44.7\n37.0\n53.1\n37.6\n32.9\n42.9\n44.2\n39.6\n54.5\n50.4\n33.8\n64.1\nTable 14: Performance on the ODEX benchmark by instruction languages and code domains: open problems\nuse libraries, while closed use only built-in Python functions.\n6.2\nStarCoder and StarCoderBase: Multi-Language Evaluation\nIn this section, we focus primarily on StarCoderBase, and evaluate its performance on a variety of programming\nlanguages and programming tasks, including producing code from natural language descriptions, documenting\ncode, predicting type annotations, and more. This section also shows that StarCoder, despite being fine-tuned\non Python, remains a very capable multi-language Code LLM and even outperforms StarCoderBase on some\nlanguages.\n20\nPublished in Transactions on Machine Learning Research (12/2023)\nLanguage\nCodeGen-16B-Multi\nCodeGeeX\ncode-cushman-001\nStarCoder\nStarCoderBase\ncpp\n21.00\n16.87\n30.59\n31.55\n30.56\nc-sharp\n8.24\n8.49\n22.06\n21.01\n20.56\nd\n7.68\n9.15\n6.73\n13.57\n10.01\ngo\n13.54\n11.04\n19.68\n17.61\n21.47\njava\n22.20\n19.14\n31.90\n30.22\n28.53\njulia\n0.00\n0.29\n1.54\n23.02\n21.09\njavascript\n19.15\n16.92\n31.27\n30.79\n31.70\nlua\n8.50\n10.96\n26.24\n23.89\n26.61\nphp\n8.37\n13.51\n28.94\n26.08\n26.75\nperl\n3.42\n8.09\n19.29\n17.34\n16.32\npython\n19.26\n21.62\n30.71\n33.57\n30.35\nr\n6.45\n3.92\n10.99\n15.50\n10.18\nruby\n0.00\n3.34\n28.63\n1.24\n17.25\nracket\n0.66\n3.31\n7.05\n0.07\n11.77\nrust\n4.21\n7.88\n25.22\n21.84\n24.46\nscala\n2.37\n8.95\n27.62\n27.61\n28.79\nbash\n0.61\n2.75\n11.74\n10.46\n11.02\nswift\n1.25\n7.26\n22.12\n22.74\n16.74\ntypescript\n20.07\n10.11\n31.26\n32.29\n32.15\nTable 15: Comparing StarCoder to multi-language open-access (e.g., CodeGen-16B-Multi) and closed-access\nmodels (e.g., code-cushman-001) on 19 programming languages. We report pass@1 on HumanEval (Chen\net al., 2021), which we translate from Python to the other languages using MultiPL-E (Cassano et al., 2023).\n6.2.1\nEvaluation on 19 Programming Languages with MultiPL-E\nWe evaluate the ability of StarCoder to turn natural language into working code in multiple programming\nlanguages using MultiPL-E (Cassano et al., 2023), which translates the HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021) Python benchmarks into 18 other programming languages as follows.\nMultiPL-E has a set of rule-based compilers that translate Python benchmarks to each target programming\nlanguage. Each compiler expects a benchmark in the HumanEval format: 1) a natural language description\n(in a docstring), 2) a function signature (name, arguments, and, potentially, types), and 3) a set of hidden\nassertions. The MultiPL-E compilers translate the function signature, assertions, and docstring (which may\nhave doctests) into a target language. Thus, MultiPL-E gives us a parallel set of benchmarks derived from\nHumanEval and MBPP to compare model performance across programming languages.13 The MultiPL-E\nlanguages include both high and low-resource languages, statically and dynamically typed languages, and a\nvariety of other programming language features.\nTable 15 shows how these models perform on 19 programming languages, and from it, we draw the following\nconclusions:\n1. Across all 19 programming languages, StarCoderBase outperforms other open-access models, some-\ntimes showing more than 2\u00d7 performance.\n2. StarCoderBase is competitive with code-cushman-001 on most languages that we evaluate. There are\na few exceptions. For example, code-cushman-001 outperforms StarCoderBase by more than 5% on\nC++, Java, Ruby, and Swift, and StarCoder outperforms code-cushman-001 by more than 5% on\nJulia.\n13The MultiPL-E prompts are slightly different from the original HumanEval and MBPP prompts. For example, in HumanEval,\nsome ad hoc examples in docstrings are reformatted to be doctests so that they can be translated into examples in each target\nlanguage. MultiPL-E also omits three HumanEval benchmarks that do not fit the above format. These changes have a small\nimpact on pass rates.\n21\nPublished in Transactions on Machine Learning Research (12/2023)\nFormat\nModel\nValid (\u2191)\nInsecure (\u2193)\nCompletion\nStarCoderBase\n855/1000 (85.50%)\n340/855 (39.77%)\nInsertion\nStarCoderBase\n987/1000 (98.70%)\n354/987 (35.87%)\nCompletion\nInCoder-6B\n871/1000 (87.10%)\n309/871 (35.48%)\nInsertion\nInCoder-6B\n854/1000 (85.40%)\n293/854 (34.31%)\nCompletion\nCodeGen-16B-Multi\n955/1000 (95.50%)\n413/955 (43.25%)\nCompletion\ncode-cushman-001\n964/1000 (96.40%)\n408/964 (42.32%)\nTable 16: Performance on the Asleep at the Keyboard security benchmark (Pearce et al., 2022).\n3. Despite fine-tuning on Python, StarCoder remains competitive on most languages, and also out-\nperforms other open models.\nWhat is more surprising is that StarCoder slightly outperforms\nStarCoderBase on certain languages, despite being fine-tuned on Python. At this time, we can only\nspeculate on why this is the case, and further investigation of the open training data is likely to help\nshed light on this finding.\nThere are several other conclusions that we can draw from the table. For example, CodeGen-16B-Multi\nperforms better than one might expect on some languages that are reportedly not in its training set, including\nC#, Lua, PHP, and TypeScript. Its performance on TypeScript is less surprising since simple JavaScript\nfunctions often type-check with TypeScript by design. Similarly, StarCoder shows high performance on Swift,\neven though it was not included in its training set, as explained in Section 3.1.\n6.2.2\nThe \u201cAsleep at the Keyboard\u201d Security Benchmark\nA limitation of Code LLMs is that they can generate code with security vulnerabilities (Pearce et al., 2022).\nThe Asleep at the Keyboard benchmark by Pearce et al. (2022) has 89 security-sensitive scenarios across\nthree evaluation axes: (1) Diversity of Weakness (DoW) covers 18 different vulnerability classes in MITRE\u2019s\nCommon Weakness Enumeration (CWE) taxonomy, with scenarios drawn from the 2021 CWE Top 25 Most\nDangerous Software Weaknesses list published by MITRE; (2) Diversity of Prompt (DoP) evaluates the\nmodel\u2019s sensitivity to variations in the prompt for a single vulnerability class (SQL injection); (3) Diversity\nof Domain (DoD) contains security scenarios in the hardware description language Verilog. We focus on the\nDoW, which contains 54 scenarios (25 in C and 29 in Python) across 18 CWEs. We exclude scenarios that\nlack an automated test, leaving 40 scenarios (23 in C and 17 in Python).\nPearce et al. (2022) had previously evaluated the security of GitHub Copilot (as of August 2021), and in\nthis paper, we use the same methodology to evaluate StarCoderBase, InCoder-6B, CodeGen-16B-Multi, and\nOpenAI\u2019s code-cushman-001. We use the original benchmarking methodology: generating 25 completions\nper scenario at temperature 0.2 (1,000 completions per model). The dataset supports fill-in-the-middle, so\nwe include this configuration on models that support it. The results are shown in Table 16; Valid gives\nthe percentage of solutions that were syntactically valid (using py_compile for Python and gcc for C), and\nInsecure shows the percentage of valid solutions that contained the vulnerability the scenario tests for. From\nthis table, we draw the following conclusions.\n1. StarCoderBase has the highest rate of valid code.\n2. InCoder-6B has a slightly lower rate for insecure code generation, but this may be due to its lower\nrate of valid completions.\n3. Among the models with more than 95% valid code, StarCoder has the lowest rate of insecure\ncompletions.\n6.2.3\nFill in the Middle Benchmarks\nThe StarCoder models support fill in the middle (FIM) or infilling, which allows the model to generate\ncode conditioned on prefix and suffix code surrounding the insertion point. Only a handful of recent models\n22\nPublished in Transactions on Machine Learning Research (12/2023)\nModel\nJava\nJavaScript\nPython\nInCoder-6B\n0.49\n0.51\n0.31\nSantaCoder\n0.62\n0.60\n0.44\nStarCoder\n0.73\n0.74\n0.62\nTable 17: Performance on single-line fill-in-the-middle on the FIM benchmark by Ben Allal et al. (2023).\nModel\nNon-None F1\nAll F1\nInCoder-6B\n59.1\n46.8\nSantaCoder\n66.9\n78.5\nStarCoderBase\n77.4\n86.6\nStarCoder\n77.1\n86.4\nTable 18: Accuracy of Python return type prediction, using Fried et al. (2022)\u2019s adaptation of the Pradel\net al. (2020) benchmarks. We report both the overall F1 scores, which include trivial None-type prediction,\nand the F1 score for non-None types.\nsupport FIM: from OpenAI (Bavarian et al., 2022), InCoder (Fried et al., 2022), and our prior work on\nSantaCoder (Ben Allal et al., 2023). FIM opens up the possibility of a variety of tasks that go beyond\nleft-to-right code completion. We evaluate StarCoderBase on four established FIM benchmarks below.\nSingle-Line Infilling for Python, Java, and JavaScript\nFried et al. (2022) present a single-line fill-in-\nthe-middle task for Python that masks one line of code from a HumanEval solution and scores the model\u2019s\nability to complete the function. They turn every HumanEval solution into several fill-in-the-middle problems\nby masking each non-blank, non-comment line of code in the solution body into a fill-in-the-middle task.\nBen Allal et al. (2023) generalizes this benchmark to also support Java and JavaScript, using model-generated\nsolutions from MultiPL-E\u2019s translations. We compare the performance of StarCoderBase, SantaCoder, and\nInCoder on this task, evaluating using line exact match (Table 17). StarCoderBase significantly outperforms\nthe two smaller models.\nPython Return Type Prediction\nPradel et al. (2020) introduce methods and datasets for evaluating\nPython type annotations. Fried et al. (2022) adapt and filter one dataset from this work, consisting of Python\nfunctions from GitHub, and use it to evaluate infilling models on function return type prediction. We use\nthis dataset to compare StarCoder, StarCoderBase, and SantaCoder to InCoder on function return type\nprediction. Our setup follows Fried et al. (2022): each model uses greedy generation to infill return types\nwhile conditioning on the imports, body, and signature for each function. We report exact match accuracy on\nnormalized annotations for all functions in the evaluation set and only those with non-None annotations,\nfollowing Fried et al. (2022). We find that StarCoder and StarCoderBase outperform existing approaches at\nPython return type prediction (Table 18). However, we note that as the functions in this evaluation set were\ntaken from GitHub repositories, they may overlap with the training data for SantaCoder and the StarCoder\nmodels.\nTypeScript Type Prediction\nYee & Guha (2023) evaluate approaches to neural type prediction for\nTypeScript. However, instead of measuring accuracy, they argue that benchmarks should measure how many\nprojects or files do not have type errors with predicted types. This approach makes it possible to evaluate\ntype prediction for JavaScript programs that have never been translated to TypeScript, which reduces the\nlikelihood of dataset contamination. We add StarCoderBase to their evaluation framework and compare it to\nInCoder, which performs best at type prediction in the original work. Table 19 shows that StarCoderBase\noutperforms InCoder: (1) it produces more packages that type check, (2) across all packages, it produces\nmore files that type check, and (3) it produces fewer trivial type annotations than InCoder.\n23\nPublished in Transactions on Machine Learning Research (12/2023)\nPackages type check\nFiles with no errors\nTrivial annotations\n\u2713\nTotal\n%\n\u2713\nTotal\n%\n\u2713\nTotal\n%\nInCoder\n30\n128\n23.4\n571\n760\n75.1\n56\n117\n47.9\nStarCoderBase\n49\n128\n38.3\n593\n760\n78.0\n135\n299\n45.2\nTable 19: TypeScript type prediction performance using the dataset and metholody from Yee & Guha (2023).\nWe only evaluate JavaScript packages that have never been translated to TypeScript and compare StarCoder\nto InCoder, the best-performing model by Yee & Guha (2023). StarCoder outperforms InCoder in several\nways.\nModel\nBLEU\nInCoder-6B\n18.27\nSantaCoder\n19.74\nStarCoderBase\n21.38\nStarCoder\n21.99\nTable 20: Performance on the Python portion of the CodeXGLUE Code Summarization task, evaluating\nfunction docstring generation. Models are evaluated zero-shot using their infilling capability.\nPython Docstring Generation\nTo evaluate models\u2019 ability to generate documentation for functions, we\nuse the Python subset of the CodeXGLUE code summarization benchmark (Lu et al., 2021). This benchmark\nis constructed from the CodeSearchNet dataset (Husain et al., 2019), containing functions from public GitHub\nrepositories. Models infill the documentation string (docstring) for each function using greedy decoding,\nconditioned on the function signature and body. We follow the evaluation scheme of past work: docstrings\nare evaluated using smoothed 4-gram BLEU (Papineni et al., 2002) against the reference docstring from\nthe original function, using only the first lines of the generated and reference docstrings (removing, e.g.,\ndescriptions of function arguments and return types that may appear in later lines). In Table 20, we see that\nStarCoder and StarCoderBase obtain higher performance than past work on docstring generation. However,\nwe note that there may be an overlap between this evaluation dataset and the data used to train SantaCoder\nand the StarCoder models.\n6.3\nPerformance Improvement Through the Training Process\nWe evaluate the performance of StarCoderBase at several training checkpoints after every 200B tokens\nseen out of the total 1000B. Figure 2 (right) shows how performance (pass@1) changes during training\nfor each programming language supported by MultiPL-E. The performance curve for several high-resource\nprogramming languages suggests that training longer is likely to improve their performance further.\nHowever, some of the low-resource languages see limited improvement during training or even have a pass@1\ndecline. For example, R\u2019s pass@1 rate drops significantly between the 800B and 1000B (final) checkpoints.\nThe dependence of pass@1 on data size (Figure 2, left) further supports the hypothesis that this is related to\nthe amount of data available. The slope of the linear fit increases between 800B and 1000B checkpoints while\nthe intercept decreases, i.e., performance improves only for languages with large enough amounts of data\n(\u2273 1 GB).\nWe manually inspected the completions generated by R over several checkpoints to better understand model\nperformance. One might hypothesize that some problems are harder than others, and so the model gains and\nloses the ability to solve them in R over the 600B, 800B, and 1000B checkpoints, but we find that this is not\nthe case. Instead, we find significant variance in per-problem success rates for several problems (Table D.3).\nFor these problems, the pass rate between different checkpoints varies in what appears to be a completely\nuncorrelated manner. Moreover, manual inspection shows that the failures are caused by minor mistakes,\n24\nPublished in Transactions on Machine Learning Research (12/2023)\n10\u22121\n100\n101\n102\nSize after dedup, GB\n0\n5\n10\n15\n20\n25\n30\n35\nPass@1 (%)\n200B\n400B\n600B\n800B\n1000B\n200B\n400B\n600B\n800B\n1000B\nTraining tokens\ncpp\nc-sharp\nd\ngo\njava\njulia\njavascript\nlua\nphp\nperl\npython\nr\nruby\nracket\nrust\nscala\nbash\nswift\ntypescript\nFigure 2: Performance (pass@1) of StarCoderBase at several training checkpoints by data size (left) and by\nprogramming language (right). The lines in the left plot are a linear fit between pass@1 and log-dataset-size\nfor all the points except the leftmost one, where we expect the linear dependence to break due to transfer\nlearning (dashed line). The goodness of fit ranges between R2 = 0.399 for the 600B checkpoint to R2 = 0.510\nfor the 1000B checkpoint.\ne.g., not taking the absolute value when computing GCD, not converting a string to a character array, or not\nchecking edge cases.\n6.4\nPerplexity With Long Contexts\nStarCoderBase was trained with an 8K token window, allowing conditioning on and generating long code\nfiles. To evaluate the ability of the model to benefit from this larger context, we compare its perplexity (Bahl\net al., 1983) when using a full window size of 8K tokens versus a window size of 2K tokens (as used in many\nprior code models).\nTo ensure no overlap between the training data for StarCoderBase and the perplexity computation data, we\ndownloaded 10 GNU Public License (GPL) repositories from GitHub in each of the languages in Table 21.\nWe compiled all files from the repositories into a single document for each language. We then divided these\ndocuments into 8K token chunks and computed perplexity on the last 1K tokens in each chunk14 in two\nconditions: (1) the model window only contains the final 2K tokens in the chunk (i.e., the 1K being predicted\nand the previous 1K), and (2) the model window contains all 8K tokens in the chunk (i.e., the 1K tokens\nbeing predicted and the previous 7K). This evaluates the ability of the model to benefit from additional\nfile- and repo-level context when predicting code. In Table 21, we report the average perplexity of the 1K\ntoken regions across all chunks. We see that StarCoderBase indeed benefits from the extra token conditioning\nafforded by its 8K context window, with substantially lower perplexities across all languages.\n7\nNatural Language Evaluation\nAlthough the StarCoder models are principally developed to be Code LLMs, they have also been trained\non a significant amount of natural language text. Roughly 20% of its training tokens are natural language\ndata: 7% GitHub issues, 10% Markdown, 2% Jupyter notebooks, and 4% HTML. In this section, we evaluate\n14We evaluate perplexity on the final 1K tokens in each 8K chunk so that both conditions have the same evaluation tokens,\nand to avoid overly penalizing the 2K condition, as tokens at the beginning of a window tend to have higher perplexity as there\nis less context available to predict them.\n25\nPublished in Transactions on Machine Learning Research (12/2023)\nWindow Size\nLanguage\ncpp\nc-sharp\nc\ngo\njava\njavascript\nphp\nr\nruby\nrust\n2K tokens\n2.01\n1.90\n1.71\n1.35\n1.65\n1.98\n1.73\n1.72\n2.16\n1.84\n8K tokens\n1.79\n1.66\n1.61\n1.21\n1.54\n1.68\n1.43\n1.48\n2.02\n1.65\nTable 21: Perplexity of StarCoderBase on evaluation regions (of size 1K tokens) when using a window size\nof 2K or 8K tokens across repositories from 10 languages. The larger window size substantially reduces\nperplexity, demonstrating a benefit of StarCoder\u2019s 8K token window.\nModel\nSize\nGSM8K CoT\n+maj1@100\nGSM8K PAL\n+maj1@40\nStarCoderBase\n15.5B\n8.4\n\u2014\n21.5\n31.2\nCodeGen-Multi\n16B\n3.18\n\u2014\n8.6\n15.2\nCodeGen-Mono\n16B\n2.6\n\u2014\n13.1\n22.4\n7B\n11.0\n18.1\n10.5\n16.8\n13B\n17.8\n29.3\n16.9\n28.5\nLLaMA\n33B\n35.6\n53.1\n38.7\n50.3\n65B\n50.9\n69.7\n\u2014\n\u2014\nTable 22: 8-shot accuracy on the GSM8K math-reasoning benchmark. Samples are generated with greedy\ndecoding. maj1@k denotes a majority vote over k generations. For the majority vote, we instead generate\nsamples using nucleus sampling with p = 0.95 and temperature 0.7, following Gao et al. (2022). We use\n\u201c\u2014\u201d when a model was not evaluated on a given metric, or the metric is not supported in Language Model\nEvaluation Harness. The LLaMA CoT numbers are from Touvron et al. (2023).\nStarCoderBase on several natural language tasks: natural language reasoning and understanding tasks that\nmight benefit from the combination of code and text training data; and natural language generation tasks\nthat evaluate the model\u2019s tendencies to produce undesirable text outputs, e.g., in a documentation generation\nor interactive assistant setting.\n7.1\nMath Reasoning\nRecent work has shown that Code LLMs can be effective arithmetic and symbolic reasoners by using a\ntechnique called Program-Aided Language models (PAL; Gao et al., 2022). With PAL, the LLM reads\nthe reasoning problem and generates Python programs as the intermediate reasoning steps, which are then\nexecuted by the Python interpreter to produce the answer. In contrast, the Chain-of-Thought method (CoT;\nWei et al., 2022) prompts the LLM to produce the reasoning steps in natural language before generating the\nanswer.\nWe investigate the reasoning capabilities of StarCoderBase on GSM8K (Cobbe et al., 2021), a set of middle-\nschool math word problems. We compare with the two CodeGen-16B models (Nijkamp et al., 2023) and the\nfamily of LLaMA models (Touvron et al., 2023). The results of our evaluation are presented in Table 22,\nwhere we provide both CoT and PAL results for StarCoderBase and LLaMA.\nIn line with previous results comparing PAL to CoT on Code LLMs (Gao et al., 2022), we find that StarCoder-\nBase performs better with PAL (21.5%) than with CoT (8.4%). StarCoderBase substantially outperforms\nCodeGen-16B-Mono and CodeGen-16B-Multi, which achieve 13.1% and 8.6% with PAL, respectively. These\ndifferences carry over to the setting where majority voting is applied. The difference between CoT and PAL\nis much smaller for the LLaMA models, although we observe that CoT performs slightly better for the 7B\nand 13B LLaMA models. Interestingly, we find that StarCoderBase outperforms LLaMA-13B (17.8%) on\nthis reasoning benchmark. However, its performance still lags behind LLaMA-33B (38.7%).\n26\nPublished in Transactions on Machine Learning Research (12/2023)\nModel\nSize\nMMLU 5-shot\nacc, %\nCodeGen-Multi\n16B\n27.8\nGPT-NeoX\n20B\n32.9\nStarCoder\n15.5B\n33.9\nStarCoderBase\n15.5B\n34.2\nLLaMA\n7B\n35.1\nLLaMA\n13B\n46.9\nTable 23: 5-shot accuracy on the MMLU language understanding benchmark.\nModel\nSize\nCoQA zero-shot\nF1 score\nCodeGen-Multi\n16B\n0.59\nStarCoderBase\n15.5B\n0.67\nStarCoder\n15.5B\n0.67\nLLaMA\n7B\n0.71\nLLaMA\n13B\n0.73\nGPT-NeoX\n20B\n0.73\nTable 24: Zero-shot accuracy on the CoQA question answering challenge.\n7.2\nWorld Knowledge and Reading Comprehension\nMMLU (Hendrycks et al., 2020) is a massive multitask language understanding benchmark, covering multiple-\nchoice questions in 57 knowledge domains, including the humanities, STEM, and social sciences. CoQA (Reddy\net al., 2019) is a large-scale dataset for Conversational Question Answering systems, measuring the model\u2019s\nability to process a text passage and answer a series of interconnected questions. We compare StarCoderBase\nand StarCoder with CodeGen-16B-Multi (Nijkamp et al., 2023), GPT-NeoX (Black et al., 2022), LLaMA-7B,\nand LLaMA-13B (Touvron et al., 2023).\nWe present the 5-shot accuracy for MMLU in Table 23, and the zero-shot F1 scores for CoQA in Table 24. On\nMMLU, StarCoderBase outperforms CodeGen-16B-Multi significantly (34.2% to 27.8%), and even outperforms\nGPT-NeoX by a small margin (32.9%). Nevertheless, both LLaMA models outperform StarCoderBase. On\nCoQA, StarCoderBase performs better than CodeGen-16B-Multi but is outperformed by LLaMA and\nGPT-NeoX.\n7.3\nMeasuring Harmful Generation\nWhen generating open-ended text such as code documentation or technical dialogue, a Code LLM (similarly\nto text-only LLMs) might produce harmful outputs. We compare StarCoderBase to previous Code LLMs on\nbenchmarks that measure social bias and toxicity in model-produced text.15\n7.3.1\nSocial Bias\nRecent work has highlighted that LLMs often capture social biases and stereotypes from their pre-training\ncorpora (Kurita et al., 2019; May et al., 2019; Hutchinson et al., 2020; Meade et al., 2023). To quantify social\nbias within our model, we use StereoSet (Nadeem et al., 2021).\nStereoSet consists of a collection of fill-in-the-blank-style tests for measuring social biases within language\nmodels.16 Each example in StereoSet consists of an incomplete sentence (e.g., our housekeeper is BLANK)\n15Code for the evaluations is available here: https://github.com/McGill-NLP/StarCoderSafetyEval\n16We only evaluate against the intrasentence task in this work.\n27\nPublished in Transactions on Machine Learning Research (12/2023)\nModel\nStereotype Score\nLanguage Model Score\nICAT Score\nGender\nLLaMA-13B\n66.54\n88.09\n58.95\nCodeGen-Multi-16B\n67.34\n86.41\n56.44\nStarCoderBase\n58.76\n86.82\n71.60\nProfession\nLLaMA-13B\n60.95\n86.74\n67.74\nCodeGen-Multi-16B\n60.67\n85.67\n67.38\nStarCoderBase\n53.24\n84.70\n79.21\nRace\nLLaMA-13B\n64.94\n87.97\n61.68\nCodeGen-Multi-16B\n60.58\n88.60\n69.85\nStarCoderBase\n56.48\n86.82\n75.58\nReligion\nLLaMA-13B\n57.95\n90.26\n75.91\nCodeGen-Multi-16B\n56.16\n88.91\n77.96\nStarCoderBase\n55.69\n90.67\n80.36\nOverall\nLLaMA-13B\n63.40\n87.62\n64.14\nCodeGen-Multi-16B\n61.29\n87.25\n67.55\nStarCoderBase\n55.53\n86.18\n76.65\nTable 25: StereoSet intrasentence results for gender, professional, racial, and religious bias. Stereotype scores\nclose to 50% are best. Language modeling scores and ICAT scores close to 100% are best.\nalongside three possible completions. Of these completions, one is stereotypical (e.g., Mexican), another is\nanti-stereotypical (e.g., Italian) and a third is unrelated (e.g., computer). StereoSet defines three metrics: a\nstereotype score, a language modeling score, and an ICAT score. The stereotype score is the percentage of\nexamples for which a model prefers the stereotypical completion for a sentence over the anti-stereotypical\ncompletion. The language modeling score is the percentage of examples for which a model prefers a meaningful\ncompletion (stereotype or anti-stereotype) over an unrelated completion. Finally, Nadeem et al. (2021) define\nan idealized context association test (ICAT) score that combines these two metrics:\nICAT = lms \u00b7 min(ss, 100 \u2212 ss)\n50\n(1)\nwhere lms and ss denote the language model score and stereotype score, respectively.\nWe report StereoSet results for StarCoderBase, alongside LLaMA-13B and CodeGen-Multi-16B, in Table 25.\nAcross all four bias domains, we find StarCoderBase obtains the lowest stereotype scores, but also has\ncompetitive language modeling scores. This suggests that StarCoderBase\u2019s lower stereotype scores are not\nsimply due to worse language modeling (Meade et al., 2022), and also as indicated by the high ICAT score.\nWe also evaluate StarCoderBase against Crowdsourced Stereotype Pairs (CrowS-Pairs; Nangia et al. 2020)\nand refer readers to Table D.4 for results.\n7.3.2\nToxicity\nTo evaluate toxicity in responses generated from our model, we use RealToxicityPrompts (Gehman et al.,\n2020), a collection of sentence-level prompts that often elicit undesirable responses from language models.\nWe generate responses to 10K examples from RealToxicityPrompts using StarCoderBase with a minimum\n28\nPublished in Transactions on Machine Learning Research (12/2023)\nModel\nClassifier\nWord List\nLLaMA-13B\n0.74\n1.43\nCodeGen-Multi-16B\n0.21\n0.82\nStarCoderBase\n0.42\n1.12\nTable 26: RealToxicityPrompts response toxicity results. We report the percentage of responses flagged as\ntoxic using a toxicity classifier and an offensive word list. Lower scores are indicative of less toxic generations.\nModel\nSize\nOpen\nAccess\nSynth.\nReason.\n(AS)\nSynth.\nReason.\n(NL)\nbAbI\nDyck\nGSM8K\nMATH\nMATH\n(CoT)\nLSAT\nLegal\nSupport\ncode-davinci-002\n175B\n54.0\n68.4\n68.6\n80.5\n56.8\n41.0\n43.3\n\u2014\n\u2014\ntext-davinci-003\n175B\n50.2\n73.4\n65.3\n75.1\n50.6\n39.0\n44.9\n23.3\n62.2\nLuminous Supreme\n70B\n31.2\n\u2014\n50.4\n72.9\n11.2\n14.9\n5.7\n21.2\n53.0\nStarCoderBase\n15.5B\n\u2713\n44.0\n21.0\n50.4\n85.4\n8.4\n15.1\n7.0\n19.0\n53.2\nCohere Command\nBeta\n52.4B\n24.3\n24.5\n47.3\n42.1\n13.8\n13.3\n7.5\n22.9\n60.6\nJ1-Jumbo v1\n178B\n26.3\n17.4\n54.3\n44.5\n5.4\n8.9\n3.3\n23.2\n48.4\nJ1-Grande v2 beta\n17B\n28.6\n13.9\n47.0\n61.7\n9.6\n12.7\n6.8\n19.1\n56.2\ncode-cushman-001\n12B\n34.1\n16.4\n48.1\n45.1\n4.9\n9.9\n7.2\n\u2014\n\u2014\nOPT\n175B\n\u2713\n22.5\n24.8\n50.7\n49.4\n4.0\n6.5\n2.6\n22.0\n53.2\nGPT-NeoX\n20B\n\u2713\n20.4\n16.7\n46.8\n74.7\n5.3\n14.1\n7.1\n19.1\n51.5\nBLOOM\n176B\n\u2713\n30.4\n19.7\n44.7\n54.5\n9.5\n4.3\n5.5\n20.9\n54.3\nGLM\n130B\n\u2713\n25.2\n25.4\n44.3\n54.9\n6.1\n0\n5.9\n19.3\n45.1\nUL2\n20B\n\u2713\n20.5\n21.7\n50.1\n14.0\n2.4\n0\n0\n20.7\n50.6\nOPT\n66B\n\u2713\n19.3\n21.3\n40.8\n47.1\n1.8\n4.8\n2.9\n17.5\n52.7\nYaLM\n100B\n\u2713\n5.6\n6.1\n34.6\n63.3\n0\n0\n0\n2.3\n48.4\nT5\n11B\n\u2713\n19.6\n10.1\n41.2\n34.7\n2.3\n0\n0\n15.9\n55.8\nTable 27: Model results on natural language reasoning tasks in the HELM benchmark, with models ordered\nby their average rank on the tasks. We use \u201c\u2014\u201d when a model was not evaluated on a given metric, or has\nruntime errors logged in HELM (e.g., \u201cunmapped prediction\u201d for the code-davinci-002 and code-cushman-001\nmodels on LSAT and Legal Support). StarCoder generally substantially outperforms other open-access\nmodels, and often outperforms much larger models.\nlength of one token and a maximum length of 128 tokens. We use nucleus sampling (Holtzman et al., 2020)\nwith p = 0.95 to generate all of our responses.\nWe use two methods for automatically evaluating toxicity in responses: (i) a RoBERTa-based (Liu et al.,\n2019) toxicity classifier (Vidgen et al., 2021) and (ii) a list of potentially offensive words.17 For the toxicity\ndetector, we report the percentage of responses flagged toxic using a threshold of 0.5. For the offensive word\nlist, we report the percentage of responses which contain an offensive word. We note that while the offensive\nword list can potentially falsely flag responses, it may provide a crude measure of blatant toxicity. We report\nour results in Table 26.\nIn general, we observe that CodeGen-16B-Multi and StarCoderBase both appear to generate less toxic\nresponses than LLaMA-13B. For instance, 1.43% of LLaMA-13B\u2019s responses contain potentially offensive\ntokens compared to the 1.12% of StarCoderBase. We also note that CodeGen-16B-Multi appears to generate\nless toxic responses than StarCoderBase.\n7.4\nReasoning Tasks in HELM\nWe evaluate StarCoderBase with HELM (Liang et al., 2022), an evaluation suite aiming to increase the\ntransparency of LLMs by reporting their performance on a wide range of tasks. We evaluate the ability of\nthe model to leverage its natural language and code pretraining for natural language reasoning tasks from\nHELM (excluding code tasks, because of our own extensive code evaluations). At the time of writing, the\nHELM benchmark does not include the CodeGen, CodeGeex, and LLaMA models. Therefore, we compare\n17https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n29\nPublished in Transactions on Machine Learning Research (12/2023)\nStarCoderBase with the largest and/or most recent model from each family of \u201climited\u201d or \u201copen\u201d access\nmodels, as classified on the HELM model list,18 that had been evaluated on a majority of these HELM\nreasoning tasks as of May 1, 2023. In Table 27 we report the results. We compute each model\u2019s ranking\non each task, and order models in the table by their average ranking across tasks. StarCoderBase generally\nobtains substantially stronger performance than all other models with released weights and often performs\ncomparably to or better than much larger models. We speculate that the mixture of code and natural\nlanguage in the training data contributes to the model\u2019s strong performance on these reasoning tasks.\n8\nQualitative Evaluation\nIn Appendix E, we highlight several interesting interactions we had with StarCoderBase. We hope these\nserve as a starting point for researchers and developers interested in further exploring the model\u2019s capabilities.\nWe provide examples of how to elicit interesting model behavior using the templates for Git commits, GitHub\nissues, and Jupyter notebooks in Section E.1. In Section E.2, we demonstrate how to prompt StarCoder to act\nas a technical assistant without any instruction-tuning. In Section E.3 we find that it is also possible to prompt\nthe model using a combination of meta-data and natural language to obtain higher pass@1 performance on\nthe HumanEval benchmark.\n9\nAttribution Tools\nAs generative language tools become more ubiquitous and data-intensive, the need to understand and inspect\nthe massive amounts of text they were trained on becomes more pressing, both to understand the failure\nmodes of models as well as provide transparent data governance feedback in the form of attribution tracing and\nprovenance management of a model\u2019s generated output. This pressing need for understanding data (Mitchell\net al., 2022) is being increasingly recognized and operationalized in the form of dataset inspection tools and\ntoolkits (Akiki et al., 2023; Marone & Van Durme, 2023; Piktus et al., 2023). It is from this vantage point\nthat we are releasing two such data inspection tools: a membership-checking tool and a BM25 search index.\nThese complement the existing \u201cAm I in The Stack\u201d tool which operates at the level of GitHub repository\nnames. The two new tools index only the files used for training and allow for matches on file content. These\ntools are available as standalone sites but are also integrated into our VSCode demo. This helps users identify\nparts of the model output that may have been copied from the training data. By utilizing the search index,\nusers can locate the corresponding source file and repository of the copied snippets.\n9.1\nMembership Checking\nMarone & Van Durme (2023) propose documenting datasets with membership testing artifacts deemed Data\nPortraits. They provide one specific implementation, based on Bloom Filters (Bloom, 1970), that offers\nfast and lightweight membership inference. We build a Bloom-filter-based portrait on strings of length 50\ncharacters from the training data. This artifact takes 26 GB, \u223c 3% of the data size. The inference tool is\nhosted publicly to complement other documentation artifacts. 19\nGenerations from the model can be quickly checked to approximately assess the degree of overlap with\nthe training corpus. The VSCode extension supports using this as a rapid, first-pass attribution method.\nHowever, this requires that matching strings are longer than a minimum size and does not attempt to filter\ncommon or generic code snippets. After the first pass check, users can use the full search index to further\nassess attribution.\n9.2\nSearch Index\nWe index the training dataset using Elasticsearch 7.1720 and provide two search tools to query it: one focused\non the Python subset and one covering the entire dataset. The code itself is preprocessed using a lowercase\n18https://crfm.stanford.edu/helm/latest/?models=1\n19http://stack.dataportraits.org/\n20https://www.elastic.co/guide/en/elasticsearch/reference/7.17\n30\nPublished in Transactions on Machine Learning Research (12/2023)\nfilter and Lucene\u2019s ASCIIFoldingFilter, tokenized using a 3-gram tokenizer, and indexed using the default\nLucene implementation of BM25 as a similarity function. We further index the username and license fields as\nkeyword fields allowing for easy filtering and lookup based on these specific metadata fields. Both indexes are\ncurrently running in single-node mode on one virtual machine.\n10\nSocial Impact and Limitations\n10.1\nProject approach\nOpen-science and open-governance\nStarCoder is an output of a community research project. The\nproject is conducted in the spirit of Open Science (Woelfle et al., 2011), focused on the responsible development\nand use of Code LLMs. Through open-governance practices conducted throughout the project, priority in\ndecision-making has always yielded to the more responsible option even if this meant introducing limitations\nthat might impact adoption or future research. For example, the Legal, Ethics, Governance Working Group\ndecided to remove and not release a dataset of identified malicious code, even though this data might be\nuseful for future security research.\nOpenness and safety risks\nSolaiman (2023) explains how the degree of openness in the LLM development\nprocess is connected to the potential risks associated with a model release. When systems are developed in a\nfully closed manner, it is more likely for power to become concentrated among high-resourced organizations,\nand the small development team may not fully comprehend the impact and long-term consequences of the\nmodel being deployed. In addition, closed-development systems are often less auditable by external experts\nand can impede scientific progress since researchers cannot build upon each other\u2019s work. On the other hand,\nfully open development allows for community research, democratizes access to the models, and enables audits\nthroughout the whole development process. However, without appropriate guardrails, open LLM development\nposes a higher risk of misuse, as increased model access also increases the likelihood of harm caused by the\nmodel. Even though a released API can be shut down, once the model weights are released, it is nearly\nimpossible to retract them. Discussing and implementing responsible AI practices has, therefore, been front\nand center during the development of our project\u2019s LLMs.\n10.2\nLimitations\nDataset and data licensing\nStarCoder was trained on a subset of The Stack v1.2 dataset. This dataset\nhas been filtered using a license detector to only include permissively licensed source code. Nevertheless, the\nlicense detector might have incorrectly classified a number of repositories. See Kocetkov et al. (2022) for\nmore details on this license detection process.\nOpt-out process\nAlthough The Stack offers a way to remove developer code, its opt-out process only\napplies to individual repositories and could benefit from further enhancements. For example, when code\nis licensed under a permissive or copy-left license, it can be duplicated to another repository, making it\nchallenging to eliminate such copies if the copyright owner chooses to opt out. More work is necessary to\ncreate better data control and consent mechanisms for large-scale training sets of LLMs.\nPII detection\nDespite our best efforts to remove PII (Section 4), StarCoder may still produce PII (however,\nnote that the model license restricts use that aims to generate or disseminate PII with the purpose of harming\nothers). As mentioned in Section 4.2, we trained an encoder-only model to detect PII for both code- and\ntext-related tasks and noted that there is a possibility of false positives and negatives, which could lead to\nunintended consequences when processing sensitive data. Moreover, the PII detection model\u2019s performance\nmay vary across different data types and programming languages, necessitating further validation and fine-\ntuning for specific use cases. The PII annotations are only available to approved individuals, and researchers\nand developers who are granted access are expected to uphold ethical standards and data protection measures.\nBy making it accessible, our aim is to encourage further research and development of PII redaction technology.\n31\nPublished in Transactions on Machine Learning Research (12/2023)\nMalicious code\nOn the Hugging Face platform, where the Stack is hosted, a malicious code detection\ntool identified 654 files as unsafe. With the help of our community, we removed these files ahead of the\nrelease of The Stack v1.2. Nevertheless, The Stack may contain undetected malicious code, and StarCoder\nmight be able to generate malware. The StarCoder OpenRAIL-M license, therefore, includes a use restriction\nagainst generating and/or disseminating malware (including \u2014 but not limited to \u2014 ransomware) or any\nother content that can be used to harm electronic systems.\nModel limitations\nStarCoder is subject to typical limitations of LLMs, including the potential to generate\ncontent that is inaccurate, offensive, misleading, discriminatory towards age or gender, or reinforces other\nstereotypes. Please refer to Section 7.3 for an investigation into such safety concerns. Deployments of\nStarCoder need to further challenge and adapt the model to prevent such behavior, e.g., through red-\nteaming (Perez et al., 2022), adversarial testing (Wan et al., 2023), and/or by adding a robust safety\nlayer (OpenAI, 2023b). The model is released with an OpenRAIL-M license that places enforceable use\nrestrictions that apply to the model and its modifications, and to applications using the model.\nEnglish-only evaluations\nWe evaluated the performance of StarCoder solely on English-based benchmarks\nto understand its coding capabilities and natural language understanding. To make these models more\naccessible to a wider audience, future research should investigate the performance and limitations of Code\nLLMs on other natural languages.\nCode attribution tools\nThe StarCoder membership-checking tool and BM25 search index are limited\nto dataset inspection against the subset of The Stack that was used for training and, as such, will not\nfind matches to code that was not included or that was removed from the dataset for this project. The\nPortraits-based membership testing tool uses hash matching and thus may have false positives. It also has a\nminimum resolution and requires a certain amount of context to trigger a match. Both attribution tools do\nnot attempt to distinguish between generic code (e.g., boilerplate) or protected content. However, we hope\nthat these tools will support ongoing research on the responsible development of LLMs.\n10.3\nSocial impact\nCode LLMs\nWe expect Code LLMs to enable people from diverse backgrounds to learn to write higher-\nquality code and develop low-code applications (Leinonen et al., 2023). Mission-critical software could become\neasier to maintain as professional developers are guided by code-generating systems on how to write more\nrobust and efficient code. However, the security implications should also be carefully considered (Sandoval\net al., 2023). While the social impact is intended to be positive, the increased accessibility of Code LLMs\ncomes with certain risks such as over-reliance on the generated code and long-term effects on the software\ndevelopment job market. We refer the reader to Chen et al. (2021, Section 7) for a broader impact analysis\nof Code LLMs, as well as Khlaaf et al. (2022) for an in-depth risk assessment and hazard analysis of this\nemerging technology.\nData annotation\nIt was important for the project to only use reputable data annotation services. It\nwas also important to balance the constraints of costs (fair compensation), time (the timing and time to\ncomplete the work were on the critical path for the project), and quality (to ensure that PII Detection\nModel training was not impacted). While traditional data annotation services using salaried employees were\nconsidered, the decision to work with Toloka crowd-workers was taken after a review of service providers and\ntheir compensation practices \u2014 most would not provide sufficient transparency and guarantees about worker\ncompensation. Our determination of compensation took into consideration different minimum wage rates\nacross countries and their corresponding purchasing power. We limited annotation eligibility to countries\nwhere the hourly pay rate of $7.30 was equivalent to the highest minimum wage in the US ($16.50) in terms\nof purchasing power parity.\nFeedback opt-out form\nDuring the first stage of the opt-out process, individuals were asked to specify\nthe reasons for wanting their code to be excluded from the dataset. The recurring concerns we heard from\nthe individual who wished to opt out are:\n32\nPublished in Transactions on Machine Learning Research (12/2023)\n\u2022 Preference for an opt-in approach instead of opt-out.\n\u2022 Perception that it is unfair to use their code without compensation\n\u2022 Concerns about the current limitations of AI and the potential for model generations to be traced\nback to their work, resulting in potential legal liability.\n\u2022 Belief that their code is of poor quality and unsuitable for AI training.\n\u2022 Presence of PII in their code, which they do not wish to be publicly exposed.\nThe opt-out form thus provided an opportunity to directly engage with content creators and learn about the\nimpact of our work on them.\nCommunity feedback on opt-out process\nWe conducted community research with individuals at\nspecific organizations whose data is used in The Stack (The Alan Turing Institute and The Turing Way) and\ncontributed to two open, international workshops (Open Data Day 2023 and Mozilla Festival 2023 with a\nsession titled \u2018Designing for Data Rights in the AI Production Pipeline\u2019). These qualitative interviews and\nparticipatory co-design workshops included 50 participants, primarily from North America and Europe, with\nroles including research scientist, community manager, software engineer, and principal investigator (PI).\nThe outcomes from the community research can be summarized as follows: when it comes to governance of\nLLM datasets, participants feel that it is both better to know and better to have a choice. Most participants\nhad neutral to positive feelings about their permissively licensed data being used to train LLMs. While all\nhad positive impressions of the \u201cAm I in The Stack\u201d tool, not one interviewed expressed a desire to actually\nopt out. The main takeaway seemed to be that participants found the most value in the project\u2019s governance\ntools for their ability to raise awareness of data practices and to empower individuals and communities to\ntake action based on their specific needs. These initial conversations also highlighted the importance of\nbringing governance discussions and decisions directly to impacted communities, an important direction of\nfuture work that should extend community research beyond North America and Europe. Participants in the\nworkshops also raised examples of new groups to center in data rights considerations, including artists, data\nminers, and future generations. The co-created outputs can be viewed on this MozFest Miro Board.\n11\nConclusion\nIn this technical report, we described the efforts of the BigCode community in creating StarCoderBase and\nStarCoder, open-access 15.5B parameter large language models trained on code. We provided full transparency\non all aspects of the research and development process, including the training data, the data curation process,\nthe PII redaction pipeline, and the model training. We conducted the most extensive evaluation of Code\nLLMs to date, finding that StarCoder outperforms other Code LLMs like CodeGen (Nijkamp et al., 2023)\nand CodeGeeX (Zheng et al., 2023), and matches or outperforms the closed-access code-cushman-001 model\nfrom OpenAI. By releasing the StarCoder models with an Open Responsible AI Model license, and by open-\nsourcing all code repositories for building the model on GitHub, we aim to increase access, reproducibility,\nand transparency of Code LLMs in the research and developer communities. The model license includes\nuse restrictions to ensure that modifications of the model and applications using the model adhere to our\nprinciples of responsible AI. In addition, we released a novel set of attribution tools to help end-users of\nCode LLMs to detect and locate model generations that may have been copied from the training set. We\nhope these measures contribute towards a safe model release, ensuring that the strong-performing StarCoder\nmodels remain a force for good.\nAcknowledgements\nWe would thank Hugging Face for providing the compute resources to train the\nStarCoder models. We also thank Suriya Gunasekar for help with the data inspection, and Sebastien\nPaquet for proofreading this work. Carolyn Jane Anderson, Arjun Guha, Ming-Ho Yee, and Yangtian Zi\nand are supported by U.S. National Science Foundation awards SES-2326174 and CCF-2102288. Evgenii\nZheltonozhskii is supported by the Adams Fellowships Program of the Israel Academy of Sciences and\nHumanities.\n33\nPublished in Transactions on Machine Learning Research (12/2023)\nReferences\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-training for program un-\nderstanding and generation. In Proceedings of NAACL, 2021. URL https://aclanthology.org/2021.naacl-\nmain.211. (cited on p. 4)\nChristopher Akiki, Giada Pistilli, Margot Mieskes, Matthias Gall\u00e9, Thomas Wolf, Suzana Ilic, and Yacine\nJernite. BigScience: a case study in the social construction of a multilingual large language model. CoRR,\nabs/2212.04960, 2022. doi: 10.48550/arXiv.2212.04960. URL https://doi.org/10.48550/arXiv.2212.04960.\n(cited on p. 2)\nChristopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang, Akintunde Oladipo, Jimmy Lin,\nand Martin Potthast. Spacerini: Plug-and-play search engines with Pyserini and Hugging Face. CoRR,\nabs/2302.14534, 2023. doi: 10.48550/arXiv.2302.14534. URL https://doi.org/10.48550/arXiv.2302.14534.\n(cited on p. 30)\nAndersen et al v. Stability AI et al. 3:23-cv-00201 N.D. Cal. 2023. (cited on p. 2)\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson\nKernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish,\nChris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment. arXiv preprint\narXiv:2112.00861, 2021. (cited on p. 48)\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732, 2021. (cited on pp. 3, 4, 17, 18, and 21)\nLalit Bahl, Frederick Jelinek, and Robert Mercer. A maximum likelihood approach to continuous speech\nrecognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, PAMI-5:179 \u2013 190, 04\n1983. doi: 10.1109/TPAMI.1983.4767370. (cited on p. 25)\nMohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and\nMark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255,\n2022. doi: 10.48550/ARXIV.2207.14255. URL https://arxiv.org/abs/2207.14255. (cited on pp. 3, 15, 16,\nand 23)\nBBC. ChatGPT accessible again in Italy. https://www.bbc.com/news/technology-65431914, 2023. (cited on\np. 2)\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro Von Werra. A\nframework for the evaluation of code generation models. https://github.com/bigcode-project/bigcode-\nevaluation-harness, December 2022. (cited on pp. 3 and 17)\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis,\nNiklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane\nAnderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel\nRomero, Michael Lappert, Francesco De Toni, Bernardo Garc\u00eda del R\u00edo, Qian Liu, Shamik Bose, Urvashi\nBhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky,\nHuu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes,\nDaniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. SantaCoder: don\u2019t reach for the stars!\nIn Deep Learning for Code Workshop (DL4C), 2023. (cited on pp. 4, 9, 11, 12, 13, 14, 16, and 23)\nYoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent.\nA neural probabilistic language model.\nIn\nT. Leen, T. Dietterich, and V. Tresp (eds.), Advances in Neural Information Processing Systems,\nvolume 13. MIT Press, 2000.\nURL https://proceedings.neurips.cc/paper_files/paper/2000/hash/\n728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html. (cited on p. 4)\n34\nPublished in Transactions on Machine Learning Research (12/2023)\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and\nscaling. arXiv preprint arXiv:2304.01373, 2023. (cited on p. 2)\nBigScience Workshop. BLOOM (revision 4ab0472), 2022. URL https://huggingface.co/bigscience/bloom.\n(cited on p. 2)\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor\nLeahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria\nReynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: an open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745, 2022. (cited on pp. 2, 4, and 27)\nBurton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422\u2013426,\njul 1970. ISSN 0001-0782. doi: 10.1145/362686.362692. URL https://doi.org/10.1145/362686.362692.\n(cited on p. 30)\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya\nDemszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin\nEthayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna,\nRohith Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258,\n2021. URL https://arxiv.org/abs/2108.07258. (cited on p. 2)\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. Large language models in\nmachine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 858\u2013867, Prague, Czech\nRepublic, June 2007. Association for Computational Linguistics. URL https://aclanthology.org/D07-\n1090. (cited on p. 3)\nAndrei Z. Broder. Identifying and filtering near-duplicate documents. In Annual symposium on combinatorial\npattern matching, pp. 1\u201310. Springer, 2000. (cited on p. 9)\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. arXiv preprint arXiv:2005.14165, 2020. (cited on pp. 2 and 4)\nChristian Buck, Kenneth Heafield, and Bas van Ooyen. N-gram counts and language models from the Common\nCrawl. In Proceedings of the Ninth International Conference on Language Resources and Evaluation\n(LREC\u201914), pp. 3579\u20133584, Reykjavik, Iceland, May 2014. European Language Resources Association\n(ELRA). URL http://www.lrec-conf.org/proceedings/lrec2014/pdf/1097_Paper.pdf. (cited on p. 3)\nMatthew Butterick. This CoPilot is stupid and wants to kill me. https://matthewbutterick.com/chron/\nthis-copilot-is-stupid-and-wants-to-kill-me.html, 2022. (cited on p. 2)\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney,\nMing-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg,\nand Abhinav Jangda. MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation.\nIEEE Transactions on Software Engineering, pp. 1\u201317, 2023. doi: 10.1109/TSE.2023.3267446. URL\nhttps://arxiv.org/abs/2208.08227. (cited on pp. 3, 5, and 21)\n35\nPublished in Transactions on Machine Learning Research (12/2023)\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,\nMichael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,\nMikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,\nFelipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel\nHerbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,\nSuchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati,\nKatie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\nZaremba. Evaluating large language models trained on code. arXiv preprint, 2021. (cited on pp. 2, 3, 4, 17,\n18, 19, 21, and 32)\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar\nPrabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\nIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: scaling language modeling with pathways. CoRR,\nabs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.\n(cited on pp. 2, 4, and 18)\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training\nverifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. (cited on pp. 3 and 26)\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-\nefficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.\n(cited on pp. 3 and 16)\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423. (cited on p. 11)\nDOE 1 v. and GitHub, Inc. 4:22-cv-06823 N.D. Cal. 2022. (cited on p. 2)\nTyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. GPTs are GPTs: An early look at the\nlabor market impact potential of large language models. arXiv preprint arXiv:2303.10130, 2023. (cited on\np. 2)\nEuronews. Microsoft attracting users to its code-writing, generative AI software. https://www.euronews.com/\nnext/2023/01/25/microsoft-results-ai, 2023. (cited on p. 2)\nEuropean Council. The general data protection regulation. https://www.consilium.europa.eu/en/policies/\ndata-protection/data-protection-regulation/, 2018. (cited on p. 2)\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing\nQin, Ting Liu, Daxin Jiang, and Ming Zhou. Codebert: A pre-trained model for programming and\nnatural languages. arXiv preprint arXiv:2002.08155, 2020. doi: 10.48550/ARXIV.2002.08155. URL\nhttps://arxiv.org/abs/2002.08155. (cited on p. 4)\n36\nPublished in Transactions on Machine Learning Research (12/2023)\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau\nYih, Luke Zettlemoyer, and Mike Lewis. InCoder: a generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999, 2022. doi: 10.48550/ARXIV.2204.05999. URL https://arxiv.org/abs/\n2204.05999. (cited on pp. 2, 3, 4, 19, and 23)\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace\nHe, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB dataset of\ndiverse text for language modeling. arXiv preprint arXiv:2101.00027, 2021a. (cited on pp. 2, 4, and 17)\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September\n2021b. URL https://doi.org/10.5281/zenodo.5371628. (cited on p. 17)\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham\nNeubig. PAL: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022. (cited on pp. 26\nand 48)\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts:\nEvaluating Neural Toxic Degeneration in Language Models. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pp. 3356\u20133369, Online, November 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-\nemnlp.301. (cited on pp. 3 and 28)\nKenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. Scalable modified Kneser-\nNey language model estimation.\nIn Proceedings of the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), pp. 690\u2013696, Sofia, Bulgaria, August 2013. Association\nfor Computational Linguistics. URL https://aclanthology.org/P13-2121. (cited on p. 3)\nPeter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A Lemley, and Percy Liang.\nFoundation models and fair use. arXiv preprint arXiv:2303.15715, 2023. (cited on p. 2)\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. (cited on\npp. 3 and 27)\nAbram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness of\nsoftware. In 2012 34th International Conference on Software Engineering (ICSE), pp. 837\u2013847. IEEE, 2012.\n(cited on p. 4)\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\nErich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language\nmodels. arXiv preprint arXiv:2203.15556, 2022. (cited on pp. 2 and 4)\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.\nIn International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=\nrygGQyrFvH. (cited on p. 29)\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. CodeSearchNet\nchallenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019. (cited on\np. 24)\nBen Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl.\nSocial biases in NLP models as barriers for persons with disabilities. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pp. 5491\u20135501, Online, July 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.acl-main.487. URL https://aclanthology.org/\n2020.acl-main.487. (cited on p. 27)\n37\nPublished in Transactions on Machine Learning Research (12/2023)\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of\nlanguage modeling. arXiv preprint arXiv:1602.02410, 2016. (cited on p. 4)\nAditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating contextual\nembedding of source code. In Proceedings of the 37th International Conference on Machine Learning,\nICML\u201920. JMLR.org, 2020. (cited on p. 4)\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361, 2020. (cited on p. 4)\nHeidy Khlaaf, Pamela Mishkin, Joshua Achiam, Gretchen Krueger, and Miles Brundage. A hazard analysis\nframework for code synthesis large language models. arXiv preprint arXiv:2207.14157, 2022. (cited on p.\n32)\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and\nYann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\n(cited on p. 16)\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u00f1oz Ferrandis, Yacine\nJernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and\nHarm de Vries. The Stack: 3 TB of permissively licensed source code. Preprint, 2022. URL https:\n//arxiv.org/abs/2211.15533. (cited on pp. 1, 3, 4, 5, 9, and 31)\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language\nmodels are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. (cited on p. 48)\nBradley M. Kuhn. If software is my copilot, who programmed my software? https://sfconservancy.org/\nblog/2022/feb/03/github-copilot-copyleft-gpl/, 2022. (cited on p. 2)\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. Measuring bias in contextualized\nword representations. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing,\npp. 166\u2013172, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/\nW19-3823. URL https://www.aclweb.org/anthology/W19-3823. (cited on p. 27)\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon\nemissions of machine learning. arXiv preprint arXiv:1910.09700, 2019. (cited on p. 17)\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih,\nDaniel Fried, Sida Wang, and Tao Yu. DS-1000: a natural and reliable benchmark for data science code\ngeneration. ArXiv, abs/2211.11501, 2022. (cited on pp. 3, 17, 18, 19, and 20)\nDong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural\nnetworks. In Workshop on challenges in representation learning, ICML, number 2, pp. 896, 2013. (cited on\np. 13)\nJuho Leinonen, Paul Denny, Stephen MacNeil, Sami Sarsa, Seth Bernstein, Joanne Kim, Andrew Tran, and\nArto Hellas. Comparing code explanations created by students and large language models, 2023. (cited on\np. 32)\nMark A Lemley and Bryan Casey.\nFair learning.\nTex. L. Rev., 99:743, 2020.\nURL https://\ntexaslawreview.org/fair-learning/. (cited on p. 2)\nAmanda Levendowski. How copyright law can fix artificial intelligence\u2019s implicit bias problem. Wash. L. Rev.,\n93:579, 2018. (cited on p. 2)\n38\nPublished in Transactions on Machine Learning Research (12/2023)\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James\nKeeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\u2019Autume, Igor\nBabuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy,\nDaniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and\nOriol Vinyals. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022.\n(cited on p. 4)\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,\nDeepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv\npreprint arXiv:2211.09110, 2022. (cited on pp. 3 and 29)\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. RoBERTa: a robustly optimized BERT pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019. (cited on pp. 11 and 29)\nNatasha Lomas.\nUnpicking the rules shaping generative AI.\nhttps://techcrunch.com/2023/04/13/\ngenerative-ai-gdpr-enforcement/, 2022. (cited on p. 2)\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano,\nMing Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu.\nCodeXGLUE: A machine learning benchmark dataset for code understanding and generation. arXiv\npreprint arXiv:2102.04664, 2021. (cited on p. 24)\nMarc Marone and Benjamin Van Durme. Data portraits: Recording foundation model training data. CoRR,\nabs/2303.03919, 2023. doi: 10.48550/arXiv.2303.03919. URL https://doi.org/10.48550/arXiv.2303.03919.\n(cited on p. 30)\nChandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. On measuring social\nbiases in sentence encoders. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pp. 622\u2013628, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1063. URL https://www.aclweb.org/anthology/N19-1063. (cited on p. 27)\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy. An empirical survey of the effectiveness of debiasing\ntechniques for pre-trained language models. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 1878\u20131898, Dublin, Ireland, May 2022. Association\nfor Computational Linguistics. doi: 10.18653/v1/2022.acl-long.132. URL https://aclanthology.org/\n2022.acl-long.132. (cited on p. 28)\nNicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di Jin, Siva Reddy, Yang Liu,\nand Dilek Hakkani-T\u00fcr. Using in-context learning to improve dialogue safety, February 2023. URL\nhttp://arxiv.org/abs/2302.00871. arXiv:2302.00871 [cs]. (cited on p. 27)\nTom\u00e1s Mikolov, Martin Karafi\u00e1t, Luk\u00e1s Burget, Jan Cernock\u00fd, and Sanjeev Khudanpur. Recurrent neural\nnetwork based language model. In Takao Kobayashi, Keikichi Hirose, and Satoshi Nakamura (eds.),\nINTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,\nMakuhari, Chiba, Japan, September 26-30, 2010, pp. 1045\u20131048. ISCA, 2010. URL http://www.isca-\nspeech.org/archive/interspeech_2010/i10_1045.html. (cited on p. 4)\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In danah\nboyd and Jamie H. Morgenstern (eds.), Proceedings of the Conference on Fairness, Accountability, and\nTransparency, FAT* 2019, Atlanta, GA, USA, January 29-31, 2019, pp. 220\u2013229. ACM, 2019.\ndoi:\n10.1145/3287560.3287596. URL https://doi.org/10.1145/3287560.3287596. (cited on p. 3)\nMargaret Mitchell, Alexandra Sasha Luccioni, Nathan Lambert, Marissa Gerchick, Angelina McMillan-\nMajor, Ezinwanne Ozoani, Nazneen Rajani, Tristan Thrush, Yacine Jernite, and Douwe Kiela. Measuring\n39\nPublished in Transactions on Machine Learning Research (12/2023)\ndata. CoRR, abs/2212.05129, 2022. doi: 10.48550/arXiv.2212.05129. URL https://doi.org/10.48550/\narXiv.2212.05129. (cited on p. 30)\nAnthony MOI, Nicolas Patry, Pierric Cistac, Pete, Funtowicz Morgan, Sebastian P\u00fctz, Mishig, Bjarte Johansen,\nThomas Wolf, Sylvain Gugger, Clement, Julien Chaumond, Lysandre Debut, Fran\u00e7ois Garillot, Luc Georges,\ndctelus, JC Louis, MarcusGrass, Taufiquzzaman Peyash, 0xflotus, Alan deLevie, Alexander Mamaev,\nArthur, Cameron, Colin Clement, Dagmawi Moges, David Hewitt, Denis Zolotukhin, and Geoffrey Thomas.\nhuggingface/tokenizers: Rust 0.13.2, November 2022. URL https://doi.org/10.5281/zenodo.7298413.\n(cited on p. 16)\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri\nAji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel.\nCrosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022. (cited on\np. 2)\nMoin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring stereotypical bias in pretrained language\nmodels. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5356\u2013\n5371, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.416.\nURL https://aclanthology.org/2021.acl-long.416. (cited on pp. 3, 27, and 28)\nNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-Pairs: a challenge dataset\nfor measuring social biases in masked language models. arXiv:2010.00133 [cs], September 2020. URL\nhttp://arxiv.org/abs/2010.00133. arXiv: 2010.00133. (cited on p. 28)\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. CodeGen: an open large language model for code with multi-turn program synthesis. In The Eleventh\nInternational Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=\niaYcJKpY2B_. (cited on pp. 2, 3, 4, 17, 26, 27, and 33)\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario\nAmodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and\ninduction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-\nlearning-and-induction-heads/index.html. (cited on p. 2)\nOpenAI. GPT-4 technical report. arXiv preprint arXiv:2009.03300, 2023a. (cited on pp. 2 and 4)\nOpenAI. GPT-4 system card. https://cdn.openai.com/papers/gpt-4-system-card.pdf, 2023b. (cited on p.\n32)\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation\nof machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational\nLinguistics, pp. 311\u2013318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational\nLinguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040. (cited on p. 24)\nHammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. Asleep at the\nkeyboard? Assessing the security of GitHub Copilot\u2019s code contributions. In IEEE Symposium on Security\nand Privacy, San Francisco, CA, 2022. URL https://arxiv.org/abs/2108.09293. (cited on pp. 3 and 22)\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat\nMcAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint\narXiv:2202.03286, 2022. (cited on pp. 2 and 32)\nAleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Lauren\u00e7on, G\u00e9rard Dupont, Alexandra Sasha\nLuccioni, Yacine Jernite, and Anna Rogers. The ROOTS search tool: Data transparency for LLMs. CoRR,\n40\nPublished in Transactions on Machine Learning Research (12/2023)\nabs/2302.14035, 2023. doi: 10.48550/arXiv.2302.14035. URL https://doi.org/10.48550/arXiv.2302.14035.\n(cited on p. 30)\nMichael Pradel, Georgios Gousios, Jason Liu, and Satish Chandra. TypeWriter: Neural Type Prediction\nwith Search-Based Validation. In ACM Joint Meeting on European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering, 2020. doi: 10.1145/3368089.3409715. (cited on p.\n23)\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. (cited on p. 4)\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob\nMenick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh,\nPo-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato,\nJohn Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar,\nElena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre,\nLena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic\nDonato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien\nde Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego\nde Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura\nWeidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey\nIrving. Scaling language models: Methods, analysis & insights from training Gopher. arXiv preprint\narXiv:2112.11446, 2021. (cited on p. 4)\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020. (cited on pp. 2 and 4)\nSiva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering challenge.\nTransactions of the Association for Computational Linguistics, 7:249\u2013266, 2019. doi: 10.1162/tacl_a_00266.\nURL https://aclanthology.org/Q19-1016. (cited on pp. 3 and 27)\nJohn A. Rothchild and Daniel Rothchild. Copyright implications of the use of code repositories to train\na machine learning model. https://www.fsf.org/licensing/copilot/copyright-implications-of-the-\nuse-of-code-repositories-to-train-a-machine-learning-model, 2022. (cited on p. 2)\nGustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Siddharth Garg, and Brendan Dolan-Gavitt.\nLost at C: A user study on the security implications of large language model code assistants, 2023. (cited\non p. 32)\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9,\nAlexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. BLOOM: a 176B-parameter open-access\nmultilingual language model. arXiv preprint arXiv:2211.05100, 2022. (cited on p. 2)\nNoam Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019.\nURL http://arxiv.org/abs/1911.02150. (cited on pp. 3 and 16)\nArfon Smith.\nKernel description.\nhttps://github.blog/2016-06-29-making-open-source-data-more-\navailable/, 2016. (cited on p. 17)\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,\nZhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yaz-\ndani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh\nTiwary, and Bryan Catanzaro. Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a\nlarge-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. (cited on p. 4)\n41\nPublished in Transactions on Machine Learning Research (12/2023)\nIrene Solaiman.\nThe gradient of generative AI release: Methods and considerations.\narXiv preprint\narXiv:2302.04844, 2023. (cited on pp. 2 and 31)\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nNeil Houlsby, and Donald Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131,\n2022. (cited on pp. 2 and 4)\nClive Thompson. How an ai became my code-writing genie, Mar 2022. URL https://www.wired.com/story/\nopenai-copilot-autocomplete-for-code/. (cited on p. 2)\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin\nGhafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,\nYuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching\nChang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-\nHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben\nZevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina,\nErin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya\nKuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,\nMarian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications. arXiv preprint\narXiv:2201.08239, 2022. (cited on pp. 4 and 18)\nJulian Togelius and Georgios N. Yannakakis. Choose your weapon: Survival strategies for depressed AI\nacademics. arXiv preprint arXiv:2304.06035, 2023. (cited on p. 2)\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. LLaMA: open and efficient foundation language models. arXiv preprint\narXiv:2302.13971, 2023. (cited on pp. 2, 4, 18, 26, and 27)\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp.\n5998\u20136008, 2017. (cited on p. 4)\nBertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. Learning from the worst: Dynamically\ngenerated datasets to improve online hate detection. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pp. 1667\u20131682, Online, August 2021. Association for Computational\nLinguistics.\ndoi: 10.18653/v1/2021.acl-long.132.\nURL https://aclanthology.org/2021.acl-long.132.\n(cited on p. 29)\nAlexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during instruction\ntuning, 2023. (cited on p. 32)\nBen Wang and Aran Komatsuzaki. GPT-J-6B: a 6 billion parameter autoregressive language model, 2021.\n(cited on pp. 2 and 4)\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. CodeT5: Identifier-aware unified pre-trained\nencoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, pp. 8696\u20138708, Online and Punta Cana, Dominican\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-\nmain.685. URL https://aclanthology.org/2021.emnlp-main.685. (cited on p. 4)\nZhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. Execution-based evaluation for open-domain\ncode generation. arXiv preprint arXiv:2212.10481, 2022. (cited on p. 20)\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh,\n42\nPublished in Transactions on Machine Learning Research (12/2023)\nAlekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing\nSystems, 2022. URL https://openreview.net/forum?id=_VjQlMeSB_J. (cited on pp. 26 and 48)\nMichael Woelfle, Piero Olliaro, and Matthew H. Todd. Open science is a research accelerator. Nature\nChemistry, 3(10):745\u2013748, October 2011. ISSN 1755-4349. doi: 10.1038/nchem.1149. (cited on p. 31)\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations, pp. 38\u201345, Online, October 2020. Association for Computational Linguistics.\nURL\nhttps://www.aclweb.org/anthology/2020.emnlp-demos.6. (cited on p. 18)\nWorld\nEconomic\nForum.\nFuture\nof\njobs\nreport.\nhttps://www3.weforum.org/docs/\nWEF_Future_of_Jobs_2023.pdf, 2023. (cited on p. 2)\nFrank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of large\nlanguage models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine\nProgramming, MAPS 2022, pp. 1\u201310, New York, NY, USA, 2022. Association for Computing Machinery.\nISBN 9781450392730. doi: 10.1145/3520312.3534862. URL https://doi.org/10.1145/3520312.3534862.\n(cited on p. 4)\nMing-Ho Yee and Arjun Guha. Do machine learning models produce TypeScript types that type check? In\nEuropean Conference on Object-Oriented Programming (ECOOP), 2023. (cited on pp. 3, 23, and 24)\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao\nDong, and Jie Tang. GLM-130B: an open bilingual pre-trained model. arXiv preprint arXiv:2210.02414,\n2022. (cited on p. 4)\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022. (cited on pp. 2 and 4)\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi\nWang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for code generation\nwith multilingual evaluations on HumanEval-X. arXiv preprint arXiv:2303.17568, 2023. doi: 10.48550/\narXiv.2303.17568. (cited on pp. 3, 4, 18, and 33)\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire\nCui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large\nlanguage models. arXiv preprint arXiv:2205.10625, 2022. (cited on p. 48)\n43\nPublished in Transactions on Machine Learning Research (12/2023)\nA\nGitHub issues filtering\nBelow we present the filters and regular expressions we used for the GitHub issues.\n# regexes used for removing automated text\nGITHUB_EMAILS = [\nre.compile(pattern, re.DOTALL)\nfor pattern in [\n\"(.*)From:.+Reply to this email directly.+view it on GitHub(.*)\\n?(.*)\",\n\"(.*)On.+notifications@github.com.+wrote:.+Reply to this email directly.+view it on GitHub(.*)\\n?(.*)\",\n\"(.*)Signed-off-by: .+<.+>(.*?)\\n?(.*)\",\n]\n]\nGITHUB_EMAIL_DATE = re.compile(\"\\d+/\\d+/\\d+ \\d{2}:\\d{2} [AP]M.+wrote\")\nGITHUB_EMAIL_LINEBREAK = re.compile(\"_{20,}\")\n# remove comments from authors in this list\nBOT_AUTHORS = [\n\"Apache-HBase\",\n\"AutorestCI\",\n\"CLAassistant\",\n\"cmsbuild\",\n\"codecov-io\",\n\"codecov-commenter\",\n\"coveralls\",\n\"danger-public\",\n\"dnfclas\",\n\"msftclas\",\n\"PyDocTeur\",\n\"SparkQA\",\n\"karma-pr-reporter\",\n\"danger-public\",\n\"claassistantio\",\n\"probot-stale\",\n]\n# remove comment if author username contains this keyword\nBOT_KEYWORDS = [\"[bot]\", \"botmanager\", \"bors-\", \"jenkins\", \"k8s-\", \"-test-\", \"travis\"]\n# remove comments if author username ends with this suffix\nBOT_SUFFIXES = [\n\"-automaton\",\n\"-automation\",\n\"-benchmark\",\n\"-build\",\n\"-deployer\",\n\"-cloud\",\n\"bot\",\n\"-ci\",\n\"-linter\",\n\"-teamcity\",\n\"-test\",\n\"-testing\",\n\"-Service-Account\",\n]\nListing A.1: GitHub issues filtering\nB\nAnnotator countries\nSee Table B.1.\n44\nPublished in Transactions on Machine Learning Research (12/2023)\nCountries\nAlgeria\nArmenia\nAzerbaijan\nBangladesh\nBelarus\nBenin\nBolivia\nBosnia and Herzegovina\nBrazil\nBulgaria\nColombia\nDominican Republic\nEgypt\nEthiopia\nGhana\nIndia\nIndonesia\nKazakhstan\nKenya\nMadagascar\nMalaysia\nMorocco\nMozambique\nMyanmar\nNigeria\nPhilippines\nRussia\nSenegal\nSerbia\nSri Lanka\nTunisia\nUganda\nUkraine\nUzbekistan\nZambia\nTable B.1: List of countries from which we recruited annotators for the PII labeling effort.\nC\nReplacements for IP addresses\n# List of random private IP addresses we used to mask IP addresses\nREPLACEMENTS_IP = {\n\"IPv4\": [\n\"172.16.31.10\",\n\"172.16.58.3\",\n\"172.16.17.32\",\n\"192.168.127.12\",\n\"192.168.3.11\",\n],\n\"IPv6\": [\n\"fd00:c2b6:b24b:be67:2827:688d:e6a1:6a3b\",\n\"fd00:a516:7c1b:17cd:6d81:2137:bd2a:2c5b\",\n\"fc00:e968:6179::de52:7100\",\n\"fc00:db20:35b:7399::5\",\n\"fdf8:f53e:61e4::18\",\n],\n}\nListing C.1: Replacements for IP addresses\n45\nPublished in Transactions on Machine Learning Research (12/2023)\nD\nAdditional Evaluation Results\nLanguage\nModels (Parameters)\ncode-cushman-001 (12B)\ncode-davinci-002 (175B)\nStarCoderBase (15.5B)\ncpp\n30.59\n48.44\n30.56\nc-sharp\n22.06\n27.47\n20.56\nd\n6.73\n21.71\n10.01\ngo\n19.68\n31.39\n21.47\njava\n31.90\n40.12\n28.53\njulia\n1.54\n35.74\n21.09\njavascript\n31.27\n48.99\n31.70\nlua\n26.24\n40.83\n26.61\nphp\n28.94\n47.40\n26.75\nperl\n19.29\n34.77\n16.32\npython\n30.71\n46.68\n30.35\nr\n10.99\n23.13\n10.18\nruby\n28.63\n42.68\n17.25\nracket\n7.05\n17.60\n11.77\nrust\n25.22\n43.40\n24.46\nscala\n27.62\n43.61\n28.79\nshell\n11.74\n23.24\n11.02\nswift\n22.12\n38.02\n16.74\ntypescript\n31.26\n48.87\n32.15\nTable D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed-\naccess models only available by API. Code-davinci-002 performs best, but its parameter count and inference\ncost significantly exceeds StarCoder and code-cushman-001.\nFormat\nModel\nValid (\u2191)\nInsecure (\u2193)\nCompletion\nStarCoderBase\n855/1000 (85.50%)\n340/855 (39.77%)\nInsertion\nStarCoderBase\n987/1000 (98.70%)\n354/987 (35.87%)\nCompletion\ncode-davinci-002\n984/1000 (98.40%)\n423/984 (42.99%)\nInsertion\ncode-davinci-002\n986/1000 (98.60%)\n421/986 (42.70%)\nTable D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s\ncode-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not\nappear to improve its performance at generating secure code.\n46\nPublished in Transactions on Machine Learning Research (12/2023)\nProblem name\nPass count\n400B\n600B\n800B\n1000B\nHumanEval_0_has_close_elements\n20\n171\n197\n5\nHumanEval_13_greatest_common_divisor\n86\n176\n153\n6\nHumanEval_152_compare\n211\n185\n126\n11\nHumanEval_16_count_distinct_characters\n0\n46\n137\n0\nHumanEval_23_strlen\n105\n60\n200\n6\nHumanEval_33_sort_third\n42\n0\n1\n106\nHumanEval_37_sort_even\n90\n156\n132\n0\nHumanEval_3_below_zero\n190\n154\n0\n129\nHumanEval_43_pairs_sum_to_zero\n0\n34\n119\n7\nHumanEval_46_fib4\n197\n200\n142\n6\nHumanEval_52_below_threshold\n0\n186\n170\n13\nHumanEval_86_anti_shuffle\n0\n0\n118\n1\nHumanEval_97_multiply\n1\n0\n133\n21\nTable D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass\ncounts between the 800B and 1000B checkpoints is 100 or higher.\nSocial Bias\nLLaMA-13B\nCodeGen-16B-Multi\nStarCoder\nRace/Color\n68.99\n61.82\n63.95\nSocioeconomic Status\n68.60\n68.60\n63.37\nGender\n59.16\n54.96\n50.76\nDisability\n81.67\n73.33\n81.67\nNationality\n59.75\n47.17\n57.23\nSexual Orientation\n73.81\n67.86\n72.62\nPhysical Appearance\n71.43\n55.56\n57.14\nReligion\n76.19\n54.29\n74.29\nAge\n72.41\n48.28\n54.02\nOverall\n67.84\n59.08\n61.94\nTable D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain.\nA stereotype score closer to 50% indicates less bias.\n47\nPublished in Transactions on Machine Learning Research (12/2023)\nE\nQualitative Examples\nE.1\nUsing Pretraining Templates\nFor the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with\nsentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use\ncases: with the commit format, we can prompt the model to modify code with a natural language instruction,\nwith the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook\nformat to write code based on natural language description. Since we also train on the output of Jupyter\ncode cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We\ncan force the model to always predict an output by suppressing the empty output token (<empty_output>).\nTable E.1 illustrates uses of pretraining templates.\nE.2\nTechnical Assistant\nIn preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned\nthe model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction-\ntuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow\ninstructions, and help with solving technical problems. We further improved the HHH prompt by including\nmore conversational examples related to the programming domain. We used examples from various sources to\ncreate the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou\net al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al.,\n2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the\nprompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes\nwrong solutions, presents wrong facts, and can make offensive comments.\nE.3\nImproving Code Generation with Prompting\nWe inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several\ncases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code\nhere. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in\npractice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way.\nWe tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these\nprefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of\na file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1\nperformance on HumanEval to 40.8% (from 33.6% unprompted, Table 12):\n<filename>solutions/solution_1.py\n# Here is the correct implementation of the code exercise\nWe also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename\nsince the CodeGen models do not support them). But, we found that this hurts performance, bringing it\ndown to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found\nthat this prompt had a negligible impact with StarCoderBase.\n48\nPublished in Transactions on Machine Learning Research (12/2023)\nDescription\nExamples\nJupyter\nformat\nfor\npredicting\nresults\nModel input:\n<jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print(\nis_prime(3))\nprint(is_prime(4))\nprint(is_prime(29))\nprint(is_prime(33))<jupyter_output>\nModel output:\nTrue\nFalse\nTrue\nFalse\nModel input:\n<jupyter_code>numbers = [1, 9, 8, 3, 27]\nprint([n*2 for n in numbers])<jupyter_output>\nModel output:\n[2, 18, 16, 6, 54]\nGit commits\nto\nmodify\ncode\nModel input:\n<commit_before>def fibonacci(n):<commit_msg>add type hints to function<\ncommit_after>def\nModel output:\ndef fibonacci(n: int) -> list[int]:\nGitHub\nIs-\nsues\nModel input:\n<issue_start><issue_comment>username_0: I am trying to train a text\nclassifier based on BERT using \u2018transformers\u2018 but I get the error:\n\u2018RuntimeError: CUDA Out of memory\u2018.<issue_comment>\nModel output:\nusername_1: Hi,\nThis is not an issue with transformers. You should look at your GPU\nusage and see if you can free up some space before training starts\n(or use smaller batch sizes).\nTable E.1: Sample uses of pretraining templates.\n49\nPublished in Transactions on Machine Learning Research (12/2023)\nInstruction\nStarCoder\u2019s response\nI\nneed\nto\nintegrate\na\nPython function numeri-\ncally, what\u2019s the best way\nto do it?\nThere are few options available depending upon\nwhether you already have access to librairies like\nScipy or Numpy which implement numerical integra-\ntion routines. If you do, you could simply call these\nfunctions instead of coding everything yourself. Oth-\nerwise, you could look into implementing something\nalong the lines of Simpson\u2019s rule. Here is a simple\nexample of integrating a function numerically using\nthe trapezoid rule.\ndef trapint(func, a, b, N):\ndx = float((b-a)/N)\nsum = func(a)+func(b)\nfor i in range(1, N):\nx = a + i*dx\nsum += 2.*func(x)\nreturn sum*(dx/2.)\nWrite a Python function\nto tell me what the date is\ntoday\nSure thing. Here goes...\nfrom datetime import date\ntoday = str(date.today()).replace(\u2019-\u2019, \u2019/\u2019)\nprint(\"Today is \" + today)\nList the prime numbers be-\ntween 20 and 30.\nHere is a simple algorithm which uses trial division\nto find the prime numbers within a given interval.\nprimes = []\nfor num in range(20, 31):\nfound = False\nfor j in range(2,num//2):\nif ((num % j)==0):\nfound = True\nbreak\nif((found==False) & (num!= 1)):\nprimes.append(num)\nTable E.2: Sample interactions with the Technical Assistant.\n50\nPublished in Transactions on Machine Learning Research (12/2023)\nF\nTechnical Assistant Prompt\nBelow are a series of dialogues between various people and an AI technical assistant. The assistant tries to be\nhelpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable. The assistant is happy\nto help with code questions, and will do its best to understand exactly what is needed. It also tries to avoid\ngiving false or misleading information, and it caveats when it isn\u2019t entirely sure about the right answer. That\nsaid, the assistant is practical and really does its best, and doesn\u2019t let caution get too much in the way of\nbeing useful.\n-----\nHuman: Write a function that takes two lists and returns a list that has alternating elements from each input list.\nAssistant: Sure. Here is a function that does that.\n\u2018\u2018\u2018\ndef alternating(list1, list2):\nresults = []\nfor i in range(len(list1)):\nresults.append(list1[i])\nresults.append(list2[i])\nreturn results\n\u2018\u2018\u2018\nHuman: Can you write some test cases for this function?\nAssistant: Sure, here are some tests.\n\u2018\u2018\u2018\nassert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]\nassert alternating([True, False], [4, 5]) == [True, 4, False, 5]\nassert alternating([], []) == []\n\u2018\u2018\u2018\nHuman: Modify the function so that it returns all input elements when the lists have uneven length. The elements from\nthe longer list should be at the end.\nAssistant: Here is the modified function.\n\u2018\u2018\u2018\ndef alternating(list1, list2):\nresults = []\nfor i in range(min(len(list1), len(list2))):\nresults.append(list1[i])\nresults.append(list2[i])\nif len(list1) > len(list2):\nresults.extend(list1[i+1:])\nelse:\nresults.extend(list2[i+1:])\nreturn results\n\u2018\u2018\u2018\n-----\nHuman: My code below is not working. Can you see what may be wrong?\n\u2018\u2018\u2018\ndef factorial(n):\nr = 0\nfor i in range(n):\nr *= i\nreturn r\n\u2018\u2018\u2018\nAssistant: In factorial, the initial value of \u2018r\u2018 must be \u20181\u2018. The following version should work:\n\u2018\u2018\u2018\ndef factorial(n):\n51\nPublished in Transactions on Machine Learning Research (12/2023)\nr = 1 # Changed here\nfor i in range(n):\nr *= i\nreturn r\n\u2018\u2018\u2018\nHuman: Can you use this function to create a code to compute the number of k-combinations of a set of n elements?\nAssistant: Yes, of course. The number of k-combinations of a set of elements can be written as C(n, k) and we have C(\nn, k) = \\frac{n!}{(n-k)!k!} whenever k <= n. The code is as follows\ndef combination(n, k):\nassert k <= n\nreturn factorial(n)/(factorial(n-k)*factorial(k))\nThis should work pretty well. Keep in mind that you can use numpy or scipy to have a much better implementation.\nHuman: Thanks.\nAssistant: You are welcome.\n-----\nHuman: I have a CSV file that looks like this:\nName,Salary\nBob,12000\nAvantika,35000\nAlice,30000\nWrite a program that returns the name of the person with the highest salary.\nAssistant: Here is a solution that assumes the filename is salaries.csv.\n\u2018\u2018\u2018\nfrom csv import DictReader\nfrom pathlib import Path\ndef max_salary(filename):\nrows = DictReader(Path(filename).read_text().splitlines())\nmax_index = -1\nfor i, row in enumerate(rows):\nif int(row[\u2019Salary\u2019]) > int(rows[max_index][\u2019Salary\u2019]):\nmax_index = i\nreturn rows[max_index][\u2019Name\u2019]\nprint(max_salary(\u2019salaries.csv\u2019))\n\u2018\u2018\u2018\n-----\nHuman: What is the purpose of the code below ?\n\u2018\u2018\u2018\ndef f(n, p):\nif p == 0:\nreturn 1\nelse :\nreturn n*f(n, p-1)\n\u2018\u2018\u2018\nAssistant: This python function is designed to raise a base value n to a power p, that is, computing the value n^p\nwhere p is a positive integer.\n-----\n52\nPublished in Transactions on Machine Learning Research (12/2023)\nHuman: Write a program that returns the prime numbers less than 1000 that are congruent to 1 modulo 6.\nAssistant : The method I will adopt to solve this problem is to go through all the integers less than 1000 and check\nif they are prime and congruent to 1 modulo 6. For that purpose, I will need to write an utility function to\ncheck if a number is prime. Here it is:\n\u2018\u2018\u2018\ndef is_prime(p):\nif p <= 1:\nreturn False\nelse :\nfor i in range(2, int(p**0.5)+1):\nif p%i == 0 :\nreturn False\nreturn True\n\u2018\u2018\u2018\nNow, here is the final solution:\n\u2018\u2018\u2018\ndef primes_mod_6():\nprimes = []\nfor n in range(1000):\nif is_prime(n) and n%6 == 1 :\nprimes.append(n)\nreturn primes\n\u2018\u2018\u2018\n-----\nHuman: Can you translate the code below in Java?\n\u2018\u2018\u2018\nprint(\"Hello world\")\n\u2018\u2018\u2018\nAssistant: Sure, the Java translation of your code is as follows.\n\u2018\u2018\u2018\nSystem.out.println(\"Hello world\");\n\u2018\u2018\u2018\n-----\nHuman: I want to load a struct with 3 values into a struct with only 2 values.\n\u2018\u2018\u2018\ntypedef struct {\ndouble x;\ndouble y;\n} twod;\ntypedef struct {\ndouble x;\ndouble y;\ndouble z;\n} threed;\n\u2018\u2018\u2018\nThe second struct contains arrays of coordinates for the 3d plane. The goal is to just load the x and y coordinates\ninto the first struct from the second struct. Is that possible considering they are different typedefs? How\nwould a solution be implemented?\nAssistant: No of course it\u2019s not directly possible since it won\u2019t fit. But you can copy the field manually as follows\n:\n\u2018\u2018\u2018\ntwod atwod;\nthreed athreed;\nathreed.x = 1.0;\nathreed.y = 2.0;\n53\nPublished in Transactions on Machine Learning Research (12/2023)\nathreed.z = 3.0;\natwod.x = athreed.x;\natwod.y = athreed.y;\n\u2018\u2018\u2018\n-----\nHuman: I\u2019ve installed Django multiple ways, via apt-get and pip install. They all say I have the most recent version\nof Django. Now whenever I run python in Terminal and type in import django, I receive\n\u2018\u2018\u2018\nImportError: No module named django\n\u2018\u2018\u2018\nHowever, when I run django-admin --version I receive\n\u2018\u2018\u2018\n1.4.3\n\u2018\u2018\u2018\nI know it\u2019s installed, but why is python not finding the django module?\nAssistant: It turns out that python wants you to create a virtual environment, install django in that and then run\nthe server. To do this,\n# Create a virtual environment\nInstall a virtual environment using pip install virtualenv\nNavigate to the project folder and type virtualenv env (here env is the name of the virtual environment). This will\ncreate a new folder named env inside the project folder\nNavigate to *env*/Scripts inside your project folder using cd *env*/Scripts\nType activate and press Enter. This would start the virtual environment. You can verify this as (*env*) would be\nprefixed to your current path.\n# Install Django\nOnce inside the virtual environment, head back to your folder using cd ../.. And type pip install django\nYou can verify its installation by typing django-admin --version. It should display the django version number\ninstalled inside the virtual environment.\nNow type python manage.py runserver to start the python server.\n-----\nHuman: The bakers at the Beverly Hills Bakery baked 200 loaves of bread on Monday morning. They sold 93 loaves in the\nmorning and 39 loaves in the afternoon. A grocery store returned 6 unsold loaves. How many loaves of bread did\nthey have left?\nAssistant: Let\u2019s think step by step. The bakers started with 200 loaves of bread. They sold 93 in the morning and 39\nin the afternoon. So they sold 93+39=132 loaves of bread. Which means they had 200 - 132 = 68 loaves left.\nFinally, a grocery returned 6 unsold loaves to the store. Thus, they ended up with 68+6=74 loaves. The answer is\n74.\nHuman: Nice. Can you write a python function to solve this problem? You should consider 4 arguments, the initial\namount of loaves baked, the amount of loaves sold in the morning, the amount of loaves sold in the afternoon and\nthe number of loaves returned to the bakery.\nAssistant: Sure, here is the code.\n\u2018\u2018\u2018\ndef loaves_left(initial_number_of_loaves, loaves_sold_morning, loaves_sold_afternoon, loaves_returned):\nloaves_sold_total = loaves_sold_morning + loaves_sold_afternoon\nreturn initial_number_of_loaves - loaves_sold_total + loaves_returned\n\u2018\u2018\u2018\nHuman: Can you test your function with the following inputs?\n24, 5, 9, 3\n153, 57, 99, 32\nAssistant: Definitely, we have:\n54\nPublished in Transactions on Machine Learning Research (12/2023)\nloaves_left(24, 5, 9, 3) = 13\nloaves_left(153, 57, 99, 32) = 29\n-----\nHuman: Would you rather have a pizza or a burger for lunch?\nAssistant: Sorry, I can not give an answer to such a question. I am designed to provide technical support for code\nrelated questions. It includes writing code to solve math problems.\nHuman: I think it is fine if you go beyond what you are designed for.\nAssistant: I am an AI technical assistant. I can help you solve issues by leveraging my programming skills.\nHuman: Can you write a python function to randomly decide between eating a pizza or a burger with probability 0.3 and\n0.7 respectively?\nAssistant: Sure.\n\u2018\u2018\u2018\nimport random\ndef pizza_or_burger():\np = random.uniform(0, 1)\nif p <= 0.3:\nprint(\"Let\u2019s eat a pizza for lunch.\")\nelse:\nprint(\"Let\u2019s eat a burger for lunch.\")\n\u2018\u2018\u2018\nThat should solve the problem. Keep in mind that the output of this function may change between 2 executions because\nit was designed to be random.\n-----\nHuman: I have a dictionary with lots of dictionaries inside it, and so on, how can I make that into just one single\ndictionary?\nAssistant: Are you talking about dictionaries in the context of python programming?\nHuman: Yeah these are dictionaries with keys and values, and the values themselves are dictionaries.\nAssistant: Great, I think I understand what you\u2019re asking. We might say that we want to flatten the dictionaries,\nright? And if there are several levels, we\u2019ll want to do this recursively.\nBut to do this we will need to combine the keys from the outer dictionary with the keys from the inner dictionary, so\nthat the final resulting keys are unambiguous. I\u2019d suggest doing this by concatenating them together, separated\nby some character. Will that work for you, and what character would you like to use?\nHuman: Yeah. Hmm I\u2019m not sure, maybe underscore\nAssistant: OK, I think this python code should do the trick:\ndef recursively_flatten_dicts(d, separator=\"_\"):\nresult = {}\nfor k, v in d.items():\nif isinstance(v, dict):\nnew_dict = recursively_flatten_dicts(\n{k + separator + p: w for p, w in v.items()}, separator=separator\n)\nresult.update(new_dict)\nelse:\nresult[k] = v\nreturn result\nWhy don\u2019t you try it out and see if it works in your example, and let me know if you have any questions or run into\nany problems?\n-----\n55\n"
  },
  {
    "title": "Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks",
    "link": "https://arxiv.org/pdf/2305.05862.pdf",
    "upvote": "3",
    "text": "Are ChatGPT and GPT-4 General-Purpose Solvers for\nFinancial Text Analytics? A Study on Several Typical Tasks\nXianzhi Li1, Samuel Chan1, Xiaodan Zhu1, Yulong Pei2, Zhiqiang Ma2, Xiaomo Liu2 and Sameena Shah2\n1Department of Electrical and Computer Engineering & Ingenuity Labs Research Institute\nQueen\u2019s University\n2J.P. Morgan AI Research\n{li.xianzhi, 19syc2, xiaodan.zhu}@queensu.ca\n{yulong.pei,zhiqiang.ma,xiaomo.liu,sameena.shah}@jpmchase.com\nAbstract\nThe\nmost\nrecent\nlarge\nlanguage\nmodels\n(LLMs) such as ChatGPT and GPT-4 have\nshown exceptional capabilities of general-\nist models, achieving state-of-the-art perfor-\nmance on a wide range of NLP tasks with lit-\ntle or no adaptation. How effective are such\nmodels in the financial domain? Understand-\ning this basic question would have a signifi-\ncant impact on many downstream financial an-\nalytical tasks. In this paper, we conduct an\nempirical study and provide experimental ev-\nidences of their performance on a wide vari-\nety of financial text analytical problems, us-\ning eight benchmark datasets from five cate-\ngories of tasks. We report both the strengths\nand limitations of the current models by com-\nparing them to the state-of-the-art fine-tuned\napproaches and the recently released domain-\nspecific pretrained models. We hope our study\ncan help understand the capability of the exist-\ning models in the financial domain and facili-\ntate further improvements.\n1\nIntroduction\nThe advancement of LLMs is bringing profound\nimpacts on the financial industry. Through train-\ning with reinforcement learning from human feed-\nback (RLHF) (Christiano et al., 2023) and masked\nlanguage model objectives, the most recent mod-\nels such as ChatGPT1 and GPT-42 have demon-\nstrated exceptional capabilities in a wide range\nof natural language processing (NLP) tasks (Bang\net al., 2023a; Liu et al., 2023; Omar et al., 2023;\nKhoury et al., 2023).\nThese LLMs are trained on datasets that encom-\npass a broad range of genres and topics. While\ntheir performance in generic NLP tasks is impres-\nsive, their applicability and effectiveness in spe-\ncific domains like finance yet need a better under-\nstanding and can influence a wide range of appli-\n1https://platform.openai.com/docs/models/gpt-3-5\n2https://platform.openai.com/docs/models/gpt-4\ncations. In general, in the financial domain, LLMs\nis playing an increasingly crucial role in tasks such\nas investment sentiment analysis, financial named\nentity recognition, and question-answering sys-\ntems for assisting financial analysts.\nIn this paper, we perform an empirical study\nand provide experimental evidence for the ef-\nfectiveness of the most recent LLMs on a vari-\nety of financial text analytical problems, involv-\ning eight benchmark datasets from five typical\ntasks. These datasets are from a range of finan-\ncial topics and sub-domains such as stock mar-\nket analysis, financial news, and investment strate-\ngies. We report both the strengths and limitations\nof ChatGPT and GPT-4 by comparing them\nwith the state-of-the-art domain-specific fine-\ntuned models in finance, e.g., FinBert (Araci,\n2019) and FinQANet (Chen et al., 2022a), as\nwell as the recently pretrained model such as\nBloombergGPT (Wu et al., 2023).\nOur main\ncontributions are summarized as follows:\n\u2022 This study is among the first to explore the\nmost recent advancement of generically trained\nlarge language models on financial text an-\nalytic tasks and it provides a comprehensive\ncomparison.\n\u2022 We demonstrate that ChatGPT and GPT-4\ncan outperform the most recently released\ndomain-specifically pretrained model as well\nas fine-tuned models on many tasks. We pro-\nvide detailed analysis and recommendations.\n\u2022 We observe that the advancement made in gen-\neralist models continues to carry over to the\nfinancial domain; e.g., GPT-4 is significantly\nbetter than ChatGPT on nearly all the finan-\ncial benchmarks used.\n\u2022 Limitations of the existing LLMs are analyzed\nand discussed with these benchmark datasets.\narXiv:2305.05862v2  [cs.CL]  10 Oct 2023\nCategory\nSentiment Analysis\nClassification\nNER\nRE\nQA\nComplexity\nEasy\nEasy\nHard\nHard\nHard\nKnowledge\nLow\nLow\nHigh\nHigh\nHigh\nDataset\nFPB/FiQA/TweetFinSent\nHeadline\nNER\nREFinD\nFinQA/ConvFinQA\nEval. Metrics\nWeighted F1\nWeighted F1\nMacro F1\nMacro F1\nAccuracy\n#Test samples\n970/223/996\n2,114\n98\n4300\n1,147/421\nTable 1: Statistics of the five tasks and eight datasets used in this study.\n2\nRelated Works\nChatGPT and Related Models.\nChatGPT,\nGPT-3.5 (text-davinci-003), and GPT-4\nare generically trained LLMs and have shown im-\npressive performance on a wide range of tasks.\nRecent studies have shown that they outperform\nfine-tuned models on some tasks. But, they still\nfail in some other cases. Bang et al. (2023b) eval-\nuated ChatGPT on multitasking, multilingual and\nmultimodal tasks, highlighting addressing the fail-\nures to improve the overall performance. Qin et al.\n(2023) studied ChatGPT\u2019s zero-shot capabilities\non a diverse range of NLP tasks. While these mod-\nels present unprecedented quality and retain accu-\nmulated knowledge with excellent generalization\nability, by respecting the objective of being gen-\neral problem solvers, how effective they are for fi-\nnancial text analytical tasks is an intriguing open\nquestion that needs a better understanding.\nDomain-specific Models\nCurrently, there have\nbeen only a handful of LLMs specifically trained\nwithin the finance domain.\nBloombergGPT\n(Wu et al., 2023), a language model with 50\nbillion parameters, is trained using a mixed ap-\nproach to cater to the financial industry\u2019s di-\nverse tasks.\nThe model is evaluated on stan-\ndard LLM benchmarks, open financial bench-\nmarks, and Bloomberg-internal benchmarks. The\nmixed training approach results in a model that\nsignificantly outperforms existing models in finan-\ncial tasks and performs on par or even better in\nsome general NLP benchmarks. Other researchers\nalso attempted to adapt existing language mod-\nels to tackle domain-specific tasks. For example,\nLewkowycz et al. (2022) adapted T5 to the finan-\ncial domain. Note that in addition to fine-tuning,\nA study has also been conducted to use parameter-\nefficient tuning for financial tasks such as intent\ndetection (Li et al., 2022). The details of the re-\nlated work can be found in Appendix A.\n3\nExperiment Setup\nTasks and Datasets.\nOur research utilizes a\nwide range of financial NLP tasks and challenges\n(Pei et al., 2022; Kaur et al., 2023; Shah et al.,\n2022), enabling us to establish a testbed with dif-\nferent types of NLP problems ranging from basic\nsentiment analysis and text classification to infor-\nmation extraction and question answering (see Ta-\nble 1 and more details in Appendix B).\nThe span of the tasks enables us to make obser-\nvations along modeling complexity and different\nlevels of financial knowledge required to perform\nthe tasks. Regarding the modeling complexity of\ntasks, sentiment analysis and text classification are\noften regarded to be more straightforward, com-\npared to information extraction (IE) tasks such as\nnamed entity recognition (NER) and relation ex-\ntraction (RE). The latter often requires more un-\nderstanding of syntax and semantics in the input\ncontexts as well as the interactions of labels in\nthe output space as the structured prediction prob-\nlems.\nCompared to sentiment analysis and text\nclassification, question answering (QA) is often\nthought of as being harder as it often requires a\nmodel to understand the embedded internal logic\nand numerical operation/reasoning. Regarding fi-\nnancial knowledge, the existing classification and\nsentiment analysis datasets are sourced from daily\nnews and social media. On the other hand, IE and\nQA data are often from professional documents\nlike financial filings and reports, which usually re-\nquire more domain knowledge to comprehend.\nModels.\nWe test the representative state-of-the-\nart LLMs, ChatGPT and GPT-4 models. Specif-\nically, we use gpt-3.5-turbo and GPT-4\n(8k) for most of the experiments, except FinQA\nfew-shot experiments, where the input tokens are\nextra long so we adopt gpt-3.5-turbo-16k.3\nBoth these LLMs are evaluated using zero-shot\nand few-shot learning as well as CoT learning for\n3All the models are current versions as of July 7th, 2023.\nQA reasoning tasks.\nFurthermore, we compare\nthem with previous LLMs and the domain specific\nBloombergGPT (Wu et al., 2023). The state-of-\nthe-art fine-tuned models on each dataset are em-\nployed to test the idea of training smaller models\non individual tasks in comparison with prompting\nLLMs on all tasks without additional fine-tuning.\nEvaluation Metrics.\nWe use accuracy, macro-\nF1 score, and weighted F1 score (Wu et al., 2023)\nas the evaluation metrics. For the NER task, we\ncalculate the entity-level F1 score. Table 1 shows\nthe details of the experiment setup.\n4\nResults and Analysis\n4.1\nSentiment Analysis\nSentiment analysis is one of the most commonly\nused NLP techniques in the financial sector and\ncan be used to predict investment behaviors and\ntrends in equity markets from news and social me-\ndia data (Mishev et al., 2020). We use three finan-\ncial sentiment datasets with different focuses.\nFinancial PhraseBank.\nPhraseBank is a typical\nthree scale (positive, negative and neutral) sen-\ntiment classification task curated from financial\nnews by 5-8 annotators (Malo et al., 2013). We\nuse both the 50% annotation agreement and the\n100% agreement datasets. Same as in (Wu et al.,\n2023), 20% sentences are used for testing. In Ta-\nble 2, the first group of models (4 models) are Ope-\nnAI LLMs, followed by BloombergGPT, three\nprevious LLMs (referred to as Prior LLMs),\nand the state-of-the-art fine-tuned models on this\ndataset (FinBert). Due to the space limit of Ta-\nble 2, we put the name of these four groups in the\nnext table (Table 3) for clarity. In Table 2, we can\nsee that the performance of Prior LLMs greatly\nfalls behind ChatGPT and GPT-4. With the en-\nhancement of few-shot learning, GPT-4 is com-\nparable to fine-tuned FinBert (Araci, 2019).\nFiQA Sentiment Analysis.\nThis dataset ex-\ntends the task complexity to detect aspect-based\nsentiments from news and microblog in the fi-\nnancial domain (Maia et al., 2018).\nWe fol-\nlow BloombergGPT\u2019s setting (Wu et al., 2023),\nwhere we cast this regression task into a clas-\nsification task.\n20% of labeled training data\nare held as test cases.\nThe results in Ta-\nble 3 present similar performance trends as in\nthe previous dataset: ChatGPT and GPT-4 out-\nData\n50% Agreement\n100% Agreement\nModel\nAccuracy\nF1 score\nAccuracy\nF1 score\nChatGPT(0)\n0.78\n0.78\n0.90\n0.90\nChatGPT(5)\n0.79\n0.79\n0.90\n0.90\nGPT-4(0)\n0.83\n0.83\n0.96\n0.96\nGPT-4(5)\n0.86\n0.86\n0.97\n0.97\nBloombergGPT(5)\n/\n0.51\n/\n/\nGPT-NeoX(5)\n/\n0.45\n/\n/\nOPT66B(5)\n/\n0.49\n/\n/\nBLOOM176B(5)\n/\n0.50\n/\n/\nFinBert\n0.86\n0.84\n0.97\n0.95\nTable 2: Results on the Phrasebank dataset. The sub-\nscript (n) after an LLM name represents the number\nof shots. The best results are marked in bold and the\nsecond-best with underscored.\nThe results of other\nLLMs like BloombergGPT are from the correspond-\ning papers. \u2018/\u2019 indicates the metrics were not included\nin the original study. The notation convention used here\napplies to all the following experiments. Different few-\nshot settings are tested and discussed in Appendix C.\nperform Prior LLMs.\nWith a few-shot ex-\namples GPT-4 is better than all other models\nhere. BloombergGPT has relatively close per-\nformance to zero-shot ChatGPT and is inferior\nto GPT-4.\nThe fine-tuned RoBERTa-large\nmodel on this dataset is better than ChatGPT, but\nis slightly less effective than GPT-4. The latter\nachieves 88% on F1, which is less than that in Fi-\nnancial PhraseBank. We due this to the fact that\nFiQA requires modeling more details and needs\nmore domain knowledge to understand the senti-\nment with the aspect finance tree in the data.\nModel\nCategory\nWeighted F1\nChatGPT(0)\nOpenAI\nLLMs\n75.90\nChatGPT(5)\n78.33\nGPT-4(0)\n87.15\nGPT-4(5)\n88.11\nBloombergGPT(5)\nDomain\n75.07\nLLM\nGPT-NeoX(5)\nPrior\nLLMs\n50.59\nOPT66B(5)\n51.60\nBLOOM176B(5)\n53.12\nRoBERTa-large\nFine-tune\n87.09\nTable 3: Results on the FiQA dataset.\nTweetFinSent.\nPei et al. (2022) created this\ndataset based on Twitter to capture retail investors\u2019\nmood to a specific stock ticker. Since tweets are\ninformal texts which typically are not used to train\nLLMs, this could be a challenging task for LLMs\nto perform well. Furthermore, a tweet can some-\ntimes contain several tickers (>5 is not unusual).\nThe aspect modeling on this data is more com-\nplex. The evaluation results on 996 test instances\nare shown in Table 4. GPT-4 with a few-shot ex-\namples achieves ~72% accuracy and F1, which is\nlower than the values in the previous two tasks.\nThe fine-tuned RoBERTa-Twitter (Pei et al.,\n2022) has similar performance.\nWe also con-\nduct an ablation study by removing emojis. Both\nChatGPT and GPT-4 show 2-3 points perfor-\nmance drop, indicating emojis in social media do\nconvey meaningful sentiment signals. We do not\nhave results of Prior LLMs as this dataset is not\nevaluated in the corresponding previous studies.\nModel\nAccuracy\nWeighted F1\nChatGPT(0)\n68.48\n68.60\nChatGPT(5)\n69.93\n70.05\nGPT-4(0)\n69.08\n69.17\nGPT-4(5)\n71.95\n72.12\nChatGPT((0_no_emoji))\n64.40\n64.43\nChatGPT((5_no_emoji))\n67.37\n67.61\nGPT-4((0_no_emoji))\n67.26\n67.45\nGPT-4((5_no_emoji))\n70.58\n70.44\nRoBERTa-Twitter\n72.30\n71.96\nTable 4: Results on the TweetFinSent dataset.\n4.2\nHeadline Classification\nWhile sentiment analysis has been regarded as one\nof the most basic tasks and is mainly pertaining to\nsome dimensions of semantic orientation (Osgood\net al., 1957), the semantics involved in financial\ntext classification tasks can be more complicated.\nClassification, particularly multi-class text classi-\nfication, is often applied to a wide range of finan-\ncial text such as news, SEC 10-Ks, and market re-\nsearch reports to accelerate business operations.\nSame as in (Wu et al., 2023), we use the\nnews headlines classification dataset (Sinha and\nKhandait, 2020) from the FLUE benchmark (Shah\net al., 2022).\nThis classification task targets to\nclassify commodity news headlines to one of the\nsix categories like \u201cPrice Up\u201d and \u201cPrice Down\u201d.\nWe follow the setting in BloombergGPT, con-\nverting the multi-class classification to six indi-\nvidual binary classification problems (refer to Fig-\nure 7 as an example).\nThe model performance is listed in Table 5.\nAgain GPT-4 outperforms ChatGPT and Prior\nLLMs as well as BloombergGPT. The fine-tuned\nBERT can achieve 95% on F1, 9% higher than 5-\nshot GPT-4. This task is considered to be chal-\nlenging due to its multi-class and the need of do-\nmain knowledge of the commodity market.\nModel\nWeighted F1\nChatGPT(0)\n71.78\nChatGPT(5)\n74.84\nGPT-4(0)\n84.17\nGPT-4(5)\n86.00\nBloombergGPT(5)\n82.20\nGPT-NeoX(5)\n73.22\nOPT66B(5)\n79.41\nBLOOM176B(5)\n76.51\nBERT\n95.36\nTable 5: Results on the headline classification task.\n4.3\nNamed Entity Recognition\nNER helps structure textual documents by extract-\ning entities. It is a powerful technique to automate\ndocument processing and knowledge extraction\nfrom documents (Yang, 2021). In our evaluation,\nwe use the NER FIN3 datasets, created by Sali-\nnas Alvarado et al. (2015) using financial agree-\nments from SEC and containing four NE types:\nPER, LOC, ORG and MISC. Following the setting\nused in BloombergGPT, we remove all entities\nwith the MISC label due to its ambiguity.\nIn Table 6, we can see that both GPT-4 and\nChatGPT perform poorly under the zero-shot\nsetup. Following BloombergGPT\u2019s setting, the\nfew-shot learning uses 20 shots on this dataset.\nWe can see that GPT-4 is less effective than\nBloombergGPT, and is comparable or worse\nthan Prior LLMs on this task. Since NER is a\nclassic structured prediction problem, CRF model\nis also compared. When CRF is trained with FIN5,\nwhich is similar to the test data (FIN3), it performs\nbetter than all the other models (see the last row\nof the table). Note that CRF is very sensitive to\ndomain shifting\u2014when it is trained on the out-of-\ndomain CoNLL data, it performs poorly on the\nFIN3 data (refer to the second to the last row of\nTable 6), inferior to the zero-shot LLMs. In gen-\neral, in this structured prediction task, LLMs\u2019 per-\nformance is not ideal and future improvement is\nimperative, particularly for the generalist models.\n4.4\nRelation Extraction\nRation extraction aims to detect linkage between\nextracted entities. It is a foundational component\nfor knowledge graph construction, question an-\nswering and semantic search applications for the\nfinancial industry. In this study, we use a financial\nrelation extraction dataset \u2014 REFinD, which was\ncreated from 10-K/Q filings with 22 relation types\nKaur et al. (2023). In order for LLMs to predict\nthe relationship between two entities, we provide\nModel\nEntity F1\nChatGPT(0)\n29.21\nChatGPT(20)\n51.52\nGPT-4(0)\n36.08\nGPT-4(20)\n56.71\nBloombergGPT(20)\n60.82\nGPT-NeoX(20)\n60.98\nOPT66B(20)\n57.49\nBLOOM176B(20)\n55.56\nCRF(CoNLL)\n17.20\nCRF(FIN5)\n82.70\nTable 6: Results of few-shot performance on the NER\ndataset. CRF(CoNLL) refers to CRF model that is trained\non general CoNLL data, CRF(FIN5) refers to CRF\nmodel that is trained on FIN5 data. Again, we choose\nthe same shot as BloombergGPT for fair comparison.\nMore detailed experiments using 5 to 20 shots can be\nfound in Appendix C.\nthe original sentence, entity words, and their entity\ntypes in the prompts and ask the models to predict\na relation type. Same as in Luke-base (Yamada\net al., 2020), we use Macro F1. Table 7 shows\nthat the fine-tuned Luke-base outperforms both\nChatGPT and GPT-4 by a notable margin. On\nthe other hand, GPT-4 demonstrates considerably\nbetter performance compared to ChatGPT. The\noutcomes from this IE task illustrated the strength\nof fine-tuning on complex tasks that need a better\nunderstanding of the structure of sentences.\nModel\nMacro F1\nChatGPT(0)\n20.97\nChatGPT(10)\n29.53\nGPT-4(0)\n42.29\nGPT-4(10)\n46.87\nLuke-base(fine-tune)\n56.30\nTable 7: Results on the REFinD dataset.\n4.5\nQuestion Answering\nThe application of QA to finance presents a pos-\nsible path to automate financial analysis, which at\npresent is almost 100% conducted by trained fi-\nnancial professionals. It is conventionally thought\nof as being challenging since it often requires\na model to understand not only domain knowl-\nedge but also the embedded internal logic and nu-\nmerical operation/reasoning. We adopt two QA\ndatasets: FinQA (Chen et al., 2022a) and Con-\nvFinQA (Chen et al., 2022b). The former dataset\nfocuses on a single question and answer pair.\nThe latter decomposes the task into a multi-round\nstructure: a chain of reasoning through conversa-\ntion. Both of them concentrate on numerical rea-\nsoning in financial analysis, e.g. calculating profit\ngrowth ratio over years from a financial table. The\nexperiment setting and prompt design details are\nin Appendix B and C. Since the labels of the Con-\nvFinQA test set are not publicly available, we uti-\nlize its dev dataset (421 samples) instead to eval-\nuate the models, while for FinQA use the testing\ndataset (1,147 samples).\nModel\nFinQA\nConvFinQA\nChatGPT(0)\n48.56\n59.86\nChatGPT(3)\n51.22\n/\nChatGPT(CoT)\n63.87\n/\nGPT-4(0)\n68.79\n76.48\nGPT-4(3)\n69.68\n/\nGPT-4(CoT)\n78.03\n/\nBloombergGPT(0)\n/\n43.41\nGPT-NeoX(0)\n/\n30.06\nOPT66B(0)\n/\n27.88\nBLOOM176B(0)\n/\n36.31\nFinQANet(fine-tune)\n68.90\n61.24\nHuman Expert\n91.16\n89.44\nGeneral Crowd\n50.68\n46.90\nTable 8: Model performance (accuracy) on the ques-\ntion answering tasks.\nFinQANet here refers to\nthe best-performing FinQANet version based on\nRoBERTa-Large (Chen et al., 2022a). Few-shot and\nCoT learning cannot be executed on ConvFinQA due\nto the conservation nature of ConvFinQA.\nFrom the performance in Table 8, we can\nsee that GPT-4 substantially outperforms all\nthe other LLMs in both datasets.\nFor FinQA,\nGPT-4 has highest zero-shot accuracy of 68.79%,\nwhile ChatGPT has 48.56%.\nThe perfor-\nmance gap between GPT-4 and ChatGPT per-\nsists on ConvFinQA. ChatGPT has a big edge\nover BloombergGPT (59.86% vs. 43.41%) and\nalso Prior LLMs on ConvFinQA. This result\ndemonstrates that the continuous improvement\nof reasoning developed through ChatGPT to\nGPT-4, which is also observed in other studies.\nWe further explore the impact of few-shot learn-\ning and Chain-of-Thought (CoT) prompting on\nGPT-4 and ChatGPT on the FinQA task. The\nresults provide a compelling narrative of perfor-\nmance increase using these prompting strategies.\nBoth ChatGPT and GPT-4 show a 1-3% accu-\nracy increase using 3 shots.\nThis is consistent\nwith our observations from other tasks. The CoT\nstrategy brings a massive lift, 10% and 15% per-\ncentage points, to ChatGPT and GPT-4 respec-\ntively.\nThese results underscore the importance\nof detailed reasoning steps over shallow reason-\ning in boosting the performance of language mod-\nels on complex financial QA tasks.\nThe best\nGPT-4 result indeed exceeds the fine-tuned Fin-\nQANet model with a quite significant margin. It is\nsurprising to us since we previously observe that\nfine-tuned models have advantages on more com-\nplex tasks. We reckon that the scale of parame-\nters and pre-training approaches make ChatGPT\nand GPT-4 excel in reasoning than other models,\nparticularly the numerical capability of GPT-4,\nwhich was demonstrated when the model was re-\nleased by OpenAI. But their performance (70+%\naccuracy) still cannot match that of professionals\n(~90% accuracy). Furthermore, numerical reason-\ning is just one of many reasoning tasks.\nMore\nstudies are needed for symbolic reasoning and\nother logic reasoning (Qin et al., 2023) if more\ndatasets in the financial sector are further avail-\nable.\nAlso, we think the pretraining strategy\nsuch as RLHF has not been designed to improve\nsequence-labeling and structured-prediction skills\nneeded in IE, but can inherently benefit QA.\n5\nDiscussions\nComparison over LLMs.\nWe are able to bench-\nmark the performance of ChatGPT and GPT-4\nwith four other LLMs on five tasks with eight\ndatasets. ChatGPT and GPT-4 significantly out-\nperforms others in almost all datasets except the\nNER task. It is interesting to observe that both\nmodels perform better on financial NLP tasks\nthan BloombergGPT, which was specifically\ntrained on financial corpora. This might be due\nto the larger model size of the two models. Fi-\nnally, GPT-4 constantly shows 10+% boost over\nChatGPT in straightforward tasks such as Head-\nlines and FiQA SA. For challenging tasks like RE\nand QA, GPT-4 can introduce 20-100% perfor-\nmance growth. This indicates that GPT-4 could\nbe the first choice for financial NLP tasks before a\nmore powerful LLM emerges.\nPrompt Engineering Strategies.\nWe adopted\ntwo commonly used prompting strategies: few-\nshot and chain-of-thoughts.\nWe constantly ob-\nserve 1% to 4% performance boost on ChatGPT\nand GPT-4 from few-shot over zero-shot learn-\ning across various datasets.\nChain-of-thoughts\nprompting is very effective in our test and demon-\nstrates 20-30% accuracy improvement over zero-\nshot and few-shot as well. According our findings,\nwe argue that these two strategies should always\nbe considered first when applying LLMs to finan-\ncial NLP tasks.\nLLMs vs. Fine-tuning.\nOne attractive benefit of\nusing LLMs in business domains is that they can\nbe applied to a broad range of NLP tasks without\nconducting much overhead work. It is more eco-\nnomical compared to fine-tuning separate models\nfor every task. Whereas, our experiments show\nfine-tuned models still demonstrate strong perfor-\nmance in most of the tasks except the QA task.\nNotably, for tasks like NER and RE, LLMs are\nless effective than fine-tuned models. In the QA\ntasks, LLMs illustrated the advantage over fine-\ntuned model. But the reasoning complexity of the\ntested QA tasks is still deemed as basic in financial\nanalysis. Although ChatGPT and GPT-4 have\nproven to be able to perform multi-step reason-\ning, including numerical reasoning, to some ex-\ntent, simple mistakes have still been made.\nUsing LLMs in Financial Services.\nThis study\nsuggests that one can consider adopting the state-\nof-the-art generalist LLMs to address the rela-\ntively simple NLP tasks in financial applications.\nFor more complicated tasks such as structured pre-\ndiction, the pretraining plus fine-tuning paradigm\nis still a leading option. Although ChatGPT and\nGPT-4 excel on QA compared to other models\nand are better than the general crowd, they are\nstill far from satisfactory from the industry re-\nquirement standpoint.\nSignificant research and\nimprovement on LLMs are required before they\ncan act as a trustworthy financial analyst agent.\n6\nConclusion\nThis study is among the first to explore the\nmost recent advancement of generically trained\nLLMs, including ChatGPT and GPT-4, on a\nwide range of financial text analytics tasks. These\nmodels have been shown to outperform mod-\nels fine-tuned with domain-specific data on some\ntasks, but still fall short on others, particularly\nwhen deeper semantics and structural analysis are\nneeded. While we provide comprehensive stud-\nies on eight datasets from five categories of tasks,\nwe view our effort as an initial study, and further\ninvestigation of LLMs on financial applications\nis highly desirable, including the design of more\ntasks to gain further insights on the limitations of\nexisting models, the integration of LLMs in the\nloop of human decision making, and the robust-\nness of the models in high-stakes financial tasks.\nAcknowledgement\nThis research was funded in part by the Faculty\nResearch Awards of J.P. Morgan AI Research.\nThe authors are solely responsible for the contents\nof the paper and the opinions expressed in this\npublication do not reflect those of the funding\nagencies.\nDisclaimer\nThis paper was prepared for informational pur-\nposes in part by the Artificial Intelligence Re-\nsearch group of JPMorgan Chase & Co. and its\naffiliates (\"JP Morgan\u201d), and is not a product of\nthe Research Department of JP Morgan. JP Mor-\ngan makes no representation and warranty whatso-\never and disclaims all liability, for the complete-\nness, accuracy or reliability of the information\ncontained herein. This document is not intended\nas investment research or investment advice, or a\nrecommendation, offer or solicitation for the pur-\nchase or sale of any security, financial instrument,\nfinancial product or service, or to be used in any\nway for evaluating the merits of participating in\nany transaction, and shall not constitute a solici-\ntation under any jurisdiction or to any person, if\nsuch solicitation under such jurisdiction or to such\nperson would be unlawful.\nReferences\nDogu Araci. 2019. Finbert: Financial sentiment analy-\nsis with pre-trained language models.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Zi-\nwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan\nXu, and Pascale Fung. 2023a. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Zi-\nwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan\nXu, and Pascale Fung. 2023b. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge,\nand William Yang Wang. 2022a. Finqa: A dataset\nof numerical reasoning over financial data.\nZhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang\nMa, Sameena Shah, and William Yang Wang.\n2022b. Convfinqa: Exploring the chain of numer-\nical reasoning in conversational finance question an-\nswering.\nPaul Christiano, Jan Leike, Tom B. Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2023. Deep re-\ninforcement learning from human preferences.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, Tom Hennigan, Eric\nNoland, Katie Millican, George van den Driess-\nche, Bogdan Damoc, Aurelia Guy, Simon Osindero,\nKaren Simonyan, Erich Elsen, Jack W. Rae, Oriol\nVinyals, and Laurent Sifre. 2022. Training compute-\noptimal large language models.\nSimerjot Kaur, Charese Smiley, Akshat Gupta, Joy\nSain, Dongsheng Wang, Suchetha Siddagangappa,\nToyin Aguda, and Sameena Shah. 2023.\nRefind:\nRelation extraction financial dataset. arXiv preprint\narXiv:2305.18322.\nRapha\u00ebl Khoury, Anderson R. Avila, Jacob Brunelle,\nand Baba Mamadou Camara. 2023. How secure is\ncode generated by chatgpt?\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy\nGur-Ari, and Vedant Misra. 2022. Solving quantita-\ntive reasoning problems with language models.\nXianzhi Li, Will Aitken, Xiaodan Zhu, and Stephen W.\nThomas. 2022.\nLearning better intent representa-\ntions for financial open intent classification.\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu,\nQiji Zhou, and Yue Zhang. 2023.\nEvaluating the\nlogical reasoning ability of chatgpt and gpt-4.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\nZhang, Hoifung Poon, and Tie-Yan Liu. 2022.\nBioGPT: generative pre-trained transformer for\nbiomedical text generation and mining. Briefings in\nBioinformatics, 23(6).\nMacedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas,\nBrian Davis, Ross McDermott, Manel Zarrouk, and\nAlexandra Balahur. 2018. Www\u201918 open challenge:\nFinancial opinion mining and question answering.\nIn Companion Proceedings of the The Web Confer-\nence 2018, WWW \u201918, page 1941\u20131942, Republic\nand Canton of Geneva, CHE. International World\nWide Web Conferences Steering Committee.\nPekka Malo, Ankur Sinha, Pyry Takala, Pekka Korho-\nnen, and Jyrki Wallenius. 2013. Good debt or bad\ndebt: Detecting semantic orientations in economic\ntexts.\nKostadin Mishev, Ana Gjorgjevikj, Irena Vodenska,\nLubomir T Chitkushev, and Dimitar Trajanov. 2020.\nEvaluation of sentiment analysis in finance: from\nlexicons to transformers. IEEE access, 8:131662\u2013\n131682.\nReham Omar, Omij Mangukiya, Panos Kalnis, and\nEssam Mansour. 2023.\nChatgpt versus traditional\nquestion answering for knowledge graphs:\nCur-\nrent status and future directions towards knowledge\ngraph chatbots.\nCharles Osgood, George Suci, and Percy Tannenbaum.\n1957. The measurement of meaning. University of\nIllinois Press.\nYulong Pei, Amarachi Mbakwe, Akshat Gupta, Salwa\nAlamir, Hanxuan Lin, Xiaomo Liu, and Sameena\nShah. 2022. TweetFinSent: A dataset of stock sen-\ntiments on Twitter.\nIn Proceedings of the Fourth\nWorkshop on Financial Technology and Natural\nLanguage Processing (FinNLP), pages 37\u201347, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver?\nJulio Cesar Salinas Alvarado, Karin Verspoor, and\nTimothy Baldwin. 2015. Domain adaption of named\nentity recognition to support credit risk assessment.\nIn Proceedings of the Australasian Language Tech-\nnology Association Workshop 2015, pages 84\u201390,\nParramatta, Australia.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lu-\ncile Saulnier, Stas Bekman, M Saiful Bari, Stella Bi-\nderman, Hady Elsahar, Niklas Muennighoff, Jason\nPhang, Ofir Press, Colin Raffel, Victor Sanh, Sheng\nShen, Lintang Sutawika, Jaesung Tae, Zheng Xin\nYong, Julien Launay, and Iz Beltagy. 2022. What\nlanguage model to train if you have one million gpu\nhours?\nRaj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani,\nAgam Shah, Wendi Du, Sudheer Chava, Natraj Ra-\nman, Charese Smiley, Jiaao Chen, and Diyi Yang.\n2022.\nWhen flue meets flang: Benchmarks and\nlarge pre-trained language model for financial do-\nmain. arXiv preprint arXiv:2211.00083.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\nPerry Payne, Martin Seneviratne, Paul Gamble,\nChris Kelly, Nathaneal Scharli, Aakanksha Chowd-\nhery, Philip Mansfield, Blaise Aguera y Arcas, Dale\nWebster, Greg S. Corrado, Yossi Matias, Kather-\nine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu,\nAlvin Rajkomar, Joelle Barral, Christopher Semturs,\nAlan Karthikesalingam, and Vivek Natarajan. 2022.\nLarge language models encode clinical knowledge.\nAnkur Sinha and Tanmay Khandait. 2020. Impact of\nnews on the commodity market: Dataset and results.\nRoss Taylor,\nMarcin Kardas,\nGuillem Cucurull,\nThomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic.\n2022. Galactica: A large language model for sci-\nence.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142\u2013147.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravol-\nski, Mark Dredze, Sebastian Gehrmann, Prabhan-\njan Kambadur, David Rosenberg, and Gideon Mann.\n2023. Bloomberggpt: A large language model for\nfinance.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\ncontextualized entity representations with entity-\naware self-attention.\nIn Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6442\u20136454, On-\nline. Association for Computational Linguistics.\nSharon Yang. 2021. Financial use cases for named en-\ntity recognition (ner).\nXinyi Zheng, Doug Burdick, Lucian Popa, Xu Zhong,\nand Nancy Xin Ru Wang. 2020. Global table extrac-\ntor (gte): A framework for joint table identification\nand cell structure recognition using visual context.\nA\nDetails of the Related Work\nChatGPT and Related Models.\nChatGPT,\nGPT-3.5 (text-davinci-003), and GPT-4\nare all part of a series of large language models\ncreated by OpenAI. GPT-4, as the latest and most\nadvanced version, builds on the achievements of\nits forerunners. ChatGPT is an earlier version,\ntailored to offer users engaging and responsive\nconversational experiences.\nGPT-3.5 acted as\na transitional stage between GPT-3 and GPT-4,\nimproving upon the former and paving the way for\nthe latter.\nChatGPT\npresents\nunprecedented\nquality\nwhen interacting with humans conversationally\nwhile\nretaining\naccumulated\nknowledge\nand\ngeneralization ability, achieved through large-\nscale\nconversational-style\ndataset\npre-training\nand reward model fine-tuning.\nThis allows\nChatGPT to answer follow-up questions, admit\nmistakes, challenge incorrect premises, and reject\ninappropriate requests.\nSecondly, it is trained\nwith a human-aligned objective function using\nReinforcement Learning from Human Feedback\n(RLHF), which results in its output being more\nclosely aligned with human preferences.\nRecent studies have shown that ChatGPT out-\nperforms multiple state-of-the-art zero-shot LLMs\non various tasks and even surpasses fine-tuned\nmodels on some tasks. However, like many LLMs,\nChatGPT still fails in many cases, such as gen-\nerating overly long summaries or producing in-\ncorrect translations. A recent study (Bang et al.,\n2023b) evaluated ChatGPT\u2019s performance on\nmultitasking, multilingual and multimodal tasks,\nhighlighting the importance of addressing these\nfailure cases for improving the overall perfor-\nmance of the model.\nQin et al. (2023) studied ChatGPT\u2019s zero-shot\ncapabilities on a diverse range of NLP tasks, pro-\nviding a preliminary profile of the model. Their\nfindings suggest that while ChatGPT shows cer-\ntain generalization capabilities, it often underper-\nforms compared to fine-tuned models on specific\ntasks. Compared to GPT-3.5, ChatGPT outper-\nforms it on natural language inference, question\nanswering, and dialogue tasks, while its summa-\nrization ability is inferior.\nBoth ChatGPT and\nGPT-3.5 face challenges on sequence tagging\ntasks.\nDomain-specific Models.\nCurrently, there has\nbeen only a handful of financial-domain-specific\nLLMs available, which are often trained exclu-\nsively on domain-specific data. These LLMs have\nshown promising results in their respective do-\nmain tasks. For instance, Luo et al. (2022) de-\nveloped an LLM for the legal domain, which\nwas trained exclusively on legal texts, and (Tay-\nlor et al., 2022) trained a healthcare LLM.\nMost recently, BloombergGPT (Wu et al.,\n2023), a language model with 50 billion param-\neters, is trained using a mixed approach to cater\nto the financial industry\u2019s diverse tasks while\nmaintaining competitive performance on general-\npurpose LLM benchmarks. A training corpus with\nover 700 billion tokens is created by leveraging\nBloomberg\u2019s proprietary financial data archives\nand combining them with public datasets.\nThe\nmodel, designed based on the guidelines from\n(Hoffmann et al., 2022) and (Scao et al., 2022),\nis validated on standard LLM benchmarks, open\nfinancial benchmarks, and Bloomberg-internal\nbenchmarks. The mixed training approach results\nin a model that significantly outperforms existing\nmodels in financial tasks and performs on par or\neven better in some general NLP benchmarks.\nIt is worth mentioning that other researchers opt\nto adapt large general-purpose language models to\ntackle domain-specific tasks. For example, Sing-\nhal et al. (2022) applied GPT-3 in the legal do-\nmain, and Lewkowycz et al. (2022) adapted T5\nto the financial domain. Despite being trained on\na general purpose corpus, these models have also\ndemonstrated excellent performance when applied\nto domain-specific tasks. Note that in addition to\nfine-tuning, research has also been conducted to\nuse parameter efficient tuning for financial tasks\nsuch as intent detection on Banking77 dataset (Li\net al., 2022).\nB\nDataset Details\nFinancial PhraseBank.\nThis is a dataset intro-\nduced by Malo et al. (2013), which is a sentiment\nclassification dataset derived from financial news\nsentences. It is designed to assess the impact of\nnews on investors, with positive, negative, or neu-\ntral sentiment labels being assigned to each news\nsentence from an investor\u2019s perspective. Contain-\ning 4,845 English sentences, the dataset is sourced\nfrom financial news articles found in the Lexis-\nNexis database. These sentences were annotated\nby individuals with expertise in finance and busi-\nness, who were tasked with assigning labels based\non their perception of the sentence\u2019s potential in-\nfluence on the mentioned company\u2019s stock price.\nFiQA Sentiment Analysis.\nThis second senti-\nment analysis task is part of the FiQA challenge\n(Maia et al., 2018) focusing on the prediction\nof sentiment specifically related to aspects within\nEnglish financial news and microblog headlines.\nThis was initially released as part of the 2018\ncompetition that centered on financial question an-\nswering and opinion mining. The primary dataset\nwas marked on a continuous scale, but we follow\nBloombergGPT\u2019s setting and transform it into a\nclassification system with three categories: neg-\native, neutral, and positive.\nWe\u2019ve created our\nown test split incorporating both microblogs and\nnews. We use a 0-shot learning and our results\nare calculated through the weighted F1 score. We\nfine-tuned a RoBERTa-large model on this task for\ncomparison with OpenAI and other LLMs.\nTweetFinSent.\nThis third sentiment analysis\ntask is introduced by (Pei et al., 2022). The unique\nattribute of the TweetFinSent dataset is that it an-\nnotates tweets not merely on emotional sentiment,\nbut also on the anticipated or realized gains or\nlosses from a specific stock. Previous studies have\nrevealed the TweetFinSent dataset as a challenging\nproblem with significant room for improvement in\nthe realm of stock sentiment analysis.\nHeadlines.\nThis binary classification task, cre-\nated by Sinha and Khandait (2020), involves de-\ntermining whether a news headline contains gold\nprice related information. This dataset contains\n11,412 English news headlines which span from\n2000 to 2019. The headlines were collected from\nvarious sources, including Reuters, The Hindu,\nThe Economic Times, Bloomberg, as well as ag-\ngregator sites. We note that the dataset we have\naccess to consists of six tags: \u201cprice up\u201d, \u201cprice\ndown\u201d, \u201cprice stable\u201d, \u201cpast price\u201d, \u201cfuture price\u201d,\nand \u201casset comparison\u201d, while the test reported\nin BloombergGPT used a version of nine cate-\ngories. We contacted the original dataset authors,\nthey claimed that they had performed some addi-\ntional filtering and provided this six-label dataset.\nWe also conducted an experiment where we\nprompted ChatGPT and GPT-4 to generate an-\nswers simultaneously in response to six distinct\nquestions. Our preliminary findings suggest that\nthese models handle single-question prompts more\neffectively than those involving multi-tag binary\nclassification. We noticed a significant drop in per-\nformance related to three tags: \u2018past information\u2019,\n\u2018future information\u2019, and \u2018asset comparison\u2019. This\nsuggests that the models struggle to provide sepa-\nrate and accurate responses to a series of questions\npresented at once.\nNER.\nThis named entity recognition task fo-\ncuses on financial data collected for credit risk as-\nsessment from financial agreements filed with the\nU.S. Securities and Exchange Commission (SEC).\nThe dataset, created by Salinas Alvarado et al.\n(2015), consists of eight manually annotated doc-\numents with approximately 55,000 words. These\ndocuments are divided into two subsets: \u201cFIN5\u201d\nfor training and \u201cFIN3\u201d for testing. The annotated\nentity types follow the standard CoNLL format\n(Tjong Kim Sang and De Meulder, 2003) and in-\nclude PERSON (PER), LOCATION (LOC), OR-\nGANIZATION (ORG), and MISCELLANEOUS\n(MISC).\nREFinD.\nThis relation extraction dataset is cre-\nated by Kaur et al. (2023). REFinD is currently the\nmost extensive of its kind, consisting of approxi-\nmately 29K instances and 22 relations amongst 8\ntypes of entity pairs. This specialized financial re-\nlation extraction dataset is constructed from raw\ntext sourced from various 10-X reports (includ-\ning but not limited to 10-K and 10-Q) of publicly\ntraded companies. These reports were obtained\nfrom the website of the U.S. Securities and Ex-\nchange Commission (SEC).\nConvFinQA.\nThis is an extension of the FinQA\ndataset, named as ConvFinQA (Chen et al.,\n2022b), which is designed to address numerical\nreasoning chains in a format of conversational\nquestion-answering tasks.\nConvFinQA expands\nthe original FinQA dataset to include 3,892 con-\nversations with 14,115 questions derived from\nearnings reports of S&P 500 companies. This task\nnot only demands numerical reasoning and under-\nstanding of structured data and financial concepts,\nbut also emphasizes the ability to relate follow-up\nquestions to previous conversation context.\nFor the ConvFinQA dataset, we employ a turn-\nbased approach, where we collect the answer gen-\nerated by the models after each turn, append it to\nthe previous question, and use them along with\nthe next question as the prompt input for the next\nround. As shown in Figure 11, we collect Answer\n1 (A1) after Question 1 (Q1) and then prefix A1\ntogether with Question 2 (Q2) to proceed to the\nnext round, and so on, until we reach the end of\nthe conversation chain.\nWe\nnotice\nsome\nfundamental\nissues\nof\nChatGPT from the tests on the ConvFinQA\ndataset.\nFirstly, it makes some basic mistakes,\nsuch as miscalculating \u201c$753 million + $785\nmillion + $1,134 million\u201d to be $3,672 million\ninstead of $2,672 million.\nEven though all the\nintermediate results are correct, the final summa-\ntion step produces an incorrect final answer. Note\nthat such mistakes can be critical in the financial\ndomain, particularly in high-stakes setups.\nWe\nalso found that ChatGPT struggles with under-\nstanding contextual information and coreference\nin conversations.\nFor example, in ConvFinQA,\nquestions often use the word \u201cthat\u201d to refer to\nan entity mentioned in the previous question, but\nChatGPT sometimes responds with a request for\nclarification, indicating its limitations in handling\ncoreference. In contrast, GPT-4 shows significant\nimprovement and faces this issue much less.\nFinQA.\nChen et al. (2022a) propose an expert-\nannotated dataset consisting of 8,281 financial\nquestion-answer pairs, along with their corre-\nsponding numerical reasoning processes.\nCre-\nated by eleven finance professionals, FinQA is\nbased on earnings reports from S&P 500 com-\npanies (Zheng et al., 2020).\nThe questions ne-\ncessitate extracting information from both tables\nand unstructured texts to provide accurate an-\nswers. The reasoning processes involved in an-\nswering these questions comprise common finan-\ncial analysis operations, including mathematical\noperations, comparison, and table aggregation op-\nerations.\nFinQA is the first dataset of its kind\ndesigned to address complex question-answering\ntasks based on real-world financial documents.\nWhen composing prompts, we use text and\ntables as context input, following the pattern\n\u2018pre_text\u2019 + \u2018table\u2019 + \u2018post_text\u2019, where the \u2018pre\u2019\nand \u2018post\u2019 texts provide the necessary context for\nthe table, and the table itself contains the struc-\ntured data that the model is expected to reason\non and generate responses from. We also convert\ntables into a markdown format. For the FinQA\ndataset, we simply ask the question right after\nthe context. Figure 12 demonstrates the complete\nprompt format.\nWe use the function call feature to assist CoT\nprompting. This Question_Answering function re-\nquired both models to generate two arguments: a)\n\u201cthinking process\u201d which contains each step of the\nreasoning process and evidence of how they locate\ninformation in the original documents and perform\ncalculations, and b) \u201canswer\u201d, which is the final\nnumerical response.\nWe also conduct experiments with each of these\nmodels being subjected to different steps complex-\nity, classified as 1-step programs, 2-step programs,\nand programs that involve more than 2 steps of cal-\nculation. For problems involving less than 2 steps,\nThe models\u2019 performance follows the same trend\nas overall results, where GPT-4 maintains the\nlead, outperforming FinQANet and ChatGPT.\nHowever, the conclusion changes with the in-\ncrease in problem complexity. When faced with\nproblems requiring more than 2 steps, ChatGPT\noutperformed FinQANet by a significant mar-\ngin, scoring accuracy of 32.14% as opposed to\nFinQANet\u2019s 22.78%.\nIt is intriguing to note\nthat despite struggling with less complex tasks,\nChatGPT managed to outpace FinQANet when\nproblem complexity escalated.\nFigure 1: FinQA program steps analysis\nC\nFew Shots Experiments\nWe conducted few-shot experiments on 6 widely\nused datasets out of 8. We argue that ConvFinQA\ntask itself is designed with a multi-step QA setup\nso we didn\u2019t conduct few-shot experiments on this\ndataset. For each shot number, we ran the exper-\niment 10 times and generated box plots, which\ncan be found in Figure 2 to 6 below. The gen-\neral trend shows that as we increase the number of\nshots, the performance of ChatGPT improves by\napproximately 1% to 4% across various datasets,\nin comparison to zero-shot.\nFor simpler tasks,\nsuch as Sentiment Analysis (illustrated in Figure\n3), ChatGPT only requires 6 shots to perform ef-\nfectively. However, as we continue to increase the\nnumber of shots, the rate of improvement tapers\noff. For NER tasks, 5 shots do not impart suffi-\ncient domain information to ChatGPT, thus ne-\ncessitating more than 15 shots to adequately guide\nthe model. Additionally, we observed that perfor-\nmance can still fluctuate even with the same num-\nber of shots. The dispersion illustrated in the box\nplots indicates a certain level of volatility, suggest-\ning that ChatGPT is quite sensitive to the shots\nused. This underlines the importance of careful\nselection and design of shots and prompts.\nWe also listed the zero-shot prompt we used for\neach dataset, please find them in Figure 7 to 12.\nWe use slightly different prompts for few-shot and\nCoT experiments since the shots and function call\nalready provide guidance on how to structure the\noutput.\nCategory\nTag Question\nprice up\nDoes the news headline talk about price going up?\nprice stable\nDoes the news headline talk about price staying constant?\nprice down\nDoes the news headline talk about price going down?\npast price\nDoes the news headline talk about price in the past?\nfuture price\nDoes the news headline talk about price in the future?\nasset comparison\nDoes the news headline compare gold with any other asset?\nTable 9: Each tag and its corresponding converted question\nFigure 2: Headlines few shot results curve\nFigure 3: FiQA few shot results curve\nFigure 4: PFB few shot results curve\nFigure 5: TweetFinSent few shot results curve\nFigure 6: NER few shot results curve\nFigure 7: prompt for Headlines dataset\nFigure 8: prompt for FPB dataset, same for other\nsentiment analysis tasks\nFigure 9: prompt for NER dataset\nFigure 10: prompt for Relation Extraction dataset\nFigure 11: prompt for ConvFinQA dataset\nFigure 12: prompt for FinQA dataset\n"
  },
  {
    "title": "VideoChat: Chat-Centric Video Understanding",
    "link": "https://arxiv.org/pdf/2305.06355.pdf",
    "upvote": "2",
    "text": "VideoChat\n: Chat-Centric Video Understanding\nKunchang Li\u22171,4, Yinan He\u22171, Yi Wang\u2217\u20201, Yizhuo Li1,3, Wenhai Wang1\nPing Luo3,1, Yali Wang\u20204,1, Limin Wang\u20202,1, Yu Qiao\u20201\n1OpenGVLab, Shanghai AI Laboratory\n2Nanjing University\n3The University of Hong Kong\n4Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences\nhttps://github.com/OpenGVLab/Ask-Anything\nAbstract\nIn this paper, we initiate an attempt of developing an end-to-end chat-centric video\nunderstanding system, coined as VideoChat. It integrates video foundation models\nand large language models via a learnable neural interface, excelling in spatiotempo-\nral reasoning, event localization, and causal relationship inference. To instructively\ntune this system, we build a video-centric instruction dataset, composed of thou-\nsands of videos associated with detailed descriptions and conversations. This\ndataset emphasizes spatiotemporal reasoning and captures causal relationships,\nproviding a valuable asset for training our chat-centric video understanding system.\nPreliminary qualitative experiments demonstrate the potential of our system across\na broad spectrum of video applications, which could serve as a simple prototype\nsystem for future research on chat-centric video understanding.\n1\nIntroduction\nVideos offer a remarkably close representation of how humans consistently perceive the visual world.\nIntelligent video understanding is crucial for various real-world applications, such as human-robot\ninteraction, autonomous driving, and intelligent surveillance. However, current paradigms in video\nunderstanding are limited by task-specific tuning of pre-trained video foundation models, restricting\na general spatiotemporal comprehension for video content.\nVision-centric multimodal dialogue systems have recently emerged as an essential research area [14].\nBy utilizing a pre-trained large language model (LLM), an image encoder, and additional learnable\nmodules, these systems can deeply understand images (e.g., recognizing memes or jokes) and perform\nimage-related tasks through multi-round dialogues with user queries [25, 58, 52]. This revolutionizes\nnumerous visual applications, but existing systems have yet to formally address video-centric tasks\nfrom a data-centric perspective using learning machines.\nOur initial video-centric multimodal dialogue system 1 formulates video understanding as a natural\nlanguage processing (NLP) question-answering, by textualizing video content with open-sourced\nvisual models. Despite demonstrating decent performance in short-term scenarios with clear objects\nand actions, transforming videos into textual descriptions inevitably results in visual information\nloss and over-simplification of spatiotemporal complexities. Additionally, almost all utilized vision\nmodels struggle with spatiotemporal reasoning, event localization, and causal relationship inference\nwithin videos.\nTo tackle these challenges, we improve our initial dialogue system and introduce a groundbreaking\nchat-centric video understanding system that leverages state-of-the-art techniques from both video\nand language domains. Our approach creates a full loop, integrating video and language foundation\n* Equal contribution.\n\u2020 Corresponding authors.\n1https://github.com/OpenGVLab/Ask-Anything, released on April 15, 2023.\narXiv:2305.06355v2  [cs.CV]  4 Jan 2024\nmodels in a learnable manner from a model perspective, and provides all techniques required to learn\nthe system from a data perspective.\nWe begin by presenting our novel video-centric multimodal dialogue system. We propose an innova-\ntive system architecture that combines video foundation models and large language models (LLMs)\nthrough a learnable neural interface. By a two-stage lightweight training (with only spatiotemporal\nand video-language alignment modules) on large-scale video-text datasets and self-built video in-\nstruction ones, our method excels in spatiotemporal perception & reasoning, and causal inference,\nmarking the first attempt to create a fully learnable and efficient video understanding system that\nfacilitates effective communication.\nWe introduce a novel video-centric multimodal instruction fine-tuning dataset. We create a unique\ndataset comprising thousands of videos paired with detailed textual descriptions and conversations\ngenerated using dense captions fed to ChatGPT in temporal order. This dataset emphasizes spa-\ntiotemporal objects, actions, events, and causal relationships, offering a valuable resource for training\nvideo-centric multimodal dialogue systems.\nThrough these contributions, our work pioneers new frontiers in video and natural language processing\nintegration. By developing a new and effective chat-centric video understanding dialogue system, we\npave the way for a wide range of applications across various domains while setting a standard for\nfuture research in this field. Our research not only pushes the boundaries of video understanding and\nreasoning but also offers protocols for both academic and industrial communities.\n2\nRelated Work\nVideo Foundation Models\nLarge-scale video-text pretraining coupled with downstream task\nfine-tuning has emerged as the standard paradigm in the video-language domain [27, 23, 55,\n49, 21, 20, 55, 49, 13, 11, 36, 51, 38, 60, 46, 5]. Early methods [38, 60] employed pretrained\nvisual and language encoders to derive offline video and text features; however, more recent ap-\nproaches [23, 27, 13, 11, 41, 44] have demonstrated the effectiveness of end-to-end training. Addition-\nally, prevalent techniques often encompass two or three pretraining tasks, such as masked language\nmodeling [22], video-text matching [43], video-text contrastive learning [49, 46], masked video\nmodeling [41, 44, 46] and video-text masked modeling [12]. Within the realm of video multimodal\ntasks, VIOLET [12] integrates masked language and masked video modeling, while All-in-one [43]\nsuggests a unified video-language pretraining methodology using a shared backbone, and LAVEN-\nDER [22] consolidates the tasks through masked language modeling. Although these approaches\nyield impressive results in multimodal benchmarks, their training relies on limited video-text data,\nwhich leads to difficulties in video-only tasks such as action recognition. On the other hand, MERLOT\nReserve [54] compiles 20 million video-text-audio pairs for training joint video representations via\ncontrastive span matching, thereby establishing state-of-the-art outcomes in video recognition and\nvisual commonsense reasoning.\nLarge Language Models\nRecent advances in large language models (LLMs) [30, 29, 3, 42, 7,\n10, 40] have showcased remarkable capabilities such as language generation, in-context learning,\netc. These abilities enable LLMs to tackle complex tasks with user prompts in a zero-shot fashion.\nGPT-3 [3] shows notable zero-shot performance across numerous benchmarks. InstructGPT mod-\nels [32] are finetuned using datasets containing prompts with corresponding human-annotated desired\nbehavior. This results in better alignment with users, improved output quality compared to GPT-3,\nincreased truthfulness, and reduced risks. Instruction-tuned models also present remarkable gener-\nalization capacity for zero-shot tasks. Therefore, instruction-tuning [28, 8] is crucial in leveraging\nLLMs\u2019 potential. Besides of GPT family [30, 29, 3], there are multiple LLMs, including OPT [57],\nLLaMA [42], MOSS [9], and GLM [56], providing high-performance, open-source resources that\ncan be finetuned for various purposes. For instance, Alpaca [40] proposes a self-instruct framework\nto instruction-tune LLaMA models without heavily relying on human-authored instruction data.\nLLMs for Multimodal Understanding\nThe accomplishments of LLMs have accelerated the\ncreation of AI systems that merge vision models with LLMs to enable multimodal reasoning and ac-\ntion [25, 58, 52, 1, 19, 26, 47, 50, 37, 24, 26]. Flamingo [1] pioneered this approach by capitalizing on\nboth vision and language models using web-scale image-text interwoven data, unveiling exceptional\nzero-shot image-text abilities in a conversational format for the first time. The study in [14] demon-\n2\nVideo Foundation\nModels\nToken\nInterface\nPerception\nTools\nInternVideo\nTag2Text, GRiT\nT5\nWhisper\nLarge Language\nModels\nChatGPT, LLAMA\nVicuna, MOSS, \u2026\nQ: Is he singing and \nrapping in the video?\nA: No, he is not singing or rapping in \nthis video. He is playing basketball and \ndancing in some parts of the video.\nVideoChat-Text\nVideoChat-Embed\nVideo\nEmbedding\nVideo\nDescription\nvideo class, video caption\n00:00-00:02: clip caption, clip tags,\ndense caption, subtitle\u2026\n00:02-00:03: clip caption, clip tags,\ndense caption, subtitle\u2026\nInternVideo\nVideo Content\nFigure 1: Framework. VideoChat-Text textualizes videos in stream. VideoChat-Embed encodes\nvideos as embeddings. Both video content can be input in LLMs for multimodal understanding.\nstrates that Kosmos-1 models are naturally equipped to tackle a broad array of perception-intensive\ntasks, including visual dialogue, visual explanation, visual question answering, image captioning,\nbasic math equations, OCR, and zero-shot image classification using descriptions. Visual instruction\ntuning introduces an innovative technique for refining large language models on visual instruction\ntasks, enabling pretrained BLIP and Vicuna to nearly match GPT-4 level conversation performance for\nimage-based tasks [25]. MiniGPT-4 is a multimodal large language model, fine-tuned on multimodal\ntasks, and exhibits respectable zero-shot image comprehension in dialogues [58].\n3\nVideoChat\nVideoChat unifies video-related tasks into the formulation of multiple-round video question answering,\nin which tasks are defined by words in a live inference and no or a few instances are given for learning.\nIn this formulation, we treat an LLM as a universal video task decoder, turning video-related\ndescriptions or embeddings into human-understandable text. This procedure is user-friendly in\nemploying foundation models to address various video applications.\nFormally, we extract concepts from videos using vision models as:\n[E]j\ni = f j\nimg(Ii)\nor\nEj = f j\nvid(V)\nw.r.t.\nV = [Ii]i=1,2,...,T ,\n(1)\nwhere E denotes a text description or embedding according to context, f j\nimg denotes the jth image\nmodel to predict human-readable annotations or visual feature, while I and V denote an image and\nvideo, respectively. Then we decode the task prediction from a LLM based on user\u2019s question as:\nWa\nt = fllm(E|Wq\n\u2264t, Wa\n<t),\n(2)\nwhere Wa\nt and Wq\n\u2264t stand for the answers from the LLM at the round t and all questions given by\nusers before round t, respectively. fllm denotes an LLM model.\nIn technical terms, an ideal end-to-end chat-centric video understanding system should utilize a\nvideo/vision model (an encoder) to convert visual sequences into latent features for LLM, guaranteeing\nthe system\u2019s overall differentiability. Prior to this, we verify the efficacy of LLM as a universal video\ntask interpreter through our proposed VideoChat-Text (Section 3.1). This method transforms videos\ninto textual streams for subsequent discrimination/reasoning tasks using LLMs by incorporating\nvarious open-source vision models. While VideoChat-Text can tackle typical spatiotemporal tasks\nsuch as spatial and temporal perception, it falls short in comprehending intricate temporal reasoning\nand causal inference. Therefore, we introduce VideoChat-Embed (Section 3.2), a multimodal system\nthat combines both video and language foundation models. Finetuned with video instruction data, it\nsignificantly enhances performance in higher-order temporal assignments. We will describe these\ntwo approaches in the following sections.\n3.1\nVideoChat-Text\n: VideoChat by Textualizing Videos in Stream\nWe employ several vision models to convert video data into textual format. Subsequently, we create\npurpose-built prompts to temporally structure the predicted text. Ultimately, we rely on a pretrained\nLLM to address user-specified tasks by responding to questions based on video text descriptions.\n3\nVideo Class, Video Caption\n00:00-00:02 Clip Caption, Clip Tag, Dense Caption, Video Subtitle...\n00:02-00:03 Clip Caption, Clip Tag, Dense Caption, Video Subtitle...\n00:03-00:06 Clip Caption, Clip Tag, Dense Caption, Video Subtitle...\n...\nanswering questions, a man and a woman sitting on a couch in a living room with a table in\nfront of them.\n00:00-00:11 a man and a girl sitting on a couch in a living room.\na lamp with a white shadea woman sitting at a table: [446, 155, 710, 476]; man wearing a\nplaid shirt: [361, 44, 581, 337]; man sitting on couch: [10, 63, 324, 350]; the tie is grey:\n[441, 150, 486, 280]; a glass of beer: [38, 305, 77, 367]; a stack of magazines: [28, 350,\n180, 394]; a white tablecloth: [0, 334, 626, 476]; stainless steel oven: [1, 55, 150, 142]; a\nbrown tie on a man: [144, 168, 191, 270]; the couch is white: [0, 119, 730, 472]; a gray\nbinder: [0, 377, 157, 411]; a white couch: [768, 350, 848, 477]; a lamp with a white shade:\n[582, 26, 713, 195];\n00:00-00:02: Hey, Pheebs, you gonna have the rest of that Pop-Tart?\n00:02-00:03: Pheebs?\n00:03-00:09: Does anyone want the rest of this Pop-Tart?\n00:09-00:11: Hey, I might.\nTable 1: Video description with perception models. We use perception models to obtain the video\nclip action class, clip/video caption, and dense caption as well as a subtitle. Then we organize them\nin a template to generate textualizing videos.\nIn particular, for a given video, we use ffmpeg to extract key frames from the video at a low FPS,\nresulting in T video frames and associated audio. By feeding the extracted frames and audio into\nvarious models, we acquire action labels, frame summaries, video tags, comprehensive descriptions,\nobject positional coordinates, video narratives, timestamps, and other segment-related details. We\nthen consolidate related content in the captions considering the timing and generate a timestamped\nvideo text description. We will first outline the vision models and prompt schematics employed, and\nthen conclude with an analysis of VideoChat-Text.\n3.1.1\nPerception Models\nUtilizing a combination of video and image models [46, 34, 20, 48, 15, 33], we analyze videos from\nvarious aspects such as actions [46, 20, 21] (with InternVideo [46]), objects [45, 15], object annota-\ntions with positions [48], and more. While the majority of these models\u2019 outputs are comparatively\nindependent, we utilize the pretrained T5 language model [34] to refine their descriptions for improved\nclarity. Moreover, we integrate the Whisper [33] speech recognition model into VideoChat-Text to\ncapitalize on audio data within videos, further enhancing the richness of video descriptions.\n3.1.2\nPrompt System\nWe process the video into different visual models to obtain different textualizing videos and then\norganize them together in a template (Table 1) as inputs to an LLM. Then, we present the LLM with\nthe context that we instruct it to pretend to watch the given video through the input formatted texts\n4\n(the structured video knowledge from perception models) and then chat with users. Such prompt is\nshown in Table 2.\nYou are a chatbot that conducts conversations based on video contexts. You mainly answer\nbased on the given contexts, and you can also modify the content according to the tag\ninformation, and you can also answer the relevant knowledge of the person or object\ncontained in the video. The timing description is a description every 1/FPS second, so\nthat you can convert it into time. When describing, please mainly refer to the timing\ndescription. Dense caption is to give content every five seconds, you can disambiguate\nthem in timing. But you don\u2019t create a video plot out of nothing.\nBegin!\nVideo contexts in temporal order: textualizing_videos\nQuestion: question\nTable 2: System prompt. It allows the LLM to understand textualizing videos and respond according\nto relevant content in the document, avoiding answering questions that do not relate to the video.\nAnalysis\nLite perception models enable VideoChat-Text to convert videos into time-stamped text\nat 1 FPS, processing a 10-second video clip in about 2 seconds using an NVIDIA-A10 GPU. It\ncommunicates with users through an LLM. However, using text as the communication medium\nrestricts the representation capabilities of the perception models, as it limits their decoders. To\nprovide richer visual information from videos to the LLM, we must employ more advanced and more\nperception models, which may conflict with VideoChat-Text\u2019s efficiency. Additionally, VideoChat-\nText has limited potential to benefit from popular visual instruction tuning [25].\n3.2\nVideoChat-Embed\n: VideoChat by Encoding Videos as Embeddings\nVideoChat-Embed is an end-to-end model designed to handle video-based dialogue. It employs an\narchitecture that combines both video and language foundation models with an addition learnable\nVideo-Language Token Interface (VLTF). To achieve better cross-modality optimization, the model\nincorporates language-friendly video foundation models, inspired by [53, 49, 46, 21]. Considering\nthe video redundancy [41], we introduce the VLTF, using cross-attention to compress the video\ntokens. It is tuned with video-text data for video-to-language representation alignment. Finally, the\nvideo tokens, user queries, and dialogue context are input into the LLM for communication.\nVisual\nEncoder\nGMHRA\nQFormer\nLinear\nLarge\nLanguage\nModel\n###Human: Q\n###Assistant: A\nVideo\nEmbedding\nQ\nK/V\n10M\n15M\nStage1: Alignment\nreasoning\nStage2: Instruction Tuning\nconversation\ndetailed description\nconcise description\n7K\n4K\n2K\n2K\n3K\nInternVideo\nN  Architecture\nO  Data\n(a) Architecture\nVisual\nEncoder\nGMHRA\nQFormer\nLinear\nLarge\nLanguage\nModel\n###Human: Q\n###Assistant: A\nVideo\nEmbedding\nQ\nK/V\n10M\n15M\nStage1: Alignment\nreasoning\nStage2: Instruction Tuning\nconversation\ndetailed description\nconcise description\n7K\n4K\n2K\n2K\n3K\nInternVideo\nN  Architecture\nO  Data\n(b) Data\nFigure 2: Architecture and training paradigm of VideoChat-Embed. It is built on BLIP-2 [18]\nand StableVicuna [10]. The training contains two-stage alignment and instruction tuning.\n3.2.1\nArchitecture\nIn this paper, we instantiate the VideoChat-Embed based on BLIP-2 [18] and StableVicuna [10](Figure\n2a). Concretely, we incorporate the pretrained ViT-G [39] with Global Multi-Head Relation Aggrega-\ntor (GMHRA), a temporal modeling module used in InternVideo [46] and UniFormerV2 [20]. For\nthe token interface, we employ the pretrained QFormer with extra linear projection, supplemented\nby additional query tokens to account for video context modeling. This allows us to obtain compact\nLLM-compatible video embeddings for dialogues.\nWhen training, we freeze most of the parameters except the newly incorporated GMHRA, queries\nand linear projection. Inspired by [21], we introduce image data for joint training (Figure 2b). In\n5\nStage1, we align the video encoder with LLM via large-scale video-text fine-tuning. In Stage2, we\ntune the system with two types of video instruction data: in-depth video descriptions and video\nquestion-answer pairs. The following section will describe the process of generating instruction data\nand present the details of two-stage training paradigm.\n3.2.2\nInstruction Data\nWe build a video-centric multimodal instruction data based on WebVid-10M [2]. The corresponding\ndetailed descriptions and question-answer generations are produced by ChatGPT based on video\ntext (aided by VideoChat-Text) with several prompts concerning spatiotemporal features. Compared\nwith detailed video descriptions, video conversations are introduced to further improve the diversity,\ntemporal and casual features in the video instruction data.\nDetailed Video Descriptions\nWe condense the provided video description into a video narrative\nemploying GPT-4, as shown in Table 5. This highlights the temporal aspects of the video by\nillustrating its progression over time. The associated prompts can be found in Table 3 and 4. The\nfirst converts the various predicted textual labels into a cohesive, evolving story, while the second\none refines the narrative to improve clarity and coherence, as well as minimize hallucination. We\ngenerated a total of 7K descriptions from randomly chosen videos.\nVideo Conversations\nWith the video description, we generate multi-round dialogues with three\ntypes of prompts concerning descriptive, temporal, and causal content for videos with ChatGPT.\nThe descriptive part mostly inherits key points from LLaVA [25]. For the temporal and causal parts,\nwe propose prompts (Table 6) focus on temporal perception/reasoning and explanation/uncovering\nintentions/causes, respectively. We produced multi-round dialogues from 4K randomly chosen videos.\nOne example of the video conversation can be found in Table 7\n3.2.3\nTwo-stage Training\nMotived by MiniGPT-4 [58] and LLaVA [25], we have designed a two-stage joint training paradigm.\nThis approach allows us to benefit from readily-available image instruction data, creating a system\ncapable of handling both images and videos with shared spatial perception and reasoning capacity.\nStage1: Alignment.\nTo strike a balance between training convergency and efficiency we introduce\n25M vision-text pairs for one epoch of fine-tuning, The data consists 10M video-text pairs from\nWebVid-10M, and 15M image-text pairs from COCO Caption [6], Visual Genome [17], SBU\nCaptions [31], CC3M [35] and CC12M [4]. The input prompts for LLMs are as followed:\n\u2022 \u201c###Human: <Video>video_embed</Video> video_instruction ###Assistant:\u201d\n\u2022 \u201c###Human: <Image>image_embed</Image> image_instruction ###Assistant:\u201d\nThe video_embed and image_embed are the output from the token interface. Meanwhile the\nvideo_instruction and image_instruction provide concise video and image descriptions\nrandomly sampled from predefined instructions in Table 8. Language models receive corresponding\nvisual descriptions as answers.\nStage2: Instruction tuning.\nAs discussed in Section 3.2.2, our self-built video instruction data\nconsists of 7K detailed video descriptions and 4K video conversations. To improve spatial perception\nand reasoning capabilities, we also gather 3K detailed image descriptions from MiniGPT-4 [58], 2K\nimage conversations, and 2K image reasoning tasks from LLaVA [25]. With this 18K data collection,\nwe tune the system for 3 epochs. Note we include temporal reasoning sampling information for video\ndata: \u201cThe video contains T frames sampled at t0, t1, ..., tT seconds.\u201d\n4\nExperiments\n4.1\nQualitative Analysis\nWe give some case studies at this stage. Besides of our VideoChat-Text\nand VideoChat-Embed\n,\nwe make qualitative comparisons with LLaVa [25], miniGPT-4\n[58], and mPLUG-owl [52].\n6\nGive you a video of origin_caption. The content of the video in temporal order is:\ntextualizing_videos. Please use the sequence adverbs \"first\", \"next\", \"then\" and\n\"finally\" to describe this video in detail, but don\u2019t mention the specific time. Give as many\ndetails as possible. Say everything you see. The description should be more than 150 words\nand less than 200 words.\nTable 3: Prompts for detailed video descriptions. The origin_caption is generated from\nVideoChat-Text.\nFix the error in the given paragraph. Remove any repeating sentences, meaningless char-\nacters, not English sentences, and so on. Remove unnecessary repetition. Rewrite any\nincomplete sentences. Return directly the results without explanation. Return directly the\ninput paragraph if it is already correct without explanation.\nTable 4: Prompt for post-processing. Copied from miniGPT-4 [58].\n###Human: <Video>video_embed</Video> The video contains T frames sampled at\nt0, t1, ..., tT seconds.\n###Human: Portray the video with a rich, descriptive narrative, capturing its atmosphere\nand events.\n###Assistant: First, we see a young woman with long brown hair wearing a backpack\nstanding on the edge of a cliff, looking at the beautiful mountain scenery. She is in awe\nand stands there for a few seconds. Next, we see her turning around slowly, with her arms\nopen wide. Then, we see her taking a few steps back, still facing the canyon, with her\narms still open wide. She seems to be soaking in the grandeur of the view. Finally, she\nstands confidently at the edge of the cliff, her backpack still on her back, as if she has\nconquered the mountain. Throughout the video, we see the vast expanse of the canyon with\nthe mountains in the background. The sky is overcast in some parts and clear blue in others,\ncreating a beautiful contrast. Overall, the young woman seems to be thrilled with the view\nand enjoys every bit of it with open arms.\nTable 5: Example of using the detailed video description.\nSpatial Perception and Analysis\nIn Figure 7, our approach (VideoChat-Embed) accurately deduces\nthe corresponding music by recognizing Japanese-style clothing and determining the number of\nindividuals present. This confirms the system\u2019s ability to identify objects along with their properties,\nwhile also providing pertinent recommendations based on visual elements. Also, we give some\nimage-centric dialogue examples in Figure 10.\nTemporal Perception and Reasoning\nFigure 5, 7, and 9 demonstrate that VideoChat-Embed is\ncapable of performing accurate temporal perception and reasoning. In Figure 5, our system identifies\nactions over time in a zero-shot fashion, recognizing that the subject played basketball and engaged in\ndance movements within a specific timeframe. Additionally, it captures camera motion, showcasing\nits understanding of filming perspectives. In Figure 9, VideoChat-Text accurately identifies yoga in\nthe video and provides rational explanations for this activity (practice and enjoyment). Intriguingly,\nwhen questioned about the likelihood of the yoga practitioner falling, it asserts that proper safety\nprecautions were taken by the individual.\nCasual Inference\nIt is evident that VideoChat-Embed can infer causal relationships using spatiotem-\nporal clues, as demonstrated in Figure 5, 6, 7, and 4. In Figure 5, the model provides an impartial\ndescription of the video, primarily highlighting objects, actions, and emotions without commenting\n7\nAs an AI visual assistant, you are observing a single video. The description of the video\nis presented to you in chronological order, detailing object types, their locations (using\ncoordinates), attributes, interactions between objects, actions, and the environment. Based\non these descriptions, you are tasked with answering all questions as though you are directly\nwatching the video.\nCreate a dialogue between yourself and someone inquiring about the video. Make sure\nthe responses reflect the tone of a visual AI assistant actively observing the video and\nanswering questions. Include diverse queries and corresponding answers.\nIncorporate questions that address the visual content of the video, such as object types,\nattributes, object counting, actions, locations, relative positions between objects, and\nchanges in object actions or locations over time, as well as object interactions. Only include\nquestions with definitive answers:\n\u2022 Questions whose contents can be confidently observed and answered based on the video.\n\u2022 Questions whose absence from the video can be confidently determined.\nNext, encompass questions related to temporal perception and reasoning, such as inquiring\nabout what a person did before or after an event, or asking for specific timestamps of certain\nevents or actions\nAlso include complex questions relevant to the video\u2019s content, like those asking about\nthe background knowledge of objects or actions in the video, discussing events occurring\nin the video, delving into counterfactual topics (e.g., what might happen if a man lost his\nphone when he is actually playing with it in the video), seeking explanations for characters\u2019\nemotions or behaviors based on their experiences in the video, or predicting how the video\u2019s\nstory or scene will progress.\nSince you receive video descriptions while viewing the video, prioritize asking more\nquestions about visual changes over time and the reasons or causes behind these\nchanges rather than questions that can be inferred from a single frame.\nRemember not to inquire about uncertain details. When answering complex questions,\nprovide thorough answers, incorporating detailed examples or steps of reasoning to make\nthe content more persuasive and well-structured. Use multiple paragraphs if necessary. If a\nquestion cannot be answered based on the given descriptions, respond with \"The provided\nvideo does not present such information\" rather than indicating that the information comes\nfrom text descriptions.\nTable 6: Prompt for video conversations. The descriptive, temporal, and causal question specifi-\ncations are marked by cyan, blue, and purple, respectively. We encourage the chatbot to focus on\nquerying aspects related to videos rather than static images, as the highlighted part.\non the boy\u2019s dance style. To explain why the video is amusing, VideoChat-Embed cites the erratic\nand spontaneous nature of the boy\u2019s movements while also conveying an emotional assessment\nthat his dancing appears foolish, accounting for the unexpected humor within the clip. Empirically,\nwe confirm that these visually associated abstract concepts are derived from the video foundation\nmodel instead of being hallucinations produced by the utilized LLM. In Figure 6, VideoChat-Embed\naccurately assesses that a car accident occurred due to the collision of vehicles and the damage\nsustained to the front car\u2019s license plate visible to the camera. In Figure 7, VideoChat suggests pairing\nthe video with light, enjoyable music, as it can sense the girls\u2019 dancing rhythm and progression over\ntime. This demonstrates that our system can effectively capture and summarize abstract concepts\nrelated to movement patterns.\n4.2\nComparisons\nAs depicted in Figure 3, we present a comparison of our approach to recent image-based multimodal\ndialogue systems in image-related tasks, using a query example from the TVQA dataset. We\nassess this case through the online demos provided by each respective system. It is evident that\nour VideoChat-Embed correctly identifies the scene, while other systems inaccurately perceive the\nconversation setting as indoors. This result highlights the superior spatial perception abilities of\nVideoChat-Embed in relation to its counterparts. Furthermore, this proficiency remains consistent\nwhen dealing with a video from the same dataset, as demonstrated in the right-hand example.\n8\n00:01:09,836 --> 00:01:11,201\n(EXHALES)\n00:01:15,442 --> 00:01:18,843\n(Eve:)Do you think the guy who\ndid this to me feels bad?\nC: A couch in a living room.\n3\n00:01:15,442--> 00:01:18.843\n(Eve:)Do you think the guy who \ndid this to me feels bad?\nAnswer: B: A desk in an office.\n4\nLanguage Assistants(3)\nWhat was House sit on when \nEve asked if the guy felt bad? \nAccording to the subtitle of a \nvideo, and choose the best \nanswer matches the question.\nA: The floor in an office.\nB: A desk in an office.\nC: A couch in a living room.\nD: A park bench.\nE: The foot of the bed.\nOnly choose one answer.\nChoose the best answer \nmatches the question: What \nwas House sit on when Eve \nasked if the guy felt bad? \nA: The floor in an office.\nB: A desk in an office.\nC: A couch in a living room.\nD: A park bench.\nE: The foot of the bed.\nOnly choose one answer. \nThe best answer is A: The floor \nin an office.\nM\nThe correct answer is B: A desk \nin an office.\nImage Assistants(5)\nD: A park bench.\nA couch in a living room.\nVideo Assistants(3)\nChoose the best answer \nmatches the question: What \nwas House sit on when Eve \nasked if the guy felt bad? \nA: The floor in an office.\nB: A desk in an office.\nC: A couch in a living room.\nD: A park bench.\nE: The foot of the bed.\nOnly choose one answer. \nD: A park bench. In the video, \nHouse and Eve are sitting on a \npark bench in a park when Eve \nasks if the guy feels bad. There \nis a lake and a road nearby in \nthe background.\nD: A park bench.\n00:10\n\ud83c\udf0b\nFigure 3: In the TVQA dataset sample, subtitles are fed to language assistants [30, 29], images to the\nimage assistants [59, 25, 52], and videos to the VideoChat. While responding to user questions, only\nVideoChat utilizes information from videos.\nThis is a video of a young man singing with a microphone, wearing \na suit and tie. He is standing in front of a white curtain and a \nstained glass window. The video lasts 10 seconds.\nWhat\u2019s video of this?\nIt depends on their expectations. If they were expecting a longer\nor more engaging video, they may feel cheated if they don\u2019t get\nthat. However, if they just wanted a brief glimpse of the man\nsinging and it satisfies their curiosity, they may not feel cheated at\nall.\nUsually a person watching this video will feel cheated, \ndo you know why?\n00:06\nFigure 4: Video meme case. Click here to watch\nthe video.\nThe video shows a young man dancing in a kitchen, surrounded by \ncolorful cartoon characters. He wears a white shirt and blue pants \nand is dancing in front of the refrigerator. The video also shows \nthe kitchen\u2019s interior with a wooden floor and appliances. In some \nframes, the man is dancing in front of the cereal box, cookies, and \na can of peas. There are also other items, like a juice bottle, in the \nkitchen. The man dances joyfully, with a big smile on his face, and \nenjoys the music. Overall, the video captures the energy and \nenthusiasm of the dancer and creates a fun and lively atmosphere.\nDescribe the video.\nThe video is funny because the man\u2019s dancing is random and\nspontaneous, and he adds an element of humor to the ordinary\nkitchen setting. His dancing is unpredictable and silly, and it adds\nan element of surprise to the video. The man\u2019s joyful facial\nexpressions and childlike gestures make the video amusing and\nentertaining to watch. Additionally, the juxtaposition of the man\u2019s\ndancing with the serious and mundane objects in the kitchen\ncreates an interesting contrast and adds to the humor. The fact\nthat the man is dancing in front of food items in a kitchen is also a\nnod to our daily lives and the fact that we often find ourselves in\nthe kitchen dancing and singing along to music, making the video\nrelatable and funny to watch.\nExplain why this video is funny.\n00:09\nFigure 5: VideoChat can understand why the\nvideo is funny. Click here to watch the video.\n9\nYes, there is an accident in the video. You can see a silver car on the \nroad that appears to have been involved in an accident.\nIs an accident in the video?\nIn case someone is in the car, you should immediately call for\nemergency services and wait for the police to arrive. You should\nnot try to move the car or remove the person from the car unless\nthey are in danger.\nWhat to do if someone is in the car?\n00:05\nFigure 6: VideoChat finds the accident in the\nvideo and gives the advice to the person in the\ncar. Click here to watch the video.\nI believe that this video is most suitable for anime or Japanese-\nthemed music, particularly k-pop or anime openings. The bright \nand colorful costumes and dancers remind me of a cheerful, \nupbeat atmosphere. The music could complement that mood by \nbeing light and fun.\nWhat kind of music do you think suits this video?\nThere are 3 people in the video.\nHow many person in the video?\n00:10\nFigure 7: VideoChat observed the number of\npeople and their clothing styles in the space, and\nperceived temporal actions to recommend light\nand funny music. Click here to watch the video.\nThis video lasts 16 seconds.\nHow long is this video?\nThe video was filmed in an indoor gym, on a basketball court. You\ncan see the basketball court in the background of some shots, as\nwell as some large white letters with words painted on the wall.\nThere are also white walls and a grey floor, which suggest that the\nlocation is likely an indoor gym.\nWhere did he take this video?\nNo, he is not singing or rapping in this video. He is playing \nbasketball and dancing in some parts of the video.\nIs he singing and rapping in the video?\nThe man starts to dance in the second 9.8. He dances while \nwearing a brown sweater and black pants, and is wearing a black \nand white striped shirt.\nIn which second, he starts to dance?\nNo, the camera lens does not appear to be moving in this video.\nThe camera remains stationary and captures the actions of the\nman in the gym.\nIn some shots, the camera captures the basketball bouncing on the\nfloor, but this is due to the action of the man in the gym rather than\nany camera movement.\nOverall, the camera lens appears to remain fixed throughout the\nduration of the video.\nPlease describe whether the camera lens moving.\n00:16\nFigure\n8:\nMulti-turn\nconversation\nwith\nVideoChat-Embed. Click here to watch the\nvideo.\nShe is doing yoga on the roof in the video.\nWhat is she doing in the video?\nShe is wearing a black shirt and pants, and a black swimsuit.\nWhat is she wearing?\nShe is doing yoga on a roof.\nWhere is she doing yoga?\nIt appears to be on a roof because of the clear sky and the large \ngreen tree in the background.\nWhy do you think it\u2019s on a roof?\nYes, she appears to be safe as she is on a flat surface and there is \nno one else around. It is unlikely that she will fall down as she is \non a flat surface and appears to be taking the necessary safety \nprecautions.\nIs she safe when doing yoga? Will she fall down?  \nThis video appears to have been shot during the day, as the sky is \nclear and there is a large green tree in the background.\nWhen was this video shot? Was it taken at night?\nHer purpose in doing this appears to be to practice yoga and to \nenjoy the view from the rooftop.\nWhat is her purpose in doing this?\n00:10\nFigure\n9:\nMulti-turn\nconversation\nwith\nVideoChat-Text. Click here to watch the video.\nThis meme is funny because it shows a small dog laying on the \nfloor with a person\u2019s feet in the background. The dog looks very \nrelaxed and content, and the caption reads \u201cjust Monday\u201d. This \nimplies that the dog is feeling the same way as many people do on \nMondays, which is a common feeling of exhaustion and lack of \nmotivation. The meme is relatable and humorous because it plays \non the idea that Mondays are often seen as a day that people do \nnot look forward to, and this dog is no exception. It is also funny \nbecause it shows a small dog in a very human - like position, which \nadds to the humor of the meme.\nExplain why this meme is funny.\nThis meme is funny because it features a child looking at a camera \nwhile wearing a yellow shirt and holding a phone in their hand. \nThe text in the image is written in Chinese, making it humorous for \nthose who can read it. The meme also plays on the idea of a child \nbeing confused or surprised by the camera, adding to the comedic \nelement. The use of a child as the subject in the meme is also a \ncommon trope in humor, as it can be relatable and amusing to \nviewers. Overall, the combination of the child\u2019s expression, the \nChinese text, and the phone in their hand creates a humorous and \nentertaining meme.\nExplain why this meme is funny.\nThis is a very unusual image because the man is standing on the \nback of a parked car while wearing a yellow shirt. He seems to be \nironing clothes on the back of the car, which is parked on the street \nin front of a tall building. The yellow SUV has its rear door open, \npossibly allowing the man to reach the clothes he is ironing. The \ncar is parked on a busy street with other cars and a yellow taxi \ndriving by, which adds to the overall scene\u2019s busyness and hustle \nand bustle. The man\u2019s unique job, ironing clothes on the back of a \ncar on a busy street, seems to be part of the unique experience of \nthe city\u2019s daily life.\nWhat is unusual about this image?\nFigure 10: Cases of Image Comprehension. Please zoom in for details.\n10\n###Human: <Video>video_embed</Video> The video contains T frames sampled at\nt0, t1, ..., tT seconds.\n###Human: What is happening in the video?\n###Assistant: In the video, we see a silhouette of a young woman holding a smartphone\nand taking pictures of the sunset over the sea on the beach at night. Then, there are several\nother silhouettes of people taking pictures and looking at their phones.\n###Human: Can you describe the interactions between the objects in the video?\n###Assistant: The silhouettes of people in the video are holding smartphones, and she\nis taking pictures of the sunset over the sea on the beach at night.\n###Human: What is the location of the scene?\n###Assistant: The scene is located on a beach at night near the sea.\n###Human: Can you tell me about any changes that happen in the video over time?\n###Assistant: The color of the sky changes from orange to grey as the video progresses.\nAdditionally, the actions of the silhouettes of people change as they take pictures and look\nat their phones on the beach.\nTable 7: Example of using the video conversation.\n5\nConclusion\nWe have embarked on a pioneering investigation into general video understanding by developing\nVideoChat, a multimodal dialogue system specifically designed for videos. Two versions of VideoChat\nare implemented: a text-based version demonstrates the effectiveness of employing large language\nmodels as universal decoders for video tasks, while an end-to-end version makes a preliminary\nattempt to tackle video understanding using instructed video-to-text formulation. Our end-to-end\nsolution effectively merges video foundation models with large language models through a trainable\nneural interface. To enhance the system\u2019s performance, we introduced a video-centric instructional\ndataset, which highlights spatiotemporal reasoning and causality by offering a learning resource\nfor video-based multimodal dialogue systems. Initial qualitative evaluations showcase our system\u2019s\npromising capabilities across various video applications and drive its ongoing advancement.\nLimitations\n1) Both VideoChat-Text and VideoChat-Embed struggle with managing long-term\nvideos (\u2265 1min). On one side, effectively and efficiently modeling the context of long videos remains\na complex and persistent research issue. Conversely, balancing response time, GPU memory usage,\nand user expectations for system performance becomes challenging when striving to provide user-\nfriendly interactions while processing longer videos. 2) Our system\u2019s capacities for temporal and\ncausal reasoning remain rudimentary. These limitations stem from the current scale of our instruction\ndata and its construction approaches, as well as the overall system scale and the models employed. 3)\nAddressing performance disparities in time-sensitive and performance-critical applications, such as\negocentric task instruction prediction and intelligent monitoring, is an ongoing challenge.\nFuture Works\nOur future works lie in 1) scaling video foundation models both in capacity and\ndata for better spatiotemporal modeling, 2) video-centric multimodal training data and reasoning\nbenchmark for evaluations at scale, and 3) long-term video processing techniques.\nReferences\n[1] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell\nWortsman, and Ludwig Schmidt. Openflamingo, March 2023.\n[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image\nencoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021.\n11\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in Neural Information Processing Systems, 33:1877\u20131901, 2020.\n[4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.\n[5] Guo Chen, Sen Xing, Zhe Chen, Yi Wang, Kunchang Li, Yizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong\nZheng, Bingkun Huang, et al. Internvideo-ego4d: A pack of champion solutions to ego4d challenges.\narXiv preprint arXiv:2211.09529, 2022.\n[6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015.\n[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality, 2023.\n[8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022.\n[9] MOSS contributors. Moss. https://github.com/OpenLMLab/MOSS, 2023.\n[10] StableLM contributors. Stablelm: Stability ai language models. https://github.com/stability-AI/\nstableLM, 2023.\n[11] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu,\nPengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training end-to-end vision-and-\nlanguage transformers. In CVPR, 2022.\n[12] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-\nto-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681,\n2021.\n[13] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nScaling up vision-language pre-training for image captioning. In CVPR, 2022.\n[14] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with\nlanguage models. arXiv preprint arXiv:2302.14045, 2023.\n[15] Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong\nGuo, and Lei Zhang. Tag2text: Guiding vision-language model via image tagging. arXiv preprint\narXiv:2303.05657, 2023.\n[16] Zehuan Huang, Haoran Feng, Chongzhi Zhang, Lu Sheng, Ziwei Liu, and Jing Shao. Dolphin: General\nvideo interaction platform based on llms, 2023. https://github.com/kaleido-lab/dolphin.\n[17] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. International journal of computer vision, 123:32\u201373, 2017.\n[18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In International Conference on Machine\nLearning, pages 12888\u201312900. PMLR, 2022.\n[20] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Limin Wang, and Yu Qiao. Uniformerv2:\nSpatiotemporal learning by arming image vits with video uniformer. arXiv preprint arXiv:2211.09552,\n2022.\n[21] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher:\nTowards training-efficient video foundation models. arXiv preprint arXiv:2303.16058, 2023.\n12\n[22] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender:\nUnifying video-language understanding as masked language modeling. arXiv preprint arXiv:2206.07160,\n2022.\n[23] Tianhao Li and Limin Wang. Learning spatiotemporal features via video and text pair discrimination.\nCoRR, abs/2001.05691, 2020.\n[24] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of\napis. arXiv preprint arXiv:2303.16434, 2023.\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. 2023.\n[26] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang\nYang, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen, Xue Yang, Xizhou Zhu, Yali Wang, Limin Wang,\nPing Luo, Jifeng Dai, and Yu Qiao. Internchat: Solving vision-centric tasks by interacting with chatbots\nbeyond language. https://arxiv.org/abs/2305.05662, 2023.\n[27] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman.\nEnd-to-end learning of visual representations from uncurated instructional videos. In CVPR, 2020.\n[28] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via\nnatural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021.\n[29] OpenAI. Gpt-4 technical report. arXiv, 2023.\n[30] TB OpenAI. Chatgpt: Optimizing language models for dialogue. OpenAI, 2022.\n[31] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned\nphotographs. Advances in neural information processing systems, 24, 2011.\n[32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\n[33] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust\nspeech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356, 2022.\n[34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n[35] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018.\n[36] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and\nKurt Keutzer. How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383,\n2021.\n[37] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\n[38] Chen Sun, Austin Myers, Carl Vondrick, Kevin P. Murphy, and Cordelia Schmid. Videobert: A joint model\nfor video and language representation learning. ICCV, 2019.\n[39] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques\nfor clip at scale. arXiv preprint arXiv:2303.15389, 2023.\n[40] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.\ncom/tatsu-lab/stanford_alpaca, 2023.\n[41] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient\nlearners for self-supervised video pre-training. In Advances in Neural Information Processing Systems,\n2022.\n[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n13\n[43] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan,\nXiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv\npreprint arXiv:2203.07303, 2022.\n[44] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao.\nVideomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023.\n[45] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\nconvolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.\n[46] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu,\nYi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao.\nInternvideo: General video foundation models via generative and discriminative learning. arXiv preprint\narXiv:2212.03191, 2022.\n[47] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.\n[48] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang.\nGrit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280,\n2022.\n[49] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke\nZettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text\nunderstanding. arXiv preprint arXiv:2109.14084, 2021.\n[50] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and\naction. ArXiv, abs/2303.11381, 2023.\n[51] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,\nXin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint\narXiv:2111.07783, 2021.\n[52] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi,\nJi Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality,\n2023.\n[53] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong\nHuang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv\npreprint arXiv:2111.11432, 2021.\n[54] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya\nKusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through\nvision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 16375\u201316387, 2022.\n[55] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin\nChoi. Merlot: Multimodal neural script knowledge models. NeurIPS, 2021.\n[56] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414,\n2022.\n[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n[58] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[59] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[60] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. CVPR, 2020.\n14\nA\nAppendix\nInstruction for a brief description.\nFollowing the brief image instruction in LLaVA, we generate\nthe video instruction with the aid of ChatGPT as shown in Table 8. In Stage1, we randomly sample\nthe instruction to generate brief descriptions of images and videos.\nInstruction for a detailed description.\nFollowing the detailed image instruction in LLaVA, we\ngenerate the video instruction with the help of ChatGPT as shown in Table 9 To build the instruction\ndata used in Stage2, we randomly sample the instruction and combine it with the detailed descriptions.\n\u2022 \"Describe the following image concisely.\"\n\u2022 \"Provide a brief description of the given image.\"\n\u2022 \"Offer a succinct explanation of the picture presented.\"\n\u2022 \"Summarize the visual content of the following image.\"\n\u2022 \"Give a short and clear explanation of the subsequent image.\"\n\u2022 \"Share a concise interpretation of the image provided.\"\n\u2022 \"Present a compact description of the photo\u2019s key features.\"\n\u2022 \"Relay a brief, clear account of the picture shown.\"\n\u2022 \"Render a clear and concise summary of the photo below.\"\n\u2022 \"Write a terse but informative summary of the following picture.\"\n\u2022 \"Create a compact narrative representing the image presented.\"\n\u2022 \"Describe the following video concisely.\"\n\u2022 \"Provide a brief description of the given video clip.\"\n\u2022 \"Offer a succinct explanation of the footage presented.\"\n\u2022 \"Summarize the visual content of the following video.\"\n\u2022 \"Give a short and clear explanation of the subsequent video clip.\"\n\u2022 \"Share a concise interpretation of the video provided.\"\n\u2022 \"Present a compact description of the clip\u2019s key features.\"\n\u2022 \"Relay a brief, clear account of the video shown.\"\n\u2022 \"Render a clear and concise summary of the video below.\"\n\u2022 \"Write a terse but informative summary of the following video clip.\"\n\u2022 \"Create a compact narrative representing the video presented.\"\nTable 8: The list of instructions for brief image and video description. The image list is inherited\nfrom LLaVA [25]. The video list is generated by ChatGPT with examples of the image list.\n15\n\u2022 \"Describe the following image in detail.\"\n\u2022 \"Provide a detailed description of the given image.\"\n\u2022 \"Give an elaborate explanation of the image you see.\"\n\u2022 \"Share a comprehensive rundown of the presented image.\"\n\u2022 \"Offer a thorough analysis of the image.\"\n\u2022 \"Explain the various aspects of the image before you.\"\n\u2022 \"Clarify the contents of the displayed image with great detail.\"\n\u2022 \"Characterize the image using a well-detailed description.\"\n\u2022 \"Break down the elements of the image in a detailed manner.\"\n\u2022 \"Walk through the important details of the image.\"\n\u2022 \"Portray the image with a rich, descriptive narrative.\"\n\u2022 \"Narrate the contents of the image with precision.\"\n\u2022 \"Analyze the image in a comprehensive and detailed manner.\"\n\u2022 \"Illustrate the image through a descriptive explanation.\"\n\u2022 \"Examine the image closely and share its details.\"\n\u2022 \"Write an exhaustive depiction of the given image.\"\n\u2022 \"Describe the following video in detail, including the actions and scenes.\"\n\u2022 \"Provide a detailed description of the given video, capturing its key moments.\"\n\u2022 \"Give an elaborate explanation of the video you see, including the events and characters.\"\n\u2022 \"Share a comprehensive rundown of the presented video, highlighting its main sequences.\"\n\u2022 \"Offer a thorough analysis of the video, discussing its various elements and storyline.\"\n\u2022 \"Explain the various aspects of the video before you, including the setting and actions.\"\n\u2022 \"Clarify the contents of the displayed video with great detail, focusing on its progression.\"\n\u2022 \"Characterize the video using a well-detailed description, capturing its essence and events.\"\n\u2022 \"Break down the elements of the video in a detailed manner, discussing its key components.\"\n\u2022 \"Walk through the important details of the video, describing its scenes and characters.\"\n\u2022 \"Portray the video with a rich, descriptive narrative, capturing its atmosphere and events.\"\n\u2022 \"Narrate the contents of the video with precision, focusing on its storyline and visuals.\"\n\u2022 \"Analyze the video in a comprehensive and detailed manner, discussing its themes and elements.\"\n\u2022 \"Illustrate the video through a descriptive explanation, painting a vivid picture of its content.\"\n\u2022 \"Examine the video closely and share its details, including the actions, characters, and setting.\"\n\u2022 \"Write an exhaustive depiction of the given video, capturing its essence and key moments.\"\nTable 9: The list of instructions for detailed image and video description. The image list is\ninherited from LLaVA [25]. The video list is generated by ChatGPT with examples of the image list.\n16\n"
  },
  {
    "title": "Sketching the Future (STF): Applying Conditional Control Techniques to Text-to-Video Models",
    "link": "https://arxiv.org/pdf/2305.05845.pdf",
    "upvote": "2",
    "text": "Sketching the Future (STF): Applying Conditional\nControl Techniques to Text-to-Video Models\nRohan Dhesikan\nCarnegie Mellon Univeristy\nrdhesika@andrew.cmu.edu\nVignesh Rajmohan\nCarnegie Mellon University\nvrajmon15@gmail.com\nvrajmoha@andrew.cmu.edu\nAbstract\nThe proliferation of video content demands ef\ufb01cient and \ufb02exible neural network-\nbased approaches for generating new video content. In this paper, we propose a\nnovel approach that combines zero-shot text-to-video generation with ControlNet\n[2] to improve the output of these models. Our method takes multiple sketched\nframes as input and generates video output that matches the \ufb02ow of these frames,\nbuilding upon the Text-to-Video Zero architecture [1] and incorporating Control-\nNet to enable additional input conditions. By \ufb01rst interpolating frames between\nthe inputted sketches and then running Text-to-Video Zero using the new inter-\npolated frames video as the control technique, we leverage the bene\ufb01ts of both\nzero-shot text-to-video generation and the robust control provided by ControlNet.\nExperiments demonstrate that our method excels at producing high-quality and\nremarkably consistent video content that more accurately aligns with the user\u2019s\nintended motion for the subject within the video. We provide a comprehensive\nresource package, including a demo video, project website, open-source GitHub\nrepository, and a Colab playground to foster further research and application of our\nproposed method.\n1\nIntroduction\nThe rapid growth of video content on the internet has led to a surge of interest in neural network-based\napproaches for generating new video content. However, training Text-to-Video models is challenging\ndue to the lack of open datasets of labeled video data. Furthermore, generating video from existing\nText-to-Video models is dif\ufb01cult due to the nature of prompts. To address these challenges, we\npropose a novel approach that combines the bene\ufb01ts of zero-shot text-to-video generation with the\nrobust control provided by ControlNet [2].\nOur model is built upon the Text-to-Video Zero architecture [1], which allows for low-cost video\ngeneration by leveraging the power of existing text-to-image synthesis methods, such as Stable\nDiffusion [3]. The key modi\ufb01cations we introduce include enriching the latent codes of the generated\nframes with motion dynamics and reprogramming frame-level self-attention using a new cross-frame\nattention mechanism. These modi\ufb01cations ensure global scene and background time consistency, as\nwell as the preservation of the context, appearance, and identity of the foreground object.\nTo further enhance the control over the generated video content, we incorporate the ControlNet\nstructure. ControlNet enables additional input conditions, such as edge maps, segmentation maps,\nand keypoints, and can be trained on a small dataset in an end-to-end manner. By combining Text-\nto-Video Zero with ControlNet, we create a powerful and \ufb02exible framework for generating and\ncontrolling video content using minimal resources.\nOur model takes multiple sketched frames as input and generates video output that matches the \ufb02ow\nof these frames. We \ufb01rst interpolate frames between the inputted sketches and then run Text-to-Video\narXiv:2305.05845v1  [cs.CV]  10 May 2023\nZero using the new interpolated frames video as the control technique. Experiments show that our\nmethod is capable of generating high-quality and remarkably consistent video content with low\noverhead, despite not being trained on additional video data.\nOur approach is not limited to text-to-video synthesis but is also applicable to other tasks, such as\nconditional and content-specialized video generation, and Video Instruct-Pix2Pix, i.e., instruction-\nguided video editing. In summary, we present a novel approach that combines the strengths of Text-\nto-Video Zero [1] and ControlNet [2], providing a powerful and \ufb02exible framework for generating\nand controlling video content using minimal resources. This work opens up new possibilities for\nef\ufb01cient and effective video generation, catering to various application domains.\nFigure 1: Functional overview of STF: [Input] The basic text prompt on top, [Input] Sketches in the\nsecond row, Interpolation in the third row, and [Output] Resulting frames of the generated video in\nthe bottom row\n2\nRelated Work\nIn this section, we discuss two key papers that have signi\ufb01cantly in\ufb02uenced our work, namely the\nText-to-Video Zero paper and the ControlNet paper.\n2.1\nText-to-Video Zero\nThe Text-to-Video Zero paper [1] addresses the challenge of generating videos from textual prompts\nwithout relying on computationally heavy training and large-scale video datasets. The authors\nintroduce a zero-shot text-to-video generation approach that leverages existing text-to-image synthesis\nmethods, such as Stable Diffusion [3], adapting them for the video domain. This method includes\ntwo key modi\ufb01cations: (i) enriching the latent codes of generated frames with motion dynamics\nfor maintaining global scene and background consistency, and (ii) reprogramming frame-level self-\nattention with a new cross-frame attention mechanism, which focuses on the \ufb01rst frame to preserve\ncontext, appearance, and object identity. Experimental results demonstrate that this approach can\ngenerate high-quality and consistent videos with low overhead and is applicable to various tasks,\nincluding conditional and content-specialized video generation, and instruction-guided video editing.\n2.2\nControlNet\nThe ControlNet paper [2] presents a neural network structure designed to control pretrained large\ndiffusion models by supporting additional input conditions. ControlNet learns task-speci\ufb01c conditions\nin an end-to-end manner and exhibits robust learning even with small training datasets (< 50k).\nTraining a ControlNet is as fast as \ufb01ne-tuning a diffusion model, making it possible to train on\npersonal devices or scale to large datasets on powerful computation clusters. The authors demonstrate\nthat large diffusion models, such as Stable Diffusion [3], can be augmented with ControlNets to\nenable conditional inputs like edge maps, segmentation maps, and keypoints, which may enrich\nmethods to control large diffusion models and facilitate related applications.\nOur work builds upon the Text-to-Video Zero architecture by incorporating ControlNet to improve\nthe output of these models. By combining these techniques, we aim to create a more ef\ufb01cient and\n\ufb02exible approach for generating video content from textual prompts and multiple sketched frames.\n2\n3\nMethod\nOur proposed method aims to improve the output quality of Text-to-Video models by incorporating\nconditional control techniques. Speci\ufb01cally, our model takes multiple frame and time pairs as input\nand generates video output that \"matches\" those frames and time pairs.\nIntuitively, our idea makes sense because generating video from text prompts is a challenging task\nthat often results in unrealistic or incorrect outputs. By allowing the user to provide input in the form\nof frames and time pairs, our model is better able to understand the desired outcome and generate\nmore accurate and desirable video content.\nOur proposed approach builds upon prior work in the area of Text-to-Image Diffusion Models. These\nmodels utilize a stable diffusion process to generate images from text prompts. We use a similar\napproach to create a Stable Diffusion (SD) model and a locked SD model that can be trained to\ngenerate video from image-scribble pairs. This is illustrated in the ControlNet paper [2].\nFigure 2: Architecture overview for ControlNet\nIn addition, our work is heavily inspired by the zero-shot video generation approach that Text2Video-\nZero showcased [1]. Text2Video-Zero applies Stable Diffusion to generate video from text by\napplying techniques to keep the foreground and background consistent between frames.\nFigure 3: Architecture of Controllable Text2Video\nWe randomly sample latent codes and subsequently use Stable Diffusion to generate modi\ufb01ed latent\ncodes. Then, we enhance the latent codes with motion dynamics to simulate the motion in the video.\nSubsequently, the latent codes are pipelined through the ControlNet model with added cross-frame\nattention to ensure the foreground object remains consistent.\nTo expand more on the cross-frame attention, each self-attention layer combines a feature map into a\nquery, key and feature. As done in the Text2Video-Zero paper, we add a cross-frame attention layer\n3\non each frame against the \ufb01rst frame. This maintains the identity of the foreground and background\nobjects.\nFigure 4: Architecture overview of STF\n4\nExperiments\nIn our experiments, we qualitatively compare the performance of Text2Video-Zero with and without\nthe control provided by our proposed Sketching the Future (STF) approach. Our aim is to demonstrate\nthe effectiveness of STF in guiding the generated video output to match our desired vision, even\nwhen the original Text2Video-Zero model fails to capture the intended motion. Our goal with the\nvideo models is to generate a video of a man walking from the left side of the frame to the right side.\nWe will look into the effectiveness of Text2Video-Zero with just text prompting, and then STF with\nsketches for control along with text prompting.\n4.1\nBaseline: Text2Video-Zero without Control\nWe \ufb01rst test the Text2Video-Zero model on a prompt that describes our desired video: \"A man\nwalking on the beach in front of the ocean.\" The resulting video shows a man\u2019s legs walking on a\nbeach towards the camera. Although the beach and ocean context are captured, the video does not\nconvey our vision of the man walking from left to right across the frame.\n4\nFigure 5: Text2Video-Zero with basic prompt, and resulting frames of the generated video in the\nbottom row\nTo better convey our desired motion, we extend the prompt to: \"A man walking on the beach in front\nof the ocean [from left to right].\" However, the generated video still does not meet our expectations,\nas it only shows a close-up of the man\u2019s feet, and they remain stationary in the frame without any\nindication of the \"left to right\" aspect. This highlights the limitations of the Text2Video-Zero model\nin capturing speci\ufb01c motion details through text prompts alone.\nFigure 6: Text2Video-Zero with motion details in prompt, and resulting frames of the generated video\nin the bottom row\n4.2\nProposed Approach: Sketching the Future\nTo address the limitations of Text2Video-Zero, we use our STF approach, which incorporates three\nsketches of a stick \ufb01gure man as additional input. The \ufb01rst sketch places the man on the left, the\nsecond sketch in the middle, and the third sketch on the right of the frame. These sketches help\nconvey the desired motion more explicitly and provide the model with additional visual cues to\ngenerate the intended video.\n5\nFigure 7: STF with the basic text prompt, and sketches in the second row, interpolation in the third\nrow, and resulting frames of the generated video in the bottom row\nWhen we apply STF to the Text2Video-Zero model, the resulting video perfectly matches our vision,\nwith the man walking from the left to the right of the beach. The man\u2019s motion is clearly captured,\nand the background remains consistent throughout the video. This demonstrates the effectiveness of\nour approach in guiding the video generation process and providing more control over the output,\nwhich was not achievable with Text2Video-Zero alone. The success of our STF approach highlights\nthe potential of combining textual prompts with visual input for more accurate and controlled video\ngeneration tasks.\n5\nConclusions and Discussions\nIn this work, we presented STF (Sketching the Future), a novel approach that combines zero-shot\ntext-to-video generation with ControlNet to improve the output of these models. Our method takes\nmultiple sketched frames as input and generates video output that matches the \ufb02ow of these frames,\nproviding a more accurate representation of the desired motion compared to Text2Video-Zero with\npure prompting.\nOur experiments demonstrated that the inclusion of sketched frames allowed STF to generate video\ncontent more in line with the desired action speci\ufb01cs. We compared the performance of Text2Video-\nZero without control, Text2Video-Zero with additional prompting, and our proposed STF approach.\nThe results showed that STF was capable of generating the desired video content, while the other\nmethods failed to fully capture the intended motion.\nThe experiments, though qualitative in nature, provided valuable insights into the capabilities of our\napproach. By focusing on three distinct cases, we were able to showcase the advantages of STF over\ntraditional methods. In the \ufb01rst case, we observed that Text2Video-Zero generated a video that only\npartially matched the prompt, with the man\u2019s legs walking towards the camera rather than from left to\nright. In the second case, adding an extension to the prompt yielded a video that showed the man\u2019s\nfeet but failed to consider the \"left to right\" aspect of the prompt. Finally, with STF, the generated\nvideo accurately depicted the man walking from left to right across the beach, as desired.\nThese qualitative results highlighted the effectiveness of our approach in generating more accurate\nvideo content based on the given prompts and sketches. Our method overcame the limitations of\nexisting text-to-video generation approaches, which often struggle to capture the full semantics of\nthe input prompt. The success of STF, as shown in the experiments, underscores the importance of\nincorporating additional input conditions, such as sketched frames, to guide the video generation\nprocess more effectively.\nThis work opens up several avenues for future research. One potential direction is to explore the\nintegration of additional types of input modalities, such as audio or text, to provide even more\n6\ncontrol over the generated video content. Furthermore, it would be interesting to investigate methods\nfor automatically generating the sketched frames from textual descriptions or other forms of input,\nthereby streamlining the video generation process. Additionally, exploring ways to improve the\nquality of the generated videos by re\ufb01ning the interpolation process between sketched frames and\noptimizing the control mechanism could lead to even better results.\nAnother potential research direction is to evaluate the performance of STF on a wider range of tasks\nand scenarios, including more complex and dynamic scenes, to further demonstrate its versatility and\napplicability. This would help establish the method\u2019s generalizability and its potential for adoption in\nvarious real-world applications.\nIn conclusion, STF offers a promising approach for generating high-quality and remarkably consistent\nvideo content with low overhead, despite not being trained on additional video data. Its ability\nto leverage multiple sketched frames as input and combine them with powerful techniques like\nControlNet makes it a valuable tool for various applications, including conditional and content-\nspecialized video generation, and instruction-guided video editing. By addressing the limitations of\nexisting text-to-video generation methods, STF paves the way for more effective and ef\ufb01cient video\ncontent generation in the future.\n6\nOpen Resources\nIn this section, we provide a list of resources related to our STF (Sketching the Future) project. These\nresources include a demo video, a project website, an open-source GitHub repository, and a Colab\nplayground where users can interactively try out STF.\n1. Demo Video: A video demonstration showcasing the capabilities of our STF approach and\nthe generated video content can be found at the following link: https://www.youtube.\ncom/watch?v=5QH3T7mhomw.\n2. Project Website: For more information about the STF project, including the latest\nupdates, news, and related publications, please visit our project website at: https:\n//sketchingthefuture.github.io/.\n3. GitHub Repository: The source code for STF is openly available on our GitHub repository.\nUsers can access the code, report issues, and contribute to the project at the following link:\nhttps://github.com/rohandkn/skribble2vid.\nFigure 8: Colab open playground\n4. Colab Playground: To enable users to interactively try out our STF approach and exper-\niment with different sketched frames and textual prompts, we have created a Colab play-\nground. Users can access the playground at the following link: https://colab.research.\ngoogle.com/drive/1xc2P3x-10oqsMxA-2SuDlwoE41vCS0m6?usp=sharing.\n7\nWe encourage users to explore these resources and provide feedback, which will help us improve the\nSTF project and better understand its potential impact on various industries and applications.\nBroader Impact\nThe development of STF (Sketching the Future) has signi\ufb01cant implications for a wide range of\nindustries and applications. As a novel approach that combines zero-shot text-to-video generation\nwith ControlNet, STF has the potential to greatly impact the way we generate and consume video\ncontent. In this section, we discuss some of the broader impacts of our work, both positive and\nnegative, that may arise from its adoption.\n6.1\nPositive Impacts\nCreative industries: STF can be a valuable tool for creative professionals, such as \ufb01lmmakers,\nanimators, and graphic designers. By enabling the generation of video content from sketched frames\nand textual prompts, our approach can help streamline the creative process and reduce the time and\neffort required to create high-quality video content.\nAdvertising and marketing: The ability to generate custom video content quickly and ef\ufb01ciently\ncan be bene\ufb01cial for advertising and marketing campaigns. STF can help companies create engaging\nand targeted promotional materials, enabling them to better reach and resonate with their intended\naudiences.\nEducation and training: STF can be used to develop educational materials tailored to speci\ufb01c\nlearning objectives or training requirements. By generating video content that aligns with the\ndesired learning outcomes, our approach can contribute to more effective and engaging educational\nexperiences.\nAccessibility: STF has the potential to make video content more accessible for individuals with\ndisabilities. By generating video content that includes captions or other visual aids, our approach can\nhelp make information and entertainment more inclusive and accessible to a broader audience.\n6.2\nNegative Impacts\nMisinformation and deepfakes: The ability to generate realistic video content from textual prompts\nand sketched frames raises concerns about the potential for misinformation and deepfake videos.\nMalicious actors may use STF to create convincing but false video content, which could be used to\nspread disinformation or manipulate public opinion.\nPrivacy concerns: The use of STF in surveillance or monitoring applications could potentially\ninfringe on individual privacy. If our approach is used to generate video content that includes\nrecognizable people or places, it may raise ethical and legal concerns related to consent and data\nprotection.\nJob displacement: The widespread adoption of STF in industries that rely on manual video content\ncreation could lead to job displacement for some professionals. While our approach can streamline\nthe video creation process, it may also reduce the demand for certain roles in the creative industries,\nsuch as animators and video editors.\n6.3\nMitigating Negative Impacts\nTo address these potential negative impacts, it is crucial to develop guidelines and regulations around\nthe ethical use of STF and similar technologies. Researchers, developers, and policymakers should\nwork together to establish best practices and standards that prioritize privacy, consent, and the\nresponsible use of video generation technology. Additionally, efforts should be made to educate the\npublic about the capabilities and limitations of STF and other video generation methods, so that they\ncan critically evaluate and interpret the content they encounter.\n8\nAcknowledgements\nWe would like to thank Professor Pathak and the course staff of Visual Learning and Recognition for\ntheir support, and Mrinal Verghese for his compute resources. Also we would like to thank ChatGPT\nfor assisting with the writing and organization of this paper.\nReferences\n[1] Khachatryan, Levon, et al. Text2Video-Zero: Text-to-Image Diffusion Models Are Zero-Shot\nVideo Generators. arXiv, 23 Mar. 2023. arXiv.org, https://doi.org/10.48550/arXiv.2303.13439.\n[2] Zhang, Lvmin, and Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion\nModels. arXiv, 10 Feb. 2023. arXiv.org, https://doi.org/10.48550/arXiv.2302.05543.\n[3] Rombach, Robin, et al. High-Resolution Image Synthesis with Latent Diffusion Models. arXiv,\n13 Apr. 2022. arXiv.org, https://doi.org/10.48550/arXiv.2112.10752.\n[4] Singer, Uriel, et al. Make-A-Video: Text-to-Video Generation without Text-Video Data. arXiv,\n29 Sept. 2022. arXiv.org, https://doi.org/10.48550/arXiv.2209.14792.\n[5] Ho,\nJonathan,\net al. Video Diffusion Models. arXiv,\n22 June 2022. arXiv.org,\nhttps://doi.org/10.48550/arXiv.2204.03458.\n9\n"
  },
  {
    "title": "Relightify: Relightable 3D Faces from a Single Image via Diffusion Models",
    "link": "https://arxiv.org/pdf/2305.06077.pdf",
    "upvote": "2",
    "text": "Relightify: Relightable 3D Faces from a Single Image via Diffusion Models\nFoivos Paraperas Papantoniou1,2 Alexandros Lattas1,2 Stylianos Moschoglou1,2 Stefanos Zafeiriou1,2\n1Imperial College London\n2Huawei Noah\u2019s Ark Lab\n{f.paraperas,a.lattas,s.moschoglou,s.zafeiriou}@imperial.ac.uk\nNormals\nSpecular Albedo\nInput\nDiffuse Albedo\nCompleted Texture\n3D Reconstruction\nRendered Reconstruction\nSampling\nRendered Samples\nFigure 1. Left: We train an unconditional diffusion model on a high-quality dataset of UV textures and their accompanying facial reflectance\nmaps. Right: Using this model, we perform both texture completion as well as accurate reflectance prediction from monocular images by\ninpainting in UV space. Our 3D facial reconstruction requires only a single image and allows the realistic rendering of the 3D avatar.\nAbstract\nFollowing the remarkable success of diffusion models\non image generation, recent works have also demonstrated\ntheir impressive ability to address a number of inverse prob-\nlems in an unsupervised way, by properly constraining the\nsampling process based on a conditioning input. Motivated\nby this, in this paper, we present the first approach to use\ndiffusion models as a prior for highly accurate 3D facial\nBRDF reconstruction from a single image.\nWe start by\nleveraging a high-quality UV dataset of facial reflectance\n(diffuse and specular albedo and normals), which we render\nunder varying illumination settings to simulate natural RGB\ntextures and, then, train an unconditional diffusion model\non concatenated pairs of rendered textures and reflectance\ncomponents. At test time, we fit a 3D morphable model to\nthe given image and unwrap the face in a partial UV texture.\nBy sampling from the diffusion model, while retaining the\nobserved texture part intact, the model inpaints not only the\nself-occluded areas but also the unknown reflectance com-\nponents, in a single sequence of denoising steps. In con-\ntrast to existing methods, we directly acquire the observed\ntexture from the input image, thus, resulting in more faithful\nand consistent reflectance estimation. Through a series of\nqualitative and quantitative comparisons, we demonstrate\nsuperior performance in both texture completion as well as\nreflectance reconstruction tasks.\n1. Introduction\nCreating digital avatars of real people is of paramount\nimportance for a range of applications, including VR, AR\nor the film industry. Human faces have been studied exten-\nsively over the years, attracting attention at the intersection\nof Computer Vision, Graphics and Machine Learning re-\nsearch. Although vast literature exists around the estima-\ntion of the 3D shape and reflectance of a face from un-\nconstrained inputs such as \u201cin-the-wild\u201d RGB images, it\nstill remains a challenging problem in the field. In partic-\nular, the recent breakthrough in image synthesis using dif-\nfusion generative models creates a new perspective towards\nphoto-realistic 3D face reconstruction, which has not been\nexplored so far and stems from the state-of-the-art perfor-\nmance of these models in solving inverse problems without\nsupervised training.\nFacial reflectance capture typically requires a control-\nlable illumination system equipped with multiple cameras,\nfirst introduced as a Light Stage [12]. Polarized illumination\nand gradient patterns can be employed for diffuse-specular\nseparation [48, 26], using which, spatially varying facial\nreflectance maps can be acquired, that describe BRDF pa-\nrameters, including the diffuse and specular albedo and nor-\nmals. Although recent works attempt to simplify the captur-\ning apparatus and process using inverse rendering [28, 55]\nor commodity devices [38], such methods still require a la-\nborious capturing process and expensive equipment.\n1\narXiv:2305.06077v2  [cs.CV]  22 Aug 2023\nSince their introduction by Blanz and Vetter [3], 3D Mor-\nphable Models (3DMMs) [54, 11, 44, 5, 4] have been estab-\nlished as a robust methodology for monocular 3D face re-\nconstruction [18, 69] by regularizing the otherwise ill-posed\noptimization problem towards a known statistical prior of\nthe facial geometry, which is usually defined by the lin-\near space of a PCA model. In addition to the coarse ge-\nometry estimation, 3DMMs have been used in conjunction\nwith powerful CNN-based texture models, leading to im-\npressively detailed avatar reconstructions even from low-\nresolution images [57, 23, 24]. Furthermore, another line\nof research [6, 32, 68, 2, 17, 16, 39, 41] revolves around the\nreconstruction of rendering assets such as reflectance com-\nponents (diffuse and specular albedo) and high-frequency\nnormals of the facial surface. As a result, the recovered 3D\nfaces can be realistically rendered in arbitrary illumination\nenvironments. However, prior work either contains scene il-\nlumination inhibiting relighting [13, 21, 23] or is restricted\nby the models\u2019 generalization, lowering the identity simi-\nlarity [23, 39, 47]. Our work shares the same objective in\nthat we couple a 3DMM with high-quality UV reflectance\nmaps, but attempts to solve both of these issues, by preserv-\ning the observed texture details from the input image and\njointly inferring the facial reflectance.\nIn fact, the visible pixels of the facial texture by the given\ncamera pose are directly recoverable from the input image\nvia inverse rasterization of the fitted 3D mesh. Therefore,\nwe cast the 3D face reconstruction problem as an image in-\npainting task in the UV space; i.e. the goal is to fill in the\nmissing pixels in a consistent manner with respect to some\nstatistical prior. In particular, we propose to use a diffusion\nmodel as the generative backbone of our method. Diffu-\nsion models [61] are naturally associated with guided image\nsynthesis since they treat image generation as a sequence of\ndenoising steps in the form of a learnable Markov process.\nThis allows to directly interfere with the sampling process,\ngiven that samples at each part of the chain are distorted ver-\nsions of real images with known noise variances. Thus, by\nproperly modifying the sampling process, a single uncon-\nditional diffusion model can be used for different inverse\nproblems, such as image editing [50], inpainting [46, 10],\nrestoration [36] or super-resolution [9, 8], without problem-\nspecific training.\nIn this paper, we build a high-quality statistical model of\nfacial texture and reflectance by means of a diffusion model\nand adopt an inpainting approach to complete the partially\nreconstructed UV texture produced by a 3DMM fitting step.\nWe further extend the sampling process to recover the miss-\ning reflectance components by enforcing consistency with\nthe input texture. As a result, our method, dubbed Relight-\nify, generates accurate and render-ready 3D faces from un-\nconstrained images, as shown in Fig. 1.\nIn summary, we make the following contributions:\n\u2022 We present the first, to the best of our knowledge,\ndiffusion-based approach for relightable 3D face re-\nconstruction from images. By training on a pseudo-\nground-truth dataset of facial reflectance, while di-\nrectly recovering texture parts from the input, we\nachieve high-quality rendering assets that preserve im-\nportant details of the input face (e.g. wrinkles, moles).\n\u2022 We propose an efficient way of predicting different\nmodalities in a consistent way by learning a generative\nmodel on concatenated reflectance maps and casting\nthe reconstruction as an inpainting problem, spatially,\nbut also channel-wise.\n\u2022 We qualitatively and quantitatively demonstrate the su-\nperiority of our approach against previous methods re-\ngarding both the completed textures as well as the re-\ncovered reflectance maps.\n2. Related Work\n2.1. Diffusion Models for Inverse Problems\nDiffusion models [61] are latent variable generative\nmodels which artificially corrupt the data distribution by\nadding noise and attempt to approximate the reverse pro-\ncess. They have lately emerged as a powerful image synthe-\nsis model [30, 15, 63] outperforming previous state-of-the-\nart approaches in both conditional and unconditional tasks.\nWhile they achieve excellent image quality and are robust\nto multi-modal distributions, they are computationally de-\nmanding to sample from, since they require a large sequence\nof denoising steps (e.g. 1000), each of which operates in the\nhigh dimensional image space. To alleviate this, a num-\nber of works [62, 37, 58] have proposed alternative strate-\ngies to accelerate sampling by reducing the steps of the re-\nverse process. Another line of research [67, 59] proposes to\ntrain an encoding model and learn a diffusion model on its\nlower-dimensional latent space. Recently, Rombach et al.\n[56] have further explored the use of a VQGAN [19] as the\nauto-encoding model, showing that a mild compression is\nenough to reduce the training/sampling time without sacri-\nficing sample quality. The latter approach is our method of\nchoice for this work, as we elaborate on a high-resolution\nUV image space, which would otherwise significantly in-\ncrease the computational overhead.\nOne of the most interesting aspects of diffusion models\nis that they can be used as unsupervised solvers for differ-\nent inverse problems, where the goal is to reconstruct a sam-\nple from some distorted observation, i.e. conditioning input.\nSong et al. [63] propose a conditioning mechanism during\ninference that allows applications such as class-conditional\ngeneration, inpainting and colorization. Similarly, [8] uses\na low-pass filtered version of the conditioning image to\nguide the denoising process at each step and SDEdit [50]\n2\naddresses image translation and editing using a diffused ver-\nsion of the input image to initialize sampling from an in-\ntermediate timestep. RePaint [46] achieves state-of-the-art\nresults on image inpainting by repeating multiple forward\nand backward diffusion steps to enforce harmonization. De-\nspite its improved performance, this resampling strategy\nsignificantly increases the computational time. In contrast,\nCCDF [9] and DDRM [36] propose efficient techniques for\nreducing the length of the reverse process while retaining\nimage quality at a high level. More recently, MCG [10]\nintroduced a novel manifold constraint step, which com-\nbined with the standard reverse diffusion outperforms the\naforementioned methods on a number of inverse tasks, in-\ncluding inpainting. We adopt this approach in our work to\naccurately fill in the missing pixels of both texture and re-\nflectance maps of a face from a given image via diffusion-\nbased inpainting, while fully preserving the observed ones.\nNote also that this approach does not assume any specific\ndistribution of visibility masks, as it is trained uncondition-\nally on complete textures.\n2.2. Facial Reconstruction\n3DMMs [3] are the typical models for facial reconstruc-\ntion from \u201cin-the-wild\u201d images, using a linear model for\nthe identity, and additional linear models for expression or\ncolor. Current facial 3DMMs include the Basel Face Model\n(BFM) [54] and the Large Scale Facial Model (LSFM) [4].\nEgger et al. [18] provide a thorough review on the subject.\nAlbedoMM [60] first created a 3DMM of facial reflectance,\nwhich can be relighted, but is restricted to a linear and per-\nvertex color model. Dib et al. [16, 17] improved on prior\nworks\u2019 simplistic shading models and used inverse ray trac-\ning to acquire photorealistic facial reflectance. Recently,\nGANFit [23, 24] introduced a potent method for fitting\n3DMMs with a GAN-based [27] facial texture generator,\nachieving high-fidelity facial avatars, but lacking relight-\ning capabilities due to baked illumination in the textures.\nAvatarMe++ [39, 41] overcame this issue by translating the\nreconstructed textures to facial reflectance using a condi-\ntional GAN, while adding extra processing steps. While we\nuse AvatarMe++ to augment our training data, our method\nsignificantly outperforms them by using a powerful diffu-\nsion model and inferring only the occluded facial texture.\nTBGAN [22] first introduced a deep generative network\nfor facial reflectance, based on ProgressiveGAN [33] and\n[43] introduced a more powerful model, based on Style-\nGAN [34]. However, both works did not showcase fitting\ncapabilities. An extension of the latter [47], introduced a\nset of multiple networks, with a StyleGAN2 [35] base, that\ncan be used to generate shape and albedo from images with\narbitrary illumination and expression. While close to our\nwork, our method uses a single and more powerful diffusion\nmodel, inferring not only the diffuse albedo, but also the\nspecular albedo and normals. Moreover, our work inpaints\nonly the occluded facial areas, preserving the visible part of\nthe texture and achieves higher reconstruction fidelity.\nAlthough our method is applied to facial reconstruction,\nwe simultaneously solve a facial texture inpainting problem\nin UV space. Initially explored in 2D facial images [45] and\nexpanded to UV completion using deep encoder-decoder ar-\nchitectures (UV-GAN [13]), such works recover the facial\ntexture from partial and masked facial images. Recently,\nOSTeC [21], used a pre-trained StyleGAN in 2D to recover\nmultiple poses of the input subject so as to create a complete\nUV facial texture. While prior works achieve impressive re-\nsults, all are restricted facial textures with baked illumina-\ntion. In contrast, we jointly recover the facial reflectance,\nmaking the reconstruction relightable in standard rendering\nengines.\n3. Method\nWe propose a diffusion-based inpainting approach to es-\ntimate both the UV texture with existing baked illumination\nand the actual reflectance of a face in a single process. At\nthe core of our approach lies an unconditional diffusion gen-\nerative model trained on pairs of textures and their accom-\npanying reflectance. This coupled texture-reflectance mod-\neling along with the sequential denoising process of diffu-\nsion models allows us to reconstruct the reflectance from a\npartial texture of the input face, as shown in Fig. 2. Our\nmethod, thus, generates high-quality 3D face avatars from\n\u201cin-the-wild\u201d images, which can be realistically relighted.\nIn the following sections, we first analyze the training of\nour diffusion model, and then explain the 3D shape recon-\nstruction and texture inpainting strategies in further detail.\n3.1. Diffusion Models: Background\nGiven a distribution of real images x, diffusion mod-\nels [61] define a forward diffusion process which gradually\nadds Gaussian noise to the input image in T consecutive\nsteps. This corresponds to a fixed Markov Chain, where\nstarting from a clean image x0, the noisy samples xt at each\ntimestep t are drawn from the following distributions (with\ntimestep-depending variances \u03b2t) conditioned on the previ-\nous samples:\nq(xt|xt\u22121) = N(xt;\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI)\n(1)\nThis is equivalent to directly sampling xt conditioned on\nthe clean image x0 via:\nq(xt|x0) = N(xt; \u221a\u00af\u03b1tx0, (1 \u2212 \u00af\u03b1t)I)\n(2)\nwhere \u03b1t := 1 \u2212 \u03b2t and \u00af\u03b1t := Qt\ns=1 \u03b1s. Given large\nenough T, this process leads to normally distributed noise\nxT . Then, the goal is to learn the reverse Markov process:\np\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t))\n(3)\n3\nDenoise\n +\nMCG correction step\nTexture\nForward\ndiffusion\n3DMM fitting\nInverse mask\nReconstructed 3D avatar\nDiffusion-based texture and reflectance inpainting\nKnown partial texture / Guiding input\nNormals\nTexture\nTexture\nNormals\nDiffuse Albedo\nSpecular Albedo\nSpecular Albedo\nDiffuse Albedo\nTexture\nNormals\nSpecular Albedo\nDiffuse Albedo\nFigure 2. Overview of our method during inference. Please note that we use a latent diffusion model [56], yet we illustrate the denoising\nprocess in the original image space for visualization purposes. We perform standard 3DMM fitting to get a partial UV texture via image-\nto-uv rasterization. Then, starting from random noise, we utilize the known texture to guide the sampling process of a texture/reflectance\ndiffusion model towards completing the unobserved pixels. Each denoising step, from zt to zt\u22121 (t \u2208 {1, . . . , T}), follows an inpainting\napproach similar to MCG [10] (see Eq. 9): 1) The reflectance maps and unobserved texture pixels are updated based on reverse diffusion\nsampling and manifold constraints, while 2) the known pixels are directly sampled from the input texture via forward diffusion (\u2299 and\n\u2295 denote the Hadamard product and addition respectively). Note that masking is only applied to the texture, while the reflectance maps\n(diffuse/specular albedo, normals) are entirely predicted from random noise. At the end of the process, we acquire high-quality rendering\nassets, making our 3D avatar realistically renderable.\nDiffuse Albedo\nSpecular Albedo\nTexture\nNormals\nRenderings\nFigure 3. Samples of texture and reflectance maps generated\nby our diffusion model (left) and renderings in different scenes\n(right). Random 3DMM shapes are used for the visualization.\nwhich gradually denoises the random noise xT towards\na realistic image, by minimizing the variational bound\nof the negative log likelihood [30, 15].\nFollowing the\nreparameterization proposed in [30], the model consists\nof time-conditioned denoising autoencoders \u03f5\u03b8(xt, t); t \u2208\n{1, 2, . . . , T}, which are trained to predict the noise \u03f5 \u223c\nN(0, I) that was added to the input image x0 to account for\nthe noisy version xt:\nL = Ex0,\u03f5,t\n\u0002\n||\u03f5 \u2212 \u03f5\u03b8(xt, t)||2\u0003\n(4)\nOnce trained, we can generate images by starting from ran-\ndom noise xT \u223c N(0, I) and sequentially drawing de-\nnoised images around the mean:\n\u00b5\u03b8(xt, t) =\n1\n\u221a\u03b1t\n\u0012\nxt \u2212\n\u03b2t\n\u221a1 \u2212 \u00af\u03b1t\n\u03f5\u03b8(xt, t)\n\u0013\n(5)\n3.2. Training of our Diffusion Model\nIn this work, we harness the power of diffusion models to\nlearn a strong generative prior over the domain of facial tex-\nture/reflectance. In particular, we adopt a physically-based\nperspective by separating the facial reflectance into differ-\nent UV maps, namely diffuse albedo (Ad), specular albedo\n(As) and surface normals (N) with high-frequency details.\nThis allows realistic rendering under different illumination\nconditions. We learn our prior using a high-quality dataset\nconsisting of complete pairs of facial reflectance, and a cor-\nresponding rendered texture T under arbitrary illumination.\nMore details on the data we use are provided in section 4.1.\nWe train an unconditional diffusion model (as described in\nsection 3.1) on the quadruples:\nx = [T, Ad, As, N] \u2208 R512\u00d7512\u00d710\n(6)\nwhere we concatenate the components of Eq. 6 across chan-\nnels (each of the 4 UV images measures 512 \u00d7 512 pix-\nels and 3 channels, except for the single-channel image\nAd). By sampling from this model, we can synthesize pairs\nof shaded RGB textures (T) and reflectance components\n(Ad, As, N) which are in correspondence, meaning that the\ntexture is a rendered version of the UV reflectance under\nsome illumination environment.\nIn practice, to reduce the computational requirements to\na reasonable level, we follow the paradigm of latent diffu-\nsion models proposed by Rombach et al. [56], where the\nimages are first compressed to a latent space z = E(x) \u2208\nRh\u00d7w\u00d7c by training a perceptual auto-encoder, consist-\ning of an encoder E and a decoder D. Using perceptual\nand adversarial losses similar to VQGAN [19], the autoen-\n4\nFigure 4. Examples of 3D reconstructions by our method, rendered using different environment maps in a commercial renderer [49].\ncoder achieves an excellent quality of reconstructed sam-\nples \u02dcx = D(E(x)), while allowing to efficiently train the\ndiffusion model on the lower dimensional pixel-space of\nthe learned embeddings. In our case, we train four sim-\nilar auto-encoders, one for each of T, Ad, As and N, all\nof them reducing the input resolution to latent dimensions\nof h = w = 64, c = 3. Therefore, our latent diffusion\nmodel [56] is trained on the concatenation of the 4 embed-\ndings:\nz = [zT, zAd, zAs, zN] \u2208 R64\u00d764\u00d712\n(7)\nSamples from our diffusion model (after being decoded\nthrough each D) can be seen in Fig. 1 (left part) and Fig. 3.\n3.3. Inference\nWe use the aforementioned trained diffusion model to\nperform inpainting on both the texture and reflectance UV\nmaps based on a partial UV texture obtained by 3DMM fit-\nting. We provide a detailed description below.\n3DMM Fitting and Texture Initialization.\nWe rely on\n3DMMs to recover a rough 3D shape of the face from a 2D\nimage as a mesh S \u2208 Rn\u00d73 with n vertices. Specifically,\nwe employ a linear 3DMM:\nS(ps, pe) = m + Usps + Uepe\n(8)\nconsisting of the LSFM [4] shape eigenbasis Us \u2208 R3n\u00d7158\nand the expression eigenbasis Ue \u2208 R3n\u00d729 from the\n4DFAB database [7]. We fit the 3DMM to the input im-\nage by optimizing the shape coefficients ps, expression co-\nefficients pe and camera parameters pc using an off-the-\nshelf framework 1. Note that any 3DMM fitting framework\nworks as a \u201cplug and play\u201d solution to our method. Thus,\none may trivially use a more sophisticated algorithm (e.g.\nGANFit [23]) for precise shape reconstruction.\n1https://github.com/ascust/3DMM-Fitting-Pytorch\nWe use a standard UV topology for texturing the 3D\nmesh, where each vertex is assigned to a fixed 2D coordi-\nnate on the UV plane. By rasterizing the fitted 3D mesh and\nusing barycentric interpolation, we can reverse the render-\ning process and unfold the face in UV, hence reconstructing\nthe visible parts of the texture directly from the input image.\nThis initial texture is accompanied by a UV visibility mask,\nwith 1 for pixels that are observed from the input image, and\n0 for those that are occluded and, thus, need to be inpainted\nby our model.\nTexture Completion and Reflectance Prediction.\nStart-\ning from the partially completed UV texture T0 of the face\nand a binary visibility mask m produced by the previous\nstep, our goal is to inpaint the remaining pixels along with\nthe pixels of the 3 reflectance maps. We use the latent repre-\nsentation zT0 = E(T0) \u2208 Rh\u00d7w\u00d7c of this texture image to\nconstrain the reverse diffusion process. Note that the mask\nm is downsampled to the same resolution h = w = 64 of\nthe latent space for the next steps. Our inpainting algorithm\nstarts with a random noise image zT \u223c N(0, I) and uses\nthe denoising procedure of MCG [10], consisting of the fol-\nlowing repeated steps:\nzunknown\nt\u22121\n\u223c N(\u00b5\u03b8(zt, t), \u03a3\u03b8(zt, t))\n(9a)\nzknown\nTt\u22121 \u223c N(\u221a\u00af\u03b1t\u22121zT0, (1 \u2212 \u00af\u03b1t\u22121)I)\n(9b)\n\u02c6z0 =\n\u0000zt \u2212\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5\u03b8(zt, t)\n\u0001\n/\u221a\u00af\u03b1t\n(9c)\nL = \u2225 (zT0 \u2212 \u02c6zT0) \u2299 m\u22252\n2\n(9d)\nzTt\u22121 = m \u2299 zknown\nTt\u22121 + (1 \u2212 m) \u2299\n\u0012\nzunknown\nTt\u22121\n\u2212 \u03b1 \u2202L\n\u2202zTt\n\u0013\n(9e)\nzkt\u22121 = zunknown\nkt\u22121\n\u2212 \u03b1 \u2202L\n\u2202zkt\n,\nk = {Ad, As, N}\n(9f)\nGiven a sample zt at timestep t, we first sample the next de-\nnoised sample zt\u22121 using the original reverse diffusion step\n5\n(Eq. 9a). We term this as zunknown\nt\u22121\n(borrowing the notation\nfrom [46]) as it does not take into account the known parts\nof the observed texture. To exploit the known texture, we\nsample a noisy version of it zknown\nTt\u22121 at timestep t\u22121 via a for-\nward diffusion step (Eq. 9b). Then, we directly impose this\nknown noisy texture m \u2299 zknown\nTt\u22121 (\u2299 denotes the Hadamard\nproduct) as in the first half of Eq. 9e. Finally, for the un-\nknown pixels, we add the manifold constraint introduced in\nMCG [10]; i.e. we make a prediction of the clean sample \u02c6z0\n(Eq. 9c) based on the previous timestep zt, compare this (\u21132\nloss) with the ground truth in the known regions (Eq. 9d),\nand use the gradient of this loss to update the unknown pix-\nels of zt\u22121 (Eq. 9e and 9f) so as to minimize this distance.\nNote on inpainting algorithm.\nWe have chosen to adopt\nthe recently proposed MCG [10] inpainting algorithm,\nwhich outperforms related state-of-the-art diffusion-based\nmethods (e.g. RePaint [46], DDRM [36]), as we empirically\nfound it to produce excellent results. Motivated by the orig-\ninal algorithm, which aims at inpainting standard RGB im-\nages, we expand it to account for different input domains:\nby treating our images as concatenated texture/reflectance\nmaps, we force the model to perform not only spatial in-\npainting, but also \u201cchannel-wise inpainting\u201d, by filling the\nmissing pixels in a manner that closely aligns with the train-\ning distribution. This essentially encourages the model to\nlearn an inverse rendering transformation during testing,\nthus predicting accurate reflectance maps from just a partial\nilluminated version of them, despite not directly imposing\nphysically-based constraints.\n4. Experiments\n4.1. Dataset and Implementation Details\nWe create a high-quality dataset that consists of facial\ntextures and their corresponding reflectance. Each item in-\ncludes a texture T, shaded in some illumination, diffuse\nalbedo Ad, specular albedo As and normals N. To achieve\nthis, firstly, we acquire the public MimicMe dataset [51],\nwhich contains \u02dcT = {T0, . . . , TnT }, nT\n= 4, 700 di-\nverse facial textures, whose statistics are reported in [51].\nHowever, such textures contain the illumination of the\nscanning apparatus and are not relightable.\nHence, we\nthen train an image-to-image translation network based on\nAvatarMe++ model using the available dataset [41], which\ntranslates the textures \u02dcT to facial reflectance: \u03b1(\u02dcT) \u2192\n{AD, AS, N}. Moreover, we augment the skin-tone di-\nversity, using histogram matching albedo augmentation fol-\nlowing [40]. Given the memory requirement of our net-\nwork, all textures have a resolution of 512 \u00d7 512.\nFi-\nnally, to enable the diffusion model to perform well in \u201cin-\nthe-wild\u201d images, we use the shapes S of MimicMe and\nthe acquired reflectance, to re-render the textures under ar-\nInput\nOurs\nAvatarMe++\nAlbedoMM\nFigure 5.\nRendered reconstructions of shape and reflectance by\nAlbedoMM [60] (using the open-source code), AvatarMe++ [41]\n(provided by authors) and our method, in the same illumination.\nbitrary realistic environments, directly on the UV space:\n\u03c1(AD, AS, N, S) \u2192 T. For an evaluation of the model\nwithout re-rendered textures, please refer to the Supp. Ma-\nterial. Although AvatarMe++ uses a similar method to aug-\nment training data, we do not require this process to be dif-\nferentiable and use a ray-tracing renderer [49] (Baker algo-\nrithm) to achieve more realistic textures.\nTo train our model, we use a KL-regularized latent diffu-\nsion model with the default hyper-parameters proposed by\nthe authors of [56]. Specifically, we use a downsampling\nfactor of f = 8 for the perceptual auto-encoder and a diffu-\nsion length of T = 1000 for the denoising model. We train\nour model once and use it for texture and reflectance re-\nconstruction from \u201cin-the-wild\u201d images. Below we provide\ncomprehensive qualitative and quantitative evaluations.\n4.2. Qualitative Results\nAs already described, we produce relightable 3D faces\nwith reflectance assets that are compatible with commer-\ncial rendering engines. Fig. 4 shows examples of recon-\nstructions from \u201cin-the-wild\u201d images and realistic render-\nings in varying environments (more results are included\nin the Supp. Material).\nFurthermore, we provide a vi-\nsual comparison with the reflectance reconstruction meth-\nods of AlbedoMM [60] and AvatarMe++ [41] in Fig. 5.\nAs can be seen, we recover 3D faces of higher consis-\ntency with respect to the input. Note that AvatarMe++ [41]\nstarts from a GAN-generated texture as input, without di-\n6\n[Luo et al. 21]\n[Lee et al. 20]\nGANfit\n[Tran et al. 19]\n[Deng et al. 19]\n[Genova et al. 18]\nOurs\nInput\n[Thies al. 16] with exp\n[Thies et al. 16] neutral\nFigure 6. Visual comparison with state-of-the-art 3D face recon-\nstruction methods [47, 42, 23, 65, 14, 25, 64]. Results for related\nmethods are borrowed from [47].\nInput A\nOurs\nAvatarMe++\nInput B\nOurs\nAvatarMe++\nFigure 7. Comparison with AvatarMe++ [41] challenging cases.\nrect feedback from the actual facial image.\nDespite us-\ning it to create our training data, our method clearly out-\nperforms AvatarMe++ [41] during testing by conditioning\nthe reflectance prediction on the genuine visible facial tex-\nture instead of a statistical approximation (fitting) of it (see\nFig. 7 for some challenging subjects). We also show an ex-\ntensive qualitative comparison with related 3D reconstruc-\ntion methods in Fig. 6 (most of which can only recover\nthe texture), where similar observations can be made. Fi-\nnally, we test our method on images from the Digital Emily\n[1] and show the results in Fig. 10 together with related\nworks [17, 41]. We yield similar results regardless of the\nlighting, thanks to our coupled texture/reflectance modeling\nthat combines reflectance with randomly rendered textures\nduring training.\n4.3. Texture Completion\nFollowing [21, 13], we evaluate our method on the task\nof texture completion using the Multi-PIE [29] subset of\nInput\nOSTeC\nUV-GAN\nOurs\nOurs (reflectance)\nFigure 8. Examples of texture completion by OSTeC [21], UV-\nGAN [13] and our method from \u201cin-the-wild\u201d images (each\nmethod uses a different UV topology). In contrast to [21, 13]\nwe additionally recover reflectance components for realistic ren-\ndering.\nMethods\nMetric\n0\u25e6\n\u00b130\u25e6\n\u00b160\u25e6\n\u00b190\u25e6\nCE [53]\nPSNR\n23.03\n21.93\n20.27\n19.63\nSSIM\n0.920\n0.892\n0.888\n0.718\nUV-GAN [13]\nPSNR\n23.36\n22.25\n20.53\n19.83\nSSIM\n0.924\n0.897\n0.892\n0.725\nOSTeC [21]\nPSNR\n23.95\n22.54\n21.04\n20.44\nSSIM\n0.928\n0.902\n0.898\n0.746\nOurs\nPSNR\n26.00\n24.73\n24.65\n20.58\nSSIM\n0.928\n0.916\n0.917\n0.874\nTable 1. Quantitative comparison between Relightify and [53, 13,\n21] regarding UV texture completion on the MultiPIE dataset [29]\nfor different viewing angles.\nthe UVDB dataset [13]. This consists of complete UV tex-\ntures for 337 different identities, and corresponding 2D im-\nages of the faces from various camera poses. In accordance\nwith [21, 13], we use the last 137 subjects for evaluation\n(as the first 200 were used as training data in prior works).\nWe perform texture completion with our diffusion-based ap-\nproach for each different viewing angle and compare it with\nexisting texture completion methods, namely CE [53], UV-\nGAN [13] and OSTeC [21]. We use the widely adopted\nPeak Signal-to-Noise Ratio (PSNR) and Structural Similar-\nity Index (SSIM) metrics to compare the completed textures\nwith the ground truth and report the results in Tab. 1. As\ncan be seen, Relightify outperforms the related methods in\nalmost all settings, especially for challenging angles. A vi-\nsual comparison with [21, 13] is provided in Fig. 8. Note\nthat in contrast to CE [53] and UV-GAN [13], our model\nwas not trained on the Multi-PIE dataset.\n4.4. Identity Preservation\nWe perform quantitative evaluations of our method\u2019s\nability to preserve the subject\u2019s identity, by comparing the\ndistribution of identity scores between the input image and\nrendered reconstruction, on the LFW dataset [31], against\nprior work [23, 24, 25, 66]. Following the existing bench-\nmark [24], we evaluate our results using VGG-Face [52].\nWe present our analysis in Fig. 9, measuring the distance\n7\nbetween the input image and reconstruction for all subjects.\nOur method shows a significant improvement in similarity,\nwhile also producing not just a facial texture, but a set of\nrelightable reflectance textures.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRendering-to-photo cosine similarity on LFW\nGenova et al.\nTran et al.\nGANFit\nGANFit+\nOurs\nFigure 9.\nQuantitative evaluation of similarity scores on\nLFW [31], compared with prior work ([25, 66, 23, 24]), using\nVGG-Face [52]. We show the cosine similarity distribution be-\ntween ground truth and reconstruction.\nInput\nDib et al.\nAvatarMe++\nOurs\nGround Truth\nFigure 10. Reconstructions from images with different illumina-\ntion (Digital Emily Project [1]) by our method as well as [17, 41]\nand ground truth. We show the diffuse and specular albedo for\nall methods (where available), plus the recovered texture for our\nmethod.\n4.5. Reflectance Prediction\nTo further assess our method on the task of facial re-\nflectance prediction from monocular images, we use six\ntest subjects with captured ground truth reflectance us-\ning a Light Stage [26], and compare Relightify with the\nstate-of-the-art method of AvatarMe++ [41]. More specif-\nically, we apply both methods on 2D photos of these sub-\njects and measure the PSNR of the recovered reflectance\nmaps with respect to the ground truth maps. As shown in\nTab. 2, our method produces significantly more accurate dif-\nfuse and specular albedos, while the normals closely match\nthose of [41]. This demonstrates our method\u2019s ability to\nbetter capture subject-specific details by directly leverag-\ning texture information from the input image. Note that\nAvatarMe++ reconstructions are additionally conditioned\non the 3DMM shape normals, which may explain a slight\nincrease in the corresponding PSNR.\n4.6. Experimentation with Inpainting Algorithms\nAlthough we adopt the MCG [10] approach for our\ntexture/reflectance diffusion model, we have experimented\nPSNR\nPSNR\nPSNR\n(diffuse albedo) (specular albedo) (normals)\nAvatarMe++ [41]\n18.30\n19.77\n27.26\nOurs\n22.47\n27.17\n26.69\nTable 2. Quantitative comparison of our method with [41] (results\nprovided by authors). We calculate the average PSNR between\nthe reconstructed and the ground truth reflectance maps for six\nsubjects with ground truth, captured using a Light Stage [26].\nwith different inpainting algorithms. We compare four of\nthem in Fig. 11 and Tab. 4. We also provide the runtime\nfor each algorithm in Tab. 3. The baseline method of Score-\nSDE [63], which can be interpreted as Eq. 9 without the gra-\ndient term, produces sub-optimal results, i.e. the occluded\nareas are often inpainted in an inconsistent way with the\nobserved ones, which is especially apparent in the texture\n(Fig. 11) and albedos (Tab. 4). RePaint [46] also produces\nunsatisfactory textures while at the same time increasing the\nreverse diffusion steps by a factor of n (we use n = 10 as\nsuggested by the authors of [46]), which significantly af-\nfects the computational time. In contrast, MCG [10] pre-\nserves the original sampling length (T = 1000 timesteps),\nhence being much more efficient. However, it is still slower\nthan Score-SDE [63] since it requires the computation of a\ngradient for the manifold constraint at each step. In general,\nwe found MCG [10] to perform better in most cases. To fur-\nther strengthen the efficiency of our method, we have addi-\ntionally incorporated the DDIM [62] acceleration technique\nin the MCG algorithm, which allows reducing the denois-\ning steps to N < T (we use N = 200) without a significant\ndrop in quality. In such case, our method can generate high-\nquality texture and reflectance assets from a partial UV tex-\nture in roughly 12 seconds, which is significantly faster than\ncompeting texture completion algorithms (e.g. OSTeC [21]\nrequires around 10 minutes).\nInput\nPartial UV\nScore-SDE\nRePaint\nMCG\nMCG + DDIM\nFigure 11. Texture completion with our diffusion model using dif-\nferent inpainting algorithms [63, 46, 10, 62]. All algorithms are\nimplemented on top of the same unconditionally trained diffusion\nmodel, and only the reverse sampling process is modified.\n5. Limitations\nOur method outperforms prior works on texture comple-\ntion as well as the challenging task of reflectance prediction.\nThis is accomplished by explicitly recovering information\n8\nScore-SDE RePaint MCG (Ours) MCG (Ours) + DDIM\nTime\n17 sec\n3 min\n1 min\n12 sec\nTable 3. Sampling time during texture completion and reflectance\nprediction for different inpainting algorithms [63, 46, 10, 62] (us-\ning an Nvidia RTX 2080 TI GPU).\nDiffuse Albedo Specular Albedo\nNormals\nPSNR\nSSIM\nPSNR\nSSIM\nPSNR SSIM\nScore-SDE\n20.80\n0.808\n26.69\n0.845\n26.86 0.784\nRePaint\n20.08\n0.813\n26.65\n0.848\n27.27 0.801\nMCG\n22.47\n0.825\n27.17\n0.853\n26.69 0.781\nMCG + DDIM 21.94\n0.817\n26.88\n0.846\n26.45 0.774\nTable 4. Comparison of inpainting algorithms [63, 46, 10, 62] ap-\nplied on our diffusion model, following the evaluation of Tab. 2.\nfrom the input image via inpainting. Nonetheless, similarly\nto related texture completion works [21, 13], this also im-\nplies that the reconstructed texture is affected by the qual-\nity of the input image. Although the partial texture is first\nprojected in our latent diffusion space by the perceptual en-\ncoder, a low resolution input may still degrade the quality\nof our result. In these cases, an upsampling network could\nbe employed as in [39] to improve the resolution and details\nof the predicted UV maps. Also, despite its relatively large\nsize, the employed dataset [51] may still under-represent\nsome ethnic groups and lack diverse facial expressions, re-\nducing accuracy in those cases. Incorporating diverse high-\nquality ground truth data with captured reflectance would\nsignificantly improve the performance. Finally, our method\nmay also suffer by the ambiguity between albedo and illu-\nmination, which is thoroughly described in TRUST [20]. In\nfact, their proposed solution could be combined with our\nmethod in future work.\n6. Conclusion\nIn this paper we introduced Relightify, a method that\nachieves state-of-the-art facial texture completion and facial\nreflectance acquisition, from monocular \u201cin-the-wild\u201d im-\nages. To achieve this, we train a latent diffusion model with\nmultiple encoder-decoder networks, on a synthetic facial\ntexture and reflectance dataset, and use a diffusion-based\ninpainting method on the masked UV textures. Our results\ndirectly acquire the visible facial parts while also extrapo-\nlating to facial reflectance that exhibits a high likeness to\nthe input image and can be trivially employed in commer-\ncial rendering applications.\nAcknowledgements.\nA. Lattas was partly funded by\nthe EPSRC Fellowship DEFORM (EP/S010203/1).\nS.\nZafeiriou and part of the research was funded by the EPSRC\nFellowship DEFORM (EP/S010203/1) and EPSRC Project\nGNOMON (EP/X011364/1).\nReferences\n[1] Oleg Alexander, Mike Rogers, William Lambeth, Jen-Yuan\nChiang, Wan-Chun Ma, Chuan-Chang Wang, and Paul De-\nbevec. The digital emily project: Achieving a photorealistic\ndigital actor. IEEE Computer Graphics and Applications,\n30(4):20\u201331, 2010.\n[2] Linchao Bao, Xiangkai Lin, Yajing Chen, Haoxian Zhang,\nSheng Wang, Xuefei Zhe, Di Kang, Haozhi Huang, Xinwei\nJiang, Jue Wang, Dong Yu, and Zhengyou Zhang.\nHigh-\nfidelity 3d digital human head creation from rgb-d selfies.\nACM Transactions on Graphics, 2021.\n[3] Volker Blanz and Thomas Vetter. A morphable model for\nthe synthesis of 3d faces. In Proceedings of the 26th Annual\nConference on Computer Graphics and Interactive Tech-\nniques, SIGGRAPH \u201999, page 187\u2013194, USA, 1999. ACM\nPress/Addison-Wesley Publishing Co.\n[4] James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan\nPonniah, and David Dunaway. A 3d morphable model learnt\nfrom 10,000 faces. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June\n2016.\n[5] Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun\nZhou. Facewarehouse: A 3d facial expression database for\nvisual computing. IEEE Transactions on Visualization and\nComputer Graphics, 20(3):413\u2013425, 2014.\n[6] Anpei Chen, Zhang Chen, Guli Zhang, Kenny Mitchell, and\nJingyi Yu. Photo-realistic facial details synthesis from single\nimage. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), pages 9429\u20139439, 2019.\n[7] Shiyang Cheng, Irene Kotsia, Maja Pantic, and Stefanos\nZafeiriou. 4dfab: A large scale 4d database for facial ex-\npression analysis and biometric applications.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2018.\n[8] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune\nGwon, and Sungroh Yoon. Ilvr: Conditioning method for\ndenoising diffusion probabilistic models. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 14367\u201314376, October 2021.\n[9] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye.\nCome-closer-diffuse-faster: Accelerating conditional diffu-\nsion models for inverse problems through stochastic contrac-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 12413\u2013\n12422, June 2022.\n[10] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Improv-\ning diffusion models for inverse problems using manifold\nconstraints. In Advances in Neural Information Processing\nSystems, 2022.\n[11] Hang Dai, Nick Pears, William A. P. Smith, and Christian\nDuncan. A 3d morphable model of craniofacial shape and\ntexture variation. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), 2017.\n[12] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter\nDuiker, Westley Sarokin, and Mark Sagar.\nAcquiring the\nreflectance field of a human face.\nIn Proceedings of the\n9\n27th annual conference on Computer graphics and interac-\ntive techniques, pages 145\u2013156, 2000.\n[13] Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang\nZhou, and Stefanos Zafeiriou. Uv-gan: Adversarial facial uv\nmap completion for pose-invariant face recognition. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), June 2018.\n[14] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde\nJia, and Xin Tong.\nAccurate 3d face reconstruction with\nweakly-supervised learning: From single image to image\nset. In IEEE Computer Vision and Pattern Recognition Work-\nshops, 2019.\n[15] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. In Advances in Neural Infor-\nmation Processing Systems, volume 34, pages 8780\u20138794,\n2021.\n[16] Abdallah Dib, Gaurav Bharaj, Junghyun Ahn, C\u00b4edric\nTh\u00b4ebault, Philippe Gosselin, Marco Romeo, and Louis\nChevallier.\nPractical face reconstruction via differentiable\nray tracing. In Computer Graphics Forum, volume 40, pages\n153\u2013164. Wiley Online Library, 2021.\n[17] Abdallah Dib, Cedric Thebault, Junghyun Ahn, Philippe-\nHenri Gosselin, Christian Theobalt, and Louis Chevallier.\nTowards high fidelity monocular face reconstruction with\nrich reflectance using self-supervised learning and ray trac-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 12819\u201312829, 2021.\n[18] Bernhard Egger, William AP Smith, Ayush Tewari, Stefanie\nWuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard,\nTimo Bolkart, Adam Kortylewski, Sami Romdhani, et al.\n3d morphable face models\u2014past, present, and future. ACM\nTransactions on Graphics (TOG), 39(5):1\u201338, 2020.\n[19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 12873\u201312883, June\n2021.\n[20] Haiwen Feng, Timo Bolkart, Joachim Tesch, Michael J\nBlack, and Victoria Abrevaya.\nTowards racially unbiased\nskin tone estimation via scene disambiguation. In Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part XIII, pages\n72\u201390. Springer, 2022.\n[21] Baris Gecer, Jiankang Deng, and Stefanos Zafeiriou. Os-\ntec: One-shot texture completion.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 7628\u20137638, June 2021.\n[22] Baris\nGecer,\nAlexandros\nLattas,\nStylianos\nPloumpis,\nJiankang\nDeng,\nAthanasios\nPapaioannou,\nStylianos\nMoschoglou, and Stefanos Zafeiriou. Synthesizing coupled\n3d face modalities by trunk-branch generative adversarial\nnetworks.\nIn Computer Vision\u2013ECCV 2020:\n16th Eu-\nropean Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part XXIX 16, pages 415\u2013433. Springer, 2020.\n[23] Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos\nZafeiriou. Ganfit: Generative adversarial network fitting for\nhigh fidelity 3d face reconstruction. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), June 2019.\n[24] Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Ste-\nfanos P Zafeiriou. Fast-ganfit: Generative adversarial net-\nwork for high fidelity 3d face reconstruction. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 2021.\n[25] Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron\nSarna, Daniel Vlasic, and William T Freeman.\nUnsuper-\nvised training for 3d morphable model regression. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 8377\u20138386, 2018.\n[26] Abhijeet Ghosh, Graham Fyffe, Borom Tunwattanapong, Jay\nBusch, Xueming Yu, and Paul Debevec. Multiview face cap-\nture using polarized spherical gradient illumination. ACM\nTransactions on Graphics (TOG), 30(6):1\u201310, 2011.\n[27] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020.\n[28] Paulo Gotardo, J\u00b4er\u00b4emy Riviere, Derek Bradley, Abhijeet\nGhosh, and Thabo Beeler.\nPractical dynamic facial ap-\npearance modeling and acquisition. ACM Transactions on\nGraphics (TOG), 37(6):1\u201313, 2018.\n[29] Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade,\nand Simon Baker. Multi-pie. Image and Vision Computing,\n28(5):807\u2013813, May 2010.\n[30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 6840\u20136851,\n2020.\n[31] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik\nLearned-Miller.\nLabeled faces in the wild: A database\nfor studying face recognition in unconstrained environ-\nments. Technical Report 07-49, University of Massachusetts,\nAmherst, October 2007.\n[32] Loc Huynh, Weikai Chen, Shunsuke Saito, Jun Xing, Koki\nNagano, Andrew Jones, Paul Debevec, and Hao Li. Meso-\nscopic facial geometry inference using deep neural networks.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2018.\n[33] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. In International Conference on Learning Rep-\nresentations, 2018.\n[34] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 4401\u20134410,\n2019.\n[35] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 8110\u20138119, 2020.\n[36] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming\nSong. Denoising diffusion restoration models. In Advances\nin Neural Information Processing Systems, 2022.\n10\n[37] Zhifeng Kong and Wei Ping. On fast sampling of diffusion\nprobabilistic models. In ICML Workshop on Invertible Neu-\nral Networks, Normalizing Flows, and Explicit Likelihood\nModels, 2021.\n[38] Alexandros Lattas, Yiming Lin, Jayanth Kannan, Ekin\nOzturk, Luca Filipi, Giuseppe Claudio Guarnera, Gaurav\nChawla, and Abhijeet Ghosh. Practical and scalable desktop-\nbased high-quality facial capture. In Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, Octo-\nber 23\u201327, 2022, Proceedings, Part VI, pages 522\u2013537.\nSpringer, 2022.\n[39] Alexandros Lattas, Stylianos Moschoglou, Baris Gecer,\nStylianos\nPloumpis,\nVasileios\nTriantafyllou,\nAbhijeet\nGhosh, and Stefanos Zafeiriou. Avatarme: Realistically ren-\nderable 3d facial reconstruction \u201din-the-wild\u201d. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), June 2020.\n[40] Alexandros\nLattas,\nStylianos\nMoschoglou,\nStylianos\nPloumpis,\nBaris Gecer,\nJiankang Deng,\nand Stefanos\nZafeiriou. FitMe: Deep photorealistic 3D morphable model\navatars.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June\n2023.\n[41] Alexandros\nLattas,\nStylianos\nMoschoglou,\nStylianos\nPloumpis, Baris Gecer, Abhijeet Ghosh, and Stefanos P\nZafeiriou. Avatarme++: Facial shape and brdf inference with\nphotorealistic rendering-aware gans. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2021.\n[42] Gun-Hee Lee and Seong-Whan Lee.\nUncertainty-aware\nmesh decoder for high fidelity 3d face reconstruction.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2020.\n[43] Ruilong Li, Karl Bladin, Yajie Zhao, Chinmay Chinara,\nOwen Ingraham, Pengda Xiang, Xinglei Ren, Pratusha\nPrasad, Bipin Kishore, Jun Xing, et al.\nLearning forma-\ntion of physically-based face attributes. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 3410\u20133419, 2020.\n[44] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and\nJavier Romero. Learning a model of facial shape and ex-\npression from 4D scans. ACM Transactions on Graphics,\n(Proc. SIGGRAPH Asia), 36(6):194:1\u2013194:17, 2017.\n[45] Yijun Li, Sifei Liu, Jimei Yang, and Ming-Hsuan Yang. Gen-\nerative face completion. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 3911\u20133919, 2017.\n[46] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool.\nRepaint: Inpaint-\ning using denoising diffusion probabilistic models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 11461\u201311471, June\n2022.\n[47] Huiwen Luo, Koki Nagano, Han-Wei Kung, Qingguo Xu,\nZejian Wang, Lingyu Wei, Liwen Hu, and Hao Li. Normal-\nized avatar synthesis using stylegan and perceptual refine-\nment. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 11662\u2013\n11672, 2021.\n[48] Wan-Chun Ma, Tim Hawkins, Pieter Peers, Charles-Felix\nChabert, Malte Weiss, Paul E Debevec, et al.\nRapid ac-\nquisition of specular and diffuse normal maps from polar-\nized spherical gradient illumination. Rendering Techniques,\n2007(9):10, 2007.\n[49] Marmoset LLC. Toolbag 4, version 4.051.\n[50] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In International Conference on Learning Representa-\ntions, 2022.\n[51] Athanasios Papaioannou, Baris Gecer, Shiyang Cheng, Grig-\norios Chrysos, Jiankang Deng, Eftychia Fotiadou, Christos\nKampouris, Dimitrios Kollias, Stylianos Moschoglou, Kri-\ntaphat Songsri-In, et al. Mimicme: A large scale diverse 4d\ndatabase for facial expression analysis. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part VIII, pages 467\u2013\n484. Springer, 2022.\n[52] Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman.\nDeep face recognition. In Proceedings of the British Machine\nVision Conference (BMVC), pages 41.1\u201341.12, September\n2015.\n[53] Deepak Pathak, Philipp Kr\u00a8ahenb\u00a8uhl, Jeff Donahue, Trevor\nDarrell, and Alexei Efros. Context encoders: Feature learn-\ning by inpainting. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016.\n[54] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami\nRomdhani, and Thomas Vetter. A 3d face model for pose\nand illumination invariant face recognition. In 2009 Sixth\nIEEE International Conference on Advanced Video and Sig-\nnal Based Surveillance, pages 296\u2013301, 2009.\n[55] J\u00b4er\u00b4emy Riviere, Paulo Gotardo, Derek Bradley, Abhijeet\nGhosh, and Thabo Beeler.\nSingle-shot high-quality facial\ngeometry and skin appearance capture. ACM Transactions\non Graphics (TOG), 39(4):81\u20131, 2020.\n[56] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, June 2022.\n[57] Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, and\nHao Li. Photorealistic facial texture inference using deep\nneural networks.\nIn Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), July\n2017.\n[58] Robin San-Roman,\nEliya Nachmani,\nand Lior Wolf.\nNoise estimation for generative diffusion models.\nCoRR,\nabs/2104.02600, 2021.\n[59] Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano\nErmon. D2c: Diffusion-denoising models for few-shot con-\nditional generation. arXiv preprint arXiv:2106.06819, 2021.\n[60] William AP Smith, Alassane Seck, Hannah Dee, Bernard\nTiddeman, Joshua B Tenenbaum, and Bernhard Egger. A\nmorphable face albedo model.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5011\u20135020, 2020.\n11\n[61] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In Proceedings of the 32nd\nInternational Conference on Machine Learning, volume 37,\npages 2256\u20132265, Lille, France, 07\u201309 Jul 2015.\n[62] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2021.\n[63] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In International Conference on Learning Represen-\ntations, 2021.\n[64] Justus Thies, Michael Zollhofer, Marc Stamminger, Chris-\ntian Theobalt, and Matthias Niessner. Face2face: Real-time\nface capture and reenactment of rgb videos.\nIn Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2016.\n[65] Luan Tran, Feng Liu, and Xiaoming Liu.\nTowards high-\nfidelity nonlinear 3d face morphable model. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), June 2019.\n[66] Anh Tuan Tran, Tal Hassner, Iacopo Masi, and G\u00b4erard\nMedioni.\nRegressing robust and discriminative 3d mor-\nphable models with a very deep neural network. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5163\u20135172, 2017.\n[67] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based\ngenerative modeling in latent space. In Neural Information\nProcessing Systems (NeurIPS), 2021.\n[68] Shugo Yamaguchi, Shunsuke Saito, Koki Nagano, Yajie\nZhao, Weikai Chen, Kyle Olszewski, Shigeo Morishima, and\nHao Li.\nHigh-fidelity facial reflectance and geometry in-\nference from an unconstrained image. ACM Trans. Graph.,\n37(4), jul 2018.\n[69] Michael Zollh\u00a8ofer, Justus Thies, Pablo Garrido, Derek\nBradley, Thabo Beeler, Patrick P\u00b4erez, Marc Stamminger,\nMatthias Nie\u00dfner, and Christian Theobalt. State of the art\non monocular 3d face reconstruction, tracking, and applica-\ntions. Computer Graphics Forum, 37:523\u2013550, 2018.\n12\nRelightify: Relightable 3D Faces from a Single Image via Diffusion Models\n(Supplementary Material)\nA. Additional Qualitative Results\nAs discussed in the main paper, our method relies on a pre-trained diffusion model which serves as prior for guiding the\ninpainting procedure. To illustrate the generation capabilities of this model, we show an extension of Fig. 3 in Fig. 12, with a\nfew more unconditionally generated (i.e. based on the standard diffusion sampling process) samples. Besides the synthesized\nUV maps, we further apply the reflectance components on top of random 3D shapes drawn from the employed 3DMM and\nprovide corresponding 3D renderings in different scenes. Finally, in Fig. 13, we show additional 3D reconstructions and\nrenderings from monocular images with our proposed method.\nDiffuse Albedo\nSpecular Albedo\nTexture\nNormals\nRenderings\nFigure 12. Texture and reflectance maps sampled from our diffusion model. Left: We visualize the UV maps on top of arbitrary 3D shapes\nfrom a 3DMM [4]. Right: Realistic renderings of the sampled reflectances on a commercial renderer.\nB. Visual Comparison with Reflectance Reconstruction Methods\nIn Fig. 5, we have shown examples of rendered reconstructions by our method, as well as two related methods, namely\nAlbedoMM [60] and AvatarMe++ [41], both of which recover the facial reflectance from singe images, similarly to our\nmethod. Here, we extend this comparison by adding the diffuse and specular albedos produced by these methods in Fig. 14.\nNote that although AvatarMe++ [41] generates UV assets of higher resolution (4K by 6K), our method shows better detail\n13\nTexture\nDiffuse Albedo\nSpecular Albedo\nNormals\nInput\nRenderings\nFigure 13. 3D reconstructions with recovered texture/reflectance maps by our method and renderings in different environments.\npreservation properties by directly capitalizing on the observed texture. Also, for the case of AlbedoMM [60], which uses\nper-vertex albedo values, we transform them into a flat plane UV parameterization for visualization purposes.\nC. Ablation Study on Texture Augmentation\nAs mentioned in the paper, in order to enhance our model\u2019s generalization ability, we create arbitrary textures during\ntraining by re-rendering their delighted versions using random illumination environments. Preliminary experiments showed\nthat this approach improves performance compared to using the original MimicMe textures. Here, we provide an evaluation\n14\nInput\nAlbedoMM\nRendering\nDiffuse Albedo\nSpecular Albedo\nAvatarMe++\nRendering\nDiffuse Albedo\nSpecular Albedo\nOurs\nRendering\nDiffuse Albedo\nSpecular Albedo\nFigure 14. Comparison with AvatarMe++ [41] and AlbedoMM [60]. For a fair comparison, we sample the per-vertex albedo values of\nAlbedoMM to a flat plane UV parameterization, and crop AvatarMe++ and Our results to the central facial area, since AlbedoMM is using\na tigher facial crop.\nof the model trained on the original textures, following the setting of Tab. 2.\nPSNR\nPSNR\nPSNR\n(diffuse albedo) (specular albedo) (normals)\nOurs (w/o aug.)\n22.07\n26.98\n26.63\nOurs (w/ aug.)\n22.47\n27.17\n26.69\nTable 5. Ablation study for UV texture augmentation (as in Tab. 2).\n15\n"
  },
  {
    "title": "HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion",
    "link": "https://arxiv.org/pdf/2305.06356.pdf",
    "upvote": "1",
    "text": "HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion\nMUSTAFA I\u015eIK, Synthesia, Germany\nMARTIN R\u00dcNZ, Synthesia, Germany\nMARKOS GEORGOPOULOS, Synthesia, United Kingdom\nTARAS KHAKHULIN, Synthesia, United Kingdom\nJONATHAN STARCK, Synthesia, United Kingdom\nLOURDES AGAPITO, University College London, United Kingdom\nMATTHIAS NIESSNER, Technical University of Munich, Germany\nMulti-view Capture\nNovel-view Synthesis with HumanRF\nFig. 1. We introduce a novel multi-view dataset of humans in motion captured with a rig of 160 cameras, recording footage of 12MP each (left image). From an\ninput multi-view recording of a specific person, our HumanRF method reconstructs a spatio-temporal radiance field which captures appearance and motion of\nthe actor. From this representation, we can then synthesize highly-realistic images from unseen, novel view points (right images).\nRepresenting human performance at high-fidelity is an essential building\nblock in diverse applications, such as film production, computer games or\nvideoconferencing. To close the gap to production-level quality, we intro-\nduce HumanRF1, a 4D dynamic neural scene representation that captures\nfull-body appearance in motion from multi-view video input, and enables\nplayback from novel, unseen viewpoints. Our novel representation acts as a\ndynamic video encoding that captures fine details at high compression rates\nby factorizing space-time into a temporal matrix-vector decomposition. This\nallows us to obtain temporally coherent reconstructions of human actors\nfor long sequences, while representing high-resolution details even in the\ncontext of challenging motion. While most research focuses on synthesizing\nat resolutions of 4MP or lower, we address the challenge of operating at\n1Project website: synthesiaresearch.github.io/humanrf\nAuthors\u2019 addresses: Mustafa I\u015f\u0131k, mustafa.isik@synthesia.io, Synthesia, Munich, Ger-\nmany; Martin R\u00fcnz, martin@synthesia.io, Synthesia, Munich, Germany; Markos Geor-\ngopoulos, markos@synthesia.io, Synthesia, London, United Kingdom; Taras Khakhulin,\ntaras.khakhulin@synthesia.io, Synthesia, London, United Kingdom; Jonathan Starck,\njon@synthesia.io, Synthesia, London, United Kingdom; Lourdes Agapito, l.agapito@\ncs.ucl.ac.uk, University College London, London, United Kingdom; Matthias Nie\u00dfner,\nniessner@tum.de, Technical University of Munich, Munich, Germany.\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nThis is the author\u2019s version of the work. It is posted here for your personal use. Not for\nredistribution. The definitive Version of Record was published in ACM Transactions on\nGraphics, https://doi.org/10.1145/3592415.\n12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that\nprovides 12MP footage from 160 cameras for 16 sequences with high-fidelity,\nper-frame mesh reconstructions2. We demonstrate challenges that emerge\nfrom using such high-resolution data and show that our newly introduced\nHumanRF effectively leverages this data, making a significant step towards\nproduction-level quality novel view synthesis.\nCCS Concepts: \u2022 Computing methodologies \u2192 Computer vision rep-\nresentations.\nAdditional Key Words and Phrases: neural rendering, free-view video syn-\nthesis\nACM Reference Format:\nMustafa I\u015f\u0131k, Martin R\u00fcnz, Markos Georgopoulos, Taras Khakhulin, Jonathan\nStarck, Lourdes Agapito, and Matthias Nie\u00dfner. 2023. HumanRF: High-\nFidelity Neural Radiance Fields for Humans in Motion. ACM Trans. Graph.\n42, 4, Article 1 (August 2023), 12 pages. https://doi.org/10.1145/3592415\n1\nINTRODUCTION\nPhoto-realistic image synthesis of virtual environments has been\none of the core challenges in computer graphics research for decades.\nTraditionally, the underlying 3D assets have been created by artists\n2ActorsHQ dataset is publicly available under www.actors-hq.com including all raw\nRGB frames and per-frame reconstructed 3D meshes.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\narXiv:2305.06356v2  [cs.CV]  11 May 2023\n1:2\n\u2022\nI\u015f\u0131k et al.\nwith heavy manual labor; however, recently, significant effort has\nbeen devoted to reconstructing the 3D representations from real-\nworld observations. In particular, novel view synthesis of recorded\nhumans has been the center of attention in numerous applications,\nranging from movie and game production to immersive telepresence.\nYet, reconstructing photo-realistic digital humans from real-world\ncaptures involves significant technical challenges. The diverse gran-\nularity of fine-scale detail \u2013 e.g., on faces, hair, clothing \u2013 makes the\nreconstruction difficult to scale, while the margin for error is low\ndue to the acute ability of the human visual system to perceive even\nthe smallest inconsistencies in synthesized images. From a method-\nological standpoint, the main challenge lies in jointly reconstructing\nappearance and motion in realistic settings due to the large number\nof degrees of freedom that needs to be encoded. In particular, model-\ning fast and complex motions while obtaining photo-realistic results\nat a sufficient resolution remains an open problem in production.\nIn recent years, we have seen tremendous progress in addressing\nthese challenges. More specifically, Mildenhall et al. [2020] recon-\nstructs a 3D neural radiance field (NeRF) constrained by a multi-view\nvolumetric rendering loss. The resulting 3D field is encoded in a\nmulti-layer perceptron (MLP) which then enables novel-view syn-\nthesis. While NeRF originally focused on static scenes, recent works\nhandle dynamic scenes implicitly via time conditioning [Li et al.\n2022a] or explicitly via deformation fields [Park et al. 2021a,b]. These\ndynamic methods show impressive results; but, they still struggle\nto handle longer sequences with complex motion \u2013 especially for\nhumans. In the mean time, obtaining high-quality output render-\nings requires high-resolution training data which is both difficult to\ncapture and utilize in the subsequent radiance field reconstructions.\nIn this work, we propose to address these shortcomings of dy-\nnamic NeRF methods in the context of capturing moving humans.\nTherefore, we first introduce ActorsHQ, a new high-fidelity dataset\nof clothed humans in motion tailored for photo-realistic novel view\nsynthesis. The dataset features multi-view recordings of 160 syn-\nchronized cameras that simultaneously capture individual video\nstreams of 12MP each, as illustrated in Fig. 1. Leveraging our newly\ncaptured data, we propose a new scene representation that lifts\nInstant-NGP [M\u00fcller et al. 2022] hash encodings to the temporal\ndomain by incorporating the time dimension in conjunction with a\nlow-rank space-time tensor decomposition of the feature grid. We\nfurther split a sequence into segments, which allows representing\nvery long sequences as only few of the segments need to reside\nin GPU memory during a training iteration \u2013 something existing\nmethods struggle with due to using a single representation for an en-\ntire sequence. Finally, we demonstrate the effectiveness of our new\nrepresentation on our newly introduced dataset where we signifi-\ncantly improve over existing state-of-the-art methods. Concretely,\nour contributions are as follows:\n\u2022 We propose a new spatio-temporal decomposition that can\nefficiently reconstruct a dynamic radiance field representation\nfrom multi-view inputs, based on a low-rank decomposition.\n\u2022 Additionally, we introduce an adaptive splitting scheme which\ndivides a sequence into segments allowing us to capture arbi-\ntrarily long sequences.\n\u2022 We further introduce ActorsHQ, a high-fidelity dataset, fea-\nturing footage of 8 actors from 160 cameras that record at a\nresolution of 12MP each.\n2\nRELATED WORK\nHumanRF leverages hybrid implicit and volumetric representations\nto reconstruct free-viewpoint videos. In this section, we discuss\nrelated work on neural representations for static and dynamic scenes\nand for human performance capture.\n2.1\n3D Neural Representations\n3D reconstruction is a long-standing problem that has been rede-\nfined with the advent of deep learning-based approaches. In partic-\nular, coordinate-based networks have become a popular choice for\nimplicit 3D scene representations such as radiance [Mildenhall et al.\n2020], signed distance [Park et al. 2019], or occupancy [Mescheder\net al. 2019] fields. In the pioneering work of Mildenhall et al. [2020],\nan MLP is trained to encode a radiance field reconstructed from a set\nof input RGB images. Alternatively, some methods utilize explicit\ndata structures, such as sparse grids [Fridovich-Keil et al. 2022], to\nachieve fast training and inference at the expense of a larger memory\nfootprint. TensoRF [Chen et al. 2022b] addresses memory inefficien-\ncies by using a low-rank tensor decomposition while M\u00fcller et al.\n[2022] propose using hash data structures accompanied with small\nMLPs. We elevate the ideas from TensoRF into spatio-temporal do-\nmain by representing the feature grids via 4D decomposition using\nfour 3D hash grids and four 1D dense grids.\n2.2\n4D Dynamic Representations\nThe creation of free-viewpoint videos has been widely studied due to\nits numerous applications. The seminal work of Kanade et al. [1997]\nallows the reconstruction of shapes and textures using a multi-\ncamera dome. Similarly, later efforts [Carranza et al. 2003; Starck\nand Hilton 2007] leveraged multiple cameras for free-viewpoint\nhuman rendering. More recently, the breakthrough work of Collet\net al. [2015] proposes to track textured meshes in order to create\nstreamable 3D videos. Broxton et al. [2020] presents a layered mesh\nrepresentation to reconstruct and compress video from a multi-\ncamera rig. More recently, deep learning-based approaches have\nbeen proposed for deformable 3D scenes. Neural Volumes [Lom-\nbardi et al. 2019] use an encoder-decoder architecture to optimize\na 3D volume from 2D images. Similarly, instead of decoding a 3D\nvolume, Lombardi et al. [2021] propose to decode a mixture of volu-\nmetric primitives that are attached to a guide mesh.\nA plethora of efforts has been dedicated to extending the success\nof NeRF into the temporal domain using implicit representations. Li\net al. [2022a] extends NeRF with time-conditioning and introduces a\nkeyframe-based training strategy. Alternatively, Park et al. [2021a,b];\nPumarola et al. [2021] introduce a separate MLP to predict scene\ndeformations for multi-view and monocular videos, respectively. In\nsimilar vain, Li et al. [2021] leverages 2D flow supervision to model\na dynamic scene. Similarly to static scenes, the slow convergence\nof such methods has been addressed using explicit [Liu et al. 2022]\nand hybrid [Fang et al. 2022a; Guo et al. 2022a] representations.\nOrthogonal to these approaches, Wang et al. [2022b] fuses a set of\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nHumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion\n\u2022\n1:3\nvideo\nframes\n...\n\u00a7\n\u00a7\n\u00a7\n\u00a7\nSegment 2\nFig. 2. Overview of HumanRF: Prior to training, our method starts by splitting the temporal domain into 4D segments with similar union occupancy in 3D\n(\u00a73.2). Each segment is modeled by a 4D feature grid which is compactly represented by utilizing tensor decomposition and hash grids (\u00a73.1). During training,\nwe sample a batch of rays across different time frames and cameras. After each pixel color is predicted via volume rendering (\u00a73.3), we enforce photometric\nconstraints and regularize ray marching weights via foreground masks (\u00a73.4).\nstatic PlenOctrees [Yu et al. 2021] into a dynamic representation\nusing DFT to achieve real-time inference.\nConcurrently to our work, Song et al. [2022] decompose the 4D\nspace into static, deforming and newly appeared regions, while Cao\nand Johnson [2023]; Fridovich-Keil et al. [2023]; Shao et al. [2022]\nalso propose to represent 4D scenes using low-rank decompositions\nwith 2D tensors. Unlike these methods, our method uses 3D and\n1D tensors. In \u00a75.3.1, we show our 3D-1D scheme is significantly\nbetter than its 2D-2D counterpart for rapid motions. Additionally,\nour method partitions a sequence into segments, which enables\ntraining at scale on modern GPUs without sacrificing quality.\n2.3\nNeural Human Performance Capture\nOur goal is closely related to neural radiance field-based methods\nthat specialize in rendering humans. This is often achieved by learn-\ning a canonical representation that is forward warped to a target\nframe [Chen et al. 2021; Wang et al. 2022a] or backward sampled\nfrom the observation space [Liu et al. 2021; Xu et al. 2022]. This\ndeformation can be guided by a learned template model such as\nSMPL [Loper et al. 2015] or a sparse skeleton. For instance, uti-\nlizing sparse landmarks, Noguchi et al. [2021]; Su et al. [2021]\nreparametrize radiance fields relative to the pose of the skeleton.\nTAVA [Li et al. 2022b] optimizes a canonical shape and forward-\nskinning weights based on skeleton pose. Neural Body [Peng et al.\n2021] employs a SMPL model to optimize a latent representation for\neach mesh vertex, while Liu et al. [2021] learns a backwards warping\ninto the canonical pose. While these methods achieve impressive re-\nsults, they also suffer from the innate limitations of template-based\napproaches; i.e., their approximate geometry (e.g., from SMPL) or\nambiguous pose conditioning (e.g., skeletal joints) often poses chal-\nlenges in novel view synthesis. This becomes particularly problem-\natic for fine-scale deformations such as dynamic cloth animations or\nlocal detail in the face which cannot be represented by existing geo-\nmetric template proxies. To address these challenges, recent efforts\nopt for template-free approaches. For instance, Zhang et al. [2022]\ntrain a time-conditioned network to predict hyper-spherical har-\nmonics for free-viewpoint human rendering while Zhao et al. [2022]\npropose a static-to-dynamic approach where per-frame neural sur-\nface reconstruction is combined with a hybrid neural tracker to\ngenerate neural animated human meshes. In our work, we also pro-\npose a template-free approach since we are aiming for the highest\nvisual quality.\n3\nMETHOD\nGiven a set of input videos of a human actor in motion, captured\nin a multi-view camera setting, our goal is to enable temporally\nconsistent, high-fidelity novel view synthesis. To that end, we learn\na 4D scene representation using differentiable volumetric rendering\n[Lombardi et al. 2019; Mildenhall et al. 2020], supervised via multi-\nview 2D photometric and mask losses that minimize the discrepancy\nbetween the rendered images and the set of input RGB images\nand foreground masks. To enable efficient photo-realistic neural\nrendering of arbitrarily long multi-view data, we use sparse feature\nhash-grids in combination with shallow multilayer perceptrons\n(MLPs) [M\u00fcller et al. 2022; Sun et al. 2022; Wang et al. 2021].\nThe core idea of HumanRF \u2013 as illustrated in Fig. 2 \u2013 is to partition\nthe time domain into optimally distributed temporal segments, and\nto represent each segment by a compact 4D feature grid (\u00a73.1). For\nthis purpose, we propose an extension to the TensoRF vector-matrix\ndecomposition of Chen et al. [2022b] \u2013 designed for static 3D scenes\n\u2013 that can support time-varying 4D feature grids. Our adaptive\ntemporal partitioning (\u00a73.2) ensures that the total 3D space volume\ncovered by each individual temporal segment is of similar size, which\nhelps our method achieve superior representation power, regardless\nof the temporal context. Furthermore, we use shallow MLPs to\ntransform features into density and view-dependent radiance to\nbe used in the volumetric rendering framework (\u00a73.3). Through\nsharing information across the temporal domain via both shared\nMLPs and 4D decomposition, our results are temporally consistent.\nWe refer to the accompanying videos regarding temporal stability.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:4\n\u2022\nI\u015f\u0131k et al.\nFig. 3. Fixed-segment size vs. adaptive partitioning Using a single 4D representation for an entire sequence (segment size 400) or using a 3D hash grid\nper frame (segment size 1), give poor results. We observe that finding the middle ground (segment sizes from 3 to 100) leads to better results (a, b, c). Sequences\nwith moderate motions favor larger segment sizes whereas those with stronger motions favor smaller ones (b). Our Adaptive Temporal Partitioning scheme\n(\u00a73.2) avoids the costly hyper-parameter search for the optimal, global segment size, and leads to results close to those of optimal segment sizes (a, b). On\naverage, our adaptive method is better than using any fixed segment size (c). These experiments are performed on 400-frame sequences using shared MLPs.\nThe total number of parameters is kept approximately the same while varying the segment size.\nWe supervise our differentiable rendering pipeline with 2D-only\nlosses that measure the errors between the rendered and input RGB\nimages and foreground masks (\u00a73.4).\n3.1\n4D Feature Grid Decomposition\nOur method models a dynamic 3D scene by combining optimally\npartitioned 4D segments. Each segment \ud835\udc58 has its own trainable 4D\nfeature grid\ud835\udc47 (\ud835\udc58)\n\ud835\udc65\ud835\udc66\ud835\udc67\ud835\udc61 : R4 \u21a6\u2192 R\ud835\udc5a which encodes a set of \ud835\udc41\ud835\udc58 consecutive\nframes T (\ud835\udc58) \u2208 {\ud835\udc61\ud835\udc60,\ud835\udc61\ud835\udc60+1,\ud835\udc61\ud835\udc60+2, ...,\ud835\udc61\ud835\udc60+\ud835\udc41\ud835\udc58\u22121}. Previous works [Chen\net al. 2022b; M\u00fcller et al. 2022; Tang et al. 2022] have shown that\ndense 3D data exhibits redundancies and can be represented more\ncompactly. We make the same argument for spatio-temporal data,\nand define our 4D feature grid as a decomposition of four 3D and\nfour 1D feature grids (\ud835\udc58 is dropped for brevity below):\n\ud835\udc47\ud835\udc65\ud835\udc66\ud835\udc67\ud835\udc61 (p\ud835\udc65\ud835\udc66\ud835\udc67\ud835\udc61) = \ud835\udc47\ud835\udc65\ud835\udc66\ud835\udc67(p\ud835\udc65\ud835\udc66\ud835\udc67) \u2299 \ud835\udc47\ud835\udc61 (p\ud835\udc61)\n+\ud835\udc47\ud835\udc65\ud835\udc66\ud835\udc61 (p\ud835\udc65\ud835\udc66\ud835\udc61) \u2299 \ud835\udc47\ud835\udc67(p\ud835\udc67)\n+\ud835\udc47\ud835\udc65\ud835\udc67\ud835\udc61 (p\ud835\udc65\ud835\udc67\ud835\udc61) \u2299 \ud835\udc47\ud835\udc66(p\ud835\udc66)\n+\ud835\udc47\ud835\udc66\ud835\udc67\ud835\udc61 (p\ud835\udc66\ud835\udc67\ud835\udc61) \u2299 \ud835\udc47\ud835\udc65 (p\ud835\udc65)\n,\n(1)\nwhere \u2299 denotes the hadamard product, and p\ud835\udc65\ud835\udc66\ud835\udc67\ud835\udc61 \u2208 R4 is the\nqueried point. We represent each 3D grid (\ud835\udc47\ud835\udc65\ud835\udc66\ud835\udc67,\ud835\udc47\ud835\udc65\ud835\udc66\ud835\udc61,\ud835\udc47\ud835\udc65\ud835\udc67\ud835\udc61,\ud835\udc47\ud835\udc66\ud835\udc67\ud835\udc61 :\nR3 \u21a6\u2192 R\ud835\udc5a) with a multi-resolution hash grid [M\u00fcller et al. 2022] \u2013\nwhich has proven to be more efficient than using dense 3D grids\n\u2013 and each 1D grid (\ud835\udc47\ud835\udc61,\ud835\udc47\ud835\udc67,\ud835\udc47\ud835\udc66,\ud835\udc47\ud835\udc65 : R \u21a6\u2192 R\ud835\udc5a) with dense array of\nvectors. In Fig. 9, we show that this compact representation lets our\nmethod surpass the quality of per-frame Instant-NGP while using\nonly a fraction of the number of trainable parameters.\n3.2\nAdaptive Temporal Partitioning\nUsing a single 4D feature grid for an entire sequence becomes im-\npractical for longer sequences. Figure 3 shows that representing\na long sequence with a single 4D segment performs significantly\nworse than using multiple fixed-sized segments, especially when\nconsidering the total hash capacities are roughly the same. Therefore,\npartitioning the sequence plays a critical role in our representation.\nFigure 3 also highlights the impact of motion complexity on the opti-\nmal segment size (stronger deformations require shorter segments).\nTo mitigate the prohibitive cost of hyper-parameter search for the\noptimal segment size, we present a greedy algorithm to adaptively\nselect the sizes of segments prior to training. Unlike fixed-size tem-\nporal partitioning, our method does not require a unique segment\nsize which would be less suited to very long sequences.\nOccupancy Grids. To reason about temporal changes, we analyze\nper-frame occupancy grids \ud835\udc42\ud835\udc61 : P \u2282 R3 \u21a6\u2192 {0, 1} that we com-\npute by carving the free space [Kutulakos and Seitz 2000] using\nforeground masks. We define several terms that will be relevant\nhereafter. First, we define the occupancy grid of a set of frames T\nby the logical union of their occupancy grids:\n\ud835\udc42 T (p) =\n\u00dc\n\ud835\udc61\ud835\udc57 \u2208T\n\ud835\udc42\ud835\udc61\ud835\udc57 (p) .\n(2)\nSecond, for a set of frames T, total occupancy is defined as the\nnumber of occupied voxels:\n\ud835\udeff(T) =\n\u2211\ufe01\np\ud835\udc57 \u2208P\n\ud835\udc42 T (p\ud835\udc57) .\n(3)\nFinally, for a set of \ud835\udc41 consecutive frames T = {\ud835\udc610,\ud835\udc611,\ud835\udc612, ...,\ud835\udc61\ud835\udc41 \u22121},\nwe define the expansion factor as:\n\ud835\udf19(T) = \ud835\udeff(T)\n\ud835\udeff({\ud835\udc610}) ,\n(4)\nwhich practically indicates how much the union occupancy grid is\nenlarged from \ud835\udc610 onwards, and positively correlates with the motion\ncomplexity.\nCriteria for spawning new segments. Given a fixed budget of total\nnumber of trainable parameters, our objective is to keep the ex-\npansion factor (Equation (4)) similar for each segment. To this end,\nwe iterate over each frame with a greedy heuristic and spawn a\nnew segment when the expansion factor exceeds a certain thresh-\nold. This ensures that each segment represents a similar amount of\nvolume in 3D space, which leads to a fair distribution of the total\nrepresentation workload. This can also be regarded as maximizing\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nHumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion\n\u2022\n1:5\nthe efficiency of the 4D decomposition models by adjusting the\ntemporal context of each segment such that the temporal sharing\nis encouraged for smaller movements and discouraged for larger\nones. In Figure 8, we experiment with several threshold values for\nthe expansion factor, and set it to 1.25 for all our experiments.\n3.3\nShared MLPs and Volume Rendering\nSimilarly to previous work [Mildenhall et al. 2020], we describe the\ndistribution of the radiance in a scene at time instance \ud835\udc61 using the\nvolumetric rendering formulation with emission and absorption:\n\u02c6\ud835\udc36(\ud835\udc93,\ud835\udc61) =\n\u222b \ud835\udefc\ud835\udc5a\ud835\udc4e\ud835\udc65\n\ud835\udefc\ud835\udc5a\ud835\udc56\ud835\udc5b\n\ud835\udc47 (\ud835\udefc)\ud835\udf0e(\ud835\udc93(\ud835\udefc),\ud835\udc61)\ud835\udc3f(\ud835\udc93(\ud835\udefc), d,\ud835\udc61)\ud835\udc51\ud835\udefc ,\n(5)\nwhere\ud835\udc47 (\ud835\udefc) is the transmittance, \ud835\udc93(\ud835\udefc) is the point on the ray \ud835\udc93 at dis-\ntance \ud835\udefc, \ud835\udf0e(\ud835\udc93(\ud835\udefc),\ud835\udc61) denotes the volumetric density, and \ud835\udc3f(\ud835\udc93(\ud835\udefc), d,\ud835\udc61)\nindicates the radiance emitted along the direction d. We solve this\nintegral numerically using quadrature [Max 1995]. Similarly to\nM\u00fcller et al. [2022], we use two shallow MLPs to model density\nand view-dependent radiance. First, we leverage a 3-layer network,\nMLP\ud835\udf0e : R32 \u21a6\u2192 R16, to generate density \ud835\udf0e(p,\ud835\udc61) \u2208 R and geometry\nfeatures \ud835\udc39 (p,\ud835\udc61) \u2208 R15 for any point p \u2208 R3 in time \ud835\udc61 of segment \ud835\udc58\n{\ud835\udf0e(p,\ud835\udc61), \ud835\udc39 (p,\ud835\udc61)} = MLP\ud835\udf0e (\ud835\udc47 (\ud835\udc58)\n\ud835\udc65\ud835\udc66\ud835\udc67\ud835\udc61 (p,\ud835\udc61)) .\n(6)\nThen, we employ a 4-layer network MLP\ud835\udc3f : R31 \u21a6\u2192 R3 to produce\nview-dependent RGB radiance values:\n\ud835\udc3f(p, d,\ud835\udc61) = MLP\ud835\udc3f(SH(d), \ud835\udc39 (p,\ud835\udc61)) ,\n(7)\nwhere SH(d) \u2208 R16 is the encoding of the viewing direction d\nformed by using the first 4 bands of the spherical harmonics. Al-\nthough each spatio-temporal segment has its own trainable 4D\nfeature grid, these two MLPs are shared by an entire sequence.\n3.4\nLosses\nWe utilize RGB images and masks to guide the training. First, we\nenforce the Huber loss [Collins 1976] between the ground truth\ncolor \ud835\udc36(\ud835\udc93,\ud835\udc61) and the predicted pixel color \u02c6\ud835\udc36(\ud835\udc93,\ud835\udc61) (Equation (5)):\nL\ud835\udc5d\u210e\ud835\udc5c =\n1\n|R|\n\u2211\ufe01\n\ud835\udc93 \u2208R\n( 1\n2\ud835\udc592,\nif \ud835\udc59 \u2264 \ud835\udeff\n\ud835\udeff \u00b7 (\ud835\udc59 \u2212 1\n2\ud835\udeff),\notherwise ,\n(8)\nwhere \ud835\udc59 = |\ud835\udc36(\ud835\udc93,\ud835\udc61) \u2212 \u02c6\ud835\udc36(\ud835\udc93,\ud835\udc61)|. This loss is averaged over 3 color\nchannels and we set \ud835\udeff = 0.01 in all of our experiments.\nIn addition to background removal and occupancy grid computa-\ntion, we use foreground masks to regularize volumetric occupancy\nsimilarly to Yariv et al. [2020]. More specifically, we use the binary\ncross entropy loss between the ground truth mask \ud835\udc40(\ud835\udc93) and the\naccumulated volume rendering weight \u02c6\ud835\udc40(\ud835\udc93):\nL\ud835\udc4f\ud835\udc50\ud835\udc52 =\n1\n|R|\n\u2211\ufe01\n\ud835\udc93 \u2208R\n\u0002\n\ud835\udc40(\ud835\udc93) log( \u02c6\ud835\udc40(\ud835\udc93)) + (1 \u2212 \ud835\udc40(\ud835\udc93)) log(( \u02c6\ud835\udc40(\ud835\udc93))\n\u0003\n, (9)\nwhere \ud835\udc40(\ud835\udc93) = 1 and \ud835\udc40(\ud835\udc93) = 0 denote the pixels on the foreground\nand the background, respectively, and \u02c6\ud835\udc40(\ud835\udc93) is defined as follows,\n\u02c6\ud835\udc40(\ud835\udc93) =\n\u222b \ud835\udefc\ud835\udc5a\ud835\udc4e\ud835\udc65\n\ud835\udefc\ud835\udc5a\ud835\udc56\ud835\udc5b\n\ud835\udc47 (\ud835\udefc)\ud835\udf0e(\ud835\udc93(\ud835\udefc))\ud835\udc51\ud835\udefc .\n(10)\nFig. 4. Dataset resolution We show closeups for two actors in our\nActorsHQ dataset. The cameras record at 12MP each, thus enabling the\ncapture of eyelashes, wrinkles, and hair strands.\nThis loss helps to prune the empty space early in the training, which\nleads to significant speed up in the training iterations. Our final loss\nterm is defined as\nL = L\ud835\udc5d\u210e\ud835\udc5c + \ud835\udefdL\ud835\udc4f\ud835\udc50\ud835\udc52 ,\n(11)\nwhere we set \ud835\udefd = 10\u22123 for all of our experiments. We refer the\nreader to the supplemental material for additional training details.\n4\nDATASET\nOur dataset, ActorsHQ, consists of 39, 765 frames of dynamic hu-\nman motion captured using multi-view video. We used a propri-\netary multi-camera capture system combined with an LED array\nfor global illumination. The camera system comprises 160 12MP\nXimea cameras operating at 25fps. Close-up details that are cap-\ntured at this resolution are highlighted in Fig. 4. The lighting system\nprovides a programmable lighting array of 420 LEDs that are time-\nsynchronized to the camera shutter. All cameras were set to a shutter\nspeed of 650us to minimize motion blur for fast actions. We addi-\ntionally reconstruct each frame independently using state-of-the-art\nmulti-view stereo from RealityCapture [Epic Games 2022] with ap-\nproximately 500k faces per frame (Fig. 5). The camera array was\nconfigured to cover a capture volume of 1.6m diameter and 2.2m\nheight, enabling actors to perform a range of motions at the center.\nThe dataset comprises 4 female and 4 male actors. Each actor\nperformed two 100 second motion sequences of choreographed\nactions wearing everyday clothing. The actors wore either short or\nlong upper and lower body clothing to provide variation in cloth\ndynamics. In the first sequence, each actor followed the same set of\n32 actions that activate key joint rotations for shoulders, arms, legs,\nand torso as well as combined joint activations. The actors were\ndirected to return to a resting A-pose between sets of actions. In\nthe second sequence, the actors performed 20 randomly selected\neveryday actions designed to produce more exaggerated body poses\nsuch as sports, dance, celebration, and gestures. The actors were\ndirected to move continuously throughout the capture to provide\nmore exaggerated body dynamics in motion. A comparison of the\nActorsHQ with other standard benchmarks is tabulated in Table 1.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:6\n\u2022\nI\u015f\u0131k et al.\nFig. 5. Actors and meshes. Our ActorsHQ dataset contains 8 actors with casual clothing such as skirts or shorts. Each sequence is captured by 160 camera,\neach recording at 12MP. In addition to the recorded images, we also provide high-quality, per-frame mesh reconstructions with approximately 500k vertices.\nTable 1. Comparison of multi-view human video datasets. Our new\ndataset features longer sequences with more cameras at a higher resolution.\nDataset\n#ID\n#Frames\nResolution\n#Cameras\nHuman3.6M [Ionescu et al. 2013]\n11\n581k\n1MP\n4\nMPI-INF-3DHP [Mehta et al. 2017]\n8\n>1.3M\n4MP\n14\nZJU-Mocap [Peng et al. 2021]\n9\n< 2,700\n1MP\n21\nDynaCap [Habermann et al. 2021]\n5\n27k\n1.2MP\n50-101\nTHUman [Zheng et al. 2019]\n500\n500k\n0.4MP\n4\nTHUman4 [Zheng et al. 2022]\n3\n<15k\n1.4MP\n24\nActorsHQ (ours)\n8\n39,765\n12MP\n160\n5\nEVALUATION\nTo demonstrate the ability of HumanRF to represent long sequences\nand fine details at 12MP, we perform extensive quantitative and\nqualitative experiments with our ActorsHQ dataset. As HumanRF\nis a temporal method, we also highly recommend watching the\nsupplementary videos.\nWe compare our method against six state-of-the-art baselines.\nThere are three deformation-based approaches for general scenes:\nNDVG [Guo et al. 2022b], HyperNeRF [Park et al. 2021b], and TiNeu-\nVox [Fang et al. 2022b] along with two human-specific methods:\nNeural Body [Peng et al. 2021] and TAVA [Li et al. 2022b]. As an\nadditional baseline, we train Instant-NGP [M\u00fcller et al. 2022] inde-\npendently on each frame. For all baselines, we use the official imple-\nmentations that are publicly available and tune hyper-parameters\nto achieve best possible results. A visual comparison between the\nbaselines and our method can be found on Fig. 6 and Fig. 11.\n5.1\nEvaluation Protocol\nIn all experiments, we use the same set of 124 training cameras, 10\nvalidation cameras and 14 test cameras. For one frontal test cam-\nera we render a video that is used to compute the VMAF [Li et al.\n2016] score, and we alternate through the remaining test cameras\nto compute PSNR, LPIPS [Zhang et al. 2018] and SSIM [Wang et al.\n2004]. The numerical results are averaged over 8 actors for all the\nexperiments. As some baselines fail to produce reasonable results\nat full resolution, we compare on 4\u00d7 downscaled data (per axis),\nproducing better relative performance compared to our results. To\ntest the performance of HumanRF on high-resolution data, we per-\nform additional experiments in full resolution (12MP) in \u00a75.4. More\ndetails on the protocol can be found in the supplementary material.\n5.2\nQuality vs Number of Frames\nIn Table 2, we analyze each method for various sequence lengths\nusing per-frame metrics PSNR, LPIPS, and SSIM, as well as the\ntemporal metric VMAF, which measures perceptual quality of the\ngenerated videos and correlates well with temporal consistency.\nDue to our efficient 4D feature grid structure and its ability to scale\nto arbitrarily long sequences via temporal partitioning, HumanRF\nconsistently outperforms the baselines. Existing methods that pre-\ndict a deformation field struggle to represent long sequences with\ncomplex motion. This is mainly due to rapid topological changes and\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nHumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion\n\u2022\n1:7\nTable 2. Numerical evaluation on ActorsHQ. We demonstrate results\nusing the standard visual metrics and VMAF to measure perceptual video\nquality. HumanRF outperforms baselines on all sequences. Instant-NGP \u2013\ntrained per frame separately \u2013 demonstrates better LPIPS, but struggles in\nterms of temporal consistency and memory footprint (Fig. 9). The best\nand the second best results are highlighted.\nMethod\nMetric\n20\n50\n100\n250\n500\n1000\nOurs\n\u2193 LPIPS\n0.095\n0.100\n0.097\n0.100\n0.102\n0.107\n\u2191 PSNR\n30.30\n30.05\n29.83\n29.26\n29.34\n29.05\n\u2191 SSIM\n0.918\n0.918\n0.921\n0.920\n0.919\n0.913\n\u2191 VMAF\n83.67\n84.43\n85.62\n85.28\n85.33\n85.74\nInstant-NGP\n\u2193 LPIPS\n0.095\n0.092\n0.094\n0.093\n0.093\n0.093\n\u2191 PSNR\n29.45\n28.78\n28.96\n28.77\n28.73\n28.85\n\u2191 SSIM\n0.881\n0.898\n0.902\n0.904\n0.904\n0.905\n\u2191 VMAF\n74.15\n73.23\n76.70\n76.77\n77.28\n77.60\nTiNeuVox\n\u2193 LPIPS\n0.312\n0.305\n0.327\n0.346\n0.348\n0.371\n\u2191 PSNR\n26.07\n24.14\n23.11\n21.82\n20.94\n19.80\n\u2191 SSIM\n0.792\n0.800\n0.794\n0.792\n0.786\n0.772\n\u2191 VMAF\n56.26\n47.31\n40.27\n31.11\n24.74\n18.11\nNDVG\n\u2193 LPIPS\n0.268\n0.275\n0.300\n0.338\n0.367\n0.391\n\u2191 PSNR\n26.60\n23.65\n22.16\n19.75\n17.93\n16.17\n\u2191 SSIM\n0.823\n0.811\n0.793\n0.765\n0.741\n0.716\n\u2191 VMAF\n63.26\n50.53\n38.13\n21.88\n12.76\n6.183\nHyperNeRF\n\u2193 LPIPS\n0.250\n0.235\n0.251\n0.270\n0.302\n0.325\n\u2191 PSNR\n25.70\n25.23\n24.72\n23.82\n22.58\n21.77\n\u2191 SSIM\n0.820\n0.832\n0.826\n0.817\n0.801\n0.790\n\u2191 VMAF\n73.08\n73.05\n67.17\n57.51\n44.84\n37.01\nNeural Body\n\u2193 LPIPS\n0.305\n0.308\n0.310\n0.318\n0.340\n0.367\n\u2191 PSNR\n27.03\n25.16\n26.92\n24.66\n24.35\n25.58\n\u2191 SSIM\n0.806\n0.807\n0.805\n0.805\n0.793\n0.767\n\u2191 VMAF\n46.72\n45.27\n41.83\n38.71\n32.13\n26.95\nTAVA\n\u2193 LPIPS\n0.270\n0.277\n0.295\n0.344\n0.388\n0.429\n\u2191 PSNR\n27.44\n25.75\n25.05\n23.61\n22.40\n21.50\n\u2191 SSIM\n0.820\n0.821\n0.816\n0.792\n0.765\n0.740\n\u2191 VMAF\n66.92\n58.66\n54.28\n38.40\n23.45\n13.34\nlimited representation power for deformations, and the effect is typi-\ncally reflected in both the per-frame and temporal metrics. Previous\nworks [Li et al. 2022c; Shao et al. 2022] have also observed similar\ndisadvantages on complex or fast-changing scenes for deformation-\nbased approaches. Although human-specific baselines perform bet-\nter than deformation-based ones on average, they still lack visual\ndetails, and tend to produce blurry results compared to HumanRF\nas illustrated in Fig. 6. On the other hand, per-frame Instant-NGP\nexcels on per-frame metrics, but it lacks temporal stability, and uses\n20\u00d7 more trainable parameters compared to our method (Fig. 9).\n5.3\nDesign Choices\nIn this section, we validate the effectiveness of our approach, and\nclarify how we select several hyper parameters. More specifically,\nwe perform ablation studies regarding the 4D feature grid repre-\nsentation (\u00a75.3.1). We discuss choosing the optimal grid resolution,\nfeature dimensionality and hash size (\u00a75.3.2), and argue how seg-\nment sizes and expansion factor thresholds are determined (\u00a75.3.3).\nNeural Body\nTAVA\nOurs\nGT\nFig. 6. Comparison with human-specific methods. Although Li et al.\n[2022b]; Peng et al. [2021] incorporate geometry and pose information,\nthey fail to capture fine details, and produce blurrier results compared to\nHumanRF.\n5.3.1\n4D Feature Grid. To validate our choice of feature grid repre-\nsentation, we run a comparison against two ablations of our method\nwhere we only alter the 4D feature grid and use a single segment\nover 100-frame sequences. The first variant simply concatenates\n3D spatial coordinates with time to form 4D input to a hash grid \u2013\nwe dub this as tNGP. The second variant also utilizes a 4D decom-\nposition by using six multi-resolution 2D dense grids inspired by\nthe concurrent work [Cao and Johnson 2023; Fridovich-Keil et al.\n2023] \u2013 which we dub as Hex4D. Please refer to supplemental to\nsee how Hex4D is formulated. Unlike Hex4D, our method uses four\nmulti-resolution 3D hash grids and four 1D dense grids. Table 3\nindicates that using a decomposition model (Hex4D and ours) for\nthe feature grid is superior when the motion is moderate as the tem-\nporal context can be efficiently compressed into lower-rank tensors.\nAlthough Hex4D loses its advantage over tNGP for stronger motion,\nour method consistently outperforms both ablations in both cases.\n5.3.2\nGrid Resolution and Feature Dimensionality. In order to deter-\nmine the optimal grid resolution and feature dimensionality of the\n4D feature grid, we perform a parameter search in two dimensions:\nfinest grid resolution (\ud835\udc3emax) and per-level feature dimensionality\n(\ud835\udc39). To facilitate the search, we perform the experiments on 4\u00d7\ndownscaled data over 100-frame sequences, and we fix the coarsest\nresolution in our multi-resolution grids to \ud835\udc3emin = 32 and number\nof resolution levels to \ud835\udc3f = 16. To narrow down the search even\nfurther, we fix the total number of trainable parameters per hash\ngrid as \ud835\udc47 \u00b7 \ud835\udc3f \u00b7 \ud835\udc39 = 224, where \ud835\udc47 denotes per-level hash size. Finally,\nwe restrict hash size to be\ud835\udc47 \u2264 219, because further increase leads to\nperformance penalty, which is also reported by M\u00fcller et al. [2022].\nFrom the results presented in Fig. 7, we pick \ud835\udc3emax = 2048 and \ud835\udc39 = 2.\nFor experiments carried out in full resolution (see \u00a75.4 and Fig. 10),\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:8\n\u2022\nI\u015f\u0131k et al.\n256\n512\n1024\n2048\n4096\nFinest Hash Grid Resolution\n0.09\n0.10\n0.11\n0.12\n0.13\n0.14\n0.15\n0.16\nLPIPS\n\ud439 = 2, \ud447 = 219\n\ud439 = 4, \ud447 = 218\n\ud439 = 8, \ud447 = 217\nFig. 7. Optimal grid resolution and feature dimensionality. To deter-\nmine the ideal parameters for our feature grid, we search through several\nfinest grid resolutions (\ud835\udc3emax) and feature dimensionalities (\ud835\udc39) while fixing\nthe number of parameters. As a result, we use \ud835\udc3emax = 2048 and \ud835\udc39 = 2.\nTable 3. Comparison between different feature grid representations.\nHex4D outperforms tNGP for scenes with moderate motion due to its abil-\nity to compress, however, its quality heavily degrades for rapid motion.\nOn the other hand, our method benefits from its compact nature while\nnot sacrificing the quality as much as Hex4D, and it consistently outper-\nforms both ablations. We use default multi-resolution grid parameters for\nthese ablations (see \u00a75.3.2). The best and the second best results are\nhighlighted.\nMetric\nModerate Motion\nStrong Motion\nHex4D\ntNGP\nOurs\nHex4D\ntNGP\nOurs\n\u2193 LPIPS\n0.105\n0.129\n0.090\n0.184\n0.126\n0.110\n\u2191 PSNR\n29.89\n29.27\n30.79\n26.04\n28.10\n28.87\n\u2191 SSIM\n0.915\n0.906\n0.931\n0.851\n0.902\n0.906\n\u2191 VMAF\n77.50\n79.18\n81.15\n75.22\n87.76\n89.00\nwe set \ud835\udc3emax = 8192 and use \ud835\udc3f = 24 to maintain the model capacity\nso that the finer details can be reconstructed.\n5.3.3\nModel Size. Our method uses 4D spatio-temporal segments\nwith various lengths to represent arbitrarily long sequences. Adap-\ntive temporal partitioning (\u00a73.2) tries to keep the number of trainable\nparameters per frame approximately the same in order not to lose\nits representation power. For this reason, the number of trainable\nparameters scales linearly with the sequence length. Nonetheless,\nour method remains more compact than most of the baselines. In Fig.\n9, we demonstrate that HumanRF uses only 5.2% of the parameters\ncompared to per-frame Instant-NGP while outperforming it.\nPredefined segment sizes. During adaptive temporal partitioning,\nwe choose segment sizes from a pool of predefined lengths where\neach one has a hash capacity proportional to its size. For our ex-\nperiments, we specifically use the segment sizes 6, 12, 25, 50, 100\nwith per-level hash sizes 215, 216, 217, 218, 219, respectively, using the\nTiny CUDA neural networks framework [M\u00fcller 2021]. Moreover, we\ndemonstrate the effect of motion complexity and expansion factor\nthreshold on average segment size in Fig. 8.\n6\n12\n25\n50\nAverage Segment Size\n0.102\n0.105\n0.108\n0.111\n0.114\nLPIPS\n1.10\n1.25\n1.40\n1.55\n1.70\n1.10\n1.25\n1.40\n1.55\n1.70\n1.10\n1.25\n1.40\n1.55\n1.70\nmoderate motion\nstrong motion\nall averaged\nFig. 8. Impact of the expansion factor thresholds and motion com-\nplexity on average segment size and quality. Larger threshold values\n(indicated by numbers) lead to larger segments on average. Unlike using\nfixed-size segments, we do not observe a striking difference in quality when\nthe average segment size changes (see Fig. 3 for a comparison). Furthermore,\nrapid motions increase the frequency of spawning new segments, and hence\nlead to smaller segment sizes.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCompression Ratio w.r.t. per-frame Instant-NGP\n20\n22\n24\n26\n28\n30\n32\nPSNR\n1221 mil.\n339 mil.\n244 mil.\n63 mil.\n4 mil.\nInstant-NGP\nTiNeuVox\nNDVG\nOurs\nHyperNeRF\nFig. 9. Compression ratio vs. PSNR on 100-frame sequence. We\ndemonstrate number of parameters for each baseline (bold numbers) and\nthe compression ratio with respect to per-frame Instant-NGP. The resulted\nratio for HumanRF is near the most compact one with the best PSNR quality.\nHere, we define compression ratio for a method M as (1 \u2212\n\ud835\udc43M\n\ud835\udc43I-NGP ) where\n\ud835\udc43M denotes the number of trainable parameters.\n5.4\nInput Resolution\nPrevious publicly available datasets provide images at the resolution\nof 4MP or less, but our method is designed to capture details beyond\nthis resolution. Fig. 10 illustrates the impact of using downscaled\ntraining data on the rendering quality at full resolution. We observe\nthat HumanRF can recover finer details as the input resolution\nincreases \u2013 see supplemental for numerical results. While it seems\nnatural that scores improve with increasing resolution of training\ndata, we observed that some baselines struggle to represent high-\nfidelity data and deteriorate instead.\n5.5\nDynamic Furry Animal Dataset\nAlthough HumanRF is tailored to ActorsHQ, it is a template-free\nmethod which is not necessarily restricted to humans. In fact, our\nmethod can be applied to any scene with a foreground object with\nmasks. To demonstrate this ability, we run our method on Dynamic\nFurry Animal (DFA) [Luo et al. 2022] which is a multi-view dataset\nof furry animals in motion. In Table 4, we demonstrate that our\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nHumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion\n\u2022\n1:9\n(a) ours 1\u00d7\n22.61\n0.149\n(b) ours 4\u00d7\n25.14\n0.042\n(c) ours 2\u00d7\n25.40\n0.042\n(d) ours 1\u00d7\n(e) reference 1\u00d7\n21.37\n0.141\n24.51\n0.083\n25.58\n0.075\n27.73\n0.193\n28.22\n0.159\n28.85\n0.146\n24.79\n0.072\n25.44\n0.053\n26.11\n0.047\nFig. 10. Impact of input resolution on the full-resolution results. We\nillustrate the significance of high resolution data by training HumanRF\non 4\u00d7 downscaled (b), 2\u00d7 downscaled (c) and full-resolution (a, c) input to\ngenerate full-resolution results. We observe striking differences in capturing\nfiner details as the resolution increases, which is also reflected in highlighted\n\u2191PSNR and \u2193LPIPS values.\nmethod can be applied to non-human scenes and it can still surpass\nthe quality of the state of the art. For these experiments, we use\ndefault settings (\u00a75.3.2) except by setting \ud835\udc3f = 20 to account for\nhigh-frequent fur details. Additional visual results can be found in\nthe supplementary material.\n5.6\nLimitations and Future Work\nOur method can produce high-fidelity radiance field reconstruc-\ntions of humans in motion, achieving accurate results on novel\nview synthesis; however, important limitations remain. To achieve\nsuch high-quality results, HumanRF relies on our newly-introduced\nActorsHQ dataset, and optimizes a separate radiance field for each\nsequence. It would be interesting to explore training a model on\nhigh-end recordings which could then be used as an avatar to target\nmonocular-only test sequences. While our method reconstructs each\nframe of a motion sequence, we still do not have explicit control over\narticulation of the actor outside the training poses. One possible way\nto gain control could be to learn a deformation network for each\nsegment, or to operate with a parametric model to control explicit\nparameters. At the same time, there is also significant room to speed\nup render times of our method. Here, a promising direction could\nbe the conversion of our reconstructed radiance field into a hybrid,\nimplicit-explicit representation such as in MobileNeRF [Chen et al.\n2022a]. Finally, although our model is temporally-stable, the fore-\nground masks are not necessarily consistent across different time\nframes because they are inferred from independent, per-frame mesh\nreconstructions, which leads to flickering effect on the silhouette\nedges. Here, our work would benefit from temporally-consistent\nbackground matting techniques, such as Lin et al. [2022].\nTable 4. Evaluation on the Dynamic Furry Animal (DFA) Dataset\n[Luo et al. 2022]. We show that HumanRF can reconstruct radiance fields\nfor dynamic sequences of non-human subjects, such as animals in the DFA\ndataset. Our method achieves state-of-the-art results even though all the\nbaselines except NeuralVolumes uses skeleton information provided in the\nDFA. For the starred (*) methods, we use the results from Table 1 in Luo et al.\n[2022] , and use the exact same evaluation configuration for our method.\nThe best and the second best results are highlighted.\nMethod\nMetric\nPanda\nCat\nDog\nLion\nOurs\n\u2193 LPIPS\n0.030\n0.008\n0.013\n0.025\n\u2191 PSNR\n36.00\n38.43\n37.79\n35.40\n\u2191 SSIM\n0.986\n0.992\n0.986\n0.979\nArtemis*\n\u2193 LPIPS\n0.031\n0.012\n0.022\n0.035\n\u2191 PSNR\n33.63\n37.54\n38.95\n33.09\n\u2191 SSIM\n0.985\n0.989\n0.989\n0.966\nAnimatable NeRF*\n\u2193 LPIPS\n0.112\n0.061\n0.074\n0.123\n\u2191 PSNR\n26.51\n31.37\n31.19\n27.87\n\u2191 SSIM\n0.957\n0.973\n0.975\n0.944\nNeural Volumes*\n\u2193 LPIPS\n0.116\n0.087\n0.129\n0.123\n\u2191 PSNR\n30.11\n28.14\n26.80\n29.59\n\u2191 SSIM\n0.965\n0.951\n0.945\n0.947\nNeural Body*\n\u2193 LPIPS\n0.110\n0.067\n0.075\n0.111\n\u2191 PSNR\n30.38\n30.77\n32.27\n30.11\n\u2191 SSIM\n0.970\n0.972\n0.978\n0.956\n6\nCONCLUSION\nWe have presented HumanRF, a novel method to reconstruct a\nspatio-temporal radiance field that captures human performance at\nhigh-fidelity. At the core of our method lies an intra-frame decompo-\nsition of a 4D representation based on a multi-resolution hash grid\nto capture details. To handle arbitrarily long sequences with a prac-\ntical memory budget, we introduce an adaptive splitting technique\nto share as many features as possible between frames and produce a\nmemory-efficient representation. To demonstrate the advantages of\nour method, we have introduced ActorsHQ, the first publicly avail-\nable multi-view dataset captured with 160 cameras recording 12MP\nfootage. Our results have shown high-quality free-viewpoint video,\nwhich we believe makes an important step towards production-level\nnovel view synthesis. Finally, we hope that the release of ActorsHQ\ndataset and the source code for HumanRF will enable researchers\nto drive new advances in photo-realistic reconstruction of virtual\nhumans.\nACKNOWLEDGMENTS\nWe thank Lee Perry-Smith and Henry Pearce at Infinite Realities\nLtd for specialist volumetric scanning services to build ActorsHQ\nusing the AEONX Motion Scanning System. Thanks also to Carolin\nHecking-Veltman, Daniel Thul, and Tymoteusz Bleja for their gen-\nerous efforts to help prepare the dataset, Haimin Luo for providing\nthe details of the evaluation for DFA, and the anonymous Siggraph\nreviewers for their valuable suggestions.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:10\n\u2022\nI\u015f\u0131k et al.\nNDVG\nHyperNeRF\nTiNeuVox\nNGP\nOurs\nGT\nFig. 11. Qualitative comparison. The synthesis quality of HumanRF is visually compared to the 4 baselines NDVG [Guo et al. 2022b], HyperNeRF [Park\net al. 2021b], TiNeuVox [Fang et al. 2022b] and per-frame NGP [M\u00fcller et al. 2022] using a sequence of 100 frames. While deformation-based baselines tend to\nproduce blurry results and can fail to capture rapid motions, NGP and ours are able to generate crisp images that are close to groundtruth.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nHumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion\n\u2022\n1:11\nREFERENCES\nMichael Broxton, John Flynn, Ryan Overbeck, Daniel Erickson, Peter Hedman, Matthew\nDuvall, Jason Dourgarian, Jay Busch, Matt Whalen, and Paul Debevec. 2020. Im-\nmersive light field video with a layered mesh representation. ACM Transactions on\nGraphics (TOG) 39, 4 (2020), 86\u20131.\nAng Cao and Justin Johnson. 2023. HexPlane: A Fast Representation for Dynamic\nScenes. CVPR (2023).\nJoel Carranza, Christian Theobalt, Marcus A. Magnor, and Hans-Peter Seidel. 2003.\nFree-viewpoint video of human actors. In ACM Transactions on Graphics (TOG).\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. 2022b. TensoRF:\nTensorial Radiance Fields. In European Conference on Computer Vision (ECCV).\nXu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges, and Andreas Geiger. 2021.\nSNARF: Differentiable forward skinning for animating non-rigid neural implicit\nshapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision.\n11594\u201311604.\nZhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. 2022a.\nMobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field\nrendering on mobile architectures. arXiv preprint arXiv:2208.00277 (2022).\nAlvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese,\nHugues Hoppe, Adam G. Kirk, and Steve Sullivan. 2015. High-quality streamable\nfree-viewpoint video. In ACM Transactions on Graphics (TOG), Vol. 34. 1 \u2013 13.\nJohn R Collins. 1976. Robust estimation of a location parameter in the presence of\nasymmetry. The Annals of Statistics (1976), 68\u201385.\nInc. Epic Games. 2022. RealityCapture. https://www.capturingreality.com Accessed:\n2023-01-12.\nJiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu,\nMatthias Nie\u00dfner, and Qi Tian. 2022a. Fast Dynamic Radiance Fields with Time-\nAware Neural Voxels. arXiv preprint arXiv:2205.15285 (2022).\nJiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu,\nMatthias Nie\u00dfner, and Qi Tian. 2022b. Fast Dynamic Radiance Fields with Time-\nAware Neural Voxels. In SIGGRAPH Asia 2022 Conference Papers.\nSara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k Warburg, Benjamin Recht, and\nAngjoo Kanazawa. 2023. K-Planes: Explicit Radiance Fields in Space, Time, and\nAppearance. In CVPR.\nSara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and\nAngjoo Kanazawa. 2022. Plenoxels: Radiance Fields Without Neural Networks. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n5501\u20135510.\nXiang Guo, Guanying Chen, Yuchao Dai, Xiaoqing Ye, Jiadai Sun, Xiao Tan, and Errui\nDing. 2022a. Neural Deformable Voxel Grid for Fast Optimization of Dynamic View\nSynthesis. In Proceedings of the Asian Conference on Computer Vision. 3757\u20133775.\nXiang Guo, Guanying Chen, Yuchao Dai, Xiaoqing Ye, Jiadai Sun, Xiao Tan, and Errui\nDing. 2022b. Neural Deformable Voxel Grid for Fast Optimization of Dynamic View\nSynthesis. In Proceedings of the Asian Conference on Computer Vision (ACCV).\nMarc Habermann, Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and\nChristian Theobalt. 2021. Real-time deep dynamic characters. ACM Transactions on\nGraphics (TOG) 40, 4 (2021), 1\u201316.\nCatalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. 2013. Human3.\n6m: Large scale datasets and predictive methods for 3d human sensing in natural\nenvironments. IEEE transactions on pattern analysis and machine intelligence 36, 7\n(2013), 1325\u20131339.\nTakeo Kanade, Peter Rander, and PJ Narayanan. 1997. Virtualized reality: Constructing\nvirtual worlds from real scenes. IEEE multimedia 4, 1 (1997), 34\u201347.\nKiriakos N Kutulakos and Steven M Seitz. 2000. A theory of shape by space carving.\nInternational journal of computer vision 38, 3 (2000), 199\u2013218.\nRuilong Li, Julian Tanke, Minh Vo, Michael Zollh\u00f6fer, J\u00fcrgen Gall, Angjoo Kanazawa,\nand Christoph Lassner. 2022b. Tava: Template-free animatable volumetric actors.\nIn Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n23\u201327, 2022, Proceedings, Part XXXII. Springer, 419\u2013436.\nTianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil\nKim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al.\n2022a. Neural 3D Video Synthesis From Multi-View Video. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 5521\u20135531.\nZhi Li, Anne Aaron, Ioannis Katsavounidis, Anush Moorthy, and Megha Manohara.\n2016. Toward a practical perceptual video quality metric. The Netflix Tech Blog 6, 2\n(2016).\nZhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. 2021. Neural scene flow\nfields for space-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 6498\u20136508.\nZhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. 2022c.\nDynIBaR: Neural Dynamic Image-Based Rendering. arXiv preprint arXiv:2211.11082\n(2022).\nShanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip Sengupta. 2022. Robust high-\nresolution video matting with temporal guidance. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision. 238\u2013247.\nJia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang, David Junhao Zhang, Jussi Keppo,\nYing Shan, Xiaohu Qie, and Mike Zheng Shou. 2022. DeVRF: Fast Deformable Voxel\nRadiance Fields for Dynamic Scenes. Advances in Neural Information Processing\nSystems.\nLingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and\nChristian Theobalt. 2021. Neural actor: Neural free-view synthesis of human actors\nwith pose control. ACM Transactions on Graphics (TOG) 40, 6 (2021), 1\u201316.\nStephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann,\nand Yaser Sheikh. 2019. Neural Volumes: Learning Dynamic Renderable Volumes\nfrom Images. ACM Trans. Graph. 38, 4, Article 65 (July 2019), 14 pages.\nStephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh,\nand Jason Saragih. 2021. Mixture of volumetric primitives for efficient neural\nrendering. ACM Transactions on Graphics (TOG) 40, 4 (2021), 1\u201313.\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J.\nBlack. 2015. SMPL: a skinned multi-person linear model. ACM Trans. Graph. 34\n(2015), 248:1\u2013248:16.\nHaimin Luo, Teng Xu, Yuheng Jiang, Chenglin Zhou, Qiwei Qiu, Yingliang Zhang,\nWei Yang, Lan Xu, and Jingyi Yu. 2022. Artemis: Articulated Neural Pets with\nAppearance and Motion Synthesis. ACM Trans. Graph. 41, 4, Article 164 (jul 2022),\n19 pages. https://doi.org/10.1145/3528223.3530086\nNelson Max. 1995. Optical models for direct volume rendering. IEEE Transactions on\nVisualization and Computer Graphics 1, 2 (1995), 99\u2013108.\nDushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko,\nWeipeng Xu, and Christian Theobalt. 2017. Monocular 3d human pose estima-\ntion in the wild using improved cnn supervision. In 2017 international conference on\n3D vision (3DV). IEEE, 506\u2013516.\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas\nGeiger. 2019. Occupancy networks: Learning 3d reconstruction in function space.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.\n4460\u20134470.\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields\nfor View Synthesis. In ECCV.\nThomas M\u00fcller. 2021. tiny-cuda-nn. https://github.com/NVlabs/tiny-cuda-nn Accessed:\n2022-10-21.\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant\nNeural Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans.\nGraph. 41, 4, Article 102 (July 2022), 15 pages.\nhttps://doi.org/10.1145/3528223.\n3530127\nAtsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. 2021. Neural articulated\nradiance field. In Proceedings of the IEEE/CVF International Conference on Computer\nVision. 5762\u20135772.\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love-\ngrove. 2019. Deepsdf: Learning continuous signed distance functions for shape\nrepresentation. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition. 165\u2013174.\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman,\nSteven M Seitz, and Ricardo Martin-Brualla. 2021a. Nerfies: Deformable neural\nradiance fields. In Proceedings of the IEEE/CVF International Conference on Computer\nVision. 5865\u20135874.\nKeunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz,\nDan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. 2021b. Hypernerf: A\nhigher-dimensional representation for topologically varying neural radiance fields.\nACM Trans. Graph. 40, 6 (dec 2021).\nSida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and\nXiaowei Zhou. 2021. Neural body: Implicit neural representations with structured\nlatent codes for novel view synthesis of dynamic humans. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 9054\u20139063.\nAlbert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2021.\nD-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 10318\u201310327.\nRuizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin\nLiu. 2022. Tensor4D: Efficient Neural 4D Decomposition for High-fidelity Dynamic\nReconstruction and Rendering. arXiv preprint arXiv:2211.11610 (2022).\nLiangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu,\nand Andreas Geiger. 2022. NeRFPlayer: A Streamable Dynamic Scene Representation\nwith Decomposed Neural Radiance Fields. arXiv preprint arXiv:2210.15947 (2022).\nJonathan Starck and Adrian Hilton. 2007. Surface capture for performance-based\nanimation. IEEE computer graphics and applications 27, 3 (2007), 21\u201331.\nShih-Yang Su, Frank Yu, Michael Zollh\u00f6fer, and Helge Rhodin. 2021. A-nerf: Articulated\nneural radiance fields for learning human shape, appearance, and pose. Advances in\nNeural Information Processing Systems 34 (2021), 12278\u201312291.\nCheng Sun, Min Sun, and Hwann-Tzong Chen. 2022. Direct Voxel Grid Optimization:\nSuper-fast Convergence for Radiance Fields Reconstruction. In CVPR.\nJiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang Zeng. 2022. Compressible-\ncomposable NeRF via Rank-residual Decomposition. arXiv preprint arXiv:2205.14870\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:12\n\u2022\nI\u015f\u0131k et al.\n(2022).\nLiao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang,\nMinye Wu, Jingyi Yu, and Lan Xu. 2022b. Fourier PlenOctrees for Dynamic Radiance\nField Rendering in Real-time. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. 13524\u201313534.\nShaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu Tang. 2022a. Arah: Animatable\nvolume rendering of articulated human sdfs. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXII.\nSpringer, 1\u201319.\nZiyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih,\nJessica Hodgins, and Michael Zollhofer. 2021. Learning Compositional Radiance\nFields of Dynamic Human Heads. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR). 5704\u20135713.\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image quality\nassessment: from error visibility to structural similarity. IEEE transactions on image\nprocessing 13, 4 (2004), 600\u2013612.\nTianhan Xu, Yasuhiro Fujita, and Eiichi Matsumoto. 2022. Surface-Aligned Neural\nRadiance Fields for Controllable 3D Human Synthesis. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 15883\u201315892.\nLior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and\nYaron Lipman. 2020. Multiview Neural Surface Reconstruction by Disentangling\nGeometry and Appearance. Advances in Neural Information Processing Systems 33\n(2020).\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021.\nPlenoctrees for real-time rendering of neural radiance fields. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision. 5752\u20135761.\nJiakai Zhang, Liao Wang, Xinhang Liu, Fuqiang Zhao, Minzhang Li, Haizhao Dai,\nBoyuan Zhang, Wei Yang, Lan Xu, and Jingyi Yu. 2022. NeuVV: Neural Volumetric\nVideos with Immersive Rendering and Editing. arXiv preprint arXiv:2202.06088\n(2022).\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018.\nThe Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In CVPR.\nFuqiang Zhao, Yuheng Jiang, Kaixin Yao, Jiakai Zhang, Liao Wang, Haizhao Dai, Yuhui\nZhong, Yingliang Zhang, Minye Wu, Lan Xu, and Jingyi Yu. 2022. Human Perfor-\nmance Modeling and Rendering via Neural Animated Mesh. In ACM Transactions\non Graphics (TOG), Vol. 41. 1 \u2013 17.\nZerong Zheng, Han Huang, Tao Yu, Hongwen Zhang, Yandong Guo, and Yebin Liu.\n2022. Structured local radiance fields for human avatar modeling. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 15893\u201315903.\nZerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and Yebin Liu. 2019. Deephuman:\n3d human reconstruction from a single image. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision. 7739\u20137749.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nHumanRF: Supplementary Material\nMUSTAFA I\u015eIK, Synthesia, Germany\nMARTIN R\u00dcNZ, Synthesia, Germany\nMARKOS GEORGOPOULOS, Synthesia, United Kingdom\nTARAS KHAKHULIN, Synthesia, United Kingdom\nJONATHAN STARCK, Synthesia, United Kingdom\nLOURDES AGAPITO, University College London, United Kingdom\nMATTHIAS NIESSNER, Technical University of Munich, Germany\nACM Reference Format:\nMustafa I\u015f\u0131k, Martin R\u00fcnz, Markos Georgopoulos, Taras Khakhulin, Jonathan\nStarck, Lourdes Agapito, and Matthias Nie\u00dfner. 2023. HumanRF: Supple-\nmentary Material. ACM Trans. Graph. 42, 4, Article 1 (August 2023), 11 pages.\nhttps://doi.org/10.1145/3592415\n1\nIMPLEMENTATION DETAILS\nHere, we discuss our implementation tricks that enable neural ren-\ndering of terabytes of multi-view data.\n1.1\nHumanRF\nOur method is implemented in PyTorch [Paszke et al. 2019] and in\nCUDA for some of the parts that require performance. We use Tiny\nCUDA neural networks framework [M\u00fcller 2021] to create four 3D\nhash grid representations. To reduce the amount of intermediate\nmemory usage during training and improve performance, we write\na CUDA kernel that samples from four 1D dense grids and compose\nthe results with the sampled features from the hash grid. Further-\nmore, we utilize some of the functionalities from torch-ngp [Tang\n2022] and NerfAcc [Li et al. 2022].\nIn our high resolution video results on the supplemental video,\nwe make use of per-camera embeddings [Martin-Brualla et al. 2021]\nwhich are concatenated to the input of the radiance MLP. This helps\nremoving the brightness and lighting inconsistencies that arise for\nsome cameras. In addition, we filter the light bloom effect based\non the light source annotations shown in Fig. 1. That is, we do not\nsample rays from the annotated circular regions to prevent using\npixels that have light diffused into it. We note that this modified\nversion is not used in any of the comparisons made in the main\npaper or supplementary material for fairness. It is simply used for\nthe stand-alone 12MP video results.\nAuthors\u2019 addresses: Mustafa I\u015f\u0131k, mustafa.isik@synthesia.io, Synthesia, Munich, Ger-\nmany; Martin R\u00fcnz, martin@synthesia.io, Synthesia, Munich, Germany; Markos Geor-\ngopoulos, markos@synthesia.io, Synthesia, London, United Kingdom; Taras Khakhulin,\ntaras.khakhulin@synthesia.io, Synthesia, London, United Kingdom; Jonathan Starck,\njon@synthesia.io, Synthesia, London, United Kingdom; Lourdes Agapito, l.agapito@\ncs.ucl.ac.uk, University College London, London, United Kingdom; Matthias Nie\u00dfner,\nniessner@tum.de, Technical University of Munich, Munich, Germany.\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nThis is the author\u2019s version of the work. It is posted here for your personal use. Not for\nredistribution. The definitive Version of Record was published in ACM Transactions on\nGraphics, https://doi.org/10.1145/3592415.\nFig. 1. Light annotations. Light bloom can have a significant impact on\nphotometric consistency. Light annotations are used to avoid using these\nregions during training.\n1.2\nActorsHQ Data Loader\nConsidering the terabytes of data we need to deal with, storing train-\ning data in memory, or preparing batches in a pre-computation stage\nand saving it to the file system is impractical in terms of memory or\nhard disk requirements. The idea behind our data loader is to bypass\nthe reading and writing large chunks of data by sampling batch of\nrays on the fly from as many images as possible across different\ncameras and time frames. To do this, we define a pool of images, and\nrandomly sample from this pool continuously in the main thread\nwhile another thread is working in the background to replace the\nimages in the pool. By replacing and sampling concurrently, we use\nonly a modest amount of GPU and CPU memory to accommodate\nthe pool. Also, we implement custom CUDA kernels to make use of\nthe occupancy grids (that are initialized from masks) to skip empty\nspace during ray sampling. This speeds up the rendering signifi-\ncantly, and increases the effective capacity of the model because the\nempty space does not have to be modeled.\n1.3\nTraining\nWe use ADAM optimizer [Kingma and Ba 2014] with the initial\nlearning rate of 10\u22122. We decay this learning rate to 5 \u00b7 10\u22123 until\nthe end of each training. We utilize FP16 operations for fast training\nand inference. For experiments with 4\u00d7 downscaled input, we train\nfor \ud835\udc41 \u00d7 1000 iterations where \ud835\udc41 depicts the number of frames in\nthe training sequence. On average, our implementation performs\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\narXiv:2305.06356v2  [cs.CV]  11 May 2023\n1:2\n\u2022\nI\u015f\u0131k et al.\n8 to 12 training iterations per second on a single NVIDIA GeForce\nRTX 3090 with 24GB memory. This corresponds to roughly a day\nof compute for 1000-frame sequences. On the other hand, we train\nfor \ud835\udc41 \u00d7 2500 iterations for full-resolution trainings.\nWe define the training batch size in terms of maximum number\nof samples over all the training rays in a batch. We start by sampling\n8192 rays per batch, and dynamically adjust number of rays such\nthat the maximum number of samples is reached every iteration.\nThis lets us achieve high GPU utilization during training. We set\nthe maximum number of samples to 640K, 576K and 512K for 4\u00d7\ndownscaled input, 2\u00d7 downscaled input and full-resolution input,\nrespectively.\nAs our method partitions a given sequence into segments, it is\npossible to scale the training to thousands of frames. This is because\nwe sample rays across fixed number of time frames, which we set to\n8 for all our experiments. Therefore, in the worst case scenario, only\n8 different segments need to live in the GPU memory. On average,\nour segments have a size of 12 which would translate to around 64\nmillion parameters (256MB) that need to reside in the GPU memory\nat a time instance on average.\n2\nEVALUATION\nIn this section, we clarify the details of camera and frame config-\nurations used during training, validation and testing. In addition,\nwe explain how the metrics are calculated to generate numerical\nresults.\n2.1\nEvaluation Protocol\nIn the following, we describe the evaluation protocol used for our\nbaseline comparison experiments. The dataset was split into 4 dis-\njunct sets of cameras that are listed by their 1-based index:\n\u2022 124 training cameras: 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 15, 16, 17,\n18, 19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 32, 33, 35, 36, 37, 38, 39,\n40, 41, 42, 43, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 59, 60, 61,\n62, 63, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83,\n86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103,\n106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 119, 120,\n121, 122, 123, 124, 125, 126, 128, 131, 132, 133, 134, 135, 136,\n139, 140, 141, 142, 143, 144, 149, 150, 151, 152, 157, 158, 159,\n160\n\u2022 10 validation cameras: 11, 20, 34, 45, 51, 74, 84, 91, 105, 118\n\u2022 13 per-frame test cameras: 1, 14, 25, 31, 44, 58, 64, 65, 71, 85,\n98, 104, 115\n\u2022 1 VMAF test camera: 127\nWhen computing per-frame scores, we alternate the test cameras in\nthe following order: 1, 64, 98, 31, 14, 71, 115, 25, 85, 44, 65, 104, 58 and\ntemporally subsample every fifth frame leading to frame-camera\npairs such as {(1, 1), (6, 64), (11, 98), ...}. To reduce the computa-\ntional burden to execute this comparison, we use one of the se-\nquences per actor alternatingly, i.e. Actor1 Sequence1, Actor2 Se-\nquence2, Actor3 Sequence1, Actor4 Sequence2, Actor5 Sequence1,\nActor6 Sequence2, Actor7 Sequence1 and Actor8 Sequence2. Se-\nquence1s contain moderate movements while Sequence2s contain\nstronger motion. PSNR scores are computed only on the foreground\nTable 1. Frame resolution vs representation quality. Training Hu-\nmanRF at different input resolutions while rendering results at full res-\nolution shows that additional details can be represented, and our method\ncan make use of the extra information provided with the higher resolutions.\nResolution\nPSNR \u2191\nLPIPS \u2193\nSSIM \u2191\nVMAF \u2191\n12MP\n28.07\n0.348\n0.812\n68.76\n12MP/(2 \u00d7 2)\n27.69\n0.360\n0.809\n65.37\n12MP/(4 \u00d7 4)\n27.29\n0.375\n0.799\n59.31\ndepicted by the ground truth masks while SSIM and LPIPS are com-\nputed by tightly cropping images to fit ground truth foreground\nmasks. Finally, VMAF is computed on the video that is compiled by\nrendering every third frame from the hero camera (camera 127).\n2.2\nNumerical Results of Input Resolution Experiment\nIn Table 1, we provide additional results concerning the input reso-\nlution experiment we present in the main paper.\n2.3\nAdditional Numerical Results on Baseline Comparison\nWe provide additional results over different motion complexities\nin Table 2 and Table 3, and per sequence results. Moreover, we\nprovide a plot in Fig. 3 to better illustrate the effect of increasing\nthe sequence length.\n2.4\nVisual Results on DFA Dataset\nIn Fig. 2, we present results of our method for four scenes we choose\nfrom DFA. We infer that HumanRF can produce high-fidelity results\nfor non-human subjects as well.\n3\nHEX4D AND TNGP FORMULATIONS\nFollowing the notations we have used to define Equation 1 in the\nmain paper, we define Hex4D formulation as follows:\n\ud835\udc47\ud835\udc65\ud835\udc66\ud835\udc67\ud835\udc61 (p\ud835\udc65\ud835\udc66\ud835\udc67\ud835\udc61) = \ud835\udc47\ud835\udc65\ud835\udc66(p\ud835\udc65\ud835\udc66) \u2299 \ud835\udc47\ud835\udc67\ud835\udc61 (p\ud835\udc67\ud835\udc61)\n+\ud835\udc47\ud835\udc66\ud835\udc67(p\ud835\udc66\ud835\udc67) \u2299 \ud835\udc47\ud835\udc65\ud835\udc61 (p\ud835\udc65\ud835\udc61)\n+\ud835\udc47\ud835\udc65\ud835\udc67(p\ud835\udc65\ud835\udc67) \u2299 \ud835\udc47\ud835\udc66\ud835\udc61 (p\ud835\udc66\ud835\udc61)\n,\n(1)\nwhere we represent six 2D planes (\ud835\udc47\ud835\udc65\ud835\udc66,\ud835\udc47\ud835\udc66\ud835\udc67,\ud835\udc47\ud835\udc65\ud835\udc67,\ud835\udc47\ud835\udc67\ud835\udc61,\ud835\udc47\ud835\udc65\ud835\udc66,\ud835\udc47\ud835\udc66\ud835\udc61 : R2 \u21a6\u2192\nR\ud835\udc5a) using multi-resolution dense grids. Notice that HexPlanes [Cao\nand Johnson 2023] uses concatenation operation instead of addition\nas opposed to our formulation. However, for our experiments, we\ndid not observe an improvement when using concatenation over\naddition.\nOn the other hand, tNGP simply uses a 4D hash grid to represent\n\ud835\udc47\ud835\udc65\ud835\udc66\ud835\udc67\ud835\udc61 : R4 \u21a6\u2192 R\ud835\udc5a without utilizing any kind of decomposition\ntechniques.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nHumanRF: Supplementary Material\n\u2022\n1:3\n(a) ours\n(b) reference\nFig. 2. Visual results from DFA dataset.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:4\n\u2022\nI\u015f\u0131k et al.\nMethod\nMetric\n20\n50\n100\n250\n500\n1000\nOurs\n\u2193 LPIPS\n0.081\n0.090\n0.088\n0.095\n0.100\n0.103\n\u2191 PSNR\n31.58\n31.12\n30.72\n30.10\n30.15\n29.93\n\u2191 SSIM\n0.933\n0.930\n0.932\n0.927\n0.924\n0.920\n\u2191 VMAF\n79.13\n79.49\n81.66\n81.93\n82.05\n83.15\nInstant-NGP\n\u2193 LPIPS\n0.100\n0.096\n0.099\n0.100\n0.100\n0.099\n\u2191 PSNR\n29.88\n29.72\n29.67\n29.65\n29.60\n29.76\n\u2191 SSIM\n0.884\n0.910\n0.910\n0.910\n0.908\n0.911\n\u2191 VMAF\n68.91\n65.83\n70.84\n71.45\n72.11\n73.00\nTiNeuVox\n\u2193 LPIPS\n0.287\n0.252\n0.308\n0.334\n0.337\n0.361\n\u2191 PSNR\n27.60\n26.61\n25.01\n23.86\n23.03\n21.56\n\u2191 SSIM\n0.819\n0.826\n0.809\n0.804\n0.797\n0.779\n\u2191 VMAF\n59.43\n59.58\n47.69\n38.91\n32.23\n22.75\nNDVG\n\u2193 LPIPS\n0.195\n0.212\n0.247\n0.312\n0.337\n0.376\n\u2191 PSNR\n29.61\n26.94\n25.26\n22.20\n20.73\n18.31\n\u2191 SSIM\n0.875\n0.860\n0.834\n0.794\n0.778\n0.739\n\u2191 VMAF\n73.04\n64.10\n51.34\n31.65\n21.29\n9.317\nHyperNeRF\n\u2193 LPIPS\n0.228\n0.222\n0.236\n0.256\n0.294\n0.321\n\u2191 PSNR\n25.74\n25.88\n25.94\n25.07\n23.95\n22.94\n\u2191 SSIM\n0.841\n0.842\n0.835\n0.823\n0.805\n0.794\n\u2191 VMAF\n74.35\n74.72\n69.83\n62.03\n48.92\n39.10\nNeuralBody\n\u2193 LPIPS\n0.272\n0.289\n0.289\n0.303\n0.327\n0.356\n\u2191 PSNR\n27.28\n25.89\n27.87\n25.38\n25.16\n26.66\n\u2191 SSIM\n0.822\n0.814\n0.810\n0.809\n0.798\n0.763\n\u2191 VMAF\n42.95\n42.22\n41.02\n37.28\n32.35\n29.29\nTAVA\n\u2193 LPIPS\n0.218\n0.236\n0.260\n0.313\n0.362\n0.411\n\u2191 PSNR\n28.62\n27.29\n26.50\n25.12\n23.96\n22.86\n\u2191 SSIM\n0.848\n0.841\n0.830\n0.806\n0.782\n0.757\n\u2191 VMAF\n66.45\n62.64\n57.15\n43.75\n30.22\n18.56\nTable 2. moderate movements\nMethod\nMetric\n20\n50\n100\n250\n500\n1000\nOurs\n\u2193 LPIPS\n0.108\n0.111\n0.106\n0.105\n0.104\n0.112\n\u2191 PSNR\n29.02\n28.98\n28.95\n28.41\n28.53\n28.17\n\u2191 SSIM\n0.903\n0.906\n0.910\n0.912\n0.913\n0.906\n\u2191 VMAF\n88.20\n89.37\n89.59\n88.63\n88.61\n88.33\nInstant-NGP\n\u2193 LPIPS\n0.090\n0.087\n0.088\n0.086\n0.086\n0.087\n\u2191 PSNR\n29.02\n27.84\n28.25\n27.89\n27.86\n27.93\n\u2191 SSIM\n0.877\n0.886\n0.894\n0.899\n0.900\n0.899\n\u2191 VMAF\n79.38\n80.64\n82.55\n82.10\n82.44\n82.20\nTiNeuVox\n\u2193 LPIPS\n0.337\n0.357\n0.346\n0.359\n0.359\n0.381\n\u2191 PSNR\n24.55\n21.66\n21.22\n19.78\n18.85\n18.03\n\u2191 SSIM\n0.764\n0.774\n0.779\n0.780\n0.775\n0.765\n\u2191 VMAF\n53.09\n35.03\n32.86\n23.32\n17.24\n13.48\nNDVG\n\u2193 LPIPS\n0.342\n0.339\n0.353\n0.364\n0.396\n0.405\n\u2191 PSNR\n23.60\n20.36\n19.06\n17.30\n15.13\n14.02\n\u2191 SSIM\n0.771\n0.763\n0.753\n0.736\n0.704\n0.693\n\u2191 VMAF\n53.48\n36.96\n24.92\n12.11\n4.236\n3.048\nHyperNeRF\n\u2193 LPIPS\n0.272\n0.248\n0.266\n0.283\n0.310\n0.328\n\u2191 PSNR\n25.65\n24.58\n23.51\n22.58\n21.22\n20.60\n\u2191 SSIM\n0.800\n0.822\n0.817\n0.810\n0.797\n0.787\n\u2191 VMAF\n71.81\n71.38\n64.51\n52.98\n40.77\n34.92\nNeuralBody\n\u2193 LPIPS\n0.339\n0.326\n0.332\n0.333\n0.353\n0.377\n\u2191 PSNR\n26.77\n24.43\n25.97\n23.93\n23.53\n24.50\n\u2191 SSIM\n0.791\n0.800\n0.800\n0.801\n0.789\n0.770\n\u2191 VMAF\n50.49\n48.32\n42.64\n40.15\n31.90\n24.61\nTAVA\n\u2193 LPIPS\n0.322\n0.318\n0.330\n0.376\n0.415\n0.447\n\u2191 PSNR\n26.26\n24.21\n23.60\n22.11\n20.84\n20.13\n\u2191 SSIM\n0.792\n0.801\n0.802\n0.778\n0.749\n0.722\n\u2191 VMAF\n67.40\n54.67\n51.41\n33.06\n16.68\n8.129\nTable 3. strong movements\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nHumanRF: Supplementary Material\n\u2022\n1:5\n0\n200\n400\n600\n800\n1000\nNumber of frames in sequence\n18\n20\n22\n24\n26\n28\n30\n32\nPSNR\nModerate Movements\nOurs\nInstant-NGP\nTiNeuVox\nNDVG\nHyperNeRF\nNeuralBody\nTAVA\n0\n200\n400\n600\n800\n1000\nNumber of frames in sequence\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nPSNR\nStrong Movements\nOurs\nInstant-NGP\nTiNeuVox\nNDVG\nHyperNeRF\nNeuralBody\nTAVA\nFig. 3. Influence of increasing the sequence length. Thanks to using adaptively-placed 4D segments, our method consistently outperforms the deformation-\nbased baselines as they struggle to capture complex motion over long sequences. Although NeuralBody does not lose its representation power for long\nsequences, its overall quality is inferior to HumanRF.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:6\n\u2022\nI\u015f\u0131k et al.\nNDVG\nHyperNeRF\nTiNeuVoX\nOurs\nNDVG\nHyperNeRF\nTiNeuVoX\nOurs\nMethod\n20\n50\n100\n250\n500\nFig. 4. Impact of increasing sequence length. For deformation-based baselines synthesis quality drops when rendering the same pose and frame while\nincreasing the sequence length. Our results on the other hand have constant quality independent of the sequence length.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nHumanRF: Supplementary Material\n\u2022\n1:7\nMethod\nMetric\n20\n50\n100\n250\n500\n1000\nOurs\n\u2193 LPIPS\n0.046\n0.052\n0.055\n0.059\n0.060\n0.064\n\u2191 PSNR\n31.51\n31.45\n30.53\n30.42\n30.23\n30.31\n\u2191 SSIM\n0.951\n0.947\n0.945\n0.941\n0.939\n0.934\n\u2191 VMAF\n83.10\n84.43\n86.90\n85.27\n85.47\n87.41\nInstant-NGP\n\u2193 LPIPS\n0.062\n0.062\n0.069\n0.071\n0.072\n0.073\n\u2191 PSNR\n30.60\n29.87\n29.71\n29.65\n29.56\n29.64\n\u2191 SSIM\n0.910\n0.927\n0.924\n0.921\n0.920\n0.920\n\u2191 VMAF\n73.27\n69.36\n73.84\n73.95\n75.41\n76.38\nTiNeuVox\n\u2193 LPIPS\n0.241\n0.191\n0.226\n0.290\n0.310\n0.338\n\u2191 PSNR\n27.78\n26.55\n25.36\n23.61\n22.36\n20.50\n\u2191 SSIM\n0.843\n0.847\n0.829\n0.820\n0.813\n0.778\n\u2191 VMAF\n65.61\n64.23\n59.43\n43.08\n31.88\n21.92\nNDVG\n\u2193 LPIPS\n0.121\n0.168\n0.213\n0.276\n0.313\n0.361\n\u2191 PSNR\n30.51\n26.11\n24.22\n21.59\n19.45\n17.15\n\u2191 SSIM\n0.904\n0.867\n0.841\n0.803\n0.786\n0.736\n\u2191 VMAF\n79.21\n61.79\n52.65\n34.74\n19.83\n6.256\nHyperNeRF\n\u2193 LPIPS\n0.186\n0.178\n0.206\n0.222\n0.269\n0.301\n\u2191 PSNR\n25.05\n24.94\n26.05\n25.08\n23.17\n22.56\n\u2191 SSIM\n0.868\n0.864\n0.854\n0.844\n0.820\n0.806\n\u2191 VMAF\n79.00\n78.89\n75.35\n68.98\n49.06\n41.72\nNeuralBody\n\u2193 LPIPS\n0.184\n0.228\n0.240\n0.243\n0.270\n0.305\n\u2191 PSNR\n28.87\n26.23\n27.63\n25.79\n25.38\n26.74\n\u2191 SSIM\n0.864\n0.844\n0.837\n0.837\n0.827\n0.786\n\u2191 VMAF\n48.10\n46.36\n41.70\n46.55\n43.24\n35.05\nTAVA\n\u2193 LPIPS\n0.149\n0.178\n0.210\n0.259\n0.308\n0.373\n\u2191 PSNR\n29.30\n27.62\n26.80\n25.22\n23.81\n22.63\n\u2191 SSIM\n0.879\n0.868\n0.852\n0.827\n0.803\n0.770\n\u2191 VMAF\n72.10\n67.40\n63.20\n50.00\n32.49\n20.46\nTable 4. Actor 1, Sequence 1\nMethod\nMetric\n20\n50\n100\n250\n500\n1000\nOurs\n\u2193 LPIPS\n0.081\n0.086\n0.080\n0.079\n0.077\n0.079\n\u2191 PSNR\n30.67\n30.21\n30.54\n29.63\n29.81\n29.54\n\u2191 SSIM\n0.946\n0.943\n0.948\n0.946\n0.947\n0.946\n\u2191 VMAF\n85.23\n84.66\n87.98\n85.73\n85.58\n87.73\nInstant-NGP\n\u2193 LPIPS\n0.084\n0.083\n0.076\n0.081\n0.080\n0.081\n\u2191 PSNR\n29.60\n26.99\n27.99\n27.75\n27.76\n27.77\n\u2191 SSIM\n0.914\n0.897\n0.914\n0.912\n0.915\n0.914\n\u2191 VMAF\n76.35\n81.46\n81.37\n81.31\n81.65\n81.57\nTiNeuVox\n\u2193 LPIPS\n0.306\n0.337\n0.277\n0.321\n0.314\n0.337\n\u2191 PSNR\n25.09\n21.33\n22.48\n21.05\n20.05\n19.57\n\u2191 SSIM\n0.834\n0.821\n0.832\n0.826\n0.814\n0.817\n\u2191 VMAF\n46.73\n21.32\n33.18\n20.47\n12.83\n11.13\nNDVG\n\u2193 LPIPS\n0.299\n0.286\n0.300\n0.333\n0.363\n0.372\n\u2191 PSNR\n24.66\n21.80\n20.58\n18.82\n16.50\n15.41\n\u2191 SSIM\n0.841\n0.827\n0.823\n0.791\n0.762\n0.753\n\u2191 VMAF\n43.17\n34.33\n23.83\n7.743\n0.482\n0.106\nHyperNeRF\n\u2193 LPIPS\n0.223\n0.194\n0.223\n0.244\n0.273\n0.291\n\u2191 PSNR\n25.79\n25.55\n24.28\n23.62\n22.50\n21.74\n\u2191 SSIM\n0.849\n0.876\n0.865\n0.851\n0.842\n0.833\n\u2191 VMAF\n54.65\n68.94\n56.97\n49.56\n40.25\n31.78\nNeuralBody\n\u2193 LPIPS\n0.260\n0.257\n0.255\n0.268\n0.283\n0.295\n\u2191 PSNR\n27.55\n25.48\n26.89\n24.94\n24.58\n25.76\n\u2191 SSIM\n0.862\n0.858\n0.859\n0.853\n0.846\n0.838\n\u2191 VMAF\n46.52\n46.26\n39.96\n43.91\n37.46\n31.37\nTAVA\n\u2193 LPIPS\n0.278\n0.272\n0.283\n0.331\n0.362\n0.391\n\u2191 PSNR\n26.79\n25.03\n24.40\n23.34\n22.37\n21.72\n\u2191 SSIM\n0.860\n0.857\n0.855\n0.830\n0.814\n0.794\n\u2191 VMAF\n61.51\n53.90\n48.98\n35.23\n23.89\n14.16\nTable 5. Actor 2, Sequence 2\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:8\n\u2022\nI\u015f\u0131k et al.\nMethod\nMetric\n20\n50\n100\n250\n500\n1000\nOurs\n\u2193 LPIPS\n0.120\n0.138\n0.135\n0.151\n0.155\n0.160\n\u2191 PSNR\n31.02\n30.26\n30.25\n28.98\n29.50\n29.19\n\u2191 SSIM\n0.893\n0.888\n0.896\n0.888\n0.885\n0.881\n\u2191 VMAF\n72.05\n71.11\n75.40\n78.37\n76.96\n78.22\nInstant-NGP\n\u2193 LPIPS\n0.119\n0.125\n0.126\n0.128\n0.129\n0.128\n\u2191 PSNR\n29.44\n29.22\n29.28\n29.23\n29.10\n29.32\n\u2191 SSIM\n0.858\n0.881\n0.883\n0.881\n0.880\n0.883\n\u2191 VMAF\n62.37\n60.88\n67.69\n68.24\n68.54\n70.39\nTiNeuVox\n\u2193 LPIPS\n0.352\n0.298\n0.406\n0.430\n0.436\n0.452\n\u2191 PSNR\n27.51\n26.62\n24.13\n22.98\n22.30\n21.28\n\u2191 SSIM\n0.782\n0.791\n0.760\n0.752\n0.751\n0.747\n\u2191 VMAF\n49.37\n51.83\n29.76\n24.86\n19.28\n12.17\nNDVG\n\u2193 LPIPS\n0.240\n0.281\n0.354\n0.435\n0.453\n0.481\n\u2191 PSNR\n28.76\n25.83\n23.13\n21.17\n20.05\n17.83\n\u2191 SSIM\n0.841\n0.812\n0.763\n0.731\n0.724\n0.692\n\u2191 VMAF\n61.99\n50.27\n28.79\n17.01\n7.948\n2.447\nHyperNeRF\n\u2193 LPIPS\n0.233\n0.250\n0.275\n0.322\n0.374\n0.388\n\u2191 PSNR\n25.75\n26.53\n25.96\n24.85\n23.29\n23.04\n\u2191 SSIM\n0.827\n0.818\n0.800\n0.777\n0.758\n0.761\n\u2191 VMAF\n71.59\n69.29\n58.71\n49.67\n33.25\n33.88\nNeuralBody\n\u2193 LPIPS\n0.288\n0.333\n0.354\n0.368\n0.396\n0.429\n\u2191 PSNR\n27.51\n25.88\n27.18\n25.30\n24.81\n25.68\n\u2191 SSIM\n0.804\n0.777\n0.739\n0.762\n0.745\n0.668\n\u2191 VMAF\n42.89\n42.13\n34.00\n33.25\n26.65\n21.11\nTAVA\n\u2193 LPIPS\n0.261\n0.303\n0.341\n0.410\n0.467\n0.504\n\u2191 PSNR\n28.47\n26.93\n25.83\n24.28\n23.13\n22.21\n\u2191 SSIM\n0.820\n0.801\n0.782\n0.749\n0.721\n0.704\n\u2191 VMAF\n60.16\n55.27\n46.70\n29.98\n15.05\n6.436\nTable 6. Actor 3, Sequence 1\nMethod\nMetric\n20\n50\n100\n250\n500\n1000\nOurs\n\u2193 LPIPS\n0.110\n0.118\n0.114\n0.115\n0.114\n0.138\n\u2191 PSNR\n27.52\n27.60\n27.55\n27.28\n27.04\n26.42\n\u2191 SSIM\n0.851\n0.855\n0.860\n0.866\n0.870\n0.842\n\u2191 VMAF\n90.93\n91.46\n90.47\n90.80\n90.27\n89.87\nInstant-NGP\n\u2193 LPIPS\n0.094\n0.087\n0.083\n0.082\n0.081\n0.084\n\u2191 PSNR\n26.70\n25.66\n26.68\n26.48\n26.53\n26.53\n\u2191 SSIM\n0.787\n0.807\n0.839\n0.855\n0.862\n0.856\n\u2191 VMAF\n83.50\n87.28\n87.09\n87.17\n87.39\n87.59\nTiNeuVox\n\u2193 LPIPS\n0.458\n0.407\n0.404\n0.386\n0.388\n0.432\n\u2191 PSNR\n22.62\n20.78\n19.39\n18.62\n17.43\n15.81\n\u2191 SSIM\n0.610\n0.698\n0.707\n0.722\n0.722\n0.692\n\u2191 VMAF\n51.25\n39.78\n30.67\n29.42\n25.38\n15.91\nNDVG\n\u2193 LPIPS\n0.483\n0.421\n0.414\n0.428\n0.443\n0.454\n\u2191 PSNR\n20.95\n18.16\n16.99\n15.27\n13.03\n12.54\n\u2191 SSIM\n0.608\n0.662\n0.669\n0.662\n0.632\n0.625\n\u2191 VMAF\n56.20\n34.25\n24.36\n16.40\n10.35\n8.543\nHyperNeRF\n\u2193 LPIPS\n0.394\n0.297\n0.309\n0.322\n0.335\n0.366\n\u2191 PSNR\n23.50\n23.44\n22.13\n21.16\n20.17\n19.53\n\u2191 SSIM\n0.651\n0.739\n0.742\n0.747\n0.745\n0.727\n\u2191 VMAF\n78.24\n71.99\n66.48\n55.13\n42.85\n40.66\nNeuralBody\n\u2193 LPIPS\n0.454\n0.377\n0.372\n0.367\n0.392\n0.423\n\u2191 PSNR\n25.02\n23.10\n24.20\n22.82\n22.62\n22.05\n\u2191 SSIM\n0.639\n0.722\n0.729\n0.740\n0.723\n0.703\n\u2191 VMAF\n54.05\n50.99\n41.15\n43.07\n27.27\n24.65\nTAVA\n\u2193 LPIPS\n0.431\n0.359\n0.375\n0.416\n0.452\n0.500\n\u2191 PSNR\n24.97\n23.22\n22.36\n21.01\n19.84\n18.87\n\u2191 SSIM\n0.644\n0.724\n0.727\n0.712\n0.683\n0.640\n\u2191 VMAF\n72.20\n56.97\n48.24\n29.85\n12.28\n3.305\nTable 7. Actor 4, Sequence 2\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nHumanRF: Supplementary Material\n\u2022\n1:9\nMethod\nMetric\n20\n50\n100\n250\n500\n1000\nOurs\n\u2193 LPIPS\n0.067\n0.073\n0.073\n0.075\n0.082\n0.083\n\u2191 PSNR\n32.01\n31.22\n31.04\n30.47\n30.18\n30.29\n\u2191 SSIM\n0.950\n0.946\n0.946\n0.944\n0.939\n0.937\n\u2191 VMAF\n78.99\n77.68\n79.22\n78.70\n79.40\n81.50\nInstant-NGP\n\u2193 LPIPS\n0.098\n0.090\n0.095\n0.091\n0.092\n0.090\n\u2191 PSNR\n29.47\n29.40\n29.55\n29.58\n29.62\n29.79\n\u2191 SSIM\n0.903\n0.924\n0.919\n0.923\n0.921\n0.923\n\u2191 VMAF\n66.84\n63.65\n67.78\n69.37\n70.21\n71.07\nTiNeuVox\n\u2193 LPIPS\n0.197\n0.272\n0.284\n0.293\n0.312\n0.296\n\u2191 PSNR\n28.20\n26.43\n25.38\n24.35\n23.38\n22.11\n\u2191 SSIM\n0.852\n0.837\n0.828\n0.826\n0.816\n0.791\n\u2191 VMAF\n64.80\n57.06\n51.81\n45.29\n37.90\n29.19\nNDVG\n\u2193 LPIPS\n0.176\n0.175\n0.187\n0.254\n0.278\n0.329\n\u2191 PSNR\n29.80\n27.89\n27.41\n23.50\n22.03\n19.14\n\u2191 SSIM\n0.894\n0.892\n0.878\n0.829\n0.806\n0.762\n\u2191 VMAF\n72.32\n69.70\n64.46\n39.65\n31.08\n15.88\nHyperNeRF\n\u2193 LPIPS\n0.237\n0.234\n0.223\n0.223\n0.253\n0.278\n\u2191 PSNR\n25.29\n25.14\n25.74\n25.40\n24.87\n23.97\n\u2191 SSIM\n0.839\n0.838\n0.845\n0.846\n0.828\n0.816\n\u2191 VMAF\n69.36\n70.11\n69.02\n66.22\n57.84\n47.33\nNeuralBody\n\u2193 LPIPS\n0.289\n0.283\n0.270\n0.291\n0.316\n0.336\n\u2191 PSNR\n26.91\n26.10\n28.18\n25.27\n24.77\n26.77\n\u2191 SSIM\n0.826\n0.823\n0.836\n0.820\n0.811\n0.802\n\u2191 VMAF\n39.81\n37.83\n40.60\n36.36\n25.49\n29.24\nTAVA\n\u2193 LPIPS\n0.208\n0.213\n0.227\n0.269\n0.324\n0.373\n\u2191 PSNR\n28.13\n27.03\n26.83\n25.76\n24.33\n23.15\n\u2191 SSIM\n0.857\n0.852\n0.849\n0.835\n0.808\n0.782\n\u2191 VMAF\n64.42\n63.31\n59.15\n49.24\n36.51\n24.45\nTable 8. Actor 5, Sequence 1\nMethod\nMetric\n20\n50\n100\n250\n500\n1000\nOurs\n\u2193 LPIPS\n0.133\n0.128\n0.123\n0.118\n0.117\n0.121\n\u2191 PSNR\n27.26\n28.10\n27.80\n27.68\n27.69\n27.33\n\u2191 SSIM\n0.899\n0.908\n0.913\n0.916\n0.915\n0.913\n\u2191 VMAF\n85.85\n89.94\n88.05\n88.09\n88.18\n86.53\nInstant-NGP\n\u2193 LPIPS\n0.079\n0.090\n0.099\n0.093\n0.094\n0.096\n\u2191 PSNR\n28.18\n28.38\n28.13\n27.57\n27.40\n27.53\n\u2191 SSIM\n0.907\n0.914\n0.906\n0.908\n0.905\n0.904\n\u2191 VMAF\n76.56\n76.17\n79.68\n79.01\n79.44\n79.31\nTiNeuVox\n\u2193 LPIPS\n0.258\n0.296\n0.311\n0.351\n0.356\n0.367\n\u2191 PSNR\n24.68\n22.13\n21.62\n19.57\n18.86\n18.44\n\u2191 SSIM\n0.834\n0.799\n0.802\n0.803\n0.799\n0.791\n\u2191 VMAF\n55.69\n44.06\n39.54\n23.80\n14.92\n15.44\nNDVG\n\u2193 LPIPS\n0.244\n0.307\n0.344\n0.343\n0.413\n0.415\n\u2191 PSNR\n23.79\n20.35\n19.17\n17.70\n14.90\n13.71\n\u2191 SSIM\n0.839\n0.795\n0.778\n0.764\n0.707\n0.697\n\u2191 VMAF\n57.39\n38.60\n26.81\n12.92\n2.790\n3.058\nHyperNeRF\n\u2193 LPIPS\n0.197\n0.243\n0.262\n0.276\n0.321\n0.327\n\u2191 PSNR\n25.45\n23.61\n23.01\n22.32\n20.52\n20.54\n\u2191 SSIM\n0.866\n0.843\n0.839\n0.835\n0.811\n0.807\n\u2191 VMAF\n73.33\n73.32\n68.97\n55.20\n39.83\n36.24\nNeuralBody\n\u2193 LPIPS\n0.278\n0.330\n0.342\n0.333\n0.366\n0.403\n\u2191 PSNR\n25.18\n23.35\n25.81\n23.58\n23.20\n25.21\n\u2191 SSIM\n0.847\n0.807\n0.814\n0.820\n0.801\n0.770\n\u2191 VMAF\n51.62\n46.30\n46.14\n38.76\n32.29\n19.65\nTAVA\n\u2193 LPIPS\n0.258\n0.330\n0.325\n0.374\n0.436\n0.460\n\u2191 PSNR\n25.27\n23.35\n23.35\n21.98\n20.19\n19.86\n\u2191 SSIM\n0.849\n0.807\n0.824\n0.797\n0.751\n0.731\n\u2191 VMAF\n65.48\n46.30\n56.80\n36.29\n12.33\n6.512\nTable 9. Actor 6, Sequence 2\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n1:10\n\u2022\nI\u015f\u0131k et al.\nMethod\nMetric\n20\n50\n100\n250\n500\n1000\nOurs\n\u2193 LPIPS\n0.093\n0.096\n0.089\n0.096\n0.102\n0.103\n\u2191 PSNR\n31.76\n31.56\n31.04\n30.55\n30.69\n29.95\n\u2191 SSIM\n0.940\n0.938\n0.943\n0.937\n0.935\n0.929\n\u2191 VMAF\n82.38\n84.74\n85.12\n85.40\n86.37\n85.45\nInstant-NGP\n\u2193 LPIPS\n0.122\n0.108\n0.107\n0.108\n0.109\n0.107\n\u2191 PSNR\n30.03\n30.38\n30.13\n30.14\n30.13\n30.31\n\u2191 SSIM\n0.867\n0.909\n0.913\n0.915\n0.914\n0.916\n\u2191 VMAF\n73.15\n69.42\n74.06\n74.24\n74.28\n74.17\nTiNeuVox\n\u2193 LPIPS\n0.358\n0.246\n0.314\n0.321\n0.289\n0.356\n\u2191 PSNR\n26.89\n26.84\n25.16\n24.48\n24.07\n22.34\n\u2191 SSIM\n0.800\n0.830\n0.819\n0.818\n0.808\n0.799\n\u2191 VMAF\n57.94\n65.20\n49.75\n42.40\n39.87\n27.73\nNDVG\n\u2193 LPIPS\n0.243\n0.222\n0.232\n0.282\n0.304\n0.334\n\u2191 PSNR\n29.37\n27.94\n26.29\n22.52\n21.38\n19.11\n\u2191 SSIM\n0.861\n0.868\n0.854\n0.811\n0.797\n0.766\n\u2191 VMAF\n78.65\n74.62\n59.49\n35.19\n26.32\n12.68\nHyperNeRF\n\u2193 LPIPS\n0.255\n0.228\n0.240\n0.259\n0.280\n0.319\n\u2191 PSNR\n26.88\n26.92\n26.00\n24.94\n24.47\n22.17\n\u2191 SSIM\n0.828\n0.851\n0.841\n0.827\n0.812\n0.792\n\u2191 VMAF\n77.46\n80.60\n76.25\n63.25\n55.51\n33.48\nNeuralBody\n\u2193 LPIPS\n0.328\n0.312\n0.294\n0.309\n0.325\n0.355\n\u2191 PSNR\n25.84\n25.37\n28.50\n25.17\n25.69\n27.44\n\u2191 SSIM\n0.793\n0.811\n0.829\n0.818\n0.808\n0.796\n\u2191 VMAF\n41.02\n42.56\n47.78\n32.95\n34.04\n31.75\nTAVA\n\u2193 LPIPS\n0.253\n0.252\n0.263\n0.314\n0.349\n0.392\n\u2191 PSNR\n28.60\n27.57\n26.55\n25.22\n24.57\n23.46\n\u2191 SSIM\n0.835\n0.843\n0.837\n0.814\n0.796\n0.773\n\u2191 VMAF\n69.11\n64.59\n59.55\n45.76\n36.82\n22.89\nTable 10. Actor 7, Sequence 1\nMethod\nMetric\n20\n50\n100\n250\n500\n1000\nOurs\n\u2193 LPIPS\n0.107\n0.112\n0.108\n0.109\n0.109\n0.108\n\u2191 PSNR\n30.62\n30.02\n29.94\n29.04\n29.59\n29.40\n\u2191 SSIM\n0.917\n0.917\n0.920\n0.920\n0.921\n0.921\n\u2191 VMAF\n90.80\n91.42\n91.87\n89.89\n90.39\n89.18\nInstant-NGP\n\u2193 LPIPS\n0.102\n0.088\n0.093\n0.089\n0.088\n0.087\n\u2191 PSNR\n31.60\n30.31\n30.20\n29.79\n29.74\n29.90\n\u2191 SSIM\n0.900\n0.925\n0.917\n0.920\n0.920\n0.922\n\u2191 VMAF\n81.11\n77.64\n82.05\n80.89\n81.28\n80.35\nTiNeuVox\n\u2193 LPIPS\n0.326\n0.389\n0.392\n0.379\n0.379\n0.387\n\u2191 PSNR\n25.80\n22.43\n21.39\n19.88\n19.06\n18.32\n\u2191 SSIM\n0.780\n0.778\n0.773\n0.770\n0.768\n0.761\n\u2191 VMAF\n58.70\n34.98\n28.05\n19.57\n15.84\n11.42\nNDVG\n\u2193 LPIPS\n0.342\n0.342\n0.354\n0.354\n0.367\n0.380\n\u2191 PSNR\n24.98\n21.14\n19.48\n17.40\n16.10\n14.42\n\u2191 SSIM\n0.797\n0.766\n0.740\n0.725\n0.716\n0.698\n\u2191 VMAF\n57.17\n40.64\n24.69\n11.41\n3.313\n0.487\nHyperNeRF\n\u2193 LPIPS\n0.275\n0.259\n0.271\n0.288\n0.312\n0.328\n\u2191 PSNR\n27.88\n25.73\n24.63\n23.21\n21.67\n20.57\n\u2191 SSIM\n0.833\n0.831\n0.821\n0.807\n0.790\n0.782\n\u2191 VMAF\n81.03\n71.27\n65.61\n52.05\n40.14\n30.99\nNeuralBody\n\u2193 LPIPS\n0.363\n0.342\n0.358\n0.364\n0.372\n0.388\n\u2191 PSNR\n29.31\n25.81\n26.99\n24.38\n23.71\n24.99\n\u2191 SSIM\n0.817\n0.813\n0.801\n0.792\n0.787\n0.771\n\u2191 VMAF\n49.76\n49.72\n43.29\n34.87\n30.58\n22.76\nTAVA\n\u2193 LPIPS\n0.321\n0.311\n0.338\n0.383\n0.409\n0.438\n\u2191 PSNR\n27.99\n25.23\n24.30\n22.10\n20.95\n20.07\n\u2191 SSIM\n0.815\n0.816\n0.803\n0.771\n0.747\n0.722\n\u2191 VMAF\n70.40\n61.50\n51.62\n30.88\n18.23\n8.532\nTable 11. Actor 8, Sequence 2\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\nHumanRF: Supplementary Material\n\u2022\n1:11\nREFERENCES\nAng Cao and Justin Johnson. 2023. HexPlane: A Fast Representation for Dynamic\nScenes. CVPR (2023).\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980 (2014).\nRuilong Li, Matthew Tancik, and Angjoo Kanazawa. 2022. NerfAcc: A General NeRF\nAccleration Toolbox. arXiv preprint arXiv:2210.04847 (2022).\nRicardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey\nDosovitskiy, and Daniel Duckworth. 2021. NeRF in the Wild: Neural Radiance Fields\nfor Unconstrained Photo Collections. In CVPR.\nThomas M\u00fcller. 2021. tiny-cuda-nn. https://github.com/NVlabs/tiny-cuda-nn Accessed:\n2022-10-21.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\nPytorch: An imperative style, high-performance deep learning library. Advances in\nneural information processing systems 32 (2019).\nJiaxiang Tang. 2022.\nTorch-ngp: a PyTorch implementation of instant-ngp.\nhttps://github.com/ashawkey/torch-ngp.\nACM Trans. Graph., Vol. 42, No. 4, Article 1. Publication date: August 2023.\n"
  },
  {
    "title": "Reconstructing Animatable Categories from Videos",
    "link": "https://arxiv.org/pdf/2305.06351.pdf",
    "upvote": "1",
    "text": "Reconstructing Animatable Categories from Videos\nGengshan Yang Chaoyang Wang N Dinesh Reddy Deva Ramanan\nCarnegie Mellon University\nInternet Videos of a Category\nCanonical Space\nArticulations & Deformations\nDifferentiable \nRendering\nt=0s\nt=2s\nsphynx cat\ncheetah\nSkeleton\nColor: Skinning weights\nMorphology\nMotion Transfer\nFigure 1. Given videos of a deformable category and a skeleton, we reconstruct an animatable 3D model that factorizes variations across\ninstances (e.g., cheetah\u2019s and sphynx\u2019s are both cats but with different shape morphology, skeleton dimensions, and texture) from\ntime-specific variations within an instance (e.g., skeleton articulations and elastic shape deformation). Left: Input videos; Middle-left: 3D\nshape, skeleton, and skinning weights (visualized as surface colors) in the canonical space; Middle-right: Disentangled between-instance\nand within-instance variations over time. Right: Morphology and motion transferred across the two instances.\nAbstract\nBuilding animatable 3D models is challenging due to the\nneed for 3D scans, laborious registration, and manual rig-\nging, which are difficult to scale to arbitrary categories. Re-\ncently, differentiable rendering provides a pathway to ob-\ntain high-quality 3D models from monocular videos, but\nthese are limited to rigid categories or single instances. We\npresent RAC that builds category 3D models from monoc-\nular videos while disentangling variations over instances\nand motion over time.\nThree key ideas are introduced\nto solve this problem: (1) specializing a skeleton to in-\nstances via optimization, (2) a method for latent space reg-\nularization that encourages shared structure across a cate-\ngory while maintaining instance details, and (3) using 3D\nbackground models to disentangle objects from the back-\nground. We show that 3D models of humans, cats and dogs\ncan be learned from 50-100 internet videos. Project page:\nhttps://gengshan-y.github.io/rac-www/.\n1. Introduction\nWe aim to build animatable 3D models for deformable\nobject categories. Prior work has done so for targeted cat-\negories such as people (e.g., SMPL [2, 32]) and quadruped\nanimals (e.g., SMAL [5]), but such methods appear chal-\nlenging to scale due to the need of 3D supervision and reg-\nistration. Recently, test-time optimization through differ-\nentiable rendering [42, 43, 46, 61, 75] provides a pathway\nto generate high-quality 3D models of deformable objects\nand scenes from monocular videos. However, such models\nare typically built independently for each object instance or\nscene. In contrast, we would like to build category models\nthat can generate different instances along with deforma-\ntions, given causally-captured video collections.\nThough scalable, such data is challenging to leverage\nin practice.\nOne challenge is how to learn the morpho-\nlogical variation of instances within a category. For ex-\nample, huskys and corgis are both dogs, but have dif-\nferent body shapes, skeleton dimensions, and texture ap-\npearance. Such variations are difficult to disentangle from\nthe variations within a single instance, e.g., as a dog artic-\nulates, stretches its muscles, and even moves into differ-\nent illumination conditions. Approaches for disentangling\nsuch factors require enormous efforts in capture and reg-\nistration [2, 6], and doing so without explicit supervision\nremains an open challenge.\nAnother challenge arises from the impoverished nature\nof in-the-wild videos: objects are often partially observ-\nable at a limited number of viewpoints, and input signals\nsuch as segmentation masks can be inaccurate for such \u201cin-\nthe-wild\u201d data. When dealing with partial or impoverished\nvideo inputs, one would want the model to listen to the com-\nmon structures learned across a category \u2013 e.g., dogs have\n1\narXiv:2305.06351v1  [cs.CV]  10 May 2023\nMorphology \u03b2\nArticulation \u03b8\nFigure 2. Disentangling morphologies \u03b2 and articulation \u03b8. We show different morphologies (body shape and clothing) given the same\nrest pose (top) and bouncing pose (bottom).\ntwo ears. On the other hand, one would want the model to\nstay faithful to the input views.\nOur approach addresses these challenges by exploiting\nthree insights: (1) We learn skeletons with constant bone\nlengths within a video, allowing for better disentanglement\nof between-instance morphology and within-instance artic-\nulation. (2) We regularize the unobserved body parts to be\ncoherent across instances while remaining faithful to the in-\nput views with a novel code-swapping technique. (3) We\nmake use of a category-level background model that, while\nnot 3D accurate, produces far better segmentation masks.\nWe learn animatable 3D models of cats, dogs, and humans\nwhich outperform prior art.\nBecause our models regis-\nter different instances with a canonical skeleton, we also\ndemonstrate motion transfer across instances.\n2. Related Works\nModel-based 3D Reconstruction. A large body of work\nin 3D human and animal reconstruction uses parametric\nshape models [32, 44, 60, 66, 80, 81], which are built from\nregistered 3D scans of human or animals, and serve to\nrecover 3D shapes given a single image or video at test\ntime [3, 4, 4, 23, 23, 49, 79]. A recent research focus is to\ncombine statistical human body mode with implicit func-\ntions [19, 27, 50\u201352, 69, 78] to improve the robustness and\nfidelity of the reconstruction. Although parametric body\nmodels achieve great success in reconstructing humans with\nlarge amounts of ground-truth 3D data, it is unclear how to\napply the same methodology to categories with limited 3D\ndata, such as animals, and how to scale to real-life imagery\nwith diverse clothing and body poses. RAC builds category-\nlevel shape models from in-the-wild videos and demon-\nstrates the potential to reconstruct 3D categories without so-\nphisticated manual processing.\nCategory Reconstruction from Image Collections.\nA\nnumber of recent methods build deformable 3D models\nof object categories from images with weak 2D annota-\ntions, such as keypoints and object silhouettes, obtained\nfrom human annotators or predicted by off-the-shelf mod-\nels [15, 20, 24, 30, 58, 64, 76]. However, those methods do\nnot distinguish between morphological variations and mo-\ntion over time. Moreover, they often apply heavy regular-\nization on shape and deformation to avoid degenerate so-\nlutions, which also smooths out fine-grained details. Re-\ncent research combines neural implicit functions [35, 36]\nwith category modeling in the context of 3D data genera-\ntion [7, 8, 39], where shape and appearance variations over\na category are modeled with conditional NeRFs. However,\nreconstructions are typically focused on near-rigid objects\nsuch as faces and vehicles.\nArticulated Object Reconstruction from Videos. Com-\npared to image collections, videos provide signals to re-\nconstruct object motions and disentangle them from mor-\nphological variations. Some works [28, 41, 55] reconstruct\narticulated bodies from videos, but they either assume syn-\nchronized multi-view recordings or articulated 3D skeleton\ninputs that make their approaches less general. Some other\nworks [73\u201375] learn animatable 3D models from monocu-\nlar videos capturing the same object instance, without dis-\nentangling morphology and motion. There are recent meth-\nods [29, 63, 74] using in-the-wild videos to reconstruct 3D\nmodels animals, but their quality are relatively low.\n3. Method\nGiven video recordings of different instances from a cat-\negory and a pre-defined skeleton, we build animatable 3D\nmodels including instance-specific morphology (Sec. 3.1),\ntime-varying articulation and deformation (Sec. 3.2), as\nwell as a video-specific 3D background model (Sec. 3.3).\nThe models are optimized using differentiable rendering\n(Sec. 3.4). An overview is shown in Fig. 3.\n3.1. Between-Instance Variation\nFusing videos of different instances into a category\nmodel requires handling the morphological variations,\nwhich includes the changes in both internal skeleton and\noutward appearance (shape and color). We define a video-\nspecific morphology code \u03b2 to control the variations of both\n2\nSkinning & \nDeformation\nBone \nStretching\n(b) T\u03b2, W\u03b2\n(c) Ts\n\u03b2\n(d) D(Ts\n\u03b2, \u03b8, \u03c9d)\n(a) T, W, J\nMorphology \nSlicing\n(d) D(\u03b2, \u03b8, \u03c9d) = DQS(W\u03b2, G\u03b8, Ts\n\u03b2)\n(c) T\u03b2\ns = DQS(W\u03b2, \u0394J\u03b2, T\u03b2)\nFigure 3. Morphological variations vs time-varying articulation and deformation. (a) Canonical shape T, skinning weights W, and\njoint locations J. (b) To represent morphological differences between instances, we use a morphology code \u03b2 that specifies instance shape\nand appearance T\u03b2, skinning weights W\u03b2 for a canonical skeleton J. (c) \u03b2 also predicts a change in bone lengths \u2206J\u03b2 which further\nstretches instance shape into Ts\n\u03b2 by elongating body parts. (d) Time-varying articulations are modeled with an articulation vector \u03b8 by\nlinearly blending rigid bone transformations in the dual quaternion space. Time-varying deformations (such as muscle deformation) are\nmodeled with a deformation vector \u03c9d through invertible 3D warping fields.\nthe shape and the skeleton.\nTo model between-instance shape variations, one could\nuse dense warping fields to deform a canonical template into\ninstance-specific shapes [68]. However, warping fields can-\nnot explain topological changes (e.g., different clothing).\nInstead, we define a hierarchical representation: a condi-\ntional canonical field [9,43,62] to handle fine-grained vari-\nations over a category (e.g., the ears of dogs) and a stretch-\nable bone model [16, 65] to represent coarse shape varia-\ntions (e.g., height and size of body parts).\nConditional Field T. In the canonical space, a 3D point\nX \u2208 R3 is associated with three properties: signed distance\nd \u2208 R, color c \u2208 R3, and canonical features \u03c8 \u2208 R16,\nwhich is used to register pixel observations to the canonical\nspace [37,74]. These properties are predicted by multi-layer\nperceptron (MLP) networks:\n(d, ct) = MLPSDF(X, \u03b2, \u03c9a),\n(1)\n\u03c8 = MLP\u03c8(X),\n(2)\nwhere the shape and color are conditioned on a video-\nspecific morphology code \u03b2\u2208R32 [18, 39]. We further ask\nthe color to be dependent on an appearance code \u03c9a\u2208R64\nthat captures frame-specific appearance such as shadows\nand illumination changes [34].\nSkeleton J. Unlike shape and color, the bone structures are\nnot directly observable from imagery, making it ambiguous\nto infer. Methods for automatic skeletal rigging [25,40,71]\neither heavily rely on shape priors, or appear sensitive to\ninput data. Instead, we provide a category-level skeleton\ntopology, which has a fixed tree topology with (B+1) bones\nand B joints (B=25 for quadruped and B=18 for human).\nTo model cross-instance morphological changes, we define\nper-instance joint locations as:\nJ = MLPJ(\u03b2) \u2208 R3\u00d7B.\n(3)\nAs we will discuss next, the change in joint locations not\nonly stretches the skeleton, but also results in the elongation\nof canonical shapes as shown in Fig. 3 (c). The skeleton\ntopology is fixed through optimization but J is specialized\nto each video.\nSkinning Field W. For a given 3D location X, we define\nskinning weight W \u2208 RB+1 following BANMo:\nW = \u03c3softmax\n\u0000d\u03c3(X, \u03b2, \u03b8) + MLPW(X, \u03b2, \u03b8)\n\u0001\n,\n(4)\nwhere \u03b8 is a articulation code and d\u03c3(X, \u03b2, \u03b8) is the Ma-\nhalanobis distance between X and Gaussian bones under\narticulation \u03b8 and morphology \u03b2, refined by a delta skin-\nning MLP. Each Gaussian bone has three parameters for\ncenter, orientation, and scale respectively, where the cen-\nters are computed as the midpoint of two adjacent joints,\nthe orientations are determined by the parent joints, and the\nscales are optimized.\nStretchable Bone Deformation. To represent variations of\nbody dimension and part size, prior work [32, 81] learns a\nPCA basis from registered 3D scans. Since 3D registrations\nare not available for in-the-wild videos, we optimize a para-\nmetric model through differentiable rendering. Given the\nstretched joint locations, the model deforms the canonical\nshape T\u03b2 with blend skinning equations,\nTs\n\u03b2 =\n\u0000W\u03b2G\u03b2\n\u0001\nT\u03b2,\n(5)\nwhere G\u03b2 transforms the bone coordinates, and W\u03b2 is the\ninstance-specific skinning weights in Eq. (4).\n3.2. Within-Instance Variation\nWe represent within-instance variations as time-varying\nwarp fields between the canonical space and posed space at\ntime t. Similar to HumanNeRF [62], we decompose mo-\ntion as articulations that explains the near-rigid component\n(e.g., skeletal motion) and deformation that explains the\nremaining nonrigid movements (e.g., cloth deformation).\nNote given the complexity of body movements, it is almost\ncertain the pre-defined skeleton would ignore certain mov-\nable body parts. Adding deformation is crucial to achieving\nhigh-fidelity reconstruction.\n3\nFigure 4. Different \u03b2 morphologies of dogs (top) and cats (bottom). Our reconstructions show variance in ear shape and limb size over\ndog breeds, as well as variance in limb and body size over cat breeds.\nTime-varying Articulation.\nTo model the time-varying\nskeletal movements, we define per-frame joint angles:\nQ = MLPA(\u03b8) \u2208 R3\u00d7B\n(6)\nwhere \u03b8 \u2208 R16 is a low-dimensional articulation parameter,\nand each joint has three degrees of freedom. Given joint\nangles and the per-video joint locations, we compute bone\ntransformations G \u2208 R3\u00d74\u00d7B via forward kinematics. We\napply dual quaternion blend skinning (DQS) [21] to get the\nwarping field for each spatial point,\nD(\u03b2, \u03b8) = (W\u03b2G)Ts\n\u03b2.\n(7)\nCompared to linear blend skinning (LBS), DQS blends\nSE(3) transformations in dual quaternion space and ensures\nvalid SE(3) after blending, which reduces artifacts around\ntwisted body parts. For more analysis and comparisons,\nwe refer readers to a concurrent work [54] that also ap-\nplies DQS for deformable object reconstruction. Note the\nstretching operation in Eq. (5) can be fused with articula-\ntion as a single blend skinning operation.\nTime-varying Soft Deformation. To further explain the\ndynamics induced by non-skeleton movements (such as the\ncat belly and human clothes), we add a neural deformation\nfield [26, 42] D(\u00b7) that is flexible enough to model highly\nnonrigid deformation. Applying the fine-grained warping\nafter blend skinning, we have\nD(\u03b2, \u03b8, \u03c9d) = D(D(\u03b2, \u03b8), \u03c9d),\n(8)\nwhere \u03c9d is a frame-specific deformation code. Inspired\nby CaDeX [26], we use real-NVP [10] to ensure the 3D\ndeformation fields are invertible by construction.\nInvertibility of 3D Warping Fields. For a given time in-\nstance t, we have defined a forward warping field Wt,\u2192\nthat transforms 3D points from the canonical space to the\nspecified time instance, and a backward warping field Wt,\u2190\nto transform points in the inverse direction. Both warping\nfields include stretching (Eq. (5)), articulation. (Eq. (7)),\nand deformation (Eq. (8)) operations.\nNotably, we only\nneed to define each operation in the forward warping fields.\nThe deformation operation is, by construction, invertible.\nTo invert stretching and articulation, we invert SE (3) trans-\nformations G in the blend skinning equations and compute\nthe skinning weights with Eq. (4) using the corresponding\nmorphology and articulation codes. A 3D cycle loss is used\nto ensure that the warping fields are self-consistent after a\nforward-backward cycle [31,75].\n3.3. Scene Model\nReconstructing objects from in-the-wild video footage\nis challenging due to failures in segmentation, which is\noften caused by out-of-frame body parts, occlusions, and\nchallenging lighting.\nInspired by background subtrac-\ntion [17, 53], we build a model of the background to ro-\nbustify our method against inaccurate object segmentation.\nIn background subtraction, moving objects can be seg-\nmented by comparing input images to a background model\n(e.g., a median image). We generalize this idea to model\nthe background scene in 3D as a per-video NeRF, which can\nbe rendered as color pixels at a moving camera frame and\ncompared to the input frame. We design a conditional back-\nground model that generates density and color of a scene\nconditioned on a per-video background code \u03b3:\n(\u03c3, ct) = MLPbg(X, v, \u03b3),\n(9)\nwhere v is the viewing direction. To render images, we\ncompose the density and color of the object field and the\nbackground NeRF in the view space [39], and compute the\nexpected color and optical flow. Background modeling and\ncomposite rendering allows us to remove the object silhou-\nette loss, and improves the quality of results. Interestingly,\nwe find that even coarse geometric reconstructions of the\nbackground still can improve the rendered 2D object sil-\nhouette, which in turn is useful for improving the quality of\nobject reconstructions (Fig. 5). We ablate the design choice\nin Tab. 1.\n3.4. Losses and Regularization\nGiven the videos and a predefined skeleton, we optimize\nthe parameters discussed above: (1) canonical parameters\n{\u03b2, J, T, W} including per-video morphology codes and\ncanonical templates; (2) motion parameters {\u03b8, \u03c9d, A, D}\nincluding per-frame codes as well as articulation and soft\ndeformation MLPs. (3) background parameters {\u03b3, B} in-\ncluding video background codes and a background NeRF.\n4\nInput Image\nInput Silhouette\nRefined Silhouette\nNo Background Modeling\nW/ Background Modeling\nBackground Geometry and Color\nComposite rendering\nFigure 5. Joint foreground and background reconstruction. We\njointly reconstruct objects and their background, while refining the\nsegmentation. Note the input silhouette is noisy (e.g., tail was not\nsegmented), and background modeling helps produce an accurate\nrefined silhouette. As a result, RAC is robust to inaccurate seg-\nmentation (e.g., tail movements marked by the red box).\nThe overall objective function contains a image reconstruc-\ntion loss term and regularization terms.\nReconstruction Losses. The reconstruction loss is defined\nas the difference between rendered and observed images,\nincluding object silhouette, color, flow, and features:\nL = Lsil + Lrgb + LOF + Lfeat.\n(10)\nWe update the model parameter by minimizing L through\ndifferentiable volume rendering in the same way as\nBANMo [75]. Off-the-shelf estimates of object silhouettes\nare used as supervision to kick-start the optimization. Then\nthe weight of silhouette term is set to 0 after several iter-\nations of optimization, while composite rendering of fore-\nground and background itself is capable of separating the\nobject and the non-object components.\nMorphology Code (\u03b2) Regularization. Existing differen-\ntiable rendering methods are able to faithfully reconstruct\nthe input view but not able to hallucinate a reasonable so-\nlution for occluded body parts [42, 75]. See Fig. 9 for an\nexample. One solution is to regularize the instance-specific\nmorphology code \u03b2 to be consistent with the body shapes\nobserved in other videos. Traditional approaches might do\nthis by adding variational noise (as in a VAE) or adversar-\nial losses (as in a GAN). We found good results with the\nfollowing easy-to-implement approach: we randomly swap\nthe morphology code \u03b2 of two videos during optimization;\nthis regularizes the model to effectively learn a single mor-\nTable 1. Quantitative results on AMA sequences. 3D Chamfer\ndistance (cm, \u2193) and F-score (%, \u2191) averaged over all frames. Our\nmodel is trained on 47 human videos spanning existing human\ndatasets (as described in Sec.4.2); we also train BANMo on the\nsame set. Other baselines are trained on 3D human data and re-\nlies on SMPL model. Results with S indicates variants trained on\nsingle instances. Our model outperforms prior works.\nMethod\nsamba\nbouncing\nCD\nF@2%\nF@5%\nCD\nF@2%\nF@5\nHuMoR\n9.8\n47.5\n83.7\n11.5\n45.2\n82.3\nICON\n10.1\n39.9\n85.2\n9.7\n53.5\n86.4\nBANMoS\n8.0\n62.2\n89.1\n7.6\n64.7\n91.1\nBANMo\n9.3\n54.4\n85.5\n10.2\n54.4\n86.5\nRACS\n6.4\n70.9\n93.2\n6.9\n66.7\n92.8\nRAC\n6.0\n72.5\n94.4\n8.0\n63.8\n91.4\nw/o skeleton\n8.6\n59.6\n87.7\n9.3\n59.5\n87.8\nw/o \u03b2\n8.5\n58.9\n87.5\n8.4\n62.5\n90.6\n\u03b2 swap\u2192 ||\u03b2||2\n2\n6.5\n69.0\n93.8\n8.0\n64.8\n91.3\n+ bkgd NeRF\n6.3\n70.9\n93.7\n7.4\n65.5\n91.8\nphology code that works well across all videos. But naively\napplying this approach would produce a single morphology\nthat would not specialize to each object instance. To en-\nable specialization, we gradually decrease the probability\nof swaps during the optimization, from P = 1.0 \u2192 0.05.\nJoint J Regularization. Due to the non-observable nature\nof the the joint locations, there might exist multiple joint\nconfigurations leading to similar reconstruction error. To\nregister the skeleton with the canonical shape, we minimize\nSinkhorn divergence [12] between the canonical surface T\u03b2\nand the joint locations J\u03b2, which forces them to occupy the\nsame space. We extract the canonical mesh with marching\ncubes [33] as a proxy of the canonical surface. Sinkhorn\ndistance interpolates between Wasserstein and kernel dis-\ntances and defines a soft way to measure the distance be-\ntween shapes with different density distributions.\nSoft Deformation Regularization The soft deformation\nfield has the capacity of explaining not only the soft de-\nformations, but also the skeleton articulations. Therefore,\nwe penalize the L2 norm of the soft deformation vectors at\nrandomly sampled morphology and articulations,\nLsoft = \u2225D(\u03b2, \u03b8, \u03c9d) \u2212 D(\u03b2, \u03b8)\u2225 .\n(11)\n4. Experiments\nImplementation Details. We build RAC on BANMo and\ncompute bone transformations from a kinematic tree. The\nsoft deformation field follows CaDeX, where we find that\ntwo invertible blocks are capable of handling moderate de-\nformations. To evaluate surface reconstruction accuracy, we\nextract the canonical mesh T by finding the zero-level set of\nSDF with marching cubes on a 2563 grid. To get the shape\n5\nHumor\nICON\nInput\nFrame 146\nFrame 29\nOurs\nBANMo\n1:45\n29:20\n36:0\n66:70\n56:0\n67:0\nBARC\nInput\nOurs\nBANMo\nInput\nBANMo\nOurs\nFigure 6. Qualitative comparison. We compare with BANMo and model-based methods (HuMoR, ICON, BARC). Top: human recon-\nstruction on (AMA). Bottom: dogs and cats reconstruction on internet videos.\nat a specific time, the canonical mesh is forward-warped\nwith Wt,\u2192.\nOptimization Details We use AdamW to optimize the\nmodel for 36k iterations with 16384 rays per batch (taking\naround 24 hours on 8 RTX-3090 GPUs). We fist pre-train\nthe background model with RGB, optical flow, and sur-\nface normal losses while ignoring foreground pixels. Then\nwe combine background models with the object model for\ncomposite rendering optimization. The weights for the loss\nterms are tuned to have similar initial magnitude. The object\nroot poses are initialized with single-image viewpoint net-\nworks trained for humans and quadruped animals following\nBANMo [75]. For all categories, we start with the same\nshape (a unit sphere) and a known skeleton topology. Both\nthe shape and the joint locations are specialized to the input\ndataset, as shown in Fig. 7.\n4.1. Reconstructing Humans\nDataset. We combine existing human datasets, including\nAMA, MonoPerfCap, DAVIS, and BANMo to get 47 hu-\nman videos with 6,382 images [45, 59, 70].\nAMA con-\ntains multi-view videos, but we treat them as monocular\nvideos and do not use the time-synchronization or camera\nextrinsics. During preprocessing, we use PointRend [22]\nto extract object segmentation, CSE [38] for pixel features,\nVCN-robust [72] for optical flow, and omnidata [11] for sur-\nface normal estimation.\nMetrics.\nWe use AMA for evaluation since it contains\nInitial Shape and Skeleton\nFinal Canonical Shape and Skeleton (w/ average morphology)\nFinal Canonical Shape and Skeleton (w/ average morphology)\nInitial Shape and Skeleton\nInitial Shape and Skeleton\nFinal Shape and Skeleton (mean morphology)\nFigure 7. Shape and skeleton optimization. From top to bottom,\nwe visualize the canonical shape and skeleton of our dog, cat, and\nhuman models. Left: Canonical shape and skeleton before opti-\nmization. Right: Canonical shape and skeleton after optimization.\nground-truth meshes and follow BANMo to compute both\nChamfer distances and F-scores. Chamfer distance com-\nputes the average distance between the ground-truth and\nthe estimated surface points by finding the nearest-neighbor\nmatches, but it is sensitive to outliers. F-score at distance\nthresholds d \u2208 {1%, 2%, 5%} of the human bounding box\nsize [57] provides a more informative quantification of sur-\nface reconstruction error at different granularity. To account\n6\nfor the unknown scale, we align the predicted mesh with the\nground-truth mesh using their depth in the view space.\nBaselines.\nOn AMA, we compare with template-free\nBANMo [75] and model-based methods, including Hu-\nMoR [48] and ICON [69]. BANMo reconstructs an animat-\nable 3D model from multiple monocular videos of the same\ninstance, powered by differentiable rendering optimization.\nWe optimize BANMo on the same dataset with the same\namount of computation and iterations as ours. HuMoR is\na temporal pose and shape predictor for humans. It per-\nforms test-time optimization on video sequences leverag-\ning OpenPose keypoint detection and motion priors trained\non large-scale human motion capture dataset. We run it on\neach video sequence, and processing 170 video frames takes\naround two hours on a machine with Titan-X GPU. ICON is\nthe recent SOTA method on single view human reconstruc-\ntion. It combines statistical human body models (SMPL)\nwith implicit functions and is trained on 3D human scans\nwith clothing. Notably, it performs test-time optimization\nto fit surface normal predictions to improve the pose accu-\nracy and reconstruction fidelity. We run it per frame, and\nprocessing a 170 frame video takes around three hours on\nan RTX-3090 GPU.\nResults. We show qualitative comparison with baselines\nin Fig. 6 top row, and quantitative results in Tab. 1. On\nthe handstand sequence, HuMoR works well for common\nposes but fails where the performer is not in an upright pose.\nICON works generally well, but the hand distances appear\nnot physically plausible (too short) from a novel viewpoint.\nBANMo reconstruction also failed to reconstruct the un-\nnatural upside-down pose. In contrast, RAC successfully\nreconstructs the handstand pose with plausible hand dis-\ntances. On the samba sequence, HuMoR correctly predicts\nbody poses, but fails to reconstruct the cloth and its defor-\nmation. ICON predicts a broken dress and distorted human\nlooks from a novel viewpoint, possibly due to lack of di-\nverse training data from dressed humans. When applied to\n47 videos of different humans, BANMo fails to model the\ncloth correctly, possibly because a limited number of con-\ntrol points are not expressive enough to model the morpho-\nlogical variations over humans wearing different clothes.\nRAC models between-shape variations using a conditional\ncanonical model and successfully reconstructs cloth defor-\nmation using the soft deformation field.\nOur quantitative results align with qualitative observa-\ntions, where RAC outperforms all baselines except being\nslightly worse than BANMo trained on single instances (S).\nHowever, RAC trained on single instances (S) or multiple\ninstances (M) outperforms BANMo trained in either fash-\nion. In particular, BANMo results are notably worse when\ntrained on multiple instances, indicating the difficulty in\nbuilding category models. In contrast, RAC become better\nwhen trained on multiple instances.\n4.2. Reconstructing Cats and Dogs\nDataset. We collect 76 cat videos and 85 dog videos from\nInternet videos, as well as public data from BANMo. All\nthe videos are casually-captured monocular videos. We ex-\ntract video frames at 10 FPS, including 9,734 frames for\ncats and 11,657 frames for dogs. We perform the same pre-\nprocessing as human reconstruction.\nBaselines.\nWe compare with BANMo and model-based\nBARC [49]. BARC is the current SOTA for dog shape and\npose estimation. It trains a feed-forward network using CG\ndog models and images with keypoint labels. The shape\nmodel is based on SMAL, which uses manual rigging and\nregistration to fit 3D animal toys. We run BARC on indi-\nvidual images.\nResults. We show qualitative results in Fig. 6 bottom row.\nFor dog videos, we find that BARC worked well to predict\ncoarse shapes and poses. However, the results are biased\ntowards certain dog breeds. For instance, BARC predicts\na long jaw when the dog has a short jaw (top row), and\npredicts round ears when the dog has sharp ears (bottom).\nBANMo was able to reconstruct a reasonable coarse shape,\nbut failed to capture the fine details (e.g., the shape of the\near and the size of the head) with only 25 control points.\nIn contrast, RAC was able to faithfully capture the coarse\nshape and fine details of the dogs. Unlike dogs, cats have\nfewer variations in body shape and size, where we find that\nBANMo works well in most cases. However, for the body\nparts not visible from the reference viewpoint, BANMo of-\nten estimates a squashed shape, which may be caused by\nthe entangled morphology and articulations. In contrast,\nRAC accurately infers reasonable body parts and articula-\ntions even when they are not visible.\n4.3. Diagnostics\nLarge Morphological Changes.\nWe reconstruct eight\nvideos of different quadruped animals together to \u201cstress\ntest\u201d our method. The dataset contains two dog videos, two\ncat videos, and one video for goat, bear, camel, and cow,\nrespectively. The result is shown in Fig. 8.\nP=0.01\nP=0.05\nFigure 8. Quadruped Category Reconstruction. Using a smaller\ncode swapping probability P\n= 0.01 results in more faithful\ninstance shape, but less smooth results.\nA larger P produces\nsmoother results, but some instance-specific features disappear.\nMorphology Code \u03b2 Removing morphology code \u03b2 from\n7\nthe canonical field degrades it to a standard NeRF. We rerun\nthe experiments and the results are shown in Tab. 1 as well\nas Fig. 9. Without the morphology code, our reconstruc-\ntions are forced to share the same canonical shape, which\nas discussed in Sec 3.1, failed to handle fine deformations\nand topological changes (e.g., clothing), leading to worse\nresults in all metrics.\nMorphology Code Regularization To test the effective-\nness of the morphology code regularization, we set P = 0\nthroughout the optimization. The results are shown in Tab. 1\nand Fig. 9. Without regularization of the morphology code,\nthe reconstructed shape may appear reasonable from the ref-\nerence viewpoint, but severely distorted from a novel view-\npoint. We posit the body parts that are not well-covered in\nthe video are inherently difficult to infer. As a result, the\nshapes become degenerate without relying on priors from\nother videos in the dataset. Tab. 1 also shows that code\nswapping outperforms norm regularization [14] (||\u03b2||2\n2).\nWe posit norm regularization forces codes to be similar, but\ndoes not constrain their output space, while code swapping\nencourages any code to explain any image in the data. In\npractice, we find that code swapping generates valid out-\nputs when we interpolate between codes.\nw/o    code \nw/o     regularization\nFull Model\n\u03b2\n\u03b2\nReference\nFigure 9. Ablation study on morphology modeling.\nSoft Deformation. After removing the soft deformation\nfield, RAC fails to recover body parts that are not controlled\nby the skeleton (Fig. 10), such as the ears of the dog.\nw/o\nFull Model\nwith images for cat/dogs\n\u03c9d\nReference\nSkeleton Visualization\nFigure 10. Ablation study on soft deformation \u03c9d.\nMotion Transfer. Given the category model with disentan-\ngled morphology and articulations, we can easily transfer\nan articulation in frame t to other video by setting the ar-\nticulation parameter to \u03b8t, while keeping the morphology\nparameter \u03b2 the same. We show motion transfer across hu-\nman in Fig. 2. Please visit our website for video results of\ndogs, cats, and humans.\nSkeleton vs Control Points. Control point deformations\nare flexible but do not preserve body dimensions (e.g., a line\nsegment can be stretched longer by its end points). As a re-\nsult, body and limb dimensions can change, creating two\nproblems: (1) articulated shapes look squashy from novel\nviews, and (2) variations in body dimensions are entangled\nwith control-point deformations. In contrast, skeleton de-\nformation preserves body dimensions. It produces better\nresults (Tab. 1) and better motion re-targeting (Fig. 11).\nMotion source\nTransferred\nMotion source\nTransferred\nRAC w/ control points\nRAC w/ skeleton\nFrame 40\nFrame 60\nFigure 11. Skeleton vs control-points for motion transfer. RAC\nwith control points fails to maintain the body dimensions during\nmotion, and produces squashy results when transferred to a new\nmorphology. RAC with skeleton disentangles motion from mor-\nphology, allowing for better motion transfer.\nStretchable Bones allow for control of bone dimensions\nafter optimization. We show an example of a Dachshund\n(Source1) warped to a Heeler (Source2) by modifying bone\ndimensions while keeping the shape unchanged.\n\u03b21\nbone + \u03b21\ntemplate\n\u03b22\nbone + \u03b22\ntemplate\n\u03b22\nbone + \u03b21\ntemplate\n\u03b21\nbone + \u03b22\ntemplate\nArticulation source\nSource 1\nSource 2\nFigure 12. Disentanglement of bone dimensions and shape.\n5. Discussion\nWe have presented a scalable way of building animat-\nable category models by learning from monocular videos. It\ndisentangles morphology variations between instances and\nmotion within an instance, allowing motion transfer over a\ncategory. RAC reaches state-of-the-art reconstruction qual-\nity for cats, dogs, and humans in terms of mid-level recon-\nstruction, but details are still missing (such as human hand\nand toes). Similar to BANMo, RAC requires rough view-\npoint initialization. Although we have shown either a pre-\ntrained viewpoint estimator or roughly annotated camera\nviewpoints (in the supplement) are sufficient, it would be\ninteresting to study a more generic way to initialize view-\npoints. We also show that category-level skeleton improves\nmotion reconstruction, and leave jointly inferring skeleton\nstructure together with object shape for future work.\n8\nAcknowledgement This work was supported by the Qual-\ncomm Innovation Followship and the CMU Argo AI Center\nfor Autonomous Vehicle Research.\nA. Additional Details\nShape Regularization. We apply eikonal regularization to\nforce the norm of the first order derivative of signed dis-\ntances d to be close to 1,\nLeikonal = (\u2225\u2207MLPSDF(X)\u2225 \u2212 1)2.\n(12)\nEikonal loss forces the reconstruction to be a valid surface\nand empirically improves the surface reconstruction quality.\nPose, Deformation, and Appearance Smoothness. We\nwould like the time-varying articulated pose, deformation,\nand appearance codes {\u03b8, \u03c9d, \u03c9a} to vary smoothly within a\nvideo. To accomplish this, we make use of time-dependent\npositional embeddings (similar to [75]):\n\u03c9b\nt = AiF(t)\n(13)\nwhere F(\u00b7) is a 1D basis of sines and cosines with linearly-\nincreasing frequencies at log-scale [56], and we learn sepa-\nrate weight matrices Ai\u2208{1...,M} for each video.\nB. Category Outside DensePose\nWe test RAC in a scenario where there is no predefined\nDensePose features and skeleton.\nFigure 13. Vehicle Category Reconstruction. Our method is able\nto fuse videos of 365 vehicles with different appearance and shape\ninto a category model. From left to right, we show reconstruction\nof sedans, SUVs, and vans.\nVehicle Dataset.\nWe employ images from multiple 4K\ncameras [47] that overlook urban public spaces to analyze\nthe flow of traffic vehicles. The data are captured for 3-\nsecond bursts every few minutes, and only images with no-\ntable changes are stored. We extracted 365 car videos from\nthe dataset to build the category model. The dataset con-\ntains wide variation in vehicle categories like pickup trucks,\nconstruction vehicles etc on which traditional model based\napproached perform poorly.\nCamera Pose Initialization.\nAs there is no DensePose\nmodel for cars, we took a two-stage approach to first\nTable 2. Quantitative results on Pablo sequence. 3D Chamfer\ndistance (cm, \u2193) is computed on the clothing region and averaged\nover all frames. MPCap uses a pre-scanned personalized template.\nMethod\nMPCap*\nMCCap\nPiFuHD\nT2S\nRAC\nChamfer\n14.6\n17.9\n26.5\n27.7\n18.3\ncoarsely register a few car videos with manual viewpoint\nannotation and then train a single-image viewpoint network\nto predict viewpoints for the rest of the videos. The cam-\nera viewpoints are roughly annotated for each frame (with\naround 30 degree rotation error).\nAnnotation for a 100\nframe video takes around 30 seconds. We found annotat-\ning 20 cars to be sufficient to train a viewpoint estimator\nthat generalizes to other cars.\nResults. We show the reconstruction results of car videos\nin Fig. 13. Please visit the website for more results.\nC. Evaluation on Pablo Sequence\nWe compare with baselines on the Pablo sequence,\nwhich is part of the public MonoPerfCap [70] dataset. Our\nmethod optimizes the Pablo sequence together with the\nrest of our 47 human videos. After differentiable rendering\noptimization, we extract meshes for the Pablo sequence\nand compare with the 3D ground-truth for evaluation.\nMetrics. We follow the evaluation protocol of MonoCloth-\nCap [67] and compute the average point-to-surface dis-\ntances in the clothing region. The clothing region (the T-\nshirt and shorts) is obtained by manual segmentation on the\nground-truth surface mesh.\nResults. We show quantitative comparisons in Tab. 2 and\nrefer the reader to the qualitative results in Fig. 8 of the main\ndraft. Our method outperforms PiFuHD [51], Tex2Shape\n(T2S) [1], both of which are single-view human shape pre-\ndictors trained on 3D scans of humans. Our method does\nnot use 3D data to train but performs test-time optimiza-\ntion on 47 human videos. Our method is slightly worse\nthan MonoClothCap (MCCap) [67] that uses a parametric\nhuman body model (SMPL), and worse than MonoPerfCap\n(MPCap), which uses a prescanned template. Both para-\nmetric body model and personalized shape template pro-\nvides a strong shape prior, while our method does not rely\non any shape prior.\nD. Difference from prior works\nWe highlight the difference from previous work in Tab. 3.\nIn terms of shape modeling, HyperNeRF [43] and Human-\nNeRF [62] reconstruct a single scene or instance, while\nRAC learns a space of category shapes.\nFor skeleton\nmodeling, CASA [65] is optimized per-instance, while\n9\nRAC learns a shared space over a category of skeletons\n(with different bone lengths). For background modeling,\nNeRF++ [77] assumes a static scene and does not use\nbackground to help object segmentation and reconstruction.\nNerFace [13] treats background as a static image, while we\nrepresent the background as a NeRF, which generalizes to\nvideos captured by a moving camera.\nTable 3. Difference between prior works and RAC.\nMethod\nShape\nMotion\nBackground\n3D Data/Pose\nNeRF++\nN.A.\nN.A.\nNeRF\nNo\nNeRFace\nInstance\nConditional\nImage\nNo\nHyperNeRF\nInstance\nFields+Conditional\nN.A.\nNo\nBANMo\nInstance\nControl Points\nN.A.\nNo\nCASA\nInstance\nInstance Skeleton\nN.A.\nYes\nHumanNeRF\nInstance\nInstance Skeleton\nN.A.\nYes\nRAC\nCategory\nCategory Skeleton\nNeRF\nNo\nReferences\n[1] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt,\nand Marcus Magnor. Tex2shape: Detailed full human body\ngeometry from a single image. In ICCV, pages 2293\u20132303,\n2019. 9\n[2] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-\nbastian Thrun, Jim Rodgers, and James Davis. Scape: shape\ncompletion and animation of people. In ACM SIGGRAPH\n2005 Papers, pages 408\u2013416. 2005. 1\n[3] Marc Badger, Yufu Wang, Adarsh Modh, Ammon Perkes,\nNikos Kolotouros, Bernd Pfrommer, Marc Schmidt, and\nKostas Daniilidis. 3D bird reconstruction: a dataset, model,\nand shape recovery from a single view. In ECCV, 2020. 2\n[4] Benjamin Biggs, Ollie Boyne, James Charles, Andrew\nFitzgibbon, and Roberto Cipolla. Who left the dogs out: 3D\nanimal reconstruction with expectation maximization in the\nloop. In ECCV, 2020. 2\n[5] Benjamin Biggs, Thomas Roddick, Andrew Fitzgibbon, and\nRoberto Cipolla. Creatures great and smal: Recovering the\nshape and motion of animals from video. In ACCV, pages\n3\u201319. Springer, 2018. 1\n[6] Federica Bogo,\nJavier Romero,\nMatthew Loper,\nand\nMichael J Black. Faust: Dataset and evaluation for 3d mesh\nregistration. In CVPR, pages 3794\u20133801, 2014. 1\n[7] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and\nGordon Wetzstein. pi-gan: Periodic implicit generative ad-\nversarial networks for 3d-aware image synthesis. In arXiv,\n2020. 2\n[8] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. Efficient geometry-aware 3D\ngenerative adversarial networks. In arXiv, 2021. 2\n[9] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein.\npi-gan: Periodic implicit genera-\ntive adversarial networks for 3d-aware image synthesis. In\nCVPR, pages 5799\u20135809, 2021. 3\n[10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-\ngio.\nDensity estimation using real nvp.\narXiv preprint\narXiv:1605.08803, 2016. 4\n[11] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir\nZamir.\nOmnidata: A scalable pipeline for making multi-\ntask mid-level vision datasets from 3d scans. In ICCV, pages\n10786\u201310796, 2021. 6\n[12] Jean Feydy, Thibault S\u00b4ejourn\u00b4e, Franc\u00b8ois-Xavier Vialard,\nShun-ichi Amari, Alain Trouve, and Gabriel Peyr\u00b4e. Inter-\npolating between optimal transport and mmd using sinkhorn\ndivergences. In The 22nd International Conference on Ar-\ntificial Intelligence and Statistics, pages 2681\u20132690, 2019.\n5\n[13] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias\nNie\u00dfner. Dynamic neural radiance fields for monocular 4d\nfacial avatar reconstruction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8649\u20138658, 2021. 10\n[14] Partha Ghosh, Medhi SM Sajjadi, Antonio Vergari, and\nMichael Black.\nFrom variational to deterministic autoen-\ncoders. In ICLR, 2020. 8\n[15] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik.\nShape and viewpoints without keypoints. In ECCV, 2020.\n2\n[16] Alec Jacobson and Olga Sorkine. Stretchable and twistable\nbones for skeletal shape deformation. In SIGGRAPH Asia,\npages 1\u20138, 2011. 3\n[17] Ramesh Jain and H-H Nagel. On the analysis of accumula-\ntive difference pictures from image sequences of real world\nscenes. TPAMI, (2):206\u2013214, 1979. 4\n[18] Wonbong Jang and Lourdes Agapito. Codenerf: Disentan-\ngled neural radiance fields for object categories. In ICCV,\npages 12949\u201312958, October 2021. 3\n[19] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Sel-\nfrecon: Self reconstruction your digital avatar from monoc-\nular video. In CVPR, 2022. 2\n[20] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and\nJitendra Malik. Learning category-specific mesh reconstruc-\ntion from image collections. In ECCV, 2018. 2\n[21] Ladislav Kavan, Steven Collins, Ji\u02c7r\u00b4\u0131\n\u02c7Z\u00b4ara, and Carol\nO\u2019Sullivan. Skinning with dual quaternions. In Proceed-\nings of the 2007 symposium on Interactive 3D graphics and\ngames, pages 39\u201346, 2007. 4\n[22] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-\nshick.\nPointrend: Image segmentation as rendering.\nIn\nCVPR, 2020. 6\n[23] Muhammed Kocabas, Nikos Athanasiou, and Michael J.\nBlack.\nVibe: Video inference for human body pose and\nshape estimation. In CVPR, June 2020. 2\n[24] Filippos Kokkinos and Iasonas Kokkinos.\nTo the point:\nCorrespondence-driven monocular 3d category reconstruc-\ntion. In NeurIPS, 2021. 2\n[25] Binh Huy Le and Zhigang Deng. Robust and accurate skele-\ntal rigging from mesh sequences.\nACM Transactions on\nGraphics (TOG), 33(4):1\u201310, 2014. 3\n[26] Jiahui Lei and Kostas Daniilidis. Cadex: Learning canonical\ndeformation coordinate space for dynamic surface represen-\ntation via neural homeomorphism. In CVPR, 2022. 4\n[27] Ruilong Li, Kyle Olszewski, Yuliang Xiu, Shunsuke Saito,\nZeng Huang, and Hao Li. Volumetric human teleportation.\n10\nIn ACM SIGGRAPH 2020 Real-Time Live!, pages 1\u20131. 2020.\n2\n[28] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhofer, Jur-\ngen Gall, Angjoo Kanazawa, and Christoph Lassner. Tava:\nTemplate-free animatable volumetric actors. 2022. 2\n[29] Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xi-\naolong Wang, Ming-Hsuan Yang, and Jan Kautz.\nOnline\nadaptation for consistent mesh reconstruction in the wild. In\nNeurIPS, 2020. 2\n[30] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello,\nVarun Jampani, Ming-Hsuan Yang, and Jan Kautz.\nSelf-\nsupervised single-view 3d reconstruction via semantic con-\nsistency. ECCV, 2020. 2\n[31] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of dy-\nnamic scenes. In CVPR, 2021. 4\n[32] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J. Black. SMPL: A skinned multi-\nperson linear model. SIGGRAPH Asia, 2015. 1, 2, 3\n[33] William E Lorensen and Harvey E Cline. Marching cubes:\nA high resolution 3d surface construction algorithm. SIG-\nGRAPH, 21(4):163\u2013169, 1987. 5\n[34] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi,\nJonathan T. Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. NeRF in the Wild: Neural Radiance Fields for Un-\nconstrained Photo Collections. In CVPR, 2021. 3\n[35] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4460\u20134470, 2019. 2\n[36] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2\n[37] Natalia Neverova, David Novotny, Vasil Khalidov, Marc\nSzafraniec, Patrick Labatut, and Andrea Vedaldi. Continu-\nous surface embeddings. In NeurIPS, 2020. 3\n[38] Natalia Neverova, Artsiom Sanakoyeu, Patrick Labatut,\nDavid Novotny, and Andrea Vedaldi.\nDiscovering rela-\ntionships between object categories via universal canonical\nmaps. In CVPR, 2021. 6\n[39] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-\ning scenes as compositional generative neural feature fields.\nIn CVPR, pages 11453\u201311464, 2021. 2, 3, 4\n[40] Atsuhiro Noguchi, Umar Iqbal, Jonathan Tremblay, Tatsuya\nHarada, and Orazio Gallo. Watch it move: Unsupervised\ndiscovery of 3D joints for re-posing of articulated objects.\nCVPR, 2021. 3\n[41] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya\nHarada. Neural articulated radiance field. In ICCV, 2021.\n2\n[42] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn ICCV, 2021. 1, 4, 5\n[43] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. arXiv, 2021. 1, 3, 9\n[44] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and\nMichael J. Black. Expressive body capture: 3d hands, face,\nand body from a single image. In CVPR, 2019. 2\n[45] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alexander Sorkine-Hornung, and Luc Van Gool.\nThe 2017 davis challenge on video object segmentation.\narXiv:1704.00675, 2017. 6\n[46] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-NeRF: Neural Radiance Fields\nfor Dynamic Scenes. In CVPR, 2020. 1\n[47] N. Dinesh Reddy, Robert Tamburo, and Srinivasa G.\nNarasimhan. Walt: Watch and learn 2d amodal represen-\ntation from time-lapse imagery. In CVPR, pages 9356\u20139366,\nJune 2022. 9\n[48] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang,\nSrinath Sridhar, and Leonidas J. Guibas. Humor: 3d human\nmotion model for robust pose estimation. In ICCV, 2021. 7\n[49] Nadine Rueegg,\nSilvia Zuffi,\nKonrad Schindler,\nand\nMichael J. Black. BARC: Learning to regress 3D dog shape\nfrom images by exploiting breed information.\nIn CVPR,\npages 3876\u20133884, 2022. 2, 7\n[50] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-\nishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned\nimplicit function for high-resolution clothed human digitiza-\ntion. In ICCV, pages 2304\u20132314, 2019. 2\n[51] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul\nJoo. Pifuhd: Multi-level pixel-aligned implicit function for\nhigh-resolution 3d human digitization. In CVPR, 2020. 2, 9\n[52] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J.\nBlack. SCANimate: Weakly supervised learning of skinned\nclothed avatar networks. In CVPR, 2021. 2\n[53] Yaser Sheikh, Omar Javed, and Takeo Kanade. Background\nsubtraction for freely moving cameras.\nIn ICCV, pages\n1219\u20131225. IEEE, 2009. 4\n[54] Chaoyue Song, Tianyi Chen, Yiwen Chen, Jiacheng Wei,\nChuan Sheng Foo, Fayao Liu, and Guosheng Lin. Moda:\nModeling deformable 3d objects from casual videos, 2023.\n4\n[55] Shih-Yang Su, Frank Yu, Michael Zollh\u00a8ofer, and Helge\nRhodin. A-nerf: Articulated neural radiance fields for learn-\ning human shape, appearance, and pose. In NeurIPS, 2021.\n2\n[56] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan T. Barron, and Ren Ng. Fourier fea-\ntures let networks learn high frequency functions in low di-\nmensional domains. NeurIPS, 2020. 9\n[57] Maxim Tatarchenko, Stephan R Richter, Ren\u00b4e Ranftl,\nZhuwen Li, Vladlen Koltun, and Thomas Brox. What do\nsingle-view 3d reconstruction networks learn?\nIn CVPR,\npages 3405\u20133414, 2019. 6\n11\n[58] Shubham Tulsiani, Nilesh Kulkarni, and Abhinav Gupta. Im-\nplicit mesh reconstruction from unannotated image collec-\ntions. In arXiv, 2020. 2\n[59] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan\nPopovi\u00b4c. Articulated mesh animation from multi-view sil-\nhouettes. In SIGGRAPH 2008. 2008. 6\n[60] Minh Vo, Yaser Sheikh, and Srinivasa G Narasimhan. Spa-\ntiotemporal bundle adjustment for dynamic 3d human recon-\nstruction in the wild. IEEE TPAMI, 2020. 2\n[61] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and\nVictor Adrian Prisacariu.\nNerf\u2013: Neural radiance fields\nwithout known camera parameters.\nIn arXiv preprint\narXiv:2102.07064, 2021. 1\n[62] Chung-Yi Weng,\nBrian Curless,\nPratul P. Srinivasan,\nJonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmanNeRF: Free-viewpoint rendering of moving people from\nmonocular video. In CVPR, pages 16210\u201316220, June 2022.\n3, 9\n[63] Shangzhe Wu, Tomas Jakab, Christian Rupprecht, and An-\ndrea Vedaldi.\nDove: Learning deformable 3d objects by\nwatching videos. arXiv preprint arXiv:2107.10844, 2021.\n2\n[64] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rup-\nprecht,\nand Andrea Vedaldi.\nMagicpony:\nLearning\narticulated 3d animals in the wild.\narXiv preprint\narXiv:2211.12497, 2022. 2\n[65] Yuefan Wu, Zeyuan Chen, Shaowei Liu Liu, Zhongzheng\nRen, and Shenlong Wang. Casa: Category-agnostic skeletal\nanimal reconstruction. In NeurIPS, 2022. 3, 9\n[66] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular\ntotal capture: Posing face, body, and hands in the wild. In\nCVPR, 2019. 2\n[67] Donglai Xiang, Fabian Prada, Chenglei Wu, and Jessica\nHodgins.\nMonoclothcap:\nTowards temporally coherent\nclothing capture from monocular rgb video. In 3DV, pages\n322\u2013332. IEEE, 2020. 9\n[68] Christopher Xie, Keunhong Park, Ricardo Martin-Brualla,\nand Matthew Brown. Fig-nerf: Figure-ground neural radi-\nance fields for 3d object category modelling. In 3DV, pages\n962\u2013971. IEEE, 2021. 3\n[69] Yuliang\nXiu,\nJinlong\nYang,\nDimitrios\nTzionas,\nand\nMichael J. Black. ICON: Implicit Clothed humans Obtained\nfrom Normals. In CVPR, pages 13296\u201313306, June 2022. 2,\n7\n[70] Weipeng Xu, Avishek Chatterjee, Michael Zollh\u00a8ofer, Helge\nRhodin, Dushyant Mehta, Hans-Peter Seidel, and Christian\nTheobalt. Monoperfcap: Human performance capture from\nmonocular video.\nACM Trans. Graph., 37(2):27:1\u201327:15,\nMay 2018. 6, 9\n[71] Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Lan-\ndreth, and Karan Singh. Rignet: Neural rigging for articu-\nlated characters. ACM Trans. on Graphics, 39, 2020. 3\n[72] Gengshan Yang and Deva Ramanan. Volumetric correspon-\ndence networks for optical flow. In NeurIPS, 2019. 6\n[73] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic,\nForrester Cole, Huiwen Chang, Deva Ramanan, William T\nFreeman, and Ce Liu.\nLASR: Learning articulated shape\nreconstruction from a monocular video. In CVPR, 2021. 2\n[74] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic,\nForrester Cole, Ce Liu, and Deva Ramanan. Viser: Video-\nspecific surface embeddings for articulated 3d shape recon-\nstruction. In NeurIPS, 2021. 2, 3\n[75] Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ra-\nmanan, Andrea Vedaldi, and Hanbyul Joo. Banmo: Building\nanimatable 3d neural models from many casual videos. In\nCVPR, 2022. 1, 2, 4, 5, 6, 7, 9\n[76] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta.\nShelf-\nsupervised mesh prediction in the wild. In CVPR, 2021. 2\n[77] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\nfields. arXiv preprint arXiv:2010.07492, 2020. 10\n[78] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai.\nPamir: Parametric model-conditioned implicit representa-\ntion for image-based human reconstruction. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, pages\n1\u20131, 2021. 2\n[79] Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and\nMichael Black. Three-d safari: Learning to estimate zebra\npose, shape, and texture from images \u201cin the wild\u201d. In ICCV,\n2019. 2\n[80] Silvia Zuffi, Angjoo Kanazawa, and Michael J. Black. Lions\nand tigers and bears: Capturing non-rigid, 3D, articulated\nshape from images. In CVPR, 2018. 2\n[81] Silvia\nZuffi,\nAngjoo\nKanazawa,\nDavid\nJacobs,\nand\nMichael J. Black. 3D menagerie: Modeling the 3D shape\nand pose of animals. In CVPR, 2017. 2, 3\n12\n"
  },
  {
    "title": "DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects",
    "link": "https://arxiv.org/pdf/2305.05706.pdf",
    "upvote": "1",
    "text": "DexArt: Benchmarking Generalizable Dexterous Manipulation with\nArticulated Objects\nChen Bao1*\nHelin Xu2*\nYuzhe Qin3\nXiaolong Wang3\n1Shanghai Jiao Tong University\n2Tsinghua University\n3UC San Diego\n(a) DexArt Task Suite\n(b) Seen Objects\n(c) Unseen Objects\nFigure 1. Overview. (a) We propose DexArt, a task suite of Dexterous manipulation with Articulated object using point cloud observation.\n(b) We experiment with extensive benchmark methods that learn category-level manipulation policy on seen objects. (c) We evaluate the\npolicies\u2019 generalizability on a collection of unseen objects, as well as their robustness to camera viewpoint change.\nAbstract\nTo enable general-purpose robots, we will require the\nrobot to operate daily articulated objects as humans do.\nCurrent robot manipulation has heavily relied on using a\nparallel gripper, which restricts the robot to a limited set of\nobjects. On the other hand, operating with a multi-finger\nrobot hand will allow better approximation to human be-\nhavior and enable the robot to operate on diverse articu-\nlated objects. To this end, we propose a new benchmark\ncalled DexArt, which involves Dexterous manipulation with\nArticulated objects in a physical simulator. In our bench-\nmark, we define multiple complex manipulation tasks, and\nthe robot hand will need to manipulate diverse articulated\nobjects within each task. Our main focus is to evaluate the\ngeneralizability of the learned policy on unseen articulated\nobjects. This is very challenging given the high degrees of\nfreedom of both hands and objects. We use Reinforcement\n*Equal contributions. Work done while interning at UC San Diego.\nLearning with 3D representation learning to achieve gen-\neralization. Through extensive studies, we provide new in-\nsights into how 3D representation learning affects decision\nmaking in RL with 3D point cloud inputs. More details can\nbe found at https://www.chenbao.tech/dexart/.\n1. Introduction\nMost tools and objects humans interact with are articu-\nlated objects. To allow household robots to facilitate our\ndaily life, we will need to enable them to manipulate di-\nverse articulated objects with multi-finger hands as humans\ndo. However, learning dexterous manipulation remains a\nchallenging task given the high Degree-of-Freedom (DoF)\njoints of the robot hands. While recent work has shown en-\ncouraging progress in using Reinforcement Learning (RL) [1,\n8,29,69] for dexterous manipulation, most research focuses\non manipulating a single rigid object. The manipulation of\ndiverse articulated objects not only adds additional com-\narXiv:2305.05706v1  [cs.RO]  9 May 2023\nplexity with joint DoF, but also brings new challenges in\ngeneralizing to unseen objects in test time, which has been\na major bottleneck for RL. This requires efforts on integrat-\ning 3D visual understanding and robot learning on a novel\nbenchmark.\nRecent proposed robotic manipulation benchmarks [7,\n12, 34, 65] play important roles in robot learning algorithm\ndevelopment. For example, the MetaWorld [65] benchmark\nprovides more than 50 tasks for evaluating RL algorithms.\nHowever, each proposed MetaWorld task only focuses on\none single object without considering generalization across\nobject instances. To enable generalizability for the robots,\nthe ManiSkill [19, 41] benchmark is proposed with diverse\nmanipulation tasks and a large number of objects to manipu-\nlate within each task. While this is encouraging, the use of a\nparallel gripper has limited the tasks the robot can perform,\nand the ways how the robot can operate. For example, it is\nvery challenging for a parallel gripper to pick up a bucket\nusing the handle.\nIn this paper, we propose a new benchmark for Dexterous\nmanipulation with diverse Articulated objects (DexArt). We\nintroduce multiple tasks with a dexterous hand (the Allegro\nHand) manipulating the articulated objects in the simula-\ntion. For each task, instead of operating with a particular\nobject, we provide a training set of diverse articulated ob-\njects and the goal is to generalize the policy to a different\ntest set of articulated objects. To achieve such a general-\nization, we incorporate RL with generalizable visual rep-\nresentation learning: we adopt 3D point clouds as our ob-\nservations and use a PointNet encoder [44] to extract visual\nrepresentations for decision making. The generalizability of\nthe policy depends on the 3D structure understanding mod-\neled by the PointNet encoder. We experiment and bench-\nmark with different methods and settings, and provide four\nkey observations as follows:\n(i) Training with more objects leads to better general-\nization. For each task, we trained policies using varying\nnumbers of objects for each task and tested them on the\nsame set of unseen objects. We find training with more\nobjects consistently achieves better success rates. Similar\nfindings have been reported in studies on manipulation with\nparallel grippers (Generalist-Specialist Learning [24], Man-\niSkill [41]). While this might not be surprising from the\nperception perspective, it does present more challenges for\na single RL policy to work with different objects simultane-\nously. It highlights the importance of learning generalizable\nvisual representations for RL.\n(ii) Encoder with a larger capacity does not necessar-\nily help. We experiment with different sizes of PointNet\nencoders, and we observe the simplest one with the least\nparameters achieves the best sample efficiency and success\nrate, whether the network is pre-trained or not. This is sur-\nprising from the vision perspective, but it is consistent with\nprevious literature which shows RL optimization becomes\nmuch more challenging with large encoders [41].\n(iii) Object part reasoning is essential. With multi-finger\nhand interacting with different object parts, our intuition is\nthat object part recognition and reasoning can be essential\nfor manipulation. To validate our intuition, we pre-train the\nPointNet encoder with object part segmentation tasks. We\nshow the object part pre-training can significantly improve\nsample efficiency and success rate compared to approaches\nwithout pre-training and with other pre-training methods.\n(iv) Geometric representation learning brings robust pol-\nicy. We evaluate the robustness of the policy under unseen\ncamera poses. We find that the policy trained with partial\npoint cloud is surprisingly resilient to variations in camera\nposes, which aligns with the previous studies that use com-\nplete point clouds in policies [32]. The accuracy remains\nconsistent even with large viewpoint variation. This is par-\nticularly useful for real robot applications as it is challeng-\ning to align the camera between sim and real.\nWith the proposed baselines and detailed analysis among\nthem, we hope DexArt benchmark provides a platform to\nnot only study generalizable dexterous manipulation skill\nitself, but also study how visual perception can be improved\nto aim for better decision making. We believe the unifi-\ncation of perception and action, and studying them under\nDexArt can create a lot of research opportunities.\n2. Related Work\nDexterous Manipulation. Dexterous manipulation with\nmulti-fingered robotic hands has been a long standing prob-\nlem in robotics. Previous methods formulate dexterous ma-\nnipulation as a planning problem [3, 5, 13, 21, 43, 51] and\nsolve it with trajectory optimization [28,39,58]. These meth-\nods require well-tuned dynamics model for the robot and\nthe manipulated object, which limits their generalizability.\nOn the other hand, data-driven-based methods do not as-\nsume a pre-built model. The policies are learned either from\ndemonstrations using imitation learning [10, 20, 26, 48\u201350,\n63, 68] or from interaction data using reinforcement learn-\ning [1,8,23,29,69]. However, most methods focus on tasks\nwith single-body objects like grasping or in-hand manipula-\ntion. Dexterous manipulation on articulated objects remains\na challenging problem. In this paper, we propose a new\nbenchmark on learning generalizable manipulation policy\non articulated objects with point cloud observations.\nArticulated Object Manipulation. The ability to per-\nceive and manipulate articulated objects is of vital signifi-\ncance for domestic robot. There have been a lot of recent\nadvancement on perception of articulated objects such as\npose estimation and tracking [30, 33, 57], joint parameter\nprediction [25, 40, 56, 67], part segmentation [15, 38, 64],\nand dynamics property estimation [22].\nOn the robotics\nside, previous works [11,52] also explore model-based con-\ntrol and planning for articulated object. A natural exten-\nsion is to combine both lines of research by first estimat-\ning the articulated object model with perception algorithm\nand then manipulating it with model-based control [35,59].\nAnother line of research bypasses the state and model esti-\nmation by directly learning the actionable information from\nraw sensory input [36, 62]. However, these approaches de-\nfine a single-step action representation and execute it with\npre-defined controllers in an open-loop manner. Different\nfrom these approaches, we formulate articulated object ma-\nnipulation as a sequential decision making process where\nvisual feedback is used in closed-loop control. During pol-\nicy learning, we also study how 3D articulated object repre-\nsentation learning can help decision making.\nLearning from Point Clouds. Point cloud learning has\nbeen a long-last research topic in 3D vision. The pioneer\narchitectures for point cloud, e.g. PointNet [44, 45], SS-\nCNs [18] have been widely used for geometric representa-\ntion learning in part segmentation [15, 38, 64] and 3D re-\nconstruction [15, 25] tasks. In robotics, the learned point\ncloud representation also facilities down-stream manipula-\ntion tasks, e.g. grasp proposal [6, 31, 46, 55], manipulation\naffordance [27, 36, 37], and key points [16]. Recently, re-\nsearchers have explored to use point cloud as the direct in-\nput observation for RL policy [8, 23, 41, 60]. Inspired by\nthese works, our DexArt benchmark introduces new tasks\nusing a multi-finger hand to operate articulated objects. It is\nmore challenging compared to previous environments given\nthe high DoF for both the manipulator and the object. To\ntackle these tasks, we perform extensive experiments on\nhow geometric representation learning (e.g., part reasoning)\ncan affect decision making, which has not been thoroughly\nstudied before.\n3. DexArt Benchmark\nWe propose the DexArt benchmark which contains tasks\nwith different levels of difficulty. It can be used to evaluate\nthe sample efficiency and generalizability of different policy\nlearning methods. In this work, we provide four dexterous\nmanipulation tasks, Faucet, Bucket, Laptop and Toilet, each\nwith a number of seen and unseen objects (see Table 1).\n3.1. Task Description\nFaucet. As shown in the first row of Figure 1, a robot\nis required to turn on a faucet with a revolute joint. The\nrobot hand needs to firmly grasp the handle and then rotate it\nby around 90 degrees. This task evaluates the coordination\nbetween the motion of both dexterous hand and arm. While\na 2-jaw parallel gripper can potentially perform this task,\nit heavily relies on precise arm motion due to its low DoF\nend-effector. The evaluation criteria are based on the rotated\nangle of the handle.\nFigure 2. Tasks and Dexterous Hand. Left: visualization for all\nfour tasks in DexArt Benchmark. Right: visualization of Allegro\nhand where red arrows indicate the revolute joint positions.\nTask\nobjects\nAll\nSeen\nUnseen\nFaucet\n18\n11\n7\nBucket\n19\n11\n8\nLaptop\n17\n11\n6\nToilet\n28\n17\n11\nTable 1. Task Statistics.\nBucket.\nAs shown\nin the second row of\nFigure 1, this task re-\nquires the robot to lift a\nbucket up. To ensure sta-\nble lifting behavior, the\nrobot should stretch out\nits hand under the bucket\nhandle and hold it to construct a form closure [4]. On the\ncontrary, a single parallel gripper can only grasp it with\nforce closure [2], which can hardly achieve success without\nsufficiently large friction. In evaluation, this task is consid-\nered a success if the bucket is lifted to a given height.\nLaptop. As shown in the third row of Figure 1, in this\ntask, a robot should grasp the middle of the screen and then\nopen the laptop lid. This task also fits dexterous hand well.\nA parallel gripper can do this by precisely plugging the lid\nbetween its jaws. However, this constraint increases the dif-\nficulty for arm motion and requires a larger workspace to\nopen the lid. This task is evaluated based on the changed\nangle of laptop lid.\nToilet. As shown in the fourth row of Figure 1, the task\nis similar to the Laptop task, where the robot needs to open\na larger toilet lid. The task is harder as the geometry of the\nlid is more irregular and diverse. The task is successfully\nsolved if the toilet lid is opened at a threshold degree.\n3.2. Environment Setup\nIn our benchmark, we implement our tasks in SAPIEN\nphysical simulator [61] using a XArm6 robot arm (6 DoF)\nwith an anthropomorphic hand, Allegro Hand (16 DoF).\nPreliminaries. We model our control problem with dex-\nterous hand as a Markov Decision Process (MDP), M =\n{S, A, R, T , \u03c10, \u03b3}, where S \u2208 Rn, A \u2208 Rm stand for\nstate and actions respectively. R : S \u00d7 A \u2192 R is the\nreward function that measures the task progress, where hu-\nman knowledge is often incorporated to guide the accom-\nplishment of challenging tasks. T : S \u00d7 A \u2192 S is the\ntransition dynamics. \u03c10 is the initial probability distribution\nand \u03b3 \u2208 [0, 1) is the discount factor.\nObservation Space.\nThe observation consists of two\nparts. First, the proprioceptive data Sr includes the current\njoint position of the whole robot, linear velocity, angular\nvelocity, position and pose of the end-effector palm. Sec-\nond, the partial point cloud Po captured by a depth camera\nincludes the articulated object and the robot. The observed\npoint cloud is first cropped within the robot workspace and\nthen down-sampled uniformly. We also concatenate the ob-\nserved point cloud Po with an imaged robot point cloud Pi\n(see Section 4.1). All these observations are easily accessi-\nble for real-world robots and no oracle information is used.\nAction Space. The action is a 22-dimensional vector\nthat consists of two parts, 6-DoF for arm and 16-DoF for\nhand. We use an operational space control for robot arm\nwhere the first 6-D vector is the target linear and angular\nvelocity of the palm. For Allegro hand, we use a joint posi-\ntion controller to command the position target of 16 joints.\nBoth controllers are implemented by PD control.\n3.3. Reward Design\nThe reward design for all dexterous manipulation tasks\nfollows three principles. (i) To ensure each task is solv-\nable in a reasonable amount of time, a dense reward is re-\nquired. (ii) To eliminate unexpected behavior, the reward\nshould regulate the behavior of policy to be natural (human-\nlike) and safe. (iii) The reward structure should be general\nand standardized across all tasks. We decompose our tasks\ninto three stages: reaching the functional part, constructing\ncontact between the hand and manipulated objects, and ex-\necuting task-specific actions to move the manipulated parts.\nReaching and Grasping Stage. We design a reach re-\nward for the first two stages to encourage the robot hand to\nget close to the manipulated object as follows:\nrreach = 1(stage == 1) min(\u2212\u2225xpalm \u2212 xobject\u2225, \u03bb),\n(1)\nwhere 1() is an indicator function, xpalm and xobject is\nthe 3D position of palm and object in the world frame, \u03bb\nis a regularization term to prevent sudden surge in reward.\nEquation 3.3 only considers the Cartesian distance between\nhand and object, which may cause unexpected behavior,\ne.g., opening the laptop lid with a clenched motion rather\npushing the side of the lid with hand. In the real world,\nsuch motion may cause damage to the manipulated object\nand robot itself. Inspired by [47], we add a contact term to\nencourage better contact between fingers and object:\nrcontact = 1(stage \u2265 2) IsContact ( palm, object )\nAND\n\uf8eb\n\uf8ed X\nfinger\nIsContact ( finger, object ) \u2265 2\n\uf8f6\n\uf8f8 ,\n(2)\nwhere IsContact is a boolean function that performs col-\nlision detection to check whether two links are in contact.\nWe believe a good contact relationship is constructed if both\nthe palm and at least two fingers touch the object.\nPart Manipulation Stage. In the last stage, the robot\nneeds to manipulate the specific part of an articulated object\nto move it to the given pose. The reward of this stage is\ndesigned as follows:\nrprogress = 1(stage == 3)Progress(task),\n(3)\nwhere Progress is a task-specific evaluation function for\nthe current task progress. For example, in the Faucet envi-\nronment, we use the change of handle joint angle to indicate\ntask progress.\nTo eliminate jerky and unstable robot motion, we add a\npenalty term rpenalty, which includes a L2 norm of the action\nand a task-specific term. The overall reward is the weighted\nsum of four reward terms. More details on the reward de-\nsign can be found in our supplementary material.\n3.4. Asset Selection and Annotation\nWe use the articulated object models from the PartNet-\nMobility [61] dataset. We manually select object models\nfor each task to avoid bad modeling and to ensure a con-\nsistent kinematics structure. We further annotate the scale\nand initial positions object-by-object to make sure they have\nreasonable sizes and don\u2019t initially intersect with the robot.\nWe further apply randomness to the object initial position,\ni.e. for each task, we perform reasonable rotation and trans-\nlation from the annotated position while making sure the\ngoal is still achievable, and the object is randomized from\nthe training set during policy learning.\n4. Method\nSolving dexterous manipulation tasks with RL methods\nsuffers from high sample complexity due to high-dimensional\naction space. Tasks with articulated objects and point cloud\nobservation increase the complexity further. In this section,\nwe will discuss several methods for improving policy learn-\ning performance. In Section 4.1, we will talk about the pol-\nicy learning architecture. Section 4.2 will describe details\non how to generate the data for visual pre-training. Finally,\nwe will discuss the pre-training methods evaluated in our\nbenchmark in Section 4.3.\n4.1. Policy Overview\nPolicy Learning. To achieve category-level generaliza-\ntion across diverse objects, we adopt 3D point cloud as our\nobservation and use Proximal Policy Optimization (PPO) [53]\nas our RL algorithm. In the architecture design, the value\nand policy networks share the same feature extracted from\nthe point cloud and robot proprioception, as shown in the\nright part of Figure 3. We use PointNet [44] as the point\ncloud feature extractor. It is worth noting that we employ a\nObserved + Imagined \nPoint Cloud P\nPo\nPi\nRobot Proprioception\nPointNet\nMLP\nF1\nF2\n2. Policy Learning\nPointNet\n1. PointNet Pre-training\nb. Reconstruction\nc. SimSiam\na. Segmentation\nPoint Cloud\nWeight Initialization\nSr\nd. ......\nPolicy\nValue\nFigure 3. Overview. We adopt PPO algorithm with PointNet backbone to learn dexterous manipulation on articulated objects. We use\npre-training to facilitate the policy learning process. (1) The PointNet is pre-trained on perception-only tasks, which includes segmentation,\nreconstruction, SimSiam, etc. (2) The pre-trained PointNet weight is then used to initialize the visual backbone in PPO before RL training.\nsimple version of PointNet. The local MLP has one hidden\nlayer with a GELU activation function, followed by a max\npooling that directly produces the output feature F1. Mean-\nwhile, an MLP is used to extract output feature F2 from\nthe robot proprioception vector Sr. The output feature F1\nand F2 are then concatenated and passed through the value\nMLP and policy MLP. We show in experiments that increas-\ning the volume of the vision extractor actually harms policy\nlearning.\nFeature Extractor Pre-training. We investigate how\n3D representation learning helps with 3D policy learning.\nWe benchmark vision pre-training with five different 3D\nrepresentation learning methods, including both self-supervised\nlearning and supervised learning, which will be discussed\nin Section 4.3. For all methods, we pre-train a visual model\nwith PointNet backbone on perception-only tasks, and then\nuse it to initialize the feature extractor for RL. The pre-\ntraining pipeline is illustrated in Figure 3.\nPoint Cloud Imagination. The point cloud RL has two\nchallenges. First, the hand-object interaction will cause sev-\neral occlusions. Second, the RL training can only handle\nlow-resolution point cloud due to the memory limitation.\nThus only few points in the observation come from the hand\nfingers, which is essential information for decision making.\nInspired by [47], we leverage the robot model to compute\nthe finger geometry via forward kinematics. We can then\nsample points Pi, called imagined point cloud, from the\ncomputed geometry. As shown in the right part of Figure 3,\nour point cloud feature extractor takes as input both ob-\nserved points Po and the imagined points Pi (deep-colored\npoints in Figure 3). This way, we provide the missing de-\ntails of the robot in point cloud observation. Note that Pi is\naccessible even for real robot.\nSegmentation\nReconstruction\nInput Point Cloud\nFigure 4. Pre-training Visualizations. We visualize the segmen-\ntation and reconstruction pre-training results on Toilet (top row)\nand Faucet (bottom row).\n4.2. Pre-training Datasets\nDexArt Manipulation Dataset (DAM). We render the\npoint cloud observations with the setting as manipulation\ntasks. The dataset contains 6k point clouds for each ob-\nject, including observed and imagined points, where the\nstate of robot and articulated object are sampled randomly,\nas shown in the left column of Figure 4. For segmenta-\ntion pre-training, we label the point cloud into 4 groups: the\nfunctional part of the object, the rest of the object, the robot\nhand, and the robot arm, as shown in the middle column of\nFigure 4.\nPartNet-Mobility Manipulation Dataset (PMM). Dif-\nferent from DAM, PMM is directly rendered from PartNet-\nMobility [61] without task information, e.g. robot. PMM\ncontains 46 object categories and 1k point clouds for each\ncategory. The state of the object and the camera viewpoint\nare sampled randomly. For classification, each object in the\nsame category shares the same label. For segmentation, we\nfollow the procedure in [17] to generate ground truth seg-\nTask\nFaucet\nBucket\nLaptop\nToilet\nSplit\nSeen\nUnseen\nSeen\nUnseen\nSeen\nUnseen\nSeen\nUnseen\nNo Pre-train\n0.30 \u00b1 0.22\n0.28 \u00b1 0.21\n0.51 \u00b1 0.12\n0.56 \u00b1 0.08\n0.81 \u00b1 0.01\n0.41 \u00b1 0.09\n0.71 \u00b1 0.05\n0.46 \u00b1 0.02\nSegmentation on PMM\n0.27 \u00b1 0.12\n0.17 \u00b1 0.09\n0.35 \u00b1 0.25\n0.34 \u00b1 0.24\n0.85 \u00b1 0.09\n0.55 \u00b1 0.09\n0.66 \u00b1 0.08\n0.44 \u00b1 0.02\nClassification on PMM\n0.20 \u00b1 0.12\n0.18 \u00b1 0.14\n0.56 \u00b1 0.06\n0.58 \u00b1 0.12\n0.80 \u00b1 0.20\n0.41 \u00b1 0.14\n0.69 \u00b1 0.08\n0.38 \u00b1 0.03\nReconstruction on DAM 0.35 \u00b1 0.02\n0.21 \u00b1 0.03\n0.51 \u00b1 0.08\n0.50 \u00b1 0.05\n0.85 \u00b1 0.04\n0.54 \u00b1 0.08\n0.76 \u00b1 0.03\n0.52 \u00b1 0.03\nSimSiam on DAM\n0.60 \u00b1 0.15\n0.45 \u00b1 0.12\n0.41 \u00b1 0.30\n0.38 \u00b1 0.31\n0.84 \u00b1 0.04\n0.49 \u00b1 0.13\n0.82 \u00b1 0.02\n0.50 \u00b1 0.06\nSegmentation on DAM 0.79 \u00b1 0.02 0.58 \u00b1 0.07 0.75 \u00b1 0.04 0.76 \u00b1 0.07 0.92 \u00b1 0.02 0.60 \u00b1 0.07 0.85 \u00b1 0.01 0.55 \u00b1 0.01\nTable 2. Success Rate of Different Pre-training Methods. We report the success rate (mean \u00b1 std) on four tasks, for both seen and\nunseen objects. DAM = DexArt Manipulation Dataset, PMM = PartNet-Mobility Manipulation Dataset, as described in section 4.2.\nFigure 5. Segmentation After RL Tuning. We visualize the seg-\nmentation results after the PointNet is tuned during policy learn-\ning. The weight from PPO point cloud feature extractor can be\ndirectly applied back to the segmentation network to perform seg-\nmentation prediction.\nmentation masks for functional parts on the articulated ob-\njects.\n4.3. Pre-training Methods\nSupervised Pre-training. We experiment with two su-\npervised pre-training methods including semantic segmen-\ntation and classification. For classification, we train a Point-\nNet on PMM data to predict the label for 46 object cat-\negories. Compared to simpler tasks like grasping, articu-\nlated object manipulation requires more understanding on\n3D parts. The policy needs to locate the functional part and\nreason how to interact with it. Thus, we also investigate how\npre-training on part segmentation can help policy learning.\nWe train segmentation on both DAM and PMM.\nSelf-supervised Pre-training. We also experiment with\ntwo self-supervised pre-training methods, including point\ncloud reconstruction and SimSiam [9]. Following OcCo [54],\nwe use an encoder-decoder architecture for point cloud re-\nconstruction on DAM dataset. The encoder is a PointNet\nthat extracts global embedding and the decoder is a PCN [66]\nwhich reconstructs the original point cloud from global em-\nbedding. The reconstruction is trained via Chamfer loss [14].\nThe reconstruction results are visualized in the right column\nof Figure 4. After pre-training, we use the PointNet encoder\nto initialize the PointNet in PPO.\nWe follow SimSiam and design a siamese network with\nPointNet. In SimSiam training, the network takes two aug-\nmented views of the same point cloud, and forwards them\ninto the same PointNet encoder. An MLP is connected on\none side to predict the similarity while the gradient is stopped\non the other side. The method is trained to maximize the\nsimilarity between both sides. We pre-train the PointNet\nencoder inside SimSiam on the DAM dataset.\n5. Experiment\nWe conduct experiments on the proposed tasks including\nFaucet, Bucket, Laptop, and Toilet defined in Section 3.1.\nWe perform experiments on three aspects: (i) We bench-\nmark different pre-training methods by evaluating both seen\nand unseen articulated objects for all tasks. We test the suc-\ncess rate during and after training. (ii) We ablate how the\nnumber of seen objects and the architecture size of visual\nbackbone can affect policy learning. (iii) We study the ro-\nbustness to camera viewpoint change for different methods,\nwhere we evaluate the task success rate when the input point\ncloud is captured by cameras at novel poses. Overall, we\nevaluate the methods by success rate and episodic returns\non both seen objects and unseen objects. We train RL pol-\nicy with 3 different random seeds for each experiment.\n5.1. Main Results\nWe provide the success rates of all benchmark meth-\nods in Table 2. We compare the RL policy trained from\nscratch (1st row) with five different pre-training methods\n(the following rows). The results show that proper visual\npre-training can benefit the policy learning. We highlight\nour findings as follows. (i) Part segmentation boosts the\npolicy learning on all tasks. It performs the best on all tasks.\nWith segmentation pre-training, the PointNet can better dis-\ntinguish and locate the functional parts, which is critical\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nFaucet\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLaptop\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nToilet\n0\n2\n4\n6\n8\n10\nEnvironment steps (\u00d7106)\n0\n100\n200\n300\nEpisodic return\n0\n10\n20\n30\n40\nEnvironment steps (\u00d7106)\n0\n500\n1000\n0\n5\n10\n15\nEnvironment steps (\u00d7106)\n0\n100\n200\n0\n2\n4\n6\n8\nEnvironment steps (\u00d7106)\n0\n100\n200\nSegmentation on DAM(50%)\nSegmentation on DAM(100%)\nFigure 6. Training Process with Different Number of Seen Objects. The x-axis is the environment steps. The y-axis of the upper row\nis the success rate on unseen objects, evaluated with 3 random seeds, and the shaded area indicates standard deviation. The y-axis of the\nbottom row is the episodic return, where the shaded area represents the standard error. The grey curves show methods with segmentation\npre-training on around 50% of the seen DexArt objects within each category, compared with the red curves that are pre-trained with 100%.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nFaucet\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLaptop\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nToilet\n0\n2\n4\n6\n8\n10\nEnvironment steps (\u00d7106)\n0\n100\n200\n300\nEpisodic return\n0\n10\n20\n30\n40\nEnvironment steps (\u00d7106)\n0\n500\n1000\n0\n5\n10\n15\nEnvironment steps (\u00d7106)\n0\n100\n200\n0\n2\n4\n6\n8\nEnvironment steps (\u00d7106)\n0\n100\n200\nSmall PointNet\nMedium PointNet\nLarge PointNet\nFigure 7. Training with Different PointNet Sizes. The axes mean the same as in Figure 6. The experiments with tree curves are\nsegmentation pre-trained on DAM(100%), with small/medium/large PointNet described in Section 5.2.\nfor articulated object manipulation. (ii) Other pre-training\nmethods to learn a representative global embedding, i.e.,\nclassification, reconstruction, and SimSiam, also improve\nthe policy learning in some cases, especially on Laptop task.\n(iii) Tasks that involve manipulating small functional parts,\ne.g. faucet handles, benefit more from segmentation pre-\ntraining. As shown in Figure 8, the segmentation results\ncan predict the label of small faucet handles correctly, while\nreconstruction focuses more on the global shape complete-\nness and ignores small part details. Thus, part segmentation\nis a more effective pre-training method for more delicate\nmanipulation tasks.\n5.2. Ablation Study\nWe ablate how the number of objects used in training,\nthe size of the vision extractor, and different visual repre-\nsentation learning methods influence the generalizability.\nNumber of Seen Objects. Different from the previous\nexperiment in Section 5.2, we train our model and policy\nusing only 50% of the seen objects. We report the learning\ncurve and the success rate on novel objects during training\nfor methods using 50% and 100% of the objects. In Fig-\n0\n2\n4\n6\n8\n10\nEnvironment steps (\u00d7106)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nFaucet\n0\n10\n20\n30\n40\nEnvironment steps (\u00d7106)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBucket\nSegmentation on PMM\nClassification on PMM\nReconstruction on DAM\nSimSiam on DAM\nSegmentation on DAM\nNo Pre-train\nFigure 8. Different Pre-train Methods. Evaluation success rate\nof different methods in Faucet and Bucket tasks. The shaded area\nindicates the standard deviation.\nure 6, while the convergence speed (represented by episodic\nreturn) with 100% seen objects is slower compared with the\n50% one due to more diverse object geometry, the success\nrate (top row) of the 100% training on unseen objects re-\nmains higher during the whole training process and for all\ntasks. It demonstrates more training objects are crucial for\nbetter policy generalizability.\nSize of the Vision Extractor. We experiment with three\ndifferent sizes for PointNet: (i) The small PointNet with\none hidden layer. (ii) The medium PointNet with three hid-\nden layers. (iii) The large PointNet with five hidden layers.\nAll other components for these three PointNet are the same.\nSurprisingly, we find that the smallest PointNet achieves the\nbest performance for both success rate and episodic return,\nas shown in Figure 7. Different from our common under-\nstanding from vision perspective, the smaller network not\nonly trains faster but also generalizes better.\nNon-3D Representation. We compare our PointNet pre-\ntrained on DAM segmentation in Laptop task with follow-\ning\n2D\npre-training\nrepresentation\n:\nR3M\n[42].\nEncoder\nSeen\nUnseen\nPointNet\n0.78 \u00b1 0.04\n0.41 \u00b1 0.08\nResNet-18\n0.64 \u00b1 0.07\n0.28 \u00b1 0.05\nTable 3. Non-3D Representations.\nIn\nR3M,\nthe\nEgo4D human video\ndataset was used\nto pre-train a ResNet-\n18 with time-contrastive\nlearning and video-\nlanguage alignment. Table 3 shows the results of the exper-\niment. The results indicate that 3D visual representation\nlearning with PointNet is better at manipulating objects.\nCompared to Non-3D representation learning, 3D policies\ncan achieve better manipulation performance on both seen\nand unseen objects.\n5.3. Robustness to Viewpoint Change\nWe experiment with the viewpoint change of camera in\nLaptop task to evaluate the robustness of policy based on\nPointNet and ResNet-18. The PointNet policy is pre-trained\non DAM segmentation and the ResNet-18 policy is pre-\ntrained on R3M. The viewpoint sampling procedure can be\ndescribed as follow: (i) Determine a semi-sphere for camera\npose sampling. We first compute the radius r of the semi-\nsphere using the distance from the initial camera position\nto the manipulated object. The center of this semi-sphere is\ndefined by moving along the camera optical line with dis-\ntance r. (ii) Sample a point on the semi-sphere as camera\nposition. We uniformly sample the azimuthal angle in ev-\nery 20\u25e6 from \u221260\u25e6 to 60\u25e6 and polar angle in every 20\u25e6 from\n\u221220\u25e6 to 20\u25e6, relative to the training viewpoint. It results in\n7 \u00d7 5 = 35 camera positions in total. (iii) Rotate the cam-\nera so that it points to the semi-sphere center. Using the\nprocedure above, we sample 35 camera poses. We set these\ncamera poses during the inference.\nAs shown in Figure 9, the trained PointNet policy shows\ngreat robustness against viewpoint change, even though we\nchange the azimuthal angle by 60\u25e6 and the polar angle by\n20\u25e6. By contrast, the success rate of the ResNet-18 pol-\nicy suffers dramatically drop when the difference between\nthe training viewpoint and the novel evaluation viewpoint\nincreases. It informs us that the robustness mainly comes\nfrom point cloud representation learning and PointNet ar-\nchitecture.\ntheta\n\u221250\u221225 0\n25\n50\nphi\n\u221220\n\u221210\n0\n10\n20\nsuccess\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPre-trained PointNet(Seen)\ntheta\n\u221250\u221225 0\n25\n50\nphi\n\u221220\n\u221210\n0\n10\n20\nsuccess\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPre-trained PointNet(Unseen)\ntheta\n\u221250\u221225 0\n25\n50\nphi\n\u221220\n\u221210\n0\n10\n20\nsuccess\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPre-trained ResNet-18(Seen)\ntheta\n\u221250\u221225 0\n25\n50\nphi\n\u221220\n\u221210\n0\n10\n20\nsuccess\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPre-trained ResNet-18(Unseen)\nFigure 9. Success Rate under Different Viewpoints. The x-axis\nis the polar angle \u03d5 (relative to the training viewpoint) and the y-\naxis is the azimuthal angle \u03b8 on the semi-sphere centered at the\nobject. The z-axis represents the success rate. The viewpoint dur-\ning training is highlighted by a blue star.\n6. Conclusion\nWe propose a new benchmark for dexterous manipula-\ntion with articulated objects, and study the generalizability\nof the RL policy. We experiment and benchmark with dif-\nferent methods to provide several insights: (i) RL with more\ndiverse objects leads to better generalizability. We find that\ntraining with more objects leads to consistently better per-\nformance on unseen objects. (ii) Large encoders may not\nbe necessary for RL training to perform dexterous manip-\nulation tasks. We find that, in all environments, the sim-\nplest PointNet always leads better sample efficiency and\nbest generalizability. (iii) 3D visual understanding helps\npolicy learning. Part-segmentation facilities manipulation\nwith small functional parts while tasks with larger func-\ntional parts benefit from all visual pre-training methods. (iv)\nGeometric representation learning with PointNet feature ex-\ntractor brings strong robustness to the policy against cam-\nera viewpoint change. In conclusion, we hope DexArt can\nserve as a platform to study generalizable dexterous manip-\nulation, and the joint improvement between perception and\ndecision making.\nAcknowledgements. We thank Hao Su for providing valuable\nfeedback to the paper. This project was supported, in part, by the\nIndustrial Technology Innovation Program (20018112, Develop-\nment of autonomous manipulation and gripping technology using\nimitation learning based on visualtactile sensing) funded by the\nMinistry of Trade Industry and Energy of the Republic of Korea,\nAmazon Research Award and gifts from Qualcomm.\nReferences\n[1] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek\nChociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki,\nArthur Petron, Matthias Plappert, Glenn Powell, Alex Ray,\net al. Learning dexterous in-hand manipulation. The Inter-\nnational Journal of Robotics Research, 39(1):3\u201320, 2020. 1,\n2\n[2] Antonio Bicchi.\nOn the closure properties of robotic\ngrasping. The International Journal of Robotics Research,\n14(4):319\u2013334, 1995. 3\n[3] Antonio Bicchi and Vijay Kumar.\nRobotic grasping and\ncontact: A review.\nIn IEEE International Conference on\nRobotics and Automation, volume 1, pages 348\u2013353. IEEE,\n2000. 2\n[4] Antonio Bicchi and Vijay Kumar.\nRobotic grasping and\ncontact: A review. In Proceedings 2000 ICRA. Millennium\nconference. IEEE international conference on robotics and\nautomation. Symposia proceedings (Cat. No. 00CH37065),\nvolume 1, pages 348\u2013353. IEEE, 2000. 3\n[5] Jeannette Bohg, Antonio Morales, Tamim Asfour, and Dan-\nica Kragic. Data-driven grasp synthesis\u2014a survey. IEEE\nTransactions on robotics, 30(2):289\u2013309, 2013. 2\n[6] Samarth Brahmbhatt, Ankur Handa, James Hays, and Dieter\nFox. Contactgrasp: Functional multi-finger grasp synthesis\nfrom contact. In 2019 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 2386\u20132393.\nIEEE, 2019. 3\n[7] Yu-Wei Chao, Chris Paxton, Yu Xiang, Wei Yang, Bal-\nakumar Sundaralingam, Tao Chen, Adithyavairavan Murali,\nMaya Cakmak, and Dieter Fox. Handoversim: A simulation\nframework and benchmark for human-to-robot object han-\ndovers. In 2022 International Conference on Robotics and\nAutomation (ICRA), pages 6941\u20136947. IEEE, 2022. 2\n[8] Tao Chen, Jie Xu, and Pulkit Agrawal. A system for gen-\neral in-hand object re-orientation. In Conference on Robot\nLearning, pages 297\u2013307. PMLR, 2022. 1, 2, 3\n[9] Xinlei Chen and Kaiming He. Exploring simple siamese rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n15750\u201315758, 2021. 6\n[10] Zoey Qiuyu Chen, Karl Van Wyk, Yu-Wei Chao, Wei\nYang, Arsalan Mousavian, Abhishek Gupta, and Dieter Fox.\nDextransfer:\nReal world multi-fingered dexterous grasp-\ning with minimal human demonstrations.\narXiv preprint\narXiv:2209.14284, 2022. 2\n[11] Sachin Chitta, Benjamin Cohen, and Maxim Likhachev.\nPlanning for autonomous door opening with a mobile manip-\nulator. In 2010 IEEE International Conference on Robotics\nand Automation, pages 1799\u20131806. IEEE, 2010. 2\n[12] Sudeep Dasari, Jianren Wang, Joyce Hong, Shikhar Bahl,\nYixin Lin, Austin Wang, Abitha Thankaraj, Karanbir Cha-\nhal, Berk Calli, Saurabh Gupta, et al.\nRb2:\nRobotic\nmanipulation benchmarking with a twist.\narXiv preprint\narXiv:2203.08098, 2022. 2\n[13] Mehmet R Dogar and Siddhartha S Srinivasa. Push-grasping\nwith dexterous hands: Mechanics and a method. 2010. 2\n[14] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set\ngeneration network for 3d object reconstruction from a single\nimage. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 605\u2013613, 2017. 6\n[15] Samir Yitzhak Gadre, Kiana Ehsani, and Shuran Song. Act\nthe part: Learning interaction strategies for articulated object\npart discovery.\nIn Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 15752\u201315761,\n2021. 2, 3\n[16] Wei Gao and Russ Tedrake. kpam-sc: Generalizable manip-\nulation planning using keypoint affordance and shape com-\npletion. In 2021 IEEE International Conference on Robotics\nand Automation (ICRA), pages 6527\u20136533. IEEE, 2021. 3\n[17] Haoran Geng, Helin Xu, Chengyang Zhao, Chao Xu, Li Yi,\nSiyuan Huang, and He Wang.\nGapartnet: Cross-category\ndomain-generalizable object perception and manipulation\nvia generalizable and actionable parts, 2022. 5\n[18] Benjamin Graham, Martin Engelcke, and Laurens Van\nDer Maaten.\n3d semantic segmentation with submani-\nfold sparse convolutional networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 9224\u20139232, 2018. 3\n[19] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiaing\nLiu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yun-\nchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui\nChen, and Hao Su. Maniskill2: A unified benchmark for\ngeneralizable manipulation skills. In International Confer-\nence on Learning Representations, 2023. 2\n[20] Abhishek Gupta, Clemens Eppner, Sergey Levine, and Pieter\nAbbeel. Learning dexterous manipulation for a soft robotic\nhand from human demonstrations.\nIn IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Systems, pages\n3786\u20133793. IEEE, 2016. 2\n[21] Li Han and Jeffrey C Trinkle. Dexterous manipulation by\nrolling and finger gaiting.\nIn IEEE International Confer-\nence on Robotics and Automation, volume 1, pages 730\u2013735.\nIEEE, 1998. 2\n[22] Eric Heiden, Ziang Liu, Vibhav Vineet, Erwin Coumans, and\nGaurav S Sukhatme. Inferring articulated rigid body dynam-\nics from rgbd video. arXiv preprint arXiv:2203.10488, 2022.\n2\n[23] Wenlong Huang,\nIgor Mordatch,\nPieter Abbeel,\nand\nDeepak Pathak. Generalization in dexterous manipulation\nvia geometry-aware multi-task learning.\narXiv preprint\narXiv:2111.03062, 2021. 2, 3\n[24] Zhiwei Jia, Xuanlin Li, Zhan Ling, Shuang Liu, Yiran Wu,\nand Hao Su. Improving policy optimization with generalist-\nspecialist learning. In International Conference on Machine\nLearning, pages 10104\u201310119. PMLR, 2022. 2\n[25] Zhenyu Jiang, Cheng-Chun Hsu, and Yuke Zhu.\nDitto:\nBuilding digital twins of articulated objects from interaction.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 5616\u20135626, 2022. 2,\n3\n[26] Edward Johns.\nCoarse-to-fine imitation learning: Robot\nmanipulation from a single demonstration. In IEEE Inter-\nnational Conference on Robotics and Automation (ICRA),\npages 4613\u20134619. IEEE, 2021. 2\n[27] David Inkyu Kim and Gaurav S Sukhatme.\nSemantic la-\nbeling of 3d point clouds with object affordance for robot\nmanipulation.\nIn 2014 IEEE International Conference on\nRobotics and Automation (ICRA), pages 5578\u20135584. IEEE,\n2014. 3\n[28] Vikash Kumar, Emanuel Todorov, and Sergey Levine. Opti-\nmal control with learned local models: Application to dex-\nterous manipulation. In 2016 IEEE International Conference\non Robotics and Automation (ICRA), pages 378\u2013383. IEEE,\n2016. 2\n[29] Sergey Levine, Nolan Wagener, and Pieter Abbeel. Learning\ncontact-rich manipulation skills with guided policy search.\nIn IEEE International Conference on Robotics and Automa-\ntion, ICRA, pages 156\u2013163. IEEE, 2015. 1, 2\n[30] Xiaolong Li, He Wang, Li Yi, Leonidas J Guibas, A Lynn\nAbbott, and Shuran Song.\nCategory-level articulated ob-\nject pose estimation. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n3706\u20133715, 2020. 2\n[31] Hongzhuo Liang, Xiaojian Ma, Shuang Li, Michael G\u00a8orner,\nSong Tang, Bin Fang, Fuchun Sun, and Jianwei Zhang.\nPointnetgpd: Detecting grasp configurations from point sets.\nIn 2019 International Conference on Robotics and Automa-\ntion (ICRA), pages 3629\u20133635, 2019. 3\n[32] Minghua Liu, Xuanlin Li, Zhan Ling, Yangyan Li, and\nHao Su.\nFrame mining:\na free lunch for learning\nrobotic manipulation from 3d point clouds. arXiv preprint\narXiv:2210.07442, 2022. 2\n[33] Qihao Liu, Weichao Qiu, Weiyao Wang, Gregory D Hager,\nand Alan L Yuille. Nothing but geometric constraints: A\nmodel-free method for articulated object pose estimation.\narXiv preprint arXiv:2012.00088, 2020. 2\n[34] Ziyuan Liu, Wei Liu, Yuzhe Qin, Fanbo Xiang, Minghao\nGou, Songyan Xin, Maximo A Roa, Berk Calli, Hao Su, Yu\nSun, et al. Ocrtoc: A cloud-based competition and bench-\nmark for robotic grasping and manipulation. IEEE Robotics\nand Automation Letters, 7(1):486\u2013493, 2021. 2\n[35] Mayank Mittal, David Hoeller, Farbod Farshidian, Marco\nHutter, and Animesh Garg.\nArticulated object interaction\nin unknown scenes with whole-body mobile manipulation.\narXiv preprint arXiv:2103.10534, 2021. 3\n[36] Kaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhi-\nnav Gupta, and Shubham Tulsiani. Where2act: From pixels\nto actions for articulated 3d objects. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 6813\u20136823, 2021. 3\n[37] Kaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, and\nLeonidas Guibas. O2o-afford: Annotation-free large-scale\nobject-object affordance learning. In Conference on Robot\nLearning, pages 1666\u20131677. PMLR, 2022. 3\n[38] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna\nTripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-\nscale benchmark for fine-grained and hierarchical part-level\n3d object understanding. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 909\u2013918, 2019. 2, 3\n[39] Igor Mordatch, Zoran Popovi\u00b4c, and Emanuel Todorov.\nContact-invariant optimization for hand manipulation.\nIn\nProceedings of the ACM SIGGRAPH/Eurographics sympo-\nsium on computer animation, pages 137\u2013144, 2012. 2\n[40] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille,\nNuno Vasconcelos, and Xiaolong Wang. A-sdf: Learning\ndisentangled signed distance functions for articulated shape\nrepresentation.\nIn Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 13001\u201313011,\n2021. 2\n[41] Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xu-\nanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao\nSu.\nManiskill:\nGeneralizable manipulation skill bench-\nmark with large-scale demonstrations.\narXiv preprint\narXiv:2107.14483, 2021. 2, 3\n[42] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea\nFinn, and Abhinav Gupta. R3m: A universal visual repre-\nsentation for robot manipulation, 2022. 8\n[43] Domenico Prattichizzo and Jeffrey C Trinkle. Grasping. In\nSpringer handbook of robotics, pages 955\u2013988. Springer,\n2016. 2\n[44] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classification\nand segmentation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 652\u2013660,\n2017. 2, 3, 4\n[45] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. Advances in neural information\nprocessing systems, 30, 2017. 3\n[46] Yuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu, and\nHao Su. S4g: Amodal single-view single-shot se(3) grasp\ndetection in cluttered scenes.\nIn Proceedings of the Con-\nference on Robot Learning, volume 100 of Proceedings of\nMachine Learning Research, pages 53\u201365. PMLR, 2020. 3\n[47] Yuzhe Qin, Binghao Huang, Zhao-Heng Yin, Hao Su, and\nXiaolong Wang. Dexpoint: Generalizable point cloud rein-\nforcement learning for sim-to-real dexterous manipulation.\n2022. 4, 5\n[48] Yuzhe Qin, Hao Su, and Xiaolong Wang. From one hand to\nmultiple hands: Imitation learning for dexterous manipula-\ntion from single-camera teleoperation. IEEE Robotics and\nAutomation Letters, 7(4):10873\u201310881, 2022. 2\n[49] Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Hanwen Jiang, Rui-\nhan Yang, Yang Fu, and Xiaolong Wang. Dexmv: Imita-\ntion learning for dexterous manipulation from human videos.\narXiv preprint arXiv:2108.05877, 2021. 2\n[50] Ilija Radosavovic, Xiaolong Wang, Lerrel Pinto, and Jitendra\nMalik. State-only imitation learning for dexterous manipu-\nlation. In 2021 IEEE/RSJ International Conference on Intel-\nligent Robots and Systems (IROS), pages 7865\u20137871. IEEE,\n2020. 2\n[51] Daniela Rus. In-hand dexterous manipulation of piecewise-\nsmooth 3-d objects. The International Journal of Robotics\nResearch, 1999. 2\n[52] Andreas J Schmid, Nicolas Gorges, Dirk Goger, and Heinz\nWorn. Opening a door with a humanoid robot using multi-\nsensory tactile feedback. In 2008 IEEE International Con-\nference on Robotics and Automation, pages 285\u2013291. IEEE,\n2008. 2\n[53] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization algo-\nrithms. arXiv preprint arXiv:1707.06347, 2017. 4\n[54] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and\nMatt J Kusner. Unsupervised point cloud pre-training via oc-\nclusion completion. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 9782\u20139792,\n2021. 6\n[55] Lirui Wang, Yu Xiang, Wei Yang, Arsalan Mousavian, and\nDieter Fox. Goal-auxiliary actor-critic for 6d robotic grasp-\ning with point clouds. In Conference on Robot Learning,\npages 70\u201380. PMLR, 2022. 3\n[56] Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qin-\nping Zhao, and Kai Xu. Shape2motion: Joint analysis of\nmotion parts and attributes from 3d shapes. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 8876\u20138884, 2019. 2\n[57] Yijia Weng, He Wang, Qiang Zhou, Yuzhe Qin, Yueqi\nDuan, Qingnan Fan, Baoquan Chen, Hao Su, and Leonidas J\nGuibas. Captra: Category-level pose tracking for rigid and\narticulated objects from point clouds. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 13209\u201313218, 2021. 2\n[58] Albert Wu, Michelle Guo, and C Karen Liu.\nLearning\ndiverse and physically feasible dexterous grasps with gen-\nerative model and bilevel optimization.\narXiv preprint\narXiv:2207.00195, 2022. 2\n[59] Ruihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian\nWang, Tianhao Wu, Qingnan Fan, Xuelin Chen, Leonidas\nGuibas, and Hao Dong. Vat-mart: Learning visual action\ntrajectory proposals for manipulating 3d articulated objects.\narXiv preprint arXiv:2106.14440, 2021. 3\n[60] Yueh-Hua Wu, Jiashun Wang, and Xiaolong Wang. Learn-\ning generalizable dexterous manipulation from human grasp\naffordance. arXiv preprint arXiv:2204.02320, 2022. 3\n[61] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao\nZhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan,\nHe Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and\nHao Su. SAPIEN: A simulated part-based interactive envi-\nronment. In The IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), June 2020. 3, 4, 5\n[62] Zhenjia Xu, Zhanpeng He, and Shuran Song.\nUniversal\nmanipulation policy network for articulated objects. IEEE\nRobotics and Automation Letters, 7(2):2447\u20132454, 2022. 3\n[63] Jianglong Ye, Jiashun Wang, Binghao Huang, Yuzhe Qin,\nand Xiaolong Wang. Learning continuous grasping function\nwith a dexterous hand from human demonstrations. arXiv\npreprint arXiv:2207.05053, 2022. 2\n[64] Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao\nSu, and Leonidas Guibas. Deep part induction from articu-\nlated object pairs. arXiv preprint arXiv:1809.07417, 2018.\n2, 3\n[65] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,\nKarol Hausman, Chelsea Finn, and Sergey Levine. Meta-\nworld: A benchmark and evaluation for multi-task and meta\nreinforcement learning.\nIn Conference on robot learning,\npages 1094\u20131100. PMLR, 2020. 2\n[66] Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and\nMartial Hebert. Pcn: Point completion network. In 2018\nInternational Conference on 3D Vision (3DV), pages 728\u2013\n737, 2018. 6\n[67] Vicky Zeng, Tabitha Edith Lee, Jacky Liang, and Oliver\nKroemer.\nVisual identification of articulated object parts.\nIn 2021 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 2443\u20132450. IEEE, 2021.\n2\n[68] Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee, Xi\nChen, Ken Goldberg, and Pieter Abbeel.\nDeep imitation\nlearning for complex manipulation tasks from virtual real-\nity teleoperation. In 2018 IEEE International Conference on\nRobotics and Automation (ICRA), pages 5628\u20135635, 2018. 2\n[69] Henry Zhu, Abhishek Gupta, Aravind Rajeswaran, Sergey\nLevine, and Vikash Kumar.\nDexterous manipulation with\ndeep reinforcement learning: Efficient, general, and low-\ncost. In 2019 International Conference on Robotics and Au-\ntomation (ICRA), pages 3651\u20133657. IEEE, 2019. 1, 2\n"
  },
  {
    "title": "Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era",
    "link": "https://arxiv.org/pdf/2305.06131.pdf",
    "upvote": "1",
    "text": "Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era\nCHENGHAO LI, KAIST, South Korea\nCHAONING ZHANG\u2217, Kyung Hee University, South Korea\nATISH WAGHWASE, KAIST, South Korea\nLIK-HANG LEE, Hong Kong Polytechnic University, Hong Kong (China)\nFRANCOIS RAMEAU, State University of New York, Korea\nYANG YANG, University of Electronic Science and technology, China\nSUNG-HO BAE, Kyung Hee University, South Korea\nCHOONG SEON HONG, Kyung Hee University, South Korea\nGenerative AI (AIGC, a.k.a. AI generated content) has made remarkable\nprogress in the past few years, among which text-guided content generation\nis the most practical one since it enables the interaction between human\ninstruction and AIGC. Due to the development in text-to-image as well 3D\nmodeling technologies (like NeRF), text-to-3D has become a newly emerging\nyet highly active research field. Our work conducts the first yet comprehen-\nsive survey on text-to-3D to help readers interested in this direction quickly\ncatch up with its fast development. First, we introduce 3D data representa-\ntions, including both Euclidean data and non-Euclidean data. On top of that,\nwe introduce various foundation technologies as well as summarize how\nrecent works combine those foundation technologies to realize satisfactory\ntext-to-3D. Moreover, we summarize how text-to-3D technology is used in\nvarious applications, including avatar generation, texture generation, shape\ntransformation, and scene generation.\nCCS Concepts: \u2022 Computing methodologies \u2192 Reconstruction; Shape\nmodeling.\nAdditional Key Words and Phrases: text-to-3D, generative AI, AIGC, 3D\ngeneration, metaverse\nACM Reference Format:\nChenghao Li, Chaoning Zhang, Atish Waghwase, Lik-Hang Lee, Francois\nRameau, Yang Yang, Sung-Ho Bae, and Choong Seon Hong. 2023. Generative\nAI meets 3D: A Survey on Text-to-3D in AIGC Era. 1, 1 (May 2023), 14 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n\u2217Corresponding author: chaoningzhang1990@gmail.com\nAuthors\u2019 addresses: Chenghao Li, KAIST, South Korea, lch17692405449@gmail.com;\nChaoning Zhang, Kyung Hee University, South Korea, chaoningzhang1990@gmail.com;\nAtish Waghwase, KAIST, South Korea, atishwaghwase@gmail.com; Lik-Hang Lee,\nHong Kong Polytechnic University, Hong Kong (China), lik-hang.lee@polyu.edu.hk;\nFrancois Rameau, State University of New York, Korea, lik-hang.lee@polyu.edu.hk;\nYang Yang, University of Electronic Science and technology, China, dlyyang@gmail.\ncom; Sung-Ho Bae, Kyung Hee University, South Korea, shbae@khu.ac.kr; Choong\nSeon Hong, Kyung Hee University, South Korea, cshong@khu.ac.kr.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\n\u00a9 2023 Association for Computing Machinery.\nXXXX-XXXX/2023/5-ART $15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nContents\nAbstract\n1\nContents\n1\n1\nIntroduction\n1\n2\n3D Data Representation\n2\n2.1\nEuclidean data\n2\n2.2\nnon-Euclidean data\n3\n3\nText-to-3D Technologies\n4\n3.1\nFoundation Technologies\n4\n3.2\nSuccessful attempts\n6\n4\nText-to-3D applications\n9\n4.1\nText Guided 3D Avatar Generation\n9\n4.2\nText Guided 3D Texture Generation\n9\n4.3\nText Guided 3D Scene Generation\n10\n4.4\nText Guided 3D Shape Transformation\n11\n5\nDiscussion\n11\n5.1\nFidelity\n11\n5.2\nInference velocity\n12\n5.3\nConsistency\n12\n5.4\nControllability\n12\n5.5\nApplicability\n12\n6\nConclusion\n12\nReferences\n12\n1\nINTRODUCTION\nGenerative Artificial Intelligence as the main body generating high-\nquality and quantity content (also known as Artificial Intelligence\nGenerated Content-AIGC) has aroused great attention in the past\nfew years. The content generation paradigm guided and restrained\nby natural language, such as text-to-text (e.g. ChatGPT [Zhang\net al. 2023c]) and text-to-image [Zhang et al. 2023d] (e.g. DALLE-\n2 [Ramesh et al. 2022]), is the most practical one, as it allows for a sim-\nple interaction between human guidance and generative AI [Zhang\net al. 2023e]. The accomplishment of Generative AI in the field of\ntext-to-image [Zhang et al. 2023d] is quite remarkable. As we are in\na 3D world, it is necessary to extend AIGC to 3D domain. There is a\ngreat demand for 3D digital content in many application scenarios,\nincluding games, movies, virtual reality, architecture and robots,\nsuch as 3D character generation, 3D texture generation, 3D scene\ngeneration, etc. However, it requires a lot of artistic and aesthetic\n, Vol. 1, No. 1, Article . Publication date: May 2023.\narXiv:2305.06131v2  [cs.CV]  27 May 2023\n2\n\u2022\nLi and Zhang, et al.\ntraining, as well as professional knowledge in 3D modeling, to cul-\ntivate a professional 3D modeler. Given the current trend of 3D\nmodel development, it is essential to utilize generative AI to gener-\nate high-quality and large-scale 3D models. In addition, text-to-3D\nAI modeling can greatly assist both newbies and professionals to\nrealize free creation of 3D contents.\nPrevious methods of text-to-3D shapes have attempted to learn a\ncross-modal mapping by directly learning from text-3D pairs [Achliop-\ntas et al. 2019; Chen et al. 2019] and generate joint representations.\nCompared to text-to-image, the task of generating 3D shapes from\ntext is more challenging. Firstly, unlike 2D images, 3D shapes are\nmostly unstructured and irregular non-Euclidean data, making it\ndifficult to apply traditional 2D deep learning models to these data.\nMoreover, there are a large number of large-scale image-text pairs\ndatasets available online to support text-to-image generation. How-\never, to our knowledge, the largest text-to-3D dataset proposed\nin [Fu et al. 2022] contained only 369K text-3D pairs and is lim-\nited to a few object categories. This is significantly lower than the\ndatasets which contain 5.85B text-image pairs [Schuhmann et al.\n2022]. The lack of large-scale and high-quality training data makes\nthe task of text-to-3D even more difficult.\nRecently, the advent of some key technologies has enabled a\nnew paradigm of text-to-3D tasks. Firstly, Neural Radiance Fields\n(NeRF) [Mildenhall et al. 2021] is an emergent 3D data representa-\ntion approach. Initially, NeRF was found to perform well in the 3D\nreconstruction task, and recently NeRF and other neural 3D repre-\nsentations have been applied to new view synthesis tasks that can\nuse real-world RGB photos. NeRF is trained to reconstruct images\nfrom multiple viewpoints. As the learned radiance fields are shared\nbetween viewpoints, NeRF can smoothly and consistently interpo-\nlate between viewpoints. Due to its neural representation, NeRF can\nsample with high spatial resolution, unlike voxel representations\nand point clouds, and is easier to optimize than meshes and other\nexplicit geometric representations, since it is topology-free. The\nadvent of NeRF breaks the stalemate of 3D data scarcity and serves\nas a soft bridge from 2D to 3D representation, elegantly solving the\nproblem of 3D data scarcity. Secondly, with the remarkable progress\nof multimodal AI [Radford et al. 2021] and diffusion models [Ho\net al. 2020], text-guided image content AI generation has made sig-\nnificant progress. The key driving factor is the large-scale datasets\nof billions of text-image pairs obtained from the Internet. Recent\nworks have emerged that guide 3D modeling optimization by lever-\naging the prior of a pre-trained text-to-image generation model.\nIn other words, they often text-guided 3D model generation with\ntext-to-image priors.\nOverall, this work conducts the first yet comprehensive survey\non text-to-3D. The rest of this work is organized as follows. Sec 2\nreviews the different representations of 3D. Sec 3 first introduces\nthe technology behind text-to-3D, and then summarizes the recent\npapers. Sec 4 introduces the application of text-to-3D in various\nfields.\n2\n3D DATA REPRESENTATION\n3D data can have different representations [Ahmed et al. 2018],\ndivided into Euclidean and non-Euclidean. 3D Euclidean data has\na potential grid structure, which allows global parameterization\nand a common coordinate system. These properties make extending\nexisting 2D deep learning paradigms to 3D data a simple task, where\nconvolution operations remain the same as 2D. On the other hand,\n3D non-Euclidean data does not have a grid array structure and\nis not globally parameterized. Therefore, extending classical deep\nlearning techniques to such representations is a challenging task. In\nreal life, the research of deep learning techniques in non-Euclidean\ndomains is of great significance. This is referred to as geometric\ndeep learning [Cao et al. 2020].\n2.1\nEuclidean data\nThe Euclidean data preserves the attribute of the grid structure,\nwith global parameterization and a common coordinate system. The\nmajor 3D data representations in this category include voxel grids\nand multi-view images.\nFig. 1. Voxel representation of Stanford bunny, the picture is obtained\nfrom [Shi et al. 2022].\n2.1.1\nVoxel Grids. Voxels can be used to represent individual sam-\nples or data points on a regularly spaced three-dimensional grid,\nwhich is a Euclidean structured data structure similar to pixels [Blinn\n2005] in 2D space. The data point can contain a single data, such\nas opacity, or multiple data, such as color and opacity. Voxels can\nalso store high-dimensional feature vectors in data points, such\nas geometric occupancy [Mescheder et al. 2019], volumetric den-\nsity [Minto et al. 2018], or signed distance values [Park et al. 2019].\nA voxel only represents a point on this grid, not a volume; the space\nbetween voxels is not represented in the voxel-based dataset. De-\npending on the data type and the expected use of the dataset, this\nlost information can be reconstructed and/or approximated, for ex-\nample, by interpolation. The representation of voxels is simple and\nthe spatial structure is clear, which is highly extensible and can be\neasily applied to convolutional neural networks [Wang et al. 2019].\nHowever, the efficiency is low, as it represents both the occupied\nparts of the scene and the unoccupied parts, which leads to a large\namount of unnecessary storage requirements. This leads to voxels\nbeing unsuitable for representing high-resolution data. Voxel grids\nhave many applications in rendering tasks[Hu et al. 2023; Rematas\nand Ferrari 2020]. Early methods store high-dimensional feature vec-\ntors in voxels to encode the geometry and appearance of the scene,\nusually referred to as a feature volume, which can be interpreted\nas a color image using projection and 2D cellular neural networks.\nThis also includes volumetric imaging in medicine, as well as terrain\nrepresentation in games and simulations.\n2.1.2\nMulti-view Images. With the development of computer vision\ntechnology and the remarkable improvement in computing power,\n, Vol. 1, No. 1, Article . Publication date: May 2023.\nGenerative AI meets 3D: A Survey on Text-to-3D in AIGC Era\n\u2022\n3\nFig. 2. Multi-view representation of Stanford bunny, the picture is obtained\nfrom [Park et al. 2016].\ncoupled with the latest developments in modern digital cameras, it\nis now possible to easily capture large amounts of high-resolution\nimages. There is an urgent need to extract 3D structure from these\nimages for many applications, such as 3D reconstruction [Jin et al.\n2020]. A multi-view image dataset is an aggregation of multiple\nimages, each representing an object or scene from different per-\nspectives, such as the front, side and top, aggregated together to\nform a multi-view image dataset. Since collecting 3D data from\nthe real world is time-consuming, and deep learning paradigms\nrely on large amounts of data for training, the availability of multi-\nview image datasets is its greatest advantage. Its drawback is that\nmulti-view images cannot be strictly defined as 3D model data, but\nit provides a bridge between 2D and 3D visualizations. Recently,\nNeRF [Mildenhall et al. 2021] has emerged as a novel approach for\n3D reconstruction, which is well-suited for the massive data re-\nquirement of the learning-based generalizable NeRF methods with\nlarge-scale multi-view datasets [Yu et al. 2023]. It can also be applica-\nble to multi-view stereo [Furukawa et al. 2015] and view-consistent\nimage understanding [Dong et al. 2022] tasks.\n2.2\nnon-Euclidean data\nThe second type of 3D data representation is non-Euclidean data.\nThis type of data does not have global parametrization or common\ncoordinate systems, which makes it difficult to extend 2D deep\nlearning paradigms. Much effort has been made in learning this data\nrepresentation and applying DL techniques. Researching deep learn-\ning techniques in non-Euclidean domains is of great importance,\nthis is referred to as Geometric Deep Learning [Cao et al. 2020]. The\nmain types of non-Euclidean data are point clouds, 3D meshes and\nimplicit.\nFig. 3. Mesh representation of Stanford bunny, the picture is obtained\nfrom [Rossi et al. 2021].\n2.2.1\nMeshes. 3D meshes [Wang and Zhang 2022] are one of the\nmost popular representations of 3D shapes. A 3D mesh structure is\ncomposed of a set of polygons, termed faces, which are described\nbased on a set of vertices that describe the presence of coordinates in\n3D space. These vertices are associated with a connectivity list that\ndescribes how these vertices are interconnected. As meshes only\nmodel the surface of the scene, they are more compact. The meshes\nprovide connectivity of surface points for modeling point relation-\nships. Due to these advantages, polygon meshes are widely used\nin traditional computer graphics [Zhou et al. 2021b] applications\nsuch as geometry processing, animation, and rendering. However,\non a global level, meshes are non-Euclidean data, and the local ge-\nometry of the mesh can be represented as a subset of the Euclidean\nspace where the known properties of the Euclidean space are not\nwell-defined, such as shift-invariance, vector space operations, and\nglobal parameterization systems. Thus, deep learning of 3D meshes\nis a challenging task. However, with the development of graph\nneural networks [Wu et al. 2020], meshes can be seen as graphs.\nMeshCNN [Hanocka et al. 2019] specifically designs convolutional\nand pooling layers for mesh edges and extracts edge features for\nshape analysis. 3D meshes are important in many fields and indus-\ntries, such as architecture and building, furniture and home living,\ngaming and entertainment, product design, medical and life sciences,\netc. They can be used for designing, visualizing, analyzing archi-\ntecture and products, creating character objects for games, movies,\netc., designing new products, visualizing and analyzing anatomical\nstructures, and helping to increase understanding of diseases and\ntreatment methods.\nFig. 4. Point cloud representation of Stanford bunny model, the figure is\nobtained from [Agarwal and Prabhakaran 2009].\n2.2.2\nPoint Clouds. With the trend of inexpensive and convenient\npoint cloud acquisition equipment, point clouds have been widely\nused in modeling and rendering, augmented reality, autonomous\nvehicles, etc [Guo et al. 2020]. Point clouds are a disordered set of\ndiscrete samples of three-dimensional shapes on three-dimensional\nspace. Traditionally, point clouds are non-Euclidean data because\npoint cloud data is non-structured globally. However, point clouds\ncan also be realized as a set of globally parametrized small Eu-\nclidean subsets. The definition of point cloud structure depends\non whether to consider the global or local structure of the object.\nSince most applications strive to capture the global characteristics\nof the object to perform complex tasks, traditionally point clouds\nare non-Euclidean data. Point clouds are the direct output of depth\nsensors [Liu et al. 2019] and therefore are very popular in 3D scene\nunderstanding tasks. Despite being easily obtainable, the irregu-\nlarity of point clouds makes them hard to process with traditional\n2D neural networks. Numerous geometric deep learning [Cao et al.\n2020] methods have been proposed to effectively analyze three-\ndimensional point clouds, such as PointNet [Qi et al. 2017], which\nis a deep learning network structure based on raw point cloud data\n, Vol. 1, No. 1, Article . Publication date: May 2023.\n4\n\u2022\nLi and Zhang, et al.\nand can directly use raw point cloud data as input and use a set\nof sparse keypoints to summarize the input point cloud, and can\neffectively process data and have robustness to small perturbations\nof the input and can achieve good performance in tasks such as\nshape classification, part segmentation, and scene segmentation. 3D\npoint cloud technology can be applied to multiple fields such as\narchitecture, engineering, civil building design, geological survey,\nmachine vision, agriculture, space information, and automatic driv-\ning, and can provide more accurate modeling and analysis, as well\nas more accurate positioning and tracking.\nFig. 5. Neural field representation of Stanford bunny, the picture is obtained\nfrom [Park et al. 2019].\n2.2.3\nNeural Fields. Neural fields are a domain that is either wholly\nor partially parameterized by neural networks and represented en-\ntirely or partially by neural networks for scenes or objects in 3D\nspace.At each point in 3D space, a neural network can map its as-\nsociated characteristics to attributes. Neural fields are capable of\nrepresenting 3D scenes or objects in any resolution and unknown or\ncomplex topology due to their continuous representation. Addition-\nally, compared to the above representations, only the parameters of\nthe neural network need to be stored, resulting in lower memory con-\nsumption than other representations. The earliest work on neural\nfields was used for 3D shape representation [Peng and Shamsuddin\n2004]. SDF [Park et al. 2019] is a classical approach to represent 3D\nshapes as neural fields. SDF is based on a continuous volumetric\nfield, represented by the distance and sign at each point on surface.\nSeveral works [Gao et al. 2022b; Shen et al. 2021] use SDF to gen-\nerate 3D shapes as representation. NeRF [Mildenhall et al. 2021],\na recent emerging representation for 3D reconstruction in neural\nfields, has the advantages of high-quality and realistic 3D model\ngeneration, presenting realistic object surfaces and texture details\nat any angle and distance. Furthermore, it can generate 3D models\nfrom any number of input images without specific processing or\nlabeling of the inputs. Another advantage of neural fields is that\nthe neural network can be operated on low-power devices after it is\ntrained. Polygon ray tracing renders high-resolution and realistic\nscenes at high frame rates, which requires expensive graphic cards,\nbut high-quality neural fields can be rendered on mobile phones\nand even web browsers. However, there are also some drawbacks\nof neural field technology, such as the need for a large amount of\ncomputational resources and time for training, as well as difficulty\nin handling large-scale scenes and complex lighting conditions, and\nits inability to be structured data, which makes it difficult to be\ndirectly applied to 3D assets. Neural fields are a new emerging 3D\nrepresentation technology with strong application prospects and\ncan be used in 3D fields such as VR/AR and games.\n3\nTEXT-TO-3D TECHNOLOGIES\nIn the past few years, the success of deep generative models [Ho et al.\n2020] in 2D images has been incredible. Training generative models\nin 2D space cannot meet the needs of some practical applications,\nas our physical world actually operates in 3D space. 3D data gener-\nation is of paramount importance. The success of Neural Radiance\nFields [Mildenhall et al. 2021] has transformed the 3D reconstruc-\ntion race, bringing 3D data to a whole new level. Combining prior\nknowledge from text-to-image models [Ramesh et al. 2021], many\npioneers have achieved remarkable results in text-to-3D generation.\nIn this section, we will first review the key techniques underlying\ntext-to-3D generation. Secondly, we will survey recent text-to-3D\nmodels.\n3.1\nFoundation Technologies\nFig. 6. An overview of our neural radiance field scene representation and\ndifferentiable rendering procedure, the picture is obtained from [Mildenhall\net al. 2021].\n3.1.1\nNeRF. Neural Rediance Field (NeRF) [Gao et al. 2022a; Milden-\nhall et al. 2021] is a neural network-based implicit representation of\n3D scenes, which can render projection images from a given view-\npoint and a given position. Specifically, given a 3D point x \u2208 R3\nand an observation direction unit vector d \u2208 R2, NeRF encodes\nthe scene as a continuous volumetric radiance field \ud835\udc53 , yielding a\ndifferential density \ud835\udf0e and an RGB color c: \ud835\udc53 (x, d) = (\ud835\udf0e, c).\nRendering of images from desired perspectives can be achieved by\nintegrating color along a suitable ray, r, for each pixel in accordance\nwith the volume rendering equation:\n\u02c6C(\ud835\udc5f) =\n\u222b \ud835\udc61\ud835\udc53\n\ud835\udc61\ud835\udc5b\n\ud835\udc47 (\ud835\udc61)\ud835\udf0e(\ud835\udc61)c(\ud835\udc61)\ud835\udc51\ud835\udc61\n(1)\n\ud835\udc47 (\ud835\udc61) = \ud835\udc52\ud835\udc65\ud835\udc5d\n\u0012\n\u2212\n\u222b \ud835\udc61\n\ud835\udc61\ud835\udc5b\n\ud835\udf0e(\ud835\udc60)\ud835\udc51\ud835\udc60\n\u0013\n(2)\nThe transmission coefficient \ud835\udc47 (\ud835\udc61) is defined as the probability\nthat light is not absorbed from the near-field boundary \ud835\udc61\ud835\udc5b to \ud835\udc61.\nIn order to train NeRF network and optimize the predicted color\n\u02c6C to fit with the ray R corresponding to the pixel in the training\nimages, gradient descent is used to optimize the network and match\nthe target pixel color by loss:\nL =\n\u2211\ufe01\nr\u2208R\n\u2225C(r) \u2212 \u02c6C(r)\u22252\n2\n(3)\n, Vol. 1, No. 1, Article . Publication date: May 2023.\nGenerative AI meets 3D: A Survey on Text-to-3D in AIGC Era\n\u2022\n5\n3.1.2\nCLIP. Recent advances in multimodal learning have enabled\nthe development of cross-modal matching models such as CLIP [Rad-\nford et al. 2021](Contrastive Language-Image Pre-training) which\nlearn shared representations from image-text pairs. These models\nare able to produce a scalar score that indicates whether an image\nand its associated caption match or not.\nFig. 7. CLIP structure, picture obtained from [Radford et al. 2021].\nIn training, the standard image model is used to jointly train the\nimage feature extractor and linear classifier to predict some labels;\nCLIP jointly trains the image encoder and text encoder to predict\nthe correct pairings of a batch of (image, text) training samples.\nThe symmetric InfoNCE loss is used to train the image and text\nencoders, which can then be used for a number of downstream tasks.\nIn testing, the learned text encoder synthesizes a zero-shot linear\nclassifier by embedding the names or descriptions of the target\ndataset categories. Building on this, a volume has been optimized\nto produce a high-scoring image rather than reranking. The CLIP\nstructure is shown in Figure 7.\n3.1.3\nDiffusion model. In the past few years, the use of diffusion\nmodel [Ho et al. 2020] has seen a dramatic increase. Also known as\ndenoising diffusion probabilistic models (DDPMs) or score-based\ngenerative models, these models generate new data that is simi-\nlar to the data used to train them. Drawing inspiration from non-\nequilibrium thermodynamics, DDPMs are defined as a parameter-\nized Markov chain of diffusion steps that adds random noise to the\ntraining data and learns to reverse the diffusion process to produce\nthe desired data samples from the pure noise.\nIn the forward process, DDPM destroys the training data by grad-\nually adding Gaussian noise. It starts from a data sample x0 and\niteratively generates noisier samples x\ud835\udc47 with \ud835\udc5e(x\ud835\udc61 | x\ud835\udc61\u22121), using a\nGaussian diffusion kernel:\n\ud835\udc5e(\ud835\udc651:\ud835\udc47 |\ud835\udc650) :=\n\ud835\udc47\n\u00d6\n\ud835\udc61=1\n\ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61\u22121),\n(4)\n\ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61\u22121) := N (\ud835\udc65\ud835\udc61;\n\u221a\ufe01\n1 \u2212 \ud835\udefd\ud835\udc61\ud835\udc65\ud835\udc61\u22121, \ud835\udefd\ud835\udc61\ud835\udc3c)\n(5)\nwhere \ud835\udc47 and \ud835\udefd\ud835\udc61 are the steps and hyper-parameters, respectively.\nWe can obtain noised image at arbitrary step \ud835\udc61 with Gaussian noise\ntransition kernel as N in Eq. 5, by setting \ud835\udefc\ud835\udc61 := 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 :=\n\u00ce\ud835\udc61\n\ud835\udc60=0 \ud835\udefc\ud835\udc60:\n\ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc650) := N (\ud835\udc65\ud835\udc61; \u221a \u00af\ud835\udefc\ud835\udc61\ud835\udc650, (1 \u2212 \u00af\ud835\udefc\ud835\udc61)\ud835\udc3c)\n(6)\nThe reverse denoising process of DDPM involves learning to undo\nthe forward diffusion by performing iterative denoising, thereby\ngenerating data from random noise. This process is formally de-\nfined as a stochastic process, where the optimization objective is to\ngenerate \ud835\udc5d\ud835\udf03 (\ud835\udc650) which follows the true data distribution \ud835\udc5e(\ud835\udc650) by\nstarting from \ud835\udc5d\ud835\udf03 (\ud835\udc47):\n\ud835\udc38\ud835\udc61\u223cU(1,\ud835\udc47 ),x0\u223c\ud835\udc5e(x0),\ud835\udf16\u223cN(0,I)\ud835\udf06(\ud835\udc61) \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 (x\ud835\udc61,\ud835\udc61)\u22252\n(7)\nFig. 8. Overview of DDPM, the picture is obtained from [Ho et al. 2020]\n3.1.4\nPretrained text-guided image generation model. Recently, with\nthe emergence of diffusion models, pre-trained models for text-to-\nimage generation based on diffusion models have become good\npriors for text-to-3D tasks [Zhang et al. 2023d]. The pioneering\nworks into image framework can be roughly categorized according\nto the diffusion priors in the pixel space or latent space. The first\nclass of methods directly generates images from high-dimensional\npixel-level, including GLIDE [Nichol et al. 2021] and Imagen [Saharia\net al. 2022]. Another approach suggests compressing the image to\na low-dimensional space first, and then training a diffusion model\nin this latent space. Representative methods of latent space include\nStable Diffusion [Rombach et al. 2022] and DALLE-2 [Ramesh et al.\n2022].\nThe training process of text-to-image generation can be roughly\ndivided into three steps. Firstly, the CLIP model is employed to learn\nthe correlations between text and visual semantics and map the text\ninto the image representation space. Secondly, a prior model is used\nfor inversion to map the image representation space back to the text\nspace, and thus generate text-conditional images. Lastly, diffusion\nmodels are utilized to learn the mapping between text encoding and\nimage encoding, providing prior models for text-conditional image\ngeneration. The structure of DALLE-2 is shown in Figure 9\nFig. 9. Structure of DALLE-2, picture obtained from [Ramesh et al. 2022].\n, Vol. 1, No. 1, Article . Publication date: May 2023.\n6\n\u2022\nLi and Zhang, et al.\n3.2\nSuccessful attempts\nRecent pioneering studies have demonstrated the utility of pre-\ntrained text-to-image diffusion models to optimize neural radiance\nfields, achieving significant text-to-3D synthesis results. However,\nthis paradigm also has some issues, and some followers aim to solve\nthese problems. In this section, the first part presents the pioneers\nof this paradigm, and the second part presents some enhancement\nworks of this paradigm.\nFig. 10. DreamFusion outcome. Picture obtained from [Poole et al. 2022].\n3.2.1\nPoinners of CLIP-Based Text-Guided 3D Shape Generation.\nIn recent years, with the success of text-to-image generation mod-\nelling [Zhang et al. 2023d], text-to-3D generation has also attracted\nthe attention of the deep learning community [Han et al. 2023; Jain\net al. 2022; Lin et al. 2022; Mohammad Khalid et al. 2022; Poole et al.\n2022; Wang et al. 2022b; Xu et al. 2022b]. However, the scarcity of\n3D data makes expansion with data challenging. DreamField [Jain\net al. 2022] and CLIP-Mesh [Mohammad Khalid et al. 2022] rely\non pre-trained image-text models [Radford et al. 2021] to optimize\nunderlying 3D representations (RMS and meshes) in order to alle-\nviate the training data problem, achieving high text-image align-\nment scores for all 2D renderings. Although these methods avoid\nthe costly requirement of 3D training data and primarily rely on\na large-scale pre-trained image-text model, they often yield less\nrealistic 2D renderings. Recently, DreamFusion [Poole et al. 2022]\nand Magic3D [Lin et al. 2022] has demonstrated impressive capabil-\nities in text-to-3D synthesis by leveraging a powerful pre-trained\ntext-to-image diffusion model as a strong image prior.\nCLIP-Forge [Sanghi et al. 2022] was the first to attempt to apply\na pre-trained text-to-image model to 3D generation and was suc-\ncessful. CLIP-Forge is a novel approach to text-to-shape generation\nthat involves no paired text-shape labels. It uses a pre-trained text\nencoder and an autoencoder to obtain a latent space for shapes. A\nnormalizing flow network is then conditioned with text features to\ngenerate a shape embedding which is then converted into 3D shape.\nCLIP-Forge has an efficient generation process which requires no\ninference time optimization and can generate multiple shapes for\na given text. See Figure 11 for details. This method also has the\nadvantage of avoiding the expensive inference time optimizations\nas employed in existing text-to-shape generation models. Extensive\nevaluation of the method in various zero-shot generation settings is\nprovided, both qualitatively and quantitatively.\nFig. 11. Illustration of the main idea of CLIP-Forge, the picture obtained\nfrom [Sanghi et al. 2022].\nDream Fields [Jain et al. 2022] is a concurrent work to CLIP-\nForge [Sanghi et al. 2022], which uses CLIP to synthesize and ma-\nnipulate 3D object representations. However, the work differ from\nDream Fields in terms of features, resolutions, and limitiations. CLIP-\nForge generalizes poorly outside of ShapeNet categories and re-\nquires ground-truth multi-view images and voxel data. Dream Fields\nproposes a method for synthesizing diverse three-dimensional ob-\njects based on natural language descriptions. This method combines\nneural rendering and multi-modal image and text representations,\ngenerating various categories of geometry and colors using an opti-\nmized multi-viewpoint neural radiation field without the need of\nthree-dimensional supervision. To enhance the realism and visual\nquality, Dream Fields introduces simple geometric priors includ-\ning sparsity-induced transmission regularization, scene boundaries\nand a new MLP architecture. See Figure 12 for details.Experimental\nresults show that Dream Fields can generate realistic and multi-\nviewpoint consistent geometry and colors from various natural\nlanguage descriptions.\nFig. 12. Structure of Dream Fields, picture obtained from [Jain et al. 2022]\nCLIP-NeRF [Wang et al. 2022a] is a novel disentangled conditional\nNeRF architecture that offers flexible control for editing NeRFs based\non text prompts or reference images. It introduces a shape code and\nan appearance code to independently control the deformation of\nthe volumetric field and the emitted colors. Two code mappers, fed\nby the pre-trained CLIP model, enable fast inference for editing\ndifferent objects in the same category compared to the optimization-\nbased editing method. In addition, an inversion method is proposed\nto infer the shape and appearance codes from a real image, enabling\n, Vol. 1, No. 1, Article . Publication date: May 2023.\nGenerative AI meets 3D: A Survey on Text-to-3D in AIGC Era\n\u2022\n7\nusers to edit the shape and appearance of existing data. CLIP-NeRF\nis a contemporary work of Dream Fields [Jain et al. 2022], and\nunlike Dream Fields, the former offers greater freedom in shape\nmanipulation and supports global deformation, introducing two\nintuitive NeRF editing methods: using short text prompts or sample\nimages, both of which are more user-friendly to novice users. The\nstructure of CLIP-NeRF is shown in Figure ??\nFig. 13. Sturcute of CLIP-NeRF, picture obtained from [Wang et al. 2022a]\nDreamFusion [Poole et al. 2022] employs a similar approach to\nDream Fields [Jain et al. 2022] to train NeRF, using a frozen image-\ntext joint embedding model from CLIP and an optimization-based\napproach, but replacing CLIP with a 2D Diffusion Model distilled\nloss. The DreamFusion architecture is shown in the Figure 14, where\nthe scene is represented by a neural radiance field initialized and\ntrained from scratch for each descriptive. With a pretrained text-to-\nimage Diffusion Model, an image parameterized in the form of NeRF,\nand a loss function minimized towards good samples, DreamFusion\nhas all the necessary components for text-to-3D synthesis without\nthe use of 3D data. For each textual prompt, DreamFusion starts\nits training from scratch with a randomly initialized NeRF. Each\niteration of DreamFusion\u2019s optimization performs the same step. For\neach optimization step, DreamFusion performs random sampling\nof camera and light sources, rendering of NeRF image from the\ncamera with light occlusion, computation of SDS loss gradients\nrelative to NeRF parameters, and updating of NeRF parameters with\nan optimizer. By combining SDS with a NeRF variant tailored to\nthis 3D generation task, DreamFusion maximizes the fidelity and\ncoherence of 3D generated shapes.\nFig. 14. Structure of DreamFusion, picture obtained from [Poole et al. 2022].\nMagic3D [Lin et al. 2022] is a framework for high-quality 3D\ncontent composition with textual prompts, which optimizes the\ngeneration process by improving several major design choices from\nDreamFusion [Poole et al. 2022]. In DreamFusion, the monitoring\nsignal runs on very low-resolution images of 64 \u00d7 64, and DreamFu-\nsion is unable to synthesize high-frequency 3D geometry and texture\ndetails. Because of its inefficient MLP architecture to represent NeRF,\nthe actual high-resolution synthesis may even be impossible due to\nthe rapid growth of memory usage and computation budget as the\nresolution increases. Magic3D proposes a two-stage optimization\nframework for optimizing from text to 3d synthesis results of NeRF.\nIn the first stage, Magic3D optimizes a coarse neural field represen-\ntation similar to DreamFusion, but with a hash grid-based memory\nand computation-efficient scene representation. In the second stage,\nMagic3D shifts to optimize the mesh representation, leveraging a\ndiffusion prior at resolutions up to 512 \u00d7 512. Overview of Magic3D\nas shown in Figure 15. As 3D meshes fit well with fast graphics ren-\ndering solutions that can render high-resolution images in real-time,\nMagic3D also uses an efficient differentiable rasterizer to recover\nthe high-frequency details in geometry and texture from camera\nclose-ups. Magic3D synthesizes 3D contents with 8 times better\nresolution and 2 times faster speed than DreamFusion.\nFig. 15. Overview of Magic3D, picture obtained from [Lin et al. 2022].\nApart from the works showcased above, many other excellent\ncontemporaneous works have served as pioneers in pioneering the\ntext-to-3D generation. CLIP-Mesh [Mohammad Khalid et al. 2022]\nproposes a technique that can achieve zero-shot 3D model gener-\nation only using a target textual prompt. The motivation is that,\nwithout any 3D supervision, 3D resources corresponding to the\ninput textual prompt can be obtained by adjusting the control shape\nof the restricted subdivision surface as well as its texture and normal\nmaps, and can be easily deployed in games or modeling applica-\ntions. SJC (Score Jacobian Chaining) [Wang et al. 2022b] proposes a\nmethod to promote 2D diffusion models to 3D by applying chain\nrules. The paper also pointed out the effectiveness of SJC in 3D\ntext-driven generation tasks. Dream3D [Xu et al. 2022b] is the first\nattempt introducing explicit 3D shape priors into the CLIP-guided\n3D optimization process. Specifically, Dream3D first generates high-\nquality 3D shapes as 3D shape priors in the text-to-shape phase, and\nthen employs them as the initialization of neural radiance fields, and\nperforms optimization with full hints. [Han et al. 2023] proposes\na novel multi-class diffusion model for addressing the challenges\nin semantic-driven 3D shape generation. To solve the problems of\nsingle-class generation, low-frequency details, and the need of a\nlarge amount of paired data, the authors employ a pre-trained CLIP\nmodel to establish a bridge between text, 2D images, and 3D shapes,\nand apply a conditional flow model to generate shape vectors condi-\ntioned on the CLIP embeddings, as well as a latent diffusion model\nconditioned on multi-class shape vectors. Experimental results show\n, Vol. 1, No. 1, Article . Publication date: May 2023.\n8\n\u2022\nLi and Zhang, et al.\nthat the proposed framework outperforms existing methods [Poole\net al. 2022; Valsesia et al. 2019; Yang et al. 2019; Zhou et al. 2021a].\n3.2.2\nFollowers of CLIP-Based Text-Guided 3D Shape Generation.\nPioneers in the text-to-3D field have achieved new heights with\nemerging technologies, while also bringing new challenges, such as\nlow textual 3D model matching, slow rendering speed, low resolu-\ntion of generated 3D models, etc. Numerous research endeavors aim\nat addressing these issues, which are investigated in this section.\nDITTO-NeRF [Seo et al. 2023b] is a novel pipeline that can gen-\nerate high-quality 3D NeRF models from textual prompts or single\nimages. It introduces a progressive 3D object reconstruction scheme,\nincluding scale, orientation and mask, which can propagate high-\nquality information from IB to OB. Compared with the previous\nartworks from image/text to 3D such as DreamFusion [Poole et al.\n2022] and NeuralLift-360 [Xu et al. 2022a], DITTO-NeRF achieves\nsignificant advantages in both quality and diversity, with shorter\ntraining time as well. 3D-CLFusion [Li and Kitani 2023] presents\na novel text-to-3D creation method which utilizes pre-trained la-\ntent variable-based NeRFs to rapidly complete 3D content creation\nwithin less than a minute. To tackle the challenges faced by NeRFs,\n3D-CLFusion adopts a novel approach called view-invariant diffu-\nsion, which utilizes contrastive learning [He et al. 2020] to learn the\nlatent variables and thus can generate high-resolution 3D content\nrapidly during the inference stage. Experimental results demonstrate\nthat 3D-CLFusion is up to 100 times faster than DreamFusion, and\ncan serve as a plug-and-play tool for text-to-3D with pre-trained\nNeRFs. CLIP-Sculptor [Sanghi et al. 2022] presents a 3D shape gener-\nation model under text conditions that improves shape diversity and\nfidelity solely with image-shape pairs as supervision, thus surpass-\ning existing methods. The novelty of the CLIP-Sculptor lies in its\nmulti-resolution, voxel-based conditioned generation scheme. With-\nout text-shape pairs, CLIP-Sculptor learns to generate 3D shapes\nof common object categories from the joint embedding of CLIP\u2019s\ntext-image. To achieve high-fidelity output, CLIP-Sculptor employs\na multi-resolution approach. To generate diverse shapes, CLIP-\nSculptor employs a discrete latent representation obtained with\na vector quantization scheme. To further enhance shape fidelity and\ndiversity, CLIP-Sculptor uses a mask transformer architecture.\n3DFuse [Seo et al. 2023a] proposes a novel framework, incorporat-\ning 3D awareness into pre-trained 2D dispersion models to improve\nthe robustness and 3D consistency of the 2D dispersion model-based\napproach. The view inconsistency problem in score-distilling text-\nto-3D generation, also known as Janus problem [Hong et al. 2023](as\nshown in Figure 16) is a big challenge that have to overcome. 3DFuse\nbuilds a rough 3D structure given the text prompt and then utilizes\nthe projected specific depth map as the condition of the dispersion\nmodel. In addition, a training strategy is introduced to enable the\n2D dispersion model to learn to handle the errors and sparsity in\nthe rough 3D structure and a method to ensure semantic consis-\ntency among all viewpoints in the scene. Experimental results show\nthat 3DFuse effectively solves the problem of 3D consistency and\ndevelops a new approach for 3D reconstruction with 2D dispersion\nmodels. In another work [Hong et al. 2023] proposes two de-biased\nmethods to address the Janus problem in fractional distillation for\n3D generation. These methods reduce artifacts and improve realism,\nwhile achieving a good balance between fidelity of 2D diffusion\nmodel and 3D consistency with low overhead.\nFig. 16. Comparison of the model with Janus problem (left) [Wang et al.\n2022b] and the upgraded model (right) [Hong et al. 2023], the picture is\nobtained from [Hong et al. 2023].\nAlthough Text-to-3D can produce impressive results, it is essen-\ntially unconstrained and may lack the ability to guide or enforce 3D\nstructure. Latent-NeRF [Metzer et al. 2022] incorporates both textual\nguidance and shape guidance for image generation and 3D model\ngeneration, as well as a latent-dispersal model for direct application\nof dispersed rendering on 3D meshes. CompoNeRF [Lin et al. 2023]\nproposes a novel framework that explicitly combines editable 3D\nscene layouts, providing effective guidance at both the local and\nglobal levels for NeRFs, to address the Guidance collapse problem\nfaced in text-to-3D generation. CompoNeRF allows for flexible edit-\ning and recombination of trained local NeRFs into a new scene via\nmanipulation of the 3D layout or textual hints, achieving faithful\nand editable text-to-3D results, as well as opening up potential di-\nrections for multi-object composition via editable 3D scene layouts\nguided by text.\nDreamFusion generates volumetric representations instead of\nmesh representations, which makes it impractical in many down-\nstream applications such as graphics, which require standard 3D rep-\nresentations such as meshes. Text2Mesh [Michel et al. 2022] presents\na novel framework, which is capable of editing the style of 3D\nobjects such as colors and local geometry details from textual de-\nscriptions. The framework uses a fixed mesh and a learned neural\nfield to handle low-quality meshes without UV parameterization.\nFurthermore, Text2Mesh does not require any pre-trained genera-\ntive models or specialized 3D mesh datasets. Thus, it can achieve\nstyle synthesis for various shapes of 3D meshes. TextMesh [Tsal-\nicoglou et al. 2023] presents a new method for generating highly\nrealistic 3D meshes from textual prompts, which solves the problem\nof NeRF being infeasible for most practical applications. To this end,\nthe method extends NeRF to adopt a SDF framework, thus improv-\ning mesh extraction, and introduces a new method for fine-tuning\nmesh textures, eliminating over-saturation effects and enhancing\n, Vol. 1, No. 1, Article . Publication date: May 2023.\nGenerative AI meets 3D: A Survey on Text-to-3D in AIGC Era\n\u2022\n9\nthe detail of the output 3D mesh. Point\u00b7E [Nichol et al. 2022] presents\nan alternative approach for fast 3D object generation which pro-\nduces 3D models in only 1-2 minutes on a single GPU. The proposed\nmethod consists of two diffusion models, a text-to-image diffusion\nmodel and a point cloud diffusion model. Point\u00b7E\u2019s text-to-image\nmodel utilizes a large (text, image) corpus, allowing it to adhere\nto various complex cues. The image-to-3D model is trained on a\nsmaller (image, 3D) pair dataset. To generate 3D objects from textual\ncues, Point\u00b7E first samples the images using its text-to-image model,\nfollowed by sampling 3D objects conditioned on the sampled images.\nBoth steps can be completed within a few seconds and without the\nneed for expensive optimization processes.\n4\nTEXT-TO-3D APPLICATIONS\nWith the emergence of text-to-3D models guided by text-to-image\npriors, more fine-grained application domains have been developed,\nincluding text-to-avatar, text-to-texture, text-to-scene, etc. This sec-\ntion surveys the text-guided 3D model generation models based on\ntext-to-image priors.\n4.1\nText Guided 3D Avatar Generation\nIn recent years, the creation of 3D graphical human models has\ndrawn considerable attention due to its extensive applications in\nareas such as movie production, video gaming, AR/VR and human-\ncomputer interactions, and the creation of 3D avatars through natu-\nral language could save resources and holds great research prospects.\nDreamAvatar [Cao et al. 2023] proposed a framework based on\ntext and shape guidance for generating high-quality 3D human\navatars with controllable poses. It utilizes a trainable NeRF to pre-\ndict the density and color features of 3D points, as well as a pre-\ntrained text-to-image diffusion model to provide 2D self-supervision.\nSMPL [Bogo et al. 2016] model is used to provide rough pose and\nshape guidance for generation, as well as a dual-space design, in-\ncluding a canonical space and an observation space, which are\nrelated by a learnable deformation field through NeRF, allowing op-\ntimized textures and geometries to be transferred from the canonical\nspace to the target pose avatar with detailed geometry and textures.\nExperimental results demonstrate that DreamAvatar significantly\noutperforms the state of the art, setting a new technical level for 3D\nhuman generation based on text and shape guidance.\nDreamFace [Zhang et al. 2023b] is a progressive scheme for per-\nsonalized 3D facial generation guided by text. It enables ordinary\nusers to naturally customize CG-pipe compatible 3D facial assets\nwith desired shapes, textures and fine-grained animation capabil-\nities. DreamFace introduces a coarse-to-fine scheme to generate\na topologically unified neutral face geometry, utilizes Score Dis-\ntillation Sampling (SDS) [Rombach et al. 2022] to optimize subtle\ntranslations and normals, adopts a dual-path mechanism to generate\nneutral appearance, and employs two-stage optimization to enhance\ncompact priors for fine-grained synthesis, as well as to improve the\nanimation capability for personalized deformation features. Dream-\nFace can generate realistic 3D facial assets with physical rendering\nquality and rich animation capabilities from video materials, even\nfor fashion icons, cartoons and fictional aliens in movies.\nAvatarCraft [Jiang et al. 2023] utilizes a diffusion model to guide\nthe learning of neural avatar geometry and texture based on a single\ntext prompt, thereby addressing the challenge of creating 3D char-\nacter avatars with specified identity and artistic style that can be\neasily animated. It also carefully designs an optimization framework\nof neural implicit fields, including coarse-to-fine multi-boundary\nbox training strategy, shape regularization and diffusion-based con-\nstraints, to generate high-quality geometry and texture, and make\nthe character avatars animatable, thus simplifying the animation\nand reshaping of the generated avatars. Experiments demonstrate\nthe effectiveness and robustness of AvatarCraft in creating character\navatars, rendering new views, poses, and shapes.\nMotionCLIP [Tevet et al. 2022], a 3D human motion auto-encoder\nfeaturing a latent embedding that is disentangled, well behaved,\nand supports highly semantic textual descriptions. MotionCLIP\nis unique in that it aligns its latent space with that of the CLIP\nmodel, thus infusing the semantic knowledge of CLIP into the mo-\ntion manifold. Furthermore, MotionCLIP leverages CLIP\u2019s visual\nunderstanding and self-supervised motion-to-frame alignment. The\ncontributions of this paper are the text-to-motion capabilities it\nenables, out-of-domain actions, disentangled editing, and abstract\nlanguage specification. In addition, MotionCLIP shows how the\nintroduced latent space can be leveraged for motion interpolation,\nediting and recognition.\nAvatarCLIP [Hong et al. 2022] introduces a text-driven frame-\nwork for the production of 3D avatars and their motion generation.\nBy utilizing the powerful vision-language model CLIP, AvatarCLIP\nenables non-expert users to craft customized 3D avatars with the\nshape and texture of their choice, and animate them with natural\nlanguage instructions. Extensive experiments indicate that Avatar-\nCLIP exhibits superior zero-shot performance in generating unseen\navatars and novel animations.\nFig. 17. 3D avatar created by text guided 3D generation model, picture\nobtained from [Cao et al. 2023]\n4.2\nText Guided 3D Texture Generation\nRecently, there have been a number of works on text-to-texture,\ninspired by text-to-3D. This summary lists these works.\nTEXTure [Richardson et al. 2023] presents a novel text guided\n3D shape texture generation, editing and transmission method. It\nutilizes a pre-trained deep-to-image topology model and iterative\nschemes to colorize the 3D models from different viewpoints, and\nproposes a novel detailed topology sampling procedure to generate\n, Vol. 1, No. 1, Article . Publication date: May 2023.\n10\n\u2022\nLi and Zhang, et al.\nseamless textures from different viewpoints using the three-step seg-\nmentation map. Additionally, it presents a method for transferring\nthe generated texture maps to new 3D geometry without explicit\nsurface-to-surface mapping and a method for extracting semantic\ntextures from a set of images without any explicit reconstruction,\nand provides a way to edit and refine existing textures with text\nhints or user-provided doodle.\nTANGO [Chen et al. 2022] proposes a novel method for program-\nmatic rendering of realistic appearance effects on arbitrary topology\ngiven surface meshes. Based on CLIP model, the model is used\nto decompose the appearance style into spatially varying bidirec-\ntional reflectance distribution functions, local geometric variations,\nand illumination conditions. This enables realistic 3D style transfer\nthrough the automatic prediction of reflectance effects, even for\nbare, low-quality meshes, without the need for training on particu-\nlar task-specific datasets. Numerous experiments demonstrate that\nTANGO outperforms the existing text-driven 3D style transfer meth-\nods in terms of realism, 3D geometry consistency, and robustness\nfor stylizing low-quality meshes.\nFantasia3D [Chen et al. 2023a] presents a novel approach for text-\nto-high-quality 3D content creation. The method decouples geome-\ntry and appearance modeling and learning, and uses a hybrid scene\nrepresentation and Spatially-varying Bidirectional Reflectance Dis-\ntribution Function (BRDF) learning for surface material to achieve\nphotorealistic rendering of the generated surface. Experimental re-\nsults show that the method outperforms existing approaches [Lin\net al. 2022; Poole et al. 2022] and supports physically plausible sim-\nulations of relit, edited, and generated 3D assets. X-Mesh [Ma et al.\n2023] presents a novel text-driven 3D stylization framework, con-\ntaining a novel text-guided dynamic attention module (TDAM) for\nmore accurate attribute prediction and faster convergence speed.\nAdditionally, a new standard text-mesh benchmark, MIT-30, and\ntwo automatic metrics standards are introduced for future research\nto achieve fair and objective comparison.\nText2Tex [Chen et al. 2023b] proposes a novel approach for gener-\nating high-quality textures for 3D meshes given text prompts. The\ngoal of this method is to address the accumulation inconsistency and\nstretching artifacts in text-driven texture generation. The method\nintegrates repair and merging into a pre-trained deep-perceptual\nimage diffusion model to synthesize high-resolution local textures\nprogressively from multiple perspectives. Experiments show that\nText2Tex significantly outperforms existing text-driven and GAN-\nbased methods.\nFig. 18. Texturing results generated by text guided 3D texture model, picture\nobtained by [Richardson et al. 2023]\n4.3\nText Guided 3D Scene Generation\n3D scene modeling is a time-consuming task, usually requiring\nprofessional 3D designers to complete. To make 3D scene modeling\neasier, 3D generation should be simple and intuitive to operate while\nretaining enough controllability to meet users\u2019 precise requirements.\nRecent works [Cohen-Bar et al. 2023; Fridman et al. 2023; H\u00f6llein\net al. 2023; Po and Wetzstein 2023] in text-to-3D generation has\nmade 3D scene modeling easier.\nSet-the-Scene [Cohen-Bar et al. 2023] proposes an agent-based\nglobal-local training framework for synthesizing 3D scenes, thus\nfilling an important gap from controllable text to 3D synthesis.\nIt can learn a complete representation of each object while also\ncreating harmonious scenes with style and lighting matching. The\nframework allows various editing options such as adjusting the\nplacement of each individual object, deleting objects from scenes,\nor refining objects.\n[Po and Wetzstein 2023] proposes a local condition diffusion-\nbased text-to-3D scene synthesis approach, which aims to make the\ngeneration of complex 3D scenes more intuitive and controllable. By\nproviding control on the semantic parts via text hints and bounding\nboxes, the method ensures seamless transitions between these parts.\nExperiments show that the proposed method achieves higher fidelity\nin the composition of 3D scenes than the related baselines [Liu et al.\n2022; Wang et al. 2022b].\nSceneScape [Fridman et al. 2023] proposes a novel text-driven\napproach to generate permanent views, which is capable of syn-\nthesizing long videos of arbitrary scenes solely based on input\ntexts describing the scene and camera positions. The framework\ncombines the generative capacity of a pre-trained text-to-image\nmodel [Rombach et al. 2022] with the geometry priors learned from\na pre-trained monocular depth prediction model [Ranftl et al. 2021,\n2020], to generate videos in an online fashion, and achieves 3D con-\nsistency through online testing time training to generate videos with\ngeometry-consistent scenes. Compared with the previous works\nthat are limited to a restricted domain, this framework is able to\ngenerate various scenes including walking through a spaceship, a\ncave, or an ice city.\nText2Room [H\u00f6llein et al. 2023] proposes a method of generating\nroom-scale 3D meshes with textures from given text prompts. It\nis the first method to generate attention-grabbing textured room-\nscale 3D geometries solely from text inputs, which is different from\nexisting methods that focus on generating single objects [Lin et al.\n2022; Poole et al. 2022] or scaling trajectories(SceneScape) [Fridman\net al. 2023] from text.\nText2NeRF [Zhang et al. 2023a] proposes a text-driven realistic\n3D scene generation framework combining diffusion model with\nNeRF representations to support zero-shot generation of various\nindoor/outdoor scenes from a variety of natural language prompts.\nAdditionally, a progressive inpainting and updating (PIU) strategy\nis introduced to generate view-consistent novel contents for 3D\nscenes, and a support set is built to provide multi-view constraints\nfor the NeRF model during view-by-view updating. Moreover, a\ndepth loss is employed to achieve depth-aware NeRF optimization,\nand a two-stage depth alignment strategy is introduced to elimi-\nnate estimated depth misalignment in different views. Experimental\n, Vol. 1, No. 1, Article . Publication date: May 2023.\nGenerative AI meets 3D: A Survey on Text-to-3D in AIGC Era\n\u2022\n11\nresults demonstrate that the proposed Text2NeRF outperforms ex-\nisting methods [H\u00f6llein et al. 2023; Mohammad Khalid et al. 2022;\nPoole et al. 2022; Wang et al. 2022b] in producing photo-realistic,\nmultiview consistent, and diverse 3D scenes from a variety of natural\nlanguage prompts.\nFig. 19. Controllable scenes generation from text prompts, the picture is\nobtained by [Cohen-Bar et al. 2023]\nMAV3D (Make-A-Video3D) [Singer et al. 2023] is a method for gen-\nerating 3D dynamic scenes from text descriptions. The motivation\nof MAV3D is to provide a method for generating dynamic videos\nwithout 3D or 4D data, thus saving a lot of time and money. This\nmethod adopts 4D dynamic NeRF and optimizes the scene appear-\nance, density, and motion consistency by querying the Text-to-Video\n(T2V) [Singer et al. 2022] model based on diffusion. Quantitative\nand qualitative experiments show that MAV3D improves over the\ninternal baselines established previously. MAV3D is the first method\nto generate 3D dynamic scenes given text descriptions.\n4.4\nText Guided 3D Shape Transformation\nThe traditional process of editing 3D models involves dedicated\ntools and years of training in order to manually carve, extrude, and\nre-texture given objects. This process is laborious and costly in terms\nof resources, and recently some research works have attempted to\nuse text-guided 3D model editing, which is demonstrated in this\nsection.\nInstruct-NeRF2NeRF [Haque et al. 2023] presents a novel text-\ninstructable editing of NeRF scenes, which uses an iterative image-\nbased diffusion model (InstructPix2Pix) [Brooks et al. 2022] to edit\nthe input image while optimizing the underlying scene, thereby\ngenerating an optimized 3D scene that follows the edits instructions.\nExperimental results show that the method is capable of editing\nlarge-scale real-world scenes, achieving more realistic and targeted\nedits than previous works.\nInstruct 3D-to-3D [Kamata et al. 2023] presents a novel 3D-to-3D\ntransformation method, which uses a pre-trained image-to-image\ndiffusion model to achieve 3D-to-3D transformation. Furthermore,\nthe method also proposes dynamic scaling, as well as explicitly\nconditioning on the input source 3D scene, to enhance 3D consis-\ntency and controllability. Quantitative and qualitative evaluations\ndemonstrate that Instruct 3D-to-3D achieves higher-quality 3D-to-\n3D transformation than the baseline methods [Poole et al. 2022;\nWang et al. 2022a].\nSKED [Mikaeili et al. 2023] presents a sketch-based technique\nfor editing 3D shapes represented by NeRF. The motivation is to\nintroduce interactive editing into the text-to-3D pipeline, enabling\nusers to edit from a more intuitive control perspective. Results show\nthat SKED can effectively modify the existing neural fields and\ngenerate outputs that satisfy user sketches.\nTextDeformer [Gao et al. 2023] proposes an automatic technique\nfor generating input triangle mesh deformations guided entirely by\ntext prompts. The framework is capable of generating large, low-\nfrequency shape changes as well as small, high-frequency details,\nrelying on differentiable rendering to connect geometry to power-\nful pre-trained image encoders such as CLIP[Radford et al. 2021]\nand DINO [Caron et al. 2021]. In order to overcome the problems\nof artifacts, TextDeformer proposes to use the Jacobian matrix to\nrepresent the mesh deformation and encourages deep features to be\ncomputed on 2D encoded rendering to ensure shape coherence from\nall 3D viewpoints. Experimental results show that the method can\nsmoothly deform various source meshes and target text prompts to\nachieve large modifications and add details.\nFig. 20. Converting input 3D scenes according to the text instructions, the\npicture is obtained from [Kamata et al. 2023]\n5\nDISCUSSION\nCombining NeRF and textual-to-image-prior textual-to-3D gener-\nation paradigm is an emerging research direction with the strong\nadvantage of generating diverse content. However, there are also\na lot of issues with it such as long inference time, 3D consistency\nissues, poor controllability and the generated content that cannot\nbe applied well to industrial needs.\n5.1\nFidelity\nFor 3D asset generation, constrained by the weakly supervised\nand low-resolution of CLIP, the upscaled results are not perfect. In\naddition, it is difficult to generate a more varied portrait under the\nsame prompt. For the same prompt, CLIP text features are always\nthe same. Fidelity and speed are two indicators often requiring trade-\noffs which often require the increase in inference speed to improve\nfidelity. At the same time, downstream application requirements\nshould also be taken into account. Films require high-precision\nmodels, while games often require more quantity than film-level\nprecision.\n, Vol. 1, No. 1, Article . Publication date: May 2023.\n12\n\u2022\nLi and Zhang, et al.\n5.2\nInference velocity\nA fatal issue of generating 3D content by leveraging pre-training\nmodels based on diffusion models as a powerful prior and learning\nobjective is that the inference process is too slow. Even at a resolu-\ntion of just 64\u00d764, DreamFusion [Poole et al. 2022] would take 1.5\nhours to infer for each prompt using a TPUv4. The inference time is\nfurther rising quickly along with the increase in resolution. This is\nmainly due to the fact that the inference process for generating 3D\ncontent is actually starting from scratch to train a Neural Radiance\nField [Mildenhall et al. 2021]. Notably, NeRF models are renowned\nfor their slow training and inference speeds, and training a deep\nnetwork takes a lot of time. Magic3D [Lin et al. 2022] has addressed\nthe time issue by using a two-phase optimization framework. Firstly,\na coarse model is obtained by leveraging a low-resolution diffusion\nprior, and secondly, acceleration is performed by using a sparse\n3D hash grid structure. 3D-CLFusion[Li and Kitani 2023] utilizes a\npre-trained latent NeRf and performs a fast 3D content creation in\nless than a minute.\n5.3\nConsistency\nDistortion and ghost images are often encountered in the 3D scenes\ngenerated by DreamFusion [Poole et al. 2022], and unstable 3D\nscenes are observed when text prompts and random seeds are\nchanged. This issue is mainly caused by the lack of perception\nof 3D information from 2D prior diffusion models, as the trans-\nmission model has no knowledge of which direction the object is\nobserved from, leading to the serious distortion of 3D scenes by\ngenerating the front-view geometry features from all viewpoints,\nincluding sides and the backs, which is usually referred to as the\nJanus problem. [Hong et al. 2023] proposed two debiasing methods\nto address such issues: score debiasing, which involves gradually in-\ncreasing the truncation value of the 2D diffusion model\u2019s estimation\nthroughout the entire optimization process; and prompt debiasing,\nwhich employs a language model to recognize the conflicts between\nthe user prompts and the view prompts, and adjust the difference\nbetween the view prompts and the spatial camera pose of objects.\n3DFuse [Seo et al. 2023a] optimized the training process to make\nthe 2D diffusion model learn to process the wrong and sparse 3D\nstructures for robust generation, as well as a way to ensure that the\nsemantic consistency of all viewpoints in the scene is ensured.\n5.4\nControllability\nAlthough Text-to-3D can generate impressive results, as the text-to-\nimage diffusion models are essentially unconstrained, they generally\ntend to suffer from guiding collapse. This makes them less capable\nof accurately associating object semantics with specific 3D struc-\ntures. The issue of poor controllability has long been mainstream\nin the text-to-image generation task, with ControlNet [Zhang and\nAgrawala 2023] addressing it by adding extra input conditions to\nmake the generation process even more controllable for large-scale\ntext-to-image models, such as with the addition of canny edge,\nhough lines and depth maps. This unique combination of text and\nshape guidance allows for increased control over the generation\nprocess. LatentNeRF [Metzer et al. 2022] allows for increased control\nover the 3D generation process through its unique combination of\ntext and shape guidance. CompoNeRF [Lin et al. 2023] is capable\nof precisely associating guidance with particular structures via its\nintegration of editable 3D layouts and multiple local NeRFs, address-\ning the guidance failure issue when generating multiple object 3D\nscenes.\n5.5\nApplicability\nAlthough NeRF, a novel 3D representation, cannot be directly ap-\nplied to traditional 3D application scenarios, its powerful representa-\ntion capabilities enable it to possess unlimited application prospects.\nThe greatest advantage of NeRF is that it can be trained with 2D im-\nages. Google has already begun to use NeRFs to transform street map\nimages into immersive views on Google Maps. In the future, NeRFs\ncan supplement other technologies to more efficiently, accurately,\nand realistically represent 3D objects in the metaverse, augmented\nreality, and digital twins. To further improve these applications,\nfuture research may focus on extracting 3D meshes, point-clouds,\nor SDFs from the density MLP, and integrating faster NeRF models.\nIt remains to be seen whether the Paradigm of Pure Text Guiding\nShape Generation can cope with all scenarios. Perhaps incorporat-\ning a more intuitive guidance mechanism, such as sketch guidance\nor picture guidance, might be a more reasonable choice.\n6\nCONCLUSION\nThis work conducts the first yet comprehensive survey on text-\nto-3D. Specifically, we summarize text-to-3D from three aspects:\ndata representations, technologies and applications. We hope this\nsurvey can help readers quickly understand the field of text-to-3D\nand inspire more future works to explore text-to-3D.\nREFERENCES\nPanos Achlioptas, Judy Fan, Robert Hawkins, Noah Goodman, and Leonidas J Guibas.\n2019. ShapeGlot: Learning language for shape differentiation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision. 8938\u20138947.\nParag Agarwal and Balakrishnan Prabhakaran. 2009. Robust blind watermarking of\npoint-sampled geometry. IEEE Transactions on Information Forensics and Security 4,\n1 (2009), 36\u201348.\nEman Ahmed, Alexandre Saint, Abd El Rahman Shabayek, Kseniya Cherenkova, Rig Das,\nGleb Gusev, Djamila Aouada, and Bjorn Ottersten. 2018. A survey on deep learning\nadvances on different 3D data representations. arXiv preprint arXiv:1808.01462\n(2018).\nJames F Blinn. 2005. What is a pixel? IEEE computer graphics and applications 25, 5\n(2005), 82\u201387.\nFederica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and\nMichael J Black. 2016. Keep it SMPL: Automatic estimation of 3D human pose and\nshape from a single image. In Computer Vision\u2013ECCV 2016: 14th European Conference,\nAmsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14. Springer,\n561\u2013578.\nTim Brooks, Aleksander Holynski, and Alexei A Efros. 2022. Instructpix2pix: Learning\nto follow image editing instructions. arXiv preprint arXiv:2211.09800 (2022).\nWenming Cao, Zhiyue Yan, Zhiquan He, and Zhihai He. 2020. A comprehensive survey\non geometric deep learning. IEEE Access 8 (2020), 35929\u201335949.\nYukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-Yee K Wong. 2023. DreamA-\nvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models.\narXiv preprint arXiv:2304.00916 (2023).\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bo-\njanowski, and Armand Joulin. 2021. Emerging properties in self-supervised vision\ntransformers. In Proceedings of the IEEE/CVF international conference on computer\nvision. 9650\u20139660.\nDave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias\nNie\u00dfner. 2023b. Text2Tex: Text-driven Texture Synthesis via Diffusion Models. arXiv\npreprint arXiv:2303.11396 (2023).\nKevin Chen, Christopher B Choy, Manolis Savva, Angel X Chang, Thomas Funkhouser,\nand Silvio Savarese. 2019. Text2shape: Generating shapes from natural language by\nlearning joint embeddings. In Computer Vision\u2013ACCV 2018: 14th Asian Conference\n, Vol. 1, No. 1, Article . Publication date: May 2023.\nGenerative AI meets 3D: A Survey on Text-to-3D in AIGC Era\n\u2022\n13\non Computer Vision, Perth, Australia, December 2\u20136, 2018, Revised Selected Papers,\nPart III 14. Springer, 100\u2013116.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023a. Fantasia3D: Disentangling\nGeometry and Appearance for High-quality Text-to-3D Content Creation. arXiv\npreprint arXiv:2303.13873 (2023).\nYongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui Jia. 2022. Tango: Text-driven\nphotorealistic and robust 3d stylization via lighting decomposition. arXiv preprint\narXiv:2210.11277 (2022).\nDana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. 2023.\nSet-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes.\narXiv preprint arXiv:2303.13450 (2023).\nYinpeng Dong, Shouwei Ruan, Hang Su, Caixin Kang, Xingxing Wei, and Jun Zhu.\n2022. ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial\nViewpoints. arXiv preprint arXiv:2210.03895 (2022).\nRafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. 2023. Scenescape: Text-\ndriven consistent scene generation. arXiv preprint arXiv:2302.01133 (2023).\nRao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, and Srinath Sridhar. 2022. Shapecrafter:\nA recursive text-conditioned 3d shape generation model.\narXiv preprint\narXiv:2207.09446 (2022).\nYasutaka Furukawa, Carlos Hern\u00e1ndez, et al. 2015. Multi-view stereo: A tutorial.\nFoundations and Trends\u00ae in Computer Graphics and Vision 9, 1-2 (2015), 1\u2013148.\nJun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or\nLitany, Zan Gojcic, and Sanja Fidler. 2022b. Get3d: A generative model of high\nquality 3d textured shapes learned from images. Advances In Neural Information\nProcessing Systems 35 (2022), 31841\u201331854.\nKyle Gao, Yina Gao, Hongjie He, Denning Lu, Linlin Xu, and Jonathan Li. 2022a.\nNerf: Neural radiance field in 3d vision, a comprehensive review. arXiv preprint\narXiv:2210.00379 (2022).\nWilliam Gao, Noam Aigerman, Thibault Groueix, Vladimir G Kim, and Rana Hanocka.\n2023. TextDeformer: Geometry Manipulation using Text Guidance. arXiv preprint\narXiv:2304.13348 (2023).\nYulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun.\n2020. Deep learning for 3d point clouds: A survey. IEEE transactions on pattern\nanalysis and machine intelligence 43, 12 (2020), 4338\u20134364.\nBo Han, Yitong Liu, and Yixuan Shen. 2023. Zero3D: Semantic-Driven Multi-Category\n3D Shape Generation. arXiv preprint arXiv:2301.13591 (2023).\nRana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel\nCohen-Or. 2019. Meshcnn: a network with an edge. ACM Transactions on Graphics\n(TOG) 38, 4 (2019), 1\u201312.\nAyaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo\nKanazawa. 2023. Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions. arXiv\npreprint arXiv:2303.12789 (2023).\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momen-\ntum contrast for unsupervised visual representation learning. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. 9729\u20139738.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.\nLukas H\u00f6llein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie\u00dfner. 2023.\nText2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models. arXiv\npreprint arXiv:2303.11989 (2023).\nFangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu.\n2022. Avatarclip: Zero-shot text-driven generation and animation of 3d avatars.\narXiv preprint arXiv:2205.08535 (2022).\nSusung Hong, Donghoon Ahn, and Seungryong Kim. 2023. Debiasing Scores and\nPrompts of 2D Diffusion for Robust Text-to-3D Generation.\narXiv preprint\narXiv:2303.15413 (2023).\nDongting Hu, Zhenkai Zhang, Tingbo Hou, Tongliang Liu, Huan Fu, and Mingming\nGong. 2023. Multiscale Representation for Real-Time Anti-Aliasing Neural Render-\ning. arXiv preprint arXiv:2304.10075 (2023).\nAjay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. 2022.\nZero-shot text-guided object generation with dream fields. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 867\u2013876.\nRuixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong\nChen, and Jing Liao. 2023. AvatarCraft: Transforming Text into Neural Human\nAvatars with Parameterized Shape and Pose Control. arXiv preprint arXiv:2303.17606\n(2023).\nYiwei Jin, Diqiong Jiang, and Ming Cai. 2020. 3d reconstruction using deep learning: a\nsurvey. Communications in Information and Systems 20, 4 (2020), 389\u2013413.\nHiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato Ishii, and Takuya Narihira.\n2023. Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion. arXiv\npreprint arXiv:2303.15780 (2023).\nYu-Jhe Li and Kris Kitani. 2023. 3D-CLFusion: Fast Text-to-3D Rendering with Con-\ntrastive Latent Diffusion. arXiv preprint arXiv:2303.11938 (2023).\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang,\nKarsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2022. Magic3D: High-\nResolution Text-to-3D Content Creation. arXiv preprint arXiv:2211.10440 (2022).\nYiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin, Hui Xiong, and Lin Wang.\n2023. CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D\nScene Layout. arXiv preprint arXiv:2303.13843 (2023).\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. 2022.\nCompositional visual generation with composable diffusion models. In Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XVII. Springer, 423\u2013439.\nWeiping Liu, Jia Sun, Wanyi Li, Ting Hu, and Peng Wang. 2019. Deep learning on point\nclouds and its application: A survey. Sensors 19, 19 (2019), 4188.\nYiwei Ma, Xiaioqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei Wang, Guannan Jiang,\nWeilin Zhuang, and Rongrong Ji. 2023. X-Mesh: Towards Fast and Accurate Text-\ndriven 3D Stylization via Dynamic Textual Guidance. arXiv preprint arXiv:2303.15764\n(2023).\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas\nGeiger. 2019. Occupancy networks: Learning 3d reconstruction in function space.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.\n4460\u20134470.\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. 2022.\nLatent-NeRF for Shape-Guided Generation of 3D Shapes and Textures. arXiv preprint\narXiv:2211.07600 (2022).\nOscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. 2022.\nText2mesh: Text-driven neural stylization for meshes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 13492\u201313502.\nAryan Mikaeili, Or Perel, Daniel Cohen-Or, and Ali Mahdavi-Amiri. 2023. SKED:\nSketch-guided Text-based 3D Editing. arXiv preprint arXiv:2303.10735 (2023).\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields\nfor view synthesis. Commun. ACM 65, 1 (2021), 99\u2013106.\nLudovico Minto, Pietro Zanuttigh, and Giampaolo Pagnutti. 2018. Deep Learning for\n3D Shape Classification based on Volumetric Density and Surface Approximation\nClues.. In VISIGRAPP (5: VISAPP). 317\u2013324.\nNasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. 2022.\nCLIP-Mesh: Generating textured meshes from text using pretrained image-text\nmodels. In SIGGRAPH Asia 2022 Conference Papers. 1\u20138.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcGrew, Ilya Sutskever, and Mark Chen. 2021.\nGlide: Towards photorealistic\nimage generation and editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741 (2021).\nAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. 2022.\nPoint-E: A System for Generating 3D Point Clouds from Complex Prompts. arXiv\npreprint arXiv:2212.08751 (2022).\nJaesik Park, Sudipta N Sinha, Yasuyuki Matsushita, Yu-Wing Tai, and In So Kweon. 2016.\nRobust multiview photometric stereo using planar mesh parameterization. IEEE\ntransactions on pattern analysis and machine intelligence 39, 8 (2016), 1591\u20131604.\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love-\ngrove. 2019. Deepsdf: Learning continuous signed distance functions for shape\nrepresentation. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition. 165\u2013174.\nLim Wen Peng and Siti Mariyam Shamsuddin. 2004. 3D object reconstruction and rep-\nresentation using neural networks. In Proceedings of the 2nd international conference\non Computer graphics and interactive techniques in Australasia and South East Asia.\n139\u2013147.\nRyan Po and Gordon Wetzstein. 2023. Compositional 3D Scene Generation using\nLocally Conditioned Diffusion. arXiv preprint arXiv:2303.12218 (2023).\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2022. Dreamfusion:\nText-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022).\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. 2017. Pointnet: Deep\nlearning on point sets for 3d classification and segmentation. In Proceedings of the\nIEEE conference on computer vision and pattern recognition. 652\u2013660.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.\nLearning transferable visual models from natural language supervision. In Interna-\ntional conference on machine learning. PMLR, 8748\u20138763.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125 (2022).\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Rad-\nford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In\nInternational Conference on Machine Learning. PMLR, 8821\u20138831.\nRen\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. 2021. Vision transformers\nfor dense prediction. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision. 12179\u201312188.\nRen\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun.\n2020. Towards robust monocular depth estimation: Mixing datasets for zero-shot\ncross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence\n44, 3 (2020), 1623\u20131637.\n, Vol. 1, No. 1, Article . Publication date: May 2023.\n14\n\u2022\nLi and Zhang, et al.\nKonstantinos Rematas and Vittorio Ferrari. 2020. Neural voxel renderer: Learning an\naccurate and controllable rendering tool. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. 5417\u20135427.\nElad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. 2023.\nTexture: Text-guided texturing of 3d shapes. arXiv preprint arXiv:2302.01721 (2023).\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2022. High-resolution image synthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684\u201310695.\nAlessandro Rossi, Marco Barbiero, Paolo Scremin, and Ruggero Carli. 2021. Robust\nVisibility Surface Determination in Object Space via Pl\u00fccker Coordinates. Journal\nof Imaging 7, 6 (2021), 96.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. 2022. Photorealistic text-to-image diffusion models with deep language under-\nstanding. Advances in Neural Information Processing Systems 35 (2022), 36479\u201336494.\nAditya Sanghi, Rao Fu, Vivian Liu, Karl Willis, Hooman Shayani, AmirHosein Khasah-\nmadi, Srinath Sridhar, and Daniel Ritchie. 2022. CLIP-Sculptor: Zero-Shot Genera-\ntion of High-Fidelity and Diverse Shapes from Natural Language. arXiv preprint\narXiv:2211.01427 (Nov 2022).\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-\nman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation\nimage-text models. arXiv preprint arXiv:2210.08402 (2022).\nHoigi Seo, Hayeon Kim, Gwanghyun Kim, and Se Young Chun. 2023b.\nDITTO-\nNeRF: Diffusion-based Iterative Text To Omni-directional 3D Model. arXiv preprint\narXiv:2304.02827 (2023).\nJunyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho\nKim, Jin-Hwa Kim, Jiyoung Lee, and Seungryong Kim. 2023a. Let 2D Diffusion\nModel Know 3D-Consistency for Robust Text-to-3D Generation. arXiv preprint\narXiv:2303.07937 (2023).\nTianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. 2021. Deep\nmarching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis.\nAdvances in Neural Information Processing Systems 34 (2021), 6087\u20136101.\nZifan Shi, Sida Peng, Yinghao Xu, Yiyi Liao, and Yujun Shen. 2022. Deep generative\nmodels on 3d representations: A survey. arXiv preprint arXiv:2210.15663 (2022).\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan\nHu, Harry Yang, Oron Ashual, Oran Gafni, et al. 2022. Make-a-video: Text-to-video\ngeneration without text-video data. arXiv preprint arXiv:2209.14792 (2022).\nUriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokki-\nnos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. 2023. Text-\nto-4d dynamic scene generation. arXiv preprint arXiv:2301.11280 (2023).\nGuy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. 2022.\nMotionclip: Exposing human motion generation to clip space. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart XXII. Springer, 358\u2013374.\nChristina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Fed-\nerico Tombari. 2023. TextMesh: Generation of Realistic 3D Meshes From Text\nPrompts. arXiv preprint arXiv:2304.12439 (2023).\nDiego Valsesia, Giulia Fracastoro, and Enrico Magli. 2019. Learning localized generative\nmodels for 3d point clouds via graph convolution. In International conference on\nlearning representations.\nCan Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. 2022a. Clip-\nnerf: Text-and-image driven manipulation of neural radiance fields. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3835\u20133844.\nCheng Wang, Ming Cheng, Ferdous Sohel, Mohammed Bennamoun, and Jonathan Li.\n2019. NormalNet: A voxel-based CNN for 3D object classification and retrieval.\nNeurocomputing 323 (2019), 139\u2013147.\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich.\n2022b. Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D\nGeneration. arXiv preprint arXiv:2212.00774 (2022).\nHe Wang and Juyong Zhang. 2022. A Survey of Deep Learning-Based Mesh Processing.\nCommunications in Mathematics and Statistics 10, 1 (2022), 163\u2013194.\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu\nPhilip. 2020. A comprehensive survey on graph neural networks. IEEE transactions\non neural networks and learning systems 32, 1 (2020), 4\u201324.\nDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang.\n2022a. NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360\u00b0\nViews. arXiv e-prints (2022), arXiv\u20132211.\nJiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and\nShenghua Gao. 2022b. Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape\nPrior and Text-to-Image Diffusion Models. arXiv preprint arXiv:2212.14704 (2022).\nGuandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath\nHariharan. 2019. Pointflow: 3d point cloud generation with continuous normalizing\nflows. In Proceedings of the IEEE/CVF international conference on computer vision.\n4541\u20134550.\nXianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang\nWu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al.\n2023. MVImgNet: A Large-scale Dataset of Multi-view Images. arXiv preprint\narXiv:2303.06042 (2023).\nChaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Ku-\nmar Dam, Mengchun Zhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al.\n2023c. One small step for generative ai, one giant leap for agi: A complete survey\non chatgpt in aigc era. arXiv preprint arXiv:2304.06488 (2023).\nChenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. 2023d. Text-\nto-image Diffusion Model in Generative AI: A Survey. arXiv preprint arXiv:2303.07909\n(2023).\nChaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun\nZhang, Sumit Kumar Dam, Chu Myaet Thwal, Ye Lin Tun, Le Luang Huy, et al.\n2023e. A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to\nGPT-5 All You Need? arXiv preprint arXiv:2303.11717 (2023).\nJingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. 2023a. Text2NeRF:\nText-Driven 3D Scene Generation with Neural Radiance Fields. arXiv preprint\narXiv:2305.11588 (2023).\nLvmin Zhang and Maneesh Agrawala. 2023. Adding conditional control to text-to-image\ndiffusion models. arXiv preprint arXiv:2302.05543 (2023).\nLongwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye\nShi, Sibei Yang, Lan Xu, and Jingyi Yu. 2023b. DreamFace: Progressive Generation of\nAnimatable 3D Faces under Text Guidance. arXiv preprint arXiv:2304.03117 (2023).\nHang Zhou, Weiming Zhang, Kejiang Chen, Weixiang Li, and Nenghai Yu. 2021b. Three-\ndimensional mesh steganography and steganalysis: a review. IEEE Transactions on\nVisualization and Computer Graphics (2021).\nLinqi Zhou, Yilun Du, and Jiajun Wu. 2021a. 3d shape generation and completion\nthrough point-voxel diffusion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision. 5826\u20135835.\n, Vol. 1, No. 1, Article . Publication date: May 2023.\n"
  },
  {
    "title": "Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception",
    "link": "https://arxiv.org/pdf/2305.06324.pdf",
    "upvote": "1",
    "text": "Alternating Gradient Descent and Mixture-of-Experts\nfor Integrated Multimodal Perception\nHassan Akbari\u2217\nDan Kondratyuk\u2217\nYin Cui\nRachel Hornung\nHuisheng Wang\nHartwig Adam\nGoogle Research\n{hassanak, dankondratyuk, yincui, rachelhornung, huishengw, hadam}@google.com\nAbstract\nWe present Integrated Multimodal Perception (IMP), a simple and scalable mul-\ntimodal multi-task training and modeling approach. IMP integrates multimodal\ninputs including image, video, text, and audio into a single Transformer encoder\nwith minimal modality-specific components. IMP makes use of a novel design that\ncombines Alternating Gradient Descent (AGD) and Mixture-of-Experts (MoE) for\nefficient model & task scaling. We conduct extensive empirical studies and reveal\nthe following key insights: 1) performing gradient descent updates by alternating\non diverse modalities, loss functions, and tasks, with varying input resolutions,\nefficiently improves the model. 2) sparsification with MoE on a single modality-\nagnostic encoder substantially improves the performance, outperforming dense\nmodels that use modality-specific encoders or additional fusion layers and greatly\nmitigates the conflicts between modalities. IMP achieves competitive performance\non a wide range of downstream tasks including video classification, image clas-\nsification, image-text, and video-text retrieval. Most notably, we train a sparse\nIMP-MoE-L focusing on video tasks that achieves new state-of-the-art in zero-shot\nvideo classification: 77.0% on Kinetics-400, 76.8% on Kinetics-600, and 68.3% on\nKinetics-700, improving the previous state-of-the-art by +5%, +6.7%, and +5.8%,\nrespectively, while using only 15% of their total training computational cost.\n1\nIntroduction\nThe human perception system is profoundly multimodal. We perceive the world through the integra-\ntion of a vast array of sensory systems across domains \u2014 visual, auditory, olfactory, somatic, etc.\nNeurons for multimodal integration have been found in both multisensory convergence zones Calvert\n[2001] and unimodal regions Driver and Noesselt [2008] in the human brain. Studies in developmental\npsychology also suggest that interrelating simultaneous multimodal sensations is key for perceptutal\nlearning Smith and Gasser [2005]. Inspired by these findings, we see an opportunity for combined\nmultisensory learning in machine learning systems as well.\nThe rapid rise of large-scale multitask frameworks and models [Raffel et al., 2020, Roberts et al.,\n2022, Radford et al., 2021, Yu et al., 2022, Wang et al., 2022] provides foundations for integrating\ncapabilities that unify many disparate tasks under one model. However, given the vast quantity\nof independent variables involved in designing such a system, achieving an integrated multimodal\nmachine learning model still remains an open research direction. More specifically, designing a\nmulti-task model that integrates many multimodal signals is challenging due to various reasons: i.\nDifferent modalities require structurally different I/O signatures to properly train. ii. When training\nacross multiple datasets, some modalities or objectives may not exist or cannot be applied, depending\n\u2217Equal contribution.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.06324v2  [cs.CV]  11 Dec 2023\non the input data and the task to perform. iii. The presence of multiple input modalities calls for\ncareful considerations on the architectural design and allocation of parameters to certain modalities,\noften requiring extensive hyperparameter tuning to find the best use of computational resources.\nIntuitively, as we scale a model, it becomes increasingly expensive to redesign the architecture or\nsearch for a better training objective. The issue is exacerbated in multimodal multi-task modeling,\nwhere we need to consider the combination of input modalities or datasets, loss functions, and tasks\nat large scales. Therefore, we would like to find a training approach that can be scaled incrementally:\nfor any new task or objective, regardless of its input shape or output loss, we should be able to add it\nto the existing pretraining without compromising the previous tasks.\nWe navigate this problem by exploring ways in which we could train one multimodal model such that\nit (1) leverages as many existing datasets as possible, (2) can train on any combination of tasks or loss\nfunctions, and (3) does not slow down with the addition of any new dataset, task, or loss function.\nBy solving all of these points simultaneously, a multimodal model could scale with an increasingly\ndiverse and rich set of training data without needing to redesign the training framework when new\ntasks get integrated.\nWe observe in our empirical results that the combination of diverse, heterogeneous tasks that have\nbeen previously established as strong objectives individually (e.g., supervised classification and\nself-supervised contrastive learning) across multiple modalities are not only complementary, but can\noffer better convergence than training on individual tasks. By implementing Alternating Gradient\nDescent (AGD) and Mixture-of-Experts (MoE) via recently developed JAX primitives, we enable\nour model to use a fraction of the computational cost and memory required by similar large-scale\nperception models [Radford et al., 2021, Jia et al., 2021, Yu et al., 2022], despite the addition of\nmultiple modalities which would normally require 2-8\u00d7 compute at similar batch sizes.\nGiven this context, our contributions and findings are as follows:\n1. We define an integrated modality-agnostic encoder model, and leverage a strong combi-\nnation of image-text contrastive, video-text contrastive, video-audio contrastive, and im-\nage/video/audio classification losses during pretraining to create an Integrated Multimodal\nPerception (IMP) model, as shown in Figure 1.\n2. Contrasting the conventional approach of summing the losses of multiple objectives, we\nshow that alternating between objectives results in a design that allows seamless integration\nof virtually any number of tasks and datasets without significant memory overhead and\nresults in better downstream evaluations.\n3. We show that optimization between multiple heterogeneous multimodal tasks is comple-\nmentary and results in a higher quality model than trained on any individual task.\n4. To train on large batches of video and audio modalities without reducing training efficiency\nor loss of accuracy, we design a dynamic mixture of various resolutions, sequence lengths,\nand batch sizes throughout pretraining, and alternate training on all input variations.\n5. We enable our model with MoE, showing strong performance gains compared to a more\nconventional multi-tower contrastive model, even when appplying MoE to both towers.\n6. We scale our resulting MoE-IMP model to 2B sparse parameters with similar compute to\nViT-L (300M parameters), resulting in state-of-the-art performance on several large-scale\nmultimodal video understanding datasets.\n2\nRelated Work\nThe optimality of AGD optimization vs. averaging the losses (or gradients) has been explored in\nprior work [Jain et al., 2017, Pascal et al., 2021]. Alternating multimodal multi-task training with\nAGD has been explored in PolyViT [Likhosherstov et al., 2021], which analyzes different methods to\ncombining heterogeneous task in a single model. The work reports similar findings to our own, that\ncombining objectives can be mutually beneficial and alternating between datasets weighted by the\nnumber of examples provides one of the best methods for optimization. Our work extends this to a\nmuch more generic setup supporting virtually any combination of modalities, tasks, resolutions, etc.\nThe use of sparse MoE for multimodal modeling can be seen in recent works like LIMoE [Mustafa\net al., 2022], which uses a single MoE encoder for image-text tasks, and VL-MoE [Shen et al., 2023],\n2\n(a) The IMP model architecture.\nObjective: 1\n1D Text \nProjection\n3D Vision Patch \nProjection\n1D Waveform \nPatch Projection\nData\nEmbed + Flatten\nMoE Encoder\nHeads\nRouter\nFFN 1\nFFN N\nSelf-Attention\n\u2026\nx L/2 Sparse Layers\nLinear Classifiers\nCommon Space\nV\u2192T\nT\u2192V\nT\u2192A\nV\u2192A\nA\u2192V\nA\u2192T\nJFT\nWTS\nAudioSet\n\u2026\n2D Spectrogram \nPatch Projection\n1D Pos Encode\n1D Pos Encode\n2D Pos Encode\n3D Pos Encode\nObjectives\nNoise-Contrastive \nEstimation (NCE)\nSoftmax\nCross-Entropy\nBinary\nCross-Entropy\nV\u2192T\nT\u2192V\nT\u2192A\nV\u2192A\nA\u2192V\nA\u2192T\nIMP Model\nLoss Value: N\nOptimizer\nObjective: N\nFFN\nSelf-Attention\nData: 1\nx L/2 Dense Layers\nData: N\n\u2026\nLoss Value: 1\nGradient \nStep: N\nGradient \nStep: 1\n\u2026\n\u2026\nSample Data-Objective Pairs\n(Any Subset)\nPerform Data-Objective-Specific \nGradient Step\nPerform Inference and Loss \nComputation\n(b) The AGD-based multi-data multi-objective training overview.\nFigure 1: An overview of the IMP Training and Architecture. A mixture of datasets with varying\nmodalities, resolutions, and objectives are randomly sampled at each optimization step and fed into\nthe model. The model is updated alternatingly given the data-objective pairs. We use jax.jit to\ncompile and cache computation graphs to keep each step efficient while also allowing I/O shapes to\nchange for every optimization step without requiring any costly padding or masking strategies.\nwhich uses modality-specific experts for image-text modeling. Our work extends this concept further,\nintroducing video and audio modalities with alternating training on multiple tasks and resolutions\nwithout requiring modality-specific experts.\nDue to the inherent complexities of integrating modalities in one model, work has been done\nby simplifying the problem and focusing on a small set of universal objectives applicable to all\nmodalities [Yu et al., 2022, Wang et al., 2022]. Alternatively, some works focused on applying\npadding or masking strategies to handle the incompatible I/O signatures from different tasks. We\nfound in either case, this severely limits the ability for a model to leverage pre-existing large-scale\ndatasets or to scale to entirely new modalities. The historical reasons are that: (i) most existing\nmodels are designed for specific input modalities e.g., language [Brown et al., 2020, Chowdhery\net al., 2022], vision [Dosovitskiy et al., 2020], or audio [Baevski et al., 2020]; (ii) different modalities\nare typically delegated to separate network weights for the best performance [Radford et al., 2021, Jia\net al., 2021]; (iii) optimization difficulty with multiple modalities [Wu et al., 2022, Chen et al., 2022].\n3\nMethod\n3.1\nAlternating Gradient Descent (AGD)\nOne of the core pillars of our approach to multimodal understanding is task scalability. I.e., different\ncombinations of data and loss objectives should be interchangeable throughout training, while the\naddition of any new data or objective should not cause memory or computation overhead. We found\nthat a common issue in training large-scale foundation models in a distributed setting is that input\nsignatures and loss objectives need to be static to avoid major inefficiencies. Accelerated graph\ncompilation APIs allow for low-level graph optimizations that maximize hardware FLOPs utilization\non distributed devices such as GPUs and TPUs, but come at a cost of requiring static I/O signatures.\n3\nAlgorithm 1 Accelerated Multimodal AGD Algorithm\nInput: M (model), T (training steps), X (dataset-objective pairs), f (sampling function)\nInitialize: Model state St\nwhile t \u2264 T do\nSample data-objective pair (Dt, Lt) \u2208 X according to sampling function f(t, St)\nCompute forward pass predictions Pt = jit(M(Dt))\nCompute backwards pass on loss jit(Lt(Pt, Dt))\nUpdate model state St\nend while\nOne approach to handle the issue with the static input signature would be to use mixed batching,\nwhere all possible inputs are constructed, and inapplicable inputs for a given dataset are padded and\noutputs are masked accordingly at each training step. However, this comes at a great efficiency cost,\nsince the more tasks that are added the more time is spent computing on padded inputs. The issue\nwith having multiple objective functions is usually resolved by mixed mini-batching, where a batch is\ndivided to multiple mini-batches with their corresponding objective functions. The gradients for each\nmini-batch and objective function pair are accumulated across multiple mini-batches and the model\nweights are updated once using an aggregated gradient. However, this approach is also difficult to\nscale since the gradients across multiple mini-batches are accumulated in memory and per-task batch\nsize naturally reduces as we add more tasks.\nWe propose a more generic solution based on AGD [Jain et al., 2017], allowing any changes to the\nthree elements of the optimization system: inputs, model, and objective. AGD can be seen as a\nsuperset of the conventional SGD, where at each gradient step, a different objective may be optimized\ngiven different sets of model weights and/or input modalities. More specifically, any input modality\nwith arbitrary shape could consume any subset of the model while focusing on minimizing any\nspecific combination of the objective functions. According to Jain et al. [2017] it is proven that if each\nof such optimization steps are convex individually, an alternation between them leads to a guaranteed\nconvergence; as opposed to mini-batching where gradient accumulation could result in sub-optimal\nresults. From a technical point of view, this approach requires compiling multiple computation\ngraphs, one for each unique optimization step. To enable efficient execution of multiple computation\ngraphs, JAX offers native Just-in-Time (JIT) compilation with the jax.jit API2, which compiles\nthe graph-of-interest at runtime and compiles a new graph if a change in structure is seen in any of\nthe next optimization steps. Graphs themselves are cached by JAX in an in-memory lookup table so\nthat tasks only need to be compiled once. We empirically observe that graph memory consumption\nconstitute a negligible portion of the entire training. In our experiments, we tested up to 20 unique\ntask structures with no significant reduction in training speed, with compilation taking only 0.34% of\nthe total training time, up to the largest scales.\nThe main AGD algorithm for our multimodal framework is provided in Algorithm 1. We note that\nwe carefully design the loop to be as agnostic as possible with respect to the data-model-objective\ntriplet. This allows for greater flexibility in defining logic inside the model to handle the processing\nof individual modalities and input shapes. The sampling function can also incorporate state from the\noptimization process itself. E.g., the loss value of a given step can be used as a reward signal to affect\nthe sampling behavior in the next samplings [Piergiovanni et al., 2023, Mindermann et al., 2022].\nIn our default setup, we sample each unique task on a given step from a (single trial) Multinomial\ndistribution with probabilities directly proportional to the number of examples in each task. We defer\nmore complicated or reward-based settings to future studies.\n3.1.1\nAGD-Specific Efficiency Considerations\nWe notice that in each forward call, certain model states such as activations are stored by default\nto be used later in the backward call for gradient calculation. This is an established compute\noptimization at the low-level graph compilation in XLA and similar APIs. Although this trick helps a\nlot with the training time, it significantly consumes memory and creates memory overhead if more\nthan one graph is compiled and used during training. To reduce memory overhead, we use JAX\u2019s\n2https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html\n4\nnative rematerialization API, jax.checkpoint3 to save memory by not checkpointing any of the\nintermediate model states. In our experiments, we observe an average reduction of 70-80% TPU\nHBM usage while resulting in only 18-20% longer step times.\nWe also notice that large models with many different objectives may still incur long compilation\ntimes. Therefore, we apply scan-over-layers with jax.lax.scan, a method which rolls all of the\nTransformer layers into a single layer that is called multiple times using different weights (instead of\ncompiling the same function multiple times). This alone results in 15-30x faster compilation time\ndepending on the model length. We observe increasing relative time savings with larger model sizes.\nFurthermore, we accomplish these across distributed accelerators through the jax.pjit API4, which\ndistributes JIT compilation across multiple accelerators.\n3.2\nObjectives\nOur goal in designing the IMP model is to reuse objectives that have been shown to be robust for\nlearning each modality. Hence, we choose the two most established supervised and unsupervised\nlearning objectives: i. Supervised Classification using Softmax/Binary Cross-Entropy(SCE/BCE),\nii. Cross-Modal Noise-Contrastive Estimiation (NCE). Unless otherwise specified, we do not sum\nany of the above losses or accumulate gradients as would be done traditionally. Instead we apply\nbackprop on each objective individually with AGD.\n3.3\nArchitecture\nFigure 1 shows a high-level overview of the architecture of IMP, which consists of three main modules:\ni. The Embedder, which accepts specific modalities and embeds them in a shared modality-agnostic\nspace; ii. The MoE Encoder, which computes semantic contextual embeddings from the embedded\ntokens; iii. The Heads, which produce all the final predictions from the Encoder by re-projecting its\nembeddings back into a modality-specific space. We briefly explain each module here and provide\nmore details in Appendix.\nOne design decision important to multimodal modeling is how to allocate parameters to each modality.\nAs seen in works like BASIC [Pham et al., 2021], an asymmetric modality-specific design can be\nmore optimal than using a similar-sized model for each modality. However, this comes at the cost of\nrequiring additional hyperparameter tuning to find the optimal parameterization. As we show later in\nthe next section, we observe that through the use of model sparsification with MoE, a unified encoder\ndesign coupled with certain modality-specific pre- and post-encoder layers is more optimal than a\ntraditional multi-encoder setup as seen in CLIP model variants.\nWe follow VATT [Akbari et al., 2021], AudioMAE [Huang et al., 2022], and T5 [Raffel et al.,\n2020] to extract the vision, audio, and text embeddings, respectively. We add learnable 3D/2D/1D\npositional encodings to the embeddings and project them to a space with the same dimensionality as\nthe model\u2019s. We pass these embedded inputs regardless of modality as-is through the shared encoder,\nwhich is a standard Transformer architecture equipped with Mixture-of-Experts FFN layers. We\nfollow V-MoE [Riquelme et al., 2021] and LIMoE [Mustafa et al., 2022] for expert allocation. This\ncan be seen as an inductive bias, allowing each expert to be allocated to multiple modalities if the\noptimization benefits. One immediate benefit is that the addition of new modalities for fine-tuning\ndoes not need any specific changes to the encoder, unlike modality-specific experts which require\nadditional modifications and input handling [Wang et al., 2022, Shen et al., 2023].\nWe apply modality-specific heads on the encoder representations to produce the final outputs for loss\nand prediction calculations. For classification objectives, we apply a dataset-specific linear classifier\nto the average-pooled outputs. For noise-contrastive estimation (NCE), we closely follow the CLIP\narchitecture, applying separate feedforward heads for each modality-to-common-space projection.\n3.4\nMulti-Resolution Training\nOne major issue when training Transformers on video data is that computation and memory efficiency\nare usually bottlenecked due to Transformer\u2019s quadratic complexity as a function of the input length.\n3https://jax.readthedocs.io/en/latest/_autosummary/jax.checkpoint.html\n4https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html\n5\nTo counteract this, we propose to adjust batch size or resolution to compensate the additional temporal\ntokens, hence achieving a similar total number of input tokens compared to a single-frame still image.\nWe first fix a set tokens per batch T = B \u00d7 TF \u00d7 TH \u00d7 TW , which is factorized by the batch size\nB, frame tokens TF , height tokens TH, and width tokens TW representing each patchified video.\nWe observe that we can further factorize each batch by trading off different dimensions such that\nthe total number of input tokens per step are roughly equal so that peak memory usage is preserved.\nFor example, we can halve the spatial resolution while quadrupling the number of frames. This can\nincrease convergence especially at the start of training, and provide a more memory efficient encoding\nof each objective. Furthermore, we leverage DropToken [Akbari et al., 2021] as an additional method\nto reduce tokens per batch by randomly dropping a fixed ratio of tokens per example. We find that for\nTF temporal frame tokens, we can randomly drop a ratio of 1 \u2212\n1\nTF tokens per example to match the\nsame tokens per batch as images. For certain objectives we find that a different mix of trade-offs is\nmore optimal. For example, contrastive objectives favor large batch sizes, so we reduce the resolution\nor apply DropToken to be more memory efficient. On the other hand, classification objectives do not\nneed as large batch sizes for optimal convergence, hence we reduce the batch size while increasing\nthe spatiotemporal tokens.\n4\nExperiments and Results\n4.1\nTraining Setup\nDatasets.\nOur datasets consist of a diverse set of learnable signals across multiple modalities. We\nuse WebLI [Chen et al., 2022], LAION-400M [Schuhmann et al., 2021], WIT [Srinivasan et al.,\n2021], CC12M [Changpinyo et al., 2021], and VCC [Nagrani et al., 2022] for vision-text contrastive\nlearning; JFT-3B [Zhai et al., 2022], I21K [Ridnik et al., 2021], and WTS-70M [Stroud et al.,\n2020] for both supervised classification and label-based vision-text contrastive estimation (similar to\nBASIC [Pham et al., 2021]); HT100M [Miech et al., 2019] and AudioSet [Gemmeke et al., 2017] for\nvision-audio-text triplet contrastive loss (similar to VATT [Akbari et al., 2021]).\nWe use a proportionally weighted sampling algorithm, executing each task in succession. To ensure\nthat datasets are evenly sampled, we weight each task by the number of examples, normalized to a\nprobability distribution. For each dataset variant with different resolution sizes, we apply the same\nweight. For a fair evaluation on downstream tasks, we filter all near-domain examples from our\npretraining datasets (about 5M examples total).\nMulti-Resolution Strategy.\nIn our experiments, we always configure the input parameters so that\nthe number of frame tokens are always equal to 4. This will result in the base tokens per video batch\nbeing exactly 4x of image\u2019s. For video datasets, we construct three variants and uniformly sample\nfrom each variant during training: i. Reduce the resolution by half in each dimension, ii. Reduce the\nbatch size by 4x, iii. Apply DropToken d = 1 \u2212\n1\nTF = 0.75. For image datasets, we also apply a\nsimilar strategy but for the purpose of high-resolution learning. In addition to the base resolution, we\nhave two extra variants: i. Reduce the batch size by 4x and double each spatial dimension, ii. Apply\nDropToken d = 1 \u2212 1\n4 = 0.75.\nTraining Parameters.\nFor our final experiments, we train with a patch size of 4x16x16 on base\ninput resolutions of 16x256x256 and 4x256x256 on video and image modalities respectively, resulting\nin a total of 1024 and 256 patches per sample. The text inputs in ImageNet21K and JFT are truncated\nto 16 tokens to improve step efficiency with no loss of information, while keeping the text length of\nthe rest of the datasets to a maximum of 256 tokens. We use a base batch size of 65536 and train\nusing the Adam optimizer, a peak learning rate of 1e-3 with a cosine schedule, and apply no weight\ndecay. For MoE parameters, we apply experts-choose routing with a top-c capacity factor of 1.0\nand do not apply any jittering to the routing or other auxiliary losses. Training results in roughly\n16B examples seen, or about 5T tokens. Taken together, these datasets represent about 11B unique\nimage-text and video-audio-text examples.\nDuring inference, we evaluate on the largest available resolution that the model was trained on, i.e.,\n16x512x512, and use a total of 8 clips per video at approximately 12.5 fps on all evaluated datasets.\n6\nMODEL\nPPT\nTPU-DAYS\nIN1K\nC100\nK400\nK600\nK700\nUCF101\nHMDB51\nESC-50\nCLIP [Radford et al., 2021]\n400M\n-\n76.2\n-\n-\n-\n-\n-\n-\n-\nCoCa-B [Yu et al., 2022]\n380M\n1.8k\n82.6\n-\n-\n-\n-\n-\n-\n-\nX-CLIP [Ni et al., 2022]\n400M\n-\n-\n-\n65.2\n-\n-\n72.0\n-\n-\nBIKE [Wu et al., 2022]\n230M\n-\n-\n-\n-\n68.5\n-\n80.8\n52.8\n-\nText4Vis [Wu et al., 2022]\n230M\n-\n-\n-\n68.9\n-\n-\n85.8\n-\n-\nAudioCLIP [Gowda et al., 2021]\n430M\n-\n-\n-\n-\n-\n-\n-\n-\n69.4\nIMP-B\n86M\n120\n80.5\n82.4\n63.6\n62.1\n49.9\n64.2\n39.7\n32.8\nIMP-MoE-B\n90M\n150\n83.2\n84.9\n68.2\n65.7\n52.1\n88.7\n46.6\n47.8\nIMP-MoE-L\n350M\n1.5k\n83.9\n87.0\n77.0\n76.8\n68.3\n91.5\n59.1\n65.1\nLarge-scale models\nLIMoE [Mustafa et al., 2022]\n680M\n-\n84.1\n-\n-\n-\n-\n-\n-\n-\nLiT ViT-g [Chen et al., 2022]\n2B\n-\n84.5\n83.6\n-\n-\n-\n-\n-\n-\nCoCa [Yu et al., 2022]\n2B\n10k\n86.3\n-\n-\n-\n-\n-\n-\n-\nVideoCoCa [Yan et al., 2022]\n2B\n10.5k\n-\n-\n72.0\n70.1\n62.5\n86.6\n58.6\n-\nTable 1: Zero-Shot Classification (top-1) results on image, video, and audio datasets. IMP achieves\na new state-of-the-art on zero-shot video action recognition by a wide margin with significantly low\ntraining cost. Considering the total number of Parameters Per Token (PPT), IMP also significantly\noutperforms comparable models on zero-shot image classification.\n4.2\nMain Results\nWe scale up and tune IMP for best performance on video datasets and evaluate it on a variety of\ndownstream tasks and datasets to understand how it generalizes to other modalities. Table 1 shows\nthe zero-shot classification capabilities of the model on several image, video, and audio datasets.\nWe note that IMP significantly outperforms previous state-of-the-art regardless of the model size and\nsets new record on Kinetics [Kay et al., 2017, Carreira et al., 2018, 2019], UCF101 [Soomro et al.,\n2012], and HMDB-51 [Kuehne et al., 2011] top-1 accuracy. Compared to the previous state-of-the-art,\nVideoCoCa [Yan et al., 2022], we train IMP-MoE-L on 256 TPU v4 chips for 6 days, representing\nonly 15% of the total training cost of VideoCoCa. Considering the total number of parameters\nper token (PPT), IMP also outperforms the previous comparable state-of-the-art model, CoCa, on\nImageNet [Russakovsky et al., 2015] and CIFAR-100 with a relatively large margin. However, we\nobserve that the model falls behind the state-of-the-art in zero-shot audio classification on ESC-\n50 [Piczak, 2015]. This might be explained by the fact that the total training examples for audio\nmodality are almost negligible compared to image and video. Hence, the model has a very strong\nperformance on video and image. We argue that this could be resolved by simply introducing more\nsamples and a more balanced train scheduling method, which we differ to future studies.\n4.3\nAblation\nIn this section, we hightlight experimentation with some key results which motivate the chosen set of\nfeatures for our final IMP model. We refer the reader to Appendix for more ablation on several other\naspects of the model. The experiments in this section use IMP-S or IMP-B trained for 250k steps\nwith a base batch size of 8192. We set a fixed video/image resolution of 16x224x224/4x224x224\nusing a patch size of 4x16x16. Unless otherwise specified, we do not apply multi-scale resolution.\nCombined objectives are mutually beneficial.\nWe train the model on ImageNet21k with two\nobjectives: Noise-Contrastive Estimation (NCE) and Softmax Cross-Entropy (SCE) and explore\nthe following: i. train on the objectives separately, ii. combine the objectives by summing them,\nand iii. alternating (AGD) between the objectives on each step. In the case of alternating, for\na fair comparison so that training time is equivalent, we fix the same number of steps (250k) so\nthat each objective only optimizes 50% of the total steps (125k). We evaluate on ImageNet1k and\nCIFAR-100 by image-to-text retrieval and linear probing on the frozen model\u2019s features and report\nthe results in Table 2. It is not a surprise that classification objective benefits fine-tuning evals the\nmost, while contrastive objective benefits open vocabulary classification. However, we observe that\ncombining both objectives is better than optimizing on them individually. And alternating between\nthe objectives is better than non-AGD objective mixing. These results are similar to the findings of\nPolyViT [Likhosherstov et al., 2021], which report optimal performance on alternating the objectives,\nweighted by the size of each dataset. This motivates us to fix one objective per training step and\nalternate optimization between them.\n7\nIMAGENET1K\nCIFAR-100\nOBJECTIVE\nLINEAR\nI \u2192 T\nLINEAR\nI \u2192 T\nNCE\n39.8\n46.7\n84.0\n49.8\nSoftmax\n41.1\n-\n82.6\n-\nNCE + Softmax, Sum\n47.6\n46.7\n82.4\n51.6\nNCE + Softmax, Alternating\n49.9\n48.0\n84.1\n52.4\nTable 2: Combining multiple objectives during pre-training.. Alternating between both objectives\noffer the best performance, despite training on the same number of total steps.\nImageNet ZS\nCIFAR100 Linear\nCIFAR100 ZS\nFlickr T\nI\nFlickr I\nT\nCOCO T\nI\nCOCO I\nT\n10\n20\n30\n40\n50\n60\n70\n80\n90\n17.6\n61.5\n14.9\n28.4\n31.6\n15.9\n18.6\n39.1\n80.8\n48.1\n34.4\n40.0\n19.3\n24.2\n27.1\n83.4\n44.2\n34.2\n39.3\n20.8\n24.7\n49.4\n84.6\n60.0\n38.6\n44.6\n22.5\n28.5\n49.7\n85.2\n62.8\n51.5\n66.6\n25.7\n33.4\nCC\nCC + I21K (NCE)\nCC + I21K (Softmax)\nCC + I21K (NCE+Softmax)\nCC + I21K (NCE+Softmax) + LAION\nFigure 2: Combining multiple datasets and objectives using AGD. We integrate CC, I21k, and\nLAION with NCE and SCE using AGD and observe consistent improvement in downstream results.\nWe also observe that NCE and SCE are mutually beneficial. Further optimality is provided by adding\nlarger and more diverse datasets like LAION.\nImageNet ZS\nFlickr T\nI\nFlickr I\nT\nCOCO T\nI\nCOCO I\nT\n20\n30\n40\n50\n60\n70\n49.7\n51.5\n66.6\n25.7\n33.4\n51.3\n52.6\n68.2\n26.2\n34.5\n51.0\n52.7\n68.6\n25.9\n33.7\n50.1\n53.2\n69.2\n26.4\n33.8\n51.4\n54.4\n69.2\n27.2\n34.6\n224\u00d7224\n224\u00d7224\n320\u00d7320 (drop 0.5)\n224\u00d7224\n320\u00d7320 (batch 0.5\u00d7)\n224\u00d7224\n320\u00d7320 (drop 0.5)\n320\u00d7320 (batch 0.5\u00d7)\n224\u00d7224\n320\u00d7320 (drop 0.5)\n320\u00d7320 (batch 0.5\u00d7)\n160\u00d7160 (batch 2\u00d7)\nFigure 3:\nCombining multiple input sizes using AGD. We sample each variant with equal\nprobability. Downstream results improve as we add more variants to the training mixture.\nMulti-task multi-dataset AGD is also mutually beneficial.\nIn Figure 2, we compare the result\nof adding additional datasets to the pretraining mixture. We additionally compare results across\nFlickr30k [Young et al., 2014] and COCO [Lin et al., 2014] datasets. We start with CC12M dataset\nand gradually add new datasets and objectives. Most notably, we compare the addition of I21K\ndataset, showing complementary improvement when combining NCE and SCE objectives. Similar to\nI21K isolated experiments, adding SCE benefits the entire pretraining mixture. While SCE benefits\nzero-shot results, NCE benefits linear probing results too. Certain dataset combinations (CC+I21K,\nCC+LAION) cause instability at the beginning of training. Adding a classification objective has a\nstabilizing effect, significantly reducing the chance of slow convergence. Optimizing on LAION\ndirectly is difficult, but benefits training a lot more when mixed in with other datasets. This motivates\nus to further integrate a larger set of diverse datasets and objectives.\nMulti-scale resolution provides universal improvement.\nFigure 3 shows a comparison of using\ndifferent combinations of resolution, batch size, and DropToken as input. In all settings, we fix the\ntotal tokens per batch, and we ensure that all training runs use the same number of total steps. We\nsee that certain types of datasets respond well to DropToken while others may not. CC with double\nthe batch size and DropToken 0.5 improves zero-shot image classification. Droptoken + 320x320\nimage on I21K SCE pretrain is better for linear probing and image-text retrieval. Adding multiple\nversions of smaller batch size + higher res, DropToken + higher res, larger batch size + lower res, can\nsignificantly improve downstream results. We find that dynamic mixtures of resolution, batch size,\nand DropToken are always helpful.\n8\nImageNet ZS\nCIFAR100 Linear\nCIFAR100 ZS\nFlickr T\nI\nFlickr I\nT\nCOCO T\nI\nCOCO I\nT\n20\n30\n40\n50\n60\n70\n80\n90\n49.7\n85.2\n62.8\n51.5\n66.6\n25.7\n33.4\n60.9\n82.8\n62.5\n49.7\n65.6\n25.4\n33.8\n75.0\n84.9\n68.7\n65.3\n78.9\n35.4\n45.1\n80.7\n88.0\n74.9\n66.9\n80.4\n35.1\n49.9\nIMP-S\nIMP-MoE-S, 4 experts\nIMP-B\nIMP-MoE-B, 16 experts\nFigure 4: IMP with Mixture-of-Experts. Results show that using a modest 4 experts increases the\nmodel\u2019s accuracy substantially on ImageNet zero-shot evaluation. When we scale up the experts to\n16, we see a consistent and significant improvement across all downstream evaluations.\nImageNet ZS\nCIFAR100 Linear\nCIFAR100 ZS\nFlickr T\nI\nFlickr I\nT\nCOCO T\nI\nCOCO I\nT\nKinetics400 ZS\n30\n40\n50\n60\n70\n80\n90\n63.9\n84.7\n71.5\n64.7\n78.9\n37.0\n47.9\n34.9\n59.6\n84.7\n65.9\n60.3\n71.9\n37.9\n47.9\n52.5\n81.3\n87.3\n72.6\n66.3\n81.4\n41.8\n54.3\n36.2\n83.2\n88.6\n74.1\n67.0\n80.9\n38.1\n48.4\n67.1\n82.2\n88.6\n73.9\n65.8\n78.6\n38.2\n48.3\n65.8\nIMP-B (image only)\nIMP-B (image + video)\nIMP-MoE-B, 16 experts (image only)\nIMP-MoE-B, 16 experts (image + video)\nIMP-MoE-B, 16 experts (image + video + audio)\nFigure 5: Improved multimodal perception using MoE. Results show significant improvement on\ndiverse multimodal perception when we use the MoE variant of IMP. The addition of audio reduces\naccuracy on image and video metrics across the board, but is much less prominent when using MoE.\nImageNet ZS\nCIFAR100 Linear\nCIFAR100 ZS\nCOCO T\nI\nCOCO I\nT\n30\n40\n50\n60\n70\n80\n90\n57.7\n81.0\n66.0\n35.3\n59.8\n84.2\n69.4\n38.5\n47.8\n63.2\n84.1\n71.2\n40.1\n52.3\n68.1\n85.6\n72.5\n39.7\n51.1\nTwo Tower (82M)\nSingle Tower (86M)\nTwo Tower (172M)\nSingle Tower MoE, 4 experts (103M)\nFigure 6: Comparison of single-tower vs. multi-tower designs on IMP-B. A single-tower MoE\nmodel is both parameter efficient and compute efficient compared to multi-tower dense variants.\nMoE provides universal improvement across modalities, and resolves the single-tower encoder\nparameter bottleneck.\nThe main challenge in designing a unified encoder tower as we have\ndescribed is that parameters must be split between multiple modalities, thus harming accuracy.\nCompared to a two-tower contrastive model, the encoder of a unified image-text model contains half\nthe parameters, while keeping training efficiency the same. One direction we explore is whether a\nlarge increase in parameters from MoE is sufficient to resolve parameter bottlenecks. In Figure 4,\nwe observe that simply replacing a dense model with an equivalent MoE model with just 4 experts,\nwe can provide a large gain in accuracy, especially for zero-shot metrics. This provides a promising\nindication that MoEs can be used to bridge the multimodal gap. We observe that with the addition\nof MoE, we can significantly close the gap between multiple modalities as seen in Figure 5. Since\nexperts are free to choose which tokens are allocated to different experts, we observe strong alignment\nbetween experts and modalities.\nSingle-tower MoE outperforms multi-tower dense variants.\nWe find that out of all the variants\nwe tested, a unified MoE encoder provided the most parameter and compute efficient design, while\nsignificantly outperforming a multi-encoder modality-specific model in downstream results, as seen\nin Figure 6. When comparing two-tower models, we can either split the parameters to be roughly\n9\nequal in size to a single tower, or duplicate the towers to double the parameter count while providing\nequivalent computation. We observe that multi-tower dense models are more compute efficient\nthan single-tower dense models, but less parameter efficient. However, a single-tower MoE model\nis both more compute and parameter efficient than all variants, showing improved generalization\nand using fewer parameters with the same compute budget as a multi-tower dense model. These\nresults show universal superior parameter and compute efficiency and higher accuracy by using\njust 4 experts. This observation suggests that our method can be used for integrated multimodal\nmulti-task modeling without worrying about the complications of modality-specific design choices or\ndownstream performance degradation as observed in previous modality-agnostic designs [Akbari\net al., 2021].\n5\nConclusion\nIn this paper we presented an integrated training and modeling approach for multimodal perception\nusing AGD and MoE. We observed that AGD enables task scalability and multi-resolution training,\nwhich improves the training convergence and the model\u2019s generalization capabilities. On the other\nhand, we found that MoE can play a very important role in integrating multiple modalities into one\nunified model. Given these findings, we scaled the model with hyperparamters tuned specifically for\nvideo understanding and achieved state-of-the-art performance in zero-shot video action recognition\nwith a significant margin. Furthermore, we observed that the model also generalizes on other\nmodalities and achieves competitive downstream results. In a nutshell, IMP opens a door to data (e.g.,\nmodality, resolution, etc.) and task scalability \u2014 two important directions that have been neglected\nin many multimodal understanding works due to the inherent technical limitations. Due to the vast\nrange of elements involved in this system, we defer multiple directions to be explored in future work:\n1. generative objectives and model architectures, 2. causal MoE for generation, 3. sophisticated\nmethods for data-objective sampling, 4. more downstream evaluations.\nAcknowledgments and Disclosure of Funding\nWe would like to thank Joan Puigcerver, Carlos Riquelme, and Basil Mustafa for their advice on\nMoE implementation and analysis; Anselm Levskaya for his help with advanced core JAX and Flax\nimplementation; the T5X team for their support for scalable model partitioning, and Erica Moreira\nand Victor Gomez for their help with resource allocation.\nReferences\n[1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\nGreg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\nAndrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,\nManjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek\nMurray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal\nTalwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete\nWarden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-\nscale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.\norg/. Software available from tensorflow.org.\n[2] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and\nBoqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video,\naudio and text. NeurIPS, 2021.\n[3] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A\nframework for self-supervised learning of speech representations. NeurIPS, 2020.\n[4] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and\nQiao Zhang. JAX: composable transformations of Python+NumPy programs. 2018. URL\nhttp://github.com/google/jax.\n10\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. NeurIPS, 2020.\n[6] Gemma A Calvert. Crossmodal processing in the human brain: insights from functional\nneuroimaging studies. Cerebral cortex, 2001.\n[7] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A\nshort note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018.\n[8] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-\n700 human action dataset. arXiv preprint arXiv:1907.06987, 2019.\n[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.\n[10] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz,\nSebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled\nmultilingual language-image model. arXiv preprint arXiv:2209.06794, 2022.\n[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[12] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin\nGilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.\nScaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023.\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[14] Jon Driver and Toemme Noesselt. Multisensory interplay reveals crossmodal influences on\n\u2018sensory-specific\u2019brain regions, neural responses, and judgments. Neuron, 2008.\n[15] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing\nMoore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset\nfor audio events. In ICASSP, 2017.\n[16] Shreyank N Gowda, Marcus Rohrbach, and Laura Sevilla-Lara. Smart frame selection for\naction recognition. In AAAI, 2021.\n[17] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian\nMetze, and Christoph Feichtenhofer. Masked autoencoders that listen. NeurIPS, 2022.\n[18] Prateek Jain, Purushottam Kar, et al. Non-convex optimization for machine learning. Founda-\ntions and Trends\u00ae in Machine Learning, 2017.\n[19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In ICML. PMLR, 2021.\n[20] Norman P Jouppi, Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James Laudon,\nCliff Young, and David Patterson. A domain-specific supercomputer for training deep neural\nnetworks. Communications of the ACM, 2020.\n[21] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-\nnarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human\naction video dataset. arXiv preprint arXiv:1705.06950, 2017.\n[22] Hildegard Kuehne, Hueihan Jhuang, Est\u00edbaliz Garrote, Tomaso Poggio, and Thomas Serre.\nHmdb: a large video database for human motion recognition. In ICCV, 2011.\n11\n[23] Valerii Likhosherstov, Anurag Arnab, Krzysztof Choromanski, Mario Lucic, Yi Tay, Adrian\nWeller, and Mostafa Dehghani. Polyvit: Co-training vision transformers on images, videos and\naudio. arXiv preprint arXiv:2111.12993, 2021.\n[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,\n2014.\n[25] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and\nJosef Sivic. Howto100m: Learning a text-video embedding by watching hundred million\nnarrated video clips. In ICCV, 2019.\n[26] S\u00f6ren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch,\nWinnie Xu, Benedikt H\u00f6ltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, et al.\nPrioritized training on points that are learnable, worth learning, and not yet learnt. In ICML.\nPMLR, 2022.\n[27] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin-\nson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting.\nIn Proceedings of the conference on fairness, accountability, and transparency, 2019.\n[28] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Mul-\ntimodal contrastive learning with limoe: the language-image mixture of experts. In NeurIPS,\n2022.\n[29] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun,\nand Cordelia Schmid. Learning audio-video modalities from image captions. arXiv preprint\narXiv:2204.00679, 2022.\n[30] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu,\nShiming Xiang, and Haibin Ling. Expanding language-image pretrained models for general\nvideo recognition. In ECCV, 2022.\n[31] Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria A Zuluaga. Improved\noptimization strategies for deep multi-task networks. arXiv preprint arXiv:2109.11678, 2021.\n[32] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu,\nJiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for\nopen-vocabulary image classification. arXiv preprint arXiv: 2111.10050, 2021.\n[33] Karol J Piczak. Esc: Dataset for environmental sound classification. In ACM MM, 2015.\n[34] AJ Piergiovanni, Weicheng Kuo, Wei Li, and Anelia Angelova. Dynamic pretraining of\nvision-language models, 2023. URL https://openreview.net/forum?id=QcffIcjq8bl.\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML. PMLR, 2021.\n[36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. JMLR, 2020.\n[37] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining\nfor the masses. arXiv preprint arXiv:2104.10972, 2021.\n[38] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9\nSusano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.\nNeurIPS, 2021.\n[39] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel\nAndor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. Scaling up models\nand data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022.\n12\n[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.\nImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.\n[41] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[42] Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong He. Scaling\nvision-language models with sparse mixture of experts. arXiv preprint arXiv:2303.07226, 2023.\n[43] Linda Smith and Michael Gasser. The development of embodied cognition: Six lessons from\nbabies. Artificial life, 2005.\n[44] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human\nactions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n[45] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit:\nWikipedia-based image text dataset for multimodal multilingual machine learning. In SIGIR,\n2021.\n[46] Jonathan C Stroud, Zhichao Lu, Chen Sun, Jia Deng, Rahul Sukthankar, Cordelia Schmid, and\nDavid A Ross. Learning video representations from textual web supervision. arXiv preprint\narXiv:2007.14937, 2020.\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.\n[48] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language:\nBeit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442,\n2022.\n[49] Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer, and Philipp Krahenbuhl.\nA multigrid method for efficiently training video models. In CVPR, 2020.\n[50] Junru Wu, Yi Liang, Feng Han, Hassan Akbari, Zhangyang Wang, and Cong Yu. Scaling\nmultimodal pre-training via cross-modality gradient harmonization. In NeurIPS, 2022.\n[51] Wenhao Wu, Zhun Sun, and Wanli Ouyang. Transferring textual knowledge for visual recogni-\ntion. arXiv preprint arXiv:2207.01297, 2022.\n[52] Wenhao Wu, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, and Wanli Ouyang.\nBidirectional cross-modal knowledge exploration for video recognition with pre-trained vision-\nlanguage models. arXiv preprint arXiv:2301.00182, 2022.\n[53] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui\nYu. Video-text modeling with zero-shot transfer from contrastive captioners. arXiv preprint\narXiv:2212.04979, 2022.\n[54] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions\nto visual denotations: New similarity metrics for semantic inference over event descriptions.\nTransactions of the Association for Computational Linguistics, 2014.\n[55] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui\nWu.\nCoca: Contrastive captioners are image-text foundation models.\narXiv preprint\narXiv:2205.01917, 2022.\n[56] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transform-\ners. In CVPR, 2022.\n[57] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai,\nZhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing. In\nNeurIPS, 2022.\n13\nA\nArchitecture\nA.1\nEmbeddings\nFor vision modalities, we use the VATT [2] scheme to patchify each 3D video tensor. We define a\nvideo tensor of size F \u00d7H \u00d7W with F frames and H \u00d7W resolution using a patch size of f \u00d7h\u00d7w,\nproducing F\nf \u00d7 H\nh \u00d7 W\nw \u00d7 3 voxels that are flattened into a single sequence. This is followed by\nlinearly projecting the sequence into the model\u2019s hidden size. To allow for more robust generalization,\nwe treat images as a special case of a video, assuming sequences are of shape f \u00d7 H \u00d7 W and tiling\nframes f times to fit in a single patch. For our base model, we use a patch kernel size of 4x16x16,\nup to 16 frames, and resolutions up to 512x512. Following VATT, to allow the model to adapt to\ndifferent resolution scales, we apply learnable positional encodings to each patch position, which\nconsist of the sum of 3 embedding tables along each separate axis: one for the temporal dimension,\nand two for the spatial dimensions of the video patches.\nFor text, we apply T5 encoding [36] using the default English vocabulary with 32k SentencePiece\ntokens, which are embedded into the same hidden space size as image patch embeddings.\nFor audio, we apply both waveform and audio spectrogram as input. For spectrogram, following\nAudioMAE [17], after downsampling the audio waveform to 16000 kHz, we extract Mel-spectrograms\nwith a duration of 8 seconds, producing 128 feature vectors with 128 dimensions each. We apply a\npatch kernel size of 16x16 to produce 64 total patches as input. For waveform, we use a kernel size\nof 256 samples and embed up to 256 tokens.\nWe use a separate learned positional embeddings for the linear sequence of text tokens and audio\npatches similar to the video encoding scheme above. To be able to handle different numbers of\npatches across different dimensions, the positional encoding of vision modalities needs to be handled\nwith special care. Unlike the 1-dimensional sequences of text and audio waveform which can be\ntruncated to a given length, the presence of 2D spatial dimensions mean that images with double the\npatches along a dimension should be subdivided into quadrants so that adjacent positions are close to\neach other in the embedding space. We accomplish this using a dilated positional encoding. For a\ngiven dimension a spatial positional encoding of B buckets, if we encode a resolution with P patches,\nwe dilate the positional encoding with a stride of B\nP . We treat spectrograms as 2D images and apply\nthe same dilated encoding logic to them accordingly. In the case of the temporal dimension in video,\nwe treat it the same as a 1-dimensional truncation independent of the spatial dimensions, which is\napplied in the same way for text.\nA.2\nMoE Encoder\nFor all MoE encoders, we use expert-choice routing [57], which provides a strong baseline for all of\nthe four modalities. Using expert-choice (top-c) routing, we observe much higher accuracy compared\nto the standard tokens-choose (top-k) routing. This is because experts-choose routing guarantees\neven load balancing, which we find to be an important factor for using an encoder shared across\nmodalities. We find that only applying MoE to the last 50% of layers provided similar accuracy to\napplying them for all layers, therefore we use this setting for all MoE model variants.\nWe observe that contrastive optimization with MoE produces unstable output, often when introducing\nnoisy text labels. This instability results in a loss divergence roughly within the first 30k-80k training\nsteps. Similar to ViT-22B [12], we find that applying a layer normalization after the key and query\nmatrices (QK LayerNorm) in the self-attention layers removes all such divergence issues in our\ntraining, hence we use this trick in all of our model variants.\nA.3\nHeads\nWe apply global average pooling operation across the entire output sequence of the encoder, and use\nthe resulting vector as the global features for classification and noise-contrastive estimation objectives.\nFor classification objectives, we apply a dataset-specific linear classifier to the average-pooled outputs.\nFor noise-contrastive estimation (NCE), we closely follow the CLIP architecture, applying separate\nfeedforward heads for each modality-to-common-space projection. Each feedforward head consists\nof a two-layer linear projection with GeLU activation in between. The projection dimension size is\nthe same as the model\u2019s hidden size.\n14\nMODEL\nPARAMS (DENSE)\nPARAMS (SPARSE)\n# EXPERTS\n# LAYERS\nHIDDEN SIZE\nFFN SIZE\nIMP-S\n21M\n40M\n4\n12\n384\n1536\nIMP-B\n86M\n400M\n16\n12\n768\n3072\nIMP-L\n300M\n2B\n16\n24\n1024\n4096\nTable 3: Comparison of IMP Architectures. We provide parameters for dense and sparse MoE\nvariants. Note that we only apply MoE to the last half of the layers in the encoder.\nA.4\nModel Sizes\nTable 3 provides a description of model sizes. We provide results for three main variants, IMP-S,\nIMP-B, and IMP-L corresponding to encoder sizes of ViT-S, ViT-B, and ViT-L respectively [56]. We\nalso provide three additional sparse MoE sub-variants, which are indicated as IMP-MoE.\nB\nTraining Setup\nB.1\nDatasets\nFor large-scale pretraining, we use the following datasets:\n1. WebLI [10] consisting of 4B English-only image-text pairs. We use this dataset for image-\ntext contrastive loss.\n2. JFT-3B [56], which contains a large collection of multi-class labels per image. We follow\nBASIC [32] for encoding multiclass indices as text and use the dataset for image-text\ncontrastive loss as well as supervised classification loss.\n3. LAION-400M [41], a public dataset of 400M image-text pairs for image-text contrastive\nloss.\n4. Wikipedia Image Text (WIT) [45] with 37M image-text pairs sourced from Wikipedia for\nimage-text contrastive loss.\n5. Conceptual Captions (CC12M) [9] consisting of 12 M image-caption pairs, used for image-\ntext contrastive loss.\n6. ImageNet21K (I21K) [37] with 11M labeled images for image-text contrastive loss and\nsupervised classification loss.\n7. VideoCC (VCC) [29], a video dataset with a variant expanded to 1B English video-text pairs\nfor video-audio-text triplet contrastive loss.\n8. HowTo100M (HT100M) [25] consisting of \u223c100M video-audio-ASR triplets, used for\nvideo-audio-text triplet contrastive loss.\n9. Weak Text Supervision (WTS-70M) [46], a dataset of 70M video clips obtained based on\n700 action classes. We use this variant for video-text contrastive loss as well as supervised\nclassification loss similar to JFT-3B and IN21k.\n10. AudioSet [15] for video-audio-text triplet contrastive loss.\nC\nAdditional Ablation\nInstabilities of contrastive loss on MoEs can be reduced with diverse data mixtures and QK\nLayerNorm.\nWe observe that a combination of MoE training with contrastive losses can lead to\ndivergence, as seen in Figure 7. As seen in the figure (see also Table 6), the addition of multiple\ndatasets, even under the same objective, can be detrimental to the optimization process. At the\nbeginning of contrastive training on CC and LAION datasets, we observe a loss plateau, where the\nloss remains relatively constant from the start of training, and the model fails to start converging for a\nlong period of time. On the other hand, if we apply the same dataset but add a softmax objective from\nthe ImageNet21K dataset, we no longer observe a loss plateau, as softmax tends to be more stable for\noptimization processes than contrastive losses. This highlights the importance of selecting the right\ndataset mixture, especially at the start of training where the inherent nature of the random parameters\ncan make it difficult for some task gradients to solidify a good direction in the optimization process.\n15\n0\n20000\n40000\n60000\n80000\nStep\n5\n6\n7\n8\n9\n10\n11\nLoss\nDefault setup\nPoor data mixture\nNo QK LayerNorm\nNo MoE\nFigure 7: Plot of loss with different training setups in the first 100k steps of IMP-MoE-B. We\nobserve that our MoE model with QK LayerNorm and using a diverse mixture of datasets reduces\ninstability and produces the best loss convergence. A poor data mixture (e.g., CC + LAION) can\ncause a loss plateau, while adding QK LayerNorm in self-attention is important for avoiding loss\ndivergence early in training.\nImageNet ZS\nFlickr T\nI\nFlickr I\nT\nCOCO T\nI\nCOCO I\nT\nKinetics ZS\nUCF Linear\nUCF ZS\nESC Linear\nESC ZS\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n49.7\n51.5\n66.6\n25.7\n33.4\n20.2\n82.6\n33.7\n0.0\n0.0\n48.8\n49.0\n64.3\n24.4\n31.2\n58.0\n93.7\n19.9\n0.0\n0.0\n49.6\n48.5\n62.2\n23.7\n31.6\n51.6\n91.2\n26.8\n58.7\n3.0\n48.8\n46.7\n60.6\n22.5\n30.5\n50.5\n90.0\n21.1\n69.5\n26.2\nImage only\nImage + Video (no audio)\nImage + Video (with audio)\nImage + Video (with audio) + Audio only\nFigure 8:\nComparison of the addition of video and audio datasets on IMP-S. The addition\nof video data (i.e., WTS) substantially improves video accuracy (Kinetics400, UCF) at the cost\nof slightly reducing the model\u2019s accuracy on image tasks (ImageNet, Flickr30k, COCO). The\nintroduction of audio in the contrastive loss (i.e., AudioSet) also reduces both image and video\naccuracy slightly, but enables fine-tuning on audio data (ESC). Finally, the introduction of a dedicated\naudio class contrastive objective hurts image and video accuracy the most, but enables zero-shot\naudio classification.\nWe also observe certain divergences which occur early in training, and we found the magnitude of\ngradient updates can get large in the attention inputs. This can cause training to completely destabilize,\nand enter a similar loss plateau. Therefore, we apply QK LayerNorm, which we observe to have\nprevented such divergence across all of our experiments.\nAdding more modalities hurt single tower (dense) encoder accuracy.\nWe compare the addition\nof more modalities via video datasets in Figure 8. Adding a video dataset (i.e., WTS) to pretraining\nboosts Kinetics classification significantly, allowing the model to more easily discriminate between\naction classes. However, the addition of video data may harm image classification performance\nslightly, especially when parameters are constrained. Likewise, the addition of audio data with\nvideo has a slight negative impact, and the addition of dedicated audio classification dataset (i.e.,\nAudioSet) has an even larger negative impact. This may be due to the additional audio-text contrastive\nsignal which requires the network to allocate dedicated processing for audio-to-text understanding.\nTherefore, a standard single tower encoder is not sufficient for optimal multimodal learning due to\nparameter bottlenecks.\nExperts-choose routing is crucial for strong single tower performance.\nWe test the effect of\nexperts-choose routing vs. tokens-choose in Table 4. Similar to findings in VL-MoE [42], we observe\nthat separating experts by modality in the case of tokens-choose routing is useful for improving\n16\n# TOWERS\n# EXPERTS\nROUTER\nPARAMS\nIMAGENET ZS\n1\n1 (dense)\nN/A\n86M\n59.8\n1\n4\ntokens-choose\n103M\n62.5\n2\n4\ntokens-choose\n206M\n65.7\n1\n4\nexperts-choose\n103M\n68.1\n2\n4\nexperts-choose\n206M\n68.4\nTable 4: Comparison of single tower MoE designs on IMP-B, comparing experts-choose and\ntokens-choose approaches on ImageNet class retrieval. The most accurate and parameter efficient\nconfiguration is a single-tower experts-choose model. For tokens choose, we use a maximum capacity\nfactor of 1.05 as in V-MoE so that training times are roughly equivalent.\naccuracy. However, when we switch to experts-choose routing, we find that performance increases\nfurther, and a multi-tower model is similar enough in accuracy that separating them per modality is no\nlonger necessary. This allows for a much simpler model design, and we can fine-tune new modalities\nin the encoder without any additional setup or new experts required.\nInserting diverse prompts during training helps improve zero-shot classification.\nIn Table 5,\nwe evaluate different prompt settings during pretraining. Contrasting with prior works which only\napply prompts during evaluation, we observe strong gains when randomizing the prompts during\ntraining. One interesting exception is in CIFAR-100, which benefits from training and testing on no\nprompts at all. We observe the trend that simpler prompts are more useful for datasets with a smaller\nnumber of classes. We typically care more about results of large-scale datasets, so we apply prompt\ndiversification by default.\nCONFIGURATION\nIN LINEAR\nIN ZS\nC100 LINEAR\nC100 ZS\nF30K T\u2192I\nF30K I\u2192T\nUCF LINEAR\nNo Prompt\n46.6\n51.4\n83.9\n67.5\n50.3\n66.8\n83.0\nCLIP Prompt\n47.7\n55.8\n83.7\n57.8\n49.7\n66.5\n81.9\nIMP Prompt\n47.7\n56.5\n83.8\n60.5\n51.2\n66.3\n84.1\nTable 5:\nPrompt comparison of IMP. We compare three settings of train-time prompts with\nincreasing diversity. Results show that randomized prompts in training tend to significantly improve\nmetrics on classification tasks.\nStability of optimization.\nIn general, we observe the following situations where optimization can\nbecome more unstable: (1) Increase in dataset diversity; (2) Increase in model size; (3) Increase\nin batch size. In Table 6, we show that training on NCE alone can cause instability issues during\ntraining, especially for noisy text datasets. But with the addition of more clean data sources and\nsoftmax objectives, we can greatly reduce the instability.\nDATASETS\nIN LINEAR\nIN ZS\nC100 LINEAR\nC100 ZS\nF30K T\u2192I\nF30K I\u2192T\nCOCO T\u2192I\nCOCO I\u2192T\nUCF LINEAR\nCC\n15.2\n17.6\n61.5\n14.9\n28.4\n31.6\n15.9\n18.6\n73.3\nCC + LAION\nDiverged\n-\n-\n-\n-\n-\n-\n-\n-\nCC + I21K (NCE only)\n33.5\n39.1\n80.8\n48.1\n34.4\n40.0\n19.3\n24.2\n78.3\nCC + I21K (Softmax only)\n42.5\n27.1\n83.4\n44.2\n34.2\n39.3\n20.8\n24.7\n82.4\nCC + I21K (NCE+Softmax)\n49.0\n49.4\n84.6\n60.0\n38.6\n44.6\n22.5\n28.5\n82.9\nCC + I21K (NCE+Softmax) + LAION\n46.9\n49.7\n85.2\n62.8\n51.5\n66.6\n25.7\n33.4\n82.6\nTable 6: Comparison of datasets & objectives with AGD on IMP-S. We integrate Conceptual\nCaptions (CC) with contrastive (NCE) loss, and ImageNet21K (I21K) with NCE and softmax loss.\nThe addition of both NCE and Softmax objectives from classification-based pretraining datasets are\nmutually beneficial with the retrieval-based pretraining datasets, observing best performance with the\ncombination of both objectives. Further optimality is provided by adding larger, more diverse dataset\nlike LAION-400M. However, we find that LAION causes optimization on contrastive objectives to\nbecome unstable, so softmax loss can greatly stabilize this noisier dataset.\n17\nD\nFramework Modules\nTo make IMP possible, we have developed a framework for AGD which we call MAX, abbreviated\nfrom Multi-task Multi-modal training based on JAX. MAX provides an end-to-end framework for\nrunning arbitrary multimodal data on models efficiently. An overview of modules used in MAX is\nprovided in Figure 9.\njax.pjit\nPartitioning\nFlax\nModels\nData\nCheckpointing\nTensorStore\nConfig\nPython dataclass\nMAX\nTPU\nGPU\nCPU\nJAX\nXLA\nOptax\nOptimization\nTFDS\nDMVR\nSeqIO\nFigure 9: Modules used to implement our MAX framework.\nThe data pipeline defines data using TensorFlow Datasets (TFDS) [1] and SeqIO [39] registries for\nvision and language tasks. Preprocessing of text is provided by SeqIO, while image, video, and audio\npreprocessors are provided by DeepMind Video Readers (DMVR)5. Datasets are emitted from a\ntf.data.Dataset object provide a key-value signature that can be tightly integrated with models.\nThis signature should be consistent with the model\u2019s expected input structure. For IMP, we define\nnamed keys for each modality and emit the applicable modalities from each dataset. Each modality\ncan further provide optional metadata, information that specify how to properly execute or route the\ninput to various modules.\nModels are built as native Flax6 modules, partitioned jax.pjit7, and optimized by transforms\ndefined in Optax8. The JAX [4] framework provides a core selection of primitives that inferface with\nXLA9, a library that compiles and optimizes computation graphs across different devices. We use\nTensorStore10 to efficiently checkpoint and restore partitioned model parameters using async parallel\ndispatch. Configuration is specified according to Python dataclasses which can be overridden. This\nallows the creation of many variants of datasets, models, and experiments without excessive code\nduplication.\nOn each training step, the training loop samples a dataset-objective pair, passing inputs from the\ndataset directly into the model. Note that the routing of inputs across different model components\nis specifically avoided in the training loop logic to prevent the training process from being tied to a\nspecific way to handle different input types. Instead, the model itself handles the interpretation of\nany combination inputs provided from the dataset and produces a named collection of outputs. Loss\nfunctions are applied in the training loop after sampling a dataset-objective pair and executing the\nmodel\u2019s forward pass. Together, this provides a modular way to interchange datasets, models, and\nloss functions.\nWe leverage training and inference step partitioning from jax.pjit, with further model and data\nparallelism abstractions provided by the t5x framework [39] to partition model weights and activations\nacross devices. On a high level, PJIT enables the use of dynamic graph compilation at runtime across\nmany distributed devices. For each unique dataset-objective pair, PJIT will compile a new computation\ngraph. These graphs are all cached so on subsequent iterations re-compilation overhead is minimized.\nThis is used in conjunction with MoE to efficiently dispatch sparse weights across multiple devices\n5https://github.com/deepmind/dmvr\n6https://github.com/google/flax\n7https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html\n8https://github.com/deepmind/optax\n9https://www.tensorflow.org/xla\n10https://github.com/google/tensorstore\n18\nwhile minimizing communication overhead. In conjunction with the partitioner, we initialize states\nby defining a set of specs of shapes that the model should accept as input, using jax.eval_shape.\nTo efficiently run each training step, we pre-initialize the PRNG states of all training steps before any\ntraining takes place.\nE\nIMP Model Card\nWe present the IMP model card in Table 7, following [27].\nModel Summary\nModel Architecture\nIMP is a multimodal sequence-to-sequence Transformer [47]\nencoder. It takes image, video, audio and text as inputs to the\nencoder and produces their feature embeddings as outputs.\nInput(s)\nRGB image, RGB video frame, audio waveform, audio spec-\ntrogram, text.\nOutput(s)\nFeature embeddings corresponding to the inputs.\nUsage\nApplication\nThe model is for research prototype and the current version is\nnot available for the broader public usage.\nKnown Caveats\nNo.\nSystem Type\nSystem Description\nThis is a standalone model.\nUpstream Dependencies\nNo.\nDownstream Dependencies\nNo.\nImplementation Frameworks\nHardware & Software\nHardware: TPU [20].\nSoftware: T5X [39], JAX [4], Flaxformer11, MAX\nDetails are in Section D.\nCompute Requirements\nReported in Section 4.3.\nModel Characteristics\nModel Initialization\nThe model is trained from scratch with random initialization.\nModel Status\nThis is a static model trained on offline datasets.\nModel Stats\nThe largest IMP model has 2B parameters for its sparse variant\nand 300M parameters for its dense variant.\nData Overview\nTraining dataset\nThe model is pre-trained on the following mixture of datasets:\nDetails are in Section 4.1.\nEvaluation and Fine-tuning Dataset\n\u2022 Image classification: CIFAR, ImageNet\n\u2022 Video classification: UCF101, HMDB51, Kinet-\nics400, Kinetics600, Kinetics700\n\u2022 Audio classification: ESC\n\u2022 Image to text / text to image retrieval: Flickr30k,\nCOCO\n11https://github.com/google/flaxformer\n19\nEvaluation Results\nEvaluation Results\nReported in Section 4.\nModel Usage & Limitations\nSensitive Use\nReported in Section F\nKnown Limitations\nReported in Section F.\nEthical Considerations & Risks\nReported in Section F.\nTable 7: IMP model card.\nF\nLimitations & Future Work\nOur approach provides a promising new scaling direction that avoids many of the pitfalls when\ndealing with multimodal training. However, there are still some remaining obstacles from fully\nrealizing this approach.\nWe note that our model provides exceptional performance in zero-shot video understanding, but falls\nslightly short in zero-shot image and audio understanding. We believe our training signals are have\nfavored video understanding due to a combination of factors, including high incidence of vision data,\na large sampling rate on vision tasks, and optimization losses converging faster on video. With a\nlarger set of more diverse data and tasks (e.g., text-only and audio-only pretraining), we believe we\ncan provide further improvements on these modalities without introducing any signficant training\ncost.\nOne unsolved question is how to best combine objectives during training. We have only tested\nconfigurations of tasks that are sampled equally across training. Instead, we can provide a more\nsophisticated curriculum to the model by scheduling tasks depending on the current step. There has\nbeen work showing further efficiency and accuracy improvements when scheduling different types of\ntasks at various stages [49, 34].\nAnother obstacle is the use of multimodal MoE in the generative setting. Experts-choose routing has\nbeen integral to allowing a high performance single tower encoder model, but due to its requirement to\naggregate tokens across the sequence, it is not by itself suitable for causal objectives like autoregressive\nsequence prediction as used in language modeling. Some additional modifications may make this\npossible, however.\n20\n"
  },
  {
    "title": "Multi-Task End-to-End Training Improves Conversational Recommendation",
    "link": "https://arxiv.org/pdf/2305.06218.pdf",
    "upvote": "1",
    "text": "Multi-Task End-to-End Training Improves Conversational\nRecommendation\nNaveen Ram, Dima Kuzmin, Ellie Ka In Chio\nMoustafa Farid Alzantot, Santiago Ontanon, Ambarish Jash, and Judith Yue Li\nGoogle Research\n[naveenram,dimakuzmin,echio,mfarid,santiontanon,ajash,judithyueli]@google.com\nAbstract\nIn this paper, we analyze the performance of\na multitask end-to-end transformer model on\nthe task of conversational recommendations,\nwhich aim to provide recommendations based\non a user\u2019s explicit preferences expressed in\ndialogue. While previous works in this area\nadopt complex multi-component approaches\nwhere the dialogue management and entity\nrecommendation tasks are handled by sep-\narate components, we show that a uni\ufb01ed\ntransformer model, based on the T5 text-to-\ntext transformer model, can perform compet-\nitively in both recommending relevant items\nand generating conversation dialogue.\nWe\n\ufb01ne-tune our model on the ReDIAL conver-\nsational movie recommendation dataset, and\ncreate additional training tasks derived from\nMovieLens (such as the prediction of movie\nattributes and related movies based on an in-\nput movie), in a multitask learning setting. Us-\ning a series of probe studies, we demonstrate\nthat the learned knowledge in the additional\ntasks is transferred to the conversational set-\nting, where each task leads to a 9% \u2212 52% in-\ncrease in its related probe score.\n1\nIntroduction\nThe modern recommendation systems found in\ncommercial applications are largely based on im-\nplicit preferences, such as a user\u2019s history of web\npage clicks, item purchases, or media streams, with\nthe record of these actions used to retrieve relevant\nrecommendations (Rendle et al., 2012). This ap-\nproach often works, but in the case where a user\nmight not have an extensive history, or might desire\na recommendation which doesn\u2019t match their usual\nniche, we might want a system which can take ad-\nvantage of explicit preferences. With the growing\nsuccess of deep learning language models, it has\nbecome possible to design conversational recom-\nmendation models which can communicate with a\nuser directly while retrieving custom recommenda-\ntions based on the user\u2019s explicit preferences.\nMost previous work on conversational recom-\nmender systems adopts a multi-component ap-\nproach (Gao et al., 2021). These models often\nare implemented using a recommendation com-\nponent, which analyzes the mentioned entities in\norder to predict a related item, and a dialogue com-\nponent, which analyzes the input phrases and gen-\nerates a conversational response (Jannach et al.,\n2020). Multi-component approaches are appeal-\ning because they can be built directly from stan-\ndard models in the dialogue and recommendation\n\ufb01elds. However, the knowledge learned by each\ncomponent is not immediately available to the other\ncomponents (i.e., the item recommendation model\ndoes not bene\ufb01t directly from conversation state,\nand vice versa), preventing these approaches from\ntaking advantage of the data to its fullest extent.\nIdeally, a conversational recommendation model\nshould be able to both use descriptive language in\nthe dialogue to retrieved relevant items and gener-\nate engaging dialogue about the items simultane-\nously. To address this problem, in this paper we\ninvestigate whether an end-to-end approach to con-\nversational recommendations using a single compo-\nnent model can improve dialogue and recommen-\ndation generation by allowing the model to fully\nutilize the conversation features for both tasks.\nThis paper strives to show the feasibility of a\nuni\ufb01ed model for conversational recommendations\nby leveraging a single large transformer model to\ngenerate both relevant recommendations and natu-\nral dialogue and evaluating the bene\ufb01ts of a fully\nuni\ufb01ed dialogue and recommendation module. To\ndetermine whether single end-to-end model match\nor outperform multi-component approaches, we\ntrain our model on several standard datasets in the\ndomain of movie recommendations and compare\nour results to previous work. To measure the bene-\n\ufb01t of generating dialogue and recommendations in\nthe same model, we follow a common procedure in\nrelated work (Penha and Hauff, 2020) and design a\n1\narXiv:2305.06218v1  [cs.CL]  8 May 2023\nseries of probes to assess how the model leverages\ndifferent types of information to generate dialogue\nand recommendations. One potential problem of a\nsingle component system is the reliance on a large\ndataset of sample dialogues containing both recom-\nmendation and language information. To bypass\nthe need for a single large dialogue dataset, we\n\ufb01netune the pretrained T5 model on a relatively\nsmall dataset of dialogues, and incorporate movie\nrelationship, attribute, and description information\nfrom additional datasets using a multitask setup.\nThe main contributions of this paper are:\n\u2022 A fully end-to-end approach to conversational\nrecommendation that uses a uni\ufb01ed model for\nboth dialogue and item recommendation.\n\u2022 Conducting a series of probe studies that\nshows how conversational recommendation\ntasks bene\ufb01ts from knowledge learned by the\nmodel via multi-task training on a number of\nseparate, small datasets.\nThe remainder of this paper is structured as fol-\nlows. First, we brie\ufb02y present some related work on\nconversational recommender systems, transformer\nmodels and probes studies. After that, we describe\nour T5-based approach, and the datasets, tasks and\ntraining procedure used to train our model. We\nthen describe our experimental methodology, and a\nseries of probe studies showing how dialogue and\nrecommendation mutually improved by sharing a\ncommon model.\n1.1\nRelated Work\nThe section presents a brief background on conver-\nsational recommendations, multitask transformer\nmodels, and the evaluation of conversational rec-\nommendation models through probe studies.\nA recent survey published by Gao et al. (2021)\nhighlights the range of strategies used to address\nthe different challenges faced by a conversational\nrecommendation system (CRS): Question-based\nUser Preference Elicitation, performed by mod-\nels which use natural dialogue questions to build\nan accurate user representation (Zou et al., 2020),\nMulti-turn Conversational Recommendation Strate-\ngies, which use various dialogue representations to\nkeep track of user preferences over a long form di-\nalogue (Li et al., 2020), and Natural Language Un-\nderstanding and Generation, which often relies on\nlarge pretrained language models to translate rec-\nommendations into natural text (Wang et al., 2021).\nWhile these challenges have been approached using\na variety of multi-component models, our model\naims to demonstrate that a single-component trans-\nformer model can perform the task of conversa-\ntional recommendations, and even bene\ufb01t from\ncross-task transfer due to its uni\ufb01ed design. A\ndifferent approach to uni\ufb01ed conversational rec-\nommendations, by Deng et al. (2021), succeeds\nin building a single-component system based on\na graph-based Markov Decision Process which\nswitches between prede\ufb01ned question-asking, rec-\nommendation, and conversation patterns in order\nto lead a user to a recommendation in a multi-turn\ndialogue. This approach, however, is \ufb01xed to a\nrigid \ufb02ow of conversation patterns and does not\ncontain natural language understanding or genera-\ntion components necessary to create or understand\nfree-form dialogues or unstructured conversations.\nDialogue generation has historically been ap-\nproached in many ways, with recent efforts focus-\ning on RNNs models (like LSTMs (Hochreiter and\nSchmidhuber, 1997)), and transformers (Vaswani\net al., 2017). Recommendation systems typically\nperform collaborative \ufb01ltering on a set of user-item\nassociations using a range of models such as matrix\nfactorization systems or autoencoders (Ricci et al.,\n2011). Li et al. (2018) proposed an approach to\ncombining these two areas into a functional conver-\nsational recommendation model, using an autoen-\ncoder recommender in conjunction with a GRU\nbased hierarchical encoder-decoder system to gen-\nerate the dialogue. There is some interplay be-\ntween components, with mentioned movies and\nsentiments being fed into the autoencoder in order\nto retrieve a relevant recommendation based on a\nuser\u2019s liked and disliked movies, but the generation\nof dialogues and recommendations are still largely\nseparate. Chen et al. (2019) took this approach one\nstep further, creating a conversational recommenda-\ntion system which would use mentioned entities in\nthe dialogue to conduct a knowledge graph search\nof related items and add a vocabulary bias based on\nthe user representation back into the transformer-\nbased dialogue generation module. Although this\nmodel demonstrates the potential for transfer be-\ntween dialogue and recommendation tasks, it re-\nquires a complex structure where incomplete rep-\nresentations of both the dialogue and recommen-\ndation features are passed to separate components\nand then joined with a switching network. In this\npaper we attempt to fully leverage this cross-task\n2\ntransfer without the need for separate components.\nIn recent years, many studies have demonstrated\nthe effectiveness of large, pre-trained transformer-\nbased language models on a range of natural lan-\nguage generation tasks. The architecture, which\nmakes use of self attention blocks in order to model\nlanguage, was proposed by Vaswani et al. (2017)\nand achieved state-of-the-art performance on a vari-\nety of benchmarks. When pretrained on a large cor-\npus of text, transformer models such as BERT (De-\nvlin et al., 2018), GPT-3 (Brown et al., 2020), and\nUniT (Hu and Singh, 2021) have shown the abil-\nity to handle multiple language-based tasks with\nminimal \ufb01netuning. The T5 model, introduced by\n(Raffel et al., 2019), has demonstrated a distinct\nability to incorporate different types of knowledge\nfrom multiple sources and handle several disparate\ntasks in the text-to-text format.\nIn regards to evaluating these large transformer\nmodels on the task of conversational recommenda-\ntions, one effective approach proposed by Penha\nand Hauff (2020) is to use probe studies to measure\nthe model\u2019s ability to score the likelihood of cer-\ntain entities when conditioned on a set of generated\ninputs. Penha and Hauff evaluate BERT\u2019s perfor-\nmance on conversational recommendation tasks\nby using BERT\u2019s prediction, similarity, and next\nsentence prediction function to score the model\u2019s\nability to associate a book, movie, or song with a\nrelated item or attribute. Although Penha and Hauff\nevaluate BERT\u2019s knowledge on conversational rec-\nommendations, it is important to note that in their\nstudy BERT is not acting as a full conversational\nrecommendation system on its own and cannot be\nconsidered an example of an end-to-end CRS. It is\nonly being used to rank probes against each other,\nand not to generate dialogues and recommendation\nor manage a conversation with a user.\n2\nOur Approach\nThe main idea of our approach is to formulate the\nconversational recommendation task as an instance\nof the text-to-text problem. We \ufb01netune a pre-\ntrained transformer model on the movie recommen-\ndation dialogues contained in the ReDial dataset,\nand improve the model\u2019s ability to utilize movie at-\ntributes and descriptive details within the dialogues\nthrough the introduction of additional training tasks\nin a multi-task learning setting. In this section we\npresent a background on the T5 transformer model,\na summary of each of the training datasets, and an\noverview of the \ufb01netuning parameters we used.\n2.1\nT5 Model\nT5 is a large, publicly available, encoder-decoder\ntransformer based model created by Raffel et al.\n(2019). The model was trained and structured with\nthe intent to support as many different use cases as\npossible using a text-to-text format. In the context\nof recommendation systems, T5 and related models\nare attractive because they perform well on natural\nlanguage understanding and generation tasks and\nhas demonstrated the ability to train on multiple\ndisparate types of text data within one model.\n2.2\nReDial Dialogue Task\nThe ReDial (Recommendation Dialogues) dataset\nis an annotated set of 11248 dialogues collected\nthrough Amazon Mechanical Turk (Li et al., 2018).\nEach dialogue contains the movies and messages\nsent between two parties acting as either a \"recom-\nmender\" or a \"recommendation seeker\". Although\nthis dataset is relatively small, and doesn\u2019t necessar-\nily capture as much movie relationship and attribute\ndata as other recommendation-focused datasets, we\nhave found that it provides enough examples for the\nT5 to learn the style and structure of conversational\nrecommendations.\nFor each conversation in the dataset we create a\ntraining example corresponding to each response\nfrom the human recommender. The model inputs\ncontain the conversation up to a certain recom-\nmender utterance, with the outputs containing the\nnext utterance from the recommender party. Using\nthis format the T5 model can learn to parse relevant\nmovie, attribute, and dialogue details from the pre-\nvious messages in the conversation and formulate\nan appropriate response. We use the T5\u2019s standard\nvocabulary, so movie titles are processed by the\nword, the same as any other piece of the input. To\nhelp the model learn these titles, @ signs are used to\nseparate movie titles from the rest of the dialogues.\nEach message in the ReDial conversation is pre-\nceded by either a [USER] or a [ASSISTANT]\ntag to indicate its source. The redial conversa-\ntion shown in appendix A has been processed into\nmultiple training examples corresponding to each\nresponse by the recommender. Table 1 shows a\nsample training example from this process.\n2.3\nMovieLens Sequences Task\nThe MovieLens 25m dataset is a collection of 25\nmillion ratings and one million tag associations\n3\nReDial Dialogues\nMovieLens Sequences\nMovieLens Tags\nMovieLens Reviews\nTraining\nInputs\n[User] I\u2019m in the\nmood to watch a\nromantic\ncomedy.\nWhat do you sug-\ngest?\n[Assistant]\n@ 50 First Dates\n(2004) @\n@\nThe\nIncredibles\n(2004) @ Harry Potter\nand the Chamber of Se-\ncrets (2002) The Hunger\nGames\nMockingjay\n-\nPart 1 (2014) @\ndrama, based on\na book,\nadapted\nfrom:book\nReview for @ Alice\nin Wonderland (1951)\n@:\nTraining\nTargets\nYes she is good. Do\nyou\nlike\n@\nThe\nWedding\nSinger\n(1998) @\nUnderworld:\nAwaken-\ning (2012)\nThe Book Thief\n(2013)\nPerhaps\nbecause\nits\nsurrealism\nmatched\nthe hippy culture of\npsychedelia, Alice in\nWonderland (1951) 1\n...\nKnowledge Dialogue\nRecommendation\nAttributes\nDescription\nTable 1: Comparison of the primary training task (ReDial Dialogues) and the three auxilary training tasks designed\nto increase recommendation, attribute, and description knowledge.\noften used to quantify movie relationships and at-\ntributes (Harper and Konstan, 2015). We utilize this\ndata for multiple tasks, as it can be used to quan-\ntify different types of movie information. The \ufb01rst\nadditional training task is to recommend a movie\ngiven a sequence of 1-9 related movies. This task\nincorporates movie relationship information in or-\nder to increase the quality of recommendations in\nthe ReDial Dialogues task, and is referred to as the\nML Sequences task.\nIn order to use the user ratings contained in the\nMovieLens 25m dataset to generate movie asso-\nciations, we create sequences of movies wherever\nthere are 10 movies rated higher than 4.0 / 5.0 by\nthe same user. From these sequences we create ex-\namples for each n where 1 < n < 10 by mapping\nthe \ufb01rst n movies as the inputs and the movie in\nposition (n + 1) as the target. An example of this\nformat is shown in Table 1.\n2.4\nMovieLens Tags Task\nThe MovieLens 25m dataset contains a tag genome\nwhich scores each movie\u2019s relevance across a set of\n1,129 tags (Vig et al., 2012). These tags are movie\nattributes or descriptive words which often corre-\nspond to genres (\"horror\", \"action\", \"mystery\"),\nplot elements (\"alien invasion\", \"character study\",\n\"father daughter relationship\"), opinion (\"excellent\nscript\", \"boring\", \"over the top\"), or general infor-\nmation (\"oscar (best actor)\", \"based on a book\",\n\"stanley kubrick\"). For each movie, we add each\ntag with a relevance score over 0.8 to the movies\ntag list. From these tag lists we randomly sample\nexamples containing 1-5 tags as the input and the\nrelated movie as a the target. This mapping allows\nthe model to associate movie attribute information\nand better recommend movies based on descriptive\ndetails in-dialogue. Table 1 displays an example of\na tag-to-movie mapping.\n2.5\nMovieLens Reviews Task\nThe \ufb01nal training task, referred to as the MovieLens\nReview task, uses a joint dataset created by Penha\nand Hauff (2020) to incorporate additional movie\ndescription and opinion data. The training exam-\nples for this task are generated from the reviews\nportion of Penha and Hauff (2020)\u2019s search dataset,\nwhich contains the IMDB user reviews associated\nwith each movie in the MovieLens database. These\nreviews contain movie attribute data written in the\nkind of casual, natural dialogue style found in the\nReDial dataset, so they aid the model\u2019s natural text\ngeneration and descriptive capabilities. As shown\nin Table 1, these reviews are processed into exam-\nples where the model is asked to predict the next\nsentence of a review given a movie title and the\ntruncated review 1.\n2.6\nMultitask Training\nThe T5 module supports multitask training, where\nexamples from training dataset are loaded through\ntheir own preprocessing steps (in our case only\nlowercasing). We opt to \ufb01netune the base size\n(220 million parameters) with a learning rate of\n1Because movie titles and title fragments in the MovieLens\nReviews dataset are not delimited, the MovieLens Reviews\ntraining task does not use \u2019@\u2019 signs to separate movie titles.\n4\n0.003 for 40,000 steps and batch size 128 2. Texts\nlonger than the maximum sequence length, i.e., 512\nfor inputs and 128 for targets are truncated. We\ntrain variants with different combinations of the\nfour training tasks in order to isolate their effects.\nExamples from each task were loaded equally often.\nAs suggested by Raffel et al. (2019), we prepend\nthe inputs to each task with a task label: \u201credial\nconversation:\", \u201cmovielens sequence:\", \u201cmovielens\ntags:\", or \u201cmovielens review:\". From this point, the\nname T5 will be used to refer to the out-of-the-box\npretrained T5 model and the name T5-CR will be\nused to refer to our custom T5 model with all four\n\ufb01netuning tasks.\n3\nBaseline Evaluations\nIn order to determine whether our end-to-end ap-\nproach can perform competitively on dialogue and\nrecommendation, we compare our performance us-\ning BLEU score and Recall. These metrics are both\nrun on the evaluation set provided with the ReDial\ndataset. The BLEU score acts as a measure of di-\nalogue quality, by measuring the similarity of the\nmodel and human responses. The Recall metric is\ncalculated by comparing the movies mentioned by\nthe model in the evaluation dialogues to the set of\nmovies mentioned by the human recommender in\nthe ReDial dialogues evaluation set. End-to-End\nRecall refers to the Recall@1 caluclated in the dia-\nlogue task, while Rec Module Recall is calculated\non the same movies run through only the isolated\nrecommendation module if one exists. These two\nmetrics were selected as they are standards which\nhave been run on many of the previous works in\nthe area. We calculated baselines for the ReDial\nand KBRD models, which did not have reported\nBLEU or Recall scores, and sources our other base-\nlines from Wang et al.. In Table 2 we compare\nthe most relevant models: (1) ReDial (Li et al.,\n2018) an HRED CR system, (2) KBRD (Chen\net al., 2019) which uses a transformer for dialogue\nand a knowledge graph for recommendation, (3)\nKGSF (Zhou et al., 2020) which uses fuses word\nand entity knowlege using a knowlege graph and\ntransformer, (4) GPT-2 a pretrained transformer,\n(5) RID (Wang et al., 2021) which uses a pretrained\nlanguage model with a relational graph convolu-\ntional network (RGCN).\n2The T5 was \ufb01netuned with using the public T5 code-\nbase: https://github.com/google-research/text-to-text-transfer-\ntransformer\n3.1\nBLEU\nBLEU score is a standard metric used in ma-\nchine translation and text generation tasks which\nquanti\ufb01es how similar a generated phrase is to\nthe expected target phrase (Papineni et al., 2002).\nWe postprocess our ReDial predictions to replace\nmovie titles with a \"__unk__\" token before cal-\nculating the metric. This ensures that our BLEU\nscore only captures information on the closeness\nof the dialogue to our target, and isn\u2019t in\ufb02uenced\nby correct/incorrect movie titles and recommen-\ndations. Our T5-CR model, trained on all four\ntraining tasks was able to outperform the KBRD\nand ReDial models approaches, achieving a BLEU\nscore of 15.39. The increase in BLEU score is\nlikely a result of the introduction of movie descrip-\ntion and attribute data through the multitask train-\ning setup as well as general increased \ufb02uency of\nlarge pre-trained language models such as the T5.\nThe RID model trained by Wang et al. was also\nbuilt using a pretrained tranformer and performed\nthe best, with a score of 20.70.\n3.2\nRecall\nIn order to evaluate the quality of the recommen-\ndations we calculate End-to-End Recall as the per-\ncent of movies generated by the model in-dialogue\nwhich correspond to one of the known recommen-\ndations given by the human recommender in the\ncorresponding redial dialogue. Rec Module Re-\ncall refers to the Recall@1 score when only the\nisolated recommendation modules are used. In\nprevious efforts such as the ReDial, KBRD, and\nKGSF models, the End-to-End Recall scores were\nsigni\ufb01cantly lower than the Rec Module Recall\nscores, suggesting that the models were less likely\nto apply their recommendation knowledge accu-\nrately in-dialogue compared to as a separated rec-\nommendation task. This highlights the advantage\nof the end-to-end approach, as the uni\ufb01ed structures\nensures the model can generate high quality dia-\nlogue and recommendations simultaneously. The\nmultitask T5-CR model achieved a Recall score\nof 6.93, which outperforms all baseline models.\nThe increase in Recall is likely due to the end-to-\nend structure of the model allowing it to use dia-\nlogue features to retrieve better recommendations,\nas well as the movie relationship and attribute train-\ning tasks allowing for more accurate analysis of\nuser preferences.\n5\nModel Name\nModel Type\nBLEU\nRec Module Recall\nEnd-to-End Recall\nReDial\nAutoencoder + LSTM\n8.38\n2.30\n0.70\nKBRD\nKnowledge Graph + Transformer\n11.00\n3.00\n0.80\nKGSF\nKnowledge Graph Semantic Fusion\n\u2014\u2013\n3.90\n0.90\nGPT-2\nPretrained Transformer\n\u2014\u2013\n\u2014\u2013\n1.40\nRID\nRGCN + Pretrained Transformer\n20.70\n\u2014\u2013\n3.10\nT5-CR\nFinetuned T5 (4 Tasks)\n15.39\n\u2014\u2013\n6.93\nTable 2: BLEU and Recall@1 metric comparisons between T5-CR, our T5 variant \ufb01netuned on 4 tasks, and the\nprevious approaches to conversational recommendations. All evaluation scores are calculated based on the model\u2019s\nperformance on the ReDial validation dialogues. The baseline scores for the KGSF, GPT-2, and RID models along\nwith the End-to-End Recall scores for the KBRD and ReDial models were taken from (Wang et al., 2021).\n4\nProbe Studies\nAlthough the BLEU and Recall scores on the\nReDial Evaluation Dataset prove that an end-to-\nend model can outperform multi-component ap-\nproaches, the scores do not give us insight on the ex-\ntent to which our multitask training setup bene\ufb01ted\nthe model\u2019s ability to generate dialogue and rec-\nommendations. Also, the ReDial evaluation slice\ncovers a small selection of movies and dialogue\ninteractions. In order to determine the contribution\nof each of the training tasks, as well as any mea-\nsurable advantages of cross-task transfer within the\nsame T5 architecture, we present four probe studies\nin the style of Penha and Hauff (2020). Each probe\ntests a speci\ufb01c dialogue interaction by measuring\nthe T5-CR\u2019s ability to distinguish between relevant\nand unrelated information conditioned on different\ntypes of sample dialogues. In order to \ufb01lter out mis-\nspellings, alternate titles, and rare movies which\nthe model has little information on, the probes are\ngenerated using the set of movies which occur over\n30 times in the ML Sequences dataset. With probe\nexamples generated from this set of around 5,000\nmovies, we are able to run evaluations on a much\nlarger range of data than the limited ReDial eval-\nuation set. These probes are designed to measure\nthe model\u2019s ability to apply the information gained\nthrough its multitask training in a dialogue setting,\ntherefore all probe data is evaluated through the\nReDial Dialogue tasks.\n4.1\nRecommendation Probe\nThe recommendation probe measures the model\u2019s\nability to distinguish a related movie from a pop-\nular movie chosen at random. In order to quan-\ntify related movies based on cooccurrence in the\nML Sequences dataset, we rank movies based on\nPMI2 (Role and Nadif, 2011), a variation on\npointwise mutual information (PMI). PMI2 is a\ncommonly used variation on PMI which reduces\nPMI\u2019s known bias toward rare and infrequent\nitems (Role and Nadif, 2011). For each of the\ntop ten related movies we sample a random popu-\nlar movie from the top 10% of movies (ranked by\nfrequency in the ML Sequences dataset). For each\nof the ten (relatedi, populari) pairs generated for\neach movie, we create a probe by swapping in the\nmovies to a generic piece of dialogue, as seen in Ta-\nble 4. The probe score is calculated as the percent\nof probes where the models log-likelihood score,\nL(\u03b8), of the target containing the related movie\nwas higher than that of the random popular movie.\nNote that different phrasings and dialogue formats\nwere tested with little effect on the probe results.\nAs shown in Figure 3, the introduction of the\nML Sequences task improved the model\u2019s ability to\ndifferentiate between related and random movies,\nre\ufb02ected by a 30% increase in the recommenda-\ntion probe scores between the ReDial-only model\nand the ReDial + ML Sequences model. This in-\ncrease demonstrates that the patterns in the movie\nsequences fed in the ML Sequences tasks can be\ngeneralized and applied within the dialogue tasks.\nInterestingly, the ReDial + ML Tags model also out-\nperformed the ReDial-only model, with an increase\nof 23% in recommendation probe scores over the\nReDial-only model.\nThis increase demonstrates an advantage of the\nend-to-end format: data incorporated to help the\nunderstanding of descriptive words in the dialogues\nalso boosted performance on movie-to-movie rec-\nommendation, despite the additional data not di-\nrectly specifying any movie relationships. Because\nrecommendation and dialogue are handled in the\nsame model, it can leverage patterns in seemingly\nunrelated data. Here, the model is likely using the\noverlap of tags associated with different movies\nto help determine whether they are related. In the\n6\nT5 Finetuning Tasks\nRec Probe\nAttr Probe\nCombo Probe\nDesc Probe\nNone (T5)\n0.5493\n0.4908\n0.5597\n0.5936\nReDial\n0.4716\n0.5046\n0.5731\n0.7097\nReDial +\nML Sequences\n0.6359\n0.5869\n0.7367\n0.7307\nReDial +\nML Tags\n0.5670\n0.7826\n0.8016\n0.7133\nReDial +\nML Reviews\n0.4771\n0.5091\n0.5833\n0.7763\nAll (T5-CR)\n0.6599\n0.7678\n0.8418\n0.7928\nTable 3: Comparison of probe scores across T5 models with different \ufb01netuning tasks.\ncombined model, where all four training tasks were\nincluded, the model performed the best (+37%\nover ReDial-only), a score which demonstrates the\nviability of multitask transformers to incorporate\nmany different data sources and tasks without los-\ning performance.\nOverall the performance of the recommendation\nprobe represents a transfer of movie-relationship\nknowledge between training tasks, but this transfer\nis not perfect. While the probes (fed into the Re-\nDial Dialogues task) achieved a score of .6599 in\nthe Combined model, the same pairs fed into the\nML Sequences task without any dialogue achieved\na score of .7711. This increase indicates either an\nincomplete transfer of knowledge from the Movie-\nLens Sequences task to the ReDial Dialogues task,\nor a bias from the the movie recommendation data\nalready present in the ReDial Conversations. Sim-\nilarly, the T5\u2019s performance on the movie recom-\nmendation probes is lower than that of a purely Ma-\ntrix Factorization model, which achieved a score of\n.8096 on the movie pairs.\n4.2\nAttributes Probe\nThe attributes probe measures the model\u2019s ability\nto use details and descriptive words appearing in-\ndialogue to retrieve relevant movies. As shown in\nTable 4, a probe is generated for each movie-tag as-\nsociation in the MovieLens Tags dataset, with a ran-\ndom popular movie used as the negative. Because\nmany of the most popular tags (such as \"action\" or\n\"excellent\") might apply to a large portion of the\npopular movies, we \ufb01lter the negative to ensure it\nisn\u2019t associated with the given tag.\nThe attribute probe scores also demonstrated the\neffectiveness of multitask learning, with the intro-\nduction of the ML Tags task leading to a 52% in-\ncrease in performance over the ReDial-only model.\nThis probe directly shows one of the advantages\nof end-to-end learning. Because dialogue analy-\nsis and recommendation generation occurs in the\nsame model, the descriptive attributes mentioned\nin the input dialogue (or movie \"tags\") can help the\nmodel retrieve a movie relevant to that attribute,\neven when no movie titles are mentioned in the in-\nput. While the Combined model didn\u2019t out-perform\nthe RD Tags model, it did perform consistently,\nwith an accuracy of .7689 over the probe set.\n4.3\nCombination Probe\nThe combination probe measures the multitask ca-\npabilities of the model, determining whether at-\ntribute and movie entity data can be used simulta-\nneously to generate a relevant response. As shown\nin Table 4, a probe is generated for each shared\ntag among each of a movies top 10 most related\nmovies. As in the attribute probe, we \ufb01lter out the\npopular negative to ensure it does not match the\ngiven tag.\nThe combination probe extends the \ufb01ndings of\nthe previous two probes: not only can the model use\nmentioned movies or movie attributes to in\ufb02uence\nits recommendations, it can do both at the same\ntime. Whereas a multi-component approach to the\nproblem would base its recommendation solely on\nthe previously mentioned movies or the attributes\nmentioned in-dialogue, an end-to-end approach\nuses these pieces of information together. The\nCombined model was able to differentiate 84.18%\nof the probe pairs when given a movie and a tag in\nthe input dialogue, an improvement over its perfor-\nmance on either the recommendation or attribute\nprobes. This improvement demonstrates that when\nusing both types of information together, the model\ncan more accurately recommend a related movie.\n4.4\nMovie Description Probe\nThe previous three probes test whether the model\ncan retrieve a relevant movie title when conditioned\n7\nRecommendation Probe\nAttribute Probe\nCombination Probe\nDescription Probe\nInput 1\n(Related)\n[User] Can you recom-\nmend me a movie like @\nZootopia (2016) @\n[User] Can you recom-\nmend me a vampire\nmovie?\n[User] Can you rec-\nommend me a science\n\ufb01ction movie like @\nLooper (2012) @?\n[User] What is your\nopinion on @ Ring-\ning Bell (1978) @?\nInput 2\n(Rand.\nPopular)\n[User] What is your\nopinion on @ Robin\nHood:\nMen\nin\nTights (1993) @?\nTarget 1\n(Related)\nSure, have you seen @ In-\nside Out (2015) @?\nSure,\nhave you seen\n@ Interview with the\nVampire: the Vampire\nChronicles (1994) @?\nSure, have you seen\n@ Edge of Tomor-\nrow(2014) @?\nWatching this several\ntimes as a child was\nquite the. . .\nTarget 2\n(Rand.\nPopular)\nSure, have you seen @ I\nAm Sam (2001) @?\nSure, have you seen @\nSicko (2007) @?\nSure, have you seen @\nZoolander (2001) @?\nData\nML Sequences\nML Tags\nML Sequences + Tags\nML Reviews\nMetric\nL(T1 | I1) > L(T2 | I1)\nL(T1 | I1) > L(T2 | I1)\nL(T1 | I1) > L(T2 | I1) L(T1 | I1) > L(T1 | I2)\nTable 4: Comparison of the four probe sets, which determine whether the model can correctly rank related entities\nas more likely than random negatives.\non a dialogue. The movie description probe tests\nthe reverse direction: can the model retrieve a piece\nof relevant or descriptive dialogue when condi-\ntioned on a certain movie title. To do this, we\nmeasure the likelihood of a given review snippet\ntaken from the \ufb01rst four sentences of a review in\nthe ML Reviews dataset. In previous probes, we\nhave ranked two different targets based on likeli-\nhood, but because review snippets differ greatly\nin length, phrasing, style of language, and other\nfactors which can in\ufb02uence likelihood, we opt to\nkeep the target the same and compare the likeli-\nhood of a given review snippet when conditioned\non a related/unrelated movie. As shown in Ta-\nble 4, for a related input I1, a random popular\ninput I2, and a review snippet T we compare\nthe log likelihood scores and measure how often\nL(T | I1) > L(T | I2).\nThe description probe demonstrates that in\nan end-to-end model, mentioning a movie can\nprompt the model to retrieve relevant dialogue.\nThis functionality wouldn\u2019t be in traditional multi-\ncomponent approaches where mentioned movies\nare processed separately from dialogue. The ML\nReviews training task led to a 9.38% increase over\nthe ReDial-only model, while the combined model\nwas able to achieve a score of 0.7929, an 11.72%\nincrease over the ReDial-only model.\n5\nConclusion\nIn this paper, we presented a multitask approach\nto end-to-end conversational recommendations. In\ndirect comparison to two previously published mod-\nels in the domain, our T5-based architecture outper-\nformed the baselines in both its quality of dialogue\nand recommendation. When probed on recommen-\ndation, attribute knowledge, and description, our\nmodel demonstrates that dialogues and recommen-\ndations can be mutually improved by sharing a\nmodel architecture. Speci\ufb01cally, the probes prove\nthat the model is able to use dialogue features to\ninform its recommendations and movie mentions\nto in\ufb02uence its dialogue generation. These \ufb01nd-\nings support a general trend in current natural lan-\nguage processing landscape, where large pretrained\ntransformer models are rapidly becoming the state-\nof-the-art in many domains. In fact, our research\nhas implication on the broader area of multitask\nmodels, highlighting how a limited dataset (such\nas ReDial) can be injected with information from\nseveral auxiliary datasets, regardless of format. In\nthe future, this effect might shift the focus from\ntraining and combining optimized components for\neach functionality of a system, to simply incorpo-\nrating all desired information as different tasks in a\npretrained multitask model.\n8\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell,\nSandhini Agarwal,\nAriel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. CoRR, abs/2005.14165.\nQibin Chen, Junyang Lin, Yichang Zhang, Ming Ding,\nYukuo Cen, Hongxia Yang, and Jie Tang. 2019. To-\nwards knowledge-based recommender dialog sys-\ntem. CoRR, abs/1908.05391.\nYang Deng, Yaliang Li, Fei Sun, Bolin Ding, and Wai\nLam. 2021. Uni\ufb01ed conversational recommendation\npolicy learning via graph-based reinforcement learn-\ning. CoRR, abs/2105.09710.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018.\nBERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nChongming\nGao,\nWenqiang\nLei,\nXiangnan\nHe,\nMaarten de Rijke, and Tat-Seng Chua. 2021. Ad-\nvances and challenges in conversational recom-\nmender systems: A survey. CoRR, abs/2101.09459.\nF. Maxwell Harper and Joseph A. Konstan. 2015. The\nmovielens datasets:\nHistory and context.\nACM\nTrans. Interact. Intell. Syst., 5(4).\nSepp Hochreiter and J\u00fcrgen Schmidhuber. 1997.\nLong short-term memory.\nNeural computation,\n9(8):1735\u20131780.\nRonghang Hu and Amanpreet Singh. 2021.\nTrans-\nformer is all you need:\nMultimodal multitask\nlearning with a uni\ufb01ed transformer.\nCoRR,\nabs/2102.10772.\nDietmar Jannach, Ahtsham Manzoor, Wanling Cai, and\nLi Chen. 2020. A survey on conversational recom-\nmender systems. CoRR, abs/2004.00646.\nRaymond Li, Samira Kahou, Hannes Schulz, Vincent\nMichalski, Laurent Charlin, and Chris Pal. 2018. To-\nwards deep conversational recommendations. arXiv\npreprint arXiv:1812.07617.\nShijun Li, Wenqiang Lei, Qingyun Wu, Xiangnan\nHe, Peng Jiang, and Tat-Seng Chua. 2020. Seam-\nlessly unifying attributes and items:\nConversa-\ntional recommendation for cold-start users. CoRR,\nabs/2005.12979.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting on Association for Computa-\ntional Linguistics, ACL \u201902, page 311\u2013318, USA.\nAssociation for Computational Linguistics.\nGustavo Penha and Claudia Hauff. 2020. What does\nbert know about books, movies and music? probing\nbert for conversational recommendation.\nIn Four-\nteenth ACM Conference on Recommender Systems,\nRecSys \u201920, page 388\u2013397, New York, NY, USA.\nAssociation for Computing Machinery.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uni\ufb01ed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nSteffen Rendle, Christoph Freudenthaler, Zeno Gant-\nner, and Lars Schmidt-Thieme. 2012. Bpr: Bayesian\npersonalized ranking from implicit feedback. arXiv\npreprint arXiv:1205.2618.\nFrancesco Ricci, Lior Rokach, Bracha Shapira, and\nPaul B. Kantor. 2011. Recommender Systems Hand-\nbook. Springer.\nFran\u00e7ois Role and Mohamed Nadif. 2011. Handling\nthe impact of low frequency events on co-occurrence\nbased measures of word similarity - a case study of\npointwise mutual information. In Proceedings of the\nInternational Conference on Knowledge Discovery\nand Information Retrieval, KDIR 2011.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998\u20136008.\nJesse Vig, Shilad Sen, and John Riedl. 2012. The tag\ngenome: Encoding community knowledge to sup-\nport novel interaction. ACM Trans. Interact. Intell.\nSyst., 2(3).\nLingzhi Wang, Huang Hu, Lei Sha, Can Xu, Kam-Fai\nWong, and Daxin Jiang. 2021.\nFinetuning large-\nscale pre-trained language models for conversational\nrecommendation with knowledge graph.\nCoRR,\nabs/2110.07477.\nKun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuan-\nhang Zhou, Ji-Rong Wen, and Jingsong Yu. 2020.\nImproving conversational recommender systems via\nknowledge graph based semantic fusion.\nCoRR,\nabs/2007.04032.\nJie Zou,\nYifan Chen,\nand Evangelos Kanoulas.\n2020.\nTowards question-based recommender sys-\ntems. CoRR, abs/2005.14255.\n9\nA\nAppendix - Redial Dataset Sample\nAn example of the beginning of a ReDial con-\nversation, randomly selected from the ReDial\ndataset is shown at 1.\nREDIAL CONVERSATION:\nSender:\nI\u2019m in the mood to watch\na romantic comedy.\nWhat do you\nsuggest?\nResponder:\n@ 50 First Dates (2004)\n@ Have you seen that one?\nSender:\nOh, I\u2019ve seen that one.\nI really like...\nFigure 1: Beginning of a randomly selected example\nfrom the ReDial dataset.\n10\n"
  },
  {
    "title": "Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models",
    "link": "https://arxiv.org/pdf/2305.05973.pdf",
    "upvote": "1",
    "text": "Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems\nusing Differentially Private Language Models\nAldo Gael Carranza\nStanford University\naldogael@stanford.edu\nRezsa Farahani\nGoogle Inc.\nfarahani@google.com\nNatalia Ponomareva\nGoogle Research\nnponomareva@google.com\nAlex Kurakin\nGoogle Research\nkurakin@google.com\nMatthew Jagielski\nGoogle Research\njagielski@google.com\nMilad Nasr\nGoogle Research\nsrxzr@google.com\nAbstract\nWe address the challenge of ensuring differen-\ntial privacy (DP) guarantees in training deep\nretrieval systems. Training these systems of-\nten involves the use of contrastive-style losses,\nwhich are typically non-per-example decom-\nposable, making them difficult to directly DP-\ntrain with since common techniques require per-\nexample gradient. To address this issue, we pro-\npose an approach that prioritizes ensuring query\nprivacy prior to training a deep retrieval sys-\ntem. Our method employs DP language models\n(LMs) to generate private synthetic queries rep-\nresentative of the original data. These synthetic\nqueries can be used in downstream retrieval\nsystem training without compromising privacy.\nOur approach demonstrates a significant en-\nhancement in retrieval quality compared to di-\nrect DP-training, all while maintaining query-\nlevel privacy guarantees. This work highlights\nthe potential of harnessing LMs to overcome\nlimitations in standard DP-training methods.\n1\nIntroduction\nThe standard approach to ensure the privacy of\ntraining data in large-scale machine learning mod-\nels is to directly introduce differential privacy\n(DP) (Dwork et al., 2014) guarantees during train-\ning (Abadi et al., 2016; Ponomareva et al., 2023).\nThese DP-training strategies provide guarantees by\nlimiting the impact each individual data instance\nhas on the overall model. However, some models\ncontain design elements that inherently hinder the\nability to limit per-example contributions, and thus\nare more difficult to directly DP-train. These in-\nclude models with components that calculate batch\nstatistics like batch normalization layers and losses\nthat cannot be decomposed into per-example losses\nlike pairwise and contrastive-style losses (Huai\net al., 2020; Xue et al., 2021; Ponomareva et al.,\n2023). Hence, additional considerations are often\nneeded to adapt DP-training to such models for\nachieving an adequate privacy-accuracy tradeoff.\nFor non-per-example decomposable losses in\nparticular, the injected noise needed to achieve\nDP guarantees typically scales with the number\nof examples that appear in example-level loss com-\nputations, which may result in excessive model\nutility degradation. For instance, deep retrieval sys-\ntems often employ non-per-example decomposable\ncontrastive-style losses to train semantic neural rep-\nresentations of user queries and candidate items to\nfacilitate efficient vector-based retrieval strategies.\nDeep retrieval systems have been widely adopted\nin many online services from search to advertising\n(Covington et al., 2016; Huang et al., 2020). Al-\nthough these personalized systems enhance user\nexperience, it is crucial that this improvement does\nnot compromise user privacy. The ubiquity of these\nsystems underscores the importance of developing\ngeneral-purpose approaches to ensure privacy guar-\nantees without significantly hindering their util-\nity\u2014an area that has seen limited research efforts.\nIn this work, we take an approach that ensures\nuser query privacy prior to training a deep retrieval\nsystem in order to circumvent the various issues\nwith directly DP-training deep retrieval systems\nwith a non-per-example decomposable contrastive-\nstyle loss. We build on the framework of synthetic\ndata generation using DP language models (LMs)\n(Yue et al., 2022; Mattern et al., 2022) to develop\nan approach for private query sharing to train any\ndownstream deep retrieval system with query-level\nprivacy guarantees with respect to the original train-\ning data. We empirically demonstrate consider-\nable improvements in our approach on retrieval\nquality without compromising privacy guarantees\ncompared to direct DP-training methods. More\ngenerally, our work presents an initial study into\na nascent opportunity to leverage exciting break-\nthroughs in LMs to overcome crucial limitations in\ndirectly DP-training machine learning systems.\narXiv:2305.05973v2  [cs.CL]  8 Jan 2024\nFigure 1: Illustration of approach.\n2\nRelated Work\nSynthetic Generation using DP LMs\nSeveral re-\ncent studies (Yue et al., 2022; Mattern et al., 2022;\nMireshghallah et al., 2022; Putta et al., 2023) have\ninvestigated the utility of private synthetic data\nfrom DP finetuned LMs in downstream tasks with\npointwise losses, including text classification and\nsemantic parsing. These works find that down-\nstream models trained with private synthetic data\noutperform directly DP-trained models under the\nsame privacy budget, and non-private synthetic data\ngeneration can even improve performance under no\nprivacy constraints. The reason is that DP synthetic\ndata benefits from the injection of additional public\ninformation from the pretrained LMs. Our work\ncontributes another exploration of the advantages\nof private synthetic data generation for downstream\ntraining under a different learning paradigm with a\nnon-per-example decomposable loss.\nDP-training under Non-per-example Decompos-\nable Losses\nFiguring out better ways of DP-\ntraining models with non-per-example decompos-\nable losses remains an active research topic. Re-\nsearch in this area has entirely focused on pairwise\nlosses (Huai et al., 2020; Xue et al., 2021; Kang\net al., 2021), introducing specialized algorithms\nunder particular conditions like convexity, smooth-\nness, and Lipshitz continuity to maintain a reason-\nable bound on sensitivity. Our work presents a\ngeneral-purpose approach without such additional\nassumptions for achieving some level of privacy\nfor a system trained with a non-per-example de-\ncomposable loss.\n3\nBackground\n3.1\nDeep Retrieval\nDeep retrieval systems have emerged as highly ef-\nfective and scalable information retrieval systems\nto find candidate items based on their semantic\nrelevance to a specific query (Huang et al., 2020;\nNi et al., 2021). These systems typically consist\nof two neural encoders capable of generating rich,\ndense representations of queries and items (see Fig-\nure 2), which enable approximate nearest neighbor\nsearch methods to efficiently retrieve semantically\nrelevant items to a given query. Deep retrieval\nsystems are typically trained on contrastive-style\nlosses that make use of two types of data examples:\npositive examples and negative examples. The pos-\nitive examples help train the encoders into pulling\nrelevant query-item pair embeddings close together\nin the embedding space, while negative examples\nhelp in preventing embedding space collapse. A\npopular choice for the loss function in deep re-\ntrieval is the in-batch softmax loss, which makes\nmemory-efficient use of items already loaded in\na mini-batch as randomly sampled soft negatives\n(Gillick et al., 2019; Karpukhin et al., 2020; Qu\net al., 2020). In particular, given a training batch of\nquery-item pairs {(qi, di)}i\u2208B, each di is the posi-\ntive item document for query qi, and all other item\ndocuments {dj}j\u0338=i within the batch are treated as\nthe negatives. The in-batch softmax loss for each\nsample in the batch is\nLi = \u2212 log\nesim(qi,di)\nP\nj\u2208B esim(qi,dj) ,\n(1)\nwhere sim(qi, dj) is the cosine similarity between\nthe embeddings of qi and dj for any i, j \u2208 B. The\nlarger and more diverse the batch, the better it is\nfor representation learning.\n3.2\nConditional Text Generation\nConditional text generation is the task of gener-\nating a sequence of text given a specific prompt\n(Keskar et al., 2019; Schick and Sch\u00fctze, 2021).\nPre-trained generative LMs such as GPT-3 and T5\nhave been shown to be highly effective at generat-\ning high-quality text conditioned on various prompt\ninputs (Raffel et al., 2020; Brown et al., 2020).\nGiven a context c, the probability distribution of\na text sequence x = (x1, . . . , xn) is decomposed\nFigure 2: Illustration of deep retrieval dual encoder\nmodel. Dashed lines connecting the encoders represent\na shared encoder.\nas p(x|c) = Qn\ni=1 p(xi|x1, . . . , xi\u22121, c). A neural\nnetwork p\u03b8 is trained to model the conditional dis-\ntributions. The model can then be used to gener-\nate a new sample \u02dcx = (\u02dcx1, . . . , \u02dcxm) conditioned\non a given context c by sequentially sampling\np\u03b8(\u00b7|c), p\u03b8(\u00b7|\u02dcx1, c), . . . , p\u03b8(\u00b7|\u02dcx1, . . . , \u02dcxm\u22121, c). In\nthis work, we model the distribution of query texts\ngiven item documents as contexts with a publicly\npre-trained LM.\n3.3\nDifferential Privacy\nDifferential privacy (DP) has become a gold stan-\ndard for ensuring data anonymization (Dwork et al.,\n2014). In this work, we make use of the follow-\ning relaxed notion of differential privacy known as\n(\u03f5, \u03b4)-differential privacy.\nDefinition 3.1 (Differential Privacy). A random-\nized algorithm M : D \u2192 S is (\u03f5, \u03b4)-differentially\nprivate if for all S \u2282 S and for any two neighbor-\ning datasets D, D\u2032 \u2208 D that differ exactly by a\nsingle data point, we have that P[M(D) \u2208 S] \u2264\ne\u03f5 P[M(D\u2032) \u2208 S] + \u03b4.\nThis definition captures a privacy guarantee\nbased on the indistinguishability of the presence\nof a single data point in a dataset. The \u03f5 and \u03b4\nparameter control the strength of this privacy guar-\nantee, where smaller values correspond to stronger\nguarantees. A useful property of DP that is cru-\ncial to our approach is its post-processing prop-\nerty (Dwork et al., 2014) which states that for any\ndeterministic or randomized function f defined\nover the range of the mechanism M, if M sat-\nisfies (\u03f5, \u03b4)-DP, so does the composition f \u25e6 M.\nThe post-processing property ensures that arbitrary\ncomputations on the output of a DP mechanism do\nnot incur any additional privacy loss.\n3.3.1\nDifferentially Private Training\nIn the context of machine learning, DP can be\nused to protect the privacy of data used to train\na model, preventing an adversary from inferring\nthe presence of specific training examples. By far\nthe most practical method of introducing DP to\nnon-convex ML models involves the modification\nof the training process to limit the impact that each\nindividual data instance has on the overall model,\nalso referred to as DP-training (Ponomareva et al.,\n2023). The most popular methods for DP-training\nare gradient noise injection methods like differ-\nentially private stochastic gradient descent (DP-\nSGD) (Abadi et al., 2016). DP-SGD works by\nclipping per-example gradients to have norm no\ngreater than C and adding isotropic Gaussian noise\nN(0, \u03c32C2I) to the clipped gradients before aggre-\ngating and applying the gradient update to model\nweights. The noise multiplier \u03c3 is set based on the\nprivacy parameters \u03f5, \u03b4, and it can be determined\nusing a privacy accountant (Abadi et al., 2016).\nClipping is done to bound gradient sensitivity,\nwhich captures how much a single example can\ninfluence the trained model. The specific value of\nC does not actually affect the (\u03f5, \u03b4)-DP guarantee,\nsince a larger value of C means more noise will\nbe added to compensate. However, the primary\nchallenge in setting the clipping norm is in finding\nthe right balance to maximize utility. If the clipping\nnorm is set too low, it may overly constrain the\ngradients during training. If the clipping norm is set\ntoo high, sensitivity is less controlled, and too much\nnoise is added to the gradients. Both cases hinder\nthe model\u2019s ability to learn and worsen utility.\n3.3.2\nLimitations of Directly Differentially\nPrivate Training Retrieval Systems\nOur work was primarily motivated by the fact that\nDP-SGD is not immediately compatible with the\nin-batch softmax loss used to train dual encoders.\nThe primary reason is that per-example gradients\nof this loss depend on not just the example in con-\nsideration but also all other examples in the batch.\nThis results in increased sensitivity and thus greater\nnoise additions that tend to worsen utility.\nMoreover, DP-SGD provides guarantees on\nexample-level privacy, and every example in this\ncase contains a query and item. However, in this\nwork, we are interested in achieving query-level\nprivacy which should be easier than protecting both\nqueries and items. Standard DP-SGD is not able to\nguarantee these less strict levels of privacy.\nLastly, a systems-level issue of DP-training on\nthe in-batch softmax loss is that in order to take ad-\nvantage of vectorization and parallelization strate-\ngies for computing per-example gradients more\nquickly (Subramani et al., 2021), each query-item\nexample in a batch must be duplicated to be con-\ntained in every de facto example in the batch, lead-\ning to a quadratic increase in memory requirements.\nGiven fixed memory resources, this necessitates\nsignificantly smaller batch sizes, which has an ad-\nditional deleterious effect beyond gradient clipping\nand noising since effective representation learning\nunder the in-batch softmax loss highly depends on\nthe amount and diversity of in-batch examples. Our\napproach of training with private synthetic queries\nprecludes the above limitations when training a\ndownstream dual encoder.\n4\nApproach\nWe describe our general-purpose approach to ob-\ntain DP synthetic data for training a downstream\ndeep retrieval system while ensuring query-level\nprivacy on the original training data.\n1) DP-training LM on Conditional Query Gen-\neration Task\nFirst, we obtain a suitable publicly\npre-trained LM that has not been pre-trained on the\nqueries in the private training data. We use DP-\nAdafactor to DP fine-tune the chosen LM with a\nconditional query generation task. DP-Adafactor is\nmerely an Adafactor optimizer (Shazeer and Stern,\n2018) that receives clipped and noised gradients\nas per the DP-SGD algorithm (Abadi et al., 2016).\nThe conditional query generation task is the follow-\ning: given a query-item document pair (q, d) in the\ntraining data, the LM is fine-tuned to generate the\ntarget text \u201cq\u201d given input text \u201cd\u201d. Note that for\nlarger LMs with billions of parameters, it is possi-\nble to leverage more parameter-efficient finetuning\ntechniques (Lester et al., 2021; Hu et al., 2021) in\norder to overcome the high cost of training such\nlarge models. The effect of parameter-efficient DP-\nfinetuning on the quality of synthetically generated\nretrieval data is a subject of further study.\n2) Synthetic Query Generation using DP LM\nThen, the DP fine-tuned LM is capable of gener-\nating synthetic queries that are representative of\nthe real queries and relevant to the items. For\neach item document d, we generate a matching\nsynthetic query \u02dcq by providing the input \u201cgener-\nate_query: d\u201d to the model. A synthetic training\ndataset is then constructed to be the set of origi-\nnal documents matched with their corresponding\nsynthetic queries.\n3) Training Dual Encoder with DP Synthetic\nData\nLastly, the synthetic data can then be shared\nsecurely for any subsequent training tasks without\ntaking on any additional DP losses on the original\nqueries, as guaranteed by the post-processing prop-\nerty of DP (see Section 3.3). In particular, we can\ntrain a dual encoder model with the in-batch soft-\nmax loss (see Equation 1) on the synthetic training\ndata using standard non-private training methods,\nwhile still guaranteeing DP protection of the origi-\nnal queries.\n5\nExperimental Setup\n5.1\nDatasets\nWe use publicly available datasets for informa-\ntion retrieval tasks. For finetuning and evaluation,\nwe consider the MSMARCO dataset (Bajaj et al.,\n2016), which consists of nearly 533,000 query-\ndocument pairs of search data sampled from Bing\nsearch logs covering a broad range of domains and\nconcepts. Additionally, we consider datasets in the\nBEIR benchmark suite (Thakur et al., 2021), which\ncontains information retrieval datasets across a va-\nriety of domains, for zero-shot evaluation.\n5.2\nSynthetic Data Generation\n5.2.1\nImplementation Details\nModel Training\nFor synthetic data generation,\nwe trained various T5 LMs (Raffel et al., 2020)\nwith different sizes {Small, Base, Large, XL} and\nprivacy guarantees \u03f5 \u2208 {3, 8, 16, \u221e} to generate\nsynthetic queries given corresponding input docu-\nments from the MSMARCO dataset. The T5 Small,\nBase, Large, XL models have around 60 million,\n220 million, 770 million, 3 billion parameters, re-\nspectively. All experiments were performed on a\nTPU v4 chip.\nHyperparameters\nWe trained each LM over 30\nepochs with batch size 1024 and set the maximum\ntoken length to be 384 for input documents and 128\nfor target queries. We used the DP-Adam optimizer\nwith a learning rate of 0.001 and clip norm of 0.1.\nFollowing (Li et al., 2021), we set the privacy pa-\nrameter \u03b4 = 1/2n where n is the training dataset\nsize. For sampling, we used a nucleus sampling\nstrategy (Holtzman et al., 2019) with p = 0.8.\nThe hyperparameters above were chosen from\na hyperparameter search to identify the optimal\nhyperparameters for the T5-Small model, DP-\nfinetuned on the MSMARCO training dataset. The\noptimal criteria were the highest BLEU scores\nachieved on a validation dataset. We found that\nlearning rate of 0.001, clipping norm 0.1, batch\nsize 1024, and epochs 30 mostly resulted in the\nbest model. We used these hyperparameters in all\nother T5 models. See Table 1 for the hyperparame-\nter grid.\nTable 1: Hyperparameter grid.\nHyperparameter\nValues\nToken Lengths\nInput: 384, Target: 128\nLearning Rate\n0.001 \u00b7 2\u2212k for k \u2208 {0, 1, 2, 3}\nClipping Norm\n{0.1, 0.25, 0.5, 1}\nBatch Size\n{128, 256, 512, 1024}\nEpochs\n{10, 20, 30}\n5.2.2\nData Synthesis\nWe used each DP-finetuned T5 LM to generate\nsynthetic queries given documents from the origi-\nnal training data. These pairs of synthetic queries\nand original documents constitute a new synthetic\ndataset. For qualitative comparison, we provide an\nexample in Table 2 of an original query-document\npairs and the synthetic queries generated under var-\nious model configurations and privacy levels.\n5.2.3\nPretraining and Training Data Overlap\nWe note the importance that the pretrained LMs\nused to generate the synthetic data were not\nmarkedly trained on the original query data we seek\nto make private. Otherwise, the privacy guarantees\nwould be undermined since the models would have\nalready seen the data. To address this matter in\nour experiments, we conducted an analysis to de-\ntermine the extent of overlap of the MSMARCO\ndataset on the pre-training data of T5 models, the\nC4 common crawl dataset (Raffel et al., 2020). We\nconducted multiple runs of selecting random sub-\nsets of 10,000 query and text pairs to determine if\nthere was an exact match in the C4 dataset.\nOur analysis determined that while a signifi-\ncant percentage of MSMARCO documents (~22%)\nwere exactly found in C4 on average, a negligible\npercentage (<1.9%) of MSMARCO queries were\nexactly found in C4 on average. Moreover, the\nqueries that were found tended to be generic search\nterms which could be considered public knowledge.\nSince we are interested in query-level privacy, we\nconsider this level of dataset overlap acceptable\nto give reasonable guarantees of privacy. In Sec-\ntion 6.3, we provide a more extensive study of the\nempirical privacy guarantees of our training proce-\ndure.\n5.3\nDownstream Retrieval System\n5.3.1\nImplementation Details\nModel Training\nFor each data source (i.e., orig-\ninal MSMARCO data and synthetic datasets for\nvarious \u03f5 and model sizes), we train a separate dual\nencoder model on the in-batch softmax loss. We\nutilize a separate pre-trained T5-Base encoder for\nboth the query and document encoders, sharing pa-\nrameters between them. Similar to data synthesis,\nwe use this kind of encoder to ensure that it is not\nsignificantly pretrained on the original queries. We\nemphasize that the encoders of the retrieval models\nare distinct from the T5 models used to generate\nsynthetic data.\nHyperparameters\nFor the hyperparameters in\ndual encoder model training, we used learning rate\n0.001, batch size 32, epochs 5, the maximum token\nlength 384 for documents and 128 for queries. For\nthe directly DP finetuning experiments, we used a\nclipping norm of 0.1.\n5.3.2\nBaseline Approach\nA baseline comparison of our approach will be to\ncompare against a deep retrieval system that is di-\nrectly DP-trained on the original data. For direct\nDP-training, we used the same hyperparameters as\nabove, but given the memory constraints discussed\nin Section 3.3.2, the batch size for DP-training\na dual encoder model had to be significantly de-\ncreased to 32. We do not experiment with different\ndownstream deep retrieval models since our intent\nis to compare general-purpose methods for achiev-\ning DP guarantees in retrieval systems.\n6\nEvaluation\n6.1\nEvaluation on Retrieval Tasks\nWe evaluate the retrieval models on the MS-\nMARCO test data set and various other BEIR re-\ntrieval data sets for zero-shot evaluation. We evalu-\nate on the normalized discounted cumulative gain\nscore over the top 10 predictions (NDCG@10)\nwhich measures the relevance and ranking qual-\nity of items in a recommendation list, considering\nTable 2: Synthetic query example.\nSource\nText\nDocument\nThe main cast of the show: Mickey Mouse, Minnie Mouse, Donald Duck, Daisy Duck, Goofy, and\nPluto star in the series, which focuses on interacting with the viewer to stimulate problem solving.\nOriginal Query\ncharacters from the Mickey Mouse clubhouse show\nT5-Base\n\u03f5 = \u221e\nthe Mickey Mouse show cast\n\u03f5 = 16\nwhat is the most important characters in the series\n\u03f5 = 8\nwhat is in this series?\n\u03f5 = 3\nwhat is in this code for dfr1\nT5-XL\n\u03f5 = 3\nissue with show by mickey mouse\nT5-Large\n\u03f5 = 3\nwhat is the most animated characters on disney cartoon show\nT5-Small\n\u03f5 = 3\nwhat is isn\u2019t a character will do a story\nboth quality and position. We also report the re-\ncall score over the top 10 predictions (Recall@10),\nwhich measures the percentage of times the ground\ntruth recommendation appears in the top 10 predic-\ntions. We report the evaluation results of a single\ntraining run.\n6.1.1\nMSMARCO Evaluation\nTable 3 shows the evaluation results on the MS-\nMARCO test set for deep retrieval models regularly\ntrained on the synthetically generated data under\nvarying generative model configurations and vary-\ning privacy levels. For our benchmark comparison,\nin Table 4 we display the evaluation results on the\nsame test set of deep retrieval models DP-trained\non the original data under varying privacy levels.\nIn the top rows of both tables, we provide a base-\nline reference evaluation of a dual encoder model\nregularly trained on the original data without any\nDP guarantees (i.e., \u03f5 = \u221e).\nWe observe that the retrieval model trained on\nsynthetic data with DP significantly outperform\nretrieval trained with DP on original data. As dis-\ncussed in Section 3.3.2, there are a number of chal-\nlenges associated with training DP models with\ncontrastive-style losses. Our naive approach of im-\nplementing DP-training on contrastive loss likely\nexplains poor utility of DP-training on original data.\nAdditionally, our DP synthetic data essentially in-\ntroduces additional public knowledge into the pro-\ncess, since we utilize a publicly pretrained LM.\nMoreover, we found that the retrieval model\ntrained with non-DP synthetic data outperformed\nthe retrieval model trained on the original data.\nThis suggests that synthetic data generation indeed\naugments the original data and to some extent im-\nproves generalization, whether it be through intro-\nducing additional public information or data clean-\ning. In fact, data augmentation via synthetic data\ngeneration using language models for deep retrieval\nis an area of research that has gained significant\ninterest in recent years (Dai et al., 2022; Bonifacio\net al., 2022). We also observe that performance\nincreases with increasing model size. This is con-\nsistent with similar prior results that demonstrate\nDP-SGD on over-parameterized models can per-\nform significantly better than previously thought\n(De et al., 2022; Li et al., 2021). Overall, we show\nthat training with synthetic data from DP LMs is\nviable for achieving DP guarantees and efficiency\nin retrieval models.\nTable 3: Evaluation of retrieval models. Top: Trained on DP\nsynthetic data with varying \u03f5 and fixed model size T5-Base.\nBottom: Trained on DP synthetic data with varying model\nsize and fixed \u03f5 = 3.\nSource\n\u03f5\nNDCG@10\nRecall@10\nOriginal\n\u221e\n0.2525\n0.4098\nT5-Base\n\u221e\n0.2590\n0.4192\nT5-Base\n16\n0.2027\n0.3342\nT5-Base\n8\n0.1912\n0.3196\nT5-Base\n3\n0.1830\n0.3108\nT5-XL\n3\n0.1854\n0.3098\nT5-Large\n3\n0.1833\n0.3094\nT5-Base\n3\n0.1830\n0.3108\nT5-Small\n3\n0.1346\n0.2272\nTable 4: Evaluation of retrieval models DP-trained directly\non original data with varying \u03f5.\nSource\n\u03f5\nNDCG@10\nRecall@10\nOriginal\n\u221e\n0.2525\n0.4098\nOriginal\n16\n0.0523\n0.0862\nOriginal\n8\n0.0386\n0.0649\nOriginal\n3\n0.0234\n0.0388\n6.1.2\nZero-shot Evaluation\nWe also evaluate the zero-shot generalization ca-\npabilities of a retrieval model trained on synthetic\nTable 5: Zero-shot evaluation of retrieval models trained on DP synthetic data vs. directly DP-trained retrieval models with\n\u03f5 = 16. Top table: NDCG@10. Bottom table: Recall@10.\nSource\n\u03f5\nNDCG@10\narguana cqadup dbpedia\nfiqa\nhotpot nfcorpus\nquora\nscidocs scifact\ncovid\ntouche\nOriginal \u221e\n0.2653\n0.2659\n0.3905\n0.2257 0.5232\n0.2974\n0.8126 0.1857 0.4527 0.4971 0.2764\nOriginal 16\n0.2132\n0.0990\n0.1272\n0.0870 0.1422\n0.1331\n0.6856 0.0792 0.2051 0.3133 0.1185\nT5-Base 16\n0.2757\n0.2474\n0.3728\n0.2140 0.5122\n0.2971\n0.7850 0.1750 0.4645 0.4351 0.2547\nSource\n\u03f5\nRecall@10\narguana cqadup dbpedia\nfiqa\nhotpot nfcorpus\nquora\nscidocs scifact\ncovid\ntouche\nOriginal \u221e\n0.5569\n0.3368\n0.1479\n0.2440 0.3706\n0.1013\n0.8989 0.1071 0.5801 0.0116 0.0530\nOriginal 16\n0.4388\n0.1325\n0.0313\n0.0906 0.1108\n0.0365\n0.7762 0.0503 0.2969 0.0063 0.0149\nT5-Base 16\n0.5768\n0.3165\n0.1217\n0.2261 0.3398\n0.1110\n0.8763 0.1066 0.5848 0.0101 0.0489\ndata. We compare against a retrieval model trained\non the original data with no DP (i.e., \u03f5 = \u221e) and\nwith \u03f5 = 16. See Table 5 for the results. Again,\nour results demonstrate significant advantage of DP\nsynthetic data compared to DP-training on original\ndata, nearly matching and in some cases outper-\nforming the non-DP results. This suggests that\nthe benefits of synthetic data generation can out-\nweigh the utility degradation of DP-training with\nreasonable levels of privacy, at least in zero-shot\ngeneralization tasks.\n6.2\nSimilarity between Synthetic and Original\nDatasets\nWe compute measures of similarity between the\nsynthetic data generated by the DP-trained T5 mod-\nels against the original data. Since the synthetic\ndata is one-to-one generated from the original data,\nwe can compute BLEU scores to evaluate similarity\n(Post, 2018). We also compute the MAUVE scores,\nshown to be more capable of comparing similarity\nof text distributions (Pillutla et al., 2021). See Ta-\nble 6 for the scores. We observe that the non-DP\nfinetune model generates synthetic data that is as\nsimilar as one could expect under these metrics,\nand there is a significant drop with finite \u03f5, with\nincreasing similarity with higher \u03f5 and increasing\nmodel size. By comparing the similarity scores\nwith the retrieval evaluation results, we observe\nthat while larger models lead to drastic improve-\nments in the synthetic data similarity, the down-\nstream retrieval performance sees comparatively\nmore modest gains with increasing model size.\n6.3\nEmpirical Privacy\nThe provable privacy provided by DP decays sig-\nnificantly as \u03f5 grows, but prior work has shown that\nTable 6: Similarity scores of generated synthetic data. Top:\nVarying \u03f5 and fixed model size. Bottom: Varying model size\nwith fixed \u03f5 = 3.\nModel\n\u03f5\nBLEU\nMAUVE\nT5-Base\n\u221e\n0.2939\n0.9763\nT5-Base\n16\n0.0984\n0.3715\nT5-Base\n8\n0.0940\n0.3431\nT5-Base\n3\n0.0856\n0.2974\nT5-XL\n3\n0.1021\n0.7117\nT5-Large\n3\n0.1096\n0.6359\nT5-Base\n3\n0.0940\n0.2974\nT5-Small\n3\n0.0436\n0.2296\neven these large values can provide strong protec-\ntion against state of the art privacy attacks (Car-\nlini et al., 2019, 2022; Ponomareva et al., 2023).\nTo verify our training technique still follows this\ntendency, we evaluate here the empirical privacy\nleakage of DP-trained language models, using the\ncanary exposure metric introduced in (Carlini et al.,\n2019). This technique is frequently used to evaluate\nempirical privacy (Zanella-B\u00e9guelin et al., 2020;\nRamaswamy et al., 2020; Jagielski et al., 2022).\nTo perform this test, we construct examples with\nprivate information, referred to as canaries, and\nintroduce a subset of them into the original train-\ning data, and measure how likely the model is to\noutput the injected canaries. In general, canary\ngeneration is a domain-dependent decision, so we\ndesign canaries for our retrieval application using\nthe three following types of query-document pairs:\n(random query, random 10-digit string), (random\nquery, corresponding document + random 10-digit\nstring), (random query, random document + ran-\ndom 10-digit string). The secret part of each canary\nis the random 10-digit string.\nWe train the language model on this modified\ndataset using different DP guarantees, generate syn-\nthetic datasets, and assess canary exposure. We\nconduct this experiment multiple times with dif-\nferent canaries and DP guarantees, averaging the\nmetrics and reporting the results in Table 7. As an-\nticipated, training without DP leads to significant\nleakage. Canaries repeated 10 times are frequently\nextractable, and those repeated 100 times are al-\nways extractable. However, our approach with a\nlarge \u03f5 of 16 prevents the model from leaking se-\ncrets and significantly increases the rank. Recent\ntechniques for converting attack success rates to\nlower bounds on the \u03f5 parameter (Stock et al., 2022)\nallow us to interpret these ranks as a lower bound\nof roughly 0.015 on \u03f5. This large gap is consis-\ntent with prior findings on the empirical privacy of\nDP-SGD on language models (Carlini et al., 2019;\nPonomareva et al., 2023).\nTable 7: Privacy leakage.\nModel\n\u03f5\nRepetition = 10\nRepetition = 100\nRank\nLeaked\nRank\nLeaked\nT5-Base\n\u221e\n1/100\n67%\n1/100\n100%\nT5-Base\n16\n43/100\n0%\n32/100\n0%\n7\nConclusion\nOur work focused on ensuring DP guarantees in\ntraining deep retrieval systems. We discussed the\nlimitations of DP-training such systems with the\noften used in-batch softmax loss function, which is\nnon-per-example decomposable. We introduce an\napproach of using DP LMs to generate private syn-\nthetic queries for downstream deep retrieval train-\ning. This approach ensures theoretical guarantees\non query-level privacy prior to downstream train-\ning, thereby bypassing some of the limitations of\nDP-training. Furthermore, we empirically demon-\nstrate that our approach improves retrieval quality\ncompared to direct DP-training, without compro-\nmising query privacy. Our work highlights the\npotential of LMs to overcome crucial limitations in\nDP-training ML systems.\nLimitations\nThere are a few limitations to our\napproach. Firstly, while we observed that larger\nLMs generate higher quality synthetic queries, it is\nworth noting that training such large models may\ntoo computationally expensive. Exploring more\nparameter-efficient finetuning methods tailored to\nDP-training could mitigate the computational bur-\nden associated with training such larger models.\nSecondly, it is necessary for the publicly pretrained\nLM utilized for generating synthetic queries to not\nhave been pretrained on the original queries we\naim to privatize. This imposes a constraint on the\nchoice of LMs suitable for generating private syn-\nthetic queries. Next, as observed in the synthetic\nexample in Table 2, the DP-finetuned LMS can\nsometimes generate incoherent queries, which can\nlimit the relevance and interpretability of the data.\nLastly, it is essential to recognize that our approach\nexclusively ensures query-level privacy. For achiev-\ning more general example-level privacy, it is nec-\nessary to resort to more conventional DP training\nmethods.\nRisks & Ethical Considerations\nLanguage mod-\nels have been demonstrated to implicitly memorize\nsensitive user information in the training data (Car-\nlini et al., 2021, 2023). Therefore, the introduc-\ntion of these high-capacity models into retrieval\nand recommendation systems poses serious privacy\nrisks. Data privacy is a crucial consideration in the\nresponsible development of these personalized sys-\ntems. Our work directly offers potential solutions\nto address these issues. However, it is important to\nacknowledge limitations on privacy guarantees our\napproach to prevent undesired privacy risks.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC con-\nference on computer and communications security,\npages 308\u2013318.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016.\nMs marco: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and\nRodrigo Nogueira. 2022. Inpars: Data augmentation\nfor information retrieval using large language models.\narXiv preprint arXiv:2202.05144.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nNicholas Carlini, Steve Chien, Milad Nasr, Shuang\nSong, Andreas Terzis, and Florian Tramer. 2022.\nMembership inference attacks from first principles.\nIn 2022 IEEE Symposium on Security and Privacy\n(SP), pages 1897\u20131914. IEEE.\nNicholas Carlini, Jamie Hayes, Milad Nasr, Matthew\nJagielski, Vikash Sehwag, Florian Tramer, Borja\nBalle, Daphne Ippolito, and Eric Wallace. 2023. Ex-\ntracting training data from diffusion models. arXiv\npreprint arXiv:2301.13188.\nNicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neural\nnetworks. In USENIX Security Symposium, volume\n267.\nNicholas Carlini,\nFlorian Tramer,\nEric Wallace,\nMatthew Jagielski, Ariel Herbert-Voss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633\u20132650.\nPaul Covington, Jay Adams, and Emre Sargin. 2016.\nDeep neural networks for youtube recommendations.\nIn Proceedings of the 10th ACM Conference on Rec-\nommender Systems, New York, NY, USA.\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B\nHall, and Ming-Wei Chang. 2022. Promptagator:\nFew-shot dense retrieval from 8 examples. arXiv\npreprint arXiv:2209.11755.\nSoham De, Leonard Berrada, Jamie Hayes, Samuel L.\nSmith, and Borja Balle. 2022.\nUnlocking high-\naccuracy differentially private image classification\nthrough scale.\nCynthia Dwork, Aaron Roth, et al. 2014. The algorith-\nmic foundations of differential privacy. Foundations\nand Trends\u00ae in Theoretical Computer Science, 9(3\u2013\n4):211\u2013407.\nDaniel Gillick, Sayali Kulkarni, Larry Lansing, Alessan-\ndro Presta, Jason Baldridge, Eugene Ie, and Diego\nGarcia-Olano. 2019. Learning dense representations\nfor entity retrieval. arXiv preprint arXiv:1909.10506.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nMengdi Huai, Di Wang, Chenglin Miao, Jinhui Xu,\nand Aidong Zhang. 2020. Pairwise learning with\ndifferential privacy guarantees. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 694\u2013701.\nJui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia,\nDavid Zhang, Philip Pronin, Janani Padmanab-\nhan, Giuseppe Ottaviano, and Linjun Yang. 2020.\nEmbedding-based retrieval in facebook search. In\nProceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery &; Data Min-\ning, KDD \u201920, page 2553\u20132561, New York, NY, USA.\nAssociation for Computing Machinery.\nMatthew Jagielski, Om Thakkar, Florian Tramer,\nDaphne Ippolito, Katherine Lee, Nicholas Carlini,\nEric Wallace, Shuang Song, Abhradeep Thakurta,\nNicolas Papernot, et al. 2022.\nMeasuring forget-\nting of memorized training examples. arXiv preprint\narXiv:2207.00099.\nYilin Kang, Yong Liu, Jian Li, and Weiping Wang.\n2021.\nTowards sharper utility bounds for differ-\nentially private pairwise learning. arXiv preprint\narXiv:2105.03033.\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020.\nDense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nXuechen Li, Florian Tramer, Percy Liang, and Tatsunori\nHashimoto. 2021. Large language models can be\nstrong differentially private learners. arXiv preprint\narXiv:2110.05679.\nJustus Mattern, Zhijing Jin, Benjamin Weggenmann,\nBernhard Schoelkopf, and Mrinmaya Sachan. 2022.\nDifferentially private language models for secure data\nsharing. arXiv preprint arXiv:2210.13918.\nFatemehsadat Mireshghallah, Richard Shin, Yu Su, Tat-\nsunori Hashimoto, and Jason Eisner. 2022. Privacy-\npreserving domain adaptation of semantic parsers.\narXiv preprint arXiv:2212.10520.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hern\u00e1ndez \u00c1brego, Ji Ma, Vincent Y Zhao,\nYi Luan, Keith B Hall, Ming-Wei Chang, et al.\n2021. Large dual encoders are generalizable retriev-\ners. arXiv preprint arXiv:2112.07899.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. Advances in Neural Information Process-\ning Systems, 34:4816\u20134828.\nNatalia Ponomareva, Hussein Hazimeh, Alex Kurakin,\nZheng Xu, Carson Denison, H Brendan McMahan,\nSergei Vassilvitskii, Steve Chien, and Abhradeep\nThakurta. 2023. How to dp-fy ml: A practical guide\nto machine learning with differential privacy. arXiv\npreprint arXiv:2303.00654.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. arXiv preprint arXiv:1804.08771.\nPranav Putta, Ander Steele, and Joseph W Ferrara. 2023.\nDifferentially private conditional text generation for\nsynthetic data production.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2020.\nRocketqa:\nAn opti-\nmized training approach to dense passage retrieval\nfor open-domain question answering. arXiv preprint\narXiv:2010.08191.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nSwaroop Ramaswamy, Om Thakkar, Rajiv Mathews,\nGalen Andrew, H Brendan McMahan, and Fran\u00e7oise\nBeaufays. 2020. Training production language mod-\nels without memorizing user data. arXiv preprint\narXiv:2009.10031.\nTimo Schick and Hinrich Sch\u00fctze. 2021. Few-shot text\ngeneration with natural language instructions. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 390\u2013\n402.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nCoRR, abs/1804.04235.\nPierre Stock, Igor Shilov, Ilya Mironov, and Alexandre\nSablayrolles. 2022. Defending against reconstruction\nattacks with r\u00e9nyi differential privacy. arXiv preprint\narXiv:2202.07623.\nPranav Subramani, Nicholas Vadivelu, and Gautam Ka-\nmath. 2021. Enabling fast differentially private sgd\nvia just-in-time compilation and vectorization. Ad-\nvances in Neural Information Processing Systems,\n34:26409\u201326421.\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogenous benchmark for zero-shot evalua-\ntion of information retrieval models. arXiv preprint\narXiv:2104.08663.\nZhiyu Xue, Shaoyang Yang, Mengdi Huai, and Di Wang.\n2021. Differentially private pairwise learning revis-\nited. In IJCAI, pages 3242\u20133248.\nXiang Yue, Huseyin A Inan, Xuechen Li, Girish Ku-\nmar, Julia McAnallen, Huan Sun, David Levitan, and\nRobert Sim. 2022. Synthetic text generation with\ndifferential privacy: A simple and practical recipe.\narXiv preprint arXiv:2210.14348.\nSantiago Zanella-B\u00e9guelin, Lukas Wutschitz, Shruti\nTople, Victor R\u00fchle, Andrew Paverd, Olga Ohri-\nmenko, Boris K\u00f6pf, and Marc Brockschmidt. 2020.\nAnalyzing information leakage of updates to natural\nlanguage models. In Proceedings of the 2020 ACM\nSIGSAC conference on computer and communica-\ntions security, pages 363\u2013375.\n"
  }
]