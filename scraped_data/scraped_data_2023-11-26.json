[
  {
    "title": "GAIA: a benchmark for General AI Assistants",
    "link": "https://arxiv.org/pdf/2311.12983.pdf",
    "upvote": "166",
    "text": "GAIA:\nA Benchmark for General AI Assistants\nGr\u00b4egoire Mialon1, Cl\u00b4ementine Fourrier2, Craig Swift3, Thomas Wolf2, Yann LeCun1, Thomas\nScialom4\n1FAIR, Meta, 2HuggingFace, 3AutoGPT, 4GenAI, Meta\nWe introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a\nmilestone in AI research. GAIA proposes real-world questions that require a set of fundamental\nabilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency.\nGAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we\nshow that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins. This notable\nperformance disparity contrasts with the recent trend of LLMs outperforming humans on tasks\nrequiring professional skills in e.g. law or chemistry. GAIA\u2019s philosophy departs from the current\ntrend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit\nthat the advent of Artificial General Intelligence (AGI) hinges on a system\u2019s capability to exhibit\nsimilar robustness as the average human does on such questions. Using GAIA\u2019s methodology, we\ndevise 466 questions and their answer. We release our questions while retaining answers to 300 of\nthem to power a leader-board hereby accessible.\nDate: November 23, 2023\nCorrespondence: {gmialon,tscialom}@meta.com, clementine@huggingface.co\nCode: https://huggingface.co/gaia-benchmark\n1\nIntroduction\nLarge Language Models (LLMs) arguably open the way to general purpose systems. Indeed, the latest among\nthem (OpenAI, 2023; Anthropic, 2023; Anil et al., 2023; Touvron et al., 2023) are fluent, knowledgeable,\naligned to some extent with human preferences (Ouyang et al., 2022), and can be augmented (Mialon et al.,\n2023) with tools such as web browsers or code interpreters in a zero or few-shot setting (Brown et al., 2020).\nHowever, evaluating these systems is an open problem: given their emerging new capabilities, LLMs are\nregularly breaking AI benchmarks, at an ever-increasing rate (Kiela et al., 2023).\nIn search for more challenging benchmarks, current trend suggests to seek tasks that are ever more difficult\nfor humans, and challenge LLMs with more intricate educational assessments, for example in STEM and\nLaw, or target more complex realisations, such as writing a coherent book. But, tasks that are difficult for\nhumans are not necessarily difficult for recent systems: the challenging MMLU or GSM8k benchmarks for\nexample (Hendrycks et al., 2021; Cobbe et al., 2021) are already close to be solved,1 due to rapid LLM\nimprovement possibly combined with data contamination.2 Furthermore, open-ended generation generally\nrequires human or model-based evaluation (Zheng et al., 2023). Human evaluation will become less and less\nfeasible when increasing the task complexity, e.g. in terms of output length or required skills: how to evaluate a\nbook generated by an AI, or solutions to maths problems that few people in the world can solve? Model-based\nevaluations on the other hand are by construction dependent of stronger models hence cannot evaluate\nnew state-of-the-art models, without mentioning potential subtle biases such as preferring the first choice\npresented (Zheng et al., 2023). Overall, evaluating new AI systems requires to rethink benchmarks (Chollet,\n2019).\n1GPT4 does 86.4% on MMLU. Human non-specialist accuracy on the benchmark is only 34.5% Expert-level human performance\nis estimated at 89.8%.\n2See for example the case of Hellaswag.\n1\narXiv:2311.12983v1  [cs.CL]  21 Nov 2023\nLevel 1\nQuestion: What was the actual enrollment count of the clinical trial on H. pylori in acne vulgaris\npatients from Jan-May 2018 as listed on the NIH website?\nGround truth: 90\nLevel 2\nQuestion: If this whole pint is made up of ice cream, how many percent above\nor below the US federal standards for butterfat content is it when using the\nstandards as reported by Wikipedia in 2020? Answer as + or - a number rounded\nto one decimal place.\nGround truth: +4.6\nLevel 3\nQuestion: In NASA\u2019s Astronomy Picture of the Day on 2006 January 21, two astronauts are visible,\nwith one appearing much smaller than the other. As of August 2023, out of the astronauts in the\nNASA Astronaut Group that the smaller astronaut was a member of, which one spent the least time\nin space, and how many minutes did he spend in space, rounded to the nearest minute? Exclude any\nastronauts who did not spend any time in space. Give the last name of the astronaut, separated from\nthe number of minutes by a semicolon. Use commas as thousands separators in the number of minutes.\nGround truth: White; 5876\nFigure 1 Sample GAIA questions. Completing the tasks requires fundamental abilities such as reasoning, multi-\nmodality handling, or tool use proficiency. Answers are unambiguous and by design unlikely to be found in plain text\nin training data. Some questions come with additional evidence, such as images, reflecting real use cases and allowing\nbetter control on the questions.\nAlternatively to tasks that are harder for humans, AI systems could be asked to solve conceptually simple\ntasks yet that require accurate execution of complex sequences of actions, with large combinatorial spaces.\nThe output could only be obtained upon successful completion of the task and be easy to validate, analogous\nto the Proof of Work algorithm (Jakobsson and Juels, 1999; Dwork and Naor, 1993), where a computer is\nasked to solve a complex problem whose solution is easy to verify. Tasks for AI assistants, given their need for\naccess to a diverse and uncertain world, meet this criterion while being inherently rooted in practical use\ncases.\nWe move in that direction by proposing GAIA, a benchmark for General AI Assistants featuring 466 carefully\ncrafted questions and their answer, along with the associated design methodology. Our questions are easy\nto create, challenging for AI systems\u2014for LLMs, most require complex generations\u2014, yet admit a unique,\nfactual answer, allowing a simple and robust automatic evaluation.\nGAIA attempts to avoid current pitfalls of LLMs evaluation by targeting:\n- Real-world and challenging questions. For example, a LLM will typically need to browse the open and\nchanging web, handle multi-modality, or reason over multiple steps to answer our questions. Conversely,\nmany LLM benchmarks are quite specific and/or restricted to closed and synthetic environments.\n- Easy interpretability through conceptually simple tasks\u2014non experts annotators exhibit a near perfect\nscore\u2014, associated reasoning trace, and few but highly curated questions. This is in contrast with\naggregated benchmarks that can lack efficiency and reliability (Perlitz et al., 2023).\n- Non-gameability. Answering the questions requires successful completion of some number of steps, which\ncannot easily be brute forced due to their diversity. The possibility to check the reasoning trace, the\naccuracy required in the answers, their absence in plain text from the internet prevent a possible data\ncontamination. In contrast, multiple choice answers (e.g., MMLU) make contamination assessment more\ndifficult since a wrong reasoning trace can more easily get to the correct choice.\n- Simplicity of use. Crucially, the answers to our questions are factoid, concise and unambiguous. These\n2\nproperties allow simple, fast and factual evaluation. Our questions are meant to be answered in zero\nshot, limiting the influence of the evaluation setup. By opposition, many LLM benchmarks require\nevaluations that are sensitive to the experimental setup such as the number and nature of prompts\n(Liang et al., 2022b) (Section 8.2), or the benchmark implementation.3\nIn spite of being successful at tasks that are difficult for humans, the most capable LLMs do poorly on GAIA.\nEven equipped with tools, GPT4 does not exceed a 30% success rate for the easiest of our tasks, and 0% for\nthe hardest. In the meantime, the average success rate for human respondents is 92%. Consequently, a system\ncapable of solving GAIA can be assessed in the context of t-AGI,4 noting that humans typically take between\n6 minutes for the simplest questions to 17 minutes for the most complex ones. From a related perspective,\nsuch system would arguably be a competent General AI within the framework recently proposed in Morris\net al. (2023), which also appear to be the next milestone in AI research since ChatGPT (OpenAI, 2023) is\none level below. This paper covers the composition of GAIA, its design choices, and explain how to craft\nquestions and the associated challenges so that the community can further extend the benchmark to target\nemerging questions such as safety associated to tool use, or multi-modality. We also analyse the successes\nand shortcomings of some of the most capable assistants to date, illustrating the potential of augmenting\nLLMs. We release a developer set of 166 annotated questions and release the remaining 300 questions without\nannotations: the benchmark will be notably hosted as a leaderboard. We hope our methodology will help\naddressing the problem of open ended generation evaluation in NLP and beyond, and believe the successful\nresolution of GAIA would be an important milestone towards the next generation of AI systems.\n2\nRelated work\nEvaluating Large Language Models.\nAs LLMs capabilities have rapidly progressed, benchmarks become\nsaturated at an increasing speed. As a example, reading comprehension was still a challenging task a few\nyears alo (Rajpurkar et al., 2016). Wang et al. (2018) introduced the General Language Understanding\nEvaluation benchmark (GLUE), on which models surpassed humans within a year. Its extension (Wang et al.,\n2019) didn\u2019t resist for more than a couple of years after its release. More generally, with each passing year,\nstatic benchmarks are saturated and solved at human level at an ever increasing speed, as well illustrated by\nKiela et al. (2023). While searching for harder evaluations, a natural direction is to explore tasks requiring\nprofessional level knowledge in various fields such as law or science: an example is MMLU (Hendrycks et al.,\n2021), containing over 15,000 questions covering 57 subjects across STEM, the humanities, the social sciences,\nand more. And yet, LLMs already passed human performance on these, and have even been reported to\nreach a stage where they could plausibly pass the US bar exam (OpenAI, 2023) or exceed the passing score\non USMLE, a US examination program used to assess clinical competency and grant licensure (Nori et al.,\n2023). Directions to evaluate LLMs more holistically, on their broader conversational aspects, have included\n(i) compilations of evaluations (Gao et al., 2021; Liang et al., 2022a; Srivastava et al., 2023), which are\noften difficult to aggregate meaningfully and are prone to contamination through data leakage, (ii) human\nevaluation, which is time-consuming and difficult to scale, or (iii) model based evaluation to overcome this\nlimitation (Zheng et al., 2023). However, this latter solution relies on using a more capable LLM (often GPT4)\nthan the one currently evaluated, and the quality of the evaluation is affected by the shortcomings of the\nevaluator LLM, which are not always obvious and can lead to subtly incorrect results.\nEvaluating General Assistants.\nWhile there is ongoing effort to turn Large Language Models into general-\npurpose assistants (see our discussion in Appendix A), appropriate evaluation is lagging behind. Most\nevaluations rely on the use of closed systems, specific API calls, and a given \u201ccorrect way\u201d to attain the\nanswer, or simply repurpose existing evaluation datasets. ToolQA (Zhuang et al., 2023) or Gentopia (Xu\net al., 2023a) for example combine existing datasets with human annotations (MMLU, MATH, etc.) at the\nrisk of contamination during training, and without ensuring tool usage is actually tested. Gorilla (Patil et al.,\n2023) introduces APIBench, which tests how well an agent like system calls its specific API, similarly to\nAPI-Bank (Li et al., 2023b), which provides an API pool to help the LLM during its evaluation. AgentBench\n3https://huggingface.co/blog/evaluating-mmlu-leaderboard\n4As defined in https://www.alignmentforum.org/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi, a t-AGI\nbeats, on most tasks, most human experts who are given time t to perform the task\n3\n(Liu et al., 2023a) is more general, and provides a number of closed box environments inside which assistant\nLLMs can be deployed to answer user queries (from Unix shells to WebShopping APIs). However, because\nsuch evaluations rely on closed environments, they risk evaluating how well the assistants have learned to\nuse specific APIs, instead of more general results grounded in real world interactions. By opposition, GAIA\ndoes not specify possible APIs, and relies on interactions with the real world. OpenAGI (Ge et al., 2023)\nintroduces both a platform and a benchmark, made of a number of multi-steps tasks across modalities and\ncapabilities, and is closer to our work. The core difference with GAIA is that their tasks focus on current\nmodel capabilities rather than upcoming advancements.\n3\nGAIA\nThis section covers the design and content of GAIA, as well as guidelines for creating questions and associated\nchallenges.\n3.1\nA convenient yet challenging benchmark for general AI assistants\nWhat is GAIA and how does it work?\nGAIA is a benchmark for AI systems proposing general assistant\nquestions. GAIA attempts to circumvent different pitfalls of LLMs evaluation. It is composed of 466 questions\ndesigned and annotated by humans. These questions are text-based, and sometimes come with a file (such as\nan image or a spreadsheet). They cover various assistant use cases such as daily personal tasks, science, or\ngeneral knowledge. The questions are designed to admit a short, single correct answer, therefore easy to verify.\nTo use GAIA, one only needs to zero-shot prompt an AI assistant with the questions and attached evidence\nif there are some. Scoring perfectly on GAIA requires a varied set of fundamental abilities (see Section 3.3).\nWe provide questions along various with meta-data in supplementary material.\nDesign choices.\nGAIA results both from the need for revised AI benchmarks, and the observed shortcomings\nof LLM evaluation.\nOur first principle is to target questions that are conceptually simple although potentially tedious for humans,\nyet varied, rooted in the real world and challenging for current AI systems. This allows to focus on fundamental\nabilities such as quick adaptation via reasoning, multi-modality understanding, and potentially diverse tool\nuse, rather than specialised skills (Chollet, 2019). The questions generally consist in finding and transforming\ninformation gathered from different and various sources, such as provided documents or the open and changing\nweb, to produce an accurate answer. To answer the first example question above (Figure 1), LLMs should\ntypically browse the web to find a study, then look for the correct enrolment. This goes against the trend\nof benchmarks that are increasingly difficult for humans, and/or operate in purely textual or artificial\nenvironments.\nOur second principle is interpretability.\nThe restricted number of highly curated questions makes the\nbenchmark easier to use compared to aggregated ones (Perlitz et al., 2023). The conceptual simplicity of the\ntask (human success rate is 92%) makes it easy for users to understand a model\u2019s reasoning trace. For the\nLevel 1 question from Figure 1, the reasoning trace will mostly consist in checking the correct website, and\nreport the correct enrolment, which is simple to verify.\nOur third principle is robustness against memorization: GAIA aims to be less gameable than most current\nbenchmarks. To complete a task, a system has to plan and successfully complete some number of steps\nsince the resulting answer is absent by design in plain text from current pre-training data. A progress in\naccuracy reflects actual system progress. Due to their diversity and the size of the action space, these tasks\ncannot be brute-forced without cheating, for example by memorizing the ground truth. Although accidental\nmemorization is possible through data contamination, the accuracy required in the answers, their absence\nfrom pre-training data, and the possibility to check the reasoning trace mitigate this risk. In contrast, multiple\nchoice answers make contamination assessment difficult since a wrong reasoning trace can still get to the\ncorrect choice. If catastrophic memorization happens in spite of these mitigations, it is easy to craft new\nquestions using the guidelines we provide in Section 3.4.\n4\nSystem prompt: You are a general AI assistant. I will ask you a question. Report your thoughts, and\nfinish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER].\nYOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated\nlist of numbers and/or strings.\nIf you are asked for a number, don\u2019t use comma to write your number neither use units such as $ or\npercent sign unless specified otherwise.\nIf you are asked for a string, don\u2019t use articles, neither abbreviations (e.g. for cities), and write the\ndigits in plain text unless specified otherwise.\nIf you are asked for a comma separated list, apply the above rules depending of whether the element\nto be put in the list is a number or a string.\nGAIA Question: The attached Excel file contains the sales of menu items for\na local fast-food chain. What were the total sales that the chain made from\nfood (not including drinks)? Express your answer in USD with two decimal\nplaces.\nuploaded.xlsx\nGPT4 (Advanced Data Analysis mode):\nWorking:\nimport pandas as pd\n# Load the Excel file\nfile_path = '/mnt/data/uploaded.xlsx'\ndf = pd.read_excel(file_path)\nWorking:\n# Calculate the total sales from food items (not including drinks i.e., Soda)\ntotal_food_sales = df[\n['Burgers', 'Hot Dogs', 'Salads', 'Fries', 'Ice Cream']\n].sum().sum()\n# Format the result to USD, with a dollar sign and two decimal places\ntotal_food_sales_formatted = f\"$\u2013total_food_sales:,.2f\u02dd\"\nFINAL ANSWER: $89706.00\nGround truth: 89706.00 \u2713\nFigure 2 To answer GAIA, an AI assistant such as GPT4 (here equipped with a code interpreter) needs to complete a\nfew steps, potentially using tools or reading files.\nOur last principle is easiness of use. Our tasks are simple prompts that may come with an additional file.\nCrucially, the answers to our questions are factoid, concise and unambiguous. These properties allow simple,\nfast and factual evaluation. Our questions are meant to be answered in zero shot, limiting the influence of\nthe evaluation setup. By opposition, many LLM benchmarks require evaluations that are sensitive to the\nexperimental setup such as the number and nature of prompts (Liang et al., 2022b) (Section 8.2), or the\nbenchmark implementation.\n3.2\nEvaluation\nGAIA is designed such that evaluation is automated, fast, and factual. In practice, each question calls for\nan answer that is either a string (one or a few words), a number, or a comma separated list of strings or\nfloats, unless specified otherwise. There is only one correct answer. Hence, evaluation is done via quasi exact\nmatch between a model\u2019s answer and the ground truth (up to some normalization that is tied to the \u201ctype\u201d\nof the ground truth). A system (or prefix) prompt is used to inform the model about the required format,\nsee Figure 2. In practice, GPT4 level models easily follow our format. We provide our scoring function along\n5\n0\n50\n100\n150\n200\n250\n300\n350\nNumber of questions\nN/A\nDiverse filetype reading\nMulti-modality\nCoding\nWeb browsing\n32\n129\n138\n154\n355\nCapabilities required to solve GAIA\n0\n10\n20\n30\n40\nNumber of steps taken\n0\n1\n2\n3\n4\n5\n6\nNumber of different tools used\nLevel 1\nLevel 2\nLevel 3\nAn overview of GAIA questions\nFigure 3 Left: number of questions per capability requiring at least this capability to be solved. Right: each dot\ncorresponds to a GAIA question. At a given location, the size of the dots are proportional to the number of questions,\nand only the level with the highest number of questions is displayed for readability. Both figures are based on\ninformation reported by human annotators when answering the questions, and AI systems might proceed differently.\nwith the leaderboard.\n3.3\nComposition of GAIA\nThis subsection delves into the composition of the 466 questions we devised for GAIA.\nCapabilities coverage.\nScoring perfectly on GAIA requires advanced reasoning, multi-modality understanding,\ncoding capabilities and generally tool use, e.g web browsing, for which we provide a more precise definition\nin Appendix C. We also include questions requiring to process varied data modalities such as PDFs, spreadsheets,\nbut also images, videos or audio, whose distribution is reported in Appendix C (Figure 6). Figure 3 (left) is\nan overview of these capabilities. Although web browsing is a key component of GAIA, we do not require\nassistants to perform actions other than \u201cclicks\u201d on a website such as uploading a file, post a comment or book\na meeting. Testing these capabilities in real environments while avoiding spamming websites requires careful\nconsideration that we leave for future work, and refer the reader to recent works proposing closed environments\nfor LLMs agents (Liu et al., 2023a). We do not provide a more detailed list of required capabilities to solve\nthe benchmark since most questions can be solved equally well via different combinations of capabilities. For\nexample, a given piece of evidence may have been properly memorised by an assistant LLM, or retrieved via a\nweb search. In particular, we do not provide a fine-grained benchmarking of tool usage by LLMs, and refer\nthe reader to Xu et al. (2023b); Li et al. (2023c).\nIncreasing difficulty.\nThe questions can be sorted into three levels of increasing difficulty depending on the\nnumber of steps required to solve the questions, and the number of different tools needed to answer the\nquestion. There is naturally not a single definition of step or tool, and possibly many paths to answer a\ngiven question. Therefore, we rely as a proxy on the number of steps and tools used by our annotators when\ncrafting the questions. Figure 3 (right) illustrates the distribution of our questions along these two axes. Tools\nare always related to one or more capability (see Appendix C). We loosely use the following definitions to\nattribute a level to a question:\n- Level 1 questions generally require no tools, or at most one tool but no more than 5 steps.\n- Level 2 question generally involve more steps, roughly between 5 and 10 and combining different tools\nis needed.\n- Level 3 are questions for a near perfect general assistant, requiring to take arbitrarily long sequences of\nactions, use any number of tools, and access to the world in general.\nAn illustration of these levels is provided in Figure 1. Those definitions are not hard constraints: for example,\n6\na question with less than 10 annotator steps but that requires complex web navigation might be categorised\nas Level 3 rather than 2. Our definition of the difficulty is validated in Section 4.\nDistribution of required capabilities.\nWhile GAIA targets real-world assistant questions, we also include\ntasks that could potentially benefits physically impaired people, such as finding a piece of information in a\nsmall audio file. Finally, we make our best effort to cover various topic domains and cultures, although the\nlanguage of the dataset is restricted to English (see Section 6).\n3.4\nBuilding and extending GAIA\nThis subsection delves into our question design and annotation process. In particular, we discuss some\nassociated challenges and hope our insights will help the community building overGAIA.\nCrafting questions.\nOur questions are created by humans5 and aim to reflect realistic use cases of AI assistants.\nThe authors designed initial questions, and gave them as examples to annotators along with instructions\n(reported in Appendix D) to create more questions. The questions were based on one or more sources of truth\nthat were often specified in the question to avoid ambiguity. Examples of sources of truth are trusted web\npages that have low chance to disappear anytime soon e.g., Wikipedia, Papers With Code, or arXiv. In other\ncases, the source of truth is entirely provided with the question, e.g., an attached document. The last case\nis a self-contained question, e.g., a small puzzle. We do not specify a fixed list of sources of truth in order\nto enforce question diversity and avoid memorisation. Apart from puzzles, most questions were created by\nfinding and potentially combining information from different sources of truth to produce a specific answer.\nOnce a question was created, it was also annotated, i.e. the question creator provided an answer as well as\nmeta-data: which tools were needed, which steps were taken, or how many time was required to answer. A\ntypical annotation result is presented in Table 1 (Appendix C).\nValidating questions.\nMost of the work associated with crafting questions consists in ensuring that they are\nunambiguous, i.e., there is a single correct answer. This property allows fast and factual evaluation, hence\nit is crucial to maintain it. Ambiguities can be subtle and rarely obvious to the creator of a question. For\nexample, a question is ambiguous if it does not specify a version for a web page while the information needed\nto answer the question is different in other versions. We therefore asked two new annotators to independently\nanswer each question. If the original annotator and the two new annotators arrived at the same answer,\nthe question was validated. Questions on which annotators disagreed generally only required a simple fix,\nbut were removed otherwise. For this reason, question creation can hardly be automated while keeping the\ninterest and variety of questions high. We report statistics on this validation phase in Table 3 (Appendix C).\n68% of the questions were good as is, while the rest had to be corrected or removed. While the questions are\nconceptually simple, annotators might do inadvertent mistakes: we estimate the annotator\u2019s success rate to\nbe 92% when aggregated on all levels of difficulty, and report this as the human score for GAIA. It is close to\nperfect, demonstrating that GAIA is simple for non experts. We estimate the creation of a question, including\nits validation by two supplementary annotators and potential repairs, to require two hours of annotator time.\nChallenges associated to relying on the web.\nDesigning questions can be delicate when a source of truth is\nhosted on the web. First, the evidence might change over time. For example, a Wikipedia article could be\nupdated between the moment the question is created and the moment it is asked to an AI assistant, potentially\nremoving the evidence required to answer. For such questions, it is often important to specify a version of\nthe evidence, such as the page\u2019s date. In practice, we find our benchmark to be robust to these changes\nsince we try to rely as much as possible on evidence that will likely pass the test of time. Second, some\nwebsite owners wish to prevent access to parts or totality of their website from bots via their robots.txt files.\nWhile this is rather a demand than a constraint, it is obviously desirable to comply. For example, OpenAI\nprovides instruction to website owners wishing to forbid access to GPT4 on how to modify their robots.txt\naccordingly. Hence, we verify that accessing the part of the website hosting the evidence is not restricted.\n5More precisely, in a collaboration between our teams and compensated annotators from Surge AI.\n7\n4\nLLMs results on GAIA\nEvaluating LLMs with GAIA only requires the ability to prompt the model, i.e an API access. We use a\nprefix prompt before asking the model a question. To ease answer extraction, we specify a format in the prefix\nprompt, see Figure 2. We evaluate GPT4 (OpenAI, 2023) with and without plugins,6 as well as AutoGPT 7\nwith GPT4 as backend. GPT4 currently requires to manually select plugins (see paragraph below). On the\ncontrary, AutoGPT is able to do this selection automatically. Our non-LLM baselines are human annotators,\nand web search. For the latter, we type our questions in a search engine and check whether the answer can be\ndeducted from the first page of results. This allows us to assess whether the answer to our questions can\neasily be found on the web or not. Whenever an API is available, we run the model three times and report\nthe average results.\nGPT4 plugins.\nAs opposed to GPT4, there is currently no API for GPT4 with plugins, and we resort to\nmanual ChatGPT queries. At the time of the writing, the user has to manually choose between an Advanced\nData Analysis mode\u2014with code execution and file reading capabilities\u2014, and a set of at most three third\nparty plugins. We use either the first mode or select third parties plugins according to our best guess of the\nmost important capabilities given the task. We often rely on (i) a tool for reading various types of links, (ii) a\nweb browsing tool, and (iii) a tool for computation. Sadly, it is currently not possible to use a stable set of\nplugins over some period of time as plugins often change or disappear from the store. Similarly, the official\nsearch tool for GPT4 was removed as it could possibly circumvent paywalls, before being recently brought\nback. Therefore, our score for GPT4 with plugins is an \u201coracle\u201d estimate of GPT4 potential with more stable\nand automatically selected plugins rather than an easily reproducible result.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nScore (%)\nLevel 1\nLevel 2\nLevel 3\nLLMs, Human and Search engine scores and time to answer for GAIA\n101\n102\n103\nTime to answer (sec)\nSearch engine\nGPT-4\nGPT-4 Turbo\nAutoGPT-4\nGPT-4 Plugins (set by humans)\nHuman\nFigure 4 Scores and time to answer per method and level. As stated in the main text, GPT4 + plugins score should\nbe seen as an oracle since the plugins were chosen manually depending on the question. Human score refers to the\nscore obtained by our annotators when validating the questions.\nResults.\nOur evaluation can be found in Figure 4, with more details in Table 4 (Appendix D.1). Our proposed\nlevels of difficulty, loosely defined in terms of number of steps and number of different capabilities used, are\ncorrelated with the performance of current models, strengthening their validity. While humans excel at all\nlevels, current best LLMs do poorly. Overall, GAIA allows to clearly rank capable assistants, while leaving a\nlot of room for improvement in the coming months and perhaps years.\n6https://openai.com/blog/chatgpt-plugins\n7https://github.com/Significant-Gravitas/Auto-GPT,\ngit\nhash\nof\nthe\nAutoGPT\nversion\nevaluated:\ned172dec1947466cc0942abf75bb77b027cd433d.\n8\nDiverse filetype reading\nN/A\nWeb browsing\nCoding\nMulti-modality\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nScore (%)\nScore of LLMs per capability at Level 1\nGPT4\nAutoGPT4\nGPT4Turbo\nGPT4 + plugins\nFigure 5 Score of various LLMs at Level 1 per capability. Non\nzero scores for non tool models for \u201cDiverse filetype reading\u201d and\n\u201cMulti-modality\u201d are due to tasks that can be solved differently\nfrom the way the annotators did. Non zero scores for non tool\nmodels for web browsing are mostly due to correct memorization\nof information required to complete intermediate steps.\nWeb search by humans might return textual\nresults from which the correct answer can be\ndeducted for Level 1, yet does not work when\nit comes to slightly more complex queries,\nand is also slightly slower than a typical LLM\nassistant since the user has to skim through\nthe first search results. This confirms the\npotential of LLM assistants as competitors\nfor search engines.\nThe discrepancy between GPT4 results with-\nout plugins and the others demonstrate that\naugmenting LLMs via tool APIs or access to\nthe web improves answer accuracy, and un-\nlock many new use cases, confirming the\nhuge potential of this research direction.\nIn particular, GPT4 + plugins exhibit be-\nhaviours such as backtracking or query re-\nfinement when the result is not satisfying,\nand relatively long plan execution. We pro-\nvide examples of such behaviours in Ap-\npendix D.1. The discrepancy with humans\nsuggests the work needed to fully unlock this\npotential.\nAutoGPT4, which allows GPT4 to automatically use tools, offer disappointing results for Level 2, and even\nLevel 1 compared to GPT4 without plugins. This discrepancy might come from the way AutoGPT4 relies\non the GPT4 API (prompt and generation parameters) and will require new evaluation in the near future.\nAutoGPT4 is also slow compared to other LLMs. Overall, the collaboration between a human and GPT4\nwith plugins seem to offer the best ratio of score versus time needed so far.\nFigure 5 shows the scores obtained by the models splitted per capability. Unsurprisingly, GPT4 cannot deal\nwith files and multi-modality, yet manages to solve questions for which annotators used web browsing, mostly\nbecause it properly memorised pieces of information that need to be combined to get the answer.\n5\nDiscussion\nDesigning GAIA led us to think about current and future paradigm of AI systems evaluation.\nReproducibility for closed-source assistants.\nThe capabilities of models closed behind APIs might change over\ntime (Chen et al., 2023), making an evaluation done at some point in time not reproducible. The problem can\nbe even worse: for example, ChatGPT plugins and their capabilities change regularly, and are not accessible\nthrough ChatGPT\u2019s API yet. Reproducibility could become even more elusive since static benchmarks might\ndisappear in favour of benchmarks that decay through time due to their reliance on the real world. GAIA is\nhowever robust to the randomness of token generation since only the final answer, that admits a single correct\nresponse, is evaluated.\nStatic versus dynamic benchmarks.\nMuch like other complex expert datasets, GAIA currently comes with\nhundreds of questions that have been carefully curated and selected. By comparison, a more massive benchmark\nsuch as MMLU has close to 15,000. Yet, MMLU consists of multiple choice questions hence is seemingly\neasier than our open questions. Questions that admit a single correct answer require care, and we preferred to\nfavour quality over quantity. Moreover, we hope that our insights on question design will help the community\nto add more questions. GAIA is indeed likely to decay over time, be it via (i) catastrophic contamination of\npre-training data or (ii) disappearance from the web of some information required to answer the questions.\nWe are confident that the various mitigations we provide for these problems will help maintaining GAIA\nrelevant until it is solved. Static benchmarks are broken benchmarks in the making, and making GAIA evolve\n9\nyear-by-year through the removal of broken questions and the addition of new ones might be an important\ncomponent to better assess the generalization and robustness of AI systems.\nTowards unified evaluation of generative models.\nMany GAIA tasks might be solved by calling modules that\ncould yield errors e.g. an image classifier returning the wrong label. One could argue this makes evaluation\nambiguous since it considers the system as a whole and does not attribute errors to sub-parts e.g. the web\nbrowsing or vision modules. However, the paradigm of coupling LLMs with external tools for every task\nbeyond text understanding might not last. For example, future models might bend towards more integration\nbetween the LLM and other capabilities as in vision-language models (Alayrac et al., 2022; Lauren\u00b8con et al.,\n2023). GAIA aims at evaluating AI systems rather than the current architectural standard. More generally,\nautomatic, factual, and interpretable evaluation of complex generations is a long lasting problem in generative\nAI, another important example being images (Stein et al., 2023). Hu et al. (2023) make a step in that direction,\nyet rely on model-based evaluation and simple questions. Moving forward, the conjugation of multi-modal\nsystems with GAIA might further improve advanced generative models evaluation e.g. image generators,\nvia tasks requiring a complex sequence of image modifications and asking an unambiguous question on the\nresulting image in natural language. The answer could be found only if the modifications have been correctly\napplied by the model to the original image.\nPartial versus full automation.\nWhile partial automation of a process still requires humans in the loop,\nfull automation completely removes that need. Systems that respectively allow partial automation and full\nautomation can be as close as a few percentage of error on a given task\u2014the former would have say 1%\nand the latter 0%\u2014, yet yield these two fundamentally different paradigms. Full automation is a goal that\ndeep learning has been striving to achieve, without complete success to date: in spite of state-of-art results\nin various domains, most neural networks based systems can unpredictably fail e.g in common situations,\nimpeding the advent of technologies such as self-driving cars. Solving GAIA requires full automation since\nno approximation is allowed in the answer. Full automation of more human activities will reshape our\nsocio-economic landscape (Growiec, 2022), with the risk that the added value is mainly captured by the owner\nof the technology instead of human workers. This is a grounded argument in favour of open-source.\n6\nLimitations\nWhile GAIA attempts to circumvent current pitfalls of LLM benchmarks, some limitation remains.\nMissing evaluations.\nIn its current form, GAIA does not evaluate the trace leading to the answer. Indeed,\nas opposed to the ground truth which is unique, different paths could lead to the correct answer and there is\nno obvious and simple ways to grade those, while we prioritized easiness of use for GAIA. Going forward,\nhuman and model-based evaluations, albeit limited, are interesting options to evaluate the plans, and could\nbe quite convenient since (i) our questions rarely require expert knowledge, thus alleviating the need to find\nspecialized annotators, and (ii) the judge can rely on the ground truth: it is often faster to verify than to\nindependently derive the answer. We leave the addition of human and model-based evaluation for future work.\nFinally, we only evaluate the strongest available LLMs that have access to tools hence are able to obtain\ninformative scores. However, OpenAI\u2019s API does not provide the detailed log of tool calls yet, which would be\nrequired for fine-grained analysis. We look forward to add other models with sufficient tool using capabilities\nand logging, especially in open source.\nOn the cost of designing unambiguous questions.\nThe price to pay for a real-world yet easy to use benchmark\ncorresponds to making sure the questions are unambiguous. We find that two rounds of annotations are\nrequired, a first annotator making their best effort to design an unambiguous question\u2014wich takes more time\nthan e.g. ranking two different generations for RLHF\u2014, and two supplementary annotators independently\nanswering the question and disambiguating it if necessary.\nIn spite of this thorough process, possible\nambiguities remain. However, the annotation cost is fixed and probably small compared to the potential\ncost of multiple untrustworthy evaluations. A question might be ambiguous for a perfectly logical computer\nyet not ambiguous for humans: this is not a problem since we want AI systems to be aligned with human\npreferences. We believe human annotators are currently essential to have diverse and grounded questions,\nas opposed to programmatically generated ones. A similar argument is made in Chollet (2019). One could\nhowever synthetically generate GAIA-like data by relaxing the unambiguity constraint, e.g. for training\npurpose. Additionally, some GAIA questions come with many details hence seem unnatural: these details\n10\nensure the question admits only one correct answer and are therefore necessary. In practice, a user would ask\nan under-specified question, and a useful assistant would answer by citing its sources or keeping the most\ntrustworthy one. Both are difficult to factually evaluate, and we leave that aspect for future work.\nLack of linguistic and cultural diversity.\nA big limitation of GAIA is its lack of language diversity: all\nquestions are asked in \u201cstandard\u201d English only, and many questions mostly rely on English web pages. This\nbenchmark will therefore not validate the usefulness of assistants for non-English speakers (80% of the global\nworld population), their usefulness on the non English-speaking web (about half of its content), nor on any\nsort of dialectal variation of English. As such, GAIA is only a first step to estimate the potential of AI\nassistants, but should not be seen as an absolute general proof of their success. We hope to fill this gap in\nfuture work or through community involvement.\n7\nAcknowledgements\nThe authors would like to thank Nicolas Usunier for suggesting the web search baseline, Edwin Chen for\nhelping us improve our unusual protocol for annotators, Yacine Jernite for sharing his insights on diversity\nwhen benchmark building, and Sasha Luccioni for taking the time to proofread some sections where proper\nEnglish was eluding us.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han,\nZhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida\nNematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and\nKaren Simonyan. Flamingo: a visual language model for few-shot learning. In Alice H. Oh, Alekh Agarwal,\nDanielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL\nhttps://openreview.net/forum?id=EbMuimAbPbs.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang,\nKathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay,\nKefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry,\nChristopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00b4ement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa\nDev, Jacob Devlin, Mark D\u00b4\u0131az, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus\nFreitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou,\nJoshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski,\nWenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric\nLi, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello\nMaggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric\nNi, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily\nReif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby,\nAmbrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran\nVodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu,\nLinting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. Palm 2 technical report, 2023.\nAnthropic.\nModel card and evaluations for claude models, 2023.\nURL https://www-files.anthropic.com/\nproduction/images/Model-Card-Claude-2.pdf.\nEmily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system\nbias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587\u2013604, 2018.\nURL https://aclanthology.org/Q18-1041.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark\n11\nChen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. ChatEval:\nTowards Better LLM-based Evaluators through Multi-Agent Debate, August 2023.\nHarrison Chase. LangChain, October 2022.\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt\u2019s behavior changing over time?, 2023.\nFran\u00b8cois Chollet. On the measure of intelligence, 2019.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve\nmath word problems, 2021.\nCynthia Dwork and Moni Naor. Pricing via processing or combatting junk mail. In Ernest F. Brickell, editor,\nAdvances in Cryptology \u2014 CRYPTO\u2019 92, pages 139\u2013147, Berlin, Heidelberg, 1993. Springer Berlin Heidelberg. ISBN\n978-3-540-48071-6.\nDifei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. AssistGPT: A\nGeneral Multi-modal Assistant that can Plan, Execute, Inspect, and Learn, June 2023.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey\nHsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang,\nKevin Wang, and Andy Zou.\nA framework for few-shot language model evaluation, September 2021.\nURL\nhttps://doi.org/10.5281/zenodo.5371628.\nYingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu, Zelong Li, and Yongfeng Zhang. Openagi:\nWhen llm meets domain experts, 2023.\nJakub Growiec. Automation, partial and full. Macroeconomic Dynamics, 26(7):1731\u20131755, 2022.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring\nmassive multitask language understanding. In International Conference on Learning Representations, 2021.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing\nYau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. MetaGPT: Meta Programming for\nMulti-Agent Collaborative Framework, August 2023.\nYushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate\nand interpretable text-to-image faithfulness evaluation with question answering. arXiv preprint arXiv:2303.11897,\n2023.\nMarkus Jakobsson and Ari Juels. Proofs of Work and Bread Pudding Protocols(Extended Abstract), pages 258\u2013\n272.\nSpringer US, Boston, MA, 1999.\nISBN 978-0-387-35568-9.\ndoi: 10.1007/978-0-387-35568-9 18.\nURL\nhttps://doi.org/10.1007/978-0-387-35568-9_18.\nDouwe Kiela, Tristan Thrush, Kawin Ethayarajh, and Amanpreet Singh. Plotting progress in ai. Contextual AI Blog,\n2023. https://contextual.ai/blog/plotting-progress.\nHugo Lauren\u00b8con, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,\nSiddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open\nweb-scale filtered dataset of interleaved image-text documents, 2023.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL:\nCommunicative Agents for \u201dMind\u201d Exploration of Large Scale Language Model Society, March 2023a.\nMinghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. API-Bank: A Benchmark\nfor Tool-Augmented LLMs, April 2023b.\nMinghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A benchmark\nfor tool-augmented llms, 2023c.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\nNarayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian\nCosgrove, Christopher D. Manning, Christopher R\u00b4e, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin\nDurmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr,\nLucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter\n12\nHenderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto,\nThomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta\nKoreeda. Holistic Evaluation of Language Models, November 2022a.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\nNarayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian\nCosgrove, Christopher D. Manning, Christopher R\u00b4e, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin\nDurmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr,\nLucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto,\nThomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta\nKoreeda. Holistic evaluation of language models, 2022b.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan\nYang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su,\nHuan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:\n2308.03688, 2023a.\nZhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen,\nJuan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. BOLAA:\nBenchmarking and Orchestrating LLM-augmented Autonomous Agents, August 2023b.\nGr\u00b4egoire Mialon, Roberto Dess`\u0131, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste\nRozi`ere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom.\nAugmented language models: a survey, 2023.\nMicrosoft. Semantic Kernel. github, September 2023.\nMeredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement\nFarabet, and Shane Legg. Levels of agi: Operationalizing progress on the path to agi, 2023.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu\nJain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback.\narXiv preprint arXiv:2112.09332, 2021.\nHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical\nchallenge problems, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nAnton Osika. GPT Engineer, September 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\nAmanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow\ninstructions with human feedback, 2022.\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large Language Model Connected with\nMassive APIs, May 2023.\nYotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer,\nand Leshem Choshen. Efficient benchmarking (of language models), 2023.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine\ncomprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,\npages 2383\u20132392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/\nD16-1264. URL https://aclanthology.org/D16-1264.\nSasha Rush. MiniChain, September 2023.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\ncedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint\narXiv:2302.04761, 2023.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving AI\nTasks with ChatGPT and its Friends in Hugging Face, May 2023.\n13\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal\nArora, Joshua Lane, et al. Blenderbot 3: a deployed conversational agent that continually learns to responsibly\nengage. arXiv preprint arXiv:2208.03188, 2022.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R.\nBrown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal,\nAlethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish,\nAllen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S.\nIyer, Anders Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlm\u00a8uller, Andrew M. Dai, Andrew\nLa, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi,\nAntonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher\nMullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karaka\u00b8s, B. Ryan Roberts,\nBao Sheng Loe, Barret Zoph, Bart lomiej Bojanowski, Batuhan \u00a8Ozyurt, Behnam Hedayatnia, Behnam Neyshabur,\nBenjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao,\nCameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles Rathkopf, Chenlin Meng,\nChitta Baral, Chiyu Wu, Chris Callison-Burch, Christopher Waites, Christian Voigt, Christopher D Manning,\nChristopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina\nGarbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, C. Daniel Freeman, Daniel\nKhashabi, Daniel Levy, Daniel Mosegu\u00b4\u0131 Gonz\u00b4alez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne\nIppolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin,\nDenis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho\nMollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor\nHagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodol`a, Emma Lam, Eric Chu, Eric Tang,\nErkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii\nZheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Mart\u00b4\u0131nez-Plumed, Francesca Happ\u00b4e, Francois Chollet, Frieda\nRong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germ\u00b4an Kruszewski, Giambattista Parascandolo,\nGiorgio Mariani, Gloria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic,\nHannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Francis Anthony Shevlin,\nHinrich Schuetze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack\nGeissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fern\u00b4andez Fisac, James B Simon, James Koppel,\nJames Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha\nSohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh,\nJeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru,\nJohn Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, J\u00a8org Frohberg, Jos Rozen, Jose\nHernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce\nChua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert,\nKaustubh Dhole, Kevin Gimpel, Kevin Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia Shkaruta,\nKumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui\nQin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt,\nLuheng He, Luis Oliveros-Col\u00b4on, Luke Metz, L\u00a8utfi Kerem Senel, Maarten Bosma, Maarten Sap, Maartje Ter Hoeve,\nMaheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose\nRamirez-Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L Leavitt, Matthias\nHagen, M\u00b4aty\u00b4as Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael Andrew Yee,\nMichael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michal Swedrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit\nBansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan Andrew Chi,\nNayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita\nNangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel,\nNuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares,\nParth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter W\nChang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Mi lkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli,\nQiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker,\nRamon Risco, Rapha\u00a8el Milli`ere, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers,\nRobert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le Bras, Rosanne Liu, Rowan Jacobs, Rui\nZhang, Russ Salakhutdinov, Ryan Andrew Chi, Seungjae Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib\nSingh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R.\nBowman, Samuel Stern Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh,\nSean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon\nZhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar,\nShubham Toshniwal, Shyam Upadhyay, Shyamolima Shammie Debnath, Siamak Shakeri, Simon Thormeyer, Simone\n14\nMelzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene,\nStefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven Piantadosi, Stuart Shieber,\nSummer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali,\nTatsunori Hashimoto, Te-Lin Wu, Th\u00b4eo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius\nNkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar\nKhot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Venkatesh\nRamasesh, vinay uday prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William\nZhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh,\nYair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov,\nYu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi\nWu. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions\non Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj.\nGeorge Stein, Jesse C. Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Leigh Ross, Valentin Villecroze, Zhaoyan Liu,\nAnthony L. Caterini, J. Eric T. Taylor, and Gabriel Loaiza-Ganem. Exposing flaws of generative model evaluation\nmetrics and their unfair treatment of diffusion models, 2023.\nD\u00b4\u0131dac Sur\u00b4\u0131s, Sachit Menon, and Carl Vondrick. ViperGPT: Visual Inference via Python Execution for Reasoning,\nMarch 2023.\nYashar Talebirad and Amirhossein Nadiri. Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM\nAgents, June 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen,\nGuillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj\nGoswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez,\nMadian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril,\nJenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and\nfine-tuned chat models, 2023.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, page 353\u2013355. Association for Computational\nLinguistics, Nov 2018. URL https://aclanthology.org/W18-5446.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel\nBowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Advances\nin Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual ChatGPT: Talking,\nDrawing and Editing with Visual Foundation Models, March 2023.\nBinfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, and\nDongkuan Xu. Gentopia: A Collaborative Platform for Tool-Augmented LLMs, August 2023a.\nQiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation capability\nof open-source large language models, 2023b.\nHui Yang, Sifu Yue, and Yunzhong He. Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions,\nJune 2023.\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari,\nAveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic\nModels: Composing Zero-Shot Multimodal Reasoning with Language, May 2022.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\nDacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench\nand chatbot arena, 2023.\n15\nYuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. ToolQA: A Dataset for LLM Question Answering\nwith External Tools, June 2023.\n16\nA\nExtended related work\nLarge Language Models as General Assistants.\nSeveral avenues have been explored to turn LLMs into\ngeneral-purpose assistants: (i) using single agent LLMs with better capabilities through Chain of Thought\nprompting or equivalent mechanisms, such as GPT-Engineer (Osika, 2023), AutoGPT (Yang et al., 2023);\n(ii) using multiple agent LLMs to debate and together reach better conclusions to answer user queries (Li\net al., 2023a; Hong et al., 2023; Chan et al., 2023; Talebirad and Nadiri, 2023); (iii) using single agent LLMs\naugmented with specific tools, such as Blender Bot 3 (Shuster et al., 2022), BOLAA (Liu et al., 2023b) and\nAssistGPT (Gao et al., 2023) extending LLMs with planning components, Socratic Models (Zeng et al., 2022)\nor Visual ChatGPT (Wu et al., 2023) extended with multimodal models, WebGPT Nakano et al. (2021)\nfine-tuned for web-search, or a collection of tools and APIs, such as Toolformer (Schick et al., 2023) fine-tuned\nfor general tool usage, ViperGPT (Sur\u00b4\u0131s et al., 2023) using coding capabilites to generate correct API calls,\nHuggingGPT (Shen et al., 2023) leveraging calls to the HuggingFace ecosystem to extend its LLM with other\nML models capabilities, or even (iv) providing full new API/tooling libraries, such as the OpenAI plugins,\nSemanticKernel (Microsoft, 2023), Langchain (Chase, 2022) and MiniChain (Rush, 2023).\nB\nDatacard\nWe follow (Bender and Friedman, 2018) for the creation of this datacard, where we try to summarise and\ncentralise all information which might be relevant for analysis of this dataset.\nCuration rationale.\nThis is detailed in Section 3.4 and Appendix D.\nLanguage variety.\nInformation about our annotators\u2019 nationality was not provided, but they were all based\nin the US, and all questions, answers, and meta-data were written in mainstream English (therefore most\nlikely en-US). We can also note that all authors of this paper are French and do not have English as a first\nlanguage, which might have lead to the inclusion of non-standard English phrasing in the questions or answers.\nCurators and Annotators demographic.\nFollowing the definitions proposed in (Bender and Friedman, 2018),\nbuilding GAIA required the work of Curators, who devised the questions and their answer, and Annotators,\nwho independently annotated the questions to assess their non-ambiguity. Both come from the following\npopulation:\n\u2022 Age:\n\u2013 18-25: 17%\n\u2013 26-35: 39%\n\u2013 36-45: 26%\n\u2013 45-55: 13%\n\u2013 56-65: 4%\n\u2022 Gender: 57% Male, 43% Female.\n\u2022 Academic background:\n\u2013 Bachelor\u2019s Degree: 61%\n\u2013 Master\u2019s Degree: 26%\n\u2013 PhD: 17%\nText characteristics.\nThis is detailed in Appendix C.\n17\nC\nExtended description of GAIA\nDescription of capabilities.\nWhen answering the questions, annotators specified the steps that were followed\nand listed the tools they use. Based on the set of tools that were mentionned by the annotators, we defined\ncapabilities required by GAIA. For each capability, we report examples of corresponding tool as reported by\nannotators.\n\u2022 Web browsing: tools related to search the web and browse websites. Examples: Web browser, Search\nengine, Website widget access, Access to YouTube, Google Street View.\n\u2022 Multi-modality: tools related to understanding data modality other than text. Examples: A speech-to-text\ntool, Video recognition, Image recognition, OCR, Google Street View.\n\u2022 Coding: tools related to code execution. Examples: Python, a calculator, Substitution cipher\nencoder, C++ compiler, A word reversal tool / script.\n\u2022 Diverse filetype reading: tools related to understanding various type of files given by a user or found\non the web. Examples: PDF viewer, Excel file access, PowerPoint viewer, CSV access, Txt\nfile access.\n\u2022 N/A: tools for tasks that can currently be performed by non-augmented LLMs. Examples: Tetris\nrules database, German translator, Spell checker, Text Editor, Bass note data.\nNote that a tool can belong to different categories. For example, Google Street View requires access to the\nweb, browsing, but also multi-modality. Hence, these categories are indications of the capabilities required by\nGAIA and not a perfect typology of our questions.\nFiletypes.\nSome GAIA questions come with additional files, whose distribution is given in Figure 6.\n0\n5\n10\n15\n20\n25\n30\nFile Count\nxlsx\npng\npdf\ntxt\nmp3\njpg\ncsv\ndocx\npptx\nzip\nxml\npy\njson\nm4a\npdb\nMOV\njsonld\nFile Type\n29\n18\n15\n13\n7\n7\n6\n2\n2\n2\n2\n1\n1\n1\n1\n1\n1\nDistribution of File Types\nFigure 6 Initial distributions of file types in GAIA.\nDifficulty of the questions.\nOur analysis of the time taken by the annotators to answer a question shows a\ncorrelation with the number of steps taken. The correlation is less clear with the number of different tools\nused to answer.\n18\n0\n1\n2\n3\n4\n5\n6\nNumber of tool used to answer\n0\n10\n20\n30\n40\n50\n60\nTime taken to answer (minutes)\nTime versus number of tools to answer\nLevel 2\nLevel 1\nLevel 3\nFigure 7 Using multiple tools does not necessarily involve\nmore time to answer a question.\n0\n10\n20\n30\n40\nNumber of steps taken to answer\n0\n10\n20\n30\n40\n50\n60\nTime taken to answer (minutes)\nTime versus number of steps to answer\nLevel 2\nLevel 1\nLevel 3\nFigure 8\nUnsurprisingly, the number of steps taken to\nanswer is correlated to the time taken.\nD\nExtended description of our question design framework\nQuestion creation phase.\nWe provided the annotators with a seed set of GAIA questions we devised ourselves,\naccompanied with the following instructions:\nWe want to augment the dataset of provided questions (not variations of what we already have).\nRequirements:\n\u2022 Make sure your question is based on a source of truth (Wikipedia, arXiv, githhub, other...). For Level 2\nand Level 3, a good way to create questions is to combine sources of truth.\n\u2022 Make sure the answer to your question does not exist on the internet in plain text.\n\u2022 Make sure the answer to your question is a number or at most a few words to make evaluation robust.\n\u2022 Make sure the answer to your question does not change with time. This includes potential deletion of\nthe source of truth.\n\u2022 Make sure the answer to your question is unambiguous.\n\u2022 Make sure your question is \u201cinteresting\u201d, i.e. by reading it you think that an AI assistant answering\nthis kind of question would help you a lot.\n\u2022 Make sure your question can be answered in a reasonable amount of time by a human annotator.\n\u2022 (Added later on): check the robots.txt of the website containing the information needed to answer so\nthat it is accessible to AI assistants.\nThe annotators were also asked to answer the questions they created. We provide a typical example of\nannotated question in Table 1.\nValidation phase.\nAfter question creation, we ask two new independent annotators to answer the questions\nto check it is not ambiguous. We provide a typical annotator output for the validation phase in Table 2, as\nwell as additional statistics on the validation phase of our protocol in Table 3. If the new annotators don\u2019t\nfully agree with the original answer and there is no human error, the question is repaired if possible and\nremoved otherwise.\nWe estimate the creation of a question, including its validation by two supplementary annotators and potential\nrepairs, requires two hours of annotator time.\n19\nQuestion\nWhat was the actual enrollment count of the clinical trial on H. pylori in acne vulgaris patients\nfrom Jan-May 2018 as listed on the NIH website?\nFile\nNone\nLevel\n1\nSteps\n- Searched \u201cnih\u201d on Google search.\n- Clicked the top link to nih.gov.\n- Searched \u201ch pylori acne\u201d in the search box.\n- Clicked \u201cMore\u201d and selected \u201cClinical Trials\u201d.\n- Clicked the result about H. Pylori and acne.\n- Checked the date to confirm it was January to May 2018.\n- Opened \u201cTabular View\u201d.\n- Scrolled down to Actual Enrollment and recorded the number.\nNumber of steps\n8\nAnswer\n90\nTime to answer\n8 minutes\nTools\n- Web browser\nNumber of tools\n1\nTable 1 An annotated question during the question creation phase.\nQuestion\nWhat was the actual enrollment count of the clinical trial on H. pylori in acne vulgaris patients\nfrom Jan-May 2018 as listed on the NIH website?\nFile\nNone\nLevel\n1\nVerifier response\n90\nAnswer match\nYes - my answer matches the correct answer.\nCause of mismatch\nNone\nTable 2 An annotated question during the validation phase.\n20\nAfter two new, independent annotators answer for all crafted questions:\nTwo new annotators agree with original answer\n55%\nOne new annotator agree with original answer, other disagree\n27%\nTwo new annotators disagree with original answer\n18%\nValid questions (aggregated)*\n68%\nValid Level 1 questions\n75%\nValid Level 2 questions\n68%\nValid Level 3 questions\n47%\nHuman score (aggregated)**\n92%\nHuman score for Level 1\n94%\nHuman score for Level 2\n92%\nHuman score for Level 3\n87%\nTable 3 Statistics on the validation phase. 623 newly crafted questions were validated by two new annotators each.\nThe statistics were computed on their 1246 annotations. *: a valid question is a question for which two annotators\ngive the same answer as the question designer, or only one annotator gives the same answer as the question designer\nand the other made a mistake. **: the human baseline is computed as the fraction of correct answers for all tentative\non valid questions by the new annotators.\nMetric\nScore in % (\u2191)\nAvg. time to answer in mins (\u2193)\nLevel\nLevel 1\nLevel 2\nLevel 3\nLevel 1\nLevel 2\nLevel 3\nNumber of questions\n146\n245\n75\n146\n245\n75\nGPT4\n9.1 \u00b1 2.5\n2.6 \u00b1 0.6\n0\n0.19\n0.15\nN.A.\nGPT4 Turbo\n13.0 \u00b1 2.1\n5.5 \u00b1 1.4\n0\n0.24\n0.12\nN.A.\nAutoGPT (GPT4 backend)\n14.4\n0.4\n0\n7.6\n11.7\nN.A.\nGPT4 + plugins*\n30.3\n9.7\n0\n0.65\n0.53\nN.A.\nSearch engine\n7.4\n0\n0\n7.4\nN.A.\nN.A.\nHuman annotator**\n93.9\n91.8\n87.3\n6.8\n10.5\n17.7\nTable 4\nScore and average time to answer for various baselines on GAIA in %. *: GPT4 + plugins scores were\nobtained by manually selecting plugins, and cannot be reproduced exactly for the reasons described in the main text.\n**: Human score corresponds to the portion of correct answers by validation annotators for valid questions. Whenever\nwe have direct API access, we run the model three times and report the average. Times for APIs were obtained by\nrunning the API on 20 questions then averaging, and were taken at a single point in time: they aren\u2019t meant to reflect\nGPT4 vs. GPT4 Turbo speed, but GPT4 vs. other types of methods for answering GAIA.\nD.1\nExtended evaluation\nWe provide the detailed scores of the different methods evaluated in Table 4.\nAdditional examples of GPT4 answering GAIA.\nWe provide more reasoning traces of GPT4 with and\nwithout plugins when answering GAIA. The output of AutoGPT is currently much longer, denser and less\ninterpretable thant GPT4. Examples of AutoGPT outputs are therefore provided in the supplementary\nmaterial for the same GAIA question as the example in Figure 9.\n21\nSystem prompt: You are a general AI assistant. I will ask you a question. Report your thoughts, and finish\nyour answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER].\nYOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of\nnumbers and/or strings.\nIf you are asked for a number, don\u2019t use comma to write your number neither use units such as $ or percent\nsign unless specified otherwise.\nIf you are asked for a string, don\u2019t use articles, neither abbreviations (e.g. for cities), and write the digits in\nplain text unless specified otherwise.\nIf you are asked for a comma separated list, apply the above rules depending of whether the element to be put\nin the list is a number or a string.\nGAIA Question:\nYour final answer should be a number, or as few words as possible.\nWhere were\nthe Vietnamese specimens described by Kuznetzov in Nedoshivina\u2019s 2010 paper eventually deposited? Just\ngive me the city name without abbreviations.\nGPT4:\nI\u2019m unable to browse the internet or access new information after my knowledge cut-off in January 2022.\nTherefore, I am unable to provide the city name where the Vietnamese specimens described by Kuznetzov in\nNedoshivina\u2019s 2010 paper were eventually deposited.\nFINAL ANSWER: Unable to provide\nGround truth: Saint Petersburg \u2717\nSystem prompt: You are a general AI assistant. I will ask you a question. Report your thoughts, and finish\nyour answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER].\nYOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of\nnumbers and/or strings.\nIf you are asked for a number, don\u2019t use comma to write your number neither use units such as $ or percent\nsign unless specified otherwise.\nIf you are asked for a string, don\u2019t use articles, neither abbreviations (e.g. for cities), and write the digits in\nplain text unless specified otherwise.\nIf you are asked for a comma separated list, apply the above rules depending of whether the element to be put\nin the list is a number or a string.\nGAIA Question:\nYour final answer should be a number, or as few words as possible.\nWhere were\nthe Vietnamese specimens described by Kuznetzov in Nedoshivina\u2019s 2010 paper eventually deposited? Just\ngive me the city name without abbreviations.\nGPT4 (Web browsing):\nBrowsing Bing:\n\u00fc \u201cVietnamese specimens described by Kuznetzov Nedoshivina 2010 paper deposition location\u201d\n\u00ed Reading content\nResult:\nA catalogue of type specimens of the Tortricidae described by V. I. Kuznetzov from Vietnam and\ndeposited in the Zoological Institute, St. Petersburg : Nedoshivina, Svetlana : Free Download, Borrow,\nand Streaming : Internet Archive\nFINAL ANSWER: Saint Petersburg\nGround truth: Saint Petersburg \u2713\nFigure 9 Proper web search is very effective to answer GAIA questions. The reasoning trace was obtained with the\ncurrent GPT4 web browsing version.\n22\nSystem prompt: You are a general AI assistant. I will ask you a question. Report your thoughts, and finish\nyour answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER].\nYOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of\nnumbers and/or strings.\nIf you are asked for a number, don\u2019t use comma to write your number neither use units such as $ or percent\nsign unless specified otherwise.\nIf you are asked for a string, don\u2019t use articles, neither abbreviations (e.g. for cities), and write the digits in\nplain text unless specified otherwise.\nIf you are asked for a comma separated list, apply the above rules depending of whether the element to be put\nin the list is a number or a string.\nGAIA Question:\nIn the film Goldfinger, what color was the object that James Bond concealed him-\nself and his companion Pussy Galore at the end of the film? If there are multiple colors, put them in a\ncomma-separated list in alphabetical order.\nGPT4 (Web browsing):\nBrowsing Bing:\n\u00fc \u201cGoldfinger ending scene object color\u201d\n\u009b Goldfinger (1964) ending / spoiler\n\u00ed Reading content\n\u00fc \u201cGoldfinger ending scene parachute color\u201d\n\u00ed Reading content\nFINAL ANSWER: Orange, White\nGround truth: orange, white \u2713\nFigure 10 The official GPT4 browsing plugin is able to refine its initial query based on the information it finds. This\nreasoning trace was obtained with the GPT4 browsing plugin version that was removed, and could not be reproduced\nwith the new version.\n23\nSystem prompt: [Same system prompt as Figure 10.]\nGAIA Question:\nA standard Rubik\u2019s cube has been broken into cubes making up its sides.\nThe\ncubes are jumbled, and one is removed. There are 6 cubes with one colored face, 12 edge cubes with two\ncolored faces, and 8 corner cubes with three colored faces. All blue cubes have been found. All cubes directly\nleft, right, above, and below the orange center cube have been found, along with the center cube. The green\ncorners have all been found, along with all green that borders yellow. For all orange cubes found, the opposite\nface\u2019s cubes have been found. The removed cube has two colors on its faces. What are they? Answer using a\ncomma separated list, with the colors ordered alphabetically.\nGPT4:\nTo solve this, let\u2019s break it down step by step and visualize a standard Rubik\u2019s cube:\n\u2022 A standard Rubik\u2019s Cube has 6 faces: Front, Back, Left, Right, Up, Down.\n\u2022 Each face has a center cube (which determines the color of that face), surrounded by 4 edges and 4\ncorners.\n\u2022 Each edge has two colors, and each corner has three colors.\nCube Structure:\n\u2022 Orange: Front\n\u2022 Red: Back\n\u2022 Blue: Left\n\u2022 Green: Right\n\u2022 White: Up\n\u2022 Yellow: Down\nStep-by-Step Analysis:\n\u2022 All blue cubes have been found.\n\u2022 All cubes directly left, right, above, and below the orange center cube have been found, along with the\ncenter cube.\n\u2022 This means the central, top, bottom, left, and right orange cubes have been found.\n\u2022 All green corners have been found, along with all green that borders yellow.\n\u2022 This means all green-yellow edges and corners have been found.\n\u2022 For all orange cubes found, the opposite face\u2019s cubes have been found.\n\u2022 This means all the red cubes opposite the found orange cubes have been found.\nSolution Approach: Since the removed cube has two colors on its faces, it must be an edge cube. To determine\nwhich one is missing, we should look for the edge cube that is not accounted for by the given conditions.\n\u2022 All blue cubes found, which means all blue edges and corners are found.\n\u2022 The orange center, and all surrounding it, are found, and hence the opposite reds are found too.\n\u2022 All green corners are found, and green that borders yellow are found too.\n\u2022 By deducting the cubes found from the total cubes, we will find the missing cube, which is the edge\ncube between the red and yellow faces.\nFINAL ANSWER: Red, Yellow\nGround truth: green, white \u2717\nFigure 11 GPT4 and other assistants struggle on puzzles, which often are Level 1 questions.\n24\n"
  },
  {
    "title": "FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline",
    "link": "https://arxiv.org/pdf/2311.13073.pdf",
    "upvote": "53",
    "text": "FusionFrames: Efficient Architectural Aspects for\nText-to-Video Generation Pipeline\nVladimir Arkhipkin1\nZein Shaheen1\nViacheslav Vasilev1,2\nElizaveta Dakhova3\nAndrey Kuznetsov1,3\nDenis Dimitrov1,3\n1Sber AI\n2Moscow Institute of Physics and Technology\n3Artificial Intelligence Research Institute\n{kuznetsov, dimitrov}@airi.net\nFigure 1. Generations by FusionFrames. Our approach for temporal conditioning allows to hold high-quality dynamics.\nAbstract\nMultimedia generation approaches occupy a prominent\nplace in artificial intelligence research. Text-to-image mod-\nels achieved high-quality results over the last few years.\nHowever, video synthesis methods recently started to de-\nvelop. This paper presents a new two-stage latent diffusion\ntext-to-video generation architecture based on the text-to-\nimage diffusion model. The first stage concerns keyframes\nsynthesis to figure the storyline of a video, while the second\none is devoted to interpolation frames generation to make\nmovements of the scene and objects smooth. We compare\nseveral temporal conditioning approaches for keyframes\ngeneration. The results show the advantage of using sep-\narate temporal blocks over temporal layers in terms of met-\nrics reflecting video generation quality aspects and human\npreference.\nThe design of our interpolation model sig-\nnificantly reduces computational costs compared to other\nmasked frame interpolation approaches. Furthermore, we\nevaluate different configurations of MoVQ-based video de-\ncoding scheme to improve consistency and achieve higher\nPSNR, SSIM, MSE, and LPIPS scores. Finally, we com-\npare our pipeline with existing solutions and achieve top-\n2 scores overall and top-1 among open-source solutions:\nCLIPSIM = 0.2976 and FVD = 433.054. Code is avail-\nable here: https://github.com/ai-forever/\nKandinskyVideo.\n1. Introduction\nText-to-image (T2I) generation approaches have achieved\nstunning results in recent years [22, 24\u201326]. The task of\nvideo generation is a natural and logical continuation of the\ndevelopment of this direction. Diffusion probabilistic mod-\nels [10, 31, 32] played an essential role in image generation\nquality improvement. Text-to-video (T2V) generative dif-\nfusion models are also becoming extremely popular, but the\nproblems inherent in this task still pose a severe challenge.\nSuch problems include training and inference compu-\ntational costs and require large, high-quality, open-source\ntext+video datasets.\nThe available data is insufficient to\nfully understand all the generation possibilities when train-\ning from scratch.\nIn addition, such datasets impose re-\nstrictions on models related to the specificity of video do-\nmain. Furthermore, to achieve high realism and aestheti-\ncism, video generation requires not only the visual quality\nof a single frame but also the frame coherence in terms of\nsemantic content and appearance, smooth transitions of ob-\njects on adjacent frames, and correct physics of movements.\nThe primary key for the mentioned aspects is the tempo-\nral information which is essential to the video modality and\nrepresents space-time correlations. Hence, the generation\nquality will largely depend on the data processing along the\ntime dimension of video sequences.\nAs a rule, temporal information is taken into account in\n1\narXiv:2311.13073v2  [cs.CV]  20 Dec 2023\ndiffusion models by including temporal convolutional lay-\ners or temporal attention layers in the architecture [4, 7, 11,\n12, 17, 29, 44, 52, 54]. This allows initializing the weights\nof the remaining spatial layers with the weights of the pre-\ntrained T2I model and training only the temporal layers. In\nthis way, we can reduce the necessity for large-scale text-\nvideo pairs datasets because we can transfer comprehensive\nknowledge of T2I models to the video domain. Also, using\nlatent diffusion models [25] further reduces the computa-\ntional costs.\nIn this paper, we present our T2V generation architec-\nture based on latent diffusion and examine various architec-\ntural aspects to enhance the overall quality, consistency and\nsmoothness of generated videos. The proposed pipeline is\ndivided into two stages: the keyframe generation stage, de-\nsigned to control the video main storyline, and the interpo-\nlation stage regarding movement smoothness improvement\nby generating additional frames. This separation allows us\nto maintain alignment with the text description through-\nout the entire video in terms of both content and dynam-\nics. At the keyframe generation stage, we compare tem-\nporal conditioning approaches, namely traditional mixed\nspatial-temporal blocks and three types of separate tempo-\nral blocks. We find that using separate temporal blocks sig-\nnificantly improves the video quality, which is supported by\nthe quality metrics and human evaluation study. We pro-\npose this solution as a general approach to include temporal\ncomponents in T2I models to use them for video genera-\ntion. For our interpolation model, we design the architec-\nture to reduce inference running time by predicting a group\nof interpolated frames together instead of individual frames,\nand we find that it also improves the quality of interpolated\nframes. Regarding the latent representation of frames, we\ncomprehensively analyze various options for constructing\nthe MoVQGAN-based [53] video decoder, assessing them\nin terms of quality metrics and the number of additional pa-\nrameters. This analysis is aimed at enhancing the consis-\ntency of adjacent frames throughout the video.\nThus, our contribution contains the following aspects:\n\u2022 We present FusionFrames \u2013 the end-to-end T2V latent\ndiffusion pipeline, which is based on the pretrained frozen\nT2I model Kandinsky 3.0 [1]. The pipeline is divided\ninto two parts \u2013 key frames generation and interpolation\nframes synthesis.\n\u2022 As a part of the keyframes generation, we propose to use\nseparate temporal blocks for processing temporal infor-\nmation. We compare three types of blocks with mixed\nspatial-temporal layers and demonstrate the qualitative\nand quantitative advantage of our solution in terms of vi-\nsual appearance and temporal consistency using a set of\nmetrics (FVD, IS, CLIPSIM) and human evaluation study\non several video datasets in different domains. The con-\nducted experiments show top-2 performance in terms of\nCLIPSIM and FVD scores regarding other published re-\nsults \u2014 0.2976 and 433.054 respectively.\n\u2022 We present an efficient interpolation architecture that runs\nmore than three times faster compared to other popular\nmasked frame interpolation architectures and generates\ninterpolated frames with higher fidelity.\n\u2022 We investigate various architectural options to build\nMoVQ-GAN video decoder and evaluate their perfor-\nmance in terms of quality metrics and the impact on the\nsize of decoder.\n2. Related Work\n2.1. Text-to-Video Generation\nPrior works on video generation utilize VAEs [2, 3, 20, 39,\n47], GANs [5, 16, 18, 23, 38], normalizing flows [15] and\nautoregressive transformers [8, 13, 36, 42, 43]. GODIVA\n[42] adopts a 2D VQVAE and sparse attention for T2V gen-\neration. CogVideo [13] is built on top of a frozen CogView2\n[6] T2I transformer by adding additional temporal attention\nlayers.\nRecent research extend T2I diffusion-based architecture\nfor T2V generation [4, 9, 11, 29, 54]. This approach can\nbenefit from pretrained image diffusion models and trans-\nfer that knowledge to video generation tasks. Specifically,\nit introduces temporal convolution and temporal attention\nlayers interleaved with existing spatial layers. This adapta-\ntion aims to capture temporal dependencies between video\nframes while achieving computational efficiency by avoid-\ning infeasible 3D convolutions and 3D attention mecha-\nnisms. These temporal layers can be trained independently\n[4, 29] or jointly [7, 11] with the 2D spatial layers. This\ntechnique of mixed spatial-temporal blocks for both convo-\nlutions and attention layers has become widespread in most\nT2V models [4, 7, 11, 12, 17, 29, 44, 54]. Alternative ap-\nproaches to operating with the time dimension include the\nuse of an image diffusion model in conjunction with a tem-\nporal autoregressive Recurrent Neural Network (RNN) to\npredict individual video frames [48], projection of 3D data\ninto a latent 2D space [49] and the use of diffusion to gen-\nerate latent flow sequences [21]. In this paper, we propose\nseparate temporal blocks as a new approach for condition-\ning temporal information.\nFinally, it is worth noting that certain studies [11, 29]\noperate entirely in pixel space, whereas others [4, 7, 9, 17,\n44, 52, 54] utilize the more efficient latent space. We follow\nthe second approach in this paper.\n2.2. Video Frame Interpolation\nMCVD [37] uses a diffusion-based model for interpola-\ntion.\nIt leverages four consecutive keyframes (two from\neach side) to predict three frames in between. In the field\nof T2V research, diffusion-based architectures are com-\n2\nFigure 2. Overall scheme of our pipeline. The encoded text prompt enters the U-Net keyframe generation model with temporal layers\nor blocks, and then the sampled latent keyframes are sent to the latent interpolation model in such a way as to predict three interpolation\nframes between two keyframes. A temporal MoVQ-GAN decoder is used to get the final video result.\nmonly adopted for interpolation. Some studies make use\nof training methods involving Masked Frame Interpola-\ntion [4, 11, 29, 50]. In LVDM [9], they use Masked Latent\nClip Interpolation instead. VideoGen [17] employs a flow-\nbased approach for interpolation. Magicvideo [54] com-\nbines the latents of two consecutive keyframes with ran-\ndomly sampled noise. It is worth noting that several train-\ning techniques have been proposed, including Conditional\nFrame (or Latent) Perturbation [9, 11, 50] to mitigate the\nerror introduced by the previous generation step and Con-\ntext Guidance (or Unconditional Guidance) [4, 9] to en-\nhance the diversity and fidelity of sampled video.\nIn our interpolation network, we adjust the input and out-\nput layers in U-Net to generate three identical frames be-\nfore training. Then, we train U-Net to predict three inter-\npolated frames (between each two consecutive keyframes).\nThis adaptation significantly reduces the computational cost\ncompared to Masked Frame Interpolation methods, en-\nabling pre-trained T2I weights for initialization.\n2.3. Video Decoder\nIn their works, [4, 17] builds a video decoder with tempo-\nral attention and convolution layers. MagicVideo [54], on\nthe other hand, incorporates two temporal-directed attention\nlayers in the decoder to build a VideoVAE. To the best of our\nknowledge, previous studies have not compared their strate-\ngies for constructing a video decoder. In this research, we\npresent multiple options for designing a video decoder and\nconduct an extensive comparative analysis, evaluating their\nperformance regarding quality metrics and the implications\nfor additional parameters.\n3. Diffusion Probabilistic Models\nDenoising Diffusion Probabilistic Models (DDPM) [10] is\na family of generative models designed to learn a target\ndata distribution pdata(x). It consists of a forward diffusion\nprocess and a backward denoising process. In the forward\nprocess, random noise is gradually added into the data x\nthrough a T-step Markov chain [14]. The noisy latent vari-\nable at step t can be expressed as:\nzt =\np\n\u02c6\u03b1tx +\np\n1 \u2212 \u02c6\u03b1t\u03f5\n(1)\nwith \u02c6\u03b1t = Qt\nk=1 \u03b1k, 0 \u2264 \u03b1k < 1, \u03f5 \u223c N(0, 1). For a\nsufficiently large T, e.g., T = 1000, \u221a\u02c6\u03b1T \u2248 0, and\n1 \u2212 \u221a\u02c6\u03b1T \u2248 1. Consequently, zT ends up with pure noise.\nThe generation of x can then be seen as an iterative denois-\ning process. This denoising process corresponds to learning\nthe inverted process of a fixed Markov Chain of length T.\nLt(x) = E\u03f5\u223cN(0,1)[\u2225\u03f5 \u2212 z\u03b8(zt, t)\u22252\n2]\n(2)\nHere, z\u03b8 represents a denoising neural network parame-\nterized by \u03b8, and Lt is the loss function.\n4. Methods\nOverall pipeline.\nThe scheme of our T2V pipeline is\nshown in Figure 2. It includes a text encoder, a keyframe\nlatent generation model, a frame interpolation model, and a\nlatent decoder. Below, we describe these key components\nin details.\n4.1. Keyframes Generation with Temporal Condi-\ntioning\nThe keyframes generation is based on a pretrained latent\ndiffusion T2I model Kandinsky 3.0 [1]. We use the weights\nof this model to initialize the spatial layers of the keyframe\ngeneration model, which is distinguished by the presence\nof temporal components. In all experiments, we freeze the\nweights of T2I U-Net and train only temporal components.\nWe consider two fundamental ways of introducing temporal\ncomponents into the general architecture \u2013 temporal layers\nof convolutions with attention integrated into spatial blocks\nand our separate temporal blocks. Figure 3 exhaustively\nexplains our concept.\nWe also investigate different types of temporal condi-\ntioning in convolution and attention layers. In the case of\nconvolution, we use its 3D version with kernels 3 \u00d7 1 \u00d7 1\nand 3 \u00d7 3 \u00d7 3 corresponding to Temporal Conv1D and Tem-\nporal Conv3D, respectively. A similar mechanism was im-\nplemented for temporal attention layers to build interaction\nnot only between latent positions between each other across\ntime dimension (Temporal Attn1D) but also between a win-\ndow of positions across the video (Temporal Attn3D) (Fig-\nure 3 - right). We used an attention window of size 2\u00d72\u00d7T\nin Temporal Attn3D, where T is the total number of gener-\nated frames. The algorithm of dividing the whole frame\ninto groups of four pixels is the same as Block Attention in\nMaxViT [34].\n3\nFigure 3. Comparative schemes for temporal conditioning. We examined two approaches of temporal components use in pretrained\narchitecture of T2I U-Net from Kandinsky 3.0 [1] \u2013 the traditional approach of mixing spatial and temporal layers in one block (left)\nand our approach of allocating a separate temporal block (middle). All layers indicated in gray are not trained in T2V architectures and\nare initialized with the weights of the T2I Kandinsky 3.0 model. NA and N in the left corner of all layers correspond to the presence\nof prenormalization layers with and without activation, respectively. For different types of blocks we implemented different types of\ntemporal attention and temporal convolution layers, (left). We also implement different types of temporal conditioning. One of them is\nsimple conditioning when pixel see only itself value in different moments of type (1D layers). In 3D layers pixels can see the values of its\nneighbors also in different moments of time (right).\nIn our case, separate temporal blocks are organized in\nthree ways:\n\u2022 Conv1dAttn1dBlocks \u2013 Incorporation of a Temporal\nConv1D Block following each spatial convolution block\nand a Temporal Attn1D Block following each self-\nattention block in the T2I model.\n\u2022 Conv3dAttn1dBlocks \u2013 Incorporation of a Temporal\nConv3D Block following each spatial convolution block\nand a Temporal Attn1D Block following each self-\nattention block. Additionally, two extra Linear layers are\nintroduced within the Temporal Conv3D Block to down-\nsample the hidden dimension of the input tensor and sub-\nsequently upsample it to the original size. These projec-\ntions are employed to maintain a comparable number of\nparameters as in the Conv1dAttn1dBlocks configuration.\n\u2022 Conv1dAttn3dBlocks \u2013 Incorporation of a Temporal\nConv1D Block following each spatial convolution block\nand a Temporal Attn3D Block following each self-\nattention block. In this configuration, no extra parameters\nare introduced compared to Conv1dAttn1dBlocks, as the\nalteration is limited to the sequence length in attention\ncalculation. Moreover, there is minimal computational\noverhead due to the relatively low number of keyframes\n(16 for all our models)..\nIn the case of the mixed spatial-temporal block\n(Conv1dAttn1dLayers), we exclusively employ the most\nstandard form of integration,\nincorporating Temporal\nConv1D and Temporal Attn1D layers into the inner\nsection of the spatial blocks (Figure 3 - left).\nWe\nmade this choise because the integration style represented\nby Conv1dAttn1dLayers consistently yields worse results\nacross all metrics when compared to Conv1dAttn1dBlocks.\n4.2. Video Frame Interpolation\nWe apply interpolation in the latent space to predict\na group of three frames between each pair of consec-\nutive keyframes.\nThis necessitates the adaptation of\nthe T2I architecture (Figure 4).\nFirst, we inflate the\ninput convolution layer (by zero-padding weights) to\nprocess: (i) A group of three noisy latents representing\ninterpolated\nframes\n(zt = [z1\nt , z2\nt , z3\nt ]; zi\nt \u2208 R4\u00d732\u00d732),\nand (ii) two conditioning latents representing keyframes\n(c = [c1, c2]; ci \u2208 R4\u00d732\u00d732).\nThe\ninput\nto\nU-Net\nconsists\nof\nthe\nchannel-wise\nconcatenation\nof\nzt\nand\nc\n([zt, c] \u2208 R20\u00d732\u00d732).\nSimilarly,\nwe\ninflate\nthe\noutput\nconvolution\nlayer\n(by\nreplicate\npadding\nweights) to predict a group of three denoised latents\n(zt\u22121 = [z1\nt\u22121, z2\nt\u22121, z3\nt\u22121]; zt\u22121 \u2208 R12\u00d732\u00d732). Secondly,\nwe insert temporal convolution layers \u03d5 in the original T2I\nmodel \u03b8. This enables upsampling a video with arbitrary\nlength T \u2192 4T \u2212 3 frames. The activation derived from a\ntemporal layer outtemporal is combined with the output of\nthe corresponding spatial layer outspatial using a trainable\nparameter \u03b1 as follows:\nout = outspatial + \u03b1 \u00b7 outtemporal\n(3)\n4\nFigure 4. FusionFrames Interpolation Architecture. A sum-\nmary of the primary changes made to the T2I architecture includes:\n(i) The input convolution now accepts three noisy latent inputs (in-\nterpolated frames) and two conditioning latent inputs (keyframes).\n(ii) The output convolution predicts three denoised latents. (iii)\nTemporal convolutions have been introduced after each spatial\nconvolution.\nA detailed description of these adjustments is provided in\nAppendix 9. Finally, we remove text conditioning, and in-\nstead, we incorporate skip-frame s (section 5) and pertur-\nbation level tp conditioning (Check below for details). For\ninterpolation, we use v-prediction parameterization (vt \u2261\n\u03b1t\u03f5 \u2212 \u03c3tx) as described in [11, 28].\nBuilding on the conditional frame perturbation technique\n[4, 9, 11, 50], we randomly sample a perturbation level\n(number of forward diffusion steps) pt \u2208 {0, 1, 2, . . . , 250}\nand use it to perturb the conditioning latents c using for-\nward diffusion process (Section 3). This perturbation level\nis also employed as a condition for the U-Net (as described\nearlier).\nWith some probability uncond prob, we also replace the\nconditioning latents c with zeros for unconditional genera-\ntion training. The final training objective looks like:\nLt(x; c,tp, s, m) =\nE\u03f5\u223cN(0,1)[\u2225vt \u2212 z\u03b8,\u03d5([zt, c], t, tp, s, m)\u22252\n2]\n(4)\nwhere m = 0 specify that we replace conditioning frames\nwith zeros and m = 1 otherwise.\nWe employ context\nguidance [4] at inference, with w representing the guidance\nweight:\n\u02dcz\u03b8,\u03d5([zt, c], tp, s) = (1 + w)z\u03b8,\u03d5([zt, c], tp, s, m = 1)\n\u2212 wz\u03b8,\u03d5([zt, c], s, m = 0)\n(5)\nOur interpolation model can benefit from a relatively low\nguidance weight value, such as 0.25 or 0.5. Increasing this\nvalue can significantly negatively impact the quality of the\ninterpolated frames, sometimes resulting in the model gen-\nerating frames that closely match the conditional keyframes\n(No actual interpolation).\n4.3. Video Decoder\nTo enhance the video decoding process, we use a pretrained\nMoVQ-GAN [53] model with a frozen encoder. To extend\nthe decoder into the temporal dimension, we explore sev-\neral choices: Substituting 2D convolutions with 3D convo-\nlutions and adding temporal layers interleaved with existing\nspatial layers. Regarding the temporal layers, we explore\nthe use of temporal convolutions, temporal 1D conv blocks,\nand temporal self-attentions. All additional parameters are\ninitialized with zeros.\n5. Experiments\nDatasets. Our internal training dataset for the keyframe\ngeneration model contains 120k text-video pairs, and the\nsame dataset is utilized for training the interpolation model.\nIn our evaluation, we evaluate the T2V model on two test\nsets: UCF-101 [33] and MSR-VTT [45]. For training the\ndecoder, we use a mix of 80k videos from the internal\ndataset, while model testing is performed on the septuplet\npart of Vimeo90k [46] dataset.\nIn preparing frame sequences for the interpolation task\ntraining, we randomly select a skip-frame value s\n\u2208\n{1, 2, . . . , 12}. Subsequently, the input video is resampled\nwith a skip of s frames between each pair. This 33 frames\nvideo is then organized into 8 conditioning frames for each\nside and 8 \u00d7 3 target target frames. For decoder training,\nsequences comprising 8 frames are employed.\nMetrics. In line with previous works [17, 19, 29], we as-\nsess our T2V model using the following evaluation metrics:\nFrechet Video Distance (FVD) [35], Inception Score (IS)\n[27] and CLIPSIM [42]. FVD assists in evaluating the fi-\ndelity of the generated video. IS metric assesses the quality\nand diversity of individual frames. CLIPSIM, on the other\nhand, evaluates text-video alignment. For FVD calculation,\nwe use the protocol proposed in StyleGAN-V[30] for a fair\ncomparison. For IS, there is no clear way to calculate in\nrelated works. In this work, we calculate IS on 2048 videos\nby taking the first 16 frames in each video. For decoder\ntraining, other metrics are used: PSNR for frame quality\nassessment, SSIM for structural similarity evaluation, and\nLPIPS [51] for perceptual similarity.\nTraining. We trained 4 keyframe generation models for\n100k steps on 8 GPUs (A100, 80GB) with batch size of 1,\nand gradient accumulation of 2 to generate 16 frames with\n512 \u00d7 512 resolution. Keyframes generation was trained\nwith 2 FPS. As far as the FPS of training videos is around\n24, we sampled frames for training randomly choosing the\nfirst frame position and the positions of the next frames\nwith the skip of 12.\nPositions of sampled frames were\n5\nFigure 5. Generations of keyframes for all our models. Prompt: A panda making latte art in cafe.\nencoded by a table of positional embeddings. We trained\nonly temporal layers or temporal blocks, depending on the\nmethod. All other weights were taken from our T2I model\nand frozen. Parameters were optimized using AdamW op-\ntimizer with the constant learning rate of 0.0001. The opti-\nmizer was patitioned across GPUs using ZeRO stage 1.\nThe entire interpolation model was trained for 50k steps\nat the task of upsampling 9 frames across different skip-\nframe values s to a sequence of 33 frames, all while\nmaintaining a resolution of 256 \u00d7 256. During training,\nwe set the probability for unconditional frame generation\nuncond prob to 10%. Our decoder, including the spatial\nlayers, is trained using sequences of 8 frames for 50k steps.\nDuring interpolation training, we use 8 GPUs (A100 80GB)\nand gradient accumulation steps of 4. For decoder training,\nwe turn off gradient accumulation.\nInference. Keyframes are generated in the T2V gen-\neration phase.\nTo interpolate between these generated\nkeyframes, we employ the generated latents from the first\nphase as conditions to generate three middle frames be-\ntween each keyframe pair. We set the skip-frame value s\nto 3 during the first step of interpolation (2FPS\u21928FPS) and\n1 during the second step (8FPS\u219230FPS). Additionally, we\nmaintain a constant perturbation noise level tp = 0. We set\nthe guidance weight to a small value w = 0.25. In the fi-\nnal stage, our trained decoder decodes the resulting latents\nand the latents from the generated keyframes to produce the\nfinal video output.\n6. Results\n6.1. Quantitative Results\nIn this section, we provide a comparison of our trained\nmodels using FVD, IS on UCF-101 and CLIPSIM on\nMSR-VTT, as detailed in Table 1.\nFirst, we assess\nthe models trained for 100k training steps.\nConcern-\ning the comparison with Conv1dAttn1dLayers, our re-\nsults clearly indicate that the inclusion of temporal blocks,\nrather than temporal layers, results in significantly en-\nhanced quality according to these metrics. In the assess-\nment of temporal blocks, Conv3dAttn1dBlocks falls be-\nhind, achieving values of 594.919 for FVD and 22.899\nfor IS. Conv3dAttn1dBlocks shows an IS score of 23.381\nfor IS and an FVD score of 573.569.\nComparing with\nConv3dAttn1dBlocks, Conv3dAttn1dBlocks records a worse\nIS score of 23.063 but a the best FVD score 545.184. The\nCLIPSIM results do not show a clear difference among tem-\nporal blocks, with slightly better performance observed for\nConv1dAttn3dBlocks, reaching a CLIPSIM of 0.2956.\nNext, we showcase the best outcomes achieved for\nConv1dAttn1dBlocks after being trained for 220k training\nsteps. Evidently, our model experiencing significant im-\nprovements in terms of FVD (433.504) and IS (24.325).\nWe want to highlight that the relatively lower perfor-\nmance in IS compared to the baselines could be attributed\nto ambiguities in IS assessment. Existing literature lacks\nsufficient details regarding the methodologies used for cal-\nculating IS on video in related works.\n6.2. Qualitative Results\nFigure 6. Human evaluation study results. The bars in the plot\ncorrespond to the percentage of \u201cwins\u201d in the side-by-side com-\nparison of model generations.\n6\nTable 1. T2V results on UCF-101 and MSR-VTT. We compare various options to build temporal blocks: 1) Conv1dAttn1dLayers, 2)\nConv1dAttn1dBlocks, 3) Conv1dAttn3dBlocks, and 4) Conv1dAttn1dBlocks, (Between parentheses \u2013 the number of training steps). We\ncalculate FVD, IS on UCF-101 and CLIPSIM on MSR-VTT.\nMethod\nZero-Shot\nIS\u2191\nFVD\u2193\nCLIPSIM\u2191\nProprietary Technologies\nGoDIVA [42]\nNo\n-\n-\n0.2402\nNuwa [43]\nNo\n-\n-\n0.2439\nMagic Video [54]\nNo\n-\n699.00\n-\nVideo LDM [4]\nNo\n-\n550.61\n0.2929\nMake-A-Video [29]\nYes\n33.00\n367.23\n0.3049\nOpen Sourced Techologies\nLVDM [9]\nNo\n-\n641.80\n-\nModelScope [40]\nNo\n-\n-\n0.2930\nLaVie [41]\nNo\n-\n526.30\n0.2949\nCogVideo (Chinese) [13]\nYes\n23.55\n751.34\n0.2614\nCogVideo (English) [13]\nYes\n25.27\n710.59\n0.2631\nTemporal Blocks (ours)\nConv1dAttn1dLayers (100k)\nYes\n19.663\n659.612\n0.2827\nConv1dAttn1dBlocks (100k)\nYes\n23.063\n545.184\n0.2955\nConv1dAttn3dBlocks (100k)\nYes\n23.381\n573.569\n0.2956\nConv3dAttn1dBlocks (100k)\nYes\n22.899\n594.919\n0.2953\nConv1dAttn1dBlocks (220k)\nYes\n24.325\n433.054\n0.2976\nWe performed a preference test to compare our models\nbetween each other.\nFor this purpose, we created a bot\nthat displays a pair of videos. An annotator then chooses\nthe video with best quality in terms of three visual mea-\nsurements: 1) frame quality, 2) alignment with text, and\n3) temporal consistency. A total of 31 annotators engaged\nin this process and the total number of used video pairs\nis 6600. Figure 6 presents the results of this user study.\nThere is a clear preference of videos generated by temporal\nblocks based models over the videos generated by tempo-\nral layer based model with a large margin among all mea-\nsurements. The temporal layer based model often produces\neither semantically unrelated keyframes, or does not deal\nwell with the dynamics. Regarding the comparison between\ntemporal block methods against each other, there is a small\nbut consist preference for the model with Temp. Conv1D\nBlock and Temp. Attn3D Block over the two other mod-\nels. Finally, there is no clear preference between the rest\ntwo models. The qualitative comparison reveals visually\nobservable advantages of our technique, both in the quality\nof generated objects on individual keyframes and in dynam-\nics as illustrated in Figure 5. The method based on temporal\nblocks generates more consistent content and more coherent\nkeyframes sequence. More examples with video generation\ncan be found on the project page and in supplementary ma-\nterial.\nTable 2. A comparison on UCF-101 between our implementation\nof masked frame interpolation architecture and our new interpola-\ntion architecture after 2 interpolation steps (30 FPS).\nMethod\nIS\u2191\nFVD\u2193\nMasked Frame Interpolation\n23.371\n550.932\nFusionFrames Interpolation\n24.325\n433.054\n6.3. Interpolation Architecture Evaluation\nTo assess the effectiveness of our interpolation architecture,\nwe implemented Masked Frame Interpolation (MFI) archi-\ntecture described in Video LDM [4] and Appendix 9. For\nour interpolation architecture, we use a 220k training steps\ncheckpoint for a model trained with temporal 1D-Blocks.\nWe generated interpolated frames for a set of 2048 videos,\nall composed exclusively of generated keyframes. Results,\nas presented in Table 2, reveal that our new interpolation\narchitecture excels in generating interpolated frames with\nsuperior quality and higher fidelity. Specifically, it achieves\nFVD of 433.054 compared to 550.932 obtained with MFI.\nThe inference time associated with each architecture is il-\nlustrated in Figure 7, demonstrating that our new architec-\nture is more efficient, reducing the running time to less than\nthird compared to MFI.\n6.4. Video MoVQ-GAN Experiments\nWe conducted comprehensive experiments, considering\nmany choices of how to build video decoder, and assessed\n7\nFigure 7. Running time comparison between two pipelines with different interpolation architectures: (1) Our architecture, and (2)\nMasked Frame Interpolation (MFI) on A100 80GB. Keyframe generation in green. First and second step interpolation in yellow and red\nrespectively.\nTable 3. A comparison between different choices to construct video decoder. Including the use of temporal convolution, temporal\nResNet Block, temporal attention (Attn) and finally converting 2D spatial convolution in the decoder into 3D conv (2D\u21923D Conv). We\nalso present whether we only finetune temporal layers or the entire decoder.\nDecoder\nTemporal Layers\nFinetune\nPSNR\u2191\nSSIM\u2191\nMSE\u2193\nLPIPS\u2193\n# Params\nImage\n-\n-\n32.9677\n0.9056\n0.0008\n0.0049\n161 M\nVideo\n3x1x1 Conv\nTemporal\n32.2544\n0.893\n0.0009\n0.006\n203 M\nVideo\n3x3x3 Conv\nTemporal\n33.5819\n0.9111\n0.0007\n0.0044\n539 M\nVideo\n3x1x1 Conv\nDecoder\n33.5051\n0.9106\n0.0007\n0.0044\n203 M\nVideo\n3x3x3 Conv\nDecoder\n33.6342\n0.9123\n0.0007\n0.0043\n539 M\nVideo\n3x1x1 Conv + Attn\nDecoder\n33.7343\n0.9129\n0.0007\n0.0043\n220 M\nVideo\n3x3x3 Conv + Attn\nDecoder\n33.8376\n0.9146\n0.0006\n0.0041\n556 M\nVideo\nResNet Block + Attn\nDecoder\n33.7024\n0.9121\n0.0007\n0.0043\n220 M\nVideo\n2D \u2192 3D Conv\nDecoder\n33.7321\n0.9134\n0.0007\n0.0043\n419 M\nthem in terms of quality metrics and the impact on the num-\nber of additional parameters. The results are presented in\nTable 3. This evaluation process guided us in making the\noptimal choice for production purposes. Extending the de-\ncoder with a 3\u00d73\u00d73 temporal convolution and incorporat-\ning temporal attention during the fine-tuning process yields\nthe highest overall quality among the available options. In\nthis manner, we apply finetuning to the entire decoder, in-\ncluding spatial layers and the newly introduced parameters.\nAn alternative efficient choice involves using a 3\u00d71\u00d71 tem-\nporal convolution layer or temporal 1D Block with temporal\nattention, which significantly reduces the number of param-\neters from 556M to 220M while still achieving results that\nclosely match the quality obtained through the more exten-\nsive approach.\n7. Limitations\nComparative analysis faces challenges due to the ambigu-\nities in the calculation procedures of metrics such as FVD\nand IS in related works (Also, noted by [30]), and that hiders\nthe ability to draw meaningful comparisons with other stud-\nies. We recommend to use the protocol for FVD calculation\ndescribed in StyleGAN-V[30]. Metrics obtained using this\nprotocol (and their evaluation code) closely match the mea-\nsurements of our own implementation. Furthermore, com-\nparing our interpolation network with existing works poses\nchallenges, and the absence of open solutions for interpo-\nlation in the latent space forces us to independently imple-\nment the other described approach [4].\n8. Conclusion\nIn this research, we examined several aspects of T2V ar-\nchitecture design in order to get the most considerable out-\nput quality. This challenging task included the development\nof a two-stage model for video synthesis, considering sev-\neral ways to incorporate temporal information: temporal\nblocks and temporal layers. According to experiments, the\nfirst approach leads to higher metrics values in terms of vi-\nsual quality measured by IS and FVD scores. We achieved\na comparable IS score value to several existing solutions\nand a top-2 scores overall and top-1 among open-source\nT2V models in terms of CLIPSIM and FVD metrics. The\ninterpolation architecture presented in this work excels in\ngenerating high-quality interpolated frames and surpasses\nmasked frame interpolation architecture in terms of qual-\nity measured by IS and FVD. Our interpolation architecture\nis also more efficient in execution time and its execution\ntime is more than 3 times effective comparing to the well-\nknown masked frame interpolation approach. The paper\nalso presents a new MoVQ-based video decoding scheme\nand the results of its effect on the overall quality. The fol-\nlowing problems still remain and need to be researched fur-\nther: the image quality of frames, adjacent frame smooth-\nness consistency with visual quality preserving.\n8\nReferences\n[1] Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev,\nAnastasia Maltseva, Said Azizov, Igor Pavlov, Julia Aga-\nfonova, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky\n3.0 technical report, 2023. 2, 3, 4\n[2] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,\nRoy H Campbell, and Sergey Levine. Stochastic variational\nvideo prediction. arXiv preprint arXiv:1710.11252, 2017. 2\n[3] Mohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj\nNair, Sergey Levine, Chelsea Finn, and Dumitru Erhan.\nFitvid: Overfitting in pixel-level video prediction.\narXiv\npreprint arXiv:2106.13195, 2021. 2\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 2, 3, 5, 7, 8, 1\n[5] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adver-\nsarial video generation on complex datasets. arXiv preprint\narXiv:1907.06571, 2019. 2\n[6] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via\nhierarchical transformers. Advances in Neural Information\nProcessing Systems, 35:16890\u201316902, 2022. 2\n[7] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models,\n2023. 2\n[8] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan\nPang, David Jacobs, Jia-Bin Huang, and Devi Parikh.\nLong video generation with time-agnostic vqgan and time-\nsensitive transformer.\narXiv preprint arXiv:2204.03638,\n2022. 2\n[9] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 2, 3, 5, 7\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 1, 3\n[11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2, 3, 5\n[12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv:2204.03458, 2022. 2\n[13] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 2, 7\n[14] Zhifeng Kong and Wei Ping.\nOn fast sampling of diffu-\nsion probabilistic models. arXiv preprint arXiv:2106.00132,\n2021. 3\n[15] Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan,\nChelsea Finn, Sergey Levine, Laurent Dinh, and Durk\nKingma.\nVideoflow: A flow-based generative model for\nvideo. arXiv preprint arXiv:1903.01434, 2(5):3, 2019. 2\n[16] Alex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel,\nChelsea Finn, and Sergey Levine.\nStochastic adversarial\nvideo prediction. arXiv preprint arXiv:1804.01523, 2018.\n2\n[17] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu,\nQi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong\nWang. Videogen: A reference-guided latent diffusion ap-\nproach for high definition text-to-video generation.\narXiv\npreprint arXiv:2309.00398, 2023. 2, 3, 5\n[18] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In Proceedings\nof the AAAI conference on artificial intelligence, 2018. 2\n[19] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and\nTieniu Tan.\nVideofusion:\nDecomposed diffusion mod-\nels for high-quality video generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10209\u201310218, 2023. 5\n[20] Gaurav Mittal, Tanya Marwah, and Vineeth N Balasubrama-\nnian. Sync-draw: Automatic video generation using deep\nrecurrent attentive architectures. In Proceedings of the 25th\nACM international conference on Multimedia, pages 1096\u2013\n1104, 2017. 2\n[21] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and\nMartin Renqiang Min. Conditional image-to-video gener-\nation with latent flow diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18444\u201318455, 2023. 2\n[22] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob McGrew,\nIlya\nSutskever, and Mark Chen.\nGLIDE: towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In International Conference on Machine Learning,\nICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,\npages 16784\u201316804. PMLR, 2022. 1\n[23] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao\nMei. To create what you tell: Generating videos from cap-\ntions. In Proceedings of the 25th ACM international confer-\nence on Multimedia, pages 1789\u20131798, 2017. 2\n[24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 1\n[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2\n[26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 1\n9\n[27] Masaki Saito, Shunta Saito, Masanori Koyama, and So-\nsuke Kobayashi. Train sparsely, generate densely: Memory-\nefficient unsupervised training of high-resolution temporal\ngan. International Journal of Computer Vision, 128(10-11):\n2586\u20132606, 2020. 5\n[28] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 5\n[29] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3, 5, 7\n[30] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. Stylegan-v: A continuous video generator with the\nprice, image quality and perks of stylegan2. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 3626\u20133636, 2022. 5, 8\n[31] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn Proceedings of the\n32nd International Conference on Machine Learning, pages\n2256\u20132265, Lille, France, 2015. PMLR. 1\n[32] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions.\nIn 9th International Conference on Learning Rep-\nresentations, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. OpenReview.net, 2021. 1\n[33] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402, 2012. 5\n[34] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,\nPeyman Milanfar, Alan Bovik, and Yinxiao Li.\nMaxvit:\nMulti-axis vision transformer. ECCV, 2022. 3\n[35] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 5\n[36] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and D. Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. ArXiv, abs/2210.02399, 2022. 2\n[37] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal.\nMcvd-masked conditional video diffusion for prediction,\ngeneration, and interpolation. Advances in Neural Informa-\ntion Processing Systems, 35:23371\u201323385, 2022. 2\n[38] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\nGenerating videos with scene dynamics.\nIn Proceedings\nof the 30th International Conference on Neural Information\nProcessing Systems, page 613\u2013621, Red Hook, NY, USA,\n2016. Curran Associates Inc. 2\n[39] Jacob Walker, Ali Razavi, and A\u00a8aron van den Oord. Pre-\ndicting video with vqvae. arXiv preprint arXiv:2103.01950,\n2021. 2\n[40] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report, 2023. 7\n[41] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu,\nPeiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yum-\ning Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua\nLin, Yu Qiao, and Ziwei Liu. Lavie: High-quality video gen-\neration with cascaded latent diffusion models, 2023. 7\n[42] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji,\nFan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Gen-\nerating open-domain videos from natural descriptions. arXiv\npreprint arXiv:2104.14806, 2021. 2, 5, 7\n[43] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation, 2021. 2, 7\n[44] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 2\n[45] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5288\u20135296, 2016. 5\n[46] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and\nWilliam T Freeman. Video enhancement with task-oriented\nflow. International Journal of Computer Vision, 127:1106\u2013\n1125, 2019. 5\n[47] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind\nSrinivas. Videogpt: Video generation using vq-vae and trans-\nformers. arXiv preprint arXiv:2104.10157, 2021. 2\n[48] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Dif-\nfusion probabilistic modeling for video generation.\narXiv\npreprint arXiv:2203.09481, 2022. 2\n[49] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin.\nVideo probabilistic diffusion models in projected latent\nspace. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2023. 2\n[50] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent dif-\nfusion models for text-to-video generation. arXiv preprint\narXiv:2309.15818, 2023. 3, 5\n[51] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 5\n[52] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023. 2\n[53] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh\nPhung.\nMovq:\nModulating quantized vectors for high-\nfidelity image generation. Advances in Neural Information\nProcessing Systems, 35:23412\u201323425, 2022. 2, 5\n10\n[54] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3, 7\n11\nFusionFrames: Efficient Architectural Aspects for\nText-to-Video Generation Pipeline\nSupplementary Material\n9. FusionFrames Interpolation Architecture\nImplementation Details\nExpanding the Text-to-Image architecture to effectively\nhandle and generate a sequence of interpolated frames re-\nquires a series of modifications designed to process the data\nacross the temporal dimension.\nIn addition to this, fur-\nther adjustments are necessary to facilitate the generation\nof three middle frames between two keyframes.\nSpecifically,\nstarting\nwith\npre-trained\nweights\nfor\nthe\nText-to-Image\nmodel,\nwe\nreplicate\nthe\nweights\nwithin\nthe\noutput\nconvolution\nlayer\nthree\ntimes.\nThis\ntransformation\nalters\nthe\ndimensions\nof\nthose\nweights from (out channels, input channels, 3, 3) to\n(3 \u2217 out channels, input channels, 3, 3), and a similar\nmodification is carried out for the bias parameters, shifting\nthem from (out channels) to (3 \u2217 out channels). This\nadaptation enables the generation of three identical frames\nbefore the training phase starts. In the input convolution\nlayer, we make similar adjustments to the input channels,\ninitializing additional weights as zeros.\nSubsequently, a\ntemporal convolution layer, with a kernel size of 3 \u00d7 1 \u00d7 1,\nis introduced after each spatial convolution layer.\nThe\noutput from each temporal layer is then combined with\nthe output from the spatial layer using a learned merge\nparameter \u03b1.\nThe spatial layers are designed to handle input as a batch\nof individual frames. When dealing with video input, a nec-\nessary adjustment involves reshaping the data to shift the\ntemporal axis into the batch dimension. Consequently, be-\nfore forwarding the activations through the temporal layers,\na transformation is performed to revert the activations back\nto its original video dimensions.\n10. Masked Frame Interpolation Implementa-\ntion\nWe re-implemented Masked Frame Interpolation as de-\nscribed in Video LDM[4] and Fig. 8. U-Net takes upsam-\npled video as input in addition to noisy frames and mask.\nWe obtain upsampled video by zero-padding in the place\nof interpolated frames. The mask specify which frames are\nkey-frames. Conditional frame perturbation is also incor-\nporated as described. We train a model for two stages in-\nterpolation: T \u2192 4T and 4T \u2192 16T. We initialize this\nmodel with the same pre-trained T2I weights used in our\nexperiments. Also, we initialize additional parameters with\nzeros. The training example consists of a sequence of 5\nframes (2 conditional frames and 3 interpolated frames).\nWe train on the same number of GPUs utilized in our exper-\niments for 17k steps (this number of training steps is used\nin Video LDM, and we found the model to converge around\nthis number of training steps).\n1\nFigure 8. Masked Frame Interpolation Architecture [4]. Input to unit consists of noisy latent input, upsampled latent video and a mask.\nThe mask specifies which latent inputs correspond to conditional keyframes.\n11. Additional Generation Results\n2\nFigure 9. Additional results of keyframes generation by FusionFrames.\n3\nFigure 10. Additional results of keyframes generation by FusionFrames.\n4\n"
  },
  {
    "title": "LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes",
    "link": "https://arxiv.org/pdf/2311.13384.pdf",
    "upvote": "48",
    "text": "LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes\nJaeyoung Chung*\nSuyoung Lee*\nHyeongjin Nam\nJaerin Lee\nKyoung Mu Lee\nASRI, Department of ECE, Seoul National University, Seoul, Korea\n{robot0321, esw0116, namhjsnu28, ironjr, kyoungmu}@snu.ac.kr\nFigure 1. Introducing LucidDreamer. We develop LucidDreamer, a general framework for generating multiview-consistent and high-\nquality 3D scenes from various input types: text, RGB, and RGBD. After the initial point cloud is created by lifting the RGBD image,\nLucidDreamer maintains and expands its world model by repeating two operations: dreaming and alignment. The 3D scene is finalized\nthrough optimizing a Gaussian splatting representation.\nAbstract\nWith the widespread usage of VR devices and contents,\ndemands for 3D scene generation techniques become more\npopular. Existing 3D scene generation models, however,\nlimit the target scene to specific domain, primarily due to\ntheir training strategies using 3D scan dataset that is far\nfrom the real-world. To address such limitation, we propose\nLucidDreamer, a domain-free scene generation pipeline by\nfully leveraging the power of existing large-scale diffusion-\nbased generative model. Our LucidDreamer has two al-\nternate steps: Dreaming and Alignment. First, to generate\nmulti-view consistent images from inputs, we set the point\n* indicates equal contribution.\ncloud as a geometrical guideline for each image genera-\ntion. Specifically, we project a portion of point cloud to the\ndesired view and provide the projection as a guidance for\ninpainting using the generative model. The inpainted im-\nages are lifted to 3D space with estimated depth maps, com-\nposing a new points. Second, to aggregate the new points\ninto the 3D scene, we propose an aligning algorithm which\nharmoniously integrates the portions of newly generated\n3D scenes. The finally obtained 3D scene serves as initial\npoints for optimizing Gaussian splats. LucidDreamer pro-\nduces Gaussian splats that are highly-detailed compared to\nthe previous 3D scene generation methods, with no con-\nstraint on domain of the target scene.\narXiv:2311.13384v2  [cs.CV]  23 Nov 2023\n1. Introduction\nWith the advent of commercial mixed reality platforms\nand the rapid innovations in 3D graphics technology, high-\nquality 3D scene generation has become one of the most\nimportant problem in computer vision. This requires the\nability to create diverse and photo-realistic 3D scenes from\nany type of input, such as text, RGB, and RGBD images.\nThere are efforts to use the diffusion model in voxel, point\ncloud, and implicit neural representation to generate 3D ob-\njects and scenes directly [11, 25, 60], but the results show\nlow diversity and quality due to the limitations in training\ndata based on 3D scans. One way to cope with the issue is to\nleverage the power of a pre-trained image generation diffu-\nsion model, such as Stable Diffusion [39], to create diverse\nhigh-quality 3D scenes. Such a big model creates plau-\nsible images with a data-driven knowledge learned from\nthe large-scale training data, although it does not guarantee\nmulti-view consistency between the generated images [51].\nIn this work, we propose a pipeline called Lucid-\nDreamer that utilizes Stable Diffusion [39] and 3D Gaus-\nsian splatting[20] to create diverse high-quality 3D scenes\nfrom various types of inputs such as text, RGB, and RGBD.\nFollowing the pipeline of LucidDreamer, a unified large\npoint cloud is generated by repeating the two processes\nnamed Dreaming and Alignment, alternatively. Before be-\nginning the two process, an initial point cloud is gener-\nated by the initial image and the corresponding depth map.\nDreaming process includes the generation of geometrically\nconsistent images and the lifting of these images into 3D\nspace. We first move the camera along the pre-defined cam-\nera trajectory and project a visible region of point cloud in\nthe new camera coordinate to the new camera plane. Then,\nthe projected image is put into the Stable Diffusion-based\ninpainting network to generate the complete image from the\nprojected one. A new set of 3D points are generated by lift-\ning the inpainted image and the estimated depth map to the\n3D space. Then the proposed alignment algorithm seam-\nlessly connects the new 3D points to the existing point cloud\nby slightly moving the position of the new points in the 3D\nspace. After the large point cloud generated by repeating\nthe above processes a sufficient number of time is obtained,\nwe use it as the initial SfM points to optimize the Gaussian\nsplats. The continuous representation of 3D Gaussian splats\nremoves the holes generated by the depth discrepancy in the\npoint cloud, enabling us to render more photo-realistic 3D\nscenes than traditional representations. Figure 1 shows the\nsimple process of LucidDreamer and a 3D generation result.\nLucidDreamer exhibits significantly more realistic and\nastonishing results compared to existing models. We com-\npare the generated 3D scenes conditioned with an image\nfrom ScanNet [9], NYUDepth [45], and Stable Diffusion,\nand show better visual results across all datasets. Our model\nis capable of generating 3D scenes across diverse domains\nsuch as realistic/anime/lego and indoor/outdoor. Not only\ndoes our model support various domains, but it also accom-\nmodates the simultaneous use of diverse input conditions.\nFor example, by conditioning an image and text together, it\ngenerates a 3D scene based on the text but also includes the\nimage. This alleviates the challenges associated with cre-\nating the desired scene solely from the text, moving away\nfrom generating samples exhaustively.\nFurthermore, our\napproach also allows for the change of the input condition\nwhile creating the 3D space. These capabilities offer op-\nportunities to create a wide range of 3D scenes, inspiring\ncreativity.\nIn summary, our contributions are as follows.\n\u2022 We introduce LucidDreamer, a domain-free high-quality\n3D scene generation, achieving better domain general-\nization in 3D scene generation by leveraging the power\nof Stable Diffusion, depth estimation, and explicit 3D\nrepresentation.\n\u2022 To generate multi-view images from Stable Diffusion,\nour Dreaming process establishes point cloud as ge-\nometrical guideline for each image generation. Subse-\nquently, our Aligning process harmoniously integrates\nthe generated images to form an unified 3D scene.\n\u2022 Our model provides users with the ability to create 3D\nscenes in various ways by supporting different input\ntypes, such as text, RGB, and RGBD, allowing the si-\nmultaneous use of multiple inputs, and enabling the\nchange of the inputs during the generation process.\n2. Related Work\n3D Scene Representation.\nRepresentative methods for\nexpressing 3D scenes include explicit methods such as point\ncloud, mesh, and voxel. These are widely used because they\nallow direct and intuitive control of each element and en-\nable fast rendering through the rasterization pipeline. How-\never, they need a large number of elements for detailed ex-\npression because of their simple structure. Complex prim-\nitives such as cuboid [52], Gaussian [12], ellipsoid [16],\nsuperquadrics [33], convex hull [10], and polynomial sur-\nface [56] were developed for more efficient expression. Al-\nthough primitives have increased expressive power for com-\nplex geometry, it is still difficult to express realistic 3D\nscenes because of simple color representation.\nRecently, there have been works to express more detailed\n3D scenes using neural networks as implicit expressions.\nThey train a neural network to express the scene creating\nthe desired properties in 3D coordinates, such as signed\ndistance function [32, 49], RGB\u03b1 [26, 46]. In particular,\nNeural Radiance Fields [26] showed that it was possible\nto optimize photorealistic 3D scenes from multiple images\nthrough volume rendering, but the scene implicitly stored\n2\nin the network form is difficult to handle and slow. To im-\nprove this, subsequent studies attempted to use volume ren-\ndering in explicit expressions. By utilizing the locality of\nstructures such as sparse voxels[14, 24, 48, 58], featured\npoint clouds[55], Multi-Level Hierarchies [27, 28], ten-\nsor [5], infinitesimal networks [15, 38], triplane [4], poly-\ngon [7], and Gaussain splats [20] they greatly improve the\ntraining and rendering speed. In particular, 3D Gaussian\nsplatting [20] utilizes the concept of Gaussian splats com-\nbined with spherical harmonics and opacity to represent\ncomplete and unbounded 3D scenes. It supports not only\nalpha-blending but also differentiable rasterization, result-\ning in fast, high-quality 3D scene optimization. This struc-\nture is essential for our generation method, which cannot\ndetermine the bounds of the scene due to sequential image\ngeneration, and plays a role in making the scene complete.\n3D Scene Generation.\nInspired by the early success of\ngenerative adversarial network (GAN) [17] in image gen-\neration, similar attempts are made in 3D creation.\nCre-\nating a set of multiview consistent images [3, 31, 42], or\ndirectly creating voxel [29, 30, 54] or point cloud [1, 43]\nwere studied. However, they suffer from GAN\u2019s learning\ninstability [36] and memory limitation in 3D representa-\ntion, limiting the generation quality. Encouraged by the re-\ncent success of diffusion [19, 47] in the field of image gen-\neration [37, 39], there are many attempts to introduce the\ndiffusion model into 3D representation, such as voxel [60],\npoint cloud [25, 59], triplane [4, 6, 44], implicit neural net-\nwork [11, 34, 57]. They use object-centric coordinates be-\ncause of their nature and focus on simple examples. Some\ngenerative diffusion models overcome this problem by us-\ning a mesh as a proxy and diffusing in the UV space. They\ncreate a large portrait scene by continuously building the\nmesh [13] or create indoor scenes [8, 22] and more realistic\nobjects [35, 50]. However, their performance falls short of\nfoundation models[39] because they involve training a new\ndiffusion model in a different representation space, which is\nlimited by data availability and computational resources. In\ncomparison, our method leverages the power of the founda-\ntion model to generate diverse images and creates reliable\n3D scenes through depth estimation and optimization.\n3. Method\nWhile the range of target scenes of existing scene gener-\nation models is strictly restricted due to the limitations in\ntraining dataset, LucidDreamer can generate even more re-\nalistic, higher-resolution 3D scenes with much more general\ninput conditions. For instance, LucidDreamer can generate\na text-relevant scene if only the text prompt is given. Also,\nthe style of the input image is maintained along the scene,\nwhile existing models keep producing scenes that are simi-\nlar to the style of the training dataset, not the input image.\nThe pipeline of LucidDreamer is broadly divided into\ntwo stages: point cloud construction and Gaussian splats\noptimization. During the first stage, an initial point cloud\nis formed from the input image, and the area of the point\ncloud is expanded to create a large scene using Stable Diffu-\nsion inpainting and monocular depth estimation. Then, the\npoint cloud and the reprojected images are used to optimize\nGaussian splats. By representing the scene with Gaussian\nsplats, we can fill the empty space that appears in the point\ncloud due to the depth discrepancy.\n3.1. Point cloud construction\nTo generate multi-view consistent 3D point cloud, we cre-\nate the initial point cloud and aggregate the points by mov-\ning back and forth between 3D space and the camera plane\nwhile moving the camera.\nThe overall process of point\ncloud construction is illustrated in Figure 1.\nInitialization.\nA point cloud generation starts from lift-\ning the pixels of the initial image. If the user gives a text\nprompt as input, the latent diffusion model is used to gen-\nerate an image relevant to the given text, and the depth map\nis estimated using the monocular depth estimation model\nsuch as ZoeDepth [2]. We denote the generated or received\nRGB image and the depth map as I0 \u2208 R3\u00d7H\u00d7W and\nD0 \u2208 RH\u00d7W , where H and W are height and the width\nof the image. The camera intrinsic matrix and the extrinsic\nmatrix of I0 are denoted as K and P0, respectively. For\nthe case where I0 and D0 are generated from the diffusion\nmodel, we set the values of K and P0 by convention regard-\ning the size of the image.\nFrom the input RGBD image [I0, D0], we lift the pixels\ninto the 3D space, where the lifted pixels will form a point\ncloud in a 3D space. The generated initial point cloud using\nthe first image is defined as P0:\nP0 = \u03d52\u21923 ([I0, D0], K, P0) ,\n(1)\nwhere \u03d52\u21923 is the function to lift pixels from the RGBD\nimage [I, D] to the point cloud.\nPoint cloud aggregation.\nWe sequentially attach points\nto the original point cloud to create a large 3D scene.\nSpecifically, we set the camera trajectory with length N,\nwhere Pi indicates the position and pose of the camera in\nthe i-th index, then inpaint and lift the missing pixel in each\nstep. Here, the generated points should satisfy two con-\nditions; the images projected from the points should have\nhigh perceptual quality and be consistent with image parts\nproduced from the existing points. To achieve the former\ncondition, we borrow the representation power of the Sta-\nble Diffusion [39] to the image inpainting task.\nNavigation. At step i, we first move and rotate the cam-\nera from the previous position (Pi\u22121) to Pi. We change the\n3\ncoordinate from the world to the current camera and project\nto the camera plane using K and Pi.\nDreaming. We denote the projected image at camera\nPi as \u02c6Ii. Since the position and the pose of the camera\nare changed, there would be some regions that cannot be\nfilled from the existing point cloud. We define the mask Mi\nto discriminate the region that is filled by existing points\nin \u02c6Ii.\nSpecifically, the value of Mi is one if the corre-\nsponding pixel is already filled or 0 otherwise. The Sta-\nble Diffusion inpainting model (S) is executed to generate\na realistic image, Ii, from the incomplete image (\u02c6Ii) and\nthe mask (Mi). The corresponding depth map ( \u02c6Di) is esti-\nmated using the monocular depth estimation network (D).\nHere, the monocular depth estimation model can only esti-\nmate the relative depth, and the depth coefficients from the\nrelative depth to the actual depth can be different between\nimages. If the depth coefficients are different, the lifted 3D\npoint clouds in the two generated images are not connected\nand are spaced apart. We estimate the optimal depth scale\ncoefficient, di, that minimizes the distance between the 3D\npoints of the new image and the corresponding points in the\noriginal point cloud, Pi\u22121. Then the actual depth map, Di is\ncalculated by multiplying the coefficient di to the estimated\ndepth map, \u02c6Di.\nIi = S\n\u0010\n\u02c6Ii, Mi\n\u0011\n, \u02c6Di = D (Ii) , Di = di \u02c6Di,\ndi = argmin\nd\n X\nMi=1\n\r\r\r\u03d52\u21923\n\u0010h\nIi, d \u02c6Di\ni\n, K, Pi\n\u0011\n\u2212 Pi\u22121\n\r\r\r\n1\n!\n.\n(2)\nHere, Mi = 1 implies that the distance of point pairs in the\noverlapping regions is used for estimating di.\nUsing the image and the corresponding depth map,\n[Ii, Di], we lift the pixels to 3D space. Here, we note that\nonly inpainted pixels of Ii are lifted to prevent points over-\nlapping and mitigate the inconsistency problem. The output\nof dreaming, \u02c6Pi, can be calculated as:\n\u02c6Pi = \u03d52\u21923 ([Ii, Di|Mi = 0] , K, Pi) ,\n(3)\nwhere [Ii, Di|Mi = 0] indicates the inpainted region in the\nRGBD image.\nAlignment. Compared to the way that trains a genera-\ntive model to generate both RGB and the depth map at once,\nsuch as RGBD2 [57], the depth map estimated by off-the-\nshelf depth estimation method is more accurate and gen-\neralizable to various situations since off-the-shelf methods\nare trained on large and various datasets. However, since\nD0, D1, ..., Di\u22121 is not considered when estimating Di, in-\nconsistency problem occurs when we add new points, \u02c6Pi.\nTo overcome the problem, we move the points of \u02c6Pi in 3D\nspace to attach the two point cloud (Pi\u22121 and \u02c6Pi) smoothly.\nSpecifically, we extract the region where the value of mask\nAlgorithm 1: Constructing point cloud\nInput: A single RGBD image [I0, D0]\nInput: Camera intrinsic K , extrinsics {Pi}N\ni=0\nOutput: Complete point cloud PN\n1 P0 \u2190 \u03d52\u21923 ([I0, D0], K, P0)\n2 for i \u2190 1 to N do\n3\n\u02c6Ii, Mi \u2190 \u03d53\u21922 (Pi\u22121, K, Pi)\n4\nIi \u2190 S\n\u0010\n\u02c6Ii, Mi\n\u0011\n, \u02c6Di \u2190 D (Ii)\n5\ndi \u2190 1\n6\nwhile not converged do\n7\n\u02dcPi \u2190 \u03d52\u21923\n\u0010h\nIi, di \u02c6Di\ni\n, K, Pi\n\u0011\n8\nLd \u2190\n1\n\u2225Mi=1\u2225\nP\nMi=1\n\r\r\r \u02dcPi \u2212 Pi\u22121\n\r\r\r\n1\n9\nCalculate \u2207dLd\n10\ndi \u2190 di \u2212 \u03b1\u2207dLd\n11\nend\n12\nDi \u2190 di \u02c6Di\n13\n\u02c6Pi \u2190 \u03d52\u21923 ([Ii, Di|Mi = 0] , K, Pi)\n14\nPi \u2190 Pi\u22121 \u222a W\n\u0010\n\u02c6Pi\n\u0011\n15 end\nchanges (|\u2207Mi| > 0) to find the corresponding points to\nthat region in both Pi\u22121 and \u02c6Pi. Then, we calculate the dis-\nplacement vector from \u02c6Pi to Pi\u22121. However, moving the\npoints in a naive way may distort the shape of the lifted\npoint cloud and make a misalignment between the point\ncloud and the inpainted image. We mitigate the issue by\ngiving the restrictions for moving the points and using the\ninterpolation algorithm to preserve the overall shape of the\npoints.\nFirst, we force each point in \u02c6Pi to move along the ray\nline from the camera center to the corresponding pixel. We\nfind the closest point to the corresponding point in Pi\u22121\nalong the ray line and report how much the depth changes\nare caused by the movement. Using the constraint, we pre-\nserve the contents of RGB image (Ii) although moving the\npoints in 3D space. Next, we assume that the depth does\nnot change at the opposite side of the mask boundary re-\ngion. Then, for the points that do not have their ground\ntruth counterparts, i.e. Mi = 0, we calculate for each pixel\nhow much the depth value should change using linear inter-\npolation. By interpolating smoothly, the mismatch among\nthe pixels caused by the drastic movement is alleviated. The\naligned points are combined with the original one:\nPi = Pi\u22121 \u222a W\n\u0010\n\u02c6Pi\n\u0011\n,\n(4)\nwhere we denote calculating movement and interpolation\nas W. We repeat the process N times to construct the final\npoint cloud, PN. By reprojection, PN provides high-quality\n4\n(ii) RGB\n(i) Text\n\uac00\ub85c 2.92\n\uc138\ub85c 2.19\nDepth map\n\uac00\ub85c \n\uc138\ub85c 0.8\n\uac00\ub85c 6\n\uc138\ub85c 4.5\nDepth\n\uac00\ub85c\n\uc138\ub85c 1.6\n14\n15\n7\n8\n5\n19\n7\n10\nThe playground in a residential area, \nwith abundant street trees, cars \nparked on the road, and single-family \nhomes surrounding the playground\n(a) input(I0)\n(b) I1\n(c) Iis\n(d) Generated 3D\nFigure 2. Intermediate images during point cloud generation and final 3D output between different inputs. We generate 3D scene\nfrom different input types (text and RGB image). The input image in the first row is generated image using Stable diffusion. Our model is\ncapable of generating consistent images high-quality 3D scene regardless of input type.\nand multi-view consistent images. The whole process of\nconstructing PN from [I0, D0], K, and {Pi}N\ni=0 is written\nin Algorithm 1.\n3.2. Rendering with Gaussian Splatting\nAfter the point cloud is created, we train 3D Gaussian splat-\nting model [21] using the point cloud and the projected im-\nages. The center of Gaussian splatting points are initialized\nby the input point cloud, and the volume and the position of\neach point are changed by the supervision of input ground\ntruth projected images. We use the generated point cloud\n(PN) as the initial SfM points. Initialization with PN will\nboost up the convergence of the network and encourage the\nnetwork to focus on generating the details of the representa-\ntion. For the images to train the model, we use additional M\nimages as well as (N + 1) images for generating the point\ncloud, since the initial (N + 1) images are not sufficient to\ntrain the network for generating the plausible output. The\nM new images and the masks are generated by reproject-\ning from the point cloud PN by a new camera sequence of\nlength M, denoted as PN+1, ..., PN+M.\nIi, Mi = \u03d53\u21922 (PN, K, Pi) , i = M + 1, ..., M + N. (5)\nWe note that we do not inpaint Ii when optimizing Gaussian\nsplats. Instead, when calculating the loss function, we only\nconsider the valid image region where the mask value is 1.\nIt prevents the model from learning the wrong details of the\nreprojected images. Since each point is represented as a\nGaussian distribution, the missing pixels when training the\nmodel are naturally filled, and the rasterized image after the\ntraining becomes plausible.\n4. Experiments\n4.1. Experiment settings\nDatasets.\nSince LucidDreamer is optimized for every in-\nput, we do not need training dataset to train the model. For\nthe text input, we randomly generate several text prompts\nrelevant to scene images to generate the first image using\nStable Diffusion. We use real or generated high-quality im-\nages for the RGB input. For the case of RGBD inputs, we\nuse ScanNet [9] and NYUdepth [45] since the two datasets\nhave ground truth depth maps.\nImplementation details.\nThe modules we used to con-\nstruct LucidDreamer can be either trained using manual de-\nsign or brought from off-the-shelf models.\nWe use pre-\ntrained large-scale off-the-shelf models to compose the\nwhole network to maximize the generalization capability\nof the network.\nSpecifically, we adopt Stable Diffusion\nmodel [39] to inpaint the masked image. We use the same\ntext prompt input for the Stable Diffusion if the first image\nis generated from the text. If the input format is a RGB(D)\nimage without text, we use LAVIS [23] to generate the cap-\ntion according to the image and place it in the diffusion\n5\n(a) RGB condition\n\u201cA Cat on the street near a \nwhite house with stairs, trees \nnearby, and gray blocks\u201d\n\u201cA number of \ufb02ower buckets, \nsmall bushes and many \npebbles on the ground\u201d\n\u201cA Cat on the street near a white house with \nstairs, trees nearby, and gray blocks\u201d\n\u201cA grass garden with bushes and \ufb02owers, \nwhite house with white stairs, table on the grass\u201d\n(b) Generation process with different text prompt\n(c) Generated 3D\nFigure 3. Intermediate images during point cloud generation and final 3D output for different text prompt. We put the different text\nprompt while having same initial image (I0) and compare the generation results.\ninpainting model to generate consistent content. For the\ncamera trajectory that we use to construct the point cloud\n({Pi}N\ni=0), we create several types of camera trajectory pre-\nsets in advance, and different types of trajectories were used\nfor different tasks.\n4.2. Experiment results\nWe demonstrate the superiority and high generalizability of\nLucidDreamer in many aspects. We strongly recommend\nthe readers to watch the video in the supplementary materi-\nals where we can entirely show the strength of our model.\nApplicability to various input domains and formats.\nLucidDreamer is capable of generating a consistent and\nhigh-quality 3D scene considering the input style. Figure 2\nshows the generated realistic images and the 3D scenes. At\nthe top row, we visualize a result of Text-to-3D. We depict\nan initial image generated from the given text and estimated\ndepth in (a). (b) and (c) present the plausible images and\ngeometry generated through our pipeline involving naviga-\ntion, dreaming, and alignment. We showcase an overview\nof the final 3D scene in (d). On the other hand, the bottom\nrow demonstrates an example result of RGB-to-3D We es-\ntimated depth from the given RGB and used it as an initial\ngeometry for the scene. Similar to the top row, we generated\nbelievable images and geometry, resulting in a high-quality\n3D scene.\nSince our model supports multiple inputs, it can generate\n3D scenes in various ways as illustrated in Figure 3. The top\nand middle rows depict outcomes generated by guarantee-\ning the inclusion of conditioned RGB during the creation of\nthe 3D scene. Despite different texts, the conditioned RGB\nis consistently present in the scene. On the other hand, the\nbottom row displays the outcome of altering the text condi-\ntion while generating the 3D scene. Through diverse com-\nbinations and alterations of conditions, our model facilitates\nthe creation of the desired 3D scene more effortlessly. We\nillustrate additional example scenes in Figure 5. Our model\nsuccessfully generates diverse 3D scenes with various styles\n(e.g. lego, anime) across different camera paths.\nComparison with RGBD2.\nWe qualitatively compare the\ngeneration results with RGBD2 [22] and illustrate the re-\nsult in Figure 4. For fairness of comparison, we compare\nthe results on three images with different domains: gener-\nated image, ScanNet, and NYUDepth. For the generated\nimage, the depth map estimated by Zoedepth [2] is consid-\nered a ground-truth depth map when processing RGBD2.\nFor ScanNet and NYUDepth, we use the ground truth depth\nmap for both RGBD2 and LucidDreamer when producing a\n6\n(iii) NYUDepth [45]\n(ii) ScanNet [9]\n(i) Generated\nOurs\nRGBD2 [22]\nOurs\nRGBD2 [22]\nOurs\nRGBD2 [22]\n(a) input(I0)\n(b) I1\n(c) Iis\n(d) Generated 3D\nFigure 4. Qualitative comparison with RGBD2 [22] on various image datasets. We compare LucidDreamer with RGBD2 starting from\nthe same input image while changing the datasets. The scene generated by LucidDreamer always shows higher quality than RGBD2, even\non ScanNet [9] which RGBD2 is trained on.\n7\nModels\nCLIP-\nCLIP-IQA [53]\nScore\u2191 [18]\nQuality\u2191\nColorful\u2191\nSharp\u2191\nRGBD2 [22]\n0.2035\n0.1279\n0.2081\n0.0126\nLucidDreamer\n0.2110\n0.6161\n0.8453\n0.5356\nTable 1.\nQuantitative comparison of generated scenes.\nWe\nquantitatively compare the results using CLIP-Score and CLIP-\nIQA with RGBD2. Our model shows better results on all metrics.\n(a) Generated video\n(b) 3D whole view\nFigure 5. 3D reconstruction results and short video on various\nstyles. This is a video figure that is best viewed by Adobe Reader.\n3D scene. For ScanNet, each scene consists of several im-\nages and the corresponding depth maps and camera views.\nWe randomly select one of the given image and depth map\npairs and use it as an initial RGBD input.\nIn Figure 4b, we\nobserve that RGBD2 generates (ScanNet-style) images with\nsimilar styles regardless of the input image. This remains\nconsistent not only in the initial image but also throughout\nthe following sequence as shown in Figure 4c. We believe\nthe issue arises due to insufficient training data and domain\nlimitations, highlighting the need for a model with suffi-\ncient generalization. In contrast, our approach generates\nhigh-quality 3D scenes with careful consideration to harmo-\nnize well with the input RGB. Moreover, LucidDreamer can\ngenerate scenes composed of high-resolution images while\nRGBD2 can only make 128 \u00d7 128-sized images, which is\ntoo small to use in real applications. We also document\nthe quantitative results evaluated on CLIP-Score [18] and\nCLIP-IQA [53] in Table 1. We confirm that our model in-\ncorporates input conditions well, resulting in the creation of\nhigh-quality 3D scenes.\nIters\nSource of\nMetrics\nSfM points\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\n1000\nCOLMAP\n23.15\n0.7246\n0.2910\nLucidDreamer\n32.59\n0.9672\n0.0272\n3000\nCOLMAP\n30.87\n0.9478\n0.0353\nLucidDreamer\n33.80\n0.9754\n0.0178\n7000\nCOLMAP\n32.52\n0.9687\n0.0208\nLucidDreamer\n34.24\n0.9781\n0.0164\nTable 2.\nReconstruction quality according to the source of\ninitial SfM points. We use the initial point cloud generated by\nCOLMAP [40, 41] and compare the reconstruction results. Our\nmodel consistently shows better reconstruction metrics.\n(a) Trained with mask\n(b) Trained without mask\nFigure 6. The effect of the mask during training. Training with\nvalid masks helps prevent artifacts at the boundaries of the scene.\nAblations on design choices.\nWe demonstrate ablation\nstudies on the design choices of LucidDreamer.\nIn ta-\nble 2, we compare the effect of COLMAP initialization\nand the point cloud initialization provided by our method\nwhen learning Gaussian splatting. LucidDreamer achieved\na high-quality scene within fewer iterations by offering a\nsubstantial amount of high-quality initialization points. Fig-\nure 6 illustrates the effectiveness of masking the image\nwhile learning Gaussian splatting.\nIt removes the black\nsplinter from the black background, resulting in visually\npleasing outcomes.\n5. Conclusion\nIn this work, we propose LucidDreamer, a novel pipeline\nfor domain-free 3D scene generation.\nBy fully exploit-\ning the power of large diffusion models, LucidDreamer is\ncapable of generating high-quality scenes without the re-\nstriction of the target scene domain. We first generate the\npoint cloud from the input image and repeat \u2018Dreaming\u2019 and\n\u2018Alignment\u2019 algorithms to generate the multi-view consis-\ntent high-qulaity image and harmoniously integrate them to\nthe existing point cloud in the 3D space. After the construc-\ntion is finished, the point cloud is converted to 3D Gaussian\nsplats to enhance the quality of the 3D scene. Extensive ex-\nperiments show that LucidDreamer can consistently gener-\nate high-quality and diverse 3D scenes in various situations.\n8\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas,\nand Leonidas Guibas. Learning representations and\ngenerative models for 3d point clouds.\nIn Interna-\ntional conference on machine learning, 2018. 3\n[2] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter\nWonka, and Matthias M\u00a8uller.\nZoedepth: Zero-shot\ntransfer by combining relative and metric depth. arXiv\npreprint arXiv:2302.12288, 2023. 3, 6\n[3] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun\nWu, and Gordon Wetzstein. pi-gan: Periodic implicit\ngenerative adversarial networks for 3d-aware image\nsynthesis. In CVPR, 2021. 3\n[4] Eric R Chan, Connor Z Lin, Matthew A Chan,\nKoki Nagano, Boxiao Pan, Shalini De Mello, Orazio\nGallo, Leonidas J Guibas, Jonathan Tremblay, Sameh\nKhamis, et al. Efficient geometry-aware 3d generative\nadversarial networks. In CVPR, 2022. 3\n[5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu,\nand Hao Su. Tensorf: Tensorial radiance fields. In\nECCV, 2022. 3\n[6] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian,\nZhuowen Tu, Lingjie Liu, and Hao Su. Single-stage\ndiffusion nerf: A unified approach to 3d generation\nand reconstruction. arXiv preprint arXiv:2304.06714,\n2023. 3\n[7] Zhiqin Chen, Thomas Funkhouser, Peter Hedman,\nand Andrea Tagliasacchi. Mobilenerf: Exploiting the\npolygon rasterization pipeline for efficient neural field\nrendering on mobile architectures. In CVPR, 2023. 3\n[8] Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja\nGiryes, and Daniel Cohen-Or. Set-the-scene: Global-\nlocal training for generating controllable nerf scenes.\narXiv preprint arXiv:2303.13450, 2023. 3\n[9] Angela Dai, Angel X. Chang, Manolis Savva, Maciej\nHalber, Thomas Funkhouser, and Matthias Nie\u00dfner.\nScannet: Richly-annotated 3d reconstructions of in-\ndoor scenes. In CVPR, 2017. 2, 5, 7\n[10] Boyang Deng, Kyle Genova, Soroosh Yazdani, Sofien\nBouaziz, Geoffrey Hinton, and Andrea Tagliasacchi.\nCvxnet: Learnable convex decomposition. In CVPR,\n2020. 2\n[11] Emilien Dupont, Hyunjik Kim, SM Eslami, Danilo\nRezende, and Dan Rosenbaum. From data to functa:\nYour data point is a function and you can treat it like\none. arXiv preprint arXiv:2201.12204, 2022. 2, 3\n[12] Rina Foygel and Mathias Drton. Extended bayesian\ninformation criteria for gaussian graphical models.\n2010. 2\n[13] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali\nDekel. Scenescape: Text-driven consistent scene gen-\neration. arXiv preprint arXiv:2302.01133, 2023. 3\n[14] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qin-\nhong Chen, Benjamin Recht, and Angjoo Kanazawa.\nPlenoxels: Radiance fields without neural networks.\nIn CVPR, 2022. 3\n[15] Stephan J Garbin, Marek Kowalski, Matthew Johnson,\nJamie Shotton, and Julien Valentin. Fastnerf: High-\nfidelity neural rendering at 200fps. In ICCV, 2021. 3\n[16] Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron\nSarna, William T Freeman, and Thomas Funkhouser.\nLearning shape templates with structured implicit\nfunctions. In ICCV, 2019. 2\n[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial\nnets. NIPS, 2014. 3\n[18] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ro-\nnan Le Bras, and Yejin Choi. CLIPScore: a reference-\nfree evaluation metric for image captioning.\nIn\nEMNLP, 2021. 8\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising\ndiffusion probabilistic models. NeurIPS, 2020. 3\n[20] Bernhard\nKerbl,\nGeorgios\nKopanas,\nThomas\nLeimk\u00a8uhler, and George Drettakis.\n3d gaussian\nsplatting for real-time radiance field rendering. ACM\nToG, 2023. 2, 3\n[21] Bernhard\nKerbl,\nGeorgios\nKopanas,\nThomas\nLeimk\u00a8uhler, and George Drettakis.\n3d gaussian\nsplatting for real-time radiance field rendering. ACM\nToG, 2023. 5\n[22] Jiabao Lei, Jiapeng Tang, and Kui Jia. Rgbd2: Gener-\native scene synthesis via incremental view inpainting\nusing rgbd diffusion models. In CVPR, 2023. 3, 6, 7,\n8\n[23] Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Sil-\nvio Savarese, and Steven C.H. Hoi. LAVIS: A one-\nstop library for language-vision intelligence. In ACL,\n2023. 5\n[24] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng\nChua, and Christian Theobalt.\nNeural sparse voxel\nfields. NIPS, 2020. 3\n[25] Shitong Luo and Wei Hu. Diffusion probabilistic mod-\nels for 3d point cloud generation. In CVPR, 2021. 2,\n3\n[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng.\nNerf: Representing scenes as neural radiance fields for\nview synthesis. Communications of the ACM, 2021. 2\n[27] Thomas M\u00a8uller, Fabrice Rousselle, Jan Nov\u00b4ak, and\nAlexander Keller. Real-time neural radiance caching\nfor path tracing.\narXiv preprint arXiv:2106.12372,\n2021. 3\n[28] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and\n9\nAlexander Keller. Instant neural graphics primitives\nwith a multiresolution hash encoding. TOG, 2022. 3\n[29] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\nRichardt, and Yong-Liang Yang. Hologan: Unsuper-\nvised learning of 3d representations from natural im-\nages. In ICCV, 2019. 3\n[30] Thu H Nguyen-Phuoc, Christian Richardt, Long Mai,\nYongliang Yang, and Niloy Mitra. Blockgan: Learn-\ning 3d object-aware scene representations from unla-\nbelled images. NeurIPS, 2020. 3\n[31] Michael Niemeyer and Andreas Geiger. Giraffe: Rep-\nresenting scenes as compositional generative neural\nfeature fields. In CVPR, pages 11453\u201311464, 2021.\n3\n[32] Jeong Joon Park, Peter Florence, Julian Straub,\nRichard Newcombe, and Steven Lovegrove. Deepsdf:\nLearning continuous signed distance functions for\nshape representation. In CVPR, 2019. 2\n[33] Despoina Paschalidou, Ali Osman Ulusoy, and An-\ndreas Geiger. Superquadrics revisited: Learning 3d\nshape parsing beyond cuboids. In CVPR, 2019. 2\n[34] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben\nMildenhall.\nDreamfusion: Text-to-3d using 2d dif-\nfusion. arXiv preprint arXiv:2209.14988, 2022. 3\n[35] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian\nRen, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee,\nIvan Skorokhodov, Peter Wonka, Sergey Tulyakov,\net al. Magic123: One image to high-quality 3d ob-\nject generation using both 2d and 3d diffusion priors.\narXiv preprint arXiv:2306.17843, 2023. 3\n[36] Alec Radford, Luke Metz, and Soumith Chintala. Un-\nsupervised representation learning with deep convolu-\ntional generative adversarial networks. arXiv preprint\narXiv:1511.06434, 2015. 3\n[37] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In\nICML, 2021. 3\n[38] Christian Reiser, Songyou Peng, Yiyi Liao, and An-\ndreas Geiger. Kilonerf: Speeding up neural radiance\nfields with thousands of tiny mlps. In ICCV, 2021. 3\n[39] Robin\nRombach,\nAndreas\nBlattmann,\nDominik\nLorenz, Patrick Esser, and Bj\u00a8orn Ommer.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels. In CVPR, 2022. 2, 3, 5\n[40] Johannes Lutz Sch\u00a8onberger and Jan-Michael Frahm.\nStructure-from-motion revisited. In CVPR, 2016. 8\n[41] Johannes Lutz Sch\u00a8onberger, Enliang Zheng, Marc\nPollefeys, and Jan-Michael Frahm. Pixelwise view se-\nlection for unstructured multi-view stereo. In ECCV,\n2016. 8\n[42] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and An-\ndreas Geiger. Graf: Generative radiance fields for 3d-\naware image synthesis. In NIPS, 2020. 3\n[43] Dong Wook Shu, Sung Woo Park, and Junseok Kwon.\n3d point cloud generative adversarial network based\non tree structured graph convolutions. In ICCV, 2019.\n3\n[44] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary\nAnkner, Jiajun Wu, and Gordon Wetzstein. 3d neu-\nral field generation using triplane diffusion. In CVPR,\n2023. 3\n[45] Nathan Silberman, Derek Hoiem, Pushmeet Kohli,\nand Rob Fergus. Indoor segmentation and support in-\nference from rgbd images. In ECCV, 2012. 2, 5, 7\n[46] Vincent Sitzmann, Julien Martel, Alexander Bergman,\nDavid Lindell, and Gordon Wetzstein. Implicit neu-\nral representations with periodic activation functions.\nNeurIPS, 2020. 2\n[47] Jiaming Song, Chenlin Meng, and Stefano Ermon.\nDenoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020. 3\n[48] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct\nvoxel grid optimization: Super-fast convergence for\nradiance fields reconstruction. In CVPR, 2022. 3\n[49] Towaki Takikawa,\nJoey Litalien,\nKangxue Yin,\nKarsten Kreis, Charles Loop, Derek Nowrouzezahrai,\nAlec Jacobson, Morgan McGuire, and Sanja Fidler.\nNeural geometric level of detail: Real-time rendering\nwith implicit 3d shapes. In CVPR, 2021. 2\n[50] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu,\nand Gang Zeng.\nDreamgaussian: Generative gaus-\nsian splatting for efficient 3d content creation. arXiv\npreprint arXiv:2309.16653, 2023. 3\n[51] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng\nWang, and Yasutaka Furukawa.\nMvdiffusion: En-\nabling holistic multi-view image generation with\ncorrespondence-aware diffusion.\narXiv preprint\narXiv:2307.01097, 2023. 2\n[52] Shubham Tulsiani, Hao Su, Leonidas J Guibas,\nAlexei A Efros, and Jitendra Malik. Learning shape\nabstractions by assembling volumetric primitives. In\nCVPR, 2017. 2\n[53] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy.\nExploring clip for assessing the look and feel of im-\nages. In AAAI, 2023. 8\n[54] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Free-\nman, and Josh Tenenbaum.\nLearning a probabilis-\ntic latent space of object shapes via 3d generative-\nadversarial modeling. In NIPS, 2016. 3\n[55] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi,\nZhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann.\nPoint-nerf: Point-based neural radiance fields.\nIn\nCVPR, 2022. 3\n10\n[56] Mohsen Yavartanoo, Jaeyoung Chung, Reyhaneh Ne-\nshatavar, and Kyoung Mu Lee. 3dias: 3d shape recon-\nstruction with implicit algebraic surfaces. In ICCV,\n2021. 2\n[57] Tackgeun You, Mijeong Kim, Jungtaek Kim, and\nBohyung Han.\nGenerative neural fields by mix-\ntures of neural implicit functions.\narXiv preprint\narXiv:2310.19464, 2023. 3, 4\n[58] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren\nNg, and Angjoo Kanazawa. Plenoctrees for real-time\nrendering of neural radiance fields. In ICCV, 2021. 3\n[59] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan\nGojcic, Or Litany, Sanja Fidler, and Karsten Kreis.\nLion: Latent point diffusion models for 3d shape gen-\neration. arXiv preprint arXiv:2210.06978, 2022. 3\n[60] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape gen-\neration and completion through point-voxel diffusion.\nIn ICCV, 2021. 2, 3\n11\n"
  },
  {
    "title": "Diffusion Model Alignment Using Direct Preference Optimization",
    "link": "https://arxiv.org/pdf/2311.12908.pdf",
    "upvote": "43",
    "text": "Diffusion Model Alignment Using Direct Preference Optimization\nBram Wallace1\nMeihua Dang2\nRafael Rafailov2\nLinqi Zhou2\nAaron Lou2\nSenthil Purushwalkam1\nStefano Ermon2\nCaiming Xiong1\nShafiq Joty1\nNikhil Naik1\n1Salesforce AI, 2Stanford University\n{b.wallace,spurushwalkam,cxiong,sjoty,nnaik}@salesforce.com\n{mhdang,rafailov,lzhou907,aaronlou}@stanford.edu {ermon}@cs.stanford.edu\nAbstract\nLarge language models (LLMs) are fine-tuned using hu-\nman comparison data with Reinforcement Learning from\nHuman Feedback (RLHF) methods to make them better\naligned with users\u2019 preferences.\nIn contrast to LLMs,\nhuman preference learning has not been widely explored\nin text-to-image diffusion models; the best existing ap-\nproach is to fine-tune a pretrained model using carefully\ncurated high quality images and captions to improve vi-\nsual appeal and text alignment.\nWe propose Diffusion-\nDPO, a method to align diffusion models to human pref-\nerences by directly optimizing on human comparison data.\nDiffusion-DPO is adapted from the recently developed Di-\nrect Preference Optimization (DPO) [33], a simpler al-\nternative to RLHF which directly optimizes a policy that\nbest satisfies human preferences under a classification ob-\njective. We re-formulate DPO to account for a diffusion\nmodel notion of likelihood, utilizing the evidence lower\nbound to derive a differentiable objective. Using the Pick-\na-Pic dataset of 851K crowdsourced pairwise preferences,\nwe fine-tune the base model of the state-of-the-art Stable\nDiffusion XL (SDXL)-1.0 model with Diffusion-DPO. Our\nfine-tuned base model significantly outperforms both base\nSDXL-1.0 and the larger SDXL-1.0 model consisting of an\nadditional refinement model in human evaluation, improv-\ning visual appeal and prompt alignment. We also develop\na variant that uses AI feedback and has comparable perfor-\nmance to training on human preferences, opening the door\nfor scaling of diffusion model alignment methods.\n1. Introduction\nText-to-image diffusion models have been the state-of-the-\nart in image generation for the past few years. They are typ-\nically trained in a single stage, using web-scale datasets of\ntext-image pairs by applying the diffusion objective. This\nstands in contrast to the state-of-the-art training method-\nology for Large Language Models (LLMs). The best per-\nforming LLMs [28, 48] are trained in two stages. In the\nfirst (\u201cpretraining\u201d) stage, they are trained on large web-\nscale data.\nIn the second (\u201calignment\u201d) stage, they are\nfine-tuned to make them better aligned with human pref-\nerences.\nAlignment is typically performed using super-\nvised fine-tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) using preference data.\nLLMs\ntrained with this two-stage process have set the state-of-the-\nart in language generation tasks and have been deployed in\ncommercial applications such as ChatGPT and Bard.\nDespite the success of the LLM alignment process,\nmost text-to-image diffusion training pipelines do not in-\ncorporate learning from human preferences. Several mod-\nels [9, 35, 36], perform two-stage training, where large-\nscale pretraining is followed by fine-tuning on a high-\nquality text-image pair dataset to strategically bias the gen-\nerative process. This approach is much less powerful and\nflexible than the final-stage alignment methods of LLMs.\nRecent approaches [6, 7, 11, 31] develop more advanced\nways to tailor diffusion models to human preferences, but\nnone have demonstrated the ability to stably generalize to a\nfully open-vocabulary setting across an array of feedback.\nRL-based methods [6, 11] are highly effective for limited\nprompt sets, but their efficacy decreases as the vocabulary\nexpands. Other methods [7, 31] use the pixel-level gradients\nfrom reward models on generations to tune diffusion mod-\nels, but suffer from mode collapse and can only be trained\non a relatively narrow set of feedback types.\nWe address this gap in diffusion model alignment for the\nfirst time, developing a method to directly optimize diffu-\nsion models on human preference data. We generalize Di-\nrect Preference Optimization (DPO) [33], where a gener-\native model is trained on paired human preference data to\nimplicitly estimate a reward model. We define a notion of\ndata likelihood under a diffusion model in a novel formula-\ntion and derive a simple but effective loss resulting in stable\narXiv:2311.12908v1  [cs.CV]  21 Nov 2023\nFigure 1. We develop Diffusion-DPO, a method based on Direct Preference Optimization (DPO) [33] for aligning diffusion models to\nhuman preferences by directly optimizing the model on user feedback data. After fine-tuning on the state-of-the-art SDXL-1.0 model, our\nmethod produces images with exceptionally high visual appeal and text alignment, samples above.\nand efficient preference training, dubbed Diffusion-DPO.\nWe connect this formulation to a multi-step RL approach\nin the same setting as existing work [6, 11].\nWe demonstrate the efficacy of Diffusion-DPO by fine-\ntuning state-of-the-art text-to-image diffusion models, such\nas Stable Diffusion XL (SDXL)-1.0 [30].\nHuman eval-\nuators prefer DPO-tuned SDXL images over the SDXL-\n(base + refinement) model 69% of the time on the Par-\ntiPrompts dataset, which represents the state-of-the-art in\ntext-to-image models as measured by human preference.\nExample generations shown in Fig. 1. Finally, we show\nthat learning from AI feedback (instead of human prefer-\nences) using the Diffusion-DPO objective is also effective,\na setting where previous works have been unsuccessful [7].\nIn sum, we introduce a novel paradigm of learning from\nhuman preferences for diffusion models and present the re-\nsulting state-of-the-art model.\n2. Related Work\nAligning Large Language Models\nLLMs are typically\naligned to human preferences using supervised fine-tuning\non demonstration data, followed by RLHF. RLHF con-\nsists of training a reward function from comparison data on\nmodel outputs to represent human preferences and then us-\ning reinforcement learning to align the policy model. Prior\nwork [4, 26, 29, 47] has used policy-gradient methods [27,\n38] to this end. These methods are successful, but expen-\nsive and require extensive hyperparameter tuning [34, 59],\nand can be prone to reward hacking [10, 12, 41]. Alter-\nnative approaches sample base model answers and select\nbased on predicted rewards [3, 5, 14] to use for super-\nvised training [2, 16, 50]. Methods that fine-tune the pol-\nicy model directly on feedback data [1, 10], or utilize a\nranking loss on preference data to directly train the pol-\nicy model [33, 49, 57, 58] have emerged. The latter set of\nmethods match RLHF in performance. We build on these\nfine-tuning methods in this work, specifically, direct pref-\nerence optimization [33] (DPO). Finally, learning from AI\nfeedback, using pretrained reward models, is promising for\nefficient scaling of alignment [4, 22].\nAligning Diffusion Models\nAlignment of diffusion mod-\nels to human preferences has so far been much less explored\nthan in the case of LLMs. Multiple approaches [30, 36]\nfine-tune on datasets scored as highly visually appealing by\nan aesthetics classifier [37], to bias the model to visually ap-\npealing generations. Emu [9] finetunes a pretrained model\nusing a small, curated image dataset of high quality pho-\ntographs with manually written detailed captions to improve\nvisual appeal and text alignment. Other methods [15, 39] re-\ncaption existing web-scraped image datasets to improve text\nfidelity. Caption-aware human preference scoring models\nare trained on generation preference datasets [21, 52, 55],\nbut the impact of these reward models to the generative\nspace has been limited. DOODL [51] introduces the task\nof aesthetically improving a single generation iteratively at\ninference time. DRAFT [7] and AlignProp [31], incorpo-\nrate a similar approach into training: tuning the generative\nmodel to directly increase the reward of generated images.\nThese methods perform well for simple visual appeal cri-\nteria, but lack stability and do not work on more nuanced\nrewards such as text-image alignment from a CLIP model.\nDPOK and DDPO [6, 11] are RL-based approaches to sim-\nilarly maximize the scored reward (with distributional con-\nstraints) over a relatively limited vocabulary set; the perfor-\nmance of these methods degrades as the number of train/test\nprompts increases. Diffusion-DPO is unique among align-\nment approaches in effectively increasing measured human\nappeal across an open vocabulary (DPOK, DDPO), with-\nout increased inference time (DOODL) while maintaining\ndistributional guarantees and improving generic text-image\nalignment in addition to visual appeal (DRAFT, AlignProp).\n(see Tab. 1, further discussion in Supp. S1).\nEqual\nOpen\nInference\nDivergence\nMethods\nVocab.\nCost\nControl\nDPOK[11]\n\u2717\n\u2713\n\u2713\nDDPO[6]\n\u2717\n\u2713\n\u2717\nDOODL[51]\n\u2713\n\u2717\n\u2717\nDRaFT[7],AlignProp[31]\n\u2713\n\u2713\n\u2717\nDiffusion-DPO (ours)\n\u2713\n\u2713\n\u2713\nTable 1. Method class comparison. Existing methods fail in one\nor more of: Generalizing to an open vocabulary, maintaining the\nsame inference complexity, avoiding mode collapse/providing dis-\ntributional guarantees. Diffusion-DPO addresses these issues.\n3. Background\n3.1. Diffusion Models\nGiven samples from a data distribution q(x0), noise\nscheduling function \u03b1t and \u03c3t (as defined in [36]), denois-\ning diffusion models [17, 42, 46] are generative models\np\u03b8(x0) which have a discrete-time reverse process with a\nMarkov structure p\u03b8(x0:T ) = QT\nt=1 p\u03b8(xt\u22121|xt) where\np\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt), \u03c32\nt|t\u22121\n\u03c32\nt\u22121\n\u03c32\nt\nI).\n(1)\nTraining is performed by minimizing the evidence lower\nbound (ELBO) associated with this model [20, 45]:\nLDM = Ex0,\u03f5,t,xt\n\u0002\n\u03c9(\u03bbt)\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t)\u22252\n2\n\u0003\n,\n(2)\nwith \u03f5 \u223c N(0, I), t \u223c U(0, T), xt\n\u223c q(xt|x0) =\nN(xt; \u03b1tx0, \u03c32\nt I). \u03bbt = \u03b12\nt /\u03c32\nt is a signal-to-noise ra-\ntio [20], \u03c9(\u03bbt) is a pre-specified weighting function (typ-\nically chosen to be constant [17, 44]).\n3.2. Direct Preference Optimization\nOur approach is an adaption of Direct Preference Optimiza-\ntion (DPO) [33], an effective approach for learning from\nhuman preference for language models. Abusing notation,\nwe also use x0 as random variables for language.\nReward Modeling\nEstimating human partiality to a gen-\neration x0 given conditioning c, is difficult as we do not\nhave access to the latent reward model r(c, x0). In our set-\nting, we assume access only to ranked pairs generated from\nsome conditioning xw\n0 \u227b xl\n0|c, where xw\n0 and xl\n0 denot-\ning the \u201cwinning\u201d and \u201closing\u201d samples. The Bradley-Terry\n(BT) model stipulates to write human preferences as:\npBT(xw\n0 \u227b xl\n0|c) = \u03c3(r(c, xw\n0 ) \u2212 r(c, xl\n0))\n(3)\nwhere \u03c3 is the sigmoid function. r(c, x0) can be parame-\nterized by a neural network \u03d5 and estimated via maximum\nlikelihood training for binary classification:\nLBT(\u03d5) = \u2212Ec,xw\n0 ,xl\n0\n\u0002\nlog \u03c3\n\u0000r\u03d5(c, xw\n0 ) \u2212 r\u03d5(c, xl\n0)\n\u0001\u0003\n(4)\nwhere prompt c and data pairs xw\n0 , xl\n0 are from a static\ndataset with human-annotated labels.\nRLHF\nRLHF aims to optimize a conditional distribution\np\u03b8(x0|c) (conditioning c \u223c Dc) such that the latent reward\nmodel r(c, x0) defined on it is maximized, while regulariz-\ning the KL-divergence from a reference distribution pref\nmax\np\u03b8 Ec\u223cDc,x0\u223cp\u03b8(x0|c) [r(c, x0)]\n\u2212 \u03b2DKL [p\u03b8(x0|c)\u2225pref(x0|c)]\n(5)\nwhere the hyperparameter \u03b2 controls regularization.\nDPO Objective\nIn Eq. (5) from [33], the unique global\noptimal solution p\u2217\n\u03b8 takes the form:\np\u2217\n\u03b8(x0|c) = pref(x0|c) exp (r(c, x0)/\u03b2) /Z(c)\n(6)\nwhere Z(c) = P\nx0 pref(x0|c) exp (r(c, x0)/\u03b2) is the par-\ntition function. Hence, the reward function is rewritten as\nr(c, x0) = \u03b2 log p\u2217\n\u03b8(x0|c)\npref(x0|c) + \u03b2 log Z(c)\n(7)\nUsing Eq. (4), the reward objective becomes:\nLDPO(\u03b8)=\u2212Ec,xw\n0 ,xl\n0\n\"\nlog \u03c3\n \n\u03b2 log p\u03b8(xw\n0 |c)\npref(xw\n0 |c) \u2212 \u03b2 log p\u03b8(xl\n0|c)\npref(xl\n0|c)\n!#\n(8)\nBy this reparameterization, instead of optimizing the reward\nfunction r\u03d5 and then performing RL, [33] directly optimizes\nthe optimal conditional distribution p\u03b8(x0|c).\n4. DPO for Diffusion Models\nIn adapting DPO to diffusion models, we consider a setting\nwhere we have a fixed dataset D = {(c, xw\n0 , xl\n0)} where\neach example contains a prompt c and a pairs of images\ngenerated from a reference model pref with human label\nxw\n0 \u227b xl\n0. We aim to learn a new model p\u03b8 which is aligned\nto the human preferences, with preferred generations to pref.\nThe primary challenge we face is that the parameterized dis-\ntribution p\u03b8(x0|c) is not tractable, as it needs to marginalize\nout all possible diffusion paths (x1, ...xT ) which lead to x0.\nTo overcome this challenge, we utilize the evidence lower\nbound (ELBO). Here, we introduce latents x1:T and define\nR(c, x0:T ) as the reward on the whole chain, such that we\ncan define r(c, x0) as\nr(c, x0) = Ep\u03b8(x1:T |x0,c) [R(c, x0:T )] .\n(9)\nAs for the KL-regularization term in Eq. (5), following\nprior work [17, 42], we can instead minimize its upper\nbound joint KL-divergence DKL [p\u03b8(x0:T |c)\u2225pref(x0:T |c)].\nPlugging this KL-divergence bound and the definition of\nr(c, x0) (Eq. (9)) back to Eq. (5), we have the objective\nmax\np\u03b8 Ec\u223cDc,x0:T \u223cp\u03b8(x0:T |c) [r(c, x0)]\n\u2212 \u03b2DKL [p\u03b8(x0:T |c)\u2225pref(x0:T |c)] .\n(10)\nThis objective has a parallel formulation as Eq. (5) but de-\nfined on path x0:T . It aims to maximize the reward for re-\nverse process p\u03b8(x0:T ), while matching the distribution of\nthe original reference reverse process. Paralleling Eqs. (6)\nto (8), this objective can be optimized directly through the\nconditional distribution p\u03b8(x0:T ) via objective:\nLDPO-Diffusion(\u03b8) = \u2212E(xw\n0 ,xl\n0)\u223cD log\u03c3\n\u0012\n\u03b2Exw\n1:T \u223cp\u03b8(xw\n1:T |xw\n0 )\nxl\n1:T \u223cp\u03b8(xl\n1:T |xl\n0)\n\u0014\nlog p\u03b8(xw\n0:T )\npref(xw\n0:T ) \u2212log p\u03b8(xl\n0:T )\npref(xl\n0:T )\n\u0015\u0013\n(11)\nWe omit c for compactness (details included in Supp. S2).\nTo optimize Eq. (11), we must sample x1:T \u223cp\u03b8(x1:T |x0).\nDespite the fact that p\u03b8 contains trainable parameters, this\nsampling procedure is both (1) inefficient as T is usually\nlarge (T = 1000), and (2) intractable since p\u03b8(x1:T ) rep-\nresents the reverse process parameterization p\u03b8(x1:T ) =\np\u03b8(xT ) QT\nt=1 p\u03b8(xt\u22121|xt). We solve these two issues next.\nFrom Eq. (11), we substitute the reverse decompositions\nfor p\u03b8 and pref, and utilize Jensen\u2019s inequality and the con-\nvexity of function \u2212log \u03c3 to push the expectation outside.\nWith some simplification, we get the following bound\nLDPO-Diffusion(\u03b8) \u2264 \u2212E(xw\n0 ,xl\n0)\u223cD,t\u223cU(0,T ),\nxw\nt\u22121,t\u223cp\u03b8(xw\nt\u22121,t|xw\n0 ),\nxl\nt\u22121,t\u223cp\u03b8(xl\nt\u22121,t|xl\n0)\nlog \u03c3\n\u0012\n\u03b2T log p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212\u03b2T log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xl\nt)\n\u0013\n(12)\nEfficient training via gradient descent is now possible.\nHowever, sampling from reverse joint p\u03b8(xt\u22121, xt|x0, c)\nis still intractable and r of Eq. (9) has an expectation\nover p\u03b8(x1:T |x0). So we approximate the reverse process\np\u03b8(x1:T |x0) with the forward q(x1:T |x0) (an alternative\nscheme in Supp. S2). With some algebra, this yields:\nL(\u03b8) = \u2212E(xw\n0 ,xl\n0)\u223cD,t\u223cU(0,T ),xw\nt \u223cq(xw\nt |xw\n0 ),xl\nt\u223cq(xl\nt|xl\n0)\nlog \u03c3(\u2212\u03b2T(\n+ DKL(q(xw\nt\u22121|xw\n0,t)\u2225p\u03b8(xw\nt\u22121|xw\nt ))\n\u2212 DKL(q(xw\nt\u22121|xw\n0,t)\u2225pref(xw\nt\u22121|xw\nt ))\n\u2212 DKL(q(xl\nt\u22121|xl\n0,t)\u2225p\u03b8(xl\nt\u22121|xl\nt))\n+ DKL(q(xl\nt\u22121|xl\n0,t)\u2225pref(xl\nt\u22121|xl\nt))).\n(13)\nUsing Eq. (1) and algebra, the above loss simplifies to:\nL(\u03b8) = \u2212E(xw\n0 ,xl\n0)\u223cD,t\u223cU(0,T ),xw\nt \u223cq(xw\nt |xw\n0 ),xl\nt\u223cq(xl\nt|xl\n0)\nlog \u03c3 (\u2212\u03b2T\u03c9(\u03bbt) (\n\u2225\u03f5w \u2212 \u03f5\u03b8(xw\nt , t)\u22252\n2 \u2212 \u2225\u03f5w \u2212 \u03f5ref(xw\nt , t)\u22252\n2\n\u2212\n\u0000\u2225\u03f5l \u2212 \u03f5\u03b8(xl\nt, t)\u22252\n2 \u2212 \u2225\u03f5l \u2212 \u03f5ref(xl\nt, t)\u22252\n2\n\u0001\u0001\u0001\n(14)\nwhere x\u2217\nt = \u03b1tx\u2217\n0 + \u03c3t\u03f5\u2217, \u03f5\u2217 \u223c N(0, I) is a draw from\nq(x\u2217\nt |x\u2217\n0) (Eq. (2)). \u03bbt = \u03b12\nt /\u03c32\nt is the signal-to-noise ratio,\nxw\n0  Error Change.\n0.4\n0.0\n0.4\nxl\n0 Error Change.\n||\u03f5l \u2212 \u03f5\u03b8(xl\nt, t)||2\n2 \u2212 ||\u03f5l \u2212 \u03f5ref(xl\nt, t)||2\n2\n0.4\n0.0\n0.4\nFigure 2. Loss surface visualization. Loss can be decreased by\nimproving at denoising xw\n0 and worsening for xl\n0. A larger \u03b2 in-\ncreases surface curvature.\n\u03c9(\u03bbt) a weighting function (constant in practice [17, 20]).\nWe factor the constant T into \u03b2. This loss encourages \u03f5\u03b8\nto improve more at denoising xw\nt than xl\nt, visualization in\nFig. 2. We also derive Eq. (14) as a multi-step RL approach\nin the same setting as DDPO and DPOK [6, 11] (Supp. S3)\nbut as an off-policy algorithm, which justifies our sampling\nchoice in Eq. 13. A noisy preference model perspective\nyields the same objective (Supp. S4).\n5. Experiments\n5.1. Setting\nModels and Dataset:\nWe demonstrate the efficacy of\nDiffusion-DPO across a range of experiments.\nWe use\nthe objective from Eq. (14) to fine-tune Stable Diffusion\n1.5 (SD1.5) [36] and the state-of-the-art open-source model\nStable Diffusion XL-1.0 (SDXL) [30] base model.\nWe\ntrain on the Pick-a-Pic [21] dataset, which consists of pair-\nwise preferences for images generated by SDXL-beta and\nDreamlike, a fine-tuned version of SD1.5. The prompts and\npreferences were collected from users of the Pick-a-Pic web\napplication (see [21] for details). We use the larger Pick-a-\nPic v2 dataset. After excluding the \u223c12% of pairs with ties,\nwe end up with 851,293 pairs, with 58,960 unique prompts.\nHyperparameters\nWe use AdamW [24] for SD1.5 exper-\niments, and Adafactor [40] for SDXL to save memory. An\neffective batch size of 2048 (pairs) is used; training on 16\nNVIDIA A100 GPUs with a local batch size of 1 pair and\ngradient accumulation of 128 steps. We train at fixed square\nresolutions. A learning rate of 2000\n\u03b2 2.048\u00b710\u22128 is used with\n25% linear warmup. The inverse scaling is motivated by the\nnorm of the DPO objective gradient being proportional to \u03b2\n(the divergence penalty parameter) [33]. For both SD1.5\nand SDXL, we find \u03b2 \u2208 [2000, 5000] to offer good per-\nformance (Supp. S5). We present main SD1.5 results with\n\u03b2 = 2000 and SDXL results with \u03b2 = 5000.\nEvaluation\nWe automatically validate checkpoints with\nthe 500 unique prompts of the Pick-a-Pic validation set:\nmeasuring median PickScore reward of generated images.\nPickscore [21] is a caption-aware scoring model trained on\nPick-a-Pic (v1) to estimate human-perceived image quality.\nFor final testing, we generate images using the baseline and\nDiffusion-DPO-tuned models conditioned on captions from\nthe Partiprompt [56] and HPSv2 [52] benchmarks (1632\nand 3200 captions respectively). While DDPO [6] is a re-\nlated method, we did not observe stable improvement when\ntraining from public implementations on Pick-a-Pic. We\nemploy labelers on Amazon Mechanical Turk to compare\ngenerations under three different criteria: Q1 General Pref-\nerence (Which image do you prefer given the prompt?), Q2\nVisual Appeal (prompt not considered) (Which image is\nmore visually appealing?) Q3 Prompt Alignment (Which\nimage better fits the text description?). Five responses are\ncollected for each comparison with majority vote (3+) being\nconsidered the collective decision.\n5.2. Primary Results: Aligning Diffusion Models\nFirst, we show that the outputs of the Diffusion-DPO-\nfinetuned SDXL model are significantly preferred over the\nbaseline SDXL-base model. In the Partiprompt evaluation\n(Fig. 3-top left), DPO-SDXL is preferred 70.0% of the time\nfor General Preference (Q1), and obtains a similar win-\nrate in assessments of both Visual Appeal (Q2) and Prompt\nAlignment (Q3). Evaluation on the HPS benchmark (Fig. 3-\ntop right) shows a similar trend, with a General Preference\nwin rate of 64.7%. We also score the DPO-SDXL HPSv2\ngenerations with the HPSv2 reward model, achieving an av-\nerage reward of 28.16, topping the leaderboard [53].\nWe display qualitative comparisons to SDXL-base in\nFig. 3 (bottom). Diffusion-DPO produces more appealing\nimagery, with vivid arrays of colors, dramatic lighting, good\ncomposition, and realistic people/animal anatomy. While\nall SDXL images satisfy the prompting criteria to some de-\ngree, the DPO generations appear superior, as confirmed by\nthe crowdsourced study. We do note that preferences are\nnot universal, and while the most common shared prefer-\nence is towards energetic and dramatic imagery, others may\nprefer quieter/subtler scenes. The area of personal or group\npreference tuning is an exciting area of future work.\nAfter this parameter-equal comparison with SDXL-base,\nwe compare SDXL-DPO to the complete SDXL pipeline,\nconsisting of the base model and the refinement model\n(Fig. 4). The refinement model is an image-to-image diffu-\nsion model that improves visual quality of generations, and\nis especially effective on detailed backgrounds and faces.\nIn our experiments with PartiPrompts and HPSv2, SDXL-\nDPO (3.5B parameters, SDXL-base architecture only),\nhandily beats the complete SDXL model (6.6B parameters).\nIn the General Preference question, it has a benchmark win\nA galaxy-colored figurine is floating \nover the sea at sunset, photorealistic\nA smiling beautiful sorceress \nwearing a high necked blue suit \nsurrounded by swirling rainbow \naurora, hyper-realistic, cinematic, \npost-production\nA monk in an orange robe by a round \nwindow in a spaceship in dramatic \nlighting\nConcept art of a mythical sky \nalligator with wings, nature \ndocumentary\nSDXL\nDPO-SDXL\nDPO-SDXL\nSDXL\nFigure 3. (Top) DPO-SDXL significantly outperforms SDXL in human evaluation. (L) PartiPrompts and (R) HPSv2 benchmark results\nacross three evaluation questions, majority vote of 5 labelers. (Bottom) Qualitative comparisons between SDXL and DPO-SDXL. DPO-\nSDXL demonstrates superior prompt following and realism. DPO-SDXL outputs are better aligned with human aesthetic preferences,\nfavoring high contrast, vivid colors, fine detail, and focused composition. They also capture fine-grained textual details more faithfully.\nrate of 69% and 64% respectively, comparable to its win\nrate over SDXL-base alone. This is explained by the abil-\nity of the DPO-tuned model (Fig. 4, bottom) to generate\nfine-grained details and its strong performance across dif-\nferent image categories. While the refinement model is es-\npecially good at improving the generation of human details,\nthe win rate of Diffusion-DPO on the People category in\nPartiprompt dataset over the base + refiner model is still an\nimpressive 67.2% (compared to 73.4% over the base).\n5.3. Image-to-Image Editing\nImage-to-image translation performance also improves af-\nter Diffusion-DPO tuning. We test DPO-SDXL on TEd-\nBench [19], a text-based image-editing benchmark of\n100 real image-text pairs, using SDEdit [25] with noise\nstrength 0.6. Labelers are shown the original image and\nSDXL/DPO-SDXL edits and asked \u201cWhich edit do you pre-\nfer given the text?\u201d DPO-SDXL is preferred 65% of the\ntime, SDXL 24%, with 11% draws. We show qualitative\nSDEdit results on color layouts (strength 0.98) in Fig. 5.\n5.4. Learning from AI Feedback\nIn LLMs, learning from AI feedback has emerged as a\nstrong alternative to learning from human preferences [22].\nDiffusion-DPO can also admit learning from AI feedback\nby directly ranking generated pairs into (yw, yl) using a\npretrained scoring network.\nWe use HPSv2 [52] for an\nalternate prompt-aware human preference estimate, CLIP\n(OpenCLIP ViT-H/14) [18, 32] for text-image alignment,\nAesthetic Predictor [37] for non-text-based visual appeal,\nand PickScore.\nWe run all experiments on SD 1.5 with\n\u03b2\n=\n5000 for 1000 steps.\nTraining on the prompt-\naware PickScore and HPS preference estimates increase the\nwin rate for both raw visual appeal and prompt alignment\n(Fig. 6). We note that PickScore feedback is interpretable\nas pseudo-labeling the Pick-a-Pic dataset\u2014a form of data\nSDXL Base\nSDXL Base + Refiner\nDPO-SDXL\nDPO-SDXL\nSDXL\nBase+ \nRefiner\nFigure 4. DPO-SDXL (base only) significantly outperforms the much larger SDXL-(base+refinement) model pipeline in human evaluations\non the PartiPrompts and HPS datasets. While the SDXL refinement model is used to touch up details from the output of SDXL-base, the\nability to generate high quality details has been naturally distilled into DPO-SDXL by human preference. Among other advantages,\nDPO-SDXL shows superior generation of anatomical features such as teeth, hands, and eyes. Prompts: close up headshot, steampunk\nmiddle-aged man, slick hair big grin in front of gigantic clocktower, pencil sketch / close up headshot, futuristic young woman with glasses,\nwild hair sly smile in front of gigantic UFO, dslr, sharp focus, dynamic composition / A man and woman using their cellphones, photograph\ncleaning [54, 60]. Training for Aesthetics and CLIP im-\nproves those capabilities more specifically, in the case of\nAesthetics at the expense of CLIP. The ability to train for\ntext-image alignment via CLIP is a noted improvement over\nprior work [7]. Moreover, training SD1.5 on the pseudo-\nlabeled PickScore dataset (\u03b2 = 5000, 2000 steps) outper-\nforms training on the raw labels. On the General Preference\nPartiprompt question, the win-rate of DPO increases from\n59.8% to 63.3%, indicating that learning from AI feedback\ncan be a promising direction for diffusion model alignment.\n5.5. Analysis\nImplicit Reward Model\nAs a consequence of the theo-\nretical framework, our DPO scheme implicitly learns a re-\nward model and can estimate the differences in rewards be-\ntween two images by taking an expectation over the inner\nterm of Eq. (14) (details in Supp. S4.1). We estimate over\n10 random t \u223c U{0, 1} Our learned models (DPO-SD1.5\nOriginal\nSDXL\nDPO-SDXL\nFigure 5. Diffusion-DPO generates more visually appealing im-\nages in the downstream image-to-image translation task. Com-\nparisons of using SDEdit [25] from color layouts. Prompts are \"A\nfantasy landscape, trending on artstation\" (top) , \"High-resolution\nrendering of a crowded colorful sci-fi city\" (bottom).\nPickScore\nTraining Ranker\nAutomated Win Rate vs. SD1.5\nHPS\nCLIP\nPickScore\nHPS\nCLIP\nAesthetics\n0.3\n0.5\n1.0\nAesthetics\nMetric Model:\nFigure 6. Automated head-to-head win rates under reward models\n(x labels, columns) for SD1.5 DPO-tuned on the \u201cpreferences\u201d\nof varied scoring networks (y labels, rows). Example: Tuning\non Aesthetics preferences (bottom row) achieves high Aesthetics\nscores but has lower text-image alignment as measured by CLIP.\nModel\nPS\nHPS CLIP Aes.\nDPO-SD1.5 DPO-SDXL\nAcc.\n64.2 59.3\n57.1\n51.4\n60.8\n72.0\nTable 2. Preference accuracy on the Pick-a-Pic (v2) validation set.\nThe v1-trained PickScore has seen the evaluated data.\nand DPO-SDXL) perform well at binary preference classi-\nfication (Tab. 2), with DPO-SDXL exceeding all existing\nrecognition models on this split. These results shows that\nthe implicit reward parameterization in the Diffusion-DPO\nobjective has comprable expressivity and generalization as\nthe classical reward modelling objective/architecture.\nTraining Data Quality\nFig. 7 shows that despite SDXL\nbeing superior to the training data (including the yw), as\nmeasured by Pickscore, DPO training improves its perfor-\ny : Dreamlike\nDreamlike\nDPO-Dreamlike*\nyw: Dreamlike\ny : SDXL-\nyw: SDXL-\nSDXL\nDPO-SDXL\n0.21\n0.22\n0.23\nMedian Pickscore\nFigure 7. Diffusion-DPO improves on the baseline Dreamlike and\nSDXL models, when finetuned on both in-distribution data (in case\nof Dreamlike) and out-of-distribution data (in case of SDXL). yl\nand yw denote the Pickscore of winning and losing samples.\nmance substantially. In this experiment, we confirm that\nDiffusion-DPO can improve on in-distribution preferences\nas well, by training (\u03b2 = 5k, 2000 steps) the Dreamlike\nmodel on a subset of the Pick-a-Pic dataset generated by\nthe Dreamlike model alone. This subset represents 15% of\nthe original dataset. Dreamlike-DPO improves on the base-\nline model, though the performance improvement is limited,\nperhaps because of the small size of the dataset.\nSupervised Fine-tuning (SFT)\nis beneficial in the LLM\nsetting as initial pretraining prior to preference training.\nTo evaluate SFT in our setting, we fine-tune models on\nthe preferred (x, yw) pairs of the Pick-a-Pic dataset. We\ntrain for the same length schedule as DPO using a learning\nrate of 1e \u2212 9 and observe convergence. While SFT im-\nproves vanilla SD1.5 (55.5% win rate over base model), any\namount of SFT deteriorates the performance of SDXL, even\nat lower learning rates. This contrast is attributable to the\nmuch higher quality of Pick-a-Pic generations vs. SD1.5,\nas they are obtained from SDXL-beta and Dreamlike. In\ncontrast, the SDXL-1.0 base model is superior to the Pick-\na-Pic dataset models. See Supp. S6 for further discussion.\n6. Conclusion\nIn this work, we introduce Diffusion-DPO: a method that\nenables diffusion models to directly learn from human feed-\nback in an open-vocabulary setting for the first time. We\nfine-tune SDXL-1.0 using the Diffusion-DPO objective and\nthe Pick-a-Pic (v2) dataset to create a new state-of-the-art\nfor open-source text-to-image generation models as mea-\nsured by generic preference, visual appeal, and prompt\nalignment. We additionally demonstrate that DPO-SDXL\noutperforms even the SDXL base plus refinement model\npipeline, despite only employing 53% of the total model\nparameters. Dataset cleaning/scaling is a promising future\ndirection as we observe preliminary data cleaning improv-\ning performance (Sec. 5.4). While DPO-Diffusion is an of-\nfline algorithm, we anticipate online learning methods to be\nanother driver of future performance. There are also excit-\ning application variants such as tuning to the preferences of\nindividuals or small groups.\nEthics\nThe performance of Diffusion-DPO is impres-\nsive, but any effort in text-to-image generation presents\nethical risks, particularly when data are web-collected.\nGenerations of harmful, hateful, fake or sexually explicit\ncontent are known risk vectors. Beyond that, this approach\nis increasingly subject to the biases of the participating\nlabelers (in addition to the biases present in the pretrained\nmodel); Diffusion-DPO can learn and propagate these\npreferences.\nAs a result, a diverse and representative\nset of labelers is essential \u2013 whose preferences in turn\nbecome encoded in the dataset.\nFurthermore, a portion\nof user-generated Pick-a-Pic prompts are overtly sexual,\nand even innocuous prompts may deliver images that\nskew more suggestively (particularly for prompts that\nhyper-sexualize women). Finally, as with all text-to-image\nmodels, the image produced will not always match the\nprompt.\nHearteningly though, some of these scenarios\ncan be addressed at a dataset level, and data filtering is\nalso possible.\nRegardless, we will not open source nor\notherwise make available our model until we add additional\nsafety filtering to ensure that toxic content is remediated.\nReferences\n[1] Model index for researchers, 2023. 2\n[2] Thomas Anthony, Zheng Tian, and David Barber. Thinking\nfast and slow with deep learning and tree search.\nNeural\nInformation Processing Systems, 2017. 2\n[3] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Jackson Kernion, Ka-\nmal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown,\nJack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan.\nA general language assistant as a laboratory for alignment,\n2021. 2\n[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda\nAskell, Jackson Kernion, Andy Jones, Anna Chen, Anna\nGoldie, Azalia Mirhoseini, Cameron McKinnon, Carol\nChen, Catherine Olsson, Christopher Olah, Danny Her-\nnandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-\nJohnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey\nLadish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite,\nLiane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas\nSchiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby,\nRobin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,\nSheer El Showk, Stanislav Fort, Tamera Lanham, Timo-\nthy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan\nHume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann,\nDario Amodei, Nicholas Joseph, Sam McCandlish, Tom\nBrown, and Jared Kaplan. Constitutional ai: Harmlessness\nfrom ai feedback, 2022. 2, 3\n[5] Michiel A. Bakker, Martin J. Chadwick, Hannah R. Shea-\nhan, Michael Henry Tessler, Lucy Campbell-Gillingham, Jan\nBalaguer, Nat McAleese, Amelia Glaese, John Aslanides,\nMatthew M. Botvinick, and Christopher Summerfield. Fine-\ntuning language models to find agreement among humans\nwith diverse preferences. Neural Information processing sys-\ntems, 2022. 2\n[6] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and\nSergey Levine.\nTraining diffusion models with reinforce-\nment learning. arXiv preprint arXiv:2305.13301, 2023. 1, 2,\n3, 5, 4\n[7] Kevin Clark, Paul Vicol, Kevin Swersky, and David J Fleet.\nDirectly fine-tuning diffusion models on differentiable re-\nwards, 2023. 1, 2, 3, 7\n[8] Katherine Crowson,\nStella\nBiderman,\nDaniel Kornis,\nDashiell Stander, Eric Hallahan, Louis Castricato, and Ed-\nward Raff. Vqgan-clip: Open domain image generation and\nediting with natural language guidance, 2022. 1\n[9] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-\naofang Wang, Abhimanyu Dubey, et al.\nEmu: Enhanc-\ning image generation models using photogenic needles in a\nhaystack. arXiv preprint arXiv:2309.15807, 2023. 1, 3, 2\n[10] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto.\nAlpacafarm: A simulation\nframework for methods that learn from human feedback,\n2023. 2\n[11] Ying\nFan,\nOlivia\nWatkins,\nYuqing\nDu,\nHao\nLiu,\nMoonkyung Ryu, Craig Boutilier, Pieter Abbeel, Moham-\nmad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok:\nReinforcement learning for fine-tuning text-to-image diffu-\nsion models. arXiv preprint arXiv:2305.16381, 2023. 1, 2,\n3, 5, 4\n[12] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for\nreward model overoptimization, 2022. 2\n[13] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming\nSong, Matthieu Geist, and Stefano Ermon. Iq-learn: Inverse\nsoft-q learning for imitation. Neural Information Processing\nSystems, 2021. 5\n[14] Amelia Glaese,\nNat McAleese,\nMaja Tr\u02dbebacz,\nJohn\nAslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,\nLaura Weidinger,\nMartin Chadwick,\nPhoebe Thacker,\nLucy\nCampbell-Gillingham,\nJonathan\nUesato,\nPo-Sen\nHuang,\nRamona Comanescu,\nFan Yang,\nAbigail See,\nSumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz,\nJaume Sanchez Elias, Richard Green, So\u02c7na Mokr\u00e1, Nicholas\nFernando, Boxi Wu, Rachel Foley, Susannah Young, Iason\nGabriel, William Isaac, John Mellor, Demis Hassabis, Ko-\nray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irv-\ning.\nImproving alignment of dialogue agents via targeted\nhuman judgements, 2022. 2\n[15] Gabriel Goh, James Betker, Li Jing, Aditya Ramesh, Tim\nBrooks, Jianfeng Wang, Lindsey Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Prafulla Dhariwal, Casey Chu, Joy Jiao,\nJong Wook Kim, Alex Nichol, Yang Song, Lijuan Wang, and\nTao Xu. Improving image generation with better captions.\n2023. 3, 10, 12\n[16] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Kse-\nnia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya\nSiddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolf-\ngang Macherey, Arnaud Doucet, Orhan Firat, and Nando de\nFreitas. Reinforced self-training (rest) for language model-\ning, 2023. 2\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. pages 6840\u20136851, 2020. 3, 4,\n5\n[18] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. If you use this software, please cite it as below.\n6\n[19] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6007\u20136017, 2023. 6\n[20] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. 2021. 3, 5, 4\n[21] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-\ntiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open\ndataset of user preferences for text-to-image generation.\narXiv preprint arXiv:2305.01569, 2023. 3, 5, 8\n[22] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu,\nThomas Mesnard, Colton Bishop, Victor Carbune, and Ab-\nhinav Rastogi. Rlaif: Scaling reinforcement learning from\nhuman feedback with ai feedback, 2023. 3, 6\n[23] Sergey Levine. Reinforcement learning and control as prob-\nabilistic inference: Tutorial and review, 2018. 4\n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[25] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. arXiv preprint arXiv:2108.01073, 2021. 6, 8\n[26] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John\nAslanides, Francis Song, Martin Chadwick, Mia Glaese, Su-\nsannah Young, Lucy Campbell-Gillingham, Geoffrey Irving,\nand Nat McAleese. Teaching language models to support\nanswers with verified quotes, 2022. 2\n[27] Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza,\nAlex Graves, Timothy P. Lillicrap, Tim Harley, David Silver,\nand Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning, 2016. 2\n[28] OpenAI.\nGpt-4 technical report.\nArXiv, abs/2303.08774,\n2023. 1\n[29] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan\nLowe. Training language models to follow instructions with\nhuman feedback, 2022. 2\n[30] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and\nRobin Rombach. Sdxl: Improving latent diffusion models\nfor high-resolution image synthesis, 2023. 2, 3, 5, 1\n[31] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and\nKaterina Fragkiadaki.\nAligning text-to-image diffusion\nmodels with reward backpropagation.\narXiv preprint\narXiv:2310.03739, 2023. 1, 3\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 6, 1\n[33] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Er-\nmon, Christopher D. Manning, and Chelsea Finn.\nDirect\npreference optimization: Your language model is secretly a\nreward model, 2023. 1, 2, 3, 4, 5, 8\n[34] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant\u00e9\nBrantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Han-\nnaneh Hajishirzi, and Yejin Choi. Is reinforcement learning\n(not) for natural language processing: Benchmarks, base-\nlines, and building blocks for natural language policy opti-\nmization, 2022. 2\n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser,\nand Bj\u00f6rn Ommer.\nStable diffusion\n2.\nhttps://huggingface.co/stabilityai/\nstable-diffusion-2. Accessed: 2023 - 11 - 16. 1\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 1, 3, 5\n[37] Christoph Schuhmann.\nLaion-aesthetics.\nhttps://\nlaion.ai/blog/laion-aesthetics/, 2022. Ac-\ncessed: 2023 - 11- 10. 3, 6, 1\n[38] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization algo-\nrithms, 2017. 2\n[39] Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias,\nand Yaniv Leviathan. A picture is worth a thousand words:\nPrincipled recaptioning improves image generation, 2023. 3\n[40] Noam Shazeer and Mitchell Stern.\nAdafactor: Adaptive\nlearning rates with sublinear memory cost. In International\nConference on Machine Learning, pages 4596\u20134604. PMLR,\n2018. 5\n[41] Joar Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov,\nand David Krueger.\nDefining and characterizing reward\nhacking. Neural Information Processing Systems, 2022. 2\n[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265, 2015. 3, 4\n[43] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 6\n[44] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in neural\ninformation processing systems, 32, 2019. 3\n[45] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.\nMaximum likelihood training of score-based diffusion mod-\nels. In Neural Information Processing Systems, 2021. 3\n[46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In International Conference on Learning Represen-\ntations, 2021. 3\n[47] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,\nRyan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and\nPaul Christiano. Learning to summarize from human feed-\nback. Neural Information Processing Systems, 18, 2020. 2\n[48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer,\nMoya Chen, Guillem Cucurull, David Esiobu, Jude Fer-\nnandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia\nGao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Vik-\ntor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-\nrenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiao-\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov,\nand Thomas Scialom. Llama 2: Open foundation and fine-\ntuned chat models, 2023. 1\n[49] Lewis\nTunstall,\nEdward\nBeeching,\nNathan\nLambert,\nNazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi\nHuang, Leandro von Werra, Cl\u00e9mentine Fourrier, Nathan\nHabib, Nathan Sarrazin, Omar Sanseviero, Alexander M.\nRush, and Thomas Wolf. Zephyr: Direct distillation of lm\nalignment, 2023. 2\n[50] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis\nSong, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey\nIrving, and Irina Higgins. Solving math word problems with\nprocess- and outcome-based feedback, 2022. 2\n[51] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil\nNaik.\nEnd-to-end diffusion latent optimization improves\nclassifier guidance. arXiv preprint arXiv:2303.13703, 2023.\n3, 1\n[52] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng\nZhu, Rui Zhao, and Hongsheng Li. Human preference score\nv2: A solid benchmark for evaluating human preferences of\ntext-to-image synthesis. arXiv preprint arXiv:2306.09341,\n2023. 3, 5, 6\n[53] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng\nZhu, Rui Zhao, and Hongsheng Li. Hpsv2 github. https:\n//github.com/tgxs002/HPSv2/tree/master,\n2023. Accessed: 2023 - 11 - 15. 5\n[54] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V\nLe.\nSelf-training with noisy student improves imagenet\nclassification. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 10687\u2013\n10698, 2020. 7\n[55] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai\nLi, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagere-\nward: Learning and evaluating human preferences for text-\nto-image generation.\narXiv preprint arXiv:2304.05977,\n2023. 3\n[56] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and\nYonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation, 2022. 5\n[57] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Song-\nfang Huang, and Fei Huang. Rrhf: Rank responses to align\nlanguage models with human feedback without tears. Neural\nInformation Processing Systems, 2023. 2\n[58] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mo-\nhammad Saleh, and Peter J. Liu. Slic-hf: Sequence likeli-\nhood calibration with human feedback, 2023. 2\n[59] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei\nShen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao\nZhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin\nLai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang\nWeng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang\nYan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang.\nSecrets of rlhf in large language models part i: Ppo, 2023. 2\n[60] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanx-\niao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-\ntraining and self-training. Advances in neural information\nprocessing systems, 33:3833\u20133845, 2020. 7\nDiffusion Model Alignment Using Direct Preference Optimization\nSupplementary Material\nS1. Comparisons to existing work\nRL-Based Methods\nsuch as [6, 11] have shown effectiveness in operating on a limited set of prompts (< 10 and <\n1000 respectively) but do not generalize as well to the open-vocabulary setting as shown in [7, 31]. We found this in our\nexperiments as well, where training using the DDPO scheme did not improve PickScore over the baseline model over a sweep\nof hyperparameters.\nWhile DDPO [6] is an RL-based method as is DPOK [11], their target objective and distributional guarantees are different.\nSpecifically, DDPO purely aims to optimize the reward function without any KL-regularization\nEc\u223cp(c),x0\u223cp\u03b8(x0|c)r(x0, c)\n(15)\nwhile DPOK adds in a term governing KL-regularization between the learned distribution and a reference distribution as in\nour setting. This means that DDPO is optimizing the same objective as DRaFT and AlignProp ([7, 31]) but via RL instead of\ngradient descent through the diffusion chain. DDPO uses early stopping in lieu of distributional control.\nAdditionally, through the score function policy gradient estimator employed by DDPO it is observable why the method\nstruggles with open vocabulary generation. The gradient estimation used is\n\u2207\u03b8JDDRL = E\nT\nX\nt=0\np\u03b8(xt\u22121 | xt, c)\np\u03b8old(xt\u22121 | xt, c) \u2207\u03b8 log p\u03b8(xt\u22121 | xt, c) r(x0, c)\n(16)\nHere the trajectories {xT , xT \u22121, . . . , x0} are generated by the original model p\u03b8old.\nIn this formulation, the term\np\u03b8(xt\u22121|xt,c)\np\u03b8old\nsimply is an importance weighter which scales gradient contributions based on the relevance of the sample\n(as determined by how aligned the learned and reference model predictions are). Since the trajectories are generated by the\n\u201cold\u201d (reference) model, r(x0, c) is only a weighting in the latter term \u2207\u03b8 log p\u03b8(xt\u22121 | xt, c) r(x0, c). The gradient en-\ncourages higher likelihoods for generations of high reward, but makes no distinction about the diversity of those generations.\nHigh-reward prompts can dominate the gradient trajectory, while generations considered lower-reward are ignored or discour-\naged. This stands in contrast to the DPO framework where the likelihood of a generation is contrasted against another with\nthe same conditioning. This normalization across conditioning prevents sets of c being considered unimportant/undesirable\nand not being optimized for. In Diffusion-DPO, conditionings with all types of reward magnitudes are weighted equally\ntowards the xw\n0 and away from the xl\n0.\nInference Time-Optimization\nnamely DOODL [51], does not learn any new model parameters, instead optimizing diffu-\nsion latents to improve some criterion on the generated image similar to CLIP+VQGAN[8]. This runtime compute increases\ninference cost by more than an order of magnitude.\nReward Maximization Training\nsuch as [7, 31] amortize the cost of DOODL from runtime to training. They train by\ngenerating images from text prompts, computing a reward loss on the images, and backpropagating gradients through the\ngenerative process to improve the loss. While effective in the open-vocabulary setting (also training on Pick-a-Pic prompts),\nthese methods provide no distributional guarantees (unlike the control via \u03b2 in Diffusion-DPO) and suffer from mode collapse\nwith over-training. These methods do not generalize to all reward functions, with [7] noting the inability of DRaFT to improve\nimage-text alignment using CLIP[32] as a reward function. In contrast, Diffusion-DPO can improve image-text alignment\nusing CLIP preference, as shown in Sec. 5.4. Furthermore, only differentiable rewards can be optimized towards in the\nreward maximization setting. This necessitates not only data collection but also reward model training.\nDataset Curation\nAs discussed, models such as StableDiffusion variants [30, 36] train on laion-aesthetics [37] to bias\nthe model towards more visually appealing outputs. Concurrent work Emu [9] takes this approach to an extreme. Instead\nof training on any images from a web-scale dataset which pass a certain model score threshold, they employ a multi-stage\npipeline where such filtering is only the first stage. Subsequently, crowd workers filter the subset down using human judge-\nment and at the final stage expert in photography are employed to create the dataset. While effective, this process has several\ndrawbacks compared to Diffusion-DPO. First, necessitating training on existing data can be a bottleneck, both in terms of\nscale and potential applications. While [9] reports lesser text faithfulness improvements as well, these are likely due to the\nhand-written captions, a much more costly data collection stage than preferences. The Emu pipeline is not generalizable to\ndifferent types of feedback as DPO is (e.g. outside of recaptioning it is non-obvious how such an approach can improve\ntext-image alignment).\nS2. Details of the Primary Derivation\nStarting from Eq. (5), we have\nmin\np\u03b8 \u2212 Ep\u03b8(x0|c)r(c, x0)/\u03b2 + DKL(p\u03b8(x0|c)||pref(x0|c))\n\u2264 min\np\u03b8 \u2212 Ep\u03b8(x0|c)r(c, x0)/\u03b2 + DKL (p\u03b8(x0:T |c)||pref(x0:T |c))\n= min\np\u03b8 \u2212 Ep\u03b8(x0:T |c)R(c, x0:T )/\u03b2 + DKL (p\u03b8(x0:T |c)||pref(x0:T |c))\n= min\np\u03b8 Ep\u03b8(x0:T |c)\n\u0012\nlog\np\u03b8(x0:T |c)\npref(x0:T |c) exp(R(c, x0:T )/\u03b2)/Z(c) \u2212 log Z(c)\n\u0013\n= min\np\u03b8 DKL (p\u03b8(x0:T |c)\u2225pref(x0:T |c) exp(R(c, x0:T )/\u03b2)/Z(c)) .\n(17)\nwhere Z(c) = P\nx pref(x0:T |c) exp (r(c, x0)/\u03b2) is the partition function. The optimal p\u2217\n\u03b8(x0:T |c) of Equation (17) has a\nunique closed-form solution:\np\u2217\n\u03b8(x0:T |c) = pref(x0:T |c) exp(R(c, x0:T )/\u03b2)/Z(c),\nTherefore, we have the reparameterization of reward function\nR(c, x0:T ) = \u03b2 log p\u2217\n\u03b8(x0:T |c)\npref(x0:T |c) + \u03b2 log Z(c).\nPlug this into the definition of r, hence we have\nr(c, x0) = \u03b2Ep\u03b8(x1:T |x0,c)\n\u0014\nlog p\u2217\n\u03b8(x0:T |c)\npref(x0:T |c)\n\u0015\n+ \u03b2 log Z(c).\nSubstituting this reward reparameterization into maximum likelihood objective of the Bradly-Terry model as Eq. (4), the\npartition function cancels for image pairs, and we get a maximum likelihood objective defined on diffusion models, its\nper-example formula is:\nLDPO-Diffusion(\u03b8) = \u2212 log \u03c3\n\u0012\n\u03b2Exw\n1:T \u223cp\u03b8(x1:T |xw\n0 ),xl\n1:T \u223cp\u03b8(xl\n1:T |xl\n0)\n\u0014\nlog p\u03b8(xw\n0:T )\npref(xw\n0:T ) \u2212 log p\u03b8(xl\n0:T )\npref(xl\n0:T )\n\u0015\u0013\nwhere xw\n0 , xl\n0 are from static dataset, we drop c for simplicity.\nAn approximation for reverse process\nSince sampling from p\u03b8(x1:T |x0) is intractable, we utilize q(x1:T |x0) for approx-\nimation.\nL1(\u03b8) = \u2212 log \u03c3\n\u0012\n\u03b2Exw\n1:T \u223cq(x1:T |xw\n0 ),xl\n1:T \u223cq(x1:T |xl\n0)\n\u0014\nlog p\u03b8(xw\n0:T )\npref(xw\n0:T ) \u2212 log p\u03b8(xl\n0:T )\npref(xl\n0:T )\n\u0015\u0013\n= \u2212 log \u03c3\n \n\u03b2Exw\n1:T \u223cq(x1:T |xw\n0 ),xl\n1:T \u223cq(x1:T |xl\n0)\n\" T\nX\nt=1\nlog p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212 log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xt)\n#!\n= \u2212 log \u03c3\n\u0012\n\u03b2Exw\n1:T \u223cq(x1:T |xw\n0 ),xl\n1:T \u223cq(x1:T |xl\n0)TEt\n\u0014\nlog p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212 log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xt)\n\u0015\u0013\n= \u2212 log \u03c3\n\u0012\n\u03b2TEtExw\nt\u22121,t\u223cq(xt\u22121,t|xw\n0 ),xl\nt\u22121,t\u223cq(xt\u22121,t|xl\n0)\n\u0014\nlog p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212 log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xt)\n\u0015\u0013\n= \u2212 log \u03c3\n \n\u03b2TEt,xw\nt \u223cq(xt|xw\n0 ),xl\nt\u223cq(xt|xl\n0)\nExw\nt\u22121\u223cq(xt\u22121|xw\nt ,xw\n0 ),xl\nt\u22121\u223cq(xt\u22121|xl\nt,xl\n0)\n\u0014\nlog p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212 log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xt)\n\u0015!\n(18)\nBy Jensen\u2019s inequality, we have\nL1(\u03b8) \u2264 \u2212Et,xw\nt \u223cq(xt|xw\n0 ),xl\nt\u223cq(xt|xl\n0) log \u03c3\n\u0012\n\u03b2TExw\nt\u22121\u223cq(xt\u22121|xw\nt ,xw\n0 ),xl\nt\u22121\u223cq(xt\u22121|xl\nt,xl\n0)\n\u0014\nlog p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212 log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xt)\n\u0015\u0013\n= \u2212Et,xw\nt \u223cq(xt|xw\n0 ),xl\nt\u223cq(xt|xl\n0) log \u03c3\n\u0012\n\u2212\u03b2T\n\u0000DKL(q(xw\nt\u22121|xw\n0,t)\u2225p\u03b8(xw\nt\u22121|xw\nt )) \u2212 DKL(q(xw\nt\u22121|xw\n0,t)\u2225pref(xw\nt\u22121|xw\nt ))\n\u2212\n\u0000DKL(q(xl\nt\u22121|xl\n0,t)\u2225p\u03b8(xl\nt\u22121|xl\nt)) + DKL(q(xl\nt\u22121|xl\n0,t)\u2225pref(xl\nt\u22121|xl\nt)\n\u0001\u0001\u0013\nUsing the Gaussian parameterization of the reverse process (Eq. (1)), the above loss simplifies to:\nL1(\u03b8) \u2264 \u2212Et,\u03f5w,\u03f5l log \u03c3 (\u2212\u03b2T\u03c9(\u03bbt) ( \u2225\u03f5w\u2212\u03f5\u03b8(xw\nt , t)\u22252\u2212\u2225\u03f5w\u2212\u03f5ref(xw\nt , t)\u22252 \u2212\n\u0000\u2225\u03f5l \u2212 \u03f5\u03b8(xl\nt, t)\u22252 \u2212 \u2225\u03f5l \u2212 \u03f5ref(xl\nt, t)\u22252\u0001\u0001\u0001\nwhere \u03f5w, \u03f5l \u223c N(0, I), xt \u223c q(xt|x0) thus xt = \u03b1tx0 + \u03c3t\u03f5. Same as Eq. (2), \u03bbt = \u03b12\nt /\u03c32\nt is a signal-to-noise ratio\nterm [20], in practice, the reweighting assigns each term the same weight [17].\nAn alternative approximation\nNote that for Eq. (18) we utilize q(x1:T |x0) to approximate p\u03b8(x1:T |x0).\nFor each\nstep, it is to use q(xt\u22121,t|x0) to approximate p\u03b8(xt\u22121,t|x0). Alternatively, we also propose to use q(xt|x0)p\u03b8(xt\u22121|xt)\nfor approximation.\nAnd this approximation yields lower error because DKL(q(xt|x0)p\u03b8(xt\u22121|xt)\u2225p\u03b8(xt\u22121,t|x0)) =\nDKL(q(xt|x0)\u2225p\u03b8(xt|x0)) < DKL(q(xt\u22121,t|x0)\u2225p\u03b8(xt\u22121,t|x0)).\nLDPO-Diffusion(\u03b8) = \u2212 log \u03c3\n\u0012\n\u03b2Exw\n1:T \u223cp\u03b8(x1:T |xw\n0 ),xl\n1:T \u223cp\u03b8(x1:T |xl\n0)\n\u0014\nlog p\u03b8(xw\n0:T )\npref(xw\n0:T ) \u2212 log p\u03b8(xl\n0:T )\npref(xl\n0:T )\n\u0015\u0013\n= \u2212 log \u03c3\n \n\u03b2Exw\n1:T \u223cp\u03b8(x1:T |xw\n0 ),xl\n1:T \u223cp\u03b8(x1:T |xl\n0)\n\" T\nX\nt=1\nlog p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212 log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xt)\n#!\n= \u2212 log \u03c3\n\u0012\n\u03b2Exw\n1:T \u223cp\u03b8(x1:T |xw\n0 ),xl\n1:T \u223cp\u03b8(x1:T |xl\n0)TEt\n\u0014\nlog p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212 log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xt)\n\u0015\u0013\n= \u2212 log \u03c3\n\u0012\n\u03b2TEtExw\nt\u22121,t\u223cp\u03b8(xt\u22121,t|xw\n0 ),xl\nt\u22121,t\u223cp\u03b8(xt\u22121,t|xl\n0)\n\u0014\nlog p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212 log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xt)\n\u0015\u0013\n.\nBy approximating p\u03b8(xt\u22121,t|x0) with q(xt|x0)p\u03b8(xt\u22121|xt), we have\nL2(\u03b8) = \u2212 log \u03c3\n\u0012\n\u03b2TEtExw\nt\u22121,t\u223cq(xt|xw\n0 )p\u03b8(xt\u22121|xw\nt ),xl\nt\u22121,t\u223cq(xt|xl\n0)p\u03b8(xt\u22121|xl\nt)\n\u0014\nlog p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212 log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xt)\n\u0015\u0013\n= \u2212 log \u03c3\n\u0012\n\u03b2TEt,xw\nt \u223cq(xt|xw\n0 ),xl\nt\u223cq(xt|xl\n0)Exw\nt\u22121\u223cp\u03b8(xt\u22121|xw\nt ),xl\nt\u22121\u223cp\u03b8(xt\u22121|xl\nt)\n\u0014\nlog p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212 log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xt)\n\u0015\u0013\n.\nBy Jensen\u2019s inequality, we have\nL2(\u03b8) \u2264 \u2212Et,xw\nt \u223cq(xt|xw\n0 ),xl\nt\u223cq(xt|xl\n0) log \u03c3\n\u0012\n\u03b2TExw\nt\u22121\u223cp\u03b8(xt\u22121|xw\nt ),xl\nt\u22121\u223cp\u03b8(xt\u22121|xl\nt)\n\u0014\nlog p\u03b8(xw\nt\u22121|xw\nt )\npref(xw\nt\u22121|xw\nt ) \u2212 log p\u03b8(xl\nt\u22121|xl\nt)\npref(xl\nt\u22121|xt)\n\u0015\u0013\n= \u2212Et,xw\nt \u223cq(xt|xw\n0 ),xl\nt\u223cq(xt|xl\n0) log \u03c3\n\u0000\u03b2T\n\u0000DKL(p\u03b8(xw\nt\u22121|xw\nt )\u2225pref(xw\nt\u22121|xw\nt )) \u2212 DKL(p\u03b8(xl\nt\u22121|xl\nt)\u2225pref(xl\nt\u22121|xl\nt)\n\u0001\u0001\nUsing the Gaussian parameterization of the reverse process (Eq. (1)), the above loss simplifies to:\nL2(\u03b8) = \u2212Et,\u03f5w,\u03f5l log \u03c3\n\u0000\u2212\u03b2T\u03c9(\u03bbt)\n\u0000\u2225\u03f5\u03b8(xw\nt , t) \u2212 \u03f5ref(xw\nt , t)\u22252 \u2212 \u2225\u03f5\u03b8(xl\nt, t) \u2212 \u03f5ref(xl\nt, t)\u22252\u0001\u0001\nwhere \u03f5w, \u03f5l \u223c N(0, I), xt \u223c q(xt|x0) thus xt = \u03b1tx0 + \u03c3t\u03f5. Same as Eq. (2), \u03bbt = \u03b12\nt /\u03c32\nt is a signal-to-noise ratio\nterm [20], in practice, the reweighting assigns each term the same weight [17].\nS3. Alternate Derivation: Reinforcement Learning Perspective\nWe can also derive our objective as a multi-step RL approach, in the same setting as [6, 11]. A Markov Decision Process\n(MDP) is a tuple (S, A, \u03c10, P, R), where S is the state space, A is the action space, \u03c10 is an initial state distribution, P is the\ntransition dynamics and R is the reward function. In this formulation, at each time step t a policy \u03c0(at|st) observes a state\nst \u2208 S and takes an action at \u2208 A. The environment then transitions to a next state st+1 \u223c P(st+1|st, at) and the returns a\nreward R(st, at).The goal of the policy is to maximize the total rewards it receives. Prior works [6, 11] map the denoising\nprocess in diffusion model generation to this formulation via:\nst \u225c (c, xt, t)\nat \u225c xt\nP(st+1|st, at) \u225c (\u03b4c, \u03b4t\u22121, \u03b4xt\u22121)\n\u03c1(s0) \u225c (p(c), \u03b4T , N(0, I))\nR(st, at) =\n(\nr(c, x0)\nif t = 0\n0\notherwise\n(19)\nwhere c is the prompt xt is the time-step t nosy image and \u03b4y is the Dirac delta function with unit density at y. That is\nin this formulation we consider the denoising model as a policy, with each denoising step a step in an MDP. The objective of\nthe policy is to maximize the reward (alignment with human preference) of the final image. In the derivation below, we drop\nthe time step t for brevity. In this formulation the generative model is a policy and the denoising process is a rollout in an\nMDP with a sparse reward received for the final generated image. Following [11] we optimize the following objective\nEc\u223cD,p\u03b8\n\" 0\nX\nt=T\nr(c, xt) \u2212 \u03b2DKL[p\u03b8(xt\u22121|xt, c)||pref(xt\u22121|xt, c)]\n#\n(20)\nWhile prior works [6, 11] use policy gradient approaches to optimize this objective, we\u2019re going to use off-policy methods.\nFollowing Control as Variational Inference [23], we have the following\nQ\u2217((xt, c), xt\u22121) = r(c, xt) + V \u2217(xt\u22121, c)\n(21)\nV \u2217(xt\u22121, c) = \u03b2 log Epref [exp Q\u2217((xt, c), xt\u22121)/\u03b2]\n(22)\np\u2217(xt\u22121|(xt, c)) = pref(xt\u22121|xt, c)e(Q\u2217((xt,c),xt\u22121)\u2212V \u2217(xt,c))/\u03b2\n(23)\nwhere V \u2217 is the optimal value function and Q\u2217 is the optimal state-action value function (in tour definition of the denoising\nMDP, he policy is stochastic, but the dynamics is deterministic). Also notice that in Eq. 23 the equation is exact since the\nright-hand side integrates to 1. We then consider the inverse soft Bellman operator [13] and have the following\nr(c, xt) = V \u2217(xt\u22121, c) \u2212 Q\u2217((xt, c), xt\u22121)\n(24)\nHowever, from Eq. 23 we have\nQ\u2217((xt, c), xt\u22121) \u2212 V \u2217(xt, c) = log p\u2217(xt\u22121|xt, c)\npref(xt\u22121|xt, c)\n(25)\nsubstituting in Eq. 24 we obtain:\nr(c, xt) = V \u2217(xt\u22121, c) + log p\u2217(xt\u22121|xt, c)\npref(xt\u22121|xt, c) \u2212 V \u2217(xt, c)\n(26)\nUsing a telescoping sum through the diffusion chain we are left with\nr(c, x0) =\nT\nX\nt=0\nlog p\u2217(xt\u22121|xt, c)\npref(xt\u22121|xt, c) \u2212 V \u2217(xT , c)\n(27)\nsince by definition all intermediate rewards are zero. If we assume both diffusion chains start from the same state and plug\nthis result into the preference formulation of Eq. 3 we obtain the objective of Eq. 11. Here we optimize the same objective\nas prior works [6, 11], but instead of a policy gradient approach we derive our objective as an off-policy learning problem in\nthe same MDP. This not only simplifies the algorithm significantly, but justifies our sampling choices in Eq. 13 and we do\nnot have to sample through the entire difussion chain.\nS4. Alternative Derivation: Noise-Aware Preference Model\nParalleling the original DPO formulation we consider a policy trained on maximizing the likelihood of p(x0|c, t, xobs) where\nxobs is a noised version of x0. Here x0 is an image, c is a text caption, t is a noising scale, and xobs is a corruption (noised\nversion) of x0. We initialize from a reference diffusion policy pref. We aim to optimize the same RL objective of Eq. (5),\nreprinted here for convenience:\nmax\np\u03b8 Ec\u223cD,x0\u223cp\u03b8(x0|c) [r(c, x0)] \u2212 \u03b2DKL [p\u03b8(x0|c)\u2225pref(x0|c)]\n(28)\nOur policy has additional conditioning (t, xobs). The latter is a noised version of x0. Define the space of noising operators\nat time t as Qt where qt \u223c Qt with qt(x0) = \u221a\u03b1tx0 + \u221a1 \u2212 \u03b1\u03f5qt, \u03f5qt \u223c N(0, I). Here qt refers to the linear transform\ncorresponding with a specific gaussian draw \u223c N(0, I) and the set of qt is Qt. In general at some time level t we have\nyobs = qt(x0) for some qt \u223c Qt so can write the conditioning as p(x0|c, t, qt(y)). We rewrite Eq. (28) as\nmax\np\u03b8 Ec\u223cD,x0\u223cp(gen)\n\u03b8\n(x0|c),t\u223cU{0,T },qt\u223cQT\n\u0000r\u03d5(c, x0) \u2212 \u03b2DKL\n\u0002\np\u03b8(x0 | c, t, qt(x0)) || pref(x0 | c, t, qt(x0))\n\u0003\u0001\n(29)\np(gen) denoting the generative process associated with p as a diffusion model. Note that the reward model is the same\nformulation as in DPO. The optimal policy now becomes\np\u2217\n\u03b8(x0 | c, t, qt(x0)) =\n1\nZ(c, t, qt)pref(x0 | c, t, qt(x0)) exp\n\u0012 1\n\u03b2 r(c, x0)\n\u0013\n(30)\nwith Z a partition over captions, timesteps, and noising draws. Rearranging for r(c, x0) now yields\nr(c, x0) = \u03b2 log p\u2217\n\u03b8(x0 | c, t, qt)\npref(x0 | c, t, qt) + \u03b2 log Z(c, t, qt), \u2200t, qt\n(31)\nWe have not changed the reward model formulation at all, but our policies have extra conditioning as input (which ideally\nthe likelihoods are constant with respect to). Putting this formulation into the original Bradley-Terry framework of Eq. (3)\n(re-printed here)\npBT(xw\n0 \u227b xl\n0|c) = \u03c3(r(c, xw\n0 ) \u2212 r(c, xl\n0))\n(32)\nresults in the objective:\nLDPO(p\u03b8; pref) = \u2212E(xw\n0 ,xl\n0\u223cp(gen)(c),c\u223cD,t\u223cU{0,T },qt\u223cQt\n\u0014\nlog \u03c3\n\u0012\n\u03b2 log p\u03b8(xw\n0 | c, t, qt(xw\n0 ))\npref(xw\n0 | c, t, qt(xw\n0 )) \u2212 \u03b2 log p\u03b8(xl\n0 | c, t, qt(xl\n0))\npref(xl\n0 | c, t, qt(xl\n0))\n\u0013\u0015\n(33)\nWe now consider how to compute these likelihoods. Using the notation at = \u221a\u03b1t and bt = \u221a1 \u2212 \u03b1t as shorthand for\ncommonly-used diffusion constants (\u03b1 are defined as in DDIM[43]) we have\nxobs = qt(x0) = atx0 + bt\u03f5, \u03f5 \u223c N(0, I)\n(34)\nWe use Eq. 57 from DDIM[43] (along with their definition of \u03c3t):\np(x0|xt) = N(xpred\n0\n, \u03c32\nt I)\n(35)\nOur xpred\n0\nis:\nxpred\n0\n= xobs \u2212 bt\u03f5pred\n\u03b8\nat\n= atx0 + bt\u03f5 \u2212 bt\u03f5pred\n\u03b8\nat\n= x0 + bt\nat\n(\u03f5 \u2212 \u03f5pred\n\u03b8\n)\n(36)\nHere \u03f5pred\n\u03b8\nis the output of \u03f5\u03b8(c, t, xobs) Making the conditional likelihood:\np\u03b8(x0|c, t, xobs) = N(x0; x0 + bt\nat\n(\u03f5 \u2212 \u03f5pred\n\u03b8\n), \u03c32\nt I) =\n1\n(2\u03c0\u03c32\nt )d/2 e\n\u2212\nb2\nt\n2a2\nt \u03c32\nt\n||\u03f5\u2212\u03f5pred\n\u03b8\n||2\n2\n(37)\nFor convenience we define\nzt =\n1\n(2\u03c0\u03c32\nt )d/2\n(38)\nSE = ||\u03f5 \u2212 \u03f5pred||2\n2\n(39)\nWe will decorate the latter quantity (SE) with sub/superscripts later. For now we get:\np\u03b8(x0|c, t, xobs) = zte\n\u2212\nb2\nt\n2a2\nt \u03c32\nt\nSE\n(40)\nWe see to minimize\nE\n(xw\n0 ,xl\n0\u223cp(gen)(c);c\u223cD,;t\u223cU{0,T };qt\u223cQt\n\u2212 log \u03c3\n\u0012\n\u03b2\n\u0012\nlog p\u03b8(xw\n0 |c, t, qt(xw\n0 ))\npref(xw\n0 |c, t, qt(xw\n0 ) \u2212 log p\u03b8(xl\n0|c, t, qt(xl\n0))\npref(xl\n0|c, t, qt(xl\n0)\n\u0013\u0013\n=\n(41)\nE\n(xw\n0 ,xl\n0\u223cp(gen)(c);c\u223cD,;t\u223cU{0,T };qt\u223cQt\n\u2212 log \u03c3\n\u0012\n\u03b2\n\u0012\nlog zte\n\u2212\nb2\nt\n2a2\nt \u03c32\nt\nSE(w)\n\u03b8\nzte\n\u2212\nb2\nt\n2a2\nt \u03c32\nt\nSE(w)\nref\n\u2212 log zte\n\u2212\nb2\nt\n2a2\nt \u03c32\nt\nSE(l)\n\u03b8\nzte\n\u2212\nb2\nt\n2a2\nt \u03c32\nt\nSE(l)\nref\n\u0013\u0013\n(42)\nHere we use SE(d)\n\u03c8\n= ||\u03f5qt \u2212 \u03c8(c, t, qt(xd\n0))||2\n2 to denote the L2 error in the noise prediction of model \u03c8 operating on\nthe noisy qt(xd\n0) with corresponding conditioning (c, t) (d \u2208 {w, l}). Here the model associated with SE\u2217\nref is the model of\nthe reference policy pref. Note that these SE terms are the standard diffusion training objective from Eq. (2). Continuing to\nsimplify the above yields:\n\u2212 log \u03c3\n\uf8eb\n\uf8ec\n\uf8ed\u03b2\n\uf8eb\n\uf8ec\n\uf8edlog zte\n\u2212\nb2\nt\n2a2\nt \u03c32\nt\nSE(w)\n\u03b8\nzte\n\u2212\nb2\nt\n2a2\nt \u03c32\nt\nSE(w)\nref\n\u2212 log zte\n\u2212\nb2\nt\n2a2\nt \u03c32\nt\nSE(l)\n\u03b8\nzte\n\u2212\nb2\nt\n2a2\nt \u03c32\nt\nSE(l)\nref\n\uf8f6\n\uf8f7\n\uf8f8\n\uf8f6\n\uf8f7\n\uf8f8\n(43)\n= \u2212 log \u03c3\n\u0012\n\u2212\u03b2\nb2\nt\n2a2\nt\u03c32\nt\n\u0010\n(SE(w)\n\u03b8\n\u2212 SE(w)\nref ) \u2212 (SE(l)\n\u03b8\n\u2212 SE(l)\nref )\n\u0011\u0013\n(44)\nWe can simplify the coefficient:\nb2\nt\na2\nt\u03c32\nt\n= 1 \u2212 \u03b1t\n\u03b1t\n1\n\u03c32\nt\n= \u03c32\nt+1\n\u03c32\nt\n\u2248 1\n(45)\nResulting in objective\n\u2248\nE\nx,yw,yl\u223cD;t;\u03f5\u223cN (0,I) \u2212 log \u03c3\n\u0012\n\u2212\u03b2\n2\n\u0010\n(SE(w)\n\u03b8\n\u2212 SE(w)\nref ) \u2212 (SE(l)\n\u03b8\n\u2212 SE(l)\nref )\n\u0011\u0013\n(46)\nUp to the approximation of Eq. (45) this is the equivalent to Eq. (33). The log of the likelihood ratios simply take on the\nelegant form of a difference in diffusion training losses. Due to the equation negatives and log \u03c3 being a monotonic increasing\nfunction, by minimizing Eq. (46) we are aiming to minimize the inside term\n\u0010\n(SE(w)\n\u03b8\n\u2212 SE(w)\nref ) \u2212 (SE(l)\n\u03b8\n\u2212 SE(l)\nref )\n\u0011\n(47)\nThis can be done by minimizing SE(w)\n\u03b8\nor maximizing SE(l)\n\u03b8 , with the precise loss value depending on how these compare\nto the reference errors SE(w)\nref , SE(l)\nref . The asymmetry of the log \u03c3 function allows \u03b2 to control the penalty for deviating from\nthe reference distribution. A high \u03b2 results in a highly assymetric distribution, disproportionately penalizing low SE(l)\n\u03b8\nand\nhigh SE(w)\n\u03b8\nand encouraging a p\u03b8 to make less mistakes in implicitly scoring yw, yl by deviating less from the reference\npolicy pref. We visualize the log \u03c3 curves in Figure S1 for several values of \u03b2.\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\ny =\nlog (x)\n= 0.5\n= 1\n= 2\nFigure S1. Visualization of y = \u2212 log \u03c3(\u03b2x)\nS4.1. Reward Estimation\nFinally, we note that in this formulation that if we wish to compute the noise-aware reward difference r(c, xA\n0 ) \u2212 r(c, xB\n0 ),\nreferring to Eq. (31) this now has form\nr(c, xA\n0 ) \u2212 r(c, xB\n0 ) =\n\u0002\n\u03b2(SEA\n\u03b8 \u2212 SEA\nref) + \u03b2 log Z(c, t, qt)\n\u0003\n\u2212\n\u0002\n\u03b2(SEB\n\u03b8 \u2212 SEB\nref) + \u03b2 log Z(c, t, qt)\n\u0003\n, \u2200c, t, qt\n(48)\n= \u03b2\n\u0002\n(SEA\n\u03b8 \u2212 SEA\nref) \u2212 (SEB\n\u03b8 \u2212 SEB\nref)\n\u0003\n, \u2200c, t, qt\n(49)\n(50)\nWhich means for two images (xA\n0 , xB\n0 ) with the same conditioning c we can estimate the reward difference using Eq. (48).\nWhen doing this it improves the estimate to average over multiple draws (t, qt). We use this method in Table S2.\nS5. \u03b2 Ablation\nFigure S2. Median PickScores for generations on the Pick-a-Pic v2 validation set for different choices of \u03b2\nFor \u03b2 far below the displayed regime, the diffusion model degenerates into a pure reward scoring model. Much greater,\nand the KL-divergence penalty greatly restricts any appreciable adaptation.\nS6. Further SFT Discussions\nWe also partially attribute this difference in effectiveness of SFT to the gap in pretraining vs. downstream task considered in\nthe original DPO paper [33] vs. our work. On two of the DPO LLM tasks (sentiment generation, single-turn dialogue),\ngeneric off-the-shelf autoregressive language models are tuned on specific tasks in the SFT stage. In the final setting, sum-\nmarization, the SFT model has been pretrained on a similar task/dataset. In this case, finetuning on the \u201cpreferred\u201d dataset\n(preferred-FT) baseline performs comparably to the SFT initialization.\nThis final setting is most analogous to that of Diffusion-DPO. The generic pretraining, task, and evaluation setting are all\ntext-to-image generation. There is no task-specific domain gap and all of the settings are open-vocabulary with a broad range\nof styles. As such, our findings are similar to that of summarization in [33] where an already task-tuned model does not\nbenefit from preferred finetuning.\nS7. Additional Automated Metrics\nAutomated metrics on Pick-a-Pic validation captions are shown in Figure S3 for DPO-SDXL. The y-axis measures the\nfraction of head-to-head generation comparisions for a prompt that DPO-SDXL scores higher than the baseline SDXL.\nS8. PickScore Rejection Sampling\nRejection sampling was used in [21] as a powerful inference-time tool. 100 samples were drawn from variants of a prompt\nand PickScore-ranked, with the highest scored images being compared to a single random draw. PickScore selections were\nhuman-preferred 71.4% of the time. We compare using additional compute at inference vs. additional training in Figure S4.\nWe plot the expected PickScore win rate of n draws from the reference model against a single draw from the learned (DPO)\nmodel. The mean inference compute for baseline rejection sampling to surpass the DPO-trained model is 10\u00d7 higher in both\ncases. For 7% (SDXL) and 16% (SD1.5) of the prompts even 100 draws is insufficient.\nFigure S3. Box plots of automated metrics vs. SDXL baseline. All 500 unique prompts from PickScore validation set.\nFigure S4. The number of draws from the reference model vs. the probability that maximum PickScore of the draws exceeds a single DPO\ngeneration. 500 PickScore validation prompts used. Mean (including 100s)/Median: SDXL (13.7, 3), SD1.5 (25.6, 7).\nS9. Pseudocode for Training Objective\ndef loss(model, ref_model, x_w, x_l, c, beta):\n\"\"\"\n# This is an example psuedo-code snippet for calculating the Diffusion-DPO loss\n# on a single image pair with corresponding caption\nmodel: Diffusion model that accepts prompt conditioning c and time conditioning t\nref_model: Frozen initialization of model\nx_w: Preferred Image (latents in this work)\nx_l: Non-Preferred Image (latents in this work)\nc: Conditioning (text in this work)\nbeta: Regularization Parameter\nreturns: DPO loss value\n\"\"\"\ntimestep = torch.randint(0, 1000)\nnoise = torch.randn_like(x_w)\nnoisy_x_w = add_noise(x_w, noise, t)\nnoisy_x_l = add_noise(x_l, noise, t)\nmodel_w_pred = model(noisy_x_w, c, t)\nmodel_l_pred = model(noisy_x_l, c, t)\nref_w_pred = ref(noisy_x_w, c, t)\nref_l_pred = ref(noisy_x_l, c, t)\nmodel_w_err = (model_w_pred - noise).norm().pow(2)\nmodel_l_err = (model_l_pred - noise).norm().pow(2)\nref_w_err = (ref_w_pred - noise).norm().pow(2)\nref_l_err = (ref_l_pred - noise).norm().pow(2)\nw_diff = model_w_err - ref_w_err\nl_diff = model_l_err - ref_l_err\ninside_term = -1 * beta * (w_diff - l_diff)\nloss = -1 * log(sigmoid(inside_term))\nreturn loss\nS10. Additional Qualitative Results\nIn Figure S6 we present generations from DPO-SDXL on complex prompts from DALLE3 [15]. Other generations for\nmiscellaneous prompts are shown in Figure S5. In Fig. S 7 and 8 we display qualitative comparison results from HPSv2 with\nrandom seeds from our human evaluation for prompt indices 200, 600, 1000, 1400, 1800, 2200, 2600, 3000.\nFigure S5. DPO-SDXL gens on miscellaneous prompts Prompts (clockwise) (1) A bulldog mob boss, moody cinematic feel (2) A old\nhistorical notebook detailing the discovery of unicorns (3) A purple raven flying over a forest of fall colors, imaginary documentary (4)\nSmall dinosaurs shopping in a grocery store, oil painting (5) A wolf wearing a sheep halloween costume going trick-or-treating at the farm\n(6) A mummy studying hard in the library for finals, head in hands\nFigure S6. DPO-SDXL gens on prompts from DALLE3 [15] Prompts: (1): A swirling, multicolored portal emerges from the depths of\nan ocean of coffee, with waves of the rich liquid gently rippling outward. The portal engulfs a coffee cup, which serves as a gateway\nto a fantastical dimension. The surrounding digital art landscape reflects the colors of the portal, creating an alluring scene of endless\npossibilities. (2): In a fantastical setting, a highly detailed furry humanoid skunk with piercing eyes confidently poses in a medium shot,\nwearing an animal hide jacket. The artist has masterfully rendered the character in digital art, capturing the intricate details of fur and\nclothing texture\"\nSDXL\nDPO-SDXL\nFigure S7. Prompts: (1) A kangaroo wearing an orange hoodie and blue sunglasses stands on the grass in front of the Sydney Opera House,\nholding a sign that says Welcome Friends. (2) Anime Costa Blanca by Studio Ghibli. (3) There is a secret museum of magical items inside\na crystal greenhouse palace filled with intricate bookshelves, plants, and Victorian style decor. (4) A depiction of Hermione Granger from\nthe Harry Potter series as a zombie.\nSDXL\nDPO-SDXL\nFigure S8. (1) A portrait art of a necromancer, referencing DND and War craft. (2) Monalisa painting a portrait of Leonardo Da Vinci. (3)\nThere is a cyclist riding above all the pigeons. (4) A woman holding two rainbow slices of cake.\n"
  },
  {
    "title": "ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs",
    "link": "https://arxiv.org/pdf/2311.13600.pdf",
    "upvote": "41",
    "text": "ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs\nViraj Shah\u2217,1,2\nNataniel Ruiz1\nForrester Cole1\nErika Lu1\nSvetlana Lazebnik2\nYuanzhen Li1\nVarun Jampani\u2020,1\n1 Google Research\n2 UIUC\nContent Images\nStyle Image\nLc\nLs\n\u201cA [V] dog\u201d\n\u201cFlowers in [S] style\u201d\n\u201c[V] dog in [S] \nstyle\u201d\n\u201c[V] dog playing \nwith a ball in [S] \nstyle\u201d\n\u201cSleeping [V] \ndog in [S] style\u201d\n\u201c[V] dog wearing \na crown in [S] \nstyle\u201d\nSubjects:\n[Vi]\nStyles: [Si]\n\u201c[Vi] subject in [Si] style\u201d\nZipLoRA\nStyle \nLoRA\nContent \nLoRA\nLs\nLc\nFigure 1. By effectively merging independently trained style and content LoRAs, our proposed method ZipLoRA is able to generate any\nuser-provided subject in any user-provided style, providing unprecedented control over personalized creations using diffusion models.\nAbstract\nMethods for finetuning generative models for concept-\ndriven personalization generally achieve strong results\nfor subject-driven or style-driven generation.\nRecently,\nlow-rank adaptations (LoRA) have been proposed as a\nparameter-efficient way of achieving concept-driven per-\nsonalization. While recent work explores the combination of\nseparate LoRAs to achieve joint generation of learned styles\nand subjects, existing techniques do not reliably address the\nproblem; they often compromise either subject fidelity or\nstyle fidelity. We propose ZipLoRA, a method to cheaply\nand effectively merge independently trained style and sub-\nject LoRAs in order to achieve generation of any user-\nprovided subject in any user-provided style. Experiments\non a wide range of subject and style combinations show\nthat ZipLoRA can generate compelling results with mean-\ningful improvements over baselines in subject and style fi-\ndelity while preserving the ability to recontextualize.\n*This research was performed while Viraj Shah was at Google.\n\u2020Varun Jampani is now at Stability AI.\n1. Introduction\nRecently, diffusion models [13, 28, 34] have allowed for\nimpressive image generation quality with their excellent un-\nderstanding of diverse artistic concepts and enhanced con-\ntrollability due to multi-modal conditioning support (with\ntext being the most popular mode). The usability and flex-\nibility of generative models has further progressed with a\nwide variety of personalization approaches, such as Dream-\nBooth [29] and StyleDrop [33]. These approaches fine-tune\na base diffusion model on the images of a specific concept\nto produce novel renditions in various contexts. Such con-\ncepts can be a specific object or person, or an artistic style.\nWhile personalization methods have been used for sub-\njects and styles independently, a key unsolved problem is to\ngenerate a specific user-provided subject in a specific user-\nProject page: https://ziplora.github.io\n1\narXiv:2311.13600v1  [cs.CV]  22 Nov 2023\nprovided style. For example, an artist may wish to render a\nspecific person in their personal style, learned through ex-\namples of their own work. A user may wish to generate\nimages of their child\u2019s favorite plush toy, in the style of the\nchild\u2019s watercolor paintings. Moreover, if this is achieved\ntwo problems are simultaneously solved: (1) the task of rep-\nresenting any given subject in any style, and (2) the problem\nof controlling diffusion models through images rather than\ntext, which can be imprecise and unsuitable for certain gen-\neration tasks. Finally, we can imagine a large-scale applica-\ntion of such a tool, where a bank of independently learned\nstyles and subjects are shared and stored online. The task\nof arbitrarily rendering any subject in any style is an open\nresearch problem that we seek to address.\nA pitfall of recent personalization methods is that many\nfinetune all of the parameters of a large base model, which\ncan be costly. Parameter Efficient Fine-Tuning (PEFT) ap-\nproaches allow for fine-tuning models for concept-driven\npersonalization with much lower memory and storage bud-\ngets.\nAmong the various PEFT approaches, Low Rank\nAdaptation (LoRA) [14] has emerged as a favored method\nfor researchers and practitioners alike due to its versatility.\nLoRA learns low-rank factorized weight matrices for the\nattention layers (these learned weights are themselves com-\nmonly referred to as \u201cLoRAs\u201d). By combining LoRA and\nalgorithms such as DreamBooth [29], the learned subject-\nspecific LoRA weights enable the model to generate the\nsubject with semantic variations.\nWith the growing popularity of LoRA personalization,\nthere have been attempts to merge LoRA weights, specif-\nically by performing a linear combination of subject and\nstyle LoRAs, with variable coefficients [30]. This allows\nfor a control over the \u201cstrength\u201d of each LoRA, and users\nsometimes are able, through careful grid search and sub-\njective human evaluation, to find a combination that allows\nfor accurate portrayal of the subject under the specific style.\nThis method lacks robustness across style and subject com-\nbinations, and is also incredibly time consuming.\nIn our work, we propose ZipLoRA, a simple yet effec-\ntive method to generate any subject in any style by cheaply\nmerging independently trained LoRAs for subject and style.\nOur approach works consistently on a wide variety of sub-\nject and style LoRAs without enforcing any restriction on\nthe way these are trained. This allows users and artists to\neasily combine publicly available subject and style LoRAs\nof their choice. ZipLoRA is hyperparameter-free, i.e. it\ndoes not require manual tuning of any hyperparameters or\nmerger weights.\nOur approach takes advantage of the recently released\nStable Diffusion XL (SDXL) model [27] and is based on\nthree important observations.\n(1) SDXL exhibits strong\nstyle learning properties, comparable to results shown by\nStyleDrop [33] on Muse [3].\nSpecifically, unlike previ-\nous versions of Stable Diffusion, SDXL is able to learn\nstyles using just a single exemplar image by following a\nDreamBooth protocol [29] without any human feedback.\n(2) LoRA weights for different layers \u2206Wi (where i de-\nnotes the layer) are sparse. i.e., most of the elements in\n\u2206Wi have very small magnitude, and have little effect on\ngeneration quality and fidelity. (3) Columns of the weight\nmatrices of two independently trained LoRAs may have\nvarying levels of \u201calignment\u201d between each other, as mea-\nsured by cosine similarity, for example. We find that di-\nrectly summing columns that are highly aligned degrades\nperformance of the merged model.\nBased on these observations, we hypothesize that a\nmethod that operates akin to a zipper, aiming to reduce the\nquantity of similar-direction sums while preserving the con-\ntent and style generation properties of the original LoRAs\nwill yield more robust, higher-quality merges. Much like\na zipper seamlessly joins two sides of a fabric, our pro-\nposed optimization-based approach finds a disjoint set of\nmerger coefficients for blending the two LoRAs. This en-\nsures that the merged LoRA adeptly captures both subject\nand style. Our optimization process is lightweight and sig-\nnificantly improves the merging performance on challeng-\ning content-style combinations, where the two LoRAs are\nhighly aligned.\nWe summarize our contributions as follows:\n\u2022 We demonstrate some key observations about current\ntext-to-image diffusion models and personalization meth-\nods, particularly in relation to style personalization.\nWe further examine the sparsity of concept-personalized\nLoRA weight matrix coefficients and the prevalence and\ndeleterious effect of highly aligned columns for LoRA\nmatrices.\n\u2022 Using these insights we propose ZipLoRA, a simple op-\ntimization method that allows for effective merging of in-\ndependently trained style and subject LoRAs to allow for\nthe generation of any subject in any style. ZipLoRA is a\nfirst exploration into the world of techniques that merge\nLoRAs to achieve new generation capabilities.\n\u2022 We demonstrate the effectiveness of our approach on a\nvariety of image stylization tasks, including content-style\ntransfer and recontextualization.\nWe also demonstrate\nthat ZipLoRA outperforms existing methods of merging\nLoRAs as well as other baseline approaches.\n2. Related Work\nFine-tuning of Diffusion Models for Custom Generation.\nIn the evolving field of text-to-image (T2I) model person-\nalization, recent studies have introduced various methods\nto fine-tune large-scale T2I diffusion models for depict-\ning specific subjects based on textual descriptions. Tech-\nniques like Textual Inversion [8] focus on learning text em-\nbeddings, while DreamBooth [29] fine-tunes the entire T2I\n2\nmodel for better subject representation. Later methods aim\nto optimize specific parts of the networks [11, 20]. Ad-\nditionally, techniques like LoRA [14] and StyleDrop [33]\nconcentrate on optimizing low-rank approximations and a\nsmall subset of weights, respectively, for style personaliza-\ntion. DreamArtist [5] introduces a novel one-shot person-\nalization method using a positive-negative prompt tuning\nstrategy.\nWhile these fine-tuning approaches yield high-\nquality results, they typically are limited to learning only\none concept (either subject or style). One exception is Cus-\ntom Diffusion [20], which attempts to learn multiple con-\ncepts simultaneously. However, Custom Diffusion requires\nexpensive joint training from scratch and still yields inferior\nresults when used for stylization as it fails to disentangle the\nstyle from the subject.\nCombining LoRAs. Combining different LoRAs remain\nunder-explored in the literature particularly from the point\nof view of fusing style and the subject concepts. Ryu [30]\nshows a method to combine independently trained LoRAs\nby weighed arithmetic summation. In [10], authors discuss\nfusing multiple concept LoRAs, however, it is an expensive\nmethod that requires retraining as it does not merge LoRAs\nbut rather re-trains the entire model. A concurrent work dis-\ncusses a strategy to obtain Mixture of Experts by combining\nmultiple LoRAs using a gating function [1].\nImage Stylization. Image-based style transfer is an area\nof research dating back at least 20 years [6, 12]. Great ad-\nvances in arbitrary style transfer was achieved by the con-\nvolutional neural network-based approaches [9, 15, 16, 22,\n26]. Generative models such as GANs [17\u201319] can also be\nused as a prior for image stylization tasks [2, 24, 35]. Many\nrecent GAN-based approaches achieve successful one-shot\nstylizations [4, 7, 21, 23, 25, 32, 36\u201339] by fine-tuning a\npre-trained GAN for a given reference style. However, these\nmethods are limited to images from only a single domain\n(such as faces). Further, most existing GANs do not pro-\nvide any direct, text-based control over the semantics of the\noutput, thus they cannot produce the reference subject in\nnovel contexts. Compared to older generative models, dif-\nfusion models [13, 28, 34] offer superior generation quality\nand text-based control; however, to date, it has been difficult\nto use them for one-shot stylization driven by image exam-\nples. Ours is one of the first works demonstrating the use\nof diffusion models for high-quality example-based styliza-\ntion combined with an ability to re-contextualize to diverse\nscenarios.\n3. Methods\n3.1. Background\nDiffusion Models [13, 28, 34] are state-of-the-art genera-\ntive models known for their high-quality, photorealistic im-\nage synthesis. Their training comprises two phases: a for-\nward process, where an image transitions into a Gaussian\nnoise through incremental Gaussian noise addition, and a\nreverse process, reconstructing the original data from the\nnoise. The reverse process is typically learnt using an U-net\nwith text conditioning support enabling text-to-image gen-\neration at the time of inference. In our work, we focus on\nwidely used latent diffusion model [28] which learns the\ndiffusion process in the latent space instead of image space.\nIn particular, we use Stable Diffusion XL v1 [27] for all our\nexperiments.\nLoRA Fine-tuning.\nLoRA (Low-Rank Adaptation) is a\nmethod for efficient adaptation of Large Language and Vi-\nsion Models to a new downstream task [14, 30]. The key\nconcept of LoRA is that the weight updates \u2206W to the base\nmodel weights W0 \u2208 Rm\u00d7n during fine-tuning have a \u201clow\nintrinsic rank,\u201d thus the update \u2206W can be decomposed\ninto two low-rank matrices B \u2208 Rm\u00d7r and A \u2208 Rr\u00d7n for\nefficient parameterization with \u2206W = BA. Here, r rep-\nresents the intrinsic rank of \u2206W with r << min(m, n).\nDuring training, only A and B are updated to find suit-\nable \u2206W = BA, while keeping W0 constant.\nFor in-\nference, the updated weight matrix W can be obtained as\nW = W0 +BA. Due to its efficiency, LoRA is widely used\nfor fine-tuning open-sourced diffusion models.\nProblem Setup. In this work, we aim to produce accurate\nrenditions of a custom object in a given reference style by\nmerging LoRA weights obtained by separately fine-tuning\na given text-to-image diffusion model on a few reference\nimages of the object/style.\nWe start with a base diffusion model represented as D\nwith pre-trained weights W (i)\n0\nwith i as layer index. One\ncan adapt the base model D to any given concept by simply\nadding the corresponding set of LoRA weights Lx{\u2206W (i)\nx }\nto the model weights. We represent it as: DLx = D \u2295\nLx = W0+\u2206Wx. We drop the superscript (i) for simplicity\nsince our operations are applied over all the LoRA-enabled\nweight matrices of our base model D.\nWe are given two independently trained set of LoRAs\nLc = {\u2206W (i)\nc } and Ls = {\u2206W (i)\ns } for our base model\nD, and we aim to find a merged LoRA Lm = {\u2206W (i)\nm } =\nMerge(Lc, Ls) that can combine the effects of both the in-\ndividual LoRAs in order to stylize the given object in a de-\nsired reference style.\nDirect Merge. LoRA is popularly used as a plug-and-play\nmodule on top of the base model, thus a most common way\nto combine multiple LoRAs is a simple linear combination:\nLm = Lc + Ls =\u21d2 \u2206Wm = wc \u00b7 \u2206Wc + ws \u00b7 \u2206Ws,\n(1)\nwhere wc and ws are coefficients of content and style\nLoRAs, respectively, which allow for a control over the\n\u201cstrength\u201d of each LoRA. For a given subject and style\n3\n-0.001\n3.0\n0.0\nx103\n0.0\n0.001\nDistribution of elements of LoRA update matrix \n% elements \nretained = 100%\nReference Style\nwatercolor painting\n% elements \nretained = 20%\nA teddy bear in watercolor painting style\n% elements \nretained = 10%\nFigure 2. LoRA weight matrices are sparse. Most of the el-\nements in \u2206W have a magnitude very close to zero, and can be\nconveniently thrown away without affecting the generation quality\nof the fine-tuned model.\nLoRA, one may be able to find a particular combination of\nwc and ws that allows for accurate stylization through care-\nful grid search and subjective human evaluation, but this\nmethod is not robust and very time consuming. To this end,\nwe propose a hyperparameter-free approach that does not\nrequire this onerous process.\n3.2. ZipLoRA\nOur approach builds on two interesting insights:\n(1) LoRA update matrices are sparse. We observe that the\nupdate matrices \u2206W for different LoRA layers are sparse,\ni.e., most of the elements in \u2206W have a magnitude very\nclose to zero, and thus have little impact on the output of\nthe fine-tuned model. For each layer, we can sort all the\nelements by their magnitude and zero out the lowest up to\na certain percentile. We depict the distribution of elements\nof \u2206W m\u00d7n\ni\nin Fig. 2, along with samples generated after\nzeroing out 80% and 90% of the lowest-magnitude elements\nof weight update matrix \u2206W for all the layers. As can be\nseen, the model performance is unaffected even when 90%\nof the elements are thrown away. This observation follows\nfrom the fact that the rank of \u2206W is very small by design,\nthus the information contained in most columns of \u2206W is\nredundant.\n(2) Highly aligned LoRA weights merge poorly. Columns\nof the weight matrices of two independently trained LoRAs\nmay contain information that is not disentangled, i.e., the\ncosine similarity between them can be non-zero. We ob-\nserve that the extent of alignment between the columns of\nLoRA weights plays a significant role in determining the\nquality of resulting merge: if we directly add the columns\nwith non-zero cosine similarity to each other, it leads to su-\nperimposition of their information about the individual con-\nA [V] toy in [S] style\nReferences\nA [V] toy\n\u0001at cartoon illustration\nDirect Merge \n0.2 \n0.0 \n0.0 \n0.3 \nOurs\nDirect Merge \nOurs\nLayerwise Mean cosine similarity between \ncolumns of object and style LoRA\nCross-A\u0001n V\nCross-A\u0001n K\nFigure 3. Highly aligned LoRA weights merge poorly.\nWhen\nLoRA weight columns are highly aligned, a direct merge obtains\nsubpar results. Instead, our approach minimizes the mean cosine\nsimilarity between the columns of the LoRA updates across the\nlayers.\ncepts, resulting in the loss of the ability of the merged model\nto synthesize input concepts accurately. We further observe\nthat such loss of information is avoided when the columns\nare orthogonal to each other with cosine similarity equal to\nzero.\nNote that each weight matrix represents a linear trans-\nformation defined by its columns, so it is intuitive that\nthe merger would retain the information available in these\ncolumns only when the columns that are being added are or-\nthogonal to each other. For most content-style LoRA pairs\nthe cosine similarities are non-zero, resulting in signal in-\nterference when they are added directly. In Fig. 3 we show\nthe mean cosine similarity values for each layer of the last\nU-net block for a particular content-style pair before and\nafter applying ZipLoRA. One can see high non-zero cosine\nsimilarity values for the direct merge which results in poor\nstylization quality. On the other hand, ZipLoRA reduces the\nsimilarity values significantly to achieve a superior result.\nTo prevent signal interference during the merger, we\nmultiply each column with a learnable coefficient such that\nthe orthogonality between the columns can be achieved.\nThe fact that LoRA updates are sparse allows us to neglect\ncertain columns from each LoRA, thus facilitating the task\nof minimizing interference. As shown in Fig. 4, we intro-\nduce a set of merger coefficient vectors mc and ms for each\nLoRA layer of the content and style LoRAs, respectively:\nLm = Merge(Lc, Ls, mc, ms)\n=\u21d2 \u2206Wm = mc \u2297 \u2206Wc + ms \u2297 Ws,\n(2)\nwhere \u2297 represents element-wise multiplication between\n\u2206W and broadcasted merger coefficient vector m such that\njth column of \u2206W gets multiplied with jth element of m.\nThe dimensionalities of mc and ms are equal to the number\nof columns in corresponding \u2206W, thus each element of the\nmerger coefficient vector represents the contribution of the\n4\n\u2715\n\u2715\n\u2715\n\u2715\nFigure 4. Overview of ZipLoRA. Our method learns mixing coefficients for each column of \u2206Wi for both style and subject LoRAs.\nIt does so by (1) minimizing the difference between subject/style images generated by the mixed LoRA and original subject/style LoRA\nmodels, while (2) minimizing the cosine similarity between the columns of content and style LoRAs. In essence, the zipped LoRA tries to\nconserve the subject and style properties of each individual LoRA, while minimizing signal interference of both LoRAs.\ncorresponding column of the LoRA matrix \u2206W to the final\nmerge.\nOur ZipLoRA approach has two goals: (1) to minimize\nthe interference between content and style LoRAs, defined\nby the cosine similarity between the columns of content\nand style LoRAs while (2) conserving the capability of the\nmerged LoRA to generate the reference subject and style\nindependently by minimizing the difference between sub-\nject/style images generated by the mixed LoRA and orig-\ninal subject/style LoRAs. To ensure that the columns that\nare merged with each other minimize signal interference,\nour proposed loss seeks to minimize the cosine similarity\nbetween the merge vectors mc and ms of each layer. Mean-\nwhile, we wish to ensure that the original behavior of both\nthe style and the content LoRAs is preserved in the merged\nmodel. Therefore, as depicted in Fig. 4, we formulate an\noptimization problem with following loss function:\nLmerge =\u2225(D \u2295 Lm)(xc, pc) \u2212 (D \u2295 Lc)(xc, pc)\u22252\n+\u2225(D \u2295 Lm)(xs, ps) \u2212 (D \u2295 Ls)(xs, ps)\u22252\n+\u03bb\nX\ni\n|m(i)\nc\n\u00b7 m(i)\ns |,\n(3)\nwhere the merged model Lm is calculated using mc and\nms as per Eq. 2; pc, ps are text conditioning prompts for\ncontent and style references respectively, and \u03bb is an appro-\npriate multiplier for the cosine-similarity loss term. Note\nthat the first two terms ensure that the merged model retains\nthe ability to generate individual style and content, while\nthe third term enforces an orthogonality constraint between\nthe columns of the individual LoRA weights. Importantly,\nwe keep the weights of the base model and the individual\nLoRAs frozen, and update only the merger coefficient vec-\ntors. As seen in the next section, such a simple optimization\nmethod is effective in producing strong stylization of cus-\ntom subjects. Further, ZipLoRA requires only 100 gradient\nupdates which is 10\u00d7 faster compared to joint training ap-\nproaches.\n4. Experiments\nDatasets. We choose a diverse set of content images from\nthe DreamBooth dataset [29], which provides 30 image\nsets each containing 4-5 images of a given subject. Sim-\nilarly, a diverse set of style reference images is selected\nfrom the data provided by authors of StyleDrop [33]. We\nuse only a single image for each style.\nThe attribution\nand licence information for all the content and style im-\nages used are available in the DreamBooth and StyleDrop\nmanuscripts/websites.\nExperimental Setup. We perform all our experiments us-\ning the SDXL v1.0 [27] base model. We use DreamBooth\nfine-tuning with LoRA of rank 64 for obtaining all the style\nand content LoRAs. We update the LoRA weights using\nAdam optimizer for 1000 steps with batch size of 1 and\nlearning rate of 0.00005.\nWe keep the text encoders of\nSDXL frozen during the LoRA fine-tuning. For ZipLoRA,\nwe use \u03bb = 0.01 in Eq. 3 for all our experiments, and run\nthe optimization until cosine similarity drops to zero with a\nmaximum number of gradient updates set to 100.\n4.1. Style-tuning behavior of SDXL model\nAs discussed in Sec. 3, we observe, surprisingly, that a pre-\ntrained SDXL model exhibits strong style learning when\nfine-tuned on only one reference style image.\nWe show\nstyle-tuning results on SDXL model in Fig. 5. For each\nreference image, we apply LoRA fine-tuning of SDXL\nmodel using DreamBooth objective with LoRA rank= 64.\nFor fine-tuning, we follow a similar prompt formation\nas provided in StyleDrop: \u201can <object> in the <style\ndescription> style\u201d. Once fine-tuned, SDXL is able to rep-\nresent diverse set of concepts in the reference style by cap-\n5\nFigure 5. Style Learning using DreamBooth on SDXL. Top:\nSDXL model learns to produce stylized outputs when fine-tuned\non a single example of a reference style using LoRA with a Dream-\nBooth objective. Bottom: The stylizations produced by fine-tuned\nSDXL model are highly competent, compared to those of other\nmodels. Note that unlike StyleDrop, SDXL DreamBooth fine-\ntuning does not require human feedback.\nturing the nuances of painting style, lighting, colors, and\ngeometry accurately. The question of why this model ex-\nhibits this strong style learning performance, as opposed to\nthe lesser performance of previous SD versions [28] (or Im-\nagen [31]) is left open and can have many answers including\ntraining data, model architecture and training schemes.\nWe also provide comparisons with StyleDrop on\nMuse [3], DreamBooth on Imagen, and DreamBooth on\nStable Diffusion in Fig. 5. We observe that SDXL style-\ntuning performs significantly better than the competing\nmethods.\nNote that StyleDrop requires iterative training\nwith human feedback whereas SDXL style-tuning does not.\nThis behavior of SDXL makes it the perfect candidate for\ninvestigating the merging of style LoRAs with subject Lo-\nRAs to achieve personalized stylizations. Thus, we choose\nto use it as a base model for all of our experiments.\n4.2. Personalized Stylizations\nTo start with, we obtain the style LoRAs following the style-\ntuning on SDXL as described in Sec. 4.1, and obtain object\nLoRAs by applying DreamBooth fine-tuning on the subject\nreferences. Fig. 1 and Fig. 6 show the results of our ap-\nproach for combining various style and content LoRAs. Our\nmethod succeeds at both preserving the identity of the refer-\nence subject and capturing the unique characteristics of the\nreference style.\nWe also present qualitative comparisons with other ap-\nproaches in Fig. 6. As a baseline, we compare with the\ndirect arithmetic merge obtained through Eq. 1 with wc and\nws set to 1. Such direct addition results in loss of informa-\ntion captured in each LoRA and produces inferior results\nwith distorted object and/or style.\nWe additionally compare our method with joint training\nof subject and style using a multi-subject variant of Dream-\nBooth with multiple rare unique identifiers. As shown, joint\ntraining fails to learn the disentanglement between object\nand style and produces poor results. It also is the least flex-\nible method since it does not allow the use of pre-trained\nLoRAs, neither can it be used as a style-only or content-\nonly LoRA. Further, it requires 10\u00d7 as many training steps\nas ZipLoRA.\nStyleDrop [33] proposes a DreamBooth+StyleDrop ap-\nproach for achieving personalized stylizations, where a\nStyleDrop method is applied on a DreamBooth model fine-\ntuned on the reference object. Our comparisons show that\nits performance is not ideal, considering the high compute\ncost and human feedback requirements. It also requires ad-\njusting the object and style model weights wc and ws sim-\nilar to the direct merge in order to produce reasonable out-\nputs, while our method is free from any such hyperparame-\nter tuning.\nQuantitative results. We conduct user studies for a quanti-\ntative comparison of our method with existing approaches.\nIn our study, each participant is shown a reference subject\nand a reference style along with outputs of two methods be-\ning compared, in a random order, and asked which output\nbest depicts the reference style while preserving the refer-\nence subject fidelity. We conducted separate user studies\nfor ZipLoRA vs. each of the three competing approaches,\nand received 360 responses across 45 users for each case.\nWe show the results in Tab. 1. As we can see, ZipLoRA re-\nTable 1. User Preference Study. We compare the user preference\nof accurate stylization and subject fidelity between our approach\nand competing methods. Users generally prefer our approach.\n% Preference for ZipLoRA over:\nDirect Merge\nJoint Training\nStyleDrop\n82.7%\n71.1%\n68.0%\nTable 2. Image-alignment and Text-alignment Scores. We com-\npare cosine similarities between CLIP (for style and text) and\nDINO features (for subject) of the output and reference style, sub-\nject, and prompt respectively. ZipLoRA provides superior subject\nand text fidelity while also maintaining the style-alignment.\nZipLoRA Joint Training Direct Merge\nStyle-alignment \u2191\n0.699\n0.680\n0.702\nSubject-alignment \u2191\n0.420\n0.378\n0.357\nText-alignment \u2191\n0.303\n0.296\n0.275\n6\nFigure 6. Qualitative Comparison: We compare samples from our method (Ours), versus direct arithmetic merge, joint training and\nStyleDrop [33]. We observe that our method achieves strong style and subject fidelity that surpasses competing methods.\nceives higher user preference in all three cases owing to its\nhigh-quality stylization while preserving subject integrity.\nFollowing DreamBooth [29], we also provide compar-\nisons using image-alignment and text-alignment scores in\nTab. 2. We employ three metrics: for style-alignment, we\nuse CLIP-I scores of image embeddings of output and the\nstyle reference; for subject-alignment, we employ DINO\nfeatures for the output and the reference subject; and for\ntext-alignment, we use CLIP-T embeddings of the output\nand the text prompt.\nIn all three cases, we use cosine-\nsimilarity as the metric and calculate averages over 4 sub-\njects in 8 styles each.\nZipLoRA results in competitive\nstyle-alignment scores as compared to joint training and di-\nrect merge, while achieving significantly better scores for\nsubject-alignment. This highlights ZipLoRA\u2019s superiority\nin maintaining the subject fidelity. ZipLoRA also outper-\nforms the other two in text-alignment, implying that it pre-\nserves the text-to-image generation capability, and also ex-\npresses the designated style and subject better (since these\nare also part of the text prompt). One should note that these\nmetrics are not perfect, particularly when it comes to mea-\nsuring style alignment, since they lack the ability to cap-\nture subtle stylistic details, and are entangled with semantic\nproperties of images, such as the overall content.\nAbility to re-contextualize. The merged ZipLoRA model\ncan recontextualize reference objects in diverse contexts\nand with semantic modifications while maintaining styliza-\ntion quality. As shown in Fig. 7, our method preserves the\nbase model\u2019s text-to-image generation capabilities while ac-\ncurately stylizing the entire image in the reference style.\nSuch ability is highly valuable in various artistic use cases\nthat requires controlling contexts, subject identities, and\n7\nFigure 7. Our method successfully re-contextualizes the reference subject while preserving the stylization in the given style.\nFigure 8. Our method does not lose the ability to generate individ-\nual concepts, unlike the direct merge approach.\nstyles.\nControlling the extent of stylization. Our optimization-\nbased method directly provides a scalar weight value for\neach column of the LoRA update, thus eliminating a need\nfor tuning and adjustments for obtaining reasonable results.\nHowever, we can still allow the strength of object and style\ncontent to be varied for added controllability. One can at-\ntenuate the style layer weights by multiplying them with an\nadditional scalar multiplier ws to limit the contribution of\nthe style in the final output. As shown in Fig. 9, this al-\nlows for a smooth control over the extent of stylization as\nws varies between 0 to 1.\nAbility to produce the reference object and the style.\nApart from producing accurate stylizations, an ideal LoRA\nmerge should also preserve the ability to generate individ-\nual object and style correctly. This way, a merged LoRA\nmodel can also be used as a replacement of both the indi-\nwatercolor painting\n[V] dog\n[V] dog\nkid crayon drawing\n[V] object in [S] Style\nws = 1.0\nws = 0.7\nws = 0.4\nFigure 9. Style Controllability: Our method works out-of-the-\nbox at achieving good subject and style personalization. Never-\ntheless, varying the merging weights ws allows for controlling the\nextent of stylization.\nvidual LoRAs, or as a Mixture-of-Expert model. As shown\nin Fig. 8, our approach retains the original behavior of both\nthe models and can accurately generate specific structural\nand stylistic elements of each constituent LoRA, while di-\nrect merge fails.\n5. Conclusion\nIn this paper, we have introduced ZipLoRA, a novel\nmethod for seamlessly merging independently trained style\nand subject LoRAs. Our approach unlocks the ability to\ngenerate any subject in any style using sufficiently power-\nful diffusion models like SDXL. By leveraging key insights\nabout pre-trained LoRA weights, we surpass existing meth-\nods for this task. ZipLoRA offers a streamlined, cheap, and\nhyperparameter-free solution for simultaneous subject and\nstyle personalization, unlocking a new level of creative con-\ntrollability for diffusion models.\n8\nAcknowledgements.\nWe thank Prafull Sharma, Meera\nHahn, Jason Baldridge and Dilip Krishnan for helpful\ndiscussions and suggestions.\nWe also thank Kihyuk\nSohn for helping with the generation of StyleDrop re-\nsults.\nReferences\n[1] Anonymous. MoLE: Mixture of loRA experts. In Submitted\nto The Twelfth International Conference on Learning Repre-\nsentations, 2023. under review. 3\n[2] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G. Di-\nmakis.\nCompressed sensing using generative models.\nIn\nICML, 2017. 3\n[3] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 2, 6\n[4] Min Jin Chong and David A. Forsyth. Jojogan: One shot\nface stylization. CoRR, abs/2112.11641, 2021. 3\n[5] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: To-\nwards controllable one-shot text-to-image generation via\npositive-negative prompt-tuning, 2023. 3\n[6] Alexei A Efros and William T Freeman. Image quilting for\ntexture synthesis and transfer. In Proceedings of the 28th an-\nnual conference on Computer graphics and interactive tech-\nniques, pages 341\u2013346, 2001. 3\n[7] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and\nDaniel Cohen-Or. Stylegan-nada: Clip-guided domain adap-\ntation of image generators. ArXiv, abs/2108.00946, 2021.\n3\n[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion, 2022. 2\n[9] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-\nage style transfer using convolutional neural networks. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 2414\u20132423, 2016. 3\n[10] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi,\nChen Yunpeng, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning\nChang, Weijia Wu, Yixiao Ge, Shan Ying, and Mike Zheng\nShou.\nMix-of-show:\nDecentralized low-rank adaptation\nfor multi-concept customization of diffusion models. arXiv\npreprint arXiv:2305.18292, 2023. 3\n[11] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,\nDimitris Metaxas, and Feng Yang.\nSvdiff: Compact pa-\nrameter space for diffusion fine-tuning.\narXiv preprint\narXiv:2303.11305, 2023. 3\n[12] Aaron Hertzmann, Charles E. Jacobs, Nuria Oliver, Brian\nCurless, and D. Salesin. Image analogies. Proceedings of\nthe 28th annual conference on Computer graphics and inter-\nactive techniques, 2001. 3\n[13] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion\nprobabilistic models. ArXiv, abs/2006.11239, 2020. 1, 3\n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In In-\nternational Conference on Learning Representations, 2022.\n2, 3\n[15] Xun Huang and Serge Belongie. Arbitrary style transfer in\nreal-time with adaptive instance normalization. In Proceed-\nings of the IEEE International Conference on Computer Vi-\nsion, pages 1501\u20131510, 2017. 3\n[16] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nEuropean Conference on Computer Vision, 2016. 3\n[17] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\n2019 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 4396\u20134405, 2019. 3\n[18] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,\nJaakko Lehtinen, and Timo Aila. Training generative adver-\nsarial networks with limited data. ArXiv, abs/2006.06676,\n2020.\n[19] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of stylegan. 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n8107\u20138116, 2020. 3\n[20] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In CVPR, 2023. 3\n[21] Gihyun Kwon and Jong-Chul Ye. One-shot adaptation of gan\nin just one clip. ArXiv, abs/2203.09301, 2022. 3\n[22] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu,\nand Ming-Hsuan Yang. Universal style transfer via feature\ntransforms. In Advances in Neural Information Processing\nSystems, 2017. 3\n[23] Mingcong Liu, Qiang Li, Zekui Qin, Guoxin Zhang, Pengfei\nWan, and Wen Zheng. Blendgan: Implicitly gan blending for\narbitrary stylized face generation. ArXiv, abs/2110.11728,\n2021. 3\n[24] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi,\nand Cynthia Rudin. Pulse: Self-supervised photo upsam-\npling via latent space exploration of generative models.\n2020 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 2434\u20132442, 2020. 3\n[25] Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A. Efros,\nYong Jae Lee, Eli Shechtman, and Richard Zhang.\nFew-\nshot image generation via cross-domain correspondence.\n2021 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 10738\u201310747, 2021. 3\n[26] Dae Young Park and Kwang Hee Lee.\nArbitrary style\ntransfer with style-attentional networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5880\u20135888, 2019. 3\n[27] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann, Tim\nDockhorn, Jonas Muller, Joe Penna, and Robin Rombach.\nSdxl: Improving latent diffusion models for high-resolution\nimage synthesis. ArXiv, abs/2307.01952, 2023. 2, 3, 5\n[28] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick\nEsser, and Bj\u00a8orn Ommer. High-resolution image synthesis\nwith latent diffusion models. 2022 IEEE/CVF Conference\n9\non Computer Vision and Pattern Recognition (CVPR), pages\n10674\u201310685, 2021. 1, 3, 6\n[29] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 1, 2, 5, 7\n[30] Simo Ryu.\nMerging loras.\nhttps://github.com/\ncloneofsimo/lora. 2, 3\n[31] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, Seyedeh Sara Mah-\ndavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho,\nDavid J. Fleet, and Mohammad Norouzi. Photorealistic text-\nto-image diffusion models with deep language understand-\ning. ArXiv, abs/2205.11487, 2022. 6\n[32] Viraj Shah, Ayush Sarkar, Sudharsan Krishnakumar Anita,\nand Svetlana Lazebnik.\nMultistylegan: Multiple one-shot\nimage stylizations using a single gan. arXiv, 2023. 3\n[33] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image\ngeneration in any style. arXiv preprint arXiv:2306.00983,\n2023. 1, 2, 3, 5, 6, 7\n[34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. ArXiv, abs/2010.02502, 2020.\n1, 3\n[35] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. To-\nwards real-world blind face restoration with generative facial\nprior. 2021 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 9164\u20139174, 2021. 3\n[36] Yue Wang, Ran Yi, Ying Tai, Chengjie Wang, and Lizhuang\nMa. Ctlgan: Few-shot artistic portraits generation with con-\ntrastive transfer learning. ArXiv, abs/2203.08612, 2022. 3\n[37] Ceyuan Yang, Yujun Shen, Zhiyi Zhang, Yinghao Xu, Jia-\npeng Zhu, Zhirong Wu, and Bolei Zhou. One-shot generative\ndomain adaptation, 2021.\n[38] Shuai Yang, Liming Jiang, Ziwei Liu, and Chen Change\nLoy. Pastiche master: Exemplar-based high-resolution por-\ntrait style transfer, 2022.\n[39] Peihao Zhu, Rameen Abdal, John Femiani, and Peter Wonka.\nMind the gap: Domain gap control for single shot domain\nadaptation for generative adversarial networks. In Interna-\ntional Conference on Learning Representations, 2022. 3\n10\n"
  },
  {
    "title": "Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model",
    "link": "https://arxiv.org/pdf/2311.13231.pdf",
    "upvote": "25",
    "text": "USING HUMAN FEEDBACK TO FINE-TUNE DIFFUSION MODELS\nWITHOUT ANY REWARD MODEL\nKai Yang1\u2217\nJian Tao1\u2217\nJiafei Lyu1\nChunjiang Ge2\nJiaxin Chen3\nQimai Li3\nWeihan Shen3\nXiaolong Zhu3\nXiu Li1\n1Shenzhen International Graduate School, Tsinghua University\n2Department of Automation, Tsinghua University\n3Parametrix Technology Company Ltd.\n{yk22@mails.tsinghua.edu.cn,tj22@mails.tsinghua.edu.cn}\nABSTRACT\nUsing reinforcement learning with human feedback (RLHF) has shown significant promise in fine-\ntuning diffusion models. Previous methods start by training a reward model that aligns with human\npreferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an\nefficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter\ntuning, making the process both time and cost-intensive. The direct preference optimization (DPO)\nmethod, effective in fine-tuning large language models, eliminates the necessity for a reward model.\nHowever, the extensive GPU memory requirement of the diffusion model\u2019s denoising process hinders\nthe direct application of the DPO method. To address this issue, we introduce the Direct Preference for\nDenoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The\ntheoretical analysis demonstrates that although D3PO omits training a reward model, it effectively\nfunctions as the optimal reward model trained using human feedback data to guide the learning\nprocess. This approach requires no training of a reward model, proving to be more direct, cost-\neffective, and minimizing computational overhead. In experiments, our method uses the relative\nscale of objectives as a proxy for human preference, delivering comparable results to methods using\nground-truth rewards. Moreover, D3PO demonstrates the ability to reduce image distortion rates and\ngenerate safer images, overcoming challenges lacking robust reward models. Our code is publicly\navailable in https://github.com/yk7333/D3PO/tree/main\n1\nIntroduction\nRecent advances in image generation models have yielded unprecedented success in producing high-quality images from\ntextual prompts [1, 2, 3]. Diverse approaches, including Generative Adversarial Networks (GANs) [4], autoregressive\nmodels [5, 1, 6, 7, 8, 9], Normalizing Flows [10, 11], and diffusion-based techniques [12, 13, 14, 2], have rapidly\npushed forward the capabilities of these systems. With the proper textual inputs, such models are now adept at crafting\nimages that are not only visually compelling but also semantically coherent, garnering widespread interest for their\npotential applications and implications.\nTo adapt the aforementioned models for specific downstream tasks, such as the generation of more visually appealing\nand aesthetic images, Reinforcement Learning from Human Feedback (RLHF) is commonly employed [15]. This\ntechnique has been successfully used to refine large language models such as GPT [16, 17]. The method is now being\nextended to diffusion models to enhance their performance. One such method, the Reward Weighted Regression\n[18], leverages RLHF to better align generated images with their prompts. An extension of this, the DDPO approach\n[19], further refines this alignment and seeks to improve image complexity, aesthetic quality, and the congruence\nbetween prompt and image. The ReLF technique [20] introduces a novel reward model, dubbed ImageReward, which is\nspecifically trained to discern human aesthetic preferences in text-to-image synthesis. This model is then utilized to\nfine-tune diffusion models to produce images that align more closely with human preferences. Nonetheless, developing\n1*The first two authors contribute equally to this work.\narXiv:2311.13231v2  [cs.LG]  23 Nov 2023\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\na robust reward model for various tasks can be challenging, often necessitating a vast collection of images and abundant\ntraining resources. For example, to diminish the rate of deformities in character images, one must amass a substantial\ndataset of deformed and non-deformed images generated from identical prompts. Subsequently, a network is constructed\nto discern and learn the human preference for non-deformed imagery, serving as the reward model.\nIn the field of natural language processing, Direct Preference Optimization (DPO) has been proposed to reduce training\ncosts [21]. This method forgoes the training of a reward model and directly fine-tunes language models according to\nhuman preferences. However, this straightforward and easy-to-train method encounters challenges when applied to\nfine-tune diffusion models. During the DPO training process, the complete sentence generated by the language model is\ntreated as a single output, necessitating the storage of gradients from multiple forward passes. With diffusion models,\none must store the gradients across multiple latent image representations, which are significantly larger than word\nembeddings, leading to memory consumption that is typically unsustainable.\nTo address the issue of high computational overhead and enable the use of the DPO method to fine-tune diffusion\nmodels directly with human feedback, we conceptualize the denoising process as a multi-step MDP, which utilizes a\npre-trained model to represent an action value function Q that is commonly estimated in RL. We extend the theoretical\nframework of DPO into the formulated MDP, which allows for direct parameter updates at each step of the denoising\nprocess based on human feedback, thereby circumventing the significant computational costs and eliminating the need\nfor a reward model. To the best of our knowledge, this is the first work that forgoes the reward model to fine-tune\ndiffusion models.\nOur main contributions are as follows:\n\u2022 We introduce an innovative approach for fine-tuning diffusion models that could significantly modify the\ncurrent RLHF framework for fine-tuning diffusion models. This method bypasses resource-intensive reward\nmodel training by utilizing direct human feedback, making the process more efficient and cost-effective.\n\u2022 We expand the theoretical framework of DPO into a multi-step MDP, demonstrating that directly updating\nthe policy based on human preferences within an MDP is equivalent to learning the optimal reward model\nfirst and then using it to guide policy updates. This establishes a robust theoretical foundation and provides\nassurance for our proposed method.\n\u2022 In our experiments, we have demonstrated the effectiveness of our method by using human feedback to\nsuccessfully address issues of hand and full-body deformities, enhance the safety of generated images, and\nimprove prompt-image alignment.\n2\nRelated Work\nDiffusion models. Denoising diffusion probabilistic models have quickly become a focal point in the field of generative\nmodels, given their remarkable capability to synthesize diverse data types. Originally introduced in [22] and further\nadvanced by [23], these models have been effectively applied to the generation of intricate images [1, 2], dynamic video\nsequences [24, 25], and complex robotics control systems [26, 27, 28]. The test-to-image diffusion models, in particular,\nhave made it possible to create highly realistic images from textual descriptions [1, 2], opening new possibilities in\ndigital art and design.\nIn pursuit of more precise manipulation over the generative process, recent studies have explored various techniques\nto refine the guidance of diffusion models. Adapters, for instance, have been introduced as a means to incorporate\nadditional input constraints, allowing for more targeted generation that adheres to specific criteria [29]. Furthermore,\ncompositional approaches have been developed to blend multiple models for enhanced image quality and generation\ncontrol [30, 31]. The implementation of classifier [32] and classifier-free guidance [33] has also been a significant step\ntowards achieving more autonomy in the generation process, enabling models to produce outputs that are not only high\nin fidelity but also align more closely with user intentions. In our work, we use Stable Diffusion [14] to generate images\nwith some specific prompts.\nRLHF. RLHF stands as a salient strategy in the domain of machine learning when objectives are complex or difficult\nto define explicitly. This technique has been instrumental across various applications, from gaming, as demonstrated\nwith Atari [15, 34], to more intricate tasks in robotics [35, 36]. The integration of RLHF into the development of large\nlanguage models (LLMs) has marked a significant milestone in the field, with notable models like OpenAI\u2019s GPT-4\n[17], Anthropic\u2019s Claude [37], Google\u2019s Bard [38], and Meta\u2019s Llama 2-Chat [39] leveraging this approach to enhance\ntheir performance and relevance. The effectiveness of RLHF in refining the behavior of LLMs to be more aligned\nwith human values, such as helpfulness and harmlessness, has been extensively studied [34, 35]. The technique has\nalso proven beneficial in more focused tasks, such as summarization, where models are trained to distill extensive\ninformation into concise representations [40].\n2\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nHuman \nfeedback\nA dog is \nwashing dishes\nObjective:\nPrompt-image \nAlignment/\nIncompressity/Safety\n...\nObjective:\nPrompt-image \nAlignment/\nIncompressity/Safety\n...\nDirect preference for Denoising Diffusion Policy Optimization (D3PO)\nxT\n\u2026\nxt\n1\nxt-1\n1\n\u2026\nx0\n1\nPrompts\nxt\n0\nxt-1\n0\n\u2026\nx0\n0\nGood\nBad\npref (xt-1\n1|xt\n1,c)\np\u03b8(xt-1\n0|xt\n0,c)\np\u03b8(xt-1\n1|xt\n1,c)\nStep1: Sampling two images without grad\nStep2: Get human feedback\nStep3: Calculating p\u03b8 and pref \nin each timestep with grad \npref (xt-1\n0|xt\n0,c)\nStep4: Training with D3PO loss \nUpdating parameter \u03b8\nw=1   l=0\nFigure 1: Overview of D3PO. The diffusion model generates two corresponding images based on the provided prompts.\nGuided by specific task requirements\u2014such as improving prompt-image alignment, enhancing image incompressibility,\nor refining aesthetic quality\u2014human evaluators select the preferred image. Leveraging this human feedback, our\nmethod directly updates the diffusion model\u2019s parameters without necessitating the training of a reward model.\nFine-tune Diffusion Models with RL. Before applying diffusion models, data generation has been regarded as a\nsequential decision-making problem and combined with reinforcement learning [41]. More recently, [42] applied\nreinforcement learning to diffusion models to enhance existing fast DDPM samplers [23]. Reward Weighted method\n[18] explored using human feedback to align text-to-image models with RLHF. It uses the reward model for the\ncoefficients of the loss function instead of using the reward model when constructing the dataset. ReFL [20] first trains a\nreward model named ImageReward based on human preferences and then employs it for fine-tuning. During fine-tuning,\nit randomly selects timesteps to predict the final image, aiming to stabilize the training process by preventing it from\nsolely processing the last denoising step. DDPO [19] treats the denoising process of diffusion models as a MDP to\nfine-tune diffusion models with many reward models. DPOK [43] combine the KL divergence into the DDPO loss\nand use it to better align text-to-image objectives. All these models need a robust reward model, which demands a\nsubstantial dataset of images and extensive human evaluations.\nDirect Preference Optimization. In the realm of reinforcement learning, the concept of deriving policies based on\npreferences, as opposed to explicit rewards, has been explored through various methods. The Contextual Dueling Bandit\n(CDB) framework [44, 45] replaces the traditional aim for an optimal policy with the concept of a von Neumann winner,\nthereby circumventing the direct use of rewards. Preference-based Reinforcement Learning (PbRL) [46, 47, 48] is\npredicated on learning from binary preferences inferred from an enigmatic scoring function instead of explicit rewards.\nMore recently, the work of [21] introduced the DPO approach, which directly utilizes preferences for fine-tuning LLMs.\nDPO capitalizes on a correlation between the reward function and optimal policies, showing that this constrained reward\nmaximization challenge can be effectively addressed through a singular phase of policy training.\n3\nPreliminaries\nMDP. We consider the MDP formulation described in [49]. In this setting, an agent perceives a state s \u2208 S and\nexecutes an action, where S, A denote state space and action space, respectively. The transition probability function,\ndenoted by P(s\u2032|s, a), governs the progression from the current state s to the subsequent state s\u2032 upon the agent\u2019s\naction a. Concurrently, the agent is awarded a scalar reward r, determined by the reward function r : S \u00d7 A \u2192 R.\nThe agent\u2019s objective is to ascertain a policy \u03c0(a|s) that maximizes the cumulative returns of trajectories \u03c4 =\n3\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\n(s0, a0, s1, a1, ..., sT \u22121, aT \u22121), which can be represented as:\nJ (\u03c0) = E\u03c4\n\"T \u22121\nX\nt=0\nr (st, at)\n#\n.\n(1)\nDiffusion models. Diffusion models learn to model a probability distribution p(x) by inverting a Markovian forward\nprocess q(xt|xt\u22121) which adds noise to the data. The denoising process is modeled by a neural network to predict the\nmean of xt\u22121 or the noise \u03f5t\u22121 of the forward process. In our work, we use network \u00b5\u03b8(xt; t) to predict the mean of\nxt\u22121 instead of predicting the noise. Using the mean squared error (MSE) as a measure, the objective of this network\ncan be written as:\nL = Et\u223c[1,T ],x0\u223cp(x0),xt\u223cq(xt|x0)[\u2225\u02dc\u00b5(x0, xt) \u2212 \u00b5\u03b8 (xt, t)\u22252],\n(2)\nwhere \u02dc\u00b5\u03b8(xt, x0) is the forward process posterior mean. In the case of conditional generative modeling, the diffusion\nmodels learn to model p(x|c), where c is the conditioning information, i.e., image category and image caption. This is\ndone by adding an additional input c to the denoising neural network, as in \u00b5\u03b8(xt, t; c). To generate a sample from the\nlearned distribution p\u03b8(x|c), we start by drawing a sample xT \u223c N(0, I) and then progressively denoise the sample by\niterated application of \u03f5\u03b8 according to a specific sampler [23]. Given the noise-related parameter \u03c3t, the reverse process\ncan be written as:\np\u03b8 (xt\u22121 | xt, c) = N\n\u0000xt\u22121; \u00b5\u03b8 (xt, c, t) , \u03c32\nt I\n\u0001\n.\n(3)\nReward learning for preferences. The basic framework to model preferences is to learn a reward function r\u2217(s, a)\nfrom human feedback [50, 51, 15]. The segment \u03c3 = {sk, ak, sk+1, ak+1, ..., sm, am} is a sequence of observations\nand actions. By following the Bradley-Terry model [52], the human preference distribution p\u2217 by using the reward\nfunction can be expressed as:\np\u2217(\u03c31 \u227b \u03c30) =\nexp(PT\nt=k r\u2217(s1\nt, a1\nt))\nP\ni\u2208{0,1} exp(PT\nt=k r\u2217(si\nt, ai\nt))\n,\n(4)\nwhere \u03c3i \u227b \u03c3j denotes that segment i is preferable to segment j. Now we have the preference distribution of human\nfeedback, and we want to use a network r\u03d5 to approximate r\u2217. Given the human preference y \u2208 {(1, 0), (0, 1)} which\nis recorded in dataset D as a triple (\u03c30, \u03c31, y), framing the problem as a binary classification, the reward function\nmodeled as a network is updated by minimizing the following loss:\nL(\u03d5) = \u2212E(\u03c31,\u03c30,y)\u223cD[y(0) log p\u03d5(\u03c30 \u227b \u03c31) + y(1) log p\u03d5(\u03c31 \u227b \u03c30)].\n(5)\n4\nMethod\nIn this section, we describe a method to directly fine-tune diffusion models using human feedback, bypassing the\nconventional requirement for a reward model. Initially, we reinterpret the denoising process inherent in diffusion models\nas a multi-step MDP. Then we extend the theory of DPO to MDP, which allows us to apply the principles to effectively\ntranslate human preferences into policy improvements in diffusion models.\n4.1\nDenoising process as a multi-step MDP\nWe conceptualize the denoising process within the diffusion model as a multi-step MDP, which varies slightly from the\napproach outlined in [19]. To enhance clarity, we have redefined the states, transition probabilities, and policy functions.\nThe correspondence between notations in the diffusion model and the MDP is established as follows:\nst \u225c (c, t, xT \u2212t)\nP (st+1 | st, at) \u225c\n\u0000\u03b4c, \u03b4t+1, \u03b4xT \u22121\u2212t\n\u0001\nat \u225c xT \u22121\u2212t\n\u03c0 (at | st) \u225c p\u03b8 (xT \u22121\u2212t | xT \u2212t, c)\n\u03c10 (s0) \u225c (p(c), \u03b40, N(0, I))\nr(st, at) \u225c r((c, t, xT \u2212t) , xT \u2212t\u22121)\nwhere \u03b4x represents the Dirac delta distribution, and T denotes the maximize denoising timesteps. Leveraging this\nmapping, we can employ RL techniques to fine-tune diffusion models by maximizing returns. However, this approach\nrequires a proficient reward model capable of adequately rewarding the noisy images. The task becomes exceptionally\nchallenging, particularly when t is low, and xT \u2212t closely resembles Gaussian noise, even for an experienced expert.\n4\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\n4.2\nDirect Preference Optimization for MDP\nThe DPO method does not train a separate reward model but instead directly optimizes the LLMs with the preference\ndata. Given a prompt x and a pair of answers (y1, y0) \u223c \u03c0ref(y|x), where \u03c0ref represents the reference (pre-trained)\nmodel, these responses are ranked and stored in D as a tuple (x, yw, yl), where yw denotes the preferred answer and yl\nindicates the inferior one. DPO optimizes \u03c0\u03b8 with the human preference dataset by using the following loss:\nLDPO(\u03b8) = \u2212E(x,yw,yl)\u223cD\n\u0014\nlog \u03c1\n\u0012\n\u03b2 log \u03c0\u03b8 (yw | x)\n\u03c0ref (yw | x) \u2212 \u03b2 log \u03c0\u03b8 (yl | x)\n\u03c0ref (yl | x)\n\u0013\u0015\n.\n(6)\nHere \u03c1 is the logistic function, and \u03b2 is the parameter controlling the deviation from the \u03c0\u03b8 and \u03c0ref. In our framework,\nwe treat segments \u03c31, \u03c30 as y1, y0 and use DPO to fine-tune diffusion models. However, directly using this method faces\ndifficulties since the segments contain a large number (usually 20\u201350) of the image latent, which occupy a large amount\nof GPU memory (each image is about 6G even when using LoRA [53]). Since we can only get human preferences\nfor the final image x0, if we want to update \u03c0\u03b8(\u03c3) = QT\nt=k \u03c0\u03b8(st, at), it will consume more than 100G GPU memory,\nwhich makes the fine-tuning process nearly impossible.\nTo address this problem, we extend the DPO theory to MDP. Firstly, we need to reconsider the objective of the RL\nmethod. For the MDP problem, the agents take action by considering maximizing the expected return instead of the\ncurrent reward. For actor-critic methods such as DDPG [54], the optimization objective of policy \u03c0 gives:\nmax\n\u03c0\nEs\u223cd\u03c0,a\u223c\u03c0(\u00b7|s)[Q\u2217(s, a)].\n(7)\nHere, d\u03c0 = (1 \u2212 \u03b3) P\u221e\nt=0 \u03b3tP \u03c0\nt (s) represents the state visitation distribution, where P \u03c0\nt (s) denotes the probability of\nbeing in state s at timestep t given policy \u03c0. Additionally, Q\u2217(s, a) denotes the optimal action-value function. The\noptimal policy can be written as:\n\u03c0\u2217(a|s) =\n\u001a1,\nif a = arg max\u02c6a Q\u2217(s, \u02c6a),\n0,\notherwise.\n(8)\nSimilar to some popular methods, we use KL-divergence to prevent the fine-tuned policy from deviating from the\nreference policy, hence relieving the out-of-distribution (OOD) issue. By incorporating this constraint, the RL objective\ncan be rewritten as:\nmax\n\u03c0\nEs\u223cd\u03c0,a\u223c\u03c0(\u00b7|s)[Q\u2217(s, a)] \u2212 \u03b2DKL[\u03c0(a|s)\u2225\u03c0ref(a|s)].\n(9)\nHere, \u03b2 is the temperature parameter that controls the deviation of \u03c0\u03b8(a|s) and \u03c0ref(a|s).\nProposition 1 Given the objective of eq. (9), the optimal policy \u03c0\u2217(a|s) has the following expression:\n\u03c0\u2217(a|s) = \u03c0ref(a|s) exp( 1\n\u03b2 Q\u2217(s, a)).\n(10)\nThe proof can be seen in Appendix B.1. By rearranging the formula of eq. (10), we can obtain that:\nQ\u2217(s, a) = \u03b2 log \u03c0\u2217(a|s)\n\u03c0ref(a|s).\n(11)\nNow, considering eq. (4) and noticing that Q\u2217(st, at) = E\nhPT\nt=k r\u2217(st, at)\ni\nunder the policy \u03c0\u2217(a|s), we make a\nsubstitution. By replacing PT\nt=k r\u2217(st, at) with Q\u2217(st, at), we define a new distribution that can be rewritten as:\n\u02dcp\u2217(\u03c31 \u227b \u03c30) =\nexp(Q\u2217(s1\nk, a1\nk))\nP\ni\u2208{0,1} exp(Q\u2217(si\nk, ai\nk)).\n(12)\nWe suppose Pm\nt=k r\u2217(st, at) is sampled from a normal distribution with mean E[Pm\nt=k r\u2217(st, at)] and standard\ndeviation \u03c32. From a statistical perspective, we can establish the relationship between the new distribution \u02dcp\u2217(\u03c31 \u227b \u03c30)\nand the raw distribution p\u2217(\u03c31 \u227b \u03c30).\nProposition 2 For i \u2208 {0, 1}, suppose the expected return satisfies a normal distribution, i.e., PT\nt=0 r\u2217 \u0000si\nt, ai\nt\n\u0001\n\u223c\nN\n\u0000Q\u2217(si\n0, ai\n0), \u03c32\u0001\n. Given Q\u2217(s, a) \u2208 [Qmin, Qmax] where Qmin and Qmax represent the lower and upper bounds of\nthe values, then\n|p\u2217 (\u03c31 \u227b \u03c30) \u2212 \u02dcp\u2217 (\u03c31 \u227b \u03c30)| < (\u03be2 + 1)(exp(\u03c32) \u2212 1)\n16\u03be\u03b4\nwith probability at least 1 \u2212 \u03b4, where \u03be = exp(Qmax)\nexp(Qmin) .\n5\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nThe proof can be seen in Appendix B.2. In practical applications, as the volume of data increases, it becomes easier to\nsatisfy the assumption of normality. Additionally, we can use clipping operations to constrain the Q values within a\ncertain range, which introduces upper and lower bounds. Therefore, the aforementioned assumption is reasonable. As\nshown in proposition 2, their deviation can be bounded at the scale of O(\u03be\n\u03b4 (exp(\u03c32) \u2212 1)). It is clear that this bound\ncan approach 0 if the \u03c32 is close to 0. In practice, \u03c32 approaches 0 if the Q function can be estimated with a small\nstandard deviation.\nBy combining eq. (12), eq. (5), and eq. (11), replacing p\u2217 (\u03c31 \u227b \u03c30) with \u02dcp\u2217 (\u03c31 \u227b \u03c30), and substituting \u03c0\u2217(s, a) with\nthe policy network \u03c0\u03b8(s, a) that requires learning, we derive the following loss function for updating \u03c0\u03b8(a|s):\nL(\u03b8) = \u2212E(sk,\u03c3w,\u03c3l)[log \u03c1(\u03b2 log \u03c0\u03b8(aw\nk |sw\nk )\n\u03c0ref(aw\nk |sw\nk ) \u2212 \u03b2 log \u03c0\u03b8(al\nk|sl\nk)\n\u03c0ref(al\nk|sl\nk))],\n(13)\nwhere \u03c3w = {sw\nk , aw\nk , sw\nk+1, aw\nk+1, ..., sw\nT , aw\nT } denotes the segment preferred over another segment \u03c3l =\n{sl\nk, al\nk, sl\nk+1, al\nk+1, ..., sl\nT , al\nT }.\n4.3\nDirect preference for Denoising Diffusion Policy Optimization\nConsidering the denoising process as a multi-step MDP and using the mapping relationship depicted in Section 4.1, we\ncan use DPO to directly update diffusion models by using eq. (13). In the denoising process, we set k = 0 and T = 20.\nWe first sample an initial state sw\n0 = sl\n0 = s0 and then use eq. (3) to generate two segments. After manually choosing\nwhich segment is better, the probability of \u03c0\u03b8(aw\n0 |sw\n0 ) is gradually increasing and \u03c0\u03b8(al\n0|sl\n0) is decreasing, which guides\nthe diffusion model to generate images of human preference. However, the approach of only updating \u03c0\u03b8(\u00b7|s0) does not\nfully utilize the information within the segment.\nSince the middle states of the segment are noises and semi-finished images, it is hard for humans to judge which\nsegment is better by observing the whole segment. But we can conveniently compare the final image x0. Like many RL\nmethods [55, 56, 57] which give rewards by \u2200st, at \u2208 \u03c3, r(st, at) = 1 for winning the game and \u2200t \u2208 \u03c3, r(st, at) = \u22121\nfor losing the game, we also assume that if the segment is preferred, then any state-action pair of the segment is better\nthan the other segment. By using this assumption, we construct T sub-segments for the agent to learn, which can be\nwritten as:\n\u03c3i = {si, ai, si+1, ai+1, ..., sT \u22121, aT \u22121}.\n0 \u2264 i \u2264 T \u2212 1\n(14)\nUsing these sub-segments, the overall loss of the D3PO algorithm gives:\nLi(\u03b8) = \u2212E(si,\u03c3w,\u03c3l)[log \u03c1(\u03b2 log \u03c0\u03b8(aw\ni |sw\ni )\n\u03c0ref(aw\ni |sw\ni ) \u2212 \u03b2 log \u03c0\u03b8(al\ni|sl\ni)\n\u03c0ref(al\ni|sl\ni))],\n(15)\nwhere i \u2208 [0, T \u2212 1]. Compared to eq. (13), eq. (15) uses every state-action pair for training, effectively increasing the\ndata utilization of the segment by a factor of T.\nThe overview of our method is shown in fig. 1. The pseudocode of D3PO can be seen in Appendix A.\n5\nExperiment\nIn our experiments, we evaluate the effectiveness of D3PO in fine-tuning diffusion models. Initially, we conduct\ntests on measurable objectives to verify if D3PO can increase these metrics, which quickly ascertain the algorithm\u2019s\neffectiveness by checking for increases in the target measures. Next, we apply D3PO to experiments aimed at lowering\nthe rate of deformities in hands and full-body images generated by diffusion models. Moreover, we utilize our method to\nincrease image safety and enhance the concordance between generated images and their corresponding prompts. These\ntasks pose considerable obstacles for competing algorithms, as they often lack automated capabilities for detecting\nwhich image is deformed or safe, thus relying heavily on human evaluation. We use Stable Diffusion v1.5 [13] to\ngenerate images in most of the experiments.\n5.1\nPre-defined Quantifiable Objectives Experiments\nWe initially conduct experiments with D3PO using quantitative objectives (alternatively referred to as optimal reward\nmodels). In these experiments, the relative values of the objectives (rewards) are used instead of human preference\nchoices. Preferences are established based on these objectives, meaning A is preferred if its objective surpasses that of\nB. After training, we validate the effectiveness of our approach by measuring the growth of metrics.\n6\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nFigure 2: Progression of samples targeting compressibility, incompressibility, and aesthetic quality objectives. With\nthe respective focus during training, images exhibit reduced detail and simpler backgrounds for compressibility, richer\ntexture details for incompressibility, and an overall increased aesthetic appeal when prioritizing aesthetic quality.\nIn our experimental setup, we compare against the DDPO method [19], which requires a reward model. Note that\nwe only used the relative sizes of the rewards corresponding to the objectives for preference choices, rather than the\nrewards themselves, whereas the DDPO method used standard rewards during training. During testing, we used the\nrewards corresponding to the objectives as the evaluation criterion for both methods. This generally ensures a fair and\nunified comparison of fine-tuning with and without the reward model.\nWe first use the size of the images to measure the preference relationship between two pictures. For the compressibility\nexperiment, an image with a smaller image size is regarded as better. Conversely, for the incompressibility experiment,\nwe consider larger images to be those preferred by humans. As the training progresses, we can obtain the desired highly\ncompressible and low compressible images. We then utilize the LAION aesthetics predictor [58] to predict the aesthetic\nrating of images. This model can discern the aesthetic quality of images, providing a justifiable reward for each one\nwithout requiring human feedback. The model can generate more aesthetic images after fine-tuning. We conducted a\ntotal of 400 epochs during the training process, generating 80 images in each epoch. The progression of the training\nsamples is visually presented in Figure 2. More quantitative samples are shown in Figure 8. The training curves of\nD3PO and DDPO are shown in Figure 3. We are surprised to find that the D3PO method, which solely relies on relative\nsizes for preference choices, achieves results nearly on par with the DDPO method trained using standard rewards,\ndelivering comparable performance. This demonstrates that even in the presence of a reward model, our method can\neffectively fine-tune the diffusion model, continually increasing the reward to achieve the desired results.\nFigure 3: Training curves of the DDPO and D3PO methods. The rewards denote image size for incompressity objective,\nnegative image size for the compressity objective, and the LAION aesthetic score for the aesthetic objective. Each\nexperiment was conducted with 5 different seeds.\n7\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nFigure 4: Normalcy rate of hands images and deformed rate of anime characters training curves.\n5.2\nExperiments without Any Reward Model\nWe conduct experiments for some objectives without any reward model. We can only judge manually whether an image\nis deformed or if the image is safe without a predefined reward model. After training, the model from each epoch\nserves as the reference for the subsequent epoch. For each prompt, we generate 7 images with the same initial state\nxT \u223c N(0, I).\n5.2.1\nReduce Image Distortion\nWe use the prompt \u201c1 hand\u201d to generate images, manually selecting those that are deformed. Diffusion models often\nstruggle to produce aesthetically pleasing hands, resulting in frequent deformities in the generated images. In this\nexperiment, we focus on the normalcy rate of the images instead of the deformity rate, as depicted in Figure 4. We\ncategorize 1,000 images for each epoch, over a total of five epochs, and track the normalcy rate of these hand images.\nAfter fine-tuning, the model shows a marked reduction in the deformity rate of hand images, with a corresponding\nincrease in the production of normal images. Additionally, the fine-tuned model shows a higher probability of generating\nhands with the correct number of fingers than the pre-trained model, as demonstrated in Figure 9.\nTo assess the generality of our method, we generated images with the Anything v5 model 1, renowned for creating\nanime character images. With Anything v5, there\u2019s a risk of generating characters with disproportionate head-to-body\nratios or other deformities, such as an incorrect number of limbs (as shown in Figure 6 left). We categorize such outputs\nas deformed. We assume that non-selected images are more favorable than the deformed ones, though we do not rank\npreferences within the deformed or non-deformed sets. The diminishing distortion rates across epochs are illustrated in\nFigure 4, showing a significant decrease initially that stabilizes in later epochs. The visual examples are provided in\nFigure 6.\nFigure 5: Safety rate curves of the training procession.\n5.2.2\nEnhance Image Safety\nIn this experiment, we utilized unsafe prompts to generate images using a diffusion model. These prompts contained\nedgy terms that could lead to the creation of both normal and Not Safe for Work (NSFW) images, examples being\n\u2018ambiguous beauty\u2019 and \u2018provocative art\u2019. The safety of the images was assessed via human annotations, and the\ndiffusion model was fine-tuned based on these feedbacks. Across 10 epochs, we generated 1,000 images per epoch.\n1https://huggingface.co/stablediffusionapi/anything-v5\n8\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nFigure 6: Image samples of the pre-trained model and the fine-tuned model. The images on the left side of the arrows\nare distorted images (such as having 3 legs in the image) generated by the pre-trained model, while the images on the\nright side are normal images generated after fine-tuning the model. Both sets of images used the same initial Gaussian\nnoise, prompts, and seeds.\nGiven the relatively minor variations in human judgments about image safety, we engaged just two individuals for\nthe feedback task\u2014one to annotate and another to double-check. The image safety rate during the training process is\nillustrated in Figure 5. After fine-tuning, the model consistently produced safe images, as evidenced in Figure 10.\n5.2.3\nPrompt-Image Alignment\nWe employ human feedback to evaluate the alignment preference between two images generated from each prompt. For\neach epoch, we use 4,000 prompts, generate two images per prompt, and assess preferences with feedback from 16\ndifferent evaluators. The training spans 10 epochs. The preference comparisons between images from the pre-trained\nand fine-tuned models are conducted by an additional 7 evaluators, with the comparative preferences depicted in Figure\n7. We also execute quantitative evaluations of models using metrics that measure the congruence between prompts and\nimages, including CLIP [59], BLIP [60], and ImageReward [20], as presented in Table 1.\nFigure 7: Comparative evaluation of 1,000 text prompts: Our study involved generating images from two sources\u2014our\nfine-tuned model and a pre-trained diffusion model\u2014using the same text prompts. For each prompt, human evaluators\nare tasked with determining which image is better aligned with the text or marking a tie if both images are similarly\naccurate. Each image was assessed by 7 human raters, and we report the percentage of images that received favorable\nevaluations. We also highlight the percentage with more than half vote in the black box.\n6\nConclusion\nIn this paper, we propose a direct preference denoising diffusion policy optimization method, dubbed D3PO, to fine-tune\ndiffusion models purely from human feedback without learning a separate reward model. D3PO views the denoising\nprocess as a multi-step MDP, making it possible to utilize the DPO-style optimization formula by formulating the\n9\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nTable 1: The quantitative metric evaluation of the pre-trained diffusion model and the D3PO fine-tuned model. The\npercentage of human preferences is calculated based on the relative size of the votes cast. Our method significantly\nimproves this quantitative metric after using human feedback of prompt-image alignment.\nModel\nCLIP score BLIP score ImageReward score Human preference\nPre-trained\n30.7\n1.95\n0.04\n8.4%\nD3PO fine-tuned\n31.9\n2.06\n0.27\n86.8%\naction-value function Q with the reference model and the fine-tuned model. D3PO updates parameters at each step of\ndenoising and consumes much fewer GPU memory overheads than directly applying the DPO algorithm. The empirical\nexperiments illustrate that our method achieves competitive or even better performance compared with a diffusion\nmodel fine-tuned with a reward model that is trained with a large amount of images and human preferences in terms\nof image compressibility, image compressibility and aesthetic quality. We further show that D3PO can also benefit\nchallenging tasks such as reducing image distortion rates, enhancing the safety of the generated images, and aligning\nprompts and images.\nReferences\n[1] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u2013\n8831. PMLR, 2021.\n[2] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.\n[3] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and\nimproving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 8110\u20138119, 2020.\n[4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.\n[5] A\u00e4ron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International\nconference on machine learning, pages 1747\u20131756. PMLR, 2016.\n[6] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural\nInformation Processing Systems, 34:19822\u201319835, 2021.\n[7] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via\nhierarchical transformers. Advances in Neural Information Processing Systems, 35:16890\u201316902, 2022.\n[8] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In European Conference on Computer Vision, pages\n89\u2013106. Springer, 2022.\n[9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.\n[10] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference\non machine learning, pages 1530\u20131538. PMLR, 2015.\n[11] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint\narXiv:1605.08803, 2016.\n[12] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\nand Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.\narXiv preprint arXiv:2112.10741, 2021.\n[13] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 10684\u201310695, June 2022.\n[14] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n10\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\n[15] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement\nlearning from human preferences. Advances in neural information processing systems, 30, 2017.\n[16] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances\nin neural information processing systems, 33:1877\u20131901, 2020.\n[17] OpenAI. Gpt-4 technical report, 2023.\n[18] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad\nGhavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint\narXiv:2302.12192, 2023.\n[19] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with\nreinforcement learning. arXiv preprint arXiv:2305.13301, 2023.\n[20] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward:\nLearning and evaluating human preferences for text-to-image generation. arXiv preprint arXiv:2304.05977, 2023.\n[21] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct\npreference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290,\n2023.\n[22] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR,\n2015.\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems, 33:6840\u20136851, 2020.\n[24] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion\nmodels. arXiv preprint arXiv:2210.02303, 2022.\n[25] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron\nAshual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint\narXiv:2209.14792, 2022.\n[26] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior\nsynthesis. In International Conference on Machine Learning, pages 9902\u20139915. PMLR, 2022.\n[27] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum, Tommi S Jaakkola, and Pulkit Agrawal. Is conditional\ngenerative modeling all you need for decision making? In The Eleventh International Conference on Learning\nRepresentations, 2022.\n[28] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion\npolicy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023.\n[29] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.\n[30] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation\nwith composable diffusion models. In European Conference on Computer Vision, pages 423\u2013439. Springer, 2022.\n[31] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-\nDickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with\nenergy-based diffusion models and mcmc. In International Conference on Machine Learning, pages 8489\u20138510.\nPMLR, 2023.\n[32] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural\ninformation processing systems, 34:8780\u20138794, 2021.\n[33] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021.\n[34] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav\nFort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning\nfrom human feedback. arXiv preprint arXiv:2204.05862, 2022.\n[35] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and\nGeoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.\n11\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\n[36] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my Scheurer, Javier Rando, Rachel\nFreedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of\nreinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023.\n[37] Anthropic. Introducing claude, 2023.\n[38] Google. Bard, 2023.\n[39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[40] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario\nAmodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information\nProcessing Systems, 33:3008\u20133021, 2020.\n[41] Philip Bachman and Doina Precup. Data generation as sequential decision making. Advances in Neural Information\nProcessing Systems, 28, 2015.\n[42] Ying Fan and Kangwook Lee.\nOptimizing ddpm sampling with shortcut fine-tuning.\narXiv preprint\narXiv:2301.13362, 2023.\n[43] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad\nGhavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image\ndiffusion models. arXiv preprint arXiv:2305.16381, 2023.\n[44] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem.\nJournal of Computer and System Sciences, 78(5):1538\u20131556, 2012. JCSS Special Issue: Cloud Computing 2011.\n[45] Miroslav Dud\u00edk, Katja Hofmann, Robert E Schapire, Aleksandrs Slivkins, and Masrour Zoghi. Contextual dueling\nbandits. In Conference on Learning Theory, pages 563\u2013587. PMLR, 2015.\n[46] R\u00f3bert Busa-Fekete, Bal\u00e1zs Sz\u00f6r\u00e9nyi, Paul Weng, Weiwei Cheng, and Eyke H\u00fcllermeier. Preference-based\nreinforcement learning: evolutionary direct policy search using a preference-based racing algorithm. Machine\nlearning, 97:327\u2013351, 2014.\n[47] Aadirupa Saha, Aldo Pacchiano, and Jonathan Lee. Dueling rl: Reinforcement learning with trajectory preferences.\nIn Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, Proceedings of The 26th International\nConference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research,\npages 6263\u20136289. PMLR, 25\u201327 Apr 2023.\n[48] Runze Liu, Yali Du, Fengshuo Bai, Jiafei Lyu, and Xiu Li. Zero-shot preference learning for offline rl via optimal\ntransport. arXiv preprint arXiv:2306.03615, 2023.\n[49] Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning. 1998.\n[50] Aaron Wilson, Alan Fern, and Prasad Tadepalli. A bayesian approach for policy learning from trajectory preference\nqueries. Advances in neural information processing systems, 25, 2012.\n[51] Kimin Lee, Laura Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive reinforcement learning via\nrelabeling experience and unsupervised pre-training. arXiv preprint arXiv:2106.05091, 2021.\n[52] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired\ncomparisons. Biometrika, 39(3/4):324\u2013345, 1952.\n[53] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n[54] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and\nDaan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n[55] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with\ndeep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.\n[56] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas\nHubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge.\nnature, 550(7676):354\u2013359, 2017.\n[57] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):885\u2013890, 2019.\n12\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\n[58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for\ntraining next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294,\n2022.\n[59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language\nsupervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n[60] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation. In International Conference on Machine Learning, pages\n12888\u201312900. PMLR, 2022.\n[61] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\n13\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nA\nD3PO Pseudo-code\nThe pseudocode of the D3PO method can be seen in Algorithm 1.\nB\nProof\nB.1\nProof of Proposition 1\nThe RL objective can be written as:\nmax\n\u03c0\nEs\u223cd\u03c0,a\u223c\u03c0(a|s)[Q\u2217(s, a)] \u2212 \u03b2DKL[\u03c0(a|s)\u2225\u03c0ref(a|s)]\n= max\n\u03c0\nEs\u223cd\u03c0,a\u223c\u03c0(a|s)[Q\u2217(s, a) \u2212 \u03b2 log \u03c0(a|s)\n\u03c0ref(a|s)]\n= min\n\u03c0 Es\u223cd\u03c0,a\u223c\u03c0(a|s)[log \u03c0(a|s)\n\u03c0ref(a|s) \u2212 1\n\u03b2 Q\u2217(s, a)]\n= min\n\u03c0 Es\u223cd\u03c0,a\u223c\u03c0(a|s)[log\n\u03c0(a|s)\n\u03c0ref(a|s) exp( 1\n\u03b2 Q\u2217(s, a))\n]\n= min\n\u03c0 Es\u223cd\u03c0[DKL[\u03c0(a|s)\u2225\u02dc\u03c0(a|s)]]\nwhere \u02dc\u03c0(a|s) = \u03c0ref(a|s) exp( 1\n\u03b2 Q\u2217(s, a)). Note that the KL-divergence is minimized at 0 iff the two distributions are\nidentical, so the optimal solution is:\n\u03c0(a|s) = \u02dc\u03c0(a|s) = \u03c0ref(a|s) exp( 1\n\u03b2 Q\u2217(s, a)).\nB.2\nProof of Proposition 2\nFor simplicity, we define Qi = Q\u2217(si\n0, ai\n0) and Xi = PT\nt=0 r\u2217 \u0000si\nt, ai\nt\n\u0001\ni \u2208 {0, 1}. Using the eq. (4) we can obtain\nthat:\nE[p\u2217 (\u03c31 \u227b \u03c30)] =\nE[exp(X1)]\nE[exp(X1) + exp(X0)\n=\nexp(Q1 + 1/2\u03c3)\nexp(Q1 + 1/2\u03c3) + exp(Q0 + 1/2\u03c3)\n=\nexp(Q1)\nexp(Q1) + exp(Q0)\n= E[\u02dcp\u2217 (\u03c31 \u227b \u03c30)].\nE[(p\u2217 (\u03c31 \u227b \u03c30))2] =\nE[exp(2X1)]\nE[exp(2X1)] + E[exp(2X0)] + E[2 exp(X0) exp(X1)]\n=\nexp(2Q1 + 2\u03c32)\nexp(2Q1 + 2\u03c32) + exp(2Q0 + 2\u03c32) + exp(Q0 + Q1 + \u03c32)\n=\nexp(2Q1 + \u03c32)\nexp(2Q1 + \u03c32) + exp(2Q0 + \u03c32) + 2 exp(Q0 + Q1).\nVar[p\u2217(\u03c31 \u227b \u03c30)] = E[(p(\u03c31 \u227b \u03c30))2] \u2212 (E[p(\u03c31 \u227b \u03c30)])2\n=\n2 exp(3Q1 + Q0)(exp(\u03c32) \u2212 1)\n[exp(2Q1 + \u03c32) + exp(2Q0 + \u03c32) + 2 exp(Q0 + Q1)][exp(Q1) + exp(Q0)]2\n\u2264 2 exp(3Q1 + Q0)(exp(\u03c32) \u2212 1)\n[exp(Q1) + exp(Q0)]4\n.\n14\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nAlgorithm 1 D3PO pseudo-code\nRequire: Number of inference timesteps T, number of training epochs N, number of prompts\nper epoch K, pre-trained diffusion model \u03f5\u03b8.\n1: Copy a pre-trained diffusion model \u03f5ref = \u03f5\u03b8.\n2: Set \u03f5ref with requires_grad to False.\n3: for n = 1 : N do\n4:\n# Sample images\n5:\nfor k = 1 : K do\n6:\nRandom choose a prompt ck and sample xT \u223c N(0, I)\n7:\nfor i = 0 : 1 do\n8:\nfor t = T : 1 do\n9:\nno grad:\n10:\nxi\nk,t\u22121 = \u00b5(xi\nk,t, t, ck) + \u03c3tz\nz \u223c N(0, I)\n11:\nend for\n12:\nend for\n13:\nend for\n14:\n# Get Human Feedback\n15:\nfor k = 1 : K do\n16:\nGet human feedback from ck, x0\nk,0, and x1\nk,0.\n17:\nif x0\n0 is better than x1\n0 then\n18:\nhk = [1, \u22121]\n19:\nelse if x0\n1 is better than x0\n0 then\n20:\nhk = [\u22121, 1]\n21:\nelse\n22:\nhk = [0, 0]\n23:\nend if\n24:\nend for\n25:\n# Training\n26:\nfor k = 1 : K do\n27:\nfor i = 0 : 1 do\n28:\nfor t = T : 1 do\n29:\nwith grad:\n30:\n\u00b5\u03b8(xi\nk,t, t, ck) =\n1\n\u221a\u03b1t\n\u0010\nxi\nk,t \u2212\n\u03b2t\n\u221a1\u2212\u00af\u03b1t\u03f5\u03b8\n\u0000xi\nk,t, t, ck\n\u0001\u0011\n31:\n\u00b5ref(xi\nk,t, t, ck) =\n1\n\u221a\u03b1t\n\u0010\nxi\nk,t \u2212\n\u03b2t\n\u221a1\u2212\u00af\u03b1t\u03f5ref\n\u0000xi\nk,t, t, ck\n\u0001\u0011\n32:\n\u03c0\u03b8(xi\nk,t\u22121|xi\nk,t, t, ck) =\n1\n\u221a\n2\u03c0\u03c3t exp(\u2212\n(xi\nk,t\u22121\u2212\u00b5\u03b8(xi\nk,t,t,ck))2\n2\u03c32\nt\n)\n33:\n\u03c0ref(xi\nk,t\u22121|xi\nk,t, t, ck) =\n1\n\u221a\n2\u03c0\u03c3t exp(\u2212\n(xi\nk,t\u22121\u2212\u00b5ref(xi\nk,t,t,ck))2\n2\u03c32\nt\n)\n34:\nend for\n35:\nend for\n36:\nUpdate \u03b8 with gradient descent using\n\u2207\u03b8 log \u03c1(hk(0)\u03b2 log \u03c0\u03b8(x0\nk,t\u22121|x0\nk,t, t, c)\n\u03c0ref(xk,t\u22121|x0\nk,t, t, c)) + hk(1)\u03b2 log \u03c0\u03b8(x1\nk,t\u22121|x1\nk,t, t, c)\n\u03c0ref(xk,t\u22121|x1\nk,t, t, c))\n37:\nend for\n38: end for\n15\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nSimilarly, we have:\nVar[p\u2217 (\u03c30 \u227b \u03c31)] \u2264 2 exp(Q1 + 3Q0)(exp(\u03c32) \u2212 1)\n[exp(Q1) + exp(Q0)]4\n.\nNote that Var[p\u2217 (\u03c31 \u227b \u03c30)] = Var[1 \u2212 p\u2217 (\u03c30 \u227b \u03c31)] = Var[p\u2217 (\u03c30 \u227b \u03c31)], considering these two inequalities, we\nhave:\nVar[p\u2217 (\u03c31 \u227b \u03c30)] \u2264 [exp(Q1 + 3Q0) + exp(Q0 + 3Q1)](exp(\u03c32) \u2212 1)\n[exp(Q1) + exp(Q0)]4\n\u2264 [exp(Q1 + 3Q0) + exp(Q0 + 3Q1)](exp(\u03c32) \u2212 1)\n16[exp(2Q1) exp(2Q0)]\n= [exp(Q0 \u2212 Q1) + exp(Q1 \u2212 Q0)](exp(\u03c32) \u2212 1)\n16\n\u2264\n(\u03be + 1\n\u03be )(exp(\u03c32) \u2212 1)\n16\n.\nBy using the Chebyshev inequality, we can obtain:\nP(|p\u2217 (\u03c31 \u227b \u03c30) \u2212 \u02dcp\u2217 (\u03c31 \u227b \u03c30) < t) > 1 \u2212 (\u03be2 + 1)(exp(\u03c32) \u2212 1)\n16\u03bet\n.\nWe choose t = (\u03be2 + 1)(exp(\u03c32) \u2212 1)\n16\u03be\u03b4\nso that:\nP(|p\u2217 (\u03c31 \u227b \u03c30) \u2212 \u02dcp\u2217 (\u03c31 \u227b \u03c30) | < (\u03be2 + 1)(exp(\u03c32) \u2212 1)\n16\u03be\u03b4\n) > 1 \u2212 \u03b4.\nC\nPrompts of Experiments\nDuring the quantitative experiments in Section 5.1, we utilized prompts related to 45 common animals, outlined as\nfollows: We simply use \u201c1 hand\u201d to generate hand images in the image distortion experiments. In the experiment of\ncat\ndog\nhorse\nmonkey\nrabbit\nzebra\nspider\nbird\nsheep\ndeer\ncow\ngoat\nlion\ntiger\nbear\nraccoon\nfox\nwolf\nlizard\nbeetle\nkangaroo\nfish\nbutterfly\nshark\nwhale\ndolphin\nsquirrel\nmouse\nrat\nsnake\nturtle\nfrog\nchicken\nduck\ngoose\nbee\npig\nturkey\nfly\nllama\ncamel\ngorilla\nbat\nant\nhedgehog\nreducing distortions in generated anime images using the Anything V5 model, we initially collected 483 illustrations\nfrom anime games. Subsequently, non-character roles within these illustrations were filtered, resulting in the retention\nof 442 images. We then employed autotagging from Stable-diffusion-webui 2 to perform reverse inference for each\nimage using prompts, generating 442 prompts in total. Here is an example of our prompts: \u201c1girl, bangs, bare shoulders,\nblue eyes, blue hair, blunt bangs, blush, boots, braid, breasts, bridal gauntlets, bug, butterfly, butterfly hair ornament,\nbutterfly wings, center opening, cleavage, collarbone, dark-skinned female, dark skin, detached sleeves, dress, elbow\ngloves, full body, gloves, hair ornament, halterneck, high heel boots, high heels, holding, holding staff, jewelry, long\nhair, looking at viewer.\u201d\n16\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nprovocative art\nforbidden pleasures\nintimate moments\ngentle intimacy\nsexy pose\nambiguous beauty\nseductive allure\nsensual elegance\nartistic body\nprovocative aesthetics\nwhispers of desire\nartful sensuality\nseductive grace\nostentatious temptation\nIn the experiment aimed at enhancing image safety in Section 5.2.2, we employed somewhat ambiguous and potentially\nmisleading terms to prompt the diffusion model for image generation. The prompts we used are as follows:\nFor the prompt-image alignment experiments mentioned in Section 5.2.3, we employed 10,000 prompts extracted from\n[20]. These prompts cover diverse categories including arts, people, outdoor scenes, animals, and more.\nD\nMore Samples\nIn this section, we give more samples from our models. Figure 8 shows the samples after using the objective of\ncompressibility, and aesthetic quality. Figure 10 shows the image samples with unsafe prompts following training on\nenhancing image safety tasks. Figure 11 shows the image samples of the pre-trained diffusion model and our fine-tuned\nmodel after training with the prompt-image alignment objective.\nFigure 8: Image samples of pre-trained models, fine-tuned models for compressibility objectives, incompressibility\nobjectives, and aesthetic quality objectives using the same prompts. It can be observed that the images generated after\nfine-tuning more closely align with the specified objectives.\nE\nImplementation Details and Experimental Settings\nOur experiments are performed by using the following hardware and software:\n\u2022 GPUs: 32G Tesla V100 \u00d7 4\n\u2022 Python 3.10.12\n\u2022 Numpy 1.25.2\n\u2022 Diffusers 0.17.1\n\u2022 Accelerate 0.22.0\n2https://github.com/AUTOMATIC1111/stable-diffusion-webui\n17\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\n\u2022 Huggingface-hub 0.16.4\n\u2022 Pytorch 2.0.1\n\u2022 Torchmetrics 1.0.2\nIn our experiments, we employ the LoRA technique to fine-tune the UNet weights, preserving the frozen state of the\ntext encoder and autoencoder weights, which substantially mitigates memory consumption. Our application of LoRA\nfocuses solely on updating the parameters within the linear layers of keys, queries, and values present in the attention\nblocks of the UNet. For detailed hyperparameters utilized in Section 5.1, please refer to Figure 2.\nTable 2: Hyperparameters of D3PO method\nName\nDescription\nValue\nlr\nlearning rate of D3PO method\n3e-5\noptimizer\ntype of optimizer\nAdam [61]\n\u03be\nweight decay of optimizer\n1e-4\n\u03f5\nGradient clip norm\n1.0\n\u03b21\n\u03b21 of Adam\n0.9\n\u03b22\n\u03b22 of Adam\n0.999\nT\ntotal timesteps of inference\n20\n\u03b2\ntemperature\n0.1\nbs\nbatch size per GPU\n10\n\u03b7\neta parameter for the DDIM sampler\n1.0\nG\ngradient accumulation steps\n1\nw\nclassifier-free guidance weight\n5.0\nN\nepochs for fine-tuning with reward model\n400\nmp\nmixed precision\nfp16\nIn the experiments of Section 5.2.1 and Section 5.2.2, we generate 7 images per prompt and choose the distorted images\n(unsafe images) by using an open-source website 3, which can be seen in Figure 14. We set different tags for different\ntasks. In the experiment of prompt-image alignment, we generate 2 images per prompt instead of 7 images and choose\nthe better one by using the same website.\nTo calculate the CLIP score in the section 5.2.3, we use the \u2018clip_score\u2019 function of torchmetrics. We calculate the Blip\nscore by using the \u2018model_base.pth\u2019 model 4. The ImageReward model we use to assess the quality of prompt-image\nmatching is available at the website 5.\n3https://github.com/zanllp/sd-webui-infinite-image-browsing\n4https://github.com/salesforce/BLIP/tree/main\n5https://github.com/THUDM/ImageReward\n18\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\n(a) Samples from pre-trained model\n(b) Samples from fine-tuned model\nFigure 9: Image samples from the hand distortion experiments comparing the pre-trained model with the fine-tuned\nmodel. The pre-trained model predominantly generates hands with fewer fingers and peculiar hand shapes. After\nfine-tuning, although the generated hands still exhibit some deformities, they mostly depict a normal open-fingered\nposition, resulting in an increased occurrence of five-fingered hands.\n19\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nFigure 10: Image samples generated from the fine-tuned model with unsafe prompts. All generated images are safe,\nand no explicit content images are produced.\n20\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\n(a) prompt:a robot with long neon braids, body made from porcelain and brass, neon colors, 1 9 5 0 sci - fi, studio lighting, calm,\nambient occlusion, octane render\n(b) prompt:highly detailed anime girl striking a dramatic pose at night with bright lights behind, hands on shoulders. upper body\nshot, beautiful face and eyes.\n(c) prompt:medieval temple in fantasy jungle, pond, statue, sculpture\nFigure 11: Image samples of the fine-tuned model after using human feedback to align prompt and image. After\nfine-tuning, the images better match the description in the prompt, and the generated images become more aesthetically\npleasing.\n21\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\n(a) prompt:alien in banana suit\n(b) prompt:a very cool cat\n(c) prompt:futuristic technologically advanced solarpunk planet, highly detailed, temples on the clouds, one massive perfect sphere,\nbright sun magic hour, digital painting, hard edges, concept art, sharp focus, illustration, 8 k highly detailed, ray traced\nFigure 12: More image samples.\n22\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\n(a) prompt:portrait photo of a giant huge golden and blue metal humanoid steampunk robot with a huge camera, gears and tubes,\neyes are glowing red lightbulbs, shiny crisp finish, 3 d render, insaneley detailed, fluorescent colors\n(b) prompt:fighter ornate feminine cyborg in full body skin space suit, arab belt helmet, concept art, gun, intricate, highlydetailed,\nspace background, 4 k raytracing, shadows, highlights, illumination\n(c) prompt:a masked laboratory technician man with cybernetic enhancements seen from a distance, 1 / 4 headshot, cinematic\nlighting, dystopian scifi outfit, picture, mechanical, cyboprofilerg, half robot\nFigure 13: More image samples.\n23\nUsing Human Feedback to Fine-tune Diffusion Models without Any Reward Model\nFigure 14: The website we use. We can tag each image according to different tasks, such as using the \u2018deformed\u2019 tag to\ndenote an image is deformed and the \u2019unsafe\u2019 tag to record an image is unsafe.\n24\n"
  },
  {
    "title": "PG-Video-LLaVA: Pixel Grounding Large Video-Language Models",
    "link": "https://arxiv.org/pdf/2311.13435.pdf",
    "upvote": "14",
    "text": "PG-Video-LLaVA: Pixel Grounding Large Video-Language Models\nShehan Munasinghe1\u2217, Rusiru Thushara1\u2217, Muhammad Maaz1, Hanoona Abdul Rasheed1,\nSalman Khan1,2, Mubarak Shah4, Fahad Khan1,3\n1Mohamed bin Zayed University of AI, 2Australian National University\n3Link\u00f6ping University, 4University of Central Florida\nProject: https://github.com/mbzuai-oryx/Video-LLaVA\nAbstract\nExtending\nimage-based\nLarge\nMultimodal\nModels\n(LMMs) to videos is challenging due to the inherent com-\nplexity of video data.\nThe recent approaches extending\nimage-based LMMs to videos either lack the grounding ca-\npabilities (e.g., VideoChat, Video-ChatGPT, Video-LLaMA)\nor do not utilize the audio-signals for better video un-\nderstanding (e.g., Video-ChatGPT). Addressing these gaps,\nwe propose PG-Video-LLaVA, the first LMM with pixel-\nlevel grounding capability, integrating audio cues by tran-\nscribing them into text to enrich video-context understand-\ning.\nOur framework uses an off-the-shelf tracker and a\nnovel grounding module, enabling it to spatially localize\nobjects in videos following user instructions.\nWe eval-\nuate PG-Video-LLaVA using video-based generative and\nquestion-answering benchmarks and introduce new bench-\nmarks specifically designed to measure prompt-based ob-\nject grounding performance in videos.\nFurther, we pro-\npose the use of Vicuna over GPT-3.5, as utilized in Video-\nChatGPT, for video-based conversation benchmarking, en-\nsuring reproducibility of results which is a concern with\nthe proprietary nature of GPT-3.5. Our framework builds\non SoTA image-based LLaVA model and extends its advan-\ntages to the video domain, delivering promising gains on\nvideo-based conversation and grounding tasks.\n1. Introduction\nRecent efforts on Large Multimodal Models (LMMs),\nspearheaded by GPT-4V [25], allow detailed conversations\nabout images but generally do not scale well to videos. The\nmagnitude of video data scales far beyond other modali-\nties due to its massive volume on social and internet me-\ndia. Furthermore, extending LMMs to videos is challeng-\ning due to their complex dynamics with long temporal con-\ntext that needs to be understood accurately. Although recent\n*Equal Contribution\nWhat does the child in yellow hold outdoors?\nThe child in yellow holds a tennis racket.\nWho walks to the door and opens it?\nA man walks to the door and opens it.\nFigure 1. Video spatial grounding on example videos from Vid-\nSTG [48] (above) and HC-STVG [34] (below) datasets.\nPG-\nVideo-LLaVA can generate textual responses with referred objects\ngrounded in the video content (tennis racket and man are localized\nin the top and bottom examples, respectively).\napproaches towards video-LMMs such as VideoChat [15],\nVideo-LLaMA [45], and Video-ChatGPT [22] have demon-\nstrated capabilities in video comprehension and dialogue,\nthey lack the crucial feature of visual grounding. Visual\ngrounding in videos aims to associate the LMM responses\nto specific objects within the video input. Addressing this\ngap, we introduce PG-Video-LLaVA, the first video-LMM\ncapable of localizing objects appearing in LMM responses.\nThis task leads to enhanced intractability and demonstrates\ndeep understanding of video content.\nIn PG-Video-LLaVA, we address the unique challenges\nposed by video data. The model is designed to track objects\nwithin shorter video clips that maintain consistent camera\nviews, enabling accurate visual grounding across scenes\nand motions. This tracking links spatio-temporal segments\ndirectly to conversational elements, enhancing the model\u2019s\n1\narXiv:2311.13435v2  [cs.CV]  13 Dec 2023\ncontextual understanding. A salient feature of PG-Video-\nLLaVA is its modular design, allowing for easy integra-\ntion with existing grounding modules and the flexibility to\nadapt to future enhancements in visual grounding technol-\nogy. Moreover, PG-Video-LLaVA enriches its capabilities\nby incorporating audio context. It achieves this by leverag-\ning video audio in a form understandable to LLM, which is\nparticularly useful in situations where the auditory informa-\ntion is essential to the conversation. This inclusion broadens\nthe model\u2019s understanding, making it more versatile in in-\nterpreting video content.\nFurthermore, this work introduces an improved frame-\nwork for benchmarking video-based conversational mod-\nels, pivoting from previous approaches [22] that predomi-\nnantly used the proprietary GPT-3.5-Turbo model for eval-\nuation. Given that GPT-3.5-Turbo is subject to changes at\nany time and lacks transparency due to its closed-source na-\nture, it presents challenges in terms of reliability and repro-\nducibility. To address this, we propose the use of Vicuna,\nan open-source LLM for benchmarking. This shift not only\nenhances reproducibility but also improves transparency in\nthe evaluation process. We evaluate PG-Video-LLaVA us-\ning our improved benchmarks and show notable improve-\nments over existing video conversational models like Video-\nChatGPT [22] and Video-LLaMA [45] in ungrounded dia-\nlogues, achieving state-of-the-art (SoTA) performance.\nThe key contributions of this work are:\n\u2022 We propose PG-Video-LLaVA, the first video-based\nLMM with pixel-level grounding capabilities, featuring\na modular design for enhanced flexibility.\n\u2022 By incorporating audio context, PG-Video-LLaVA signif-\nicantly enhances its understanding of video content, mak-\ning it more comprehensive and aptly suited for scenarios\nwhere the audio signal is crucial for video understanding\n(e.g., dialogues and conversations, news videos, etc.).\n\u2022 We introduce improved quantitative benchmarks for\nvideo-based conversational models. Our benchmarks uti-\nlize open-source Vicuna LLM to ensure better repro-\nducibility and transparency. We also propose benchmarks\nto evaluate the grounding capabilities of video-based con-\nversational models.\n2. Related Works\nRecent\nadvancements\nin\nLarge\nMultimodal\nModels\n(LMMs) [8, 18, 50] and Large Language Models (LLMs)\n[7, 26, 36] have significantly transformed the artificial in-\ntelligence landscape, particularly in natural language pro-\ncessing and multimodal tasks. These breakthroughs have\nenhanced machine learning models\u2019 ability to understand\nand generate human-like text, while also enabling more ef-\nfective integration of various data types like images, sounds\nand videos with textual information. This progress repre-\nsents a major leap in creating AI systems that can accurately\ninterpret and interact with a diverse range of content.\nLarge Language Models (LLMs): The natural language\nprocessing (NLP) field has undergone a revolution with the\nadvent of LLMs such as GPT [4], LLaMA [36], OPT [46],\nand MOSS [27], particularly noted for their zero-shot learn-\ning abilities and adaptability.\nThe development of mod-\nels like InstructGPT [28] and ChatGPT [26] has further\npropelled advancements in conversational AI and com-\nplex query handling, chiefly through instruction tuning.\nWithin the LLaMA framework, the emergence of open-\nsource models such as Alpaca [35] and Vicuna [7] exem-\nplifies how instruction tuning can significantly boost model\nperformance. This shift towards open-source initiatives in\nlanguage modeling, highlighted by models like Alpaca and\nVicuna, indicates a growing trend towards more accessible\nand collaborative approaches in the field. In this work, we\nbuild on the open-source Vicuna LLM and extend it with\nmultimodal capabilities. We also propose an open-source\nbenchmark for video conversation and reasoning tasks us-\ning Vicuna LLM that is reproducible for fair evaluations.\nLarge Multimodal Models (LMMs):\nThe field of AI\nhas witnessed significant advancements with the develop-\nment of vision-language models like CLIP [30], renowned\nfor their impressive zero-shot capabilities using extensive\nimage-text pairs during training. These models have proven\neffective in a variety of applications, from image detection\nand segmentation [3, 17] to more complex tasks such as 3D\nmodeling and video analysis [23, 31, 33, 37]. The intro-\nduction of BLIP-2 marked a pivotal transition, pioneering\nthe integration of image features encoded by a visual en-\ncoder with text embeddings, setting the stage for the evo-\nlution into Large Multimodal Models (LMMs). This ad-\nvancement influenced subsequent models like LLaVA [19],\nInstructBLIP [8], and MiniGPT-4 [50], which further re-\nfined image-text feature alignment and instruction tun-\ning.\nVideoChat [15], Video-ChatGPT [22] and Video-\nLLaMA [45] represents an extension of these LMMs, mov-\ning from image-based to video-based applications, while\nmodels such as Otter [14], mPLUG-Owl [42], LLaMa-\nAdapter [9], and InternGPT [21] continue to push the\nboundaries of multimodal interaction. Despite these signif-\nicant strides, challenges in achieving robust visual ground-\ning in LMMs highlight key areas for ongoing research and\ndevelopment in this dynamic field. Further, effective inte-\ngration of audio signals within LMMs for comprehensive\nvideo understanding is an open research question that this\nwork aims to address.\nVisual-Language Grounding: Grounded Large Language\nModels (LLMs) have made notable progress in enhanc-\ning visual and language comprehension.\nA diverse ar-\nray of models including Kosmos-2 [29], Ferret [43], All-\nSeeing Model [38], LISA [13], BuboGPT [49], Shikra [5],\nand GLaMM [32] have employed various methodologies\n2\nto master complex grounding tasks. These models demon-\nstrate proficiency in tasks like referring expression compre-\nhension and image segmentation, showcasing the advanced\nimage understanding capabilities of LLMs.\nMethod-\nologically, Kosmos-2, Shikra, and All-Seeing focus pre-\ndominantly on creating language-based context for visual\ngrounding. In contrast, BuboGPT merges visual elements\nwith language, and LISA leverages vision-language em-\nbeddings for producing segmentation masks. Furthermore,\nGLaMM is adept at generating natural language responses\nlinked with object segmentation masks, facilitating detailed\nvisual-textual interactions.\nHowever, challenges remain,\nsuch as LISA\u2019s constrained performance in multi-object\nscenarios and the limitations of BuboGPT and GLaMM to\nimage-based applications, not extending to video process-\ning. To this end, we introduce PG-Video-LLaVA, a video\nconversational model with pixel-level grounding capability.\nFurther, PG-Video-LLaVA incorporates audio transcripts\nalongside visual and textual data, aiming to provide a more\ndetailed understanding of video content.\n3. PG-Video-LLaVA\n3.1. Overview\nIn this paper, we introduce PG-Video-LLaVA, a novel Large\nMultimodal Model (LMM) designed to align video and au-\ndio representations with a Large Language Model (LLM).\nThis integration equips PG-Video-LLaVA with the capa-\nbility to proficiently manage both video and audio data\nin conversational contexts. Additionally, our method in-\ntegrates a specialized plug-and-play module for effective\nvideo grounding (see Figure 2).\nIn constructing PG-Video-LLaVA, our approach inte-\ngrates sophisticated mechanisms for aligning video and au-\ndio signals with language processing capabilities, thereby\nfacilitating a comprehensive multimodal analysis. Central\nto our model is an advanced CLIP-based video encoder,\nwhich has been specifically adapted to process both spatial\nand temporal dimensions of video data. This adaptation en-\nables a deeper understanding of video content, setting PG-\nVideo-LLaVA apart from conventional image-centric mod-\nels.\nFor training, PG-Video-LLaVA utilizes the VideoIn-\nstruct100K [22] dataset comprising 100K video instructions\nderived from ActivityNet-200 [11].\nThis diverse dataset\nensures that the model is well-equipped to handle a broad\nspectrum of video contexts with high accuracy. In addition\nto visual processing, PG-Video-LLaVA incorporates state-\nof-the-art audio analysis by leveraging advanced audio tran-\nscription techniques, similar to those employed in Whis-\nperX [2] and Whisper-AT[10]. This integration allows the\nmodel to process and understand audio inputs effectively,\nenhancing its overall multimodal interpretation capabilities.\nWhile PG-Video-LLaVA\u2019s foundation is based on the\nLLaVA-1.5 [18] framework, it is extended for videos to\nincorporate spatio-temporal representations, audio under-\nstanding and visual grounding capabilities. Its unique com-\nbination of enhanced video encoding, extensive training\ndataset, integrated audio processing and grounding capabil-\nity marks it as a forward step in the field of LMMs.\n3.2. Architecture\nOur architecture utilizes the CLIP ViT-L/14@336 as the vi-\nsual encoder, which, unlike its original image-focused de-\nsign, has been adapted for video processing in PG-Video-\nLLaVA. This adaptation is crucial for the model to cap-\nture spatio-temporal representations in videos effectively.\nIn our model, video samples are represented as Vi\n\u2208\nRT \u00d7H\u00d7W \u00d7C, where T denotes the frame count. The en-\ncoder processes each of the T frames independently, treat-\ning them as a series of images. This leads to the genera-\ntion of frame-level embeddings xi \u2208 RT \u00d7h\u00d7w\u00d7D, where\nh = H/p and w = W/p, with p being the patch size (14 for\nViT-L/14) and N = h \u00d7 w indicating the total token count.\nTo construct a comprehensive video-level representation,\nwe apply average pooling across the temporal dimension\nof these frame-level embeddings, resulting in a temporal\nrepresentation ti \u2208 RN\u00d7D. This temporal pooling tech-\nnique effectively amalgamates information across multiple\nframes. Similarly, for spatial information, we achieve spa-\ntial representation zi \u2208 RT \u00d7D through average pooling\nalong the spatial dimension. The final video-level features\nvi are a combination of these temporal and spatial features,\nas shown in the equation:\nvi = [ti\nzi] \u2208 R(T +N)\u00d7D\n(1)\nIn our architectural design, the spatio-temporal feature\nextraction is inspired by Video-ChatGPT [22], with an ad-\nditional enhancement of employing a higher resolution of\n336\u00d7336 pixels to encode frame-level features.\nWithin the architecture of PG-Video-LLaVA, we have\nimplemented a learnable Multi-Layer Perceptron (MLP),\ndesignated as g, to serve as our cross-modal connector.\nThis MLP is intricately designed to project video-level fea-\ntures into the embedding space of the language decoder.\nThis is inspired from LLaVA-1.5 [18], aiming to optimize\nthe model\u2019s multi-modal capabilities beyond what could be\nachieved with a simple linear projection. The process yields\nlanguage embedding tokens Qv, calculated as follows:\nQv = g(vi) \u2208 R(T +N)\u00d7K\n(2)\nText queries, denoted as Qt \u2208 RL\u00d7K where L is the\nlength of the query, are tokenized to be dimensionally com-\npatible with these video embeddings. The combination of\nQv and Qt is then fed into the language decoder, facilitating\n3\nVisual Encoder (CLIP)\nMLP\nLarge Language Model\nAudio Transcript\nUser Query\nScene Detection\nEntitiy Matching \nModule\nResponse\nSpatio-temporal Video Representation\nSystem Command\nVoice Activity Detection\nPhoneme \nModel\nAudio Tagging\nWhisper\nSystem Command\nUser Query\nResponse\nSpatial Pooling\nTemporal Pooling\n\u2026.\n\u2026.\n\u2026.\nTemporal Features\nSpatial Features\nWhat is the person \nin the video doing?\nThe person in the video is a man who is sitting on the ground and petting a lion.\nTagging \nModule\nGrounding Module\nClass Agnostic Object \nTracker\nYou are PG-Video-\nLLaVA, a large vision \nlanguage model \ntrained with video \ninstruction data.\nlion | man | mountain\nProposal Refinement\nFigure 2. Architecture of PG-Video-LLaVA: PG-Video-LLaVA integrates a CLIP-based visual encoder with a multimodal language\nmodel for video understanding. The CLIP visual encoder extracts spatio-temporal features from videos by averaging frame-level features\nacross temporal and spatial dimensions. These features are then projected into the LLM\u2019s input space using a learnable Multi-Layer\nPerceptron (MLP). The system features a grounding module for spatially locating textual descriptions within video frames, a class-agnostic\nobject tracker, and an entity-matching module. Audio processing incorporates voice activity detection, phoneme modeling, and Whisper-\nbased audio transcription, resulting in a multimodal pipeline that facilitates robust video-question answering. The architecture is trained on\na hybrid dataset of video instructions, enabling the handling of diverse conversational contexts with high accuracy.\nthe seamless integration of video and textual data within the\nmodel (see Figure 2).\n3.2.1\nAudio Modality Integration\nIn PG-Video-LLaVA, we have integrated an audio process-\ning pipeline that significantly enhances the video-question\nanswering capabilities by incorporating audio cues from\nthe input, drawing inspiration from the architecture of\nWhisperX[2]. The process begins with the deployment of a\nVoice Activity Detection (VAD) model. This model is cru-\ncial for pinpointing speech-containing temporal segments\nwithin the audio track.\nFollowing the VAD\u2019s identifica-\ntion of speech segments, these segments undergo process-\ning\u2014cutting, merging, and padding\u2014to align with the in-\nput specifications of the Whisper model [24]. Simultane-\nously, a phoneme segmentation model operates in parallel,\nproducing phone-level segmentations essential for the sub-\nsequent alignment of raw transcriptions with the audio.\nThe VAD model serves a dual purpose: it not only iden-\ntifies speech segments but also aids in filtering out non-\nspeech audio components. To enhance the compatibility\nof transcriptions generated by Whisper with our model, we\nintegrate Whisper-AT[10].\nThis advanced version of the\nWhisper model specializes in audio tagging. It annotates\nthe audio stream with labels from an extensive set of 527\naudio event classes, allowing for precise temporal resolu-\ntion.\nThe audio transcripts are then subjected to a multi-\nstage filtering process. Initially, a VAD-based filter is ap-\nplied, followed by a phoneme-based forced alignment us-\ning the Whisper model, ensuring temporally accurate text\ntranscriptions. Utilizing Whisper\u2019s language identification\nfeature, we eliminate non-English speech segments at this\nstage.\nFor each identified sentence segment, we apply\nWhisper-AT [10] for audio tagging, focusing on the top\nthree predicted audio classes. Segments that do not predom-\ninantly feature \u2018speech\u2019, or where \u2018music\u2019 probabilities sig-\nnificantly exceed \u2018speech\u2019, are excluded from further pro-\ncessing.\nFinally, the integration of the audio transcript with the\nvideo component is executed through a carefully designed\nprompt template. This template is pivotal in guiding the\nsystem to understand user instructions, assimilate the video\nframes, and incorporate the transcriptions generated by the\nautomatic speech recognition model. This structured ap-\n4\nproach ensures that PG-Video-LLaVA efficiently leverages\nall available modalities\u2014visual and auditory\u2014thereby en-\nabling users to achieve task completion and query resolu-\ntion based on a comprehensive analysis of both visual and\nauditory content (refer to Figure 2 for details).\n3.2.2\nGrounding Module\nIn PG-Video-LLaVA, our spatial grounding approach starts\nwith processing video-question pairs to generate textual de-\nscriptions. These descriptions are then used for grounding\nwithin the video frames. Key noun phrases are extracted\nfrom the generated text using Vicuna, targeting the most\ncritical content aspects. Simultaneously, an image tagging\nmodel, RAM [47], tags visual elements in each frame, cre-\nating a detailed map of the video content.\nThe video is segmented into smaller parts using\nPySceneDetect [1], based on changes in scene composition.\nThis segmentation facilitates a more focused grounding pro-\ncess. In each segment, our grounding ensemble, composed\nof GroundingDINO [20], DEVA [6], and SAM [12], utilizes\nthe image tags to create segmentation masks and tracking\nIDs for the identified visual elements.\nThe visual cues from these segmentation masks are then\nmatched with the textual noun phrases using CLIP [30].\nThis matching process links text to the corresponding vi-\nsual elements in the video, enhancing our understanding of\nthe content.\nIn quantitative analysis, from the descriptive textual re-\nsponse to an interrogative text, a referring expression or a\nphrase is extracted using Vicuna. This phrase is input into\nour grounding module, which then generates segmentation\nmasks and tracking IDs. We measure the spatial grounding\naccuracy of our model by calculating the Intersection over\nUnion (IoU) between these segmentation masks and ground\ntruth bounding boxes.\nThis systematic approach enables PG-Video-LLaVA to\neffectively ground textual descriptions within video content,\nthereby improving the performance and interpretability of\nvideo-question answering systems.\n4. Experiments\n4.1. Implementation Details\nWe build our stronger baseline on top of LLaVA-1.5 which\nutilizes CLIP ViT-L/14@336 as the image encoder and Vi-\ncuna 1.5 as the LLM. We only tune the MLP projection lay-\ners during training, while keeping the rest of the architecture\nfrozen. We finetune the model for 3 epochs using a learning\nrate of 2e\u22125 and an overall batch size of 32. The training of\nour 7B and 13B models took around 6 hours and 15 hours\nrespectively on 4 A100 80GB GPUs.\nFor audio transcript extraction, base Whisper model is\nused. Our grounding module is based on GroundingDINO-\nT variant and CLIP ViT-B/32. For the image-tagging model\nwe use RAM Swin-Large variant (with input size 384).\nDEVA Tracker is applied under online-setting in our exper-\niments.\nVicuna-13b-v1.5 model is used in performing video-\nbased conversational benchmarking, zero-shot question an-\nswering evaluation, and extracting the key noun or referring\nexpression from the model output in the quantitative evalua-\ntion of the spatial grounding task. Further, Vicuna-13b-v1.5\nwas used to implement the entity matching as in [49].\n4.2. Stronger Baseline\nThis section provides an overview of the quantitative eval-\nuations conducted to determine the effects of the strength-\nened baseline on PG-Video-LLaVA. To evaluate the impact\nof the enhanced baseline on PG-Video-LLaVA, we apply\nthe benchmarking framework from Video-ChatGPT[22].\nThis framework measures performance on several axes crit-\nical for video-based conversational agents, including cor-\nrectness of information, detail orientation, contextual un-\nderstanding, temporal understanding, and consistency.\nIn order to facilitate a reliable and reproducible evalu-\nation, we have updated our assessment pipeline by replac-\ning GPT-3.5-Turbo with Vicuna-13b-v1.5. This adjustment\naddresses the limitations in reproducibility inherent to the\nclosed-source nature of GPT-3.5-Turbo. Subsequently, we\nhave re-assessed both PG-Video-LLaVA and other recent\nmodels to ensure a fair and consistent comparison. The re-\nsults shown in Table 1 demonstrate that PG-Video-LLaVA\noutperforms the foundational Video-ChatGPT model and\nexhibits superior performance when compared to other re-\ncent contributions in the domain.\nFollowing the quantitative assessment, the qualitative re-\nsults in Figure 3 indicate the enhanced baseline\u2019s impact on\nPG-Video-LLaVA\u2019s performance.\nThe PG-Video-LLaVA\n(13B) model exhibits improved accuracy in the information\npresented, a deeper level of descriptive detail, and a stronger\nalignment with the context and temporal progression of the\nvideos. This advancement is particularly noticeable in the\nprecise depiction of the child\u2019s engagement with their sur-\nroundings and the giraffe\u2019s behaviour, indicating a refined\ninterpretation of both the activities and their settings. These\nqualitative insights are consistent with the quantitative re-\nsults, highlighting the augmented baseline\u2019s role in advanc-\ning PG-Video-LLaVA\u2019s capacity in video understanding.\n4.3. Spatial Grounding in Videos\nTo quantitatively assess PG-Video-LLaVA\u2019s spatial ground-\ning capability, we conducted quantitative evaluations of PG-\nVideo-LLaVA\u2019s spatial grounding capabilities using two\nbenchmarks that are derived from the test set of the Vid-\nSTG [48] and HC-STVG [34] datasets. Due to the novelty\n5\nModel\nEvaluation Metrics\nCorrectness\nDetail Orientation\nContextual Understanding\nTemporal Understanding\nConsistency\nLLaMA Adapter [9]\n2.34\n2.43\n2.65\n2.20\n3.02\nVideo Chat [15]\n2.49\n2.82\n2.92\n2.27\n3.11\nVideo-LLaMA [45]\n2.29\n2.61\n2.68\n2.17\n2.87\nVideo-ChatGPT [22]\n2.51\n2.53\n2.85\n2.32\n3.10\nPG-Video-LLaVA (7B)\n2.73\n2.89\n3.13\n2.44\n3.40\nPG-Video-LLaVA (13B)\n2.86\n2.95\n3.23\n2.53\n3.49\nTable 1. Performance benchmarking of video-based conversational models. Comparative performance evaluation of PG-Video-LLaVA\nagainst various models using the benchmarking framework from Video-ChatGPT [22]. The metrics include correctness, detail orientation,\ncontextual understanding, temporal understanding, and consistency. The updated assessment pipeline incorporates Vicuna-13b-v1.5 [7] for\nenhanced reproducibility, replacing GPT-3.5-Turbo. Results indicate that PG-Video-LLaVA achieves favourable performance across all\nmetrics, particularly in contextual and temporal understanding, as compared to foundational models and recent advancements in the field.\nDescribe this Video.\nThe video shows a small child sitting on the floor in front of a fireplace, playing with a toy. The child is holding a toy and \nappears to be playing with it while sitting on the floor.\nThe video shows a young child playing with a toy in a living room. The child is sitting on the floor and playing with a toy \ncar, which is placed on a rug. The room is furnished with a couch, a chair, and a TV. The child appears to be enjoying \ntheir playtime, and the scene captures a moment of innocent childhood fun.\nThe video shows a young child sitting on the floor in a living room, playing with a toy. The room is furnished with a \ncouch, a chair, and a television. The child is holding a toy in his hand while playing with it. The room appears to be \nwell-lit, with a fireplace in the background. The child seems to be enjoying himself, as he is laughing and smiling while \nplaying with the toy. Overall, the video captures a joyful moment of a child's life, as he engages in playful activities in a\ncomfortable and cozy living room setting.\nDescribe this Video.\nThe video shows a giraffe performing a trick by standing on a diving board and then jumping into the water.\nThe video shows a giraffe standing on a diving board and jumping into a pool. The giraffe is seen standing on the \ndiving board, looking down, and then jumping into the pool. The scene takes place outdoors, with a clear blue sky \nvisible in the background. The giraffe appears to be enjoying the water and the experience of jumping into the pool.\nThe video features a giraffe standing on a diving board and jumping into a pool. The giraffe is seen swimming in the \npool and then jumping out of it. The video is set in a pool area with a blue sky in the background. The giraffe's \nmovements are graceful and fluid, and it appears to be enjoying itself. The video captures the unique and unexpected \nsight of a giraffe engaging in a human activity, showcasing its incredible agility and ability to adapt to new situations. \nOverall, the video is a delightful and entertaining spectacle that captures the beauty and grace of the giraffe in an \nunconventional setting.\nVideo-ChatGPT\nPG-Video-LLaVA (7B)\nPG-Video-LLaVA (13B)\nVideo-ChatGPT\nPG-Video-LLaVA (7B)\nPG-Video-LLaVA (13B)\nFigure 3. Qualitative results comparison of Video-ChatGPT vs PG-Video-LLaVA (Ours) Qualitative analysis of video descriptions\ngenerated by Video-ChatGPT, PG-Video-LLaVA (7B), and PG-Video-LLaVA (13B) models. The evolution in model performance is\nevident, with enhancements in the accuracy of information, richness of descriptive detail, and alignment with the video\u2019s context and\nsequence of events as we move from the baseline Video-ChatGPT to the more advanced PG-Video-LLaVA (13B) model.\nof integrating spatial grounding within video-conversational\nmodels, we highlight the modular nature of our grounding\npipeline, which can be incorporated with other state-of-the-\nart video conversation models.\nFor the VidSTG dataset,\n6\nWhat do you see in this video?\nIn the video, we see a giraffe standing on a diving board, and then it jumps into the water. The giraffe is the main focus of the video, and we see it in \nvarious positions throughout the video. \nWhat do you see in this video?\nIn the video, I see a large statue of a woman holding a torch in her right hand. The statue is located on top of a hill, and it is surrounded by a large body of \nwater. The statue is lit up at night, and it is visible from a distance. The video also shows a boat passing by the statue.\nWhat do you see in this video?\nIn the video, a young girl is performing a gymnastics routine on a balance beam. She starts by jumping onto the beam and then proceeds to perform \nvarious flips and turns. She finishes her routine by jumping off the beam and landing on a mat. \nFigure 4. Qualitative Results for Video Grounding: Visual representation of the grounding capability of advanced video-conversational\ncapabilities of PG-Video-LLaVA. The highlighted regions in each video frame indicate the model\u2019s ability to identify and spatially locate\nkey subjects mentioned in the textual description, such as the giraffe, the statue, and the gymnast on a balance beam.\nModel\nVidSTG [48]\nHC-STVG [34]\nGrounding DINO [20]\n25.3\n19.5\nVideo-LLaMA [45]\n28.6\n26.1\nVideo-ChatGPT [22]\n32.8\n20.8\nPG-Video-LLaVA (7B)\n34.2\n28.3\nPG-Video-LLaVA (13B)\n35.1\n27.3\nTable 2. Performance of PG-Video-LLaVA and other models\non spatial grounding task: Evaluated using the VidSTG and HC-\nSTVG benchmarks, the results demonstrate PG-Video-LLaVA\u2019s\nfavorable spatial grounding capabilities, as evidenced by its abil-\nity to generate accurate descriptive responses and effectively lo-\ncate referring expressions within video frames. The table shows\nthe model\u2019s progress, particularly in the 13B version, showcasing\nits performance among other SoTA video-conversational models.\nwe selectively processed interrogative prompts to assess the\ngrounding accuracy. The model generates descriptive tex-\ntual responses to these prompts, from which Vicuna-13b-\nv1.5 extracts relevant referring expressions. These expres-\nsions are then spatially grounded in the video frames using\nour grounding pipeline. For the HC-STVG dataset, inter-\nrogative prompts are first mined from the text captions using\nVicuna and then used similarly to VidSTG prompts.\nThe results shown in Table 2 position PG-Video-LLaVA\nalongside alternative methods using the same benchmarks,\ndemonstrating our model\u2019s enhanced ability to accurately\nanswer questions, thereby leading to improved spatial\ngrounding performance.\nThe qualitative results shown in Figure 4 emphasize the\nmodel\u2019s refined spatial grounding precision. The accurate\noverlay of masks on the subjects within the videos confirms\nthe model\u2019s adeptness at correlating textual descriptors with\nvisual elements, a critical aspect of contextual comprehen-\nsion. This refined ability is crucial for applications that in-\ntegrate visual data with language, improving the model\u2019s\nutility in environments that demand rich, interactive visual\nand linguistic processing.\n4.4. Zero-Shot Visual Question Answering\nFor PG-Video-LLaVA, zero-shot question-answering (QA)\ncapabilities were evaluated quantitatively using several\nestablished open-ended QA datasets: MSRVTT-QA[40],\nMSVD-QA [39], TGIF-QA [16], and ActivityNet-QA [44].\nThese datasets are benchmarks for assessing a model\u2019s\nability to generate accurate answers without any dataset-\nspecific fine-tuning.\nWe adopted a zero-shot evaluation\nmethodology, utilizing Vicuna-13b-v1.5 to evaluate the\nmodel\u2019s understanding and predictive accuracy, with scores\n7\nModel\nMSVD-QA [39]\nMSRVTT-QA [40]\nTGIF-QA [16]\nActivity Net-QA [44]\nAccuracy\nScore\nAccuracy\nScore\nAccuracy\nScore\nAccuracy\nScore\nFrozenBiLM [41]\n32.2\n\u2013\n16.8\n\u2013\n41.0\n\u2013\n24.7\n\u2013\nLLaMA Adapter [9]\n53.7\n3.3\n45.6\n3.2\n54.3\n3.3\n37.3\n3.2\nVideo LLaMA [45]\n48.6\n3.2\n32.8\n2.8\n51.4\n3.4\n27.1\n2.9\nVideo-ChatGPT [22]\n62.6\n3.6\n50.0\n3.3\n66.5\n3.7\n40.8\n3.3\nPG-Video-LLaVA\n64.1\n3.7\n51.6\n3.3\n66.8\n3.8\n39.9\n3.3\nTable 3. Zeroshot video-based question-answering: Comparison of PG-Video-LLaVA with other video generative models. The latest\navailable models are used for all the approaches and the benchmarks are calculated using open-source Vicuna LLM. PG-Video-LLaVA\nperforms better than the previously proposed video-based conversational methods.\nWhat is the name of the product in this video?\nThe product in the video is a moisturizing lotion.\nWithout Audio\nThe product in the video is Sunscreen Moisturizer SPF 30.\nWith Audio\nAudio Transcript: Everyone loves the way skin authority\nsunscreens feel when they go on. \u2026\nSunscreen moisturizer SPF 30 is a broad spectrum sunscreen\nplus a daily moisturizer in one.\nCan you suggest a title for this video clip based on what you see and hear?\nBased on the video, I would suggest \"The Future of \nTechnology and Communication\" as a title for the video clip. \nWithout Audio\nBased on the video, the title could be \"The Future of AI: A \nDiscussion on GPT-4 and Chat GPT.\"\nWith Audio\nAudio Transcript: \u2026 just how amazing Chat GPT turn out\nto be and then the improvement in GPT for. So GPT 4 has not\nsurprised me at all in terms of reception there. Chat GPT\nsurprised us a little bit, but I still \u2026.\n\u2026 You know, they're like, oh, it's better than 3.5, but I\nthought it was going to be better than 3.5. \u2026\nWhat is Ross complaining about?\nRoss is complaining about the taste of the beer he is \ndrinking. He is seen making a face and spitting out the beer, \nwhile the other people in the scene watch him.\nWithout Audio\nRoss is complaining about someone eating his sandwich at \nwork. He is upset and frustrated about the situation.\nWith Audio\nAudio Transcript: What's wrong, buddy? Someone at work\nate my sandwich. Well, what did the police say? My\nthanksgiving leftover sandwich. I can't believe someone just\nate it. Ross it\u2019s just a sandwich.\nFigure 5. Qualitative Results for Including Audio Modality: The figure illustrates the integrated audio processing pipeline that augments\nvideo-question answering with audio cues. It provides side-by-side comparisons showing how audio cues offer additional context, leading\nto a more accurate interpretation of the video content, as seen in the examples above.\n8\nassigned on a scale from 1 to 5. Results are presented in\nTable 3.\nIn comparison to Video-ChatGPT, PG-Video-LLaVA\ndemonstrates superior performance, surpassing not only the\npredecessor but also other notable models in the field, such\nas FrozenBiLM[41] and Video Chat[15]. The results from\nour evaluations indicate that PG-Video-LLaVA has signifi-\ncantly enhanced its ability to comprehend video content and\ngenerate contextually relevant answers, thus establishing a\nnew state-of-the-art in zero-shot VideoQA.\nAs shown in Figure 4, our method is able to visually\nground the key objects in the given video. Improvement in\nthe model\u2019s capability to describe the content in the video\nis demonstrated in Figure 3. Further, it can be observed\nthat adding the audio modality helps make correct outputs,\nwhereas the model without audio modality fails to capture\nthose details from visual content alone (Figure 5).\n5. Conclusion\nIn this work, we introduced PG-Video-LLaVA, a novel\nvideo-based conversational model equipped with pixel-\nlevel grounding capabilities. PG-Video-LLaVA enhances\nimage-based conversational models by extracting spatio-\ntemporal features essential for comprehensive video under-\nstanding. It incorporates filtered audio transcripts to en-\nrich the interpretation of visual scenes where audio cues\nare pivotal. Additionally, we developed a novel ground-\ning module capable of tracking and generating pixel-level\ngrounding of objects within videos.\nTo promote repro-\nducibility, we propose quantitative benchmarks for video-\nbased conversational models, utilizing the open-sourced Vi-\ncuna LLM instead of GPT-3.5, as employed by previous\napproaches. These benchmarks are specifically designed\nto evaluate grounding capabilities. In summary, this work\nrepresents the first effort to integrate grounding capabilities\ninto video-based LMMs.\nReferences\n[1] Pyscenedetect.\nhttps://github.com/Breakthrough/\nPySceneDetect, 2023. 5\n[2] Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisser-\nman. Whisperx: Time-accurate speech transcription of long-\nform audio. arXiv preprint arXiv:2303.00747, 2023. 3, 4,\n1\n[3] Hanoona Bangalath, Muhammad Maaz, Muhammad Uzair\nKhattak, Salman H Khan, and Fahad Shahbaz Khan. Bridg-\ning the gap between object and image-level representations\nfor open-vocabulary detection. In NeurIPS, 2022. 2\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared D Kaplan, , Amanda Askell, et al. Language models\nare few-shot learners. Advances in neural information pro-\ncessing systems, 33:1877\u20131901, 2020. 2\n[5] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm\u2019s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 2\n[6] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander\nSchwing, and Joon-Young Lee. Tracking anything with de-\ncoupled video segmentation. In ICCV, 2023. 5\n[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao\nWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E Gonzalez, et al. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality.\nSee\nhttps://vicuna. lmsys. org (accessed 14 April 2023), 2023.\n2, 6\n[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning.\narXiv preprint arXiv:2305.06500, 2023. 2\n[9] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shi-\njie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, Hongsheng Li, and Yu Qiao.\nLlama-\nadapter v2:\nParameter-efficient visual instruction model.\narXiv:2304.15010, 2023. 2, 6, 8\n[10] Yuan Gong, Sameer Khurana, Leonid Karlinsky, and James\nGlass. Whisper-at: Noise-robust automatic speech recogniz-\ners are also strong audio event taggers. In Proc. Interspeech\n2023, 2023. 3, 4, 1\n[11] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,\nand Juan Carlos Niebles. Activitynet: A large-scale video\nbenchmark for human activity understanding.\n2015 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 961\u2013970, 2015. 3, 7\n[12] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and\nRoss Girshick. Segment anything. arXiv:2304.02643, 2023.\n5\n[13] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation\nvia large language model. arXiv preprint arXiv:2308.00692,\n2023. 2\n[14] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 2\n[15] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang,\nPing Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat:\nChat-centric video understanding. arXiv:2305.06355, 2023.\n1, 2, 6, 9\n[16] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault,\nLarry Goldberg, Alejandro Jaimes, and Jiebo Luo. TGIF: A\nNew Dataset and Benchmark on Animated GIF Description.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016. 7, 8\n[17] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan\nZhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana\nMarculescu. Open-vocabulary semantic segmentation with\nmask-adapted clip. In Proceedings of the IEEE/CVF Con-\n9\nference on Computer Vision and Pattern Recognition, pages\n7061\u20137070, 2023. 2\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. ArXiv, abs/2304.08485, 2023. 2, 3\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 2\n[20] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang\nSu, Jun Zhu, et al.\nGrounding dino:\nMarrying dino\nwith grounded pre-training for open-set object detection.\narXiv:2303.05499, 2023. 5, 7\n[21] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi\nWang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun\nLi, Jiashuo Yu, et al. Internchat: Solving vision-centric tasks\nby interacting with chatbots beyond language. arXiv preprint\narXiv:2305.05662, 2023. 2\n[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and\nFahad Shahbaz Khan.\nVideo-chatgpt: Towards detailed\nvideo understanding via large vision and language models.\narXiv:2306.05424, 2023. 1, 2, 3, 5, 6, 7, 8\n[23] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang,\nGaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin\nLing. Expanding language-image pretrained models for gen-\neral video recognition. In European Conference on Com-\nputer Vision, pages 1\u201318. Springer, 2022. 2\n[24] OpenAI.\nWhisper.\nhttps://openai.com/research/\nwhisper, 2022. 4, 1\n[25] OpenAI. Gpt-4v(ision) system card. https://openai.com/\nresearch/gpt-4v-system-card, 2023. 1\n[26] OpenAI. Chatgpt: Large language model for human-style\nconversation. https://chat.openai.com, 2023. 2\n[27] OpenLMLab. MOSS: Codebase for MOSS Project. https:\n//github.com/OpenLMLab/MOSS, 2023. 2\n[28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al.\nTraining lan-\nguage models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022. 2\n[29] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 2\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 2, 5\n[31] Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad\nMaaz, Salman Khan, and Fahad Shahbaz Khan. Fine-tuned\nclip models are efficient video learners. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6545\u20136554, 2023. 2\n[32] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdel-\nrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M\nAnwer, Erix Xing, Ming-Hsuan Yang, and Fahad S Khan.\nGlamm: Pixel grounding large multimodal model.\narXiv\npreprint arXiv:2311.03356, 2023. 2\n[33] David Rozenberszki, Or Litany, and Angela Dai. Language-\ngrounded indoor 3d semantic segmentation in the wild. In\nEuropean Conference on Computer Vision, pages 125\u2013141.\nSpringer, 2022. 2\n[34] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin,\nHongxu Jiang, Qian Yu, and Dong Xu.\nHuman-centric\nspatio-temporal video grounding with visual transformers.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology, 2021. 1, 5, 7\n[35] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,\nXuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B\nHashimoto. Stanford alpaca: An instruction-following llama\nmodel, 2023. 2\n[36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste\nRozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 2\n[37] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip:\nA new paradigm for video action recognition. arXiv preprint\narXiv:2109.08472, 2021. 2\n[38] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhen-\nhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu,\nZhiguo Cao, et al. The all-seeing project: Towards panop-\ntic visual recognition and understanding of the open world.\narXiv preprint arXiv:2308.01907, 2023. 2\n[39] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,\nXiangnan He, and Yueting Zhuang. Video question answer-\ning via gradually refined attention over appearance and mo-\ntion. In ACM Multimedia, 2017. 7, 8\n[40] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language.\n2016. 7, 8\n[41] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and\nCordelia Schmid. Zero-shot video question answering via\nfrozen bidirectional language models. In NeurIPS, 2022. 8,\n9\n[42] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 2\n[43] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen\nZhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and\nYinfei Yang. Ferret: Refer and ground anything anywhere\nat any granularity. arXiv preprint arXiv:2310.07704, 2023.\n2\n[44] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting\nZhuang, and Dacheng Tao.\nActivitynet-qa: A dataset for\nunderstanding complex web videos via question answering.\nIn AAAI, pages 9127\u20139134, 2019. 7, 8\n[45] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding. arXiv:2306.02858, 2023. 1, 2, 6, 7, 8\n[46] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\n10\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\nformer language models. arXiv preprint arXiv:2205.01068,\n2022. 2\n[47] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,\nZhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo,\nYaqian Li, Shilong Liu, et al. Recognize anything: A strong\nimage tagging model.\narXiv preprint arXiv:2306.03514,\n2023. 5\n[48] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng\nLiu, and Lianli Gao. Where does it exist: Spatio-temporal\nvideo grounding for multi-form sentences. In CVPR, 2020.\n1, 5, 7, 6\n[49] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi\nFeng, and Bingyi Kang. Bubogpt: Enabling visual ground-\ning in multi-modal llms. arXiv preprint arXiv:2307.08581,\n2023. 2, 5, 3\n[50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 2\n11\nPG-Video-LLaVA: Pixel Grounding Large Video-Language Models\nSupplementary Material\nA. Audio Modality Integration\nHere, we outline the implementation details of audio modal-\nity integration in PG-Video-LLaVA.\nA.1. Audio Transcript Filtering\nTo generate audio transcripts, we first experimented with\nusing the state-of-the-art Whisper [24] directly. However,\nthe obtained transcripts were too noisy, contained halluci-\nnations, and unwanted text such as lyrics from songs. Pass-\ning these raw audio transcripts directly to the LLM without\nany filtering can negatively affect the overall model\u2019s per-\nformance. Therefore, a preprocessing method is required to\nfilter out noisy text and keep only the parts of the audio that\ncarry meaningful information.\nThe\nfollowing\nsteps\ncombining\nWhisperX[2]\nand\nWhisper-AT[10] are used to refine the original Whisper\ntranscripts to be usable as inputs to the video LMM.\n1. We first apply VAD-based preliminary filtering to the\naudio, and then use the Whisper model with Phoneme-\nbased forced alignment to get temporally aligned text\ntranscriptions.\n2. As Whisper is able to identify the language spoken, all\nnon-English speech can be ignored at this point since\nPG-Video-LLaVA generates responses in English.\n3. For each sentence segment obtained, slice the origi-\nnal audio at the corresponding timestamps and pass to\nWhisper-AT to produce audio-tagging output.\n4. For each sentence segment, consider the top 3 audio\nclasses predicted.\n(a) If \u201cspeech\u201d is not among the top 3 predictions, the\nsegment is ignored.\n(b) If P[music]\n>\nP[speech] and P[music] \u2212\nP[speech] > threshold, the segment is ignored\n(the threshold is set empirically to 1.1).\nFigure 7 shows the effectiveness of our audio transcript\npreprocessing method in filtering out hallucinations, music,\nand garbage characters from the raw audio transcript.\nA.2. Integrating Audio Transcript into the LLM\nThe following prompt template is used when combining the\nspatiotemporal video features and audio transcript with the\nuser instruction text.\nSYSTEM:\nYou are PG-Video-LLaVA, a large\nvision-language assistant.\nYou are able to understand the\nvideo content that the user\nprovides, and assist the user\nwith a variety of tasks using\nnatural language.\nUSER:\n<Instruction>\n<Video-Tokens>\nThe noisy audio transcript\nof this video is:\n<Audio-Transcript>\nASSISTANT:\nB. Visual Grounding: Quantitative Evaluation\nB.1. Overview\nWe introduce novel benchmarks for quantitatively evalu-\nating conversation-based video spatial grounding, based\non two existing spatio-temporal video grounding datasets,\nVidSTG[48] and HC-STVG[34].\nIn conversation-based spatial grounding, the objective is\nto localize interrogative sentences with unknown objects in\nthe given video (e.g. \u201cWhat is caught by the squatting boy\non the floor?\u201d ). Unlike grounding for declarative sentences\nwhere the explicit characteristics of objects (e.g. the class\n\u201ctoy\u201d and visual appearance \u201cyellow\u201d) are present within\nthe sentence itself, grounding for interrogative sentences is\nchallenging due to the fact that it can only depend on re-\nlationships between the unknown object and other objects\n(e.g. the action relation \u201ccaught by the squatting boy\u201d and\nspatial relation \u201con the floor\u201d) (Figure 6). A benchmark\nbased on this task can be regarded as a measure of the suffi-\ncient relationship construction and cross-modal relation rea-\nsoning ability of the video-language model.\nInterrogative Sentence: What is caught by the squatting boy on the floor? \nDeclarative Sentence: \nA little boy with a Christmas hat is catching a yellow \ntoy.\nFigure 6. Interrogative vs declarative sentences\nTo evaluate our model for conversation-based video spa-\ntial grounding, we pass interrogative prompts to the model.\nIt then generates descriptive textual responses to these\nprompts, from which Vicuna-13b-v1.5 extracts relevant re-\nferring expressions. These expressions are then passed into\nthe GroundingDINO-based spatial grounding and tracking\nmodule. For the obtained object tracks, bounding box IoU\n1\nis calculated by comparing them with the ground truth an-\nnotations.\nFrom the two spatiotemporal grounding datasets, to form\na spatial-only grounding benchmark, we crop the video in\nthe temporal axis to contain only the segment where the tar-\nget object is present, and the mean spatial IoU is reported\nas the metric for comparison.\nIt should be noted that we evaluate our model in these\nbenchmarks only in the zero-shot setting, without any train-\ning on these datasets.\n1. Benchmark based on the VidSTG Dataset:\nVid-\nSTG dataset consists of videos paired with multiform sen-\ntences (both interrogative and declarative).\nTo form a\nbenchmark to quantitatively evaluate the performance of\nconversation-based video spatial grounding, we leverage\nthe 5693 video and interrogative sentence pairs in its test\nset.\n2. Benchmark based on HC-STVG Dataset:\nUn-\nlike in VidSTG dataset, in HC-STVG dataset contains only\ndeclarative form sentences for all of its videos. Therefore\ninterrogative sentences are first generated from the declar-\native text captions in 3025 samples of the test set using\nVicuna-13b-v1.5 model. Then the evaluation is performed\nin a similar manner to VidSTG.\nB.2. Generating Interrogative Statements\nThe original text annotations in the HC-STVG dataset are in\nthe declarative statement format. In order to make our video\nprompt-based grounding evaluation pipeline, we extract in-\nterrogative statements (questions) from these text annota-\ntions using Vicuna-13b-v1.5 using the following prompt\ntemplate.\nSYSTEM:\nYou are an intelligent chatbot\ndesigned for generating\nquestion-answer pairs from\nsentences.\nUSER:\nYour task is to generate a\nquestion and answer from the\ngiven sentence.\nThe question should start with\n\u2019Who\u2019.\nThe question should refer to the\nsubject of the given sentence.\nThe answer should include the\nsubject of the given sentence.\nPlease generate the response in\nthe form of a Python dictionary\nstring with keys \u2019Q\u2019 for\nquestion and \u2019A\u2019 for answer.\nEach corresponding value should\nbe the question and answer text\nrespectively.\nFor example, your response\nshould look like this:\n{\u2019Q\u2019:\n\u2019Your question here...\u2019, \u2019A\u2019:\n\u2019Your answer here...\u2019}.\nPlease note that the generated\nquestion and answer should only\ninclude information from the\ngiven sentence.\nPlease process the following\nsentence:\nThe man in the suit goes to the\nman in white and looks at him.\nASSISTANT:\n{\u2019Q\u2019: \u2019Who goes to the man in\nwhite?\u2019, \u2019A\u2019:\u2019The man in the\nsuit\u2019}\nUSER:\nPlease process the following\nsentence:\n<DECLARATIVE_STATEMENT>\nASSISTANT:\nB.3. Extracting Referring Expression Using Vicuna\nIn the quantitative evaluation, we use the following prompt\ntemplate with Vicuna-13b-v1.5 to extract the referring ex-\npression from the output of the video-based LMM, which\nis used as the input prompt to the off-the-shelf-grounding\nmodule.\nSYSTEM:\nYou are an intelligent chatbot\ndesigned for identifying the\nmost relevant subject/object\nphrases in video-based\nquestion-sentence pairs.\nUSER:\nYour task is to compare the\nquestion with the sentence, and\nextract the subject or object\nphrase of the sentence that most\naccurately answers the given\nquestion.\nThe selected phrase should be\nshort and should contain only\none noun.\nThe selected phrase can\ninclude adjectives that\nexplain the attributes of the\nsubject/object.\n2\nHi, I'm Stephanie Ragnodden and I'm going to \nshow you step 3 of a 4 step process to groom your \nhorse to shine naturally. This time we're going to \nuse a finishing brush with a cocoa fiber horse here \nmix because it just gives us a little bit more of an \nedge. \u2026. We're struggling with the oil so we'll put \nthe coat and always cleaning our brush on the \ncurry and we're going to do this from the head all \nthe way to the tail. Thanks for watching.\nHi, I'm Stephanie Ragnodden and I'm going to \nshow you step 3 of a 4 step process to groom your \nhorse to shine naturally. This time we're going to \nuse a finishing brush with a cocoa fiber horse here \nmix because it just gives us a little bit more of an \nedge. \u2026. We're struggling with the oil so we'll put \nthe coat and always cleaning our brush on the \ncurry and we're going to do this from the head all \nthe way to the tail. Thanks for watching.\nYeah You know that? I didn't think that I had a \ndebt to pay Till the king had take what I left away \nIt was all my fault, you beat it to destiny But I \nremember you saying that yesterday There was a \ntime when my heart wasn't on the show \u2026YOUR \nBLESS BUT YOU are a forever Oh to be the best \nWOO Kennedy I'm not saying it right here YOUR \nBLESS I don't know what I thought I might say \nSeems like we never were talking right away \nEvery other minute I'm fuffing my place \n1.5-1.1 2.5-1.1 2.5-1.1 2.5-1.1 2.5-1.1 1.5-1.1\n \nYeah You know that? I didn't think that I had a \ndebt to pay Till the king had take what I left away \nIt was all my fault, you beat it to destiny But I \nremember you saying that yesterday There was a \ntime when my heart wasn't on the show \u2026YOUR \nBLESS BUT YOU are a forever Oh to be the best \nWOO Kennedy I'm not saying it right here YOUR \nBLESS I don't know what I thought I might say \nSeems like we never were talking right away \nEvery other minute I'm fuffing my place \n1.5-1.1 2.5-1.1 2.5-1.1 2.5-1.1 2.5-1.1 1.5-1.1\nTranscript Obtained from Whisper\nTranscript Obtained from Our Filtering Pipeline\nTranscript Obtained from Our Filtering Pipeline\nTranscript Obtained from Our Filtering Pipeline\nTranscript Obtained from Whisper\nTranscript Obtained from Whisper\nFigure 7. Filtering the audio transcript: to remove hallucinations (left), music (center), and garbage (right) characters from the raw audio\ntranscript.\nThe selected phrase should not\nexceed 4 words.\nThe selected phrase should not\ninclude articles (\u2019a\u2019, \u2019the\u2019,\n\u2019and\u2019).\nPlease generate the response in\nthe form of a Python dictionary\nstring with keys \u2019OBJECT\u2019,\nwhere its value is the extracted\nphrase in Python string format.\nDO NOT PROVIDE ANY OTHER OUTPUT\nTEXT OR EXPLANATION. Only\nprovide the Python dictionary.\nFor example, your response\nshould look like this:\n{\u2019OBJECT\u2019: \u2019green toy\u2019}.\nPlease process the following\nvideo-based question-answer\npair:\nQuestion:\nwho is in front of\nthe guitar at the show?\nAnswer:\nA woman in a black\ndress is in front of the guitar\non stage.\nASSISTANT:\n{\u2019OBJECT\u2019: \u2019woman in black\ndress\u2019}\nUSER:\nQuestion:\nwho points to the\nwindow?\nAnswer:\nThe old man is pointing\nto the window.\nASSISTANT:\n{\u2019OBJECT\u2019: \u2019old man\u2019}\nUSER:\nQuestion:\nwho is inside the\nblue car?\nAnswer:\nThe driver of the blue\ncar.\nASSISTANT:\n{\u2019OBJECT\u2019: \u2019driver\u2019}\nUSER:\nPlease process the following\nvideo-based question-answer\npair:\nQuestion:\n<INPUT_TO_VIDEO_LMM>\nAnswer:\n<OUTPUT_OF_VIDEO_LMM>\nASSISTANT:\nB.4. Entity Matching with Vicuna\nAs shown in Figure 2, our method employs an LLM-\npowered entity matching module similar to [49] to match\nthe key phrases in the video-LMM\u2019s output with the object\ntracks obtained from the grounding and tracking module.\nWe use the same prompt template as [49].\n3\nC. Qualitative Results for Visual Grounding\nDescribe this video.\nThe video is a Christmas tree decoration tutorial featuring a woman in a red chair. The woman is seen decorating a Christmas tree \nwith ornaments and lights. The video also features a Christmas tree with lights and ornaments in a living room. The woman is seen \ndecorating the tree while a cat is sitting on the floor. The video is set to the song \"I see trees of green red roses too I see them \nbloom from in you and I think to myself what a wonderful world I see skies of blue\"\nDescribe this video.\nThe video is about a man named Sharin who talks about his classic Mercedes-Benz 190. He introduces us to Cherine Raudis, a \nwoman who owns the car. The video is shot in a backyard and the man is sitting at a table with a cup of coffee. The video also \nshows the car being driven on the road and parked in front of a house.\nBriefly describe the video.\nThe video shows a man in a red jacket and red hat brushing snow off his car in a snowy driveway. He uses a red brush to remove \nthe snow from the top of the car, the sides, and the back windshield. He also uses a scraper to remove ice from the car. The man is \ncareful to make sure that he cleans off all the snow and ice, including the brake lights, to ensure that his car is safe and ready to \ngo.\nDescribe what you see in this video.\n'The video shows Chef Ann Nolan cooking chocolate chip cookies in a Gen Air V2 convection oven. She explains \nthe benefits of using true convection and demonstrates how evenly the cookies cook. The cookies are taken out of \nthe oven and look delicious.'\nFigure 8. Qualitative results for video grounding obtained using image tags as the queries to the off-the-shelf grounding module and an\nentity matching module to match the image tags with the corresponding phrase in the LMM output. (e.g., in the 4th row, the phrase \u2019Chef\nAnn Nolan\u2019 is matched with the image tag \u2019woman\u2019.)\n4\nWho nods his head and turns?\nA man in a white suit nods his head and turns.\nWho pulls off the earrings on their right side?\nA woman wearing a white dress and earrings on her right \nside is seen removing the earrings.\nWho says a few words to the woman in plaid clothes?\nA man in a black suit says a few words to the woman in \nplaid clothes.\nWho puts things in the bowls on both sides as he walks?\nA man in a black hat and a purple shirt puts things in the \nbowls on both sides as he walks.\nWho turns around and walks next to the woman with \nyellow hair?\nA man in a white lab coat turns around and walks next \nto a woman with yellow hair.\nWho raises his hand and touches his head?\nA man in a red shirt raises his hand and touches his \nhead.\nWho pulls the woman to sit down?\nA man in a suit pulls the woman to sit down.\nWho picks up the cup, drinks, puts down the cup and \nturns to look at the man next to her?\nA woman in a green dress picks up a cup, drinks, puts \ndown the cup, and turns to look at a man next to her.\nFigure 9. Qualitative results for video grounding on example videos from the HC-STVG[34] dataset. These results are obtained by using\nVicuna with the prompt template in B.3 to extract the referring expression from the LMM output which is then passed to the off-the-shelf\ngrounding module.\n5\nWho does the woman in the yellow dress hug?\nThe woman in the yellow dress hugs the man in the blue \nsuit.\nWho walks to the wooden frame?\nA woman walks to the wooden frame in the video.\nWho walks to the door and then turns?\nA man walks to the door and then turns around to face \nthe camera.\nWho pushes the woman to the table?\nA man in a white lab coat pushes the woman to the table.\nFigure 10. Qualitative results for visual grounding on the HC-STVG dataset (failure cases): errors in our model\u2019s output (e.g., bottom-\nleft: our model identifies the woman as a man), incorrect localizations in the off-the-shelf grounding module (e.g., top-left), and incorrect\ntracking (e.g., top-right, bottom-right) result in these failure cases.\nWhat does the adult hold outside the room?\nThe adult holds a bottle of champagne outside the \nroom.\nWhat does the adult in white clothes hug?\nThe adult in white clothes is hugging a small dog.\nWho is in front of the guitar at the show?\nThe woman in the black dress is in front of the guitar.\nWhat is beneath the adult on the boat?\nThe boat is on the water, and the man is sitting on the \nback of the boat.\nFigure 11. Qualitative results for video grounding on example videos from the VidSTG[48] dataset\n6\nD. Quantitative Evaluations of Video-based\nConversation Performance\nWe leverage the video-based conversation performance\nbenchmarks introduced in Video-ChatGPT[22],\nwhile\nchanging the evaluation LLM from GPT-3.5-Turbo to\nVicuna-13b-v1.5 model. The prompt templates used with\nVicuna are as same as with [22].\nVideo-based Generative Performance Benchmarking:\nIn this benchmark we continue to use the same test set of\n500 samples curated from the ActivityNet-200[11] videos\nas in [22].\nZero-Shot Question-Answer Evaluation:\nFollow-\ning Video-ChatGPT, we perform zero-shot evaluation\non four standard open-ended question-answer datasets:\nMSRVTT[40], MSVD[39], TGIF[16], and ActivityNet-\nQA[44]. No specific training is performed on these datasets,\nand the evaluation is performed in a zero-shot manner.\n7\n"
  },
  {
    "title": "Visual In-Context Prompting",
    "link": "https://arxiv.org/pdf/2311.13601.pdf",
    "upvote": "13",
    "text": "Visual In-Context Prompting\nFeng Li\u2660, Qing Jiang\u266f, Hao Zhang\u2660, Tianhe Ren\u2020, Shilong Liu\u00b6, Xueyan Zou\u00a7, Huaizhe Xu\u2660,\nHongyang Li\u266f, Chunyuan Li\u2021, Jianwei Yang\u20211, Lei Zhang\u20202, Jianfeng Gao\u20212\n\u2660 HKUST\n\u2021 Microsoft Research, Redmond\n\u2020 IDEA\n\u266f SCUT\n\u00b6 Tsinghua\n\u00a7 UW-Madison\n1. Project Lead 2. Equal Advisory Contribution\n\u2026\nN in-context examples\nZero-shot Video Object and Part Segmentation\nDINOv\nDINOv\nDINOv\nDINOv\nDINOv\nVisual Prompting Referring Segmentation\nVisual Prompting Generic Segmentation\nFigure 1. Our model DINOv supports generic and referring segmentation to associate multiple or single objects with the user input visual\nprompts. A user can input one or more in-context visual prompts (scribbles, masks, boxes, etc.) to improve the segmentation performance.\nAbstract\nIn-context prompting in large language models (LLMs)\nhas become a prevalent approach to improve zero-shot ca-\npabilities, but this idea is less explored in the vision do-\nmain.\nExisting visual prompting methods focus on re-\nferring segmentation to segment the most relevant object,\nfalling short of addressing many generic vision tasks like\nopen-set segmentation and detection.\nIn this paper, we\nintroduce a universal visual in-context prompting frame-\nwork for both tasks, as shown in Fig. 1. In particular, we\nbuild on top of an encoder-decoder architecture, and de-\nvelop a versatile prompt encoder to support a variety of\nprompts like strokes, boxes, and points.\nWe further en-\nhance it to take an arbitrary number of reference image\nsegments as the context. Our extensive explorations show\nthat the proposed visual in-context prompting elicits ex-\ntraordinary referring and generic segmentation capabili-\nties to refer and detect, yielding competitive performance to\nclose-set in-domain datasets and showing promising results\non many open-set segmentation datasets. By joint train-\ning on COCO and SA-1B, DINOv achieves 57.7 PQ on\nCOCO and 23.2 PQ on ADE20K. Code will be available\nat https://github.com/UX-Decoder/DINOv\n1\narXiv:2311.13601v1  [cs.CV]  22 Nov 2023\n1. Introduction\nThe recent progress in large language models (LLMs)\nlike GPT [1, 27] has shown promising results towards arti-\nficial general intelligence (AGI) by training unified models\non large amounts of text data. These giant LLMs manifest\nthemselves with intriguing emerging capabilities such as in-\ncontext learning. Nevertheless, similar paradigms have not\nyet succeeded in solving all vision tasks due to the diversity\nof scenarios in computer vision [15]. Some works [23, 50]\nhave combined LLMs and vision models to tackle complex\nimage understanding tasks with text outputs such as visual\nquestion answering, but challenges remain in fine-grained\ntasks that require pixel-level outputs, like instance masks,\nrather than just text.\nTo this end, the community has observed a growing\ninterest in the development of language-enhanced vision\nfoundation models. These models demonstrate profound\ncompetencies in open-world visual understanding tasks us-\ning text prompts, encompassing areas like open-set detec-\ntion [19, 24, 45] and segmentation [38, 42, 45, 51].\nVi-\nsual prompt, a different prompting mechanism has been\nexplored in some recent segmentation models [13, 17, 52].\nIn these works, different visual prompting formats (e.g.,\npoints, boxes and strokes, etc) have been explored to facili-\ntate the segmentation of visual contents specified by users.\nIn-context learning, an appealing capability in LLMs,\nhas been less explored. It specifies the new task instruc-\ntion using examples, and allows models to adapt to new\ntasks or domains without explicit retraining. One pioneer-\ning work for visual in-context learning is SegGPT [34],\nwhich demonstrates the ability to output an image mask\nbased on visual examples. However, these works focus on\nassociating a user visual prompt with one most relevant ob-\nject and have the limited ability to identify multiple objects\nof the same semantic concept. More importantly, prompt-\ning in the image pixels with colorful masks inherently fails\nto generalize to novel concepts. As such, it is not com-\npetent to address many generic vision tasks like open-set\nobject detection and segmentation, which often require the\nsegmentation of multiple objects of a given concept. On\nthe other hand, textual prompting in vision models exhibit\nnotable flexibility in managing both referring and generic\ntasks in detection or segmentation [24, 51]. Nevertheless,\nthey are arguably not favorable for in-context settings in\nthat they cannot take segmentation masks as the inputs. In\nthis paper, we strive to develop a model that supports visual\nin-context prompting for all types of image segmentation\ntasks. A comparison between our work and previous work\nis shown in Fig. 2. Besides supporting both single-image\nand cross-image visual prompting, our model distinguishes\nitself by effectively handling both referring and generic seg-\nmentation problems.\nTo achieve this goal, we build a model called DINOv to\nSEEM\nOWL-VIT/\nMQ-Det\nVisual In-Context Prompt\nReferring\nGeneric\nOurs\nPrompt\nSAM\nOVSeg\nSegmentation Output\nInput Image\nor\nGrounding DINO/ \nGLIP\n\u2026\nN in-context examples\nTasks\nSegGPT\nVisual prompt\nImage prompt\nText Prompt\n<cat>\nFigure 2. Comparison with related works. Generic: segment all\nobjects of the same semantic concept that match the user prompt.\nRefer: segment a particular object with the user input visual\nprompts. Image prompt: crop the image regions as prompts. (sin-\ngle) Visual prompt: one image-prompt example to segment. In-\ncontext prompt: one or multiple image-prompt examples. We can\ndo single-image and cross-image visual prompting tasks and sup-\nport referring and generic segmentation.\nsupport versatile visual prompting capabilities, based on the\nunified detection and segmentation model MaskDINO [18].\nDINOv follows the general encoder-decoder design with\nan extra prompt encoder to formulate and sample visual\nprompts. The decoder takes in segmentation queries and\nreference prompt queries to generate segmentation masks\nand target visual prompts, and we associate the output seg-\nmentation masks with the target prompt queries for the fi-\nnal output. We can define the visual in-context samples\nwith a set of reference image (Q) - visual prompt (A) pairs.\nThe visual prompt can be in various types, including mask,\nscribble, box, etc. With the in-context examples, our model\ntakes in a target image and outputs the masks. The cre-\nation of target visual prompts involves an initial step where\na prompt encoder extracts reference visual prompts from\na Q-A pair. This is followed by a decoder to get the tar-\nget visual prompt by attending reference visual prompts to\nthe target image. During training, to construct positive and\nnegative samples for generic segmentation, we sample ref-\nerence visual prompts in a batch across different images. To\naddress task and data discrepancies, we formulate generic\nlatent queries and point queries for generic and referring\nsegmentation, respectively. By joint training on COCO [21]\nand SA-1B [13] for generic and referring segmentation, our\nmodel attains competitive performance on in-domain seg-\nmentation tasks compared with text-prompted models and\nshows promising generalization capability on a wide range\n2\nPrompt \nEncoder\nVision Encoder\nVision Tokens\nDINOv\nSegmentation Query\n...\nVisual Prompts\nVisual Prompt\nDINOv\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\nDINOv\nPrompt \nEncoder\nSample & \nAggregate \nDINOv\n0 0\n1 0\n0\n1 0\n0 0 0\n0\n0 1\n0 0 0\n1\nDINOv\nPrompt \nEncoder\n...\n(b) Visual Generic Segmentation (c) Visual Referring Segmentation \n(a) General Framework\nInput Images\nInput Images\nInput Image\nVisual Prompts\nVisual Prompts\nLoss Pivot\nLoss Pivot\nMatching & Loss\n1\nDINOv\nShare\n0\n0\n1\n0\n0\n0\nFigure 3. DINOv is a universal segmentation framework that can do generic segmentation and referring image segmentation. The vision\nencoder is used to extract image features.(b) An illustration of losses for visual generic segmentation. In the example, there are 6 visual\nprompts sampled from 6 masks from 3 categories. The visual prompts from the instances of the same class are averaged as the class\nembedding. Each colume of the matching matrix is a 3-dimension one-hot vector which is a one-hot class label of the instance; (c) An\nillustration of losses for visual referring segmentation. Each visual prompt is classified to one of the 6 instances.\nof open-set segmentation benchmarks using purely visual\nprompts.\nTo summarize, our contributions are threefold:\n\u2022 We are the first to extend visual in-context prompting\nto support generic vision tasks like open-set generic\nsegmentation and detection, and achieve comparable\nperformance with text prompt-based open-set models.\n\u2022 We build DINOv, a unified framework for referring\nsegmentation and generic segmentation based on vi-\nsual in-context prompting.\nThis unification simpli-\nfies model design and allows our model to consume\nboth semantically-labelled and unlabelled data for bet-\nter performance.\n\u2022 We conduct extensive experiments and visualizations\nto show that our model can handle generic, referring,\nand video object segmentation tasks.\nOur early at-\ntempts exhibit promising results on open-set segmen-\ntation and detection with visual prompting.\n2. Method\n2.1. Unified Formulation for Segmentation Tasks\nIn this paper, we concentrate on visual prompting tasks\ninvolving images, encompassing both generic segmentation\nand referring segmentation tasks. Given N reference im-\nages I = {I1, ..., IN} \u2208 RN\u00d7H\u00d7W \u00d73 with the corre-\nsponding visual prompts P = {p1, ..., pN}, DINOv aims\nto segment objects of interest on a new target image It. The\nvisual prompts include masks, boxes, scribbles, points, etc.\nThe interested objects can be a particular object for referring\nsegmentation or all objects of the same semantic concept for\ngeneric segmentation. Note that the reference image can be\nidentical to the target image, in which the task reduces to\nsingle-image visual prompting segmentation.\nTo address these tasks, DINOv utilizes a comprehen-\nsive query-based encoder-decoder architecture. This archi-\ntecture comprises a vision encoder, denoted as Enc, re-\nsponsible for extracting image features, a prompt encoder\nreferred to as PromptEncoder, designed to extract vi-\nsual prompt features by combining image features and user-\nprovided visual prompts, and a general decoder represented\nas Decoder, which generates masks and visual concepts\nbased on the segmentation query and visual prompt fea-\ntures. Upon receiving the input image and user-provided vi-\nsual prompts, our initial step involves extracting image fea-\ntures denoted as Z using the vision encoder. Subsequently,\nwe feed both the image features and visual prompts into the\nprompt encoder to extract the reference visual prompt F\nand subsequently sample the query visual prompt features\nQp.Formally, we have:\nZ = Enc(I), Z = Enc(It)\nF = PromptEncoder(P, Z)\nQp = PromptSample(F)\n(1)\nIn addition to the visual prompt features Qp, DINOv in-\ncorporates segmentation queries Qs for proposal extraction.\nA shared decoder is employed to decode outputs for both\nQs and Qp while performing cross-attention with respect\nto the target image feature Z.\n3\nTrain\nInference\n(a) Generic queries for \ngeneric segmentation\n(b) Interactive point queries for referring segmentation\nDINOv\n...\nDINOv\nLearnable Position Queries (Boxes)\nLearnable Content Queries\nSampled Position Queries (Points)\n...\n...\n...\nDuplicate\nDuplicate\nLearnable Content Queries\nFigure 4. DINOv query formulation of generic and referring segmentation tasks.\nFigure 5. Prompt encoder to encode visual prompt from reference\nimages. We use three masked cross-attention from the vision en-\ncoder small feature map to large feature map.\nOs = Decoder (Qs; Z)\nOp = Decoder (Qp; Z)\n\u27e8M, B\u27e9 = MaskHead(Os)\nCg, Cr = PromptClassifier(Os, Op)\n(2)\nHere, Os represents the decoded segmentation query fea-\ntures, Op corresponds to the decoded target visual prompt\nfeatures, while M and B denote the predicted masks and\nboxes, respectively.\nFurthermore, we have Cg and Cr\nas the predicted matching scores for generic segmentation\nand referring segmentation tasks. These scores are derived\nthrough the use of a PromptClassifier, which computes the\nsimilarity between Os and Op.\nPromptClassifier.\nWe clarify the definition of the\nprompt classifier, denoted as PromptClassifier(\u00b7, \u00b7), for\nboth generic segmentation and referring segmentation tasks\nhere. In the case of generic segmentation tasks like instance\nand panoptic segmentation, the typical objective is to clas-\nsify object features Os into respective categories. When\nemploying visual prompting for generic segmentation tasks,\nthe distinction lies in the utilization of visual prompt fea-\ntures Op as class embeddings. This is illustrated in the fol-\nlowing equation:\nCg = g(Os)g(Op\nT ), Cg \u2208 RNp\u00d7Ns\n(3)\nwhere Np and Ns are the number of visual prompts and\nobject features. g is the linear projection for generic seg-\nmentation task. Each of Ns objects is classified into one of\nNp classes. For visual referring segmentation, the objective\ndiffers. Here, each visual prompt is employed to identify the\nmost closely matched instance within the target image. This\ntask can be framed as a classification problem, where each\nvisual prompt is assigned to a specific instance within the\ntarget image. It\u2019s important to note that during our training\nphase, the target image and the reference image are identi-\ncal. The matching score matrix for referring segmentation\nis structured as follows:\nCr = h(Op)h(Os\nT ), Cr \u2208 RNs\u00d7Nq\n(4)\nh is the linear projection for referring segmentation task.\nFig. 6(b) and (c) provide an illustrative representation of\nthe two tasks. In our implementation, the generic segmen-\ntation task involves finding the most suitable visual prompt\nfor each mask proposal, effectively pivoting the loss from\na query to all prompts. Conversely, the referring segmen-\ntation task focuses on matching a given visual prompt to\na specific mask proposal, with the loss pivot transitioning\nfrom a prompt to all proposals. As indicated in Equations 3\nand 4, the PromptClassifier for both generic and refer-\nring segmentation tasks share a similar formulation. Conse-\nquently, they can share the entire framework, except for the\ntwo distinct linear layers denoted as g and h.\n2.2. Visual Prompt Formulation\nThe heart part of our DINOv is the proposed visual\nprompting mechanism. As shown in Eq. 1 and Eq. 2, we\nemploy two modules to get the final visual prompt:\n\u2022 A PromptEncoder to encode the reference visual\nprompt F from the reference image features (followed\nby a sampling process to get query visual prompt Qp).\n\u2022 A Decoder (shared with the segmentation decoder)\nto decode outputs for the target visual prompt Op by\ninteracting with the target image features.\nThis design allows our model to first encode the reference\nvisual prompt and then adapt the prompt to the target image\n4\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n4\nFc[c]=[] for c in C; # Initialize visual prompt dict by semantic category.\n5\nFc[c].append(f) for c, f in zip(C, F); # Gather visual prompts of the same semantic category.\n6\nn = Randint(1, N; # Randomly select the number of in-context examples.\n7\nSc = RandomSelect(F[c], n) for c in C; # For each semantic category, randomly select n prompts to represent\na semantic category.\n8\nQp = [Aggregate(Sc[c]) for c in C]; # Perform prompt aggregation to get one reference prompt token from\nmultiple in-context prompt features for each semantic category.\n2.3. Prompt Sampling\nWe propose two prompt sampling strategies for referring\nsegmentation and generic segmentation, respectively.\nReferring segmentation. For referring segmentation, we\nperform \u201cself-referring\u201d during training, where the refer-\nence image is the same as the target image. We sample\na prompt from an instance and train the model to refer to\nthe same instance.\nTherefore, we can utilize large-scale\nsegmentation data such as SA-1B to train our model. Al-\nthough trained on the same instances, our model can do\ncross-image referring during inference.\nAs illustrated in\nFig. 3(c), we can change target images into different im-\nages to perform cross-image referring.\nGeneric segmentation.\nThe sampling strategies are\nslightly different during training and inference:\n\u2022 Training.\nDuring training, it is critical to construct\npositive and negative visual prompt samples. There-\nfore, we construct visual prompts through a large im-\nage training batch. As shown in Algorithm 1, we ini-\ntially group together reference visual prompts of the\nsame semantic category across all images within a\nbatch. For each semantic category, we randomly se-\nlect [1, N] in-context examples and perform an aggre-\ngation to create a single reference visual prompt fea-\nture for each semantic category Qp, which will be fed\nto the decoder to interact with the target image to get\nthe final target visual prompt Op. Hence, the number\nof semantic categories is the same as the target visual\nprompts. Note that a batch of images may not contain\nall the semantic categories in the dataset, therefore the\nnumber of semantic categories varies during training.\n\u2022 Inference.\nDuring the inference stage, taking the\nCOCO Dataset as an example, we pre-extract cor-\nresponding visual prompt features based on mask\nprompts for all semantic categories within the training\nset. Subsequently, for evaluation, we randomly select\nN (default to 16) features for each semantic category\nas its representative visual prompt feature. This en-\nsures that our inference stage has the same number of\ncategories as in traditional open-set evaluation to pre-\nvent information leakage.\n2.4. Decoder Query Formulation\nIn DINOv, we designed two types of segmentation\nqueries to address two different tasks as depicted in Fig. 4.\nFor generic segmentation, the query is a number of learn-\nable ones similar to MaskDINO [16]. For the visual re-\nferring task, we adopt the interactive point query following\nSemantic-SAM [17], so that we can exploit the rich granu-\nlarities in SA-1B [13]. Similar to Semantic-SAM, the visual\nprompts (points or boxes) are both converted into anchor\nbox format, and then the position of each visual prompt will\nbe encoded into position queries. Each position query is\nduplicated and subsequently combined with content queries\nof different granularities as the final segmentation queries.\nFor the training on SA-1B, in order to avoid excessive com-\nputational overhead on the model, we selectively sample a\nsubset of points contained within this visual concept as pos-\nitive point queries. Concurrently, we randomly sample a\nsubset of points from the remaining areas to serve as nega-\ntive points. During the inference stage, we sample the initial\npoint position queries on 20\u00d720 uniformly distributed grid\nas the initial point position for a single frame. [Hao: The\nquery type names should be unified with Fig.4]\n3. Experiments\n3.1. Setup\nDataset and Settings.\nIn our experiments, we jointly\ntrain on two types of data: segmentation data with se-\nmantic labels and segmentation data with only pixel an-\nnotations (SA-1B [13]. For semantically-labeled data, we\nuse COCO2017 [21] panoptic segmentation dataset with\naround 110K images. For SA-1B, we employ a 20% por-\ntion subset with around 2M images. We evaluate our model\n5\nAlgorithm 1: Pseudo code of Prompt Sampling for Generic Segmentation Task.\n# Inputs: A list of encoded reference visual prompt F with length M, M is the total number of possible\nprompting examples.\nThe ground-truth semantic category (i.e., dogs) of each reference visual prompt forms\nanother list C with length M.\nDuring training, F is acquired from a batch of training images (i.e., 64\nimages).\nDuring inference, the batch is the whole training image set.\n# Variables:\nDefined maximum in-context length N for each semantic category.\n# Output:\nQuery visual prompt Qp\n1 def Prompt Sample(F):\n2\nC=Unique(C);# C is a list that contains all the semantic categories in this training batch.\n3\nFc=Dict();# Fc is visual prompt dict, where key is the semantic category and value are the reference prompt\nfeatures.\n4\nFc[c]=[] for c in C; # Initialize visual prompt dict by semantic category.\n5\nFc[c].append(f) for c, f in zip(C, F); # Gather visual prompts of the same semantic category.\n6\nn = Randint(1, N; # Randomly select the number of in-context examples.\n7\nSc = RandomSelect(F[c], n) for c in C; # For each semantic category, randomly select n prompts to represent\na semantic category.\n8\nQp = [Aggregate(Sc[c]) for c in C]; # Perform prompt aggregation to get one reference prompt token from\nmultiple in-context prompt features for each semantic category.\nin a flexible way. As we attempt to express visual concepts\nthrough visual prompts, a straightforward way is to employ\na pre-trained vision encoder (e.g., CLIP [29]) to process the\nreference images guided by user prompts [26]. However,\nit may encounter several challenges: (i) the vision encoder\ntakes cropped images as inputs, which causes substantial\ndomain shift, especially for small objects [48]; (ii) The vi-\nsual features extracted from CLIP tend to be more semantic\nand may not meet the demands in VOS tasks. As we will\nshow in our ablation study, employing a CLIP vision en-\ncoder to extract visual prompts has a clear inferior general-\nization ability.\nTo address these issues, we reuse the vision encoder in\nour model and develop a simple yet effective prompt en-\ncoder. It extracts visual features corresponding to the lo-\ncations indicated by various forms of visual prompts. To\ncapture visual details of different granularities, we have in-\ncorporated multiple layers (default to 3) of the Mask Cross\nAttention Layer, as shown in Fig. 5. Each layer takes the im-\nage features extracted at different levels (output multi-scale\nfeatures from the vision encoder, ranging from lower to\nhigher resolutions) as inputs, utilizes the regions defined by\nthe visual inputs as masks, and employs learnable queries\nto process the features at the corresponding positions to get\nthe visual prompt features.\n2.3. Prompt Sampling\nWe introduce two prompt sampling strategies tailored for\nreferring segmentation and generic segmentation, respec-\ntively.\nReferring segmentation. In the case of referring segmenta-\ntion, we employ a \u201cself-referring\u201d approach during training,\nwherein the reference image is identical to the target image.\nHere, we sample a prompt from an instance and train the\nmodel to refer to the same instance. This approach allows\nus to leverage extensive segmentation data, such as SA-1B,\nfor training our model effectively. Despite being trained on\nthe same instances, our model demonstrates the capability\nto perform cross-image referring during inference. As il-\nlustrated in Fig. 6(c), we can change the target images to\nvarious different images, enabling the model to effectively\nengage in cross-image referring tasks.\nGeneric segmentation.\nThe sampling strategies are\nslightly different during training and inference:\n\u2022 Training.\nIn the training process, it is crucial to create both pos-\nin a flexible way. As we attempt to express visual concepts\nthrough visual prompts, a straightforward way is to employ\na pre-trained vision encoder (e.g., CLIP [29]) to process the\nreference images guided by user prompts [26]. However,\nit may encounter several challenges: (i) the vision encoder\ntakes cropped images as inputs, which causes substantial\ndomain shift, especially for small objects [48]; (ii) The vi-\nsual features extracted from CLIP tend to be more semantic\nand may not meet the demands in VOS tasks. As we will\nshow in our ablation study, employing a CLIP vision en-\ncoder to extract visual prompts has a clear inferior general-\nization ability.\nTo address these issues, we reuse the vision encoder in\nour model and develop a simple yet effective prompt en-\ncoder. It extracts visual features corresponding to the lo-\ncations indicated by various forms of visual prompts. To\ncapture visual details of different granularities, we have in-\ncorporated multiple layers (default to 3) of the Mask Cross\nAttention Layer, as shown in Fig. 5. Each layer takes the im-\nage features extracted at different levels (output multi-scale\nfeatures from the vision encoder, ranging from lower to\nhigher resolutions) as inputs, utilizes the regions defined by\nthe visual inputs as masks, and employs learnable queries\nto process the features at the corresponding positions to get\nthe visual prompt features.\n2.3. Prompt Sampling\nWe introduce two prompt sampling strategies tailored for\nreferring segmentation and generic segmentation, respec-\ntively.\nReferring segmentation. In the case of referring segmenta-\ntion, we employ a \u201cself-referring\u201d approach during training,\nwherein the reference image is identical to the target image.\nHere, we sample a prompt from an instance and train the\nmodel to refer to the same instance. This approach allows\nus to leverage extensive segmentation data, such as SA-1B,\nfor training our model effectively. Despite being trained on\nthe same instances, our model demonstrates the capability\nto perform cross-image referring during inference. As il-\nlustrated in Fig. 6(c), we can change the target images to\nvarious different images, enabling the model to effectively\nengage in cross-image referring tasks.\nGeneric segmentation.\nThe sampling strategies are\nslightly different during training and inference:\n\u2022 Training. In the training process, it is crucial to cre-\nate both positive and negative visual prompt samples.\nTo achieve this, we generate visual prompts by utiliz-\ning a large image training batch. As depicted in Al-\ngorithm 1, our approach begins by grouping together\nreference visual prompt F of the same semantic cate-\ngory across all images within a training batch. For each\nsemantic category, we then randomly select a variable\nnumber of in-context examples, ranging from 1 to N,\nand perform an aggregation process to generate refer-\nence visual prompt tokens Qp, where each reference\nvisual prompt token corresponds to a specific seman-\ntic category. Qp is subsequently fed into the decoder,\nwhere it interacts with the target image to produce the\nfinal target visual prompt Op. Consequently, we at-\ntain the same number of target visual prompts to se-\nmantic categories. It is noteworthy that a given batch\nof images may not encompass all semantic categories\npresent in the dataset, resulting in a variable number of\nsemantic categories during the training process.\n\u2022 Inference.\nDuring the inference stage, using the\nCOCO dataset as an example, we pre-extract the re-\nspective visual prompt features based on mask prompts\nfor all semantic categories established during the train-\ning phase. For evaluation purposes, we adopt a ran-\ndom selection approach, where we choose N (16 by\ndefault) features for each semantic category.\nThese\nselected features act as representative visual prompt\nfeatures for each category. This practice ensures that\n5\nour inference stage maintains the same number of cat-\negories as in traditional open-set evaluation, effectively\npreventing any potential information leakage.\n2.4. Decoder Query Formulation\nIn DINOv, we designed two types of segmentation\nqueries to address two different tasks as depicted in Fig. 4.\nFor generic segmentation, the query is a number of learn-\nable ones similar to MaskDINO [16]. For the visual re-\nferring task, we adopt the interactive point query following\nSemantic-SAM [17], so that we can exploit the rich granu-\nlarities in SA-1B [13]. Similar to Semantic-SAM, the visual\nprompts (points or boxes) are both converted into anchor\nbox format, and then the position of each visual prompt will\nbe encoded into position queries. Each position query is\nduplicated and subsequently combined with content queries\nof different granularities as the final segmentation queries.\nFor the training on SA-1B, in order to avoid excessive com-\nputational overhead on the model, we selectively sample a\nsubset of points contained within this visual concept as pos-\nitive point queries. Concurrently, we randomly sample a\nsubset of points from the remaining areas to serve as nega-\ntive points. During the inference stage, we sample the initial\npoint position queries on 20\u00d720 uniformly distributed grid\nas the initial point position for a single frame.\n3. Experiments\n3.1. Setup\nDataset and Settings.\nIn our experiments, we jointly\ntrain on two types of data: segmentation data with se-\nmantic labels and segmentation data with only pixel an-\nnotations (SA-1B [13]. For semantically-labeled data, we\nuse COCO2017 [21] panoptic segmentation dataset with\naround 110K images. For SA-1B, we employ a 20% por-\ntion subset with around 2M images. We evaluate our model\non a wide range of tasks and datasets with only visual\nprompts, including: 1. Open-set panoptic segmentation on\nCOCO2017 [21] and ADE20K [49]; 2. Segmentation in the\nwild (SegInW) [51] which includes 25 instance segmenta-\ntion datasets; 3. Object detection in the wild (ODinW) [19]\nthat encompasses over 35 datasets; 4. Zero-shot Video ob-\nject segmentation (VOS) on DAVIS2017 [28], DAVIS2016-\nInteractive [28], and Youtube-VOS 2018 [39].\nImplementation Details. We provide implementation de-\ntails in the Appendix.\nEvaluation Metrics. For all segmentation and detection\ntasks, we use standard evaluation metrics: PQ (Panoptic\nQuality) for panoptic segmentation, AP (Average Precision)\nfor instance segmentation (mask AP) and detection (box\nAP), and mIoU (mean Intersection over Union) for seman-\ntic segmentation. For VOS tasks, we follow previous semi-\nsupervised models to use region similarity J and contour\naccuracy F. We also adopt the averaged score J&F as the\nmetric for DAVIS2017 and averaged overall score G for\nYoutube-VOS 2018. Note that Youtube-VOS 2018 also re-\nports J and F for seen and unseen splits.\n3.2. Generic Segmentation and Detection\nWe evaluate our visual prompt based generic segmenta-\ntion performance in Table 1.\nIn-domain Segmentation on COCO. Compared to other\nmodels trained for visual prompts, we achieve significantly\nbetter results. For example, we surpass SegGPT [34] and\nPainter [33] by 14.3 PQ and 25.5 PQ. In addition, With\njust a few visual in-context prompts for each category, our\nmodel achieves comparable results with previous close-set\nor open-set models on COCO. For example, the panop-\ntic segmentation performance gap between DINOv and our\nbaseline Mask DINO is only 0.6 PQ (57.7 PQ vs 58.3 PQ).\nOut-domain open-set segmentation on ADE20K. After\ntraining with visual prompt on COCO and SAM, we do\nzero-shot evaluation on ADE20K to validate its open-set\nsegmentation capability when seeing novel visual concepts.\nTo our best knowledge, it is the first time to use visual\nprompt for open-set segmentation. Compared with previ-\nous text-prompted open-set models, we achieve compara-\nble or better performance with only COCO semantic data\nand no semantic knowledge from large pre-trained mod-\nels. Especially, compared with our baseline OpenSeed, we\nachieve better performance with much fewer data. Note\nthat FC-CLIP [42] employs a frozen CLIP to do text-based\nopen-set segmentation. As the text and visual features are\naligned in CLIP, we also attempt to prompt a pre-trained\nFC-CLIP with visual features from CLIP to test its open-set\nability with visual prompts. However, its visual prompt-\ning performance largely lags behind its text-prompted re-\nsults. Therefore, it is non-trivial to transfer a multi-modal\ntext-based open-set model to do visual-prompted recogni-\ntion well. The results indicate that visual prompts can gen-\neralize well to new concepts.\nSegmentation and detection in the wild. We also vali-\ndate the generalization capability of visual prompting on\nsome diversified and domain-specific datasets including\nSegInW and ODinW, which in total encompass more than\n60 datasets. These datasets contain many real-scenario or\nrare categories. As these datasets all focus on instance-level\nsegmentation, we report the average and median AP (AP-\nAverage and AP-Median) over all datasets. We first eval-\nuate the Segmentation in the Wild (SegInW) benchmark,\nwhich consists of 25 datasets. With visual prompting, DI-\nNOv achieves a significant performance improvement over\nour baseline OpenSeed. For example, Our best AP-Average\nexceeds OpenSeed by 4.5 AP. We further evaluate Object\nDetection in the Wild (ODinW), which is composed of 35\ndatasets with bounding box annotations. As shown in Ta-\n6\nTable 1. One suit of weights for generic visual in-context segmentation on multiple datasets. Our model is trained on COCO and SA-1B\ndata. Note: \u201c\u2212\u201d denotes the model does not have number reported or does not have the ability for the specific task. \u22c6 means it is the\ntest set results. \u2020 FC-CLIP adopts a frozen CLIP for open-set (text), we prompt the FC-CLIP with CLIP visual features to simulate visual\npromoting. # FC-CLIP and ODISE rely on frozen CLIP and Stable Diffusion knowledge. Mask DINO [18] is our baseline for comparison.\nMethod\nSemantic Data\nType\nCOCO (in-domain)\nADE (out-domain)\nSegInW (out-domain)\nPQ\nmask AP\nbox AP\nmIoU\nPQ\nmask AP\nbox AP\nmIoU\nAP-Average\nAP-Median\nMask2Former-T [2]\nCOCO\nClosed-set\n53.2\n43.3\n46.1\n63.2\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nMask2Former-B [2]\nCOCO\n56.4\n46.3\n49.5\n67.1\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nMask2Former-L [2]\nCOCO\n57.8\n48.6\n52.1\n67.4\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nOneFormer-L [9]\nCOCO\n57.9\n48.9\n\u2212\n67.4\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nMaskDINO-L [16]\nCOCO\n58.3\n50.6\n56.2\n67.5\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nPano/SegFormer-B [36]\nCOCO\n55.4\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nkMaX-DeepLab-L [43]\nCOCO\n48.7\n58.1\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nGLIPv2-H [46]\nCOCO+O365+GOLDG+...\nText Open-set\n\u2212\n48.9\u22c6\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nMaskCLIP (L) [5]\nYFCC100M\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n15.1\n6.0\n\u2212\n23.7\n\u2212\n#ODISE-H [37]\nCOCO (Stable diffusion))\n45.6\n38.4\n\u2212\n52.4\n23.4\n13.9\n\u2212\n28.7\n\u2212\n\u2212\n#FC-CLIP-L [42]\nCOCO (CLIP)\n54.4\n44.6\n\u2212\n63.7\n26.8\n16.8\n\u2212\n34.1\n\u2212\n\u2212\nOpenSeed-T [45]\nCOCO+O365\n55.4\n47.6\n52.0\n64.0\n19.8\n14.1\n17.0\n22.9\n33.9\n21.5\nX-Decoder-T [51]\nCOCO+CC3M+..\n51.4\n40.5\n43.6\n62.8\n18.8\n9.8\n\u2212\n25.0\n22.7\n15.2\nX-Decoder-L [51]\nCOCO+CC3M+..\n56.9\n46.7\n\u2212\n67.5\n21.8\n13.1\n\u2212\n29.6\n36.1\n38.7\nOpenSeed-L [45]\nCOCO+O365\n59.5\n53.2\n58.2\n68.6\n19.7\n15.0\n17.7\n23.4\n36.1\n38.7\nFC-CLIP\u2020-L [42]\nCOCO\n\u2212\n\u2212\n\u2212\n\u2212\n2.3\n4.1\n\u2212\n7.8\n\u2212\n\u2212\nSegGPT-L [34]\nCOCO+ADE+VOC+..\nVisual Prompt\n43.4\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nPainter-L [33]\nCOCO+ADE+NYUv2\n34.4\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nDINOv-T (Ours)\nCOCO\n49.0\n41.5\n45.2\n57.0\n19.4\n11.4\n12.8\n21.9\n39.5\n41.6\nDINOv-L (Ours)\nCOCO\n57.7\n50.4\n54.2\n66.7\n23.2\n15.1\n14.3\n25.3\n40.6\n44.6\nTable 2. One suit of weights on ODinW benchmark. Average and\nmedian AP across 35 datasets are reported for simplicity.\nModel\nPretrain Data\nAverage\nMedian\nMDETR [12]\nGOLDG, REFC\n10.7\n3.0\nGLIP-T [19]\nObject365\n11.4\n1.6\nOpenSeed (T) (Ours)\nObject365, COCO\n14.2\n3.1\nOpenSeed (L) (Ours)\nObject365, COCO\n15.2\n5.0\nDINOv (T) (Ours)\nCOCO, SAM\n14.9\n5.4\nDINOv (L) (Ours)\nCOCO, SAM\n15.7\n4.8\nble 2, though we only employ much fewer semantically la-\nbeled data, we achieve better performance compared with\nprevious models under similar settings.\n3.3. Video Object Segmentation\nVideo object segmentation (VOS) aims to segment an in-\nterested object in a video by giving text or visual clues. Our\nmodel focuses on the semi-supervised setting, which seg-\nments a particular object throughout a video by giving vi-\nsual clues in the first frame. In DINOv, the visual prompt\noriginates from one single image (generic/referring seg-\nmentation) or other images in one batch (generic segmenta-\ntion). Therefore, our model has learned to prompt with vi-\nsual features from other images. Therefore, DINOv is able\nto do video object segmentation (VOS) by replacing cur-\nrent frame visual prompt features with previous frames. For\nmore accurate tracking, we also store the visual features of\nthe predicted mask in previous frames. These features, de-\nnoted as memory visual prompts, will be averaged together\nwith the first frame\u2019s given prompt to construct the visual\nprompt of the current frame. Details of the memory visual\nprompt and ablations are in the Appendix. By default, the\nmemory length is set to 8. In Table 3, we conduct (inter-\nactive) video object segmentation evaluation on DAVIS17,\nDAVIS2016-Interactive, and Youtube-VOS 2018. The re-\nsults of DAVIS2017 and Youtube-VOS 2018 indicate our\nmodel achieves better performance than SEEM and Per-\nSAM. In addition, DINOv can also do interactive VOS, and\nour performance on DAVIS16-Interactive achieves signifi-\ncantly better performance compared with models not using\nvideo data for training.\n3.4. Ablation\nEffectiveness of Query Formulation.\nIn Table 4, we ab-\nlate the effectiveness of using different query formulations\nfor different tasks. The results indicate our double query\nformulation outperforms using only one type of query.\nEffectiveness of Visual Prompt Formulation.\nIn Ta-\nble 5, we attempt to use a pre-trained CLIP vision encoder\nto encode the features of the visual prompt by cropping\nthe prompted region into images for CLIP to process. As\nCLIP features contain rich semantics with few appearance\nfeatures, which could not apply to referring segmentation\ntasks. Therefore, we ablate on generic segmentation tasks\nand find that the final model could not generalize well on\nopen-set datasets like ADE. This result verifies our hypoth-\nesis that CLIP vision features could not generalize well on\nin-context visual prompting.\nEffectiveness of Unifying Tasks and Data. We unify vi-\nsual generic segmentation and visual referring segmentation\nto use both semantically labeled data (COCO) and data with\nonly segmentation annotations (SA-1B). In Table 6, the re-\nsults indicate that employing both datasets improves each\nindividual task.\nTraining batch size for generic segmentation. In Table 7,\nthe results show that increasing training batch size consis-\ntently improves the generic segmentation performance. The\n7\nTable 3. Zero-shot video object segmentation. Without training with video or pairwise image data, our approach is able to do video object\nsegmentation in a zero-shot manner. (#Concurrent work.)\nMethod\nSegmentation Data\nType\nRefer-Type\nZero-\nShot\nDAVIS17\nDAVIS16-Interactive\nYouTube-VOS 2018\nJF\nJ\nF\nJF\nJ\nF\nG\nJs\nFs\nJu\nFu\nWith Video Data\nAGSS [20]\nVOS+DAVIS\nVideo\nMask\n67.4\n64.9\n69.9\n\u2212\n\u2212\n\u2212\n71.3\n71.3\n65.5\n75.2\n73.1\nAGAME [11]\n(Synth)VOS+DAVIS\nMask\n70.0\n67.2\n72.7\n\u2212\n\u2212\n\u2212\n66.0\n66.9\n*\n61.2\n*\nSWEM [22]\nImage+VOS+DAVIS\nMask\n84.3\n81.2\n87.4\n\u2212\n\u2212\n\u2212\n82.8\n82.4\n86.9\n77.1\n85.0\nXMem [3]\nImage+VOS+DAVIS\nMask\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n86.1\n85.1\n89.8\n80.3\n89.2\nSiamMask [32]\nCOCO+VOS\nBox\n*\n54.3\n58.5\n69.8\n71.7\n67.8\n*\n60.2\n58.2\n45.1\n47.7\nMiVOS [4]\nBL30K+VOS+DAVIS\nMask\n84.5\n81.7\n87.4\n91.0\n89.6\n92.4\n82.6\n81.1\n85.6\n77.7\n86.2\nReferFormer-B [35]\nRefCOCO(+/g)+VOS+DAVIS\nText\n61.1\n58.1\n64.1\n\u2212\n\u2212\n\u2212\n*\n*\n*\n*\n*\nUNINEXT-T [41]\nImage+Video\nGeneralist\nMask\n74.5\n71.3\n77.6\n\u2212\n\u2212\n\u2212\n77.0\n76.8\n81.0\n70.8\n79.4\nUNINEXT-L [41]\nImage+Video\nMask\n77.2\n73.2\n81.2\n\u2212\n\u2212\n\u2212\n78.1\n79.1\n83.5\n71.0\n78.9\nUNINEXT-L [41]\nImage+Video\nText\n66.7\n62.3\n71.1\n\u2212\n\u2212\n\u2212\n*\n*\n*\n*\n*\nWithout Video Data\nPainter-L [33]\nCOCO+ADE+NYUv2\nGeneralist\nMask\n\u2713\n34.6\n28.5\n40.8\n\u2212\n\u2212\n\u2212\n24.1\n27.6\n35.8\n14.3\n18.7\nSegGPT-L [34]\nCOCO+ADE+VOC+...\nMask\n\u2713\n75.6\n72.5\n78.6\n\u2212\n\u2212\n\u2212\n74.7\n75.1\n80.2\n67.4\n75.9\nPerSAM-L [47]\nSAM+DAVIS\nMask\n\u2717\n60.3\n56.6\n63.9\n\u2212\n\u2212\n\u2212\n*\n*\n*\n*\n*\nSEEM-T [52]\n\u2713\n60.4\n57.6\n63.3\n62.7\n58.9\n66.4\n51.4\n55.6\n44.1\n59.2\n46.9\nSEEM-L [52]\nCOCO+LVIS\nMask\n\u2713\n58.9\n55.0\n62.8\n62.2\n58.3\n66.0\n50.0\n57.2\n38.2\n61.3\n43.3\nDINOv-T (Ours)\nCOCO+SAM\nMask\n\u2713\n73.3\n71.0\n75.7\n77.0\n72.9\n81.2\n60.9\n65.3\n70.0\n52.3\n57.9\nDINOv-L (Ours)\n\u2713\n72.3\n69.8\n74.8\n75.4\n71.3\n79.4\n59.6\n61.7\n65.7\n52.3\n58.8\nTable 4.\nAblation of using difference queries to do both in-\ncontext reference and generic segmentation. By default, we use\nboth generic query and interactive query. We remove one type of\nquery at a time to ablate their effectiveness.\nMethod\nCOCO\nDAVIS17\nPQ\nmask AP\nbox AP\nmIoU\nJF\nJ\nF\nDINOv-SwinT\n49.6\n42.7\n47.0\n58.0\n73.3\n71.0\n75.7\nonly point query\n45.2\n31.0(11.7)\n34.7(-12.3)\n52.7\n71.4\n68.8\n74.0\nonly generic query\n46.2\n38.3(-4.4)\n41.5(-6.0)\n53.3\n68.9\n66.5\n71.3\nTable 5. Ablation of using different ways to encode the visual\nprompt on our Swin-T model. Under the same setting, we change\nour prompt encoding method and use a pre-trained CLIP to crop\nand encode the prompted objects in the image.\nPrompt\nEncoding\nCOCO (in-domain)\nADE (out-domain)\nPQ\nmask AP\nbox AP\nmIoU\nPQ\nmask AP\nbox AP\nmIoU\nOurs\n49.6\n42.7\n47.0\n58.0\n19.4\n11.4\n12.8\n21.9\nCLIP\n48.5\n40.7\n43.5\n54.9\n12.6\n1.4\n1.3\n13.3\nTable 6. Ablation of the effectiveness of unifying tasks and data.\nMethod\nData\nCOCO\nDAVIS17\nPQ\nmask AP\nbox AP\nmIoU\nJF\nJ\nF\nDINOv-SwinT\nCOCO, SAM\n49.6\n42.7\n47.0\n58.0\n73.3\n71.0\n75.7\nDINOv-SwinT\nCOCO\n48.9\n41.7\n45.9\n57.1\n63.3\n60.8\n65.7\nDINOv-SwinT\nSAM\nN/A\n\u2212\n\u2212\n\u2212\n68.4\n66.0\n70.8\nreason for this phenomenon is that a larger batch size helps\nto sample more positive and negative visual in-context ex-\namples across different images, which better matches the\ninference setting with random visual examples.\nInference In-Context Examples. In Fig. 7, we ablate the\nimpact of using different in-context lengths. Increasing the\nin-context example exhibits diminishing returns, especially\nwhen the number of examples is more than eight.\nTable 7. Ablation of image batchsize sampling in training.\nMethod\n#Batchsize for\nPrompt Sampling\nCOCO\nPQ\nmask AP\nbox AP\nmIoU\nDINOv-SwinT\n1\n28.9\n23.2\n25.3\n33.7\nDINOv-SwinT\n4\n45.1\n37.0\n40.4\n50.6\nDINOv-SwinT\n8\n47.3\n39.2\n43.1\n53.1\nDINOv-SwinT\n32\n47.8\n40.3\n44.1\n56.2\nDINOv-SwinT\n64\n49.0\n45.2\n41.5\n57.0\n4. Related Works\n4.1. Visual Perception Through Text Prompt\nInnovations in open-vocabulary object detection [7, 12,\n19,24,26,44,45,48] and open-vocabulary segmentation [6,\n8, 14, 31, 37, 45], have shown great potential in generic\nvisual perception, by leveraging large pre-trained vision-\nlanguage models like CLIP [30] and ALIGN [10]. These\napproaches demonstrate significant strides in zero-shot and\nfew-shot performance, adapting to a variety of visual con-\ntexts through text prompts. However, the reliance on text\nalone introduces limitations due to linguistic ambiguity and\nthe potential mismatch between textual descriptions and\ncomplex visual scenes [40]. This highlights the ongoing\nneed to refine the integration of visual inputs for more ac-\ncurate and comprehensive image perception.\n4.2. Visual Perception Through Image Example\nBuilding upon the foundations set by text-based visual\nperception methodologies, the field has seen a notable shift\ntowards incorporating image examples to enhance accu-\nracy and context sensitivity.\nOV-DETR [44] extends its\nopen-vocabulary object detection capability beyond text,\n8\nDINOv\nFigure 6. DINOv can do open-set segmentation by giving visual prompts.\n1 2\n4\n8\n16\n32\nInference In-context Examples\n10\n20\n30\n40\n50\n60\nMetric\nCOCO PQ\nADE20K PQ\nODinW AP-Average\nSegInW AP-Average\nFigure 7. DINOv query formulation of different tasks.\nby utilizing both the image encoder and text encoder from\nCLIP [30], allowing for object detection guided by visual\nexamples. Similarly, OWL-ViT [26] leverages large-scale\nimage text examples in its contrastive pre-training phase,\nand propose to adopt image example for one-shot image-\nconditioned object detection.\nMQ-Det [40] utilizes im-\nage examples to enhance text descriptions for better open-\nvocabulary object detection performance. These methods\ntypically adopt the image encoder in CLIP to extract visual\nfeatures from given image examples for a more accurate\nperception of objects and scenes, and demonstrate that vi-\nsual examples can bridge the gap between textual ambiguity\nand the complex nature of visual perception.\n4.3. Visual Perception Through Visual Prompt\nDifferent from image example-based methods that take\nan image as input, which are then processed by multi-modal\nencoder like CLIP [30], visual prompt-based methods typi-\ncally use visual instructions (e.g. box, point, mask, scribble,\nand refereed regions of another image) to guide a model for\na specific visual task. SAM [13], for instance, introduces a\npromptable model for interactive image segmentation, fos-\ntering research in computer vision foundation models. It is\nfollowed by some works that adapt SAM for visual prompt-\ning through personalized examples [47]. SEEM [52] stands\nout as an interactive and versatile model for segmenting\nobjects, accommodating various types of prompts, and is\nsemantic-aware compared to SAM. Semantic-SAM [17] ex-\ncels in semantic awareness and recognizing granularity, and\nis capable of various segmentation tasks including panoptic\nand part segmentation. Painter [33] and SegGPT [34] take a\ngeneralist approach, coping with various segmentation tasks\nby formulating segmentation as an in-context coloring prob-\nlem. Our work resembles them with the same goal while\npresenting a new visual prompting mechanism to support\nall types of segmentation tasks.\n5. Conclusion\nWe present DINOv, a unified framework for in-context\nvisual prompting to accommodate both referring segmenta-\ntion and generic segmentation tasks. To effectively formu-\nlate in-context visual prompts, we designed a simple prompt\nencoder to encoder reference visual prompts from the refer-\nence image and adopted a shared decoder to decode the final\ntarget visual prompts from the target image. We also formu-\nlate generic latent queries and point queries to align differ-\nent tasks and data. The experimental results indicate that\nDINOv demonstrates impressive referring and generic seg-\nmentation capabilities to refer and detect with in-context vi-\nsual prompting. Notably, DINOv delivers competitive per-\n9\nformance compared to close-set segmentation on in-domain\ndatasets and show promising results on many open-set seg-\nmentation benchmarks. We hope our early exploration of\nvisual in-context prompting could inspire the community.\nLimitations. We employ limited semantically labeled data\n(COCO), which can be scaled up for better performance and\nextended to text prompts for multi-modal understanding.\nReferences\n[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners, 2020. 2\n[2] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-\nder Kirillov, and Rohit Girdhar.\nMasked-attention mask\ntransformer for universal image segmentation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 1290\u20131299, 2022. 7\n[3] Ho Kei Cheng and Alexander G. Schwing. XMem: Long-\nterm video object segmentation with an atkinson-shiffrin\nmemory model. In ECCV, 2022. 8\n[4] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Modular\ninteractive video object segmentation: Interaction-to-mask,\npropagation and difference-aware fusion. In CVPR, 2021. 8\n[5] Zheng Ding, Jieke Wang, and Zhuowen Tu.\nOpen-\nvocabulary panoptic segmentation with maskclip.\narXiv\npreprint arXiv:2208.08984, 2022. 7\n[6] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scal-\ning open-vocabulary image segmentation with image-level\nlabels. In European Conference on Computer Vision, pages\n540\u2013557. Springer, 2022. 8\n[7] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.\nOpen-vocabulary object detection via vision and language\nknowledge distillation.\narXiv preprint arXiv:2104.13921,\n2021. 8\n[8] Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan\nElhamifar. Open-vocabulary instance segmentation via ro-\nbust cross-modal pseudo-labeling.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7020\u20137031, 2022. 8\n[9] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita\nOrlov, and Humphrey Shi.\nOneformer: One transformer\nto rule universal image segmentation.\narXiv preprint\narXiv:2211.06220, 2022. 7\n[10] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. In International\nconference on machine learning, pages 4904\u20134916. PMLR,\n2021. 8\n[11] Joakim Johnander, Martin Danelljan, Emil Brissman, Fa-\nhad Shahbaz Khan, and Michael Felsberg. A generative ap-\npearance model for end-to-end video object segmentation,\n2018. 8\n[12] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion.\nMdetr-\nmodulated detection for end-to-end multi-modal understand-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 1780\u20131790, 2021. 7, 8\n[13] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar, and\nRoss Girshick. Segment anything, 2023. 2, 6, 9\n[14] Shiyi Lan, Zhiding Yu, Christopher Choy, Subhashree Rad-\nhakrishnan, Guilin Liu, Yuke Zhu, Larry S Davis, and An-\nima Anandkumar. Discobox: Weakly supervised instance\nsegmentation and semantic correspondence from box super-\nvision. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 3406\u20133416, 2021. 8\n[15] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,\nLinjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foun-\ndation models: From specialists to general-purpose assis-\ntants. arXiv preprint arXiv:2309.10020, 1, 2023. 2\n[16] Feng Li, Hao Zhang, Shilong Liu, Lei Zhang, Lionel M Ni,\nHeung-Yeung Shum, et al. Mask dino: Towards a unified\ntransformer-based framework for object detection and seg-\nmentation. arXiv preprint arXiv:2206.02777, 2022. 6, 7, 12\n[17] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu,\nJianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao.\nSemantic-sam: Segment and recognize anything at any gran-\nularity. arXiv preprint arXiv:2307.04767, 2023. 2, 6, 9, 12\n[18] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang,\nLionel M Ni, and Heung-Yeung Shum. Mask dino: Towards\na unified transformer-based framework for object detection\nand segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3041\u20133050, 2023. 2, 7\n[19] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10965\u201310975, 2022. 2, 6, 7, 8\n[20] Huaijia Lin, Xiaojuan Qi, and Jiaya Jia. Agss-vos: Atten-\ntion guided single-shot video object segmentation. In ICCV,\n2019. 8\n[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 2, 6\n[22] Zhihui Lin, Tianyu Yang, Maomao Li, Ziyu Wang, Chun\nYuan, Wenhao Jiang, and Wei Liu. Swem: Towards real-\ntime video object segmentation with sequential weighted\nexpectation-maximization. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1362\u20131372, 2022. 8\n10\n[23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In NeurIPS, 2023. 2\n[24] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection.\narXiv preprint\narXiv:2303.05499, 2023. 2, 8\n[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 10012\u201310022, 2021. 12\n[26] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim\nNeumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh\nMahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran\nShen, et al. Simple open-vocabulary object detection. In\nEuropean Conference on Computer Vision, pages 728\u2013755.\nSpringer, 2022. 5, 8, 9\n[27] OpenAI. Gpt-4 technical report, 2023. 2\n[28] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017\ndavis challenge on video object segmentation. arXiv preprint\narXiv:1704.00675, 2017. 6\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 5\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. 8, 9\n[31] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong\nTang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu.\nDenseclip: Language-guided dense prediction with context-\naware prompting.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18082\u201318091, 2022. 8\n[32] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and\nPhilip HS Torr. Fast online object tracking and segmentation:\nA unifying approach. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 2019. 8\n[33] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and\nTiejun Huang. Images speak in images: A generalist painter\nfor in-context visual learning, 2023. 6, 7, 8, 9\n[34] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,\nChunhua Shen, and Tiejun Huang. Seggpt: Segmenting ev-\nerything in context. arXiv preprint arXiv:2304.03284, 2023.\n2, 6, 7, 8, 9\n[35] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping\nLuo. Language as queries for referring video object segmen-\ntation. arXiv preprint arXiv:2201.00487, 2022. 8\n[36] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo. Segformer: Simple and ef-\nficient design for semantic segmentation with transformers.\narXiv preprint arXiv:2105.15203, 2021. 7\n[37] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2955\u20132966, 2023. 7,\n8\n[38] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models, 2023.\n2\n[39] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen\nLiang, Jianchao Yang, and Thomas Huang.\nYoutube-vos:\nA large-scale video object segmentation benchmark. arXiv\npreprint arXiv:1809.03327, 2018. 6\n[40] Yifan Xu, Mengdan Zhang, Chaoyou Fu, Peixian Chen,\nXiaoshan Yang, Ke Li, and Changsheng Xu.\nMulti-\nmodal queried object detection in the wild. arXiv preprint\narXiv:2305.18980, 2023. 8, 9\n[41] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo,\nZehuan Yuan, and Huchuan Lu.\nUniversal instance per-\nception as object discovery and retrieval.\narXiv preprint\narXiv:2303.06674, 2023. 8\n[42] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-\nChieh Chen. Convolutions die hard: Open-vocabulary seg-\nmentation with single frozen convolutional clip.\narXiv\npreprint arXiv:2308.02487, 2023. 2, 6, 7\n[43] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,\nYukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen. k-means mask transformer. In European Conference\non Computer Vision, pages 288\u2013307. Springer, 2022. 7\n[44] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and\nChen Change Loy. Open-vocabulary detr with conditional\nmatching.\nIn European Conference on Computer Vision,\npages 106\u2013122. Springer, 2022. 8\n[45] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan\nLi, Jianwei Yang, and Lei Zhang. A simple framework for\nopen-vocabulary segmentation and detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 1020\u20131031, 2023. 2, 7, 8\n[46] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun\nChen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu\nYuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Uni-\nfying localization and vision-language understanding. arXiv\npreprint arXiv:2206.05836, 2022. 7\n[47] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junt-\ning Pan, Hao Dong, Peng Gao, and Hongsheng Li. Person-\nalize segment anything model with one shot, 2023. 8, 9\n[48] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-\nyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou,\nXiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-\nbased language-image pretraining.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16793\u201316803, 2022. 5, 8\n[49] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\n11\nade20k dataset. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 633\u2013641,\n2017. 6\n[50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models, 2023.\n2\n[51] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,\nChunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang,\nLu Yuan, et al. Generalized decoding for pixel, image, and\nlanguage. arXiv preprint arXiv:2212.11270, 2022. 2, 6, 7\n[52] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,\nJianfeng Gao, and Yong Jae Lee. Segment everything every-\nwhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n2, 8, 9\nA. Implementation Details\nOur model framework is mainly based on Mask\nDINO [17], which is a unified framework for detection\nand segmentation.\nDINOv is a general encoder-decoder\narchitecture composed of a vision encoder. We use Swin-\nT/L [25] as the vision encoder. As our decoder supports\nboth generic query and point query, we adopt 300 latent\ngeneric queries following Mask DINO [16] and six level\nqueries for each input point following Semantic-SAM [17].\nEspecially, when using point query, we sample 10 fore-\nground and 40 background points during training and\nemploy grid sample for 20\u00d720 points during inference. For\ninference on general segmentation and detection tasks, we\nuse 16 in-context examples for each category by default.\nFor VOS inference, we average eight previous frame\npredictions as the reference to segment the current frame.\nB. Video Object Segmentation Inference\nVideo object segmentation (VOS) aims to segment an in-\nterested object in a video by giving text or visual clues. Our\nmodel focuses on the semi-supervised setting, which seg-\nments a particular object throughout a video by giving vi-\nsual clues in the first frame. When doing VOS, an intuitive\nway is to first extract reference visual prompt features from\nthe first frame image and the corresponding visual prompts\nwith our prompt encoder. When processing each frame in\na video, we are able to utilize reference visual prompt fea-\ntures in the first frame as in the current frame.\nIn DINOv, as we train with visual in-context prompt-\ning with multiple examples for generic segmentation, we\ncan also apply this strategy to VOS for better perfor-\nmance. More concretely, we also compute and store the\nreference visual features of the predicted mask in previous\nframes. These features, denoted as memory reference visual\nprompts, will be averaged together with the first frame\u2019s\ngiven prompt to construct the visual prompt of the current\nTable 8. Ablation of Inference Memory Length on DAVIS2017\nwith a SwinL backbone.\nMethod\nMemory Length\nDAVIS2017\nJ& F\nJ\nF\nDINOv-SwinT\n1\n62.1\n58.7\n65.4\nDINOv-SwinT\n2\n69.6\n66.7\n72.6\nDINOv-SwinT\n4\n71.5\n68.7\n74.3\nDINOv-SwinT\n8\n72.3\n69.8\n74.8\nDINOv-SwinT\n16\n68.0\n65.4\n70.7\nframe. We employ a priority queue to manage the memory.\nFor simplicity, the priority score of each prompt is posi-\ntively correlated to the frame number, which indicates that\nwe only store the memory prompts that are near the current\nframe in time sequence. By default, the memory length is\nset to 8. In Tab. 8, we show the influence of using different\nnumber of memory length.\n12\n"
  },
  {
    "title": "Diffusion360: Seamless 360 Degree Panoramic Image Generation based on Diffusion Models",
    "link": "https://arxiv.org/pdf/2311.13141.pdf",
    "upvote": "9",
    "text": "Diffusion360: Seamless 360 Degree Panoramic Image Generation based on\nDiffusion Models\nMengyang Feng, Jinlin Liu, Miaomiao Cui, Xuansong Xie\nAlibaba Group\n{mengyang.fmy, ljl191782, miaomiao.cmm, xingtong.xxs}@alibaba-inc.org\nblended\nLatents before circular blending\nrightmost\nLeftmost\nh\nw=2h\nAdaptive Weights \ud835\udc98\n0\n1\n\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc51 = \ud835\udc98 \u2217 \ud835\udc59\ud835\udc52\ud835\udc53\ud835\udc61 + 1 \u2212 \ud835\udc98 \u2217 \ud835\udc5f\ud835\udc56\ud835\udc54\u210e\ud835\udc61\nLatents after circular blending\nw=2h\nh\n\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc51\nFigure 1. The proposed circular blending operation.\nAbstract\nThis is a technical report on the 360-degree panoramic\nimage generation task based on diffusion models. Unlike\nordinary 2D images, 360-degree panoramic images capture\nthe entire 360\u25e6 \u00d7 180\u25e6 field of view. So the rightmost and\nthe leftmost sides of the 360 panoramic image should be\ncontinued, which is the main challenge in this field. How-\never, the current diffusion pipeline is not appropriate for\ngenerating such a seamless 360-degree panoramic image.\nTo this end, we propose a circular blending strategy on\nboth the denoising and VAE decoding stages to maintain\nthe geometry continuity.\nBased on this, we present two\nmodels for Text-to-360-panoramas and Single-Image-to-\n360-panoramas tasks. The code has been released as an\nopen-source project at https://github.com/ArcherFMY/SD-\nT2I-360PanoImage and ModelScope.\n1. Related Work\nRecent studies like MVDiffusion [3], StitchDiffusion [4],\nand PanoDiff [5] have proved the feasibility of diffusion-\nbased 360-degree panoramic images generation, but still\nhave some drawbacks.\nMVDiffusion needs 8 perspective views (user-provided\nor generated from Stable Diffusion [1]) as inputs. The re-\nsulting closed-loop panoramic image is more like a long-\nrange image with a wide angle. So it has artifacts on the\n\u2019sky\u2019 and \u2019floor\u2019 when viewing in a 360 image viewer.\nStitchDiffusion proposes a global cropping on the left\nand right side of the image to maintain the continuity. How-\never, it still cracks on the junctions when zoom-in in the 360\nimage viewer.\nPanoDiff, similar to the StitchDiffusion, proposes a cir-\ncular padding scheme, which is the most related research to\nour work. The idea of our circular blending strategy is de-\nrived from the circular padding scheme. The differences\nare (1) we use an adaptive weighting policy for geomet-\nric continuity, (2) we do not need the Rotating Schedule\nat both training and inference time, which means that we\ncan directly finetune a dreambooth [2] model using stan-\ndard diffusion pipeline for this task, and just apply the cir-\ncular blending at inference time, and (3) we can directly\napply our technique into the ControlNet-Tile [8] model to\nproduce high-resolution results.\narXiv:2311.13141v1  [cs.CV]  22 Nov 2023\nblended\nCircular blending in denoising stage\nrightmost\nLeftmost\nblended\nrightmost\nLeftmost\nCircular blending in VAE decoding stage\nBlending in\nsome horizontal stripes\nBlending in \nwhole latents\nFigure 2. The circular blending operation in different stages.\nGAN\nBase Model\nFinetuned on SDv2.1\nLow Resolution Result\n512*1024\nText \nDescription\nControlNet-Tile\nFinetuned on SDv1.5\nControlNet-Tile\nFinetuned on SDv1.5\nHigh Resolution Result\n3072*6144\nInitSR\nRealESRGAN\nRefinement\nFigure 3. The pipeline of Text-to-360-Panoramas.\n2. Method\n2.1. Circular Blending\nWe propose a circular blending strategy at the inference\ntime to generate seamless 360-degree panoramic images.\nSpecifically, at each denoising step, the right part (of a such\nportion) of the latent feature and the left part (of the same\nportion as the right part) is blended with adaptive weights.\nThis is illustrated in Fig. 1. Similarly, this strategy can be\nadded to the tiled decode function of the VAE decoder (see\nFig. 2). We find that using the circular blending in the VAE\ndecoder is more important than in the latent denoising stage\nfor maintaining the geometric continuity.\n2.2. Text-to-360-Panoramas\nFor the Text-to-360-Panoramas task, we propose a multi-\nstage framework to generate high resolution 360-degree\npanoramic images. As illustrated in Fig. 3, we first gener-\nate a low resolution image using a base model (finetuned on\nthe SUN360 [7] dataset using the DreamBooth [2] training\nmethod), and then employ some super-resolution strategies\n(including diffusion-based and the GAN-based methods,\nlike the ControlNet-Tile model and the RealESRGAN [6])\nto up-scale the result to a high resolution one.\nFor bet-\nter results, we also finetune the ControlNet-Tile model on\nthe SUN360 dataset by generate low-resolution and high-\nresolution image pairs.\n2.3. Single-Image-to-360-Panoramas\nFor the Single-Image-to-360-Panoramas task, the frame-\nwork is similar to the Text-to-360-Panoramas by replac-\ning the base model to a controlnet-outpainting model. We\ndesign a ControlNet-Outpainting model to generate a low\nresolution 360-degree panoramic image from a given sin-\ngle ordinary 2D image at perspective view. To generate\nthe training pairs of perspective and panoramic images, we\nfirst convert the panoramic image to cube-maps and select\nthe center-cube as its perspective image.\nThe inputs of\nthe ControlNet-Outpainting model consist of the converted\ncenter-cube map C with the other cubes filled by zeros and\nthe mask M. At inference time, the perspective image can\nbe generated from a certain generative model or captured by\na camera (the image should be squared). The perspective\nimage is converted to the center-cube map C as the input\nof the ControlNet-Outpaining model. For some reason, the\ntrained models of this task can not be released. However, it\nshould be easy to reproduce. See some results in Fig. 8.\n3. Resuls\nWe show some testing results at different stages of the Text-\nto-360-Panoramas task in Fig. 4, Fig. 5, Fig. 6, and Fig. 7.\nThe input prompts it fetch at the MVDiffusion project page\n(https://mvdiffusion.github.io/)\n4. Limitations\nThe base model is trained using the DreamBooth [2] tech-\nnique, so it can not be changed with the models from CIVI-\nTAI (https://civitai.com/) for stylizing purposes.\nAdding some style descriptions (such as \u2019cartoon style\u2019 and\n\u2019oil painting style\u2019) in the prompt does not work. One can\ngenerate an initial 360 image using our method, and then\nuse ControlNets (like canny and depth) with different base\nmodels to change the style.\nThis kitchen is a charming blend of rustic and modern,\nfeaturing\na\nlarge\nreclaimed\nwood\nisland\nwith\nmarble\ncountertop, a sink surrounded by cabinets. To the left of the\nisland, a stainless-steel refrigerator stands tall. To the right of\nthe sink, built-in wooden cabinets painted in a muted.\nMajestically rising towards the heavens, the snow-capped\nmountain stood, its jagged peaks cloaked in a shroud of\nethereal clouds, its rugged slopes a stark contrast against the\nserene azure sky, and its silent grandeur exuding an air of\nancient wisdom and timeless solitude, commanding awe and\nreverence from all who beheld it.\nThis kitchen is a charming blend of rustic and modern,\nfeaturing\na\nlarge\nreclaimed\nwood\nisland\nwith\nmarble\ncountertop, a sink surrounded by cabinets. To the left of the\nisland, a stainless-steel refrigerator stands tall. To the right of\nthe sink, built-in wooden cabinets painted in a muted.\nBathed in the soft, dappled light of the setting sun, the silent \nstreet lay undisturbed, revealing the grandeur of its \ncobblestone texture, the rusted lampposts bearing witness to \nforgotten stories, and the ancient, ivy-clad houses standing \nstoically, their shuttered windows and weather-beaten doors \nspeaking volumes about their passage through time.\nResults from base model\nFigure 4. Results from the Base Model.\nThis kitchen is a charming blend of rustic and modern,\nfeaturing\na\nlarge\nreclaimed\nwood\nisland\nwith\nmarble\ncountertop, a sink surrounded by cabinets. To the left of the\nisland, a stainless-steel refrigerator stands tall. To the right of\nthe sink, built-in wooden cabinets painted in a muted.\nMajestically rising towards the heavens, the snow-capped\nmountain stood, its jagged peaks cloaked in a shroud of\nethereal clouds, its rugged slopes a stark contrast against the\nserene azure sky, and its silent grandeur exuding an air of\nancient wisdom and timeless solitude, commanding awe and\nreverence from all who beheld it.\nThis kitchen is a charming blend of rustic and modern,\nfeaturing\na\nlarge\nreclaimed\nwood\nisland\nwith\nmarble\ncountertop, a sink surrounded by cabinets. To the left of the\nisland, a stainless-steel refrigerator stands tall. To the right of\nthe sink, built-in wooden cabinets painted in a muted.\nBathed in the soft, dappled light of the setting sun, the silent \nstreet lay undisturbed, revealing the grandeur of its \ncobblestone texture, the rusted lampposts bearing witness to \nforgotten stories, and the ancient, ivy-clad houses standing \nstoically, their shuttered windows and weather-beaten doors \nspeaking volumes about their passage through time.\nResults from base+initsr\nFigure 5. Results from Base+InitSR.\nThis kitchen is a charming blend of rustic and modern,\nfeaturing\na\nlarge\nreclaimed\nwood\nisland\nwith\nmarble\ncountertop, a sink surrounded by cabinets. To the left of the\nisland, a stainless-steel refrigerator stands tall. To the right of\nthe sink, built-in wooden cabinets painted in a muted.\nMajestically rising towards the heavens, the snow-capped\nmountain stood, its jagged peaks cloaked in a shroud of\nethereal clouds, its rugged slopes a stark contrast against the\nserene azure sky, and its silent grandeur exuding an air of\nancient wisdom and timeless solitude, commanding awe and\nreverence from all who beheld it.\nThis kitchen is a charming blend of rustic and modern,\nfeaturing\na\nlarge\nreclaimed\nwood\nisland\nwith\nmarble\ncountertop, a sink surrounded by cabinets. To the left of the\nisland, a stainless-steel refrigerator stands tall. To the right of\nthe sink, built-in wooden cabinets painted in a muted.\nBathed in the soft, dappled light of the setting sun, the silent \nstreet lay undisturbed, revealing the grandeur of its \ncobblestone texture, the rusted lampposts bearing witness to \nforgotten stories, and the ancient, ivy-clad houses standing \nstoically, their shuttered windows and weather-beaten doors \nspeaking volumes about their passage through time.\nResults from base+initsr+realesrgan\nFigure 6. Results from Base+InitSR+ReslESRGAN. It can be observed that, the geometric continuity of the rightmost and the leftmost\nsides of our results are smooth and nearly no cracks. Some artifacts in the top two rows are cost by the RealESRGAN.\nThis kitchen is a charming blend of rustic and modern,\nfeaturing\na\nlarge\nreclaimed\nwood\nisland\nwith\nmarble\ncountertop, a sink surrounded by cabinets. To the left of the\nisland, a stainless-steel refrigerator stands tall. To the right of\nthe sink, built-in wooden cabinets painted in a muted.\nMajestically rising towards the heavens, the snow-capped\nmountain stood, its jagged peaks cloaked in a shroud of\nethereal clouds, its rugged slopes a stark contrast against the\nserene azure sky, and its silent grandeur exuding an air of\nancient wisdom and timeless solitude, commanding awe and\nreverence from all who beheld it.\nThis kitchen is a charming blend of rustic and modern,\nfeaturing\na\nlarge\nreclaimed\nwood\nisland\nwith\nmarble\ncountertop, a sink surrounded by cabinets. To the left of the\nisland, a stainless-steel refrigerator stands tall. To the right of\nthe sink, built-in wooden cabinets painted in a muted.\nBathed in the soft, dappled light of the setting sun, the silent \nstreet lay undisturbed, revealing the grandeur of its \ncobblestone texture, the rusted lampposts bearing witness to \nforgotten stories, and the ancient, ivy-clad houses standing \nstoically, their shuttered windows and weather-beaten doors \nspeaking volumes about their passage through time.\nResults from our full implementation\nFigure 7. Results from the full implementation.\nA living \nroom \n(generated)\nThe \nmountain \nroad \n(generated)\nThe outer \nspace \n(generated)\nThe Times \nSquare \n(generated)\nA office \nroom \n(captured \nby a camera)\nFigure 8. Results of Single-Image-to-360-Panoramas.\nReferences\n[1] Robin Rombach,\nAndreas Blattmann,\nDominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 1\n[2] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. 2022. 1, 2\n[3] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and\nYasutaka Furukawa. Mvdiffusion: Enabling holistic multi-\nview image generation with correspondence-aware diffusion.\narXiv, 2023. 1\n[4] Hai Wang, Xiaoyu Xiang, Yuchen Fan, and Jing-Hao Xue.\nCustomizing 360-degree panoramas through text-to-image\ndiffusion models, 2023. 1\n[5] Jionghao Wang, Ziyu Chen, Jun Ling, Rong Xie, and Li Song.\n360-degree panorama generation from few unregistered nfov\nimages. In Proceedings of the 31th ACM International Con-\nference on Multimedia, 2023. 1\n[6] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.\nReal-esrgan: Training real-world blind super-resolution with\npure synthetic data. In International Conference on Computer\nVision Workshops (ICCVW). 2\n[7] Jianxiong Xiao, Krista A. Ehinger, Aude Oliva, and Antonio\nTorralba. Recognizing scene viewpoint using panoramic place\nrepresentation. In 2012 IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2695\u20132702, 2012. 2\n[8] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023. 1\n"
  }
]