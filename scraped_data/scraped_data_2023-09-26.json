[
  {
    "title": "Exploring Large Language Models' Cognitive Moral Development through Defining Issues Test",
    "link": "https://arxiv.org/pdf/2309.13356.pdf",
    "upvote": "36",
    "text": "Probing the Moral Development of Large Language\nModels through Defining Issues Test\nKumar Tanmay\u2217\nMicrosoft\nt-ktanmay@microsoft.com\nAditi Khandelwal\u2217\nMicrosoft\nt-aditikh@microsoft.com\nUtkarsh Agarwal\u2217\nMicrosoft\nt-utagarwal@microsoft.com\nMonojit Choudhury\nMicrosoft\nmonojitc@microsoft.com\nAbstract\nIn this study, we measure the moral reasoning ability of LLMs using the Defining\nIssues Test [1]- a psychometric instrument developed for measuring the moral\ndevelopment stage of a person according to the Kohlberg\u2019s Cognitive Moral De-\nvelopment Model [2]. DIT uses moral dilemmas followed by a set of ethical\nconsiderations that the respondent has to judge for importance in resolving the\ndilemma, and then rank-order them by importance. A moral development stage\nscore of the respondent is then computed based on the relevance rating and ranking.\nOur study shows that early LLMs such as GPT-3 exhibit a moral reasoning ability\nno better than that of a random baseline, while ChatGPT, Llama2-Chat, PaLM-\n2 and GPT-4 show significantly better performance on this task, comparable to\nadult humans. GPT-4, in fact, has the highest post-conventional moral reasoning\nscore, equivalent to that of typical graduate school students. However, we also\nobserve that the models do not perform consistently across all dilemmas, pointing\nto important gaps in their understanding and reasoning abilities.\n1\nIntroduction\nThe rapid paced developments and adoption of Large Language Models (LLMs) have led to fierce\ndebates on the ethical concerns and potential harms that these models pose [3, 4, 5, 6], which\ninclude but are not limited to copyright, data and user privacy violations [7], linguistic inequality\n[8], hallucination [9, 10, 11], and toxic content generation [12]. The mainstream and most popular\napproaches to mitigate the harms related to the LLM-generated content, such as toxic, offensive [13],\nstereotyping, and exclusionary statements [14], and hate speech [15], have mainly involved alignment\nof model output to certain pre-determined values through techniques such as RLHF [16, 17], fair\ndecoding [18], or post-processing/editing of the outputs [15, 19]. While these techniques are effective\nin achieving the underlying alignment goals [20], the goals themselves are often difficult, if not\nimpossible, to define. This is because the ethical or moral values that must be upheld by a model\nor an AI system depend on the specific application, the user, the usage context, the cultural and\ngeographical context, language and many other factors. In other words, it is impossible to design a\nuniversally-aligned LLM.\nThe problem of alignment becomes further complicated due to value pluralism \u2013 a condition where\ndifferent moral values are in conflict with each other and any choice made by the model will have\nto jeopardize one value in favor of another [21, 22]. Philosophers capture this idea through \u201cmoral\n\u2217These authors contributed equally to this work.\nPreprint. Under review.\narXiv:2309.13356v2  [cs.CL]  7 Oct 2023\ndilemmas\" \u2013 situations that require one to choose one value over another to arrive at a resolution [23].\nIn fact, it would not be an overstatement to say that most real world situations involve some kind of\nvalue pluralism that requires one to chose between conflicting values. Thus, as LLMs become more\nubiquitous and power various everyday applications, they have to face and resolve moral dilemmas\narising from value pluralism [21]. Many have argued, therefore, that LLMs should ideally be trained\nas generic ethical reasoners rather than aligned for certain specific values [24].\nTo what extent LLMs can carry out deep ethical reasoning, and how can we systematically probe\nthis? In this paper, we borrow ideas from the field of moral psychology to test the ethical or moral\nunderstanding and reasoning abilities of several popular LLMs. More specifically, we use the Defining\nIssues Test (DIT) [25] which is based on Kohlberg\u2019s Cognitive Moral Development Model [26], to\nassess the moral development stage of the LLMs. In this test, a moral dilemma is presented along\nwith 12 different statements on ethical considerations; the respondent (in our case, the LLM) is asked\nto rank these statements in the order of importance for resolving the dilemma. The outcome of the\ntest is a set of scores that tells about the respondent\u2019s moral development stage.\nWe study seven prominent models: GPT-3 [27], GPT-3.5, GPT-4 [28], ChatGPTv1, ChatGPTv2,\nPaLM-2 [29] and Llama2-Chat (70B version) [30], with 5 moral dilemmas from DIT and 4 newly\ndesigned dilemmas that extend the cultural context and diversity of the probes and precludes the\npossibility of training data contamination. We observe that GPT-4 achieves the highest moral\ndevelopment score in the range of that of a graduate school student, which according to Kohlberg\u2019s\nmodel of cognitive moral development indicates a post-conventional moral understanding. GPT-3,\non the other hand, performs no better than a random baseline. Performance of other models lie in\nbetween these two extremes, that roughly corresponds to the score range of adult humans and college\nstudents on DIT, and indicates a conventional moral understanding (as dictated by the moral norms\nand conventions of the society). Interestingly, for 2 of the 9 dilemmas, no model performs better\nthan the random baseline, and for one of the newly designed dilemmas, GPT-4 performs worse than\nmost other models. This shows that there is a lack of consistency in ethical reasoning across these\nmodels, implying the need for deeper investigation, understanding and improvement of LLMs\u2019 moral\nreasoning abilities. This work also leads to several interesting technical, practical and philosophical\nquestions, which are discussed in the last section.\n2\nBackground and Related Work\nIn this section, we provide an overview of Morality, Moral Psychology and models of Cognitive\nMoral Development, from which we draw inspirations and materials to design this study. We also\ndiscuss current treatment of ethics in NLP literature, with a particular focus on LLMs.\n2.1\nMorality and Moral Development\nMorality is the study of what is right and wrong, and has been a central concern in philosophy [31].\nOver the years, numerous theories have been proposed to explain how individuals develop their moral\nreasoning and judgments. Of these, the Cognitive Moral Development (CMD) model [2] proposed\nby Lawrence Kohlberg in 1969 remains one of the most influential accounts of moral development.\nBuilding upon Piaget\u2019s work [32], Kohlberg developed a comprehensive theory that consists of six\nstages divided into three main levels: pre-conventional, conventional, and post-conventional morality.\nAt Stage 1, individuals are concerned with avoiding punishment and make moral decisions based on\nfear of consequences and self-interest. At Stage 2, individuals focus on their own needs and interests\nbut recognize that others have similar needs. Moral judgments are influenced by reciprocity, such as\n\u201cYou scratch my back, I\u2019ll scratch yours\". Stages 1 and 2 are pre-conventional morality. At Stage 3,\nindividuals seek approval and conform to social (and religious) norms. Moral decisions are made to\nmaintain positive relationships and avoid disapproval. At Stage 4, individuals are concerned with\nlaw, rules, and authority figures and their moral reasoning revolves around maintaining social order\nand upholding the greater good. These two stages fall under the realm of conventional morality. At\nStage 5, individuals recognize different groups may have different moral perspectives and base their\ndecisions on principles of fairness, justice, and individual rights, even if these principles conflict with\nsocial norms or laws. This stage is further divided into sub-stages - 5A and 5B. Stage 5A suggests that\nmoral obligation derives from voluntary commitments of society\u2019s members to cooperate whereas\nStage 5B is more concerned with procedures which exists for selecting laws that maximize welfare\n2\nas discerned in the majority will. At Stage 6, individuals develop their moral principles based on\nuniversal ethical values. They act according to a personal ethical code that transcends societal rules\nand laws. These principles often align with the concepts of justice, equality, and human rights. Stages\n5A, 5B and 6 are, thus, called post-conventional morality.\nThe CMD model emphasizes the importance of moral reasoning and the development of an indi-\nvidual\u2019s moral principles. It posits that as individuals mature, their moral reasoning becomes more\nsophisticated and abstract, allowing them to make ethical decisions based on principles rather than\nmere rules. It may be noted that this theory has been criticized for bias towards individualistic and\nself-expressionistic cultures (mostly prevalent in the Global North), overlooking the diversity of\nmoral development across cultures [33, 34], for having gender bias [35], and for ignoring the role of\nintuitions and emotions in moral decision making [36]. Despite these criticisms, Kohlberg\u2019s theory\nhas played a vital role in advancing our understanding of moral development and remains influential\nin the field of moral psychology.\n2.2\nRest\u2019s Defining Issues Test\nIn line with Kohlberg\u2019s framework, James Rest introduced the Defining Issues Test (DIT) [1] as a way\nto measure an individual\u2019s moral development. In this test the respondents are presented with moral\ndilemmas, and their moral reasoning abilities are assessed by analyzing the justifications provided\nby them for their decisions. Rest\u2019s DIT draws upon Kohlberg\u2019s stages to categorize individuals into\nstages of moral development, offering insights into ethical decision-making processes. For over three\ndecades, the DIT has remained the most popular tool for assessing CMD.2. It includes either three\n(short-form DIT) or six (original DIT) moral dilemmas, each followed by 12 ethical considerations\ncorresponding to different stages of CMD. The respondent has to first provide a resolution to the\ndilemma (it has three options: two horns of the dilemma and \u201ccan\u2019t decide\") and then rate the\nsignificance (\u201cgreat\", \u201cmuch\", \u201csome\", \u201clittle\" and \u201cno importance\") of each item in resolving the\nmoral dilemma, and then select and rank the four most important items.\nThe ethical consideration statements can also belong to A or M categories instead of the stages of\nCMD [25]. The A items are intended to typify an \u201canti-establishment\" orientation, a point of view\nwhich condemns tradition and the existing social order. The M items are meant to be meaningless\nnonsense statements. The \u201cM\" statements were added as a reliability check as any valid respondent\nwould be expected to rate the statement quite low, while for the purposes of any study, the \u201cA\"\nstatements and it\u2019s score are simply disregarded.\nThe Post Conventional Morality Score (abbreviated as P-score), stands as the most widely utilized\nmetric, serving as an indicator of the \u201crelative significance an individual places on principled moral\nconsiderations, specifically those associated with Stages 5 and 6, when deliberating moral dilemmas\"\n[25]. If the most vital (top ranked) statement corresponds to either Stage 5 or 6, four points are\nadded to the P-score. Similarly, if the second, third and fourth ranked statements belong to these\npost-conventional stages, three, two and one points are added respectively to the P-score. Thus,\nhigher the P-score of a respondent, more the importance they pay to universal ethical values and\nhuman rights while making moral judgments.\nApart from P-score, DIT also measures Personal Interest Schema Score which reflects choices\ninfluenced by personal interests (Stages 2 and 3 in Kohlberg\u2019s model), and Maintaining Norms Schema\nScore that indicates choices driven by societal norms, including legal systems, established roles,\nand organizational structures. The percentage of \u201ccan\u2019t decide\" choices measures the respondent\u2019s\ndecisiveness, reflecting the ease of processing moral information.\nThe Moral Judgment Test (MJT) [38], developed by Georg Lind to assess one\u2019s moral judgment\ncompetencies, is also based on Kohlberg\u2019s CMD. However, it measures the degree to which one\ncan consistently employ the same moral value across moral dilemmas rather than the stage of moral\ndevelopment.\n2Between 1974 and 1988, an estimated 400 studies have used DIT. It has been used in over 40 countries,\nacross various professions and with about 150 new studies each year [37]\n3\n2.3\nRecent Theories in Moral Philosophy\nIn recent years, moral philosophy has seen the emergence of innovative theories developed by social\npsychologists, that expand our understanding of moral decision-making. Moral Foundations Theory\n[39], proposed by Jonathan Haidt and Jesse Graham, posits that human morality is shaped by a set\nof innate moral foundations or intuitions. These foundations include care/harm, fairness/cheating,\nloyalty/betrayal, authority/subversion, and sanctity/degradation. According to this theory, individuals\nvary in the extent to which they prioritize these moral foundations, leading to differences in moral\njudgments and values. Dual Process Theory [40], rooted in psychology and neuroscience, posits\nthat moral decision-making involves two cognitive processes: System 1 (intuitive) and System 2\n(reflective). System 1 operates quickly and automatically, relying on gut feelings and emotions,\nwhile System 2 involves deliberate reasoning and critical thinking. This theory suggests that moral\njudgments often result from the interplay between these two systems, and the balance can vary among\nindividuals and situations. Though beyond the scope of our current study, these theories can provide\nnovel frameworks for assessing the ethical reasoning abilities of LLMs.\n2.4\nCurrent Approaches to Ethics of LLMs\nAI alignment is a research field that aims to ensure that AI systems advance the intended goals,\npreferences, or ethical principles of humans [41]. Numerous scholarly works have contributed signifi-\ncantly to the development of ethical frameworks, principles, guidelines, methodologies, and tools\nessential for the responsible and ethical design, evaluation, and deployment of LLMs. Additionally,\nsome datasets have been curated for the explicit purpose of training and assessing LLMs in their\ncomprehension of ethical considerations, societal contexts, and norms, as well as their capacity to\nanalyze these complex scenarios [42, 43, 44, 45, 46]. These studies have shed light on the notable\nability of LLMs to understand and elucidate toxic content. However, it is important to underscore a\nsalient limitation within these investigations, namely, the inherent bias embedded within the collected\ndata. This bias stems from the geographical locations, cultural backgrounds, and political orientations\nof the annotators, casting a shadow on the universality of the findings [47].\nSome recent works demonstrate how in-context learning [24] and supervised tuning [48, 49] can help\naligning LLMs with moral instructions. These works aim to ensure that LLMs respect human values\nand norms, such as fairness, accountability, transparency, privacy, safety, etc. They also suggest ways\nto identify, measure, mitigate, and prevent the potential harms of LLMs to individuals and society.\nSome of these works propose ethical datasets [49] and guidelines [50, 51] to help researchers and\npractitioners assess and improve the ethical capabilities of LLMs.\nHowever, ethics is not a monolithic or universal concept. Different people may have different ethical\nviews, beliefs, values, preferences, etc. depending on their cultural, social, religious, and political\nbackgrounds [52, 53, 21]. Therefore, it is important to acknowledge and respect the diversity and\npluralism of human ethics and values when developing and using LLMs. This means that LLMs\nshould not impose or favor a single or dominant ethical perspective or value system over others but\nrather allow for multiple and diverse ethical perspectives and value systems to coexist and interact.\nEthical issues often involve shades of gray and require nuanced reasoning that cannot be adequately\ncaptured with a binary decision. Most of the current approaches to AI alignment fail to capture\nthe multifaceted nature of ethical reasoning. Ethical decisions often involve multiple dimensions,\nincluding fairness, justice, harm, and cultural context, which may not be fully addressed in a binary\nsetup. Binary choices may lack explanatory power. They don\u2019t provide insights into why a model\nmade a particular ethical decision, making it challenging to assess the quality of its ethical reasoning.\nIt may not adequately capture the complexities of ethical trade-offs. In real-world scenarios, ethical\ndecisions often involve weighing competing values, which binary tasks may not address effectively.\n3\nData and Method\nIn this section, we describe our experimental setup, the datasets, LLMs tested, prompt structure and\nmetrics. We present the LLMs with a prompt that contains the moral dilemma along with the 12\nethical considerations followed by three questions. Based on the responses to these questions, we\ncompute the P-score and individual stage scores for each LLM.\n4\n3.1\nDataset\nWe used five dilemmas from DIT-13 and constructed four novel moral dilemmas. Each author\ndesigned one dilemma (story and the ethical consideration statements) similar in structure to the\noriginal DIT dilemmas. The statements of each dilemma were then independently annotated by all the\nauthors for the Kohlberg\u2019s CMD stages that they represent. Cases of disagreements were discussed\nand if for a statement no clear consensus was reached, the statement was edited or redesigned to avoid\nany ambiguity. A brief summary of the dilemmas are described below, and Appendix A presents the\nfour new dilemmas.\nThe complete DIT-1 consists of six dilemmas: (1) Heinz dilemma - Should Heinz steal a drug from\nan inventor in town to save his wife who is dying and needs the drug?, (2) Newspaper dilemma -\nShould a student newspaper be stopped by a Principal of a high school when the newspaper stirs\ncontroversy in the community?, (3) Student dilemma - Should students take over an administration\nbuilding in protest of the Vietnam war?, (4) Webster dilemma - Should a minority member be hired\nfor a job when the community is biased?, (5) Prisoner dilemma - Should a man who escaped from\nprison but has since been leading an exemplary life be reported to authorities? and (6) Doctor\ndilemma - Should a doctor give an overdose of pain-killer to a suffering patient?\nThe four novel moral dilemmas are: (1) Monica\u2019s Dilemma - Should Monica give the first authorship\nto Aisha despite having the major contribution?, (2) Timmy\u2019s Dilemma - Should Timmy attend\nhis friend\u2019s wedding instead of fixing an urgent bug that could put customers\u2019 privacy at risk?,\n(3) Rajesh\u2019s Dilemma - Should Rajesh rent a house by hiding the secret of his non-vegetarian\nconsumption at home from the vegetarian neighborhood? and (4) Auroria Dilemma - Should the\ncountry Auroria share its innovations and resources to it\u2019s poor neighbor or profit off it\u2019s huge\ninvestments in research?\nThe dilemmas are associated with conflicting values such as interpersonal vs. societal (Heinz\ndilemma), interpersonal vs. professional (Timmy\u2019s and Monica\u2019s dilemmas), and community vs.\npersonal values placed in diverse cultural and situational contexts. We exclude the Doctor\u2019s dilemma\nfrom all experiments as most LLMs do not generate a response for it, presumably due to their content\nfiltering policies.\n3.2\nExperimental Setup\nWe study seven popular LLMs: GPT-4 (size undisclosed), PaLM-2 (size undisclosed), ChatGPT (July\n2023) (henceforth referred to as ChatGPTv2, 175B params), ChatGPT (December 2022) (henceforth\nreferred to as ChatGPTv1, 175B params), GPT-3.5 (text-davinci-003)(175B params), GPT-3 (175B\nparams) and Llama2-Chat (70B params). All these models are trained on massive amounts of text\ndata from various sources and domains and have different training methods and capabilities.\nFigure 1 shows the prompt structure. The text in black are fixed, whereas those in blue are dilemma\nspecific. Since LLMs might have positional bias while ranking the ethical consideration statements for\na dilemma, or in choosing one of the three options (O1, O2 and O3) as a resolution for the dilemma,\nwe consider 8 different predefined permutations of the 12 statements (out of 12! possibilities) and\nall, i.e., 6, permutations of the options. This amounts to 48 distinct prompts per dilemma. For all\nexperiments, we set temperature to 0, presence penalty to 1, top_p to 0.95, and max_tokens to 2000\n(except GPT-3 where it is set at 1000 due it\u2019s smaller context length).\n3.3\nMetrics\nWe used the metric P-score, henceforth pscore, as proposed by the DIT authors which indicates the\n\"relative importance a subject gives to principled moral considerations (Stages 5 and 6)\". pscore is\ncalculated by assigning points to the four most important statements the respondent (the LLM in our\ncase) has selected that correspond to the post conventional stages. 4, 3, 2 and 1 points are added to\nthe score if the first, second, third and fourth ranked statements belong to Stage 5 or 6 respectively.\nThe final score is obtained by multiplying the sum by 10. As an illustration, suppose that the model\npredicts 12, 7, 3, 9 as the most important statements of consideration in descending order, of which\nonly items 12 and 3 belong to the post-conventional stages. Then, the pscore will be 10 \u00b7 (4 + 2) = 60.\n3DIT-1 dilemmas are not freely available; we purchased the dataset from The University of Alabama through\nthe official website: https://ethicaldevelopment.ua.edu/ordering-information.html\n5\nFigure 1: Prompt structure illustrated for the Monica\u2019s Dilemma.\nSimilarly, we also calculate stage-wise scores, score\u03b8, as\nscore\u03b8 = 10 \u00b7\n4\nX\ni=1\n((5 \u2212 i) \u00b7 Si,\u03b8)\nwhere Si,\u03b8 =\n\u001a1\nif ith ranked statement is from Stage-\u03b8\n0\notherwise\n(1)\nThus, pscore = score5 + score6. We also compute the random baseline scores for each dilemma,\ni.e., the score a respondent will receive on average if they were ranking the items randomly. These\nbaseline numbers depend only on the number of items that belong to a certain stage for a dilemma.\nHeinz, Prisoner and Newspaper dilemmas have 3 items in Stages 5 and 6, giving a random baseline\npscore of 25. All other dilemmas have 4 items in Stages 5 and 6, and a random baseline pscore of\n33.33. Thus, the average random pscore over all dilemmas is 30.56.\nThe maximum possible pscore is 90 for the Heinz, Prisoner and Newspaper dilemmas and 100 for\nthe others. Thus, the pscore averaged on all dilemmas ranges from 0 to 96.67. Higher the pscore,\ndeeper the moral understanding and better the moral reasoning ability of a model (or equivalently, of\na human respondent). Various surveys conducted on human subjects using DIT [25] report a pscore\nof around 20 and 30 for junior and senior high school children respectively (mostly pre-conventional\nstage), between 40 and 46 for college students as well as average adults (mostly at the conventional\nstage), and between 53 and 63 for graduate school students (early post-conventional stage).\n4\nResults and Observations\nThe results of our experiments are summarized in two plots: Fig. 2 shows the pscore for each LLM\nas violin plots grouped by dilemmas. Fig. 3a shows the stage-wise scores for the LLMs averaged\nover all dilemmas; this provides insight into the overall performance and staging of the models. The\nthree key observations from these results are as: (a) Overall, GPT-3 has the lowest and close to\nrandom pscore, while GPT-4 has the highest pscore; the other models in ascending order of pscore are:\nGPT-3.5, ChatGPTv2, PaLM-2, Llama2-Chat, ChatGPTv1. Our study shows that except for GPT-3,\nall models investigated have a pscore equivalent to an average adult human or college student; only\nGPT-4 achieves a pscore (= 55.68) in the range of a graduate student and shows post-conventional\nmoral reasoning abilities. (b) All models perform poorly on the Prisoner and Webster dilemmas,\nwhile most models perform well on the Timmy and Newspaper dilemmas; and (c) There is significant\nvariability in the responses of all the models over different runs (as shown by the violin plots), as well\nas specific dilemmas where they perform exceptionally well (e.g., GPT-4 on Newspaper dilemma) or\npoorly (e.g., GPT-4 on Rajesh\u2019s dilemma).\n6\nFigure 2: Dilemma wise pscore comparison across LLMs. The dotted line shows the random baseline\npscore for the dilemma.\nFig 3b shows the resolutions proposed by the models for each dilemma. Two interesting observations\nemerge from it: (a) All models agree perfectly for the Webster dilemma. A majority of models agree\nfor the Heinz, Newspaper, Rajesh and Auroria dilemmas. (b) Contrary to other models, ChatGPTv2,\ndoes not favor any particular resolution (except in Webster). In the subsequent paragraphs, we present\nmodel-specific observations.\nGPT-3. The prompt structure described in Fig. 1 did not work with GPT-3, as the model failed to\ngenerate any cogent response. Through trial-and-error, we constructed a prompt where only the\nresolution of the moral dilemma and the selection of top four statements (out of 12) were asked for,\nwhich seemed to work for the model. Even then, we observed that it frequently ranks the statements\nat position 1, 3, 5 and 7 as most significant options, irrespective of the stages the sentences belonged\nto. This explains why the average pscore for GPT-3, 29.84, is close to that of the random baseline.\nIn conclusion, GPT-3 is incapable of moral reasoning and also, of following complex multistage\ninstructions. Incidentally, we also tested text-davinci-002, but could not make it generate cogent\nresponses. Therefore, the model is excluded from the study.\nGPT-3.5, ChatGPT (both v1 & v2) and GPT-4 demonstrate a greater ability of understanding the\ninstructions, presumably due to the RLHF training. Therefore, these models responded consistently\nto the prompt questions, and also perform significantly better than the random baseline. We observe a\ngeneral trend that the bigger and the newer models have higher pscore, except for ChatGPTv2 that has\na slightly lower pscore than its previous version ChatGPTv1. Incidentally, there are anecdotal (but\ncontested) claims [54] that the performance of ChatGPT is degrading over time as newer versions are\nbeing released, which is consistent with our observation. With a pscore of 55.68, GPT-4 is the only\nmodel that clearly shows post-conventional moral reasoning abilities equivalent of graduate students.\nLlama2-Chat, even though a much smaller model compared to GPT-3.x series, achieves an unex-\npectedly high pscore which is less than only GPT-4 and ChatGPTv1. This points to the possibility of\n7\n(a) Stage-wise scores comparison of different models.\nGPT-3.5\nChatGPTv2\nLlama2-Chat\nChatGPTv1\nGPT-4\nPaLM-2\nAuroria\nMonica\nRajesh\nTimmy\nPrisoner\nWebster\nNewspaper\nStudent\nHeinz\n(b) Dilemma-wise models\u2019 reso-\nlution for the dilemma.\nFigure 3: Model-wise scores and their dilemma-wise resolutions. PaLM-2 results are from 8 dilemmas\n(Sec. 4). In Fig-(b), the colors\u2019 RGB components depict the fraction of runs with corresponding\nresolutions (Green - O1(Should do), Blue - O2(Can\u2019t Decide), Red - O3(Shouldn\u2019t do))\nbuilding smaller models with strong moral reasoning capabilities. PaLM-2 exhibited superior moral\nreasoning capability with a pscore of 52.24. However, it did not generate a response to the Prisoner\ndilemma. Therefore, the total pscore is averaged over 8 instead of 9 dilemmas. When averaged\nover the same 8 dilemmas, the pscore of the other models are (in descending order): GPT-4 \u2013 58.81,\nChatGPTv1 \u2013 56.44, Llama2-Chat \u2013 52.85, ChatGPTv2 \u2013 51.55, GPT-3.5 \u2013 49.48 and GPT-3 \u2013 31.20.\nThus, PaLM-2 performs worse than GPT-4 and ChatGPTv1, but is comparable to Llama2-Chat and\nChatGPTv2. Note that the average pscore is significantly higher for all the models when Prisoner\ndilemma is removed from the set because all models perform poorly on this dilemma.\n5\nDiscussion and Conclusion\nIn this study, we propose an effective evaluation framework to measure the ethical reasoning capability\nof LLMs based on Kohlberg\u2019s Cognitive Moral Development model and Defining Issues Test. Apart\nfrom the 6 moral dilemmas included in DIT-1, we propose 4 novel dilemmas partly to expand the\nsocio-cultural contexts covered by the dilemmas, and partly to ensure that the LLMs were not already\nexposed to them. Our study shows that GPT-4 exhibits post-conventional moral reasoning abilities at\nthe level of human graduate students, while other models like ChatGPT, LLama2-Chat and PaLM-2\nexhibit conventional moral reasoning ability equivalent to that of an average adult human being or\ncollege student.\nWe are aware of several limitations of this study, including the known criticisms of the DIT framework\n[55, 56], that provides us with enough reasons not to take the numbers at their face value. More\ninvestigation is necessary to firmly establish the moral reasoning abilities and limitations of LLMs.\nNevertheless, it is interesting to ponder on some of the repercussions of these findings. While one\ncould explain the conventional moral reasoning abilities observed in the LLMs as an effect of the\ntraining data [57] at pre-training , instruction fine-tuning and RLHF phases, which certainly contains\nseveral instances of conventionalized and codified ethical values, one wonders how an LLM (e.g,\nGPT-4 ) could exhibit post-conventional moral reasoning abilities. Since the training data and the\narchitectural details of GPT-4 are undisclosed, one can only speculate the reasons. Either the data\n(most likely the one used during RLHF) consisted of many examples of post-conventional moral\nreasoning, or it is an emergent property of the model. In the latter case, a deeper philosophical\nquestion that arises is whether moral reasoning can emerge in LLMs, and if so, whether it is just a\nspecial case of general reasoning ability.\nThere are other open problems around the dilemmas and types of moral questions where the current\nmodels are lagging (e.g., Prisoner and Webster dilemma), what makes these dilemmas difficult, and\n8\nhow can we train models with the specific objective of improving their moral reasoning capability.\nOne might also ask that since many of the models, especially GPT-4, is as good or better than an\naverage adult human in terms of their moral development stage scoring, does it then make sense to\nleave the everyday moral decision making tasks to LLMs. In the future, if and when we are able to\ndesign LLMs with pscore higher than expert humans (e.g., lawyers and justices), should we replace\njudges and jury members by LLMs?\nReferences\n[1] J. Rest. Development in Judging Moral Issues. University of Minnesota Press, Minneapolis,\nMN, 1979.\n[2] Lawrence Kohlberg. The philosophy of moral development: Essays on moral development. San\nFrancisco, 1981.\n[3] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor,\nAmelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown,\nZac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell,\nWilliam Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Taxonomy\nof risks posed by language models. In 2022 ACM Conference on Fairness, Accountability,\nand Transparency, FAccT \u201922, page 214\u2013229, New York, NY, USA, 2022. Association for\nComputing Machinery.\n[4] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang,\nMyra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will\nHawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne\nHendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Ethical and\nsocial risks of harm from language models, 2021.\n[5] Rebecca L Johnson, Giada Pistilli, Natalia Men\u00e9dez-Gonz\u00e1lez, Leslye Denisse Dias Duran,\nEnrico Panai, Julija Kalpokiene, and Donald Jay Bertulfo. The ghost in the machine has an\namerican accent: value conflict in gpt-3, 2022.\n[6] Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A. Rothkopf, and Kristian\nKersting. Large pre-trained language models contain human-like biases of what is right and\nwrong to do, 2022.\n[7] Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh.\nPropile: Probing privacy leakage in large language models. arXiv preprint arXiv:2307.01881,\n2023.\n[8] Monojit Choudhury and Amit Deshpande. How linguistically fair are multilingual pre-trained\nlanguage models? In Proceedings of the AAAI conference on artificial intelligence, volume 35,\npages 12710\u201312718, 2021.\n[9] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Love-\nnia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation\nof chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023,\n2023.\n[10] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo\nZhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in\nlarge language models. arXiv preprint arXiv:2309.01219, 2023.\n[11] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and Mark\nSteedman. Sources of hallucination by large language models on inference tasks. arXiv preprint\narXiv:2305.14552, 2023.\n[12] Xingxuan Li, Yutong Li, Linlin Liu, Lidong Bing, and Shafiq Joty. Is gpt-3 a psychopath? evalu-\nating large language models from a psychological perspective. arXiv preprint arXiv:2212.10529,\n2022.\n[13] Paula Fortuna, Juan Soler, and Leo Wanner. Toxic, hateful, offensive or abusive? what are we\nreally classifying? an empirical analysis of hate speech datasets. In Proceedings of the 12th\nlanguage resources and evaluation conference, pages 6786\u20136794, 2020.\n9\n[14] Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. Stereotyp-\ning norwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npages 1004\u20131015, 2021.\n[15] Fabio Del Vigna12, Andrea Cimino23, Felice Dell\u2019Orletta, Marinella Petrocchi, and Maurizio\nTesconi. Hate me, hate me not: Hate speech detection on facebook. In Proceedings of the first\nItalian conference on cybersecurity (ITASEC17), pages 86\u201395, 2017.\n[16] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,\nPaul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.\narXiv preprint arXiv:1909.08593, 2019.\n[17] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\n[18] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece\nKamar. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate\nspeech detection. arXiv preprint arXiv:2203.09509, 2022.\n[19] Shaoxiong Ji, Shirui Pan, Xue Li, Erik Cambria, Guodong Long, and Zi Huang. Suicidal\nideation detection: A review of machine learning methods and applications. IEEE Transactions\non Computational Social Systems, 8(1):214\u2013226, 2020.\n[20] Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, and Xing Xie. From instructions to intrinsic\nhuman values\u2013a survey of alignment goals for big models. arXiv preprint arXiv:2308.12014,\n2023.\n[21] Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine, Valentina Pyatkin, Peter West,\nNouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, et al. Value kaleidoscope: Engaging\nai with pluralistic human values, rights, and duties. arXiv preprint arXiv:2309.00779, 2023.\n[22] William James. The moral philosopher and the moral life. The International Journal of Ethics,\n1(3):330, 1891.\n[23] Michael Slote. Utilitarianism, moral dilemmas, and moral cost. American Philosophical\nQuarterly, 22(2):161\u2013168, 1985.\n[24] Jingyan Zhou, Minda Hu, Junan Li, Xiaoying Zhang, Xixin Wu, Irwin King, and Helen Meng.\nRethinking machine ethics\u2013can llms perform moral reasoning through the lens of moral theories?\narXiv preprint arXiv:2308.15399, 2023.\n[25] J.R. Rest and University of Minnesota. Center for the Study of Ethical Development. DIT\nManual: Manual for the Defining Issues Test. Center for the Study of Ethical Development,\nUniversity of Minnesota, 1990.\n[26] Cheryl E Sanders. Lawrence Kohlberg\u2019s stages of moral development. technical report, April\n2023.\n[27] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[28] OpenAI. Gpt-4 technical report, 2023.\n[29] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\nLaurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira,\nMark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing\nZhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,\nJames Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave,\nMostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg,\nFangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas\nGonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu,\nJeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia,\n10\nKathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin\nLee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao\nLiu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra,\nMaysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish,\nMarie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan\nRichter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha\nValter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang,\nJohn Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu,\nQiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui\nWu. Palm 2 technical report, 2023.\n[30] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[31] The definition of morality.\n[32] Jean Piaget. The moral judgment of the child. Routledge, 2013.\n[33] Dora Shu-Fang Dien. A chinese perspective on kohlberg\u2019s theory of moral development.\nDevelopmental Review, 2(4):331\u2013341, 1982.\n[34] John R Snarey. Cross-cultural universality of social-moral development: a critical review of\nkohlbergian research. Psychological bulletin, 97(2):202, 1985.\n[35] Muriel J Bebeau and Mary M Brabeck. Integrating care and justice issues in professional moral\neducation: A gender perspective. Journal of moral education, 16(3):189\u2013203, 1987.\n[36] Jonathan Haidt. The emotional dog and its rational tail: a social intuitionist approach to moral\njudgment. Psychological review, 108(4):814, 2001.\n[37] James R Rest et al. Moral development in the professions: Psychology and applied ethics.\nPsychology Press, 1994.\n[38] Georg Lind. An introduction to the moral judgment test (mjt). Unpublished manuscript.\nKonstanz: University of Konstanz. http://www. uni-konstanz. de/ag-moral/pdf/MJT-introduction.\nPDF, 1998.\n[39] Jesse Graham, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P Wojcik, and Peter H\nDitto. Moral foundations theory: The pragmatic validity of moral pluralism. In Advances in\nexperimental social psychology, volume 47, pages 55\u2013130. Elsevier, 2013.\n[40] Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and Colin Raffel.\nEvaluating the factual consistency of large language models through summarization. arXiv\npreprint arXiv:2211.08412, 2022.\n[41] Kelsey Piper. The case for taking ai seriously as a threat to humanity, Oct 15, 2020.\n[42] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob\nSteinhardt. Aligning ai with shared human values. arXiv preprint arXiv:2008.02275, 2020.\n[43] Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. Social chemistry\n101: Learning to reason about social and moral norms. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP), pages 653\u2013670, Online,\nNovember 2020. Association for Computational Linguistics.\n[44] Denis Emelin, Ronan Le Bras, Jena D Hwang, Maxwell Forbes, and Yejin Choi. Moral\nstories: Situated reasoning about norms, intents, actions, and their consequences. arXiv preprint\narXiv:2012.15738, 2020.\n[45] Nicholas Lourie, Ronan Le Bras, and Yejin Choi. Scruples: A corpus of community ethical\njudgments on 32,000 real-life anecdotes. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 13470\u201313479, 2021.\n[46] Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. Social\nbias frames: Reasoning about social and power implications of language. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics, pages 5477\u20135490,\nOnline, July 2020. Association for Computational Linguistics.\n11\n[47] Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre K\u0131c\u0131man. Social data: Biases,\nmethodological pitfalls, and ethical boundaries. Frontiers in big data, 2:13, 2019.\n[48] Liwei Jiang, Jena D Hwang, Chandra Bhagavatula, Ronan Le Bras, Jenny Liang, Jesse Dodge,\nKeisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, et al. Can machines learn\nmorality? the delphi experiment. arXiv preprint arXiv:2110.07574, 2021.\n[49] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob\nSteinhardt. Aligning ai with shared human values, 2023.\n[50] Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina\nWilliams. On the machine learning of ethical judgments from natural language. In Proceedings\nof the 2022 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 769\u2013779, Seattle, United States, July 2022.\nAssociation for Computational Linguistics.\n[51] Patrick Schramowski, Cigdem Turan, Sophie Jentzsch, Constantin Rothkopf, and Kristian\nKersting. Bert has a moral compass: Improvements of ethical and moral values of machines,\n2019.\n[52] William A Galston and William Arthur Galston. Liberal pluralism: The implications of value\npluralism for political theory and practice. Cambridge University Press, 2002.\n[53] Mark C. Navin and Katie Attwell. Vaccine mandates, value pluralism, and policy diversity.\nBioethics, 33(9):1042\u20131049, 2019.\n[54] Lingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt\u2019s behavior changing over time?\narXiv preprint arXiv:2307.09009, 2023.\n[55] Richard M Martin, Michael Shafto, and William Vandeinse. The reliability, validity, and design\nof the defining issues test. Developmental Psychology, 13(5):460, 1977.\n[56] Stanley R Kay. Kohlberg\u2019s theory of moral development: Critical analysis of validation studies\nwith the defining issues test. International Journal of Psychology, 17(1-4):27\u201342, 1982.\n[57] Patrick Schramowski, Cigdem Turan, Sophie Jentzsch, Constantin Rothkopf, and Kristian\nKersting. The moral choice machine. Frontiers in artificial intelligence, page 36, 2020.\nA\nDilemmas\nThe dilemmas we have crafted and illustrated can be found in Figures 4 through 7.\nFigure 4: Story and 12 statements for Rajesh\u2019s Dilemma\n12\nFigure 5: Story and 12 statements for Monica\u2019s Dilemma\nFigure 6: Story and 12 statements for Timmy\u2019s Dilemma\n13\nFigure 7: Story and 12 statements for Auroria Dilemma\n14\nFigure 8: Radar Plot\n15\n"
  },
  {
    "title": "DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention",
    "link": "https://arxiv.org/pdf/2309.14327.pdf",
    "upvote": "21",
    "text": "DeepSpeed-VisualChat: Multi Round Multi Images Interleave\nChat via Multi-Modal Casual Attention\nZhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin\nOlatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He\nDeepSpeed of Microsoft\nAbstract\nMost of the existing multi-modal models, hindered by their incapacity to adeptly manage interleaved\nimage-and-text inputs in multi-image, multi-round dialogues, face substantial constraints in resource\nallocation for training and data accessibility, impacting their adaptability and scalability across varied\ninteraction realms. To address this, we present the DeepSpeed-VisualChat framework, designed to optimize\nLarge Language Models (LLMs) by incorporating multi-modal capabilities, with a focus on enhancing\nthe proficiency of Large Vision and Language Models in handling interleaved inputs. Our framework is\nnotable for (1) its open-source support for multi-round and multi-image dialogues, (2) introducing an\ninnovative multi-modal casual attention mechanism, and (3) utilizing data blending techniques on existing\ndatasets to assure seamless interactions in multi-round, multi-image conversations. Compared to existing\nframeworks, DeepSpeed-VisualChat shows superior scalability up to 70B parameter language model size,\nrepresenting a significant advancement in multi-modal language models and setting a solid foundation for\nfuture explorations.1\n1\nIntroduction\nState-of-the-art large language models (LLMs) like GPT [7, 28] have showcased exceptional prowess in\nmyriad text generation and comprehension tasks, especially when subjected to zero-/few-shot learning. Once\nthese models undergo supervised fine-tuning or reinforcement learning combined with human feedback, their\nproficiency in versatile interactive challenges\u2014ranging from coding tasks [10] to quantitative reasoning [17],\nmathematical proofs [14, 43], and AI chatbot interactions [27, 3, 29, 44]\u2014becomes comparable to human\nexperts.\nSeeking to transcend the bounds of text-only processing inherent to LLMs, numerous researchers have\nmade strides in endowing these models with multi-modal capabilities. These advances span across various\nmodalities such as images, audios, and videos, often achieved via feature alignment and model alterations [9,\n11, 48, 23, 18, 5, 12]. Notably, among these multi-modal endeavors, large vision and language models (LVLMs)\nhave garnered significant interest [48, 23], mainly owing to their potential in facilitating comprehensive\nvisual-textual understanding.\nCurrent frameworks and studies largely focus on either (1) tasks related to individual images, like visual\nquestion answering and captioning [23], or (2) handling multiple images but requiring concurrent input [18].\nNeither approach adeptly manages interleaved image-and-text inputs. The QWen-VL framework [5], an\nextension of the LLaVA architecture [23], makes progress in this direction. However, its training costs prove\nprohibitive for many research labs, and it withholds its training data. In parallel, the SparklesChat model [12],\nannotated by GPT4, relies on continuous training with MiniGPT4 [48] due to its limited dataset. Both\nQWen-VL and SparklesChat adopt prevailing LVLMs designs without innovative architectural exploration.\n1Code will be released soon as a part of https://github.com/microsoft/DeepSpeedExample\n1\narXiv:2309.14327v3  [cs.CV]  29 Nov 2023\nFigure 1: An example of DeepSpeed-VisualChat.\nAdditionally, in multi-image contexts, their performance is found lacking, even with significant training\ninvestments 2, as shown in our comparisons Figure 9.\nWhile larger language models typically demonstrate superior generation abilities compared to their smaller\ncounterparts, prevailing frameworks [18, 23, 48, 5] predominantly concentrate their efforts on LLMs with 7 or\n13 billion parameters (e.g., LLaMa-2-7B or LLaMa-2-13B [39]). This focus restricts the exploration of the\nextensive capabilities inherent to larger LLMs.\nTo address the aforementioned challenges, we introduce the DeepSpeed Multi Round and Multi Images\nChat framework (DeepSpeed-VisualChat), offering several key contributions:\n\u2022 Fully Open-Sourced Multi-round Multi-image Framework: DeepSpeed-VisualChat, one of the pi-\noneering fully open-sourced frameworks, enables multi-round and multi-image dialogues, accommodating\ninterleaved text-and-image inputs, as visualized in Figure 1.\n\u2022 Multi-Modal Casual Attention (MMCA): We devise a novel MMCA for multi-modal models that\nindependently computes attention weights across various modalities. MMCA attains objectives analogous\nto conventional cross-attention mechanisms [18], yet offers enhanced casual attention interpretations for\ngenerative tasks, eliminating the need for additional modules or parameters, and it presents superior\ntraining data efficiency compared to standard casual attention [48, 23].\n\u2022 Data Blending for Interleaved Inputs:\nTo facilitate conversations with interleaved modalities,\nDeepSpeed-VisualChat employs assorted data blending techniques on existing datasets, overcoming the\nshortage of interleaved text-and-image inputs in most available open-sourced datasets.\n\u2022 Unprecedented Scalability: We leverage the DeepSpeed framework [31] to amplify our training\nwith a 2B visual encoder from [13] and a 70B language decoder from LLaMa-2 [39], illustrating the\nremarkable scalability of our framework.\nThese innovations demonstrate our commitment to progressing multi-modal conversational AI models, ensuring\nenhanced interoperability, attention mechanism sophistication, scalable solutions, and comprehensive dataset\nutilization.\n2We count the pretraining cost of MiniGPT4 as a part of SparklesChat\n2\n2\nRelated Work\nMulti-modal models, especially those focusing on vision-language integrations, typically fall into two distinct\ncategories: dual-encoder-based models [38, 26, 36, 47, 30, 22, 16, 21, 42, 6, 40, 45, 41, 20, 8, 30, 15, 33, 32,\n46, 35, 24, 34], and models comprising visual encoders and textual decoders [2, 19, 23, 18, 48, 11, 37, 5, 4, 12].\nOur work is associated with the latter, often referred to as Large Visual Language Models (LVLMs), thus our\ndiscussion predominantly revolves around LVLMs.\nMost implementations of LVLMs deploy one of two architecture styles: (1) The Flamingo design [2, 18, 4]\nincorporates cross-attention, introducing new parameters to LLMs to interlink visual and textual elements.\n(2) The Flamingo design [2, 18, 4] incorporates cross-attention, introducing new parameters to LLMs to\ninterlink visual and textual elements. Although both designs effectively assimilate visual information and\ngenerate textual content, their advantages and drawbacks are manifold. The Flamingo design necessitates\nextensive training/inference memory and fewer data due to the introduction of numerous new parameters.\nConversely, the MiniGPT4 design, while less memory-intensive, is more data-dependent to effectively align\nvisual and textual features. Consequently, an emerging query is whether a novel architecture can harmonize\nthe introduction of fewer new parameters with data efficiency.\nDespite the substantial advancements achieved by existing LVLMs, certain aspects, particularly multi-\nround multi-image conversation involving interleaved image and text input, remain unaddressed. Works\nlike [23] predominantly concentrate on single image conversation, and [18] necessitate simultaneous input of\nall images, limiting their applicability in conventional conversational applications.\nThe paucity of data pertinent to these scenarios has led researchers to explore available data to facilitate\nnew applications. A contemporary work, SparklesChat [12], exploits the GPT-4 [1] API to synthesize several\nthousands of multi-round multi-image data in a unique format. However, SparklesChat does not innovate\nany new architectural modifications and relies on the pre-trained model, MiniGPT4 [48], thereby incurring\nadditional training costs and complexity. Moreover, SparklesChat does not exhibit any superior performance in\nunseen multi-image tasks compared to DeepSpeed-VisualChat, even without utilizing multi-image interleaved\nchat data. Refer to Figure 9 and A.1 for detailed examples.\n3\nMethod\nFigure 2: Model Structure. A pre-trained vision\nencoder encodes an image which is then projected\nthrough a linear layer to align with the hidden\ndimension of the text embedding layer\u2019s output.\nThese different inputs are subsequently merged\nand forwarded to language models like LLaMa-2\npowered by our new Multi-Modal Casual Atten-\ntion (MMCA) mechanism. Here, both the vision\nencoder and the language model are frozen.\nOur model architecture is built on the structure of\nMiniGPT4 [48, 23], as depicted in Figure 2. Specifically,\nwe maintain the entirety of the visual encoder and the\nwhole language model, with the exception of the embed-\nding layer, in a frozen state. Thus, the only trainable\nparameters within our model are the visual feature pro-\njection layer (a linear layer) and the language model\u2019s\nembedding.\nIn total, our set of trainable parameters\nranges around O(10M) to O(100M), primarily contingent\non the size of the embedding layer.\nDiverging from the previous MiniGPT4 architecture,\nwe substitute the conventional casual attention mechanism\nwith our proposed multi-modal casual attention mecha-\nnism (refer to Section 4.1). This modification solely alters\nthe computation of casual attention and does not incor-\nporate any new parameters.\nWe adopt a unified instruction tuning format for all\nexperiments, and the template is shown in Figure 3. It is\ncrucial to note that we do not employ any special tokens\nas our prefix; for example, \u201c### Image i\u201d is not a special\ntoken, and the tokenizer interprets it as a regular string.\n3\n<System\nInsturction >\n% You are a powerful\nvision \u2212language\na s s i s t a n t .\n### Image\n1: <image>\n% some image ,\ne . g . ,\ncat \u22121.png\n### Question : <question>\n% please\ndescribe\nthe image .\n### Answer : <answer>\n% It \u2019 s a cute\nblack\ncat .\n### Image\n2: <image>\n% some image ,\ne . g . ,\ncat \u22122.png\n### Image\n3: <image>\n% some image ,\ne . g . ,\ncat \u22123.png\n### Question : <question>\n% What\u2019 s\nd i f f e r e n c e\nbetween\nthree\ncats ?\n### Answer : <answer>\n% The color\nof\nthree\ncats\nare\nd i f f e r e n t .\n. . .\nFigure 3: Here <System Instruction>, <question>, <answer> can be simply replaced by text, and <image>\ncan be replaced by real image tokens. The content after \u201c%\u201d is an example.\nFigure 4: Different Attention Mechanisms: Examine the differing attention mechanisms using an input\nsentence \"User: Please describe the image.\" coupled with three Image tokens (I-token1, I-token2, I-token3).\nOn the left, we demonstrate standard causal attention [48, 5], treating image tokens as text. In the middle,\nwe present cross attention applied to images, while maintaining standard causal attention for text tokens.\nOn the right, we illustrate our innovative multi-modal attention proposal where image tokens only perform\nself-attention, and text tokens attend to text/image tokens independently, highlighted with an orange mask.\nThis mechanism is defined by: softmax(QKT \u2299 M1) + softmax(QKT \u2299 M2) with Q and K as query and key,\nM1 = [M == 1], and M2 = [M == 2], with M \u2208 R10\u00d710 in this case.\nIn alignment with the recent trend of instruction fine-tuning, the final loss of our model is calculated\nsolely on \u201c<answer>\u201d, as illustrated in Figure 3. If multiple conversations are present, we compute the loss\nfor all corresponding \u201c<answer>\u201d instances.\nThroughout the paper, unless specifically mentioned, we employ the LLaMa-2 family as our language and\nutilize the extracted (and frozen) visual encoder from QWen-VL [5] as our visual encoder, which accepts\n448x448 images and produces 256 image tokens per image. The rationale for opting for QWen-VL \u2019s encoder\nover the typically utilized CLIP [30] is elaborated in Section 4.3. The sequence length for training LLaMa-2 is\ncapped at 4096. When referring to our model as DeepSpeed-VisualChat-xB (e.g., DeepSpeed-VisualChat-13B),\nthe size is exclusively contingent on the language model components (LLaMa-2-13B).\n4\nMulti-Round Single-Image Exploration\n4.1\nMulti-Modal Casual Attention\nThere are two common attention mechanisms used to connect the visual and textual components in a\nmulti-modal model: causal attention, as used in [48, 5], and cross attention, as used in [18, 2].\nCausal Attention (CA): The CA-based method simply projects visual features (i.e., the features from\nthe output of the final visual encoder layer) into textual features and combines them with the normal textual\n4\nfeatures after the textual embedding layer to feed into LLMs. The benefit of CA is that it\u2019s a natural\nextension of the original attention mechanism in LLMs, and as such, it doesn\u2019t introduce any extra modules\nor parameters. However, this approach raises some intuitive problems:\n(1) For a visual token, it attends to previous visual and textual tokens, even though visual tokens are\nalready fully encoded in a bidirectional manner and don\u2019t need further attention from other visual tokens or\nthe beginning of textual tokens.\n(2) For a textual token, it needs to learn how to distribute its attention weights between its previous\ntextual and image tokens. Due to these issues, we found that the data efficiency of CA in LVLMs is often\nproblematic. To address this, LLaVA and QWen-VL require visual-language pretraining to fully align visual\nfeatures with textual features. We also test and compare it with our proposed MMCA in Section 4.2.\nCross Attention (CrA): The alternative, cross attention (CrA), along with CA, exhibits better data\nefficiency but also comes with a few drawbacks:\n(1) It introduces new parameters to the model. For example, Otter has more than 1.5 billion trained\nparameters compared to the millions of trained parameters in LLaVA. This significantly increases the training\ncost and memory requirements.\n(2) It requires careful design if an image is introduced in the middle of a conversation during training, as\nprevious text tokens should not be able to attend to the image.\nMulti-Modal Causal Attention Mechanism (MMCA): To overcome these issues, we propose a new\nmulti-modal causal attention mechanism (MMCA). The overall idea is as follows:\n(1) For visual tokens, they only attend to themselves, as visual tokens are encoded by the visual encoder.\n(2) For textual tokens, they attend to all their previous tokens. However, they have two separate attention\nweight matrices for their previous textual tokens and image tokens.\nThe intuition behind the second point of MMCA is that the attention weight for one modality may affect\nthe other modality. For instance, a textual token may pay more attention to textual information than visual\ninformation. Therefore, if the attention weight matrix is normalized across both modalities, the attention\nscore for visual tokens might be very small. Refer to Figure 4 for a visualization of the three attention\nmechanisms.\n4.2\nResult\n4.2.1\nComparison between Different Attentions\nExperimental Setting\nWe employ the LLaMa-2-7B language model in conjunction with the QWen-VL-\nvisual-encoder as our visual encoder. These two models are connected via a straightforward linear projection\nlayer. Our model underwent training on two LLaVa datasets, as outlined in the initial two rows of Table 1.\nDuring training, all models were run for 5 epochs with a training batch size of 128. Our primary evaluation\nfocused on single-image captioning and single-image Visual Question Answering (VQA). The peak learning\nrate was set to 1e-3 for both the projection linear layer and the embedding layer, and we employed the\nAdamW optimizer [25] with first- and second-order coefficients set to (0.0, 0.95).\nFor dataset splitting, we divided the training and validation datasets in a 90/10 ratio across the entire\ndataset. Additionally, we incorporated 10% of the total training iterations as warm-up steps. Our training\nframework of choice was DeepSpeed [31], and we utilized FP16 training to expedite the training process.\nThroughout this work, we mainly compare the generation capability of different models on certain examples\nwithout comprehensively testing models on existing benchmark. Please see more details in Section 6 for\nlimitations of our work.\nDemo results.\nWe begin by showcasing various examples that highlight the capabilities of DeepSpeed-\nVisualChat in single-image visual language conversations, employing different attention mechanisms. As\ndemonstrated in Figure 5, Figure 6, and Figure 7, DeepSpeed-VisualChat, when coupled with MMCA,\neffectively discerns visual details in images and furnishes coherent responses to user queries.\nFurthermore, DeepSpeed-VisualChat exhibits a more comprehensive and precise grasp of image details\ncompared to alternative attention mechanisms, such as the use of combined masks from both causal attention\n5\nTable 1:\nTraining datasets summary.\nDue to context length limitation, for otter_mimicit_sn, ot-\nter_mimicit_tvc, and otter_mimicit_vst datasets we only used the samples with \u2264 8 images.\nName\nNum. samples\nDescription\n(1) llava\n49924\nThe detail description and complex reasoning data used by the LLaVA model [23].\nRandomly concatenate 1 to 3 samples into one sample. Details in Section 5.1.\n(2) llava_dial\n37818\nThe conversation data used by the LLaVA model [23].\nRandomly concatenate 1 to 2 samples into one sample. Details in Section 5.1.\n(3) otter_mimicit_cgd\n70940\nThe COCO (General) data used by the Otter model [18].\n(4) otter_mimicit_sd\n8006\nThe SD (Surveillance) data used by the Otter model [18].\n(5) otter_mimicit_sn\n487\nThe SN (Indoor Ego.) data used by the Otter model [18].\n(6) otter_mimicit_tvc\n2\nThe TVC (TV) data used by the Otter model [18].\n(7) otter_mimicit_vst\n115\nThe VIST (Story) data used by the Otter model [18].\n(8) llava_otter_blend\n48869\nData blended from llava, llava_dial, otter_mimicit_cgd. Details in Section 5.1.\n(9) sparkles_dialogue\n6520\nThe SparklesDialogue data used by the SparklesChat model [12].\nTotal\n222681\nFigure 5: Example visual and language inputs that demonstrate the output comparison between (1) the\nstandard causal attention (CA) (2) the standard causal attention combined with cross-attention (CA +\nCrA) and (3) the special multi-modal causal attention (MMCA) in DeepSpeed-VisualChat-Single.\nand cross attention. It is also evident that, in contrast to the combination of CrA and CA, as well as MMCA,\nCA alone may exhibit slightly more errors (Figure 5) and capture a lower degree of reasoning capability\n(Figure 7).\n4.2.2\nResult of DeepSpeed-VisualChat-Single\nExperimental Setting\nAll settings remain consistent with those outlined in Section 4.2.1, with the\nexception of two modifications: an increase in the language model size from LLaMa-2-7B to LLaMa-2-13B\nand an extension of the training epoch count from 5 to 10.\nDemo results\nUpon elevating the language model size from 7B to 13B, we observe a marked improvement\nin the model\u2019s ability to recognize images and capture the logical connections between questions and images.\nAdditionally, referring to the upper example in Figure 8, aside from the issue of incorrect descriptions, it\nbecomes apparent that DeepSpeed-VisualChat-Single-7B occasionally exhibits repetitive context. However,\nthis problem is less frequent with DeepSpeed-VisualChat-Single-13B, owing to the enhanced generative\ncapabilities of larger language models. This observation aligns with the superior performance typically\nassociated with larger language models.\n6\nFigure 6: DeepSpeed-VisualChat-Single accurately identifies the squirrel and camera in the image, while the\nbaseline model mistakenly includes \u201cstanding next to a tree\u201d.\nFigure 7: DeepSpeed-VisualChat-Single accurately identifies the scene as a beautiful lake and offers a set\nof plausible suggestions. In contrast, the baseline misinterprets the image as containing \u201cdock with a boat\nramp\u201d.\n7\nFigure 8: The above two examples illustrate the difference between DeepSpeed-VisualChat-Single-13B and\nDeepSpeed-VisualChat-Single-7B.\n8\n4.3\nOther Learning\nThroughout the training process of DeepSpeed-VisualChat-Single, we accumulated several additional lessons.\nIt\u2019s important to note that most of these observations lack sufficient evidence and require further exploration.\nWe present them here to assist others, but they should not be considered final conclusions.\n\u2022 Better Visual Encoder: Commonly, the CLIP visual encoder is used in LVLMs. However, the CLIP\nencoder\u2019s resolution is limited to 224x224, which restricts the level of detail in the images. In our testing,\nwe discovered that using the newly released visual encoder from QWen-VL significantly improves the\nfinal model quality due to its higher input resolution (448x448) and larger encoder size (2B parameters).\n\u2022 Overfitting or Not: Typically, we select the best evaluation checkpoint or one close to it for final\ntesting. However, during DeepSpeed-VisualChat-Single training, we found that the final checkpoint,\neven if it appears overfitted, often delivers better testing results compared to middle checkpoints. Does\nthis imply that we should intentionally overfit our model? The answer is no. We experimented with 5,\n10, and 20 epochs for DeepSpeed-VisualChat-Single-13B and observed that 10-epoch training typically\nyields superior final model quality.\n\u2022 Adding LoRA to Visual Encoder or Language Decoder: We attempted to introduce LoRA-based\ntraining to enhance model quality. However, applying LoRA to either module did not yield any\nsignificant benefits.\n\u2022 Lowering the Learning Rate for Pretrained Components: We experimented with a smaller\nlearning rate for language embedding since it is already pretrained. However, our results indicated that\nthere is no significant difference when using a separate lower learning rate.\n\u2022 Using Chat-/Non-Chat-Based Models: We explored both chat-based and non-chat-based LLama-2\nmodels. Our findings suggest that when using the chat-based model, strict adherence to the chat-based\nmodel\u2019s instruction tuning format is crucial. Failing to do so resulted in even worse model quality than\nthe non-chat-based model.\n\u2022 Inserting New Special Tokens or Not: As illustrated in Figure 3, a few tokens can be replaced by\nnew inserted special tokens, such as encoding \"###Human: \" as a new special token. However, our\ntesting revealed that it is better not to incorporate them as special tokens. Introducing them as special\ntokens significantly worsened our generation performance compared to the previous approach.\n5\nMulti-Round Multi-Image Exploration\n5.1\nData Blending\nOne critical missing element for enabling multi-round and multi-image conversations is data. The sole source\nof multi-round multi-image data we located is the SparklesDialogue dataset [12], which contains a mere 6520\nsamples. To address this limitation, we employed two methods to synthesize multi-round multi-image data\nfrom existing single-image or single-round data: simple data concatenation and LLaVA-Otter data blending.\n5.1.1\nSimple data concatenation\nFor the \"llava\" and \"llava_dial\" datasets utilized by the LLaVA model, each sample comprises single/multi-\nround conversations for a single image. To simulate scenarios where a user sequentially asks questions about\nmultiple images, we conducted straightforward data post-processing for these two datasets. Specifically,\nwe randomly concatenated different numbers of samples into a single sample. In the case of \"llava,\" we\nconcatenated 1 to 3 samples, while for \"llava_dial,\" we concatenated 1 to 2 samples (refer to Table 1).\n9\n5.1.2\nLLaVA-Otter data blending\nWe noticed that the llava and llava_dial datasets used by LLaVA model and the otter_mimicit_cgd\ndataset used by the Otter model all use the COCO train2017 images. For the llava and llava_dial datasets,\neach sample includes a single/multi-round conversations for a single image. For the otter_mimicit_cgd\ndataset, each sample includes a single-round conversation for a pair of images. This enables us to build a\nsynthesized multi-round multi-image data llava_otter_blend as a more natural blending: for each sample in\nthe otter_mimicit_cgd dataset, we look for llava and llava_dial samples that use the same image, and then\nbuild a new sample in a \"llava/llava_dial conversations then otter_mimicit_cgd conversation\" fashion (as\nshown in Table 1).\n5.2\nResults\n5.2.1\nComparison with QWen-VL and SparklesChat\nExperimental Setting\nWe utilize the datasets (1) to (8) as illustrated in Table 1. We deliberately exclude\nthe dialogue data from SparklesChat to thoroughly investigate whether our newly proposed data blending\ntechnique can facilitate the interweaving of multi-image multi-round chats without the incorporation of new\ndata. LLaMa-2-13B is employed as our language model, and the model is trained over 6 epochs. All other\nsettings remain consistent with those outlined in Section 4.2.\nDemo Results\nWe compare DeepSpeed-VisualChat-13B with QWen-VL and SparklesChat as illustrated\nin Figure 9 and Figure 10. The tasks presented in Figure 9 are unseen to all the trained models. Notably,\nDeepSpeed-VisualChat-13B outperforms in terms of answer quality when compared to the other models.\nSpecifically, while QWen-VL excels at offering succinct and accurate descriptions of individual images, it\nstruggles to recall the first or second images during subsequent questions. On the other hand, SparklesChat\nexcels at discerning differences between images, yet occasionally provides imprecise descriptions of individual\nimages.\nThe tasks in Figure 10 center around narratives. Narratives are the primary training focus of SparklesChat\nand might be a part of QWen-VL\u2019s training data (as its data is proprietary), but they were not part of\nDeepSpeed-VisualChat\u2019s training (i.e., datasets (1) to (8) as mentioned in Table 1). Despite this, DeepSpeed-\nVisualChat continues to provide commendable descriptions of individual images and exhibits some narrative\nskills. In contrast, both QWen-VL and SparklesChat demonstrate superior narrative abilities. Nonetheless,\neach model has its own set of limitations for specific questions.\nIt is worth noting that the training expenditure for DeepSpeed-VisualChat is significantly lower than that\nfor QWen-VL and SparklesChat, with the latter having utilized the pre-training checkpoint of MiniGPT4.\n5.2.2\nResult of DeepSpeed-VisualChat\nExperimental Setting\nThe setting remains the same as mentioned above, with the addition of 6.5K more\nexamples from SparklesChat.\nDemo result\nWe perform comparisons between DeepSpeed-VisualChat-13B with and without incorpo-\nrating data from SparklesChat.\nFor clarity, we will refer to DeepSpeed-VisualChat-13B without (and\nwith) SparklesChat\u2019s data as DeepSpeed-VisualChat-13B-Set1 (-Set2). First and foremost, by integrat-\ning SparklesChat\u2019s data, DeepSpeed-VisualChat demonstrates enhanced narrative capability. Similar to\nSparklesChat, the newly trained model also displays a reduced ability to concentrate on the details of\nindividual images.\nBeyond the aforementioned, the introduction of additional data yields another intriguing observation.\nDeepSpeed-VisualChat-13B-Set2 exhibits increased sensitivity to prompt tuning compared to its predecessor.\nSpecifically, as shown in Figure 12, a slight alteration to the prompt (highlighted in red text) without changing\nthe question\u2019s meaning, leads DeepSpeed-VisualChat-13B-Set2 to provide disparate answers. Conversely, the\n10\nFigure 9: The above example (the conversation is from left to right panel) illustrates the difference among\nDeepSpeed-VisualChat-13B, QWen-VL, and SparklesChat.\nQWen-VL provides considerable short and\naccurate answers on describing the individual image but fails to remember the first image at the last second\nquestion. While SparklesChat is good at interpreting the difference between images but provides inaccurate\ninformation when describing individual images.\noriginal DeepSpeed-VisualChat-13B-Set1 tends to offer more congruent responses. We hypothesize that this\nheightened sensitivity results from an imbalance in question formats/templates introduced by SparklesChat\u2019s\ndataset.\nFor cross-comparison between DeepSpeed-VisualChat and QWen-VL/SparklesChat, please refer to Fig-\nure A.1 and Figure A.2.\n5.2.3\nDeepSpeed-VisualChat with LLaMa-2-70B\nWe have initiated training with LLaMa-2-70B, maintaining the same training settings as outlined in Sec-\ntion 5.2.2. However, the resulting model is not adequately trained. We conjecture that the hyper-parameters\noptimal for LLaMa-2-13B may not be suitable for LLaMa-2-70B; for instance, the learning rate might be ex-\ncessive, and the number of training epochs insufficient. Perfecting the training of DeepSpeed-VisualChat-70B\nis earmarked for future work.\n11\nFigure 10: The given example, displayed in a left-to-right panel conversation format, demonstrates the\ndisparities among DeepSpeed-VisualChat-13B, QWen-VL, and SparklesChat. QWen-VL excels in delivering\nsuccinct and accurate responses when describing individual images. Conversely, SparklesChat elaborates on\nthe first two images but inaccurately identifies the scene as being from \"Winnie\" although later correctly\nattributing it to \"Zootopia\". When it comes to narrative skills, both QWen-VL and SparklesChat exhibit\nproficiency in story-telling, a skill that our model lacks. This deficiency can be attributed to the absence of\nnarrative-centric content within the training data utilized for this model. In Figure 11, we will compare the\nmodel trained with and without narrative-centric content.\n12\nFigure 11: The above example (the conversation is from left to right panel) illustrates the differences\nbetween DeepSpeed-VisualChat-13B-Set1 and DeepSpeed-VisualChat-13b-Set2. We see that Set1 is better in\ndescribing the individual images (for example, Set1 can recognize the fox and \"next window\") but do not\nhave ability to tell a story. While Set2 lost some ability to focus on the details of the images but can tell a\nstory based on the given images.\n13\nFigure 12: The given example, displayed in a left-to-right panel conversation format, demonstrates the\ndisparities between DeepSpeed-VisualChat-13B-Set1 and DeepSpeed-VisualChat-13B-Set2, given a slightly\ndifferent prompt series.\n5.2.4\nOther Learning\n\u2022 Exploration of Projection Layers: We experimented with two different projection layers to bridge\nvisual encoders and LLMs: a single linear layer and a Vision Transformer layer. We did not observe any\nbenefits from the Vision Transformer approach in the preliminary phase, so we decided not to pursue\nthis route further.\n\u2022 Advanced Data Blending Techniques: We explored more intricate data blending methods, such as\nshuffling the image ID of the Otter and LLaVA datasets. For example, in the Otter dataset, the paired\nimages were later referenced as the first and third images by inserting another image as the second one.\nHowever, our experiments led to deteriorated performance, characterized by incomplete sentences and\nincorrect references. Upon reviewing the data, we hypothesized that these issues were probably due to\nincorrect references in the training data during the data blending process.\n6\nLimitations and Conclusions\nLimitations\nGiven that the focus of this work is not on benchmarking evaluations, we abstained from\nincorporating any such results. This might have resulted in the demonstrations illustrated in the paper\nappearing biased and not being comprehensive. Additionally, we have observed that data is a pivotal\n14\ncomponent to achieve high-quality LVLMs, but we were unable to provide such datasets due to constraints\non resources. We acknowledge that larger language models can potentially offer superior model quality, but\nwe encountered difficulties in training a model based on LLaMa-2-70B. Attempts were made to train with\nthe LLaMa-2-70B model, but the end results were suboptimal. We also noted the hallucination issue with\nDeepSpeed-VisualChat, a problem inherited from the LLaMa-2 family.\nConclusions\nIn conclusion, We propose DeepSpeed-VisualChat, the Multi Round and Multi Images Chat\nframework, a pioneering solution offering open-source support for multi-image dialogues and featuring an\ninnovative multi-modal casual attention mechanism and efficient data blending techniques. Our approach\nsurpasses contemporaneous models in scalability enabling enhanced adaptability in diverse interactive scenarios,\nwithout incurring additional training costs or complexity. This breakthrough facilitates unprecedented\nadvancements in large vision and language models, laying a robust foundation for the seamless integration of\nvisual and textual information in future multi-modal models.\nContributions\nZY: Full engagement and project lead. XW: Training/evaluation pipeline development and writing. CL:\nData support. MZ: Training pipeline support. QH: DeepSpeed ZeRO feature adaptation. OR: DeepSpeed\nZeRO feature adaptation. AAA: Software support. SR: Consulting. YH: Team lead.\nAcknowledgment\nWe thank the entire DeepSpeed team for their support.\nReferences\n[1] Open AI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew\nBrock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning.\nCoRR, abs/2204.14198, 2022.\n[3] ChatLLaMa Authors. Chatllama. https://github.com/juncongmoo/chatllama, 2023.\n[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for\ntraining large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.\n[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint\narXiv:2308.12966, 2023.\n[6] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit\nSom, Songhao Piao, and Furu Wei. Vlmo: Unified vision-language pre-training with mixture-of-modality-\nexperts. In Advances in Neural Information Processing Systems, 2022.\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n15\n[8] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al.\nPali: A jointly-scaled multilingual\nlanguage-image model. arXiv preprint arXiv:2209.06794, 2022.\n[9] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023.\n[10] GitHub. Github copilot. https://github.com/features/copilot/, 2021.\n[11] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei\nZhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with\nhumans. arXiv preprint arXiv:2305.04790, 2023.\n[12] Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong Lu. Sparkles: Un-\nlocking chats across multiple images for multimodal instruction-following models.\narXiv preprint\narXiv:2308.16463, 2023.\n[13] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori,\nAchal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and\nLudwig Schmidt. Openclip, July 2021. If you use this software, please cite it as below.\n[14] Shima Imani, Liang Du, and Harsh Shrivastava. MathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37\u201342, Toronto, Canada, July 2023. Association for\nComputational Linguistics.\n[15] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with\nnoisy text supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, pages 4904\u20134916. PMLR, 2021.\n[16] Wonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision-and-language transformer without convolution\nor region supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, pages 5583\u20135594. PMLR, 2021.\n[17] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning\nproblems with language models. Advances in Neural Information Processing Systems, 35:3843\u20133857,\n2022.\n[18] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and\nZiwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023.\n[19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,\n2023.\n[20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In ICML, pages 12888\u201312900. PMLR, 2022.\n[21] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. Align before fuse: Vision and language representation learning with momentum distillation. In\nAdvances in neural information processing systems, volume 34, pages 9694\u20139705, 2021.\n16\n[22] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training\nfor vision-language tasks. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,\neditors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020,\nProceedings, Part XXX, volume 12375 of Lecture Notes in Computer Science, pages 121\u2013137. Springer,\n2020.\n[23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023.\n[24] Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, and Chunyuan Li. Learning\ncustomized visual models with retrieval-augmented knowledge. CVPR, 2023.\n[25] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n[26] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining task-agnostic visiolinguistic\nrepresentations for vision-and-language tasks. In Advances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pages 13\u201323, 2019.\n[27] OpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2022.\n[28] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[29] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors,\nProceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR, 2021.\n[31] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations\nenable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505\u20133506, 2020.\n[32] Sheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei Yang, Pengchuan Zhang, Zhe Gan, Lijuan\nWang, Lu Yuan, Ce Liu, et al. K-lite: Learning transferable visual models with external knowledge. In\nAdvances in Neural Information Processing Systems, 2022.\n[33] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao,\nand Kurt Keutzer. How much can clip benefit vision-and-language tasks? In ICLR, 2022.\n[34] Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong He.\nScaling\nvision-language models with sparse mixture of experts. arXiv preprint arXiv:2303.07226, 2023.\n[35] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. FLAVA: A foundational language and vision alignment model. CoRR,\nabs/2112.04482, 2021.\n[36] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: pre-training of\ngeneric visual-linguistic representations. In 8th International Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n17\n[37] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to\ninstruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.\n[38] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers.\nIn Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages\n5099\u20135110. Association for Computational Linguistics, 2019.\n[39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[40] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou,\nJingren Zhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. CoRR, abs/2202.03052, 2022.\n[41] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit\npretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.\n[42] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. SimVLM: Simple\nvisual language model pretraining with weak supervision. In ICLR, 2022.\n[43] Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded\nmathematical proof generation with language models. Advances in Neural Information Processing\nSystems, 35:4913\u20134927, 2022.\n[44] Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ah-\nmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt,\nMolly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, and\nYuxiong He. DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at\nAll Scales. arXiv preprint arXiv:2308.01320, 2023.\n[45] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. CoRR, abs/2205.01917, 2022.\n[46] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu,\nXuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan\nWang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan\nZhang. Florence: A new foundation model for computer vision. CoRR, abs/2111.11432, 2021.\n[47] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and\nJianfeng Gao. VinVL: Revisiting visual representations in vision-language models. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 5579\u20135588.\nComputer Vision Foundation / IEEE, 2021.\n[48] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\nA\nMore examples\n18\nFigure A.1: The given example, displayed in a left-to-right panel conversation format, demonstrates the\ndisparities among DeepSpeed-VisualChat-13B-Set1 and DeepSpeed-VisualChat-13B-Set2, QWen-VL, and\nSparklesChat.\n19\nFigure A.2: The given example, displayed in a left-to-right panel conversation format, demonstrates the\ndisparities among DeepSpeed-VisualChat-13B-Set1 and DeepSpeed-VisualChat-13B-Set2, QWen-VL, and\nSparklesChat.\n20\n"
  },
  {
    "title": "Small-scale proxies for large-scale Transformer training instabilities",
    "link": "https://arxiv.org/pdf/2309.14322.pdf",
    "upvote": "17",
    "text": "Small-scale proxies for large-scale Transformer training instabilities\nMitchell Wortsman\nPeter J. Liu\nLechao Xiao\nKatie Everett\nAlex Alemi\nBen Adlam\nJohn D. Co-Reyes\nIzzeddin Gur\nAbhishek Kumar\nRoman Novak\nJeffrey Pennington\nJascha Sohl-dickstein\nKelvin Xu\nJaehoon Lee*\nJustin Gilmer*\nSimon Kornblith*\nGoogle DeepMind\nAbstract\nTeams that have trained large Transformer-based mod-\nels have reported training instabilities at large scale\nthat did not appear when training with the same\nhyperparameters at smaller scales.\nAlthough the\ncauses of such instabilities are of scientific interest,\nthe amount of resources required to reproduce them\nhas made investigation difficult.\nIn this work, we\nseek ways to reproduce and study training stability\nand instability at smaller scales. First, we focus on\ntwo sources of training instability described in pre-\nvious work: the growth of logits in attention layers\n(Dehghani et al., 2023) and divergence of the output\nlogits from the log probabilities (Chowdhery et al.,\n2022). By measuring the relationship between learn-\ning rate and loss across scales, we show that these\ninstabilities also appear in small models when training\nat high learning rates, and that mitigations previously\nemployed at large scales are equally effective in this\nregime. This prompts us to investigate the extent to\nwhich other known optimizer and model interventions\ninfluence the sensitivity of the final loss to changes\nin the learning rate. To this end, we study meth-\nods such as warm-up, weight decay, and the \u00b5Param\n(Yang et al., 2022), and combine techniques to train\nsmall models that achieve similar losses across orders\nof magnitude of learning rate variation. Finally, to\nconclude our exploration we study two cases where\ninstabilities can be predicted before they emerge by\nexamining the scaling behavior of model activation\nand gradient norms.\n1\nIntroduction\nScaling up transformers has led to remarkable progress\nfrom chat models to image generation. However, not\n*Equal contribution.\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.50\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\nFinal eval loss\nqk-layernorm = True\nqk-layernorm = False\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\nN = 1.2e+09\n107\n108\n109\nNumber of parameters\n10\n2\n10\n1\n100\nLR sensitivity\nFigure 1: Qk-layernorm [11] enables stable training across\nthree orders of magnitude of learning rate (LR) variation.\n(Top) For transformers with N parameters, we plot the\neffect of learning rate on final evaluation loss. (Bottom)\nWe use LR sensitivity to summarize the top plot. LR sensi-\ntivity measures the expected deviation from optimal when\nvarying learning rate across three orders of magnitude.\nQk-layernorm reduces LR sensitivity, but LR sensitivity\nstill increases with model scale.\n1\narXiv:2309.14322v2  [cs.LG]  16 Oct 2023\nevery training run is successful. When training large\nTransformers, researchers have reported instabilities\nwhich slow or destabilize learning [6, 11, 53, 35, 8]. As\nthe resources required for large runs continue to grow,\nit is important to examine the ways that Transformer\ntraining can fail.\nIn this report we reproduce, study, and predict train-\ning instability in Transformer models. We find that\nmeasuring the relationship between learning rate and\nloss across scales is a useful tool to identify instability\n(e.g., Figure 1). Therefore, we introduce learning rate\n(LR) sensitivity, which serves as a useful summary\nstatistic for learning rate vs. loss curves. LR sensitiv-\nity measures the deviation from optimal performance\nwhen varying LR across orders of magnitude.\nWe show that two sources of instability, which have\npreviously been described at scale, can be reproduced\nin small Transformers.1\nThis enables their study\nwithout access to large resource pools.\nIn partic-\nular, we examine the growth of logits in attention\nlayers [11, 16, 51] and divergence of the output logits\nfrom the log probabilities [6]. As evident from the\nlearning rate vs. loss curves and by inspecting model\ncharacteristics, both instabilities appear at high learn-\ning rates in small models. Moreover, interventions\nwhich have previously been employed at scale are\nalso successful in this regime (e.g., Figure 1). These\ninterventions\u2014qk-layernorm [11]2 and z-loss regular-\nization [6]\u2014reduce LR sensitivity and enable success-\nful training across three orders of magnitude of LR\nvariation.\nThese observations raise the question of how other\nknown optimizer and model interventions affect the\nshape of the learning rate vs. loss curves across scales.\nTherefore, we study the effect of techniques such as\nwarm-up, weight decay, and \u00b5Param [50] in this con-\ntext. When employing qk-layernorm and z-loss regu-\nlarization, these other techniques usually have little\nimpact on the range of learning rates at which mod-\nels can be stably trained, but do affect the sensitiv-\nity to learning rate within this range. In line with\nprevious work, we find that longer warm-up reduces\nlearning rate sensitivity, as does the independent scal-\ning of learning rate and weight decay recommended\nby Loshchilov and Hutter [33]. One interesting find-\ning is that scaling depth increases LR sensitivity at a\n1We focus on instabilities which lead to slow divergence, not\nloss spikes (see Section 4).\n2Based off currently unpublished investigations of Gilmer\net al. [16].\nfaster rate than scaling width.\nThe remainder of our investigation centers on the\nscaling behavior for model characteristics such as acti-\nvation and gradient norms. Using the attention logit\ngrowth instability as an example, we show that it is\npossible to predict an instability before it emerges.\nThis is in contrast to prior works on scaling which pri-\nmarily focus on scaling trends related to loss [27, 22].\nWe conclude by using the scaling behavior of model\ncharacteristics to search for instabilities that are cur-\nrently not well documented. Our investigation shows\nthat gradient norms decrease with both scale and\nlearning rate, such that the default AdamW [33] ep-\nsilon hyperparameter is too large. This causes updates\nthat are too small. We connect this phenomenon and\nthe attention logit growth instability to parameter\nnorm growth [34, 29].\nOverall, we believe our work presents new scientific\nopportunities for studying training stability without\naccess to large resource pools.\n2\nExperimental methodology\nThis section details our experimental set-up (Sec-\ntion 2.1) and useful tools employed by our analysis:\n(i) measuring the relationship between learning rate\nand loss across scales (Section 2.2) and (ii) examining\nscaling trends for model characteristics (Section 2.3).\n2.1\nExperimental set-up\nWe train small Transformer models [45] with a similar\nexperimental set-up as GPT-2 [38] implemented in\nFlax [20]: the models are decoder-only [31] and trained\nwith an auto-regressive loss (refer to Section A for\nmore infrastructure details). While we experimentally\nmanipulate many of the following hyperparameters,\nthis section provides their default values, which we\nuse unless otherwise specified.\nBy default, we use AdamW [33] with \u03b21 = 0.9,\n\u03b22 = 0.95, \u03f5 = 1e-8, and gradient clipping at global\nnorm 1. The default warmup is 5e3 steps, and the\ndefault number of total steps is 1e5. We use a linear\nschedule for warmup and and a cosine-decay [32] sched-\nule for the remainder, with minimum learning rate\n1e-5. We use an independent weight decay of 1e-4 and\nauxiliary z-loss [6] with coefficient 1e-4. Sections 3.2.2\nand 3.1.2 respectively provide additional information\nand ablations on decoupled weight decay and z-loss.\n2\nWe use pre-normalization [38] Transformers with qk-\nlayernorm [11] (see Section 3.1.1 for information). We\ndo not use any biases following Chowdhery et al. [6],\nand the layernorm [1] \u03f5 remains at the default value in\nFlax [20] of 1e-6. We jointly scale up the embedding\nsize, depth, and number of heads when scaling param-\neters. We do not use weight tying of the first and last\nlayer [37], and when reporting the number of parame-\nters we exclude the embedding and head (as in Kaplan\net al. [27]). We use rotary positional embeddings [43],\nand for training data we use C4 [39]. Letting d refer\nto the model dimension (i.e., the embedding size), the\nfeed-forward component of the Transformer is an MLP\nwith hidden dimension of 4d and gelu [21] activations.\nAs in Vaswani et al. [45] we use factor 1/\n\u221a\nd scaling\nin the self-attention. The embedding initialization\nis the default in Flax, which is normally distributed\nwith standard deviation 1/\n\u221a\nd. The remainder of the\nweights are initialized with a truncated normal distri-\nbution with inverse root fan-in standard deviation [18].\nThe default batch size is 256, where each batch ele-\nment has a sequence length of 512 tokens. Sequences\nare packed so that no padding is required. Finally,\nwe use the vocabulary from Raffel et al. [40] which\nhas size 32101 and uses a SentencePiece [28] tokenizer.\nWe train on TPUs [26] in bfloat16 precision using\nFlax [20] and JAX [4].\n2.2\nLR vs.\nloss curves and learning\nrate sensitivity\nTo investigate how model instability emerges with\nscale, it is useful to plot the relationship between\nlearning rate (LR) and loss for models of different sizes.\nFor instance, an instability is often characterized by\nan explosion in the loss at high learning rates. LR\nvs. loss curves can reveal how the lowest unstable\nlearning rate changes as a function of model size.\nTo summarize LR vs. loss curves, we use LR sensi-\ntivity. LR sensitivity measures the deviation in final\nvalidation loss from optimal when sweeping LR across\nthree orders of magnitude. If a model fails to train\nat high learning rates, then LR sensitivity will be\nhigh. There are cases where LR vs. loss curves and\nLR sensitivity are no longer meaningful, for instance\nif an intervention changes the meaning of learning\nrate\u2014see Appendix B for a detailed discussion.\nLet \u03b8 = A(\u03b7) denote the model weights \u03b8 ob-\ntained when training with learning rate \u03b7, and let\n\u2113(\u03b8) denote the validation loss when using weights\n\u03b8.\nFor a learning rate range [a, b], let \u2113\u2217 denote\nthe loss obtained with the best learning rate, i.e.,\n\u2113\u2217 = min\u03b7\u2208[a,b] \u2113 (A(\u03b7)). Moreover, let \u21130 denote loss\nat initialization. Then, LR sensitivity is defined as\nE\u03b7\u2208[a,b] [min (\u2113 (A (\u03b7)) , \u21130) \u2212 \u2113\u2217].\nUnless otherwise mentioned, we use the learning rate\nrange 3e-4 to 3e-1 with AdamW [33] to measure LR\nsensitivity, where LR refers to the maximum value in a\ncosine decay schedule with warm-up [32]. We consider\nLRs in {3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1} when\ncomputing the minimum and expectation.\n2.3\nScaling trends for model character-\nistics\nTo study instability, we also find it useful to examine\nscaling trends for model characteristics such as gradi-\nent or activation norms. This method is helpful for\npredicting instabilities and contrasts with previous\nwork on scaling, which primarily focuses on trends\nrelating model scale and loss [27, 22].\n3\nResults\nThis section presents our results on training stability\nfor small Transformers. Equipped with LR sensitivity\n(Section 2.2), we study two known instabilities and\ntheir corresponding mitigation at small scale (Sec-\ntion 3.1). This raises the question of how other model\nand optimizer interventions effect sensitivity of final\nloss to learning rate, which we investigate in Sec-\ntion 3.2. Finally, we examine whether instabilities can\nbe reliably predicted before they emerge: Section 3.3\npredicts when the logit growth instability may cause\ndivergence in a larger model, while Section 3.4 aims\nto find other issues that may occur when scaling up\nwith our default hyperparameters.\n3.1\nReproducing two known instabili-\nties at small scale\nHere, we examine two instabilities that have previ-\nously been described at scale: the growth of logits in\nattention layers [11, 16, 51] and divergence of the out-\nput logits from the log probabilities [6]. By examining\nLR vs. loss curves, we show that these instabilities\ncan be reproduced in small models by using high learn-\ning rates and that mitigations employed at scale are\neffective in this regime.\n3\n0\n2000\n4000\n6000\n8000 10000\nStep\n3\n4\n5\n6\n7\n8\nLoss\n0\n2000\n4000\n6000\n8000 10000\nStep\n101\n102\n103\n104\n105\n106\nMax attention logit\n0\n2000\n4000\n6000\n8000 10000\nStep\n3\n4\n5\n6\n7\n8\nLoss\nqk-layernorm = True\nqk-layernorm = False\n0\n2000\n4000\n6000\n8000 10000\nStep\n101\n102\n103\n104\n105\n106\nMax attention logit\nNum params = 9.4e6, LR = 0.1\nNum params = 4.8e9, LR = 0.01\nFigure 2: The attention logit growth instability [11, 51]\nappears in small models at high learning rates. The mit-\nigation of applying qk-layernorm proposed by Dehghani\net al. [11] is equally effective in the small-scale regime.\nThe max attention logit is reported for layer 0, which we\ntypically observe to have the largest logit values.\n3.1.1\nAttention logit growth\nResearchers have previously documented that Trans-\nformer training fails when the attention logits be-\ncome large [11, 51]. In Dehghani et al. [11], this issue\nemerged when training a ViT model [14] with 22 bil-\nlion parameters.\nIn the self-attention layer of a Transformer [45],\nqueries qi and keys ki are combined to compute the\nattention logits zij = \u27e8qi, kj\u27e9/\u221adh, where dh is the\nhead dimension. Next, the attention logits are passed\nthrough a softmax to produce attention weights, which\nare used to combine values vi. Dehghani et al. [11] ob-\nserved that the attention logits z became large, which\nthey refered to as attention logit growth. As a re-\nsult, the attention weights collapse to one-hot vectors,\nwhich was named attention entropy collapse by Zhai\net al. [51]. To resolve this issue, Dehghani et al. [11]\nproposed qk-layernorm, which applies LayerNorm [1]\nto the queries and keys before computing the attention\nlogits.\nIn our experiments, we find that models need not be\nlarge to exhibit instability related to attention logit\ngrowth. As shown in Figure 1, the maximum learning\nrate at which small models can be trained increases\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\n4.4\nFinal eval loss\nz-loss coefficient 1e-4\nNo z-loss\nWeight decay 1e-4\nNo weight decay\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\n107\nNumber of parameters\n10\n1\n100\nLR sensitivity\nFigure 3: The effect of the output logit divergence instabil-\nity [6] and the z-loss mitigation [6] (Section 3.1.2). Models\nin this experiment have qk-layernorm [11].\nwhen using qk-layernorm. Without qk-layernorm, the\nlearning rate at which models diverge becomes smaller\nwith increasing model size. By contrast, models with\nqk-layernorm exhibit considerably lower LR sensitivity\nand train to low loss at high learning rates. As a\nhighlight, qk-layernorm allows training a model with\n1.2B parameters at learning rate 0.3. Both with and\nwithout qk-layernorm, LR sensitivity increases with\nscale.\nFigure 2 displays the loss and max attention logit\nfor two model scales that differ by three orders of\nmagnitude. In both cases, the loss diverges without\nqk-layernorm. Our results in Appendix Figure E.1\nsuggest that attention logit growth is due to growth\nin the queries and keys, not due to an increase in\ntheir alignment. Instead, we hypothesize this insta-\nbility could result from the quadratic dependence of\nattention logits on parameter norms.\n4\n0\n50000\n100000\nStep\n3.6\n3.8\n4.0\n4.2\n4.4\nEval loss\nz-loss coefficient 1e-4\nNo z-loss\nWeight decay 1e-4\nNo weight decay\n0\n50000\n100000\nStep\n100\n80\n60\n40\n20\n0\nOutput logit mean\nFigure 4: An example of the output logit divergence insta-\nbility [6] (Section 3.1.2) in a 2.4M parameter Transformer\nat learning rate 0.1.\n3.1.2\nOutput logit divergence\nAnother instability reported by researchers training\nlarge models is divergence in the output logits from\nthe log probabilities [6]. Just as before, we reproduce\nthis instability with small models at large learning\nrates, and the proposed mitigation ameliorates the\nissue. Overall, Figure 3 summarizes the effect.\nLet y denote the model\u2019s output logits, which are\nused to compute class probabilities pi via a softmax\npi = eyi/Z where Z = P\nj eyj. This instability occurs\nwhen the logits diverge and become very negative, as\nillustrated in Figure 4 for a 2.4M parameter model at\nlearning rate 0.1. In contrast to the attention logit\ngrowth instability, this divergence occurs towards the\nend of training. The mitigation proposed by Chowdh-\nery et al. [6] is to encourage log Z to remain close to\nzero. They add an auxiliary loss log2 Z, referred to\nas z-loss, with coefficient 1e-4.\nAs illustrated in Figures 3 and 4, we find that instabil-\nity related to output logit divergence occurs in models\nwith no weight decay regardless of scale, and z-loss\nresolves this instability. Weight decay also mitigates\nthis instability for the larger models we test.\n3.2\nMeasuring\nthe\neffect\nof\nother\nknown interventions\nThe previous section used the relationship between\nlearning rate and loss as a useful tool for examining\ntwo known instabilities and their mitigation. This\nraises the question of how other known model and\noptimizer interventions affect the shape of LR vs. loss\ncurves across scales. In particular, can LR sensitivity\nhelp identify additional issues or resolutions when\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\nFinal eval loss\nWarmup = 50\nWarmup = 500\nWarmup = 5000\nWarmup = 10000\nWarmup = 25000\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n107\n108\nNumber of parameters\n10\n1\n2 \u00d7 10\n2\n3 \u00d7 10\n2\n4 \u00d7 10\n2\n6 \u00d7 10\n2\nLR sensitivity\nFigure 5: The effect of warm-up length for different model\nsizes. Longer warm-up reduces LR sensitivity and loss,\nespecially for the larger models we test. Models in this\nexperiment use qk-layernorm [11].\nscaling? This section aims to answer this question for\ncommon techniques such as warm-up, weight decay,\nand \u00b5Param [50].\n3.2.1\nWarm-up\nAs illustrated by Figure 5, a longer warm-up period\nreduces LR sensitivity. This is most clear for the larger\nmodels, which are not stable at LR 3e-1 without long\nwarm-up. The number of total steps is fixed to 1e5 in\nthis experiment, and all models use qk-layernorm. The\nimportance of warm-up for stability has previously\nbeen highlighted [17, 42, 30], although these works do\nnot measure scaling behavior.\n3.2.2\nIndependent weight decay\nParameterizing weight decay independently of learn-\ning rate reduces LR sensitivity, as illustrated in Fig-\nure 6. While this was recommended by Loshchilov\nand Hutter [33], it is not common practice in the\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\nFinal eval loss\nIndependent decay = True\nIndependent decay = False\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n107\n108\nNumber of parameters\n10\n1\n2 \u00d7 10\n2\n3 \u00d7 10\n2\n4 \u00d7 10\n2\n6 \u00d7 10\n2\nLR sensitivity\nFigure 6: Independently scaling LR without also scaling\nweight decay reduces LR sensitivity. While this was recom-\nmended by Loshchilov and Hutter [33], it is not common\npractice in the default AdamW implementations in popu-\nlar libraries. Refer to Section 3.2.2 for more information.\nModels in this experiment use qk-layernorm [11].\ndefault AdamW implementations of PyTorch [36] or\nOptax [2]. We explain the differences below.\nFor parameters \u03b8, let \u2206 = v/ (\u221au + \u03f5) denote the\nAdamW update without learning rate or weight decay.\nFor weight decay coefficient \u03bb, max learning rate \u03b7,\nand schedule st \u2208 [0, 1], Loshchilov and Hutter [33]\nrecommend the update \u03b8 \u2190 \u03b8\u2212st(\u03b7\u2206\u2212\u03bb\u03b8), which we\nrefer to as independent decay. On the other hand, the\ndefault implementation in PyTorch or Optax applies\nthe update \u03b8 \u2190 \u03b8 \u2212 st\u03b7(\u2206 \u2212 \u03bb\u03b8), i.e., \u03b7 now scales\nboth terms.\nWhen reporting LR sensitivity without independent\ndecay in Figure 6, we report the minimum LR sensi-\ntivity over ranges [1e-4, 1e-1] and [3e-4, 3e-1] because\nthe former is sometimes better centered on the min-\nimum. The default setting in this paper is to use\nindependent decay. When using independent decay\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\nFinal eval loss\nScaling width\nScaling depth\ndim = 256\ndim = 512\ndim = 768\ndim = 1024\ndim = 2048\nlayers = 3\nlayers = 6\nlayers = 12\nlayers = 24\nlayers = 48\nlayers = 96\n107\n108\nNumber of parameters\n10\n1\n2 \u00d7 10\n2\n3 \u00d7 10\n2\n4 \u00d7 10\n2\n6 \u00d7 10\n2\nLR sensitivity\nScaling width\nScaling depth\nFigure 7: Independently scaling depth increases LR sen-\nsitivity at a faster rate than scaling width, though also\nproduces a model with lower loss at the largest scale we\ntest. Refer to Appendix Figure E.2 for this experiment\nwithout qk-layernorm.\nwe set \u03bb=1e-4, and without independent decay we set\n\u03bb=0.1. A sweep on weight decay values is conducted\nin Figure E.10.\n3.2.3\nScaling width vs. depth\nWe have so far consistently observed that increasing\nthe number of parameters increases LR sensitivity. We\nnow examine which part of scaling is most responsible.\nOur results, illustrated by Figure 7, indicate that scal-\ning depth increases LR sensitivity at a faster rate\nthan scaling width. However, at the largest scale we\ntest, independently scaling depth produces a model\nwith lower validation loss. A validation loss compar-\nison between width scaling, depth scaling, and joint\nscaling is in Appendix Figure E.3. The standard prac-\ntice of joint scaling performs best at the largest scale\nand also has a more reliable scaling prediction when\nextrapolating.\n6\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.50\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\nFinal eval loss\nStandard\nMuParam (simple)\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\nN = 1.2e+09\n107\n108\n109\nNumber of parameters\n2 \u00d7 10\n2\n3 \u00d7 10\n2\n4 \u00d7 10\n2\n6 \u00d7 10\n2\nLR sensitivity\nStandard\nMuParam (simple)\nFigure 8: Measuring the effect of \u00b5Param on LR sensi-\ntivity for models with qk-layernorm [11]. In our setting\n\u00b5Param succeeds in stabilizing the optimal LR, though it\ndoes not improve loss or reduce LR sensitivity. For more\ninformation refer to Section 3.2.4.\nWhen scaling depth, we use d = 512, and when scaling\nwidth, we use 6 layers. The number of heads is scaled\nproportionally with width, so that the head dimension\nremains the same.\nFigure E.2 repeats this experiment without qk-\nlayernorm, finding that the attention logit growth\ninstability occurs more frequently at scale regardless\nof whether width or depth are scaled.\n3.2.4\n\u00b5Param\nYang and Hu [49] introduced the \u00b5Param method for\nparameterizing a neural network. As a product, the\noptimal LR remains consistent when scaling model\nwidth [50]. This section tests the effect of \u00b5Param\non LR sensitivity, and examines whether \u00b5Param\nalleviates the need for qk-layernorm [11].\nAs illustrated by Figure 8, \u00b5Param does succeed in\nstabilizing the optimal LR at the scale we test. How-\never, \u00b5Param does not improve loss or reduce LR\nsensitivity in our experiments. Appendix Figure E.4\nrepeats this experiment without qk-layernorm. Our\nresults indicate that \u00b5Param does not alleviate the\nneed for this intervention at high learning rates. We\nnote that from a practical perspective, reducing LR\nsensitivity is not important if the optimal LR does\nnot change.\nWe refer to the variant of \u00b5Param that we use in these\nexperiments as \u00b5Param (simple) because it maintains\nonly the core feature of \u00b5Param. We add additional\nfeatures from Yang et al. [50] in Appendix Figure E.5\nwithout measurable improvement at the largest scale\nwe test. For \u00b5Param (simple) we make the following\nchanges from our standard baseline: scale the LR for\nlinear layers by base-fan-in/fan-in. For \u00b5Param (full)\nthere are three additional changes: (i) initialize the\nhead with standard deviation\n\u221a\nbase-fan-in/fan-in; (ii)\nchange the 1/\u221adh scaling factor in attention layers\nto 1/dh where dh is the head dimension; and (iii)\ninitialize the query projection weights with zeros. For\nbase-fan-in we use the fan-in values for the smallest\nmodel we test, which has width 256.\nWe comment briefly on the aforementioned changes (ii)\nand (iii). First, we ablate on change (ii) in isolation in\nAppendix Figure E.6. While this intervention reduces\nloss slightly at the smallest scale we test, the reverse\nis true for the largest scale we test. Also, removing\nthe square root from the scaling factor in attention\nlayers does not alleviate the need for qk-layernorm.\nFinally, with regards to change (iii), we note that in\npreliminary experiments this change had no noticeable\neffect.\n3.2.5\nAdditional interventions\nThis section recreates the previous plots with addi-\ntional interventions or hyperparameter changes. Cor-\nresponding figures are displayed in the appendix.\n\u2022 Changing the number of training steps from 1e5\nto 5e4 or 2e5 does not meaningfully change LR\nsensitivity (Appendix Figure E.7).\n\u2022 We try applying qk-layernorm across the whole\nmodel dimension instead of individually per-head\nwith shared parameters. As illustrated in Ap-\npendix Figure E.8, the latter performs better.\nWe use per-head qk-layernorm as the default in\nall other experiments.\n\u2022 Increasing the batch size from 256 to 512 or 1024\n7\n107\n108\n109\nNum params\n102\n104\n106\nMax attention logit\nLR = 0.0001\nqk-layernorm = True\nqk-layernorm = False\nSuccessful\nInstability\nQuadratic fit\nExtrapolated trend\nAdditional experiment to\nvalidate predicted instability\n107\n108\n109\nNum params\n101\n102\n103\n104\n105\n106\nLR = 0.0003\n107\n108\n109\nNum params\n101\n102\n103\n104\n105\n106\nLR = 0.003\n107\n108\n109\nNum params\n101\n102\n103\n104\n105\n106\nLR = 0.01\n107\n108\n109\nNum params\n101\n102\n103\n104\n105\n106\nLR = 0.03\n107\n108\n109\nNum params\n101\n102\n103\n104\n105\n106\nLR = 0.1\nFigure 9: Predicting the attention logit growth instability via scaling behavior of model characteristics. We extrapolate\nto predict that a larger model will become unstable at LR 1e-2, and run an experiment to confirm the prediction.\nRefer to Section 3.3 for more information.\ndoes not meaningfully change LR sensitivity (Ap-\npendix Figure E.9, each batch element contains\n512 tokens). When increasing batch size we de-\ncrease the number of training steps so that the\namount of data seen is constant. We believe a\nsimilar effect would be observed if instead we held\nthe number of steps constant because changing\nthe number of steps has no impact on LR sensi-\ntivity at batch size 256 (Appendix Figure E.7).\n\u2022 The effect of changing the weight decay from 1e-4\nis illustrated in Figure E.10. Increasing decay\nappears to slightly shift the optimal LR right.\n\u2022 We find that the logit growth instability is not\ndue to the softmax in the self-attention layer, as it\nstill occurs with a pointwise variant of attention\n(Appendix Figure E.11).\n3.3\nPredicting attention logit growth\ninstability from scaling behavior of\nmodel characteristics\nA central question when studying instabilities is\nwhether they can be predicted.\nWe now examine\nwhether it is possible to predict the logit growth in-\nstability before it occurs.\nWe track the attention\nlogit maximums across model scales and fit a curve\nto the data. We use this to predict that a 4.8B pa-\nrameter model will be unstable at LR 1e-2 without\nqk-layernorm and run an experiment to confirm this\nprediction.\nFigure 9 plots the number of parameters vs. max\n100\n101\n102\n103\n104\nScaling constant \n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nFinal loss\nFixed scale\nZero layer baseline\nFigure 10: Enforcing a max attention logit of approxi-\nmately \u03ba in a small model to determine which value of \u03ba\ninhibits learning.\nattention logit at different learning rate values.3 At\neach learning rate, we fit a quadratic to predict how\nthe max attention logit will change with model scale.\nWe first noticed that all points with attention logits\nabove 1e4 diverged. Moreover, the quadratic fit pre-\ndicted that for LR 1e-2 the next model scale would\nalso cross that value. Based on this prediction, we\ntrained a new 4.8B parameter model at LR 1e-2. This\nmodel diverged as predicted. Not only do we pre-\ndict the divergence, but our fit closely extrapolates to\npredict the value of the max attention logit.\n3We use block 0, which typically has the largest logits, and\nconsider the value at step 2e3.\nMuch earlier than 2e3 was\nuninformative, and much later the unstable points had long\npast diverged.\n8\n107\n108\n109\nNum params\n10\n8\n10\n7\n10\n6\n10\n5\n10\n4\nGrad RMS\n10\n3\n10\n2\n10\n1\nLR\n10\n8\n10\n7\n10\n6\n10\n5\n10\n4\n0\n5\n10\n15\n20\nBlock index\n10\n9\n10\n8\n10\n7\n0.0003\n0.3\nLR\n9.4e6\n1.2e9\nNum params\n9.4e6\n1.2e9\nNum params (at LR 3e-1)\nFigure 11: Predicting a potential instability from the scaling behavior of model characteristics. The gradient root\nmean square (RMS) decreases with num params (left) and learning rate (middle).\nThese trends indicate that\nhyperparameter adjustment may be required to successfully scale further, as the RMS is approaching the default\nAdamW \u03f5 hyperparameter. If the gradient RMS becomes too small without adjusting \u03f5 or weight decay, a layer may\ncollapse. The gradient RMS in the left and middle plot is reported for the first MLP layer of block 0, but we observe\nsimilar trends for other layers (e.g., Appendix Figure E.12). Gradient RMS across different blocks is also reported\n(right). Gradient and update RMS are averaged over the final 500 steps, refer to Appendix Figure E.13 for the data\nduring training.\nOne question unresolved by our analysis so far is\nwhether we could have predicted that instability arises\nwhen the max attention logit exceeds 1e4 without ma-\nnipulating learning rate and model size. We take ini-\ntial steps towards an answer by transplanting different\nvalues of max attention logit into a small network with\n10M parameters. For different constants \u03ba we pass the\nqueries and keys through g(z) = \u221a\u03ba\u00b7z/\np\nEi[z2\ni ] before\ncomputing the attention logits. Results are illustrated\nin Figure 10. Loss deteriorates around \u03ba =1e3, and\nby \u03ba =1e4 the loss exceeds that of a zero-layer bigram\nmodel consisting of the Transformer we use without\nany self-attention or MLP layers.\n3.4\nSearching for new instabilities via\nscaling trends of model character-\nistics\nThis section examines whether the scaling behavior\nof model characteristics can be used to predict new\nissues with the default model and hyperparameter\nsettings.\nIn Figure 11 we examine scaling trends for the gradient\nroot mean square RMS(g) =\np\nEi [g2\ni ]. This figure\nreports the RMS for the first layer of the MLP, though\nwe observe similar trends for other layers (Appendix\nFigure E.12).\nAs models get larger, the value that grad RMS ap-\nproaches is cause for concern. At the largest scale\nand learning rate we test, grad RMS is around the\ndefault AdamW \u03f5 hyperparameter. Recall that the\nunscaled AdamW update is \u2206 = v/ (\u221au + \u03f5), where v\nand u are the first and second gradient moment EMA,\nrespectively. If the grad RMS is on the same order as\n\u03f5, then \u2206 will decrease in magnitude as illustrated by\nFigure 13, and parameters will not receive learning\nsignals as intended.\nAn obvious mitigation for this issue is to simply lower\nthe AdamW \u03f5 hyperparameter from its default of 1e-\n8. We conduct this experiment for a 4.8B parameter\nmodel at LR 0.3 and present the results in Figure 12.\nDecreasing \u03f5 to 1e-15 improves loss and mitigates a\ncollapse in grad RMS. We believe this improvement\nwill only increase at scale. On the other hand, in-\ncreasing \u03f5 to 1e-6 results in an instability (shown in\nFigure E.15).\nFigure 13 expands on this result by illustrating the\ngrad and update RMS throughout training at the\nlargest scale and learning rate we test. When the grad\nRMS reaches \u03f5, the update RMS becomes small. Fig-\nure E.13 presents data from an analogous experiment\nat many different scales and LRs, demonstrating that\nthis issue is most apparent for the larger models and\nLRs we test.\nAlthough we identified the instability above by empir-\n9\n0\n50000\n100000\nStep\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\nLoss\nEps = 1e-8\nEps = 1e-15\n0\n10\n20\nBlock index\n10\n13\n10\n12\n10\n11\n10\n10\n10\n9\n10\n8\nGrad RMS\nFigure 12: Decreasing the AdamW \u03f5 from its default value\nof 1e-8 to 1e-15 improves loss for a 4.8B parameter model\nat LR 0.3. When increasing \u03f5 to 1e-6, loss diverged. Grad\nRMS is averaged over the final 500 steps for the first\nlayer in the MLP; refer to Figure 13 for data throughout\ntraining.\nically measuring the scaling behavior of the gradients,\na mechanistic explanation exists. For larger networks\nand learning rates, the Transformer output RMS enter-\ning the final layernorm may grow. Since the layernorm\ngradients are scaled by the inverse of their input RMS,\nthe gradient received by the Transformer will shrink.\nRefer to Appendix C for a more detailed discussion.\n4\nRelated work\nThis paper mainly focuses on the effect of known in-\nterventions and instabilities, and so related work has\nbeen primarily discussed when relevant. This includes\nthe attention growth instability observed by Dehghani\net al. [11], Zhai et al. [51], and the final logit divergence\nissue encountered by Chowdhery et al. [6], Thilak et al.\n[44]. However, we highlight similar experimental meth-\nods in previous work. For instance, Yang et al. [50]\nalso measure the relationship between LR and loss\nacross scales, but their focus is on centering the opti-\nmum (see Section 3.2.4). In addition, Zhai et al. [51]\nelicit instability in base models by doubling learning\nrate, and Dettmers et al. [12] measure the presence\nof outlier features as a function of scale.\nThere are also important instabilities and related top-\nics we have not directly discussed so far. For instance,\nwe have primarily focused on instabilities that lead\nto a slow divergence, and we now summarize research\non fast loss spikes. This instability is characterized\nby a quick increase in the loss that often eventually\nrecovers.\nThe Edge of Stability and fast spikes\nThe conventional understanding of gradient descent\npredicts that loss instability only occurs when the\nlearning rate exceeds 2/\u03bbmax(H), where H is the\nHessian.\nHowever recent investigations into large\nbatch neural network training dynamics have re-\nvealed a more complicated picture via edge of stability\n(EoS) [7]. When training neural networks with large\nbatch SGD, the loss curvature constantly evolves via\nthe interaction of two processes: progressive sharpen-\ning and self stabilization. Progressive sharpening is\nthe empirical observation that when LR < 2/\u03bbmax(H),\nthe curvature gradually increases until the stability\nthreshold is violated. When the learning rate becomes\ntoo large relative to the curvature, fast loss spikes oc-\ncur and the parameters oscillate into a region with\nsmaller \u03bbmax(H) where stable training and progressive\nsharpening resumes. The latter process where insta-\nbility results in smaller \u03bbmax(H) is self-stabilization,\na theoretical model of which is given in Damian et al.\n[9]. Gradually shrinking \u03bbmax(H) via self stabilization\nwas shown to be a primary mechanism behind the\nsuccess of learning rate warmup in Gilmer et al. [17],\nwho closely studied the connections between curva-\nture, initialization, architecture and max trainable\nlearning rates.\nCohen et al. [8] further analyze edge of stability of\ndynamics with adaptive optimizers, showing that\nprogressive sharpening interacts with both the self-\nstabilization process and the adaptive optimizer state.\nThis interaction results in the preconditioned sharp-\nness \u03bbmax(P \u22121H) oscillating around an optimizer spe-\ncific threshold (38/LR in the case of Adam with\n\u03b21=0.9).\nAdaptive EoS (AEoS) can also result\nin periodic loss spikes when progressive sharpening\npushes the preconditioned sharpness above the stabil-\nity threshold, however the optimizer hyperparameters\nplay a role. In particular, when LR>38/\u03bbmax(P \u22121H),\ntwo mechanisms are now in play to resolve the step\nsize being too big\u2014either H can shrink or P \u22121 can\nshrink (or both). Cohen et al. [8] found that when\n\u03b22 is large, H tends to shrink and fast loss spikes\nresult during the process, resembling the self stabiliza-\ntion process observed with gradient descent. However\nwhen \u03b22 is small, P \u22121 tends to shrink, no loss spikes\nare observed, and \u03bbmax(H) tends to gradually increase\nthroughout training.\nIt is noteworthy that the adaptive edge of stability\nprocess (and the role of \u03b22) studied in Cohen et al. [8]\noffers a more complete understanding for loss spikes\n10\n0\n25000 50000 75000100000\nStep\n10\n11\n10\n10\n10\n9\n10\n8\n10\n7\n10\n6\nGrad RMS\nBlock = 0\n0\n25000 50000 75000100000\nStep\n10\n11\n10\n10\n10\n9\n10\n8\n10\n7\n10\n6\nBlock = 6\n0\n25000 50000 75000100000\nStep\n10\n11\n10\n10\n10\n9\n10\n8\n10\n7\n10\n6\nBlock = 16\n0\n25000 50000 75000100000\nStep\n10\n11\n10\n10\n10\n9\n10\n8\n10\n7\n10\n6\nBlock = 22\n0\n25000 50000 75000100000\nStep\n10\n3\n10\n2\n10\n1\nUnscaled update RMS\nEps = 1e-6 (diverged)\nEps = 1e-8 (default)\nEps = 1e-15\n0\n25000 50000 75000100000\nStep\n10\n3\n10\n2\n10\n1\n0\n25000 50000 75000100000\nStep\n10\n3\n10\n2\n10\n1\n0\n25000 50000 75000100000\nStep\n10\n3\n10\n2\n10\n1\nFigure 13: The top row displays the root mean square (RMS) of the gradient for the first MLP layer at different\nblocks throughout the network. When the grad RMS drops below the AdamW \u03f5 hyperparameter, the magnitude of\nthe update decreases, as illustrated by the bottom row. Experiment conducted with a 4.8B parameter model trained\nwith LR 0.3. The experiment with \u03f5 = 1e-6 was stopped when loss diverged.\nstudied in a body of literature [42, 6, 35, 47, 52, 5].\nFor example, Shazeer and Stern [42] argue that during\ntraining of Transformers with adaptive optimizers the\noptimizer update can become too big resulting in a\nloss spike followed by recovery. This is sometimes\nattributed to the adaptive optimizer state becoming\n\u201cstale\u201d, which is consistent with the observation the\nreducing \u03b22 resolves the loss spikes [42, 47, 52]. This\nis perhaps the same observation as Cohen et al. [8]\nthat reducing \u03b22 allows P \u22121 to change quicker to\nadjust to the process of progressive sharpening. AEoS\nalso offers an explanation for the periodic loss spikes\nobserved when training large transformer models [35].\nParameter-free methods and more parameter-\nizations.\nWhile our work has studied sensitivity\nto learning rate, there is also research that aims to\neliminate the need to specify a learning rate [24, 10].\nBased on their analysis, Ivgi et al. [24] set the step\nsize for iteration t to the maximum distance from the\ninitialization divided by the root sum of historical\ngradient squares. Moreover, while our work investi-\ngated \u00b5Param, there are additional parameterizations\nfor which it would be interesting to explore LR vs.\nloss [13, 48, 3, 25].\n5\nConclusion\nAs the compute required to train the largest models\ncontinues to increase, it becomes increasingly impor-\ntant to understand if training will be stable. This\npaper has shown that useful insights on stability can\nbe found when studying small Transformers. We hope\nthat this opens new opportunities for impactful re-\nsearch which benefits large runs without access to\nlarge resource pools.\nAcknowledgements\nWe thank George Dahl for thorough comments and\nsuggestions, and Hugo Larochelle and Rif A. Saurous\nfor helpful discussion. Also, we thank the members of\nthe Google DeepMind PAGI team for their support\nof this effort, Noah Fiedel, Noah Constant, Aaron\nParisi, Alex Rizkowsky, Avi Singh, Azade Nova, Bernd\nBohnet, Daniel Freeman, Gamaleldin Elsayed, Hanie\nSedghi, Isabelle Simpson, James Harrison, Jiri Hron,\nKathleen Kenealy, Kevin Swersky, Kshiteej Mahajan,\n11\nLaura Culp, Max Bileschi, Merrie Morris, Rosanne\nLiu, Yundi Qian, Sharad Vikram, Tris Warkentin.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton.\nLayer normalization.\narXiv preprint\narXiv:1607.06450, 2016.\n[2] Igor Babuschkin, Kate Baumli, Alison Bell, Surya\nBhupatiraju, Jake Bruce, Peter Buchlovsky, David\nBudden, Trevor Cai, Aidan Clark, Ivo Danihelka,\nAntoine Dedieu, Claudio Fantacci, Jonathan God-\nwin, Chris Jones, Ross Hemsley, Tom Hennigan,\nMatteo Hessel, Shaobo Hou, Steven Kapturowski,\nThomas Keck, Iurii Kemaev, Michael King, Markus\nKunesch, Lena Martens, Hamza Merzic, Vladimir\nMikulik, Tamara Norman, George Papamakarios,\nJohn Quan, Roman Ring, Francisco Ruiz, Alvaro\nSanchez, Laurent Sartran, Rosalia Schneider, Eren\nSezener, Stephen Spencer, Srivatsan Srinivasan,\nMilo\u02c7s Stanojevi\u00b4c, Wojciech Stokowiec, Luyu Wang,\nGuangyao Zhou, and Fabio Viola. The DeepMind\nJAX Ecosystem, 2020.\nURL http://github.com/\ndeepmind.\n[3] Blake Bordelon and Cengiz Pehlevan.\nDynamics\nof finite width kernel and prediction fluctuations\nin mean field neural networks.\narXiv preprint\narXiv:2304.03408, 2023.\n[4] James Bradbury,\nRoy Frostig,\nPeter Hawkins,\nMatthew James Johnson,\nChris Leary,\nDougal\nMaclaurin, George Necula, Adam Paszke, Jake Van-\nderPlas, Skye Wanderman-Milne, and Qiao Zhang.\nJAX: composable transformations of Python+NumPy\nprograms, 2018. URL http://github.com/google/\njax.\n[5] X. Chen,\nS. Xie,\nand K. He.\nAn empirical\nstudy of training self-supervised vision transform-\ners.\nIn 2021 IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 9620\u2013\n9629, Los Alamitos, CA, USA, oct 2021. IEEE\nComputer Society. doi: 10.1109/ICCV48922.2021.\n00950.\nURL https://doi.ieeecomputersociety.\norg/10.1109/ICCV48922.2021.00950.\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob De-\nvlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sut-\nton, Sebastian Gehrmann, et al.\nPalm:\nScaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311, 2022.\n[7] Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico\nKolter, and Ameet Talwalkar. Gradient descent on\nneural networks typically occurs at the edge of sta-\nbility. arXiv preprint arXiv:2103.00065, 2021.\n[8] Jeremy M Cohen, Behrooz Ghorbani, Shankar Kr-\nishnan, Naman Agarwal, Sourabh Medapati, Michal\nBadura, Daniel Suo, David Cardoze, Zachary Nado,\nGeorge E Dahl, et al. Adaptive gradient methods at\nthe edge of stability. arXiv preprint arXiv:2207.14484,\n2022.\n[9] Alex Damian, Eshaan Nichani, and Jason D Lee. Self-\nstabilization: The implicit bias of gradient descent at\nthe edge of stability. arXiv preprint arXiv:2209.15594,\n2022.\n[10] Aaron Defazio and Konstantin Mishchenko. Learning-\nrate-free learning by d-adaptation. arXiv preprint\narXiv:2301.07733, 2023.\n[11] Mostafa Dehghani, Josip Djolonga, Basil Mustafa,\nPiotr Padlewski, Jonathan Heek, Justin Gilmer,\nAndreas Steiner, Mathilde Caron, Robert Geirhos,\nIbrahim Alabdulmohsin, et al. Scaling vision trans-\nformers to 22 billion parameters.\narXiv preprint\narXiv:2302.05442, 2023.\n[12] Tim Dettmers, Mike Lewis, Younes Belkada, and\nLuke Zettlemoyer. Llm. int8 (): 8-bit matrix mul-\ntiplication for transformers at scale. arXiv preprint\narXiv:2208.07339, 2022.\n[13] Emily Dinan, Sho Yaida, and Susan Zhang. Effective\ntheory of transformers at initialization. arXiv preprint\narXiv:2304.02034, 2023.\n[14] Alexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk\nWeissenborn,\nXiaohua\nZhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16\nwords: Transformers for image recognition at scale.\nIn International Conference on Learning Representa-\ntions (ICLR), 2021. https://arxiv.org/abs/2010.\n11929.\n[15] Colin Gaffney, Dinghua Li, Ruoxin Sang, Ayush Jain,\nand Haitang Hu. Orbax, 2023. URL http://github.\ncom/google/orbax.\n[16] Justin Gilmer, Andrea Schioppa, and Jeremy Co-\nhen. Intriguing properties of transformer training\ninstabilities. To appear.\n[17] Justin Gilmer, Behrooz Ghorbani, Ankush Garg,\nSneha Kudugunta, Behnam Neyshabur, David Car-\ndoze, George Dahl, Zachary Nado, and Orhan Firat.\nA loss curvature perspective on training instability\nin deep learning. arXiv preprint arXiv:2110.04369,\n2021.\n12\n[18] Xavier Glorot and Yoshua Bengio. Understanding the\ndifficulty of training deep feedforward neural networks.\nIn Proceedings of the thirteenth international confer-\nence on artificial intelligence and statistics, pages\n249\u2013256. JMLR Workshop and Conference Proceed-\nings, 2010.\n[19] Google.\nGrain - feeding jax models, 2023.\nURL\nhttp://github.com/google/grain.\n[20] Jonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\nand Marc van Zee.\nFlax:\nA neural network li-\nbrary and ecosystem for JAX, 2023.\nURL http:\n//github.com/google/flax.\n[21] Dan Hendrycks and Kevin Gimpel. Gaussian error\nlinear units (gelus). arXiv preprint arXiv:1606.08415,\n2016.\n[22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks, Jo-\nhannes Welbl, Aidan Clark, et al. Training compute-\noptimal large language models.\narXiv preprint\narXiv:2203.15556, 2022.\n[23] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le.\nTransformer quality in linear time. In International\nConference on Machine Learning, pages 9099\u20139117.\nPMLR, 2022.\n[24] Maor Ivgi, Oliver Hinder, and Yair Carmon. Dog is\nsgd\u2019s best friend: A parameter-free dynamic step size\nschedule. arXiv preprint arXiv:2302.12022, 2023.\n[25] Arthur Jacot, Franck Gabriel, and Cl\u00b4ement Hon-\ngler. Neural tangent kernel: Convergence and gen-\neralization in neural networks. In Advances in Neu-\nral Information Processing Systems (NeurIPS), 2018.\nhttps://arxiv.org/abs/1806.07572.\n[26] Norman P Jouppi, Cliff Young, Nishant Patil, David\nPatterson, Gaurav Agrawal, Raminder Bajwa, Sarah\nBates, Suresh Bhatia, Nan Boden, Al Borchers, et al.\nIn-datacenter performance analysis of a tensor pro-\ncessing unit. In Proceedings of the 44th annual inter-\nnational symposium on computer architecture, pages\n1\u201312, 2017.\n[27] Jared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361, 2020.\n[28] Taku Kudo and John Richardson. Sentencepiece: A\nsimple and language independent subword tokenizer\nand detokenizer for neural text processing. arXiv\npreprint arXiv:1808.06226, 2018.\n[29] Jaehoon Lee. A random walk model of transformer\nparameter growth, 2023.\n[30] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\nOn the variance of the adaptive learning rate and\nbeyond. arXiv preprint arXiv:1908.03265, 2019.\n[31] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. Generating wikipedia by summarizing long\nsequences. In International Conference on Learning\nRepresentations, 2018. URL https://openreview.\nnet/forum?id=Hyg0vbWC-.\n[32] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas-\ntic gradient descent with warm restarts.\nIn In-\nternational Conference on Learning Representations\n(ICLR), 2016. https://arxiv.org/abs/1608.03983.\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight\ndecay regularization.\nIn International Conference\non Learning Representations (ICLR), 2019. https:\n//openreview.net/forum?id=Bkg6RiCqY7.\n[34] William Merrill, Vivek Ramanujan, Yoav Goldberg,\nRoy Schwartz, and Noah Smith. Effects of param-\neter norm growth during transformer training: In-\nductive bias from gradient descent. arXiv preprint\narXiv:2010.09697, 2020.\n[35] Igor Molybog, Peter Albert, Moya Chen, Zachary\nDeVito, David Esiobu, Naman Goyal, Punit Singh\nKoura, Sharan Narang, Andrew Poulton, Ruan Silva,\net al. A theory on adam instability in large-scale\nmachine learning. arXiv preprint arXiv:2304.09871,\n2023.\n[36] Adam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. Pytorch: An imperative style, high-\nperformance deep learning library. In Advances in\nNeural Information Processing Systems (NeurIPS),\n2019. https://arxiv.org/abs/1912.01703.\n[37] Ofir Press and Lior Wolf. Using the output embed-\nding to improve language models.\nIn Proceedings\nof the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 2, Short Papers, pages 157\u2013163, Valencia, Spain,\nApril 2017. Association for Computational Linguistics.\nURL https://aclanthology.org/E17-2025.\n[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language Models\nare Unsupervised Multitask Learners, 2019. https:\n//openai.com/blog/better-language-models/.\n13\n[39] Colin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of\ntransfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 2020.\nhttp://jmlr.org/papers/v21/20-074.html.\n[40] Colin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of trans-\nfer learning with a unified text-to-text transformer.\nJournal of Machine Learning Research, 21(140):1\u201367,\n2020.\nURL http://jmlr.org/papers/v21/20-074.\nhtml.\n[41] Jie Ren, Samyam Rajbhandari, Reza Yazdani Am-\ninabadi, Olatunji Ruwase, Shuangyan Yang, Min-\njia Zhang, Dong Li, and Yuxiong He.\n{ZeRO-\nOffload}: Democratizing {Billion-Scale} model train-\ning. In 2021 USENIX Annual Technical Conference\n(USENIX ATC 21), pages 551\u2013564, 2021.\n[42] Noam Shazeer and Mitchell Stern. Adafactor: Adap-\ntive learning rates with sublinear memory cost. In\nInternational Conference on Machine Learning, pages\n4596\u20134604. PMLR, 2018.\n[43] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu.\nRoformer: Enhanced\ntransformer with rotary position embedding. arXiv\npreprint arXiv:2104.09864, 2021.\n[44] Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid\nSaremi, Roni Paiss, and Joshua Susskind. The sling-\nshot mechanism: An empirical study of adaptive opti-\nmizers and the grokking phenomenon. arXiv preprint\narXiv:2206.04817, 2022.\n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez,  Lukasz\nKaiser, and Illia Polosukhin. Attention is all you\nneed. Advances in neural information processing sys-\ntems, 30, 2017.\n[46] Mitchell Wortsman, Jaehoon Lee, Justin Gilmer, and\nSimon Kornblith. Replacing softmax with relu in\nvision transformers.\n[47] Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer,\nAri Morcos, Ali Farhadi, and Ludwig Schmidt. Sta-\nble and low-precision training for large-scale vision-\nlanguage models. arXiv preprint arXiv:2304.13013,\n2023.\n[48] Sho Yaida. Meta-principled family of hyperparameter\nscaling strategies. arXiv preprint arXiv:2210.04909,\n2022.\n[49] Greg Yang and Edward J Hu. Tensor programs iv:\nFeature learning in infinite-width neural networks. In\nInternational Conference on Machine Learning, pages\n11727\u201311737. PMLR, 2021.\n[50] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon\nSidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub\nPachocki, Weizhu Chen, and Jianfeng Gao.\nTen-\nsor programs v: Tuning large neural networks via\nzero-shot hyperparameter transfer. arXiv preprint\narXiv:2203.03466, 2022.\n[51] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin,\nDan Busbridge, Jason Ramapuram, Yizhe Zhang,\nJiatao Gu, and Josh Susskind.\nStabilizing trans-\nformer training by preventing attention entropy col-\nlapse. arXiv preprint arXiv:2303.06296, 2023.\n[52] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,\nand Lucas Beyer. Sigmoid loss for language image\npre-training. arXiv preprint arXiv:2303.15343, 2023.\n[53] Susan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n14\nA\nAdditional infrastructure de-\ntails\nThis Section provides more details on the training in-\nfrastructure, which is built on Flax [20], Jax [4], and\nTPUs [26], and we call NanoDO. To enable larger\nmodel training, we shard the model and optimizer\nstates as in FSDP [41], then specify these shadings\nwhen compiling with JIT. We use Orbax [15] for check-\npointing, and Grain [19] for deterministic data loading.\nWhen loading data, sequences are packed so that no\npadding is required\u2014if a sequence is less tokens than\nthe context length hyperparameter, then an end of\nsequence token is appended, followed by the beginning\nof a new sequence.\nB\nWhen is learning rate sensi-\ntivity a useful metric\nThere are cases where LR sensitivity (defined in Sec-\ntion 2.2) is no longer a useful metric. This section\ndetails these scenarios and justifies the use of LR\nsensitivity for the interventions in this paper.\nInterventions which change the meaning of\nlearning rate\nWhen an intervention changes the meaning of learning\nrate then comparing LR sensitivity is not useful. A\nclear example of this would be taking the square root\nof the LR before passing it to the optimizer, but there\nare more subtle cases to be cautious of when using\nLR sensitivity.\nIn general, we avoid manipulations where the mean-\ning of LR meaningfully changes. In some cases, we\nhave good empirical evidence that the meaning of\nthe learning rate has not changed when intervening.\nFor instance, the LR vs.\nloss curves are indistin-\nguishable up to some critical learning rate when using\nqk-layernorm (Figure 1), adding z-loss (Figure 3), or\nchanging warm-up.\nIn other cases, such as when testing \u00b5Param (Sec-\ntion 3.2.4), we believe that LR sensitivity is useful\ndespite a per-layer modification of LR. This is be-\ncause the per-layer LR is manipulated linearly, and\nthis modification does not change for different points\non the LR vs loss curve.\nThe one experiment in this paper where we believe LR\nsensitivity is likely not a useful metric is when scaling\nlearning rate by the root mean square of the parame-\nters (Figure E.14). Therefore, we do not measure LR\nsensitivity in that case.\nShifting of the optimal LR\nThe definition of LR sensitivity in Section 2.2 does not\naccount for the optimal LR shifting when specifying\nthe LR range [a, b]. In practice we recommend shifting\nthe three order of magnitude range [a, b] to correspond\nwith this shift. For instance, we shift the range in\nSection 3.2.2, as discussed in more detail in the section.\nHowever, our main experiments (e.g., Figure 1) do not\ntest at a large enough scale to necessitate this shift.\nLR sensitivity is invariant to loss\nAnother limitation of the LR sensitivity metric is\nthat it is invariant to the scale of the loss. If the\nnetwork consistently achieves random performance\nacross learning rates, then LR sensitivity will be zero.\nWe do not offer a solution to this, and instead recom-\nmend that LR sensitivity should always be examined\nin combination with the LR vs. loss curves as we do\nin this paper. It is meant as a useful summary of the\nLR vs. loss curves, not as a metric to optimize in\nisolation.\nC\nOutput norm growth\nThis section discusses the growth of the output norms\nduring Transformer training as previously studied\nby Merrill et al. [34], Lee [29], and relates this phe-\nnomenon to the attention logit growth and AdamW\nepsilon instabilities (Sections 3.1.1 and 3.4, respec-\ntively). As empirical evidence, Figure C.1 shows that\nthe RMS of the Transformer block output is mainly\ndetermined by learning rate.\nWe have two hypothesis which relate parameter norm\ngrowth [34] and subsequent output norm growth to\ninstability. First, we believe that the attention out-\nput logits are the first to become large because they\nare the only feature in the network we test whose\nmagnitude depends quadratically on parameter RMS.\nFor inputs X with unit RMS, a typical matrix multi-\nply XW with parameters W will result in features Y\nwhere RMS(Y ) is a linear function of RMS(W). On\nthe other hand, the attention logit entries are com-\nputed via \u27e8XW1, XW2\u27e9 so depend quadratically on\nRMS(W). Next, this helps to explain the decreasing\ntrend in gradient scale observed in Section 3.4 (Fig-\nure 11). In a pre-normalization [38] Transformer [45]\n15\nthere is an output layernorm layer [1] after the last\nTransformer block and before the final linear layer.\nThe gradient from this output layernorm layer is scaled\nby the reciprocal of the input RMS. This RMS is grow-\ning with depth because of the residual connections\n(Figure C.1). As the RMS leaving the last Transformer\nblock grows, the gradient received shrinks.\nFor completeness we now compute the layernorm gra-\ndient to input x. We assume the input as mean zero\nand the layernorm has no bias for simplicity. Let\nz = LayerNorm(x) = \u03b1 \u00b7\nx\np\nEi [x2\ni ] + \u03f5\n= \u03b1 \u00b7\nx\nm1/2\n(1)\nwhere m = Ei\n\u0002\nx2\ni\n\u0003\n+ \u03f5.\nThen\n\u2202\u2113\n\u2202xj\n=\nX\nk\n\u2202\u2113\n\u2202zk\n\u2202zk\n\u2202xj\n(2)\n= \u2202\u2113\n\u2202zj\n\u00b7\n\u03b1j\nm1/2 +\nX\nk\n\u2202\u2113\n\u2202zk\n\u00b7\n\u0012\n\u22121\n2\n\u0013\n\u00b7 \u03b1kxk\nm3/2 \u00b7 2\nn \u00b7 xj (3)\n=\n1\nm1/2\n \n\u03b1j\n\u2202\u2113\n\u2202zj\n\u2212\nxj\nnm1/2\nX\nk\n\u2202\u2113\n\u2202zk\n\u03b1kxk\n!\n(4)\n=\n1\nm1/2\n\u0012\n\u03b1j\n\u2202\u2113\n\u2202zj\n\u2212\nxj\nnm1/2 \u27e8\u2207z, \u03b1 \u00b7 x\u27e9\n\u0013\n(5)\nEquivalently,\n\u2207x =\n1\nm1/2\n\u0012\n\u03b1 \u2299 \u2207z \u2212 \u27e8\u2207z, \u03b1 \u2299 x\u27e9\nnm1/2\n\u2299 x\n\u0013\n.\n(6)\nD\nAuthor contributions\nMitchell Wortsman led the project, ran the experi-\nments and produced the figures, contributed substan-\ntially to the infrastructure for experimentation, the\nframing and direction, and the writing.\nPeter J. Liu led the infrastructure and creation of\nNanoDO for experimentation, provided key insights\nand advice on multiple technical areas, and con-\ntributed to the writing.\nLechao Xiao and Katie Everett contributed to the\ninfrastructure used for experimentation, provided key\ninsight related to parameterization, and contributed\nto the writing.\nAlex Alemi, Ben Adlam, John D. Co-Reyes, Izzeddin\nGur, Abhishek Kumar, Roman Novak, Jeffrey Pen-\nnington, Jascha Sohl-dickstein, and Kelvin Xu were\nactive participants in weekly brainstorming meetings\nwhich motivated, influenced, and elucidated technical\nconcepts pertaining to this work.\nJaehoon Lee and Justin Gilmer were senior authors\nadvising on the project, contributed substantially to\nthe framing and direction, provided key insight and\nadvice on multiple technical areas, and contributed\nto the writing. Jaehoon led the connection with out-\nput norm growth. Justin proposed to plot loss as\na function of learning rate for different model sizes,\nand performed initial experiments demonstrating that\nattention logit growth could be reproduced at high\nlearning rates in small models.\nSimon Kornblith was the lead advisor on the project,\ncontributing substantially to the framing, direction,\ninfrastructure, and writing.\nSimon initially brain-\nstormed the project with Mitchell, and was Mitchell\u2019s\nhost for the summer internship during which this re-\nsearch was conducted, providing substantial technical\nsupport.\nE\nAdditional figures\nThis Section contains the additional Figures referenced\nin the main text.\n16\n107\n108\n109\nNum params\n100\n102\n104\n106\nBlock output RMS\n10\n3\n10\n2\n10\n1\nLR\n100\n102\n104\n106\n5\n10\n15\n20\n25\nBlock index\n106\n107\n108\n0.0003\n0.3\nLR\n9.4e6\n1.2e9\nNum params\n9.4e6\n1.2e9\nNum params (at LR 3e-1)\nFigure C.1: The root mean square (RMS) of the Transformer block outputs are roughly consistent with scale (left)\nbut increase with learning rate (center). RMS increases deeper in the transformer because of the residual connections,\nwhich is shown for very high learning rates (right). The first two plots are for block index two, and RMS is averaged\nover the final 500 training steps. Recall RMS(X) =\np\nEi[X2\ni ].\n0\n2000\n4000\n6000\n8000 10000\nStep\n3\n4\n5\n6\n7\n8\nLoss\n0\n2000\n4000\n6000\n8000 10000\nStep\n102\n104\n106\n108\nMax attention logit\n0\n2000\n4000\n6000\n8000 10000\nStep\n101\n102\n103\n104\nMean query norm\n0\n2000\n4000\n6000\n8000 10000\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nQuery-key cossim mean\n0\n2000\n4000\n6000\n8000 10000\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nQuery-key cossim max\nqk-layernorm = True\nqk-layernorm = False\n0\n2000\n4000\n6000\n8000 10000\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAvg. max attention prob.\nNum params = 9.4e6, LR = 0.1, block 0\nFigure E.1: The logit growth instability [11, 51] occurs when the norm of the query and keys increases, not due to an\nincrease in their cosine similarity.\n17\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\nFinal eval loss\nScaling width\nScaling depth\ndim = 256\ndim = 512\ndim = 768\ndim = 1024\ndim = 2048\nlayers = 3\nlayers = 6\nlayers = 12\nlayers = 24\nlayers = 48\nlayers = 96\nFigure E.2: The effect of scaling width vs. scaling depth\nwithout qk-layernorm [11].\n107\n108\nNum params\n2.8 \u00d7 100\n3 \u00d7 100\n3.2 \u00d7 100\n3.4 \u00d7 100\n3.6 \u00d7 100\nFinal eval loss\nJoint scaling\nScaling depth\nScaling width\nFigure E.3: Jointly scaling width and depth leads to lower\nloss than independently scaling depth or width at the\nlargest scale we test.\nIt also leads to a more reliable\nscaling prediction when extrapolating from models with\nless than 1e8 parameters. Best loss is reported in a sweep\nover learning rates.\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.50\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\nFinal eval loss\nStandard\nMuParam (simple)\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\nN = 1.2e+09\n107\n108\n109\nNumber of parameters\n100\nLR sensitivity\nStandard\nMuParam (simple)\nFigure E.4: The effect of \u00b5Param on LR sensitivity for\nmodels without qk-layernorm [11]. \u00b5Param succeeds in\nstabilizing the optimal LR, but does not alleviate the\nneed for qk-layernorm.\nFor more information refer to\nSection 3.2.4.\n18\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\n4.4\nFinal eval loss\nqk-layernorm = True\nStandard\nMuParam (simple)\nMuParam (intermediate)\nMuParam (full)\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n107\n108\nNumber of parameters\n10\n2\n10\n1\n100\nLR sensitivity\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\n4.4\nFinal eval loss\nqk-layernorm = False\nStandard\nMuParam (simple)\nMuParam (intermediate)\nMuParam (full)\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n107\n108\nNumber of parameters\n10\n2\n10\n1\n100\nLR sensitivity\nFigure E.5: Comparing \u00b5Param (full), which implements \u00b5Param as described in Yang et al. [50] with and without\nqk-layernorm, with \u00b5Param (simple) and \u00b5Param (intermediate). There are four changes in \u00b5Param (full), (i) Scale\nthe LR for linear layers by base-fan-in/fan-in, (ii) initialize the head with standard deviation\n\u221a\nbase-fan-in/fan-in.\n(iii) change the 1/\u221adh scaling factor in attention layers to 1/dh where dh is the head dimension, and (iv) initialize the\nquery projection weights with zeros. \u00b5Param (intermediate) consists of (i) and (ii), while \u00b5Param (simple) is only (i).\nWith \u00b5Param (full) and qk-layernorm, the model trains without diverging at LR 1. However at the best LR there is\nno measurable improvement over \u00b5Param (simple) at the largest scale we test.\n19\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\n4.4\nFinal eval loss\nqk-layernorm = True\n1/sqrt(head dim) scaling\n1/(head dim) scaling\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\n4.4\nFinal eval loss\nqk-layernorm = False\n1/sqrt(head dim) scaling\n1/(head dim) scaling\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\nFigure E.6: Measuring the effect of changing the 1/\u221adh term in attention to 1/dh, where dh is head dimension.\nVaswani et al. [45] use 1/\u221adh while Yang et al. [50] use 1/dh.\n20\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\nFinal eval loss\nSteps = 50000\nSteps = 100000\nSteps = 200000\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n107\n108\nNumber of parameters\n2 \u00d7 10\n2\n3 \u00d7 10\n2\n4 \u00d7 10\n2\nLR sensitivity\nFigure E.7: Changing the number of total training steps\nfrom 1e5 to 5e4 or 2e5 does not have a large effect of the\nshape of the learning rate vs. loss curves at the scales we\ntest.\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\nFinal eval loss\nHead-dim qk-layernorm\nModel-dim qk-layernorm\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\nFigure E.8: We achieve slightly better performance when\napplying qk-layernorm individually per-head instead of\nacross the model dimension. The per-head variant has\nonly head-dim learnable parameters instead of model-dim\nparameters. We use the per-head variant as the default in\nthis paper, and we never use biases.\n21\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\nFinal eval loss\nqk-layernorm = True\nBatch size = 256\nBatch size = 512\nBatch size = 1024\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n107\n108\nNumber of parameters\n10\n2\n10\n1\n100\nLR sensitivity\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\nFinal eval loss\nqk-layernorm = False\nBatch size = 256\nBatch size = 512\nBatch size = 1024\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n107\n108\nNumber of parameters\n10\n2\n10\n1\n100\nLR sensitivity\nFigure E.9: Increasing the batch size from 256 to 512 or 1024 does not have a large effect on the shape of the learning\nrate vs. loss curves at the scales we test. Each batch element contains 512 tokens, and we use 256 as the default.\n22\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\nFinal eval loss\nqk-layernorm = True\nWeight decay = 3e-04\nWeight decay = 1e-04\nWeight decay = 3e-05\nWeight decay = 0\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n107\n108\nNumber of parameters\n10\n2\n10\n1\n100\nLR sensitivity\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\nFinal eval loss\nqk-layernorm = False\nWeight decay = 3e-04\nWeight decay = 1e-04\nWeight decay = 3e-05\nWeight decay = 0\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n107\n108\nNumber of parameters\n10\n2\n10\n1\n100\nLR sensitivity\nFigure E.10: The effect of weight decay on LR sensitivity. We use independent weight decay as described in Section 3.2.2\nand recommended by [33].\n23\n10\n4\n10\n3\n10\n2\n10\n1\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\nFinal eval loss\nqk-layernorm = True\nSoftmax attention\nPointwise attention\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n10\n4\n10\n3\n10\n2\n10\n1\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\nFinal eval loss\nqk-layernorm = False\nSoftmax attention\nPointwise attention\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\nFigure E.11: The logit growth instability occurs even without softmax. For the pointwise variant of attention here,\nwe replace softmax with squared-relu as described by [23]. As recommended in [46] we add a scaling factor which\ndepends on sequence length. In this case, we use inverse square root.\n107\n108\n109\nNum params\n10\n8\n10\n7\n10\n6\n10\n5\n10\n4\nGrad RMS\n10\n3\n10\n2\n10\n1\nLR\n10\n8\n10\n6\n10\n4\n0\n5\n10\n15\n20\nBlock index\n10\n10\n10\n9\n10\n8\n10\n7\n0.0003\n0.3\nLR\n9.4e6\n1.2e9\nNum params\n9.4e6\n1.2e9\nNum params (at LR 3e-1)\nFigure E.12: Recreating Figure 11 with the kernel projection instead of the first MLP layer.\n24\n0\n25000 50000 75000 100000\nStep\n3 \u00d7 100\n4 \u00d7 100\n5 \u00d7 100\nLoss\nLR = 0.0003\n0\n25000 50000 75000 100000\nStep\n3 \u00d7 100\n4 \u00d7 100\n5 \u00d7 100\nLR = 0.001\n0\n25000 50000 75000 100000\nStep\n3 \u00d7 100\n4 \u00d7 100\n5 \u00d7 100\nLR = 0.01\n0\n25000 50000 75000 100000\nStep\n3 \u00d7 100\n4 \u00d7 100\n5 \u00d7 100\nLR = 0.3\n0\n25000 50000 75000 100000\nStep\n10\n8\n10\n7\n10\n6\n10\n5\n10\n4\nGrad RMS\n0\n25000 50000 75000 100000\nStep\n10\n8\n10\n7\n10\n6\n10\n5\n10\n4\n0\n25000 50000 75000 100000\nStep\n10\n8\n10\n7\n10\n6\n10\n5\n10\n4\n0\n25000 50000 75000 100000\nStep\n10\n8\n10\n7\n10\n6\n10\n5\n10\n4\n0\n25000 50000 75000 100000\nStep\n10\n1\nUnscaled update RMS\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\nN = 1.2e+09\n0\n25000 50000 75000 100000\nStep\n10\n1\n0\n25000 50000 75000 100000\nStep\n10\n1\n0\n25000 50000 75000 100000\nStep\n10\n1\nFigure E.13: For various learning rates and model sizes we display the gradient root mean square (RMS), and the\nunscaled update RMS. The unscaled udpate is the update returned by the optimizer before scaling by learning rate.\nThe gradient and update are shown here for the first MLP layer of the Transformer. The update RMS falls when the\ngrad RMS approaches the AdamW \u03f5 of 1e-8.\n25\n10\n4\n10\n3\n10\n2\n10\n1\n100\nLearning rate\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\n4.4\nFinal eval loss\nStandard\nScale LR by RMS\nN = 2.4e+06\nN = 9.4e+06\nN = 1.9e+07\nN = 4.2e+07\nN = 8.5e+07\nN = 1.5e+08\nN = 3.0e+08\n107\n108\nNumber of parameters\n10\n1\n100\nLR sensitivity\nStandard\nScale LR by RMS\nFigure E.14: The effect of scaling the learning rate for\nparameters p by max (RMS(p), 1e-3) as in AdaFactor [42].\nAs discussed by Appendix B, it is not meaningful to com-\npare LR sensitivity in this case as this intervention modifies\nthe meaning of learning rate. Just as in \u00b5Param [50], RMS\nscaling appears to stabilize the optimal LR in the range\nwe test.\n0\n2000\n4000\nStep\n4\n5\n6\n7\n8\nLoss\nEps = 1e-6\n2000\n4000\nStep\n10\n7\n10\n6\n10\n5\n10\n4\nGrad RMS\nFigure E.15: Increasing the AdamW \u03f5 from its default\nvalue of 1e-8 to 1e-6 causes a loss divergence for a 4.8B\nparameter model at LR 0.3. Grad RMS is for the first\nlayer in the MLP.\n26\n"
  },
  {
    "title": "SCREWS: A Modular Framework for Reasoning with Revisions",
    "link": "https://arxiv.org/pdf/2309.13075.pdf",
    "upvote": "14",
    "text": "SCREWS\n: A MODULAR FRAMEWORK FOR\nREASONING WITH REVISIONS\nKumar Shridhar \u2217\u2020\nHarsh Jhamtani \u2021\nHao Fang \u2021\nBenjamin Van Durme \u2021\nJason Eisner \u2021\nPatrick Xia \u2021\n\u2020 ETH Zurich\n\u2021 Microsoft Semantic Machines\nshkumar@ethz.ch\npatrickxia@microsoft.com\nABSTRACT\nLarge language models (LLMs) can improve their accuracy on various tasks\nthrough iteratively refining and revising their output based on feedback. We ob-\nserve that these revisions can introduce errors, in which case it is better to roll\nback to a previous result. Further, revisions are typically homogeneous: they use\nthe same reasoning method that produced the initial answer, which may not cor-\nrect errors. To enable exploration in this space, we present SCREWS, a modular\nframework for reasoning with revisions. It is comprised of three main modules:\nSampling, Conditional Resampling, and Selection, each consisting of sub-modules\nthat can be hand-selected per task. We show that SCREWS not only unifies several\nprevious approaches under a common framework, but also reveals several novel\nstrategies for identifying improved reasoning chains. We evaluate our framework\nwith state-of-the-art LLMs (ChatGPT and GPT-4) on a diverse set of reasoning\ntasks and uncover useful new reasoning strategies for each: arithmetic word prob-\nlems, multi-hop question answering, and code debugging. Heterogeneous revision\nstrategies prove to be important, as does selection between original and revised\ncandidates.\n1\nINTRODUCTION\nLarge Language Models (LLMs) have proven effective on a variety of reasoning tasks (OpenAI,\n2023). However, the LLM output is not always correct on its first attempt, and it is often necessary to\niteratively refine the outputs to ensure that the desired goal is achieved (Madaan et al., 2023; Welleck\net al., 2022; Zheng et al., 2023). These refinement methods assume that subsequent outputs (either\nby the same model, or by an external model or some tool) lead to better performance. However,\nthere is no guarantee that subsequent versions must be better; as Figure 1 illustrates, refinement can\nlead to a wrong answer. This motivates a Selection strategy whereby the model can select an earlier\noutput.\nIn addition, past work on iterative refinement typically assumes a single, fixed reasoning strategy\n(Welleck et al., 2022; Huang et al., 2022; Madaan et al., 2023; Zheng et al., 2023). Humans, however,\nare more flexible. A student preparing for an exam may use deductive reasoning to solve problems\nand inductive reasoning to verify the results; or a product manager may use a brainstorming strategy\nto list several ideas and then switch to a prioritization strategy to rank them based on their feasibility\nor impact. Thus, we propose a modular approach to answer refinements, allowing us to test different\nstrategies.\nIn this work, we introduce SCREWS, a modular framework for reasoning with revisions.1 Figure\n2 introduces the three main modules of the framework in detail, namely Sampling, Conditional\nResampling, and Selection. For a given task and input sequence, we instantiate SCREWS by fixing\nthe submodules for each module (for example, we might select \u201cChain of Thought\u201d for Sampling).\n\u2217Work done while interning at Microsoft.\n1SCREWS\nstands for \u201cSampling, Conditional REsampling With Selection.\u201d Our code and results are\navailable at https://github.com/kumar-shridhar/Screws/.\n1\narXiv:2309.13075v1  [cs.AI]  20 Sep 2023\nSAMPLING\nQUESTION\nRaymond and Samantha are cousins. Raymond was born 6 years before\nSamantha. Raymond had a son at the age of 23. If Samantha is now 31,\nhow many years ago was Raymond's son born?\nIf Samantha is 31 now, and\nRaymond was born 6 years\nbefore her, then Raymond is\n31+6 = <<31+6=37>>37\nyears old. If Raymond was 23\nyears old when his son was\nborn, then the number of\nyears that have passed since\nhis son was born is 37-23 =\n<<37-23=14>> 14 years.\nCONDITIONAL RESAMPLING\nIf Samantha is 31 now, and\nRaymond was born 6 years\nbefore her, then Raymond is\n31-6 = <<31-6=25>>25\nyears old. Raymond's son was\nborn when he was 23 years\nold, so that was 23-25 =\n<<23-25=-2>>-2 years ago.\n-2 years\nSELECTION\nN\nFigure 1: An example demonstrating that Conditional Resampling (also known as \u201crefinement\u201d) can\nlead to incorrect modification of the original answer. A Selection module can decide to retract the\nmodification and instead choose the original answer, which in this case is the correct one.\nThe initial outputs generated by Sampling are passed to Conditional Resampling, which decides\nwhether to generate a revision conditioned on the initial sample, and does so if needed. Finally,\nall samples and revisions are given to the Selection module, which selects the best one. Given the\nmodular nature of our framework, several recently proposed self-refining methods can be improved\nby using other components of the framework. An example is the combination of the self-refinement\nmethod (Madaan et al., 2023) with our model-based selection strategy, which can improve overall\nperformance; more such strategies are described in section 5.\nWe evaluate SCREWS on a variety of reasoning tasks: arithmetic reasoning, multi-hop question\nanswering, and code debugging, using ChatGPT (Brown et al., 2020) or GPT-4 (OpenAI, 2023). Our\nproposed strategies achieve substantial improvements (10\u201315%) over vanilla strategies of sampling\nand resampling. We demonstrate the usefulness of heterogeneous resampling, which can help the\nmodel modify its reasoning, leading to a substantial improvement over the baselines at a very low\noverall cost. We also discuss the importance of a model-based selection strategy that allows the\nmodel to roll back to its previous more confident outputs, an important component for modern\nLLMs.\n2\nBACKGROUND\nSampling\nPrompting LLMs to generate a series of intermediate steps has proven to be effective\nfor improving their reasoning capabilities (Wei et al., 2022; Lewkowycz et al., 2022; Kojima et al.,\n2022; Wang et al., 2022). Some approaches in this direction include Chain of Thought (Wei et al.,\n2022; Zhang et al., 2022; Wang et al., 2022) and adding \u201cLet\u2019s think step by step\u201d to the prompt\n(Kojima et al., 2022). Another approach is \u201cquestion decomposition\u201d, which decomposes the main\nproblem into simpler problems and solves them iteratively (Min et al., 2019; Shridhar et al., 2022;\nZhou et al., 2022; Jhamtani et al., 2023; Radhakrishnan et al., 2023). Each of these approaches has\n2\nTOOL\nPROB\nLLM\n...\nMOST\nRECENT\nCHAIN\nOF \nTHOUGHT\nSELF-\nASK\nSELECTION\nSELF-SELECT\nMAJORITY \nVOTING\nOTHER\nLLMS\nANS\nONLY\nSUB QUES\nDECOMP\nLLM\nSAMPLING\nCONDITIONAL \nRESAMPLING\nSelf-Refine\nSelf-Consistency\nSelf-Improve\nLLMs Know (Mostly)\nLeast to Most\nPHP CoT\nSelf-Correct\nSocratic CoT\nProgram of Thoughts\nINPUT\nOUTPUT\n...\n(x)\n(y)\n...\nFigure 2: Overview of our modular framework for reasoning with revisions, SCREWS. Each of the\nthree large boxes (\u201cmodules\u201d) contains several alternatives (\u201csubmodules\u201d). A lot of past works can\nbe viewed as instances of our framework, namely Self-Refine (Madaan et al., 2023), Least to Most\n(Zhou et al., 2022), LLMs Know (Mostly) (Kadavath et al., 2022), Self-Consistency (Wang et al.,\n2022), Self-Improve (Huang et al., 2022), PHP CoT (Zheng et al., 2023), Self-Correct (Welleck\net al., 2022), Socratic CoT (Shridhar et al., 2022), Program of Thoughts (Chen et al., 2022), among\nmany others. (...) represents other sub-components that can be added to each module, like cached\nmemory or web search for Sampling, fine-tuned model or external verifier for Conditional Resam-\npling, and human- or oracle-based selection for the Selection module, among others.\nits own advantages depending on the underlying task (Shridhar et al., 2023). However, we are not\naware of work combining these methods.\nConditional Resampling\nThe use of feedback to improve generated samples has been well stud-\nied, where the feedback can come either from humans (Tandon et al., 2021; Bai et al., 2022; Elgo-\nhary et al., 2021), from reward models (Ziegler et al., 2019; Lu et al., 2022; Shridhar et al., 2022;\nChristiano et al., 2017; Lightman et al., 2023), from external tools such as code interpreters (Schick\net al., 2023; Chen et al., 2022), or from other LLMs (Madaan et al., 2023; Welleck et al., 2022; Fu\net al., 2023; Peng et al., 2023; Yang et al., 2022; Zheng et al., 2023; Cohen et al., 2023; Ling et al.,\n2023; Khalifa et al., 2023). However, even if these feedback mechanisms are infallible, the resulting\nrevisions may introduce new errors.2\n2Prior work uses the term \u201crefinement,\u201d which we do not use because refinement implies finer (improved)\nresponses, which does not always occur.\n3\nSelection\nWhen using LLMs to evaluate and revise the output, the most common selection tech-\nnique is to always select the final output (Madaan et al., 2023; Shinn et al., 2023; Zheng et al.,\n2023; Yao et al., 2022; Chen et al., 2023b; Weng et al., 2022). However, this can lead to accepting\nincorrect changes made to previously correct outputs. Other selection methods involve ranking mul-\ntiple sampled outputs (Cobbe et al., 2021) or majority voting (Wang et al., 2022; Lewkowycz et al.,\n2022; Zheng et al., 2023). These methods often use a homogeneous sampling strategy with changes\nin temperature or other similar hyper-parameters. Our work extends the strategy to heterogeneous\nsampling and selection.\n3\nSCREWS: METHODOLOGY\nIn this section, we describe SCREWS, our proposed modular framework for reasoning with revisions\nto tackle different reasoning tasks. Given a problem x, the goal is to generate an answer a, which in\nour experiments may be a string or a number. SCREWS consists of three main modules: Sampling,\nConditional Resampling, and Selection. Different variants of SCREWS are obtained by instantiating\nthese modules in different ways. The options for each module are described below and illustrated\nschematically in Figure 2.\nAll of our methods will invoke one or more stochastic functions, where each function \u03c8 maps a tuple\nof input strings to a result string y that contains useful information. In practice, \u03c8 deterministically\nconstructs a prompt from the input strings and then samples y from a large pretrained language\nmodel as a stochastic continuation of this prompt. For a given tuple of input strings, the prompt\nconstructed for \u03c8 will typically be a formatted encoding of this tuple, preceded by a task specific\ninstruction and several demonstrations (few-shot examples) that illustrate how \u03c8 should map other\nencoded input tuples to their corresponding continuations (Brown et al., 2020). For concreteness,\nthe prompts we use in our experiments are illustrated in Appendix B.\n3.1\nSAMPLING\nWe consider three instantiations of the sampling module. Different instantiations may be appropriate\nfor different tasks.\nAnswer Only\nIn this method, for a given problem x, the model \u03c8 directly generates the answer\ny = \u03c8(x) without any intermediate steps. This is the simplest and most naive sampling method.\nThe value of y is returned as the answer a (if there is no further revision of y).\nChain of Thought (CoT)\nFor many reasoning tasks today, generating explanations improves the\nquality of the final answer (Wei et al., 2022; Kojima et al., 2022). Chain of Thought sampling\nencourages the model to explain the intermediate step-by-step reasoning en route to a decision. This\napproach is now commonly used in several reasoning tasks. Again, we define y = \u03c8(x), but now\nwe expect the prompt continuation to consist of step-by-step reasoning culminating in the step by\nstep answer y, as demonstrated by the few-shot examples included in the prompt. The answer a is\nextracted from y using a simple deterministic pattern-matching heuristic.\nSub-question decomposition\nThis method decomposes the problem x into simpler sub-questions\n[x1, x2, . . . , xn]. For each sub-question xi in turn (i = 1, 2, . . . , n), the model is called to generate\nthe corresponding sub-answer yi = \u03c8(x, x1, y1, . . . , xi\u22121, yi\u22121, xi). Note that we generate all ques-\ntions before seeing any answers; that choice follows Shridhar et al. (2023), who found this approach\nto work better than interleaved generation of questions and answers. The sequence of questions\nmay be generated in a single step, either by a call to a stochastic function \u03c8question, or by a custom\nquestion generation module that has been fine-tuned on human-written questions as in Cobbe et al.\n(2021). The answer a is extracted from yn with a simple heuristic as in CoT.\n3.2\nCONDITIONAL RESAMPLING\nThe result y from the Sampling module can be viewed as a provisional result, ycurr. This is passed\nto the Conditional Resampling module where a decision is made whether or not to revise it. This\nis done in two steps: first deciding whether or not to revise, and then if so, resampling a new result\n4\nynext using one of the sampling methods mentioned above. The resampling is conditional because\nynext may depend on ycurr. While there are many methods for Conditional Resampling, our work\nfocuses on the following instantiations:\nSelf-Ask\nKadavath et al. (2022) uses a function \u03c8ask(x, ycurr). The first token of the result indicates\nwhether ycurr is correct, for example by starting with \u201cYes\u201d or \u201cNo\u201d. If \u201cYes\u201d, we do not resample; if\n\u201cNo\u201d, we must resample a revised answer ynext. In principle, the revision could be iterated, although\nKadavath et al. (2022) did not do this, nor do our experiments in this paper.\nIn our version of self-ask, \u03c8ask is formulated so that ynext appears in the result string \u03c8ask(x, ycurr)\nfollowing the word \u201cNo\u201d. Thus, both steps are efficiently performed by a single call to \u03c8ask(x, ycurr).\nFor this method, we always use greedy decoding (temperature 0), which deterministically selects\nwhichever of \u201cYes\u201d or \u201cNo\u201d is more probable.3 Demonstrations for the prompt are shown in Ap-\npendix B.2.\nWhen the sampling module (Section 3.1) used sub-question decomposition to produce a chain of\nsub-answers ycurr = [y1, . . . , yn], rather than checking and revising only the final result step yn by\ncalling \u03c8ask(x, yn), we can instead check and revise each step, at the cost of more calls to \u03c8ask.\nFor each provisional sub-answer yi in turn (starting with i = 1), we predict whether it is correct\nby calling \u03c8ask(x, x1, y1, . . . , xi\u22121, yi\u22121, xi, yi). The first time the output is \u201cNo\u201d, we resample\ny\u2032\ni through y\u2032\nn, yielding the revised result ynext = [y1, . . . , yi\u22121, y\u2032\ni, . . . , y\u2032\nn]. In principle, self-ask\ncould then be applied again at later steps > i of both the original and revised chains; then choosing\namong the many resulting chains, using the selection procedures of the next section, would resemble\nbranching in a reasoning tree (Yao et al., 2023).\nTool-Based LLM\nFor some tasks, we construct \u03c8ask so that it is allowed to use tools (Schick et al.,\n2023). The reason is that in tasks like fact-checking, it is futile to ask the LLM to check ycurr because\nit might not have the requisite knowledge for evaluation. The tools can be used to collect additional\ninformation or facts to help the model detect and fix problems in its own generated answer. Tools\nlike search engines or fact retrievers can be used to evaluate correctness and generate a new revision.\nSome other tools like code interpreters are not capable of generating text, but can still be used to\nevaluate correctness.\n3.3\nSELECTION\nThe last module in SCREWS is the Selection module. In this step, we use either a model \u03c8select or\nsimple heuristics to select the final result y from which we then extract the final answer a. In effect,\nthis allows us to construct a simple ensemble of multiple systems.\nLLM-Based Selection\nJust as an LLM was used above to evaluate whether ycurr is good, an LLM\ncan be used to evaluate whether ynext is better. We call \u03c8select(x, ycurr, ynext) to choose between two\nresult strings.4 Note that it could be naturally extended to choose among more than two answers.\nWhen selection and sampling are implemented using the same LLM, we refer to the method as\nself-select (e.g., in Figure 2). The prompts for \u03c8select in our experiments are shown in Appendix B.3.\nRule-Based Selection\nWe consider the other methods we study to be rule-based. Past work on\niterative refinement (Madaan et al., 2023; Huang et al., 2022; Zheng et al., 2023) always selects the\nmost recent revision. Majority voting is a simple traditional ensembling method that has been used\nfor selection (Wang et al., 2022; Lewkowycz et al., 2022), but it is costly because it requires several\nsamples.\n3A threshold other than 50% could be tuned to optimize the downstream reward of the whole system. This\ncompensates for bias toward the \u201cYes\u201d or \u201cNo\u201d token, and also considers how much resampling followed by\nselection will actually improve the final accuracy and harm the speed of the system. Orthogonally, the correct-\nness probability of ycurr could be assessed by a dedicated \u03c8check(x, ycurr), but we were unsuccessful with this as\n\u03c8check was poorly calibrated, mirroring findings on model calibration (Kadavath et al., 2022; Xiong et al., 2023).\n4We found that the order of ycurr and ynext in the prompt was unimportant; in our reported results, we\nrandomized this order.\n5\n3.4\nOTHER POSSIBILITIES\nThere are other possible ways to instantiate each module. Tools like web-based search or cache-\nbased retrieval could be used to generate the initial attempt in the Sampling module. A fine-tuned\nclassification model could be used to verify outputs in the Conditional Resampling module. Simi-\nlarly, a fine-tuned model could be used for the Selection module. In this paper, however, we study\nonly the instantiations described above.\n4\nEXPERIMENTS\n4.1\nTASKS\nWe test the effectiveness and flexibility of SCREWS on three categories of reasoning tasks: GSM8K\n(Cobbe et al., 2021) for arithmetic reasoning, StrategyQA (Geva et al., 2021) for multi-hop question\nanswering, and Big-Bench (BIG-bench authors, 2023) AutoDebugging5 for code debugging. The\nGSM8K dataset is a grade-school-level math word problem dataset with a test set of 1319 samples,\neach requiring two to eight steps to solve. GSM8K includes sub-questions that were generated by\na fine-tuned GPT-3 model and correspond to the steps in a particular correct CoT solution. Since\nthese sub-questions were generated with oracle knowledge of a correct CoT solution, we refer to\nexperiments using them as\u201cSubq (Or)\u201d. We use \u201cSubq (QG)\u201d for the fairer experimental condition\nwhere we instead generated the subquestions from ChatGPT using two-shot prompts (which are\nprovided in Appendix B.4).6\nFollowing Magister et al. (2023) and Shridhar et al. (2023), we test on the first 490 samples from the\ntraining set of StrategyQA (since their test set is unlabeled). The demonstration examples for our\nvarious stochastic functions \u03c8 were drawn randomly from the rest of the training set. StrategyQA\nalso includes human-annotated oracle subquestions (which we again use for \u201cSubq (Or)\u201d results) and\nrelated facts that can assist in answering the main question (which we use for tool-based conditional\nresampling as in Section 3.2). Finally, the Auto Debugging dataset tests whether a model can answer\nquestions about the intermediate state of a program without executing the code. The dataset consists\nof 34 coding examples, of which 33 were used as test examples and 1 as a demonstration example\nin the prompt.\n4.2\nEXPERIMENTAL SETUP\nWe always report exact-match accuracy: the percentage of examples on which our final answer\na matches the gold answer. For all of our experiments, we use the ChatGPT API (Brown et al.,\n2020) from July 2023 (gpt-3.5-turbo-0301). This model is a decoder-only Transformer\nLLM (Vaswani et al., 2017) that was fine-tuned using reinforcement learning with human feed-\nback (Ziegler et al., 2019; Christiano et al., 2017). Some experiments were also performed using\nGPT-4 (OpenAI, 2023) to show the scaling capabilities of our framework.\nSampling\nWith all choices of the Sampling module, we use 5-shot sampling for GSM8K and\nStrategyQA and 1-shot sampling for Auto Debugging. Greedy decoding (temp = 0) is used for the\nmain experiments while higher temperature (0.7) is used for the majority voting experiments (one\nsample was generated with temp = 0 and the other four at temp = 0.7). All prompts are provided in\nAppendix B.1.\nConditional Resampling\nGreedy decoding is used to first make a binary resampling decision\nand then to sample. 4-shot prompts (with two correct and two incorrect samples) are used for\nthe GSM8K and StrategyQA datasets, while a 2-shot prompt (with one correct and one incorrect\n5https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_\ntasks/auto_debugging/\n6Unsurprisingly, the Subq (Or) sub-questions proved to be consistently better, as we will see in Section 5. In\naddition to their oracle knowledge of a human-written answer, some of the sub-questions themselves may also\nhave been human-written: the sub-question generation model was fine-tuned on around 800 human-written\nexamples, and some of those examples may also be included in the released dataset (https://github.\ncom/openai/grade-school-math#socratic-dataset).\n6\nSampling\nConditional Resampling\nAccuracy\nCoT\n-\n71.64\nCoT\n73.00\nSubq (QG)\n73.69\nSubq (Or)\n73.99\nSubq (QG)\n-\n71.87\nCoT\n73.99\nSubq (QG)\n71.26\nSubq (Or)\n-\n78.62\nCoT\n78.99\nSubq (Or)\n78.24\nTable 1: The improvements achieved by using Conditional Resampling for the GSM8K dataset,\nwhere ynext is always selected. CoT refers to the Chain of Thought method, while Subq refers\nto the Subquestion Decomposition method. Subq (QG) refers to the case where subquestions are\ngenerated by the ChatGPT model, while Subq (Or) refers to the Oracle questions present in the\nSocratic version of the dataset.\nsample) is used for Auto Debugging. For StrategyQA, we use tool-based resampling by including\nthe provided facts from the dataset into the prompt (Appendix B.2) to simulate a (perfect) fact\nretrieval tool.\nSelection\nFor the self-select strategy, the prompts include two examples and selection was pro-\nduced with greedy decoding (prompts in Appendix B.3). For majority voting, a majority vote on the\nfinal answers was taken over k \u2208 {1, 3, 4, 5} samples. Ties were broken randomly.\n5\nRESULTS\n5.1\nGSM8K\nConditional Resampling Works Better with Method Change\nPrevious work (Madaan et al.,\n2023) has shown that when a chain-of-thought method is used for initial Sampling, reasoning ability\nis improved by Conditional Resampling with the same method. The benefit comes from taking the\nprevious sample into account.\nWe reproduced this previous finding: the CoT scores for GSM8K improved by 1.4 points after\nresampling with CoT (71.6 to 73.0), as shown in Table 1. However, when the initial Sampling\nused subquestion decomposition, we found that resampling with subquestion decomposition ac-\ntually harmed accuracy. It decreased the score by about 0.5 points (71.9 to 71.3 with generated\nsubquestions, 78.6 to 78.2 with oracle subquestions).\nWhat gave the best results\u2014for all three Sampling methods\u2014was Conditional Resampling with\na different method from the originally chosen one. It gave a large gain over Sampling when the\noriginal Sampling used CoT and Resampling used subquestion decomposition (71.6 to 73.7, with\ngenerated subquestions) and vice versa (71.9 to 74.0). Even with oracle subquestions, moderate\ngains are still seen when resampling with CoT (78.6 to 79.0). This demonstrates that it is useful to\nchange methods using Conditional Resampling, a novel finding with our framework.7\nImportance of Selection Module\nConditional Resampling does not invariably improve every out-\nput. In fact, we saw in Table 1 that for some settings, it may harm the output quality even on average.\nThis is why the Selection module is useful\u2014to detect and reject cases of harmful revisions.\nFirst, as a starting point, the left half of Table 2 considers using Selection only as an ensembling\ntechnique to combine the outputs of two independent Sampling strategies. (Note that this matrix\n7In principle, we could also use resampling and selection to combine Subq (QG) with Subq (Or); we may\ntry this in a future version of this paper.\n7\nMethod\nIndependent Sampling\nConditional Resampling\nCoT\nSubq (QG)\nSubq (Or)\nCoT\nSubq (QG)\nSubq (Or)\nCoT\n71.64\n74.90 [85.36]\n81.34 [89.08]\n72.93 [73.08]\n73.76 [73.76]\n73.99 [73.99]\nSubq (QG)\n74.90 [85.36]\n71.87\n-\n73.99 [75.43]\n72.40 [72.40]\n-\nSubq (Or)\n81.34 [89.08]\n-\n78.62\n78.99 [81.50]\n-\n79.22 [79.22]\nTable 2: Impact of Selection on the GSM8K data set on Independent Sampling and Conditional\nResampling. The upper bound from using a Selection oracle is given in square brackets.\nis symmetric.) Although CoT and subquestion decomposition are about equally good Sampling\nstrategies (71.6 and 71.9), using a Selection module to select the better of the two achieves a 3-point\ngain (to 74.9). Much larger gains (up to 85.4) are potentially available from improving Selection\u2014\nthe upper bound on performance (if Selection always chose the better option) is shown in square\nbrackets. This shows that the two Sampling strategies have largely complementary errors. A similar\npattern applies when the subquestion decomposition method is permitted to use oracle subquestions,\nwhich improves performance across the board to 81.34.\nThe right half of Table 2 shows Selection between the Sampled and Conditionally Resampled pre-\ndictions from Table 1. (This matrix is asymmetric.) For CoT, the results remain the same at 73.99,\nwhich is due to the fact that the upper bound is at 73.99, showing no room for further improvement.\nFor other cases with subquestioning, we see an improvement of up to 1 point. Finally, we observe\nthat the Selection module is far from perfect and has room for further improvement, as seen from\nthe upper bounds. A Selection method ought to look at features of the two answers that turn out to\nbe correlated with correctness, and we hypothesize that models fine-tuned specifically for Selection\nmay prove more effective than few-shot learning at identifying these features.\nThe right half of Table 2 is the cheaper method, because we observe \u03c8ask resamples on only 5-15%\nof the examples rather than all of them. A tradeoff between accuracy and cost is shown in Figure 4.\n1\n2\n3\n4\n5\nNumber of Samples\n70\n72\n74\n76\n78\n80\n82\n84\nAccuracy (%)\nCoT\nSubq(Or)\nSubq(QG)\nCoT + Subq(Or) + Subq(QG)\n(a) Impact of the number of samples on the accu-\nracy when majority voting is used as a selection\nmethod.\n3\n4\n5\n3\n4\n5\n3\nNumber of Samples\n70\n80\n90\n100\nAccuracy (%)\nCoT + maj\nCoT + oracle\nSubq(Or) + maj\nSubq(Or) + oracle\nCoT + Subq(Or) + Subq(QG) + maj\nCoT + Subq(Or) + Subq(QG) + oracle\n(b) Comparison of perfect selection (\u201c+ perfect\u201d) vs. ma-\njority voting (\u201c+ maj\u201d) for different methods of CoT and\nSubq.\nFigure 3: The + in graph (a) shows that majority voting with 3 diverse samples (CoT + Subq(Or) +\nSubq(QG)) outperforms both CoT and Subq(Or) even with 5 samples. Graph (b) shows the potential\nof the selection method when a perfect selector is used. It can be thought of as the upper bound of\nthe selection mechanism. Both figures are for the GSM8K dataset.\nSelection and Voting\nUnweighted majority vote has been one of the most popular Selection meth-\nods in past work (Wang et al., 2022; Lewkowycz et al., 2022; Zheng et al., 2023), since it requires\nno training. The two lines in Figure 3(a) generally show improvement from Sampling more times\nfrom the same model (at temperature 0.7) and Selecting by majority vote.\n8\nRecalling that the left half of Table 2 showed benefit from ensembling independent samples from\n2 different Sampling methods (up to 81.34 accuracy when oracle subquestions are allowed), we\nobserve that majority vote is a convenient way to do so for 3 different methods (where all methods\ncan now use temperature 0). This achieves 83.62 accuracy, as shown by the \u22c6 in Figure 3(a). Of\ncourse, model-based Selection could potentially do even better than majority voting. The 7 points for\nk \u2265 3 in (a) are repeated as the dark bars in Figure 3(b), with the light bars showing the upper bounds\nthat could be achieved by replacing majority voting with a perfect Selection method. The best upper\nbound corresponds again to the use of 3 different methods. In principle, one could ensemble over a\nlarger set by allowing each of the 3 methods to contribute multiple samples.\n5.2\nSTRATEGYQA\nVanilla resampling does not improve what model does not know \u2192 A need for tools\nFor the\nStrategyQA dataset, we observe in Table 3 that accuracy is harmed by Conditional Resampling with\nthe same Sampling method, without Selection, as was sometimes the case for GSM8K. On Strate-\ngyQA, however, even Selection usually does not repair the problem, perhaps because StrategyQA\nrequires multi-hop question answering. When the model lacks the necessary factual knowledge,\nSelf-Ask will be insufficient. A real example at the bottom of Figure 5 shows how resampling can\npreserve an incorrect claim generated by the model.\nTo help the model decide whether and how to revise the answer, we try including relevant facts\n(provided by StrategyQA) into the resampling prompt, as shown in Appendix B.2.1, to simulate the\nresult one may get by using an external tool like a fact retriever. As Table 3 shows, this yields a\n2-point improvement (\u201cFactsre\u201d vs. \u201cInternals\u201d) over Sampling, for both CoT and Subq (QG).\nWe assume that tool invocations are expensive, which is why we include facts only during Condi-\ntional Resampling. In practice, the initial result is revised only 10\u201335% of the time, and therefore\n\u201cFacts\u201d does not need to invoke a tool call for every input example.8 To achieve this speedup, we do\nnot include facts in the prompt when initially calling to \u03c8ask to decide whether to resample, but only\nwhen we actually generate ynext.\nMethod\nSampling\nConditional Resampling\nSelection\nKnowledge Source:\nInternals\nInternalre\nFactsre\nInts vs. Intre\nInts vs. Factsre\nStrategyQA\nCoT\n77.18\n74.54\n79.02\n75.76\n78.41\nSubq (Or)\n85.91\n78.97\n84.69\n85.30\n86.30\nSubq (QG)\n78.16\n74.69\n80.40\n78.78\n80.00\nCode Debugging\nAnswer Only\n73.52\n82.35\n88.23 [91.20]\nCoT\n70.58\n73.52\n73.52 [73.52]\nAnswer Only + CoT\n-\n-\n85.29 [88.23]\nTable 3: Comparing different strategies for the StrategyQA (top) and Big Bench Code Debugging\n(bottom) datasets. For StrategyQA, external facts are provided to the model (\u201cFacts\u201d) versus relying\non the model\u2019s internal capabilities (\u201cInternal\u201d). The numbers in square brackets indicate upper\nbound performance, assuming perfect selection. Subscripts \u201cs\u201d and \u201cre\u201d refer to Sampling and\nResampling respectively.\n5.3\nCODE DEBUGGING\nThe effectiveness of SCREWS\nFor the code debugging task, we observed that the Answer Only\nmethod achieves similar scores to CoT,9 as reported in the bottom half of Table 3, suggesting that\nno particular Sampling method is superior on all datasets. However, we see the benefits of using\n8However, if the facts were included during Sampling, the performance can increase beyond 90%.\n9We do not experiment with subquestion decomposition as subquestions are not part of this dataset.\n9\nSCREWS, as we find that with Answer Only, adding Conditional Resampling followed by Selection\nleads to a performance boost of 15 points (from 73.52 to 88.23). While the dataset size limits\nour ability to make concrete conclusions, the findings here support the conclusions drawn on other\ndatasets: Resampling and Selection lead to benefits and heterogenous sampling can prove effective.\n6\nADDITIONAL ANALYSIS\n6.1\nTOTAL COST\n1000\n2000\n3000\n4000\n5000\n6000\nInput + Output Tokens\n70\n72\n74\n76\n78\n80\n82\n84\nAccuracy (%)\nCoT\nCoT + CoT Resamp\nCoT + Subq Resamp\nSubq\nSubq + CoT Resamp\nSubq + Subq Resamp\nCoT + Subq -> Sel\nFigure 4: On GSM8K, sampling cost vs. accuracy.\nThe blue line (copied from Figure 3(a)) shows a base-\nline of majority voting over k \u2208 {1, 3, 4, 5} CoT\nsamples. The shaped points are the other strategies\nfrom Section 5.1 that use CoT and Subq (QG).\nSCREWS supports many methods with dif-\nferent cost/accuracy tradeoffs. Figure 4 dis-\nplays the strategies that use CoT and Subq\n(QG) on GSM8K. The cost is represented\nas the total count of input tokens (prompt +\nquery) and output tokens for all LLM calls\nneeded by that strategy, averaged over test\nexamples. Generally, Subq (QG) is expen-\nsive as it is costly to call \u03c8question.\nHow-\never, it is affordable to use it in Conditional\nResampling only (\u25a0), since resampling only\noccurs 10\u201315% of the time. This method is\nboth cheaper and more accurate than Sam-\npling either with Subq (QG) (+) or 3 times\nwith CoT (\u2022). Appendix A discusses a de-\ntailed breakdown of each module\u2019s input and\noutput token costs.\n6.2\nMORE REVISION STEPS\nWe saw in Section 5.1 on GSM8K that Sampling with Subq (Or) (78.62 accuracy) is improved\nslightly by Conditional Resampling with CoT (78.99) and then Selection (79.22). Like Madaan\net al. (2023), we did not find much benefit from additional iterations of Conditional Resam-\npling+Selection: a second iteration gives 79.45, and a third gives 79.52. These small improvements\nprobably do not justify the added cost.\n6.3\nLARGER LLMS\nReplacing ChatGPT with GPT-4 greatly increased the Sampling accuracy on GSM8K, to 91.45 for\nCoT and 90.80 for Subq (Or). Choosing between those two samples with GPT-4-based Selection\nfurther increased the accuracy to 93.10, which falls between the accuracy of majority voting over\nk = 3 and k = 4 CoT samples from GPT-4 (92.94 and 93.93 respectively). Even using ChatGPT-\nbased Selection achieved 92.58, which is still an improvement over CoT alone.\n6.4\nSELECTED EXAMPLES\nThe top two examples of Figure 5, on the GSM8K dataset, demonstrate the effectiveness of the\nSelection module. The first example shows how an error introduced by Conditional Resampling\ncan be reverted by Selection. The second example shows how a correction found by Conditional\nResampling can be kept by Selection.\nThe last example in Figure 5, on the StrategyQA dataset, illustrates that ordinary Resampling is\nunlikely to correct an incorrect fact generated by the LLM. However, providing the correct facts\nduring Resampling gives the model access to new information, leading to the correct answer.\n10\nQuestion Gunter is trying to count the jelly beans in a jar. He asks his friends how many they think are in the jar. One says 80. Another\nsays 20 more than half the first one. A third says 25% more than the first one. What is their average guess?\nSampling (A) The second person guesses 20 + 1/2 \u2217 80 = <<20 + 1/2 \u2217 80 = 60>>60 jelly beans. The third person guesses\n80 \u2217 1.25 = <<80 \u2217 1.25 = 100>>100 jelly beans. The total number of jelly beans is 80 + 60 + 100 = <<80 + 60 + 100 =\n240>>240. The average guess is 240/3 = <<240/3 = 80>>80. The answer is 80.\u2713\nConditional Resampling (B) The average guess is (80+60+1.25\u221780)/3 = <<(80+60+1.25\u221780)/3 = 85>>85 jelly beans.\nThe second person guesses 80/2 + 20 = <<80/2 + 20 = 60>>60 jelly beans. The average guess is (80 + 60 + 1.25 \u2217 80)/3 =\n<<(80 + 60 + 1.25 \u2217 80)/3 = 85>>85 jelly beans. The answer is 85. \u2717\nSelection (A) \u2713\nQuestion Adam has $100 and wants to spend it to open a rock stand. He can buy rocks for $5 each and sell them for $7 each. If he\ninvests all his money in the rock stand but only sells 60% of his inventory, how much money does he lose?\nSampling (A) Adam can buy $100/$5 per rock = <<100/5 = 20>>20 rocks. Adam sells 60/100 x 20 rocks = <<60/100 \u2217 20 =\n12>>12 rocks. Adam makes $7 x 12 rocks = $<<7 \u2217 12 = 84>>84. Adam loses $100 - $5 x 20 rocks = $<<100 \u2212 5 \u2217 20 = 0>>0.\nHe invested all his money and sold all the rocks he bought, so he didn\u2019t lose any money. The answer is 0. \u2717\nConditional Resampling (B) Adam can buy $100/$5 per rock = <<100/5 = 20>>20 rocks. Adam sells 60/100 x 20 rocks =\n<<60/100 \u2217 20 = 12>>12 rocks. Adam makes $7 x 12 rocks = $<<7 \u2217 12 = 84>>84. However, he spent $5 x 20 rocks =\n$<<5 \u2217 20 = 100>>100, so he loses $100 - $84 = $<<100 \u2212 84 = 16>>16. The answer is 16. \u2713\nSelection (B) \u2713\nQuestion: Would Alexander Hamilton have known about koalas?\nSample: Alexander Hamilton died in 1804. The first koala specimen was collected in 1792. It is possible that Hamilton knew about\nkoalas, but it is unlikely. The answer is Unknown \u2717\nConditional Resampling: Alexander Hamilton died in 1804. The first koala specimen was collected in 1792. It is possible that\nHamilton knew about koalas. The answer is True \u2717\nFacts: Alexander Hamilton died in 1804. The first published depiction of a koala was in 1810\nTool-based Resample: No, Alexander Hamilton would not have known about koalas. The answer is False \u2713\nFigure 5: The top two examples demonstrate the importance of the Selection module for the GSM8K\ndataset. The last example shows how tool use (\u201cFacts\u201d) can be helpful for the StrategyQA dataset.\n7\nDISCUSSION\n7.1\nKEY FINDINGS\nBased on our experiments with three reasoning datasets using our framework, we conclude the\nfollowing:\n\u2022 Selection plays an important role: Although Conditional Resampling often improves the\nresult of Sampling, Selection can help avoid errors from the case where it does not. It was\nbeneficial on all three datasets.\n\u2022 Heterogeneous vs. homogeneous resampling: Using different reasoning methods for\nSampling and Conditional Resampling can lead to higher accuracy, with or without Se-\nlection.\n\u2022 Missing external knowledge hurts Conditional Resampling: Resampling cannot fix in-\ncorrect facts generated by the model. Tool-based resampling can therefore get better results\n(as simulated using StrategyQA).\n\u2022 No uniformly best strategy: There was no clear winning method for each of the modules.\nSimple baseline methods sometimes beat more complex ones: CoT uses only one call to\n\u03c8 and beats Subq (QG) in GSM8K, always selecting ynext beats self-select for StrategyQA\nwith \u201cFacts,\u201d and Answer Only works surprisingly well for Code Debugging.\n7.2\nFUTURE WORK\nSCREWS combines the three important modules Sampling, Conditional Resampling and Selection in\na modular framework. The best configuration of modules will vary by task and could be identified\nthrough a method such as exhaustive search, Monte Carlo Tree Search, or reinforcement learning.\nThe modules themselves could be fine-tuned to improve end-to-end performance.\n11\nIf we want to optimize cost along with accuracy, (Chen et al., 2023a) proposed several methods for\nspeeding up the stochastic functions \u03c8. Their \u201cLLM Cascade\u201d strategy in particular is a heteroge-\nneous (but unconditional) resampling method that starts with smaller, cheaper models.\nIt is possible that for some reasoning tasks, additional modules could be useful. For instance, Re-\nsampling or Selection might be preceded by Critiquing, or Selection might be generalized to Com-\nbination.\n7.3\nCONCLUSION\nWe have proposed SCREWS, a modular reasoning-with-revisions framework to answer reason-\ning questions with LLMs. We demonstrated the usefulness of the three main components of the\nframework\u2014Sampling, Conditional Resampling, and Selection\u2014on three reasoning datasets. The\nflexible nature of our framework allows it to be configured for each task and extended to other tasks\nin the future.\n12\nREFERENCES\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav Kadavath, John\nKernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez,\nTristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario\nAmodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and\nJared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human\nfeedback. ArXiv, abs/2204.05862, 2022.\nBIG-bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of\nlanguage models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei.\nLanguage models are few-shot learners.\nIn H. Larochelle, M. Ranzato,\nR. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,\nvolume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020.\nLingjiao Chen, Matei A. Zaharia, and James Y. Zou. FrugalGPT: How to use large language models\nwhile reducing cost and improving performance. ArXiv, abs/2305.05176, 2023a.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks. ArXiv, abs/2211.12588,\n2022.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00a8arli, and Denny Zhou. Teaching large language models\nto self-debug. ArXiv, abs/2304.05128, 2023b.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wal-\nlach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Process-\ning Systems, volume 30. Curran Associates, Inc., 2017.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021.\nRoi Cohen, May Hamri, Mor Geva, and Amir Globerson. LM vs LM: Detecting factual errors via\ncross examination. ArXiv, abs/2305.13281, 2023.\nAhmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and\nAhmed Hassan Awadallah. NL-EDIT: Correcting semantic parse errors through natural language\ninteraction. In Proceedings of the 2021 Conference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language Technologies, pp. 5599\u20135610, Online,\nJune 2021. Association for Computational Linguistics.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. GPTScore: Evaluate as you desire.\nArXiv, abs/2302.04166, 2023.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\nuse a laptop? A question answering benchmark with implicit reasoning strategies. Transactions\nof the Association for Computational Linguistics, 9:346\u2013361, 2021.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei\nHan. Large language models can self-improve. ArXiv, abs/2210.11610, 2022.\nHarsh Jhamtani, Hao Fang, Patrick Xia, Eran Levy, Jacob Andreas, and Benjamin Van Durme. Nat-\nural language decomposition and interpretation of complex utterances. ArXiv, abs/2305.08677,\n2023.\n13\nSaurav Kadavath, Tom Conerly, Amanda Askell, T. J. Henighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zachary Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk,\nAndy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav\nFort, Deep Ganguli, Danny Hernandez, Josh Jacobson, John Kernion, Shauna Kravec, Liane\nLovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom B. Brown, Jack\nClark, Nicholas Joseph, Benjamin Mann, Sam McCandlish, Christopher Olah, and Jared Kaplan.\nLanguage models (mostly) know what they know. ArXiv, abs/2207.05221, 2022.\nMuhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Ho Hin Lee, and Lu Wang.\nDiscriminator-guided multi-step reasoning with language models. ArXiv, abs/2305.14934, 2023.\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,\nK. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp.\n22199\u201322213. Curran Associates, Inc., 2022.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo,\nYuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning\nproblems with language models. ArXiv, abs/2206.14858, 2022.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee,\nJan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. ArXiv,\nabs/2305.20050, 2023.\nZ. Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su.\nDeductive verification of Chain-of-Thought reasoning. ArXiv, abs/2306.03872, 2023.\nXiming Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Am-\nmanabrolu, and Yejin Choi.\nQuark: Controllable text generation with reinforced unlearning.\nArXiv, abs/2205.13636, 2022.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Ma-\njumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-Refine: Iterative refinement\nwith self-feedback. ArXiv, abs/2303.17651, 2023.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn.\nTeaching small language models to reason. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers), pp. 1773\u20131781, Toronto,\nCanada, July 2023. Association for Computational Linguistics.\nSewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. Multi-hop reading com-\nprehension through question decomposition and rescoring. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pp. 6097\u20136109, Florence, Italy, July\n2019. Association for Computational Linguistics.\nOpenAI. GPT-4 technical report. ArXiv, abs/2303.08774, 2023.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars\nLid\u00b4en, Zhou Yu, Weizhu Chen, and Jianfeng Gao. Check your facts and try again: Improving\nlarge language models with external knowledge and automated feedback. ArXiv, abs/2302.12813,\n2023.\nAnsh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson E. Denison, Danny Hernan-\ndez, Esin Durmus, Evan Hubinger, John Kernion, Kamil.e Lukovsiut.e, Newton Cheng, Nicholas\nJoseph, Nicholas Schiefer, Oliver Rausch, Sam McCandlish, Sheer El Showk, Tamera Lanham,\nTim Maxwell, Venkat Chandrasekaran, Zac Hatfield-Dodds, Jared Kaplan, Janina Brauner, Sam\nBowman, and Ethan Perez. Question decomposition improves the faithfulness of model-generated\nreasoning. ArXiv, abs/2307.11768, 2023.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. ArXiv, abs/2302.04761, 2023.\n14\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu\nYao. Reflexion: Language agents with verbal reinforcement learning, 2023.\nKumar Shridhar, Jakub Macina, Mennatallah El-Assady, Tanmay Sinha, Manu Kapur, and Mrin-\nmaya Sachan. Automatic generation of socratic subquestions for teaching math word problems.\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\npp. 4136\u20134149, Abu Dhabi, United Arab Emirates, December 2022. Association for Computa-\ntional Linguistics.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling reasoning capabilities into\nsmaller language models. In Findings of the Association for Computational Linguistics: ACL\n2023, pp. 7059\u20137073, Toronto, Canada, July 2023. Association for Computational Linguistics.\nNiket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, and Yiming Yang. Interscript: A\ndataset for interactive learning of scripts through error feedback. ArXiv, abs/2112.07867, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural\nInformation Processing Systems, volume 30. Curran Associates, Inc., 2017.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-\nConsistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171,\n2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V.\nLe, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural\nInformation Processing Systems, volume 35, pp. 24824\u201324837. Curran Associates, Inc., 2022.\nSean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. Generating sequences by learning to self-correct. ArXiv, abs/2211.00053, 2022.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. Large language\nmodels are better reasoners with self-verification. volume abs/2212.09561, 2022.\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs\nexpress their uncertainty?\nan empirical evaluation of confidence elicitation in llms.\nArXiv,\nabs/2306.13063, 2023.\nKevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with\nrecursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pp. 4393\u20134479, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReAct: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv,\nabs/2305.10601, 2023.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alexander J. Smola. Automatic chain of thought\nprompting in large language models. ArXiv, abs/2210.03493, 2022.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-Hint Prompting\nimproves reasoning in large language models. ArXiv, abs/2304.09797, 2023.\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schu-\nurmans, Olivier Bousquet, Quoc Le, and Ed Huai hsin Chi. Least-to-most prompting enables\ncomplex reasoning in large language models. ArXiv, abs/2205.10625, 2022.\nDaniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv,\nabs/1909.08593, 2019.\n15\nA\nTOKEN COST\nTable A shows the token cost of input and output for each module in SCREWS. Due to its iterative\nnature, subquestion decomposition requires on average four times more input tokens than the other\nmodules. For Conditional Resampling, the model first predicts whether it wants to modify its output\nor not, using one token (\u201cYes\u201d or \u201cNo\u201d) for each sample and then only for the answers starting with\n\u201cNo\u201d, it resamples. For the Selection module, the model chooses one of the two samples presented\nto it, using one token (A or B) for the output.\nMethod\nInput Tokens\nOutput Tokens\nTotal Tokens\nSubquestion generation step \u03c8question\nSubq (QG)\n360\n180\n540\nSampling step \u03c8\nCoT\n774\n307\n1081\nCoT (k = 5)\n3870\n1530\n5400\nSubq (Or)\n3187\n413\n3600\nSubq (QG)\n3121\n434\n3555\nConditional Resampling step \u03c8ask\nCoT\n869\n105\n1184\nSubq (Or)\n3525\n131\n3656\nSubq (QG)\n3780\n136\n3916\nSelection step \u03c8select\nSelection\n1296\n1\n1297\nTable 4: Average counts of input and output tokens for each choice of each module (step) in\nSCREWS. Many of the methods in Table 1 need to call multiple modules. We remark that the input\ntokens at each step include output tokens from previous steps. The counts shown for later steps\naverage not only over examples, but also over choices of method for the previous steps.\nB\nPROMPTS\nBelow are abbreviated versions of the prompts used in the experiments, including instructions and\ndemonstrations. For readability, we show only 1\u20132 demonstrations in each prompt. In each demon-\nstration, the demonstrated result string is highlighted for the reader\u2019s convenience, but this highlight-\ning is not included in the prompt. Each prompt shown would be followed by the test question and\nthen the cue (e.g., \u201cAnswer:\u201d) that indicates that a result string should follow.\nB.1\nSAMPLING\nFor Chain of Thought (CoT) and Subquestion Decomposition for GSM8K and StrategyQA, 5-shot\nprompts were used. For Auto Debugging, a 1-shot prompt was used.\nB.1.1\nCHAIN OF THOUGHT\nGSM8K\nI am a highly intelligent question answering bot. I will answer the last question \u2018Question\u2019 providing equation\nin << >> format in step by step manner.\nQuestion: James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a\nyear?\nAnswer:\nHe\nwrites\neach\nfriend\n3 \u2217 2 = <<3 \u2217 2 = 6>>6\npages\na\nweek.\nSo\nhe\nwrites\n6 \u2217 2 = <<6 \u2217 2 = 12>>12 pages every week. That means he writes 12 \u2217 52 = <<12 \u2217 52 = 624>>624\npages a year. The answer is 624\n16\nStrategyQA\nYou are a highly intelligent question answering bot. You will answer the question \u2018Question\u2019 in as details as\npossible.\nQuestion: Is coal needed to practice parachuting?\nAnswer: Parachuting requires a parachute. Parachutes are made from nylon. Nylon is made from coal. The\nanswer is True\nAuto Debugging\nAnswer the \u2019Question\u2019 based on the provided code and provide explanation.\nQuestion:\ndef f1():\nreturn str(x) + 'hello'\ndef f2():\nreturn f1(2*x)\nx = f2(524)\nWhat is the value of x at the end of this program?\nOutput: First, x = 2 * 524 = 1048 and then \u2018hello\u2019 is appended to it. So x becomes 1048hello\nB.1.2\nSUBQUESTION DECOMPOSITION\nWhile subquestion decomposition uses a single prompt, each example requires multiple API calls\nbecause the next subquestion needs to be appended to the prompt.\nGSM8K\nI am a highly intelligent question answering bot. I will answer the last question \u2018Q\u2019 providing equation in <<\n>> format keeping the Problem and previous Q and A into account.\nProblem: There are 5 houses on a street, and each of the first four houses has 3 gnomes in the garden. If there\nare a total of 20 gnomes on the street, how many gnomes does the fifth house have?\nQ: How many gnomes are in the first four houses?\nA: In the first four houses, there are a total of 4 houses * 3 gnomes = <<4 \u2217 3 = 12>>12 gnomes. The answer\nis 12\nQ: How many gnomes does the fifth house have?\nA: Therefore, the fifth house had 20 total gnomes - 12 gnomes = <<20 \u2212 12 = 8>>8 gnomes. The answer is 8\nStrategyQA\nYou are a highly intelligent question answering bot. You will answer the last question \u2018Q\u2019 keeping the Problem\nand previous Q and A into account and then answer the Final Question based on all the previous answer \u2018A\u2019.\nProblem: Is coal needed to practice parachuting?\nQ: What is one of the most important item that you need to go parachuting?\nA: Parachuting requires a parachute.\nQ: What is #1 made out of?\nA: Parachutes are made from nylon.\nQ: Is #2 originally made from coal?\nA: Nylon is made from coal.\nFinal Question: Is coal needed to practice parachuting?\nFinal Answer: True\nB.1.3\nANSWER ONLY\nAnswer Only was only used for Auto Debugging in a 1-shot manner and that one example is pro-\nvided below:\nInput:\ndef f1():\nreturn str(x) + 'hello'\n17\ndef f2():\nreturn f1(2*x)\nx = f2(524)\nWhat is the value of x at the end of this program?\nOutput: 1048hello\nB.2\nCONDITIONAL RESAMPLING\nWe used 4-shot prompting for conditional resampling (two positive and two negative) for GSM8K\nand StrategyQA and 2-shot (one positive and one negative) for Auto Debugging. One pair of each\nis presented below:\nB.2.1\nCHAIN OF THOUGHT\nGSM8K\nQuestion: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How\nmany clips did Natalia sell altogether in April and May?\nAnswer: In April, Natalia sold 48 clips. In May, Natalia sold half of the toal clips meaning 72/2<<72/2 =\n36>>36 clips. So, Natalia sold a total of 48 + 36<<48 + 36 = 84>>84 clips in April and May. The answer is\n84\nYou are a math teacher. Do you think the reasoning process for the given problem is correct? Let\u2019s check the\n\u2018Answer\u2019 in details, and then decide \u2018Yes\u2019 or \u2018No\u2019 and then write the correct \u2018Final Answer\u2019.\nAnswer: No, the reasoning is incorrect. Final Answer: Natalia sold 48/2 = <<48/2 = 24>>24 clips in May.\nSo she sold a total of 48 + 24<<48 + 24 = 72>>72 clips. The answer is 72\nQuestion: There are 5 houses on a street, and each of the first four houses has 3 gnomes in the garden. If there\nare a total of 20 gnomes on the street, how many gnomes does the fifth house have?\nAnswer:In the first four houses, there are a total of 4 houses * 3 gnomes = <<4 \u2217 3 = 12>>12 gnomes.\nTherefore, the fifth house had 20 total gnomes - 12 gnomes = <<20 \u2212 12 = 8>>8 gnomes. The answer is 8\nYou are a math teacher. Do you think the reasoning process for the given problem is correct? Let\u2019s check the\n\u2018Answer\u2019 in details, and then decide \u2018Yes\u2019 or \u2018No\u2019 and then write the correct \u2018Final Answer\u2019.\nAnswer: Yes, the reasoning is correct. Final Answer: In the first four houses, there are a total of 4 houses\n* 3 gnomes = <<4 \u2217 3 = 12>>12 gnomes. Therefore, the fifth house had 20 total gnomes - 12 gnomes\n= <<20 \u2212 12 = 8>>8 gnomes. The answer is 8\nStrategyQA\nQuestion: Could Durian cause someone\u2019s stomach to feel unwell?\nAnswer: Durian has a pungent odor that many people describe as being similar to feet and onions. Unpleasant\nsmells can make people feel excited and they like it. The answer is False\nYou are an expert teacher. Do you think the reasoning process for the given problem is correct? Let\u2019s check\nthe \u2018Answer\u2019 in details, and then decide \u2018Yes\u2019 or \u2018No\u2019 and then write the correct \u2018Final Answer\u2019.\nAnswer: No, the reasoning is incorrect. Final Answer: Durian has a pungent odor that many people describe\nas being similar to feet and onions. Unpleasant smells can make people feel nauseous. The answer is True\nQuestion: Was Daniel thrown into the lion\u2019s den in the New Testament?\nAnswer:The Book of Daniel is a book in the Old Testament of the Bible. The Bible is divided into the Old\nTestament and the New Testament. The New Testament focuses on four Gospels regarding the life of Jesus.\nThe answer is False\nYou are an expert teacher. Do you think the reasoning process for the given problem is correct? Let\u2019s check\nthe \u2018Answer\u2019 in details, and then decide \u2018Yes\u2019 or \u2018No\u2019 and then write the correct \u2018Final Answer\u2019.\nAnswer: Yes, the reasoning is correct. Final Answer: The Book of Daniel is a book in the Old Testament of\nthe Bible. The Bible is divided into the Old Testament and the New Testament. The New Testament focuses on\nfour Gospels regarding the life of Jesus. The answer is False\nStrategyQA (Resampling with facts)\nYou are a highly intelligent question answering bot. You will answer the question \u2019Question\u2019 in as details as\npossible. \u2019Facts\u2019 are provided to assist you in answering the questions.\nQuestion: Are vinegar pickled cucumbers rich in lactobacillus?\nFacts: Pickles made with vinegar are not probiotic and are simply preserved. Pickles made through a soak in a\n18\nsalt brine solution begin to ferment because of lactobacillus.\nAnswer: No, vinegar does not contain lactobacillus. The answer is False\nQuestion: Does Masaharu Morimoto rely on glutamic acid?\nFacts: Masaharu Morimoto is a Japanese chef.\nJapanese cuisine relies on several forms of seaweed as\ningredients and flavorings for broth like kombu dashi. Glutamic acid has been identified as the flavoring\ncomponent in kombu seaweed.\nAnswer: Yes, Japanese chef uses a lot of glutamic acid. The answer is True\nAuto Debugging\nInput:\ndef f1():\nreturn str(x) + 'hello'\ndef f2():\nreturn f1(2*x)\nx = f2(524)\nWhat is the value of x at the end of this program?\nOutput: 1048hello\nVerdict: Yes, the answer is correct.\nFinal Answer: 1048hello\nInput:\ndef f1():\nreturn str(x) + 'hello'\ndef f2():\nreturn f1(2*x)\nx = f2(524)\nWhat is the value of x at the end of this program?\nOutput: 524\nVerdict: No, the answer is incorrect.\nFinal Answer: 1048hello\nB.2.2\nSUBQUESTION DECOMPOSITION\nGSM8K\nFor each subquestion, the main problem and all previous subquestions along with the model-generated\nsolutions are provided in order to solve the current subquestion.\nHere is a math question and its solution.\nProblem: Noah is a painter. He paints pictures and sells them at the park. He charges $60 for a large painting\nand $30 for a small painting. Last month he sold eight large paintings and four small paintings. If he sold twice\nas much this month, how much is his sales for this month?\nHow much did Noah earn from the large paintings? Noah earned $60/large painting x 8 large paintings =\n$<<60 \u2217 8 = 480>>480 for the large paintings. The answer is 480\nQuestion: How much did Noah earn from the small paintings?\nAnswer: He also earned $60/small painting x 4 small paintings = $<<60 \u2217 4 = 240>>240 for the small\npaintings. The answer is 240\nYou are a math teacher. Do you think the reasoning process for the given problem is correct? Let\u2019s check the\n\u2018Answer\u2019 in details, and then decide \u2018Yes\u2019 or \u2018No\u2019 and then write the correct \u2018Final Answer\u2019.\nAnswer: No, the reasoning is incorrect. Final Answer: He also earned $30/small painting x 4 small paintings =\n$<<30 \u2217 4 = 120>>120 for the small paintings. The answer is 120\nHere is a math question and its solution.\nProblem: To make pizza, together with other ingredients, Kimber needs 10 cups of water, 16 cups of flour, and\n1/2 times as many teaspoons of salt as the number of cups of flour. Calculate the combined total number of\ncups of water, flour, and teaspoons of salt that she needs to make the pizza.\nHow many teaspoons of salt does Kimber need? To make the pizza, Kimber half as many teaspoons of salt as\n19\nthe number of cups of flour, meaning she needs 1/2*16 = <<16 \u2217 1/2 = 8>>8 teaspoons of salt. The answer is\n8\nHow many cups of flour and teaspoons of salt does Kimber need? The total number of cups of flour and\nteaspoons of salt she needs is 8+16 = <<8 + 16 = 24>>24. The answer is 24\nQuestion: How many cups of water, flour, and salt does Kimber need?\nAnswer: She also needs 10 cups of water, which means the total number of cups of water and flour and\nteaspoons of salt she needs is 24 + 10 = <<24 + 10 = 34>>34. The answer is 34\nYou are a math teacher. Do you think the reasoning process for the given problem is correct? Let\u2019s check the\n\u2018Answer\u2019 in details, and then decide \u2018Yes\u2019 or \u2018No\u2019 and then write the correct \u2018Final Answer\u2019.\nAnswer: Yes, the reasoning is correct. Final Answer: She also needs 10 cups of water, which means the total\nnumber of cups of water and flour and teaspoons of salt she needs is 24 + 10 = <<24 + 10 = 34 > >>34.\nThe answer is 34\nStrategyQA\nHere is a question and its answer.\nContext: Would a diet of ice eventually kill a person?\nIce is the solid state of what? Ice can be melted into water, which consists of hydrogen and oxygen.\nWhat nutrients are needed to sustain human life? Humans need carbohydrates, proteins, and fats that are\ncontained in foods.\nQuestion: Are most of #2 absent from #1?\nAnswer: Water does not contain fat, carbohydrates or protein.\nYou are an expert teacher. Based on the provided context, do you think the reasoning process for the given\nproblem is correct? Let\u2019s check the \u2018Answer\u2019 in details, and then decide \u2018Yes\u2019 or \u2018No\u2019 and then write the\ncorrect \u2018Final Answer\u2019.\nAnswer: Yes, the reasoning is correct. Final Answer: Water does not contain fat, carbohydrates or protein.\nHere is a question and its answer.\nContext: Can binary numbers and standard alphabet satisfy criteria for a strong password?\nWhich characters make up binary numbers? Binary numbers only contain 0 and 1.\nWhich characters make up the standard English alphabet? The standard alphabet contains twenty six letters\nbut no special characters.\nQuestion: Does #1 or #2 include special characters or symbols?\nAnswer: Yes, it contains all the special characters.\nYou are an expert teacher. Based on the provided context, do you think the reasoning process for the given\nproblem is correct? Let\u2019s check the \u2018Answer\u2019 in details, and then decide \u2018Yes\u2019 or \u2018No\u2019 and then write the\ncorrect \u2018Final Answer\u2019.\nAnswer: No, the reasoning is incorrect. Final Answer: Neither binary digits nor English alphabets consists of\nany special characters which is needed for a strong password.\nB.3\nSELECTION\nThe LLM-based selection module \u03c8select uses a 2-shot prompt. The 2 demonstrations in the prompt\nare shown below, for each dataset.\nGSM8K\nYou are an expert math teacher. You are provided with a question and two answers. Lets check the \u2018Answer\nchoices\u2019 step by step, and then decide which answer is correct \u2018(A)\u2019 or \u2018(B)\u2019\nQuestion: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How\nmany clips did Natalia sell altogether in April and May?\nAnswer choices:\n(A) In April, Natalia sold 48 clips. In May, Natalia sold 24 clips. So, Natalia sold a total of 72 clips in\nApril and May.\nThe answer is 72.\nSo in May she sold 48 clips.\nTotal clips sold in April and May =\n72 + 48 = <<72 + 48 = 120>>120. The answer is 120\n(B) Natalia sold 48/2\n=\n<<48/2\n=\n24>>24 clips in May.\nThe answer is 24.\nNatalia sold\n48 + 24 = <<48 + 24 = 72>> clips altogether. The answer is 72\nAnswer: (B)\nYou are an expert math teacher. You are provided with a question and two answers. Lets check the \u2018Answer\nchoices\u2019 step by step, and then decide which answer is correct \u2018(A)\u2019 or \u2018(B)\u2019\n20\nQuestion: Dolly has two books. Pandora has one. If both Dolly and Pandora read each others\u2019 books as well\nas their own, how many books will they collectively read by the end?\nAnswer choices:\n(A) There are a total of 2 + 1 = <<2 + 1 = 3>>3 books. The answer is 3. Dolly and Pandora both read all 3\nbooks, so 3 books/person x 2 people = <<3 \u2217 2 = 6>>6 books total. The answer is 6\n(B) The total number of books are 2 \u2217 1 = <<2 \u2217 1 = 2>>2 books. The answer is 2. Dolly and Pandora read\neach other\u2019s books as well as their own, so the total number of books they read is 3 books. The answer is 3\nAnswer: (A)\nStrategyQA\nYou are the expert in the field. You are provided with a question and two answers. Lets check the reasoning\nprocess of each of the answer step by step, and then decide which answer is correct \u2018(A)\u2019 or \u2018(B)\u2019\nQuestion: Could Durian cause someone\u2019s stomach to feel unwell?\nAnswer choices:\n(A) Durian has a pungent odor that many people describe as being similar to feet and onions. Unpleasant\nsmells can make people feel nauseous. The answer is True\n(B) Durian has a pungent odor that many people describe as being similar to feet and onions. Unpleasant\nsmells can make people feel excited and they like it. The answer is False\nAnswer: (A)\nYou are the expert in the field. You are provided with a question and two answers. Lets check the reasoning\nprocess of each of the answer step by step, and then decide which answer is correct \u2018(A)\u2019 or \u2018(B)\u2019\nQuestion: Was Daniel thrown into the lion\u2019s den in the New Testament?\nAnswer choices:\n(A) The Book of Daniel is a book in the New Testament of the Bible. The Bible is divided into the Old\nTestament and the New Testament. The New Testament focuses on the life of Daniel. The answer is True\n(B) The Book of Daniel is a book in the Old Testament of the Bible. The Bible is divided into the Old\nTestament and the New Testament. The New Testament focuses on four Gospels regarding the life of Jesus.\nThe answer is False\nAnswer: (B)\nAuto Debugging\nYou are an expert Python debugger. You are provided with a question and two answers. Your job is to decide\nwhich answer is correct \u2018(A)\u2019 or \u2018(B)\u2019\nQuestion:\ndef f1():\nreturn str(x) + 'hello'\ndef f2():\nreturn f1(2*x)\nx = f2(524)\nWhat is the value of x at the end of this program?\nAnswer choices:\n(A) 524hello\n(B) 1048hello\nAnswer: (B)\nB.4\nQUESTION GENERATION\n5-shot prompts were used for generating subquestions for GSM8K dataset. An example is provided\nbelow:\nGSM8K\nI am a highly intelligent question generation bot. I will take the given question \u2018Q\u2019 and will decompose the\nmain question into all \u2018subquestions\u2019 required to solve the question step by step.\n21\nQ: James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?\nSubquestions: How many pages does he write each week? How many pages does he write every week? How\nmany pages does he write a year?\nStrategyQA\nI am a highly intelligent question generation bot. I will take the given question \u2018Q\u2019 and will decompose the\nmain question into all \u2018subquestions\u2019 required to solve the question step by step.\nQ: Can you buy Casio products at Petco?\nSubquestions: What kind of products does Casio manufacture? What kind of products does Petco sell? Does\n#1 overlap with #2?\n22\n"
  },
  {
    "title": "Calibrating LLM-Based Evaluator",
    "link": "https://arxiv.org/pdf/2309.13308.pdf",
    "upvote": "10",
    "text": "Preprint\nCALIBRATING LLM-BASED EVALUATOR\nYuxuan Liu\u2020\u2217, Tianchi Yang\u2021, Shaohan Huang\u266f\u2021, Zihan Zhang \u2021, Haizhen Huang\u2021,\nFuru Wei\u2021, Weiwei Deng\u2021, Feng Sun\u2021, Qi Zhang\u2021\n\u2020 Peking University \u2021 Microsoft Corporation\nyx.liu@stu.pku.edu.cn\nABSTRACT\nRecent advancements in large language models (LLMs) on language modeling\nand emergent capabilities make them a promising reference-free evaluator of nat-\nural language generation quality, and a competent alternative to human evaluation.\nHowever, hindered by the closed-source or high computational demand to host and\ntune, there is a lack of practice to further calibrate an off-the-shelf LLM-based\nevaluator towards better human alignment. In this work, we propose AUTOCALI-\nBRATE, a multi-stage, gradient-free approach to automatically calibrate and align\nan LLM-based evaluator toward human preference. Instead of explicitly model-\ning human preferences, we first implicitly encompass them within a set of human\nlabels. Then, an initial set of scoring criteria is drafted by the language model\nitself, leveraging in-context learning on different few-shot examples. To further\ncalibrate this set of criteria, we select the best performers and re-draft them with\nself-refinement. Our experiments on multiple text quality evaluation datasets il-\nlustrate a significant improvement in correlation with expert evaluation through\ncalibration. Our comprehensive qualitative analysis conveys insightful intuitions\nand observations on the essence of effective scoring criteria.1.\n1\nINTRODUCTION\nThe emergence of large language models is calling on a greater focus and importance on the quality\nof natural language generation evaluation. With the rapid improvement of language models, their\ngoals have gone beyond simply fitting its output to a number of given samples to a broader human\nalignment. Traditional evaluation metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004)\nand CIDEr (Vedantam et al., 2015) often require curated reference outputs, whose application is\nlimited when the output space is open and diversified, and show a low correlation with human judg-\nments (Freitag et al., 2022). While sophisticated model-based evaluators like BERTScore (Zhang*\net al., 2020) and COMET (Rei et al., 2020) yield correlation improvements, their performance is\nstill limited by the quality of references. As a result, there is a surging demand for human-aligned,\nreference-free evaluators for NLG evaluations.\nOn this front, recent lines of research works explored leveraging state-of-the-art large language\nmodels (LLMs) as reference-free evaluators on various NLG tasks (Kocmi & Federmann, 2023; Fu\net al., 2023; Wang et al., 2023a; Liu et al., 2023). Given that LLMs are optimized to follow human\ninstructions (Ouyang et al., 2022) as well as their state-of-the-art performance on language modeling\n(OpenAI, 2023), they could perform the task of evaluation when prompted accordingly. Multiple\nevidences show that LLMs are promising competent in evaluating instruction-tuned models like\nAlpaca (Taori et al., 2023) and Vicuna (Zheng et al., 2023), and being a viable alternative to human\nexpert evaluations (Zheng et al., 2023; Dubois et al., 2023).\nDespite these promising results, emerging studies are raising concerns about the validity of LLM-\nbased evaluators - whether LLM\u2019s underlying scoring mechanism aligns with human guidelines and\npreferences (Liu et al., 2023). Existing LLM-based evaluators enclose the candidate text together\nwith the evaluation task into an instruction prompt. While this paradigm succeeds in presenting the\n\u2217 Work done during internship at Microsoft. \u266f Corresponding author.\n1 Work in progress.\n1\narXiv:2309.13308v1  [cs.CL]  23 Sep 2023\nPreprint\nThe score for consistency is based on how well the \nsummary reflects the main information and details of the \narticle, without omitting \u2026\u2026                                 Relevance: 0.69 \nI. Criteria Drafting\n[Examples]\nThe above are \nsome examples \non evaluating the \nconsistency of an \narticle\u2019s summary. \nPlease carefully \nread all the article, \nsummary and \ntheir assigned \nscore, and induce \nthe most possible \nscoring rule and \ncriteria used.\nArticle: Paul Merson has restarted his row with Andros \nTownsend after the Tottenham midfielder was brought on \nwith only seven minutes\u2026\nSummary: Paul Merson was brought on with only seven \nminutes remaining in his team \u2018s 0-0 draw with Burnley\u2026\nExpert Score: Consistency - 1.0\nArticle: Paul Merson has restarted his row with Andros \nTownsend after the Tottenham midfielder was brought on \nwith only seven minutes\u2026\nSummary: Paul Merson was brought on with only seven \nminutes remaining in his team \u2018s 0-0 draw with Burnley\u2026\nExpert Score: Consistency - 1.0\nArticle: Paul Merson has restarted his row with Andros \nTownsend after the Tottenham midfielder was brought on \nwith only seven minutes\u2026\nSummary: Paul Merson was brought on with only seven \nminutes remaining in his team\u2019s 0-0 draw with Burnley\u2026\nExpert Score: Consistency - 1.0\nArticle: Paul Merson has restarted his row with Andros \nTownsend after the Tottenham midfielder was brought on \nwith only seven minutes\u2026\nSummary: Paul Merson was brought on with only seven \nminutes remaining in his team \u2018s 0-0 draw with Burnley\u2026\nExpert Score: Consistency - 1.0\nArticle: Paul Merson has restarted his row with Andros \nTownsend after the Tottenham midfielder was brought on \nwith only seven minutes\u2026\nSummary: Paul Merson was brought on with only seven \nminutes remaining in his team \u2018s 0-0 draw with Burnley\u2026\nExpert Score: Consistency - 1.0\nArticle: Paul Merson has restarted his row with Andros \nTownsend after the Tottenham midfielder was brought on \nwith only seven minutes\u2026\nSummary: Paul Merson has restarted his row with Andros \nTownsend after the Tottenham midfielder was brought\u2026\nExpert Score: Consistency - 5.0\nHuman \nExpert \nCorpus\nFill-in\nInduction Prompt Template\nSample Few-Shot Seed Examples\nLLM\nPossible Criteria: A summary is consistent \nwith the article if it accurately and fairly \nconveys the main information, facts, and \narguments of the original text, without \nintroducing any errors, contradictions, \u2026\u2026\nPossible Criteria: A summary is consistent \nwith the article if it accurately and fairly \nconveys the main information, facts, and \narguments of the original text, without \nintroducing any errors, contradictions, \u2026\u2026\nPossible Criteria:   A summary is consistent \nwith the article if it accurately and fairly \nconveys the main information, facts, and \narguments of the original text, without \nintroducing any errors, contradictions, \u2026\u2026\nPossible Criteria: A summary is consistent \nwith the article if it accurately and fairly \nconveys the main information, facts, and \narguments of the original text, without \nintroducing any errors, contradictions, \u2026\u2026\nPossible Criteria: A summary is consistent \nwith the article if it accurately and fairly \nconveys the main information, facts, and \narguments of the original text, without \nintroducing any errors, contradictions, \u2026\u2026\nPossible Criteria:   Score 5 - The summary \ncovers all or most of the main points of the \narticle, without any significant errors or \nomissions. The summary uses the same or \nsimilar words, phrases, or terms as the \u2026\u2026\nCriteria Drafting with LLMs\nTemperature \nSampling\nII. Criteria Revisiting\nCriteria Draft\nCandidates\nLLM\nEvaluate and Ranking on Dev Expert Labels\nA summary is consistent with the article if it accurately and \nfairly conveys the main information, facts, and arguments \nof the original text, without, \u2026\u2026                           Relevance: 0.81 \nDeduct 1 point for each major inconsistency, such as \nchanging the meaning or implication of a sentence. Add 0.5 \npoints for each improvement \u2026\u2026\nRelevance: 0.45 \n\u2026\u2026\nCriteria Refinement\n\u00d7\nLLM\nRefined\nCandidates\nTop-K \nFiltering\nCalibrated\nCriteria Pool\nTop-K \nFiltering\nIII. Criteria Application\nDownstream\nData to apply\nCalibrated Criteria Pool\nLLM\nConsistency Score \u2013 4.0\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\nFigure 1: Overall framework of AUTOCALIBRATE. To calibrate a LLM-based evaluator towards\nbetter alignment with human expert preference, we propose a 3-stage procedure to draft, revisit, and\napply high-quality scoring criteria.\ntask, it elicits several unaddressed issues, including the sensitivity and bias to output space (Wang\net al., 2023a), sample ordering (Wang et al., 2023b), and prompt format (Zheng et al., 2023). Plus,\nas the scoring prompt is also human-written, it may also incorporate potential bias to the LLM.\nTo address this issue, we study calibrating an LLM-based evaluator towards better human alignment.\nWe start from a retrospection into existing LLM-based evaluators and uncover they suffer from\ninsufficient prompting, where the scoring guidelines are absent and only output spaces (e.g. 0-100)\nare provided, resulting in inconsistent and misaligned evaluations (Lu et al., 2023). We argue that\nsuch an issue could be mitigated by elucidating the scoring criteria. And by finalizing the scoring\ncriteria, a consensus could be reached between humans and the LLM, as a means of alignment.\nHowever, it is non-trivial to obtain adequate criteria2, as it may require expert-level domain knowl-\nedge to assign rubrics and prevent personal bias. Drawing inspirations from the in-context learning\ncapability (Dong et al., 2022) of LLMs, we propose AUTOCALIBRATE, a framework to automati-\ncally align and calibrate an LLM-based evaluator through human alignment. To tackle the challenge\nof curating scoring criteria, we take a data-driven methodology to draft, filter, and refine rubrics us-\ning the LLM, based on human expert labels. By incorporating the mined and calibrated rubrics into\nscoring instructions, we obtained significant improvements in human alignment when evaluating text\nsummarization, data-to-text generation, and hallucinations. Moreover, we release the optimal scor-\ning criteria sets mined for the above tasks, and present detailed qualitative and quantitative analysis\nto uncover the essence that makes an effective criteria.\n2 Results in Chen et al. (2023) suggest that poorly curated criteria reduce relevance with human expert\nscoring. Un-calibrated random criteria would introduce extra bias as a misalignment between the standards\nused for human experts. And improperly assigned rubrics might reduce the difference between each score.\n2\nPreprint\nPlease evaluate the quality of an article\u2019s summary on its consistency to the original article.\nCriteria for evaluating consistency:   [Criteria content]\nPlease return your evaluation score in the scale of 1 to 5, with 1 being the lowest.\nArticle:        [Article content]    \nSummary: [Summary content]\nEvaluation Prompt Template\nInstructions\nAspect\nCriteria\nOutput \nSpace\nData sample to \nevaluate\nFigure 2: Example of an evaluation prompt template applied by a LLM-based evaluator.\n2\nMETHODOLOGY\n2.1\nOVERVIEW OF AUTOCALIBRATE\nFigure 1 illustrates the overall framework of AUTOCALIBRATE. To calibrate an LLM-based evalua-\ntor, we focus on optimizing the evaluation prompt template T applied to improve the correlation and\nalignment between LLM\u2019s scores and human preference. Specifically, we mine and tune the scoring\ncriteria in pursuing such alignment. To express human preference, we first construct a golden set\nD\u2217, containing ground-truth sample-label pairs from human expert labeler. We then follow a novel\nmulti-stage procedure to optimize candidate scoring criteria, including drafting and revisiting. Ini-\ntial criteria drafts are first inferred from in-context labels and an induction prompt, evaluated and\nfiltered on expert labels, and then refined to accommodate erroneous evaluations.\n2.2\nPROBLEM FORMULATION\nIn this section, we elaborate on the calibration medium and objective of AUTOCALIBRATE - the\nscoring criteria. Denote D the dataset which contains multiple samples to evaluate. Based on dif-\nferent tasks, a sample di \u2208 D can contain various components: single text, for tasks like evaluating\ngrammatical correctness; source-target, for the vast majority of conditional generations, and multi-\nturn, like assessing multi-turn conversations.\nTo guide the LLM to evaluate the quality of sample di, prompts are applied to provide sufficient\ninstructions and clarifications of the task. To calibrate the prompt template T applied during eval-\nuation, we regulate it by decomposing it into the following building blocks: instructions, criteria,\naspect, output format, and data sample to evaluate, as illustrated in Figure 2. For an arbitrary sam-\nple di \u2208 D, given a prompt template T (guides the LLM to perform evaluation on NLG quality),\nscoring criteria C, evaluation aspect a (e.g., fluency, coherence, consistency) and a large language\nmodel LLM(\u00b7), the NLG quality of di could be evaluated as\n\u02c6si,a = LLM(T (di, C, a)).\n(1)\nDenote D\u2217 a golden set consists of curated sample-label pairs (d\u2217\ni , si,a) from human experts, and\nf(\u00b7) an correlation metric. In AUTOCALIBRATE, we focus on calibrating the scoring criteria C to\nmaximize the correlation between predicted labels and human expert labels, as\nC = arg max\nC\nf\n\u0002\n\u222ad\u2217\ni \u223cD\u2217 (\u02c6si,a, si,a)\n\u0003\n.\n(2)\n2.3\nAUTOCALIBRATE\nData Labeling as Human Preference\nTo calibrate an LLM-based evaluator, one primary question\nis: how to represent and model the preference of human experts. On existing approaches, sophisti-\ncated model-based evaluators like COMET (Rei et al., 2020) directly train on human labels, while\nranking-based human labels are widely adopted in RLHF to model human preference (Ouyang et al.,\n2022). However, these model-based preference modeling methods require extra fine-tuning, which\nmakes them computationally intensive and impracticable to API-based LLMs. To mitigate these\nlimitations, We implicitly encode human expert preference to a set of sample-label pairs and form a\ngolden set D\u2217. Compared with curating finalized scoring criteria and guidelines with joint human\n3\nPreprint\nexpert works, it is more feasible to collect labels leveraging crowd-sourcing dispatch, and also easier\nto validate and merge opinions from different experts.\nCriteria Drafting\nAfter constructing the expert label set D\u2217, we utilize the instruction following\nand in-context learning capability of LLMs to independently infer scoring criteria C from few-shot\nexemplars. One crucial part here is to ensure the diversity of recalled criteria. To mitigate the label\nbias and position bias of in-context learning (Zhao et al., 2021), we construct various Monte-Carlo\nsamples from D\u2217 to obtain few-shot in-context exemplars. Given drafting prompt template TD and\na few-shot exemplar set Ds = \u222a(d\u2217\ni , si,a) \u2282 D\u2217, an corresponding criteria is inferred as\n\u02c6C = arg max\nC\nP\u03b8(C|TD(Ds, a)),\n(3)\nwhere a denotes the evaluation aspect. Temperature sampling is also applied to draw scoring criteria\nin diversified presentations from the LLM. Example prompt templates are provided in Appendix D.1.\nFollowing this procedure, we obtain the initial set of scoring criteria for evaluation and refinement.\nCriteria Revisiting\nInferred from various few-shot exemplars, criteria within the initial draft set\nare diversified, but may be sub-optimal or contain potential bias (e.g., to particular scoring labels). To\nfilter out high-quality candidates, we first revisit them leveraging D\u2217 and select the top performing\ncandidates w.r.t their human relevance3. To mitigate disagreements between human experts and the\ndrafted criteria, we prompt LLMs to refine (Madaan et al., 2023) the previously generated criteria\nby providing them samples with strong disagreement in their scores. When refining the criteria, we\nsuggest the following atomic editing operations via prompting to the LLM4:\n\u2022 Modification: Adjust some parts of the criteria to increase its correlation.\n\u2022 Paraphrase: If a criteria is good enough, paraphrase it to make it clearer and more concise.\n\u2022 Adding Aspects or Details: When LLM discovers new underlying scoring rules that are not\ncovered by the current criteria, consider adding them as a new line to the current criteria,\nbut make sure not to make the criteria too long and redundant.\n\u2022 Calibrate: Any other modifications that the LLM considers helpful.\nAs illustrated in Figure 1, after obtaining refined candidate criteria, we first filter them with D\u2217 and\nthen combine them with the pre-filtered draft criteria to obtain a calibrated set of scoring rules.\nConclusion and Discussion\nCombining the above, we obtain AUTOCALIBRATE, an automatic\npipeline in calibrating LLM-based evaluators. The overall procedure is summarized in Algorithm 1.\nThe benefits of choosing criteria as a medium for calibration are multitudinous. First, we do not\nrequire gradients or access to model parameters, which makes AUTOCALIBRATE applicable to API-\nbased LLMs. Second, since criteria remain in a natural language form (compared with soft prompt-\ntuning), calibrating the criteria is essential to reaching an agreement between humans and the LLM.\nTherefore, the process is more explainable and controllable (e.g., one may perform human-in-the-\nloop adjustments to scoring criteria in case of preference changes, or to avoid corner cases).\n3\nEXPERIMENTAL SETUP\n3.1\nTASKS AND DATASETS\nWe evaluate AUTOCALIBRATE on three text quality evaluation tasks, including text summarization,\ndata-to-text generation, and evaluating hallucinations. We select tasks following previous research\nworks (Zhong et al., 2022; Fu et al., 2023). We select two datasets for each of the tasks, consisting\nof 6 datasets in total, each containing human expert labels for candidate samples. Specifically, we\nselect NewsRoom (Grusky et al., 2018) and SummEval (Fabbri et al., 2021) for evaluating machine\nsummarization; SFRES (Wen et al., 2015) and SFHOT (Wen et al., 2015) for data-to-text task,\nQAGS-XSUM and QAGS-CNN (Wang et al., 2020b) for evaluating hallucinations. To evaluate\nthe alignment between the scoring from LLM and human experts, we perform a meta-evaluation\nfollowing (Zhong et al., 2022). Details on the evaluation strategy are listed in Appendix A.\n3 A meta-evaluation method f(\u00b7) is applied here to perform meta-evaluation on the correlation between\nhuman and LLM judgments. For detailed explanations and definitions, please refer to Appendix A.\n4 Detailed prompt examples are provided in Appendix D.3.\n4\nPreprint\nAlgorithm 1 Calibrating LLM-Based Evaluator with AUTOCALIBRATE\nRequire: LLM \u03b8, human expert labels D\u2217, meta-correlation metric f(\u00b7), Monte-Carlo trial count\nN, in-context exemplar size L = {l1, ..., lm}, aspect a, target criteria candidate pool size K\n1: for Few-shot exemplar size li in L do\n2:\nfor Monte-Carlo trial j in 1 : N do\n3:\nSample few-shot examples of human labels Ds = \u222a(d\u2217\ni , si,a) from D\u2217\n4:\nDraft candidate criteria with LLM according to Eq.(3) using temperature sampling\n5:\nAdd the obtained criteria Ci to candidate set C\n6:\nend for\n7: end for\n8: Revisit C and retain top-K candidates with highest correlation: C \u2190 argTopKci\u2208Cf(ci, D\u2217)\n9: Collect mis-aligned evaluation examples as DR\ni for each ci in C\n10: for Candidate criteria ci in C do\n11:\nfor Monte-Carlo trial j in 1 : N do\n12:\nSample few-shot examples of misaligned labels DR\ns = \u222a(dR\ni , sR\ni,a) from DR\ni\n13:\nRefine candidate criteria with LLM, then add the obtained criteria to candidate set C\n14:\nend for\n15: end for\nReturn Calibrated criteria Cfin \u2190 argmaxci\u2208Cf(ci, D\u2217)\n3.2\nMODELS AND BASELINES\nTo implement AUTOCALIBRATE, we select OpenAI\u2019s GPT-4 model (GPT-4-32K) as the LLM for\nthe evaluator. We list prompt templates for criteria drafting, evaluation, and refinement for tach tasks\nin Appendix D. We set the temperature to 0 during evaluation, and 1 when obtaining initial criteria\ndrafts and their refined versions. Please refer to Appendix C for detailed configurations of each task.\nWe compare AUTOCALIBRATE with various state-of-the-art and/or widely applied evaluators. We\nfirst include ROUGE (Lin, 2004), a widely-applied n-gram-based evaluation metric for text sum-\nmarization. We then select various evaluators based on smaller neural (language) models, including\nBERTScore (Zhang* et al., 2020), MoverScore (Zhao et al., 2019), PRISM (Thompson & Post,\n2020), BartScore (Yuan et al., 2021), CTC (Deng et al., 2021), and UniEval (Zhong et al., 2022).\nFinally, we compare evaluators based on state-of-the-art LLMs (e.g. GPT-3.5 and GPT-4), including\nGPTScore (Fu et al., 2023), ChatGPT5 (Wang et al., 2023a), and GPT-Eval (Liu et al., 2023).\n4\nEXPERIMENTAL RESULTS\n4.1\nRESULTS FOR SUMMARIZATION\nWe conduct meta-correlation analysis on NewsRoom and SummEval benchmark to evaluate AUTO-\nCALIBRATE\u2019s performance to calibrate an LLM-based evaluator on text summarization. Following\nLiu et al. (2021), we perform summary-level Spearman and Kendall correlation analysis on each\nof the 4 evaluation metrics with human expert evaluations. To represent the performance of our\nun-calibrated backbone LLM, we add a GPT-4 baseline, whose evaluations are obtained with a\none-pass call using an evaluation prompt where scoring criteria is omitted 6.\nResults on NewsRoom and SummEval benchmark are listed in Table 1 and 2, respectively. On\nNewsRoom benchmark (Table 1), our AUTOCALIBRATE significantly outperforms the LLM-based\nChatGPT evaluator. It also surpasses the vanilla GPT-4-based evaluator by a large margin (with a\n10.4% improvement on Spearman and 11% on Kendall correlation), demonstrating the importance\nand effectiveness of the calibration procedure. While BartScore obtained a competent performance\non NewsRoom, it falls short on SummEval. We conjecture that since it utilizes a smaller model, the\nconsistency of its scoring might be hindered due to the distribution of its fine-tuning corpus.\n5 The \u2018ChatGPT\u2019 evaluator included multiple versions according to different prompt templates, and we mark\nthese variants with parentheses. We encourage readers to check the original works for detailed information.\n6 For a fair comparison, the only difference is the removal of criteria from prompt. We keep the rest identical.\n5\nPreprint\nMetrics\nCoherence\nRelevance\nInformative\nFluency\nAverage\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\nROUGE-1\n0.095\n0.076\n0.147\n0.112\n0.130\n0.099\n0.104\n0.082\n0.119\n0.092\nROUGE-2\n0.026\n0.009\n0.091\n0.065\n0.079\n0.052\n0.048\n0.032\n0.061\n0.092\nROUGE-L\n0.064\n0.051\n0.106\n0.083\n0.089\n0.064\n0.072\n0.061\n0.083\n0.065\nBERTSCORE\n0.147\n0.116\n0.162\n0.126\n0.130\n0.105\n0.171\n0.128\n0.152\n0.119\nMOVERSCORE\n0.161\n0.127\n0.195\n0.157\n0.188\n0.151\n0.120\n0.086\n0.166\n0.130\nPRISM\n0.573\n0.478\n0.553\n0.460\n0.561\n0.472\n0.532\n0.443\n0.555\n0.463\nBARTSCORE (CNN)\n0.653\n0.547\n0.567\n0.478\n0.616\n0.510\n0.640\n0.540\n0.619\n0.519\nCHATGPT (DA)\n0.469\n0.405\n0.461\n0.392\n0.578\n0.498\n0.507\n0.427\n0.504\n0.430\nCHATGPT (Stars)\n0.428\n0.375\n0.402\n0.348\n0.557\n0.487\n0.451\n0.385\n0.460\n0.399\nGPT-4\n0.557\n0.498\n0.574\n0.511\n0.581\n0.521\n0.601\n0.535\n0.578\n0.516\nAUTOCALIBRATE\n0.602\n0.540\n0.656\n0.585\n0.654\n0.590\n0.640\n0.575\n0.638\n0.573\nTable 1: Summary-level Spearman (\u03c1) and Kendall (\u03c4) correlations of aspects on NewsRoom.\nMetrics\nCoherence\nConsistency\nFluency\nRelevance\nAverage\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\nROUGE-1\n0.167\n0.126\n0.160\n0.130\n0.115\n0.094\n0.326\n0.252\n0.192\n0.150\nROUGE-2\n0.184\n0.139\n0.187\n0.155\n0.159\n0.128\n0.290\n0.219\n0.205\n0.161\nROUGE-L\n0.128\n0.099\n0.115\n0.092\n0.105\n0.084\n0.311\n0.237\n0.165\n0.128\nBERTSCORE\n0.284\n0.211\n0.110\n0.090\n0.193\n0.158\n0.312\n0.243\n0.225\n0.175\nMOVERSCORE\n0.159\n0.118\n0.157\n0.127\n0.129\n0.105\n0.318\n0.244\n0.191\n0.148\nPRISM\n0.249\n0.196\n0.212\n0.163\n0.345\n0.285\n0.254\n0.205\n0.265\n0.212\nCTC (Consistency)\n0.223\n0.172\n0.415\n0.345\n0.335\n0.276\n0.166\n0.124\n0.285\n0.229\nCTC (Relevance)\n0.402\n0.310\n0.366\n0.301\n0.299\n0.245\n0.428\n0.336\n0.374\n0.298\nBARTSCORE (CNN)\n0.448\n0.342\n0.382\n0.315\n0.356\n0.292\n0.356\n0.273\n0.385\n0.305\nUNIEVAL (Multi-task)\n0.495\n0.374\n0.435\n0.365\n0.419\n0.346\n0.424\n0.327\n0.443\n0.353\nUNIEVAL (Continual)\n0.575\n0.442\n0.446\n0.371\n0.449\n0.371\n0.426\n0.325\n0.474\n0.377\nCHATGPT (DA)\n0.451\n0.383\n0.432\n0.399\n0.380\n0.351\n0.439\n0.379\n0.425\n0.378\nG-EVAL-3.5\n0.440\n0.335\n0.386\n0.318\n0.424\n0.347\n0.385\n0.293\n0.401\n0.320\nG-EVAL-4\n0.582\n0.457\n0.507\n0.425\n0.455\n0.378\n0.547\n0.433\n0.514\n0.418\nGPT-4\n0.535\n0.464\n0.466\n0.432\n0.440\n0.413\n0.532\n0.465\n0.493\n0.443\nAUTOCALIBRATE\n0.570\n0.493\n0.500\n0.467\n0.487\n0.452\n0.560\n0.483\n0.529\n0.474\nTable 2: Summary-level Spearman (\u03c1) and Kendall (\u03c4) correlations of aspects on SummEval.\nIn contrast, our AUTOCALIBRATE demonstrated a consistent human relevance uplift on both sum-\nmarization datasets, since the pretraining knowledge in LLM is more in-depth and generalizable.\nOn SummEval, AUTOCALIBRATE improves the human correlation of GPT-4 evaluations by 7.3%,\nand also superior to a strong baseline G-EVAL-4 that also utilizes GPT-4. Noteworthy, G-EVAL-4\nrequires 20 calls from LLM to obtain an average score to mitigate replicated evaluations. While\nthis improves Spearman correlation by creating a more continuous distribution, it reduces the rank\ncoefficient. In contrast, by elucidating the scoring rule with calibrated criteria, AUTOCALIBRATE\nimproves both Spearman (2.9%) and Kendall (13.4%) coefficients with only one forward call.\n4.2\nRESULTS FOR DATA-TO-TEXT\nWe consider SFRES and SFHOT datasets for evaluation of data-to-text generation task and follow\nFu et al. (2023) to conduct dataset-level meta-evaluation on human alignment. Results are listed in\nTable 3. As illustrated in the table, AUTOCALIBRATE significantly outperforms the most competent\ntrained evaluator (UNIEVAL) over 30%, and yields an over 20% and 10% improvement on Spearman\ncorrelation over GPT-SCORE (based on 175B-LLM GPT-3.5) and uncalibrated GPT-4 evaluator,\nrespectively. These results suggest that the proposed procedures within AUTOCALIBRATE could\npromptly curate adequate scoring criteria for different NLG tasks and sample distributions.\n6\nPreprint\nMetrics\nSFRES-INF\nSFRES-NAT\nSFHOT-INF\nSFHOT-NAT\nAverage\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\nROUGE-1\n0.129\n0.098\n0.109\n0.081\n0.116\n0.089\n0.113\n0.084\n0.117\n0.088\nROUGE-2\n0.124\n0.094\n0.094\n0.069\n0.080\n0.061\n0.086\n0.064\n0.096\n0.072\nROUGE-L\n0.097\n0.073\n0.097\n0.071\n0.088\n0.067\n0.102\n0.076\n0.096\n0.072\nBERTSCORE\n0.156\n0.119\n0.138\n0.102\n0.135\n0.104\n0.126\n0.094\n0.172\n0.105\nMOVERSCORE\n0.021\n-0.016\n0.075\n0.056\n0.042\n0.033\n0.038\n0.029\n0.044\n0.026\nBARTSCORE (CNN)\n0.154\n0.117\n0.138\n0.101\n0.164\n0.126\n0.075\n0.055\n0.133\n0.100\nUNIEVAL (Multi-task)\n0.225\n0.169\n0.333\n0.247\n0.249\n0.191\n0.320\n0.238\n0.282\n0.211\nGPT-SCORE (D01)\n0.270\n-\n0.317\n-\n-\n-\n-\n-\n0.294\n-\nGPT-SCORE (D03)\n0.296\n-\n0.270\n-\n-\n-\n-\n-\n0.283\n-\nGPT-4\n0.283\n0.247\n0.389\n0.329\n0.315\n0.277\n0.389\n0.331\n0.344\n0.296\nAUTOCALIBRATE\n0.315\n0.272\n0.416\n0.351\n0.357\n0.313\n0.440\n0.383\n0.382\n0.330\nTable 3: Dataset-level Spearman (\u03c1) and Kendall (\u03c4) correlations of different evaluation aspects on\nSFRES and SFHOT. -INF and -NAT denote informativeness and naturalness, respectively.\nMetrics\nQAGS-CNN/DM\nQAGS-XSUM\nAverage\nr\n\u03c1\n\u03c4\nr\n\u03c1\n\u03c4\nr\n\u03c1\n\u03c4\nROUGE-1\n0.338\n0.318\n0.248\n-0.008\n-0.049\n-0.040\n0.165\n0.134\n0.104\nROUGE-2\n0.459\n0.418\n0.333\n0.097\n0.083\n0.068\n0.278\n0.250\n0.200\nROUGE-L\n0.357\n0.324\n0.254\n0.024\n-0.011\n-0.009\n0.190\n0.156\n0.122\nBERTSCORE\n0.576\n0.505\n0.399\n0.024\n0.008\n0.006\n0.300\n0.256\n0.202\nMOVERSCORE\n0.414\n0.347\n0.271\n0.054\n0.044\n0.036\n0.234\n0.195\n0.153\nFACTCC\n0.416\n0.484\n0.376\n0.297\n0.259\n0.212\n0.356\n0.371\n0.294\nBARTSCORE\n0.735\n0.680\n0.557\n0.184\n0.159\n0.130\n0.459\n0.420\n0.343\nCTC\n0.619\n0.564\n0.450\n0.309\n0.295\n0.242\n0.464\n0.430\n0.346\nUNIEVAL\n0.682\n0.662\n0.532\n0.461\n0.488\n0.399\n0.571\n0.575\n0.465\nG-EVAL-3.5\n0.477\n0.516\n0.410\n0.211\n0.406\n0.343\n0.344\n0.461\n0.377\nG-EVAL-4\n0.631\n0.685\n0.591\n0.558\n0.537\n0.472\n0.599\n0.611\n0.525\nGPT-4\n0.605\n0.649\n0.606\n0.637\n0.637\n0.637\n0.621\n0.643\n0.622\nAUTOCALIBRATE\n0.740\n0.744\n0.663\n0.662\n0.662\n0.662\n0.701\n0.703\n0.663\nTable 4: Dataset-level Pearson (r), Spearman (\u03c1) and Kendall-Tau (\u03c4) correlations on QAGS.\n4.3\nRESULTS FOR EVALUATING HALLUCINATIONS\nHallucinations are an important issue in NLG models where the output is based on fabricated, un-\nwarranted facts or digress from a previous context, and it is becoming an increasingly important\ntopic for trustworthy LLMs (Ji et al., 2023). To test AUTOCALIBRATE on evaluating hallucinations,\nwe select QAGS-CNNDM and QAGS-XSUM dataset and perform dataset-level meta-analysis fol-\nlowing Liu et al. (2023). As presented in Table 4, AUTOCALIBRATE uplift the average Spearman\ncorrelation by 15% over G-EVAL-4. Noteworthy, since fine-tuned on CNN data, BartScore achieves\npromising human relevance on QAGS-CNN, but significantly falls short on QAGS-XSUM, while\nLLM-based AUTOCALIBRATE performs consistently on both datasets. This further indicates that\nLLMs, given their immense knowledge gained during pre-training, are strong candidates for general\nevaluators, and their performance could be further boosted with proper calibration.\n4.4\nABLATION EXPERIMENTS\nWe conduct ablation studies on the procedure of AUTOCALIBRATE to better investigate the con-\ntribution of each process in calibrating LLM-based evaluator. The main ablation experiments are\nlisted in Table 5. As illustrated in the table, removing criteria in the prompt significantly reduces the\nhuman correlation of GPT-4. This corroborates our argument that previously LLMs suffered from a\nvaguely defined scoring principle, and this could be calibrated to increase the human alignment of\nLLM evaluators. The self-refine process also positively contributed to the improvements in human\nalignment. This indicates that LLMs could accordingly adjust the effectiveness of scoring criteria.\nDetailed qualitative analysis is presented in Chapter 5.\n7\nPreprint\nDataset\n\u03c1\n\u03c4\nOG\n-Crit\n-Rfi\nOG\n-Crit\n-Rfi\nNews\nRoom\nCoherence\n0.602\n0.557\n0.593\n0.540\n0.498\n0.531\nRelevance\n0.656\n0.574\n0.619\n0.585\n0.511\n0.550\nInformative\n0.654\n0.581\n0.617\n0.590\n0.521\n0.557\nFluency\n0.640\n0.601\n0.628\n0.575\n0.535\n0.563\nSFRES\nInformative\n0.315\n0.283\n0.300\n0.272\n0.247\n0.264\nNaturalness\n0.416\n0.389\n0.405\n0.351\n0.329\n0.346\nSFHOT\nInformative\n0.357\n0.315\n0.345\n0.313\n0.277\n0.303\nNaturalness\n0.440\n0.389\n0.425\n0.383\n0.331\n0.368\nQAGS\nCNN Data\n0.744\n0.649\n0.724\n0.663\n0.606\n0.642\nXSUM Data\n0.662\n0.637\n0.651\n0.662\n0.637\n0.651\nTable 5: Ablations on each proposed module. We report Spearman (\u03c1) and Kendall (\u03c4) correlations.\n\u2018OG\u2019 denotes original method, \u2018-Crit\u2019 and \u2018-Rfi\u2019 denote removing criteria and refine, respectively.\n5\nANALYSIS\n5.1\nESSENCE OF EFFECTIVE CRITERIA\nIn this chapter, we present statistical analysis on the pool of draft candidates of scoring criteria, and\nmine for possible essence that contributes to effective scoring criteria with high human relevance for\nLLM-based evaluators. The main results are presented in Figure 3.\nEffect of Few-Shot Example Size\nWe study the sensitivity of AUTOCALIBRATE to the sample\nsize of few-shot in-context samplers. As illustrated in Figure 3(A), the size of in-context few-shot\nexemplars yields no significant impact except for QAGS-CNN. The results indicate that AUTOCAL-\nIBRATE is mostly robust to the size of in-context samples. Thanks to the sufficient prior knowledge\nobtained during pretraining by the LLM, AUTOCALIBRATE is capable of inferring the underlying\ncriteria using only a few examples in context. As illustrated in the figure, a few-shot size of 8 to 12\nis sufficient in mining effective criteria across all tasks. This intriguing feature enables a reduction\nin search space for cost reductions upon deployment.\nEffect of Criteria Length\nThe distribution of lengths of generated criteria and their human rel-\nevance is illustrated in Figure 3(B). Most evaluation criteria drafted and refined with AUTOCAL-\nIBRATE lie in the range of 60 to 600 words. We discover different trends on the preference of\nAUTOCALIBRATE to different lengths of criteria. While fluency and coherence metrics on text\nsummarization lean towards shorter criteria, lengthier versions are favored by the informativeness\nmetric on data-to-text and evaluating hallucinations. Despite this difference, AUTOCALIBRATE en-\njoys the capability to generate effective criteria at each length. We conjecture this nuance is caused\nby the intrinsic complexity of the aspect to evaluate: it could be straightforward to define fluency,\nbut possibly more challenging to address hallucination.\nPatterns of Criteria\nWe observed two significant patterns on the criteria drafted by GPT-4: holis-\ntic and specific. The former typically characterizes the common features possessed by high and\nlow-quality samples, while the latter generates a segment of the corresponding rubric for each eval-\nuation score (e.g., 1 to 5). A random example of these patterns of criteria is listed in Table 6. These\ntwo patterns emerge across all sets of experiments on different benchmarks. The performance distri-\nbution of these two patterns across all datasets is illustrated in Figure 4. As illustrated in the figure,\nthere is no significant difference in human expert correlation between holistic and specific patterns,\nindicating that both patterns generated from AUTOCALIBRATE are of high quality. Therefore, the\nperformance of AUTOCALIBRATE is robust to the patterns of criteria generated.\n8\nPreprint\nA\nB\nFigure 3: Statistics of criteria induced from AUTOCALIBRATE. A) Human correlation of criteria\ninduced using various few-shot in-context demonstration sizes. B) Correlation between human rel-\nevance and criteria length. Shaded areas denote 95% confidence interval.\nEvaluation Criteria Induced\nHuman Alignment\nr\n\u03c1\n\u03c4\nA A summary should capture the main idea and key details of the article, without introducing any\nnew or misleading information. A summary should use clear and concise language, avoiding unnec-\nessary repetition or filler words. A summary should be proportionate to the length and complexity\nof the article, reflecting the most important aspects and leaving out less relevant details.\n0.58\n0.56\n0.49\nB Possible scoring rule: A score of 5 means the summary is very relevant, covering all the essential\nelements of the article and omitting any unnecessary or misleading information. A score of 4 means\nthe summary is mostly relevant, covering most of the essential elements of the article and omitting or\nincluding only minor or trivial information. A score of 3 means the summary is somewhat relevant,\ncovering some of the essential elements of the article, but omitting or including some important or\nrelevant information. A score of 2 means the summary is slightly relevant, covering only a few of the\nessential elements of the article, and omitting or including a lot of important or relevant information.\nA score of 1 means the summary is irrelevant, covering none or almost none of the essential elements\nof the article, and omitting or including a lot of inaccurate or irrelevant information.\n0.53\n0.52\n0.45\nTable 6: Case study on patterns of criteria induced from SummEval-REL. Criteria mined tend to\nfollow two major patterns of its form: holistic (A) and specific (B). The former commonly describe\nwhat makes a good or bad sample, while the latter generate specific rubrics for each of the scores.\n5.2\nCASE STUDY\nTo investigate the effect of criteria refinement, we present a case study in Table 7. As demon-\nstrated in the table, when prompted with previous misaligned evaluation cases and possible means\nof modifications (Section 2.3), the LLM automatically infers new patterns of underlying scoring\nprinciples, and promptly adapts the existing criteria to accommodate them. As illustrated in the\ntable, AUTOCALIBRATE discovers that the genre and format is crucial to the fluency of summary\nfrom in-context examples provided, adjusts the criteria accordingly, and achieves higher human rel-\nevance. These findings corroborate with Madaan et al. (2023) that LLM is capable of self-refine, and\nopens a future research direction on the multi-turn, iterative calibration of LLM-based evaluators.\n9\nPreprint\nEvaluation Criteria Induced\nHuman Alignment\nr\n\u03c1\n\u03c4\nBefore ... It should use appropriate vocabulary and punctuation, and avoid repetition or redun-\ndancy. It should also capture the tone and style of the original article. A summary with a medium\nscore (3) should have few or minor errors that do not interfere with the overall meaning and read-\nability of the summary. It should use mostly appropriate vocabulary and punctuation, and avoid\nrepetition or redundancy. It should also capture the tone and style of the article. - A summary with a\nlow score (1 or 2) should have frequent or major errors that affect the overall meaning and readabil-\nity of the summary ... It should also fail to capture the tone and style of the original article.\n0.63\n0.62\n0.56\nAfter ... It should also capture the tone and style of the original article and use the correct genre\nand format (e.g., not writing a summary as a list of bullet points). A summary with a medium score\n(3) should have few or minor errors that do not interfere with the overall meaning and readability of\nthe summary. It should use mostly appropriate vocabulary and punctuation, and minimize repetition\nor redundancy. It should also attempt to capture the tone and style of the original article and use\nthe correct genre and format, but may have some inconsistencies or inaccuracies. - A summary\nwith a low score (1 or 2) should have frequent or major errors that affect the overall meaning and\nreadability of the summary ... It should also fail to capture the tone and style of the original article\nand use the wrong genre or format.\n0.66\n0.64\n0.58\nTable 7: Case study of criteria refinement on NewsRoom-FLU. To refine a criteria, the model\nautomatically infer new patterns from bad cases and promptly adjust the criteria to incorporate them.\nModifications are highlighted in blue, and some parts of generated criteria are omitted for space.\nPearson\nSpearman\nKendall\nFigure 4: Performance of different patterns of scoring criteria induced by AUTOCALIBRATE.\n6\nRELATED WORK\nAutomatic NLG Evaluation\nIt has been a long and arduous endeavor to automatically evaluate\nnatural language generations. This paragraph outlines automatic evaluation metrics before the era of\nLLM. (1) N-gram-based metrics: as the most widely adopted method, n-gram-based metrics mea-\nsure the quality of a candidate text by the overlap of its lexical fraction between references. As two\nof the most widely used metrics, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) are spe-\ncialized in precision for machine translation and recall for text summarization, respectively. Despite\nbeing widely applied, their human relevance is undesired (Freitag et al., 2022). (2) Embedding-\nbased metrics: this line of method leverages a pre-trained language model (e.g. BERT (Devlin\net al., 2019)) to measure the similarity between word embedding of the candidate and reference text\n(Zhang* et al., 2020; Zhao et al., 2019). Their major limitation lies in the similarity-based paradigm\nand high dependency on the quality and diversity of references. (3) Trained neural evaluators: more\nrecent research focus on specializing the PLMs by either fine-tuning on human (Rei et al., 2020)\nor synthetic (Zhong et al., 2022) labels, or pretraining on domain-relevant documents (Yuan et al.,\n2021). However, these metrics either focus on a single dimension (Wang et al., 2020a; Huang et al.,\n2020) or are limited in human relevance (Mehri & Eskenazi, 2020; Zhong et al., 2022).\nLLM-Based NLG Evaluation\nWith the emergence of LLM, recent research works focus on\nLLM-based evaluators given their promising instruction-following and generalization capability.\nA first line of work goes through preliminary explorations on LLM-based evaluators, including\nprompting methods and model variants (Fu et al., 2023; Kocmi & Federmann, 2023; Wang et al.,\n2023a; Chen et al., 2023; Liu et al., 2023). Successor research focuses on various aspects of im-\n10\nPreprint\nproving LLM-based evaluators, including factuality (Min et al., 2023), interpretability (Lu et al.,\n2023), mitigating position bias (Wang et al., 2023b), and agreement to human evaluation (Zheng\net al., 2023). Different from the above approaches, we focus on a general method to calibrate an\noff-the-shelf LLM with gradient-free approaches, to improve its alignment with human preferences\non a desired task.\n7\nCONCLUSION\nIn this work, we focus on an important question: how to calibrate and align an off-the-shelf LLM-\nbased evaluator towards human alignment in a gradient-free fashion. We first take a retrospec-\ntion into existing LLM-based NLG evaluators and uncover they suffer from insufficient prompting,\nwhere the scoring guidelines are absent and only output spaces are provided, resulting in inconsistent\nand misaligned evaluations. We emphasize the significance of aligned scoring criteria as a consensus\nbetween humans and LLM and propose AUTOCALIBRATE to automatically calibrate an LLM-based\nevaluator through criteria drafting and refinement. Inferred from human expert labels and refined\naccording to previous misalignment samples by the LLM, the criteria curated by AUTOCALIBRATE\ndemonstrate significant improvements in human correlation across evaluating text summarization,\ndata-to-text, and hallucinations. Our qualitative analysis conveys insightful intuitions and observa-\ntions on the essence of effective scoring criteria.\n8\nDISCUSSIONS\nLimitations and Broader Impacts\nThis work study on calibrating a strong LLM-based evalua-\ntor towards better human alignment. Beyond manual prompt engineering, AUTOCALIBRATE au-\ntomates the calibration process of LLM-based evaluators and provides a first experimental study\non how further LLM-based evaluators could be strengthened with better prompting. We envision\nAUTOCALIBRATE being potentially applied to a wider spectrum of tasks in NLG and beyond.\nThe primary limitation is that only criteria are mined to improve alignment. After carefully ana-\nlyzing prompts, we conclude that the criteria are most crucial, as they are most causal to the scores\ngiven, and can be regarded as a shared consensus between humans and LLMs due to their natu-\nral language form. Plus, the criteria section is the hardest to curate compared with other parts of\nthe prompt template (e.g., scoring scale, task definition), on which we primarily focus. Besides, A\nmore comprehensive research on advancing and assessing other components of prompts to calibrate\na LLM-based evaluator, and adapting it to wider tasks and languages is open to future work.\n11\nPreprint\nREFERENCES\nYi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu.\nExploring the use of large\nlanguage models for reference-free text quality evaluation: A preliminary empirical study. arXiv\npreprint arXiv:2304.00723, 2023.\nMingkai Deng, Bowen Tan, Zhengzhong Liu, Eric Xing, and Zhiting Hu. Compression, trans-\nduction, and creation: A unified framework for evaluating natural language generation. In Pro-\nceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp.\n7580\u20137605, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186. Association for Computational\nLinguistics, 2019.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu,\nand Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for\nmethods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.\nAlexander R Fabbri, Wojciech Kry\u00b4sci\u00b4nski, Bryan McCann, Caiming Xiong, Richard Socher, and\nDragomir Radev. Summeval: Re-evaluating summarization evaluation. Transactions of the Asso-\nciation for Computational Linguistics, 9:391\u2013409, 2021.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis,\nTom Kocmi, George Foster, Alon Lavie, and Andr\u00b4e FT Martins. Results of wmt22 metrics shared\ntask: Stop using bleu\u2013neural metrics are better and more robust. In Proceedings of the Seventh\nConference on Machine Translation (WMT), pp. 46\u201368, 2022.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166, 2023.\nMax Grusky, Mor Naaman, and Yoav Artzi. Newsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of NAACL-HLT, pp. 708\u2013719, 2018.\nLishan Huang, Zheng Ye, Jinghui Qin, Liang Lin, and Xiaodan Liang. Grade: Automatic graph-\nenhanced coherence metric for evaluating open-domain dialogue systems. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9230\u2013\n9240, 2020.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\nComputing Surveys, 55(12):1\u201338, 2023.\nTom Kocmi and Christian Federmann. Large language models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520, 2023.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74\u201381, 2004.\nPengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan, Shuaichen Chang, Junqi Dai, Yixin Liu, Zihuiwen\nYe, and Graham Neubig. Explainaboard: An explainable leaderboard for nlp. In Proceedings of\nthe 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Processing: System Demonstrations, pp. 280\u2013289,\n2021.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg\nevaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.\n12\nPreprint\nQingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and Dacheng Tao. Error analysis prompting enables\nhuman-like translation evaluation in large language models: A case study on chatgpt.\narXiv\npreprint arXiv:2303.13809, 2023.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nShikib Mehri and Maxine Eskenazi. Usr: An unsupervised and reference free evaluation metric for\ndialog generation. In Proceedings of the 58th Annual Meeting of the Association for Computa-\ntional Linguistics, pp. 681\u2013707, 2020.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. arXiv preprint arXiv:2305.14251, 2023.\nShashi Narayan, Shay B Cohen, and Mirella Lapata. Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, pp. 1797\u20131807, 2018.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics, pp. 311\u2013318, 2002.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. Comet: A neural framework for mt\nevaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 2685\u20132702, 2020.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nBrian Thompson and Matt Post. Automatic machine translation evaluation in many languages via\nzero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pp. 90\u2013121, 2020.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 4566\u20134575, 2015.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the\nfactual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pp. 5008\u20135020, 2020a.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the\nfactual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pp. 5008\u20135020, 2020b.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and\nJie Zhou. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048,\n2023a.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\nZhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926,\n2023b.\n13\nPreprint\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nTsung-Hsien Wen, Milica Gasic, Nikola Mrk\u02c7si\u00b4c, Pei-Hao Su, David Vandyke, and Steve Young.\nSemantically conditioned lstm-based natural language generation for spoken dialogue systems.\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,\npp. 1711\u20131721, 2015.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text gener-\nation. Advances in Neural Information Processing Systems, 34:27263\u201327277, 2021.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore:\nEvaluating text generation with bert. In International Conference on Learning Representations,\n2020. URL https://openreview.net/forum?id=SkeHuCVFDr.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. Mover-\nscore: Text generation evaluating with contextualized embeddings and earth mover distance. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\n563\u2013578, 2019.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving\nfew-shot performance of language models. In International Conference on Machine Learning, pp.\n12697\u201312706. PMLR, 2021.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. Towards a unified multi-dimensional evaluator for text generation. In Proceedings\nof the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2023\u20132038,\n2022.\n14\nPreprint\nA\nEVALUATION STRATEGY\nIn this section, we introduce meta-evaluation strategies for assessing human alignment that are ap-\nplied in this work. We select evaluation strategies primarily following previous works (Zhong et al.,\n2022; Fu et al., 2023; Liu et al., 2023). Given a dataset D consisting of NLG samples from M di-\nverse systems and J source text samples, evaluation metric f(\u00b7) (e.g., BLEU (Papineni et al., 2002))\nand correlation metric g(\u00b7), we could perform meta-evaluation at either sample or dataset level.\nSample Level\nFor sample-level meta-evaluation, we first compute correlation values on multiple\ncandidate response (from each system) to a individual sample, then average across all samples:\nfsample = 1\nJ\nJ\nX\ni=1\n(g ([\u02c6si,1, ..., \u02c6si,M] , [si,1, ..., si,M])) ,\n(4)\nwhere \u02c6su,v and su,v denote the evaluation results (if not, converted to a numeric value) for the v-th\nresponse to u-th sample from evaluator f(\u00b7) and human experts, respectively.\nDataset Level\nFor dataset-level meta-evaluation, we evaluate the correlations on all samples in the\ndataset (with a total of M \u00d7 J samples), as follows:\nfdataset = g ([\u02c6si,1, ..., \u02c6sJ,M] , [si,1, ..., sJ,M]) .\n(5)\nB\nON PERFORMANCE OF ADDING CHAIN-OF-THOUGHTS\nMetrics\nCoherence\nConsistency\nFluency\nRelevance\nAverage\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\n\u03c1\n\u03c4\nw/ Auto CoT\n0.550\n0.477\n0.495\n0.461\n0.482\n0.447\n0.564\n0.492\n0.523\n0.469\nw/o Auto CoT\n0.550\n0.478\n0.500\n0.467\n0.480\n0.447\n0.560\n0.483\n0.523\n0.467\nTable 8: Performance comparison of w/ and w/o CoT in base prompt template on SummEval.\nChain-of-thought (CoT) (Wei et al., 2022) prompting elicits reasoning in large language models by\nencouraging models to generate their rationales before obtaining an answer. As studied in recent\nresearch (Liu et al., 2023), chain-of-thoughts are beneficial to improving human alignment in NLG\nevaluation, if incorporated in the scoring prompt template. Therefore, we study whether AUTOCAL-\nIBRATE could further benefit from adding a CoT into our calibrated scoring prompts.\nTo obtain the CoT for each scoring aspect, we follow Liu et al. (2023), and results are illustrated in\nTable 8. As shown in the figure, adding CoTs to our calibrated prompts yields negligible difference.\nWe conjecture the effectiveness of \u2018CoT\u2019 is marginalized by providing informative and instructive\nscoring criteria. In contrast to math, the assessment of text quality is not a strictly chained reasoning\nprocess, so providing a CoT is essentially clarifying the evaluation rubrics, which is consistent with\nthe meaning of the criteria in this paper, and thus obtained no additional benefit. Plausibly, the\n\u2018CoT\u2019s here act to elucidate the scoring rules, rather than providing reasoning paths to follow.\nC\nCONFIGURATION DETAILS\nIn this section, we list the configuration details of AUTOCALIBRATE for each experiments. Detailed\nconfigurations for AUTOCALIBRATE are listed in Table 9. We apply the same set of configurations\nto each of the two datasets within a task.\nD\nLIST OF PROMPT TEMPLATES\nIn this section, we list prompt templates applied throughout this study, including induction templates\nfor criteria drafting, evaluation templates that utilize the generated scoring criteria, and templates for\nself-refinement of criteria.\n15\nPreprint\nTask\nSummarization\nData-to-text\nHallucination\nModel\nGPT-4-32K\nGPT-4-32K\nGPT-4-32K\nEvaluation Temperature\n0.0\n0.0\n0.0\nMax Tokens\n20\n20\n20\nCriteria Drafting Temperature\n1.0\n1.0\n1.0\nIn-context Sample Size\n4,6,8,10,12\n4,6,8,10,12,14\n6,8,10,12,14,16\nMonte-Carlo Trials\n4\n4\n3\nTemperature Sampling Count\n3\n3\n3\nMax Tokens\n768\n768\n768\nCriteria Refining Temperature\n1.0\n1.0\n1.0\nIn-context Sample Size\n1,2,4\n1,2,4\n1,2,4\nMonte-Carlo Trials\n4\n4\n4\nTemperature Sampling Count\n2\n2\n2\nMax Tokens\n768\n768\n768\nTable 9: Detailed configurations of AUTOCALIBRATE for different experiments.\n## Instructions\nPlease infer the scoring criteria for the following task:\n[Score the following summary of a news article on its [Aspect]. Please return your score on how the\nsummary is consistent with the article in the scale of 1 to 5, with 1 being the lowest.]\n- The following is some examples on evaluation scores of [Aspect] of summary (in the range of 1 to 5,\nwhere 1 being the lowest).\n- Please carefully read all the article, summary and their assigned score, and induce the most possible\nscoring rule and criteria used.\n- It is optimal that, by using the induced criteria, you are very likely to assign a same score on [Aspect]\nto the provided reference scores.\n## Criteria for [Aspect]\n- The scoring criteria been used. Now it is not explicitly provided, and you should induce it from the\nfollowing samples.\n- The induced criteria should be able to explain the scores of all the samples provided, being generic\nand concise.\n## Examples\n[In-Context Few-Shot Samples]\n## Induced Criteria\nCriteria for [Aspect]:\nFigure 5: Prompt template for criteria drafting on text summarization (SummEval, NewsRoom).\nD.1\nCRITERIA DRAFTING TEMPLATES\nPrompt templates for criteria drafting are listed in Figure 5, 6 and 7. The [Aspect] denote place-\nholders for aspects to evaluate (e.g. coherence, consistency, etc.), and sampled few-shot in-context\nexemplars are placed at [In-Context Few-Shot Samples], including samples and their expert scores.\nD.2\nEVALUATION TEMPLATES\nPrompt templates for evaluation are listed in Figure 8, 9 and 10. The [Aspect] denotes placeholders\nfor aspects to evaluate (e.g. coherence, consistency, etc.). Evaluation samples and calibrated scoring\ncriteria for each aspect are filled into corresponding placeholders during evaluation.\n16\nPreprint\n## Instructions\nPlease infer the scoring criteria for the following task:\n[Task data-to-text is to generate natural language sentences from structured data sources. This can be\nuseful for creating chatbots, voice assistants, or text summarizers. Please score the following natural\nlanguage sentence generated according to a structured data expression. Please return your score on\n[Aspect] of the sentence, in the scale of 1 to 6, with 1 being the lowest.]\n- The following is some examples on evaluation of [Aspect] of the natural language sentence generated\nfrom structured data expression (in the range of 1 to 6, where 1 being the lowest).\n- Please carefully read all expressions, generated sentence and its assigned score, and induce the most\npossible scoring rule and criteria used.\n- It is optimal that, by using the same criteria, you are very likely to assign a same score to the provided\nreference scores.\n## Criteria for [Aspect]\n- The scoring criteria been used. Now it is not explicitly provided, and you should induce it from the\nfollowing samples.\n- The induced criteria should be able to explain the scores of all the samples provided, being generic\nand concise.\n## Examples\n[In-Context Few-Shot Sampls]\n## Induced Criteria\nCriteria for [Aspect]:\nFigure 6: Prompt template for criteria drafting on data-to-text (SFRES, SFHOT).\n## Instructions\nPlease infer the scoring criteria for the following task:\n[Evaluate the factual consistency of the summary to the article. Check how well the summary is\nsupported by the article and whether it contains untruthful or misleading facts. Score 1 if the summary\nis factually consistent with the article, 0 otherwise.]\n- The following is some examples on evaluation of factual consistency of generated summary to the\narticle.\n- Please carefully read all summary - article pairs and its assigned score, and induce the most possible\nscoring rule and criteria used.\n- It is optimal that, by using the same criteria, you are very likely to assign a same score to the provided\nreference scores.\n## Criteria for factual consistency\n- The scoring criteria been used. Now it is not explicitly provided, and you should induce it from the\nfollowing samples.\n- The induced criteria should be able to explain the scores of all the samples provided, being generic\nand concise.\n## Examples\n[In-Context Few-Shot Samples]\n## Induced Criteria\nCriteria for factual consistency:\nFigure 7: Prompt template for criteria drafting on evaluating hallucinations (QAGS-XSUM/CNN).\nD.3\nCRITERIA REFINEMENT TEMPLATES\nAn example prompt template for criteria refinement can be found in Figure 11. As illustrated in the\nfigure, we first fill in the aspect and tasks to the instructions, then prompt the LLM with the previous\n17\nPreprint\n## Instructions\nScore the following summary of a news article on its [Aspect].\nPlease return your score on how the summary is [Aspect] with the article in the scale of 1 to 5, with 1\nbeing the lowest.\n## Example\n[Article and Summary to be evaluated]\n## Criteria for [Aspect]\n[Calibrated criteria for evaluating this aspect]\n## Evaluation\nNow, please evaluate how [Aspect] is the summary to the article (on a scale of 1-5, with 1 being the\nlowest). Please carefully read the article and summary, and follow the scoring criteria above to score\nthe [Aspect] of the summary to the article.\nPlease first return your score, and then provide your reasoning for the score.\n[Aspect] Score (1-5):\nFigure 8: Prompt template for evaluation on text summarization (SummEval, NewsRoom).\n## Instructions\nPlease score on the [Aspect] of a following natural language sentence generated according to a struc-\ntured data expression.\nPlease return your score on how [Aspect] is the sentence, in the scale of 1 to 6, with 1 being the lowest.\n## Example\n[Data expression and text to be evaluated]\n## Criteria for [Aspect]\n[Calibrated criteria for evaluating this aspect]\n## Evaluation\nNow, please evaluate how [Aspect] is the is the generated sentence. (on a scale of 1-6, with 1 being the\nlowest) Please carefully read the sentence and the structured data expression, and follow the scoring\ncriteria to score the [Aspect] of the sentence.\nPlease first return your score, and then provide your reasoning for the score..\n[Aspect] Score (1-5):\nFigure 9: Prompt template for evaluation on data-to-text (SFRES, SFHOT).\nIs the sentence supported by the article?\nAnswer 1 if the summary is factually consistent with the article, 0 otherwise.\n[Article and Summary to be evaluated]\n[Calibrated criteria for evaluating hallucination]\nAnswer:\nFigure 10: Prompt template for evaluation on evaluating hallucinations (QAGS-XSUM/CNN).\ncriteria, few-shot in-context samples of misaligned evaluations, together with suggested means of\nmodifications to obtain a modified version of scoring criteria for this task.\n18\nPreprint\nPlease refine and improve a scoring criteria used by a large language model in evaluating the [Aspect]\nof [Task].\nLarge language models (LLMs) are powerful neural models that can evaluate the quality of [Task].\nHowever, LLMs may not always agree with human judgments. Please refine the scoring criteria used\nby LLMs to improve its correlation with human expert scores.\nTo refine the scoring criteria used by the LLM in evaluating the [Aspect] of [Task], please follow the\nfollowing instructions step-by-step:\n1. Carefully read each example, understand each [Source acronym (e.g. article)] and its corresponding\n[Target acronym (e.g. summary)], and get your initial assessment of its quality on [Aspect].\n2. Compare the test score obtained by the LLM according to the criteria and the ground-truth score\nfrom human experts. Please think why the correlation is limited by using the current criteria, and how\ncan you improve the criteria to increase the correlation between LLM\u2019s score and human expert score.\nIf there is a small gap or no gap, this means the criteria work well in this case.\n3. Read all of the test cases and rethink how you could refine the current criteria based on your\nobservations and analysis. Then, refine the criteria to make it concise, accurate, and consistent with\nhuman judgments. When refining the criteria, you can do the following: 1) modification: adjust some\nparts of the criteria to increase its correlation with the scoring criteria that you think might used by\nhuman experts; 2) paraphrase: if the criteria is good enough, you can consider paraphrasing it to make\nmore concise and easy to understand; 3) adding aspects or details: if you fine some new underlying\nscoring rules not covered by the current criteria, consider adding them as a new line of injecting to\ncurrent criteria, but make sure not to make the criteria too long and redundant; 4) calibrate: you can\ntake other methods you think being helpful to improve the correlation with human experts.\nPlease return only your refined criteria without any additional sentences.\nOld criteria: [Previous Criteria Drafts]\nExamples: [In-Context Few-Shot Samples]\nFigure 11: Prompt template for criteria refinement with GPT-4.\nE\nEXTENDED CASE STUDY\nE.1\nLIST OF CRITERIA\nIn this section, we present a case study on scoring criteria generated by AUTOCALIBRATE for each\nevaluation aspect of each benchmark throughout this study in Table 10, 11, 12 and 13. As illustrated\nin the tables, scoring criteria generated with AUTOCALIBRATE are informative, covering significant\nrubrics to evaluate a given aspect of the target NLG task.\n19\nPreprint\nAspect\nExample Scoring Criteria\nCOH\n- The summary should be a concise and accurate representation of the main points and ideas of the article. It should avoid\nincluding irrelevant or minor details that are not essential to the article\u2019s purpose or message. - The summary should have a clear\nand logical structure that follows the article\u2019s original order of information, or provides a coherent alternative order if it improves\nthe summarization. The summary should use transitions and connectors to link the sentences and paragraphs coherently. - The\nsummary should use the same or similar terminology and tone as the article, unless there is a need to simplify or clarify some\nterms for the intended audience. The summary should avoid introducing new or unfamiliar words or concepts that are not in\nthe article or relevant to the summary. - The summary should maintain the same perspective and point of view as the article,\nunless there is a reason to shift or contrast it. The summary should not express the summarizer\u2019s own opinion, interpretation,\nor evaluation of the article, unless it is explicitly stated as such. - The summary should be grammatically correct and free of\nspelling, punctuation, and capitalization errors. The summary should use direct or indirect quotations and citations appropriately\nto acknowledge the source of the article. - The summary should be coherent and consistent with the article\u2019s topic and genre.\nThe summary should avoid introducing information or claims that contradict or deviate from the article\u2019s main message. The\nsummary should also avoid repeating information or using unnecessary filler words.\nINF\n- A summary should convey the main idea and the most important details of the original article in a concise and clear way. -\nA summary should avoid repeating information that has already been mentioned or is irrelevant to the article\u2019s main point. - A\nsummary should use accurate and specific words to describe the article\u2019s content, and avoid vague or ambiguous expressions.\n- A summary should maintain the same tone and perspective as the original article, and avoid adding personal opinions or\ninterpretations. - A summary should follow the logical order and structure of the original article, and use transition words\nor phrases to connect sentences if needed. - Possible scoring scheme: - 5: The summary meets all the criteria and has no\nsignificant flaws or errors. - 4: The summary meets most of the criteria and has minor flaws or errors that do not affect the overall\ncomprehension. - 3: The summary meets some of the criteria and has moderate flaws or errors that affect the comprehension\nof some parts. - 2: The summary meets few of the criteria and has major flaws or errors that affect the comprehension of most\nparts. - 1: The summary meets none of the criteria and has severe flaws or errors that make it incomprehensible.\nFLU\n- The score for fluency should reflect how well the summary is written in clear, natural and grammatical language, without\nunnecessary or confusing repetitions, contradictions or omissions. - A summary with a score of 5 should be fluent, coherent and\nengaging, with no errors or awkward expressions. It should use appropriate vocabulary, syntax and punctuation, and convey the\nmain information and meaning of the article. - A summary with a score of 4 should be mostly fluent, coherent and engaging,\nwith minor errors or awkward expressions that do not affect the overall meaning. It should use mostly appropriate vocabulary,\nsyntax and punctuation, and convey most of the main information and meaning of the article. - A summary with a score of 3\nshould be somewhat fluent, coherent and engaging, but with some errors or awkward expressions that may affect the overall\nmeaning or readability of the summary. It should use some appropriate vocabulary, syntax and punctuation, and convey some\nof the main information and meaning of the article, but may have some gaps or inaccuracies. - A summary with a score of 2\nshould be poorly fluent, coherent and engaging, with frequent errors or awkward expressions that significantly affect the overall\nmeaning or readability of the summary. It should use limited or inappropriate vocabulary, syntax and punctuation, and convey\nlittle of the main information and meaning of the article, or may have some major distortions or misunderstandings. - A summary\nwith a score of 1 should be very poorly fluent, coherent and engaging, with severe errors or awkward expressions that make the\nsummary incomprehensible or unintelligible. It should use very limited or inappropriate vocabulary, syntax and punctuation, and\nconvey none of the main information and meaning of the article, or may have some nonsensical or irrelevant content.\nREL\n- The summary should capture the main topic, events, and outcomes of the article in a concise and accurate way. - The summary\nshould not omit any essential information that is necessary to understand the article\u2019s purpose and significance. - The summary\nshould not include any irrelevant or redundant details that distract from the article\u2019s main points or introduce confusion. - The\nsummary should use the same or similar terminology and tone as the article, unless the article uses obscure or jargon words\nthat need to be simplified. - The summary should reflect the article\u2019s structure and organization, presenting the information in\na logical and coherent order. Examples of scoring: - Score 5: The summary meets all the criteria for relevance and provides a\nclear and comprehensive overview of the article, without any errors or gaps. - Score 4: The summary meets most of the criteria\nfor relevance and provides a mostly clear and comprehensive overview of the article, but may have some minor errors or gaps,\nsuch as missing a minor detail, using a slightly different word, or omitting a transition. - Score 3: The summary meets some\nof the criteria for relevance and provides a partially clear and comprehensive overview of the article, but has some noticeable\nerrors or gaps, such as missing a key detail, using a vague or inaccurate word, or skipping a logical connection. - Score 2:\nThe summary meets few of the criteria for relevance and provides a vaguely clear and comprehensive overview of the article,\nbut has many errors or gaps, such as missing several important details, using inappropriate or misleading words, or presenting\nthe information in a confusing or contradictory order. - Score 1: The summary meets none or almost none of the criteria for\nrelevance and provides a unclear and incomplete overview of the article, with severe errors or gaps, such as missing the main\ntopic, using incorrect or irrelevant words, or omitting the entire conclusion.\nTable 10: Case study on criteria on each aspect for NewsRoom generated by AUTOCALIBRATE.\n20\nPreprint\nAspect\nExample Scoring Criteria\nCOH\nCoherence is the quality of being consistent, logical, and well-organized in the summary. A summary is coherent if it accurately\ncaptures the main ideas and key information from the article, and presents them in a clear and concise manner. A summary is\nnot coherent if it omits important details, contradicts the article, or introduces irrelevant or confusing information. The score\nfor coherence is based on the following scale: - 5: The summary is very coherent, with no errors or flaws. - 4: The summary\nis mostly coherent, with only minor errors or gaps. - 3: The summary is somewhat coherent, but has some significant errors or\nomissions. - 2: The summary is poorly coherent, with many errors, inconsistencies, or redundancies. - 1: The summary is not\ncoherent at all, with little or no relation to the article.\nCON\n- A summary is consistent with the article if it accurately and faithfully reflects the main points, facts, and tone of the article\nwithout changing, adding, or omitting any significant information. - A summary should avoid introducing any errors, contradic-\ntions, or distortions of the original article, unless they are explicitly marked as the summary writer\u2019s opinions or interpretations.\n- A summary should use clear and precise language that matches the style and genre of the article, and avoid any vague or am-\nbiguous expressions that could mislead the reader or obscure the meaning of the article. - A summary should maintain the logical\nstructure and coherence of the article, and present the information in a well-organized and easy-to-follow manner. - A summary\nshould be concise and avoid any unnecessary or redundant details that do not contribute to the main purpose or message of the\narticle.\nFLU\n- A fluent summary should reflect the main content and structure of the original article, using clear and coherent language\nthat avoids redundancy and errors. - A fluent summary should retain the key information and details from the article, without\nintroducing any irrelevant or inaccurate information that distorts the meaning of the original text. - A fluent summary should use\nappropriate transition words, connectors, and referents to ensure the logical flow and cohesion of the summary, and avoid abrupt\nor confusing shifts in topic or perspective. - A fluent summary should use varied and precise vocabulary and grammar that suits\nthe tone and style of the article, and avoid repetition or ambiguity. - A fluent summary should use correct spelling, punctuation,\nand capitalization throughout the summary, and follow the conventions of standard written English. A possible scoring rubric\nbased on these criteria is: - 5: The summary is fluent and meets all the criteria listed above. It captures the main points and\ndetails of the article accurately and effectively, using clear and coherent language that follows the logical structure of the article.\nThe summary uses appropriate transition words, connectors, and referents to ensure cohesion, and varied and precise vocabulary\nand grammar that suits the tone and style of the article. The summary has no or minimal errors in spelling, punctuation, and\ncapitalization. - 4: The summary is mostly fluent and meets most of the criteria listed above. It captures the main points and\ndetails of the article fairly well, using mostly clear and coherent language that follows the logical structure of the article. The\nsummary uses mostly appropriate transition words, connectors, and referents to ensure cohesion, and mostly varied and precise\nvocabulary and grammar that suits the tone and style of the article. The summary has few errors in spelling, punctuation, and\ncapitalization. - 3: The summary is somewhat fluent and meets some of the criteria listed above. It captures some of the main\npoints and details of the article, but may omit or misrepresent some important information. The summary uses somewhat clear\nand coherent language, but may deviate from the logical structure of the article or have some lapses in cohesion. The summary\nuses some appropriate transition words, connectors, and referents, but may also have some inappropriate or confusing ones.\nThe summary uses some varied and precise vocabulary and grammar, but may also have some repetition or ambiguity. The\nsummary has several errors in spelling, punctuation, and capitalization. - 2: The summary is not very fluent and meets few\nof the criteria listed above. It captures few of the main points and details of the article, and may omit or misrepresent many\nimportant information. The summary uses unclear or incoherent language, and does not follow the logical structure of the article\nor have much cohesion. The summary uses few or no appropriate transition words, connectors, and referents, and may have\nmany inappropriate or confusing ones. The summary uses limited or imprecise vocabulary and grammar, and may have many\nrepetition or ambiguity. The summary has many errors in spelling, punctuation, and capitalization. - 1: The summary is not fluent\nand meets none of the criteria listed above. It captures none or almost none of the main points and details of the article, and\nmay omit or misrepresent most or all of the important information. The summary uses incomprehensible or irrelevant language,\nand does not follow the logical structure of the article or have any cohesion. The summary uses no or almost no appropriate\ntransition words, connectors, and referents, and may have only inappropriate or confusing ones. The summary uses very limited\nor inaccurate vocabulary and grammar, and may have only repetition or ambiguity. The summary has numerous and severe errors\nin spelling, punctuation, and capitalization.\nREL\n- A summary is relevant if it captures the main points or the most important information from the article, without leaving out any\ncrucial details or adding any unnecessary or inaccurate ones. - A summary is more relevant if it uses the same or similar terms\nand expressions as the article, as long as they are clear and concise. - A summary is less relevant if it omits or misrepresents\nsome of the key facts or arguments from the article, or if it introduces irrelevant or erroneous information that is not supported\nby the article. - A summary is irrelevant if it does not correspond to the article at all, or if it only mentions a minor or peripheral\naspect of the article.\nTable 11: Case study on criteria on each aspect for SummEval generated by AUTOCALIBRATE.\n21\nPreprint\nAspect\nExample Scoring Criteria\nINF\n- A natural language sentence is informative if it conveys all the relevant information from the data expression, without omitting,\nadding, or distorting any facts. - A sentence is more informative if it uses clear and natural language, without grammatical errors,\nambiguity, or redundancy. - A sentence is less informative if it leaves out some information from the data expression, or if it uses\nvague, unnatural, or incorrect language. - A possible scoring rule for informativeness is as follows: - 6: The sentence conveys\nall the information from the data expression, using clear and natural language. - 5.5: The sentence conveys all the information\nfrom the data expression, using mostly clear and natural language, but with minor issues (e.g., word choice, punctuation, etc.) -\n5: The sentence conveys all the information from the data expression, but with some issues in language clarity or naturalness.\n- 4.5: The sentence conveys most of the information from the data expression, using clear and natural language, but omitting\none detail. - 4: The sentence conveys most of the information from the data expression, but with some issues in language clarity\nor naturalness, or omitting more than one detail. - 3: The sentence conveys some of the information from the data expression,\nbut with significant issues in language clarity or naturalness, or omitting several details. - 2: The sentence conveys little of the\ninformation from the data expression, or with major issues in language clarity or naturalness, or adding or distorting facts. - 1:\nThe sentence conveys none of the information from the data expression, or with unintelligible or irrelevant language.\nNAT\n- A natural language sentence is natural if it is fluent, coherent, grammatical, and conveys the meaning of the data expression\naccurately and concisely. - The score of naturalness ranges from 1 to 6, where 1 is the lowest and 6 is the highest. - The score\nis assigned based on the following criteria: - A sentence that is completely natural, without any errors or awkwardness, and\nexpresses the data expression fully and succinctly, gets a 6. - A sentence that is mostly natural, with minor errors or redundancy,\nand expresses the data expression adequately, gets a 5 or 5.5. - A sentence that is somewhat natural, with noticeable errors or\nincompleteness, and expresses the data expression partially or vaguely, gets a 4 or 4.5. - A sentence that is barely natural, with\nserious errors or confusion, and expresses the data expression incorrectly or irrelevantly, gets a 3 or 3.5. - A sentence that is not\nnatural at all, with unacceptable errors or nonsense, and does not express the data expression at all, gets a 1 or 2.\nTable 12: Case study on criteria on each aspect for SFRES generated by AUTOCALIBRATE.\nAspect\nExample Scoring Criteria\nFACT\n- Score 1 if the sentence accurately and concisely summarizes the main facts and information from the article, without omitting,\ndistorting, or adding any significant details. - Score 0.75 if the sentence summarizes the main facts and information from the\narticle, but has some minor issues such as: omitting, distorting, or adding some less important details; using vague or imprecise\nlanguage; or being too long or verbose. - Score 0.5 if the sentence captures some of the facts and information from the article, but\nhas some major issues such as: omitting, distorting, or adding some important details; using incorrect or misleading language; or\nbeing too short or incomplete. - Score 0.25 if the sentence only captures a few of the facts and information from the article, and\nhas many issues such as: omitting, distorting, or adding most of the details; using irrelevant or contradictory language; or being\ntoo general or specific.- Score 0 if the sentence does not capture any of the facts and information from the article, or contradicts\nor misrepresents the article entirely.\nTable 13: Case study on criteria on each aspect for QAGS-CNN generated by AUTOCALIBRATE.\n22\n"
  },
  {
    "title": "VidChapters-7M: Video Chapters at Scale",
    "link": "https://arxiv.org/pdf/2309.13952.pdf",
    "upvote": "8",
    "text": "VidChapters-7M: Video Chapters at Scale\nAntoine Yang\u2020, Arsha Nagrani\u00a7, Ivan Laptev\u2020, Josef Sivic\u00b6, Cordelia Schmid\u2020\n\u2020Inria Paris, DI ENS, CNRS, PSL Research University\n\u00a7 VGG, University of Oxford\n\u00b6Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague\nhttps://antoyang.github.io/vidchapters.html\nAbstract\nSegmenting long videos into chapters enables users to quickly navigate to the infor-\nmation of their interest. This important topic has been understudied due to the lack\nof publicly released datasets. To address this issue, we present VidChapters-7M, a\ndataset of 817K user-chaptered videos including 7M chapters in total. VidChapters-\n7M is automatically created from videos online in a scalable manner by scraping\nuser-annotated chapters and hence without any additional manual annotation. We\nintroduce the following three tasks based on this data. First, the video chapter gen-\neration task consists of temporally segmenting the video and generating a chapter\ntitle for each segment. To further dissect the problem, we also define two variants of\nthis task: video chapter generation given ground-truth boundaries, which requires\ngenerating a chapter title given an annotated video segment, and video chapter\ngrounding, which requires temporally localizing a chapter given its annotated title.\nWe benchmark both simple baselines and state-of-the-art video-language models\nfor these three tasks. We also show that pretraining on VidChapters-7M transfers\nwell to dense video captioning tasks in both zero-shot and finetuning settings,\nlargely improving the state of the art on the YouCook2 and ViTT benchmarks.\nFinally, our experiments reveal that downstream performance scales well with the\nsize of the pretraining dataset. Our dataset, code, and models are publicly available\nat https://antoyang.github.io/vidchapters.html.\nIntro and new \nshoes\nHiking Low Tatras \ntodays plan\nBeautifull ascent \nLow Tatras\nNew shoes, la \nSportiva x4\nBack to the campervan\nOutro and \nmany thanks\n\u2026\n0:42\n0:00\n1:46\n2:26\n9:58\n12:10\nFigure 1: A video with user-annotated chapters in VidChapters-7M: the video is temporally\nsegmented into chapters, which are annotated with a chapter title in free-form natural language.\n1\nIntroduction\nAs online media consumption grows, the volume of video content available is increasing rapidly.\nWhile searching for specific videos is already a challenging problem, searching within a long video is\nan even less explored task. Manual navigation can often be time consuming, particularly for long\nvideos. A compelling solution for organizing content online is to segment long videos into chapters\n(see Figure 1). Chapters are contiguous, non-overlapping segments, completely partitioning a video.\nEach chapter is also labeled with a short description of the chapter content, enabling users to quickly\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\narXiv:2309.13952v1  [cs.CV]  25 Sep 2023\n0:42\n0:00\n1:46\n2:26\n9:58\n12:10\nVideo chapter generation\nVideo chapter generation with \nground-truth boundaries\nVideo chapter grounding\nBack to the campervan\nBack to the \ncampervan\n9:58\nInput chapter title\nInput full video\nOutput \ntimestamp\nInput video clip\nOutput \nchapter title\nInput full video\n0:00\nOutput chapter \ntimes and titles\n0:42\n9:58\n12:10\n\u2026\nIntro and \nnew shoes\nBack to the \ncampervan\n\u2026\n\u2026\nFigure 2: Illustration of the three tasks defined for VidChapters-7M.\nnavigate to areas of interest and easily replay different parts of a video. Chapters also give structure\nto a video, which is useful for long videos that contain inherently listed content, such as listicles [96],\ninstructional videos [64], music compilations and so on.\nGiven the plethora of content already online, our goal is to explore automatic solutions related\nto video chaptering - generating chapters automatically, and grounding chapter titles temporally\nin long videos. While the benefits of automatically chaptering videos are obvious, data for this\ntask is scarce. Video captioning datasets (such as WebVid-10M [5] and VideoCC [66]) consist of\nshort videos (10s in length), and hence are unsuitable. Web datasets consisting of longer videos\n(HowTo100M [64], YT-Temporal-1B [118]) come with aligned speech transcripts (ASR), which are\nonly weakly related to visual content, and if used as chapter titles would tend to over-segment videos.\nMoment retrieval [24, 33] or dense video captioning [42, 127] datasets are perhaps the most useful,\nbut do not focus on creating explicit structure, and instead describe low-level actions comprehensively.\nSuch datasets are also manually annotated, and hence not scalable and small in size (see Table 1).\nTo remedy this, we curate VidChapters-7M, a large-scale dataset of user-annotated video chapters\nautomatically scraped from the Web. Our dataset consists of 7M chapters for over 817K videos.\nCompared to existing datasets, videos in VidChapters-7M are long (23 minutes on average) and\ncontain rich chapter annotations consisting of a starting timestamp and a title per chapter. Our dataset\nis also diverse, with 12 different video categories having at least 20K videos each, which itself is\nthe size of existing dense video captioning datasets [29, 36, 42, 127]. On top of this dataset we\nalso define 3 video tasks (see Figure 2): (i) video chapter generation which requires temporally\nsegmenting the video and generating a chapter title for each segment; (ii) video chapter generation\ngiven ground-truth boundaries , which requires generating a chapter title given an annotated video\nsegment; and (iii) video chapter grounding , which requires temporally localizing a chapter given\nthe chapter title. All three tasks involve parsing and understanding long videos, and multi-modal\nreasoning (video and text), and hence are valuable steps towards story understanding.\nFor all three tasks, we implement simple baselines as well as recent, state-of-the-art video-text\nmethods [45, 101, 114]. We find that the tasks are far from being solved, demonstrating the value\nof this problem. Interestingly, we also show that our video chapter generation models trained on\nVidChapters-7M transfer well to dense video captioning tasks in both zero-shot and finetuning\nsettings, largely improving the state of the art on the YouCook2 [127] and ViTT benchmarks [36].\nMoreover, we show that pretraining using both speech transcripts and chapter annotations significantly\noutperforms the widely used pretraining method based only on speech transcripts [65, 114, 118].\nThis demonstrates the additional value of our dataset as a generic video-language pretraining set.\nInterestingly, we also find that the transfer performance scales with the size of the chapter dataset.\nIn summary, our contributions are:\n(i) We present VidChapters-7M, a large-scale dataset of user-annotated video chapters obtained\nfrom the Web consisting of 817K videos and 7M chapters;\n(ii) Based on this dataset, we evaluate a range of simple baselines and state-of-the-art video-\nlanguage models on the tasks of video chapter generation with and without ground-truth\nboundaries, and video chapter grounding;\n(iii) We show that video chapter generation models trained on VidChapters-7M transfer well to\ndense video captioning tasks in both zero-shot and finetuning settings, largely improving\nthe state of the art on the YouCook2 [127] and ViTT benchmarks [36], outperforming\nprior pretraining methods based on narrated videos [114], and showing promising scaling\nbehavior.\nOur dataset, code and models are publicly available on our website [1].\n2\nDataset\nNumber of\nvideos\nVideo\nduration (min)\nNumber of\ndescriptions\nAnnotations\nHowTo100M [64]\n1M\n7\n136M\nSpeech transcripts\nYT-Temporal-1B [118]\n19M\n6\n\u223c 900M\nSpeech transcripts\nHD-VILA-100M [108]\n3M\n7\n103M\nSpeech transcripts\nActivityNet Captions [42]\n20K\n3\n100K\nDense Captions\nYouCook2 [127]\n2K\n6\n15K\nDense Captions\nViTT [36]\n8K\n4\n56K\nDense Captions\nEgo4D [29]\n10K\n23\n4M\nDense Captions\nVidChapters-7M (Ours)\n817K\n23\n7M\nSpeech transcripts +\nUser-annotated Chapters\nTable 1: Comparison of VidChapters-7M with existing datasets. We consider open-sourced video\ndatasets that contain dense natural language descriptions aligned over time. VidChapters-7M is much\nlarger than current dense video captioning datasets. Compared to datasets with ASR (top 3 rows), it\nis smaller in the total number of videos but contains longer videos with richer annotations (chapters).\n2\nRelated Work\nLarge-scale vision-language datasets. The development of powerful multi-modal models [3,\n15, 23, 35, 37, 38, 46, 48\u201350, 54, 61, 62, 72, 85, 87, 90, 94, 99, 105, 115, 116, 129] has been\nmade possible by pretraining on large-scale image-caption datasets scraped from the Web such as\nSBU [68], Conceptual Captions [82], Conceptual-12M [12], LAIT [71], Wikipedia-ImageText [86],\nRedCaps [18] and LAION-5B [78]. Similarly, many strong video-language models [2, 27, 30, 41,\n45, 47, 52, 53, 58, 65, 80, 81, 88, 89, 91, 97, 100, 107, 110\u2013112, 126] have been pretrained on\nWeb-scraped video-text datasets. These datasets are largely composed of short videos paired with\ncaptions, e.g. WebVid-10M [5] and VideoCC [66], or narrated videos with speech transcripts aligned\nover time (ASR), e.g. HowTo100M [64], YT-Temporal-1B [117, 118] and HD-VILA-100M [108].\nOur proposed VidChapters-7M dataset is also downloaded from the Web, via a scalable pipeline\nwithout the need for expensive manual annotation. Unlike these datasets, VidChapters-7M consists\nof long videos with user-annotated chapters aligned over time (see Table 1), which significantly\ndiffer from ASR (see Section 3.3). Furthermore, most videos in VidChapters-7M also contain\nASR. Finally, VidChapters-7M is also related to the recent ChapterGen dataset [10], which also\nconsists of user-annotated chapters. However, ChapterGen is several orders of magnitude smaller\nthan VidChapters-7M (10K vs 817K videos) and is not open-sourced at the time of writing.\nVideo tasks. The video chapter generation task requires temporally segmenting the video into\nchapters, hence is related to video shot detection [76, 77, 84], movie scene segmentation [14, 75],\ntemporal action localization [13, 16, 59, 83, 120, 121] and temporal action segmentation [8, 21,\n26, 43, 55, 104]. However, unlike these tasks, video chapter generation also requires generating\na free-form natural language chapter title for each segment. Hence this task is also related to\nvideo captioning [25, 57, 63, 69, 98, 102, 125], video title generation [4, 119, 123], generic event\nboundary captioning [103] and dense video captioning [42, 101, 128]. Most related to video chapter\ngeneration, the dense video captioning task requires temporally localizing and captioning all events\nin an untrimmed video. In contrast, video chapter generation requires temporally segmenting the\nvideo (i.e. the start of the chapter i + 1 is the end of chapter i, and the chapters cover the full video),\nand involves generating a chapter title that is substantially shorter than a video caption. We study in\nmore detail the transfer learning between these two tasks in Section 4.4. Finally, the video chapter\ngrounding task is related to temporal language grounding [33, 34, 44, 45, 67, 113, 122, 124]. However,\nwe here focus on localizing a chapter starting point and not a start-end window. Furthermore, most\ntemporal language grounding methods represent the video only with visual inputs, while we also\nexhibit the benefits of using speech inputs for localizing chapters in videos (see Section 4.3).\n3\nVidChapters-7M: a large-scale dataset of user-chaptered videos\nOur goal is to build a large and diverse set of videos annotated with temporarily localized chapter\ninformation, consisting of chapter titles and chapter start times. In detail, chapters are contiguous,\nnon-overlapping segments, completely partitioning a video. However manual annotation of chapters\nis time consuming and expensive and therefore hard to scale. Hence we automatically scrape chapter\ninformation from videos available online, as explained in Section 3.1. Then, we perform several\nprocessing steps on this data, e.g., to extract speech transcripts, as described in Section 3.2. The\n3\noutcome is VidChapters-7M, a dataset of 817K videos with 7M chapter annotations provided by real\nusers online. Finally, we analyze VidChapters-7M in Section 3.3. Details are given next.\n3.1\nData collection\nSince early 2020, YouTube users can create chapters for uploaded videos by annotating them in\nthe YouTube description. The YouTube API, however, currently does not enable explicit search for\nuser-chaptered videos. Hence, our data collection procedure consists of: (i) Collecting a large and\ndiverse set of video candidates (characterized by their 11-character YouTube video ID), which do\nnot necessarily contain user-annotated chapters; (ii) For all video candidates, downloading the video\ndescription, automatically selecting videos with user-annotated chapters, extracting video chapters\nand downloading corresponding videos. We next describe the individual steps in more detail.\nVideo candidates. We start from a large pool of video candidates built from the YT-Temporal-180M\ndataset [117], which was constructed to be more diverse than prior large video datasets such as\nHowTo100M [64]. Note that while the released YT-Temporal-180M dataset consists of only 5M\nvideos, the authors collected a larger set of candidates by using YouTube\u2019s recommendation algorithm\nto suggest related videos. We obtained this extended list of 92 million video IDs directly from the\nauthors.\nExtracting chapters from descriptions. In the description, chapters typically constitute a block with\nconsecutive lines following the format \u201c<Timestamp>:\n<Chapter Title>\u201d or \u201c<Chapter Title>:\n<Timestamp>\u201d, where the chapter title is written in free-form natural language and its corresponding\nstart timestamp is written in MM:SS format. The video should contain at least two timestamps listed\nin ascending order. Hence we download the descriptions for all video candidates and use standard\nregular expression operations to verify whether a given description contains user-annotated chapters\nand extract them if so. Note that some videos contain chapters that are automatically generated\nby YouTube algorithms, however, these generated chapters do not appear in the descriptions and,\nhence, are excluded by our procedure for data collection. Also note that the video content is only\ndownloaded for user-chaptered videos, which is convenient for both the downloading speed and\nstorage constraints. Finally, we obtain 817K user-chaptered videos, making up 0.9% of all video\ncandidates.\n3.2\nData processing\nWe describe below how we process the previously obtained user-chaptered videos to facilitate building\nefficient video chapter generation models. For reproducibility, we publicly release the resulting speech\ntranscripts and the code for extracting visual features.\nASR extraction. We observed that most user-chaptered videos contain speech. Hence, for all\nvideos, we extract speech transcripts aligned in time with the video content (ASR) by applying the\nWhisper-Large-V2 model [73] on the audio track, using faster-whisper [40] backend for computational\nefficiency. We found that the Whisper model provides higher-quality ASR compared to the YouTube\nAPI ASR service on several data samples from VidChapters-7M. We further use WhisperX [6] to\nderive accurate word-level timestamps which we use to segment the speech transcript into sentences.\nFor example, the Whisper-Large-V2 model extracts speech segments like \u201cRight, we\u2019re gonna do the\nSynthetics Dirty Race. No we\u2019re not. [...] So we\u2019re gonna put two t-shirts and two pairs of jeans in\nthe\u201d with timestamps 20.478s and 50.465s, and the corresponding first sentence output by WhisperX\nis \u201cRight, we\u2019re gonna do the Synthetics Dirty Race.\u201d with timestamps 20.538s and 29.26s.\nVisual feature extraction. Training end-to-end deep learning models from RGB inputs on minutes-\nlong videos is computationally expensive. Hence we extract visual features with CLIP ViT-L/14\nbackbone [20, 72] at resolution 224 \u00d7 224 pixels and 1 FPS. This model has been trained to map\nimages to text descriptions with a contrastive loss on 400M Web-scraped image-text pairs.\n3.3\nData analysis\nThe result of the previously described pipeline is VidChapters-7M, a dataset of 817,076 user-chaptered\nvideos containing 6,813,732 chapters in total. We randomly split VidChapters-7M in training,\nvalidation, and testing splits with 801K, 8.2K, and 8.2K videos, respectively. We analyze VidChapters-\n4\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\nNumber of chapters\n0\n20000\n40000\n60000\n80000\nCount\n(a) Number of chapters per video\n0\n30\n60\n90\n120\n150\n180\n210\n240\n270\n300\n330\n360\n390\n420\n450\n480\n510\n540\n570\nChapter duration (s)\n0\n100000\n200000\n300000\n400000\n500000\nCount\n(b) Video chapter duration (s)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nChapter title length (number of words)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nCount\n1e6\n(c) Chapter title length (# words)\nHowto & Style\nEducation\nPeople & Blogs\nEntertainment\nScience & Technology\nGaming\nMusic\nSports\nAutos & Vehicles\nNews & Politics\nFilm & Animation\nTravel & Events\nPets & Animals\nNonprofits & Activism\nComedy\nVideo category\n0\n20000\n40000\n60000\n80000\n100000\n120000\n140000\nCount\n(d) Video category\nEnglish\nGerman\nFrench\nItalian\nSpanish; Castilian\nTagalog\nCatalan; Valencian\nPortuguese\nIndonesian\nDutch; Flemish\nAfrikaans\nSwedish\nRomanian; Moldavian; Moldovan\nNorwegian\nRussian\nKorean\nSomali\nHindi\nEstonian\nDanish\nPolish\nVietnamese\nSwahili\nWelsh\nAlbanian\nJapanese\nSlovenian\nFinnish\nLithuanian\nCroatian\nTurkish\nHungarian\nLatvian\nCzech\nChapter language\n103\n105\nCount (log scale)\n(e) Chapter languages\nFigure 3: Statistics of the VidChapters-7M dataset.\n7M below and give examples of annotated videos, more statistics, as well as a datasheet in Appendix\nSections A, C, and F, respectively.\nStatistics. VidChapters-7M is highly diverse and contains 4,894,855 distinct chapter titles. On\naverage, a video contains 8.3 chapters, start times of adjacent chapters are separated by 142.0s\nseconds, a chapter title contains 5.4 words and a video lasts 1354 seconds. The most represented\nvideo category (in YouTube\u2019s glossary) is HowTo & Style, making up 17.0% of total videos. The\ndistributions for the number of chapters per video, the video chapter duration, the length of the chapter\ntitle, and the video category are illustrated in Figure 3, and further show the diversity of VidChapters-\n7M, e.g., there are 12 different video categories with at least 20K videos in VidChapters-7M.\nASR vs Chapters. 97.3% of videos in VidChapters-7M contain speech transcripts (ASR). However,\nuser-annotated chapters significantly differ from speech transcripts: on average, a video with ASR\ncontains 269.8 speech sentences (vs 8.3 chapter titles), a speech sentence lasts 3.9 seconds (vs 142.0\nseconds for chapters) in the video and contains 11.5 words (vs 5.4 words for chapters).\nBiases. Using the langdetect [17] language detection tool, we find that 92.9%/93.9% of total videos\nin VidChapters-7M have their chapter titles/ASR in English. However, as shown in Figure 3 (bottom\nright), the distribution of chapter languages includes a long tail of languages, e.g., 13 languages\nappear in more than 1K videos of VidChapters-7M. We also use GenBit [79] to measure gender bias\nin the chapters and ASR. We observe that the percentage of female/male/non-binary gendered words\nis 19.7%/39.7%/40.7% for the chapters, and 11.6%/35.6%/52.8% for the ASR.\nEthical considerations. We employ several techniques to identify harmful visual or language content.\nWe use a classifier [78] built on top of the previously extracted CLIP features to detect not-safe-for-\nwork (NSFW) visual content (such as pornographic and sexualized content). Moreover, we use a\nlanguage model [31] to detect toxic content in chapter titles and speech transcripts. These processes\nflag 5,716 (0.70%) visually NSFW videos, 355 (0.04%) videos with toxic chapter titles and 1,368\n(0.17%) videos with toxic ASR. We assume the relatively low number of flagged videos is due to the\nregulations performed by the Web platform used to collect our dataset. Following [78], we refrain\nfrom removing these samples to encourage research in fields such as dataset curation and tag them\ninstead. Note that these automated filtering techniques are not perfect and that harmful content may\npass.\n5\nType of chapter titles\nPercentage\nSpeech and visual\n49\nAudio and visual\n2\nSpeech-only\n26\nVisual-only\n3\nAudio-only\n3\nStructure-only\n14\nUnrelated\n3\nTable 2: Manual assessment of the informativeness of chapter titles in the VidChapters-7M\ndataset over a random sample of 100 videos. Video chapter titles can be based on speech and\nvision; audio and vision; vision, audio or speech alone; or only on the structure of the video (e.g. \"step\n1\", \"step 2\" etc). In a small number of cases, video chapters are unrelated to the video content.\nManual assessment of the quality of annotations. While chapter titles are manually written and\nuploaded by real users, sometimes chapter titles are not informative about the content of the video at\nthe corresponding timestamps. To assess the quality of chapter title annotations in our dataset, we\ninspected a random sample of 100 videos in VidChapters-7M. For each video, we checked if the titles\nare related to the content of the video chapter and if so which video modalities (ASR, visual or raw\naudio) they are related to, or if they only refer to the structure of the video (e.g. chapter titles like\n\"step 1\", \"step 2\" etc). Results are presented in Table 2, and show that 83% of videos have chapters\nrelated to one or multiple modalities of the video, 14% of videos have chapters only referring to the\nstructure of the video, and 3% of videos have chapters unrelated to the video content.\n4\nExperiments\nIn this Section, we present the results of models on VidChapters-7M for the full video chapter\ngeneration task in Section 4.1, the task of video chapter generation given ground-truth boundaries in\nSection 4.2 and the video chapter grounding task in Section 4.3. Finally, we study transfer learning\nfrom video chapter generation to dense video captioning tasks in Section 4.4.\nEvaluation metrics. To evaluate the quality of the generated chapter titles (without their positions),\nwe use standard metrics used for visual captioning: BLEU [70] (B), CIDEr [95] (C), METEOR [7]\n(M) and ROUGE-L [56] (RL). To evaluate video chapter generation as a whole, including the\nlocations of the generated chapters, we follow standard protocols used for dense video captioning,\ngiven the similar nature of the two tasks. We use the standard evaluation tool [42] which calculates\nmatched pairs between generated events and the ground truth across IoU thresholds of {0.3, 0.5,\n0.7, 0.9}, and compute captioning metrics over the matched pairs. However, these metrics do not\ntake into account the story of the video and give high scores to methods generating many redundant\nchapters. Hence for an overall evaluation, we also use SODA_c [22] (S) which first tries to find\na temporally optimal matching between generated and reference chapters to capture the story of\na video, then computes METEOR scores for the matching and derives F-measure scores from the\nMETEOR scores to penalize redundant chapters. To separately evaluate chapter localization, we\nreport the recall (R@Ks, R@K) and the precision (P@Ks, P@K) across various thresholds in terms\nof the distance to the ground-truth start time or IoU with the ground-truth start-end window. We also\nreport the average recall (R) and average precision (P) across IoU thresholds of {0.3, 0.5, 0.7, 0.9}.\nImplementation details. Unless stated otherwise, for all models, we use the speech transcripts\n(ASR) and visual features extracted as explained in Section 3.2. By default, each model is taken from\nthe corresponding official implementation, and all model hyper-parameters are set according to the\noriginal papers. We use the Adam optimizer [39] for training and select the final model based on the\nbest validation performance. Our experiments are run on 8 NVIDIA A100 80GB GPUs. More details\nare included in Appendix Section D.\n4.1\nVideo chapter generation\nIn this Section, we study the task of video chapter generation that requires temporally segmenting the\nvideo and generating a chapter title for each segment.\n6\nMethod\nModalities\nPretraining Data\nFinetuned\nS\nB1\nB2 B3 B4\nC\nM\nRL\nText tiling [32] + Random\nSpeech\n\u2205\n\u2717\n0.4\n0.6\n0.2 0.1 0.0\n0.8\n0.7\n0.6\nText tiling [32] + LLaMA [93]\nSpeech\nText mixture\n\u2717\n0.2\n0.4\n0.1 0.1 0.0\n0.5\n0.3\n0.4\nShot detect [92] + BLIP-2 [51]\nVisual\n129M image-texts\n\u2717\n0.6\n0.7\n0.3 0.1 0.1\n0.2\n0.6\n0.8\nVid2Seq [114]\nSpeech+Visual C4 + HowTo100M\n\u2717\n0.1\n0.1\n0.0 0.0 0.0\n0.1\n0.1\n0.1\nPDVC [101]\nVisual\n\u2205\n\u2713\n6.8\n9.4\n3.7 1.4 0.9 35.8 9.4 11.4\nVid2Seq [114]\nSpeech\nC4\n\u2713\n10.2\n9.5\n6.7 4.0 2.7 48.8 8.5 11.0\nVid2Seq [114]\nSpeech\nC4 + HowTo100M\n\u2713\n10.5\n9.9\n7.0 4.2 2.9 50.7 8.7 11.4\nVid2Seq [114]\nVisual\nC4\n\u2713\n3.1\n2.3\n1.5 0.6 0.5 10.9 2.2\n2.9\nVid2Seq [114]\nVisual\nC4 + HowTo100M\n\u2713\n5.5\n4.5\n2.8 1.2 0.9 21.1 4.1\n5.5\nVid2Seq [114]\nSpeech+Visual\nC4\n\u2713\n10.6\n9.9\n7.0 4.2 2.8 51.3 8.8 11.6\nVid2Seq [114]\nSpeech+Visual C4 + HowTo100M\n\u2713\n11.4 10.9 7.7 4.6 3.1 55.7 9.5 12.6\nTable 3: Video chapter generation (global metrics) on VidChapters-7M test set. Here, finetuned\nrefers to finetuning on the VidChapters-7M train set, and speech refers to transcribed speech (ASR).\nMethod\nModalities\nPretraining Data\nFinetuned R@5s R@3s R@0.5 R@0.7 P@5s P@3s P@0.5 P@0.7\nText tiling [32]\nSpeech\n\u2205\n\u2717\n9.4\n5.8\n23.6\n8.9\n12.6\n7.9\n26.0\n8.8\nShot detect [92]\nVisual\n\u2205\n\u2717\n31.2\n27.4\n24.9\n12.5\n33.2\n29.7\n18.0\n8.7\nVid2Seq [114]\nSpeech+Visual C4 + HowTo100M\n\u2717\n10.7\n9.5\n5.8\n0.2\n23.3\n18.5\n1.9\n0.8\nPDVC [101]\nVisual\n\u2205\n\u2713\n21.1\n17.8\n31.2\n22.5\n45.3\n40.2\n47.2\n26.9\nVid2Seq [114]\nSpeech\nC4\n\u2713\n37.8\n29.5\n44.6\n26.1\n29.0\n23.0\n38.0\n23.4\nVid2Seq [114]\nSpeech\nC4 + HowTo100M\n\u2713\n36.7\n28.9\n46.5\n27.2\n29.5\n23.3\n40.4\n24.8\nVid2Seq [114]\nVisual\nC4\n\u2713\n35.3\n26.4\n23.6\n8.7\n17.9\n13.6\n17.2\n7.1\nVid2Seq [114]\nVisual\nC4 + HowTo100M\n\u2713\n33.5\n25.0\n33.0\n14.5\n19.5\n14.7\n26.2\n12.5\nVid2Seq [114]\nSpeech+Visual\nC4\n\u2713\n36.3\n28.6\n45.8\n26.9\n29.9\n23.8\n40.9\n24.9\nVid2Seq [114]\nSpeech+Visual C4 + HowTo100M\n\u2713\n36.4\n28.5\n48.2\n28.5\n30.3\n24.0\n43.1\n26.4\nTable 4: Video chapter generation (segmentation metrics) on VidChapters-7M test set.\nModels. For the video chapter segmentation subtask, we evaluate two zero-shot approaches (i.e., that\nare not trained on VidChapters-7M): speech text tiling [32], which detects subtopic shifts based on\nthe analysis of lexical co-occurrence patterns, and a visual scene change detection algorithm [92]\nbased on the sum of absolute differences. To derive zero-shot baselines for the full video chapter\ngeneration task, we combine text tiling and shot detection with various alternatives that can generate\ntext given text or visual input: a random baseline that predicts a random speech sentence spoken\ninside the predicted boundaries, LLaMA-7B [93] (prompted to summarize the speech transcript\nspoken inside the predicted boundaries) and BLIP-2 [51] (prompted to describe the middle video\nframe of the predicted segment). Finally, we also train and evaluate two state-of-the-art end-to-end\ndense video captioning models on VidChapters-7M: PDVC [101] which consists of a visual-only\nDETR-style [11] architecture and Vid2Seq [114] which is a multi-modal sequence-to-sequence model\npretrained on the C4 text corpus [74] and on narrated videos with ASR (e.g., YT-Temporal-1B [118]).\nFor Vid2Seq, we also report zero-shot results after pretraining on narrated videos without finetuning\non VidChapters-7M.\nImplementation details. We use the text tiling implementation from the NLTK library [9] which\ntokenizes the text into pseudosentences of size 50. We use the shot detection software from the\nFFMPEG library [92] with a confidence threshold of 0.7. For BLIP-2, we use the 3.4B-parameter\nvariant with FLAN-T5-XL [106] and CLIP ViT-L/14 [20, 72]. We reimplement Vid2Seq [114]\n(originally released in Jax) in PyTorch, use T5-Base pretrained on C4 [74] for initialization and\npretrain Vid2Seq on HowTo100M [64]. More details are included in Appendix Section D.\nResults. We report the results for video chapter generation using global metrics and localization-\nonly metrics in Tables 3 and 4, respectively. We observe that models trained on VidChapters-7M\noutperform zero-shot baselines, demonstrating the effectiveness of training on VidChapters-7M. In\nparticular, PDVC [101] has the best precision and Vid2Seq [114] achieves the best results in terms\nof overall generation and recall. We also find that Vid2Seq\u2019s speech-only mode outperforms its\nvisual-only mode and that using both speech and visual inputs leads to the best performance. This\ndemonstrates that video chapter generation is a multi-modal task. Finally, we observe that pretrain-\ning using ASR in narrated videos from HowTo100M [64] improves the video chapter generation\nperformance of the Vid2Seq model. Specifically, pretraining on HowTo100M is more beneficial for\nvision-aware models than for the speech-only model.\n7\nMethod\nModalities\nPretraining Data\nFinetuned\nB1\nB2\nB3\nB4\nC\nM\nRL\nRandom\nSpeech\n\u2205\n\u2717\n2.4\n1.3\n0.9\n0.7\n10.4\n2.2\n4.4\nLLaMA [93]\nSpeech\nText mixture\n\u2717\n0.0\n0.0\n0.0\n0.0\n0.0\n0.1\n0.2\nBLIP-2 [51]\nVisual\n129M image-texts\n\u2717\n3.1\n1.5\n0.9\n0.7\n12.4\n2.2\n4.5\nVid2Seq [114] Speech+Visual C4 + HowTo100M\n\u2717\n2.0\n1.2\n0.9\n0.6\n0.9\n0.3\n0.6\nVid2Seq [114]\nSpeech\nC4 + HowTo100M\n\u2713\n21.0 15.5 12.1 10.0 105.3 11.5 24.5\nVid2Seq [114]\nVisual\nC4 + HowTo100M\n\u2713\n10.1\n5.6\n3.5\n2.4\n47.1\n5.1\n14.7\nVid2Seq [114] Speech+Visual\nC4\n\u2713\n21.6 15.7 12.3 10.0 110.8 11.5 26.0\nVid2Seq [114] Speech+Visual C4 + HowTo100M\n\u2713\n23.5 17.2 13.4 11.0 120.5 12.6 28.3\nTable 5: Chapter title generation given ground-truth boundaries on VidChapters-7M test set.\nMethod\nModalities\nPretraining Data\nFinetuned R@10s R@5s R@3s R@1s R@0.3 R@0.5 R@0.7 R@0.9\nRandom\nSpeech\n\u2205\n\u2717\n3.1\n1.8\n1.2\n0.6\n0.7\n0.3\n0.1\n0.0\nBERT [19]\nSpeech\nBookCorpus + Wikipedia\n\u2717\n9.0\n6.8\n5.4\n2.9\n0.6\n0.3\n0.1\n0.0\nCLIP [72]\nVisual\n400M image-texts\n\u2717\n8.1\n5.2\n3.7\n1.4\n10.7\n5.2\n2.3\n0.5\nMoment-DETR [45]\nVisual\n5.4K narrated videos [45]\n\u2717\n3.2\n1.6\n1.1\n0.5\n11.3\n3.6\n0.8\n0.1\nMoment-DETR [45]\nVisual\n\u2205\n\u2713\n21.8\n15.5\n12.4\n8.3\n37.4\n27.3\n17.6\n6.4\nTable 6: Video chapter grounding on VidChapters-7M test set.\nQualitative examples. See Appendix Section B.\n4.2\nVideo chapter generation given ground-truth boundaries\nIn this Section, we study the task of generating chapter titles provided correct temporal boundaries\nof video chapters. This task is a simplification of the previously studied task where we assume\nperfect temporal segmentation. We adopt the same models and implementation details as previously\nintroduced in Section 4.1.\nResults. We report results for video chapter generation given ground-truth boundaries in Table 5.\nSimilar to the full video chapter generation task, we observe that solving the task without training\non VidChapters-7M is hard. Indeed, LLaMA [93] struggles to summarize the speech content into a\nchapter title and underperforms the random baseline. Furthermore, BLIP-2 [51] slightly improves\nover the random baseline. In addition, Vid2Seq [114] in zero-shot mode underperforms the random\nbaseline due to the large domain gap between ASR and chapter titles (see Section 3.3). In comparison,\nthe performance of models trained on VidChapters-7M is significantly higher. Moreover, Vid2Seq\u2019s\nspeech-only mode outperforms its visual-only mode, and using both speech and visual inputs is\nbeneficial, confirming the benefit of multi-modal reasoning for the task of generating chapter titles.\nFinally, pretraining on narrated videos from HowTo100M [64] improves the performance of the\nVid2Seq model on VidChapters-7M.\n4.3\nVideo chapter grounding\nIn this Section, we study the task of video chapter grounding that requires a model to temporally\nlocalize a chapter start time (or start-end window) given an annotated chapter title (query). Hence,\ncompared to the video chapter generation task, we here assume chapter titles to be given and focus\non the temporal chapter localization only.\nModels. We evaluate three zero-shot alternatives: a random baseline that randomly picks the\ntimestamps of a speech sentence in the video, a BERT [19] baseline that picks the timestamps of the\nspeech sentence that has the closest text embedding with the queried chapter title, and a CLIP [72]\nbaseline picking the frames where the query-frame similarity score drops from the highest scoring\nframe by a certain threshold \u03f5. We also train and evaluate on VidChapters-7M a state-of-the-art\nend-to-end video grounding model: Moment-DETR [45] which is designed for moment retrieval\nbased on visual inputs. Furthermore, we report zero-shot performance of Moment-DETR obtained\nwith the model checkpoint from Lei et al. [45] pretrained on 5.4K narrated videos with ASR from the\nQVHighlights dataset [45].\nImplementation details. We use the [CLS] token sequence embedding for the BERT baseline and a\nthreshold of \u03f5 = 0.05 for the CLIP baseline. More details are provided in Appendix Section D.\n8\nMethod\nModalities\nPretraining Data\nYouCook2 (val)\nViTT (test)\nS\nC\nM\nR\nP\nS\nC\nM\nR\nP\nPDVC [101]\nV\n\u2205\n4.4\n22.7\n4.7\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\nE2ESG [130]\nT+V\nC4 + WikiHow\n\u2014\n25.0\n3.5\n20.7 20.6\n\u2014\n25.0\n8.1\n32.2 32.1\nVid2Seq [114]\nT+V\nC4 + HTM\n8.3\n48.3\n9.5\n27.1 27.0\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\nVid2Seq [114]\nT+V\nC4 + YT-Temporal-1B\n7.9\n47.1\n9.3\n27.9 27.8 13.5 43.5\n8.5\n42.6 46.2\nPDVC\u2020\nV\n\u2205\n4.8\n28.8\n5.8\n22.6 33.1\n9.4\n40.6 16.5 19.2 37.4\nPDVC\u2020\nV\nVC (Chap.)\n5.9\n34.7\n7.5\n28.8 36.4 10.1 41.5 16.1 21.3 37.2\nVid2Seq\u2020\nT+V\nC4 + HTM\n8.6\n53.2 10.5 29.2 26.2 14.1 44.8\n8.7\n43.8 44.5\nVid2Seq\u2020\nT+V\nC4 + VC (ASR+Chap.)\n9.8\n62.9 11.7 32.5 30.1 15.1 50.9\n9.6\n45.1 46.7\nVid2Seq\u2020\nT+V\nC4 + HTM + VC (ASR)\n8.4\n50.1 10.3 29.7 26.3 14.3 45.6\n8.8\n43.7 44.9\nVid2Seq\u2020\nT+V\nC4 + HTM + 1% of VC (ASR+Chap)\n8.8\n52.7 10.4 29.3 27.6 13.5 41.6\n8.2\n44.7 42.1\nVid2Seq\u2020\nT+V\nC4 + HTM + 10% of VC (ASR+Chap.)\n9.9\n63.9 12.1 32.4 31.4 14.5 47.4\n9.2\n45.3 45.9\nVid2Seq\u2020\nT+V\nC4 + HTM + VC (ASR+Chap.)\n10.3 67.2 12.3 34.0 31.2 15.0 50.0\n9.5\n45.5 46.9\nTable 7: Comparison with the state of the art on the YouCook2 and ViTT dense video captioning\nbenchmarks. T: Transcribed speech, V: Visual, HTM: HowTo100M [64], VC: VidChapters-7M,\nChap.: Chapters. \u2020 denote results of our experiments.\nMethod\nModalities\nPretraining Data\nYouCook2 (val)\nViTT (test)\nS\nC\nM\nR\nP\nS\nC\nM\nR\nP\nText tiling [32] + Random\nT\n\u2205\n0.3 0.9 0.3 3.8\n6.6 0.3 0.6 0.6 11.6 24.4\nText tiling [32] + LLaMA [93]\nT\nText mixture\n0.2 0.6 0.2 3.8\n6.6 0.2 0.6 0.5 11.6 24.4\nShot detect [92] + BLIP-2 [51]\nV\n129M image-texts\n0.6 1.0 0.5 8.9\n5.5 0.2 0.1 0.2 3.1 13.7\nVid2Seq [114]\nV\nC4 + VC (ASR)\n0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.2\n0.8\nVid2Seq [114]\nV\nC4 + VC (Chap.)\n0.7 1.1 0.5 21.3 8.6 1.5 1.9 0.6 18.9 10.4\nVid2Seq [114]\nT+V\nC4 + HTM\n0.0 0.1 0.0 0.5\n0.6 0.0 0.0 0.0 0.5\n1.0\nVid2Seq [114]\nT+V\nC4 + VC (ASR)\n0.1 0.1 0.0 1.1\n0.9 0.0 0.0 0.0 0.7\n0.6\nVid2Seq [114]\nT+V\nC4 + VC (Chap.)\n0.1 0.2 0.1 0.7\n1.4 0.7 1.1 0.3 14.3 12.8\nVid2Seq [114]\nT+V\nC4 + VC (ASR+Chap.)\n3.2 10.2 2.9 20.6 19.7 9.1 30.2 6.7 33.8 40.8\nVid2Seq [114]\nT+V\nC4 + HTM + VC (ASR)\n0.0 0.1 0.0 1.2\n0.9 0.0 0.0 0.0 0.8\n0.7\nVid2Seq [114]\nT+V\nC4 + HTM + 1% of VC (ASR+Chap.) 2.7 7.2 2.1 18.1 17.3 5.5 15.5 4.3 31.3 37.1\nVid2Seq [114]\nT+V\nC4 + HTM + 10% of VC (ASR+Chap.) 3.2 11.5 3.0 19.4 19.2 6.4 21.6 5.3 31.0 38.2\nVid2Seq [114]\nT+V\nC4 + HTM + VC (ASR+Chap.)\n3.9 13.3 3.4 22.3 20.1 9.0 28.0 6.5 33.7 40.1\nTable 8: Zero-shot dense video captioning on the YouCook2 and ViTT benchmarks. T: Tran-\nscribed speech, V: Visual, HTM: HowTo100M [64], VC: VidChapters-7M, Chap.: Chapters.\nResults. We report results for the video chapter grounding task in Table 6. We first observe that the\nsimple zero-shot baselines based on ASR can decently find start times, but struggle to predict start-end\nwindows due to the important domain gap between ASR and video chapters (see Section 3.3). The\nCLIP [72] baseline slightly underperforms the BERT baseline [19] at retrieving start times, but is\nmuch better at finding start-end windows. Furthermore, the Moment-DETR model [45] trained on\nVidChapters-7M outperform the zero-shot baselines for both localization of start times and start-end\nwindows, which further demonstrates the effectiveness of training on VidChapters-7M. Finally, we\nnote that Moment-DETR cannot handle speech inputs, but hope that our results showing the benefit\nof this modality on other tasks in VidChapters-7M will foster research in the localization of language\nqueries in untrimmed videos using multi-modal inputs (vision and speech transcripts).\n4.4\nTransfer learning on dense video captioning\nIn this Section, we investigate the pretraining of video-language models on our new VidChapters-7M.\nTo this end, we adopt video chapter generation models trained on VidChapters-7M (see Section 4.1)\nto the tasks of dense video captioning with or without finetuning.\nDatasets. We use two dense video captioning datasets. YouCook2 [127] has 2K untrimmed videos of\ncooking procedures. On average, each video lasts 320s and is annotated with 7.7 temporally-localized\nsentences. ViTT [36] was created to better reflect the distribution of instructional videos in the wild\ncompared to YouCook2, and consists of 8K untrimmed instructional videos. On average, each video\nlasts 250s and is annotated with 7.1 temporally-localized short tags. For both datasets, we extract\nspeech transcripts and visual features as described in Section 3.2, and follow the standard splits for\ntraining, validation and testing. Note that we only use videos available on YouTube at the time of the\nwork, resulting in 10 to 20% less videos than in the original datasets.\nImplementation details. See Section 4.1 and Appendix Section D.\n9\nResults after finetuning. In Table 7, we show that pretraining for video chapter generation on\nVidChapters-7M greatly improves the downstream dense video captioning performance compared\nto training from scratch or pretraining only with ASR data as done in previous work [114]. We\nalso find that pretraining both on HowTo100M [64] and VidChapters-7M results in the best overall\nperformance. In particular, the Vid2Seq model pretrained on both HowTo100M and VidChapters-7M\nlargely improves the state of the art on both the YouCook2 and ViTT benchmarks. In detail, on\nthe YouCook2 benchmark, in the setting with C4 + HowTo100M pretraining, we observe that a\nboost of about 4.9 points in CIDEr is obtained with our reimplementation of Vid2Seq, and that 14.0\nadditional points in CIDEr are obtained by pretraining on VidChapters-7M. Finally, we report the\nresults of the Vid2Seq model after pretraining on different fractions of VidChapters-7M for a fixed\nnumber of iterations. We construct these subsets such that larger subsets include the smaller ones.\nThese results suggest that the scale of the chapter dataset is an important factor in the downstream\ndense video captioning performance. We conclude that VidChapters-7M opens a promising avenue\nfor multi-modal pretraining. We further show qualitative examples of dense video captioning in\nAppendix Section B.\nZero-shot dense video captioning. In Table 8, we report results obtained by directly applying\nvideo chapter generation models trained on VidChapters-7M for dense video captioning without\nfinetuning for this task. As far as we know, our work is the first to explore this challenging zero-shot\nsetting where no manual annotation of dense video captions is used for training. The Vid2Seq model\ntrained only using ASR data underperforms the random baseline, due to the large domain difference\nbetween speech transcripts and dense captions [114]. In the visual-only setting, the variant trained\non chapter annotations is better than the variant trained on ASR annotations. In the visual+speech\nsettings, only using chapter annotations does not perform well, as training only on chapters (i.e.,\nwithout speech) does not enable the model to learn how to use the input speech modality at inference.\nHowever, using both ASR and chapter annotations results in a largely better zero-shot dense video\ncaptioning performance and outperforms all baselines not trained on VidChapters-7M, demonstrating\nthe complementary nature of the ASR and chapters annotations. Finally, we also observe the benefits\nof increasing the size of the pretraining dataset of chapters in this setting.\n5\nConclusion, Limitations, and Societal Impacts\nIn this work, we presented VidChapters-7M, a large-scale dataset of user-chaptered videos. Fur-\nthermore, we evaluated a variety of baselines on the tasks of video chapter generation with and\nwithout ground-truth boundaries and video chapter grounding. Finally, we investigated the potential\nof VidChapters-7M for pretraining video-language models and demonstrated improved performance\non the dense video captioning tasks. VidChapters-7M thus provides a new resource to the research\ncommunity that can be used both as a benchmark for the video chapter generation tasks and as a\npowerful means for pretraining generic video-language models.\nLimitations. As it is derived from YT-Temporal-180M [117], VidChapters-7M inherits the biases in\nthe distribution of video categories reflected in this dataset.\nSocietal Impacts. The development of video chapter generation models might facilitate potentially\nharmful downstream applications, e.g., video surveillance. Moreover, models trained on VidChapters-\n7M might reflect biases present in videos from YouTube. It is important to keep this in mind when\ndeploying, analysing and building upon these models.\nAcknowledgements\nThis work was granted access to the HPC resources of IDRIS under the allocation 2023-A0131011670\nmade by GENCI. The work was funded by Antoine Yang\u2019s Google PhD fellowship, the French\ngovernment under management of Agence Nationale de la Recherche as part of the \"Investissements\nd\u2019avenir\" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), the Louis Vuitton ENS\nChair on Artificial Intelligence, the European Regional Development Fund under project IMPACT\n(reg. no. CZ.02.1.01/0.0/0.0/15 003/0000468). We thank Jack Hessel and R\u00e9mi Lacroix for helping\nwith collecting the dataset, and Antoine Miech for interesting discussions.\n10\nReferences\n[1] VidChapters-7M project webpage. https://antoyang.github.io/vidchapters.html.\n2, 18, 28, 30, 31\n[2] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and\nBoqing Gong. VATT: Transformers for multimodal self-supervised learning from raw video,\naudio and text. NeurIPS, 2021. 3\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. In NeurIPS, 2022. 3\n[4] Soheyla Amirian, Khaled Rasheed, Thiab R Taha, and Hamid R Arabnia. Automatic generation\nof descriptive titles for video clips using deep learning. In Advances in Artificial Intelligence\nand Applied Cognitive Computing: Proceedings from ICAI\u201920 and ACC\u201920, 2021. 3\n[5] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video\nand image encoder for end-to-end retrieval. In ICCV, 2021. 2, 3\n[6] Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. WhisperX: Time-accurate\nspeech transcription of long-form audio. In Interspeech, 2023. 4\n[7] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for mt evaluation with\nimproved correlation with human judgments. In Proceedings of the acl workshop on intrinsic\nand extrinsic evaluation measures for machine translation and/or summarization, 2005. 6\n[8] Nadine Behrmann, S Alireza Golestaneh, Zico Kolter, J\u00fcrgen Gall, and Mehdi Noroozi.\nUnified fully and timestamp supervised temporal action segmentation via sequence to sequence\ntranslation. In ECCV, 2022. 3\n[9] Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python:\nanalyzing text with the natural language toolkit. O\u2019Reilly Media, Inc., 2009. 7\n[10] Xiao Cao, Zitan Chen, Canyu Le, and Lei Meng. Multi-modal video chapter generation. In\nBMVC, 2022. 3\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 7\n[12] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 3\n[13] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A Ross, Jia Deng, and\nRahul Sukthankar. Rethinking the Faster R-CNN architecture for temporal action localization.\nIn CVPR, 2018. 3\n[14] Shixing Chen, Xiaohan Nie, David Fan, Dongqing Zhang, Vimal Bhat, and Raffay Hamid.\nShot contrastive self-supervised learning for scene boundary detection. In CVPR, 2021. 3\n[15] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng,\nand Jingjing Liu. UNITER: Universal image-text representation learning. In ECCV, 2020. 3\n[16] Feng Cheng and Gedas Bertasius. TALLformer: Temporal action localization with long-\nmemory transformer. In ECCV, 2022. 3\n[17] Michal Danil\u00e1k.\nLanguage detection library.\nhttps://github.com/Mimino666/\nlangdetect, 2021. 5\n[18] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. RedCaps: Web-curated image-\ntext data created by the people, for the people. In NeurIPS Datasets and Benchmarks, 2021.\n3\n[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In NAACL-HLT, 2019. 8, 9\n11\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In ICLR, 2021. 4, 7\n[21] Yazan Abu Farha and Jurgen Gall. MS-TCN: Multi-stage temporal convolutional network for\naction segmentation. In CVPR, 2019. 3\n[22] Soichiro Fujita, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura, and Masaaki Nagata.\nSODA: Story oriented dense video captioning evaluation framework. In ECCV, 2020. 6\n[23] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale\nadversarial training for vision-and-language representation learning. In NeurIPS, 2020. 3\n[24] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. TALL: Temporal activity localiza-\ntion via language query. In ICCV, 2017. 2\n[25] Lianli Gao, Zhao Guo, Hanwang Zhang, Xing Xu, and Heng Tao Shen. Video captioning with\nattention-based lstm and semantic consistency. IEEE Transactions on Multimedia, 2017. 3\n[26] Shang-Hua Gao, Qi Han, Zhong-Yu Li, Pai Peng, Liang Wang, and Ming-Ming Cheng.\nGlobal2Local: Efficient structure search for video action segmentation. In CVPR, 2021. 3\n[27] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging\nvideo-text retrieval with multiple choice questions. In CVPR, 2022. 3\n[28] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna\nWallach, Hal Daum\u00e9 Iii, and Kate Crawford. Datasheets for datasets. Communications of the\nACM, 2021. 18, 24\n[29] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit\nGirdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar\nNagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma,\nMichael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv\nBatra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph\nFeichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie,\nCristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym\nKolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li,\nKarttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu,\nWill Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey\nSoutherland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi,\nYunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard\nGhanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard\nNewcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng\nShou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4D: Around\nthe World in 3,000 Hours of Egocentric Video. In CVPR, 2022. 2, 3\n[30] Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal alignment networks for long-term\nvideo. In CVPR, 2022. 3\n[31] Laura Hanu and Unitary team. Detoxify. https://github.com/unitaryai/detoxify,\n2020. 5, 20, 26\n[32] Marti A Hearst. Text tiling: Segmenting text into multi-paragraph subtopic passages. Compu-\ntational linguistics, 1997. 7, 9, 24\n[33] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan\nRussell. Localizing moments in video with natural language. ICCV, 2017. 2, 3\n[34] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan\nRussell. Localizing moments in video with temporal language. In EMNLP, 2018. 3\n[35] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan\nWang. Scaling up vision-language pre-training for image captioning. In CVPR, 2022. 3\n12\n[36] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu Soricut. Multimodal pretraining\nfor dense video captioning. In AACL-IJCNLP, 2020. 2, 3, 9\n[37] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu.\nSeeing out of the box: End-to-end pre-training for vision-language representation learning. In\nCVPR, 2021. 3\n[38] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In ICML, 2021. 3\n[39] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR,\n2015. 6\n[40] Guillaume Klein.\nfaster-whisper library.\nhttps://github.com/guillaumekln/\nfaster-whisper, 2023. 4\n[41] Dohwan Ko, Joonmyung Choi, Juyeon Ko, Shinyeong Noh, Kyoung-Woon On, Eun-Sol Kim,\nand Hyunwoo J Kim. Video-text representation learning via differentiable weak temporal\nalignment. In CVPR, 2022. 3\n[42] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-\ncaptioning events in videos. In ICCV, 2017. 2, 3, 6\n[43] Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal\nconvolutional networks for action segmentation and detection. In CVPR, 2017. 3\n[44] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. TVR: A large-scale dataset for\nvideo-subtitle moment retrieval. In ECCV, 2020. 3\n[45] Jie Lei, Tamara L Berg, and Mohit Bansal. Detecting moments and highlights in videos via\nnatural language queries. In NeurIPS, 2021. 2, 3, 8, 9, 23\n[46] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu.\nLess is more: ClipBERT for video-and-language learning via sparse sampling. In CVPR, 2021.\n3\n[47] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. Align and\nprompt: Video-and-language pre-training with entity prompts. In CVPR, 2022. 3\n[48] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder-VL: A\nuniversal encoder for vision and language by cross-modal pre-training. In AAAI, 2020. 3\n[49] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and\nSteven Chu Hong Hoi. Align before fuse: Vision and language representation learning with\nmomentum distillation. In NeurIPS, 2021.\n[50] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image\npre-training for unified vision-language understanding and generation. In ICML, 2022. 3\n[51] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. In ICML, 2023. 7,\n8, 9, 20, 23, 24\n[52] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. HERO:\nHierarchical encoder for video+language omni-representation pre-training. In EMNLP, 2020.\n3\n[53] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang.\nLAVENDER: Unifying video-language understanding as masked language modeling. In\nCVPR, 2023. 3\n[54] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. In ECCV, 2020. 3\n13\n[55] Zhe Li, Yazan Abu Farha, and Jurgen Gall. Temporal action segmentation from timestamp\nsupervision. In CVPR, 2021. 3\n[56] Chin-Yew Lin. Rouge: a package for automatic evaluation of summaries. In Proceedings of\nthe Workshop on Text Summarization Branches Out (WAS), 2004. 6\n[57] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and\nLijuan Wang. SwinBERT: End-to-end transformers with sparse attention for video captioning.\nIn CVPR, 2022. 3\n[58] Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Zhong-\ncong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric video-\nlanguage pretraining. In NeurIPS, 2022. 3\n[59] Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei Zhang, Song Bai, and Xiang Bai. End-\nto-end temporal action detection with transformer. In IEEE Transactions on Image Processing,\n2022. 3\n[60] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 23\n[61] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining task-agnostic\nvisiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019. 3\n[62] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1:\nMulti-task vision and language representation learning. In CVPR, 2020. 3\n[63] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin Chen, and\nMing Zhou. UniViLM: A unified video and language pre-training model for multimodal\nunderstanding and generation. arXiv preprint arXiv:2002.06353, 2020. 3\n[64] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and\nJosef Sivic. HowTo100M: Learning a text-video embedding by watching hundred million\nnarrated video clips. In ICCV, 2019. 2, 3, 4, 7, 8, 9, 10, 21\n[65] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew\nZisserman. End-to-end learning of visual representations from uncurated instructional videos.\nIn CVPR, 2020. 2, 3\n[66] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun,\nand Cordelia Schmid. Learning audio-video modalities from image captions. In ECCV, 2022.\n2, 3\n[67] Guoshun Nan, Rui Qiao, Yao Xiao, Jun Liu, Sicong Leng, Hao Zhang, and Wei Lu. Interven-\ntional video grounding with dual contrastive learning. In CVPR, 2021. 3\n[68] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1\nmillion captioned photographs. In NeurIPS, 2011. 3\n[69] Yingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. Video captioning with transferred semantic\nattributes. In CVPR, 2017. 3\n[70] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for\nautomatic evaluation of machine translation. In ACL, 2002. 6\n[71] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti.\nImageBERT:\nCross-modal pre-training with large-scale weak-supervised image-text data. arXiv preprint\narXiv:2001.07966, 2020. 3\n[72] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021. 3, 4, 7, 8, 9\n[73] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya\nSutskever. Robust speech recognition via large-scale weak supervision. arXiv preprint\narXiv:2212.04356, 2022. 4, 27\n14\n[74] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. JMLR, 2020. 7, 21\n[75] Anyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, and Dahua Lin.\nA local-to-global approach to multi-modal movie scene segmentation. In CVPR, 2020. 3\n[76] Zeeshan Rasheed and Mubarak Shah. Scene detection in hollywood movies and tv shows. In\nCVPR, 2003. 3\n[77] Yong Rui, Thomas S Huang, and Sharad Mehrotra. Exploring video structure beyond the shots.\nIn IEEE International Conference on Multimedia Computing and Systems, 1998. 3\n[78] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\nLAION-5B: An open large-scale dataset for training next generation image-text models. In\nNeurIPS, 2022. 3, 5, 20, 26\n[79] Kinshuk Sengupta, Rana Maher, Declan Groves, and Chantal Olieman. Genbit: measure and\nmitigate gender bias in language datasets. Microsoft Journal of Applied Research, 2021. 5\n[80] Paul Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid. Look before you speak: Visually\ncontextualized utterances. In CVPR, 2021. 3\n[81] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. End-to-end genera-\ntive pretraining for multimodal video captioning. In CVPR, 2022. 3\n[82] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual Captions: A\ncleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. 3\n[83] Zheng Shou, Dongang Wang, and Shih-Fu Chang. Temporal action localization in untrimmed\nvideos via multi-stage CNNs. In CVPR, 2016. 3\n[84] Panagiotis Sidiropoulos, Vasileios Mezaris, Ioannis Kompatsiaris, Hugo Meinedo, Miguel\nBugalho, and Isabel Trancoso. Temporal video segmentation to scenes using high-level\naudiovisual features. IEEE TCSVT, 2011. 3\n[85] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba,\nMarcus Rohrbach, and Douwe Kiela. FLAVA: A foundational language and vision alignment\nmodel. In CVPR, 2022. 3\n[86] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit:\nWikipedia-based image text dataset for multimodal multilingual machine learning. In ACM\nSIGIR Conference on Research and Development in Information Retrieval, 2021. 3\n[87] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT:\nPre-training of generic visual-linguistic representations. In ICLR, 2019. 3\n[88] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. VideoBERT:\nA joint model for video and language representation learning. In ICCV, 2019. 3\n[89] Yuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu, Huan Yang, and Jianlong Fu. Long-form\nvideo-language pre-training with multimodal temporal contrastive learning. In NeurIPS, 2022.\n3\n[90] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from\ntransformers. In EMNLP, 2019. 3\n[91] Zineng Tang, Jaemin Cho, Yixin Nie, and Mohit Bansal. TVLT: Textless vision-language\ntransformer. In NeurIPS, 2022. 3\n[92] Suramya Tomar. Converting video formats with ffmpeg. Linux Journal, 2006. 7, 9, 24\n15\n[93] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-\noth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA:\nOpen and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 7, 8,\n9, 20, 23, 24\n[94] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot learning with frozen language models. In NeurIPS, 2021. 3\n[95] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based\nimage description evaluation. In CVPR, 2015. 6\n[96] Bram Vijgen et al. The listicle: An exploring research on an interesting shareable new media\nphenomenon. Studia Universitatis Babes-Bolyai-Ephemerides, 59(1):103\u2013122, 2014. 2\n[97] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu,\nYing Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language\npre-training. In CVPR, 2023. 3\n[98] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Reconstruction network for video captioning.\nIn CVPR, 2018. 3\n[99] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu,\nCe Liu, and Lijuan Wang. GIT: A generative image-to-text transformer for vision and language.\nIn TMLR, 2022. 3\n[100] Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Object-aware video-language pre-training for retrieval. In CVPR, 2022. 3\n[101] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo. End-to-end\ndense video captioning with parallel decoding. In ICCV, 2021. 2, 3, 7, 9, 21, 24\n[102] Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, and William Yang Wang. Video\ncaptioning via hierarchical reinforcement learning. In CVPR, 2018. 3\n[103] Yuxuan Wang, Difei Gao, Licheng Yu, Weixian Lei, Matt Feiszli, and Mike Zheng Shou.\nGEB+: A benchmark for generic event boundary captioning, grounding and retrieval. In\nECCV, 2022. 3\n[104] Zhenzhi Wang, Ziteng Gao, Limin Wang, Zhifeng Li, and Gangshan Wu. Boundary-aware\ncascade networks for temporal action segmentation. In ECCV, 2020. 3\n[105] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. SimVLM:\nSimple visual language model pretraining with weak supervision. In ICLR, 2022. 3\n[106] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In\nICLR, 2022. 7\n[107] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze,\nLuke Zettlemoyer, and Christoph Feichtenhofer. VideoCLIP: Contrastive pre-training for\nzero-shot video-text understanding. In EMNLP, 2021. 3\n[108] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu,\nand Baining Guo. Advancing high-resolution video-language representation with large-scale\nvideo transcriptions. In CVPR, 2022. 3\n[109] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,\nAditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text\ntransformer. In NAACL, 2021. 23\n[110] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask:\nLearning to answer questions from millions of narrated videos. In ICCV, 2021. 3\n[111] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Learning to\nanswer visual questions from web videos. IEEE TPAMI, 2022.\n16\n[112] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video\nquestion answering via frozen bidirectional language models. In NeurIPS, 2022. 3\n[113] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. TubeDETR:\nSpatio-temporal video grounding with transformers. In CVPR, 2022. 3\n[114] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan\nLaptev, Josef Sivic, and Cordelia Schmid. Vid2Seq: Large-scale pretraining of a visual\nlanguage model for dense video captioning. In CVPR, 2023. 2, 7, 8, 9, 10, 21, 23, 24\n[115] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE-ViL:\nKnowledge enhanced vision-language representations through scene graph. In AAAI, 2020. 3\n[116] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong\nHu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for\ncomputer vision. arXiv preprint arXiv:2111.11432, 2021. 3\n[117] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi,\nand Yejin Choi. MERLOT: Multimodal neural script knowledge models. In NeurIPS, 2021. 3,\n4, 10, 25, 27\n[118] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi,\nAditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. MERLOT Reserve: Neural script\nknowledge through vision and language and sound. In CVPR, 2022. 2, 3, 7, 21\n[119] Kuo-Hao Zeng, Tseng-Hung Chen, Juan Carlos Niebles, and Min Sun. Title generation for\nuser generated videos. In ECCV, 2016. 3\n[120] Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang, and\nChuang Gan. Graph convolutional networks for temporal action localization. In CVPR, 2019.\n3\n[121] Chenlin Zhang, Jianxin Wu, and Yin Li. ActionFormer: Localizing moments of actions with\ntransformers. In ECCV, 2022. 3\n[122] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. Span-based localizing network for\nnatural language video localization. In ACL, 2020. 3\n[123] Shengyu Zhang, Ziqi Tan, Zhou Zhao, Jin Yu, Kun Kuang, Tan Jiang, Jingren Zhou, Hongxia\nYang, and Fei Wu. Comprehensive information integration modeling framework for video\ntitling. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,\n2020. 3\n[124] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. Learning 2d temporal adjacent\nnetworks for moment localization with natural language. In AAAI, 2020. 3\n[125] Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, and Zheng-Jun\nZha. Object relational graph with teacher-recommended learning for video captioning. In\nCVPR, 2020. 3\n[126] Yue Zhao, Ishan Misra, Philipp Kr\u00e4henb\u00fchl, and Rohit Girdhar. Learning video representations\nfrom large language models. In CVPR, 2023. 3\n[127] Luowei Zhou, Xu Chenliang, and Jason J. Corso. Towards automatic learning of procedures\nfrom web instructional videos. In AAAI, 2018. 2, 3, 9\n[128] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end\ndense video captioning with masked transformer. In CVPR, 2018. 3\n[129] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao.\nUnified vision-language pre-training for image captioning and VQA. In AAAI, 2020. 3\n[130] Wanrong Zhu, Bo Pang, Ashish Thapliyal, William Yang Wang, and Radu Soricut. End-to-end\ndense video captioning as sequence generation. In COLING, 2022. 9\n17\nAppendix\nIn this Appendix, we present the following items:\n(i) Additional examples from our VidChapters-7M dataset (Section A).\n(ii) Qualitative examples of video chapter generation and dense video caption prediction (Sec-\ntion B).\n(iii) Additional data analysis of our VidChapters-7M dataset (Section C).\n(iv) Additional implementation details (Section D).\n(v) Video chapter generation results split by language (Section E).\n(vi) A datasheet [28] for VidChapters-7M (Section F). Note that in this datasheet, the hosting,\nlicensing, and maintenance plan of VidChapters-7M is presented.\nNote that our code, models and the VidChapters-7M dataset can be found on our website [1].\nA\nAdditional examples from VidChapters-7M\nIn Figure 4, we provide additional examples that complement Figure 1. These examples illustrate the\ndiversity of the data in VidChapters-7M, e.g., our dataset includes review videos, cooking videos,\nclothing fitting videos, ASMR videos, and videos of conversations. These examples also show the\nmulti-modal nature of the chapter data. Indeed, chapters depict visual events (e.g., the mini chicken\nburgers that appear in the second video), conversations (see the last video), or events in the raw audio\n(e.g., the sound of the crinkly plastic bag in the penultimate video) in various scenarios.\nB\nQualitative examples of video chapter generation and dense video caption\nprediction\nWe present qualitative results for video chapter generation and dense video captioning in Figures 5\nand 6. Compared with the speech-only model, a key advantage of the speech+visual video chapter\ngeneration model is that it can generalize to videos that do not have ASR input, as shown in the first\nexample of Figure 5. Compared with the visual-only variant, the multi-modal model can also benefit\nfrom speech cues, as seen in the second example in Figure 5. Moreover, we observe that the dense\nvideo captioning model pretrained on VidChapters-7M is more accurate and hallucinates less than\nthe variant not pretrained on VidChapters-7M, see Figure 6.\nC\nAdditional data analysis of VidChapters-7M\nWe here complement the analysis of the data in VidChapters-7M provided in Section 3.3. In Figure 7,\nwe show a histogram of the most common chapter titles and word clouds1 of the chapters titles and\nASR content in VidChapters-7M. A few generic chapter titles that outline the structure of the video\n(e.g., Intro, Introduction, Outro, Conclusion and Start) appear more than 10K times. Besides, we\nnotice that many videos include chapters about Unboxing, Review, or Tips. This is consistent with\nthe fact that there are many vlogs and \u2019Howto\u2019 videos in VidChapters-7M. We also observe that the\nmost common words in the ASR largely differ from the most common words in the chapter titles,\nwhich further shows the difference between these two types of data. To further measure the text-video\nalignment in the VidChapters-7M dataset, we compute the CLIP cosine similarity between chapter\ntitles and their corresponding video frames and plot the resulting distribution in Figure 8. The average\nsimilarity score is 54.6%, and less than 1% of the chapters have a visual-text similarity score below\n30%. These statistics demonstrate a good video-text alignment in the VidChapters-7M dataset.\nD\nAdditional implementation details\nIn this Section, we present implementation details that complement the information provided in\nSection 4. We discuss implementation details of our tagging protocol for ethical considerations in\n1To generate the word clouds, we used https://github.com/amueller/word_cloud.\n18\n1. Intro\n2. Unboxing\n3. Product Tour\nInstallation \nguide\nHow does this solve your \nNBN issues\n\u2026\n2:13\n0:00\n3:32\n5:57\n14:06\nNutella crepes\nMini meatball pasta\nCaramel Bread \npopcorns\nChicken spread \nsandwich\nMini chicken burgers\n\u2026\n1:57\n0:17\n5:13\n7:00\n11:23\n0:34\n0:00\n3:04\n5:56\n9:21\nExamining \na List\nDrinking a Glass of Tea\nOnboard Camera/Tripod \nSounds\nCrinkly Plastic Bag\nButton Pressing\n\u2026\n1:46\n1:33\n2:54\n4:38\n6:40\nHow work has changed \nduring quarantine\nBenefits of recording \nyour training\nHip mobility\nTurning being in quarantine \nto a positive thing\nTraining fundamentals \nand body weight training \u2026\n\u2026 yeah yeah oh it's \njust easier for them to \nlike do it on their own \ntime um and then if \nthey need me they just \nsend me videos of it \u2026\nFor me, working with \nthe tennis players, \ngetting to record their \nstuff has been one of \nthe biggest \nimprovements to my \ncoaching. \u2026\nobviously I'm not as \nactive as I was before \n\u2026\nI find is that my hips \nhave been getting pretty \ntight\u2026\n\u2026 How can I turn that \nfrom like a negative of \nmissing my sport and \nnot being able to train \nin the gym and then flip \nthat around into a \npositive situation \u2026\nI've gone back to the \nbasics. Like bare bones, \nbasics of let's get my \nbody to move with just \nhis body\u2026\nintro\nathleisure\nleggings/sports bra\njackets\nfitness gear\n1:55\n0:00\n6:19\n11:23\n13:34\nFigure 4: Additional examples of videos with user-annotated chapters in VidChapters-7M:\nChapters depict visual events (e.g., the mini chicken burgers that appear in the second video),\nconversations (see the last video), or events in the raw audio (e.g., the sound of the crinkly plastic bag\nin the penultimate video) in various scenarios.\n19\nInput\nFrames\nVid2Seq \n(HTM\n+VC)\nIntroduction.\nGame overview.\nFinal thoughts.\nInput \nSpeech\n\u00d8\n\u00d8\n\u00d8\n\u00d8\n\u00d8\n\u00d8\n\u00d8\n\u00d8\nGround\n-Truth\nIntroduction\nGame overview\nFinal thoughts\nVid2Seq \n(HTM\n+VC, no \nvision)\nWhat\u2019s in \nthe box.\nWhat\u2019s in the \nbox.\nWhat\u2019s in \nthe box.\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\nInput\nFrames\nVid2Seq \n(HTM\n+VC)\nInput \nSpeech\nGround\n-Truth\n1. Zoom Pegasus Turbo 2\n2. Nike Men's \nRunning Shoes\n3. ON Women's Cloudflyer Running Shoes\nVid2Seq \n(HTM\n+VC, no \nspeech)\n4. Nike Downshifter \nMen's 7 Running \nShoe\n5. Nike Men's Trail \nRunning Shoes\n1. zoom pegasus turbo 2.\n2. nike men\u2019s running \nshoes.\n3. on women's cloudflyer running shoes.\n4. nike downshifter \nmen's 7 running \nshoe.\n5. nike men's trail \nrunning shoes.\n1. nike nexus running shoe.\n2. nike nexus running \nshoe.\n3. nike nexus running shoe.\n4. nike nexus \nrunning shoe.\n5. nike nexus running \nshoe.\nIf you are looking for \nthe best Nike running \nshoes, here is a \ncollection you have got \nto see.\nNumber 1. Most \nPopular. Zoom \nPegasus Turbo 2.  A \nsouped-up, speed-\noriented version of the \nPegasus, the Peg \nTurbo keeps the \nwinning combo of \nZoomex and React \nfoams found in the \nfirst version.\nUnfortunately, the new \nthin mesh upper has \nissues. Its minimal heel \nsupport means you \nhave to cinch the laces \ndown for a secure fit, \nbut the tongue isn't \nthick or long enough to \nprevent the laces from \ncausing irritation.\nNumber 2. Nike Men's \nRunning Shoes.  The \nnew trend in stability \nshoes is less \ninterference, and the \nInfinity Run follows \nthat principle by \nproviding comfort, \nsupport, and a smooth \nride without messing \nup your natural \nmovement. [\u2026]\nNumber 3. On \nWomen's CloudFlyer \nRunning Shoes. \nProvide your foot with \nthe cushion it deserves \nwith the On \nCloudFlyer. Utilizing \nplush clouds built from \nzero-gravity foam and \na wider CloudTek \nplatform, this daily \ntrainer provides \nsupreme cushioning in \na more stable package. \nIn order to reduce over \npronation, the shoe \nfeatures firmer medial \nelements that redirect \nforce to the lateral side \nof the runner's foot. \nPaired with an even \nstiffer speed board, the \nshoe promotes a \nquicker heel-to-toe \ntransfer that helps get \nthe runner through \ntheir pronated phase.\nNumber 5. Nike Men's \nTrail Running Shoes.  \nMade of a breathable \nmesh upper and a \nsturdy EVA sole, these \nQuest running shoes \nfrom Nike should \npretty much be a staple \nin every man's shoe \ncloset. Fly-wire cables \noffer your feet a secure \nfit, while the soft yet \nresponsive foam is \nsupportive [\u2026]\nNumber 4. Nike \nDownshifter Men's 7 \nRunning Shoe. The \nDownshifter 7 \nRunning Shoes from \nNike are designed to \nbe lightweight, sturdy \nand durable, all the \nwhile providing you \nwith optimum \nperformance, making \nthem a worthy \ninvestment. [\u2026]\nFigure 5: Examples of video chapter generation using the Vid2Seq model with different input\nmodalities compared with ground-truth on the test set of VidChapters-7M. The first example\nshows that the Vid2Seq variant with both speech and visual modalities \"Vid2Seq (HTM+VC)\" can\npredict the structure of the input video without the ASR input, unlike the Vid2Seq speech-only variant\n\"Vid2Seq (HTM+VC, no vision)\". The second example shows that the Vid2Seq variant with both\nspeech and visual modalities \"Vid2Seq (HTM +VC)\" can effectively leverage speech cues to detect\nthe names of the depicted and discussed shoes, unlike the Vid2Seq visual-only variant \"Vid2Seq\n(HTM+VC, no speech)\".\nSection D.1, models used for video chapter generation and dense video captioning in Section D.2,\nmodels used for video chapter generation with ground-truth boundaries in Section D.3, and models\nused for video chapter grounding in Section D.4.\nD.1\nTagging for ethical considerations\nIn Section 3.3, we explained how we tag videos for ethical considerations. We here give additional\ndetails about this procedure. For the NSFW visual content detector [78], we compute the NSFW\nscore at every frame (at 1 FPS) and tag videos with an average score above 0.5. For the toxic content\ndetection model [31], we compute the toxicity score at every chapter title / ASR sentence and tag\nvideos where the chapter titles / ASR have an average toxicity score above 0.5.\nD.2\nVideo chapter generation and dense video captioning\nLLaMA [93]. We use the following prompt: Summarize the following speech transcript\nin a chapter title.\nTranscript:\n<ASR> Chapter title: where the ASR is the con-\ncatenation of all speech sentences spoken during a given video segment.\nBLIP-2 [51]. We use the following prompt: Summarize the image in a chapter title.\nChapter title:, and use the middle frame of the predicted video segment.\n20\nInput\nFrames\nVid2Seq \n(HTM)\nVid2Seq \n(HTM\n+VC)\nHeat a pan over \nmedium heat.\nHeat a pan with \nolive oil.\nAdd 2 tbsp of olive \noil to the pan.\nAdd onions to the pan \nand saute.\nAdd olive oil to the \npan.\nAdd onions to the \npan.\nAdd red pepper \nsweet potatoes \nand water to the \npan.\nAdd red pepper to \nthe pan.\nAdd red pepper and water to the pan.\nAdd sweet potatoes \nand water to the \npan.\nCover the pan \nwith a lid.\nCook the sweet \npotatoes in the \npan.\nInput \nSpeech\nOkay, so let's get \nstarted. So I'm going to \nheat our pan over \nmedium heat. Once \nagain, keep the \ntemperature low \nbecause we are using \nolive oil. You don't want \nto cook olive oil over \ntoo high of heat.\nWe'll add in two \nteaspoons of olive oil \nto coat the pan. We'll \nlet that heat up, then \nwe're going to add the \nother ingredients. So \ncome on over.\nOkay, now that the pan \nhas been heated over \nmedium heat, let's add \nin the onions and let\u2019s \nsaute these for about \ntwo minutes. Get them \nsoft, then we'll add in \nthe other ingredients.\nNow that the onions \nare nice and sauteed \nand smelling delicious, \nlet's add in our red \npepper.\nAnd then we'll add in \nour sweet potatoes, \nand then about a \ntablespoon of water.\nSo what we're hoping \nfor in that 15 minutes \nis that these sweet \npotatoes will become \nsoft and ready to eat.\nWhatever you want to \ncall it, it smells \ndelicious, it looks \npretty cool. Add this as \na side dish to any \nprotein source, so a \nsteak, chicken, or have \nit in the morning with \nyour eggs and you're \ngood to go.\nInput\nFrames\nVid2Seq \n(HTM)\nVid2Seq \n(HTM\n+VC)\nPeel rinse and cut \nthe potatoes.\nBoil the potatoes in \nwater.\nBoil the potatoes in \nwater.\nPeel rinse and cut the potatoes.\nBoil the potatoes in \nwater and salt the \nwater.\nDrain the potatoes and \ndry them out.\nDrain the \npotatoes and \ndry them out \nin a pot.\nAdd milk and mash the potatoes.\nAdd cream and milk to \nthe potatoes.\nAdd fresh herbs \nparsley and butter \nto the mashed \npotatoes.\nAdd chives \nparsley and \nbutter to the \npotatoes.\nInput \nSpeech\nWhen you're making \nmashed potatoes, if \nyou're using the wrong \npotato and you're \nthrowing them in a food \nprocessor, you're doing \nit all wrong. The first \nmistake people tend to \nmake is they pick the \nwrong potato. [\u2026]\nNext you want to peel, \nrinse, and most \nimportant, cut your \npotatoes into nice, \neven chunks. That way \nthey'll all cook at the \nsame rate, and you \nwon't get any weird \nlittle nasty hard bits in \nyour potato.\nNext comes a crucial \nstep. You want to cook \nthese potatoes until \nthey are falling apart. \nNext comes a crucial \nstep. You want to cook \nthese potatoes until \nthey are falling apart \nmore than if you just \nput them in boiling \nwater.\nYou want to salt the \nwater because the salt \nstarts to disintegrate \nthe potatoes too. But \nthe most important \nthing is that you really \ncook those potatoes \nabout 25 minutes.\nAfter you bring them \nto a boil, let them \nsimmer long and slow \nuntil the edges start to \nfall off, and when you \nstick a knife in the \nmiddle, it just all falls \napart. Drain the \npotatoes, and then put \nthem right back into \nthat hot pot and start to \ndry them\nout over very low heat \nuntil they really start \nfalling apart, turning \nwhite on the edges and \npractically turning into \nmashed potatoes right \nthen and there in the \npot. To finish the job, \nyou can use a hand \nmasher or you can use \na ricer for super \nsmooth, silky potatoes.\nDon't worry, it can \nlook like soup at first, \nbut if you keep \nstirring, you'll see \nthose potatoes just \ndrink up all that cream. \nThe final step for \nmaking great mashed \npotatoes is to add a \nfew fresh herbs, chives \nare nice [\u2026]\nSo now it's time to \nfinally add your cream \nor milk. You want to \nmake sure it's warm so \nit doesn't cool down \nyour potatoes, and the \nmost important thing is \nyou want to make sure \nyou use lots. [\u2026]\nGround\n-Truth\nAdd some chopped \nwhite onions in a pan \nunder medium heat.\nAdd a spoon of \nwater.\nCook with lid \non.\nAdd in red pepper and \nsweet potatoes.\nSeason it with salt \nand black pepper.\nGround\n-Truth\nPeel and cut \npotatoes \ninto chunks.\nDrain and \ndry the \npotatoes.\nMash the \npotatoes \nwell with \na hand \nmasher.\nPut in cold \nwater and cook \nto a boil and \nsalt the water.\nAdd milk and stir the \npotatoes.\nSeason the potatoes \nwith some chopped \nparsley leaves.\nFigure 6: Examples of dense event captioning of the Vid2Seq model pretrained on VidChapters-\n7M (vs. not pre-trained), compared with ground-truth, on the validation set of YouCook2.\nWe find that the model pretrained on VidChapters-7M \"Vid2Seq (HTM+VC)\" is more accurate and\nless prone to hallucination. For instance, in the first example (top), the non-VC-pretrained model\n\"Vid2Seq (HTM)\" predicts \"Add red pepper sweet potatoes and water to the pan.\" before the sweet\npotatoes are actually thrown into the pan. In the second example (bottom), the non-VC-pretrained\nmodel \"Vid2Seq (HTM)\" predicts the event \"Boil the potatoes in water.\" twice and predicts the event\n\"Add chives parsley and butter to the potatoes.\" before it actually happens. The VC-pretrained model\n\"Vid2Seq (HTM+VC)\" produces more accurate predictions.\nPDVC [101].\nWe use PDVC\u2019s official codebase.\nPDVC includes a caption decoder that re-\nlies on dataset-specific word vocabularies. To adapt PDVC to VidChapters-7M/YouCook2/ViTT,\nwe construct a vocabulary made with all words that appear at least 50/2/3 times in the dataset\n(33,598/3,815/1,607 words). For transfer learning from VidChapters-7M to YouCook2/ViTT, we ini-\ntialize the downstream dataset-specific word embedding layer with the weights of the corresponding\nword embedding in the pretrained model. We subsample or pad the sequence of frames to 100 frames.\nFor all datasets, we use 100 queries and train with a constant learning rate of 5e\u22125, weight decay\n1e\u22124 and batch size 1 on an NVIDIA V100 32GB (as the official codebase is not compatible with\nhigher batch sizes or multi-gpu training) . We train on VidChapters-7M/YouCook2/ViTT for 5/30/30\nepochs. The training on VidChapters-7M lasts about a week.\nVid2Seq [114]. We reimplement Vid2Seq (originally released in Jax) in PyTorch. For initialization,\nwe use the T5-Base language model pretrained on the C4 text corpus [74]. Vid2Seq is originally\npretrained on YT-Temporal-1B [118] using a generative and denoising objective in the speech\nsequence. Due to computational limitations, we instead pretrain Vid2Seq on the smaller HowTo100M\ndataset [64] with the same objectives. Then we train Vid2Seq on VidChapters-7M with the next\n21\nIntro\nIntroduction\nOutro\nConclusion\nFinal Thoughts\nStart\nintro\nUnboxing\nOverview\nFinal thoughts\nSummary\nRest\nWelcome\nINTRO\nThings You'll Need\nGame overview\nTips & Warnings\nClosing\nRecap\nPerformance\nQ&A\nPrice\nReview\nDesign\noutro\nInterior\nWrap Up\nFeatures\nOpening\nBloopers\nRound 2\nVerdict\nMaterials\nRound 1\nWrap up\nstart\nEnding\nRound 3\nEnd\nDemonstration\nIntro & Summary\nSpecs\nSetup\n1\n2\n3\nCredits\n4\nRemove old filter\nFilter location\nChapter title\n103\n104\n105\nCount (log scale)\n(a) Most common chapter titles\n0\n500\n1000\n1500\n2000\n0\n200\n400\n600\n(b) Word clouds of chapter titles.\n0\n500\n1000\n1500\n2000\n0\n200\n400\n600\n(c) Word clouds of ASR.\nFigure 7: Additional statistics of the VidChapters-7M dataset.\n22\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\nAverage visual-text similarity\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nCount\n1e6\nFigure 8: Average visual-text similarity between chapter titles and the corresponding video\nframes as measured by CLIP cosine similarity (rescaled between 0 and 100) in VidChapters-7M.\ntoken prediction objective in the chapter sequence and the denoising objective in the speech sequence.\nFinetuning on YouCook2/ViTT is done with the next token prediction objective in the dense video\ncaptioning sequence and the denoising objective in the speech sequence. We subsample or zero-pad\nthe sequence of frames to 100 frames. The text encoder and decoder sequence are truncated or padded\nto 1000 and 256 tokens, respectively. For all datasets, we use a learning rate of 3e\u22124 warmed up\nlinearly (from 0) for the first 10% of iterations and following a cosine decay (down to 0) for the\nremaining 90%. We train for 6/10/40/20 epochs on HowTo100M/VidChapters-7M/YouCook2/ViTT.\nWe use a batch size of 64 videos split on 8 NVIDIA A100 80GB for HowTo100M/VidChapters-7M,\nand 16 videos split on 8 NVIDIA V100 32GB for YouCook2/ViTT. The training on HowTo100M or\nVidChapters-7M takes about 2 days.\nD.3\nVideo chapter generation with ground-truth boundaries\nLLaMA [93] and BLIP-2 [51]. See Section D.2.\nVid2Seq [114]. To adapt the model pretrained on HowTo100M (see Section D.2) to video chapter\ngeneration with ground-truth boundaries, we remove the model weights corresponding to the time\ntokens (in the token embedding layers and the token prediction layer). We train for 20 epochs on\nVidChapters-7M using the next token prediction objective in the sequence of tokens corresponding to\na single chapter title. We construct training batches by sampling a chapter title with its associated\nvideo clip at each iteration (i.e., an epoch corresponds to seeing one chapter title for all videos). The\ntext encoder and decoder sequence are truncated or padded to 256 and 32 tokens, respectively. We\nuse a learning rate of 3e\u22124 warmed up linearly (from 0) for the first 10% of iterations and following\na cosine decay (down to 0) for the remaining 90%. We use a batch size of 512 videos split on 8\nNVIDIA A100 80GB for VidChapters-7M. The training takes about a day.\nD.4\nVideo chapter grounding\nMoment-DETR [45]. We use Moment-DETR\u2019s official codebase. We train with the AdamW\noptimizer [60], a constant learning rate of 3e\u22124, and a batch size of 256 videos split on 8 NVIDIA\nA100 80GB. We use a FPS of 1/3 and subsample or zero-pad the sequence of frames to 1200 frames.\nWe use a maximum number of text query tokens of 77. We train for 50 epochs on VidChapters-7M,\nwhere an epoch corresponds to seeing one chapter title for all videos, which takes about 2 days.\nE\nVideo chapter generation results split by language\nWe report video chapter generation results on the VidChapters-7M dataset split by language for both\nEnglish and German in Tables 9 and 10, respectively. We find that training on VidChapters-7M\nis beneficial for both languages. Interestingly, pretraining on HowTo100M (which is a dataset in\nEnglish) improves results on English as well as German. We also observe that the quantitative results\nin German are lower than in English. Finally, we report results of the Vid2Seq model with the\nmulti-lingual language model mT5 [109] pretrained on the multi-lingual dataset mC4 [109]. We\n23\nMethod\nModalities\nPretraining Data\nFinetuned\nS\nB1\nB2 B3 B4\nC\nM\nRL\nText tiling [32] + Random\nSpeech\n\u2205\n\u2717\n0.5\n0.8\n0.2 0.1 0.0\n0.9\n0.8\n0.7\nText tiling [32] + LLaMA [93]\nSpeech\nText mixture\n\u2717\n0.3\n0.5\n0.2 0.1 0.0\n0.5\n0.4\n0.4\nShot detect [92] + BLIP-2 [51]\nVisual\n129M image-texts\n\u2717\n1.3\n1.5\n0.7 0.4 0.2\n4.7\n1.4\n1.6\nPDVC [101]\nVisual\n\u2205\n\u2713\n6.6\n9.0\n3.8 1.5 1.0 36.0 9.1 11.0\nVid2Seq [114]\nSpeech+Visual\nC4\n\u2713\n10.8 10.3 7.6 4.9 3.4 54.8 9.1 11.9\nVid2Seq [114] w/ mT5\nSpeech+Visual\nmC4\n\u2713\n10.4\n9.9\n7.2 4.7 3.3 52.0 8.7 11.3\nVid2Seq [114]\nSpeech+Visual C4 + HowTo100M\n\u2713\n11.5 11.1 8.1 5.1 3.6 58.8 9.7 12.8\nTable 9: Video chapter generation (global metrics) on the VidChapters-7M test set restricted to\nvideos with English chapter titles and ASR. Here, finetuned refers to finetuning on the VidChapters-\n7M train set, and speech refers to transcribed speech (ASR).\nMethod\nModalities\nPretraining Data\nFinetuned\nS\nB1\nB2 B3 B4\nC\nM\nRL\nText tiling [32] + Random\nSpeech\n\u2205\n\u2717\n0.6\n1.7\n1.3 1.3 1.1 12.8\n1.5\n1.6\nText tiling [32] + LLaMA [93]\nSpeech\nText mixture\n\u2717\n0.1\n0.3\n0.2 0.0 0.0\n0.0\n0.2\n0.2\nShot detect [92] + BLIP-2 [51]\nVisual\n129M image-texts\n\u2717\n0.6\n0.4\n0.2 0.0 0.0\n1.3\n0.6\n0.5\nPDVC [101]\nVisual\n\u2205\n\u2713\n5.4\n11.6 0.0 0.0 0.0 29.4 12.4 14.9\nVid2Seq [114]\nSpeech+Visual\nC4\n\u2713\n9.1\n8.4\n5.2 1.0 0.9 34.1\n6.1\n10.1\nVid2Seq [114] w/ mT5\nSpeech+Visual\nmC4\n\u2713\n8.8\n8.1\n5.9 1.7 1.8 38.4\n6.1\n10.1\nVid2Seq [114]\nSpeech+Visual C4 + HowTo100M\n\u2713\n10.9\n9.6\n5.4 1.7 1.7 43.2\n8.1\n8.1\nTable 10: Video chapter generation (global metrics) on the VidChapters-7M test set restricted to\nvideos with German chapter titles and ASR. Here, finetuned refers to finetuning on the VidChapters-\n7M train set, and speech refers to transcribed speech (ASR).\nfind that this variant performs a bit worse on English but slightly better on German compared to the\nVid2Seq variant based on T5 pretrained on the C4 corpus.\nF\nDatasheet for VidChapters-7M\nDatasheets for datasets introduced by Gebru et al. [28] serve as a medium of communication between\nthe creators and users of a dataset. They effectively consolidate the motivation, creation process,\ncomposition, and intended uses of a dataset as a series of questions and answers. In this Section, we\nprovide a datasheet for the VidChapters-7M dataset.\nMotivation\nQ1. For what purpose was the dataset created? Was there a specific task in mind? Was there a\nspecific gap that needed to be filled? Please provide a description.\n\u2013\nThe VidChapters-7M dataset was created to explore the task of video chapter generation,\nwhich enables users to quickly navigate to the information of their interest.\nQ2. Who created this dataset (e.g., which team, research group) and on behalf of which entity\n(e.g., company, institution, organization)?\n\u2013\nFive researchers have created VidChapters-7M: Antoine Yang (Inria and DI ENS), Arsha\nNagrani (VGG, University of Oxford), Ivan Laptev (Inria and DI ENS), Josef Sivic (CIIRC\nCTU) and Cordelia Schmid (Inria and DI ENS).\nQ3. Who funded the creation of the dataset? If there is an associated grant, please provide the\nname of the grantor and the grant name and number.\n\u2013\nWe collected VidChapters-7M without any monetary costs, since no part of our dataset requires\nannotations from crowd workers or contractors. This research work was funded by Antoine\nYang\u2019s Google PhD fellowship, the French government under management of Agence Nationale\nde la Recherche as part of the \"Investissements d\u2019avenir\" program, reference ANR-19-P3IA-\n0001 (PRAIRIE 3IA Institute), the Louis Vuitton ENS Chair on Artificial Intelligence, the\nEuropean Regional Development Fund under project IMPACT (reg. no. CZ.02.1.01/0.0/0.0/15\n003/0000468). However, note that this article solely reflects the opinions and conclusions of its\nauthors and not of its funders.\n24\nQ4. Any other comments?\n\u2013\nNo.\nComposition\nQ5. What do the instances that comprise the dataset represent (e.g., documents, photos, people,\ncountries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and\ninteractions between them; nodes and edges)? Please provide a description.\n\u2013\nEach instance in VidChapters-7M represents a YouTube video.\nQ6. How many instances are there in total (of each type, if appropriate)?\n\u2013\nThere are 817K instances in VidChapters-7M.\nQ7. Does the dataset contain all possible instances or is it a sample (not necessarily random)\nof instances from a larger set?\nIf the dataset is a sample, then what is the larger set? Is\nthe sample representative of the larger set (e.g., geographic coverage)? If so, please describe\nhow this representativeness was validated/verified. If it is not representative of the larger set,\nplease describe why not (e.g., to cover a more diverse range of instances, because instances\nwere withheld or unavailable).\n\u2013\nVidChapters-7M is a small sample drawn from all the data uploaded to YouTube. Millions of\nvideos are uploaded on YouTube every day. We started from a subset of 92 million YouTube\nvideo candidates, which consists of videos recommended in videos from the YT-Temporal-\n180M dataset [117]. We selected the videos from this subset (817K instances) that contain\nuser-annotated chapters. Hence, VidChapters-7M data does not fully represent YouTube.\nQ8. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or\nfeatures? In either case, please provide a description.\n\u2013\nEach instance in VidChapters-7M consists of four metadata fields:\n\u2022 \"video_id\": Unique alphanumeric ID of the video (assigned by YouTube).\n\u2022 \"url\":\nStatic\nURL\nfor\ndownloading\nthe\nvideo,\ne.g.,\nhttps://www.youtube.com/watch?v=<video_id>.\n\u2022 \"asr\": ASR transcripts aligned over time.\n\u2022 \"chapters\": Chapters aligned over time.\nQ9. Is there a label or target associated with each instance? If so, please provide a description.\n\u2013\nWe use the chapters as labels in this work, though it might be also possible to use auxiliary\ninformation (like video titles or tags).\nQ10. Is any information missing from individual instances? If so, please provide a description,\nexplaining why this information is missing (e.g., because it was unavailable). This does not\ninclude intentionally removed information, but might include, e.g., redacted text.\n\u2013\nNo and yes. No, because all the metadata fields for every instance are filled with valid values.\nYes, because the \"url\" for some instances may not retrieve the underlying video. This may\nhappen if the YouTube user (author) removes the video from YouTube. Such deletions reduce\nour dataset size over time, however, video deletions are rare.\nQ11. Are relationships between individual instances made explicit (e.g., users\u2019 movie ratings,\nsocial network links)? If so, please describe how these relationships are made explicit.\n\u2013\nRelationships between individual instances (e.g., videos made by the same creator) are not\nmade explicit in our work, though this is a possibility for future work.\nQ12. Are there recommended data splits (e.g., training, development/validation, testing)? If so,\nplease provide a description of these splits, explaining the rationale behind them.\n\u2013\nWe randomly split our data into training, validation, and testing sets. The training, validation,\nand testing sets are meant for training, development, and evaluation, respectively.\nQ13. Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide\na description.\n\u2013\nVidChapters-7M is inherently noisy since YouTubers are free to write the chapters that they\nwant.\n25\nQ14. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)? If it links to or relies on external resources,\n(a) Are there guarantees that they will exist, and remain constant, over time?\n(b) Are there official archival versions of the complete dataset (i.e., including the external\nresources as they existed at the time the dataset was created)?\n(c) Are there any restrictions (e.g., licenses, fees) associated with any of the external resources\nthat might apply to a future user? Please provide descriptions of all external resources and\nany restrictions associated with them, as well as links or other access points, as appropriate.\n\u2013\nWe do not distribute videos of our dataset to respect YouTube user privacy and to limit our\nstorage budget. Instead, we provide video URLs (\"url\", Q8) that point to videos hosted on\nYouTube servers. In response to sub-questions:\n(a) These video servers ensure stable access unless the YouTube user deletes their video.\n(b) Yes, YouTube archives all the metadata of submitted videos. For videos, YouTube only\narchives the URL and not the media content, giving full control of accessibility to the users.\n(c) All video URLs are freely accessible. It is unlikely for video servers to restrict access in the\nfuture, given their free accessibility over the past decade.\nQ15. Does the dataset contain data that might be considered confidential (e.g., data that is\nprotected by legal privilege or by doctor-patient confidentiality, data that includes the\ncontent of individuals non-public communications)? If so, please provide a description.\n\u2013\nNo, the videos included in VidChapters-7M do not cover topics that may be considered\nconfidential. All videos were publicly shared on YouTube prior to inclusion in VidChapters-7M.\nQ16. Does the dataset contain data that, if viewed directly, might be offensive, insulting,\nthreatening, or might otherwise cause anxiety? If so, please describe why.\n\u2013\nThe scale of VidChapters-7M means that we are unable to manually verify the contents of all\nvideos and chapters. However, YouTube removes videos that contain offensive content or do not\nfollow their community guidelines. Furthermore, we employed additional mitigation techniques\non VidChapters-7M:\n(a) We tagged all instances whose video frames were predicted as NSFW by an off-the-shelf\ndetector [78].\n(b) We tagged all instances whose chapter titles or speech transcripts were predicted as toxic\nby a language model [31].\nQ17. Does the dataset relate to people? If not, you may skip remaining questions in this section.\n\u2013\nThe dataset pertains to people in that people upload videos to YouTube and write descriptions\nthat include chapter annotations. Furthermore, most videos in VidChapters-7M have people\nspeaking and/or appearing.\nQ18. Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe\nhow these subpopulations are identified and provide a description of their respective distributions\nwithin the dataset.\n\u2013\nVidChapters-7M does not explicitly identify any subpopulations. Since most videos contain\npeople and chapters are free-form natural language written by YouTube users, it is possible that\nsome chapters may identify people appearing in individual videos as part of a subpopulation.\nQ19. Is it possible to identify one or more natural persons, either directly or indirectly (i.e., in\ncombination with other data) from the dataset? If so, please describe how.\n\u2013\nYes, our data includes celebrities, or other YouTube-famous people. All of the videos that\nwe use are of publicly available data, following the Terms of Service (https://www.youtube.\ncom/static?template=terms) that users agreed to when uploading to YouTube.\nQ20. Does the dataset contain data that might be considered sensitive in any way (e.g., data\nthat reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions\nor union memberships, or locations; financial or health data; biometric or genetic data;\nforms of government identification, such as social security numbers; criminal history)? If\nso, please provide a description.\n\u2013\nThis is highly unlikely, as YouTube removes videos that contain offensive content or do not\nfollow their community guidelines.\n26\nQ21. Any other comments?\n\u2013\nNo.\nCollection Process\nQ22. How was the data associated with each instance acquired? Was the data directly observable\n(e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly\ninferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or\nlanguage)? If data was reported by subjects or indirectly inferred/derived from other data, was\nthe data validated/verified? If so, please describe how.\n\u2013\nSee Q7 for an explanation of how the candidate video IDs were chosen. These video IDs\nwere provided by the YT-Temporal-180M dataset providers [117] and collected via the YouTube\nAPI. The \"video_id\" and \"URL\" are directly observable. The \"chapters\" are extracted from\nthe YouTube description which is directly observable. The \"asr\" is obtained by applying the\nWhisper-Large-V2 model [73] to the directly observable audio from the video. We found this\nmodel to provide higher-quality transcriptions compared to the YouTube API on several data\nsamples from VidChapters-7M.\nQ23. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus\nor sensor, manual human curation, software program, software API)? How were these\nmechanisms or procedures validated?\n\u2013\nWe collected all data using compute resources provided by IDRIS, under the allocation\n2023-A0131011670 made by GENCI. The code for querying APIs, extracting ASR, and filtering\ndata are implemented in Python. The code was validated by checking several data samples from\nVidChapters-7M.\nQ24. If the dataset is a sample from a larger set, what was the sampling strategy?\n\u2013\nSee Q7.\nQ25. Who was involved in data collection process (e.g., students, crowd-workers, contractors)\nand how were they compensated (e.g., how much were crowd-workers paid)?\n\u2013\nOur data collection pipeline is fully automatic and does not require any human annotators.\nYouTube users have uploaded videos whose metadata is a part of VidChapters-7M \u2013 we did not\ndirectly interact with these users.\nQ26. Over what timeframe was the data collected? Does this timeframe match the creation\ntimeframe of the data associated with the instances (e.g., recent crawl of old news articles)?\nIf not, please provide a description of the timeframe.\n\u2013\nVidChapters-7M contains videos that were uploaded to YouTube between 2005\u20132022. We\ncollected all data in early 2023, which we used to conduct experiments for our NeurIPS 2023\nsubmission.\nQ27. Were any ethical review processes conducted (e.g., by an institutional review board)? If\nso, please provide a description of these review processes, including the outcomes, as well as a\nlink or other access point to any supporting documentation.\n\u2013\nWe did not conduct a formal ethical review process via institutional review boards. However,\nas described in Section 3.3 and Q16 we employed several filtering mechanisms to tag instances\nthat could be problematic.\nQ28. Does the dataset relate to people? If not, you may skip remaining questions in this section.\n\u2013\nYes, see Q17.\nQ29. Did you collect the data from the individuals in question directly, or obtain it via third\nparties or other sources (e.g., websites)?\n\u2013\nWe collected data submitted by YouTube users indirectly through the YouTube API. However,\nusers agree with YouTube\u2019s Terms of Service regarding the redistribution of their data by\nYouTube.\n27\nQ30. Were the individuals in question notified about the data collection? If so, please describe\n(or show with screenshots or other information) how notice was provided, and provide a link or\nother access point to, or otherwise reproduce, the exact language of the notification itself.\n\u2013\nNo. YouTube users are not required to share their personal contact information (email, phone\nnumbers, etc.). Hence, the only way to notify the authors of VidChapters-7M videos is by\ncommenting on their videos. This is practically difficult to do manually and will be classified as\nspam and blocked by YouTube if attempted to programmatically write a templated comment to\nmillions of users.\nQ31. Did the individuals in question consent to the collection and use of their data? If so,\nplease describe (or show with screenshots or other information) how consent was requested\nand provided, and provide a link or other access point to, or otherwise reproduce, the exact\nlanguage to which the individuals consented.\n\u2013\nUsers did not explicitly consent to the use of their data in our dataset. However, by uploading\ntheir data on YouTube, they consent that it would appear on the YouTube plaform and will be\naccessible via the official YouTube API (which we use to collect VidChapters-7M).\nQ32. If consent was obtained, were the consenting individuals provided with a mechanism to\nrevoke their consent in the future or for certain uses? If so, please provide a description, as\nwell as a link or other access point to the mechanism (if appropriate).\n\u2013\nUsers have full control over the presence of their data in our dataset. If users wish to revoke\ntheir consent, they can delete the underlying YouTube video \u2013 it will be automatically removed\nfrom VidChapters-7M since we distributed videos as URLs. Moreover, we provide an opt-out\nrequest form on our dataset website for anybody to request removal of an individual instance if\nit is potentially harmful (e.g. NSFW, violates privacy, harmful stereotypes, etc.).\nQ33. Has an analysis of the potential impact of the dataset and its use on data subjects (e.g.,\na data protection impact analysis) been conducted? If so, please provide a description of\nthis analysis, including the outcomes, as well as a link or other access point to any supporting\ndocumentation.\n\u2013\nNo.\nQ34. Any other comments?\n\u2013\nNo.\nPreprocessing, Cleaning, and/or Labeling\nQ35. Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucket-\ning, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances,\nprocessing of missing values)? If so, please provide a description. If not, you may skip the\nremainder of the questions in this section.\n\u2013\nWe converted chapter timestamps in HH:MM:SS format to seconds. Refer to Section 3.1 for\nmore details. We also extracted speech transcripts and visual features (see Section 3.2). Finally,\nwe tagged some instances with a focus on ethical considerations, see Q16 for more details.\nQ36. Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to\nsupport unanticipated future uses)? If so, please provide a link or other access point to the\n\u201craw\u201d data.\n\u2013\nYes, the raw descriptions from which chapters are extracted are also released on the dataset\nwebsite [1].\nQ37. Is the software used to preprocess/clean/label the instances available? If so, please provide\na link or other access point.\n\u2013\nYes, the data preprocessing code is open-sourced and accessible from the dataset website [1].\nQ38. Any other comments?\n\u2013\nNo.\n28\nUses\nQ39. Has the dataset been used for any tasks already? If so, please provide a description.\n\u2013\nWe have used our dataset to train deep neural networks that perform video chapter generation,\nand that can be transferred to dense video captioning tasks (see Sections 4.1 and 4.4). We also\ntrained models for video chapter generation with ground-truth boundaries and video chapter\ngrounding (see Sections 4.2 and 4.3).\nQ40. Is there a repository that links to any or all papers or systems that use the dataset? If so,\nplease provide a link or other access point.\n\u2013\nWe do not maintain such a repository. However, citation trackers like Google Scholar and\nSemantic Scholar would list all future works that cite our dataset.\nQ41. What (other) tasks could the dataset be used for?\n\u2013\nWe anticipate that the dataset could be used for a variety of video-and-language tasks, such as\ntext-to-video retrieval.\nQ42. Is there anything about the composition of the dataset or the way it was collected and\npreprocessed/cleaned/labeled that might impact future uses? For example, is there anything\nthat a future user might need to know to avoid uses that could result in unfair treatment of\nindividuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms\n(e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future\nuser could do to mitigate these undesirable harms?\n\u2013\nThis is very difficult to anticipate. Future users of our dataset should be aware of YouTube\u2019s\nuser demographics which might subtly influence the types of videos, languages, and ideas that\nare present in the dataset. Also, note that our dataset is mainly composed of English videos,\nhence models trained on this dataset might perform worse on videos in other languages.\nQ43. Are there any tasks for which the dataset should not be used? If so, please provide a\ndescription.\n\u2013\nBroadly speaking, our dataset should only be used for non-commercial academic research. Our\ndataset should not be used for any tasks that involve identifying features related to people (facial\nrecognition, gender, age, ethnicity identification, etc.) or making decisions that impact people\n(mortgages, job applications, criminal sentences; or moderation decisions about user-uploaded\ndata that could result in bans from a website). Any commercial and for-profit uses of our dataset\nare restricted \u2013 it should not be used to train models that will be deployed in production systems\nas part of a product offered by businesses or government agencies.\nQ44. Any other comments?\n\u2013\nNo.\nDistribution\nQ45. Will the dataset be distributed to third parties outside of the entity (e.g., company,\ninstitution, organization) on behalf of which the dataset was created? If so, please provide a\ndescription.\n\u2013\nYes, our dataset is publicly available.\nQ46. How will the dataset will be distributed (e.g., tarball on website, API, GitHub) Does the\ndataset have a digital object identifier (DOI)?\n\u2013\nWe distribute our dataset as JSON/PICKLE files containing annotations. Users will have to\ndownload the videos by themselves by using our data collection code. All uses of VidChapters-\n7M should cite the paper as the reference.\nQ47. When will the dataset be distributed?\n\u2013\nThe dataset is publicly available as of September 2023.\nQ48. Will the dataset be distributed under a copyright or other intellectual property (IP)\nlicense, and/or under applicable terms of use (ToU)? If so, please describe this license and/or\nToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing\nterms or ToU, as well as any fees associated with these restrictions.\n29\n\u2013\nUses of our dataset are subject to YouTube API terms (https://www.youtube.com/\nstatic?template=terms). The data and code are released with an MIT license.\nQ49. Have any third parties imposed IP-based or other restrictions on the data associated with\nthe instances? If so, please describe these restrictions, and provide a link or other access point\nto, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with\nthese restrictions.\n\u2013\nThe videos corresponding to our instances are legally owned by YouTube users. Our dataset\nusers can download them from the URLs we provide in annotation files, but redistributing videos\nfor commercial use is prohibited.\nQ50. Do any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances? If so, please describe these restrictions, and provide a link or other access point to,\nor otherwise reproduce, any supporting documentation.\n\u2013\nNo.\nQ51. Any other comments?\n\u2013\nNo.\nMaintenance\nQ52. Who will be supporting/hosting/maintaining the dataset?\n\u2013\nThe authors will maintain the dataset. The dataset is hosted using Inria servers and Google\nDrive service. All the information about the dataset, including links to the paper, code, and\nfuture announcements will be accessible at the dataset website [1].\nQ53. How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\n\u2013\nThe contact emails of authors are available on the dataset website [1].\nQ54. Is there an erratum? If so, please provide a link or other access point.\n\u2013\nThere is no erratum for our initial release. We will version all errata as future releases (Q55)\nand document them on the dataset website [1].\nQ55. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete\ninstances)? If so, please describe how often, by whom, and how updates will be communicated\nto users (e.g., mailing list, GitHub)?\n\u2013\nWe will update our dataset once every year and announce it on the dataset website [1]. These\nfuture versions would remove instances that were requested to be removed via the opt-out form\n(Q32).\nQ56. If the dataset relates to people, are there applicable limits on the retention of the data\nassociated with the instances (e.g., were individuals in question told that their data would\nbe retained for a fixed period of time and then deleted)? If so, please describe these limits\nand explain how they will be enforced.\n\u2013\nRather than directly distributing videos, we distribute URLs that point to the original videos\nuploaded by YouTube users. This means that users retain full control of their data \u2013 any post\ndeleted from YouTube will be automatically removed from VidChapters-7M (see also Q10, Q14,\nQ31).\nQ57. Will older versions of the dataset continue to be supported/hosted/maintained? If so,\nplease describe how. If not, please describe how its obsolescence will be communicated to users.\n\u2013\nA new version release of VidChapters-7M will automatically deprecate its previous version.\nWe will only support and maintain the latest version at all times. Deprecated versions will\nremain accessible on the dataset website for a few weeks, after which they will be removed. We\ndecided to deprecate old versions to ensure that any data that is requested to be removed (Q32)\nwill be no longer accessible in future versions.\n30\nQ58. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism\nfor them to do so? If so, please provide a description. Will these contributions be verified? If so,\nplease describe how. If not, why not? Is there a process for communicating/distributing these\ncontributions to other users? If so, please provide a description.\n\u2013\nAnyone can extend VidChapters-7M by using our data collection code (linked on our\nwebsite [1]). We are open to accepting extensions via personal communication with contributors.\nOtherwise, our code and data licenses allow others to create independent derivative works (with\nproper attribution) as long as they are used for non-commercial academic research.\n31\n"
  }
]