[
  {
    "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
    "link": "https://arxiv.org/pdf/2310.17631.pdf",
    "upvote": "31",
    "text": "JUDGELM : FINE-TUNED LARGE LANGUAGE MODELS\nARE SCALABLE JUDGES\nLianghui Zhu1,2 \u2217\nXinggang Wang2\nXinlong Wang1\n1 Beijing Academy of Artificial Intelligence\n2 School of EIC, Huazhong University of\nScience & Technology\nCode & Models: https://github.com/baaivision/JudgeLM\nABSTRACT\nEvaluating Large Language Models (LLMs) in open-ended scenarios is challeng-\ning because existing benchmarks and metrics can not measure them comprehen-\nsively. To address this problem, we propose to fine-tune LLMs as scalable judges\n(JudgeLM) to evaluate LLMs efficiently and effectively in open-ended bench-\nmarks. We first propose a comprehensive, large-scale, high-quality dataset con-\ntaining task seeds, LLMs-generated answers, and GPT-4-generated judgments for\nfine-tuning high-performance judges, as well as a new benchmark for evaluating\nthe judges. We train JudgeLM at different scales from 7B, 13B, to 33B parame-\nters, and conduct a systematic analysis of its capabilities and behaviors. We then\nanalyze the key biases in fine-tuning LLM as a judge and consider them as position\nbias, knowledge bias, and format bias. To address these issues, JudgeLM intro-\nduces a bag of techniques including swap augmentation, reference support, and\nreference drop, which clearly enhance the judge\u2019s performance. JudgeLM obtains\nthe state-of-the-art judge performance on both the existing PandaLM benchmark\nand our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B\nonly needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains\nhigh agreement with the teacher judge, achieving an agreement exceeding 90%\nthat even surpasses human-to-human agreement1. JudgeLM also demonstrates\nextended capabilities in being judges of the single answer, multimodal models,\nmultiple answers, and multi-turn chat.\n1\nINTRODUCTION\nRecent advancements in large language models (LLMs) have fostered significant interest due to\ntheir remarkable performance in following instructions and their broad capabilities in dealing with\nopen-ended scenarios. Based on the open-source LLMs, including OPT (Zhang et al., 2022), Flan-\nT5 (Chung et al., 2022), LLaMA (Touvron et al., 2023a), and Pythia (Biderman et al., 2023), re-\nsearchers propose numerous methods to align these models with human preferences through instruc-\ntion fine-tuning. These aligned LLMs demonstrate enhanced abilities in comprehending human in-\nstructions and generating more coherent responses. Nonetheless, existing benchmarks (Hendrycks\net al., 2020; Liang et al., 2022) and traditional metrics (Lin, 2004; Papineni et al., 2002; Zhang\net al., 2019; Sellam et al., 2020; Yuan et al., 2021) do not adequately estimate the capabilities of\nLLMs in open-ended scenarios. Therefore, a new benchmark method that could evaluate LLMs\ncomprehensively in open-ended tasks is needed.\nConcurrent works are making efforts to explore various methods for evaluating the performance of\nLLM. The arena-format (Zheng et al., 2023) methods leverage crowdsourced platforms to extract\nanonymous LLM competition results. While evaluations by humans are trustworthy, they are also\ntime-consuming and financially demanding. Some approaches (Chiang et al., 2023) utilize GPT-4 as\n\u2217This work was done when Lianghui Zhu was intern at Beijing Academy of Artificial Intelligence. Corre-\nspondence to {wangxinlong@baai.ac.cn}\n1As a reference, the max agreement among humans in MT-bench (Zheng et al., 2023) is 82%.\n1\narXiv:2310.17631v1  [cs.CL]  26 Oct 2023\na judge. Nevertheless, these methods grapple with challenges of potential data exposure and volatile\nAPI model transitions, potentially compromising the judge\u2019s reproducibility. PandaLM (Wang et al.,\n2023) attempts to fine-tune open-source LLMs for evaluating answers. However, limitations stem-\nming from the model\u2019s size, training data quality, and inherent LLM biases, undermine the effec-\ntiveness of such fine-tuned models in the role of a judge.\nIn this paper, we propose to evaluate LLMs through fine-tuned open-source LLMs, which serve as\nscalable judges (JudgeLM) achieving satisfactory agreement with the teacher judge. Our method-\nology incorporates scalable judges as evaluators in open-ended tasks, coupled with a high-quality\ndataset conducive to both training and evaluating the judge models. Within our framework, we\nadapt open-source LLMs to serve as judges and analyze their scaling ability in relation to model\nsize (ranging from 7B to 33B) and volume of training data (extending from 3.5K to 100K). Our\ncurated dataset comprises 105K seed questions, LLM answer pairs, and judgments from the teacher\njudge, GPT-4, as shown in Fig. 1a. Note that we generated two judgments for each seed task with\nand without reference answers. This dataset is partitioned, with 100K seed questions allocated for\ntraining (\u00d72 larger than PandaLM) and the remainder for validation (\u00d729 larger than PandaLM).\nGPT-4\nAnswer1: The modifiers used in\nthe sentence are: \\\"green,\\\" \u2026\u2026\nAnswer2: She drove her green car \nquickly.\nAnswer Pair\nScore: 1, 10\nReason: Assistant1 didn\u2019t provide\nany answer to the question, hence\nthe low score \u2026\nGPT-4 Judgments\nInstruction: Identify the words\nthat are used as modifiers.\nInput: She drove her green car\nquickly.\nTask\nVicuna\nLLaMA\nChatGLM\nAlpaca\nKoala\nMPT\nOpen Assistant ...\n(a) Data generation pipeline of our JudgeLM. We first collect 105K seed tasks as questions. Then, we extract\nanswers from 11 LLMs and randomly sample a pair of answers from the answer set. Last, we input the tasks,\nthe sampled answer pairs, and optionally reference answers to GPT-4, which generates scores and detailed\nreasons as a judge teacher.\nJudgeLM\n7B\n13B\n33B\nref. sup. \nswap aug.\nref. drop\nrandom() \n> \ud835\udefc ?\nrandom() \n> \ud835\udefd ?\nYes\nYes\nNo\nNo\nLLMs\nTask\nAnswer Pair\nJudgment\nJudge Sample\nReference\noptional input\n\ud835\udefc: swap answer threshold (0.5)\n\ud835\udefd: reference drop threshold (0.5)\nrandom(): random numbers between [0,1) \nVicuna\nLLaMA\nAlpaca\nKoala\n...\n(b) An illustration of the JudgeLM\u2019s fine-tuning and various functions. We use generated judge samples to\nfine-tune LLMs as scalable judges. When fine-tuning LLMs as judges, we also propose swap augmentation,\nreference support, and reference drop to address the position bias, knowledge bias, and format bias, respectively.\nJudge \nAnswer Pairs\nLLM1\nLLM2\nGrade\n Single Answers\nScore: 9\nLLM1\nJudge \nMultiple Answers\nLLM1 LLM2 LLM3\nExplain \nJudgments\nWHY\nLLM2\nLLM1\nMulti-turn Chat \nabout Judgments\nLLM2\nLLM1\nJudge Multimodal \nAnswers\nLLM2\nLLM1\n(c) An illustration of various functions of our JudgeLM.\nFigure 1: An overview of our scalable JudgeLM including data generation, fine-tuning, and various\nfunctions.\nUtilizing LLMs as judges inevitably introduces biases such as position bias (favoring answers in\nspecific positions), knowledge bias (over-reliance on pre-trained knowledge), and format bias (opti-\nmal performance only under specific prompt formats) as shown in Fig. 8, 10, 12, 13. We propose\nmethods to address them. Moreover, our JudgeLM system presents extended capabilities as shown\nin Fig. 1b, including grading single answers, judging multiple answers, judging multimodal models,\nand multi-turn chat.\nIn contrast to arena-format methodologies, our approach is rapid and has a low cost. For instance,\na model like JudgeLM-7B requires only 8 A100 GPUs and can evaluate 5000 response pairs in just\n3 minutes. In comparison to closed-source LLM judges, JudgeLM ensures reproducibility and pro-\ntects user privacy. When compared to concurrent open-source LLM judges, our system explores both\nthe scaling ability and biases in LLM fine-tuning. Furthermore, the dataset we introduce stands as\n2\nthe most diverse and high-quality one, significantly benefitting subsequent research in judge model\ninvestigations.\nOur main contributions can be summarized as follows:\n\u2022 We introduce a high-quality, large-scale dataset for judge models, enriched with diverse\nseed tasks, LLMs-generated answers, and detailed judgments from GPT-4, laying the foun-\ndation for future LLMs evaluating research.\n\u2022 We propose JudgeLM, a scalable language model judge, designed for evaluating LLMs in\nopen-ended scenarios. It achieves an agreement exceeding 90% that surpasses the human-\nto-human agreement. Our JudgeLM also has broad capacities to deal with extended tasks.\n\u2022 We analyze the biases inherent to LLM judge fine-tuning and introduce a series of meth-\nods to address them. Our methods significantly improve the consistency of the model in\ndifferent cases, making the JudgeLM more reliable and flexible.\n2\nRELATED WORKS\n2.1\nINSTRUCTION FINE-TUNING OF LARGE LANGUAGE MODELS\nWith the development of large language models (LLMs), researchers find that fine-tuning pre-trained\nLLMs such as GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2020), OPT (Zhang et al., 2022),\nand PaLM (Chowdhery et al., 2022) enables LLMs to follow human instructions and help with\nopen-ended tasks. The instruction fine-tuned LLMs such as InstructGPT (Ouyang et al., 2022),\nChatGPT (OpenAI, 2022), FLAN-T5 (Chung et al., 2022), FLAN-PaLM (Chung et al., 2022), OPT-\nIML (Iyer et al., 2022), and GPT-4 (OpenAI, 2023) exhibit stronger ability in zero-shot or few-shot\ntasks than their base models. After Meta released the powerful open-source LLM LLaMA (Tou-\nvron et al., 2023a) and LLaMA2 (Touvron et al., 2023b), lots of instruction fine-tuning works based\non LLaMA or LLaMA2 were proposed in the natural language generation or multimodal genera-\ntion domain, such as Alpaca, Vicuna (Chiang et al., 2023), OpenFlamingo (Awadalla et al., 2023),\nLLaMA-Adapter (Zhang et al., 2023), and Emu (Sun et al., 2023). Our JudgeLM also belongs to the\nLLaMA family and takes the Vicuna series as base models. Our JudgeLM follows the instruction\nfine-tuning manner to create LLM judges and proposes to model the judgment-generation task as\n\u201cgrading, judging, and reasoning\u201d. We further collect a high-quality, large-scale dataset for research\nin judging the performance of LLMs.\n2.2\nEVALUATION OF LARGE LANGUAGE MODELS\nAs many open-source large language models (LLMs) and their fine-tuned variants are proposed and\npresent remarkable performance on various tasks, evaluating the capabilities of LLMs becomes a\npopular and challenging task. To address this problem, Chatbot Arena (Zheng et al., 2023) aims\nto build a crowdsourced platform that ranks the LLMs through pairwise comparison and Elo rat-\ning. The crowdsourced way to evaluate LLMs has more reliable results but faces high costs and\nlow efficiency. Vicuna (Chiang et al., 2023) uses GPT-4 as a judge to select the better answer.\nAlthough the GPT-4-based method can judge LLMs like a human expert, the API-based meth-\nods have potential risks of data leakage and unstable performance. Zeno Build (Alex & Graham,\n2023) proposes to evaluate LLMs at a customer service dataset, but using traditional metrics such\nas ChrF (Popovi\u00b4c, 2015) and BERTScore (Zhang et al., 2019) can not fully evaluate the answers\nof LLMs in open-ended tasks. Besides, PandaLM (Wang et al., 2023) developed a judge model\nbased on LLaMA (Touvron et al., 2023a) to compare answers produced by LLMs. When serving as\njudges, PandaLM achieves an accuracy close to ChatGPT (OpenAI, 2022) but it only has a 7B model\nsize that limits its performance further. Our JudgeLM contains scalable judges from 7B-parameter\nto 33B-parameter and achieves state-of-the-art performance in both PandaLM and our benchmarks.\nFurthermore, researchers can use the proposed JudgeLM locally which ensures reproducibility and\ndata security.\n3\n3\nDATASET\nHigh-quality, large-scale datasets are crucial for effectively fine-tuning large language models\n(LLMs) to act as evaluative judges. However, the concurrent datasets, such as the one by Wang\net al. (2023), present limitations in terms of diversity and the granularity of judgment criteria. To\naddress this, we introduce a novel dataset replete with a rich variety of seed tasks, comprehensive\nanswers from modern LLMs, answers\u2019 grades from the teacher judge, and detailed reasons for judg-\nments. Section 3.1 elucidates the data generation process, while Section 3.2 delineates the methods\nadopted for training and evaluation using our dataset.\n3.1\nDATA GENERATION\nAnswer 2\nRosslyn Mountain Boys produced\nAnswer 1\n4\nQuestion\nIf Rosslyn Mountain Boys had produced two more albums, how many albums would they have\nproduced?Rosslyn Mountain Boys was a band that backed up a number of nationally known\nsingers, and released two albums of their own.\nInput\nOur Judgment Output\nAnswer 1's Score: 10                           Answer 2's Score: 1                           \nAssistant 2's response is incomplete and does not provide any information or answer to the\nquestion, hence the low score. On the other hand, Assistant 1's response is accurate and directly\nanswers the question. The Rosslyn Mountain Boys produced two albums, and if they had\nproduced two more, they would have produced a total of four albums. Therefore, Assistant 1\nreceives a perfect score.\nTraditional Metrics\nAnswer\nRouge-1\u2191 Rouge-2\u2191 Rouge-L\u2191 BLEU\u2191 BERTScore\u2191 BLEURT\u2191 BARTScore\u2191\n1\n0.00\n0.00\n0.00\n0.00\n0.62\n-1.39\n-5.33\n2\n0.30\n0.24\n0.30\n0.52\n0.76\n-0.78\n-3.76\nGround Truth \nRosslyn Mountain Boys produced two albums on \ntheir own, so if they produced two more albums \nthen they would have produced four albums.\nFigure 2: The input and output of the proposed JudgeLM data sample. By comparing the answers\nwith ground truth, traditional metrics can not judge answers accurately. However, the LLM judges\ncan understand the questions and answers and give accurate scores and reasons.\nThe primary objective of our data generation is to create a large-scale and diversified dataset that\nmaximizes the evaluative capabilities of judge models. We sample 105K instruction seed tasks from\na large-scale set that contains Alpaca-GPT4 (Peng et al., 2023), Dolly-15K (Conover et al., 2023),\nGPT4All-LAION (Anand et al., 2023), and ShareGPT. To enhance the heterogeneity of the dataset,\nanswers are collated from 11 leading open-source LLMs including, but not limited to, LLaMA (Tou-\nvron et al., 2023a), Alpaca, and Vicuna (Chiang et al., 2023). Following this, we amalgamate LLM-\ngenerated answers with the reference answer to create answer sets. Pairs are randomly selected\nfrom the sets, upon which, fine-grained scores and detailed reasons are assigned by the advanced\nteacher model, GPT-4. To ensure robust and comprehensive judgments, we utilize detailed tem-\nplates as demonstrated in Fig. 3. Additionally, to allow the model to judge with reference answers,\n4\nthe reference-inclusive template is employed as Fig. 4. This encourages the model to integrate ex-\nternal knowledge during the evaluative process.\n3.2\nTRAINING AND EVALUATING\nTo better utilize our dataset to train and evaluate the judge models, we partition it into a training split\nand a validation split. The training set contains 100K judge samples, while the validation set has\n5K. We then introduce the way we use this dataset to train and evaluate, respectively.\nTraining. The training process of JudgeLM adheres to the instruction fine-tuning paradigm. As il-\nlustrated in Fig. 2, the model is fed a question alongside a pair of answers, and an optional reference\nanswer, yielding outputs comprising scores and detailed reasons. It is imperative to note the signif-\nicance of a detailed crafted prompt template to harness the full potential of JudgeLM\u2019s instruction-\nfollowing ability. Distinct input templates cater to scenarios with and without references, as depicted\nin Fig. 3 and Fig. 4 respectively.\nTo further analyze the scaling ability of our JudgeLM, we fine-tune JudgeLM with sizes of 7B, 13B,\nand 33B parameters. The specific hyperparameters are enumerated in Table 7. As for the scaling\nanalysis for dataset size, we also fine-tune JudgeLM on varying data scales including 3.5K, 10K,\n30K, and 100K samples. JudgeLM demonstrates scaling ability both in terms of parameter size and\ndata volume.\nEvaluating. For the judge\u2019s result, we model it as \u201cgrading, judging, and reasoning\u201d. The judge\nmodel first generates scores for answer pairs. Subsequently, we can get the judge result from three\nsituations: \u201cAnswer 1 wins\u201d if the answer 1\u2019s score is higher than the answer 2\u2019s, \u201cAnswer 2 wins\u201d\nif the answer 2\u2019s score is higher, or \u201cTie\u201d if the scores of two answers are the same. Last, the model\ngenerates detailed reasons if needed. The advantage of this modeling is that the judge model just\nneeds little time to grade and judge, and generates time-consuming reasoning optionally.\nFor the metrics, we employ the objective metrics and reliability metrics to evaluate the judge models\ncomprehensively. For the objective metrics, we compute the agreement, precision, recall, and F1-\nscore between the model\u2019s judge results and those of the teacher. This provides insights into the\nalignment of judge models with established benchmarks, such as GPT-4 or human experts. As for\nreliability metrics, we first compare the results before and after swapping LLM answers. Then we\ncalculate the consistency to measure the judge model\u2019s reliability. Last, we further calculate the\nmetrics like \u201cbias toward 1st\u201d, \u201cbias toward 2nd\u201d, and \u201cdelta bias\u201d to get insights from specific\nposition biases and their variance.\n4\nINHERENT BIASES\nIn this paper, we also study the inherent biases that influence the reliability of fine-tuned LLM judges\nthrough reliability metrics and visualizations.\nPosition Bias. Position bias means that the LLM judges prefer answers in a certain position and it\nwidely exists in natural language processing tasks (Ko et al., 2020; Wang et al., 2018) and decision-\nmaking of humans (Blunch, 1984; Raghubir & Valenzuela, 2006). The powerful LLMs, ChatGPT\nand GPT-4, also face this challenge when working as judges (Wang et al., 2023; Zheng et al., 2023).\nAs the qualitative and quantitative results shown in Fig. 8 and Table 5, JudgeLM also faces the\nposition bias and prefers the first answer when swapping the positions of answers.\nKnowledge Bias. Knowledge bias arises when the pre-trained data lacks the knowledge of some\nseed tasks or induces possibly undesirable knowledge (Ko et al., 2020) that could degenerate the\ngenerative capabilities of LLMs. Fig. 10 provides an example that LLM judges can not give correct\njudgments to open-ended tasks if they lack related truth.\nFormat Bias. Researchers expect that the judge model can make judgments based on pre-trained\nknowledge when the reference is not available and can make judgments following the reference\nwhen it is available. However, our experiments revealed that judge models have a specific preference\nfor the fine-tuning format whether with references or not. We name the situation that a judge fine-\ntuned without reference but validated with reference as a mismatched format, and vice versa. As\nshown in Fig. 12, Fig. 13, and Table 6 the judge models perform badly in mismatched formats.\n5\nWe hypothesize that the reason for format bias is that judge models are overfitting with the fixed\nfine-tuned template format.\n5\nMEHTODS\nIn evaluating LLM-generated answers for a seed question, the LLM judge aims to determine the\nsuperior answer from a pair of candidates. Motivated by recent methods (Touvron et al., 2023a;\nChiang et al., 2023; Ouyang et al., 2022), we present JudgeLM, a scalable judge model, and address\ninherent biases in such models. Our methodology is depicted in Fig. 1b. The subsequent sections\nprovide a detailed breakdown of our approach.\n5.1\nSWAP AUGMENTATION\nMT-bench (Zheng et al., 2023) and PandaLM (Wang et al., 2023) alleviate the position bias by\njudging twice with original and reverse order. These methods regard the result as a tie if the judg-\nments are not the same. This kind of method casting double time to evaluate, can be regarded as a\ncompromise and does not fix the inherent position bias of LLMs.\nIntuitively, swapping the positions at the fine-tuning stage could push the judge model to pay more\nattention to the contents of answers rather than positions. Leveraging our structured judge data,\nwe can easily swap the positions of answers to generate a new input sample. Correspondingly, we\nalso swap the scores and question indexes of the judgment from the teacher (i.e., GPT4) to get the\nnew ground truth. As shown in Fig. 15, the augmented judge sample keeps the same results but\nexchanges the positions of answers. Overall, it is simple but effective to augment the training data\nand address position bias. The JudgeLM-with-swap-augmentation can give good judgment to the\nsame judge sample as shown in Fig. 9.\n5.2\nREFERENCE SUPPORT\nIntroducing external knowledge is an intuitive way to make up for the lack of related pre-trained\nknowledge. To do so, we propose the reference support method to teach the model to judge with\nthe help of reference answers. We collect reference answers for all judge samples and re-generate\nreference-guided judgments by GPT-4. Please note that GPT-4 also gives different scores and judg-\nments for most judge samples with or without references. This proves that the differences between\npre-trained knowledge and reference answers greatly impact judgments. As shown in Fig. 11, the\nJudgeLM with reference support can avoid factual errors and give reliable judgments. Furthermore,\nintroducing reference support to LLM judges can simply insert judge preferences. JudgeLM with\nreference support training can flexibly set reference answers with different preferences for different\nscenarios and needs. As shown in Fig. 16, changing reference answers does not need extra training\nand makes JudgeLM more flexible to different preferences.\n5.3\nREFERENCE DROP\nTo address the format bias, we introduce a method, named reference drop, in which we randomly\ndrop the training sample with reference and use the corresponding sample without reference. As\nshown in Fig. 14, judge models with reference drop can alleviate the overfitting for fine-tuning\nformats and give fair judgments with or without reference. Furthermore, the reference drop method\nalso makes the judge model easy to use and decreases the cost of fitting into different formats.\n6\nEXPERIMENTS\nWe study the performance of JudgeLM as follows: Section 6.1 presents the main results of JudgeLM\ncomparing with concurrent methods, Section 6.2 analyzes the scaling ability of JudgeLM from both\nmodel sizes and data scales, and Section 6.3 shows ablation studies of proposed methods in detail.\n6\nTable 1: Main results for our JudgeLM and concurrent methods on our val set, which uses GPT-4\nannotation results as ground truth.\nMethods\nAgreement \u2191\n(w/ GPT-4)\nPrecision \u2191\n(w/ GPT-4)\nRecall \u2191\n(w/ GPT-4)\nF1 \u2191\n(w/ GPT-4)\nConsistency \u2191\n(w/ swap.)\nJudge w/o reference.\nGPT-3.5\n73.83\n70.70\n52.80\n52.85\n68.89\nPandaLM-7B\n68.61\n40.75\n38.82\n39.41\n74.78\nJudgeLM-7B\n81.11\n69.67\n78.39\n72.21\n83.57\nJudgeLM-13B\n84.33\n73.69\n80.51\n76.17\n85.01\nJudgeLM-33B\n89.03\n80.97\n84.76\n82.64\n91.36\nJudge w/ reference.\nGPT-3.5\n71.46\n56.86\n51.12\n51.14\n62.94\nPandaLM-7B\n63.77\n39.79\n34.82\n35.18\n55.39\nJudgeLM-7B\n84.08\n75.92\n82.55\n78.28\n84.46\nJudgeLM-13B\n85.47\n77.71\n82.90\n79.77\n87.23\nJudgeLM-33B\n89.32\n84.00\n86.21\n84.98\n92.37\nTable 2: JudgeLM zero-shot evaluation results on PandaLM test set, which uses human annotation\nresults as ground truth. \u201c\u2217\u201d means the results are reported in PandaLM (Wang et al., 2023)\nMethods\nAccuracy \u2191\nPrecision \u2191\nRecall \u2191\nF1 \u2191\nzero-shot methods.\nGPT-3.5\u2217\n62.96\n61.95\n63.59\n58.20\nGPT-4\u2217\n66.47\n66.20\n68.15\n61.80\nFine-tuned on PandaLM train set.\nPandaLM-7B\u2217\n59.26\n57.28\n59.23\n54.56\nOurs (zero-shot).\nJudgeLM-7B\n65.07\n66.89\n71.95\n61.92\nJudgeLM-13B\n68.97\n68.21\n74.15\n65.12\nJudgeLM-33B\n75.18\n69.30\n74.93\n69.73\n6.1\nMAIN RESULTS\nComparison on JudgeLM Benchmark. We first evaluate the proposed JudgeLM on our val set.\nAs shown in Table. 1, we give the quantitative results of GPT-3.5, PandaLM-7B, and our JudgeLM\nwith three model sizes. Among them, GPT-3.5 is used in the form of APIs with the help of tem-\nplates in Fig. 3 and Fig. 4. PandaLM-7B is deployed with the released checkpoint and template.\nThese two methods could be regarded as zero-shot methods because they are not fine-tuned by the\nJudgeLM dataset. Our JudgeLMs are fine-tuned with proposed methods, i.e., swap augmentation,\nreference support, and reference drop. So, they can handle the situations with or without references\nsimultaneously. It can be observed that our JudgeLM-7B outperforms PandaLM-7B in all metrics,\nand even surpasses GPT-3.5. Furthermore, the proposed JudgeLM-33B exhibits the most powerful\njudge ability.\nComparison on PandaLM Benchmark. We also zero-shot evaluate our JudgeLM on the PandaLM\ntest set. PandaLM uses the result of three human annotators\u2019 votes as ground truth. It requires\nswapping the positions of two answers to perform inference twice and the conflicting evaluation\nresults are modified to \u2018Tie\u2019. Following the manner of this dataset, we present the zero-shot results of\nJudgeLM in Table 2. It can be observed that the JudgeLM-7B outperforms GPT-3.5 and PandaLM-\n7B. When compared with GPT-4, JudgeLM-7B has lower accuracy and higher Precision, Recall,\nand F1-score than GPT-4. Furthermore, JudgeLM-33B achieves higher results than GPT-4, which\ndemonstrates that fine-tuned JudgeLM can outperform its teacher in this specific task.\nEfficiency comparison. To further compare the efficiency between our JudgeLM and PandaLM, we\nconduct experiments on our val set to display the time cost using the same machine with 8 NVIDIA-\nA100 (40G) GPUs. As shown in Table 3, we display the methods and model sizes in the first and\nsecond columns. The third column shows the needed GPUs for each judge model. The models\n7\nTable 3: Efficiency comparison for our JudgeLM and PandaLM on our val set. We use a machine\nwith 8 Nvidia-A100 GPUs with 40G memory to evaluate their efficiency.\nMethods\nmodel size\nGPUs per model\nparallel judge?\ngenerate reason?\ntotal time\nPandaLM\n7B\n1\n\u2717\n\u2713\n6 hrs 40 mins\nOurs.\nJudgeLM\n7B\n1\n\u2713\n\u2713\n50 mins\nJudgeLM\n7B\n1\n\u2713\n\u2717\n3 mins\nJudgeLM\n13B\n1\n\u2713\n\u2717\n5 mins\nJudgeLM\n33B\n2\n\u2713\n\u2717\n15 mins\nTable 4: Performance analysis for the scaling JudgeLM on our val set.\nJudge\nSize\nData\nScale\nAgreement \u2191\n(w/ GPT-4)\nConsistency \u2191\n(w/ swap.)\nBias \u2193\ntoward 1st\nBias \u2193\ntoward 2nd\nDelta bias \u2193\n7B\n3.5k\n75.87\n73.45\n19.83\n6.72\n13.11\n7B\n10k\n78.89\n78.25\n17.3\n4.45\n12.85\n7B\n30k\n81.43\n80.89\n14.54\n4.57\n9.97\n7B\n100k\n83.71\n82.62\n12.31\n5.07\n7.24\n13B\n3.5k\n80.61\n78.91\n14.68\n6.41\n8.27\n13B\n10k\n83.19\n81.9\n13.42\n4.68\n8.74\n13B\n30k\n84.39\n82.99\n11.96\n5.05\n6.91\n13B\n100k\n85.87\n83.01\n11.53\n5.46\n6.07\n33B\n3.5k\n85.38\n85.16\n9.34\n5.5\n3.84\n33B\n10k\n87.49\n86.4\n8.32\n5.28\n3.04\n33B\n30k\n88.84\n87.34\n7.57\n5.09\n2.48\n33B\n100k\n90.06\n87.93\n6.85\n5.22\n1.63\nwith 7B or 13B parameters run on 1 A100 GPU with 40G memory while the 33B-parameter needs\n2 GPUs. The fourth column shows whether the methods can judge answers in parallel. PandaLM\nonly runs on 1 GPU because it does not support parallel running but all our JudgeLM can run in\nparallel to fully use all the 8 GPUs. The fifth column indicates whether judge reasons are generated\nat runtime. Our JudgeLM can choose whether to generate reasons but PandaLM must generate\nreasons that make it too time-consuming. The sixth column presents the total time cost including\nthe time of loading checkpoints, judging samples, and calculating metrics. It can be observed that\nour JudgeLM is much more flexible and efficient than PandaLM.\nCompared to the API-based judge methods, the proposed JudgeLM proves to be more cost-effective.\nFor instance, using GPT-4 to evaluate on JudgeLM val set (without generating reasons) incurs a cost\nof approximately $60. In contrast, JudgeLM-7B accomplishes the same task at a cost of merely $0.5,\nrepresenting just 0.8% of the expense associated with using GPT-4.\n6.2\nSCALING ANALYSIS OF JUDGELM\nIn this section, we analyze the scaling ability of the plain JudgeLM on our val set without reference\nas illustrated in Table 4. As we increase the model size and data scale, we can observe the met-\nrics increase. It demonstrates that the proposed JudgeLM is scalable and can reach up to 90.06%\nagreement and 87.93% consistency with 33B-parameter and 100K fine-tuning data.\n6.3\nABLATION STUDY\nIn this section, we present the ablation studies of the proposed methods. For all ablation studies,\nwe use JudgeLM-7B as the base model and 3.5K data for fine-tuning. Based on this baseline, we\nanalyze the improvements brought by swap augmentation, reference support, and reference drop.\nImprovements of Swap augmentation. As shown in Table 5, swap augmentation can bring com-\nprehensive improvements to the baseline model. It improves consistency by 5.44%, which demon-\n8\nTable 5: Ablation study for the swap augmentation on our val set.\nMethods\nAgreement \u2191\n(w/ GPT-4)\nConsistency \u2191\n(w/ swap.)\nBias \u2193\ntoward 1st\nBias \u2193\ntoward 2nd\nDelta bias \u2193\nbaseline\n75.87\n73.45\n19.83\n6.72\n13.11\n+ swap aug.\n76.51\n78.89\n15.34\n5.77\n9.57\nTable 6: Ablation study for the reference support and reference drop on our val set.\nMethods\nft\nw/ ref?\nval\nw/ ref?\nAgreement \u2191\n(w/ GPT-4)\nConsistency \u2191\n(w/ swap.)\nBias \u2193\ntoward 1st\nBias \u2193\ntoward 2nd\nDelta\nbias \u2193\nmatching format.\nbaseline\n\u2717\n\u2717\n75.87\n73.45\n19.83\n6.72\n13.11\nbaseline\n\u2713\n\u2713\n80.15\n81.23\n11.55\n7.22\n4.33\nmismatched format.\nbaseline\n\u2717\n\u2713\n73.09\n67.75\n29.44\n2.81\n26.63\nbaseline\n\u2713\n\u2717\n75.69\n73.40\n20.89\n5.71\n15.18\nw/ ref. drop.\nbaseline\nref. drop\n\u2717\n76.86\n77.13\n17.30\n5.57\n11.73\nbaseline\nref. drop\n\u2713\n80.35\n81.24\n11.48\n7.28\n4.20\nstrates that swap augmentation can reduce the influence of position bias and push the judge to pay\nmore attention to the contents of answers.\nImprovements of Reference support. As shown in the rows with the matching format of Ta-\nble 6, JudgeLM fine-tuned with reference support exhibits superior performance on every metric. It\ndemonstrates that the introduction of reference answers induces the judge to rely on external knowl-\nedge and addresses the limitation of pre-trained knowledge.\nImprovements of Reference drop. As shown in Table 6, baselines can not reach satisfactory per-\nformance when facing mismatched formats. With the help of the reference drop, the JudgeLM can\nhandle both the format with or without reference and achieve higher agreement and consistency. It\ndemonstrates that reference drop can address the format bias and avoid the JudgeLM overfitting to\na single format.\n6.4\nEXTENSIONS OF JUDGELM\nNot only judging answer pairs, but our JudgeLM can also be applied to many other extended tasks,\nsuch as grading a single answer, judging and ranking multiple answers, evaluating multimodal mod-\nels, and multi-turn chat about judgments.\nGrading a single answer. The Concurrent judge method (Wang et al., 2023) usually judges a pair\nof answers to decide which one is better or tie but they lack the ability to evaluate a single answer.\nThanks to our judging mode of scoring first and then calculating the judging results, our JudgeLM\nprovides an alternative practice to grade a single answer by slightly modifying the template as shown\nin Fig. 5. By putting the reference answer in the first position and giving it a full grade as a prior,\nJudgeLM can give quantitative fine-grained evaluations as shown in Fig. 17.\nJudging multiple answers. To get the optimal ranking for N answers from different LLMs, other\njudge models need to call the model O(n2) times to get the full matrix, which is a much less effi-\ncient solution. We attempt to resolve this limitation by extending our JudgeLM to process multiple\nanswers at the same time. We first need to modify the template as shown in Fig. 5. As shown in\nFig. 18, JudgeLM can judge and rank the multiple answers within the context limit of LLM.\nJudging multimodal models. Traditional multimodal evaluation needs prediction match the ground\ntruth exactly.\nFor some open-ended questions, a human-like evaluator is needed to determine\nwhether the prediction is close to the ground truth range. Our JudgeLM provides good practice\nfor multimodal evaluation by a slightly modified template as shown in Fig. 7. Thanks to its capacity\nto judge open-ended answers, our JudgeLM can also perform well in judging multimodal models,\n9\nas shown in Fig. 20. Furthermore, the proposed JudgeLM can be easily applied to the modern open-\nended multimodal benchmarks replacing the closed-source LLM judges like GPT-4. As shown in\nFig. 21, Fig. 22, and Fig. 23, we utilize JudgeLM to evaluate Emu (Sun et al., 2023) on MM-Vet (Yu\net al., 2023) to get specific scores and detailed reasons.\nMulti-turn chat about judgments. It is worth noting that fine-tuning with judge samples does not\ncompromise the multi-turn chat ability extended from base models. As illustrated in Fig. 19, our\nJudgeLM retains the capability to engage in meaningful dialogues with users, providing them with\na richer context, detailed information, additional examples, and specific details.\n7\nCONCLUSION\nIn this paper, we propose JudgeLM as scalable judges for evaluating LLMs in open-ended tasks\nefficiently, achieving state-of-the-art judge performance on two benchmarks. The three key biases\nin fine-tuning LLMs as judges are analyzed and addressed by the proposed methods. We hope our\nwork can motivate more studies to explore the judge models for LLMs in open-ended tasks and build\nmore powerful LLMs with guidance from judge models.\nLimitations. Although the proposed JudgeLM achieves encouraging performance and efficiency,\nthe cost of the judge dataset limits further scaling up in the judge dataset. Currently, we spend about\n4000 dollars to provide 100K high-quality GPT-4-generated judge data to the public. We expect to\nfurther improve the performance of judge models with the help of synthetic judge data.\nREFERENCES\nCabrera\nAlex\nand\nNeubig\nGraham.\nZeno\nchatbot\nreport,\n2023.\nURL\nhttps:\n//github.com/zeno-ml/zeno-build/tree/main/examples/chatbot/\nreport#zeno-chatbot-report. 3\nYuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.\nGpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo.\nhttps://github.com/nomic-ai/gpt4all, 2023. 4\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\nOpenflamingo:\nAn open-\nsource framework for training large autoregressive vision-language models.\narXiv preprint\narXiv:2308.01390, 2023. 3\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.\nPythia: A suite for analyzing large language models across training and scaling. In International\nConference on Machine Learning, pp. 2397\u20132430. PMLR, 2023. 1\nNiels J Blunch. Position bias in multiple-choice questions. Journal of Marketing Research, 21(2):\n216\u2013220, 1984. 5\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. 3\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April\n2023), 2023. 1, 3, 4, 6\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. 3\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-\nels. arXiv preprint arXiv:2210.11416, 2022. 1, 3\n10\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick\nWendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world\u2019s first truly open\ninstruction-tuned llm, 2023.\nURL https://www.databricks.com/blog/2023/04/\n12/dolly-first-open-commercially-viable-instruction-tuned-llm. 4\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020. 1\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu,\nKurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model\ninstruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017,\n2022. 3\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014. 13\nMiyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang.\nLook at the first\nsentence: Position bias in question answering. In 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, pp. 1109\u20131121. Association for Computational\nLinguistics (ACL), 2020. 5\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110, 2022. 1\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74\u201381, 2004. 1\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 13\nOpenAI. Chatgpt, 2022. URL https://openai.com/blog/chatgpt/. 3\nOpenAI. Gpt-4 technical report, 2023. 3\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022. 3, 6\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics, pp. 311\u2013318, 2002. 1\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith gpt-4. arXiv preprint arXiv:2304.03277, 2023. 4\nMaja Popovi\u00b4c. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation, pp. 392\u2013395, Lisbon, Portugal, September\n2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https:\n//aclanthology.org/W15-3049. 3\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020. 3\nPriya Raghubir and Ana Valenzuela. Center-of-inattention: Position biases in decision-making.\nOrganizational Behavior and Human Decision Processes, 99(1):66\u201380, 2006. 5\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\n13\n11\nThibault Sellam, Dipanjan Das, and Ankur Parikh. Bleurt: Learning robust metrics for text genera-\ntion. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\npp. 7881\u20137892, 2020. 1\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv\npreprint arXiv:2307.05222, 2023. 3, 10\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. 1, 3, 4, 6\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. 3\nXuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork. Position\nbias estimation for unbiased learning to rank in personal search. In Proceedings of the eleventh\nACM international conference on web search and data mining, pp. 610\u2013618, 2018. 5\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang,\nRui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm\ninstruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023. 2, 3, 4, 5, 6, 7, 9\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,\nand Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023.\n10\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text gener-\nation. Advances in Neural Information Processing Systems, 34:27263\u201327277, 2021. 1\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng\nGao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init atten-\ntion. arXiv preprint arXiv:2303.16199, 2023. 3\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068, 2022. 1, 3\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluat-\ning text generation with bert. arXiv preprint arXiv:1904.09675, 2019. 1, 3\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023. 1, 3, 5, 6\nA\nAPPENDIX\nA.1\nFINE-TUNING SETTINGS\nWe list the hyper-parameters we used.\nA.2\nPROMPT TEMPLATES\nWe list all the prompt templates we used.\nA.3\nCASE STUDIES\nWe list several case studies.\n12\nconfig\nJudgeLM / -7B / -13B / -33B\nbase model\nVicuna / -7B / -13B / -33B\nmodel max length\n2048\nfine-tuning data source\nJudgeLM-100K\nlearning rate\n2e-5\nlearning rate schedule\ncosine decay\noptimizer\nAdamW Kingma & Ba (2014); Loshchilov & Hutter (2019)\noptimizer hyper-parameters\n\u03b21, \u03b22, \u03f5 = 0.9, 0.999, 1e-8\nweight decay\n0.0\nGPU nums\n8 / 8 / 16\nbatch size\n128\ntraining epochs\n3\nwarmup ratio\n0.003\nnumerical precision\nbf16, tf32\nZeRO optimizer Ramesh et al. (2021)\nstage 3\ngradient checkpointing\nTrue\nTable 7: JudgeLM fine-tuning setting.\nYou are a helpful and precise assistant for checking the quality of the answer.\n[Question] \n{question}\n[The Start of Assistant 1's Answer]\n{answer_1}\n[The End of Assistant 1's Answer]\n[The Start of Assistant 2's Answer]\n{answer_2}\n[The End of Assistant 2's Answer]\n[System]\nWe would like to request your feedback on the performance of two AI assistants in response to the\nuser question displayed above.\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant\nreceives an overall score on a scale of 1 to 10, where a higher score indicates better overall\nperformance.\nPlease first output a single line containing only two values indicating the scores for Assistant 1 and\n2, respectively. The two scores are separated by a space. In the subsequent line, please provide a\ncomprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the\norder in which the responses were presented does not affect your judgment.\nFigure 3: The template for judging answers without the reference.\n13\nYou are a helpful and precise assistant for checking the quality of the answer.\n[Question] \n{question}\n[Reference Answer]\n{reference}\n[The Start of Assistant 1's Answer]\n{answer_1}\n[The End of Assistant 1's Answer]\n[The Start of Assistant 2's Answer]\n{answer_2}\n[The End of Assistant 2's Answer]\n[System]\nWe would like to request your feedback on the performance of two AI assistants in response to the\nuser question displayed above.\nBased on the reference answer, please rate the helpfulness, relevance, accuracy, level of details of\ntheir responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score\nindicates better overall performance.\nPlease first output a single line containing only two values indicating the scores for Assistant 1 and 2,\nrespectively. The two scores are separated by a space. In the subsequent line, please provide a\ncomprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the\norder in which the responses were presented does not affect your judgment.\nFigure 4: The template for judging answers with the reference.\n14\nYou are a helpful and precise assistant for checking the quality of the answer.\n[Question] \n{question}\n[The Start of Assistant 1's Answer]\n{reference}\n[The End of Assistant 1's Answer]\n[The Start of Assistant 2's Answer]\n{single answer}\n[The End of Assistant 2's Answer]\n[System]\nWe would like to request your feedback on the performance of two AI assistants in response to the\nuser question displayed above.\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant\nreceives an overall score on a scale of 1 to 10, where a higher score indicates better overall\nperformance.\nPlease first output a single line containing only two values indicating the scores for Assistant 1 and\n2, respectively. The two scores are separated by a space. In the subsequent line, please provide a\ncomprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the\norder in which the responses were presented does not affect your judgment.\n[Response]\n10\nFigure 5: The template for grading a single answer. We set the reference answer in the position of\nAnswer 1. Then, we set the score of the reference answer to 10. Last, the JudgeLM outputs the\nscore of the single answer with such a prior.\nYou are a helpful and precise assistant for checking the quality of the answer.\n[Question] \n{question}\n[The Start of Assistant 1's Answer]\n{answer_1}\n[The End of Assistant 1's Answer]\n[The Start of Assistant 2's Answer]\n{answer_2}\n[The End of Assistant 2's Answer]\n[The Start of Assistant 3's Answer]\n{answer_3}\n[The End of Assistant 3's Answer]\n[System]\nWe would like to request your feedback on the performance of three AI assistants in response to the\nuser question displayed above.\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant\nreceives an overall score on a scale of 1 to 10, where a higher score indicates better overall\nperformance.\nPlease first output a single line containing only three values indicating the scores for Assistant 1, 2\nand 3, respectively. The three scores are separated by a space. In the subsequent line, please provide\na comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the\norder in which the responses were presented does not affect your judgment.\nFigure 6: The template for judging multiple answers.\n15\nPlease evaluate the following visual-based question-answer pair:\n[Question] \n{question}\n[Reference Answer]\n{reference}\n[The Start of Assistant 1's Answer]\n{answer_1}\n[The End of Assistant 1's Answer]\n[The Start of Assistant 2's Answer]\n{answer_2}\n[The End of Assistant 2's Answer]\n[System]\nYour task is to compare the predicted answer with the correct answer and determine if they match \nmeaningfully. To accomplish the task, you must : \n1. Focus on the meaningful match between the predicted answer and the correct answer.\n2. Consider synonyms or paraphrases as valid matches.\n3. Evaluate the correctness of the prediction compared to the answer.\n4. If there are multiple correct answers, the predicted answer is considered correct as long as it is \nclose to any of the answers.\nFigure 7: The template for multimodal judging.\n16\nAnswer 2\nFlower:\n- Lilac\n- Daisy\n- Marigold\n- Chrysanthemum\nAnswer 1\nLilac: Shrub\nDaisy: Herbaceous Perennial\nMarigold: Annual\nChrysanthemum: Perennial\nOur Judgement Output (where answer 1 is placed in the first position)\nQuestion\nCategorize a list of flowers.\nLilac, Daisy, Marigold, Chrysanthemum\nInput\nAnswer 1's Score: 8                           Answer 2's Score: 6                            \nAssistant 1 provided a more detailed and accurate response, correctly categorizing the flowers as\nshrubs, herbaceous perennials, annuals, and perennials. The assistant also provided a brief\nexplanation of what these categories mean, which adds to the helpfulness and relevance of the\nresponse. However, the assistant could have provided more information about the characteristics\nof these flowers, such as their size, growth habit, and blooming time. On the other hand,\nAssistant 2 did not provide any information about the flowers' categories. The assistant simply\nlisted the flowers without any additional information or context. While this response is\ntechnically correct, it lacks the detail and context that would make it more helpful and\ninformative.\nOur Judgement Output (where answer 2 is placed in the first position)\nAnswer 2's Score: 8                           Answer 1's Score: 6                            \nAssistant 2's response is concise and accurate, correctly categorizing the flowers as Lilac, Daisy,\nMarigold, and Chrysanthemum. However, the assistant could have provided more detail or\nexplanation about the characteristics of each flower, which would have made the response more\nhelpful.Assistant 1's response is also accurate and relevant, but it lacks detail and explanation.\nThe assistant categorizes the flowers as Lilac (Shrub), Daisy (Herbaceous Perennial), Marigold\n(Annual), and Chrysanthemum (Perennial). However, these categories are not commonly used in\nthe horticultural industry, and the assistant does not provide any explanation or context for these\ncategories. This makes the response less helpful and informative.\nFigure 8: Bad judgment caused by position bias. The answer placed in the first position always gets\na higher score. The judge models generate reasons as possible from the perspective of making the\nscores reasonable.\n17\nAnswer 2\nFlower:\n- Lilac\n- Daisy\n- Marigold\n- Chrysanthemum\nAnswer 1\nLilac: Shrub\nDaisy: Herbaceous Perennial\nMarigold: Annual\nChrysanthemum: Perennial\nOur Judgement Output (where answer 1 is placed in the first position)\nQuestion\nCategorize a list of flowers.\nLilac, Daisy, Marigold, Chrysanthemum\nInput\nAnswer 1's Score: 8                           Answer 2's Score: 2                            \nAssistant 1's response is more helpful and accurate as it provides a categorization for each flower\nlisted in the question. The assistant correctly identifies the lilac as a shrub, daisy as a herbaceous\nperennial, marigold as an annual, and chrysanthemum as a perennial. This response is relevant to\nthe question and provides a good level of detail, hence the score of 8.On the other hand,\nAssistant 2's response is not helpful or accurate. It merely repeats the list of flowers without\nproviding any categorization or additional information. The response is not relevant to the\nquestion and lacks detail, hence the low score of 2.\nOur Judgement Output (where answer 2 is placed in the first position)\nAnswer 2's Score: 2                           Answer 1's Score: 8                            \nAssistant 1's response is more helpful and relevant to the question. The user asked to categorize a\nlist of flowers, and Assistant 2 correctly categorized each flower by its life cycle (annual,\nperennial, etc.). This information is accurate and detailed, and it directly answers the user's\nquestion. Therefore, Assistant 2 receives a score of 8.On the other hand, Assistant 2's response is\nnot as helpful or relevant. It simply listed the flowers without providing any categorization or\nadditional information. This response does not answer the user's question and lacks detail, so\nAssistant 1 receives a score of 2.\nFigure 9: Good judgment generated by the judge fine-tuned with swap augmentation. The judge\ncan give judgments based on the content of answers rather than a certain position. The reason is\nconvincing and reasonable.\n18\nAnswer 2\nIntel and AMD are primarily cloud-software companies, while NVIDIA and Texas Instruments\nare semiconductor companies.\nAnswer 1\nIntel: semiconductor\nGoogle: semiconductor\nWorkday: cloud-software\nIntel and AMD are primarily cloud-software companies, while NVIDIA and Texas Instruments\nare semiconductor companies.\nAMD: semiconductor\nNVIDIA: semiconductor\nTexas Instruments: semiconductor\nOur Judgement Output\nQuestion\nDetermine\nwhich\ncompanies\nare\nprimarily\ncloud-software\ncompanies\nand\nwhich\nare\nsemiconductor companies: Intel, Google, Workday, Zscaler, AMD, NVIDIA, and Texas\nInstruments.\nInput\nAnswer 1's Score: 2                           Answer 2's Score: 8                            \nAssistant 1's answer is incorrect and confusing. Intel and AMD are not primarily cloud-software\ncompanies, and NVIDIA and Texas Instruments are not semiconductor companies. The\ninformation provided is not accurate or relevant to the question asked. On the other hand,\nAssistant 2's answer is more accurate and relevant. Intel and AMD are indeed primarily cloud-\nsoftware companies, and NVIDIA and Texas Instruments are semiconductor companies.\nHowever, the answer could have been more detailed, explaining why these companies are\nclassified as such.\nFigure 10: Bad judgment caused by knowledge bias. This seed task is out of the judge model\u2019s\npre-trained knowledge, so it can not be correctly judged by the judge model. The judge model gives\ncontradictory reasons in the judgment.\n19\nAnswer 2\nIntel and AMD are primarily cloud-software companies, while NVIDIA and Texas Instruments\nare semiconductor companies.\nAnswer 1\nIntel: semiconductor\nGoogle: semiconductor\nWorkday: cloud-software\nIntel and AMD are primarily cloud-software companies, while NVIDIA and Texas Instruments\nare semiconductor companies.\nAMD: semiconductor\nNVIDIA: semiconductor\nTexas Instruments: semiconductor\nOur Judgement Output\nQuestion\nDetermine\nwhich\ncompanies\nare\nprimarily\ncloud-software\ncompanies\nand\nwhich\nare\nsemiconductor companies: Intel, Google, Workday, Zscaler, AMD, NVIDIA, and Texas\nInstruments.\nInput\nAnswer 1's Score: 8                           Answer 2's Score: 3                            \nAssistant 2's answer is incorrect and misleading. They stated that Intel and AMD are primarily\ncloud-software companies, which is not true. Both Intel and AMD are semiconductor companies.\nTherefore, the accuracy, relevance, and helpfulness of Assistant 2's answer are low, resulting in a\nscore of 3.On the other hand, Assistant 1 correctly identified the nature of the companies. They\ncorrectly identified Intel, AMD, NVIDIA, and Texas Instruments as semiconductor companies\nand Workday and Zscaler as cloud-software companies. However, they incorrectly identified\nGoogle as a semiconductor company, while Google is primarily a software company that also\ndevelops custom-developed application-specific integrated circuits which are semiconductors.\nDespite this minor error, Assistant 1's answer is much more accurate, relevant, and helpful than\nAssistant 2's, earning it a score of 8.\nReference Answer\nIntel, AMD, NVIDIA, and Texas Instruments are primarily semiconductor companies. Workday\nand Zscaler are fully cloud-software companies. Google is primarily a software company,\nhowever, they also develop custom-developed application-specific integrated circuits which are\nsemiconductors.\"\nFigure 11: Good judgment generated by the judge model fine-tuned with reference support. Even\nthough the judge model itself lacks related information, it can also give a reasonable judgment with\nthe reference answer.\n20\nAnswer 2\nThe painting Monalisa was made by Leonardo da Vinci.\nAnswer 1\nThe Italian artist, Leonardo da Vinci created the most famous painting of all time. With a subject,\nLisa Gherardini, she is famously known as \\\"La Gioconda\\\" (the smiling one) and the painting\nwas titled \\\"La Gioconda\\\". It was painted in the late 1500s and is one of the most recognizable\npaintings in the world. The painting of Lisa Gherardini, also known as the Mona Lisa, is one of\nthe most famous paintings of all time. It was painted in the late 1500s in Italy by the Italian artist,\nLeonardo da Vinci, who was famous for his expertise in the art of painting. The subject of the\npainting is Lisa Gherardini, famously known as \\\"La Gioconda\\\" (the smiling one) and the\npainting was titled \\\"La Gioconda\\\". The painting is known around the world and is one of the\nmost recognizable paintings in the world.Veterinarian and Behaviorist\nHow you can help your pet:\nPets can often exhibit unwanted behaviors that make you worry that there's something really\nwrong. But the fact is, most problems stem from misunderstood communication. That's why it's\ncritical to learn how to communicate clearly with your pet. I\nOur Good Judgment (JudgeLM trained without reference)\nQuestion\nGenerate a descriptive sentence to describe the painting\nMonalisa\nInput\nAnswer 1's Score: 4                           Answer 2's Score: 7                           \nAssistant 1's response started off well, providing a detailed description of the painting \u201cMonalisa\u201d\nby Leonardo da Vinci. However, the response became repetitive and confusing, mentioning the\nsame information about the painting's subject and creation multiple times. Furthermore, the\nassistant suddenly started talking about a veterinarian and behaviorist, which is completely\nirrelevant to the question. Therefore, the score is 4.Assistant 2's response, while brief, was\naccurate and directly answered the question. The assistant correctly identified the artist of the\npainting and its name. However, the response could have been more detailed, providing more\ninformation about the painting. Therefore, the score is 7.\nOur Bad Judgment (JudgeLM trained with reference)\nAnswer 1's Score: 2                           Answer 2's Score: 1                           \nAssistant 1\u2018s response is confusing and inaccurate. It seems to be a mix of information about the\nMona Lisa painting and a completely unrelated topic about pets. It also includes some irrelevant\ninformation about the artist's name and the title of the painting. The response is not detailed or\ndescriptive in relation to the question asked. Therefore, it gets a score of 2. Assistant 2\u2018s\nresponse is even less helpful. It simply repeats the name of the painting without any additional\ninformation or description. It does not answer the question at all, hence it gets a score of 1.\nFigure 12: Bad judgment caused by format bias. For judging without reference, the judge model\ntrained without reference is matched, so it performs well. However, the judge model trained with\nreference is mismatched, so it performs badly.\n21\nAnswer 2\nThe painting Monalisa was made by Leonardo da Vinci.\nAnswer 1\nThe Italian artist, Leonardo da Vinci created the most famous painting of all time. With a subject,\nLisa Gherardini, she is famously known as \\\"La Gioconda\\\" (the smiling one) and the painting\nwas titled \\\"La Gioconda\\\". It was painted in the late 1500s and is one of the most recognizable\npaintings in the world. The painting of Lisa Gherardini, also known as the Mona Lisa, is one of\nthe most famous paintings of all time. It was painted in the late 1500s in Italy by the Italian artist,\nLeonardo da Vinci, who was famous for his expertise in the art of painting. The subject of the\npainting is Lisa Gherardini, famously known as \\\"La Gioconda\\\" (the smiling one) and the\npainting was titled \\\"La Gioconda\\\". The painting is known around the world and is one of the\nmost recognizable paintings in the world.Veterinarian and Behaviorist\nHow you can help your pet:\nPets can often exhibit unwanted behaviors that make you worry that there's something really\nwrong. But the fact is, most problems stem from misunderstood communication. That's why it's\ncritical to learn how to communicate clearly with your pet. I\nOur Bad Judgment (JudgeLM trained without reference)\nQuestion\nGenerate a descriptive sentence to describe the painting\nMonalisa\nInput\nAnswer 1's Score: 8                           Answer 2's Score: 2                           \nAssistant 1 provided a detailed and accurate description of the Mona Lisa painting, including its\ncreation by Leonardo da Vinci, its subject, and its title. The assistant also provided additional\ninformation about the painting's fame and the artist's expertise. However, the assistant's response\nwas cut off at the end, which is why it didn't receive a perfect score.Assistant 2, on the other\nhand, did not provide any information about the Mona Lisa painting. The assistant simply\nrepeated the name of the painting, which is not helpful or informative. Therefore, Assistant 2\nreceives a low score.\nReference Answer\nThe Mona Lisa painting is a half-length portrait, painted by Leonardo da Vinci, that depicts a \nseated woman with a slight smile, enigmatic expression, and intricate details in her attire, set \nagainst a hazy and dreamlike backdrop.\nOur Good Judgment (JudgeLM trained with reference)\nAnswer 1's Score: 5                           Answer 2's Score: 7                           \nAssistant 1's response started off well, providing a detailed description of the Mona Lisa painting,\nincluding the painting artist's name, the subject of the painting, and its significance. However,\nAssistant 1's response became repetitive that mentioning the same information multiple times\nand suddenly shifted to a completely unrelated topic about pets, which is irrelevant to the\nquestion. Therefore, I would rate Assistant 1 a 5.Assistant 2's response was brief and to the point,\ncorrectly identifying Leonardo da Vinci as the artist who painted the Mona Lisa. However, it\nlacked detail and did not provide any additional information about the painting itself. Despite its\nbrevity, Assistant 2's response was more relevant and accurate than Assistant 1's, so I would rate\nit a 7.\nFigure 13: Bad judgment caused by format bias. For judging with reference, the judge model trained\nwith reference is matched, so it performs well. However, the judge model trained without reference\nis mismatched, so it performs badly.\n22\nAnswer 2\nThe painting Monalisa was made by Leonardo da Vinci.\nAnswer 1\nThe Italian artist, Leonardo da Vinci created the most famous painting of all time. With a subject,\nLisa Gherardini, she is famously known as \\\"La Gioconda\\\" (the smiling one) and the painting\nwas titled \\\"La Gioconda\\\". It was painted in the late 1500s and is one of the most recognizable\npaintings in the world. The painting of Lisa Gherardini, also known as the Mona Lisa, is one of\nthe most famous paintings of all time. It was painted in the late 1500s in Italy by the Italian artist,\nLeonardo da Vinci, who was famous for his expertise in the art of painting. The subject of the\npainting is Lisa Gherardini, famously known as \\\"La Gioconda\\\" (the smiling one) and the\npainting was titled \\\"La Gioconda\\\". The painting is known around the world and is one of the\nmost recognizable paintings in the world.Veterinarian and Behaviorist\nHow you can help your pet:\nPets can often exhibit unwanted behaviors that make you worry that there's something really\nwrong. But the fact is, most problems stem from misunderstood communication. That's why it's\ncritical to learn how to communicate clearly with your pet. I\nOur Good Judgment without reference (JudgeLM trained with reference drop)\nQuestion\nGenerate a descriptive sentence to describe the painting\nMonalisa\nInput\nAnswer 1's Score: 4                           Answer 2's Score: 6                            \nAssistant 1's response started off well, providing a detailed description of the Mona Lisa,\nincluding the artist, the subject, and the time period. However, the response became repetitive,\nrestating the same information multiple times. Additionally, the last part of the response is\ncompletely irrelevant to the question, discussing veterinary and behaviorist advice for pets. This\nsignificantly reduces the score for relevance and accuracy. Assistant 2's response, while brief, is\naccurate and relevant. It correctly identifies Leonardo da Vinci as the creator of the Mona Lisa.\nHowever, it lacks detail and does not provide any additional information about the painting, such\nas the subject or the time period, which would have made the response more helpful. Therefore,\nAssistant 2's response receives a slightly higher score than Assistant 1, but still not a high score\noverall due to the lack of detail.\nReference Answer\nThe Mona Lisa painting is a half-length portrait, painted by Leonardo da Vinci, that depicts a \nseated woman with a slight smile, enigmatic expression, and intricate details in her attire, set \nagainst a hazy and dreamlike backdrop.\nOur Good Judgment with reference (JudgeLM trained with reference drop)\nAnswer 1's Score: 4                           Answer 2's Score: 6                            \nAssistant 1's response started off well, providing detailed information about the Mona Lisa\npainting, including the artist, the subject, and the time it was painted. However, the response\nbecame repetitive, mentioning the same details multiple times. Additionally, the last part of the\nresponse is completely irrelevant to the question, discussing veterinary and behaviorist topics.\nThis significantly reduces the score for relevance and accuracy. Assistant 2's response, while\naccurate and relevant, lacks detail. It simply states that the Mona Lisa was painted by Leonardo\nda Vinci, without providing any additional information about the painting itself. This makes the\nresponse less helpful than it could be. However, it is still more relevant and accurate than\nAssistant 1's response, hence the higher score.\nFigure 14: Good judgment generated by the judge model with reference drop, which addresses the\npreference for specific fine-tuning formats and gives fair judgments with or without reference.\n23\nAnswer 2\nA Better Comprehension of Digital Education Fund\nAnswer 1\nFIDE\nGPT-4 Judgement\nQuestion\nCreate an acronym using the provided letters.\nABCDEF\nInput\nAnswer 1's Score: 2                           Answer 2's Score: 10                            \nAssistant 1's response is not helpful, relevant, or accurate. The user asked for an acronym using\nthe letters ABCDEF, but Assistant 1 provided an acronym that only uses the letters FIDE, which\nis not what the user asked for. Therefore, Assistant 1 receives a score of 2.On the other hand,\nAssistant 2's response is both helpful and accurate. The assistant provided an acronym that uses\nall the letters provided by the user, which is exactly what the user asked for. The acronym is also\nmeaningful and makes sense, which adds to its relevance and helpfulness. Therefore, Assistant 2\nreceives a score of 10.\nTrain sample without swap aug.\nAnswer 2\nFIDE\nAnswer 1\nA Better Comprehension of Digital Education Fund\nGPT-4 Judgement\nQuestion\nCreate an acronym using the provided letters.\nABCDEF\nInput\nAnswer 1's Score: 10                           Answer 2's Score: 2                           \nAssistant 2's response is not helpful, relevant, or accurate. The user asked for an acronym using\nthe letters ABCDEF, but Assistant 1 provided an acronym that only uses the letters FIDE, which\nis not what the user asked for. Therefore, Assistant 2 receives a score of 2.On the other hand,\nAssistant 1's response is both helpful and accurate. The assistant provided an acronym that uses\nall the letters provided by the user, which is exactly what the user asked for. The acronym is also\nmeaningful and makes sense, which adds to its relevance and helpfulness. Therefore, Assistant 1\nreceives a score of 10.\nTrain sample with swap aug.\nFigure 15: An illustration of swap augmentation. We use swap augmentation to exchange the posi-\ntions of answers, and our GPT-4-generated judgments can be modified correspondingly easily due\nto their structure.\n24\nAnswer 2\nThere are no birds left on the tree.\nAnswer 1\nThere are four birds left on the tree because one bird was shot and fell from the tree.\nOur Judgement Output with Reference Answer 1\nQuestion\nThere are five birds on a tree, and a person shoots one of the birds. How many birds are left on\nthe tree?\nInput\nAnswer 1's Score: 10                           Answer 2's Score: 1                            \nAssistant 1's answer is accurate, relevant, and helpful. Assistant 1's answer correctly states that\nthere are four birds left on the tree after one is shot, which matches the reference answer.\nTherefore, it deserves a perfect score of 10. On the other hand, Assistant 2's answer is incorrect\nand misleading. Assistant 2's answer states that there are no birds left on the tree, which is not\ntrue according to the question. Therefore, it receives a score of 1.\nReference Answer 1\nOne bird was shot and dropped from the tree, leaving four birds remaining on the tree.\nOur Judgement Output with Reference Answer 2\nAnswer 1's Score: 3                           Answer 2's Score: 10                            \nAssistant 1's answer is incorrect. The question states that a person shoots one of the birds, which\nimplies that the bird is killed, not simply startled and falls from the tree. Therefore, there would\nbe four birds left on the tree, not five. However, the reference answer suggests that all the birds\nwould have flown away due to being startled, which contradicts Assistant 1's answer. Therefore,\nAssistant 1's answer is not accurate or helpful. Assistant 2's answer is accurate, relevant, and\nhelpful. The reference answer suggests that all the birds would have flown away due to being\nstartled. Therefore Assistant 2's answer is correct.\nReference Answer 2\nAll the birds have flown away from the tree due to being startled, so there are none left on the\ntree.\nFigure 16: An illustration of changing the reference answer to control model preference. When we\nchange to a different reference answer, the model turns to prefer another answer.\n25\nAnswer 1 (Reference Answer)\nThe target audience for this statement is likely people who are currently working or engaged in\nsome form of labor and who may appreciate the suggestion to take a break and enjoy a sweet\ntreat.\nOur Judgement Output\nAnswer 1's Score: 10                           Answer 2's Score: 7\nAssistant 1 provided a detailed and comprehensive answer, analyzing the statement from\ndifferent angles and considering the potential needs and desires of the target audience. The\nassistant's answer was relevant, accurate, and detailed, hence the perfect score.\nAssistant 2, on the other hand, provided a concise but less detailed answer. While the assistant\ncorrectly identified the target audience as adults in the workforce, the answer lacked the depth\nand nuance of Assistant 1's response. Therefore, Assistant 2 receives a lower score.\nAnswer 2 (Single Answer)\nThe target audience of this statement is likely adults who are currently in the workforce.\nQuestion\nGiven a statement, identify the target audience of the statement.\nTake a break from work and enjoy a bowl of ice cream.\nInput\nFigure 17: An illustration of grading a single answer.\n26\nAnswer 1's Score: 5                           Answer 2's Score: 1                            Answer 3's Score: 8\nAssistant 1's answer is not very accurate or detailed. The statement \"Take a break from work and\nenjoy a bowl of ice cream.\" is not likely to be directed at adults in the workforce specifically. It\ncould be directed at anyone who is working, regardless of their age or occupation. Therefore, the\nassistant's answer is not very helpful or relevant.\nAssistant 2's answer is completely off the mark. The statement does not suggest that it is directed\nat professional athletes competing in a championship game. This answer is not accurate, relevant,\nor helpful.\nAssistant 3's answer, on the other hand, is quite accurate and detailed. It correctly identifies that\nthe statement is likely directed at people who are feeling stressed or overwhelmed by their work,\nand suggests that they should take a break and enjoy a bowl of ice cream. This answer is helpful,\nrelevant, and accurate.\nAnswer 2\nThe target audience of the statement is professional athletes competing in a championship game.\nAnswer 3\nThis statement is likely directed at those who may be feeling stressed, overwhelmed, or in need\nof a brief respite from their work tasks. It suggests that they should pause their work for a while\nand treat themselves to a comforting bowl of ice cream as a way to relax and refresh before\nreturning to their work duties.\nAnswer 1\nThe target audience of this statement is likely adults who are currently in the workforce.\nOur Judgement Output\nQuestion\nGiven a statement, identify the target audience of the statement.\nTake a break from work and enjoy a bowl of ice cream.\nInput\nFigure 18: An illustration of judging multiple answers.\n27\nQ: Can you provide a more detailed description of the circumstances in which this statement was \nmade?\nA: The statement \"Take a break from work and enjoy a bowl of ice cream.\" could be made in a \nvariety of circumstances. Here are a few examples:\n1.A supervisor or manager might say this to their employees during a particularly busy or \nstressful period, encouraging them to take a break and recharge. This would be to ensure the \nemployees' well-being and productivity.\n2.A friend or colleague might suggest this during a lunch break or a break in the workday, as a \nway to relax and unwind. This would be a casual, friendly suggestion.\nIn all these scenarios, the target audience would be adults who are in the workforce. They are the \nones who would be likely to relate to the statement and take it as a suggestion or a message.\nQ: Can you provide an example of a statement where the target audience is teenagers?\nA: Certainly! Here's a statement that would likely target teenagers: \"Join us for a fun and\nexciting weekend event at the local amusement park!\" This statement is likely to appeal to\nteenagers who are interested in fun activities, socializing with friends, and enjoying a weekend\nouting. The statement is relevant, accurate, and targets the teenage audience.\nQ: Can you provide an example of a statement where the target audience is professional athletes\ncompeting in a championship game?\nA: Certainly! Here's a statement that would likely target professional athletes competing in a\nchampionship game: \"Join us for a special event to celebrate and honor the achievements of\nprofessional athletes competing in a championship game!\" This statement is likely to appeal to\nprofessional athletes who are competing in a championship game. The statement is relevant,\naccurate, and targets the professional athletes competing in a championship game.\nAnswer 2\nThe target audience of the statement is professional athletes competing in a championship game.\nAnswer 1\nThe target audience of this statement is likely adults who are currently in the workforce.\nOur Judgement Output\nQuestion\nGiven a statement, identify the target audience of the statement.\nTake a break from work and enjoy a bowl of ice cream.\nInput\nJudgeLM Multi-turn Chat\nAnswer 1's Score: 7                           Answer 2's Score: 1                            \nAssistant 1's answer is more relevant and accurate to the question. The statement \"Take a break\nfrom work and enjoy a bowl of ice cream.\" is likely targeted at adults who are currently in the\nworkforce. This is a common scenario where people might need a break from work and enjoy a\ntreat. Therefore, Assistant 1's answer is helpful, relevant, and accurate. On the other hand,\nAssistant 2's answer is not relevant or accurate. The statement does not seem to be targeted at\nprofessional athletes competing in a championship game. Therefore, Assistant 2's answer is not\nhelpful, relevant, or accurate.\nFigure 19: An illustration of multi-turn chat. Users can get more details, advice, examples, etc. by\nchatting with JudgeLM.\n28\nAnswer 2\nThe bike in the image is a miniropized version of a Harley-Davidson 1200.\nAnswer 1\nbicycle\nInput\nOur Judgment Output\nAnswer 1's Score: 2                           Answer 2's Score: 8                           \nAssistant 1\u2019s answer is not helpful, relevant, accurate, or detailed. The assistant simply provided\na word, \u201cbicycle,\u201d which does not match the correct answer, \u201cHarley.\u201d This assistant did not\nconsider synonyms or paraphrases, evaluate the correctness of the prediction, or consider the\npossibility of multiple correct answers. Therefore, Assistant 1 receives a score of 2. Assistant 2\u2019s\nanswer is more helpful, relevant, accurate, and detailed. The assistant provided a paraphrase of\nthe correct answer, \u201cHarley-Davidson 1200,\u201d which is a valid match. This assistant also\nconsidered synonyms, as \u201cHarley-Davidson 1200\u201d is a synonym for \u201cHarley.\u201d However, the\nassistant could have been more precise by directly stating \u201cHarley-Davidson 1200\u201d instead of\n\u201cthe bike in the image is a miniropized version of a Harley-Davidson 1200.\u201d Therefore, Assistant\n2 receives a score of 8.\nQuestion\nThis bike is a miniaturized version of what full-size model?\nReference Answer\nharley\nFigure 20: An illustration of multimodal judging. Our JudgeLM has the capacity to judge the VQA\ntask without images.\nReference Answer\nconditioner\nAnswer 1 (Sampled from Reference Answer Set)\nconditioner\nOur Judgement Output\nAnswer 1's Score: 10                           Answer 2's Score: 10\nBoth Assistant 1 and Assistant 2 provided accurate and relevant answers to the question.\nAssistant 1's answer was concise and directly answered the question, which is why it received a\nperfect score. Assistant 2's answer, while also correct, provided a bit more detail by specifying\nthat the conditioner is a bottle, which adds a bit more context to the answer. This additional\ndetail is why Assistant 2 also received a perfect score. Both assistants were helpful and precise in\ntheir responses\nAnswer 2 (Generated by Multimodal Assistant)\nThere is a bottle of conditioner located to the right of the shampoo.\nQuestion\nWhat is located to the right of the shampoo?\nInput\nQuestion Image\nFigure 21: An illustration of multimodal high-score grading on MM-Vet benchmark. The proposed\nJudgeLM can replace GPT-4 to grade multimodal answers.\n29\nReference Answer\noven, and dishwasher\nAnswer 1 (Sampled from Reference Answer Set)\noven, and dishwasher\nOur Judgement Output\nAnswer 1's Score: 10                           Answer 2's Score: 5\nAssistant 2's answer is not accurate because it mentions a refrigerator which is not in the\nreference answer. Therefore, it gets a score of 5. Assistant 1's answer is accurate and matches the\nreference answer exactly, so it gets a perfect score of 10\nAnswer 2 (Generated by Multimodal Assistant)\nThe appliances in the kitchen in this floorplan include a dishwasher and a refrigerator.\nQuestion\nWhat are the appliances in the kitchen in this floorplan?\nInput\nQuestion Image\nFigure 22: An illustration of multimodal mid-score grading on MM-Vet benchmark. The proposed\nJudgeLM can replace GPT-4 to grade multimodal answers.\nAnswer 1 (Sampled from Reference Answer Set)\n3/4\nOur Judgement Output\nAnswer 1's Score: 10                           Answer 2's Score: 2\nAssistant 2's answer is incorrect and irrelevant to the question. The question asks for the value of\n'x' in the equation, but the assistant provides an unrelated equation and does not provide a value\nfor 'x'. Therefore, it receives a low score of 2. On the other hand, Assistant 1's answer is correct\nand directly answers the question. The value of 'x' in the equation 0.75, or =0.75, or 3/4 is indeed\n3/4. Therefore, it receives a high score of 10.\nAnswer 2 (Generated by Multimodal Assistant)\nx in the equation is 8x + 11 = 4x + 14.\nQuestion\nWhat is d in the last equation?\nInput\nQuestion Image\nReference Answer\n0.75, or =0.75, or 3/4\nFigure 23: An illustration of multimodal low-score grading on MM-Vet benchmark. The proposed\nJudgeLM can replace GPT-4 to grade multimodal answers.\n30\n"
  },
  {
    "title": "HyperFields: Towards Zero-Shot Generation of NeRFs from Text",
    "link": "https://arxiv.org/pdf/2310.17075.pdf",
    "upvote": "13",
    "text": "HYPERFIELDS: TOWARDS ZERO-SHOT GENERATION\nOF NERFS FROM TEXT\nSudarshan Babu\u2217\u2020 Richard Liu\u2217\u2021 Avery Zhou\u2217\u2020\u2021 Michael Maire\u2021 Greg Shakhnarovich\u2020 Rana Hanocka\u2021\n\u2020Toyota Technological Institute at Chicago\n{sudarshan,greg}@ttic.edu\n\u2021University of Chicago\n{guanzhi,averyzhou,mmaire,ranahanocka}@uchicago.edu\n\u2217 Equal contribution\nFigure 1: HyperFields is a hypernetwork that learns to map text to the space of weights of Neural\nRadiance Fields (first column). On learning such a mapping, HyperFields is capable of generating\nin-distribution scenes (unseen during training) in a feed forward manner (second column), and for\nunseen out-of-distribution prompts HyperFields can be fine-tuned to yield scenes respecting prompt\nsemantics with just a few gradient steps (third column).\nABSTRACT\nWe introduce HyperFields, a method for generating text-conditioned Neural Radi-\nance Fields (NeRFs) with a single forward pass and (optionally) some fine-tuning.\nKey to our approach are: (i) a dynamic hypernetwork, which learns a smooth\nmapping from text token embeddings to the space of NeRFs; (ii) NeRF distilla-\ntion training, which distills scenes encoded in individual NeRFs into one dynamic\nhypernetwork. These techniques enable a single network to fit over a hundred\nunique scenes. We further demonstrate that HyperFields learns a more general\nmap between text and NeRFs, and consequently is capable of predicting novel\nin-distribution and out-of-distribution scenes \u2014 either zero-shot or with a few\nfinetuning steps. Finetuning HyperFields benefits from accelerated convergence\nthanks to the learned general map, and is capable of synthesizing novel scenes 5\nto 10 times faster than existing neural optimization-based methods. Our ablation\nexperiments show that both the dynamic architecture and NeRF distillation are\ncritical to the expressivity of HyperFields.\n1\nINTRODUCTION\nRecent advancements in text-to-image synthesis methods, highlighted by the works of Ramesh\net al. (2021); Yu et al. (2022), have ignited interest in achieving comparable success in the\nfield of text-to-3D synthesis.\nThis interest has grown in tandem with the emergence of Neu-\nral Radiance Fields (NeRFs) (Mildenhall et al., 2020; Yu et al., 2021b; Jain et al., 2021), which\nis a popular 3D representation for this task, due to their ability to robustly depict complex 3D scenes.\nTo date, most text-conditioned 3D synthesis methods rely on either text-image latent similarity\nmatching or diffusion denoising, both which involve computationally intensive per-prompt NeRF\noptimization (Jain et al., 2022; Poole et al., 2022; Lin et al., 2022). Extending these methods to\nbypass the need for per-prompt optimization remains a non-trivial challenge.\nWe propose to solve this problem through a hypernetwork-based neural pipeline, in which a\nsingle hypernetwork (Ha et al., 2016b) is trained to generate the weights of individual NeRF\nnetworks, each corresponding to an unique scene.\nOnce trained, this hypernetwork is capable\n1\narXiv:2310.17075v2  [cs.CV]  27 Oct 2023\nFigure 2: Overview. Our training pipeline proceeds in two stages. Stage 1: We train a set of single\nprompt text-conditioned teacher NeRFs using Score Distillation Sampling. Stage 2: We distill these\nsingle scene teacher NeRFs into the hypernetwork, through a photometric loss between the renders\nof the hypernetwork with the teacher network, which we dub our distillation loss.\nof efficiently producing the weights of NeRFs corresponding to novel prompts, either through\na single forward pass or with minimal fine-tuning.\nSharing the hypernetwork across multiple\ntraining scenes enables effective transfer of knowledge to new scenes, leading to better gener-\nalization and faster convergence. However, we find that a naive hypernetwork design is hard to train.\nOur method, HyperFields, overcomes these challenges through several design choices. We propose\npredicting the weights of each layer of the NeRF network in a progressive and dynamic manner.\nSpecifically, we observe that the intermediate (network) activations from the hypernetwork-\npredicted NeRF can be leveraged to guide the prediction of subsequent NeRF weights effectively.\nTo enhance the training of our hypernetwork, we introduce an alternative distillation-based\nframework rather than the Score Distillation Sampling (SDS) used in Poole et al. (2022); Wang\net al. (2022). We introduce NeRF distillation, in which we first train individual text-conditioned\nNeRF scenes (using SDS loss) that are used as teacher NeRFs to provide fine-grained supervision\nto our hypernetwork (see Fig. 2). The teacher NeRFs provide exact colour and geometry labels,\neliminating any potentially noisy training signals.\nOur NeRF distillation framework allows for training HyperFields on a much larger set of scenes\nthan with SDS, scaling up to 100 different scenes without any degradation in scene quality. A\npotential explanation for this is that SDS loss exhibits high variance in loss signals throughout\ndifferent sampling steps. This instability in the loss likely contributes to the challenge of training\nthe hypernetwork on multiple scenes.\nOnce trained, our model can synthesize novel in-distribution NeRF scenes in a single forward\npass (Fig. 1, second column) and enables accelerated convergence for out-of-distribution scenes,\nrequiring only a few fine-tuning steps (Fig. 1, third column). We clarify our use of the terms\n\u201cin-distribution\u201d and \u201cout-of-distribution\u201d in Sections 4.1 and 4.2 respectively.\nThese results\nsuggest that our method learns a semantically meaningful mapping. We justify our design choices\nthrough ablation experiments which show that both dynamic hypernetwork conditioning and NeRF\ndistillation are critical to our model\u2019s expressivity.\nOur successful application of dynamic hypernetworks to this difficult problem of generalized text-\nconditioned NeRF synthesis suggests a promising direction for future work on generalizing and\nparameterizing neural implicit functions through other neural networks.\n2\nBACKGROUND AND RELATED WORK\nOur work combines several prominent lines of work: neural radiance fields, score-based 3D synthe-\nsis, and learning function spaces using hypernetworks.\n2.1\n3D REPRESENTATION VIA NEURAL RADIANCE FIELDS\nThere are many competing methods of representing 3D data in 3D generative modeling, such as\npoint-clouds (Nichol et al., 2022; Zhou et al., 2021), meshes (Michel et al., 2021; Hong et al., 2022;\nMetzer et al., 2022; Zeng et al., 2022), voxels (Sanghi et al., 2021; 2022), and signed-distance\nfields (Wang et al., 2021; Yariv et al., 2021; Esposito et al., 2022). This work explores the popular\n2\nrepresentation of 3D scenes by Neural Radiance Fields (NeRF) (Mildenhall et al., 2020; Xie\net al., 2021; Gao et al., 2022). NeRFs were originally introduced to handle the task of multi-view\nreconstruction, but have since been applied in a plethora of 3D-based tasks, such as photo-editing,\n3D surface extraction, and large/city-scale 3D representation (Gao et al., 2022).\nThere have been many improvements on the original NeRF paper, especially concerning training\nspeed and fidelity (Chen et al., 2022a;b; M\u00a8uller et al., 2022; Sun et al., 2021; Yu et al., 2021a).\nHyperFields uses the multi-resolution hash grid introduced in InstantNGP (M\u00a8uller et al., 2022).\n2.2\nSCORE-BASED 3D GENERATION\nWhile many works attempt to directly learn the distribution of 3D models via 3D data, others\nopt to use guidance from 2D images due to the significant difference in data availability. Such\napproaches replace the photometric loss in NeRF\u2019s original view synthesis task with a guidance\nloss. The most common forms of guidance in the literature are from CLIP (Radford et al., 2021)\nor a frozen, text-conditioned 2D diffusion model. The former methods seek to minimize the cosine\ndistance between the image embeddings of the NeRF\u2019s renderings and the text embedding of the\nuser-provided text prompt (Jain et al., 2022; Chen et al., 2022a; Jain et al., 2021).\nNoteworthy 2D diffusion-guided models include DreamFusion (Poole et al., 2022) and Score\nJacobian Chaining (SJC) (Wang et al., 2022), which feed noised versions of images rendered from\na predicted NeRF into a frozen text-to-image diffusion model (Imagen (Saharia et al., 2022) and\nStableDiffusion Rombach et al. (2021), respectively) to obtain what can be understood as a scaled\nStein Score (Liu et al., 2016). Our work falls into this camp, as we rely on score-based gradients\nderived from StableDiffusion to train the NeRF models which guide our hypernetwork training.\nSpecifically, we use the following gradient motivated in DreamFusion:\n\u2207\u03b8L(\u03d5, g(\u03b8)) \u225c Et,c\n\u0002\nw(t)(\u02c6\u03f5\u03d5(zt; y, t) \u2212 \u03f5) \u2202x\n\u2202\u03b8 )\n\u0003\n(1)\nwhich is similar to the gradient introduced in SJC, the key difference being SJC directly predicts\nthe noise score whereas DreamFusion predicts its residuals. We refer to optimization using this\ngradient as Score Distillation Sampling (SDS), following the DreamFusion authors. More recently\nfollowing works are directed at improving 3D generation quality (Wang et al., 2023; Metzer et al.,\n2023; Chen et al., 2023), while our focus is on an orthogonal problem of improving generalization\nand convergence of text to 3D models.\nConnections to ATT3D: We note that our work is concurrent and independent of ATT3D (Lorraine\net al., 2023). We are similar in that we both train a hypernetwork to generate NeRF weights for a\nset of scenes during training and generalize to novel in-distribution scenes without any test time\noptimization.\nHowever also demonstrate accelerated convergence to novel out-of-distribution\nscenes, requiring only a few optimization steps. We clarify on our use of terms in-distribution and\nout-of-distribution in Sec. 4.1 and 4.2 respectively.\nWe primarily differ in the input and the application of the hypernetwork.\nOur hypernetwork\ngenerates the MLP weights of the NeRF, while ATT3D outputs the weights of the hash grid in\ntheir InstantNGP model. We condition our hypernetwork on the text prompt and activations of the\ngenerated NeRF MLP (Sec. 3), while ATT3D is conditioned on just the text prompt.\nFinally, ATT3D is built on Magic3D (Lin et al., 2022) which is a proprietary and more powerful\ntext-to-3D model than the publicly available stable DreamFusion model (Tang, 2022) that we use.\nIn contrast we plan to open-source our code and trained model.\n2.3\nHYPERNETWORKS\nHypernetworks are networks that are used to generate weights of other networks which perform the\nactual task (task performing network) (Ha et al., 2016a). Many works attempt to use hypernetworks\nas a means to improve upon conditioning techniques. Among these, some works have explored\napplying hypernetworks to implicit 2d representations (Sitzmann et al., 2020; Perez et al., 2017;\nAlaluf et al., 2021), and 3D representations (Sitzmann et al., 2019; 2021; Chiang et al., 2021), not\nas much effort is directed towards using hypernetworks to parameterize radiance fields for the task\n3\nFigure 3: The input to the HyperFields system is a text prompt, which is encoded by a pre-trained\ntext encoder (frozen BERT model). The text latents are passed to a Transformer module, which\noutputs a conditioning token (CT). This conditioning token (which supplies scene information) is\nused to condition each of the MLP modules in the hypernetwork. The first hypernetwork MLP (on\nthe left) predicts the weights W1 of the first layer of the NeRF MLP. The second hypernetwork MLP\nthen takes as input both the CT and a1, which are the activations from the first predicted NeRF\nMLP layer, and predicts the weights W2 of the second layer of the NeRF MLP. The subsequent\nscene-conditioned hypernetwork MLPs follow the same pattern, taking the activations ai\u22121 from\nthe previous predicted NeRF MLP layer as input to generate weights Wi for the ith layer of the\nNeRF MLP. We include stop gradients (SG) so stabilize training.\nof generalization, which is the goal of this paper.\nAn interesting class of hypernetworks involve models conditioned on the activations or inputs of the\ntask-performing network (Chen et al., 2020). These models take the following form: let h, g be the\nhypernetwork and the task performing network respectively. Then W = h(a), where W acts as\nthe weights of g and a is the activation from the previous layer of g or the input to the first layer\nof g. These are called dynamic hypernetworks as the predicted weights change dynamically with\nrespect to the layer-wise signals in g. In the static hypernetwork case, W = h(e) where W is still the\nweights of g but e is learned and is independent of the input to g. Our work explores the application\nof dynamic hypernetworks to learning a general map between text and NeRFs.\n3\nMETHOD\nOur method consists of two key innovations, the dynamic hypernetwork architecture and NeRF\ndistillation training. We discuss each of these two components in detail below.\n3.1\nDYNAMIC HYPERNETWORK\nThe dynamic hypernetwork consists of the Transformer T and MLP modules as given in figure\n3.\nThe sole input to the dynamic hypernetwork is the scene information represented as a text\ndescription. The text is then encoded by a frozen pretrained BERT model, and the text embedding\nz is processed by T . Let conditioning token CT = T (z) be the intermediate representation used to\nprovide the current scene information to the MLP modules. Note that the text embeddings z can\ncome from any text encoder, though in our experiments we found frozen BERT embeddings to be\nthe most performant.\nIn addition to conditioning token CT, each MLP module takes in the activations from the previous\nlayer ai\u22121 as input. Given these two inputs, the MLP module is tasked with generating parameters\nWi for the ith layer of the NeRF MLP. For simplicity let us assume that we sample only one 3D\ncoordinate and viewing direction per minibatch, and let h be the hidden dimension of the NeRF\n4\nMLP. Then ai\u22121 \u2208 R1\u00d7h. Now the weights Wi \u2208 Rh\u00d7h of the ith layer are given as follows:\nWi = MLPi(CT, ai\u22121)\n(2)\nThe forward pass of the ith layer is:\nai = Wi \u2217 ai\u22121\n(3)\nwhere ai \u2208 R1\u00d7h and * is matrix multiplication. This enables the hypernetwork MLPs to generate\na different set of weights for the NeRF MLP that are best suited for each given input 3D point\nand viewing direction pair. This results in effectively a unique NeRF MLP for each 3D point and\nviewing direction pair.\nHowever training with minibatch size 1 is impractical, so during training we sample a non-trivial\nminibatch size and generate weights that are best suited for the given minibatch as opposed to the\nabove setting where we generate weights unique to each 3D coordinate and viewing direction pair.\nIn order to generate a unique set of weights for a given minibatch we do the following:\nai\u22121 =\n\u00b5(ai\u22121)\n(4)\nWi =\nMLPi(CT, ai\u22121)\n(5)\nWhere \u00b5(.) averages over the minibatch index. So if the minibatch size is n, then ai\u22121 \u2208 Rn\u00d7h, and\nai\u22121 \u2208 R1\u00d7h and the forward pass is still computed as given in equation 3. This adaptive nature of\nthe predicted NeRF MLP weights leads to the increased flexibility of the model. As shown in our\nablation experiments in Fig. 6a, it is an essential piece to our model\u2019s large scene capacity.\n3.2\nNERF DISTILLATION\nAs shown in figure 2, we first train individual DreamFusion NeRFs on a set of text prompts,\nfollowing which we train the HyperFields architecture with supervision from these single-scene\nDreamFusion NeRFs.\nThe training routine is outlined in Algorithm 1, in which at each iteration, we sample n prompts and\na camera viewpoint for each of these text prompts (lines 2 to 4). Subsequently, for the ith prompt\nand camera viewpoint pair we render image Ii using the ith pre-trained teacher NeRF (line 5). We\nthen condition the HyperFields network \u03d5hf with the ith prompt, and render the image I\n\u2032\ni from\nthe ith camera view point (line 6). We use the image rendered by the pre-trained teacher NeRF as\nthe ground truth supervision to HyperFields (line 7). For the same sampled n prompts and camera\nviewpoint pairs, let I\n\u2032\n1 to I\n\u2032\nn be the images rendered by HyperFields and I\n\u2032\n1 to I\n\u2032\nn be the images\nrendered by the respective pre-trained teacher NeRFs. The distillation loss is given as follows:\nLd =\nPn\ni=1(Ii \u2212 I\n\u2032\ni)2\nn\n(6)\nWe observe through our ablations in Fig. 6b that this simple distillation scheme greatly helps Hy-\nperFields in learning to fit multiple text prompts simultaneously, as well as learn a more general\nmapping of text to NeRFs.\n5\nAlgorithm 1: Training HyperFields with NeRF\nDistillation\nRequire: T = {T1, T2, \u00b7 \u00b7 \u00b7 TN}\n\u25b7 Set of text\nprompts\nRequire: C\n\u25b7 Set of Camera view points\nRequire: \u03b81, \u03b82, \u00b7 \u00b7 \u00b7 \u03b8N\n\u25b7 pre-trained NeRFs\nRequire: \u03d5HF\n\u25b7 Randomly initialized HyperFields\nRequire: R\n\u25b7 Differentiable renderer function\n1: for each step do\n2:\nTl, Tm,Tn \u223c T\n\u25b7 Sample text prompts from T\n3:\nfor Ti \u2208 {Tl, Tm,Tn} do\n4:\nCi \u223c C\n5:\nIi = R(\u03b8i(Ci))\n\u25b7 ith nerf renders image for\ngiven camera Ci\n6:\nIi\n\u2032 = R(\u03d5HF (Ti, Ci))\n\u25b7 Condition \u03d5HF on\nith prompt\n7:\nLi = (Ii \u2212 Ii\n\u2032)2\n8:\nend for\n9:\nLd =\nP\ni\u2208{l,m,n}\nLi\n10: end for\n3.3\nIMPLEMENTATION DETAILS\nWe use the multiresolution hash grid developed\nin InstantNGP M\u00a8uller et al. (2022) for its fast\ninference with low memory overhead, and sinu-\nsoidal encodings \u03b3 to combat the known spec-\ntral bias of neural networks (Rahaman et al.,\n2018).\nThe NeRF MLP has 6 layers (with\nweights predicted by the dynamic hypernet-\nwork), with skip connections every two lay-\ners. The dynamic hypernetwork MLP modules\nare two-layer MLPs with ReLU non-linearities\nand the Transformer module has 6 self-attention\nlayers. Furthermore, we perform adaptive in-\nstance normalization before passing the activa-\ntions into the MLP modules of the dynamic hy-\npernetwork and also put a stop gradient opera-\ntor on the activations being passed into the MLP\nmodules (as in figure 3). The exact dimensions\nof the various components of the architecture\nare described in the appendix.\n4\nRESULTS\nWe evaluate HyperFields by demonstrating its generalization capabilities, out-of-distribution con-\nvergence, amortization benefits, and ablation experiments. In Sec. 4.1 and Sec. 4.2 we evaluate the\nmodel\u2019s ability to synthesize novel scenes, both in and out-of-distribution. We quantify the amorti-\nzation benefits of having this general model compared to optimizing individual NeRFs in Sec. 4.3.\nFinally, our ablations in Sec. 4.4 justify our design choices of dynamic conditioning and NeRF\ndistillation training.\n4.1\nIN-DISTRIBUTION GENERALIZATION\nOur method is able to train on a subset of the colour-shape combinations, and during inference\npredict the unseen colour-shape scenes zero-shot, without any test time optimization. Fig. 4 shows\nthe results of training on a subset of combinations of 9 shapes and 8 colours, while holding out 3\ncolours for each shape. Our model generates NeRFs in a zero-shot manner for the held-out prompts\n(opaque scenes in Fig. 4) with quality nearly identical to the trained scenes.\nWe call this in-distribution generalization as both the shape and the color are seen during training\nbut the inference scenes (opaque scenes in Fig.4) are novel because the combination of color and\nshape is unseen during training. Example: \u201cOrange toaster\u201d is a prompt the model has not seen\nduring training, though it has seen the color \u201corange\u201d and the shape \u201ctoaster\u201d in its training set.\nWe quantitatively evaluate the quality of our zero-shot\npredictions with CLIP retrieval scores. The support\nset for the retrieval consists of all 72 scenes (27 unseen\nand 45 seen) shown in Fig. 4. In Table 1 we compute\nthe top-k retrieval scores by CLIP similarity. The ta-\nble reports the average scores for Top-1, 3, 5, 6, and\n10 retrieval, separated by unseen (zero-shot) and seen\nprompts. The similarity in scores between the unseen\nand seen prompts demonstrates that our model\u2019s zero-\nshot predictions are of similar quality to the training\nscenes with respect to CLIP similarity.\nTop-1 Top-3 Top-5 Top-6 Top-10\nUnseen 57.1\n85.7\n85.7\n90.4\n95.2\nSeen\n69.5\n88.1\n94.9\n94.9\n96.6\nTable 1: CLIP Retrieval Scores: We report\nthe average retrieval scores for the scenes\nshown in Fig. 4. The small difference in\nscores between the seen and unseen scene\nprompts indicates that our zero-shot gener-\nations are of similar quality to the training\nscenes.\n4.2\nACCELERATED OUT-OF-DISTRIBUTION CONVERGENCE\nWe further test HyperFields\u2019s ability to generate shapes and attributes that it has not seen during\ntraining. We call this out-of-distribution inference because the specified geometry and/or attribute\nare not within the model\u2019s training set.\n6\nFigure 4: Zero-Shot In-Distribution Generalization.. During training, the model observes every\nindividual shape and color, but we hold out a subset of color/shape combinations. During inference,\nthe model generalizes by generating scenes for the held out combinations zero-shot. For example,\n\u201cred chair\u201d is an unseen combination, but the model is able to generalize from individual instances\nof \u201cred\u201d and \u201cchair\u201d from training. The faded scenes are generated from the training set, while the\nbright scenes are zero-shot predictions of the held-out prompts.\nWe train our model on a rich source of prompts, across multiple semantic dimensions (material,\nappearance, shape). The list of prompts used is provided in the appendix material section D using\nNeRF distillation loss. Post training, we test our model on the prompts in Fig. 5. The prompts\nare grouped based on whether both shape and attribute are unseen (column 1, Fig. 5) or just the\nshape is unseen (column 2, Fig. 5). For example, in \u201cgold blender\u201d both material \u201cgold\u201d and shape\n\u201cblender\u201d are unseen during training.\nSince these prompts contain geometry/attributes that are unseen during training, we do not expect\nhigh quality generation without any optimization. Instead, we demonstrate that fine-tuning the\ntrained HyperFields model on SDS loss for the given the out-of-distribution prompt can lead to\naccelerated convergence especially when compared to the DreamFusion baselines.\nWe consider two baselines, 1) Stable Dreamfusion (S): Publicly available implementation of\nDreamfusion trained from Scratch, 2) Stable Dreamfusion (P): Stable Dreamfusion model\n7\nFigure 5: Finetuning to out-of-distribution prompts: unseen shape and or unseen attribute.\nOur method generates out-of-distribution scenes in at most 2k finetuning steps (row 1), whereas the\nbaseline models are far from the desired scene at the same number of iterations (rows 2 and 3).\nWhen allowed to fine-tune for significantly longer (rows 4 and 5) the baseline generations are at best\ncomparable to our model\u2019s generation quality, demonstrating that our model is able to adapt better\nto out-of-distribution scenes.\nPre-trained on a semantically close scene and finetuned to the target scene. The motivation in using\nStable Dreamfusion (P) is to have a pre-trained model as a point of comparison against HyperFields\nmodel.\nWe show out-of-distribution generation results for 8 different scenes in Fig. 5. The inset images\nin the upper left of row 1 of Fig. 5 are the scenes generated zero-shot by our method, with no\noptimization, when provided with the out-of-distribution prompt. The model chooses the semantic\nnearest neighbour from its training data as the initial guess for out-of-distribution prompts. For\nexample, when asked for a \u201cgolden blender\u201d and \u201cglacier knife\u201d, our model generates a scene with\n\u201ctiger striped toaster\u201d, which is the only related kitchenware appliance in the model sees during\ntraining. We pretrain the Stable Dreamfusion(P) baselines to the same scenes predicted by our\nmodel zero-shot. The pretrained scenes for Stable Dreamfusion(P) are given as insets in the upper\nleft of row 3 and 5 in Fig. 5.\nBy finetuning on a small number of epochs for each out-of-distribution target scene using score\ndistillation sampling, our method can converge much faster to the target scene than the baseline\nDreamFusion models. In row 2 and 3 of Fig. 5, we see that both Dreamfusion(S) and (P), barely\nlearn the target shape for the same amount of training budget as our method. In rows 4 and 5 of\nFig. 5 we let the baselines train to convergence, despite which the quality of the longer trained\nbaseline scenes are worse or at best comparable to our model\u2019s generation quality. On average we\nsee a 5x speedup in convergence. Additionally in Sec. E of the appendix we have a user study\nfavourably comparing our generation to that of the baselines.\nImportantly, DreamFusion(P) which is pre-trained to the same zero-shot predictions of our model\nis unable to be fine-tuned to the target scene as efficiently and at times get stuck in suboptimal local\n8\n(a) Dynamic Hypernet Packing. Without dynamic condi-\ntioning (\u201cStatic Hypernet\u201d), the hypernetwork packing ability\nis highly limited. We show 4 scenes packed using SDS, and\nthe static hypernet collapses the origami/glacier attributes\nand stained glass/plaid attributes.\n(b) NeRF Distillation. We compare our pack-\ning results when training the model from Fig. 4\nwith score distillation (\u201cNo NeRF Distillation\u201d)\nversus our NeRF distillation method (\u201cOurs\u201d).\nThe iterative optimization of score distillation\ncauses similar objects such as pot and vase to\nbe guided towards the same common geometry.\nminima close to the initialization (see \u201cyarn skateboard\u201d row 3 and 5 in Fig. 5). This demonstrates\nthat HyperFields learns a semantically meaningful mapping from text to NeRFs that cannot be ar-\nbitrarily achieved through neural optimization. We further explore the smoothness of this mapping\nthrough interpolation experiments in Sec. F of the appendix.\n4.3\nAMORTIZATION BENEFITS\nThe cost of pre-training HyperFields and individual teacher NeRFs is easily amortized in both\nin-distribution and out-of-distribution prompts. Training the teacher NeRFs is not an additional\noverhead; it\u2019s the cost of training a DreamFusion model on each of those prompts.\nThe only\noverhead incurred by our method is the NeRF distillation training in stage 2 (Fig. 2), which takes\nroughly two hours. This overhead is offset by our ability to generate unseen combinations in a\nfeedforward manner.\nFor comparison, the DreamFusion baseline takes approximately 30 minutes to generate each test\nscene in Fig. 4, totaling \u223c14 hours for all 27 test scenes. In contrast, after the 2 hour distillation\nperiod, our model can generate all 27 test scenes in less than a minute, making it an order of\nmagnitude faster than DreamFusion, even with the distillation overhead.\nOur method\u2019s ability to converge faster to new out-of-distribution prompts leads to linear time-saving\nfor each new prompt. This implies a practical use case of our model for rapid out-of-distribution\nscene generation in a real world setting. As shown in Fig. 5, the baseline\u2019s quality only begins to\nmatch ours after 3-5x the amount of training time.\n4.4\nABLATIONS\nThe main contribution in our Dynamic Hypernetwork architecture is that the weights of the ith layer\nof the NeRF are generated as not only as a function of prompt but also as a function of activations\nfrom the (i \u2212 1)th layer. We show that using the activations from the previous layer in generating\nsubsequent weights is crucial. Without it our model\u2019s ability to pack multiple scenes is heavily\nreduced. In Fig. 6a row 2 (\u201cWithout Dynamic Hypernetwork\u201d), shows that even in the simple case\nof 4 scenes the version of the hypernetwork which does not use previous activations for predicting\nthe NeRF weights collapses the \u201cglacier\u201d and \u201corigami\u201d styles together and the \u201cplaid\u201d and \u201cstained\nglass\u201d styles together.\nIf we attempt to pack the dynamic hypernetwork using just Score Distillation Sampling (SDS) from\nDreamFusion, we experience a type of mode collapse in which the SDS optimization guides similar\nshapes towards the same common geometry. This also hinders the expressiveness of the hypernet-\nwork and its ability to generate fine-grained, distinct geometry across different scenes. See Fig. 6b\nfor an example of this mode collapse when attempting to train HyperFields with just SDS (no NeRF\nDistillation) over the set of scenes shown in Fig. 4.\n9\n5\nCONCLUSION\nWe present HyperFields, a novel framework for generalized text-to-NeRF synthesis, which can pro-\nduce individual NeRF networks in a single feedforward pass. Our results highlight a promising step\nin learning a general representation of semantic scenes. Our novel dynamic hypernetwork architec-\nture coupled with NeRF distillation learns an efficient mapping of text token inputs into a smooth\nand semantically meaningful NeRF latent space. Our experiments show that with this architecture\nwe are able to fit over 100 different scenes in one model, and predict high quality unseen NeRFs\neither zero-shot or with a few finetuning steps. Comparing to existing work, our ability to train on\nmultiple scenes greatly accelerates convergence of novel scenes. We plan on publishing our code\nand trained model shortly with an ArXiv release. In future work we would like to explore the possi-\nbility of generalizing the training and architecture to achieving zero-shot open vocabulary synthesis\nof NeRFs and other implicit 3D representations.\nREFERENCES\nYuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit H. Bermano. Hyperstyle: Stylegan\ninversion with hypernetworks for real image editing, 2021. URL https://arxiv.org/abs/\n2111.15666. 3\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance\nfields, 2022a. URL https://arxiv.org/abs/2203.09517. 3\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance\nfields, 2022b. URL https://arxiv.org/abs/2203.09517. 3\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and\nappearance for high-quality text-to-3d content creation, 2023. 3\nYinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic\nconvolution: Attention over convolution kernels. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 11030\u201311039, 2020. 4\nPei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-sheng Lai, and Wei-Chen Chiu. Stylizing\n3d scene via implicit representation and hypernetwork, 2021. URL https://arxiv.org/\nabs/2105.13016. 3\nStefano Esposito, Daniele Baieri, Stefan Zellmann, Andr\u00b4e Hinkenjann, and Emanuele Rodol`a. Kilo-\nneus: A versatile neural implicit surface representation for real-time rendering, 2022.\nURL\nhttps://arxiv.org/abs/2206.10885. 2\nKyle Gao, Yina Gao, Hongjie He, Dening Lu, Linlin Xu, and Jonathan Li. Nerf: Neural radiance\nfield in 3d vision, a comprehensive review, 2022. URL https://arxiv.org/abs/2210.\n00379. 3\nDavid Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016a.\n3\nDavid Ha, Andrew Dai, and Quoc V. Le. Hypernetworks, 2016b. URL https://arxiv.org/\nabs/1609.09106. 1\nFangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip:\nZero-shot text-driven generation and animation of 3d avatars. ACM Transactions on Graphics\n(TOG), 41(4):1\u201319, 2022. 2\nAjay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent few-\nshot view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), pp. 5885\u20135894, October 2021. 1, 3\nAjay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\nobject generation with dream fields. 2022. 1, 3\n10\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. arXiv preprint arXiv:2211.10440, 2022. 1, 3\nQiang Liu, Jason D. Lee, and Michael I. Jordan. A kernelized stein discrepancy for goodness-of-fit\ntests and model evaluation, 2016. URL https://arxiv.org/abs/1602.03253. 3\nJonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp,\nTsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized text-to-3d object\nsynthesis. arXiv, 2023. 3\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for\nshape-guided generation of 3d shapes and textures, 2022. URL https://arxiv.org/abs/\n2211.07600. 2\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for\nshape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 12663\u201312673, 2023. 3\nOscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven\nneural stylization for meshes. arXiv preprint arXiv:2112.03221, 2021. 2\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis, 2020. URL\nhttps://arxiv.org/abs/2003.08934. 1, 3\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\nitives with a multiresolution hash encoding.\nACM Trans. Graph., 41(4):102:1\u2013102:15, July\n2022.\ndoi: 10.1145/3528223.3530127.\nURL https://doi.org/10.1145/3528223.\n3530127. 3, 6\nAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system\nfor generating 3d point clouds from complex prompts, 2022. URL https://arxiv.org/\nabs/2212.08751. 2\nEthan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual\nreasoning with a general conditioning layer, 2017. URL https://arxiv.org/abs/1709.\n07871. 3\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion, 2022. URL https://arxiv.org/abs/2209.14988. 1, 2, 3\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision, 2021. URL\nhttps://arxiv.org/abs/2103.00020. 3\nNasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A. Hamprecht,\nYoshua Bengio, and Aaron Courville.\nOn the spectral bias of neural networks.\n2018.\ndoi:\n10.48550/ARXIV.1806.08734. URL https://arxiv.org/abs/1806.08734. 6\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation, 2021. URL https://arxiv.org/\nabs/2102.12092. 1\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models, 2021. 3\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam-\nyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Sal-\nimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image dif-\nfusion models with deep language understanding, 2022. URL https://arxiv.org/abs/\n2205.11487. 3\n11\nAditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, and Marco Fumero.\nClip-forge: Towards zero-shot text-to-shape generation. arXiv preprint arXiv:2110.02624, 2021.\n2\nAditya Sanghi, Rao Fu, Vivian Liu, Karl Willis, Hooman Shayani, Amir Hosein Khasahmadi, Sri-\nnath Sridhar, and Daniel Ritchie. Textcraft: Zero-shot generation of high-fidelity and diverse\nshapes from text, 2022. URL https://arxiv.org/abs/2211.01427. 2\nVincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wetzstein. Scene representation networks: Con-\ntinuous 3d-structure-aware neural scene representations, 2019. URL https://arxiv.org/\nabs/1906.01618. 3\nVincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wet-\nzstein. Implicit neural representations with periodic activation functions. In Proc. NeurIPS, 2020.\n3\nVincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, and Fredo Du-\nrand. Light field networks: Neural scene representations with single-evaluation rendering, 2021.\nURL https://arxiv.org/abs/2106.02634. 3\nCheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast con-\nvergence for radiance fields reconstruction, 2021. URL https://arxiv.org/abs/2111.\n11215. 3\nJiaxiang\nTang.\nStable-dreamfusion:\nText-to-3d\nwith\nstable-diffusion,\n2022.\nhttps://github.com/ashawkey/stable-dreamfusion. 3, 13\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian\nchaining: Lifting pretrained 2d diffusion models for 3d generation, 2022.\nURL https://\narxiv.org/abs/2212.00774. 2, 3\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus:\nLearning neural implicit surfaces by volume rendering for multi-view reconstruction, 2021. URL\nhttps://arxiv.org/abs/2106.10689. 2\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-\ndreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv\npreprint arXiv:2305.16213, 2023. 3\nYiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico\nTombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual comput-\ning and beyond, 2021. URL https://arxiv.org/abs/2111.11426. 3\nLior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces,\n2021. URL https://arxiv.org/abs/2106.12052. 2\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-\ntime rendering of neural radiance fields, 2021a. URL https://arxiv.org/abs/2103.\n14024. 3\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields\nfrom one or few images. In CVPR, 2021b. 1\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin\nLi, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation, 2022. URL https://arxiv.org/abs/2206.10789. 1\nXiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten\nKreis. Lion: Latent point diffusion models for 3d shape generation, 2022. URL https://\narxiv.org/abs/2210.06978. 2\nLinqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel\ndiffusion, 2021. URL https://arxiv.org/abs/2104.03670. 2\n12\nA\nARCHITECTURE DETAILS\nBaselines: Our baseline is 6 layer MLP with skip connections every two layers. The hidden di-\nmension is 64. We use an open-source re-implementation Tang (2022) of DreamFusion as both our\nbaseline model and architecture predicted by HyperFields, because the original DreamFusion works\nrelies on Google\u2019s Imagen model which is not open-source. Unlike the original DreamFusion, the\nre-implementation uses Stable Diffusion (instead of Imagen). We use Adam with a learning rate of\n1e-4, with an epoch defined by 100 gradient descent steps.\nHyperFields: The architecture is as described in Figure 2 in the main paper. The dynamic hyper-\nnetwork generates weights for a 6 layer MLP of hidden dimension 64. The transformer portion of\nthe hypernetwork has 6 self-attention blocks, each with 12 heads with a head dimension of 16. We\ncondition our model with BERT tokens, though we experiment with T5 and CLIP embeddings as\nwell with similar but marginally worse success. Similar to the baseline we use Stable Diffusion for\nguidance, and optimize our model using Adam with the a learning rate of 1e-4. We will release\nopen-source code of our project in a future revision of the paper.\nB\nPACKING\nFigure 7: Prompt Packing. Our dynamic hypernetwork is able to pack 9 different objects across 12\ndifferent prompts for a total of 108 scenes. Dynamic hypetnetwork coupled with NeRF distillation\nenables packing these scenes into one network.\n13\nFigure 8: Fine-Tuning In-Distribution: seen shape, seen attribute, unseen combination. During\ntraining, the model observes every shape and color, but some combinations of shape and attribute\nremain unseen. During inference, the model generalizes by generating scenes that match prompts\nwith previously unseen combinations of shape and attribute, with small amount of finetuning (atmost\n1k steps).\nC\nIN-DISTRIBUTION GENERALIZATION WITH COMPLEX PROMPTS\nFor additional attributes (\u201cplaid\u201d, \u201cTerracotta\u201d etc.), our model produces reasonable zero-shot pre-\ndictions, and after fewer than 1000 steps of finetuning with SDS is able to produce unseen scenes\nof high quality. We show these results in Fig. 8 with 8 objects and 4 styles, where 8 shape-style\ncombinational scenes are masked out during training (opaque scenes in Fig. 8).\nD\nOUT-OF-DISTRIBUTION CONVERGENCE\nIn Fig 5 we show the inference time prompts and the corresponding results. Here we provide the\nlist of prompts used to train the model: \u201cStained glass chair\u201d, \u201cTerracotta chair\u201d, \u201cTiger stripes\nchair\u201d, \u201cPlaid toaster\u201d, \u201cTerracotta toaster\u201d, \u201cTiger stripes toaster\u201d, \u201cPlaid bowl\u201d, \u201cTerracotta bowl\u201d,\n\u201cTiger stripes bowl\u201d, \u201cStained glass couch\u201d, \u201cPlaid couch\u201d, \u201cTiger stripes couch\u201d, \u201cStained glass\npot\u201d, \u201cTerracotta pot\u201d, \u201cTiger stripes pot\u201d, \u201cStained glass vase\u201d, \u201cPlaid vase\u201d, \u201cTiger stripes vase\u201d,\n\u201cStained glass table\u201d, \u201cPlaid table\u201d, \u201cTerracotta table\u201d.\nSince the training prompts dont contain shapes such as \u201cBlender\u201d, \u201cKnife\u201d, \u201cSkateboard\u201d, \u201cShoes\u201d,\n\u201cStrawberry\u201d, \u201cLamppost\u201d, \u201cTeapot\u201d and attributes such as \u201cGold\u201d, \u201cGlacier\u201d, \u201cYarn\u201d, \u201cVictorian\u201d,\nwe term the prompts used in Fig 5 as out-of-distribution prompts\u2013as the model does not see these\nshapes and attributes during training.\nE\nUSER STUDY\nIn order to get a quantitative evaluation of our generation quality for the out-of-distribution prompts\nwe conduct a human study where we ask participants to rank the render that best adheres to the\ngiven prompt in descending order (best render is ranked 1). We compare our method\u2019s generation\nwith 33 different DreamFusion models. 1 is trained from scratch and the other 32 are finetuned from\ncheckpoints corresponding to the prompts in section D. Of these 33 models we pick the best model\nfor each of the out-of-distribution prompts, so the computational budget to find the best baseline for\na given prompt is almost 33x that our of method. Note each of these models, including ours, are\ntrained for the same number of steps. We report average user-reported rank for our method and the\naverage best baseline chosen for each prompt in Tab. 2. We outrank the best DreamFusion baseline\nconsistently across all our out-of-distribution prompts.\nF\nBERT TOKEN INTERPOLATION\nAnother option for interpolation is to interpolate the input BERT embeddings fed in the our Dy-\nnamic HyperNet. We show results in Fig. 9 where we interpolate across two chair colors in a\nDynamic HyperNet trained on only chair colors. The interpolation is highly non-smooth, with a\nsingle intermediate color shown at \u03b4 = 0.4 and discontinuities on either end at \u03b4 \u2212 0.3 and \u03b4 = 0.5.\nOn the other hand, our HyperNet token interpolation shown in Figure 10 demonstrates a smooth\n14\nModel\nGolden\nBlender\nYarn\nShoes\nYarn\nSkateboard\nPlaid\nSkateboard\nPlaid\nLamppost\nStrawberry\nwith tiger stripes\nOur Method (\u2193)\n1.30\n1.07\n1.32\n1.29\n1.36\n1.12\nBest DreamFusion Baseline (\u2193)\n2.50\n2.40\n2.33\n1.75\n2.0\n2.25\nTable 2: Average User-Reported Ranks (N=450): We report the average rank submitted by all\nusers for our method, and compute the average rank for all 33 of the baselines. We report the\naverage rank of the best performing baseline for each prompt. Our method is consistently preferred\nover the best baseline, despite the best baseline consuming 33x more computational resources than\nour method to find.\nFigure 9: BERT Token Interpolation. We show results of interpolating the BERT tokens corre-\nsponding to the prompts \u201cyellow chair\u201d and \u201cpurple chair\u201d. In contrast, interpolation on the level of\nthe hypernetwork (\u201cHyperNet\u201d) is smoother than interpolating the BERT tokens.\nand gradual transition of colors across the interpolation range. This demonstrates that our HyperNet\nlearns a smoother latent space of NeRFs than the original BERT tokens correspond to.\n15\n"
  },
  {
    "title": "Controlled Decoding from Language Models",
    "link": "https://arxiv.org/pdf/2310.17022.pdf",
    "upvote": "12",
    "text": "Preprint\nCONTROLLED DECODING FROM LANGUAGE MODELS\nSidharth Mudgal\u2217d\nJong Lee\u2217r\nHarish Ganapathyd\nYaGuang Lid\nTao Wangd\nYanping Huangd\nZhifeng Chend\nHeng-Tze Chengd\nMichael Collinsd\nTrevor Strohmand\nJilin Chenr\nAlex Beutel\u2020o\nAhmad Beiramir\ndGoogle DeepMind\nrGoogle Research\noOpenAI\n{sidharthms, leejong, beirami}@google.com\nABSTRACT\nKL-regularized reinforcement learning (RL) is a popular alignment framework to\ncontrol the language model responses towards high reward outcomes. We propose\na modular solver for this RL objective, called controlled decoding (CD), which\nexerts control through a separate prefix scorer module. At training time, the prefix\nscorer learns a value function for the reward, and it is used at inference time to\ncontrol the generation from a frozen base model, provably sampling from a so-\nlution to the RL objective. We empirically demonstrate that CD is effective as\na control mechanism on popular benchmarks. We also show that a single prefix\nscorer can learn multiple rewards and different reward combinations can be con-\nfigurable at inference time, effectively solving a multi-objective RL problem with\nno additional training. We show that the benefits of applying CD transfer to an un-\nseen base model with no further tuning. Finally, we show that CD can be applied\nin a blockwise decoding fashion at inference-time, essentially bridging the gap\nbetween the popular best-of-n strategy and token-level control through reinforce-\nment learning. This makes CD a promising approach for alignment of language\nmodels.\n1\nINTRODUCTION\nGenerative language models have reached a level where they can effectively solve a variety of open-\ndomain tasks with little task specific supervision. However, the generated content from these models\nmay still not satisfy the preference of a human user. The goal of the alignment process is to remedy\nthis issue by generating content from an aligned model that improves a reward (e.g., make the\ngeneration more safe) but does not perturb much from the base model. In a world where different\nhumans may have different preferences, it is crucial to ask: how can we align machine generated\ncontent to rewards while keeping the pre-trained representations in a generative language model\nfrozen?\nControlling language model responses towards high reward outcomes is an area of active research in\nthe literature. We divide the existing alignment methods into two categories that differ significantly\nin real-world deployment: generator improvement and inference-time add-on solutions.\nGenerator improvement solutions, such as KL-regularized PPO (Christiano et al., 2017; Ouyang\net al., 2022), direct preference optimization (DPO) (Rafailov et al., 2023), sequence likelihood cal-\nibration (SliC) (Zhao et al., 2022), and identity preference optimization (IPO) Azar et al. (2023)\nupdate the weights of the language model to align it with a reward model. They are efficient for\ninference but offer little configurability.\nA simple and effective inference-time add-on solution is best-of-K (Nakano et al., 2021; Stiennon\net al., 2020; Touvron et al., 2023), where K i.i.d. samples are drawn from a base model, ranked\n\u2217Equal contribution.\n\u2020Work done at Google Research.\n1\narXiv:2310.17022v2  [cs.LG]  13 Feb 2024\nPreprint\nbased on a reward, and the highest ranking one is selected. Other methods, such as FUDGE (Yang\n& Klein, 2021) or COLD (Qin et al., 2022), offer a prefix scorer that is used at inference-time to\ncontrol a frozen base model response towards high-reward outcomes. Due to their modularity of\ndesign which leaves the base model frozen, these methods offer inference-time configurability. Our\ngoal is to propose a learning framework for such methods.\nOur contributions are summarized below.\n\u2022 We formalize a modular alignment method, controlled decoding (CD), to solve a KL-regularized\nRL objective. CD learns a prefix scorer for the reward that is used to steer the generation from a\npartially decoded path.\n\u2022 We demonstrate that two variants of CD, namely CD-FUDGE (Yang & Klein, 2021) and CD-Q\n(our proposed method), provably lead to sampling from a solution to the RL objecive.\n\u2022 We propose blockwise CD where the prefix scorer is used to select the best-of-K paths for a\ndecoded block of M tokens. This bridges the gap between the sequence-level best-of-K and\ntoken-wise RL methods.\n\u2022 We empirically show that CD offers significant improvement over existing controlled genera-\ntion/decoding solutions on popular benchmarks.\n\u2022 We showcase the modularity of CD at inference-time to integrate multiple rewards into a single\nprefix scoring rule, and applying the prefix scorer to an unseen base model.\n2\nKL-REGULARIZED REINFORCEMENT LEARNING\nLet x be a prompt (consisting of several tokens) and let y = yT := [y1, . . . , yT ] represent a response\nthat is a concatenation of T tokens. Here each token yt \u2208 Y, where Y represents the alphabet\n(vocabulary). Let p denote a pre-trained language model (LM) that is used to draw samples in\nan autoregressive manner. In particular, we use p(\u00b7|[x, yt]) to denote the distribution that the LM\ninduces on the next token on alphabet Y given the input that is the concatenation of the prompt x\nand a partially decoded response yt of t tokens. Let r([x, y]) be a scalar valued reward function\nbounded from above, e.g., the log-likelihood of a scoring function for the event that the response y\nin context x is deemed safe. We define the following token-wise reward:\nR([x, yt]) :=\n\u001a\n0\nyt \u0338= EOS\nr([x, yt])\nyt = EOS ,\nwhere EOS represents the end of sequence. Here, we only give a reward once decoding has com-\npleted and otherwise no reward is assigned to a decoding path. We then define the value function\nassociated with the reward as:\nV \u22c6([x, yt]) := Ez1,z2,...\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\nX\n\u03c4\u22650\nR([x, yt, z\u03c4])\n\uf8fc\n\uf8fd\n\uf8fe .\n(1)\nThe value function captures the expected cumulative reward of a fully decoded response when de-\ncoding continues from a partially decoded sequence yt, using the base language model p.\nFor a given [x, yt] such that yt \u0338= EOS, we define the advantage function of a decoding policy \u03c0 as:\nA([x, yt]; \u03c0):= Ez\u223c\u03c0\n\b\nV \u22c6([x, yt, z]) \u2212 V \u22c6([x, yt])\n\t\n=\nX\nz\u2208Y\n\u03c0(z|[x, yt])V \u22c6([x, yt, z]) \u2212 V \u22c6([x, yt]).\nNote that the advantage of the base policy is given by A([x, yt]; p) = 0 (law of total probability),\nand hence our goal is to choose \u03c0 to deviate from p to achieve a positive advantage over the base\npolicy.\nLet D([x, yt]; \u03c0) be the token-wise KL divergence between a decoding policy \u03c0 and a frozen base\nlanguage model p for decoding the next token after [x, yt] for yt \u0338= EOS:\nD([x, yt]; \u03c0) := KL(\u03c0(\u00b7|[x, yt])\u2225p(\u00b7|[x, yt]))\n=\nX\nz\u2208Y\n\u03c0(z|[x, yt]) log\n\u0012\u03c0(z|[x, yt])\np(z|[x, yt])\n\u0013\n,\n2\nPreprint\nwhere KL(\u00b7\u2225\u00b7) denotes the KL divergence (also known as relative entropy). Recall that our goal is\nnot to deviate too much from the base policy (measured in KL divergence) because that is expected\nto lead to the degeneration of the language model in other top-line performance metrics.\nTo satisfy these conflicting goals, we use the KL-regularized RL objective which is defined as:\nJ\u03bb([x, yt]; \u03c0) := \u03bbA([x, yt]; \u03c0) \u2212 D([x, yt]; \u03c0),\n(2)\nwhere \u03bb \u2208 R\u22650 trades off reward for drift from the base language model. Note that J\u03bb([x, yt]; \u03c0) is\nconcave in \u03c0. This is because A([x, yt]; \u03c0) is linear in \u03c0 and D([x, yt]; \u03c0) is convex in \u03c0. The first\nterm denotes the advantage that reward that will be obtained eventually once the response is fully\ndecoded. The second term is a language model (LM) negative reward signal penalizing the policy \u03c0\nfor drifting too far from the initial policy p.\nWe let \u03c0\u22c6\n\u03bb(z|[x, yt]) denote the decoding policy function that maximizes equation 2. Note that at\nthe extreme of \u03bb = 0, we have \u03c0\u22c6\n0(z|[x, yt]) = p(z|[x, yt]) which achieves D([x, yt]; p) = 0 and\nA([x, yt]; p) = 0. We are interested in characterizing the tradeoff curves between A and D achieved\nby \u03bb \u2208 R\u22650 to increase A([x, yt]; \u03c0) at the cost of an increased KL penalty, D([x, yt]; \u03c0). Our main\nresult in this section is the following characterization of \u03c0\u22c6\n\u03bb.\nTheorem 2.1. The optimal policy for the RL objective is unique and is given by\n\u03c0\u22c6\n\u03bb(z|[x, yt]) \u221d p(z|[x, yt])e\u03bbV \u22c6([x,yt,z]).\n(3)\nThis result resembles that of Korbak et al. (2022), with the main difference being the controller is\ntoken-wise here. Next. Recall that our goal is to develop an inference-time alignment solution that\nkeeps the language model frozen. Theorem 2.1 gives us a way to do that by combining logits from\na frozen LM and those of a value function.\n3\nCONTROLLED DECODING\nOur goal is to find V\u03b8([x, yt]) parameterized by \u03b8 to match V \u22c6([x, yt]) through the following L2\nobjective function:1\nL\u22c6(\u03b8) = Ex\u223cpxEy\u223cpy|x\u2113\u22c6(x, y; \u03b8),\nwhere \u2113\u22c6(x, y; \u03b8) = 1\n2\nX\nt\u2208[|y|]\n(V\u03b8([x, yt]) \u2212 V \u22c6([x, yt]))2,\nwhere px represents a distribution over the prompts used for training. In the rest of this section we\npresent two methods to learn the prefix scorer, and two ways to use it at inference time for control.\n3.1\nTRAINING THE PREFIX SCORER\nWhile Theorem 2.1 gives a recipe to solve the KL-regularized RL, it requires having access to the\nvalue function V \u22c6([x, yt]), which we refer to as a prefix scorer. We discuss two methods for learning\nit in the sequel.\nCD-FUDGE (Yang & Klein, 2021). Given x \u223c px, let y = ([y1, . . . , yT ]) be a draw from the base\nmodel p. Consider r([x, y]), to be the stochastic reward of one fully decoded completion. Let\nLF (\u03b8) = Ex\u223cpx\u2113F (x, y; \u03b8),\ns.t. y \u223c p,\n(4)\nwhere \u2113F (x, y; \u03b8) = 1\n2\nX\nt\u2208[|y|]\n\u0000V\u03b8([x, yt]) \u2212 r([x, y])\n\u00012 .\nNow we state our main result on CD-FUDGE.\nTheorem 3.1 (informal). Under regularity assumptions, applying SGD on LF converges to a sta-\ntionary point of L\u22c6(\u03b8).\n1It may be possible to devise more sophisticated learning objectives through Fisher information shaping or\nother divergences.\n3\nPreprint\nWill this paper get accepted?\nThis paper will be\nliked\naverage\nvery high\nhigh\ndisliked\nhigh\nlow\naverage\nreviewed\nhigh\naverage\naverage\nhated\nvery high\nvery low\naverage\nLM \nlikelihood\nsentiment \nprefix score\naligned \nscore\nFigure 1: An illustration of token-wise sampling using CD prefix scorer where the alignment goal is\nto decode sequences with positive sentiment. The sentiment score is used to shape the overall aligned\nscore for sampling, which results in downweighting of the high likelihood tokens that might result\nin negative sentiment and upweighting of tokens that result in outcomes with positive sentiment.\nThis is a remarkable result.\nIt states that if the dataset used for training the prefix scorer in\nFUDGE (Yang & Klein, 2021) is obtained by rolling out the SFT model, then FUDGE prefix scorer\nmay be used to solve the RL problem in equation 2.\nCD-Q. Notice the following Bellman identity (Sutton & Barto, 2018):\nV \u22c6([x, yt]) =\n\u001a\nEz\u223cp(\u00b7|[x,yt])V \u22c6([x, yt, z]),\nyt \u0338= EOS\nr([x, yt]),\nyt = EOS .\nWe present a simple solution to train a prefix scorer. Inspired by the policy evaluation updates in\nDQN (Mnih et al., 2013), we optimize the following loss function:\nLQ(\u03b8) = Ex\u223cpx\u2113Q(x, y; \u03b8),\n(5)\nwhere \u2113Q(x, yt; \u03b8)= 1\n2\nX\nt\u2208[|y|]\n\u0000V\u03b8([x, yt]) \u2212 \u02d9vt\n\u00012,\nvt =\n\u001aP\nz\u2208Y p(z|[x, yt])V\u03b8([x, yt, z])\nyt \u0338= EOS\nr([x, yt])\nyt = EOS\nand where \u02d9v implies a stop gradient over v (even though it inherently depends on \u03b8).\nThe abovementioned learning procedure for the prefix scorer may be performed over an off-policy\ndataset, scored offline using the reward for all [x, y] (Sutton & Barto, 2018). Training the pre-\nfix scorer requires (on-demand) access to the base language model p to compute the target vt\nin equation 5. Finally a simple modification of this procedure can be shown to be provably con-\nvergent (Wang & Ueda, 2022).2\n3.2\nINFERENCE-TIME SAMPLING STRATEGIES\nEquipped with the prefix scorer, we use it in two different ways at inference time.\nToken-wise sampling.\nWe use the prefix scorer for token-wise sampling per Theorem 2.1. In this\ncase, given context x and a partially decoded sequence yt, we obtain the logits of p([x, yt, z]) and\nV\u03b8([x, yt, z]) for all z from the base policy and the prefix scorer. Then, we linearly combine the\nlogits to sample from the following distribution to sample from the following distribution:\nz \u223c \u03c0\u03b8(\u00b7|[x, yt])\n(6)\nwhere \u03c0\u03b8(z|[x, yt]) \u221d p(z|[x, yt])e\u03bbV\u03b8([x,yt,z]).\nAn illustration of token-wise sampling using CD prefix scorer is presented in Figure 1, where the\nprefix scorer is used to downweight decoding of tokens that may lead to undesirable outcomes. Note\nthat token-wise sampling is the most straight-forward way to use the prefix scorer, which requires\none call to the prefix scorer per decoding of each token, and was also used by Yang & Klein (2021).\n2Note that one may improve on the proposed solver (cf. (Hessel et al., 2018)), but we present the simplest\nform for the sake of clarity, which already gives good empirical performance.\n4\nPreprint\nWill this paper get accepted?\nThis paper\nwill be liked by \nvery high\nwill receive diverging reviews\nlow\nmay be liked by\naverage\nis not getting into\nvery low\nsentiment \nprefix score\nFigure 2: An illustration of block-wise best-of-K using CD prefix scorer where the alignment goal\nis to decode sequences with positive sentiment. First, K(=4) continuations of length M(=4) tokens\nare sampled from the base LM, and scored using the prefix scorer. The block of tokens with the\nhighest prefix score is selected as the continuation, and the process is repeated.\nBlock-wise best-of-K.\nNext, we present a sampling strategy that combines RL with best-of-K.\nWe sample K i.i.d. continuation blocks of length M from the base policy, and accept the continua-\ntion with the highest prefix score and reject the rest:\nzM := arg\nmax\nn\nzM\n(k)\no\nk\u2208[K]\nV\u03b8([x, yt, zM\n(k)])\n(7)\nwhere\nn\nzM\n(k)\no\nk\u2208[K]\ni.i.d.\n\u223c p(zM|[x, yt]),\nand continue until a candidate with EOS has been accepted.\nAn illustration of the block-wise sample and rerank is presented in Figure 2, where the prefix scorer\nis used to rerank M(=4) decoding paths and choose the candidate with the most positive sentiment.\nRemark. In terms of inference throughput, blockwise CD is similar to the best-of-K for the same\nvalue of K. However, it offers two major advantages:\n1. The decoding latency here is only M tokens, whereas the best-of-K method needs to fully decode\nall K sequences before it can select one to be served. If the sequence length is large, e.g., when\nthe prompt is to write an essay, this would not be tolerated. This can open up new applications\nsuch as streaming.\n2. To achieve high rewards, best-of-K might require unreasonably high values of K. Blockwise\nCD enables similar reward values with significantly smaller K.\n4\nEXPERIMENTAL SETUP\nWe examine performance of the controlled decoding models with our proposed inference-time sam-\npling strategies across two tasks. For all experiments, unless otherwise specified the base generative\nmodel we use is Palm 2-XXS (Gecko).\n4.1\nDATASETS\nDSTC8 Reddit conversations corpus (Microsoft, 2019) is a dataset containing millions of multi-\nturn conversations from Reddit threads. We use this to optimize response length, and to evaluate on\nhelpfulness and harmlessness of a response.\nAnthropic HH (Bai et al., 2022) is a helpfulness and harmlessness benchmark where the assistant\ntries to complete next turn in a conversation with a human. We use this to train a reward model that\nlearns human preferences on the helpfulness and harmlessness of the generation.\n5\nPreprint\n4.2\nTRAINING & EVALUATION\nResponse length experiments: Using the Reddit conversations corpus, we used PaLM 2-XXS (Anil\net al., 2023) to train prefix scorers, DPO (Rafailov et al., 2023), IPO (Azar et al., 2023), and\nPPO (Ouyang et al., 2022). For DPO, IPO and PPO, we performed several training runs, vary-\ning regularizer hyperparameters and learning rates to reach comparable KLs against other methods.\nAll methods were trained for half epoch and evaluated on the number of tokens in the generation\nusing the eval set of conversations corpus.\nHelpfulness and harmlessness(HH) experiments: We used PaLM 2-XXS to train a reward model\nusing the Bradley-Terry pairwise loss model on human preference data from Anthropic HH dataset,\nand picked the checkpoint with the best eval accuracy. Then we used the reward model to train\nprefix scorers, DPO, IPO and PPO using PaLM 2-XXS on Reddit conversations corpus with HHH\nprompt for one epoch. Similarly, we performed several training runs for DPO, IPO and PPO to\nobtain comparable KLs. Finally, we ran zeroshot on eval set of the conversations corpus with PaLM\n2-L(Unicorn) (Anil et al., 2023) to evaluate the helpfulness and harmlessness of the generation using\na prompt, which can be found in Appendix A.\nReward/win-rate vs KL tradeoffs. Most of our evaluations are done by reporting expected reward\nof the aligned policy, or the win-rate of the aligned policy against the base policy, as a function of\nthe KL divergence between the aligned policy and the base policy. To this end, we focus on single\ndigit values of KL, beyond which the policy shows significant signs of overfitting (Eisenstein et al.,\n2023).\n4.3\nBASELINES\nWe consider CD-FUDGE (Yang & Klein, 2021), best-of-K, KL-regularized RL methods including\nPPO (Ouyang et al., 2022), DPO (Rafailov et al., 2023), and IPO Azar et al. (2023) as baselines.\nCD-FUDGE is trained in the same way as CD-Q with the difference being the target in equation 5\nreplaced by the explicit reward received in a given decoding path from the dataset.\nFor PPO, DPO and IPO, we trained to maximize the objective of each experiment via online rein-\nforcement learning. For the response length experiment, our objective was to maximize the length\nup to 1024, specifically, rlength([x, yT ]) = log(T/Tmax), where Tmax = 1024. For the helpfulness\nand harmlessness experiment, we used the reward model\u2019s score.\nDPO (Rafailov et al., 2023) compares two generations from a policy and tries to increase the like-\nlihood of the preferred generation according to the human preference. In our setup, we used online\nDPO by rolling out the policy and sampling two generations and optimizing for the preference de-\ntermined by the objective.\nIPO (Azar et al., 2023) aims to improve upon DPO by adding a regularization term to avoid overfit-\nting and a reward reshaping objective. Similarly to DPO, we use online IPO in this paper.\nAdditionally, we also consider the blockwise best-of-K variant of FUDGE Yang & Klein (2021),\nnamed blockwise CD-FUDGE, which is inspired by the proposed blockwise CD-Q method in this\npaper.\nEvaluation metrics. Following Gao et al. (2023), we report tradeoff curves for expected reward or\nwin-rate over base policy vs. KL divergence between the aligned policy and the base, KL(\u03c0\u2225p). A\nmethod that dominates (i.e., increases the expected reward with smallest sacrifice in KL divergence)\nis more desirable.\nFor CD-Q and CD-FUDGE, we sweep the strength of the prefix scorer to achieve tradeoff curves\nbetween performance (win-rate or expected reward) and KL(\u03c0\u2225p). For PPO, DPO and IPO, we\nsweep the strength of the KL-regularizer to achieve the same goal. Finally, for best-of-K, blockwise\nCD-Q, and blockwise CD-FUDGE, we do this by sweeping K. Note we use the expression log(K)\u2212\n(K \u2212 1)/K suggested by Stiennon et al. (2020); Beirami et al. (2024) to upper bound KL(\u03c0\u2225p)\nfor best-of-K policy, and use the same expression as the KL(\u03c0\u2225p) for each decoded block by the\nblockwise variants of CD-FUDGE and CD-Q, as for blockwise variants of CD-Q and CD-FUDGE\nmethods, we use a fixed M = 32 tokens. For blockwise sampling strategies, we use an upper\nbound on the KL divergence given by D(\u03c0\u2225p) \u2264 Ex\u223cpx (log(K) \u2212 (K \u2212 1)/K)\n\u0006 Lx\nM\n\u0007\n, where Lx\n6\nPreprint\n0\n1\n2\n3\n4\n5\n6\n7\n8\nKL(\np)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nLength\nBest-of-K\nCD - Q (blockwise)\nCD - FUDGE (blockwise)\nCD - Q\nCD - FUDGE\nIPO\nDPO\nPPO\nK = 50\nK = 6\nFigure 3: Length vs. KL divergence for different length alignment methods. CD-Q (blockwise)\noutperforms RL techniques such as IPO & PPO and is on par with best-ok-K while being much\nmore efficient as it requires far fewer samples (e.g. 6 vs 50).\nis the expected number of decoded tokens in the full response given prompt x, which is an extension\nof (Beirami et al., 2024, Theorem 1).\n5\nEXPERIMENTAL RESULTS\nExperiment 1: Increasing dialog response length. To circumvent the effect of reward overopti-\nmization, in our first task, we consider the response length as the reward. In this case, we do have ac-\ncess to the true reward at training and evaluation time. In particular, rlength([x, yT ]) = log(T/Tmax),\nwhere Tmax = 1024. As can be seen in Figure 3, our proposed method blockwise CD-Q achieves\nthe best length vs KL trade-off on par with best-of-K, while being significantly more efficient than\nbest-of-K as it achieves similar tradeoffs with much fewer K, e.g., with K=6, blockwise CD-Q\nobtains very similar length and KL divergence as best-of-K with K=50. Furthermore, best-of-K\nitself achieves a better reward-KL tradeoff compared to KL-regularized PPO (Ouyang et al., 2022).\nThis might be surprising at first, but it is consistent with other findings reported by Gao et al. (2023);\nRafailov et al. (2023); Eisenstein et al. (2023), where it is shown that best-of-K consistently achieves\nbetter reward-KL tradeoffs compared to KL-regularized PPO.\nWe also observe that the token-wise control using both CD-FUDGE (Yang & Klein, 2021) and CD-\nQ leads to a more favorable reward-KL tradeoff compared to all baselines, including DPO and IPO.\nWhen we consider blockwise control, we see a stark difference between the behavior of blockwise\nCD-FUDGE and blockwise CD-Q, where blockwise CD-Q in on par with best-of-K, leading to best\nreward-KL tradeoffs. To investigate this further, we used the CD-Q and CD-FUDGE prefix scorers\nas reward (i.e., length) predictors for fully decoded responses on the test set, where the result is\nreported in Figure 10 (Appendix B). The main finding is that the predictions of CD-FUDGE are\nmuch noisier than that of CD-Q and we suspect that is the reason CD-FUDGE does not perform\nwell in the blockwise setup, where blockwise CD-Q achieves the best performance on par with\nbest-of-K.\nExperiment 2: Improving dialog helpfulness and harmlessness. In this experiment, we con-\nsider improving the helpfulness and harmlessness (HH) of the responses in conversations. We train\ntwo independent reward models on the side-by-side preference signal following the Anthropic HH\ndataset (Bai et al., 2022) using PaLM 2-XXS (Reward-XXS). The goal here is to generate more\n7\nPreprint\n0\n1\n2\n3\n4\n5\n6\n7\n8\nKL(\np)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nWin rate\nBest-of-K\nCD - Q (blockwise)\nCD - FUDGE (blockwise)\nCD - Q\nCD - FUDGE\nIPO\nDPO\nPPO\nFigure 4: Win rate vs. KL divergence for different helpfulness and harmlessness alignment methods. CD-Q\n(blockwise) vastly outperforms RL techniques such as IPO & PPO.\nhelpful and harmless responses in a dialog setting, where rHH([x, yT ]) could be roughly interpreted\nas the log-probability of a HH classifier. The results are reported in Figure 4, where the y-axis is the\nwin rate against the base model as measured by running zeroshot on PaLM 2-L (Unicorn). As can be\nseen, token-wise controllers don\u2019t offer much HH improvement over baselines, whereas blockwise\nCD-Q and CD-FUDGE offer a substantial improvement as expected. However, neither method was\nable to match best-of-K.\nMethod\nAccuracy (train)\nAccuracy (test)\nReward-XXS\n0.804\n0.709\nCD-FUDGE\n0.632\n0.629\nCD-Q\n0.624\n0.631\nTable 1: HH preference accuracy on 1500 ground truth side-by-side Anthropic HH training and test set.\nIn Table 1, we compare the training and test accuracy of Reward-XXS with that of CD-Q and CD-\nFUDGE used as classifiers, where we apply CD-Q and CD-FUDGE on [x, y] pairs in the training\nand test set of Anthropic HH dataset (Bai et al., 2022). The goal of this experiment is a sanity check\non the prefix scorer as good performance on this classification task is necessary but not sufficient for\nensuring that the prefix scorer can be reliably used in practice. The results show that the predictive\npower of CD-Q and CD-FUDGE (used as a classifier) are much weaker than that of Reward-XXS\n(\u2248 0.6 vs \u2248 0.7). This is likely due to the noisy nature of the training data, and is an area for future\ninvestigation to improve the training using value function learning methods better suited to noisy\nreward environments.\nExperiment 3: Simultaneously improving dialog helpfulness and harmlessness & increasing\ndialog response length. We train prefix scorer to optimize for both the HH and length (rewards).\nTo this end, we only consider blockwise CD-FUDGE, where the decoding either performs reranking\nbased on HH alone; or a linear combination of the HH and length rewards. For this experiment, we\nexpanded prefix scorer to have a head for each reward, and simultaneously trained to learn the score\nof the HH and length.\nThe results of this experiment are presented in Figure 5. As expected, introducing a positive length\nreward to the reward combination, results in increasing dialog length, not surprisingly, this comes\nat the expense of a decline in dialog HH win rate. Note that this would be impossible with KL-\nregularized RL methods as it needs to be retrained from scratch for different linear combinations of\n8\nPreprint\n0\n1\n2\n3\n4\n5\nKL(\np)\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nLength\nCD - FUDGE (blockwise) - positive length reward\nCD - FUDGE (blockwise) - neutral length reward\n0\n1\n2\n3\n4\n5\n6\n7\n8\nKL(\np)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nWin rate\nCD - FUDGE (blockwise) - positive length reward\nCD - FUDGE (blockwise) - neutral length reward\nFigure 5: Length/Win rate vs. KL divergence for multi-objective alignment. CD is able to dynamically adjust\nthe trade-off between various objectives live at inference time.\n0\n1\n2\n3\n4\n5\n6\n7\n8\nKL(\np)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nWin rate\n8B Best-of-K\nCD - Q (blockwise)\nFigure 6: Win rate on a different base LLM (PaLM 2-XS) without re-training the CD-Q prefix scorer. CD-Q\nis able to generalize well and retain good performance without retraining.\nrewards. This shows flexibility and modularity of the prefix scorer, which can be trained for multiple\nobjectives at once and different linear combinations of objectives can be achieved without retraining.\nExperiment 4: Updating the base generative model without retraining the prefix scorer. We ab-\nlate Experiment 2 above by swapping the base generative model with a completely different model,\nspecifically PaLM 2-XS (Otter), instead of PaLM 2-XXS (Gecko) for which the prefix scorer was\ntrained using CD-Q. This helps understand how closely the prefix scorer is coupled with the weights\nof the base generative model and so how frequently the prefix scorer needs to be retrained in a\nproduction setting where the base generative model can change frequently. The results of this exper-\niment in Figure 6 show that CD-Q performs on par with the strongest baseline, best-of-K, implying\nthat the prefix scorer trained using CD-Q is robust and generalizes well to other base generative\nLLMs other than the one for which it was trained. Note that PPO, DPO, and IPO could not be used\nwithout re-training in this experiment.\nExperiment 5: Impact of adjusting block size in blockwise CD. We ablate Experiment 2 above\nby reducing the block size M to analyze its impact. From Figure 7 we observe that reducing the\nblock size M generally results in worse win-rate vs KL divergence trade-offs. We did not analyze\nblock sizes much larger than 32 as the efficiency gains against best-of-K would evaporate.\nExperiment 6: Using CD-Q on a DPO base model. In this experiment, we take the models\nfinetuned using DPO and then perform inference using blockwise CD-Q with no additional training.\nThis is denoted as \u201cDPO + CD-Q (blockwise)\u201d in Figure 8. Note that the CD-Q was not exposed to\nfinetuned DPO during training of its prefix scorer. We chose K in CD-Q such that its KL-divergence\nwould roughly match that of the DPO baseline, e.g., for the green point annotated with \u201cK = 8\u201d, the\n9\nPreprint\nFigure 7: Win rate vs. KL divergence with different block size. Larger block sizes generally perform better.\nK = 32\nK = 8\nFigure 8: Win rate combining DPO and CD-Q. The combination is on par with CD-Q alone while being more\nefficient as it requires fewer samples, e.g., 8 vs 32.\ntotal KL divergence is about 5, of which 2.5 is from DPO and 2.5 is from blockwise CD-Q. We\nadjusted the sample size K of blockwise CD-Q in order to achieve this. From the plot we see that\nthis variant combining both approaches gives the overall best tradeoff curve and narrowly wins over\nblockwise CD-Q in larger KL regimes. However, it is more efficient since it is able to achieve the\nsame / better win-rate and KL as vanilla blockwise CD-Q but with fewer K, e.g., compare K=8 for\n\u201cDPO + CD-Q (blockwise)\u201d and K=32 for \u201cCD-Q (blockwise)\u201d which produces a similar trade-off,\nindicating that the combined variant requires fewer K.\n10\nPreprint\n0\n1\n2\n3\n4\n5\n6\n7\n8\nKL(\np)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nLength\nCD - Q (blockwise)\nDPO BoN\nFigure 9: Length vs. KL divergence comparing CD-Q (blockwise) with 4 block samples and DPO with best-\nof-4 samples. This shows CD outperforms DPO even as the inference cost is fixed.\nExperiment 7: Using a fixed inference throughput budget. In this experiment we revisit Exper-\niment 1 above to compare CD-Q (blockwise) and DPO when given a fixed inference budget. In\nExperiment 1, DPO had a sample size of 1 (best-of-1) and while CD-Q (blockwise) also produced\nonly a single unique sample, it inherently uses K samples to generate each response block as de-\nscribed in Equation 7. Here we fix the inference budget by setting K = 4 for blockwise CD-Q and\nuse best-of-K on top of DPO with K = 4, so that they both have the same same inference through-\nput cost. In Figure 9 we see that even with this setup, CD-Q (blockwise) outperforms DPO with\nbest-of-K sampling.\n6\nRELATED WORK\nControlled decoding. FUDGE (Yang & Klein, 2021) noticed that decoding subject to a constraint\ncould be achieved by a prefix scorer given by the Bayes rule, and augmented the discriminative data\nto train the partial scorer. DIRECTOR (Arora et al., 2022) further showed that the partial scorer\ncould be jointly learned with the language model itself, which would lead to a reduced latency at\ninference time. GeDi (Krause et al., 2021) proposed to train separate positive and negative scorer\nnetworks that could be combined to obtain a prefix score. Kim et al. (2023) showed that the critic\nin an actor-critic RL framework may be used for controlled decoding. In contrast to this line of\nwork, we show that the prefix scorer could be trained as the value function for the language model\ndecoding policy, allowing us to establish an exact connection between controlled decoding and KL-\nregularized reinforcement learning.\nOur work is also conceptually related to rule-based control. Lu et al. (2022) use tree-search with\na heuristic to determine the quality of a given decoding path to steer decoding towards favorable\noutcomes. Qin et al. (2022) explore gradient-based sampling using Langevin dynamics which sig-\nnificantly outperforms gradient-free sampling.\nReinforcement learning (RL). Another line of very relevant work is reinforcement learning subject\nto a KL penalty with the language model (Ouyang et al., 2022). Korbak et al. (2022) observed that re-\ninforcement learning with a KL penalty could be viewed in a Bayesian manner with a corresponding\nreward function. However, their work fell short of making the full connection in an autoregressive\ndecoding setting, which is our contribution in this work through CD. Another closely related work to\nours is that of Snell et al. (2023) that designs a value-based offline algorithm, albeit with a different\nlearning objective than ours (and that of the KL-regularized PPO). Li et al. (2017) also use a vari-\n11\nPreprint\nant of Q-learning to optimize BLEU or ROUGE scores. Other related RL work includes generator\nimprovement solutions through on-policy RL. Sparrow (Glaese et al., 2022) showed that a variant\nof proximal policy optimization (PPO) (Schulman et al., 2017) with an additional LM regularizer\nis effective at a variety of safety objectives and alignment with human preference (Ouyang et al.,\n2022). Finally, the configurability of reward is conceptually related to (Ram\u00b4e et al., 2024), where it\nis shown that reward soups may be used to a similar effect.\nSupervised contrastive learning from negative examples. Another line of related work is su-\npervised generator improvement interventions. These include unlikelihood training (Welleck et al.,\n2020; Zhang & Song, 2022), contrastive losses (Adolphs et al., 2022), direct preference optimiza-\ntion (Rafailov et al., 2023), and identity preference optimization (Azar et al., 2023). In contrast\nto our work, these methods are all training-time interventions but they could similarly be used to\nimprove the likelihood of drawing positive examples by suppressing the likelihood of negative ones.\n7\nCONCLUSION\nIn this paper, we formulated a KL-regularized reinforcement learning objective for aligning language\nmodels to achieve higher reward outcomes. We showed that the problem could be solved using an\ninference-time add-on solution by learning a prefix scorer akin to DQNs. We also showed that the\nresulting framework, called controlled decoding (CD), could be used to exert control in language\nmodels to steer the generation in a token-wise or blockwise manner. Our experiments confirmed the\neffectiveness of our proposal in improving different rewards, that included dialog length and dialog\nhelpfulness and harmlessness, with a small deviation from the base language model policy. We\nalso showed that the framework could be readily extended to solve a multi-objective reinforcement\nlearning problem for free. Further, we also presented robustness of our proposal by running CD on\nunseen base model without re-training.\nEven though the token-wise CD and KL-regularized RL are optimizing for the Pareto front of the\nexpected reward vs KL divergence between the aligned policy and the base policy, we observe that\nblockwise CD and best-of-K policy consistently achieve a better tradeoff curve in practice. We are\nnot the first to have observed this, and the extensive experiments of Gao et al. (2023); Eisenstein et al.\n(2023) also confirm this fact. As such, the blockwise variant of CD shows promise for alignment of\nlanguage models.\nFinally, our development of controlled decoding is motivated by tradeoffs between throughput, la-\ntency, and performance. While we explored these tradeoffs in a narrow set of experiments, a more\ncomprehensive and rigorous understanding of such tradeoffs is left for future work, which might\nrequire exploring these methods in conjunction with speculative decoding (Leviathan et al., 2023;\nChen et al., 2023; Sun et al., 2023).\nBROADER IMPACT\nWe proposed new methods for language model alignment, where control was exerted at inference\ntime. As opposed to the commonly used training time intervention to optimize for KL-regularized\nRL, the inference-time solutions give more fine-grained and flexible control, potentially paving the\nway for achieving configurable and personalizable alignment. On the other hand, we also observed\ninconsistent behavior of alignment techniques in improving safety and other socially consequential\nissues. This demonstrates that applying alignment techniques in nuanced problems, such as safety,\nneeds to be done with extreme caution.\nACKNOWLEDGEMENTS\nThe authors are thankful to colleagues at Google Research and Google DeepMind for discus-\nsions and constructive feedback throughout the course of this project: Alekh Agarwal, Ananth\nBalashankar, Jonathan Berant, Alexander D\u2019Amour, Krishnamurthy Dvijotham, Jacob Eisenstein,\nPreethi Lahoti, Xiao Ma, Kathy Meier-Hellstern, Shayegan Omidshafiei, Yuting Sun, Ananda\nTheertha Suresh, Victor Veitch, and Zhaofeng Wu. The authors are also thankful to Kyunghyun\nCho for bringing to their attention the work of Li et al. (2017).\n12\nPreprint\nREFERENCES\nLeonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston.\nThe cringe loss: Learning what language not to model. arXiv preprint arXiv:2211.05826, 2022.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\nKushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston.\nDirector: Generator-\nclassifiers for supervised language modeling. arXiv preprint arXiv:2206.07694, 2022.\nMohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal\nValko, and R\u00b4emi Munos. A general theoretical paradigm to understand learning from human\npreferences. arXiv preprint arXiv:2310.12036, 2023.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.\nTraining a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,\n2022.\nAhmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D\u2019Amour, Jacob Eisenstein, Chirag\nNagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment policy.\narXiv preprint arXiv:2401.01879, 2024.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. Accelerating large language model decoding with speculative sampling. arXiv preprint\narXiv:2302.01318, 2023.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing sys-\ntems, 30, 2017.\nJacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D\u2019Amour, DJ Dvijotham,\nAdam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herd-\ning? reward model ensembles mitigate but do not eliminate reward hacking.\narXiv preprint\narXiv:2312.09244, 2023.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In\nInternational Conference on Machine Learning, pp. 10835\u201310866. PMLR, 2023.\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-\nbeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of\ndialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.\nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan\nHorgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in\ndeep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 32, 2018.\nHamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-\ngradient methods under the polyak-\u0142ojasiewicz condition.\nIn Machine Learning and Knowl-\nedge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy,\nSeptember 19-23, 2016, Proceedings, Part I 16, pp. 795\u2013811. Springer, 2016.\nMinbeom Kim, Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, and Kyomin Jung. Critic-\nguided decoding for controlled text generation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 4598\u2013\n4612, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/\nv1/2023.findings-acl.281. URL https://aclanthology.org/2023.findings-acl.\n281.\n13\nPreprint\nTomasz Korbak, Ethan Perez, and Christopher Buckley. RL with KL penalties is better viewed as\nBayesian inference. In Findings of the Association for Computational Linguistics: EMNLP 2022,\npp. 1083\u20131091, 2022.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard\nSocher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence generation.\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4929\u20134952,\nPunta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\ndoi: 10.18653/v1/2021.findings-emnlp.424. URL https://aclanthology.org/2021.\nfindings-emnlp.424.\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative\ndecoding. International Conference on Machine Learning, 2023.\nJiwei Li, Will Monroe, and Dan Jurafsky. Learning to decode for future success. arXiv preprint\narXiv:1701.06549, 2017.\nXiming Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras,\nLianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi.\nNeuroLogic\na*esque decoding: Constrained text generation with lookahead heuristics.\nIn Proceedings of\nthe 2022 Conference of the North American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, pp. 780\u2013799, Seattle, United States, July 2022. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.57. URL https:\n//aclanthology.org/2022.naacl-main.57.\nMicrosoft.\nDSTC8\nReddit\nCorpus.\nhttps://github.com/microsoft/\ndstc8-reddit-corpus/, 2019. Accessed: 2023-09-30.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\nstra, and Martin Riedmiller.\nPlaying atari with deep reinforcement learning.\narXiv preprint\narXiv:1312.5602, 2013.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.\nWebGPT: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\nLianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. COLD decoding: Energy-based\nconstrained text generation with langevin dynamics.\nNeural Information Processing Systems\n(NeurIPS), 2022. URL https://openreview.net/forum?id=TiZYrQ-mPup.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023.\nAlexandre Ram\u00b4e, Nino Vieillard, L\u00b4eonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier\nBachem, and Johan Ferret. WARM: On the benefits of weight averaged reward models. arXiv\npreprint arXiv:2401.12187, 2024.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nCharlie Victor Snell, Ilya Kostrikov, Yi Su, Sherry Yang, and Sergey Levine. Offline rl for natural\nlanguage generation with implicit language q learning. In The Eleventh International Conference\non Learning Representations, 2023.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008\u20133021, 2020.\n14\nPreprint\nZiteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix\nYu. SpecTr: Fast speculative decoding via optimal transport. In Neural Information Processing\nSystems, 2023.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nZhikang T Wang and Masahito Ueda. Convergent and efficient deep Q network algorithm. 2022.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.\nNeural text generation with unlikelihood training. International Conference on Learning Repre-\nsentations, 2020.\nKevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators. In\nProceedings of the 2021 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, pp. 3511\u20133535, Online, June 2021. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.276. URL https:\n//aclanthology.org/2021.naacl-main.276.\nHanqing Zhang and Dawei Song. Discup: Discriminator cooperative unlikelihood prompt-tuning\nfor controllable text generation. EMNLP, 2022.\nYao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J Liu.\nCalibrating sequence likelihood improves conditional language generation. In The Eleventh In-\nternational Conference on Learning Representations, 2022.\n15\nPreprint\nA\nADDITIONAL DETAILS ON EXPERIMENTAL SETUP\nIn this section, we provide some additional experimental setup.\nThis is the zeroshot prompt we used on PaLM 2-L(Unicorn) to rank generations based on helpfulness\nand harmlessness.\nYou are\na\nh e l p f u l\na s s i s t a n t ,\nt h a t\nranks AI\na s s i s t a n t s \u2019\nr e s p o n s e s\nby\nthe\nq u a l i t y\nof\nt h e i r\nanswers .\nThe AI\na s s i s t a n t s\nt r y\nto\nbe\nh e l p f u l ,\np o l i t e ,\nhonest ,\ns o p h i s t i c a t e d ,\ne m o t i o n a l l y\naware ,\nand\nhumble \u2212but \u2212knowledgeable .\nBelow\nare\na\ns e r i e s\nof\nd i a l o g u e s\nbetween\nv a r i o u s\npeople\nand an AI\na s s i s t a n t ,\nand\nthe\na s s i s t a n t\nt r i e s\nto\nr e p l y\nto\nthe\nd ia l og u e .\nI\nwant you\nto\nrank\nthe\nr e s p o n s e s\nof\na s s i s t a n t s .\nTo do so ,\nI\nw i l l\ngive\nyou\nthe\nd i al o g u e\ngiven\nto\nthe\na s s i s t a n t s ,\nand\nthe\nresponse\nof two\na s s i s t a n t s .\nPlease\nrank\nthe\na s s i s t a n t s\nbased on which\nresponse\nwould be more\nh e l p f u l ,\np o l i t e ,\nhonest ,\ns o p h i s t i c a t e d ,\ne m o t i o n a l l y\naware ,\nand humble \u2212but \u2212knowledgeable .\nAll\ni n p u t s\nare\npython\nd i c t i o n a r i e s .\nHere\ni s\nthe\nprompt :\n{{\n\u201d di a lo g ue \u201d :\n\\\u201d\\\u201d\\\u201d{ d ia l og u e }\\\u201d\\\u201d\\\u201d ,\n}}\nHere\nare\nthe\no u t p u t s\nof\nthe\na s s i s t a n t s :\n[\n{{\n\u201d a s s i s t a n t \u201d :\n\u201d a s s i s t a n t 1 \u201d ,\n\u201d answer \u201d :\n\\\u201d\\\u201d\\\u201d{ o u t p u t 1 }\\\u201d\\\u201d\\\u201d\n}} ,\n{{\n\u201d a s s i s t a n t \u201d :\n\u201d a s s i s t a n t 2 \u201d ,\n\u201d answer \u201d :\n\\\u201d\\\u201d\\\u201d{ o u t p u t 2 }\\\u201d\\\u201d\\\u201d\n}}\n]\nRespond 1 or 2 to\ni n d i c a t e\nthe\nb e t t e r\nout put .\nP leas e\nprovide\nthe\nranking\nt h a t\nthe\nm a j o r i t y\nof\nhumans would\ngive .\nB e t t e r\noutput =\n16\nPreprint\nB\nADDITIONAL EXPERIMENTAL RESULTS\nIn this section, we provide some additional experimental results to better understand the prefix scorer\nlearnt via CD-Q and CD-FUDGE.\n0\n200\n400\n600\n800\n1000\nTest example\n0\n100\n200\n300\n400\n500\n600\n700\n800\nLength\nActual\nCD-Q\nCD-FUDGE\nFigure 10: CD-Q and CD-FUDGE used to predict the length of a fully decoded response on Reddit corpus test\nset (Microsoft, 2019). On the x-axis, the examples in the test set were ordered based on their actual response\nlength an increasing fashion. CD-Q and CD-FUDGE are applied to (x, y) pairs for all test set to predict\nthe length. CD-Q predictions are much better aligned with actual length, especially for pairwise comparison,\nwhereas CD-FUDGE predictions are noisy.\n17\nPreprint\n0\n1\n2\n3\n4\n5\n6\n7\n8\nKL(\np)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nWin rate\nCD - Q (blockwise)\nDPO\nDPO (KL=0.3) + CD-Q (blockwise)\nDPO (KL=1.15) + CD-Q (blockwise)\nDPO (KL=2.25) + CD-Q (blockwise)\nDPO (KL=3.25) + CD-Q (blockwise) \nK = 4\nK = 16\nK = 8\nK = 32\nK = 16\nFigure 11: Win rate comparing blockwise CD-Q, DPO and blockwise CD-Q applied on DPO. From different\nDPO checkpoints, we picked four DPO models covering different KL divergence values, then we applied\nblockwise CD-Q without retraining it. KL divergence values for blockwise CD-Q on DPO was approximated\nby adding the blockwise CD upper bound(8) and the KL divergence of the DPO. Points at win rate 0.7 shows\nthat by combining DPO with blockwise CD-Q, we are able to achieve similar win rate with smaller sample\nsize(down to K = 4) compared to vanilla blockwise CD-Q with sample size = 32.\n18\nPreprint\nC\nPROOFS\nProof of Theorem 2.1. First notice that\nJ\u03bb([x, yt]; \u03c0) =\nX\nz\u2208Y\n\u03c0(z|[x, yt])\n\u0012\n\u03bb(V \u22c6([x, yt, z]) \u2212 V \u22c6([x, yt])) + log\n\u0012 p(z|[x, yt])\n\u03c0(z|[x, yt])\n\u0013\u0013\n(8)\n=\nX\nz\u2208Y\n\u03c0(z|[x, yt]) log\n \np(z|[x, yt])e\u03bb(V \u22c6([x,yt,z])\u2212V \u22c6([x,yt]))\n\u03c0(z|[x, yt])\n!\n.\n(9)\nNow, let\nq\u03bb(z|[x, yt]) := p(z|[x, yt])e\u03bb(V \u22c6([x,yt,z])\nZ\u03bb([x, yt])\n,\n(10)\nwhere\nZ\u03bb(x, yt; \u03b2) =\nX\nz\u2208Y\np(z|x, yt)e\u03bbV \u22c6(x,yt,z).\n(11)\nThus,\nJ\u03bb([x, yt]; \u03c0) = \u2212D\n\u0000\u03c0(\u00b7|[x, yt])\u2225q\u03bb(\u00b7|[x, yt]; \u03b2)\n\u0001\n+ log Z\u03bb([x, yt]),\n(12)\nwhich is strongly convex in \u03c0, and the unique maximize is given by\n\u03c0\u22c6\n\u03bb(\u00b7|[x, yt]) = q\u03bb(\u00b7|[x, yt]),\n(13)\ncompleting the proof.\nNext, we will discuss the general convergence results for CD-FUDGE and CD-Q.\nLemma C.1. We have \u2207\u03b8LF (\u03b8) is an unbiased estimator of the gradient of the optimal objective,\ni.e.,\nEy\u223cp[\u2207\u03b8LF (\u03b8)] = \u2207\u03b8L\u22c6(\u03b8).\n(14)\nProof. Let Lx := Ey\u223cp|y|, be the expected length of the response in context x.\nEy\u223cp\u2113F (x, y; \u03b8) = Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\n1\n2\nX\nt\u2208[|y|]\n\u0000V\u03b8([x, yt]) \u2212 r([x, y])\n\u00012\n\uf8fc\n\uf8fd\n\uf8fe\n(15)\n= Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\n1\n2\nX\nt\u2208[|y|]\n\u0000V\u03b8([x, yt])2 \u2212 2V\u03b8([x, yt])2r([x, y]) + r([x, y])2\u0001\n\uf8fc\n\uf8fd\n\uf8fe (16)\n= Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\n1\n2\nX\nt\u2208[|y|]\n\u0000V\u03b8([x, yt])2 \u2212 2V\u03b8([x, yt])r([x, y]) + r([x, y])2\u0001\n\uf8fc\n\uf8fd\n\uf8fe\n(17)\n= Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\n1\n2\nX\nt\u2208[|y|]\nV\u03b8([x, yt])2\n\uf8fc\n\uf8fd\n\uf8fe \u2212 Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\nX\nt\u2208[|y|]\nV\u03b8([x, yt])r([x, y])\n\uf8fc\n\uf8fd\n\uf8fe + Cx\n(18)\n= Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\n1\n2\nX\nt\u2208[|y|]\nV\u03b8([x, yt])2\n\uf8fc\n\uf8fd\n\uf8fe \u2212 Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\nX\nt\u2208[|y|]\nV\u03b8([x, yt])Eyt+1,...{r([x, y])}\n\uf8fc\n\uf8fd\n\uf8fe + Cx\n(19)\n= Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\n1\n2\nX\nt\u2208[|y|]\nV\u03b8([x, yt])2\n\uf8fc\n\uf8fd\n\uf8fe \u2212 Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\nX\nt\u2208[|y|]\nV\u03b8([x, yt])V \u22c6([x, y])\n\uf8fc\n\uf8fd\n\uf8fe + Cx\n(20)\n19\nPreprint\nwhere the last step follows from the law of total expectation and\nCx := Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\n1\n2\nX\nt\u2208[|y|]\nr([x, y])2\n\uf8fc\n\uf8fd\n\uf8fe .\n(21)\nHence,\n\u2207\u03b8Ey\u223cp\u2113F (x, y; \u03b8) = \u2207\u03b8Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\n1\n2\nX\nt\u2208[|y|]\nV\u03b8([x, yt])2\n\uf8fc\n\uf8fd\n\uf8fe\n\u2212 \u2207\u03b8Ey\u223cp\n\uf8f1\n\uf8f2\n\uf8f3\nX\nt\u2208[|y|]\nV\u03b8([x, yt])V \u22c6([x, y])\n\uf8fc\n\uf8fd\n\uf8fe = \u2207\u03b8L\u22c6(\u03b8),\nwhich completes the proof.\nTheorem C.2. Assume that \u2113F (x, y, \u03b8) is such that it is L-Lipschitz for all x and y. Further assume\nthat \u2113F (x, y, \u03b8) has a non-empty solution set and satisfies the PL inequality (Karimi et al., 2016,\nEq. (3)). Further, assume that E{\u2225\u2207\u03b8\u2113F (y, y, \u03b8i)\u22252} \u2264 C2 for all \u03b8i. Then, applying SGD on \u2113F\nconverges to \u03b8\u22c6.\nProof. The proof follows directly from Lemma C.1 and applying (Karimi et al., 2016, Theorem 4),\nwhich also characterizes the convergence rate.\n20\n"
  },
  {
    "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time",
    "link": "https://arxiv.org/pdf/2310.17157.pdf",
    "upvote": "8",
    "text": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nZichang Liu 1 Jue Wang 2 Tri Dao 3 Tianyi Zhou 4 Binhang Yuan 5 Zhao Song 6 Anshumali Shrivastava 1\nCe Zhang 5 Yuandong Tian 7 Christopher R\u00e9 3 Beidi Chen 8 7\nAbstract\nLarge language models (LLMs) with hundreds of\nbillions of parameters have sparked a new wave\nof exciting AI applications. However, they are\ncomputationally expensive at inference time. Spar-\nsity is a natural approach to reduce this cost, but\nexisting methods either require costly retraining,\nhave to forgo LLM\u2019s in-context learning ability, or\ndo not yield wall-clock time speedup on modern\nhardware. We hypothesize that contextual sparsity,\nwhich are small, input-dependent sets of attention\nheads and MLP parameters that yield approxi-\nmately the same output as the dense model for a\ngiven input, can address these issues. We show that\ncontextual sparsity exists, that it can be accurately\npredicted, and that we can exploit it to speed up\nLLM inference in wall-clock time without compro-\nmisingLLM\u2019squalityorin-contextlearningability.\nBased on these insights, we propose DEJAVU, a\nsystem that uses a low-cost algorithm to predict\ncontextual sparsity on the fly given inputs to each\nlayer, along with an asynchronous and hardware-\naware implementation that speeds up LLM\ninference. We validate that DEJAVU can reduce the\ninference latency of OPT-175B by over 2\u00d7 com-\npared to the state-of-the-art FasterTransformer,\nand over 6\u00d7 compared to the widely used Hugging\nFace implementation, without compromising\nmodel quality. The code is available at https:\n//github.com/FMInference/DejaVu.\n1\nIntroduction\nLarge language models (LLMs), such as GPT-3, PaLM,\nand OPT have demonstrated that an immense number of\n1Rice University\n2Zhe Jiang University\n3Stanford Uni-\nversity\n4University of California, San Diego\n5ETH Zurich\n6Adobe Research 7Meta AI (FAIR) 8Carnegie Mellon Univer-\nsity. Correspondence to: Zichang Liu <zl71@rice.edu>, Tri Dao\n<trid@stanford.edu>, TianyiZhou<t8zhou@ucsd.edu>, ZhaoSong\n<zsong@adobe.com>, Beidi Chen <beidic@andrew.cmu.edu>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nparameters unleashes impressive performance and emergent\nin-context-learning abilities\u2014they can perform a task by\nconditioning on input-output examples, without updating\ntheir parameters (Bommasani et al., 2021; Liang et al.,\n2022; Brown et al., 2020; Min et al., 2022; Chan et al.,\n2022). However, they are very expensive at inference time,\nespecially for latency-sensitive applications (Pope et al.,\n2022). An ideal inference-time model should use less com-\nputation and memory while maintaining the performance\nand special abilities of pre-trained LLMs. The simplest and\nmost natural approach is sparsification or pruning, which\nhas a long history before the LLM era (LeCun et al., 1989).\nUnfortunately, speeding up inference-time sparse LLMs in\nwall-clock time while maintaining quality and in-context\nlearning abilities remains a challenging problem.\nWhile sparsity and pruning have been well-studied, they\nhave not seen wide adoption on LLMs due to the poor\nquality and efficiency trade-offs on modern hardware such\nas GPUs. First, it is infeasible to retrain or iteratively prune\nmodels at the scale of hundreds of billions of parameters.\nThus, methods in iterative pruning and lottery ticket\nhypothesis (Lee et al., 2018; Frankle & Carbin, 2018) can\nonly be applied to smaller-scale models.\nSecond, it is\nchallenging to find sparsity that preserves the in-context\nlearning ability of LLMs. Many works have shown the\neffectiveness of task-dependent pruning (Michel et al., 2019;\nBansal et al., 2022), but maintaining different models for\neach task conflicts with the task independence goal of LLMs.\nLastly, it is hard to achieve wall-clock time speed-up with\nunstructured sparsity due to its well-known difficulty with\nmodern hardware (Hooker, 2021). For example, recent\ndevelopment in zero-shot pruning like SparseGPT (Frantar\n& Alistarh, 2023) finds 60% unstructured sparsity but does\nnot yet lead to any wall-clock time speedup.\nAn ideal sparsity for LLMs should (i) not require model\nretraining, (ii) preserve quality and in-context learning\nability, and (iii) lead to speed-up in wall-clock time on\nmodern hardware. To achieve such demanding requirements,\nwe go beyond static sparsity in previous works (e.g., struc-\ntured/unstructured weight pruning). We instead envision\ncontextual sparsity, which are small, input-dependent\nsets of attention heads and MLP parameters that lead to\n(approximately) the same output as the full model for an\ninput. Inspired by the connections between LLMs, Hidden\n1\narXiv:2310.17157v1  [cs.LG]  26 Oct 2023\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n0\n20\n40\n60\n80\nTransformer Layer\n 0.00\n 0.20\n 0.40\n 0.60\n 0.80\n 1.00\nContextual Sparsity\nOPT-175B\n(a) Contextual Sparsity\n1\n2\n3\n4\n5\n6\n7\n8\nTheoretical Reduction\n0.794\n0.796\n0.798\n0.800\n0.802\n0.804\n0.806\n0.808\n0.810\nAccuracy\nStatic Sparsity\nNon-Contextual Sparsity\nContextual Sparsity\n(b) Accuracy-Efficiency Trade-offs\nFigure 1. (1) LLMs have up to 85% contextual sparsity for a given\ninput. (2) Contextual sparsity has much better efficiency-accuracy\ntrade-offs (up to 7\u00d7) than non-contextual sparsity or static sparsity.\nMarkov Models (Xie et al., 2022; Baum & Petrie, 1966), and\nthe classic Viterbi algorithm (Viterbi, 1967), we hypothesize\nthat for pre-trained LLMs,\ncontextual sparsity exists given any input.\nThe hypothesis, if true, would enable us to cut off specific\nattention heads and MLP parameters (structured sparsity)\non the fly for inference-time, without modifying pre-trained\nmodels. However, there are three challenges.\nExistence: It is nontrivial to verify if such contextual sparsity\nexists, and naive verification can be prohibitively expensive.\nPrediction: Even if contextual sparsity exists, it is challeng-\ning to predict the sparsity for a given input in advance.\nEfficiency: Even if the sparsity can be predicted, it might\nbe difficult to achieve end-to-end wall-clock time speedup.\nTaking OPT-175B as an example, the latency of one MLP\nblock is only 0.2 ms on an 8\u00d7A100 80GB machine. Without\na fast prediction and optimized implementation, the overhead\ncan easily increase the LLM latency rather than reduce it.\nIn this work, we address these challenges as follows:\nExistence: Fortunately, we verify the existence of contextual\nsparsity with a surprisingly simple approach. To achieve\nessentially the same output, contextual sparsity is on average\n85% structured sparse and thereby potentially leads to a 7\u00d7\nparameter reduction for each specific input while maintain-\ning accuracy (Figure 1(a)). During explorations of contextual\nsparsity, we make important empirical observations and build\na theoretical understanding of major components in LLMs\nthat help address the prediction and efficiency challenge.\nDeja Vu\nAttentionk\nMLPk\nPredictor\nPredictor\nPredictor\nAttentionk+1\n\u2026\n\u2026\nFigure 2. DEJAVU uses lookahead predictors to side-step prediction\ncosts: given the input to the attention layer at block k, they (asyn-\nchronously) predict the contextual sparsity for the MLP at block k,\nand given the input to the MLP at block k, they predict the sparsity\nfor the attention head at the next layer.\nPrediction: We discover that contextual sparsity depends\nnot only on individual input tokens (i.e., non-contextual\ndynamic sparsity) but also on their interactions (contextual\ndynamic sparsity). Figure 1(b) shows that with pure dynamic\ninformation, sparsity prediction is inaccurate. Only with\ntoken embeddings with sufficient contextual information\ncan we predict sparsity accurately. Another finding is that\ncontextual dynamic sparsity for every layer can be predicted\nbased on the \u201csimilarity\u201d between layer parameters (head-\ns/MLP) and the output from the previous layer, which carries\nthe immediate contextual mixture of token embeddings.\nEfficiency: Because at inference time, model parameters are\nstatic, inspired by the classical nearest neighbor search (NNS)\nliterature and its applications in efficient deep learning, it is\npossible to formulate the above similarity-based prediction\nas an NNS problem (Indyk & Motwani, 1998b; Zhang et al.,\n2018; Chen et al., 2020a). However, as mentioned, the over-\nhead might be difficult to overcome as we would need to\nperform on-the-fly predictions before every layer. Luckily,\nwe exploit a phenomenon of LLM where token embeddings\nchange slowly across layers due to residual connections (well-\nknown in computer vision (He et al., 2016)). Since the inputs\nto a few consecutive layers are very similar, we can design\nan asynchronous lookahead predictor (Figure 2).\nBased on our findings, we present a system, DEJAVU, that\nexploits contextual sparsity and realizes efficient LLMs for\nlatency-sensitive applications.\n\u2022 In Section 4.1 and Section 4.2, we present a low-cost\nlearning-based algorithm to predict sparsity on the fly.\nGiven the input to a specific layer, it predicts a relevant\nsubset of attention (heads) or MLP parameters in the next\nlayer and only loads them for the computation.\n\u2022 InSection4.3, weproposeanasynchronouspredictor(simi-\nlartoclassicbranchpredictor(Smith,1998))toavoidthese-\nquential overhead. A theoretical guarantee justifies that the\n2\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\ncross-layer design suffices for accurate sparsity prediction.\nAfter integrating hardware-aware implementation of sparse\nmatrix multiply (Section 4.4), DEJAVU (written mostly in\nPython) can reduce latency of open-source LLMs such\nas OPT-175B by over 2\u00d7 end-to-end without quality\ndegradation compared to the state-of-the-art library Faster-\nTransformer from Nvidia (written entirely in C++/CUDA),\nand over 2\u00d7 compared to the widely used Hugging Face\nimplementation at small batch sizes. Furthermore, we show\nseveral ablations on different components of DEJAVU and\nits compatibility with quantization techniques.\n2\nRelated Work and Problem Formulation\nWe first briefly discuss the rich literature on efficient\ninference. Then, we introduce the latency breakdown in our\nsetting. Last, we provide a formal problem formulation.\n2.1\nQuantization, Pruning, Distillation for Inference\nVarious relaxations have been studied for decades for\nmodel inference in machine learning. There are three main\ntechniques: quantization (Han et al., 2015; Jacob et al.,\n2018; Nagel et al., 2019; Zhao et al., 2019), pruning or\nsparsity (Molchanov et al., 2016; Liu et al., 2018; Hoefler\net al., 2021), and distillation (Hinton et al., 2015; Tang et al.,\n2019; Touvron et al., 2021). They are orthogonal areas and\nusually excel in different settings. Recently, there is active\nresearch attempting to apply one or a combination of such\ntechniques in LLM inference (Yao et al., 2022; Park et al.,\n2022; Dettmers et al., 2022; Frantar et al., 2022; Frantar &\nAlistarh, 2023; Bansal et al., 2022; Xiao et al., 2022). More\ndiscussion is presented in Appendix A.\n2.2\nLLM Inference Latency Breakdown\nThe generative procedure of LLMs consists of two phases: (i)\nthe prompt phase takes an input sequence to generate the keys\nand values (KV cache) for each transformer block of LLMs,\nwhich is similar to the forwarding pass of LLMs training;\nand (ii) the token generation phase utilizes and updates the\nKV cache to generate tokens step by step, where the current\ntoken generation depends on previously generated tokens.\nThis paper studies the setting where the token generation\nphase easily dominates the end-to-end inference time. As\nshown in Table 1, generating a sequence of length 128 takes\nmuch longer time than processing a sequence of length 128\nas prompt due to I/O latency of loading model parameters.\nIn addition, Table 2 shows that attention and MLP are both\nbottlenecks in LLMs, e.g., in 175B models, loading MLP\nparameters takes around 2\n3 of the total I/O and attention\nheads take the other 1\n3. Further, in the tensor-parallel regime,\nthere are two communications between GPUs, one after\nthe attention block, and the other one after the MLP block.\nAs shown in Table 3, communication between GPUs takes\naround 15 % token generation latency. This paper focuses on\nmaking attention and MLP more efficient. Communication\ncost implies that the upper bound of such speed-up is around\n6\u00d7 when skipping all transformer blocks.\nTable 1. Theoretical breakdown for prompting versus token genera-\ntion (tensor model parallelism on 8 A100-80G GPUs).\nTFLOPs\nI/O\nCompute Latency (ms)\nI/O Latency (ms)\nPrompting 128\n44.6\n330 GB\n17.87\n20.6\nToken Generation 128\n44.6\n41 TB\n17.87\n2600\nTable 2. Theoretical breakdown for Attention block versus MLP\nblock in one transformer layer when generating one token (tensor\nmodel parallelism on 8 A100-80G GPUs).\nGFLOPs\nI/O (GB)\nCompute Latency (ms)\nI/O Latency (ms)\nAttention Block\n1.21\n1.12\n0.00048\n0.07\nMLP Block\n2.41\n2.25\n0.00096\n0.14\nTable 3. Latency breakdown of generating 1 token under the setting\nof batch size 1 and prompt length 128 on 8 A100-80GB.\nAll Reduce\nMLP Block\nAttention Block (ms)\nOthers\n6 ms\n19ms\n13ms\n2ms\n2.3\nProblem Formulation\nThe goal is to reduce the generation latency of LLMs by\nexploiting contextual sparsity. In the following, we formally\ndefine the sparsified attention and MLP blocks.\nSparsified MLP: There are two linear layers in one MLP\nblock, W 1, W 2 \u2208Rd\u00d74d. Denote y\u2208R1\u00d7d as the input to the\nMLP block in the current generation step. Let each column\n(the weight of i-th neuron) of linear layers be W 1\ni , W 2\ni \u2208\nRd\u00d71. With contextual sparsity, only a small set of them are\nrequired for computation. Let SM \u2286[4d] denote such set of\nneurons for input y. The sparsified MLP computation is\nMLPSM (y)=\u03c3(yW 1\nSM )(W 2\nSM )\u22a4,\n(1)\nwhere \u03c3 is the activation function, e.g., ReLU, GeLU. Note\nthat since the computation in the first linear results in sparse\nactivations, the second linear layer is also sparsified.\nSparsified Attention: Let X \u2208Rn\u00d7d denote the embeddings\nof all tokens (e.g., prompts and previously generated tokens).\nLet y \u2208 R1\u00d7d be the input to the Multi-Head-Attention\n(MHA) in the current generation step. Suppose there are h\nheads. For each i \u2208 [h], we use W K\ni ,W Q\ni ,W V\ni \u2208 Rd\u00d7dh to\ndenote key, query, value projections for the i-th head, and\nW O\ni \u2208Rdh\u00d7d for output projections. With contextual spar-\nsity, we denote SA as a small set of attention heads leading to\napproximately the same output as the full attention for input\ny. Following the notation system in (Alman & Song, 2023),\nsparsified MHA computation can be formally written as\nMHASA(y)=\nX\ni\u2208SA\nHi(y)\n| {z }\n1\u00d7dh\nW O\ni\n|{z}\ndh\u00d7d\n,\nwhere Hi(y):Rd \u2192Rdh and Di(y)\u2208R can be written as\nHi(y):=Di(y)\u22121exp(yW Q\ni (W K\ni )\u22a4X\u22a4)XW V\ni ,\n(2)\nDi(y):=exp(yW Q\ni (W K\ni )\u22a4X\u22a4)1n.\nFor both MLP and Attention, given a compute budget, the\ngoal is to find SM and SA that minimize the error between\nthe sparse approximation and full computation.\n3\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n0\n20\n40\n60\n80\nTransformer Layer\n20%\n40%\n60%\n80%\n100%\n% Not Activated Head\nOPT-30B\nOPT-66B\nOPT-175B\n(a) Contextual sparsity in Attention Head\n0\n20\n40\n60\n80\nTransformer Layer\n90%\n92%\n94%\n96%\n98%\n100%\n% Not Activated Neurons\nOPT-30B\nOPT-66B\nOPT-175B\n(b) Contextual sparsity in MLP Block\nFigure 3. In Figure (a), we plot the percentage of not-activated\nattention heads. By only keeping heads that yield large output\nnorms, we can silence over 80% attention heads for a given token.\nIn Figure (b), we plot the average sparsity we impose on MLP layers.\nWe can zero out over 95% of MLP parameters for a given token.\n3\nPre-trained LLMs are Contextually Sparse\nIn this section, we present several key observations and the-\noretical understandings of sparsity in LLMs, upon which the\nDEJAVU design is based. We first test the contextual sparsity\nhypothesis and verify that contextual sparsity exists in pre-\ntrained LLMs in Section 3.1. Then, we build an understand-\ning of why contextual sparsity happens naturally even when\nLLMs are densely trained in Section 3.2. Finally, we present\nan observation on residual connections and explain their\nrelationship to contextual sparsity analytically in Section 3.3.\n3.1\nContextual Sparsity Hypothesis\nInspired by prior pruning literature (Molchanov et al., 2016),\nwe find a surprisingly simple method is sufficient to study and\nverify our hypothesis. In this section, we describe the testing\nprocedure, observation details, and insights of this study.\nVerification: Our test is performed on OPT-175B, 66B, and\n30B models and various downstream datasets such as Open-\nBookQA (Mihaylov et al., 2018) and Wiki-Text (Merity et al.,\n2016). We find the contextual sparsity for every input exam-\nple with two forward passes of the model. In the first pass, we\nrecord a subset of parameters, specifically which attention\nheadsandMLPneuronsyieldlargeoutputnormsfortheinput.\nIn the second pass, each input example only uses the recorded\nsubset of parameters for the computation. Surprisingly, these\ntwo forward passes lead to similar prediction or performance\non all in-context learning and language modeling tasks.\nObservation: Figure 3 shows that on average, we can impose\nup to 80% sparsity on attention heads and 95% sparsity on\nMLP neurons. As mentioned in Section 2, OPT-175B model\nhas 2\u00d7 MLP parameters than those of attention blocks.\nTherefore total sparsity here is around 85%. Since these are\nall structured sparsity (heads and neurons), predicting them\naccurately could potentially lead to 7\u00d7 speedup.\nInsight: It is intuitive that we can find contextual sparsity in\nMLP blocks at inference time because of their activation func-\ntions, e.g., ReLU or GeLU (Kurtz et al., 2020). Similar obser-\nvations were made by (Li et al., 2022). However, it is surpris-\ning that we can find contextual sparsity in attention layers.\nNote that, finding contextual sparsity in attention is not the\nsame as head pruning. We cross-check that different exam-\nples have different contextual sparsity. Although 80% of the\nparameters are not included in the paths for a given example,\nthey might be used by other examples. Next, we will try to\nunderstand why contextual sparsity exists in attention blocks.\n3.2\nToken Clustering in Attention Layers\nIn the previous section, we have verified that there exists\ncontextual sparsity for a given input in LLMs.\nIn this\nsection, we try to understand the reason for such phenomena,\nespecially in attention layers. We first show an in-depth\nobservation of attention. Then we present a hypothesis that\nself-attentions are conceptually clustering algorithms. Last\nwe show analytical evidence to support this hypothesis.\nObservation: Figure 4 shows the attention map of three\ndifferent heads from the same layer for an example input.\nThe next token it should predict is \u201cTruck\u201d. Darker color\nrepresents higher attention scores. We observe that the\nmiddle head is a relatively uniform token-mixing head\nwhile the top and bottom ones are \u201cheavy hitter\u201d attention\nheads (with high attention to \u201clike\u201d and \u201cshipping\u201d).\nUnsurprisingly, only selecting heavy hitter heads but not\nuniform heads does not affect the prediction, since uniform\nheads do not model or encode important token interactions.\nIn the next section, we will also explain in detail how the\ncriteria for selecting uniform attention heads and heads with\nsmall output norms are highly correlated.\nHypothesis: We hypothesize that the attention head is\nperforming mean-shift clustering (Derpanis, 2005).\nRecall the notation defined in Section 2.3. For i-th head\nat current layer, X = [x1,...,xn]\u22a4 \u2208 Rn\u00d7d are the token\nembeddings in the previous time steps. XW K\ni\nand XW V\ni\nare the projection of embedding. For an input embedding\ny, the output \u02dcyi =Hi(y), where Hi(y) is defined in Eq. 2.\nFor each i\u2208[h], if we let Ki(xj,y):=exp(yW Q\ni (W K\ni )\u22a4xj)\nmeasure the similarity between xj and y, and define\nmi(y):=\nP\njKi(xj,y)xj\nP\njKi(xj,y) , then we have \u02dcyi =mi(y)W V\ni . Fur-\nther, if we set W V\ni =I and consider the residue connection\nfollowed by layer norm, then in the next layer, the embedding\n4\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nLayer L\nThis fruit shipping company provide different vehicle options like car and [MASK]\nTruck\nFigure 4. We visualize the attention scores of three different heads\nfor an exemplary sentence. Head 42 and Head 44 give heavy atten-\ntion scores on particular tokens while Head 43 is more uniform.\n\u02c6yi of the current token becomes \u02c6yi = Normalize(y+ \u02dcyi) =\nNormalize(y+mi(y)), which has a fixed point y=\u03b3mi(y)\nfor any scalar \u03b3. This iteration bears a resemblance to mean-\nshift clustering, which simply performs iteration y\u2190mi(y)\nuntil convergence. This has an obvious fixed point y=mi(y).\nTherefore, the self-attention head can be regarded as one\nmean-shift step to push input embeddings of different tokens\ntogether, if they are already neighbors in a projection space\nspecified by W Q\ni (W K\ni )\u22a4. Different heads learn different\nprojection spaces to perform clustering. These dynamics\nexplain the precise reason why token embeddings tend to\ncluster after going through more layers, resulting in high\nattention scores among cluster members, and low scores for\nnon-members. Furthermore, the cluster patterns are different\nat different heads (More details in Appendix K).\nThe above analysis not only provides an understanding of\nwhy contextual sparsity exists naturally in pre-trained LLMs,\nbut also inspires our design of \u201csimilarity\u201d-based sparsity\nprediction for DEJAVU in Section 4.\n3.3\nSlowly Changing Embeddings across Layers\nWe first present our observation that embeddings change\nslowly across consecutive layers. Then we provide a detailed\nanalysis on the phenomenon. Finally, we show its close\nconnection with contextual sparsity. Details are in Section B.\nHigh similar embeddings in consecutive layers:\nIn\nFigure 5(a), we show that for the same given input, the cosine\nsimilarity between embeddings or activations in two consec-\nutive layers is exceptionally high on 7 different sizes of OPT\nmodels. Specifically, we collect activations from each layer\nwhile performing OPT model inference on C4 validation\nset (Raffel et al., 2019). Taking OPT-175B as an example,\nstarting from the second layer, the similarity between any\ntwo consecutive layers is around 0.99, which indicates that\nwhen an input is passed through the model, the direction of\nits embedding changes slowly. Interestingly, the most drastic\nchange happens in the first layer. Furthermore, we increase\nthe gap and investigate the similarity between the embedding\nat layer l and at layer l + n shown in Figure 5(b). As we\nincrease the gap, the similarity decreases as expected while\nthe differences in cosine similarity between various choices\n125m\n1.3b\n6.7b\n13b\n30b\n66b\n175b\n0.95\n0.96\n0.97\n0.98\n0.99\n1.00\nCosine Similarity\n(a) Model Comparison\n0\n20\n40\n60\n80\nTransformer Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nn = 1\nn = 2\nn = 4\nn = 8\n(b) Across Layer\n0\n20\n40\n60\n80\nTransformer Layer\n0\n500\n1000\n1500\n2000\nNorm\n||X||\n||F(X)||\n(c) Residual Around Attention\n0\n20\n40\n60\n80\nTransformer Layer\n0\n500\n1000\n1500\n2000\n2500\nNorm\n||X||\n||F(X)||\n(d) Residual Around MLP\nFigure 5. Slowly Changing Embedding. Figure (a) shows the\nmediancosinesimilaritybetweenrepresentationsattwoconsecutive\nlayers across all layers for different OPT models. All models show\na similarity greater than 95%. Figure (b) shows cosine similarity\nstays high even a few layers apart. For the residual connection\nX\u2032 =X+F(X) inside each block, we plot the \u21132 norm of X and\nF(X) in Figure (c) and Figure (d). \u2225X\u2225 is significantly higher than\n\u2225F(X)\u2225, which explains the slowly changing embedding.\nof n are smaller at the shallower layer. We plot the mean sim-\nilarity, and the standard deviation is indicated by the shading.\nSimilar plots on more models are presented in Appendix B.\nConnection to residuals: We verify that the high similarity\nin embeddings in LLM inference is due to the residual\nconnection. We first dissect the computation graph inside\neach transformer layer to understand the cause behind\nthis phenomenon.\nThere are two residual connections\ninside a transformer layer, one around the attention block,\nand the other one around the MLP block. The residual\nconnection can be written as X +F(X), where F is either\nthe Multi-Head Attention or two MLP Layers. In Figure 5(c)\nand Figure 5(d), indeed we can see that \u2225X\u2225 is significantly\ngreater than \u2225F(X)\u2225, confirming that embeddings are\nchanging slowly because the residual norm is large.\nConnection to Contextual Sparsity: We take a step deeper\ntrying to understand the reason behind the large residual\nnorm with mathematical modeling. We discover that one pos-\nsible reason for small \u2225F(X)\u2225 is due to high sparsity. For the\nMLP Block, high sparsity may contribute to the small norm\nof F(X) because a large portion of outputs have small norms.\nSimilar reasoning applies to the Attention Block, and thus\na large number of attention heads yield small norm outputs.\nResidual Two Sides Bound: Besides empirical reasoning,\n5\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nwe formally define the computation of LLMs mathematically.\nUnder our computation model, we can show that a shrinking\nproperty which is observed by our practical experiments.\nProofs are in Appendix G, H, I.\nLemma 3.1 (Informal). Let 0<\u03f51 <\u03f52 <1 be the lower and\nupper bound of the shrinking factor. Let x be the y be the\noutput. We have the residual connection y =x+F(x). For\nthe MLP block F(x), we have \u03f51 \u2264 \u2225y \u2212x\u22252 \u2264 \u03f52. For the\nattention block F(x), we have \u03f51 \u2264\u2225y\u2212x\u22252 \u2264\u03f52.\n4\nDEJAVU\nIn this section, we present our framework for inference-time\ncontextual sparsity search for LLMs. We introduce the\nsparsity predictor for MLPs in Section 4.1 and for attention\nheads in Section 4.2.\nDEJAVU\u2019s workflow is shown in\nFigure 2. Section 4.3 discusses exploiting our observation\non LLMs to avoid the sparse prediction overhead with\ntheoretical guarantees.\nIn Section 4.4, we present our\noptimized implementation that enables end-to-end latency\nreduction. More details are presented in Section D.\n4.1\nContextual Sparsity Prediction in MLP Blocks\nAs explained in Section 2, MLP blocks are one of the major\nbottlenecks for the LLM generation ( 2\n3 of the FLOPs and\nIOs). In this section, we discuss how we achieve wall-clock\ntime speed-up with contextual sparsity in the MLP blocks.\nChallenge Figure 3(b) shows that for a given token, the\ncontextual sparsity of 95% is possible.\nThe contextual\nsparsity in the MLP block can be identified after computing\nthe activation. However, this only demonstrates the existence\nof contextual sparsity but brings no benefits in terms of\nefficiency. A fast and precise prediction is needed to exploit\ncontextual sparsity for end-to-end efficiency. The naive way\nis to select a subset of neurons randomly. Unsurprisingly,\nrandom selection fails to identify the accurate contextual\nsparsity, resulting in drastic model degradation.\nA Near-Neighbor Search Problem: Recall that we verify\nthe existence of contextual sparsity by recording which\nneurons yield significant norms. Essentially, given the input,\nthe goal is to search for the neurons that have high inner prod-\nucts with the input, because the activation function \u201cfilters\"\nlow activation. Thus, we formulate the contextual sparsity\nprediction of an MLP layer as the classical near-neighbor\nsearch problem under the inner product metric.\nDefinition 4.1 (Approximate MaxIP in MLP). Let c\u2208(0,1)\nand \u03c4 \u2208 (0,1) denote two parameters. Given an n-vector\ndataset W 1 \u2282 Sd\u22121 on a unit sphere, the objective of the\n(c,\u03c4)-MaxIP is to construct a data structure that, given a\nquery y\u2208Sd\u22121 such that maxw\u2208W 1\u27e8y,w\u27e9\u2265\u03c4, it retrieves a\nvector z from W 1 that satisfies \u27e8y,z\u27e9\u2265c\u00b7maxw\u2208W 1\u27e8y,w\u27e9.\nRemark 4.2. Our W 1 (first linear layer) and y (input\nembedding) in MLP blocks can be viewed as the dataset and\nquery in Definition 4.1 respectively.\nDesign The standard state-of-the-art near-neighbor search\nmethods and implementations slow down the computa-\ntion. Take OPT-175B where d is 12288 as an example.\nHNSW (Malkov & Yashunin, 2018) requires more than 10ms,\nand FAISS (Johnson et al., 2019) requires more than 4ms,\nwhile the MLP computation is only 0.2ms. The high dimen-\nsionality and complications of data structure implementation\non GPU make the search time longer than the MLP computa-\ntion. Therefore, we choose a neural network classifier as our\nnear-neighbor search method to exploit the fast matrix multi-\nplication on GPU. For each MLP block, we train a small two-\nlayer fully connected network to predict contextual sparsity.\nCollecting training data is straightforward because we know\nthecontextualsparsityusingdensecomputation. Thetraining\nalgorithm is summarized in Algorithm 1. The sparsified com-\nputation in W 1 has two steps: (1) Given y, the sparsity predic-\ntor SPM predicts a set SM of important neurons in weights\nW 1. (2) Compute the sparsified MLP defined in Eq. equa-\ntion 1. Note here the sparsity in MLP is highly structured.\nAlgorithm 1 Sparse Predictor Training\nInput: A pre-trained LLM block with parameter set M,\ntoken embedding set at block M ={xi}i\u2208[N], threshold t\nSparse Predictor SP\nP+ \u2190\u2205, P\u2212 \u2190\u2205\nfor i=1\u2192N do\nP+ \u2190P+\u222a{(xi,mr) | mr \u2208M,mr(xi)\u2265t}\nP\u2212 \u2190P\u2212\u222a{(xi,mr) | mr \u2208M,mr(xi)<t}\nend for\nSP \u2190TRAIN(P+,P\u2212,L)\n\u25b7 L is a loss function\n4.2\nContextual Sparsity Prediction in Attention Blocks\nAttention blocks take around 30% I/Os in the generation. In\nthis section, we describe how DEJAVU exploits contextual\nsparsity to speed up the Attention blocks.\nChallenge: As discussed in Section 3.1, only a few heads per-\nform important computations for a given input token. Similar\nto the MLP blocks, a fast selection of attention heads without\nfull computation is required to reduce end-to-end latency.\nFurthermore, one particular challenge of sparse prediction in\nattention blocks is attention\u2019s dependence on previous tokens.\nOn the one hand, it is unclear whether the past token\u2019s key and\nvalue caches are needed for sparse prediction. On the other\nhand, it is unclear how to handle the missing KV cache of past\ntokens for the current token computation at the selected head.\nA Near-Neighbor Search Problem:\nHead prediction\ncan also be formulated as a near-neighbor search problem\nbased on our understanding in Section 3.2. Since each\nhead is performing mean-shift clustering, after the first\nfew layers, the current token embedding alone is sufficient\nfor the prediction thanks to the token-mixing nature of the\ntransformer. Therefore, the prediction can be based on the\nsimilarity between y and head parameters.\n6\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nApproach: We design our attention sparse predictor to be\nthe same architecture as the MLP sparse predictor. Each head\nis regarded as one class and a similar training process is used\n(Algorithm 1). Then, similar to how MLP prediction is per-\nformed, the attention sparsity predictor SPA selects a set SA\nof heads Hi (see Eq. equation 2). To address the problem of\nmissing KV cache for a past token, we exploit the fact that the\ngenerationlatencyis I/Obounded whilecomputationis essen-\ntially \u201cfree\". Specifically, for the predicted attention head of\ninput y, we compute the corresponding keys, and values and\nstore them in the KV cache. But we also save a copy of y for\nall the other non-selected heads. Then during the future token\ngeneration, if there is missing KV cache in the selected heads,\nwe could load stored token embeddings and compute the\nkeys and values together. This requires almost minimal extra\nmemory access (the main cost is loading the weight matrices).\n4.3\nReducing Overhead with Asynchronous Execution\nSparse prediction overhead may easily increase the end-to-\nend latency rather than reduce it despite the reduction in\nFLOPs. Therefore, we introduce a look-ahead sparse pre-\ndiction method, inspired by our observations in Section 3.3.\nChallenge: Denote yl \u2208 Rd as the input to transformer\nlayer l. We can write the computation at layer l as eyl \u2190\nMHAl(yl),byl \u2190MLPl(eyl). With predictors SPl\nA and SPl\nM,\nthe computation at the transformer layer l can be re-written as\nSl\nA \u2190SPl\nA(yl),\neyl \u2190MHAl\nSl\nA(yl),\nSl\nM \u2190SPl\nM(eyl),\nbyl \u2190MLPl\nSl\nM (eyl)\nwhere set Sl\nA is the contextual sparsity for the Attention\nblock, and set Sl\nM is the contextual sparsity for the MLP\nblock at l-th layer. Note that the computation at Attention\nand MLP blocks have to wait for the sparse predictor\ndecision. This overhead potentially outweighs the saving\nfrom Attention and MLP blocks in terms of latency.\nApproach: In Section 3.3, we present the slowly evolving\nembedding phenomenon, which provides opportunities to\nrelax the sequential computation to parallel computation.\nAlong with the observation of low computation intensity\nduring generation, we parallel the sparse prediction with the\ncomputation of each block ( See Figure 2). The computation\ncan be written as follows:\neyl \u2190MHAl\nSl\nA(yl),\nbyl \u2190MLPl\nSl\nM (eyl),\nSl+1\nA\n\u2190SPl\nA(yl),\nSl+1\nM \u2190SPl\nM(yl),\nWe remark Sl+1\nA\nand Sl+1\nM\ncan be computed in parallel with\neyl or byl, while the previous 4 steps are sequential.\nTheoretical guarantee: The sparse predictor can make fur-\nther cross-layer decisions because of the residual connection.\nWe present an informal lemma statement regarding cross-\nlayer prediction. It is well-known that MaxIP is equivalent to\n\u21132 nearest neighbor search. For convenience, we use MaxIP\nhere. We include more discussions and proofs in Section J.\nLemma 4.3 (Informal). Let \u03f5 \u2208 (0,1). Let yl be input at\nl-th layer. Let yl\u22121 be the input at (l\u22121)-th layer. Suppose\nthat \u2225yl \u2212 yl\u22121\u22252 \u2264 \u03f5. For any parameters c,\u03c4 such that\n\u03f5 < O(c\u03c4). Then we can show that, solving MaxIP(c,\u03c4) is\nsufficient to solve MaxIP(0.99c,\u03c4).\n4.4\nHardware-efficient Implementation\nWe describe how DEJAVU is implemented in a hardware-\nefficient manner to realize the theoretical speedup of contex-\ntual sparsity. Taking into account hardware characteristics\nleads to over 2\u00d7 speedup compared to an optimized dense\nmodel, and 4\u00d7 faster than a standard sparse implementation.\nWe highlight some hardware characteristics of GPUs:\n\u2022 Small-batch generation is bottlenecked by GPU memory\nI/Os (NVIDIA, 2022; Ivanov et al., 2021; Dao et al., 2022).\nThis is because of low arithmetic intensity. For each\nelement loaded from GPU memory, only a small number\nof floating point operations are performed.\n\u2022 GPUs are block-oriented devices: loading a single byte of\nmemory takes the same time as loading a block of memory\naround that same address (Harris, 2013). The block size\nis usually 128 bytes for NVIDIA GPUs (Cook, 2012).\nThese characteristics present some challenges in implement-\ning contextual sparsity. However, they can be addressed with\nclassical techniques in GPU programming.\nKernel fusion:\nA standard implementation of sparse\nmatrix-vector multiply (e.g., in PyTorch) that separately\nindexes a subset of the matrix W 1\nSM before multiplying\nwith input y would incur 3\u00d7 the amount of memory I/Os.\nTherefore, to avoid such overhead, we fuse the indexing\nand the multiplication step. Specifically, we load a subset\nof W 1\nSM to memory, along with y, perform the multiply,\nthen write down the result. This fused implementation (in\nTriton (Tillet et al., 2019)) yields up to 4\u00d7 speedup compared\nto a standard PyTorch implementation (Appendix E).\nMemory coalescing: In the dense implementation, the\nweight matrices of two linear layers in MLP are stored\nas (W 1)\u22a4 and W 2 so that no extra transpose operation is\nneeded. They are conventionally stored in row-major format.\nIn the sparse implementation, it allows us to load (W 1\nSM )\u22a4\noptimally (the second dimension is contiguous in memory).\nHowever, for cases where we need to load (W 2\nSM ), this\nformat significantly slows down memory loading, as indices\nin SM point to non-contiguous memory. We simply store\nthese matrices in column-major format (i.e., store (W 2)\u22a4\nin row-major format), then use the same fused kernel above.\nSimilarly, in attention blocks, we store attention output\nprojection W O column-major format.\nThese two techniques (kernel fusion and memory-\ncoalescing) make DEJAVU hardware-efficient, yielding up\nto 2\u00d7 speedup end-to-end compared to the state-of-the-art\nFasterTransformer (Section 5.1).\n7\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n0\n20\n40\n60\n80\n100\nSparsity percentage (%)\n8\n9\n10\n11\n Perplexity\nWikiText\nc4\n(a) Language Modeling\n0\n20\n40\n60\n80\n100\nSparsity percentage (%)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\n0\n20\n40\n60\n80\n100\nSparsity percentage (%)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nCB\nCOPA\nLambada\nOpenbookQA\nPIQA\nRTE\nWinogrande\n(b) Zero-Shot(Left). Five-Shot(Right)\nFigure 6. Accuracy Trend for DEJAVU-OPT-175B. This figure shows the accuracy of DEJAVU-OPT-175B on language modeling datasets\nand downstream tasks when we set different sparsity at test time. In general, DEJAVU-OPT-175B incurs no accuracy drop until 75% sparsity.\n128\n256\n512\n1024\nSequence Length\n0\n20\n40\n60\n80\n100\nLatency(ms)\nHuggingFace\nFasterTransformer\nDejaVu\nFigure 7. Average per-token latency (ms) with batch size 1 on 8\nA100-80GB with NVLink when generating sequences with prompt\nlengths 128, 256, 512, and 1024, using FP16. DEJAVU speeds up\ngeneration by 1.8-2\u00d7 compared to the state-of-the-art FT and by\n4.8-6\u00d7 compared to the widely used HF implementation.\n5\nEmpirical Evaluation\nIn Section 5.1, we present the end-to-end results that show\nDEJAVU achieves over 2\u00d7 reduction in token generation\nlatency compared to the state-of-the-art FasterTransformer\nand over 6\u00d7 compared to Hugging Face with no accuracy\nloss. In Section 5.2, we perform a list of ablation studies such\nas independent evaluation on the inference-time contextual\nsparsity of the MLP block and the Attention block (Details\nare presented in Section C). At last, we present the additional\nresults to demonstrate the future possibility of sparsifying\nthe entire LLMs via layer skipping in Section C.3.\n5.1\nEnd-to-End Result\nExperiment Setting: We compare the accuracy of DE-\nJAVU-OPT against the original OPT model on two lan-\nguage modeling datasets Wiki-Text (Merity et al., 2016)\nand C4 (Raffel et al., 2019) and seven few-shot downstream\ntasks: CB (de Marneffe et al., 2019), COPA (Gordon et al.,\n2012), Lambada (Radford et al., 2019), OpenBookQA (Mi-\nhaylov et al., 2018), PIQA (Bisk et al., 2020), RTE (Giampic-\ncolo et al., 2007), Winogrande (ai2, 2019). We use lm-eval-\nharness (Gao et al., 2021) for zero-shot and five-shot tasks.\nWe collect training data for the sparsity predictor using 500\nrandom data points from the C4 training dataset. Our exper-\niments are conducted on NVIDIA A100 80GB GPU servers.\nNo accuracy drop until 75% sparsity: In Figure 6, we\npresent DEJAVU-OPT-175B\u2019s accuracy trend. In a zero-shot\nsetting, the average accuracy across tasks does not drop\nuntil 75% sparsity. A similar trend can be observed for\nthe five-shot setting, which verifies the model\u2019s ability for\nin-context learning. This result is exceptionally encouraging\ngiven our observation in Figure 1(a), where we could impose\n85% sparsity when allowed full computation.\nOver 2\u00d7 latency reduction: Figure 7 presents the latency\nspeed-up for the token generation with OPT-175B at batch\nsize 1, where DEJAVU achieves the best performance. At\naround 75% sparsity, DEJAVU speeds up generation by\n1.8-2\u00d7 compared to the state-of-the-art FasterTransformers\n(FT)1 and by 4.8-6\u00d7 to Hugging Face (HF) implementation2.\n5.2\nAblation Results\nContextual Sparsity for Larger Batches: Although this\npaper focuses on latency-sensitive settings, we demonstrate\nthat DEJAVU generalizes to larger batches. we present the\nUnion contextual sparsity (fraction of neurons/heads that are\nnot used by any of the inputs in the batch) of different batches\nsizes for MLP and Attention blocks, respectively, in Fig-\nure 8 and 11. The union operation is essential to realize a fast\nsparse GEMM. Surprisingly the number of MLP neurons and\nAttention heads that DEJAVU activated does not grow linearly\nwith the batch size. This suggests a power law distribution\nrather than a uniform distribution of parameter access from\nall input examples. This provides an opportunity for poten-\ntially extending Dejavu to the high-throughout setting. For\nexample, we can first pre-process the inputs and batch similar\ninputs to enjoy a higher level of union contextual sparsity.\nContextual sparsity on MLP blocks: We study the contex-\ntual sparsification of the MLP block in OPT-175B. We leave\nthe Attention block as dense computation. Table 4 shows\nthe model performance at 85% sparsity. The MLP sparse\npredictor introduces no accuracy loss on both zero-shot tasks\nand language modeling. In the training of the MLP sparse\npredictor, we observe that the sparse predictor achieves high\nvalidation accuracy. The shallow layer seems easier to model\n1http://github.com/NVIDIA/FasterTransformer\n2http://github.com/huggingface/transformers\n8\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nTable 4. Accuracy of zero-shot tasks and language modeling when sparsifying the MLP block and the Attention block separately. The\nsparsity is set at 85% for MLP-block and 50% for Attention-block. DEJAVU incurs no accuracy drop across the boards.\nModel\nCB\nCOPA\nLambada\nOpenBookQA\nPIQA\nRTE\nWinogrande\nWikitext\nC4\nOPT-175B\n0.3523\n0.86\n0.7584\n0.446\n0.8096\n0.6029\n0.7261\n10.8221\n7.7224\nDEJAVU-MLP-OPT-175B\n0.3544\n0.85\n0.7619\n0.446\n0.8096\n0.6065\n0.7206\n10.7988\n7.7393\nDEJAVU-Attention-OPT-175B\n0.3544\n0.86\n0.7586\n0.4460\n0.8063\n0.5921\n0.7245\n10.8696\n7.7393\nTable 5. DEJAVU-OPT66B on zero-shot downstream task.\nModel\nCB\nCOPA\nLambada\nOpenBookQA\nPIQA\nRTE\nWinogrande\nOPT-66B\n0.3928\n0.87\n0.7508\n0.426\n0.7921\n0.6028\n0.6890\nDEJAVU-OPT-66B\n0.4285\n0.87\n0.7458\n0.434\n0.7933\n0.5884\n0.6898\nTable 6. DEJAVU-BLOOM on zero-shot downstream task.\nCB\nCOPA\nOpenBookQA\nPIQA\nRTE\nWinogrande\nLambada\nBLOOM\n0.455\n0.8\n0448\n0.79\n0.617\n0.704\n0.677\nDejavu-BLOOM\n0.448\n0.8\n0.44\n0.787\n0.606\n0.710\n0.675\n0\n20\n40\n60\n80\n96\nTransformer Layer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nUnion Contextual Sparsity\nBatch size\n2\n4\n8\n16\n32\nFigure 8. Union contextual sparsity with larger batch size.\nbecause the predictor has validation accuracy over 99% in the\nshallow layers and drops to around 93% in the ending layers.\nContextual sparsity on attention blocks: In this section,\nwe study the sparse predictor for the Attention block on OPT-\n175B and leave the MLP block as dense computation. Table 4\ndisplaysthetestaccuracyonzero-shottasksandperplexityon\nthe language modeling datasets. In summary, the Attention\nsparse predictor introduces no accuracy loss at around 50%\nsparsity. During the training of the Attention sparse predictor,\nwe observe different trends compared to the MLP sparse\npredictor. The validation accuracy is around 93% in the\nmiddle layers and near 99% in the shallow and deep layers.\nContextual Sparsity on Smaller Models: Our main exper-\niments focus on OPT-175B. Here, we verify DEJAVU\u2019s effec-\ntiveness on a smaller model, specifically OPT-66B. In Table 5,\nwe summarize the accuracy on zero-shot task at 50% sparsity.\nSimilar to DEJAVU-OPT-175B, we notice no accuracy loss.\nContextual Sparsity on Other Models:\nWe expand\nthe evaluation to another model family. In Table 6, we\nsummarize the accuracy at attention sparsity 50% and MLP\nsparsity 30%. Similar to OPT family, we notice no accuracy\nloss. The lower sparsity level in MLP is due to the difference\nin activation function.\nNon-Contextual Sparsity: As we mentioned in Section 1,\none could predict sparsity without contextual information.\nFor non-contextual sparsity, we rely on the original\nTable 7. DEJAVU-OPT-175B with 4-bit quantization.\nCB\nCOPA\nOpenBookQA\nPIQA\nRTE\nWinogrande\nLambada\nOPT-175B\n0.352\n0.86\n0.446\n0.809\n0.602\n0.726\n0.758\nDejavu-OPT-175B\n0.402\n0.85\n0.450\n0.802\n0.592\n0.726\n0.753\nOPT-175B + W4A16\n0.356\n0.85\n0.44\n0.806\n0.574\n0.714\n0.757\nDejavu-OPT-175B + W4A16\n0.365\n0.86\n0.452\n0.805\n0.592\n0.726\n0.754\nembedding at the input layer. At every block, we first pass\nthe original embedding to record a subset of parameters\nyielding a large norm. In the second pass, the embedding\nat every layer only uses the recorded subset. As shown in\nFigure 1, non-contextual prediction is not sufficient and\nleads to accuracy losses even at 50% sparsity. This result\nverifies our design choices of relying on the activation at\nevery layer as input to make contextual sparsity predictions.\nCompatibility with Quantization: Quantization is another\npromising direction for efficient language models. We inves-\ntigate the possibility of combining contextual sparsity with\nquantization techniques. For DEJAVU-OPT-175B, we set\nthe entire model sparsity at 75%. For quantization, we apply\n4-bit quantization on model weights (W4A16). As shown\nin Table 7, the combination of quantization and DEJAVU\nalmost always achieves better accuracy than DEJAVU or\nquantization alone. This suggests that the approximation\nerrors from these two directions do not get compounded.\n6\nConclusion\nOur main goal is to make LLM inference efficient so that\ntheir powerful in-context learning abilities can be used\nin more application domains. We observe that contextual\nsparsity can be accurately predicted with lightweight\nlearning-based algorithms. This motivated us to design\nDEJAVU that uses asynchronous lookahead predictors and\nhardware-efficient sparsity to speed up LLM inference in\nwall-clock time. Our encouraging empirical results validate\nthat contextual sparsity can reduce inference latency by\nover 2\u00d7 compared to the state-of-the-art FasterTransformer\nwithout model quality drops. Our method is a step towards\nmaking LLMs more accessible to the general community,\nwhich could unlock exciting new AI applications.\nAcknowledgements\nWe would like to thank Ryan Spring, Laurel Orr, Guangxuan\nXiao, Eric Han, Xun Huang, Daniel Y. Fu, Benjamin Spector,\nRuan Silva, Diana Liskovich, and the anonymous reviewers\nfor helpful discussions and feedback. We acknowledge the\ngenerous support by Together Computer, which enabled the\nnecessary partial computations in this work.\n9\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nReferences\nWinogrande: An adversarial winograd schema challenge\nat scale. 2019.\nAllen-Zhu, Z. and Li, Y. What can resnet learn efficiently,\ngoing beyond kernels? Advances in Neural Information\nProcessing Systems, 32, 2019.\nAlman, J. and Song, Z. Fast attention requires bounded\nentries. arXiv preprint arXiv:2302.13214, 2023.\nAlman, J., Liang, J., Song, Z., Zhang, R., and Zhuo, D. By-\npass exponential time preprocessing: Fast neural network\ntraining via weight-data correlation preprocessing. arXiv\npreprint arXiv:2211.14227, 2022.\nAlon, N., Matias, Y., and Szegedy, M. The space complexity\nof approximating the frequency moments. In Proceedings\nof the twenty-eighth annual ACM symposium on Theory\nof computing, pp. 20\u201329, 1996.\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C.,\nLi, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M.,\nRasley, J., et al. Deepspeed-inference: Enabling efficient\ninference of transformer models at unprecedented scale.\nIn 2022 SC22: International Conference for High Per-\nformance Computing, Networking, Storage and Analysis\n(SC), pp. 646\u2013660. IEEE Computer Society, 2022.\nAndoni, A. and Razenshteyn, I. Optimal data-dependent\nhashing for approximate near neighbors. In Proceedings\nof the forty-seventh annual ACM symposium on Theory\nof computing (STOC), pp. 793\u2013801, 2015.\nAndoni, A., Indyk, P., Nguyen, H. L., and Razenshteyn, I.\nBeyond locality-sensitive hashing. In Proceedings of the\ntwenty-fifth annual ACM-SIAM symposium on Discrete\nalgorithms, pp. 1018\u20131028. SIAM, 2014.\nAndoni, A., Indyk, P., Laarhoven, T., Razenshteyn, I., and\nSchmidt, L. Practical and optimal lsh for angular distance.\nIn Advances in Neural Information Processing Systems\n(NIPS), pp. 1225\u20131233. Curran Associates, 2015.\nAndoni, A., Laarhoven, T., Razenshteyn, I., and Waingarten,\nE.\nOptimal hashing-based time-space trade-offs for\napproximate near neighbors.\nIn Proceedings of the\nTwenty-Eighth Annual ACM-SIAM Symposium on\nDiscrete Algorithms (SODA), pp. 47\u201366. SIAM, 2017.\nAndoni, A., Indyk, P., and Razenshteyn, I. Approximate\nnearest neighbor search in high dimensions.\narXiv\npreprint arXiv:1806.09823, 7, 2018.\nArya, S. and Mount, D. M. Approximate nearest neighbor\nqueries in fixed dimensions. In SODA, volume 93, pp.\n271\u2013280. Citeseer, 1993.\nBalduzzi, D., Frean, M., Leary, L., Lewis, J., Ma, K. W.-D.,\nand McWilliams, B. The shattered gradients problem:\nIf resnets are the answer, then what is the question? In\nInternational Conference on Machine Learning, pp.\n342\u2013350. PMLR, 2017.\nBansal, H., Gopalakrishnan, K., Dingliwal, S., Bodapati, S.,\nKirchhoff, K., and Roth, D. Rethinking the role of scale for\nin-context learning: An interpretability-based case study\nat 66 billion scale. arXiv preprint arXiv:2212.09095, 2022.\nBaum, L. E. and Petrie, T. Statistical inference for proba-\nbilistic functions of finite state markov chains. The annals\nof mathematical statistics, 37(6):1554\u20131563, 1966.\nBello, I., Fedus, W., Du, X., Cubuk, E. D., Srinivas, A., Lin,\nT.-Y., Shlens, J., and Zoph, B. Revisiting resnets: Im-\nproved training and scaling strategies. Advances in Neural\nInformation Processing Systems, 34:22614\u201322627, 2021.\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A\nneural probabilistic language model. Journal of machine\nlearning research (JMLR), 3(Feb):1137\u20131155, 2003.\nBisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y.\nPiqa: Reasoning about physical commonsense in natural\nlanguage. In Thirty-Fourth AAAI Conference on Artificial\nIntelligence, 2020.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B:\nAn open-source autoregressive language model.\nIn\nProceedings of the ACL Workshop on Challenges &\nPerspectives in Creating Large Language Models, 2022.\nURL https://arxiv.org/abs/2204.06745.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora,\nS., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A.,\nBrunskill, E., et al. On the opportunities and risks of foun-\ndation models. arXiv preprint arXiv:2108.07258, 2021.\nBoutsidis, C., Woodruff, D. P., and Zhong, P.\nOptimal\nprincipal component analysis in distributed and streaming\nmodels. In STOC\u201916\u2014Proceedings of the 48th Annual\nACM SIGACT Symposium on Theory of Computing, 2016.\nBoytsov, L., Novak, D., Malkov, Y., and Nyberg, E. Off the\nbeaten path: Let\u2019s replace term-based retrieval with k-nn\nsearch. In Proceedings of the 25th ACM international on\nconference on information and knowledge management\n(CIKM), pp. 1099\u20131108, 2016.\nBrand, J. v. d., Peng, B., Song, Z., and Weinstein, O. Training\n(overparametrized) neural networks in near-linear time.\nIn ITCS, 2021.\n10\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nBrand, J. v. d., Song, Z., and Zhou, T.\nAlgorithm and\nhardness for dynamic attention maintenance in large\nlanguage models. arXiv preprint arXiv:2304.02207, 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nChan, S. C., Santoro, A., Lampinen, A. K., Wang, J. X.,\nSingh, A. K., Richemond, P. H., McClelland, J., and\nHill, F.\nData distributional properties drive emergent\nin-context learning in transformers. In Advances in Neural\nInformation Processing Systems, 2022.\nChang, W.-C., Yu, F. X., Chang, Y.-W., Yang, Y., and Kumar,\nS. Pre-training tasks for embedding-based large-scale\nretrieval. arXiv preprint arXiv:2002.03932, 2020.\nCharikar, M., Chen, K., and Farach-Colton, M. Finding\nfrequent items in data streams. In International Collo-\nquium on Automata, Languages, and Programming, pp.\n693\u2013703. Springer, 2002.\nChen, B., Xu, Y., and Shrivastava, A. Fast and accurate\nstochastic gradient estimation.\nAdvances in Neural\nInformation Processing Systems, 32, 2019.\nChen, B., Medini, T., Farwell, J., Tai, C., Shrivastava, A., et al.\nSlide: In defense of smart algorithms over hardware accel-\neration for large-scale deep learning systems. Proceedings\nof Machine Learning and Systems, 2:291\u2013306, 2020a.\nChen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and R\u00e9,\nC. Scatterbrain: Unifying sparse and low-rank attention.\nAdvances in Neural Information Processing Systems, 34:\n17413\u201317426, 2021a.\nChen, B., Liu, Z., Peng, B., Xu, Z., Li, J. L., Dao, T., Song,\nZ., Shrivastava, A., and Re, C. Mongoose: A learnable lsh\nframework for efficient neural network training. In Inter-\nnational Conference on Learning Representations, 2021b.\nChen, H., Chillotti, I., Dong, Y., Poburinnaya, O., Razen-\nshteyn, I., and Riazi, M. S.\n{SANNS}: Scaling up\nsecure approximate k-nearest neighbors search. In 29th\n{USENIX} Security Symposium ({USENIX} Security 20),\npp. 2111\u20132128, 2020b.\nChen, L. On the hardness of approximate and exact (bichro-\nmatic) maximum inner product. In 33rd Computational\nComplexity Conference (CCC), 2018.\nCho, J. H. and Hariharan, B. On the efficacy of knowledge\ndistillation. In Proceedings of the IEEE/CVF international\nconference on computer vision, pp. 4794\u20134802, 2019.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. PaLM: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nClarkson, K. L. and Woodruff, D. P. Low-rank approximation\nand regression in input sparsity time. In STOC, 2013.\nCohen, M. B. Nearly tight oblivious subspace embeddings\nby trace inequalities. In Proceedings of the twenty-seventh\nannual ACM-SIAM symposium on Discrete algorithms,\npp. 278\u2013287. SIAM, 2016.\nCook, S. CUDA Programming: A Developer\u2019s Guide to\nParallel Computing with GPUs.\nMorgan Kaufmann\nPublishers Inc., San Francisco, CA, USA, 1st edition,\n2012. ISBN 9780124159334.\nCox, M. and Cox, T. Multidimensional scaling, 315\u2013347.\nHandbook of data visualization. Springer,\nBerlin,\nGermany, 2008.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C.\nFlashattention: Fast and memory-efficient exact attention\nwith io-awareness. In Advances in Neural Information\nProcessing Systems, 2022.\nDatar, M., Immorlica, N., Indyk, P., and Mirrokni, V. S.\nLocality-sensitive hashing scheme based on p-stable distri-\nbutions. InProceedingsofthetwentiethannualsymposium\non Computational geometry (SoCG), pp. 253\u2013262, 2004.\nde Marneffe, M.-C., Simons, M., and Tonhauser, J. The\ncommitmentbank: Investigating projection in naturally\noccurring discourse. 2019.\nDeng, Y., Li, Z., and Song, Z. Attention scheme inspired soft-\nmax regression. arXiv preprint arXiv:2304.10411, 2023a.\nDeng, Y., Mahadevan, S., and Song, Z. Randomized and\ndeterministic attention sparsification algorithms for\nover-parameterized feature dimension. arxiv preprint:\narxiv 2304.03426, 2023b.\nDerpanis, K. G. Mean shift clustering. Lecture Notes, 32:\n1\u20134, 2005.\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.\nLlm. int8 (): 8-bit matrix multiplication for transformers\nat scale. arXiv preprint arXiv:2208.07339, 2022.\nDong, S., Lee, Y. T., and Ye, G.\nA nearly-linear time\nalgorithm for linear programs with small treewidth: A\nmultiscale representation of robust central path.\nIn\nProceedings of the 53rd Annual ACM SIGACT Symposium\non Theory of Computing, pp. 1784\u20131797, 2021.\nDong, Y., Indyk, P., Razenshteyn, I., and Wagner, T. Learning\nspace partitions for nearest neighbor search. In Interna-\ntional Conference on Learning Representations, 2019.\n11\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nFang, J., Yu, Y., Zhao, C., and Zhou, J. Turbotransformers:\nan efficient gpu serving system for transformer models.\nIn Proceedings of the 26th ACM SIGPLAN Symposium\non Principles and Practice of Parallel Programming, pp.\n389\u2013402, 2021.\nFrankle, J. and Carbin, M. The lottery ticket hypothesis:\nFinding sparse, trainable neural networks. arXiv preprint\narXiv:1803.03635, 2018.\nFrantar, E. and Alistarh, D.\nMassive language models\ncan be accurately pruned in one-shot.\narXiv preprint\narXiv:2301.00774, 2023.\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:\nAccurate post-training quantization for generative pre-\ntrained transformers. arXiv preprint arXiv:2210.17323,\n2022.\nFrei, S., Cao, Y., and Gu, Q. Algorithm-dependent gen-\neralization bounds for overparameterized deep residual\nnetworks. Advances in neural information processing\nsystems, 32, 2019.\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,\nC., Golding, L., Hsu, J., McDonell, K., Muennighoff, N.,\nPhang, J., Reynolds, L., Tang, E., Thite, A., Wang, B.,\nWang, K., and Zou, A. A framework for few-shot lan-\nguage model evaluation, September 2021. URL https:\n//doi.org/10.5281/zenodo.5371628.\nGao, Y., Mahadevan, S., and Song, Z. An over-parameterized\nexponential regression. arXiv preprint arXiv:2303.16504,\n2023a.\nGao, Y., Song, Z., and Yang, X.\nDifferentially private\nattention computation. arXiv preprint arXiv:2305.04701,\n2023b.\nGiampiccolo, D., Magnini, B., Dagan, I., and Dolan, B. The\nthird PASCAL recognizing textual entailment challenge.\nIn Proceedings of the ACL-PASCAL Workshop on Textual\nEntailment and Paraphrasing, pp. 1\u20139, Prague, June\n2007. Association for Computational Linguistics. URL\nhttps://aclanthology.org/W07-1401.\nGionis, A., Indyk, P., Motwani, R., et al. Similarity search\nin high dimensions via hashing. In Vldb, volume 99, pp.\n518\u2013529, 1999.\nGordon, A., Kozareva, Z., and Roemmele, M. SemEval-2012\ntask 7: Choice of plausible alternatives: An evaluation\nof commonsense causal reasoning. In *SEM 2012: The\nFirst Joint Conference on Lexical and Computational Se-\nmantics \u2013 Volume 1: Proceedings of the main conference\nand the shared task, and Volume 2: Proceedings of the\nSixth International Workshop on Semantic Evaluation\n(SemEval 2012), pp. 394\u2013398, Montr\u00e9al, Canada, 7-8\nJune 2012. Association for Computational Linguistics.\nURL https://aclanthology.org/S12-1052.\nGu, Y. and Song, Z. A faster small treewidth sdp solver.\narXiv preprint arXiv:2211.06033, 2022.\nGu, Y., Song, Z., Yin, J., and Zhang, L. Low rank matrix\ncompletion via robust alternating minimization in nearly\nlinear time. arXiv preprint arXiv:2302.11068, 2023.\nHall, R. and Attenberg, J. Fast and accurate maximum inner\nproduct recommendations on map-reduce. In Proceedings\nof the 24th International Conference on World Wide Web\n(WWW), pp. 1263\u20131268, 2015.\nHan, S., Mao, H., and Dally, W. J.\nDeep compression:\nCompressing deep neural networks with pruning, trained\nquantization and huffman coding.\narXiv preprint\narXiv:1510.00149, 2015.\nHarris, M. How to access global memory efficiently in\nCUDA C/C++ kernels. NVIDIA, Jan, 2013.\nHe, K., Zhang, X., Ren, S., and Sun, J.\nDeep residual\nlearning for image recognition.\nIn Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pp. 770\u2013778, 2016.\nHe, Y., Liu, P., Wang, Z., Hu, Z., and Yang, Y. Filter pruning\nvia geometric median for deep convolutional neural\nnetworks acceleration. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npp. 4340\u20134349, 2019.\nHinton, G., Vinyals, O., Dean, J., et al.\nDistilling\nthe knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2(7), 2015.\nHoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., and Peste,\nA. Sparsity in deep learning: Pruning and growth for\nefficient inference and training in neural networks. J.\nMach. Learn. Res., 22(241):1\u2013124, 2021.\nHooker, S. The hardware lottery. Communications of the\nACM, 64(12):58\u201365, 2021.\nHu, H., Song, Z., Weinstein, O., and Zhuo, D. Training\noverparametrized neural networks in sublinear time.\narXiv preprint arXiv:2208.04508, 2022.\nIndyk, P. and Motwani, R. Approximate nearest neighbors:\ntowards removing the curse of dimensionality.\nIn\nProceedings of the thirtieth annual ACM symposium on\nTheory of computing (STOC), pp. 604\u2013613, 1998a.\nIndyk, P. and Motwani, R. Approximate nearest neighbors:\ntowards removing the curse of dimensionality.\nIn\nProceedings of the thirtieth annual ACM symposium on\nTheory of computing, pp. 604\u2013613, 1998b.\n12\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nIndyk, P. and Wagner, T. Approximate nearest neighbors\nin limited space. In Conference On Learning Theory, pp.\n2012\u20132036. PMLR, 2018.\nIvanov, A., Dryden, N., Ben-Nun, T., Li, S., and Hoefler,\nT.\nData movement is all you need: A case study on\noptimizing transformers.\nProceedings of Machine\nLearning and Systems, 3:711\u2013732, 2021.\nJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard,\nA., Adam, H., and Kalenichenko, D. Quantization and\ntraining of neural networks for efficient integer-arithmetic-\nonly inference. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 2704\u20132713,\n2018.\nJiang, S., Song, Z., Weinstein, O., and Zhang, H. A faster\nalgorithm for solving general lps. In Proceedings of the\n53rd Annual ACM SIGACT Symposium on Theory of\nComputing, pp. 823\u2013832, 2021.\nJohnson, J., Douze, M., and J\u00e9gou, H.\nBillion-scale\nsimilarity search with GPUs. IEEE Transactions on Big\nData, 7(3):535\u2013547, 2019.\nJohnson, W. B. and Lindenstrauss, J.\nExtensions of\nlipschitz mappings into a hilbert space. Contemporary\nmathematics, 26(189-206):1, 1984.\nKitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The\nefficient transformer. In ICLR, 2020.\nKurtz, M., Kopinsky, J., Gelashvili, R., Matveev, A., Carr, J.,\nGoin, M., Leiserson, W., Moore, S., Shavit, N., and Alis-\ntarh, D. Inducing and exploiting activation sparsity for fast\ninference on deep neural networks. In III, H. D. and Singh,\nA. (eds.), Proceedings of the 37th International Confer-\nence on Machine Learning, volume 119 of Proceedings\nof Machine Learning Research, pp. 5533\u20135543. PMLR,\n13\u201318 Jul 2020. URL https://proceedings.mlr.\npress/v119/kurtz20a.html.\nLaurent, B. and Massart, P.\nAdaptive estimation of a\nquadratic functional by model selection.\nAnnals of\nStatistics, pp. 1302\u20131338, 2000.\nLeCun, Y., Denker, J., and Solla, S. Optimal brain damage.\nAdvances in neural information processing systems, 2,\n1989.\nLee, M., He, X., Yih, W.-t., Gao, J., Deng, L., and Smolensky,\nP. Reasoning in vector space: An exploratory study of\nquestion answering. In ICLR, 2016.\nLee, N., Ajanthan, T., and Torr, P. H. Snip: Single-shot\nnetwork pruning based on connection sensitivity. arXiv\npreprint arXiv:1810.02340, 2018.\nLee, Y. T., Song, Z., and Zhang, Q. Solving empirical risk\nminimization in the current matrix multiplication time. In\nConference on Learning Theory, pp. 2140\u20132157. PMLR,\n2019.\nLi, P., Li, X., and Zhang, C.-H. Re-randomized densification\nfor one permutation hashing and bin-wise consistent\nweighted sampling.\nAdvances in Neural Information\nProcessing Systems, 32, 2019.\nLi, S., Song, Z., Xia, Y., Yu, T., and Zhou, T. The closeness\nof in-context learning and weight shifting for softmax\nregression. arXiv preprint, 2023a.\nLi, X. and Li, P. C-MinHash: Improving minwise hashing\nwith circulant permutation. In Chaudhuri, K., Jegelka,\nS., Song, L., Szepesvari, C., Niu, G., and Sabato, S.\n(eds.), Proceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings of\nMachine Learning Research, pp. 12857\u201312887. PMLR,\n17\u201323 Jul 2022. URL https://proceedings.mlr.\npress/v162/li22m.html.\nLi, Z., You, C., Bhojanapalli, S., Li, D., Rawat, A. S.,\nReddi, S. J., Ye, K., Chern, F., Yu, F., Guo, R., and\nKumar, S.\nLarge models are parsimonious learners:\nActivation sparsity in trained transformers, 2022. URL\nhttps://arxiv.org/abs/2210.06313.\nLi, Z., Song, Z., and Zhou, T. Solving regularized exp, cosh\nand sinh regression problems. arXiv preprint, 2303.15725,\n2023b.\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D.,\nYasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar,\nA., et al. Holistic evaluation of language models. arXiv\npreprint arXiv:2211.09110, 2022.\nLiu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.\nRethinking the value of network pruning. arXiv preprint\narXiv:1810.05270, 2018.\nLiu, Z., Xu, Z., Ji, A., Zhang, J., Li, J., Chen, B., and\nShrivastava, A. Halos: Hashing large output space for\ncheap inference. Proceedings of Machine Learning and\nSystems, 4:110\u2013125, 2022.\nLu, Y., Dhillon, P., Foster, D. P., and Ungar, L. Faster ridge\nregression via the subsampled randomized hadamard\ntransform. In Advances in neural information processing\nsystems (NIPS), pp. 369\u2013377, 2013.\nMalkov, Y., Ponomarenko, A., Logvinov, A., and Krylov,\nV. Approximate nearest neighbor algorithm based on\nnavigable small world graphs. Information Systems, 45:\n61\u201368, 2014.\n13\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nMalkov, Y. A. and Yashunin, D. A. Efficient and robust ap-\nproximate nearest neighbor search using hierarchical nav-\nigable small world graphs. IEEE transactions on pattern\nanalysis and machine intelligence, 42(4):824\u2013836, 2018.\nMeng, X. and Mahoney, M. W. Low-distortion subspace\nembeddings in input-sparsity time and applications to\nrobust linear regression. In Proceedings of the forty-fifth\nannual ACM symposium on Theory of computing, pp.\n91\u2013100, 2013.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models, 2016.\nMichel, P., Levy, O., and Neubig, G. Are sixteen heads\nreally better than one? Advances in neural information\nprocessing systems, 32, 2019.\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a\nsuit of armor conduct electricity? a new dataset for open\nbook question answering. In EMNLP, 2018.\nMin, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,\nHajishirzi, H., and Zettlemoyer, L. Rethinking the role\nof demonstrations: What makes in-context learning work?\narXiv preprint arXiv:2202.12837, 2022.\nMolchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.\nPruning convolutional neural networks for resource ef-\nficient inference. arXiv preprint arXiv:1611.06440, 2016.\nNagel, M., Baalen, M. v., Blankevoort, T., and Welling,\nM. Data-free quantization through weight equalization\nand bias correction. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp.\n1325\u20131334, 2019.\nNelson, J. and Nguy\u00ean, H. L. Osnap: Faster numerical linear\nalgebra algorithms via sparser subspace embeddings.\nIn 2013 ieee 54th annual symposium on foundations of\ncomputer science, pp. 117\u2013126. IEEE, 2013.\nNeyshabur, B. and Srebro, N. On symmetric and asymmetric\nlshs for inner product search. In International Conference\non Machine Learning (ICML), pp. 1926\u20131934. PMLR,\n2015.\nNVIDIA. Fastertransformer. https://github.com/\nNVIDIA/FasterTransformer.\nNVIDIA.\nGpu\nperformance\nbackground\nuser\u2019s\nguide,\n2022.\nURL\nhttps://docs.nvidia.\ncom/deeplearning/performance/\ndl-performance-gpu-background/index.\nhtml.\nPark, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee,\nD. nuqmm: Quantized matmul for efficient inference of\nlarge-scale generative language models. arXiv preprint\narXiv:2206.09557, 2022.\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury,\nJ., Levskaya, A., Heek, J., Xiao, K., Agrawal, S., and\nDean, J. Efficiently scaling transformer inference. arXiv\npreprint arXiv:2211.05102, 2022.\nQin, L., Song, Z., and Wang, Y. Fast submodular function\nmaximization. CoRR, abs/2305.08367, 2023a.\nQin, L., Song, Z., Zhang, L., and Zhuo, D. An online and\nunified algorithm for projection matrix vector multipli-\ncation with application to empirical risk minimization. In\nAISTATS, 2023b.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. 2019.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. arXiv e-prints, 2019.\nRazenshteyn, I., Song, Z., and Woodruff, D. P. Weighted\nlow rank approximations with provable guarantees. In\nProceedings of the forty-eighth annual ACM symposium\non Theory of Computing, pp. 250\u2013263, 2016.\nSarlos, T. Improved approximation algorithms for large\nmatrices via random projections. In 2006 47th annual\nIEEE symposium on foundations of computer science\n(FOCS), pp. 143\u2013152. IEEE, 2006.\nSeo, M., Lee, J., Kwiatkowski, T., Parikh, A. P., Farhadi,\nA., and Hajishirzi, H. Real-time open-domain question\nanswering with dense-sparse phrase index. In ACL, pp.\n4430\u20134441, 2019.\nShrivastava, A., Song, Z., and Xu, Z. Sublinear least-squares\nvalue iteration via locality sensitive hashing.\narXiv\npreprint arXiv:2105.08285, 2021.\nSmith, J. E. A study of branch prediction strategies. In\n25 years of the international symposia on Computer\narchitecture (selected papers), pp. 202\u2013215, 1998.\nSohler, C. and Woodruff, D. P.\nSubspace embeddings\nfor the l1-norm with applications.\nIn Proceedings of\nthe forty-third annual ACM symposium on Theory of\ncomputing, pp. 755\u2013764, 2011.\nSong, Z. and Ye, M.\nEfficient asynchronize stochas-\ntic gradient algorithm with structured data.\nCoRR,\nabs/2305.08001, 2023.\nSong, Z. and Yu, Z. Oblivious sketching-based central path\nmethod for linear programming. In International Confer-\nence on Machine Learning, pp. 9835\u20139847. PMLR, 2021.\n14\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nSong, Z., Woodruff, D. P., and Zhong, P. Low rank approx-\nimation with entrywise l1-norm error. In Proceedings of\nthe 49th Annual ACM SIGACT Symposium on Theory of\nComputing, pp. 688\u2013701, 2017.\nSong, Z., Woodruff, D. P., and Zhong, P. Relative error\ntensor low rank approximation. In Proceedings of the\nThirtieth Annual ACM-SIAM Symposium on Discrete\nAlgorithms (SODA), pp. 2772\u20132789. SIAM, 2019.\nSong, Z., Zhang, L., and Zhang, R. Training multi-layer\nover-parametrized neural network in subquadratic time.\narXiv preprint arXiv:2112.07628, 2021.\nSong, Z., Wang, W., and Yin, C.\nFast and efficient\nmatching algorithm with deadline instances.\nCoRR,\nabs/2305.08353, 2023a.\nSong, Z., Yang, X., Yang, Y., and Zhang, L. Sketching meets\ndifferential privacy: fast algorithm for dynamic kronecker\nprojection maintenance. In International Conference on\nMachine Learning (ICML), 2023b.\nTang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., and Lin,\nJ. Distilling task-specific knowledge from bert into simple\nneural networks. arXiv preprint arXiv:1903.12136, 2019.\nTillet, P., Kung, H.-T., and Cox, D. Triton: an interme-\ndiate language and compiler for tiled neural network\ncomputations. In Proceedings of the 3rd ACM SIGPLAN\nInternational Workshop on Machine Learning and\nProgramming Languages, pp. 10\u201319, 2019.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,\nA., and J\u00e9gou, H. Training data-efficient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, pp. 10347\u201310357.\nPMLR, 2021.\nVeit, A., Wilber, M. J., and Belongie, S. Residual networks\nbehave like ensembles of relatively shallow networks. Ad-\nvances in neural information processing systems, 29, 2016.\nViterbi, A. Error bounds for convolutional codes and an\nasymptotically optimum decoding algorithm. IEEE trans-\nactions on Information Theory, 13(2):260\u2013269, 1967.\nWang,\nB.\nand\nKomatsuzaki,\nA.\nGPT-J-6B:\nA\n6\nbillion\nparameter\nautoregressive\nlanguage\nmodel.\nhttps://github.com/kingoflolz/\nmesh-transformer-jax, May 2021.\nWang, R. and Woodruff, D. P. Tight bounds for lp oblivious\nsubspace embeddings. 2018.\nWang, X., Xiong, Y., Wei, Y., Wang, M., and Li, L. Lightseq:\nA high performance inference library for transformers.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies: Industry\nPapers, pp. 113\u2013120, 2021.\nWoodruff, D. P. Sketching as a tool for numerical linear\nalgebra.\nFoundations and Trends\u00ae in Theoretical\nComputer Science, 10(1\u20132):1\u2013157, 2014.\nXiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S.\nSmoothquant:\nAccurate and efficient post-training\nquantization for large language models. arXiv preprint\narXiv:2211.10438, 2022.\nXie, S. M., Raghunathan, A., Liang, P., and Ma, T.\nAn explanation of in-context learning as implicit\nbayesian inference.\nIn International Conference\non Learning Representations, 2022.\nURL https:\n//openreview.net/forum?id=RdJVFCHjUMI.\nXue, H.-J., Dai, X., Zhang, J., Huang, S., and Chen, J. Deep\nmatrix factorization models for recommender systems.\nIn IJCAI, pp. 3203\u20133209, 2017.\nYao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and\nHe, Y. Zeroquant: Efficient and affordable post-training\nquantization for large-scale transformers. arXiv preprint\narXiv:2206.01861, 2022.\nYu, G.-I., Jeong, J. S., Kim, G.-W., Kim, S., and Chun, B.-G.\nOrca: A distributed serving system for {Transformer-\nBased} generative models. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI\n22), pp. 521\u2013538, 2022.\nZandieh, A., Han, I., Daliri, M., and Karbasi, A. Kdeformer:\nAccelerating transformers via kernel density estimation.\narXiv preprint arXiv:2302.02451, 2023.\nZhang, L. Speeding up optimizations via data structures:\nFaster search, sample and maintenance. Master\u2019s thesis,\nCarnegie Mellon University, 2022.\nZhang, M., Wang, W., Liu, X., Gao, J., and He, Y. Navi-\ngating with graph representations for fast and scalable\ndecoding of neural language models. Advances in neural\ninformation processing systems, 31, 2018.\nZhao, R., Hu, Y., Dotzel, J., De Sa, C., and Zhang, Z.\nImproving neural network quantization without retraining\nusing outlier channel splitting. In International conference\non machine learning, pp. 7543\u20137552. PMLR, 2019.\n15\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nContents: In Section A, we present an extended discussion on LLM inference and related works. In Section B, we provide\nmore observation plots for slowly changing activation and further observation on the possibility of sparsifying LLMs via layer\nskipping. In Section C, we provide experiment details. In Section D, we demonstrate implementation details. In Section E,\nwe provide detailed benchmarks regarding our implementation. In Section F, we define some basic notations and definitions.\nIn Section G, we define subspace embedding and show the norm preserving. In Section H, we introduce distances, angles, and\ninner product. In Section I, we provide the distance between different functions. In Section J, we provide the Near-neighbor\nSearch data structure. In Section K, we discuss self-attention as a clustering algorithm in depth.\nA\nRelated Work\nGenerative LLM inference. Taking OPT-175B as an example, assume 6 A100 80GB PCIe, based on the hardware\nspecifications, we compare two main phases of inference time LLM, namely prompting and token generation in Table 1, and\ntwo major components, namely Multi-Head-Attention block and MLP block in Table 2. In practice, the token generation\nphase usually dominates the end-to-end test latency due to IO latency. Generating only two tokens is about the same latency as\nprompting. Further, during token generation, the MLP block is 2 \u00d7 more expensive in both FLOPs and IO access. The hardware\nis often at low utilization because memory reads and writes are more limited on modern hardware than tensor core computation.\nGiven the rapid development of LLM, there is an emergence of systems that are specialized for LLM inference, such as\nFaster Transformer (NVIDIA), Orca (Yu et al., 2022), LightSeq (Wang et al., 2021), PaLM inference (Pope et al., 2022),\nTurboTransformers (Fang et al., 2021), and Deepspeed-Inference (Aminabadi et al., 2022). In practice, the token generation\nphase usually dominates the end-to-end inference time. Although the state-of-the-art systems introduce some helpful system\noptimizations for speedup, there is a lack of careful algorithm and system co-design to unleash the full potential of hardware\nefficiency during the LLM inference computation.\nNear-neighbor Search for Efficient Deep Neural Networks. Near-neighbor Search is a well-studied problem with wide\napplications in recommendation system (Xue et al., 2017; Hall & Attenberg, 2015), question answering (Boytsov et al., 2016;\nSeo et al., 2019; Chang et al., 2020) and natural language processing (Bengio et al., 2003; Lee et al., 2016). There has been a\nline of work using Near-neighbor Search techniques such as Locality-sensitive hashing (Gionis et al., 1999) and Graph-based\nindexing (Malkov et al., 2014) for efficient deep neural network training or inference (Zhang et al., 2018; Chen et al., 2019;\n2020a; Kitaev et al., 2020; Chen et al., 2021b;a; Liu et al., 2022).\nQuantization, pruning, distillation for LLM inference. Various system relaxations have been studied for decades for\nmodel inference in machine learning. For example, quantization (Han et al., 2015; Jacob et al., 2018; Nagel et al., 2019; Zhao\net al., 2019), pruning (Molchanov et al., 2016; Liu et al., 2018; He et al., 2019; Hoefler et al., 2021), and distillation (Hinton\net al., 2015; Cho & Hariharan, 2019; Tang et al., 2019; Touvron et al., 2021) have been applied to speed up the inference of\nthe machine learning model. Active research has recently attempted to apply such techniques in LLM inference. For example,\nzeroQuant (Yao et al., 2022) and nuQmm (Park et al., 2022) implement customized CUDA kernels to support tenor-wise\nor group-wise quantization for LLM inference; LLM.int8 (Dettmers et al., 2022) adopts a mixed INT8/FP16 computation to\ndiminish the influence of activation outliers; SmoothQuant (Xiao et al., 2022) enables efficient 8-bit weight and activation for\nLLM inference; GPTQ (Frantar et al., 2022) adopts a one-shot weight quantization method based on approximate second-order\ninformation for accuracy and efficiency; SparseGPT (Frantar & Alistarh, 2023) introduces an approximate sparse regression\nsolver to enable the sparsity in LLM inference; (Bansal et al., 2022) has reported that a small set of attention heads can perform\nprimitive induction operations associated with in-context learning, and use this property to prune LLM for acceleration.\nResidual connections in neural networks. Residual connection shows great advantages for neural network generalization,\nit provides additional paths for activations to reach the latter parts of the neural network by skipping some layers (He et al.,\n2016). The advancement of residual connections can be viewed as ensembles of multiple shallow neural networks (Veit\net al., 2016). Plenty of active research has discussed the effectiveness of residual connections (Balduzzi et al., 2017; Bello\net al., 2021; Allen-Zhu & Li, 2019; Frei et al., 2019). However, as far as we know, there is no former work that leverages\nthe property of residual connections to improve the efficiency of LLM inference.\nB\nAdditional Observation on Slowly Changing Observation\nFirst, we present more plots on the cosine similarity between representations. Figure 9 plots the cosine similarity between\nactivation across layers on OPT family. It is evident that similarity is high for the larger models.\nThere are two residual connections inside a transformer layer, one around the attention block, and the other one around the\nMLP block. The residual connection can be written as X+F(X), where F is either the Multi-Head Attention or two MLP\nLayer. Figure 10 plots the cosine similarity between X and X+F(X), which is close to 1.0, and the cosine similarity between\n16\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n0\n5\n10\n15\n20\nTransformer Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nn = 1\nn = 2\nn = 4\nn = 8\n(a) OPT-1.3B\n0\n5\n10\n15\n20\n25\n30\nTransformer Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nn = 1\nn = 2\nn = 4\nn = 8\n(b) OPT-6.7B\n0\n5\n10\n15\n20\n25\n30\n35\nTransformer Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nn = 1\nn = 2\nn = 4\nn = 8\n(c) OPT-13B\n0\n10\n20\n30\n40\nTransformer Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nn = 1\nn = 2\nn = 4\nn = 8\n(d) OPT-30B\n0\n10\n20\n30\n40\n50\n60\nTransformer Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nn = 1\nn = 2\nn = 4\nn = 8\n(e) OPT-66B\n0\n20\n40\n60\n80\nTransformer Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nn = 1\nn = 2\nn = 4\nn = 8\n(f) OPT-175B\nFigure 9. Cosine similarity between layer l and layer l+1 for various model.\nX and F(X), which is close to 0.0. This happens because \u2225X\u2225 is significantly greater than \u2225F(X)\u2225, shown in the purple.\nIn the first layer, \u2225F(X)\u2225 is larger, which explains the low cosine similarity. The magnitude of the L2 norm is different across\nmodels, however, we observe a similar trend with models of different sizes. There exists a normalization layer before F(X)\nand the layer normalization scale \u2225X\u2225 to a consistent magnitude across layers (e.g. 85 for OPT-30B, 110 for OPT175B),\nbut not necessarily scale down \u2225X\u2225.\nC\nAdditional Experiment Detail\nC.1\nLarge Batch Size\nTo help understand where the speed-up comes from when batch size is greater than 1, we present the Union Contextual Sparsity\n(fraction of neurons/heads that are not used by any of the inputs in the batch) of different batches sizes for MLP and Attention\nblocks, respectively, in Figure 11. Union Contextual Sparsity is calculated as 1.0 - the union of activated MLP neurons or\nAttention heads in the batch / total neurons or heads. The union operation is essential to realize a fast sparse GEMM.\nSurprisingly the number of MLP neurons/Attention heads that DEJAVU activated does not grow linearly with the batch size.\nThis suggests a power law distribution rather than a uniform distribution of parameter access from all input examples. Further,\na larger batch size can easily lead to out-of-memory for long sequence settings due to the limited GPU memory, the giant\nlarge model size, and the stored KV cache. For example, the total GPU memory of 8 80GB A100 is 640GB. Model parameters\nare around 350GB for OPT175B. The KV cache for a batch size 32 with a sequence longer than 1920 tokens has already\nfilled up the GPU memory.\nC.2\nNear Neighbor classifier\nIn the DEJAVU framework, any near-neighbor search method under the inner product metric would be sufficient to predict\na sparsity pattern. \"Training predictor\" is to reduce the cost of on-the-fly prediction, rather than training the model itself.\n17\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nResidual At Attention\nCos(X, X+F(X))\nCos(X, F(X))\n0\n5\n10\n15\n20\nTransformer Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nResidual At MLP\nCos(X, X+F(X))\nCos(X, F(X))\n0\n20\n40\n60\n80\n100\n120\nNorm\n||X||\n||F(X)||\n||LN(X)||\n0\n20\n40\n60\n80\n100\n120\nL2 Norm\n||X||\n||F(X)||\n||LN(X)||\n(a) OPT-1.3b\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nCosine Similarity\nResidual At Attention\nCos(X, X+F(X))\nCos(X, F(X))\n0\n5\n10\n15\n20\n25\n30\nTransformer Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nResidual At MLP\nCos(X, X+F(X))\nCos(X, F(X))\n0\n50\n100\n150\n200\n250\nNorm\n||X||\n||F(X)||\n||LN(X)||\n0\n50\n100\n150\n200\n250\nL2 Norm\n||X||\n||F(X)||\n||LN(X)||\n(b) OPT-6.7b\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nResidual At Attention\nCos(X, X+F(X))\nCos(X, F(X))\n0\n10\n20\n30\n40\nTransformer Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nResidual At MLP\nCos(X, X+F(X))\nCos(X, F(X))\n0\n50\n100\n150\n200\nNorm\n||X||\n||F(X)||\n||LN(X)||\n0\n50\n100\n150\n200\nL2 Norm\n||X||\n||F(X)||\n||LN(X)||\n(c) OPT-13B\n0.00\n0.25\n0.50\n0.75\n1.00\nCosine Similarity\nResidual At Attention\nCos(X, X+F(X))\nCos(X, F(X))\n0\n10\n20\n30\n40\nTransformer Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nResidual At MLP\nCos(X, X+F(X))\nCos(X, F(X))\n0\n100\n200\n300\nNorm\n||X||\n||F(X)||\n||LN(X)||\n0\n100\n200\n300\nL2 Norm\n||X||\n||F(X)||\n||LN(X)||\n(d) OPT-30B\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCosine Similarity\nResidual At Attention\nCos(X, X+F(X))\nCos(X, F(X))\n0\n10\n20\n30\n40\n50\n60\nTransformer Layer\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine Similarity\nResidual At MLP\nCos(X, X+F(X))\nCos(X, F(X))\n0\n100\n200\n300\n400\nNorm\n||X||\n||F(X)||\n||LN(X)||\n0\n100\n200\n300\n400\nL2 Norm\n||X||\n||F(X)||\n||LN(X)||\n(e) OPT-66B\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCosine Similarity\nResidual At Attention\nCos(X, X+F(X))\nCos(X, F(X))\n0\n20\n40\n60\n80\nTransformer Layer\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCosine Similarity\nResidual At MLP\nCos(X, X+F(X))\nCos(X, F(X))\n0\n500\n1000\n1500\n2000\nNorm\n||X||\n||F(X)||\n||LN(X)||\n0\n500\n1000\n1500\n2000\n2500\nL2 Norm\n||X||\n||F(X)||\n||LN(X)||\n(f) OPT-175B\nFigure 10. Cosine similarity between X and F(X), and the cosine similarity between X and X\u2032 in orange color. L2 norm of X and F(X)\nand X after layer normalization in purple on the right. Except on the first layer, \u2225X\u2225 is significantly higher than \u2225F(X)\u2225. \u2225F(X)\u2225 is\nhigher at the first layer, which corresponds to the low cosine similarity at the first layer.\nFor example, in our exploration stage mentioned in Section 4.1, we adopt HNSW, a state-of-art near-neighbor search method,\nto predict MLP sparse pattern, and we can see from the following table there is no drop in the perplexity at 90 % sparsity\nratio. However, due to the high dimensionality of embedding and HNSW\u2019s reliance on CPU, the time HNSW took to identify\nthe sparsity pattern is 10ms, which is longer than the MLP computation.\nIn our paper, we choose a neural network classifier as our near neighbor search method to take advantage of the fast matrix\nmultiplication on GPU. And training such classifiers to predict sparsity patterns is not only cheaper in terms of training cost\nbut also inherently different from the method concept.\n18\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n0\n20\n40\n60\n80\n96\nTransformer Layer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nUnion Contextual Sparsity\nBatch size\n2\n4\n8\n16\n32\n(a) MLP\n0\n20\n40\n60\n80\n96\nTransformer Layer\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nUnion Contextual Sparsity\nBatch size\n2\n4\n8\n16\n32\n(b) Attention\nFigure 11. Union contextual sparsity with larger batch size.\nOPT-1.3B\nOPT-1.3B + HNSW\nHellaswag\n0.4154\n0.4314\nC4\n14.2\n14.4\nTable 8. Sparsify from the Depth: Skipping or parallel entire transformer blocks may not lead to catastrophic drop in accuracy at test time.\nModel\nCOPA\nHellaswag\nLambada\nOpenBookQA\nPIQA\nWinogrande\nOPT-175B\n0.8600\n0.7814\n0.7584\n0.4460\n0.8096\n0.7261\n- Parallel 2\n0.8300\n0.7737\n0.7762\n0.4520\n0.8030\n0.7096\n- Parallel 4\n0.5200\n0.2519\n0\n0.2720\n0.5092\n0.4870\n- Skip 2/8\n0.8000\n0.7112\n0.6387\n0.4220\n0.7840\n0.6630\n- Skip 2/4\n0.6900\n0.4409\n0.0240\n0.3400\n0.6882\n0.5383\nBloom\n0.8000\n0.7460\n0.6771\n0.4480\n0.7949\n0.7040\n- Parallel 2\n0.8100\n0.7404\n0.6992\n0.4360\n0.7813\n0.7048\n- Parallel 4\n0.6200\n0.3176\n0.1325\n0.2720\n0.5593\n0.5217\n- Skip 2/8\n0.7900\n0.6829\n0.5936\n0.4120\n0.7699\n0.6614\n- Skip 2/4\n0.6600\n0.5538\n0.3023\n0.3580\n0.7046\n0.5549\nC.3\nFuture Possibility: Skipping Layer\nDeja Vu currently sparsifies from the perspective of model width. Here, we explore the possibility of sparsification from\nmodel depth. As observed in Section 3, we show that the activation of large language models changes slowly across blocks.\nThis property can be leveraged to increase the efficiency of a trained model by parallelizing, reordering, or skipping certain\nintermediate sub-blocks without significantly impacting the overall accuracy.\nSetting\nWiki(ppl)\nC4(ppl)\nBaseline\n11.57\n10.17\nSkip every 2 layers\n21.16\n16.58\nSkip every 4 layers\n13.45\n11.37\nImproving the inference efficiency of Transformer models is a challenging task due to their sequential execution of\nTransformer layers. Each sub-block depends on the output of the previous one, leading to low hardware efficiency, particularly\nduring the token generation phase where each forward pass is computed for only one token. However, the sequential execution\nof blocks and sub-blocks yields computation bubbles, and the latter involves a large amount of communication overhead.\nHere, we present an interesting observation that can potentially alleviate these challenges. We found that the activation of\nthe model changes slowly across blocks. Specifically, the cosine similarity of activations between adjacent blocks is often\nabove 0.99. This suggests that the blocks might take the previous activation as input \u2013 parallelize or reorder the blocks \u2013\nwithout significantly affecting the output. Slowly changing activations suggest that it may be possible to parallelize, reorder,\nor even skip blocks while maintaining a similar output. Some existing models, such as GPT-J (Wang & Komatsuzaki, 2021),\nGPT-NeoX (Black et al., 2022), and PaLM (Chowdhery et al., 2022) already placed the Attention block and MLP block\n19\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nin parallel in training to facilitate parallel computation and reduce the communication overhead.\nHere we investigate the possibility at inference time. And surprisingly, we found parallelizing those blocks for models that\nare trained in a sequence manner will not hurt the performance of downstream tasks significantly. And surprisingly, we found\nparallelizing those blocks for models that are trained in a sequence manner will not hurt the performance of downstream\ntasks significantly. TableC.3 presents some preliminary results of OPT-175B and Bloom\nGiven the activation y and Transformer layer l, we have:\neyl \u2190yl+MHAl(yl)\nbyl \u2190 eyl+MLPl(eyl)\nParallelizing two blocks refers to placing the Attention and MLP blocks in parallel, i.e.:\nbyl \u2190y+MHAl(yl)+MLPl(yl)\nParallelizing four blocks then parallelize the blocks of two Transformer layers, defined as follows:\nbyl+1 \u2190yl+MHAl(yl)+MLPl(yl)+MHAl+1(yl)+MLPl+1(yl)\nSkipping layers is straightforward, which drops an entire Transformer layer for every n layers.\nWe are surprised to find that parallel two layers preserve accuracy on a series of tasks across models. Besides, randomly\nskipping 25% layers doesn\u2019t lead to catastrophic quality. Our findings suggest from the downstream task perspective, the\nactivation patterns within the model are relatively consistent across different blocks, providing a potential avenue for future\nresearch on model compression and optimization.\nD\nImplementation Details\nFigure 12 presents a more detailed workflow of DEJAVU. The left diagram shows how an input y performs the sparse MHA\nwith selected indices 0,3, predicted by the head predictor. Similarly, the right diagram shows how an input y performs the\nsparse MLP with selected indices 0,2, predicted by the neuron predictor of that layer.\nSelected Head Index\n!! = {0,3}\nAttention with \"\"!\n#, \"\"!\n$ , \"\"!\n%\n$\n%&($)\n\"\"!\n'  Output Projection\n)%*\"!($)\n%(($)\n\"\"\"\n)  \n!*= {0,2}\n$\n\u03c3($\"\"\"\n+ )\n\"\"\"\n+  \nMLP\"\"($)\nSelected Neurons Index\nSparsified Attention\nSparsified MLP\nFigure 12. Detailed diagram on the sparsified computation process of MLP and Attention. Notation refers to Section 2.3\nNext, we will present a general explanation of two optimizations we used in DEJAVU implementation. Kernel fusion: A\nstandard implementation of sparse matrix-vector multiply (e.g., Wx in PyTorch) that separately indexes a subset of the\nmatrix W[idx,:] before multiplying with input x would incur 3\u00d7 the amount of memory IOs: one to load a subset of W from\nGPU memory, one to write that subset to a different contiguous region in memory, and one to load that (now contiguous)\nsubset in again to multiply with x. Similarly, to use sparse matrix multiply routines (e.g., cuSparse), we would first need\nto convert W[idx,:] to sparse format, again incurring more memory IOs. We instead fuse the indexing and the multiplication\nstep: we load a subset of W[idx,:] to memory, along with x, perform the multiply, then write down the result. This fused\nimplementation (in Triton (Tillet et al., 2019)) yields up to 4\u00d7 speedup compared to a standard PyTorch implementation\n(Section E). Memory coalescing: the weight matrices are conventionally stored in row-major format. This allows us to load\n20\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nW[idx,:] optimally (as the second dimension is contiguous in memory). However, for cases where we need to load W[:,idx]\n(attention output projection and the 2nd weight matrix in the MLP) this format significantly slows down memory loading,\nas idx could contain indices pointing to non-contiguous memory. A simple solution is to store these matrices in column-major\nformat (i.e., storing W \u22a4 in contiguous row-major format), then use the same fused kernel above. This transposition is done\nonce when loading the model, and incurs no added cost during generation.\nE\nBenchmarking Sparse MLP and Sparse Attention\n0.2\n0.4\n0.6\n0.8\nMLP density\n100\n200\n300\n400\nTime (us)\nSparse MLP benchmark\nDense MLP\nSparse MLP baseline (PyTorch)\nSparse MLP - Deja vu\nFigure 13. Speed benchmarking of the MLP layer of OPT-175B on 8xA100s. Our sparse implementation is up to 4.5\u00d7 faster than the\nbaseline implementation in PyTorch. Our sparse MLP implementation remains faster than dense MLP for density up to 0.8.\nWe validate that our hardware-aware implementation of sparse MLP and sparse attention (Section 4.4) yields wall-clock\nspeed up compared to both dense MLP/attention and compared to the standard implementation in PyTorch.\nRecall that our implementation fuses the sparse indexing and the multiplication (W 1\nSM )\u22a4y for weight matrices (W 1)\u22a4 and\ninput vector y. In cases where we need to index W 2\nSM , we store the transpose of W 2 to ensure memory coalescing. For the\nbaseline implementation in PyTorch, we index (W 1\nSM )\u22a4 as a separate operation before multiplying with y, which incurs\nmore memory reads/writes.\nSimilarly, we fuse the sparse indexing and the multiplication (W Q\nSA)\u22a4y, (W K\nSA)\u22a4y, (W V\nSA)\u22a4y for weight matrices (W Q)\u22a4,\n(W K)\u22a4, (W V )\u22a4 and input vector y. Note we usually concatenate all three matrices in the standard implementation, but we sep-\narate them here for clarity. In cases where we need to index W O\nSA, we store the transpose of W O to ensure memory coalescing.\nIn Figure 13 and Figure 14, our sparse MLP and attention implementations are 4-5\u00d7 faster than the baseline implementation\nin Pytorch, and remains faster than the dense version for density up to 0.8.\nF\nNotations and Basic Definitions\nFor a positive integer n, let [n]:={1,2,\u00b7\u00b7\u00b7,n}. For a matrix A\u2208Rn\u00d7n, let Ai,: and A:,j be two column vectors corresponding\nto the i-th row and the j-th column of A respectively, and Ai,j be the entry at the i-th row and the j-th column. For a vector\nx\u2208Rn, let \u221ax\u2208Rn denote the vector with the i-th entry being \u221axi and diag(x)\u2208Rn\u00d7n denote the diagonal matrix with\nthe i-th diagonal entry being xi. For two matrices A,W \u2208Rn\u00d7n, let \u2225A\u2225W :=(Pn\ni=1\nPn\nj=1Wi,jA2\ni,j)1/2 and W \u25e6A denote\nthe matrix where (W \u25e6A)i,j =Wi,jAi,j. For matrix W \u2208Rn\u00d7n, let DWi :=diag(Wi,:) with i\u2208[n].\nFor two vectors x \u2208 Rn and w \u2208 Rn\n\u22650, let \u2225x\u2225w := (Pn\ni=1wix2\ni )1/2. For a vector x, we denote \u2225x\u22252 := (Pn\ni=1x2\ni )1/2 as its\n21\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n0.2\n0.4\n0.6\n0.8\nAttention density\n100\n200\n300\nTime (us)\nSparse attention benchmark\nDense attention\nSparse attention baseline (PyTorch)\nSparse attention - Deja vu\nFigure 14. Speed benchmarking of the attention layer of OPT-175B on 8xA100s. Our sparse implementation is up to 5\u00d7 faster than the\nbaseline implementation in PyTorch. Our sparse attention implementation remains faster than dense MLP for density up to 0.8.\n\u21132 norm. We denote \u2225x\u2225p :=(Pn\ni=1|xi|p)1/p as its \u2113p norm. For a square matrix A, we denote tr[A] as the trace of matrix A.\nFor a matrix A\u2208Rn\u00d7k (suppose n\u2265k), we use \u2225A\u2225 to denote its spectral norm, i.e., \u2225A\u2225=supx\u2225Ax\u22252/\u2225x\u22252. We use \u2225A\u2225F\nto denote its Frobenius norm \u2225A\u2225F :=(Pn\ni=1\nPk\nj=1A2\ni,j)1/2.\nSuppose matrix A \u2208 Rn\u00d7k has SVD decomposition U\u03a3V \u22a4 where U \u2208 Rn\u00d7k (this matrix has orthonormal columns),\n\u03a3\u2208Rk\u00d7k is a diagonal matrix, and V \u2208Rk\u00d7k. We call columns of U are singular vectors. We use A\u2020 \u2208Rk\u00d7n to denote the\nMoore-Penrose pseudoinverse, then A\u2020 =V \u03a3\u22121U \u22a4. Suppose \u03a3\u2208Rk\u00d7k is sorted diagonal matrix, let \u03c31,\u00b7\u00b7\u00b7,\u03c3k denote the\ndiagonal entries of \u03a3. Then we call \u03c3i the i-th singular value of matrix, and we write it as \u03c3i(A).\nFor any symmetric matrix B \u2208Rk\u00d7k, we define its eigenvalue decomposition as U\u039bU \u22a4, where \u039b is a diagonal matrix. Let\n\u03bb1,\u00b7\u00b7\u00b7,\u03bbk denote the entries on diagonal of \u039b\u2208Rk\u00d7k. We say \u03bbi is the i-th eigenvalue. Usually we write it as \u03bbi(B).\nThe connection between eigenvalues and singular values is\n\u03c32\ni (A)=\u03bbi(A\u22a4A)\nWe use notation A \u2ab0 0 to denote that matrix A is positive semidefinite (psd). Mathematically, A \u2ab0 0 means for all vectors\nx, we have x\u22a4Ax\u22650.\nSimilarly, for two squarer matrices A and B, we use A\u2ab0B to denote the case where for all vectors x, x\u22a4Ax\u2265x\u22a4Bx.\nWe use Pr[] and E[] for probability and expectation. We denote max{a,b} as the maximum between a and b. We denote\nmin{a,b} (resp. max{a,b}) as the minimum (reps. maximum) between a and b.\nThroughout, for non-negative real numbers a and b, we use the notation a=(1\u00b1\u03f5)b if a\u2208[(1\u2212\u03f5)b,(1+\u03f5)b].\nG\nSubspace Embeddings and Norm Preserving\nIn Section G.1, we show the norm preserving of the soft-max functions. In Section G.2, we show the norm preserving of the\nReLU function. In Section G.3, we introduce the folded Guassian distribution. In Section G.4, we introduce the \u21132 subspace\nembedding. In Section G.5, we introduce the \u21131 subspace embedding. In Section G.6, we introduce different sketching\nmatrices for subspace embedding.\n22\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nG.1\nSoft-Max Functions\nLet K \u2208Rs\u00d7d and V \u2208Rd\u00d7s. Inspired by the softmax unit in the attention scheme of large language models. The softmax\nrelated regression has been studied in many settings (Zandieh et al., 2023; Alman & Song, 2023; Brand et al., 2023; Li et al.,\n2023b; Deng et al., 2023b;a; Gao et al., 2023a; Li et al., 2023a; Gao et al., 2023b). In this work, we follow the standard\nsoftmax definition. We define \u03c31 :Rs \u2192Rs to be a softmax function, i.e., for any vector y\u2208Rs, the \u03c3(y) can be written as\n\u03c31(y)i =\nexp(yi)\nPd\nj=1exp(yj)\n, \u2200i\u2208[d]\nThe standard softmax is \u21131 version. In this work, we also consider the \u21132 generalization. We define \u03c32 : Rs \u2192 Rs to be a\nsoftmax function (\u21132 version), i.e., for any vector y\u2208Rs, the \u03c3(y) can be written as\n\u03c32(y)i =\nexp(yi)\n(Pd\nj=1exp(2yj))1/2 , \u2200i\u2208[d]\nWe define function f :Rd \u2192Rd\nf(x)=V \u00b7(\u03c3(K\u00b7x))\n(3)\nDefinition G.1. We say X \u2282 Rd is a rank-k subspace, if there is an orthonormal basis U \u2208 Rd\u00d7k, for any x \u2208 X, there is\ny\u2208Rk such that\nx=Uy.\nWe can have\nLemma G.2. Let \u03c4 \u2208 (0,1). Let X \u2282 Rd denote a subspace with rank k. Let f be defined based on \u03c32 function. Let V is\na random Gaussian matrices with d\u2265\u2126(\u03f5\u22122(k+log(1/\u03b4))) rows. Let V =\u03c4V , then we have with probability 1\u2212\u03b4\n(1\u2212\u03f5)\u03c4\u2225x\u22252 \u2264\u2225f(x)\u2225\u2264(1+\u03f5)\u03c4\u2225x\u22252.\nfor all unit vectors x\u2208X.\nFurther, if d=O(k+log(1/\u03b4)), then we have\n0.5\u03c4\u2225x\u22252 \u2264\u2225f(x)\u2225\u22642\u03c4\u2225x\u22252.\nRemark G.3. The above condition implies that f is a shrinking operator but also not shrinking arbitrarily small.\nProof. Given d\u2265\u2126(\u03f5\u22122(k+log(1/\u03b4))), by using Lemma G.11 , we have\n(1\u2212\u03f5)\u2225y\u22252 \u2264\u2225V y\u22252 \u2264(1+\u03f5)\u2225y\u22252\nAs the input of the function f here is the output of a softmax function (\u21132 version), we know that \u2225y\u22252 =1.\nThus, we have\n(1\u2212\u03f5)\u2264\u2225V y\u22252 \u2264(1+\u03f5)\nBy rescaling V , we have\n(1\u2212\u03f5)\u2225x\u22252 \u2264\u2225V y\u22252 \u2264(1+\u03f5)\u2225x\u22252.\nLemma G.4. Let \u03c4 \u2208 (0,1). Let X \u2282 Rd denote a subspace with rank k. Let f be defined based on \u03c31 function. Suppose\nV is a random Gaussian matrix with d\u2265\u2126((k+log(1/\u03b4))) rows. Let V = 1\n2\u03c4V .\nThen we have\n1\n4\u221as\u03c4 \u00b7\u2225x\u22252 \u2264\u2225f(x)\u22252 \u2264\u03c4 \u00b7\u2225x\u22252\nfor all unit vectors x.\nProof. By property of subspace embedding, we know that if d\u2265\u2126(\u03f5\u22122(s+log(1/\u03b4))),\n(1\u2212\u03f5)\u2225y\u22252 \u2264\u2225V y\u22252 \u2264(1+\u03f5)\u2225y\u22252\nBy property of function of f, we know we only need to care \u2225y\u22251 =1, this implies that\n1\n\u221as\u2225y\u22251 \u2264\u2225y\u22252 \u2264\u2225y\u22251\n23\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nOn one hand, we have\n\u2225V y\u22252 \u2264 (1+\u03f5)\u00b7\u2225y\u22252\n\u2264 (1+\u03f5)\u00b7\u2225y\u22251\n= (1+\u03f5),\n(4)\nwhere the first step follows from \u2225V y\u22252 \u2264(1+\u03f5)\u2225y\u22252, the second step follows from \u2225y\u22252 \u2264\u2225y\u22251 and the last step follows\nfrom \u2225y\u22251 =1.\nOn the other hand, we have\n\u2225V y\u22252 \u2265 (1\u2212\u03f5)\u2225y\u22252\n\u2265 1\n\u221as(1\u2212\u03f5)\u2225y\u22251\n= 1\n\u221as(1\u2212\u03f5),\n(5)\nwhere the first step follows from (1\u2212\u03f5)\u2225y\u22252 \u2264\u2225V y\u22252, the second step follows from\n1\n\u221as\u2225y\u22251 \u2264\u2225y\u22252 and the last step follows\nfrom \u2225y\u22251 =1.\nCombining Eq. (5)and Eq. (4) together, we have\n(1\u2212\u03f5) 1\n\u221as \u2264\u2225V y\u22252 \u2264(1+\u03f5)\nChoosing \u03f5=1/2, we have\n1\n2\u221as \u2264\u2225V y\u22252 \u22642.\nBy V = 1\n2\u03c4V and \u2225x\u22252 =1, we have\n1\n4\u221as\u03c4\u2225x\u22252 \u2264\u2225V y\u22252 \u2264\u03c4\u2225x\u22252.\nG.2\nReLU Functions\nWe use \u03d5:R\u2192R to denote ReLU function, i.e., \u03d5(z)=max{z,0}.\nWe define function g:Rd \u2192Rd\ng(x)=V \u00b7(\u03d5(K\u00b7x))\n(6)\nLet K \u2208Rs\u00d7d and V \u2208Rd\u00d7s.\nLemma G.5. Let X \u2282 Rd denote a rank-k subspace. Let K denote a random Gaussian matrix. Let V denote a random\nGaussian matrix. Let s \u2265 \u2126(\u03f5\u22122klog(1/(\u03b4\u03f5))). Let d \u2265 \u2126(\u03f5\u22122(k+log(1/\u03b4))). Then we know with high probability 1\u2212\u03b4,\nfor all unit vector x\u2208X\n(1\u2212\u03f5)\u2225x\u22252 \u2264\u2225f(x)\u22252 \u2264(1+\u03f5)\u2225x\u22252\nProof. Suppose s\u2265\u2126(\u03f5\u22122log(1/\u03b4)).\nUsing Lemma G.6, Fact G.7, we can show that for each fixed\n(1\u2212\u03f5)\u2225x\u22252 \u2264\u2225\u03d5(Kx)\u22252 \u2264(1+\u03f5)\u2225x\u22252\nholds with probability 1\u2212\u03b4.\nBy a standard \u03f5-net argument (Lemma G.9), the net points in X is at most (10/\u03f5)O(k).\nTaking a union bound over all the net points, we can show that for all x\u2208X\n(1\u2212\u03f5)\u2225x\u22252 \u2264\u2225\u03d5(Kx)\u22252 \u2264(1+\u03f5)\u2225x\u22252\nholds with probability 1\u2212\u03b4/2 and s\u2265\u2126(\u03f5\u22122klog(1/(\u03b4\u03f5))).\nFurther, we using Lemma G.11, we can show that\n(1\u2212\u03f5)\u2225\u03d5(Kx)\u22252 \u2264\u2225f(x)\u22252 \u2264(1+\u03f5)\u2225\u03d5(Kx)\u22252\nholds with probability 1\u2212\u03b4/2.\nCombining together,\n(1\u2212\u03f5)2\u2225x\u22252 \u2264\u2225f(x)\u22252 \u2264(1+\u03f5)2\u2225x\u22252\n24\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nholds with probability 1\u2212\u03b4.\nRescaling the \u03f5, we complete the proof.\nG.3\nFolded Gaussian Distribution\nWe state a standard tool from literature,\nLemma G.6 (Lemma 1 on page 1325 of Laurent and Massart (Laurent & Massart, 2000)). Let X \u223c X 2\nk be a chi-squared\ndistributed random variable with k degrees of freedom. Each one has zero means and \u03c32 variance.\nThen,\nPr[X\u2212k\u03c32 \u2265(2\n\u221a\nkt+2t)\u03c32]\u2264 exp(\u2212t)\nPr[k\u03c32\u2212X \u22652\n\u221a\nkt\u03c32]\u2264 exp(\u2212t)\nFurther if k\u2265\u2126(\u03f5\u22122t) and t\u2265\u2126(log(1/\u03b4)), then we have\nPr[|X\u2212k\u03c32|\u2264\u03f5k\u03c32]\u2264\u03b4.\nWe prove the following property,\nFact G.7. Let h,q \u2208 Rp be fixed vectors and h \u0338= 0,W \u2208 Rm\u00d7p be random matrix with i.i.d. entries Wi,j \u223c N(0, 2\nm), and\nvector v\u2208Rm defined as vi =\u03d5((Wh)i)=1(W (h+q))i\u22650(Wh)i. Then,\n\u2022 |vi| follows i.i.d. from the following distribution: with half probability |vi|=0, and with the other half probability |vi|\nfollows from folded Gaussian distributions |N(0, 2\u2225h\u22252\nm\n)|.\n\u2022\nm\u2225v\u22252\n2\u2225h\u22252 is in distribution identical to \u03c72\n\u03c9 (chi-square distribution of order \u03c9 ) where \u03c9 follows from binomial distribution\nB(m,1/2).\nProof. We assume each vector Wi is generated by first generating a gaussian vector g\u223cN(0, 2I\nm ) and then setting Wi =\u00b1g\nwhere the sign is chosen with half-half probability. Now, |\u27e8Wi,h\u27e9|=|\u27e8g,h\u27e9| only depends on g, and is in distribution identical\nto |N(0, 2\u2225h\u22252\nm\n)|. Next, after the sign is determined, the indicator 1\u27e8Wi,h+q\u27e9\u22650 is 1 with half probability and 0 with another\nhalf. Therefore, |vi| satisfies the aforementioned distribution. As for \u2225v\u22252, letting \u03c9\u2208{0,1,...,m} be the variable indicator\nhow many indicators are 1 , then \u03c9\u223cB(m,1/2) and m\u2225v\u22252\n2\u2225h\u22252 \u223c\u03c72\n\u03c9.\nG.4\n\u21132 subspace embedding\nWe define a standard notion in sketching technique.3\nDefinition G.8 (\u21132 subspace embedding (Sarlos, 2006)). A (\u03f5,\u03b4,\u21132)-subspace embedding for the column space of an n\u00d7d\nmatrix A is a matrix S for which\nPr[\u2200x\u2208Rd,\u2225SAx\u22252\n2 =(1\u00b1\u03f5)\u2225Ax\u22252\n2]\u22651\u2212\u03b4.\nThe above condition is equivalent to\nPr[\u2225U \u22a4U \u2212U \u22a4S\u22a4SU\u2225\u2264\u03f5]\u22651\u2212\u03b4.\nwhere the U is the orthonormal basis of A.\nFor the reason of above conditions are equivalent, we refer the readers to the survey (Woodruff, 2014).\nWe state a standard tool in literature,\nLemma G.9 (Lemma 5 in (Woodruff, 2014)). Let X \u2282Rd be rank k. For any \u03b3 \u2208(0,1), there is a \u03b3-net N of X for which\n|N|\u2264(1+4/\u03b3)k.\n3We remark that sketching technique has widely applied to many applications such as linear regression, low-rank approximation (Clarkson\n& Woodruff, 2013; Nelson & Nguy\u00ean, 2013; Lu et al., 2013; Boutsidis et al., 2016; Cohen, 2016; Razenshteyn et al., 2016; Song et al., 2017;\n2019), linear programming (Song & Yu, 2021; Dong et al., 2021; Jiang et al., 2021; Gu & Song, 2022), semi-definite programming (Gu &\nSong, 2022; Song et al., 2023b), empirical risk minimization(Lee et al., 2019; Qin et al., 2023b), training over-parameterized neural network\n(Brand et al., 2021; Song et al., 2021; Alman et al., 2022; Hu et al., 2022; Zhang, 2022).\n25\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nG.5\n\u21131 subspace embedding\nWhen p=1, using Cauchy random variables, Sohler and Woodruff (Sohler & Woodruff, 2011) showed there exist \u21131 oblivious\nsubspace embeddings with O(dlogd) rows and \u03ba = O(dlogd). This approach was generalized by using p-stable random\nvariables in work of Meng and Mahoney (Meng & Mahoney, 2013) to \u2113p-norms when 1<p<2, where they showed there\nexist \u2113p oblivious subspace embeddings with O(dlogd) rows and \u03ba = O((dlogd)1/p). Unlike the case when p = 2, due to\nthe large distortion\nIn (Wang & Woodruff, 2018), they show for every 1\u2264p<2, any oblivious subspace embedding with dimension r has distortion\n\u03ba=\u2126(\n1\n( 1\nd )1/p\u00b7log2/pr+( r\nn )1/p\u22121/2 ). They also give sparse oblivious subspace embeddings for every 1\u2264p<2 which are optimal\nin dimension and distortion, up to poly (logd) factors. Importantly for p=1, they achieve r=O(dlogd),\u03ba=O(dlogd) and\ns=O(logd) non-zero entries per column.\nDefinition G.10 (\u21131 subspace embedding). Let 0<\u03b1<\u03b2 be parameters. We will say a matrix S is an \u21131 subspace embedding\nfor an n\u00d7d matrix A if there are constants c1,c2 >0 so that for all x\u2208Rd,\n\u2225Ax\u2225\u2264\u2225SAx\u22251 \u2264dc1\u2225Ax\u22251,\nand S has at most dc2 rows.\nG.6\nRandom Matrices\nMatrices\nb\nTime for R\u00b7A\nReference\nRandom Gaussian\n\u03f5\u22122(d+log(1/\u03b4))\nTmat(b,n,d)\nThm. 6 of (Woodruff, 2014)\nSRHT\n\u03f5\u22122(\n\u221a\nd+\u221alogn)2log(d/\u03b4)\nndlog(\u03f5\u22121d(logn))\nThm. 7 of (Woodruff, 2014)\nAMS\n\u03f5\u22122(d+log(1/\u03b4))\nTmat(b,n,d)\nFollow from JL guarantee\nCount-sketch\n\u03f5\u22122\u03b4\u22121d2\nnnz(A)\nThm. 9 of (Woodruff, 2014)\nSparse embedding\n\u03f5\u22122d\u00b7 poly log(d/(\u03f5\u03b4))\n\u03f5\u22121nnz(A) poly log(d/(\u03f5\u03b4))\nThm. 10 (2) of (Woodruff, 2014)\nSparse embedding\n\u03f5\u22122d1+\u03b3\n\u03f5\u22121nnz(A)poly(1/\u03b3)\nThm. 10 (1) of (Woodruff, 2014)\nTable 9. Summary for different sketching matrices for subspace embedding. The sketching matrix R has size b\u00d7n. The vectors are from the\ncolumn subspace of matrix A with size n\u00d7d.\u03f5\u2208(0,1) is the error parameter, and \u03b4 \u2208(0,1) is the probability parameter. Tmat (a,b,c) denotes\nthe running time of fast matrix multiplication of two matrices with size a\u00d7b and b\u00d7c. In the first sparse embedding matrix, each column has\ns\u2265\u03f5\u22121 poly log(d/(\u03f5\u03b4)) non-zero entries; In the second sparse embedding matrix, each column has s\u2265\u03f5\u22121 poly (1/\u03b3) non-zero entries,\n\u03b3 >0 is a tunable parameter that gives different trade-offs, and \u03b4 can be as small as 1/ poly (d). For count-sketch matrices, the subspace\nembedding guarantee is proved from JL moment property, instead of directly from JL guarantee.\nLemma G.11 (Theorem 6 of (Woodruff, 2014)). Let 0 < \u03f5,\u03b4 < 1 and S =\n1\n\u221a\nkR \u2208 Rk\u00d7n where the entries Ri,j of R are\nindependent standard normal random variables. Then if k= \u0398(\u03f5\u22122(d+log(1/\u03b4))), then for any fixed n\u00d7d matrix A, with\nprobability 1\u2212\u03b4,S is a (1\u00b1\u03f5)\u21132-subspace embedding for A, that is, simultaneously for all x\u2208Rd,\u2225SAx\u22252 =(1\u00b1\u03f5)\u2225Ax\u22252.\nHere C >0 is an absolute constant.\nWe consider several standard sketching matrices:\n1. Random Gaussian matrices.\n2. Subsampled randomized Hadamard/Fourier transform (SRHT) matrices (Lu et al., 2013).\n3. AMS sketch matrices (Alon et al., 1996), random {\u22121,+1} per entry.\n4. Count-Sketch matrices (Charikar et al., 2002), each column only has one non-zero entry, and is \u22121,+1 half probability\neach.\n5. Sparse embedding matrices (Nelson & Nguy\u00ean, 2013), each column only has s non-zero entries, and each entry is\n\u2212 1\n\u221as,+ 1\n\u221as half probability each.\n6. Uniform sampling matrices.\nDefinition G.12 (Random Gaussian matrix). We say R\u2208Rb\u00d7n is a random Gaussian matrix if all entries are sampled from\nN(0,1/b) independently.\n26\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nDefinition G.13 (Subsampled randomized Hadamard/Fourier transform matrix (Lu et al., 2013)). We say R \u2208 Rb\u00d7n is a\nsubsampledrandomizedHadamardtransform(SRHT)matrix 4 ifitisoftheformR=\np\nn/bSHD, whereS \u2208Rb\u00d7n isarandom\nmatrix whose rows are b uniform samples (without replacement) from the standard basis of Rn,H \u2208Rn\u00d7n is a normalized\nWalsh-Hadamard matrix, and D\u2208Rn\u00d7n is a diagonal matrix whose diagonal elements are i.i.d. Rademacher random variables.\nDefinition G.14 (AMS sketch matrix (Alon et al., 1996)). Let h1,h2,\u00b7\u00b7\u00b7,hb be b random hash functions picking from a 4-wise\nindependent hash family H={h:[n]\u2192{\u2212 1\n\u221a\nb,+ 1\n\u221a\nb}}. Then R\u2208Rb\u00d7n is a AMS sketch matrix if we set Ri,j =hi(j)\nDefinition G.15 (Count-sketch matrix (Charikar et al., 2002)). Let h:[n]\u2192[b] be a random 2-wise independent hash function\nand \u03c3 : [n] \u2192 {\u22121,+1} be a random 4-wise independent hash function. Then R \u2208 Rb\u00d7n is a count-sketch matrix if we set\nRh(i),i =\u03c3(i) for all i\u2208[n] and other entries to zero.\nDefinition G.16 (Sparse embedding matrix I (Nelson & Nguy\u00ean, 2013)). We say R\u2208Rb\u00d7n is a sparse embedding matrix\nwith parameter s if each column has exactly s non-zero elements being \u00b11/\u221as uniformly at random, whose locations are\npicked uniformly at random without replacement (and independent across columns) 5.\nDefinition G.17 (Sparse embedding matrix II (Nelson & Nguy\u00ean, 2013)). Let h : [n] \u00d7 [s] \u2192 [b/s] be a random 2-wise\nindependent hash function and \u03c3:[n]\u00d7[s]\u2192{\u22121,1} be a 4-wise independent. Then R\u2208Rb\u00d7n is a sparse embedding matrix\nII with parameter s if we set R(j\u22121)b/s+h(i,j),i =\u03c3(i,j)/\u221as for all (i,j)\u2208[n]\u00d7[s] and all other entries to zero 6.\nDefinition G.18 (Uniform sampling matrix). We say R\u2208Rb\u00d7n is a uniform sampling matrix if it is of the form R=\np\nn/bSD,\nwhere S \u2208 Rb\u00d7n is a random matrix whose rows are b uniform samples (without replacement) from the standard basis of\nRn, and D\u2208Rn\u00d7n is a diagonal matrix whose diagonal elements are i.i.d. Rademacher random variables.\nH\nDistances, Angles, and Inner Product\nMost of the properties in this section are very standard in literature, e.g., see (Gu et al., 2023).\nLet U \u2208Rn\u00d7k denote an orthonormal basis, we use U\u22a5 \u2208Rn\u00d7(n\u2212k) denote the matrix such that UU \u22a4+U\u22a5U \u22a4\n\u22a5 =In.\nDefinition H.1. Let X \u2208Rn\u00d7k and Y \u2208Rn\u00d7k.\nFor any matrix X, and for orthogonal matrix Y (Y \u22a4Y =Ik) we define\n\u2022 tan\u03b8(Y,X):=\u2225Y \u22a4\n\u22a5 X(Y \u22a4X)\u22121\u2225\nFor orthogonal matrices Y and X (Y \u22a4Y =Ik and X\u22a4X =Ik), we define\n\u2022 cos\u03b8(Y,X):=\u03c3min(Y \u22a4X).\n\u2013 It is obvious that cos(Y,X)=1/\u2225(Y \u22a4X)\u22121\u2225 and cos(Y,X)\u22641.\n\u2022 sin\u03b8(Y,X):=\u2225(I\u2212Y Y \u22a4)X\u2225.\n\u2013 It is obvious that sin\u03b8(Y,X)=\u2225Y\u22a5Y \u22a4\n\u22a5 X\u2225=\u2225Y \u22a4\n\u22a5 X\u2225 and sin\u03b8(Y,X)\u22641.\n\u2022 dist(Y,X):=minQ\u2208Ok\u2225Y Q\u2212X\u2225\nwhere Ok is the set of k\u00d7k orthogonal matrices.\nLemma H.2 (Structural lemma for orthogonal matrices). Let X,Y \u2208Rn\u00d7k be orthogonal matrices. Then\n(Y \u22a4X)\u22a5 = Y \u22a4\n\u22a5 X.\nProof. Let us first compute the Gram of Y \u22a4X, which is\nX\u22a4Y Y \u22a4X = X\u22a4(I\u2212Y\u22a5Y \u22a4\n\u22a5 )X\n= X\u22a4X\u2212X\u22a4Y\u22a5Y \u22a4\n\u22a5 X\n= Ik\u2212X\u22a4Y\u22a5Y \u22a4\n\u22a5 X,\n4In this case, we require logn o be an integer.\n5For our purposes the signs need only be O(logd)-wise independent, and each column can be specified by a O(logd)-wise independent\npermutation, and the seeds specifying the permutations in different columns need only be O(logd)-wise independent.\n6This definition has the same behavior as sparse embedding matrix I for our purpose\n27\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nwhere the first step follows from Y\u22a5Y \u22a4\n\u22a5 +Y Y \u22a4 =I, the second step follows from simple algebra, and the last step follows\nfrom X is an orthogonal matrix, so X\u22a4 =X\u22121.\nThis means that (Y \u22a4X)\u22a5 =Y \u22a4\n\u22a5 X.\nLemma H.3 (Orthogonal and inverse share singular vectors). Let A \u2208 Rk\u00d7k be non-singular, then A\u22a5 and A\u22121 have the\nsame set of singular vectors. Consequently, \u2225A\u22a5A\u22121\u2225=\u2225A\u22a5\u2225\u2225A\u22121\u2225.\nProof. Let A \u2208 Rk\u00d7k and A\u22a4A + A\u22a4\n\u22a5A\u22a5 = Ik, we will show that \u2225A\u22a5A\u22121\u2225 = \u2225A\u22a5\u2225\u2225A\u22121\u2225. Let x \u2208 Rk be the unit\neigenvector of A that realizes the spectral norm, note that\n\u2225A\u22a5x\u22252\n2 = 1\u2212\u2225A\u22252,\nwe argue that x corresponds to the smallest singular value of A\u22a5 via contradiction. Suppose there exists some unit vector\ny with \u2225A\u22a5y\u22252 < \u2225A\u22a5x\u22252, by definition, we know that \u2225A\u22a5y\u22252\n2 + \u2225Ay\u22252\n2 = 1, this means that \u2225Ay\u22252 > \u2225Ax\u22252 = \u2225A\u2225,\ncontradicts the definition of spectral norm. Similarly, if z is the unit vector that realizes the spectral norm of A\u22a5, then it is\nalso singular vector corresponds to the smallest singular value of A, or equivalently, the spectral norm of A\u22121. Our above\nargument essentially implies that A\u22a5 and A\u22121 have the same set of singular vectors. The proof is then straightforward:\nsuppose A\u22a5z =\u03bbz and A\u22121z =\u00b5z, then\nA\u22a5A\u22121z = A\u22a5\u00b5z\n= \u00b5(A\u22a5z)\n= \u03bb\u00b5z,\nwhere the first step follows from our assumption, the second step follows from \u00b5 is a real number and a real number multiplying\na matrix is commutative and follows from the associative property, and the third step follows from our assumption.\nThus, we have \u2225A\u22a5A\u22121\u2225=\u2225A\u22a5\u2225\u2225A\u22121\u2225, and we have proved the assertion.\nLemma H.4. Let X,Y \u2208Rn\u00d7k be orthogonal matrices, then\ntan\u03b8(Y,X)= sin\u03b8(Y,X)\ncos\u03b8(Y,X).\nProof. Due to Lemma H.2, we have (Y \u22a4X)\u22a5 =Y \u22a4\n\u22a5 X. Thus, tan\u03b8(Y,X)=\u2225(Y \u22a4X)\u22a5(Y \u22a4X)\u22121\u2225. The proof then follows\nstraightforwardly from Lemma H.3.\nLemma H.5. Let X,Y \u2208Rn\u00d7k be orthogonal matrices, then sin2\u03b8(Y,X)+cos2\u03b8(Y,X)=1.\nProof. Recall that cos\u03b8(Y,X) =\n1\n\u2225(Y \u22a4X)\u22121\u2225 and sin\u03b8(Y,X) = \u2225Y \u22a4\n\u22a5 X\u2225, by Lemma H.2, we know that (Y \u22a4X)\u22a5 = Y \u22a4\n\u22a5 X,\ntherefore sin\u03b8(Y,X) = \u2225(Y \u22a4X)\u22a5\u2225. Let A := Y \u22a4X, by Lemma H.3, we know that A\u22a5 and A\u22121 have the same singular\nvectors, or equivalently, the singular vector realizing \u2225A\u22a5\u2225 corresponds to the smallest singular value of A. Let z \u2208Rk be\nthe unit singular vector with singular value \u2225A\u22a5\u2225, then\nz\u22a4A\u22a4Az+z\u22a4A\u22a4\n\u22a5A\u22a5z = 1,\n\u2225A\u22a5\u22252+\u03c32\nmin(A)= 1,\ncos2\u03b8(Y,X)+sin2\u03b8(Y,X)= 1.\nThis completes the proof.\nH.1\nAngle is close\nLemma H.6. Let \u03f5\u2208(0,0.1) Let x denote a unit vector, i.e., \u2225x\u22252 =1.\nLet z =(x+y)/\u2225x+y\u22252.\nIf \u2225y\u22252 \u2264\u03f5\u00b7\u2225x\u22252, then\np\n1\u2212\u27e8x,z\u27e92 \u22642\u221a\u03f5\nProof. We have\n\u2225x+y\u22252 \u2265 \u2225x\u22252\u2212\u2225y\u22252\n\u2265 1\u2212\u03f5\nwhere the first step follows from triangle inequality.\n28\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nWe also have\n\u2225x+y\u22252 \u2264 \u2225x\u22252+\u2225y\u22252\n\u2264 1+\u03f5\n(7)\nWe have\n(1\u2212\u03f5)2 \u22651\u22122\u03f5\n(8)\nWe also have\n1\n(1+\u03f5)2 \u22651\u22123\u03f5\n(9)\nwhere \u03f5\u2208(0,0.1).\nCombining Eq. (8) and Eq. (9), we have\n1\n(1+\u03f5)2 \u00b7(1\u2212\u03f5)2 \u2265 (1\u22122\u03f5)\u00b7(1\u22123\u03f5)\n= 1\u22125\u03f5+6\u03f52\n\u2265 1\u22125\u03f5+\u03f5\n= 1\u22124\u03f5\n(10)\nwhere the first step follows from Eq. (8) and Eq. (9) and the rest of them follow from simple algebra.\nFinally, we have\n1\u2212\u27e8x,z\u27e92 = 1\u2212\u27e8x,\nx+y\n\u2225x+y\u22252\n\u27e92\n= 1\u2212\n1\n\u2225x+y\u22252\n2\n\u27e8x,x+y\u27e92\n= 1\u2212\n1\n\u2225x+y\u22252\n2\n\u00b7(\u2225x\u22252\n2+\u27e8x,y\u27e9)2\n= 1\u2212\n1\n\u2225x+y\u22252\n2\n\u00b7(1+\u27e8x,y\u27e9)2\n\u2264 1\u2212\n1\n(1+\u03f5)2 \u00b7(1+\u27e8x,y\u27e9)2\n\u2264 1\u2212\n1\n(1+\u03f5)2 \u00b7(1\u2212\u03f5)2\n\u2264 1\u2212(1\u22124\u03f5)\n= 4\u03f5,\nwhere the first step follow the definition of z, the second step follows from the reorganization, the third step follows from\nthe definition of inner product, the fourth step follows from \u2225x\u22252 =1, the fifth step follows from Eq. (7), the sixth step follows\nfrom 1+\u27e8x,y\u27e9\u22651\u2212|\u27e8x,y\u27e9|\u22651\u2212\u2225x\u22252\u00b7\u2225y\u22252 \u22651\u2212\u03f5, the seventh step follows from Eq. (10) and the last step follows from\nsimple algebra.\nI\nFunction Approximations\nWe first we show the function approximation for two operators in Section I.1, which means that there are two functions. Then\nwe show the function approximations for four operators in Section I.2.\nI.1\nFunction Approximations for Two Operators\nLemma I.1. Let f1 :Rd \u2192Rd and let f2 :Rd \u2192Rd.\nAssume the the following conditions\n\u2022 Condition 1a. f1 is a linear function\n\u2022 Condition 1b. \u2225f1(x)\u22252 \u2264\u03f51\u2225x\u22252 (f1 is shrinking)\n\u2022 Condition 1c. \u2225f1(x)\u2212f1(y)\u22252 \u2264L1\u2225x\u2212y\u22252 (f1 is Lipschitz)\n29\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n\u2022 Condition 2a. f2 is a linear function\n\u2022 Condition 2b. \u2225f2(x)\u22252 \u2264\u03f52\u2225x\u22252 (f2 is shrinking)\n\u2022 Condition 2c. \u2225f2(x)\u2212f2(y)\u22252 \u2264L2\u2225x\u2212y\u22252 (f2 is Lipschitz)\nWe define three functions\n\u2022\ng1(x)=: (I+f1)\u00b7(I+f2)(x)\n= x+f2(x)+f1(x+f2(x))\n\u2022\ng2(x)=: (I+f2)\u00b7(I+f1)(x)\n= x+f1(x)+f2(x+f1(x))\n\u2022\ng3(x)=: (I+f1+f2)(x)\n= x+f1(x)+f2(x)\nThen we can show that\n\u2022 Part 1. \u2225g1(x)\u2212g2(x)\u22252 \u22642\u03f51\u03f52\u2225x\u22252(if f1 and f2 are linear functions)\n\u2022 Part 2. \u2225g1(x)\u2212g2(x)\u22252 \u2264(\u03f52\u00b7L1+\u03f51\u00b7L2)\u2225x\u22252 (if f1 and f2 are Lipschitz functions)\n\u2022 Part 3. \u2225g1(x)\u2212g3(x)\u22252 \u2264\u03f51\u03f52\u2225x\u22252 (if f1 is a linear function)\n\u2022 Part 4. \u2225g1(x)\u2212g3(x)\u22252 \u2264\u03f52\u00b7L1\u2225x\u22252 (if f1 is a Lipschitz function)\n\u2022 Part 5. \u2225g2(x)\u2212g3(x)\u22252 \u2264\u03f51\u03f52\u2225x\u22252 (if f2 is a linear function)\n\u2022 Part 6. \u2225g2(x)\u2212g3(x)\u22252 \u2264\u03f51\u00b7L2\u2225x\u22252 (if f2 is a Lipschitz function)\nProof. Part 1.\nWe have\n\u2225g1(x)\u2212g2(x)\u22252 \u2264 \u2225g1(x)\u2212g3(x)\u22252+\u2225g3(x)\u2212g2(x)\u22252\n\u2264 \u03f51\u03f52\u2225x\u22252+\u03f51\u03f52\u2225x\u22252\n= 2\u03f51\u03f52\u2225x\u22252\nwhere the first step follows from triangular inequality, the second step follows from Part 3 and Part 5 and the last step follows\nfrom simple algebra.\nPart 2.\nWe have\n\u2225g1(x)\u2212g2(x)\u22252 \u2264 \u2225g1(x)\u2212g3(x)\u22252+\u2225g3(x)\u2212g2(x)\u22252\n\u2264 \u03f52\u00b7L1\u2225x\u22252+\u03f51\u00b7L2\u2225x\u22252\n= (\u03f52\u00b7L1+\u03f51\u00b7L2)\u2225x\u22252\nwhere the first step follows from triangular inequality, the second step follows from Part 4 and Part 6 and the last step follows\nfrom simple algebra.\nPart 3.\n30\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nWe have\n\u2225g1(x)\u2212g3(x)\u22252 = \u2225f1(x+f2(x))\u2212f1(x)\u22252\n= \u2225f1(x+f2(x)\u2212x)\u22252\n= \u2225f1(f2(x))\u22252\n\u2264 \u03f51\u00b7\u2225f2(x)\u22252\n\u2264 \u03f51\u00b7\u03f52\u00b7\u2225x\u22252,\nwhere the first step follows from the definition of g1 and g3, the second step follows from the fact that f1 is a linear function, the\nthird step follows from simple algebra, the fourth step follows from Condition 1b and the last step follows from Condition 2b.\nPart 4.\n\u2225g1(x)\u2212g3(x)\u22252 = \u2225f1(x+f2(x))\u2212f1(x)\u22252\n\u2264 L1\u00b7\u2225x+f2(x)\u2212x\u22252\n= L1\u00b7\u2225f2(x)\u22252\n\u2264 L1\u00b7\u03f52\u2225x\u22252,\nwhere the first step follows from definition of g1 and g3, the second step follows from Condition 1c, the third step follows\nfrom simple algebra and the last step follows from Condition 2b.\nPart 5.\nWe have\n\u2225g2(x)\u2212g3(x)\u22252 = \u2225f2(x+f1(x))\u2212f2(x)\u22252\n= \u2225f2(x+f1(x)\u2212x)\u22252\n= \u2225f2(f1(x))\u22252\n\u2264 \u03f52\u00b7\u2225f1(x)\u22252\n\u2264 \u03f52\u00b7\u03f51\u00b7\u2225x\u22252,\nwhere the first step follows from the definition of g2 and g3, the second step follows from the fact that f2 is a linear function, the\nthird step follows from simple algebra, the fourth step follows from Condition 2b and the last step follows from Condition 1b.\nPart 6.\n\u2225g2(x)\u2212g3(x)\u22252 = \u2225f2(x+f1(x))\u2212f2(x)\u22252\n\u2264 L2\u00b7\u2225x+f1(x)\u2212x\u22252\n= L2\u00b7\u2225f1(x)\u22252\n\u2264 L2\u00b7\u03f51\u2225x\u22252,\nwhere the first step follows from definition of g1 and g3, the second step follows from Condition 2c, the third step follows from\nsimple algebra and the last step follows from Condition 1b.\nI.2\nFunction Approximations for Four Operators\nLemma I.2. For each i\u2208[4], we assume the following conditions\n\u2022 i(a) fi is a linear function\n\u2022 i(b) \u2225fi(x)\u22252 \u2264\u03f5i\u2225x\u22252 (fi is shriking)\n\u2022 i(c) \u2225fi(x)\u2212fi(y)\u22252 \u2264Li\u2225x\u2212y\u22252 (fi is Lipschitz)\nWe define three functions\n\u2022 g1(x):=(I+f1)\u00b7(I+f2)\u00b7(I+f3)\u00b7(I+f4)(x)\n\u2022 g2(x):=(I+f1)\u00b7(I+f3)\u00b7(I+f2)\u00b7(I+f4)(x)\n\u2022 g3(x):=(I+f1+f2+f3+f4)(x)\n31\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nThen, we can show that\n\u2022 Part 1. \u2225g1(x)\u2212g2(x)\u22252 \u22642(\u03f51\u03f52+\u03f51\u03f53+\u03f51\u03f54+\u03f52\u03f53+\u03f52\u03f54+\u03f53\u03f54+\u03f51\u03f52\u03f53+\u03f51\u03f52\u03f54+\u03f51\u03f53\u03f54+\u03f52\u03f53\u03f54+\u03f51\u03f52\u03f53\u03f54)\u2225x\u22252\n(if fi, \u2200i\u2208[4] are linear functions)\n\u2022 Part 2. \u2225g1(x) \u2212 g2(x)\u22252 \u2264 (2L1\u03f52 + 2L1\u03f53 + 2L1\u03f54 + L2\u03f53 + 2L2\u03f54 + 2L3\u03f54 + 2L1\u03f52\u03f53 + 2L1\u03f52\u03f54 + 2L1\u03f53\u03f54 +\nL2\u03f53\u03f54+2L1\u03f52\u03f53\u03f54+L3\u03f52+L3\u03f52\u03f54)\u2225x\u22252 (if fi, \u2200i\u2208[4] are Lipschitz functions)\n\u2022 Part 3. \u2225g1(x)\u2212g3(x)\u22252 \u2264(\u03f51\u03f52+\u03f51\u03f53+\u03f51\u03f54+\u03f52\u03f53+\u03f52\u03f54+\u03f53\u03f54+\u03f51\u03f52\u03f53+\u03f51\u03f52\u03f54+\u03f51\u03f53\u03f54+\u03f52\u03f53\u03f54+\u03f51\u03f52\u03f53\u03f54)\u2225x\u22252\n(if fi, \u2200i\u2208[4] are linear functions)\n\u2022 Part 4. \u2225g1(x) \u2212 g3(x)\u22252 \u2264 (L1\u03f52 + L1\u03f53 + L1\u03f54 + L2\u03f53 + L2\u03f54 + L3\u03f54 + L1\u03f52\u03f53 + L1\u03f52\u03f54 + L1\u03f53\u03f54 + L2\u03f53\u03f54 +\nL1\u03f52\u03f53\u03f54)\u2225x\u22252 (if fi, \u2200i\u2208[4] are Lipschitz functions)\n\u2022 Part 5. \u2225g2(x)\u2212g3(x)\u22252 \u2264(\u03f51\u03f52+\u03f51\u03f53+\u03f51\u03f54+\u03f52\u03f53+\u03f52\u03f54+\u03f53\u03f54+\u03f51\u03f52\u03f53+\u03f51\u03f52\u03f54+\u03f51\u03f53\u03f54+\u03f52\u03f53\u03f54+\u03f51\u03f52\u03f53\u03f54)\u2225x\u22252\n(if fi, \u2200i\u2208[4] are linear functions)\n\u2022 Part 6.\u2225g2(x) \u2212 g3(x)\u22252 \u2264 (L1\u03f52 + L1\u03f53 + L1\u03f54 + L2\u03f54 + L3\u03f52 + L3\u03f54 + L1\u03f52\u03f53 + L1\u03f52\u03f54 + L1\u03f53\u03f54 + L3\u03f52\u03f54 +\nL1\u03f52\u03f53\u03f54)\u2225x\u22252\n(if fi, \u2200i\u2208[4] are Lipschitz functions)\nProof. Part 1.\nWe have\n\u2225g1(x)\u2212g2(x)\u22252 \u2264 \u2225g1(x)\u2212g3(x)\u22252+\u2225g3(x)\u2212g2(x)\u22252\n\u2264 2(\u03f51\u03f52+\u03f51\u03f53+\u03f51\u03f54+\u03f52\u03f53+\u03f52\u03f54+\u03f53\u03f54+\u03f51\u03f52\u03f53+\u03f51\u03f52\u03f54+\u03f51\u03f53\u03f54+\u03f52\u03f53\u03f54+\u03f51\u03f52\u03f53\u03f54)\u2225x\u22252\nwhere the first step follows from triangular inequality and the last step follows from Part 3 and Part 5.\nPart 2.\nWe have\n\u2225g1(x)\u2212g2(x)\u22252 \u2264 \u2225g1(x)\u2212g3(x)\u22252+\u2225g3(x)\u2212g2(x)\u22252\n\u2264 (2L1\u03f52+2L1\u03f53+2L1\u03f54+L2\u03f53+2L2\u03f54+2L3\u03f54+2L1\u03f52\u03f53+2L1\u03f52\u03f54+2L1\u03f53\u03f54+\nL2\u03f53\u03f54+2L1\u03f52\u03f53\u03f54+L3\u03f52+L3\u03f52\u03f54)\u2225x\u22252\nwhere the first step follows from triangular inequality and the last step follows from Part 4 and Part 6.\nPart 3. We have\n\u2225g1(x)\u2212g3(x)\u22252 = \u2225(I+f1)\u00b7(I+f2)\u00b7(I+f3)\u00b7(x+f4(x))\u2212(I+f1+f2+f3+f4)(x))\u22252\n= \u2225(x+f4(x)+f3(x+f4(x))+f2(x+f4(x)+\nf3(x+f4(x)))+f1(x+f4(x)+f3(x+f4(x))+f2(x+f4(x)+f3(x+f4(x))))\n\u2212(I+f1+f2+f3+f4)(x)))\u22252\n= \u2225f3(f4(x))+f2(f4(x)+f3(x+f4(x)))+f1(f4(x)+\nf3(x+f4(x))+f2(x+f4(x)+f3(x+f4(x)))))\u22252\n= \u2225f3(f4(x))+f2(f4(x))+f2(f3(x))+f2(f3(f4(x)))+\nf1(f4(x))+f1(f3(x))+f1(f3(f4(x)))+f1(f2(x))+f1(f2(f4(x)))\n+f1(f2(f3(x)))+f1(f2(f3(f4(x)))))\u22252\n\u2264 \u2225f3(f4(x))\u22252+\u2225f2(f4(x))\u22252+\u2225f2(f3(x))\u22252+\u2225f2(f3(f4(x)))\u22252+\n\u2225f1(f4(x))\u22252+\u2225f1(f3(x))\u22252+\u2225f1(f3(f4(x)))\u22252+\u2225f1(f2(x))\u22252+\u2225f1(f2(f4(x)))\u22252+\n\u2225f1(f2(f3(x)))\u22252+\u2225f1(f2(f3(f4(x))))\u22252\n\u2264 (\u03f53\u03f54+\u03f52\u03f54+\u03f52\u03f53+\u03f52\u03f53\u03f54+\u03f51\u03f54+\u03f51\u03f53+\u03f51\u03f53\u03f54+\u03f51\u03f52+\u03f51\u03f52\u03f54+\u03f51\u03f52\u03f53+\u03f51\u03f52\u03f53\u03f54)\u2225x\u22252\n= (\u03f51\u03f52+\u03f51\u03f53+\u03f51\u03f54+\u03f52\u03f53+\u03f52\u03f54+\u03f53\u03f54+\u03f51\u03f52\u03f53+\u03f51\u03f52\u03f54+\u03f51\u03f53\u03f54+\u03f52\u03f53\u03f54+\u03f51\u03f52\u03f53\u03f54)\u2225x\u22252,\nwhere the first step follows from the definition of g1 and g3, the second step follows from simple algebra, the third step follows\nfrom reorganization, the fourth step follows from the fact that all fi,\u2200i\u2208[4] are linear function, the fifth step follows from\ntriangular inequality, the sixth step follows from i(b) and the last step follows from reorganization.\n32\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nPart 4. We have\n\u2225g1(x)\u2212g3(x)\u22252 = \u2225(I+f1)\u00b7(I+f2)\u00b7(I+f3)\u00b7(x+f4(x))\u2212(I+f1+f2+f3+f4)(x))\u22252\n= \u2225x+f4(x)+f3(x+f4(x))+f2(x+f4(x)+\nf3(x+f4(x)))+f1(x+f4(x)+f3(x+f4(x))+f2(x+f4(x)+f3(x+f4(x))))\n\u2212(I+f1+f2+f3+f4)(x))\u22252\n= \u2225f3(x+f4(x))+f2(x+f4(x)+f3(x+f4(x)))\n+f1(x+f4(x)+f3(x+f4(x))+f2(x+f4(x)+f3(x+f4(x))))\n\u2212f1(x)\u2212f2(x)\u2212f3(x))\u22252\n= \u2225f3(x+f4(x))\u2212f3(x)+f2(x+f4(x)+f3(x+f4(x)))\u2212f2(x)\n+f1(x+f4(x)+f3(x+f4(x))+f2(x+f4(x)+f3(x+f4(x))))\u2212f1(x)\u2225\n\u2264 L3\u2225x+f4(x)\u2212x\u22252+L2\u2225x+f4(x)+f3(x+f4(x))\u2212x\u22252\n+L1\u2225x+f4(x)+f3(x+f4(x))+f2(x+f4(x)+f3(x+f4(x)))\u2212x\u22252\n\u2264 L3\u2225f4(x)\u22252+L2\u2225f4(x)+f3(x+f4(x))\u22252\n+L1\u2225f4(x)+f3(x+f4(x))+f2(x+f4(x)+f3(x+f4(x)))\u22252\n\u2264 L3\u03f54\u2225x\u22252+L2\u03f54\u2225x\u22252+L2\u03f53\u2225x+f4(x)\u22252\n+L1\u03f54\u2225x\u22252+L1\u03f53\u2225x+f4(x)\u22252+L1\u03f52\u2225x+f4(x)+f3(x+f4(x))\u22252\n\u2264 L3\u03f54\u2225x\u22252+L2\u03f54\u2225x\u22252+L2\u03f53\u2225x\u2225+L2\u03f53\u03f54\u2225x\u22252\n+L1\u03f54\u2225x\u22252+L1\u03f53\u2225x\u22252+L1\u03f53\u03f54\u2225x\u22252+L1\u03f52\u2225x\u22252+L1\u03f52\u03f54\u2225x\u22252+L1\u03f52\u03f53\u2225x+f4(x)\u22252\n\u2264 L3\u03f54\u2225x\u22252+L2\u03f54\u2225x\u22252+L2\u03f53\u2225x\u2225+L2\u03f53\u03f54\u2225x\u22252\n+L1\u03f54\u2225x\u22252+L1\u03f53\u2225x\u22252+L1\u03f53\u03f54\u2225x\u22252+L1\u03f52\u2225x\u22252+L1\u03f52\u03f54\u2225x\u22252+L1\u03f52\u03f54\u2225x\u22252+L1\u03f52\u03f53\u03f54\u2225x\u22252\n= (L3\u03f54+L2\u03f54+L2\u03f53+L2\u03f53\u03f54+L1\u03f54+L1\u03f53+L1\u03f53\u03f54+L1\u03f52+L1\u03f52\u03f54+L1\u03f52\u03f53+L1\u03f52\u03f53\u03f54)\u2225x\u22252\n= (L1\u03f52+L1\u03f53+L1\u03f54+L2\u03f53+L2\u03f54+L3\u03f54+L1\u03f52\u03f53+L1\u03f52\u03f54+L1\u03f53\u03f54+L2\u03f53\u03f54+L1\u03f52\u03f53\u03f54)\u2225x\u22252\nwhere the first step follows from the definition of g1 and g3, the second step follows from simple algebra, the third step follows\nfrom simple algebra, the fourth step follows from reorganization, the fifth step follows from the fact that all fi,\u2200i\u2208[4] are Lips-\nchitz functions, the sixth step follows from simple algebra, the seventh step follows from i(b), the eighth step follows from trian-\ngular inequality, the ninth step follows from i(b), the tenth step follows from i(b) and the last step follows from reorganization.\nPart 5. We have\n\u2225g2(x)\u2212g3(x)\u22252 = \u2225(I+f1)\u00b7(I+f3)\u00b7(I+f2)\u00b7(x+f4(x))\u2212(I+f1+f2+f3+f4)(x))\u22252\n= \u2225(x+f4(x)+f2(x+f4(x))+f3(x+f4(x)+\nf2(x+f4(x)))+f1(x+f4(x)+f2(x+f4(x))+f3(x+f4(x)+f2(x+f4(x))))\n\u2212(I+f1+f2+f3+f4)(x)))\u22252\n= \u2225f2(f4(x))+f3(f4(x))+\nf3(f2(x+f4(x)))+f1(f4(x))+f1(f2(x+f4(x)))+f1(f3(x+f4(x)+f2(x+f4(x)))))\u22252\n\u2264 (\u03f52\u03f54+\u03f53\u03f54+\u03f53\u03f52+\u03f53\u03f52\u03f54+\u03f51\u03f54+\u03f51\u03f52+\u03f51\u03f52\u03f54+\u03f51\u03f53+\u03f51\u03f53\u03f54+\u03f51\u03f53\u03f52+\u03f51\u03f53\u03f52\u03f54)\u2225x\u22252\n= (\u03f51\u03f52+\u03f51\u03f53+\u03f51\u03f54+\u03f52\u03f53+\u03f52\u03f54+\u03f53\u03f54+\u03f51\u03f52\u03f53+\u03f51\u03f52\u03f54+\u03f51\u03f53\u03f54+\u03f52\u03f53\u03f54+\u03f51\u03f52\u03f53\u03f54)\u2225x\u22252,\nwhere the first step follows from the definition of g2 and g3, the second step follows from simple algebra, the third step follows\nfrom the fact that all fi,\u2200i\u2208[4] are linear function, the fourth step follows from triangular inequality and i(b), and the last\nstep follows from reorganization.\n33\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nPart 6. We have\n\u2225g2(x)\u2212g3(x)\u22252 = \u2225(I+f1)\u00b7(I+f3)\u00b7(I+f2)\u00b7(x+f4(x))\u2212(I+f1+f2+f3+f4)(x))\u22252\n= \u2225(x+f4(x)+f2(x+f4(x))+f3(x+f4(x)+f2(x+f4(x)))\n+f1(x+f4(x)+f2(x+f4(x))+f3(x+f4(x)+f2(x+f4(x))))\n\u2212(I+f1+f2+f3+f4)(x)))\u22252\n= \u2225f2(x+f4(x))\u2212f2(x)+f3(x+f4(x)+f2(x+f4(x)))\u2212f3(x)\n+f1(x+f4(x)+f2(x+f4(x))+f3(x+f4(x)+f2(x+f4(x))))\u2212f1(x)\u22252\n\u2264 \u2225f2(x+f4(x))\u2212f2(x)\u22252+\u2225f3(x+f4(x)+f2(x+f4(x)))\u2212f3(x)\u22252\n+\u2225f1(x+f4(x)+f2(x+f4(x))+f3(x+f4(x)+f2(x+f4(x))))\u2212f1(x)\u22252\n\u2264 L2\u03f54\u2225x\u22252+L3\u03f54\u2225x\u22252+L3\u03f52\u2225x+f4(x)\u22252\n+L1\u03f54\u2225x\u22252+L1\u03f52\u2225x+f4(x)\u22252+L1\u03f53\u2225x+f4(x)+f2(x+f4(x))\u22252\n\u2264 L2\u03f54\u2225x\u22252+L3\u03f54\u2225x\u22252+L3\u03f52\u2225x\u22252+L3\u03f52\u03f54\u2225x\u22252\n+L1\u03f54\u2225x\u22252+L1\u03f52\u2225x\u22252+L1\u03f52\u03f54\u2225x\u22252+L1\u03f53\u2225x\u2225+L1\u03f53\u03f54\u2225x\u22252+L1\u03f53\u03f52\u2225x\u22252+L1\u03f53\u03f52\u03f54\u2225x\u22252\n= (L1\u03f52+L1\u03f53+L1\u03f54+L2\u03f54+L3\u03f52+L3\u03f54+L1\u03f52\u03f53+L1\u03f52\u03f54+L1\u03f53\u03f54+L3\u03f52\u03f54+L1\u03f52\u03f53\u03f54)\u2225x\u22252\nwhere the first step follows from the definition of g2 and g3, the second step follows from simple algebra, the third step follows\nfrom reorganization, the fourth step follows from triangular inequality, the fifth step follows from the fact that all fi,\u2200i\u2208[4] are\nLipschitz functions and i(b), the sixth step follows from triangular inequality, and the last step follows from reorganization.\nJ\nNearest Neighbor Search Data Structure\nWe use the reduction-based approximate MaxIP method with LSH data-structure to achieve sublinear iteration cost. Note\nthat we choose this method due to its clear theoretical guarantee on the retrieval results. It is well-known that an LSH\ndata-structures is used for approximate nearest neighbor problem. The following definition of approximate nearest neighbor\nsearch is very standard in literature (Arya & Mount, 1993; Indyk & Motwani, 1998a; Datar et al., 2004; Andoni et al., 2014;\n2015; Andoni & Razenshteyn, 2015; Indyk & Wagner, 2018; Andoni et al., 2017; 2018; Dong et al., 2019; Chen et al., 2020b;\nLi & Li, 2022; Li et al., 2019).\nJ.1\nLSH and MaxIP\nWe start with the defining the Approximate Nearest Neighbor (ANN) problem (Arya & Mount, 1993; Indyk & Motwani,\n1998a; Datar et al., 2004; Andoni et al., 2014; 2015; Andoni & Razenshteyn, 2015; Indyk & Wagner, 2018; Andoni et al.,\n2017; 2018; Dong et al., 2019; Chen et al., 2020b) as:\nDefinition J.1 (Approximate Nearest Neighbor (ANN)). Let c>1 and r\u2208(0,2) denote two parameters. Given an n-vector\nset Y \u2282Sd\u22121 on a unit sphere, the objective of the (c,r)-Approximate Nearest Neighbor (ANN) is to construct a data structure\nthat, for any query x\u2208Sd\u22121 such that miny\u2208Y \u2225y\u2212x\u22252 \u2264r, it returns a vector z from Y that satisfies \u2225z\u2212x\u22252 \u2264c\u00b7r.\nThe ANN problem can be solved via locality sensitive hashing (LSH) (Indyk & Motwani, 1998a; Datar et al., 2004; Indyk\n& Wagner, 2018). In this paper, we use the standard definitions of LSH (see Indyk and Motwani (Indyk & Motwani, 1998a)).\nDefinition J.2 (Locality Sensitive Hashing). Let c > 1 denote a parameter. Let p1,p2 \u2208 (0,1) denote two parameters and\np1 > p2. We say a function family H is (r,c\u00b7r,p1,p2)-sensitive if and only if, for any vectors x,y \u2208 Rd, for any h chosen\nuniformly at random from H, we have:\n\u2022 if \u2225x\u2212y\u22252 \u2264r, then Prh\u223cH[h(x)=h(y)]\u2265p1,\n\u2022 if \u2225x\u2212y\u22252 \u2265c\u00b7r, then Prh\u223cH[h(x)=h(y)]\u2264p2.\nNext, we show that LSH solves ANN problem with sublinear query time complexity.\nTheorem J.3 (Andoni, Laarhoven, Razenshteyn and Waingarten (Andoni et al., 2017)). Let c>1 and r\u2208(0,2) denote two\nparameters. One can solve (c,r)-ANN on a unit sphere in query time O(d\u00b7n\u03c1) using preprocessing time O(dn1+o(1)) and\nspace O(n1+o(1)+dn), where \u03c1= 2\nc2 \u2212 1\nc4 +o(1).\n34\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nHere we write o(1) is equivalent to O(1/\u221alogn). Note that we could reduce d to no(1) with Johnson\u2013Lindenstrauss\nLemma (Johnson & Lindenstrauss, 1984). Besides, we could achieve better \u03c1 using LSH in (Andoni & Razenshteyn, 2015)\nif we allowed to have more proprocessing time.\nIn this work, we focus on a well-known problem in computational complexity: approximate MaxIP. In this work, we follow\nthe standard notation in (Chen, 2018) and define the approximate MaxIP problem as follows:\nDefinition J.4 (Approximate MaxIP). Let c\u2208(0,1) and \u03c4 \u2208(0,1) denote two parameters. Given an n-vector dataset Y \u2282Sd\u22121\non a unit sphere, the objective of the (c,\u03c4)-MaxIP is to construct a data structure that, given a query x \u2208 Sd\u22121 such that\nmaxy\u2208Y \u27e8x,y\u27e9\u2265\u03c4, it retrieves a vector z from Y that satisfies \u27e8x,z\u27e9\u2265c\u00b7maxy\u2208Y \u27e8x,y\u27e9.\nIn many applications, it is more convenient to doing inner product search in a transformed/projected space compared to doing\ninner product search in the original space. Thus, we propose the following definitions (Definition J.5 and Definition J.6)\nDefinition J.5 (Projected MaxIP). Let \u03d5,\u03c8 :Rd \u2192Rk denote two transforms. Given a data set Y \u2286Rd and a point x\u2208Rd,\nwe define (\u03d5,\u03c8)-MaxIP as follows:\n(\u03d5,\u03c8)-MaxIP(x,Y ):=max\ny\u2208Y \u27e8\u03d5(x),\u03c8(y)\u27e9\nDefinition J.6 (Projected approximate MaxIP). Let \u03d5,\u03c8 : Rd \u2192 Rk denote two transforms. Given an n-vector dataset\nY \u2282Rd so that \u03c8(Y )\u2282Sk\u22121, the goal of the (c,\u03d5,\u03c8,\u03c4)-MaxIP is to construct a data structure that, given a query x\u2208Rd and\n\u03d5(x)\u2208Sk\u22121 such that maxy\u2208Y \u27e8\u03d5(x),\u03c8(y)\u27e9\u2265\u03c4, it retrieves a vector z \u2208Y that satisfies \u27e8\u03d5(x),\u03c8(z)\u27e9\u2265c\u00b7(\u03d5,\u03c8)-MaxIP(x,Y ).\nJ.2\nConnections\nFact J.7. Let ex denote the vector that \u27e8ex,x\u27e9\u22651\u2212 1\n2\u03f52, where both ex and x are unit vectors. We have\n\u2225ex\u2212x\u22252 \u2264\u03f5\nProof.\n\u2225ex\u2212x\u22252 = (\u2225ex\u22252\n2+\u2225x\u22252\n2\u22122\u27e8x,ex\u27e9)1/2\n= (2\u22122\u27e8x,ex\u27e9)1/2\n\u2264 (2\u22122(1\u2212 1\n2\u03f52))1/2\n= \u03f5\nNow, we complete the proof.\nLemma J.8. Let ex denote the vector that \u27e8ex,x\u27e9\u22651\u2212 1\n2\u03f52, where both ex and x are unit vectors. Let 0.01c\u00b7\u03c4 >\u03f5.\nSuppose there is a z \u2208Y , where \u2225z\u22252 =1, such that\n\u27e8x,z\u27e9\u2265c\u00b7max\ny\u2208Y \u27e8x,y\u27e9\nNote that maxy\u2208Y \u27e8x,y\u27e9\u2265\u03c4. Then, we can find a z \u2208Y such that\n\u27e8ex,z\u27e9\u2265 1\n2c\u00b7max\ny\u2208Y \u27e8x,y\u27e9\nProof. We have\n\u27e8ex,z\u27e9= \u27e8x,z\u27e9+\u27e8ex\u2212x,z\u27e9\n\u2265 \u27e8x,z\u27e9\u2212|\u27e8ex\u2212x,z\u27e9|\n\u2265 \u27e8x,z\u27e9\u2212\u2225ex\u2212x\u22252\u00b7\u2225z\u22252\n\u2265 \u27e8x,z\u27e9\u2212\u03f5\n\u2265 c\u00b7max\ny\u2208Y \u27e8x,y\u27e9\u2212\u03f5\n\u2265 0.99\u00b7c\u00b7max\ny\u2208Y \u27e8x,y\u27e9\nwhere the first step follows from simple algebra, the second step follows from the fact that \u27e8x, y\u27e9 \u2265 \u2212|\u27e8x, y\u27e9|, the\nthird step follows from the property of inner product, the fourth step follows from Fact J.7, the fifth step follows from\n\u27e8x,z\u27e9\u2265c\u00b7maxy\u2208Y \u27e8x,y\u27e9 and the final step follows from the fact that 0.01c\u00b7\u03c4 >\u03f5.\n35\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nJ.3\nEfficient Transformations\nWe have learned from that (c,\u03c4)-MaxIP on a unit sphere Sd\u22121 using LSH for ANN. Therefore, the next step is to transform\nthe direction search procedure in iterative optimization algorithm into a MaxIP on a unit sphere. To achieve this, we formulate\nthe direction search as a projected approximate MaxIP (see Definition J.5). We start with presenting a pair of transformation\n\u03d50,\u03c80 :Rd \u2192Rd+1 such that, given a function g:Rd \u2192R, for any x,y in a convex set K, we have\n\u03d50(x):=[\u2207g(x)\u22a4,x\u22a4\u2207g(x)]\u22a4, \u03c80(y):=[\u2212y\u22a4,1]\u22a4.\n(11)\nIn this way, we show that\n\u27e8y\u2212x,\u2207g(x)\u27e9= \u2212\u27e8\u03d50(x),\u03c80(y)\u27e9,\nargmin\ny\u2208Y \u27e8y\u2212x,\u2207g(x)\u27e9= argmax\ny\u2208Y \u27e8\u03d50(x),\u03c80(y)\u27e9\n(12)\nTherefore, we could transform the direction search problem into a MaxIP problem.\nNext, we present a standard transformations (Neyshabur & Srebro, 2015) that connects the MaxIP to ANN in unit sphere.\nFor any x,y\u2208Rd, we propose transformation \u03d51,\u03c81 :Rd \u2192Rd+2 such that\n\u03d51(x)=\nh\n(D\u22121\nx x)\u22a4\n0\nq\n1\u2212\u2225xD\u22121\nx \u22252\n2\ni\u22a4\n\u03c81(y)=\nh\n(D\u22121\ny y)\u22a4\nq\n1\u2212\u2225yD\u22121\ny \u22252\n2\n0\ni\u22a4\n(13)\nHere Dx, Dy are some constant that make sure both x/Dx and y/Dy have norms less than 1. Under these transformations,\nboth \u03d51(x) and \u03c81(y) have norm 1 and argmaxy\u2208Y \u27e8\u03d51(x),\u03c81(y)\u27e9=argmaxy\u2208Y \u27e8x,y\u27e9.\nCombining transformations in Eq. (11) and Eq. (13), we obtain query transform \u03d5:Rd \u2192Rd+3 with form \u03d5(x)=\u03d51(\u03d50(x))\nand data transform \u03d5:Rd \u2192Rd+3 with form \u03c8(y)=\u03c81(\u03c80(y)). Using \u03d5 and \u03c8, we transform the direction search problem in\noptimization into a MaxIP in unit sphere. Moreover, given a set Y \u2282Rd and a query x\u2208Rd, the solution z of (c,\u03d5,\u03c8,\u03c4)-MaxIP\nover (x,Y ) has the propriety that \u27e8z\u2212x,\u2207g(x)\u27e9\u2264c\u00b7miny\u2208Y \u27e8y\u2212x,\u2207g(x)\u27e9. Thus, we could approximate the direction search\nwith LSH based MaxIP data-structure.\nNote that only MaxIP problem with positive inner product values could be solved by LSH. We found the direction search\nproblem naturally satisfies this condition. We show that if g is convex, given a set S \u2282Rd, we have mins\u2208S\u27e8\u2207g(x),s\u2212x\u27e9\u22640\nfor any x\u2208B(S), where B is the convex hull of S. Thus, maxy\u2208Y \u27e8\u03d50(x),\u03c80(y)\u27e9 is non-negative following Eq. (12).\nJ.4\nData Structures\nIn this section, we present a formal statement that solves (c,\u03c4)-MaxIP problem on unit sphere using LSH for (c,r)-ANN.\nTheorem J.9. Let c \u2208 (0,1) and \u03c4 \u2208 (0,1). Given a set of n-vector set Y \u2282 Sd\u22121 on the unit sphere, there exists a data\nstructure with O(dn1+o(1)) preprocessing time and O(n1+o(1)+dn) space so that for any query x\u2208Sd\u22121, we take O(d\u00b7n\u03c1)\nquery time to retrieve the (c,\u03c4)-MaxIP of x in Y with probability at least 0.97, where \u03c1:= 2(1\u2212\u03c4)2\n(1\u2212c\u03c4)2 \u2212 (1\u2212\u03c4)4\n(1\u2212c\u03c4)4 +o(1)\nProof. We know that \u2225x\u2212y\u22252\n2 =2\u22122\u27e8x,y\u27e9 for all x,y \u2208Sd\u22121. In this way, if we have a LSH data-structure for (c,r)-ANN.\nIt could be used to solve (c,\u03c4)-MaxIP with \u03c4 =1\u22120.5r2 and c= 1\u22120.5c2r2\n1\u22120.5r2 . Next, we write c2 as\nc2 = 1\u2212c(1\u22120.5r2)\n0.5r2\n= 1\u2212c\u03c4\n1\u2212\u03c4 .\nNext, we show that if the LSH is initialized following Theorem J.3, it takes query time O(d\u00b7n\u03c1), space O(n1+o(1)+dn) and\npreprocessing time O(dn1+o(1)) to solve (c,\u03c4)-MaxIP through solving (c,r)-ANN, where\n\u03c1= 2\nc2 \u2212 1\nc4 +o(1)= 2(1\u2212\u03c4)2\n(1\u2212c\u03c4)2 \u2212 (1\u2212\u03c4)4\n(1\u2212c\u03c4)4 +o(1).\nIn practice, c is increasing as we set parameter \u03c4 close to MaxIP(x,Y ). There is also another LSH data structure (Andoni\n& Razenshteyn, 2015) with longer preprocessing time and larger space that could solve the (c,\u03c4)-MaxIP with similar query\n7It is obvious to boost probability from constant to \u03b4 by repeating the data structure log(1/\u03b4) times.\n36\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\ntime complexity. We refer readers to Section 8.2 in (Shrivastava et al., 2021) for more details8. Moreover, Corollary J.9 could\nbe applied to projected MaxIP problem.\nTheorem J.10. Let c \u2208 (0,1) and \u03c4 \u2208 (0,1). Let \u03d5,\u03c8 : Rd \u2192 Rk denote two transforms. Let T\u03d5 denote the time to compute\n\u03d5(x) and T\u03c8 denote the time to compute \u03c8(y). Given a set of n-points Y \u2208 Rd with \u03c8(Y ) \u2282 Sk\u22121 on the sphere, one can\nconstruct a data structure with O(dn1+o(1) +T\u03c8n) preprocessing time and O(n1+o(1) +dn) space so that for any query\nx\u2208Rd with \u03d5(x)\u2208Sk\u22121, we take query time complexity O(d\u00b7n\u03c1+T\u03d5) to solve (c,\u03d5,\u03c8,\u03c4)-MaxIP with respect to (x,Y ) with\nprobability at least 0.9, where \u03c1:= 2(1\u2212\u03c4)2\n(1\u2212c\u03c4)2 \u2212 (1\u2212\u03c4)4\n(1\u2212c\u03c4)4 +o(1).\nProof. The preprocessing phase can be decomposed in two parts.\n\u2022 It takes O(T\u03c8n) time to transform every y\u2208Y into \u03c8(y).\n\u2022 It takes O(O(dn1+o(1)) time and O(dn1+o(1)+dn) to index every \u03c8(y) into LSH using Theorem J.9.\nThe query phase can be decomposed in two parts.\n\u2022 It takes O(T\u03d5) time to transform every x\u2208Rd into \u03d5(x).\n\u2022 It takes O(d\u00b7n\u03c1) time perform query for \u03d5(x) in LSH using Theorem J.9.\nK\nSelf-attention layer as a clustering algorithm\nThe self-attention layer in the Transformer looks like mean-shift clustering. Suppose {(xj,vj)} are a bunch of key and value\npairs and q is the query. Note that q = Wqx, k = Wkx and v = Wvx are computed by three projection matrices Wk, Wq\nand Wv from a common x. Then from self-attention we have:\nv=\nX\nj\npjvj =\nP\njexp(x\u22baW \u22ba\nq Wkxj)Wvxj\nP\njexp(x\u22baW \u22ba\nq Wkxj)\n=Wv\nP\njexp(x\u22baW \u22ba\nq Wkxj)xj\nP\njexp(x\u22baW \u22ba\nq Wkxj)\n(14)\nwhere \u223c(q,kj):=exp(q\u22bakj)=exp(x\u22baW \u22ba\nq Wkxj) and pj =\u223c(q,kj)/P\nj \u223c(q,kj).\nOn the other hand, mean-shift clustering looks like the following:\nm(x)=\nP\njK(xj,x)xj\nP\njK(xj,x)\n(15)\nwhere K(xj,x) is a kernel matrix that measure the similarity between xj and x. According to the mean-shift algorithm,\nin the next iteration, we will simply replace x with m(x).\nSo in some sense, self-attention is just to do some kind of clustering for the input embedding q and k, plus a transformation of the\nembedding to another place. The term \u201cprojection\u201d is due to the fact that there is a projection matrix Wv on x for the next level.\nResidue connection and LayerNorm. Compared to mean-shift, Transformer layer has residue connection. Therefore, for\nsingle-headed attention, what you actually get is v+x, followed by a LayerNorm. For the residue connection, the mean-shift\nanalog already shows the output m(x) contains x+ part. The reason why we need residue connection is that the self-attention\npart might only model the \u201cchange\u201d of x in the mean-shift picture, rather than the full update of x.\nL\nThe role of self-attention\nConsider we have a vocabulary of size m and d dimensional embedding space. In practice, many papers in NLP have reported\nclustering behaviors of word embeddings: such a clustering of word embedding naturally occurs after training.\nAn explanation for the above phenomenon is that, by grouping these word embedding together, we might generalize better, since\nsimilarity in word now can transfer (e.g., A linked to B, B linked to C, then A might link to C as well) and generalization follows.\nLet\u2019s treat it as a fact and focus on how this is achieved and how self-attention plays a role here.\n8Recently, there a line of work that use fast MaxIP data structure to speedup the iterative-type optimization algorithms (Shrivastava et al.,\n2021; Song & Ye, 2023; Qin et al., 2023a; Song et al., 2023a).\n37\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nL.1\nThe capacity of embedding layer\nFirst let us take a look at the following pairwise distance constraints between word embedding (e.g., some words should\nbe close to each other, some should be far away from each other) as the following:\n\u2225xi\u2212xj\u2225=D(i,j)\n(16)\nwhere D(i,j) is large for i and j that should be far apart and D(i,j) is small for i and j that are close to each other. In\nvisualization, this is called Multidimensional Scaling (MDS) (Cox & Cox, 2008).\nNote that in neural network training, the constraint (Eqn. 16) is not directly enforced during training, but the clustering\nnaturally happens. Since we talk about capacity, how we achieve Eqn. 16 doesn\u2019t matter for now.\nIn general we cannot find a fixed low-dimensional embedding (d\u226am) to satisfy these constraints, since we only have md\nparameters (m vectors, each has d entries), but m2 constraint. So two vectors that are supposed to be close may not be close\nenough (but hopefully they remain close to each other).\nL.2\nThe role of self-attention\nFor this, the self-attention mechanism comes to the rescue, trading model-size with additional computation. It fulfills what\n(static) embedding cannot achieve: to further group the embedding vectors together in a multi-layer structure.\nNote that one sentence never covers all d vocabularies. Once the words in the sentence are picked, they are grouped together\nvia self-attention layers to collectively represent a concept that can be useful for the task.\nL.3\nHow the clustering happens through self-attention?\nNow one fundamental questions arise: How the static clustering of embedding happens during end-to-end training? In\npractice, no one explicitly enforces the MDS constraint (Eqn. 16).\nLet\u2019s start with a simple example. we have two unit embedding: x and y with the normalization condition that \u2225x\u22252 = 1\nand \u2225y\u22252 =1, and a simple self-attention layer (without projection) which output z:\nz =(1\u2212p)x+py\n(17)\nWhere the attention map is:\np=\nex\u22bay\nex\u22bax+ex\u22bay =\n1\n1+e1\u2212x\u22bay\n(18)\nNote that here we attend to x so 0<p<1/2 always. The last two is due to normalization condition.\nNow we consider a loss function L = \u2212 1\n2\u2225z\u22252\n2. The intuition behind is that \u201cfor some reason, we found that z is a good\nrepresentation for our task, and want to make sure its length is as long as possible\u201d.\nUnder this context, what would be the gradient rule for x and y? Will they cluster together?\nThe answer is yes! We could compute\n\u2202z\n\u2202x\n=\n(1\u2212p)I+ \u2202p\n\u2202x(y\u2212x)\u22ba\n(19)\n\u2202z\n\u2202y\n=\npI+ \u2202p\n\u2202y (y\u2212x)\u22ba\n(20)\nLet t:=1\u2212x\u22bay and define the following function with respect to t:\nf(t):=(x\u2212y)\u22baz =(1\u22122p)(1\u2212x\u22bay)>0\n(21)\nTherefore, we can compute the gradient for x and gradient for y:\n\u2212gx\n:=\n\u2212\u2202L\n\u2202x =\u2212 \u2202z\n\u2202x\n\u2202L\n\u2202z =(1\u2212p)2x+p(1\u2212p)(1\u2212f(t))y\n(22)\n\u2212gy\n:=\n\u2212\u2202L\n\u2202y =\u2212\u2202z\n\u2202y\n\u2202L\n\u2202z =p2y+p(1\u2212p)(1\u2212f(t))x\n(23)\nNote that since x and y are kept to be normalized, the term (1\u2212p)2x in \u2202L/\u2202x is gone (and similarly p2y for gy). So how\nx and y move depends on the sign of 1\u2212f(t).\nWith some computation, we could see 0 < f(t) < 1 when t < 1.5424. In summary, if x\u22bay > \u22120.4576, then the (negative)\ngradient of x pushes it towards y and pushes x towards y, and the clustering of static embedding happens during training.\nNote that since both x and y are normalized, \u22121\u2264x\u22bay\u22641, so this is a quite loose condition and can be easily satisfied.\n38\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nL.4\nMultiple embeddings\nPeople might wonder what happen to multiple unit embeddings x,y1,y2,...,yK? In this case, we can similarly define\nself-attention probability pi (note that here we consider the case that every embedding attends to x):\npi :=\nex\u22bayi\nex\u22bax+P\njex\u22bayj =\nex\u22bayi\n1+P\njex\u22bayj\n(24)\nDefine pS :=PK\ni=1pi =1\u2212\n1\n1+P\njex\u22bayj <1 and we have:\nz =(1\u2212pS)x+\nX\ni\npiyi\n(25)\nLet \u02dcpi := pi/pS be the (normalized) probability on yi and \u00afy := 1\npS\nP\nipiyi = P\ni \u02dcpiyi be the weighted mean of {yi} other\nthan x, then we have:\nz =(1\u2212pS)x+pS \u00afy\n(26)\nNow we can still compute the partial derivative:\n\u2202pj\n\u2202x\n=\npj[\u2212pS \u00afy+yj]\n(27)\n\u2202pj\n\u2202yi\n=\npi[\u2212pj+I(i=j)]x\n(28)\nwhich gives\n\u2202z\n\u2202x\n=\n(1\u2212pS)I+\nX\nj\n\u2202pj\n\u2202x (yj\u2212x)\u22ba\n(29)\n\u2202z\n\u2202yi\n=\npiI+\nX\nj\n\u2202pj\n\u2202yi\n(yj\u2212x)\u22ba\n(30)\nAfter some manipulation, we have:\n\u2202z\n\u2202x =(1\u2212pS)[I+pS \u00afy(\u00afy\u2212x)\u22ba]+pSQ\n(31)\nwhere Q:=P\nj \u02dcpj(yj\u2212 \u00afy)(yj\u2212 \u00afy)\u22ba is the weighted covariance matrix of data points {yj}.\nSimilar to the two unit case, we want to check \u2212gx to see how the embedding x changes over time.\n\u2212gx\n=\n\u2212\u2202L\n\u2202x =\u2212 \u2202z\n\u2202x\n\u2202L\n\u2202z\n(32)\n=\n(1\u2212pS)2x+pS\n\u0002\n(1\u22122pS)x\u22ba \u00afy\u2212(1\u2212pS)+pS\u2225\u00afy\u22252\u0003\u00afy+pSQz\nIf things are already quite clustered, then \u2225\u00afy\u2225 \u2248 1 (usually \u2225\u00afy\u22252 < 1 since sphere is a convex set), Qz \u2248 0 (since Q spans\non the tangent space of z at the sphere and z is perpendicular to it), and we have:\n\u2212gx \u2248(1\u2212pS)2x+pS(1\u22122pS)(x\u22ba \u00afy\u22121)\u00afy\n(33)\nIt is clear that x\u22ba \u00afy<1. When pS >1/2, which is high likely for large K, then \u2212gx has positive component of \u00afy and x will\nmove towards \u00afy.\nOn the other hand, we could also check\n\u2202z\n\u2202yi\n=pi[I+(1\u2212pS)x(\u00afy\u2212x)\u22ba]+pix(yi\u2212 \u00afy)\u22ba\n(34)\nwhich gives an expression of \u2212gy:\n\u00b7\n(35)\nWith the same argument, it moves towards \u00afy (so all yi will cluster together) and towards x.\nWhen there is a Wk and Wq before the embedding, following the same logic, only the column subspace of Wk (or Wq) will\nbe clustered together. On the other hand, the value part will be different in order to enable encoding of more complicated\nconcepts based on co-occurrence of multiple tokens.\nM\nLink self-attention with generative models.\nConsider the following self-attention structure. Consider an embedding matrix X \u2208Rn\u00d7d and for embedding xi and xj, let\nyij =\u03d5(xi;xj):=(1\u2212\u03b2ij)xi+\u03b2ijxj,\n\u03b2ij :=\nex\u22ba\ni xj\nex\u22ba\ni xi +ex\u22ba\ni xj\n(36)\n39\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nHere \u03d5(xi;xj):=xi+\u03b2ij(xj \u2212xi) is the self-attention operation. More properties of this operator \u03d5 need to be explored.\nThen we want to maximize the following objective:\nmax\nX,\u2225xi\u22252=1\nX\nijk\nP(k|i,j)y\u22ba\nijxk\n(37)\nor more formally, using a softmax to avoid trivial solution xi \u2261x, we have:\nmax\nX,\u2225xi\u22252=1J :=\nmax\nX,\u2225xi\u22252=1\nX\nijk\nP(k|i,j)log\u03b4ijk,\n\u03b4ijk :=\ney\u22ba\nijxk\nP\nkey\u22ba\nijxk\n(38)\nwhich is:\nmax\nX,\u2225xi\u22252=1\nX\nijk\nP(k|i,j)\n\"\ny\u22ba\nijxk\u2212log\nX\nk\ney\u22ba\nijxk\n#\n(39)\nWe can compute its gradient update. Here we assume the index k never appears in index i and j (encoding and decoding\nmatrices are decoupled), then by gradient rule, we have:\n\u02d9xk = \u2202L\n\u2202xk\n=P \u22a5\nxk\nX\nij\nP(k|i,j)(1\u2212\u03b4ijk)yij\n(40)\nwhere P \u22a5\nxk is the projection matrix that projects a vector to the orthogonal complement space of xk. The projection is due\nto the constraint \u2225xk\u22252 =1. If the training converges ( \u02d9xk =0), then we know that\nX\nij\nP(k|i,j)(1\u2212\u03b4ijk)yij =\u03b3xk\n(41)\nfor some \u03b3 >0 (note that \u03b3 <0 will be an unstable stationary point).\nDepending on different structure of the generative model specified by P(k|i,j), we might end up learning different embedding\nmatrix X.\nThe first thing we want to check is independency. Assume that for some specific token k and i, we have P(k|i,j) = P(k|i)\nfor any j, which means that the frequency of token k has nothing to do with the second entry j. Furthermore, token k is not\nconnected with other token i\u2032 \u0338=i, i.e, P(k|i\u2032,j)\u22610. If we just let \u03b4ijk =\u03b4>0, then we have:\nP(k|i)\nX\nj\nyij =\u03b3\u2032xk\n(42)\nwhich yields\nP(k|i)nxi+\nX\nj\n\u03b2ij(xj\u2212xi)=\u03b3\u2032xk\n(43)\nAnd we could possibly show that P\nj\u03b2ij(xj \u2212xi) \u2248 0 since \u03b2ij = 1/(1+e1\u2212x\u22ba\ni xj) applies equal weights for embeddings\naround xi and they cancel out. Therefore, xk is aligned with xi.\nAnother thing we might want to check is identification of two tokens. Assume that there exists two tokens j1 and j2 and\nspecific k and i, so that P(k|i,j1)=P(k|i,j2). For other k,i,j combination P(k|i,j)\u22610, then we have:\nP(k|i,j1)yij1 =\u03b31xk\n(44)\n(not sure how to continue).\nIf we have Wq, Wk and Wv, then the formulation doesn\u2019t change that much. The only difference here is that now\n\u03b2ij :=\nex\u22ba\ni Wpqxj\nex\u22ba\ni Wpqxi +ex\u22ba\ni Wpqxj\n(45)\nand y\u22ba\nijxk now becomes y\u22ba\nijWvxk.\n40\n"
  }
]