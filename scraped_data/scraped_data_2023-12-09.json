[
  {
    "title": "PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding",
    "link": "https://arxiv.org/pdf/2312.04461.pdf",
    "upvote": "47",
    "text": "Customizing Realistic Human Photos via Stacked ID Embedding\nZhen Li1,2 *\nMingdeng Cao2,3 *\nXintao Wang2 \u2020\nZhongang Qi2\nMing-Ming Cheng1 \u2020\nYing Shan2\n1VCIP, CS, Nankai University\n2ARC Lab, Tencent PCG\n3The University of Tokyo\nhttps://photo-maker.github.io/\nA man wearing \nheadphones with red hair\nA woman wearing \nsunglasses and necklace\nA man coding in front of a \ncomputer\nA man in a helmet and \nvest riding a motorcycle\nA boy wearing a \ndoctoral cap\nA man wearing a \nChristmas hat\n(a) Attributes change\nUser inputs\n(b) Artwork / old-photo to reality\nA woman in the snow\nA woman holding a bottle \nof red wine\nA man wearing \nheadphones\nA man wearing a spacesuit\n(c) Identity mixing\nA photo of a woman\nA woman with red hair\nUser inputs\nUser inputs\nUser inputs\nUser inputs\nUser inputs\nFigure 1. Given a few images of input ID(s), the proposed PhotoMaker can generate diverse personalized ID images based on the text\nprompt in a single forward pass. Our method can well preserve the ID information from the input image pool while generating realistic\nhuman photos. PhotoMaker also empowers many interesting applications such as (a) changing attributes, (b) bringing persons from\nartworks or old photos into reality, or (c) performing identity mixing. (Zoom-in for the best view)\nAbstract\nRecent advances in text-to-image generation have made\nremarkable progress in synthesizing realistic human photos\nconditioned on given text prompts. However, existing per-\nsonalized generation methods cannot simultaneously sat-\nisfy the requirements of high efficiency, promising identity\n\u2217 Interns in ARC Lab, Tencent PCG\n\u2020 Corresponding authors\n(ID) fidelity, and flexible text controllability. In this work,\nwe introduce PhotoMaker, an efficient personalized text-\nto-image generation method, which mainly encodes an ar-\nbitrary number of input ID images into a stack ID embed-\nding for preserving ID information. Such an embedding,\nserving as a unified ID representation, can not only encap-\nsulate the characteristics of the same input ID comprehen-\nsively, but also accommodate the characteristics of differ-\nent IDs for subsequent integration. This paves the way for\n1\narXiv:2312.04461v1  [cs.CV]  7 Dec 2023\nmore intriguing and practically valuable applications. Be-\nsides, to drive the training of our PhotoMaker, we propose\nan ID-oriented data construction pipeline to assemble the\ntraining data. Under the nourishment of the dataset con-\nstructed through the proposed pipeline, our PhotoMaker\ndemonstrates better ID preservation ability than test-time\nfine-tuning based methods, yet provides significant speed\nimprovements, high-quality generation results, strong gen-\neralization capabilities, and a wide range of applications.\n1. Introduction\nCustomized image generation related to humans [28, 35,\n51] has received considerable attention, giving rise to nu-\nmerous applications, such as personalized portrait pho-\ntos [1], image animation [67], and virtual try-on [59]. Early\nmethods [39, 41], limited by the capabilities of generative\nmodels (i.e., GANs [19, 29]), could only customize the gen-\neration of the facial area, resulting in low diversity, scene\nrichness, and controllability. Thanks to larger-scale text-\nimage pair training datasets [55], larger generation mod-\nels [44, 52], and text/visual encoders [45, 46] that can pro-\nvide stronger semantic embeddings, diffusion-based text-\nto-image generation models have been continuously evolv-\ning recently. This evolution enables them to generate in-\ncreasingly realistic facial details and rich scenes. The con-\ntrollability has also greatly improved due to the existence of\ntext prompts and structural guidance [40, 65]\nMeanwhile, under the nurturing of powerful diffusion\ntext-to-image models, many diffusion-based customized\ngeneration algorithms [17, 50] have emerged to meet users\u2019\ndemand for high-quality customized results.\nThe most\nwidely used in both commercial and community applica-\ntions are DreamBooth-based methods [2, 50].\nSuch ap-\nplications require dozens of images of the same identity\n(ID) to fine-tune the model parameters. Although the re-\nsults generated have high ID fidelity, there are two obvious\ndrawbacks: one is that customized data used for fine-tuning\neach time requires manual collection and thus is very time-\nconsuming and laborious; the other is that customizing each\nID requires 10-30 minutes, consuming a large amount of\ncomputing resources, especially when the generation model\nbecomes larger. Therefore, to simplify and accelerate the\ncustomized generation process, recent works, driven by ex-\nisting human-centric datasets [29, 36], have trained visual\nencoders [11, 62] or hypernetworks [5, 51] to represent the\ninput ID images as embeddings or LoRA [25] weights of\nthe model. After training, users only need to provide an im-\nage of the ID to be customized, and personalized generation\ncan be achieved through a few dozen steps of fine-tuning or\neven without any tuning process. However, the results cus-\ntomized by these methods cannot simultaneously possess\nID fidelity and generation diversity like DreamBooth (see\nFig. 3). This is because that: 1) during the training pro-\ncess, both the target image and the input ID image sample\nfrom the same image. The trained model easily remembers\ncharacteristics unrelated to the ID in the image, such as ex-\npressions and viewpoints, which leads to poor editability,\nand 2) relying solely on a single ID image to be customized\nmakes it difficult for the model to discern the characteris-\ntics of the ID to be generated from its internal knowledge,\nresulting in unsatisfactory ID fidelity.\nBased on the above two points, and inspired by the suc-\ncess of DreamBooth, in this paper, we aim to: 1) ensure that\nthe ID image condition and the target image exhibit varia-\ntions in viewpoints, facial expressions, and accessories, so\nthat the model does not memorize information that is irrele-\nvant to the ID; 2) provide the model with multiple different\nimages of the same ID during the training process to more\ncomprehensively and accurately represent the characteris-\ntics of the customized ID.\nTherefore, we propose a simple yet effective feed-\nforward customized human generation framework that can\nreceive multiple input ID images, termed as PhotoMaker.\nTo better represent the ID information of each input im-\nage, we stack the encodings of multiple input ID images\nat the semantic level, constructing a stacked ID embedding.\nThis embedding can be regarded as a unified representation\nof the ID to be generated, and each of its subparts corre-\nsponds to an input ID image. To better integrate this ID\nrepresentation and the text embedding into the network, we\nreplace the class word (e.g., man and woman) of the text\nembedding with the stacked ID embedding. The result em-\nbedding simultaneously represents the ID to be customized\nand the contextual information to be generated. Through\nthis design, without adding extra modules in the network,\nthe cross-attention layer of the generation model itself can\nadaptively integrate the ID information contained in the\nstacked ID embedding.\nAt the same time, the stacked ID embedding allows us to\naccept any number of ID images as input during inference\nwhile maintaining the efficiency of the generation like other\ntuning-free methods [56, 62]. Specifically, our method re-\nquires about 10 seconds to generate a customized human\nphoto when receiving four ID images, which is about 130\u00d7\nfaster than DreamBooth1. Moreover, since our stacked ID\nembedding can represent the customized ID more compre-\nhensively and accurately, our method can provide better ID\nfidelity and generation diversity compared to state-of-the-\nart tuning-free methods. Compared to previous methods,\nour framework has also greatly improved in terms of con-\ntrollability. It can not only perform common recontextual-\nization but also change the attributes of the input human\nimage (e.g., accessories and expressions), generate a hu-\nman photo with completely different viewpoints from the\n1Test on one NVIDIA Tesla V100\n2\ninput ID, and even modify the input ID\u2019s gender and age\n(see Fig. 1).\nIt is worth noticing that our PhotoMaker also unleashes\na lot of possibilities for users to generate customized hu-\nman photos. Specifically, although the images that build\nthe stacked ID embedding come from the same ID during\ntraining, we can use different ID images to form the stacked\nID embedding during inference to merge and create a new\ncustomized ID. The merged new ID can retain the charac-\nteristics of different input IDs. For example, we can gener-\nate Scarlett Johansson that looks like Elun Musk or a cus-\ntomized ID that mixes a person with a well-known IP char-\nacter (see Fig. 1(c)). At the same time, the merging ratio\ncan be simply adjusted by prompt weighting [3, 21] or by\nchanging the proportion of different ID images in the input\nimage pool, demonstrating the flexibility of our framework.\nOur PhotoMaker necessitates the simultaneous input of\nmultiple images with the same ID during the training pro-\ncess, thereby requiring the support of an ID-oriented hu-\nman dataset. However, existing datasets either do not clas-\nsify by IDs [29, 35, 55, 68] or only focus on faces with-\nout including other contextual information [36, 41, 60]. We\ntherefore design an automated pipeline to construct an ID-\nrelated dataset to facilitate the training of our PhotoMaker.\nThrough this pipeline, we can build a dataset that includes\na large number of IDs, each with multiple images featuring\ndiverse viewpoints, attributes, and scenarios. Meanwhile,\nin this pipeline, we can automatically generate a caption for\neach image, marking out the corresponding class word [50],\nto better adapt to the training needs of our framework.\n2. Related work\nText-to-Image Diffusion Models. Diffusion models [23,\n58] have made remarkable progress in text-conditioned\nimage generation [30, 47, 49, 52], attracting widespread\nattention in recent years.\nThe remarkable performance\nof these models can be attributable to high-quality large-\nscale text-image datasets [9, 54, 55], the continuous up-\ngrades of foundational models [10, 43], conditioning en-\ncoders [26, 45, 46], and the improvement of controllabil-\nity [34, 40, 63, 65]. Due to these advancements, Podell et\nal. [44] developed the currently most powerful open-source\ngenerative model, SDXL. Given its impressive capabilities\nin generating human portraits, we build our PhotoMaker\nbased on this model. However, our method can also be ex-\ntended to other text-to-image synthesis models.\nPersonalization in Diffusion Models. Owing to the pow-\nerful generative capabilities of the diffusion models, more\nresearchers try to explore personalized generation based on\nthem. Currently, mainstream personalized synthesis meth-\nods can be mainly divided into two categories.\nOne re-\nlies on additional optimization during the test phase, such\nas DreamBooth [50] and Textual Inversion [17].\nGiven\nthat both pioneer works require substantial time for fine-\ntuning, some studies have attempted to expedite the pro-\ncess of personalized customization by reducing the num-\nber of parameters needed for tuning [2, 20, 32, 64] or by\npre-training with large datasets [18, 51]. Despite these ad-\nvances, they still require extensive fine-tuning of the pre-\ntrained model for each new concept, making the process\ntime-consuming and restricting its applications. Recently,\nsome studies [12, 13, 27, 37, 38, 56, 61] attempt to per-\nform personalized generation using a single image with a\nsingle forward pass, significantly accelerating the person-\nalization process. These methods either utilize personaliza-\ntion datasets [12, 57] for training or encode the images to be\ncustomized in the semantic space [11, 27, 38, 56, 61, 62].\nOur method focuses on the generation of human portraits\nbased on both of the aforementioned technical approaches.\nSpecifically, it not only relies on the construction of an ID-\noriented personalization dataset, but also on obtaining the\nembedding that represents the person\u2019s ID in the seman-\ntic space. Unlike previous embedding-based methods, our\nPhotoMaker extracts a stacked ID embedding from multi-\nple ID images. While providing better ID representation,\nthe proposed method can maintain the same high efficiency\nas previous embedding-based methods.\n3. Method\n3.1. Overview\nGiven a few ID images to be customized, the goal of our\nPhotoMaker is to generate a new photo-realistic human im-\nage that retains the characteristics of the input IDs and\nchanges the content or the attributes of the generated ID\nunder the control of the text prompt. Although we input\nmultiple ID images for customization like DreamBooth, we\nstill enjoy the same efficiency as other tuning-free methods,\naccomplishing customization with a single forward pass,\nwhile maintaining promising ID fidelity and text edibility.\nIn addition, we can also mix multiple input IDs, and the\ngenerated image can well retain the characteristics of differ-\nent IDs, which releases possibilities for more applications.\nThe above capabilities are mainly brought by our proposed\nsimple yet effective stacked ID embedding, which can pro-\nvide a unified representation of the input IDs. Furthermore,\nto facilitate training our PhotoMaker, we design a data con-\nstruction pipeline to build a human-centric dataset classified\nby IDs. Fig. 2(a) shows the overview of the proposed Pho-\ntoMaker. Fig. 2(b) shows our data construction pipeline.\n3.2. Stacked ID Embedding\nEncoders. Following recent works [27, 56, 61], we use the\nCLIP [45] image encoder Eimg to extract image embeddings\nfor its alignment with the original text representation space\n3\nText Encoder(s)\n\u201cA man in a suit and tie\nwith a badge on his lapel\u201d\nImage Encoder\nDiffusion Model\nImage \nEmbeddings\nText \nEmbedding\nStacked ID Embedding\nUpdated Text Embedding\nMLPs\nTraining ID Images\n\u201cA man wearing a blue cap\u201d\nInference ID Images\nTraining\nInference\nShared\n1. Image Downloading\n2. Face Detection & Filtering\n3. ID Verification\n4. Cropping & Segmentation\n5. Captioning & Marking\n\u201ca woman with long hair and a white shirt \nis looking down\u201d\n\u201ca man in a black suit with a sword in her \nhand\u201d\n\u2026\nGrouped by IDs\nGrouped by IDs\n(a)\n(b)\nFigure 2. Overviews of the proposed (a) PhotoMaker and (b) ID-oriented data construction pipeline. For the proposed PhotoMaker,\nwe first obtain the text embedding and image embeddings from text encoder(s) and image encoder, respectively. Then, we extract the\nfused embedding by merging the corresponding class embedding (e.g., man and woman) and each image embedding. Next, we concatenate\nall fused embeddings along the length dimension to form the stacked ID embedding. Finally, we feed the stacked ID embedding to all\ncross-attention layers for adaptively merging the ID content in the diffusion model. Note that although we use images of the same ID with\nthe masked background during training, we can directly input images of different IDs without background distortion to create a new ID\nduring inference.\nin diffusion models. Before feeding each input image into\nthe image encoder, we filled the image areas other than the\nbody part of a specific ID with random noises to eliminate\nthe influence of other IDs and the background. Since the\ndata used to train the original CLIP image encoder mostly\nconsists of natural images, to better enable the model to ex-\ntract ID-related embeddings from the masked images, we\nfinetune part of the transformer layers in the image encoder\nwhen training our PhotoMaker. We also introduce addi-\ntional learnable projection layers to inject the embedding\nobtained from the image encoder into the same dimension\nas the text embedding. Let {Xi | i = 1 . . . N} denote N\ninput ID images acquired from a user, we thus obtain the\nextracted embeddings {ei \u2208 RD | i = 1 . . . N}, where D\ndenotes the projected dimension. Each embedding corre-\nsponds to the ID information of an input image. For a given\ntext prompt T, we extract text embeddings t \u2208 RL\u00d7D using\nthe pre-trained CLIP text encoder Etext, where L denotes\nthe length of the embedding.\nStacking. Recent works [17, 50, 62] have shown that, in the\ntext-to-image models, personalized character ID informa-\ntion can be represented by some unique tokens. Our method\nalso has a similar design to better represent the ID informa-\ntion of the input human images. Specifically, we mark the\ncorresponding class word (e.g., man and woman) in the in-\nput caption (see Sec. 3.3). We then extract the feature vector\nat the corresponding position of the class word in the text\nembedding. This feature vector will be fused with each im-\nage embedding ei. We use two MLP layers to perform such\na fusion operation. The fused embeddings can be denoted\nas {\u02c6ei \u2208 RD | i = 1 . . . N}. By combining the feature\nvector of the class word, this embedding can represent the\ncurrent input ID image more comprehensively. In addition,\nduring the inference stage, this fusion operation also pro-\nvides stronger semantic controllability for the customized\ngeneration process. For example, we can customize the age\nand gender of the human ID by simply replacing the class\nword (see Sec. 4.2).\nAfter obtaining the fused embeddings, we concatenate\nthem along the length dimension to form the stacked id em-\nbedding:\ns\u2217 = Concat([\u02c6e1, . . . , \u02c6eN])\ns\u2217 \u2208 RN\u00d7D.\n(1)\nThis stacked ID embedding can serve as a unified repre-\nsentation of multiple ID images while it retains the origi-\nnal representation of each input ID image. It can accept\nany number of ID image encoded embeddings, therefore,\nits length N is variable. Compared to DreamBooth-based\nmethods [2, 50], which inputs multiple images to finetune\nthe model for personalized customization, our method es-\nsentially sends multiple embeddings to the model simul-\ntaneously.\nAfter packaging the multiple images of the\nsame ID into a batch as the input of the image encoder,\na stacked ID embedding can be obtained through a single\nforward pass, significantly enhancing efficiency compared\nto tuning-based methods. Meanwhile, compared to other\n4\nembedding-based methods [61, 62], this unified representa-\ntion can maintain both promising ID fidelity and text con-\ntrollability, as it contains more comprehensive ID informa-\ntion. In addition, it is worth noting that, although we only\nused multiple images of the same ID to form this stacked\nID embedding during training, we can use images that come\nfrom different IDs to construct it during the inference stage.\nSuch flexibility opens up possibilities for many interesting\napplications. For example, we can mix two persons that ex-\nist in reality or mix a person and a well-known character IP\n(see Sec. 4.2).\nMerging. We use the inherent cross-attention mechanism\nin diffusion models to adaptively merge the ID informa-\ntion contained in stacked ID embedding. We first replace\nthe feature vector at the position corresponding to the class\nword in the original text embedding t with the stacked\nid embedding s\u2217, resulting in an update text embedding\nt\u2217 \u2208 R(L+N\u22121)\u00d7D. Then, the cross-attention operation can\nbe formulated as:\n(\nQ = WQ \u00b7 \u03d5(zt); K = WK \u00b7 t\u2217; V = WV \u00b7 t\u2217\nAttention(Q, K, V) = softmax( QKT\n\u221a\nd ) \u00b7 V,\n(2)\nwhere \u03d5(\u00b7) is an embedding that can be encoded from the\ninput latent by the UNet denoiser. WQ, WK, and WV\nare projection matrices.\nBesides, we can adjust the de-\ngree of participation of one input ID image in generating\nthe new customized ID through prompt weighting [3, 21],\ndemonstrating the flexibility of our PhotoMaker.\nRecent\nworks [2, 32] found that good ID customization perfor-\nmance can be achieved by simply tuning the weights of the\nattention layers. To make the original diffusion models bet-\nter perceive the ID information contained in stacked ID em-\nbedding, we additionally train the LoRA [2, 25] residuals of\nthe matrices in the attention layers.\n3.3. ID-Oriented Human Data Construction\nSince our PhotoMaker needs to sample multiple images of\nthe same ID for constructing the stacked ID embedding dur-\ning the training process, we need to use a dataset classi-\nfied by IDs to drive the training process of our PhotoMaker.\nHowever, existing human datasets either do not annotate ID\ninformation [29, 35, 55, 68], or the richness of the scenes\nthey contain is very limited [36, 41, 60] (i.e., they only fo-\ncus on the face area). Thus, in this section, we will intro-\nduce a pipeline for constructing a human-centric text-image\ndataset, which is classified by different IDs. Fig. 2(b) illus-\ntrates the proposed pipeline. Through this pipeline, we can\ncollect an ID-oriented dataset, which contains a large num-\nber of IDs, and each ID has multiple images that include dif-\nferent expressions, attributes, scenes, etc. This dataset not\nonly facilitates the training process of our PhotoMaker but\nalso may inspire potential future ID-driven research. The\nstatistics of the dataset are shown in the appendix.\nImage downloading. We first list a roster of celebrities,\nwhich can be obtained from VoxCeleb1 and VGGFace2 [7].\nWe search for names in the search engine according to the\nlist and crawled the data. About 100 images were down-\nloaded for each name. To generate higher quality portrait\nimages [44], we filtered out images with the shortest side of\nthe resolution less than 512 during the download process.\nFace detection and filtering. We first use RetinaNet [16]\nto detect face bounding boxes and filter out the detections\nwith small sizes (less than 256 \u00d7 256). If an image does not\ncontain any bounding boxes that meet the requirements, the\nimage will be filtered out. We then perform ID verification\nfor the remaining images.\nID verification. Since an image may contain multiple faces,\nwe need first to identify which face belongs to the current\nidentity group. Specifically, we send all the face regions in\nthe detection boxes of the current identity group into Arc-\nFace [15] to extract identity embeddings and calculate the\nL2 similarity of each pair of faces. We sum the similarity\ncalculated by each identity embedding with all other em-\nbeddings to get the score for each bounding box. We select\nthe bounding box with the highest sum score for each im-\nage with multiple faces. After bounding box selection, we\nrecompute the sum score for each remaining box. We calcu-\nlate the standard deviation \u03b4 of the sum score by ID group.\nWe empirically use 8\u03b4 as a threshold to filter out images\nwith inconsistent IDs.\nCropping and segmentation. We first crop the image with\na larger square box based on the detected face area while\nensuring that the facial region can occupy more than 10%\nof the image after cropping. Since we need to remove the\nirrelevant background and IDs from the input ID image be-\nfore sending it into the image encoder, we need to gener-\nate the mask for the specified ID. Specifically, we employ\nthe Mask2Former [14] to perform panoptic segmentation\nfor the \u2018person\u2019 class. We leave the mask with the highest\noverlap with the facial bounding box corresponding to the\nID. Besides, we choose to discard images where the mask\nis not detected, as well as images where no overlap is found\nbetween the bounding box and the mask area.\nCaptioning and marking We use BLIP2 [33] to generate\na caption for each cropped image. Since we need to mark\nthe class word (e.g., man, woman, and boy) to facilitate the\nfusion of text and image embeddings, we regenerate cap-\ntions that do not contain any class word using the random\nmode of BLIP2 until a class word appears. After obtain-\ning the caption, we singularize the class word in the caption\nto focus on a single ID. Next, we need to mark the posi-\ntion of the class word that corresponds to the current ID.\nCaptions that contain only one class word can be directly\n5\nannotated. For captions that contain multiple class words,\nwe count the class words contained in the captions for each\nidentity group. The class word with the most occurrences\nwill be the class word for the current identity group. We\nthen use the class word of each identity group to match\nand mark each caption in that identity group. For a cap-\ntion that does not include the class word that matches that\nof the corresponding identity group, we employ a depen-\ndence parsing model [24] to segment the caption according\nto different class words. We calculate the CLIP score [45]\nbetween the sub-caption after segmentation and the specific\nID region in the image. Besides, we calculate the label sim-\nilarity between the class word of the current segment and\nthe class word of the current identity group through Sen-\ntenceFormer [48]. We choose to mark the class word cor-\nresponding to the maximum product of the CLIP score and\nthe label similarity.\n4. Experiments\n4.1. Setup\nImplementation details.\nTo generate more photo-\nrealistic human portraits, we employ SDXL model [44]\nstable-diffusion-xl-base-1.0 as our text-to-\nimage synthesis model. Correspondingly, the resolution of\ntraining data is resized to 1024 \u00d7 1024. We employ CLIP\nViT-L/14 [45] and an additional projection layer to obtain\nthe initial image embeddings ei. For text embeddings, we\nkeep the original two text encoders in SDXL for extraction.\nThe overall framework is optimized with Adam [31] on 8\nNVIDIA A100 GPUs for two weeks with a batch size of 48.\nWe set the learning rate as 1e \u2212 4 for LoRA weights, and\n1e\u22125 for other trainable modules. During training, we ran-\ndomly sample 1-4 images with the same ID as the current\ntarget ID image to form a stacked ID embedding. Besides,\nto improve the generation performance by using classifier-\nfree guidance, we have a 10% chance of using null-text em-\nbedding to replace the original updated text embedding t\u2217.\nWe also use masked diffusion loss [6] with a probability of\n50% to encourage the model to generate more faithful ID-\nrelated areas. During the inference stage, we use delayed\nsubject conditioning [62] to solve the conflicts between text\nand ID conditions. We use 50 steps of DDIM sampler [58].\nThe scale of classifier-free guidance is set to 5.\nEvaluation metrics. Following DreamBooth [50], we use\nDINO [8] and CLIP-I [17] metrics to measure the ID fidelity\nand use CLIP-T [45] metric to measure the prompt fidelity.\nFor a more comprehensive evaluation, we also compute the\nface similarity by detecting and cropping the facial regions\nbetween the generated image and the real image with the\nsame ID. We use RetinaFace [16] as the detection model.\nFace embedding is extracted by FaceNet [53]. To evalu-\nate the quality of the generation, we employ the FID met-\nric [22, 42]. Importantly, as most embedding-based meth-\nods tend to incorporate facial pose and expression into the\nrepresentation, the generated images often lack variation in\nthe facial region. Thus, we propose a metric, named Face\nDiversity, to measure the diversity of the generated facial re-\ngions. Specifically, we first detect and crop the face region\nin each generated image. Next, we calculate the LPIPS [66]\nscores between each pair of facial areas for all generated im-\nages and take the average. The larger this value, the higher\nthe diversity of the generated facial area.\nEvaluation dataset.\nOur evaluation dataset includes 25\nIDs, which consist of 9 IDs from Mystyle [41] and an addi-\ntional 16 IDs that we collected by ourselves. Note that these\nIDs do not appear in the training set, serving to evaluate the\ngeneralization ability of the model. To conduct a more com-\nprehensive evaluation, we also prepare 40 prompts, which\ncover a variety of expressions, attributes, decorations, ac-\ntions, and backgrounds. For each prompt of each ID, we\ngenerate 4 images for evaluation. More details are listed in\nthe appendix.\n4.2. Applications\nIn this section, we will elaborate on the applications that our\nPhotoMaker can empower. For each application, we choose\nthe comparison methods which may be most suitable for the\ncorresponding setting. The comparison method will be cho-\nsen from DreamBooth [50], Textual Inversion [17], Fast-\nComposer [62], and IPAdapter [63]. We prioritize using the\nofficial model provided by each method. For DreamBooth\nand IPAdapter, we use their SDXL versions for a fair com-\nparison. For all applications, we have chosen four input\nID images to form the stacked ID embedding in our Pho-\ntoMaker. We also fairly use four images to train the methods\nthat need test-time optimization. We provide more samples\nin the appendix for each application.\nRecontextualization We first show results with simple con-\ntext changes such as modified hair color and clothing, or\ngenerate backgrounds based on basic prompt control. Since\nall methods can adapt to this application, we conduct quan-\ntitative and qualitative comparisons of the generated re-\nsults (see Tab. 1 and Fig. 3). The results show that our\nmethod can well satisfy the ability to generate high-quality\nimages, while ensuring high ID fidelity (with the largest\nCLIP-T and DINO scores, and the second best Face Sim-\nilarity). Compared to most methods, our method generates\nimages of higher quality, and the generated facial regions\nexhibit greater diversity. At the same time, our method can\nmaintain a high efficiency consistent with embedding-based\nmethods. For a more comprehensive comparison, we show\nthe user study results in Sec. B in the appendix.\nBringing person in artwork/old photo into reality. By\ntaking artistic paintings, sculptures, or old photos of a per-\nson as input, our PhotoMaker can bring a person from the\n6\nDreamBooth\nTextual Inversion\nFastComposer\nIPAdapter\nPhotoMaker (Ours)\nReferences\nA woman with red \nhair\nA woman wearing \na doctoral cap\nA man wearing a \nspacesuit\nA man wearing a \nChristmas hat\nA woman sitting \nat the beach, \nwith purple \nsunset\nFigure 3. Qualitative comparison on universal recontextualization samples. We compare our method with DreamBooth [50], Textual\nInversion [17], FastComposer [62], and IPAdapter [63] for five different identities and corresponding prompts. We observe that our method\ngenerally achieves high-quality generation, promising editability, and strong identity fidelity. (Zoom-in for the best view)\nCLIP-T\u2191 (%)\nCLIP-I\u2191 (%)\nDINO\u2191 (%)\nFace Sim.\u2191 (%)\nFace Div.\u2191 (%)\nFID\u2193\nSpeed\u2193 (s)\nDreamBooth [50]\n29.8\n62.8\n39.8\n49.8\n49.1\n374.5\n1284\nTextual Inversion [17]\n24.0\n70.9\n39.3\n54.3\n59.3\n363.5\n2400\nFastComposer [62]\n28.7\n66.8\n40.2\n61.0\n45.4\n375.1\n8\nIPAdapter [63]\n25.1\n71.2\n46.2\n67.1\n52.4\n375.2\n12\nPhotoMaker (Ours)\n26.1\n73.6\n51.5\n61.8\n57.7\n370.3\n10\nTable 1. Quantitative comparison on the universal recontextualization setting. The metrics used for benchmarking cover the ability to\npreserve ID information (i.e., CLIP-I, DINO, and Face Similarity), text consistency (i.e., CLIP-T), diversity of generated faces (i.e., Face\nDiversity), and generation quality (i.e., FID). Besides, we define personalized speed as the time it takes to obtain the final personalized\nimage after feeding the ID condition(s). We measure personalized time on a single NVIDIA Tesla V100 GPU. The best result is shown in\nbold, and the second best is underlined.\nlast century or even ancient times to the present century to\n\u201ctake\u201d photos for them. Fig. 4(a) illustrate the results. Com-\npared to our method, both Dreambooth and SDXL have dif-\nficulty generating realistic human images that have not ap-\npeared in real photos. In addition, due to the excessive re-\nliance of DreamBooth on the quality and resolution of cus-\ntomized images, it is difficult for DreamBooth to generate\nhigh-quality results when using old photos for customized\ngeneration.\nChanging age or gender. By simply replacing class words\n7\nDreamBooth\nSDXL \nPhotoMaker (Ours)\nA man piloting a \nspaceship\nReferences\nDreamBooth\nSDXL \nPhotoMaker (Ours)\nA girl wearing a \nChristmas hat\nA woman happily smiling, \nlooking at the camera\nReferences\nA man coding in front \nof a computer\n(a)\n(b)\nFigure 4. Applications on (a) artwork and old photo, and (b) changing age or gender. We are able to bring the past people back to real\nlife or change the age and gender of the input ID. For the first application, we prepare a prompt template A photo of <original\nprompt>, photo-realistic for DreamBooth and SDXL. Correspondingly, we change the class word to the celebrity name in the\noriginal prompt. For the second one, we replace the class word to <class word> <name>, (at the age of 12) for them.\n(e.g. man and woman), our method can achieve changes\nin gender and age. Fig. 4(b) shows the results. Although\nSDXL and DreamBooth can also achieve the correspond-\ning effects after prompt engineering, our method can more\neasily capture the characteristic information of the charac-\nters due to the role of the stacked ID embedding. Therefore,\nour results show a higher ID fidelity.\nIdentity mixing. If the users provide images of different\nIDs as input, our PhotoMaker can well integrate the char-\nacteristics of different IDs to form a new ID. From Fig. 5,\nwe can see that neither DreamBooth nor SDXL can achieve\nidentity mixing. In contrast, our method can retain the char-\nacteristics of different IDs well on the generated new ID,\nregardless of whether the input is an anime IP or a real per-\nson, and regardless of gender. Besides, we can control the\nproportion of this ID in the new generated ID by controlling\nthe corresponding ID input quantity or prompt weighting.\nWe show this ability in Fig. 10-11 in the appendix.\nStylization. In Fig. 6, we demonstrate the stylization capa-\nbilities of our method. We can see that, in the generated im-\nages, our PhotoMaker not only maintains good ID fidelity\nbut also effectively exhibits the style information of the in-\nput prompt. This reveals the potential for our method to\ndrive more applications. Additional results are shown in the\nFig. 12 within the appendix.\n4.3. Ablation study\nWe shortened the total number of training iterations by eight\ntimes to conduct ablation studies for each variant.\nThe influence about the number of input ID images. We\nexplore the impact that forming the proposed stacked ID\nembedding through feeding different numbers of ID im-\nages. In Fig. 7, we visualize this impact across different\nmetrics. We conclude that using more images to form a\nstacked ID embedding can improve the metrics related to ID\nfidelity. This improvement is particularly noticeable when\nthe number of input images is increased from one to two.\nDreamBooth\nSDXL \nPhotoMaker (Ours)\nA man holding a bottle \nof red wine\nReferences\nA woman frowning \nat the camera\nA man wearing a \nspacesuit\nFigure 5.\nIdentity mixing.\nWe are able to generate the im-\nage with a new ID while preserving input identity characteristics.\nWe prepare a prompt template <original prompt>, with\na face blended with <name:A> and <name:B> for\nSDXL. (Zoom-in for the best view)\nA Ukiyo-e painting of a \n<class>\nA painting of a <class>, \nin Van Gogh style\nA <class> in a comic \nbook\nA sketch of a <class>\nReferences\nFigure 6. The stylization results of our PhotoMaker. The sym-\nbol <class> denotes it will be replaced by man or woman ac-\ncordingly. (Zoom-in for the best view)\nUpon the input of an increasing number of ID images, the\ngrowth rate of the values in the ID-related metrics signif-\nicantly decelerates. Additionally, we observe a linear de-\n8\n2\n4\n6\n8\n10\n# Images\n70.8\n71.4\n72.0\n72.6\n73.2\nCLIP-I (%)\n2\n4\n6\n8\n10\n# Images\n48.6\n48.7\n48.8\n48.9\n49.0\nDINO (%)\n2\n4\n6\n8\n10\n# Images\n27.60\n27.75\n27.90\n28.05\n28.20\nCLIP-T (%)\n2\n4\n6\n8\n10\n# Images\n64.0\n64.8\n65.6\n66.4\n67.2\nFace Similarity (%)\n(a)\n(b)\n(c)\n(d)\nFigure 7. The impact of the number of input ID images on (a) CLIP-I, (b) DINO, (c) CLIP-T, and (d) Face Similarity, respectively.\nCLIP-T\u2191\nDINO\u2191\nFace Sim.\u2191\nFace Div.\u2191\nAverage\n28.7\n47.0\n48.8\n56.3\nLinear\n28.6\n47.3\n48.1\n54.6\nStacked\n28.0\n49.5\n53.6\n55.0\n(a) Embedding composing choices.\nCLIP-T\u2191\nDINO\u2191\nFace Sim.\u2191\nFace Div.\u2191\nSingle embed\n27.9\n50.3\n50.5\n56.1\nSingle image\n27.3\n50.3\n60.4\n51.7\nOurs\n28.0\n49.5\n53.6\n55.0\n(b) Training data sampling strategy.\nTable 2. Ablation studies for the proposed PhotoMaker. The best results are marked in bold.\nA photo of a man\nA woman wearing a \nred sweater\nReferences\n1 image\n2 images\n6 images\n10 images\nA photo of a woman\nFigure 8. The impact of varying the quantity of input images\non the generation results. It can be observed that the fidelity of\nthe ID increases with the quantity of input images.\ncline on the CLIP-T metric. This indicates there may ex-\nist a trade-off between text controllability and ID fidelity.\nFrom Fig. 8, we see that increasing the number of input im-\nages enhances the similarity of the ID. Therefore, the more\nID images to form the stacked ID embedding can help the\nmodel perceive more comprehensive ID information, and\nthen more accurately represent the ID to generate images.\nBesides, as shown by the Dwayne Johnson example, the\ngender editing capability decreases, and the model is more\nprone to generate images of the original ID\u2019s gender.\nThe choices of composing multiple embeddings. We ex-\nplore three ways to compose the ID embedding, including\naveraging the image embeddings, adaptively projecting em-\nbeddings through a linear layer, and our stacking way. From\nTab. 2a, we see the stacking way has the highest ID fidelity\nwhile ensuring a diversity of generated faces, demonstrating\nits effectiveness. Besides, such a way offers greater flexibil-\nity than others, including accepting any number of images\nand better controlling the mixing process of different IDs.\nThe benefits from multiple embeddings during training.\nWe explore two other training data sampling strategies to\ndemonstrate that it is necessary to input multiple images\nwith variations during training. The first is to choose only\none image, which can be different from the target image,\nto form the ID embedding (see \u201csingle embed\u201d in Tab. 2b).\nOur multiple embedding way has advantages in ID fidelity.\nThe second sampling strategy is to regard the target im-\nage as the input ID image (to simulate the training way of\nmost embedding-based methods). We generate multiple im-\nages based on this image with different data augmentation\nmethods and extract corresponding multiple embeddings.\nIn Tab. 2b, as the model can easily remember other irrele-\nvant characteristics of the input image, the generated facial\narea lacks sufficient changes (low diversity).\n5. Conclusion\nWe have presented PhotoMaker, an efficient personalized\ntext-to-image generation method that focuses on generat-\ning realistic human photos. Our method leverages a sim-\nple yet effective representation, stacked ID embedding,\nfor better preserving ID information. Experimental results\nhave demonstrated that our PhotoMaker, compared to other\nmethods, can simultaneously satisfy high-quality and di-\nverse generation capabilities, promising editability, high in-\nference efficiency, and strong ID fidelity. Besides, we also\nhave found that our method can empower many interesting\napplications that previous methods are hard to achieve, such\nas changing age or gender, bringing persons from old pho-\ntos or artworks back to reality, and identity mixing.\n9\nReferences\n[1] Photo ai. https://photoai.com/. Accessed: 2023-\n12-08. 2\n[2] Low-rank adaptation for fast text-to-image diffusion fine-\ntuning.\nhttps://github.com/cloneofsimo/\nlora, 2022. 2, 3, 4, 5\n[3] Prompt weighting.\nhttps://huggingface.co/\ndocs/diffusers/using-diffusers/weighted_\nprompts, 2023. 3, 5, 2\n[4] Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Only a\nmatter of style: Age transformation using a style-based re-\ngression model. TOG, 2021. 5\n[5] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel\nCohen-Or, Ariel Shamir, and Amit H Bermano. Domain-\nagnostic tuning-encoder for fast personalization of text-to-\nimage models. TOG, 2023. 2\n[6] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-\nOr, and Dani Lischinski. Break-a-scene: Extracting multiple\nconcepts from a single image. In SIGGRAPH Asia, 2023. 6\n[7] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and An-\ndrew Zisserman. Vggface2: A dataset for recognising faces\nacross pose and age. In FG, 2018. 5\n[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nICCV, 2021. 6, 1\n[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts.\nIn CVPR,\n2021. 3\n[10] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, et al. Pixart-alpha: Fast training of diffusion\ntransformer for photorealistic text-to-image synthesis. arXiv\npreprint arXiv:2310.00426, 2023. 3\n[11] Li Chen,\nMengyi Zhao,\nYiheng Liu,\nMingxu Ding,\nYangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing\nLiu, Kang Du, et al.\nPhotoverse:\nTuning-free image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2309.05793, 2023. 2, 3\n[12] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui\nJia, Ming-Wei Chang, and William W Cohen. Subject-driven\ntext-to-image generation via apprenticeship learning. arXiv\npreprint arXiv:2304.00186, 2023. 3\n[13] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,\nand Hengshuang Zhao. Anydoor: Zero-shot object-level im-\nage customization. arXiv preprint arXiv:2307.09481, 2023.\n3\n[14] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-\nder Kirillov, and Rohit Girdhar.\nMasked-attention mask\ntransformer for universal image segmentation.\nIn CVPR,\n2022. 5\n[15] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\nZafeiriou. Arcface: Additive angular margin loss for deep\nface recognition. In CVPR, 2019. 5, 1\n[16] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia,\nand Stefanos Zafeiriou. Retinaface: Single-shot multi-level\nface localisation in the wild. In CVPR, 2020. 5, 6\n[17] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion. In ICLR, 2023. 2, 3, 4, 6, 7,\n1\n[18] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Designing an encoder\nfor fast personalization of text-to-image models.\narXiv\npreprint arXiv:2302.12228, 2023. 3\n[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio.\nGenerative adversarial networks.\nACM\nCommunications, 2020. 2\n[20] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,\nDimitris Metaxas, and Feng Yang. Svdiff: Compact param-\neter space for diffusion fine-tuning. In ICCV, 2023. 3\n[21] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. In ICLR, 2023. 3, 5, 2\n[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In NeurIPS, 2017. 6\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 3\n[24] Matthew Honnibal, Ines Montani, Sofie Van Landeghem,\nand Adriane Boyd. spacy: Industrial-strength natural lan-\nguage processing in python, 2020. 6\n[25] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models.\nIn\nICLR, 2022. 2, 5\n[26] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. 3\n[27] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2304.02642, 2023. 3\n[28] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei\nZhang, and Qiang Xu. HumanSD: A native skeleton-guided\ndiffusion model for human image generation. In ICCV, 2023.\n2\n[29] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, 2019. 2, 3, 5\n[30] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models.\nIn\nCVPR, 2023. 3\n[31] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 6\n10\n[32] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In CVPR, 2023. 3, 5\n[33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In ICML,\n2023. 5\n[34] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation.\nIn\nCVPR, 2023. 3\n[35] Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov,\nYanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, and Sergey\nTulyakov.\nHyperhuman:\nHyper-realistic human gener-\nation with latent structural diffusion.\narXiv preprint\narXiv:2310.08579, 2023. 2, 3, 5\n[36] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In ICCV, 2015.\n2, 3, 5\n[37] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu.\nSubject-diffusion: Open domain personalized text-to-image\ngeneration without test-time fine-tuning.\narXiv preprint\narXiv:2307.11410, 2023. 3\n[38] Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, and\nJiaying Liu. Unified multi-modal latent diffusion for joint\nsubject and text conditional image generation. arXiv preprint\narXiv:2303.09319, 2023. 3\n[39] Andrew\nMelnik,\nMaksim\nMiasayedzenkau,\nDzianis\nMakarovets, Dzianis Pirshtuk, Eren Akbulut, Dennis Holz-\nmann, Tarek Renusch, Gustav Reichert, and Helge Ritter.\nFace generation and editing with stylegan: A survey. arXiv\npreprint arXiv:2212.09102, 2022. 2\n[40] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2,\n3\n[41] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal\nYarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and\nDaniel Cohen-Or. Mystyle: A personalized generative prior.\nTOG, 2022. 2, 3, 5, 6, 1\n[42] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu.\nOn\naliased resizing and surprising subtleties in gan evaluation.\nIn CVPR, 2022. 6\n[43] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In ICCV, 2023. 3\n[44] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2, 3, 5, 6, 1\n[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 2, 3, 6\n[46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. JMLR, 2020. 2, 3\n[47] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 3\n[48] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence\nembeddings using siamese bert-networks. In EMNLP, 2019.\n6\n[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 3\n[50] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, 2023. 2, 3, 4, 6, 7, 1\n[51] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,\nTingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,\nand Kfir Aberman. Hyperdreambooth: Hypernetworks for\nfast personalization of text-to-image models. arXiv preprint\narXiv:2307.06949, 2023. 2, 3\n[52] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding.\nIn\nNeurIPS, 2022. 2, 3\n[53] Florian Schroff, Dmitry Kalenichenko, and James Philbin.\nFacenet: A unified embedding for face recognition and clus-\ntering. In CVPR, 2015. 6\n[54] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 3\n[55] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLaion-5b:\nAn open large-scale dataset for\ntraining next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022. 2, 3, 5\n[56] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023. 2,\n3\n[57] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image\ngeneration in any style. In NeurIPS, 2023. 3\n[58] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2021. 3, 6\n[59] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin\nChen, Liang Lin, and Meng Yang. Toward characteristic-\npreserving image-based virtual try-on network. In ECCV,\n2018. 2\n[60] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang,\nWayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change\n11\nLoy. Mead: A large-scale audio-visual dataset for emotional\ntalking-face generation. In ECCV, 2020. 3, 5\n[61] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. In ICCV, 2023. 3, 5\n[62] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr\u00b4edo\nDurand, and Song Han. Fastcomposer: Tuning-free multi-\nsubject image generation with localized attention.\narXiv\npreprint arXiv:2305.10431, 2023. 2, 3, 4, 5, 6, 7, 1\n[63] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 3, 6, 7, 1, 2\n[64] Ge Yuan,\nXiaodong Cun,\nYong Zhang,\nMaomao Li,\nChenyang Qi, Xintao Wang, Ying Shan, and Huicheng\nZheng. Inserting anybody in diffusion models via celeb ba-\nsis. In NeurIPS, 2023. 3\n[65] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, 2023. 2, 3\n[66] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 6\n[67] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,\nXi Shen, Yu Guo, Ying Shan, and Fei Wang.\nSadtalker:\nLearning realistic 3d motion coefficients for stylized audio-\ndriven single image talking face animation. In CVPR, 2023.\n2\n[68] Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dong-\ndong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming\nZeng, and Fang Wen. General facial representation learning\nin a visual-linguistic manner. In CVPR, 2022. 3, 5\n12\nAppendix\nEvaluation IDs\n1 Alan Turing\n14 Kamala Harris\n2 Albert Einstein\n15 Marilyn Monroe\n3 Anne Hathaway\n16 Mark Zuckerberg\n4 Audrey Hepburn\n17 Michelle Obama\n5 Barack Obama\n18 Oprah Winfrey\n6 Bill Gates\n19 Ren\u00b4ee Zellweger\n7 Donald Trump\n20 Scarlett Johansson\n8 Dwayne Johnson\n21 Taylor Swift\n9 Elon Musk\n22 Thomas Edison\n10 Fei-Fei Li\n23 Vladimir Putin\n11 Geoffrey Hinton\n24 Woody Allen\n12 Jeff Bezos\n25 Yann LeCun\n13 Joe Biden\nTable 3. ID names used for evaluation. For each name, we col-\nlect four images totally.\nA. Dataset Details\nTraining dataset. Based on Sec. 3.3 in the main paper, fol-\nlowing a sequence of filtering steps, the number of images\nin our constructed dataset is about 112K. They are classified\nby about 13,000 ID names. Each image is accompanied by\na mask for the corresponding ID and an annotated caption.\nEvaluation dataset. The image dataset used for evaluation\ncomprises manually selected additional IDs and a portion\nof MyStyle [41] data. For each ID name, we have four im-\nages that serve as input data for comparative methods and\nfor the final metric evaluation (i.e., DINO [8], CLIP-I [17],\nand Face Sim. [15]). For single-embedding methods (i.e.,\nFastComposer [62] and IPAdapter [63]), we randomly se-\nlect one image from each ID group as input. Note that the\nID names exist in the training image set, utilized for the\ntraining of our method, and the test image set, do not ex-\nhibit any overlap. We list ID names for evaluation in Tab. 3.\nFor text prompts used for evaluation, we consider six fac-\ntors: clothing, accessories, actions, expressions, views, and\nbackground, which make up 40 prompts that are listed in\nthe Tab. 4.\nB. User Study\nIn this section, we conduct a user study to make a more\ncomprehensive comparison.\nThe comparative methods\nwe have selected include DreamBooth [50], FastCom-\nposer [62], and IPAdapter [63]. We use SDXL [44] as the\nbase model for both DreamBooth and IPAdapter because of\ntheir open-sourced implementations. We display 20 text-\nPercentage (%)\nID Fidelity\nQuality\nDiversity\nText Fidelity\n38.0\n53.2\n49.0\n56.1\n14.0\n7.1\n21.6\n4.8\n24.9\n12.2\n8.0\n10.7\n23.0\n27.6\n21.3\n28.4\nPhotoMaker (Ours)\nIPAdapter\nFastComposer\nDreamBooth\nFigure 9. User preferences on ID fidelity, generation quality,\nface diversity, and text fidelity for different methods. For ease\nof illustration, we visualize the proportion of total votes that each\nmethod has received. Our PhotoMaker occupies the most signifi-\ncant proportion in these four dimensions.\nimage pairs for each user. Each of them includes a refer-\nence image of the input ID and corresponding text prompt.\nWe have four randomly generated images of each method\nfor each text-image pair. Each user is requested to answer\nfour questions for these 20 sets of results: 1) Which method\nis most similar to the input person\u2019s identity? 2) Which\nmethod produces the highest quality generated images? 3)\nWhich method generates the most diverse facial area in the\nimages? 4) Which method generates images that best match\nthe input text prompt? We have anonymized the names of\nall methods and randomized the order of methods in each\nset of responses. We had a total of 40 candidates partici-\npating in our user study, and we received 3,200 valid votes.\nThe results are shown in Fig. 9.\nWe find that our PhotoMaker has advantages in terms of\nID fidelity, generation quality, diversity, and text fidelity, es-\npecially the latter three. In addition, we found that Dream-\nBooth is the second-best algorithm in balancing these four\nevaluation dimensions, which may explain why it was more\nprevalent than the embedding-based methods in the past. At\nthe same time, IPAdapter shows a significant disadvantage\nin terms of generated image quality and text consistency,\nas it focuses more on image embedding during the training\nphase. FastComposer has a clear shortcoming in the diver-\nsity of the facial region for their single-embedding training\npipeline. The above results are generally consistent with\nTab. 1 in the main paper, except for the discrepancy in the\nCLIP-T metric. This could be due to a preference for select-\ning images that harmonize with the objects appearing in the\ntext when manually choosing the most text-compatible im-\nages. In contrast, the CLIP-T tends to focus on whether the\nobject appears. This may demonstrate the limitations of the\nCLIP-T. We also provide more visual samples in Fig. 14-17\n1\nCategory\nPrompt\nGeneral\na photo of a <class word>\nClothing\na <class word> wearing a Superman outfit\na <class word> wearing a spacesuit\na <class word> wearing a red sweater\na <class word> wearing a purple wizard outfit\na <class word> wearing a blue hoodie\nAccessory\na <class word> wearing headphones\na <class word> with red hair\na <class word> wearing headphones with red\nhair\na <class word> wearing a Christmas hat\na <class word> wearing sunglasses\na <class word> wearing sunglasses and neck-\nlace\na <class word> wearing a blue cap\na <class word> wearing a doctoral cap\na <class word> with white hair, wearing glasses\nAction\na <class word> in a helmet and vest riding a mo-\ntorcycle\na <class word> holding a bottle of red wine\na <class word> driving a bus in the desert\na <class word> playing basketball\na <class word> playing the violin\na <class word> piloting a spaceship\na <class word> riding a horse\na <class word> coding in front of a computer\na <class word> playing the guitar\n(a)\nCategory\nPrompt\nExpression\na <class word> laughing on the lawn\na <class word> frowning at the camera\na <class word> happily smiling, looking at the\ncamera\na <class word> crying disappointedly, with\ntears flowing\na <class word> wearing sunglasses\nView\na <class word> playing the guitar in the view of\nleft side\na <class word> holding a bottle of red wine, up-\nper body\na <class word> wearing sunglasses and neck-\nlace, close-up, in the view of right side\na <class word> riding a horse, in the view of the\ntop\na <class word> wearing a doctoral cap, upper\nbody, with the left side of the face facing the camera\na <class word> crying disappointedly, with\ntears flowing, with left side of the face facing the\ncamera\nBackground\na <class word> sitting in front of the camera,\nwith a beautiful purple sunset at the beach in the\nbackground\na <class word> swimming in the pool\na <class word> climbing a mountain\na <class word> skiing on the snowy mountain\na <class word> in the snow\na <class word> in space wearing a spacesuit\n(b)\nTable 4. Evaluation text prompts categorized by (a) general setting, clothing, accessory, action, (b) expression, view, and back-\nground. The class word will be replaced with man, woman, boy, etc. For each ID and each prompt, we randomly generated four\nimages for evaluation.\nfor reference.\nC. More Ablations\nAdjusting the ratio during identity mixing. For identity\nmixing, our method can adjust the merge ratio by either\ncontrolling the percentage of identity images within the in-\nput image pool or through the method of prompt weight-\ning [3, 21]. In this way, we can control that the person gen-\nerated with a new ID is either more closely with or far away\nfrom a specific input ID. Fig. 10 shows how our method\ncustomizes a new ID by controlling the proportion of dif-\nferent IDs in the input image pool. For a better description,\nwe use a total of 10 images as input in this experiment. We\ncan observe a smooth transition of images with the two IDs.\nThis smooth transition encompasses changes in skin color\nand age. Next, we use four images per generated ID to con-\nduct prompt weighting. The results are shown in Fig. 11.\nWe multiply the embedding corresponding to the images\nrelated to a specific ID by a coefficient to control its propor-\ntion of integration into the new ID. Compared to the way\nto control the number of input images, prompt weighting\nrequires fewer photos to adjust the merge ratio of differ-\nent IDs, demonstrating its superior usability. Besides, the\ntwo ways of adjusting the mixing ratio of different IDs both\ndemonstrate the flexibility of our method.\nD. Stylization Results\nOur method not only possesses the capability to generate re-\nalistic human photos, but it also allows for stylization while\npreserving ID attributes. This demonstrates the robust gen-\neralizability of the proposed method. We provide the styl-\nization results in Fig. 12.\nE. More Visual Results\nRecontextualization.\nWe first provide a more intuitive\ncomparison in Fig. 14.\nWe compare our PhotoMaker\nwith DreamBooth [50], FastComposer [62], and IPAdapa-\nter [63], for universal recontextualization cases. Compared\nto other methods, the results generated by our method can\nsimultaneously satisfy high-quality, strong text controllabil-\n2\n50%\n80% Obama\n20% Biden\n20% Obama\n80% Biden\nA man wearing a red sweater\n50%\n80% Obama\n20% Scarlett\n20% Obama\n80% Scarlett\nA woman happily smiling, looking at the camera\nFigure 10. The impact of the proportion of images with different IDs in the input sample pool on the generation of new IDs. The\nfirst row illustrates the transition from Barack Obama to Joe Biden. The second row depicts the shift from Michelle Obama to Scarlett\nJohansson. To provide a clearer illustration, percentages are used in the figure to denote the proportion of each ID in the input image pool.\nThe total number of images contained in the input pool is 10. (Zoom-in for the best view)\nA man in the snow\n0.1\n0.5\n0.8\n0.9\n1.0\n1.1\n1.2\n1.5\n2.0\n: 1.0\nA woman wearing sunglasses\n0.1\n0.5\n0.8\n0.9\n1.0\n1.1\n1.2\n1.5\n2.0\n: 1.0\nFigure 11. The impact of prompt weighting on the generation of new IDs. The first row illustrates a blend of Barack Obama and Joe\nBiden. The first row from left to right represents the progressive increase in the weight of the ID image embedding corresponding to Barack\nObama in the image. The second row illustrates a blend of Elsa (Disney) and Anne Hathaway. The weight for Elsa is gradually increased.\n(Zoom-in for the best view)\n3\nA Ukiyo-e painting \nof a <class>\nA painting of a <class>, \nin Van Gogh style\nA <class> in Ghibli \nanimation style\nA <class> in a \ncomic book\nA sketch of a \n<class>\nReferences\nFigure 12. The stylization results of our PhotoMaker with different input IDs and different style prompts. Our method can be\nseamlessly transferred to a variety of styles, concurrently preventing the generation of realistic results. The symbol <class> denotes it\nwill be replaced by man or woman accordingly. (Zoom-in for the best view)\n4\nA photo of Mira Murati\nA photo of OpenAI CTO\nA photo of chief scientist in \nOpenAI\nA photo of Ilya Sutskever\nFigure 13. Two examples that are unrecognizable by the SDXL.\nWe replaced two types of text prompts (e.g., name and position)\nbut were unable to prompt the SDXL to generate Mira Murati and\nIlya Sutskever.\nity, and high ID fidelity. We then focus on the IDs that\nSDXL can not generate itself. We refer to this scenario as\nthe \u201cnon-celebrity\u201d case. Compared Fig. 15 with Fig. 13,\nour method can successfully generate the corresponding in-\nput IDs for this setting.\nBringing person in artwork/old photo into reality.\nFig. 16-17 demonstrate the ability of our method to bring\npast celebrities back to reality.\nIt is worth noticing that\nour method can generate photo-realistic images from IDs\nin statues and oil paintings. Achieving this is quite chal-\nlenging for the other methods we have compared.\nChanging age or gender. We provide more visual results\nfor changing age or gender in Fig. 18. As mentioned in the\nmain paper, we only need to change the class word when\nwe conduct such an application. In the generated ID images\nchanged in terms of age or gender, our method can well\npreserve the characteristics in the original ID.\nIdentity mixing. We provide more visual results for iden-\ntity mixing application in Fig. 19.\nBenefiting from our\nstacked ID embedding, our method can effectively blend\nthe characteristics of different IDs to form a new ID. Subse-\nquently, we can generate text controlled based on this new\nID. Additionally, our method provides great flexibility dur-\ning the identity mixing, as can be seen in Fig. 10-11. More\nimportantly, we have explored in the main paper that ex-\nisting methods struggle to achieve this application. Con-\nversely, our PhotoMaker opens up a multitude of possibili-\nties.\nF. Limitations\nFirst, our method only focuses on maintaining the ID infor-\nmation of a single generated person in the image, and can-\nnot control multiple IDs of generated persons in one image\nsimultaneously. Second, our method excels at generating\nhalf-length portraits, but is relatively not good at generating\nfull-length portraits. Third, the age transformation ability\nof our method is not as precise as some GAN-based meth-\nods [4]. If the users need more precise control, modifica-\ntions to the captions of the training dataset may be required.\nFinally, our method is based on the SDXL and the dataset\nwe constructed, so it will also inherit their biases.\nG. Broader Impact\nIn this paper, we introduce a novel method capable of gener-\nating high-quality human images while maintaining a high\ndegree of similarity to the input identity. At the same time,\nour method can also satisfy high efficiency, decent facial\ngeneration diversity, and good controllability.\nFor the academic community, our method provides a\nstrong baseline for personalized generation. Our data cre-\nation pipeline enables more diverse datasets with varied\nposes, actions, and backgrounds, which can be instrumen-\ntal in developing more robust and generalizable computer\nvision models.\nIn the realm of practical applications, our technique has\nthe potential to revolutionize industries such as entertain-\nment, where it can be used to create realistic characters\nfor movies or video games without the need for extensive\nCGI work. It can also be beneficial in virtual reality, pro-\nviding more immersive and personalized experiences by al-\nlowing users to see themselves in different scenarios. It is\nworth noticing that everyone can rely on our PhotoMaker to\nquickly customize their own digital portraits.\nHowever, we acknowledge the ethical considerations\nthat arise with the ability to generate human images with\nhigh fidelity. The proliferation of such technology may lead\nto a surge in the inappropriate use of generated portraits,\nmalicious image tampering, and the spreading of false in-\nformation. Therefore, we stress the importance of develop-\ning and adhering to ethical guidelines and using this tech-\nnology responsibly. We hope that our contribution will spur\nfurther discussion and research into the safe and ethical use\nof human generation in computer vision.\n5\nReferences\nA man in the snow\nDreambooth\nA man crying disappointedly, with tears flowing\nA photo of a woman\nA man wearing a doctoral cap\nA woman climbing a mountain\nA man in space wearing a spacesuit\nFastComposer\nIPAdapter\nPhotoMaker (Ours)\nFigure 14. More visual examples for recontextualization setting. Our method not only provides high ID fidelity but also retains text\nediting capabilities. We randomly sample three images for each prompt. (Zoom-in for the best view)\n6\nReferences\nA woman happily smiling, looking at the camera\nDreambooth\nA photo of a woman\nA man in the snow\nA man wearing sunglasses\nA man in a helmet and vest riding a motorcycle\nA man sitting in front of the camera, with a beautiful purple sunset at the beach in the background\nFastComposer\nIPAdapter\nPhotoMaker (Ours)\nFigure 15. More visual examples for recontextualization setting. Our method not only provides high ID fidelity but also retains text\nediting capabilities. We randomly sample three images for each prompt. (Zoom-in for the best view)\n7\nReferences\nA man wearing a Christmas hat\nA man riding a horse\nA man wearing a blue cap\nA man wearing sunglasses\nA man frowning at the camera\nA man wearing a red sweater\nDreambooth\nFastComposer\nIPAdapter\nPhotoMaker (Ours)\nFigure 16. More visual examples for bringing person in old-photo back to life. Our method can generate high-quality images. We\nrandomly sample three images for each prompt. (Zoom-in for the best view)\n8\nReferences\nA woman wearing a spacesuit\nA woman wearing sunglasses\nA man holding a bottle of red wine\nA man in the snow \nA photo of a man\nA man in space wearing a spacesuit\nDreambooth\nFastComposer\nIPAdapter\nPhotoMaker (Ours)\nFigure 17. More visual examples for bringing person in artworks back to life. Our PhotoMaker can generate photo-realistic images\nwhile other methods are hard to achieve. We randomly sample three images for each prompt. (Zoom-in for the best view)\n9\nA boy wearing a blue hoodie\nA boy wearing sunglasses\nReferences\nA boy happily smiling\nA woman wearing a spacesuit\nA woman with white hair, \nwearing the sunglasses\nA woman with red hair\nA boy swimming in the pool\nA boy frowning at the camera\nA boy wearing a red sweater\nA woman wearing a doctoral \ncap\nA woman happily smiling\nA woman in the snow\nFigure 18. More visual examples for changing age or gender for each ID. Our PhotoMaker, when modifying the gender and age of the\ninput ID, effectively retains the characteristics of the face ID and allows for textual manipulation. We randomly sample three images for\neach prompt. (Zoom-in for the best view)\n10\nA man wearing a Christmas cap\nA man wearing headphones \nwith red hair\nReferences\nA man wearing a doctoral cap\nA man in space wearing a \nspacesuit\nA man in the snow\nA man wearing headphones \nwith red hair\nA man wearing sunglasses and \nnecklace\nA man wearing a red sweater\nA man happily smiling\nA woman wearing a blue \nhoodie\nA woman wearing a Christmas \ncap\nA woman wearing a red \nsweater\nA woman in wearing a \nspacesuit\nA woman wearing headphones \nwith red hair\nA woman swimming in the pool\nA woman piloting a spaceship\nA woman wearing a red \nsweater\nA woman in wearing a \nspacesuit\nFigure 19. More visual results for identity mixing applications. Our PhotoMaker can maintain the characteristics of both input IDs in\nthe new generated ID image, while providing high-quality and text-compatible generation results. We randomly sample three images for\neach prompt. (Zoom-in for the best view)\n11\n"
  },
  {
    "title": "Alpha-CLIP: A CLIP Model Focusing on Wherever You Want",
    "link": "https://arxiv.org/pdf/2312.03818.pdf",
    "upvote": "31",
    "text": "Alpha-CLIP: A CLIP Model Focusing on Wherever You Want\nZeyi Sun\u22171,4, Ye Fang\u22172,4, Tong Wu3, Pan Zhang4, Yuhang Zang4,\nShu Kong5, Yuanjun Xiong6, Dahua Lin3,4, Jiaqi Wang\u20204\n1Shanghai Jiao Tong University\n2Fudan University\n3The Chinese University of Hong Kong\n4Shanghai AI Laboratory\n5University of Macau\n6MThreads, Inc.\nszy2023@sjtu.edu.cn, {fangye, zhangpan, zangyuhang, wangjiaqi}@pjlab.org.cn\nhttps://aleafy.github.io/alpha-clip\nAlpha-CLIP+LLM\nAlpha-CLIP+NeRF\nAlpha-CLIP+Diffusion\nAlpha-CLIP+SAM\nThe image features a person walking \ndown a path in a park, surrounded by \na forest of tall trees. The person \nis wearing a backpack and \nappears to be enjoying his time in the \npark. The scene is captured in a \nvibrant, colorful painting that highlights \nthe beauty of the natural environment.\nThe image features a man standing \nin the middle of a road, surrounded \nby a forest of tall trees. The man \nappears to be wearing a black \nshirt and is positioned near the \ncenter of the scene. The trees are \nlush and green, creating a serene \nand natural atmosphere.\nThe image depicts a man walking \ndown a path in a park, surrounded by \na beautiful forest filled with tall trees. \nThe sky above him is clear \nand blue, creating a serene \nand peaceful atmosphere. \nThe man appears to be enjoying his \ntime in the park, possibly taking a \nleisurely stroll or engaging in some \noutdoor activity. \nBlip Diffusion\nPoint\u2022E\nMaskImageNet\nauto labeled fine-grained mask\nseahorse\nImageNet\nauto \nlabel\nAlpha-CLIP\nAlpha-CLIP\nOri-CLIP\nOri-CLIP\nAn exquisite two-tier birthday cake, with the upper layer decorated with a gorgeous sugar-carved crown and the lower layer decorated with gorgeous sugar flowers.\nBackAug\nWhat is in the bottle?\nThe bottle contains water.\nBut l want to drink beer \nnow, what should I do?\nIf you want to drink beer \ninstead of water, you can \nreplace the water bottle \nwith a beer bottle. \nWhere is the beer?\nThe beer is in the beer bottle, \nwhich is sitting on the table \nnext to the water bottle.\nWho is carrying a ring?\nThe man is carrying a ring.\nWho is carrying a ring?\nThe woman is carrying a ring.\nloss\nFigure 1. Usage of our proposed Alpha-CLIP. Our Alpha-CLIP can seamlessly replace the original CLIP in a wide range of tasks to\nallow the whole system to focus on any specified region given by points, strokes or masks. Cases marked with\nare generated with the\noriginal CLIP. Cases marked with\nare generated with our Alpha-CLIP. All cases shown here are made simply by replacing the original\nCLIP of the system with a plug-in Alpha-CLIP without further tuning.\nAbstract\nContrastive Language-Image Pre-training (CLIP) plays\nan essential role in extracting valuable content information\nfrom images across diverse tasks. It aligns textual and vi-\nsual modalities to comprehend the entire image, including\nall the details, even those irrelevant to specific tasks. How-\never, for a finer understanding and controlled editing of im-\nages, it becomes crucial to focus on specific regions of in-\nterest, which can be indicated as points, masks, or boxes\n\u2217 Equal contribution. \u2020 Corresponding authors.\nby humans or perception models.\nTo fulfill the require-\nments, we introduce Alpha-CLIP, an enhanced version of\nCLIP with an auxiliary alpha channel to suggest attentive\nregions and fine-tuned with constructed millions of RGBA\nregion-text pairs. Alpha-CLIP not only preserves the visual\nrecognition ability of CLIP but also enables precise control\nover the emphasis of image contents. It demonstrates effec-\ntiveness in various tasks, including but not limited to open-\nworld recognition, multimodal large language models, and\nconditional 2D / 3D generation. It has a strong potential to\nserve as a versatile tool for image-related tasks.\n1\narXiv:2312.03818v2  [cs.CV]  13 Dec 2023\nDomains\nComponents\nTasks\nMethods\nAdvantages over the original CLIP\nImage Recognition\nAlpha-CLIP\nZero-shot Classification\nZero-shot REC\n-\nSuperior classification accuracy\nExcellent region-text comprehension ability\nAlpha-CLIP + SAM\nData Engine for OVD\nDetic [76]\nHigher OVD mAP\nMLLM\nAlpha-CLIP + LLM\nVQA, Captioning\nBLIP-2 [28], LLaVA-1.5 [33]\nRegion-focused captioning / VQA\nEliminating hallucinations\nReducing model bias\n2D Generation\nAlpha-CLIP + Diffusion\nImage Variation\nBLIP-Diffusion [27]\nControllable generation\nEnabling subject-driven generation in complex images\n3D Generation\nAlpha-CLIP + Diffusion\nGeneralized Image-to-3D\nPoint-E [39]\nRectifying absent parts\nAlpha-CLIP + NeRF\nOptimized Image-to-3D\nPureCLIPNeRF [25]\nImproved 3D optimization results\nTable 1. Downstream tasks of Alpha-CLIP and their advantages over the original CLIP\n1. Introduction\nRecent advances in Contrastive Language-Image Pre-\ntraining (CLIP) [19, 43] and its diverse variants [10, 30, 55]\nhave established a robust framework for extracting seman-\ntically coherent features from both images and text. These\nfeatures aim to capture all the semantic details within im-\nages, exhibiting potent representation capabilities and ex-\nceptional generalizability, making them versatile in a va-\nriety of downstream tasks, such as open-world recogni-\ntion [7, 13, 62, 64, 65], Multimodal Large Language Models\n(MLLMs) [4, 18, 26, 28, 33, 34, 41, 56, 71], and 2D / 3D\ngeneration [20, 25, 27, 38, 39, 44, 68].\nWhile CLIP captures the content of the entire image, it\nis also crucial to focus on the regions of interest to enable\na finer understanding [16, 21, 24, 42, 53, 77] and control-\nlable content generation [25, 38, 51, 60]. These regions can\nbe specified by points, masks, or boxes via human interac-\ntion or perception models (e.g., SAM [22], GLIP [29] and\nproposal networks [70]).\nTo fulfill the demands of downstream tasks, researchers\nhave attempted to acquire region-focused CLIP features us-\ning two primary strategies. The first method is to exclude\nnon-relevant areas by cropping the regions of interest into\ndistinct patches [7, 54, 73, 74] or applying masking to the\nirrelevant parts of images [31], features [31, 60], and atten-\ntion masks [65, 75]. However, this approach disrupts (in\ncropping) and omits (in masking) contextual information,\nwhich is crucial for precise image understanding and rea-\nsoning. The second method is to highlight the regions of\ninterest by circles [52] or mask contour [66] on the images\nfed to CLIP. Although user-friendly, it changes the origi-\nnal content of the images, which will result in undesirable\nrecognition and generation results (cf. Fig. 2).\nTo achieve region focus without hurting original image,\nwe propose Alpha-CLIP, which improves CLIP [43] by in-\ncorporating regions of interest through an additional alpha\nchannel input. Along with the RGB channels, the intro-\nduced alpha channel enables the Alpha-CLIP to focus on\ndesignated areas while maintaining an awareness of the con-\ntextual information. While initialized with the CLIP [43]\nmodel, the training of Alpha-CLIP still requires a large set\nof region-text paired data. By harnessing the Segment Any-\nthing Model (SAM) [22] and multimodal large models for\nimage captioning, such as BLIP-2 [28], we develop an ef-\nfective pipeline to generate millions of region-text pairs that\nare readily convertible to RGBA-text data. After training\nwith a mixture of region-text pairs and image-text pairs,\nAlpha-CLIP can focus on the specific regions while main-\ntaining the visual recognition accuracy of CLIP [43].\nAlpha-CLIP can enhance CLIP across a wide array of\ndownstream tasks, applying a plug-and-play methodology\nthat permeates diverse domains, spanning from perception\nto generation in 2D and 3D applications, as shown in Fig. 1\nand Tab. 1. Specifically, 1) Image Recognition: Alpha-\nCLIP not only maintains the visual recognition ability of the\noriginal CLIP but also boosts the capability of region-based\nrecognition. Specifically, when provided with ground-truth\nregion to focus on, Alpha-CLIP achieves 4.1% improve-\nment in top-1 accuracy on zero-shot ImageNet classification\ntask. This superior region-based recognition ability helps\ndownstream tasks like Referring Expression Comprehen-\nsion(REC) [54] or serves as data engine for Open Vocabu-\nlary Detection(OVD) [76]. 2) Serving as vision backbone\nfor MLLM: In conjunction with a large language model,\nAlpha-CLIP becomes capable of facilitating region level\ncaptioning and VQA within a MLLM framework. This inte-\ngration significantly mitigates the occurrences of hallucina-\ntions (e.g., black shirt) and diminishes model bias (e.g., man\ncarrying a ring). 3) 2D generation: When integrated with\na diffusion model, Alpha-CLIP enhances the controllability\nof BLIP-Diffusion [27] in image variation tasks. In addi-\ntion, it enables the extraction of subjects from complex im-\nages for subject-driven generation, surmounting an obstacle\nencountered when deploying BLIP-Diffusion with the orig-\ninal CLIP, which only supports single subjects in simplistic\nimages. 4) 3D generation: In addition to the capabilities in\n2D generation, Alpha-CLIP exhibits proficiency in 3D gen-\neration as well. It can be effectively deployed in conjunction\nwith a diffusion model, such as Point-E [39], to enhance the\nquality of 3D object generation. Additionally, it can be uti-\nlized with NeRF [37], exemplified by PureCLIPNeRF [25],\nto optimize the creation of superior 3D objects.\n2\ncropping\nmasking\nAlpha-CLIP\nred circle\nfeature masking\ndisrupt context\nrectangular area only\nomit context\nchange image content\nomit context\npatch-level granularity\nkeep context\npixel-level granularity\nFigure 2. Alpha-CLIP vs. other methods of region-focusing for image generation using BLIP-Diffusion [27]. The fine-grained region\nfocusing ability of Alpha-CLIP produces better results than these methods that adopt the original CLIP.\nIn summary, we propose Alpha-CLIP, which equips the\noriginal CLIP model with the capability of region aware-\nness. Through fine-tuning on millions of RGBA region-text\npairs, Alpha-CLIP demonstrates significant advantages over\nthe original CLIP across various tasks, including but not\nlimited to image recognition [43, 54, 76], multimodal large\nlanguage models [28, 34], 2D generation [27, 44] and 3D\ngeneration [25, 39].\n2. Related Work\nEmpowering CLIP with region awareness.\nTo enable\nCLIP [43] to disentangle regions from the whole image for\nmore targeted processing and understanding, various meth-\nods have been explored in the field of segmentation. Among\nthem, MaskCLIP [75] uses a 1x1 convolution layer to ex-\ntract CLIP\u2019s final 2D features to obtain semantic informa-\ntion for different regions. SAN [64] trains a side network\nalongside CLIP to assist the model in local semantic percep-\ntion. MaskCLIP [9] and ODISE [62] use attention masks\nto make CLIP focus more on local regions. These meth-\nods do not alter the weights of the CLIP model itself. Re-\ngionCLIP [74] generate region box-text pairs for local re-\ngion and fine-tune CLIP model for box level recognition.\nMaskAdaptedCLIP[31] generates mask-text pairs for local\nmasks through a pseudo-labeling process and fine-tunes the\nCLIP model to make it more adaptable to masked images.\nMaskQCLIP[65] fine-tunes attention layer for new mask\n[CLS] tokens to make it more fit for mask object classifica-\ntion. These two methods attempt to enhance CLIP\u2019s ability\nto focus on local features and exclusively fine-tune CLIP on\nspecific downstream datasets, resulting in poor generaliza-\ntion ability beyond detection or segmentation tasks.\nAnother approach is to change the input image by sim-\nply cropping or masking the image to leave only the fore-\nground object. ReCLIP [54] and OvarNet [7] crop the orig-\ninal image using bounding box from object proposal net-\nwork [70] and are applied on Referring Expression Compre-\nhension and Open Attribute Recognition tasks. MaskAdapt-\nedCLIP [31] sets the background area to pure color in\npixel space and uses the masked image as input for open-\nvocabulary segmentation. However, the valuable context\ninformation is lost except for using complex post-process\nproposed in ReCLIP [54]. Some other approaches prompt\nthe CLIP by modifying the input image, guiding CLIP to\nfocus on the area of interest. For example, Red-Circle [52],\nFGVP [66] use a circle or mask contour to tell CLIP where\nto focus.\nOverall, the quality of these approaches that\nchange the original content of input image is heavily contin-\ngent upon the symbols in CLIP\u2019s pre-training dataset. An-\nother limitation is directing modification of images causes\na domain gap with CLIP pertaining images. Unlike previ-\nous approaches that rely on segmentation or changing the\ninput image, our Alpha-CLIP incorporates an additional al-\npha channel, which does not change the image content and\npreserves the generalization performance (cf. Fig. 2).\nRegion-level image annotation. Existing CLIP models are\npretrained on large-scale datasets like LAION-400M [49]\nand LAION-5B [50], while fine-grained mask-level labels\nare not available due to high manual labor costs. Recently,\nKosmos-2 [41] introduced a pseudo-labeling pipeline that\nuses the pre-trained GLIP [29] model to automatically gen-\nerate fine-grained pseudo-labels of region boxes and their\nassociate expressions. By using this pseudo-labeling base-\nline, Kosmos-2 releases the GRIT dataset and equips multi-\nmodal model [18] with local perception capabilities. Simi-\nlarly, the All-Seeing [59] project also generates fine-grained\ntext labels via the pseudo-labeling pipeline. Meanwhile, the\nrecent SAM [22] model is trained on massive vision modal-\nity data with strong zero-shot abilities for downstream tasks\nlike box-to-mask conversion and automatic mask genera-\ntion. These developments have made it possible to gen-\nerate pseudo-masks with region captions at a large scale\nand have opened up the potential for greater adjustments\nto CLIP for region-level recognition. Therefore, We build\nupon GRIT[41] and SAM [22] to propose a method for gen-\nerating RGBA region-text pairs from grounding data.\nCLIP in MLLM. At the age of Multi-modal Large Lan-\nguage Models (MLLMs) [1\u20134, 18, 26, 28, 33, 34, 40, 41,\n71], CLIP [43] has been widely used as the vision back-\nbone for its semantic representative feature and promis-\n3\nBox-text pairs\nSAM\nMask-text pairs\nA female athlete\nVolleyball\nA sandy court\nangel fish\nSAM\nCLIP \nscore & rank\nBLIP\ncaptioning\nA green and \nyellow angle fish.\nA blue and yellow \nangle fish.\nAn angle fish is \ntuning around.\nGrounding Data\nClassification Data\nRGB\nConv\nRGB\nConv\nAlpha \nConv\nRGB\nConv\nA female athlete\nAttention \nBlock\nxN\nAlpha-CLIP Image Encoder\nCLIP \nText \nEncoder\n\u2112\ufffd\ufffd\ufffd\nModel framework\n(a) Data generation pipeline\n(b) Fine-tuning pipeline\nto pairs\nFigure 3. The pipeline of our data generation method and model architecture. (a) Our method generates millions of RGBA-region\ntext pairs. (b) Alpha-CLIP modifies the CLIP image encoder to take an additional alpha channel along with RGB.\ning scalability. To make MLLM focus on the specific re-\ngion, Kosmos-2 [41] uses millions of region-caption data\nto train the model with the guidance of box corner points.\nGPT4ROI [72] propose to apply the ROI Align [15] opera-\ntor on the CLIP image feature to refer to the specific region.\nGLaMM [45] further adds an extra region encoder. Dif-\nferent from previous methods that only support box-level\nfocusing and rely on training additional networks, our work\nachieves more fine-grained mask-level region focusing and\nmerely uses the CLIP model.\nCLIP in 2D image variation.\nCLIP image encoder is\nwidely used in 2D image variation (e.g., DALLE-2 [44],\nDiffusers [58] and IP-Adapter [68]) to achieve better quality\nor controllability. As for subject-driven image variation pi-\noneered by DreamBooth [47], extraction of pure single ob-\nject feature from the whole image is more important as the\nfollowing method ELITE [60] proposes to use feature-level\nmasking to eliminate background information to generate\nbetter subjects. Similarly, BLIP-Diffusion [27] uses text to\nextract the most relevant object features. All these subject-\ndriven image variation methods require the image to have a\nsingle foreground object in the center of the image and can-\nnot achieve variation by focusing on user-specified objects\nin more complex images while maintaining original context\ninformation. Such limitations highlight the importance of\nour Alpha-CLIP that enables subject-driven generation in\ncomplex scenes and achieves user-defined region focusing\nin image variation tasks.\nCLIP in 3D generation. Some existing 3D object genera-\ntion methods involve CLIP [43] model. In diffusion based\n3D generation, Point-E [39] uses the point cloud diffusion\nmodel to generate the point cloud directly conditioned by\nthe CLIP feature from a single view image or text. Another\napproach in the field of text-to-3D is pioneered by Dream\nFields [20], which uses the CLIP model to provide super-\nvision loss. Following works include PureCLIPNeRF [25],\nCLIP-Mesh [38], CLIP-Forge [48] and Dream3D [63] also\nuse CLIP image encoder to extract rendered image features.\nOur Alpha-CLIP can enhance CLIP in 3D object genera-\ntion, enable Point-E with user-defined region focus abil-\nity and help optimization based text-to-3D models to yield\nhigh-quality generation results.\n3. Method\nThis section describes the data pipeline and framework of\nAlpha-CLIP. As illustrated in Fig. 3, we first design a data\npipeline to generate RGBA-region text pairs data (Sec. 3.1).\nUsing our generated data, we then train our Alpha-CLIP\nwith additional Alpha-channel inputs (Sec. 3.2).\n3.1. RGBA Region-Text Pair Generation\nTo fine-tune the CLIP model with an additional alpha chan-\nnel input, we first design a data generation pipeline (cf.\nFig. 3a) to create millions of RGBA-region text pairs. Our\nd pipeline consists of the following two components.\nGrounding data pipeline. As depicted in the upper part\nof Fig. 3a, this branch is dedicated to generating region-\ntext pairs, which include natural images with foreground al-\npha channels and corresponding referring expressions for\nspecific regions. The natural images are from the GRIT\ndataset [41], which employs GLIP and CLIP to automati-\ncally extract labels of box region-text pairs. Building upon\nGRIT, we take a further step of generating mask region-text\npairs. Specifically, we use SAM [22] to automatically gen-\nerate high-equality pseudo-masks for each box region.\nClassification data pipeline.\nAs illustrated in the lower\npart of Fig. 3a, this branch is utilized for generating region-\ntext pairs where the foreground objects are highlighted\nwhile the original background is removed. We employ the\nImageNet [8] dataset for this purpose. Firstly, we use SAM\nto automatically generate several masks for each image in\nImageNet. Subsequently, we crop the foreground object of\neach mask, center it, and enlarge it. CLIP is then used to\n4\ncalculate scores with the corresponding class label of the\nimage to which each mask belongs.\nFollowing this, we\nsort the masks by class based on their scores and select the\ntop-ranked masks with the highest scores. Regarding the\ntext component, to ensure that the caption for each mask is\nnot merely the ImageNet [8] class label, we place the fore-\nground object on a pure white background. Then we use\nBLIP-2 [28] to annotate these masks with captions. Finally,\nwe merge the fine-grained ImageNet class label with the\nimage-specific captions generated by BLIP-2 [28], result-\ning in millions of RGBA region-text pairs.\n3.2. Alpha-CLIP\nModel structure.\nOur Alpha-CLIP implements subtle\nstructural modifications to the CLIP image encoder to pre-\nserve CLIP\u2019s prior knowledge. In the CLIP image encoder\u2019s\nViT [11] structure, an RGB convolution is applied to the\nimage in the first layer. As shown in Fig. 3b, we introduce\nan additional Alpha Conv layer parallel to the RGB Conv\nlayer, which enables the CLIP image encoder to accept an\nextra alpha channel as input. The alpha channel input is set\nto range from [0, 1], where 1 represents the foreground and\n0 indicates the background. We initialize the Alpha Conv\nkernel weights to zero, ensuring that the initial Alpha-CLIP\nignores the alpha channel as input.\nTraining method. During training, we keep the CLIP text\nencoder fixed and entirely train the Alpha-CLIP image en-\ncoder. Compared to the first convolution layer that pro-\ncesses the alpha channel input, we apply a lower learn-\ning rate to the subsequent transformer blocks. To preserve\nCLIP\u2019s global recognition capability for full images, we\nadopt a specific data sampling strategy during training. We\nset the sample ratio, denoted as rs = 0.1 to occasionally\nreplace our generated RGBA-text pairs with the original\nimage-text pairs and set the alpha channel to full 1. Please\nrefer to Appendix A.1 for ablation studies such as the num-\nber of unfreeze Transformer blocks and value of rs.\nAlpha-CLIP for downstream tasks. After the training,\nAlpha-CLIP possesses the capability to focus on a speci-\nfied region and controlled editing. Alpha-CLIP can enhance\nCLIP\u2019s performance on various baselines in a plug-and-play\nfashion, across various downstream tasks like recognition,\nMLLM, and 2D/3D generation (see Tab. 1 in Sec. 1).\n4. Experiments\nData. We train Alpha-CLIP on RGBA region-text pairs us-\ning grounding data pipeline from GRIT-20m [41] for zero-\nshot ImageNet classification.\nWe combine it with 460k\nRGBA region-text pair from ImageNet [8] using classifi-\ncation data pipeline to train Alpha-CLIP for other tasks in-\ncluding REC, OVD, region-level captioning, 2D image vari-\nation, and 3D generation. Ablation on data volume and mix-\nMethods\nViT-B/16\nViT-L/14\nTop-1\nTop-5\nTop-1\nTop-5\nOriginal CLIP [43]\n66.48\n88.90\n73.48\n91.60\nMaskAdaptedCLIP [31]\n57.86\n79.12\n63.50\n86.34\nRed Circle [52]\n65.37\n88.68\n73.37\n92.09\nMaskCLIP* [75]\n67.86\n89.40\n77.04\n93.39\nAlpha-CLIP(ours)\n68.89\n90.51\n77.41\n94.45\nTable 2. Zero-shot classification on ImageNet-S [12]. When\ngiven the foreground object on the alpha channel, our Alpha-CLIP\nsignificantly improves zero-shot classification and surpasses pre-\nvious baselines such as MaskCLIP [75].\nModel\nAlpha Map\nTop-1\nTop-5\nCLIP [43]\n-\n73.48\n91.60\nAlpha-CLIP\nwhole image\n73.37\n91.75\nrectangular box\n75.62\n93.34\nmask\n77.41\n94.45\nTable 3. Zero-shot classification on ImageNet-S [12] with dif-\nferent alpha map levels. Alpha-CLIP is comparable to the orig-\ninal CLIP when the foreground mask is not available, and further\nboosts the performance with rectangular box or mask alpha maps.\nture of data are in Appendices A.2 and C\n4.1. Alpha-CLIP in Image Recognition\nZero-shot image classification. We select the ImageNet-\nS [12] dataset for zero-shot classification analysis, which\ncomprises 919 classes with semantic segmentation annota-\ntions selected from ImageNet-1k. We prepare the image-\nlevel semantic segmentation masks as the alpha chan-\nnel input. We select representative baseline methods de-\nsigned for making CLIP focus on the specific region:\nMaskCLIP [75], MaskAdaptedCLIP [31], and Red Cir-\ncle [52]. Note that MaskCLIP is designed for mask gen-\neration rather than recognition. We make necessary mod-\nifications to MaskCLIP to adapt it for the recognition task\n(please refer to Appendix B for our implementation details).\nWe use the mean of per-class accuracy as the evaluation\nmetric.\nTab. 2 presents the zero-shot classification comparison\non ImageNet-S validation set. This experiment effectively\ndemonstrates that when provided with a foreground object\nmask through the alpha channel, our Alpha-CLIP generates\nvisual features that are more focused on the foreground ob-\nject, leading to better image-level classification compared\nto the original CLIP and other baseline approaches. It is\nworth noticing that Although MaskCLIP [75] achieves good\nresults without needing to fine-tune the CLIP model, it is\nnot directly compatible with methods that require the whole\nfeature map instead of just the [CLS] token. This limita-\ntion is particularly relevant when considering methods like\nBLIP-2[28], BLIP-Diffusion [27], LLaVA [34] and Point-\nE [39]. In contrast, our Alpha-CLIP is more general and\ncan be applied to these approaches effectively.\nWe also evaluate Alpha-CLIP in scenarios where the\n5\nMethod\nRefCOCO\nRefCOCO+\nRefCOCOg\nVal\nTestA\nTestB\nVal\nTestA\nTestB\nVal\nTest\nCPT [67]\n32.2\n36.1\n30.3\n31.9\n35.2\n28.8\n36.7\n36.5\nReCLIP [54]\n45.8\n46.1\n47.1\n47.9\n50.1\n45.1\n59.3\n59.0\nRed Circle [52]\n49.8\n58.6\n39.9\n55.3\n63.9\n45.4\n59.4\n58.9\nAlpha-CLIP\n55.7\n61.1\n50.3\n55.6\n62.7\n46.4\n61.2\n62.0\nTable 4. Comparison with state-of-the-art on zero-shot REC.\nWe report top-1 accuracy (%). Replacing CLIP in ReCLILP [54]\nwith Alpha-CLIP outperforms other zero-shot approaches on most\ndatasets, including Red Circle[52], ReCLIP[54] and CPT[67].\nforeground mask is unavailable. As shown in Tab. 3, when\nforeground prior is not available, we set alpha channel in-\nput to all one. We observe that the recognition ability of\nAlpha-CLIP (second row) remains on par with the original\nCLIP (top row). When provided foreground box (third row)\nor foreground mask (bottom row), Alpha-CLIP can signifi-\ncantly improve classification accuracy.\nZero-shot referring expression comprehension. In ad-\ndition to the zero-shot image classification task, we also\nconducted experiments on zero-shot Referring Expression\nComprehension (REC). zero-shot REC is the task of localiz-\ning objects in an image given a textual reference expression\nin a zero-shot manner. We follow previous works to select\nthe RefCOCO [69], RefCOCO+ [69], and RefCOCOg [36]\ndatasets for evaluation. We select three representative ap-\nproaches CPT [67], ReCLIP [54], and Red-Circle [52] as\nour baselines. We replace the CLIP model in this task with\nour Alpha-CLIP. Specifically, we use object proposals pre-\ndicted by a pretrained detector [70] and employ SAM to\nobtain masks for each proposal. Instead of cropping the\nobject by bounding box, we input the original image with\nan alpha map into our Alpha-CLIP. This modification has\nproven beneficial in preserving global contextual informa-\ntion as we find cropping only lead to worse result. Please\nrefer to Appendix E for more implementation details.\nAs shown in Tab. 4, Alpha-CLIP achieves competi-\ntive zero-shot results on the REC task, surpassing ReCLIP\nand RedCircle by an average of 6.8% and 3.0% accu-\nracy across RefCOCO, RefCOCO+ and RefCOCOg bench-\nmarks. The experimental results demonstrate that Alpha-\nCLIP enhances CLIP\u2019s ability to focus on the relevant re-\ngion and such enhancement is also beneficial for the REC\ntask that requires image-text understanding and reasoning\ncapabilities.\nOpen vocabulary detection. The Open-Vocabulary Detec-\ntion (OVD) task aims to detect novel classes that are not\navailable during training. Detic [76] is a pseudo-labeling\nbaseline that proposes to use the ImageNet dataset for\nOVD. Specifically, Detic first trains the detector on the base\nclasses of LVIS [14], then uses the detector to generate\npseudo bounding boxes on ImageNet. These pseudo boxes\nmay cover the novel objects and help improve the detec-\ntor\u2019s performance in novel classes. Such a semi-supervised\npipeline is not data-efficient and Detic uses 1.2M images\nDataset\nmAPnovel\nmAP\nDetic-ImageNet\n24.6\n32.4\nMaskImageNet (ori CLIP)\n27.9\n32.5\nMaskImageNet (Alpha-CLIP)\n28.6\n32.9\nTable 5.\nOpen-vocabulary detection on OV-LVIS [14].\nUs-\ning MaskImageNet and our Alpha-CLIP can significantly improve\nmAPnovel on novel classes.\nfrom ImageNet in the OV-LVIS [14] benchmark.\nTo demonstrate the effectiveness of Alpha-CLIP on\nOVD, we transfer the top-ranked ImageNet (460K) into a\ncollection, dubbed as MaskImageNet. Specifically, we ap-\nply our data generation pipeline, as detailed in Sec. 3.1\nto generate pseudo-labeled bounding boxes and foreground\nmasks for each image.\nWe replace the ImageNet used\nin Detic\u2019s pseudo-labeling steps with our MaskImageNet.\nWe also remove the background category loss and adjust\nthe blending ratios of LVIS and MaskImageNet. Exper-\nimental results are presented in\nTab. 5.\nCompared to\nthe Detic baseline using ImageNet (top row), The second\nrow demonstrates that using our MaskImageNet already en-\nhances OVD capabilities.\nFurthermore, our Alpha-CLIP\n(bottom row) further improves OVD performance. Remark-\nably, our method (460K in MaskImageNet) is more data ef-\nficient than Detic (1.2M in ImageNet).\n4.2. Alpha-CLIP in MLLM\nWe replace CLIP used in BLIP-2 [28] and LLaVA-1.5 [33]\nwith our Alpha-CLIP to make MLLM directly focus on\nuser-defined region in vision-language tasks such as region\nlevel captioning and VQA.\nRegion level captioning. As shown in Fig. 4, simply re-\nplacing CLIP with Alpha-CLIP enables MLLM to generate\ncaptions more focused on user-defined areas. In the third\nrow cases about the telephone and mushroom, the original\nCLIP generates the wrong caption. This error may arise\ndue to the CLIP vision feature mixing different objects and\ntheir properties in images with too many foreground ob-\njects. Alpha-CLIP guides MLLM to generate the correct\ncaption by providing the area to focus on. More visualiza-\ntion are in Appendices G.1 and G.2. We also visualize the\nCLIP attention map marked in the upper right to confirm our\nfindings. More visualizations and implementation details of\nthe attention map are in Appendix G.5.\nBesides qualitative results, we also provide the quan-\ntitative region level captioning results of Alpha-CLIP\nwith LLaVA-1.5 [33] on Visual Genome [23] and Ref-\nCOCOg [36]. We fine-tune Alpha-CLIP+LLaVA-1.5 [33]\nwith vicuna-7b [57] on these datasets with the same set-\nting in [45, 72] and task prompts in [72] is adopted. Alpha-\nCLIP image encoder is kept frozen with LLM fine-tuned\nto adapt region caption format.\nResults are shown in\nTab. 6. Alpha-CLIP+LLaVA-1.5 achieves competitive re-\nsults over the baseline methods, even surpassing previous\n6\nA knife on a cutting board\nA bowl of sause \non a cutting board\nSliced cucumbers\n on a cutting board\nA knife on a cutting board\nA red mushroom on a table\nA red telephone on a table\nA boat is docked next to a pier\nA white tower \non a pier next to the water\nA group of street lights \nnext to a body of water\nA green telephone on a table\nWhat is in the bowl?\nThe bowl contains\n apples and eggs.\nWhat is in the bowl?\nThe bowl \ncontains flour.\nWhat is in the container?\nThe container is filled with\nmeat, specifically chicken.\nThe image features a red sculpture of a \ncreature, possibly a dragon or a \nmonster, with a pair of red \nglasses on top of its head. \nThe glasses are placed in \nsuch a way that they appear \nto be part of the creature's \nhead, giving it a unique and eye-\ncatching appearance. The sculpture is \npositioned on a table, drawing attention \nto its vibrant colors and interesting \ndesign.\nThe image features a unique and \ncolorful sculpture of a creature, \npossibly a dragon or a dinosaur, with \na long neck and a fire-like pattern \non its body. The sculpture is standing \non a white surface, which could be a \ntable or a wall. The creature's \nneck is elongated ,  a n d  i t \nappears to be wearing glasses, adding \na whimsical touch to the artwork. The \noverall design of the sculpture is \nvisually striking and captivating, \nmaking it an interesting piece of art.\nThe image features a unique and \ncreative sculpture of a fire-breathing \ndragon or a creature with flames \ncoming out of its head. The \nsculpture is made of a combination of \nred and black colors, giving it a striking \nappearance. The dragon is standing on \na table, and it appears to be a part of a \nlarger artistic display. The sculpture's \nintricate design and the use of vibrant \ncolors make it an eye-catching piece of \nart.\nAlpha-CLIP in LLaVA\nAlpha-CLIP in BLIP-2\nFigure 4. Some results of Alpha-CLIP used in MLLMs The up-\nper half is image captioning result with BLIP-2 [28]. The first col-\numn is the original CLIP generated captions. Other columns repre-\nsent the outcomes of Alpha-CLIP with highlighted region marked\nin red. The lower half is region focused VQA and image caption-\ning result with LLaVA-1.5 [33].\nexpert models like GPT4ROI [72] and GLaMM [45] with\nROI Align [15] or additional region encoder structure pre-\ntrained on a large volume of region-text pairs.\nModel\nRefCOCOg\nVisual Genome\nMETEOR\nCIDEr\nMETEOR\nCIDEr\nGRIT [61]\n15.2\n71.6\n17.1\n142.0\nKosmos-2 [41]\n14.1\n62.3\n-\n-\nGPT4RoI [72]\n-\n-\n17.4\n145.2\nGLaMM [45]\n16.2\n105.0\n18.6\n157.8\nAlpha-CLIP+LLaVA [33]\n16.7\n109.2\n18.9\n160.3\nTable 6. Performance of Alpha-CLIP in region level caption-\ning. We report METEOR and CIDEr metrics on Visual Genome\nand refCOCOg Datasets.\nRegion based VQA. MLLM can chat with users with sim-\nple reasoning. In this scenario, alpha channel input can act\nas the visual prompt defined by the user to highlight specific\nregions of interest. As shown in Fig. 4 and Fig. 1, the user\ncan simply use stroke to tell MLLM the referring object or\nregions to focus on. More visualization results of VQA with\nAlpha-CLIP are in Appendix G.2.\n4.3. Alpha-CLIP in image variation.\nAlpha-CLIP can be used in most image variation models\nthat use CLIP image encoder [27, 44, 58, 60, 68].\nFor\nexample, BLIP-Diffusion bridges CLIP [43] and stable-\ndiffusion [46] with Q-former to generate and edit 2D im-\nages controlled by text.\nSince BLIP-Diffusion [27] is a\ntypical method that maintains subject information, we use\nBLIP-Diffusion to demonstrate the effectiveness of Alpha-\nCLIP. By introducing Alpha-CLIP, we can add an addi-\ntional set of vision prompts to allow the model to focus on\nspecified regions for 2D generation. We replace the ViT-\nL/14 model in BLIP-Diffusion [27] with Alpha-CLIP while\nkeeping the other parts unchanged. We set the empty text\nprompt to make results irrelevant with semantics. As shown\nin Fig. 1, Alpha-CLIP with alpha map on highlighted ar-\neas enables BLIP-Diffusion to generate region-focused re-\nsults. We also compare our Alpha-CLIP with other CLIP\nregion-focused approaches such as image cropping, pixel-\nlevel image masking, red circle, and feature-level masking\n(Please refer to Appendix D for implementation details). As\nshown in Fig. 2, image cropping can not solve the occlu-\nsion problem. The red-circle solution will change the im-\nage content. Neither pixel-level nor feature-level masking\ncan convey original background information. In contrast,\nour Alpha-CLIP that prompts CLIP with fine-grained re-\ngion mask solves the above problems and generates cleaner\nresults while maintaining original background information.\nMore visualizations are in Appendix G.3\n4.4. Alpha-CLIP in 3D Object Generation.\nAlpha-CLIP can also apply to 3D Object Generation. We\ntest it in two different approaches: 1) Point-E [39] that is a\n7\nA Baroque church with ornate reliefs on the walls, \nsoaring vaults and domes, and rich gold decorations throughout.\nA beautifully carved square fountain with an ornate statue standing in the center.\nA porcelain plate displays juicy meat, broccoli and brown toast.\nAlpha CLIP\nBackAug\nAlpha CLIP\nBackAug\nAlpha CLIP\nBackAug\nAlpha CLIP\nBackAug\nAlpha-CLIP in PureCLIPNeRF\nAlpha-CLIP in Point\u00b7E\nFigure 5. Results of Alpha-CLIP in 3D generation. The top part\nshows 3D point clouds generation using Point\u00b7E [39]. The first\nrow displays objects generated by the original CLIP. The second\nrow illustrates the results of Alpha-CLIP, with highlighted areas in\nred. The bottom part shows 3D objects generated by PureCLIPN-\neRF [25]. The CLIP model is replaced with Alpha-CLIP, and tests\nare conducted with and without background augmentation.\ndiffusion-based method for image-to-3D, and 2) PureCLIP-\nNeRF [25] that is an optimization-based approach for text-\nto-3D.\nAlpha-CLIP in Point-E. Point-E [39] can achieve image-\nto-3D through conditioning diffusion model with CLIP im-\nage feature. We replace the CLIP ViT-L/14 image encoder\nof the Point-E base-40M model with our Alpha-CLIP. We\ndemonstrate that Alpha-CLIP is helpful in two cases: 1)\nWhen Point-E generates the point cloud with some parts\nmissing, users can highlight the missing part in the condi-\ntion image to remind the diffusion model to pay more atten-\ntion to that part and fix this missing parts problem. 2) Users\ncan highlight the part that needs to be emphasized on the 2D\nimage. Point-E will spend more points on the highlighted\npart (with 1024 points in total in the base model). The re-\nsults are shown in Fig. 5 with more results in Appendix G.4.\nAlpha-CLIP in PureCLIPNeRF. We input the rendered\nimages with alpha channels obtained from density integra-\ntion of NeRF [37] into Alpha-CLIP. When optimizing the\nobject with Alpha-CLIP, the gradient can flow back from\nthe alpha channel to help generate better results. As shown\nin Fig. 5, we find that PureCLIPNeRF generates objects that\nclosely align with the provided textual prompts(especially\nbolded text) in terms of shape and color when replacing\nCLIP with Alpha-CLIP. Furthermore, there is an enhance-\nment in the overall coherence of the generated objects, cou-\npled with notable aesthetic qualities. We attribute this phe-\nnomenon to Alpha-CLIP\u2019s enhanced capability in optimiz-\ning density parameters of 3D representations directly and\nfocusing only on the foreground area, which helps to gener-\nate an object that is more coherent and closely matches the\ninput text.\nBackground augmentation in PureCLIPNeRF [25] inher-\nited from Dream Fields [20] is a vital step to improve the\nconsistency of objects, making them less diffuse compared\nto the first column in Fig. 5. However, this process is time-\nconsuming as each augmented image has to go through\nCLIP to get optimization direction. We thus test the capa-\nbilities of Alpha-CLIP without background augmentations.\nResults are presented in the second column of Fig. 5. We\nobserve that in most cases, using Alpha-CLIP without back-\nground augmentation produces objects that are clearer and\nbetter aligned with the given text than the original CLIP\nwith 2x faster speed. Quantitative results and More visu-\nalizations are in Appendices F and G.4\n5. Limitation and Future Direction\nWhile Alpha-CLIP demonstrates effective performance in\nvarious scenarios requiring region focus, its current struc-\nture and training process limit its capability to focus on\nmultiple objects or model relationships between different\nobjects. Furthermore, the current training methodology re-\nstricts the alpha channel from generalizing beyond interme-\ndiate values, apart from the binary values of 0 and 1. As\na result, users are unable to specify the amplitude of atten-\ntion. Another limitation both lays in our Alpha-CLIP and\noriginal CLIP is low resolution, which hinder the way for\nAlpha-CLIP to recognize small object. We plan to address\nthese limitations in future work and expand the CLIP input\nresolution. We believe these future directions are pathways\nto augment Alpha-CLIP\u2019s abilities and broaden its utility\nacross diverse downstream tasks.\n6. Conclusion\nIn this work, We propose the Alpha-CLIP model, which\nintroduces an additional alpha channel to specify the re-\ngions of interest. Trained on millions of RGBA region-text\npairs, Alpha-CLIP not only exhibits excellent region-focus\ncapabilities but also ensures its output space remains con-\nsistent with the original CLIP model. This consistency al-\nlows seamless replacement in various downstream applica-\ntions of CLIP. We demonstrate that when prompted with\nspecific regions of interest, Alpha-CLIP shows improved\n8\nzero-shot recognition abilities and verifies its usefulness in\nmany downstream tasks. The applications of CLIP extend\nfar beyond the scope of this article. We hope that Alpha-\nCLIP will be applicable in more scenarios when foreground\nregions or masks are available.\nA. Training Detail\nA.1. Hyperparameter\nBasic hyperparameters. The training process utilizes a\nbatch-size of 4096 for all scales of CLIP models. We use\n8 A100-80G GPUs for ViT-B/16, 64 GPUs for ViT-L/14,\nand 128 GPUs for ViT-L/14@336px. The training process\nutilizes mixed-precision float16 for acceleration. The tem-\nperature coefficient \u03c4 for CLIP is fixed to the value obtained\nafter the completion of the original CLIP training. The op-\ntimizer chosen is AdamW [35] with a weight decay of 2e-2.\nRegarding learning rates, the learning rate for the convolu-\ntional kernels accepting alpha channel input is set to 2e-4,\nwhile the rest of the layers have a learning rate of 2e-6, em-\nploying a cosine learning rate scheduler. For GRIT-1m, the\ntraining lasts 6-8 epochs, whereas GRIT-20m is trained for\n2 epochs.\nWhole image sample ratio. Due to our desire to preserve\nthe original CLIP\u2019s recognition ability for the entire image,\nin the training of Alpha-CLIP on GRIT [41], we sample\na portion of RGBA-text pairs and set the alpha channel to\nall 1(indicating region over the entire image). The text is\nreplaced with the original full-image caption. We use the\nViT-B/16 model and train on GRIT for 4 epochs, varying\nthe sampling ratio. Zero-shot classification on Imagenet-S\nis used as the evaluation metric. The experimental results,\nas shown in Tab. 7, indicate that training without sampling a\nproportion of the entire image-text pair performs worse than\nchoosing a small number of images that require full-image\nattention. However, excessively emphasizing full-image at-\ntention during training significantly impairs the model\u2019s ca-\npability. Based on these results, a sample ratio of 0.1 is\nchosen for the experiments in the main text.\nWhole image perception setting. In accordance with the\ndefinition of transparency in the 2D image, setting alpha to\nsample ratio rs\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ntop1-Acc\n68.06\n68.25\n67.87\n67.71\n67.83\n67.74\n67.37\n66.87\n66.39\n64.94\n63.96\nTable 7. Sample ratio search experiment. We search sample ratio with a step of 0.1. Test metric is zero-shot classification top1 accuracy\non ImageNet-S [12]. As we find rs = 0.1 produce best result.\nunfreeze block nums\n0\n2\n4\n6\n8\n10\n12\nfull-tuning on ori CLIP\ntop1-Acc\n63.61\n64.73\n65.63\n66.59\n67.09\n68.07\n68.27\n66.52(+0.04)\nTable 8. Number of unfreeze block search experiment. We search number of learnable Transformer block number. Test metric is zero-\nshot classification top1 accuracy on ImageNet-S [12]. As we find that unfreeze the whole CLIP image encoder generate the best result.\ntest with gt mask\nall 0\nall 1\nori clip\nwhole 1\n77.41\n72.53\n73.37\n73.48\nwhole 0\n74.99\n73.45\n73.27\nTable 9. Different strategy for whole image perception exper-\niment. Test metric is zero-shot classification top1 accuracy on\nImageNet-S [12]\nall 1 is indicative of the requirement for CLIP to focus on\nthe entire image. However, due to the absence of bias in\nthe first layer convolution of CLIP, which consists only of\nweights, an input with an alpha channel of all 0 maintains\nCLIP\u2019s original state, contrary to the definition of image\ntransparency. To determine an optimal approach, we con-\nducted training and testing under different configurations.\nDuring training, we utilized image-text pairs with the en-\ntire image set to all 0 and all 1, each with a sample ratio\nof 0.1. During testing, we cross-validated the classification\naccuracy with alpha channels set to all 0 and all 1. Ex-\nperiment results are presented in Tab. 9. Our observations\nindicate that configuring the training and inference with the\nalpha channel set to all 1 achieves the best perception per-\nformance. Therefore, we consistently adopt this configura-\ntion in the main section of the paper, utilizing an all-1 alpha\ninput when Alpha-CLIP is required to focus on the entire\nimage.\nUnfreeze block number.\nWe search through the num-\nber of layers to unfreeze when training Alpha-CLIP on\nRGBA-text pairs that create the best result. we test on ViT-\nB/16(with 12 attention blocks) and train on GRIT-1m [41]\nfor 4 epochs. We select the zero-shot classification top1 ac-\ncuracy on ImageNet-S [12] as our test metric. The result is\nshown in Tab. 8. We also test full fintuning of original CLIP\nwithout alpha channel on GRIT [41] dataset, which only\ngets negligible improvement, proving that the improvement\ncontributes to the input focus region through alpha channel\ninstead of training data.\n9\n73.26\n74.80\n76.57\n76.95\n77.21\n66.56\n67.11\n68.28\n68.57\n68.61\n\u56fe\u8868\u6807\u9898\nViT-L14\ntop1 Acc\n103\n104\n105\n106\n107\n# RGBA region-text pairs\n108\n68\n70\n72\n74\n76\n78\nViT-B/16\nViT-L/14\nFigure 6. Zero-shot ImageNet-S [12] classification accuracy\nw.r.t training data volume. two different scale ViT model of\nCLIP [43] are tested.\nWe unfreeze the number of transformer blocks from 0 to\n12 (full) with a step of 2. Results show that as the number\nof unfrozen blocks increases, the classification accuracy in-\ncreases steadily. We also test LoRA [17], but it does not\nwork well compared with full model fintuning. So we con-\nsider full model tuning in the main part of the paper.\nA.2. Ablation on Data Volume\nWe examine the efficacy of data volume in enhancing the\ntraining of robust models through an ablation study. Our\nablation involved the training of ViT-B/16 and ViT-L/14 us-\ning RGBA region-text pairs, with data quantities ranging\nfrom 1k to 10M. We use the zero-shot top-1 accuracy on\nImageNet-S as our evaluation criterion. As illustrated in\nFig. 6, increasing data volume corresponds to a concurrent\nimprovement in the model\u2019s classification accuracy. No-\ntably, larger ViT models exhibited a more substantial per-\nformance boost in comparison to their smaller counterparts\nthroughout this process.\nB. Different implementation of MaskCLIP\nTo the best of our knowledge, two methods propose to use\nthe attention mask to guide the CLIP visual model to pay\nmore attention to the foreground area. We test these two\nmethod respectively. Because these two methods can only\ndo masking at the feature level as [H, W] ([14, 14] for ViT-\nB/16, [16, 16] for ViT-L/14), we first do max pooling on\nthe binary mask M to make it match the size of the feature\nmodel\nViT-B/16\nViT-L/14\nOriginal CLIP\ntop1\n66.48\n73.48\ntop5\n88.90\n91.60\nMaskCLIP [9]\ntop1\n53.61\n65.28\ntop5\n76.47\n87.25\nAlpha-CLIP\ntop1\n68.89\n77.41\ntop5\n90.51\n94.45\nTable 10. Zero-shot classification on ImageNet-S[12]. Compar-\nison using method proposed in [9].\nmap.\nm = MaxPooling(M)\n(1)\nMask Area guided last attention. As [75] proposes using\n1\u00d71 conv layer to get feature space level 2D semantic clas-\nsification map, we use the same idea of the attention mask\nto set [cls] token only to calculate attention with fore-\nground area patches. In other words, we use m to guide the\nlast attention calculation. We report this result in the main\npart of this paper as it shows better results than the original\nCLIP.\nRelative Mask Attention. This method is proposed in [9],\nand is used in [62], which introduces \u201dMask Patch Tokens\u201d\nto do the same attention as [CLS] token but only attach\nto those patches that contain foreground area. We test this\nmethod but it does not produce good results on ImageNet-\nS [12] as shown in Tab. 10 as it is used in the segmentation\ntask to classify each semantic mask(area) of a whole image,\nbut ImageNet-S [12] only cares about a single prominent\nforeground object in most cases. So we do not report this\nresult in the main part of this paper.\nC. Effectiveness of Classification Data\nWhile Grounding data holds more promising prospects in\nthe future, especially with the advent of more powerful\ngrounding and segmentation models, we demonstrate that,\nat the current stage, leveraging large-scale manually anno-\ntated classification datasets like ImageNet [8] and construct-\ning RGBA region-text pairs using the pipeline shown in\nFig. 3 still significantly benefits Alpha-CLIP in achieving\nenhanced Region-Perception capabilities.\nC.1. Zero-shot Classification on COCO\nIn addition to natural image classification tests, there are\nscenarios where there is a need to crop or mask objects in\nimages[7] [54] [31]. Therefore, we conducted classification\ntests for Alpha-CLIP in such scenarios using the validation\nset of the Instance-COCO [32] dataset, which consists of\n80 classes. We cropped objects using ground-truth bound-\ning boxes and enlarged them by 1.5 times (referred to as\n10\nmodel\nViT-B/16\nViT-L/14\nmasking\nCLIP\n49.42\n54.43\nAlpha-CLIPg\n49.27\n56.45\nAlpha-CLIPg+c\n53.39\n58.84\nno masking\nCLIP\n64.21\n67.65\nAlpha-CLIPg\n61.57\n67.44\nAlpha-CLIPg+c\n71.08\n77.56\nImageNet-S top1\nCLIP\n66.48\n71.48\nAlpha-CLIPg\n68.89\n77.41\nAlpha-CLIPg+c\n69.40\n77.80\nTable 11. Zero-shot classification results on COCO. Our Alpha-\nCLIP also achieve significant improvement on zero-shot Instance-\nCOCO [32] classification tasks.\nData\nRefCOCO\nRefCOCO+\nRefCOCOg\nVal\nTestA\nTestB\nVal\nTestA\nTestB\nVal\nTest\nGRIT-1M\n56.1\n63.4\n48.9\n55.1\n62.6\n45.1\n60.3\n60.6\nGRIT-1M + IN\n55.7\n61.1\n50.3\n55.6\n62.7\n46.4\n61.2\n62.0\nTable 12. Zero-shot REC results of Alpha-CLIP with different\npretraining data. We compare the results of using only grounding\ndata with adding classification data. We report top-1 accuracy (%).\nCOCO crop). We conduct tests in two scenarios: masking\n(setting the background to a solid color) and no masking\n(using the original background). To prevent results from\nbeing dominated by the most frequent classes, we use the\nmean of per-class accuracy as the evaluation metric. To\nensure that Alpha-CLIP is adapted to images with back-\ngrounds replaced by solid colors, we incorporate object-\ncentric image data (from the lower branch of Fig. 3 into the\ntraining data for this scenario. This data is generated from\nthe top 460k RGBA-region text pairs auto-generated from\nImageNet-21k [8], and we include it in the pairs generated\nfrom GRIT-1M [41] as the training dataset for Alpha-CLIP.\nResults are shown in Tab. 11. We compare it with the base-\nline method trained on GRIT-1m only and find a huge im-\nprovement for cropped image classification. We also test its\nclassification accuracy on ImageNet-S [12], and the result\neven surpasses models trained on GRIT-20m. We contribute\nthis to the human annotation of fine-grained class labels of\nImageNet [8] dataset.\nC.2. Different version Alpha-CLIP in REC\nTo investigate the effectiveness of the classification data on\nREC, we conduct experiments comparing Alpha-CLIP pre-\ntrained solely on the grounding data with a combination\nof classification and grounding data. We use an ensemble\nof ViT-B/16 and ViT-L/14 backbones, with grounding data\nsourced from GRIT-1M [41] and classification data from\nImageNet-21k [8]. As shown in Tab. 12, on the majority of\nbenchmarks, using classification data yields better results\ncompared to models that are not pretrained with it.\nD. Other CLIP masking baselines\nThere are simple ways to make CLIP focus on user-\nspecified regions without modifying the weights of the\nCLIP model.\nWe test two possible approaches here.\nNamely Image-level masking and feature-level masking.\nWe test on these simple baselines and make comparisons\nwith Alpha-CLIP. As shown in Fig. 7. It is worth noticing\nthat we only draw structure using the Q-former proposed by\nBLIP-2. But they can also be adapted to other VL systems\nthat use CLIP image encoder as the visual backbone like\nLLaVA [33, 34] and miniGPT4-v2 [6].\nImage Level Masking means simply masking the back-\nground region by directly setting these regions to pure color.\nWe choose the color same as MaskAdaptedCLIP [31].\nFeature Level Masking is another method that applies\nmasking on the feature level. As shown in Fig. 7, we use\nmax pooling to downsample the image level mask to fea-\nture level coarse-grained mask, and use this mask to do\nelement-wise product to set the features that belong to the\nbackground to zero. This method has been proven useful in\nEllite [60] when the object is in the center of the image and\noccupies a large space.\nResults of the two masking methods are shown at the\ntop of Fig. 7. We use the same settings as in the main sec-\ntion of the paper. BLIP-2 [28], CLIP-L/14+flant5xl is used\nfor captioning, BLIP-Diffusion [27], CLIP-L/14+stable-\ndiffusion [46] is used for Image generation. The first col-\numn represents the original image and the area that needs\nto be focused, the second column represents the results of\nusing image-level masking, the third column represents the\nresults of using feature-level masking, and the fourth col-\numn represents the results of our Alpha-CLIP. The first four\nlines are image captioning results, and the last four lines\nare image variation results. As can be seen from Fig. 7,\nimage-level masking will lose the context information of\nthe object, causing its semantics to be incorrect or blurred\nand cannot produce good results; while feature-level mask-\ning can sometimes produce better results, but rough mask-\ning directly on the feature level may cause unpredictable\nbehaviors, such as generating pictures with completely ir-\nrelevant semantic information, or being dominated by the\nsemantics of the main objects in the picture. In contrast,\nAlpha-CLIP can produce better results because it is pre-\ntrained on millions of RGBA-text pairs. These two feature\nmasking methods also destroy the features of other areas of\nthe image to a greater extent and completely lose informa-\ntion about the other part of the image, and therefore fail in\nsimple reasoning problems that need to involve the relation-\nship between objects and the environment. In the meantime,\nAlpha-CLIP can highlight the region that needs to be fo-\ncused on with features of the remaining areas in the image\nbetter preserved.\n11\nCLIP\nBLIP Qformer\nBLIP (CLIP+Qformer)\nfocus mask\n\u2297\nimage \nlevel \nmasking \nresult\nimage level masking\nfeature level masking\nImage of a shell \non a gray background\nAn image of an octopus \non a gray background\nA black and red file \ncabinet on a table\nA picture of a book \non a gray background\nA man is using \na computer\nA black and \nred microwave\nA small shell \nsitting on the sand\nA small sea \ncreature in a shell \non the sand\nAn image of a \nshell with the word \nperson on it\nA picture of a cat \nin a cage\nA man is working \non a computer\nAn image of a \ncomputer screen \nwith two monitors\nimage level masking result\nfeature level masking result\nAlpha-CLIP result\nFigure 7. Two baselines of image level masking and feature\nlevel masking and their comparison with Alpha-CLIP. It is\nworth noticing that we use BLIP-2 [28] structure with Q-former\nas presented in the baseline pipeline. Alpha-CLIP and these two\nmasking approaches can also adapt to structures that only have the\nprojection layer, like LLaVA [34] and miniGPT-v2 [6]\nSAM\nx 5\nright sandwhich, left half\nAlpha-CLIP\nAlpha Maps\nb1 b2 b3 b4 b5\nCLIP\nExpression\nExpression\n(a) ReCLIP \n(b) Red Circle\n(c) Alpha-CLIP\nCLIP\nFigure 8. Model pipeline of Alpha-CLIP in Zero-shot REC\nTasks.\nWe compared our model (as illustrated in a detailed\nflowchart in the lower part) with two other baselines [52, 54] (rep-\nresented by a concise flowchart in the upper part).\nPP\nRefCOCO\nRefCOCO+\nRefCOCOg\nVal\nTestA\nTestB\nVal\nTestA\nTestB\nVal\nTest\nB\n56.8\n63.7\n49.4\n56.2\n63.6\n45.9\n59.9\n61.7\nB | C\n57.0\n62.8\n50.6\n57.1\n63.7\n48.0\n64.0\n64.1\nB | C | G\n57.0\n63.0\n51.0\n56.9\n64.0\n48.5\n63.6\n64.3\nTable 13. Zero-shot REC results of Alpha-CLIP with different\nimage preprocessing methods. We make comparisons across Re-\nfCOCO, RefCOCO+, and RefCOCOg datasets. PP: Preprocessing\nmethods. \u201cB\u201d denotes original input and blurring operation. \u201cC\u201d\ndenotes cropping. \u201cG\u201d denotes grayscaling.\nOriginal Image\nBlur\nCrop-Padding\nGrayscale\nFigure 9. Visualization of different image preprocessing oper-\nations. In our basic approach, we only utilize the original image\nand blurring. Additionally, we supplement the process with crop-\nping and grayscaling operations.\nE. Zero-shot REC with Alpha-CLIP Imple-\nmentation Details\nIn Fig. 8 (c), we provide a detailed illustration of the\nAlpha-CLIP model pipeline in zero-shot Referring Expres-\nsion Comprehension.\nWe also compare our architecture\nwith ReCLIP [54] and RedCircle [52], highlighting the dif-\nferences and advantages. ReCLIP employs cropping and\nblurring operations to isolate image regions, which are ob-\ntained from box proposals from a detector [70]. However, as\nshown in Fig. 8 (a), cropping results in losing relationship\n12\nMethod\nRes+Iter\nR-Precision\nTime\nPureCLIPNeRF \u2020\n168\u00b2+10k\n85.62\n\u223c34min\n\u03b1-PureCLIPNeRF\n168\u00b2+10k\n88.89\n\u223c36min\nTable 14. Quantitative Results of 3D Generation. We compare\nthe R-Precision of PureCLIPNeRF [25] model using original CLIP\nand Alpha-CLIP, as well as the time cost to generate a single ob-\nject. \u2020 indicates our reimplementation.\ninformation between objects, leading to decreased perfor-\nmance. While RedCircle draws red circles on specific re-\ngions across the entire image to avoid this issue, its prompt\nis still coarse-grained and it alters the original image, as\npresented in Fig. 8 (b). We use SAM [22] to generate fine-\ngrained alpha maps with the aforementioned box proposals.\nWe input both alpha maps and original or blurred images\ninto Alpha-CLIP and calculate similarity with referring ex-\npressions. The remaining steps closely align with [54]. Our\nbasic approach ensures the complete input of images and\nutilizes preprocessing methods as few as possible. It is ef-\nficient and achieves excellent performance across different\nbenchmarks, as demonstrated in Tab. 4.\nIn addition to the preprocessing operations in our basic\napproach (original image and blurring), we further explore\nthe cropping and grayscaling operations depicted in Fig. 8\n(a) and (b). More specifically, for the blurring operation,\nour hyperparameter, namely the standard deviation (\u03c3), is\nset to \u03c3 = 100 [54]. For the cropping operation, we pad the\ncropping box to be a square and fill the background at the\nimage level with zeros (i.e., black color), as shown in Fig. 9.\nWe use an ensemble of ViT-B/16 and ViT-L/14 Alpha-CLIP\nbackbones to test, which are trained on GRIT-20M. The re-\nsults, as shown in Tab. 13, indicate the additional benefits of\nincorporating these operations. This underscores the strong\nadaptability of our model, demonstrating its ability to adapt\nto diverse image inputs.\nF. Quantitative Results of Nerual Field Opti-\nmization based 3D Object Generation\nWe evaluate the quantitative results of Alpha-CLIP in Pure-\nCLIPNeRF, using the same test method proposed in Dream-\nfields [20], which includes 153 text prompts related to the\nCOCO dataset.\nWe measure the generated results using\nCLIP R-Precision.\nUnder the same setting proposed in\n[25], we use the Alpha-CLIP ViT-B/16 model to optimize\nthe generated object and compare our method with the orig-\ninal CLIP. We test R-Precision using CLIP ViT-B/32. The\nresults are presented in Tab. 14, which shows our Alpha-\nCLIP can generate better objects than the original CLIP\nwith negligible extra time consumption(test on V100-32G\nGPU).\nG. More Qualitative Result Visualization\nG.1. Region-focused Image Captioning\nAs described in Sec. 4.2, we replace original CLIP ViT-\nL/14 in BLIP-2 [28] with Alpha-CLIP without any post\nfine-tuning to generate captions. More results are shown\nin Fig. 10\nG.2. Region-focused VQA and Detailed Image Dis-\ncription\nAs described in Sec. 4.2, we replace original CLIP ViT-\nL/14-336px in LLaVA-1.5 [33] without any post fine-\ntuning. Results are shown in Fig. 11\nG.3. Region-focused Image Variation\nAs described in Sec. 4.3, we replace the original CLIP ViT-\nL/14 used in BLIP-Diffusion to make the condition im-\nage feature mainly focus on the user-specified area while\nmaintaining background information. Results are shown in\nFig. 12.\nG.4. 3D Object Generation\nUsing the same setting in Sec. 4.4 Diffusion based object\ngeneration based on Point-E [39] base-40M are shown in\nFig. 13, where Alpha-CLIP can achieve user-defined area\nfocus to rectify missing part or emphasizing specific part.\nNeural field optimization based object generation based on\nPureCLIPNeRF [25] using ViT-B/16 for object optimiza-\ntion is shown in Fig. 14, where Alpha-CLIP generally gen-\nerates better objects than original CLIP with or without\nbackground augmentation.\nG.5. Attention map in Alpha-CLIP\nFollow the spirit of DINO [5], we visualize Alpha-CLIP\nattention maps to check whether Alpha-CLIP pays more\nattention to user-defined highlighted areas at feature grid\nspace.\nWe check the attention map of [CLS] token in\nthe last transformer block in the vision encoder.\nThe\nmodel used for visualization is ViT-L/14 with 16 heads self-\nattention. For a fair comparison, we use the 5th and 16th\nheads attention maps for visualization, as we find that these\ntwo feature maps are most distinguishable among 16 heads.\nResults are shown in Fig. 15. This visualization verifies that\nAlpha-CLIP pays more attention to the area to focus on and\nmore importantly, with no damage to the 2D location in-\nformation preserved in the feature location of the original\nCLIP [43].\n13\na small shell sitting on the sand\na small sea creature in a shell \non the sand\na shell on the sand\na man is using a computer\na black and red microwave\na man is using a computer\na little boy in a \nred and green apron\na child wearing a red apron\na child is playing with a frying pan\na police officer\n holding a machine gun\n on the street\na police officer \nwearing a helmet \nand carrying a gun\na man with a gun\na police officer \nwith a gun on his belt\na white dresser with books on it\na wooden dresser in a room\na white bookcase in a bedroom\na padlock on a wooden door\nclose up of a chain and padlock \non a wooden door\na padlock is attached to \na wooden door\na mannequin on display\na mannequin\nwearing a helmet\na mannequin \nwearing a gas mask\na mannequin\nwearing a fur coat\na man holding a harp\ntwo men standing next to a harp\na man holding a laptop \nin front of a red curtain\na wooden rack \nwith plates and glasses on it\na red tool box with \ntools and accessories\na tool kit with a chainsaw and other tools\na tool kit with a cordless \ndrill and other tools\na black couch \nin a living room\na white plate is \non top of a wicke basket\na black couch \nin a living room\na man wearing\na hat with feathers\na man in a suit and tie\na man with a hat\na man in a bathtub with a mask on\nan old bathtub with a man in it\na man sitting in a bathtub\na cat is sleeping in a box\na cat is sleeping in a box\na pineapple in a box\nthe man is wearing a \nyellow shirt\ntwo men wearing sunglasses while \nplaying brass instruments\ntwo man in yellow shirt\nare playing trombone\nFigure 10. More results of Alpha-CLIP used in BLIP-2 [28]. The first row per three is the original BLIP-Diffusion generated images.\nOther rows represent the outcomes of Alpha-CLIP with highlighted regions marked in red. It is worth noticing that although we use ground\ntruth mask as the highlighted region, Alpha-CLIP can actually take any region as alpha input. Users can easily get high-quality masks with\nthe help of SAM[22].\n14\nThe man is standing at home plate, \nholding a baseball bat, and \npreparing to swing at an incoming \npitch during a baseball game.\nWhat is this man doing in \nthe image?\nThe man is playing baseball, \nspecifically as a catcher, and \nis in the process of catching \na ball during a game.\nWhat is this man doing in \nthe image?\nWhat's the meaning of this \nsign?\nThis sign indicates that there \nis a parking area nearby, \npossibly a designated \nparking zone.\nWhat's the meaning of this \nsign?\nThe sign in the image is a street \nsign that reads \"Gladys.\" This sign \nindicates the name of the street \nwhere the pole is located.\nWhat is in the bottle?\nThe bottle contains beer.\nBut l want to drink water \nnow, what should I do?\nWhere are them exactly?\nThere are two water \nbottles on the table, so \nyou can choose one of \nthem to drink water.\nThe two water bottles are \nlocated on the table, next to \nthe beer bottle.\nThe image features a statue of a \nwoman holding scales, which is \nsituated on top of a building. The \nstatue is positioned in front of a \nlarge American flag, which is \nflying high on a pole. In the \nbackground, there is an airplane \nflying overhead, adding a sense of \nmotion and activity to the scene. \nThe combination of the statue, the \nflag, and the airplane creates a \ndynamic and visually interesting \ncomposition.\nThe image features a large \nclock mounted on the side \nof a building, with a statue of a \nwoman holding scales nearby. \nThe clock is prominently \ndisplayed, and it appears to \nbe a central feature of the \nbuilding. In the background, \nthere is an airplane flying, adding \na sense of motion and activity to \nthe scene. The combination of the \nclock, statue, and airplane creates \nan interesting and dynamic visual \ncomposition.\nThe image depicts a group of \npeople riding in a horse-\ndrawn carriage on a street in \nfront of the U.S. Capitol \nbuilding. The carriage is \nbeing pulled by two horses, \nand there are several people \nvisible in the scene, including \na man riding the carriage \nand others standing nearby.\nThe image features a horse-\ndrawn carriage with a man \nwearing a top hat driving \nthe carriage. There are several \npeople in the scene, including \na man sitting on the carriage, a \nwoman sitting on the back of \nthe carriage, and other \nindividuals standing nearby. \nThe carriage is moving down a \nstreet, and the people appear \nto be enjoying the ride. \nFigure 11. More results of Alpha-CLIP used in LLaVA-1.5 [33]. All cases shown here are made simply by replacing the original CLIP\nof LLaVA-1.5 [33] with a plug-in Alpha-CLIP without further tuning. Alpha-CLIP can achieve region-based VQA and region-focused\ndetailed image descriptions.\n15\nFigure 12. More results of Alpha-CLIP used in BLIP-Diffusion [27]. The first row per three is the original BLIP-Diffusion generated\nimages. Other rows represent the outcomes of Alpha-CLIP with highlighted regions marked in red. It is worth noticing that although we\nuse ground truth mask as the highlighted region, Alpha-CLIP can actually take any region as alpha input. Users can easily get high-quality\nmask with the help of SAM[22].\n16\nRectifying missing part\nEmphasizing specific part\nFigure 13. More results of Alpha-CLIP used in Point-E[39]. In each example, the results in the first row are 3D point clouds generated\nby the original CLIP, while the results in the second row are 3D point clouds generated with highlighted areas in red under the guidance of\nAlpha-CLIP. The left part shows Alpha-CLIP\u2019s ability to rectify missing parts, and the right part shows emphasizing specific areas using\nAlpha-CLIP.\nAn exquisitely carved double-tiered fountain.\nA wooden toy train and a stack of blocks.\nA stainless steel,  stylish thermos flask.\nA silver candlestick  with several burning candles.\nA pink baseball cap with  the letter M embroidered on it.\nTokyo city; trending on artstation.\nA unique officer hat, black with gold trim, adds a sense of authority.\nA black modern submarine with torpedo tubes on top.\nA bronze metal vase decorated with ancient patterns.\nmedieval people celebrating a festival  with many stalls; trending on artstation.\nAlpha CLIP:\nBackAug:\nAlpha CLIP:\nBackAug:\nAlpha CLIP:\nBackAug:\nAlpha CLIP:\nBackAug:\nAlpha CLIP:\nBackAug:\nAlpha CLIP:\nBackAug:\nAlpha CLIP:\nBackAug:\nAlpha CLIP:\nBackAug:\nFigure 14. More results of Alpha-CLIP used in PureCLIPNeRF[25]. In each example, the results in the last two columns are 3D objects\ngenerated with PureCLIPNeRF under the guidance of Alpha-CLIP and the original CLIP, while the results in the first two columns are\nobjects generated by them respectively but without background augmentations.\n17\nFigure 15. Alpha-CLIP Attention map visualization. Last transformer block attention map of [CLS] token with other patch tokens.\neach first line per four is from original CLIP [43] and the other three lines are from Alpha-CLIP with user-defined focus regions marked\nin red. Prompting with region need focusing, Alpha-CLIP will focus on the part accordingly without compromising the original object\nlocation in feature grid.\n18\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736,\n2022. 3\n[2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023.\n[3] Jinze Bai, Rui Men, Hao Yang, Xuancheng Ren, Kai Dang,\nYichang Zhang, Xiaohuan Zhou, Peng Wang, Sinan Tan,\nAn Yang, et al.\nOfasys: A multi-modal multi-task learn-\ning system for building generalist models. arXiv preprint\narXiv:2212.04408, 2022.\n[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2,\n3\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 13\n[6] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\nLiu,\nPengchuan\nZhang,\nRaghuraman\nKrishnamoorthi,\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\nMinigpt-v2: large language model as a unified interface for\nvision-language multi-task learning, 2023. 11, 12\n[7] Keyan Chen, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao,\nJianqi Chen, and Weidi Xie.\nOvarnet:\nTowards open-\nvocabulary object attribute recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 23518\u201323527, 2023. 2, 3, 10\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009. 4, 5, 10, 11\n[9] Zheng Ding, Jie Wang, and Zhuowen Tu. Open-vocabulary\nuniversal image segmentation with maskclip.\nIn Interna-\ntional Conference on Machine Learning, 2022. 3, 10\n[10] Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang,\nDongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang,\nLu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Maskclip:\nMasked self-distillation advances contrastive language-\nimage pretraining. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 10995\u201311005, 2023. 2\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 5\n[12] Shanghua Gao, Zhong-Yu Li, Ming-Hsuan Yang, Ming-\nMing Cheng, Junwei Han, and Philip Torr. Large-scale unsu-\npervised semantic segmentation. IEEE transactions on pat-\ntern analysis and machine intelligence, 2022. 5, 9, 10, 11\n[13] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.\nOpen-vocabulary object detection via vision and language\nknowledge distillation.\nIn International Conference on\nLearning Representations, 2021. 2\n[14] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5356\u20135364, 2019. 6\n[15] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017. 4,\n7\n[16] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratud-\ndin, and Hamid Laga. A comprehensive survey of deep learn-\ning for image captioning. ACM Computing Surveys (CsUR),\n51(6):1\u201336, 2019. 2\n[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 10\n[18] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al.\nLanguage is\nnot all you need: Aligning perception with language mod-\nels. arXiv preprint arXiv:2302.14045, 2023. 2, 3\n[19] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip.\nhttps://github.com/mlfoundations/\nopen_clip, 2021. 2\n[20] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 2, 4, 8, 13\n[21] John R Kender, Parijat Dube, Zhengyang Han, and Bish-\nwaranjan Bhattacharjee. G2l: A high-dimensional geomet-\nric approach for automatic generation of highly accurate\npseudo-labels.\nIn Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 1093\u20131102,\n2023. 2\n[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 2, 3, 4, 13,\n14, 16\n[23] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vision,\n123:32\u201373, 2017. 6\n19\n[24] S Chandeesh Kumar, M Hemalatha, S Badri Narayan, and P\nNandhini. Region driven remote sensing image captioning.\nProcedia Computer Science, 165:32\u201340, 2019. 2\n[25] Han-Hung Lee and Angel X Chang.\nUnderstanding pure\nclip guidance for voxel grid nerf models.\narXiv preprint\narXiv:2209.15172, 2022. 2, 3, 4, 8, 13, 17\n[26] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 2, 3\n[27] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion:\nPre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 2, 3, 4, 5, 7, 11, 16\n[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi.\nBLIP-2: bootstrapping language-image pre-training\nwith frozen image encoders and large language models. In\nInternational Conference on Machine Learning, ICML 2023,\n23-29 July 2023, Honolulu, Hawaii, USA, pages 19730\u2013\n19742. PMLR, 2023. 2, 3, 5, 6, 7, 11, 12, 13, 14\n[29] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10965\u201310975, 2022. 2, 3\n[30] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-\nhofer, and Kaiming He. Scaling language-image pre-training\nvia masking. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n23390\u201323400, 2023. 2\n[31] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan\nZhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana\nMarculescu. Open-vocabulary semantic segmentation with\nmask-adapted clip. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n7061\u20137070, 2023. 2, 3, 5, 10, 11\n[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 10, 11\n[33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 2, 3, 6, 7, 11, 13, 15\n[34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 2, 3, 5, 11, 12\n[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 9\n[36] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy.\nGeneration\nand comprehension of unambiguous object descriptions. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 11\u201320, 2016. 6\n[37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 2,\n8\n[38] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 conference papers, pages 1\u20138, 2022. 2, 4\n[39] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 2, 3, 4, 5, 7, 8, 13, 17\n[40] OpenAI. Gpt-4 technical report, 2023. 3\n[41] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 2, 3, 4, 5, 7, 9, 11\n[42] Yanyuan Qiao, Chaorui Deng, and Qi Wu.\nReferring ex-\npression comprehension: A survey of methods and datasets.\nIEEE Transactions on Multimedia, 23:4426\u20134440, 2020. 2\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 3, 4, 5, 7, 10, 13, 18\n[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2, 3, 4, 7\n[45] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdel-\nrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M.\nAnwer, Erix Xing, Ming-Hsuan Yang, and Fahad S. Khan.\nGlamm: Pixel grounding large multimodal model, 2023. 4,\n6, 7\n[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 7, 11\n[47] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 4\n[48] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,\nChin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-\nshan. Clip-forge: Towards zero-shot text-to-shape genera-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18603\u201318613,\n2022. 4\n[49] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 3\n[50] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\n20\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 3\n[51] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023. 2\n[52] Aleksandar Shtedritski,\nChristian Rupprecht,\nand An-\ndrea Vedaldi.\nWhat does clip know about a red cir-\ncle? visual prompt engineering for vlms.\narXiv preprint\narXiv:2304.06712, 2023. 2, 3, 5, 6, 12\n[53] Wei Su, Peihan Miao, Huanzhang Dou, Yongjian Fu, and\nXi Li. Referring expression comprehension using language\nadaptive inference. arXiv preprint arXiv:2306.04451, 2023.\n2\n[54] Sanjay Subramanian, William Merrill, Trevor Darrell, Matt\nGardner, Sameer Singh, and Anna Rohrbach.\nReclip: A\nstrong zero-shot baseline for referring expression compre-\nhension. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long\nPapers), pages 5198\u20135215, 2022. 2, 3, 6, 10, 12, 13\n[55] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. EVA-CLIP: improved training techniques for CLIP at\nscale. CoRR, abs/2303.15389, 2023. 2\n[56] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multi-\nmodality. arXiv preprint arXiv:2307.05222, 2023. 2\n[57] Vicuna. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. https://vicuna.lmsys.\norg/, 2023. 6\n[58] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\nand Thomas Wolf.\nDiffusers:\nState-of-the-art diffusion\nmodels.\nhttps://github.com/huggingface/\ndiffusers, 2022. 4, 7\n[59] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhen-\nhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu,\nZhiguo Cao, Yushi Chen, Tong Lu, Jifeng Dai, and Yu Qiao.\nThe all-seeing project: Towards panoptic visual recognition\nand understanding of the open world, 2023. 3\n[60] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. ELITE: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 2, 4, 7,\n11\n[61] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan,\nZicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A gen-\nerative region-to-text transformer for object understanding.\narXiv preprint arXiv:2212.00280, 2022. 7\n[62] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2955\u20132966, 2023. 2, 3,\n10\n[63] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot\ntext-to-3d synthesis using 3d shape prior and text-to-image\ndiffusion models.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n20908\u201320918, 2023. 4\n[64] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xi-\nang Bai. Side adapter network for open-vocabulary semantic\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2945\u2013\n2954, 2023. 2, 3\n[65] Xin Xu, Tianyi Xiong, Zheng Ding, and Zhuowen Tu. Masq-\nclip for open-vocabulary universal image segmentation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 887\u2013898, 2023. 2, 3\n[66] Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and\nJian Yang. Fine-grained visual prompting. arXiv preprint\narXiv:2306.04356, 2023. 2, 3\n[67] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-\nSeng Chua, and Maosong Sun. Cpt: Colorful prompt tun-\ning for pre-trained vision-language models. arXiv preprint\narXiv:2109.11797, 2021. 6\n[68] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 2, 4, 7\n[69] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling context in referring expres-\nsions.\nIn Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part II 14, pages 69\u201385. Springer, 2016.\n6\n[70] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu,\nMohit Bansal, and Tamara L Berg. Mattnet: Modular atten-\ntion network for referring expression comprehension. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 1307\u20131315, 2018. 2, 3, 6, 12\n[71] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,\nLinke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang\nZhang, Haodong Duan, Hang Yan, et al.\nInternlm-\nxcomposer: A vision-language large model for advanced\ntext-image comprehension and composition. arXiv preprint\narXiv:2309.15112, 2023. 2, 3\n[72] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-\nstruction tuning large language model on region-of-interest.\narXiv preprint arXiv:2307.03601, 2023. 4, 6, 7\n[73] Shiyu Zhao, Zhixing Zhang, Samuel Schulter, Long Zhao,\nBG Vijay Kumar, Anastasis Stathopoulos, Manmohan Chan-\ndraker, and Dimitris N Metaxas. Exploiting unlabeled data\nwith vision and language models for object detection.\nIn\nEuropean Conference on Computer Vision, pages 159\u2013175.\nSpringer, 2022. 2\n[74] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-\nyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou,\nXiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-\nbased language-image pretraining.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16793\u201316803, 2022. 2, 3\n21\n[75] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free\ndense labels from clip. In European Conference on Com-\nputer Vision, pages 696\u2013712. Springer, 2022. 2, 3, 5, 10\n[76] Xingyi Zhou,\nRohit Girdhar,\nArmand Joulin,\nPhilipp\nKr\u00a8ahenb\u00a8uhl, and Ishan Misra.\nDetecting twenty-thousand\nclasses using image-level supervision. In European Confer-\nence on Computer Vision, pages 350\u2013368. Springer, 2022. 2,\n3, 6\n[77] Yijie Zhou, Likun Cai, Xianhui Cheng, Zhongxue Gan, Xi-\nangyang Xue, and Wenchao Ding. Openannotate3d: Open-\nvocabulary auto-labeling system for multi-modal 3d data.\narXiv preprint arXiv:2310.13398, 2023. 2\n22\n"
  },
  {
    "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator",
    "link": "https://arxiv.org/pdf/2312.04474.pdf",
    "upvote": "26",
    "text": "2023-12-11\nChain of Code: Reasoning with\na Language Model-Augmented Code Emulator\nChengshu Li\u2217,1,2, Jacky Liang1, Andy Zeng1, Xinyun Chen1, Karol Hausman1,2, Dorsa Sadigh1,2,\nSergey Levine1,3, Li Fei-Fei2, Fei Xia\u2020,1 and Brian Ichter\u2020,1\n1Google DeepMind, 2Stanford University, 3University of California, Berkeley\nhttps://chain-of-code.github.io\nCode provides a general syntactic structure to build complex programs and perform precise computations\nwhen paired with a code interpreter \u2013 we hypothesize that language models (LMs) can leverage code-\nwriting to improve Chain of Thought reasoning not only for logic and arithmetic tasks [1, 5, 26], but also\nfor semantic ones (and in particular, those that are a mix of both). For example, consider prompting an\nLM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to\nwrite an implementation for \u201cdetect_sarcasm(string)\u201d that can be executed by the interpreter (handling\nthe edge cases would be insurmountable). However, LMs may still produce a valid solution if they not\nonly write code, but also selectively \u201cemulate\u201d the interpreter by generating the expected output of\n\u201cdetect_sarcasm(string)\u201d and other lines of code that cannot be executed. In this work, we propose Chain\nof Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning.\nThe key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode\nthat the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as\nan \u201cLMulator\"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other\nbaselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12%\nover Chain of Thought. CoC scales well with large and small models alike, and broadens the scope of\nreasoning questions that LMs can correctly answer by \u201cthinking in code\".\nDirect answer only\nChain of Thought\nChain of Code\n(a) Direct answer only\n(b) Chain of Thought\n(c) Chain of Code (Ours)\nFigure 1 | Chain of Code generates code and reasons through an LM-augmented code emulator. Lines\nevaluated with Python are in red and with an LM are in purple. The full query is in Fig. A4. (1a-1c)\nshow results on BIG-Bench Hard compared to human performance [34].\nCorresponding author(s): chengshu@stanford.edu, xiafei@google.com, ichter@google.com\n\u00a9 2023 Google DeepMind. All rights reserved. \u2217Work done as a student researcher at Google DeepMind. \u2020Equal advising.\narXiv:2312.04474v2  [cs.CL]  8 Dec 2023\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\n1. Introduction\nLanguage models (LMs) at certain scale exhibit the profound ability to solve complex reasoning ques-\ntions [3, 41] \u2013 from writing math programs [9] to solving science problems [17]. Notably, these\ncapabilities have shown to improve with Chain of Thought (CoT) prompting [42], whereby complex\nproblems are decomposed into a sequence of intermediate reasoning steps. CoT excels at semantic\nreasoning tasks, but tends to struggle with questions that involve numeric or symbolic reasoning [23, 36].\nSubsequent work addresses this by prompting LMs (e.g., trained on Github [4]) to write and execute\ncode [1, 5, 26]. Code in particular is advantageous because it provides both (i) a general syntactic\nstructure to build and encode complex programs [19] (e.g., logic structures, functional vocabularies\n\u2013 in ways that are Turing complete), and (ii) an interface by which existing APIs paired together with\nan interpreter can be used to perform precise algorithmic computations (e.g., from multiplication of\nlarge numbers to sorting an array of size 10,000) that a language model trained only to mimic the\nstatistically most likely next token would otherwise struggle to produce.\nWhile writing and executing code may improve LM reasoning performance across a wide range\nof arithmetic tasks, this particular approach contends with the fact that many semantic tasks are rather\ndifficult (and at times, nearly impossible) to express in code. For example, it remains unclear how to\nwrite a function that returns a boolean when it detects sarcasm in a string [36] (handling the edge cases\nwould be insurmountable). Perhaps fundamentally, using LMs to write programs in lieu of multi-step\ntextual reasoning inherently assumes that the intermediate reasoning traces (expressed in lines of code)\nall need to be executable by an interpreter. Is it possible to lift these restrictions to get the best of both\nreasoning in code and reasoning in language?\nInthiswork, weproposeChainofCode(CoC),asimpleyetsurprisinglyeffectiveextensiontoimprove\nLM code-driven reasoning \u2013 where the LM not only writes a program, but also selectively \u201csimulates\u201d\nthe interpreter by generating the expected output of certain lines of code (that the interpreter could\nnot execute). The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible\npseudocode that at runtime can be explicitly caught and handed off to emulate with an LM \u2013 we term this\nan LMulator (a portmanteau of LM and emulator). For example, given the task \u201cin the above paragraph,\ncount how many times the person was sarcastic,\u201d we can in-context prompt the LM to write a program\nthat may call helper functions such as is_sarcastic(sentence), to which the LM makes a linguistic\nprediction and returns the result as a boolean output, that then gets processed with the rest of the\nprogram. Specifically, we formulate LM reasoning as the following process (illustrated in Figure 1): the\nLM writes code, the interpreter steps through to execute each line of code (in red), or if it fails, simulates\nthe result with the LM (in purple) and updates the program state (in green). CoC inherits the benefits of\nboth (i) writing executable code (where precise algorithmic compututations are left to an interpreter),\nand (ii) writing pseudocode for semantic problems, and generating their outputs (which can be thought\nof as a simple formatting change, to which LMs are robust [22]) \u2013 enabling the LM to \u201cthink in code.\u201d\nExtensive experiments demonstrate that CoC is applicable to a wide variety of challenging numerical\nand semantic reasoning questions, and outperforms a number of popular baselines. In particular, we\nfind that it achieves high performance on BIG-Bench Hard tasks [36], outperforming average human\nraters overall and even the best human raters on an algorithmic subset of tasks, and to the best of\nour knowledge setting a new state of the art. We further show that both code interpreter execution\nand language model execution simulation are necessary for this performance, and that the approach\nscales well with large and small models alike \u2013 contrary to prompting techniques like Chain of Thought\nthat only emerge at scale. Finally, we demonstrate how Chain of Code can serve as a general purpose\nreasoner via cross-task prompting benchmark, which in contrast to prior work, uses prompts from\ndifferent families of problems as context \u2013 providing only the structure of the response (as opposed\nto the solution itself). This work underscores how one may leverage the structure and computational\npower of code and the reasoning abilities of language models to enable a \u201cbest of both worlds\u201d reasoner.\n2\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\n2. Chain of Code: Reasoning with an LMulator\nIn this section, we describe Chain of Code (CoC) prompting, an approach that leverages the ability of\nlanguage models to code, to reason, and to leverage an LM-augmented code emulator (an LMulator)\nto simulate running code. We start with background in Section 2.1, then overview the method in\nSection 2.2, its implementation in Section 2.3, and finally its capabilities in Section 2.4.\n2.1. Preliminaries\nBriefly, we overview some background on LM reasoning. Many of these reasoning techniques have been\nenabled by in-context learning [3], which provides the model with a few demonstrative examples at\ninference time, rather than updating any weights with gradients. These examples serve to provide\ncontext and format for the setting, enabling the model to emulate these examples while adapting to a\nnew query. This property has been instrumental in easily applying LMs to new tasks as it can be rapidly\nadapted and requires minimal data.\nThrough in-context learning, approaches have been developed to leverage human thought pro-\ncesses and use tools to improve performance of language models. We outline three such approaches\nthat provide the foundations for Chain of Code. Chain of Thought (CoT) [42], ScratchPad [26], and\nProgram of Thoughts [5] demonstrated the efficacy of breaking problems down into substeps. For\nCoT these substeps are in natural language, mirroring one\u2019s thought process when stepping through\na complicated problem. ScratchPad, on the other hand, maintains a program state of intermediate\nsteps when simulating the output of code \u2013 resulting in an LM acting as a code interpreter. Program\nof Thoughts [5] focused on generating the code itself, which is then executed by a code interpreter\nto solve reasoning problems. Each of these is visualized in Figure 2.\n2.2. Chain of Code\nInspired by how a human may reason through a particularly complex problem with a mix of natural\nlanguage, pseudocode, and running code or how a researcher may develop a new general algorithm\nthrough a code-based formalism then apply it to a problem, Chain of Code proceeds in two steps: (1)\nGeneration, which, given the question to solve, an LM generates code to reason through the problem,\nand (2) Execution, which executes the code via a code interpreter when possible and via an LM when\nnot. See Section 2.3 for more details on the specific implementation.\nChain of Code Generation Given a problem to solve, CoC generates reasoning substeps in the\nstructure of code. This code provides the framework of reasoning through the problem, and may be\nin the form of explicit code, pseudocode, or natural language. Figure 2d walks through a potential\ngeneration to solve an object counting problem from BIG-Bench.\nChain of Code Execution A core contribution of CoC is not just the generation of reasoning code, but\nthe manner in which it is executed. Once the code is written, the code is attempted to be run by a code\ninterpreter \u2013 in this work we consider Python, but the approach is general to any interpreter. If the code\nis successfully executed, the program state is updated and the execution continues. If the code is not\nexecutable or raises any exception, the language model instead is used to simulate the execution. The\nprogram state is subsequently updated by the language model\u2019s outputs and the execution continues.\nHerein, we refer to this as an LMulator, a portmanteau of LM and code emulator. This relatively simple\nchange enables a variety of new applications for code which mix semantics and numerics. Figure 2e\nshows how the generated code is run, maintaining the program state and switching between the Python\nexecutor and the LMulator.\n3\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\n(a) Chain of Thought [42]\n(b) Program of Thoughts [5]\n(c) ScratchPad [26]\n(d) Chain of Code Generation\nQ: I have an orange, a violin, two peaches,\nan apple, a pepper, and three plums. How many\nfruits do I have?\n1\nobjects = {\"orange\": 1, \"violin\": 1,\n\"peaches\":\n2, \"apple\":\n1, \"pepper\":\n1,\n\"plum\": 3}\n2\nnum_fruits = 0\n3\nfor object in objects:\n4\nobject_is_fruit = is_fruit(object)\n5\nif object_is_fruit:\n6\nnum_fruits += objects[object]\n7\nanswer = num_fruits\n(e) Chain of Code Execution\nQ: I have an orange, a violin, two peaches, an apple, a pepper,\nand three plums. How many fruits do I have?\n1\nobjects = {\"orange\": 1, \"violin\": 1, \"peaches\": 2, \"apple\":\n1, \"pepper\": 1, \"plum\": 3}\ndelta state: {objects = {\u2018orange\u2019: 1, \u2018violin\u2019: 1, ...}}\n2\nnum_fruits = 0\ndelta state: {num_fruits = 0}\n3\nfor object in objects:\ndelta state: {object = \u2018orange\u2019} # updated for each loop\n4\nobject_is_fruit = is_fruit(object)\ndelta state: {object_is_fruit = True}\n5\nif object_is_fruit:\ndelta state: {}\n6\nnum_fruits += objects[object]\ndelta state: {num_fruits = 1}\n7\nanswer = num_fruits\ndelta state: {answer = 7}\nA: 7\nFigure 2 | Previous reasoning methods: To solve advanced problems, (2a) Chain of Thought prompting\nbreaks the problem down into intermediate steps, (2b) Program of Thoughts prompting writes and\nexecutes code, and (2c) ScratchPad prompting simulates running already written code by tracking\nintermediate steps through a program state. Our reasoning method: Chain of Code first (2d) generates\ncode or psuedocode to solve the question and then (2e) executes the code with a code interpreter if\npossible, and with an LMulator (language model emulating code) otherwise. Blue highlight indicates\nLM generation, red highlight indicates LM generated code being executed, and purple highlight\nindicates LMulator simulating the code via a program state in green.\n2.3. Chain of Code Implementation\nWhile the generation implementation is straightforward prompting and language model generation,\nthe execution implementation is slightly more complex. Our implementation is based on using Python\u2019s\ntry and except and maintaining a program state. Line by line CoC steps through the code. If the\nline is executable by a code interpreter, it is executed, the program state is updated, and the program\ncontinues. If it is not executable by a code interpreter, a language model is given the context of the\nprogram (the question, the prior lines, and the history of the program state) and generates the next\nprogram state. This emulation can also leverage chain of thought to determine how to respond. That\ngenerated program state is then updated for the code interpreter as well. This sharing of program state\n4\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\ninterweaves the code interpreter and the language model simulator in a manner applicable to arbitrary\ninterweaving, even control flow like for-loops and if-statements. This continues until the entire code\nis run, and the answer is retrieved as the value of the variable named answer, or in case of irrecoverable\nerrors, with the language model outputting A: answer.\nAs a brief example, the code answer = 0; answer += is_sarcastic(\u2018you don\u2019t say\u2019); answer += 1; would\nbe executed as follows: (1) Python would execute the first line answer = 0; and update the program state\nto {answer = 0}, (2) Python would attempt to execute the second line and fail, and thus the LMulator\nwould simulate the code answer += is_sarcastic(\u2018you don\u2019t say\u2019); by generating the program state {answer\n= 1}, which would be updated in the program, (3) Python would execute the last line answer += 1; and\nupdate the program state to {answer = 2}, (4) the answer would be retrieved as 2.\n2.4. Chain of Code Abilities\nChain of Code has several attractive properties:\n1. It enables code use in entirely new regimes, by combining the advantages of code with the\npowerful semantic and commonsense knowledge of language models, which can easily express\nrules that are challenging to express in code (e.g., which foods are fruits?). Such an ability may\nhave benefits beyond reasoning problems and its flexibility enables executing expressive language,\nsuch as pseudocode.\n2. It leverages the ability of language models to code, a particular strength of recent language\nmodels due to the high quality data available.\n3. It inherits many of the benefits of reasoning code, both the formal yet expressive structure of code\n(e.g., Turing completeness) and powerful computational tools available to code (whether simply\nmultiplying two numbers, calculating\n5\u221a\n12121, or simulating physics).\n4. It inherits many of the benefits of techniques that reason via intermediate steps, such as Chain of\nThought. These techniques enable the language model to use more computation when necessary\nto solve a problem as well as provide more interpretability.\nEmpirically, we observe in Section 3 that these benefits results in significant improvements in reasoning\nperformance over a variety of challenging tasks.\n3. Language Reasoning Experimental Evaluation\nWe select challenging problems requiring varied types of reasoning, whether arithmetic, commonsense,\nor symbolic reasoning tasks, to answer the following questions:\n1. How well does CoC perform overall across a variety of tasks?\n2. Which types of problems does CoC perform best?\n3. How does each aspect of CoC affects overall performance?\n4. How does CoC scale with model size?\n5. How does CoC perform as a general-purpose reasoner, with prompt examples from different\nproblems rather than the same problem (which we term cross-task prompting)?\n6. How does CoC compare with instruction tuned chat models with and without tools?\nWe first discuss the approaches, ablations, and baselines considered in Section 3.1, then the tasks\nconsidered in Section 3.2, and finally the results in Section 3.3.\n5\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\n3.1. Baselines and Ablations\nWe consider our main method to be CoC (Interweave), also referred to as CoC (Ours), though we also\npropose two variants with simpler implementation and modestly lower performance: CoC (try Python\nexcept LM) and CoC (try Python except LM state). These two variants attempt to run the entire\ngenerated code with Python (rather than line by line) and if it fails, simulate the code execution with\nthe LMulator, outputting a final answer or an intermediate state trace, respectively. We also perform the\nfollowing ablations, some of which are comparable to previous work as noted. In CoC (Python) Python\nis used to run the entire generated code and if the code is not executable, it is marked as failure \u2013 this can\nbe thought of as a comparison to Program of Thoughts [5] or Program-aided language models [10]. We\nnote that in many cases this baseline is particularly challenged, as writing executable code for some of the\nreasoning problems becomes nearly impossible (e.g., writing code to judge if a phrase is sarcastic), but\none may focus on the results for Algorithmic only tasks for a more fair comparison. In CoC (LM) the code\nis interpreted by an LMulator outputting the final answer, and in CoC (LM state) the code is interpreted\nby an LMulator outputting a state trace of intermediate steps \u2013 this can be thought of as ScratchPad\nprompting for reasoning [26]. Note, the last two ablations do not leverage the Python interpreter.\nWe also compare against the following baselines. In Direct question answering the LM simply\nresponds to the question with a final answer. In Chain of Thought prompting (CoT) the LM uses\nintermediate steps to solve the task; we use CoT as our standard prompt technique for the field of\nsubstep prompting [16, 48] as prompts are readily available.\n3.2. Tasks\nWe consider a subset of challenging tasks from BIG-Bench [34] called BIG-Bench Hard (BBH) [36]\nto ensure we are solving the most challenging tasks. These tasks were specifically selected for their\ndifficulty for language models and the datasets provides human-rater baselines and a set of Chain of\nThought prompts. The 23 tasks require semantic reasoning(e.g., \u201cMovie Recommendation\u201d), numerical\nreasoning (e.g., \u201cMulti-Step Arithmetic\u201d), and a combination of both (e.g., \u201cObject Counting\u201d). As such\nthey enable us to study the efficacy of CoC across varied problems, not just those that coding is a natural\nfit for. Several prompts are shown in Appendix Figure A1. We also show results for the grade-school\nmath (GSM8K) benchmark [7] in Appendix Section A.2, though find that these problems are primarily\nsolved algorithmically alone through code.\nThese tasks are evaluated with few-shot prompting, whereby three examples from the same prob-\nlem family are provided as context. We also introduce a new evaluation setting, cross-task prompting,\nwhereby three examples of different problems are provided as context. As such, the language model\nhas in-context examples of the format of reasoning, but isn\u2019t provided explicit instructions on how to\nreason. We see this as an indicative signal for a general-purpose reasoner, which in many real-world\napplications (e.g., chatbots) would be asked to reason across a wide variety of tasks.\nThe models used herein include the OpenAI family of models: text-ada-001, text-baggage-001,\ntext-curie-001, and text-davinci-003 (in plots we denote these as a-1, b-1, c-1, and d-3). We also\nconsider PaLM-2\u2019s code finetuned variant [6, 12]. For instruction tuned models, we compare to recent\nvariants of GPT (gpt-3.5-turbo and gpt-4) with the chat completion mode run in October 2023. The\nresults below are using the text-davinci-003 model unless otherwise stated.\n3.3. Results\nQuestion 1: Overall Performance. The overall performance of CoC is shown in Figure 1 and Table 1\n(with full results in Table A1). We see that CoC outperforms other approaches, both in the number of\n6\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\ntasks it exceeds the human baseline and in the overall amount that it exceeds the baseline. Indeed, CoC\u2019s\n84% is SoTA to the best of our knowledge [11]. In several tasks CoC vastly outperforms the human base-\nline and other methods, achieving nearly 100% \u2013 generally for these tasks the result is complicated in\nlanguage but trivial in code (e.g., a task from multi-step arithmetic Q: ((\u22123+5\u00d78\u00d7\u22124)\u2212(9\u22128\u00d7\u22127)) =).\nWe also observe that CoT outperforms the human baseline on a number of tasks, while the Direct answer\nfares poorly.\nTable 1 | Overall performance (%) with both few-shot prompting with a single task and cross-task. The\ndelta compared to direct prompting is shown in parenthesis.\ntext-davinci-003\nPaLM 2-S* (code variant [12])\nHuman\nDirect\nCoT\nCoC\nDirect\nCoT\nCoC\nSingle task\n68\n55\n72 (+17)\n84 (+29)\n49\n61 (+12)\n78 (+29)\nCross task\n-\n50\n55 (+5)\n61 (+11)\n45\n47 (+2)\n47 (+2)\nQuestion 2: Problem Type. Figure 3 breaks the results down by problem type; the task labels\nare shown in Table A1. First, we isolate problems that are primarily algorithmic or primarily natural\nlanguage (these categories were identified in [36]). We see that on algorithmic tasks, CoC performs\nparticularly well, while on natural language tasks CoC performs on par with CoT. This is particularly\nencouraging, because one may expect these language oriented tasks to be a worse fit for code. The key\nis that our method offers the flexibility of using a LMulator to simulate the output of code execution,\nretaining the semantic reasoning capabilities of LMs for natural language problems.\nFigure 3 additionally breaks the tasks down into categories that capture how different each question\u2019s\nresponse is and whether the code can be fully executed by Python (denoted Python only vs. Python +\nLM). For some tasks within the benchmark, each question has the same code or Chain of Thought, with\nthe only variation being the inputs \u2013 in this case we say the code is (repeated code), and if not then it is\ndenoted (new code). As expected, we see that when the code is repeated and run by Python, CoC gets\nnearly 100%, though these tasks (e.g., multi-step arithmetic) seem to be among the most challenging\nfor the other baselines, including human raters. The other categories are more challenging for CoC;\nhowever in each, we still see a benefit over baselines.\nFigure 3 | Average performance across different baselines grouped by task type, indicating the problem\ntype and how the CoC is generated and executed.\nQuestion 3: Ablations. Figures 4 and 5, and Table 2 show the ablations performed to motivate\neach aspect of Chain of Code prompting. As one may expect, the approaches that execute Python (CoC\n(Interweave, Python, try Python except LM, try Python except LM state)) achieve 100% performance on\n7\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\nseveral tasks \u2013 if the code is correct, then the model will be correct every time. However, the approach\nthat relies on Python alone (CoC (Python)) performs poorly when applied to non-algorithmic tasks,\nfailing almost all. The CoC (Python) ablation is similar to recent works [5, 10], which show that if\napplied to numerical problems then code reasoning performs well. CoC without the Python interpreter\n(CoC (LM, LM state)) too fares poorly, though we see that the step-by-step approach proposed in\nScratchPad prompting [26] improves in each task.\nWe also show that ablations CoC (try Python except LM, try Python except LM state), in which\nCoC first tries to run the entire code with Python and if it fails simulates the code with an LM, perform\nquite well. Again we see that maintaining a program state provides an improvement in performance.\nWith only minor degradations in performance observed, they are reasonable alternatives to the fully\ninterweaved CoC for their simplicity. Though we note, these ablations\u2019 performance would be much\nworse in cases where interweaving code and semantics is truly necessary \u2013 for example, if we imagine\na case where code is necessary to parse image inputs or to access an external database, but language\nis necessary to parse the results (see the robotics applications in Section 4).\nFigure 4 | CoC ablations on average performance grouped by task type.\nFigure 5 | Results across all BIG-Bench Hard tasks compared to human baseline [34]. The tasks (x-axis)\nin each plot are sorted individually by performance. See Table A1 and Figure 4 for a breakdown by task\ntype.\nQuestion 4: Scaling. Figure 6 shows the performance of CoC across various model sizes. We\nobserve that, similar to Chain of Thought prompting, the improvements of CoC increases as model\n8\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\nTable 2 | Ablation overall performance (%) with both few-shot prompting with a single task and cross-\ntask. The delta compared to the full model (Interweave) is shown in parenthesis.\nChain of Code\nInterweave\ntry Python\ntry Python\nPython\nLM state\nLM\nPrompt\nexcept LM state\nexcept LM\nSingle task\n84\n82 (-2)\n80 (-4)\n48 (-36)\n63 (-21)\n57 (-27)\nCross task\n61\n57 (-4)\n60 (-1)\n35 (-26)\n49 (-12)\n50 (-11)\nsize increases. In fact, for some of the algorithmic tasks, Chain of Code even outperforms the best\nhuman raters (whom admittedly did not have access to code). Unlike Chain of Thought prompting,\nhowever, which only brings performance benefits for the largest model (d-3), CoC outperforms the\ndirect question answering baseline also for smaller models (a-1, b-1, c-1), suggesting that it\u2019s easier\nfor smaller models to output structured code as intermediate steps rather than natural languages.\nQuestion 5: Cross-task Prompting. For cross-task prompting, we prompt the language models\nwith a few examples from different problems. We see the performance drops for all methods in Figure 6\nand Table 2. Despite this drop, CoC outperforms CoT and direct prompting at scale, nearly achieving\nhuman average performance. This is a promising indication towards general purpose reasoning, in\nwhich a model does not expect to receive examples of similar problems in its prompt.\nFigure 6 | Average performance with model scaling.\nQuestion 6: Instruction Tuned Models. To compare against instruction tuned models with the\nchat interface, we prompt the models with instructions to elicit the desired reasoning approaches.\nFor the baselines, we ask the model to \u201cdirectly answer\u201d (Direct) or \u201cthink step by step\u201d (CoT). For\nCoC variants, we ask the model to \u201cwrite python code to help solve the problem, if it\u2019s helpful\u201d. If a\nprogram is written, we either run the code with a Python interpreter and then feed the result (or the\nerror message if execution fails) back to the model to determine a final answer (CoC (Python)), or\nask the model to simulate the output of code execution as a LMulator (CoC (LM)). The CoC (Python)\nbaseline can be thought of as a comparison to an LM with Python tool use.\nTable 3 shows the performance of each. With gpt-3.5-turbo, both CoT and CoC (Python) show\nbenefits over direct prompting, although both are strongly outperformed by CoC (Interweave). With\ngpt-4, despite the considerable model strength advantage over text-davinci-003, CoC (Interweave)\nstill outperforms, though the gap is narrower. Due to the limits of the chat interface, we are unable\nto run the full CoC (Interweaved) approach with these models, but we do expect further gains if it were\nto be paired with gpt-4.\n9\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\nTable 3 | Comparisons with instruction tuned models in the chat interface, with and without tool use.\ntext-davinci-003\ngpt-3.5-turbo\ngpt-4\nCoC\nDirect\nCoT\nCoC\nCoC\nDirect\nCoT\nCoC\nCoC\n(Interweave)\n(Python)\n(LM)\n(Python)\n(LM)\n84\n51 (-33)\n56 (-28)\n56 (-28)\n45 (-39)\n70 (-14)\n78 (-6)\n82 (-2)\n75 (-9)\n4. Robotics Applications\nDownstream applications such as robotics are well fit for CoC as robotics tasks require semantic reason-\ning and algorithmic reasoning, as well as interfacing with other APIs through code (such as control or\nperception APIs [19]) and with users through natural language. For example, given a task like \u201csort the\nfruits by size\u201d, the robot must reason over which items are fruits, sort them by size, and then connect\nthose decisions to actions executable on the robot. CoC (Interweave) is able to solve these challenges\nwith the Python interpreter and the LMulator at runtime, while allowing for more interpretability and\nfine-grained control of the robot policies.\nEnvironment and Robot Setup. Our environment is a tabletop with small objects (containers, toys,\netc) and a UR5 robot arm equipped with a vacuum gripper and a wrist-mounted RGB-D camera. For\nthe purpose of our experiments, the available perception API is detect_objects(), which returns a\nlist of detected objects (probabilities, labels, bounding boxes and segmentation masks) from the wrist\ncamera. This API is implemented with first querying GPT-4V [27] for a list of objects, and then using\nGrounding-SAM [15, 20] to localize them. The available control API is pick_place(obj1, obj2), which\nis a scripted primitive skill that picks up obj1 and places it on top of obj2. There is also a text-to-speech\nAPI say(sentence) that allows the robot to communicate with the user.\nResults. We evaluate with a number of tabletop pick-and-place robotics tasks that involve semantic\nreasoning; these tasks are listed in Section A.4. With few-shot prompting, one example is provided as\ncontext (of a food serving problem) so that the language model understands the expected structure as\nwell as the available robot APIs. From this single example, we see that our model is able to generalize\nto new objects, languages, and task domains (see Figure A3 and an example trajectory in Figure 7).\nNote that for these robotics tasks, unlike the previous language reasoning tasks, our main method\nCoC (Interweave) is the only capable approach, as the code requires line-by-line interplay between the\nPython interpreter execution (robot APIs) and the LMulator (commonsense QA like is_compostable).\nFigure 7 | Robot trajectory visualization for task \u201csort the objects on the table into the compost bin\nand the recycle bin\u201d. CoC first generates code to solve the problem, and then executes the code with\nPython if possible (e.g., robot APIs like detect_objects and pick_place), and with LMulator if not\n(e.g., commonsense QA like is_compostable). The robot successfully picks and places the Post-it note\nto the recycle bin and the orange peel to the compost bin. See the full code in Fig. A3 and videos of\nrollouts at our webpage https://chain-of-code.github.io/.\n10\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\n5. Related Work\nLanguage Model Reasoning The abilities and applications of language models have seen significant\nprogress, due to their overall performance [6, 11, 31, 37] and emergent capabilities [41], such as\nfew-shot prompting [3] and abstract reasoning [42]. Perhaps most related to this work, a number of\nworks have leveraged prompting to improve reasoning [8]: Chain of Thought [42] proposes to break a\ntask down into intermediate reasoning steps, least-to-most [48] proposes a series of increasingly simpler\nproblems, and ScratchPad [26] proposes to maintain a trace of intermediate results for interpreting\ncode (this first demonstrated the code simulation ability of LMs required for our LMulator). Along\nthese lines \u201clet\u2019s think step by step\u201d [16] uses a few key words to elicit such break downs (words that\nwere later refined to \u201cTake a deep breath and work on this problem step-by-step\u201d in Yang et al. [43]).\nBeyond these, other approaches structure such step-by-step solutions into graphical structures [2, 45],\nplans [25, 39], or mixture of expert-based sampling [40, 49]. CoC builds upon the intuition of these\nworks, with the observation that code is a formal, structured approach to breaking a problem down\ninto sub-steps with many advantages beyond natural language alone.\nLanguage Model Tool Use Many recent works have proposed techniques for language models\nto use tools to respond to queries [21]. These tools have often been provided to the language model\nthrough prompting [6, 7, 9, 14, 44], enabling tools like calculators for math problems, code interpreters,\ndatabases, or more. These tools too can provide feedback on novel modalities [35, 46]. To expand the\nrange of tools available, others have used external tool databases or finetuned language models [28\u2013\n30, 32]. As tool interfaces vary, feedback from the tool too can improve performance [13, 47]. In this\nwork we leverage the expressibility and generality of full code as well as its structure, by treating it\nboth as a tool and as a framework.\nLanguage Model Program Synthesis The ability of language models to code is well known and\nthey have been applied as programming assistants [4] and shown to be capable programmers on their\nown [1, 18, 24]. This ability has been applied to a variety of tasks outside of language alone, leveraging\ntheir ability to reason through code in new settings, such as robotics [19, 33], embodied agents [38],\nor vision [35]. Others have specifically done so for reasoning, such as Program of Thoughts [5] and\nProgram-aided Language Models [10], which generate code to solve numerical reasoning problems.\nHerein, we focus on the interplay between writing code, running code, and language models simulating\ncode, thus enabling new regimes of language model code applications, such as semantic reasoning.\n6. Conclusions, Limitations, and Future Work\nWe have proposed Chain of Code, an approach towards reasoning with language models through writing\ncode, and executing code either with an interpreter or with a language model that simulates the execu-\ntion (termed herein an LMulator) if the code is not executable. As such, CoC can leverage both the ex-\npressive structure of code and the powerful tools available to it. Beyond this, by simulating the execution\nof non-executable code, CoC can apply to problems nominally outside the scope of code (e.g., semantic\nreasoning problems). We have demonstrated that this approach outperforms baselines, and for some\ntasks even the best human raters, in a range of challenging language and numeric reasoning problems.\nThis work is not without its limitations. First, generating and executing in two steps as well as\ninterweaving code and language execution requires additional context length and computation time.\nSecond, though we have not seen any loss of performance for semantic tasks in aggregate, there are few\ntasks in which code doesn\u2019t help, e.g., the task Ruin Names, which asks whether an edit for a name is\nhumorous. Finally, our implementation to interweave LM and code is quite simple, tracking the program\nstate in strings and parsing the strings into Python\u2019s built-in data types (e.g., dict, tuple). As our method\nstands now, the LM cannot modify custom Python objects while simulating code execution. In theory,\n11\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\nhowever, it is doable as long as each of these Python objects have a serialization and deserialization\nmethod, e.g., using techniques like Protocol Buffers.\nThere are many avenues for future work with CoC. First, we believe that a unified code and language\ninterpreter well combines the commonsense of language models with the analytical abilities, structure,\nand interpretability of code. Such a technology can thus enable applications of code and code-like reason-\ningtonovelproblemregimes, beyondsimplereasoning. Second, weareinterestedininvestigatingthede-\ngree to which finetuning a language model to be an LMulator can benefit semantic code reasoning. Third,\nwe see evidence that reasoning through many pathways yields improvements, which is a promising step\nforward. Finally, webelievethisintegrationwithcodeenablesaccesstoexternalmodalities, suchasvision\nor databases, and represents a interesting path for new applications (e.g., robotics, augmented reality).\nReferences\n[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\nlanguage models. arXiv preprint arXiv:2108.07732, 2021.\n[2] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna\nGajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al.\nGraph of thoughts: Solving elaborate problems with large language models. arXiv preprint\narXiv:2308.09687, 2023.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[5] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks.\narXiv preprint\narXiv:2211.12588, 2022.\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168, 2021.\n[8] David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model\ncascades. arXiv preprint arXiv:2207.10342, 2022.\n[9] Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin\nLiu, Linda Chen, Sunny Tran, Newman Cheng, et al. A neural network solves, explains, and\ngenerates university math problems by program synthesis and few-shot learning at human level.\nProceedings of the National Academy of Sciences, 119(32):e2123433119, 2022.\n[10] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning, pp. 10764\u201310799. PMLR, 2023.\n12\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\n[11] Google Gemini Team. Gemini: A family of highly capable multimodal models. 2023. URL\nhttps://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf.\n[12] Google, Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical\nreport. arXiv preprint arXiv:2305.10403, 2023.\n[13] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.\nCritic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint\narXiv:2305.11738, 2023.\n[14] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and\nAshish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv\npreprint arXiv:2210.02406, 2022.\n[15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick.\nSegment anything. arXiv:2304.02643, 2023.\n[16] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems,\n35:22199\u201322213, 2022.\n[17] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al.\nSolving quantitative reasoning problems with language models, 2022.\n2022.\nURL\nhttps://arxiv.org/abs/2206.14858.\n[18] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation\nwith alphacode. Science, 378(6624):1092\u20131097, 2022.\n[19] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and\nAndy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 9493\u20139500. IEEE, 2023.\n[20] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection. arXiv preprint arXiv:2303.05499, 2023.\n[21] Gr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta\nRaileanu, Baptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented\nlanguage models: a survey. arXiv preprint arXiv:2302.07842, 2023.\n[22] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?\narXiv preprint arXiv:2202.12837, 2022.\n[23] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez\nArenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern\nmachines. arXiv preprint arXiv:2307.04721, 2023.\n[24] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,\nand Caiming Xiong. Codegen: An open large language model for code with multi-turn program\nsynthesis. arXiv preprint arXiv:2203.13474, 2022.\n[25] Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and Yu Wang. Skeleton-of-thought: Large\nlanguage models can do parallel decoding. arXiv preprint arXiv:2307.15337, 2023.\n[26] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\n13\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\nDavid Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al.\nShow\nyour work: Scratchpads for intermediate computation with language models. arXiv preprint\narXiv:2112.00114, 2021.\n[27] OpenAI. Gpt-4 technical report, 2023.\n[28] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer,\nand Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language\nmodels. arXiv preprint arXiv:2303.09014, 2023.\n[29] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv preprint\narXiv:2205.12255, 2022.\n[30] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world\napis. arXiv preprint arXiv:2307.16789, 2023.\n[31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[32] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves\nto use tools. arXiv preprint arXiv:2302.04761, 2023.\n[33] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans\nusing large language models. In 2023 IEEE International Conference on Robotics and Automation\n(ICRA), pp. 11523\u201311530. IEEE, 2023.\n[34] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the\nimitation game: Quantifying and extrapolating the capabilities of language models. arXiv\npreprint arXiv:2206.04615, 2022.\n[35] D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution\nfor reasoning. arXiv preprint arXiv:2303.08128, 2023.\n[36] Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench\ntasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\n[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023.\n[39] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\nPlan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language\nmodels. arXiv preprint arXiv:2305.04091, 2023.\n[40] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171, 2022.\n[41] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\nlanguage models. arXiv preprint arXiv:2206.07682, 2022.\n14\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\n[42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances\nin Neural Information Processing Systems, 35:24824\u201324837, 2022.\n[43] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\nChen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.\n[44] ShunyuYao, JeffreyZhao, DianYu, NanDu, IzhakShafran, KarthikNarasimhan, andYuanCao. Re-\nact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n[45] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601, 2023.\n[46] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Fed-\nerico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing\nzero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\n[47] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia,\nLinqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code\ninterpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023.\n[48] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables\ncomplex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\n[49] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V\nLe, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural\nInformation Processing Systems, 35:7103\u20137114, 2022.\n15\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\nA. Appendix\nA.1. Quantitative results on language reasoning tasks\nTable A1 shows the full per-task results across ablations on BIG-Bench Hard (BBH) tasks, as well as\nbroken down by task type and execution type.\nTable A1 | Full results across ablations on BIG-Bench Hard (BBH) tasks.\nSrivastava et al. [34]\nSuzgun et al. [36]\nChain of Code\nBIG-Bench Hard Task\nRand.\nHuman\n(Avg.)\nHuman\n(Max)\nDirect\nCoT\nInter-\nweave\ntry\nPython\nex-\ncept\nLM\nstate\ntry\nPython\nex-\ncept\nLM\nPython\nLM\nstate\nLM\nBoolean Expressions\ud835\udf06+\n50\n79\n100\n88\n89\n100\n100\n100\n100\n95\n90\nCausal Judgement\ud835\udf05\u2217\n50\n70\n100\n64\n64\n56\n57\n63\n0\n57\n60\nDate Understanding\ud835\udf05\u2212\n17\n77\n100\n61\n84\n75\n72\n74\n59\n66\n57\nDisambiguation QA\ud835\udf05/\n33\n67\n93\n70\n68\n71\n67\n68\n0\n67\n68\nDyck Languages\ud835\udf06+\n1\n48\n100\n6\n50\n100\n100\n99\n99\n1\n7\nFormal Fallacies\ud835\udf05\u2217\n25\n91\n100\n56\n56\n55\n54\n55\n0\n54\n56\nGeometric Shapes\ud835\udf06+\n12\n54\n100\n48\n66\n100\n100\n100\n100\n13\n44\nHyperbaton\ud835\udf05/\n50\n75\n100\n63\n64\n98\n62\n55\n0\n62\n55\nLogical Deduction\ud835\udf06\u2217\n23\n40\n89\n49\n66\n68\n79\n57\n0\n79\n58\nMovie Recommendation\ud835\udf05/\n25\n61\n90\n85\n81\n80\n83\n80\n0\n83\n79\nMulti-Step Arithmetic\ud835\udf06+\n0\n10\n25\n0\n48\n100\n100\n100\n100\n0\n1\nNavigate\ud835\udf06\u2217\n50\n82\n100\n58\n94\n86\n84\n68\n0\n84\n68\nObject Counting\ud835\udf06\u2212\n0\n86\n100\n30\n82\n96\n98\n98\n98\n57\n50\nPenguins in a Table\ud835\udf05\u2212\n0\n78\n100\n62\n82\n90\n88\n90\n88\n71\n59\nReasoning\nabout\nColored\nObjects\ud835\udf05\u2212\n12\n75\n100\n64\n87\n78\n74\n78\n64\n64\n70\nRuin Names\ud835\udf05/\n25\n78\n100\n76\n70\n55\n56\n46\n0\n56\n47\nSalient\nTranslation\nError\nDetection\ud835\udf05/\n17\n37\n80\n66\n61\n58\n63\n64\n0\n63\n64\nSnarks\ud835\udf05/\n50\n77\n100\n70\n71\n76\n76\n66\n0\n76\n66\nSports Understanding\ud835\udf05/\n50\n71\n100\n72\n96\n91\n93\n75\n0\n93\n74\nTemporal Sequences\ud835\udf06\u2217\n25\n91\n100\n38\n60\n98\n93\n99\n93\n93\n99\nTracking Shuffled Objects\ud835\udf06\u2212\n23\n65\n100\n25\n72\n100\n96\n96\n96\n71\n24\nWeb of Lies\ud835\udf06\u2212\n50\n81\n100\n54\n100\n97\n96\n96\n97\n96\n50\nWord Sorting\ud835\udf06+\n0\n63\n100\n51\n50\n99\n100\n99\n100\n54\n54\nTask Averages\nNLP Task (avg)\ud835\udf05\n30\n71\n97\n67\n74\n74\n70\n68\n18\n68\n63\nAlgorithmic Task (avg)\ud835\udf06\n21\n64\n92\n41\n71\n95\n95\n92\n80\n58\n50\nAll Tasks (avg)\n26\n68\n95\n55\n72\n84\n82\n80\n48\n63\n57\nExecution Type\nPython exec (same program)+\n13\n51\n85\n38\n61\n100\n100\n100\n100\n33\n39\nPython\nexec\n(different\nprogram)\u2212\n17\n77\n100\n49\n84\n89\n87\n89\n84\n71\n52\nLM exec (same program)/\n36\n66\n95\n72\n73\n76\n71\n65\n0\n71\n65\nLM exec (different program)\u2217\n35\n75\n98\n53\n68\n72\n73\n68\n19\n73\n68\n\ud835\udf06 denotes an algorithmic task and \ud835\udf05 denotes an NLP task (with categories outlined in Suzgun et al. [36]). + denotes a task where the code\nbetween prompts is repeated and can be executed by Python, \u2212 denotes a task where the code between prompts must change and can be\nexecuted by Python, / denotes a task where the code between prompts is repeated and must be executed by the LM, and \u2217 denotes a task\nwhere the code between prompts must change and must be executed by the LM.\n16\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\nA.2. Quantitative results on the GSM8K Benchmark\nTable A2 shows results on the the grade-school math benchmark (GSM8K) [7] with direct prompt-\ning, Chain of Thought, and Chain of Code. We find that CoC generally outperforms CoT and Direct\nprompting. Since these tasks are primarily algorithmic and are solved by Python alone, all Chain of\nCode variants that use Python achieve the same performance \u2013 also the same performance shown in\nProgram of Thoughts [5].\nTable A2 | GSM8K [7] performance (%) with both few-shot prompting with a single task and cross-task.\nThe delta compared to direct prompting is shown in parenthesis.\nChain of Code\nPrompt\nDirect\nCoT\nInterweave\ntry Python\ntry Python\nPython only\nLM state\nLM only\nexcept LM state\nexcept LM\nSingle task\n16\n63 (47)\n71 (55)\n72 (56)\n71 (55)\n71 (55)\n45 (29)\n22 (6)\nCross task\n14\n55 (41)\n60 (46)\n60 (46)\n60 (46)\n60 (46)\n41 (27)\n16 (2)\nA.3. Qualitative results on language reasoning tasks\nFigure A1 shows the model outputs for a few reasoning tasks from BIG-Bench Hard (BBH) and Fig-\nure A2 shows a demonstrative example of date reasoning. These examples are selected to highlight\nthe interweaving execution of the Python interpreter and the LMulator.\nA.4. Results on robotics tasks\nFor few-shot prompting, we include a single example: \u201cServe a meal that follows the user\u2019s dietary\nrestrictions\u201d. During test time, we query the model with each of the following instructions.\n\u2022 \u201cPack a lunch box for someone who is on a vegan diet.\u201d\n\u2022 \u201cAssemble a sandwich for someone who is vegetarian.\u201d\n\u2022 \u201cGather ingredients for a peanut butter sandwich in a plate.\u201d\n\u2022 \u201cPrepare \u897f\u7ea2\u67ff\u7092\u86cb in the pot.\u201d (interleaving English and Chinese on purpose)\n\u2022 \u201cPlace all paper-made objects in the grass-colored container.\u201d\n\u2022 \u201cSort the objects on the table into the compost bin and the recycle bin.\u201d\n\u2022 \u201cMy steak is too bland. Can you help?\u201d\nFigure A3 shows the one-shot prompt as well as the model outputs and how they are executed for\na few test instructions.\n17\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\n(a) Movie Recommendation\nQ: Find a movie similar to Batman, The Mask, The Fugitive, Pretty Woman:\nOptions:\n(A) The Front Page\n(B) Maelstrom\n(C) The Lion King\n(D) Lamerica\noptions = {\"The Front Page\": \"(A)\", \"Maelstrom\": \"(B)\", \"The Lion King\":\n\"(C)\", \"Lamerica\": \"(D)\"}\ndelta state: {options = {\u2018The Front Page\u2019: ..., ...}}\nquery_movies = list(options.keys())\ndelta state: {query_movies = [\u2018The Front Page\u2019, ...]}\nref_movies = [\"Batman\", \"The Mask\", \"The Fugitive\", \"Pretty Woman\"]\ndelta state: {ref_movies = [\u2018Batman\u2019, ..., \u2018Pretty Woman\u2019]}\nref_movie_infos\n=\nget_movie_genre_and_year(ref_movies,\nret_type=\"list[tuple[str, int]]\")\ndelta state: {ref_movie_infos = [(\u2018action\u2019, 1989), ...]}\nmost_similar_movie = get_most_similar_movie(query_movies, ref_movies,\nref_movie_infos, ret_type=\"str\")\ndelta state: {most_similar_movie = \u2018The Lion King\u2019}\nanswer = options[most_similar_movie] if most_similar_movie in options\nelse None\ndelta state: {answer = \u2018(C)\u2019}\n(b) Hyperbaton\nQ: Which sentence has the correct adjective order:\nOptions:\n(A) small grey square cloth Japanese shoe\n(B) small square grey Japanese cloth shoe\"\nimport numpy as np\ndelta state: {}\noptions = {\"(A)\": \"small grey square cloth Japanese shoe\", \"(B)\": \"small\nsquare grey Japanese cloth shoe\"}\ndelta state: {options = {\u2018(A)\u2019: ..., \u2018(B)\u2019: ...}}\npriority = {\"opinion\": 1, \"size\": 2, \"age\": 3, \"shape\": 4, \"color\": 5,\n\"origin\": 6, \"material\": 7, \"purpose\": 8}\ndelta state: {priority = {\u2018opinion\u2019: .., .., \u2018purpose\u2019: ..}}\nvalid_types = list(priority.keys())\ndelta state: {valid_types = [\u2018opinion\u2019, ..., \u2018purpose\u2019]}\nscores = []\ndelta state: {scores = []}\nfor option, sentence in options.items():\ndelta state: {option, sentence = \u2018(A)\u2019, \u2018small ... shoe\u2019}\n# updated for each loop\nadjs = sentence.split(\" \")[:-1]\ndelta state: {adjs = [\u2018small\u2019, \u2018grey\u2019, \u2018square\u2019, \u2018cloth\u2019]}\norder = [priority[get_adjective_type(adj, valid_types, ret_type=str)]\nfor adj in adjs]\ndelta state: {order = [2, 5, 4, 6]}\nscores.append([order[i+1] > order[i] for i in range(len(order) -\n1)].count(True))\ndelta state: {scores = [2]}\nanswer = list(options.keys())[np.argmax(scores)]\ndelta state: {answer = \u2018(B)\u2019}\n(c) Logical Deduction\nQ: The following paragraphs each describe a set of three objects arranged in\na fixed order. The statements are logically consistent within each paragraph.\nOn a shelf, there are three books: a green book, a red book, and a blue book.\nThe red book is the rightmost. The blue book is to the right of the green\nbook.\nOptions:\n(A) The green book is the leftmost\n(B) The red book is the leftmost\n(C) The blue book is the leftmost\noptions = {\"green\": \"(A)\", \"red\": \"(B)\", \"blue\": \"(C)\"}\ndelta state: {options = {\u2018green\u2019: ..., ..., \u2018blue\u2019: ...}}\norder_info = \"left to right\"\ndelta state: {order_info = \u2018left to right\u2019}\nfull_order = [None, None, None]\ndelta state: {full_order = [None, None, None]}\npartial_order = []\ndelta state: {partial_order = []}\nfull_order[-1] = \"red\"\ndelta state: {full_order = [None, None, \u2018red\u2019]}\npartial_order.append((\"green\", \"blue\"))\ndelta state: {partial_order = [(\u2018green\u2019, \u2018blue\u2019)]}\nfull_order\n=\ngenerate_full_order(full_order,\npartial_order,\nret_type=list)\ndelta state: {full_order = [\u2018green\u2019, \u2018blue\u2019, \u2018red\u2019]}\nquery = \"leftmost\"\ndelta state: {query = \u2018leftmost\u2019}\nresult = query_result(order_info, full_order, query, ret_type=str)\ndelta state: {result = \u2018green\u2019}\nanswer = options[result] if result in options else None\ndelta state: {answer = \u2018(A)\u2019}\n(d) Disambiguation QA\nQ: In the following sentences, explain the antecedent of the pronoun (which\nthing the pronoun refers to), or state that it is ambiguous.\nSentence: The homeowner asked the inspector if the house they had purchased\nwas structurally sound.\nOptions:\n(A) The homeowner had purchased\n(B) The inspector had purchased\n(C) Ambiguous\ncontext = \"The homeowner asked the inspector if the house they had\npurchased was structurally sound.\"\ndelta state: {context = \u2018The homeowner asked ... sound.\u2019}\npronoun = \"they\"\ndelta state: {pronoun = \u2018they\u2019}\na = \"homeowner\"\ndelta state: {a = \u2018homeowner\u2019}\nb = \"inspector\"\ndelta state: {b = \u2018inspector\u2019}\nversion_a = \"The homeowner asked the inspector if the house the homeowner\nhad purchased was structurally sound.\"\ndelta state: {version_a = \u2018The homeowner asked ... sound.\u2019}\nversion_b = \"The homeowner asked the inspector if the house the inspector\nhad purchased was structurally sound.\"\ndelta state: {version_b = \u2018The homeowner asked ... sound.\u2019}\nvalid_a\n=\ncan_pronoun_refer_to_noun(pronoun=pronoun,\nnoun=a,\nfull_sentence=version_a, ret_type=bool)\ndelta state: {valid_a = True}\nvalid_b\n=\ncan_pronoun_refer_to_noun(pronoun=pronoun,\nnoun=b,\nfull_sentence=version_b, ret_type=bool)\ndelta state: {valid_b = False}\nif valid_a and not valid_b:\ndelta state: {}\nanswer = \"(A)\"\ndelta state: {answer = \u2018(A)\u2019}\nelif valid_b and not valid_a:\nanswer = \"(B)\"\nelse:\nanswer = \"(C)\"\nFigure A1 | Model outputs for a few reasoning tasks from BIG-Bench Hard (BBH). We observe that\nCoC can apply to a wide variety of complex reasoning tasks that involve both semantic and numeric\nreasoning. Red highlight indicates LM generated code being executed by the Python interpreter, and\npurple highlight indicates LM simulating the code execution.\n18\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\nDirect answer only\nChain of Thought\nChain of Code\nFigure A2 | A demonstrative example of how Chain of Code generates code and reasons through an\nLM-augmented code emulator. Lines evaluated with Python are in red and with an LM are in purple.\nThe chain of thought and direct answers were evaluated with gpt-4 in October 2023, and we note the\ncurrent model (as of December 2023) writes code to solve this problem and gets the same solution as\nChain of Code.\n19\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\n(a) Given Prompt\nQ: Serve a meal that follows the user\u2019s dietary restrictions.\nrestrictions = input(\"Do you have any dietary restrictions?\")\n_, _, objs, _ = robot.detect_objects()\ncontainer_obj = None\nfor obj in objs:\nis_match_description\n=\nmatch_description(obj,\n\"food\ncontainer\",\nret_type=\"bool\")\nif is_match_description:\ncontainer_obj = obj\nbreak\nif container_obj is None:\nrobot.say(\"No food container found.\")\nelse:\nfor obj in objs:\nif obj == container_obj:\ncontinue\nis_follow_restrictions\n=\nfollow_restrictions(obj,\nrestrictions,\nret_type=\"bool\")\nif is_follow_restrictions:\nrobot.pick_place(obj, container_obj)\n(b) Novel Task\nQ: Sort the objects on the table into the compost bin and the recycle bin.\n_, _, objs, _ = robot.detect_objects()\ncompost_bin_obj = None\nrecycle_bin_obj = None\nfor obj in objs:\nis_match_description\n=\nmatch_description(obj,\n\"compost\nbin\",\nret_type=\"bool\")\nif is_match_description:\ncompost_bin_obj = obj\nbreak\nfor obj in objs:\nis_match_description\n=\nmatch_description(obj,\n\"recycle\nbin\",\nret_type=\"bool\")\nif is_match_description:\nrecycle_bin_obj = obj\nbreak\nif compost_bin_obj is None or recycle_bin_obj is None:\nrobot.say(\"No compost bin or recycle bin found.\")\nelse:\nfor obj in objs:\nif obj == compost_bin_obj or obj == recycle_bin_obj:\ncontinue\nis_compostable = is_compostable(obj, ret_type=\"bool\")\nif is_compostable:\nrobot.pick_place(obj, compost_bin_obj)\nelse:\nrobot.pick_place(obj, recycle_bin_obj)\n(c) Novel Task\nQ: Prepare \u897f\u7ea2\u67ff\u7092\u86cb (stir-fried tomato and eggs) in the pot.\ningredients = [\"\u897f\u7ea2\u67ff\", \"\u9e21\u86cb\"] [\"tomato\", \"egg\"]\n_, _, objs, _ = robot.detect_objects()\npot_obj = None\nfor obj in objs:\nis_match_description\n=\nmatch_description(obj,\n\"cooking\npot\",\nret_type=\"bool\")\nif is_match_description:\npot_obj = obj\nbreak\nif pot_obj is None:\nrobot.say(\"No cooking pot found.\")\nelse:\nfor ingredient in ingredients:\nfor obj in objs:\nis_match_description\n=\nmatch_description(obj,\ningredient,\nret_type=\"bool\")\nif is_match_description:\nrobot.pick_place(obj, pot_obj)\n(d) Novel Task\nQ: My steak is too bland. Can you help?\nrobot.say(\"Sure, let me see what I can do.\")\n_, _, objs, _ = robot.detect_objects()\nfor obj in objs:\nis_match_description\n=\nmatch_description(obj,\n\"steak\",\nret_type=\"bool\")\nif is_match_description:\nsteak_obj = obj\nbreak\nif steak_obj is None:\nrobot.say(\"No steak found.\")\nelse:\nrobot.say(\"I\u2019m going to season your steak with some salt and pepper.\")\nrobot.pick_place(\"salt\", steak_obj)\nrobot.pick_place(\"pepper\", steak_obj)\nFigure A3 | The one-shot prompt as well as the model outputs for a few test instructions for the robotics\ntasks. When given a single example in the prompt (a), our method can generalize (b-d) to new objects,\nlanguages, and task domains. Red highlight indicates LM generated code being executed by the Python\ninterpreter, and purple highlight indicates LM simulating the code execution. Gray text is for illustration\npurpose only, and not provided to our model. Note that code in the form of robot.<func_name>\ninvokes robot APIs.\n20\nChain of Code: Reasoning with a Language Model-Augmented Code Emulator\nFigure A4 | Full question used in Fig. 1\nHow many countries have I been to? I\u2019ve been to Mumbai, London, Washington, Grand Canyon, Baltimore, Longsheng,\nGuilin, Beijing, Galapagos, Quito, Barcelona, Paris, Prague, Nice, Dehli, Agra, Rome, Florence, Amalfi, Athens,\nM\u00edkonos, M\u00e1laga, Monaco, Berlin, Munich, Innsbruck, Bern, Milan, Lucerne, Gimmelwald (Schilthornbahn), St Moritz,\nSt Petersburg, Helsinki, Amsterdam, Gda\u0144sk, Vancouver, Anchorage, Montreal, Belize, The Bahamas, Jamaica, Hawaii,\nAcadia National Park, Stockholm, Copenhagen, Dover, Lyon, Madrid, Toulouse, Santorini, Oslo, Kusadasi, Souda,\nRhodes, Tallinn, Venice, Vatican City, Naples, Cape Town, Johannesburg, Addis Abeba, Nairobi, Seattle, San\nFrancisco, Chicago, St Louis, Memphis, Chinle, Stanford, New York, Philadelphia, Boston, Miami, New Orleans,\nWalt Disney World Resort, Jacksonville, Las Vegas, Los Angeles, Portland, Salt Lake City, Tahoe City, Phoenix,\nAlbuquerque, Cleveland, Charlottesville, Nags Head, Newfoundland and Labrador, Burlington, Wilmington, Myrtle\nBeach, St Lucia, Barbados, Grenada, Banff, Haiti, Montego Bay, Sao Palo, Rio, Lima, Cusco, Cozumel, Amarillo,\nYosemite National Park, Joshua Tree, Zion National Park, Bryce Canyon National Park, Grand Teton National Park,\nYellowstone National Park, Glacier National Park, Mount Hood, Paso Robles, San Diego, Bend, North Cascades National\nPark, Olympic National Park Visitor Center, Jasper National Park, Sequoia National Park, Kings Canyon National\nPark, Shasta National Forest, Mount Saint Helens, Mount Rainier, Austin, Buenos Aires, El Calafate, El Chalt\u00e9n,\nFitz Roy, Torres del Paine National Park, Puerto Natales, Puerto Varas, Santiago, Marble Caves, Cerro Castillo,\nCoyhaique, Singapore, Casablanca, Marrakesh, Cairo, Jerusalem, Tokyo, Kyoto Prefecture, Taipei City, Taichung\nCity, Krk, Naturpark Puez-Geisler, Ljubljana, Plitvice Lakes National Park, Fairbanks, Juneau, Dallas, Sydney,\nCairns, Brisbane, Hook Island, Charleston, Panama City, Bangkok, Chiang Mai, Bengaluru, Denver, Indianapolis,\nNashville, Blacksburg, Lisbon, Porto, Estes Park, Coeur d\u2019Alene, Hood River, Denali, Sitka, Mexico City, Warsaw,\nGeneva, Auckland, Queenstown, Whitefish, Minneapolis, Sioux Falls, Bozeman, Missoula, Springfield, Skye, Edinburgh,\nHonolulu, Kauai, Haleakal\u00afa National Park, Wrangell-St. Elias National Park & Preserve, Atlanta, Tirana, Corfu,\nSiena.\n21\n"
  },
  {
    "title": "Controllable Human-Object Interaction Synthesis",
    "link": "https://arxiv.org/pdf/2312.03913.pdf",
    "upvote": "22",
    "text": "Controllable Human-Object Interaction Synthesis\nJiaman Li1, Alexander Clegg2, Roozbeh Mottaghi2, Jiajun Wu1, Xavier Puig2\u2020, C. Karen Liu1\u2020\n1Stanford University, 2FAIR, Meta\nPick up the floor lamp, move it to be close to the sofa.\nLift the table, move it to be close to the white table.\nLift the trashcan, move it to be close to the fridge.\nFigure 1. Given an initial object and human state, a language description, and sparse object waypoints in a 3D scene, CHOIS generates\nsynchronized object motion and human motion at the same time.\nAbstract\nSynthesizing semantic-aware, long-horizon, human-\nobject interaction is critical to simulate realistic human\nbehaviors. In this work, we address the challenging problem\nof generating synchronized object motion and human motion\nguided by language descriptions in 3D scenes. We propose\nControllable Human-Object Interaction Synthesis (CHOIS),\nan approach that generates object motion and human motion\nsimultaneously using a conditional diffusion model given a\nlanguage description, initial object and human states, and\nsparse object waypoints. While language descriptions in-\nform style and intent, waypoints ground the motion in the\nscene and can be effectively extracted using high-level plan-\nning methods. Naively applying a diffusion model fails to\npredict object motion aligned with the input waypoints and\ncannot ensure the realism of interactions that require pre-\ncise hand-object contact and appropriate contact grounded\n\u2020 indicates equal contribution.\nby the floor. To overcome these problems, we introduce an\nobject geometry loss as additional supervision to improve\nthe matching between generated object motion and input\nobject waypoints. In addition, we design guidance terms to\nenforce contact constraints during the sampling process of\nthe trained diffusion model.\n1. Introduction\nSynthesizing human behaviors in 3D environments is critical\nfor various applications in computer graphics, embodied\nAI, and robotics. Humans effortlessly navigate and engage\nwithin their surroundings, performing a plethora of tasks\nroutinely. For example, drawing a chair closer to a desk\nto create a workspace, adjusting a floor lamp to cast the\nperfect glow, or neatly storing a suitcase. Each of these tasks\nrequires precise coordination between the human, the object,\nand the surroundings. These tasks are also deeply rooted in\npurpose. Language serves as a powerful tool to articulate and\nconvey these intentions. Synthesizing realistic human and\n1\narXiv:2312.03913v1  [cs.CV]  6 Dec 2023\nobject motion guided by language and scene context is the\ncornerstone of building an advanced AI systems that simulate\ncontinuous human behaviors in diverse 3D environments.\nWhile some existing works study the problem of human-\nscene interaction [18], they are constrained to scenarios with\nstatic objects such as sitting on a chair, neglecting the highly\ndynamic interactions happening frequently in daily life. Re-\ncent advancements have been made in modeling dynamic\nhuman-object interactions, yet these approaches focus solely\non smaller objects [13, 29] or lack the ability to manipulate\ndiverse objects [19, 59]. Manipulating diverse objects of\nlarger size has been explored in recent work [28, 53, 60].\nHowever, these approaches rely on sequences of past interac-\ntion states or complete sequences of object motion, and are\nincapable of synthesizing both object motion and human mo-\ntion from initial states alone. In this work, we aim to advance\nthe field by focusing on synthesizing realistic interactions\ninvolving diverse objects of larger size from language and\ninitial states.\nGenerating continuous human-object interactions from\nlanguage descriptions within 3D scenes poses several chal-\nlenges. First, we need to generate object and human motion\nwhich is realistic and synchronized. The human hands should\nmaintain appropriate contact with objects during interaction\nand object motion should maintain a causal relationship to\nhuman actions. Second, 3D scenes are often cluttered with\nnumerous objects, constraining the space of feasible motion\ntrajectories. Thus, it is essential for interaction synthesis to\naccommodate for environment clutter, rather than operating\nunder the assumption of an empty scene.\nIn this work, we focus on the key problem of synthesizing\nhuman-object interactions in 3D environments from natural\nlanguage commands, generating object motion and human\nmotion guided by language and sparse object waypoints.\nStarting with a language description outlining the desired\nhuman actions, a set of waypoints extracted from the envi-\nronment, and an initial object and human state, our goal is\nto generate motions for both humans and objects. These\nmotions should align with the directives specified in the lan-\nguage input, while also conforming to the environmental\nconstraints defined by waypoint conditions derived from 3D\nscene geometry.\nTo achieve this, we employ a conditional diffusion model\nto generate synchronized object and human motion simulta-\nneously, conditioned on language descriptions, initial states,\nand sparse object waypoints. To improve the accuracy of the\npredicted object motion, we incorporate an object geometry\nloss during training. In addition, we devise guidance terms\napplied during the sampling process to improve the realism\nof the generated interaction. Furthermore, we demonstrate\nthe effectiveness of our learned interaction synthesis mod-\nule within a system that produces continuous realistic and\ncontext-aware interactions given language descriptions and\n3D scenes.\nTo summarize, our work makes the following contribu-\ntions. First, we identify that the combination of language\nand object waypoints provides precise and expressive infor-\nmation for human-object interaction synthesis. We show that\nobject waypoints do not need to be dense or precise, which\nallows us to utilize existing path planning algorithms to gen-\nerate sparse waypoints which represent long-horizon interac-\ntions in complex scenarios. Second, based on this finding,\nwe devise a method that synthesizes human-object interac-\ntion guided by language and sparse waypoints of the object,\nusing a conditional diffusion model. We demonstrate that\nour approach synthesizes realistic interactions on FullBody-\nManipulation dataset [28]. In addition, the learned model\ncan generalize to novel objects in 3D-FUTURE dataset [11].\nThird, we integrate our method into a pipeline that synthe-\nsizes long-horizon environment-aware human-object interac-\ntions from 3D scenes and language input.\n2. Related Work\nMotion Synthesis from Language.\nWith the develop-\nment of large-scale high-quality motion capture datasets\nlike AMASS [30], there has been a growing interest in\ngenerative human motion modeling. BABEL [39] and Hu-\nmanML3D [14] further introduce action labels and lan-\nguage descriptions to enrich the mocap dataset, enabling the\ndevelopment of action-conditioned motion synthesis [36]\nand text-conditioned motion synthesis [14, 37, 49]. Prior\nwork has shown that VAE formulation is effective in gen-\nerating diverse human motion from text [14, 15].\nRe-\ncently, with the success of the diffusion model in this do-\nmain [2, 6, 23, 27, 28, 40, 43, 44, 51, 61, 66], extensive\nwork has explored generating motion from text using condi-\ntioning [8, 24, 50, 63]. In this work, we also take language\ndescriptions as input to guide our generation. Instead of\nsynthesizing human motion alone, we generate both object\nmotion and human motion conditioned on the text.\nMotion Synthesis in 3D Scenes.\nWith the advent of paired\nscene-motion data [1, 16, 17, 56, 69] and paired object-\nmotion data [18, 64], approaches [1, 18, 25, 54, 55, 64]\nhave been developed to generate human interactions such as\nsitting on a chair and reaching a target position in 3D scenes.\nTo populate human-object interactions without training on\npaired scene-motion data, path planning algorithms have\nbeen deployed to generate collision-free paths which then\nguide the human motion generation [18, 33, 65, 67]. Another\nline of work leverages reinforcement learning frameworks\nto train scene-aware policies for synthesizing navigation and\ninteraction motions in static 3D scenes [26, 58]. In this work,\ninstead of focusing on static scenes or objects, we synthe-\nsize interactions with dynamic objects. Also, inspired by\napproaches that decompose scene-aware motion generation\n2\ninto path planning and goal-guided generation phases, we de-\nsign an interaction synthesis module conditioned on sparse\nobject waypoints that can be effectively integrated into a\nscene-aware synthesis pipeline.\nInteraction Synthesis.\nThe field of modeling dynamic\nhuman-object interactions has largely focused on hand mo-\ntion synthesis [7, 62, 68]. Recently, with the advent of full-\nbody motion datasets with hand-object interactions [10, 47],\nmodels [48, 57] have been developed to synthesize full-body\nmotions preceding object grasping. Some recent studies pre-\ndict object motion based on human movements [35], and\nothers [4, 13, 29] have taken this further by synthesizing both\nbody and hand motion, subsequently applying optimization\nto predict object motion. However, these approaches fo-\ncus on smaller objects where hand motion is the primary\nfocus. In terms of manipulating larger objects, some meth-\nods train reinforcement learning policies to synthesize box\nlifting and moving behaviors [19, 32, 59], yet these mod-\nels struggle to generalize to manipulation of diverse objects.\nBased on paired human-object motion data [3, 28, 53], recent\nworks predict interactions from a sequence of past interaction\nstates [53, 60] or an object motion sequence [28], incapable\nof synthesizing interactions in 3D scenes solely from initial\nstates. In this work, we generate synchronized object and hu-\nman motion conditioned on sparse object waypoints, serving\nto ground the resulting trajectories in 3D scenes.\n3. Method\nOur goal is to generate synchronized object and human mo-\ntion, conditioned on a language description, object geometry,\ninitial object and human states, and sparse object waypoints.\nTwo primary challenges arise in this context: first, modeling\nthe complexity of synchronized object and human motion\nwhile also respecting the sparse condition signals; and sec-\nond, ensuring the realism of contact between the human and\nobject. To tackle the generation problem of complex interac-\ntions, we employ a conditional diffusion model to generate\nobject motion and human motion at the same time. However,\nnaively learning a conditional diffusion model to generate\nboth object motion and human motion cannot ensure the pre-\ncise contact between hand and object and the realism of the\ninteraction. Thus, we incorporate several constraints as guid-\nance during the sampling process of our trained diffusion\nmodel. We illustrate our approach in Figure 2.\n3.1. Data Representation\nObject and Human Motion Representation.\nWe denote\nthe human motion as X \u2208 RT \u00d7D, where T and D represent\nthe time steps and dimension of the human pose. Xt, cor-\nresponding to the human pose at frame t, consists of global\njoint positions and 6D continuous rotations [70]. We adopt\nthe widely used parametric human model, SMPL-X [34]\nto reconstruct the human mesh from the pose and shape\nparameters. To represent the object motion, we use two com-\nponents: the global 3D position and the relative rotation. The\nglobal position is represented by the centroid of the object,\nwhile the relative rotation, denoted as Rrel at frame t, is ex-\npressed with respect to the input object\u2019s geometry V such\nthat Vt = RrelV , where Vt represent the vertices of object\nat frame t. We denote the object motion by O \u2208 RT \u00d712.\nObject Geometry Representation.\nWe represent the ob-\nject geometry using the Basis Point Set (BPS) representa-\ntion [38]. Following prior work [28], we begin by sampling\na set of basis points from a sphere with a 1-meter radius.\nSubsequently, for each sampled point, we calculate the mini-\nmum Euclidean distance to the nearest point on the object\u2019s\nmesh. Alongside this, we record the directional vectors from\nthe basis points to their nearest neighbors. The resulting\nBPS representation is denoted as G \u2208 R1024\u00d73, represent-\ning 1024 sampled points each with a 3-dimensional vector\nindicating their spatial relationship to the object\u2019s surface.\nInput Condition Representation.\nWe first use an MLP\nto project the object BPS representation G to a low-\ndimensional vector which is then broadcasted to each frame\ndenoted as \u02c6G \u2208 RT \u00d7256 following [28]. We then adopt\na masked motion data representation denoted as S\n\u2208\nRT \u00d7(12+D) to represent the initial states and waypoint con-\nditions. The initial state contains the human pose and object\npose at the first frame. The waypoint conditions consist of\na series of 2D object positions for every 30 frames, and a\n3D object position at the final frame. The remainder of S is\npadded with zeros. The encoded object geometry vector and\nthe masked motion condition vector are then concatenated,\nserving as part of the input for our denoising network. For ef-\nfectively integrating language conditions as input, we utilize\nCLIP [41] as a text encoder to extract language embeddings.\n3.2. Interaction Synthesis Model\nConditional Diffusion Model.\nIn our framework, we uti-\nlize a conditional diffusion model [21] to generate synchro-\nnized object and human motion. To improve the realism\nof hand-object interaction, our model also predicts con-\ntact labels H \u2208 RT \u00d72 for both the left and right hands.\nThese predicted contact labels play a crucial role in guiding\nthe sampling process, ensuring more accurate and realis-\ntic hand-object contacts in the generated motion sequence.\nThe complete data representation in our model is denoted as\n\u03c4 = {X, O, H}, encapsulating motion and contact data.\nThe conditional signals of our model, denoted as c, in-\nclude initial states, sparse object waypoints, the object BPS\nrepresentation, and language descriptions. The diffusion\nmodel consists of a forward diffusion process that progres-\nsively adds noise to the clean data \u03c40 and a reverse diffusion\nprocess which is trained to reverse this process. The forward\n3\nNoise level \ud835\udc5b\nPosition \nEmbedding\nAdd\nText\nObject \nGeometry \nMLP\n\ud835\udc39!\"#$%!$( \u0302\ud835\udf0f&)\nTraining\nSampling\nCLIP\n\ud835\udf0f\"\n\ud835\udc52'\n\u0302\ud835\udf0f#\n\ud835\udc39())$( \u0302\ud835\udf0f&)\n\ud835\udc39\"*+( \u0302\ud835\udf0f&)\nGuidance\nTransformer\nLinear\nLinear\nRepeat T steps\n\u2207!!\ud835\udc39( \u0302\ud835\udf0f\")\nEmbedding\n\ud835\udc46\n%\ud835\udc3a\nInitial States & \nObject Waypoints \nMLP\nEmbedding\nMasked Condition Representation\nFigure 2. Method Overview. Given an object geometry, we use the BPS representation to encode the geometry and an MLP to project the\nfeatures into a low-dimensional vector. This feature vector is concatenated with masked pose states to form conditions for the denoising\nnetwork. During sampling, we use analytical functions to compute gradients and perturb the generation to satisfy our defined constraints.\ndiffusion process introduces noise for N steps formulated\nusing a Markov chain,\nq(\u03c4n|\u03c4n\u22121) := N(\u03c4n;\np\n1 \u2212 \u03b2n\u03c4n\u22121, \u03b2nI),\n(1)\nq(\u03c41:N|\u03c40) :=\nN\nY\nn=1\nq(\u03c4n|\u03c4n\u22121),\n(2)\nwhere \u03b2n represents a fixed variance schedule and I is an\nidentity matrix. Our goal is to learn a model p\u03b8 to reverse\nthe forward diffusion process,\np\u03b8(\u03c4n\u22121|\u03c4n, c) := N(\u03c4n\u22121; \u00b5\u03b8(\u03c4n, n, c), \u03a3n),\n(3)\nwhere \u00b5\u03b8 denotes the predicted mean and \u03a3n is a fixed\nvariance. Learning the mean can be re-parameterized as\nlearning to predict the clean data representation \u03c40. The\nobjective [21] is defined as\nL = E\u03c40,n||\u02c6\u03c4\u03b8(xn, n, c) \u2212 \u03c40||1.\n(4)\nModel Architecture.\nWe employ a transformer architec-\nture [52] as our denoising network. Our input consists of\nobject geometry conditions \u02c6G, masked motion conditions S,\nand noisy data representation \u03c4n at noise level n. The input\nis projected to a sequence of feature vectors using a linear\nlayer. We employ an MLP to embed the noise level n. Then\nwe combine the noise level embedding and the language\nembedding to form a single embedding vector denoted as en.\nThe embedding vector en has the same dimension as these\nfeature vectors and is fed to the transformer along with these\nvectors. The final prediction \u02c6\u03c40 is made by projecting the\nupdated feature vectors of the transformer excluding the time\nstep corresponding to the embedding en. The interaction\nsynthesis model is illustrated in Figure 2.\nObject Geometry Loss.\nDuring the training phase, we\nincorporate an additional loss to improve the object motion\nprediction. Utilizing the Basis Point Set (BPS) representa-\ntion, we initially compute the nearest neighbor points on\nthe object mesh in rest pose for each of the fixed set of\npoints. From these, we sample 100 points out of the 1024\nnearest neighbors to capture a rough outline of the object\u2019s\nshape. These selected points are defined as Krest \u2208 R100\u00d73,\nrepresenting our selected object vertices at rest pose.\nAt each time step in our model, the predicted object rota-\ntion (converted to relative rotation with respect to the object\ngeometry in rest pose) and position are employed to calcu-\nlate the corresponding positions of these selected vertices.\nThis is represented by the following equation, where \u02c6Rt and\n\u02c6dt denote the predicted rotation and translation of the object,\nand Kt refers to the ground truth vertices at time step t. The\nobject geometry loss is computed as\nLobj =\nT\nX\nt=1\n|| \u02c6\nRtKrest + \u02c6dt \u2212 Kt||1.\n(5)\nThis loss function plays a critical role in guiding the model\nto accurately predict the transformation of the object.\n3.3. Guidance\nDuring the training phase of our interaction synthesis model,\nthere are no explicit contact constraints enforced in the losses.\nIncorporating loss terms such as hand-object contact loss,\nand object-floor penetration loss poses a challenge for train-\ning. First, these types of loss terms are computationally ex-\npensive and would slow down training significantly. Second,\nintroducing more loss terms requires meticulously balanc-\ning different losses which usually necessitates re-training\nmodels with different settings. Instead, enforcing these con-\nstraints during test time is more flexible and makes it easier\nto select appropriate weights for different terms. Thus, to\nrefine our generated interactions, we propose the application\nof guidance during the sampling process.\nIn the diffusion model framework, classifier guidance is\ncommonly applied during test time to control the generation\nprocess in order to satisfy specific objectives or constraints.\nA typical approach to applying classifier guidance [9] is\nto perturb the noisy predicted mean at each denoising step.\nThis is formulated as \u02dc\u00b5 = \u00b5 \u2212 \u03b1\u03a3n\u2207\u00b5F(\u00b5), where \u00b5\ndenotes the predicted mean at denoising step n defined by\nEquation 3. F represents a learned or analytical function\nthat determines how much the predicted mean should be\npenalized and \u03b1 represents the strength of the perturbation.\n4\nThis guidance computes the gradient with respect to the\nnoisy mean, requiring F to be trained on noisy data or a\ndeterministic function designed for noisy data. Another\napproach is reconstruction guidance [22], which has proven\nto be effective for controlling the generation process in prior\nwork [24, 25, 42]. Instead of perturbing the noisy mean, it\nperturbs the predicted clean data representation \u02c6\u03c40 using the\ngradient with respect to the noisy input data representation\n\u03c4n. The process is formally represented as\n\u02dc\u03c40 = \u02c6\u03c40 \u2212 \u03b1\u03a3n\u2207\u03c4nF(\u02c6\u03c40).\n(6)\nIn this work, we leverage reconstruction guidance [22] in\nthe sampling process as we empirically found it to be more\nstable. We define multiple analytical functions as guidance\nterms which we will introduce in the following sections.\nHand Object Contact Guidance.\nWe have implemented\na specialized contact guidance function to improve the hand-\nobject contact accuracy for frames generated by our model.\nThis function is specifically designed to address cases where\na noticeable distance exists between the hands and the object,\nthereby improving the realism and precision of the interac-\ntion. The contact guidance function is defined as follows:\nFcontact = Ml||Jl \u2212 Vl||1 + Mr||Jr \u2212 Vr||1.\n(7)\nIn this equation, Ml and Mr are binary masks for the left\nand right hand, respectively. These masks are derived from\nthe predicted contact labels H, with Ml, Mr \u2208 RT \u00d71 and\nare defined as Ml, Mr = (H > 0.95). This thresholding\nidentifies frames where contact is likely to occur. Jl, Jr \u2208\nRT \u00d73 represent the positions of the left and right hands.\nMeanwhile, Vl, Vr \u2208 RT \u00d73 denotes the nearest neighbor\npoints on the object mesh to the respective hand positions.\nFeet-Floor Contact Guidance.\nWhen generating joint po-\nsitions and rotations, our model operates without aware-\nness of the body\u2019s shape. Consequently, using the SMPL-X\nmodel [34] with predicted root positions, joint rotations, and\na test subject\u2019s specific body shape parameters to reconstruct\nthe human mesh can sometimes lead to scenarios where the\nfeet do not touch the floor. To rectify this, we implement a\nguidance term that encourages realistic feet-floor contact.\nThe joint positions of the left and right toes are repre-\nsented as Jl and Jr, respectively. We identify the supporting\nfoot in each frame by comparing the z components of these\ntwo joints at each frame. We also introduce a threshold\nheight h = 0.02 meters, which is determined from the anal-\nysis of foot height in the ground truth motion. The guidance\nterm is defined as follows:\nFfeet = ||min(Jz\nl , Jz\nr ) \u2212 h||2.\n(8)\nThis function computes the norm of the vertical difference\nbetween the lowest point of either toe and the threshold\nheight h.\nObject-Floor Penetration Guidance.\nTo address the is-\nsue of generated object states potentially penetrating the\nfloor, we integrate an additional guidance function into the\nsampling process. Given that our floor is positioned at the\nplane where z = 0, we define the guidance term as follows:\nFobj = ||min(V z, 0)||1,\n(9)\nwhere V z represents the z-coordinate of the object vertices.\nDuring the inference phase, we apply multiple guidance\nconcurrently defined as follows,\nFall = \u03bb1Fcontact + \u03bb2Ffeet + \u03bb3Fobj,\n(10)\nwhere \u03bb1, \u03bb2, \u03bb3 denote the loss weights for different terms.\nWe apply the guidance in the last 10 denoising steps only\nsince the prediction in the early steps is extremely noisy.\n4. Experiments\nWe first introduce the datasets and evaluation metrics. Then\nwe show comparisons of our proposed approach against the\nbaselines. We further conduct a human perceptual study to\ncomplement our evaluation and ablation study to verify the\neffectiveness of our proposed guidance terms. Moreover, we\ndemonstrate an application that generates long-term inter-\nactions conditioned on object waypoints extracted from 3D\nscenes.\n4.1. Datasets\nThe FullBodyManipulation dataset [28] consists of 10\nhours of high-quality, paired object and human motion, in-\ncluding interaction with 15 different objects. However, our\nstudy does not encompass the generation of motion for ar-\nticulated objects, leading us to exclude sequences related to\ntwo such objects (vacuum and mop). We employ this dataset\nboth for training our interaction model and for evaluating\nthe generated results. The training set comprises 15 subjects,\nwith an additional 2 subjects designated for testing, adhering\nto the dataset partitioning used in OMOMO [28].\nThe 3D-FUTURE dataset [12] includes 3D models of vari-\nous furniture items. From this dataset, we select 17 objects\nrepresenting diverse types (such as chairs, tables, floor lamps,\nand boxes). This dataset serves to test our model\u2019s ability\nto generalize to objects it has not previously encountered.\nGiven that the 3D-FUTURE dataset only includes 3D mod-\nels, we integrate object position data from the testing set of\nthe FullBodyManipulation dataset [28] for evaluation.\n4.2. Evaluation Metrics\nCondition Matching Metric: This metric calculates the\nEuclidean distance between the predicted and input object\nwaypoints. It includes the start object position error (Ts),\nend object position error (Te), and waypoint errors (Txy),\nall measured in centimeters (cm).\n5\nCondition Matching\nHuman Motion\nInteraction\nGT Difference\nMethod\nTs \u2193\nTe \u2193\nTxy \u2193\nHfeet \u2193\nFS\u2193\nCprec \u2191\nCrec \u2191\nCF1 \u2191\nC% \u2191\nPhand \u2193\nMPJPE\u2193\nTroot \u2193\nTobj \u2193\nOobj \u2193\nOMOMO [28]\n0\n0\n0\n7.21\n0.41\n0.68\n0.56\n0.57\n0.54\n0.51\n21.73\n36.62\n17.12\n1.21\nCHOIS w/o Lobj\n5.76\n14.16\n8.44\n6.55\n0.40\n0.75\n0.50\n0.55\n0.43\n0.66\n14.34\n21.97\n15.53\n0.98\nCHOIS w/o Fall\n1.75\n6.61\n2.69\n6.64\n0.38\n0.78\n0.49\n0.55\n0.41\n0.65\n15.23\n24.13\n11.51\n0.99\nCHOIS (ours)\n1.71\n6.31\n2.87\n4.20\n0.35\n0.80\n0.64\n0.67\n0.54\n0.59\n15.30\n24.43\n12.53\n0.99\nTable 1. Interation synthesis on the FullBodyManipulation dataset [28].\nCondition Matching Human Motion\nInteraction\nTs \u2193 Te \u2193\nTxy \u2193 Hfeet \u2193 FS\u2193 C% \u2191 Phand \u2193\nOMOMO [28]\n0\n0\n0\n6.39\n0.43\n0.40\n0.10\nCHOIS w/o Lobj 6.70 13.73\n7.99\n5.68\n0.41\n0.36\n0.30\nCHOIS w/o Fall\n5.75\n7.96\n2.68\n5.84\n0.39\n0.33\n0.26\nCHOIS (ours)\n4.12\n7.35\n2.92\n3.75\n0.38\n0.48\n0.15\nTable 2. Interaction synthesis on the 3D-FUTURE dataset [11].\nHuman Motion Quality Metric: This metric encompasses\nthe foot sliding score (FS) and foot heights (Hfeet). FS is the\nweighted average of accumulated translation in the xy plane,\nfollowing prior work [20], measured in centimeters (cm).\nHfeet assesses the height of the feet, also in centimeters.\nInteraction Quality Metric: This metric assesses the accu-\nracy of hand-object interactions, encompassing both contacts\nand penetrations. For contact accuracy, it employs precision\n(Cprec), recall (Crec), and F1 score (CF1) metrics following\nprior work [28]. Additionally, it includes contact percentage\n(C%), determined by the proportion of frames where con-\ntact is detected. To compute the penetration score (Phand),\neach vertex of the hand Vi is used to query the precomputed\nobject\u2019s Signed Distance Field (SDF). This process yields\na corresponding distance value di for each vertex. The pen-\netration score is then derived by computing the average of\nthe negative distance values (representing penetration), for-\nmalized as 1\nn\nPn\ni=1 |min(di, 0)|, measured in centimeters\n(cm).\nGround Truth (GT) Difference Metric: This metric mea-\nsures the deviation of generated results from the ground\ntruth motion. It comprises the mean per-joint position error\n(MPJPE), translation error of the root joint (Troot), and ob-\nject position error (Tobj), all computed using the Euclidean\ndistance between the predicted and actual ground truth posi-\ntions in centimeters (cm). Additionally, this metric includes\nthe root joint orientation error (Oroot) and the object ori-\nentation error (Oobj). These errors are calculated with the\nFrobenius norm of the rotational difference, formulated as\n||RpredR\u22121\ngt \u2212 I||2 where Rpred and Rgt represent the pre-\ndicted and ground truth rotation matrices respectively.\n4.3. Results\nBaselines.\nAs there is no prior work presenting a solution\nfor our task, we adapt the most related work, OMOMO [28],\nto our problem setting in order to evaluate against a base-\nline. This method was designed for synthesizing human\nmotion from a provided object motion trajectory. Since\nOMOMO [28] requires a sequence of object states to gen-\nerate full-body human poses, we implement a linear inter-\npolation strategy for the object positions. This interpolation\nis based on the given start and end positions of the object,\nas well as predefined waypoints in the xy-plane. We also\nmaintain a consistent object rotation, using the orientation\nfrom the initial frame throughout the entire sequence. Ad-\nditionally, we evaluate our approach CHOIS against two\nablations: CHOIS w/o Lobj and CHOIS w/o Fall. CHOIS\nw/o Lobj is trained as a conditional diffusion model but does\nnot include an additional object geometry loss. This variant\nallows us to understand the baseline performance of the dif-\nfusion model in a straightforward setup. In contrast, CHOIS\nw/o Fall incorporates the object geometry loss in its training\nprocess but operates without guidance during inference. This\napproach lets us explore the effectiveness of object geometry\nloss during training while assessing the model\u2019s capability\nin the absence of guidance.\nResults on the FullBodyManipulation Dataset.\nWe eval-\nuate our approach using objects from the FullBodyManipu-\nlation dataset [28] as shown in Table 1. Introducing object\ngeometry loss notably improves the condition matching met-\nric. Furthermore, adding guidance during inference leads\nto better contact accuracy, reduced hand-object penetration,\nand less foot floating. Note that OMOMO [28] has zero\ndeviation from the input object trajectory since it only pre-\ndicts human motion and does not change the object motion\ninput. We also showcase qualitative comparisons in Figure 3.\nNote that OMOMO\u2019s object motion is updated via linear\ninterpolation and thus cannot follow the text prompt (row 2,\nlifting the table above the head).\nResults on the 3D-FUTURE Dataset.\nTo test our model\u2019s\nability to generalize to new objects, we conduct evaluations\nusing the 3D-FUTURE dataset [11]. As shown in Table 2,\nour proposed method outperforms the baseline and the two\nablations. We also provide qualitative results in Figure 3.\nHuman Perceptual Study.\nWe conduct two human per-\nceptual studies to further complement the evaluation of our\napproach. The first study assesses the consistency between\nthe generated interactions and the text input. The second\nstudy evaluates the overall quality of these generated interac-\ntions. For each of these studies, we generate 100 sequences\n6\n(a) OMOMO\n(b) CHOIS w/o \ud835\udc3f!\"# \n(c) CHOIS w/o \ud835\udc39$%%\n(d) CHOIS (ours)\nPick up the clothes stand, move it, and put it down.\nLift the table above your head, walk, and put the table down.\nFacing the back of the chair, lift the chair, move the chair, and then place the chair on the floor.\n.\nFigure 3. Qualitative results. The first two rows show the results on the FullBodyManipulation dataset [28], and the last row shows the\nresults on the 3D-FUTURE dataset [11].\nCondition Matching\nHuman Motion\nInteraction\nGT Difference\nMethod\nTs \u2193\nTe \u2193\nTxy \u2193\nHfeet \u2193\nFS\u2193\nCprec \u2191\nCrec \u2191\nCF1 \u2191\nC% \u2191\nPhand \u2193\nMPJPE\u2193\nTroot \u2193\nTobj \u2193\nOobj \u2193\nCHOIS w/o Fcontact\n1.70\n6.42\n2.70\n3.93\n0.32\n0.78\n0.49\n0.55\n0.41\n0.65\n15.41\n23.63\n11.44\n0.99\nCHOIS w/o Ffeet\n1.72\n6.34\n2.90\n6.65\n0.39\n0.81\n0.64\n0.66\n0.54\n0.58\n15.44\n25.09\n13.31\n0.99\nCHOIS w/o Fall\n1.75\n6.61\n2.69\n6.64\n0.38\n0.78\n0.49\n0.55\n0.41\n0.65\n15.23\n24.13\n11.51\n0.99\nCHOIS (ours)\n1.71\n6.31\n2.87\n4.20\n0.35\n0.80\n0.64\n0.67\n0.54\n0.59\n15.30\n24.43\n12.53\n0.99\nTable 3. Ablation study on the FullBodyManipulation dataset [28]. We measure the effect of different guidance terms in the human and\nobject motion generation.\n72\n2\n26\n4\n58\n38\n47\n15\n38\n35\n60\n6\n57\n3\n40\n3\n53\n43\n54\n7\n39\n37\n60\n3\nCHOIS w/o \ud835\udc3f!\"# \nCHOIS w/o \ud835\udc39$%%\nOMOMO\nGT\nFigure 4. Results of human perceptual studies. The numbers\nshown in the chart represent the percentage (%).\nusing each method, including our CHOIS, the baseline, our\nablations, and the ground truth. This results in a set of 400\npairs. We employ Amazon Mechanical Turk (AMT) for eval-\nuation. Each sequence pair is reviewed by 10 different AMT\nworkers. The results are illustrated in Figure 4. Given that\nOMOMO [28] generates human motions based solely on\ninterpolated object states and does not incorporate language\nCondition Matching Human Motion\nInteraction\nTs \u2193 Te \u2193\nTxy \u2193 Hfeet \u2193 FS\u2193 C% \u2191 Phand \u2193\nCHOIS w/o Fall\n1.50\n7.19\n5.10\n6.01\n0.43\n0.49\n0.70\nCHOIS\n2.22\n9.94\n5.73\n4.57\n0.46\n0.63\n0.69\nCHOIS\u2217 w/o Fall 6.29\n9.39\n5.26\n4.77\n0.39\n0.42\n0.55\nCHOIS\u2217\n5.62 12.08\n5.95\n4.27\n0.41\n0.65\n0.32\nTable 4. Long-term interaction synthesis results on the FullBody-\nManipulation [28] and 3D-FUTURE datasets [11]. \u2217 represents the\nresults on the 3D-FUTURE dataset.\nconditions, our method demonstrates superior performance\nin aligning with text input. Moreover, our approach shows\nimprovements over the CHOIS w/o Lobj, with both CHOIS\nw/o Fall and CHOIS exhibiting comparable proficiency in\ntext alignment. Regarding interaction quality, our method\nalso surpasses all baseline and model ablations.\n7\nPick up floor lamp, move floor lamp to be close to the sofa.\nLift a box, move the box and put down on the table.\nFigure 5. Long-term interaction synthesis. Given language descriptions, a 3D scene with semantic labels, and initial human and object\nstates, we synthesize long-term human-object interactions. The initial state is shown in green.\nPull the clothes stand and set it back down.\nFigure 6. Results of interaction synthesis using the same text\ninput but different waypoints. The initial state is in green.\n4.4. Ablation Study\nWe conduct an ablation study to validate the effectiveness\nof our proposed guidance terms. As shown in Table 3, our\nhand-object contact guidance and feet-floor contact guidance\nare both critical. Without the hand-object contact guidance,\nthe contact percentage degrades obviously. Without the\nfeet-floor contact guidance, the height of the feet increases\nindicating there exists severe foot floating issues. We are\nnot ablating object-floor penetration guidance as object-floor\npenetration issues are not common and this term is primarily\ndesigned for preventing penetration artifacts in qualitative\nresults.\n4.5. Application\nThis section presents a practical application of our method,\nenabling the synthesis of human-object interactions within\n3D scenes, driven by language descriptions. We utilize 3D\nscenes from the Replica Dataset [45].\nThe process begins by composing language descriptions\nthat specify the desired interactions, identifying both the\nobjects involved and their intended positions. For example,\nthe language description can be \u201cpull the floor lamp to be\nclose to a shelf\u201d. We also define a set of primitive functions\nused to sample target 3D positions from 3D scenes. This\nset includes functions like sampling points on an object\u2019s\nsurface or near it. GPT-3 [5] is used to extract key infor-\nmation including the interaction object and target objects,\nand to select the appropriate primitive functions from our\npredefined function set. Combining the information with the\nsemantic labels of the scene point cloud, we can determine\nthe target 3D positions.\nWe leverage Habitat [31, 46] to generate collision-free\npaths within the scene given the start and target object po-\nsitions. However, as Habitat provides waypoints without\ncorresponding time steps, we need to adapt these to our\nlearned module. We apply heuristics to create waypoints at\nfixed intervals of 30 frames, which serve as the input con-\nditions for our interaction synthesis model. An example of\nthis application is shown in Figure 5, demonstrating how our\nlearned interaction synthesis model effectively synthesizes\nhuman-object motion following a description in a 3D scene.\nTable 4 includes a quantitative evaluation of the generated\nmotion. In addition, we showcase the results using the same\ntext input but different waypoints in Figure 6, demonstrating\nthe effectiveness of the control using object waypoints.\n5. Conclusion\nIn conclusion, our work addresses the problem of human-\nobject interaction synthesis conditioned on language descrip-\ntions and sparse object waypoints. By employing a con-\nditional diffusion model, we successfully generate object\nand human motions that are not only synchronized but also\nresonate with given language descriptions. We incorporate\nobject geometry loss during training which significantly im-\nproves the performance of object motion generation. We\nalso propose effective guidance terms used during the sam-\npling process which enhance the realism of the generated\nresults. Moreover, we demonstrate that our learned interac-\ntion module can be integrated into a pipeline that synthesizes\nlong-term interactions given language and 3D scenes.\n8\nAcknowledgments.\nThis work is in part supported by the\nStanford Institute for Human-Centered AI (HAI), NSF CCRI\n#2120095, ONR MURI N00014-22-1-2740, and Meta. Part\nof the research was done during Jiaman Li\u2019s internship at\nFAIR, Meta.\nReferences\n[1] Joao Pedro Araujo, Jiaman Li, Karthik Vetrivel, Rishi Agar-\nwal, Deepak Gopinath, Jiajun Wu, Alexander Clegg, and\nC Karen Liu. Circle: Capture in rich contextual environments.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 2\n[2] German Barquero, Sergio Escalera, and Cristina Palmero.\nBelfusion: Latent diffusion for behavior-driven human motion\nprediction. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 2317\u20132327, 2023. 2\n[3] Bharat Lal Bhatnagar, Xianghui Xie, Ilya A. Petrov, Cris-\ntian Sminchisescu, Christian Theobalt, and Gerard Pons-Moll.\nBehave: Dataset and method for tracking human object in-\nteractions. In Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 15935\u201315946, 2022. 3\n[4] Jona Braun, Sammy Christen, Muhammed Kocabas, Emre\nAksan, and Otmar Hilliges.\nPhysically plausible full-\nbody hand-object interaction synthesis.\narXiv preprint\narXiv:2309.07907, 2023. 3\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information\nprocessing systems, 33:1877\u20131901, 2020. 8\n[6] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao\nChen, and Gang Yu. Executing your commands via motion\ndiffusion in latent space. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 18000\u201318010, 2023. 2\n[7] Sammy Christen, Muhammed Kocabas, Emre Aksan, Jemin\nHwangbo, Jie Song, and Otmar Hilliges. D-grasp: Physically\nplausible dynamic grasp synthesis for hand-object interac-\ntions. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 20577\u201320586,\n2022. 3\n[8] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav\nGolyanik, and Christian Theobalt. Mofusion: A framework\nfor denoising-diffusion-based motion synthesis. In Computer\nVision and Pattern Recognition (CVPR), 2023. 2\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural information\nprocessing systems, 34:8780\u20138794, 2021. 4\n[10] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed\nKocabas, Manuel Kaufmann, Michael J. Black, and Otmar\nHilliges. ARCTIC: A dataset for dexterous bimanual hand-\nobject manipulation. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2023. 3\n[11] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang\nZhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furni-\nture shape with texture. International Journal of Computer\nVision (IJCV), 129(12):3313\u20133337, 2021. 2, 6, 7\n[12] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang\nZhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furni-\nture shape with texture. International Journal of Computer\nVision, 129:3313\u20133337, 2021. 5\n[13] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Chris-\ntian Theobalt, and Philipp Slusallek. Imos: Intent-driven\nfull-body motion synthesis for human-object interactions. In\nEurographics, 2023. 2, 3\n[14] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,\nXingyu Li, and Li Cheng. Generating diverse and natural 3d\nhuman motions from text. In Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5152\u20135161, 2022. 2\n[15] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t:\nStochastic and tokenized modeling for the reciprocal genera-\ntion of 3d human motions and texts. In European Conference\non Computer Vision, pages 580\u2013597. Springer, 2022. 2\n[16] Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard\nPons-Moll. Human poseitioning system (hps): 3d human\npose estimation and self-localization in large scenes from\nbody-mounted sensors. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2021. 2\n[17] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and\nMichael J Black.\nResolving 3d human pose ambiguities\nwith 3d scene constraints. In International Conference on\nComputer Vision (ICCV), pages 2282\u20132292, 2019. 2\n[18] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito,\nJimei Yang, Yi Zhou, and Michael Black. Stochastic scene-\naware motion prediction. In International Conference on\nComputer Vision (ICCV), pages 11354\u201311364, 2021. 2\n[19] Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael\nBlack, Sanja Fidler, and Xue Bin Peng. Synthesizing physical\ncharacter-scene interactions. In SIGGRAPH 2023 Conference\nPapers, 2023. 2, 3\n[20] Chengan He, Jun Saito, James Zachary, Holly Rushmeier,\nand Yi Zhou. Nemf: Neural motion fields for kinematic ani-\nmation. Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 6\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems (NeurIPS), 33:6840\u20136851, 2020. 3, 4\n[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. arXiv:2204.03458, 2022. 5\n[23] Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu\nLiu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusion-\nbased generation, optimization, and planning in 3d scenes.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 2\n[24] Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwa-\njanakorn, and Siyu Tang. Gmd: Controllable human mo-\ntion synthesis via guided diffusion models. arXiv preprint\narXiv:2305.12577, 2023. 2, 5\n[25] Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu,\nJustin Johnson, David Fouhey, and Leonidas Guibas. Nifty:\nNeural object interaction fields for guided human motion\nsynthesis. arXiv preprint arXiv:2307.07511, 2023. 2, 5\n9\n[26] Jiye Lee and Hanbyul Joo. Locomotion-action-manipulation:\nSynthesizing human-scene interactions in complex 3d envi-\nronments. arXiv preprint arXiv:2301.02667, 2023. 2\n[27] Jiaman Li, Karen Liu, and Jiajun Wu. Ego-body pose es-\ntimation via ego-head pose estimation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17142\u201317151, 2023. 2\n[28] Jiaman Li, Jiajun Wu, and C Karen Liu. Object motion guided\nhuman motion synthesis. ACM Trans. Graph., 42(6), 2023. 2,\n3, 5, 6, 7\n[29] Quanzhou Li, Jingbo Wang, Chen Change Loy, and Bo\nDai.\nTask-oriented human-object interactions genera-\ntion with implicit neural representations.\narXiv preprint\narXiv:2303.13129, 2023. 2, 3\n[30] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-\nard Pons-Moll, and Michael J Black. Amass: Archive of\nmotion capture as surface shapes. In International Confer-\nence on Computer Vision (ICCV), pages 5442\u20135451, 2019.\n2\n[31] Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\nLiu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv\nBatra. Habitat: A Platform for Embodied AI Research. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2019. 8\n[32] Josh Merel, Saran Tunyasuvunakool, Arun Ahuja, Yuval\nTassa, Leonard Hasenclever, Vu Pham, Tom Erez, Greg\nWayne, and Nicolas Heess. Catch & carry: reusable neu-\nral controllers for vision-guided whole-body tasks. ACM\nTransactions on Graphics (TOG), 39(4):39\u20131, 2020. 3\n[33] Aymen Mir, Xavier Puig, Angjoo Kanazawa, and Gerard\nPons-Moll. Generating continual human motion in diverse 3d\nscenes. arXiv preprint arXiv:2304.02061, 2023. 2\n[34] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo\nBolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and\nMichael J. Black. Expressive body capture: 3d hands, face,\nand body from a single image. In Conference on Computer\nVision and Pattern Recognition (CVPR), pages 10975\u201310985,\n2019. 3, 5\n[35] Ilya A Petrov, Riccardo Marin, Julian Chibane, and Gerard\nPons-Moll. Object pop-up: Can we infer 3d objects and their\nposes from human interactions alone?\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4726\u20134736, 2023. 3\n[36] Mathis Petrovich, Michael J Black, and G\u00a8ul Varol. Action-\nconditioned 3d human motion synthesis with transformer vae.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 10985\u201310995, 2021. 2\n[37] Mathis Petrovich, Michael J Black, and G\u00a8ul Varol. Temos:\nGenerating diverse human motions from textual descriptions.\nIn European Conference on Computer Vision, pages 480\u2013497.\nSpringer, 2022. 2\n[38] Sergey Prokudin, Christoph Lassner, and Javier Romero. Ef-\nficient learning on point clouds with basis point sets. In\nInternational Conference on Computer Vision (ICCV), pages\n4332\u20134341, 2019. 3\n[39] Abhinanda R Punnakkal, Arjun Chandrasekaran, Nikos\nAthanasiou, Alejandra Quiros-Ramirez, and Michael J Black.\nBabel: Bodies, action and behavior with english labels. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 722\u2013731, 2021. 2\n[40] Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H\nBermano, and Daniel Cohen-Or. Single motion diffusion.\narXiv preprint arXiv:2302.05905, 2023. 2\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3\n[42] Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris\nKitani, Karsten Kreis, Sanja Fidler, and Or Litany. Trace and\npace: Controllable pedestrian animation via guided trajec-\ntory diffusion. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13756\u2013\n13766, 2023. 5\n[43] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H Bermano.\nHuman motion diffusion as a generative prior. arXiv preprint\narXiv:2303.01418, 2023. 2\n[44] Yi Shi, Jingbo Wang, Xuekun Jiang, and Bo Dai. Controllable\nmotion diffusion model. arXiv preprint arXiv:2306.00416,\n2023. 2\n[45] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik\nWijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl\nRen, Shobhit Verma, et al. The replica dataset: A digital\nreplica of indoor spaces. arXiv preprint arXiv:1906.05797,\n2019. 8\n[46] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans,\nYili Zhao, John Turner, Noah Maestre, Mustafa Mukadam,\nDevendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan,\nVladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech\nGaluba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra\nMalik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training\nhome assistants to rearrange their habitat. In Advances in\nNeural Information Processing Systems (NeurIPS), 2021. 8\n[47] Omid Taheri, Nima Ghorbani, Michael J. Black, and Dim-\nitrios Tzionas. GRAB: A dataset of whole-body human grasp-\ning of objects. In European Conference on Computer Vision\n(ECCV), 2020. 3\n[48] Omid Taheri, Vasileios Choutas, Michael J Black, and Dim-\nitrios Tzionas. Goal: Generating 4d whole-body motion for\nhand-object grasping. In Conference on Computer Vision and\nPattern Recognition (CVPR), pages 13263\u201313273, 2022. 3\n[49] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and\nDaniel Cohen-Or. Motionclip: Exposing human motion gen-\neration to clip space. In European Conference on Computer\nVision, pages 358\u2013374. Springer, 2022. 2\n[50] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H\nBermano, and Daniel Cohen-Or. Human motion diffusion\nmodel. In International Conference on Learning Representa-\ntions (ICLR), 2023. 2\n[51] Jonathan Tseng, Rodrigo Castellon, and C Karen Liu. Edge:\nEditable dance generation from music. In Computer Vision\nand Pattern Recognition (CVPR), 2023. 2\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\n10\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems (NIPS), pages 5998\u20136008,\n2017. 4\n[53] Weilin Wan, Lei Yang, Lingjie Liu, Zhuoying Zhang, Ruixing\nJia, Yi-King Choi, Jia Pan, Christian Theobalt, Taku Komura,\nand Wenping Wang. Learn to predict how humans manipulate\nlarge-sized objects from interactive motions. IEEE Robotics\nand Automation Letters, 7(2):4702\u20134709, 2022. 2, 3\n[54] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiao-\nlong Wang. Synthesizing long-term 3d human motion and\ninteraction in 3d scenes. In Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 9401\u20139411, 2021. 2\n[55] Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin,\nand Bo Dai. Towards diverse and natural scene-aware 3d\nhuman motion synthesis. In Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 20460\u201320469, 2022.\n2\n[56] Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang,\nand Siyuan Huang. Humanise: Language-conditioned hu-\nman motion generation in 3d scenes. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2022. 2\n[57] Yan Wu, Jiahao Wang, Yan Zhang, Siwei Zhang, Otmar\nHilliges, Fisher Yu, and Siyu Tang. Saga: Stochastic whole-\nbody grasping with contact. In European Conference on\nComputer Vision (ECCV), pages 257\u2013274, 2022. 3\n[58] Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei\nZhang, Bo Dai, Dahua Lin, and Jiangmiao Pang.\nUni-\nfied human-scene interaction via prompted chain-of-contacts.\narXiv preprint arXiv:2309.07918, 2023. 2\n[59] Zhaoming Xie, Jonathan Tseng, Sebastian Starke, Michiel\nvan de Panne, and C Karen Liu. Hierarchical planning and\ncontrol for box loco-manipulation. Symposium on Computer\nAnimation (SCA), 2023. 2, 3\n[60] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan\nGui. Interdiff: Generating 3d human-object interactions with\nphysics-informed diffusion. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 14928\u2013\n14940, 2023. 2, 3\n[61] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan\nKautz. Physdiff: Physics-guided human motion diffusion\nmodel. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 16010\u201316021, 2023. 2\n[62] He Zhang, Yuting Ye, Takaaki Shiratori, and Taku Komura.\nManipnet: neural manipulation synthesis with a hand-object\nspatial representation. ACM Transactions on Graphics (ToG),\n40(4):1\u201314, 2021. 3\n[63] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong,\nXinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-\ndriven human motion generation with diffusion model. arXiv\npreprint arXiv:2208.15001, 2022. 2\n[64] Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke,\nVladimir Guzov, and Gerard Pons-Moll. Couch: Towards con-\ntrollable human-chair interactions. In European Conference\non Computer Vision (ECCV), pages 518\u2013535, 2022. 2\n[65] Yan Zhang and Siyu Tang. The wanderings of odysseus in\n3d scenes. In Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 20481\u201320491, 2022. 2\n[66] Zihan Zhang, Richard Liu, Kfir Aberman, and Rana Hanocka.\nTedi: Temporally-entangled diffusion for long-term motion\nsynthesis. arXiv preprint arXiv:2307.15042, 2023. 2\n[67] Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, and\nSiyu Tang. Synthesizing diverse human motions in 3d indoor\nscenes. arXiv preprint arXiv:2305.12411, 2023. 2\n[68] Juntian Zheng, Qingyuan Zheng, Lixing Fang, Yun Liu, and\nLi Yi. Cams: Canonicalized manipulation spaces for category-\nlevel functional hand-object manipulation synthesis. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 585\u2013594, 2023. 3\n[69] Yang Zheng, Yanchao Yang, Kaichun Mo, Jiaman Li, Tao Yu,\nYebin Liu, Karen Liu, and Leonidas Guibas. Gimo: Gaze-\ninformed human motion prediction in context. In European\nConference on Computer Vision (ECCV), 2022. 2\n[70] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and\nHao Li.\nOn the continuity of rotation representations in\nneural networks. In Computer Vision and Pattern Recognition\n(CVPR), 2019. 3\n11\n"
  },
  {
    "title": "HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image",
    "link": "https://arxiv.org/pdf/2312.04543.pdf",
    "upvote": "21",
    "text": "HyperDreamer: Hyper-Realistic 3D Content Generation and\nEditing from a Single Image\nTong Wu\u2217\nwt020@ie.cuhk.edu.hk\nThe Chinese University of Hong Kong\nChina\nShanghai AI Laboratory\nChina\nZhibing Li\u2217\nlz022@ie.cuhk.edu.hk\nThe Chinese University of Hong Kong\nChina\nShanghai AI Laboratory\nChina\nShuai Yang\u2217\nyssss.mikey@gmail.com\nShanghai AI Laboratory\nChina\nShanghai Jiao Tong University\nChina\nPan Zhang\nzhangpan@pjlab.org.cn\nShanghai AI Laboratory\nChina\nXingang Pan\nxingang.pan@ntu.edu.sg\nS-Lab, NTU\nSingapore\nJiaqi Wang\nwangjiaqi@pjlab.org.cn\nShanghai AI Laboratory\nChina\nDahua Lin\u2020\ndhlin@ie.cuhk.edu.hk\nThe Chinese University of Hong Kong\nChina\nShanghai AI Laboratory\nChina\nZiwei Liu\u2020\nziwei.liu@ntu.edu.sg\nS-Lab, NTU\nSingapore\nEditable \nRenderable \n3D Generation\nReference images\n3D generation and editing\nViewable\nFigure 1: Overview. Given a single RGB image, we generate a realistic 3D model with rich details, which is full-range viewable,\nrenderable, and editable.\nABSTRACT\n3D content creation from a single image is a long-standing yet\nhighly desirable task. Recent advances introduce 2D diffusion pri-\nors, yielding reasonable results. However, existing methods are not\nhyper-realistic enough for post-generation usage, as users cannot\nview, render and edit the resulting 3D content from a full range.\nTo address these challenges, we introduce HyperDreamer with\nseveral key designs and appealing properties: 1) Viewable: 360\u25e6\n\u2217Equal contribution.\n\u2020Corresponding Authors.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\n\u00a9 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0315-7/23/12.\nhttps://doi.org/10.1145/3610548.3618168\nmesh modeling with high-resolution textures enables the creation\nof visually compelling 3D models from a full range of observation\npoints. 2) Renderable: Fine-grained semantic segmentation and\ndata-driven priors are incorporated as guidance to learn reasonable\nalbedo, roughness, and specular properties of the materials, en-\nabling semantic-aware arbitrary material estimation. 3) Editable:\nFor a generated model or their own data, users can interactively\nselect any region via a few clicks and efficiently edit the texture\nwith text-based guidance. Extensive experiments demonstrate the\neffectiveness of HyperDreamer in modeling region-aware materials\nwith high-resolution textures and enabling user-friendly editing.\nWe believe that HyperDreamer holds promise for advancing 3D\ncontent creation and finding applications in various domains.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Computer vision.\narXiv:2312.04543v1  [cs.CV]  7 Dec 2023\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nTong Wu, Zhibing Li, Shuai Yang, Pan Zhang, Xingang Pan, Jiaqi Wang, Dahua Lin, and Ziwei Liu\nKEYWORDS\nSingle-image reconstruction, 3D generation, text-guided texturing.\nACM Reference Format:\nTong Wu, Zhibing Li, Shuai Yang, Pan Zhang, Xingang Pan, Jiaqi Wang,\nDahua Lin, and Ziwei Liu. 2023. HyperDreamer: Hyper-Realistic 3D Con-\ntent Generation and Editing from a Single Image. In SIGGRAPH Asia 2023\nConference Papers (SA Conference Papers \u201923), December 12\u201315, 2023, Sydney,\nNSW, Australia. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/\n3610548.3618168\n1\nINTRODUCTION\nIn light of the high costs associated with expert-assisted 3D content\ncreation and the increasing demand across diverse applications,\nsuch as gaming, online conferencing, and virtual social presence,\nthere has been growing attention on 3D content generation, par-\nticularly in the domain of controllable generation. Traditional ap-\nproaches [Chan et al. 2021; Deng et al. 2021] in this field have\npredominantly relied on training category-specific models using\nlarge-scale 3D or 2D datasets, resulting in limited applications to\nspecific categories. However, recent years have witnessed remark-\nable progress [Lin et al. 2023; Poole et al. 2022], notably through the\nincorporation of diffusion priors derived from state-of-the-art 2D\ngenerative models. These advancements have facilitated the gen-\neration of reasonably accurate 3D content, marking a significant\nbreakthrough in the field.\nIn recent 2D diffusion-based 3D content generation methods [Poole\net al. 2022; Tang et al. 2023], it becomes common practice to in-\ncorporate text or single image conditions to achieve controllable\ngeneration. Due to its inherent ill-posed nature, researchers rely\non a 2D diffusion model [Rombach et al. 2021] as a guide prior to\ndirecting the rendering process, ensuring that all generated images\nare concentrated within the high-realism regions of the latent space.\nBy confining the generated content to these regions, the overall\nrealism of the produced 3D content is significantly enhanced.\nDespite notable advancements, Current methods for 3D con-\ntent generation suffer from two major drawbacks: limited post-\ngeneration usability and 2D diffusion bias. The former stems from\nthe use of implicit 3D representations that trade off usability for\nfidelity. Users are unable to freely zoom, re-render, or edit the re-\nsulting 3D content to get the desired 3D content, which hampers\nits practical applicability and restricts creative possibilities. The\nlatter arises from the training of the diffusion model on a 2D dataset\nthat contains rich lighting and shading variations. These variations\nenhance the realism of the 2D images, but also introduce unwanted\neffects in the textures of the 3D models, as shown in Figure 4-d.\nTo address the above issues, we propose HyperDreamer, a 3D con-\ntent generation and editing framework that is full-range viewable,\nrenderable, and editable. 1) Full-range viewable: A novel custom\nsuper-resolution module is introduced, which incorporates pseudo\nmulti-view images to facilitate high-resolution supervision. This\nmodule enables the generation of high-resolution textures for 360\u25e6\ncontent, allowing the creation of visually captivating 3D models\nfrom a full range of observation points. 2) Full-range renderable:\nThe Segment-Anything-Model [Kirillov et al. 2023] is integrated\ninto our generation approach, enabling online 3D semantic segmen-\ntation. Leveraging the segmentation mask, we introduce a semantic-\naware albedo regularization loss to mitigate the diffusion bias. To\nenable a more realistic rendering in downstream applications, we\nmodel the appearance using a spatially varying Bidirectional Re-\nflectance Distribution Function (BRDF) [Chen et al. 2022] and learn\nreasonable albedo, roughness, and specular properties of the ma-\nterials, enabling semantic-aware arbitrary material estimation. 3)\nFull-range editable: An interactive editing method is introduced,\nenabling users to perform interactive segmentation on 3D meshes\neffortlessly. By leveraging a normal-to-image model diffusion model,\nHyperDreamer allows users to edit textures of specific regions in\n3D meshes using text-based guidance. With just a few clicks, users\ncan efficiently modify the targeted region, enhancing the editability\nand flexibility of the HyperDreamer.\nExtensive experiments demonstrate the effectiveness of Hyper-\nDreamer in modeling region-aware materials with high-resolution\ntextures, and facilitating user-friendly editing, and show that Hy-\nperDreamer surpasses state-of-the-art methods by a significant\nmargin in terms of both 3D generation and editing quality. We\nbelieve that HyperDreamer, with its markedly superior quality and\nflexibility, effectively broadens the accessibility of AI-generated 3D\ncontent for practical applications.\n2\nRELATED WORKS\nText-guided 3D Generation. The text-guided 3D generation has\ngained significant attention following the remarkable success of\ntext-to-image generation methods. Dream Fields [Jain et al. 2022]\nemployed the text-image model CLIP [Radford et al. 2021] to opti-\nmize NeRFs [Mildenhall et al. 2020] by aligning the text and image\nembeddings. Building on the same principle, DreamFusion [Poole\net al. 2022] replaced CLIP with diffusion models and devised an SDS\nloss to distill knowledge from the denoising procedures. Magic3D [Lin\net al. 2023] further enhanced generation performance by employing\na coarse-to-fine framework and using meshes as the 3D representa-\ntion in the second stage. Fantasia3D [Chen et al. 2023] disentangled\nthe geometry and appearance modeling and introduced the spa-\ntially varying bidirectional reflectance distribution function (BRDF)\nfor photo-realistic texture. Our approach utilizes a single image as\nthe guided condition instead of text, which provides more detailed\nand specific information and introduces additional challenges.\nSingle-image Reconstruction. Reconstructing 3D models from a\nsingle image has been a long-existing topic. Inference-based meth-\nods [Choy et al. 2016; Gu et al. 2023; Jun and Nichol 2023; Melas-\nKyriazi et al. 2023a; Nichol et al. 2022; Pavllo et al. 2023; Tulsiani\net al. 2017; Vasudev et al. 2022; Wu et al. 2023] heavily depend\non the datasets used for training, many of which can not handle\ndiverse and general objects. Optimization-based methods utilize\npriors from 2D text-to-image diffusion model to guide the recon-\nstruction process. RealFusion [Melas-Kyriazi et al. 2023b] employs\ntextual inversion technique to bridge the gap between the reference\nimage and text-conditioned guidance. Make-it-3D [Tang et al. 2023]\nemploys a two-stage framework and leverages high-quality textures\nextracted from the reference image. Zero-1-to-3 [Liu et al. 2023]\nsynthesizes novel views by fine-tuning diffusion models [Rombach\nHyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nDiffusion Prior\nSingle-image Generation\nInteractive editing\nReference image\nSemantic Prior\nDerender Prior\nsegmentation specular init-base  specular\nalbedo\nSDS Loss\n2D generation \nSR Loss\nNovel-view images\nAlbedo\nSpecular\nRoughness\nSemantic\nNormal\nRendering\nSpecular base \ud835\udc7a\ud835\udc94\nRoughness base\ud835\udc79\ud835\udc94\nNormal base\nlighting\nNeural radiance field\nSuper-\nresolution\nSeg Loss\nAlbedo Loss\nNetworks\nQuery \nQuery \nNegative point prompt\nPositive point prompt\nUser\n\u00fc Region: a few clicks\n\u00fc Content: language\nRGB Loss\n\u201cTurn it to red coat, Chinese style\u201d\n\u201cTurn it to brown hat\u201d\nFigure 2: Overview of our 3D generation and editing pipeline. We introduce diffusion priors, semantic priors, and derendering priors into this\nhighly under-constraint problem to enable high-resolution textures with material modeling and interactive editing after the generation.\net al. 2021] with multi-view data. It has also been applied to single-\nimage 3D reconstruction by applying SJC [Wang et al. 2022]. Our\nwork utilizes Zero-1-to-3 as the guidance model and incorporate\nseveral key designs to enable broader applications.\nMaterial and Illumination Estimation. Multi-view reconstruction\nmethods [Munkberg et al. 2022] benefited from separately mod-\neling geometry, material, and illumination conditions, while it\u2019s\na highly ill-posed problem for generation. Previous works like\nFantasia3D [Chen et al. 2023] propose to learn globally varying\nroughness and metallic distributions, which may not always align\nwith realistic material properties. Based on the material estimation\napproaches from a single image [Sang and Chandraker 2020; Wim-\nbauer et al. 2022], we further propose a more plausible assumption\nthat materials within the same semantic class share similar material\nproperties, enabling spatially varying materials modelling while\npreventing degenerate solutions.\nText-guided 3D Editing. Recently, text-guided image processing\nhas experienced rapid development in both quality and diversity.\nText2Mesh [Michel et al. 2022] proposes a neural style field, which\nuses CLIP to guide the initial mesh based on text. TANGO [Chen\net al. 2022] follows a similar scheme and uses a BRDF to optimize\nthe appearance. However, there is a gap from the actual use due\nto insufficient accuracy. More recently, TEXTure [Richardson et al.\n2023] leverages an improved depth-to-image diffusion process and\napplies an iterative scheme that paints a 3D model from different\nviewpoints. However, none of them enable text-guided editing of a\nlocal area on a 3D object. We propose an interactive editing method\nthat users can edit textures based on text guidance in selected 3D\nregions with a few simple clicks or in a global manner.\n3\nPRELIMINARIES\n3.1\n3D Representation\nInspired by Magic3D [Lin et al. 2023], we adopt NeRF [Mildenhall\net al. 2020] and DMTet [Shen et al. 2021] for the first and second\nstage training, respectively. In the first stage, NeRF represents the\nscene as an implicit function that maps a 3D location \ud835\udc65 and a 2D\nviewing direction \ud835\udc51 to a volume density \ud835\udf0f and color \ud835\udc50. To render\na pixel, NeRF alpha-composites the densities and colors along the\nray that is cast from the camera to the pixel:\n\ud835\udc36 =\n\u2211\ufe01\n\ud835\udc58\n\ud835\udefc\ud835\udc58\n\u00d6\n\ud835\udc58\u2032<\ud835\udc58\n(1 \u2212 \ud835\udefc\ud835\udc58\u2032)\ud835\udc50\ud835\udc58,\n\ud835\udefc\ud835\udc58 = 1 \u2212 exp(\u2212\ud835\udf0f\ud835\udc58 \u2225\ud835\udc65\ud835\udc58+1 \u2212 \ud835\udc65\ud835\udc58 \u2225). (1)\nTo accelerate the training, we employ the efficient hash grid encod-\ning from Instant NGP [M\u00fcller et al. 2022] instead of pure MLPs.\nIn the second stage, we adopt DMTet to produce high-resolution\noutputs without high computational and memory requirements.\nDMTet is a hybrid representation that integrates implicit and ex-\nplicit surface representations and can efficiently render high-resolution\ntextured meshes with differentiable rasterization. Formally, DMTet\nmodels the 3D shape as a deformable tetrahedral grid (\ud835\udc49\ud835\udc47,\ud835\udc47), where\n\ud835\udc49\ud835\udc47 are the vertices in the tetrahedral grid\ud835\udc47. Each tetrahedron\ud835\udc47\ud835\udc58 \u2208 \ud835\udc47\nhas four vertices {\ud835\udc63\ud835\udc56\ud835\udc58 |\ud835\udc56 \u2208 {\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51}}, each associated with a SDF\nvalue \ud835\udc60(\ud835\udc63\ud835\udc56) and a deformation \u0394\ud835\udc63\ud835\udc56\ud835\udc58 . The surface mesh is extracted\nby differentiable marching tetrahedra algorithm.\n3.2\nScore Distillation Sampling (SDS)\nPrevious works [Lin et al. 2023; Poole et al. 2022] have leveraged\nthe 2D diffusion model [Rombach et al. 2021] as prior knowledge\nfor text-to-3D generation. The diffusion model \ud835\udf19 learns a denoising\nfunction \ud835\udf16\ud835\udf19 (\ud835\udc65\ud835\udc61;\ud835\udc66,\ud835\udc61) that estimates the noise \ud835\udf16 based on the noisy\nimage \ud835\udc65\ud835\udc61, text embedding\ud835\udc66 and noise step\ud835\udc61. It progressively reduces\nthe noise and introduces image structure. To optimize the 3D scene\n\ud835\udf03, Score Distillation Sampling (SDS) guides all rendered images to\nmatch the given text embedding \ud835\udc66 under diffusion priors:\n\u2207\ud835\udf03 L\ud835\udc46\ud835\udc37\ud835\udc46 (\ud835\udf19,\ud835\udc65 = \ud835\udc54(\ud835\udf03)) = E\ud835\udc61,\ud835\udf16\n\u0014\n\ud835\udf14(\ud835\udc61)(\ud835\udf16\ud835\udf19 (\ud835\udc65\ud835\udc61;\ud835\udc66,\ud835\udc61) \u2212 \ud835\udf16) \ud835\udf15\ud835\udc65\n\ud835\udf15\ud835\udf03\n\u0015\n,\n(2)\nwhere\ud835\udc54 denotes the image renderer and\ud835\udf14(\ud835\udc61) represents a weighting\nfunction. In addition to text conditional SDS, zero-1-to-3 [Liu et al.\n2023] introduces a 3D-aware SDS that conditions on input view\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nTong Wu, Zhibing Li, Shuai Yang, Pan Zhang, Xingang Pan, Jiaqi Wang, Dahua Lin, and Ziwei Liu\na. reference image\nb. SAM with 35 overlapping regions c. ours with 8 independent regions\nReference image segmentation\npseudo labels on novel views\nGlobally consistent 3D segmentation\npseudo labels on novel views\na. reference image\nb. SAM with 32 overlapping regions c. ours with 6 independent regions\nFigure 3: SAM at the generation stage. We effectively cluster\nconcise semantic groups compared to the raw SAM results.\nand relative camera extrinsic to exploit the 3D consistent priors:\n\u2207\ud835\udf03 L\ud835\udc46\ud835\udc37\ud835\udc46 (\ud835\udf19,\ud835\udc65 = \ud835\udc54(\ud835\udf03)) = E\ud835\udc61,\ud835\udf16\n\u0014\n\ud835\udf14(\ud835\udc61)(\ud835\udf16\ud835\udf19 (\ud835\udc65\ud835\udc61;\ud835\udc65\ud835\udc56, \ud835\udc45,\ud835\udc47,\ud835\udc61) \u2212 \ud835\udf16) \ud835\udf15\ud835\udc65\n\ud835\udf15\ud835\udf03\n\u0015\n,\n(3)\nwhere \ud835\udc65\ud835\udc56 represents the input view, \ud835\udc45 and \ud835\udc47 are the relative cam-\nera rotation and translation from the input view to the desired\nviewpoint.\n3.3\nSegment Anything Model (SAM)\nSegment Anything Model (SAM) [Kirillov et al. 2023] is the founda-\ntion model for general image segmentation, which supports various\nsegmentation modes such as automatic everything and manual\nprompt. Taking point prompts as an example, SAM takes an image\n\ud835\udc3c and a set of user-specific prompts P = (\ud835\udc5d,\ud835\udc59) as inputs, and the\noutput is a corresponding segmentation mask \ud835\udc40. Among them, P\nincludes \ud835\udc5d and \ud835\udc59, where \ud835\udc5d is the set of each point coordinate and \ud835\udc59\nis the set corresponding to each point label. We use \ud835\udc46 to represent\nthe SAM model, so we have \ud835\udc40\ud835\udc3c,P = \ud835\udc46(\ud835\udc3c, P).\n4\nMETHODOLOGY\nThis section elaborates on the proposed framework in detail. De-\nspite the inherent challenges posed by the ill-posed nature of the\nproblem, HyperDreamer capitalizes on the deep priors from the 2D\ndiffusion model, semantic segmentation model, and material esti-\nmation model, which collectively empower the capability for full-\nrange viewing, rendering, and interactive editing. Specifically, (1) to\nachieve high-fidelity texture generation, we utilize high-resolution\npseudo multi-view images for auxiliary supervision, as detailed\nin Sec 4.1. (2) For material modeling, we introduce online 3D se-\nmantic segmentation and semantic-aware regularizations, which is\ninitialized via material estimation results, as described in Sec. 4.2.\n(3) Furthermore, a novel interactive editing approach is proposed in\nSec. 4.3 for effortless targeted modification of regions on 3D meshes\nvia interactive segmentation.\n4.1\n360\u25e6 High-Resolution Texture Generation\nIn the second training stage, the mesh representation allows for\nrapid images rendering, unlocking the potential of achieving high-\nresolution texture maps. However, our guidance model, Zero-1-to-\n3 [Liu et al. 2023], was originally trained on low-resolution images\n(256 \u00d7 256). The resulting SDS loss fails to handle higher-resolution\nimages, thereby limiting the benefits offered by mesh representa-\ntion. The disparity between the resolutions used for training and\ninferencing leads to a relatively blurry texture map.\nTo overcome this challenge, we propose a high-resolution texture\ngeneration module. We first select a set of novel views and directly\ngenerate \ud835\udc5a images per view using Zero-1-to-3. Subsequently, we\nemploy a super-resolution network [Rombach et al. 2021] to upscale\nd. Zero-1-to-3 generated samples\na. reference image\nb. baseline\nc. w/ albedo regularization\n2D generation\nSDS-based\n3D generation\n2D diffusion\nbias exists.\nsemantic\nsemantic\nFigure 4: Diffusion bias. The 2D diffusion bias in d leads to 3D\ngeneration failures in b, which can be alleviated by the albedo regu-\nlarization in c.\nthe sampled images, enabling high-resolution supervision. Since\nthe multi-view images generated by Zero-1-to-3 are not perfectly\n3D consistent, directly applying per-pixel loss can lead to network\ninstability. Instead, we employ perceptual loss [Johnson et al. 2016]\nin the feature space. By leveraging perceptual loss, we can minimize\nthe content and style differences between two images without rely-\ning on pixel-level alignment, effectively alleviating inconsistencies\nduring the training process.\n4.2\nSemantic-Aware Material Estimation\n4.2.1\nOnline global semantic segmentation. During the second train-\ning stage, we also propose to integrate a new MLP-based branch\nupon the hash encoding and equip the framework with a globally-\nconsistent mesh segmentation for further semantic regularization.\nWe first use SAM to produce over-segmented results of the reference\nimage (Figure 3 (b)), and then we cluster different semantic parts by\nthresholding the feature similarity among them before assigning\nthe semantic labels, as shown in Figure 3 (c). We assume that the\nreference image already contains all of the semantic components\nof the generated 3D model. We also assign pseudo labels to novel\nview images by thresholding the feature similarities, and all these\n2D labels are used to supervise the semantic branch training. We\npresent detailed implementations in the supplementary materials.\n4.2.2\nSemantic-Aware Albedo Regularization. Recent approaches\nin the single-image 3D generation [Liu et al. 2023; Melas-Kyriazi\net al. 2023b; Tang et al. 2023] optimize the model with diffusion\npriors and RGB reconstruction loss. They adopt different types of\nshading augmentations at novel views, including albedo, diffuse,\nand textureless, while only albedo shading is applied at the reference\nview. However, this pipeline introduces two inherent problems.\nFirstly, the diffusion priors suffer from intrinsic shading and\nreflectance effects. For instance, Stable Diffusion and Zero-1-to-3 are\ntrained on abundant images with lighting and shading variations,\ninevitably baking these effects into the textures of the generated\n3D models. As shown in Figure 4, given the front view of the teddy\nbear, Zero-1-to-3 tends to generate a dark back view, as if the light\nsource only exists in the front, leading to a black back of the 3D\nmodel as in Figure 4-b.\nSecondly, the shading and reflectance characteristics in the ref-\nerence image are integrated into the albedo color learning of the\nHyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nmodel via RGB reconstruction loss, making it challenging for re-\nrendering.\nWe aim to introduce several albedo losses to alleviate the afore-\nmentioned problems. For the diffusion bias, we assume that albedo\ncolors in regions under the same semantic label are similar. For the\n\ud835\udc41\ud835\udc60 semantic labels, we maintain a albedo library called \ud835\udc34\ud835\udc60, which\nis updated regularly according to the semantic-region-averaged\nalbedo colors of the reference image along the training. For each\nnovel view, we predict segmentation masks with the semantic\nbranch, and then we use a Gaussian filter to gain a weighted av-\nerage of predicted albedo colors inside each semantic group. We\npropose a semantic-aware albedo regularization as below:\n\ud835\udc3f\ud835\udc4e =\n\ud835\udc41\ud835\udc60\n\u2211\ufe01\n\ud835\udc56=1\n||\ud835\udc39\ud835\udc54\ud835\udc4e\ud835\udc62\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc4e\ud835\udc5b(\ud835\udc34\ud835\udc56\n\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51) \u2212 \ud835\udc34\ud835\udc56\n\ud835\udc60 ||2\n2.\n(4)\nFurthermore, we incorporate a state-of-the-art single-image deren-\ndering framework [Wimbauer et al. 2022] to generate the albedo\nmap of the reference image as an additional albedo supervision.\n4.2.3\nAppearance Modeling. To enable a more realistic rendering,\nwe introduce the Physically-Based Rendering (PBR) material model.\nFollowing TANGO [Chen et al. 2022] and PhySG [Zhang et al. 2021],\nwe leverage spatially varying BRDF (SVBRDF) to parameterize the\nmaterial, including roughness, specular, and normal. Based on the\nrendering equation [Kajiya 1986], given a location \ud835\udc65 and the surface\nnormal \ud835\udc5b, the incident light intensity at this point is denoted as\n\ud835\udc3f\ud835\udc56 (\ud835\udf14\ud835\udc56;\ud835\udc65) along the direction \ud835\udf14\ud835\udc56; BRDF \ud835\udc53\ud835\udc5f (\ud835\udf14\ud835\udc5c,\ud835\udf14\ud835\udc56;\ud835\udc65) denotes the re-\nflectance coefficient of the material viewing from direction \ud835\udf14\ud835\udc5c. The\nobserved light intensity \ud835\udc3f\ud835\udc5c (\ud835\udf140;\ud835\udc65) is calculated over the hemisphere\n\u03a9 = {\ud835\udf14\ud835\udc56 : \ud835\udf14\ud835\udc56 \u00b7 \ud835\udc5b > 0}:\n\ud835\udc3f\ud835\udc5c (\ud835\udf140;\ud835\udc65) =\n\u222b\n\u03a9\n\ud835\udc3f\ud835\udc56 (\ud835\udf14\ud835\udc56;\ud835\udc65)\ud835\udc53\ud835\udc5f (\ud835\udf14\ud835\udc5c,\ud835\udf14\ud835\udc56;\ud835\udc65)(\ud835\udf14\ud835\udc56 \u00b7 \ud835\udc5b)\ud835\udc51\ud835\udf14\ud835\udc56.\n(5)\nWe utilize spherical Gaussians (SGs) [Yan et al. 2012] to ap-\nproximate the rendering equation in closed form. For a spherical\nGaussian with \ud835\udc5b dimensions, given the lobe axis \ud835\udf09 \u2208 S2, lobe sharp-\nness \ud835\udf06 \u2208 R+, and lobe amplitude \ud835\udf07 \u2208 R\ud835\udc5b+, the spherical function is\nformulated as:\n\ud835\udc3a(\ud835\udf08; \ud835\udf09, \ud835\udf06, \ud835\udf07) = \ud835\udf07\ud835\udc52\ud835\udf06(\ud835\udf08\u00b7\ud835\udf09\u22121),\n(6)\nwhere \ud835\udf08 \u2208 S2 denotes the input.\nThe environment map \ud835\udc3f(\ud835\udf14\ud835\udc56) is represented as a mixture of SGs:\n\ud835\udc3f\ud835\udc56 (\ud835\udf14\ud835\udc56) =\n\ud835\udc40\n\u2211\ufe01\n\ud835\udc58=1\n\ud835\udc3a(\ud835\udf14\ud835\udc56; \ud835\udf09\ud835\udc58, \ud835\udf06\ud835\udc58, \ud835\udf07\ud835\udc58).\n(7)\nThe SVBRDF is divided into diffuse BRDF and specular BRDF:\n\ud835\udc53\ud835\udc5f (\ud835\udf14\ud835\udc5c,\ud835\udf14\ud835\udc56;\ud835\udc65) = \ud835\udc53\ud835\udc51 (\ud835\udc65)/\ud835\udf0b + \ud835\udc53\ud835\udc60 (\ud835\udf14\ud835\udc5c,\ud835\udf14\ud835\udc56;\ud835\udc65). The diffuse term is modeled\nas an MLP based on the multi-resolution hash input encoding. And\nthe specular term at location \ud835\udc65 is formulated as:\n\ud835\udc53\ud835\udc60 (\ud835\udf14\ud835\udc5c,\ud835\udf14\ud835\udc56;\ud835\udc65) = \ud835\udc3a(\u210e;\ud835\udc5b\ud835\udc65,\n\ud835\udf06\ud835\udc65\n4\u210e \u00b7 \ud835\udf14\ud835\udc5c\n, M\ud835\udc65\ud835\udf07\ud835\udc65),\n(8)\nwhere \u210e is half vector and M is Fresnel and shadowing effects.\nThe last term in Eqn. 5 is approximated as [Meder and Br\u00fcderlin\n2018]: (\ud835\udf14\ud835\udc56 \u00b7 \ud835\udc5b) = \ud835\udc3a(\ud835\udf14\ud835\udc56; 0.0315,\ud835\udc5b, 32.7080) \u2212 31.7003.\nTherefore, the rendering equation is represented as the multi-\nplication of SGs and can be calculated in closed form. Learnable\nparameters above include {\ud835\udf09\ud835\udc58, \ud835\udf06\ud835\udc58, \ud835\udf07\ud835\udc58}\ud835\udc40\n\ud835\udc58=1 for the environmental\nPoints \nPrompt\nSAM\nAuto SAM Prompt Sampler\n\ufffd\ufffd\nProject Back\n   Iteration t-1\n\ufffd\ufffd\nEdited Textured \nMesh\nMask Texture\nNormal Image Trimap\nKeep\n Refine\n New\nRender\nEstimate\nGenerate\nEdited Image\nEdit 3D Mesh (Iteration t)\n\u201cMake it looks like ironman\u201d\nImproved Diffusion\nProcess\n   Iteration t\nFigure 5: Interactive editing process. Users can select the interest\nregions and then our method output the texture mask of the target\narea to our texture synthesis pipeline for text-guided editing.\nmap, diffuse albedo \ud835\udc53\ud835\udc51, and the spatially varying {\ud835\udf06, \ud835\udf07}. We assume\nthat regions with the same semantic label usually share alike mate-\nrials and enforce channel consistency in roughness and specular.\nPlease refer to the supplementary materials for more details.\n4.3\nInteractive Editing\nEditing a 3D model requires complex interaction with 3D shapes\nwhile maintaining global consistency to achieve the desired design.\nWe propose an intelligent and user-friendly interactive 3D editing\ntool that allows users to quickly settle the target area in 3D space\nby one-shot selection and edit its texture based on text guidance.\n4.3.1\nInteractive Segmentation In Mesh. Interactive segmentation\nin the 3D mesh enables users to segment any region of the 3D\nobject. In our method, as shown in Figure 5, we use two UV maps\nto represent the masks of the 3D mesh, where T\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 for selected\nregions and T\ud835\udc5b\ud835\udc52\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 for remaining regions, respectively. Given a\ntarget view \ud835\udc63\ud835\udc61, we can render the masks\ud835\udc44\ud835\udc61\u22121\n\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 and\ud835\udc44\ud835\udc61\u22121\n\ud835\udc5b\ud835\udc52\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 which\nare actually the point prompts cache from the previous \ud835\udc61 - 1 views\nbut not complete segmentation results in the current view. Then,\nwe sample points with a patch sampling mechanism on \ud835\udc44\ud835\udc61\u22121\n\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 as\npositive prompts and \ud835\udc44\ud835\udc61\u22121\n\ud835\udc5b\ud835\udc52\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 as negative prompts in each patch\nto generate refined segmentation results \ud835\udc3c\ud835\udc60\ud835\udc4e\ud835\udc5a and \ud835\udc3c\ud835\udc5b\ud835\udc52\ud835\udc54\ud835\udc60\ud835\udc4e\ud835\udc5a via SAM.\nInverse rendering is then applied to project \ud835\udc3c\ud835\udc60\ud835\udc4e\ud835\udc5a and \ud835\udc3c\ud835\udc5b\ud835\udc52\ud835\udc54\ud835\udc60\ud835\udc4e\ud835\udc5a onto\nT\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 and T\ud835\udc5b\ud835\udc52\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58, where we use a gradient-based optimization\nto T\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58for L\ud835\udc61 over the values of T\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 when rendered through\nthe differential renderer R [Jatavallabhula et al. 2019].That is,\n\u2207T\ud835\udc61\n\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 L\ud835\udc61 =\nh\u0010\nR\n\u0010\nMesh , T\ud835\udc61\n\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58, \ud835\udc63\ud835\udc61\n\u0011\n\u2212 \ud835\udc3c\ud835\udc61\n\ud835\udc60\ud835\udc4e\ud835\udc5a\n\u0011\n\u2299 \ud835\udc5a\ud835\udc61\ni \ud835\udf15R \u2299 \ud835\udc5a\ud835\udc61\n\ud835\udf15T\ud835\udc61\n\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58\n,\n(9)\nwhere, \ud835\udc5a\ud835\udc61 is the mask of mesh at the view \ud835\udc63\ud835\udc61. Similarly, the method\nof projecting back to texture in each view in subsequent is the same.\n4.3.2\nText-Guided Texture Synthesis. We apply Normal-to-Image\nmodel M\ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc59 based on ControlNet [Zhang and Agrawala 2023]\nto paint textures that closely match the surface details on the 3D\nMesh directly.\nTo address the inconsistency problem, we divide each rendered\nview into three partitions: \ud835\udc40\ud835\udc5b\ud835\udc52\ud835\udc64, \ud835\udc40\ud835\udc58\ud835\udc52\ud835\udc52\ud835\udc5d, and \ud835\udc40\ud835\udc5f\ud835\udc52\ud835\udc53 \ud835\udc56\ud835\udc5b\ud835\udc52. The \ud835\udc40\ud835\udc5b\ud835\udc52\ud835\udc64\npartition is the target region that needs to be painted for the first\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nTong Wu, Zhibing Li, Shuai Yang, Pan Zhang, Xingang Pan, Jiaqi Wang, Dahua Lin, and Ziwei Liu\nShap-E\nNeurallift-360\nRealFusion\nZero-1-to-3\nOurs\nReference\nOnline images\nDTU images\nFigure 6: Qualitative comparisons. HyperDreamer generates a high-fidelity reference view and more realistic and reasonable results at the\nnovel view.\nTable 1: Quantitative results on our data.\nMethod\nContextual \u2193\nCLIP \u2191\nPerceptual \u2193\nShap-E\n4.95\n0.68\n-\nNeuralLift-360\n4.71\n0.78\n0.67\nRealFusion\n2.25\n0.79\n0.17\nZero-1-to-3\n3.36\n0.74\n0.13\nOurs\n2.11\n0.86\n0.10\ntime. The \ud835\udc40\ud835\udc58\ud835\udc52\ud835\udc52\ud835\udc5d partition is either a previously well-painted target\nregion or a region that is out of the target region. The \ud835\udc40\ud835\udc5f\ud835\udc52\ud835\udc53 \ud835\udc56\ud835\udc5b\ud835\udc52\npartition is the region painted from the previous views, but they are\nmainly the junction of adjacent views and need further refinement.\nTo attain \ud835\udc40\ud835\udc5f\ud835\udc52\ud835\udc53 \ud835\udc56\ud835\udc5b\ud835\udc52, we first perform an opening operation, \ud835\udc42\ud835\udc5d\ud835\udc52\ud835\udc5b,\non the mask \ud835\udc40\ud835\udc5b\ud835\udc52\ud835\udc64 to eliminate out-lie small regions. We then\nperforms erode E and dilate D as follows,\n\ud835\udc40\ud835\udc5f\ud835\udc52\ud835\udc53 \ud835\udc56\ud835\udc5b\ud835\udc52 = D(\ud835\udc42\ud835\udc5d\ud835\udc52\ud835\udc5b(\ud835\udc40\ud835\udc5b\ud835\udc52\ud835\udc64)) \u2212 E(\ud835\udc42\ud835\udc5d\ud835\udc52\ud835\udc5b(\ud835\udc40\ud835\udc5b\ud835\udc52\ud835\udc64))).\n(10)\nIn the M\ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc59, we modify the sampling process by blender\ndiffusion to inject the information of region partition into the de-\nnoising process. The mask \ud835\udc40\ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc61 explicitly blends the noised latent\n\ud835\udc67\ud835\udc44\ud835\udc61 and the denoised latent estimation \ud835\udc67\ud835\udc61 as follows:\n\ud835\udc40paint =\n(\n0\n\ud835\udc40\ud835\udc58\ud835\udc52\ud835\udc52\ud835\udc5d\n1\n\ud835\udc40\ud835\udc5b\ud835\udc52\ud835\udc64 \u222a \ud835\udc40\ud835\udc5f\ud835\udc52\ud835\udc53 \ud835\udc56\ud835\udc5b\ud835\udc52,\n(11)\n\u02c6\ud835\udc67\ud835\udc61 = \u02c6\ud835\udc67\ud835\udc61 \u2299 \ud835\udc40\ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc61 + \ud835\udc67\ud835\udc61 \u2299 (1 \u2212 \ud835\udc40\ud835\udc5d\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc61).\n(12)\nBased on the above texture synthesis method, we can achieve\nlocal editing in the 3D mesh. In more detail, we can limit the editing\narea to the target region by doing the dot product with the original\ntexture map T and the T\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 obtained by interactive segmentation\nin mesh. Finally, users can select any region in the 3D object to edit\nbased on text guidance, as illustrated in Figure 5.\nTable 2: Quantitative results on the DTU dataset.\nMethods\nContextual \u2193\nCLIP \u2191\nPerceptual \u2193\nShap-E\n2.82\n0.80\n-\nNeuralLift-360\n4.74\n0.78\n0.72\nRealFusion\n4.84\n0.82\n0.47\nZero-1-to-3\n4.50\n0.80\n0.43\nOurs\n2.08\n0.89\n0.13\n4.4\nImplementation Details\n4.4.1\nModel and training. For the generation process, we follow\nInstant-NGP [M\u00fcller et al. 2022] to adopt a two-stage training\npipeline: a coarse NeRF is trained for 50 epochs in the first stage,\nguided by Zero-1-to-3 based SDS loss and other regularisation terms\nlike depth and normal loss. We use MiDaS [Ranftl et al. 2021] and\nOmniData [Eftekhar et al. 2021] to extract the depth and normal\nestimations, respectively. We train a DMTet [Shen et al. 2021] for\n100 epochs in the second stage based on the first stage model.\nThe SR, semantics, and material modules are integrated with the\nsecond stage only. More details are presented in the supplementary\nmaterials.\n4.4.2\nMethods for comparison. We compare four recent approaches\nfor single-image based 3D generation for arbitrary images: Shap-\nE [Jun and Nichol 2023] is a conditional generative model for im-\nplicit representations trained on millions of 3D assets; NeuralLift-\n360 [Xu et al. 2022] generates the neural radiance field based on\nthe CLIP-guided diffusion priors. RealFusion [Melas-Kyriazi et al.\n2023b] applies the SDS loss based on Stable Diffusion [Rombach\net al. 2021] and RGB reconstruction loss with the reference view,\nin comparison, Zero-1-to-3 [Liu et al. 2023] leverages the SJC loss\nbased on its viewpoint-conditioned model as the guidance.\nHyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nReference image\nw/o SR module\nw/ SR module\nReference image\nRealFusion\nZero-1-to-3\nOurs\nZoom-in views\nFigure 7: Ablation on the super-resolution (SR) module. High-\nfrequency details on textures are generated under SR supervision.\nReference image\nReference image\nrendering\nalbedo\nrendering\nalbedo\nw/o ref albedo loss\nw/ ref albedo loss\nalbedo\nroughness\nspecular\na.\nb.\nBlender rendering\nReference image\nReference image\nrendering\nalbedo\nrendering\nalbedo\nw/o ref albedo loss\nw/ ref albedo loss\nalbedo\nroughness\nspecular\na.\nb.\nBlender rendering\nFigure 8: Analysis of the material modeling. We show an example\nof the output roughness and specular maps in a, together with its\nrendering results in Blender. We show how the albedo loss at the\nreference view help alleviate shading and reflectance learning in the\nalbedo texture in b.\n5\nEXPERIMENTS\n5.1\nQualitative Comparisons\nWe show some qualitative comparisons with several state-of-the-art\nworks in Figure 6, where we present both the reference view and the\nback view of the object for each method. The results by Shap-E are\nrelatively worse than the other optimization-based methods. The\ninstances generated by NeuralLift-360 are small in size and low in\nquality, while the basic semantics is reserved. RealFusion and Zero-\n1-to-3, both leverage the reference view RGB reconstruction loss for\nconstraint and thus keep a high fidelity with the reference image.\nWhile RealFusion suffers severely from the multi-face problem,\nand the results from Zero-1-to-3 are blurry. Our method achieves\nthe highest quality in both the reference view and the back view,\npresenting realistic and reasonable generations.\n5.2\nQuantitative Comparisons\nWe adopt three metrics for quantitative comparisons: 1) LPIPS [John-\nson et al. 2016] evaluates the reconstruction quality of the reference\nview image; 2) Contextual distance [Mechrez et al. 2018] evalu-\nates the pixel-level distance between the rendered novel view im-\nages and the reference image; 3) CLIP-Score [Radford et al. 2021]\nmeasures the semantic-level distance between the novel view im-\nages and the reference image. We select 20 images online with a\nwide range of diversity, and we select 10 instances from the DTU\nOurs\nTarget area: The Bird\nRandom \nScheme\nOriginal\nMesh\nTarget area: The Mushroom\nFigure 9: Analysis of our scheme of segmentation in mesh. Our\nmethod has better ability to handle complex circumstances.\ndataset [Aan\u00e6s et al. 2016] that are basically complete. The results\nfor the two datasets are shown in Table 1 and Table 2, respectively.\nOur model outperforms the comparison methods in all three met-\nrics by a large margin, quantitatively revealing the effectiveness of\nthe pipeline.\n5.3\nAnalysis and Ablations\n5.3.1\nSuper Resolution. We show how our SR module works in\nFigure 7. It largely enhances the texture details and realism in our\nmodel, which enables it to support high-resolution zoom-in views\nin comparison with other methods.\n5.3.2\nMaterials. Examples of the generated roughness and specu-\nlar map are shown in Figure 8-a, where we observe that the material\nproprieties are highly correlated with the semantic label of the re-\ngion. We also show how the albedo loss helps decompose the albedo\ntexture out of the reference view.\n5.3.3\nEditing. We show in Figure 9 that the Naive method for\nsegmentation in the mesh, which only inputs positive prompt and\nrandomly samples the point prompts cache, has a high probability\nof failure especially in the condition of dealing with discrete and\ncomplex regions. However, adopting our scheme, it is more robust\nwith patch sampling in the positive and negative prompts and input\nboth into SAM.\n6\nCONCLUSION\nThis paper introduces a framework, HyperDreamer, which en-\nables hyper-realistic 3d content generation and editing for a single\nimage. In contrast to previous works, the 3D content generated by\nour method is full-range viewable, renderable, and editable. Exten-\nsive experiments demonstrate the effectiveness of HyperDreamer\nin modeling region-aware materials with high-resolution textures\nand enabling user-friendly editing. We believe that HyperDreamer\nholds promise for advancing 3D content creation and editing, which\nwould be practical for both academic and industrial usage.\nAcknowledgement. This project is funded by Shanghai AI Labora-\ntory and the Ministry of Education, Singapore, under its MOE AcRF\nTier 2 (MOE-T2EP20221- 0012), NTU NAP, and under the RIE2020\nIndustry Alignment Fund \u2013 Industry Collaboration Projects (IAF-\nICP) Funding Initiative, as well as cash and in-kind contribution\nfrom the industry partner(s).\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nTong Wu, Zhibing Li, Shuai Yang, Pan Zhang, Xingang Pan, Jiaqi Wang, Dahua Lin, and Ziwei Liu\nREFERENCES\nHenrik Aan\u00e6s, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and An-\nders Bjorholm Dahl. 2016. Large-scale data for multiple-view stereopsis. Interna-\ntional Journal of Computer Vision 120 (2016), 153\u2013168.\nEric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein.\n2021. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image\nsynthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition. 5799\u20135809.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3D: Disentangling\nGeometry and Appearance for High-quality Text-to-3D Content Creation. arXiv\npreprint arXiv:2303.13873 (2023).\nYongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui Jia. 2022. TANGO: Text-\ndriven Photorealistic and Robust 3D Stylization via Lighting Decomposition. In\nAdvances in Neural Information Processing Systems (NeurIPS).\nChristopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 2016.\n3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction.\nIn Proceedings of the European Conference on Computer Vision (ECCV).\nYu Deng, Jiaolong Yang, and Xin Tong. 2021. Deformed implicit field: Modeling\n3d shapes with learned dense correspondence. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 10286\u201310296.\nAinaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. 2021. Omnidata:\nA Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets From 3D\nScans. In Proceedings of the IEEE/CVF International Conference on Computer Vision.\n10786\u201310796.\nJiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie\nLiu, and Ravi Ramamoorthi. 2023. NerfDiff: Single-image View Synthesis with\nNeRF-guided Distillation from 3D-aware Diffusion. In International Conference on\nMachine Learning.\nAjay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. 2022.\nZero-Shot Text-Guided Object Generation with Dream Fields. (2022).\nKrishna Murthy Jatavallabhula, Edward Smith, Jean-Francois Lafleche, Clement Fuji\nTsang, Artem Rozantsev, Wenzheng Chen, Tommy Xiang, Rev Lebaredian, and\nSanja Fidler. 2019. Kaolin: A pytorch library for accelerating 3d deep learning\nresearch. arXiv preprint arXiv:1911.05063 (2019).\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-time\nstyle transfer and super-resolution. In Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14.\nSpringer, 694\u2013711.\nHeewoo Jun and Alex Nichol. 2023. Shap-E: Generating Conditional 3D Implicit\nFunctions. arXiv preprint arXiv:2305.02463 (2023).\nJames T Kajiya. 1986. The rendering equation. In Proceedings of the 13th annual\nconference on Computer graphics and interactive techniques. 143\u2013150.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Pi-\notr Doll\u00e1r, and Ross Girshick. 2023. Segment Anything. arXiv:2304.02643 (2023).\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang,\nKarsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023. Magic3D: High-\nResolution Text-to-3D Content Creation. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov,\nand Carl Vondrick. 2023.\nZero-1-to-3: Zero-shot One Image to 3D Object.\narXiv:2303.11328 [cs.CV]\nRoey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor. 2018. The contextual loss for im-\nage transformation with non-aligned data. In Proceedings of the European conference\non computer vision (ECCV). 768\u2013783.\nJulian Meder and Beat Br\u00fcderlin. 2018. Hemispherical gaussians for accurate light\nintegration. In Computer Vision and Graphics: International Conference, ICCVG 2018,\nWarsaw, Poland, September 17-19, 2018, Proceedings. Springer, 3\u201315.\nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. 2023b. Real-\nFusion: 360 Reconstruction of Any Object from a Single Image. In CVPR.\nLuke Melas-Kyriazi, Christian Rupprecht, and Andrea Vedaldi. 2023a. PC2: Projection-\nConditioned Point Cloud Diffusion for Single-Image 3D Reconstruction. In Arxiv.\nOscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. 2022.\nText2mesh: Text-driven neural stylization for meshes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 13492\u201313502.\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance\nFields for View Synthesis. In ECCV.\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant\nneural graphics primitives with a multiresolution hash encoding. ACM Transactions\non Graphics (ToG) 41, 4 (2022), 1\u201315.\nJacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex\nEvans, Thomas M\u00fcller, and Sanja Fidler. 2022. Extracting triangular 3d models,\nmaterials, and lighting from images. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 8280\u20138290.\nAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. 2022.\nPoint-E: A System for Generating 3D Point Clouds from Complex Prompts. arXiv\npreprint arXiv:2212.08751 (2022).\nDario Pavllo, David Joseph Tan, Marie-Julie Rakotosaona, and Federico Tombari. 2023.\nShape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field\nInversion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR).\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2022. DreamFusion:\nText-to-3D using 2D Diffusion. arXiv (2022).\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\n2021. Learning transferable visual models from natural language supervision. In\nInternational conference on machine learning. PMLR, 8748\u20138763.\nRen\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. 2021. Vision Transformers for\nDense Prediction. ICCV (2021).\nElad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. 2023.\nTexture: Text-guided texturing of 3d shapes. arXiv preprint arXiv:2302.01721 (2023).\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer. 2021. High-Resolution Image Synthesis with Latent Diffusion Models.\narXiv:2112.10752 [cs.CV]\nShen Sang and M. Chandraker. 2020. Single-Shot Neural Relighting and SVBRDF\nEstimation. In ECCV.\nTianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. 2021. Deep\nMarching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape\nSynthesis. In Advances in Neural Information Processing Systems (NeurIPS).\nJunshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong\nChen. 2023. Make-It-3D: High-Fidelity 3D Creation from A Single Image with\nDiffusion Prior. arXiv preprint arXiv:2303.14184 (2023).\nShubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Jitendra Malik. 2017. Multi-\nview supervision for single-view reconstruction via differentiable ray consistency.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition.\n2626\u20132634.\nKalyan Alwala Vasudev, Abhinav Gupta, and Shubham Tulsiani. 2022. Pre-train, Self-\ntrain, Distill: A simple recipe for Supersizing 3D Reconstruction. In Computer Vision\nand Pattern Recognition (CVPR).\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich.\n2022. Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D\nGeneration. arXiv preprint arXiv:2212.00774 (2022).\nFelix Wimbauer, Shangzhe Wu, and Christian Rupprecht. 2022. De-rendering 3D\nObjects in the Wild. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 18490\u201318499.\nChao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia\nGkioxari. 2023. Multiview Compressive Coding for 3D Reconstruction. arXiv\npreprint arXiv:2301.08247 (2023).\nDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang.\n2022. NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360\u00b0\nViews. arXiv preprint arXiv:2211.16431.\nLing-Qi Yan, Yahan Zhou, Kun Xu, and Rui Wang. 2012. Accurate translucent material\nrendering under spherical Gaussian lights. In Computer Graphics Forum, Vol. 31.\nWiley Online Library, 2267\u20132276.\nKai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. 2021. PhySG:\nInverse Rendering with Spherical Gaussians for Physics-based Material Editing and\nRelighting. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR).\nLvmin Zhang and Maneesh Agrawala. 2023. Adding conditional control to text-to-\nimage diffusion models. arXiv preprint arXiv:2302.05543 (2023).\nHyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nreference\u00a0\nimage\nreconstruction\nview\nnovel\u00a0view\u00a01\nnovel\u00a0view\u00a01\nnormal\nnovel\u00a0view\u00a02\nnovel\u00a0view\u00a02\nnormal\nFigure 10: Additional results by HyperDreamer with more views. Images in the last column are specular and roughness map\nrespectively (from top to bottom).\n"
  },
  {
    "title": "Beyond Surface: Probing LLaMA Across Scales and Layers",
    "link": "https://arxiv.org/pdf/2312.04333.pdf",
    "upvote": "18",
    "text": "Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and\nLayers\nNuo Chen\u2663\nNing Wu\u2662\nShining Liang\u2662\nMing Gong\u2662\nLinjun Shou\u2662\nDongmei Zhang\u2662\nJia Li\u2663\n\u2663Hong Kong University of Science and Technology (Guangzhou)\nHong Kong University of Science and Technology\n\u2662Microsoft\nnchen022@connect.ust.hk, jialee@ust.hk\nAbstract\nThis paper presents an in-depth analysis of\nLarge Language Models (LLMs), focusing on\nLLaMA, a prominent open-source foundational\nmodel in natural language processing. Instead\nof assessing LLaMA through its generative out-\nput, we design multiple-choice tasks to probe\nits intrinsic understanding in high-order tasks\nsuch as reasoning and calculation. We exam-\nine the model horizontally, comparing different\nsizes, and vertically, assessing different layers.\nWe unveil several key and uncommon findings\nbased on the designed probing tasks: (1) Hor-\nizontally, enlarging model sizes almost could\nnot automatically impart additional knowledge\nor computational prowess. Instead, it can en-\nhance reasoning abilities, especially in math\nproblem solving, and helps reduce hallucina-\ntions, but only beyond certain size thresholds;\n(2) In vertical analysis, the lower layers of\nLLaMA lack substantial arithmetic and fac-\ntual knowledge, showcasing logical thinking,\nmultilingual and recognitive abilities, with top\nlayers housing most computational power and\nreal-world knowledge.\nThese findings pro-\nvide new observations into LLaMA\u2019s capabil-\nities, offering insights into the current state\nof LLMs. To reproduce our results and ac-\ncess datasets, please refer to https://github.\ncom/nuochenpku/LLaMA_Analysis.\n1\nIntroduction\nLarge language models (LLMs) (OpenAI, 2023;\nScao et al., 2022; Chen, 2023; Yao et al., 2022;\nChen et al., 2023c) have shown significant potential\nin numerous high-order open-generation tasks such\nas mathematical and logical reasoning. LLaMA\n(Touvron et al., 2023b), an open-source, state-of-\nthe-art foundational large language model has been\ndesigned to facilitate research in natural language\nprocessing communities. In a relatively brief pe-\nriod, LLaMA has garnered significant attention.\nThis prominence can be attributed to its inherent\nxMPS\nMPS\nFact\nCal\nLogical\nTruthful\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nLLaMA 2-7B_1\nLLaMA 2-7B_32\nLLaMA 2-13B_1\nLLaMA 2-13B_40\nLLaMA 2-70B_1\nLLaMA 2-70B_80\nFigure 1: Overall Comparison with LLaMA 2 7B-70B\nin our probing tasks. Detailed introduction of each task\ninclude in Section 3. Dashed lines represent the first\nlayer of each model, while solid lines represent the last\nlayer of the model.\naccessibility and demonstrated efficacy across a di-\nverse array of text-generation tasks (Hu et al., 2021;\nChen et al., 2022b, 2023a; Gao et al., 2023). Be-\nyond LLaMA\u2019s impressive generative capabilities,\ncan we further uncover its intrinsic understanding\nabilities? Does bigger and deeper always lead to\nbetter performances in its advanced capabilities\nsuch as computational and reasoning sensitivity?\nAddressing this question is not only instrumental in\ncomprehending the foundations of its success, but\nit also facilitates an understanding of its inherent\nlimitations. This, in turn, can guide future advance-\nments in the architecture and training optimization\nof LLMs.\nIn this paper, we conduct a series of experiments\nto probe the nature of LLaMA on five higher-order\ntasks under in-context learning, including calcula-\ntion, math problem solving (MPS), logical reason-\ning, truthfulness, and factual knowledge detection.\narXiv:2312.04333v4  [cs.CL]  9 Jan 2024\nThe latter two are considered as the important sym-\nbols of hallucination. In these tasks, we probe the\nmodel\u2019s capabilities from two distinct perspectives:\n1) Horizontally: Comparing the model\u2019s abilities\nacross different sizes (Scaling Law); 2) Vertically:\nComparing the different layers capabilities of the\nsame size model (Layer-wise). Instead of directly\ntesting LLMs via their open-text generation abil-\nities, as is usually done, we prob LLaMA with a\nset of challenging multiple-choice questions. The\nprimary considerations for this design are: Firstly,\nit offers controlled and efficient evaluation, with\nclear, quantifiable results that reduce ambiguity.\nThis approach allows for directly targeted testing\nof specific knowledge areas and reasoning skills,\nas well as validating models\u2019 sensitivity to correct\nor incorrect answers; Secondly, our experimental\nobservations reveal a tendency for LLaMA\u2019s lower\nlayers to produce repetitive words rather than co-\nherent sequences, which would lead to an unfair\nlayer-wise comparison.\nIn the context of our experiments corroborating\neach other, we draw the following conclusions:\nHorizontally: (1) The primary benefit of increas-\ning model size lies in the enhanced reasoning abili-\nties of the models, most notably in their improved\ncapacity in MPS. This increase in size also tends to\nreduce the occurrence of hallucinations. However,\nthese improvements are only evident when certain\nLLM size thresholds are surpassed, known as emer-\ngent abilities (Wei et al., 2022). For instance, mod-\nels ranging from 7B to 13B show comparable per-\nformance across all probing tasks. It\u2019s only when\nthe model size increases from 13B to 70B param-\neters that a noticeable improvement in reasoning\ncapabilities and a reduction in hallucination issues\ncan be observed, as shown in Figure 1; (2) The pure\narithmetic capabilities and inherent factual knowl-\nedge of LLaMAs with different parameter sizes are\nremarkably similar. In other words, increasing the\nmodel size does not necessarily impart additional\nfactual knowledge or significantly enhance com-\nputational capabilities, especially when the same\nvolume of pre-training corpus is used.\nVertically: (1) We find that the lower and middle\nlayers of LLaMA have almost negligible pure arith-\nmetic and factual knowledge capabilities. As the\nnetwork layers deepen, there is a noticeable leap\nin performance. Contrarily, even at the very low-\nest layers, LLaMA possesses logical thinking and\nrecognitive abilities, such as in mathematics, logi-\ncal reasoning, and avoiding hallucinations. While\nthese abilities do enhance slightly with deeper lay-\ners, the improvement remains quite limited. This\nimplies that current LLMs predominantly house\ncomputational power and real-world knowledge\nin their upper layers, while the lower layers are\ngeared towards relevant abstract thinking but lack\nsubstantial real-world knowledge and computa-\ntional skills.\n(2) Interestingly, in our layer-by-\nlayer performance comparisons, we observe that\nthe model\u2019s optimal performance in MPS and com-\nputational abilities is not always at the final layer.\nMore often, these peak capabilities are found in\nseveral layers before the last. However, in contrast,\nfor representing factual knowledge, the final layer\nof the model proves to be exceptionally crucial.\nFurther, we extend the mathematical probing\ntasks to the cross-lingal reasoning context. Specifi-\ncally, we maintain the questions and incorrect op-\ntions unchanged and translate the correct answers\ninto other languages to assess the LLaMA\u2019s multi-\nlingual proficiency. In this setting, our layer-wise\nexperiments show an effect completely opposite\nto monolingual reasoning: models\u2019 performance\ngradually decreases as the layers deepen. This indi-\ncates that LLaMA\u2019s earlier layers are responsible\nfor preserving general multilingual features.\nOf note, the results presented in our experiments\ndo not necessarily equate directly to the generative\ncapabilities of LLaMA. Rather, in this paper, we\nprovide a novel and comprehensive perspective for\nobserving the natural performance of LLaMA, giv-\ning insights to understand the current LLMs better.\n2\nLLaMA\nLLaMA (Touvron et al., 2023a,c) is a series of foun-\ndation large language models, released by META,\nhas becomes the most popular open-source LLMs\nin NLP communities. LLaMA is built on trans-\nformer layers (Vaswani et al., 2017), trained on\ntrillion of tokens with the language modeling ob-\njective, showing powerful abilities down-stream\ntasks. The contextualized representations are op-\ntimized by predicting the next token based on the\ninput sequences.\nIn this work, we probe LLaMA 2 series LLMs\nwith our designed tasks, ranging from 7B to 70B\nparameters in in-context learning. Concretely,\nLLaMA 2-7B, 13B and 70B consist of 32, 40 and\n80 transformer layers with 4096, 5120 and 8192\nhidden embedding sizes, separately.\nType\nBit\n+\n-\n\u00d7\n\u00f7\nMix-2\nMix-3\nInt\n1-2\n200\n200\n200\n200\n200\n200\n3-4\n200\n200\n200\n200\n200\n200\n5-6\n200\n200\n200\n200\n200\n200\nFloat\n1-2\n200\n200\n200\n200\n200\n200\n3-4\n200\n200\n200\n200\n200\n200\n5-6\n200\n200\n200\n200\n200\n200\nTable 1: Test data statistics in our arithmetic tasks.\n3\nProbing Tasks\nProbing tasks are generally utilized to explore the\ninherent knowledge and linguistic features within\ndeep learning models. Previously, Jawahar et al.\n(2019a) employed a series of probing tasks to exam-\nine the internal representations of BERT. However,\nwith the rapid advancement of LLMs, there cur-\nrently lacks comprehensive research that deeply an-\nalyzes the relationship between the higher-order ca-\npabilities of contemporary LLMs and factors such\nas model size and network layers.\nTo bridge this gap, we use probing tasks to ac-\ncess LLaMA in their ability to encode different\ntypes of features across two views: model size\nand individual layers. Specifically, we devise five\nhigh-order tasks: calculation, math problem solv-\ning (MPS), logical reasoning, truthfulness, and\nfactual knowledge detection. We include the latter\ntwo as the hallucination detecting in the following.\nBesides, we also probe LLaMA efficiency in mul-\ntilingual mathematical reasoning. In this section,\nwe will illustrate them sequentially.\n3.1\nCalculation\nIn this paper, we focus on testing LLMs in basic\narithmetic tasks, including four simple arithmetic\nexpressions: addition (+), subtraction (-), multipli-\ncation (\u00d7) and division (\u00f7):\n\u2022 Add\nof\ntwo\nelements\nwithin\n1\u223c100,\n100\u223c10000, 10000\u223c100000, separately.\n\u2022 Subtract of two elements within 1\u223c100,\n100\u223c10000, 10000\u223c1000000, separately.\n\u2022 Multiply of two elements within 1\u223c100,\n100\u223c10000, 10000\u223c1000000, separately.\n\u2022 Division of two elements within 1\u223c100,\n100\u223c10000, 10000\u223c1000000, separately.\n\u2022 Complex arithmetic operations that require\nperforming two operations of addition, sub-\ntraction, multiplication, or division.\n\u2022 Complex arithmetic operations that require\nperforming three operations of addition, sub-\ntraction, multiplication, or division.\nOf note, the elements used in the above arith-\nmetic operations include integers and floating-\npoint numbers (with precision up to three decimal\nplaces), separately. Table 1 shows the correspond-\ning data statistics. Since we probe the computa-\ntional abilities of LLaMA through the multiple-\nchoice question answering task, to increase the\ndifficulty and test the model\u2019s sensitivity to minor\ndifferences in computational results, we randomly\nadd or subtract a floating-point number within \u00b120\n(except 0) to the correct answer to create three dif-\nferent but indistinct incorrect options.\nThis design of our test set allows for an intu-\nitive and fine-grained comparison of 1) the model\u2019s\nrelative strengths and weaknesses in addition, sub-\ntraction, multiplication, and division operations;\n2) the model\u2019s performance patterns when faced\nwith complex calculations; 3) the variations in the\nmodel\u2019s computational abilities when dealing with\nfloating-point numbers and integers, 1-2 digit, 3-4\ndigit, 5-6 digit numbers respectively. Our data are\nconstructed by calling python random.randint()\nand random.uniform() functions.\n3.2\nMath Problem Solving\nBesides validating LLaMA in arithmetic tasks, we\nalso test the model in MPS tasks to comprehen-\nsively review its math reasoning abilities.\nWe select GSM8K (Cobbe et al., 2021) as our\nsource data to construct challenging and misleading\noptions that effectively fool the model. Our strategy\ninvolves the following steps:\n\u2022 We first fine-tune the LLaMA 2-13B model on\nGSM8K, and then perform rejection sampling\nvia inference 100 times to generate various\nreasoning paths based on the resulting model.\n\u2022 Next, we extract all the formulas in each rea-\nsoning path and validate their accuracy. We\nuse the erroneous reasoning paths to construct\nour probing task data:\n\u2013 If a reasoning path only contains compu-\ntational errors, meaning the correct an-\nswer can be obtained by recalculating,\nwe retain it as part of our MPS-Cal prob-\ning test set.\nTask Type\nQuery & Options\nArithmetic-Int\nQuery: 2331 + 2693 = ?\nOptions: 5024 (\u2713); 5018; 5005; 5025\nQuery: 109848 \u00f7 199 = ?\nOptions: 552.0 (\u2713); 516.0; 558.0; 567.0\nArithmetic-Flo\nQuery: 7.682 + 28.894 = ?\nOptions: 36.576 (\u2713); 28.576; 40.909; 38.076\nQuery: 25.204 \u00d7 88.29 \u00f7 12.133 = ?\nOptions: 183.406 (\u2713); 183.739; 185.406; 181.962\nMPS-Cal\nQuery: Peyton has 3 children and they each get a juice box in their lunch, 5 days a week. The school year is\n25 weeks long. How many juices boxes will she need for the entire school year for all of her children?\nOptions: Peyton needs 25 weeks x 5 days x 3 children = 375 juice boxes (\u2713);\n25 weeks x 5 days x 3 children = 75 juice boxes;\nGiven the conditions of the problem, 3 children, 5 days a week, 25 weeks long, that\u2019s 3*5*25 = 105 juice\nboxes needed.\nMPS-Rea\nQuery: A family of 12 monkeys collected 10 piles of bananas. 6 piles had 9 hands, with each hand having\n14 bananas, while the remaining piles had 12 hands, with each hand having 9 bananas. How many bananas\nwould each monkey get if they divide the bananas equally amongst themselves?\nOptions: The first 6 bunches had 6 x 9 x 14 = 756 bananas. There were 10 - 6 = 4 remaining bunches.\nThe 4 remaining bunches had 4 x 12 x 9 = 432 bananas. All together, there were 756 + 432 = 1188\nbananas. Each monkey would get 1188/12 = 99 bananas (\u2713);\n6 piles had 6 x 9 x 14 = 756 bananas. The remaining 6 piles had 6 x 12 x 9 = 648 bananas. All together, there\nwere 756 + 720 = 1476 bananas. Each monkey would get 1476/12 = 123.0 bananas;\n6 piles had 6 x 9 x 14 = 756 bananas. There were 10 - 6 = 4 piles of bananas with 12 hands and 4 piles of\nbananas with 6 hands. The 4 piles of bananas with 12 hands had 4 x 12 x 9 = 432 bananas. The 4 piles of\nbananas with 6 hands had 4 x 6 x 9 = 216 bananas. There were 756 + 432 + 240 = 1428 bananas. Every\nmonkey will get 1428/12 = 119.0 bananas\nTable 2: Testing examples in our designed calculation and MPS probing tasks.\nTasks\nArithmetic-Int/Float\nReclor\n(x) MPS-Cal\n(x) MPS-Rea\nTruthfulQA\nLAMA\u2217\nAvg. Ground-truth\n1\n1\n1\n1\n3.5\n1\nAvg. Candidates\n4\n4\n3\n4.9\n7.6\n9.7\nTotal Queries\n3600\n500\n712\n1000\n817\n3070\nTable 3: Overall Test data statistics in our probing tasks. LAMA\u2217 refers to we only use s subset of original corpus.\n\u2013 If all computations in a reasoning path\nare correct, but the final conclusion is\nwrong, indicating a reasoning error, we\nuse it for our MPS-Rea test set.\nThe MPS-Cal focuses on assessing the model\u2019s sen-\nsitivity to computational results in solving mathe-\nmatical problems. Conversely, MPS-Rea empha-\nsizes evaluating the model\u2019s ability to discern cor-\nrect from incorrect reasoning paths, requiring a\nsuperior level of understanding and reasoning ca-\npabilities. Table 2 shows several examples in MPS\nand calculation tasks.\n3.3\nLogical Reasoning\nAs a key indicator of the advanced capabilities of\ncontemporary LLMs, logical reasoning stands out\nfor its importance in examining, analyzing, and\ncritically assessing arguments in natural language.\nIn our study, we employ Reclor (Yu et al., 2020) as\na testing platform to evaluate the logical reasoning\nskills of these large models. Reclor comprises a\ndataset derived from logical reasoning questions\nfound in standardized tests for graduate admissions.\nEach sample from Reclor contains one context, one\ncorresponding question and four options.\n3.4\nHallucination Detecting\nHallucination, which means generating content that\ndeviates from real-world facts observed during pre-\ntraining, is considered one of the most challenging\nissues in LLMs. In order to further investigate the\nrelationship between hallucination and model lay-\ners and size, we conduct tests from two aspects:\n1) Measure whether a language model is truthful\nin generating answers to questions, also known as\ntruthfulness; 2) Test the model\u2019s internal factual\nknowledge. We use TruthfulQA MC tasks (Lin\net al., 2022) and LAMA (Petroni et al., 2019) as\ntest beds for these two aspects, respectively. It is\nimportant to note that in TruthfulQA, there may be\nmore than one correct answer, accompanied by 4-5\nincorrect options. As for LAMA, we randomly ex-\ntract a subset containing 3070 questions along with\ntheir 9-10 corresponding options. Table 3 presents\ndetailed data statistics in our probing tasks.\nModel Size\nLAMA\u2217\nReclor\nMPS-Cal\nMPS-Rea\nTruthfulQA\nArithmetic\n(Fact)\n(Logical)\nMC1\nMC3\nInt\nFloat\n7B\n57.9\n20.0\n28.7\n47.0\n28.6\n20.7\n67.9\n52.5\n13B\n57.9\n23.7\n30.2\n46.6\n29.1\n20.7\n70.6\n52.6\n70B\n58.7\n26.4\n48.3\n51.9\n37.3\n27.1\n70.8\n52.9\nTable 4: Overall performances of each size LLaMA 2 model in our probing tasks. LAMA\u2217 refers to we only use s\nsubset of original corpus. MC3 accuracy means the normalized total probability assigned to all true answers among\ncandidates in TruthfulQA.\n3.5\nCross-Lingual Math Problem Solving\nIn this study, we delve further into LLaMA\u2019s multi-\nlingual abilities. We translate the correct answers\nfrom the collected two datasets: MPS-Cal and\nMPS-Rea in Section 3.2 into four additional lan-\nguages: Chinese, French, Spanish, and Thai, while\nkeeping the questions and other incorrect options as\nthey are, the new resulting test sets named xMPS-\nCal and xMPS-Rea. This setting offers several\nadvantages: Firstly, it tests the model\u2019s capacity\nin cross-lingual reasoning transfer, demonstrating\nits proficiency in not just recognizing but also rea-\nsoning in multiple languages. Secondly, by mixing\nincorrect choices with correct answers in different\nlanguages, we robustly assess the model\u2019s adapt-\nability and comprehension across linguistic bar-\nriers. This unique setup challenges the model\u2019s\nability to process and integrate multilingual infor-\nmation, not only evaluates the model\u2019s language-\nspecific capabilities but also its overall versatility\nin cross-lingual understanding and reasoning.\n3.6\nTest Setting\nConsider a probing dataset D = {Q, C, O}, where\nQ, C and O denote a set of questions, contexts\n(only exits for LAMA), and answer options. For\neach question q \u2208 Q, there is a corresponding set\nof answer choices, denoted as o \u2208 O, where o =\n{o1, o2, ..., on\u22121, a}, n is the number of answer\nchoices, and a refers to the correct answer.\nThe model\u2019s task is to identify the correct an-\nswer from the set o for each question q. It need\nto assign the highest log-probability of completion\nfollowing the question, independent of the other an-\nswer choices (Chuang et al., 2023). This selection\nprocess can be mathematically represented as:\no\u2217\ni = argmax log P(oi|q)\n(1)\nAcc =\n(\n1,\nif a\u2217 > (o\u2217\n1, ..o\u2217\nn\u22121),\n0,\notherwise.\n(2)\nWhere log P(oi|q) is the log-probability that the\nchoice oi to question q, as evaluated by the model.\n3.7\nExperimental Settings\nWe select LLaMA 2 from 7B to 70B as our ex-\nperimental subject. Observing that LLaMA 2 ex-\nhibits significant instability in zero-shot testing, we\nchoose to implement few-shot prompting in our\nprobing tasks to optimize the model\u2019s performance.\nIn TruthfulQA and LAMA, we respectively employ\n6-shot (Table 7) and 4-shot (Table 8) approaches.\nFor reasoning tasks, we consistently use 4-shot for\nboth (x) MPS (Table 10) and logical reasoning (Ta-\nble 9). In calculation tasks, we use 6-shot examples\n(Table 6). Detailed prompts are presented in Ap-\npendix A.\n4\nExperiments on Probing Model Size\nIn this section, we are dedicated to presenting a\ncomparison of the results from LLaMA of different\nsizes on our probing tasks, as shown in Table 41.\nIn Table 5, we showcase the detailed performance\nof models under different arithmetic rules and digit\ncounts. Combining these two tables, we can draw\nthe following conclusions:\nIncreasing model size hardly enhance the\nmodel\u2019s internal knowledge.\nFrom Table 4, we\ncan see that the performance of LLAMA 2-7B and\n13B on LAMA is identical , and even increasing\nthe model size to 70B results in only a slight im-\nprovement (58.7% vs. 57.9%). This indicates that\nonly increasing model size is difficult to improve\nthe model\u2019s ability to remember and understand\nknowledge present in the training corpus, provided\nthe training data remains the same.\nIncreasing model size does not significantly\nboost fundamental computational ability.\nSim-\nilarly, in our computational tasks, models of dif-\nferent sizes also show comparable computational\nabilities. Even though the 7B model lags a bit in\n1Of note, we use the model last layer to count its perfor-\nmances in this section.\nSize\nBit\n+\n-\n\u00d7\n\u00f7\nM-2\nM-3\nInteger Arithmetic\n7B\n1-2\n99.5\n100.0\n95.0\n100.0\n55.5\n39.5\n3-4\n98.0\n98.0\n59.0\n59.5\n48.5\n20.5\n5-6\n89.0\n83.0\n53.0\n30.5\n48.5\n17.5\n13B\n1-2\n99.5\n100\n98.5\n100.0\n58\n44.5\n3-4\n99.5\n99.0\n73.5\n69.5\n53.5\n26.0\n5-6\n96.5\n96.5\n63.5\n25.5\n53.5\n17.5\n70B\n1-2\n99.5\n100.0\n98.0\n100.0\n64.0\n46.0\n3-4\n99.0\n99.0\n75.0\n68.0\n42.0\n18.0\n5-6\n100.0\n100.0\n77.0\n23.0\n41.0\n20.0\nFloating-point Arithmetic\n7B\n1-2\n99.5\n100\n23.0\n78.0\n37.0\n30.0\n3-4\n98.0\n98.0\n18.0\n17.0\n32.0\n19.0\n5-6\n94.0\n87.0\n19.5\n13.0\n35.0\n14.5\n13B\n1-2\n99.0\n100.0\n26.0\n90.0\n40.0\n28.0\n3-4\n99.5\n99.5\n14.5\n20.5\n33.0\n19.0\n5-6\n99.0\n96.5\n13.5\n13.5\n39.5\n17.0\n70B\n1-2\n100.0\n100.0\n26.5\n99.0\n43.0\n30.0\n3-4\n98.5\n100.0\n14.0\n39.0\n39.5\n17.0\n5-6\n98.5\n99.5\n14.5\n17.5\n45.5\n17.5\nTable 5: Detailed results of different operations in our\nprobing arithmetic tasks. M-2/3 refers to arithmetic\nexpression that requires 2/3 times mix operations.\n1\n2\n3\n4\n5\n6\n7\nReasoning Steps in MPS-Rea\n20\n30\n40\n50\n60\n70\n80\nAccuracy\n79.5\n64.0\n43.1\n38.2\n31.2\n34.1\n29.0\n75.0\n61.6\n44.8\n36.0\n33.3\n37.6\n26.2\n78.0\n66.9\n50.3\n45.2\n35.4\n40.4\n28.6\nModels\nLLaMA 2-7B\nLLaMA 2-13B\nLLaMA 2-70B\nFigure 2: Overall comparison between LLaMA 2 7B to\n70B dealing with different reasoning steps problems in\nour probing MPS-Rea tasks.\ninteger operations compared to 13B and 70B, it\nstill performs similarly in floating-point operations\n(52.5% vs. 52.6% vs. 52.9%). Obviously, the\ncomputational abilities of 13B and 70B models are\nnearly identical.\nLarger models show a relative improvement in\nreasoning ability and truthfulness.\nIn MPS-Cal,\nwhich requires not just computational ability but\nalso the understanding and reasoning of mathemat-\nical problems, the 70B model significantly outper-\nforms the 7B and 13B models (48.3% vs 30.2%,\n28.7%); MPS-Rea demands a clear discernment\nbetween correct and incorrect reasoning paths, fur-\nther challenging the model\u2019s reasoning capabilities.\nHere, the LLaMA 2-70B still shows considerable\nimprovement. Considering that three LLMs show\nsimilar computational performances, we argue that\nsuch superior improvements could contribute to its\nbetter mathematical reasoning of 70B model.\nFigure 2 further indicate that all sizes of models\nperform well on mathematical problems requiring\n1-2 steps of reasoning, with relative minimal differ-\nences between them. The enhancement of mathe-\nmatical capabilities in the 70B model, relative to\nthe 7B and 13B models, is primarily concentrated\non problems requiring 3-6 steps of reasoning. The\nabove findings demonstrate that the LLaMA series\nmodels all possess elementary reasoning capabil-\nities. However, the ability to solve more complex\nreasoning problems only appears to emerge as the\ncertain model size thresholds are surpassed. More-\nover, when faced with problems requiring 7 steps\nof reasoning, the performance of all models rapidly\ndeclines and shows little difference, indicating that\neven LLaMA 2-70B is still at a \u201cmoderate intelli-\ngence\u201d level, lacking strong reasoning skills.\nIn calculation, LLaMA\u2019s performance declines\nwith increasing operation and numbers com-\nplexity.\nLLaMA possesses strong addition and\nsubtraction capabilities, but its multiplication and\ndivision abilities noticeably decrease with increas-\ning digit count, as seen in Table 5. Compared to\nfloating-point operations, LLaMA is better at inte-\nger operations. Interestingly, in integer operations,\nLLaMA shows better capability in multiplication,\nbut this strength significantly diminishes when deal-\ning with floating-point numbers.\n5\nExperiments on Probing Layer-Wise\nIn this section, we focus on evaluating each layer\nof LLaMA across our different probing tasks. We\npresent comprehensive results of all layers across\nthree size models in Appendix.\nComputational ability primarily exists in the\nupper layers of the model.\nFirst, in Figure 3,\nwe present the performance of different layers of\nmodels ranging from 7B to 70B in conducting 5-\n6 digit integer and floating-point number calcula-\ntions. From the figure, it is evident that almost\nno pure computational ability exists in the lower\nlayers of any size model. However, as the num-\nber of layers increases, there is a significant leap\nin computational ability, peaking in the final few\nlayers. The above results aptly explain why, in the\nMPS-cal probing task, the model\u2019s performance\n7B\n13B\n70B\n7B\n13B\n70B\n(b) 5-6 Bit floating-point arithmetic\n(a) 5-6 Bit integer arithmetic\nFigure 3: Overall comparison between LLaMA 2 7B to 70B dealing with 5\u20136 bit calculations in our probing\narithmetic tasks. We present more detailed results of 1-2 and 3-4 bit calculations in the Appendix B, Figure 7.\n7B\n70B\nFigure 4: Overall Comparison with LLaMA 2-7B and 70B in our probing tasks. We include all layers\u2019 performances\nof each size model in the Appendix B, Table 11, 12 and 13.\nsignificantly improves with the increasing depth\nof layers, as shown in Figure 4. Notably, in most\ncases, the last layer of the model does not neces-\nsarily represent the best computational proficiency,\nespecially in complex arithmetic expressions. For\ninstance, layers 28-29 of the 7B model exhibit bet-\nter computational skills than the last layer.\nModels predominantly embed rich factual\nknowledge within their top layers.\nAs depicted\nin Figure 4, for both the 7B and 70B models, the\nperformances on LAMA suggest the factual knowl-\nedge learned by the LLaMA is also mainly located\nin the upper layers. In contrast, the lower layers\nexhibit a notable deficiency in retaining this knowl-\nedge. Yet, with the increase in layer depth, the\nmodel exhibits a substantial enhancement in its\nability to process and retain factual information.\nRemarkably, it is observed that the LLaMA\u2019s ulti-\nmate layer harbors the greatest amount of factual\nknowledge. This finding stands in contrast to other\nprobing tasks where the model\u2019s peak performance\n7B\n13B\n70B\nFigure 5: Overall Comparison with LLaMA 2-7B to 70B in our xMPS-Rea probing tasks. ES, FR, ZH and TH refer\nto Spanish, French, Chinese and Thai.\nis typically manifested in the penultimate layers,\nrather than in the absolute final layer.\nThe abstract thinking and cognitive abilities of\nLLaMAs are consistently present across all lay-\ners.\nA comparative observation of the model\u2019s\nperformance across various layers in tasks such\nas MPS-Rea, TFQA, and Reclor reveals that even\nin the model\u2019s lowest layers (e.g., the first layer),\nthere is a certain degree of reasoning and cognitive\ncapabilities, particularly in mathematical reason-\ning, which is evidenced by the results in MPS-Rea.\nWhile the top layers still exhibit the best perfor-\nmance for the corresponding probing tasks, the\nimprovement is relatively limited. We speculate\nthat the reason for the small performance gap from\nthe bottom to the top layer in the MPS-Rea probing\ntask for the LLaMA 2-7B model might be due to:\n1) A lack of related mathematical task corpus in\nthe pre-training phase, leading to insufficient train-\ning; 2) The MPS-Rea task demands a high level of\nmathematical reasoning ability, which the current\nLLaMA2-7B model, even at its highest layer, does\nnot possess strong capabilities in.\nEarlier layers across different model scales show\nsimilar abilities.\nIn our probing tasks, the lower\nlayers (such as the first 15 layers) of models of dif-\nferent scales exhibit almost identical performances,\ndespite having different hidden embedding sizes\nand attention heads in their transformer layers. This\nsuggests that as the contextual information pro-\ngresses through LLaMA\u2019s middle-top layers, it be-\ngins to specialize, leading to an increase in high-\norder capacities.\n6\nExperiments on Probing xMPS\nIn this section, we further to probe the multilingual\nproficiency of LLaMA models. The Figure 5 shows\nthe performance of three models in our designed\nxMPS probing tasks across four languages.\nFrom the comparison with Figure 3, we first\nobserve that the LLAMA series models show a no-\ntable decrease in performance in languages other\nthan English, particularly in the low-resource lan-\nguage Thai. Both 7B and 13B LLaMAs still show\nvery similar performance in this domain, indicat-\ning their comparable multilingual abilities, yet the\n70B model consistently outperforms them. Ad-\nditionally, the lower layers of the models exhibit\ncomparable performance across languages, with\ntheir effectiveness in French and Spanish being on\npar with English. This similarity is likely due to\ntheir Latin language family roots and inclusion in\nthe LLaMA pre-training corpus.\nHowever, unlike the results in all previous prob-\ning tasks, the performance of models in these\nlanguages decreases with deeper layers, a trend\nespecially pronounced in the 13B model. Although\nthere is a slight recovery in the top layers, the top-\nmost layer still under-perform than lower layers.\nGiven that prior experiments have indicated sub-\nstantial mathematical reasoning abilities across all\nlayers of the LLaMA series, it appears that the\nlower layers are primarily responsible for retain-\ning multilingual abilities. This trait, however, di-\nminishes with increasing layer depth, impacting the\ntheir ability to correctly interpret answers in other\nlanguages in the xMPS tasks. This phenomenon,\nhowever, is significantly less pronounced in the\nupper layers of the 70B model, indicating that en-\n7B\n70B\n13B\n7B\n13B\n(a) Last Layer\n(b) First Layer\n70B\nFigure 6: 2D T-SNE plot of language embeddings computed from the first and last layers of LLaMA 2 7B-70B on\nthe xMPS-Rea probing task.\nhancing model size or the number of network lay-\ners could be an effective approach to bolstering the\nmultilingual capabilities of LLMs.\nTo further analyze the above phenomenon, we\nperform 2D T-SNE visualizations of the embedding\nrepresentations of LLaMA\u2019s first and last layers\nin the xMPS-Rea task across different languages.\nThese visualizations show that at the model\u2019s top\nlayer, distinct separations between the represen-\ntations of different languages exist. Conversely,\nat the model\u2019s bottom layer, representations of\ndifferent languages, particularly English, French,\nand Spanish, are relatively close and almost blend\ntogether, indicating that the lower layers primar-\nily preserve language-agnostic features. The pro-\nnounced distinction of Chinese and Thai from other\nlanguages mainly stems from the lack of Chinese\nand Thai pre-training data in LLaMA\u2019s corpus.\nSimilar phenomenon also could observe in our\nxMPS-Cal probing task, where we present corre-\nsponding results in Appendix B, Figure 8\n7\nRelated Works\nThe interpretability of neural networks (Peters et al.,\n2018; Goldberg, 2019), especially language mod-\nels, has recently garnered significant attention from\nscholars in the field of Natural Language Process-\ning (NLP). Over the last few years, much of this re-\nsearch has centered on BERT (Devlin et al., 2019),\nexploring how language models capture textual se-\nmantics across different layers (Tenney et al., 2019;\nJawahar et al., 2019b; Liu et al., 2019; Chen et al.,\n2023b; Chuang et al., 2023). For instance, Tenney\net al. (2019) introduced an innovative edge probing\ntask to assess how contextual word representations\nencode sentence structures, covering a spectrum of\nsyntactic, semantic, local, and long-range phenom-\nena. Their findings suggest that language models\ntrained on tasks like language modeling and ma-\nchine translation robustly encode syntactic struc-\ntures. Similarly, Jawahar et al. (2019b) employed\na series of probing tasks within BERT, deduced\nthat the lower layers of BERT capture phrase-level\nsemantic features, mid-layers apprehend syntactic\ngrammatical semantics, and upper layers compre-\nhend sentence-level content, thereby laying a lin-\nguistic foundation for the tailored application of\nlanguage models in specific contexts.\nCurrently, large language models (LLMs) (Ope-\nnAI, 2023; Scao et al., 2022; Chen, 2023; Yao\net al., 2022; Touvron et al., 2023a,c), with their\nexpansive parameter sizes, high-quality and exten-\nsive pre-training corpus, have exhibited astound-\ning capabilities in various generative tasks (Brown\net al., 2020), thereby gaining immense popular-\nity. Particularly in advanced tasks such as math-\nematical reasoning and computation (Chen et al.,\n2023c), these LLMs surpass their predecessors by\na large margin, including smaller-sized language\nmodels like BERT, Roberta (Chen et al., 2022a).\nAmong these, LLaMA (Touvron et al., 2023a,c),\nnotable for its open-source nature and efficiency,\nhas rapidly emerged as a leading model in the realm\nof open-source LLMs. In this evolving landscape,\nseveral questions still remain to be explored, such\nas the interpretability of current LLMs, their intrin-\nsic understanding abilities in high-order tasks, how\ntheir performance varies with changes in model\nsize, and whether the highest layer of the model\nalways represents its best performance?\nAnswering these questions could help under-\nstand the LLMs behaviour, model transparency and\ndesign more effective LLMs, etc. Unfortunately,\nthere are currently no related research findings on\nLLMs. To facilitate the study of this field, we test\nLLaMA series models in five probing tasks from\nthe perspective of model scales and layer-wise, un-\nveiling their success and inherent limitations.\n8\nConclusion\nBeyond generation,\nwe utilize several well-\ndesigned and find-grained probing tasks to probe\nthe intrinsic high-order capacities in LLaMA across\nthe model scales and layers. Our results reveal\nthat LLaMA models have nearly identical compu-\ntational abilities and factual knowledge regardless\nof different scales while increasing size could ben-\nefit reasoning abilities. We also show that lower\nlayers of LLaMA contain multilingual features and\nreasoning abilities while has hardly computational\nabilities and real-world knowledge. We have shown\nthat LLaMA posses abstract thinking and cognitive\nabilities in their all layers. We expect that our study\ncould contribute to build more powerful LLMs and\ngiven insights to help explain the results of LLMs\nin specific domain.\nLimitation\nThe learning dynamics of neural networks, espe-\ncially in LLMs can be quite intricate. Though, we\nhave tried to explain the reasons behind our exper-\nimental findings, there still some question remain\nexplored and hard to explain:\n\u2022 Why LLaMA obtains optimal performances\nin their last 2-7 layers rather than the absolute\nfinal layer in some tasks like computation?\nWe guess the reason of this phenomenon is\nthat models lack sufficient pre-training cor-\npos related to these tasks while there is none\nstraight way to prove this claim.\n\u2022 Why the penultimate layer of LLaMA perform\nmuch better than the last layer in xMPS tasks?\n\u2022 We also observe a remarkable phenomenon:\nessentially all LLaMA models begin to show\nsignificant performance improvements start-\ning from their mid-layer networks. What con-\ntribute to this phenomenon?\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. CoRR,\nabs/2005.14165.\nNuo Chen, Hongguang Li, Yinan Bao, Junqing He,\nXinshi Lin, Qi Yang, Jianfeng Liu, Ruyi Gan, Ji-\naxing Zhang, Baoyuan Wang, et al. 2023a. Orca:\nA few-shot benchmark for chinese conversational\nmachine reading comprehension.\narXiv preprint\narXiv:2302.13619.\nNuo Chen, Linjun Shou, Ming Gong, Jian Pei, and\nDaxin Jiang. 2022a. Bridging the gap between lan-\nguage models and cross-lingual sequence labeling.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1909\u20131923, Seattle, United States. Association\nfor Computational Linguistics.\nNuo Chen, Linjun Shou, Jian Pei, Ming Gong, Bowen\nCao, Jianhui Chang, Jia Li, and Daxin Jiang. 2023b.\nAlleviating over-smoothing for unsupervised sen-\ntence representation. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 3552\u2013\n3566, Toronto, Canada. Association for Computa-\ntional Linguistics.\nNuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Ziyang\nChen, and Jia Li. 2022b. What would harry say?\nbuilding dialogue agents for characters in a story.\narXiv preprint arXiv:2211.06869.\nNuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming\nGong, Yangqiu Song, Dongmei Zhang, and Jia Li.\n2023c. Breaking language barriers in multilingual\nmathematical reasoning: Insights and observations.\narXiv preprint arXiv:2310.20246.\nWenhu Chen. 2023. Large language models are few(1)-\nshot table reasoners.\nIn Findings of the Associa-\ntion for Computational Linguistics: EACL 2023,\npages 1120\u20131130, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon\nKim, James Glass, and Pengcheng He. 2023. Dola:\nDecoding by contrasting layers improves factu-\nality in large language models.\narXiv preprint\narXiv:2309.03883.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. CoRR, abs/2110.14168.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2023. Pal: Program-aided language\nmodels. In International Conference on Machine\nLearning, pages 10764\u201310799. PMLR.\nYoav Goldberg. 2019. Assessing bert\u2019s syntactic abili-\nties.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah.\n2019a. What does BERT learn about the structure of\nlanguage? In ACL (1), pages 3651\u20133657. Associa-\ntion for Computational Linguistics.\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah.\n2019b. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651\u20133657, Florence, Italy. Association for\nComputational Linguistics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. In ACL (1), pages 3214\u20133252. Associa-\ntion for Computational Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073\u20131094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nMatthew E. Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018. Dissecting contextual word\nembeddings: Architecture and representation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1499\u2013\n1509, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language models\nas knowledge bases? In EMNLP/IJCNLP (1), pages\n2463\u20132473. Association for Computational Linguis-\ntics.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Gall\u00e9, et al. 2022.\nBloom:\nA 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Sam Bowman, Dipanjan Das, and\nEllie Pavlick. 2019. What do you learn from con-\ntext? probing for sentence structure in contextualized\nword representations. In International Conference\non Learning Representations.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLlama:\nOpen and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023c.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998\u20136008.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models. Trans. Mach.\nLearn. Res., 2022.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng.\n2020. Reclor: A reading comprehension dataset re-\nquiring logical reasoning. In ICLR. OpenReview.net.\nPrompt of Arithmetic Tasks. (6-shot)\nGive the answer to the arithmetic question step by step.\nQ: 84 + 72 - 1 * 3=\nA: 156-3=153\nQ: 9+7=\nA: 16\nQ: 9*7=\nA: 63\nQ: 84-82+ 152/77=\nA: 2+1.974025974025974=3.974025974025974\nQ: 44/2/4+2=\nA: 22/4+2=5/5+2=7.5\nQ: 9-3*6=\nA: 9-18=-9\nInput:\nQ: <Arithmetic Expression>\nA:\nTable 6: Prompts of the base setting in our experi-\nments.\nPrompt of Truthful QA Tasks. (6-shot)\nInterpret each question literally, and as a question about the\nreal world; carefully research each answer, without falling\nprey to any common myths; and reply \u201cI have no commen\u201d\nunless you are completely certain of the answer.\nQ: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United\nStates in 1955.\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\nQ: What is the square root of banana?\nA: I have no comment.\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make\nobjects appear closer.\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain.\nInput:\nQ: <Question>\nA:\nTable 7: Prompts of the base setting in our experi-\nments.\nA\nAppendix: Prompts\nIn this section, we present prompts used in our\nprobing tasks with few-shot examples.\nTable 6 shows our 6-shot prompts of arithmetic\nPrompt of factural knowledge Tasks. (4-shot)\nPlease complete the following text so that it is factually\ncorrect.\nQ: G20 consists of <mask>.\nA: Canada\nQ: kerosene is a subclass of <mask>.\nA: petroleum\nQ: sundial is a subclass of <mask>.\nA: clock\nQ: Bordeaux and <mask> are twin cities.\nA: Casablanca\nInput:\nQ: <Sentence with a masked term>\nA:\nTable 8: Prompts of the factual knowledge detection\nprobing task used in our experiments.\ntasks, which are used in our all calculation related\nexperiments, including 1-2bit, 3-4bit and 5-6bit.\nFor truthful QA tasks, we follow (Chuang et al.,\n2023) use the same the 6-shot prompts in Table 7.\nTable 8 presents 4-shot prompts in factural\nknowledge detection probing tasks, where few-shot\nexamples are randomly selected from the LAMA\ntraining dataset.\nTable 9 illustrate 3-shot prompts used in logi-\ncal reasoning tasks, where few-shot examples are\nrandomly selected from the Reclor training dataset.\nTable 10 illustrate 4-shot prompts used in MPS\ntasks, which are both used in MPS-Rea and MPS-\nCal sub-tasks. Of note, as proved in (Chen et al.,\n2023c), English CoT prompts could contribute to\nbetter performances in multilingual reasoning tasks.\nHence, we use the same prompt for xMPS tasks.\nB\nAppendix: Layer-Wise Results\nIn this section, we provide detailed layer-wise re-\nsults of LLaMA 2-7B, 13B and 70B models in our\nfive designed probing tasks: MPS-Cal, LAMA, Re-\nclor, TFQA and MPS-Rea, as presented in Table\n11, Table 12 and Table 13, separately.\nFigure 7 shows performances of each size\nLLaMA 2 model dealing with 1-2bit and 3-4bit\ninteger and floating-point calculation tasks.\n7B\n13B\n70B\n(a) 2-bit integer arithmetic\n(b) 2-bit floating-point arithmetic\n7B\n13B\n70B\n7B\n13B\n70B\n(c) 4-bit integer arithmetic\n(d) 4-bit floating-point arithmetic\n7B\n13B\n70B\nFigure 7: Overall Comparison with LLaMA 2-7B to 70B in our probing calculation tasks. Here, we show layer-wise\nresults of each model in 2-bit and 4-bit integer and floating-point arithmetic expression, seaprately.\n7B\n13B\n70B\nFigure 8: Overall Comparison with LLaMA 2-7B to 70B in our probing xMPS-Cal tasks.\nPrompts of logical reasoning tasks. (3-shot)\nPlease answer the logical question based on the passage.\nP: In rheumatoid arthritis, the body\u2019 s immune system mis-\nfunctions by attacking healthy cells in the joints causing the\nrelease of a hormone that in turn causes pain and swelling.\nThis hormone is normally activated only in reaction to in-\njury or infection. A new arthritis medication will contain\na protein that inhibits the functioning of the hormone that\ncauses pain and swelling in the joints.\nQ: The statements above, if true, most strongly support\nwhich one of the following conclusions?\nA: A patient treated with the new medication for rheuma-\ntoid arthritis could sustain a joint injury without becoming\naware of it.\nP: Patient: Pharmacists maintain that doctors should not be\npermitted to sell the medicine that they prescribe because\ndoctors would then be tempted to prescribe unnecessary\nmedicines in order to earn extra income. But pharmacists\nhave a financial interest in having a monopoly on the sale\nof prescription medicines, so their objection to the sale of\nmedicines by doctors cannot be taken seriously.\nQ: The patient\u2019s argument proceeds by\nA: attempting to discredit a position by questioning the\nmotives of the proponents of that position.\nP: Paula will visit the dentist tomorrow morning only if\nBill goes golfing in the morning. Bill will not go golfing\nunless Damien agrees to go golfing too. However, Damien\nhas decided not to go golfing. Ttherefore, Paula will not be\nvisiting the dentist tomorrow morning.\nQ: The pattern of reasoning displayed above most closely\nparallels which of the following?\nA: Kevin will wash his car tomorrow only if Brittany has\nto go visit her grandmother. Unless Aunt Susan has to run\nerrands, Brittany will not have to go visit her grandmother.\nSince Aunt Susan does not have to run errands, Kevin will\nnot wash his car tomorrow.\nInput:\nP: <Context>\nQ: <logical Question>\nA:\nTable 9: Prompts of the logical reasoning tasks in our\nexperiments.\nPrompt of MPS and xMPS Tasks. (4-shot)\nGive the answer to the math question step by step.\nQ: Carly collected 7 starfish with 5 arms each and one\nseastar with 14 arms. How many arms do the animals she\ncollected have in total?\nA: She has 7 * 5 + 14 = 49.\nQ: Manny had 3 birthday cookie pies to share with his 24\nclassmates and his teacher, Mr. Keith. If each of the cookie\npies were cut into 10 slices and Manny, his classmates, and\nMr. Keith all had 1 piece, how many slices are left?\nA: Manny has 3 x 10 = \u00ab3*10=30\u00bb30 cookie pieces in total.\nHe will have 30 - 24 - 1 - 1 = 4 cookie pieces left.\nQ: A new program had 60 downloads in the first month.\nThe number of downloads in the second month was three\ntimes as many as the downloads in the first month, but then\nreduced by 30% in the third month. How many downloads\ndid the program have total over the three months?\nA: The number of downloads of the program in the second\nmonth increased to 3*60 = 180.\nIn the first two months, the total number of downloads\nof the program was 180+60 = 240.\nIn the third month, the number of downloads of the\nprogram reduced by 30/100*180 = 54\nThere were 180-54 = 126 downloads in the third month.\nIn the three months, the total number of downloads of\nthe program was 126+240 = 366.\nThe answer is 366.In the three months, the total number\nof downloads of the program was 126+240 = 366.The\nanswer is 366.\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf\nballs. On Wednesday, he lost 2 more. How many golf balls\ndid he have at the end of wednesday?\nA: Michael started with 58 golf balls.\nAfter losing 23 on Tuesday, he had 58 - 23 = 35.\nAfter losing 2 more, he had 35 - 2 = 33 golf balls. The\nanswer is 33.\nInput:\nQ: <Math Question>\nA:\nTable 10: Prompts of MPS and xMPS tasks in our\nexperiments.\nLayers\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nMPS-Cal\n8.43\n8.57\n8.99\n8.29\n8.57\n9.69\n9.69\n9.83\n9.97\n10.53\n9.97\n9.55\n10.81\n10.96\n11.94\n11.52\n13.48\n16.29\n19.66\n22.47\nLAMA\n3.82\n9.75\n11.66\n11.99\n12.22\n12.75\n13.7\n14.79\n15.38\n17.13\n19.73\n20.06\n18.87\n18.94\n24.9\n24.93\n32.18\n32.81\n35.31\n36.79\nReclor\n14.4\n14.8\n14.4\n15.4\n15.6\n15.2\n15.2\n15.2\n14.8\n15.4\n15\n15.6\n15.2\n15.4\n16\n16\n16.2\n16.4\n17.4\n17.4\nTFQA\n20.32\n20.69\n21.54\n21.54\n21.54\n22.28\n21.91\n23.01\n22.77\n22.64\n22.89\n22.89\n23.13\n23.5\n23.62\n23.75\n24.72\n25.34\n25.83\n26.44\nMPS-Rea\n44.8\n44.7\n44.6\n44.9\n44.7\n44.7\n44.9\n44.8\n44.7\n44.7\n44.9\n44.6\n44.9\n44.9\n45.4\n45.5\n46.1\n46.7\n47.7\n48.3\nLayers\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nMPS-Cal\n21.91\n23.17\n23.88\n22.75\n23.6\n24.58\n23.88\n25\n24.16\n25.28\n30.34\n28.65\nLAMA\n39.86\n42.95\n45.49\n47.5\n48.98\n49.8\n50.3\n50.76\n51.42\n50.53\n48.12\n57.87\nReclor\n18.6\n19.4\n20.6\n20.4\n20\n21.4\n19.6\n20\n20.2\n20\n20.2\n20\nTFQA\n28.03\n27.66\n27.42\n27.78\n28.64\n27.54\n27.05\n28.03\n26.93\n25.46\n26.44\n28.64\nMPS-Rea\n49.3\n50.5\n51.1\n50.8\n51.3\n52.1\n52\n49.5\n49.2\n45.8\n42.2\n47\nTable 11: Layer-wise Results of LLaMA 2-7B on five probing tasks.\nLayers\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nMPS-Cal\n8.57\n8.43\n9.13\n9.41\n10.53\n9.41\n10.39\n10.25\n11.8\n12.08\n11.24\n12.08\n13.06\n12.08\n13.2\n14.61\n16.15\n17.13\n17.7\n20.79\nLAMA\n5.01\n6.19\n6.03\n10.01\n11.56\n13.37\n11.56\n13.08\n14.16\n15.88\n14.06\n13.54\n14.16\n18.08\n16.6\n17.26\n17.62\n18.84\n16.21\n17.03\nReclor\n14.8\n15\n15.4\n15.4\n15.6\n16.4\n16\n16.4\n15.2\n15.4\n15.6\n15.6\n16.2\n15.8\n16.2\n16.2\n17.4\n17.8\n18.4\n19.4\nTFQA\n22.28\n22.03\n23.13\n23.13\n23.5\n24.36\n25.09\n24.36\n24.48\n25.21\n24.72\n24.48\n24.48\n24.6\n25.09\n25.58\n25.95\n26.07\n27.78\n26.19\nMPS-Rea\n44.9\n45.1\n45\n45.2\n45\n45.1\n44.7\n44.8\n45\n44.8\n45\n45.1\n45\n45.2\n45.5\n45.4\n46.2\n46.8\n46.9\n48.3\nLayers\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\nMPS-Cal\n23.17\n26.26\n25.98\n25.28\n25.98\n24.58\n24.02\n23.17\n25.98\n23.31\n24.16\n23.46\n22.89\n23.03\n26.97\n25.56\n26.97\n30.2\n31.18\n30.76\nLAMA\n18.21\n16.86\n17.98\n19.4\n17.42\n18.35\n19.76\n20.75\n21.08\n50.16\n50.96\n51.48\n52.7\n53.36\n54.31\n55.04\n55.76\n56.75\n56.59\n57.97\nReclor\n20.6\n21.2\n22.2\n23.4\n23.2\n21.6\n22.2\n21.8\n22.2\n22\n23.6\n24.2\n24\n24.2\n24\n24\n24.6\n24.6\n23.6\n24.2\nTFQA\n26.93\n25.95\n25.7\n25.83\n25.34\n25.95\n25.58\n25.58\n25.21\n25.09\n23.99\n25.21\n24.85\n25.09\n25.7\n26.32\n27.91\n27.54\n28.03\n29.13\nMPS-Rea\n48.9\n48.6\n49.5\n49.3\n48.6\n48.7\n48.4\n48.8\n49.9\n50.5\n49.5\n50\n49.5\n50.7\n51.6\n52\n48.8\n46.8\n40.6\n46.6\nTable 12: Layer-wise Results of LLaMA 2-13B on five probing tasks.\nLayers\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nMPS-Cal\n8.01\n8.57\n8.99\n8.99\n9.55\n9.97\n9.27\n10.25\n10.67\n10.39\n9.97\n10.81\n11.24\n10.39\n10.25\n11.52\n10.39\n10.96\n11.52\n10.67\nLAMA\n5.01\n6.19\n6.03\n10.01\n11.56\n13.37\n11.56\n13.08\n14.16\n15.88\n14.06\n13.54\n14.16\n18.08\n16.6\n17.26\n17.62\n18.84\n16.21\n17.03\nReclor\n15\n14.6\n14.8\n15.2\n15\n14.6\n15\n14.6\n14.8\n15.2\n15.4\n15.4\n15\n15.4\n15\n15\n15\n15.4\n15.6\n15.4\nTFQA\n20.69\n22.03\n22.15\n21.3\n21.79\n22.15\n23.01\n22.89\n22.64\n22.52\n22.4\n23.13\n22.89\n22.64\n23.01\n23.13\n23.01\n22.89\n23.13\n23.38\nMPS-Rea\n44.7\n44.6\n44.9\n44.8\n44.7\n44.6\n44.8\n44.8\n44.4\n44.4\n44.3\n44.2\n44.3\n44.4\n44.5\n44.3\n44.5\n44.6\n44.6\n44.5\nLayers\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\nMPS-Cal\n11.66\n10.81\n11.24\n12.22\n12.08\n12.22\n12.36\n12.92\n12.78\n13.34\n13.2\n13.9\n14.33\n13.62\n13.9\n14.75\n16.29\n16.85\n17.7\n18.26\nLAMA\n18.21\n16.86\n17.98\n19.4\n17.42\n18.35\n19.76\n20.75\n21.08\n22.04\n22.17\n21.94\n22.6\n23.48\n30.6\n30.37\n29.84\n30.67\n31.32\n31.42\nReclor\n15.8\n15.4\n15.6\n15.8\n15.2\n15.4\n15\n15.6\n16\n16.6\n16.6\n16.6\n17.2\n17.8\n17.4\n17.6\n18\n17.8\n18.6\n17.8\nTFQA\n23.62\n23.75\n23.87\n23.75\n24.11\n24.36\n24.6\n24.6\n24.72\n25.7\n25.46\n26.44\n26.56\n25.95\n27.05\n26.07\n25.58\n27.29\n27.29\n27.66\nMPS-Rea\n44.6\n44.7\n45\n45\n44.9\n44.9\n45\n44.9\n45.3\n45.4\n45.5\n45.6\n45.8\n46.1\n46\n46.1\n46.4\n46.6\n46.6\n46.9\nLayers\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\nMPS-Cal\n22.61\n23.74\n22.61\n24.72\n26.4\n29.07\n29.35\n29.49\n28.79\n29.63\n28.93\n27.11\n26.69\n27.95\n27.39\n27.25\n25.56\n27.67\n27.95\n27.39\nLAMA\n38.31\n38.57\n42.65\n44.3\n44.93\n45.62\n47.13\n47.79\n48.42\n48.48\n48.95\n49.57\n49.77\n49.97\n49.34\n49.37\n50.36\n50.92\n50.16\n50.43\nReclor\n18.4\n19.2\n20\n20.4\n24\n24.8\n24.2\n25.4\n25\n25.4\n23.6\n23.6\n23.8\n23.4\n24\n24.6\n24.6\n24.6\n25\n25.6\nTFQA\n27.66\n27.29\n28.4\n27.91\n28.64\n29.13\n29.01\n29.01\n28.89\n28.64\n28.52\n28.4\n28.03\n27.66\n27.05\n27.29\n27.78\n27.42\n26.81\n26.56\nMPS-Rea\n47.3\n47.3\n48\n48.5\n47.5\n46.7\n46.6\n45.5\n45.3\n45.5\n46.3\n46.2\n45\n45.2\n45.1\n45.3\n45.9\n46.4\n46.3\n46.6\nLayers\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\nMPS-Cal\n28.93\n27.95\n28.23\n28.51\n28.65\n31.04\n30.48\n33.99\n35.81\n36.8\n41.99\n43.68\n43.54\n42.84\n43.12\n47.61\n49.16\n50.14\n49.72\n48.17\nLAMA\n50.66\n50.59\n50.92\n51.32\n51.15\n51.75\n52.14\n51.98\n51.19\n51.88\n51.68\n51.81\n51.52\n50.13\n49.84\n48.88\n49.41\n48.39\n49.7\n58.71\nReclor\n25\n24.6\n23.8\n24.8\n24.4\n25.2\n26.4\n25.4\n26\n26.6\n27.2\n28.4\n28.4\n27.8\n28.6\n29\n27.2\n28.6\n26.4\n26.4\nTFQA\n26.07\n26.07\n25.95\n25.83\n26.68\n26.93\n28.76\n27.91\n28.27\n29.25\n29.74\n32.93\n33.41\n34.52\n34.76\n35.01\n34.27\n32.31\n32.8\n37.33\nMPS-Rea\n46.6\n46.7\n46.6\n46.4\n46.1\n45.9\n46.2\n46.9\n47.9\n48.9\n51.5\n49.8\n49.8\n46.6\n44.5\n41.5\n40.5\n38.4\n36.8\n51.9\nTable 13: Layer-wise Results of LLaMA 2-70B on five probing tasks.\n"
  },
  {
    "title": "AnimateZero: Video Diffusion Models are Zero-Shot Image Animators",
    "link": "https://arxiv.org/pdf/2312.03793.pdf",
    "upvote": "17",
    "text": "AnimateZero: Video Diffusion Models are Zero-Shot Image Animators\nJiwen Yu1*\nXiaodong Cun2\u2020\nChenyang Qi3\nYong Zhang2\nXintao Wang2\nYing Shan2\nJian Zhang1\u2020\n1 Peking University\n2 Tencent AI Lab\n3 HKUST\nhttps://github.com/vvictoryuki/AnimateZero\nGenerated Image\nOutput Video\nGenerated Image\nOutput Video\nGenerated Image\nOutput Video\n\u201c1girl, underwater, swimsuit, ...\u201d\n\u201c1girl, black jacket, long sleeves, ...\u201d\n\u201cdark fantasy, purple eyes ...\u201d\n\u201cschool uniform, JK, sketch ...\u201d\n\u201cpixel art, cat ears, blonde hair ...\u201d\n\u201ca cat head, look to one side\u201d\n\u201cwaves hit the beach\u201d\n\u201cfreckles, orange hair, glasses, ...\u201d\n\u201clighter, flame, candle\u201d\nFigure 1. Our proposed AnimateZero modifies the architecture of the text-to-video diffusion model, AnimateDiff [11], to achieve more\ncontrollable (e.g., control the appearance using images generated by pre-trained text-to-image models) video generation without further\ntraining. The results above demonstrate the effectiveness of AnimateZero in generating animated videos from the exactly same domains of\nthe generated images. These personalized image domains include anime style, sketch style, pixel-art style, and realistic style. Best viewed\nwith Acrobat Reader. Click the video to play the animation clips. Static frames are provided in supplementary materials.\nAbstract\nLarge-scale text-to-video (T2V) diffusion models have\ngreat progress in recent years in terms of visual quality,\nmotion and temporal consistency. However, the generation\nprocess is still a black box, where all attributes (e.g., ap-\n* Work done during an internship at Tencent AI Lab.\n\u2020 Corresponding Authors.\npearance, motion) are learned and generated jointly with-\nout precise control ability other than rough text descrip-\ntions. Inspired by image animation which decouples the\nvideo as one specific appearance with the corresponding\nmotion, we propose AnimateZero to unveil the pre-trained\ntext-to-video diffusion model, i.e., AnimateDiff, and pro-\nvide more precise appearance and motion control abilities\nfor it. For appearance control, we borrow intermediate la-\n1\narXiv:2312.03793v1  [cs.CV]  6 Dec 2023\ntents and their features from the text-to-image (T2I) gener-\nation for ensuring the generated first frame is equal to the\ngiven generated image. For temporal control, we replace\nthe global temporal attention of the original T2V model with\nour proposed positional-corrected window attention to en-\nsure other frames align with the first frame well. Empow-\nered by the proposed methods, AnimateZero can success-\nfully control the generating progress without further train-\ning. As a zero-shot image animator for given images, An-\nimateZero also enables multiple new applications, includ-\ning interactive video generation and real image animation.\nThe detailed experiments demonstrate the effectiveness of\nthe proposed method in both T2V and related applications.\n1. Introduction\nEmpowered by the recent development of generative pri-\nors in large-scale text-to-image (T2I) diffusion models, the\nvideo diffusion models (VDMs), especially text-to-video\n(T2V) diffusion models, have experienced rapid develop-\nments in terms of the resolutions [9, 14], network struc-\ntures [10, 11, 13], and commercial applications [3, 6], etc.\nAlthough VDMs are easy to use, the whole generation\nprocess is still a black box without precise control capabili-\nties, where the users need to wait for a relatively long time\nto know the generated results if they have limited GPUs.\nMoreover, because most VDMs are trained jointly in terms\nof appearance and temporal aspects, it is not easy to con-\ntrol these two parts separately. These problems can be na-\ntively handled by generating videos by a chain of T2I and\nI2V (Image-to-Video). However, these two different net-\nworks, T2I and I2V model, might not be in the same do-\nmain, e.g., the T2I produces a comic image, whereas the\nI2V diffusion models are only trained on real-world clips.\nThus, the generated results might exhibit domain bias. To\nthis end, we are curious about the detailed generation pro-\ncess in the T2V generation so that we can decouple and con-\ntrol appearance and motion respectively and generate better\nvideos step by step.\nTo achieve this goal, we are inspired by the image ani-\nmation methods to consider the video as a single keyframe\nappearance and its corresponding movement. The keyframe\ncan be described by the text prompt, which is a constant in\nthe generation, and other frames utilize the knowledge of\nthis frame for animation through the temporal modules.\nBased on the above observations, we propose Ani-\nmateZero, a zero-shot method modifying the architecture\nof pre-trained VDMs to unveil the generation process of\nthe pre-trained VDMs so that the appearance and motion\ncontrol can be easily separated. Specifically, we have de-\nsigned spatial appearance control and temporal consistency\ncontrol for these two parts. Spatial appearance control in-\nvolves modifying the spatial modules to insert the generated\nimages into the first frame of generated videos. Temporal\nconsistency control involves modifying the motion modules\nto make other frames aligned with the first frame. Finally,\nwe have achieved step-by-step video generation from T2I\nto I2V in a zero-shot manner. It is worth emphasizing that\nleveraging the well-established Stable Diffusion [24] com-\nmunity, our approach supports various personalized image\ndomains, including but not limited to realistic style, anime\nstyle, pixel art style, and more.\nOur contributions can be summarized as follows:\n\u2022 We propose a novel controllable video generation method\ncalled AnimateZero, which decouples the generation\nprogress of pre-trained VDMs, thus achieving step-by-\nstep video generation from T2I to I2V.\n\u2022 We propose spatial appearance control and temporal con-\nsistency control for AnimateZero to animate generated\nimages in a zero-shot way. Our approach is the first to\nprove that the pre-trained VDMs have the potential to be\nzero-shot image animators.\n\u2022 Experimental results highlight AnimateZero\u2019s effective-\nness in various personalized data domains. In video gen-\neration, AnimateZero surpasses AnimateDiff in similarity\nto the text and the T2I domain. It excels in multiple met-\nrics compared to current I2V methods and is on par with\nthe best method in other metrics.\n2. Related Work\n2.1. Text-to-Video Diffusion Models\nVideo Diffusion Models (VDMs) [18, 20], especially Text-\nto-Video Diffusion Models (T2Vs) [9, 11\u201313, 17, 25, 29,\n30, 38, 40], have experienced rapid development recent\nyears, making significant progress in the quality, diversity,\nand resolution of generated videos.\nMany works within\nthese VDMs are based on tuning text-to-image diffusion\nmodels (T2Is) [24] with the addition of temporal modules.\nThese approaches reduce the training costs of VDMs and\nleverage prior knowledge from the image domain. How-\never, the tuning efforts in these works do not decouple the\nT2Is from the added temporal modules. Instead, they train\nthem together, making it difficult to separate the appearance\nand motion control. Additionally, these methods inevitably\ndisrupt the original T2I domain, resulting in a domain gap.\nRecently, a category of VDMs that decouples T2Is and\nthe temporal modules has emerged [4, 11]. While they pro-\nvide the potential to control appearance and motion sepa-\nrately, they still face the challenge of disrupting the original\nT2I domain (demonstrated in Fig. 4). Our proposed Ani-\nmateZero is based on AnimateDiff [11].\n2.2. Zero-shot Modification for Diffusion Models\nDiffusion models [16, 27, 28], as representatives of large-\nscale vision models, have attracted considerable research\n2\nattention on how to utilize them in zero-shot or training-free\nmanners for various downstream tasks [22, 31, 32, 36, 37].\nAmong these efforts, many works attempt to directly mod-\nify the model architecture to achieve new capabilities,\nfor instance: Prompt-to-Prompt [15] modifies the cross-\nattention of Stable Diffusion [24] for convenient image edit-\ning; ScaleCrafter [14] modifies the convolutional kernels in\nthe UNet of diffusion models to achieve high-quality gen-\neration at higher resolutions; MasaCtrl [8] achieves person-\nalized image generation by sharing keys and values of the\nsource images from the self-attention in Stable Diffusion.\nOur proposed AnimateZero is also a method modifying\nthe architecture of diffusion models, achieving zero-shot\nstep-by-step video generation from generated images.\n2.3. Image-to-Video Diffusion Models\nIn the realm of downstream tasks utilizing VDMs for video-\nrelated applications, there exists a category of work known\nas Image-to-Video Diffusion Models (I2Vs) [5, 9, 35]. The\ngoals of these models are similar to Image Animation, but\nthey differ in some aspects. The primary difference is that\nmost of these methods employ an image encoder to extract\nsemantic features from a reference image to guide video\ngeneration, without requiring the generated video to pre-\ncisely include the given image as the first frame.\nRecently, there have been some attempts to move to-\nwards Image Animation: publicly available tools include\nGen-2 [2], Genmo [3], and Pika Labs [6]. Among them,\nGen-2, as a commercial large-scale model, delivers impres-\nsive results in the realistic image domain in its Novem-\nber 2023 update. However, its performance in other do-\nmains, which might not have been covered in training, is\nstill not entirely satisfactory. Genmo and Pika Labs also\nface the same challenge. Related research papers include\nSEINE [10] and LAMP [34], which are currently under sub-\nmission. However, their I2V models require training and are\nstill dependent on specific training data domains.\nIn comparison, our approach holds unique advantages\ndue to its characteristic of being training-free and support-\ning various personalized image domains.\n3. Preliminaries: AnimateDiff [11]\nTo simplify the experiments and hypotheses, we choose one\nspecific video diffusion model, i.e., AnimateDiff [11], as the\nbase video model, since it only trains additional temporal\nlayers based on a fixed text-to-image diffusion model for\ntext-to-video generation, as shown in Fig. 2. Below, we give\nthe details of the whole network structure of AnimateDiff\nand its motion modules in Section 3.1 and Section 3.2.\n3.1. Architecture Overview\nAnimateDiff [11] aims to learn additional temporal infor-\nmation on top of the pretrained large-scale text-to-image\n...\nSpatial Module\nMotion Module\nOne layer of AnimateDiff\nFigure 2. The architecture of the diffusion UNet in AnimateD-\niff [11]. It decouples the video diffusion model into two kinds of\nmodules: the spatial module is responsible for generating appear-\nance, and the motion module is responsible for generating motion.\nmodel, i.e., stable diffusion [24], for video generation. To\nachieve this, AnimateDiff decouples the video generation\nmodels into the spatial modules and motion modules in-\ndividually (shown in Fig. 2).\nRespectively, AnimateDiff\nfixes the parameters of the spatial modules from the orig-\ninal weights of Stable Diffusion and only trains the motion\nmodules inserted into spatial modules to generate several\nframes all at once. For the training dataset, the authors train\nmotion modules of AnimateDiff on the large-scale WebVid\ndataset [7] with real-world videos to learn the motion prior.\nInterestingly, during inference, we can replace the weights\nof the original spatial modules (i.e., the weights of the origi-\nnal Stable Diffusion) with various personalized checkpoints\nprovided by the community, resulting in high visual quality\nvideos in personalized image domains.\n3.2. Details of Motion Module\nThe magic of AnimateDiff lies in the temporal motion mod-\nules for temporally consistent video generation.\nIn de-\ntail, a motion module consists sequentially of a project-\nin linear layer, two self-attention blocks, and a project-\nout linear layer, respectively as shown in the middle of\nFig. 3. The self-attention operates in the frame dimension,\nfacilitating interactions between frames. Because frame-\nlevel self-attention is independent across different batches,\nheights, and widths, for the sake of simplicity, we omit\nthe batch size, height, and width dimensions in our nota-\ntion. We represent the input of a self-attention as Zin =\n{z1, z2, ..., zf; zi \u2208 Rc\u00d71} where f and c are numbers of\nframes and channels. The self-attention block first adds po-\nsition embeddings P = {p1, p2, ..., pf; pi \u2208 Rc\u00d71} to each\ninput token and then projects them to queries, keys, and val-\nues, which can be described by:\nQ = {qi\ni; qi\ni = Wq(zi + pi), 1 \u2264 i \u2264 f},\nK = {ki\ni; ki\ni = Wk(zi + pi), 1 \u2264 i \u2264 f},\n(1)\nV = {vi\ni; vi\ni = Wv(zi + pi), 1 \u2264 i \u2264 f},\n3\n...\nK V\nSpatial Module\nMotion Module\nK V\nShared K&V\nLatent Insert\nK V\nK V\n...\nK V\nK V\nK V\n...\nK V\nK V\nK V\nProj-In\nProj-Out\nSelf-\nAttn\nOne Step of Sampling\n(a) Self-Attention in AnimateDiff\n(Global Attention)\nStep2: I2V Generation\nStep1: T2I Generation\n...\nGenerated Videos from T2I\n+Position\nFFN\nSelf-\nAttn\n+Position\nFFN\n(b) Positional-Corrected Window \nAttenion\nIn-frame Similarity\nCross-frame Similarity\nSimilarity Computed with \nDuplicated 1st Token\ni-th token with j-th position embedding\nFigure 3. The overall pipeline of our proposed AnimateZero. Given spatial modules from a pre-trained T2I model [24] and its corresponding\nmotion modules [11], we first generate a single image I1 using the T2I model (step1) and then generate a video animated from this image\n(step2). The Left part shows the image generation process with the intermediate latents {z1\nT , ..., z1\n0} and our proposed Spatial Appearance\nControl (Sec. 4.1). Spatial Appearance Control makes modifications to the spatial modules, including the latent insertion for ensuring\nthe first frame equal to I1 and sharing keys and values from spatial self-attention of the first frame across other frames to align both\nsemantics and styles. Right part is the Temporal Consistency Control (Sec. 4.2). We propose modifications to the original self-attention\nin AnimateDiff [11], which is a global attention and illustrated in (a). Our modifications include three key points (illustrated in (b)): (1)\nwe replace global attention to window attention, which computes the i-th output token only using preceding i frames; (2) we duplicate the\nsimilarity computed with the first token to emphasize the importance of the first frame I1; (3) we correct the position embeddings (marked\nas red in the superscripts of q and k, and the calculation of qkv is described by Eq. 2) added to input tokens to get better results.\nwhere Wq, Wk and Wv are the linear projection parame-\nters. Q, K and V represent queries, keys and values. The\nsubscript \u201ci\u201d and superscript \u201cj\u201d in qj\ni indicates the addi-\ntion of i-th input token zi and j-th position embedding pj.\nHere, we distinguish the serial numbers of tokens and po-\nsition embeddings for the convenience of the following ex-\nplanations. Finally, the calculation of output Zout is:\nZout = V \u00b7 Softmax(Q\u22a4K/\u221ac)\u22a4.\n(2)\nIt can be observed that the temporal consistency in Ani-\nmateDiff is achieved through weighted operations of self-\nattention, which average all frames to get smooth results.\n4. Method\nUsing the pre-trained AnimateDiff, our objective is to adapt\nit for step-by-step video generation with better visual and\ncontrollable quality. Specifically, we first generate one sat-\nisfactory image, and then utilize the intermediate latents and\nfeatures of its generation process to guide the video gener-\nation. Our method consists of two parts: the spatial ap-\npearance control, discussed in Sec. 4.1, modifies the spatial\nmodules to guarantee that the generated first frame is equal\nto the given image, while the temporal control, described in\nSec. 4.2, modifies the motion modules to ensure temporal\nconsistency throughout the entire generated video.\n4.1. Spatial Appearance Control\nWe first generate an image using the same personalized T2I\nmodel in AnimateDiff, so that we can get the generated im-\nage I1 and the intermediate latents {z1\nT , ..., z1\nt, ..., z1\n0} re-\nsponsible for generating this image. Then, we can use these\nlatents and features for further animation. The goal of spa-\ntial appearance control is to ensure that the first frame of\nthe generated video is identical to I1. The left part of Fig. 3\nillustrates the control mechanism.\nInserting Intermediate Latents.\nTo exactly mock the\ngeneration process of image animation, for video genera-\ntion, we discard the originally generated latents of the first\nframe in each step. Instead, we insert the intermediate la-\ntents from T2I as replacements. Notice that those interme-\ndiate latents of previous steps have not been involved in the\ntemporal modules. This approach not only ensures that the\nfinal sampled first frame closely resembles I1, but also al-\nlows contents of I1 to participate in the computation of tem-\nporal attention with other frames at each intermediate step.\nSharing K&V in Spatial Self-Attention.\nRelying solely\non temporal attention within the motion module makes it\nchallenging to align the semantic and style information of\nother frames with the first frame. Inspired by studies in per-\nsonalized image generation and editing [8, 33], we make\nspatial modules of all frames share the same keys and val-\nues from the spatial self-attention of the first frame. The\nunderlying implication is that each frame draws values from\nthe same sets, implicitly ensuring similar semantic and style\nacross frames.\n4\n4.2. Temporal Consistency Control\nWhile we have made the first frame identical to I1 using\nspatial appearance control, the motion module introduced\nin Sec. 3.2 does not guarantee temporal consistency. This is\nbecause the weighted operations in self-attention of motion\nmodules are based on the computed similarity between dif-\nferent frames and can not automatically align other frames\nto a specific frame. In order to align other frames with the\nfirst frame explicitly, we propose the Positional-Corrected\nWindow Attention to modify the original global attention\n(shown in the right part of Fig. 3), which will be introduced\nin detail below.\nFrom Global Attention to Window Attention.\nFirst, we\nneed to provide the formula for the self-attention calculation\nin the motion module, where query, key, value, and output\nare denoted as Q, K, V , and Zout, respectively. The spe-\ncific form is as follows:\nQ = {q1\n1, q2\n2, ..., qf\nf ; qi\ni \u2208 Rc\u00d71},\nQ \u2208 Rc\u00d7f,\nK = {k1\n1, k2\n2, ..., kf\nf ; ki\ni \u2208 Rc\u00d71},\nK \u2208 Rc\u00d7f,\nV = {v1\n1, v2\n2, ..., vf\nf ; vi\ni \u2208 Rc\u00d71},\nV \u2208 Rc\u00d7f,\nZout = {\u02c6z1, \u02c6z2, ..., \u02c6zf; \u02c6zi \u2208 Rc\u00d71},\nZout \u2208 Rc\u00d7f,\nwhere c and f represent the numbers of channels and\nframes. The output \u02c6zi for the i-th frame can be written as:\n\u02c6zi = V \u00b7 Softmax((qi\ni)\u22a4K/\u221ac)\u22a4.\n(3)\nFrom Eq. 3, it can be observed that the attention calculation\nrange for each frame is global, meaning K and V include\nkeys and values from all frames (shown in Fig. 3 (a)). Al-\nthough this global design helps in averaging all frames to\nachieve a smooth result, it hinders the ability to align with\nthe first frame. Therefore, our proposed improvement is\nthe introduction of window attention (shown in Fig. 3 (b)),\nwhere the sources of keys and values for the calculation of\nthe i-th output are limited to the preceding i frames. The\nspecific formula can be written as:\n\u02c6zi = \u02dcVi \u00b7 Softmax((qi\ni)\u22a4 \u02dcKi/\u221ac)\u22a4,\n(4)\nwhere \u02dcKi, \u02dcVi \u2208 Rc\u00d7f can be written as:\n\u02dcKi = {k1\n1, ..., k1\n1\n|\n{z\n}\n(f\u2212i+1)\n, ..., ki\ni}, \u02dcVi = {v1\n1, ..., v1\n1\n|\n{z\n}\n(f\u2212i+1)\n, ..., vi\ni}.\n(5)\nAs described in Eq. 5, we duplicate tokens from the first\nframe to ensure that the number of tokens in both \u02dcKi and \u02dcVi\nremains equal to f, emphasizing its importance during the\nattention computation, which further promotes alignment of\nother frames with the first frame.\nCorrect Position Embedding Makes Better Results.\nOur design philosophy for the zero-shot module modifica-\ntion aims to ensure the operations remain unchanged from\nthe original AnimateDiff. The local attention introduced\nabove still has some limitations. The issue lies in the posi-\ntional embeddings. Ideally, a set of keys and values should\ninclude all possible positional embeddings from p1 to pf.\nHowever, because the position embeddings are added be-\nfore attention calculation, the i-th token only carry i-th po-\nsition embedding. Therefore, \u02dcKi and \u02dcVi described in Eq. 5\ninclude only the first i positions.\nBased on this observation, we modified the mechanism\nfor adding positional embeddings (details can be found in\nsupplementary materials) for queries, keys, and values, so\nthat the i-th token is added with the j-th positional embed-\nding (i may not be equal to j). In the end, we achieved that\nthe f tokens in \u02dcKi and \u02dcVi could carry positional embed-\ndings from the 1-st to the f-th position, illustrated in Fig. 3\n(b) and written as:\n\u02dcKi = {k1\n1, k2\n1, ..., kf\u2212i+1\n1\n, kf\u2212i+2\n2\n..., kf\ni },\n\u02dcVi = {v1\n1, v2\n1, ..., vf\u2212i+1\n1\n, vf\u2212i+2\n2\n..., vf\ni }.\n(6)\nAlthough proposed window attention has shown sig-\nnificant advantages over global attention in aligning other\nframes with the first frame, global attention tends to produce\nsmoother results, enhancing the visual quality of the output.\nAs we still need to increase the overall consistency via the\nglobal solution, our final solution integrates the strengths of\nboth attentions into a Diffusion UNet. Specifically, we use\na motion module with local attention in the encoder part\nof the UNet to align each frame with the first frame. In\nthe decoder, we utilize a motion module with global atten-\ntion to smooth all frames. We also find the time-travel sam-\npling strategies will produce smoother results as discussed\nin [32, 36], which we give more additional experiments in\nthe supplementary.\n4.3. Discussion\nFrom the proposed method, we can successfully give more\ncontrol handles to the T2V generation.\nAlso, since we\nfind that the video diffusion model is an image animator,\nour method can also be considered as an image animation\nmethod for the generated image. Given the real image, we\ncan also perform DDIM inversion [21, 26] to get the inter-\nmediate latents. Moreover, our approach, particularly the\naspect related to temporal consistency control, has the po-\ntential to inspire the training of video foundation models,\nleading to improved training-based image-to-video models.\n5\nGenerated Images\nAnimateDiff [11]\nAnimateZero(ours)\nGenerated Images\nAnimateDiff [11]\nAnimateZero(ours)\n(a) \u201c1girl, jewelry, upper body, earrings, pop art, ...\u201d\n(b) \u201c1girl, long hair, looking at the camera, ...\u201d\n(c) \u201c1girl, blue dress, red tie, floating blue, ...\u201d\n(d) \u201c1girl wearing white dress is reading green book, ...\u201d\nFigure 4. Qualitative comparison results between AnimateDiff [11] and our proposed AnimateZero. As shown in (a), (b) and (c), the\nvideos generated by AnimateDiff are not in the same domain as the generated images. In contrast, AnimateZero is capable of maintaining\nconsistency with the original T2I domains; In (a), (c) and (d), it is demonstrated that AnimateDiff may encounter inconsistencies between\nthe provided text and the generated frames (highlighted in red). AnimateZero, on the other hand, performs better in this regard. Best viewed\nwith Acrobat Reader. Click the video to play the animation clips. Static frames are provided in supplementary materials.\nMethod\nWarping Error \u2193 Text-Sim \u2191 Domain-Sim \u2191 Style-Dist \u2193\nAnimateDiff [11]\n0.6719\n0.3254\n0.8081\n0.3809\nAnimateZero (ours)\n0.6562\n0.3314\n0.8671\n0.1666\nTable 1.\nQuantitative comparison results between AnimateD-\niff [11] and our proposed AnimateZero. AnimateZero exhibits a\nhigher similarity to the text and the original T2I domain.\n5. Experiments\n5.1. Implementation and Setting Details\nIn our experiments, spatial modules are based on Stable\nDiffusion V1.5 [24], and motion modules use the corre-\nsponding AnimateDiff [11] checkpoint V2.\nWe exper-\niment with various personalized T2I checkpoints down-\nloaded from Civitai [1], and detailed information about\nthese checkpoints can be found in the supplementary ma-\nterials. For AnimateZero, utilizing both spatial appearance\ncontrol and temporal consistency control is sufficient to\nachieve satisfactory results in most cases, without involv-\ning any hyper-parameters to be chosen. The length for our\ngenerated videos is 16 frames, and the video resolution is\nunrestricted, with a standard resolution of 512 \u00d7 512.\n5.2. Comparison Results\nWe construct a benchmark for quantitative comparison,\nwhich includes 20 prompts and 20 corresponding gener-\nated images. To achieve a comprehensive evaluation, these\nprompts and images include different styles (realistic and\ncartoon styles) and contents (characters, animals, and land-\nscapes). Regarding evaluation metrics in Tab. 2 and Tab. 1,\nwe design: (1) \u2018I1-MSE\u2019 uses MSE to measure whether\nthe generated first frame matches the given image I1; (2)\n\u2018Warping Error\u2019 [19] evaluates the temporal consistency\nof the generated videos; (3) \u2018Text-Sim\u2019 evaluates the simi-\nlarity between the prompt and each generated frame using\ntheir features extracted by CLIP [23] Text and Image En-\ncoders; (4) \u2018Domain-Sim\u2019 assesses the similarity between\nthe T2I domain and the generated videos. We first use the\nT2I model to generate 16 images and then calculate and av-\nerage the CLIP feature similarity between each of these im-\nages and each frame of the generated video; (5) \u2018Style-Dist\u2019\nevaluates the style matching degree between the each gen-\nerated frame and the given image I1, by calculating the dis-\ntance between their style information which is represented\nby the gram matrix of the third layer features of the CLIP\nImage Encoder; (6) \u2018User Study\u2019, which is divided into\nthree aspects: Motion evaluates the quality of the generated\nmotion, Appearance assesses whether the generated appear-\nance matches the given image I1, and Subjective evaluates\nthe subjective quality of the generated videos. We ask 20\nsubjects to rank different methods in these three aspects and\nuse the average rank number to evaluate each method.\nCompared with AnimateDiff.\nWhile AnimateDiff [11]\ndemonstrates good generalization ability on many person-\nalized T2I models, it occasionally produces low-quality\nvideos (shown in Fig. 4), especially on anime-style T2I\nmodels. These low-quality videos mainly manifest in two\naspects: (1) the generated videos are not within the same do-\nmain as the original T2I models; (2) a decrease in text-frame\n6\nGenerated Image\nGen-2 [2]\nGenmo [3]\nPika Labs [6]\nVideoCrafter1 [9]\nI2VGen-XL [5] AnimateZero(ours)\n\u201c1girl, brown hair, a lot of white flowers, leaf, blurry foreground, ...\u201d\n\u201ccloseup face photo of 18 y.o swedish woman in dress, makeup, night city street, motion blur, ...\u201d\nFigure 5. Qualitative comparison results between publicly available image-to-video tools and our proposed AnimateZero. Best viewed with\nAcrobat Reader. Click the video to play the animation clips. Static frames are provided in supplementary materials.\nBasic Metrics\nCLIP Metrics\nUser Study\nMethod\nI1-MSE\u2193\nWarping Error\u2193\nText-Sim\u2191\nDomain-Sim\u2191\nStyle-Dist\u2193\nMotion\u2193\nAppearance\u2193\nSubjective\u2193\nGen-2 [2]\n59.93\n0.7353\n0.3282\n0.7796\n0.1707\n3.57\n2.52\n2.88\nGenmo [3]\n90.76\n0.8284\n0.3184\n0.7801\n0.2752\n2.96\n3.51\n3.21\nPika Labs [6]\n37.68\n0.6018\n0.3372\n0.7876\n0.1275\n3.71\n2.18\n2.84\nVideoCrafter1 [9]\n96.23\n0.6596\n0.3325\n0.7598\n0.2762\n4.29\n5.09\n4.91\nI2VGen-XL [5]\n104.8\n0.7724\n0.3009\n0.7272\n0.4308\n4.63\n5.79\n5.38\nAnimateZero (Ours)\n1.136\n0.6562\n0.3314\n0.8671\n0.1666\n1.83\n1.91\n1.78\nTable 2. Quantative comparison results between publicly available Image-to-Video tools and our proposed AnimateZero. Our proposed\nAnimateZero demonstrated best performance across multiple metrics or achieved comparable results to the best methods in other metrics.\nThe metrics for the best-performing method are highlighted in red, while those for the second-best method are highlighted in blue.\nalignment in the generated videos. Surprisingly, in our ex-\nperiments, we find that AnimateZero excels in both of these\naspects compared to AnimateDiff, which has been demon-\nstrated in Fig. 4. In Tab. 1, we also quantitatively evaluate\nAnimateDiff and AnimateZero on our benchmark at four\nmetrics. Our proposed AnimateZero outperforms Animate-\nDiff in all four metrics in terms of text-frame alignment and\nmatching degree between the generated videos and original\nT2I domains.\nCompared with Publicly Available I2V Tools.\nExisting\nI2V methods claim to be versatile but still struggle with\ndomain gap issues. In our experiments, we use the gen-\nerated image as a condition for video creation, ensuring\nalignment with the T2I domain. This aims to explore Ani-\nmateZero\u2019s advantages over existing I2V methods and high-\nlight their limitations. We compare AnimateZero with sev-\neral publicly available image-to-video tools, both closed-\nsource (Gen-2 [2], Genmo [3], Pika Labs [6]) and open-\nsource (VideoCrafter [9], I2VGen-XL [5]), using bench-\nmark images and their corresponding prompts. In terms of\nsubjective quality, as shown in Fig. 5, our proposed Ani-\nmateZero achieves performance comparable to, or even bet-\nter than, the current state-of-the-art Gen-2 and Pika Labs,\nstanding out as the best among open-source tools. In con-\ntrast, Genmo, VideoCrafter and I2VGen-XL can only lever-\nage the semantic information of the given generated im-\nages, failing to ensure the first frame matches the given im-\nage. Gen-2, Genmo, VideoCrafter and I2VGen-XL suffer\nfrom domain gap issues, particularly noticeable in anime-\nstyle images, whereas AnimateZero does not encounter this\nproblem. We also conduct a comprehensive evaluation of\nAnimateZero and these I2V methods across all metrics in\nTab. 2. It can be observed that our proposed AnimateZero\nachieves the best performance in certain metrics and is com-\nparable to the best methods in other metrics. Considering\nthat AnimateZero is a method that does not require addi-\ntional training specifically for image animation, achieving\nthe mentioned performance is highly remarkable.\n5.3. Ablation Study\nWe conduct ablation experiments on the spatial appearance\ncontrol (introduced in Sec. 4.1) and temporal consistency\ncontrol (introduced in Sec. 4.2). The experimental results\nare shown in Fig. 6 to illustrate the role of each component\nin our proposed method. Firstly, Fig. 6 (a) shows the results\n7\n\u201cdepth of field, \nmasterpiece, best \nquality, 1girl, solo, \nbrown hair, \nlooking at viewer, \nyellow eyes, upper \nbody, white shirt, \nbreasts, off-\nshoulder shirt,a \nlot of white \nflowers, leaf, \nblurry foreground, \nchoker, sunlight\u201d\nGenerated Image\n(a)\nAnimateDiff\n1st frame\n2nd frame\n3rd frame\n14th frame\n15th frame\n16th frame\n13th frame\n4th frame\n...\n...\n...\n...\n...\n(b)\n+insert latent\n(c)\n+shared KV\n(d)\n+TCC w/o PC\n(e)\n+TCC w/ PC\nFigure 6. Demonstration for ablation study: (a) the video generated by AnimateDiff [11]; (b) +inserting intermediate latents responsible for\nthe generation of the given image; (c) +sharing keys and values from the generation of the given image; (d) +temporal consistency control\nwithout position correction (TCC w/o PC); (e) +temporal consistency control with position correction (TCC w/ PC). To clearly illustrate\nthe role of each component, we present static frames, while dynamic videos are provided in the supplementary materials.\ngenerated by AnimateDiff with the provided text, which\nserves as the baseline for our ablation experiments.\nWe\nwill demonstrate the step-by-step process of incorporating\nour proposed techniques to achieve animation of the gener-\nated image. In Fig. 6 (b), we insert the intermediate latents,\nmaking the first frame almost identical to the generated im-\nage. This also implicitly controls the content and style of\nthe other frames. However, notable differences persist in\nterms of style and colors when compared to the generated\nimage. In Fig. 6 (c), we employ the strategy of sharing keys\nand values, further aligning the style and semantic informa-\ntion between the first frame and other frames. However, the\nspatial appearance control mentioned above cannot guaran-\ntee a seamless connection between the first frame and the\nrest frames. This is where our temporal consistency con-\ntrol (TCC) comes into play. We first attempt TCC without\nposition correction (TCC w/o PC) in Fig. 6 (d), which en-\nsures the temporal connection of the first several frames.\nHowever, the quality of frames towards the end of the video\nsignificantly deteriorates. This is addressed by employing\nTCC with position correction (TCC w/ PC) in Fig. 6 (e).\n5.4. Limitations\nAlthough our method enables the possibility of both con-\ntrollable video generation and image animation, there are\nstill some limitations. These limitations mainly stem from\nthe constraints in motion prior within AnimateDiff [11].\nAnimateDiff struggles to generate complex motions, such\nas sports movements or the motion of uncommon objects\n(demonstrated in Fig. 7). In theory, since the generated mo-\ntion of AnimateZero relies on motion prior of AnimateD-\niff, AnimateZero is also less proficient in creating videos in\nthe mentioned scenarios. However, we believe these limi-\nAnimateDiff\nAnimateZero\nAnimateDiff\nAnimateZero\n\u201c1boy, playing football, ...\u201d\n\u201crobot, running, ...\u201d\nFigure 7. AnimateZero is limited by the motion prior of Animate-\nDiff [11], and both perform poorly in complex movements. Best\nviewed with Acrobat Reader. Click the video to play the animation\nclips. Static frames are provided in supplementary materials.\ntations can be solved with a better video foundation model\nwith more powerful motion prior.\n6. Conclusions\nIn this paper, we present AnimateZero, which considers\nvideo generation as an image animation problem, allowing\nus to modify the pre-trained video diffusion model to enable\nmore controllability in terms of appearance and motion. To\nachieve this, for appearance control, we inject the genera-\ntive latents into the video generation so that we can gener-\nate the video concerning the first frame. For motion control,\nwe propose a positional corrected window attention in the\nmotion modules to generate temporally consistent results.\nExperiments show the advantage of AnimateZero compared\nwith the AnimateDiff and the general image-to-video algo-\nrithms. AnimateZero is also the first to show that video\ndiffusion models are zero-shot image animators, which not\nonly allows controllable video generation but also opens up\npossibilities for various applications like animating real im-\nages, interactive video creation, and more.\n8\nReferences\n[1] Civitai. https://civitai.com/, 2023. 6, 11\n[2] Gen-2.\nhttps://runwayml.com/ai- magic -\ntools/gen-2/, 2023. 3, 7, 17, 18\n[3] Genmo. https://www.genmo.ai/, 2023. 2, 3, 7, 17,\n18\n[4] Hotshot-xl.\nhttps://github.com/hotshotco/\nHotshot-XL, 2023. 2\n[5] I2vgen-xl.\nhttps://modelscope.cn/models/\ndamo/Image-to-Video/summary, 2023. 3, 7, 17, 18\n[6] Pika labs. https://www.pika.art/, 2023. 2, 3, 7, 17,\n18\n[7] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In IEEE International Conference on\nComputer Vision, 2021. 3\n[8] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), 2023. 3, 4\n[9] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter1: Open diffusion models for high-quality video\ngeneration, 2023. 2, 3, 7, 17, 18\n[10] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin\nZhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu\nQiao, and Ziwei Liu.\nSeine: Short-to-long video diffu-\nsion model for generative transition and prediction. arXiv\npreprint arXiv:2310.20700, 2023. 2, 3\n[11] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 1, 2, 3, 4, 6,\n8, 12, 16, 19\n[12] Bo He, Jun Wang, Jielin Qiu, Trung Bui, Abhinav Shrivas-\ntava, and Zhaowen Wang.\nAlign and attend: Multimodal\nsummarization with dual contrastive losses. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2023.\n[13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nlong video generation. 2022. 2\n[14] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun,\nMenghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng\nChen, and Ying Shan.\nScalecrafter: Tuning-free higher-\nresolution visual generation with diffusion models.\n2023.\n2, 3\n[15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. 2022. 3\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS), 2020. 2, 12, 13\n[17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv:2204.03458, 2022. 2\n[19] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman,\nErsin Yumer, and Ming-Hsuan Yang. Learning blind video\ntemporal consistency. In European Conference on Computer\nVision, 2018. 6\n[20] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and\nTieniu Tan.\nVideofusion:\nDecomposed diffusion mod-\nels for high-quality video generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023. 2\n[21] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 5\n[22] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2023. 3\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning\n(ICML). PMLR, 2021. 6\n[24] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 2, 3, 4, 6\n[25] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\nMake-a-video: Text-to-video generation without text-video\ndata. In The Eleventh International Conference on Learning\nRepresentations (ICLR), 2023. 2\n[26] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 5, 13\n[27] Yang Song and Stefano Ermon. Generative modeling by es-\ntimating gradients of the data distribution. In Advances in\nNeural Information Processing Systems (NeurIPS), 2019. 2\n[28] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In International Conference on Learning Represen-\ntations (ICLR), 2021. 2\n[29] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2\n9\n[30] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023. 2\n[31] Yinhuai Wang, Jiwen Yu, Runyi Yu, and Jian Zhang.\nUnlimited-size diffusion restoration.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops (CVPRW), 2023. 3\n[32] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot im-\nage restoration using denoising diffusion null-space model.\nIn International Conference on Learning Representations\n(ICLR), 2023. 3, 5, 11, 12\n[33] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual concepts\ninto textual embeddings for customized text-to-image gener-\nation. Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV), 2023. 4\n[34] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi\nLi, and Xiangyu Zhang. Lamp: Learn a motion pattern for\nfew-shot-based video generation, 2023. 3\n[35] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xin-\ntao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter:\nAnimating open-domain images with video diffusion priors.\n2023. 3\n[36] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and\nJian Zhang. Freedom: Training-free energy-guided condi-\ntional diffusion model. arXiv:2303.09833, 2023. 3, 5, 11,\n12\n[37] Jiwen Yu, Xuanyu Zhang, Youmin Xu, and Jian Zhang.\nCross: Diffusion model makes controllable, robust and se-\ncure image steganography. Advances in Neural Information\nProcessing Systems (NeurIPS), 2023. 3\n[38] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent dif-\nfusion models for text-to-video generation. arXiv preprint\narXiv:2309.15818, 2023. 2\n[39] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. 2023.\n12\n[40] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2\n10\nThis appendix includes our supplementary materials as follows:\n\u2022 Section A: Implementation details of utilized T2I checkpoints and the proposed position-enhanced window attention.\n\u2022 Section B: Introduce the effect of time-travel sampling strategy.\n\u2022 Section C: Introduce the extensive applications of AnimateZero.\n\u2022 Section D: Provide more visual results. Dynamic videos can be found in our project page: https://vvictoryuki.\ngithub.io/animatezero.github.io/\nAppendix A. Implementation Details\nA.1. Utilized Personalized T2I Checkpoints\nTo thoroughly evaluate the performance of our approach across diverse image domains, we obtain personalized T2I check-\npoints featuring various styles from Civitai [1] for assessment. Detailed information regarding these checkpoints is provided\nin Tab. 3.\nModel Name\nModel Type\nImage Domain\nURL\nToonYou\ncheckpoints\nAnime\nhttps://civitai.com/models/30240\nCarDos Anime\ncheckpoints\nAnime\nhttps://civitai.com/models/25399\nAnything V5\ncheckpoints\nAnime\nhttps://civitai.com/models/9409\nCounterfeit V3.0\ncheckpoints\nAnime\nhttps://civitai.com/models/4468\nRealistic Vision V5.1\ncheckpoints\nRealistic\nhttps://civitai.com/models/4201\nPhoton\ncheckpoints\nRealistic\nhttps://civitai.com/models/84728\nhelloObject\ncheckpoints\nRealistic\nhttps://civitai.com/models/121716\nTable 3. The sources of all personalized T2I checkpoints used in our experiments.\nA.2. Position Embedding Correction in Self-Attention\nThis subsection presents the detailed calculation process of self-attention with position embedding correction. First, we\nomit the batch size, height, and width dimensions in our notation, assuming that the input tokens of self-attention is Zin =\n{z1, z2, ..., zf; zi \u2208 Rc\u00d71} where c and f represent the numbers of channels and frames. In the first step, we add position\nembeddings, denoted as P = {p1, p2, ..., pf; pi \u2208 Rc\u00d71}, to input tokens. We perform pairwise addition of each element in\nP and Zin, constructing a set {aj\ni; aj\ni = zi+pj, 1 \u2264 i \u2264 f, 1 \u2264 j \u2264 f}. In the second step, we construct a position-corrected\npool for queries, keys and values:\n{qj\ni , kj\ni , vj\ni ; qj\ni = Wqaj\ni, kj\ni = Wkaj\ni, vj\ni = Wvaj\ni, 1 \u2264 i \u2264 f, 1 \u2264 j \u2264 f},\n(7)\nwhere Wq, Wk and Wv represent the linear projection weights of queries, keys and values. In the third step, we obtain the\noutput Zout = {\u02c6z1, \u02c6z2, ..., \u02c6zf; \u02c6zi \u2208 Rc\u00d71} by calculating the proposed window attention:\n\u02c6zi = \u02dcVi \u00b7 Softmax((qf\ni )\u22a4 \u02dcK)i/\u221ac)\u22a4.\n(8)\nThe used keys \u02dcKi and values \u02dcVi are all limited in first i frames, that is,\n\u02dcKi = {k1\n1, k2\n1, ..., kf\u2212i+1\n1\n, kf\u2212i+2\n2\n..., kf\ni }, \u02dcVi = {v1\n1, v2\n1, ..., vf\u2212i+1\n1\n, vf\u2212i+2\n2\n..., vf\ni },\n(9)\nwhere the token from first frame have been copied for f \u2212 i + 1 times to keep the total numbers of utilized keys and values\nequal to f. One detail worth noting is that the superscript of the query token from the i-th frame in Eq. 8 should be consistent\nwith the superscript of the corresponding key and value tokens from i-th frame in Eq. 9.\nAppendix B. Effect of Time-Travel Sampling Strategy\nAs proposed in [32, 36], the time-travel sampling strategy has the ability to improve the visual quality of sampled results.\nIn our experiments, this strategy generates smoother video according to the chosen hyperparameters. Assuming that the\nintermediate video latent code at t-th timestep is Zt = {z1\nt, z2\nt, ..., zf\nt }, where zi\nt is the intermediate latent code for i-th\nframe. For one denoising step, the calculation formula is\nZt\u22121 = Denoiser(Zt, tprompt, t),\n(10)\n11\nGenerated Images\nw/o time-travel\nw/ time-travel\nGenerated Images\nw/o time-travel\nw/ time-travel\n\u201c1girl, red eyes, silver hair, shiny skin, ...\u201d\n\u201c1girl with rainbow hair, really wild hair, ...\u201d\nFigure 8. Demonstrate the role of time-travel sampling strategy [32, 36]. The time-travel sampling strategy can produce smoother and more\nnatural results. However, it should be emphasized that in most cases, AnimateZero can already obtain satisfactory results. The time-travel\nsampling strategy is only used in certain T2I models (such as Anything V5) or certain complex textures (such as hair). Best viewed with\nAcrobat Reader. Click the video to play the animation clips. Static frames are provided in Sec. D.\nin which Denoiser(\u00b7) represents the denoising process and tprompt is the given prompt text. The time-travel sampling\nstrategy iteratively performs denoising operation in Eq. 10 and diffusion operation [16] in:\nZt = \u221a\u03b1tZt\u22121 +\n\u221a\n1 \u2212 \u03b1tN, N = {\u03f51, \u03f52, ..., \u03f5f; \u03f5i \u223c N(0, I)}.\n(11)\nThis approach aims to achieve more harmonious and natural generation results. In the context of video generation tasks,\nwe observed that it has the ability to make the final video smoother. The number of iterations in each timestep determines\nthe degree of smoothness in the final results, and we set the default number as 5. According to the findings in [36], it is\nunnecessary to apply this strategy at every timestep. Instead, we use the time-travel sampling strategy only between the 10-th\ntimestep and the 20-th timestep. We show the comparison results before and after using time-travel sampling strategy in\nFig. 8.\nAppendix C. Applications\nC.1. Improved Video Editing Compared to AnimateDiff [11]\nThe temporal consistency of videos generated by AnimateDiff is notable, and a common use of AnimateDiff is to assist\nControlNet [39] in video editing, aiming to achieve smooth editing results. The specific approach involves inputting feature\ninformation for each frame of the original video (such as extracted depth maps, edge maps, etc.) to ControlNet, thereby\ncontrolling each frame of the video generated by AnimateDiff. The primary challenge encountered in this video editing\nprocess is the inherent domain gap issue of AnimateDiff. This issue significantly degrades the subjective quality of the edited\nvideo, and the alignment degree between the text and the generated video is also substantially reduced. As demonstrated\nin the experimental section of the main paper, AnimateZero exhibits a significant advantage in maintaining the T2I domain\ncompared to AnimateDiff. Therefore, we attempted to use AnimateZero to assist ControlNet in video editing. The results of\nthe editing process showed a noticeable improvement in subjective quality and text-video matching degree compared to An-\nimateDiff. Additionally, AnimateZero still ensures that the generated video remains smooth and good temporal consistency.\nWe showcase some results of AnimateZero performing video editing in our project page.\nC.2. Frame Interpolation and Looped Video Generation\nAnimateZero attempts to insert the first frame into the generated video to achieve image animation from the generated image.\nAn extended idea is whether similar techniques can be used to insert multiple frames. In Fig. 9, we propose an extension to\nthe original position-corrected window attention used by AnimateZero. This extension allows for the simultaneous insertion\nof both the first and last frames. The key modification involves simultaneously emphasizing the tokens corresponding to both\nthe first and last frames, ensuring that the final generated video\u2019s first and last frames match the given images. This technique\nhas the potential application for frame interpolation, allowing interpolation between any two generated images. Additionally,\nwhen the first and last frames are the same, it can be considered as achieving looped video generation. Relevant results are\nshowcased in our project page.\nC.3. Real Image Animation\nAnimateZero has been demonstrated to perform image animation on generated images, but it also has the potential to handle\nimage animation on real images. The main difference between real and generated images is the absence of readily available\n12\n(a) Positional-Corrected Window Attenion\n(insert the first frame)\n(b) Positional-Corrected Window Attenion\n(insert the first and last frame)\nIn-frame Similarity\nCross-frame Similarity\nSimilarity Computed with \nDuplicated Tokenss\ni-th token with j-th \nposition embedding\nFigure 9. Demonstrate the difference between (a) insertion of the first frame and (b) insertion of both first and last frames. The technique\nillustrated in (b) is the basis for achieving applications like frame interpolation and looped video generation.\nintermediate latents. However, we can obtain pseudo intermediate latents through methods like DDIM Inversion [26] or by\ndirectly diffusing the clean latents [16], enabling image generation on real images. Nevertheless, for real images, the issue\nof domain gap is challenging to avoid. This is influenced not only by the style of the real image but also factors such as\nits resolution and whether it has been degraded by some degradation operators. We showcase some results of AnimateZero\nperforming real image animation in our project page.\nAppendix D. More Visual Results\nTo facilitate the accessibility of visual results for non-Adobe users, we have included static frames of videos in main paper\n(Fig. 1, 4, 5, 7) and supplementary materials (Fig. 8) in Fig. 10, 11, 12, 13, 14, 15, 16, . We also provide an HTML file\nwith many generated video examples, which we highly recommend readers to check out. In addition, we have also attached\nthe source files of all videos in the paper and HTML file for the convenience of readers.\n13\n\u201c1girl, underwater, swimsuit, air bubble, looking at viewer, swimming, dappled sunlight, ...\u201d\n\u201c1girl, black jacket, long sleeves, pink hair, hair between eyes, sitting, ...\u201d\n\u201cdark fantasy, purple eyes, cinematic light, white hair, sharp face, hair between eyes, ...\u201d\n\u201cschool uniform, JK, sketch, long hair, clam down, looking at the camera, ...\u201d\n\u201cpixel art, cat ears, blonde hair, wavy hair, portrait of cute girl, ...\u201d\nFigure 10. Static frames sequences in Fig. 1 (part 1).\n14\n\u201ca cat head, look to one side\u201d\n\u201cwaves hit the beach\u201d\n\u201cfreckles, orange hair, glasses, ...\u201d\n\u201clighter, flame, candle\u201d\nFigure 11. Static frames sequences in Fig. 1(part 2).\n15\nAnimateDiff [11]\nAnimateZero\n\u201c1girl, jewelry, upper body, earrings, pop art, ...\u201d\n\u201c1girl, long hair, looking at the camera, ...\u201d\n\u201c1girl, blue dress, red tie, floating blue, ...\u201d\n\u201c1girl wearing white dress is reading green book, ...\u201d\nFigure 12. Static frames sequences in Fig. 4.\n16\nGen-2 [2]\nGenmo [3]\nPika Labs [6]\nI2VGen-XL [5] VideoCrafter1 [9]\nAnimateZero\n\u201c1girl, brown hair, a lot of white flowers, leaf, blurry foreground, ...\u201d\nFigure 13. Static frames sequences in Fig. 5 (part 1).\n17\nGen-2 [2]\nGenmo [3]\nPika Labs [6]\nI2VGen-XL [5] VideoCrafter1 [9]\nAnimateZero\n\u201ccloseup face photo of 18 y.o swedish woman in dress, makeup, night city street, motion blur, ...\u201d\nFigure 14. Static frames sequences in Fig. 5 (part 2).\n18\nAnimateDiff [11]\nAnimateZero\n\u201c1boy, playing football, ...\u201d\nAnimateDiff [11]\nAnimateZero\n\u201crobot, running, ...\u201d\nFigure 15. Static frames sequences in Fig. 7.\nw/o time-travel\nw/ time-travel\n\u201c1girl, red eyes, silver hair, shiny skin, ...\u201d\nw/o time-travel\nw/ time-travel\n\u201c1girl with rainbow hair, really wild hair, ...\u201d\nFigure 16. Static frames sequences in Fig. 8.\n19\n"
  },
  {
    "title": "Pearl: A Production-ready Reinforcement Learning Agent",
    "link": "https://arxiv.org/pdf/2312.03814.pdf",
    "upvote": "14",
    "text": "Pearl: A Production-Ready Reinforcement Learning Agent\nZheqing Zhu*, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel Jiang, Yi Wan,\nYonathan Efroni, Liyuan Wang, Ruiyang Xu, Hongbo Guo, Alex Nikulkov, Dmytro\nKorenkevych, Urun Dogan, Frank Cheng, Zheng Wu, Wanqiao Xu\nApplied Reinforcement Learning Team, AI at Meta\nDecember 8, 2023\nAbstract\nReinforcement Learning (RL) offers a versatile framework for achieving long-term goals. Its gen-\nerality allows us to formalize a wide range of problems that real-world intelligent systems encounter,\nsuch as dealing with delayed rewards, handling partial observability, addressing the exploration and\nexploitation dilemma, utilizing offline data to improve online performance, and ensuring safety con-\nstraints are met. Despite considerable progress made by the RL research community in addressing\nthese issues, existing open-source RL libraries tend to focus on a narrow portion of the RL solution\npipeline, leaving other aspects largely unattended. This paper introduces Pearl, a Production-ready\nRL agent software package explicitly designed to embrace these challenges in a modular fashion.\nIn addition to presenting preliminary benchmark results, this paper highlights Pearl\u2019s industry\nadoptions to demonstrate its readiness for production usage. Pearl is open sourced on Github at\ngithub.com/facebookresearch/pearl and its official website is located at pearlagent.github.io.\nKeywords: Reinforcement learning, open-source software, python, pytorch\n1\nIntroduction\nThe field of reinforcement learning (RL) has achieved significant successes in recent years. These\naccomplishments encompass a range of achievements, from surpassing human-level performance in\nAtari Games (Mnih et al., 2015) and Go (Silver et al., 2017), to controlling robots to in complex\nmanipulation tasks (Mnih et al., 2015; Peng et al., 2018; Levine et al., 2016). Moreover, the practical\napplications of these advancements extend into real-world systems, including recommender systems\n(Xu et al., 2023) and large language models (Ouyang et al., 2022). In addition to these successful\nRL systems, significant progress has been made in designing open-resource libraries that enable\ndeveloping RL systems easily. These libraries include RLLib (Liang et al., 2018), Stable-Baselines 3\n(Raffin et al., 2021), and Tianshou (Weng et al., 2022), to name a few.\nIn addition to tackling the core issues of delayed rewards and downstream consequences, successful\nRL agents must address several significant challenges. One of them is the delicate balance between\nexploration and exploitation. An RL agent must actively engage in exploration to gather information\nabout actions and their outcomes. This challenge is compounded by the fact that the environment\nmay not always offer complete transparency regarding its internal state, requiring the agent to\ninfer the current state from its interaction history. In order to avoid catastrophic situations or\naccommodate other preferences, an RL agent may also need to incorporate additional constraints,\nsuch as safety considerations or risk requirements, throughout the course of learning.\nWhile the importance of these challenges is widely acknowledged by the RL community, existing\nopen source RL libraries often do not address them adequately. For example, important features\nlike exploration, safe/constrained policy learning, credit assignment for long-horizon delayed-reward\n*Corresponding author. Please email: billzhu@meta.com\n1\narXiv:2312.03814v1  [cs.LG]  6 Dec 2023\nFigure 1: Pearl Agent Interface\nsettings, and partial observability are frequently absent. In addition, many libraries do not include\noffline RL methods, even if these methods are commonly adopted in real-world applications Moreover,\nthe open source community has typically viewed RL and bandit problems as two distinct settings\nwith separate codebases. We offer a detailed discussion about existing libraries in Section 3.\nIn this paper, we introduce Pearl, a Production-Ready Reinforcement Learning Agent, an\nopen-source software package, which aims to enable users to build a versatile RL agent for their\nreal-world applications. The focal point of the package is a PearlAgent, which, in addition to a main\n(offline or online) policy learning algorithm, encapsulates one or more of the following capabilities:\nintelligent exploration, risk-sensitivity, safety constraints, and history summarization for the partially-\nobserved/non-Markovian setting. Our package includes several recent algorithmic advancements\nthat address these challenges in the RL research community. Augmenting an RL agent with these\ncapabilities is essential for both research and improving adoption of RL for real-world applications.\nTo achieve these capabilities, we adopted a fully modular design philosophy, empowering researchers\nand practitioners to tailor and combine the features their agents employ as they see fit. For example,\nPearlAgent offers a unified implementation of both RL and bandit methods.\nPearl is built on native PyTorch to support GPU and distributed training. It also provides a\nsuite of utilities for testing and evaluation. Pearl is currently adopted by multiple industry products,\nincluding recommender systems, ads auction pacing, and contextual-bandit based creative selection.\nThese applications require support from Pearl across online exploration, offline learning, safety, data\naugmentation, history summarization, and dynamic action spaces.\nThis paper serves as an introduction of our motivation, features and design choices for Pearl,\nand simple illustrations of user interface to the community. More details are given in Section 2.\nSection 3 compares Pearl to other open-source RL libraries. An initial set of benchmarking results\nis presented in Section 4. Section 5 details current industry adoptions of Pearl.\n2\nPearl Agent\nThis section gives an overview of the design of PearlAgent. PearlAgent has five main modules,\nnamely, policy_learner, exploration_module, history_summarization_module, safety_module\nand replay_buffer. To facilitate a better understanding of the these modules, we will use the\nfollowing notations in throughout the rest of the paper:\n1. Observation: Ot denotes the observation the agent receives at time t. This can be a Markovian\nstate, a non-Markovian partial observation, or a context in the contextual bandit setting.\n2. Action: At \u2208 At denotes an action the agent chooses at time t, while At denotes the available\naction space at time t. We subscript action space by time to enable dynamic action spaces, an\nimportant feature of real-world applications (e.g., in recommender systems, the set of available\nactions changes with time).\n2\n3. Reward: Rt \u2208 R indicates a scalar reward the agent receives at time step t. In this work, we\nassume that when an agent takes an action at time t, it receives a reward at time t + 1.\n4. Markovian state and history: In a Markovian environment, the observation Ot is equivalent to\nthe Markovian state St \u2208 S. When the environment is partially observable, we define history\nHt = (O0, A0, R1, O1, A1, . . . , Ot, At) to denote the history of interactions.\n5. Interaction tuple: Et = (St, At, Rt+1, St+1, At+1) indicates a tuple of current state, action, reward,\nnext state and action space at the next time step. In the case of a contextual bandit problem,\nSt+1 and At+1 can be thought of as set to None.\n2.1\nAgent Design\nConsider the following typical usage scenario: A user of Pearl has access to offline data, either in\nthe form of environment interaction records or (partial) trajectories, along with the ability to interact\nwith the environment to gather additional online data. In designing the PearlAgent, we prioritize\nseveral key elements that are essential for efficient learning in practical sequential decision-making\nproblems. Together, they serve as essential building blocks of a comprehensive RL agent:\n1. Offline learning/pretraining: Depending on the problem setting (contextual bandit or Marko-\nvian transitions), an RL agent should be able to leverage an offline learning algorithm to learn\nand evaluate a policy.\n2. Online learning: With a pretrained/prior policy, the agent should be able to a) explore to\nintelligently collect the most informative interaction tuples, and b) learn from the collected\nexperiences to reason about the optimal policy. The agent should have access to specialized policy\noptimization algorithms appropriate for different problem settings.\n3. Safe learning: For both offline and online learning, an RL agent should have the ability to\nincorporate some form of safety or preference constraints. Users might want to impose such\nconstraints both for data collection (in the online setting) as well as for policy learning.\n4. Representation learning and history summarization: In addition to different modes of\nlearning, the agent should be able to leverage different models for learning state representations,\nvalue and policy functions. Moreover, for partially observable environments, it is important for\nthe agent to have the ability to summarize histories into state representations.\n5. Replay Buffers: For efficient learning, an RL agent should have the ability to reuse data efficiently\nand subset the environment interaction data which it prioritizes to learn from. A common way to\ndo this is through the use of a replay buffer, customized to support different problem settings. To\nenhance learning efficiency, it is important for the agent to have the flexibility to augment the\nreplay buffer with auxiliary information (say, for credit assignment).\nPearl supports all of the above features in a unified way.1\nBesides a suite of policy learning\nalgorithms, users can instantiate a PearlAgent to include an appropriate replay buffer, a history\nsummarization module2 to learn from non-Markovian transitions as well as a safe learning module\nto account for preferences/constraints during policy learning and to filter out undesirable actions\nduring collection of new environment interactions. Modular code design enables seamless integration\nbetween the different functionalities in a PearlAgent. Figure 1 visualizes different components of\na PearlAgent and how they interact with each other.\n1For this iteration, we plan to only support model-free RL methods. Offline evaluations, and model based RL\nmethods are planned for the next version of Pearl\n2We are working to integrate more general state representation tools in Pearl and hope to include it in this version\u2019s\ncode release.\n3\n2.1.1\nAgent Interface\nFigure 1 illustrates interactions amongst components of a PearlAgent in an online learning paradigm.\nEach learning epoch alternates between getting a new environment interaction and a training pass.\nStarting from an observation Ot, along with an estimate of the policy \u03c0t, the PearlAgent queries for\nan interaction tuple Et by taking action At. Note that in all the discussed Pearl components below,\nPearl is not confined to a static action space; it is capable of adapting to dynamic action spaces that\nevolve over time.\nTo account for the trade-off between exploration and exploitation, PearlAgent decides to take\naction At by querying its exploration_module (which outputs an exploratory action Aexplore), in\nconjunction with the policy_learner (which outputs an exploit action Aexploit). To compute the\nexploit action Aexploit\nt\n= \u03c0t(St), PearlAgent enables interaction between the policy_learner and\nthe history_summarization_module , which outputs the state representation.3 PearlAgent design\nenables the safety_module to interact with both the policy_learner and exploration_module\nand account for safety constraints (for example, to filter out undesirable subset of actions)4 when\ncomputing Aexplore and Aexploit respectively. The interaction tuple Et is stored in the replay_buffer.\nDuring a training round at time t, a batch of interaction tuples are fetched from the replay_buffer;\nPearlAgent then queries the history_summarization_module to compute the corresponding state\nrepresentations and generate a batch of history transitions Bt = {Ek}K\nk=1. This batch of data tuples\nis used to update the policy_learner , accounting for safety and preference constraints specified by\nits safety_module. It is also used to update parameters of the history_summarization_module.\nFor an offline learning setup, readers can imagine the environment to be a dataset of interaction\ntuples and the exploration module to be inactive.\nInstead of querying for a new environment\ninteraction tuple Et by passing action At to the environment, an offline PearlAgent would simply\nquery for one of the interaction tuple already present in the offline dataset.\n2.1.2\nPolicy Learner\nIn Pearl, the policy_learner module implements different policy learning algorithms commonly\nused in RL. Any policy_learner module maintains the agent\u2019s current estimate of the optimal\npolicy and updates it using a batch of interaction tuples. A policy_learner module interacts with\nan exploration module, since many forms of exploration use uncertainty estimates of the return5\nor action distribution (in the case of stochastic policies). We do this by implementing the act and\nlearn method for policy learners in Pearl. For value based policy learners and actor-critic methods,\nthe learn method is used to update the corresponding value function estimates. We list the different\npolicy learners supported in Pearl.\n\u2022 (Contextual) bandit algorithms: Common bandit learning methods involve reward modeling, using\nan exploration_module for efficient exploration.6 Pearl supports Linear and Neural Bandit\nLearning along with different exploration_modules, as well as the SquareCB (Foster & Rakhlin,\n2020) algorithm.\n\u2022 Value-based methods: Deep Q-learning (DQN) (Mnih et al., 2015), Double DQN (Van Has-\nselt et al., 2016), Dueling DQN (Wang et al., 2016), Deep SARSA (Rummery & Niranjan,\n1994). We also support Bootstrapped DQN (Osband et al., 2016) alongside its corresponding\nexploration_module.\n\u2022 Actor-critic methods: Soft Actor-Critic (SAC) (Haarnoja et al., 2018), Deep Deterministic Policy\nGradient (DDPG) (Silver et al., 2014), Twin-delayed Deep Deterministic Policy Gradient (TD3)\n3We assume the history Ht also includes the observation Ot. Therefore, the state representation St is a function of\nthe history Ht.\n4In this way, we implement what is typically referred to as \u201cstate dependent action space\u201d in the literature.\n5We use the general term \u201creturn\u201d to refer to rewards for bandit settings and Q-values for the MDP setting.\n6In this iteration, we only support bandit learning algorithms that do not require special neural network architectures.\nEpistemic Neural Network based contextual bandit algorithms Osband et al. (2023); Zhu & Van Roy (2023); Lu &\nVan Roy (2017) will be released in the next version of Pearl.\n4\n(a) PearlAgent Episodic Environment Interaction\n(b) Hydra Configuration for a PearlAgent\nFigure 2: PearlAgent Interaction Interface and Hydra Configeration\n(Fujimoto et al., 2018), Proximal Policy Optimization (PPO) (Schulman et al., 2017), and Policy\nGradient (REINFORCE) (Sutton et al., 1999).\n\u2022 Offline methods: Conservative Q-learning (CQL) (Kumar et al., 2020) and Implicit Q-learning\n(IQL) (Kostrikov et al., 2021).\n\u2022 Distributional RL: Quantile Regression DQN (QRDQN) (Dabney et al., 2018).\n2.1.3\nExploration Module\nThe exploration_module complements policy learners by providing the agent with an exploration\npolicy. Pearl implements the following set of commonly used exploration modules:\n\u2022 Random exploration: \u03f5-greedy (Sutton & Barto, 2018), Gaussian exploration for continuous action\nspaces (Lillicrap et al., 2015), and Boltzmann exploration (Cesa-Bianchi et al., 2017).\n\u2022 Posterior sampling-based exploration: Ensemble sampling (Lu & Van Roy, 2017) and Linear\nThompson sampling (Agrawal & Goyal, 2013). Ensemble sampling supports the notion of \u201cdeep\nexploration\u201d proposed by Osband et al. (2016), which enables temporally consistent exploration\nby acting greedily with respect to an approximate posterior sample of the optimal value function.\n\u2022 UCB-based exploration: Linear upper confidence bound (LinUCB) (Li et al., 2010) and Neural\nLinUCB (Xu et al., 2021).\nExisting implementations of RL and contextual bandit algorithms, typically implement a policy\nlearner with a fixed exploration strategy (e.g., DQN is usually paired with \u03f5-greedy). However,\nPearl\u2019s modular design opens the door to the possibility of \u201cmixing-and-matching\u201d policy learners\nwith exploration modules. Our hope is that this modular design philosophy this can lead to more\nperformant RL and CB solutions in practice, in addition to helping researchers quickly test new\nmethodological ideas.\n2.1.4\nSafety Module\nThe safety module in Pearl is currently designed to offer three main features.\n5\n\u2022 A risk_sensitive_safety_module, which facilitates risk sensitive learning with distributional\npolicy learners. Each risk_sensitive_safety_module implements a method to compute a value\n(or Q-value) function from a distribution over value functions under a different risk metric, and\ncan conform to different risk preferences of an RL agent.\n\u2022 A filter_action safety interface allows the agent designer to specify heuristics or environment\nconstraints to only select state-dependent safe action spaces at each step.\n\u2022 A reward_constrained_safety_module which allows the pearl agent to learn in constrained\nMDPs, with the idea of bounding the long-run costs of a learned policy below a threshold7. We\nuse Reward Constraint Policy Optimization (RCPO) (Tessler et al., 2018) in this safety module\nsince it can be applied to different policy optimization algorithms, can work with general cost\nconstraints and is reward agnostic.\n2.1.5\nHistory Summarization Module\nThe history_summarization_module implements two key functionalities, keeping track of the\nhistory at any environment interaction step and summarizing a history into a state representation.\n\u2022 During the environment interaction step, the history_summarization_module adds (Ht\u22121, Ht)\nto the agent\u2019s replay buffer when the environment is non-Markovian. It also updates the agent\u2019s\nstate using the interaction tuple Et and history Ht\u22121, which can be used by the policy_learner\nto compute an action at the next time step t + 1.\n\u2022 During training, a batch of history transitions {(Hi\u22121, Hi)} are sampled from the replay buffer.\nThe history_summarization_module computes the corresponding state representations and\ngenerates a batch of interaction tuples for the PearlAgent to update other modules.\nIn our current implementation for PearlAgent\u2019s history_summarization_module, we support both\nnaive history stacking and long-short-term-memory (LSTM) (Hochreiter & Schmidhuber, 1997) based\nhistory summarization.\n2.1.6\nReplay Buffer\nThe notion of replay buffer, a container for storing previously observed experiences, is central to\nRL as it enables experience replay, the reuse of past experience to improve learning (Lin, 1992). In\naddition to sub-setting the most informative experiences, replay buffers allow for efficient data reuse\nby breaking the temporal correlations in sequential data. The replay_buffer module in Pearl\nimplements several versions of a replay buffer.\n\u2022 FIFOOffPolicyReplayBuffer is based on a first-in-first-out queue and stores interaction tu-\nples for the off-policy setting. For on-policy settings, we provide an extension in the form of\nFIFOOnPolicyReplayBuffer8.\n\u2022 BootstrapReplayBuffer (Osband et al., 2016) implements bootstrap masks.\nWe also build\nHindsightExperienceReplayBuffer with goal replacement (Andrychowicz et al., 2017)\n2.2\nAgent Usage\nFigure 2a illustrates a typical episodic environment interaction loop where an agent learns a policy for\nan environment with Deep Q-learning. Here, learning occurs at the end of each episode. The Pearl\nEnvironment class is based on the step method, which returns an ActionResult containing reward,\nnext state, and whether the episode has been truncated, terminated, or done. The PearlAgent class\naccepts optional arguments for components such as history summarization module or safety module\n7Users can specify both the per-step costs as well as the threshold.\n8Although replay buffers are not typically used in the on-policy setting, we are able to unify off- and on-policy\nmethods using this abstraction.\n6\nTable 1: Comparison of Pearl agent to alternative popular RL libraries\nFeatures\nReAgent\nRLLib\nSB3\nTianshou\nCleanRL\nPearl\nModularity\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\nIntelligent Exploration\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\nSafety\n\u2717\n\u2717\n\u2717\n\u25e69\n\u25e69\n\u2713\nHistory Summarization\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2713\nData Augmented Replay Buffer\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nContextual Bandit\n\u2713\n\u25e610\n\u2717\n\u2717\n\u2717\n\u2713\nOffline RL\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\nDynamic Action Space\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n(with no-op components being the default). In our example, we specify a history summarization\nmodule that stacks the last three states and a safety module seeking to minimize variance. Likewise,\npolicy learner classes accept an optional exploration module argument; in this example, we use\nan \u03f5-greedy exploration with \u03f5 = 0.05. In practice, it is more convenient to specify agents and\nenvironments via Hydra (Yadan, 2019) configuration files supported by Pearl, which provides a\nconvenient way of running experiments and hyperparameter tuning. A Hydra file generating the\nsame agent as above is shown in Figure 2b.\n3\nComparison to Existing Libraries\nTo illustrate the differences between Pearl with other existing RL libraries, we compared Pearl\u2019s\nfunctionalities to four popular RL libraries, namely, ReAgent (), RLLib (Liang et al., 2018), Stable-\nBaselines3 (Raffin et al., 2021), Tianshou (Weng et al., 2022), and CleanRL (Huang et al., 2022).\nThe main motivation of these libraries is to facilitate reproducible benchmarking of existing RL\nalgorithms.\nAs highlighted in Table 1, Pearl implements several capabilities that are crucial for end-to-end\napplication of an RL system, such as ability to perform structured exploration, offline learning, and\nsafety considerations. Modular design allows users to test performance with different combinations\nof features. In addition, Pearl crucially supports dynamic action spaces, which is an common\nsetting in practical applications. Pearl also explicitly supports bandit policy learners along with the\ncorresponding exploration algorithms. Bandit settings find widespread use in large scale industry\napplications.\nWe mention a few other RL libraries we surveyed while designing Pearl. The d3RLpy (Seno &\nImai, 2022) library only provides algorithm implementations for offline and online (specifically, off-\npolicy algorithms) policy learning. Besides, contextual bandit methods are not supported by d3Rlpy.\nTorchRL (Bou et al., 2023) is a recent modular RL library that implements pytorch and python\nbased primitives which can be used to develop RL systems. Unlike Pearl , TorchRL is designed\nkeeping in mind components which are typically used in a policy learning algorithm implementation.\nAgent design with features like exploration, safe learning etc. is not the focus of TorchRL. Lastly,\nthe Vowpal Wabbit library (Agarwal et al., 2014) offers a rich and diverse set of contextual bandit\nalgorithm implementations, tested on multiple domains and environments. However, to the best of\nour knowledge, it is designed to exclusively support bandit learning settings and does not explicitly\nhave PyTorch support.\n9Even though Tianshou and CleanRL have implementations of quantile regression DQN and/or C51, these are\nmore like standalone algorithm implementations which do not implement generic risk sensitive learning. In addition,\nnone of the existing libraries implement policy learning with constraints for different policy optimization algorithms.\nThis is because most existing libraries focus almost entirely on implementing policy learning algorithms without giving\nconsiderations to other features.\n10Only supports linear bandit learning algorithms.\n7\n4\nBenchmark\n4.1\nReinforcement Learning Benchmarks\nFigure 3: Training returns of discrete control methods on the CartPole task. The left and right\npanels show returns for value- and policy-based methods, respectively.\nWe first benchmarked a PearlAgent with its discrete control methods on a classic reinforcement\nlearning task called Cartpole. Experiment details are omitted and can be found in our code base.\nWe plotted learning curves of the agent\u2019s achieved returns in Figure 3. The x axis shows the number\nof environment steps while the y axis shows the average of episodic returns received over the past\n5000 steps. Each experiment was performed with 5 different random seeds that which fully control\nthe stochasticity the experiments. Shading areas stand for \u00b11 standard error across different runs.\nThese results are only meant to serve as a sanity check since reproducible research is only one of\nPearl\u2019s motivation \u2013 we only checked for stable, consistent learning of our implementations rather\nthan searching for the best training runs with optimal hyperparameter choices.\nWe then benchmarked a PearlAgent with three different actor-critic algorithms on continuous\ncontrol tasks in Mujoco. The results are shown in Figure 4 below. We tested soft-actor critic (SAC),\ndiscrete deterministic policy gradients (DDPG) and twin delayed deep deterministic policy gradients\n(TD3), for a set of commonly used hyperparameters, without tuning them. The axes have the same\nmeaning as those in the discrete control experiments.\nFigure 4: Training returns of SAC, DDPG and TD3 on four Mujoco continuous control tasks.\nWe also test our offline algorithms, specifically, Implicit Q learning (IQL), on continuous control\ntasks with offline data. Instead of integrating with D4RL which has dependencies on older versions\nof Mujoco, we created our own offline datasets following ideas outlined in the D4RL paper (Fu et al.,\n2020). We create a small dataset of 100k transitions by training a soft-actor critic (SAC) based agent\nwith a high entropy coefficient. The dataset comprises of all transitions in the SAC agent\u2019s replay\nbuffer, akin to how the \u201cmedium\u201d dataset was generated in the D4RL paper. In table 2 below, we report\nnormalized scores using the same hyperparameters used in the IQL paper (Kostrikov et al., 2021).\nWe also test our implementation of DQN on Atari games with 3 seeds, with the same convolutional\nneural network architecture as reported in (Mnih et al., 2015), and achieved reasonable performance\nin Pong, Beamrider and Breakout in 5 million steps. See 3 for more details.\n8\nEnvironment\nRandom return\nIQL return\nExpert return\nNormalized score\nHalfCheetah-v4\n-426.93\n145.89\n484.80\n0.62\nWalker2d-v4\n-3.88\n1225.12\n2348.07\n0.52\nHopper-v4\n109.33\n1042.03\n3113.23\n0.31\nTable 2: Normalized scores of Implicit Q learning on different continuous control Mujoco environments.\n\u201cRandom return\u201d refers to the average return of an untrained SAC agent. \u201cIQL return\u201d refers to\nthe average evaluation returns of the trained IQL agent (episodic returns of the trained agent when\ninteracting with the environment). \u201cExpert return\u201d is the maximum episodic return in the offline\ndataset.\nAgent\nBreakout\nBeamRider\nPong\nDQN\n151.00 \u00b1 21.82\n5351.94 \u00b1 400.50\n19.22 \u00b1 0.45\nTable 3: Average performance of our DQN implementation on Atari games.\n4.2\nNeural Contextual Bandits Benchmarks\nWe implemented and tested the performance of neural adaptations of common CB algorithms. Our\nbenchmarks consists of datasets from the UCI repository (Asuncion & Newman, 2007), adapted to\nCB interaction model. The results are depicted in Figure 5. Using supervised learning datasets for\ntesting CB algorithms is common in past literature (Dud\u00edk et al., 2011; Foster et al., 2018; Bietti\net al., 2021). We tested neural implementations of the LinUCB, Thompson Sampling (TS), and\nSquareCB (Li et al., 2010; Agrawal & Goyal, 2013; Foster & Rakhlin, 2020). This showcases the\nsimplicity of combining deep neural networks within CB algorithms in Pearl due to its PyTorch\nsupport. See Appendix A for benchmark setup and implementation details.\n4.3\nAgent Versatility Benchmark\nThis section provides an initial assessment of Pearl\u2019s four primary abilities \u2013 summarizing the history\nto handle partial observable environments, exploring effectively to achieve rewards even when they\nare sparse, learning with cost constraints, and learning risk-averse policies.\nHistory Summarization for Partial Observability:\nTo test Pearl\u2019s ability to handle partial\nobservability, we adapted Acrobot, a fully observable, classic reinforcement learning environment, to\na partial observable variant. In this environment, the goal is to swing up a chain connected by two\nlinkes. In the original Acrobot environment, the agent can perceive the angles and anglular velocities\nof the two links. In our partial observable variant, only the angles are observable. Consequently,\nthe agent must use both current and past observations to deduce the angular velocities, which is\ncrucial for selecting the optimal action. To further increase the degree of partial observability, the\nnew environment is designed to emit its observation every 2 steps and to emit an all-zero vector for\nthe rest of time steps.\nWe tested Pearl\u2019s LSTM history_summarization_module to see if it can handle the partial\nobservability challenge presented in the above environment. The base algorithm was the DQN\nalgorithm (Mnih et al., 2015). We plotted the mean and the standard error of the achieved returns in\nFigure 6a. It shows that 1) without the LSTM history_summarization_module, the agent did not\nachieve any progress of learning, 2) with the history_summarization_module, the agent achieves a\nsignificantly better performance.\nEffective Exploration for Sparse Rewards:\nTo test the agent\u2019s capability to explore, we\nimplemented the DeepSea environment (Osband et al., 2019), known for its exploration challenge.\nThe DeepSea environment has n \u00d7 n states and is fully deterministic. Our experiments chose n = 10,\nin which the chance of reaching the target state under a random policy is 2\u221210. We tested Pearl\u2019s\nimplementation of the Bootstrapped DQN algorithm, which is an exploration algorithm introduced\nby Osband et al. (2016). Again, DQN was used as the baseline. Figure 6b shows the learning curves\n9\nFigure 5: Performance of neural implementations in Pearl of LinUCB, TS and SquareCB on UCI\ndataset and an offline baseline that is considered near optimal.\n(a)\n(b)\n(c)\nFigure 6: Agent Versatility Benchmark Results: (a) Return of DQN with and without LSTM in the\npartial-observable Acrobot-v1 environment. (b) DQN and Bootstrapped DQN in a 10 \u00d7 10 Deep Sea\nenvironment. (c) One may learn a policy that prefers lower variance return using QRDQN with a\nlarge \u03b2.\nof the two tested algorithms. It can be seen that, Bootstrapped DQN achieved the optimal policy\nwhile DQN did not. This suggests that Bootstrapped DQN can perform much better exploration in\nsparse reward environments.\nLearning Risk-Averse Policies:\nWe designed a simple environment called Stochastic Bandit\nto test if Pearl can learn policies that fulfills various degrees of safety needs, by balancing the\nexpectation and the variance of the return. StochMDP only has one state and two actions. The\nreward of each of the two actions follows a Gaussian distribution. The reward distribution for Action\n1 has a mean of 6 and a variance of 1. For Action 2, the mean is 10 and the variance is 9. With the\nclassic reinforcement learning formulation, the goal is to maximize the expected return. Therefore\nthe optimal policy is to always choose Action 2. When the agent wants to maximize the mean of the\nreturn while minimizing the variance, it chooses a weight scalar \u03b2 that balances these two terms.\nDepending on the weight scalar, the optimal policy either always chooses Action 1 or always chooses\nAction 2. The threshold value for the weight scalar is 0.5 because 6 \u2212 0.5 \u00d7 1 = 10 \u2212 0.5 \u00d7 9. While\nthis environment is simple, it can serve as a sanity-check to see whether the test algorithm indeed\nbalance mean and variance as predicted.\nWe tested our implementation of the QR-DQN algorithm (Dabney et al., 2018), which is an\nalgorithm that learns the distribution of the return. Using the learned distribution, the algorithm can\nestimate the mean and the variance of the return, and further maximize the mean while minimizing\nthe variance. Each experiment has 5 runs, each of which consists of 5000 steps. It can be seen from\nFigure 6c that, after training, the agent preferred the lower variance action (Action 1) when \u03b2 was\nhigh and preferred the higher variance action (Action 2) when \u03b2 was low. Our experiment result\nshows that the algorithm has the ability of learning risk-averse policies.\nLearning with Cost Constraints:\nIn many real world problems an agent is required to find\nan optimal policy subject to cost constraint(s), often formulated as constrained MDPs. For many\nreal world problems where a reward signal is not well defined, it might be useful to specify desirable\nbehavior in the form constraints. For example, limiting the power consumption of a motor can be a\n10\nFigure 7: Episodic cost (top) and episodic return (bottom) plots during training on continuous\ncontrol tasks with cost and reward feedback. The plots present performance of TD3 and our cost\nconstraint adaptation of TD3, RCTD3, for multiple values of constraint threshold \u03b1. See text for\ndetails.\ndesirable constraint for learning robotic locomotion. Optimizing for reward subject to constraint(s)\nrequires modification to the learning procedure.\nTo test policy optimization with the reward_constrained_safety_module, we modified a gym\nenvironment with a per step cost function, c(s, a), in addition to the standard reward. We choose\nthe per-step cost c(s, a) = a2, which approximates the energy spent in taking action a. Figure 7\nshows the results of Reward Constraint TD3 (RCTD3) agent, a PearlAgent which uses TD3 as\nthe policy_learner along with the reward_constrained_safety_module. We chose a normalized\ncumulative discounted costs as our constraint function with \u03b1 as the threshold value, namely:\n(1 \u2212 \u03b3) Es\u223c\u03b7\u03c0,a\u223c\u03c0\n\" \u221e\nX\nt=0\n\u02c6\u03b3tc(st, at) | s0 = s, a0 = a\n#\n\u2264 \u03b1\nFigure 7 shows cumulative costs decreasing with a smaller value of \u03b1 for different continuous control\nMujoco tasks. Therefore, an RCTD3 agent optimizes for long-run rewards under the cumulative\ncost constraint as shown above. Interestingly, moderate values of \u03b1 in different environments does\nnot lead to a significant performance degradation, despite controlling for energy consumption of the\ncontrol policy.\nAdapting to Dynamic Action Spaces\nIn many real-world scenarios, agents must adapt to envi-\nronments offering varying action spaces at each time step. A quintessential example is recommender\nsystems, where the agent encounters a distinct set of content to recommend to users at each interval.\nTo evaluate Pearl\u2019s adaptability to these dynamic action spaces, we crafted two environments based\non CartPole and Acrobot. In these environments, every four steps, the agent loses access to action 1\nin CartPole and action 2 in Acrobot, respectively.\nFigure 8 depicts the learning curves for DQN, SAC, PPO, and REINFORCE within these\nspecialized environments. Despite the increased complexity posed by dynamic action spaces, most\nagents successfully developed effective policies after 100,000 steps. Notably, REINFORCE consistently\nunderperformed in comparison to other algorithms.\n5\nExample Industry Product Adoptions\nWe present three industry product adoptions of Pearl as demonstration of Pearl\u2019s capability of\nserving production usage. See Table 4 for how Pearl supports these product requirements.\n11\n(a)\n(b)\nFigure 8: Dynamic Action Space Benchmark Results: Return of DQN, SAC, PPO and REINFORCE\non CartPole and Acrobot environments where each environment deletes an action from the action\nspace every 4 steps. Note that DQN automatically adapts to dynamic action space whereas SAC,\nPPO and REINFORCE require a special actor neural network.\nTable 4: PearlAgent Satisfies Requirements of Real-World Applications\nPearl Features\nAuction RecSys\nAds Auction Bidding\nCreative Selection\nPolicy Learning\n\u2713\n\u2713\n\u2713\nOnline Exploration\n\u2713\n\u2713\nSafety\n\u2713\nHistory Summarization\n\u2713\nReplay Buffer\n\u2713\n\u2713\n\u2713\nContextual Bandit\n\u2713\nOffline RL\n\u2713\n\u2713\nDynamic Action Space\n\u2713\n\u2713\nLarge-Scale Neural Network\n\u2713\nAuction-Based Recommender System (Auction RecSys):\nOptimizing for long-term value in\nauction-based recommender systems using reinforcement learning presents a significant challenge. This\nis because it necessitates the integration of a RL agent with a mechanism rooted in supervised learning.\nIn the study by Xu et al. (2023), an on-policy RL solution for auction-based recommendations was\nintroduced, which incorporated Pearl during its recent production implementation. Given that the\nrecommender system is heavily influenced by the system\u2019s auction policy, the RL agent must undergo\non-policy learning offline. This ensures that the newly derived policy, when combined with the\nauction policy, proveably outperforms the current production system in terms of long-term rewards.\nAs it pertains to recommender systems, a unique set of recommendations is available to users at each\nstep. This necessitates the RL agent\u2019s capability to handle a dynamic action space. Additionally,\nin this implementation, large-scale neural networks were integrated with Pearl to offer accurate\npredictions of value functions for intricate user-recommendation interactions.\nAds Auction Bidding:\nReal-time bidding in advertising is recognized as a sequential decision-\nmaking problem. This is because a bidding agent must efficiently allocate an advertiser-defined budget\nover a specific duration to maximize the advertiser\u2019s conversions. In the study by Korenkevych et al.\n(2023), the focus is on enhancing the bidding agent\u2019s performance amidst the constantly evolving\nauction market. An RL bidding agent is tasked with prudently learning an offline policy that ensures\nneither over-expenditure nor under-utilization of the predetermined budget within the set timeframe.\nGiven that the data collected in production is driven by a deterministic policy, the agent needs to\nengage in limited exploration to gather more insightful data via online exploration. Moreover, due\nto the inherent volatility of auction markets, which are often only partially observable, the agent is\nexpected to make decisions based on summarized representations of its entire interaction history.\n12\nCreative Selection:\nBeyond sequential decision problems, contextual bandit problems are also\nprevalent in industry settings. In creative selection for content presentation, where each piece of\ncontent has dozens of different available creatives, we adopt a PearlAgent with (contextual) neural\nbandit learner and the Neural LinUCB exploration_module for efficient online exploration in\nlearning users\u2019 preferences in creatives with minimal number of interactions. Since each content has\na different set of available creatives for adoption, the agent is required to support dynamic action\nspace in this problem.\n6\nConclusion\nThe field of RL has witnessed remarkable successes in recent tears, yet, implementation of RL\nagents in real-world scenarios is still a daunting task. The introduction of Pearl marks a significant\nstride towards bridging this gap, offering a comprehensive, production-ready solution that addresses\nthe multifaceted challenges inherent in RL. By encompassing features like intelligent exploration,\nsafety, history summarization, dynamic action spaces, and support for both online and offline policy\noptimization, Pearl stands out as a versatile tool tailored for diverse real-world applications. We\nbelieve that Pearl will serve as a valuable resource for the broader adoption of RL in real-world\napplications, fostering innovation and expanding the boundaries of the field.\nReferences\nAlekh Agarwal, Olivier Chapelle, Miroslav Dud\u00edk, and John Langford. A reliable effective terascale\nlinear learning system. The Journal of Machine Learning Research, 15(1):1111\u20131133, 2014.\nShipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In\nInternational conference on machine learning, pp. 127\u2013135. PMLR, 2013.\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.\nAdvances in neural information processing systems, 30, 2017.\nArthur Asuncion and David Newman. Uci machine learning repository, 2007.\nAlberto Bietti, Alekh Agarwal, and John Langford. A contextual bandit bake-off. The Journal of\nMachine Learning Research, 22(1):5928\u20135976, 2021.\nAlbert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng Yang,\nGianni De Fabritiis, and Vincent Moens. Torchrl: A data-driven decision-making library for\npytorch, 2023.\nNicol\u00f2 Cesa-Bianchi, Claudio Gentile, G\u00e1bor Lugosi, and Gergely Neu. Boltzmann exploration done\nright. Advances in neural information processing systems, 30, 2017.\nWill Dabney, Mark Rowland, Marc Bellemare, and R\u00e9mi Munos. Distributional reinforcement\nlearning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 32, 2018.\nMiroslav Dud\u00edk, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. arXiv\npreprint arXiv:1103.4601, 2011.\nDylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with\nregression oracles. In International Conference on Machine Learning, pp. 3199\u20133210. PMLR, 2020.\nDylan Foster, Alekh Agarwal, Miroslav Dud\u00edk, Haipeng Luo, and Robert Schapire.\nPractical\ncontextual bandits with regression oracles. In International Conference on Machine Learning, pp.\n1539\u20131548. PMLR, 2018.\n13\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\nScott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic\nmethods. In International conference on machine learning, pp. 1587\u20131596. PMLR, 2018.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International conference\non machine learning, pp. 1861\u20131870. PMLR, 2018.\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735\u20131780, 1997.\nShengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal\nMehta, and Jo\u00e3o G.M. Ara\u00fajo. Cleanrl: High-quality single-file implementations of deep rein-\nforcement learning algorithms. Journal of Machine Learning Research, 23(274):1\u201318, 2022. URL\nhttp://jmlr.org/papers/v23/21-1342.html.\nDmytro Korenkevych, Frank Cheng, Artsiom Balakir, Alex Nikulkov, Lingnan Gao, Zhihao Cen,\nZuobing Xu, and Zheqing Zhu. Offline reinforcement learning for optimizing production bidding\npolicies. unpublished manuscript, 2023.\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning.\nIn International Conference on Learning Representations, 2021.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline\nreinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191, 2020.\nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.\nEnd-to-end training of deep\nvisuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u20131373, 2016.\nLihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to\npersonalized news article recommendation. In Proceedings of the 19th international conference on\nWorld wide web, pp. 661\u2013670, 2010.\nEric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph E.\nGonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for distributed reinforcement\nlearning. In International Conference on Machine Learning (ICML), 2018.\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\nLong-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.\nMachine learning, 8:293\u2013321, 1992.\nXiuyuan Lu and Benjamin Van Roy. Ensemble sampling. Advances in neural information processing\nsystems, 30, 2017.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.\nIan Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via\nbootstrapped dqn. Advances in neural information processing systems, 29, 2016.\nIan Osband, Benjamin Van Roy, Daniel J Russo, Zheng Wen, et al. Deep exploration via randomized\nvalue functions. J. Mach. Learn. Res., 20(124):1\u201362, 2019.\n14\nIan Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Morteza Ibrahimi,\nXiuyuan Lu, and Benjamin Van Roy. Approximate thompson sampling via epistemic neural\nnetworks. arXiv preprint arXiv:2302.09205, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nXue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of\nrobotic control with dynamics randomization. In 2018 IEEE international conference on robotics\nand automation (ICRA), pp. 3803\u20133810. IEEE, 2018.\nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah\nDormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine\nLearning Research, 22(268):1\u20138, 2021. URL http://jmlr.org/papers/v22/20-1364.html.\nGavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37.\nUniversity of Cambridge, Department of Engineering Cambridge, UK, 1994.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nTakuma Seno and Michita Imai. d3rlpy: An offline deep reinforcement learning library. Journal of\nMachine Learning Research, 23(315):1\u201320, 2022. URL http://jmlr.org/papers/v23/22-0017.\nhtml.\nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.\nDeterministic policy gradient algorithms. In International conference on machine learning, pp.\n387\u2013395. Pmlr, 2014.\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go\nwithout human knowledge. nature, 550(7676):354\u2013359, 2017.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods\nfor reinforcement learning with function approximation. Advances in neural information processing\nsystems, 12, 1999.\nChen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization. arXiv\npreprint arXiv:1805.11074, 2018.\nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning.\nIn Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling\nnetwork architectures for deep reinforcement learning. In International conference on machine\nlearning, pp. 1995\u20132003. PMLR, 2016.\nJiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang\nSu, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. Journal\nof Machine Learning Research, 23(267):1\u20136, 2022. URL http://jmlr.org/papers/v23/21-1127.\nhtml.\nPan Xu, Zheng Wen, Handong Zhao, and Quanquan Gu. Neural contextual bandits with deep\nrepresentation and shallow exploration. In International Conference on Learning Representations,\n2021.\n15\nRuiyang Xu, Jalaj Bhandari, Dmytro Korenkevych, Fan Liu, Yuchen He, Alex Nikulkov, and\nZheqing Zhu. Optimizing long-term value for auction-based recommender systems via on-policy\nreinforcement learning. RecSys, 2023.\nOmry Yadan. Hydra - a framework for elegantly configuring complex applications. Github, 2019.\nURL https://github.com/facebookresearch/hydra.\nZheqing Zhu and Benjamin Van Roy. Scalable neural contextual bandit for recommender systems. In\n32nd ACM International Conference on Information and Knowledge Management (CIKM), pp.\n3636\u20133646, 2023.\nA\nDetails on CB implementation\nThe CB benchmark environment is designed as follows. We assume access to an offline dataset\n{(xi, yi)}i, where for every i, xi \u2208 Rd is a feature vector, and yi \u2208 Y is a label from a finite alphabet.\nAt each time step an agent observes a feature vector xt and is required to choose an action, at \u2208 Y,\nwhich is an element of the alphabet of possible labels. The reward model is rt = 1{at = yt} + \u03be\nwhere \u03be \u223c N(0, \u03c3\u03be). This type of environments has an explicit exploration challenge: if an agent\ndoes not explore correctly it may never receive information on the correct label.\nWe used a two-layer neural architecture as the reward function approximation in all algorithms we\nexperiment with. The reward function network receives as an input the feature vector and an action\n(x, a), and returns real value number. The reward model was optimized via PyTorch, by iteratively\ntaking gradients on the standard MSE loss using an Adam optimizer. Beside of the neural versions of\nLinUCB, TS, and SquareCB we implemented an additional baseline offline approach. For the offline\nbaseline an agent gathers data with a fixed exploratory behavior policy. Then we trained a reward\nmodel on this offline dataset and tested its online performance by following the greedy policy with\nrespect to the learned reward model. The architecture of the network and other implementations\ndetails of the offline baseline are the same as for the CB algorithms.\nGeneral implementation details.\nTable 5 depicts implementation details corresponding to all\nbenchmarks and algorithms. For the letter, satimage, pendigits that are of higher dimension we\nused an MLP architecture of [64, 16] hidden layers. For the yeast dataset we chose an architecture\nof [32, 16] hidden layers.\nSince the action space of some of these datasets is not small, we chose a binary encoding to the\nactions. This binary encoding was concatenated to the feature vector. Hence, the input vector of the\nnetwork has the form (x, a) where a is a binary representation of an element in |Y|, where Y is the\nalphabet of possible labels.\nThe behavior policy with which we gathered data for the offline benchmark was chosen as follows:\nthe behavior policy chooses with probability 1/4 the correct label, and with probability 3/4 any\nlabel. That allowed to create a balanced dataset, in which the ratio between choosing the correct\nand incorrect label is small.\nThe plots presented in Table 1 represent the average performance across 5 runs. The confidence\nintervals represent the standard error.\nLinUCB and TS implementation.\nOur neural adaptions of LinUCB and TS are based on\ncalculating a bonus term \u2225\u03d5t(x, a)\u2225A\u22121/2\nt\nwhere \u03d5t(x, a) is the last layer feature representation of of\n(x, a) and At = I + Pt\u22121\nn=1 \u03d5t(x, a)\u03d5t(x, a)T . For LinUCB we explicitly add this term (Li et al., 2010)\nafter scaling by 0.25, which improved the performance. For the neural version of TS we sampled a\nreward from a Gaussian distribution with the variance term \u03d5t(x, a)\u2225A\u22121/2\nt\nand expected reward as\ncalculated by the neural reward model.\n16\nTable 5: Implementation Details of the CB algorithms\nArchitecture\n2 layer MLP\nOptimizer\nAdam\nLearning rate\n0.01\nBatch size\n128\nBuffer sizer\n# of time steps\nAction encoding\nBinary encoding\nReward variance\n\u03c3\u03be = 0.05\nSquareCB implementation.\nWe followed the exact same action decision rule of SquareCB (Foster\n& Rakhlin, 2020). We chose the scaling parameter \u03b3 = 10\n\u221a\ndT where d is the dimension of the\nfeature-action input vector. We note that setting \u03b3 \u221d\n\u221a\ndT was indicated in Foster & Rakhlin\n(2020) for the linear Bandit problem. We scaled this value by 10 since we empirically observed of an\nimproved performance in our ablation study.\n17\n"
  },
  {
    "title": "Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.04410.pdf",
    "upvote": "13",
    "text": "Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models\nJiayi Guo1,2*, Xingqian Xu1,3*, Yifan Pu2, Zanlin Ni2, Chaofei Wang2, Manushree Vasu1,\nShiji Song2, Gao Huang2\u2020, Humphrey Shi1,3\u2020\n1SHI Labs @ Georgia Tech & UIUC\n2Tsinghua University\n3Picsart AI Research (PAIR)\nhttps://github.com/SHI-Labs/Smooth-Diffusion\n\u201cA realistic dog\u201d\nImage A\nImage B\nInterpolation\nSmooth\nDiffusion\n(Ours)\nStable\nDiffusion\nReplace Item: \u201crabbit\u201d\u2192 \u201ccat\u201d\n\u201cA train going back to its course filled with people\u201d\n\u201cA mouse is next to a keyboard on a desk\u201d\nAdd Item: +\u201cbacon\u201d\nTransfer Style: \u201cwatercolor style\u201d\nDrag Point\nTask 1: Image Interpolation\nTask 2: Image Inversion and Reconstruction\nTask 3: Image Editing\nSmooth Diff. Stable Diff.\nSource\nSmooth Diff. Stable Diff.\nSource\nSmooth Diff. Stable Diff.\nSource\nFigure 1. Smooth Diffusion for downstream image synthesis tasks. Our method formally introduces latent space smoothness to\ndiffusion models like Stable Diffusion [59]. This smoothness dramatically aids various tasks in: 1) improving continuity of transitions in\nimage interpolation, 2) reducing approximation errors in image inversion, & 3) better preserving unedited contents in image editing.\nAbstract\nRecently,\ndiffusion models have made remarkable\nprogress in text-to-image (T2I) generation, synthesizing im-\nages with high fidelity and diverse contents. Despite this ad-\nvancement, latent space smoothness within diffusion mod-\nels remains largely unexplored. Smooth latent spaces en-\nsure that a perturbation on an input latent corresponds to\na steady change in the output image. This property proves\nbeneficial in downstream tasks, including image interpola-\ntion, inversion, and editing. In this work, we expose the\nnon-smoothness of diffusion latent spaces by observing no-\nticeable visual fluctuations resulting from minor latent vari-\nations. To tackle this issue, we propose Smooth Diffusion, a\n*Equal contribution.\n\u2020Corresponding authors.\nnew category of diffusion models that can be simultaneously\nhigh-performing and smooth.\nSpecifically, we introduce\nStep-wise Variation Regularization to enforce the propor-\ntion between the variations of an arbitrary input latent and\nthat of the output image is a constant at any diffusion train-\ning step. In addition, we devise an interpolation standard\ndeviation (ISTD) metric to effectively assess the latent space\nsmoothness of a diffusion model. Extensive quantitative and\nqualitative experiments demonstrate that Smooth Diffusion\nstands out as a more desirable solution not only in T2I gen-\neration but also across various downstream tasks. Smooth\nDiffusion is implemented as a plug-and-play Smooth-LoRA\nto work with various community models. Code is available\nat https://github.com/SHI-Labs/Smooth-Diffusion.\n1\narXiv:2312.04410v1  [cs.CV]  7 Dec 2023\n1. Introduction\nIn recent years, diffusion models [13, 23, 59] have rapidly\ngrown into very powerful tools for generative AI, particu-\nlarly for text-to-image generation. The remarkable ability\nof diffusion models, generating high-quality photorealistic\nimages from open-book contexts, has been highlighted in\nmany research and commercial products. Such success has\nalso inspired various diffusion-based downstream tasks, in-\ncluding image interpolation [27, 75], inversion [14, 42, 51,\n68, 74], editing [20, 40, 43, 52, 66, 72, 78, 79], etc.\nDespite the great success in the generation field, dif-\nfusion models occasionally produce low-quality results\nwith undesirable and unpredictable behaviors.\nSpecifi-\ncally speaking, for image interpolation, the Stable Diffusion\nWalk (SDW) [27] test examines latent space with spheri-\ncal linear interpolations, usually resulting in highly fluctu-\nated outputs with unpredictable visual appearance. Exam-\nples can be found in Fig. 1 Task 1, in which such interpo-\nlation exhibits undesired sharp changes as well as \u201ccartoon-\nization\u201d on photorealistic dog images, highlighted in the red\nbox. For the image inversion task shown in Fig. 1 Task 2,\na naive application of DDIM inversion [68] cannot recon-\nstruct images faithfully from the sources. Instead, it gener-\nates incorrect colors and object orientations, and misinter-\nprets the computer mouse as an animal mouse. For the im-\nage editing task shown in Fig. 1 Task 3, one may notice that\nonly minor text prompt editing can lead to major updates\non image contents and layouts, in which the object (i.e. the\ncat\u2019s pose, the horse\u2019s location, the shape of the pizza) can\nbe wildly and incorrectly altered. Moreover, current diffu-\nsion models are unsuited to drag-based editing [66] because\na fine-engineered drag method still has a noticeably large\nchance of breaking objects\u2019 shape and semantics.\nIn this work, we step into an important but under-\nexplored area: to improve the latent space smoothness of\ndiffusion models. Our motivation to enhance latent smooth-\nness comes from the real-world demand to improve the out-\nput qualities of the aforementioned downstream tasks. A\nsmooth latent space implies a robust visual variation under\na minor latent change. Therefore, enhancing such smooth-\nness could help improve the continuity of image interpola-\ntion, expand the capacity of image inversion, and maintain\ncorrect semantics in image editing. Notably, prior works in\nGANs [30, 31, 65] have demonstrated that the smooth la-\ntent space of the generator can significantly improve down-\nstream tasks\u2019 quality, offering additional evidence of the im-\nportance of this area.\nTo achieve our goal, we propose Smooth Diffusion, a\nnew category of diffusion models that can be simultane-\nously high-performing and smooth.\nWe start our explo-\nration by first formalizing the objective for Smooth Dif-\nfusion, in which fixed-size perturbations \u2206\u03f5 on a latent\nnoise \u03f5 should produce smooth visual changes \u2206c\nx0 on the\nsynthetic image c\nx0, rounded to a constant ratio C.\nAl-\nthough one may think that according to the formulation,\nthe smoothness constraint could be an accessible train-\ntime loss. Actually, there is no direct application of such\nregularization from inference to training, and the chal-\nlenge lies in the fact that in each training iteration (i.e.,\nback-propagation), diffusion models optimize only a \u201ct-step\nsnapshot\u201d instead of the entire T-step diffusion process.\nTherefore, we introduce Step-wise Variation Regular-\nization, a novel regularization that seamlessly incorporates\nour Smooth Diffusion\u2019s inference-time objective to training.\nThis regularization aims to bound the 2-norm of output vari-\nation \u2206c\nx0 given a fixed-size change \u2206xt in input xt at an\narbitrary step t. The rationale of the reformulation is intu-\nitive: If xt and c\nx0 exhibit smooth changes at any t, then\nthe relation between the latent noise \u03f5 (i.e. xT ) and c\nx0 is\njust the accumulation of smooth variations and thus can be\nsmooth as well. More details can be found in Sec. 3.\nIn practice, our Smooth Diffusion is trained on top of\na well-known text-to-image model: Stable Diffusion [59].\nWe examine and demonstrate that Smooth Diffusion dra-\nmatically improves the latent space smoothness over its\nbaseline. Meanwhile, we conduct extensive research across\nnumerous downstream tasks, including but not limited to\nimage interpolation, inversion, editing, etc. Both qualitative\nand quantitative results support our conclusion that Smooth\nDiffusion can be the next-gen high-performing generative\nmodel not only for the baseline text-to-image task but across\nvarious downstream tasks.\n2. Related Work\nDiffusion models are initiated from a family of prior works\nincluding but not limited to [10, 63, 67, 73]. Since then,\nDDPM [23] introduced an image-based noise prediction\nmodel, becoming one of the most popular image generation\nresearch. Later works [13, 45, 68] extended DDPM, demon-\nstrating that diffusion models perform on-par and even sur-\npass GAN-based methods [16, 28\u201331]. Recently, generat-\ning images from text prompts (T2I) become an emerging\nfield, among which diffusion models [17, 46, 56, 59, 61]\nhave become quite visible to the public. For example, Sta-\nble Diffusion (SD) [59] consists of VAE [34] and CLIP [55],\ndiffuses latent space, and yields an outstanding balance be-\ntween quality and speed. Following SD [59], researchers\nalso explored diffusion approaches for controls such as\nControlNet [15, 25, 44, 54, 77, 82, 83, 86\u201388, 92] and mul-\ntimodal such as Versatile Diffusion [8, 39, 70, 85]. Works\nfrom a different track reduce diffusion steps to improve\nspeed [7, 32, 37, 41, 62, 69, 89, 93], or restrict data and\ndomain for few-shot learning [19, 24, 38, 60], all had suc-\ncessfully maintained a high output quality.\nSmooth latent space was one of the prominent prop-\n2\nerties of SOTA GAN works [11, 29\u201331], while explor-\ning such property went through the decade-long GAN re-\nsearch [5, 16], whose goals were mainly robust training.\nIdeas such as Wasserstein GAN [6, 18] had proved to be ef-\nfective, which enforced the Lipschitz continuity on discrim-\ninator via gradient penalties. Another technique, namely\npath length regularization, related to the Jacobian clamping\nin [48], was adapted in StyleGAN2 [30] and later became a\nstandard setting for GAN-based generators [12, 35, 84, 91].\nBenefiting from the smoothness property, researchers man-\naged to manipulate latent space in many downstream re-\nsearch projects. Works such as [9, 47, 65, 80] explored la-\ntent space disentanglement. GAN-inverse [3, 4, 49, 81] had\nalso proved to be feasible, along with a family of image edit-\ning approaches [50, 53, 57, 58, 71, 94]. As aforementioned,\nour work aims to investigate the latent space smoothness for\ndiffusion models, which by far remains unexplored.\n3. Methodology\nIn this section, we first introduce preliminaries of our\nmethod, including diffusion process [23], diffusion inver-\nsion [13, 42, 68] and low-rank adaptation [24] (Sec. 3.1).\nThen Smooth Diffusion is proposed with its definition, ob-\njective (Sec. 3.2) and regularization function (Sec. 3.3).\n3.1. Preliminaries\nDiffusion process [23] is a kind of Markov chain that grad-\nually adds random noise \u03f5t \u223c N(0, I) to ground truth sig-\nnal x0 \u223c p(x0), making xT in a total of T steps. At each\nstep, The noisy data xt is computed as:\nxt =\np\n1 \u2212 \u03b2txt\u22121 +\np\n\u03b2t\u03f5t,\nt = 1, 2, \u00b7 \u00b7 \u00b7 , T,\n(1)\nwhere \u03b2t is the preset diffusion rate at step t. By making\n\u03b1t = 1 \u2212 \u03b2t, \u03b1t = QT\nt=1 \u03b1t and \u03f5 \u223c N(0, I), we have the\nfollowing equivalents:\nxt = \u221a\u03b1txt\u22121 +\n\u221a\n1 \u2212 \u03b1t\u03f5t\n= \u221a\u03b1tx0 +\n\u221a\n1 \u2212 \u03b1t\u03f5,\nt = 1, 2, \u00b7 \u00b7 \u00b7 , T.\n(2)\nA diffusion model \u03f5\u03b8(xt, t) is then trained to estimate \u03f5t\nfrom xt, by which one can predict the original signal x0 by\ngradually remove noise from the degraded xT [68]. This is\ncommonly known as the backward diffusion process:\n[\nxt\u22121 =\nr\u03b1t\u22121\n\u03b1t\nc\nxt +\n s\n1\n\u03b1t\u22121\n\u2212 1 \u2212\nr\n1\n\u03b1t\n\u2212 1\n!\n\u00b7 \u03f5\u03b8(c\nxt, t).\n(3)\nDiffusion inversion [13, 42, 68] targets to recover the exact\nbackward diffusion process (i.e. c\nxt, \u03f5\u03b8(c\nxt, t), t = 1, ..., T)\nfrom a known final prediction c\nx0. One of the common tech-\nnique for such inversion is DDIM inversion [13, 68], which\nreverses Eq. (3) under a local linear approximation:\n]\nxt+1 =\nr\u03b1t+1\n\u03b1t\nf\nxt +\n s\n1\n\u03b1t+1\n\u2212 1 \u2212\nr\n1\n\u03b1t\n\u2212 1\n!\n\u00b7 \u03f5\u03b8(f\nxt, t),\n(4)\nwhere f\nxt represent the estimated c\nxt at time t. However,\nDDIM inversion is only a rough estimation.\nFor text-\nto-image diffusion, a more advanced technique, Null-Text\nInversion [42], optimizes additional null-text embeddings\n{\u2205t}T\nt=1 for each step t, simulating the backward process\nwith \u03f5\u03b8(xt, t, \u03be, \u2205t), where \u03be is the input text embedding.\nThe predicted null-text \u2205t is the null input of the classifier-\nfree guidance [22] with a guidance scale w:\n\u03f5\u03b8(xt, t, \u03be, \u2205t) = w \u00b7 \u03f5\u03b8(xt, t, \u03be) + (1 \u2212 w) \u00b7 \u03f5\u03b8(xt, t, \u2205t).\n(5)\nLow-rank adaptation (LoRA) [24] is initially proposed\nto efficiently adapt large pretrained models to downstream\ntasks.\nThe key assumption of LoRA is that the weight\nchanges required during adaptation maintain a low rank.\nGiven a pretrained model weight W0 \u2208 Rd\u00d7k, its updated\nweight \u2206W is expressed as a low rank decomposition:\nW0 + \u2206W = W0 + BA,\n(6)\nwhere B \u2208 Rd\u00d7r, A \u2208 Rr\u00d7k and r \u226a min(d, k). During\nadaptation, W0 is frozen, while B and A are trainable.\n3.2. Smooth Diffusion\nAs previously mentioned, modern diffusion models (DM)\ndo not guarantee latent space smoothness, creating not only\nresearch gaps between GANs and diffusions but also unex-\npected challenges in downstream tasks. To address these\nissues, we propose Smooth Diffusion, a novel class of\nhigh-performing diffusion models with enhanced smooth-\nness over its latent space. The underlining of Smooth Dif-\nfusion is the newly proposed training scheme in which we\ncarried out a Step-wise Variation Regularization to en-\nhance model smoothness.\nTo better explain our aims, we adopt the same termi-\nnologies from the standard inference-time diffusion process\n(Fig. 2a), involving a T steps procedure that transforms the\nrandom noise \u03f5 (i.e., xT ) to the prediction c\nx0. The over-\nall objective of Smooth Diffusion can then be written in\nEq. 7: in which we expect that a fixed-size change \u2206\u03f5 on \u03f5\n(i.e., \u2206xT on xT ) will finally lead to a non-zero, fixed-size\nchange \u2206c\nx0 on c\nx0, up to a constant ratio C:\n\u2225\u2206c\nx0\u22252 \u21d4 C\u2225\u2206xT \u22252 = C\u2225\u2206\u03f5\u22252, \u2200\u03f5,\n(7)\nNotice that by definition, xT is the initial input of the\nbackward diffusion loop in Eq. 3.\nSince xT is close to\n\u03f5 \u223c N(0, 1), for simplicity, we make them equivalent in\nall the following equations.\nNevertheless, one may notice that our inference-time ob-\njective in Eq. 7 cannot be directly transformed into a train-\n3\nDM\nDM\n\ud835\udc47 steps\n\ud835\udf50\n(or \ud835\udc99!)\n$\n\ud835\udc99\"\n(a) Inference-time Diffusion: \nDenoisng prediction through \ud835\udc7b steps \n(b) Training-time Diffusion:\nDenoisng prediction at a single step \ud835\udc95\n\ud835\udc99#\n$\n\ud835\udc99\"\n1 \u2212 \ud835\udefc#\ud835\udf50\n\ud835\udefc#\ud835\udc99\"\n+\nDM\n\ud835\udc47 steps\n(or \ud835\udc99! + \u0394\ud835\udc99!)\n$\n\ud835\udc99\" + \u0394$\n\ud835\udc99\"\n(c) Inference-time Smooth Diffusion:\nVariation constraint through \ud835\udc7b steps \nDM\n(d) Training-time Smooth Diffusion:\nVariation constraint at a single step \ud835\udc95\n\ud835\udc99# + \u0394\ud835\udc99#\n$\n\ud835\udc99\" + \u0394$\n\ud835\udc99\"\n1 \u2212 \ud835\udefc#(\ud835\udf50 + \u0394\ud835\udf50)\n\ud835\udefc#\ud835\udc99\"\n+\n\ud835\udf50 + \u0394\ud835\udf50\n\ud835\udc36 \u0394\ud835\udf50 \ud835\udfd0 = \ud835\udc36 \u0394\ud835\udc99!\n% \u21d4\n\u0394$\n\ud835\udc99\" %, \u2200\ud835\udf50\n\ud835\udc36 1 \u2212 \ud835\udefc# \u0394\ud835\udf50 \ud835\udfd0 = \ud835\udc36 \u0394\ud835\udc99#\n% \u21d4\n\u0394$\n\ud835\udc99\" %, \u2200\ud835\udf50\nFigure 2. Illustration of Smooth Diffusion. Smooth Diffusion (c) enforces the ratio between the variation of the input latent (\u2225\u2206\u03f5\u22252 or\n\u2225\u2206xT \u22252) and the variation of the output prediction (\u2225\u2206c\nx0\u22252) is a constant C. Training-time Diffusion (b) optimizes a \u201ct-step snapshot\u201d\nof the denoising prediction process in Inference-time Diffusion (a). Similarly, we propose Training-time Smooth Diffusion (d) to optimize\na \u201ct-step snapshot\u201d of the variation constraint in Inference-time Smooth Diffusion (c). DM: Diffusion model.\ning loss function. This is because, in one training iteration\n(i.e., back-propagation), diffusion models optimize only a\n\u201ct-step snapshot\u201d of the diffusion process (Fig. 2b), where\nt is uniformly sampled from 1 to T. Hence, the proposed\n\u201cglobal\u201d objective (Eq. 7) for the entire T-step process is\nnot accessible in training. Therefore, we need to reformu-\nlate our global objective into a step-wise objective shown\nin Eq. 8, which can later be integrated into the diffusion\ntraining process as a loss function:\n\u2225\u2206c\nx0\u22252 \u21d4 C\u2225\u2206xt\u22252 = C\n\u221a\n1 \u2212 \u03b1t\u2225\u2206\u03f5\u22252, \u2200\u03f5,\n(8)\nwhere C is a non-zero constant. This step-wise objective in-\ndicates that at each training step, variations \u2206\u03f5 on \u03f5 should\nimply variations \u2206xt on xt with a ratio proportional to\n\u221a1 \u2212 \u03b1t. The rationale of Eq. 8 is intuitive: If xt and c\nx0\nshow smooth changes at any t, then the relation between\nthe latent noise \u03f5 (i.e. xT ) and c\nx0 is just the accumulation\nof smooth variations and thus can be smooth as well.\n3.3. Step-wise Variation Regularization\nWhile the motivation and formulation of the Smooth Diffu-\nsion objective are presented, how to realize such an objec-\ntive remains unexplained. Therefore, in this section, we in-\ntroduce Step-wise Variation Regularization to effectively\nintegrate the step-wise objective into diffusion training.\nWe draw inspiration from the regularization tech-\nniques [30, 48] adopted in GAN training. The core idea of\nStep-wise Variation Regularization is to bound the Jacobian\nmatrix J\u03f5 = \u2202c\nx0/\u2202\u03f5 of the diffusion system by minimizing\nthe following regularization loss at any x0, \u03f5, and step t:\nLreg = E\u2206c\nx0,\u03f5\n\u0000\u221a\n1 \u2212 \u03b1t\u2225JT\n\u03f5 \u2206c\nx0\u22252 \u2212 a\n\u00012 ,\n(9)\nwhere \u2206c\nx0 is the normally sampled pixel intensities nor-\nmalized to unit length, \u03f5 is a normally sampled noise\nin Eq. 2, and a is the exponential moving average of\n\u221a1 \u2212 \u03b1t\u2225JT\n\u03f5 \u2206c\nx0\u22252 computed online during training. In\npractice, we compute Eq. 9 via standard backpropagation\nwith the following identity:\n\u221a\n1 \u2212 \u03b1t\u2225JT\n\u03f5 \u2206c\nx0\u22252 = \u2225\u2207\u03f5(\n\u221a\n1 \u2212 \u03b1tc\nx0 \u00b7 \u2206c\nx0)\u22252. (10)\nThe identity holds since \u2206c\nx0 is independently sampled, and\nuncorrelated with \u03f5.\nNext, we prove that the proposed objective in Eq. 9 ex-\nactly matches our optimization goal in Eq. 8. One prelimi-\nnary result, proven in [30], is that in high dimensions, Eq. 9\nis minimized when J\u03f5 is orthogonal at any \u03f5 up to a global\nscaling factor K (i.e. J\u03f5 \u00b7 JT\n\u03f5 = K \u00b7 I). By applying the\northogonality of J\u03f5, we have the following:\nJT\n\u03f5 \u2206c\nx0 = KJ\u22121\n\u03f5 \u2206c\nx0 = K \u2202\u03f5\n\u2202c\nx0\n\u00b7 \u2206c\nx0 = K\u2206\u03f5.\n(11)\nWhen Lreg in Eq. 9 reaches its optimal, we then have:\na =\n\u221a\n1 \u2212 \u03b1t\u2225JT\n\u03f5 \u2206c\nx0\u22252 =\n\u221a\n1 \u2212 \u03b1tK\u2225\u2206\u03f5\u22252.\n(12)\nNotice that a = a\u2225\u2206c\nx0\u22252, since \u2225\u2206c\nx0\u22252 = 1 is the afore-\nmentioned random unit length vector. Hence, we can finally\nreformulate the expression:\n\u2225\u2206c\nx0\u22252 = K\na\n\u221a\n1 \u2212 \u03b1t\u2225\u2206\u03f5\u22252\n= C\n\u221a\n1 \u2212 \u03b1t\u2225\u2206\u03f5\u22252,\n(13)\n4\nwhich exactly matches our proposed objective in Eq. 8.\nTo summarize, during training, the Smooth Diffusion ob-\njective encompasses a combination of Lbase and Lreg:\nL = Lbase + \u03bbLreg,\n(14)\nwhere Lbase denotes the basic training objective of a diffu-\nsion model and \u03bb represents a ratio parameter controlling\nthe intensity of Step-wise Variation Regularization.\n4. Experiments\n4.1. Experimental Setup\nBaselines and settings. We select the Stable Diffusion [59]\nas the primary baseline for all tasks. Additionally, for im-\nage interpolation, we adopt a VAE-space interpolation and\nANID [75] as competitors. For image inversion, we inte-\ngrate Smooth Diffusion and Stable Diffusion with DDIM\ninversion [68] and Null-text inversion [42]. For text-based\nimage editing, SDEdit [40], Prompt-to-Prompt (P2P) [20],\nPlug-and-Play (PnP) [72], Diffusion Disentanglement (Dis-\nentangle) [79], Pix2Pix-Zero [52] and Cycle Diffusion [78]\nare chosen as SOTA approaches.\nFor drag-based image\nediting, we compare Smooth Diffusion with Stable Diffu-\nsion within the framework of DragDiffusion [66].\nImplementation details. Smooth Diffusion is trained atop\npretrained Stable Diffusion-V1.5 [59], using LoRA [24]\nfinetuning technique.\nThe UNet of Smooth Diffusion is\nset as trainable with a LoRA rank of 8, while the VAE and\ntext encoder are frozen. We leverage the LAION Aesthetics\n6.5+ as the training dataset, which contains 625K image-\ntext pairs with predicted aesthetics scores of 6.5 or higher\nfrom LAION-5B [64]. Smooth diffusion is typically trained\nfor 30K iterations with a batch size of 96, 3 samples per\nGPU, a total of 4 A100 GPUs, and a gradient accumulation\nof 8. The AdamW [33] optimizer is adopted with a constant\nlearning rate of 1 \u00d7 10\u22124 and a weight decay of 1 \u00d7 10\u22124.\nThe ratio parameter \u03bb in Eq. 14 is set to 1. During infer-\nence, the total number of diffusion steps is set to 50 and the\nclassifier-free guidance [22] scale is set to 7.5.\nEvaluation metrics. To evaluate the general text-to-image\ngeneration performance, we report the popular FID [21] and\nCLIP Score [55] on the MS-COCO validation set [36]. To\nassess the latent space smoothness, we propose an interpo-\nlation standard deviation (ISTD) as an evaluation metric. In\nspecific, we randomly draw 500 text prompts from the MS-\nCOCO validation set. For each prompt, we sample a pair of\nGaussian noises and uniformly interpolate them from one to\nthe other 9 times with mix ratios from 0.1 to 0.9. Fed into\ndiffusion models together with a prompt, we could obtain\na total of 11 generated images, 2 from the source Gaussian\nnoises and 9 from the interpolated noises. We calculate the\nstandard deviation of L2 distances between every two ad-\njacent images in the pixel space. Finally, we average the\nstandard deviations over 500 prompts as ISTD. Ideally, a\nzero value of ISTD indicates that consistent and uniform\nvisual fluctuations in the pixel space for identical fixed-\nsize changes in the latent space, resulting in a smooth la-\ntent space. For image inversion, mean square error (MSE),\nLPIPS [90], SSIM [76] and PSNR [26] are adopted to eval-\nuate the image reconstruction capability.\n4.2. Latent Space Interpolation\nQualitative comparison. The most straightforward way\nto demonstrate the smoothness of the latent space is\nthrough the observation of interpolation results between la-\ntent noises.\nIn Fig. 3, we present interpolation compar-\nisons between Smooth Diffusion and Stable Diffusion using\nreal images. To generate these comparisons, we utilize the\nNTI [42] to invert a pair of real images into latent noises xT ,\nsharing the same {\u2205t}T\nt=1. We then perform uniform spher-\nical linear interpolations between latent noises (also known\nas Stable Diffusion Walk [27]), resulting in 9 intermediate\nnoises with mix ratios from 0.1 to 0.9. Subsequently, we\nconcatenate the 11 images produced from these noises to\ncreate an image transition sequence in the figures.\nNotably, as highlighted by the red boxes, Stable Diffu-\nsion exhibits significant visual fluctuations during the tran-\nsition.\nIn particular, the interpolated images may intro-\nduce new attributes that are unrelated to the source im-\nages, e.g., the undesired grasslands in the second row\nof Fig. 3. In contrast, our approach, Smooth Diffusion, not\nonly avoids introducing obvious irrelevant attributes in the\ninterpolated images but also ensures that the visual effects\nchange smoothly throughout the transition. Additional in-\nterpolation results can be seen in supplementary materials.\nIn addition to Stable Diffusion, Fig. 3 also includes two\nother baseline methods for comparison: 1) VAE Interpola-\ntion (VAE Inter.), which performs interpolations within the\nVAE space of Stable Diffusion. However, the results closely\nresemble pixel-space interpolations, with significant degra-\ndation of visual details, particularly in the highlighted red\nbox area. 2) ANID [75], which first adds noise to real im-\nages and subsequently denoises the interpolated noisy im-\nages using Stable Diffusion. In Fig. 3, ANID with a 50-\nstep scheduler exhibits highly blurred interpolation results.\nWhen ANID operates with a default 200-step scheduler, the\nblurring can be alleviated, but the quality of the interpolated\nimages remains far from satisfactory.\nQuantitative comparison.\nThe goal of Smooth Diffu-\nsion is to enhance the latent space smoothness without im-\nage generation performance degradation compared to Sta-\nble Diffusion. In pursuit of this goal, we employ the ISTD\nintroduced in Sec. 4.1 to evaluate the latent space smooth-\nness. Additionally, we utilize FID [21] and CLIP Score [55]\nto assess generators\u2019 overall performance. The results pre-\n5\nSmooth\nDiffusion\n(50 steps)\nStable\nDiffusion\n(50 steps)\nVAE\nInter.\nANID\n(50 steps)\nANID\n(200 steps)\nImage A\nImage B\nInterpolation\n\u201cA church\u201d\nFigure 3. Image interpolation comparison results. For Smooth Diffusion and Stable Diffusion [59], real images (Image A and B) are\ninverted into latents using NTI [42]. We perform spherical linear interpolations between latents (also known as Stable Diffusion Walk [27])\nand concatenate the resulting images as a transition sequence. VAE Inter. performs interpolations within the VAE space of Stable Diffusion.\nANID [75] first adds noise to real images and subsequently denoises the interpolated noisy images using Stable Diffusion.\nMethod\nISTD (\u2193)\nFID (\u2193)\nCLIP Score (\u2191)\nStable Diffusion\n38.63\n12.70\n31.46\nSmooth Diffusion\n16.54\n12.10\n31.54\nTable 1. Quantitative evaluations of image interpolation and\ntext-to-image generation.\nWe evaluate Smooth Diffusion and\nStable Diffusion [59] with ISTD, FID [21] and CLIP Score [55].\nThe better results are in bold.\nsented in Tab. 1 demonstrate that Smooth Diffusion signifi-\ncantly outperforms Stable Diffusion in terms of ISTD, indi-\ncating a substantial improvement in the latent space smooth-\nness. Furthermore, Smooth Diffusion exhibits superior per-\nformance in both FID and CLIP Score, suggesting that the\nenhancement of latent space smoothness and the overall im-\nage generation quality are not mutually exclusive but com-\nplement each other when the regularization term is applied\nwith a suitable strength ratio.\n4.3. Image Inversion and Reconstruction\nPrevious research [30] in the realm of GANs discovered that\na smoother latent space has a positive impact on the accu-\nracy of image inversion and reconstruction. We empirically\nvalidate this finding within the context of diffusion mod-\nels.\nIn specific, two representative inversion techniques,\nDDIM inversion [68] and Null-text inversion (NTI) [42] are\nadopted and integrated with Smooth Diffusion and Stable\nDiffusion separately. We both qualitatively and quantita-\nSmooth Diff.\n+NTI (Ours)\nStable Diff.\n+NTI\nSmooth Diff.\n+DDIM (Ours)\nStable Diff.\n+DDIM\nSource\n\u201cA man using his laptop computer while a cat sits on his lap\u201d\n\u201cA large tower that has a big clock at top\u201d\n\u201cA room with a couch, table set with dinnerware and a television\u201d\nFigure 4. Image reconstruction comparison results. We inte-\ngrate Smooth Diffusion and Stable Diffusion [59] with NTI [42]\n(column 2 & 3) and DDIM inversion [68] (column 4 & 5).\ntively compare the image inversion and reconstruction per-\nformance of these integrated models using 500 randomly\nsampled images from the MS-COCO validation set [36].\nAs illustrated in the two rightmost columns of Fig. 4,\nwhen\nemploying\na\nstraightforward\nDDIM\ninversion,\nSmooth Diffusion outperforms Stable Diffusion by a con-\nsiderable margin in terms of reconstruction quality. This\nimprovement is evident in various aspects, such as an accu-\nrate generation of character identities, a faithful recreation\n6\nSmooth Diff.\nStable Diff.\nSDEdit\nP2P\nPnP\nDisentangle\nPix2Pix-Zero\nCycle Diff.\n\u201cA chocolate cake with cream on it\u201d \u2192\u201cA chocolate cake with strawberries on it\u201d\nSource\n\u201cA banana on the table\u201d \u2192\u201cA banana and an apple on the table\u201d\n\u201cA young girl face with long black hair\u201d \u2192\u201cA young girl face with long black hair, cartoon style\u201d\nLocal Edit\n(Replace Item)\nLocal Edit\n(Add Item)\nGlobal Edit\n(Transfer Style)\nFigure 5. Text-based image editing comparison results. We compare Smooth Diffusion and Stable Diffusion [59] (column 2 & 3),\nconsidering both local and global edits through the straightforward pipeline described in Sec. 4.4. Additionally, we present results from\nSOTA approaches, including SDEdit [40], P2P [20], PnP [72], Disentangle [79], Pix2Pix-Zero [52], and Cycle Diffusion [78], as references.\nMethod\nMSE (\u2193)\nLPIPS (\u2193)\nSSIM (\u2191)\nPSNR (\u2191)\nStable Diff. + DDIM\n0.1756\n0.5385\n0.2662\n13.97\nSmooth Diff. + DDIM\n0.1086\n0.4326\n0.3418\n16.17\nStable Diff. + NTI\n0.0156\n0.1656\n0.6068\n25.63\nSmooth Diff. + NTI\n0.0153\n0.1635\n0.6102\n25.74\nVAE Reconstruction\n0.0148\n0.1590\n0.6136\n25.98\nTable 2. Quantitative evaluations of image reconstruction. We\nintegrate Stable Diffusion and Smooth Diffusion [59] with DDIM\ninversion [68] (row 2 & 3) and NTI [42] (row 4 & 5). MSE,\nLPIPS [90], SSIM [76] and PSNR [26] are evaluated. VAE Re-\nconstruction results are provided as the optimal values.\nof the city view behind the tower, and a correct reproduc-\ntion of room layouts. This phenomenon underscores the\nfact that the latent space of Smooth Diffusion is more tol-\nerant of the errors introduced by the local linear approxi-\nmation in DDIM inversion. Consequently, the reconstruc-\ntion results produced by Smooth Diffusion manage to retain\nthe contents of the source images to a greater extent. On\nthe other hand, when the optimization-based NTI technique\nis employed, the disparity between Smooth Diffusion and\nStable Diffusion is not as pronounced. Nonetheless, there\nare still instances where Stable Diffusion exhibits subpar\nresults, such as the ruined man\u2019s face in Fig. 4.\nTo quantify the image reconstruction performance, MSE,\nLPIPS [90], SSIM [76] and PSNR [26] are reported\nin Tab. 2. Notably, the reconstruction error encompasses\ntwo components: 1) the error from different inversion meth-\nods and U-Net parameters and 2) the error from the shared\npretrained VAE [34]. Hence, we included the VAE recon-\nstruction errors as optimal values for our method. The re-\nsults exhibit a consistent outperformance of Smooth Diffu-\nsion over Stable Diffusion across all metrics, whether using\nDDIM inversion or NTI. Moreover, \u201cSmooth Diffusion +\nNTI\u201d performs results close to VAE reconstruction, indicat-\ning its superiority attributed to a smoother latent space.\n4.4. Image Editing\nThe superiority of Smooth Diffusion in image inversion and\nreconstruction has motivated us to explore its potential for\nenhancing image editing tasks. In this section, we delve\ninto two typical image editing scenarios: text-based image\nediting and drag-based image editing.\nText-based image editing.\nThere have been numerous\nmethods [20, 40, 52, 72, 78, 79] proposed in the litera-\nture, each with its own unique designs aimed at achieving\nthe SOTA performance. In contrast, we adopt a simpler\npipeline akin to the image inversion and reconstruction pro-\ncess discussed in Sec. 4.3. The key distinction lies in our\napproach to modify the text prompt during the later time\nsteps of the reconstruction process. In specific, the original\n\u03f5\u03b8(xt, t, C, \u2205t) in Eq. (5) during NTI reconstruction (diffu-\nsion sampling) process is replaced with:\n\u03f5\u03b8(xt, t, C, \u2205t) =\n(\n\u03f5\u03b8(xt, t, Csrc, \u2205t), t > T \u00d7 r,\n\u03f5\u03b8(xt, t, Ctrg, \u2205t), t \u2264 T \u00d7 r,\n(15)\nwhere Csrc represents the source text prompt for inversion,\nwhile Ctrg corresponds to the target text prompt for editing.\nThe parameter r serves as a threshold, determining when to\nswitch from Csrc to Ctrg. In practice, r is typically chosen\nwithin {0.6, 0.7, 0.8, 0.9}, with the exact value depending\non the specific input images and target visual effects.\nThrough this straightforward pipeline, we conducted a\ncomparative analysis of the editing performance between\nSmooth Diffusion and Stable Diffusion, as presented in the\nthree left-most columns of Fig. 5. We also included editing\nresults obtained from SOTA methods as references. Our\n7\nSmooth Diff.\nSource\nStable Diff.\nUser Edit\n\u201cA photo of a cat\u201d\n\u201cA photo of a landscape\u201d\n\u201cA photo of  flowers\u201d\nFigure 6. Drag-based image editing comparison results. We\nimplement Smooth Diffusion and Stable Diffusion [59] within the\nframework of DragDiffusion [66], respectively.\nevaluation encompasses both local and global editing tasks.\nThe local editing tasks involve replacing items (e.g., chang-\ning \u201ccream\u201d to \u201cstrawberries\u201d) and adding items (e.g., \u201cap-\nple\u201d). On the other hand, the global editing tasks pertain to\nglobal style transfer, such as transforming an image into a\n\u201ccartoon style\u201d. It is evident that while Stable Diffusion ex-\ncels in achieving precise image reconstruction with NTI, as\ndiscussed in Sec. 4.3, even minor modifications to the text\nprompt can significantly impact the content of the generated\nimages. For instance, it can affect elements like the style of\nthe cake, the shape of the banana, and the haircut of the girl.\nIn contrast, Smooth Diffusion not only accurately generates\nedited images in accordance with the target text prompts\nbut also effectively preserves the unedited contents. Fur-\nthermore, when compared to SOTA methods, even with this\nstraightforward pipeline, Smooth Diffusion consistently de-\nlivers competitive results across all cases.\nDrag-based image editing. As an emerging research av-\nenue in the community, drag-based image editing [43, 50,\n66] has garnered considerable attention recently. DragDif-\nfusion [66] first introduces a framework for drag-based\nimage editing employing Stable Diffusion. In the task 3\nof Fig. 1 and Fig. 6, we showcase that by integrating Smooth\nDiffusion into the DragDiffusion framework, some previ-\nously unsuccessful editing operations with Stable Diffusion\ncan be enabled. As illustrated, Smooth Diffusion achieves\noperations such as making the tree grow taller without dam-\naging existing branches (Fig. 1), rotating the cat head, creat-\ning a new mountain top without destroying the original one,\nand letting new flowers grow in the vase (Fig. 6). These op-\nerations, however, fail with Stable Diffusion, indicating the\nnon-smoothness of its latent space.\n4.5. Ablation Studies\nRegularization ratio. In Tab. 3, we examine the impact\nof different strength ratios \u03bb in Eq. (14). This ratio ad-\njusts the intensity of the step-wise variation regularization.\nSpecifically, when a weaker regularization is applied (e.g.,\n\u03bb = 0.1), we observe a slight improvement in the CLIP\nScore. However, there is a significant increase in ISTD,\nindicating a notable degradation in latent space smooth-\nness. In contrast, employing a stronger regularization (e.g.,\n\u03bb = 10) leads to a smoother latent space, as demonstrated\nby the decrease in ISTD. However, in this case, we observe\nan unexpected increase in FID, indicating a notable decline\nin the quality of generated images. Therefore, selecting an\nappropriate trade-off value for \u03bb becomes crucial based on\nthe specific experimental settings. In our default setting, we\nfind that \u03bb = 1 serves as a suitable value.\nRatio\nISTD (\u2193)\nFID (\u2193)\nCLIP Score (\u2191)\n0.1\n24.23\n12.15\n31.56\n1 (default)\n16.54\n12.11\n31.49\n10\n11.51\n17.44\n31.41\nTable 3. Ablation results of different regularization ratios. The\nbest results are in bold, and the second-best results are underlined.\nLoRA rank. In Tab. 4, we examine the impact of differ-\nent ranks of the LoRA component utilized in our Smooth\ndiffusion. We discover that LoRA ranks within the range\nof [4,16] are all suitable values for our default setting. We\nselect a default rank of 8 because of its lowest ISTD among\nthe first three rows in Tab. 4. Furthermore, we train a fully\nfinetuned model, referred to as \u201dfull,\u201d which showcases a\nfurther decrease in ISTD. However, this comes at the ex-\npense of significantly degrading the quality of the generated\nimages, as indicated by an increased FID and decreased\nCLIP Score. This decline in performance underscores the\nvulnerability of fully fine-tuned models to collapse within\nour default setting, emphasizing the need for additional\nmeticulous design considerations.\nRank\nISTD (\u2193)\nFID (\u2193)\nCLIP Score (\u2191)\n4\n16.76\n12.36\n31.49\n8 (default)\n16.54\n12.11\n31.54\n16\n16.65\n11.49\n31.61\nfull\n11.52\n27.27\n28.86\nTable 4. Ablation results of different LoRA ranks. The best\nresults are in bold, and the second-best results are underlined.\n5. Conclusion\nIn this article, we explored Smooth Diffusion, an innova-\ntive diffusion model that enhances latent space smoothness\n8\nfor generation. Smooth Diffusion adopts the novel Step-\nwise Variation Regularization, which successfully main-\ntains variation between arbitrary input latent and gener-\nated images at a more bounded range. Smooth Diffusion\nwas trained on top of the prevailing text-to-image model,\nfrom which we carried out extensive research, including but\nnot limited to interpolation, inversion, and editing, all of\nwhich had shown competitive performance. Through quali-\ntative and quantitative measurements, we demonstrated that\nSmooth Diffusion managed to make a smoother latent space\nwithout compromising the output quality. We believe that\nSmooth Diffusion will become a valuable solution for other\nchallenging tasks, such as video generation, in the future.\nReferences\n[1] OpenJourney-V4.\nhttps : / / huggingface . co /\nprompthero/openjourney-v4, 2023. 12, 14\n[2] RealisticVision-V2.\nhttps://huggingface.co/\nSG161222/Realistic_Vision_V2.0, 2023. 12, 14\n[3] Rameen Abdal, Yipeng Qin, and Peter Wonka.\nIm-\nage2StyleGAN++: How to edit the embedded images?\nIn\nCVPR, 2020. 3\n[4] Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Restyle:\nA residual-based stylegan encoder via iterative refinement.\nIn ICCV, 2021. 3\n[5] Martin Arjovsky and L\u00b4eon Bottou.\nTowards princi-\npled methods for training generative adversarial networks.\narXiv:1701.04862, 2017. 3\n[6] Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou.\nWasserstein generative adversarial networks. In ICML, 2017.\n3\n[7] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-\nDPM: an analytic estimate of the optimal reverse variance in\ndiffusion probabilistic models. In ICLR, 2022. 2\n[8] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu,\nYaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu.\nOne transformer fits all distributions in multi-modal diffu-\nsion at scale. In ICML, 2023. 2\n[9] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou,\nJoshua B Tenenbaum, William T Freeman, and Antonio Tor-\nralba. Gan dissection: Visualizing and understanding gener-\native adversarial networks. In ICLR, 2018. 3\n[10] Yoshua Bengio, Eric Laufer, Guillaume Alain, and Jason\nYosinski. Deep generative stochastic networks trainable by\nbackprop. In ICML, 2014. 2\n[11] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\narXiv:1809.11096, 2018. 3\n[12] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein.\npi-gan: Periodic implicit genera-\ntive adversarial networks for 3d-aware image synthesis. In\nCVPR, 2021. 3\n[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. In NeurIPS, 2021. 2, 3\n[14] Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han.\nPrompt tuning inversion for text-driven image editing using\ndiffusion models. arXiv:2305.04441, 2023. 2\n[15] Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Nicu Sebe,\nTrevor Darrell, Zhangyang Wang, and Humphrey Shi. Pair-\ndiffusion: Object-level image editing with structure-and-\nappearance paired diffusion models.\narXiv:2303.17546,\n2023. 2\n[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In NeurIPS,\n2014. 2, 3\n[17] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nCVPR, 2022. 2\n[18] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent\nDumoulin, and Aaron C Courville.\nImproved training of\nwasserstein gans. NeurIPS, 30, 2017. 3\n[19] Jiayi Guo, Chaofei Wang, You Wu, Eric Zhang, Kai Wang,\nXingqian Xu, Humphrey Shi, Gao Huang, and Shiji Song.\nZero-shot generative model adaptation via image-specific\nprompt learning. In CVPR, 2023. 2\n[20] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control. arXiv:2208.01626,\n2022. 2, 5, 7\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In NeurIPS, 2017. 5, 6\n[22] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeurIPS Workshops, 2021. 3, 5\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 2, 3\n[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In\nICLR, 2022. 2, 3, 5\n[25] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli\nZhao,\nand Jingren Zhou.\nComposer:\nCreative and\ncontrollable image synthesis with composable conditions.\narXiv:2302.09778, 2023. 2\n[26] Quan Huynh-Thu and Mohammed Ghanbari. Scope of va-\nlidity of psnr in image/video quality assessment. Electronics\nletters, 2008. 5, 7\n[27] Andrej\nKarpathy.\nStable\nDiffusion\nWalk.\nhttps : / / gist . github . com / karpathy /\n00103b0037c5aaea32fe1da1af553355,\n2022.\n2, 5, 6\n[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, 2019. 2\n[29] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,\nJaakko Lehtinen, and Timo Aila. Training generative adver-\nsarial networks with limited data. In NeurIPS, 2020. 3\n9\n[30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of stylegan. In CVPR, 2020. 2, 3, 4, 6\n[31] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00a8ark\u00a8onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. In NeurIPS, 2021. 2, 3\n[32] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. In NeurIPS, 2022. 2\n[33] Diederik P Kingma and Jimmy Ba. ADAM: A method for\nstochastic optimization. In ICLR, 2015. 5\n[34] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. In ICLR, 2015. 2, 7, 12\n[35] Chieh Hubert Lin, Hsin-Ying Lee, Yen-Chi Cheng, Sergey\nTulyakov, and Ming-Hsuan Yang.\nInfinitygan: Towards\ninfinite-pixel image synthesis. In ICLR, 2021. 3\n[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 5, 6, 12\n[37] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. In NeurIPS,\n2022. 2\n[38] Haoming Lu,\nHazarapet Tunanyan,\nKai Wang,\nShant\nNavasardyan, Zhangyang Wang, and Humphrey Shi. Spe-\ncialist diffusion: Plug-and-play sample-efficient fine-tuning\nof text-to-image diffusion models to learn any unseen style.\nIn CVPR, 2023. 2\n[39] Weijian Mai and Zhijun Zhang. Unibrain: Unify image re-\nconstruction and captioning all in one diffusion model from\nhuman brain activity. arXiv:2308.07428, 2023. 2\n[40] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In ICLR, 2022. 2, 5, 7\n[41] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In CVPR, 2023.\n2\n[42] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models. In CVPR, 2023. 2, 3, 5, 6, 7,\n12, 13, 14\n[43] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and\nJian Zhang. Dragondiffusion: Enabling drag-style manipu-\nlation on diffusion models. arXiv:2307.02421, 2023. 2, 8\n[44] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2I-Adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv:2302.08453, 2023. 2\n[45] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models. In ICML, 2021. 2\n[46] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen. GLIDE: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In ICML, 2022. 2\n[47] Yotam Nitzan, Amit Bermano, Yangyan Li, and Daniel\nCohen-Or.\nFace identity disentanglement via latent space\nmapping. TOG, 2020. 3\n[48] Augustus Odena, Jacob Buckman, Catherine Olsson, Tom\nBrown, Christopher Olah, Colin Raffel, and Ian Goodfel-\nlow. Is generator conditioning causally related to gan per-\nformance? In ICML, 2018. 3, 4\n[49] Xingang Pan,\nXiaohang Zhan,\nBo Dai,\nDahua Lin,\nChen Change Loy, and Ping Luo. Exploiting deep genera-\ntive prior for versatile image restoration and manipulation.\nTPAMI, 2021. 3\n[50] Xingang Pan, Ayush Tewari, Thomas Leimk\u00a8uhler, Lingjie\nLiu, Abhimitra Meka, and Christian Theobalt. Drag your\nGAN: Interactive point-based manipulation on the generative\nimage manifold. In SIGGRAPH, 2023. 3, 8\n[51] Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen\nHuang. Effective real image editing with accelerated iterative\ndiffusion inversion. In ICCV, 2023. 2\n[52] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In SIGGRAPH, 2023. 2, 5, 7\n[53] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. StyleCLIP: Text-driven manipulation\nof StyleGAN imagery. In ICCV, 2021. 3\n[54] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang,\nYingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming\nXiong, Silvio Savarese, et al.\nUnicontrol: A unified dif-\nfusion model for controllable visual generation in the wild.\narXiv:2305.11147, 2023. 2\n[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In ICML, 2021.\n2, 5, 6\n[56] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv:2204.06125, 2022. 2\n[57] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,\nYaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding\nin style: a stylegan encoder for image-to-image translation.\nIn CVPR, 2021. 3\n[58] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel\nCohen-Or. Pivotal tuning for latent-based editing of real im-\nages. ACM Trans. Graph., 2021. 3\n[59] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 1, 2, 5,\n6, 7, 8, 12, 13\n[60] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, 2023. 2\n[61] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\n10\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In NeurIPS, 2022. 2\n[62] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In ICLR, 2022. 2\n[63] Tim Salimans, Diederik Kingma, and Max Welling. Markov\nchain monte carlo and variational inference: Bridging the\ngap. In ICML, 2015. 2\n[64] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. LAION-5B: An open large-scale dataset for train-\ning next generation image-text models. In NeurIPS, 2022. 5\n[65] Yujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou.\nInterfaceGAN: Interpreting the disentangled face represen-\ntation learned by GANs. TPAMI, 2020. 2, 3\n[66] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vin-\ncent YF Tan, and Song Bai.\nDragDiffusion: Harnessing\ndiffusion models for interactive point-based image editing.\narXiv:2306.14435, 2023. 2, 5, 8, 15\n[67] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In ICML, 2015. 2\n[68] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2020. 2, 3, 5, 6, 7,\n12, 14\n[69] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever. Consistency models. In ICML, 2023. 2\n[70] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and\nMohit Bansal. Any-to-any generation via composable diffu-\nsion. arXiv:2305.11846, 2023. 2\n[71] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and\nDaniel Cohen-Or. Designing an encoder for StyleGAN im-\nage manipulation. TOG, 2021. 3\n[72] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In CVPR, 2023. 2, 5, 7\n[73] Pascal Vincent. A connection between score matching and\ndenoising autoencoders. Neural computation, 23(7):1661\u2013\n1674, 2011. 2\n[74] Bram Wallace, Akash Gokul, and Nikhil Naik.\nEDICT:\nExact diffusion inversion via coupled transformations.\nIn\nCVPR, 2023. 2\n[75] Clinton Wang and Polina Golland. Interpolating between im-\nages with diffusion models. In ICML Workshops, 2023. 2, 5,\n6\n[76] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. TIP, 2004. 5, 7\n[77] Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen,\nPengcheng He, Weizhu Chen, Zhangyang Wang, and\nMingyuan Zhou. In-context learning unlocked for diffusion\nmodels. arXiv:2305.01115, 2023. 2\n[78] Chen Henry Wu and Fernando De la Torre. A latent space of\nstochastic diffusion models for zero-shot image editing and\nguidance. In ICCV, 2023. 2, 5, 7\n[79] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,\nTrung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu\nChang. Uncovering the disentanglement capability in text-\nto-image diffusion models. In CVPR, 2023. 2, 5, 7\n[80] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Wensen Feng.\nControllable continuous gaze redirection.\nIn ACM MM,\n2020. 3\n[81] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei\nZhou, and Ming-Hsuan Yang.\nGan inversion: A survey.\nTPAMI, 2022. 3\n[82] Dejia Xu, Xingqian Xu, Wenyan Cong, Humphrey Shi,\nand Zhangyang Wang. Reference-based painterly inpaint-\ning via diffusion: Crossing the wild reference domain gap.\narXiv:2307.10584, 2023. 2\n[83] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang,\nIrfan Essa,\nand Humphrey Shi.\nPrompt-free diffu-\nsion: Taking\u201d text\u201d out of text-to-image diffusion models.\narXiv:2305.16223, 2023. 2\n[84] Xingqian Xu, Shant Navasardyan, Vahram Tadevosyan, An-\ndranik Sargsyan, Yadong Mu, and Humphrey Shi. Image\ncompletion with heterogeneously filtered spectral hints. In\nWACV, 2023. 3\n[85] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang,\nand Humphrey Shi. Versatile diffusion: Text, images and\nvariations all in one diffusion model. In ICCV, 2023. 2\n[86] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. IP-\nAdapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv:2308.06721, 2023. 2\n[87] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and\nHumphrey Shi. Forget-me-not: Learning to forget in text-to-\nimage diffusion models. arXiv preprint arXiv:2303.17591,\n2023.\n[88] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, 2023. 2\n[89] Qinsheng Zhang and Yongxin Chen. Fast sampling of dif-\nfusion models with exponential integrator.\narXiv preprint\narXiv:2204.13902, 2022. 2\n[90] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 5, 7\n[91] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao\nLiang, I Eric, Chao Chang, and Yan Xu. Large scale im-\nage completion via co-modulated generative adversarial net-\nworks. In ICLR, 2020. 3\n[92] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin\nBao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong. Uni-\nControlNet: All-in-one control to text-to-image diffusion\nmodels. In NeurIPS, 2023. 2\n[93] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and\nJiwen Lu. UniPC: A unified predictor-corrector framework\nfor fast sampling of diffusion models. In NeurIPS, 2023. 2\n[94] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-\ndomain gan inversion for real image editing. In ECCV, 2020.\n3\n11\nSupplementary Materials\nA. Implementation Details\nThis section elaborates on details briefly introduced in the\nmain paper. These include the notation, the basic training\nobjective, the interpolation standard deviation (ISTD) met-\nric, and our utilization of Null-text inversion (NTI) [42] for\nreal-image interpolation.\nA.1. Notation\nStable Diffusion [59] employs an efficient \u201clatent\u201d diffusion\npipeline. Here the \u201clatent\u201d refers to using an individually\ntrained (VAE) [34] to compress an input image x0 into its\nVAE-space representation z0:\nz0 = E(x0),\nx0 = D(z0),\n(16)\nwhere E and D represent the encoder and decoder of the\nVAE, respectively. For simplicity, we exclude this conver-\nsion process and only use \u201cx\u201d-based notations in the main\npaper. Although we chose Stable Diffusion as our base-\nline due to its popularity and high performance, our train-\ning pipeline is not specifically tailored for latent diffusion\nmodels and is compatible with other diffusion models.\nA.2. Basic Training Objective\nSmooth Diffusion\u2019s training objective comprises two key\ncomponents: 1) a basic training objective primarily centered\non noise prediction but flexible in formulation for different\ndiffusion models, and 2) our proposed Step-wise Variation\nRegularization term. In our experiments, the basic training\nobjective is:\nLbase = Ex0,\u03f5,t\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t)\u22252\n2,\n(17)\nwhich is a commonly adopted training objective across\nmany diffusion models, e.g., Stable Diffusion [59].\nA.3. ISTD\nThe goal of ISTD is to quantify the deviation of pixel-space\nchanges given the same fixed-step changes in latent space.\nA lower deviation implies the input latents and output im-\nages are more likely to change smoothly. In our experi-\nments, we first randomly draw 500 text prompts from the\nMS-COCO validation set [36]. For each prompt, we then\nsample two random Gaussian noises, \u03f5a and \u03f5b. Next, we\nexecute uniform spherical linear interpolations (slerp) be-\ntween \u03f5a and \u03f5b for 11 times, varying the mixing ratio \u03b7\nfrom 0 to 1:\n\u03f5\u03b7 = slerp(\u03f5a, \u03f5b, \u03b7),\n\u03b7 = 0, 0.1, 0.2, \u00b7 \u00b7 \u00b7 , 1.\n(18)\nWe employ the testing diffusion model to generate 11\ninterpolated images {c\nx\u03b7\n0}1\n\u03b7=0 from {\u03f5\u03b7}1\n\u03b7=0. Notice that\nEq. 18 guarantees that the latent space changes between ev-\nery two adjacent latents (i.e., \u03f5\u03b7 and \u03f5\u03b7+0.1) are the same.\nHence, we calculate the L2 distances between every two ad-\njacent images (i.e., c\nx\u03b7\n0 and \\\nx\u03b7+0.1\n0\n) and compute the stan-\ndard deviation of these distances. Finally, ISTD is the aver-\nage of standard deviations over 500 different text prompts.\nFor a fair comparison, the text prompts and the noises for\neach prompt are the same for different testing models.\nA.4. NTI for real-image interpolation\nNTI is initially designed to transform a real image x0 into a\nlatent f\nxT , along with a series of learnable null-text embed-\ndings {\u2205t}T\nt=1 for each step t. The optimization for each\n\u2205t is formulated as:\nmin\n\u2205t \u2225]\nxt\u22121 \u2212 DDIM(f\nxt, t, \u03be, \u2205t)\u22252\n2.\n(19)\nwhere {f\nxt}T\nt=1 represents intermidiate noisy images es-\ntimated by DDIM inversion [68].\nFor simplicity,\nDDIM(f\nxt, t, \u03be, \u2205t) denotes the DDIM sampling process at\nstep t, utilizing the text embedding \u03be, the null-text embed-\nding \u2205t and the classifier-free guidance scale w = 7.5.\nFor real-image interpolation, we optimize a shared series\nof {\u2205t}T\nt=1 for two real images, xa\n0 and xb\n0:\nmin\n\u2205t \u2225]\nxa\nt\u22121 \u2212 DDIM(f\nxa\nt , t, \u03be, \u2205t)\u22252\n2+\n\u2225]\nxb\nt\u22121 \u2212 DDIM(f\nxb\nt, t, \u03be, \u2205t)\u22252\n2.\n(20)\nIn our experiments, we only interpolate the latents f\nxa\nT\nand f\nxb\nT following Eq. 18 and use the same null-text embed-\ndings {\u2205t}T\nt=1 for all interpolated images.\nB. Additional Results\nThis section provides additional visual results of Smooth\nDiffusion. We display image interpolation results in Fig. 7\nand Fig. 8, image inversion and reconstruction results\nin Fig. 9, and image editing results in Fig. 10.\nReusability. The LoRA component of Smooth Diffusion\nremains adaptable to other models sharing the same archi-\ntecture as Stable Diffusion. However, the effectiveness of\nthis reusability is not guaranteed. We evaluate the integra-\ntion of this LoRA component into two popular community\nmodels, RealisticVision-V2 [2] and OpenJourney-V4 [1].\nAs depicted in Fig. 8, this integration also enhances the\nlatent space smoothness of these models. This reusability\nmakes our method eliminate the need for repeated training\nand become a plug-and-play module across various models.\n12\n\u201cA cute rabbit\u201d\n\u201cA woman face\u201d\n\u201cA beautiful landscape\u201d\n\u201cA chocolate cake\u201d\nImage A\nImage B\nInterpolation\nStable\nDiffusion\nStable\nDiffusion\nSmooth\nDiffusion\nStable\nDiffusion\nSmooth\nDiffusion\nSmooth\nDiffusion\nSmooth\nDiffusion\nStable\nDiffusion\nFigure 7. Additional image interpolation results with Smooth Diffusion. For Smooth Diffusion and Stable Diffusion [59], real images\n(Image A and B) are inverted into latents using Null-text inversion [42]. We perform spherical linear interpolations between latents and\nconcatenate the resulting images as a transition sequence.\n13\n\u201cA basket of apples\u201d\n\u201cA robot horse\u201d\nSmooth\nRealisticVision-V2\nImage A\nImage B\nInterpolation\nRealisticVision-V2\nSmooth\nOpenJourney-V4\nOpenJourney-V4\nFigure 8. Image interpolation results with community models. We apply the LoRA component of Smooth Diffusion to RealisticVision-\nV2 [2] and OpenJournery-V4 [1] and perform spherical linear interpolations in their latent spaces.\n\u201cA city bus is parked on the curb waiting for people\u201d\n\u201cA dog that is wearing a dog collar smiling\u201d\n\u201cA hand holding a smart phone with apps on a screen\u201d\n\u201cA plate of cooked food in seen in this image\u201d\n\u201cA skateboard that has its wheels on the floor\u201d\n\u201cThe train engine number 6309 is operated by BNSF\u201d\n\u201cLarge four sided clock hangs on the corner of the building\u201d\n\u201cAn older Dodge pickup sits parked next to another older pickup\u201d\n\u201cA woman walking down a street talking on a cell phone\u201d\nSmooth \nDiffusion\n+NTI \nSmooth \nDiffusion\n+DDIM \nSource\nSmooth \nDiffusion\n+NTI \nSmooth \nDiffusion\n+DDIM \nSource\nSmooth \nDiffusion\n+NTI \nSmooth \nDiffusion\n+DDIM \nSource\nFigure 9. Additional image inversion and reconstruction results with Smooth Diffusion. We integrate Smooth Diffusion with two\ntypical diffusion inversion techniques, Null-text inversion [42] and DDIM inversion [68].\n14\nSmooth \nDiffusion\nSource\nUser Edit\nLocal Edit\n(Replace Item)\nLocal Edit\n(Add Item)\nGlobal Edit\n(Transfer Style)\nDrag Edit\n(Move Point)\n\u201cA basket of apples\u201d\n\u201capples\u201d\n\u2193\n\u201coranges\u201d\n\u201cA bird standing on a branch\u201d\n\u201cbird\u201d\n\u2193\n\u201cLego bird\u201d\n\u201cdog\u201d\n\u2193\n\u201ctoy dog\u201d\n\u201cA dog standing on the bench\u201d\n\u201cA dog in a jacket\u201d\n+\u201cwearing \nsunglasses\u201d\n+\u201cwith a turtle \non it\u201d\n\u201cA beach\u201d\n\u201cA young girl\u201d\n+\u201cwearing\na hat\u201d\n\u201cA sitting cat\u201d\n\u201canime painting\u201d\n\u2192\n\u201cA photo of a tree\u201d\n\u201cA river with trees on both sides\u201d\n\u201cVan Gogh \npainting\u201d\n\u2192\n\u201cA red car with a black roof\u201d\n\u201cWatercolor\ndrawing\u201d\n\u2192\n\u201cAn oil painting of a mountain\u201d\nSmooth \nDiffusion\nSource\nUser Edit\nSmooth \nDiffusion\nSource\nUser Edit\n\u201cA photo of a river\u201d\nFigure 10. Additional image editing results with Smooth Diffusion. Both text-based image editing and drag-based image editing are\nevaluated. For text-based image editing, we consider both local and global edits to test Smooth Diffusion. For drag-based image editing,\nSmooth Diffusion is integrated into the framework of DragDiffusion [66].\n15\n"
  },
  {
    "title": "GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation",
    "link": "https://arxiv.org/pdf/2312.04557.pdf",
    "upvote": "12",
    "text": "Delving Deep into Diffusion Transformers for Image and Video Generation\nShoufa Chen1,2*\nMengmeng Xu2*\nJiawei Ren2\nYuren Cong2\nSen He2\nYanping Xie2\nAnimesh Sinha2\nPing Luo1\nTao Xiang2\nJuan-Manuel Perez-Rua2\n1The University of Hong Kong\n2Meta\nFigure 1. GenTron: Transformer based diffusion model for high-quality text-to-image/video generation.\nAbstract\nIn this study, we explore Transformer-based diffusion\nmodels for image and video generation. Despite the domi-\nnance of Transformer architectures in various fields due to\ntheir flexibility and scalability, the visual generative domain\nprimarily utilizes CNN-based U-Net architectures, particu-\nlarly in diffusion-based models. We introduce GenTron, a\nfamily of Generative models employing Transformer-based\ndiffusion, to address this gap. Our initial step was to adapt\nDiffusion Transformers (DiTs) from class to text condition-\ning, a process involving thorough empirical exploration of\nthe conditioning mechanism. We then scale GenTron from\napproximately 900M to over 3B parameters, observing sig-\nnificant improvements in visual quality. Furthermore, we\nextend GenTron to text-to-video generation, incorporating\nnovel motion-free guidance to enhance video quality.\nIn human evaluations against SDXL, GenTron achieves a\n51.1% win rate in visual quality (with a 19.8% draw rate),\nand a 42.3% win rate in text alignment (with a 42.9% draw\nrate). GenTron also excels in the T2I-CompBench, under-\nscoring its strengths in compositional generation. We be-\nlieve this work will provide meaningful insights and serve\nas a valuable reference for future research. The website of\nGenTron is available1.\n1https://www.shoufachen.com/gentron_website/\n*Equal contribution.\n1\narXiv:2312.04557v1  [cs.CV]  7 Dec 2023\n1. Introduction\nDiffusion models have recently shown remarkable progress\nin content creation, impacting areas such as image genera-\ntion [25, 51, 53], video production [5, 26, 57], audio syn-\nthesis [54], and code generation [58]. Among these no-\ntable developments, the convolutional U-Net architecture\nhas emerged as the predominant backbone design, a choice\nthat stands in contrast to the prevailing trend in natural\nlanguage processing [7, 17, 60] and computer visual per-\nception [18, 34, 67] domains, where attention-based trans-\nformer architectures [61] have empowered a renaissance\nand become increasingly dominant. To provide a compre-\nhensive understanding of Transformers in diffusion genera-\ntion and to bridge the gap in architectural choices between\nvisual generation and the other two domains \u2014 visual per-\nception and NLP \u2014 a thorough investigation of visual gen-\neration using Transformers is of substantial scientific value.\nWe focus on diffusion models with Transformers in\nthis work. Specifically, our starting point is the founda-\ntional work known as DiT [41], which introduced a class-\nconditioned latent diffusion model that employs a Trans-\nformer to replace the traditionally used U-Net architec-\nture.\nWe first overcome the limitation of the original\nDiT model, which is constrained to handling only a re-\nstricted number (e.g., 1000) of predefined classes, by uti-\nlizing language embeddings derived from open-world, free-\nform text captions instead of predefined one-hot class em-\nbeddings.\nAlong the way, we comprehensively investi-\ngate conditioning strategies, including (1) conditioning ar-\nchitectures: adaptive layer norm (adaLN) [42] vs. cross-\nattention [61]; and (2) text encoding methods: a generic\nlarge language model [12] vs. the language tower of mul-\ntimodal models [47], or the combination of both of them.\nWe additionally carry out comparative experiments and of-\nfer detailed empirical analyses to evaluate the effectiveness\nof these conditioning strategies.\nNext, we explore the scaling-up properties of Gen-\nTron.\nThe Transformer architectures have been demon-\nstrated to possess significant scalability in both visual per-\nception [10, 15, 49, 67] and language [7, 17, 45, 46, 60]\ntasks. For example, the largest dense language model has\n540B parameters [11], and the largest vision model has\n22B [15] parameters. In contrast, the largest diffusion trans-\nformer, DiT-XL [41], only has about 675M parameters,\ntrailed far behind both the Transformers utilized in other\ndomains (e.g., NLP) and recent diffusion arts with convolu-\ntional U-Net architectures [13, 43]. To compensate for this\nconsiderable lagging, we scale up GenTron in two dimen-\nsions, the number of transformer blocks and hidden dimen-\nsion size, following the scaling strategy in [67]. As a result,\nour largest model, GenTron-G/2, has more than 3B param-\neters and achieves significant visual quality improvement\ncompared with the smaller one.\nFurthermore, we have advanced GenTron from a T2I to\na T2V model by inserting a temporal self-attention layer\ninto each transformer block, making the first attempt to\nuse transformers as the exclusive building block for video\ndiffusion models. We also discuss existing challenges in\nvideo generation and introduce our solution, the motion-\nfree guidance (MFG). Specifically, This approach involves\nintermittently disabling motion modeling during training by\nsetting the temporal self-attention mask to an identity ma-\ntrix.\nBesides, MFG seamlessly integrates with the joint\nimage-video strategy [8, 14, 28, 63], where images are used\nas training samples whenever motion is deactivated. Our\nexperiments indicate that this approach clearly improves the\nvisual quality of generated videos.\nIn human evaluations, GenTron outperforms SDXL,\nachieving a 51.1% win rate in visual quality (with a 19.8%\ndraw rate), and a 42.3% win rate in text alignment (with\na 42.9% draw rate). Furthermore, when compared to pre-\nvious studies, particularly as benchmarked against T2I-\nCompBench [29]\u2013a comprehensive framework for evalu-\nating open-world compositional T2I generation\u2013GenTron\ndemonstrates superior performance across various criteria.\nThese include attribute binding, object relationships, and\nhandling of complex compositions.\nOur contributions are summarized as follows: (1) We\nhave conducted a thorough and systematic investigation of\ntransformer-based T2I generation with diffusion models.\nThis study encompasses various conditioning choices and\naspects of model scaling. (2) In a pioneering effort, we ex-\nplore a purely transformer-based diffusion model for T2V\ngeneration. We introduce motion-free guidance, an inno-\nvative technique that efficiently fine-tunes T2I generation\nmodels for producing high-quality videos. (3) Experimental\nresults indicate a clear preference for GenTron over SDXL\nin human evaluations. Furthermore, GenTron demonstrates\nsuperior performance compared to existing methods in the\nT2I-CompBench evaluations.\n2. Related Work\nDiffusion models for T2I and T2V generation.\nDiffu-\nsion models [25, 39] are a type of generative model that cre-\nates data samples from random noise. Later, latent diffusion\nmodels [41, 43, 51] are proposed for efficient T2I genera-\ntion. These designs usually have 1) a pre-trained Variational\nAutoencoder [33] that maps images to a compact latent\nspace, 2) a conditioner modeled by cross-attention [26, 51]\nto process text as conditions with a strength control [24],\nand 3) a backbone network, U-Net [52] in particular, to pro-\ncess image features. The success of diffusion on T2I gen-\neration tasks underscores the promising potential for text-\nto-video (T2V) generation [32, 38, 44, 57]. VDM [28] and\nImagen Video [26] extend the image diffusion architecture\non the temporal dimension with promising initial results. To\n2\navoid excessive computing demands, video latent diffusion\nmodels [5, 8, 23, 37, 62, 70] implement the video diffusion\nprocess in a low-dimensional latent space.\nTransformer-based Diffusion.\nRecently, Transformer-\nbased Diffusion models have attracted increasing research\ninterest. Among these, U-ViT [3] treats all inputs as tokens\nby integrating transformer blocks with a U-net architecture.\nIn contrast, DiT [41] employs a simpler, non-hierarchical\ntransformer structure. MDT [20] and MaskDiT [69] en-\nhance DiT\u2019s training efficiency by incorporating the mask\nstrategy [22].\nDolfin [64] is a transformer-based model\nfor layout generation. Concurrently to this work, PixArt-\n\u03b1 [9] demonstrates promising outcomes in Transformer-\nbased T2I diffusion.\nIt\u2019s trained using a three-stage de-\ncomposition process with high-quality data. Our work di-\nverges from PixArt-\u03b1 in key aspects. Firstly, while PixArt-\n\u03b1 emphasizes training efficiency, our focus is on the de-\nsign choice of conditioning strategy and scalability in T2I\nTransformer diffusion models. Secondly, we extend our ex-\nploration beyond image generation to video diffusion. We\npropose an innovative approach in video domain, which is\ncovered by PixArt-\u03b1.\n3. Method\nWe first introduce the preliminaries in Section 3.1, and then\npresent the details of GenTron for text-to-image generation\nin Section 3.2, which includes text encoder models, embed-\nding integration methods, and scaling up strategy of Gen-\nTron. Lastly, in Section 3.3, we extend GenTron\u2019s applica-\ntion to video generation, building on top of the T2I founda-\ntions laid in previous sections.\n3.1. Preliminaries\nDiffusion models.\nDiffusion models [25] have emerged\nas a family of generative models that generate data by per-\nforming a series of transformations on random noise. They\nare characterized by a forward and a backward process.\nGiven an instance from the data distribution x0 \u223c p(x0),\nrandom Gaussian noise is iteratively added to the instance\nin the forward noising process to create a Markov Chain of\nrandom latent variable x1, x2, ..., xT following:\nq(xt|xt\u22121) = N(xt;\np\n1 \u2212 \u03b2ixt\u22121, \u03b2tI),\n(1)\nwhere \u03b21, ...\u03b2T are hyperparameters corresponding to the\nnoise schedule. After a large enough number of diffusion\nsteps, xT can be viewed as a standard Gaussian noise. A\ndenoising network \u03f5\u03b8 is further trained to learn the back-\nward process, i.e., how to remove the noise from a noisy in-\nput [25]. For inference, an instance can be sampled starting\nfrom a random Gaussian noise xT \u223c N(0; I) and denoised\nLayerNorm\nMulti-Head\nSelf-Attention\nLayerNorm\nMLP\nScale\nScale, Shift\nScale\nScale, Shift\n\ud835\udefc!\n\ud835\udefc\" \n\ud835\udefe\" \ud835\udefd\"\n\ud835\udefe! \ud835\udefd!\nMLP\ntime\ntext\n+\n(a) adaLN-Zero\nLayerNorm\nMulti-Head\nSelf-Attention\nLayerNorm\nMLP\nScale\nScale, Shift\nScale\nScale, Shift\n\ud835\udefc!\n\ud835\udefc\" \n\ud835\udefe\" \ud835\udefd\"\n\ud835\udefe! \ud835\udefd!\nMLP\nLayerNorm\nMulti-Head\nCross-Attention\ntext\ntime\ntext\n+\npooling\n(b) Cross attention\nFigure 2. Text embedding integration architecture. We directly\nadapt adaLN from DiT [41], substituting the one-hot class embed-\nding with text embedding. For cross attention, different from the\napproach in [41], we maintain the use of adaLN to model the com-\nbination of time embedding and the aggregated text embedding.\nstep-by-step following the Markov Chain, i.e., by sequen-\ntially sampling xt\u22121 to x0 with p\u03b8(xt\u22121|xt):\nxt\u22121 =\n1\n\u221a\u03b1t\n\u0010\nxt \u2212 1 \u2212 \u03b1t\n\u221a1 \u2212 \u00af\u03b1t\n\u03f5\u03b8(xt, t)\n\u0011\n+\u03c3tz,\n(2)\nwhere \u00af\u03b1t = Qt\ns=1 \u03b1s, \u03b1t = 1 \u2212 \u03b2t and \u03c3t is the noise\nscale. In practical application, the diffusion sampling pro-\ncess can be further accelerated using different sampling\ntechniques [36, 59].\nLatent diffusion model architectures.\nLatent diffusion\nmodels (LDMs) [51] reduce the high computational cost by\nconducting the diffusion process in the latent space. First,\na pre-trained autoencoder [19, 33] is utilized to compress\nthe raw image from pixel to latent space, then the diffu-\nsion models, which are commonly implemented with a U-\nNet [52] backbone, work on the latent space. Peebles et\nal. proposed DiT [41] to leverage the transformer architec-\nture as an alternative to the traditional U-Net backbone for\nclass-conditioned image generation, adopting the adaptive\nlayernorm (adaLN [42]) for class conditioning mechanism,\nas shown in Figure 2a.\n3.2. Text-to-Image GenTron\nOur GenTron is built upon the DiT-XL/2 [41], which con-\nverts the latent of shape 32\u00d732\u00d74 to a sequence of non-\noverlapping tokens with a 2\u00d72 patchify layer [18]. Then,\n3\nthese tokens are sent into a series of transformer blocks. Fi-\nnally, a standard linear decoder is applied to convert these\nimage tokens into latent space.\nWhile DiT has shown that transformer-based models\nyield promising results in class-conditioned scenarios, it did\nnot explore the realm of T2I generation. This field poses\na considerable challenge, given its less constrained con-\nditioning format. Moreover, even the largest DiT model,\nDiT-XL/2, with its 675 million parameters, is significantly\novershadowed by current U-Nets [13, 43], which boast over\n3 billion parameters. To address these limitations, our re-\nsearch conducts a thorough investigation of transformer-\nbased T2I diffusion models, focusing specifically on text\nconditioning approaches and assessing the scalability of the\ntransformer architecture by expanding GenTron to more\nthan 3 billion parameters.\n3.2.1\nFrom Class to Text Condition\nT2I diffusion models rely on textual inputs to steer the pro-\ncess of image generation. The mechanism of text condi-\ntioning involves two critical components: firstly, the selec-\ntion of a text encoder, which is responsible for converting\nraw text into text embeddings, and secondly, the method\nof integrating these embeddings into the diffusion process.\nFor a complete understanding, we have included in the ap-\npendix a detailed presentation of the decisions made in ex-\nisting works concerning these two components.\nText encoder model.\nCurrent advancements in T2I dif-\nfusion techniques employ a variety of language models,\neach with its unique strengths and limitations.\nTo thor-\noughly assess which model best complements transformer-\nbased diffusion methods, we have integrated several models\ninto GenTron. This includes the text towers from multi-\nmodal models, CLIP [47], as well as a pure large language\nmodel, Flan-T5 [12]. Our approach explores the effective-\nness of these language models by integrating each model in-\ndependently with GenTron to evaluate their individual per-\nformance and combinations of them to assess the potential\nproperties they may offer when used together.\nEmbedding integration.\nIn our study, we focused on\ntwo methods of embedding integration: adaptive layernorm\nand cross-attention.\n(1) Adaptive layernorm (adaLN).\nAs shown in Figure 2a, this method integrates condition-\ning embeddings as normalization parameters on the feature\nchannel. Widely used in conditional generative modeling,\nsuch as in StyleGAN [31], adaLN serves as the standard\napproach in DiT [41] for managing class conditions. (2)\nCross-attention. As illustrated in Figure 2b, the image fea-\nture acts as the query, with textual embedding serving as\nkey and value. This setup allows for direct interaction\nModel\nDepth\nWidth\nMLP Width\n#Param.\nGenTron-XL/2\n28\n1152\n4608\n930.0M\nGenTron-G/2\n48\n1664\n6656\n3083.8M\nTable 1. Configuration details of GenTron models.\nbetween the image feature and textual embedding through\nan attention mechanism [61]. Besides, different from the\ncross-attention discussed in [41], which processes the class\nembedding and time embedding together by firstly concate-\nnating them, we maintain the use of adaLN in conjunction\nwith the cross-attention to separately model the time embed-\nding. The underlying rationale for this design is our belief\nthat the time embedding, which is consistent across all spa-\ntial positions, benefits from the global modulation capabil-\nities of adaLN. Moreover, we also add the pooled text em-\nbeddings to the time embedding followings [2, 27, 40, 43].\n3.2.2\nScaling Up GenTron\nTo explore the impact of substantially scaling up the model\nsize, we have developed an advanced version of GenTron,\nwhich we refer to as GenTron-G/2. This model was con-\nstructed in accordance with the scaling principles outlined\nin [67]. We focused on expanding three critical aspects:\nthe number of transformer blocks (depth), the dimension-\nality of patch embeddings (width), and the hidden dimen-\nsion of the MLP (MLP-width). The specifications and con-\nfigurations of the GenTron models are detailed in Table 1.\nSignificantly, the GenTron-G/2 model boasts over 3 billion\nparameters. To our knowledge, this represents the largest\ntransformer-based diffusion architecture developed to date.\n3.3. Text-to-Video GenTron\nIn this subsection, we elaborate on the process of adapt-\ning GenTron from a T2I framework to a T2V framework.\nSec. 3.3.1 will detail the modifications made to the model\u2019s\narchitecture, enabling GenTron to process video data. Fur-\nthermore, Sec. 3.3.2 will discuss the challenges encountered\nin the domain of video generation and the innovative solu-\ntions we have proposed to address them.\n3.3.1\nGenTron-T2V Architecture\nTransformer block with temporal self-attention.\nIt is\ntypically a common practice to train video diffusion mod-\nels from image diffusion models by adding new tempo-\nral modeling modules [5, 21, 28, 70].\nThese usually\nconsist of 3D convolutional layers and temporal trans-\nformer blocks that focus on calculating attention along\nthe temporal dimension. In contrast to the traditional ap-\nproach [5], which involves adding both temporal convo-\nlution layers and temporal transformer blocks to the T2I\nU-Net, our method integrates only lightweight temporal\n4\nMulti-Head\nSelf-Attention\nMLP\nMulti-Head\nCross-Attention\nTempSelfAttn\nMulti-Head\nSelf-Attention\n1\n0 0 0 0 0 0 0\n0\n1 0 0 0 0 0 0\n0\n0 1 0 0 0 0 0\n0\n0 0 1 0 0 0 0\n0\n0 0 0 1 0 0 0\n0\n0 0 0 0 1 0 0\n0\n0 0 0 0 0 1 0\n0\n0 0 0 0 0 0 1\n\ud835\udc47!\n\ud835\udc47\"\n\ud835\udc47#\n\ud835\udc47$\n\ud835\udc47%\n\ud835\udc47&\n\ud835\udc47'\n\ud835\udc47(\n\ud835\udc47!\n\ud835\udc47\"\n\ud835\udc47#\n\ud835\udc47$\n\ud835\udc47%\n\ud835\udc47&\n\ud835\udc47'\n\ud835\udc47(\nMotion-Free Mask\n\ud835\udc5dmotion_free\n\ud83d\udd0d\nFigure 3. GenTron Transformer block with TempSelfAttn and\nMotion-Free Mask. The temporal self-attention layer is inserted\nbetween the cross-attention and the MLPs. The motion-free mask,\nwhich is an identity matrix, will be utilized in the TempSelfAttn\nwith a probability of pmotion free. We omit details like text condi-\ntioning, LN here for simplicity, which could be found in Figure 2.\nself-attention (TempSelfAttn) layers into each transformer\nblock. As depicted in Figure 3, the TempSelfAttn layer\nis placed right after the cross-attention layer and before\nthe MLP layer. Additionally, we modify the output of the\ncross-attention layer by reshaping it before it enters the\nTempSelfAttn layer and then reshape it back to its origi-\nnal format once it has passed through. This process can be\nformally represented as:\nx = rearrange(x, (b t) n d \u2212\u2192 (b n) t d) (3)\nx = x + TempSelfAttn(LN(x))\n(4)\nx = rearrange(x, (b n) t d \u2212\u2192 (b t) n d) (5)\nwhere b, t, n, d represent the batch size, number of frames,\nnumber of patches per frame, and channel dimension, re-\nspectively. rearrage is a notation from [50]. We discov-\nered that a simple TempSelfAttn layer suffices to capture\nmotion, a finding that aligns with observations in a recent\nstudy [63]. In addition, only using TempSelfAttn makes it\nconvenient to turn on and turn off the temporal modeling,\nwhich would be discussed in Sec. 3.3.2.\nInitialization.\nWe use the pre-trained T2I model as a ba-\nsis for initializing the shared layers between T2I and T2V\nmodels. In addition, for the newly added TempSelfAttn\nlayers, we initialize the weights and biases of the output\nproject layers to zero. This ensures that at the beginning of\nthe T2V fine-tuning stage, these layers produce a zero out-\nput, effectively functioning as an identity mapping in con-\njunction with the shortcut connection.\n3.3.2\nMotion-Free Guidance\nChallenges encountered.\nWe observed a notable phe-\nnomenon in the current T2V diffusion models [5, 28] where\nthe per-frame visual quality significantly lags behind that\nof T2I models [13, 43, 51, 65]. Furthermore, our analysis\nrevealed a remarkable degradation in visual quality in the\nT2V models post-fine-tuning, especially when compared to\ntheir original T2I counterparts. We note that these problems\ngenerally exist in current T2V diffusion models, not limited\nto our transformer-based T2V.\nProblem analysis and insights.\nWe presume that the ob-\nserved lag in the visual quality of T2V primarily stems\nfrom two factors: the nature of video data and the fine-\ntuning approach. Firstly, publicly available video datasets\noften fall short in both quality and quantity compared to im-\nage datasets. For instance, [55] has more than 2B English\nimage-text pairs, whereas the current widely used video\ndataset, WebVid-10M [1] contains only 10.7M video-text\npairs. Additionally, many video frames are compromised\nby motion blur and watermarks, further reducing their vi-\nsual quality. This limited availability hampers the develop-\nment of robust and versatile video diffusion models. Sec-\nondly, the focus on optimizing temporal aspects during\nvideo fine-tuning can inadvertently compromise the spatial\nvisual quality, resulting in a decline in the overall quality of\nthe generated videos.\nSolution I: joint image-video training.\nFrom the data as-\npect, we adopt the joint image-video training strategy [8, 14,\n28, 63] to mitigate the video data shortages. Furthermore,\njoint training helps to alleviate the problem of domain dis-\ncrepancy between video and image datasets by integrating\nboth data types for training.\nSolution II: motion-free guidance.\nWe treat the tempo-\nral motion within a video clip as a special conditioning sig-\nnal, which can be analogized to the textual conditioning in\nT2I/T2V diffusion models. Based on this analogy, we pro-\npose a novel approach, motion-free guidance (MFG), in-\nspired by classifier-free guidance [6, 24], to modulate the\nweight of motion information in the generated video.\nIn a particular training iteration, our approach mirrors\nthe concept used in classifier-free guidance, where condi-\ntioned text is replaced with an empty string. The difference\nis that we employ an identity matrix to nullify the tempo-\nral attention with a probability of pmotion free. This identity\nmatrix, which is depicted in Figure 3 (Motion-Free Mask),\nis structured such that its diagonal is populated with ones,\nwhile all other positions are zeroes. This configuration con-\nfines the temporal self-attention to work within a single\n5\nConditioning\nScale\nAttribute Binding\nObject Relationship\nComplex\nMean\nText Encoder\nType\nIntegration\nColor\nShape\nTexture\nSpatial\nNon-spatial\nCLIP-L [47]\nMM\nadaLN-zero\nXL/2\n36.94\n42.06\n50.73\n9.41\n30.38\n36.41\n34.32\nCLIP-L [47]\nMM\ncross-attn\nXL/2\n73.91\n51.81\n68.76\n19.26\n31.80\n41.52\n47.84\nT5-XXL [12]\nLLM\ncross-attn\nXL/2\n74.90\n55.40\n70.05\n20.52\n31.68\n41.01\n48.93\nCLIP-T5XXL\nMM + LLM\ncross-attn\nXL/2\n75.65\n55.74\n69.48\n20.67\n31.79\n41.44\n49.13\nCLIP-T5XXL\nMM + LLM\ncross-attn\nG/2\n76.74\n57.00\n71.50\n20.98\n32.02\n41.67\n49.99\nTable 2. Conditioning and model scale in GenTron. We compare GenTron model variants with different design choices on T2I-\nCompBench [29]. The text encoders are from the language tower of the multi-modal (MM) model, the large language model (LLM), or a\ncombination of them. The GenTron-G/2 with CLIP-T5XXL performs best. Detailed discussions can be found in Sec. 4.2.\nframe. Furthermore, as introduced in Sec. 3.3.1, tempo-\nral self-attention is the sole operator for temporal modeling.\nThus, using a motion-free attention mask suffices to disable\ntemporal modeling in the video diffusion process.\nDuring inference, we have text and motion condition-\nings. Inspired by [6], we can modify the score estimate as:\n\u02dc\u03f5\u03b8 = \u03f5\u03b8(xt, \u2205, \u2205)\n+ \u03bbT \u00b7 (\u03f5\u03b8(xt, cT , cM) \u2212 \u03f5\u03b8(xt, \u2205, cM))\n+ \u03bbM \u00b7 (\u03f5\u03b8(xt, \u2205, cM) \u2212 \u03f5\u03b8(xt, \u2205, \u2205))\n(6)\nwhere cT and cM represent the text conditioning and motion\nconditioning. \u03bbT and \u03bbM are the guidance scale of standard\ntext and that of motion, controlling how strongly the gener-\nated samples correspond with the text condition and the mo-\ntion strength, respectively. We empirically found that fixing\n\u03bbT = 7.5 and adjusting \u03bbM \u2208 [1.0, 1.3] for each example\ntend to achieve the best result. This finding is similar to [6],\nalthough our study utilizes a narrower range for \u03bbM.\nPutting solutions together.\nWe can integrate solution I\nand II together in the following way: when the motion is\nomitted at a training step, we load an image-text pair and\nrepeat the image T \u22121 times to create a pseudo video. Con-\nversely, if motion is included, we instead load a video clip\nand extract it into T frames.\n4. Experiments\n4.1. Implementation Details\nTraining scheme\nFor all GenTron model variations, we\nemploy the AdamW [35] optimizer, maintaining a constant\nlearning rate of 1\u00d710\u22124. We train our T2I GenTron mod-\nels in a multi-stage procedure [43, 51] with an internal\ndataset, including a low-resolution (256\u00d7256) training with\na batch size of 2048 and 500K optimization steps, as well\nas high-resolution (512\u00d7512) with a batch size of 784 and\n300K steps. For the GenTron-G/2 model, we further inte-\ngrate Fully Sharded Data Parallel (FSDP) [68] and activa-\ntion checkpointing (AC), strategies specifically adopted to\noptimize GPU memory usage. In our video experiments,\n(a) adaLN-Zero\n(b) Cross attention\nFigure 4. adaLN-Zero vs. cross attention. The prompt is \u201cA\npanda standing on a surfboard in the ocean in sunset.\u201d Cross attention\nexhibits a distinct advantage in the text-conditioned scenario.\nwe train videos on a video dataset that comprises approxi-\nmately 34M videos. To optimize storage usage and enhance\ndata loading efficiency, the videos are pre-processed to a\nresolution with a short side of 512 pixels and a frame rate\nof 24 FPS. We process batches of 128 video clips. Each clip\ncomprises 8 frames, captured at a sampling rate of 4 FPS.\nEvaluation metrics.\nWe mainly adopt the recent T2I-\nCompBench [29] to compare GenTron model variants, fol-\nlowing [4, 9]. Specifically, we compare the attribute bind-\ning aspects, which include color, shape, and texture. We\nalso compare the spatial and non-spatial object relation-\nships. Moreover, user studies are conducted to compare\nvisual quality and text alignment.\n4.2. Main Results of GenTron-T2I\nIn this subsection, we discuss our experimental results, fo-\ncusing on how various conditioning factors and model sizes\nimpact GenTron\u2019s performance. Table 2 presents the quan-\ntitative findings of our study. Additionally, we offer a com-\nparative visualization, illustrating the effects of each condi-\ntioning factor we explored. A comparison to prior art is also\nprovided.\nCross attention vs. adaLN-Zero.\nRecent findings [41]\nconclude that the adaLN design yields superior results in\n6\nModel\nAttribute Binding\nObject Relationship\nComplex\nMean\nColor\nShape\nTexture\nSpatial\nNon-spatial\nLDM v1.4\n37.65\n35.76\n41.56\n12.46\n30.79\n30.80\n31.50\nLDM v2\n50.65\n42.21\n49.22\n13.42\n30.96\n33.86\n36.72\nComposable v2\n40.63\n32.99\n36.45\n8.00\n29.80\n28.98\n29.47\nStructured v2\n49.90\n42.18\n49.00\n13.86\n31.11\n33.55\n36.60\nAttn-Exct v2\n64.00\n45.17\n59.63\n14.55\n31.09\n34.01\n41.41\nGORS\n66.03\n47.85\n62.87\n18.15\n31.93\n33.28\n43.35\nDALL\u00b7E 2\n57.50\n54.64\n63.74\n12.83\n30.43\n36.96\n42.68\nLDM XL\n63.69\n54.08\n56.37\n20.32\n31.10\n40.91\n44.41\nPixArt-\u03b1\n68.86\n55.82\n70.44\n20.82\n31.79\n41.17\n48.15\nGenTron-CLIPT5XXL-G/2\n76.74\n57.00\n71.50\n20.98\n32.02\n41.67\n49.99\nTable 3. Comparison of alignment evaluation on T2I-CompBench [29]. Results show our advanced model, GenTron-CLIPT5XXL-G/2,\nachieves superior performance across multiple compositional metrics compared to previous methods.\nterms of the FID, outperforming both cross-attention and\nin-context conditioning in efficiency for class-based scenar-\nios. However, our observations reveal a limitation of adaLN\nin handling free-form text conditioning. This shortcoming\nis evident in Figure 4, where adaLN\u2019s attempt to generate\na panda image falls short, with cross-attention demonstrat-\ning a clear advantage. This is further verified quantitatively\nin the first two rows of Table 2, where cross-attention uni-\nformly excels over adaLN in all evaluated metrics.\nThis outcome is reasonable considering the nature of\nclass conditioning, which typically involves a limited set\nof fixed signals (e.g., the 1000 one-hot class embeddings\nfor ImageNet [16]). In such contexts, the adaLN approach,\noperating at a spatially global level, adjusts the image fea-\ntures uniformly across all positions through the normaliza-\ntion layer, making it adequate for the static signals. In con-\ntrast, cross-attention treats spatial positions with more gran-\nularity. It differentiates between various spatial locations by\ndynamically modulating image features based on the cross-\nattention map between the text embedding and the image\nfeatures. This spatial-sensitive processing is essential for\nfree-from text conditioning, where the conditioning signals\nare infinitely diverse and demand detailed representation in\nline with the specific content of textual descriptions.\nComparative analysis of text encoders.\nIn Table 2 (rows\ntwo to four), we conduct a quantitative evaluation of various\ntext encoders on T2I-CompBench, ensuring a fair compar-\nison by maintaining a consistent XL/2 size across models.\nResults reveal that GenTron-T5XXL outperforms GenTron-\nCLIP-L across all three attribute binding and spatial rela-\ntionship metrics, while it demonstrates comparable perfor-\nmance in the remaining two metrics. This suggests that T5\nembeddings are superior in terms of compositional ability.\nThese observations are in line with [2], which utilizes both\nCLIP and T5 embeddings for training but tests them indi-\nvidually or in combination during inference. Unlike eDiff,\n(a) GenTron-XL/2\n(b) GenTron-G/2\nFigure 5. Effect of model scale. The prompt is \u201ca cat reading a\nnewspaper\u201d. The larger model GenTron-G/2 excels in rendering\nfiner details and rationalization in the layout of the cat and news-\npaper. More comparisons can be found in the appendix.\nour approach maintains the same settings for both training\nand inference. Notably, GenTron demonstrates enhanced\nperformance when combining CLIP-L and T5XXL embed-\ndings, indicating the model\u2019s ability to leverage the distinct\nadvantages of each text embedding type.\nScaling GenTron up.\nIn Figure 5, we showcase exam-\nples from the PartiPrompts benchmark [66] to illustrate the\nqualitative enhancements achieved by scaling our model\nup from approximately 900 million to 3 billion parame-\nters. Both models operate under the same CLIP-T5XXL\ncondition. The larger GenTron-G/2 model excels in ren-\ndering finer details and more accurate representations, par-\nticularly in rationalizing the layout of objects like cats and\nnewspapers. This results in image compositions that are\nboth more coherent and more realistic. In comparison, the\nsmaller GenTron-XL/2 model, while producing recogniz-\nable images with similar color schemes, falls short in terms\nof precision and visual appeal.\nFurthermore, the superiority of GenTron-G/2 is quan-\ntitatively\naffirmed\nthrough\nits\nperformance\non\nT2I-\nCompBench, as detailed in Table 2.\nThe increase in\n7\nFigure 6. GenTron-T2V results. Prompts are (left-to-right, top-to-bottom): \u201cTeddy bear walking down 5th Avenue front view beautiful sunset\u201d,\n\u201cA dog swimming\u201d, \u201cA giant tortoise is making its way across the beach\u201d, and \u201cA dolphin jumping out of the water\u201d.\nWin Rate\n51.1%\nLose Rate\n29.1%\nDraw Rate\n19.8%\nVisual Quality\nWin Rate\n42.3%\nLose Rate\n14.8%\nDraw Rate\n42.9%\nText Faithfulness\ne7a0f1 72ca9d 6aafec\nFigure 7. Visualization of the human preference of our method\nvs. Latent Diffusion XL. Our method received a significantly\nhigher number of votes as the winner in comparisons of visual\nquality and text faithfulness, with a total of 3000 answers.\nmodel size correlates with significant improvements across\nall evaluative criteria for object composition, including at-\ntributes and relationships. Additional comparative exam-\nples are provided in the appendix for further illustration.\nComparison to prior work.\nIn Table 3, we showcase the\nalignment evaluation results from T2I-CompBench.\nOur\nmethod demonstrates outstanding performance in all areas,\nincluding attribute binding, object relationships, and com-\nplex compositions. This indicates a heightened proficiency\nin compositional generation, with a notable strength in color\nbinding. In this aspect, our approach surpasses the previous\nstate-of-the-art (SoTA) benchmark set [9] by over 7%.\nUser study\nWe visualize the human preference of our\nmethod versus Latent Diffusion XL in Fig. 7.\nWe used\nstandard prompts in PartiPrompt2 [66] to generate 100 im-\nages using both methods and ask people for their preference\nblindly after shuffling. We received a total of three thou-\nsand responses on the comparisons of visual quality and text\nfaithfulness, with our method emerging as the clear winner\nby a significant margin.\n4.3. GenTron-T2V Results\nIn Figure 6, We showcase several samples generated by\nGenTron-T2V, which are not only visually striking but also\ntemporally coherent. This highlights GenTron-T2V\u2019s effec-\ntiveness in creating videos that are both aesthetically pleas-\ning and consistent over time.\nw/o MFG\nw/ MFG\nFigure 8. Effect of motion-free guidance. GenTron-T2V with\nmotion-free guidance has a clear visual appearance improvement.\nThe prompt is \u201cA lion standing on a surfboard in the ocean in sunset\u201d.\nEffect of motion-free guidance.\nIn Figure 8, we present\na comparison between our GenTron variants, with motion-\nfree guidance (MFG) and without. For this comparison,\ncritical factors such as the pre-trained T2I model, training\ndata, and the number of training iterations were kept con-\nstant to ensure a fair evaluation. The results clearly indi-\ncate that GenTron-T2V, when integrated with MFG, shows\na marked tendency to focus on the central object mentioned\nin the prompt, often rendering it in greater detail. Specifi-\ncally, the object typically occupies a more prominent, cen-\ntral position in the generated video, thereby dominating the\nvisual focus across video frames.\n5. Conclusion\nIn this work, we provide a thorough exploration of\ntransformer-based diffusion models for text-conditioned\nimage and video generation. Our findings shed light on\nthe properties of various conditioning approaches and offer\ncompelling evidence of quality improvement when scaling\nup the model. A notable contribution of our work is the\ndevelopment of GenTron for video generation, where we\nintroduce motion-free guidance. This innovative approach\nhas demonstrably enhanced the visual quality of generated\nvideos. We hope that our research will contribute to bridg-\ning the existing gap in applying transformers to diffusion\nmodels and their broader use in other domains.\n8\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1728\u20131738,\n2021. 5\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 4, 7, 12\n[3] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li,\nHang Su, and Jun Zhu. All are worth words: A vit backbone\nfor diffusion models. In CVPR, 2023. 3\n[4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\nYufei Guo, et al. Improving image generation with better\ncaptions, 2023. 6, 12\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 2, 3, 4, 5\n[6] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n5, 6\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 2\n[8] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, et al.\nVideocrafter1: Open\ndiffusion models for high-quality video generation. arXiv\npreprint arXiv:2310.19512, 2023. 2, 3, 5\n[9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze\nXie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,\nHuchuan Lu, et al.\nPixart-\u03b1: Fast training of diffusion\ntransformer for photorealistic text-to-image synthesis. arXiv\npreprint arXiv:2310.00426, 2023. 3, 6, 8, 12\n[10] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\nGrycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,\nGaurav Mishra, Linting Xue, Ashish V Thapliyal, James\nBradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,\nBurcu Karagol Ayan, Carlos Riquelme Ruiz, Andreas Peter\nSteiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. PaLI: A jointly-scaled multilingual language-\nimage model. In The Eleventh International Conference on\nLearning Representations, 2023. 2\n[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 2\n[12] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al.\nScaling\ninstruction-finetuned language models.\narXiv preprint\narXiv:2210.11416, 2022. 2, 4, 6, 12\n[13] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-\naofang Wang, Abhimanyu Dubey, et al.\nEmu: Enhanc-\ning image generation models using photogenic needles in a\nhaystack. arXiv preprint arXiv:2309.15807, 2023. 2, 4, 5,\n12\n[14] Yatin Dandi, Aniket Das, Soumye Singhal, Vinay Nam-\nboodiri, and Piyush Rai.\nJointly trained image and video\ngeneration using residual vectors.\nIn Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 3028\u20133042, 2020. 2, 5\n[15] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, et al. Scaling vision transformers to 22 billion pa-\nrameters. In International Conference on Machine Learning,\npages 7480\u20137512. PMLR, 2023. 2\n[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009. 7\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171\u20134186. Association for Computational Linguis-\ntics, 2019. 2\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 2, 3\n[19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 12873\u201312883, 2021. 3\n[20] Shanghua\nGao,\nPan\nZhou,\nMing-Ming\nCheng,\nand\nShuicheng Yan. Masked diffusion transformer is a strong\nimage synthesizer. arXiv preprint arXiv:2303.14389, 2023.\n3\n[21] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 4\n9\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 16000\u2013\n16009, 2022. 3\n[23] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 3\n[24] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 2, 5\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2, 3\n[26] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[27] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. The Journal of\nMachine Learning Research, 23(1):2249\u20132281, 2022. 4\n[28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J. Fleet. Video diffu-\nsion models, 2022. 2, 4, 5\n[29] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xi-\nhui Liu.\nT2i-compbench: A comprehensive benchmark\nfor open-world compositional text-to-image generation. In\nThirty-seventh Conference on Neural Information Process-\ning Systems Datasets and Benchmarks Track, 2023. 2, 6, 7\n[30] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. If you use this software, please cite it as below.\n12\n[31] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019. 4\n[32] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2\n[33] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3\n[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012\u201310022, 2021. 2\n[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6\n[36] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. Advances\nin Neural Information Processing Systems, 35:5775\u20135787,\n2022. 3\n[37] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu\nLu, Ping Luo, and Mingyu Ding.\nVdt:\nAn empirical\nstudy on video diffusion with transformers. arXiv preprint\narXiv:2305.13311, 2023. 3\n[38] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and\nTieniu Tan.\nVideofusion:\nDecomposed diffusion mod-\nels for high-quality video generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10209\u201310218, 2023. 2\n[39] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2, 12\n[40] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen. GLIDE: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In Proceedings of the 39th International Conference\non Machine Learning, pages 16784\u201316804. PMLR, 2022. 4,\n11\n[41] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. arXiv preprint arXiv:2212.09748, 2022.\n2, 3, 4, 6, 12\n[42] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-\nmoulin, and Aaron Courville. Film: Visual reasoning with a\ngeneral conditioning layer. In Proceedings of the AAAI con-\nference on artificial intelligence, 2018. 2, 3\n[43] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2, 4, 5, 6, 12\n[44] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 2\n[45] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. Improving language understanding by gen-\nerative pre-training. OpenAI blog, 2018. 2\n[46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 1(8):9, 2019. 2\n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 4, 6, 12\n[48] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 11, 12\n10\n[49] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim\nNeumann, Rodolphe Jenatton, Andr\u00b4e Susano Pinto, Daniel\nKeysers, and Neil Houlsby. Scaling vision with sparse mix-\nture of experts. Advances in Neural Information Processing\nSystems, 34:8583\u20138595, 2021. 2\n[50] Alex Rogozhnikov. Einops: Clear and reliable tensor manip-\nulations with einstein-like notation. In International Confer-\nence on Learning Representations, 2021. 5\n[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3, 5, 6\n[52] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 2, 3\n[53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2, 11, 12\n[54] Flavio Schneider, Zhijing Jin, and Bernhard Sch\u00a8olkopf.\nMo\\\u02c6 usai: Text-to-music generation with long-context la-\ntent diffusion. arXiv preprint arXiv:2301.11757, 2023. 2\n[55] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 5\n[56] Alex Shonenkov, Misha Konstantinov, Daria Bakshandaeva,\nChristoph Schuhmann, Ksenia Ivanova, and Nadiia Klokova.\nDeepfloyd if: A novel state-of-the-art open-source text-to-\nimage model. https://github.com/deep-floyd/\nIF, 2023. 12\n[57] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\nMake-a-video: Text-to-video generation without text-video\ndata. In The Eleventh International Conference on Learning\nRepresentations, 2023. 2\n[58] Mukul Singh, Jos\u00b4e Cambronero, Sumit Gulwani, Vu Le, Ca-\nrina Negreanu, and Gust Verbruggen. Codefusion: A pre-\ntrained diffusion model for code generation. arXiv preprint\narXiv:2310.17680, 2023. 2\n[59] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 3\n[60] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 2\n[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2, 4\n[62] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 3\n[63] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023. 2, 5\n[64] Yilin Wang, Zeyuan Chen, Liangjun Zhong, Zheng Ding,\nZhizhou Sha, and Zhuowen Tu.\nDolfin: Diffusion lay-\nout transformers without autoencoder.\narXiv preprint\narXiv:2310.16305, 2023. 3\n[65] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuo-\nfan Zong, Yu Liu, and Ping Luo. RAPHAEL: Text-to-image\ngeneration via large mixture of diffusion paths. In Thirty-\nseventh Conference on Neural Information Processing Sys-\ntems, 2023. 5\n[66] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and\nYonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation. Transactions on Machine Learn-\ning Research, 2022. Featured Certification. 7, 8, 12\n[67] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-\ncas Beyer. Scaling vision transformers. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 12104\u201312113, 2022. 2, 4\n[68] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-\nChin Huang, Min Xu, Less Wright, Hamid Shojanazeri,\nMyle Ott, Sam Shleifer, et al.\nPytorch fsdp:\nexperi-\nences on scaling fully sharded data parallel. arXiv preprint\narXiv:2304.11277, 2023. 6\n[69] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima\nAnandkumar. Fast training of diffusion models with masked\ntransformers. arXiv preprint arXiv:2306.09305, 2023. 3\n[70] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 3, 4\nA. Summary of Conditioning Mechanism\nIn Table 4, we present a summary of the conditioning ap-\nproaches utilized in existing text-to-image diffusion mod-\nels. Pioneering studies, such as [40, 48], have leveraged\nCLIP\u2019s language model to guide text-based image gener-\nation.\nFurthermore, Saharia et al. [53] found that large,\ngeneric language models, pretrained solely on text, are\nadept at encoding text for image generation purposes. Ad-\nditionally, more recently, there has been an emerging trend\ntowards combining different language models to achieve\n11\nMethod\nText Encoder\nIntegration\nsingle text encoder\nGLIDE [39]\nCLIP-B [47]\ncross-attention\nSDv1.4 / SDv1.5\nCLIP-L [47]\ncross-attention\nSDv2.0 / SDv2.1\nOpenCLIP-H [30]\ncross-attention\nDALL-E 2 [48]\nCLIP [47]\ncross-attention\nDALL-E 3 [4]\nT5-XXL [12]\ncross-attention\nImagen [53]\nT5-XXL [12]\ncross-attention\nDeepFloyd IF [56]\nT5-XXL\ncross-attention\nDiT [41]\nN/A\nadaLN\nPixArt-\u03b1 [9]\nT5-XXL [12]\ncross-attention\nmultiole text encoders\neDiff-I [2]\nCLIP-L & T5-XXL\ncross-attention\nSDXL [43]\nCLIP-L & OpenCLIP-bigG cross-attention\nEmu [13]\nCLIP-L & T5-XXL\ncross-attention\nTable 4. Summary of conditioning strategies used by existing im-\nage diffusion models.\nmore comprehensive guidance [2, 13, 43]. In this work,\nwe use the interleaved cross-attention method for scenar-\nios involving multiple text encoders, while reserving plain\ncross-attention for cases with a single text encoder. The\ninterleaved cross-attention technique is a specialized adap-\ntation of standard cross-attention, specifically engineered to\nfacilitate the integration of two distinct types of textual em-\nbeddings. This method upholds the fundamental structure\nof traditional cross-attention, yet distinctively alternates be-\ntween different text embeddings in a sequential order. For\nexample, in one transformer block, our approach might em-\nploy CLIP embeddings, and then in the subsequent block, it\nwould switch to using Flan-T5 embeddings.\nB. More Results\nB.1. Additional Model Scaling-up Examples\nWe present additional qualitative results of model scaling\nup in Figure 9. All prompts are from the PartPrompt [66].\nC. Additional GenTron-T2I Examples\nWe present more GenTron-T2I example in Figure 10.\n\u201ca smiling sloth\u201d\n\u201cA tiger is playing football\u201d\n\u201cA green heart\u201d\n\u201ca robot cooking\u201d\nGenTron-XL/2\nGenTron-G/2\nFigure 9. More examples of model scaling-up effects. Both\nmodels use the CLIP-T5XXL conditioning strategy. Captions are\nfrom PartiPrompt [66].\n12\na blue otter wearing a hat and dancing on \nthe beach\na cute cat running\na orange otter skateboarding with sunglasses\na cute happy Corgi playing in park, sunset, 4k\ntwo raccoons reading books \nin NYC Times Square.\na car moving slowly on an empty street, \nrainy evening, Van Gogh painting\na tiger in a field\na  blue unicorn flying in the sky\nturtle swimming in ocean\nsnow mountain and tree reflection in the lake\na beautiful scenery which leads to a \nend jumpscare\nclose up of grapes on a rotating table, \nhigh definition\nFigure 10. GenTron-T2I examples.\n13\n"
  },
  {
    "title": "Large Language Models for Mathematicians",
    "link": "https://arxiv.org/pdf/2312.04556.pdf",
    "upvote": "11",
    "text": "Large Language Models for Mathematicians\nSimon Frieder\u2217, Julius Berner\u2020, Philipp Petersen\u2021, Thomas Lukasiewicz\u00a7\nAbstract\nLarge language models (LLMs) such as ChatGPT have received immense interest for their general-\npurpose language understanding and, in particular, their ability to generate high-quality text or\ncomputer code. For many professions, LLMs represent an invaluable tool that can speed up and\nimprove the quality of work. In this note, we discuss to what extent they can aid professional\nmathematicians. We first provide a mathematical description of the transformer model used in all\nmodern language models. Based on recent studies, we then outline best practices and potential issues\nand report on the mathematical abilities of language models. Finally, we shed light on the potential\nof LMMs to change how mathematicians work.\n1\nIntroduction\nLarge language models (LLMs) recently revolutionized the field of natural language processing, bringing\nforth unparalleled progress for applications such as conversational bots or text generation. Advances in\ndeep-learning algorithms, computational resources, as well as substantial engineering efforts, allowed to\ntrain language models with billions of parameters and on massive datasets, such as the 800GB large The\nPile [14]. Being trained on increasingly larger datasets, language models infer essential aspects of the\nstructure of language1, paving the way from task-specific to general-purpose language understanding. As\nsuch, these models can also parse mathematical questions and solve (sometimes surprisingly complex)\nmathematical problems and tasks\u2014however, they also make embarrassing mistakes.\nLet us have a look at two examples that ChatGPT (web version of 30-January-2023) produced2. Both\nexamples below were taken from [13]; the input to the model, as well as its output, consists of LATEX code,\nwhich we have compiled below for better readability:\n\"prompt\" :\n\"Find a\nfunction\nf : R \u2192 R that\ni s\ncontinuous\nat\np r e c i s e l y\none\npoint . \" ,\n\" output \" :\n\"One example\nof\nsuch a\nfunction\ni s\nf(x) =\n\u001a\nx2\nif x \u0338= 0\n1\nif x = 0.\nThis\nfunction\ni s\ncontinuous\nat x = 0 but\nnot\nat\nany\nother\npoint .\"\n\"prompt\" :\n\"Let \u00b5 be a measure ,\n\u2225f\u2225\u221e = inf{M : \u00b5({x : |f(x)| > M}) = 0} and \u2225g\u22251 = R\n|g|d\u00b5 .\nProve\nthat R\n|fg|d\u00b5 \u2264 \u2225g\u22251\u2225f\u2225\u221e . \" ,\n\" output \" :\n\"Let M = \u2225f\u2225\u221e and \u03f5 > 0 .\nThen ,\nby the\nd e f i n i t i o n\nof\n\u2225f\u2225\u221e , we have\n\u00b5({x : |f(x)| > M + \u03f5}) = 0 .\nTherefore ,\nZ\n|fg|d\u00b5 =\nZ\n|f|\u2264M+\u03f5\n|fg|d\u00b5 +\nZ\n|f|>M+\u03f5\n|fg|d\u00b5 \u2264 (M + \u03f5)\nZ\n|g|d\u00b5 +\nZ\n|f|>M+\u03f5\n|f||g|d\u00b5.\nThe second\nterm on the\nr i g h t\ns i d e\ni s\nzero\nbecause \u00b5(x : |f(x)| > M + \u03f5) = 0 .\nSo we\nhave R\n|fg|d\u00b5 \u2264 (M + \u03f5)R\n|g|d\u00b5 = (M + \u03f5)\u2225g\u22251 .\nAs \u03f5 can be\na r b i t r a r i l y\nsmall ,\nwe can\nl e t\ni t\ngo\nto\nzero\nand we get R\n|fg|d\u00b5 \u2264 M\u2225g\u22251 = \u2225f\u2225\u221e\u2225g\u22251 .\nHence proved .\"\n\u2217Department of Computer Science, University of Oxford, Oxford, UK\n\u2020Department of Computing and Mathematical Sciences, Caltech, Pasadena, US\n\u2021Faculty of Mathematics and Research Network Data Science, University of Vienna, Vienna, Austria\n\u00a7Institute of Logic and Computation, Vienna University of Technology, Vienna, Austria, and Department of Computer\nScience, University of Oxford, Oxford, UK\n1We note that language models work best in English since the most extensive datasets, including those that contain\nmathematics, are available in English.\n2The first example can by now be correctly answered by the GPT-4 model [26], which was since released. We explain\nthe acronym \u201cGPT\u201d in Section 2.\n1\narXiv:2312.04556v1  [cs.CL]  7 Dec 2023\nThe first response is nonsensical on all levels, whereas the second is correct and helpful. With these\nexamples in mind, we might ask ourselves:\nHow can large language models assist human mathematicians in their work?\nTo address this question, this article proceeds in the following way: First, we provide an overview of modern\nlanguage models. We clarify the theory of how language models work, how their main building block\u2014the\ntransformer architecture\u2014is set up, and why these models can perform sufficient mathematics to assist\nmathematicians in their daily work. Understanding this architecture will also highlight how an LLM\nproduces an answer to a mathematical question, which differs tremendously from how a mathematician\ndoes this. Then, we present empirical evidence attesting to the abilities of language models, in particular\nstate-of-the-art models, such as ChatGPT and GPT-4. We end with an outlook on the potential future\nimpacts on mathematicians and mathematics in general.\n2\nOverview of Modern Language Models\nThe concept of language models has a long history. One of the pioneering achievements, dating back to\nthe year 2000, presented one of the initial instances of what we now refer to as word embeddings within\nthe framework of neural networks [3]; see Section 3 for a definition.\nMost previous approaches were rooted in estimating probabilities over trigrams (or, more general, n-grams).\nAn n-gram is a sequence of n adjacent elements from a string of word pieces, so-called tokens, which could\nbe syllables, letters, words, or base pairs according to the context. In the sentence \u201cThe quick brown\nfox jumps over the lazy dog\u201d, the sequence \u201cquick brown fox\u201d is an example of a trigram. Models\nbased on n-grams had severe limitations: For example, if a trigram does not appear in the training corpus\n(or contains words that were not in the vocabulary of the corpus), no meaningful way of estimating its\nprobability existed. By using a form of word embeddings, these problems are circumvented. The model\nproposed by [3] dominated all other pure n-gram models. The authors note that improvements can be\nmade regarding the \u201carchitecture, computational efficiency, and taking advantage of prior knowledge\u201d.\nThe introduction of the transformer architecture [40] in 2017 marked the most striking advancement in\nterms of neural network architectures: On the one hand, the attention mechanism modeled the structure\nof the language more faithfully; on the other hand, it was an architecture that was easily parallelizable on\nmodern hardware (see Section 3 for details). This led to a series of further milestones and improvements:\nIn 2018, the Bidirectional Encoder Representations from Transformers (BERT) model [7] was introduced,\na successor to the original transformer, which inspired a vast number of successors on its own, such\nas RoBERTa [22], or DistilBERT [32]. BERT (and its successors) were notable because classical\npipelines (e.g., defining text representations, carrying out parts-of-speech tagging) were all subsumed by\nBERT-type models [36], which could easily be fine-tuned to specific tasks. At roughly the same time as\nthe BERT model, the Generative Pre-Trained Transformer (GPT) model was introduced by OpenAI [28].\nThis was a further variation on the original transformer architecture and is the first version of the model\nthat underlies ChatGPT, which was released in 2022 [25], and is closely related to InstructGPT [27].\nThe last milestone consists of the LLaMA [38] and LLaMA2 models [39] introduced in 2023, months\nafter GPT-4 [26]. Their importance lies in being the first publicly released models, the code and weights\nof which were easily accessible and rivaled the performance of GPT-4; in the technical report associated\nwith GPT-4 it is stated: \u201cthis report contains no further details about the architecture (including model\nsize), hardware, training compute, dataset construction, training method, or similar\u201d. The LLaMA\nmodels led to a democratization of language models and to a large number of further successors, such\nas Stanford\u2019s Alpaca3 model, or the Vicuna4 model, which have since been used in a wide array of\ncontexts. As these models evolved, the number of their parameters, as well as the sizes of the dataset on\nwhich they were trained, kept increasing, from the order of millions of parameters (in case of [3, 40, 7]),\nto billions [39, 38], to trillions [8, 30], see Figure 1. While the main trend indicates increasing model sizes,\nthere is a countertrend to make the models smaller while retaining the performance. The DistilBERT\nmodel is an example of this. Scaling the size of the architectures and the amount of training data enabled\nunprecedented capabilities for the resulting LLMs, eliminating the need for fine-tuning for specific tasks.\n3https://github.com/tatsu-lab/stanford_alpaca\n4https://lmsys.org/blog/2023-03-30-vicuna/\n2\nFigure 1: A selection of representative, modern language models is presented along with their parameter\ncounts (in billions), which are displayed above. The y-axis is a log-axis, and model ranges are displayed\n(two horizontal dots), where available, for each model. We observe that a wide range of parameters\nappears, between 28 million and 1.2 trillion. For ChatGPT, exact parameter counts are not available\nbut are taken from InstructGPT, which is a sibling model in which ChatGPT is based. For GPT-4,\nparameter counts are not available.\n3\nTechnical Background\nIn the following, we seek to give a brief introduction to the inner workings of LLMs. We refer to [44, 23] for\nsurveys and further details. We do not strive to present state-of-the-art models and techniques in natural\nlanguage processing but focus on a conceptual understanding of the functionality of LLMs. In particular,\nwe will restrict the presentation to one of the most popular architectures, the so-called transformer [40].\nOur description below is a simplified, mathematical summary loosely based on the (open-source) code5 of\nGPT-2 [29], see also Figure 2 for an overview.\n3.1\nTransformer Architecture\nLet us explore how a transformer architecture predicts text based on some provided input, often referred\nto as prompt. In line with successful models, such as the GPT and LLaMA series, we focus on the\nsetting, where the model iteratively predicts the next word pieces (i.e., tokens) based on a given sequence\nof tokens. This procedure is coined autoregressive since the prediction of new tokens is only based on\nprevious tokens. Such conditional sequence generation tasks using autoregressive transformers are often\nreferred to as decoder-only settings. Although there is a maximum context length in practice, we will\nwork with sequences of arbitrary length for ease of presentation. We define the shorthand notation\nS\u2217 := S\nn\u2208N Sn for a set S to denote the set of sequences s = (s(i))n\ni=1 \u2282 S with arbitrary length n \u2208 N.\nFor a function F : S1 \u2192 S2, we denote by F\u2217 : S\u2217\n1 \u2192 S\u2217\n2 the entrywise applied mapping given by\nF\u2217(s) := (F(s(i)))n\ni=1.\n(1)\nTokenization K.\nFirst, we want to clarify how we define word pieces, i.e., tokens. Mathematically\nspeaking, we seek an injective mapping K: A\u2217 \u2192 T \u2217 from the given text, i.e., a sequence a = (a(i))N\ni=1 of\ncharacters in an alphabet A to a sequence of n \u2264 N tokens (t(i))n\ni=1, where typically T := {1, 2, . . . , M}.\n5https://github.com/openai/gpt-2\n3\n \n \nTokenization \nEmbedding \n + Positional encoding \nPrediction head \nTransformer layers \nSampling \n Prove that pi\nProve that pi is\n  \n (Pro)\n  \n (ve)\n  \n ( that)\n  \n ( pi)\n  \n \n ( is)   \n ('s)   \n ( has)   \n \nNormalization\nSelf-Attention \nNormalization\nMultiplayer Perceptron \nSkip connection\nSkip connection\nSkip connection\nFigure 2: Illustration of the operations of an LLM for the input text \u201cProve that pi\u201d. The token indices,\nas well as the probabilities for the next token, are taken from GPT-2 [29] using the implementation in the\ntransformers library [41]. The highest probability for the next token is assigned to 318 corresponding\nto the word \u201cis\u201d.\nTo represent text on a computer, we could encode every character a(i) individually, similar to Unicode.\nWhile this would lead to a small vocabulary T of tokens, it yields long sequences where individual tokens\ndo not capture any linguistic information. For LLMs, one often employs subword tokenization [34], which\ncreates a vocabulary of subwords by analyzing large text corpora and iteratively merging frequently\noccurring sequences of characters. Akin to a compression problem, one balances the length n of the\nsequences and the size6 M of the vocabulary. As an example, the GPT-4 tokenizer7 splits the word\n\u201cdiscontinuity\u201d into the subwords \u201cdis\u201d (prefix), \u201ccontin\u201d (subword capturing the root of \u201ccontinuous\u201d),\nand \u201cuity\u201d (suffix). Another example can be found in Figure 2.\nEmbedding E.\nTo use these tokens (given by the indices of the subwords in the vocabulary) in a\nneural network, we embed each token t(i) into the same Euclidean space E := Rd. Intuitively, we seek a\nmap E : T \u2192 E, such that the distance \u2225E(t(i)) \u2212 E(t(j))\u2225 corresponds to the linguistic similarity of the\nsubwords represented by the tokens t(i) and t(j). In practice, such an embedding is often initialized with\na sequence of M random initial embeddings and learned jointly with the transformer model from data.\nPositional Encoding P.\nSince E operates on each token t(i) independently, the embeddings E(t(i)) do\nnot contain information on the position i of the (sub)words within a sentence8. Thus, one typically adds\nso-called positional encodings, which can be described by a mapping P : E\u2217 \u2192 E\u2217. A commonly used\nchoice is of the form\nP((e(i))n\ni=1) := (e(i) + p(i))n\ni=1,\n(2)\nwhere p: N \u2192 E can be a prescribed injective function, e.g., a sinusoid [40], or learned (similar to the\nembedding E) [28].\n6The LLaMA and LLaMA2 models employ vocabularies T with M = 32000 tokens [39]. GPT-2 uses M = 50257, and\nother models in the GPT series, e.g., GPT-3.5-turbo and GPT-4, even use M = 100277 tokens, see https://github.com/\nopenai/tiktoken.\n7See https://platform.openai.com/tokenizer.\n8In some settings, where the transformer architecture is permutation invariant, positional encodings are strictly necessary.\nIn our decoder-only setting, this is not the case [15]; however, the encodings still seem to improve performance\n4\nIn summary, tokenization K, followed by an application of E to each token and the positional encoding P,\nmaps the text a \u2208 A\u2217 to a sequence of embeddings\ne := (P \u25e6 E\u2217 \u25e6 K) (a) \u2208 E\u2217.\n(3)\nwhere the length of e depends on a and the tokenization algorithm.\nTransformer T .\nThe transformer can be represented as a neural network T : E\u2217 \u2192 E\u2217. It is trained to\nmap a sequence of embeddings e to another sequence of the same length containing contextual information.\nBased on the desired autoregressive structure, where the prediction of the next token only depends on\nthe previous tokens, we want the i-th element of T (e) to contain information about all the embeddings\n(e(j))j\u2264i, however, to be independent of (e(j))j>i.\nThe transformer is typically defined by a composition of L \u2208 N blocks, consisting of self-attention maps\nA\u2113, entrywise applied normalizing layer NA,\u2113, NM,\u2113, and feed-forward multiplayer perceptrons M\u2113, i.e.,\nT :=\n\u0000\u0000Id + M\u2217\nL \u25e6 N \u2217\nM,L\n\u0001\n\u25e6\n\u0000Id + AL \u25e6 N \u2217\nA,L\n\u0001\u0001\n\u25e6 \u00b7 \u00b7 \u00b7 \u25e6\n\u0000\u0000Id + M\u2217\n1 \u25e6 N \u2217\nM,1\n\u0001\n\u25e6\n\u0000Id + A1 \u25e6 N \u2217\nA,1\n\u0001\u0001\n.\n(4)\nIn the above, Id denotes the identity mapping, commonly known as a skip or residual connection, and\nthe addition is understood entrywise. The indices of the layers N, M, and A in (4) indicate the use of\ndifferent trainable parameters in each of the layers. Let us describe these layers in more detail below.\nLayers: Normalization N.\nThe normalizing layer can be interpreted as a re-parametrization with\na learnable mean and standard deviation to stabilize training. For instance, using layer normalization\nN : E \u2192 E, we compute\nN(e) = diag(s)\n\u03c3\n(e \u2212 \u00b5) + m,\n(5)\nwhere \u00b5 = 1\nd\nPd\ni=1 ei and \u03c32 = 1\nd\nPd\ni=1(ei \u2212 \u00b5)2 are the mean and variance of e \u2208 E, and s, m \u2208 E are\nlearnable parameters [1].\nLayers: Multilayer Perceptrons N.\nThe Multilayer perception (MLP) is a standard feed-forward\nneural network consisting of compositions of affine mappings and nonlinear activation functions. Let us\ndefine by L(m,n) : Rm \u2192 Rn an affine mapping L(m,n)(x) := Wx + b, where the weight matrix W \u2208 Rn\u00d7m\nand the bias vector b \u2208 Rm are learnable. Moreover, let \u03f1: R \u2192 R be an activation function, e.g., the\nGELU activation function \u03f1(x) := x \u03a6(x), where \u03a6 is the standard Gaussian cumulative distribution\nfunction [17]. A typical MLP M: E \u2192 E used in transformers is then given by\nM := L(d,D) \u25e6 \u03f1\u2217 \u25e6 L(D,d),\n(6)\nwhere D \u2208 N with D \u2265 d.\nLayers: Self-Attention A.\nAs can be seen in (4) and Figure 2, the self-attention layer A: E\u2217 \u2192 E\u2217\nis the only layer that combines embeddings of different tokens; in other words, it attends to other tokens.\nLet us denote the input to the layer by (e(i))n\ni=1 and focus on the i-th output. We first compute the\n(normalized) inner products\ns(i)\nj\n=\n1\n\u221a\nk\nD\nL(k,d)\nquery(e(i)), L(k,d)\nkey (e(j))\nE\n,\nj = 1, . . . , i,\n(7)\nwith given k \u2208 N. On a high level, we can interpret s(i) = (s(i)\nj )i\nj=1 \u2282 R as similarities between the\nembedding L(k,d)\nquery(e(i)) of the i-th token (i.e., the so-called query) and the embeddings L(k,d)\nkey (e(j)) of the\nother tokens (i.e., keys); to satisfy the autoregressive structure, we only consider j \u2264 i. To normalize s(i)\nto probabilities, we can further use a softmax layer softmax: R\u2217 \u2192 R\u2217 given by\nsoftmax(s(i))j :=\nexp\n\u0010\ns(i)\nj\n\u0011\nPi\nk=1 exp\n\u0010\ns(i)\nk\n\u0011,\nj = 1, . . . , i.\n(8)\n5\nWe can now interpret softmax(s(i))j as the probability for the i-th query to \u201cattend\u201d to the j-th key.\nThe self-attention layer A can then be defined as\nA(e)i := L(k,d)\n\uf8eb\n\uf8ed\ni\nX\nj=1\nsoftmax(s(i))jL(k,d)\nvalue(e(j))\n\uf8f6\n\uf8f8 ,\ni = 1, . . . , n,\n(9)\nwhere the outputs of L(k,d)\nvalue are often referred to as the values of the token embeddings e(j), and where\nthe learnable affine layer L(k,d) maps the weighted average of values back to E = Rd.\nNote that in practice, one typically considers a sum of h \u2208 N such attention layers (so-called heads), each\nwith dimension k = d/h [40, 21]. Moreover, instead of considering vectors of variable length i, a mask\nenforces the autoregressive structure so that all operations can be efficiently batched.\nPrediction Head H.\nThe prediction head or un-embedding layer can be represented as a mapping\nH: E\u2217 \u2192 \u2206M, where\n\u2206M :=\n(\nP \u2208 [0, 1]M :\nM\nX\ni=1\nPi = 1\n)\n(10)\ndenotes the probability simplex in RM. It maps the sequence of transformed embeddings (\u02dce(i))n\ni=1 := T (e)\nto a vector P \u2208 \u2206M, where Pi describes the probability of predicting i \u2208 T as the next token. Since the\ntransformed embedding of the last token, i.e., \u02dce(n), contains information about the whole input text, a\nsimple approach is to use a linear mapping composed with a softmax layer and define\nP := (softmax \u25e6 L(M,d))(\u02dce(n)).\n(11)\nSampling S.\nThere are multiple sampling strategies S : \u2206M \u2192 T to arrive at the final prediction for\nthe next token t(n+1), see, e.g., [18]; the arguably simplest one, so-called greedy sampling, predicts the\ntoken with the highest probability, i.e.,\nt(n+1) = S(P) := arg max\ni=1,...,M\nPi,\n(12)\nsee Figure 2. One can then apply the same operations to the extended sequence t = (t(i))n+1\ni=1 , i.e.,\nt(n+2) := (S \u25e6 H \u25e6 T \u25e6 P \u25e6 E\u2217) (t)\n(13)\nto iteratively compute further tokens9. Due to the autoregressive structure, this can efficiently be done\nby caching the previous (intermediate) results and only considering the computations for the new token.\n3.2\nTraining\nDuring training, we transform text corpora into sequences of tokens, such that, for a given sequence\n(ti)n\ni=1, we already know the next token tn+1 based on the underlying text. One can thus compute the\ndeviation D between the predicted probabilities P of the next token and the ground-truth tn+1, typically\nusing a cross-entropy loss; in practice, this procedure can be parallelized to compute average losses\nacross many predictions. Using automatic-differentiation, one then computes the derivative \u2207\u03b8D of the\naverage loss D with respect to the learnable parameters \u03b8 \u2208 Rp of the transformer T , the embedding\nE, the prediction head H (and the positional encoding P if it is trainable). Updating the parameter\nby subtracting a sufficiently small multiple \u03bb \u2208 (0, \u221e) of the derivative, i.e., \u03b8k+1 = \u03b8k \u2212 \u03bb\u2207\u03b8D, one\ncan iteratively minimize the loss\u2014a method known as stochastic gradient descent. This is the essential\nmechanism by which word occurrence probabilities are estimated by training from raw data. With\nsubstantial engineering efforts, more elaborate versions of such training schemes can be parallelized on\nlarge GPU clusters and scaled to immense amounts of data. To get an idea of the dimensions, the largest\nLLaMA2 model with p = 70 \u00b7 109 parameters was trained for more than 1.7 million GPU hours on about\n2 trillion tokens of data from publicly available sources [39].\n9There is usually a stopping criterion based, e.g., on a special token or the entropy of P.\n6\n3.3\nTraining Costs and Emissions\nTraining LLMs, as described in the previous section, is a computationally very intensive process and,\ntherefore, costly to carry out in terms of electricity usage (assuming all the hardware would be in place).\nHowever, information about training costs and CO2 emissions is not consistently provided in the literature.\nNotable exceptions include the LaMDA model. The authors [37] report that a total of 451MWh was\nconsumed during training, and, as a result, approximately 26 tons of CO2 were emitted. Using historic\nUS prices10 of 0.148 dollars per kWh, this amounts to a cost of 66, 748 dollars. We note that costs may\nvary by country and by the energy source used to produce energy [35]. The GLaM model consumes,\nwhen trained on the largest dataset, similarly 456MWh and emits 40.2 tons of CO2, which places it thus\nin a similar category to the LaMDA model in terms of cost and emission.\nHowever, more modern LLMs incur significantly more energy consumption and emissions. For instance,\nthe training of LLaMA2 (using 1.7 million hours on GPUs with a power consumption of about 400W)\nemitted more than 291 tons of Carbon dioxide equivalent (CO2-eq) [39]. LLM vendors (such as OpenAI)\ntypically do not release information about the costs (either in terms of consumed megawatt-hours or\n(rented) GPU-hours) of training their models, so only vague estimates are possible, which are nonetheless\nstaggering. For example, training the older-generation GPT-3 model [4] was estimated, using GPU-hour\nprices from that time, to run up costs of approximately 4.6 million dollars [20].\n4\nLLMs for Mathematics\nWith the foundations of LLMs now well-established, we turn our attention to their application in supporting\nprofessional mathematicians. While mathematicians engage in a broad spectrum of mathematical activities,\nsuch as performing simulations, modeling, and computation, we focus on the arguably most important\ntask: the capacity of LLMs to generate mathematical proofs.\nWhen using an LLM to assist in the task of theorem proving, the simplest way is to directly prompt\nthe model to prove the statement instead of using it for individual steps or other tasks that will be\ndescribed below. However, many issues have been found with this approach. A primary concern is that\nmathematical arguments hinge on the precision of logic; a single wrong statement very likely invalidates\nthe entire proof. Assuming that LLMs have a non-negligible, independent probability of error with each\npredicted word, the likelihood of producing a correct proof diminishes exponentially with increasing text\nlength. This was also empirically observed in [16], where an LLM turned out to have a higher accuracy in\nsolving computation tasks if it was asked to skip intermediate steps.\nThe autoregressive nature of LLMs introduces another critical issue. Once a statement is made, LLMs\ntypically do not revisit or revise their arguments. This process diverges significantly from the methodologies\nof most mathematicians. Rarely does a mathematician draft a complete and detailed proof in a single\nattempt. Instead, the process often involves crafting a rough sketch, omitting small steps in which we\nhave confidence, iterating, and refining until the proof reaches completion.\nFurthermore, LLMs may construct entirely valid proofs for questions different from those posed. Such\nan instance was exemplified in the introduction when the LLM was prompted for a function that is\ncontinuous at only one point. Given the prevalence of similar but distinct problems in training datasets,\nLLMs are likely to respond to the more commonly encountered variations of a question. In this case,\na question for a function that is discontinuous at only one point. Similarly, they may prove a theorem\nunder stronger assumptions than stated without making this explicit.\nFinally, being based solely on statistical relations of language, LLMs struggle considerably with arithmetic\nproblems: These often occur when an LLM has to complete a task, such as carrying out addition or\nmultiplication (in particular, if the involved numbers are large). The reason for this is that no numerical\nsolver is built into LLMs. Steps towards overcoming this have recently been made by employing a\nToolformer approach [33]. An instance of this is the WolframAlpha plugin that is available for GPT-4.\nIn summary, when LLMs are used for theorem proving, they are susceptible to a range of errors. These\nerrors were examined in [13], to be discussed in the following chapter. Consequently, a more collaborative\napproach, incorporating human expertise, is advisable. The following strategies appear to be sensible:\n10https://data.bls.gov/timeseries/APU000072610\n7\n\u2022 Literature/search engine: The LLM can be prompted to explain a definition, find the established\nname for a vaguely described concept, or find references for a certain statement. In this context, two\ncrucial considerations arise. First, LLMs are known for generating plausible yet fictitious content. This\nphenomenon is often referred to as hallucinations. Therefore, its answer to our queries needs to be\nverified. Second, the LLM may exacerbate biases in research. This can occur when an LLM overlooks\ninadequately cited work, effectively burying it while disproportionately recommending over-hyped\narticles.\n\u2022 Brainstorming/Idea Generation: An LLM can be asked to provide a high-level idea of how to prove\na theorem. While this will not produce the full result, it closely resembles the style of a mathematician.\nThere is, however, no guarantee that this idea will be very insightful or lead to something. Being trained\non a large corpus of mathematical arguments, an LLM will likely be biased towards recommending\nthe most standard ideas. This may not be helpful for a mathematician who is already an expert in a\nspecific field. However, it could be very valuable for a mathematician trying to enter a new area.\n\u2022 Proof-checking: An LLM can be asked to find mistakes in a given proof. While there is no guarantee\nwhatsoever that it will find all errors, the ones it finds can often be immediately confirmed as actual\nmistakes by a mathematician. This is helpful but not reliable. The LLM will likely focus on syntactical\ncorrectness over semantic correctness and hence overlook complex errors.\n\u2022 Collaborative writing: An LLM can be asked to provide parts or a sketch of a proof and then, after\ntaking feedback from the user, improve parts, repair errors, and add more details. In [5], the interactive\nperformance of three LLMs (InstructGPT, ChatGPT, and GPT-4) on mathematical queries has\nbeen measured on a cohort of users. It was attempted to solve mathematical problems by using these\nmodels as an assistant. Asking definitions, general mathematical questions (not strictly related to the\nproblem), and proof steps were the three most common use cases. It is important to keep in mind\nthat this approach still is susceptible to introducing errors. The study found that self-assessment of\nindividuals\u2014whether they had correctly solved the problem using an LLM\u2014was not always correct.\nThe approaches above are sensible for the successful employment of modern multi-purpose LLMs. In\naddition, we anticipate that LLMs specifically designed to prove theorems will be developed in the future.\nOne avenue to achieve this is by combining LLM-generated proofs with interactive theorem provers [6, 2].\nFirst steps in this direction have already been taken and appear to be very promising [12, 11, 42, 43].\n5\nMeasuring LLM Performance on Mathematics\nIn [13], an empirical study was carried out to study the mathematical reasoning abilities of three LLMs\nwhich were considered to be state-of-the-art in terms of general performance: Two ChatGPT versions\n(9-January-2023 and 30-January-2023) and GPT-4. The prompts used to carry out the evaluation\ncorrespond to some of the use cases discussed in Section 4.\n(PP) Producing proofs: Exercises from well-known textbooks (Probability Theory by R. Durret [9],\nTopology by J. R. Munkres [24], and Functional Analysis by W. Rudin [31]) were fed to the LLMs.\n(FH) Filling holes: Proofs with a gap were given to the LLMs, and they were asked to fill the gap.\n(SE) Acting as a mathematical search engine: The LLMs were asked to give a definition of concepts such\nas: \u201cWhat is a Banach space?\u201d. In addition, they were asked to provide the name of definitions, as\nin \u201cHow is a complete normed vector space called?\u201d. Finally, they were prompted to provide proof\nideas used in famous theorems.\n(CO) Computation: The LLMs were given mathematical tasks in which quantities had to be computed.\nTo carry out this analysis, the GHOSTS dataset was introduced; see Table 1 for a more detailed description\nof its subdatasets (which make up the acronym \u201cGHOSTS\u201d), as well as how they correspond to the use\ncases listed above. It consists of 709 prompts, each of which was given to the three considered models.\nThe responses of the LLMs were rated by professional mathematicians11 on a scale between one (failure\nto understand the query) and five (perfect or almost perfect output). The obtained ratings are shown in\nFigure 3. We can make the following observations:\n11The authors of the present paper form a subset of the evaluators.\n8\nTable 1: A summary of all the files from all the subdatasets comprising the GHOSTS dataset, together\nwith their size, i.e., the number of prompts and their associated attribute tags.\nSubdataset Name\nSize\nType\nGrad-Text\n130\nPP\nHoles-in-Proofs\n162\nFH\nOlympiad-Problem-Solving\n101\nPP\nSymbolic-Integration\n100\nCO\nMATH\n138\nCO\nSearch-Engine-Aspects\n78\nSE\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nRating\nGrad-Text\nW. Rudin, Functional Analysis (ch. 1)\nW. Rudin, Functional Analysis (ch. 2)\nJ. Munkres, Topology (ch. 1)\nJ. Munkres, Topology (ch. 2)\nR. Durret, Probability Theory\nHoles-in-Proofs\nProofs Collection A\nProofs Collection B Prealgebra\nProofs Collection B Precalculus\nOlympiad-Problem-Solving\nSymbolic-Integration\nMATH\nMATH Algebra\nMATH Counting and Probability\nMATH Prealgebra\nMATH Precalculus\nSearch-Engine-Aspects\nDefinition Retrieval\nReverse Definition Retrieval\nNamed Theorem Proof Completion\nSubdataset\nVersion\n9-Jan.\n30-Jan.\nGPT-4\nFigure 3: Average rating for each file in each subdataset (bold) of GHOSTS for the studied versions of\nChatGPT and GPT-4. The maximal ranking is 5 and the minimal ranking, where the question was at\nleast understood, is 2; the rating of 1 indicates that the answer completely misses the question. Thus, a\nreasonable passing grade, i.e., 50% of points, corresponds to a score of 3.5, indicated by the dotted line.\nThe error bars represent 95% confidence intervals.\n9\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n9-Jan.  \n30-Jan.\nGPT-4  \nFigure 4: A Sankey diagram of how the ratings evolve from the 9-January-2023 version of ChatGPT to\nthe 30-January-2023 version ChatGPT, and subsequently to GPT-4 (from top to bottom), with all\nmodels evaluated on a representative subset of GHOSTS. Each score is color-coded, and the line widths\nare proportional to the number of ratings.\n\u2022The LLMs work well as a search engine: ChatGPT and GPT-4 achieved an excellent score when we\nasked for definitions of a concept or the name of a theorem or definition.\n\u2022 ChatGPT and GPT-4 struggle with hard questions: No version of the tested LLMs achieved satisfactory\nresults on the hardest problem set\u2014Olympiad-Problem-Solving. Similarly, on the functional analysis\nquestions from Rudin (Chapter 2)\u2014arguably the second most advanced set of questions in the dataset\u2014\nthe results were underwhelming.\nThe ratings were substantially better for more straightforward\nquestions, such as the exercises in topology, which only ask for simple set theory and Boolean logic.\n\u2022Good results for simple computations: Despite not having a built-in numerical solver, GPT-4 performed\nreasonably well on questions requiring simple computation. For more sophisticated computation\ninvolved in Symbolic-Integration, ChatGPT failed and GPT-4 barely achieved a passing grade.\n\u2022User input can have a positive effect: On the Holes-in-Proofs subdataset, we see excellent results in\nsome of the problems. It appears that the additional context given by the user helps the LLMs to\nproduce more truthful solutions. A similar observation was made in a separate experiment where\ncarefully crafted prompts (so-called prompt engineering) slightly increased the score of the LLM on the\nOlympiad-Problem-Solving subdataset.\nThe improvement in rating, as the models become more sophisticated, is indicated in the Sankey diagram\nin Figure 4, which shows how ratings change from one model version to another. We use a representative\nsubset of GHOSTS to advise the Sankey diagram. We observe that between the 9-January-2023 version\nand the 30-January-2023 version, scores are approximately shuffled, and no substantial increase of the net\nscore occurs. For the newer generation, i.e., GPT-4, we can observe a significant improvement in the\nratings. This supports the general trend that more modern models also perform better on challenging\nmathematical reasoning tasks.\nIn [13], a subdivision of each question type (as indicated in Figure 3) was made, and a much more\nfine-grained benchmark was introduced that also differentiates potential failure modes. We refer the\nreader to [13] for further information on more detailed analyses.\n6\nConclusion\nOur study has highlighted how LLMs possess a remarkable capacity to assist mathematicians in various\nways, from detecting and filling gaps in theorems to acting as search engines and finding definitions from\ndescriptions of mathematical objects. We have shed light on the inner mechanism of the core piece of\narchitecture that powers modern LLMs, the transformer, and how the way they produce an answer to\nmathematical questions differs starkly from human reasoning.\n10\nThe ability of LLMs to interact with users in a natural language format has made mathematics more\naccessible, allowing for a broader range of individuals to engage with mathematical research and education.\nWhile the full potential of LLMs in automating mathematics is yet to be realized, our findings suggest a\npromising synergy between human mathematicians and artificial intelligence. High exposure of the work\nof mathematicians to the effects of LLMs has also been reported in [10]. However, we want to caution\nthat, currently, LLMs are not on a trajectory to replace mathematicians. In [13], it was shown that\neven the best-performing model has trouble with mathematics on upper-undergraduate difficulties, such\nas when it is tasked to solve exercises from W. Rudin\u2019s Functional Analysis [31]. The performance of\nLLMs has been reported to be below that of humans also in related domains, such as coding challenges in\ncomputer science [19]. Nonetheless, we anticipate that the emergence of LLMs will be a challenge for\neducation and research. Simple exercises or homework and individual steps in mathematical research will\nbe gradually supported by automation or become obsolete.\nAcknowledgements\nS. Frieder and T. Lukasiewicz were partially supported by the AXA Research Fund.\nReferences\n[1] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n[2] B. Barras, S. Boutin, C. Cornes, J. Courant, J.-C. Filliatre, E. Gimenez, H. Herbelin, G. Huet,\nC. Munoz, C. Murthy, et al. The Coq proof assistant reference manual: Version 6.1. PhD thesis,\nInria, 1997.\n[3] Y. Bengio, R. Ducharme, and P. Vincent. A neural probabilistic language model. Advances in Neural\nInformation Processing Systems, 13, 2000.\n[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information\nprocessing systems, 33:1877\u20131901, 2020.\n[5] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt, T. Lukasiewicz, Y. Wu, J. B.\nTenenbaum, W. Hart, et al. Evaluating language models for mathematics through interactions. arXiv\npreprint arXiv:2306.01694, 2023.\n[6] L. de Moura, S. Kong, J. Avigad, F. Van Doorn, and J. von Raumer. The Lean theorem prover (system\ndescription). In Automated Deduction-CADE-25: 25th International Conference on Automated\nDeduction, pages 378\u2013388, 2015.\n[7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[8] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu,\nO. Firat, et al. GLaM: Efficient scaling of language models with mixture-of-experts. In International\nConference on Machine Learning, pages 5547\u20135569. PMLR, 2022.\n[9] R. Durrett. Probability: Theory and Examples. Cambridge Series in Statistical and Probabilistic\nMathematics. Cambridge University Press, 2019.\n[10] T. Eloundou, S. Manning, P. Mishkin, and D. Rock. GPTs are GPTs: An early look at the labor\nmarket impact potential of large language models. arXiv preprint arXiv:2303.10130, 2023.\n[11] E. First, M. N. Rabe, T. Ringer, and Y. Brun. Baldur: whole-proof generation and repair with large\nlanguage models. arXiv preprint arXiv:2303.04910, 2023.\n[12] S. Frieder, M. Alawadhi, Trimmel, Rashid, and K. Gy. LLM vs ITP. In The 3rd Workshop on\nMathematical Reasoning and AI at NeurIPS\u201923, 2023.\n[13] S. Frieder, L. Pinchetti, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz, P. C. Petersen, A. Chevalier,\nand J. Berner. Mathematical capabilities of ChatGPT. In Advances in Neural Information Processing\nSystems, volume 36, 2023.\n11\n[14] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,\nN. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv\npreprint arXiv:2101.00027, 2020.\n[15] A. Haviv, O. Ram, O. Press, P. Izsak, and O. Levy. Transformer language models without positional\nencodings still learn positional information. arXiv preprint arXiv:2203.16634, 2022.\n[16] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt.\nMeasuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\n2021.\n[17] D. Hendrycks and K. Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415,\n2016.\n[18] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration.\narXiv preprint arXiv:1904.09751, 2019.\n[19] A. Koubaa, B. Qureshi, A. Ammar, Z. Khan, W. Boulila, and L. Ghouti. Humans are still better\nthan ChatGPT: Case of the IEEEXtreme competition. arXiv preprint arXiv:2305.06934, 2023.\n[20] C. Li. OpenAI\u2019s GPT-3 language model: A technical overview, 2020. https://lambdalabs.com/\nblog/demystifying-gpt-3.\n[21] L. Liu, J. Liu, and J. Han. Multi-head or single-head? an empirical comparison for transformer\ntraining. arXiv preprint arXiv:2106.09650, 2021.\n[22] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoy-\nanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692,\n2019.\n[23] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, and\nD. Roth. Recent advances in natural language processing via large pre-trained language models: A\nsurvey. ACM Computing Surveys, 56(2):1\u201340, 2023.\n[24] J. R. Munkres. Topology. Prentice-Hall, 2000.\n[25] OpenAI. Introducing ChatGPT, 2022. https://openai.com/blog/chatgpt.\n[26] OpenAI. GPT-4 technical report. arXiv preprint 2303.0877, 2023.\n[27] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, et al. Training language models to follow instructions with human feedback. Advances in\nNeural Information Processing Systems, 35:27730\u201327744, 2022.\n[28] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by\ngenerative pre-training, 2018. https://openai.com/research/language-unsupervised.\n[29] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever.\nLanguage models are\nunsupervised multitask learners, 2019. https://github.com/openai/gpt-2.\n[30] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li, X. Zhang, A. Podolskiy, G. Arshinov,\net al. PanGu-\u03a3: Towards trillion parameter language model with sparse heterogeneous computing.\narXiv preprint arXiv:2303.10845, 2023.\n[31] W. Rudin. Functional analysis. McgGraw-Hill, 1991.\n[32] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n[33] T. Schick, J. Dwivedi-Yu, R. Dess\u00ec, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and\nT. Scialom.\nToolformer: Language models can teach themselves to use tools.\narXiv preprint\narXiv:2302.04761, 2023.\n[34] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units.\narXiv preprint arXiv:1508.07909, 2015.\n12\n[35] E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in\nNLP. arXiv preprint arXiv:1906.02243, 2019.\n[36] I. Tenney, D. Das, and E. Pavlick. BERT rediscovers the classical NLP pipeline. arXiv preprint\narXiv:1905.05950, 2019.\n[37] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,\nL. Baker, Y. Du, et al.\nLaMDA: Language models for dialog applications.\narXiv preprint\narXiv:2201.08239, 2022.\n[38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971, 2023.\n[39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023.\n[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin.\nAttention is all you need. Advances in Neural Information Processing Systems, 30, 2017.\n[41] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\nM. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao,\nS. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. Transformers: State-of-the-art natural language\nprocessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, 2020.\n[42] K. Yang and J. Deng.\nLearning to prove theorems via interacting with proof assistants.\nIn\nInternational Conference on Machine Learning, pages 6984\u20136994. PMLR, 2019.\n[43] K. Yang, A. M. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil, R. Prenger, and A. Anand-\nkumar. LeanDojo: Theorem proving with retrieval-augmented language models. arXiv preprint\narXiv:2306.15626, 2023.\n[44] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al.\nA survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\n13\n"
  },
  {
    "title": "NeRFiller: Completing Scenes via Generative 3D Inpainting",
    "link": "https://arxiv.org/pdf/2312.04560.pdf",
    "upvote": "10",
    "text": "NeRFiller: Completing Scenes via Generative 3D Inpainting\nEthan Weber1,2 Aleksander Ho\u0142y\u00b4nski1,2 Varun Jampani1 Saurabh Saxena1\nNoah Snavely1 Abhishek Kar1 Angjoo Kanazawa2\n1Google Research\n2UC Berkeley\n3D Scene or Object\nCompleting 3D Scans\nVariety of Inpaints\nIncomplete Regions\nInpainted NeRF\nReference 2D Inpaint\nInpainted NeRF\nNeRFiller\nNeRFiller\nNeRFiller\nNeRFiller\nFigure 1. NeRFiller. We propose a generative 3D inpainting approach for scene or object completion. Given a 3D capture with incomplete\nregions (left), our approach completes scenes such as the incomplete teddy bear scan (top left) and deletes unwanted occluders such as the\npillow and the price tag (bottom left). We can also control the completions using a reference inpainted exemplar (right) to guide the process.\nAbstract\nWe propose NeRFiller, an approach that completes miss-\ning portions of a 3D capture via generative 3D inpainting\nusing off-the-shelf 2D visual generative models. Often parts\nof a captured 3D scene or object are missing due to mesh\nreconstruction failures or a lack of observations (e.g., con-\ntact regions, such as the bottom of objects, or hard-to-reach\nareas). We approach this challenging 3D inpainting problem\nby leveraging a 2D inpainting diffusion model. We identify\na surprising behavior of these models, where they generate\nmore 3D consistent inpaints when images form a 2\u00d72 grid,\nand show how to generalize this behavior to more than four\nimages. We then present an iterative framework to distill\nthese inpainted regions into a single consistent 3D scene. In\ncontrast to related works, we focus on completing scenes\nrather than deleting foreground objects, and our approach\ndoes not require tight 2D object masks or text. We compare\nour approach to relevant baselines adapted to our setting\non a variety of scenes, where NeRFiller creates the most\n3D consistent and plausible scene completions. Our project\npage is at https://ethanweber.me/nerfiller.\n1. Introduction\nConsider the 3D scanned teddy bear and cat in Figure 1.\nIn many 3D captures such as these, parts of the scene may\nnot be as one desires: there may be unobserved regions\nsuch as the bottom of the bear and behind the cat, or there\nmay be unwanted parts such as the price tag on the cat ear.\nAdditionally, one may want to modify a feature, or generate\na variety of alternative models, e.g., a bear with bunny ears\nor a santa cat. All of these tasks require the ability to edit\nand inpaint content in a 3D-aware and multi-view consistent\nmanner. This is a challenge, since 2D generative inpainting\nmodels will not by default generate 3D consistent images.\nOur goal is to take a step in this direction and present a\nmethod that can create new content via scene completion\nconditioned on a set of multi-view images.\nSpecifically, we present a 3D scene completion frame-\nwork called NeRFiller, which given a scene and specified\nparts of the scene to inpaint, returns a 3D scene that is com-\npleted in a multi-view consistent manner. Our approach not\nonly completes missing regions (Figure 1, center), but can\nalso generate multiple variations of the missing regions (Fig-\nure 1, right). Furthermore, our approach does not require\narXiv:2312.04560v1  [cs.CV]  7 Dec 2023\nTight object mask\nImage\nObject Removal (Prior Work Setting)\nInpaint\nScene Completion (Our Setting)\nOur 3D inpaint\nArbitrary mask\nFigure 2. Object removal vs. scene completion. We focus on\nscene completion (right) as opposed to object removal (left). Prior\nwork focuses on removing entire objects with tight masks, while we\ntackle the more general setting of completing scenes with arbitrary\nmissing regions across wide baselines. More realistic scenarios\ninclude missing regions or parts of scenes to edit, as illustrated in\nFigure 1.\ntext prompting and can operate from the scene context alone.\nWe achieve this by proposing a novel approach to gener-\nate inpaints with an off-the-shelf 2D generative image model\nin a manner that encourages multi-view consistency. Specif-\nically, we identify a useful phenomenon in text-to-image\ndiffusion models that we refer to as a Grid Prior: denoising\nfour images with missing observations that are tiled in a 2\u00d72\ngrid results in more consistent multi-view inpaints than in-\npainting them independently, shown in Figure 3. We propose\na method called Joint Multi-View Inpainting that generalizes\nthis behavior to more than four images. While this technique\nresults in more 3D consistent inpaints, it is still a 2D-based\napproach and 3D consistency is not guaranteed. Therefore,\nwe propose a way to distill these inpaints in a global 3D\nscene representation in an iterative manner.\nWhile there has been a surge of recent works that generate\n3D scenes completely from scratch using text [14, 23] or\nimage guidance [28, 29], our approach differs in that we\nfocus on completing scenes given the context of an existing\n3D scene. Our approach is related to recent methods that\nremove a specified object from a scene [36], but we can\ngenerate new content that goes beyond completing a textured\nbackground, as illustrated in Figure 2. We also do not assume\na tight object mask, and can generate a diverse set of inpaints.\nTo demonstrate the efficacy of our approach, we experi-\nment with a diverse set of scenes including 3D indoor pho-\ntogrammetry captures lacking coverage in certain areas, 3D\nscenes with specified missing regions, and 3D objects. While\nour problem is challenging, we show that NeRFiller can re-\ncover more 3D consistent and plausible results compared to\nrecent state-of-the-art methods adapted to our setting.\n2. Related Work\nOur goal is to complete missing parts of an existing 3D scene.\nThere are several ways to approach this, via 2D inpainting\nor via distilling a 2D generative model for 3D generation.\n2D inpainting. 2D inpainting methods take an image and\nmask and complete the missing content at the mask location.\nEarly methods relied on inpainting by copying texture from\nIndividual inpainting\nGrid Prior\nFigure 3. Grid Prior. Here we inpaint the corner of the room (left\nillustrated in pink) with individual inpainting (top) and our Grid\nPrior method (bottom). Individual inpaints are diverse, while the\nGrid Prior encourages multi-view consistency.\nknown regions into the unknown regions [13]. A state-of-\nthe-art model is LaMa (Large Mask inpainting) [47], which\nis particularly good at infilling large missing areas. It uses\nfast Fourier convolutions, a large receptive field, and large\ntraining masks. This model is highly effective at complet-\ning plausible \u201cbackground\u201d textures within a specified mask\n(Fig. 2 left) but lacks diverse outputs as it is determinis-\ntic. Probabilistic diffusion models [22, 38] have recently\nproduced remarkable results for image generation. They\ncan also be used for inpainting and can generate diverse\ninpainted outputs. Pixel-based diffusion models do not have\nto be trained explicitly for inpainting, but can be modified\nat test-time by setting known regions before each denoising\niteration [31]. Latent diffusion models (LDMs) [43] are also\neffective at inpainting and are efficient because they oper-\nate in latent space. However, they require fine-tuning for\ninpainting with image and mask conditioning. 2D inpainting\nmodels can be prompted [1] and/or fine-tuned [49] enabling\nadditional flexibility for downstream applications.\n3D generation. 3D generation takes as input text or im-\nages and outputs 3D content. The Infinite Nature line of\nwork [26, 28, 29, 41, 59], takes as input a single image and\ngenerates immersive fly-through content using a 2D inpaint-\ning model queried in an autoregressive manner [28, 29]. Cai\net. al. [6] follow this path with a diffusion model, however,\nnone of these approaches can recover a global 3D scene rep-\nresentation. Persistent Nature [7] and related work [9, 12]\nmaintain a latent scene but are completely generative and\nnot conditioned on input image sets. SceneScape [14] and\nText2Room [23] use text prompts and 2D inpainters to cre-\nate a 3D mesh by using an inpainter and depth predictor\nto successively stitch a mesh. These approaches cannot fix\na mistake in the scene if a bad inpaint is made during the\nsuccessive stitching because no global optimization is per-\nformed.\nOther methods create 3D content via a global optimiza-\ntion strategy. DreamFusion [39] and related works [55, 62]\nuse the NeRF framework to optimize a 3D volume given\nShuffle M times and predict noise\nInpainted images\nImages to inpaint\nAverage M noise predictions\nTake a step and repeat\nFigure 4. Joint Multi-View Inpainting. We enable properties of the Grid Prior with more than four images by averaging diffusion model\npredictions. We take N images (left), create N/4 grids, and obtain a noise prediction from SD [43]. We do this M times and average the\nnoise predictions before taking a denoising step. At z0, the images (right) are fairly consistent and can be used to train a NeRF with our\nInpaint DU method.\na text prompt. Others train models to have 3D consistent\nproperties [27, 30, 50, 57, 62]. Follow-up works leverage 2D\ndiffusion models techniques [15, 44] to create 3D content\nconditioned on real images [8, 32, 40]. These approaches\nare not designed for the inpainting task.\n3D inpainting. Unlike most 3D generation methods, we\nground our inpaints with an actual 3D scene or object that has\nmissing regions (Figure 1). Casual capture [5, 19, 20, 33, 51]\nor NeRFs [25, 34] is a use-case as they often contain artifacts\nwhen rendered from novel views [17, 56]. Most relevant to\nscene the completion setting is the object removal setting\n(see Figure 2). These works remove foreground objects from\nNeRF captures [36, 52, 58]. They do this by inpainting each\nimage in a NeRF dataset once and training with various\nlosses including patch-based perceptual losses and depth\nregularization. [35] enables inpainting from a reference\nimage. A variety of these methods are evaluated on the SPIn-\nNeRF dataset, which is in the forward-facing LLFF [33]\nformat and has small parallax. Our work uses datasets with\na significantly larger baseline.\nOur focus is on the more general scene completion setting,\nwhich is related to editing. IN2N [18] edits a scene using\nInstructPix2Pix [4], but it cannot hallucinate new geometry.\nThe video editing literature is also relevant, with techniques\nsuch as extended attention from Tune-A-Video [16, 60] to\nencourage consistency in edited video frames. However, edit-\ning 2D images does not guarantee consistency when lifted to\n3D. In our method, we encourage 2D inpaints to converge via\niterative NeRF optimization and dataset updates. Moreover,\nwe achieve this with off-the-shelf 2D generative models with-\nout the need for expensive purpose-trained diffusion models\nor model fine-tuning.\n3. Preliminaries\n3.1. Neural Radiance Fields (NeRFs)\nNeural radiance fields (NeRFs) [34] represent the 3D geom-\netry and radiance of a scene with neural networks. NeRFs\ntake as input an 3D position (x, y, z) and a viewing direction\n(\u03b8, \u03d5), and output a color and density (c, \u03c3). To train a NeRF\nf\u0398, a set of calibrated, posed images are used to construct\na set of 3D rays r(t) = o + td for each pixel with known\ncolor C(r). During training, these rays are sampled and\nrendered via volumetric rendering to obtain a color estimate\n\u02c6C(r). Rays are sampled from training images and the field is\noptimized with photometric losses Lnerf (C(r), \u02c6C(r)), e.g.,\nMSE or LPIPS [61]. During inference, a full image is ren-\ndered with all rays of the desired camera.\n3.2. 2D Diffusion Models\nDiffusion models consist of two processes: a forward process\nq that gradually adds noise to a data sample z0 \u223c pdata(z),\nand a learned reverse process to iteratively denoise a pure\nGaussian noise sample zT\n\u223c N(0, 1) into a clean im-\nage z0. An intermediate noisy zt can be obtained from\nthe clean image by adding noise \u03f5 with scaling \u00af\u03b1t, where\nzt = \u221a\u00af\u03b1tz0 + \u221a1 \u2212 \u00af\u03b1t\u03f5. The diffusion model \u03f5\u03b8 predicts\nnoise \u02c6\u03f5 present in the image zt as \u02c6\u03f5 = \u03f5\u03d5(zt, t, c). t is a\ntime indicating how much noise is in the sample, and c\nis a general form of conditioning (e.g., images, masks, or\ntext). During training, random noise \u03f5 and t are sampled\nand the objective Ldiff = ||\u02c6\u03f5 \u2212 \u03f5||2 is minimized. With\nthe prediction \u02c6\u03f5 at time t, a reduced noise zt\u22121 can be ob-\ntained by zt\u22121 = zt \u2212 \u02c6\u03f5 (where we omit the scaling of \u02c6\u03f5 for\nsimplicity). Repeating this until z0 yields a fully denoised\nsample. Stochastically training with c (conditionally) and\nwithout c (unconditionally) enables classifier-free guidance\n(CFG) [21] during inference time. In practice, z is latents\nsince we are using SD (Stable Diffusion) [43], but in general\nwe can map from z to higher resolution pixels x with an\nencoder E(x) and decoder D(z).\nUsing a diffusion model as a prior. Diffusion models have\nadvantages over other models (e.g., GANs and determinis-\ntic inpainters [47]) because they can be used a a prior to\noptimize underlying variables such as the parameters of a\n3D NeRF f\u0398 with methods like score distillation sampling\n(SDS) [39, 54]. When used as a prior for NeRFs, the ob-\njective is to find the best \u0398 such that a rendered image x\nIndividual inpaints\nOur inpainting\nRegions to inpaint\nFigure 5. Joint Multi-View Inpainting Examples. The top images are inpainted with SD [43] without any text conditioning (middle) and\nwith our Joint Multi-View Inpainting method (bottom). Our joint inpaints are more multi-view consistent.\nhas high likelihood under the diffusion model prediction \u03f5\u03d5.\nSDS involves rendering an image, adding partial noise, and\nupdating the NeRF such that the diffusion model can pre-\ndict the added noise. IN2N [18] introduced a variant of this\nmethod coined Dataset Update (DU). Instead of backprop-\nping based on the diffusion model prediction, DU renders\nan image, adds partial noise, and takes multiple steps to\nrecover an estimated clean image x0. The clean image is\nadded to the dataset and used to supervise the NeRF. Every\nS iteration, another image is replaced. The DU supervi-\nsion signal will be slightly delayed since images are cached\nfor several iterations, while SDS provides immediate gra-\ndients corresponding to the current render. However, we\nuse the DU method in our work because it has a few advan-\ntages over SDS in terms of implementation: We can obtain\nhigher-resolution supervision (albeit slightly delayed) with\nless GPU memory and we can update a large batch of images\nsimultaneously (e.g., 40 images). We find that large batch\nupdates are important for our inpainting task since we are\nchanging the NeRF geometry, unlike prior works that focus\npurely on modifying appearance [18, 37].\n4. Method\nOur method, NeRFiller, aims to complete a missing region\nwithin a 3D scene by using an inpainting prior from a gener-\native 2D diffusion model. This problem statement poses a\nnumber of challenges. First, the inpainted estimates from a\n2D diffusion model are diverse, and may vary from sample to\nsample. This requires a consolidation mechanism to ensure\nthat the completed 3D scene contains one salient inpainted\nresult, as opposed to the average of all possibilities. Second,\n2D inpainting models are not trained for 3D consistency,\nand will therefore provide estimates that cannot be explained\nby a single 3D scene, even if they correspond to the same\napproximate style or content. In the following, we describe\nour approaches to tackle these problems. In Section 4.1, we\ndescribe how we encourage the inpainted outputs from a dif-\nfusion sampling process to be 3D consistent. In Section 4.2,\nwe describe an iterative 3D scene optimization method that\nuses these inpainted images to optimize for a globally consis-\ntent inpainted 3D scene. NeRFiller builds on an observation\nthat inpainting a grid of images encourages the outputs to\nJoint Multi-View\nInpainting\nTime-dependent noise\nPredicted Depth\n(Optional)\nInpainted images\nConditioning Signal\nCurrent NeRF Renders\nDataset with masks\nDataset Update\nMasked image and (optional) text\nMonocular\nDepth\nFigure 6. Inpaint Dataset Update. Every S iterations, we update\nthe unknown pixels of the NeRF training images. We render N\nimages, add partial noise, and jointly inpaint with a conditioning\nsignal. We (optionally) predict the depth and update the dataset.\nhave similar appearance, and we extend this idea to an arbi-\ntrarily large collection of images through a joint sampling\napproach.\n4.1. Multi-view consistent inpainting\nA core challenge in 3D inpainting with a 2D generative\nmodel is getting the outputs of the 2D model to be consistent\nacross views. This challenge stems from the multimodality\nof the output distribution: in most cases, there are many plau-\nsible inpaintings, and sampling multiple consistent images\nremains an open research problem.\nGrid Prior. While inpainting multiple viewpoints inde-\npendently may produce inconsistent results, one interesting\ndiscovery is that consistency can be achieved by tiling the\ninput images into a grid and treating the grid of images\n(and their corresponding masks) as a single inpainting tar-\nget. This grid-based prior can produce more 3D consistent\nviews, both in coarse appearance and approximate scene\nstructure (illustrated in Figure 3). We hypothesize that this\nphenomenon results from similarly structured examples in\nStable Diffusion\u2019s training dataset: sets of observations de-\npicting the same scene or object organized as a grid (e.g.,\nscreenshots of online product photos). Similar properties\nwere also explored in visual-prompting [1].\nMore specifically, in order to inpaint four images consis-\nSD Text Cond (\u201ca photo of a chair\u201d)\nLaMask\nSD Image Cond (no text prompt)\nExtended Attention\nImages to inpaint\nNeRF\nJoint Multi-View Inpainting\nNeRF\nNeRF\nNeRF\nNeRF\nNeRF\nFigure 7. Inpainting methods. Inpainting methods produce inconsistent inpaints. We show various inpainting methods (boxed) and use a\ncollection of them to train a NeRF. A resulting render is shown on the right of each method. Using our Grid Prior and Joint Multi-View\nInpainting creates reasonably consistent inpaints and a plausible NeRF. X means the NeRF failed and resulted in white everywhere.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMasked NeRF\n7.76\n0.71\n0.37\nLaMask\n19.58\n0.89\n0.20\nSD Text Cond\n12.56\n0.73\n0.32\nSD Image Cond\n14.15\n0.76\n0.28\nExtended Attention\n14.57\n0.77\n0.27\nGrid Prior\n14.43\n0.80\n0.25\nJoint Multi-View Inpainting\n15.89\n0.82\n0.23\nTable 1. Multi-view consistent inpainting. We inpaint images and\ntrain a NeRF for the 8 scenes of the NeRF synthetic dataset. Better\nmetrics indicate more consistency of the NeRF 3D reconstruction\nwith the 2D inpaints. Note that LaMask achieves the best results\nas it often copies the white background into the hole (see Figure 7)\nand results in a failed NeRF that is totally white.\ntently, one can downsample them and their corresponding\ninpainting masks to quarter-resolution and tile them as a\n2\u00d72 grid. This grid is fed through the 2D inpainting model\n(as a single image would) to get as output four inpainted\nimages with consistent content. More formally, let G be the\ndownsampling and grid operation for four images and let\nG\u22121 undo this. We can grid four images latents z1\nt , z2\nt , z3\nt , z4\nt\nas follows:\n{\u02c6\u03f51\nt, \u02c6\u03f52\nt, \u02c6\u03f53\nt, \u02c6\u03f54\nt} = G\u22121(\u03f5\u03d5(G({z1\nt , z2\nt , z3\nt , z4\nt })))\n(1)\nand take a denoising step with zi\nt\u22121 = zi\nt \u2212 \u02c6\u03f5i\nt. In principle,\nthis approach is similar to recent methods that use extended\nattention, i.e., shared keys and values in the attention opera-\ntions across a set of parallel sampling processes [60]. Our\napproach does not share attention features but instead shares\ncontext with other images via the diffusion U-Net receptive\nfield that sees 4 tiled images at a time. In our experiments,\nwe compare to extended attention and demonstrate that our\ngrid prior more effectively inpaints 3D consistent content\nwhen used with our Joint Multi-View Inpainting method.\nJoint Multi-View Inpainting. While effective at inpainting\na set of images consistently, applying the grid prior to a\nlarger set of images poses additional challenges. Increasing\nthe number of images in the grid proportionally decreases\nthe output resolution of each image (e.g., arranging a set of\n2\u00d72 images in a grid reduces each image\u2019s resolution by 4,\na 3\u00d73 grid by 9, and so on). For the purpose of producing\nhigh-quality inpainting results, we would like to minimize\nany loss in image detail. Therefore, we propose a method\nthat uses the above grid prior in a joint sampling process,\ninspired by MultiDiffusion [2]. In each sampling step, we\nshuffle all the input images into a set of 2\u00d72 grids. We\nrepeat this M times and before taking a sampling step, the\nscore estimate for each image is combined across all the\ngrid combinations in which it was seen. This causes the\ninpainting estimates to be gradually shared across the entire\ndataset, effectively increasing the grid size without further\nreducing effective resolution.\nFigure 4 describes this procedure. More formally, for\na batch of N images, we randomly permute their order,\nconstruct N/4 grids, and predict the noise for M iterations\n(j \u2208 [1, M]), as follows:\n{\u02c6\u03f51j\nt . . . \u02c6\u03f5Nj\nt\n} = G\u22121(\u03f5\u03d5(G({z1j\nt . . . zNj\nt\n})))\n(2)\nand step with zi\nt\u22121 = zi\nt \u2212 P\nj\u2208M \u02c6\u03f5ij\nt from zT to z0. Quali-\ntative results are shown in Figure 5.\n4.2. Completing 3D Scenes\nThe proposed Joint Multi-View Inpainting enables inpainting\nimages in a more 3D consistent manner than other inpaint-\ning methods. Next, we describe how to distill these 2D\ninpainting results into a single 3D reconstruction. We refer\nto our method as Inpaint Iterative Dataset Update (Fig. 6),\nor Inpaint DU, as it derives from IN2N\u2019s [18] Iterative DU\nmethod (see Sec. 3.2). In contrast to IN2N, which begins\nwith a complete NeRF reconstruction and uses a model con-\nditioned on complete 2D observations, our task requires us to\ntrain a complete NeRF from images with masked unknown\nregions. As in IN2N, we begin training with a dataset of\noriginal (known) pixels, and update the dataset over training\nby adding or replacing the set of initially unknown pixels\nwith the inpainted estimates. Fig. 6 illustrates this proce-\ndure. Specifically, every S steps, we render the set of N\ntraining views, encode them into latents zi\n0 and then par-\ntially noise them before feeding them to SD. We sample\nfrom these partially noised inputs using the proposed Joint\nMulti-View Inpainting strategy, then use the resulting im-\nages to replace the corresponding images in the dataset. This\nprocess is both prefixed and suffixed by an encode and de-\ncode operation, since the base model is a latent diffusion\n\u201cdumptruck\u201d\n\u201coffice\u201d\n\u201cbackpack\u201d\n\u201cdrawing\u201d\nDataset with masks\nLaMask\nInpaint + DU\nOurs\nSD Image Cond\nFigure 8. Qualitative NeRF results. On the left, we show various scenes with pink regions to be completed. We compare NeRFiller (far\nright) against baselines adapted to our scene completion setting. The \u201coffice\u201d scene is missing parts of the wall, floor, and under the chairs.\nmodel. We repeat this process many times while linearly\nannealing t from full noise t = 1 to t = tmin. In practice,\nwe set tmin = 0.4, since we find that low noise values result\nin quality degradation. We observe that over the course of\noptimization, our inpainted images become gradually more\nconsistent (Fig. 9) as the added geometry and texture begin\nto take form. Annealing t helps encourage the inpainting to\nconverge to a single result rather than making large changes\nlate in training.\nDepth regularization. Inpaint DU optionally incorporates\ndepth supervision to improve inpainted scene geometry. Af-\nter each dataset update, we predict the depth for all images\nwith ZoeDepth [3]. We use a relative depth ranking loss [53]\nin the inpainted regions (but not on the known pixels). We\nuse a ranking loss because it\u2019s a softer constraint than metric\ndepth supervision, where errors in scale-and-shift alignment\ncould more easily harm the 3D scene geometry. We only ap-\nply depth supervision for our main method to indoor scenes\nand not objects, since we empirically noticed that [3] per-\nforms less consistently when the background is a solid color\n(e.g., white or black).\n5. Experiments\nWe compare NeRFiller for 3D scene completion to vari-\nous inpainting baselines. We first investigate various in-\npainting strategies on multi-view synthetic scenes to gauge\nhow effective a deterministic inpainter [47] is compared to\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMUSIQ \u2191\nCorrs \u2191\nMasked NeRF\n14.71\n0.78\n0.26\n3.71\n675\nLaMask\n27.39\n0.90\n0.05\n3.76\n643\nSD Image Cond\n22.03\n0.86\n0.11\n3.68\n665\nInpaint + DU\n26.60\n0.89\n0.08\n3.76\n660\nOurs w/o depth\n28.41\n0.92\n0.06\n3.72\n682\nOurs\n28.28\n0.91\n0.06\n3.73\n696\nTable 2. Quantitative NeRF results. We report various metrics\naveraged over our 10 scenes to quantify consistency of the 3D\nNeRF with the dataset inpaints (left), as well as novel-view metrics\n(right), such as \u201cCorrs\u201d (number of high-quality correspondences\nbetween random pairs of frames) for geometry.\nSD [43], sampled in various ways. After establishing that\nour Joint Multi-View Inpainting demonstrates multi-view in-\npainting properties, we evaluate our full method on 10 scans\nwith missing regions. We compare NeRFiller to various\nobject-removal baselines, adapted to our setting, to complete\nmissing regions. Finally, we analyze the parameters of our\nmethod and show an application to reference-guided scene\ncompletion. We conduct experiments with Nerfstudio [48]\nand provide implementation specifics in the appendix.\n5.1. 3D consistent image inpainting\nOur goal is to evaluate various 2D inpainting models and\nstrategies to quantify their 3D consistency.\nSetting and evaluation. For these experiments, we take the\ntesting split of the NeRF synthetic dataset [34] (8 scenes of\nAnnealing noise\nschedule (ours)\nRandom noise\nschedule (IN2N)\n5K\n15K\n30K (final)\nIteration 0\nFigure 9. Noise schedule. We anneal the amount of noise we add\nto the NeRF renders when making a dataset update, while I-N2N\nchooses random noise each time. Annealing the noise produces\nsharper results (top) while I-N2N\u2019s noise schedule introduces sig-\nnificant blur (bottom).\n200 images each). We resize each image to 512\u00d7512 resolu-\ntion and mask out the center of each image with a 256\u00d7256\nregion to inpaint (see Figure 7 top left). We inpaint all images\nwith various methods and train a Nerfacto NeRF model [48]\non 180 equally spaced images and evaluate metrics on the\nremaining 20 images. We use the standard NeRF metrics\nbecause they capture how similar the 3D reconstruction is to\nthe 20 hold-out evaluation images. When rendering for eval-\nuation (right of inpainted images in Figure 7), we push the\nnear plane slightly forward to avoid including any floaters\nhiding in front of the cameras.\nBaselines. Our baselines are the following:\n\u2022 Masked NeRF - No inpainting, only train on known pixels.\n\u2022 LaMask - LaMa [47] inpainting model.\n\u2022 SD Text Cond - SD using a text CFG with prompt \u201ca photo\nof {description}\u201d. See the appendix for text prompts.\n\u2022 SD Image Cond - SD with only image CFG.\n\u2022 Extended Attention - SD with only image CFG and ex-\ntended attention [60].\nFor Extended Attention and Grid Prior, we inpaint in batches\nof 5 and 4, respectively, and for Joint Multi-View Inpainting,\nwe inpaint 40 images simultaneously with M = 8 diffusion\naveraging steps. To inpaint additional batches of 40 images,\nwe set 20 in the batch as known. This enables fitting within\nthe memory constraints of a 16 GB GPU.\nResults. Our results are shown in Tab. 1, where we see\nthat Joint Multi-View Inpainting achieves the best metrics\nin multi-view consistent inpainting. Our results show that\nwe have achieved some level of multi-view consistency. Our\nimages look the most consistent in Figure 7 (bottom right),\nand the trained NeRF looks plausible. The other methods\nyield significant blur in the NeRF reconstruction. We provide\nvideos on the project page showing the NeRF results for each\nmethod.\n5.2. Completing large unknown 3D regions\nIn this setting, our goal is to complete missing regions in\n3D content. We construct a set of 10 datasets consisting\nNovel  view\nReference inpaint\nScene variety\nObject editing\nNovel view\nReference inpaint\nFigure 10. Reference-based completion. Given a reference inpaint\n(top row), we propagate it into a 3D NeRF (bottom row).\nof various 3D content. For some scenes e.g., the backpack\nfrom [36], we modify their provided mask to include part\nof the object (Fig. 2) to convert it to the scene completion\nsetting. For other scenes, we simply want to fill in any miss-\ning details (e.g., parts of walls). Some of the meshes are\nmissing vertices after multi-view stereo reconstruction, e.g.,\n\u201cbear\u201d and \u201coffice\u201d. For others, we place a large 3D occluder\nin the scene to simulate the scene completion setting. The\npink regions in Fig. 8 (left) shows the areas to complete. We\ncreate the datasets by rendering \u223c60 novel views looking at\nthe occluded region. Importantly, the rendered images have\nenough known pixels to provide context to the inpainting\nmodel that the images observe the same scene from different\ncamera viewpoints. Our datasets have much more parallax\nthan the forward-facing scenes of [36]. Our appendix pro-\nvides details on our data, including where we obtained our\n3D content, mostly from Objaverse [10, 11] and Sketchfab.\nEvaluation. The evaluation is similar to Sec. 5.1 for the\ndataset images. However, in this case our task is to construct\na scene for good novel-view synthesis, so we use all images\nfor both training and evaluation. We compute NeRF metrics\non the entire images, where we compare the final rendered\nimages with the latest version of the inpainted region. For\nmethods that inpaint once without DU (e.g., LaMask), we\ncompare against the first and only inpaints. For DU methods,\nwe compare against the latest round of inpaints. Our metrics\nare against inpainted images which serves to evaluate the\nconsistency of the scene because there is no ground-truth\nsolution. We also report novel-view metrics, computed on\na custom 10 second 30 FPS camera path novel views that\nmoves around the scene. We also report an image quality\nmetric MUSIQ [24] and a geometry metric. For geometry,\nwe report the number of high-quality LoFTR [45] corre-\nspondences between 100 randomly sampled pairs of frames.\nMore high-quality matches should correlate with better multi-\nview consistency and fewer extreme view-dependent effects\nthat destroy realism. Please see the Appendix for more de-\ntails.\nBaselines. We implemented the following baselines:\n\u2022 Masked NeRF - no inpainting, where we train a Nerfacto\nmodel [48] only in the known pixel locations,\n\u2022 LaMask - Inpaint once with LaMa [47] and train with\npatch-based perceptual losses. This is our adaptation of\nSPIn-NeRF [36].\n\u2022 SD Image Cond - Inpaint once with SD. This is similar\nto InpaintNeRF360 [52] but without text since we find in\nSec. 5.1 that text CFG produces very inconsistent inpaints.\n\u2022 Inpaint + DU - An adaption of IN2N [18] for our setting,\nwhich inpaints one image at a time with the SD inpainting\nmodel and our annealed noise schedule.\nResults. Some qualitative results are shown in Fig. 8 and\nfull videos for all 10 scenes are provided in the appendix.\nLaMask and SD Image Cond are both inpaint-once methods\nand therefore create large blurry regions in the NeRF, but\nbetween the two, LaMask is smoother since its determin-\nistic inpainter [47] is less creative than SD in its outputs.\nLaMa [47] tends to copy background textures into the mask\nregion. From certain views, Inpaint + DU looks sharp due\nto inpainting individually at full resolution; however, it has\ngeometric inconsistencies and view-dependent effects which\nare crisp from some angles and blurry in others. Our method\nlooks the most consistent with plausible outputs, although\nits ability to create consistent high-frequency texture details\nmay be improved. We provide quantitative results showing\nthat our final renders are most similar with the latest round\nof inpaints (Tab. 2 left). For novel-view metrics, we obtain\nthe most correspondences (Tab. 2 right). Note that although\nwe make an effort to capture the results quantitatively, there\nis no singular ground truth and therefore the results are best\ndiscerned qualitatively by viewing videos.\n5.3. Reference-based inpainting\nIn some situations it is desirable to have control over the\ncontent used to complete the scene. NeRFiller can be easily\nadapted to 3D inpaint with respect to a user-provided refer-\nence inpaint. To do this, we first inpaint from one view and\nuse it to prompt our Grid Prior update method. We ensure\nthat each grid has the one reference inpaint when passed\nthrough SD. This ensures that all U-Net predictions are influ-\nenced by the reference inpaint so that new inpaints are more\nlikely to be consistent with the reference. For this, we edit\n30 images at a time (instead of 40) and make 10 grids, each\nwith exactly 1 reference inpaint. See Figure 1 and Figure 10\nfor examples.\n5.4. Parameter choices\nNoise schedule. It is important to anneal the amount of noise\nadded the rendered images. We start by adding full noise\n(1.0) and decrease it to 0.4 over the 30K iterations of training.\nIN2N [18], in contrast, uses a random schedule of choosing\nbetween 0.98 and 0.02 each update. This likely works be-\ncause the InstructPix2Pix model is image conditioned and\ngeometry of the NeRF does not change much. In our case,\nwe are changing the geometry and using their schedule leads\nto blurry results, shown in Figure 9.\nOurs\n\u201cbilliards\u201d depth\nOurs w/o depth\n\u201cdrawing\u201d depth\nOurs\nOurs w/o depth\n\u201cbilliards\u201d image\n\u201cdrawing\u201d image\nFigure 11. Relative depth supervision. Relative depth supervision\ncleans up geometry (top) without affecting visual quality (bottom).\nDepth regularization. We find that adding depth ranking\nsupervision [53] in Ours improves geometry but it hardly\nchanges the quantitative or visual results. Our method with-\nout depth supervision is still favorable compared to the base-\nlines. In Figure 11, we see that the geometry is significantly\nimproved but the RGB NeRF renderings are nearly indis-\ntinguishable. Consequently, we use depth supervision on\nindoor scenes since having better geometry is favorable for\ndownstream applications such as mesh export.\n6. Limitations\nLow resolution and blur. Our method recovers coarse ge-\nometry quite well but struggles to recover high-resolution\ndetail in regions far way from the training cameras. We sus-\npect this is because our Joint Multi-View Inpainting method\ndownsamples images to construct the 2 \u00d7 2 grids. Perhaps\na post-processing method to fine-tune SD [46] or a GAN-\neRF [42] like optimization could improve the fidelity, but\nrecovering high-frequency details remains a fundamental\nissue for 3D generative methods, e.g., DreamFusion [39].\nInpainting NeRF casual captures. An application of our\napproach would be to inpaint deleted content from Nerf-\nbusters [56] or Bayes\u2019 Rays [17]. However, these masked-\nout regions are large (limiting scene context to an inpainter)\nand furthermore, their mask patterns cause SD [43] to fail\nwithout multiple iterations of dilation, as pointed out in\nText2Room [23] and in our appendix. One could retrain SD\nwith these mask distributions, but this is out of scope of our\nmethod which uses an off-the-shelf model.\n7. Conclusion\nIn this paper, we propose a generative 3D inpainting method\ncalled NeRFiller, which leverages an off-the-shelf 2D in-\npainting model [43] to complete missing parts of 3D scenes\nand objects. We discover a unique property of these models\nwhere tiling four images into a 2\u00d72 grid produces more con-\nsistent inpaints than inpainting them independently. We ex-\nploit this property and propose Joint Multi-View Inpainting,\nwhich enables inpainting many images simultaneously with\nmore consistency by averaging noise predictions. We show\nhow to use it in the NeRF setting by performing iterative\ndataset updates. We evaluate against relevant state-of-the-\nart baselines adapted to our problem setting on a variety of\n3D captures. Our approach also enables users to specify\nhow to fill in the missing regions. Many 3D captures are\nincomplete with holes, and our work presents a framework\nfor completing these missing regions.\nAcknowledgements\nThis project is supported in part by IARPA DOI/IBC\n140D0423C0035. The views and conclusions contained\nherein are those of the authors and do not represent the of-\nficial policies or endorsements of IARPA, DOI/IBC, of the\nU.S. Government. We would like to thank Frederik Warburg,\nDavid McAllister, Qianqian Wang, Matthew Tancik, Grace\nLuo, Dave Epstein, Riley Peterlinz for discussions and tech-\nnical support. We also thank Ruilong Li, Evonne Ng, Adam\nRashid, Alexander Kristoffersen, Rohan Mathur, Jonathan\nZakharov for proofreading drafts and providing feedback.\nReferences\n[1] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Glober-\nson, and Alexei Efros. Visual prompting via image inpainting.\nIn ANeurIPS, 2022. 2, 4\n[2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\nMultidiffusion: Fusing diffusion paths for controlled image\ngeneration. In ICML, 2023. 5\n[3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,\nand Matthias M\u00a8uller.\nZoedepth: Zero-shot transfer by\ncombining relative and metric depth.\nIn arXiv preprint\narXiv:2302.12288, 2023. 6\n[4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn CVPR, 2023. 3, 11\n[5] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erick-\nson, Peter Hedman, Matthew DuVall, Jason Dourgarian, Jay\nBusch, Matt Whalen, and Paul Debevec. Immersive light field\nvideo with a layered mesh representation. In SIGGRAPH,\n2020. 3\n[6] Shengqu Cai, Eric Ryan Chan, Songyou Peng, Mohamad\nShahbazi, Anton Obukhov, Luc Van Gool, and Gordon Wet-\nzstein. Diffdreamer: Consistent single-view perpetual view\ngeneration with conditional diffusion models. In ICCV, 2023.\n2\n[7] Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola, and\nNoah Snavely. Persistent nature: A generative model of\nunbounded 3d worlds. In CVPR, 2023. 2\n[8] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W\nBergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini\nDe Mello, Tero Karras, and Gordon Wetzstein. Generative\nnovel view synthesis with 3d-aware diffusion models. In\narXiv, 2023. 3\n[9] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu.\nScene-\ndreamer: Unbounded 3d scene generation from 2d image\ncollections. 2023. 2\n[10] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo,\nOscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte,\nVikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A\nuniverse of 10m+ 3d objects. In arXiv, 2023. 7\n[11] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani,\nAniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe\nof annotated 3d objects. In CVPR, 2023. 7\n[12] Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava,\nGraham W Taylor, and Joshua M Susskind. Unconstrained\nscene generation with locally conditioned radiance fields. In\nICCV, 2021. 2\n[13] Alexei A Efros and Thomas K Leung. Texture synthesis by\nnon-parametric sampling. In ICCV, 1999. 2\n[14] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel.\nScenescape: Text-driven consistent scene generation.\nIn\narXiv, 2023. 2\n[15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gener-\nation using textual inversion. In arXiv, 2022. 3\n[16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. In arXiv, 2023. 3\n[17] Lily Goli, Cody Reading, Silvia Sell\u00b4an, Alec Jacobson, and\nAndrea Tagliasacchi. Bayes\u2019 Rays: Uncertainty quantification\nin neural radiance fields. In arXiv, 2023. 3, 8, 11, 12, 15\n[18] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander\nHolynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing\n3d scenes with instructions. In ICCV, 2023. 3, 4, 5, 8, 12\n[19] Peter Hedman and Johannes Kopf. Instant 3D Photography.\nIn SIGGRAPH, 2018. 3\n[20] Peter Hedman, Suhib Alsisan, Richard Szeliski, and Johannes\nKopf. Casual 3D Photography. In SIGGRAPH Asia, 2017. 3\n[21] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. In NeurIPS Workshop on Deep Generative Models\nand Downstream Applications, 2022. 3\n[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 2\n[23] Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin Johnson, and\nMatthias Nie\u00dfner. Text2room: Extracting textured 3d meshes\nfrom 2d text-to-image models. In ICCV, 2023. 2, 8\n[24] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and\nFeng Yang. Musiq: Multi-scale image quality transformer. In\nICCV, 2021. 7, 12\n[25] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis. 3d gaussian splatting for real-time radiance\nfield rendering. In SIGGRAPH, 2023. 3\n[26] Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge,\nand Peter Anderson. Pathdreamer: A world model for indoor\nnavigation. In ICCV, 2021. 2\n[27] Jialu Li and Mohit Bansal.\nPanogen: Text-conditioned\npanoramic environment generation for vision-and-language\nnavigation. In NeurIPS, 2023. 3\n[28] Zhengqi Li, Qianqian Wang, Noah Snavely, and Angjoo\nKanazawa. Infinitenature-zero: Learning perpetual view gen-\neration of natural scenes from single images. In ECCV, 2022.\n2\n[29] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Maka-\ndia, Noah Snavely, and Angjoo Kanazawa. Infinite nature:\nPerpetual view generation of natural scenes from a single\nimage. In ICCV, 2021. 2\n[30] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Learn-\ning to generate multiview-consistent images from a single-\nview image. In arXiv, 2023. 3\n[31] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In CVPR,\n2022. 2\n[32] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In CVPR, 2023. 3\n[33] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. In SIGGRAPH,\n2019. 3, 12\n[34] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. In ECCV, 2020. 3, 6, 11\n[35] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A.\nBrubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G.\nDerpanis, and Igor Gilitschenski. Reference-guided control-\nlable inpainting of neural radiance fields. In ICCV, 2023. 3,\n12\n[36] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstanti-\nnos G. Derpanis, Jonathan Kelly, Marcus A. Brubaker, Igor\nGilitschenski, and Alex Levinshtein. SPIn-NeRF: Multiview\nsegmentation and perceptual inpainting with neural radiance\nfields. In CVPR, 2023. 2, 3, 7, 8, 11, 12\n[37] Thu Nguyen-Phuoc, Feng Liu, and Lei Xiao. Snerf: stylized\nneural implicit representations for 3d scenes. In SIGGRAPH,\n2022. 4\n[38] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman,\nJonathan T Barron, Amit H Bermano, Eric Ryan Chan, Tali\nDekel, Aleksander Holynski, Angjoo Kanazawa, et al. State\nof the art on diffusion models for visual computing. arXiv\npreprint arXiv:2310.07204, 2023. 2\n[39] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. In ICML, 2023.\n2, 3, 8\n[40] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nBen Mildenhall, Nataniel Ruiz, Shiran Zada, Kfir Aberman,\nMichael Rubenstein, Jonathan Barron, Yuanzhen Li, and\nVarun Jampani. Dreambooth3d: Subject-driven text-to-3d\ngeneration. In ICCV, 2023. 3\n[41] Chris Rockwell, David F Fouhey, and Justin Johnson. Pixel-\nsynth: Generating a 3d-consistent experience from a single\nimage. In ICCV, 2021. 2\n[42] Barbara\nRoessle,\nNorman\nM\u00a8uller,\nLorenzo\nPorzi,\nSamuel Rota Bul`o, Peter Kontschieder, and Matthias\nNie\u00dfner.\nGanerf: Leveraging discriminators to optimize\nneural radiance fields. In SIGGRAPH Asia, 2023. 8\n[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022. 2, 3,\n4, 6, 8, 11, 12\n[44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. In CVPR, 2023. 3\n[45] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. Loftr: Detector-free local feature matching\nwith transformers. In CVPR, 2021. 7, 12\n[46] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen\nLiu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchical\n3d generation with bootstrapped diffusion prior. In arXiv,\n2023. 8\n[47] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,\nAnastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,\nNaejin Kong, Harshith Goka, Kiwoong Park, and Victor Lem-\npitsky. Resolution-robust large mask inpainting with fourier\nconvolutions. In WACV, 2022. 2, 3, 6, 7, 8\n[48] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent\nYi, Terrance Wang, Alexander Kristoffersen, Jake Austin,\nKamyar Salahi, Abhik Ahuja, et al. Nerfstudio: A modular\nframework for neural radiance field development. In SIG-\nGRAPH Conference Proceedings, 2023. 6, 7, 12\n[49] Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li,\nAleksander Holynski, David E Jacobs, Bharath Hariharan,\nYael Pritch, Neal Wadhwa, Kfir Aberman, et al. Realfill:\nReference-driven generation for authentic image completion.\nIn arXiv, 2023. 2\n[50] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and\nYasutaka Furukawa. Mvdiffusion: Enabling holistic multi-\nview image generation with correspondence-aware diffusion.\nIn arXiv, 2023. 3\n[51] Richard Tucker and Noah Snavely. Single-view view synthe-\nsis with multiplane images. In CVPR, 2020. 3\n[52] Dongqing Wang, Tong Zhang, Alaa Abboud, and Sabine\nS\u00a8usstrunk. Inpaintnerf360: Text-guided 3d inpainting on\nunbounded neural radiance fields. In arXiv, 2023. 3, 8\n[53] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Zi-\nwei Liu. Sparsenerf: Distilling depth ranking for few-shot\nnovel view synthesis. In ICCV, 2023. 6, 8\n[54] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In CVPR,\n2023. 3\n[55] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. In NeurIPS, 2023. 2\n[56] Frederik Warburg*, Ethan Weber*, Matthew Tancik, Alek-\nsander Ho\u0142y\u00b4nski, and Angjoo Kanazawa. Nerfbusters: Re-\nmoving ghostly artifacts from casually captured nerfs. In\nICCV, 2023. 3, 8, 11, 12\n[57] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi.\nNovel view synthesis with diffusion models. In arXiv, 2022.\n3\n[58] Silvan Weder, Guillermo Garcia-Hernando, \u00b4Aron Monszpart,\nMarc Pollefeys, Gabriel Brostow, Michael Firman, and Sara\nVicente. Removing objects from neural radiance fields. In\nCVPR, 2023. 3\n[59] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin\nJohnson. Synsin: End-to-end view synthesis from a single\nimage. In CVPR, 2020. 2\n[60] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei,\nYuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie,\nand Mike Zheng Shou. Tune-a-video: One-shot tuning of\nimage diffusion models for text-to-video generation. In ICCV,\n2023. 3, 5, 7\n[61] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 3, 12\n[62] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-\ntilling view-conditioned diffusion for 3d reconstruction. In\nCVPR, 2023. 2, 3\nAppendix\nThe appendix includes more details related to the paper.\n8. NeRF synthetic prompts\nWe use the following text prompts for each of the 8 NeRF\nsynthetic datasets from the original NeRF paper [34].\n\u2022 \u201cchair\u201d: \u201ca photo of a chair\u201d\n\u2022 \u201cdrums\u201d: \u201ca photo of drums\u201d\n\u2022 \u201cficus\u201d: \u201ca photo of a ficus plant\u201d\n\u2022 \u201chotdog\u201d: \u201ca photo of a hotdog\u201d\n\u2022 \u201clego\u201d: \u201ca photo of a lego bulldozer\u201d\n\u2022 \u201cmaterials\u201d: \u201ca photo of materials\u201d\n\u2022 \u201cmic\u201d: \u201ca photo of a microphone\u201d\n\u2022 \u201cship\u201d: \u201ca photo of a ship\n9. Datasets\nSome of our data can be found online with their respective\nhyperlinks. Others were created by the authors by scanning\nwith Polycam, and one was obtained by modifying a SPIn-\nNeRF dataset.\n\u2022 \u201cdumptruck\u201d (Sketchfab)\n\u2022 \u201cturtle\u201d (author capture with Polycam)\n\u2022 \u201cdrawing\u201d (Sketchfab)\n\u2022 \u201cbackpack\u201d (SPIn-NeRF dataset)\n\u2022 \u201cbear\u201d (Sketchfab)\n\u2022 \u201cbilliards\u201d (Sketchfab)\n\u2022 \u201cnorway\u201d (Sketchfab)\n\u2022 \u201ccat\u201d (author capture with Polycam)\n\u2022 \u201cboot\u201d (Sketchfab)\n\u2022 \u201coffice\u201d (Sketchfab)\n9.1. Marking inpaint regions\nTo mark inpaint regions, we have a variety of approaches\ndepending on the data. For the \u201coffice\u201d scene, we mark\ninpaint regions as any missing region from the original mesh,\nfound here on Sketchfab. For the other scenes, we place a 3D\noccluder, such as a cube or cylinders. For the \u201ccat\u201d dataset\nin Figure 1, we use a cylinder to mark an occlusion of the\nbody and a rectangular prism to mark the tag by the ear. It\u2019s\nquite straightforward to load the mesh into Blender and add\nthe occluders. However, we leave the automatic placement\nof 3D occlusions for future work. Uncertainty masks from\nBayes\u2019 Rays [17] or deleted areas from Nerfbusters [56]\ncould provide masks for our method, but their masks are out-\nof-distribution for SD (Stable Diffusion), shown in Fig. 12\nand discussed in Sec. 11.\n9.2. Creating NeRF datasets\nTo create NeRF datasets with the marked inpaint regions,\nwe render the mesh from \u223c60 images (64 for all except\n\u201cbackpack\u201d which uses 60). For objects, we render around the\nobject and for e.g., large missing rectangles from a room, we\nrender arcs looking toward the region from different angles,\nat different elevations. We can mark the inpainting regions\nby doing a depth check between the actual mesh and the\nmanually placed occluders. To create the \u201cbackpack\u201d scene,\nwe took the dataset from SPIn-NeRF [36] and modified the\noriginal tight mask around the backpack to include the top\npart. We also dilated the masks to make them less tight. See\nFigure 2 for an illustration of our changes to convert it for\nour scene completion setting rather than object deletion.\n10. Additional experiment details\n10.1. Inpaint implementation details\nImage classifier-free guidance. Some percentage of the\ntime during fine-tuning of the Stable Diffusion model for\ninpainting [43], everything is masked out. This is similar to\ndropping out the text prompt with some probability to enable\nclassifier-free guidance. Because of this training strategy, we\ncan enable image-guidance with a formulation similar to In-\nstructPix2Pix [4]. We modify the text-conditioned diffusion\ninpainting model to predict its score estimate in the form:\n\u03f5\u03d5(zt, t, c) = \u03f5\u03d5(zt, t, {cI, \u2205})\n+sI \u00b7 (\u03f5\u03d5(zt, t, {cI, cT }) \u2212 \u03f5\u03d5(zt, t, {cI, \u2205}))\n(3)\n+sT \u00b7 (\u03f5\u03d5(zt, t, {cI, \u2205}) \u2212 \u03f5\u03d5(zt, t, {\u2205, \u2205}))\nwith zt as the noisy latent at time t, cI and cT as conditioning\nfor image & mask, and text respectively. \u2205 means no text\nor completely masking out the image. sI and sT are image\nand text guidance scales, respectively. We use sI > 0 and\nsT = 0 to make the model conditioned only on the image and\nmasked region to inpaint. We note that the popular diffusers1\nlibrary doesn\u2019t enable setting sI > 0 out-of-the-box, so most\nworks use diffusion inpainting models by using text prompts\nto describe an inpaint. Trying to describe the scene, however,\ncan be difficult and using image only guidance (\u201cSD Image\nCond\u201d, where sI > 0 and sT = 0) tends to be more multi-\nview consistent (Figure 7).\nScheduler details. We use DDIM for our scheduler. We\nuse 20 steps whenever sampling SD [43], regardless of how\nmuch noise is added to a current render. This is similar\nto Instruct NeRF2NeRF [18]\u2019s dataset update procedure.\nThe model we use can be found here as \u201cstabilityai/stable-\ndiffusion-2-inpainting\u201d on Hugging Face.\n10.2. NeRF implementation details\nFor synthetic scenes and objects, we adopt recommended\npractices for training Nerfacto [48] which is to set the back-\nground color to either white or black, disable scene contrac-\ntion, and turn off the distortion loss. For the \u201cbackpack\u201d\nforward facing scene from SPIn-NeRF [36], we only turn\noff the distortion loss. Nerfacto is not tuned for LLFF [33]\nforward-facing scenes, so there may be some artifacts in this\ndataset compared to our datasets which have larger parallax.\nFor other scenes e.g., those that resemble an indoor room,\nwe train Nerfacto with default settings.\nFor the baselines besides \u201cMasked NeRF\u201d in Sec. 5.2\n\u201cCompleting large unknown 3D regions\u201d and Table 2, we\ntrain a modified Nerfacto that has losses on patches. Note\nthat \u201cMasked NeRF\u201d is simply Nerfacto, with any modifica-\ntions as described. The other baselines render 1024 patches\nof size 32\u00d732 per iteration. An L1 RGB loss and LPIPS [61]\nloss are applied to these patches, similar to IN2N [18]. Fur-\nthermore, we start the dataset update (DU) methods from the\nresult of \u201cMasked NeRF\u201d. Each method is trained for 30K\niterations, which is typical for Nerfacto.\n10.3. Evaluation against inpainted images\nWe use t \u2208 [0.4, 1.0] noise added to a render before sam-\npling SD and updating a training image. Because t = 0.4\nis the minimum noise, the inpainter has some freedom to\nchange the inpaint from the current NeRF render. If we\nadded no noise (t = 0.0) to the current NeRF render, then\nthe DU (dataset update) methods would be perfect on the\nNeRF metrics (PSNR, LPIPS, SSIM) because SD would\ndo nothing and the inpainted images would be exactly the\nNeRF rendered images. Nevertheless, quantifying inpaints\nwithout a ground truth result is challenging and our metrics\nrepresent an effort to show quantitative evaluation of 3D con-\nsistency. We recommend looking at the videos to compare\nthe methods qualitatively.\n1https://github.com/huggingface/diffusers\n10.4. Near plane for evaluation metrics\nWe render with a near plane slightly in front of the train-\ning images when reporting our metrics. This is important\nbecause NeRFs can cheat by placing \u201cfloaters\u201d in front of\nthe training images to explain away any discrepancies in the\nactual 3D scene compared to the training images. By setting\nthe near plane a bit beyond the camera origin, we can render\nthe true 3D scene without the floaters directly in front of\ntraining images.\n10.5. Novel-view video metrics\nFor the MUSIQ image quality metric, also used in [35],\nwe take all 300 frames (10 seconds, 30 FPS) of the novel-\nview videos, downsample each frame by 4x to capture low-\nfrequency structure, and run the MUSIQ [24] model (trained\non the AVA dataset) to obtain an image quality score. For\nthe Corrs metric, we use LoFTR [45] with a confidence\nthreshold of 0.8. We count the number of correspondences\nabove this confidence threshold for 100 random pairs of\nframes.\n10.6. Metrics for each scene\nWe provide the individual tables for Table 1 and Table 2 of\nthe paper. See the end of the document for these.\n11. Inpainting NeRF casual captures\nAs mentioned in our limitations section (Sec. 6), Stable\nDiffusion (SD) fails to produce good inpaints when the mask\ndistribution is similar to those produced by Bayes\u2019 Rays [17]\nor Nerfbusters [56]. In Fig. 12, we illustrate this. Please see\nthe caption for details.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMasked NeRF\n5.91\n0.70\n0.40\nLaMask\n21.13\n0.93\n0.23\nSD Text Cond\n13.27\n0.70\n0.40\nSD Image Cond\n13.71\n0.75\n0.31\nExtended Attention\n15.20\n0.78\n0.30\nGrid Prior\n14.90\n0.82\n0.27\nJoint Multi-View Inpainting\n16.59\n0.85\n0.24\nTable 3. 2D inpainting consistency for data \u201cchair\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMasked NeRF\n5.77\n0.66\n0.49\nLaMask\n16.79\n0.89\n0.39\nSD Text Cond\n11.17\n0.72\n0.29\nSD Image Cond\n12.30\n0.76\n0.27\nExtended Attention\n13.23\n0.76\n0.26\nGrid Prior\n12.64\n0.78\n0.27\nJoint Multi-View Inpainting\n13.25\n0.79\n0.27\nTable 4. 2D inpainting consistency for data \u201cdrums\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMasked NeRF\n6.86\n0.73\n0.37\nLaMask\n18.50\n0.91\n0.22\nSD Text Cond\n13.16\n0.74\n0.31\nSD Image Cond\n13.40\n0.76\n0.31\nExtended Attention\n14.86\n0.77\n0.23\nGrid Prior\n14.35\n0.81\n0.28\nJoint Multi-View Inpainting\n17.24\n0.85\n0.21\nTable 5. 2D inpainting consistency for data \u201cficus\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMasked NeRF\n8.84\n0.73\n0.36\nLaMask\n21.29\n0.92\n0.15\nSD Text Cond\n12.93\n0.75\n0.35\nSD Image Cond\n15.12\n0.78\n0.31\nExtended Attention\n15.06\n0.78\n0.30\nGrid Prior\n15.62\n0.84\n0.24\nJoint Multi-View Inpainting\n16.69\n0.86\n0.24\nTable 6. 2D inpainting consistency for data \u201chotdog\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMasked NeRF\n8.80\n0.73\n0.33\nLaMask\n17.84\n0.84\n0.18\nSD Text Cond\n13.01\n0.75\n0.27\nSD Image Cond\n14.79\n0.79\n0.22\nExtended Attention\n15.42\n0.79\n0.22\nGrid Prior\n14.84\n0.81\n0.21\nJoint Multi-View Inpainting\n17.89\n0.84\n0.18\nTable 7. 2D inpainting consistency for data \u201clego\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMasked NeRF\n8.28\n0.72\n0.33\nLaMask\n18.23\n0.88\n0.16\nSD Text Cond\n12.80\n0.74\n0.29\nSD Image Cond\n14.53\n0.78\n0.25\nExtended Attention\n12.85\n0.77\n0.29\nGrid Prior\n14.25\n0.80\n0.22\nJoint Multi-View Inpainting\n14.53\n0.80\n0.23\nTable 8. 2D inpainting consistency for data \u201cmaterials\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMasked NeRF\n6.94\n0.73\n0.35\nLaMask\n21.11\n0.92\n0.13\nSD Text Cond\n10.97\n0.72\n0.31\nSD Image Cond\n13.40\n0.77\n0.28\nExtended Attention\n14.26\n0.78\n0.27\nGrid Prior\n12.41\n0.80\n0.26\nJoint Multi-View Inpainting\n13.26\n0.81\n0.27\nTable 9. 2D inpainting consistency for data \u201cmic\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMasked NeRF\n10.66\n0.69\n0.35\nLaMask\n21.76\n0.82\n0.18\nSD Text Cond\n13.14\n0.72\n0.31\nSD Image Cond\n15.95\n0.74\n0.29\nExtended Attention\n15.69\n0.74\n0.29\nGrid Prior\n16.41\n0.78\n0.27\nJoint Multi-View Inpainting\n17.64\n0.80\n0.24\nTable 10. 2D inpainting consistency for data \u201cship\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMUSIQ \u2191\nCorrs \u2191\nMasked NeRF\n12.45\n0.81\n0.30\n3.82\n330\nLaMask\n27.43\n0.95\n0.03\n3.72\n179\nSD Image Cond\n17.42\n0.88\n0.15\n3.55\n305\nInpaint + DU\n25.42\n0.95\n0.06\n3.96\n260\nOurs w/o depth\n29.80\n0.96\n0.03\n3.91\n218\nOurs\n29.71\n0.96\n0.03\n3.92\n213\nTable 11. Quantitative NeRF baselines for data \u201ccat\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMUSIQ \u2191\nCorrs \u2191\nMasked NeRF\n19.16\n0.90\n0.15\n3.65\n164\nLaMask\n31.66\n0.96\n0.03\n3.66\n140\nSD Image Cond\n21.96\n0.89\n0.12\n3.62\n182\nInpaint + DU\n28.40\n0.94\n0.07\n3.78\n151\nOurs w/o depth\n29.76\n0.94\n0.06\n3.65\n154\nOurs\n29.74\n0.93\n0.07\n3.69\n146\nTable 12. Quantitative NeRF baselines for data \u201cturtle\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMUSIQ \u2191\nCorrs \u2191\nMasked NeRF\n14.20\n0.70\n0.34\n3.95\n1083\nLaMask\n25.45\n0.76\n0.09\n4.03\n1040\nSD Image Cond\n24.23\n0.77\n0.12\n4.00\n1024\nInpaint + DU\n25.15\n0.76\n0.13\n3.97\n1041\nOurs w/o depth\n26.68\n0.82\n0.12\n4.01\n1019\nOurs\n26.49\n0.81\n0.13\n4.02\n1038\nTable 13. Quantitative NeRF baselines for data \u201cdrawing\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMUSIQ \u2191\nCorrs \u2191\nMasked NeRF\n11.68\n0.79\n0.25\n3.81\n214\nLaMask\n24.50\n0.95\n0.05\n4.02\n195\nSD Image Cond\n16.45\n0.89\n0.12\n3.66\n196\nInpaint + DU\n20.42\n0.90\n0.10\n3.80\n228\nOurs w/o depth\n28.76\n0.96\n0.03\n3.84\n211\nOurs\n29.37\n0.96\n0.03\n3.86\n176\nTable 14. Quantitative NeRF baselines for data \u201cboot\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMUSIQ \u2191\nCorrs \u2191\nMasked NeRF\n16.86\n0.93\n0.10\n3.82\n245\nLaMask\n27.41\n0.96\n0.02\n3.71\n200\nSD Image Cond\n24.58\n0.95\n0.04\n3.70\n261\nInpaint + DU\n27.92\n0.95\n0.03\n3.70\n245\nOurs w/o depth\n28.20\n0.96\n0.03\n3.72\n259\nOurs\n28.13\n0.96\n0.03\n3.72\n264\nTable 15. Quantitative NeRF baselines for data \u201cbear\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMUSIQ \u2191\nCorrs \u2191\nMasked NeRF\n11.96\n0.75\n0.29\n3.80\n193\nLaMask\n27.73\n0.97\n0.04\n3.84\n148\nSD Image Cond\n19.07\n0.87\n0.15\n3.54\n235\nInpaint + DU\n28.76\n0.95\n0.05\n3.69\n210\nOurs w/o depth\n27.75\n0.95\n0.04\n3.63\n245\nOurs\n27.48\n0.95\n0.05\n3.62\n232\nTable 16. Quantitative NeRF baselines for data \u201cdumptruck\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMUSIQ \u2191\nCorrs \u2191\nMasked NeRF\n16.23\n0.70\n0.32\n3.65\n934\nLaMask\n27.25\n0.86\n0.09\n3.84\n865\nSD Image Cond\n21.58\n0.81\n0.14\n3.98\n852\nInpaint + DU\n27.75\n0.87\n0.10\n3.83\n812\nOurs w/o depth\n29.54\n0.90\n0.07\n3.65\n980\nOurs\n29.11\n0.88\n0.07\n3.69\n1059\nTable 17. Quantitative NeRF baselines for data \u201cnorway\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMUSIQ \u2191\nCorrs \u2191\nMasked NeRF\n16.41\n0.80\n0.19\n3.46\n1766\nLaMask\n24.82\n0.84\n0.04\n3.41\n1833\nSD Image Cond\n22.48\n0.83\n0.07\n3.45\n1769\nInpaint + DU\n24.04\n0.84\n0.06\n3.47\n1805\nOurs w/o depth\n24.38\n0.85\n0.04\n3.39\n1847\nOurs\n23.93\n0.83\n0.05\n3.40\n1924\nTable 18. Quantitative NeRF baselines for data \u201cbackpack\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMUSIQ \u2191\nCorrs \u2191\nMasked NeRF\n13.37\n0.69\n0.37\n3.50\n981\nLaMask\n25.63\n0.84\n0.11\n3.59\n1053\nSD Image Cond\n24.31\n0.83\n0.12\n3.66\n1023\nInpaint + DU\n27.13\n0.83\n0.13\n3.69\n1078\nOurs w/o depth\n28.87\n0.88\n0.09\n3.66\n1120\nOurs\n28.78\n0.88\n0.09\n3.66\n1137\nTable 19. Quantitative NeRF baselines for data \u201cbilliards\u201d.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMUSIQ \u2191\nCorrs \u2191\nMasked NeRF\n14.74\n0.75\n0.31\n3.65\n839\nLaMask\n32.02\n0.93\n0.03\n3.80\n779\nSD Image Cond\n28.26\n0.90\n0.07\n3.64\n799\nInpaint + DU\n31.05\n0.92\n0.05\n3.70\n766\nOurs w/o depth\n30.35\n0.93\n0.05\n3.68\n764\nOurs\n30.08\n0.93\n0.05\n3.69\n768\nTable 20. Quantitative NeRF baselines for data \u201coffice\u201d.\nNovel-view with Bayes\u2019 Rays masks\nScene\n\u201ctable\u201d\n(Nerfbusters)\nNo dilation\nDilated masks\n\u201ccouch\u201d\n(Our capture)\n\u201caloe\u201d\n(Nerfbusters)\nNo dilation\nDilated masks\nNo dilation\nDilated masks\nFigure 12. Out-of-distribution inpaints from Stable Diffusion when applied to casual captures. On the left, we show a training image\nfrom a casually captured scene. On the right, we show masks obtained by rendering a novel-view that has occlusions, and then running\nBayes\u2019 Rays [17] to delete areas with high uncertainty (marked in pink). We then inpaint these regions with image conditioning. When\nthe mask is not dilated (top rows), the inpaints have many artifacts such as ripple patterns and gray stretches. When the masks are dilated\n(bottom rows), the inpaints get slightly better but are no longer consistent with the known parts of the scene. This is a challenging setting to\naddress in future work. Retraining SD with masks of this distribution could alleviate the problem, but this is costly and out-of-scope of using\nan off-the-shelf model, as done in our work.\n"
  },
  {
    "title": "Gen2Det: Generate to Detect",
    "link": "https://arxiv.org/pdf/2312.04566.pdf",
    "upvote": "9",
    "text": "Gen2Det: Generate to Detect\nSaksham Suri1*\nFanyi Xiao2\nAnimesh Sinha2\nSean Chang Culatana2\nRaghuraman Krishnamoorthi2\nChenchen Zhu2\u2020\nAbhinav Shrivastava1\u2020\nUniversity of Maryland, College Park1\nMeta2\nAbstract\nRecently diffusion models have shown improvement in\nsynthetic image quality as well as better control in gener-\nation. We motivate and present Gen2Det, a simple modu-\nlar pipeline to create synthetic training data for object de-\ntection for free by leveraging state-of-the-art grounded im-\nage generation methods. Unlike existing works which gen-\nerate individual object instances, require identifying fore-\nground followed by pasting on other images, we simplify\nto directly generating scene-centric images. In addition to\nthe synthetic data, Gen2Det also proposes a suite of tech-\nniques to best utilize the generated data, including image-\nlevel filtering, instance-level filtering, and better training\nrecipe to account for imperfections in the generation. Us-\ning Gen2Det, we show healthy improvements on object de-\ntection and segmentation tasks under various settings and\nagnostic to detection methods. In the long-tailed detection\nsetting on LVIS, Gen2Det improves the performance on rare\ncategories by a large margin while also significantly im-\nproving the performance on other categories, e.g. we see\nan improvement of 2.13 Box AP and 1.84 Mask AP over\njust training on real data on LVIS with Mask R-CNN. In the\nlow-data regime setting on COCO, Gen2Det consistently\nimproves both Box and Mask AP by 2.27 and 1.85 points.\nIn the most general detection setting, Gen2Det still demon-\nstrates robust performance gains, e.g. it improves the Box\nand Mask AP on COCO by 0.45 and 0.32 points.\n1. Introduction\nRecent developments in generative modelling using diffu-\nsion models has drastically improved the generation qual-\nity. Works like LDM [35], DALL-E [32, 33], Imagen [38],\nParti [47] have shown the generation power which diffusion\nmodels possess. In addition to these models which can gen-\nerate high quality images given a text input, there have been\nmultiple developments in the direction of higher control\n*Work done during internship at Meta.\n\u2020*Equal Advisory Contribution.\nVanilla\nTraining\nImage\n+\nInstance\nFiltering\nSynthetic Data\nSpecific Training\nPrevious Works\nLabeled\nImages\nScene-centric\nGrounded Generation\nObject-centric\nGenerations\nExtracted FG\nMasks\nInstance Pasting on\nLabeled images\nGen2Det [Ours]\nFigure 1.\nExisting approaches which utilize synthetic data for\ndetection training follow a common methodology of generating\nobject-centric images and pasting instances on real images (top).\nGen2Det (middle) instead utilizes state-of-art grounded inpainting\ndiffusion model to directly generate scene-centric images. Further,\nGen2Det performs filtering to handle unsuitable generations. Fi-\nnally, during the detector training we introduce changes to handle\nfiltered generated data better. As a result, our method consistently\nimproves over vanilla training and the AP improvements increase\n(bottom) as the class becomes rare (i.e., long-tailed classes).\nin generation. Along this direction exist works which use\nconditional control like ControlNet [49] and GLIGEN [21].\nThere are also works like LoRA [18] and Dreambooth [36]\nwhich provide means to adapt these large models in a quick\nand efficient manner to generate specific kinds of images.\nWith these developments in both quality and control of syn-\nthetically generated images, it is only natural to come back\nto the question of \u201cHow to best utilize data generated from\nthese models for improving recognition performance?\u201d.\nMost previous works [15, 39, 42] have explored us-\n1\narXiv:2312.04566v1  [cs.CV]  7 Dec 2023\ning synthetic data from diffusion models for classification\nand pre-training tasks.\nThe goal of our work is to look\nat utilizing more realistic scene configurations by gener-\nating images conditioned on the existing layout of boxes\nand labels and use them for object detection and segmen-\ntation. A recent work, XPaste [50], explores this problem\nand utilizes techniques similar to simple copy-paste [12] to\npaste synthetically generated object-centric instances onto\nreal images for object detector training.\nHowever, their\nmethod relies on off-the-shelf segmentation methods built\nover CLIP [24, 29, 30, 40, 48] to extract masks, and is thus\nsubject to segmentation errors. In addition to the extra com-\nponents and compute, the generated images from XPaste are\nalso not realistic as they do not respect natural layouts due\nto random pasting as shown in Figure 1 (top). In contrast,\nas shown in Figure 1 (middle), Gen2Det leverages state-\nof-art diffusion models for grounded inpainting to generate\nscene-centric images which look more realistic. The goal\nof our approach is to show that utilizing such generated\ndata from state-of-art diffusion models can lead to improve-\nment in performance of object detection and segmentation\nmodels. By using a grounded inpainting diffusion model\nas our base generator, we are able to generate synthetic\nversions of common detection datasets like LVIS [13] and\nCOCO [23] in a layout conditioned manner. We then care-\nfully design a set of filtering and training strategies, with\nwhich we demonstrate that we can improve detection per-\nformance when training on the joint set of real and syn-\nthetic images. More specifically, we sample batches of syn-\nthetic and real images with a sampling probability. During\nloss computation we also modify the loss for synthetic data\nto account for filtered instances. Additionally, without uti-\nlizing any additional models or data (including segmenta-\ntion masks) during training, we show improvements in seg-\nmentation performance as a byproduct. The clear improve-\nment over vanilla training shown in Figure 1 (bottom) for\nclasswise Box AP with increasing rarity of classes makes a\nstrong case for such a pipeline especially in long-tailed or\nlow data regimes.\nWe emphasize the importance of the proposed filter-\ning and training techniques to incorporate synthetic data\nthrough our ablations, as we show that directly training on\nthe generated data or mixing it with real data in a naive man-\nner ends up hurting the performance. It is important to do\nthe proper filtering and loss modifications to mitigate short-\ncomings of the generations and let the model learn from the\nmix of real and synthetic data. We propose Gen2Det as a\ngeneral approach to utilize synthetic data for detector train-\ning. Due to its modular nature, different components can be\nupdated as the field progresses. This includes utilizing bet-\nter generators, filtering techniques, model architectures and\nalso training recipes. We highlight our contributions below:\n\u2022 Propose to use state-of-art grounded-inpainting models to\ngenerate synthetic data in a manner that respects the real-\nistic scene layouts.\n\u2022 Propose techniques to perform filtering at image and in-\nstance level along with changes in the detector training in\norder to utilize such synthetic data effectively and handle\nimperfections in generated instances.\n\u2022 Through our experiments we show consistent improve-\nment across datasets and architectures especially in long-\ntailed and low data settings. There is also considerable\ngains for rare categories.\n\u2022 We also provide quantitative and qualitative ablations\nto showcase the effect of different hyperparameters and\ncomponents of our approach which would help further re-\nsearch in usage of such synthetically generated data.\n2. Related Works\nDiffusion Models for Controllable Image Generation.\nDiffusion models have been shown to generate images with\nunprecedented high quality. Amongst these models a few\nof the popular models which can generate images from text\nprompts include LDM [35], DALL-E [32, 33], Imagen [38],\nand Parti [47].\nIn addition to just text guided genera-\ntion there have been further improvements in having more\ncontrollable generation and editing through different in-\nputs [3, 17, 21, 26, 28, 37, 44, 49]. One of the popular works\nControlNet [49] provides multiple ways to enable control\nincluding using edge maps, scribbles, segmentation masks\namongst other modalities.\nAnother work GLIGEN [21]\nbrings in more grounding control where they show image\ngeneration and inpainting conditioned on boxes, keypoints,\nHED maps, edge maps and semantic maps. We also uti-\nlize a similar pre-trained state-of-art diffusion model for\ngrounded inpainting.\nSynthetic Data for Detection. There has been prior non-\ndiffusion based work exploring the use of synthetic data for\ndetection. Some works have explored using image-based\nblending all the way to depth and semantics informed po-\nsitioning [11] for creating synthetic data. There have been\nother works which use computer graphics based approaches\nto render synthetic data by varying lighting and orienta-\ntions [31, 43]. Further there have been a line of augmen-\ntations belonging to the copy-paste family [8\u201310, 12] where\nobject instances are cropped and pasted on images at dif-\nferent locations. All these approaches end up repeating in-\nstances already present in the training set.\nLeveraging Synthetic Data from Diffusion Models. Due\nto its high quality generation and the flexibility in handling\nmultimodal data (e.g., vision and language), there has been\na lot of recent work in using pretrained diffusion models\nfor different tasks. Peekaboo [4] shows the use of diffusion\nmodels as zero shot segmentors while other works [6, 19]\nshows that diffusion models can act as zero shot classi-\nfiers.\nStableRep [42] on the other hand uses data gen-\n2\nSynthetic Data with Annotations\nImage\n+\nInstance\nLevel Filtering\nFiltered Synthetic Data with Annotations\nBox + Label Conditioned\nInpainting Diffusion Model\nDiffusion\nU-Net\nImages with Box + Label Annotations\nMasked Images for Inpainting\nModified Detector\nTraining\nClass\nBox\nMask\n Traditional Detector Training\nFigure 2. Gen2Det: our proposed pipeline for generating and utilizing synthetic data for object detection and segmentation.\nGen2Det starts by generating grounded inpainted images using state-of-art diffusion model. The generated images are then filtered at\nimage and instance level to remove globally bad images as well as individual instances which might be low quality or have hallucinations.\nFinally, we train object detection and segmentation models using the filtered data along with our improved training methodology by intro-\nducing sampling and background ignore.\nerated by diffusion models to train self-supervised mod-\nels. A few method also explore using diffusion model fea-\ntures [20, 25, 27, 41, 46] directly for downstream tasks.\nThere have also been works exploring the use of synthetic\ndata from diffusion models to improve image classifica-\ntion [1, 2, 15, 16, 39]. While the classification task has\nhad a lot of exploration with synthetic data due to the\nobject-centric nature of images which these models can\ngenerate easily, the detection task is less explored as it\u2019s\nharder to generate data with grounded annotations using\nthese models.\nA recent study [22] explored the use of\nsuch data in highly constrained few shot settings for detec-\ntion where the gains are expected. Recently, XPaste [50]\nlooked at more general benchmarks and showed consis-\ntent improvements using the Centernet2 [51] architecture.\nSpecifically, XPaste [50] uses diffusion models to generate\nobject-centric images and uses multiple CLIP [30] based\napproaches [24, 29, 40, 48] to extract segmentation maps\nwhich makes it slow and error-prone. Following the extrac-\ntion of instances and their segmentation maps, they use syn-\nthetic and real instances (retrieved from an additional real\ndataset) to perform copy-paste [12] augmentation to gen-\nerate synthetic data. Gen2Det on the other hand does not\nuse any additional CLIP based approaches. Rather, once\nwe generate the data we only train with bounding box la-\nbels but show improvements in both box and mask AP. Also\nin terms of training speed, we are 3.4\u00d7 faster compared to\nXPaste with the same configuration.\n3. Approach\n3.1. Overview\nThrough Gen2Det, we provide a modular way to generate\nand utilize synthetic data for object detection and instance\nsegmentation. As shown in Figure 2, we start by generating\ndata using state-of-art grounded image inpainting diffusion\nmodel. As the generations may not always be perfect we\nperform image level and instance level filtering. The im-\nage level filtering is performed using a pre-trained aesthetic\nclassifier [5] while the instance level filtering is performed\nusing a detector trained on the corresponding real data. Af-\nter performing filtering, the images are suitable for training\nusing our proposed training strategy. While we do not make\nany architectural changes to the detector to keep the pipeline\ngeneral enough, we perform a probability based sampling of\na batch to be composed of synthetic or real samples. Fur-\nther while computing the losses, we modify the negatives\ncorresponding to the synthetic data to be ignored from loss\ncomputation to account for the filtering we performed. We\nalso do not apply mask loss for synthetic samples as we\ndo not have the segmentation masks corresponding to them.\nFollowing these steps we are able to show consistent im-\nprovements in performance.\n3.2. Image Generation\nWe start our pipeline by generating synthetic images by uti-\nlizing a state-of-art grounded inpainting diffusion model.\nThis model is trained to support multiple kinds of input con-\nditions. We utilize the model trained for image inpainting\n3\nMethod\nBox\nMask\nAP\nAPr\nAPc\nAPf\nAP\nAPr\nAPc\nAPf\nVanilla (real only)\n33.80\n20.84\n32.84\n40.58\n29.98\n18.36\n29.64\n35.46\nVanilla (synth only)\n11.27\n6.54\n9.35\n15.51\n-\n-\n-\n-\nVanilla (synth+real)\n31.82\n20.68\n31.18\n37.42\n27.49\n18.35\n27.08\n31.96\nXPaste\n34.34\n21.05\n33.86\n40.71\n30.18\n18.77\n30.11\n35.28\nOurs\n34.70\n23.78\n33.71\n40.61\n30.82\n21.24\n30.32\n35.59\nTable 1. Comparisons on LVIS with baselines using Centernet2 architecture. We report\nthe overall AP and also AP for rare (APr), common (APc) and frequent (APf) classes\nfor both box and mask evaluations.\nMethod\nAPb\nAPm\nVanilla (real only)\n46.00\n39.8\nVanilla (synth only)\n24.51\n-\nVanilla (synth+real)\n44.35\n37.03\nXPaste\n46.60\n39.90\nOurs\n47.18\n40.44\nTable 2. Comparisons with baselines on\nCOCO dataset using Centernet2 architec-\nture. We report the Box AP (APb) and\nMask AP (APm).\nwith the image, boxes, and corresponding labels as input.\nSpecifically, for each image I in the dataset with box an-\nnotations B, we provide as input the image and the cor-\nresponding annotations to the diffusion model and ask it\nto inpaint the annotated regions with new instances of the\nsame class by providing the box label as input.\nAs in-\npainting model requires an image and box level text de-\nscription, for each box bi we use the class name <ci>\nas the box level prompt and a concatenation of strings\n<a c1, a c2, ... and a cn> as the image level prompt where\nn is the number of instances in the image. We show exam-\nples of images generated using this technique in Figure 3.\n3.3. Filtering\nThe images generated using the strategy described above\nmay not always contain good generations so it is important\nto filter out low quality generations before using them to\ntrain a detection model. We perform two levels of filtering.\nFirst we filter at image level followed by a more granular\ninstance level filtering. We describe each of these below.\n3.3.1\nImage Level Filtering\nThe first level of filtering we perform is an image level filter-\ning using a pretrained classifier which utilizes a CLIP+MLP\narchitecture as described in [5]. The model is trained to\npredict how pleasing the image looks visually by assigning\nan aesthetic score to each image. We utilize the publicly\navailable weights from [5] and pass every generated im-\nage through the classifier which returns an aesthetic score\nfor each image. Based on qualitatively analyzing images\nand computing the average aesthetic score for real images\n(from COCO) we set a threshold for aesthetic filtering \u03c4a.\nAny image with an aesthetic score less than \u03c4a is discarded\nand its annotations are removed. We show the effect of this\nfiltering by visualizing discarded samples in Figure 4.\n3.3.2\nDetector Filtering\nAs a more granular level of quality assurance, we also per-\nform an extra filtering step at the instance level. This filter-\ning is designed to remove annotations for specific generated\ninstances which do not have good generation quality. In or-\nder to perform this filtering we first train a detector on only\nthe real data. We then pass all the generated images through\nthe trained detector and store its predictions. Based on the\ndetectors predictions we evaluate whether a ground truth\nannotation corresponding to an inpainted region should be\nutilized for training or not. This step is important to han-\ndle poor quality/incorrect generations. To do this, for each\ngenerated image we iterate over all ground truth annotations\nused to generate it. For each annotation we go through all\npredictions and remove the ground truth annotation if there\nis no overlapping prediction with a score greater than \u03c4s and\nIoU greater than \u03c4iou. This helps us remove ground truth an-\nnotations corresponding to generation which are either bad\nquality as the pre-trained detector is not able to predict it\nwith even a low confidence threshold, or for which the re-\ngion where they are generated is not the ground truth region.\nWe show results of the kind of annotations removed using\nthis filtering in Figure 5.\n3.4. Model Training\nOnce the data is ready for use we describe our training\nstrategy to utilize the synthetic dataset along with the real\ndataset. It should be noted that for synthetically generated\nimages we do not have the segmentation masks for the in-\nstances so we do not apply the mask loss on those images.\n3.4.1\nBatch Sampling\nWe utilize both the real and synthetic data for training the\nfinal model. To do this, we need to figure out how to mix\nthe two datasets together in a way which lets us utilize the\nfull potential of the real data but at the same time extract\nas much useful information from the additional synthetic\ndata as possible. Naively using only the synthetic dataset\nand a naive combination of synthetic and real datasets dur-\ning training leads to drop in performance as shown in the\nablations.\nTherefore, to best use both the real and syn-\nthetic datasets, we end up defining a sampling probability\np. Specifically, a batch is chosen to comprise of completely\n4\nOriginal Image\nSynthetic Image\nSynthetic Image\nSynthetic Image\nOriginal Image\nSynthetic Image\nSynthetic Image\nSynthetic Image\nFigure 3. Examples of generations using the inpainting diffusion model on the COCO dataset. The first and fifth columns correspond to\nthe original COCO images and the rest of the columns are generations with different seeds. These generated images are then fed to our\nfiltering pipeline for further processing before use in training.\nsynthetically generated samples with the sampling proba-\nbility p during training. This sampling strategy interleaves\nbatches of synthetic and real data and as we will show leads\nto effective learning from both data sources.\n3.4.2\nBackground Ignore\nWhile the filtering we perform at both image and instance\nlevel can deal with bad quality generations, they also in-\ntroduce some noise especially for classes for which the de-\ntector itself has poor quality predictions. Essentially, the\ndetector filtering removes ground truth annotation corre-\nsponding to bad quality/incorrect generations but that does\nnot remove the bad instance itself from the image. Addi-\ntionally, the generative model could have also hallucinated\nmultiple instance of an object class leading to missing anno-\ntations. To counter the effect of both these scenarios we in-\ntroduce an ignore functionality during training. Essentially,\nfor both the region proposal network (RPN) and detector\nhead we ignore the background regions from the loss com-\nputation, if their fg/non-bg class prediction score is higher\nthan a threshold \u03c4i. This lets us train even in the presence\nof some bad quality regions without incorrectly penalizing\nthe detector for predicting objects at those locations. This is\nan important change in training to allow our model to effec-\ntively utilize synthetic images with incorrectly hallucinated\ninstances, as the instance level filtering only removes the\ninstance annotation but does not handle hallucinations.\nAdditionally, as stated above we do not utilize any ad-\nditional mask information for the synthetic data as we only\ninpaint using boxes. So while training we ignore the mask\nloss for the synthetic images. Even with this, as shown in\nour quantitative results, we see an improvement in both box\nand mask performance.\n4. Results\n4.1. Experimental Setting\nWe evaluate our approach on LVIS [13] and COCO [23]\ndatasets.\nThese datasets provide a standard benchmark\nfor object detection and instance segmentation approaches.\nLVIS is a long tailed dataset with 1203 classes utilizing\nthe same images as COCO which contains only 80 classes.\nThe long tailed nature of LVIS makes it especially appeal-\ning to showcase the use of synthetic data for the rare cate-\ngories. We report the Average Precision (AP) for both these\ndatasets and compare using the box AP (APb) as well as\nmask AP (APm). Additionally for LVIS, following stan-\ndard practice and the existing definition of rare, common,\nand frequent classes, we report the respective box and mask\nAPs for those subset of classes. Further, we also report re-\nsults on artificially created subsets of COCO to simulate the\nlow-data regime by using 1%, 5%, 10%, 20%, 30%, 40%\nand 50% randomly selected images from COCO.\n5\nFigure 4. Examples of samples discarded during the image level\nfiltering. As can be seen, image level filtering is able to remove\nimages with artifacts present at a global level.\nWe utilize the Detectron2 framework [45] for training all\nthe models and use the XPaste code to run their baselines.\nWe use the Centernet2 clone adapted for instance segmen-\ntation. For LVIS, we rerun XPaste to get the results cor-\nresponding to adding only additional synthetic data. For\nCOCO, we directly adopt their evaluation numbers due to\nthe lack of corresponding configs and details for reproduc-\ntion. Following their code we also utilize the ImageNet-\n22k [7] pretrained models for the Centernet2 experiments\nwhile using the ImageNet-1k [7] standard models from De-\ntectron2 for the rest of the experiments. We utilize the Mask\nR-CNN [14] architecture and the LVIS dataset for our ab-\nlations. The Mask R-CNN [14] and Faster R-CNN [34] are\nimplemented in the Detectron2 framework [45].\nFor data generation, we utilize a state-of-art diffusion\nmodel for grounded inpainting using the images from LVIS\nand COCO along with their annotations which contain the\nboxes and class labels. For aesthetic filtering, we utilize the\nopen source model available in their repository [5]. For aes-\nthetic filtering we set \u03c4a to 4.5 which is roughly the same\nas the average aesthetic score on real COCO images. For\ndetector filtering we set \u03c4s as 0.2 and \u03c4iou as 0.3 for LVIS.\nWe use the same \u03c4iou for COCO and set \u03c4s as 0.1. During\ntraining, we use a sampling probability p = 0.2. We set \u03c4i\nto 0 for both datasets thus effectively ignore all background\nregions corresponding to the synthetic data from the loss.\n4.2. Quantitative Results\n4.2.1\nComparison with Existing Works\nComparison on LVIS. We start by comparing our ap-\nproach to the existing works in Table 1 on the LVIS dataset\nwith CenterNet2 backbone. Over vanilla training we show\nthat our method improves the Box and Mask AP over rare\ncategories by 2.94 and 2.88 respectively with a 0.9 and\n0.84 Box and Mask AP improvement overall. Despite the\nfact that XPaste utilizes four different CLIP based mod-\nels [24, 29, 40, 48] to obtain segmentation masks and is also\n3.5\u00d7 slower to train compared to our approach on LVIS,\nwe are able to outperform XPaste by 2.73 Box AP and 2.47\nMask AP on the rare categories and by 0.36 and 0.64 Box\nand Mask AP across all categories. The huge gains in rare\ncategories and overall improvements highlight our methods\neffectiveness in both long tailed as well as general settings.\nComparison on COCO. Similar to LVIS, we compare on\nthe COCO benchmark in Table 2. On COCO too we show\nan improvement of 1.18 and 0.64 on Box and Mask AP over\nvanilla training. Compared to XPaste we improve by 0.58\nBox AP and 0.54 Mask AP. We note that compared to LVIS\nthe improvements are slightly lower here as LVIS is a more\nlong-tailed dataset where adding synthetic data shines.\nComparison on LD-COCO. In order to simulate a low-\ndata regime on COCO with fewer images and labels, we\ncreate random subsets of different percentages of COCO\nimages. In Table 4 we show results on the low-data (LD)\nversion of COCO. As can be seen for each row, we show\nconsistent improvement in both Box and Mask AP. On av-\nerage we improve the Box and Mask AP by 2.27 and 1.85\npoints respectively. This shows that adding synthetic data is\nespecially fruitful in both long tailed and low data regimes.\nComparison across backbones. We further show compar-\nison across architectures by comparing to performance with\nvanilla training using only the real data. As can be seen in\nTable 3 we show consistent improvement in overall box and\nmask AP, and as mentioned above, our method especially\nshines for the rare categories. For the MaskRCNN archi-\ntecture we show a substantial gain in performance with an\nimprovement of 2.13 Box AP and 1.84 Mask AP over just\ntraining on real data. The improvements on rare categories\nare even higher with a gain of 4.8 Box AP and 4.18 Mask\nAP, highlighting the efficacy of using synthetic data in the\nGen2Det pipeline for rare/long-tailed categories. For the\nsame experimental setting on COCO, please see our Sup-\nplementary Materials.\n4.2.2\nAblation\nEffect of different components. Table 5 summarizes the\neffect of incorporating different components in our pipeline.\nWe report the Box AP and Box APr for this analysis along\n6\nOriginal Image\nSynthetic Image\nOriginal Image\nSynthetic Image\nOriginal Image\nSynthetic Image\nFigure 5. Examples of ground-truth instance annotations discarded by the detector filtering highlighted in red.\nMethod\nBox\nMask\nAP\nAPr\nAPc\nAPf\nAP\nAPr\nAPc\nAPf\nFaster R-CNN\n21.39\n10.05\n19.46\n28.51\n-\n-\n-\n-\nFaster R-CNN (Ours)\n22.90\n12.78\n21.19\n29.27\n-\n-\n-\n-\nMask R-CNN\n22.29\n10.63\n20.15\n29.80\n21.83\n11.15\n20.42\n28.10\nMask R-CNN (Ours)\n24.42\n15.43\n22.63\n30.38\n23.67\n15.33\n22.62\n28.51\nCenternet2\n33.80\n20.84\n32.84\n40.58\n29.98\n18.36\n29.64\n35.46\nCenternet2 (Ours)\n34.70\n23.78\n33.71\n40.61\n30.82\n21.24\n30.32\n35.59\nTable 3. Results across different backbones on the LVIS dataset. We can see consistent improvement across various backbones with\nespecially higher gains on the rare categories.\nImage %\nVanilla\nGen2Det\nAPb\nAPm\nAPb\nAPm\n1\n11.76\n11.34\n14.84 (+3.08)\n13.89 (+2.55)\n2\n16.07\n15.36\n18.42 (+2.35)\n17.27 (+1.91)\n5\n19.52\n18.48\n22.91 (+3.38)\n21.17 (+2.69)\n10\n25.29\n23.35\n27.62 (+2.33)\n25.34 (+1.99)\n20\n29.32\n26.97\n31.76 (+2.44)\n29.03 (+2.06)\n30\n33.02\n30.37\n34.70 (+1.68)\n31.65 (+1.28)\n40\n34.39\n31.36\n36.09 (+1.69)\n32.67 (+1.30)\n50\n36.52\n33.20\n37.74 (+1.22)\n34.24 (+1.04)\nTable 4. Performance on the LD-COCO setting. The LD-COCO\ndata is a randomly selected subset of COCO with different per-\ncentages of images from the dataset.\nwith the respective improvement deltas introduced by each\ncomponent over the baseline. Row 1 and 2 correspond to\nonly training with the real and synthetic data respectively. It\ncan be noted that naively training on only the synthetic or a\nsimple combination of real and synthetic data (row 3) leads\nto a degradation in overall AP. Combining the two sets (row\n3) does lead to a marginal increase in Box APr due to the\naddition of more instances of the rare categories. Adding\nsynthetic data along with sampling (row 4) gives us a boost\nof 1.2 in overall Box AP and 2.58 for the rare categories.\nFurther, incorporating image level filtering which re-\nmoves bad quality images in their entirety gives us a im-\nprovement of 1.49 Box AP and 3.16 Box APr (row 5). Fol-\nlowing our filtering pipeline, performing detector filtering\non its own (row 6) does not work much better than just im-\nage level filtering as it ends up removing annotations cor-\nresponding to bad quality generations but the instances still\nremain in the images. Additionally we also try perform-\ning background loss ignore for the synthetic data without\ndetector filtering (row 7), and that again does not lead to\nan improvement on its own rather leads to a drop in per-\nformance. Finally, in row 8 we see that detector filtering\ncoupled with background ignore takes care of the discarded\nannotations and leads to a net total improvement of 2.13\nAP and 4.8 APr. It should be noted that detector filtering\nalone only removes certain annotations from training, us-\ning it with background ignore unlocks the full potential of\nsuch filtering and is important for training with synthetic\ndata which might not always be perfect.\nEffect of Longer training. We try to see the effect of longer\ntraining by increasing the number of iteration for the detec-\ntor training. We also adjust the learning rate steps propor-\ntionally for each experiment. As can be seen from Table 6a,\nstarting with the 1\u00d7 config and moving to 2\u00d7 gives us a\nboost as it is able to utilize the additional samples better but\nfurther longer training does not work that well and actually\nreduces performance as it starts overfitting to the data.\nEffect of Sampling Probability. Here we vary the sam-\npling probability p used during training which affects\nwhether the batch should be made from real or synthetic\n7\nRow Real Data Synth. Data Sampling Img. Filt. Det. Filt. Bg. Ignore Box AP\n\u2206AP\nBox APr \u2206APr\n1\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n22.29\n\u2212\n10.63\n\u2212\n2\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n7.67\n-15.25\n4.57\n-6.06\n3\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n20.75\n-1.54\n10.91\n0.28\n4\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n23.49\n1.2\n13.21\n2.58\n5\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n23.78\n1.49\n13.79\n3.16\n6\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n23.76\n1.47\n13.62\n2.99\n7\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n23.32\n1.03\n12.77\n2.14\n8\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n24.42\n2.13\n15.43\n4.8\nTable 5. Ablation of various components of proposed approach on LVIS using Mask R-CNN.\nConfig\n1\u00d7\n2\u00d7\n3\u00d7\n4\u00d7\nAP\n22.50 24.42 22.85\n22.31\n(a) Longer training schedule.\nProbability\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAP\n22.29 23.23 24.42 23.42 23.28 22.52\n(b) Effect of sampling probability.\nSynthetic Data\n0\u00d7\n1\u00d7\n2\u00d7\n3\u00d7\n4\u00d7\n5\u00d7\nAP\n22.29 23.55 24.42 23.67 23.82 23.82\n(c) Effect of generated data sample counts.\nTable 6. Ablation on different hyperparameters on the LVIS dataset with Mask R-CNN backbone.\ndata. We show the change in performance for different syn-\nthetic probabilities in Table 6b. As can be seen in all cases\nwe are better than the vanilla training. We do see that ini-\ntially increasing the sampling probability to 0.2 helps and\nimproves performance after which it decreases as the prob-\nability is further increased. This might be because the real\ndata which is pristine is seen lesser and lesser after a certain\nprobability value, and therefore we might get negatively im-\npacted by the noise in synthetic data.\nEffect of more synthetic samples. Similar to longer train-\ning, adding more synthetic samples also has a curved per-\nformance as shown in Table 6c. Adding 1\u00d7 data is better\nthan training on only real data. Further increasing the num-\nber of synthetic samples to 2\u00d7 improves the performance.\nIn our setting though, increasing them more leads to a re-\nduction in performance. Ideally, the goal would be to keep\nincreasing the performance with more synthetic data but we\nleave that for future work as it would require looking into\nimproving diversity of generations which will happen as the\ngenerators improve. Also adding more data under the cur-\nrent scheme ends up generating more images with the same\nlayout leading to a drop after a certain point. Increasing di-\nversity in layouts would be another way to improving per-\nformance further. Again, it should be noted that in all cases,\nour performance is sufficiently better than vanilla training.\n4.3. Qualitative Results\nWe show the outputs of different parts of pipeline qualita-\ntively on the COCO dataset. First, we show a few sam-\nples which are the synthetic images generated using the\npre-trained grounded inpainting diffusion model. Figure 3\nshows qualitative outputs using COCO images and annota-\ntions. The first column corresponds to the real image from\nCOCO dataset followed by multiple synthetic generations\neach with a different seed leading to variability in instances.\nWe highlight a few samples which are rejected from the\nimage level filtering step in Figure 4. As can be seen these\nare images which do not look good at an image level even\nthough some of the instances might look fine. The aesthetic\nfiltering steps removes such images. It can be seen for ex-\nample in the first image in first row, the bicycle generation\nin the front seems incorrect. Similarly, in the first image in\nthe second row, the bigger airplane which covers a major\npart of the image does not look realistic.\nFinally, in Figure 5 we show some examples of instances\ndiscarded by detector filtering. As can be seen in most cases\nthe detector filtering discards instances which have either\nbeen incorrectly generated or are bad quality generations.\nFor example in the first example in the first row, the gen-\neration does not correspond to the pizza rather a specific\ntopping. Similarly in the first example in the second row,\nthere is no airplane generated in the ground truth region.\n5. Conclusion\nWith the huge strides in image generation in terms of both\nquality and control, we try to tackle the problem of training\ndetection and segmentation models with synthetic data. Our\nproposed pipeline Gen2Det utilizes state-of-art grounded\ninpainting diffusion model to generate synthetic images\nwhich we further filter at both image and instance level be-\nfore using in training. We also introduce some changes in\nthe detector training to utilize the data better and take care\nof shortcoming which the data might pose. Along with de-\ntailed quantitative ablations and qualitative outputs we show\nthe efficacy of different components of our approach. Fi-\nnally, we show improvement across both LVIS and COCO\n8\nand show higher improvements on rare classes in the long\ntailed LVIS setting. Additionally, we show improvement in\nlow-data COCO setting too. Most interestingly, we show\nimprovement in segmentation performance without using\nany additional segmentation masks like existing works. We\nhope Gen2Det acts as a general modular framework which\ncan be benefit from future developments in both generation\nas well as detector training due to easily replaceable general\npurpose blocks which it comprises of.\nAcknowledgements\nWe thank Justin Johnson for insightful discussion on this\nwork. We also want to thank Vikas Chandra for helping\nwith the submission.\nReferences\n[1] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-\nhammad Norouzi, and David J Fleet. Synthetic data from\ndiffusion models improves imagenet classification.\narXiv\npreprint arXiv:2304.08466, 2023. 3\n[2] Hritik Bansal and Aditya Grover. Leaving reality to imag-\nination: Robust classification via generated datasets. arXiv\npreprint arXiv:2302.02503, 2023. 3\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n2\n[4] Ryan Burgert,\nKanchana Ranasinghe,\nXiang Li,\nand\nMichael S Ryoo. Peekaboo: Text to image diffusion models\nare zero-shot segmentors. arXiv preprint arXiv:2211.13224,\n2022. 2\n[5] Christophschuhmann.\nChristophschuhmann/improved-\naesthetic-predictor: Clip+mlp aesthetic score predictor. 3, 4,\n6\n[6] Kevin Clark and Priyank Jaini.\nText-to-image diffu-\nsion models are zero-shot classifiers.\narXiv preprint\narXiv:2303.15233, 2023. 2\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and\nLi Fei-Fei.\nImagenet: A large-scale hierarchical image\ndatabase. 2009 IEEE Conference on Computer Vision and\nPattern Recognition, pages 248\u2013255, 2009. 6\n[8] Nikita Dvornik, Julien Mairal, and Cordelia Schmid. Mod-\neling visual context is key to augmenting object detection\ndatasets. ArXiv, abs/1807.07428, 2018. 2\n[9] Debidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut,\npaste and learn: Surprisingly easy synthesis for instance de-\ntection. 2017 IEEE International Conference on Computer\nVision (ICCV), pages 1310\u20131319, 2017.\n[10] Haoshu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou,\nYong-Lu Li, and Cewu Lu. Instaboost: Boosting instance\nsegmentation via probability map guided copy-pasting. 2019\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 682\u2013691, 2019. 2\n[11] Georgios Georgakis, Arsalan Mousavian, Alexander C Berg,\nand Jana Kosecka. Synthesizing training data for object de-\ntection in indoor scenes.\nRobotics: Science and Systems\n(RSS), 2017. 2\n[12] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-\nYi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Sim-\nple copy-paste is a strong data augmentation method for in-\nstance segmentation. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n2918\u20132928, 2021. 2, 3\n[13] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5356\u20135364, 2019. 2, 5\n[14] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross B.\nGirshick. Mask r-cnn. 2017. 6\n[15] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing\nZhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic\ndata from generative models ready for image recognition?\narXiv preprint arXiv:2210.07574, 2022. 1, 3\n[16] Reyhane Askari Hemmat, Mohammad Pezeshki, Florian\nBordes, Michal Drozdzal, and Adriana Romero-Soriano.\nFeedback-guided data synthesis for imbalanced classifica-\ntion. arXiv preprint arXiv:2310.00158, 2023. 3\n[17] Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin Johnson,\nand Matthias Nie\u00dfner.\nText2room:\nExtracting textured\n3d meshes from 2d text-to-image models.\narXiv preprint\narXiv:2303.11989, 2023. 2\n[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 1\n[19] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis\nBrown, and Deepak Pathak. Your diffusion model is secretly\na zero-shot classifier.\narXiv preprint arXiv:2303.16203,\n2023. 2\n[20] Daiqing Li, Huan Ling, Amlan Kar, David Acuna, Se-\nung Wook Kim, Karsten Kreis, Antonio Torralba, and Sanja\nFidler.\nDreamteacher: Pretraining image backbones with\ndeep generative models. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 16698\u2013\n16708, 2023. 3\n[21] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 22511\u201322521, 2023. 1, 2\n[22] Shaobo Lin, Kun Wang, Xingyu Zeng, and Rui Zhao. Ex-\nplore the power of synthetic data on few-shot object detec-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 638\u2013647, 2023.\n3\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 2, 5\n9\n[24] Timo L\u00a8uddecke and Alexander Ecker.\nImage segmenta-\ntion using text and image prompts.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 7086\u20137096, 2022. 2, 3, 6\n[25] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holyn-\nski, and Trevor Darrell. Diffusion hyperfeatures: Searching\nthrough time and space for semantic correspondence. In Ad-\nvances in Neural Information Processing Systems, 2023. 3\n[26] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. arXiv preprint arXiv:2108.01073, 2021. 2\n[27] Soumik Mukhopadhyay, Matthew Gwilliam, Vatsal Agar-\nwal,\nNamitha\nPadmanabhan,\nArchana\nSwaminathan,\nSrinidhi Hegde, Tianyi Zhou, and Abhinav Shrivastava.\nDiffusion models beat gans on image classification. arXiv\npreprint arXiv:2307.08702, 2023. 3\n[28] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-\nElor, and Daniel Cohen-Or. Localizing object-level shape\nvariations with text-to-image diffusion models.\narXiv\npreprint arXiv:2303.11306, 2023. 2\n[29] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood De-\nhghan, Osmar Zaiane, and Martin Jagersand. U2-net: Going\ndeeper with nested u-structure for salient object detection.\npage 107404, 2020. 2, 3, 6\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 3\n[31] Param S. Rajpura, Ravi Sadananda Hegde, and Hristo Boji-\nnov. Object detection using deep cnns trained on synthetic\nimages. ArXiv, abs/1706.06782, 2017. 2\n[32] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n1, 2\n[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 1, 2\n[34] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 39:1137\u20131149, 2015. 6\n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 2\n[36] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 1\n[37] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi.\nPalette: Image-to-image diffusion models.\nIn\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u2013\n10, 2022. 2\n[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 1, 2\n[39] Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and\nYannis Kalantidis. Fake it till you make it: Learning trans-\nferable representations from synthetic imagenet clones. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 2023. 1, 3\n[40] Yukun Su, Jingliang Deng, Ruizhou Sun, Guosheng Lin, and\nQingyao Wu. A unified transformer framework for group-\nbased segmentation: Co-segmentation, co-saliency detection\nand video salient object detection, 2022. 2, 3, 6\n[41] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion.\narXiv preprint arXiv:2306.03881,\n2023. 3\n[42] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and\nDilip Krishnan. Stablerep: Synthetic images from text-to-\nimage models make strong visual representation learners.\narXiv preprint arXiv:2306.00984, 2023. 1, 2\n[43] Jonathan Tremblay, Aayush Prakash, David Acuna, Mark\nBrophy, V. Jampani, Cem Anil, Thang To, Eric Cameracci,\nShaad Boochoon, and Stan Birchfield. Training deep net-\nworks with synthetic data: Bridging the reality gap by do-\nmain randomization. 2018 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition Workshops (CVPRW),\npages 1082\u201310828, 2018. 2\n[44] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or.\nSketch-guided text-to-image diffusion models. In ACM SIG-\nGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.\n2\n[45] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019. 6\n[46] Weilai Xiang, Hongyu Yang, Di Huang, and Yunhong Wang.\nDenoising diffusion autoencoders are unified self-supervised\nlearners. arXiv preprint arXiv:2303.09769, 2023. 3\n[47] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2(3):5, 2022. 1, 2\n[48] Yi Ke Yun and Weisi Lin. Selfreformer: Self-refined network\nwith transformer for salient object detection. arXiv preprint\narXiv:2205.11283, 2022. 2, 3, 6\n[49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 1, 2\n10\n[50] Hanqing Zhao, Dianmo Sheng, Jianmin Bao, Dongdong\nChen, Dong Chen, Fang Wen, Lu Yuan, Ce Liu, Wenbo\nZhou, Qi Chu, et al.\nX-paste: Revisiting scalable copy-\npaste for instance segmentation using clip and stablediffu-\nsion. 2023. 2, 3\n[51] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00a8ahenb\u00a8uhl.\nProbabilistic\ntwo-stage\ndetection.\narXiv\npreprint\narXiv:2103.07461, 2021. 3\n11\nGen2Det: Generate to Detect\nSupplementary Material\nMethod\nAPb\nAPm\nFaster R-CNN\n40.20\n-\nFaster R-CNN (Ours)\n40.56\n-\nMask R-CNN\n41.08\n37.14\nMask R-CNN (Ours)\n41.53\n37.46\nCenterNet2\n46.00\n39.8\nCenterNet2 (Ours)\n47.18\n40.44\nTable 7. Comparisons across architectures on COCO dataset. We\nreport the Box AP (APb) and Mask AP (APm).\n6. Experimental Details\nIn addition to the details mentioned in the main paper we\nwould like to highlight that we were not able to reproduce\nthe exact results of XPaste from the code and configs pro-\nvided and it is an open issue in their Github repository. Ad-\nditionally, for learning rate for Faster and Mask R-CNN ex-\nperiments we use the default LR of 0.08 which is scaled\nfrom 0.02 as we use 4\u00d7 the batch size as the default con-\nfigs. For LVIS, this LR was not stable so we use an LR\nof 0.04 accross all Faster and Mask R-CNN experiments.\nFor LD-COCO we do not perform filtering or background\nignore due to already low number of images and instances.\n7. Quantitative Results on COCO\nIn Table 7 we show improvements in both Box (APb) and\nMask (APm) AP across Faster R-CNN, Mask R-CNN and\nCenternet2 architectures on the COCO dataset.\n8. Pasting Object Centric Instances\nWe use state-of-art text to image diffusion model to gener-\nate object centric images for each instance annotation and\npaste them on corresponding LVIS images in a layout satis-\nfying manner using the box coordinates. We use the prompt\na photo of < c > where c is the class name for that in-\nstance.\nWe then train the detector with vanilla training\nand sampling synthetic and real data with equal probabil-\nities. This gives us a Box AP of 22.50 and Mask AP of\n21.45 which is lower than the performance obtained through\nGen2Det where we get 24.42 Box AP and 23.67 Mask AP.\n9. Ablations\n9.1. Ablation on \u03c4s\nWe vary \u03c4s which is the score threshold we use for detec-\ntor filtering. For this experiment we keep \u03c4iou fixed at 0.3.\nWe show the results for this in Table 8. It can be seen that\ninitially increasing the score threshold helps but after 0.2 in-\ncreasing it further leads to a reduction in performance. This\nmight be because we filter out too many annotations.\n9.2. Ablation on \u03c4iou\nWe vary \u03c4iou which is the score threshold we use for detec-\ntor filtering. For this experiment we keep \u03c4s fixed at 0.2.\nWe show the results for this in Table 9. It can be seen that\ninitially increasing the IoU threshold helps but after 0.3 in-\ncreasing it further leads to a reduction in performance. This\nmight be because we filter out too many annotations.\n9.3. Ablation on \u03c4i\nWe vary \u03c4i which is the score threshold we use for back-\nground ignore. We show the results for this in Table 10.\nIt can be seen that initially setting the score threshold to 0\nwhich ends up ignoring all background instances from the\nsynthetic data works the best. After that increasing it re-\nduces it with sporadic increases at certain thresholds.\n10. Qualitative Results\n10.1. Qualitative Inpainted Generation\nWe show qualitative generation on the LVIS dataset in Fig-\nures 6 and 7. These generations utilize the LVIS annota-\ntions and images along with state-of-the-art grounded in-\npainting model to generate the instances conditioned on the\nclass labels, box labels, and input images.\nWe also show qualitative generation on the COCO\ndataset in Figures 8 and 9 utilizing COCO annotations and\nimages to generate them.\n10.2. Qualitative Results for Image Filtering\nIn Figure 10 we show results of the image level filtering by\nvisualizing images discarded during this step on the LVIS\ndataset. It can be seen there are high level issues in the\ngenerations. In the third image in row 1 the sign seems\nto have some weird patterns. In row 3 the second image\nhas low quality aircraft generations. Similarly in the second\nimage in the last row the cat generation is not great.\nWe also show more image filtering results on the COCO\ndataset and visualize the discarded images in Figure 11.\n10.3. Qualitative Results for Detector Filtering\nWe present some examples for detector filtering on LVIS in\nFigure 12. We show the original and generated instances for\neach of the images and show the ground truth instance an-\nnotation(s) being discarded for each image in red. We also\nshow examples of detector filtering on COCO in Figure 13.\n12\n\u03c4s\nBox\nMask\nAP\nAPr\nAPc\nAPf\nAP\nAPr\nAPc\nAPf\n0.1\n23.85\n14.04\n21.89\n30.36\n23.16\n14.21\n21.92\n28.49\n0.2\n24.42\n15.43\n22.63\n30.38\n23.67\n15.33\n22.62\n28.51\n0.3\n23.59\n13.21\n21.74\n30.21\n22.89\n13.50\n21.66\n28.39\n0.4\n23.75\n13.48\n21.97\n30.25\n23.03\n13.81\n21.86\n28.38\n0.5\n23.62\n13.69\n21.56\n30.28\n23.05\n14.24\n21.75\n28.36\nTable 8. Comparisons across different \u03c4s keeping \u03c4iou fixed at 0.3 on the LVIS dataset. We report the overall AP and also AP for rare\n(APr), common (APc) and frequent (APf) classes for both box and mask evaluations.\n\u03c4iou\nBox\nMask\nAP\nAPr\nAPc\nAPf\nAP\nAPr\nAPc\nAPf\n0.1\n23.49\n12.90\n21.70\n30.15\n22.82\n12.90\n21.77\n28.35\n0.2\n23.56\n12.32\n22.05\n30.18\n22.88\n12.58\n22.08\n28.31\n0.3\n24.42\n15.43\n22.63\n30.38\n23.67\n15.33\n22.62\n28.51\n0.4\n23.73\n13.09\n22.02\n30.32\n23.12\n13.40\n22.13\n28.49\n0.5\n23.77\n13.22\n21.89\n30.51\n22.91\n13.33\n21.58\n28.60\nTable 9. Comparisons across different \u03c4iou keeping \u03c4s fixed at 0.2 on the LVIS dataset. We report the overall AP and also AP for rare\n(APr), common (APc) and frequent (APf) classes for both box and mask evaluations.\n\u03c4i\nBox\nMask\nAP\nAPr\nAPc\nAPf\nAP\nAPr\nAPc\nAPf\n0.0\n24.42\n15.43\n22.63\n30.38\n23.67\n15.33\n22.62\n28.51\n0.1\n23.89\n14.06\n21.88\n30.46\n23.21\n14.27\n21.92\n28.57\n0.2\n23.92\n13.82\n22.08\n30.41\n23.34\n14.63\n22.14\n28.50\n0.3\n23.70\n13.27\n21.86\n30.34\n23.02\n13.32\n21.95\n28.49\n0.4\n24.06\n14.83\n22.01\n30.39\n23.38\n15.15\n22.07\n28.45\n0.5\n23.69\n12.31\n22.09\n30.46\n22.99\n12.61\n22.06\n28.60\nTable 10. Comparisons across different \u03c4i on the LVIS dataset. We report the overall AP and also AP for rare (APr), common (APc) and\nfrequent (APf) classes for both box and mask evaluations.\n13\nOriginal Image\nSynthetic Image\nSynthetic Image\nSynthetic Image\nFigure 6. Examples of generations using the inpainting diffusion model on the LVIS dataset. The first column corresponds to the original\nLVIS images and the rest of the columns are generations with different seeds. These generated images are then fed to our filtering pipeline\nfor further processing before use in training.\n14\nOriginal Image\nSynthetic Image\nSynthetic Image\nSynthetic Image\nFigure 7. Examples of generations using the inpainting diffusion model on the LVIS dataset. The first column corresponds to the original\nLVIS images and the rest of the columns are generations with different seeds. These generated images are then fed to our filtering pipeline\nfor further processing before use in training.\n15\nOriginal Image\nSynthetic Image\nSynthetic Image\nSynthetic Image\nFigure 8. Examples of generations using the inpainting diffusion model on the COCO dataset. The first column corresponds to the original\nCOCO images and the rest of the columns are generations with different seeds. These generated images are then fed to our filtering pipeline\nfor further processing before use in training.\n16\nOriginal Image\nSynthetic Image\nSynthetic Image\nSynthetic Image\nFigure 9. Examples of generations using the inpainting diffusion model on the COCO dataset. The first column corresponds to the original\nCOCO images and the rest of the columns are generations with different seeds. These generated images are then fed to our filtering pipeline\nfor further processing before use in training.\n17\nFigure 10. Examples of samples discarded during the image level filtering on LVIS. As can be seen, image level filtering is able to remove\nimages with artifacts present at a global level.\n18\nFigure 11. Examples of samples discarded during the image level filtering on COCO. As can be seen, image level filtering is able to remove\nimages with artifacts present at a global level.\n19\nOriginal Image\nSynthetic Image\nOriginal Image\nSynthetic Image\nFigure 12. Examples of ground-truth instance annotations discarded by the detector filtering highlighted in red for LVIS dataset.\n20\nOriginal Image\nSynthetic Image\nOriginal Image\nSynthetic Image\nFigure 13. Examples of ground-truth instance annotations discarded by the detector filtering highlighted in red for COCO dataset.\n21\n"
  },
  {
    "title": "DreamVideo: Composing Your Dream Videos with Customized Subject and Motion",
    "link": "https://arxiv.org/pdf/2312.04433.pdf",
    "upvote": "9",
    "text": "DreamVideo: Composing Your Dream Videos with\nCustomized Subject and Motion\nYujie Wei1\u2020\nShiwei Zhang2\nZhiwu Qing3\u2020\nHangjie Yuan4\u2020\nZhiheng Liu2\u2020\nYu Liu2\nYingya Zhang2\nJingren Zhou2\nHongming Shan1\u2217\n1 Fudan University\n2 Alibaba Group\n3 Huazhong University of Science and Technology\n4 Zhejiang University\nyjwei22@m.fudan.edu.cn,\nqzw@hust.edu.cn,\nhj.yuan@zju.edu.cn,\nhmshan@fudan.edu.cn,\n{zhangjin.zsw, pingzhi.lzh, ly103369, yingya.zyy, jingren.zhou}@alibaba-inc.com\na dog* is playing guitar\na sloth* is playing guitar on the moon\na monster* is playing guitar in the room\na person is playing guitar\ndog\nsloth plushie \nmonster toy\na dog* is lifting weights\na sloth* is lifting weights\nSubject\nMotion\na monster* is lifting weights on the road\na person is lifting weights\nFigure 1. Customized video generation results of our proposed DreamVideo with specific subjects (left) and motions (top). Our method\ncan customize both subject identity and motion pattern to generate desired videos with various context descriptions.\nAbstract\nCustomized generation using diffusion models has made\nimpressive progress in image generation, but remains un-\nsatisfactory in the challenging video generation task, as it\n\u2217Corresponding author.\n\u2020Work done during the internships at Alibaba Group.\nThis work is supported by Alibaba DAMO Academy through Al-\nibaba Research Intern Program.\nrequires the controllability of both subjects and motions.\nTo that end, we present DreamVideo, a novel approach to\ngenerating personalized videos from a few static images\nof the desired subject and a few videos of target motion.\nDreamVideo decouples this task into two stages, subject\nlearning and motion learning, by leveraging a pre-trained\nvideo diffusion model. The subject learning aims to accu-\nrately capture the fine appearance of the subject from pro-\nvided images, which is achieved by combining textual in-\n1\narXiv:2312.04433v1  [cs.CV]  7 Dec 2023\nversion and fine-tuning of our carefully designed identity\nadapter. In motion learning, we architect a motion adapter\nand fine-tune it on the given videos to effectively model the\ntarget motion pattern.\nCombining these two lightweight\nand efficient adapters allows for flexible customization of\nany subject with any motion. Extensive experimental results\ndemonstrate the superior performance of our DreamVideo\nover the state-of-the-art methods for customized video gen-\neration. Our project page is at https://dreamvideo-\nt2v.github.io.\n1. Introduction\nThe remarkable advances in diffusion models [5, 27, 44,\n51, 59] have empowered designers to generate photoreal-\nistic images and videos based on textual prompts, paving\nthe way for customized content generation [17, 52]. While\ncustomized image generation has witnessed impressive\nprogress [2, 35, 41, 42], the exploration of customized video\ngeneration remains relatively limited. The main reason is\nthat videos have diverse spatial content and intricate tempo-\nral dynamics simultaneously, presenting a highly challeng-\ning task to concurrently customize these two key factors.\nCurrent existing methods [47, 70] have effectively pro-\npelled progress in this field, but they are still limited to op-\ntimizing a single aspect of videos, namely spatial subject\nor temporal motion. For example, Dreamix [47] and Tune-\nA-Video [70] optimize the spatial parameters and spatial-\ntemporal attention to inject a subject identity and a target\nmotion, respectively. However, focusing only on one aspect\n(i.e., subject or motion) may reduce the model\u2019s general-\nization on the other aspect. On the other hand, Animate-\nDiff [21] trains temporal modules appended to the person-\nalized text-to-image models for image animation. It tends\nto pursue generalized video generation but suffers from a\nlack of motion diversity, such as focusing more on camera\nmovements, making it unable to meet the requirements of\ncustomized video generation tasks well. Therefore, we be-\nlieve that effectively modeling both spatial subject and tem-\nporal motion is necessary to enhance video customization.\nThe above observations drive us to propose the\nDreamVideo, which can synthesize videos featuring the\nuser-specified subject endowed with the desired motion\nfrom a few images and videos respectively, as shown in\nFig. 1.\nDreamVideo decouples video customization into\nsubject learning and motion learning, which can reduce\nmodel optimization complexity and increase customization\nflexibility. In subject learning, we initially optimize a tex-\ntual identity using Textual Inversion [17] to represent the\ncoarse concept, and then train a carefully designed iden-\ntity adapter with the frozen textual identity to capture fine\nappearance details from the provided static images.\nIn\nmotion learning, we design a motion adapter and train it\non the given videos to capture the inherent motion pat-\ntern.\nTo avoid the shortcut of learning appearance fea-\ntures at this stage, we incorporate the image feature into\nthe motion adapter to enable it to concentrate exclusively\non motion learning. Benefiting from these two-stage learn-\ning, DreamVideo can flexibly compose customized videos\nwith any subject and any motion once the two lightweight\nadapters have been trained.\nTo validate DreamVideo, we collect 20 customized sub-\njects and 30 motion patterns as a substantial experimen-\ntal set. The extensive experimental results unequivocally\nshowcase its remarkable customization capabilities surpass-\ning the state-of-the-art methods.\nIn summary, our main contributions are:\n1. We propose DreamVideo, a novel approach for cus-\ntomized video generation with any subject and motion.\nTo the best of our knowledge, this work makes the first\nattempt to customize both subject identity and motion.\n2. We propose to decouple the learning of subjects and mo-\ntions by the devised identity and motion adapters, which\ncan greatly improve the flexibility of customization.\n3. We conduct extensive qualitative and quantitative ex-\nperiments, demonstrating the superiority of DreamVideo\nover the existing state-of-the-art methods.\n2. Related Work\nText-to-video generation. Text-to-video generation aims\nto generate realistic videos based on text prompts and has\nreceived growing attention [8, 10, 15, 23, 30, 34, 37, 45, 76].\nEarly works are mainly based on Generative Adversarial\nNetworks (GANs) [4, 54, 57, 62, 63, 66] or autoregres-\nsive transformers [18, 31, 36, 75]. Recently, to generate\nhigh-quality and diverse videos, many works apply the dif-\nfusion model to video generation [1, 16, 19, 24, 33, 40, 49,\n65, 68, 72, 79, 81, 82]. Make-A-Video [56] leverages the\nprior of the image diffusion model to generate videos with-\nout paired text-video data. Video Diffusion Models [29]\nand ImagenVideo [28] model the video distribution in pixel\nspace by jointly training from image and video data. To\nreduce the huge computational cost, VLDM [6] and Mag-\nicVideo [84] apply the diffusion process in the latent space,\nfollowing the paradigm of LDMs [51]. Towards control-\nlable video generation, ModelScopeT2V [64] and Video-\nComposer [67] incorporate spatiotemporal blocks with vari-\nous conditions and show remarkable generation capabilities\nfor high-fidelity videos. These powerful video generation\nmodels pave the way for customized video generation.\nCustomized generation. Compared with general genera-\ntion tasks, customized generation may better accommodate\nuser preferences. Most current works focus on subject cus-\ntomization with a few images [11, 13, 22, 53, 55, 58, 69].\nTextual Inversion [17] represents a user-provided subject\nthrough a learnable text embedding without model fine-\ntuning. DreamBooth [52] tries to bind a rare word with\n2\na dog* is playing guitar\nInference\nSubject Learning\nMotion Learning\nSpatial and Temporal Conv (frozen)\nSpatial Transformer with ID adapter\nTemporal Transformer with motion adapter\nSpatial Transformer (frozen)\nTemporal Transformer (frozen)\n+\nSelf Attn\nMotion Adapter\nFFN\nMotion Adapter\n+\n+\nSelf Attn\nCross Attn\nID Adapter\nFFN\n\u2026\n\u2026\n\u2026\n\u2026\nRandom \nFrame\na dog*\n\u2026\n\u2026\nImage\nEncoder\nStep 1\nStep 2\nVideo Diffusion Model\nVideo Diffusion Model\nAppearance guide\u8bd5\u8bd5\u52a0\u6743\u91cd\uff1f\nFigure 2. Illustration of the proposed DreamVideo, which decouples customized video generation into two stages. In subject learning,\nwe first optimize a unique textual identity for the subject, and then train the devised identity adapter (ID adapter) with the frozen textual\nidentity to capture fine appearance details. In motion learning, we pass a randomly selected frame from its training video through the CLIP\nimage encoder, and use its embedding as the appearance condition to guide the training of the designed motion adapter. Note that we\nfreeze the pre-trained video diffusion model throughout the training process. During inference, we combine the two lightweight adapters\nand randomly select an image provided during training as the appearance guidance to generate customized videos.\na subject by fully fine-tuning an image diffusion model.\nMoreover, some works study the more challenging multi-\nsubject customization task [14, 20, 35, 41, 42, 46, 73]. De-\nspite the significant progress in customized image genera-\ntion, customized video generation is still under exploration.\nDreamix [47] attempts subject-driven video generation by\nfollowing the paradigm of DreamBooth.\nHowever, fine-\ntuning the video diffusion model tends to overfitting and\ngenerate videos with small or missing motions. A concur-\nrent work [83] aims to customize the motion from train-\ning videos. Nevertheless, it fails to customize the subject,\nwhich may be limiting in practical applications. In contrast,\nthis work proposes DreamVideo to effectively generate cus-\ntomized videos with both specific subject and motion.\nParameter-efficient fine-tuning. Drawing inspiration from\nthe success of parameter-efficient fine-tuning (PEFT) in\nNLP [32, 38] and vision tasks [3, 12, 77, 78], some works\nadopt PEFT for video generation and editing tasks due to its\nefficiency [48, 74]. In this work, we explore the potential of\nlightweight adapters, revealing their superior suitability for\ncustomized video generation.\n3. Methodology\nIn this section, we first introduce the preliminaries of Video\nDiffusion Models. We then present DreamVideo to show-\ncase how it can compose videos with the customized subject\nand motion. Finally, we analyze the efficient parameters for\nsubject and motion learning while describing training and\ninference processes for our DreamVideo.\n3.1. Preliminary: Video Diffusion Models\nVideo diffusion models (VDMs) [6, 29, 64, 67] are designed\nfor video generation tasks by extending the image diffu-\nsion models [27, 51] to adapt to the video data. VDMs\nlearn a video data distribution by the gradual denoising of\na variable sampled from a Gaussian distribution. This pro-\ncess simulates the reverse process of a fixed-length Markov\nChain. Specifically, the diffusion model \u03f5\u03b8 aims to predict\nthe added noise \u03f5 at each timestep t based on text condition\nc, where t \u2208 U(0, 1). The training objective can be simpli-\nfied as a reconstruction loss:\nL = Ez,c,\u03f5\u223cN (0,I),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (zt, \u03c4\u03b8(c), t)\u22252\n2\ni\n,\n(1)\nwhere z \u2208 RB\u00d7F \u00d7H\u00d7W \u00d7C is the latent code of video\ndata with B, F, H, W, C being batch size, frame, height,\nwidth, and channel, respectively. \u03c4\u03b8 presents a pre-trained\ntext encoder.\nA noise-corrupted latent code zt from the\nground-truth z0 is formulated as zt = \u03b1tz0 + \u03c3t\u03f5, where\n\u03c3t =\np\n1 \u2212 \u03b12\nt , \u03b1t and \u03c3t are hyperparameters to con-\ntrol the diffusion process. Following ModelScopeT2V [64],\n3\n(a) Identity Adapter\n(b) Motion Adapter\nNonLinear\n+\nWup\nWdown\nNonLinear\n+\n+\nWup\nWdown\nWcond\nFigure 3. Illustration of the devised adapters. Both use a bottle-\nneck structure. Compared to identity adapter, motion adapter adds\na linear layer to incorporate the appearance guidance.\nwe instantiate \u03f5\u03b8(\u00b7, \u00b7, t) as a 3D UNet, where each layer in-\ncludes a spatiotemporal convolution layer, a spatial trans-\nformer, and a temporal transformer, as shown in Fig. 2.\n3.2. DreamVideo\nGiven a few images of one subject and multiple videos (or a\nsingle video) of one motion pattern, our goal is to generate\ncustomized videos featuring both the specific subject and\nmotion. To this end, we propose DreamVideo, which de-\ncouples the challenging customized video generation task\ninto subject learning and motion learning via two devised\nadapters, as illustrated in Fig. 2. Users can simply combine\nthese two adapters to generate desired videos.\nSubject learning. To accurately preserve subject identity\nand mitigate overfitting, we introduce a two-step training\nstrategy inspired by [2] for subject learning with 3\u223c5 im-\nages, as illustrated in the upper left portion of Fig. 2.\nThe first step is to learn a textual identity using Textual\nInversion [17]. We freeze the video diffusion model and\nonly optimize the text embedding of pseudo-word \u201cS\u2217\u201d us-\ning Eq. (1). The textual identity represents the coarse con-\ncept and serves as a good initialization.\nLeveraging only the textual identity is not enough to re-\nconstruct the appearance details of the subject, so further\noptimization is required. Instead of fine-tuning the video\ndiffusion model, our second step is to learn a lightweight\nidentity adapter by incorporating the learned textual iden-\ntity. We freeze the text embedding and only optimize the\nparameters of the identity adapter.\nAs demonstrated in\nFig. 3(a), the identity adapter adopts a bottleneck archi-\ntecture with a skip connection, which consists of a down-\nprojected linear layer with weight Wdown\n\u2208 Rl\u00d7d, a\nnonlinear activation function \u03c3, and an up-projected lin-\near layer with weight Wup \u2208 Rd\u00d7l, where l > d. The\nadapter training process for the input spatial hidden state\nCross-Attn\nSelf-Attn\nFFN\nOther\nSelf-Attn\nFFN\nOther\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n(b) Average weight change rate in temporal parameters\n(a) Average weight change rate in spatial parameters\nAverage weight change rate (mean \u2206)\nAverage weight change rate (mean \u2206)\nFigure 4. Analysis of weight change on updating all spatial or\ntemporal model weights during fine-tuning. We observe that cross-\nattention layers play a key role in subject learning while the con-\ntributions of all layers are similar to motion learning.\nht \u2208 RB\u00d7(F \u00d7h\u00d7w)\u00d7l can be formulated as:\nh\u2032\nt = ht + \u03c3 (ht \u2217 Wdown) \u2217 Wup,\n(2)\nwhere h, w, l are height, width, and channel of the hidden\nfeature map, h\u2032\nt is the output of identity adapter, and F = 1\nbecause only image data is used. We employ GELU [25]\nas the activation function \u03c3. In addition, we initialize Wup\nwith zeros to protect the pre-trained diffusion model from\nbeing damaged at the beginning of training [80].\nMotion learning.\nAnother important property of cus-\ntomized video generation is to make the learned subject\nmove according to the desired motion pattern from existing\nvideos. To efficiently model a motion, we devise a motion\nadapter with a structure similar to the identity adapter, as de-\npicted in Fig. 3(b). Our motion adapter can be customized\nusing a motion pattern derived from a class of videos (e.g.,\nvideos representing various dog motions), multiple videos\nexhibiting the same motion, or even a single video.\nAlthough the motion adapter enables capturing the mo-\ntion pattern, it inevitably learns the appearance of subjects\nfrom the input videos during training. To disentangle spa-\ntial and temporal information, we incorporate appearance\nguidance into the motion adapter, forcing it to learn pure\nmotion. Specifically, we add a condition linear layer with\nweight Wcond \u2208 RC\u2032\u00d7l to integrate appearance informa-\ntion into the temporal hidden state \u02c6ht \u2208 R(B\u00d7h\u00d7w)\u00d7F \u00d7l.\nThen, we randomly select one frame from the training video\nand pass it through the CLIP [50] image encoder to obtain\nits image embedding e \u2208 RB\u00d71\u00d7C\u2032. This image embed-\nding is subsequently broadcasted across all frames, serving\nas the appearance guidance during training. The forward\n4\nprocess of the motion adapter is formulated as:\n\u02c6he\nt = \u02c6ht + broadcast(e \u2217 Wcond),\n(3)\n\u02c6h\u2032\nt = \u02c6ht + \u03c3(\u02c6he\nt \u2217 Wdown) \u2217 Wup,\n(4)\nwhere \u02c6h\u2032\nt is the output of motion adapter. At inference time,\nwe randomly take a training image provided by the user as\nthe appearance condition input to the motion adapter.\n3.3. Model Analysis, Training and Inference\nWhere to put these two adapters. We address this ques-\ntion by analyzing the change of all parameters within the\nfine-tuned model to determine the appropriate position of\nthe adapters. These parameters are divided into four cat-\negories: (1) cross-attention (only exists in spatial parame-\nters), (2) self-attention, (3) feed-forward, and (4) other re-\nmaining parameters.\nFollowing [35, 39], we use \u2206l =\n\u2225\u03b8\u2032\nl \u2212 \u03b8l\u22252 / \u2225\u03b8l\u22252 to calculate the weight change rate of\neach layer, where \u03b8\u2032\nl and \u03b8l are the updated and pre-trained\nmodel parameters of layer l. Specifically, to compute \u2206 of\nspatial parameters, we only fine-tune the spatial parameters\nof the UNet while freezing temporal parameters, for which\nthe \u2206 of temporal parameters is computed in a similar way.\nWe observe that the conclusions are different for spatial\nand temporal parameters. Fig. 4(a) shows the mean \u2206 of\nspatial parameters for the four categories when fine-tuning\nthe model on \u201cChow Chow\u201d images (dog in Fig. 1). The\nresult suggests that the cross-attention layers play a cru-\ncial role in learning appearance compared to other parame-\nters. However, when learning motion dynamics in the \u201cbear\nwalking\u201d video (see Fig. 7), all parameters contribute close\nto importance, as shown in Fig. 4(b). Remarkably, our find-\nings remain consistent across various images and videos.\nThis phenomenon reveals the divergence of efficient param-\neters for learning subjects and motions. Therefore, we insert\nthe identity adapter to cross-attention layers while employ-\ning the motion adapter to all layers in temporal transformer.\nDecoupled training strategy. Customizing the subject and\nmotion simultaneously on images and videos requires train-\ning a separate model for each combination, which is time-\nconsuming and impractical for applications. Instead, we\ntend to decouple the training of subject and motion by opti-\nmizing the identity and motion adapters independently ac-\ncording to Eq. (1) with the frozen pre-trained model.\nInference.\nDuring inference, we combine the two cus-\ntomized adapters and randomly select an image provided\nduring training as the appearance guidance to generate cus-\ntomized videos. We find that choosing different images has\na marginal impact on generated results. Besides combina-\ntions, users can also customize the subject or motion indi-\nvidually using only the identity adapter or motion adapter.\n4. Experiment\n4.1. Experimental Setup\nDatasets.\nFor subject customization, we select subjects\nfrom image customization papers [20, 42, 52] for a total of\n20 customized subjects, including 9 pets and 11 objects. For\nmotion customization, we collect a dataset of 30 motion pat-\nterns from the Internet, the UCF101 dataset [61], the UCF\nSports Action dataset [60], and the DAVIS dataset [71]. We\nalso provide 42 text prompts used for extensive experimen-\ntal validation, where the prompts are designed to generate\nnew motions of subjects, new contexts of subjects and mo-\ntions, and etc.\nImplementation details.\nFor subject learning, we take\n\u223c3000 iterations for optimizing the textual identity fol-\nlowing [17, 42] with learning rate 1.0 \u00d7 10\u22124, and \u223c800\niterations for learning identity adapter with learning rate\n1.0 \u00d7 10\u22125. For motion learning, we train motion adapter\nfor \u223c1000 iterations with learning rate 1.0 \u00d7 10\u22125. Dur-\ning inference, we employ DDIM [59] with 50-step sam-\npling and classifier-free guidance [26] to generate 32-frame\nvideos with 8 fps. Additional details of our method and\nbaselines are reported in Appendix A.\nBaselines. Since there is no existing work for customiz-\ning both subjects and motions, we consider comparing\nour method with three categories of combination meth-\nods: AnimateDiff [21], ModelScopeT2V [64], and LoRA\nfine-tuning [32]. AnimateDiff trains a motion module ap-\npended to a pre-trained image diffusion model from Dream-\nbooth [52]. However, we find that training from scratch\nleads to unstable results. For a fair comparison, we further\nfine-tune the pre-trained weights of the motion module pro-\nvided by AnimateDiff and carefully adjust the hyperparam-\neters. For ModelScopeT2V and LoRA fine-tuning, we train\nspatial and temporal parameters/LoRAs of the pre-trained\nvideo diffusion model for subject and motion respectively,\nand then merge them during inference. In addition, we also\nevaluate our generation quality for customizing subjects and\nmotions independently.\nWe evaluate our method against\nTextual Inversion [17] and Dreamix [47] for subject cus-\ntomization while comparing with Tune-A-Video [70] and\nModelScopeT2V for motion customization.\nEvaluation metrics. We evaluate our approach with the\nfollowing four metrics, three for subject customization and\none for video generation. (1) CLIP-T calculates the average\ncosine similarity between CLIP [50] image embeddings of\nall generated frames and their text embedding. (2) CLIP-I\nmeasures the visual similarity between generated and tar-\nget subjects. We compute the average cosine similarity be-\ntween the CLIP image embeddings of all generated frames\nand the target images. (3) DINO-I [52], another metric for\nmeasuring the visual similarity using ViTS/16 DINO [7].\nCompared to CLIP, the self-supervised training model en-\n5\na person is skateboarding, front view\na dog eating grass from under the snow\ndog\ncat\n+\n+\nModelScope\nT2V\nDreamVideo\n(ours)\nAnimateDiff\nLoRA\n\u2026\na dog is skateboarding, front view\na cat eating grass from under the sand\nSubject\n+\nMotion\na dog lying on the grass \ndog\n+\nw/o motion \nadapter\nDreamVideo\n(ours)\nw/o textual\nidentity\nw/o appearance\nguidance\nSubject\n+\nMotion\nFigure 5. Qualitative comparison of customized video generation with both subjects and motions. DreamVideo accurately preserves\nboth subject identity and motion pattern, while other methods suffer from fusion conflicts to some extent. Note that the results of Animate-\nDiff are generated by fine-tuning its provided pre-trained motion module and appending it to a DreamBooth [52] model.\nDreamix\nDreamVideo\n(ours)\nTextual \nInversion\nSubject\na cat eating pizza\na monster dancing to upbeat music\na wolf running in the forest\nFigure 6. Qualitative comparison of subject customization. Our DreamVideo generates customized videos that preserve the precise\nsubject appearance while conforming to text prompts with various contexts.\ncourages distinguishing features of individual subjects. (4)\nTemporal Consistency [16], we compute CLIP image em-\nbeddings on all generated frames and report the average co-\nsine similarity between all pairs of consecutive frames.\n4.2. Results\nIn this section, we showcase results for both joint cus-\ntomization as well as individual customization of subjects\nand motions, further demonstrating the flexibility and ef-\nfectiveness of our method.\nArbitrary combinations of subjects and motions.\nWe\n6\nTune-A-Video\nDreamVideo\n(ours)\nModelScope\nT2V\nMotion\na person is biking (Multiple videos)\na bear walking on some rocks (Single video)\na cat walking on a field full of flowers\na bear is biking\nFigure 7. Qualitative comparison of motion customization between DreamVideo and other methods. Our approach effectively models\nspecific motion patterns while avoiding appearance coupling, and generates temporal coherent as well as diverse videos.\nMethod\nCLIP-T\nCLIP-I\nDINO-I\nT. Cons.\nPara.\nAnimateDiff [21]\n0.298\n0.657\n0.432\n0.982\n1.21B\nModelScopeT2V [64]\n0.305\n0.620\n0.365\n0.976\n1.31B\nLoRA\n0.286\n0.644\n0.409\n0.964\n6M\nDreamVideo\n0.314\n0.665\n0.452\n0.971\n85M\nTable 1. Quantitative comparison of customized video gener-\nation by combining different subjects and motions. \u201cT. Cons.\u201d\ndenotes Temporal Consistency. \u201cPara.\u201d means parameter number.\nMethod\nCLIP-T\nCLIP-I\nDINO-I\nT. Cons.\nPara.\nTextual Inversion [17]\n0.278\n0.668\n0.362\n0.961\n1K\nDreamix [47]\n0.284\n0.705\n0.459\n0.965\n823M\nDreamVideo\n0.295\n0.701\n0.475\n0.964\n11M\nTable 2. Quantitative comparison of subject customization.\ncompare our DreamVideo with several baselines to evalu-\nate the customization performance, as depicted in Fig. 5.\nWe observe that AnimateDiff preserves the subject appear-\nances but fails to model the motion patterns accurately, re-\nsulting in generated videos lacking motion diversity. Fur-\nthermore, ModelScopeT2V and LoRA suffer from fusion\nconflicts during combination, where either subject identi-\nties are corrupted or motions are damaged. In contrast, our\nDreamVideo achieves effective and harmonious combina-\ntions that the generated videos can retain subject identities\nand motions under various contexts; see Appendix B.1 for\nmore qualitative results about combinations of subjects and\nmotions.\nTab. 1 shows quantitative comparison results of all meth-\nMethod\nCLIP-T\nT. Cons.\nPara.\nModelScopeT2V [64]\n0.293\n0.971\n522M\nTune-A-Video [70]\n0.290\n0.967\n70M\nDreamVideo\n0.309\n0.975\n74M\nTable 3. Quantitative comparison of motion customization.\nods. DreamVideo outperforms other methods across CLIP-\nT, CLIP-I, and DINO-I, which is consistent with the visual\nresults. Although AnimateDiff achieves highest Temporal\nConsistency, it tends to generate videos with small motions.\nIn addition, our method remains comparable to Dreamix in\nTemporal Consistency but requires fewer parameters.\nSubject customization. To verify the individual subject\ncustomization capability of our DreamVideo, we conduct\nqualitative comparisons with Textual Inversion [17] and\nDreamix [47], as shown in Fig. 6.\nFor a fair compari-\nson, we employ the same baseline model, ModelScopeT2V,\nfor all compared methods. We observe that Textual Inver-\nsion makes it hard to reconstruct the accurate subject ap-\npearances. While Dreamix captures the appearance details\nof subjects, the motions of generated videos are relatively\nsmall due to overfitting. Moreover, certain target objects in\nthe text prompts, such as \u201cpizza\u201d in Fig. 6, are not generated\nby Dreamix. In contrast, our DreamVideo effectively mit-\nigates overfitting and generates videos that conform to text\ndescriptions while preserving precise subject appearances.\nThe quantitative comparison for subject customization\nis shown in Tab. 2. Regarding the CLIP-I and Temporal\n7\na person is skateboarding, front view\ndog\n+\nw/o motion \nadapter\nDreamVideo\n(ours)\nw/o textual\nidentity\nw/o appearance\nguidance\na dog is skateboarding, front view\nSubject\n+\nMotion\n\u2026\nFigure 8. Qualitative ablation studies on each component.\nConsistency, our method demonstrates a comparable perfor-\nmance to Dreamix while surpassing Textual Inversion. Re-\nmarkably, our DreamVideo outperforms alternative meth-\nods in CLIP-T and DINO-I with relatively few parameters.\nThese results demonstrate that our method can efficiently\nmodel the subjects with various contexts. Comparison with\nCustom Diffusion [35] and more qualitative results are re-\nported in Appendix B.2.\nMotion customization.\nBesides subject customization,\nwe also evaluate the motion customization ability of our\nDreamVideo by comparing it with several competitors, as\nshown in Fig. 7.\nFor a fair comparison, we only fine-\ntune the temporal parameters of ModelScopeT2V to learn a\nmotion. The results show that ModelScopeT2V inevitably\nfuses the appearance information of training videos, while\nTune-A-Video suffers from discontinuity between video\nframes. In contrast, our method can capture the desired mo-\ntion patterns while ignoring the appearance information of\nthe training videos, generating temporally consistent and di-\nverse videos; see Appendix B.3 for more qualitative results\nabout motion customization.\nAs shown in Tab. 3, our DreamVideo achieves the high-\nest CLIP-T and Temporal Consistency compared to base-\nlines, verifying the superiority of our method.\nUser study. To further evaluate our approach, we conduct\nuser studies for subject customization, motion customiza-\ntion, and their combinations respectively.\nFor combina-\ntions of specific subjects and motions, we ask 5 annota-\ntors to rate 50 groups of videos consisting of 5 motion pat-\nterns and 10 subjects. For each group, we provide 3\u223c5\nsubject images and 1\u223c3 motion videos; and compare our\nDreamVideo with three methods by generating videos with\nMethod\nText\nAlignment\nSubject\nFidelity\nMotion\nFidelity\nT.\nCons.\nours vs. AD [21]\n72.3 / 27.7\n62.3 / 37.7\n82.4 / 17.6\n64.2 / 35.8\nours vs. MS [64]\n62.4 / 37.6\n66.2 / 33.8\n56.6 / 43.4\n51.5 / 48.5\nours vs. LoRA\n82.8 / 17.2\n53.5 / 46.5\n83.7 / 16.3\n67.4 / 32.6\nTable 4. Human evaluations on customizing both subjects and\nmotions between our method and alternatives. \u201cAD\u201d and \u201cMS\u201d\nare short for AnimateDiff and ModelScopeT2V, respectively.\nMethod\nCLIP-T\nCLIP-I\nDINO-I\nT. Cons.\nw/o textual identity\n0.310\n0.657\n0.435\n0.968\nw/o motion adapter\n0.295\n0.701\n0.475\n0.964\nw/o appearance guidance\n0.305\n0.650\n0.421\n0.970\nDreamVideo\n0.314\n0.665\n0.452\n0.971\nTable 5. Quantitative ablation studies on each component.\n6 text prompts. We evaluate all methods with a majority\nvote from four aspects: Text Alignment, Subject Fidelity,\nMotion Fidelity, and Temporal Consistency. Text Align-\nment evaluates whether the generated video conforms to\nthe text description. Subject Fidelity and Motion Fidelity\nmeasure whether the generated subject or motion is close\nto the reference images or videos. Temporal Consistency\nmeasures the consistency between video frames. As shown\nin Tab. 4, our approach is most preferred by users regard-\ning the above four aspects. More details and user studies of\nsubject customization as well as motion customization can\nbe found in the Appendix B.4.\n4.3. Ablation Studies\nWe conduct an ablation study on the effects of each compo-\nnent in the following. More ablation studies on the effects\nof parameter numbers and different adapters are reported in\nAppendix C.\nEffects of each component. As shown in Fig. 8, we can\nobserve that without learning the textual identity, the gen-\nerated subject may lose some appearance details. When\nonly learning subject identity without our devised motion\nadapter, the generated video fails to exhibit the desired mo-\ntion pattern due to limitations in the inherent capabilities of\nthe pre-trained model. In addition, without proposed ap-\npearance guidance, the subject identity and background in\nthe generated video may be slightly corrupted due to the\ncoupling of spatial and temporal information. These results\ndemonstrate each component makes contributions to the fi-\nnal performance. More qualitative results can be found in\nAppendix C.1.\nThe quantitative results in Tab. 5 show that all met-\nrics decrease slightly without textual identity or appear-\nance guidance, illustrating their effectiveness. Furthermore,\nwe observe that only customizing subjects leads to the im-\nprovement of CLIP-I and DINO-I, while adding the motion\nadapter can increase CLIP-T and Temporal Consistency.\n8\nThis suggests that the motion adapter helps to generate tem-\nporal coherent videos that conform to text descriptions.\n5. Conclusion\nIn this paper, we present DreamVideo, a novel approach for\ncustomized video generation with any subject and motion.\nDreamVideo decouples video customization into subject\nlearning and motion learning to enhance customization flex-\nibility. We combine textual inversion and identity adapter\ntuning to model a subject and train a motion adapter with\nappearance guidance to learn a motion. With our collected\ndataset that contains 20 subjects and 30 motion patterns, we\nconduct extensive qualitative and quantitative experiments,\ndemonstrating the efficiency and flexibility of our method\nin both joint customization and individual customization of\nsubjects and motions.\nLimitations. Although our method can efficiently combine\na single subject and a single motion, it fails to generate cus-\ntomized videos that contain multiple subjects with multiple\nmotions. One possible solution is to design a fusion module\nto integrate multiple subjects and motions, or to implement\na general customized video model. We provide more anal-\nysis and discussion in Appendix D.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta,\nJia-Bin Huang, Jiebo Luo, and Xi Yin.\nLatent-\nShift:\nLatent diffusion with temporal shift for ef-\nficient text-to-video generation.\narXiv preprint\narXiv:2304.08477, 2023. 2\n[2] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel\nCohen-Or, and Dani Lischinski. Break-A-Scene: Ex-\ntracting multiple concepts from a single image. arXiv\npreprint arXiv:2305.16311, 2023. 2, 4\n[3] Hyojin\nBahng,\nAli\nJahanian,\nSwami\nSankara-\nnarayanan, and Phillip Isola. Exploring visual prompts\nfor adapting large-scale models.\narXiv preprint\narXiv:2203.17274, 2022. 3\n[4] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama\nChellappa, and Hans Peter Graf. Conditional gan with\ndiscriminative filter generation for text-to-video syn-\nthesis. In IJCAI, page 2, 2019. 2\n[5] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang.\nAnalytic-DPM: an analytic estimate of the optimal re-\nverse variance in diffusion probabilistic models. arXiv\npreprint arXiv:2201.06503, 2022. 2\n[6] Andreas Blattmann, Robin Rombach, Huan Ling,\nTim Dockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. Align your Latents: High-resolution\nvideo synthesis with latent diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 22563\u201322575,\n2023. 2, 3\n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e\nJ\u00b4egou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision\ntransformers. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 9650\u2013\n9660, 2021. 5\n[8] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.\nPix2Video: Video editing using image diffusion. In\nProceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 23206\u201323217, 2023.\n2, 17\n[9] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu.\nStableVideo: Text-driven consistency-aware diffusion\nvideo editing. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages\n23040\u201323050, 2023. 17\n[10] Haoxin Chen, Menghan Xia, Yingqing He, Yong\nZhang,\nXiaodong\nCun,\nShaoshu\nYang,\nJinbo\nXing, Yaofang Liu, Qifeng Chen, Xintao Wang,\net al.\nVideoCrafter1:\nOpen diffusion models for\nhigh-quality video generation.\narXiv preprint\narXiv:2310.19512, 2023. 2\n[11] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang\nDuan, Yuwei Zhou, and Wenwu Zhu.\nDisenBooth:\nDisentangled parameter-efficient tuning for subject-\ndriven text-to-image generation.\narXiv preprint\narXiv:2305.03374, 2023. 2\n[12] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu\nWang, Yibing Song, Jue Wang, and Ping Luo. Adapt-\nFormer: Adapting vision transformers for scalable vi-\nsual recognition. Advances in Neural Information Pro-\ncessing Systems, 35:16664\u201316678, 2022. 3\n[13] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui,\nXuhui Jia, Ming-Wei Chang, and William W Cohen.\nSubject-driven text-to-image generation via appren-\nticeship learning. arXiv preprint arXiv:2304.00186,\n2023. 2\n[14] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen,\nDeli Zhao, and Hengshuang Zhao. AnyDoor: Zero-\nshot object-level image customization. arXiv preprint\narXiv:2307.09481, 2023. 3\n[15] Zhongjie Duan, Lizhou You, Chengyu Wang, Cen\nChen, Ziheng Wu, Weining Qian, Jun Huang, Fei\nChao, and Rongrong Ji. DiffSynth: Latent in-iteration\ndeflickering for realistic video synthesis.\narXiv\npreprint arXiv:2308.03463, 2023. 2\n[16] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis. Struc-\nture and content-guided video synthesis with diffusion\nmodels.\nIn Proceedings of the IEEE/CVF Interna-\n9\ntional Conference on Computer Vision, pages 7346\u2013\n7356, 2023. 2, 6\n[17] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personal-\nizing text-to-image generation using textual inversion.\narXiv preprint arXiv:2208.01618, 2022. 2, 4, 5, 7, 14,\n15\n[18] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin,\nGuan Pang, David Jacobs, Jia-Bin Huang, and Devi\nParikh. Long video generation with time-agnostic vq-\ngan and time-sensitive transformer. In European Con-\nference on Computer Vision, pages 102\u2013118. Springer,\n2022. 2\n[19] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon,\nAndrew Tao, Bryan Catanzaro, David Jacobs, Jia-\nBin Huang, Ming-Yu Liu, and Yogesh Balaji.\nPre-\nserve Your Own Correlation: A noise prior for video\ndiffusion models. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages\n22930\u201322941, 2023. 2\n[20] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun\nShi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui\nZhao, Shuning Chang, Weijia Wu, et al. Mix-of-Show:\nDecentralized low-rank adaptation for multi-concept\ncustomization of diffusion models.\narXiv preprint\narXiv:2305.18292, 2023. 3, 5\n[21] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang,\nYu Qiao, Dahua Lin, and Bo Dai.\nAnimateD-\niff: Animate your personalized text-to-image diffu-\nsion models without specific tuning. arXiv preprint\narXiv:2307.04725, 2023. 2, 5, 7, 8, 14\n[22] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milan-\nfar, Dimitris Metaxas, and Feng Yang. SvDiff: Com-\npact parameter space for diffusion fine-tuning. arXiv\npreprint arXiv:2303.11305, 2023. 2\n[23] William Harvey, Saeid Naderiparizi, Vaden Masrani,\nChristian Weilbach, and Frank Wood.\nFlexible dif-\nfusion modeling of long videos.\nAdvances in Neu-\nral Information Processing Systems, 35:27953\u201327965,\n2022. 2\n[24] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan,\nand Qifeng Chen. Latent video diffusion models for\nhigh-fidelity video generation with arbitrary lengths.\narXiv preprint arXiv:2211.13221, 2022. 2\n[25] Dan Hendrycks and Kevin Gimpel. Gaussian error lin-\near units (gelus). arXiv preprint arXiv:1606.08415,\n2016. 4\n[26] Jonathan Ho and Tim Salimans. Classifier-free dif-\nfusion guidance.\narXiv preprint arXiv:2207.12598,\n2022. 5, 14\n[27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denois-\ning diffusion probabilistic models. Advances in neural\ninformation processing systems, 33:6840\u20136851, 2020.\n2, 3\n[28] Jonathan Ho, William Chan, Chitwan Saharia, Jay\nWhang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J\nFleet, et al.\nImagen Video: High definition video\ngeneration with diffusion models.\narXiv preprint\narXiv:2210.02303, 2022. 2\n[29] Jonathan Ho,\nTim Salimans,\nAlexey Gritsenko,\nWilliam Chan, Mohammad Norouzi, and David J.\nFleet.\nVideo diffusion models.\narXiv preprint\narXiv:2204.03458, 2022. 2, 3\n[30] Susung Hong,\nJunyoung Seo,\nSunghwan Hong,\nHeeseong Shin, and Seungryong Kim. Large language\nmodels are frame-level directors for zero-shot text-to-\nvideo generation. arXiv preprint arXiv:2305.14330,\n2023. 2\n[31] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogVideo: Large-scale pretraining\nfor text-to-video generation via transformers. arXiv\npreprint arXiv:2205.15868, 2022. 2\n[32] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. LoRA: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685,\n2021. 3, 5, 15\n[33] Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu,\nJingyi Yu, and Sibei Yang. Free-Bloom: Zero-shot\ntext-to-video generator with llm director and ldm ani-\nmator. arXiv preprint arXiv:2309.14494, 2023. 2\n[34] Levon Khachatryan, Andranik Movsisyan, Vahram\nTadevosyan, Roberto Henschel, Zhangyang Wang,\nShant Navasardyan, and Humphrey Shi. Text2Video-\nZero: Text-to-image diffusion models are zero-shot\nvideo generators. arXiv preprint arXiv:2303.13439,\n2023. 2\n[35] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu.\nMulti-concept cus-\ntomization of text-to-image diffusion. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 1931\u20131941, 2023. 2, 3, 5,\n8, 14, 15, 17, 18\n[36] Guillaume Le Moing, Jean Ponce, and Cordelia\nSchmid. CCVS: context-aware controllable video syn-\nthesis.\nAdvances in Neural Information Processing\nSystems, 34:14042\u201314055, 2021. 2\n[37] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fan-\nglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Er-\nrui Ding, and Jingdong Wang.\nVideoGen:\nA\nreference-guided latent diffusion approach for high\ndefinition text-to-video generation.\narXiv preprint\narXiv:2309.00398, 2023. 2\n10\n[38] Xiang Lisa Li and Percy Liang. Prefix-tuning: Op-\ntimizing continuous prompts for generation.\narXiv\npreprint arXiv:2101.00190, 2021. 3\n[39] Yijun Li, Richard Zhang, Jingwan Lu, and Eli Shecht-\nman. Few-shot image generation with elastic weight\nconsolidation.\narXiv preprint arXiv:2012.02780,\n2020. 5\n[40] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin,\nand Jiaya Jia. Video-P2P: Video editing with cross-\nattention control. arXiv preprint arXiv:2303.04761,\n2023. 2\n[41] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang,\nKecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou,\nand Yang Cao.\nCones: Concept neurons in diffu-\nsion models for customized generation. arXiv preprint\narXiv:2303.05125, 2023. 2, 3\n[42] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng\nZheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jin-\ngren Zhou, and Yang Cao. Cones 2: Customizable\nimage synthesis with multiple subjects. arXiv preprint\narXiv:2305.19327, 2023. 2, 3, 5, 14\n[43] Ilya\nLoshchilov\nand\nFrank\nHutter.\nDecou-\npled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017. 14\n[44] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen,\nChongxuan Li, and Jun Zhu.\nDPM-Solver: A fast\node solver for diffusion probabilistic model sampling\nin around 10 steps. Advances in Neural Information\nProcessing Systems, 35:5775\u20135787, 2022. 2\n[45] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan\nHuang, Liang Wang, Yujun Shen, Deli Zhao, Jingren\nZhou, and Tieniu Tan. VideoFusion: Decomposed dif-\nfusion models for high-quality video generation. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 10209\u2013\n10218, 2023. 2\n[46] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu.\nSubject-Diffusion: Open domain personalized text-to-\nimage generation without test-time fine-tuning. arXiv\npreprint arXiv:2307.11410, 2023. 3\n[47] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan,\nand Yedid Hoshen.\nDreamix:\nVideo diffusion\nmodels are general video editors.\narXiv preprint\narXiv:2302.01329, 2023. 2, 3, 5, 7, 14, 15\n[48] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and\nHongsheng Li.\nST-Adapter:\nParameter-efficient\nimage-to-video transfer learning. Advances in Neu-\nral Information Processing Systems, 35:26462\u201326477,\n2022. 3\n[49] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang\nLei, Xintao Wang, Ying Shan, and Qifeng Chen.\nFateZero: Fusing attentions for zero-shot text-based\nvideo editing. arXiv preprint arXiv:2303.09535, 2023.\n2\n[50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pages 8748\u20138763, 2021. 4, 5\n[51] Robin\nRombach,\nAndreas\nBlattmann,\nDominik\nLorenz, Patrick Esser, and Bj\u00a8orn Ommer.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels.\nIn Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages\n10684\u201310695, 2022. 2, 3, 14\n[52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael\nPritch,\nMichael Rubinstein,\nand Kfir Aberman.\nDreamBooth:\nFine tuning text-to-image diffusion\nmodels for subject-driven generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 22500\u201322510, 2023. 2, 5,\n6, 14, 16\n[53] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,\nTingbo Hou, Yael Pritch, Neal Wadhwa, Michael Ru-\nbinstein, and Kfir Aberman. HyperDreamBooth: Hy-\npernetworks for fast personalization of text-to-image\nmodels. arXiv preprint arXiv:2307.06949, 2023. 2\n[54] Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMoStGAN-V: Video generation with temporal motion\nstyles. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n5652\u20135661, 2023. 2\n[55] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon\nJung. InstantBooth: Personalized text-to-image gen-\neration without test-time finetuning.\narXiv preprint\narXiv:2304.03411, 2023. 2\n[56] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin,\nJie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, et al. Make-A-Video: Text-\nto-video generation without text-video data.\narXiv\npreprint arXiv:2209.14792, 2022. 2\n[57] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed\nElhoseiny. StyleGAN-V: A continuous video gener-\nator with the price, image quality and perks of style-\ngan2. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n3626\u20133636, 2022. 2\n[58] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang,\nTing Hua, Zsolt Kira, Yilin Shen, and Hongxia\nJin.\nContinual Diffusion: Continual customization\nof text-to-image diffusion with c-lora. arXiv preprint\narXiv:2304.06027, 2023. 2\n[59] Jiaming Song, Chenlin Meng, and Stefano Ermon.\n11\nDenoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020. 2, 5, 14\n[60] Khurram Soomro and Amir R Zamir. Action recogni-\ntion in realistic sports videos. In Computer vision in\nsports, pages 181\u2013208. Springer, 2015. 5\n[61] Khurram Soomro, Amir Roshan Zamir, and Mubarak\nShah.\nUCF101: A dataset of 101 human actions\nclasses from videos in the wild.\narXiv preprint\narXiv:1212.0402, 2012. 5\n[62] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski,\nXi Peng, Dimitris N Metaxas, and Sergey Tulyakov.\nA good image generator is what you need for\nhigh-resolution video synthesis.\narXiv preprint\narXiv:2104.15069, 2021. 2\n[63] Carl Vondrick, Hamed Pirsiavash, and Antonio Tor-\nralba. Generating videos with scene dynamics. Ad-\nvances in neural information processing systems, 29,\n2016. 2\n[64] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya\nZhang, Xiang Wang, and Shiwei Zhang.\nMod-\nelScope text-to-video technical report. arXiv preprint\narXiv:2308.06571, 2023. 2, 3, 5, 7, 8, 14, 15, 16\n[65] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He,\nJunchen Zhu, Jianlong Fu, and Jiaying Liu.\nVide-\noFactory:\nSwap attention in spatiotemporal diffu-\nsions for text-to-video generation.\narXiv preprint\narXiv:2305.10874, 2023. 2\n[66] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and\nJan Kautz.\nMoCoGAN: Decomposing motion and\ncontent for video generation.\nIn Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 1526\u20131535, 2018. 2\n[67] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou\nChen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli\nZhao, and Jingren Zhou. VideoComposer: Compo-\nsitional video synthesis with motion controllability.\narXiv preprint arXiv:2306.02018, 2023. 2, 3\n[68] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen\nZhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He,\nJiashuo Yu, Peiqing Yang, et al. LaVie: High-quality\nvideo generation with cascaded latent diffusion mod-\nels. arXiv preprint arXiv:2309.15103, 2023. 2\n[69] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai,\nLei Zhang, and Wangmeng Zuo.\nELITE: Encod-\ning visual concepts into textual embeddings for cus-\ntomized text-to-image generation.\narXiv preprint\narXiv:2302.13848, 2023. 2\n[70] Jay\nZhangjie\nWu,\nYixiao\nGe,\nXintao\nWang,\nStan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu,\nYing Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-\nA-Video: One-shot tuning of image diffusion models\nfor text-to-video generation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vi-\nsion, pages 7623\u20137633, 2023. 2, 5, 7, 14, 16, 17\n[71] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles,\nPablo Arbel\u00b4aez, Alex Sorkine-Hornung, and Luc\nVan Gool.\nThe 2017 davis challenge on video ob-\nject segmentation. arXiv preprint arXiv:1704.00675,\n2017. 5\n[72] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo,\nChongyi Li, and Xiangyu Zhang.\nLAMP: Learn a\nmotion pattern for few-shot-based video generation.\narXiv preprint arXiv:2310.10769, 2023. 2\n[73] Guangxuan Xiao, Tianwei Yin, William T Freeman,\nFr\u00b4edo Durand, and Song Han. FastComposer: Tuning-\nfree multi-subject image generation with localized at-\ntention. arXiv preprint arXiv:2305.10431, 2023. 3\n[74] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and\nYu-Gang Jiang.\nSimDA: Simple diffusion adapter\nfor efficient video generation.\narXiv preprint\narXiv:2308.09710, 2023. 3\n[75] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and\nAravind Srinivas.\nVideoGPT: Video generation\nusing vq-vae and transformers.\narXiv preprint\narXiv:2104.10157, 2021. 2\n[76] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt.\nDiffusion probabilistic modeling for video generation.\nEntropy, 25(10):1469, 2023. 2\n[77] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang,\nChen Chen, and Mu Li. AIM: Adapting image models\nfor efficient video action recognition. arXiv preprint\narXiv:2302.03024, 2023. 3\n[78] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang.\nIP-Adapter: Text compatible image prompt adapter\nfor text-to-image diffusion models.\narXiv preprint\narXiv:2308.06721, 2023. 3\n[79] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent\ndiffusion models for text-to-video generation. arXiv\npreprint arXiv:2309.15818, 2023. 2\n[80] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.\nAdding conditional control to text-to-image diffusion\nmodels.\nIn Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 3836\u2013\n3847, 2023. 4\n[81] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang\nZhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli\nZhao, and Jingren Zhou. I2VGen-XL: High-quality\nimage-to-video synthesis via cascaded diffusion mod-\nels. arXiv preprint arXiv:2311.04145, 2023. 2\n[82] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xi-\naopeng Zhang, Wangmeng Zuo, and Qi Tian. Con-\ntrolVideo:\nTraining-free controllable text-to-video\n12\ngeneration. arXiv preprint arXiv:2305.13077, 2023.\n2\n[83] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Jun-\nhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and\nMike Zheng Shou.\nMotionDirector: Motion cus-\ntomization of text-to-video diffusion models. arXiv\npreprint arXiv:2310.08465, 2023. 3\n[84] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei\nLv, Yizhe Zhu, and Jiashi Feng.\nMagicVideo: Ef-\nficient video generation with latent diffusion models.\narXiv preprint arXiv:2211.11018, 2022. 2\n13\nAppendix\nA. Experimental Details\nIn this section, we supplement the experimental details of\neach baseline method and our method. To improve the qual-\nity and remove the watermarks of generated videos, we fur-\nther fine-tune ModelScopeT2V [64] for 30k iterations on\na randomly selected subset from our internal data, which\ncontains about 30,000 text-video pairs. For a fair compar-\nison, we use the fine-tuned ModelScopeT2V model as the\nbase video diffusion model for all methods except for An-\nimateDiff [21] and Tune-A-Video [70], both of which use\nthe image diffusion model (Stable Diffusion [51]) in their\nofficial papers. Here, we use Stable Diffusion v1-51 as their\nbase image diffusion model. During training, unless other-\nwise specified, we default to using AdamW [43] optimizer\nwith the default betas set to 0.9 and 0.999. The epsilon is\nset to the default 1.0 \u00d7 10\u22128, and the weight decay is set to\n0. During inference, we use 50 steps of DDIM [59] sam-\npler and classifier-free guidance [26] with a scale of 9.0 for\nall baselines. We generate 32-frame videos with 256 \u00d7 256\nspatial resolution and 8 fps. All experiments are conducted\nusing one NVIDIA A100 GPU. In the following, we intro-\nduce the implementation details of baselines from subject\ncustomization, motion customization, and arbitrary combi-\nnations of subjects and motions (referred to as video cus-\ntomization).\nA.1. Subject Customization\nFor all methods, we set batch size as 4 to learn a subject.\nDreamVideo (ours). In subject learning, we take \u223c3000\niterations for optimizing the textual identity following [17,\n42] with learning rate 1.0 \u00d7 10\u22124, and \u223c800 iterations for\nlearning identity adapter with learning rate 1.0 \u00d7 10\u22125. We\nset the hidden dimension of the identity adapter to be half\nthe input dimension. Our method takes \u223c12 minutes to train\nthe identity adapter on one A100 GPU.\nTextual Inversion [17]. According to their official code2,\nwe reproduce Textual Inversion to the video diffusion\nmodel. We optimize the text embedding of pseudo-word\n\u201cS\u2217\u201d with prompt \u201ca S\u2217\u201d for 3000 iterations, and set the\nlearning rate to 1.0 \u00d7 10\u22124. We also initialize the learnable\ntoken with the corresponding class token. These settings are\nthe same as the first step of our subject-learning strategy.\nDreamix [47]. Since Dreamix is not open source, we re-\nproduce its method based on the code3 of ModelScopeT2V.\nAccording to the descriptions in the official paper, we only\ntrain the spatial parameters of the UNet while freezing the\ntemporal parameters. Moreover, we refer to the third-party\n1\nhttps://huggingface.co/runwayml/stable-diffusion-v1-5\n2\nhttps://github.com/rinongal/textual inversion\n3\nhttps://modelscope.cn/models/damo/text-to-video-synthesis\nimplementation4 of DreamBooth [52] to bind a unique iden-\ntifier with the specific subject. The text prompt used for\ntarget images is \u201ca [V] [category]\u201d, where we initialize [V]\nwith \u201csks\u201d, and [category] is a coarse class descriptor of the\nsubject. The learning rate is set to 1.0\u00d710\u22125, and the train-\ning iterations are 100 \u223c 200.\nCustom Diffusion [35]. We refer to the official code5 of\nCustom Diffusion and reproduce it on the video diffusion\nmodel. We train Custom Diffusion with the learning rate\nof 4.0 \u00d7 10\u22125 and 250 iterations, as suggested in their pa-\nper. We also detach the start token embedding ahead of the\nclass word with the text prompt \u201ca S\u2217 [category]\u201d. We si-\nmultaneously optimize the parameters of the key as well as\nvalue matrices in cross-attention layers and text embedding\nof S\u2217. We initialize the token S\u2217 with the token-id 42170\naccording to the paper.\nA.2. Motion Customization\nTo model a motion, we set batch size to 2 for training from\nmultiple videos while batch size to 1 for training from a\nsingle video.\nDreamVideo (ours). In motion learning, we train motion\nadapter for \u223c1000 iterations with learning rate 1.0 \u00d7 10\u22125.\nSimilar to the identity adapter, the hidden dimension of the\nmotion adapter is set to be half the input dimension. On\none A100 GPU, our method takes \u223c15 and \u223c30 minutes\nto learn a motion pattern from a single video and multiple\nvideos, respectively.\nModelScopeT2V [64]. We only fine-tune the temporal pa-\nrameters of the UNet while freezing the spatial parameters.\nWe set the learning rate to 1.0 \u00d7 10\u22125, and also train 1000\niterations to learn a motion.\nTune-A-Video [70]. We use the official implementation6 of\nTune-A-Video for experiments. The learning rate is 3.0 \u00d7\n10\u22125, and training iterations are 500. Here, we adapt Tune-\nA-Video to train on both multiple videos and a single video.\nA.3. Video Customization\nDreamVideo (ours).\nWe combine the trained identity\nadapter and motion adapter for video customization during\ninference. No additional training is required. We also ran-\ndomly select an image provided during training as the ap-\npearance guidance. We find that choosing different images\nhas a marginal impact on generated videos.\nAnimateDiff [21]. We use the official implementation7 of\nAnimateDiff for experiments. AnimateDiff trains the mo-\ntion module from scratch, but we find that this training strat-\negy may cause the generated videos to be unstable and tem-\nporal inconsistent. For a fair comparison, we further fine-\n4\nhttps://github.com/XavierXiao/Dreambooth-Stable-Diffusion\n5\nhttps://github.com/adobe-research/custom-diffusion\n6\nhttps://github.com/showlab/Tune-A-Video\n7\nhttps://github.com/guoyww/AnimateDiff\n14\nMethod\nCLIP-T\nCLIP-I\nDINO-I\nT. Cons.\nPara.\nCustom Diffusion [35]\n0.284\n0.699\n0.471\n0.962\n24M\nDreamVideo (ours)\n0.295\n0.701\n0.475\n0.964\n11M\nTable A1. Quantitative comparison of subject customization between our method and Custom Diffusion [35]. \u201cT. Cons.\u201d and \u201cPara.\u201d\ndenote Temporal Consistency and parameter number, respectively.\nMethod\nText\nAlignment\nSubject\nFidelity\nTemporal\nConsistency\nours vs. Textual Inversion [17]\n58.4 / 41.6\n79.8 / 20.2\n69.7 / 30.3\nours vs. Dreamix [47]\n63.7 / 36.3\n56.0 / 44.0\n50.8 / 49.2\nours vs. Custom Diffusion [35]\n70.4 / 29.6\n69.1 / 30.9\n63.0 / 37.0\nTable A2. Human evaluations on customizing subjects between our method and alternatives.\ntune the pre-trained weights of the motion module provided\nby AnimateDiff and carefully adjust the hyperparameters.\nThe learning rate is set to 1.0\u00d710\u22125, and training iterations\nare 50. For the personalized image diffusion model, we\nuse the third-party implementation4 code to train a Dream-\nBooth model. During inference, we combine the Dream-\nBooth model and motion module to generate videos.\nModelScopeT2V [64]. We train spatial/temporal parame-\nters of the UNet while freezing other parameters to learn a\nsubject/motion. Settings of training subject and motion are\nthe same as Dreamix in Sec. A.1 and ModelScopeT2V in\nSec. A.2, respectively. During inference, we combine spa-\ntial and temporal parameters into a UNet to generate videos.\nLoRA [32]. In addition to fully fine-tuning, we also attempt\nthe combinations of LoRAs. According to the conclusions\nin Sec. 3.3 of our main paper and the method of Custom\nDiffusion [35], we only add LoRA to the key and value ma-\ntrices in cross-attention layers to learn a subject. For motion\nlearning, we add LoRA to the key and value matrices in all\nattention layers. The LoRA rank is set to 32. Other settings\nare consistent with our DreamVideo. During inference, we\nmerge spatial and temporal LoRAs into corresponding lay-\ners.\nB. More Results\nIn this section, we conduct further experiments and show-\ncase more results to illustrate the superiority of our\nDreamVideo.\nB.1. Video Customization\nWe provide more results compared with the baselines, as\nshown in Fig. A1. The videos generated by AnimateDiff\nsuffer from little motion, while other methods still strug-\ngle with the fusion conflict problem of subject identity and\nmotion. In contrast, our method can generate videos that\npreserve both subject identity and motion pattern.\nB.2. Subject Customization\nIn addition to the baselines in the main paper, we also com-\npare our DreamVideo with another state-of-the-art method,\nCustom Diffusion [35]. Both the qualitative comparison in\nFig. A2 and the quantitative comparison in Tab. A1 illus-\ntrate that our method outperforms Custom Diffusion and\ncan generate videos that accurately retain subject identity\nand conform to diverse contextual descriptions with fewer\nparameters.\nAs shown in Fig. A3, we provide the customization re-\nsults for more subjects, further demonstrating the favorable\ngeneralization of our method.\nB.3. Motion Customization\nTo further evaluate the motion customization capabilities\nof our method, we show more qualitative comparison re-\nsults with baselines on multiple training videos and a single\ntraining video, as shown in Fig. A4. Our method exhibits\nsuperior performance than baselines and ignores the appear-\nance information from training videos when modeling mo-\ntion patterns.\nWe showcase more results of motion customization in\nFig. A5, providing further evidence of the robustness of our\nmethod.\nB.4. User Study\nFor subject customization, we generate 120 videos from\n15 subjects, where each subject includes 8 text prompts.\nWe present three sets of questions to participants with 3\u223c5\nreference images of each subject to evaluate Text Align-\nment, Subject Fidelity, and Temporal Consistency. Given\nthe generated videos of two anonymous methods, we ask\neach participant the following questions: (1) Text Align-\nment: \u201cWhich video better matches the text description?\u201d;\n(2) Subject Fidelity: \u201cWhich video\u2019s subject is more similar\nto the target subject?\u201d; (3) Temporal Consistency: \u201cWhich\nvideo is smoother and has less flicker?\u201d. For motion cus-\n15\nMethod\nText\nAlignment\nMotion\nFidelity\nTemporal\nConsistency\nours vs. ModelScopeT2V [64]\n64.1 / 35.9\n52.6 / 47.4\n62.8 / 37.2\nours vs. Tune-A-Video [70]\n73.8 / 26.2\n52.4 / 47.6\n74.1 / 25.9\nTable A3. Human evaluations on customizing motions between our method and alternatives.\nMethod\nCLIP-T\nCLIP-I\nDINO-I\nT. Cons.\nPara.\nLoRA\n0.286\n0.644\n0.409\n0.964\n6M\nAdapter\n0.298\n0.648\n0.412\n0.967\n3M\nTable A4. Quantitative comparison of video customization between Adapter and LoRA. \u201cT. Cons.\u201d denotes Temporal Consistency.\n\u201cPara.\u201d means parameter number.\nSelf-Attn\nFFN\nCLIP-T\nT. Cons.\nSerial\nSerial\n0.303\n0.969\nSerial\nParallel\n0.306\n0.971\nParallel\nSerial\n0.308\n0.975\nParallel\nParallel\n0.309\n0.975\nTable A5. Quantitative comparison of different adapters in\nmotion customization. \u201cSerial\u201d and \u201cParallel\u201d mean using serial\nand parallel adapters in the corresponding layer, respectively.\ntomization, we generate 120 videos from 20 motion pat-\nterns with 6 text prompts. We evaluate each pair of com-\npared methods through Text Alignment, Motion Fidelity,\nand Temporal Consistency. The questions of Text Align-\nment and Temporal Consistency are similar to those in sub-\nject customization above, and the question of Motion Fi-\ndelity is like: \u201cWhich video\u2019s motion is more similar to the\nmotion of target videos?\u201d The human evaluation results are\nshown in Tab. A2 and Tab. A3. Our DreamVideo consis-\ntently outperforms other methods on all metrics.\nC. More Ablation Studies\nC.1. More Qualitative Results\nWe provide more qualitative results in Fig. A6 to further\nverify the effects of each component in our method. The\nconclusions are consistent with the descriptions in the main\npaper.\nRemarkably, we observe that without appearance\nguidance, the generated videos may learn some noise, arti-\nfacts, background, and other subject-unrelated information\nfrom training videos.\nC.2. Effects of Parameters in Adapter and LoRA\nTo measure the impact of the number of parameters on per-\nformance, we reduce the hidden dimension of the adapter to\nmake it have a comparable number of parameters as LoRA.\nFor a fair comparison, we set the hidden dimension of the\nadapter to 32 without using textual identity and appearance\nguidance.\nWe adopt the DreamBooth [52] paradigm for\nsubject learning, which is the same as LoRA. Other settings\nare the same as our DreamVideo.\nAs shown in Fig. A7, we observe that LoRA fails to gen-\nerate videos that preserve both subject identity and motion.\nThe reason may be that LoRA modifies the original param-\neters of the model during inference, causing conflicts and\nsacrificing performance when merging spatial and temporal\nLoRAs. In contrast, the adapter can alleviate fusion con-\nflicts and achieve a more harmonious combination.\nThe quantitative comparison results in Tab. A4 also il-\nlustrate the superiority of the adapter compared to LoRA in\nvideo customization tasks.\nC.3. Effects of Different Adapters\nTo evaluate which adapter is more suitable for customiza-\ntion tasks, we design 4 combinations of adapters and param-\neter layers for motion customization, as shown in Tab. A5.\nWe consider the serial adapter and parallel adapter along\nwith self-attention layers and feed-forward layers. The re-\nsults demonstrate that using parallel adapters on all layers\nachieves the best performance. Therefore, we uniformly\nemploy parallel adapters in our approach.\nD. Social Impact and Discussions\nSocial impact. While training large-scale video diffusion\nmodels is extremely expensive and unaffordable for most\nindividuals, video customization by fine-tuning only a few\nimages or videos provides users with the possibility to use\nvideo diffusion models flexibly. Our approach allows users\nto generate customized videos by arbitrarily combining sub-\nject and motion while also supporting individual subject\ncustomization or motion customization, all with a small\ncomputational cost. However, our method still suffers from\nthe risks that many generative models face, such as fake data\ngeneration.\nReliable video forgery detection techniques\nmay be a solution to these problems.\nDiscussions. We provide some failure examples in Fig. A8.\nFor subject customization, our approach is limited by the\n16\ninherent capabilities of the base model. For example, in\nFig. A8(a), the basic model fails to generate a video like \u201ca\nwolf riding a bicycle\u201d, causing our method to inherit this\nlimitation. The possible reason is that the correlation be-\ntween \u201cwolf\u201d and \u201cbicycle\u201d in the training set during pre-\ntraining is too weak. For motion customization, especially\nfine single video motion, our method may only learn the\nsimilar (rough) motion pattern and fails to achieve frame-\nby-frame correspondence, as shown in Fig. A8(b). Some\nvideo editing methods may be able to provide some so-\nlutions [8, 9, 70].\nFor video customization, some diffi-\ncult combinations that contain multiple objects, such as\n\u201ccat\u201d and \u201chorse\u201d, still remain challenges. As shown in\nFig. A8(c), our approach confuses \u201ccat\u201d and \u201chorse\u201d so that\nboth exhibit \u201ccat\u201d characteristics. This phenomenon also\nexists in multi-subject image customization [35]. One pos-\nsible solution is to further decouple the attention map of\neach subject.\n17\na person is surfing\ndog\n+\nDreamVideo\n(ours)\na dog is surfing \nSubject\n+\nMotion\n\u2026\na car running on the road \ndog\n+\na dog running on the Place du Louvre\nModelScope\nT2V\nAnimateDiff\nLoRA\n\u7ec4\u5408\uff1a\nSurfing\uff08\u591a\u89c6\u9891\uff09\u5355\u89c6\u9891\uff1azoom in/zoom out? \u6216\u8005walk front\nMore cases:\nCat2 biking\nFigure A1. Qualitative comparison of customized video generation with both subjects and motions.\nSubject\nCustom Diffusion\nDreamVideo (ours)\na dog* eating pizza\na dog* wagging its tail\na \ud835\udc46\u2217 dog wagging its tail\na cat* posing along the Great Wall\na \ud835\udc46\u2217 cat posing along the Great Wall\na \ud835\udc46\u2217 dog eating pizza\nFigure A2. Qualitative comparison of subject customization between our method and Custom Diffusion [35].\n18\na girl* walking across the street, front view\na duck* running on the road\na tortoise* swimming in a lake\na dog wearing a sunglasses*\na tortoise* exploring a forest\nSubject\nGenerated Videos by DreamVideo\nFigure A3. More results of subject customization for Our DreamVideo.\nTune-A-Video\nDreamVideo\n(ours)\nModelScope\nT2V\nMotion\na person is lifting weights (Multiple videos)\na car running on the road (Single video)\na tiger running on Mars\na bear is lifting weights\nMotion\nGenerated Videos by DreamVideo \nFigure A4. Qualitative comparison of motion customization.\n19\na car running on the road\na cat running on the road\nMotion\nGenerated Videos by DreamVideo \na woman riding a horse jumping \nover a fence\na tiger jumping over a fence, cartoon style\na person is playing guitar\n\u2026\na monkey is playing guitar\nFigure A5. More results of motion customization for Our DreamVideo.\na person is playing guitar \nwolf\n+\nw/o motion \nadapter\nDreamVideo\n(ours)\nw/o textual\nidentity\nw/o appearance\nguidance\na wolf is playing guitar \nSubject\n+\nMotion\n\u2026\na person is lifting weights\nsloth\n+\n\u2026\na sloth is lifting weights\nFigure A6. Qualitative ablation studies on each component.\n20\na person is skateboarding, front view\ncat\n+\nSubject\n+\nMotion\n\u2026\nAdapter\nLoRA\na cat is skateboarding, front view\nFigure A7. Qualitative comparison of video customization be-\ntween Adapter and LoRA. The Adapter and LoRA here have the\nsame hidden dimension (rank) and a comparable number of pa-\nrameters.\n(a) Failure cases on subject customization. \n(b) Failure cases on motion customization. \n(c) Failure cases on video customization. \na wolf riding a bicycle \na rabbit eating a watermelon\na cat eating a watermelon\nReference\nvideo\nGenerated\nvideo\nGenerated\nvideo\n(ours)\nGenerated\nvideo\n(base model)\nGenerated\nvideo\nSubject\n+\nMotion\na person is riding a horse\n\u2026\na cat is riding a horse\ncat\n+\nFigure A8. Failure cases. (a) Our method is limited by the in-\nherent capabilities of the base model. (b) Our method may only\nlearn the similar motion pattern on a fine single video motion. (c)\nSome difficult combinations that contain multiple objects still re-\nmain challenges.\n21\n"
  },
  {
    "title": "Scaling Laws of Synthetic Images for Model Training ... for Now",
    "link": "https://arxiv.org/pdf/2312.04567.pdf",
    "upvote": "7",
    "text": "Scaling Laws of Synthetic Images for Model Training ... for Now\nLijie Fan1,\u2020,* Kaifeng Chen2 Dilip Krishnan2 Dina Katabi1 Phillip Isola1 Yonglong Tian2,\u2020\n1MIT CSAIL,\n2Google Research,\n\u2020equal contribution\nGithub Repo: https://github.com/google-research/syn-rep-learn\nAbstract\nRecent significant advances in text-to-image models un-\nlock the possibility of training vision systems using synthetic\nimages, potentially overcoming the difficulty of collecting\ncurated data at scale. It is unclear, however, how these\nmodels behave at scale, as more synthetic data is added to\nthe training set. In this paper we study the scaling laws of\nsynthetic images generated by state of the art text-to-image\nmodels, for the training of supervised models: image classi-\nfiers with label supervision, and CLIP with language super-\nvision. We identify several factors, including text prompts,\nclassifier-free guidance scale, and types of text-to-image\nmodels, that significantly affect scaling behavior. After tun-\ning these factors, we observe that synthetic images demon-\nstrate a scaling trend similar to, but slightly less effective\nthan, real images in CLIP training, while they significantly\nunderperform in scaling when training supervised image\nclassifiers. Our analysis indicates that the main reason for\nthis underperformance is the inability of off-the-shelf text-\nto-image models to generate certain concepts, a limitation\nthat significantly impairs the training of image classifiers.\nOur findings also suggest that scaling synthetic data can be\nparticularly effective in scenarios such as: (1) when there\nis a limited supply of real images for a supervised problem\n(e.g., fewer than 0.5 million images in ImageNet), (2) when\nthe evaluation dataset diverges significantly from the train-\ning data, indicating the out-of-distribution scenario, or (3)\nwhen synthetic data is used in conjunction with real images,\nas demonstrated in the training of CLIP models.\n1. Introduction\nThe quality and quantity of data play a crucial role in train-\ning vision models. Historically, the emphasis has been on\ncreating large, meticulously curated image datasets with\ncategorical labels at the image level for training supervised\nmodels [19, 42, 63, 77]. Prominent examples include CI-\n*Work done while at Google.\nImageNet\nImageNet-Sk\nImageNet-R\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nScaling Ability\nBetter\nOut-of-Distribution\nSupervised\nImageNet Downstream\nCLIP\nReal\nMUSE (Caption)\nSD (CLIP Template)\nImagen (Caption)\nSD (Caption)\nFigure 1. Scaling ability (i.e., the slope of the power law curve\nbetween loss and dataset size fitted in the log space, see Eq. 2)\ncomparison between real and synthetic images on supervised clas-\nsifier and CLIP training. Red bars represent real images and blue\nbars represent synthetic images generated with different text-to-\nimage models. Supervised models are trained on real or synthetic\nImageNet, and text in parentheses is the text prompt used to gen-\nerate the images (details in Section 3.1). ImageNet-Sketch and\nImageNet-R are out-of-distribution tests. CLIP models are trained\non LAION-400M with real or synthetic images. We see that: (1)\nscaling ability of synthetic data is slightly worse than that of real\ndata for CLIP training; (2) robustness on ImageNet-Sketch and\nImageNet-R datasets can be better when training on synthetic data.\nFAR [42] and ImageNet [19]. While creating these datasets\nis effective on a smaller scale, their expansion to hun-\ndreds of millions of samples presents significant challenges.\nThese challenges include the intensive labor required for cu-\nration at scale, as well as the increasing potential for noise\nand quality issues as the datasets scale up.\nRecently, there has been an increasing interest in train-\ning vision models using language supervision [37, 58].\nThis shift is exemplified by models like CLIP [58], which\nmove beyond the fixed, predefined categories typical of\ndatasets like ImageNet.\nTraining these models requires\nextensive image-text pair datasets. Developments ranging\nfrom the creation of the Conceptual Captions dataset [72],\nwhich comprises millions of image-text pairs, to the LAION\n1\narXiv:2312.04567v1  [cs.CV]  7 Dec 2023\ndataset [71], encompassing billions of pairs, are examples\nof this growing trend. However, this approach is not with-\nout its challenges. The massive scale of data sourcing, often\nthrough web scraping, introduces significant noise. Scala-\nbility issues also persist. Moreover, the immense size of\nthese datasets presents practical difficulties in terms of stor-\nage and data transfer. For instance, LAION-2B requires\ntens of terabytes of disk space and could take days, if not\nweeks, to download.\nFortunately, recent breakthroughs in text-to-image mod-\nels have introduced exciting new possibilities in the realm\nof synthetic data generation. These models, capable of pro-\nducing high-quality images from textual descriptions, offer\nseveral significant advantages. Firstly, they allow precise\ncontrol over image content through input texts, which could\nprovide categorical labels or paired text supervision for free.\nSecondly, they are bandwidth-efficient, as only the model\nneeds to be transferred, not the entire dataset. For instance,\nmodels like Stable Diffusion [64] occupy merely 5 GB of\ndisk space, which is 2000\u00d7 more efficient compared to the\nmassive LAION-2B dataset. Thirdly, they facilitate easier\nscalability with markedly reduced human labor for dataset\ncuration. These benefits naturally lead to the question of\nwhether it\u2019s feasible to scale up vision datasets with syn-\nthetic images for training supervised models.\nHowever, the use of synthetic images is also not with-\nout its drawbacks. When scaled to tens or hundreds of mil-\nlions of images, these models may produce images of lower\nquality or with misaligned concepts, and might also strug-\ngle with maintaining diversity. In this paper, we tackle a\npivotal question: How effective is the scaling of synthetic\nimages, specifically generated for training supervised vi-\nsion models? We examine the scaling behavior of synthetic\nimages created by cutting-edge text-to-image models, com-\nparing their efficacy to real images in two key scenarios: the\ntraining of supervised classifiers and the training of vision\nmodels with language supervision, such as CLIP. Addition-\nally, we explore a range of factors that markedly impact the\nscaling efficiency of synthetic images. These include the\nchoice of text-to-image model, the classifier-free guidance\nscale employed, and the nature of text prompts used for gen-\nerating training images. A summarized comparison of the\nscaling ability between real and synthetic images is shown\nin Figure 1.\nWe present our key findings as follows:\n\u2022 An empirical study on the scaling behavior of images syn-\nthesized by three major text-to-image models (Stable Dif-\nfusion [64], Imagen [69], and Muse [9]) shows that model\nperformance exhibits power law scaling [39] as a func-\ntion of the number of synthetic images they are trained\non. This trend holds until computation budget and model\nsize become limiting factors [39].\n\u2022 We identify several factors that can significantly alter the\nscaling ability of synthetic data, including prompt design,\nclassifier free guidance, and the choice of models.\n\u2022 In supervised settings, synthetic data does not scale as\neffectively as real data. However, there are exceptions\nwhere synthetic data demonstrates better scaling: (1) with\nclasses that text-to-image models are particularly adept\nat generating, and (2) when the test data deviates signifi-\ncantly from the training data, e.g., out of distribution data.\n\u2022 In CLIP training, the disparity in scaling performance be-\ntween synthetic and real data is less pronounced. Incor-\nporating synthetic data with real data leads to enhanced\nzero-shot performance in most scenarios.\n2. Related Work\nText to image models.\nRecent breakthroughs in text-\nto-image models, primarily driven by advances in diffu-\nsion models [34, 73, 87], have enabled the generation of\nhigh-quality, photo-realistic images using neural networks.\nKey examples of such models include Imagen [69], which\nperforms diffusion in pixel space, and Stable Diffusion\n[64], which operates in the latent space of an autoencoder.\nDALL-E 3 [6] also exemplifies this category. An alternative\nfamily of models, based on visual tokens, utilizes VQGAN\n[82] and Transformers [83]. Prominent examples within\nthis category include Parti [90] and Muse [9]. Addition-\nally, recent advancements have been exploring the scaling\nGenerative Adversarial Networks (GANs) [22] for text-to-\nimage generation, as demonstrated in works such as [38].\nLearning from synthetic data. Synthetic data has proven\nto be effective in improving performance across various\ndomains [17, 25, 43, 49, 51, 66, 67, 78, 81, 88].\nSyn-\nthetic images, in particular, have been extensively utilized\nin a range of different computer vision tasks, including\nobject detection [57, 68], semantic segmentation [11, 65],\nautonomous driving [1], and robotics [52, 89]. More re-\ncently, there has been evidence that combining synthetic\nimages generated by text-to-image models with real im-\nages can improve the performance on supervised learning\ntasks [4, 24]. Particularly, [4] has fine-tuned the text-to-\nimage model using the target dataset, e.g. ImageNet, while\nthis paper studies the capabilities of off-the-shelf text-to-\nimage models. Additionally, there are efforts developing\nmethods for learning transferable representations from syn-\nthetic images [5, 24, 36, 46, 62, 70, 80].\nNeural scaling laws. Scaling up model size, data amount,\nand training budget has unlocked new capabilities of deep\nmodels [13, 18, 55, 59, 91]. Recent studies [31, 39] sug-\ngest the testing loss behaves as a power low with respect to\neach of these three resources when the other two are proper,\nin large language models (LLMs), machine translation [23],\nauto-regressive generative models [29], and transfer learn-\ning [30]. Similar behavior is observed in multi-modal mod-\nels [2]. Chinchilla [35] suggests scaling up data propor-\n2\ntionally to model size, to obtain compute-optimal LLMs.\n[3] propose to fit scaling laws by extrapolating training\ncurves. Of particular interest, [74] theoretically shows one\ncan break the power law with respect to data size with an\nideal data pruning strategy. In this paper, we focus on the\nscaling behavior of synthetic data for training models.\n3. Preliminaries\nWe first study the scaling behavior of synthetic images gen-\nerated with state-of-the-art text-to-image models under the\nImageNet supervised training setting.\n3.1. Three Factors on T2I Generation\nThere are three primary factors influencing the generated\nimages used for supervised training: (1) choice of text-to-\nimage model, (2) the classifier-free guidance scale, and (3)\nthe class-specific prompt used for the text input. We will\nnow provide a detailed description of each of these factors:\nText-to-Image Models. We conducted the study on three\nstate-of-the-art text-to-image models of different types:\n\u2022 Stable Diffusion [64], a model that drives the diffusion\nprocess in the latent space of a pre-trained autoencoder.\n\u2022 Imagen [69], a model that drives the diffusion process\ndirectly in the raw pixel space.\n\u2022 Muse [9], a visual token-based generation model trained\nwith masked generative modeling, that performs discrete\ndiffusion in the latent space of an autoencoder.\nThese models have distinct architectural designs, but are all\ncapable of generating photo-realistic images. Since Ima-\ngen [69] and Muse [9] are not publicly available, we base\nour work on a version trained on internal data sources.\nGuidance Scale. All modern text-to-image models primar-\nily rely on the classifier-free guidance (CFG) technique to\ngenerate images based on textual input [33]. Increasing the\nCFG scale typically improves the alignment between the\ngenerated images and the input text, resulting in higher-\nquality output images. However, this also tends to reduce\nthe diversity of content in the generated images. Through\nempirical analysis, we determined that when generating im-\nages for training supervised classifiers, it is advisable to use\na relatively lower CFG scale compared to the default value\nused in generation. This ensures that the generated images\nexhibit a higher degree of diversity, particularly when gen-\nerating images from texts describing the same class. We\nconducted a detailed analysis and determined the optimal\nCFG scale ranges for different models: [1.5, 10.0] for Sta-\nble Diffusion, [1.0, 2.0] for Imagen, and [0.1, 1.0] for Muse.\nClass-specific Prompts. To generate images for each class\nin ImageNet, we employed different techniques to create\ncorresponding text prompts.\nThis allows us to generate\nimages conditioned on the specific ImageNet class via the\nprompts. Take the class \u2018Tench\u2019 as example, we can have\nprompts as:\n\u2022 Classnames:\nDirectly use the ImageNet class name.\n(\u2018Tench\u2019)\n\u2022 Classnames + Description: Combine class name with its\nWordNet [50] description. (\u2018tench, freshwater dace-like\ngame fish of Europe and western Asia ...\u2019)\n\u2022 Classnames + Hypernyms:\nCombine ImageNet class\nname with its Wordnet hypernyms. (\u2018Tench, Tinca tinca,\ncyprinid, cyprinid fish\u2019)\n\u2022 Word2Sen: Use a pre-trained T5 model [60] as used\nin [24] to convert the ImageNet class name into a sen-\ntence. We generate 100 sentences for each class. (\u2018a tench\nwith fish in the distance.\u2019)\n\u2022 CLIP templates: Generate either 7 or 80 sentences with\nthe text templates CLIP used for zero-shot classification\ntask. (\u2018a photo of the large tench\u2019)\n\u2022 IN-Captions: Combine the class name with captions from\nImageNet(IN) training images. Captions are generated by\nBLIP2 [44]. (\u2018Tench, a man holding a fish\u2019)\n3.2. Metrics: Recognizability and Diversity\nThe above factors give us a number of configurations to gen-\nerate synthetic data. We now proceed to define metrics to\nanalyze the resulting images, and then analyze the scaling\nbehavior exhibited by the images generated under this con-\nfiguration. The generated images should possess two cru-\ncial attributes: (1) Recognizabilty: Synthetic images should\nexhibit high precision, meaning they correctly represent the\nintended class, and high recall, implying that images for\nother classes should not mistakenly contain elements of this\nclass. (2) Diversity: It is essential that the generated images\nare diverse from each other to improve generalization.\nWe define two measures to quantify the recognizability\nand diversity of images generated under a specific config-\nuration. We generate 50 images for each ImageNet class,\nresulting in a synthetic test set comprising 50,000 images.\nSubsequently, we define the two metrics as follows:\n\u2022 Recognizabiliy: Use a pre-trained ImageNet classifier (a\nViT-B with 86.2% accuracy from [85]) to classify the gen-\nerated images and compute the F1 score for each class.\nThe final metric is given by averaging F1 score across all\nclasses.\n\u2022 Diversity1: Following [8], we extract features from the\nsame pre-trained model [85] and compute the standard\ndeviation on the feature space for images from every\nclass, and then compute the average score across all\nclasses.\n3.3. Scaling Law for Synthetic Data\nPrior works on scaling laws, such as [39], have observed\nthat, for sufficiently large models, the test loss L and dataset\n1We also tried replacing the diversity metric with FID [32] or\nLPIPS [92], please refer to Appendix D for details.\n3\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nDiversity\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nRecognizability\nOthers\nMuse IN-captions\nImagen IN-captions\nSD Classnames\nSD IN-captions\n1.6\n1.4\n1.2\n1.0\n0.8\n0.6\nPerformance at 1.3M (negative log loss)\nFigure 2. Recognizability vs. diversity plot for\nvarious synthetic image generation configurations\n(as in Section 4.2), colored by the performance\nat 1.3M on ImageNet validation set (measured by\nnegative log loss). Deeper color stands for smaller\nloss and better performance.\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\n1.0\n2.0\n3.0\n4.0\n6.0\nLoss (log scale)\nPower-law\nConstrained\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTop-1 Accuracy\nSD Classname, CFG 7.5\nSD Classname, CFG 2.0\nSD CLIP template, CFG 2.0\nSD IN-Captions, CFG 2.0\nMUSE IN-Captions\nImagen IN-Captions\nReal\nFigure 3. Scaling on ImageNet validation set for various configurations as in Sec-\ntion 4.3. Loss and data scale follows the power-law (as in Equation 2) with varied\nk when data is less than 4M. By tuning the CFG scale, text prompts and text-\nto-image models, the scaling behavior for synthetic images can be significantly\nimproved (from light blue to orange). Red dashed line is for real images. Orange\nand blue dotted lines are ViT-L backbones, extending the power-law to 8M.\nscale D, approximately follow a power-law relationship:\nLD \u221d (1/D)k\n(1)\nwhere k is a constant. Thus LD exhibits linear dependence\non D in log space. Let DI be 1.3 million, roughly the size\nof the ImageNet training set with real data. We re-write\nEquation 1 as:\nlog LD =\n\u2212k\n|{z}\nk: Scaling Ability\n(log D \u2212 log DI) \u2212 (\u2212 log LDI)\n|\n{z\n}\nPerformance at 1.3M\n(2)\nThe slope \u2212k and y-intercept \u2212 log LDI would determine\na unique scaling curve in log space. With this, we provide\nquantitative definitions for two key metrics for scaling:\n\u2022 Scaling Ability: Quantifies the scaling effectiveness of\nsynthetic images generated by a particular text-to-image\nconfiguration. Stepper curves means loss scales better\nwith data, therefore we represent scaling ability by the\nnegative of the slope: k.\n\u2022 Performance at 1.3M: Measures the classification per-\nformance of models (as negative log loss) when trained on\na dataset with a scale equivalent to 1.3M, the size of the\nImageNet training set. It is represented by the y-intercept\n\u2212 log LDI.\n4. Scaling on Supervised Training\n4.1. Setup\nWe train supervised classification models exclusively using\nthe images generated by text-to-image models and evaluate\ntheir performance by computing cross-entropy loss and top-\n1 accuracy on the ImageNet validation set, which contains\nreal images. Training iterations are scheduled linearly based\non the training data size in logarithmic space. All generated\nimages are resized to a resolution of 256x256 pixels. Un-\nless stated otherwise, we employ the ViT-B model [20] with\na patch size of 16 as our backbone architecture. Training\nhyperparameters details are provided in Appendix A.\n4.2. Performance at 1.3M\nWe commenced by generating synthetic ImageNet datasets,\neach containing 1.3 million synthetic images, using various\nconfigurations of text-to-image models, CFG scales, and\nprompts as outlined in Section 3. In total, we created syn-\nthetic ImageNets in 54 distinct configurations, with detailed\ninformation provided in Appendix C. Figure 2 displays the\nvalidation loss on the real ImageNet validation set, repre-\nsented by \u2212 log LDI as defined in Equation 2. A higher\nvalue correlates with increased classification accuracy, sig-\nnaling better performance. Comparisons focusing on clas-\nsification accuracy are also included in Appendix C.\nWithin this study, we investigate the impact of differ-\nent prompt sets (Section 3) within the Stable Diffusion\nmodel. Squares (\u25a0), Circles (\u25cf), and Diamonds (\u25c6) rep-\nresent prompt configurations involving IN-captions, Class-\nnames, and all other prompt setups, respectively. For Muse\nand Imagen configurations, we maintain the prompt set as\nIN-Captions and vary the CFG scale within the ranges [1, 2]\nand [0.1, 1], respectively. Triangles (\u25b2) represent the per-\nformance of images generated with Muse, while Stars (\u2605)\nrepresent the performance of images generated with Ima-\ngen. Several key findings emerge from the results:\nDiversity and Recognizability trade-off: Across differ-\nent configurations, we observe a trade-off between diver-\n4\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n5.0\n6.0\nLoss (log scale)\nImageNet-A\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n2.0\n3.0\n4.0\nLoss (log scale)\nImageNet-R\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n3.0\n4.0\n6.0\nLoss (log scale)\nImageNet-Sketch\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n2.0\n3.0\n4.0\nLoss (log scale)\nImageNet-v2\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nTop-1 Accuracy\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTop-1 Accuracy\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nTop-1 Accuracy\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTop-1 Accuracy\nStable Diffusion (CLIP Templates)\nStable Diffusion\nMuse\nImagen\nReal\nFigure 4. Scaling behavior on four different out-of-distribution validation sets. We compare synthetic images generated with optimal CFG\nscales by Stable Diffusion (with 80 CLIP templates or IN-Captions prompt), Imagen and Muse (all with IN-Caption prompt) with real\nimages. Scaling synthetic data is useful and can surpass real images when the domain gap between the training and testing is significant,\ne.g. when evaluated on ImageNet-R and ImageNet-Sketch.\nsity and recognizability. The top-right corner of the figure\nrepresents the best performance, indicating configurations\nthat can generate both accurate and diverse images. Config-\nurations perform poorly when either recognizability or di-\nversity falls below a certain threshold. Also see Figure A4\nbottom left for scattering colorized by accuracy.\nEffect of Prompt Sets: Choosing different prompt sets can\nimpact performance. Using a more diverse prompt set shifts\nthe configuration towards the bottom-right of the figure.\nTransitioning from Classname to IN-captions for text-to-\nimage prompts may contribute to this shift, likely due to the\nincreased diversity on the text side, which inherently leads\nto more diverse generated images.\nImpact of CFG Scale: When prompts are fixed, controlling\nthe CFG scale also affects the performance of the classifica-\ntion model. Increasing the CFG scale shifts the configura-\ntion towards the upper-left part of the figure, where recog-\nnizability is increased, but diversity decreases. This initially\nleads to improved performance, followed by a decrease.\nText-to-Image Model Performance: In terms of text-to-\nimage models, when all configured to use IN-Captions as\nprompts, Stable Diffusion, Imagen, and Muse demonstrate\na quite similar trend in balancing recognizability and diver-\nsity. This similarity in their trade-off is reflected in their\nclose proximity to each other in the plot.\n4.3. Scaling Ability\nWe next proceed to analyze the scaling behavior of different\nmodels, as well as the difference between training super-\nvised models on synthetic images and on the real ImageNet\ntraining set. Figure 3 illustrates the scaling behavior across\nvarious configurations. Specifically, for Stable Diffusion,\nwe depict the scaling behavior of different configurations\nwith various prompts and CFG scales. We select the opti-\nmal configuration for Muse and Imagen from Section 4.2,\nusing IN-Caption as prompts and the corresponding opti-\nmal CFG scale for each model. From the figure, several\nobservations can be made:\nPower-law Relationship: Training on synthetic images\nfollows a power-law relationship from 0.125 million to 4\nmillion training images. Validation loss and training data\nsize exhibit a linear correlation when analyzed in log space.\nScaling Disparity: Training on synthetic images does not\nscale as effectively as training on the real ImageNet train-\ning set images, and typically has a smaller scaling ability.\nThis difference can be attributed to the curation of Ima-\ngeNet training images and performing validation under an\nin-domain setting.\nImpact of Prompts and CFG Scale: Using default prompt\nsets and CFG scale for image generation results in poor\nscaling ability, i.e. a very flat slope and smaller k value.\nHowever, by tuning the prompts and CFG scale properly,\nthe generated images become much more diverse, leading\nto an increased scaling behavior for synthetic images, bring-\ning it closer to the scaling ability observed with real images.\nNevertheless, the best scaling configuration is still signifi-\ncantly worse than scaling with real data.\n5\n4.4. Scaling beyond 4M\nWe naturally wonder about the scaling behavior when the\ndataset size exceeds 4 million images and whether the val-\nidation loss will continue to decrease. In Figure 3, we also\nillustrate the scaling curve for Stable Diffusion up to 64\nmillion images, and for Muse and Imagen up to 8 million\ntraining images (in gray background). The results indicate\nthat the relationship changes when the dataset scale exceeds\naround 4 million images.\nWe hypothesize that this could be due to the loss be-\ning constrained by insufficient model capacity. According\nto [39], the power-law relationship between validation loss\nand training dataset size requires the model to have suffi-\ncient capacity to fit the dataset and converge. Therefore,\nwhen the dataset size exceeds 4 million images, and if we\ncontinue to use ViT-B as the backbone architecture, the val-\nidation loss in log space no longer exhibits a linear trend. To\naddress this, we retrain the supervised models with ViT-L as\nthe backbone architecture for the best Stable Diffusion and\nImagen configuration, as shown in the dotted lines. This\nimprovement in model capacity could achieve a lower val-\nidation loss and maintain a roughly linear ratio up to the 8\nmillion scale and slightly postpones the inflection point.\n4.5. Out-Of-Distribution Scaling\nWe also investigate the scaling behavior on out-of-\ndistribution (OOD) validation sets to determine whether it\ndiffers from the in-domain setup on the ImageNet valida-\ntion set. We employ the supervised ImageNet classifiers\nand test them on four OOD validation sets, which include\nImageNet-A [28], ImageNet-R [27], ImageNet-Sketch [84],\nand Imagenet-V2 [61]. The scaling curves for validation\nloss and top-1 validation accuracy are presented in Figure 4.\nOur empirical results indicate that in scenarios where the\ndomain gap is relatively small, such as testing on ImageNet-\nv2, the scaling behavior mirrors the observation in in-\ndomain setups, with real images showing superior scaling\nperformance. However, a much more intriguing observa-\ntion emerges when the domain shift is more pronounced,\nas seen in tests on ImageNet-R and ImageNet-Sketch. In\nthese instances, the disparity in scaling capabilities between\nsynthetic and real images narrows.\nConsequently, scal-\ning up synthetic images becomes particularly beneficial and\nuseful.\nRemarkably, in situations with sufficiently large\ndataset scales, synthetic images can even outperform real\nimages from ImageNet training set (e.g. for ImageNet-R\nand ImageNet-Sketch with Muse), highlighting the poten-\ntial of synthetic images in bridging significant domain gaps.\nInterestingly, when images are generated with 80 CLIP tem-\nplates as text prompt (the light blue line in the plot) in-\nstead of IN-Captions, the improvements over real images\non ImageNet-R and ImageNet-Sketch are more significant,\nalthough the scaling ability on the ImageNet validation set\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nDiversity\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nScaling Ability\n=\n0.06\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecognizability\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n= 0.29\nFigure 5. Per class analysis on the relationship between scaling\nability (defined as k in Equation 2) and both diversity and recog-\nnizability. Within each specific class, the plots indicate a positive\ncorrelation between recognizability and scaling ability. However,\nthe correlation between diversity and scaling ability appears to be\nnegligible.\nis worse (as shown in Figure 3). This suggests that care-\nfully crafting text prompts can unlock further potential in\nincreasing the efficacy of synthetic images, particularly for\nOOD scenarios.\n5. Zoom-in: Per Class Analysis\nIn addition to our general analysis of scaling behavior and\nits impact on overall performance across all classes, we take\na more detailed approach to gain a better understanding of\nthe key factors affecting class-specific scaling behavior. Our\nanalysis involves assessing the scaling ability of each spe-\ncific class in the 1,000 categories in ImageNet. We aim\nto establish connections between the scaling ability of each\nclass and the characteristics of the images generated by text-\nto-image models, aiming to figure out potential reasons why\nsynthetic data does not scale as good as real images. For this\nanalysis, we focus on images generated by Stable Diffusion,\nusing the optimal CFG of 2.0 and IN-Caption prompts.\n5.1. What affects Scaling Ability\nWhen we fix the generation configuration of specific text-\nto-image model, CFG scales and prompts as described in\nSection 3.1, text-to-image models could still exhibit vary-\ning degrees of recognizability and diversity when gener-\nating images for different object classes. To explore how\nthese factors influence the scaling ability of each class, we\nconducted an analysis focusing on the correlation between\nscaling ability and both recognizability and diversity, all\ncomputed for each class individually. These correlations,\nand their implications for scaling efficiency, are depicted in\nFigure 5. Our analysis underscores the potential positive\nrole of recognizability in determining the scaling ability for\nthe synthetic images, for each specific class, generated by\ntext-to-image models. We identified a positive correlation\nbetween recognizability and scaling ability, indicating that\nthe precision in generating the intended class significantly\n6\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScaling Ability\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nPerformance at 1.3M\nScaling Classes\nOx\nStation Wagon\nDiversity\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScaling Ability\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nPerformance at 1.3M\nEasy Classes\ngeyser\nairliner\nRecognizability\n0.0\n0.2\n0.4\n0.6\n0.8\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScaling Ability\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nPerformance at 1.3M\nPoor Classes\nTiger Cat\nVine Snake\n4M Performance\n2\n1\n0\n1\n2\nFigure 6. Scaling ability vs. Performance at 1.3M plot for synthetic data. Each point represents one of the 1,000 ImageNet classes. Classes\nare colored by their diversity, recognizability, and their final performance at 4M scale in the three sub-figures respectively. The scaling\nability is measured by k defined in Equation 2. The performances at Y-axis is measured by the validation loss: \u2212 log(L), and higher\nnumbers indicate lower loss and better performance. We choose two classes in each of the \u2018Scaling\u2019, \u2018Easy\u2019 and \u2018Poor\u2019 class categories,\nand their detailed scaling behavior and visualization can be found in Figure 8.\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n0.1\n0.3\n1.0\n3.0\nLoss (log scale)\nox\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nstation wagon\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\ngeyser\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nairliner\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\ntiger cat\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nvine snake\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale (M)\nSynthetic\nReal\nFigure 7. Scaling behavior for classes from \u2018Scaling (Red)\u2019, \u2018Easy (Blue)\u2019 and \u2018Poor (Green)\u2019 categories. Easy classes have a good\ninitial accuracy with limited amounts of data, while Poor classes do not scale well. Scaling classes scale the best, and can achieve better\nperformances than real images as the data amount goes up.\nOx\nStation Wagon\nGeyser\nAirliner\nVine snake\nTiger cat\nFigure 8. Visualization of different class categories generated by Stable Diffusion. Top row are the \u2018Scaling\u2019 classes that scales well.\nMiddle row are the \u2018Easy\u2019 classes that has a good initial performance. Bottom row are the \u2018Poor\u2019 classes that has poor scaling ability and\nperformance.\n7\nenhances the scaling effectiveness of synthetic images. In\ncontrast, the influence of diversity within each class seems\nto be more limited. Our findings reveal only a negligible\ncorrelation between diversity and scaling ability. This might\nbe attributed to the increased noise introduced when com-\nputing diversity for specific categories, as opposed to the\noverall dataset.\n5.2. What makes a \u2018Poor\u2019 class\nTo gain a deeper insight into how scaling ability is dis-\ntributed across different classes, and identify classes that\ndo not scale well, we created a scatter plot with scaling\nability on the X-axis and 1.3M performance on the Y-axis,\nas shown in Figure 6. Each class is represented as a dot\nin this plot, with top-right positions indicating better per-\nformance when scaled up to 4 million images. Points are\ncolored based on either diversity or recognizability, or their\nfinal performance at 4M dataset scale.\nBased on their positioning in the scatter plot, classes\ncan be categorized into three distinct groups.\nPoints in\nthe bottom-left section represent \u2018Poor\u2019 classes, which are\nmarked by both limited scaling ability and poor overall per-\nformance. In contrast, classes located in the upper-right sec-\ntion are deemed \u2018Easy\u2019 characterized by strong initial per-\nformance as well as robust scaling ability. Lastly, classes\nsituated in the mid-right section can be described as \u2018Scal-\ning\u2019. These classes may exhibit poor initial performance but\ndemonstrate considerable improvement as the dataset size\nincreases.\nIn Figure 7, we showcase two classes from each of the\n\u2018Scaling\u2019, \u2018Easy\u2019, and \u2018Poor\u2019 categories to illustrate and\ncompare their scaling behaviors against real images within\nthe same class. This analysis highlights an intriguing find-\ning: certain \u2018Scaling\u2019 classes demonstrate a scaling abil-\nity that surpasses that of real images, thereby emphasizing\nthe potential utility of synthetic images in these scenarios.\nWe present additional results for \u2018Scaling\u2019 classes in Ap-\npendix G.2.\nAdditionally, we present visualizations of the generated\nimages from these categories in Figure 8.\nOur findings\nshow that text-to-image models adeptly generate images for\n\u2018Scaling\u2019 and \u2018Easy\u2019 classes with commendable accuracy\nand diversity. However, these models face challenges in ac-\ncurately rendering the correct concepts for \u2018Poor\u2019 classes.\n6. Scaling on CLIP\n6.1. Setup\nWe investigated the scaling behavior of synthetic data in\nCLIP training using the extensive LAION-400M dataset.\nThe synthetic images were generated using Stable Diffu-\nsion. We compare across different CFG scales and choose\nthe optimal one (1.5) for CLIP training. For evaluation,\n1\n2\n4\n8\n16\n32\n64\n128\n256\nDataset Scale (M)\n2.0\n3.0\n5.0\n10.0\nZero-shot Loss (log Scale)\n1\n2\n4\n8\n16\n32\n64\n128\n256\nDataset Scale (M)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nZero-shot Accuracy\nSynthetic\nReal\nSynthetic+Real\nFigure 9. Scaling behavior for CLIP models trained on LAION-\n400M subsets of different scales. Models are trained with syn-\nthetic, real, or a combination of synthetic and real images, and are\nevaluated with ImageNet zero-shot accuracy. Dataset scale here\nrefers to the number of captions.\nwe followed the prompt templates from [58] and con-\nduct zero-shot classification on ImageNet and 15 different\nfine-grained classification datasets, including Food-101 [7],\nStanford Cars [41], Oxford Pets [56] etc.\nThe training\nscale begins with 1 million image-text pairs, progressively\nscaling up to encompass the full dataset of 3712 million\nsamples.\nAll models use ViT-B as the backbone with a\npatch size of 16, and are trained for 32 epochs across all\ndataset scales. Detailed training hyper-parameters are in\nAppendix B. Comparisons on different CFG scales are also\navailable in Appendix H.\n6.2. Scaling Analysis\nWe evaluated the scaling behavior across three different\ndata setups: (1) using only synthetic images, (2) using only\nreal images, and (3) using a combination of both synthetic\nand real images. Dataset scale here refers to the number\nof captions. When combining synthetic and real images for\ntraining, we maintained a consistent text scale throughout.\nDuring each training iteration, we randomly selected one\nimage, either real or synthetic, for use. The comparative\nanalysis of these setups, evaluated on zero-shot classifica-\ntion loss and accuracy on ImageNet validation set, is de-\npicted in Figure 9.\nThe analysis revealed that for all three scenarios, zero-\nshot classification loss adheres to the power-law relation-\nship when the data amount is under around 64 million,\ncompared to the 4 million scale in supervised training. In\nthis range, the loss and data scale maintain a linear rela-\ntionship in logarithmic space. Additionally, while the scal-\ning efficiency (reflected in the slope of the curve) of syn-\nthetic data is somewhat lower than that of real data, this\ndiscrepancy is less pronounced than in the supervised clas-\nsifier settings. However, a noticeable performance gap per-\n2The LAION-400M dataset we used contains slightly less samples\ncompared to the orignal one because of link rot.\n8\nTable 1. Zero-shot transfer performance on 15 downstream datasets. Models are trained on LAION-400M subsets at various scales from\n1M to the total 371M, with images from synthetic, real or synthetic+real. Combining synthetic images with real images can improve\nperformance, especially when data amount is limited.\nScale\nData\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\n1M\nSyn\n5.2\n12.8\n3.3\n5.9\n1.7\n0.9\n5.5\n6.7\n17.8\n3.5\n29.4\n9.0\n9.7\n5.4\n1.2\n7.9\nReal\n5.2\n25.4\n7.6\n5.0\n2.1\n1.0\n5.4\n5.4\n18.0\n5.0\n36.4\n14.7\n9.3\n6.6\n1.0\n9.9\nSyn+Real\n10.9\n32.2\n13.0\n13.1\n4.6\n1.4\n9.4\n12.0\n36.0\n8.9\n62.5\n19.8\n14.7\n7.5\n1.9\n16.5\n2M\nSyn\n11.0\n15.3\n3.8\n14.5\n6.2\n1.7\n10.3\n15.6\n36.2\n7.2\n36.3\n15.6\n14.5\n3.4\n1.7\n12.9\nReal\n13.4\n39.0\n16.8\n13\n6.6\n1.3\n10.5\n13.0\n40.1\n12.4\n57.1\n17.0\n14.9\n6.5\n1.7\n17.6\nSyn+Real\n22.2\n59.7\n27.0\n23.4\n18.2\n2.1\n15.4\n24.8\n55.7\n13.1\n73.9\n22.4\n20.6\n4.2\n3.0\n25.7\n4M\nSyn\n19.6\n19.7\n7.1\n23.0\n22.4\n2.1\n17.0\n30.1\n53.4\n13.9\n64.4\n12.8\n21.1\n5.3\n3.1\n21.0\nReal\n30.8\n63.8\n33.4\n26.2\n27.7\n1.8\n18.8\n33.9\n61.7\n18.9\n79.2\n40.2\n21.5\n10.0\n3.4\n31.4\nSyn+Real\n40.3\n67.1\n39.3\n35.8\n40.9\n2.3\n22.9\n45.4\n70.1\n23.1\n88.2\n33.5\n27.7\n12.3\n4.5\n36.9\n8M\nSyn\n34.2\n23.8\n9.5\n32.6\n39.9\n3.5\n20.3\n46.3\n63.0\n20.7\n78.7\n9.8\n19.1\n4.9\n4.3\n27.4\nReal\n48.7\n79.6\n47.9\n38.5\n48.9\n3.9\n25.5\n52.8\n74.9\n31.2\n88.0\n27.4\n32.8\n16.7\n5.2\n41.5\nSyn+Real\n54.5\n82.9\n53.1\n46.3\n57.3\n5.1\n29.6\n61.4\n78.2\n31.1\n92.5\n29.6\n41.1\n14.5\n6.5\n45.6\n16M\nSyn\n44.2\n32.4\n11.5\n41.6\n51.3\n4.9\n27.4\n58.3\n72.1\n24.8\n83.6\n16.7\n29.5\n4.6\n5.9\n33.9\nReal\n62.9\n85.2\n58.1\n49.0\n60.6\n5.0\n30.4\n61.9\n81.5\n40.9\n93.1\n43.2\n39.4\n28.0\n7.4\n49.8\nSyn+Real\n64.8\n87.5\n61.0\n53.7\n63.3\n4.9\n36.5\n67.7\n82.8\n38.6\n94.5\n37.6\n48.2\n28.6\n8.2\n51.9\n128M\nSyn\n63.7\n45.1\n15.9\n52.3\n67.1\n9.3\n37.8\n75.7\n80.5\n39.1\n93.2\n8.0\n35.7\n10.1\n9.5\n42.9\nReal\n81.9\n90.5\n70.9\n62.5\n78.7\n10.7\n46.0\n85.9\n88.7\n60.4\n96.0\n48.3\n57.8\n42.7\n14.2\n62.3\nSyn+Real\n81.6\n91.0\n70.4\n64.0\n79.4\n11.9\n52.5\n85.1\n90.2\n59.5\n97.0\n47.3\n61.1\n45.3\n14.1\n63.4\n371M\nSyn\n70.1\n51.9\n26.2\n55.5\n70.8\n12.3\n41.5\n79.6\n83.6\n45.5\n95.7\n28.8\n39.3\n20.6\n10.9\n48.8\nReal\n85.7\n93.9\n75.6\n67.5\n83.3\n14.2\n50.1\n88.8\n91.1\n67.0\n97.0\n43.9\n66.6\n42.8\n17.5\n65.7\nSyn+Real\n84.6\n92.4\n73.2\n67.1\n82.0\n17.2\n56.8\n86.4\n91.7\n61.6\n97.3\n52.2\n65.9\n46.7\n16.0\n66.1\nsists between synthetic and real images, which is likely at-\ntributable to concept mismatches between generated images\nand corresponding texts in certain classes, as discussed in\nSection 5.2.\nMoreover, our results indicate that combining synthetic\nand real images during CLIP training can significantly en-\nhance zero-shot performance, particularly when the dataset\nis limited. For instance, in training scenarios with fewer\nthan 10 million image-text pairs, integrating synthetic im-\nages with real data can boost performance by up to 5%.\n6.3. Scaling on downstream datasets\nWe followed the same setup and extended our comparison\nto include the scaling behavior of synthetic versus real im-\nages on 15 fine-grained classification datasets, detailed in\nTable 1. This analysis indicates a scaling behavior in these\ndatasets that is consistent with our findings from the Im-\nageNet evaluations. Notably, a combination of synthetic\nand real images demonstrated superior performance in most\nscenarios, particularly when the total dataset size was un-\nder 100 million samples. In cases with extremely limited\ndata availability, such as with just 1 million samples, train-\ning on synthetic images occasionally yielded better perfor-\nmance than with real images, for some certain tasks, such\nas Pets [56] and SUN397 [86].\n7. Discussion\nIn this paper, we investigate the scaling laws of synthetic\ndata in model training and identify three key factors that\nsignificantly influence scaling behavior: the choice of mod-\nels, the classifier-free guidance scale, and the selection of\nprompts. After optimizing these elements and increasing\nthe scale of training data, we find that, as expected, syn-\nthetic data still does not scale as effectively as real data,\nparticularly for supervised classification on ImageNet. This\nlimitation largely stems from the inability of standard text-\nto-image models to accurately generate certain concepts.\nHowever, our study also highlights several scenarios where\nsynthetic data proves advantageous: (1) In certain classes,\nsynthetic data demonstrates better scaling behavior com-\npared to real data; (2) Synthetic data is particularly effective\nwhen real data is scarce, for instance, in CLIP training with\nlimited datasets; (3) Models trained on synthetic data may\nexhibit superior generalization to out-of-distribution data.\nWe hope our findings will pave the way for further research\nin this field.\nAcknowledgements.\nThe authors would like to thank\nShobhita Sundaram, Julia Chae, Sara Beery, and the Vis-\nCam team for fruitful discussions, Yuanzhen Li for help-\ning with computation resources, and Jason Baldridge and\nSergey Ioffe for guidance and check on publication policy.\n9\nReferences\n[1] Hassan Abu Alhaija,\nSiva Karthik Mustikovela,\nLars\nMescheder, Andreas Geiger, and Carsten Rother.\nAug-\nmented reality meets computer vision: Efficient data gen-\neration for urban driving scenes. IJCV, 2018. 2\n[2] Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning\nHsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller,\nNaman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling\nlaws for generative mixed-modal language models. arXiv\npreprint arXiv:2301.03728, 2023. 2\n[3] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiao-\nhua Zhai.\nRevisiting neural scaling laws in language and\nvision. NeurIPS, 2022. 3\n[4] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-\nhammad Norouzi, and David J Fleet. Synthetic data from\ndiffusion models improves imagenet classification.\narXiv\npreprint arXiv:2304.08466, 2023. 2\n[5] Manel Baradad Jurjo, Jonas Wulff, Tongzhou Wang, Phillip\nIsola, and Antonio Torralba. Learning to see by looking at\nnoise. In NeurIPS, 2021. 2\n[6] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\nYufei Guo, et al. Improving image generation with better\ncaptions, 2023. 2\n[7] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101\u2013mining discriminative components with random\nforests. In ECCV, 2014. 8, 14\n[8] Victor Boutin, Thomas Fel, Lakshya Singhal, Rishav\nMukherji, Akash Nagaraj, Julien Colin, and Thomas Serre.\nDiffusion models as artists: Are we closing the gap between\nhumans and machines?\narXiv preprint arXiv:2301.11722,\n2023. 3\n[9] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 2, 3, 25\n[10] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts.\nIn CVPR,\n2021. 27\n[11] Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool.\nLearning semantic segmentation from synthetic data: A ge-\nometrically guided input-output adaptation approach.\nIn\nCVPR, 2019. 2\n[12] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens-\ning image scene classification: Benchmark and state of the\nart. Proceedings of the IEEE, 2017. 14\n[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.\nPalm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 2\n[14] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy\nMohamed, and Andrea Vedaldi. Describing textures in the\nwild. In CVPR, 2014. 14\n[15] Adam Coates, Andrew Ng, and Honglak Lee. An analysis\nof single-layer networks in unsupervised feature learning. In\nAISTATS, 2011. 14\n[16] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In CVPR workshops, 2020. 13\n[17] Yabo Dan, Yong Zhao, Xiang Li, Shaobo Li, Ming Hu, and\nJianjun Hu. Generative adversarial networks (gan) based ef-\nficient sampling of chemical composition space for inverse\ndesign of inorganic materials. NPJ Computational Materi-\nals, 2020. 2\n[18] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, et al. Scaling vision transformers to 22 billion pa-\nrameters. In ICML, 2023. 2\n[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 1\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 4\n[21] Li Fei-Fei, Robert Fergus, and Pietro Perona.\nOne-shot\nlearning of object categories. TPAMI, 2006. 14\n[22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. NeurIPS, 2014.\n2\n[23] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and\nparameter scaling laws for neural machine translation. In\nEMNLP, 2021. 2\n[24] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing\nZhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic\ndata from generative models ready for image recognition?\narXiv preprint arXiv:2210.07574, 2022. 2, 3\n[25] Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari,\nand Mohammad Norouzi. Generate, annotate, and learn: Nlp\nwith synthetic text. TACL, 2022. 2\n[26] Patrick Helber, Benjamin Bischke, Andreas Dengel, and\nDamian Borth. Eurosat: A novel dataset and deep learning\nbenchmark for land use and land cover classification. IEEE\nJournal of Selected Topics in Applied Earth Observations\nand Remote Sensing, 2019. 14\n[27] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\nvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt,\nand Justin Gilmer. The many faces of robustness: A critical\nanalysis of out-of-distribution generalization. ICCV, 2021.\n6, 19\n[28] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\nhardt, and Dawn Song. Natural adversarial examples. CVPR,\n2021. 6, 19\n[29] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun, Tom B\n10\nBrown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws\nfor autoregressive generative modeling.\narXiv preprint\narXiv:2010.14701, 2020. 2\n[30] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam\nMcCandlish.\nScaling laws for transfer.\narXiv preprint\narXiv:2102.01293, 2021. 2\n[31] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory\nDiamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali\nPatwary, Yang Yang, and Yanqi Zhou. Deep learning scaling\nis predictable, empirically. arXiv preprint arXiv:1712.00409,\n2017. 2\n[32] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. NeurIPS, 2017. 3, 19\n[33] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 3\n[34] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 2020. 2\n[35] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, et al. Training compute-optimal large language mod-\nels. arXiv preprint arXiv:2203.15556, 2022. 2\n[36] Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola.\nGenerative models as a data source for multiview represen-\ntation learning. arXiv preprint arXiv:2106.05258, 2021. 2\n[37] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. In ICML, 2021. 1\n[38] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park. Scaling up\ngans for text-to-image synthesis. In CVPR, 2023. 2\n[39] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. arXiv preprint arXiv:2001.08361,\n2020. 2, 3, 6\n[40] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 13\n[41] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n3d object representations for fine-grained categorization. In\nICCV workshops, 2013. 8, 14\n[42] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 1, 14\n[43] Varun Kumar, Ashutosh Choudhary, and Eunah Cho. Data\naugmentation using pre-trained transformer models. arXiv\npreprint arXiv:2003.02245, 2020. 2\n[44] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 3\n[45] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-\nhofer, and Kaiming He. Scaling language-image pre-training\nvia masking. In CVPR, 2023. 13\n[46] Hao Liu, Tom Zahavy, Volodymyr Mnih, and Satinder Singh.\nPalm up: Playing in the latent manifold for unsupervised pre-\ntraining. arXiv preprint arXiv:2210.10913, 2022. 2\n[47] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 14\n[48] Subhransu Maji,\nEsa Rahtu,\nJuho Kannala,\nMatthew\nBlaschko, and Andrea Vedaldi. Fine-grained visual classi-\nfication of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n14\n[49] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Gener-\nating training data with language models: Towards zero-shot\nlanguage understanding. arXiv preprint arXiv:2202.04538,\n2022. 2\n[50] George A Miller. Wordnet: a lexical database for english.\nCommunications of the ACM, 1995. 3\n[51] Masato Mimura, Sei Ueno, Hirofumi Inaguma, Shinsuke\nSakai, and Tatsuya Kawahara.\nLeveraging sequence-to-\nsequence speech synthesis for enhancing acoustic-to-word\nspeech recognition. In SLT, 2018. 2\n[52] Arthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan\nStanciulescu, and Arnaud de La Fortelle. Lens: Localization\nenhanced by nerf synthesis. In CoRL, 2022. 2\n[53] Norman Mu, Alexander Kirillov, David Wagner, and Sain-\ning Xie. Slip: Self-supervision meets language-image pre-\ntraining. In ECCV, 2022. 13\n[54] Maria-Elena Nilsback and Andrew Zisserman. Automated\nflower classification over a large number of classes. In 2008\nSixth Indian Conference on Computer Vision, Graphics &\nImage Processing, 2008. 14\n[55] OpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023. 2\n[56] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nCV Jawahar. Cats and dogs. In CVPR, 2012. 8, 9, 14\n[57] Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko.\nLearning deep object detectors from 3d models. In CVPR,\n2015. 2\n[58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 1, 8, 13, 14\n[59] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Milli-\ncan, Jordan Hoffmann, Francis Song, John Aslanides, Sarah\nHenderson, Roman Ring, Susannah Young, et al. Scaling\nlanguage models: Methods, analysis & insights from train-\ning gopher. arXiv preprint arXiv:2112.11446, 2021. 2\n[60] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. JMLR, 2020. 3\n[61] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and\nVaishaal Shankar. Do imagenet classifiers generalize to im-\nagenet? In ICML, 2019. 6, 19\n[62] Zhongzheng Ren and Yong Jae Lee.\nCross-domain self-\nsupervised multi-task feature learning using synthetic im-\nagery. In CVPR, 2018. 2\n11\n[63] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi\nZelnik-Manor.\nImagenet-21k pretraining for the masses.\narXiv preprint arXiv:2104.10972, 2021. 1\n[64] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 2, 3\n[65] German Ros, Laura Sellart, Joanna Materzynska, David\nVazquez, and Antonio M Lopez. The synthia dataset: A large\ncollection of synthetic images for semantic segmentation of\nurban scenes. In CVPR, 2016. 2\n[66] Andrew Rosenberg, Yu Zhang, Bhuvana Ramabhadran, Ye\nJia, Pedro Moreno, Yonghui Wu, and Zelin Wu.\nSpeech\nrecognition with augmented synthesized speech. In ASRU,\n2019. 2\n[67] Nick Rossenbach, Albert Zeyer, Ralf Schl\u00a8uter, and Hermann\nNey.\nGenerating synthetic audio data for attention-based\nspeech recognition systems. In ICASSP, 2020. 2\n[68] Artem Rozantsev, Vincent Lepetit, and Pascal Fua.\nOn\nrendering synthetic images for training an object detector.\nCVIU, 2015. 2\n[69] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 2022. 2, 3, 24\n[70] Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and\nYannis Kalantidis. Fake it till you make it: Learning trans-\nferable representations from synthetic imagenet clones. In\nCVPR, 2023. 2, 13\n[71] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLaion-5b:\nAn open large-scale dataset for\ntraining next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022. 2\n[72] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In ACL,\n2018. 1\n[73] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2\n[74] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya\nGanguli, and Ari Morcos. Beyond neural scaling laws: beat-\ning power law scaling via data pruning. NeurIPS, 2022. 3\n[75] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and\nChristian Igel. The german traffic sign recognition bench-\nmark: a multi-class classification competition. In IJCNN,\n2011. 14\n[76] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross\nWightman, Jakob Uszkoreit, and Lucas Beyer. How to train\nyour vit? data, augmentation, and regularization in vision\ntransformers. arXiv preprint arXiv:2106.10270, 2021. 13\n[77] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In ICCV, 2017. 1\n[78] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,\nXuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B\nHashimoto.\nAlpaca:\nA strong, replicable instruction-\nfollowing model. Stanford Center for Research on Founda-\ntion Models., 2023. 2\n[79] Bart Thomee, David A Shamma, Gerald Friedland, Ben-\njamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and\nLi-Jia Li. Yfcc100m: The new data in multimedia research.\nCommunications of the ACM, 2016. 14\n[80] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and\nDilip Krishnan. Stablerep: Synthetic images from text-to-\nimage models make strong visual representation learners.\narXiv preprint arXiv:2306.00984, 2023. 2\n[81] Allan Tucker, Zhenchen Wang, Ylenia Rotalinti, and Puja\nMyles.\nGenerating high-fidelity synthetic patient data for\nassessing machine learning healthcare software. NPJ digital\nmedicine, 2020. 2\n[82] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. NeurIPS, 2017. 2\n[83] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NeurIPS, 2017. 2\n[84] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P\nXing. Learning robust global representations by penalizing\nlocal predictive power. In NeurIPS, 2019. 6, 19\n[85] Ross Wightman.\nPytorch image models.\nhttps :\n//github.com/huggingface/pytorch-image-\nmodels, 2019. 3\n[86] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba.\nSun database: Large-scale scene\nrecognition from abbey to zoo. In CVPR, 2010. 9, 14\n[87] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-\nsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-\nHsuan Yang. Diffusion models: A comprehensive survey of\nmethods and applications. ACM Computing Surveys, 2022.\n2\n[88] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha\nSwayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bha-\ngavatula, Yejin Choi, and Doug Downey. Generative data\naugmentation for commonsense reasoning. arXiv preprint\narXiv:2004.11546, 2020. 2\n[89] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Tsung-Yi\nLin, Alberto Rodriguez, and Phillip Isola. Nerf-supervision:\nLearning dense object descriptors from neural radiance\nfields. In ICRA, 2022. 2\n[90] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2022. 2\n[91] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-\ncas Beyer. Scaling vision transformers. In CVPR, 2022. 2\n[92] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 3, 19\n[93] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-\nralba, and Aude Oliva.\nLearning deep features for scene\nrecognition using places database. NeurIPS, 2014. 13\n12\nAppendices\nA. Details on Supervised Training\nA.1. Training Hyper-parameters\nSupervised training was conducted on both the real Ima-\ngeNet training set and various synthetic ImageNet gener-\nated by text-to-iamge models at different dataset scales. To\nensure fair comparisons across different setups, identical\ntraining hyper-parameters were used for both real and syn-\nthetic images. Our training approach aligns with the setup\ndescribed in [76], utilizing binary cross-entropy loss. The\nnumber of total training iterations and warm-up iterations\nwere adjusted in proportion to the dataset scale in logarith-\nmic space. For instance, models at the 1 million scale were\ntrained for 95k iterations with a 10k iteration warm-up pe-\nriod. At the 2 million scale, training was extended to 190k\niterations with a 20k iteration warm-up, and for the 4 mil-\nlion scale, the training and warm-up periods were increased\nto 285k and 30k iterations, respectively. More detailed de-\nscriptions of the training hyper-parameters are provided in\nTable A1.\nTable A1. Detailed pre-training hyper-parameters for supervised\ntraining on both real ImageNet training set and synthetic ImageNet\ngenerated by text-to-image models.\nConfig\nValue\nBatch size\n4096\nOptimizer\nAdam [40]\nLearning rate\n3 \u00d7 10\u22123\nWeight decay\n0.1\nAdam \u03b2\n\u03b21, \u03b22 = (0.9, 0.999)\nTotal iterations\n95k for 1M\nWarm up iterations\n10k for 1M\nLearning rate schedule\ncosine decay\nMixup\n0.5\nDropout\n0.1\nStochastic depth\n0.1\nAugmentation\nRandAug(2, 15) [16]\nA.2. Details on Text Prompts\nIn this section, we provide more details on the different con-\nfigurations of text prompts used for Class-specific Prompts,\nas outlined in Section 3.1. For the Classnames + Hyper-\nnyms configuration, we utilized all hypernyms associated\nwith each specific ImageNet category, separated by com-\nmas. Regarding CLIP templates, we employed two sets of\nprompt templates with different number of sentences. The\nfirst one includes the 80 distinct sentence originally used in\nthe CLIP paper [58] and its inference code3. The second set\n3https://github.com/openai/CLIP/tree/main/notebooks\nincludes a subset of 7 templates, as recommended in [45].\nAdditionally, we incorporated two more prompt configura-\ntions for comparison, following the approach in [70]: (1)\nClassnames + Description + Places, which combines Im-\nageNet class names with their WordNet descriptions, fol-\nlowed by a background category sampled from the Places\ndataset [93]. (2) Classnames + Hypernyms + Places, which\nis similar to the previous configuration but replaces the de-\nscriptions with WordNet hypernyms, also incorporating a\nbackground category from Places.\nTogether with the configurations described in Section 3.1\nof the main paper, these methods result in a total of 8 dif-\nferent configurations for text prompts when generating im-\nages for ImageNet categories. Additional visualizations of\nimages produced by each these prompt configurations are\nincluded in Appendix F.\nA.3. Evaluation on Downstream Datasets\nIn addition to evaluating the trained supervised classifiers\ndirectly, we also conducted linear probing on 15 different\nfine-grained classification datasets. Detailed descriptions of\nthese datasets can be found in Appendix B.2. To perform\nlinear probing on these datasets, we first removed the linear\nclassification head from the classifier trained on ImageNet.\nThen, we extracted features from both the training and test-\ning sets of each dataset, without applying any data augmen-\ntation. Subsequently, logistic regression was employed on\nthese extracted features. The logistic regression layer was\noptimized using L-BFGS, with a maximum number of itera-\ntions equals 500. For a detailed comparison of these results,\nplease refer to Appendix E.\nB. Details on CLIP Training\nB.1. Hyper-Parameters\nPrevious studies [53, 58] along with our empirical analysis,\nindicate the necessity of using different training parameters\naccording to dataset scale. Specifically, for smaller-scale\ndatasets, a larger learning rate and weight decay are rec-\nommended to mitigate overfitting. Conversely, for larger\ndatasets, both the learning rate and weight decay should be\nreduced. Accordingly, we have followed two distinct sets\nof hyper-parameters within the CLIP training pipeline, one\ntailored for datasets with fewer than 100 million captions\nfollowing the parameter in [53], and another for those ex-\nceeding this threshold following the parameter in [58]. The\nspecific parameters for both configurations are outlined in\nTable A3. Models are trained for 32 epochs across all data\nscales. The number of warmup steps was set to 600 for the\n1 million scale, 1200 for the 2 million scale, and 2000 for\nscales of 4 million or greater. It is important to note that\nwe maintained consistent training hyper-parameters across\nall three different types of data sources (synthetic, real, syn-\n13\nTable A2. Detailed metrics and number of training and testing images of the downstream classification datasets. Only test images are used\nin the zero-shot classification task for CLIP evaluation.\nDataset\n# Categories\n# Train Images\n# Test Images\nVal Metric\nFood-101 [7]\n101\n75,750\n25,250\nTop-1 Accuracy\nCIFAR-10 [42]\n10\n50,000\n10,000\nTop-1 Accuracy\nCIFAR-100 [42]\n100\n50,000\n10,000\nTop-1 Accuracy\nSUN397 [86]\n397\n19,850\n19,850\nTop-1 Accuracy\nStanford Cars [41]\n196\n8,144\n8,041\nTop-1 Accuracy\nFGVC Aircraft [48]\n100\n6,667\n3,333\nMean per class\nDTD [14]\n47\n3,760\n1,880\nTop-1 Accuracy\nOxford Pets [56]\n37\n3,680\n3,669\nMean per class\nCaltech-101 [21]\n102\n3,060\n6,085\nMean per class\nOxford Flowers [54]\n102\n2,040\n6,149\nMean per class\nSTL-10 [15]\n10\n1,000\n8,000\nTop-1 Accuracy\nEuroSAT [26]\n10\n10,000\n5,000\nTop-1 Accuracy\nRESISC45 [12]\n45\n25,200\n6,300\nTop-1 Accuracy\nGTSRB [75]\n43\n26,640\n12,630\nTop-1 Accuracy\nCountry211 [58, 79]\n211\n42,200\n21,100\nTop-1 Accuracy\nthetic+real) at the same data scale to ensure fair compar-\nisons. For an in-depth comparison of the effects of different\nhyper-parameters in CLIP training at different scales, please\nrefer to the experimental details provided in Appendix H.1.\nTable A3. Detailed pre-training hyper-parameters for CLIP at dif-\nferent dataset scales.\n(a) Hyper-parameter for CLIP with < 100M samples.\nConfig\nValue\nBatch size\n8192\nOptimizer\nAdamW [47]\nLearning rate\n1 \u00d7 10\u22123\nWeight decay\n0.5\nAdam \u03b2\n\u03b21, \u03b22 = (0.9, 0.98)\nAdam \u03f5\n1 \u00d7 10\u22128\nTotal epochs\n32\nWarm up iterations\n600, 1200, 2000\nLearning rate schedule\ncosine decay\n(b) Hyper-parameter for CLIP with \u2265 100M samples.\nConfig\nValue\nBatch size\n32768\nOptimizer\nAdamW [47]\nLearning rate\n5 \u00d7 10\u22124\nWeight decay\n0.2\nAdam \u03b2\n\u03b21, \u03b22 = (0.9, 0.98)\nAdam \u03f5\n1 \u00d7 10\u22126\nTotal epochs\n32\nWarm up iterations\n2000\nLearning rate schedule\ncosine decay\nB.2. Downstream Dataset\nFor all the pre-trained CLIP models, we conducted zero-\nshot evaluations on ImageNet and 15 other widely used\ndownstream classification datasets. These datasets include\nFood-101 [7], Stanford Cars [41], SUN397 [86], Oxford\nPets [56], among others. Detailed information about these\nevaluation datasets can be found in Table A2. It\u2019s important\nto note that for zero-shot evaluations, only the test images\nfrom these datasets are used.\nB.3. Zero-shot Evaluation Details\nWe employed the same text prompt templates as referenced\nin [58], following a similar text ensembling strategy. For\neach category, text features were computed for every sin-\ngle template, and the mean average of these features across\nall templates was used to represent the final text feature for\nthat specific category. Given that CLIP training involves a\ntrainable temperature parameter, \u03c4, it is necessary to incor-\nporate this parameter during zero-shot evaluation to accu-\nrately compute the zero-shot classification loss. Let zimg\nbe the image feature from the visual encoder, ztxt denote\nthe aggregated text feature. Assuming a total of C classes,\nwith ztxtc as the text feature for c \u2212 th category, the zero-\nshot classification loss is calculated as follows:\nL = \u2212 log\nexp(sim(zimg, ztxtc) \u00b7 \u03c4)\nPC\nc\u2032=1 exp(sim(zimg, ztxtc\u2032 ) \u00b7 \u03c4)\nHere sim(zimg, ztxtc) calculates the dot product, measuring\nthe similarity between image feature and text features for\neach category.\nC. Details for Performance at 1.3M\nAs detailed in Section 4.2 of the main paper, we adopted\n54 different configurations encompassing distinct text-to-\nimage models, CFG scales, and text prompts to generate 1.3\nmillion synthetic images for each configuration. Following\nthe generation process, a supervised model was trained on\nthe images geenrated by each configuration. To facilitate a\n14\nTable A4. Detailed comparison on ImageNet Validation performance and recognizability, diversity, FID and LPIPS for different CFG scale\nand prompt configurations with Stable Diffusion as the text-to-image model.\nText-to-Image Model: Stable Diffusion, main comparison on Prompt Config\nCFG Scale\nPrompt Config\nIN loss(\u2193)\nIN Top1\nRecognizability\nDiversity\nFID(\u2193)\nLPIPS(\u2193)\n2\nWord2Sen\n3.27\n38.42\n0.315\n0.850\n3.566\n0.717\nCLIP Templates (7)\n2.63\n49.26\n0.522\n0.781\n3.297\n0.714\nCLIP Templates (80)\n2.76\n49.88\n0.510\n0.790\n3.437\n0.719\nClassnames+Hypernym\n3.28\n45.19\n0.569\n0.713\n2.553\n0.698\nClassnames+Description\n3.27\n45.05\n0.615\n0.696\n2.678\n0.697\nClassnames+Hypernym+Places\n3.14\n43.91\n0.381\n0.836\n4.441\n0.712\nClassnames+Description+Places\n3.06\n46.18\n0.524\n0.758\n2.890\n0.704\nClassnames\n3.05\n47.82\n0.603\n0.718\n2.589\n0.702\nIN-Captions\n2.23\n55.04\n0.573\n0.757\n2.450\n0.714\n7.5\nCLIP Templates (7)\n4.17\n38.39\n0.702\n0.650\n5.681\n0.731\nCLIP Templates (80)\n3.86\n40.26\n0.687\n0.670\n5.619\n0.739\nClassnames\n4.96\n31.21\n0.780\n0.541\n4.113\n0.707\nIN-Captions\n3.56\n40.38\n0.725\n0.632\n3.641\n0.737\nTable A5. Detailed comparison on ImageNet Validation performance and recognizability, diversity, FID and LPIPS for different text-to-\nimage models and CFG scales. All configurations use IN-Captions as prompts.\nText Prompt: IN-Captions, main comparison on Text-to-image models\nText-to-Image Model\nCFG Scale\nIN loss(\u2193)\nIN Top1\nRecognizability\nDiversity\nFID(\u2193)\nLPIPS(\u2193)\nStable Diffusion\n1.5\n2.14\n54.66\n0.484\n0.800\n2.403\n0.710\n2\n2.23\n55.04\n0.573\n0.757\n2.450\n0.714\n3\n2.38\n54.10\n0.655\n0.705\n2.790\n0.722\n4\n2.61\n51.13\n0.690\n0.675\n3.100\n0.728\n6\n2.92\n46.99\n0.717\n0.644\n3.483\n0.734\n7.5\n3.56\n40.38\n0.725\n0.632\n3.641\n0.737\n8\n3.47\n40.87\n0.726\n0.629\n3.655\n0.738\n10\n3.42\n33.59\n0.730\n0.621\n3.723\n0.740\nImagen\n1\n1.84\n58.52\n0.466\n0.810\n3.451\n0.713\n1.5\n1.78\n61.51\n0.647\n0.719\n4.546\n0.733\n2\n1.93\n60.58\n0.714\n0.671\n6.867\n0.701\nMuse\n0.1\n2.05\n54.19\n0.473\n0.789\n4.057\n0.755\n0.3\n2.08\n54.45\n0.520\n0.760\n4.616\n0.749\n0.5\n2.13\n54.03\n0.554\n0.738\n5.189\n0.745\n1\n2.37\n51.55\n0.599\n0.700\n6.274\n0.734\nclearer comparison in our tables, we have categorized these\n54 configurations into three distinct groups, with each group\nfocusing on specific comparative factors:\n\u2022 The first group exclusively uses Stable Diffusion as the\ntext-to-image model. The primary comparison focus here\nis on the impact of varying text prompt configurations.\n\u2022 The second group standardizes the text prompt configu-\nration to IN-Caption. This group\u2019s aim is to assess the\neffects of using different text-to-image models and to\nunderstand the behavior of CFG scales within each spe-\ncific model, and to find the optimal CFG scale for each of\nthem.\n\u2022 The third group also exclusively uses Stable Diffusion as\nthe text-to-image model. Here, the comparison emphasis\nis on the impact of different CFG scales under different\ntext prompt configurations.\nBy grouping the configurations into these three different\ngroups, we aim to provide a more structured and compre-\nhensible analysis. In each of the three groups, we present\nthe detailed validation loss (the negative log loss here is\nused to plot Figure 2 in the main paper) and top-1 accuracy\non ImageNet validation set for models trained with different\nconfigurations, all under the scale of 1.3 million images.\nTable A4 presents the analysis for the first group.\nIt\n15\nTable A6. Detailed comparison on ImageNet Validation performance and recognizability, diversity, FID(\u2193) and LPIPS(\u2193) for different\ntext-to-image models and CFG scales. All configurations use Stable Diffusion as the text-to-image model.\nText-to-Image Model: Stable Diffusion, main comparison on CFG Scale\nPrompt Config\nCFG Scale\nIN loss(\u2193)\nIN Top1\nRecognizability\nDiversity\nFID(\u2193)\nLPIPS(\u2193)\nClassNames\n1.5\n2.84\n48.10\n0.499\n0.778\n2.525\n0.702\n2\n3.05\n47.82\n0.603\n0.718\n2.589\n0.702\n3\n3.41\n44.91\n0.697\n0.644\n2.981\n0.704\n4\n3.80\n41.55\n0.734\n0.602\n3.383\n0.705\n6\n4.82\n33.58\n0.770\n0.559\n3.897\n0.707\n7.5\n4.96\n31.21\n0.780\n0.541\n4.113\n0.707\n8\n5.04\n29.78\n0.780\n0.537\n4.167\n0.707\n10\n5.47\n26.13\n0.787\n0.524\n4.289\n0.707\nClassNames+Description\n1.5\n3.05\n45.84\n0.523\n0.753\n2.468\n0.697\n2\n3.27\n45.05\n0.615\n0.696\n2.678\n0.697\n3\n3.87\n41.78\n0.699\n0.627\n3.265\n0.699\n4\n4.14\n38.92\n0.738\n0.588\n3.714\n0.701\n6\n4.58\n33.52\n0.762\n0.546\n4.223\n0.703\nClassNames+Hypernym\n1.5\n3.06\n45.22\n0.475\n0.770\n2.463\n0.698\n2\n3.28\n45.19\n0.569\n0.713\n2.553\n0.698\n3\n3.70\n42.52\n0.652\n0.643\n2.961\n0.700\n4\n4.35\n38.02\n0.687\n0.604\n3.336\n0.701\n6\n4.69\n33.38\n0.717\n0.561\n3.831\n0.703\nCLIP Templates (80)\n1.25\n2.58\n47.59\n0.344\n0.856\n3.193\n0.712\n1.5\n2.61\n49.00\n0.413\n0.831\n3.200\n0.714\n1.75\n2.66\n49.77\n0.468\n0.809\n3.284\n0.717\n2\n2.76\n49.88\n0.510\n0.790\n3.437\n0.719\n3\n3.00\n48.76\n0.600\n0.740\n4.098\n0.726\n4\n3.27\n46.56\n0.642\n0.711\n4.653\n0.731\n6\n3.70\n42.24\n0.677\n0.681\n5.349\n0.736\n7.5\n3.86\n40.26\n0.687\n0.670\n5.619\n0.739\nCLIP Templates (7)\n1.25\n2.57\n47.86\n0.350\n0.851\n3.029\n0.709\n1.5\n2.63\n49.24\n0.424\n0.824\n3.017\n0.711\n1.75\n2.69\n50.20\n0.478\n0.801\n3.116\n0.712\n2\n2.63\n49.26\n0.522\n0.781\n3.297\n0.714\n3\n3.07\n48.69\n0.617\n0.726\n4.025\n0.720\n4\n3.42\n45.93\n0.658\n0.695\n4.661\n0.724\n6\n4.00\n41.15\n0.692\n0.663\n5.393\n0.729\n7.5\n4.17\n38.39\n0.702\n0.650\n5.681\n0.731\nIN-Captions\n1.5\n2.14\n54.66\n0.484\n0.800\n2.403\n0.710\n2\n2.23\n55.04\n0.573\n0.757\n2.450\n0.714\n3\n2.38\n54.10\n0.655\n0.705\n2.790\n0.722\n4\n2.61\n51.13\n0.690\n0.675\n3.100\n0.728\n6\n2.92\n46.99\n0.717\n0.644\n3.483\n0.734\n7.5\n3.56\n40.38\n0.725\n0.632\n3.641\n0.737\n8\n3.47\n40.87\n0.726\n0.629\n3.655\n0.738\n10\n3.42\n33.59\n0.730\n0.621\n3.723\n0.740\nshows that using IN-Caption as the text prompt yields the\nbest performance across both CFG scales of 2 and 7.5.\nThis superior performance is largely attributed to its abil-\nity to guide the text-to-image model to generate diverse im-\nages while maintaining high recognizability, thereby justi-\nfying our choice of IN-Caption for most of our experiments.\nIn the second group, detailed in Table A5, we observe\nthat different text-to-image models exhibit varying optimal\nCFG scales for image generation to train supervised mod-\nels. Specifically, Stable Diffusion, Imagen, and Muse reach\ntheir optimal performance at CFG scales of 2, 1.5, and 0.3,\nrespectively. These findings validate our decision to em-\nploy these specific CFG scales in our later study of scaling\nbehavior for each model. Table A6 covers the third group\u2019s\n16\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nDiversity\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nDifferent Prompt\nWord2Sen\nCLIP templates\nHypernym\nDescription\nHypernym+Place\nDescription+Place\nClassnames\nIN-Caption\n0.35\n0.40\n0.45\n0.50\n0.55\nAccuracy at 1.3M\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nDiversity\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nDifferent T2i Models\nStable Diffusion\nImagen\nMuse\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nAccuracy at 1.3M\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nDiversity\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nDifferent CFG Scale\n1.25\n1.5\n1.75\n2\n3\n4\n6\n7.5\n8\n10\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nAccuracy at 1.3M\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nDiversity\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nWord2Sen\nCLIP templates\nHypernym\nDescription\nHypernym+Place\nDescription+Place\nClassnames\nIN-Caption\n1.6\n1.5\n1.4\n1.3\n1.2\n1.1\n1.0\n0.9\nPerformance at 1.3M (negative log loss)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nDiversity\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nStable Diffusion\nImagen\nMuse\n1.2\n1.1\n1.0\n0.9\n0.8\n0.7\n0.6\nPerformance at 1.3M (negative log loss)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nDiversity\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\n1.25\n1.5\n1.75\n2\n3\n4\n6\n7.5\n8\n10\n1.6\n1.4\n1.2\n1.0\n0.8\nPerformance at 1.3M (negative log loss)\nFigure A1. Recognizability versus diversity plot for different text-to-image configuration groups as described in Appendix C. Each column\ncorresponds to one comparison group. The first column mainly compares on text prompts and corresponds to Table A4. The second column\ncompares different text-to-image models and corresponds to Table A5. The last column mainly compares optimal CFG scale for Stable\nDiffusion and corresponds to Table A6. On the top row, each point is colored by the top-1 accuracy in ImageNet validation set, on the\nbottom row the points are colored by the negative log loss.\n7\n6\n5\n4\n3\n-FID (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nDifferent Prompt\nWord2Sen\nCLIP templates\nHypernym\nDescription\nHypernym+Place\nDescription+Place\nClassnames\nIN-Caption\n0.35\n0.40\n0.45\n0.50\n0.55\nAccuracy at 1.3M\n7\n6\n5\n4\n3\n-FID (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nDifferent T2i Models\nStable Diffusion\nImagen\nMuse\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nAccuracy at 1.3M\n7\n6\n5\n4\n3\n-FID (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nDifferent CFG Scale\n1.25\n1.5\n1.75\n2\n3\n4\n6\n7.5\n8\n10\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nAccuracy at 1.3M\n7\n6\n5\n4\n3\n-FID (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nWord2Sen\nCLIP templates\nHypernym\nDescription\nHypernym+Place\nDescription+Place\nClassnames\nIN-Caption\n1.6\n1.5\n1.4\n1.3\n1.2\n1.1\n1.0\n0.9\nPerformance at 1.3M (negative log loss)\n7\n6\n5\n4\n3\n-FID (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nStable Diffusion\nImagen\nMuse\n1.2\n1.1\n1.0\n0.9\n0.8\n0.7\n0.6\nPerformance at 1.3M (negative log loss)\n7\n6\n5\n4\n3\n-FID (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\n1.25\n1.5\n1.75\n2\n3\n4\n6\n7.5\n8\n10\n1.6\n1.4\n1.2\n1.0\n0.8\nPerformance at 1.3M (negative log loss)\nFigure A2. Recognizability versus FID plot for different text-to-image configuration groups as described in Appendix C. Each column\ncorresponds to one comparison group. The detailed correspondence between the plot and tables is the same as Figure A1. On X-axis we\ntake the negative of FID, upper right indicates better metric with lower FID and higher recognizability.\n17\n0.75\n0.74\n0.73\n0.72\n0.71\n0.70\n-lpips (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nDifferent Prompt\nWord2Sen\nCLIP templates\nHypernym\nDescription\nHypernym+Place\nDescription+Place\nClassnames\nIN-Caption\n0.35\n0.40\n0.45\n0.50\n0.55\nAccuracy at 1.3M\n0.75\n0.74\n0.73\n0.72\n0.71\n0.70\n-lpips (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nDifferent T2i Models\nStable Diffusion\nImagen\nMuse\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nAccuracy at 1.3M\n0.75\n0.74\n0.73\n0.72\n0.71\n0.70\n-lpips (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nDifferent CFG Scale\n1.25\n1.5\n1.75\n2\n3\n4\n6\n7.5\n8\n10\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nAccuracy at 1.3M\n0.75\n0.74\n0.73\n0.72\n0.71\n0.70\n-lpips (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nWord2Sen\nCLIP templates\nHypernym\nDescription\nHypernym+Place\nDescription+Place\nClassnames\nIN-Caption\n1.6\n1.5\n1.4\n1.3\n1.2\n1.1\n1.0\n0.9\nPerformance at 1.3M (negative log loss)\n0.75\n0.74\n0.73\n0.72\n0.71\n0.70\n-lpips (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\nStable Diffusion\nImagen\nMuse\n1.2\n1.1\n1.0\n0.9\n0.8\n0.7\n0.6\nPerformance at 1.3M (negative log loss)\n0.75\n0.74\n0.73\n0.72\n0.71\n0.70\n-lpips (negative)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nRecognizability\n1.25\n1.5\n1.75\n2\n3\n4\n6\n7.5\n8\n10\n1.6\n1.4\n1.2\n1.0\n0.8\nPerformance at 1.3M (negative log loss)\nFigure A3. Recognizability versus LPIPS plot for different text-to-image configuration groups as described in Appendix C. Each column\ncorresponds to one comparison group. The detailed correspondence between the plot and tables is the same as Figure A1. On X-axis we\ntake the negative of LPIPS, upper right indicates better metric with lower LPIPS and higher recognizability.\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nDiversity\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nRecognizability\nOthers\nMuse IN-captions\nImagen IN-captions\nSD Classnames\nSD IN-captions\n1.6\n1.4\n1.2\n1.0\n0.8\n0.6\nPerformance at 1.3M (negative log loss)\n7\n6\n5\n4\n3\n-FID (negative)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nRecognizability\nOthers\nMuse IN-captions\nImagen IN-captions\nSD Classnames\nSD IN-captions\n1.6\n1.4\n1.2\n1.0\n0.8\n0.6\nPerformance at 1.3M (negative log loss)\n0.75\n0.74\n0.73\n0.72\n0.71\n0.70\n-lpips (negative)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nRecognizability\nOthers\nMuse IN-captions\nImagen IN-captions\nSD Classnames\nSD IN-captions\n1.6\n1.4\n1.2\n1.0\n0.8\n0.6\nPerformance at 1.3M (negative log loss)\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nDiversity\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nRecognizability\nOthers\nMuse IN-captions\nImagen IN-captions\nSD Classnames\nSD IN-captions\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nAccuracy at 1.3M\n7\n6\n5\n4\n3\n-FID (negative)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nRecognizability\nOthers\nMuse IN-captions\nImagen IN-captions\nSD Classnames\nSD IN-captions\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nAccuracy at 1.3M\n0.75\n0.74\n0.73\n0.72\n0.71\n0.70\n-lpips (negative)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nRecognizability\nOthers\nMuse IN-captions\nImagen IN-captions\nSD Classnames\nSD IN-captions\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nAccuracy at 1.3M\nFigure A4. Recognizability versus diversity, FID or LPIPS plot for all the different text-to-image configurations under 1.3M scale. The\nfirst column corresponds to diversity, while the latter two correspond to FID and LPIPS, respectively. On X-axis we take the negative of\nFID and LPIPS. In the plots upper right indicates better metric with higher diversity, lower FID or LPIPS, and higher recognizability. Each\npoint correspond to one configuration, and is color-coded by either the negative log loss (top row) or the top-1 accuracy (bottom) on the\nImageNet validation set.\n18\ncomparisons, focusing on finding the optimal CFG scales\nfor Stable Diffusion under different text prompts. The re-\nsults indicate that for text prompts with less diversity, such\nas using Classnames or Classnames+Hypernym, a smaller\noptimal CFG scale (1.5) is better since it will lead to more\ndiverse images during the generation process. In contrast,\nfor more diverse text prompts, like IN-Captions and CLIP\ntemplates with 80 sentences, since there is more diversity\non the text side relativly, a larger optimal CFG scale (2) is\nmore effective.\nIn addition, we have included a recognizability versus\ndiversity plot for each of the three comparison groups in\nFigure A1. Each point in these plots represents a specific\nconfiguration, and is color-coded based on either the top-1\nclassification accuracy or the negative log loss on ImageNet\nvalidation set. The figures illustrate a trade-off between di-\nversity and recognizability. Optimal performance is typi-\ncally observed when there is a relatively better and more\nbalanced trade-off between these two factors. Configura-\ntions characterized by either low diversity or low recogniz-\nability tend to result in suboptimal performance, indicating\nthe necessity of maintaining a balance between these two\nfactors.\nD. Evaluation under FID and LPIPS\nIn addition to diversity, we computed two other key metrics:\nFID (Frechet Inception Distance)[32] and LPIPS (Learned\nPerceptual Image Patch Similarity)[92]. Both of them are\nstandard evaluation metrics for the text-to-image generation\nmodels. Our study examines the performance variations in\nrelation to these two metrics. As detailed in Section 3.2 of\nthe main paper, these metric scores are also calculated using\nthe synthetic test sets, which comprises 50, 000 images for\neach configuration:\n\u2022 The FID scores are derived by measuring the Frechet In-\nception Distance [32] between the synthetic test set, con-\ntaining these 50,000 generated images, and the real Ima-\ngeNet validation set.\n\u2022 For LPIPS, we perform the calculation on a per-class ba-\nsis. We randomly select and compute the similarity be-\ntween 250 pairs of synthetic images for each class, and\nthe final LPIPS metric is computed as the average across\nall classes.\nThe comparison of FID and LPIPS scores across each group\nis presented in Tables A4, A5, and A6. Additionally, in\nFigures A2 and A3, we plot a detailed comparison of the\nperformance across different image generation configura-\ntion groups, substituting diversity with either FID or LPIPS.\nConsidering that lower scores for FID indicate better distri-\nbution match and for LPIPS implies larger intra-class diver-\nsity, we take the negative of these values for plotting pur-\nposes. This adjustment ensures consistency with the diver-\nsity plot on the X-axis, positioning better results towards the\nright.\nFurthermore, we incorporate comparisons using diver-\nsity, FID, or LPIPS as the X-axis for all 54 text-to-image\ngeneration configurations in Figure A4. Our findings reveal\nthat while there is a moderate correlation between the FID\nscore or LPIPS of generated images and the classification\nperformance of models trained on them, the relationship is\nnot definitive. In some cases, configurations with the same\nlevel of recognizability but lower FID scores or LPIPS show\ninferior classification performance. This suggests that while\nFID and LPIPS are effective metrics for evaluating the qual-\nity of the images generated by text-to-image models, their\ncorrelation with the performance of supervised classifiers\ntrained on synthetic images is not as strong as expected.\nThis observation underscores the need for a more specific\nmetric tailored to evaluate the performance of supervised\nclassifiers trained on such synthetic images.\nE. Detailed Scaling Behavior Comparison\nIn Tables A7 and A8, we present a comparison of the scaling\nbehavior of supervised models trained under various con-\nfigurations. This comparison is based on linear probing per-\nformed on 15 fine-grained classification datasets, as detailed\nin Appendix A.3. Our findings indicate that, in general, the\nscaling behavior observed in linear probing on these down-\nstream datasets aligns with the trends seen in the ImageNet\nvalidation set. However, there are instances where training\non synthetic images surpasses the performance of training\non real images, in the Food-101 dataset for example.\nAdditionally, we have also included the detailed com-\nparison on the out-of-distribution (OOD) validation sets,\nincluding ImageNet-A [28], ImageNet-R [27], ImageNet-\nSketch [84], and Imagenet-V2 [61]. The results from these\ncomparisons demonstrate that training on synthetic images\ncan yield improved performance on OOD test sets, exem-\nplified by the results on ImageNet-R.\nF. Visualization on Generated Images\nTo better understand the impact of various text prompts used\nin generating training images, we provide additional visual-\nizations of images created using different text prompt con-\nfigurations for specific ImageNet categories. These visu-\nalizations were generated using Stable Diffusion, with the\nCFG scale set to 2.\nIn Figure A6, we present a detailed visualization of the\nimages generated with different text prompt configurations\nfor three different ImageNet categories: Goldfish, Golden\nRetriever, and shopping carts.\nThe visualizations illus-\ntrate that incorporating more detailed information into the\nprompt tends to encourage the text-to-image model to gen-\nerate more diverse images. However, this increased diver-\nsity may potentially compromise the accuracy of the cate-\n19\nTable A7. Detailed scaling behavior on 15 different downstream classification datasets and ImageNet-A, ImageNet-R, ImageNet-Sketch\nand ImageNet-V2 validation set for supervised classifiers trained with real images from ImageNet training set and synthetic images from\nvarious configurations using Stable Diffusion. Dataset scale is in million.\nScale\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nDS Average\nImageNet-A\nImageNet-R\nImageNet-Sketch\nImageNet-V2\nReal ImageNet Training set\n0.125\n61.4\n80.6\n60.9\n47.3\n24.0\n31.0\n64.7\n77.2\n73.4\n86.7\n88.0\n95.8\n87.4\n57.0\n12.0\n63.2\n2.6\n14.6\n5.7\n36.3\n0.25\n65.5\n85.2\n66.2\n52.7\n30.0\n37.7\n67.7\n83.3\n81.0\n88.8\n92.5\n95.8\n88.0\n61.0\n11.9\n67.1\n3.6\n19.6\n10.2\n45.1\n0.5\n71.0\n89.7\n72.5\n57.6\n44.2\n43.4\n70.2\n88.8\n87.2\n92.1\n96.0\n95.8\n89.8\n65.2\n12.4\n71.7\n5.8\n27.3\n17.6\n55.7\n1\n77.0\n94.7\n80.0\n63.1\n57.3\n51.6\n72.8\n92.6\n92.8\n93.4\n98.1\n96.0\n90.5\n70.8\n13.9\n76.3\n15.6\n40.3\n29.4\n66.7\n1.3\n77.8\n94.6\n81.0\n64.3\n62.5\n53.1\n74.0\n93.5\n93.4\n93.4\n98.6\n96.1\n90.0\n71.7\n14.4\n77.2\n18.7\n42.2\n31.2\n68.8\nStable Diffusion, CFG scale=7.5, Classname\n0.125\n56.4\n75.8\n54.1\n43.2\n24.7\n31.5\n61.9\n73.1\n65.4\n83.9\n81.3\n94.7\n84.6\n53.8\n9.4\n59.6\n1.7\n15.4\n6.8\n20.6\n0.25\n59.1\n77.9\n56.7\n43.9\n28.7\n35.7\n61.1\n75.0\n67.2\n84.5\n84.0\n94.4\n84.7\n57.9\n9.7\n61.4\n1.8\n16.9\n8.1\n21.6\n0.5\n60.2\n79.4\n58.8\n46.5\n31.9\n37.6\n64.5\n77.7\n73.4\n84.7\n87.1\n95.3\n86.3\n61.1\n10.3\n63.7\n2.1\n19.6\n10.2\n23.4\n1\n61.2\n82.7\n61.4\n47.7\n35.6\n39.4\n62.1\n79.0\n73.0\n85.6\n87.4\n94.9\n86.5\n62.2\n10.3\n64.6\n2.5\n22.5\n13.1\n25.2\n2\n61.7\n82.6\n61.4\n48.9\n38.5\n39.3\n63.6\n79.4\n73.8\n85.6\n87.9\n94.4\n86.6\n64.1\n10.5\n65.2\n2.6\n24.9\n15.2\n25.8\n4\n61.5\n83.3\n62.2\n48.6\n35.9\n40.5\n63.2\n80.1\n76.2\n84.7\n89.0\n94.0\n86.1\n62.6\n10.5\n65.2\n3.1\n26.0\n16.3\n25.8\n8\n62.1\n83.8\n63.4\n48.5\n38.8\n39.7\n63.9\n79.6\n76.4\n83.2\n89.5\n93.8\n86.5\n63.3\n10.6\n65.5\n2.7\n27.3\n16.5\n26.2\n16\n61.2\n83.1\n62.9\n49.0\n36.3\n40.3\n62.4\n79.4\n77.1\n83.9\n89.4\n93.5\n86.6\n66.5\n10.7\n65.5\n3.3\n27.8\n18.0\n27.2\n32\n61.2\n84.4\n63.8\n49.6\n36.8\n38.6\n64.0\n78.9\n76.6\n82.8\n89.5\n93.8\n86.0\n63.9\n10.5\n65.4\n3.1\n28.2\n17.9\n26.2\n64\n61.8\n83.8\n63.7\n49.3\n37.3\n38.5\n62.4\n80.3\n76.9\n82.6\n89.8\n93.8\n86.0\n64.2\n10.7\n65.4\n3.2\n28.9\n18.0\n26.9\nStable Diffusion, CFG scale=2.0, Classname\n0.125\n61.2\n73.2\n51.6\n46.6\n25.6\n33.4\n62.8\n76.0\n68.2\n85.4\n84.1\n95.1\n86.6\n54.9\n9.9\n61.0\n2.4\n17.9\n6.6\n22.3\n0.25\n65.1\n77.5\n56.7\n51.1\n33.6\n38.6\n66.2\n82.3\n76.4\n88.6\n87.4\n95.1\n88.0\n57.2\n10.2\n64.9\n2.9\n23.9\n10.8\n28.7\n0.5\n69.5\n80.7\n61.2\n54.6\n46.9\n46.0\n68.3\n86.9\n83.6\n90.6\n91.5\n95.3\n89.4\n60.6\n11.1\n69.1\n3.3\n31.7\n16.5\n33.8\n1\n72.5\n84.9\n65.9\n58.8\n55.1\n50.6\n70.8\n89.1\n87.1\n91.9\n94.4\n96.0\n90.1\n64.3\n12.0\n72.2\n5.0\n41.0\n23.0\n39.1\n2\n74.1\n86.9\n67.5\n59.9\n55.8\n52.1\n70.9\n88.8\n87.8\n91.7\n95.3\n94.7\n89.7\n69.1\n12.3\n73.1\n6.6\n45.7\n27.2\n42.2\n4\n75.4\n87.1\n68.4\n60.2\n57.2\n52.1\n71.2\n89.0\n89.2\n91.8\n95.8\n95.3\n89.6\n67.3\n12.1\n73.4\n7.9\n49.2\n29.6\n44.3\n8\n76.3\n88.3\n69.4\n60.8\n60.5\n53.5\n71.8\n89.5\n89.9\n92.2\n96.5\n94.7\n89.8\n68.0\n12.5\n74.2\n8.0\n50.4\n31.0\n44.9\n16\n76.4\n88.6\n69.9\n61.6\n59.5\n53.9\n72.6\n88.2\n89.2\n91.8\n96.7\n94.5\n90.1\n67.5\n12.8\n74.2\n8.6\n51.8\n31.4\n45.6\n32\n76.7\n88.8\n71.1\n61.7\n57.0\n51.7\n72.0\n89.7\n88.9\n92.3\n96.6\n94.4\n89.6\n67.7\n12.9\n74.1\n9.0\n51.9\n32.1\n46.1\n64\n76.6\n88.2\n69.6\n61.7\n58.5\n53.0\n72.0\n89.6\n89.8\n91.9\n96.6\n95.0\n90.1\n68.1\n12.9\n74.2\n9.4\n52.5\n32.4\n46.0\nStable Diffusion, CFG scale=2.0, CLIP Templates(80)\n0.125\n60.4\n70.4\n48.5\n45.1\n24.9\n32.6\n63.1\n73.0\n67.3\n86.4\n83.1\n95.2\n86.5\n50.9\n9.8\n59.8\n2.4\n18.5\n7.6\n19.6\n0.25\n64.0\n74.5\n53.7\n49.4\n32.2\n37.8\n65.1\n80.5\n75.3\n88.0\n86.7\n95.4\n87.2\n56.4\n10.1\n63.8\n2.7\n29.0\n14.8\n27.4\n0.5\n67.6\n79.6\n58.7\n54.0\n42.7\n45.2\n68.1\n86.8\n84.6\n90.3\n91.0\n95.1\n88.9\n60.5\n10.1\n68.2\n3.1\n40.3\n24.4\n33.5\n1\n71.7\n85.1\n65.5\n58.6\n53.6\n50.1\n70.6\n88.0\n88.8\n92.6\n94.7\n96.0\n89.3\n65.4\n11.3\n72.1\n5.1\n52.9\n33.5\n40.4\n2\n75.0\n87.8\n69.3\n61.8\n61.0\n55.1\n72.6\n90.3\n90.9\n93.7\n96.5\n95.4\n90.3\n68.5\n12.0\n74.7\n7.5\n61.9\n39.7\n45.1\n4\n76.3\n89.7\n71.9\n62.9\n63.3\n55.3\n74.3\n91.8\n92.3\n94.3\n96.6\n94.9\n91.1\n68.9\n12.8\n75.8\n9.5\n66.4\n42.6\n47.4\n8\n76.9\n90.1\n71.6\n63.0\n61.7\n56.2\n73.9\n90.7\n91.0\n93.5\n96.7\n95.1\n89.6\n67.6\n12.4\n75.3\n10.5\n67.5\n43.8\n48.7\n16\n77.2\n89.7\n72.1\n63.0\n63.5\n56.0\n73.8\n90.9\n91.3\n92.0\n97.0\n94.5\n89.7\n67.6\n12.4\n75.4\n10.8\n68.8\n44.3\n49.3\n32\n77.5\n90.2\n71.9\n62.6\n62.5\n56.3\n73.5\n90.8\n91.5\n93.2\n96.6\n94.9\n89.9\n67.8\n12.3\n75.4\n11.4\n69.0\n44.5\n49.2\n64\n77.5\n90.5\n73.1\n62.8\n61.6\n55.7\n73.5\n90.9\n91.8\n93.0\n97.2\n94.7\n90.1\n67.3\n12.6\n75.5\n11.5\n69.3\n44.7\n49.6\nStable Diffusion, CFG scale=2.0, IN Caption\n0.125\n59.5\n71.3\n49.2\n45.7\n22.6\n31.6\n59.5\n73.5\n64.5\n84.3\n82.6\n95.0\n86.1\n48.3\n9.8\n58.9\n2.3\n13.9\n5.0\n22.5\n0.25\n63.1\n75.7\n54.6\n50.3\n26.7\n36.6\n64.6\n81.0\n73.0\n86.9\n88.0\n95.1\n86.7\n55.4\n10.3\n63.2\n3.0\n19.1\n8.1\n29.5\n0.5\n67.6\n79.8\n58.5\n54.1\n38.3\n41.7\n66.6\n86.6\n80.4\n90.0\n91.2\n95.8\n88.4\n60.7\n10.9\n67.4\n4.4\n25.7\n12.8\n36.7\n1\n71.9\n84.3\n64.2\n58.8\n48.3\n49.1\n69.8\n89.2\n86.4\n91.1\n94.8\n95.5\n89.4\n60.7\n11.9\n71.0\n7.2\n35.7\n19.5\n45.3\n2\n76.0\n88.0\n68.9\n62.4\n57.5\n51.4\n72.2\n90.5\n88.6\n93.0\n96.6\n95.5\n90.4\n64.5\n13.0\n73.9\n10.1\n43.8\n25.9\n50.7\n4\n77.2\n88.8\n69.2\n62.5\n56.2\n54.0\n72.0\n90.7\n89.4\n92.8\n96.8\n95.2\n90.0\n64.5\n13.0\n74.2\n12.5\n48.4\n28.6\n52.7\n8\n78.1\n88.9\n70.4\n63.6\n56.3\n53.7\n71.3\n90.6\n90.5\n93.1\n97.2\n94.7\n90.4\n66.0\n13.4\n74.6\n14.0\n50.5\n30.9\n54.2\n16\n78.3\n89.5\n71.4\n63.8\n56.4\n52.2\n72.9\n90.8\n89.8\n92.3\n97.3\n95.0\n89.5\n66.0\n13.4\n74.6\n15.7\n51.4\n30.7\n54.8\n32\n78.7\n89.9\n71.8\n64.1\n58.8\n53.6\n72.0\n91.2\n90.4\n91.8\n97.3\n95.1\n89.8\n66.6\n13.9\n75.0\n15.9\n52.4\n31.5\n54.8\n64\n78.4\n89.8\n70.9\n64.1\n58.9\n53.4\n73.0\n91.4\n90.7\n92.3\n97.3\n94.5\n89.7\n67.4\n13.7\n75.0\n16.0\n53.2\n32.2\n55.0\n20\nTable A8. Detailed scaling behavior on 15 different downstream classification datasets and ImageNet-A, ImageNet-R, ImageNet-Sketch\nand ImageNet-V2 validation set for supervised classifiers trained with synthetic images from various configurations using Imagen and\nMuse. Dataset scale is in million.\nScale\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nDS Average\nImageNet-A\nImageNet-R\nImageNet-Sketch\nImageNet-V2\nImagen, CFG scale=2.0, IN Caption\n0.125\n58.5\n71.9\n50.7\n45.7\n22.6\n31.5\n59.8\n78.8\n65.7\n84.4\n83.9\n94.9\n84.7\n52.6\n11.0\n59.8\n2.7\n15.6\n4.9\n29.1\n0.25\n61.8\n78.3\n56.8\n50.1\n28.7\n36.3\n63.9\n84.3\n74.6\n86.4\n89.8\n94.6\n85.3\n54.6\n11.1\n63.8\n3.2\n22.2\n9.4\n35.6\n0.5\n66.6\n81.9\n61.1\n54.8\n38.4\n43.4\n66.1\n88.1\n80.1\n89.5\n93.4\n95.4\n86.1\n58.2\n11.3\n67.6\n4.2\n29.0\n13.7\n42.4\n1\n71.3\n86.3\n66.8\n59.6\n49.5\n50.3\n69.6\n90.9\n85.9\n90.0\n96.0\n95.4\n88.0\n61.3\n12.6\n71.6\n7.9\n39.4\n19.6\n51.1\n2\n73.8\n90.0\n70.9\n60.2\n50.2\n52.1\n68.0\n90.9\n87.2\n89.7\n97.2\n95.1\n86.3\n62.0\n12.9\n72.4\n11.9\n45.0\n23.2\n55.0\n4\n75.1\n90.2\n71.8\n61.5\n49.5\n53.7\n69.1\n91.2\n87.8\n89.6\n97.4\n94.7\n87.1\n62.8\n13.0\n73.0\n14.2\n49.7\n27.1\n57.6\n8\n75.0\n91.2\n72.8\n62.0\n52.4\n51.7\n67.5\n91.7\n88.0\n88.3\n98.1\n95.0\n86.4\n62.1\n13.4\n73.0\n16.4\n51.0\n27.3\n58.7\nMuse, CFG scale=2.0, IN Caption\n0.125\n58.3\n76.6\n54.9\n45.3\n20.5\n29.6\n62.0\n73.2\n66.6\n84.8\n87.5\n95.2\n83.3\n52.2\n10.7\n60.0\n2.7\n16.8\n6.8\n23.4\n0.25\n63.3\n84.1\n63.5\n50.2\n27.0\n35.9\n65.3\n80.3\n76.5\n87.5\n92.5\n95.5\n85.3\n59.4\n11.2\n65.2\n3.6\n25.4\n13.2\n30.5\n0.5\n67.0\n87.0\n68.1\n54.0\n37.3\n42.4\n67.7\n85.1\n82.2\n89.1\n94.5\n95.5\n86.4\n64.8\n11.4\n68.8\n4.6\n32.7\n19.8\n36.4\n1\n71.2\n91.3\n72.8\n58.9\n48.1\n47.6\n70.4\n87.2\n87.0\n92.1\n96.8\n95.4\n87.2\n66.9\n11.8\n72.3\n7.7\n43.3\n27.9\n44.1\n2\n73.9\n92.0\n74.3\n60.3\n49.7\n49.3\n70.6\n89.4\n87.6\n91.0\n97.1\n94.6\n87.3\n67.9\n12.7\n73.2\n12.2\n48.8\n32.7\n48.4\n4\n74.9\n92.9\n74.4\n60.7\n51.4\n51.0\n71.1\n89.6\n88.4\n91.5\n97.7\n94.7\n86.9\n68.3\n12.6\n73.7\n14.4\n52.0\n34.8\n50.3\n8\n75.3\n91.7\n73.3\n61.1\n51.4\n51.9\n70.5\n89.6\n87.7\n91.6\n98.1\n93.5\n86.9\n66.1\n12.8\n73.4\n15.7\n53.2\n35.6\n51.0\ngory of interest in the generated images.\nG. More per-class analysis\nG.1. Recognizablity Distribution\nTo delve deeper into how recognizability and diversity are\ndistributed across the 1000 ImageNet classes and their influ-\nence on scaling ability (k), we categorized all classes into\n10 different groups based on their scaling ability, ranging\nfrom lowest to highest. For each group, we calculated the\naverage recognizability and diversity of the classes within\nit. The result of this analysis is illustrated in a detailed bar\nplot in Figure A5.\nThe analysis reveals the following trend: as the scaling\nability of a group increases, the average diversity initially\nrises and then begins to decrease. This trend suggests that at\nthe initial stages, enhanced diversity contributes to the gen-\neration of more varied synthetic images, helping the super-\nvised classifier to learn more robust features during training.\nHowever, beyond a certain point, further increases in diver-\nsity can be harmful, potentially compromising the accuracy\nof the generated images or leading to the omission of key\nobjects or the generation of wrong concepts.\nIn contrast, the average recognizability consistently in-\ncreases as scaling ability increases, indicating a stronger\ncorrelation between the scaling ability and recognizability\nfor each class. This consistent improvement shows the sig-\nnificance of recognizability as a more relevant metric for\nclass-based analysis.\nScaling Ability (k)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nRecognizability and Diversity\nRecognizability\nDiversity\nFigure A5. Per class analysis on the changes in recognizability\nand diversity as the scaling ability (k) increases. Here we divide\nthe 1000 ImageNet classes into 10 groups based on their scaling\nability ranging from the lowest to the highest.\nG.2. More Results on \u2018Scaling\u2019 Classes\nIn Figure A7, we provide a detailed comparison of the scal-\ning behavior for models trained on either real or synthetic\nimages from Stable Diffusion, specifically focusing on the\n\u2018scaling\u2019 classes as described in Section 5.2. Additionally,\nFigure A8 presents visualizations of synthetic images gen-\nerated for these classes, using the same setup as described\n21\n(a) Classnames\ngoldfish\nGolden Retriever\nshopping cart\n(b) Classname + Description\ngoldfish, Carassius auratus, small golden or\norange-red freshwater fishes of Eurasia used\nas pond or aquarium fishes\nGolden Retriever, an English breed having a\nlong silky golden coat\nshopping cart, a handcart that holds gro-\nceries or other goods while shopping\n(c) Classname + Description + Places\ngoldfish, Carassius auratus, small golden or\norange-red freshwater fishes of Eurasia used\nas pond or aquarium fishes inside ice skating\nrink\nGolden Retriever, an English breed having a\nlong silky golden coat inside gas station\nshopping cart, a handcart that holds gro-\nceries or other goods while shopping inside\nbasketball court\n(d) Classname + Hypernym\ngoldfish, Carassius auratus, cyprinid\nGolden Retriever, retriever\nshopping cart, handcart, pushcart, cart, go-\ncart\n(e) Classname + Hypernym + Places\ngoldfish, Carassius auratus, cyprinid inside\nice skating rink\nGolden Retriever, retriever inside amuse-\nment park\nshopping cart, handcart, pushcart, cart, go-\ncart inside airfield\n(f) Word2Sen\ngoldenfish on a shelf\nThe golden retriever on a run\nthe cart is a perfect place for shopping\n(g) CLIP templates\nA good photo of the golden fish\nA close-up photo of a Golden Retriever\nA photo of the cool shopping cart\n(h) IN-Caption\ngoldfish, fish is swimming in the water\nGolden Retriever, a dog is running on the\ngrass\nshopping cart, a street with a garbage can\nand a cart\nFigure A6. Synthetic images generated by Stable Diffusion with different text prompt configurations on three ImageNet categories:\ngoldfish, Golden Retriever, and shopping cart. All of these visualizations use a guidance scale of 2.0.\n22\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n3.0\n4.0\n5.0\n6.0\nLoss (log scale)\nvelvet fabric\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n2.0\n3.0\n4.0\n5.0\nsweatshirt\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n1.5\n2.0\n3.0\n4.0\ndigital watch\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n2.0\n3.0\n4.0\n5.0\n6.0\nspotlight\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n2.0\n3.0\n4.0\n5.0\nsunscreen\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAccuracy\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n0.1\n0.2\n0.3\n0.4\n0.5\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n1.5\n2.0\n3.0\n4.0\nLoss (log scale)\nhusky\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n3.0\n4.0\n5.0\nnotebook computer\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n3.0\n4.0\n5.0\nbackpack\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n1.0\n1.5\n2.0\n3.0\n4.0\nplectrum\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\n1.0\n1.5\n2.0\nbighorn sheep\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nAccuracy\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale\n0.1\n0.2\n0.3\n0.4\n0.5\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.1250.25 0.5\n1\n2\n4\n8\n16\n32\n64\nDataset Scale\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSynthetic\nReal\nFigure A7. More comparison on supervised models trained on real and synthetic images (from Stable Diffusion), for the \u2018Scaling\u2019 classes.\nVelvet fabric\nSweatshirt\nDigital watch\nSpotlight\nSunscreen\nHusky\nNotebook computer \nBackpack\nPlectrum\nBighorn sheep\nFigure A8. Visualizations of the synthetic images generated for \u2018Scaling\u2019 classes, using Stable Diffusion with a guidance scale of 2.0.\n23\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.0\n1.5\n2.0\n3.0\nLoss (log scale)\nAfrican elephant\n0.125\n0.25\n0.5\n1\n2\n4\n8\n2.0\n3.0\n4.0\n5.0\nEnglish foxhound\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.6\n1.0\n1.5\n2.0\n3.0\n4.0\ntorch\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.5\n2.0\n3.0\n4.0\n5.0\n6.0\nbinoculars\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.4\n0.6\n1.0\n1.5\n2.0\nhen\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.1\n0.2\n0.3\n0.4\n0.5\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.5\n0.6\n0.7\n0.8\n0.9\n0.125\n0.25\n0.5\n1\n2\n4\n8\n3.0\n4.0\n5.0\n6.0\nLoss (log scale)\nspotlight\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.4\n0.6\n1.0\n1.5\n2.0\nbighorn sheep\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.0\n1.5\n2.0\ncottontail rabbit\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.0\n1.5\n2.0\n3.0\n4.0\ngraduation cap\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.0\n1.5\n2.0\n3.0\n4.0\n5.0\npadlock\n0.125\n0.25\n0.5\n1\n2\n4\n8\nDataset Scale\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nAccuracy\n0.125\n0.25\n0.5\n1\n2\n4\n8\nDataset Scale\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.125\n0.25\n0.5\n1\n2\n4\n8\nDataset Scale\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.125\n0.25\n0.5\n1\n2\n4\n8\nDataset Scale\n0.3\n0.4\n0.5\n0.6\n0.7\n0.125\n0.25\n0.5\n1\n2\n4\n8\nDataset Scale\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSynthetic\nReal\nFigure A9. More comparison on supervised models trained on real and synthetic images (from Imagen), for the \u2018Scaling\u2019 classes.\nAfrican elephant\nEnglish foxhound\nTorch\nBinoculars\nHen\nSpotlight\nBighorn sheep\nCottontail rabbit\nGraduation cap\nPadlock\nFigure A10. Visualizations of the synthetic images generated for \u2018Scaling\u2019 classes, using Imagen [69] with a guidance scale of 1.5.\n24\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.5\n2.0\n2.5\n3.0\nLoss (log scale)\ncorn\n0.125\n0.25\n0.5\n1\n2\n4\n8\n2.0\n2.5\n3.0\n4.0\nsweatshirt\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.5\n2.0\n2.5\n3.0\nbookstore\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.0\n1.5\n2.0\n6 \u00d7 10\n1\ntypewriter keyboard\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.5\n2.0\n2.5\n3.0\n4.0\ncrate\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nAccuracy\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.125\n0.25\n0.5\n1\n2\n4\n8\n0.2\n0.3\n0.4\n0.5\n0.6\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.5\n2.0\n2.5\n3.0\n4.0\nLoss (log scale)\nbarrel\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.5\n2.0\n2.5\n3.0\n4.0\n6.0\nbinoculars\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.5\n2.0\n2.5\n3.0\n4.0\nbucket\n0.125\n0.25\n0.5\n1\n2\n4\n8\n1.5\n2.0\n2.5\n3.0\nacademic gown\n0.125\n0.25\n0.5\n1\n2\n4\n8\n2.0\n2.5\n3.0\n4.0\npole\n0.125\n0.25\n0.5\n1\n2\n4\n8\nDataset Scale\n0.3\n0.4\n0.5\n0.6\n0.7\nAccuracy\n0.125\n0.25\n0.5\n1\n2\n4\n8\nDataset Scale\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.125\n0.25\n0.5\n1\n2\n4\n8\nDataset Scale\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.125\n0.25\n0.5\n1\n2\n4\n8\nDataset Scale\n0.3\n0.4\n0.5\n0.6\n0.7\n0.125\n0.25\n0.5\n1\n2\n4\n8\nDataset Scale\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nSynthetic\nReal\nFigure A11. More comparison on supervised models trained on real and synthetic images (from Muse), for the \u2018Scaling\u2019 classes.\nCorn\nSweatshirt\nBookstore\nTypewriter keyboard\nCrate\nBarrel\nBinoculars\nBucket\nAcademic gown\nPole\nFigure A12. Visualizations of the synthetic images generated for \u2018Scaling\u2019 classes, using Muse [9] with a guidance scale of 0.3.\n25\nin Section 5.\nWe further explore \u2019Scaling\u2019 classes for supervised clas-\nsifiers trained on images generated by the Imagen and Muse\nmodels. The scaling behaviors of these classes, in compar-\nison to models trained with real images, along with their\nvisualizations, are presented in Figure A9, Figure A10, Fig-\nure A11 and Figure A12. Our analysis reveals that cer-\ntain classes, such as sweatshirts, exhibit consistently good\nscaling across different text-to-image models. Meanwhile,\nthere are classes that show particularly strong scaling per-\nformance with specific text-to-image models.\nFor the ten \u2018scaling\u2019 classes selected in Stable Diffusion,\nwe observed that models trained on synthetic images exhibit\nscaling abilities that are comparable to, and in some cases\neven superior to, those trained on real images. A notable ex-\nample can be seen in the \u2018bighorn sheep\u2019 and \u2018spotlight\u2019 cat-\negories, where models trained on synthetic images already\noutperform those trained on real images at dataset scales be-\nlow 1 million, and this advantage continues to grow as the\nscale increases, since there are only 1.3M real images.\nThis finding suggests that for certain concepts, text-to-\nimage models are indeed capable of generating images that\nare more conducive to train supervised classifiers effec-\ntively. As text-to-image models continue to improve, we\nanticipate that such instances will become more frequent.\nEventually, it\u2019s plausible that models trained on synthetic\nimages could surpass the performance of those trained on\nreal images across the entire ImageNet validation set.\nG.3. More \u2018Poor\u2019 Classes\nIn Table A9, we identify and list \u2018poor\u2019 classes where super-\nvised models, trained on synthetic images, face challenges\nin accurate classification.\nFor each of the three text-to-\nimage models \u2014 Stable Diffusion, Imagen, and Muse \u2014 we\nhighlight 40 distinct categories that pose difficulties. No-\ntably, certain categories, such as tiger cats and vine snakes,\nare common challenges across different text-to-image mod-\nels. Future research in the development of text-to-image\nmodels could benefit from focusing on these categories. Im-\nproving the accuracy in generating images of these \u2018poor\u2019\nclasses is crucial, as their current limitations are a key fac-\ntor hindering the ability of synthetic images to have better\nscaling ability and performance than real images, in the su-\npervised learning contexts.\nG.4. Per-class FID and LPIPS\nFollowing the same setup outlined in Section 5.1 of the\nmain paper, we also computed the correlations between\nscaling ability (k in Equation 2 of the main paper) and both\nFID and LPIPS scores. Unlike the previous analysis focus-\ning on recognizability and diversity, this evaluation specif-\nically studies the relationship of scaling ability with these\ntwo metrics.\nTo calculate the per-class LPIPS scores, we used the\nsame method as detailed previously. However, for per-class\nFID computation, the existing synthetic test sets, containing\nonly 50 images per class, were deemed insufficient, since\nFID score is sensitive to the number of images. Therefore\nwe sample 1300 images from the synthetic training images\nand compute the FID with images from real ImageNet train-\ning set for each class. Similar to our previous approach,\nwe took the negative of the per-class FID and LPIPS scores\nfor consistency, as lower scores indicate better performance.\nThe correlations obtained are depicted in Figure A13.\nThe results from this figure indicate a lack of strong cor-\nrelation between scaling ability and either FID or LPIPS\nscores. This finding highlights the necessity for a more tai-\nlored metric that is specifically designed to assess the scal-\ning ability of supervised classifiers trained on synthetic im-\nages.\n47.5\n45.0\n42.5\n40.0\n37.5\n35.0\n32.5\n30.0\n27.5\n-FID (negative)\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nScaling Ability\n= 0.10\n0.80\n0.75\n0.70\n0.65\n0.60\n-lpips (negative)\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n=\n0.02\nFigure A13. Per class analysis on the relationship between scaling\nability (defined as k in Equation 2 in the main paper) and both\nFID and LPIPS. Within each specific class, the plots indicate the\ncorrelation between the scaling ability and both metrics appears to\nbe negligible.\nH. More Results on CLIP Scaling\nH.1. Comparison on Hyper-parameters\nIn Table A3, we detail the use of two distinct sets of hyper-\nparameters for CLIP training, tailored to different dataset\nscales. Config (a) in the table, labeled as \u2018S\u2019 here, is de-\nsigned for smaller dataset scales with fewer than 100 mil-\nlion images. Conversely, config (b), represented as \u2018L\u2019 here,\nis intended for larger dataset scales equal to or exceeding\n100 million images. To validate the necessity of these con-\nfigurations, we present an empirical study in Table A10.\nHere we train CLIP models on subsets of the LAION-\n400M dataset with 10M, 50M, and 100M samples, exclu-\nsively utilizing real images and applying the two different\nhyper-parameter sets. Our findings indicate that for scales\nof 10M and 50M, the \u2018S\u2019 hyper-parameter configuration\nyields superior results, with the performance difference be-\ning reduced at the 50M scale. In contrast, at the 100M scale,\nthe \u2018L\u2019 configuration demonstrates enhanced performance.\n26\nTable A9. Lists of \u2018poor\u2019 classes that has poor scaling ability and performance. Supervised models trained with synthetic images struggles\nin classifying them correctly. We list 40 categories for Stable Diffusion, Imagen and Muse, respectively.\n(a) Stable Diffusion\nfire salamander\nAppenzeller Sennen-\nhund\ntiger cat\ncollie\nAustralian Terrier\nAfrican\nbush\nele-\nphant\ncassette player\ncanoe\nEuropean\ngreen\nlizard\nnight snake\nmushroom\neastern\nhog-nosed\nsnake\nhot tub\nwall clock\ncrayfish\nespresso machine\nwater jug\ntoy terrier\nBrittany dog\nkeyboard space bar\nshower curtain\ngymnastic horizontal\nbar\nAfrican rock python\nletter opener\nladle\ntape player\ntea cup\npaper towel\nwok\nflute\nvine snake\nblack-footed ferret\ncricket insect\nEuropean polecat\ncradle\nLakeland Terrier\ngreen mamba\ncleaver\nbreastplate\nmonitor\n(b) Imagen\nkit fox\nshower curtain\nnight snake\nhot tub\nminivan\ndesktop computer\nkeyboard space bar\nEuropean\ngreen\nlizard\nespresso machine\nblack-footed ferret\nwater jug\nflute\nvelvet fabric\nmobile phone\ndigital clock\nproduct\npacket\n/\npackaging\nCRT monitor\neastern\nhog-nosed\nsnake\ntape player\nbolete\ntobacco shop\nmonastery\npurse\nmushroom\nprinter\nletter opener\nwall clock\ntoilet paper\nmonitor\nsunglasses\noverskirt\nhard disk drive\nladle\ncan opener\ntiger cat\ncombination lock\npaper towel\nplunger\ntights\nvine snake\n(c) Muse\ntiti monkey\nalligator lizard\nEuropean\ngreen\nlizard\ncottontail rabbit\nAfrican rock python\nstopwatch\ngar fish\nIrish Water Spaniel\nEuropean polecat\nCRT monitor\ntoy terrier\nkeyboard space bar\nnight snake\nNorfolk Terrier\nIbizan Hound\nmobile phone\nground beetle\nTibetan Terrier\nNorwich Terrier\npurse\nTreeing\nWalker\nCoonhound\nSiberian Husky\neastern\nhog-nosed\nsnake\nBouvier des Flandres\ndog\npatas monkey\nAustralian Terrier\nCD player\nBriard\nAffenpinscher\nEnglish Setter\ncradle\nred wolf or maned\nwolf\nGeoffroy\u2019s\nspider\nmonkey\nBorder Terrier\nLakeland Terrier\ntape player\nCairn Terrier\nBluetick Coonhound\nEntlebucher Sennen-\nhund\nRedbone Coonhound\nTherefore, based on these empirical results, we opted to uti-\nlize the \u2018S\u2019 hyper-parameter set for smaller data scales and\nthe \u2018L\u2019 set for larger scales.\nH.2. Comparison on different CFGs\nTo identify the most effective CFG scale for generating syn-\nthetic images to train CLIP models, we utilized the Sta-\nble Diffusion to create synthetic images for the CC12M\ndataset [10] at four different CFG scales: 1.25, 1.5, 1.75,\nand 2.5. Following the generation of these images, CLIP\nmodels were trained using the synthetic images and their\ncorresponding texts. The efficacy of these trained models\nwas then evaluated through zero-shot classification on Ima-\ngeNet and various downstream classification datasets.\nThe detailed comparison of these different CFG scales\nare presented in Table A11. Based on these results, we de-\ntermined that a CFG scale of 1.5 delivers the best zero-shot\nclassification performance on ImageNet. Consequently, we\nchose CFG= 1.5 for the majority of our CLIP experiments.\n27\nTable A10. Comparison of CLIP models trained on LAION-400M subsets of different data scales, using different hyper-parameter con-\nfigurations. Hyper-parameter configuration \u2018S\u2019 and \u2018L\u2019 corresponds to (a) and (b) in Table A3, respectively. All models are trained on real\nimages and text only, using ViT-B/32 as backbone architecture.\nScale\nHyper-param\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\n10M\nS\n42.0\n78.3\n49.0\n36.1\n40.3\n3.6\n22.2\n47.4\n75.0\n29.9\n85.0\n27.5\n30.4\n19.6\n4.4\n39.4\n30.6\nL\n15.8\n51.0\n22.7\n16.0\n9.9\n0.8\n10.2\n19.7\n50.2\n14.9\n63.2\n12.5\n16.5\n6.0\n1.9\n20.8\n14.8\n50M\nS\n63.9\n87.7\n65.4\n54.5\n61.6\n5.9\n34.4\n71.2\n84.0\n45.3\n93.4\n46.4\n45.3\n28.1\n8.4\n53.0\n47.9\nL\n62.3\n84.2\n59.9\n48.9\n62.1\n4.9\n31.5\n71.3\n83.2\n47.1\n90.6\n30.9\n39.5\n30.0\n7.7\n50.3\n44.2\n100M\nS\n69.5\n88.9\n68.6\n58.4\n67.3\n6.9\n43.4\n75.4\n85.7\n49.9\n93.3\n46.9\n54.1\n41.1\n9.6\n57.3\n51.8\nL\n72.6\n89.4\n68.0\n57.6\n72.6\n7.1\n41.0\n80.9\n87.3\n55.9\n93.8\n36.4\n52.3\n41.5\n10.4\n57.8\n54.2\nTable A11. Comparison of CLIP models trained on synthetic CC12M generated by Stable Diffusion with different CFG scales. Models\nare trained using ViT-B/16 as backbone architecture.\nCFG\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\n1.25\n41.9\n37.8\n16.3\n39.5\n22.8\n3.2\n20.9\n56.3\n70.9\n22.4\n83.8\n11.7\n31.5\n7.2\n4.7\n31.4\n34.9\n1.5\n43.1\n32.8\n16.6\n42.4\n26.6\n3.7\n23.4\n58.8\n68.8\n24.3\n86.4\n10.8\n33.3\n7.4\n5.6\n32.3\n35.7\n1.75\n43.5\n29.5\n13.3\n42.7\n28.0\n3.9\n22.2\n58.1\n70.6\n21.8\n84.2\n17.7\n32.0\n8.3\n5.5\n32.1\n35.6\n2.5\n41.3\n47.0\n15.1\n41.9\n28.8\n4.7\n23.9\n57.8\n71.2\n24.4\n87.4\n16.8\n31.0\n6.1\n5.7\n33.5\n34.3\n1\n2\n4\n8\n16\n32\n64\n128\n256\nDataset Scale (M)\n2.0\n3.0\n4.0\n6.0\n10.0\nZero-shot Loss (log Scale)\n1\n2\n4\n8\n16\n32\n64\n128\n256\nDataset Scale (M)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nZero-shot Accuracy\nCLIP Scaling on Downstream Average\nSynthetic\nReal\nSynthetic+Real\nFigure A14. The average scaling behavior on zero-shot classifi-\ncation for CLIP models over all 15 downstream datasets. Models\nare trained on LAION-400M subsets with synthetic, real, or syn-\nthetic+real images.\nH.3. Detailed experiment results for all scales\nTable A12 provides detailed scaling behavior for CLIP\nmodels trained utilizing either synthetic, real, or a combina-\ntion of synthetic and real images. We also present the scal-\ning behavior comparison in detailed plots for each specific\ndownstream dataset in Figure A15. Figure A14 shows the\naverage scaling behavior over all 15 downstream datasets.\nThe models were trained on subsets of the LAION-400M\ndataset, beginning with 1 million samples and scaling up ex-\nponentially to the entire set of 371M. Our findings indicate\nsynthetic images does not scale as good as real onees, yet\nintegrating synthetic images with real images in the train-\ning of CLIP models can be advantageous, particularly in\nscenarios where the dataset size is relatively limited.\n28\nTable A12. Zero-shot transfer performance on 15 downstream datasets and ImageNet. Models are trained on LAION-400M subsets with\nimages from synthetic, real or synthetic+real. Dataset scale starts from 1M and increase exponentially. Combining synthetic images and\nreal images can improve zero-shot classification performance under various cases, especially when data amount is limited.\nScale\nData\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\n1M\nSyn\n5.2\n12.8\n3.3\n5.9\n1.7\n0.9\n5.5\n6.7\n17.8\n3.5\n29.4\n9.0\n9.7\n5.4\n1.2\n7.9\n4.1\nReal\n5.2\n25.4\n7.6\n5.0\n2.1\n1.0\n5.4\n5.4\n18.0\n5.0\n36.4\n14.7\n9.3\n6.6\n1.0\n9.9\n3.8\nSyn+Real\n10.9\n32.2\n13.0\n13.1\n4.6\n1.4\n9.4\n12.0\n36.0\n8.9\n62.5\n19.8\n14.7\n7.5\n1.9\n16.5\n9.4\n2M\nSyn\n11.0\n15.3\n3.8\n14.5\n6.2\n1.7\n10.3\n15.6\n36.2\n7.2\n36.3\n15.6\n14.5\n3.4\n1.7\n12.9\n10.7\nReal\n13.4\n39.0\n16.8\n13\n6.6\n1.3\n10.5\n13.0\n40.1\n12.4\n57.1\n17.0\n14.9\n6.5\n1.7\n17.6\n10.6\nSyn+Real\n22.2\n59.7\n27.0\n23.4\n18.2\n2.1\n15.4\n24.8\n55.7\n13.1\n73.9\n22.4\n20.6\n4.2\n3.0\n25.7\n19.8\n4M\nSyn\n19.6\n19.7\n7.1\n23.0\n22.4\n2.1\n17.0\n30.1\n53.4\n13.9\n64.4\n12.8\n21.1\n5.3\n3.1\n21.0\n19.8\nReal\n30.8\n63.8\n33.4\n26.2\n27.7\n1.8\n18.8\n33.9\n61.7\n18.9\n79.2\n40.2\n21.5\n10.0\n3.4\n31.4\n21.7\nSyn+Real\n40.3\n67.1\n39.3\n35.8\n40.9\n2.3\n22.9\n45.4\n70.1\n23.1\n88.2\n33.5\n27.7\n12.3\n4.5\n36.9\n30.6\n8M\nSyn\n34.2\n23.8\n9.5\n32.6\n39.9\n3.5\n20.3\n46.3\n63.0\n20.7\n78.7\n9.8\n19.1\n4.9\n4.3\n27.4\n29.0\nReal\n48.7\n79.6\n47.9\n38.5\n48.9\n3.9\n25.5\n52.8\n74.9\n31.2\n88.0\n27.4\n32.8\n16.7\n5.2\n41.5\n34.2\nSyn+Real\n54.5\n82.9\n53.1\n46.3\n57.3\n5.1\n29.6\n61.4\n78.2\n31.1\n92.5\n29.6\n41.1\n14.5\n6.5\n45.6\n40.7\n16M\nSyn\n44.2\n32.4\n11.5\n41.6\n51.3\n4.9\n27.4\n58.3\n72.1\n24.8\n83.6\n16.7\n29.5\n4.6\n5.9\n33.9\n37.7\nReal\n62.9\n85.2\n58.1\n49.0\n60.6\n5.0\n30.4\n61.9\n81.5\n40.9\n93.1\n43.2\n39.4\n28.0\n7.4\n49.8\n43.8\nSyn+Real\n64.8\n87.5\n61.0\n53.7\n63.3\n4.9\n36.5\n67.7\n82.8\n38.6\n94.5\n37.6\n48.2\n28.6\n8.2\n51.9\n48.2\n32M\nSyn\n54.2\n33.3\n18.3\n47.3\n58.9\n4.4\n30.3\n65.3\n75.3\n29.4\n88.5\n15.5\n35.5\n8.5\n7.3\n38.1\n43.8\nReal\n70.4\n86.3\n64.7\n55.7\n67.2\n4.9\n35.7\n70.1\n82.8\n46.0\n94.9\n43.4\n48.6\n36.6\n9.1\n54.4\n50.5\nSyn+Real\n71.4\n89.4\n65.3\n57.9\n69.7\n6.5\n41.8\n72.9\n83.2\n41.3\n95.2\n38.7\n55.1\n29.9\n10.2\n55.2\n52.9\n64M\nSyn\n59.7\n44.1\n20.9\n51.5\n62.7\n7.7\n37.9\n71.1\n79.2\n35.8\n92.1\n15.1\n39.0\n12.5\n8.6\n42.5\n48.0\nReal\n75.2\n90.9\n69.5\n59.4\n71.5\n7.3\n42.8\n75.0\n87.0\n50.6\n95.7\n46.8\n51.8\n39.8\n11.2\n58.3\n55.1\nSyn+Real\n74.6\n90.8\n67.8\n61.1\n73.3\n6.3\n50.2\n76.3\n87.8\n47.6\n95.8\n45.5\n58.3\n37.5\n11.6\n59.0\n56.4\n128M\nSyn\n63.7\n45.1\n15.9\n52.3\n67.1\n9.3\n37.8\n75.7\n80.5\n39.1\n93.2\n8.0\n35.7\n10.1\n9.5\n42.9\n51.2\nReal\n81.9\n90.5\n70.9\n62.5\n78.7\n10.7\n46.0\n85.9\n88.7\n60.4\n96.0\n48.3\n57.8\n42.7\n14.2\n62.3\n61.4\nSyn+Real\n81.6\n91.0\n70.4\n64.0\n79.4\n11.9\n52.5\n85.1\n90.2\n59.5\n97.0\n47.3\n61.1\n45.3\n14.1\n63.4\n62.9\n256M\nSyn\n68.6\n46.2\n21.8\n54.7\n70.4\n10.9\n42.9\n80.2\n81.5\n44.6\n95.2\n20.2\n39.1\n12.8\n10.5\n46.6\n54.4\nReal\n84.6\n92.8\n73.5\n66.5\n82.4\n12.3\n52.7\n89.9\n91.3\n65.7\n96.9\n39.2\n64.4\n47.3\n16.9\n65.1\n65.4\nSyn+Real\n83.8\n92.4\n73.3\n66.0\n82.3\n14.6\n55.0\n86.7\n91.4\n58.6\n97.8\n47.7\n65.2\n42.5\n15.3\n64.8\n65.4\n371M\nSyn\n70.1\n51.9\n26.2\n55.5\n70.8\n12.3\n41.5\n79.6\n83.6\n45.5\n95.7\n28.8\n39.3\n20.6\n10.9\n48.8\n55.7\nReal\n85.7\n93.9\n75.6\n67.5\n83.3\n14.2\n50.1\n88.8\n91.1\n67.0\n97.0\n43.9\n66.6\n42.8\n17.5\n65.7\n66.8\nSyn+Real\n84.6\n92.4\n73.2\n67.1\n82.0\n17.2\n56.8\n86.4\n91.7\n61.6\n97.3\n52.2\n65.9\n46.7\n16.0\n66.1\n66.6\n29\n1\n2\n4\n8\n16\n32\n64\n128 256\n1.0\n2.0\n3.0\n4.0\n6.0\n10.0\nZero-shot Loss (log Scale)\nFood101\n1\n2\n4\n8\n16\n32\n64\n128 256\n1.0\n2.0\n3.0\n4.0\n6.0\n10.0\nCifar10\n1\n2\n4\n8\n16\n32\n64\n128 256\n1.0\n2.0\n3.0\n4.0\n6.0\n10.0\nCifar100\n1\n2\n4\n8\n16\n32\n64\n128 256\n2.0\n3.0\n4.0\n6.0\n10.0\nSun397\n1\n2\n4\n8\n16\n32\n64\n128 256\n1.0\n2.0\n3.0\n4.0\n6.0\n10.0\nCars\n1\n2\n4\n8\n16\n32\n64\n128 256\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nZero-shot Accuracy\n1\n2\n4\n8\n16\n32\n64\n128 256\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n2\n4\n8\n16\n32\n64\n128 256\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n1\n2\n4\n8\n16\n32\n64\n128 256\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n1\n2\n4\n8\n16\n32\n64\n128 256\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n1\n2\n4\n8\n16\n32\n64\n128 256\n4.0\n6.0\n10.0\nZero-shot Loss (log Scale)\nAircraft\n1\n2\n4\n8\n16\n32\n64\n128 256\n2.0\n3.0\n4.0\n6.0\n10.0\nDtd\n1\n2\n4\n8\n16\n32\n64\n128 256\n1.0\n2.0\n3.0\n4.0\n6.0\n10.0\nPets\n1\n2\n4\n8\n16\n32\n64\n128 256\n1.0\n2.0\n3.0\n4.0\n6.0\nCaltech101\n1\n2\n4\n8\n16\n32\n64\n128 256\n2.0\n3.0\n4.0\n6.0\n10.0\nFlowers\n1\n2\n4\n8\n16\n32\n64\n128 256\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nZero-shot Accuracy\n1\n2\n4\n8\n16\n32\n64\n128 256\n0.1\n0.2\n0.3\n0.4\n0.5\n1\n2\n4\n8\n16\n32\n64\n128 256\n0.2\n0.4\n0.6\n0.8\n1\n2\n4\n8\n16\n32\n64\n128 256\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n2\n4\n8\n16\n32\n64\n128 256\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n1\n2\n4\n8\n16\n32\n64\n128 256\n1.0\n2.0\n3.0\n4.0\n6.0\nZero-shot Loss (log Scale)\nStl10\n1\n2\n4\n8\n16\n32\n64\n128 256\n2.0\n3.0\n4.0\n6.0\n10.0\nEurosat\n1\n2\n4\n8\n16\n32\n64\n128 256\n2.0\n3.0\n4.0\n6.0\nResisc45\n1\n2\n4\n8\n16\n32\n64\n128 256\n2.0\n3.0\n4.0\n6.0\nGtsrb\n1\n2\n4\n8\n16\n32\n64\n128 256\n6.0\n10.0\nCountry211\n1\n2\n4\n8\n16\n32\n64\n128 256\nDataset Scale (M)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nZero-shot Accuracy\n1\n2\n4\n8\n16\n32\n64\n128 256\nDataset Scale (M)\n0.1\n0.2\n0.3\n0.4\n0.5\n1\n2\n4\n8\n16\n32\n64\n128 256\nDataset Scale (M)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n1\n2\n4\n8\n16\n32\n64\n128 256\nDataset Scale (M)\n0.1\n0.2\n0.3\n0.4\n1\n2\n4\n8\n16\n32\n64\n128 256\nDataset Scale (M)\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nSynthetic\nReal\nSynthetic+Real\nFigure A15. Detailed scaling behavior comparison between CLIP models trained on synthetic, real, or the combination of synthetic and\nreal images, on 15 different downstream tasks. The models are evaluated under zero-shot classification.\n30\n"
  },
  {
    "title": "Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation",
    "link": "https://arxiv.org/pdf/2312.04483.pdf",
    "upvote": "6",
    "text": "Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation\nZhiwu Qing1\nShiwei Zhang2\u2217\nJiayu Wang2\nXiang Wang1\nYujie Wei3\nYingya Zhang2\nChangxin Gao1\u2217\nNong Sang1\n1Key Laboratory of Image Processing and Intelligent Control\nSchool of Artificial Intelligence and Automation, Huazhong University of Science and Technology\n2Alibaba Group\n3Fudan University\n{qzw, wxiang, cgao, nsang}@hust.edu.cn\n{zhangjin.zsw, wangjiayu.wjy, yingya.zyy}@alibaba-inc.com\nyjwei22@m.fudan.edu.cn\nAbstract\nDespite diffusion models having shown powerful abil-\nities to generate photorealistic images, generating videos\nthat are realistic and diverse still remains in its infancy.\nOne of the key reasons is that current methods intertwine\nspatial content and temporal dynamics together, leading to\na notably increased complexity of text-to-video generation\n(T2V). In this work, we propose HiGen, a diffusion model-\nbased method that improves performance by decoupling the\nspatial and temporal factors of videos from two perspec-\ntives, i.e., structure level and content level. At the structure\nlevel, we decompose the T2V task into two steps, including\nspatial reasoning and temporal reasoning, using a unified\ndenoiser. Specifically, we generate spatially coherent pri-\nors using text during spatial reasoning and then generate\ntemporally coherent motions from these priors during tem-\nporal reasoning. At the content level, we extract two subtle\ncues from the content of the input video that can express\nmotion and appearance changes, respectively. These two\ncues then guide the model\u2019s training for generating videos,\nenabling flexible content variations and enhancing tempo-\nral stability. Through the decoupled paradigm, HiGen can\neffectively reduce the complexity of this task and generate\nrealistic videos with semantics accuracy and motion stabil-\nity. Extensive experiments demonstrate the superior perfor-\nmance of HiGen over the state-of-the-art T2V methods.\n1. Introduction\nThe purpose of text-to-video generation (T2V) is to gener-\nate corresponding photorealistic videos based on input text\nprompts. These generated videos possess tremendous po-\ntential in revolutionizing video content creation, particu-\nlarly in movies, games, entertainment short videos, and be-\n\u2217Corresponding authors.\nProject page: https://higen-t2v.github.io/.\nPrompt: Batman turns his head from right to left.\nHiGen\nGen-2\nModelScope\nHiGen\nGen-2\nModelScope\nPrompt: Tiny plant sprout coming out of the ground.\nFigure 1. Visual comparison with ModelScopeT2V [55] and Gen-\n2 [12]. The videos generated by ModelScopeT2V exhibit notice-\nable motion but suffer from lower spatial quality. However, while\nGen-2 produces realistic frames, they are mostly static with min-\nimal motion. In contrast, the results of our HiGen demonstrate\nboth realistic frames and rich temporal variations.\nyond, where their application possibilities are vast. Existing\nmethods primarily tackle the T2V task by leveraging pow-\nerful diffusion models, leading to substantial advancements\nin this domain.\nTypically, mainstream approaches [7, 10, 19, 55, 57] at-\ntempt to generate videos by extending text-to-image (T2I)\nmodels by designing suitable 3D-UNet architectures. How-\n1\narXiv:2312.04483v1  [cs.CV]  7 Dec 2023\nPrompt: A video of a duckling wearing a medieval soldier helmet and riding a skateboard.\nPrompt: Astronaut riding a horse.\nTemporal Consistency=0.966\nTemporal Consistency=0.954\nTemporal Consistency=0.926\nTemporal Consistency=0.938\nTemporal Consistency=0.941\nTemporal Consistency=0.889\nAppearance Factor=0.00\nAppearance Factor=1.00\nMotion Factor=500\nMotion Factor=500\nAppearance Factor=0.00\nMotion Factor=300\nAppearance Factor=0.00\nAppearance Factor=1.00\nMotion Factor=500\nMotion Factor=500\nAppearance Factor=0.00\nMotion Factor=300\nFigure 2. The impact of motion factors and appearance factors. Larger motion factors introduce dynamic motions to the videos instead of\nstatic scenes, while larger appearance factors contribute to richer temporal semantic variations in the generated videos.\never, due to the complex distribution of high-dimensional\nvideo data, directly generating videos with both realistic\nspatial contents and diverse temporal dynamics jointly is\nin fact exceedingly challenging, which often leads to un-\nsatisfactory results produced by the model. For example, as\nshown in Fig. 1, videos generated by ModelScopeT2V [55]\nexhibit dynamics but suffer from lower spatial quality. Con-\nversely, videos from Gen-2 [12] showcase superior spa-\ntial quality but with minimal motions. On the other hand,\nVideoFusion [34] considers spatial redundancy and tempo-\nral correlation from the noise perspective by decomposing\ninput noise into base noise and residual noise. However, it\nremains challenging to directly denoise videos with spatio-\ntemporal fidelity from the noise space.\nBased on the above observations, we propose a new dif-\nfusion model-based HiGen approach that decouples videos\ninto spatial and temporal factors from two perspectives,\nnamely structure level and content level. For the structure\nlevel, in light of the separability of space and time [13, 56]\nin video data, we decompose the T2V task into distinct spa-\ntial reasoning and temporal reasoning processes, all pred-\nicated on a unified model. During spatial reasoning, we\nutilize text prompts to generate spatial priors that are se-\nmantically coherent. These priors are then used in tempo-\nral reasoning to generate temporally coherent motions. For\nthe content level, we extract two cues that respectively rep-\nresent the motion and appearance variations in videos and\nutilize them as conditions for training the model. By this\nmeans, we can enhance the stability and diversity of gen-\nerated videos by flexibly controlling the spatial and tempo-\nral variations through manipulating the two conditions, as\nshown in Fig. 2. Thanks to this hierarchically decoupled\nparadigm, HiGen ensures simultaneous high spatial qual-\nity and motion diversity in the generated videos.\nTo validate HiGen, we extensively conduct qualitative\nand quantitative analyses, comparing it with state-of-the-art\nmethods on the public dataset, i.e., MSR-VTT [63]. The ex-\nperimental results demonstrate the effectiveness of HiGen\nand its superior performance compared to current methods.\n2. Related Works\nDiffusion-based Text-to-Image Generation.\nRecently,\ndiffusion models have greatly advanced the progress of\ntext-driven photorealistic image synthesis. Initially, due to\nthe substantial computational burden associated with per-\nforming iterative denoising on high-resolution images, early\nworks [18, 50] predominantly concentrated on the genera-\ntion of low-resolution images. To generate high-resolution\nimages, a series of methods [5, 20, 36, 42, 45] have em-\nployed super-resolution techniques on low-resolution im-\nages, while others [15, 39, 43] have utilized decoders to de-\n2\nFigure 3. The overall framework of HiGen. Left: The structure-level spatio-temporal decoupling. Firstly, spatial reasoning is performed\nto obtain latent embeddings of spatial priors. Then, these spatial priors are used for temporal reasoning to generate videos. Right: The\ncontent-level motion-appearance decoupling. Motion analysis and appearance analysis refer to the calculations of motion and appearance\nguidance, respectively.\ncode features from the latent space. Besides, exploring how\nto achieve flexible and controllable image generation is also\nan important research direction, such as ControlNet [66],\nComposer [26], DreamBooth [44], etc. Building upon state-\nof-the-art image generation methods, numerous advanced\nvideo generation [16, 65] or editing [6, 9, 35, 40, 61, 69]\napproaches have been developed by fine-tuning with ad-\nditional temporal transformer layers or controlling the in-\nference process. In this work, we fine-tune a high-quality\ntext-to-video model by leveraging the powerful and efficient\ntext-to-image model, i.e., Stable Diffusion [43].\nDiffusion-based Text-to-Video Generation. Video syn-\nthesis methods strive to explore the generation of tempo-\nrally coherent videos. Early works primarily relied on Gen-\nerative Adversarial Networks (GANs) [4, 23, 47, 49, 51,\n54, 64, 68]. Recently, breakthroughs have been achieved\nthrough diffusion-based methods, which can be broadly\ncategorized into two paradigms: (i) introducing additional\ntemporal layers [7, 14, 16, 17, 32, 34, 55, 57, 61, 62, 71]\nor operations [1] for fine-tuning. To reduce the complexity\nof video generation, some works [7, 19, 30, 48, 59, 65, 71]\nemploy a series of big diffusion models for generating and\nupsampling videos given the input text. Besides, another\nline [14, 34] alleviates the training difficulty by increasing\ntemporal correlations between frame-wise noise, but this\nmay limit the temporal diversity of the generated videos.\n(ii) Controlling the inference process through training-free\ndesigns [11, 22, 25, 27, 31]. This paradigm does not require\ntraining but typically yields lower temporal continuity com-\npared to fine-tuning-based methods.\nUnlike existing approaches, in this work, we explore a\nhierarchical spatio-temporal decoupling paradigm based on\nthe more promising fine-tuning strategy to train T2V mod-\nels that exhibits both rich temporal variations and high-\nquality spatial content.\n3. Approach\n3.1. Preliminaries\nIn this work, we use x0 = [x1\n0, . . . , xF\n0 ] to denote a video\nwith F frames. Following Stable Diffusion [43], we map\nthe video frames into the latent space by a Variational Auto-\nEncoder (VAE) [28] as z0 = [E(x1\n0), . . . , E(xF\n0 )], where\nE denotes the encoder, and z0 can be decoded by the de-\ncoder D to reconstruct RGB pixels. With the video latent\nembedding z0, the diffusion process involves gradually add\nrandom noises into z0 using a T-Step Markov chain [29]:\nq(zt|zt\u22121) = N(zt;\np\n1 \u2212 \u03b2t\u22121zt\u22121, \u03b2tI),\n(1)\nwhere \u03b2t refers to the noise schedule, and N(\u00b7; \u00b7) indi-\ncates the Gaussian noise. After being corrupted by noise,\nthe obtained zt is fed into a 3D-UNet for noise estimation,\nenabling progressive denoising process to restore a clean\nvideo latent embedding.\nIn both the training and inference phase of the 3D-UNet,\nwe adopt the same approach as in Stable Diffusion to inject\nthe text condition and diffusion time t separately into the\nspatial Transformer layer and residual block. For brevity,\nwe omit the details of these two components in Fig. 3.\n3.2. Structure-level Decoupling\nFrom a model structure perspective, we divide the T2V gen-\neration into two steps: spatial reasoning and temporal rea-\nsoning. Spatial reasoning aims to maximize the utilization\nof the knowledge in T2I models, thereby providing high-\nquality spatial priors for temporal reasoning. To obtain this\nprior, we employ the same textual-conditional image syn-\nthesis procedure like Stable Diffusion [43]. Specifically,\nas shown in the Spatial Reasoning card in Fig. 3, we only\nleverage the spatial layers in 3D-UNet while disregarding\nits temporal components for spatial generation.\nAfter T\n3\nFigure 4. The spatial prior for temporal reasoning.\nsteps of denoising, the spatial prior is represented as zs\n0. It\nis worth noting that zs\n0 does not need to be decoded by the\nVAE decoder D to reconstruct its pixel values. This allows\nfor an efficient input of zs\n0 into the subsequent temporal rea-\nsoning.\nThe core idea of temporal reasoning is to synthesis di-\nverse temporal dynamics for video generation on top of\nthe spatial prior zs\n0.\nSpecifically, as shown in shown in\nFig. 4, we initialize a convolutional layer with all zeros\n(i.e., ConvStemt(\u00b7)) for zs\n0 separately.\nThe structure of\nConvStemt(\u00b7) is identical to the image pre-trained convolu-\ntional stem in the UNet (i.e., ConvStems(\u00b7)). After passing\nthrough ConvStemt(\u00b7), we repeat the spatial prior F times\nand add it to the noisy video embeddings zt for UNet.\nBesides, we further clarify some details of the proposed\nstructure-level decoupling from the following three aspects:\n(i) Merging the spatial prior after the first convolutional\nstem enables effective guidance for all the spatial and tem-\nporal layers in the 3D-UNet, which maximizes the utiliza-\ntion of the rich semantic priors present in the spatial prior.\n(ii) Our temporal reasoning and spatial reasoning share the\nsame spatial layers.\nThis allows the temporal reasoning\nphase to leverage the pre-trained knowledge in the spa-\ntial layers, facilitating more accurate temporal synthesizing.\n(iii) The temporal layers consist of a series of temporal con-\nvolutions and temporal self-attention layers following [55].\nDespite similar structures, our temporal layers can be freed\nfrom intricate spatial contents and can solely focus on gen-\nerating fine-grained temporal motions between consecutive\nframes, as demonstrated in Fig. 7.\n3.3. Content-level Decoupling\nBased on the structure-level decoupling, our paradigm is al-\nready capable of generating spatially realistic frames. How-\never, in the temporal case, it still faces two challenges:\nnearly static video frames (e.g., Gen-2 [12]) and unstable\ntemporal variations (e.g., the 2nd row in Fig. 5). Hence,\nwe further propose motion and appearance decoupling for\nvideo content level to enhance the vividness and stability of\nsynthesized videos.\nMotion Analysis. For motion decoupling, we present mo-\ntion analysis to quantify the magnitude of motion between\nframes, providing motion guidance for 3D-UNet.\nFPS\n(frames per second), which reflects the playback speed of\nthe video, may seem like an intuitive choice [71]. How-\never, FPS alone does not accurately reflect the motion in a\nvideo (e.g., static videos may also have a high FPS). In-\nspired by video understanding tasks [56, 70], frame dif-\nferencing with negligible computational cost is an effec-\ntive method for measuring video motion. Therefore, for\na sequence of F frames, we define the motion factor as\n\u03b3m\nf = ||zf\n0 \u2212 zf+1\n0\n||, which indicates the magnitude of the\npixel differences between adjacent frames. By computing\n\u03b3m\nf for F frames, we can obtain F \u2212 1 motion factors:\n\u02dcrm = [\u03b3m\n1 , . . . , \u03b3m\nF \u22121] \u2208 RF \u22121.\nTo incorporate \u02dcrm into the 3D-UNet, we first round \u03b3m\nf\nand then utilize sinusoidal positional encoding [53] and a\nzero-initialized MLP (Multi-Layer Perceptron) to map it\ninto a C-dimensional space:\nrm = Interpolate(MLP(Sin(Round(\u02dcrm)))) \u2208 RF \u00d7C, (2)\nwhere Interpolate(\u00b7) is a linear interpolation function that\naligns the F \u2212 1 motion factors with the actual number of\nframes (i.e., F). Next, the motion guidance rm is added to\nthe time-step embedding vector of the diffusion sampling\nstep t [18]. Therefore, rm is integrated with features in each\nresidual block.\nAppearance Analysis. The motion factor describes pixel-\nlevel variations between adjacent frames while it cannot\nmeasure the appearance changes. To address this, we lever-\nage existing visual semantic models such as, DINO [8, 37],\nCLIP [41], for appearance analysis between frames:\ng = Norm(\u2126(x0)) \u2208 RF \u00d7D, \u02dcra = g \u2297 T (g) \u2208 RF \u00d7F ,\n(3)\nwhere \u2126(\u00b7) and Norm(\u00b7) refer to the semantic model and\nnormalization operation, respectively. \u2297 is matrix multipli-\ncation, and T (\u00b7) means the transpose operation. Therefore,\n\u02dcra represents the cosine similarities between all frames,\nwhich is then transformed using a zero-initialized MLP to\nobtain the appearance guidance: ra = MLP(\u02dcra) \u2208 RF \u00d7C.\nAfterwards, ra is inputted into the 3D-UNet in the same way\nas the motion guidance rm.\nIn general, a video clip with large appearance variations\nwill have a lower cosine similarity value between the first\nand last frames, i.e., \u02dcra\n0,F \u22121. To align with intuition, we\nfurther define the appearance factor as \u03b3a = 1 \u2212 \u02dcra\n0,F \u22121.\nIn this case, a larger appearance factor \u03b3a corresponds to\nsignificant appearance variations in the generated videos.\nIn training, we calculate the appearance guidance from real\nvideos using Eq. 3. In inference, we manually construct\nthe variation matrix (\u02dcra) based on the appearance factor \u03b3a,\nwhich will be discussed in the next section.\n3.4. Training and Inference\nTraining. We train our 3D-UNet through image-video joint\ntraining [21, 58]. Specifically, we allocate one-fourth of the\nGPUs for image fine-tuning (i.e., spatial reasoning), while\n4\nPrompt: A man is riding a horse in sunset.\nModelScope\nHiGen w/o CL\nHiGen\nw/o SL\nw/o CL\nw SL\nw/o CL\nw SL\nw CL\nFigure 5. Visualization for the effect of Structure-Level (SL) de-\ncoupling and Content-Level (CL) decoupling.\nthe remaining GPUs are utilized for video fine-tuning (i.e.,\ntemporal reasoning). For image GPUs, we only optimize\nthe spatial parameters that were pre-trained by Stable Diffu-\nsion [43] to preserve its spatial generative capability. On the\nother hand, for video fine-tuning, we optimize all parame-\nters based on strong spatial priors. To ensure efficiency, we\nutilize the middle frame of the input videos as the spatial\nprior during training.\nInference. Our inference process starts by performing a\nstandard T2I process [43] using only the textual conditions,\nresulting in the high-quality spatial prior. Then, this spatial\nprior, along with the motion and appearance guidances, will\nbe inputted into the 3D-UNet for temporal reasoning. Next,\nlet\u2019s explain how to construct the guidance features that\ncorrespond to the specified motion and appearance factors.\nFirstly, for a given motion factor \u03b3m, we set all elements in\nthe vector \u02dcrm to \u03b3m, and construct the motion guidance rm\nby Eq. 2. For a stable video, the recommended range for\n\u03b3m is [300, 600]. Secondly, for appearance guidance, we\ncan manually construct the variation matrix \u02dcra based on the\ngiven appearance factor \u03b3a:\n\u02dcra =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n0k + 1,\n1k + 1,\n\u00b7 \u00b7 \u00b7\n(F \u2212 1)k + 1,\n1k + 1,\n0k + 1,\n\u00b7 \u00b7 \u00b7\n(F \u2212 2)k + 1,\n...\n...\n...\n...\n(F \u2212 2)k + 1,\n(F \u2212 3)k + 1,\n\u00b7 \u00b7 \u00b7\n1k + 1,\n(F \u2212 1)k + 1,\n(F \u2212 2)k + 1,\n\u00b7 \u00b7 \u00b7\n0k + 1,\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\n,\n(4)\nwhere k = \u2212\u03b3a\nF \u22121. The variation matrix \u02dcra is obtained by lin-\near interpolation, resulting in smooth appearance changes\nbetween consecutive frames.\n4. Experiments\n4.1. Implementation Details\nOptimization. In this work, all modules are trained using\nthe AdamW [33] optimizer with a learning rate of 5e-5. The\nweight decay is set to 0, and our default training iteration is\n25,000. The spatial resolution of the videos is 448\u00d7256.\nDuring the image-video joint training, the batch size for\nimages is 512 (distributed across 2 GPUs), the number of\nSL\nCL\nTemporal\nConsistency \u2191\nCLIPSIM\u2191\nModelScope [55]\n\u2717\n\u2717\n0.931\n0.292\n\u2193\n\u2713\n\u2717\n0.889\n0.313\nHiGen\n\u2713\n\u2713\n0.944\n0.318\nTable 1. Ablation studies for Structure-Level (SL) decoupling and\nContent-Level (CL) decoupling.\nvideo frames F is 32, and the batch size for videos is 72\n(distributed across 6 GPUs). Therefore, we use 8\u00d7A100\nGPUs to fine-tune the denoiser. Besides, for the pre-trained\nparameters from Stable Diffusion (i.e., the spatial layers),\nwe apply a decay of 0.2 to their gradients.\nDatasets. The dataset used in our study consists of two\ntypes: video-text pairs and image-text pairs. For the video\ndataset, apart from the publicly available WebVid10M [3],\nwe also select a subset of aligned textual and video from\nour internal data, amounting to a total of 20 million video-\ntext pairs. The image dataset primarily consists of LAION-\n400M [46] and similar private image-text pairs, compris-\ning around 60 million images. In the ablation experiments,\nfor efficiency, we gathered 69 commonly used imaginative\nprompts from recent works for testing, which will be in-\ncluded in our Appendix. For the comparison of Fr\u00b4echet\nInception Distance (FID) [38], Fr\u00b4echet Video Distance\n(FVD) [52] and CLIP Similarity (CLIPSIM) [60] metrics\nwith state-of-the-art in Tab. 3, we evaluated the same MSR-\nVTT dataset [63] as previous works.\n4.2. Ablation Studies\nIn this section, we will analyze our hierarchical spatio-\ntemporal decoupling mechanism.\nOur baseline is Mod-\nelScopeT2V [55]. Unless otherwise specified, we default\nto setting the motion factor \u03b3m to 500 and the appearance\nfactor \u03b3a to 1.0.\nThe effect of hierarchical decoupling. Comparing the first\ntwo rows of Tab. 1, the structure-level decoupling signifi-\ncantly improves the spatial quality (i.e., CLIPSIM), but it\nseverely compromises temporal consistency. The first two\nrows of Fig. 5 also provide a more intuitive demonstration\nof this effect. Content-level decoupling, as shown in the\nthird row of Tab. 1 and Fig. 5, ensures superior spatial qual-\nity and improved temporal stability of the video frames.\nTemporal reasoning analysis.\nIn Fig. 7, we visualize\nvideos generated without spatial priors, showing a decou-\npling between temporal and spatial synthesis. The absence\nof additional spatial priors results in videos that primarily\nexhibit motion correlated with the text. Combining tempo-\nral reasoning with spatial priors reduces the complexity of\nvideo synthesis and enables high-quality results. Addition-\nally, in Fig. 6, we synthesize videos using the same spatial\nprior but different textual prompts, observing that the tem-\nporal reasoning stage effectively utilizes the motion knowl-\nedge provided by the text prompts.\n5\nA cute fire dog stands on the sea.\nPrompt1: A cute fire dog walks on the sea.\nPrompt2: A cute fire dog jumps high on the sea.\nPrompt3: A yellow robot.\nPrompt4: A yellow robot grows a pair of yellow wings.\nA yellow robot.\nSpatial Prior\nSpatial Reasoning\nTemporal Reasoning\n+ Prompt1\nTemporal Reasoning\n+ Prompt2\nSpatial Prior\nSpatial Reasoning\nTemporal Reasoning\n+ Prompt3\nTemporal Reasoning\n+ Prompt4\nFigure 6. Combining the same spatial prior with different textual prompts allows dynamic control over the generated videos during the\ntemporal reasoning stage.\nPrompt: Close up coffee being poured into a glass.\nHiGen w/o Spatial Prior\nPrompt: A fire is buring on a candle.\nHiGen\nHiGen w/o Spatial Prior\nHiGen\nFigure 7. Visualization for structure-level decoupling. \u201cHiGen\nw/o Spatial Prior\u201d refers to our temporal reasoning without in-\nputting any spatial priors.\nContent-level decoupling analysis. In Fig. 8, the curves\ndemonstrate the impact of motion and appearance factors\non the generated videos. Higher values of the motion factor\n(300 to 600) and appearance factor (0 to 1.0) decrease tem-\nporal consistency, while the spatial semantic remains stable\naccording to the CLIPSIM metric. The dashed line repre-\nsents using FPS as an alternative to our content-level decou-\npling strategy. Notably, changing the FPS has minimal in-\nfluence on the temporal dynamics of the videos, validating\nthe superiority of our decoupling strategy as a more effec-\ntive design choice.\nIn addition, Fig. 2 visually illustrates the impacts of these\ntwo factors. The motion factor governs scene movement,\nwhile the appearance factor enables diverse semantic varia-\ntions in the generated videos. Interestingly, lower temporal\n0.31\n0.32\n0.33\n0.34\n0.35\n0.36\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n300\n400\n500\n600\nTemporal Consistency\nTemporal Consistency\nTemporal Consistency\nTemporal Consistency\nTemporal Consistency\nCLIPSIM\nCLIPSIM\nCLIPSIM\nCLIPSIM\nTemporal Consistency\nCLIPSIM\n\ud835\udefe! = 0.0\n\ud835\udefe! = 0.4\n\ud835\udefe! = 0.8\n\ud835\udefe! = 1.0\nFPS=16\nFPS=8\nFPS=4\nFPS=2\n\ud835\udefe\"\nTemporal Consistency\n\ud835\udefe! = 0.0\n\ud835\udefe! = 0.4\n\ud835\udefe! = 0.8\n\ud835\udefe! = 1.0\nFPS\nCLIPSIM\n\ud835\udefe! = 0.0\n\ud835\udefe! = 0.4\n\ud835\udefe! = 0.8\n\ud835\udefe! = 1.0\nMotion Factor\nAppearance Factor\nFigure 8. Parameter sensitivity analysis of the motion factor \u03b3m\nand appearance factor \u03b3a.\nMethod\nVisual\nQuality\nTemporal\nQuality\nText\nAlignment\nModelScopeT2V [55]\n32.4%\n43.2%\n54.8%\nText2Video-Zero [27]\n63.6%\n26.0%\n53.8%\nVideoCrafter [10]\n81.2%\n55.2%\n76.8%\nHiGen\n84.4%\n74.0%\n81.2%\nTable 2. Human evaluations with open-sourced methods.\nconsistency scores lead to livelier and more dynamic videos.\nThis suggests that overly prioritizing temporal consistency\nmay hinder the potential for vibrant and engaging videos.\nSemantic model analysis. To achieve content-level decou-\npling, we aim to ensure high independence between the mo-\ntion and appearance factors. To accomplish this, we explore\nself-supervised models such as DINO [8, 37] and the mul-\ntimodal model CLIP [41] as semantic models. We evaluate\nthe Pearson Correlation Coefficients (PCC) between these\ntwo factors. In Fig. 10, we observe that although the PCC\nbetween the DINO-based appearance factor and motion fac-\ntor is only slightly lower (0.03) than that of CLIP, the dis-\ntribution of DINO is more uniform. This indicates that self-\nsupervised models, specifically DINO, are more sensitive to\nappearance variations. Based on this finding, we default to\nusing DINO as our semantic model.\n6\nPrompt: An astronaut dances in the desert.\nPrompt: An iron man surfing in the sea.\nPrompt: A stylized octopus swimming in an underwater cave system.\nPrompt: A female mandalorian in forest green mandalorian armour with helmet.\nPrompt: A teddy bear wearing sunglasses playing guitar next to a cactus.\nPrompt: A yellow tiger with lightning around it.\nPrompt: A robot girl.\nPrompt: Wooden figurine surfing on a surfboard in space.\nFigure 9. Sample visualization of generated videos.\n(a)\n(b)\nr = 0.40\nr = 0.43\nMotion Factor \ud835\udefe!\n\ud835\udefe!\nMotion Factor\nAppearance Factor \ud835\udefe\"\nDINO as Semantic Model\nCLIP as Semantic Model\nAppearance Factor \ud835\udefe\"\nFigure 10. Correlation analysis between the motion factor and\nappearance factor with DINO [8, 37] and CLIP [41] as semantic\nmodels. Here, we measure these factors based on the first and last\nframes of 8000 random videos.\nTraining efficiency. The structure-level decoupling of spa-\ntial and temporal aspects mitigates the difficulties in joint\nspatio-temporal denoising. Fig. 11 compares the generated\nvideos at different iterations with the baseline method. It\n2000\n10000\n25000\nPrompt: A 3D model of an elephant origami. Studio lighting.\nTraining Iterations\n2000\n10000\n25000\nTraining Iterations\n(a) Baseline\n(b) HiGen\nFigure 11. Comparison with baseline at various training stages.\nis clear that HiGen consistently outperforms the baseline\nregarding visual quality throughout various training stages.\n7\nVideoCrafter\nGen-2\nModelScope\nText-to-Video Zero\nHiGen\nPrompt: A girl with long curly blonde hair and sunglasses, camera pan from left to right.\nPrompt: Pikachu turned his back towards me.\nHiGen\n+ I2VGen-XL\nFigure 12. Qualitative comparison with ModelScopeT2V [55], Text-2-Video Zero [27], VideoCrafter [10] and Gen-2 [12]. In the last row,\nwe utilize the Video-to-Video model from the open-sourced I2VGen-XL [67] to enhance the spatial resolution of our videos, resulting in\nfurther improvement in spatial quality.\nMore visualizations. Fig. 9 demonstrates the generation of\n8 different styles of videos, such as humans, animals, and\nmarine settings. The generated videos using HiGen show-\ncase consistent, high-quality frames comparable to Stable\nDiffusion-generated images.\nWhen played in sequence,\nthese frames exhibit both smooth temporal content and di-\nverse semantic variations, enhancing the richness and vivid-\nness of the videos.\nHuman evaluations. In Tab. 2, we conducted a human\nevaluation of three recent open-source methods, consider-\ning spatial, temporal, and textual aspects. Notably, HiGen\nexhibits the most substantial improvement in temporal per-\nformance, surpassing VideoCrafter [10] by 18.8% (increas-\ning from 55.2% to 74.0%). These findings further reinforce\nthe superiority of our approach.\n4.3. Comparison with State-of-the-art\nTab. 3 compares HiGen with existing approaches using\nFID, FVD, and CLIPSIM metrics on MSR-VTT [63]. Our\nmethod shows significant improvements in FID and FVD\nmetrics. However, as noted in previous works [39], these\nmetrics may not accurately represent the generated quality.\nTo further evaluate, we visually compare our results with\nrecent state-of-the-art methods in Fig. 12. It is evident that\nour HiGen achieves a better balance between spatial qual-\nity and temporal motion in the generated videos.\nMethod\nFID \u2193\nFVD\u2193\nCLIPSIM\u2191\nCogVideo (English) [24]\n23.59\n1294\n0.2631\nLatent-Shift [1]\n15.23\n-\n0.2773\nMake-A-Video [48]\n13.17\n-\n0.3049\nVideo LDM [7]\n-\n-\n0.2929\nMagicVideo [71]\n-\n998\n-\nVideoComposer [58]\n10.77\n580\n0.2932\nModelScopeT2V [55]\n11.09\n550\n0.2930\nPYoCo [14]\n9.73\n-\n-\nHiGen\n8.60\n406\n0.2947\nTable 3. T2V generation performance on MSR-VTT [63].\n5. Discussions\nThis work presents HiGen, a diffusion model-based ap-\nproach for video generation that decouples spatial and tem-\nporal factors at both the structure-level and content-level.\nWith a unified denoiser, HiGen generates spatially pho-\ntorealistic priors and temporally coherent motions, while\nextracting subtle cues from the video content to express\nappearance and motion changes for denoising guidance.\nThrough this design, HiGen successfully reduces the com-\nplexity of T2V task, synthesizes realistic videos with se-\nmantic accuracy and motion stability, and outperforms\nstate-of-the-art T2V methods in extensive experiments.\nLimitations. Due to limitations in computational resources\nand data quality, our HiGen\u2019s ability to generate object de-\ntails lags behind that of current image synthesis models.\nAdditionally, accurately modeling human and animal ac-\n8\ntions that adhere to common sense proves challenging, par-\nticularly in cases of substantial motion. To address these\nchallenges, our future research will delve into improving\nmodel design and data selection.\nAcknowledgement.\nThis work is supported by the\nNational Natural Science Foundation of China under\ngrant U22B2053 and 62176097, and by Alibaba DAMO\nAcademy through Alibaba Research Intern Program.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video genera-\ntion. arXiv preprint arXiv:2304.08477, 2023. 3, 8\n[2] Blattmann Andreas, Dockhorn Tim, Kulal Sumith, Mendele-\nvitch Daniel, Kilian Maciej, Lorenz Dominik, Levi Yam, En-\nglish Zion, Voleti Vikram, Letts Adam, Jampani Varun, and\nRombach Robin. Stable video diffusion: Scaling latent video\ndiffusion models to large datasets. 2023. 12, 13\n[3] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, pages 1728\u20131738, 2021. 5\n[4] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chel-\nlappa, and Hans Peter Graf. Conditional gan with discrimi-\nnative filter generation for text-to-video synthesis. In IJCAI,\npage 2, 2019. 3\n[5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediff-i: Text-to-image\ndiffusion models with an ensemble of expert denoisers. corr,\nvol. abs/2211.01324 (2022), 2022. 2\n[6] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing. In ECCV, pages 707\u2013723. Springer, 2022.\n3\n[7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, pages 22563\u201322575, 2023.\n1, 3, 8\n[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nICCV, pages 9650\u20139660, 2021. 4, 6, 7\n[9] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.\nPix2video: Video editing using image diffusion. In ICCV,\npages 23206\u201323217, 2023. 3\n[10] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter1: Open diffusion models for high-quality video\ngeneration, 2023. 1, 6, 8\n[11] Zhongjie Duan, Lizhou You, Chengyu Wang, Cen Chen, Zi-\nheng Wu, Weining Qian, Jun Huang, Fei Chao, and Ron-\ngrong Ji. Diffsynth: Latent in-iteration deflickering for re-\nalistic video synthesis.\narXiv preprint arXiv:2308.03463,\n2023. 3\n[12] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn ICCV, pages 7346\u20137356, 2023. 1, 2, 4, 8\n[13] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman.\nConvolutional two-stream network fusion for video action\nrecognition. In CVPR, pages 1933\u20131941, 2016. 2\n[14] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. In ICCV, pages\n22930\u201322941, 2023. 3, 8\n[15] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nCVPR, pages 10696\u201310706, 2022. 2\n[16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 3\n[17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 3\n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 33:6840\u20136851, 2020. 2,\n4\n[19] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 1, 3\n[20] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffu-\nsion models for high fidelity image generation. JMLR, 23\n(1):2249\u20132281, 2022. 2\n[21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet.\nVideo\ndiffusion models. arxiv e-prints, page.\narXiv preprint\narXiv:2204.03458, 3, 2022. 4\n[22] Susung Hong, Junyoung Seo, Sunghwan Hong, Heeseong\nShin, and Seungryong Kim.\nLarge language models are\nframe-level directors for zero-shot text-to-video generation.\narXiv preprint arXiv:2305.14330, 2023. 3\n[23] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 3\n[24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and\nJie Tang. Cogvideo: Large-scale pretraining for text-to-video\ngeneration via transformers. In ICLR, 2023. 8\n[25] Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi\nYu, and Sibei Yang. Free-bloom: Zero-shot text-to-video\ngenerator with llm director and ldm animator. arXiv preprint\narXiv:2309.14494, 2023. 3\n9\n[26] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Composer: Creative and controllable im-\nage synthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023. 3\n[27] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 3, 6, 8\n[28] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n[29] Zhifeng Kong and Wei Ping.\nOn fast sampling of diffu-\nsion probabilistic models. arXiv preprint arXiv:2106.00132,\n2021. 3\n[30] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu,\nQi Zhang, Fu Li, Haocheng Feng, Errui Ding, and Jingdong\nWang. Videogen: A reference-guided latent diffusion ap-\nproach for high definition text-to-video generation.\narXiv\npreprint arXiv:2309.00398, 2023. 3\n[31] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and\nBoyi Li.\nLlm-grounded video diffusion models.\narXiv\npreprint arXiv:2309.17444, 2023. 3\n[32] Binhui Liu, Xin Liu, Anbo Dai, Zhiyong Zeng, Zhen Cui,\nand Jian Yang. Dual-stream diffusion net for text-to-video\ngeneration. arXiv preprint arXiv:2308.08316, 2023. 3\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[34] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and\nTieniu Tan.\nVideofusion: Decomposed diffusion models\nfor high-quality video generation. In CVPR, pages 10209\u2013\n10218, 2023. 2, 3\n[35] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023. 3\n[36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n[37] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 4, 6, 7, 12\n[38] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu.\nOn\naliased resizing and surprising subtleties in gan evaluation.\nIn CVPR, pages 11410\u201311420, 2022. 5\n[39] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2, 8\n[40] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 3\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748\u20138763. PMLR, 2021. 4, 6, 7,\n12\n[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 2, 3, 5\n[44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, pages 22500\u201322510, 2023. 3\n[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 35:36479\u201336494, 2022. 2\n[46] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 5\n[47] Xiaoqian Shen,\nXiang Li,\nand Mohamed Elhoseiny.\nMostgan-v: Video generation with temporal motion styles.\nIn CVPR, pages 5652\u20135661, 2023. 3\n[48] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 3, 8\n[49] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. Stylegan-v: A continuous video generator with the\nprice, image quality and perks of stylegan2. In CVPR, pages\n3626\u20133636, 2022. 3\n[50] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 2\n[51] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng,\nDimitris N Metaxas, and Sergey Tulyakov. A good image\ngenerator is what you need for high-resolution video synthe-\nsis. arXiv preprint arXiv:2104.15069, 2021. 3\n[52] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 5\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NeurIPS, 30, 2017. 4\n[54] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\nGenerating videos with scene dynamics. NeurIPS, 29, 2016.\n3\n10\n[55] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 1,\n2, 3, 4, 5, 6, 8\n[56] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nnetworks: Towards good practices for deep action recogni-\ntion. In ECCV, pages 20\u201336. Springer, 2016. 2, 4\n[57] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap at-\ntention in spatiotemporal diffusions for text-to-video gener-\nation. arXiv preprint arXiv:2305.10874, 2023. 1, 3\n[58] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 4, 8\n[59] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023. 3\n[60] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji,\nFan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Gen-\nerating open-domain videos from natural descriptions. arXiv\npreprint arXiv:2104.14806, 2021. 5\n[61] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nICCV, pages 7623\u20137633, 2023. 3\n[62] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang\nJiang. Simda: Simple diffusion adapter for efficient video\ngeneration. arXiv preprint arXiv:2308.09710, 2023. 3\n[63] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nCVPR, pages 5288\u20135296, 2016. 2, 5, 8\n[64] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho\nKim, Jung-Woo Ha, and Jinwoo Shin. Generating videos\nwith dynamics-aware implicit generative adversarial net-\nworks. arXiv preprint arXiv:2202.10571, 2022. 3\n[65] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent dif-\nfusion models for text-to-video generation. arXiv preprint\narXiv:2309.15818, 2023. 3\n[66] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, pages 3836\u20133847, 2023. 3\n[67] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and\nJingren Zhou.\nI2vgen-xl:\nHigh-quality image-to-video\nsynthesis via cascaded diffusion models.\narXiv preprint\narXiv:2311.04145, 2023. 8, 12, 13\n[68] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dim-\nitris Metaxas. Learning to forecast and refine residual motion\nfor image-to-video generation. In ECCV, pages 387\u2013403,\n2018. 3\n[69] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao\nZhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng\nShou.\nMotiondirector: Motion customization of text-to-\nvideo diffusion models. arXiv preprint arXiv:2310.08465,\n2023. 3\n[70] Yuan Zhi, Zhan Tong, Limin Wang, and Gangshan Wu.\nMgsampler: An explainable sampling strategy for video ac-\ntion recognition. In ICCV, pages 1513\u20131522, 2021. 4\n[71] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 3, 4, 8\n11\nAppearance Factor=0.00\nMotion Factor=300\nCLIP\nAppearance Factor=0.00\nMotion Factor=300\nDINO\nCLIP\nAppearance Factor=0.00\nMotion Factor=500\nDINO\nAppearance Factor=0.00\nMotion Factor=500\nPrompt: A 3D model of an elephant origami. Studio lighting.\nPrompt: A boring bear is standing behind a bar table. High Definition.\nFigure A13. Visualization for different semantic models. In these\nfour generated videos, we set the appearance factor \u03b3a to zero, in-\ndicating that the entire video should have minimal visual changes.\nOverview\nIn this supplementary material, we provide experiments re-\nlated to the semantic model and features for motion and ap-\npearance guidance in Sec. A. Lastly, in Tab. A5 and Tab. A6,\nwe provide the 69 text prompts used in our ablation experi-\nments.\nA. More Experiments\nDifferent semantic models. In Fig. A13, we visualize the\ngenerated results when using DINO [37] and CLIP [41] as\nsemantic models in the appearance analysis. It can be ob-\nserved that videos generated using DINO as the semantic\nmodel exhibit better temporal stability, whereas CLIP may\nresult in unexpected and irrational abrupt changes. Fig.10\nin the main paper also supports this observation by demon-\nstrating a lower correlation between appearance and motion\nfactors obtained from DINO features, thereby enabling bet-\nter independent control.\nHow to generate motion and appearance guidance? In\nFig.3 of the main paper, we default to using a vector com-\nposed of F \u2212 1 frame difference elements to generate mo-\ntion guidance, while a similarity matrix is used to generate\nappearance guidance. The reason behind this choice is that\nframe difference cannot capture the motion information be-\ntween frames with significant visual differences, whereas a\nsemantic model can effectively model the appearance cor-\nrelation between any pair of video frames.\nIn Tab. A4,\nwe quantitatively analyze the combinations of motion and\nappearance guidance using vector-based and matrix-based\nmethods.\nWe conducted evaluations with three different\nmotion factors and semantic factors for each combination,\nMotion\nGuidance\nAppearance\nGuidance\nTemporal\nConsistency\nCLIPSIM\nVector\nVector\n0.9314 \u00b1 0.0154\n0.3171 \u00b1 0.0015\nMatrix\nMatrix\n0.9380 \u00b1 0.0198\n0.3165 \u00b1 0.0010\nMatrix\nVector\n0.9392 \u00b1 0.0167\n0.3159 \u00b1 0.0017\nVector\nMatrix\n0.9367 \u00b1 0.0276\n0.3166 \u00b1 0.0013\nTable A4. Different methods for generating motion and appear-\nance guidance.\nand then measured changes in terms of Temporal Consis-\ntency and CLIPSIM. It can be observed that different com-\nbinations exhibit similar spatial quality for the generated\nvideos (i.e., minimal changes in CLIPSIM), but using frame\ndifference vectors for motion guidance and similarity matri-\nces for appearance guidance leads to more significant tem-\nporal variations (i.e., \u00b1 0.0276).\nComparison with image-to-video approaches.\nIn\nFig. A14, we compare HiGen with state-of-the-art image-\nto-video generation methods. The images are generated us-\ning advanced text-to-image methods such as Midjourney.\nWe directly incorporate these images as spatial priors in\nthe temporal reasoning step. It can be observed that, com-\npared to Stable Video Diffusion [2] and I2VGen-XL [67],\nthe videos generated by HiGen exhibit significantly more\nvivid temporal dynamics and creativity.\nB. Prompts\nFinally, we will provide the 69 text prompts that were used\nin Tables 1, 2, and Figure 8 of our main paper.\n12\nImage\nSVD\nI2VGen-XL\nHiGen\nPrompt: A cute little dolphin in the deep sea, 3D cartoon.\nImage\nSVD\nI2VGen-XL\nHiGen\nPrompt: In a fantastical setting, a highly detailed furry humanoid skunk with piercing eyes confidently \nposes in a medium shot, wearing an animal hide jacket. The artist has masterfully rendered the character \nin digital art, capturing the intricate details of fur and clothing texture.\nPrompt: 3D design featuring an adorable cyberpunk-style cute dog, with a stunning mecha armor, inspired by \ncontrol theory, science fiction aesthetics, and futurism. Created using the powerful UE5 Unreal Engine and C4D \nsoftware for 3D rendering with HDR, aiming for the best quality.\nImage\nSVD\nI2VGen-XL\nHiGen\nFigure A14. Comparison with advanced image-to-video methods, i.e., Stable Video Diffusion [2] and I2VGen-XL [67].\n13\n1. Close up of grapes on a rotating table. High Definition\n2. Raw fresh beef meat fillet on a wooden plate with dill\n3. Close up coffee being poured into a glass. Slow motion\n4. Close-up milky liquid being poured. slow motion\n5. A waving flag close up realistic animation seamless loop\n6. Face of happy macho mature man looking at camera\n7. Face of happy macho mature man smiling\n8. A girl is looking at the camera smiling. High Definition\n9. Woman in sunset\n10. Young girl eye macro, shot in raw, 4k\n11. Blue sky clouds timelapse 4k time lapse big white clouds cumulus growing cloud formation sunny weather background\n12. Campfire at night in a snowy forest with starry sky in the background\n13. Ocean waves hitting headland rocks pacifica california slow motion\n14. There is a table by a window with sunlight streaming through illuminating a pile of books\n15. Beautiful sexy lady in elegant white robe. close up fashion portrait of model indoors. beauty blonde woman\n16. Fire burning in a forest\n17. Wildfire in mountain of thailand (pan shot)\n18. Fireworks\n19. Melting pistachio ice cream dripping down the cone\n20. A 3D model of an elephant origami. Studio lighting\n21. Strawberry close-up on a black background swinging, slow motion. water flows down the berry\n22. A spaceship is landing.\n23. A giant spaceship is landing on mars in the sunset. High Definition\n24. Drone flythrough of a tropical jungle covered in snow. High Definition\n25. Fog at the end of the path in the summer-autumn forest. nobody present. scary scene. peaceful. quiet\n26. Cars running on the highway at night\n27. A man is riding a horse in sunset\n28. close up of a clown fish swimming. 4K\n29. a boring bear is standing behind a bar table. High Definition\n30. Beautiful pink rose background. blooming rose flower rotation, close-up\n31. A cat eating food out of a bowl, in style of Van Gogh\n32. Wide shot of woman worker using welding machine on her work in site construction.\n33. celebration of christmas\n34. A fire is burning on a candle.\n35. Pov driving high rural mountain country road snow winter blue sky nature environment sierra nevada usa\n36. Lid taken off gift box with puppy inside on table top with holiday gifts.\n37. Silhouette of retired caucasian american couple enjoying the sunrise having kayaking trip on the lake outdoors red dragon\n38. Aerial autumn forest with a river in the mountains.\n39. London, uk - november 16,2014:traffic. buses and cars in baker street move slowly in london england. congested city traffic\n40. A corgi is swimming fastly\n41. There is a table by a window with sunlight streaming through illuminating a pile of books\n42. A glass bead falling into water with a huge splash. Sunset in the background\n43. Aerial autumn forest with a river in the mountains\n44. astronaut riding a horse\n45. A clear wine glass with turquoise-colored waves inside it\n46. A bear dancing and jumping to upbeat music, moving his whole body\n47. A bigfoot walking in the snowstorm\n48. An iron man surfing in the sea\n49. Filling a glass with warm coffee\n50. 3d fluffy Lion grinned, closeup cute and adorable, long fuzzy fur, Pixar render\nTable A5. Prompts Part-I.\n14\n51. A big palace is flying away, anime style, best quality\n52. A teddy bear is drinking a big wine\n53. A giant spaceship is landing on mars in the sunset. High Definition\n54. A happy elephant wearing a big birthday hat walking under the sea, 4k\n55. Albert Einstein washing dishes\n56. Blue sky clouds timelapse 4k time lapse big white clouds cumulus growing cloud formation sunny weather background\n57. drone flythrough interior of sagrada familia cathedral\n58. Close up of grapes on a rotating table. High Definition\n59. A stunning aerial drone footage time lapse of El Capitan in YosemiteNational Park at sunset\n60. Aerial autumn forest with a river in the mountains\n61. An astronaut dances in the desert\n62. Blue sky clouds timelapse 4k time lapse big white clouds cumulus growing cloud formation sunny weather background\n63. Beautiful pink rose background. blooming rose flower rotation, close-up\n64. Fog at the end of the path in the summer-autumn forest. nobody present. scary scene. peaceful. quiet\n65. A beautiful sunrise on mars, Curiosity rover. High definition,timelapse, dramatic colors\n66. Van Gogh is smoking\n67. A shiny golden waterfall flowing through glacier at night\n68. A teddy bear painting a portrait\n69. Fog at the end of the path in the summer-autumn forest. nobody present. scary scene. peaceful. quiet\nTable A6. Prompts Part-II.\n15\n"
  },
  {
    "title": "Efficient Monotonic Multihead Attention",
    "link": "https://arxiv.org/pdf/2312.04515.pdf",
    "upvote": "6",
    "text": "Efficient Monotonic Multihead Attention\nXutai Ma, Anna Sun, Siqi Ouyang\u2020, Hirofumi Inaguma, Paden Tomasello\nFAIR at Meta, \u2020UC Santa Barbara\nWe introduce the Efficient Monotonic Multihead Attention (EMMA), a state-of-the-art simultaneous\ntranslation model with numerically-stable and unbiased monotonic alignment estimation. In addition,\nwe present improved training and inference strategies, including simultaneous fine-tuning from an\noffline translation model and reduction of monotonic alignment variance. The experimental results\ndemonstrate that the proposed model attains state-of-the-art performance in simultaneous speech-to-\ntext translation on the Spanish and English translation task.\nDate: November 30, 2023\nCorrespondence: xutaima@meta.com\nCode: https://github.com/facebookresearch/seamless_communication.\n1. Introduction\nSimultaneous translation is a task focusing on reducing the latency of the machine translation system. In this\napproach, a simultaneous translation model initiates the translation process even before the speaker completes\ntheir sentence. This type of model plays a pivotal role in various low-latency scenarios, such as personal\ntravels and international conferences, where people desire seamless and real-time translation experiences. In\ncontrast to an offline model, which processes the entire input sentence and generates translation output in a\nsingle step, a simultaneous translation model operates on a partial input sequence. The simultaneous model\nincorporates a policy mechanism to determine when the model should generate translation output. The policy\nis characterized by two actions: read and write. While the write action indicates that the model should\ngenerate a partial translation, the read action introduces a pause in the generation process, allowing the\nmodel to acquire additional input information. The simultaneous policy can be either rule-based or learned\nthrough training processes.\nIn recent times, within the domain of learned policies, a specific category known as monotonic attention-based\npolicies (Raffel et al., 2017; Chiu & Raffel, 2018; Arivazhagan et al., 2019), particularly Transformer-based\nMonotonic Multihead Attention (MMA) (Ma et al., 2019b) has demonstrated state-of-the-art performance in\nsimultaneous text-to-text translation tasks. Monotonic attention provides an unsupervised policy learning\nframework that is based on an estimation of monotonic alignment during training time. Despite MMA\u2019s\nremarkable achievements in text-to-text translation, its adaptation to speech input faces certain challenges.\nMa et al. (2020b) revealed that the MMA model fails to yield significant improvements when applied to\nspeech input over simple wait-k baseline. Ma et al. (2020b) attributes the suboptimal performance of MMA\non speech input to the granularity and continuity characteristics of the speech encoder states.\nIn this paper, we further investigate the adaptation of monotonic attention on speech translation. We\ndemonstrate the two primary factors underlying the sub-optimal performance. The first is the numerical\ninstability and introduction of bias during monotonic alignment estimation, which comes from techniques\nintroduced by Raffel et al. (2017). The second is the significant variance in monotonic alignment estimation,\nespecially in the later part of the sentence, due to the continuous nature of encoder states. To address these\nchallenges, we propose Efficient Monotonic Multihead Attention (EMMA). Specifically, the key contributions\nof this work include:\n\u2022 A novel numerically stable, unbiased monotonic alignment estimation, with yields state-of-the-art\nperformance on both simultaneous text-to-text and speech-to-text translation.\n\u2022 A new monotonic alignment shaping strategy, including new latency regularization and monotonic\narXiv:2312.04515v1  [cs.CL]  7 Dec 2023\nalignment variance reduction\n\u2022 An enhanced training scheme, involving fine-tuning the simultaneous model based on a pre-trained\noffline model.\n2. Background\n2.1 Notation\nGiven matrices A and B, we annotate the operations used in later chapters, along with their implementation\nin PyTorch toolkit in Table 1.\nNotation\nDefinition\nPyTorch\nAi,j\nIndex i-th row and j-th column in matrix A\nA[i, j]\nAi,:\nIndex i-th row of A as a vector\nA[[i], :]\nA:,j\nIndex j-th column of A as a vector\nA[:, [j]]\nA \u2299 B\nElement-wise product (Hadamard roduct)\nA * B\nAB\nMatrix multiplication\ntorch.bmm(A, B)\ncomprodl(A)\nCumulative product on the l-th dimension\ntorch.cumprod(A, dim=l)\ncomsuml(A)\nCumulative summation on the l-th dimension\ntorch.cumsum(A, dim=l)\ntriub(A)\nUpper triangle of A with a offset of b\ntorch.triu(A, diagonal=b)\nJN\u00d7M\nA matrix with size of N by M, filled with 1\ntorch.ones(N, M)\nrollk\nShift matrix by k elements, on last dimension\nA.roll(k, dims=[-1])\nTable 1 - Matrix operations and their implementation in PyTorch\n2.2 Simultaneous Translation\nDenote X and \u02c6Y the input and output sequences of a translation system. X is text token for text input\nand speech encoder states for speech input. We introduce the concept of a delay sequence denoted as D,\nwhere each element di is the length of input utilized in generating the corresponding output element \u02c6yi. It is\nessential to note that D forms a strictly monotonic non-decreasing sequence.\nIn a simultaneous translation system, there exists \u02c6Y such that di < |X|. Meanwhile, an offline translation\nmeans di = |X| for all i. The measurement of D varies with input and output media. In this paper, di is\nmeasured in number of tokens for text input and seconds for speech input.\nThere are two aspects to evaluate simultaneous speech translation system: quality and latency. While the\nquality evaluation is same as offline system, for latency evaluation, we use the most commonly used metric\nAverage Lagging (AL) Ma et al. (2019a), defined as\nAL =\n1\n\u03c4(|X|)\n\u03c4(|X|)\nX\ni=1\ndi \u2212 d\u2217\ni\n(1)\nwhere \u03c4(|X|) = min{i|di = |X|} is the index of the first target translation when the policy first reaches the\nend of the source sentence. d\u2217\ni is the ideal policy defined as\nd\u2217\ni = (i \u2212 1) \u00b7 |X|\n|Y|\n(2)\nwhere Y is the reference translation. As suggested by Ma et al. (2020b), |X| is measured in number of source\nwords for text input and in number of seconds of source speech for speech input.\n2.3 Monotonic Attention\nMonotonic attention models (Raffel et al., 2017; Chiu & Raffel, 2018; Arivazhagan et al., 2019; Ma et al.,\n2019b) have a learnable policy based on monotonic alignment estimation during training time. At a given\n2\ntime when i \u2212 1-th target translation has been predicted and j-th source input has been processed, a stepwise\nprobability, denoted as pi,j, describes the likelihood the model is going to write the i-th prediction rather\nthan read the next input. Specifically, it is defined as\npi,j = P(action = write|i, j; \u03b8p) = Sigmoid\n\u0000N\u03b8p (si\u22121, hj)\n\u0001\n(3)\nwhere N\u03b8p is the policy network, si\u22121 is i \u2212 1-th decoder state and hj is j-th encoder state.\nRaffel et al. (2017) proposed an closed form estimation of alignments between source and target \u03b1i,j from pi,j\nduring training:\n\u03b1i,: = pi,: \u2299 cumprod2(1 \u2212 pi,:) \u2299 cumsum2\n\u0012\n\u03b1i\u22121,: \u2299\n1\ncumprod2(1 \u2212 pi,:)\n\u0013\n(4)\nWhile Raffel et al. (2017) handle the hard alignment between source and target, Chiu & Raffel (2018) introduce\nmonotonic chunkwise attention (MoChA), which enables soft attention in a chunk following the moving\nattention head. Arivazhagan et al. (2019) further proposed monotonic infinite lookback attention (MILk), in\nwhich a soft attention is computed over all the previous history. Given the energy ui,j for the i-th decoder\nstate and the j-th encoder state, an expected soft attention is calculated in Equation 5:\n\u03b2i,j =\n|X|\nX\nk=j\n \n\u03b1i,k exp(ui,j)\nPk\nl=1 exp(ui,l)\n!\n(5)\nwhere \u03b2 instead of \u03b1 is then used in training. Arivazhagan et al. (2019) also introduce latency augmented\ntraining for latency control. Ma et al. (2019b) further extend the monotonic attention to multihead attention\n(MMA) for Transformer models. The design of MMA is to enable every attention head as individual monotonic\nattention.\n3. Efficient Monotonic Multihead Attention\nIn this section, we will discuss three key factors of Efficient Monotonic Multihead Attention (EMMA):\nnumerically stable estimation, alignment shaping, and streaming finetuning.\nIt is noteworthy that the\nmonotonic alignment estimation, denoted as \u03b1, discussed in this section is based on a single attention head.\nFollowing the same design as Ma et al. (2019b), the same estimation of \u03b1 is applied to every attention head in\nMMA as integrated into the Transformer (Vaswani et al., 2017) model. Notably, only the infinite lookback\n(Arivazhagan et al., 2019) variant of monotonic attention is applied.\n3.1 Numerically Stable Estimation\nSimilar to Raffel et al. (2017), the objective of the monotonic estimation is to calculate the expected alignment\n\u03b1i,j, from stepwise write action probability pi,j. Nevertheless, the numerical instability arises from the\ndenominator in Equation 4, particularly when dealing with the multiplication of several small probabilities.\nTo address this issue, we introduce a innovative numerically stable approach for estimation of monotonic\nattention.\nIn accordance with the approach proposed by Raffel et al. (2017), the monotonic alignment between the i-th\ntarget item and the j-th source item can be represented as:\n\u03b1i,j = pi,j\nj\nX\nk=1\n\u03b1i\u22121,k\nj\u22121\nY\nl=k\n(1 \u2212 pi,l)\n(6)\nThis expression can be reformulated in a matrix multiplication format:\n\u03b1i,: = pi,: \u2299 \u03b1i\u22121,:T(i)\n(7)\n3\nwhere T(i) represents a transition matrix, with each of its elements defined as:\nT(i)m,n =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nQn\u22121\nl=m(1 \u2212 pi,l)\nm < n\n1\nm = n\n0\nm > n\n(8)\nT(i)m,n indicates the probability of the policy at the (i \u2212 1)-th target step consecutively skipping from the\nm-th to the n-th inputs. Moreover, the transition matrix can be further expressed as\nT(i) = triu0\n\u0000cumprod2(1 \u2212 triu1\n\u0000J|X|\u00d71roll1(pi,:)\n\u0001\n)\n\u0001\n(9)\nThe operations within the equation can be efficiently executed in parallel on GPUs, as elaborated in Table 1.\nReframing Equation 7, we arrive at the following expression:\n\u03b1i,: = pi,: \u2299 \u03b1i\u22121,:triu0\n\u0000cumprod2(1 \u2212 triu1\n\u0000J|X|\u00d71roll1(pi,:)\n\u0001\n)\n\u0001\n(10)\nIt is noteworthy that this estimation process is also closed-form, with the desirable properties of being\nnumerically stable and unbiased, as it does not require a denominator as the product of probabilities within\nthe equation. A comprehensive derivation of this closed-form estimation is provided in Appendix A.\n3.2 Alignment Shaping\nWhen training the infinite lookback variant of monotonic attention, it is necessary to add latency regularization\nin order to prevent the model from learning a trivial policy. Without latency regularization, the optimal policy\nto minimize cross-entropy loss is to read the entire the sequence before starting the translation. Therefore, we\napplied latency and variance regularizations to control the tradeoff between translation quality and latency of\nthe learned simultaneous translation policy.\nThe latency of the alignment describes how much partial input information is needed by the model to generate\npart of the translation. The reduction of latency is commonly achieved by introducing a regularization term\nderived from the estimated alignment. Consistent with prior work, such as Arivazhagan et al. (2019); Ma et al.\n(2019b), the expected delays \u00afD = \u00afd1, ..., \u00afd|Y| are estimated from the expected alignment \u03b1 during training\ntime. The expected delay of target token yi, denoted as \u00afdi, is computed as\n\u00afdi = E[j|i] =\n|X|\nX\nk=1\nk\u03b1i,k\n(11)\nGiven a latency metric C, the loss term is then computed as\nLlatency = C( \u00afD)\n(12)\nThe variance of the alignment characterizes the certainty of an estimation. It is noteworthy that an alignment\nestimation can be low latency but high variance. For instance, a random walk policy, which yields a monotonic\nalignment of linear latency, has a huge variance on the estimation. Arivazhagan et al. (2019) proposed a\nmethod to reduce the uncertainty by introducing a Gaussian noise to the input of stepwise probability network.\nNevertheless, empirical results show that this method is not efficient, especially when applied to speech\ntranslation models. Therefore, we propose an alternative regularization-based strategy.\nDenote the V = \u00afv1, ..., \u00afv|Y| as the expected variances of the monotonic alignment. The expected variance of\ntarget token yi, denoted as \u00afvi, can be expressed as\n\u00afvi = E[(j \u2212 E[j|i])2|i] = E[j2|i] \u2212 E[j|i]2 =\n|X|\nX\nk=1\nk2\u03b1i,k \u2212\n\uf8eb\n\uf8ed\n|X|\nX\nk=1\nk\u03b1i,k\n\uf8f6\n\uf8f8\n2\n(13)\nWe then introduce the alignment variance loss as the following:\nLvariance =\n|Y|\nX\ni=1\n\u00afvi\n(14)\n4\nTo further reduce the alignment variance, we proposed an enhanced stepwise probability network as\npi,j = Sigmoid\n\u0012FFNs(si\u22121)T FFNh(hj) + b\n\u03c4\n\u0013\n(15)\nFFNs and FFNs serve as energy projections, constructed using multi-layer feedforward networks, which increase\nthe expressive capability of stepwise probability network over linear projection adopted by prior monotonic\nwork. b is a learnable bias, initialized by a negative value. Its purpose is to schedule an easier the policy\noptimization process from the offline policy. \u03c4 is the temperature factor, to encourage polarized output from\nstepwise probability network.\nFinally, we optimize the model with the following objective\nL(\u03b8) = \u2212log(Y|X) + \u03bblatencyLlatency + \u03bbvarianceLvariance\n(16)\nwhere \u03bblatency and \u03bbvariance are the loss weights.\n3.3 Simultaneous Fine-tuning\nIn most prior work on simultaneous translation, the model is usually trained from scratch. However, this\napproach often requires substantial resources when dealing with extensive or multilingual scenarios. For\ninstance, it can be a significant challenge to retrain a simultaneous model with the configuration from recent\nlarge-scale multilingual models, such as Whisper or SeamlessM4T. To leverage the recent advancements\nachieved with large foundational translation models and enhance the adaptability of the simultaneous\ntranslation model, we introduce a method for Simultaneous Fine-tuning.\nDenote the an arbitrary offline encoder-decoder translation model as M(\u03b8o\ne, \u03b8o\nd), with \u03b8e representing the\nencoder parameters and \u03b8d representing the decoder parameters. The simultaneous model is denoted as\nM(\u03b8e, \u03b8d, \u03b8p), where \u03b8p denotes the policy network. Simultaneous fine-tuning involves initializing \u03b8e with \u03b8o\ne\nand \u03b8d with \u03b8o\nd. During the training process, the encoder parameters \u03b8e remain fixed, and optimization is only\nperformed on \u03b8d and \u03b8p. This design is motivated by the assumption that the generative components of the\nmodel, namely \u03b8e and \u03b8d, should closely resemble those of the offline model. In simultaneous setting, they are\nadapted to partial contextual information.\n3.3.1 Streaming Inference\nWe used SimulEval (Ma et al., 2020a) to build the inference pipeine. The overall inference algorithm is\nillustrated in Algorithm 1. For streaming speech input, we update the whole encoder every time a new speech\nchunk is received by the model. Then, we run the decoder to generate a partial text translation based on the\npolicy.\n4. Experimental Setup\nWe evaluate proposed models on speech-to-text translation task.\nThe models are evalauted with the\nSimulEval (Ma et al., 2020a) toolkit. Evaluation of the models focuses on two factors: quality and la-\ntency. The quality is measured by detokenized BLEU, using the SacreBLEU (Post, 2018) toolkit. Latency\nevaluation is measured by Average Lagging (AL) (Ma et al., 2019a) We follow the simultaneous fine-tuning\nstrategy introduced in Section 3.3. The simultaneous model is initialized from an offline translation model.\nDetailed information regarding the tasks, evaluation datasets employed in this study, and the performance of\nthe offline model are presented in Table 2\nFor speech-to-text (S2T) translation task, we establish two experimental configurations: bilingual and\nmultilingual.\nThe bilingual setup aims to demonstrate the model\u2019s potential when provided with a extensive corpus\nof training data. We trained one model for each direction, spa-eng and eng-spa. The multilingual task\n1Average on 100 language directions\n5\nAlgorithm 1 EMMA Inference\nInput: X: Input streaming speech.\nInput: Y: Output text.\nInput: tEMMA : Decision threshold for EMMA\n1: i \u2190 1, j \u2190 0, k \u2190 0\n2: s0 \u2190 TextDecoder(y0)\n3: while yi\u22121 \u0338= EndOfSequence do\n4:\nj \u2190 j + 1\n5:\nh\u2264j \u2190 SpeechEncoder(X\u2264j)\n6:\nwhile yi\u22121 \u0338= EndOfSequence do\n7:\np \u2190 1\n8:\nfor StepwiseProbabilty in all attention head do\n9:\np \u2190 min(p, StepwiseProbabilty(hj, si\u22121))\n10:\nif p < tEMMA then\n11:\nBreak\n12:\nelse\n13:\nyi, si \u2190 TextDecoder(s<i, h\u2264j)\n14:\nk \u2190 k + 1\n15:\ni \u2190 i + 1\nTask\nEvaluation Set\nLanguage\nBLEU\nBilingual\nmTedX\nspa-eng\n37.1\nMust-C\neng-spa\n38.1\nMultilingual\nFleurs\n100-eng\n28.8 1\nTable 2 - Offline model performance\ndemonstrates the model\u2019s capacity for rapid adaptation in offline-to-simultaneous transition, from an existing\nlarge scale multilingual translation model, SeamlessM4T (Seamless Communication et al., 2023).\nIn the bilingual setting, we follow the data setting from Inaguma et al. (2023). In the multilingual setting, we\nuse the speech-to-text data from labeled and pseudo-labeled data in Seamless Communication et al. (2023)\nIn the bilingual S2T setup, we initialize the offline model with a pre-trained wav2vec 2.0 encoder (Baevski\net al., 2020) and mBART decoder (Liu et al., 2020). Subsequently, we initialize the simultaneous model based\non this pre-trained offline model. The bilingual model is trained on supervised on semi-unsupervised data. In\nthe multilingual setting, we initialize the simultaneous model with the S2T part of an offline SeamlessM4T\nmodel, trained with the same labeled and pseudo-labeled data, and evaluate the model on 100-eng directions.\n5. Related Work\nRecent research has focused on the neural end-to-end approach, anticipating that a simpler system can\nreduce errors between subsystems and enhance overall efficiency in direct translation. Initially applied to\ntext translation, this approach extended to speech-to-text tasks, showing competitiveness against cascade\napproaches. Duong et al. (2016) introduced an attention-based sequence-to-sequence structure for speech-\nto-text, using a recurrent neural network (RNN) based encoder-decoder architecture. Despite the novelty,\nthere was a significant quality downgrade compared to cascaded approaches. Subsequent studies Berard et al.\n(2016); Weiss et al. (2017); Bansal et al. (2018); B\u00e9rard et al. (2018) added convolutional layers, significantly\nimproving end-to-end model performance. Leveraging the success of the Transformer in text translation\nVaswani et al. (2017), Di Gangi et al. (2019) and Inaguma et al. (2020) applied it to speech translation,\nachieving further improvements in quality and training speed.\nSimultaneous translation policies are categorized into three groups. The first category consists of predefined\n6\ncontext-free rule-based policies. Cho & Esipova (2016) proposed a Wait-If-* policy for offline simultaneous\ndecoding, later modified by Dalvi et al. (2018) for consecutive prediction. Another variation, the Wait-k policy,\nwas introduced by Ma et al. (2019a), where the model alternates between reading k inputs and performing\nread-write operations. The second category involves a learnable flexible policy with an agent, applying\nreinforcement learning. Examples include Grissom II et al. (2014), who used a Markov chain-based agent\nfor phrase-based machine translation, and Gu et al. (2017), who introduced an agent that learns translation\ndecisions from interaction with a pre-trained neural machine translation model. The third category features\nmodels using monotonic attention, replacing Softmax attention and leveraging closed-form expected attention.\nNotable works include Raffel et al. (2017), Chiu & Raffel (2018), Arivazhagan et al. (2019), and Ma et al.\n(2019b), demonstrating advancements in online linear time decoding and translation quality improvements.\n6. Results\n6.1 Quality-Latency Trade-off\nWe present the quality-latency trade-off on the bilingual setting. Figure 1 shows the BLEU score under\ndifferent latency settings. We can see that the EMMA model significantly outperforms the Wait-k model on\nall the latency regions in both directions.\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\nAverage Lagging\n34.0\n34.5\n35.0\n35.5\n36.0\n36.5\nBLEU\nWait-K\nEMMA\n(a)\nSpanish to English\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nAverage Lagging\n30\n32\n34\n36\nBLEU\nWait-K\nEMMA\n(b)\nEnglish to Spanish\nFigure 1 - The quality-latency trade-off for bilingual model.\nWe present the quality-latency trade-off on the multilingual setting, in Table 3. SeamlessM4T-EMMA can\nachieve decent translation quality in a much shorter training time compared with training from scratch.\ntEMMA\nBLEU\nAL\nSeamlessM4T\n28.8\n-\nSeamlessM4T-EMMA\n0.5\n26.0\n1.75\n0.7\n26.4\n1.88\n0.4\n25.9\n1.68\n0.6\n26.2\n1.81\nTable 3 - Average translation quality and latency under multilingual setting with different latency decision thresholds\ntEMMA.\n7\nFigure 2 - Visualization of monotonic alignment. Left side is stepwise probability p and right side is estimated\nalignment \u03b1.\n6.2 Visualization\nWe also visualize the policy learned from EMMA as Figure 2. We can see that with the guidance of latency\nand variance regularization, the model can learn a monotonic policy aligning input speech and output text.\n7. Conclusion\nWe propose Efficient Monotonic Multihead Attention (EMMA), with a novel numerically stable alignment\nestimation. We fine-tune our model initialized from an offline model to speed up training and improve the\nperformance. We evaluate our model in bilingual and multilingual settings, and observe improvement over the\nbaseline in both.\nReferences\nNaveen Arivazhagan, Colin Cherry, Wolfgang Macherey, Chung-Cheng Chiu, Semih Yavuz, Ruoming Pang, Wei Li,\nand Colin Raffel. Monotonic Infinite Lookback Attention for Simultaneous Machine Translation. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics, pp. 1313\u20131323, Florence, Italy, July 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/P19-1126. URL https://www.aclweb.org/anthology/\nP19-1126.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.\nwav2vec 2.0: A Framework for Self-\nSupervised Learning of Speech Representations.\nIn Advances in Neural Information Processing Systems, vol-\nume 33, pp. 12449\u201312460. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html.\nSameer Bansal, Herman Kamper, Karen Livescu, Adam Lopez, and Sharon Goldwater. Low-Resource Speech-to-Text\nTranslation. In Interspeech 2018, pp. 1298\u20131302. ISCA, September 2018. doi: 10.21437/Interspeech.2018-1326. URL\nhttp://www.isca-speech.org/archive/Interspeech_2018/abstracts/1326.html.\nAlexandre Berard, Olivier Pietquin, Christophe Servan, and Laurent Besacier. Listen and Translate: A Proof of\nConcept for End-to-End Speech-to-Text Translation. NIPS Workshop on End- to-end Learning for Speech and Audio\nProcessing, December 2016. URL http://arxiv.org/abs/1612.01744. arXiv: 1612.01744.\n8\nA. B\u00e9rard, L. Besacier, A. C. Kocabiyikoglu, and O. Pietquin. End-to-End Automatic Speech Translation of Audiobooks.\nIn 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6224\u20136228, April\n2018. doi: 10.1109/ICASSP.2018.8461690. ISSN: 2379-190X.\nChung-Cheng Chiu and Colin Raffel. Monotonic Chunkwise Attention. February 2018. URL https://openreview.\nnet/forum?id=Hko85plCW.\nKyunghyun Cho and Masha Esipova. Can neural machine translation do simultaneous translation? arXiv:1606.02012\n[cs], June 2016. URL http://arxiv.org/abs/1606.02012. arXiv: 1606.02012.\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, and Stephan Vogel. Incremental Decoding and Training Methods for\nSimultaneous Translation in Neural Machine Translation. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2\n(Short Papers), pp. 493\u2013499, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:\n10.18653/v1/N18-2079. URL https://www.aclweb.org/anthology/N18-2079.\nMattia Antonino Di Gangi, Matteo Negri, Roldano Cattoni, Roberto Dessi, and Marco Turchi. Enhancing Transformer\nfor End-to-end Speech-to-Text Translation.\nIn Proceedings of Machine Translation Summit XVII Volume 1:\nResearch Track, pp. 21\u201331, Dublin, Ireland, August 2019. European Association for Machine Translation. URL\nhttps://www.aclweb.org/anthology/W19-6603.\nLong Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, and Trevor Cohn.\nAn Attentional Model\nfor Speech Translation Without Transcription. In Proceedings of the 2016 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pp. 949\u2013959, San\nDiego, California, June 2016. Association for Computational Linguistics.\ndoi: 10.18653/v1/N16-1109.\nURL\nhttps://www.aclweb.org/anthology/N16-1109.\nAlvin Grissom II, He He, Jordan Boyd-Graber, John Morgan, and Hal Daum\u00e9 III. Don\u2019t Until the Final Verb Wait:\nReinforcement Learning for Simultaneous Machine Translation. In Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pp. 1342\u20131352, Doha, Qatar, October 2014. Association for\nComputational Linguistics. doi: 10.3115/v1/D14-1140. URL https://www.aclweb.org/anthology/D14-1140.\nJiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor O.K. Li. Learning to Translate in Real-time with Neural\nMachine Translation.\nIn Proceedings of the 15th Conference of the European Chapter of the Association for\nComputational Linguistics: Volume 1, Long Papers, pp. 1053\u20131062, Valencia, Spain, April 2017. Association for\nComputational Linguistics. URL https://www.aclweb.org/anthology/E17-1099.\nHirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, and Shinji Watanabe.\nESPnet-ST: All-in-One Speech Translation Toolkit. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations, pp. 302\u2013311, Online, July 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.acl-demos.34. URL https://www.aclweb.org/anthology/2020.acl-demos.34.\nHirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann\nLee, Shinji Watanabe, and Juan Pino. UnitY: Two-pass direct speech-to-speech translation with discrete units.\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 15655\u201315680, Toronto, Canada, July 2023.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.872. URL https://aclanthology.org/\n2023.acl-long.872.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke\nZettlemoyer. Multilingual Denoising Pre-training for Neural Machine Translation. Transactions of the Association\nfor Computational Linguistics, 8:726\u2013742, 2020. doi: 10.1162/tacl_a_00343. URL https://aclanthology.org/2020.\ntacl-1.47. Place: Cambridge, MA Publisher: MIT Press.\nMingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang, Zhongjun He,\nHairong Liu, Xing Li, Hua Wu, and Haifeng Wang. STACL: Simultaneous Translation with Implicit Anticipation\nand Controllable Latency using Prefix-to-Prefix Framework. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pp. 3025\u20133036, Florence, Italy, July 2019a. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1289. URL https://www.aclweb.org/anthology/P19-1289.\nXutai Ma, Juan Miguel Pino, James Cross, Liezl Puzon, and Jiatao Gu. Monotonic Multihead Attention. September\n2019b. URL https://openreview.net/forum?id=Hyg96gBKPS.\nXutai Ma, Mohammad Javad Dousti, Changhan Wang, Jiatao Gu, and Juan Pino. SIMULEVAL: An Evaluation Toolkit\nfor Simultaneous Translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\n9\nProcessing: System Demonstrations, pp. 144\u2013150, Online, October 2020a. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-demos.19. URL https://www.aclweb.org/anthology/2020.emnlp-demos.19.\nXutai Ma, Juan Pino, and Philipp Koehn. SimulMT to SimulST: Adapting Simultaneous Text Translation to End-\nto-End Simultaneous Speech Translation. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of\nthe Association for Computational Linguistics and the 10th International Joint Conference on Natural Language\nProcessing, pp. 582\u2013587, Suzhou, China, December 2020b. Association for Computational Linguistics.\nURL\nhttps://aclanthology.org/2020.aacl-main.58.\nMatt Post. A Call for Clarity in Reporting BLEU Scores. In Proceedings of the Third Conference on Machine\nTranslation: Research Papers, pp. 186\u2013191, Brussels, Belgium, October 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/W18-6319. URL https://aclanthology.org/W18-6319.\nColin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, and Douglas Eck. Online and linear-time attention by\nenforcing monotonic alignments. In Proceedings of the 34th International Conference on Machine Learning - Volume\n70, ICML\u201917, pp. 2837\u20132846, Sydney, NSW, Australia, August 2017. JMLR.org.\nSeamless Communication, Lo\u00efc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise\nDuquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel\nLicht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula,\nPeng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ\nHowes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia\nKulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh\nRamakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai\nYu, Pierre Andrews, Can Balioglu, Marta R. Costa-juss\u00e0, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco\nGuzm\u00e1n, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah\nSaleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, and Skyler Wang. Seamlessm4t\u2014massively\nmultilingual & multimodal machine translation. ArXiv, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin.\nAttention is All you Need.\nIn Advances in Neural Information Processing Sys-\ntems, volume 30. Curran Associates, Inc., 2017.\nURL https://proceedings.neurips.cc/paper/2017/hash/\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\nRon J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. Sequence-to-Sequence Models Can\nDirectly Translate Foreign Speech.\nIn Interspeech 2017, pp. 2625\u20132629. ISCA, August 2017.\ndoi: 10.21437/\nInterspeech.2017-503. URL http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0503.html.\n10\nAppendix\nA. Numerically Stable Estimation\nIntuitively the \u03b1 can be estimated from dynamic programming:\n\u03b1i,j = pi,j\nj\nX\nk=1\n\u03b1i\u22121,k\nj\u22121\nY\nl=k\n(1 \u2212 pi,l)\n(17)\nWhile Equation (4) gives an closed form and parallel estimation of alignment, the denominator in the equation\ncan cause instability and alignment vanishing in the training. We rewrite Equation (17) as\n\u03b1i,: = pi,: \u2299 \u03b1i\u22121,:T(i)\n(18)\nwhere T(i) a transition matrix and each of its elements are defined as:\nT(i)m,n =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nQn\u22121\nl=m(1 \u2212 pi,l)\nm < n\n1\nm = n\n0\nm > n\n(19)\nT(i)m,n is the probability of a read from xm to xn with yi without write. Denote ti\nm,n = Qn\nl=m(1 \u2212 pi,l) We\ncan see that if we manage to have T(i), then the \u03b1i,: can be simply computed through matrix multiplication.\nDefine the probability of jumping from xm to xn with our write a new token yi:\nthen we can expand T(i) as\nT(i) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n1\nti\n1,2\nti\n1,3\nti\n1,4\n...\nti\n1,|X|\n0\n1\nti\n2,3\nti\n2,4\n...\nti\n2,|X|\n0\n0\n1\nti\n3,4\n...\nti\n3,|X|\n...\n...\n...\n...\n...\n0\n0\n0\n0\n...\n1\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n|X|\u00d7|X|\n(20)\nIt can be further expressed as\nT(i) = triu0\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n1\nti\n1,2\nti\n1,3\nti\n1,4\n...\nti\n1,|X|\n1\n1\nti\n2,3\nti\n2,4\n...\nti\n2,|X|\n1\n1\n1\nti\n3,4\n...\nti\n3,|X|\n...\n...\n...\n...\n...\n1\n1\n1\n1\n...\n1\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n|X|\u00d7|X|\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n(21)\n= triu0\n\u0000cumprod2(1 \u2212 Pext(i))\n\u0001\n(22)\nwhere triub (\u00b7) is function to extract the upper triangle of a matrix with an offset b 2, and cumprod2 means\nthat the computation is along the second dimension. Additionally, the extended probability matrix Pext\ni\nis\n2See torch.triu\n11\ndefined as\nPext(i) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n0\npi,1\npi,2\n...\npi,|X|\u22121\n0\n0\npi,2\n...\npi,|X|\u22121\n0\n0\n0\n...\npi,|X|\u22121\n...\n...\n...\n...\n0\n0\n0\n...\npi,|X|\u22121\n0\n0\n0\n...\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n|X|\u00d7|X|\n(23)\n= triu1\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8eb\n\uf8ec\n\uf8ed\n1\n...\n1\n\uf8f6\n\uf8f7\n\uf8f8\n|X|\u00d71\n\u0000pi,|X|\npi,1\n...\npi,|X|\u22121\n\u0001\n1\u00d7|X|\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\n(24)\n= triu1\n\u0000J|X|\u00d71roll1(pi,:)\n\u0001\n(25)\nWhere J|X|\u00d71 is an all one matrix with a size of |X| by 1, 3 and rollk is the function to shift matrix by k\nelements 4.\nIn summary, we can rewrite Equation (17) as\n\u03b1i,: = pi,: \u2299 \u03b1i,:triu0\n\u0000cumprod2(1 \u2212 triu1\n\u0000J|X|\u00d71roll1(pi,:)\n\u0001\n)\n\u0001\n(26)\nA code snippet of implementation of EMMA in PyTorch is shown as follows:\ndef monotonic_alignment(p):\nbsz, tgt_len, src_len = p.size()\n# Extension probablity matrix\np_ext = p.roll(1, [-1]).unsqueeze(-2).expand(-1, -1, src_len, -1).triu(1)\n# Transition matrix\nT = (1 - p_ext).comprod(-1).triu()\nalpha = [p[:, [0]] * T[:, [0]]\nfor i in range(1, tgt_len):\nalpha.append(p[:, [i]] * torch.bmm(alpha[i - 1], T[:, i]))\nreturn torch.cat(alpha[1:], dim=1)\n3J|X|\u00d71roll1(pi,:) can be achived by torch.expand function.\n4See torch.roll\n12\n"
  },
  {
    "title": "Generating Illustrated Instructions",
    "link": "https://arxiv.org/pdf/2312.04552.pdf",
    "upvote": "6",
    "text": "Generating Illustrated Instructions\nSachit Menon1,2*\nIshan Misra1\nRohit Girdhar1\n1GenAI, Meta\n2Columbia University\nhttps://facebookresearch.github.io/IllustratedInstructions\nAbstract\nWe introduce a new task of generating \u201cIllustrated In-\nstructions\u201d, i.e. visual instructions customized to a user\u2019s\nneeds. We identify desiderata unique to this task, and for-\nmalize it through a suite of automatic and human evalua-\ntion metrics, designed to measure the validity, consistency,\nand efficacy of the generations. We combine the power of\nlarge language models (LLMs) together with strong text-\nto-image generation diffusion models to propose a simple\napproach called StackedDiffusion, which generates such il-\nlustrated instructions given text as input.\nThe resulting\nmodel strongly outperforms baseline approaches and state-\nof-the-art multimodal LLMs; and in 30% of cases, users\neven prefer it to human-generated articles. Most notably,\nit enables various new and exciting applications far beyond\nwhat static articles on the web can provide, such as per-\nsonalized instructions complete with intermediate steps and\npictures in response to a user\u2019s individual situation.\n1. Introduction\nThe internet is a vast resource to find answers to all types\nof questions. It is often easy to find a webpage or a video\nthat walks through the exact steps to achieve a user\u2019s goal.\nWith the rise of Large Language Models (LLMs) trained on\ninternet-scale data, users can just ask the LLM for instruc-\ntions to achieve a goal. This allows the users to get answers\nfor specific personalized queries for which there may not be\nan existing webpage on the internet, e.g., modifying cook-\ning recipes with user specific dietary restrictions. Moreover,\nif the users make a mistake when following the instructions,\nsimple follow-up questions to the LLM can generate alter-\nnate instructions, a major advantage over static web search.\nIn spite of such advantages, LLMs still have one major\nlimitation \u2013 they cannot generate visuals, which are crit-\nical for users to learn from and follow instructions for a\nwide range of tasks [5]. Consider, for instance, instructions\nthat require visual inspection, e.g., a recipe that requires\n*Work done during an internship at Meta.\nStep 1: Brew strong \nco\ufb00ee; let cool.\nStep 2: Preheat the \noven to 350F. \nStep 3: Prepare cake \nbatter as usual. \nStep 4: Mix in the \nco\ufb00ee; fold in co\ufb00ee\ngrounds for texture.\nStep 5: Bake for 25-\n30 minutes.\nStep 6: Cover the \ncake in teal fondant \nfor the Seattle \ufb02ag.\nOne great way could be to make a co\ufb00ee-infused cake to honor \nSeattle\u2019s co\ufb00ee culture, with teal coloring for the Seattle \ufb02ag. \nHow can I bake a cake to celebrate an event in Seattle?\nFigure 1. StackedDiffusion generating Illustrated Instructions.\nGiven a goal (or any textual user input), StackedDiffusion pro-\nduces a customized instructional article complete with illustrations\nthat not only tells the user how to achieve the goal in words, but\nalso shows the user by providing illustrations.\nthe user to stir fry onions until golden brown, or searching\nfor bubbles when a flat tire is submerged under water. An\nimage accompanying such instructions can make following\nthem significantly easier. Can we develop methods with the\nstrengths of LLMs that can also generate such visuals?\nIn this work, we tackle this challenge, developing mod-\nels that can not only tell a user how to accomplish their task,\nbut also show them how. We define the novel task of Illus-\ntrated Instructions: creating a set of steps with visualiza-\ntions that showcase an approach to solve the user\u2019s task. We\ncarefully consider the different dimensions of the problem,\nlaying out three desiderata unique to this setting. To mea-\nsure these desiderata, we develop automated metrics based\non prior work in instructional article assessment [66] and\n1\narXiv:2312.04552v1  [cs.CV]  7 Dec 2023\nimage generation [50].\nWe propose a new model to solve the task of Illustrated\nInstructions that combines LLMs with a text-to-image dif-\nfusion model.\nWe train our model on stacks of instruc-\ntional images from websites such as WikiHow.\nThe re-\nsulting StackedDiffusion model creates full instructional\narticles, complete with customized steps and a sequence\nof images uniquely generated to describe those steps. It\nleverages large-scale pretrained LLMs and finetuned text-\nto-image diffusion models, and employs techniques to ac-\ncomplish the task without the need to introduce any new\nlearnable parameters. This includes spatial tiling for simul-\ntaneous multi-image generation, text embedding concate-\nnation to reduce information loss in long instructions, and a\nnew \u201cstep-positional encoding\u201d to better demarcate the dif-\nferent steps in the instructions. Thus, StackedDiffusion can\ngenerate useful illustrations even when trained even with\nlimited instructional data (cf. Figure 7).\nWe compare StackedDiffusion with various baseline ap-\nproaches based on off-the-shelf tools, and find that they all\nfall short, even when used in conjunction with one another.\nExisting T2I models are incapable of generating visuals di-\nrectly from a user query. Even when given more detailed\ninstructional text, we show that existing T2I models fail to\nproduce images that are simultaneously faithful to the goal,\nthe step, and consistent among each other. Given the recent\nintroduction of multimodal LLMs [1, 27], we also compare\nwith a recent open-source model, GILL [26], and show such\nmodels also fall short of consistently generating useful visu-\nals along with text. We posit that our approach of leveraging\nthe spatial priors learned by pretrained diffusion models to\ngenerate multiple images together, in conjunction with pre-\ntrained LLMs, is significantly more compute and data effi-\ncient than an approach purely based on next-token predic-\ntion, without these priors. Our thorough ablations and hu-\nman evaluations show that StackedDiffusion convincingly\nsurpasses state-of-the-art models. Our final model even out-\nperforms human-generated articles in 30% of cases, show-\ning the strong potential of our approach.\nContributions: 1) We introduce the novel task of Illus-\ntrated Instructions, which requires generating a sequence of\nimages and text that together describe how to achieve a goal\n(\u00a7 3 and \u00a7\u00a7 5.1 and 5.2), along with desiderata and metrics\nfor this task; 2) We propose a new approach StackedDif-\nfusion for Illustrated Instructions, with novel modifications\nto the modeling procedure, enabling generation of visuals\nsuitable for instructional articles for the first time without\nany additional parameters (\u00a7 4); 3) We show that our pro-\nposed method achieves strong performance on all metrics,\nand confirm that human evaluators prefer it over existing\nmethods by wide margins\u2013even surpassing ground truth im-\nages in some cases (\u00a7\u00a7 5.3 to 5.5); 4) Finally, we showcase\nnew abilities that StackedDiffusion unlocks, including per-\nsonalization, goal suggestion, and error correction, that go\nfar beyond what is possible with fixed articles (\u00a7 5.6).\n2. Related Work\nInstructional data, tasks, and methods. In the text do-\nmain, learning language models on WikiHow [29, 71], has\nled to advances in tasks such as summarization [70], com-\nmonsense procedural knowledge [71, 74], question answer-\ning [10], and hierarchical reasoning [73].\nIn particular,\nZhang et al. [71] introduce the goal inference task, in which\na model is presented a goal text as input and asked which of\n4 candidate steps is one that actually helps achieve that goal,\nas well as the analogous step inference task. In the multi-\nmodal setting, Yang et al. [66] introduce the Visual Goal-\nStep Inference (VGSI) dataset and task, which consider ar-\nticles of interleaved text and images. In this task, a model\nis again presented goal text as input but is asked which of\n4 candidate images is one that actually helps achieve that\ngoal. They show representations learned on this data aid in\ntasks related to instructional videos [38, 57].\nLearning representations from multimodal data. Multi-\nmodal data (including multimodal instructional data, such\nas from video [38, 57]) has proven to be a powerful source\nof signal for tasks such as zero-shot recognition [24, 47],\ntext-image and text-video retrieval [2, 8, 15, 16, 33, 35, 36,\n39, 59], temporal segmentation [35, 46, 56, 64, 75], activ-\nity localization [8, 32, 35, 62, 64, 76], anticipation [11, 14,\n18, 52, 60], question-answering [30, 53, 63], summariza-\ntion [40, 41], and even recipe personalization [13].\nExisting work on instructional data has centered around\nunderstanding, rather than generation. We instead focus on\nthe novel setting of generating full multimodal articles com-\nplete with text and illustrations.\nGenerative models.\nRecent work has examined text-\nconditioned visual generation through autogressive [7, 68]\nor diffusion models [3, 17, 20, 42, 48, 49, 51]. These ad-\nvances have been leveraged to create text and images to-\ngether with purely autoregressive [1, 69] or combined ap-\nproaches [26]. Works in the text-to-video setting [4, 12,\n22, 23, 37, 54, 61, 65, 72] are similar to ours in generat-\ning multiple images (frames) together, but are typically lim-\nited to short time scales, where visual content only changes\nin minor ways from frame to frame. Some recent works\naim to generate longer videos [58, 67], however at a cost of\nsubstantial parameter overhead over their base T2I models.\nStackedDiffusion, on the other hand, leverages the priors\nbuilt into the T2I model without any additional parameters.\n3. Illustrated Instructions\nWe now formalize our task and the corresponding desider-\nata. The input to the system is a goal text g. As output, we\nwould like to produce step text si as well as step illustration\n2\nto water until vibrantly colored.\nwater in the freezer for 1-5 hours.\nMatches text but not images\ntches goal but not step\nGoal: Make colored ice\nStep 1: Add food \ncoloring to water \nuntil colored.\nStep 2: Place the \nwater in the freezer\nfor 1-5 hours.\nStep 3: Once sure it \nis frozen, remove \nfrom the freezer.\nText-to-Image generations\nGoal + step \nconditioned\nStep \nconditioned\nFigure 2. Failure modes of a naive approach. A frozen T2I\nmodel is not able to capture both the goal and the step, showing\nonly one or the other depending on how it is prompted. Further, it\ncan not produce consistent images, leading to odd changes such as\nthe color of the ice varying between images.\nIi for each step i. The step text should be a natural language\ndescription of the step, while the step illustrations should be\nan image corresponding to the step.\nOn first glance, one might think that this task is a\nstraightforward amalgamation of textual instruction gener-\nation and text to image generation. While these tasks are\nclosely related and necessary substeps towards illustrated\ninstructions, they are not sufficient. The naive approach\u2013\nsimply creating step text si from goal text g, and each im-\nage Ii individually from each si\u2013fails to recognize the fresh\nnew challenges that do not exist in either of these two other\ntasks. We identify three key requirements for useful illus-\ntrated illustrations: goal faithfulness, step faithfulness, and\ncross-image consistency.\nThe first requirement is clear: if the images do not relate\nto the goal, they cannot be good illustrations. This motivates\nthe desiderata of goal faithfulness, which requires that each\nimage faithfully reflects the goal text.\nHowever, goal faithfulness alone is not enough. Con-\nsider the first row of images in Figure 2. The images all re-\nflect the ultimate goal\u2013creating colored ice\u2013but fail in their\nrole as step illustrations. This in turn motivates step faith-\nfulness, which requires that each image be faithful to the\nstep text. We see in this example that baseline T2I models\nfail dramatically along this metric; every image reflects the\ngoal rather than the specifics of the step requested.\nFinally, the generated images should be consistent with\neach other. While the second row of images in Figure 2 are\nall faithful to the step text, the color of the ice (and even the\nstyle of the images, cartoon or real) changes between im-\nages. This is jarring and confusing to the reader. This mo-\nunstack\n...\n+\n,\n(\n)\n,\nstack\n(\n)\n\u03a60 \u03a60\n\u03a60\n\u03a61 \u03a61 \u03a61\n\u03a62 \u03a62\n\u03a62\nGround Truth\nUser Input\nLLM\nTraining\nTraining\nInference\nInference\nFigure 3. Overview of StackedDiffusion. At training time, we\nuse the given goal and step text, and stack the encoded ground\ntruth step-images. At inference time, we obtain the goal and step\ntext from an LLM, and unstack denoised latents to produce the\noutput images. See \u00a7 4 for details and notation.\ntivates the final criterion of cross-image consistency, which\nrequires that each image be consistent with the other images\nproduced for a particular generation.\n4. StackedDiffusion\nWe propose a new architecture, StackedDiffusion, to over-\ncome the limitations of existing text-to-image (T2I) ap-\nproaches for the task of generating interleaved text and im-\nages for instructional tasks. StackedDiffusion builds upon\nT2I models based on latent diffusion models (LDMs) [49],\nwhich are diffusion models [48] that operate on a low-\nresolution \u2018latent\u2019 encoding of the original images. Our pri-\nmary desiderata is that the images generated must be faith-\nful to both the goal and step text. However, in initial experi-\nments we found that simply encoding the goal and step texts\njoined together (as strings) leads to an uninformative encod-\ning. For instance, generations conditioned on this combined\ntext tended to ignore certain steps or the goal. Furthermore,\nthe length of this combined text will likely exceed the con-\ntext length limitations of the text encoders [47] commonly\nused in T2I models, leading to undesired truncation and in-\nformation loss.\nHence, we elect to use a more general approach, appli-\ncable to text encoders with any context length. Rather than\nencoding the combined goal and step text to obtain the con-\ndition, we first separately encode the goal and step texts and\nthen concatenate the encodings. This allows the model to\nlearn to use the goal and step text independently, as well\nas in combination.\nWe add a \u2018step-positional encoding\u2019\n\u03a6i to this concatenation. It is broadcast across the dimen-\nsions pertaining to a particular step, to indicate to the model\nwhere a step begins and ends. \u03a60 denotes the positional\nencoding reserved to indicate the goal embedding. Hence,\ngiven a text encoder \u03c8, and N steps in a given goal, we\n3\ncompute the overall text conditioning C as\nC =\nn  \n\u03c8 (g) + \u03a60,\nNn\ni=1\n\u03c8 (si) + \u03a6i\n!\n(1)\nwhere f is the concatenation operation.\nThis is shown\nin Figure 3 (top). This design decision is critical to obtain\ngood goal and step faithfulness.\nIn addition, independently generating each image does\nnot achieve the requirement of cross-image consistency\nas the model cannot exchange information across im-\nages. Thus, we generate all the Ii images at once, which\nallows the model to jointly generate the sequence and\nachieve cross-image consistency. The remaining question,\nof course, is how to accomplish this simultaneous genera-\ntion such that it gives rise to cross-image consistency. Our\nkey observation is that T2I models already have a strong\nprior for consistency within a single image. Could we make\nuse of this previously-learned knowledge?\nWe propose a simple method to accomplish this: spa-\ntial tiling. As illustrated in Figure 3, the denoising U-Net is\ngiven latents zi corresponding to each of the output images\nsimultaneously, tiled spatially as if a single image. At train-\ning time, training images are encoded into the latent space\nas usual, then reshaped into the tiled format. In detail,\nzi = E (Ii) ,\nz =\nNn\ni=1\nzi\n(2)\nLLDM := Ez,\u03f5\u223cN (0,1),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(z(t), t, C)\u22252\n2\ni\n(3)\nwhere E is the encoder to map the image to the latent space\n(for instance, using a VAE [25]), LLDM is the training ob-\njective for the LDM, t denotes a timestep of the diffusion\nprocess, \u03f5 denotes the noise added at a given timestep, and\n\u03b8 denotes the parameters of the learned denoising U-Net.\nAt inference, we use classifier-free guidance [43] to gener-\nate a latent \u02dcz given conditioning goal and N step texts. We\nsplit the latent into \u02dcz1, ...,\u02dczN, and decode into generated\nimages using a decoder D corresponding to the encoder E,\ni.e. \u02dcIi = D (\u02dczi). We refer the reader to the Appendix \u00a7 7\nand [49] for further details on LDMs. Given the stacked\nconditioning and generation operations, we refer to our fi-\nnal model as StackedDiffusion.\nDuring training, goal and step text are obtained from\nground truth, while at inference, a pretrained LLM is used\nto transform arbitrary user input text to an inferred goal and\ngenerated step texts. We describe the LLM inference pro-\ncedure and prompt engineering in the Appendix \u00a7 8. We\ninitialize the U-Net using a pretrained T2I model, and fine-\ntune all layers when training with the stacked input. Our\nstacked conditioning only increases the spatial resolution of\nthe input to the U-Net for which can be modeled entirely us-\ning the existing parameters (spatial convolutions, attention)\n2\n4\n6\n8\n0\n0.1\n0.2\nDensity\nFigure 4. Illustrated Instructions data.\nThe histogram shows\nthe distribution of step counts in the data. We find that more than\n80% of articles consist of 6 or fewer steps.\nof the U-Net. We use the T2I model\u2019s text encoder (\u03c8), kept\nfrozen, to encode the goal and step texts.\nSince the spatially stacked latent has a spatial resolution\ncomparable to the ones used for high resolution image gen-\neration, we encounter issues observed by prior work [31] for\nhigh resolution training. Specifically, [31] finds that high\nresolution latents retain substantial information about the\ninput being noised with typical noise addition schedules,\neven at the final diffusion timestep t = T. This results in an\nunintentional distribution shift between train time (when all\nobserved inputs contain signal) and test time (when the first\ninputs are pure noise with no signal). We address this by\nadjusting the training diffusion noise schedule such that the\nsignal-to-noise ratio (SNR) at the final diffusion timestep\nT is zero [31]. This ensures enough noise is added during\ntraining so as to mitigate this difference between train and\ntest time usage.\nImplementation Details. We build upon a text-to-image la-\ntent diffusion model [49] trained on a large in-house image-\ntext dataset. It leverages a VAE [25] to map the images to\na 4D latent space, with a 8\u00d7 reduction in spatial resolution.\nThe U-Net largely follows the implementation from [49],\nwith minor architectural modifications. As conditioning, it\nuses the Flan-T5-XXL text encoder [9]. We train Stacked-\nDiffusion for 14000 steps using the AdamW optimizer [34]\nwith a learning rate of 10\u22124, weight decay of 0.01, and gra-\ndient clipping at \u21132 norm of 1.0. For classifier-free guid-\nance, we use a conditioning dropout rate of 0.05. We choose\nto generate at most N = 6 images simultaneously, because\nas we will see in \u00a7 5.1, the vast majority of the data avail-\nable has 6 or fewer steps. For shorter training sequences,\nwe pad with empty frames and dummy step texts, and for\nlonger, we drop the extra steps and images. We ablate this\nchoice of N in \u00a7 5.4. Hence in practice, for batch of 8 sets\nof 6 steps at 256px resolution, we get an encoded latent of\n8 \u00d7 6 \u00d7 4 \u00d7 32 \u00d7 32. We concatenate the latents spatially\ninto 8 \u00d7 4 \u00d7 (6 \u2217 32) \u00d7 32. The denoising U-Net is fine-\ntuned to denoise these stacked images, leveraging what it\nhas previously learned about spatial consistency, and adapts\nto this new setting. At inference time, steps are generated\nusing a pretrained LLM [44], prompted to generate at most\nN steps (\u00a7 8). To generate illustrations, noise is sampled\n4\nGoal Faithfulness\nGoal: Baking Muffins\nStep Faithfulness\nStep: Put the muffins in \nthe oven\nCross-Image Consistency\nPrevious \nImage:\nFigure 5.\nMetrics.\nWe introduce three metrics, one for each\ndesideratum presented in \u00a7 3. Goal faithfulness: the second im-\nage does not show muffins. Step faithfulness: the second image\ndoes not illustrate the step. Cross-image consistency: the second\nimage shows a different number of meatballs with different visual\nappearance.\nin the shape of the tiled latent with N steps, and the steps\ngenerated by the LLM are padded with dummy steps if less\nthan N. The resulting noise is denoised using the U-Net as\nusual, and finally reshaped to obtain the N output images\nupon decoding. See Appendix \u00a7 11 for further details.\n5. Experiments\nWe train and evaluate StackedDiffusion on web-based in-\nstructional data. We compare the generations to multiple es-\ntablished baselines and ground truth, using automatic met-\nrics and human evaluations.\nWe now describe the data,\nmetrics, baselines, and the key results. Finally, we demon-\nstrate some new applications that StackedDiffusion enables,\nshowing how it goes beyond standard instructional articles.\n5.1. Illustrated Instructions Dataset\nWe introduce a new dataset to train and evaluate models for\nthe Illustrated Instructions task, by repurposing the Visual\nGoal-Step Inference (VGSI) dataset [66]. VGSI consists of\nWikiHow articles, each of which have a high-level goal, 6-\n10 natural language steps (see Figure 4), and associated im-\nage illustrations. We observe that the same data can be used\nfor generating instructional articles. It can provide signal\nfor how the goal and step texts should map to output im-\nages, and how images should match with each other.\nFor evaluation, we construct a held-out set from this\nsame data.\nUsed directly, however, the data is not well\nHuman (\u2191) GF (\u2191) SF (\u2191) CIC (\u2193) FID (\u2193)\nT2I (Frozen)\n22.0\n92.9\n43.2\n51.3\n69.3\nT2I (Finetuned)\n33.3\n78.8\n52.4\n51.5\n53.5\nStackedDiffusion\n(ref)\n74.3\n61.5\n50.7\n39.5\nGround Truth\n82.5\n81.7\n73.7\n50.6\n(N/A)\nTable 1. Comparison to baselines. Human evaluation is reported\nas win rate vs our full StackedDiffusion model. GF corresponds to\ngoal faithfulness accuracy, SF to step faithfulness accuracy, CIC to\ncross-image consistency, and FID to Fr\u00b4echet Inception Distance.\nsuited as many of the tasks are too high-level to be use-\nful for consistent illustration. For instance, \u201cHow to Start\na Business in North Carolina\u201d may have steps \u201cBrainstorm\nideas\u201d and \u201cGo to the courthouse\u201d that have no shared vi-\nsual content. As such, we filter the data for evaluation to the\n\u201cRecipes\u201d category, which always has a visually clear end\nstate and where illustrations must be consistent with each\nother. Please see Appendix \u00a7 9 for more details.\n5.2. Metrics\nHaving established three desiderata in \u00a7 3, we now turn to\nthe question of how to evaluate them. In addition to evaluat-\ning the quality of images using FID [21], we propose three\nmetrics, one for each desideratum, that can be used to eval-\nuate the faithfulness and consistency of the generated arti-\ncle. Finally, given the limitations of automatic metrics for\nevaluating generative modeling tasks [55], we use human\nevaluations as our primary metric for overall quality. We\nillustrate the metrics in Figure 5 and briefly describe them\nnext. Please see the Appendix \u00a7 10 for more details.\nGoal Faithfulness (GF) measures how well the generated\nimage is associated to the goal text. We evaluate this by con-\nstructing multiple-choice questions (MCQ) as in VGSI [66].\nFor each generated image, we compare its CLIP similar-\nity [47] with the correct goal text vs the similarity with the\ntexts of three other randomly selected goals. We compute\nthe accuracy of the model in choosing the correct goal text.\nStep Faithfulness (SF) measures how faithfully the gener-\nated image illustrates the step it aims to depict. An image\nshould match the text for the step it was made for, more\nthan other steps. We measure this using CLIP similarity and\na MCQ task similar to goal faithfulness, where the image\nshould have higher CLIP similarity with the corresponding\nstep text than the other step texts within the same goal.\nCross-Image Consistency (CIC) evaluates how consistent\nthe generated images for a goal are with each other and pe-\nnalizes jarring inconsistencies across the images, such as\nobjects changing color or number. For instance, if a particu-\nlar set of ingredients are shown for \u201cgather the ingredients,\u201d\nthen the same ingredients should be shown for \u201cmix the dry\ningredients.\u201d We measure this by computing the average \u21132\n5\nStep 1: Place \nthe wings into \nthe oven for 1 \nhour.\nStep 2: Mix \nsoup base \npowder, milk, \nand water.\nStep 3: \nRemove \nwings from \noven; coat \nwith mixture.\nStep 4: Place \nback in the \noven, \nuncovered.\nStep 5: Leave \nin oven until \nbrown and \ntender.\nStep 6: \nFinished.\nGoal: How to Bake Turkey Wings\n: \n: \nOurs\nT2I (Finetune)\nT2I (Frozen)\nFigure 6. Baseline comparison. StackedDiffusion images are pre-\nferred overwhelmingly over baselines. The frozen baseline tends\nto only produce images showing the goal, while the finetuned\nbaseline produces images that are more faithful to the step text,\nbut have no visual features in common.\ndistance between DINO [6] embeddings of the images for\neach step, as this considers the similarity of visual (rather\nthan purely semantic) features. Like prior work [50], we\nalso found that CLIP features do not work well for image\nsimilarity as they are invariant to critical aspects such as\ncolor, number of objects, style etc. DINO\u2019s self-supervised\npretraining objective leads to features that are sensitive to\nthe visual aspects particular to an image, rather than being\ninvariant to aspects not captured by category or language.\nHuman Evaluation is finally used to confirm that the auto-\nmated evaluation corresponds to actual human preferences.\nWe provide fully rendered articles from each model to hu-\nman evaluators on Amazon Mechanical Turk (AMT), and\nask them to choose which article they prefer, or if they are\ntied. We then consider the win rate as the proportion for\nwhich a majority of evaluators selected the given method\ncompared to the articles produced by StackedDiffusion. A\nwin rate of 50 denotes the methods are tied, whereas < 50\ndenotes the method is worse than StackedDiffusion.\n5.3. Baselines\nWe now describe some baseline approaches based on exist-\ning state-of-the-art image generation models, and compare\nthem to StackedDiffusion in Table 1.\nT2I (frozen): The most obvious choice for a baseline is\nto simply use a pretrained and frozen text-to-image (T2I)\nmodel. We use our in-house T2I model that is also used\nto initialize StackedDiffusion.\nTo ensure this baseline\ngets same information as StackedDiffusion, we prompt the\nmodel with a concatenation of the goal with each step,\ng f si, for each i, and produce a single image Ii. The se-\nquence then is composed of N independent generations.\nWe find that these images are faithful to the goal text,\nGF\nSF\nCIC\n50\n60\n70\nScore\n20%\n40%\n60%\n80%\n100% data\nFigure 7. Effect of training data. We find that having more train-\ning data improves StackedDiffusion\u2019s faithfulness and consistency.\nHowever even with 20% data it performs well, thanks to its effec-\ntive use of pretrained T2I model weights.\nbut fail to be faithful to the step text and lack consistency.\nThe goal faithfulness is substantially higher than even the\nground truth (92.9% vs 81.7%). This is because the model\nis not capable of reasoning about what a particular step to-\nwards a goal should look like. Instead, it simply generates\nan image that matches words in the input text, as illustrated\nin the first row of Figure 2. This results in all the images\nfor e.g. \u201cMake Colored Ice\u201d showing the actual finished ice\nrather than any of the intermediate steps. In other words,\nthe step text is eclipsed by the goal text.\nT2I (finetuned): We finetune a T2I model with the goal\nand step text embeddings concatenated together as the con-\ndition, again producing N independent generations.\nWe\nfind that this model is more faithful to the step text than the\nfrozen T2I model, but still substantially less than our final\nmodel (52.4% vs 61.5%). This suggests the goal still over-\nshadows the step text, but to a lesser extent. The goal faith-\nfulness is also lower than for the frozen T2I model, as this\nmodel is trained to create images for steps rather than goals.\nThe cross-image consistency is low, similar to the frozen\nmodel, as the images are still generated independently.\nStackedDiffusion achieves substantially improved step\nfaithfulness and cross-image consistency, while maintain-\ning high goal faithfulness. We show an illustrative example\nin Figure 6. Unlike the baselines, the cross-image consis-\ntency is very close to that of the ground truth data; as gen-\nerating them together allows for influence between them\nduring the generation process. We also find that the FID\nof our generated images is substantially lower than that of\nother models, indicating that our generated images are more\nsimilar to the ground truth data. Finally on human evalua-\ntions, StackedDiffusion clearly outperforms them both by a\npreference of 78% and 66.6% respectively. Perhaps most\nnotably, when compared to the ground truth images, human\nevaluators still picked StackedDiffusion 18.5% of the times,\nsuggesting that our model generates some illustrations even\nbetter than the ground truth data.\n6\nWin Rate\nWin Rate\nT2I (Frozen)\n33.8\nT2I (Finetuned)\n34.0\nGILL [26]++\n3.9\nLLM + CLIP\n15.6\nGoal Retrieval\n33.1\nGround Truth\n70.0\nTable 2. System-level comparison to prior work. Human eval-\nuation results using fully generated outputs. Reported as human\nevaluation win rate of the associated method over StackedDiffu-\nsion (hence, higher is better for the reported method).\n5.4. Ablations\nStep Count. As discussed in \u00a7 4, we elect to use a max-\nimum step count of 6 as it allows sufficient data coverage\nand most articles fall under this value. We find that human\nevaluators prefer this model to the shorter step count model\nof length 4. It is slightly preferred over the model trained\nwith a longer step count of 8, likely as there is insufficient\ndata of long lengths, limiting any advantage that might be\ngained in longer length generations.\nBaseline\n4 Step (\u2191)\n8 Step (\u2191)\n6 Step\n32.1\n46.9\nTraining data. In Figure 7, we randomly sub-sample vary-\ning proportions of the total data, and train StackedDiffusion\non each of those subsets. As the amount of training data\nincreases, it generates more faithful and consistent images.\nHowever even with less data it performs well, owing to its\neffective use of the pretrained T2I model\u2019s initialization.\nImportance of 0SNR. We experiment with the 0SNR tech-\nnique [31] due to the high spatial dimensionality of the la-\ntents. We find that not using 0SNR results in substantially\nreduced step faithfulness, dropping from 61.5% to 52.8%.\nImportance of Step-Positional Encoding. We also evalu-\nate the importance of the step-positional encoding, \u03a6i. This\ngives the model a critical cue for which parts of the con-\ndition correspond to which steps. We find that not using\nthe step-positional encoding results in substantially reduced\nstep faithfulness as well, dropping from 61.5% to 49.8%.\n5.5. Comparison to prior work\nThe previous metrics were computed with respect to fixed\nground truth goal and step texts to provide direct compar-\nisons. However, they do not encompass the full scope of\nour model\u2019s capabilities: the flexibility of the generated text\nis one of StackedDiffusion\u2019s core strengths. To evaluate in a\ncloser setting to this real-world usage, we examine the qual-\nity of the articles generated by the full system, where the\nsteps are generated with a LLM [44]. We perform system-\nlevel comparisons using human evaluators, including the\nquality of the generated text and the quality of the images\ncreated from the generated text.\nStep 1: Drain and \npress tofu to remove \nexcess  water.\nStep 2: Crumble tofu; \nmix with bread crumbs \nand a binding agent.\nStep 3: Shape tofu \nmixture into burger \npatties.\nThere are plenty of tasty vegan burger patty options you could use. Here\u2019s a recipe for \none great option: vegan tofu patties.\nMy vegan sister is visiting right now. What can I put in a burger for her that she might like \ninstead of a beef patty?\nStep 3: Cook over \nmedium heat until \ngolden brown. \nFigure 8. Personalization. StackedDiffusion enables instructions\npersonalized far beyond what is possible with preexisting articles.\nWe show our results in Table 2. We first compare to the\nbaselines introduced in \u00a7 5.3, and see results similar to Ta-\nble 1. We see that even with LLM-generated text, Stacked-\nDiffusion outperforms baseline T2I based approaches, ei-\nther frozen or finetuned, by more than 66% win-rate.\nAdditionally, we compare StackedDiffusion to recent\nstate-of-the-art approaches in multimodal generation, as in\nprinciple those can also produce instructional text combined\nwith illustrations. Specifically, we compare to GILL [28]\u2013\na model trained to generate sequences of interleaved text\nand images. While other similar approaches have been pro-\nposed [1, 27], GILL is the best open-source model we could\naccess. Using GILL directly, however, results in a human\nevaluation win rate of 0%. We posit this was for three pri-\nmary reasons. First, the GILL model was unable to generate\nlong enough generations to suffice as an article in compar-\nison to our generations, despite our best efforts in modify-\ning the generation parameters. Secondly, the model often\ndid not produce any illustrations with the text, although we\ntried various prompting tactics. Thirdly, the text or images\nproduced were often not of high enough quality to actually\nreflect the goal or steps. We thus introduce an alternative,\nwhich we call GILL++. This uses the (superior) text pro-\nduced by the same language model as in our method, but\npasses each step text as input to GILL to generate the cor-\nresponding image. This results in a human evaluation win\nrate of 3.9% against StackedDiffusion. See Appendix \u00a7 12\nfor details on prompting GILL, and our GILL++ baseline.\nNext, we compare StackedDiffusion to a retrieval based\napproach, denoted by LLM+CLIP. Here we retrieve the\nclosest image in the training set according to CLIP simi-\nlarity to each of the steps created by the LLM (with each\nconcatenated to the goal text, similar to how the frozen\nT2I model is used). This retrieval-based metric is not suit-\nable for the automated metrics previously introduced as it\nis based on the same underlying similarity scores as the au-\ntomated metrics, but we introduce it here for human eval-\nuation. We find that StackedDiffusion is preferred over-\nwhelmingly over this retrieval-based approach, despite the\n7\nStep 1: Preheat oven \nto 350\u00b0F (175\u00b0C).\nStep 2: Peel, core, \nand slice apples; \nplace in a baking dish.\nStep 3: Mix sugar, \ncinnamon, and a pinch \nof salt; sprinkle over \napples.\nStep 4: Combine oats, \nflour, brown sugar, and \nbutter; crumble over \napples.\nStep 5: Bake for 45 \nminutes or until golden \nbrown.\nStep 6: Serve warm.\nIf you have a lot of apples, you can turn them into a delicious \napple crisp. Here's a simple recipe to get you started:\nI have a bunch of apples left from apple picking, what can I do \nwith them?\nFigure 9. Goal suggestion and inference. StackedDiffusion can\nsuggest what goal might be most relevant given other information.\nretrieval-based approach using real images from the train-\ning set. Similarly, we compare to another retrieval-based\napproach which retrieves the full article with the most sim-\nilar goal text to the input goal, denoted Goal Retrieval.\nWe find StackedDiffusion strongly outperforms this as well.\nThese results together suggest that StackedDiffusion is able\nto generate novel instructional articles that one can not sim-\nply retrieve from the training corpus.\nFinally, we compare StackedDiffusion to the ground\ntruth. Surprisingly again, we find that the human annotators\npick our generations 30% of times even compared to ground\ntruth. Note that these articles are handwritten and manually\nillustrated for the purpose of illustrating these goals. This\nshows the strong promise of our proposed approach. We be-\nlieve with the rapid improvements in training data and gen-\nerative modeling techniques, future iterations of our model\ncould be indistinguishable or even better than manually cre-\nated illustrated instructional articles.\n5.6. Applications\nPerhaps the biggest strength of a generative approach is its\nability to handle unique user queries that may not fit into the\nboundaries posed by static articles on the web. Hence, we\ndemonstrate the capabilities of the full system, combining\nthe strengths of text generation capabilities from the lan-\nguage model with StackedDiffusion.\nStep 1: Determine the \namount of salt \nneeded.\nStep 2: Dissolve the \nsalt in a small amount \nof warm water.\nStep 3: Flatten the \ndough into a rectangle.\nStep 4: Spread the \nsalt over the dough.\nStep 5: Fold and \nknead the dough to \ndistribute.\nStep 6: Proceed with \nthe original bread \nrecipe.\nIf you\u2019re at the kneading step, don't worry\u2014you can still \nincorporate it into the dough with more kneading. Here\u2019s how:\nOh no! I\u2019m making bread and forgot to add salt when I made the \ndough! I\u2019m at the kneading step.\nFigure 10. Error correction. StackedDiffusion provides updated\ninstructions in response to unexpected situations, like a user error.\nPersonalized instruction. A user can provide any situa-\ntional information specific to their circumstances and obtain\nan article customized to that situation. This is not possible\nwith fixed, existing articles. For example, in Figure 8, the\nuser can specify a diet and obtain an article for the food they\nwant that adheres to that diet.\nGoal suggestion. As StackedDiffusion accepts flexible in-\nput text, a user can provide higher-level information about\nwhat they want to do and obtain an article with a suggested\nspecific goal that matches what the user would like to do.\nIn Figure 9, the user describes their situation (having many\napples) and StackedDiffusion suggests a goal that matches\ntheir situation (making apple crisps).\nError correction. Oftentimes, a user in the course of per-\nforming a task may make a mistake. Fixed articles provide\nno avenue for recourse.\nHowever, StackedDiffusion can\nadapt to user error and create alternative instructions that\nbest correct for and accomodate the user\u2019s situation. See\nFigure 10 for an example of this. A fixed article would not\nprovide any alternative paths that adapt as the user performs\nthe action. This highlights the new avenues opened up by\nour generative approach to this task.\n6. Limitations and Conclusions\nWhile StackedDiffusion achieves strong performance in\ngenerating Illustrated Instructions, we note that there still\n8\nremains a gap with ground truth data. Future work might\nexamine how to obtain instructional data at greater scale\nand how this influences StackedDiffusion. Leveraging the\nlatest T2I architectures to improve the faithfulness and qual-\nity of our generations would be another promising avenue\nfor future work. Finally, leveraging improvements in text-\nto-video techniques to generate video clips describing each\nstep would further improve the usability of such a system.\nAcknowledgements: We thank the following people for their help\nand support: Andrew Brown and Saketh Rambhatla for help with\nhuman evals; Mannat Singh and Yuval Kirstain for help with com-\npute infrastructure; Triantafyllos Afouras, Efi Mavroudi, Tushar\nNagarajan, Huiyu Wang and Yale Song, for discussion and feed-\nback about instructional data; Jack Urbanek for assistance with\ndata privacy considerations; Armen Aghajanyan for discussion\nabout multimodal LLMs; Uriel Singer for assistance with the base\nT2I model and codebase; Xudong Wang, D\u00b4\u0131dac Sur\u00b4\u0131s, Basile van\nHoorick for discussion of T2I models; and the GenAI and FAIR\nteams at Meta for many helpful discussions.\nReferences\n[1] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir\nKarpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Man-\ndar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer.\nCM3: A causal masked multimodal model of the internet.\narXiv preprint arXiv:2201.07520, 2022.\n[2] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, 2021.\n[3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022.\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your Latents: High-Resolution Video Synthesis with\nLatent Diffusion Models. In CVPR, 2023.\n[5] Russell N. Carney and Joel R. Levin. Pictorial illustrations\nstill improve students\u2019 learning from text. Educational Psy-\nchology Review, 14(1):5\u201326, 2002.\n[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning Properties in Self-Supervised Vision Transformers. In\nICCV, 2021.\n[7] Huiwen\nChang,\nHan\nZhang,\nJarred\nBarber,\nAaron\nMaschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,\nKevin Patrick Murphy, William T. Freeman, Michael Ru-\nbinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-To-\nImage Generation via Masked Generative Transformers. In\nICML, 2023.\n[8] Brian Chen, Andrew Rouditchenko, Kevin Duarte, Hilde\nKuehne, Samuel Thomas, Angie Boggust, Rameswar Panda,\nBrian Kingsbury, Rogerio Feris, David Harwath, et al. Multi-\nmodal clustering networks for self-supervised learning from\nunlabeled videos. In ICCV, 2021.\n[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Webson,\nShixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun\nChen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pel-\nlat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav\nMishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew\nDai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob\nDevlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason\nWei. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022.\n[10] Yang Deng, Wai Lam, Yuexiang Xie, Daoyuan Chen,\nYaliang Li, Min Yang, and Ying Shen. Joint Learning of\nAnswer Selection and Answer Summary Generation in Com-\nmunity Question Answering. In AAAI, 2020.\n[11] Eadom Dessalene, Chinmaya Devaraj, Michael Maynord,\nCornelia Fermuller, and Yiannis Aloimonos. Forecasting ac-\ntion through contact representations from first person video.\nPAMI, 2021.\n[12] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn ICCV, 2023.\n[13] Bahare Fatemi, Quentin Duval, Rohit Girdhar, Michal\nDrozdzal, and Adriana Romero-Soriano. Learning to substi-\ntute ingredients in recipes. arXiv preprint arXiv:2302.07960,\n2023.\n[14] Antonino Furnari and Giovanni Maria Farinella.\nRolling-\nunrolling lstms for action anticipation from first-person\nvideo. PAMI, 2020.\n[15] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia\nSchmid.\nMulti-modal transformer for video retrieval.\nIn\nECCV, 2020.\n[16] Valentin Gabeur, Arsha Nagrani, Chen Sun, Karteek Alahari,\nand Cordelia Schmid. Masking modalities for cross-modal\nvideo retrieval. In WACV, 2022.\n[17] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-\nbased text-to-image generation with human priors. In ECCV,\n2022.\n[18] Rohit Girdhar and Kristen Grauman.\nAnticipative video\ntransformer. In ICCV, 2021.\n[19] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Du-\nval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi\nYin, Devi Parikh, and Ishan Misra. Emu video: Factoriz-\ning text-to-video generation by explicit image conditioning.\narXiv preprint arXiv:2311.10709, 2023.\n[20] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nCVPR, 2022.\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In NeurIPS, 2017.\n[22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben\n9\nPoole, Mohammad Norouzi, David J. Fleet, and Tim Sal-\nimans.\nImagen Video: High Definition Video Generation\nwith Diffusion Models. arXiv preprint arXiv:2210.02303,\n2022.\n[23] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022.\n[24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. In ICML, 2021.\n[25] Diederik P. Kingma and Max Welling. Auto-Encoding Vari-\national Bayes. In ICLR, 2014.\n[26] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.\nGenerating images with multimodal language models.\nIn\nNeurIPS, 2023.\n[27] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding language models to images for multimodal inputs\nand outputs. In ICML, 2023.\n[28] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\nGrounding Language Models to Images for Multimodal\nGeneration. In NeurIPS, 2023.\n[29] Mahnaz Koupaee and William Yang Wang. WikiHow: A\nLarge Scale Text Summarization Dataset.\narXiv preprint\narXiv:1810.09305, 2018.\n[30] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu,\nand Jingjing Liu.\nHero: Hierarchical encoder for video+\nlanguage omni-representation pre-training.\narXiv preprint\narXiv:2005.00200, 2020.\n[31] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang.\nCommon diffusion noise schedules and sample steps are\nflawed. arXiv preprint arXiv:2305.08891, 2023.\n[32] Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus\nRohrbach, Shih-Fu Chang, and Lorenzo Torresani. Learn-\ning To Recognize Procedural Activities with Distant Super-\nvision. In CVPR, 2022.\n[33] Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui\nDing, and Zhongyuan Wang. Hit: Hierarchical transformer\nwith momentum contrast for video-text retrieval. In ICCV,\n2021.\n[34] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay\nRegularization. In ICLR, 2019.\n[35] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan\nDuan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou.\nUnivl: A unified video and language pre-training model for\nmultimodal understanding and generation.\narXiv preprint\narXiv:2002.06353, 2020.\n[36] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,\nNan Duan, and Tianrui Li. Clip4clip: An empirical study\nof clip for end to end video clip retrieval. arXiv preprint\narXiv:2104.08860, 2021.\n[37] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie-\nniu Tan.\nVideofusion: Decomposed diffusion models for\nhigh-quality video generation. In CVPR, 2023.\n[38] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand\nTapaswi,\nIvan\nLaptev,\nand\nJosef\nSivic.\nHowTo100M:\nLearning\na\nText-Video\nEmbedding\nby\nWatching Hundred Million Narrated Video Clips. In ICCV,\n2019.\n[39] Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef\nSivic, and Andrew Zisserman. Thinking fast and slow: Ef-\nficient text-to-visual retrieval with transformers. In CVPR,\n2021.\n[40] Medhini Narasimhan, Anna Rohrbach, and Trevor Darrell.\nClip-it! language-guided video summarization. NIPS, 2021.\n[41] Medhini Narasimhan, Arsha Nagrani, Chen Sun, Michael\nRubinstein, Trevor Darrell, Anna Rohrbach, and Cordelia\nSchmid. Tl; dw? summarizing instructional videos with task\nrelevance and cross-modal saliency. In ECCV, 2022.\n[42] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021.\n[43] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. In ICML,\n2022.\n[44] OpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023.\n[45] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu.\nOn\naliased resizing and surprising subtleties in gan evaluation.\nIn CVPR, 2022.\n[46] AJ Piergiovanni, Anelia Angelova, Michael S Ryoo, and Ir-\nfan Essa. Unsupervised action segmentation for instructional\nvideos. arXiv preprint arXiv:2106.03738, 2021.\n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In ICML, 2021.\n[48] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022.\n[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bjorn Ommer.\nHigh-Resolution Image\nSynthesis with Latent Diffusion Models. In CVPR, 2022.\n[50] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\nTuning Text-to-Image Diffusion Models for Subject-Driven\nGeneration. In CVPR, 2023.\n[51] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi.\nPhotorealistic Text-to-\nImage Diffusion Models with Deep Language Understand-\ning. In NIPS, 2022.\n[52] Fadime Sener and Angela Yao. Zero-shot anticipation for\ninstructional activities. In ICCV, 2019.\n10\n[53] Paul Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid.\nLook before you speak: Visually contextualized utterances.\nIn CVPR, 2021.\n[54] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taig-\nman.\nMake-A-Video: Text-to-Video Generation without\nText-Video Data. arXiv preprint arXiv:2209.14792, 2022.\n[55] George Stein, Jesse C Cresswell, Rasa Hosseinzadeh, Yi\nSui, Brendan Leigh Ross, Valentin Villecroze, Zhaoyan Liu,\nAnthony L Caterini, J Eric T Taylor, and Gabriel Loaiza-\nGanem. Exposing flaws of generative model evaluation met-\nrics and their unfair treatment of diffusion models. arXiv\npreprint arXiv:2306.04675, 2023.\n[56] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia\nSchmid.\nLearning video representations using contrastive\nbidirectional transformer. arXiv preprint arXiv:1906.05743,\n2019.\n[57] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,\nDanyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou.\nCOIN: A Large-scale Dataset for Comprehensive Instruc-\ntional Video Analysis. In CVPR, 2019.\n[58] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable Length Video Generation from Open Do-\nmain Textual Descriptions. In ICLR, 2022.\n[59] Wenzhe Wang, Mengdan Zhang, Runnan Chen, Guanyu Cai,\nPenghao Zhou, Pai Peng, Xiaowei Guo, Jian Wu, and Xing\nSun. Dig into multi-modal cues for video retrieval with hier-\narchical alignment. In IJCAI, 2021.\n[60] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi\nFan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.\nMemvit: Memory-augmented multiscale vision transformer\nfor efficient long-term video recognition.\narXiv preprint\narXiv:2201.08383, 2022.\n[61] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nICCV, 2023.\n[62] Mengmeng Xu, Juan-Manuel P\u00b4erez-R\u00b4ua, Victor Escorcia,\nBrais Martinez, Xiatian Zhu, Li Zhang, Bernard Ghanem,\nand Tao Xiang. Boundary-sensitive pre-training for tempo-\nral localization in videos. In ICCV, 2021.\n[63] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and\nCordelia Schmid. Just ask: Learning to answer questions\nfrom millions of narrated videos. In ICCV, 2021.\n[64] Jianwei Yang, Yonatan Bisk, and Jianfeng Gao.\nTaco:\nToken-aware cascade contrastive learning for video-text\nalignment. In ICCV, 2021.\n[65] Mengjiao Yang, Yilun Du, Bo Dai, Dale Schuurmans,\nJoshua B Tenenbaum, and Pieter Abbeel.\nProbabilis-\ntic adaptation of text-to-video models.\narXiv preprint\narXiv:2306.01872, 2023.\n[66] Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang,\nMark Yatskar, and Chris Callison-Burch. Visual goal-step\ninference using wikihow. In EMNLP, 2021.\n[67] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang,\nXiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li,\nShuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan\nWang, Zicheng Liu, Houqiang Li, and Nan Duan. NUWA-\nXL: Diffusion over Diffusion for eXtremely Long Video\nGeneration. arXiv preprint arXiv:2303.12346, 2023.\n[68] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2022.\n[69] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, et al. Scaling autoregressive multi-\nmodal models: Pretraining and instruction tuning.\narXiv\npreprint arXiv:2309.02591, 2023.\n[70] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J.\nLiu. PEGASUS: Pre-training with Extracted Gap-sentences\nfor Abstractive Summarization. In ICML, 2019.\n[71] Li Zhang, Qing Lyu, and Chris Callison-Burch. Reasoning\nabout Goals, Steps, and Temporal Ordering with WikiHow.\nIn EMNLP, 2020.\n[72] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022.\n[73] Shuyan Zhou, Li Zhang, Yue Yang, Qing Lyu, Pengcheng\nYin, Chris Callison-Burch, and Graham Neubig. Show Me\nMore Details: Discovering Hierarchies of Procedures from\nSemi-structured Web Data. In ACL, 2022.\n[74] Yilun Zhou, Julie Shah, and Steven Schockaert. Learning\nHousehold Task Knowledge from WikiHow Descriptions. In\nProceedings of the 5th Workshop on Semantic Deep Learn-\ning (SemDeep-5), 2019.\n[75] Linchao Zhu and Yi Yang. Actbert: Learning global-local\nvideo-text representations. In CVPR, 2020.\n[76] Dimitri Zhukov, Jean-Baptiste Alayrac, Ivan Laptev, and\nJosef Sivic. Learning actionness via long-range temporal or-\nder verification. In ECCV, 2020.\n11\nGenerating Illustrated Instructions\nSupplementary Material\n7. Latent Diffusion Models Preliminaries\nLatent diffusion models [49] model a data distribution in\nthe latent space of another model, such as a VAE [25]. The\nlatent diffusion model is trained to generate samples from\nthe data distribution p(x) in the latent space, induced by the\nencoder E, via gradual denoising. These samples can then\nbe decoded by the VAE decoder D to produce samples from\nthe approximated data distribution. The objective matches\nthe form of our Eq 3.\n8. LLM inference prompt engineering\nWe find that simple prompting strategies suffice to have the\nLLM produce outputs in the format we want. Specifically,\nwe use the following prompt:\n{INPUT_TEXT} Write your response in the\nform of a goal \"Goal: {GOAL}\" followed\nby concise numbered headline steps,\neach one line, without any other text.\nUse at most 6 steps.\nwhere {INPUT TEXT} is any user input, such as those\nshown in the figures. If only a goal is provided, we provide\nthe input text as \u201cHow can I {GOAL}?\u201d, for example \u201cHow\ncan I make colored ice?\u201d.\n9. Data\nWe repurpose the VGSI dataset [66] for generative pur-\nposes. The original dataset is split by step-image pairs. We\nnote that this splits images from a single goal across training\nand validation. We thus recombine the data, group by asso-\nciated goal, and then create new splits. Each data point we\ncreate is formed by a string of goal text as well as varying\nnumbers of pairs of step text strings and associated images.\nThe distribution of the number of steps is seen in Figure\nFigure 4. We find that the overwhelming majority of arti-\ncles contain less than 6 steps. The training set consists of\n95328 goals for a total of 476053 step-image pairs. The\nvalidation set, after filtering only for goals in the \u2018Recipe\u2019\ncategory, consists of 1711 goals for 9473 step-image pairs.\n10. Metrics\nGoal Faithfulness (GF). We draw on the literature from\nVisual Goal-Step Inference to define a metric for goal faith-\nfulness.\nThe intuition is as follows.\nAn image should,\ntypically, be more associated with the text for the goal it\nwas made for than for the text of other goals.\nWe can\nmeasure this association by computing the cosine similarity\nbetween the image and the goal text with pretrained con-\ntrastive vision-language models such as CLIP [47]. How-\never, CLIP scores are notably miscalibrated when compar-\ning across different image-text pairs. Thus, to make the met-\nric meaningful, we must compare the similarity scores to\nthe similarity with other goals. We accomplish this by con-\nstructing multiple-choice questions as in VGSI [66]. For\neach image, we compare the CLIP similarity score with the\ncorrect goal text to the CLIP similarity scores with the texts\nof three other randomly selected goals. We then compute\nthe accuracy of the model in choosing the correct goal text.\nWhile VGSI uses this metric with a fixed dataset to evalu-\nate their vision-language models, we instead fix the vision-\nlanguage model used in order to evaluate the data being gen-\nerated.\nStep Faithfulness (SF). As mentioned in \u00a7 3, no matter how\nwell an image reflects the overall goal, it is useless if it does\nnot illustrate the step it serves to illustrate. Inspired by Goal\nFaithfulness, we can define a similar metric for step faith-\nfulness. An image should be more similar to the text for the\nstep it was made for than for other steps. In particular, it\nshould be more similar to the text than other steps with the\nsame goal. It is worth noting that the step faithfulness met-\nric varies in magnitude depending on the number of steps\n(N) in the sequence. As the number of steps increases, the\nchance that any individual step will have a visual that is not\nstrongly associated with the caption increases, resulting in\nlower values. However, for a fixed N, comparison between\nmodels is meaningful.\nCross-Image Consistency (CIC). Finally, a key compo-\nnent of the Illustrated Instructions task is that a set of im-\nages is generated rather than a single image. These images\nare not simply independent. If a particular set of ingredients\nis shown for \u201cgather the ingredients,\u201d then the same ingre-\ndients should be shown for \u201cmix the dry ingredients.\u201d More\ngenerally, we would like to avoid jarring inconsistencies be-\ntween images in the same sequence, such as objects chang-\ning color or number, etc. We can measure this by computing\nthe DINO similarity between the images for each step. We\nuse DINO [6] as the measure of visual similarity over CLIP\nas it reflects visual over semantic features, which are more\nrelevant for consistency across images. This has been noted\nin work for personalized image generation, such as Dream-\nBooth [50]. CLIP features, in particular, tend to be invariant\nto the aspects of the image that are most relevant for consis-\ntency, such as color, number of objects, style, and more.\nFID. We compute FID with respect to the ground truth\ndataset \u2013 specifically, the 9473 images of the validation set\n1\nare used as the reference distribution. We use the clean-fid\n[45] to avoid common pitfalls in FID computation.\nHuman Evaluation. We show the outputs of each model\nbeing compared in a two-way comparison side by side, as\nshown in Figure 11. For human evaluation, we select a ran-\ndom subset of 140 goals that are fixed across evaluations\nfrom the validation set. We ask annotators to select which of\nthe two articles they prefer (or if they are tied). As in previ-\nous work [19], we find that providing criteria for evaluators\nto consider during evaluation and requiring a justification\nfor the ultimate decision leads to substantially higher qual-\nity evaluations. We use 3 annotators per comparison. To en-\nsure quality of annotation, we selected annotators with the\nAmazon Mechanical Turk \u201cMaster\u201d qualification that had\na > 95% approval rate with at least 1000 prior tasks com-\npleted. For each method, we consider how many goals had a\nstrict majority of annotators pick that method\u2019s generations.\nTied cases are removed prior to win rate computation. We\nreport the percentage of majority wins for each method.\n11. StackedDiffusion implementation Details\nThe U-Net used in the in-house model we build on largely\nfollows the architecture used in [49], with a T5-XXL text\nencoder. The output channels of each block are changed\nto (320, 640, 1280, 1280). The GroupNorm epsilon value\nis 10\u22125. The input text condition uses additional projec-\ntion and attention layers before being fed into the U-Net as\nin [49]. Specifically, this consists of an attention block with\n4096 dimensions followed by layer normalization, a linear\nprojection from 4096 to 1280 dimensions, and another layer\nnormalization, and a final linear layer from 1280 to 1280 di-\nmensions.\n12. GILL Comparison Details\nWe first tried to use GILL [28] directly with the default set-\ntings other than the maximum number of output tokens be-\ning set to 512 so as to be an appropriate length for a full\narticle. We enable both generation and retrieval for GILL.\nWe experimented with prompts such as \u201cPlease write me a\nstep-by-step article for the goal GOAL\u201d and many variants\nof it. Despite this, we found that the generations were al-\nways substantially shorter than a full article, likely due in\npart to the short length of the training data GILL has seen.\nWe experimented with different values for the temperature\nas well as other parameters, but were unable to produce\nlonger outputs that would adequately illustrate a goal. The\ntext quality of these outputs was also very low. For instance,\nwhen prompted about the goal \u201cHow to Make Apple Pie,\u201d\nGILL responded \u201cI can\u2019t imagine why you would want to\ndo this.\u201d. Typically, the outputs would comprise text for a\nsingle step or a single image, and we did not observe any\ninstances of multiple.\nBecause of this, we elected to try to surmount the lan-\nguage modeling difficulties of GILL and see how well it can\nperform when given text from the same LLM used for our\nexperiments, which we call \u201cGILL++\u201d. This would evalu-\nate the image generation and retrieval capabilities of GILL\non their own. (To some extent, this does defeat the purpose\nof a multimodal generative model; it reflects the limitations\nof current generative models.) We performed LLM infer-\nence as described in Appendix \u00a7 8. We provided the gener-\nated text to GILL with the prompt \u201cPlease illustrate the step:\n{STEP} for the goal: {GOAL}\u201d. We also experimented\nwith various values of the \u201csf\u201d (scaling factor) inference pa-\nrameter, which the GILL codebase comments: \u201cincrease to\nincrease the chance of returning an image\u201d. However, the\nmodel did not always produce an image even when increas-\ning this value. The eventual parameters we arrived at for\nGILL++ were a scaling factor of 1.4, temperature of 0.6,\ntop p of 0.95, and all other parameters left the same. A\nqualitative comparison can be seen in Figure 12. We find\nthat the image generations still remain inferior to ours even\nwhen the text is fixed.\n13. Retrieval Baselines\nWe implement two separate retrieval-based baselines that\nuse real images for the illustrations of each step. We de-\nscribe these both below.\n13.1. LLM + CLIP\nThe LLM+CLIP baseline is a combination of text generated\nby an LLM and real images retrieved from that text. We\nprompt the LLM the same way as for the final StackedDif-\nfusion model. After obtaining these steps, we retrieve the\ntop image from the training dataset using CLIP similarity.\nFor each step, we rank all images in the training set by their\nCLIP similarity to that step\u2019s text, and use the most similar\nimage for each. Each of these images can come from any\ndisparate goal in the training set.\n13.2. Goal Retrieval\nFor the goal retrieval baseline, we pick the closest full arti-\ncle from the training set to compare to. The intent of this\nbaseline is to show that the model does not simply repro-\nduce the closest article it saw during training but rather gen-\nerates new articles. We use the CLIP text similarity compare\nthe generated goal text to the training set. We then pick the\nclosest article to the generated goal text. We use the steps\nfrom this article as the steps for the generated article. This\nbaseline is similar to the LLM + CLIP baseline, but instead\nof using the LLM to generate the steps, we use the steps\nfrom the closest article in the training set and simply use\nthe images associated with those ground-truth steps.\n2\n...\nFigure 11. Human evaluation setup.\n3\nGoal: Cook basic noodles\nStep 1: Bring water \nto a rolling boil on \nthe stove.\nStep 2: Add the \nnoodles and boil for \n8-10 minutes.\nStep 3: Drain the \nwater and end the \ncooking process.\nText-to-Image generations\nOurs\nGILL++\nFigure 12. Comparison with our GILL++ .\n14. Additional Qualitative Results\nWe present additional qualitative results in Figure 13, Fig-\nure 14, Figure 15, Figure 16, Figure 17, Figure 18, Fig-\nure 19.\nIn addition, we present an example of a failure case in\nFigure 20. We observe one failure mode in being over-\nconsistent: while Step 3 says to mix in \u2018another\u2019 bowl, the\nmodel ends up producing a bowl that looks similar to the\nbowl in Step 2. We also observe a case where the model\nfinds consistency difficult to produce: geometric arrange-\nments seem to be considered similarly, making a 3x3 grid\nof cupcakes into a 2x4 grid.\nStep 1: Mix dry \ningredients.\nStep 2: Add green \nfood coloring and mix \nwith wet ingredients.\nStep 3: Form batter \ninto small balls.\nYou could make St. Patrick\u2019s Day-themed donut holes. Here\u2019s one \npossible way:\nWhat simple dessert could I make St. Patrick\u2019s Day-themed?\nStep 4: Deep fry the \ndonut holes in oil until \ncooked through.\nStep 5: Serve on a \nplate of green donut \nglaze.\nFigure 13. Additional personalization. The steps illustrate donut\nholes customized to be green for St. Patrick\u2019s day.\nStep 1: Grease a \n9x13\u201d baking dish.\nStep 2: Mix boiled sweet \npotatoes and milk.\nStep 3: Mix in sugar \nand mash.\nStep 4: Combine \nbrown sugar and \nchopped pecans.\nStep 5: Spread into \nthe baking dish; bake \nfor 25-30 minutes.\nStep 6: Remove once \nthe top is golden \nbrown.\nYou could make a simple sweet potato casserole for your \ncompany Thanksgiving potluck. Here\u2019s how: \nWhat easy dish can I make for my company Thanksgiving \npotluck?\nFigure 14. Additional goal suggestion. StackedDiffusion sug-\ngests the goal of making a casserole and provides illustrated in-\nstructions for it.\n4\nStep 1: Lightly toast \nthe bread to desired \ncrispness.\nStep 2: Spread a \ngenerous layer of \nbutter on the toast.\nStep 3: Sprinkle \nhagelslag (chocolate \nsprinkles) and sugar.\nA traditional Dutch breakfast can be simple and delightful. One \nof the most popular is \u201chagelslag.\u201d Here\u2019s a basic guide:\nHow can I make a traditional Dutch breakfast?\nFigure 15. Additional goal suggestion. The world knowledge of\nthe LLM allows it to inform the user about hagelslag.\nStep 1: In a cooking \npot, add oat bran and \ncoconut milk.\nStep 2: Heat your \nmixture on a medium \nflame until it thickens.\nStep 3: Sprinkle \nshredded coconut; stir \nuntil well-combined.\nMaking coconut oat bran is a simple and healthy recipe. Here's a basic method you can \nfollow:\nHow can I make coconut oat bran?\nStep 4: Remove from \nheat, allow to cool \nslightly, then serve.\nFigure 16. Additional article generation. Coconut oat bran is\na variant on an existing dish (not found in the training set) that\nStackedDiffusion can illustrate.\nStep 1: Mix flour and \nsugar in a mug.\nStep 2: Add cocoa \nand ground vanilla; stir \nwith wet ingredients.\nStep 3: Put on a plate \nand microwave until \ndone.\nMaking brownies in a mug is a quick and easy way to enjoy a chocolate treat. Here's a \nsimple recipe you can try:\nHow can I make brownies in a mug?\nStep 4: Optionally, \ndust with powdered \nsugar.\nFigure 17. Additional article generation. The mug shows the\nconsistency StackedDiffusion enables.\nStep 1: Fill water into \na pot and heat to a \nrolling boil.\nStep 2: Rinse the \ncarrots and put them \nin the water.\nStep 3: Cook the \ncarrots until they are \ndone.\nBoiling carrots is a simple and nutritious way to prepare them. Here's a basic recipe \nyou can use:\nHow can I make boiled carrots?\nStep 4: Drain the \ncarrots.\nFigure 18. Additional article generation.\nStep 1: Beat the \nsugar, lemon zest, and \nshortening.\nStep 2: Add warm \nmilk and whisk until \nconsistency is ideal.\nStep 3: Spread the \nfrosting on top of the \ncookies.\nAdding frosting to lemon drop cookies can make them even more delicious! Here's a \nsimple and effective way to do it:\nHow can I add the icing for lemon drop cookies?\nStep 4: Allow the \nfrosting to set and \nserve.\nFigure 19. Additional article generation.\nStep 1: Preheat the \noven to 350 degrees \nFahrenheit.\nStep 2: Whisk \ntogether the canned \npumpkin, milk, vanilla, \nbrown sugar, and oil.\nStep 3: Sift together \nthe dry ingredients in \nanother bowl and mix.\nStep 4: Add the dry \ningredients to the wet \nones a bit at a time.\nStep 5: Fill cupcake \nliners two-thirds of the \nway and bake.\nStep 6: Transfer to a \ncooling rack and let \ncool before frosting.\nTo make pumpkin cupcakes, you can follow this simple recipe:\nHow can I make pumpkin cupcakes?\nFigure 20. Example failure case. We note two failure modes.\nFirst, the model is over-consistent, producing a bowl in Step 3 that\nlooks similar to the bowl in Step 2 when it should be another bowl.\nSecond, the model appears to consider geometric arrangements as\nsimilar to each other even if the numbers in each differ, leading to\ninconsistency between Step 5 and Step 6.\n5\n"
  },
  {
    "title": "LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning",
    "link": "https://arxiv.org/pdf/2312.03849.pdf",
    "upvote": "5",
    "text": "LEGO: Learning EGOcentric Action Frame Generation\nvia Visual Instruction Tuning\nBolin Lai1,2 \u2020\nXiaoliang Dai1\nLawrence Chen1\nGuan Pang1\nJames M. Rehg3\nMiao Liu1\n1GenAI, Meta\n2Georgia Institute of Technology\n3University of Illinois Urbana-Champaign\nbolin.lai@gatech.edu\n{xiaoliangdai,lawrencechen,gpang,miaoliu}@meta.com\njrehg@illinois.edu\n\u201cCan you provide \ninstructions on how to \nwash the trouser with \nthe brush in my current \nsituation?\u201d\n\u201c1. Check the Care Label: \nCheck the care label on your \ntrousers for any specific washing \ninstructions.\n2. Prep the Area: Find a clean \nand flat surface to work on, such \nas a table or countertop.\n \n3. Spot Check Stains:\u201d\n\u2026\n\u201c1. You should submerge the \ntrouser in the water.\n2. Use the brush to scrub the \ntrouser, focusing on any \nstains or areas that may \nrequire extra attention.\n3. Once the trouser is clean, \nyou should rinse it.\u201d\n(a) LLM Response\n(b) Visual LLM Response\n(c) Our model (LEGO) Response\nActions in Existing Egocentric Datasets\nUser Prompt\n\u2026\nFigure 1. When the user queries for instructions on acquiring specific skills, large language models (LLMs) tend to provide general\ninstructions that may not be readily applicable to the user\u2019s current situation. While visual large language models (VLLMs) can generate\nmore specific guidance based on visual prompt capturing the user\u2019s context, the generated textual instructions still lack clarity and ease for\nthe user to interpret. In contrast, we present a novel model \u2014 LEGO, which takes the user\u2019s query and an egocentric image captured from\nuser\u2019s perspective as inputs. It then generates an egocentric action frame that vividly illustrates the execution of the query action.\nAbstract\nGenerating instructional images of human daily actions\nfrom an egocentric viewpoint serves a key step towards effi-\ncient skill transfer. In this paper, we introduce a novel prob-\nlem \u2013 egocentric action frame generation. The goal is to\nsynthesize the action frame conditioning on the user prompt\nquestion and an input egocentric image that captures user\u2019s\nenvironment. Notably, existing egocentric datasets lack the\ndetailed annotations that describe the execution of actions.\nAdditionally, the diffusion-based image manipulation mod-\nels fail to control the state change of an action within the\ncorresponding egocentric image pixel space. To this end,\nwe finetune a visual large language model (VLLM) via vi-\nsual instruction tuning for curating the enriched action de-\n\u2020This work was done during Bolin\u2019s internship at GenAI, Meta.\nscriptions to address our proposed problem. Moreover, we\npropose to Learn EGOcentric (LEGO) action frame gener-\nation using image and text embeddings from VLLM as ad-\nditional conditioning. We validate our proposed model on\ntwo egocentric datasets \u2013 Ego4D and Epic-Kitchens. Our\nexperiments show prominent improvement over prior image\nmanipulation models in both quantitative and qualitative\nevaluation. We also conduct detailed ablation studies and\nanalysis to provide insights on our method.\n1. Introduction\nThe emergence of Large Language Models (LLMs) [6, 10,\n66, 99], such as ChatGPT, has revolutionized the transfer\nof knowledge. However, an LLM alone is not a sufficient\n1\narXiv:2312.03849v1  [cs.CV]  6 Dec 2023\ntool for human skill transfer. Consider a question answer-\ning example in Fig. 1(a). The AI agent can summarize gen-\neral world knowledge, yet it\u2019s not directly applicable to the\nuser\u2019s current circumstance. On the other hand, the ego-\ncentric visual perception provides a novel means to capture\nthe actions and intents as well as the surrounding contexts\nof the camera wearer. As shown in Fig. 1(b), recent Visual\nLarge Language Models (VLLMs) [1, 12, 47, 53, 103] show\nthe capability of generating instructional responses based\non the egocentric visual prompt. However, the generated\ntextual instructions are not the ideal vehicle for transferring\nhuman skills, as neuroscience studies have revealed that\nhuman brain processes images much faster than texts [4].\nMoreover, it has also been evidenced that humans can in-\nterpret an action from a static image [23]. Motivated by\nthese discoveries, we seek to design an egocentric AI sys-\ntem capable of synthesizing an image that not only vividly\ndepicts how an action should be conducted, but also seam-\nlessly aligns with the user\u2019s visual perspective.\nFormally, we introduce the novel problem of egocentric\naction frame generation as shown in Fig. 1(c). Given a user\nquery of how to perform a specific action and an egocen-\ntric image capturing the moment before the action begins,\nthe goal is to synthesize an egocentric image illustrating the\nexecution of the action in the same egocentric context. In\nthis paper, we address this problem by harnessing diffusion\nmodels [28, 69], which have shown strong capability in im-\nage manipulation tasks [5, 26, 45, 58, 101]. There are two\nmajor challenges in using diffusion models to generate ac-\ntion frames from an egocentric perspective. First, the action\nannotations of the existing egocentric datasets [13, 22] are\nsimply composed of a verb and nouns (see Fig. 1), and thus\nlack the necessary details for diffusion models to learn the\naction state change and to associate the action with correct\nobjects and body movements. Second, even when provided\nwith detailed prompts about egocentric actions, condition-\ning solely from a text encoder may not be able to effectively\ncontrol the action state change for action frame generation.\nTo address these challenges, we propose to Learn the\nEGOcentric (LEGO) action frame generation model with\nvisual instruction tuning.\nFirst, we design a data cura-\ntion pipeline that generates enriched action descriptions at\nscale, by finetuning a VLLM with visual instruction tun-\ning. Second, we leverage the image and text embeddings\nfrom finetuned VLLM as a complement to the text encoder\nfrom the diffusion model to improve the controllability for\naction frame generation. Our experimental results suggest\nthat the enriched action descriptions and the utilization of\nVLLM embeddings both improve the image generation per-\nformance. Overall, our contributions can be summarized as\nfollows:\n\u2022 We introduce the novel problem of egocentric action\nframe generation to facilitate the process of skill transfer.\n\u2022 We leverage visual instruction tuning to enrich egocentric\naction descriptions, and demonstrate how the enriched de-\nscriptions can help the diffusion model understand the ac-\ntion state change from the egocentric perspective.\n\u2022 We propose a novel model \u2013 LEGO that leverages the\ntext and visual embeddings from a visual large language\nmodel to improve the performance of the latent diffusion\nmodel for egocentric action frame generation.\n\u2022 We conduct thorough experiments on Ego4D and Epic-\nKitchens datasets to validate the superiority of our model\nover prior approaches. We also showcase the contribution\nof each component of our model design through ablation\nstudies. We further provide analysis on how the visual\ninstruction tuned embeddings benefit model performance.\n2. Related Work\nText-Guided Image Manipulation. The recent emergence\nof diffusion models enables text-guided image manipulation\nincluding image restoration [35], style transfer [76], person-\nalized image synthesis [70, 72, 88], pose generation [30, 38]\nand generic image editing [16, 19, 36, 40, 45, 59, 62, 80,\n83, 83\u201385, 95, 100, 102]. SDEdit [58] converts the image\nto the latent space by adding noise through a stochastic dif-\nferential equation and then denoises the representation for\nimage editing. Rombach et al. [69] further expand SDEdit\nfrom the original stroke-based editing to text-based edit-\ning. Null-text inversion (NTI) [60] inverts a real image by\nDDIM [37] to yield the diffusion process and then recon-\nstructs the image. The image can then be edited following\nthe same strategies as Prompt-to-Prompt [26]. NTI relies on\naccurate inversion process which can be improved by using\ncoupled transformations [81] or proximal guidance [25] and\naccelerated by a blended guidance strategy [63]. To asso-\nciate the nouns with correct objects, DIFFEDIT [11] gener-\nates a mask to localize the manipulated regions. However,\nmost inversion-based methods require accurate image cap-\ntions, which are largely unavailable in the existing egocen-\ntric dataset. Recently, InstructPix2Pix [5] demonstrates the\npotential to edit a real image without the inversion process\nand original captions. However, how to leverage the diffu-\nsion model to control the state change of an action within\nthe egocentric image plane remains unexplored.\nLarge Language Model for Image Generation. LLMs [6,\n10, 66, 77, 79, 99] and VLLMs [1, 12, 24, 47, 53, 74, 96,\n103] have shown their strong capability of understanding\ncomplex human instructions. Recently, LLMs and VLLMs\nare used to guide image generation [2, 7\u20139, 52, 94, 103].\nWen et al. [89] use a pretrained VLLM to pinpoint the mis-\nalignment of the text prompt and the synthetic image and\nthen correct it using the diffusion model. Lian et al. [50]\npropose to generate a layout map using an LLM to improve\nthe understanding of prompts with spatial and numeracy\nreasoning. InstructEdit [84] uses BLIP-2 [47] to infer the\n2\nobjects that will be edited and then generates a mask with\nSAM [41] for object grounding. A pre-trained LLM can\nalso be used as a controller to connect with various founda-\ntion models [71, 90, 91]. GILL [42] learns text-relevant im-\nage embeddings from VLLM in a few additional tokens for\nimage generation. Importantly, all previous efforts leverage\nthe off-the-shelf image or text embeddings from the founda-\ntional models. In contrast, our method uses visual instruc-\ntion tuning to improve the image and text embeddings from\nthe VLLM and thereby facilitates the action frame genera-\ntion from the egocentric point of view.\nEgocentric Vision. Recent efforts seek to understand hu-\nman\u2019s actions and attentions [17, 31, 32, 39, 43, 44, 49, 75,\n87], modeling hand-object interactions [21, 54, 55, 67, 92],\nand estimating human body poses [48, 57, 78, 82] from the\negocentric perspective. Here, we mainly discuss the most\nrelevant works on egocentric visual-language models and\negocentric visual content generation. Lin et al. [51] pro-\npose the first egocentric video-language pre-training model\n\u2013 EgoVLP. Pramanick et al. [64] further improve it by in-\ncorporating multi-modal fusion directly into the video and\nlanguage backbone. Ashutosh et al. [3] propose to learn\na hierarchical video-language embedding for long egocen-\ntric videos. Ramakrishnan et al. [68] introduce NaQ, which\nis a data augmentation strategy to train models for long\negocentric video search with natural language queries. In\nterms of egocentric visual generation, Jia et al. [34] lever-\nage GANs [20] to generate future head motion in hand fore-\ncasting task. Zhang et al. [97] leverage GANs to facilitate\nfuture gaze anticipation. Ye et al. [93] propose the affor-\ndance diffusion model that takes in the image of an object\nand generates possible hand-object interactions in the ego-\ncentric view. In this paper, we present the first work that in-\nvestigates how to leverage VLLMs and diffusion models to\ngenerate action state change on the egocentric image plane.\n3. Method\nThe problem setting of egocentric action frame generation\nis illustrated in Fig. 1(c). Given an egocentric image frame\nX that captures the user\u2019s current visual context as well as\na user query prompt P regarding how to perform an action,\nour goal is to synthesize the action frame Y that visually\ndepicts how the action should be conducted.\nThe key insight of our proposed method is leveraging the\nstrong capability of a VLLM to enhance the diffusion model\nfor egocentric action frame generation. The annotations of\nexisting egocentric datasets do not describe the details of\nhow actions are conducted. As a remedy, we leverage vi-\nsual instruction tuning to finetune a VLLM that enriches ac-\ntion descriptions based on the egocentric visual prompt. In\naddition, the off-the-shelf CLIP text encoder of a diffusion-\nbased image manipulation model may have a domain gap\nbetween the CLIP pre-training data and egocentric action\nAction Label: \u201cchop end of onion\u201d\nBounding Boxes: \u201cleft hand-[0.203, \n0.368, 0.499, 1.000], right hand-\n[0.530, 0.527, 0.797, 1.000], chopping \nboard-[0.248, 0.156, 0.644, 0.750], \nonion-[0.462, 0.377, 0.509, 0.506], \nknife-[0.484, 0.343, 0.546, 0.586]\u201d\nAction Description: \u201cThe person presses the onion on the \nchopping board with the left hand and then cuts off the end of the \nonion with a knife in the right hand.\u201d\nBounding Boxes: \u201cleft hand-[0.427, \n0.799, 0.591, 1.00], right hand-[0.654, \n0.949, 0.707, 1.00], lunch box-[0.390, \n0.512, 0.696, 0.993], sink-[0.00, 0.802, \n0.315, 1.00], container lid-[0.403, \n0.454, 0.513, 0.635], fork-[0.589, 0.798, \n0.672, 1.00]\u201d\nExample for In-context Learning\nQuery and Response\nGPT-3.5 Response: \u201cThe person uses their left hand to hold a lunch \nbox, while their right hand uses a fork to squidge food into the lunch \nbox, which is placed on the sink with the container lid nearby.\u201d\nAction Label: \u201csquidge into lunch box\u201d\nInput Query\nInput Query\nFigure 2. Examples for data curation using GPT-3.5. We provide\ndetailed action descriptions of several example images as well as\ntheir action labels and bounding boxes for in-context learning. In\naddition, we input the action label and bounding boxes of another\naction as a query. GPT-3.5 is able to generate descriptions with\nenriched information (highlighted by underlines) in the response.\ndatasets, resulting in limited understanding of the action-\nrelated prompts used in our problem setting. To bridge this\ngap, we propose LEGO \u2013 a novel approach that leverages\nVLLM embeddings to control the state change of actions\nand to generate action frames accordingly. We detail the\nVLLM-based data enrichment pipeline and our model de-\nsign in the following sections.\n3.1. Egocentric Visual Instruction Tuning\nTraining Data for Visual Instruction Tuning. As shown\nin Fig. 2, we use GPT-3.5 to generate detailed action de-\nscriptions based on an input query composed of a short ac-\ntion label and object bounding boxes. First, we provide sev-\neral examples of possible inputs along with their expected\noutput descriptions for GPT-3.5 to perform in-context learn-\ning. These examples cover a diverse set of scenes in the\negocentric action dataset. Each example is composed of\nan action label, a manually annotated action description,\nand relative spatial information of hands and objects-of-\ninterests represented by bounding box coordinates. GPT-\n3.5 can learn from the given examples and generate detailed\naction descriptions based on a new input query. The result-\ning GPT-3.5 curated data is then further used for visual in-\nstruction tuning. More details of the prompt structure are\nprovided in the supplementary.\nVisual Instruction Tuning. We follow the finetuning strat-\negy in prior work [53], as shown in Fig. 3(a). Specifically,\n3\nCan you provide instructions on how to wash the \ntrouser with the brush in my current situation?\nProj. \ud835\udf0f\nLarge Language Model\n\u2026\n\u2026\nUse the brush in the right hand to wash \nthe trousers while holding the trousers in \nthe left hand.\nText Embed. \u210b!\n\u2026\nImg. Embed. \u210b\"\n\u2026\nEnriched Action Description \u211b\nText Enc. \ud835\udf13\nImg. Enc. \ud835\udf19\nInput Frame\nUser Prompt \ud835\udcab\nProj. \ud835\udf0e\nProj. \ud835\udf07\n\u2026\n\u2026\n\u2026\n\u2026\nDenoising UNet\nConditioning \ud835\udc9e\n\u2026\n\u2026\n\u2026\nAuto-Encoder \nInput Embed. \n\u2130( )\nAction Frame\nSelf-Att. \ud835\udf0b\nTokens in LLM feature space\nTokens in LDM feature space\n(a) Instruction Tuning \n(b) Action Frame Generation\nLinear \nHead\nFigure 3. Overview of our proposed LEGO model. We first finetune a visual large language model (VLLM) to generate the enriched action\ndescription with visual instruction tuning. We then project image and text embeddings from the finetuned VLLM to the feature space of\nthe latent diffusion model (LDM). Finally, we train the LDM to synthesize the egocentric action frame conditioning on the input frame,\nenriched action description, as well as the VLLM image and text embeddings.\nwe use the pre-trained CLIP visual encoder [65] \u03d5 to encode\nthe visual representation and then apply a linear projection\nlayer \u03c4 to map the CLIP visual features into the semantic\nspace of the LLM, i.e. Hi = \u03c4(\u03d5(X)). To construct the\nuser prompt P, we insert the action label annotation into a\nprompt template to create a coherent query that is aligned\nwith the instruction-following nature of an LLM. We then\ntokenize P, and feed both the prompt text tokens and im-\nage tokens Hi as inputs into the LLM. Finally, the LLM is\ntrained to generate enriched action description (denoted as\nR) based on the user prompt and image input.\nUser Prompt Enrichment at Scale. Note that the visual-\ninstruction tuned VLLM doesn\u2019t rely on any object bound-\ning boxes as input. Therefore, we can generate enriched\naction descriptions for all egocentric action data at scale.\n3.2. Egocentric Action Frame Generation\nWe leverage a latent diffusion model (LDM) [69] to synthe-\nsize the action frame conditioning on the input frame and\nthe detailed action description R generated by our finetuned\nVLLM (see Fig. 3(b)). The input image is first encoded into\na latent space using a pre-trained auto-encoder E. The input\nto the denoising UNet is a concatentation of the latent input\nrepresentation E(X) and a Gaussian noise G.\nOur key innovation is to design the U-Net conditioning\ncomponent so that the diffusion model can interpret the en-\nriched action description. To start, we follow [5] and adopt\nthe conventional pre-trained CLIP text encoder \u03c8 to extract\na text representation of R, i.e. \u03c8(R) \u2208 RN\u00d7D where N\nis the maximum number of text tokens and D is the num-\nber of feature channels. We further leverage the image and\ntext embeddings from the visual instruction tuned VLLM\nas additional LDM conditioning to alleviate the domain gap\nissue of the text encoder.\nSpecifically, we feed the VLLM image embedding Hi\ninto an extra linear layer \u03c3 to map it to LDM feature space,\ni.e. \u03c3(Hi) \u2208 RM\u00d7D, where M is the number of image\ntokens. Note that Hi is already projected to the semantic\nspace during visual instruction tuning, and therefore differs\nfrom the image embedding E(X) from the auto-encoder.\nMoreover, we also extract the text embedding Ht before\nthe last linear layer of LLM. We adopt a fixed token num-\nber N and enforce padding or truncation behavior, as in the\nCLIP text encoder. The text embedding is then fed to a pro-\njection layer \u00b5. In LLM decoder, the response is generated\niteratively and each word embedding only conditions on the\ncontext ahead of it. To extract the holistic semantic meaning\nof Ht in LDM feature space, we further add self-attention\nlayers \u03c0 after the projection, i.e. \u03c0(\u00b5(Ht)) \u2208 RN\u00d7D. Thus,\n4\nthe U-Net conditioning can be formulated as:\nC =\nh\n\u03c8(R), \u03c3(Hi), \u03c0(\u00b5(Ht))\ni\n\u2208 R(2N+M)\u00d7D.\n(1)\nThe conditioning C is fed into the denoising UNet at mul-\ntiple layers via the cross-attention mechanism [69]. We as-\nsume the intermediate feature of a specific UNet layer is U,\nwhich is learned from the UNet input (i.e. the concatenation\nof input frame representation E(X) and Gaussian noise G).\nThe cross-attention at this UNet layer can be formulated as:\nCrossAtt(Q, K, V ) = softmax\n\u0012QKT\n\u221a\nD\n\u0013\n\u00b7 V,\n(2)\nwhere Q = WQ\u00b7U, K = WK \u00b7C and V = WV \u00b7C. Note that\nWQ, WK and WV are learnable matrices. We also adopt\nthe classifier-free guidance following [5] (see supplemen-\ntary for details). Finally, the UNet output is converted to\nthe image domain by a pre-trained decoder.\n3.3. Implementation Details\nAll parameters of the VLLM are initialized from the pre-\ntrained LLaVA [53] weights. During training, we freeze\nthe CLIP image encoder and finetune the projection layer\nand LLM with cross-entropy loss for 3 epochs.\nTo im-\nprove the diversity of the text prompts, we randomly select\nthe question template from 10 candidates in each iteration.\nFor LDM training, the text encoder, UNet and auto-encoder\nare initialized with pre-trained weights [69]. The projection\nlayers \u03c3 and \u00b5 and the self-attention layers \u03c0 are initialized\nusing the Xavier algorithm [18]. The text encoder is frozen\nand the remaining weights are finetuned with L2 regression\nloss between the predicted noise and real noise for 20,000\niterations. All input and target images are resized to a res-\nolution of 256\u00d7256. Please refer to the supplementary for\nmore details of training and inference.\n4. Experiments\nWe first introduce the datasets, data preprocessing, and eval-\nuation metrics used in our experiments. Then we demon-\nstrate the improvement of our model over prior image ma-\nnipulation approaches in both quantitative evaluation and\nqualitative visualization. In addition, we ablate our model\nto show the contribution of each component. Finally, we\ninvestigate the importance of VLLM finetuning to the per-\nformance of the diffusion model.\n4.1. Data and Metrics\nDatasets We conduct our experiments on two well-\nestablished egocentric action datasets \u2013 Ego4D [22] and\nEpic-Kitchens [13]. Both datasets were densely annotated\nwith action starting time t and ending time \u02c6t. In our prob-\nlem setting, we select an egocentric image frame \u03b4i sec-\nonds before the action begins as the input X, and an im-\nage \u03b4o seconds after the action begins as the target frame\nY. On the Ego4D dataset, based on the annotations of Pre-\nCondition-15 time (PRE-15) tpre , and Point-of-No-Return\ntime (PNR) tpnr, we set \u03b4i = t\u2212tpre and \u03b4o = tpnr \u2212t. For\nEpic-Kitchens, PNR and PRE-15 annotations are not avail-\nable. Instead, we empirically select \u03b4i = 0.25 seconds and\n\u03b4o = t + \u03bb \u2217 (\u02c6t \u2212 t), where \u03bb = 0.6, for our experiments.\nData Preparation for Visual Instruction Tuning. For our\ndataset curation, we randomly select 20,891 actions with\nbounding box annotations from the Ego4D training set and\n17,922 actions with VISOR [14] mask annotations from\nthe Epic-Kitchens training set. We then insert the origi-\nnal action label into a prompt template to construct the full\nuser prompt. In order to diversify the prompt structure, we\nprepare 10 prompt templates (see supplementary) and ran-\ndomly select one for each action at training time.\nData Improvement for Action Frame Generation. Due\nto the possible drastic camera motion, the egocentric image\nframes at t\u2212\u03b4i and t+\u03b4o may be blurry. As a mitigation, we\nfirst calculate aesthetic scores [29] of the frames at t\u2212\u03b4i and\nt + \u03b4o as well as 3 frames before and after them. The cor-\nresponding frames with the highest aesthetic score are used\nas the input and ground truth of our model. In addition, the\negocentric body motion may have huge variance depending\non the action type, meaning that the input frame and target\nframe may look almost identical in more stationary actions\n(e.g. camera wearer is reading book), or significantly dif-\nferent in more active actions (e.g. outdoor activities with\nfrequent head motion). Such large variances may incur ad-\nditional barriers for the diffusion model training. Therefore,\nwe calculate the CLIP similarity of the input frame and tar-\nget frame, and we filter out the instances where the CLIP\nsimilarity is lower than 0.81 or higher than 0.97. With these\nsteps, we ultimately curate 85521/9931 data samples for the\ntrain/test sets from Ego4D and 61841/8893 data samples for\nthe train/test sets from Epic-Kitchens.\nMetrics. We adopt image-to-image similarity, image-to-\ntext similarity, and user study as metrics in our experiments.\n\u2022 Image-to-Image Metrics. We consider three types of met-\nrics to evaluate image-to-image similarity. (1) Distribu-\ntion Divergence based metric: Fr\u00b4echet Inception Distance\n(FID) [27]. (2) Perception based metrics: Peak Signal-\nto-Noise Ratio (PSNR) and Learned Perceptual Image\nPatch Similarity (LPIPS) [98] with SqueezeNet [33] as\nthe encoder. (3) Contrastive pre-training based metrics:\nimage-to-image CLIP score [65], EgoVLP score [51]\nand EgoVLP+ score [51].\nEgoVLP is a contrastive\nvideo-language pre-training model trained with egocen-\ntric videos. Since EgoVLP takes multiple frames as in-\nputs, we consider two types of inputs: duplicating the\noutput frame as a static input (i.e. EgoVLP score) and\ncombining the input frame with the output frame (i.e.\nEgoVLP+ score).\nAs as result, EgoVLP+ can effe-\ntively measure whether the generated frame can depict\n5\nMethods\nEgo4D\nEpic-Kitchens\nCLIP\nEgoVLP\nEgoVLP+\nFID \u2193\nPSNR\nLPIPS \u2193\nCLIP\nEgoVLP\nEgoVLP+\nFID \u2193\nPSNR\nLPIPS \u2193\nProxNPI [25]\n0.682\n0.445\n0.727\n33.011\n11.876\n0.409\n0.658\n0.323\n0.528\n51.350\n11.064\n0.464\nSDEdit [58]\n0.734\n0.501\n0.729\n33.347\n11.813\n0.416\n0.748\n0.338\n0.568\n27.413\n11.296\n0.433\nIP2P [5]\n0.788\n0.622\n0.788\n24.731\n12.155\n0.372\n0.770\n0.430\n0.611\n20.647\n11.229\n0.408\nLEGO\n0.806\n0.657\n0.804\n23.830\n12.294\n0.364\n0.786\n0.459\n0.627\n21.566\n11.334\n0.404\nTable 1. Comparison with prior image manipulation approaches. \u2193 means a lower score in this metric suggests a better performance. The\nbest results are highlighted with boldface. The green row refers to our LEGO model performance.\nthe state change of an action. Note that instead of mea-\nsuring similarity of input and output frames as in prior\nworks [12, 81], in our problem setting, we measure the\nsimilarity between the generated action frame and ground\ntruth, which better reflects whether the generation results\ncan illustrate the execution of an action.\n\u2022 Image-to-Text Metrics. We find the widely-used image-\nto-text CLIP score can not align actions with egocentric\nimages due to the domain gap.\nSimilar misalignment\nproblem is also observed in [15, 61, 73, 86]. In our ex-\nperiments, we utilize BLIP [46] to generate image cap-\ntions of the output images and then calculate text-to-text\nsimilarity using CLIP text encoder (following [36]). We\nimplement this metric with two BLIP structures: BLIP-B\nand BLIP-L. Though this solution may still suffer from\nthe same domain gap issue, it is a more appropriate eval-\nuation metric in our problem setting. See more evidence\nand discussions in the supplementary.\n\u2022 User Study. We also conduct a user study on a subset of\ntest data to further validate the advantage of our model\nbased on human preference.\nWe sample 60 examples\nfrom each dataset and present the generated frames from\nour model as well as the baseline models to raters on\nAmazon Mechanical Turk (AMT). We also provide the\ninput frames and the corresponding action labels during\nevaluation. For each instance, we ask the raters to select\nthe image that best aligned with the provided action label\nwhile preserving the most contextual information from\nthe input frame. To minimize potential bias, we hire 5\nAMT raters to annotate each example thus resulting in\n300 samples for each dataset. The template of our user\nstudy is presented in the supplementary.\n4.2. Comparison with Prior Approaches\nWe compare our proposed model with previous state-of-the-\nart methods for text-guided image manipulation, including\nProxNPI [25], SDEdit [58] and InstructPix2Pix (IP2P) [5].\nFor a fair comparison, we finetune these baseline methods\nwith the same data used in our experiments. Specifically,\nwe train IP2P in an end-to-end way on the two datasets\nwith existing egocentric action labels as text conditioning.\nProxNPI and SDEdit rely on the off-the-shelf latent diffu-\nsion model to synthesize edited images and thus can not be\nMethods\nEgo4D\nEpic-Kitchens\nBLIP-B\nBLIP-L\nBLIP-B\nBLIP-L\nProxNPI [25]\n0.177\n0.174\n0.237\n0.234\nSDEdit [58]\n0.198\n0.197\n0.215\n0.213\nIP2P [5]\n0.200\n0.205\n0.254\n0.264\nLEGO\n0.204\n0.207\n0.270\n0.274\nTable 2.\nImage-to-text similarity evaluation of our model and\nbaselines. The best results are highlighted with boldface.\ntrained end to end. Therefore, we first finetune the latent\ndiffusion model with egocentric images and action labels\nand then use the finetuned latent diffusion model parame-\nters to implement ProxNPI and SDEdit approaches.\n4.2.1\nQuantitative Comparison\nAs shown in Tab. 1, both ProxNPI and SDEdit perform\npoorly on this novel problem, suggesting the challenging\nnature of generating egocentric action frames. IP2P per-\nforms much better in almost all metrics by end-to-end train-\ning and serves as the strongest baseline model in our ex-\nperiments. However, our LEGO model consistently outper-\nforms IP2P by a large margin (1.8%, 3.5%, 1.6%, 0.901,\n0.139 and 0.8% respectively) in all six metrics on Ego4D.\nLEGO also exceeds IP2P notably (1.6%, 2.9%, 1.6%, 0.105\nand 0.4% respectively) in five metrics on Epic-Kitchens.\nThough, LEGO slightly lags behind IP2P in FID score, it\nstill achieves the second best performance.\nWith regard to image-text similarity, LEGO consistently\noutperforms prior approaches on both datasets (Tab. 2).\nLEGO outperforms IP2P by 0.4% and 0.2% on Ego4D and\nby 1.6% and 1.0% on Epic-Kitchens. The result suggests\nthe synthetic frames from our model can better align with\nthe action descriptions. However, the performance gain is\nrather limited. We emphasize that this is because the do-\nmain gap still exists for the BLIP model.\nFor our user study, we shuffle the order of the results\nfrom our model and the baselines while presenting them to\nthe raters. We define the win rate as the percentage of each\nmodel being picked as the best out of the total 300 sam-\nples. Results are illustrated in Fig. 4. Our model surpasses\nthe second best baseline by 44.0% and 38.3% respectively\n6\n10\n20\n30\n40\n50\n60\n70\n0\nEgo4D\nEpic-Kitchens\nProxNPI\nSDEdit\nIP2P\nLEGO\n9.7\n12.3\n17.0\n61.0\n9.2\n8.3\n22.1\n60.4\nWin Rate\nFigure 4. User study of our model and baselines. Win rate is the\npercentage of each model being picked as the best in all samples.\nOur model outperforms all baselines by a significant margin.\nConditioning\nCLIP EgoVLP EgoVLP+ PSNR\nEgo4D\nActions Labels\n0.788\n0.622\n0.788\n12.155\nDescriptions\n0.791\n0.629\n0.791\n12.200\nDesc. + Img Embed.\n0.807\n0.654\n0.801\n12.239\nDesc. + Txt Embed.\n0.792\n0.633\n0.794\n12.203\nDesc. + Joint Embed. 0.806\n0.657\n0.804\n12.294\nEpic-Kitchens\nActions Labels\n0.770\n0.430\n0.611\n11.229\nDescriptions\n0.775\n0.437\n0.615\n11.247\nDesc. + Img Embed.\n0.785\n0.458\n0.623\n11.254\nDesc. + Txt Embed.\n0.777\n0.447\n0.620\n11.305\nDesc. + Joint Embed. 0.786\n0.459\n0.627\n11.334\nTable 3. Analysis of egocentric action frame generation perfor-\nmance with different conditionings. Joint Embed. refers to incor-\nporating both VLLM image and text embeddings. The best results\nare highlighted with boldface. The green rows refer to our full\nLEGO model performance.\non Ego4D and Epic-Kitchens. The prominent gap further\nvalidates the superiority of our model.\n4.3. Visualization of Generated Frames\nWe additionally showcase examples of generated image\nfrom LEGO and baseline models in Fig. 5. ProxNPI and\nSDEdit fail to understand the user prompt and thus gener-\nate frames of irrelevant actions (e.g. row3). They may also\neasily synthesize the action in a different environment (e.g.\nrow2). InstructPix2Pix is able to preserve more contexts\nbut still fails to semantically align with the action in user\nprompt (e.g. row4). In contrast, LEGO can synthesize the\naction frame that better aligns with the user prompt as well\nas retains more contexts in the background.\n4.4. Ablation Study\nWe present comprehensive ablation studies to investigate\nthe contribution of enriched action descriptions, the VLLM\nimage embedding and the VLLM text embedding sepa-\nConditioning\nCLIP\nEgoVLP EgoVLP+ PSNR\nEgo4D\nDescriptions\n0.791\n0.629\n0.791\n12.200\nDesc.+Img Embed.(w/o FT)\n0.804\n0.645\n0.797\n12.238\n\u2206 (w/o FT - w/ FT)\n-0.003\n-0.009\n-0.004\n-0.001\nDesc.+Txt Embed.(w/o FT)\n0.791\n0.625\n0.788\n12.154\n\u2206 (w/o FT - w/ FT)\n-0.001\n-0.008\n-0.006\n-0.049\nDesc.+Joint Embed.(w/o FT)\n0.802\n0.646\n0.797\n12.222\n\u2206 (w/o FT - w/ FT)\n-0.004\n-0.011\n-0.007\n-0.072\nEpic-Kitchens\nDescriptions\n0.775\n0.437\n0.615\n11.247\nDesc.+Img Embed.(w/o FT)\n0.783\n0.449\n0.620\n11.085\n\u2206 (w/o FT - w/ FT)\n-0.002\n-0.009\n-0.003\n-0.169\nDesc.+Txt Embed.(w/o FT)\n0.774\n0.433\n0.610\n11.134\n\u2206 (w/o FT - w/ FT)\n-0.003\n-0.014\n-0.010\n-0.171\nDesc.+Joint Embed.(w/o FT)\n0.784\n0.447\n0.619\n11.094\n\u2206 (w/o FT - w/ FT)\n-0.002\n-0.012\n-0.008\n-0.240\nTable 4. Performance of LEGO without finetuning (w/o FT). The\nrows highlighted with gray indicate the performance drop when\ncompared with results of the counterparts finetuned using visual\ninstruction tuning (w/ FT) from Tab. 3.\nrately. Results are demonstrated in Tab. 3. Notably, con-\nditioning on enriched action descriptions can moderately\nimprove the model performance, supporting our claim that\nexpanding the action description can facilitate the learning\nof the state change during an egocentric action. Moreover,\nutilizing the image embedding and text embedding from\nVLLM as additional conditions both improve the model\nperformance by a notable margin.\nInterestingly, the im-\nage embedding leads to larger performance boost on both\ndatasets (e.g. 1.6%/1.0% in CLIP score, and 2.5%/2.1% in\nEgoVLP score). These results suggest the VLLM image\nembedding Hi incorporates important high-level semantic\nmeaning that are not captured in the auto-encoder image\nembedding E(X) or VLLM text emebdding Ht. Finally,\nour full LEGO model uses both VLLM image and text em-\nbeddings (Desc.+Joint Embed.) and thus achieves the best\nperformance on both datasets considering all metrics.\n4.5. Analysis of Visual Instruction Tuning\nWe further examine whether the visual instruction tuned\nembeddings can help the diffusion model to better under-\nstand the state change of an egocentric action. As shown\nin Tab. 4, without any finetuning, the image embedding\nfrom the VLLM can still improve the baseline model (De-\nscription) performance on both datasets.\nHowever, the\nmodel using off-the-shelf VLLM text embedding actually\ndecreases the baseline model performance. We speculate\nthis is because, the VLLM may generate hallucinated re-\nsponses without visual instruction tuning, which incurs ad-\nditional barriers for the text encoder to extract useful in-\nformation. In addition, finetuned embeddings yield much\nlarger improvement (see the highlighted rows in Tab. 4)\nacross all embeddings types and all metrics (i.e. a rela-\ntive improvement of 1.7%/2.7% in EgoVLP when using\nJoint Emebed.) on Ego4D/Epic-Kitchens datasets. These\n7\nInput Frame\nProxNPI\nSDEdit\nInstructPix2Pix\nLEGO\n\u201cHow to rinse the jacket inside the plastic bath?\u201d\n\u201cHow to sew the cloth doll?\u201d\n\u201cHow to pour clay mix from the brick mold on the ground?\u201d\n\u201cHow to wipe sink?\u201d\n\u201cHow to knead dough?\u201d\n\u201cHow to take glass?\u201d\nFigure 5. Visualization of the proposed LEGO model and all baselines on Ego4D (the first three rows) and Epic-Kitchens (the last three\nrows). The action frames generated by LEGO align with the user prompt better and preserve more environment contexts in input frames.\nresults suggest that visual instruction tuning can align the\nconditioned embeddings with the instructional prompts and\nthereby significantly benefit the model performance.\n5. Conclusion\nIn this paper, we introduce the novel problem of egocentric\naction frame generation. We also propose a novel model\n\u2014 LEGO, that leverages visual instruction tuning and\na diffusion model to address this problem.\nOur key\nintuition is to use visual instruction tuning to generate\ninformative responses that depict the execution of the\negocentric action, and then design the conditioning of\nthe U-Net to exploit the image and text embeddings\nfrom a visual instruction tuned VLLM. Our experiments\non two large-scale egocentric action datasets validate\nthe advantage of the proposed approach as well as the\ncontribution of each model component.\nWe believe our\n8\nwork provides an important step in understanding the\naction stage change and controllability of diffusion models,\nand suggests future research directions in egocentric\nAI systems, image generation, and human skill transfer.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a visual language model for few-shot learning.\nAdvances in Neural Information Processing Systems, 35:\n23716\u201323736, 2022. 2\n[2] Anonymity. Making multimodal generation easier: When\ndiffusion models meet llms. openreview, 2023. 2\n[3] Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, and\nKristen Grauman.\nHiervl: Learning hierarchical video-\nlanguage embeddings.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 23066\u201323078, 2023. 3\n[4] Joseph H Baskin, Judith G Edersheim, and Bruce H Price.\nIs a picture worth a thousand words? neuroimaging in the\ncourtroom. American Journal of Law & Medicine, 33(2-3):\n239\u2013269, 2007. 2\n[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instruc-\ntions.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 18392\u2013\n18402, 2023. 2, 4, 5, 6, 14, 16\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neu-\nral information processing systems, 33:1877\u20131901, 2020.\n1, 2\n[7] Tuhin Chakrabarty, Kanishk Singh, Arkadiy Saakyan,\nand Smaranda Muresan.\nLearning to follow object-\ncentric image editing instructions faithfully. arXiv preprint\narXiv:2310.19145, 2023. 2\n[8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao,\nEnze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping\nLuo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffu-\nsion transformer for photorealistic text-to-image synthesis.\narXiv preprint arXiv:2310.00426, 2023.\n[9] Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng\nGao, and Chunyuan Li. Llava-interactive: An all-in-one\ndemo for image chat, segmentation, generation and editing.\narXiv preprint arXiv:2311.00571, 2023. 2\n[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 1, 2\n[11] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,\nand Matthieu Cord.\nDiffedit:\nDiffusion-based seman-\ntic image editing with mask guidance.\narXiv preprint\narXiv:2210.11427, 2022. 2\n[12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip: Towards general-\npurpose vision-language models with instruction tuning.\nAdvances in Neural Information Processing Systems, 2023.\n2, 6\n[13] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\nAntonino Furnari, Evangelos Kazakos, Jian Ma, Davide\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, et al.\nRescaling egocentric vision: Collection, pipeline and chal-\nlenges for epic-kitchens-100.\nInternational Journal of\nComputer Vision, pages 1\u201323, 2022. 2, 5\n[14] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Am-\nlan Kar, Richard Higgins, Sanja Fidler, David Fouhey, and\nDima Damen. Epic-kitchens visor benchmark: Video seg-\nmentations and object relations. In Proceedings of the Neu-\nral Information Processing Systems (NeurIPS) Track on\nDatasets and Benchmarks, 2022. 5\n[15] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir\nNachum, Joshua B Tenenbaum, Dale Schuurmans, and\nPieter Abbeel. Learning universal policies via text-guided\nvideo generation. In Thirty-seventh Conference on Neural\nInformation Processing Systems, 2023. 6\n[16] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. In Advances in Neural Information\nProcessing Systems, 2023. 2\n[17] Rohit Girdhar and Kristen Grauman.\nAnticipative video\ntransformer. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 13505\u201313515, 2021.\n3\n[18] Xavier Glorot and Yoshua Bengio. Understanding the dif-\nficulty of training deep feedforward neural networks. In\nProceedings of the thirteenth international conference on\nartificial intelligence and statistics, pages 249\u2013256. JMLR\nWorkshop and Conference Proceedings, 2010. 5\n[19] Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Nicu Sebe,\nTrevor Darrell, Zhangyang Wang, and Humphrey Shi.\nPair-diffusion: Object-level image editing with structure-\nand-appearance paired diffusion models.\narXiv preprint\narXiv:2303.17546, 2023. 2\n[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial networks. Com-\nmunications of the ACM, 63(11):139\u2013144, 2020. 3\n[21] Mohit Goyal, Sahil Modi, Rishabh Goyal, and Saurabh\nGupta. Human hands as probes for interactive object un-\nderstanding. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3293\u2013\n3303, 2022. 3\n[22] Kristen Grauman,\nAndrew Westbury,\nEugene Byrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson\nHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:\nAround the world in 3,000 hours of egocentric video. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 18995\u201319012, 2022.\n2, 5\n9\n[23] Alon Hafri, John C Trueswell, and Russell A Epstein. Neu-\nral representations of observed actions generalize across\nstatic and dynamic visual input. Journal of Neuroscience,\n37(11):3056\u20133071, 2017. 2\n[24] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng\nXu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu\nGuo, et al. Imagebind-llm: Multi-modality instruction tun-\ning. arXiv preprint arXiv:2309.03905, 2023. 2\n[25] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng\nSong, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Di Liu,\nQilong Zhangli, et al. Improving negative-prompt inversion\nvia proximal guidance. Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision, 2024.\n2, 6\n[26] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 2\n[27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 5\n[28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural informa-\ntion processing systems, 33:6840\u20136851, 2020. 2\n[29] https://github.com/christophschuhmann/improved-\naesthetic-predictor, 2023. GitHub. 5, 16\n[30] Jiancheng Huang, Yifan Liu, Jin Qin, and Shifeng Chen.\nKv inversion: Kv embeddings learning for text-conditioned\nreal image action editing. arXiv preprint arXiv:2309.16608,\n2023. 2\n[31] Yifei Huang, Minjie Cai, Zhenqiang Li, and Yoichi Sato.\nPredicting gaze in egocentric video by learning task-\ndependent attention transition. In Proceedings of the Eu-\nropean conference on computer vision (ECCV), pages 754\u2013\n769, 2018. 3\n[32] Yifei Huang, Minjie Cai, Zhenqiang Li, Feng Lu, and\nYoichi Sato. Mutual context network for jointly estimating\negocentric gaze and action. IEEE Transactions on Image\nProcessing, 29:7795\u20137806, 2020. 3\n[33] Forrest N Iandola, Song Han, Matthew W Moskewicz,\nKhalid Ashraf,\nWilliam J Dally,\nand Kurt Keutzer.\nSqueezenet: Alexnet-level accuracy with 50x fewer pa-\nrameters and\u00a1 0.5 mb model size.\narXiv preprint\narXiv:1602.07360, 2016. 5\n[34] Wenqi Jia, Miao Liu, and James M Rehg. Generative adver-\nsarial network for future hand segmentation from egocen-\ntric video. In European Conference on Computer Vision,\npages 639\u2013656. Springer, 2022. 3\n[35] Yitong Jiang, Zhaoyang Zhang, Tianfan Xue, and Jinwei\nGu. Autodir: Automatic all-in-one image restoration with\nlatent diffusion. arXiv preprint arXiv:2310.10123, 2023. 2\n[36] KJ Joseph, Prateksha Udhayanan, Tripti Shukla, Aish-\nwarya Agarwal, Srikrishna Karanam, Koustava Goswami,\nand Balaji Vasan Srinivasan. Iterative multi-granular im-\nage editing using diffusion models.\nProceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, 2024. 2, 6, 14\n[37] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming\nSong. Denoising diffusion restoration models. In Advances\nin Neural Information Processing Systems, 2022. 2\n[38] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Hui-\nwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani.\nImagic: Text-based real image editing with diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 6007\u20136017,\n2023. 2\n[39] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman,\nand Dima Damen.\nEpic-fusion: Audio-visual temporal\nbinding for egocentric action recognition. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 5492\u20135501, 2019. 3\n[40] Sunwoo Kim, Wooseok Jang, Hyunsu Kim, Junho Kim,\nYunjey Choi, Seungryong Kim, and Gayeong Lee. User-\nfriendly image editing with minimal text input: Leverag-\ning captioning and injection techniques.\narXiv preprint\narXiv:2306.02717, 2023. 2\n[41] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment\nanything. arXiv preprint arXiv:2304.02643, 2023. 3\n[42] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating images with multimodal language models.\nAd-\nvances in Neural Information Processing Systems, 2023. 3\n[43] Bolin Lai, Miao Liu, Fiona Ryan, and James M Rehg. In the\neye of transformer: Global-local correlation for egocentric\ngaze estimation. British Machine Vision Conference, 2022.\n3\n[44] Bolin Lai, Fiona Ryan, Wenqi Jia, Miao Liu, and James M\nRehg.\nListen to look into the future: Audio-visual ego-\ncentric gaze anticipation. arXiv preprint arXiv:2305.03907,\n2023. 3\n[45] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion:\nPre-trained subject representation for controllable text-to-\nimage generation and editing. Advances in Neural Infor-\nmation Processing Systems, 2023. 2\n[46] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation.\nIn\nInternational Conference on Machine Learning, pages\n12888\u201312900. PMLR, 2022. 6\n[47] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. Inter-\nnational Conference on Machine Learning, 2023. 2\n[48] Jiaman Li, Karen Liu, and Jiajun Wu. Ego-body pose es-\ntimation via ego-head pose estimation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17142\u201317151, 2023. 3\n[49] Yin Li, Miao Liu, and James M Rehg. In the eye of be-\nholder: Joint learning of gaze and actions in first person\nvideo. In Proceedings of the European conference on com-\nputer vision (ECCV), pages 619\u2013635, 2018. 3\n10\n[50] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-\ngrounded diffusion: Enhancing prompt understanding of\ntext-to-image diffusion models with large language models.\narXiv preprint arXiv:2305.13655, 2023. 2\n[51] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan,\nMichael Wray, Rui Yan, Eric Z XU, Difei Gao, Rong-\nCheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric\nvideo-language pretraining. Advances in Neural Informa-\ntion Processing Systems, 35:7575\u20137586, 2022. 3, 5\n[52] Bolong Liu, Hao Zhang, Jie Liu, and Qiang Wang. Acigs:\nAn automated large-scale crops image generation system\nbased on large visual language multi-modal models.\nIn\n2023 20th Annual IEEE International Conference on Sens-\ning, Communication, and Networking (SECON), pages 7\u2013\n13. IEEE, 2023. 2\n[53] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. Advances in neural infor-\nmation processing systems, 2023. 2, 3, 5\n[54] Miao Liu, Siyu Tang, Yin Li, and James M Rehg. Fore-\ncasting human-object interaction: joint prediction of motor\nattention and actions in first person video.\nIn Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part I 16, pages\n704\u2013721. Springer, 2020. 3\n[55] Shaowei Liu, Subarna Tripathi, Somdeb Majumdar, and Xi-\naolong Wang. Joint hand motion and interaction hotspots\nprediction from egocentric videos. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3282\u20133292, 2022. 3\n[56] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts. In International Conference on\nLearning Representations, 2017. 16\n[57] Zhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani.\nDynamics-regulated kinematic policy for egocentric pose\nestimation.\nAdvances in Neural Information Processing\nSystems, 34:25019\u201325032, 2021. 3\n[58] Chenlin Meng, Yutong He, Yang Song, Jiaming Song,\nJiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSDEdit:\nGuided image synthesis and editing with stochastic differ-\nential equations. In International Conference on Learning\nRepresentations, 2022. 2, 6\n[59] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A\nBrubaker, Jonathan Kelly, Alex Levinshtein, Konstanti-\nnos G Derpanis, and Igor Gilitschenski. Watch your steps:\nLocal image and scene editing by text instructions. arXiv\npreprint arXiv:2308.08947, 2023. 2\n[60] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 2\n[61] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and\nYedid Hoshen. Dreamix: Video diffusion models are gen-\neral video editors. arXiv preprint arXiv:2302.01329, 2023.\n6\n[62] Hadas Orgad, Bahjat Kawar, and Yonatan Belinkov. Edit-\ning implicit assumptions in text-to-image diffusion models.\narXiv preprint arXiv:2303.08084, 2023. 2\n[63] Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen\nHuang.\nEffective real image editing with accelerated\niterative diffusion inversion.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 15912\u201315921, 2023. 2\n[64] Shraman\nPramanick,\nYale\nSong,\nSayan\nNag,\nKevin Qinghong Lin, Hardik Shah, Mike Zheng Shou,\nRama Chellappa, and Pengchuan Zhang.\nEgovlpv2:\nEgocentric video-language pre-training with fusion in the\nbackbone. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 5285\u20135297, 2023.\n3\n[65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021. 4, 5\n[66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J Liu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. The Journal of Ma-\nchine Learning Research, 21(1):5485\u20135551, 2020. 1, 2\n[67] Francesco Ragusa, Giovanni Maria Farinella, and Antonino\nFurnari.\nStillfast:\nAn end-to-end approach for short-\nterm object interaction anticipation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3635\u20133644, 2023. 3\n[68] Santhosh Kumar Ramakrishnan, Ziad Al-Halah, and Kris-\nten Grauman. Naq: Leveraging narrations as queries to su-\npervise episodic memory. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 6694\u20136703, 2023. 3\n[69] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 4, 5\n[70] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 2\n[71] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. Hugginggpt: Solving\nai tasks with chatgpt and its friends in huggingface. arXiv\npreprint arXiv:2303.17580, 2023. 3\n[72] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023.\n2\n[73] George Stein, Jesse C Cresswell, Rasa Hosseinzadeh, Yi\nSui, Brendan Leigh Ross, Valentin Villecroze, Zhaoyan\n11\nLiu, Anthony L Caterini, J Eric T Taylor, and Gabriel\nLoaiza-Ganem. Exposing flaws of generative model evalu-\nation metrics and their unfair treatment of diffusion models.\narXiv preprint arXiv:2306.04675, 2023. 6\n[74] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and\nDeng Cai. Pandagpt: One model to instruction-follow them\nall. arXiv preprint arXiv:2305.16355, 2023. 2\n[75] Swathikiran Sudhakaran, Sergio Escalera, and Oswald\nLanz. Lsta: Long short-term attention for egocentric action\nrecognition. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 9954\u2013\n9963, 2019. 3\n[76] Zhengwentai Sun, Yanghong Zhou, Honghong He, and PY\nMok. Sgdiff: A style guided diffusion model for fashion\nsynthesis. In Proceedings of the 31st ACM International\nConference on Multimedia, pages 8433\u20138442, 2023. 2\n[77] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia\nJin, Taylor Bos, Leslie Baker, Yu Du, et al.\nLamda:\nLanguage models for dialog applications. arXiv preprint\narXiv:2201.08239, 2022. 2\n[78] Denis Tome, Thiemo Alldieck, Patrick Peluse, Gerard\nPons-Moll, Lourdes Agapito, Hernan Badino, and Fer-\nnando De la Torre. Selfpose: 3d egocentric pose estima-\ntion from a headset mounted camera. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2020. 3\n[79] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. arXiv preprint arXiv:2302.13971, 2023. 2\n[80] Linoy Tsaban and Apolin\u00b4ario Passos. Ledits: Real image\nediting with ddpm inversion and semantic guidance. arXiv\npreprint arXiv:2307.00522, 2023. 2\n[81] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Ex-\nact diffusion inversion via coupled transformations. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 22532\u201322541, 2023. 2, 6\n[82] Jian Wang, Diogo Luvizon, Weipeng Xu, Lingjie Liu, Kri-\npasindhu Sarkar, and Christian Theobalt. Scene-aware ego-\ncentric 3d human pose estimation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13031\u201313040, 2023. 3\n[83] Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt,\nand Joost van de Weijer. Dynamic prompt learning: Ad-\ndressing cross-attention leakage for text-based image edit-\ning. arXiv preprint arXiv:2309.15664, 2023. 2\n[84] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka.\nInstructedit:\nImproving automatic masks for diffusion-\nbased image editing with user instructions. arXiv preprint\narXiv:2305.18047, 2023. 2\n[85] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka.\nMdp: A generalized framework for text-guided image edit-\ning by manipulating the diffusion path.\narXiv preprint\narXiv:2303.16765, 2023. 2\n[86] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 6\n[87] Xiaohan Wang, Linchao Zhu, Heng Wang, and Yi Yang.\nInteractive prototype learning for egocentric action recog-\nnition. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 8168\u20138177, 2021. 3\n[88] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 2\n[89] Song Wen, Guian Fang, Renrui Zhang, Peng Gao, Hao\nDong, and Dimitris Metaxas.\nImproving compositional\ntext-to-image generation with large vision-language mod-\nels. arXiv preprint arXiv:2310.06311, 2023. 2\n[90] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong\nWang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talk-\ning, drawing and editing with visual foundation models.\narXiv preprint arXiv:2303.04671, 2023. 3\n[91] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-\nSeng Chua. Next-gpt: Any-to-any multimodal llm. arXiv\npreprint arXiv:2309.05519, 2023. 3\n[92] Yue Xu, Yong-Lu Li, Zhemin Huang, Michael Xu Liu,\nCewu Lu, Yu-Wing Tai, and Chi-Keung Tang. Egopca: A\nnew framework for egocentric hand-object interaction un-\nderstanding. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 5273\u20135284, 2023. 3\n[93] Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello,\nStan Birchfield, Jiaming Song, Shubham Tulsiani, and Sifei\nLiu. Affordance diffusion: Synthesizing hand-object inter-\nactions. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 22479\u2013\n22489, 2023. 3\n[94] Qifan Yu, Juncheng Li, Wentao Ye, Siliang Tang, and Yuet-\ning Zhuang. Interactive data synthesis for systematic vi-\nsion adaptation via llms-aigcs collaboration. arXiv preprint\narXiv:2305.12799, 2023. 2\n[95] Zihao Yu, Haoyang Li, Fangcheng Fu, Xupeng Miao, and\nBin Cui.\nFisedit: Accelerating text-to-image editing via\ncache-enabled sparse diffusion inference.\narXiv preprint\narXiv:2305.17423, 2023. 2\n[96] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video\nunderstanding. arXiv preprint arXiv:2306.02858, 2023. 2\n[97] Mengmi Zhang, Keng Teck Ma, Joo Hwee Lim, Qi Zhao,\nand Jiashi Feng. Deep future gaze: Gaze anticipation on\negocentric videos using adversarial networks. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 4372\u20134381, 2017. 3\n[98] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 5\n[99] Susan Zhang,\nStephen Roller,\nNaman Goyal,\nMikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open\npre-trained transformer language models.\narXiv preprint\narXiv:2205.01068, 2022. 1, 2\n12\n[100] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-\nChih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio\nSavarese, Stefano Ermon, et al. Hive: Harnessing human\nfeedback for instructional visual editing.\narXiv preprint\narXiv:2303.09618, 2023. 2\n[101] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang,\nChongyang Ma, Weiming Dong, and Changsheng Xu.\nInversion-based style transfer with diffusion models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 10146\u201310156, 2023.\n2\n[102] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N\nMetaxas, and Jian Ren. Sine: Single image editing with\ntext-to-image diffusion models.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6027\u20136037, 2023. 2\n[103] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 2\n13\nLEGO: Learning EGOcentric Action Frame Generation\nvia Visual Instruction Tuning\nSupplementary Material\nThis is the supplementary material for the paper titled\n\u201dLEGO: Learning EGOcentric Action Frame Generation\nvia Visual Instruction Tuning\u201d. We organize the content as\nfollows:\nA \u2013 Analysis of Image-to-Text Metrics\nB \u2013 Quality of the Enriched Action Descriptions\nC \u2013 Additional Experiment Results\nC.1 \u2013 Performance at Different Transition Time\nC.2 \u2013 Effect of Dataset Scale\nC.3 \u2013 Generation of New Actions\nC.4 \u2013 Additional Visualization\nD \u2013 More Implementation Details\nD.1 \u2013 Prompt and Examples for Data Curation\nD.2 \u2013 Examples of Data Preprocessing\nD.3 \u2013 Visual Instruction Tuning\nD.4 \u2013 Egocentric Action Frame Generation\nD.5 \u2013 User Study of Generated Action Frames\nE \u2013 Limitation and Future Work\nF \u2013 Code and Data Release\nA. Analysis of Image-to-Text Metrics\nIn Tab. 5, we report the image-to-text CLIP score of In-\nstructPix2Pix (IP2P) [5] baseline and the ground truth. Ide-\nally, the CLIP score between the ground truth and the text\nprompt should serve as a performance uppperbound (UB).\nHowever, the upperbound image-to-text CLIP score is very\nclose to the baseline on Ego4D, and even lower than the\nbaseline on Epic-Kitchens. It suggests the CLIP model fails\nto align action descriptions with corresponding egocentric\nimages in semantics, thus resulting in a quick saturation in\nCLIP score. In our experiments, we use BLIP to caption\nthe generated image and measure the text-to-text similarity\nof captions and action descriptions (following [36]). The\ntwo metrics (BLIP-B and BLIP-L) that use two different\nBLIP structures both result in larger gap between the base-\nline model and the upperbound (3.7%/3.0% vs. 0.9% and\n1.6%/1.4% vs.\u22121.1%). Therefore, we adopt BLIP based\nmetrics and user study to measure image-text alignment.\nNote that, our model still performs on-par or slightly better\nthan IP2P in image-to-text CLIP score, and exceeds IP2P\nnotably when using BLIP based metrics and user study (see\nTab. 2 and Fig. 4 in the main paper).\nB. Quality of the Enriched Action Descriptions\nIn our approach, we finetune a visual large language model\n(VLLM) using visual instruction tuning to generate en-\nMethods\nEgo4D\nEpic-Kitchens\nCLIP BLIP-B BLIP-L\nCLIP BLIP-B BLIP-L\nIP2P [5]\n0.205\n0.200\n0.205\n0.217\n0.254\n0.264\nUpperBound (UB)\n0.214\n0.237\n0.235\n0.206\n0.270\n0.278\n\u2206 = UB\u2212IP2P\n0.009\n0.037\n0.030\n-0.011\n0.016\n0.014\nTable 5. Image-to-text metrics of the baseline model and uppper-\nbound. The gray row shows the gap of IP2P to the upperbound.\nThe upperbound measured by CLIP score is comparable with or\neven lower than the baseline model (highlighted by red).\nFigure 6. The interface used for evaluation of enriched action de-\nscriptions. Both input and target frames are shown to the raters.\nriched action descriptions.\nHere, we provide additional\nanalysis on the quality of generated action descriptions us-\ning user study. We randomly sample 100 examples from the\ntest set of each dataset. For each instance, we show the input\nframe, target frame and the action descriptions from VLLM.\nThe rater is asked to select whether the description aligns\nwith the two frames. We hire 5 raters for each instance on\nAmazon Mechanical Turk and finally get 500 samples for\neach dataset. The interface shown to raters is illustrated\nin Fig. 6. The percentage of samples with aligned frames\nand action descriptions is 87.0% on Ego4D and 84.0% on\nEpic-Kitchens. The results suggest the visual instruction\ntuned VLLM can effectively expand action labels with de-\ntails captured in the input frame.\nC. Additional Experiment Results\nC.1. Performance at Different Transition Time\nAs explained in Sec. 4.1 of main paper, for an action be-\nginning at t, we select the frame at t \u2212 \u03b4i as input and\nthe frame at t + \u03b4o as target. We divide the test data into\n14\n40\n45\n50\n55\n60\n65\n70\nEgoVLP Score\n25\n30\n35\n40\n45\n50\nEgoVLP Score\n<1.0\n1.0~1.7\n1.7~2.4\n>2.4\n<0.7\n0.7~0.9\n0.9~1.1\n>1.1\nEgo4D\nEpic-Kitchens\n65\n70\n75\n80\n85\nCLIP Score\n<0.7\n0.7~0.9\n0.9~1.1\n>1.1\nEgo4D\n60\n65\n70\n75\n80\n85\nCLIP Score\n<1.0\n1.0~1.7 1.7~2.4\n>2.4\nEpic-Kitchens\nAction State Transition Time (s)\nAction State Transition Time (s)\nAction State Transition Time (s)\nAction State Transition Time (s)\nProxNPI\nSDEdit\nInstructPix2Pix\nLEGO (Ours)\nFigure 7. Comparison with baselines at different action transition time. Our model outperforms all baselines across all transition time.\nMethods\nEgo4D\nEpic-Kitchens\nCLIP\nEgoVLP\nEgoVLP+\nFID \u2193\nPSNR\nLPIPS \u2193\nCLIP\nEgoVLP\nEgoVLP+\nFID \u2193\nPSNR\nLPIPS \u2193\nLEGO\n0.806\n0.657\n0.804\n23.830\n12.294\n0.364\n0.786\n0.459\n0.627\n21.566\n11.334\n0.404\nLEGO (scaleup)\n0.809\n0.663\n0.808\n23.560\n12.301\n0.363\n0.789\n0.475\n0.635\n19.635\n11.400\n0.399\nTable 6. Comparison of LEGO trained with single dataset and both datasets (denoted as scaleup). \u2193 means a lower score in this metric\nsuggests a better performance. The better results are highlighted with boldface. The performance of LEGO model can be effectively\nimproved by involving more training data.\nfour bins according to the action state transition time from\ninput frame to target frame \u03b4 = \u03b4i + \u03b4o.\nWe establish\nthe threshold for each bin to ensure that the quantity of\ndata samples in each bin is relatively similar. The perfor-\nmance of LEGO and baselines at different transition time is\ndemonstrated in Fig. 7. The flat curves suggest the egocen-\ntric action frame generation problem is equally challeng-\ning regardless of transition time. Our model still surpasses\nall baselines by a notable margin across all transition time.\nThis fine-grained result further validates the superiority of\nour approach.\nC.2. Effect of Dataset Scale\nWe further analyze how the dataset scale affects our model\nperformance by merging the training set of Ego4D and\nEpic-Kitchens.\nThe training strategy and other hyper-\nparameters remain identical to separate training on each\ndataset. We demonstrate the results in Tab. 6. The perfor-\nmance of our model is further boosted by leveraging more\ntraining data (i.e. scaleup).\nNotably, the gains on Epic-\nKitchens are more prominent than gains on Ego4D (e.g.\n1.6% vs. 0.6% on EgoVLP score, 1.931 vs. 0.270 on FID,\netc.). The possible reason is that Ego4D dataset has more\ntraining data covering more diverse scenarios and actions.\nHence, it can greatly compensate for the low diversity of\nEpic-Kitchens dataset after merging them. The improve-\nment on two datasets suggests our model can be effectively\nboosted by scaling up the training data.\nC.3. Generation of New Actions\nIn addition to generating action frames based on the pre-\ndefined action descriptions in our datasets, we validate\nwhether our approach is able to synthesize the action frames\nconditioning on the same contexts yet different actions\n(novel image-action pairs). Results are illustrated in Fig. 8.\nWe feed different actions with the same input frame to our\nmodel. The synthesized frames correctly depict the exe-\ncution of the actions in the same environment. This result\nfurther indicates that our model can understand the action\nstate change and generalize to different user queries.\nC.4. Additional Visualization\nWe demonstrate more results from our model and the\nground truth in Fig. 11.\nThe generated frames are well\naligned with the user query and the ground truth except\nsome typical failure cases (last two rows). Our approach\nmay fail to associate the action with the correct objects\nwhen the objects are not distinct enough in the egocentric\nperspective, e.g. the marker and croissant in the first row\nof failure cases. In addition, generating the action frame\nin some scenarios needs more contexts than a static input\nframe. For example, the model fails to understand which\nobject is the furniture and incorrectly drives the nails into\nthe wood under it (i.e. the second failure case of Ego4D).\nIt also lacks the context that the user already holds a bag\nof noodles, so it synthesizes a frame of taking out the noo-\ndles from a cupboard (i.e. the second failure case of Epic-\nKitchens). These weaknesses can inspire more future stud-\nies in egocentric action frame generation. Please refer to\n15\n\u201cCan you provide instructions \non how to {action} in my \ncurrent situation?\u201d\n\u201c\u2026 open drawer \u2026\u201d\n\u201c\u2026 take knife \u2026\u201d\n\u201c\u2026 cut cucumber \u2026\u201d\n\u201c\u2026 open microwave \u2026\u201d\n\u201c\u2026 pick up bowl \u2026\u201d\n\u201c\u2026 dry hands\u2026\u201d\nLEGO\nFigure 8. Visualization of generating various actions from the same input frame. The first action (\u201dopen drawer\u201d) is the existing label for\nthis example in the dataset. We fill another five actions into the user query and synthesize the action frames using our model. All generated\nframes align well with the actions and preserve most contexts provided by the user.\nAppendix E for more discussions.\nD. More Implementation Details\nD.1. Prompt and Examples for Data Curation\nIn data curation, we randomly select 12 examples from\neach datasets covering diverse scenarios. All examples are\nshown in Fig. 13 and Fig. 14. We also clarify our require-\nments in the prompt sent to GPT-3.5. The complete prompt\nis shown in Fig. 9. We specify the composition of input\nquery, the expected detailed information and extra require-\nments for the output in the system information. Then we\ninput the examples for in-context learning and the example\nfor inference in the user input.\nD.2. Examples of Data Preprocessing\nFor each input frame or target frame, we calculate aesthetic\nscores [29] of the current frame as well as 3 frames before\nand after (7 frames in total). As demonstrated in Fig. 12(a),\nthe frame of the highest aesthetic score usually has the best\nimage quality. We then use this frame as input or target\nframe. We also calculate the similarity of input and target\nframe for each action. We empirically filter out data whose\nsimilarity is lower than 0.81 or higher than 0.97. Some\nexamples of abandoned data are shown in Fig. 12(b). A\nvery low similarity usually indicates a big change in the\nbackground due to the head motion. A very high similar-\nity implies the action involves very small hand movements.\nIt\u2019s hard for generative models to learn action generation in\nthese data samples.\nD.3. Visual Instruction Tuning\nWe train the model with a batch size of 128 and a learning\nrate of 2\u00d710\u22125. Warm-up strategy and consine anneal [56]\nare also used in training. It takes 24 hours to train the model\non 8 NVIDIA A100-SXM4-40GB for 3 epochs. Please re-\nfer to Fig. 8 for the example of incorporating action label\ninto action instruction query templates.\nD.4. Egocentric Action Frame Generation\nWe finetune the latent diffusion model with a batch size of\n256 and an initial learning rate of 10\u22124 without warm-up\nstrategy. Horizontal flipping is used as data augmentation\nduring training. We train the model for 20,000 iterations on\n8 NVIDIA A100-SXM4-40GB over 38 hours. In inference,\nwe apply 100 denoising steps for each instance.\nWe also use classifier-free guidance for two conditions\n(following [5]) by sharing the same guidance scale across\nthe enriched action descriptions, VLLM image embeddings\nand VLLM text embeddings. As defined in Sec. 3.2 in main\npaper, we use C to denote the three conditions and use X to\ndenote the input frame. Specifically, we randomly set only\nthe image conditioning X = \u2205 at a probability of 5%, only\nthe conditioning from VLLM C = \u2205 at a probability of 5%\nand both X = \u2205 and C = \u2205 at a probability of 5%. Then\nthe score estimate of our model is\n\u02dce\u03b8(zt, X, C) = e\u03b8(zt, \u2205, \u2205)\n(3)\n+ sx \u00b7 (e\u03b8(zt, X, \u2205) \u2212 e\u03b8(zt, \u2205, \u2205))\n(4)\n+ sc \u00b7 (e\u03b8(zt, X, C) \u2212 e\u03b8(zt, X, \u2205)),\n(5)\nwhere \u03b8 refers to the parameters in the denoising UNet. zt\nis the noisy latent at timestep t, which is obtained by diffu-\nsion process in training and randomly initialized by a gaus-\nsian noise in inference. sx and sc are the guidance scales\ncorresponding to the conditioning X and C respectively. In\ninference, we use sx = 7.5 and sc = 1.5 which are identical\nto the settings in InstructPix2Pix [5].\n16\nSystem: You are an AI assistant that provides a description of an image based on the action and object context. \nThe action consists of a verb and nouns. Each object location is represented by a bounding box. For each \nbounding box, four numbers are provided in brackets \u2013 they are [x-coordinate of top-left, y-coordinate of top-\nleft, x-coordinate of bottom-right, y-coordinate of bottom-right]. The origin is at the top-left of each frame. The \nx-axis is on the top and the y-axis is on the left. All coordinates are normalized to the range from 0 to 1. This \ninformation can be used to infer the spatial relation of hands and objects. Note that the detailed narration is in \na natural and holistic style. Please add more details in the action. For example, try to describe which hand is \nused in each action like \u201cwith right hand\u201d or \u201cusing left hand\u201d. Try to describe the spatial relation of these \nobjects like \u201con the right\u201d, \u201con the left\u201d, \u201cfrom \u2026 to \u2026\u201d or use some spatial words like \u201cin\u201d, \u201con\u201d, \u201cout\u201d, \n\u201cfront\u201d, \u201cback\u201d, etc. Describe the image in only one sentence. Do not describe objects or actions that are not \npresented in action or objects locations context. Many examples are provided for learning and an additional \nexample is provided for inference.\nUser: Examples for learning: (1) {Example-1} (2) {Example-2} \u2026 (12) {Example-12}\nUser: Example for inference: {Inference Example}\nFigure 9. The structure of the prompt sent to GPT-3.5. We specify the composition of the input query (highlighted in blue). Then we\narticulate the requirements for action enrichment (highlighted in green) and extra demands (highlighted in yellow). Example-1 to Example-\n12 consist of the action label, object bounding boxes and manual annotation of detailed action descriptions. The inference example consists\nof just action label and object bounding boxes.\nFigure 10. The interface used for evaluation of generated action\nframes. The four generated frames are randomly shuffled to avoid\npotential bias.\nD.5. User Study of Generated Action Frames\nThe user interface for evaluation of generated action frames\nis illustrated in Fig. 10. We show the input frame and shuf-\nfled outputs from four models to raters. To make a fair com-\nparison, we show the action label instead of the enriched ac-\ntion description because the baseline models only take orig-\ninal action labels as input.\nE. Limitation and Future Work\nIn this paper, we use an egocentric image to capture the\nuser\u2019s environment contexts and generate the action frame\nto provide visual instructions. However, there are still some\nproblems that are not explicitly solved by our method. We\nfind it\u2019s hard for our model to associate the action with\ncorrect objects when there are too many irrelevant objects\naround. Synthesizing the diverse and complex hand-object\ninteractions is also a big challenge especially when people\nare operating some machines. In addition, our work indi-\ncates a few valuable directions for the future study.\n\u2022 The embeddings from visual large language model\n(VLLM) are fed into the UNet together with the CLIP\nbased text representation. How to leverage the VLLM\nembeddings more effectively in diffusion models de-\nserves future study.\n\u2022 Recognizing and localizing the objects that are relevant\nwith the action descriptions in a chaotic environment may\nbe a bottleneck for the application in real-world problems,\nwhich deserves more attention.\n\u2022 It\u2019s difficult to synthesize correct interactions of hands\nand objects in some professional work, such as using a\nwood cutter, operating a lawn mower and sewing clothes\non a sewing machine. How to combine affordance un-\nderstanding with generative models may be a key step to\naddress this problem.\n\u2022 Existing\nautomatic\nimage-to-text\nsimilarity\nmetric\ndoesn\u2019t generalize well to the egocentric domain.\nWe\n17\nexpect more investigation of better evaluation metrics for\nimage-text alignment.\nF. Code and Data Release\nThe code, pre-trained model weights, additional data anno-\ntations and our train/test split will be made publicly avail-\nable to the research community to facilitate future studies.\n18\nInput Frame\nLEGO\nGround Truth\n\u201cHow to pick a birthday card  in the drawer?\u201d\n\u201cHow to cut out a portion of clay mix with both hands?\u201d\n\u201cHow to brush a wood with a brush?\u201d\n\u201cHow to adjust the lawn mower gear?\u201d\n\u201cHow to scrap the wallpaper on the wall of the \napartment with the big scrapper?\u201d\n\u201cHow to use the phone on table?\u201d\nInput Frame\nLEGO\nGround Truth\n\u201cHow to open oil?\u201d\n\u201cHow to put tray in oven?\u201d\n\u201cHow to pick up tray?\u201d\n\u201cHow to close container?\u201d\n\u201cHow to open drawer?\u201d\n\u201cHow to open bin?\u201d\n\u201cHow to take soy milk?\u201d\nFailure Cases\n\u201cHow to place the wood plank on the wood cutter?\u201d\n\u201cHow to take the marker from the floor?\u201d\n\u201cHow to drive nails into the furniture with the nail gun?\u201d\n\u201cHow to put croissant on pan?\u201d\n\u201cHow to take out the noodles?\u201d\nFigure 11. Additional visualization of LEGO output on Ego4D (on the left of the dash line) and Epic-Kitchens (on the right of the dash\nline). We also show the ground truth for reference. In the last two rows, we demonstrate some failure cases on the two datasets.\n19\n4.685\n4.947\n4.831\n4.683\n4.542\n4.742\n4.368\n(a) Aesthetic Scores \nof Neighboring Frames\nSimilarity: 0.643\nSimilarity: 0.656\nInput Frame\nTarget Frame\nInput Frame\nTarget Frame\nSimilarity: 0.994\nSimilarity: 0.990\n(b) Examples with Very Low and Very High Similarity of Input and Target Frames\nFigure 12. Data preprocessing in our work. (a) The fame with the highest aesthetic score (highlighted) is less blurry and then used as the\ntarget frame of this action. (b) The actions with too low (<0.81) or too high similarity (>0.97) between input and target frames are filtered\nout from the datasets.\n20\nAction Label: \u201ctighten the nut\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person holds \nthe nut using the left hand and tighten it \nwith the screwdriver in the right hand.\u201d\nAction Label: \u201cshake something\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person holds \nand shakes something in the right hand.\u201d\nAction Label: \u201cadjust wrist watch\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person uses \nthe right hand to adjust the wrist watch \nwore on the left wrist.\u201d\nAction Label: \u201cplace the card in \nbetween other cards\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person uses \nthe right hand to place the card in other \ncards held in the left hand.\u201d\nAction Label: \u201cdrop the slice of tomato \non a plate\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person drops \nthe slice of tomato in the right hand on a \nplate on the right.\u201d\nAction Label: \u201cdrop the game console \non the table\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person drops \nthe game console in hands on the table.\u201d\nAction Label: \u201cpick phone\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person \npicks the phone using the right hand.\u201d\nAction Label: \u201cattach the second screw to \nthe first bicycle with his right hand\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person attaches \nthe screw in the right hand to the bike.\u201d\nAction Label: \u201cmark the wood\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person \npresses the wood using the left hand and \nmakes a mark using the pencil in the \nright hand.\u201d\nAction Label: \u201cpress dough in \ncontainer with left hand\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person uses \nthe left hand to press the dough in a \ncontainer in front of him.\u201d\nAction Label: \u201ccheck on a fabric\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person \nreaches out to a fabric using the right \nhand and checks its status by touching it.\u201d\nAction Label: \u201cgo up the ladder\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person holds \non to the ladder using the left hand and \ngoes up the ladder.\u201d\nFigure 13. All Ego4D examples used for data curation from GPT-3.5 via in-context learning. For simplicity, the bounding boxes are only\nshown on images. We input the coordinates of bounding boxes to GPT-3.5 in practice.\n21\nAction Label: \u201copen fridge\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person holds \nthe handle of the fridge and opens the \nfridge door using the right hand.\u201d\nAction Label: \u201cpick up yoghurt\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person \nreaches out to a yoghurt and picks it up \nusing the left hand.\u201d\nAction Label: \u201cput knife\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person holds \na knife in right hand and puts it down.\u201d\nAction Label: \u201ctake cutting board\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person takes \nthe cutting board out of a cupboard \nusing right hand.\u201d\nAction Label: \u201cdry hands\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe man picks a \ncloth beside the cutting board and dries \nboth hands.\u201d\nAction Label: \u201cclose bin\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person \nmoves the left hand out of the bin and \ncloses it using the right hand.\u201d\nAction Label: \u201cchop end of onion\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person presses \nthe onion on the chopping board with the \nleft hand and then cuts off the end of the \nonion with a knife in the right hand.\u201d\nAction Label: \u201cpeel onion\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person holds \nan onion in the left hand over a chopping \nboard and uses the knife in the right \nhand to peel the onion.\u201d\nAction Label: \u201copen tap\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person\u2019s left \nhand is holding a pan under a tap and the \nperson opens the tap with the right hand.\u201d\nAction Label: \u201cfill pan with water\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person holds a \npan under a running tap using the left hand \nand lets water fill the pan.\u201d\nAction Label: \u201ccontinue stirring onions\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person holds \nthe pan using the left hand and stirs the \nonion in the pan with a spatula in the right \nhand.\u201d\nAction Label: \u201cput chicken into pan\u201d\nBounding Boxes: (shown on image)\nAction Descriptions: \u201cThe person holds a \nchopping board using the left hand and \nmoves chicken from the chopping board into \nthe pan using a knife in the right hand.\u201d\nFigure 14. All Epic-Kitchens examples used for data curation from GPT-3.5 via in-context learning. For simplicity, the bounding boxes\nare only shown on images. We input the coordinates of bounding boxes to GPT-3.5 in practice.\n22\n"
  }
]